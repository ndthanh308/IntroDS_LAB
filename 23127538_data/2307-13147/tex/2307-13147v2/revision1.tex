\documentclass[11pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\twocolumns{0}
\def\viewchanges{1}
\def\addToC{0}
\def\jmlrstyle{1}
\def\preprint{1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc} %I allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[round]{natbib}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumerate}
\graphicspath{ {./images/} }
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{lipsum}
\usepackage{multirow}
\usepackage{array}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{enumitem}
\usepackage{lscape}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{csvsimple}
\usepackage{csquotes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\N}{\mathbb{N}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\1}{\mathbf{1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\bb}{\textcolor{blue}}
\newcommand{\OurAlgo}{RLSM}
\newcommand{\OurAlgoS}{RLSM }
\newcommand{\OurAlgoR}{RFQI}
\newcommand{\OurAlgoRS}{RFQI }
\newcommand{\OurAlgoRR}{RRLSM}
\newcommand{\OurAlgoRRS}{RRLSM }
%\newcommand{\OurAlgo}{Lisa}
%\newcommand{\OurAlgoS}{Lisa }
%\newcommand{\OurAlgoR}{Flora}
%\newcommand{\OurAlgoRS}{Flora }
%\newcommand{\OurAlgoRR}{Rosa}
%\newcommand{\OurAlgoRRS}{Rosa }
\newcolumntype{P}[1]{>{\centering\arraybackslash }p{#1}}
\renewcommand{\algorithmiccomment}[1]{\bgroup\hfill ~#1\egroup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{xr}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% chasing changes with colors:
\usepackage[dvipsnames]{xcolor}
\usepackage{colortbl}
\definecolor{Gray}{gray}{0.85}
\let\add\undefined
\let\del\undefined
\let\com\undefined
\let\cha\undefined
\newcommand{\g}{\textcolor{red}}
%%% display changes in color:
\if\viewchanges1
	\newcommand{\add}[1]{{\color{ForestGreen} #1}}
	\newcommand{\del}[1]{{\color{red} #1}}
	\newcommand{\com}[1]{{\color{orange} #1}}
	\newcommand{\cha}[2]{{\textcolor{red}{#1}} {\textcolor{ForestGreen}{#2}}}
%%% apply changes:
\else
	\newcommand{\add}[1]{{#1}}
	\newcommand{\del}[1]{}
	\newcommand{\com}[1]{}
	\newcommand{\cha}[2]{{#2}}
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[preprint]{tmlr}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Answers to Reviewers}
\author{}

\begin{document}
\maketitle


\section{Reviewer Zmm1}

We are grateful for your very positive feedback on our paper and the suggestions to further improve it. 
Below, we separately answer each point in detail. Additionally, we highlighted the changes relevant for all reviewers in colour.

\begin{itemize}
    \item As suggested by the reviewer, we added a notation glossary section at the end of the appendix. In the paper we added hyperlinks that point to the corresponding entries in the notation glossary.

    \item We also added a pseudo code section to the appendix, where we provide pseudo code for the forward pass through the PD-NJ-ODE method, the computation of the loss and the training of the method. We note however, that we didn't provide pseudo code for the computation of the signature, since there are a handful of specialized python packages for this purpose. We listed 3 of them in the paper now, including the one we use in our implementation. We added a mathematical definition of the truncated signature $\pi_m$ into the paper.

    \item We agree with the reviewer that experiments on real data are needed to show the methods practical usefulness. We want to note however, that this only applies for the extension with noisy observations, since our theoretical results of this paper show that the framework used in the predecessor paper \citep{krach2022optimal} can deal with dependence between the observed process and the observation framework. 
    Moreover, there does not exist a version of PD-NJ-ODEs that \emph{cannot} deal with this dependence.
    Hence, the results on real world data that were shown in \cite{krach2022optimal} are representative here as well. The usefulness of the dependence result is purely theoretical, by showing that the existing method is also theoretically sound under more general settings.

    On the other hand, the extension to noisy observations actually uses a different loss function and therefore allows for a comparison between the previous method (which could not deal with observation noise) and the new method on real world data.

    Therefore, we added the new Section~3.5 to the paper, where we speak about the practical relevance of the noise-adapted loss function depending on the the problem setting. Moreover, we added experiments on real world data in Section~5.3, where the practical importance of the noise-adapted loss function becomes visible, when comparing it to the original loss function.
    
\end{itemize}


\section{Reviewer oEbU}

We thank you for your detailed feedback and your great suggestions to improve the paper. In the following, we answer each point in detail. Additionally, we highlighted the changes relevant for all reviewers in colour.

\subsection{Weaknesses}
\begin{itemize}
    \item Even though our setting in Section~3.1 focuses on centered (zero-mean) noise, we extend this setting in Section~3.3 to more general noise structure, including deterministically biased noise or even bias functions depending on the previous observations.  

    Additionally, we want to highlight that even in the centered case, the results are not immediate. 
    You are correct that the $L^2$ minimizer with centered noise is also the $L^2$ minimizer without noise. But the important subtlety is that this is true everywhere except at observation times. At an observation time we have in general that $\hat{X}_{t_i} = \E[X_{t_{i}} \, | \, O_{t_0}, \dotsc, O_{t_{i}}] \neq \hat{O}_{t_i} = \E[O_{t_{i}} \, | \, O_{t_0}, \dotsc, O_{t_{i}}]=O_{t_{i}}$.
    By the definition of the original loss function of the  PD-NJ-ODE, it learns $\hat{O}_t$, and therefore the wrong behaviour at observations. In contrast to this, our noise-adapted version of it provably learns to predict $\hat{X}_t$. The different behaviours can be observed in the comparison in Figure~1.
    
    To summarize: We agree for classical regression you don't need a different loss for zero-mean noise (the classical L2-loss obviously works for both settings - noise-less and zero-mean noise). However in our paper we point out the surprising fact, that for PD-NJ-ODEs one can \emph{not} use the same loss for zero-mean noise; instead we had to propose a \emph{new} loss that can deal with noisy observations, while the old loss used by \citet{krach2022optimal} does \emph{not} work for zero-mean noise (see Figure~1).

    Our contribution with respect to noise is not only that we provide a loss and a theory for noisy settings with known mean. Our second contribution with respect to noise is that we explain that even in the case of zero-mean noise the classical L2-loss proposed in \citet{krach2022optimal} does not lead to the correct result. We think this result is particularly relevant, since some people might be mislead by statements such as \enquote{the L2 minimizer with zero-mean noise is also the L2 minimizer without noise} to believe that they can use the same classical L2-loss proposed in \citet{krach2022optimal} for zero-mean noise settings. But we show in Figure~1 that this is not the case.
    We agree that mathematically the proof is not extremely advanced, but in practice, in the case of zero-mean noise, it can have a big impact if people use our loss rather than the one proposed by \citet{krach2022optimal} (see Figure~1).

    Further note that zero-mean noise is very common in practice (for example in the case of measurement-noise). This is a very common assumption in various Machine-learning regression settings. If one only observes noisy versions $O$ of $X$ and never any noise free version of $X$ during training it is impossible for any method to get rid of the bias of the noise. For example if the weight-scale at a hospital always measures 1 kg too much of body weight. It is impossible to see only from this data that the actual weight of all patients was always 1kg lower than measured. The setting we are studying is learning how to forecasting an unknown dynamic from data, where the training data only consists of noisy observations $O$. Filtering is a different problem, where one observes $X$ and $Y$ during training (or the dynamics are even known a priori) and then during inference one only observes $Y$ to forecast $X$. As mentioned in Section 1.2, we can easily extend our setting to these filtering tasks, where $Y$ and $X$ can have a very complicated relationship as already explained in \cite{krach2022optimal}. (We tried to additionally clarify this in the Related Work section of the updated version of the paper.) One has to be careful not to confuse the role of $Y$ in the filtering problem with the role of $O$ in the forecasting problem.
    
    \item Thank you for noting that the work was not self contained. We added the definition of the (truncated) signature to the paper and also made the paper more self-contained at other points in accordance with the suggestion of the other reviewer.

    \item You are right that the examples we give are rather simple. However, their main purpose is to illustrate the main theoretical contributions of our work experimentally. We note that our theoretical results prove that our method should work in nearly any higher-dimensional or more complex setting, which is also supported by the experimental results on real world datasets presented in \citet{krach2022optimal}. Especially for the purely theoretical extension to dependence between the underlying process and the observation framework, these experiments of \citet{krach2022optimal} are representative, since the method itself didn't change. 
    In particular, it is very likely that there actually is such a dependence in the Physionet dataset, as discussed in the Introduction and in Section~4.1. Hence, our paper provides the theoretical foundation for those empirical results of \citet{krach2022optimal} and, vice versa, those results show that the method discussed in our paper is applicable to complex high-dimensional settings.
    
    We also added a note on this to the paper.

    On the other hand, our method \emph{did} change for the case of zero-mean (or known mean) noise. Therefore, we now added experiments in higher dimensions based on real word-data to show the benefits of our new method in the presence of zero-mean noise.

    \item You are right that we need to take gradients through time during training (not when evaluating the PD-NJ-ODE during inference). However, this is not a big issue and the method scales to higher dimensional settings as was shown in the experiments on real world datasets in \citet{krach2022optimal}. Note that our method is differentiable w.r.t. the parameters $\theta$ in contrast to particle filters. The reason why we only have 1-d experiments is that we consider this work as an extension of the previous work, where the practical applicability of the method in higher-dimensional and more complex settings was already shown experimentally. In this work we mainly wanted to illustrate the (theoretical) extensions with simply synthetic datasets.
    However, we added high-dimensional experiments that show the advantages of our new loss in the presence of zero-mean noise. The training on the real-world dataset takes ca. 1 day  on a CPU (would be faster on a GPU), but inference (i.e., making forecasts based on a new path) takes less than a second. 

    \item Thank you for the references to particle filters. Even though particle filters are used for similar problems as the one studied in our paper, there are several (theoretical and practical) differences such that they cannot be applied to our problem setting. 
    Particle filtering methods are applied in the context of state-space models (SSM), which are characterized by a discrete latent Markov process $(X_t)_{t\geq 1}$ and a discrete observation process $(Y_t)_{t\geq 1}$ defined on a fixed time-grid. 
    Particle filters are used to approximate e.g.\ the conditional distribution of $X_t$ given the observations $(Y_s)_{1\leq s\leq t}$, or the joint distribution of $(X_s, Y_s)_{1 \leq s \leq t}$, for any $t\geq 1$, using weighted sequential Monte Carlo samples. 
    In our work, we allow for a much more general setting than the SSM. In particular, we allow for a continuous-time (instead of discrete-time), non-Markovian stochastic process. Since our setting allows for jumps of the process, this also includes the SSM case of a discrete-time Markov process. 
    Moreover, in our setting, the underlying process can be observed at random, irregularly sampled, discrete observation times and the framework allows for incomplete observations, where some coordinates might not be observed. This irregular observation times are not fixed, but they can vary substantially from one path to another.
    Also the primary goal of the neural jump ODE method is different, trying to make optimal forecasts for $X_t$ given all observations of $X$ prior to time $t$. As a special case (since the framework can deal with incomplete observations), this includes the filtering problem, however, in a more general setting, allowing for example to predict $X_t$ while only having (discrete) observations of $Y$ at randomly sampled observation times until time $s < t$. This was explained in \citet{krach2022optimal}. 
    In principle, the filtering approach could also be used to deal with noisy observations. However, this would require some type of knowledge of the underlying distribution and the noise distribution (or access to noise-free observations of $X$ during training). In contrast to this, our extension for noisy observation, doesn't need such knowledge, i.e., we do not need any assumption what the underlying distribution of $X$ is, but it is enough to observe the noisy data samples.
    Another big part of our work are the proofs showing that we can learn the dynamics only from observing finitely many noisy observations $O$ from sufficiently many paths (without any noise-free observations of $X$). We do not find such results in any of the papers on particle filters and SSMs you cited.

    For these reasons, particle filters are not an alternative to the neural jump ODE method in these general settings.

    %\add{Draft: Just to avoid misunderstandings. We assume that the dynamics of $X$ are completely unknown and in the noisy case we never observe $X$. We prove that without knowing the dynamics of $X$, we can learn the dynamics only from observing finitely many noisy observations $O$ from sufficiently many paths. We do not find such results in any of the papers on particle filters and SSMs you cited. Did we overlook something? The papers you cited mainly focused on how to solve the filtering problems once the dynamics are known, but we see very little results on how to learn the dynamics consistently in the first place.
    As far as we understood, all the particle filter and SSM papers you cited consider a fixed time grid, while the main focus of our work lies on irregular random observation times. %Did we overlook something on irregular observations in your citations? 
    %}
\end{itemize}

\subsection{Requested changes}
\begin{itemize}
    \item As requested, we added experiments on a higher-dimensional real world dataset in Section~5.3.
    
    \item As requested, some explanation about particle filters and why they are not suitable as baseline was added to the related work.
\end{itemize}



\section{Reviewer g67F}

We thank you for your detailed feedback and the numerous suggestions to improve the paper. In particular, we thank you for the numerous remarks to make the paper more self-contained (which is definitely a great benefit for readers who are not familiar with \citet{krach2022optimal}) as well as your suggestions to explain additional insights. We tried to address all these remarks, while not repeating too much of the content of \citet{krach2022optimal}.
In the following, we answer each point in detail. Additionally, we highlighted the changes relevant for all reviewers in colour. 

\begin{enumerate}
    \item We added more details to the abstract on how we propose to resolve the prior limitations.

    \item We kept the main title of section 2 as ``Recall: the PD-NJ-ODE'' (since recalling the previous framework is the main purpose of the section) but called the new subsection 2.2 ``Technical Background and Mathematical Notation'' as you suggested.

    \item Since Section 2 is meant to recall (the most important parts of) the PD-NJ-ODE framework from \cite{krach2022optimal}, we didn't include additional information like this, because we did not want to be too repetitive wrt. \cite{krach2022optimal}.
    However, we agree with you that the definition of $\tilde{X}^{\leq t}$ is rather complex and therefore added the additional information you asked for, also including more details why the Doob-Dynkin Lemma applies. For an explanation of the Doob-Dynkin lemma, see the answer to point 13.

    \item Since Section 2 is recalling the setup of the PD-NJ-ODE (as noted in the very beginning of it), we do not think that there is a problem with mentioning PD-NJ-ODE before its formal definition in Definition~2.5. In particular, we speak about the framework defined in the cited work \cite{krach2022optimal}, which is, in that sense, already introduced in the abstract and again in the introduction.

    \item We added the new Definition~2.4, which formally introduces the signature and the truncated signature $\pi_m$. $\tau(t)$ is introduced in the list in the beginning of Section~2.2 (where leaving away $\tilde{\omega}$ is the standard notation for random variables).
    You are right that $H_t$ is a latent process depending on the past observations through the signature of $\tilde{X}$. $Y_t$ is the output of the PD-NJ-ODE model (as mentioned in the paper). $Y_t$ is not the observed process (we do not use the notation of filtering here). $Y_t$ is an estimator for the conditional expectation $\hat{X}_t=\E[X_t | \mathcal{A}_{t-}]$. In simple words, at time $s$, $\lim_{m\to\infty}Y_t^{\theta_{m,N_m}^\text{min}}(\tilde{X}^{\leq s}$ is the optimal forecast/prediction for the future value $X_t$ given all the (noisy) observations of $X$ observed up to time $s$. To repeat, we use the notation of forecasting, where we have (noisy) observations $O_{t_k}$ of $X$ and we want to predict the expected value of future values $X_t$. We do not use the notation of filtering, i.e., we do not observe any other process $Y$ in order to predict $X$. If one would apply our method to filtering than one would consider some coordinates of $X$ to be the observed process and other coordinates of $X$ to be the coordinates we want to predict and use the observations masks $M$; but we never use the variable $Y$ to denote the observed process (except for section 1.1 where we discuss related filtering work).
    We added a short explanation for the intuition of the model and its single parts, while trying not to repeat too much of the previous papers. 

    \item We added more details for the existence and uniqueness of the PD-NJ-ODE model. In particular, we made it more precise that Lipschitz continuous activation functions are needed in the neural networks and cited the more detailed existence results of \citet{cohen2015stochastic}. Since we integrate with respect to finite variation processes, integrability is trivially satisfied in a path-wise manner. No additional assumptions on the observations are needed, because the neural network structure together with the finite variation integrators are regular enough. We added a footnote citing all relevant definitions giving short explanations why they are satisfied.
    Note, that we are only stating that given fixed deterministic parameters $\theta$, given fixed deterministic observation data, the fixed deterministic ODE has an unique solution. (Given any finite number of training paths $N$ and a large model complexity $m$, there are multiple $\theta$ that minimize the loss, that result in different $Y^{\theta}$.)

    \item We added a short explanation for the left limits after Definition 2.4, where this notation was first used. Additionally, we added a short explanation of the two terms in the loss function (3) as you requested. Moreover, in the new Section 3.5 we added a discussion on the practical difference between the original loss function (3) and the noise-adapted loss function (10).

    \item We added Definition~2.4 where the signature and the truncated signature of a path are defined.

    \item As explained in the beginning of Section 2.2, $n$ is a random variable defining the number of observations. In particular, for any fixed $(\omega, \Tilde{\omega})$, $n(\tilde{\omega})$ is the number of observations along the path of $X(\omega)$. At any time $t$, there can either be an observation (if $t=t_k(\Tilde{\omega})$ for some $k$) or not (the number of coordinates that are observed at an observation time $t_k$ can be inferred from $M_k$). 
    
    As is standard when working with data, we then introduce $N$ i.i.d.\ random variables and processes $(X^{(j)}, M^{(j)}, n^{(j)}, t_1^{(j)},  \dotsc, t^{(j)}_{n^{(j)}})$ with the same distribution as $X$ and the observation framework $(M, n, t_1,  \dotsc, t_n)$, which correspond to the realizations of the training paths. With this we can define the Monte Carlo approximation (5) of the (theoretical) loss function (3). This is important, since clearly, only the Monte Carlo approximation (5) can actually be computed with data. We tried to make this a bit clearer in the paper.
    
    Since we consider multiple i.i.d.\ realizations $X^{(j)}$ and $(M^{(j)}, n^{(j)}, t_1^{(j)},  \dotsc, t^{(j)}_{n^{(j)}})$ in the Monte Carlo approximation, it can of course happen, that some of them are observed at the same time (i.e., that in one state of the world $\tilde{\omega}$, there exist $j_1, j_2$ and $k_1, k_2$ s.t.\ $t_{k_1}^{(j_1)}(\tilde{\omega}) = t_{k_2}^{(j_2)}(\tilde{\omega})$). However, this is not relevant for our analysis.

    We have added a sentence explaining $n^{(j)}$ in the notation glossary of $n$, which should (in combination with the new intuitive Section 2.1) answer your question. In simple words, for different paths (e.g., corresponding to different patients) $j$ in our training data-set, we can have different numbers of observations $n^{(j)}$.

    \item Our results show that, given we find the minimiser of the loss function, the output $Y$ of the PD-NJ-ODE model converges to the optimal prediction, i.e., to the conditional expectation $\hat{X}$ (which is proven to be the unique optimizer of the loss function). In particular, the result shows that learning the optimal prediction can be done by finding the optimizer to our loss function (i.e., by finding the optimal weights $\theta$ for the neural network), which is not trivially clear. Without this result, one wouldn't know whether optimizing the loss function yields a model output that is helpful for our problem (i.e., close to the optimal prediction). Moreover, one would not even know that the model framework can in principle approximate the conditional expectation arbitrarily well (i.e., that the minimizer of (4) is also a minimizer of (3), since the minimum in (4) is taken over a much smaller set than in (3)). A less flexible model might not be able to do this. Furthermore, we show the corresponding convergence results for the minimizers of the empirical loss function (5).
    
    You are correct that we do not consider the problem of finding the minimizer of the loss function. The task of finding global minimizers of loss functions with neural network methods is a quite large field on its own, which is somewhat orthogonal to our research. Orthogonal in the sense that any result there can be used in combination with our results to show convergence with respect to the respective optimisation scheme. This was discussed in more detail in the first work \citet{herrera2021neural}. We added a citation to this discussion as well as a short explanation of the theorem as requested. In our experiments adam stochastic gradient descent works sufficiently well.

    \item Even though the noise model is rather simple, we believe it is the one which is most relevant in practice, since measurement noise is often of this form. We also want to highlight again that noise which has a more complex structure, can also be dealt with by the stochastic filtering approach described in \citet[Section~6]{krach2022optimal}. However, this requires additional assumptions, i.e., some type of knowledge of the underlying distribution and the noise distribution (or access to noisy \emph{and} noise-free observations of $X$ during training).
    In contrast to this, our extension for noisy observation doesn't need such knowledge, i.e., it is enough to observe the noisy data samples (without ever observing noise-free samples) to learn to optimally predict the noise-free process $X$.

    We added Remark 3.7 which explains the case of a linear observation model $O = AX+\epsilon$, where the ultimate goal is to predict $X$ from observations of $O$.
    Moreover, we added Remark 3.8, which discusses more general observation model.

    \item We changed the numbering of the items in the assumptions as suggested. However, for a better visual detection which items changed or were added and which stayed the same, we do not repeat those items that didn't change in Assumptions 3.1 and 4.1.

    \item Existence of $F_j$ is a consequence of the Doob-Dynkin lemma, which basically states that if a random variable $A$ is measurable with respect to the sigma-algebra $\sigma(B)$ generated by another random variable $B$, then there exists a measurable function $g$ such that $A = g(B)$. This is a very basic result in probability theory, which is often implicitly used without noticing. 
    Before Assumption 2.1, we added the intermediate steps that were used before applying the Doob-Dynkin lemma. In Section~3 this works identically, therefore, we think it is not needed to repeat the arguments. 

    The existence of $F_j$ can therefore be regarded as a basic fact, hence, this does not further interact with the assumptions (except that we know that the function for which we make these assumptions exists in general). 
    The assumptions we then make on $F_j$ are obviously much stronger, ensuring a certain regularity/structure of this function, which is then used by our model.

    \item Since in some applications the described relaxation might be needed, we still think that this remark can be helpful just to point out that this is possible. However, the details of it are not very insightful, but rather technical; therefore, we do not think it would help to speak more about it.
    %Moreover, we tried to give a lot of context, especially in the beginning of Sections 3 and 4, where we discuss the previous results of \citet{krach2022optimal} and the extension that we provide here. Additionally, we added the new Sections 3.5 and 5, which give further context for the results. From our point of view, the main points of \citet{krach2022optimal} were stated in Section 2. However, if you have explicit suggestions, we are happy to further discuss them.

    

    \item As requested, we added the new Theorem 3.3, which is proven afterwards. Moreover, we added a short explanation what the main differences are in the proof, before starting it.

    \item Short answer: When show that the prediction $Y$ of our model converges to the true conditional expectation $\hat{X}$ as the model size $m$ and the number of observed training paths $N$ tend to infinity. (Note again that $Y$ is the prediction of our model and not the observation process.) Please see the answer in point 10 for more details.

    \item One practically relevant case where $\beta_i$ are known is given in Remark 3.6, where we show how to learn the conditional expectation of higher moments of $X$. This is an important example if one is interested not only in the prediction of the conditional mean, but rather in the conditional distribution.
    %In real world applications, it depends on the setting whether such an assumption is reasonable or not. For example, when there is a certain measurement device, to which one has access, one can make several measurements with it of a fixed known quantity and compute the empirical error distribution from the observed measurement errors.

    \item Please see the answer for point 11 above. 

    \item As suggested, we added a discussion of the main technical challenges in the proof and some remarks about the helper lemmas directly after stating Theorem 4.3.

    \item We added the new Theorem 4.3 as requested.

    \item It is easy to see from equation (28) in the full proof in the appendix that for $\theta^\star$ satisfying $\Phi(\theta^\star) \leq \inf_{\theta\in\Theta}\Phi(\theta) + \epsilon = \Psi(\hat{X}) + \epsilon$, we have $d_k(\hat{X}, Y^{\theta^\star}) \leq C \, \sqrt{\epsilon}$ for every $k$ and for some constant $C > 0$. In particular, a loss close to the minimal loss value implies that $Y^{\theta^\star}$ is close to the true conditional expectation in terms of our pseudo-metrics. In the newly added Section 5 (and particularly in Section 5.2) we further discuss what can be inferred from closeness in the pseudo-metrics about the distance of the path produced by PD-NJ-ODE to the path of the true conditional expectation. Intuitively speaking, Proposition 5.4 tells us that the paths must be close at all $t$ which are in the support of one of the observation times $t_k$. Under further assumptions on the distribution of the $t_k$, we get more tractable ``closeness statements'', as shown in Examples 5.5 and 5.7.

    Based on your question, we now additionally added Section 5.3 where this is discussed.

    
    
    
    

    
    
\end{enumerate}






\section{Outline of additional changes}
Due to internal discussions between the authors based on the reviews, we made some additional changes to the paper since first submitting it, which were not explicitly requested by the reviewers. Below we outline them.
\begin{enumerate}
    \item We did some reformulation in the beginning of Sections~3 and~4. Moreover, we fixed some typos and made some statements more precise.
    
    \item We added Section~2.1, which introduces the intuitive example of patient health data that is used in Sections~3 and~4.

    \item We added Remark~2.2, which clarifies that our results still hold if we allow for ``pseudo observation times''.

    \item We added Remark~2.3, which explains that Assumption 2.1 (i) is always satisfied and therefore not needed.

    \item In Section 4.3 after the proof of Theorem 4.3 we added Proposition 4.8 and Remark 4.9 which explain that the conditional independence of $X_{t_i-}$ and $n$  given $\mathcal{A_{t_i-}}$ can be dropped when changing the weighting in the loss function from an equal weighting of all observations to a weighting depending on the time difference since the last observation. This weighting gives more importance to observations after a long time of no observation. However, in turn, we need an additional assumption on the integrability of the time steps size.

    \item In Section~4.4.1 we replaced the usage of a dummy variable (that is always observed) by pseudo observation times (see item 3.), where no coordinate is observed. Even though it practically doesn't change anything, we believe it is a bit cleaner and easier to follow for readers not familiar with the topic.

    \item We added the new Section~5 where we discuss the practical implications of the convergence results that we proved.
    First, in Section~5.1, we study the practically relevant version of the conditional expectation that we would like to predict. We give an intuitive counterexample, where our model does not converge to it, and then give a sufficient condition (with proof) for this to hold.
    Secondly, in Section~5.2, we discuss the implications of convergence in the (pseudo) metrics $d_k$ (which ensures a good approximation at left-limits of observation times) for general times $t$. In particular, we study two practically relevant examples for the conditional distribution of the observation times and show that in these cases, with high probability, our model approximates the conditional expectation well on the entire support of the observation times.

    \item For the proof in Section~5.1, we added the new Proposition~A.9 to Appendix~A.

    \item In Remark~5.6 (also in the new Section~5.2), we provide convergence guarantees for the evaluation metric in one of the practically relevant examples. The evaluation metric was used in \citet{krach2022optimal} and in our paper for the experiments on synthetic data as a quality measure of the model approximation.

    \item We added more details about the usage of Dynkin's $\pi$-$\lambda$ theorem in the proof of Proposition~A.1.

    \item We added Appendix B that discusses the inductive bias of our PD-NJ-ODE in detail for a better understanding how our model generalizes to out-of-sample data.
\end{enumerate}



\bibliographystyle{tmlr}
\bibliography{references} 


\end{document}