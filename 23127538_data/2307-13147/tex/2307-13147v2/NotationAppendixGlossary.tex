\def\fastCompileJakob{1}
\usepackage{transparent}
\DeclareMathOperator{\supp}{supp}

%%%%%%%%%%% Technical stuff
% { 
\glsaddkey
{alttextLong}           % key
{todo}              % default value
{\glsentryLong}  %no link cs ... command analogous to \glsentrytext
{\GlsentryLong}  %no link ucfirst cs ... command analogous to \Glsentrytext
{\glsLong}       %link cs ... command analogous to \glstext
{\GlsLong}       %link ucfirst cs ... command analogous to \Glstext
{\GLSLong}       %link allcaps cs ... command analogous to \GLStext

\glsaddkey
{alttextt}           % key
{todo}              % default value
{\glsentryt}  %no link cs ... command analogous to \glsentrytext
{\Glsentryt}  %no link ucfirst cs ... command analogous to \Glsentrytext
{\glst}       %link cs ... command analogous to \glstext
{\Glst}       %link ucfirst cs ... command analogous to \Glstext
{\GLSt}       %link allcaps cs ... command analogous to \GLStext

\glsaddkey
{alttextT}           % key
{todo}              % default value
{\glsentryT}  %no link cs ... command analogous to \glsentrytext
{\GlsentryT}  %no link ucfirst cs ... command analogous to \Glsentrytext
{\glsT}       %link cs ... command analogous to \glstext
{\GlsT}       %link ucfirst cs ... command analogous to \Glstext
{\GLST}       %link allcaps cs ... command analogous to \GLStext

\glsaddkey
{alttexttk}           % key
{todo}              % default value
{\glsentrytk}  %no link cs ... command analogous to \glsentrytext
{\Glsentrytk}  %no link ucfirst cs ... command analogous to \Glsentrytext
{\glstk}       %link cs ... command analogous to \glstext
{\Glstk}       %link ucfirst cs ... command analogous to \Glstext
{\GLStk}       %link allcaps cs ... command analogous to \GLStext

\glsaddkey
{alttexts}           % key
{todo}              % default value
{\glsentrys}  %no link cs ... command analogous to \glsentrytext
{\Glsentrys}  %no link ucfirst cs ... command analogous to \Glsentrytext
{\glss}       %link cs ... command analogous to \glstext
{\Glss}       %link ucfirst cs ... command analogous to \Glstext
{\GLSs}       %link allcaps cs ... command analogous to \GLStext

\glsaddkey
{alttextOm}           % key
{todo}              % default value
{\glsentryOm}  %no link cs ... command analogous to \glsentrytext
{\GlsentryOm}  %no link ucfirst cs ... command analogous to \Glsentrytext
{\glsOm}       %link cs ... command analogous to \glstext
{\GlsOm}       %link ucfirst cs ... command analogous to \Glstext
{\GLSOm}       %link allcaps cs ... command analogous to \GLStext

\glsaddkey
{alttextF}           % key
{todo}              % default value
{\glsentryF}  %no link cs ... command analogous to \glsentrytext
{\GlsentryF}  %no link ucfirst cs ... command analogous to \Glsentrytext
{\glsF}       %link cs ... command analogous to \glstext
{\GlsF}       %link ucfirst cs ... command analogous to \Glstext
{\GLSF}       %link allcaps cs ... command analogous to \GLStext

\glsaddkey
{alttextFF}           % key
{todo}              % default value
{\glsentryFF}  %no link cs ... command analogous to \glsentrytext
{\GlsentryFF}  %no link ucfirst cs ... command analogous to \Glsentrytext
{\glsFF}       %link cs ... command analogous to \glstext
{\GlsFF}       %link ucfirst cs ... command analogous to \Glstext
{\GLSFF}       %link allcaps cs ... command analogous to \GLStext

\glsaddkey
{alttextFt}           % key
{todo}              % default value
{\glsentryFt}  %no link cs ... command analogous to \glsentrytext
{\GlsentryFt}  %no link ucfirst cs ... command analogous to \Glsentrytext
{\glsFt}       %link cs ... command analogous to \glstext
{\GlsFt}       %link ucfirst cs ... command analogous to \Glstext
{\GLSFt}       %link allcaps cs ... command analogous to \GLStext


\glsaddkey
{alttextP}           % key
{todo}              % default value
{\glsentryP}  %no link cs ... command analogous to \glsentrytext
{\GlsentryP}  %no link ucfirst cs ... command analogous to \Glsentrytext
{\glsP}       %link cs ... command analogous to \glstext
{\GlsP}       %link ucfirst cs ... command analogous to \Glstext
{\GLSP}       %link allcaps cs ... command analogous to \GLStext
% }


%%%%%%%%%%% fruther Technical stuff
% {
\newcommand{\optionalSub}[1]{\ifblank{#1}{}{\ensuremath{_{#1}}}}
\newcommand{\optionalSup}[1]{\ifblank{#1}{}{\ensuremath{^{#1}}}}

%\newcommand{\mandatorySub}[2][command]{\ifblank{#2}{\PackageWarning {NotationAppendixGlossaryJakob}{The argument of #1 is not supposed to be empty}}{\ensuremath{_{#2}}}}
\NewExpandableDocumentCommand{\mandatorySub}{O{command}m}{\ifblank{#2}{\PackageWarning {NotationAppendixGlossaryJakob}{The argument of #1 is not supposed to be empty}}{\ensuremath{_{#2}}}}
\newcommand{\mandatorySup}[2][command]{\ifblank{#2}{\PackageWarning {NotationAppendixGlossaryJakob}{The argument of #1 is not supposed to be empty}}{\ensuremath{^{#2}}}}
% \NewDocumentCommand\mandatorySubb{O{command}m}{\ifblank{#2}{\PackageWarning {NotationAppendixGlossaryJakob}{The argument of #1 is not supposed to be empty}}{\ensuremath{_{#2}}}}
% \newcommand{\optionalSubb}[2][a]{\ifblank{#1}{}{\ensuremath{_{#2}}}}
% \newcommand{\optionalSubbb}[2]{\ifblank{#1}{}{\ensuremath{_{#2}}}}



\newcommand{\nblack}[1]{n}
\newcommand{\njNotNumberOfObservations}[1]{n\mandatorySup[\njNotNumberOfObservations]{#1}}

\newcommand{\mathglslink}[2]{\mathrlap{\glslink{#1}{\phantom{\ensuremath{#2}}}}\mathlinkcolored{#2}} %like \glslink but with fixed spacing

\if\fastCompileJakob1
\newcommand{\myglslink}[2]{\rlap{\glslink{#1}{\phantom{\ensuremath{#2}}}}\mathlinkcolored{#2}} %like \glslink but with fixed spacing
\else
\newcommand{\myglslink}[2]{\mathrlap{\glslink{#1}{\phantom{\ensuremath{#2}}}}\mathlinkcolored{#2}} %like \glslink but with fixed spacing
\fi

\newcommand{\makeCommandsForGLSEntryWithoutArguments}[3]{%#1=commandname #2=GLSEntry #3=symbol
\newcommand{#1}{\ensuremath{\myglslink{#2}{#3}}%
}}


% \newcommand{\makeCommandsForGLSEntrySubSuper}[3]{%#1=commandname #2=GLSEntry #3=symbol
% \newcommand{#1}[2]{\colorlet{saved}{.}\ensuremath{\glslink{#2}{#3\ifthenelse{\equal{##1}{}}{}{_{\mathcolor{saved}{##1}}}\ifthenelse{\equal{##2}{}}{}{^{\mathcolor{saved}{##2}}}}}%
% }}

\newcommand{\makeCommandsForGLSEntrySubSuper}[3]{%#1=commandname #2=GLSEntry #3=symbol
\NewDocumentCommand#1{mm}{\colorlet{saved}{.}\ensuremath{\glslink{#2}{#3\ifblank{##1}{}{_{\mathcolor{saved}{##1}}}\ifblank{##2}{}{^{\mathcolor{saved}{##2}}}}}%
}}
%\makeCommandsForGLSEntrySubSuper{commandname}{GLSEntry}{symbol}

% \newcommand{\makeCommandsForGLSEntryColoredSubSuper}[3]{%#1=commandname #2=GLSEntry #3=symbol
% \NewDocumentCommand#1{mm}{\colorlet{saved}{.}\ensuremath{\glslink{#2}{#3\ifblank{##1}{\PackageWarning {NotationAppendixGlossaryJakob}{The argument of #1 is not supposed to be empty}}{_{##1}}\ifblank{##2}{}{^{\mathcolor{saved}{##2}}}}}%
% }}
\if\fastCompileJakob1
\newcommand{\makeCommandsForGLSEntryColoredSubSuper}[3]{%#1=commandname #2=GLSEntry #3=symbol %%Attention, this does not work with other default colors than black
\newcommand{#1}[2]{\ensuremath{\glslink{#2}{#3\ifblank{##1}{\PackageWarning {NotationAppendixGlossaryJakob}{The argument of #1 is not supposed to be empty}}{_{##1}}\ifblank{##2}{}{^{\mathcolor{black}{##2}}}}}%
}}
\else
\newcommand{\makeCommandsForGLSEntryColoredSubSuper}[3]{%#1=commandname #2=GLSEntry #3=symbol %%this also works with other default colors than black
\newcommand{#1}[2]{\ensuremath{\myglslink{#2}{#3\ifblank{##1}{\PackageWarning {NotationAppendixGlossaryJakob}{The argument of #1 is not supposed to be empty}}{_{##1}}}\ifblank{##2}{}{^{##2}}}%
}}
\fi
%\makeCommandsForGLSEntryColoredSubSuper{commandname}{GLSEntry}{symbol}

\newcommand{\makeCommandsForGLSEntryColoredMandatorySub}[3]{%#1=commandname #2=GLSEntry #3=symbol
\NewDocumentCommand#1{m}{\ensuremath{\myglslink{#2}{#3\mandatorySub[#1]{##1}}}%
}}
%\makeCommandsForGLSEntryColoredMandatorySub{commandname}{GLSEntry}{symbol}

\newcommand{\makeCommandsForGLSEntryOptionalSub}[3]{%#1=commandname #2=GLSEntry #3=symbol
\NewDocumentCommand#1{m}{\ensuremath{\myglslink{#2}{#3}\optionalSub{##1}}%
}}
%\makeCommandsForGLSEntryOptionalSub{commandname}{GLSEntry}{symbol}

\newcommand{\makeCommandsForGLSEntrySubIcludedSuper}[4]{%#1=commandname #2=GLSEntry #3=symbol #4=Supscript which is part of the symbol
\NewDocumentCommand#1{mm}{\colorlet{saved}{.}\ensuremath{\glslink{#2}{#3\ifblank{##1}{}{_{\mathcolor{saved}{##1}}}\ifblank{##2}{^{#4}}{^{#4,\mathcolor{saved}{##2}}}}}%
}}
%\makeCommandsForGLSEntrySubIcludedSuper{commandname}{GLSEntry}{symbol}{InclSup}

\newcommand{\makeCommandsForGLSEntrySubSuperSimple}[2]{%#1=commandname #2=GLSEntry
\makeCommandsForGLSEntrySubSuper{#1}{#2}{\glsentrytext{#2}}}
%\makeCommandsForGLSEntrySubSuper{commandname}{GLSEntry}{symbol}

\if\fastCompileJakob1
\newcommand{\makeCommandsForGLSEntrySuper}[3]{%#1=commandname #2=GLSEntry #3=symbol
\NewDocumentCommand#1{m}{\colorlet{saved}{.}\ensuremath{\glslink{#2}{#3\ifblank{##1}{}{^{\mathcolor{saved}{##1}}}}}%
}}
\else
\newcommand{\makeCommandsForGLSEntrySuper}[3]{%#1=commandname #2=GLSEntry #3=symbol
\NewDocumentCommand#1{m}{\ensuremath{\myglslink{#2}{#3}\optionalSup{##1}}}%
}
\fi
%\makeCommandsForGLSEntrySuper{commandname}{GLSEntry}{symbol}

\newcommand{\makeCommandsForGLSEntryPost}[3]{%#1=commandname #2=GLSEntry #3=symbol
\NewDocumentCommand#1{m}{\colorlet{saved}{.}\ensuremath{\glslink{#2}{\mathcolor{saved}{\mathlinkcolored{#3}##1}}}%
}}
%\makeCommandsForGLSEntryPost{commandname}{GLSEntry}{symbol}
% }










\newcommand{\inSectionTildeButInOtherSection}[3][1]{In \if#11\Cref{sec:Recall: the PD-NJ-ODE,sec:PD-NJ-ODE with Noisy Observations}\else\Cref{sec:PD-NJ-ODE with Noisy Observations}\fi, #2, but in \Cref{sec:PD-NJ-ODE with Dependence betweenXand Observation Framework} (and all sections thereafter including the appendix), #3}

% \newcommand{\twoDifferentGeneralSpacesText}[4][is]{In \Cref{sec:Recall: the PD-NJ-ODE,sec:PD-NJ-ODE with Noisy Observations}, $#2$ #1 defined on #3, but in \Cref{sec:PD-NJ-ODE with Dependence betweenXand Observation Framework} (and all sections thereafter including the appendix), $#2$ #1 defined on #4}
\newcommand{\twoDifferentGeneralSpacesText}[4][is]{\inSectionTildeButInOtherSection{$#2$ #1 defined on #3}{ $#2$ #1 defined on #4}}

\newcommand{\twoDifferentSpacesText}[2][is]{\twoDifferentGeneralSpacesText[#1]{#2}{\tOmFP{}}{\OmFP{}}}

\newcommand{\twoDifferentFilteredSpacesText}[2][is]{\twoDifferentGeneralSpacesText[#1]{#2}{\tOmFFP{}}{\OmFFP{}}}

\newcommand{\inSectionNoiselessButInOtherSection}[2]{In \Cref{sec:Recall: the PD-NJ-ODE,sec:PD-NJ-ODE with Dependence betweenXand Observation Framework,sec:Dependent Observation Framework -- a Black--Scholes Example}, #1, but in \Cref{sec:PD-NJ-ODE with Noisy Observations} and in the appendix, #2. In \Cref{sec:Noisy Observations -- Brownian Motion with Gaussian Observation Noise,sec:Physionet with Observation Noise} we compare both losses against each other. Note that in \Cref{sec:More General Noise Structure Conditional Moments} we use another modification of the loss~\eqref{equ:loss noisy obs generalised} for the case of noise with known non-zero mean}



\makeglossaries
\newglossaryentry{X}
{
  name={\ensuremath{X}},
  alttextt={\ensuremath{X_{\normalcolor t}}},
  alttexts={\ensuremath{X_{\normalcolor s}}},
  description={The stochastic process~$X :={(X_t)}_{t \in [0,T]}$ we want to study (which is an adapted c\`adl\`ag  stochastic process $X:\gls{OmFFP}\to {\R^{\gls{dX}}}^{[0,\gls{T}]}$). In our notation $\gX{t,k}^{(j)}$ refers to the $k$-th coordinate of the $j$-th path at time $t$ for $1\le k \le \gls{dX}$, $1\le j \le N$ and $t\in [0,\gls{T}]$ (see \Cref{sec:Recall: the PD-NJ-ODE}). E.g., for a medical data-set, the index $j$ could correspond to a patient and the first coordinate could correspond to the body-temperature and the second coordinate could correspond to the blood-pressure, then $\gX{t,1}^{(7)}\omb=36.9$ would mean that the 7\textsuperscript{th} patient had a body temperature of $36.9$\textdegree C at time (or age) $t$}
}
%\makeCommandsForGLSEntrySubSuper{\gX}{X}{X}
%\makeCommandsForGLSEntrySubSuperSimple{\gX}{X}
\makeCommandsForGLSEntryOptionalSub{\gX}{X}{X}
%\newcommand{\ggX}[2]{\colorlet{saved}{.}\ensuremath{\glslink{X}{X\ifblank{#1}{}{_{\mathcolor{saved}{#1}}}\ifblank{#1}{}{^{\mathcolor{saved}{#2}}}}}}
\newcommand{\mathbfX}{\mathbf{X}}
\newcommand{\Xblack}{X}

\newglossaryentry{dX}
{
  name={\ensuremath{d_X}},
  description={The dimension~$d_X \in \N$ of $\gls{X}$ (i.e., $\gX{}_t\omb\in\R^{d_X}$) (see \Cref{sec:Recall: the PD-NJ-ODE})}
}
\newglossaryentry{T}
{
  name={\ensuremath{T}},
  description={The largest time~$T \in\Rp$ we consider (see \Cref{sec:Recall: the PD-NJ-ODE})}
}
\newglossaryentry{OmFFP}
{
  name={\ensuremath{\left(\Omega, \F, \mathbb{F}, \PBlack \right)}},
  alttextLong={\ensuremath{\left(\Omega, \F, \mathbb{F} := (\F_t)_{0 \leq t \leq \gls{T}}, \PBlack\right)}},
  alttextOm={\ensuremath{\Omega}},
  alttextF={\ensuremath{\F}},
  alttextFF={\mathbb{F}},
  alttextFt={\ensuremath{\F_t}},
  alttextP={\ensuremath{\PBlack}},
  description={The filtered probability space  $\left(\Omega, \F, \mathbb{F} := (\F_t)_{0 \leq t \leq \gls{T}}, \PBlack\right)$ on which \gls{X} is defined (see \Cref{sec:Recall: the PD-NJ-ODE}). In \Cref{sec:PD-NJ-ODE with Dependence betweenXand Observation Framework} (and all sections thereafter including the appendix) all stochastic objects $\gls{X}, \gn{}, \gls{K}, \tk{i}, \tau, \gM{}, \At{t}$ are defined on this probability space. However, in \Cref{sec:Recall: the PD-NJ-ODE,sec:PD-NJ-ODE with Noisy Observations} only $\gls{X}$ is defined on this space, while the observation framework $\gn{}, \gls{K}, \tk{i}, \tau, \gM{}$ is defined on a different filtered probability space \gls{tOmFFP}}
}
\makeCommandsForGLSEntryColoredSubSuper{\gF}{OmFFP}{\glsentryF{OmFFP}}
\newcommand{\Om}[1]{\glsOm{OmFFP}#1}
\makeCommandsForGLSEntryPost{\OmFFP}{OmFFP}{\glsentrytext{OmFFP}}
\makeCommandsForGLSEntryPost{\OmFP}{OmFFP}{\left(\Omega, \F, \PBlack\right)}
\makeCommandsForGLSEntryPost{\gP}{OmFFP}{\PBlack}
\makeCommandsForGLSEntryWithoutArguments{\P}{OmFFP}{\PBlack}
\newcommand{\Omblack}{\Omega}
\newcommand{\Fblack}{\F}

\newglossaryentry{Xs}
{
  name={\ensuremath{X^\star}},
  alttextt={\ensuremath{X^\star_{\normalcolor t}}},
  alttextT={\ensuremath{X^\star_{\normalcolor \gls{T}}}},
  description={The running maximum process of \gls{X}, i.e., $X^\star_t\omb :=\sup_{s \in [0,t]}\onenorm{\gX{s}\omb} \ \forall t \in [0,\gls{T}] \ \forall \omega\in\glsOm{OmFFP}$ with $X^\star:\gls{OmFFP}\to {\Rpz}^{[0,\gls{T}]}$ (see \Cref{sec:Recall: the PD-NJ-ODE})}
}
\makeCommandsForGLSEntrySubIcludedSuper{\gXs}{Xs}{X}{\star}

\newglossaryentry{J}
{
  name={\ensuremath{\mathcal{J}}},
  description={The random set of the jump times $\mathcal{J}:\OmFP{}\to \Powerset{([0,\gls{T}])}$ of \gls{X} (see \Cref{sec:Recall: the PD-NJ-ODE})}
}

\newglossaryentry{powerset}
{
  name={\ensuremath{\mathscr{P}}},
  description={The powerset $\mathscr{P}(S)$ of a set $S$ is the set of all subsets of $S$}
}
\newcommand{\Powerset}[1]{\gls{powerset}\left(#1\right)}

\newglossaryentry{tOmFFP}
{
  name={\ensuremath{\left(\tilde{\Omega}, \tilde{\F}, \tilde{\mathbb{F}}, \tilde{\PBlack} \right)}},
  alttextLong={\ensuremath{\left(\tilde\Omega, \tilde{\F},  \tilde{\mathbb{F}} := ( \tilde{\F}_t )_{0 \leq t \leq \gls{T}}, \tilde{\PBlack}\right)}},
  alttextOm={\ensuremath{\tilde{\Omega}}},
  alttextF={\ensuremath{\tilde{\F}}},
  alttextFF={\ensuremath{\tilde{\mathbb{F}}}},
  alttextFt={\ensuremath{\tilde{F_t}}},
  %alttextFtk={\ensuremath{\tilde{\F}_{\tk{k}}}},
  alttextP={\ensuremath{\tilde{\PBlack}}},
  description={The filtered probability space  $\left(\tilde\Omega, \tilde{\F},  \tilde{\mathbb{F}} := ( \tilde{\F}_t )_{0 \leq t \leq \gls{T}}, \tilde{\PBlack}\right)$ on which $\gls{n}, \gls{K}, \tk{i}, \tau, \gls{M}$ are defined describing the observation framework, but $\gls{X}$ is not defined on this space (see \Cref{sec:Recall: the PD-NJ-ODE}). In \Cref{sec:PD-NJ-ODE with Dependence betweenXand Observation Framework} (and all sections thereafter including the appendix) this space is not  needed since all stochastic objects $\gls{X}, \gls{n}, \gls{K}, \tk{i}, \tau, \gls{M}, \At{t}$ are defined on \gls{OmFFP}. However, in \Cref{sec:Recall: the PD-NJ-ODE,sec:PD-NJ-ODE with Noisy Observations} this space is used for $\gls{n}, \gls{K}, \tk{i}, \tau, \gls{M}$ related to the events of observations}
}
\makeCommandsForGLSEntryPost{\tOmFFP}{tOmFFP}{\glsentrytext{tOmFFP}}
\makeCommandsForGLSEntryPost{\tOmFP}{tOmFFP}{\left(\tilde\Omega, \tilde{\F}, \tilde{\PBlack}\right)}
\makeCommandsForGLSEntryPost{\tP}{tOmFFP}{\tilde{\PBlack}}
\newcommand{\tFt}[1]{\glslink{tOmFFP}{\tilde{\mathcal{F}}_{#1}}}

\newglossaryentry{n}
{
  name={\ensuremath{n}},
  description={The random number of observations $n: \tOmFP{}  \to \N_{\geq 0}$ up to time $T$ % is an $\glsF{tOmFFP} $-measurable random variable
  (see \Cref{sec:Recall: the PD-NJ-ODE}). Every observation time $\tk{i}\in[0,\gls{T}]$ counts as 1 observation for this count (also for incomplete observations). \twoDifferentSpacesText{n}. In our medical example, $n^{(j)}$ denotes the number of observation times for the $j$-th patient%In \Cref{sec:Recall: the PD-NJ-ODE,sec:PD-NJ-ODE with Noisy Observations} $n$ is defined on \tOmFP{}, but in \Cref{sec:PD-NJ-ODE with Dependence betweenXand Observation Framework} (and all sections thereafter including the appendix) $n$ is defined on \OmFP{}
  }
}
\makeCommandsForGLSEntrySuper{\gn}{n}{n} %\gn{} or \gn{superscript}

\newglossaryentry{K}
{
  name={\ensuremath{K}},
  description={The \enquote{maximal} value of $\gn{}$, i.e., the essential supremum $K := \sup \left\{k \in \N \, | \, \glsP{tOmFFP}(\gn{} \geq k) > 0 \right\} \in \N \cup\{\infty\}$ of $\gn{}$ (see \Cref{sec:Recall: the PD-NJ-ODE}). % In \Cref{sec:Recall: the PD-NJ-ODE,sec:PD-NJ-ODE with Noisy Observations}, $K$ is defined with respect to \tP{}, but in \Cref{sec:PD-NJ-ODE with Dependence betweenXand Observation Framework} (and all sections thereafter including the appendix), $K$ is defined with respect to \gP{}
  \inSectionTildeButInOtherSection{$K$ is defined with respect to \tP{}}{$K$ is defined with respect to \gP{}}}
}

\newglossaryentry{tk}
{
  name={\ensuremath{t_i}},
  description={The random observation times $t_i: \tOmFFP{}  \to [0,\gls{T}] \cup \{ \infty \}$ for $0 \leq i \leq \gls{K}$ are sorted stopping times, with $t_i(\tilde{\omega}) := \infty$ if $\gn{}(\tilde{\omega}) < i$ (see \Cref{sec:Recall: the PD-NJ-ODE}). In our practical implementation we replace \enquote{$\infty$} by \enquote{$\gls{T}$} (see \Cref{algo:1}), since we are not interested in times after $\gls{T}$ anyway. \twoDifferentFilteredSpacesText[are]{t_i}}
}
\makeCommandsForGLSEntryColoredSubSuper{\tksup}{tk}{t}
%\newcommand{\tk}[1]{\glslink{tk}{t_{#1}}}
\makeCommandsForGLSEntryColoredMandatorySub{\tk}{tk}{t}


\let\oldtau\tau
\newglossaryentry{tau}
{
  name={\ensuremath{\oldtau}},
  description={The last observation time $\oldtau(t)$ before a certain time $t$, i.e., $\oldtau : [0,\gls{T}] \times \glsOm{tOmFFP}  \to [0,\gls{T}], \; (t, \tilde\omega) \mapsto \tau(t, \tilde\omega) := \max\{ \tk{i}(\tilde\omega) | 0 \leq i \leq \gn{}(\tilde\omega), \tk{i}(\tilde\omega) \leq t \}$ (see \Cref{sec:Recall: the PD-NJ-ODE}). \twoDifferentSpacesText{\oldtau}, i.e., $\oldtau : [0,\gls{T}] \times \glsOm{OmFFP}  \to [0,\gls{T}]$}
}
%\remakeCommandsForGLSEntryWithoutArguments{\tau}{tau}{\oldtau}
\renewcommand{\tau}{\ensuremath{\myglslink{tau}{\oldtau}}}

\newglossaryentry{M}
{
  name={\ensuremath{M}},
  description={The observation mask~$M = (M_k)_{0 \leq k \leq \gls{K}}$, which is a sequence of random variables on  $(\glsOm{tOmFFP} , \glsF{tOmFFP}, \glsP{tOmFFP} )$ taking values in $\{ 0,1 \}^{\gls{dX}}$ such that $M_k$ is $\tFt{\tk{k}}$-measurable.
The $j$-th coordinate of the $k$-th element of the sequence $M$, i.e., $M_{k,j}$, signals whether $\gX{}_{\tk{k}, j}$, denoting the $j$-th coordinate of the stochastic process at observation time $\tk{k}$, is observed. By abuse of notation we also write $M_{\tk{k}} := M_{k}$ (see \Cref{sec:Recall: the PD-NJ-ODE}). \twoDifferentSpacesText{M}}
}
\makeCommandsForGLSEntryOptionalSub{\gM}{M}{M}
\newcommand{\ThetaM}{\Theta_M}%Not Obervation mask M



\newglossaryentry{At}
{
  name={\ensuremath{\mathbb{A} := (\mathcal{A}_t)_{t \in [0,\gls{T}]}}},
  description={The filtration of the currently available information $\mathbb{A} := (\mathcal{A}_t)_{t \in [0,\gls{T}]}$ defined by 
\begin{equation*}
\mathcal{A}_t := \boldsymbol{\sigma}\left(\gX{}_{\tk{i}, j}, \tk{i}, \gM{\tk{i}} \,\middle|\, \tk{i} \leq t,\, j \in \{1 \leq l \leq \gls{dX} | \gM{\tk{i}, l} = 1  \} \right),
\end{equation*}%
in all sections, where we have noiseless observations $\gX{\tk{i}}$ and analogously defined as
\begin{equation*}
\mathcal{A}_t := \boldsymbol{\sigma}\left(\gO{\tk{i}, j}, \tk{i}, \gM{\tk{i}} \,\middle|\, \tk{i} \leq t,\, j \in \{1 \leq l \leq \gls{dX} | \gM{\tk{i}, l} = 1  \} \right),
\end{equation*}
in all other sections, where we have noisy observations $\gO{\tk{i}}$.
In both cases this corresponds to the information obtained from seeing the observations until time $t$ where $\boldsymbol\sigma(\cdot)$ denotes the generated $\sigma$-algebra (see \Cref{sec:Recall: the PD-NJ-ODE,sec:Setting with Noisy Observations}).
We use the notion of stopped $\sigmab$-algebras
\begin{equation*}
\mathcal{A}_{\tk{k}} := \boldsymbol{\sigma}\left(\gX{}_{\tk{i}, j}, \tk{i}, \gM{\tk{i}} \,\middle|\, i\leq k,\, j \in \{1 \leq l \leq \gls{dX} | \gM{\tk{i}, l} = 1  \} \right),
\end{equation*}%
and pre-stopped $\sigmab$-algebras
\begin{equation*}
\mathcal{A}_{\tk{k}-} := \boldsymbol{\sigma}\left(\gX{}_{\tk{i}, j}, \tk{i}, \gM{\tk{i}}, \tk{k} \,\middle|\, i < k,\, j \in \{1 \leq l \leq \gls{dX} | \gM{\tk{i}, l} = 1  \} \right)
\end{equation*}
from \citet[Definitions~2.37 and~8.1]{KarandikarRao2018}. In the sections with noise, we replace $\gls{X}$ by $\gls{O}$%
}%
}
\newcommand{\bA}{\ensuremath{\myglslink{At}{\ensuremath{\mathbb{A}}}}}
\makeCommandsForGLSEntryColoredMandatorySub{\At}{At}{\mathcal{A}}


\newglossaryentry{hX}
{
  name={\ensuremath{\hat{X} = (\hat X_t)_{0 \leq t \leq \gls{T}}}},
  description={The conditional expectation process of $\gls{X}$, which is its $L^2$-optimal prediction \citep[Proposition 2.5]{krach2022optimal} given the currently available information, is defined as $\hat{X} = (\hat X_t)_{0 \leq t \leq \gls{T}}$, with $\hat{X}_t := \E_{\glsP{OmFFP}\times\glsP{tOmFFP}}[\gX{}_t | \At{t}]$ (see \Cref{sec:Recall: the PD-NJ-ODE}). \inSectionTildeButInOtherSection{the expectation is taken with respect to $\glsP{OmFFP}\times\glsP{tOmFFP}$}{the expectation is taken with respect to $\glsP{OmFFP}$}}
}
%\newcommand{\hX}{\ensuremath{\myglslink{hX}{\ensuremath{\hat{X}}}}}
\makeCommandsForGLSEntryOptionalSub{\hX}{hX}{\hat{X}}
\newcommand{\hXblack}{\hat{X}}

\newglossaryentry{tildeX}
{
  name={\ensuremath{\tilde{X}^{\leq t}}},
  description={The interpolated observation process $\tilde X^{\leq t}$ continuously interpolates\footnote{Interpolation also includes extrapolation within this paper. $\tilde X^{\leq t}$ interpolates the observations before time $t$ without leaking any information from observations after time $t$. Furthermore, its time-consistency allows for efficient online updates of its signature instead of recomputing its signature for the whole path at every new observation time $\tk{k}$.} the observations of $\gls{X}$ that where observed before time $t$. To be precise the first \gls{dX} coordinates of $\tilde X^{\leq t}$ interpolate the observations of $\gls{X}$, while the next \gls{dX} coordinates of $\tilde X^{\leq t}$ captures explicit information on when which coordinate was observed and the last coordinate is just the time.
  At time $s \in [0,\gls{T}]$, the $j$-th coordinate $\tilde X^{\leq t}_{s,j}\omb$ of $\tilde X^{\leq t}_{s}\omb\in \R^{2\gls{dX}+1}$ is defined in \Cref{sec:Recall: the PD-NJ-ODE}. %(see \Cref{sec:Recall: the PD-NJ-ODE})
  \inSectionTildeButInOtherSection{$\tilde X^{\leq t}$ is an adapted stochastic process on $(\glsOm{OmFFP} \times \glsOm{tOmFFP}  , \glsF{OmFFP} \otimes \glsF{tOmFFP}, \glsFF{OmFFP} \otimes \glsFF{tOmFFP}, \glsP{OmFFP} \times \glsP{tOmFFP})$}{$\tilde X^{\leq t}$ is an adapted stochastic process on $\gls{OmFFP}$}%
  }
}
\newcommand{\tildeXle}[1]{\ensuremath{\myglslink{tildeX}{\tilde{X}^{\leq #1}}}}
\newcommand{\tildeXleSup}[2]{%
\rlap{\glslink{tildeX}{\phantom{\ensuremath{\tilde{X}^{\leq #1}}}}}\mathlinkcolored{\tilde{X}}^{\mathlinkcolored{\leq #1,} #2}%
}

\newglossaryentry{dk}
{
  name={\ensuremath{d_k}},
  description={A pseudo metric between two c\`adl\`ag $\bA{}$-adapted processes defined in \Cref{def:indistinguishability} in \Cref{sec:Recall: the PD-NJ-ODE} to measure the distance between processes}
}
\makeCommandsForGLSEntryColoredMandatorySub{\dk}{dk}{d}
\newcommand{\dkof}[2]{\dk{#1}\left( #2\right)}

\newglossaryentry{tu}
{
  name={\ensuremath{\tilde{u}}},
  description={The jump process $\tilde{u}_{t,j} := \sum_{k=0}^{\gls{K}} \gM{k, j} \1_{\tk{k} \leq t}$ counts the coordinate-wise observations (see \Cref{sec:Recall: the PD-NJ-ODE})}
  %sort={utilde}
}
\makeCommandsForGLSEntryOptionalSub{\tu}{tu}{\tilde{u}}


\newglossaryentry{u}
{
  name={\ensuremath{u}},
  description={The jump process $u_t := \sum_{k=1}^{\gls{K}} \1_{\tk{k} \leq t}$ counts the observations without considering which coordinates where observed (see \Cref{sec:Recall: the PD-NJ-ODE})}
}
\makeCommandsForGLSEntryOptionalSub{\gu}{u}{u}
\newcommand{\uBlack}{u}


\newglossaryentry{pim}
{
  name={\ensuremath{\pi_m}},
  description={The truncated signature~$\pi_m(\mathbfX{})$ of order $m \in \N$ of a continuous path with finite variation $\mathbfX{}$ is defined in \Cref{def:signature}. In simple words, $\pi_m(\mathbfX{})$ is a finite dimensional feature-vector representing a continuous path~$\mathbfX{}$. For every finite truncation level $m$, $\pi_m(\mathbfX{})$ does not capture all the information about the infinite dimensional object $\mathbfX{}$, but in our proof we use that there always exists a $m\in\N$ such that $\pi_m(\mathbfX{})$ describes $\mathbfX{}$ sufficiently well}
}
\makeCommandsForGLSEntryColoredMandatorySub{\pim}{pim}{\ensuremath{\pi}}


\let\oldPsi\Psi
\newglossaryentry{Psi}
{
  name={\ensuremath{\oldPsi}},
  description={The objective function~$\oldPsi:\bD\to\R$ (cf.\ \emph{equivalent objective function} from Remark~4.7 \& Appendix A.1.4 of \cite{krach2022optimal}) on the path-space $\bD$ in expectation (i.e., \enquote{for an infinite amount of training paths}). %mention 2 different losses
  \inSectionNoiselessButInOtherSection{the old objective function~\eqref{equ:Psi} that only works for noiseless observations is used}{the new objective function~\eqref{equ:Psi noisy obs} that also works for noisy observations is used}%
  }
}
%\remakeCommandsForGLSEntryWithoutArguments{\Psi}{Psi}{\oldPsi}
\renewcommand{\Psi}{\ensuremath{\myglslink{Psi}{\oldPsi}}}


\newglossaryentry{bD}
{
  name={\ensuremath{\mathbb{D}}},
  description={The set of all c\`adl\`ag $\R^{\gls{dX}}$-valued $\bA{}$-adapted processes (see \Cref{sec:Recall: the PD-NJ-ODE}).
  \inSectionTildeButInOtherSection{the stochastic processes in $\mathbb{D}$ live on $(\glsOm{OmFFP} \times \glsOm{tOmFFP}  , \glsF{OmFFP} \otimes \glsF{tOmFFP}, \glsFF{OmFFP} \otimes \glsFF{tOmFFP}, \glsP{OmFFP} \times \glsP{tOmFFP})$}{they live on $\gls{OmFFP}$}}
}
\newcommand{\bD}{\ensuremath{\myglslink{bD}{\mathbb{D}}}}


\newglossaryentry{Y}
{
  name={\ensuremath{Y}},
  description={The output $Y^{\theta}(\tildeXle{\tau(\cdot)})$ of our PD-NJ-ODE~\eqref{equ:PD-NJ-ODE} which should approximate $\hX{}$ for properly trained parameters $\theta$ (see \Cref{def:Sig-NJ-ODE}). We use the short notation $Y^{\theta, j} := Y^{\theta }(\tildeXleSup{\tau(\cdot)}{(j)})$}
}
\makeCommandsForGLSEntryOptionalSub{\gY}{Y}{Y}
\newcommand{\Yblack}{Y}
\newcommand{\Zfilter}{Z}


\let\oldtheta\theta
\newglossaryentry{theta}
{
  name={\ensuremath{\oldtheta}},
  description={The trainable parameters $\oldtheta = (\oldtheta_1, \oldtheta_2, \tildetheta{}_3) \in \Theta$ contain all trainable parameters. I.e., $\oldtheta$ contains $\oldtheta_i = (\tildetheta{}_i, \gamma_i)$ for $i \in \{1,2 \}$ which parameterise the bounded output feedforward neural networks $f_{\oldtheta_1}, \rho_{\oldtheta_2} \in \cN{}$ and $\tildetheta{}_3$ parameterize the feedforward neural network $\tilde g_{\tildetheta{}_3}  \in \tcN{}$. Thus, $\oldtheta = (\oldtheta_1, \oldtheta_2, \tildetheta{}_3) \in \Theta$ parameterize all 3 parameterized functions $f_{\oldtheta_1}, \rho_{\oldtheta_2},  g_{\tildetheta{}_3}$ in our PD-NJ-ODE~\eqref{equ:PD-NJ-ODE} (see \Cref{def:Sig-NJ-ODE})}
}
\renewcommand{\theta}{\ensuremath{\mathglslink{theta}{\oldtheta}}}


\newglossaryentry{tildetheta}
{
  name={\ensuremath{\tilde{\oldtheta}}},
  description={The trainable parameters $\tilde\oldtheta = (\tilde\oldtheta_1, \tilde\oldtheta_2, \tilde \oldtheta_3)$ contain all trainable weights and biases of $\theta = (\theta_1, \theta_2, \tilde \oldtheta_3) \in \Theta$ but not the bounds $\gamma_1$ and $\gamma_2$. I.e., the classical feedforward neural network $\tilde g_{\tilde \oldtheta_3}  \in \tcN{}$ is fully parametrized by $\tilde \oldtheta_3$, while the bounded output feedforward neural networks $f_{\theta_1}, \rho_{\theta_2} \in \cN{}$ are paremetrized by $\theta_i = (\tilde \oldtheta_i, \gamma_i)$ for $i \in \{1,2 \}$ which also includes $\gamma_1$ and $\gamma_2$ additionally to $\tilde\oldtheta_1$ and $\tilde\oldtheta_2$ (see \Cref{def:Sig-NJ-ODE})}
}
\makeCommandsForGLSEntryOptionalSub{\tildetheta}{tildetheta}{\tilde{\oldtheta}}

\let\oldgamma\gamma
\newglossaryentry{gamma}
{
  name={\ensuremath{\oldgamma}},
  description={The parameters $\oldgamma$ are contained in $\theta=\left((\tildetheta{1},\oldgamma_1),(\tildetheta{2},\oldgamma_2),\tildetheta{3}\right)$ and bound the outputs of bounded output neural networks.
  I.e., for every bounded output neural network $f_{\theta_1}, \rho_{\theta_2} \in \cN{}$ it holds that $\left|f_{\theta_1}(x)\right|_2 \leq \oldgamma_1$ and $\left|\rho_{\theta_2}(x)\right|_2 \leq \gamma_2$, because of the bounded output activation function $\Gamma_{\oldgamma_i} : \R^d \to \R^d, x \mapsto \Gamma_{\oldgamma_i}(x)=x \cdot \min\left(1, \frac{\oldgamma_i}{|x|_2}\right)$ (see \Cref{sec:Recall: the PD-NJ-ODE}).
  I.e., $f_{\theta_1}(x)=f_{(\tildetheta{1},\oldgamma_{1})}(x)=\Gamma_{\oldgamma_1}\left(\tilde{f}_{\tildetheta{1}} (x)\right)$ and $\rho_{\theta_2}(x)=\rho_{(\tildetheta{2},\oldgamma_{2})}(x)=\Gamma_{\oldgamma_2}\left(\tilde{\rho}_{\tildetheta{2}} (x)\right)$%
  }
}
\renewcommand{\gamma}{\ensuremath{\mathglslink{gamma}{\oldgamma}}}


\let\oldTheta\Theta
\newglossaryentry{Theta}
{
  name={\ensuremath{\oldTheta}},
  description={The set of all possible trainable parameters $\theta = (\theta_1, \theta_2, \tildetheta{}_3) \in \Theta$ for our our PD-NJ-ODE~\eqref{equ:PD-NJ-ODE} (see \Cref{def:Sig-NJ-ODE})}
}
\renewcommand{\Theta}{\ensuremath{\myglslink{Theta}{\oldTheta}}}

\newglossaryentry{Thetam}
{
  name={\ensuremath{\oldTheta_m}},
  text={\ensuremath{\oldTheta_m}},
  description={The  compact subset $\oldTheta_m\subset\Theta$ consists of all $\theta = \left(\theta_1, \theta_2, \tildetheta{}_3\right) = \left((\tildetheta{}_1,\gamma_1), (\tildetheta{}_2,\gamma_2), \tildetheta{}_3\right)\in \Theta$ that correspond to (bounded output) neural networks with widths and depths that are at most $m$ and such that the truncated signature of level $m$ or smaller is used and such that the norms of the weights $\tildetheta{}_i$ and the bounds $\gamma_i$ are bounded by $m$. I.e., $\oldTheta_m := \{ \theta = ((\tildetheta{}_1, \gamma_1), (\tildetheta{}_2, \gamma_2), \tildetheta{}_3 ) \in \hat\oldTheta_m \, | \, |\tildetheta{}_i|_2 \leq m, \gamma_i \leq m \} \subset \oldTheta_m\subset \Theta$, where $\hat \oldTheta_m \subset \Theta$ is defined as the set of possible parameters for the $3$ (bounded output) neural networks, such that their widths and depths are at most $m$ and such that the truncated signature of level $m$ or smaller is used (see \Cref{sec:Recall: the PD-NJ-ODE}). We use the notation\footnote{\add{The definition is less ambiguous, if we write more precisely: %$\oldTheta_m^1:=\left\{ \theta_1 \,\middle|\, \exists (\theta_2, \tildetheta{}_3) : \left(\theta_1, \theta_2, \tildetheta{}_3\right)\in\oldTheta_m \right\}$, or even more precisely:
  $\oldTheta_m^i:=\left\{ \theta_i \,\middle|\, \exists \left(\theta_1', \theta_2', \tildetheta{}_3'\right)\in\oldTheta_m : \theta_i = \theta_i'\right\}$.}} $\oldTheta_m^i:=\left\{ \theta_i \,\middle|\, \left(\theta_1, \theta_2, \tildetheta{}_3\right)\in\oldTheta_m \right\}$ and  $\tilde{\oldTheta}_m^i:=\left\{ \tildetheta{i} \,\middle|\, \left((\tildetheta{}_1,\gamma_1), (\tildetheta{}_2,\gamma_2), \tildetheta{}_3\right)\in\oldTheta_m \right\}$ for the projections of the sets on the weights $\theta_i$ and $\tildetheta{i}$ respectively}
}
\makeCommandsForGLSEntryColoredMandatorySub{\Thetam}{Thetam}{\ensuremath{\oldTheta}}

%\newcommand{\tildeThetam}[1]{\tilde{\Theta}_{#1}}
%\newcommand{\tildeThetamNew}[1]{\tilde{\oldTheta}_{#1}}
\makeCommandsForGLSEntryColoredMandatorySub{\tildeThetam}{Thetam}{\ensuremath{\tilde{\oldTheta}}}

\newglossaryentry{ThetamMin}
{
  name={\ensuremath{\oldtheta^{\min}_m\in\oldTheta_m^{\min}}},
  text={\ensuremath{\oldTheta_m^{\min}}},
  description={The set of all minimizers $\oldtheta^{\min}_m \in \oldTheta_m^{\min} := \argmin_{\theta \in \Thetam{m}}\{ \Phi(\theta) \}$ of the objective function $\Phi$ (corresponding to infinitely many training data) under the constraints of \Thetam{m} for any given $m \in \N$ (see \Cref{thm:1})}
}
\makeCommandsForGLSEntryColoredMandatorySub{\ThetamMin}{ThetamMin}{\ensuremath{\oldTheta^{\min}}}
%\newcommand{ThetamMin}[1]{\ensuremath{\glslink{ThetamMin}{\ensuremath{\oldTheta^{\min}\mandatorySub{#1}}}}}
\makeCommandsForGLSEntryColoredMandatorySub{\thetamMin}{ThetamMin}{\ensuremath{\oldtheta^{\min}}}

\newglossaryentry{ThetamNMin}
{
  name={\ensuremath{\oldtheta^{\min}_{m,N} \in\oldTheta_{m,N}^{\min}}},  text={\ensuremath{\oldTheta_{m,N}^{\min}}},
  description={The set of all minimizers $\oldtheta^{\min}_{m,N} \in \oldTheta_{m}^{\min} := \argmin_{\theta \in \Thetam{m}}\{ \hPhi{N}(\theta) \}$ of the objective function $\hPhi{N}$ (corresponding to $N$ training paths) under the constraints of \Thetam{m} for any given $m,N \in \N$ (see \Cref{thm:1})}
}
\makeCommandsForGLSEntryColoredMandatorySub{\ThetamNMin}{ThetamNMin}{\ensuremath{\oldTheta^{\min}}}
\makeCommandsForGLSEntryColoredMandatorySub{\thetamNMin}{ThetamNMin}{\ensuremath{\oldtheta^{\min}}}
\newcommand{\hatthetamNMin}[1]{\hat\oldtheta^{\min}_{#1}}




\let\oldPhi\Phi
\newglossaryentry{Phi}
{
  name={\ensuremath{\oldPhi}},
  description={The objective function~$\oldPhi:\Theta\to\R, \theta \mapsto \oldPhi(\theta) := \Psi(Y^{\theta}(\gX{}))$ on the parameter-space $\Theta$ in expectation (i.e., \enquote{for an infinite amount of training paths}). %mention 2 different losses
  \inSectionNoiselessButInOtherSection{the old objective function given in \labelcref{equ:Psi,equ:Phi} that only works for noiseless observations is used}{the new objective function given in \labelcref{equ:Psi noisy obs,equ:Phi noisy obs} that also works for noisy observations is used}. $\oldPhi(\theta)$ is the expected value of $\hPhi{N}(\theta)$, where $\hPhi{N}$ is the loss we actually train on based on $N$ training paths%
  }
}
\renewcommand{\Phi}{\ensuremath{\myglslink{Phi}{\oldPhi}}}

\newglossaryentry{hPhi}
{
  name={\ensuremath{\hat{\oldPhi}_N}},
  description={The objective function~$\hat{\oldPhi}_N:\Theta\to\R, \theta \mapsto \hat{\oldPhi}_N(\theta) := \Psi(Y^{\theta}(\gX{}))$ on the parameter-space $\Theta$ for a finite number of $N\in\N$ training paths  (i.e., the Monte Carlo approximation of $\Phi$). %mention 2 different losses
  \inSectionNoiselessButInOtherSection{$\hat{\oldPhi}_N$ is defined in \Cref{equ:appr loss function} (this old objective function only leads to the correct results for noiseless observations}{$\hat{\oldPhi}_N$ is defined analogously via \labelcref{equ:Psi noisy obs,equ:Phi noisy obs} (this new objective function is different from the old one and even leads to the right result for noisy observations)}.
  Note that these 3 variants of $\hat{\oldPhi}_N$ are the loss functions on which we actually train our parameters in practice with finitely many training data%
  }
}
%\newcommand{\hPhi}{\ensuremath{\myglslink{hPhi}{\hat{\oldPhi}}}}
\makeCommandsForGLSEntryColoredMandatorySub{\hPhi}{hPhi}{\ensuremath{\hat{\oldPhi}}}



\newglossaryentry{boundedOutputNN}
{
  name={\ensuremath{\mathcal{N}}},
  description={The set $\mathcal{N}$ of bounded output neural networks consists of all feed-forward neural networks that have bounded outputs. In particular we assume that the final activation function applied to the output of the neural network is the bounded output activation function $\Gamma_{\gamma} = (\cdot) \min\left(1, \frac{\gamma}{|(\cdot)|_2}\right)$ (see \Cref{sec:Recall: the PD-NJ-ODE}).
  Throughout the paper we assume that the functions $f_{\theta_1}, \rho_{\theta_2} \in \mathcal{N}$ in the PD-NJ-ODE~\eqref{equ:PD-NJ-ODE} are bounded output feedforward neural networks.
  Note that this assumption is not really important in practice but facilitates our theoretical proof}
}

\newcommand{\cN}{\gls{boundedOutputNN}}


\newglossaryentry{classicalNN}
{
  name={\ensuremath{\tilde{\mathcal{N}}}},
  description={The set $\tilde{\mathcal{N}}$ of feedforward neural networks consists of all (classical) feed-forward neural networks.
  We assume that  $\tilde{\mathcal{N}}$ is a set of standard feedforward neural networks with $\operatorname{id} \in \tilde{\mathcal{N}}$ that satisfies the standard universal approximation theorem with respect to the supremum-norm on compact sets, see for example \citet[Theorem~2]{hornik1991approximation}
  Throughout the paper we assume that the functions $\tilde{g}_{\tildetheta{}_1} \in \tilde{\mathcal{N}}$ in the PD-NJ-ODE~\eqref{equ:PD-NJ-ODE} is a feedforward neural networks}
}

\newcommand{\tcN}{\gls{classicalNN}}


\newglossaryentry{normalDistribution}
{
  name={\ensuremath{\mathscr{N}(\mu,\sigma^2)}},
  text={\ensuremath{\mathscr{N}}},
  description={The normal distribution $\mathscr{N}(\mu,\sigma^2)$ (also known as Gaussian distribution) with mean $\mu$ and standard deviation $\sigma$}
}
\newcommand{\normalDistribution}{\gls{normalDistribution}}


%%%%%%%%%% Entry for Fj? %%%%%%%%%%
\newglossaryentry{Fj}
{
  name={\ensuremath{F_j}},
  description={The Doob-Dynkin Lemma \citep[Lemma 2]{taraldsen2018optimal} implies the existence of measurable functions $F_j : [0,\gls{T}] \times [0,\gls{T}] \times BV^c([0,\gls{T}]) \to \R$ such that $\hX{}_{t,j} = F_j\left(t, \tau(t), \tildeXle{\tau(t)}\right)$, since $\hX{}_{t,j}:=\E[\gX{t,j}|\At{t}]=\E[\gX{t,j}|\tildeXle{\tau(t)}]$ (see \Cref{sec:Recall: the PD-NJ-ODE}). The goal of our model $\gY{}^{\theta}$ is to learn this function $F=(F_j)_{1\leq j \leq \gls{dX}}$, i.e., $\lim_{m\to\infty}\gY{t}^{\thetamNMin{m,N_m}}\left(\tildeXle{\tau(t)}\right)= F\left(t, \tau(t), \tildeXle{\tau(t)}\right)=\hX{}_{t} $, while we are usually not able to write down $F$ explicitly. In settings with noise (e.g., in \Cref{sec:PD-NJ-ODE with Noisy Observations} and in the appendix) we replace $\tildeXle{\tau(t)}$ by $\tildeOle{\tau(t)}$}
}
\makeCommandsForGLSEntryOptionalSub{\Fj}{Fj}{F}


%%%%%%%%%% Entry for N? %%%%%%%%%%
% carful about the possion process N^


%%%%%%%%%% Entry for possion process N^? %%%%%%%%%%

%%%%%%%%%% Entry for m? %%%%%%%%%%
% most of the time m stands for the model-complexity


%%%%%%%%%% Entry for \Gamma? %%%%%%%%%%


%%%%%%%%%% Entry for \theta_m^\star? %%%%%%%%%%


\newglossaryentry{O}
{
  name={\ensuremath{O}},
  description={The noisy observations $O_{\tk{k}} := \gX{}_{\tk{k}} + \epsilon_k$ for $0 \leq k \leq \gn{}$, where $\epsilon_k$ is the noise with known mean (see \Cref{sec:Setting with Noisy Observations}). For the majority of the paper we assume that the noise has zero mean (whenever we use (the MC-approximation of) the loss~\labelcref{equ:Psi noisy obs,equ:Phi noisy obs}), except \Cref{sec:More General Noise Structure Conditional Moments} where we assume any known mean~$\beta_i(\tilde O^{\leq \tau(t)}) := \E[\epsilon_i | \At{\tk{i}-}]$ and thus use (the MC-approximation of) the loss~\eqref{equ:loss noisy obs generalised}. We define $O_{\tk{i} -} := \gX{}_{\tk{i} -} + \epsilon_i$ and therefore also have that $O_{\tk{i} } = O_{\tk{i} -}$} almost surely.
  \inSectionTildeButInOtherSection[0]{$\epsilon_k$ are i.i.d.\ random variables on $(\glsOm{tOmFFP}, \glsF{tOmFFP}, \glsP{tOmFFP})$}{$\epsilon_k$ are i.i.d.\ random variables on $\OmFP{}$}
}
\makeCommandsForGLSEntryOptionalSub{\gO}{O}{O}

\newglossaryentry{tildeO}
{
  name={\ensuremath{\tilde{O}^{\leq t}}},
  description={The interpolated observation process $\tilde O^{\leq t}$ continuously interpolates the noisy observations $\gO{\tk{i}}$ that where observed before time $t$ (see \Cref{sec:Setting with Noisy Observations}). To be precise the first \gls{dX} coordinates of $\tilde O^{\leq t}$ interpolate the noisy observations $\gO{\tk{i}}$, while the next \gls{dX} coordinates of $\tilde O^{\leq t}$ captures explicit information on when which coordinate was observed and the last coordinate is just the time.
  At time $s \in [0,\gls{T}]$, the interpolated observation process $\tilde O^{\leq t}_{s}\omb\in \R^{2\gls{dX}+1}$ is defined analogously to  $\tilde X^{\leq t}_{s}\omb\in \R^{2\gls{dX}+1}$ from \Cref{sec:Recall: the PD-NJ-ODE}, by replacing $\gls{X}$ by $\gls{O}$. %(see \Cref{sec:Recall: the PD-NJ-ODE})
  \inSectionTildeButInOtherSection[0]{$\tilde O^{\leq t}$ is an adapted stochastic process on $(\glsOm{OmFFP} \times \glsOm{tOmFFP}  , \glsF{OmFFP} \otimes \glsF{tOmFFP}, \glsFF{OmFFP} \otimes \glsFF{tOmFFP}, \glsP{OmFFP} \times \glsP{tOmFFP})$}{$\tilde O^{\leq t}$ is an adapted stochastic process on $\gls{OmFFP}$}%
  }
}
\newcommand{\tildeOle}[1]{\ensuremath{\myglslink{tildeO}{\tilde{O}^{\leq #1}}}}
\newcommand{\tildeOleSup}[2]{%
\rlap{\glslink{tildeO}{\phantom{\ensuremath{\tilde{O}^{\leq #1}}}}}\mathlinkcolored{\tilde{O}}^{\mathlinkcolored{\leq #1,} #2}%
}


\newglossaryentry{hO}
{
  name={\ensuremath{\hat{O}}},
  description={The conditional expectation of $\gls{O}$, which is its $L^2$-optimal prediction \citep[Proposition 2.5]{krach2022optimal} given the currently available information, is defined as $\hat{O}_{\tk{i}-} := \E_{\glsP{OmFFP}\times\glsP{tOmFFP}}[{\gO{}_{\tk{i}-} | \At{\tk{i}-}}]=\hX{\tk{i}-}$ (see the proof of \Cref{lem:L2 identity noisy obs setting}). Only directly at observation times, $\hat{O}_{\tk{i}}$ and $\hX{\tk{i}}$ deviate from each other; i.e., in general $\hX{\tk{i},k}=\E[\gX{\tk{i},k} \, | \, \At{\tk{i}}] \ne \gO{\tk{i},k} =\E[\gO{\tk{i},k} \, | \, \At{\tk{i}}]=\hat{O}_{\tk{i},k}$ if $\gM{i,k}=1$ (see \Cref{sec:PD-NJ-ODE with Noisy Observations}). \inSectionTildeButInOtherSection[0]{the expectation is taken with respect to $\glsP{OmFFP}\times\glsP{tOmFFP}$}{the expectation is taken with respect to $\glsP{OmFFP}$}}
}
%\newcommand{\hO}{\ensuremath{\myglslink{hO}{\ensuremath{\hat{O}}}}}
\makeCommandsForGLSEntryOptionalSub{\hO}{hO}{\hat{O}}









\newglossaryentry{aaa}
{
  name={\ensuremath{aaaaaaa}},
  description={a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a (see \Cref{sec:Recall: the PD-NJ-ODE})}
}

\newglossaryentry{bbb}
{
  name={\ensuremath{bbbb}},
  description={b b b b b b (see \Cref{sec:Recall: the PD-NJ-ODE})}
}

