\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Brouwer et~al.(2019)Brouwer, Simm, Arany, and Moreau]{Brouwer2019GRUODEBayesCM}
Edward~De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau.
\newblock {GRU-ODE-Bayes}: Continuous modeling of sporadically-observed time series.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Caruana et~al.(1997)Caruana, Pratt, and Thrun]{MultitaskCaruana1997}
Rich Caruana, Lorien Pratt, and Sebastian Thrun.
\newblock Multitask learning.
\newblock \emph{Machine Learning 1997 28:1}, 28:\penalty0 41--75, 1997.
\newblock ISSN 1573-0565.
\newblock \doi{10.1023/A:1007379606734}.
\newblock URL \url{https://link.springer.com/article/10.1023/A:1007379606734}.

\bibitem[Cohen \& Elliott(2015)Cohen and Elliott]{cohen2015stochastic}
Samuel~N Cohen and Robert~James Elliott.
\newblock Stochastic calculus and applications.
\newblock \emph{Springer}, 2015.

\bibitem[Corenflos et~al.(2021)Corenflos, Thornton, Deligiannidis, and Doucet]{corenflos2021differentiable}
Adrien Corenflos, James Thornton, George Deligiannidis, and Arnaud Doucet.
\newblock Differentiable particle filtering via entropy-regularized optimal transport.
\newblock In \emph{International Conference on Machine Learning}, pp.\  2100--2111. PMLR, 2021.

\bibitem[Durrett(2010)]{Durrett:2010:PTE:1869916}
Rick Durrett.
\newblock {Probability: Theory and Examples}.
\newblock \emph{Cambridge University Press}, 2010.

\bibitem[Eaton(2007)]{Eaton2007Multi}
Morris~L Eaton.
\newblock \emph{Multivariate Statistics: A Vector Space Approach}.
\newblock Institute of Mathematical Statistics, 2007.

\bibitem[Goldberger et~al.(2000)Goldberger, Amaral, Glass, Hausdorff, Ivanov, Mark, Mietus, Moody, Peng, and Stanley]{physionet}
Ary~L. Goldberger, Luis A.~N. Amaral, Leon Glass, Jeffrey~M. Hausdorff, Plamen~Ch. Ivanov, Roger~G. Mark, Joseph~E. Mietus, George~B. Moody, Chung-Kang Peng, and H.~Eugene Stanley.
\newblock Physiobank, physiotoolkit, and physionet.
\newblock \emph{Circulation}, 2000.

\bibitem[Hansen(2015)]{stackexchange_hansen}
Stefan Hansen.
\newblock Conditional expectation on more than one sigma-algebra.
\newblock Mathematics Stack Exchange, 2015.
\newblock URL \url{https://math.stackexchange.com/q/365319}.
\newblock URL:https://math.stackexchange.com/q/365319 (version: 2015-11-16).

\bibitem[Heiss et~al.(2019)Heiss, Teichmann, and Wutte]{heiss2019ImplReg1}
Jakob Heiss, Josef Teichmann, and Hanna Wutte.
\newblock How implicit regularization of relu neural networks characterizes the learned function--part i: the 1-d case of two layers with random first layer.
\newblock \emph{arXiv preprint arXiv:1911.02903}, 2019.

\bibitem[Heiss et~al.(2022)Heiss, Teichmann, and Wutte]{HeissImplReg3}
Jakob Heiss, Josef Teichmann, and Hanna Wutte.
\newblock How infinitely wide neural networks can benefit from multi-task learning - an exact macroscopic characterization.
\newblock \emph{arXiv preprint arXiv:2112.15577}, 2022.
\newblock \doi{10.3929/ETHZ-B-000550890}.
\newblock URL \url{http://hdl.handle.net/20.500.11850/550890}.

\bibitem[Heiss et~al.(2023)Heiss, Teichmann, and Wutte]{heiss2023ImplReg2}
Jakob Heiss, Josef Teichmann, and Hanna Wutte.
\newblock How (implicit) regularization of relu neural networks characterizes the learned function--part ii: the multi-d case of two layers with random first layer.
\newblock \emph{arXiv preprint arXiv:2303.11454}, 2023.

\bibitem[Herrera et~al.(2021)Herrera, Krach, and Teichmann]{herrera2021neural}
Calypso Herrera, Florian Krach, and Josef Teichmann.
\newblock Neural jump ordinary differential equations: Consistent continuous-time prediction and filtering.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Hornik(1991)]{hornik1991approximation}
Kurt Hornik.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \emph{Neural networks}, 4\penalty0 (2):\penalty0 251--257, 1991.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and White]{10.5555/70405.70408}
Kurt Hornik, Maxwell Stinchcombe, and Halbert White.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural Networks}, 2, 1989.

\bibitem[Kallenberg(2021)]{kallenberg1997foundations}
Olav Kallenberg.
\newblock \emph{Foundations of modern probability}.
\newblock Springer, 3 edition, 2021.

\bibitem[Karandikar \& Rao(2018)Karandikar and Rao]{KarandikarRao2018}
Rajeeva~L. Karandikar and B.~V. Rao.
\newblock \emph{Introduction to Stochastic Calculus}.
\newblock Indian Statistical Institute Series. Springer Singapore, Singapore, 2018.
\newblock ISBN 978-981-10-8317-4.
\newblock \doi{10.1007/978-981-10-8318-1}.
\newblock URL \url{http://link.springer.com/10.1007/978-981-10-8318-1}.

\bibitem[Kidger et~al.(2020)Kidger, Morrill, Foster, and Lyons]{Kidger2020NeuralCD}
Patrick Kidger, James Morrill, James Foster, and Terry Lyons.
\newblock Neural controlled differential equations for irregular time series.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{International Conference on Learning Representations}, 2014.

\bibitem[Krach et~al.(2022)Krach, N{\"u}bel, and Teichmann]{krach2022optimal}
Florian Krach, Marc N{\"u}bel, and Josef Teichmann.
\newblock Optimal estimation of generic dynamics by path-dependent neural jump odes.
\newblock \emph{arXiv}, 2022.

\bibitem[Lai et~al.(2022)Lai, Domke, and Sheldon]{lai2022variational}
Jinlin Lai, Justin Domke, and Daniel Sheldon.
\newblock Variational marginal particle filters.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  875--895. PMLR, 2022.

\bibitem[Lapeyre \& Lelong(2021)Lapeyre and Lelong]{lapeyre2019neural}
Bernard Lapeyre and Jérôme Lelong.
\newblock Neural network regression for bermudan option pricing.
\newblock \emph{Monte Carlo Methods and Applications}, 27\penalty0 (3):\penalty0 227--247, 2021.

\bibitem[Le et~al.(2017)Le, Igl, Rainforth, Jin, and Wood]{le2017auto}
Tuan~Anh Le, Maximilian Igl, Tom Rainforth, Tom Jin, and Frank Wood.
\newblock Auto-encoding sequential monte carlo.
\newblock \emph{arXiv}, 2017.

\bibitem[Maddison et~al.(2017)Maddison, Lawson, Tucker, Heess, Norouzi, Mnih, Doucet, and Teh]{maddison2017filtering}
Chris~J Maddison, John Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, and Yee Teh.
\newblock Filtering variational objectives.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Mitchell(1980)]{Mitchell:1980:InductiveBias}
Tom~M. Mitchell.
\newblock The need for biases in learning generalizations.
\newblock In Jude~W. Shavlik and Thomas~G. Dietterich (eds.), \emph{Readings in Machine Learning}, pp.\  184--191. Morgan Kauffman, 1980.
\newblock URL \url{http://www.cs.nott.ac.uk/~bsl/G52HPA/articles/Mitchell:80a.pdf}.
\newblock Book published in 1990.

\bibitem[Morrill et~al.(2021)Morrill, Salvi, Kidger, and Foster]{morrill2021neural}
James Morrill, Cristopher Salvi, Patrick Kidger, and James Foster.
\newblock {Neural rough differential equations for long time series}.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7829--7838. PMLR, 2021.

\bibitem[Morrill et~al.(2022)Morrill, Kidger, Yang, and Lyons]{morrill2022on}
James Morrill, Patrick Kidger, Lingyi Yang, and Terry Lyons.
\newblock {On the choice of interpolation scheme for neural {CDE}s}.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock ISSN 2835-8856.

\bibitem[Neyshabur(2017)]{neyshabur2017implicit}
Behnam Neyshabur.
\newblock Implicit regularization in deep learning, 2017.

\bibitem[Ongie et~al.(2019)Ongie, Willett, Soudry, and Srebro]{ongie2019function}
Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro.
\newblock A function space view of bounded norm infinite width relu nets: The multivariate case.
\newblock \emph{arXiv preprint arXiv:1910.01635}, 2019.
\newblock URL \url{https://arxiv.org/pdf/1910.01635.pdf}.

\bibitem[Parhi \& Nowak(2022)Parhi and Nowak]{parhi2022kinds}
Rahul Parhi and Robert~D Nowak.
\newblock What kinds of functions do deep neural networks learn? insights from variational spline theory.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 4\penalty0 (2):\penalty0 464--489, 2022.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019_9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{NIPS}. Curran Associates, Inc., 2019.

\bibitem[Reizenstein \& Graham(2018)Reizenstein and Graham]{reizenstein2018iisignature}
Jeremy Reizenstein and Benjamin Graham.
\newblock The iisignature library: efficient calculation of iterated-integral signatures and log signatures.
\newblock \emph{arXiv}, 2018.

\bibitem[Rubanova et~al.(2019)Rubanova, Chen, and Duvenaud]{ODERNN2019}
Yulia Rubanova, Ricky T.~Q. Chen, and David~K Duvenaud.
\newblock Latent ordinary differential equations for irregularly-sampled time series.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Savarese et~al.(2019)Savarese, Evron, Soudry, and Srebro]{savarese2019infinite}
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro.
\newblock How do infinite width bounded norm networks look in function space?
\newblock \emph{arXiv preprint arXiv:1902.05040}, 2019.
\newblock URL \url{https://arxiv.org/abs/1902.05040}.

\bibitem[Sch{\"a}fer \& Zimmermann(2006)Sch{\"a}fer and Zimmermann]{schafer2006recurrent}
Anton~Maximilian Sch{\"a}fer and Hans~Georg Zimmermann.
\newblock Recurrent neural networks are universal approximators.
\newblock In \emph{Artificial Neural Networks--ICANN 2006: 16th International Conference, Athens, Greece, September 10-14, 2006. Proceedings, Part I 16}, pp.\  632--640. Springer, 2006.

\bibitem[Taraldsen(2018)]{taraldsen2018optimal}
Gunnar Taraldsen.
\newblock Optimal learning from the doob-dynkin lemma.
\newblock \emph{arXiv}, 2018.

\bibitem[Yoo(2014)]{stackexchange_yoo}
Jisang Yoo.
\newblock Conditional expectation on more than one sigma-algebra.
\newblock Mathematics Stack Exchange, 2014.
\newblock URL \url{https://math.stackexchange.com/q/915935}.
\newblock URL:https://math.stackexchange.com/q/915935 (version: 2014-09-01).

\end{thebibliography}
