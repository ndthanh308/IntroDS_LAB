\documentclass{article}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% %TODO :
% - 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Controlling behavior of output file
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Include Appendix (0: False, 1: True)
\def\inclapp{1}
%%% View Changes in Color or final version (0: Final, 1: Changes)
\def\viewchanges{1}
%%% View Authors (0: No, 1: Yes)
\def\viewauthors{1}
%%% View Keywords (0: No, 1: Yes)
\def\viewkeywords{0}
%%% use hyperlinks (0: No, 1: Yes)
\def\usehyperlinks{1}
%%% add acknowledgement (0: No, 1: Yes)
\def\addackn{0}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\usepackage[dvipsnames]{xcolor}


% ready for submission
\usepackage{iclr2021_conference,times}
\if\viewauthors1
\iclrfinalcopy
\fi



% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
 %    \usepackage[nonatbib]{neurips_2020}
 
\usepackage{natbib}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{bbm}
% \usepackage{showframe}
\allowdisplaybreaks        % for page brake in equations
\if\usehyperlinks1
	\usepackage[colorlinks, citecolor=blue, linkcolor=magenta]{hyperref}       % hyperlinks
\fi


% Add for algo
\usepackage{graphicx,wrapfig,lipsum}
\usepackage{subfigure}
%\usepackage{subcaption}
\usepackage{verbatim}
\usepackage{bm}
\usepackage{adjustbox}
\usepackage{footnote}
\usepackage{algorithm}
\usepackage{algorithmic}


\if\inclapp0
	%for references
	\usepackage{xr}
	%\externaldocument{NJODE_appendix}
\fi


% new commands:
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lem}[theorem]{Lemma}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{rem}[theorem]{Remark}
\newtheorem{cor}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}[theorem]{Assumption}

\newcommand{\blue}{\textcolor{blue}}
\let\P\undefined%
\newcommand{\1}{\mathbbm{1}}
\newcommand{\P}{\mathbb{P}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\E}{\mathbb{E}} % expectation
\newcommand{\N}{\mathbb{N}} 
\newcommand{\R}{\mathbb{R}} 
\newcommand{\argmin}{\operatorname{argmin}} 
\newcommand{\id}{\operatorname{id}} 
\newcommand{\argmax}{\operatorname{argmax}} 
\newcommand{\sigmab}{\boldsymbol{\sigma}}

\renewcommand{\l}{\left} % overwrites smth
\renewcommand{\r}{\right}
\newcommand{\ind}[1]{\mathbbm{1}_{#1}}


\makeatletter
\newcommand{\expect}[1]{
  \if@display \mathbb{E} \left[ #1 \right]
  \else \mathbb{E} [ #1 ]
  \fi
}
\makeatother

\makeatletter
\newcommand{\cexpect}[2]{
  \if@display \expect{#1 \mid #2}
  \else \expect{#1 \, | \, #2}
  \fi
}
\makeatother

\makeatletter
\newcommand{\abs}[1]{
  \if@display \left| #1 \right|
  \else | #1 |
  \fi
}
\makeatother

%%% chasing changes with colors:
%\usepackage[dvipsnames]{xcolor}
\let\add\undefined
\let\del\undefined
\let\com\undefined
\let\cha\undefined
\newcommand{\g}{\textcolor{red}}
%%% display changes in color:
\if\viewchanges1
	\newcommand{\add}[1]{{\color{OliveGreen}{#1}}}
	\newcommand{\del}[1]{{\color{red}{#1}}}
	\newcommand{\com}[1]{{\color{orange}{#1}}}
	\newcommand{\cha}[2]{{\del{#1}} {\add{#2}}}
%%% apply changes:
\else
	\newcommand{\add}[1]{{#1}}
	\newcommand{\del}[1]{}
	\newcommand{\com}[1]{}
	\newcommand{\cha}[2]{{#2}}
\fi




\title{{Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework}}



\if\viewauthors1
	\author{
William Andersson\footnote{} \quad
	  Jakob Heiss\footnote{} \quad
	  Florian Krach\footnotemark[2] \quad 
	  Josef Teichmann\footnotemark[2]
	}
\else
	\author{}
\fi


\providecommand{\keywords}[1]{\textbf{{Keywords:}} \textit{#1}}





\begin{document}

\maketitle

\begin{abstract}
The Path-Dependent Neural Jump ODE (PD-NJ-ODE) \citep{krach2022optimal} is a model for predicting continuous-time stochastic processes with irregular and incomplete observations. In particular, the method learns optimal forecasts given irregularly sampled time series of incomplete past observations.
So far the process itself and the coordinate-wise observation times were assumed to be independent and observations were assumed to be noiseless.
In this work we discuss two extensions to lift these restrictions and provide theoretical guarantees as well as empirical examples for them.
\end{abstract}




\if\viewkeywords1
	\keywords{}
\fi


%%%% ------------------- add footnotes for authors ---------------------------
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\setcounter{footnote}{1}
\footnotetext{
Department of Computer Science, ETH Zurich, Switzerland, \texttt{anwillia@ethz.ch}
}
\setcounter{footnote}{2}
\footnotetext{
Department of Mathematics, ETH Zurich, Switzerland, \texttt{\{firstname.lastname\}@math.ethz.ch}
}
\renewcommand{\thefootnote}{\arabic{footnote}}
\setcounter{footnote}{0}


%%% ==================================================================
\section{Introduction}\label{sec:Introduction}
While the online prediction\footnote{With \emph{online} prediction we mean that we use the currently available information to predict until we get new information. As soon as new information becomes available, it is part of the available information and therefore taken into account for subsequent predictions.} of regularly observed or sampled time series is a classical machine learning problem that can be solved with recurrent neural networks (RNNs) as proven e.g.\ by \cite{schafer2006recurrent}, the forecasting of continuous-time processes  with irregular observation has long been an unsolved problem.  
The Neural Jump ODE (NJ-ODE) \citep{herrera2021neural} was the first framework with theoretical guarantees to converge to the optimal prediction in this setting.
However, it was restricted to Markovian It\^o-diffusions with irregular but complete (i.e.\ all coordinates are observed at the same time) observations. This was heavily generalised with the Path-Dependent NJ-ODE \citep{krach2022optimal}, where the convergence guarantees hold for very general (non-Markovian) stochastic processes with irregular and incomplete observations. Still, the process itself and the observation framework were assumed to be independent and observations were assumed to be noisefree.
In practice both of these assumptions are often unrealistic. E.g., for medical patient data collected at a hospital irregularly over time such as \citet{physionet}, measurements are never noise-free and the decision whether to make a measurement depends on the status of the patient.
Therefore, the focus of this work is to lift those two restrictions. A detailed outline is given below. For more related work we refer the interested reader to the respective sections in \cite{herrera2021neural} and \cite{krach2022optimal}.



% \subsection{Related Work}\label{sec:Related Work}



\subsection{Outline of the Work}\label{sec:Outline}
We introduce two extensions of the PD-NJ-ODE \citep{krach2022optimal} that can be used separately or jointly. To highlight the needed adjustments for each of the extensions, we first recall the setup, model and results from \cite{krach2022optimal} (Section~\ref{sec:Recall: the PD-NJ-ODE}) and then introduce the respective changes in the assumptions and proofs for noisy observations (Section~\ref{sec:PD-NJ-ODE with Noisy Observations}) and dependence between the underlying process and the observation framework (Section~\ref{sec:PD-NJ-ODE with Dependence between X and Observation Framework}) separately. 
We focus on re-proving the main results \cite[Theorem~4.1 and Theorem~4.4]{krach2022optimal} in the new settings, by giving the arguments which need to be adjusted while skipping those which remain unchanged.
In Appendix~\ref{sec:Combining the Two Extensions With Full Proof} we give the full proof for the most general result with both extensions, making the paper self-contained.
We remark here that also the results for the conditional variance and for stochastic filtering \cite[Section~5 and 6]{krach2022optimal} follow in these extended settings similarly as the main results. Due to the similarity, we do not elaborate on this but leave the details to the interested reader.
Finally, in Section~\ref{sec:Experiments} we show empirically that the PD-NJ-ODE performs well in these generalised settings.

%%% ==================================================================
\section{Recall: the PD-NJ-ODE}\label{sec:Recall: the PD-NJ-ODE}

We start by recalling the most relevant parts of the problem setting of the PD-NJ-ODE. For more details, please refer to \citep{krach2022optimal}.

For $d_X \in \N$ and $T > 0$ we consider a filtered probability space  $(\Omega, \F, \mathbb{F} := \{\F_t\}_{0 \leq t \leq T}, \P )$ with an adapted c\`adl\`ag  stochastic process $X :={(X_t)}_{t \in [0,T]}$ taking values in $\R^{d_X}$. We denote its running maximum process by $X^\star$ and the random set of its jump times by $\mathcal{J}$. The random observation framework is defined independently of $X$ on another filtered probability space  $(\tilde\Omega, \tilde\F,  \tilde{\mathbb{F}} := \{ \tilde{\F}_t \}_{0 \leq t \leq T}, \tilde\P )$ by
\begin{itemize}
\item $n: \tilde\Omega \to \N_{\geq 0}$, an $\tilde{\F}$-measurable integrable random variable, the random number of observations, 
\item $K := \sup \left\{k \in \N \, | \, \tilde\P(n \geq k) > 0 \right\} \in \N \cup\{\infty\}$, the maximal value of $n$, 
\item  $t_i: \tilde\Omega \to [0,T] \cup \{ \infty \}$ for $0 \leq i \leq K$, {sorted} stopping times, which are the random observation times, with $t_i(\tilde{\omega}) := \infty$ if $n(\tilde{\omega}) < i$,
\item $\tau : [0,T] \times \tilde\Omega \to [0,T], \; (t, \tilde\omega) \mapsto \tau(t, \tilde\omega) := \max\{ t_i(\tilde\omega) | 0 \leq i \leq n(\tilde\omega), t_i(\tilde\omega) \leq t \}$, the  last observation time before a certain time $t$, and
\item $M = (M_k)_{0 \leq k \leq K}$, the observation mask, which is a sequence of random variables on  $(\tilde\Omega, \tilde\F, \tilde\P )$ taking values in $\{ 0,1 \}^{d_X}$ such that $M_k$ is $\tilde{\mathcal{F}}_{t_k}$-measurable.
The $j$-th coordinate of the $k$-th element of the sequence $M$, i.e. $M_{k,j}$, signals whether $X_{t_k, j}$, denoting the $j$-th coordinate of the stochastic process at observation time $t_k$ is observed. By abuse of notation we also write $M_{t_k} := M_{k}$. 
\end{itemize}
In the following we consider the filtered product probability space $(\Omega \times \tilde\Omega , \F \otimes \tilde\F, \mathbb{F} \otimes \tilde{\mathbb{F}}, \P \times \tilde\P)$ and the filtration of the currently available information $\mathbb{A} := (\mathcal{A}_t)_{t \in [0,T]}$ defined by 
\begin{equation*}
\mathcal{A}_t := \boldsymbol{\sigma}\left(X_{t_i, j}, t_i, M_{t_i} | t_i \leq t,\, j \in \{1 \leq l \leq d_X | M_{t_i, l} = 1  \} \right),
\end{equation*} 
where $\boldsymbol\sigma(\cdot)$ denotes the generated $\sigma$-algebra. 
We note that $\mathcal{A}_t = \mathcal{A}_{\tau(t)}$ for all $t \in [0,T]$.
The conditional expectation process of $X$, which is its $L^2$-optimal prediction \citep[Proposition 2.5]{krach2022optimal}, is defined as $\hat X = (\hat X_t)_{0 \leq t \leq T}$, with $\hat{X}_t := \E_{\P\times\tilde\P}[X_t | \A_t]$.
Moreover, for any $0 \leq t \leq T$ the $j$-th coordinate of the interpolated observation process $\tilde X^{\leq t} \in \R^{2d_X}$  at time $0 \leq s \leq T$ is defined by
\begin{equation*}
\tilde X^{\leq t}_{s,j} := \begin{cases}
	X_{t_{l(s,t)},j} \frac{t_{\ell(s,t)} - s}{t_{\ell(s,t)} - t_{\ell(s,t)-1}} + X_{t_{\ell(s,t)},j} \frac{s - t_{\ell(s,t)-1}}{t_{\ell(s,t)} - t_{\ell(s,t)-1}}, & \text{if }  t_{\ell(s,t)-1} < s \leq t_{\ell(s,t)} \text{ and}  \\ & 1 \leq j \leq d_X, \\
	X_{t_{l(s,t)},j}, & \text{if } s \leq t_{\ell(s,t)-1} \text{ and } 1 \leq j \leq d_X, \\
    u_{t_{l(s,t)},j-d_X}  +  \frac{s - t_{\ell(s,t)-1}}{t_{\ell(s,t)} - t_{\ell(s,t)-1}}, & \text{if }  t_{\ell(s,t)-1} < s \leq t_{\ell(s,t)} \text{ and } \\ & d_X < j \leq 2 d_X, \\
	u_{t_{l(s,t)},j-d_X}, & \text{if } s \leq t_{\ell(s,t)-1} \text{ and}    d_X < j \leq 2 d_X , \\
\end{cases}
\end{equation*}
where $u_{t,j} := \sum_{k=0}^K M_{k, j} \1_{t_k \leq t}$ is the jump process that counts the coordinate-wise observations and
\begin{equation*}
\begin{split}
l(s,t) &:= l(s,t, j):=  \max\{ 0 \leq l \leq n \vert t_l \leq \min(s,t), M_{t_l,j}  = 1 \}, \\
\ell(s,t) &:= \ell(s,t,j) :=  \inf\{ 1 \leq \ell \leq n \vert s \leq t_\ell \leq t, M_{t_\ell,j}  = 1 \},
\end{split}
\end{equation*}
with $t_\infty := T$. 
The paths of $\tilde X^{\leq t}$ belong to $BV^c([0,T])$, the set of continuous $\R^{d_X}$-valued paths of bounded variation on $[0,T]$.
Since $\tilde X^{\leq \tau(t)} \in \mathcal{A}_{\tau(t)}$ carries all available information, the Doob-Dynkin Lemma \citep[Lemma 2]{taraldsen2018optimal} implies the existence of measurable functions $F_j : [0,T] \times [0,T] \times BV^c([0,T]) \to \R$ such that $\hat X_{t,j} = F_j(t, \tau(t), \tilde X^{\leq \tau(t)}) $. 
In \citep{krach2022optimal} convergence of the PD-NJ-ODE model to the conditional expectation $\hat{X}$  was shown under the following assumptions. 
\begin{assumption} \label{assumption:F}
We assume that 
\begin{enumerate}
\item for every $1\leq k, l \leq K$, $M_k$ is independent of  $t_l$ and of $n$,  $\tilde \P (M_{k,j} =1 ) > 0$ and $M_{0,j}=1$ for all  $1 \leq j \leq d_X$  (i.e. every coordinate can be observed at any observation time and $X$ is completely observed at $0$) and $|M_k|_1 > 0$ for every $1 \leq k \leq K$ $\tilde{\P}$-almost surely (i.e. at every observation time at least one coordinate is observed), \label{ass_main_1}
\item the probability that any two observation times are closer than $\epsilon>0$ converges to $0$ when $\epsilon$ does, i.e. if $\delta(\tilde \omega) := \min_{0 \leq i \leq n(\tilde \omega)} |t_{i+1}(\tilde \omega) - t_i(\tilde \omega)|$ then $\lim_{\epsilon \to 0} \tilde \P (\delta < \epsilon) = 0$,\label{ass_main_2}
\item almost surely $X$ is  not observed at a jump, i.e. $(\P \times \tilde{\P})( t_j \in \mathcal{J} | j\leq n ) = (\P \times \tilde{\P})( \Delta X_{t_j} \neq 0 | j\leq n) = 0$ for all $ 1 \leq j \leq K$,\label{ass_main_3}
\item $F_j$ are  continuous and differentiable in their first coordinate $t$ such that their partial derivatives with respect to $t$, denoted by $f_j$, are again continuous and there exists a $B >0$ and $p \in \N$ such that for every $t \in [0,T]$ the functions $f_j, F_j$ are polynomially bounded in $X^\star$, i.e. 
$$|F_j(\tau(t), \tau(t), \tilde X^{\leq \tau(t)})| +  | f_j(t, \tau(t), \tilde X^{\leq \tau(t)})  | \leq B (X_t^\star +1)^p , $$ \label{ass_main_4}
\item $X^\star$ is $L^{2 p}$-integrable, i.e. $\E[(X^\star_T)^{2 p}] < \infty$. \label{ass_main_5}
\end{enumerate}
\end{assumption} 
Moreover, relaxations on the assumption of observing $X_0$ completely were discussed in \citep[Remark 2.3]{krach2022optimal}.

\begin{definition}\label{def:indistinguishability}
    Let $c_0 := c_0(k) := (\tilde\P (n\geq k))^{-1}$.
    A distance between c\`adl\`ag $\mathbb{A}$-measurable processes $Z, \xi  : [0,T] \times (\Omega \times \tilde{\Omega}) \to \R^{r}$ is defined through the pseudo metrics 
\begin{equation}\label{equ:pseudo metric dk}
    d_k (Z, \xi) = c_0(k)\,  \E_{\P \times \tilde{\P}}\left[ \1_{\{n \geq k\}} | Z_{t_k-} - \xi_{t_k-} | \right], 
\end{equation}
for $1 \leq k \leq K$ and two processes are called indistinguishable, if $d_k (Z, \xi) = 0$ for all $1 \leq k \leq K$.

\end{definition}

The path-dependent generalisation of the Neural Jump ODE model \citep{herrera2021neural} uses the truncated signature transformation $\pi_m$ \citep[Definition 3.4]{krach2022optimal} and bounded output neural networks $f_{(\tilde \theta, \gamma)}$, where $\tilde\theta$ are the weights of the standard neural network and $\gamma > 0$ is the trainable parameter of the bounded output activation function \citep[Definition 3.12]{krach2022optimal}
\begin{equation*}
\Gamma_\gamma : \R^d \to \R^d, x \mapsto x \cdot \min\left(1, \frac{\gamma}{|x|_2}\right),
\end{equation*}
applied to the output of the standard neural network. By $\mathcal{N}$ we denote the set of all bounded output neural networks based on a set $\tilde{\mathcal{N}}$ of standard neural networks. 
In the following we assume that  $\tilde{\mathcal{N}}$ is a set of standard feedforward neural networks with $\operatorname{id} \in \tilde{\mathcal{N}}$ that satisfies the standard universal approximation theorem with respect to the supremum-norm on compact sets, see for example \cite[Theorem~2]{hornik1991approximation}.


\begin{definition}\label{def:Sig-NJ-ODE}
    The  \textit{Path-Dependent Neural Jump ODE (PD-NJ-ODE)} model is given by
    \begin{equation}\label{equ:PD-NJ-ODE}
    \begin{split}
    H_0 &= \rho_{\theta_2}\left(0, 0, \pi_m (0), X_0 \right), \\
    dH_t &= f_{\theta_1}\left(H_{t-}, t, \tau(t), \pi_m (\tilde X^{\leq \tau(t)} -X_0 ), X_0 \right) dt  \\
    & \quad + \left( \rho_{\theta_2}\left( H_{t-}, t, \pi_m (\tilde X^{\leq \tau(t)}-X_0 ), X_0 \right) - H_{t-} \right) du_t, \\
    Y_t &= \tilde g_{\tilde \theta_3}(H_t).
    \end{split}
    \end{equation}
    The functions $f_{\theta_1}, \rho_{\theta_2} \in \mathcal{N}$ are bounded output feedforward neural networks and $g_{\tilde \theta_3} \in \tilde{\mathcal{N}}$ is a feedforward neural network  with trainable parameters $\theta = (\theta_1, \theta_2, \tilde \theta_3) \in \Theta$, where $\theta_i = (\tilde \theta_i, \gamma_i)$ for $i \in \{1,2 \}$ and $\Theta$ is the set of all possible weights for the PD-NJ-ODE; $m \in \N$ is the signature truncation level and $u$ is the jump process counting the observations defined as $u_t := \sum_{k=1}^K \1_{t_k \leq t}$.
\end{definition}
The existence and uniqueness of a solution of \eqref{equ:PD-NJ-ODE} is implied by \citep[Thm. 7, Chap. V]{Pro1992}. To emphasize the dependence of the PD-NJ-ODE output $Y$ on $\theta$ and $X$ we write $Y^\theta(X)$.

The objective function (cf.\ \emph{equivalent objective function} from Remark~4.7 \& Appendix A.1.4 of \cite{krach2022optimal}) for the training of the PD-NJ-ODE is defined as
\begin{align}
\Psi: \, &\mathbb{D} \to \R, \nonumber \\
&Z \mapsto \Psi(Z) := \E_{\P\times\tilde\P}\left[ \frac{1}{n} \sum_{i=1}^n  \left(  \left\lvert M_i \odot ( X_{t_i} - Z_{t_i} ) \right\rvert_2 + \left\lvert M_i \odot (X_{t_i} - Z_{t_{i}-} ) \right\rvert_2 \right)^2 \right], \label{equ:Psi} \\
\Phi : \, &\Theta \to \R, \theta \mapsto \Phi(\theta) := \Psi(Y^{\theta}(X)), \label{equ:Phi}
\end{align}
where $\odot$ is the element-wise multiplication (Hadamard product) and $\mathbb{D}$ the set of all c\`adl\`ag $\R^{d_X}$-valued $\mathbb{A}$-adapted processes on the product probability space $\Omega \times \tilde\Omega$.
Moreover, for $N \in \N$ and  $X^{(j)} \sim X$, $M^{(j)} \sim M$ and $(n^{(j)}, t_1^{(j)}, \dotsb, t_{n^{(j)}}^{(j)}) \sim ( n, t_1, \dotsb, t_{n})$  i.i.d.\ random processes (respectively variables) for $1 \leq j \leq N$ the Monte Carlo approximation of \eqref{equ:Phi} is 
\begin{equation}\label{equ:appr loss function}
\hat\Phi_N(\theta) := \frac{1}{N} \sum_{j=1}^N  \frac{1}{n^{(j)}}\sum_{i=1}^{n^{(j)}} \left(  \left\lvert M_{i}^{(j)} \odot \left( X_{t_i^{(j)}}^{(j)} - Y_{t_i^{(j)}}^{\theta, j } \right) \right\rvert_2 + \left\lvert M_{i}^{(j)} \odot \left( X_{t_i^{(j)}}^{(j)} - Y_{t_{i}^{(j)}-}^{\theta, j } \right) \right\rvert_2 \right)^2,
\end{equation}
where $Y^{\theta, j} := Y^{\theta }(X^{(j)})$.


Based on these loss functions, the following convergence guarantees can be derived, where $\hat \Theta_m \subset \Theta$ is defined as the set of possible weights for the $3$ (bounded output) neural networks, such that their widths are at most $m$ and such that the truncated signature of level $m$ or smaller is used. We use $\Theta_m := \{ \theta = ((\tilde{\theta}_1, \gamma_1), (\tilde{\theta}_2, \gamma_2), \tilde{\theta}_3 ) \in \hat{\Theta}_m \, | \, |\tilde \theta_i|_2 \leq m, \gamma_i \leq m \} \subset \hat{\Theta}_m$, which is a compact subset of $\Theta_m$.

\begin{theorem}\label{thm:1}
Let $\theta^{\min}_m \in \Theta_m^{\min} := \argmin_{\theta \in \Theta_m}\{ \Phi(\theta) \}$ for every $m \in \N$. If Assumption~\ref{assumption:F} is satisfied, then, for $m \to \infty$, the value of the loss function $\Phi$ \eqref{equ:Phi} converges to the minimal value of $\Psi$ \eqref{equ:Psi} which is uniquely achieved by $\hat{X}$ up to indistinguishability, i.e.
\begin{equation*}
\Phi(\theta_m^{\min}) \xrightarrow{m \to \infty} \min_{Z \in \mathbb{D}} \Psi(Z) = \Psi(\hat{X}).
\end{equation*}
Furthermore, for every $1 \leq k \leq K$ we have that $Y^{\theta_m^{\min}}$ converges to $\hat{X}$ in the metric $d_k$ \eqref{equ:pseudo metric dk} as $m \to \infty$.

Let $\theta^{\min}_{m,N} \in \Theta^{\min}_{m,N} := \argmin_{\theta \in \Theta_m}\{ \hat\Phi_N(\theta)\}$ for every $m, N \in \N$. 
Then, for every $m \in \N$, $(\P\times\tilde\P)$-a.s. 
%$\hat\Phi_N$ converges uniformly on $\tilde\Theta_m$ to $\Phi$ for $N \to \infty$.
\begin{equation*}
\hat\Phi_N \xrightarrow{N \to \infty} \Phi \quad \text{uniformly on } \Theta_m.
\end{equation*}
Moreover, for every $m \in \N$, $(\P\times \tilde\P)$-a.s.
\begin{equation*}
\Phi(\theta^{\min}_{m,N}) \xrightarrow{N \to \infty} \Phi(\theta^{\min}_{m}) \quad \text{and} \quad \hat\Phi_N(\theta^{\min}_{m,N}) \xrightarrow{N \to \infty} \Phi(\theta^{\min}_{m}). 
\end{equation*}
In particular, one can define an increasing sequence $(N_m)_{m \in \N}$ in $\N$ such that for every $1 \leq k \leq K$ we have that $Y^{\theta_{m, N_m}^{\min}}$ converges to $\hat{X}$ 
in the metric $d_k$  as $m \to \infty$.
\end{theorem}



\section{PD-NJ-ODE with Noisy Observations}\label{sec:PD-NJ-ODE with Noisy Observations}
So far the PD-NJ-ODE model does not support measurement noise in the form of i.i.d. noise terms that are added to each observation. 
Using the stochastic filtering approach described in \cite[Section~6]{krach2022optimal} would be a possibility to include (discrete or continuous) noise. However, this requires strong additional assumptions (i.e.\ the knowledge of the joint distribution of the process and the noise or equivalently training samples split up into the noise-free observations and the noise terms) which are not satisfied easily. Therefore, we want to adapt our framework, such that it can be applied to noisy observations, while only imposing weak assumptions that are easily satisfied.

In this section, we introduce observation noise (e.g., measurement-noise), i.e.\ i.i.d.\ noise terms $\epsilon_i$ that are added to the process $X$ at each observation time $t_i$ leading to the noisy observations $O_{t_i} := X_{t_i} + \epsilon_i$. Even though we only observe the $O_{t_i}$, the goal still is to predict $X$, in particular to compute $\E[X_t \, | \, O_{t_0},\dots,O_{\tau(t)}]$.

Inspecting the PD-NJ-ODE model and its loss function (replacing $X$ by $O$), we notice two things. First that there is no reason why the model architecture should not be able to learn this modified objective, which should still have the same properties. And secondly that the loss function needs to be modified. Indeed, the first term of the loss function would train the model to jump to the noisy observation $O_{t_i}$. However, this would not be the correct behaviour, since in general $\E[X_{t_i} \, | \, O_{t_0}, \dotsc, O_{t_i}] \ne O_{t_i}$ as the conditional expectation filters out the noise as good as possible. Therefore we drop the first term of the loss function.

On the other hand, it is easy to see that the conditional expectations of $X$ and $O$ coincide in between observation times\footnote{Note that, in general we have $\E[X_{t_i} \, | \, O_{t_0}, \dotsc, O_{t_i}]\neq\E[O_{t_i} \, | \, O_{t_0}, \dotsc, O_{t_i}]=O_{t_i}$ at observation times, in contrast to their equality between observation times.} if the observation noise $\epsilon_i$ is independent of the observations and has mean $0$. Therefore, the second term of the loss function is minimised if the model learns the conditional expectation $\E[X_{t} \, | \, O_{t_0}, \dotsc, O_{\tau(t)}]$ between observation times.

Along these lines it turns out that it is enough to omit the first term of the loss function to recover the original results of Theorem~\ref{thm:1} under noisy observations. 
In particular, to optimize the loss, the model learns to jump to the conditional expectation of $X$ at observation times even without the respective loss term. Indeed, since it evolves continuously after an observation it would otherwise be different from the optimal prediction right after the observation time and therefore not optimizing the loss. 
In the following this is formalised.

\subsection{Setting with Noisy Observations}\label{sec:Setting with Noisy Observations}
The process $X$ as well as the $n, K, t_i, \tau, M$ are defined as in Section~\ref{sec:Recall: the PD-NJ-ODE}. Additionally, we define
\begin{itemize}
    \item $(\epsilon_k)_{0 \leq k \leq K}$, the observation noise, which is a sequence of i.i.d.\ random variables on $(\tilde\Omega, \tilde\F, \tilde\P )$ taking values in $\R^{d_X}$,
    \item $O_{t_k} := X_{t_k} + \epsilon_k$ for $0 \leq k \leq n$, the noisy observation sequence.
\end{itemize}
Since the goal is to predict $X$ given the observations $O_{t_i}$, we redefine the filtration of the currently available information via 
\begin{equation*}
\mathcal{A}_t := \boldsymbol{\sigma}\left(O_{t_i, j}, t_i, M_{t_i} | t_i \leq t,\, j \in \{1 \leq l \leq d_X | M_{t_i, l} = 1  \} \right),
\end{equation*} 
such that $\hat{X}_t = \E_{\P\times\tilde\P}[X_t | \A_t]$ is the conditional expectation of $X$ given the noisy observations. We define $\tilde O^{\leq t}$ in the same way as $\tilde X^{\leq t}$ and note that similarly as before there exist measurable functions $F_j$ such that $\hat X_{t,j} = F_j(t, \tau(t), \tilde O^{\leq \tau(t)})$.
We need the following slight modification of Assumption~\ref{assumption:F}.
\begin{assumption}\label{ass:noisy obs setting}
We assume that assumptions \ref{ass_main_1}, \ref{ass_main_2}, \ref{ass_main_3} and \ref{ass_main_5} of Assumption~\ref{assumption:F} hold and additionally that
\begin{enumerate}
\item[4.] $F_j$ are  continuous and differentiable in their first coordinate $t$ such that their partial derivatives with respect to $t$, denoted by $f_j$, are again continuous and there exists a $B >0$ and $p \in \N$ such that for every $t \in [0,T]$ the functions $f_j, F_j$ are polynomially bounded in $X^\star$, i.e. 
$$|F_j(\tau(t), \tau(t), \tilde O^{\leq \tau(t)})| +  | f_j(t, \tau(t), \tilde O^{\leq \tau(t)})  | \leq B (X_t^\star +1)^p + B \sum_{i=0}^n |\epsilon_i|, $$ \label{ass_noisy_4}
\item[6.] the i.i.d.\ random noise variables $\epsilon_k$ are independent of $X, n, M, (t_i)_{1 \leq i \leq K}$, are centered and square-integrable, i.e.\ $\E_{\tilde\P}[\epsilon_k] = 0$ and $\E_{\tilde\P}[|\epsilon_k|^{2}] < \infty$,  \label{ass_noisy_6}
\item[7.] $n$ is square-integrable, i.e.\ $\E_{\tilde\P}[|n|^{2}] < \infty$. \label{ass_noisy_7}
\end{enumerate}
\end{assumption}
\begin{rem}
    The relaxations on the assumption of observing $X_0$ completely discussed in \citep[Remark 2.3]{krach2022optimal} can equivalently be applied in this setting here.
\end{rem}
In this setting, the PD-NJ-ODE uses the noisy observations $O_{t_i}$ and $ \tilde O^{\leq \tau(t)}$ as inputs instead of $X_{t_i}$ and $\tilde X^{\leq \tau(t)}$. Moreover, we redefine the objective function as described before as 
\begin{align}
\Psi: \, &\mathbb{D} \to \R,\, Z \mapsto \Psi(Z) := \E_{\P\times\tilde\P}\left[ \frac{1}{n} \sum_{i=1}^n    \left\lvert M_i \odot (O_{t_i} - Z_{t_{i}-} ) \right\rvert_2^2 \right], \label{equ:Psi noisy obs} \\
\Phi : \, &\Theta \to \R, \, \theta \mapsto \Phi(\theta) := \Psi(Y^{\theta}(X)), \label{equ:Phi noisy obs}
\end{align}
and its Monte Carlo approximation accordingly.

\subsection{Convergence Theorem with Noisy Observations}\label{sec:Convergence Theorem with Noisy Observations}
In the setting defined in Section~\ref{sec:Setting with Noisy Observations}, Theorem~\ref{thm:1} holds equivalently as before. To prove this, we first need to adjust \cite[Lemma 4.2]{krach2022optimal} for this setting.

\begin{lem}\label{lem:L2 identity noisy obs setting}
For any $\mathbb{A}$-adapted process $Z$ it holds that
\begin{multline*}
\E_{\P \times\tilde\P}\left[\tfrac{1}{n} \sum_{i=1}^n \left\lvert M_{t_i} \odot ( O_{t_i} - Z_{t_i-} ) \right\rvert_2^2\right] \\
	= \E_{\P \times\tilde\P}\left[ \tfrac{1}{n}\sum_{i=1}^n \left\lvert M_{t_i} \odot ( O_{t_i} - \hat{X}_{t_i-} ) \right\rvert_2^2\right] + \E_{\P \times\tilde\P}\left[\tfrac{1}{n}\sum_{i=1}^n \left\lvert M_{t_i} \odot (  \hat{X}_{t_i-} - Z_{t_i-}) \right\rvert_2^2\right] .
\end{multline*}
\end{lem}

\begin{proof}
    First note that by Assumption~\ref{ass:noisy obs setting} point \ref{ass_main_3} we have that $X_{t_i} = X_{t_i-}$ almost surely and when defining $O_{t_i -} := X_{t_i -} + \epsilon_i$ we therefore also have that $O_{t_i } = O_{t_i -}$ almost surely.
Similarly as in  \cite[Lemma 4.2]{krach2022optimal}, we can derive for $\hat{O}_{t_i-} := \E_{\P \times\tilde\P}[O_{t_i -} \, | \, \mathcal{A}_{t_i -}]$ that
\begin{multline*}
\E_{\P \times\tilde\P}\left[\tfrac{1}{n}\sum_{i=1}^n \left\lvert M_{t_i} \odot ( O_{t_i-} - Z_{t_i-} ) \right\rvert_2^2\right] \\
	 = \E_{\P \times\tilde\P}\left[\tfrac{1}{n}\sum_{i=1}^n \left\lvert M_{t_i} \odot ( O_{t_i-} - \hat{O}_{t_i-} ) \right\rvert_2^2\right] + \E_{\P \times\tilde\P}\left[\tfrac{1}{n}\sum_{i=1}^n \left\lvert M_{t_i} \odot (  \hat{O}_{t_i-} - Z_{t_i-} ) \right\rvert_2^2\right].
\end{multline*}
To conclude the proof, it is enough to note that 
\begin{equation}\label{equ:O hat equal X hat}
    \hat{O}_{t_i-} = \hat{X}_{t_i-} + \E[\epsilon_i | \mathcal{A}_{t_i -}] = \hat{X}_{t_i-} + \E[\epsilon_i] = \hat{X}_{t_i-},
\end{equation}
using that $\epsilon_i$ has expectation $0$ and is independent of $\mathcal{A}_{t_i -}$. 
\end{proof}

In the following we sketch the proof of Theorem~\ref{thm:1} in the setting of noisy observations, by only outlining those parts of it that need to be changed in comparison with the original proof in \citep{krach2022optimal}. A full proof is given in Appendix~\ref{sec:Combining the Two Extensions With Full Proof}.

\begin{proof}[Sketch of Proof of Theorem~\ref{thm:1} for noisy observations]
    First, it follows directly from Lemma~\ref{lem:L2 identity noisy obs setting} that $\Psi(\hat{X}) = \min_{Z \in \mathbb{D}}\Psi(Z)$, i.e. that $\hat{X}$ is a minimizer of the redefined objective function $\Psi$.
    Secondly, again by Lemma~\ref{lem:L2 identity noisy obs setting}, we have for any process $Z \in \mathbb{D}$ that 
    \begin{equation*}
        \Psi(Z) = \Psi(\hat{X})  + \E_{\P \times\tilde\P}\left[\tfrac{1}{n}\sum_{i=1}^n \left\lvert M_{t_i} \odot (  \hat{X}_{t_i-} - Z_{t_i-}) \right\rvert_2^2\right] ,
    \end{equation*} 
    hence $\Psi(Z) > \Psi(\hat{X})$ follows as before if $Z$ is not indistinguishable from $\hat{X}$, meaning that $\hat{X}$ is the unique minimizer of $\Psi$.

    Under Assumption~\ref{ass:noisy obs setting} the approximation of the functions $f_j, F_j$ by bounded output feedforward neural networks works similarly as before, with the slight adjustment that their differences are now upper bounded by 
    $$U := 3 B\left((X_T^\star +1)^p + \sum_{i=0}^n |\epsilon_i| \right).$$ 
    Defining 
    \begin{equation*}
        c_m := c \, \varepsilon (T+1) d_X +  c (T+1) d_X U \left( \1_{\{ X_T^\star \geq 1/\varepsilon \}} + \1_{\{ n \geq  1/\varepsilon \}} + \1_{\{ \delta \leq \varepsilon \}} \right)
    \end{equation*}
    it follows that for all $t \in [0,T]$ we have $\left\lvert Y_t^{\theta_m^*} - \hat{X}_t  \right\rvert_2 \leq c_m$.
    Convergence of $\Phi(\theta_m^\star)$ to $\Psi(\hat{X})$ then follows similarly as before, when noting that by Assumption~\ref{ass:noisy obs setting}
    \begin{equation}\label{equ:integraility in noisy obs setting}
        \E \left[ (X_t^\star +1)^{2p} + \left(\sum_{i=0}^n |\epsilon_i|\right)^2 \right] \leq \E \left[ (X_t^\star +1)^{2p} \right] +  \E[n^2] \E \left[|\epsilon_0|^2 \right] < \infty,
    \end{equation}
    using Cauchy--Schwarz and that the $\epsilon_i$ are i.i.d.\ and independent of $n$ for the first step and the integrability of $X^\star$, $\epsilon_0$ and $n^2$ for the upper bound.
    Moreover, the convergence of $d_k(\hat{X}, Y^{\theta_m^\star}) \to 0$ follows as before.
    
    Finally, the remaining claims of the theorem (including the Monte Carlo convergence) also hold similarly as before upon replacing $X_{t_i}$ by $O_{t_i}$ and noting that the integrability of $\sup_\theta h(\theta, \xi_j)$ follows from \eqref{equ:integraility in noisy obs setting}.
\end{proof}


\subsection{More General Noise Structure \& Conditional Moments}
Revisiting the proof in Section~\ref{sec:Convergence Theorem with Noisy Observations}, we see that the noise terms need neither be independent nor centered.
If we assume that the conditional bias of the noise,
\[\beta_i(\tilde O^{\leq \tau(t)}) := \E[\epsilon_i | \mathcal{A}_{t_i -}],\]
is a \emph{known} function of the observations (using Doob-Dynkin Lemma \cite[Lemma~2]{taraldsen2018optimal} for its existence), then we can modify the objective function by subtracting it. This leads to
\begin{align}\label{equ:loss noisy obs generalised}
\Psi: \, &\mathbb{D} \to \R,\, Z \mapsto \Psi(Z) := \E_{\P\times\tilde\P}\left[ \frac{1}{n} \sum_{i=1}^n    \left\lvert M_i \odot \left(\left(O_{t_i} - \beta_i(\tilde O^{\leq t_{i-1}})\right) - Z_{t_{i}-} \right) \right\rvert_2^2 \right].
\end{align}
Revisiting \eqref{equ:O hat equal X hat}, which is the only part of the proof where we needed the noise terms to be centered, we see that
\begin{equation}\label{eq:lossNoiseKnownMean}
    \E\left[ \left(O_{t_i-} - \beta_i(\tilde O^{\leq t_{i-1}})\right) | \mathcal{A}_{t_i-} \right] = \hat{X}_{t_i-} + \E[\epsilon_i \mid \mathcal{A}_{t_i -}] - \beta_i = \hat{X}_{t_i-}.
\end{equation}
This implies that the statement of Lemma~\ref{lem:L2 identity noisy obs setting} holds equivalently under the reduced assumption of a known conditional bias function, when using the adjusted loss \eqref{equ:loss noisy obs generalised}.
Additionally assuming that
$\E\left[ \left(\sum_{i=0}^n |\epsilon_i| \right)^2 \right] < \infty$, the following result follows as before.
% \begin{cor}\label{cor:extended noisy obs thm}
%     In the setting described in this section, which is a generalisation of the setting in Section~\ref{sec:Setting with Noisy Observations}, Theorem~\ref{thm:1} holds equivalently as before.
% \end{cor}
\begin{cor}\label{cor:extended noisy obs thm}
    In the setting described in this sub-section (i.e., arbitrary known mean of the noise and no independence assumption on the noise), which is a generalisation of the setting in Section~\ref{sec:Setting with Noisy Observations}, Theorem~\ref{thm:1} holds equivalently as before when using the objective function~\eqref{eq:lossNoiseKnownMean}.
\end{cor}
The following remark explains how Corollary~\ref{cor:extended noisy obs thm} can be used to predict conditional higher moments (instead of only the conditional expectation) under certain assumptions. 
\begin{rem}
    This result makes it possible to compute the conditional moments of $X$ given the noisy observations, which doesn't work in the setting of Section~\ref{sec:Setting with Noisy Observations}.
    In particular, we consider observations $O_{t_i-} = X_{t_i-} + \epsilon_i$, where we assume that
    \begin{itemize}
        \item $\epsilon_i$ is independent of $\mathcal{A}_{t_i-}$,
        \item $\epsilon_i$ is conditionally independent of $X_{t_i-}$ given $\mathcal{A}_{t_i-}$
        \item and $\epsilon_i$ have known finite moments.
    \end{itemize} 
    Remark that Proposition~\ref{prop:conditional independence prop 6} implies that the first two assumptions are in particular satisfied if $\epsilon_i$ is independent of $\sigmab(\mathcal{A}_{t_i-}, X_{t_i-})$.
    The binomial theorem implies for any $q \in \N$
    \begin{equation*}
        O_{t_i-}^q = X_{t_i-}^q + \sum_{j=1}^q \binom{q}{j} X_{t_i-}^{q-j} \epsilon_i^j.\footnote{Note that $q$, $j$ and $q-j$ denote exponents here rather than superscripts.}
    \end{equation*}
    We interpret the entire sum as the observation noise and accordingly define the conditional bias of the observation noise of the $q$-th moment as
    \begin{equation*}
        \beta_i^q := \E\left[ \sum_{j=1}^q \binom{q}{j} X_{t_i-}^{q-j} \epsilon_i^j \mid \mathcal{A}_{t_i-} \right] = \sum_{j=1}^q \binom{q}{j} \E[ X_{t_i-}^{q-j} | \mathcal{A}_{t_i-} ] \E[\epsilon_i^j],
    \end{equation*}
    where we use the assumptions on $\epsilon_i$ together with Proposition~\ref{prop:conditional independence prop 5} for the second inequality.
    
    Then an inductive argument shows that $\beta_i^q$ is a known function of the observations, using the assumption that the moments of $\epsilon_i$ are known.
    Indeed, to compute $\beta_i^q$ the conditional expectations of smaller moments  $\E[ X_{t_i-}^{q-j} | \mathcal{A}_{t_i-} ]$ need to be computed, which can be done according to the induction hypothesis (note that the base case follows directly from Corollary~\ref{cor:extended noisy obs thm} and the assumptions on $\epsilon_i$). Therefore, Corollary~\ref{cor:extended noisy obs thm} implies that we can compute $\E[ X_{t_i-}^{q} | \mathcal{A}_{t_i-} ]$ (assuming that we reach the limit where the PD-NJ-ODE output equals the conditional expectation). In case of an exponential moment assumption $\E[ \exp(\lambda |X_{t_i-}|) ] < \infty$ for some $ \lambda > 0 $ we can therefore infer the conditional law of $X_{t_i-}$.
\end{rem}




\subsection{Examples}\label{sec:examples noisy obs}
In principle, all the examples presented in \citep[Section 7]{krach2022optimal} are valid examples for this setting when adding some type of i.i.d.\ observation noise  satisfying our assumptions, as e.g. Gaussian or uniform noise.
However, it is important to note that the (true) conditional expectation is not the same, since we now condition on the noisy observations $O_{t_i}$ instead of the original observations $X_{t_i}$.
Therefore, we give one explicit example where we compute the conditional expectation in the noisy observation setting.

    \subsubsection{Brownian Motion with Gaussian Observation Noise}
    \label{sec:Brownian Motion with Gaussian Observation Noise}
    Let $X := W$ be a standard Brownian motion and let $\epsilon_0=0$, $\epsilon_i \sim N(0, \sigma^2)$ for $i\geq 1$ be the i.i.d.\ noise terms for some $\sigma > 0$. Then $O_{t_i} = X_{t_i}+ \epsilon_i$ are the observations. Clearly, all integrability assumptions are satisfied by $X$ and $\epsilon_i$ (cf. \citep[Section 7.6]{krach2022optimal}). 
    To compute the true conditional expectations we first note that the independent increments property of the Brownian motion imply for $t_k \leq t < t_{k+1}$
    \begin{equation*}
        \E[X_t | \mathcal{A}_t] = \E[W_t - W_{t_k} | \mathcal{A}_{t_k} ] + \E[ W_{t_k} | \mathcal{A}_{t_k} ] = \E[ W_{t_k} | \mathcal{A}_{t_k} ] =  \E[ W_{t_k} | O_{t_1}, \dotsc, O_{t_{k}}],
    \end{equation*}
    and therefore, $f(s, \tau(t), \tilde O^{\leq \tau(t)} ) = 0$.
    Since $W$ is a Brownian motion and $\epsilon_i$ are independent i.i.d. Gaussian noise terms, we know that
    $$v := ({O}_{t_1}, \dotsc, {O}_{t_k}, W_{t_k})^\top \sim N(0, \Sigma)$$
    where
    $$  \Sigma  = \begin{pmatrix}
    \Sigma_{11} &  \Sigma_{12} \\
    \Sigma_{21} &  \Sigma_{22}
    \end{pmatrix} \in \R^{(k+1) \times (k+1)},$$
    with $\Sigma_{11} \in \R^{k\times k}$ and  $(\Sigma_{11})_{i,j} = \min(t_i,t_j) + \sigma^2 \1_{\{i=j\}}$, $\Sigma_{12}^\top = \Sigma_{21} = (t_1, \dotsc, t_k) \in \R^{1\times k}$ and $\Sigma_{22} = t_k$.
    Then the conditional distribution of $(W_{t_k} \, | \, O_{t_1}, \dotsc, O_{t_{k}})$ is again normal with mean $\hat \mu :=  \Sigma_{21}  \Sigma_{11}^{-1} (O_{t_1}, \dotsc, O_{t_{k}} )^\top $ and variance $\hat \Sigma :=  \Sigma_{22} -   \Sigma_{21}  \Sigma_{11}^{-1}  \Sigma_{12}$ \citep[Proposition~3.13]{Eaton2007Multi}. 
    In particular we have 
     \begin{equation*}
        \E[X_t | \mathcal{A}_t] = \E[ W_{t_k} | O_{t_1}, \dotsc, O_{t_{k}}] = \hat{\mu}.
    \end{equation*}
    
    

\section{PD-NJ-ODE with Dependence between \texorpdfstring{$X$}{X} and the Observation Framework}\label{sec:PD-NJ-ODE with Dependence between X and Observation Framework}
Recall that the observation mask process is given by $M$ and the underlying process by $X$. In this section, we remove the assumptions that the observation times are independent of $X$, and that $M$ is independent of the observation times and of $X$. In essence, the model is now defined on only one probability space $\P$ and no independence assumptions between the random variables are made. Instead, we need some weaker conditional independence assumptions to recover the results of Theorem~\ref{thm:1}.


\subsection{Intuition on independence assumptions}
In many real word applications the independence of the process $X$ and the observation framework (i.e., $(t_i)_{i\in \{1,\dots,n\}}$ and $M$) is heavily violated. Think for example of irregular measurements of patients' health-parameters taken at a hospital. Usually (expensive) %(in terms of costs or side-effects)
measurements are only taken if any information on the state $X$ of the patient hints that this measurement might be relevant. In practice different measurements are taken from different patients depending on observations of their state. This motivates the crucial importance of lifting the independence assumption for real-world applications.

However, even in this paper, we cannot completely remove any independence assumption; we still need conditional independence of the process $X$ and the observation framework given all past observations (as we will precisely formulate in item~\ref{ass_condindep} in Assumption~\ref{ass:dependence}). In theory this is a perfectly realistic assumption, \emph{if} we assume that \emph{every} piece of information the hospital gets from the patient is immediately logged as an (possibly incomplete and noisy) observation of $X$; in this case the hospital does not have any further information on $X$ rather than the observations, which makes decisions whether a measurement should be taken conditionally independent of $X$ given the past observations (thus fulfilling item~\ref{ass_condindep}).

However, in real-world practical applications one has to be careful, since it can for example happen that the hospital gets access to information on $X$ that is not officially logged as an observation; e.g., imagine the doctor measures the body temperature of a patient who told her that he feels feverish. If the doctor often does not log the observation of feverish feelings of her patients, but only logs their temperature-measurements, our model (and most other classical forecasting methods too) would learn a quite biased forecast of the body temperature. For example, in the extreme case that the body temperature is only measured if patients feel very feverish, most measurements will give a high body temperature, which leads our model to always predicting a high body temperature (i.e., the expected body temperature conditioned on feeling feverish and all other past observations) even if the patient does not feel feverish.
This problem can be mitigated in this example, if the hospital also logs whether the patient feels feverish in a further coordinate of $X$. Then one should first enter the feverish feeling of the patient as an observation and then enter the measurement of the body temperature as the next observation with a (slightly) later time stamp. In this case one would still have a problem with the conditional independence assumption if the patients mainly report feverish feelings rather than the absence of such a feeling. This could be fixed if the software automatically logs the absence of feverish feelings whenever they do not report a feverish feeling, if one can rely on the patients always reporting the intervals of feverish feelings. 
In such practical situations one should be aware that even our weakened, more realistic assumption of conditional independence is often not fully satisfied.


\subsection{Setting with Dependence}\label{sec:Setting with Dependence}
To allow for dependence, we only consider the probability measure $\P$ and define $X, n, K, t_i, \tau, M, \mathcal{A}_t$ similar as before, but all on the same probability space associated with $\P$.
For a random variable $Z$ and a family of sets $\mathcal{B}$ we use the natural notation for their smallest jointly generated sigma algebra $\boldsymbol{\sigma}(Z, \mathcal{B}) := \boldsymbol{\sigma}(Z) \lor \boldsymbol{\sigma}(\mathcal{B})$.
Then we need the following assumptions, where the main difference to the original Assumption~\ref{ass_main_1} is in item \ref{ass_condindep} (and \ref{ass_newass}).
\begin{assumption} \label{ass:dependence}
We assume that 
\begin{enumerate}
    \item $M_{0, j} = 1$ for all $1 \le j \le d_X$ (i.e. $X$ is completely observed at $0$) and $\abs{M_k}_1 > 0$ for every $1 \le k \le K$ $\P$-almost surely (i.e. at every observation time at least one coordinate is observed), \label{ass_dependence_1}
    
    \item the probability that any two observation times are closer than $\epsilon > 0$ converges to 0 when $\epsilon$ does, i.e. if $\delta(\omega) := \min_{0 \le i \le n(\omega)} \abs{t_{i + 1}(\omega) - t_i(\omega)}$ then $\lim_{\epsilon \to 0} \P(\delta < \epsilon) = 0$, \label{ass_dependence_2}
    
    \item almost surely $X$ is not observed at a jump, i.e. $\P(t_j \in \mathcal J \mid j \le n) = \P(\Delta X_{t_j} \neq 0 \mid j \le n) = 0$ for all $1 \le j \le K$, \label{ass_dependence_3}
    
    \item $F_j$ are continuous and differentiable in their first coordinate $t$ such that their partial derivatives with respect to $t$, denoted by $f_j$, are again continuous and there exists a $B > 0$ and $p \in \N$ such that for every $t \in [0, T]$ the functions $f_j, F_j$ are polynomially bounded in $X^\star$, i.e. 
    \begin{align*}
        \abs{F_j(\tau(t), \tau(t), \tilde{X}^{\le \tau(t)})} + \abs{f_j (t, \tau(t), \tilde{X}^{\le \tau(t)})} \le B(X^\star_t + 1)^p,
    \end{align*} \label{ass_polynomiallybounded}
    
    \item $X^\star$ is $L^{2p}$-integrable, i.e. $\expect{(X^\star_T)^{2p}} < \infty$, \label{ass_integrable}
    
    \item for every  $1 \le i \le n$, $X_{t_i-}$ is conditionally independent of $\sigmab(n, M_{t_i})$ given $\mathcal{A}_{t_i-}$, and \label{ass_condindep} 

    \item for all $1 \le k \le K$, $1 \le j \le d_X$ there is some $\eta_{k,j} >0$ such that $\P ( M_{k, j} = 1 \mid \sigmab(n, \mathcal{A}_{t_k-} )) > \eta_{k,j}$ (i.e. given the currently known information and $n$, for each coordinate the probability of observing it at the next observation time is positive). \label{ass_newass} 
\end{enumerate}
\end{assumption}
\begin{rem}
    The relaxations on the assumption of observing $X_0$ completely discussed in \citep[Remark 2.3]{krach2022optimal} can equivalently be applied here.
\end{rem}

We can use the original objective function \eqref{equ:Phi} and its Monte Carlo approximation \eqref{equ:appr loss function}.


\subsection{Convergence Theorem with Dependence}
In the Setting defined in Section~\ref{sec:Setting with Dependence}, Theorem~\ref{thm:1} holds equivalently. Before we can show this we need some additional results.
First, we prove an extension of the $L^2$-orthogonality result \citep[Proposition B.2]{herrera2021neural}.

\begin{prop}
\label{prop:newb2}
Let $(\Omega, \mathcal F, \P)$ be a fixed probability space, and $\mathcal A, \mathcal B$ be sub-$\sigma$-algebras such that $\mathcal B \subseteq \mathcal A \subseteq \mathcal F$. For some random variable $X \in L^2(\Omega, \mathcal F, \P)$ we define $\hat X := \cexpect{X}{\mathcal A}$. Then for every random variable $Z \in L^2(\Omega, \mathcal A, \P)$  with $\P(Z \neq \hat X) > 0$ we have
\begin{align}
    \cexpect{\abs{X - Z}^2_2}{\mathcal{B}} &= \cexpect{\abs{X - \hat X}^2_2}{\mathcal{B}} + \cexpect{\abs{Z - \hat X}^2_2}{\mathcal{B}} \\
    &\ge \cexpect{\abs{X - \hat X}^2_2}{\mathcal B},
\end{align}
with strict inequality with positive probability. 
\end{prop}

The proof is based on  in \cite[Theorem 5.1.8]{Durrett:2010:PTE:1869916}. We focus on the one-dimensional case, though this can easily be generalised to multiple dimensions via the 2-norm, as in \cite{herrera2021neural}.
\begin{proof}
We begin by expanding the left hand side.
\begin{multline*}
    \cexpect{\abs{X - Z }^2}{\mathcal B} = \cexpect{\abs{(X - \hat X) - (Z - \hat X)}^2}{\mathcal B} \\
    = \cexpect{\abs{X - \hat X}^2}{\mathcal{B}} + \cexpect{\abs{Z - \hat X}^2}{\mathcal B} - 2\cexpect{(X - \hat X)(Z - \hat X)}{\mathcal B}
\end{multline*}
We now analyse the cross term, which expands to $\cexpect{Z(X-\hat{X})}{\mathcal B} - \cexpect{\hat{X} (X - \hat X)}{\mathcal B}$. Focusing on the first term, we note that since $Z \in  L^2(\Omega, \mathcal A, \P)$, it holds that $Z \cexpect{X}{\mathcal A} = \cexpect{Z X}{\mathcal A}$. By taking expectation (conditioned on $\mathcal B$) of both sides, we get
\begin{align*}
    \cexpect{Z \hat X}{\mathcal B} = \cexpect{Z \cexpect{X}{\mathcal A}}{\mathcal B} = \cexpect{\cexpect{Z X}{\mathcal A}}{\mathcal B} = \cexpect{Z X}{\mathcal B}
\end{align*}
via the tower property, as $\mathcal B \subseteq \mathcal A$. Hence, $\cexpect{Z(X - \hat X)}{\mathcal B} = 0$.
Note that showing this only required that $Z$ is $\mathcal A$-measurable. Since this is also satisfied by $\hat X$ we directly have $\cexpect{\hat X(X - \hat X)}{\mathcal B} = 0$ and therefore the cross term vanishes and the equality follows.
The inequality holds since $\cexpect{\abs{Z - \hat X}^2_2}{\mathcal{B}}$ is non-negative. We therefore just need to show that the inequality is strict with positive probability. To this end, assume for the sake of contradiction that $\cexpect{\abs{Z - \hat X}^2_2}{\mathcal{B}} = 0$ $\P$-a.s, which implies (by the tower property) that $\expect{\abs{Z - \hat X}^2_2} = 0$. This is only possible if $Z = \hat X$ $\P$-a.s., which contradicts the assumption that $\P(Z \neq \hat X) > 0$.
\end{proof}

Next we re-prove \citep[Lemma~4.2]{krach2022optimal} under the relaxed assumptions.
\begin{lemma}
\label{lem:L2 identity dependence}
For any $\mathbb A$-adapted process $Z \in L^2(\Omega, \mathbb A, \P)$ it holds that 
\begin{multline*}
    \expect{\frac{1}{n} \sum_{i=1}^n \abs{ M_{t_i} \odot (X_{t_i} - Z_{t_i-})}^2_2} \\ 
    = \ \expect{\frac{1}{n} \sum_{i=1}^n \abs{ M_{t_i} \odot (X_{t_i} - \hat{X}_{t_i-}) }^2_2} + \expect{\frac{1}{n} \sum_{i=1}^n \abs{ M_{t_i} \odot (\hat{X}_{t_i-} - Z_{t_i-} )}^2_2}.
\end{multline*}
\end{lemma}
\begin{proof}
\begin{align*}
    \E & \left[ \frac{1}{n} \sum_{i=1}^n \abs{M_{t_i} \odot (X_{t_i-} - Z_{t_i-})}^2_2  \right]
    = \ \sum_{i = 1}^K \expect{\frac{1}{n} \ind{\{i \le n\}} \abs{M_{t_i} \odot (X_{t_i-} - Z_{t_i-})}^2_2} \\
    &= \ \sum_{i = 1}^K \expect{\cexpect{\frac{1}{n} \ind{\{i \le n\}} \abs{M_{t_i} \odot (X_{t_i-} - Z_{t_i-})}^2_2}{\sigmab(n, M_{t_i}, \mathcal A_{t_i-})}} \\
    &= \ \sum_{i = 1}^K \expect{\frac{1}{n} \ind{\{i \le n\}} \sum_{j=1}^{d_X} M_{t_i, j} \cexpect{ \abs{X_{t_i-,} - Z_{t_i-,j}}^2 }{\sigmab(n, M_{t_i}, \mathcal A_{t_i-})}} \\
    &= \ \sum_{i = 1}^K \expect{\frac{1}{n} \ind{\{i \le n\}} \sum_{j=1}^{d_X} M_{t_i, j} \cexpect{ \abs{X_{t_i-,} - Z_{t_i-,j}}^2 }{\mathcal A_{t_i-}}} .
\end{align*}
The first step follows by monotone convergence, the last by Lemma~\ref{claim:condindep4.2} below. Now we can conclude by first applying the equality from Proposition~\ref{prop:newb2} with $\mathcal A, \mathcal B = \mathcal A_{t_i-}$ and then reversing the above steps to arrive at the desired form. 
\end{proof}

\begin{lemma}\label{claim:condindep4.2}
Assume the context of Lemma~\ref{lem:L2 identity dependence}. Then for all $i, j$ it holds that
\begin{align*}
    \cexpect{ \abs{ X_{t_i-, j} - Z_{t_i-, j}}^2}{\sigmab(n, M_{t_i}, \mathcal A_{t_i-})} = \cexpect{ \abs{ X_{t_i-, j} - Z_{t_i-, j}}^2}{\mathcal A_{t_i-}}.
\end{align*}
\end{lemma}
\begin{proof}
We prove this by showing that if we expand the $\abs{X_{t_i-, j} - Z_{t_i-, j}}^2$ term, all three resulting terms can just be conditioned on $\mathcal A_{t_i-}$. This is a valid argument as $X$ and $Z$ are both assumed to be square-integrable. Note that squaring a random variable plays no role in the information given by the $\sigma$-algebra it is being conditioned on, and so we only need to analyse the terms $Z_{t_i-, j}$, $X_{t_i-, j}$, and $X_{t_i-, j} Z_{t_i-, j}$. See Appendix~\ref{app:condindep} for an overview of conditional independence and how it's used here. \\
\\
CASE $Z_{t_i-, j}$:
$Z$ is $\mathbb{A}$-adapted, and so $Z_{t_i-, j}$ is $\mathcal A_{t_i-}$-measurable. Thus we have 
\begin{align*}
    \cexpect{Z_{t_i-, j}}{\sigmab(n, M_{t_i}, \mathcal A_{t_i-})} = Z_{t_i-, j} = \cexpect{Z_{t_i-, j}}{\mathcal A_{t_i-}}
\end{align*} as desired. \\
\\
CASE $X_{t_i-, j}$:
Assumption~\ref{ass:dependence} point \ref{ass_condindep} implies that $X_{t_i-, j}$ is conditionally independent of $\sigmab(n, M_{t_i})$ given $\mathcal A_{t_i-}$. We therefore have by Proposition~\ref{prop:conditional independence prop 2}
\begin{align*}
    \cexpect{X_{t_i-, j}}{\sigmab(n, M_{t_i}, \mathcal A_{t_i-})} = \cexpect{X_{t_i-, j}}{\mathcal A_{t_i-}}.
\end{align*}
CASE $X_{t_i-, j} Z_{t_i-, j}$:
We combine the previous two ideas, namely that $Z_{t_i-, j}$ is $\mathcal A_{t_i-}$-measurable and that $X_{t_i-, j}$ is conditionally independent of $\sigmab(n, M_{t_i})$ given $\mathcal A_{t_i-}$. Thus 
\begin{multline*}
    \cexpect{X_{t_i-, j} Z_{t_i-, j}}{\sigmab(n, M_{t_i}, \mathcal A_{t_i-})}
    = \  Z_{t_i-, j} \cexpect{X_{t_i-, j} }{\sigmab(n, M_{t_i}, \mathcal A_{t_i-})} \\
    = \  Z_{t_i-, j} \cexpect{X_{t_i-, j} }{\mathcal A_{t_i-}}
    = \ \cexpect{Z_{t_i-, j} X_{t_i-, j} }{\mathcal A_{t_i-}}.
\end{multline*}
Combining these 3 cases proves the claim.
\end{proof}

Finally, the following lemma shows that Assumption~\ref{ass:dependence} point \ref{ass_newass} can replace the independence assumption for $M$ and $X$ in the context of the following lemma. 
\begin{lemma}\label{lem:ck}
There exists some $c_2(k) > 0$ such that for any $\mathbb A$-adapted process $Z \in L^2(\Omega, \mathbb A, \P)$ we have
\begin{align*}
    \expect{\ind{\{n \ge k\}} \abs{\hat{X}_{t_k-} - Z_{t_k-}}_1 } \le \frac{1}{c_2(k)} \expect{\ind{\{n \ge k\}} \abs{M_{t_k} \odot (\hat{X}_{t_k-} - Z_{t_k-})}_1 }.
\end{align*}
\end{lemma}
\begin{proof}
Assumption~\ref{ass:dependence} point \ref{ass_newass} states that $0 < \eta_{k,j} < \P ( M_{k, j} = 1 \mid \sigma(n, A_{t_k-} )) = \cexpect{M_{t_k, j}}{\sigmab(n, \mathcal{A}_{t_k-})} $ for all $k, j$.
Let $c_2 := c_2(k) := \min_{1 \le j \le d_X} \eta_{k,j}$, then $c_2 > 0$ and
\begin{multline*}
    \expect{ \ind{\{n \ge k\}} \abs{M_{t_k} \odot ( \hat{X}_{t_k-} - Z_{t_k-}) }_1 } 
    = \sum_{j=1}^{d_X} \expect{\ind{\{n \ge k\}} M_{t_k, j} \abs{ \hat{X}_{t_k-, j} - Z_{t_k-, j} } } \\
    = \sum_{j=1}^{d_X} \expect{\ind{\{n \ge k\}} \abs{ \hat{X}_{t_k-, j} - Z_{t_k-, j}} \cexpect{M_{t_k, j}}{\sigmab(n, \mathcal{A}_{t_k-})} } \\
    \ge c_2 \ \expect{\ind{\{n \ge k\}} \abs{ \hat{X}_{t_k-} - Z_{t_k-}}_1 },
\end{multline*}
where we used that $\ind{\{n \ge k\}} \abs{ \hat{X}_{t_k-, j} - Z_{t_k-, j}}$ is $\sigmab(n, \mathcal{A}_{t_k-})$-measurable in the second line and the definition of $c_2$ in the last line.
\end{proof}

Now we are ready to prove Theorem~\ref{thm:1} in the setting with dependence.
In the following we give again a sketch of the proof, by only outlining those parts of it that need to be changed in comparison with the original proof in \citep{krach2022optimal} and refer the interested reader to the full proof in Appendix~\ref{sec:Combining the Two Extensions With Full Proof}.

\begin{proof}[Sketch of Proof of Theorem~\ref{thm:1} with dependence]
As before it follows that $\hat{X}$ is a minimizer of $\Psi$. To show its uniqueness, we first note that 
\begin{equation}
 \E_{\P \times \tilde\P}\left[ \1_{\{n \geq k\}} \left\lvert \hat{X}_{t_k-} - Z_{t_k-}  \right\rvert_2 \right] \leq \frac{c_3}{c_2(k)} \E_{\P \times \tilde\P}\left[ \1_{\{n \geq k\}} \left\lvert M_{t_k} \odot ( \hat{X}_{t_k-} - Z_{t_k-} ) \right\rvert_2 \right]
\end{equation}
is implied as before when using Lemma~\ref{lem:ck} instead of the independence.
Then 
\begin{equation*}
\begin{split}
 \E_{\P \times\tilde\P}\left[\tfrac{1}{n}\sum_{i=1}^n \left\lvert M_{t_i} \odot (  \hat{X}_{t_i-} - Z_{t_i-}) \right\rvert_2^2\right]
	\geq  \left( \frac{c_2(k)}{c_0 c_1 c_3} \right)^2 d_k(\hat{X}, Z)^2 > 0,
\end{split}
\end{equation*} 
follows as before, implying the uniqueness of $\hat{X}$ as minimizer of $\Psi$.

The approximation of the functions $f_j$, $F_j$ also works as before. With this we have 
\begin{align*}
    \min_{Z \in \mathbb D} \Psi(Z) &\le \Phi (\theta^{\min}_m) \le \Phi(\theta^\ast_m) \\
    &= \ \expect{\frac{1}{n}\sum_{i = 1}^n \l(\abs{M_{t_i} \odot (X_{t_i} - Y^{\theta^\ast_m}_{t_i} ) }_2 + \abs{M_{t_i} \odot (Y^{\theta^\ast_m}_{t_i} - Y^{\theta^\ast_m}_{t_i-} ) }_2 \r)^2 } \\
    &\le \ \E \l[ \frac{1}{n}\sum_{i = 1}^n \l(\abs{M_{t_i} \odot (\hat{X}_{t_i} - Y^{\theta^\ast_m}_{t_i} ) }_2 + \abs{M_{t_i} \odot (Y^{\theta^\ast_m}_{t_i} - \hat{X}_{t_i}) \r. }_2 \r. \\
    &\l. \l. \hspace{60 pt} + \abs{M_{t_i} \odot ( \hat{X}_{t_i} - \hat{X}_{t_i-} ) }_2 + \abs{M_{t_i} \odot ( \hat{X}_{t_i-} - Y^{\theta^\ast_m}_{t_i-} ) }_2 \r)^2 \r] \\
    &\le \ \E \l[ \frac{1}{n}\sum_{i = 1}^n \l(\abs{ \hat{X}_{t_i} - Y^{\theta^\ast_m}_{t_i} }_2 + \abs{ Y^{\theta^\ast_m}_{t_i} - \hat{X}_{t_i} \r. }_2 \r.  \\ 
    &\l. \l. \hspace{60 pt} + \abs{M_{t_i} \odot ( \hat{X}_{t_i} - \hat{X}_{t_i-} ) }_2 + \abs{ \hat{X}_{t_i-} - Y^{\theta^\ast_m}_{t_i-} }_2 \r)^2 \r] \\
    &\le \ \expect{\frac{1}{n}\sum_{i = 1}^n \l( \abs{M_{t_i} \odot ( X_{t_i} - \hat{X}_{t_i-} ) }_2 + 3 c_m \r)^2 } \\
    &= \ \Psi(\hat X) + \expect{\frac{1}{n}\sum_{i = 1}^n \l(6 c_m \abs{M_{t_i} \odot ( X_{t_i} - \hat{X}_{t_i-} ) }_2 + 9 c^2_m \r) } \\
    &= \ \Psi(\hat X) + \expect{\frac{1}{n}\sum_{i = 1}^n 6 c_m \abs{M_{t_i} \odot ( X_{t_i} - \hat{X}_{t_i-} )}_2} + 9 \expect{c^2_m } \\
    &\le \ \Psi(\hat X) + 6 \expect{\frac{1}{n}\sum_{i = 1}^n c_m \abs{ X_{t_i} - \hat{X}_{t_i-} }_2} + 9 \expect{c^2_m } \\
    &\le \ \Psi(\hat X) + 6 \expect{\frac{1}{n}\sum_{i = 1}^n c_m \cdot 2 T B (X^\star_T + 1)^p } + 9 \expect{c^2_m } \\
    &= \ \Psi(\hat X) + 12 T B \expect{ c_m (X^\star_T + 1)^p } + 9 \expect{c^2_m },
\end{align*}
where the triangle inequality was used in the third line, and we use Assumption~\ref{ass:dependence} point \ref{ass_polynomiallybounded} (which implies that $|\hat X_t| \le TB(X^\star + 1)^p$ for all $t$) to construct a crude bound in the second to last line.
As in \cite{krach2022optimal}, dominated convergence can be used to show 
\begin{align*}
    \min_{\mathbb Z \in \mathbb D} \Psi(Z) = \Psi(\hat X) \le \Phi (\theta^{\min}_m) \le \Phi(\theta^\ast_m) \xrightarrow[]{m \to \infty} \min_{\mathbb Z \in \mathbb D} \Psi(Z),
\end{align*}
since $2p$ is again the largest power of $X^\star_T$ in both terms (remember $c_m$ has an $({X^\star_T})^p$ term). The convergence of $d_k(\hat{X}, Y^{\theta_m^\star}) \to 0$ follows as before, finishing the first part of the theorem. Finally, it is easy to see that  the proof of the convergence of the Monte Carlo approximation is not affected by our more general dependence assumptions such that also the second part of the theorem follows.
\end{proof}

\subsection{Examples}\label{sec:Examples - dependence setting}
Clearly, all the examples from \cite{krach2022optimal} are trivially valid for our generalised settings of Section~\ref{sec:Setting with Dependence} since points 6 \& 7 of Assumption~\ref{ass:dependence} are implied by independence.
Furthermore, we provide a relatively general class of examples in Section~\ref{sec:Class of Examples Incorporating Dependence} and extend two of the examples from \cite{krach2022optimal} to a setting where independence does not hold in Example~\ref{sec:Extending the homogeneous Poisson point process} and Example~\ref{exa:Black--Scholes with Dependent Observations}.

\subsubsection{Class of Examples Incorporating Dependence}\label{sec:Class of Examples Incorporating Dependence}
A main problem when constructing examples where the observation times $t_i$ have a dependence on the process $X$,  is that in general this will  also lead to $n$ having a dependence on $X$ (since $n$, as the amount of observations, increases when observations become more likely). This, in turn, might lead to a contradiction of Assumption~\ref{ass:dependence} point \ref{ass_condindep}. 
One way to circumvent this problem is to define $n$ conditionally independent of $X_{t_i-}$ given $\mathcal{A}_{t_i-}$ for all $i$ and to introduce a dummy variable, e.g. the time $t$ itself, that is observed at every $t_i$. Then we can control whether the process $X$ is observed at an observation time $t_i$ via the observation mask $M^X_i$ (denoting the mask for the $X$-part of the observation, i.e.\, without the dummy variable).
Since the dummy variable is always observed, the assumption that at least one coordinate of the mask has to be positive is trivially satisfied, such that $M^X_i = 0$ is admissible. In this way, the times at which $X$ is observed can depend on $X$ through the observation mask.
Therefore, the original problem is replaced by having an observation mask depending on $X$, which will be discussed in detail below.

When using a time grid on which the process $X$ is sampled, one concrete example of defining $n$  conditionally independent of $X_{t_i-}$ given $\mathcal{A}_{t_i-}$ is to use the grid points as observation times $t_i$ leading to $n$ being the number of grid points. In this case, both $n$ and the $t_i$ are deterministic, hence, they satisfy the conditional independence assumptions.

We need to ensure that Assumption~\ref{ass:dependence} points \ref{ass_condindep} and \ref{ass_newass} are satisfied (assuming that a dummy variable is observed at every observation time), when defining the observation mask.
A way to define $M^X_i$ such that point \ref{ass_condindep} is satisfied is to define it as a function of random variables that are $\mathcal{A}_{t_i-}$-measurable and random variables that are independent of $\sigmab(\mathcal{A}_{t_i-}, X_{t_i-}, n)$. 
In particular, let $M^X_i := g_i\left( (U^i_j)_{j \in J^i_1}, (V^i_j)_{j \in J^i_2} \right)$, where $g_i$ is a measurable function (mapping to $\{0,1\}^{d_X}$), $J^i_1, J^i_2 \subseteq \N$, $U^i_j$ is a $\mathcal{A}_{t_i-}$-measurable random variable and $V^i_j$ a random variable independent of $\sigmab(\mathcal{A}_{t_i-}, X_{t_i-}, n)$ for all $j$ in $J^i_1$ and $J^i_2$ respectively.
By Proposition~\ref{prop:conditional independence prop 3} we need to show that for any measurable function $\phi$ we have $\cexpect{\phi(X_{t_i-})}{\sigmab(n, M_i, \mathcal{A}_{t_i-})} = \cexpect{\phi(X_{t_i-})}{ \mathcal{A}_{t_i-} }$.
Indeed, $M_i$ is $\sigmab(\mathcal{A}_{t_i-}, (V^i_j)_{j \in J^i_2})$-measurable  by construction, therefore, we have for such a $\phi$ that
\begin{equation*}
    \cexpect{\phi(X_{t_i-})}{\sigmab(n, M_i, \mathcal{A}_{t_i-})} 
    = \cexpect{ \cexpect{\phi(X_{t_i-})}{\sigmab(n, (V^i_j)_{j \in J_2^i}, \mathcal{A}_{t_i-})} }{\sigmab(n, M_i, \mathcal{A}_{t_i-})},
\end{equation*}
by the tower property.
On the other hand, 
\begin{align*}
    \cexpect{\phi(X_{t_i-})}{\sigmab(n, (V^i_j)_{j \in J_2^i}, \mathcal{A}_{t_i-})} 
    = \cexpect{\phi(X_{t_i-})}{\sigmab(n, \mathcal{A}_{t_i-})} 
    = \cexpect{\phi(X_{t_i-})}{\mathcal{A}_{t_i-} },
\end{align*}
using the independence of $V_j^i$ together with Corollary~\ref{cor:conditional independence prop 1.2}  in the first equality and $n$ being conditionally independent together with Proposition~\ref{prop:conditional independence prop 3} in the second equality.
Together, this implies the claim and therefore Assumption~\ref{ass:dependence} point \ref{ass_condindep}.

For Assumption~\ref{ass:dependence} point \ref{ass_newass}, we note that by \cite[Lemma~6.2.1]{Durrett:2010:PTE:1869916} we have
\begin{align*}
    \P ( M_{k} = 1 \mid \sigmab(n, A_{t_k-} )) 
    &= \cexpect{M_{t_k}}{\sigmab(n, \mathcal{A}_{t_k-})} \\
    &= \cexpect{g_k\left( (U^k_j)_{j \in J^k_1}, (V^k_j)_{j \in J^k_2} \right)}{\sigmab(n, \mathcal{A}_{t_k-})} \\
    &= \tilde{g}_k((U^k_j)_{j \in J^k_1}),
\end{align*}
for $\tilde{g}_k((u^k_j)_{j \in J^k_1}) := \E\left[g_k\left( (u^k_j)_{j \in J^k_1}, (V^k_j)_{j \in J^k_2} \right)\right]$.
Hence, we need to define the $g_k$ and $V_j^k$ such that $\tilde{g}_k > \eta_k$ (coordinate-wise) for some $\eta_k >0$.


\begin{example}[Homogeneous Poisson Point Process with Dependent Observations]\label{sec:Extending the homogeneous Poisson point process}
We use a homogeneous Poisson point process $X = N^\lambda$  \cite[Section 7.4]{krach2022optimal} and defined observations depending on its value following the instructions above. To begin with, we define the observation times to be the grid points of the sampling grid of the process and $n$ accordingly to be the number of grid points. To permit observation times of the process depending on the process, we use time as dummy variable that is always observed and define the observation mask for $X$ as 
\begin{align*}
    M^X_i = 
    \begin{cases}
         \ind{\{x_i \ge \lambda t_{i-1} \}}, \ x_i \sim \mathcal{N}(N^\lambda_{t_{i-1}}, \sigma^2) &\quad \text{if $M^X_{i-1} = 1$}, \\
         u_i \sim \operatorname{Bernoulli}(p) &\quad \text{if $M^X_{i-1} = 0$}.
    \end{cases}
\end{align*}
for some $\sigma > 0$ and $p \in (0,1)$. Thus the process is more likely to be observed if the previous value was observed and was large (note $\expect{N^\lambda_{t_{j}}} = \lambda t_j$ for all $j$). Otherwise the mask value is sampled from a Bernoulli distribution. 

To satisfy Assumption~\ref{ass:dependence} point 1, we define $M_0 := 1$. 
To see that Assumption \ref{ass:dependence} point \ref{ass_condindep} holds, let $V^i_1 \sim \operatorname{Bernoulli}(p)$ and $V^i_2 \sim \mathcal N(0, \sigma^2)$ be independent random variables independent of $\sigmab(\mathcal A_{t_i-}, N^\lambda_{t_i-})$. Then 
$$M^X_i = \ind{\{M^X_{i-1} = 1\}} \ind{\{N^\lambda_{t_{i-1}} + V^i_2 \ge \lambda t_{i-1}\}} + V^i_1 \ind{\{M^X_{i-1} = 0\}} =: g_i(M^X_{i-1}, N^\lambda_{t_{i-1}}, t_{i-1}, V^i_1, V^i_2),$$
and the claim follows as explained above.
Moreover, Assumption~\ref{ass:dependence} point \ref{ass_newass} is satisfied because 
\begin{align*}
    \Tilde{g}_i(M^X_{i-1}, N^\lambda_{t_{i-1}}, t_{i-1}) 
    &= \ind{\{M^X_{i-1} = 1\}} \, \E\left[ \ind{\{a + V^i_2 \ge \lambda b \}} \right] \Big\vert_{(a,b)=\left(N^\lambda_{t_{i-1}}, t_{i-1}\right)} + \ind{\{M^X_{i-1} = 0\}} \, \E\left[ V^i_1 \right] \\
    & \geq \min\left( \P\left[ V^i_2 \ge \lambda b - a \right] \Big\vert_{(a,b)=\left(N^\lambda_{t_{i-1}}, t_{i-1}\right)}, p \right) \\
    & \geq  \min\left( \P\left[ V^i_2 \ge \lambda T \right], p \right) =: \eta_i > 0,
\end{align*}
using that $\lambda t_{i-1} - N^\lambda_{t_{i-1}} \leq \lambda T$, since $N^\lambda \geq 0$, $t_{i-1} \leq T$ and $\lambda > 0$, in the last line.
\end{example}


\begin{rem}
    We note that the choice $X = N^\lambda$ is only explicitly used for the fact that $N^\lambda$ is lower bounded (by $0$). Hence, the Poisson point process could be replaced by any other process that is lower bounded and satisfies Assumption~\ref{ass:dependence} points \ref{ass_polynomiallybounded} \& \ref{ass_integrable}.
\end{rem}


\begin{example}[Black--Scholes with Dependent Observations]
\label{exa:Black--Scholes with Dependent Observations}
    We use a $1$-dimensional Black--Scholes process (geometric Brownian motion) \cite[Example~7.3]{krach2022optimal} with constant drift and volatility $\mu, \sigma$ starting at $x_0$ and again define observations depending on its value. The observation times and $n$ are defined as in the previous example and we define the actual times when $X$ is observed via the mask $M^X$. We set $M^X_0=1$. Moreover, we define the last time before $t$ at which $X$ was observed as $\tau_X(t) = \max\{ t_i \, | \, t_i < t, M^X_i = 1\}$.
    Let $V^i_1 \sim \operatorname{Bernoulli}\left(\frac{t_i - t_{i-1}}{t_i - \tau_X(t_i)}\right)$, $V^i_2 \sim \mathcal{N}(0, \eta^2)$ and $V^i_3 \sim \operatorname{Bernoulli}\left(p\right)$ be independent random variables for some $\eta > 0$ and $p \in (0,1)$.
    Then we define 
    \begin{equation*}
        M^X_i := V^i_1 \, \1_{\{ X_{\tau_X(t_i)} + V^i_2 \geq x_0 e^{\mu t_i} \} } + (1-V^i_1) \, V^i_3,
    \end{equation*}
    for all $i \geq 1$. In particular, if $X$ was observed at the previous observation time $V^i_1=1$ and the probability of observing $X$ increases with the size of the last observation of $X$ (compared to the current expected value of $X$ at the current time). The further the last observation of $X$ is in the past, the more likely $V_1^i = 0$ in which case $X$ is observed with probability $p$.
    Upon noting that the $t_i$ are deterministic, it follows as in the previous example that Assumption~\ref{ass:dependence} point \ref{ass_condindep} and \ref{ass_newass} are satisfied.
\end{example}



\section{Experiments}\label{sec:Experiments}
The code with all new experiments and those from  \citep{krach2022optimal} is available at \url{https://github.com/FlorianKrach/PD-NJODE}. Further details about the experiments can be found in Appendix~\ref{sec:Experimental Details}. In particular, in Appendix~\ref{sec:Differences between the Implementation and the Theoretical Description of the PD-NJ-ODE} we give details on  the slightly deviation of the practical implementation from the theoretical description.

As in \citep{krach2022optimal} we use the following evaluation metric to quantify and compare the training success.
\begin{equation*}\label{equ:evaluation metric}
\operatorname{eval}(\hat X, Y^{\theta}) := \frac{1}{N_2} \sum_{j=1}^{N_2}  \frac{1}{\kappa+1} \sum_{i=0}^{\kappa} \left(  \hat X_{\frac{i T}{\kappa}}^{(j)} - Y_{\frac{i T}{\kappa}}^{\theta, j } \right)^2,
\end{equation*}
where the outer sum runs over the test set of size $N_2$ and the inner sum runs over the equidistant grid points on the time interval $[0,T]$.

\subsection{Noisy Observations -- Brownian Motion with Gaussian Observation Noise}
We test the PD-NJ-ODE trained with the loss function adapted to noisy observations \eqref{equ:Psi noisy obs} in the context of Section~\ref{sec:Brownian Motion with Gaussian Observation Noise}. In particular, $X$ is a Brownian motion and we assume to have observation noise of a centered normal distribution with standard deviation $\sigma=0.5$. Moreover, we compare these results to using the original loss function \eqref{equ:Psi} with the noisy observation. PD-NJ-ODE adapted to noisy observations achieves a minimal evaluation metric of $1.1 \cdot 10^{-3}$ while using the original loss function leads to a nearly $20$ times larger evaluation metric of $1.9 \cdot 10^{-2}$. Moreover, in Figure~\ref{fig:NoisyObs} we see that the noise-adapted method learns to correctly jump when new observations become available, while the original method jumps to the noisy observations and afterwards tries to get close to the true conditional expectation quickly. We note that this is the expected behaviour.

% Figure environment removed



\subsection{Dependent Observation Framework -- a Black--Scholes Example}
Based on Example~\ref{exa:Black--Scholes with Dependent Observations} we train a PD-NJ-ODE on a $1$-dimensional geometric Brownian motion with drift $\mu=2$ and volatility $\sigma=0.3$ and with observation probability depending on the last observation of $X$, the time since the last observation and on independent random variables $V_{\{2,3\}}^i$ for which we use the parameters $\eta=3$ and $p=0.1$.
As our theoretical result suggests, our model learns to predict the conditional expectation well with a minimal evaluation metric of $1.1 \cdot 10^{-3}$ which is also visible in Figure~\ref{fig:DepObs}. 

% Figure environment removed



\section{Conclusion}
In this work we broadened the applicability of the PD-NJ-ODE of \cite{krach2022optimal} by extending the theoretical foundation to allow for noisy observations and for the observation framework (i.e., observation times and masks) to depend on previous information.
In particular, we showed that any centered i.i.d.\ observation noise satisfying some integrability conditions can be dealt with by switching to the noise-adapted objective function \eqref{equ:Psi noisy obs}. Moreover, we showed that the proof of the main result can be retained when lifting the independence assumption between the process $X$ and the observation framework, by extensively working with conditional independence. 
Finally, we provided experiments showing empirically that the PD-NJ-ODE works well in those extended settings.






%%% ==================================================================
\if\addackn1
	\section*{Acknowledgement}
	
\fi

%%% ==================================================================
\bibliographystyle{iclr2021_conference}
\bibliography{references.bib}



%%% ==================================================================
\if\inclapp1
	\clearpage
	\appendix

\section{Conditional independence}
\label{app:condindep}
Let $(\Omega, \mathcal F, \P)$ be a probability space, $\mathcal G, \mathcal H \subseteq \mathcal F$ be two sigma-algebras and let $X$ be a random variable.

The assumption that $X$ is independent of $\mathcal{G}$ leads to the natural but incorrect conclusion that $\cexpect{X}{\sigmab(\mathcal G, \mathcal H)} = \cexpect{X}{\mathcal H}$. For this to hold, we actually need a stronger assumption as in the following proposition that is due to \cite{stackexchange_hansen, stackexchange_yoo}.

\begin{prop}\label{prop:conditional independence prop 1}
    If $\sigmab(X, \mathcal H)$ is independent of $\mathcal G$ then $\cexpect{X}{\sigmab(\mathcal G, \mathcal H)} = \cexpect{X}{\mathcal H}$.
\end{prop}

\begin{proof}
We prove the desired equality in the context of basic measure theory. We first assume that $X$ is integrable on $\sigmab(\mathcal G, \mathcal H)$, since otherwise neither of the expectations are valid. Then we recall that conditional expectation is simply a random variable $Z$ that satisfies the following three properties
\begin{enumerate}
    \item $Z$ is $\sigmab(\mathcal G, \mathcal H)$-measurable,
    \item $Z$ is $\sigmab(\mathcal G, \mathcal H)$-integrable,
    \item $\int_{A} X d \P = \int_{A} Z d \P$ for all $A \in \sigmab(\mathcal G, \mathcal H)$.
\end{enumerate}
By the definition of the conditional expectation we know that $\cexpect{X}{\sigmab(\mathcal G, \mathcal H)}$  satisfies these properties. To show the claim, it is therefore enough to prove that also $Z = \cexpect{X}{\mathcal H}$ satisfies them.
The first two follow trivially since $\mathcal H \subseteq \sigmab(\mathcal G, \mathcal H)$. For the third, we note that it is enough to consider the $\cap$-stable generator $\{ A \cap B \mid A \in \mathcal G, B \in \mathcal H\}$ of $\sigmab(\mathcal G, \mathcal H)$ and show that
\begin{align*}
    \int_{A \cap B} X d \P = \int_{A \cap B} Z d \P \quad   \forall A \in \mathcal G, B \in \mathcal H,
\end{align*}
or equivalently
\begin{align*}
    \expect{X \cdot \mathbbm{1}_{A \cap B}} = \expect{\cexpect{X}{\mathcal H} \cdot \mathbbm{1}_{A \cap B}} \ \ \  \ \forall A \in \mathcal G, B \in \mathcal H.
\end{align*}

Note  that $\sigmab(X, \mathcal H)$ being independent of $\mathcal G$ implies that $\mathcal H$ is independent of $\mathcal G$. Therefore,
\begin{align*}
    \expect{X \cdot \mathbbm{1}_{A \cap B}} &= \ \expect{X \cdot \mathbbm{1}_B \mathbbm{1}_A} \\
    &= \ \expect{X \cdot \mathbbm{1}_B} \expect{\mathbbm{1}_A} \tag{$\sigmab(X, \mathcal H)$ indep. of $\mathcal G$} \\ % X is X-mbl (=> also XvH mbl), 1_B is H-mbl (=> also XvH mbl.). Prop 1.7 in prob skript says product of r.v.s also a r.v. so take the inverses to get sigma algebras of them and end up with XvH anyway. Then we have this is indep of G (which 1_A is in)
    &= \ \expect{\cexpect{X \cdot \mathbbm{1}_{B}}{\mathcal H}} \cdot \expect{\mathbbm{1}_A} \tag{tower property} \\
    &= \ \expect{\cexpect{X \cdot \mathbbm{1}_{B}}{\mathcal H} \cdot \mathbbm{1}_A} \tag{$\mathcal H$ indep. of $\mathcal G$} \\
    &= \ \expect{\cexpect{X}{\mathcal H} \cdot \mathbbm{1}_{A} \mathbbm{1}_{B}} \tag{$\mathbbm{1}_{B}$ is $\mathcal{H}$-measurable} \\
    &= \ \expect{\cexpect{X}{\mathcal H} \cdot \mathbbm{1}_{A \cap B}},
\end{align*}
completing the proof.
\end{proof}

\begin{cor}\label{cor:conditional independence prop 1.2}
    If $\sigmab(X, \mathcal H)$ is independent of $\mathcal G$ then $\cexpect{\phi(X)}{\sigmab(\mathcal G, \mathcal H)} = \cexpect{\phi(X)}{\mathcal H}$ for all measurable and integrable functions $\phi$.
\end{cor}
\begin{proof}
    This follows directly from Proposition~\ref{prop:conditional independence prop 1} upon replacing $X$ by $\phi(X)$ and noting that $\sigmab(\phi(X), \mathcal H) \subseteq \sigmab(X, \mathcal H)$, implying the needed condition.
\end{proof}

It should now be apparent that the assumption that $X$ and $\mathcal G$ are independent is insufficient if we want to show $\cexpect{X}{\sigmab(\mathcal G, \mathcal H)} = \cexpect{X}{\mathcal H}$ (a counterexample is provided in \citep{stackexchange_hansen}).
However, we can actually make a weaker assumption (though still stronger than the assumption that $X$ and $\mathcal G$ are independent) to attain the same result, as shown in the next proposition.

\begin{definition}
    $X$ and $\mathcal G$ are conditionally independent given $\mathcal H$ if for all $x \in \sigmab(X)$, $A \in \mathcal G$, 
\begin{align*}
    \cexpect{\ind{x} \ind{A}}{\mathcal H} = \cexpect{\ind{x}}{\mathcal H} \cexpect{\ind{A}}{\mathcal H}.
\end{align*}
\end{definition}

\begin{prop}\label{prop:conditional independence prop 2}
    If $X$ is conditionally independent of $\mathcal G$ given $\mathcal H$ then $\cexpect{X}{\sigmab(\mathcal G, \mathcal H)} = \cexpect{X}{\mathcal H}$.
\end{prop}

\begin{proof}
Following the previous proof it is clear that we only need to show 
$$\expect{X \cdot \mathbbm{1}_{A \cap B}} = \expect{\cexpect{X}{\mathcal H} \cdot \mathbbm{1}_{A \cap B}}$$ 
for all $A \in \mathcal{G}$ and $B \in \mathcal{H}$. We do this by measure-theoretic induction (as in \cite[Proof of Theorem 1.6.9]{Durrett:2010:PTE:1869916}), in particular, we proceed in a four-part case distinction of $X$. In the first case, assume $X$ is an indicator function, i.e. $X = \ind{x}$ for some $x \in \mathcal{F}$. Then 
\begin{align*}
    \expect{X \cdot \ind{A \cap B}} &= \expect{\ind{x} \ind{A \cap B}} \\
    &= \ \expect{\cexpect{\ind{x} \ind{A \cap B}}{\mathcal H}} \tag{tower property} \\
    &= \ \expect{\cexpect{\ind{x} \ind{A}}{\mathcal H} \ind{B} } \tag{$B \in \mathcal H$} \\
    &= \ \expect{\cexpect{\ind{x}}{\mathcal H} \cexpect{\ind{A}}{\mathcal H} \ind{B} } \tag{cond. indep.} \\
    &= \ \expect{ \cexpect{ \cexpect{\ind{x}}{\mathcal H} \cdot \ind{A}}{\mathcal H} \ind{B} } \tag{$\cexpect{\ind{x}}{\mathcal H}$ $\mathcal H$-mbl.} \\
    &= \ \expect{ \cexpect{ \cexpect{\ind{x}}{\mathcal H} \cdot \ind{A \cap B}}{\mathcal H} } \tag{$B \in \mathcal H$} \\
    &= \ \expect{ \cexpect{\ind{x}}{\mathcal H} \cdot \ind{A \cap B}} \tag{tower property} \\
    &= \ \expect{ \cexpect{X }{\mathcal H} \cdot \ind{A \cap B}}.
\end{align*}
Thus in the most basic case we have the required property. In the second case we let $X = \sum_{i = 1}^n c_i \ind{x_i}$ be a finite weighted sum of indicator random variables, where $c_i \in \R$ and $x_i \in \mathcal{F}$. The result of the first case combined with linearity of expectation immediately shows that the property holds for $X$ in this form too. 

In the third case, we assume $X$ is some non-negative function. We define a random variable $X_k$ that is simple and such that $X_k \uparrow X$ as $k \to \infty$. For example, we can take $X_k = \sum_{i=0}^{n2^n - 1} \frac{i}{2^n} \ind{} \{\frac{i}{2^n} \le X < \frac{i+1}{2^n}\} + n \ind{n \le X}$, as in \cite[Proof of Theorem 1.6.9]{Durrett:2010:PTE:1869916}. Then, by monotone convergence and the previous case, the property also holds when $X$ is an arbitrary non-negative function. 

Finally, in the fourth case, we let $X$ be an arbitrary integrable function. Then we can write $X = X^+ - X^-$ where $f^+(x) := \max\{0, f(x) \}$ and \\ $f^-(x) := \min\{0, f(x)\}$. Integrability of $X$ means that $X^+$ and $X^-$ are themselves integrable. We can use linearity of expectation and the previous cases to conclude that, in this general setting, the property still holds. This concludes the proof by measure-theoretic induction. 
\end{proof}

\begin{prop}\label{prop:conditional independence prop 3}
    $X$ is conditionally independent of $\mathcal G$ given $\mathcal H$ if and only if $\cexpect{\phi(X)}{\sigmab(\mathcal G, \mathcal H)} = \cexpect{\phi(X)}{\mathcal H}$ for all measurable and integrable functions $\phi$.
\end{prop}
\begin{proof}
    The ``$\Rightarrow$'' direction is a simple corollary of Proposition~\ref{prop:conditional independence prop 2} that follows with the same proof upon replacing $X$ by $\phi(X)$ and noticing that $\phi(X)$ is a $\sigmab(X)$-measurable random variable.

    The ``$\Leftarrow$'' direction is also easy to see. Let $x \in \sigmab(X)$, $A \in \mathcal G$ and define $\phi$ such that $\phi(X) = \ind{x}$.
    Then we have 
    \begin{align*}
        \cexpect{\ind{x} \ind{A}}{\mathcal H} 
        &= \cexpect{\cexpect{\ind{x} \ind{A}}{\sigmab(\mathcal{G}, \mathcal{H})}}{\mathcal{H}} \tag{tower property}\\
        &= \cexpect{\cexpect{\ind{x} }{\sigmab(\mathcal{G}, \mathcal{H})} \ind{A} }{\mathcal{H}} \tag{$A \in \mathcal G \subseteq \sigmab(\mathcal{G}, \mathcal{H})$}\\
        &= \cexpect{\cexpect{\ind{x} }{\mathcal{H}} \ind{A} }{\mathcal{H}} \tag{RHS of claim with $\phi(X) = \ind{x}$}\\
        &= \cexpect{\ind{x} }{\mathcal{H}} \cexpect{ \ind{A} }{\mathcal{H}} \tag{$\cexpect{\ind{x} }{\mathcal{H}}$ $\mathcal{H}$-mbl.}
    \end{align*}
    This completes the proof.
\end{proof}

\begin{prop}\label{prop:conditional independence prop 4}
    If $X$ is conditionally independent of $\mathcal G$ given $\mathcal H$, and $Y$ is independent of $\sigmab(X, \mathcal{G}, \mathcal{H})$ then for any measurable and integrable function $f$ also $f(X,Y)$ is conditionally independent of $\mathcal G$ given $\mathcal H$.
\end{prop}
\begin{proof}
    By Proposition~\ref{prop:conditional independence prop 3} we have to show that for any measurable $\phi$ we have $\cexpect{\phi(f(X,Y))}{\sigmab(\mathcal G, \mathcal H)} = \cexpect{\phi(f(X,Y))}{\mathcal H}$.
    First note that it is enough to show this for $\phi$ being the identity, since both $\phi$ and $f$ are arbitrary measurable  and integrable functions.
    Then we have  for $g(x) := \E_Y[{f(x,Y)}]$, which is a measurable and integrable function again, that
    \begin{align*}
        \cexpect{f(X,Y)}{\sigmab(\mathcal G, \mathcal H)} 
        &= \cexpect{\cexpect{f(X,Y)}{\sigmab(X, \mathcal G, \mathcal H)}}{\sigmab(\mathcal G, \mathcal H)} \tag{tower property}\\
        &= \cexpect{g(X)}{\sigmab(\mathcal G, \mathcal H)} \tag{\cite[Lemma~6.2.1]{Durrett:2010:PTE:1869916}} \\
        &= \cexpect{g(X)}{\mathcal H} \tag{Proposition~\ref{prop:conditional independence prop 3}}, \\
        &= \cexpect{f(X,Y)}{\mathcal H} \tag{reversing step 1 \& 2}, 
    \end{align*}
    as wanted, proving the claim.
\end{proof}

\begin{prop}\label{prop:conditional independence prop 5}
    If X is conditionally independent of Y given $\mathcal{H}$ then $$\E[XY \mid \mathcal{H}] = \E[X \mid \mathcal{H}] \, \E[Y \mid \mathcal{H}].$$
\end{prop}
\begin{proof}
    From Proposition~\ref{prop:conditional independence prop 2} we know that $\E[X \mid \sigmab(Y, \mathcal{H})] = \E[X \mid \mathcal{H}] $.
    Therefore, 
    \begin{align*}
        \cexpect{XY}{\mathcal H} 
        &= \cexpect{\cexpect{XY}{\sigmab(Y, \mathcal{H})}}{\mathcal{H}} \tag{tower property}\\
        &= \cexpect{\cexpect{X }{\sigmab(Y, \mathcal{H})} Y }{\mathcal{H}} \tag{$Y \in  \sigmab(Y, \mathcal{H})$}\\
        &= \cexpect{\cexpect{X }{\mathcal{H}} Y }{\mathcal{H}} \tag{Proposition~\ref{prop:conditional independence prop 2}}\\
        &= \cexpect{X }{\mathcal{H}} \cexpect{ Y }{\mathcal{H}} \tag{$\cexpect{X }{\mathcal{H}}$ $\mathcal{H}$-mbl.},
    \end{align*}
    as wanted.
\end{proof}

\begin{prop}\label{prop:conditional independence prop 6}
     If $X$ is independent of $\sigmab(\mathcal G, \mathcal H)$ then $X$ is conditionally independent of $\mathcal G$ given $\mathcal H$.
\end{prop}
\begin{proof}
    Follows directly from Proposition~\ref{prop:conditional independence prop 3}.
\end{proof}

\section{Combining the Two Extensions With Full Proof}
\label{sec:Combining the Two Extensions With Full Proof}
In this section we prove convergence of the PD-NJ-ODE to the optimal prediction in the most general setting that allows for non-Markovian processes with irregular incomplete noisy observations, where dependence between the observation framework and the process is possible.

\subsection{Setting}\label{sec:Setting both ext}
As in Section~\ref{sec:Setting with Dependence} we consider only the probability space $(\Omega, \F, \mathbb{F}, \P )$ on which we define  $X, n, K, t_i, \tau, M$.  As in Section~\ref{sec:Setting with Noisy Observations}, we additionally define the observation noise $(\epsilon_k)_{0 \leq k \leq K}$, the noisy observations $O_{t_k} := X_{t_k} + \epsilon_k$ for $0 \leq k \leq n$, the filtration of the currently available information via
\begin{equation*}
\mathcal{A}_t := \boldsymbol{\sigma}\left(O_{t_i, j}, t_i, M_{t_i} | t_i \leq t,\, j \in \{1 \leq l \leq d_X | M_{t_i, l} = 1  \} \right),
\end{equation*} 
$\tilde{O}^{\leq t}$ and the functions $F_j$ such that $\hat X_{t,j} = F_j(t, \tau(t), \tilde O^{\leq \tau(t)})$.
Then we make the following assumptions.
\begin{assumption} \label{ass:noise&dependence}
We assume that 
\begin{enumerate}
    \item $M_{0, j} = 1$ for all $1 \le j \le d_X$ (i.e. $X$ is completely observed at $0$) and $\abs{M_k}_1 > 0$ for every $1 \le k \le K$ $\P$-almost surely (i.e. at every observation time at least one coordinate is observed), \label{ass_both_1}
    
    \item the probability that any two observation times are closer than $\epsilon > 0$ converges to 0 when $\epsilon$ does, i.e. if $\delta(\omega) := \min_{0 \le i \le n(\omega)} \abs{t_{i + 1}(\omega) - t_i(\omega)}$ then $\lim_{\epsilon \to 0} \P(\delta < \epsilon) = 0$,\label{ass_both_2}
    
    \item almost surely $X$ is not observed at a jump, i.e. $\P(t_j \in \mathcal J \mid j \le n) = \P(\Delta X_{t_j} \neq 0 \mid j \le n) = 0$ for all $1 \le j \le K$, \label{ass_both_3}
    
    \item $F_j$ are  continuous and differentiable in their first coordinate $t$ such that their partial derivatives with respect to $t$, denoted by $f_j$, are again continuous and there exists a $B >0$ and $p \in \N$ such that for every $t \in [0,T]$ the functions $f_j, F_j$ are polynomially bounded in $X^\star$, i.e. 
    $$|F_j(\tau(t), \tau(t), \tilde O^{\leq \tau(t)})| +  | f_j(t, \tau(t), \tilde O^{\leq \tau(t)})  | \leq B (X_t^\star +1)^p + B \sum_{i=0}^n |\epsilon_i|, $$\label{ass_both_4}
    
    \item $X^\star$ is $L^{2p}$-integrable, i.e. $\expect{(X^\star_T)^{2p}} < \infty$,\label{ass_both_5}

    \item the i.i.d.\ $\epsilon_k$ are independent of $X, n, M, (t_i)_{1 \leq i \leq K}$, are centered and square-integrable, i.e. $\E_{\tilde\P}[\epsilon_k] = 0$ and $\E_{\tilde\P}[|\epsilon_k|^{2}] < \infty$ and $n$ is square-intgrable, \label{ass_both_6}
    
    \item for every  $1 \le i \le n$, $X_{t_i-}$ is conditionally independent of $\sigmab(n, M_{t_i})$ given $\mathcal{A}_{t_i-}$, and \label{ass_both_7}

    \item for all $1 \le k \le K$, $1 \le j \le d_X$ there is some $\eta_{k,j} >0$ such that $\P ( M_{k, j} = 1 \mid \sigmab(n, \mathcal{A}_{t_k-} )) > \eta_{k,j}$ (i.e. given the currently known information and $n$, for each coordinate the probability of observing it at the next observation time is positive). \label{ass_both_8}
\end{enumerate}
\end{assumption}
\begin{rem}
    The relaxations on the assumption of observing $X_0$ completely discussed in \citep[Remark 2.3]{krach2022optimal} can equivalently be applied in this setting here.
\end{rem}
As in Section~\ref{sec:Setting with Noisy Observations}, the PD-NJ-ODE uses the noisy observations $O_{t_i}$ and $ \tilde O^{\leq \tau(t)}$ as inputs instead of $X_{t_i}$ and $\tilde X^{\leq \tau(t)}$ and is trained with the objective functions \eqref{equ:Psi noisy obs} respectively \eqref{equ:Phi noisy obs} and their Monte Carlo approximations.


\subsection{Convergence Theorem}\label{sec:Convergence Theorem both ext}
We start with the following result which is a combination of Lemma~\ref{lem:L2 identity noisy obs setting} and Lemma~\ref{lem:L2 identity dependence}.

\begin{lem}\label{lem:L2 identity both ext}
For any $\mathbb{A}$-adapted process $Z$ it holds that
\begin{multline*}
\E \left[\tfrac{1}{n} \sum_{i=1}^n \left\lvert M_{t_i} \odot ( O_{t_i} - Z_{t_i-} ) \right\rvert_2^2\right] \\
	= \E \left[ \tfrac{1}{n}\sum_{i=1}^n \left\lvert M_{t_i} \odot ( O_{t_i} - \hat{X}_{t_i-} ) \right\rvert_2^2\right] + \E \left[\tfrac{1}{n}\sum_{i=1}^n \left\lvert M_{t_i} \odot (  \hat{X}_{t_i-} - Z_{t_i-}) \right\rvert_2^2\right] .
\end{multline*}
\end{lem}

\begin{proof}
    First note that by Assumption~\ref{ass:noise&dependence} point \ref{ass_both_3} we have that $X_{t_i} = X_{t_i-}$ almost surely and when defining $O_{t_i -} := X_{t_i -} + \epsilon_i$ we therefore also have that $O_{t_i } = O_{t_i -}$ almost surely.
    Next notice that Assumption~\ref{ass:noise&dependence} point \ref{ass_both_6} \& \ref{ass_both_7} together with Proposition~\ref{prop:conditional independence prop 4} imply that $O_{t_i-}$ is conditionally independent of $\sigmab(n, M_i)$ given $\mathcal{A}_{t_i-}$.
    Hence, for $\hat{O}_{t_i-} := \E_{\P \times\tilde\P}[O_{t_i -} \, | \, \mathcal{A}_{t_i -}]$ it follows as in Lemma~\ref{lem:L2 identity dependence} that
    \begin{multline*}
        \E \left[\tfrac{1}{n}\sum_{i=1}^n \left\lvert M_{t_i} \odot ( O_{t_i-} - Z_{t_i-} ) \right\rvert_2^2\right] \\
	    = \E \left[\tfrac{1}{n}\sum_{i=1}^n \left\lvert M_{t_i} \odot ( O_{t_i-} - \hat{O}_{t_i-} ) \right\rvert_2^2\right] + \E \left[\tfrac{1}{n}\sum_{i=1}^n \left\lvert M_{t_i} \odot (  \hat{O}_{t_i-} - Z_{t_i-} ) \right\rvert_2^2\right].
    \end{multline*}
    Then we can conclude the proof as in Lemma~\ref{lem:L2 identity noisy obs setting}, by noting that \begin{equation*}
    \hat{O}_{t_i-} = \hat{X}_{t_i-} + \E[\epsilon_i | \mathcal{A}_{t_i -}] = \hat{X}_{t_i-} + \E[\epsilon_i] = \hat{X}_{t_i-},
    \end{equation*}
    using that $\epsilon_i$ has expectation $0$ and is independent of $\mathcal{A}_{t_i -}$.
\end{proof}


\begin{proof}[Proof of Theorem~\ref{thm:1} -- Part 1]
    We start by showing that $\hat{X} \in \mathbb{D}$ is the unique minimizer of $\Psi$ up to indistinguishability (as defined in Definition~\ref{def:indistinguishability}). Note that for every $t_i$ we have $M_{t_i} \odot \hat{X}_{t_i} = M_{t_i} \odot X_{t_i}$ and that $X_{t_i} = X_{t_i-}$ if $t_i \notin \mathcal{J}$, hence with probability 1. It follows directly from Lemma~\ref{lem:L2 identity both ext} that $\hat{X}$ is a minimizer of $\Psi$, since
    \begin{equation}\label{equ:Psi of Z split}
        \Psi(Z) = \Psi(\hat{X})  + \E \left[\tfrac{1}{n}\sum_{i=1}^n \left\lvert M_{t_i} \odot (  \hat{X}_{t_i-} - Z_{t_i-}) \right\rvert_2^2\right].
    \end{equation} 
    Before we can show uniqueness of $\hat{X}$, we need some additional results. For those, let $Z \in \mathbb{D}$.
    Let $c_1 := \E\left[ n \right]^{1/2} \in (0, \infty)$, then  the H\"older inequality, together with the fact that $n \geq 1$, yields 
    \begin{equation}\label{equ:HI}
        \E\left[ \left\lvert Z \right\rvert_2 \right] 
	   = \E\left[ \frac{\sqrt{n}}{\sqrt{n}} \left\lvert Z \right\rvert_2 \right] 
	   \leq c_1 \, \E\left[ \frac{1}{n} \left\lvert Z \right\rvert_2^2 \right]^{1/2}.
    \end{equation}
    
    By Lemma~\ref{lem:ck} and by the equivalence of $1$- and $2$-norm we have for some constant $c_3 > 0$ and for any $1 \leq k \leq K$ 
    \begin{equation}\label{equ:M split both}
        \E\left[ \1_{\{n \geq k\}} \left\lvert \hat{X}_{t_k-} - Z_{t_k-}  \right\rvert_2 \right] 
        \leq \frac{c_3}{c_2(k)} \E\left[ \1_{\{n \geq k\}} \left\lvert M_{t_k} \odot ( \hat{X}_{t_k-} - Z_{t_k-} ) \right\rvert_2 \right].
    \end{equation}

    To see that $\hat{X}$ is the unique minimiser up to indistinguishability, let $Z \in \mathbb{D}$ be a process which is not indistinguishable from $\hat{X}$. Hence, there exists some $1 \leq k \leq K$ such that $d_k(\hat{X}, Z) > 0$. We have
    \begin{equation}\label{equ:thm1-positive expectation}
    \begin{split}
         \E\left[\tfrac{1}{n}\sum_{i=1}^n \left\lvert M_{t_i} \odot (  \hat{X}_{t_i-} - Z_{t_i-}) \right\rvert_2^2\right]
	   &=  \E\left[\tfrac{1}{n}\sum_{i=1}^K \1_{\{ n \geq i \}} \left\lvert M_{t_i} \odot (  \hat{X}_{t_i-} - Z_{t_i-}) \right\rvert_2^2\right] \\
	   & \geq \E\left[\tfrac{1}{n}  \1_{\{ n \geq k \}} \left\lvert M_{t_k} \odot (  \hat{X}_{t_k-} - Z_{t_k-}) \right\rvert_2^2\right] \\
	   & \geq c_1^{-2} \E\left[ \1_{\{ n \geq k \}} \left\lvert M_{t_k} \odot (  \hat{X}_{t_k-} - Z_{t_k-}) \right\rvert_2\right]^2 \\
	   & \geq \left( \frac{c_2}{c_1 c_3} \right)^2 \E\left[ \1_{\{ n \geq k \}} \left\lvert  \hat{X}_{t_k-} - Z_{t_k-}  \right\rvert_2\right]^2 \\
	   & = \left( \frac{c_2}{c_0 c_1 c_3} \right)^2 d_k(\hat{X}, Z)^2 > 0,
    \end{split}
    \end{equation} 
    where we used \eqref{equ:HI} for the 3rd, \eqref{equ:M split both} for the 4th and \eqref{equ:pseudo metric dk} for the last line. Together with \eqref{equ:Psi of Z split} this implies $\Psi(Z) > \Psi(\hat{X})$.

    Next we show that \eqref{equ:PD-NJ-ODE} can approximate $\hat{X}$ arbitrarily well. Since the dimension $d_H$ can be chosen freely, let us fix it to $d_H := d_X$. 
    Furthermore, let us fix  $\tilde \theta_3^{\star}$ such that $ \tilde g_{\theta_3^{\star}} = \id$, which is possible since we assumed that $\id \in \tilde{\mathcal{ N}}$. 
    Let $\varepsilon > 0$,   $N_\varepsilon := \lceil 2 (T+1) \varepsilon^{-4} \rceil  $ (implying that $\lim_{\varepsilon \to 0} N_\varepsilon = \infty$) and $\mathcal{P}_\varepsilon$ be the closure of the set $A_{ N_\varepsilon  }$ of \citep[Remark~3.11]{krach2022optimal}, which is compact.
    For any $1 \leq j \leq d_X$, the function $f_j$ is continuous by Assumption~\ref{ass:noise&dependence} and can (by abuse of notation) equivalently be written as (continuous) function $ f_j(t, \tau(t), \tilde O^{\leq \tau(t) } -O_0, O_0 )$.
    Therefore, \citep[Proposition~3.8]{krach2022optimal} implies that there exists an $m_0 = m_0(\varepsilon) \in \N$ and a continuous function $\hat f_j$ such that
    \begin{equation*}
        \sup_{(t, \tau, X) \in [0,T]^2\times \mathcal{P}_\varepsilon } \left| f_j(t, \tau, X ) - \hat f_j(t, \tau, \pi_{m_0}( X -X_0 ), X_0 )\right| \leq \varepsilon/2.
    \end{equation*}
    Since the variation of functions in $\mathcal{P}_\varepsilon$ is uniformly bounded by a  finite constant,  the set of their truncated signatures $\pi_{m_0}(\mathcal{P}_\varepsilon)$ is a bounded subset in $\R^d$ for some $d \in \N$ (depending on $d_X$ and $m_0$), hence its closure, denoted by $\Pi_\varepsilon$, is compact. 
    Therefore, the universal approximation theorem for neural networks  \citep[Theorem 2.4]{10.5555/70405.70408} implies that there exists an $m_1 = m_1(\varepsilon) \in \N$ and neural network weights $\tilde{\theta}_1^{\star, m_1} \in \tilde{\Theta}_{m_1}^1$ such that for every $1 \leq j \leq d_X$ the function $\hat f_j$ is approximated up to $\varepsilon/2$ by the $j$-th coordinate of the neural network $\tilde f_{\tilde{\theta}_1^{\star, m_1}} \in \tilde{\mathcal{N}}$ (denoted by $\tilde f_{\tilde{\theta}_1^{\star, m_1}, j}$) on the compact set $[0,T]^2\times \Pi_\varepsilon$. Hence, combining the two approximations we get (by triangle inequality)
    \begin{equation*}
        \sup_{(t, \tau, X) \in [0,T]^2\times \mathcal{P}_\varepsilon } \left| f_j(t, \tau, X ) -  \tilde f_{\tilde \theta_1^{\star, m_1}, j}  (t, \tau, \pi_{m_0}( X -X_0 ), X_0 )\right| \leq \varepsilon.
    \end{equation*}
    Obviously, extending the input of the neural network does not make the approximation worse, by simply setting the corresponding weights to $0$, hence, also $H_{t-}$ can be used as additional input. 
    Similarly we get that  there exists an $m_2 = m_2(\varepsilon) \in \N$ and neural network weights $\tilde \theta_2^{\star, m_2} \in \tilde \Theta_{m_2}^2$ such that for every $1 \leq j \leq d_X$
    \begin{equation*}
        \sup_{(t, X) \in [0,T] \times \mathcal{P}_\varepsilon } \left| F_j(t, t, X ) -  \tilde \rho_{\tilde \theta_2^{\star, m_2}, j}  (t, \pi_{m_1}( X -X_0 ), X_0 )\right| \leq \varepsilon.
    \end{equation*}
    As before, $H_{t-}$ can be used as additional input without worsening the approximation.
    
    Next we define the bounded output neural networks based on these neural networks. For this let us define 
    \begin{equation*}
    \gamma_1 := \max_{(t, \tau, X) \in [0,T]^2\times \mathcal{P}_\varepsilon } \left|  \tilde f_{\tilde \theta_1^{\star, m_1}}  (t, \tau, \pi_{m_0}( X -X_0 ), X_0 )\right|
    \end{equation*}
    and $\gamma_2$ equivalently for $\tilde \rho_{\tilde \theta_2^{\star, m_2}}$. Since the neural networks are continuous functions they take a finite maximum on the compact sets, hence $\gamma_1, \gamma_2$  are finite.
    Then we define the bounded output neural networks  $f_{\theta_1^{\star, m_1}}, \rho_{ \theta_2^{\star, m_2}} \in \mathcal{N}$ with $\theta_i^{\star, m_i} := (\tilde \theta_i^{\star, m_i}, \gamma_i)$.
    Clearly, these bounded output neural networks coincide with the neural networks on the compact sets. Therefore, they satisfy the same $\varepsilon$-approximation and since $F_j, f_j$ are bounded by $U:=B\left((X_T^\star +1)^p+\sum_{i=0}^n |\epsilon_i|\right)$ (Assumption~\ref{ass:noise&dependence} item \ref{ass_both_4}), it follows that $f_{\theta_1^{\star, m_1}}, \rho_{ \theta_2^{\star, m_2}}$ are bounded by $U + \varepsilon$.
    In particular, we have for $\epsilon<B$ the global bounds $|f_j - f_{\theta_1^{\star, m_1}, j} |_\infty \leq 3U$ and $|F_j - \rho_{\theta_2^{\star, m_2}, j}|_\infty \leq 3U$.
    Setting $m := \max(m_0, m_1, m_2, \gamma_1, \gamma_2, |\tilde \theta_i^{\star, m_2}|_2, |\tilde \theta_2^{\star, m_2}|_2)$, it follows that $\theta^\star_m := (\theta^{\star, m_1}_1, \theta^{\star, m_2}_2, \tilde \theta^\star_3 ) \in \Theta_m$.

    Now we can bound the distance between $Y_t^{\theta_m^\star}$ and $\hat{X}$. Whenever $X_T^\star < 1/\varepsilon$,  the number of observations satisfies $n < 1/\varepsilon$, the minimal difference between any two consecutive observation times $\delta > \varepsilon$ and all noise terms satisfy $| \epsilon_i |_2 < 1/\varepsilon$ we know that the corresponding path $\tilde X^{\leq \tau(t)} - X_0$ is an element of $A_{N_\varepsilon}$ and therefore the neural network approximations up to $\varepsilon$ hold. Otherwise, one of those conditions is not satisfied and the global upper bound can be used. 
    Hence, if $t \in \{ t_1, \dotsc, t_n \}$, we have for $F = (F_j)_{1 \leq j \leq d_X}$ and $f = (f_j)_{1 \leq j \leq d_X}$
    \begin{equation*}
    \begin{split}
        \left\lvert Y_t^{\theta_m^*} - \hat{X}_t  \right\rvert_1 
	   & = \left\lvert  \rho_{\theta_2^{\star, m_2}}\left(H_{t-}, t,\pi_{m}( \tilde O^{\leq t} - O_0 ), O_0 \right) - F \left(t, t, \tilde O^{\leq t} \right) \right\rvert_1  \\
	   & \leq \varepsilon d_X \1_{\{ X_T^\star < 1/\varepsilon \}}  \1_{\{ n < 1/\varepsilon \}}  \1_{\{ \delta > \epsilon \}} \1_{\{ \forall 0 \leq i \leq n: | \epsilon_i |_2 < 1/\varepsilon \} } \\
	   & \quad +  d_X 3 U \left( \1_{\{ X_T^\star \geq 1/\varepsilon \}} + \1_{\{ n \geq  1/\varepsilon \}} + \1_{\{ \delta \leq \epsilon \}} + \1_{\{ \exists 0 \leq i \leq n: | \epsilon_i |_2 \geq 1/\varepsilon \}}  \right),
    \end{split}
    \end{equation*}
    and if $t \notin \{ t_1, \dotsc, t_n \}$,
    \begin{equation*}
    \begin{split}
        \left\lvert Y_t^{\theta_m^*} - \hat{X}_t  \right\rvert_1  
	   &\leq \left\lvert Y_{\tau(t)}^{\theta_m^*} - \hat{X}_{\tau(t)}  \right\rvert_1 \\
	   &\quad +  \int_{\tau(t)}^t \left\lvert f_{\theta_1^{\star, m_1}}\left(H_{s-}, s, \tau(t), \pi_{m} (\tilde O^{\leq \tau(t)} -O_0 ), O_0 \right) 
		- f(s, \tau(t), \tilde O^{\leq \tau(t)})  \right\rvert_1 ds\\
	   & \leq \varepsilon (T+1) d_X \1_{\{ X_T^\star < 1/\varepsilon \}}  \1_{\{ n < 1/\varepsilon \}}  \1_{\{ \delta > \varepsilon \}} \1_{\{ \forall 0 \leq i \leq n: | \epsilon_i |_2 < 1/\varepsilon \} }  \\
	   & \quad + (T+1) d_X 3U \left( \1_{\{ X_T^\star \geq 1/\varepsilon \}} + \1_{\{ n \geq  1/\varepsilon \}} + \1_{\{ \delta \leq \varepsilon \}} + \1_{\{ \exists 0 \leq i \leq n: | \epsilon_i |_2 \geq 1/\varepsilon \}} \right).
    \end{split}
    \end{equation*}
    Moreover, by  equivalence of the $1$- and $2$-norm, there exists a constant $c > 0$ such that for all $t \in [0,T]$
    \begin{equation*}
    \begin{split}
        \left\lvert Y_t^{\theta_m^*} - \hat{X}_t  \right\rvert_2 
	   & \leq c \, \varepsilon (T+1) d_X \\
        & \quad \; +  c (T+1) d_X 3 U \left( \1_{\{ X_T^\star \geq 1/\varepsilon \}} + \1_{\{ n \geq  1/\varepsilon \}} + \1_{\{ \delta \leq \varepsilon \}} + \1_{\{ \exists 0 \leq i \leq n: | \epsilon_i |_2 \geq 1/\varepsilon \}} \right) \\
	   & =: c_m.
    \end{split}
    \end{equation*}

    So far, we have fixed an $\varepsilon >0$ and argued that there exists some $ m \in \N$ such that the neural network approximation bounds hold with $\varepsilon$-error. 
    However, what we actually need to show is that this error converges to $0$ when increasing the  truncation level and network size $m$.
    Therefore, we define $\varepsilon_m \geq 0$ to be the smallest number such that the above bounds hold with error $\varepsilon_m$ when using an architecture with signature truncation level  $m \in \N$ and weights in $\Theta_m$. Since increasing $m$ can only make the approximations better (by setting the new weights to $0$, the same approximation error as before is achieved, but potentially there exists a better choice), we have $\varepsilon_{m_1} \geq \varepsilon_{m_2}$ for any $m_1 \leq m_2$. In particular $(\varepsilon_m)_{m \geq 0}$ is a a decreasing sequence, hence, our derivations before proof that $\lim_{m \to \infty} \varepsilon_m = 0 $. In the following we denote by $\theta^\star_m \in \Theta_m$ the best choice for the weights within the set $\Theta_m$ to approximate the functions $F_j, f_j$.

   Since $\theta_m^{\min} \in \argmin _{\theta \in \Theta_m}\{ \Phi(\theta) \}$  (note that at least one minimum exists in the compact set $\Theta_m$ since $\Phi$ is continuous), we get with Lemma~\ref{lem:L2 identity both ext} that
    \begin{equation}\label{equ:phi convergence 1 both}
    \begin{split}
        \min_{Z \in \mathbb D} \Psi(Z) &\le \Phi (\theta^{\min}_m) \le \Phi(\theta^\ast_m)
        =  \expect{\frac{1}{n}\sum_{i = 1}^n \abs{M_{t_i} \odot (O_{t_i} - Y^{\theta^\ast_m}_{t_i} ) }_2^2 } \\
        &= \Psi(\hat X) + \E \l[ \frac{1}{n}\sum_{i = 1}^n \abs{M_{t_i} \odot ( \hat{X}_{t_i-} - Y^{\theta^\ast_m}_{t_i-} ) }_2^2 \r]
        \le \Psi(\hat X) + \E \l[ c_m^2 \r].
    \end{split}
    \end{equation}
    Integrability of $|X_T^\star|_2$, $|n|$ and $|\epsilon_i|$ together with Assumption~\ref{ass:noise&dependence} item \ref{ass_both_2} on $\delta$ imply that 
    $$\1_{\{ X_T^\star \geq 1/\varepsilon_m \}} + \1_{\{ n \geq  1/\varepsilon_m \}} + \1_{\{ \delta \leq \varepsilon_m \}} + \1_{\{ \exists 0 \leq i \leq n: | \epsilon_i |_2 \geq 1/\varepsilon_m \}} \xrightarrow[m \to \infty]{\P-a.s.} 0.$$ 
    Therefore, we have for a suitable constant $c>0$ (not depending on $\varepsilon_m$ and $m$),
    \begin{equation*}
    \begin{split}
        \E\left[ c_m^2 \right] 
	   \leq c \varepsilon_m^2+ &  c  \E\left[U^2 \left(  \1_{\{ X_T^\star \geq 1/\varepsilon_m \}} + \1_{\{ n \geq  1/\varepsilon_m \}} \right.\right. \\
    & \qquad \qquad \left.\left. + \1_{\{ \delta \leq \varepsilon_m \}} + \1_{\{ \exists 0 \leq i \leq n: | \epsilon_i |_2 \geq 1/\varepsilon_m \}} \right) \right] \xrightarrow{m \to \infty} 0,
    \end{split}
    \end{equation*}
    by dominated convergence, since $U$ is $L^{2}$-integrable. Indeed, 
    \begin{multline}\label{equ:integraility in both ext}
        \E[U^2] 
        \leq 8B^2 \E \left[ (X_t^\star +1)^{2p} + n \sum_{i=0}^n |\epsilon_i|^2 \right] \\
        = 8B^2 \l( \E \left[ (X_t^\star +1)^{2p} \right] +  \E[n^2] \E \left[|\epsilon_0|^2 \right] \r) < \infty,
    \end{multline}
    using Cauchy-Schwarz inequality for the first step, that the $\epsilon_i$ are i.i.d. and independent of $n$ for the equality and the integrability of $X^\star$, $\epsilon_0$ and $n^2$ for the upper bound.    
    Using this and $\Psi(\hat{X}) = \min_{Z \in \mathbb{D}} \Psi(Z)$, we get from \eqref{equ:phi convergence 1 both}
    \begin{equation*}
        \min_{Z \in \mathbb{D}} \Psi(Z) 
	   \leq \Phi(\theta_m^{\min}) \leq \Phi(\theta_m^{*}) \xrightarrow{m \to \infty} \min_{Z \in \mathbb{D}} \Psi(Z).
    \end{equation*}
    Finally, we show that $\lim_{m\to\infty} d_k \left( \hat{X},  Y^{\theta_m^{\min}} \right) = 0$ for all $1 \leq k \leq K$.
    Applying \eqref{equ:HI}, \eqref{equ:M split both} and \eqref{equ:pseudo metric dk} in  reverse order than it was done in  \eqref{equ:thm1-positive expectation} and finally Lemma~\ref{lem:L2 identity both ext}, yields
    \begin{equation}\label{equ:convergence in L1}
    \begin{split}
        d_k \left( \hat{X} , Y^{\theta_m^{*}}  \right)
	   & \leq \frac{c_0 \, c_1 \, c_3}{c_2} \, \E\left[ \frac{1}{n} \sum_{i=1}^n \left\lvert M_{t_i} \odot ( \hat{X}_{t_i-} - Y^{\theta_m^{*}}_{t_i-} ) \right\rvert_2^2 \right]^{1/2} \\
	   & =   \frac{c_0 \, c_1 \, c_3}{c_2} \, \left( \Phi(\theta_m^{*}) - \Psi(\hat{X}) \right)^{1/2}  \xrightarrow{m \to \infty} 0,
    \end{split}
    \end{equation}
    which completes the first part of the proof.
\end{proof}

Next we assume the size $m$ of the neural network and of the signature truncation level is fixed and we study the convergence of the Monte Carlo approximation when the number of samples $N$ increases.
The convergence analysis is based on \cite[Chapter 4.3]{lapeyre2019neural} and follows \cite[Theorem E.13]{herrera2021neural}.
We define the separable Banach space $\mathcal{S} := \{ x = (x_i)_{ i \in \N} \in \ell^1(\R^{d}) \; \vert \; \lVert x \rVert_{\ell^1} < \infty \}$ for a suitable $d$ (see below) with the norm $\lVert x \rVert_{\ell^1} := \sum_{i \in \N} \lvert x_i \rvert_2$, the function
\begin{equation*}
F(x,y,m) :=  \left\lvert m \odot ( x - y ) \right\rvert_2
\end{equation*}
and $\xi_j := (\xi_{j, 0}, \dotsc, \xi_{j, n^{(j)}}, 0, \dotsc)$, where $\xi_{j,k} := (t_{k}^{(j)}, O_{t_k^{(j)}}^{(j)}, M_{t_k^{(j)}}^{(j)}, \pi_m(\tilde O^{\leq t_k^{(j)}, {(j)}})) \in \R^d$ and $t_k^{(j)}$, $M_{t_k^{(j)}}^{(j)}$ and $O^{(j)}_{t^{(j)}_i}$ (with $0$ entries for coordinates which are not observed) are random variables describing the $j$-th realization of the training data (cf. Section~\ref{sec:Recall: the PD-NJ-ODE}).
Let $n^{j}(\xi_j) := \max_{k \in \N}\{ \xi_{j,k} \neq 0\}$, $t_k(\xi_j):= t_{k}^{(j)}$, $O_k (\xi_j) := O_{t_k^{(j)}}^{(j)}$ and $M_k(\xi_j) := M_{t_k^{(j)}}^{(j)}$.
By this definition we have  $n^{(j)} = n^{j}(\xi_j)$ $\P$-almost-surely. Moreover, we have that $\xi_j$ are i.i.d. random variables taking values in $\mathcal{S}$.
Let us write $Y_t^{\theta}(\xi)$ to make the dependence of $Y$ on the input and the weight $\theta$ explicit.
Then we define
\begin{equation*}
h(\theta, \xi_j) := \frac{1}{n^{j}(\xi_j)}\sum_{i=1}^{n^{j}(\xi_j)}  F \left( O_i(\xi_j), Y^\theta_{t_i(\xi_j)-}(\xi_j), M_i(\xi_j) \right)^2.
\end{equation*}

The following lemma is known from \cite{krach2022optimal}.
\begin{lemma}\label{lem:properties for MC conv thm}
Almost-surely the random function $\theta\in \Theta_M \mapsto Y_{t}^{\theta}$ is uniformly continuous for every $t \in [0,T]$.
\end{lemma}
Now we are ready to prove the second part of our main theorem.

\begin{proof}[Proof of Theorem \ref{thm:1} -- Part 2.]
First we note that, $Y^\theta_t$ is the (integration over the) output of (bounded output) neural networks and therefore bounded in terms of the input, the weights (which are bounded by $m$), $T$ and some constant depending on the architecture and the activation functions of the neural network. 
In particular we have that $|Y^\theta_t(\xi_j)| \leq \tilde B \left((X_T^{\star, (j)} +1)^p+\sum_{i=0}^n |\epsilon_i^{(j)}|\right)$ for all $t \in [0,T]$ and $\theta \in \tilde\Theta_m$ for some constant $\tilde B$ (possibly depending on $m$), where $X^{\star,(j)}, \epsilon_i^{(j)}$ corresponds to the input $\xi_j$.
Hence,
\begin{multline*}
    F \left( O_i(\xi_j), Y^\theta_{t_i(\xi_j)-}(\xi_j), M_i(\xi_j) \right)^2
	=  \left\lvert M_i(\xi_j) \odot ( O_{i}(\xi_j) - Y^\theta_{t_i(\xi_j)-}(\xi_j) ) \right\rvert_2^2\\
	\leq \left((B+\tilde B)\left((X_T^\star +1)^p+\sum_{i=0}^n |\epsilon_i|\right)\right)^{2} 
    = \left( \frac{B+\tilde B}{B} U^{(j)}\right)^2,
\end{multline*}
where $U^{(j)}$ is as defined before corresponding to the input $\xi_j$.
Hence,
\begin{equation}
\label{equ:dominating bound loss function}
\E \left[\sup_{\theta \in \tilde\Theta_M} h(\theta, \xi_j)\right] 
\leq   \E \left[\frac{1}{n}\sum_{i=1}^{n} \left( \frac{B+\tilde B}{B} U^{(j)}\right)^2 \right]  < \infty,
\end{equation}
by Assumption~\ref{assumption:F} and \eqref{equ:integraility in both ext}.
By Lemma~\ref{lem:properties for MC conv thm}, the function $\theta \mapsto h(\theta, \xi_1)$ is continuous, hence, we can apply \citep[Lemma E.15]{herrera2021neural}, yielding that almost-surely for $N \to \infty$ the function 
\begin{equation}\label{equ:unif conv 1}
\theta \mapsto \frac{1}{N} \sum_{j=1}^{N} h(\theta, \xi_j) = \hat\Phi_N(\theta)
\end{equation}
converges uniformly on $\Theta_m$ to 
\begin{equation}\label{equ:unif conv 2}
\theta \mapsto \E [h(\theta, \xi_1)] = \Phi(\theta).
\end{equation} 
We deduce from \citep[Lemma E.14]{herrera2021neural} that $d(\theta^{\min}_{m,N},\Theta^{\min}_m)\to 0$ a.s. when $N\to \infty$. 
Then there exists a sequence $(\hat\theta^{\min}_{m,N})_{N \in \N}$ in $\Theta_m^{\min}$ such that $\lvert \theta^{\min}_{m,N} - \hat\theta^{\min}_{m,N} \rvert_2 \to 0$ a.s. for $N \to \infty$.
The uniform continuity of the random functions $\theta \mapsto Y_{t}^{\theta}$ on ${\Theta}_m$ implies that 
$$\lvert Y_{t}^{\theta^{\min}_{m,N}}(\xi_1) - Y_{t}^{\hat\theta^{\min}_{m,N}}(\xi_1) \rvert_2 \to 0 \text{ a.s. for all  } t \in [0,T] \text{ as } N \to \infty.$$ 
By continuity of $F$ this yields $\lvert h(\theta^{\min}_{m,N}, \xi_1) - h(\hat\theta^{\min}_{m,N}, \xi_1) \rvert \to 0$ a.s. as $N \to \infty$.
With \eqref{equ:dominating bound loss function} we can apply dominated convergence which yields
\begin{equation*}
\lim_{N \to \infty} \E \left[ \lvert h(\theta^{\min}_{m,N}, \xi_1) - h(\hat\theta^{\min}_{m,N}, \xi_1) \rvert \right] = 0.
\end{equation*}
Since for every integrable random variable $Z$ we have $0 \leq \lvert \E[Z] \rvert \leq \E[\lvert Z \rvert] $ and since $\hat\theta^{\min}_{m,N}\in \Theta_m^{\min}$ we can deduce
\begin{equation}
\label{equ: MC convergence}
\lim_{N \to \infty} \Phi(\theta^{\min}_{m,N}) = \lim_{N \to \infty} \E \left[  h(\theta^{\min}_{m,N}, \xi_1) \right] = \lim_{N \to \infty} \E \left[  h(\hat\theta^{\min}_{m,N}, \xi_1) \right] = \Phi(\theta^{\min}_m).
\end{equation}
Now by triangle inequality,
\begin{equation}\label{equ: MC convergence 2}
\lvert \hat\Phi_N(\theta^{\min}_{m,N}) -  \Phi(\theta^{\min}_{m}) \rvert \leq \lvert \hat\Phi_N(\theta^{\min}_{m,N}) -  \Phi(\theta^{\min}_{m,N}) \rvert + \lvert \Phi(\theta^{\min}_{m,N}) -  \Phi(\theta^{\min}_{m}) \rvert.
\end{equation}
\eqref{equ:unif conv 1}, \eqref{equ:unif conv 2} and \eqref{equ: MC convergence} imply that both terms on the right hand side converge to 0 when $N \to \infty$, which finishes the proof of the convergence with respect to $N$.

Finally, we want to show the joint convergence.
We define $N_0 := 0$ and for every $m \in \N$
\begin{equation*}
N_m := \min\left\{ N \in \N \; \vert \; N > N_{m-1}, \lvert \Phi(\theta^{\min}_{m,N}) - \Phi(\theta^{\min}_{m}) \rvert  \leq \tfrac{1}{m} \right\},
\end{equation*}
which is possible due to \eqref{equ: MC convergence}. Then Theorem \ref{thm:1} implies that
\begin{equation*}
\lvert \Phi(\theta^{\min}_{m,N_m}) - \Psi(\hat{X}) \rvert  \leq \tfrac{1}{m} +  \lvert \Phi(\theta^{\min}_{m}) - \Psi(\hat{X}) \rvert \xrightarrow{m \to \infty} 0.
\end{equation*}
Therefore, we can apply the same arguments as in the first part of the proof (cf. \eqref{equ:convergence in L1}) to show that 
\begin{equation*}
d_k \left(  \hat{X} , Y^{\theta^{\min}_{m,N_m}}  \right)
\leq   \frac{c_0 \, c_1 \, c_3}{c_2} \, \left( \Phi(\theta^{\min}_{m,N_m}) - \Psi(\hat{X}) \right)^{1/2}  \xrightarrow{m \to \infty} 0,
\end{equation*}
for every $1 \leq k \leq K$.
\end{proof}

The following corollary follows as in \cite{krach2022optimal}.
\begin{cor}\label{cor:1}
In the setting of Theorem~\ref{thm:1}, we also have that $\P$-a.s.
\begin{equation*}
\Phi(\theta^{\min}_{m,N_m}) \xrightarrow{m \to \infty} \Psi(\hat{X}) \quad \text{and} \quad \hat\Phi_{\tilde{N}_m}(\theta^{\min}_{m,\tilde{N}_m}) \xrightarrow{m \to \infty} \Psi(\hat{X}),
\end{equation*}
where $(\tilde{N}_m)_{m \in \N}$ is another increasing sequence in $\N$.
\end{cor}





\section{Experimental Details}\label{sec:Experimental Details}

Our experiments are based on the implementation used by \cite{krach2022optimal} which is available at \url{https://github.com/FlorianKrach/PD-NJODE}, therefore we refer the reader to its Appendix A for any details that are not provided here.

\subsection{Differences between the Implementation and the Theoretical Description of the PD-NJ-ODE}\label{sec:Differences between the Implementation and the Theoretical Description of the PD-NJ-ODE}
Since we use the same implementation of the PD-NJ-ODE, all differences between the implementation and the theoretical description listed in \cite[Appendix~A.1.1]{krach2022optimal} also apply here. In particular, we use standard neural networks for $f_{\theta_1}$ and $\rho_{\theta_2}$ with additional inputs. 
In contrast to the original objective function, we do not need to add a regularizing constant for the noise-adapted objective functions \eqref{equ:Psi noisy obs} and \eqref{equ:Phi noisy obs}, since no square-root needs to be computed.
We do not use the $u$-coordinates of $\tilde X$ in the implementation, since we anyways use the observation times and masks as additional inputs for $\rho_{\theta_2}$.


\subsection{Details for Noisy Observations}\label{sec:Details for Noisy Observations}
\textbf{Dataset.} 
We sample paths from a standard $1$-dimensional Brownian motion on the interval $[0,1]$, i.e.\ with $T=1$, and discretisation time grid with step size $0.01$. At each time point we observe the process with probability $p=0.1$. Whenever the process is observed, an independent observation noise is sampled from a centered normal distribution with standard deviation $\sigma=0.5$ and added to the observation. The model never sees the observation of the original process, but only the noisy observation. We sample $20'000$ paths of which $80\%$ are used as training set and the remaining $20\%$ as test set.

\textbf{Architecture.}
We use the PD-NJ-ODE with the following architecture. The latent dimension is $d_H = 100$, the readout network is a linear map  and the other 2 neural networks have the same structure of 1 hidden layer with $\operatorname{ReLU}$ activation function and $100$ nodes. The signature is used up to truncation level $3$, the encoder is recurrent and the decoder uses a residual connection.

\textbf{Training.}
We use the Adam optimizer with the standard choices $\beta = (0.9, 0.999)$, weight decay of $0.0005$ and learning rate $0.001$. Moreover, a dropout rate of $0.1$ is used for every layer and training is performed with a mini-batch size of $200$ for $200$ epochs.
The model is trained once with the noise-adapted objective function \eqref{equ:Psi noisy obs} and once with the original one \eqref{equ:Psi}.


\subsection{Details for Dependent Observation Framework}\label{sec:Details for Dependent Observation Framework}
\textbf{Dataset.}
We use the Euler scheme to sample paths from a $1$-dimensional Black--Scholes model (geometric Brownian motion) with drift $\mu = 2$, volatility $\sigma = 0.3$, and starting value $X_0 = 1$.
At each time point we observe the process with the probability $\P(M_i^X = 1 \, | \, \mathcal{A}_{t_i-}) = \E[M_i^X \, | \, \mathcal{A}_{t_i-} ]$, where $M_i^X$ is described in Example~\ref{exa:Black--Scholes with Dependent Observations}, using $\eta=3$ and $p=0.1$.
We use the same discretisation grid and dataset sizes as in Section~\ref{sec:Details for Noisy Observations}.

\textbf{Architecture.}
We use the PD-NJ-ODE with the following architecture. The latent dimension is $d_H = 50$ and all 3 neural networks have the same structure of 2 hidden layers with $\tanh$ activation function and $50$ nodes. 
The signature is used up to truncation level $3$, the encoder is recurrent and the decoder does not use a residual connection.

\textbf{Training.}
Same as in Section~\ref{sec:Details for Noisy Observations} but only trained with original loss function \eqref{equ:Psi}.



\fi


%%% ==================================================================





\end{document}
