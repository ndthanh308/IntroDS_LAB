\documentclass[10pt,twocolumn,letterpaper, dvipsnames]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
% \usepackage{authblk}
%%%%%
\usepackage[dvipsnames]{xcolor}
\usepackage{color, colortbl}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{nicematrix}
\usepackage{makecell}
% \usepackage{xcolor}
%\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%             Custom setup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{blindtext}
%\usepackage{amsmath}
\usepackage{multirow}
%\usepackage{graphicx}
\usepackage{bm}
\usepackage{bbm} % \mathbbm
\usepackage{comment}
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage{subfig} % define \subfloat, incompatible with this latex template
\usepackage{subcaption} %%%% this one is for the subtables 
\usepackage{array} % for extended column definitions, e.g. >{\em}c
%\usepackage{booktabs}
%\usepackage{wrapfig}

% \usepackage[misc,geometry]{ifsym} % for \Letter
\makeatletter
\renewcommand\paragraph{
  \@startsection{paragraph} % name
  {4} % level
  {\z@} % indent
  {.5em \@plus1ex \@minus.2ex} % beforeskip
  {-1.5em} % afterskip
  {\normalfont\normalsize\bfseries} % style
}
\makeatother

% make the letter symbol appear the first in \thanks
\makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \textsuperscript{~\Letter}\or \ddagger\or
   \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

\newcommand{\tableCellHeight}{1}
\newcommand{\tabstyle}[1]{
  \setlength{\tabcolsep}{#1}
  \renewcommand{\arraystretch}{\tableCellHeight}
  \centering
  \small
}

\newcommand{\romannum}[1]{\romannumeral #1} % roman numbering
\newcommand{\rotbox}[1]{\rotatebox{90}{#1}}

\newcommand{\hgreen}[1]{\textcolor{ForestGreen}{\textbf{#1}}} % highlight color
\newcommand{\hblue}[1]{\textcolor{NavyBlue}{\textbf{#1}}} % highlight color
\newcommand{\cavan}[1]{{\color{blue}(cavan: {#1})}} % cavan's comments
\newcommand{\ky}[1]{{\color{red}(ky: {#1})}} % ky's comments
\definecolor{tabhighlight}{HTML}{e5e5e5}
\definecolor{citecolor}{HTML}{0071bc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
% \usepackage[pagebackref,breaklinks,colorlinks,citecolor=citecolor]{hyperref}





%%%%%%

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\iccvfinalcopy % *** Uncomment this line for the final submission

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}



\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi




\begin{document}

%%%%%%%%% TITLE
\title{Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts}

% option1 : LLM's can prompt CLIP!?

\author{
    Mayug Maniparambil,
    Chris Vorster,
    Derek Molloy,
    Noel Murphy, \\
    Kevin McGuinness,
    Noel E. O'Connor \\
    ML Labs, Dublin City University,\\
    Dublin, Ireland
}


\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\footnotetext[1]{Corresponding author: mayugmaniparambil@gmail.com}
%%%%%%%%% ABSTRACT
\begin{abstract}
   Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (\(\sim 7\%\)), DTD (\(\sim 7\%\)), SUN397 (\(\sim 4.6\%\)), and CUB (\(\sim 3.3\%\))  when compared to CLIP's default prompt. We also design a simple few-shot adapter that learns to choose the best possible sentences to construct generalizable classifiers that outperform the recently proposed CoCoOP  by \(\sim 2\%\) on average and by over \(4\%\) on 4 specialized fine-grained datasets. We will release the code, prompts, and auxiliary text dataset upon acceptance.

\end{abstract}

%%%%%%%%% BODY TEXT
% TODO not sure where this fig should go:
% Figure environment removed
\section{Introduction}

%%% Important point. Maybe write about how CLIP can be considered as a general visual  recognition system, and how for fine-grained recognition even humans require some descriptions of the thing we are trying to recognize. Hence we look at visual sentences...

Contrastive pre-training of large-scale VLMs has demonstrated remarkable image classification performance on open-set classes. Models like CLIP \cite{RadfordLearningSupervision} and ALIGN \cite{Jia2021ScalingSupervision} are pretrained on web-scale datasets consisting of image-text pairs (over 400 million and 1.8 billion respectively), resulting in a highly generalizable model with competent 0-shot domain adaptation capabilities. While vanilla supervised training is performed on a closed set of concepts or classes, CLIP pretraining uses natural language. This results in a joint text-vision embedding space that is not constrained to a fixed set of classes. In CLIP the classifier is constructed by plugging the class name into a predetermined prompt template like `a photo of \{class name\}'. A straightforward way to adapt CLIP to different domains is by prompt engineering, which usually involves modifying the prompt template to include semantic information about the target task. For example, to classify bird images, one could construct a prompt `a photo of \{classname\}, a type of bird'. This prompt engineering process, however, is not optimal because it:  1.) requires domain expertise in the target domain; 2.) has high variance -- small changes to the prompt result in large variation in performance; 3.) has only a single prompt template for all the classes in the domain so that only the class name is present  to provide the classification anchor, which might not contain enough information to distinguish different classes. For example, in Fig \ref{fig:illustrative} we see an image of a Green Heron, which from the name would suggest that it is  predominantly a green-colored bird and we would assume that it is similar to Green Woodpecker if we have never seen either bird. However, we can see that it is in fact a blackish-brown bird with a chestnut-colored neck and visually more similar to a bird like Black Bittern. For 0-shot transfer to fine-grained datasets like this to work well, CLIP has to either have seen and associated images of a Green Heron to the text `Green Heron' from its large pretraining dataset or additional information in the form of \textit{visually descriptive textual} (VDT) information is required. Here we define VDT as a set of sentences that describe the visual features of the class under consideration including shape, size, color, environment, patterns, composition, etc.  While most humans can identify many different common bird species just from their names, they would need access to an ornithology taxonomy of bird descriptions to identify more rare bird species. Similarly, we argue that CLIP's 0-shot accuracy can be improved by incorporating VDT information into the prompts. As shown, in Figure \ref{fig:illustrative}, including VDT information like \textit{black crown} and \textit{black rump} moves the classification prototype of Green Heron away from the  classification prototype of Green Woodpecker and towards that of Black Bittern in the text-encoder's embedding space.

In this work, we first show that we can use VDT information for each class in the target domain to construct class conditional prompts that achieve performance improvements over CLIP's default prompt. We show this on the CUB dataset \cite{Caltech-UCSD200} by constructing sentences from domain experts about the bird species in section \ref{sec:vdt} as they are readily available as part of the dataset. 

However, we acknowledge that domain expert annotations are costly and time-consuming to obtain, hampering the scalability of our method to other datasets. To address this, we focus on the recent advances in generative pretrained Large Language Models (LLMs) like GPT-4 to construct these class conditional prompts in a manner easily scalable to other datasets. These models are a good fit for the task of constructing sophisticated prompts, because: 1) they are a condensed form of human knowledge (trained on web-scale text data) \cite{YuKoLA:Models}; 2) they can be manipulated to produce information in any form or structure which makes compatibility with CLIP's prompt style relatively simple. Therefore we use GPT-4 to construct visually descriptive textual information about the classes with special emphasis in the GPT-4 prompts about visual cues like shape, color, structure, and compositionality. We use the generated VDT information to construct prompt ensembles that are passed through CLIP's text encoder and aggregated to generate classifiers that are then used for 0-shot classification. Using GPT-4 circumvents the need for domain knowledge and conveniently provides class conditional prompts. Prompt ensembling the VDT sentences reduce CLIP's performance sensitivity to small changes in the prompt.
We show performance improvements over vanilla CLIP with the default prompt on 12 datasets with an average improvement of 2\% and even better improvements in fine-grained datasets like EuroSAT  (\(\sim 7\%\)), DTD (\(\sim 7\%\)), SUN397 (\(\sim 4.6\%\)), and CUB (\(\sim 3.3\%\)). The prompts and all the auxiliary class information will be made publicly available to promote research in prompt ensembling and multi-modal adapter design.

Finally, we design a simple adapter that learns to adaptively select the best possible sentences for any given dataset and show that making use of this additional VDT information improves the few-shot domain transfer performance of CLIP. We show the few-shot adaptation performance for the recently proposed Base-to-New setting on a benchmark of 12 datasets and outperform recent methods like CoCoOp \cite{ZhouConditionalModels}  despite having fewer model parameters, shorter training time and a simpler model architecture.

In short, our contributions are as follows.
\begin{enumerate}
    \item We show that including visually descriptive textual (VDT) information in prompts results in better 0-shot domain transfer performance of CLIP. 
    \item We use GPT-4 to generate VDT sentences in a scalable manner and show consistent performance improvements over CLIP in 0-shot domain transfer. 
    \item We design a simple adapter network to make use of this extra information for few-shot transfer and show performance improvements over methods like CLIP-Adapter and CoCoOp \cite{ZhouConditionalModels} for few-shot domain transfer in the Base-to-New setting.
    \item We release all the VDT information for all 12 datasets to promote further research in multi-modal prompt and adapter design for low-shot domain transfer of large VLMs. 
\end{enumerate}


%-------------------------------------------------------------------------


\section{Related Works}

\subsection{Vision Language Models}
Historically vision and language models were generally constructed and trained independently, and their outputs often aligned using additional losses and modules. Image encoding often leveraged manually developed descriptors \cite{socher2013zero} or neural networks \cite{lei2015predicting, 7293699,frome2013devise}, while text encoding was accomplished via pre-trained word vectors \cite{socher2013zero, frome2013devise},  or frequency-grounded TF-IDF features \cite{elhoseiny2013write}. Various strategies such as metric learning \cite{frome2013devise}, multi-label classification \cite{joulin2016learning}, n-gram learning \cite{li2017learning}, and recently proposed image captioning \cite{desai2021virtex} were implemented for cross-modality alignment.

Recent VLMs \cite{Jia2021ScalingSupervision, radford2021learning, furst2021cloob} jointly learn the vision and language encoders from scratch and have demonstrated impressive 0-shot domain transfer performance. As mentioned in \cite{zhou2021coop}, this can be attributed to transformer networks \cite{vaswani2017attention}, contrastive losses \cite{chen2020simple, he2020momentum}, and web-scale training datasets \cite{radford2021learning, jia2021scaling}.

While our GPT-generated prompt ensembles are similar to CLIP's prompt ensembles, CLIP's prompt ensembles were constructed and tuned manually, and are class agnostic, while ours were generated by GPT models that were prompted to provide VDT information for each class.

\subsection{Prompt Learning}
Prompt learning is a well-researched topic in NLP, where LLMs are viewed as a knowledge base and they are adapted to downstream tasks by only tuning a part of the prompt to the language model \cite{jiang2020can, shin2020autoprompt, zhong2021factual, li2021prefix, lester2021power, liu2021pre}. Recently CoOp \cite{zhou2021coop} extended this idea to VLMs by learning continuous prompts from a few-shot dataset, demonstrating impressive transfer performance to downstream datasets. However, \cite{ZhouConditionalModels} showed that CoOp often overfits to the few-shot dataset it is trained on, resulting in poor generalizability to new classes. They proposed CoCoOp, a prompt learning method that is image conditioned using a meta-network, demonstrating improvements over CoOp in the Base-to-New class setting. While this works, it is considerably more resource intensive than CoOp because of the image conditioning. Our work shows an orthogonal direction for the generalizability problem by making use of class conditional VDT information instead -- our GPT-A-self, GPT-A-mlp adapters improve upon CoCoOp despite having a simpler and less compute intensive design.


\subsection{Few-shot adapters for Vision Language models}
CLIP-Adapter \cite{gao2021clip} (CLIP-A) proposed a simpler alternative to prompt learning for few-shot transfer of VLMs by training a simple Multi-Layer Perceptron (MLP) on top of the fixed image and/or text encoders. To prevent overfitting of the MLP parameters to the few-shot training set, residual ratios were introduced as hyper-parameters. These residual ratios mix the original CLIP encoder's knowledge with that of the finetuned knowledge and are tuned per dataset. Our GPT-A-mlp is very similar to the text variant of CLIP-Adapter with the only difference being that we apply the adapter on top of an average of text encodings from a prompt ensemble constructed from VDT information. CLIP-A applied the MLP on top of a single text encoding from the default prompt. However, our GPT-A-self is different from CLIP-A in that we apply a self-attention mechanism on the set of all sentences for any class, learning the best possible subset of VDT information for the dataset at hand from the few-shot training set. Recently Tip-adapter \cite{ZhangTip-Adapter:Classification} demonstrated state-of-the-art performance on base classes by constructing a simple cache model for the few-shot training dataset. This method however cannot be directly used in the Base-to-New setting as it makes use of the few-shot examples from the test classes to construct the cache model -- hence we don't compare against this approach.

\subsection{Semantic information from Large Language Models}
Recent advancements in language modeling, spearheaded by transformer-based architectures like the GPT family \cite{brown2020language, openai2023gpt4} has showcased remarkable capabilities in understanding and generating human-like text. This ability to extract semantic information is not merely limited to individual sentences but extends to larger, more complex bodies of text. In the vision-language domain, the synergy between image understanding and semantic extraction from LLMs remains a fertile area for exploration. \cite{FerjadNaeemI2MVFormer:Classification} prompts Palm540B LLM \cite{chowdhery2022palm} to create semantic information which is then used to construct unsupervised class embedding vectors for zero-shot classification. However, this work only measures performance for 3 legacy datasets in the generalized ZSL research setting, whereas our work reports results for a modern benchmark of 12 datasets. To the best of our knowledge, our work is the first to prompt GPT-4 for visually descriptive sentences and use them to improve CLIP's zero-shot and few-shot domain transfer performance. 


\section{Methodology}
\subsection{Review of CLIP and CLIP-Adapter}

Contrastive Language-Image Pretraining or CLIP has been shown to perform image classification on an open set of concepts by contrastive pretraining on a large dataset of image-text pairs. The contrastive loss forces images and their corresponding descriptions to be aligned in a joint embedding space while pushing apart the embeddings of dissimilar images and texts. After pretraining, CLIP directly performs image classification on the target dataset without any finetuning. First, we review how the CLIP model performs 0-shot classification on an open set.

The CLIP model consists of two main components: a vision model and a language model. The vision model processes the input image and encodes it into a visual embedding, capturing visual features and concepts. The language model takes the textual caption/alt-text as input and encodes it into a textual embedding, representing the semantic content of the text. During inference, these two embeddings are then compared in the joint embedding space using \textit{cosine similarity}. Given an image \(I\in \mathbb{R}^{H\times W \times C}\), where $H$, $W$, $C$ denotes the height, width, and number of channels of the image, the vision encoder transforms the image into the joint embedding space to get the image features \(f\in \mathbb{R}^{D}\) where $D$ represents the dimension of the features.

During inference, a prompt template such as 'A photo of \{classname\}' is used to generate sentences for $K$ different classes. These are then passed through the text-encoder to obtain the classifier weight matrix \(W\in \mathbb{R}^{D \times K}\) where $K$ is the number of classes. Then we calculate the prediction probabilities by multiplying the image feature $f$ with the classifier weight matrix $W$ and passing through a soft-max function as follows: 

\begin{equation}
    \label{logits}
    % \begin{align}
        f = \mathrm{Backbone}(\mathbf{I}),~~p_i = \frac
        {\operatorname{exp}(\mathbf{W}_{i}^T f) / \tau}
        {\sum_{j=1}^{K} \operatorname{exp}(\mathbf{W}_{j}^T f) / \tau}, 
    % \end{align}
\end{equation}

In CLIP \cite{radford2021learning}, 0-shot domain transfer is achieved by appending domain-specific information into the prompt template. For instance, for photos of birds, a prompt template like 'A photo of a \{class-name\}, a type of bird' is used to generate the classification vectors. The work in \cite{radford2021learning} reports that careful prompt design and prompt ensembling are important to improve 0-shot classification accuracy. Prompt ensembling is achieved by constructing several prompts for each class and then averaging the classification vectors. In our work, we show that prompt ensembles of VDT information can be used to improve the 0-shot domain transfer of CLIP. %%Unlike \cite{RadfordLearningSupervision} which makes use of domain knowledge and trial and error to design the prompts, in this paper we make use of GPT models to construct the prompt ensembles.    %% something about our method here..

CLIP adapter \cite{gao2021clip} is a simple learnable MLP adapter on top of the image encoder features and/or word encoder features to enable few-shot transfer to the target datasets. During few-shot transfer, we are given $N$ images for each class of the base dataset along with their labels. Let \(\left ( x_{i,k},  y_{i,k}\right)_{i=1,k=1}^{i=N,j=K}\) be the image, label pairs comprising the few-shot training set. CLIP adapter constructs $K$ classifier weights using the prompt template $H$ to get the classifier weight \(W\in \mathbb{R}^{D \times K}\) by plugging in the class name \(classname(\left \{ y_{i,k} \right \}) \) into the prompt template $H$ and transforming using the text encoder $g$ as \(W = g(H(classname(\left \{ y_{i,k} \right \}) ))\). Then image features $f$ and text features $W$ are passed through the learnable adapters \(A_{v}\), \(A_{t}\) respectively to obtain the adapted features as follows:

\begin{align}
        f^{\star} &= \alpha A_{v}(f)^{T} + (1-\alpha) f, \\
        \mathbf{W}^{\star} &= \beta A_{t}(\mathbf{W})^{T} + (1 - \beta) \mathbf{W}.
\end{align}

Residual ratios \(\alpha\) and \(\beta\) are hyperparameters that ensure that CLIP's knowledge is mixed with the finetuned knowledge to prevent overfitting of the CLIP-Adapter. The logits are then calculated according to \ref{logits}, and cross entropy loss over the complete training set \(\left ( x_{i,k},  y_{i,k}\right)_{i=1,k=1}^{i=N,j=K}\) is used to learn the parameters of \(A_{v}\), \(A_{t}\).

In the \textit{All} setting, the few-shot transfer is evaluated on a hold-out dataset containing images from the same $K$ classes from which the few-shot training dataset was derived while in the Base-to-New setting proposed in \cite{ZhouConditionalModels}, the few-shot transfer is evaluated on $U$ classes that do not have any overlap with the $K$ classes in the few-shot training set or base set. We evaluate our model on the more practical Base-to-New setting. 

% From clip adapter paper we make use of the variant of the CLIP-Adapter with only visual adaptation for all our comparisons. # write about this in baselines section.

% Figure environment removed

\subsection{Language Model Prompt Design}

In this section, we show that using VDT information in the prompt template improves CLIP's 0-shot transfer capabilities and describe our approach to using an LLM to generate class-specific prompts.

\subsubsection{Visual Descriptive Sentences} \label{sec:vdt}
\cite{RadfordLearningSupervision} demonstrates that careful prompt design and prompt ensembling improve the 0-shot classification performance of CLIP. Here we ask the question: What type of information can be appended to the prompt template to improve the 0-shot domain transfer performance? We show that appending visually descriptive information to the prompt template and ensembling improves the 0-shot performance over the default prompt and prompts containing non-visual
information. 

% in the paragraph below, add an image for explaining the gt attribute extraction process.
We investigate this on the CUB dataset because domain expert annotations are already available or easily obtainable from the internet. We compare the 0-shot performance of a prompt ensemble containing class conditional information that is either visual descriptions of different body parts of the bird or non-visual descriptions of bird calls, migration patterns, behavioral patterns, and habitat.
The CUB dataset consists of images of bird species along with a class attribute vector for each class, containing a score for 4 to 6 different values like color, pattern, shape etc for 28 different body parts of a bird resulting in 312 total scores per bird.  We obtain the visual information by thresholding class attribute vectors such that we use the value for each attribute corresponding to the highest score. We then use these 28 different attribute-value pairs for each bird to construct visual sentences.  Each of these sentences are appended to the prompt template to get a set of class conditional prompts for each class. This set forms the visual descriptive prompts for the dataset. For instance, for the bird Green Heron the prompt changes from `A photo of green heron' to a set of prompts set(`A photo of Green Heron. Green Heron has a greenish-black head cap.', `A photo of Green Heron. Green Heron has a long chestnut-colored neck.',...).To obtain the non-visual information we scrape information about the bird calls, migration patterns, behavioral patterns, and habitat distributions to construct 12 different sentences that are then appended to the prompt template just like before. For Green Heron, this would look like a set(`A photo of Green Heron. The green heron's bird call is a loud, harsh `skeow'', `A photo of Green Heron. Green Heron's are skilled stealth hunters', ...)
We ensemble by averaging the class-level sentence embeddings in CLIP's joint embedding space to get the classification vectors for both the visual set and the non-visual set. This is because CLIP's context space is only 77 tokens long and all the visual information cannot be appended to the prompt template to create one large sentence. In table \ref{table_visual}, we show that the prompt with non-visual information provides no further improvement over the default prompt, while the prompt with visual information provides 4\(\%\) improvement over CLIP.

%%% any other way to show that the classifiers are superior?
%%% t-sne plots?

\begin{table}[]
\caption{Comparing visual and non-visual prompt ensembles for 0-shot domain transfer to the CUB dataset.}
\begin{tabular}{llll}
\toprule
Method                                                                      & Base & New  & H \\
\midrule
\vspace{1em}
\begin{tabular}[c]{@{}l@{}}CLIP \\ default prompt\end{tabular}              & 58.7 & 70.3 & 63.97         \\
\vspace{1em}
\begin{tabular}[c]{@{}l@{}}CLIP \\ default prompt + non-visual-gt\end{tabular} & 57.5 & 71.0 & 63.54         \\
\vspace{1em}
\begin{tabular}[c]{@{}l@{}}CLIP \\ default prompt + visual-gt\end{tabular}     & \textbf{62.3} & \textbf{73.9} & \textbf{67.60}   \\
\vspace{1em}
\begin{tabular}[c]{@{}l@{}}CLIP \\ default prompt + visual-gpt\end{tabular}     & \textbf{62.0} & \textbf{73.1} & \textbf{67.09} \\
\bottomrule
\end{tabular}
\label{table_visual}
\end{table}

\begin{table*}[t]
    \caption{Results of 11 datasets under 16-shots setting with ViT-B/16.}
    \centering
    \tabcolsep=0.8mm
    % \footnotesize
    \begin{NiceTabular}{ccccccccccccc|c}
        \toprule
        Methods & EuroSAT & Caltech101 & \makecell[c]{Oxford\\Flowers} & Food101 & \makecell[c]{FGVC\\Aircraft} & DTD & \makecell[c]{Oxford\\Pets} & \makecell[c]{Stanford\\Cars} & Sun397 & UCF101 & CUB & ImageNet & Average \\
        \midrule
        CLIP & 47.69 & 93.75 & 70.69 & 85.97 & \textbf{24.81} & 43.09 & 89.07 & \textbf{65.55} & 62.61 & \textbf{67.54} & 54.70 &64.51 & 64.16 \\
        CLIP-gpt & \textbf{54.86} & \textbf{94.51} & \textbf{73.40} & \textbf{86.43} & 23.42 & \textbf{50.15} & \textbf{91.54} & 65.01 & \textbf{67.24} & 65.51 & \textbf{57.43} & \textbf{68.9} & \textbf{66.53} \\

        \bottomrule
    \end{NiceTabular}
    \label{table_main_results_zs}
\vspace{-5mm}
\end{table*}

\subsubsection{Prompting LLMs for visually descriptive information}

In the previous section, we show that visually descriptive information from domain experts can be used to construct class conditional prompts that improve 0-shot performance. However, obtaining domain expert annotations is expensive and time-consuming. GPT language models are a good source of knowledge and their knowledge generally scales with size. They can also be easily manipulated to generate output in any manner that we want by simply modifying the prompt. Hence we make use of a simple strategy to prompt GPT-4 to obtain visually descriptive information for any given dataset that can then be used to construct prompt ensembles for CLIP.

Our prompting strategy takes inspiration from chain-of-thought prompting and is as follows: First, we ask GPT-4 to list all the attributes that may be necessary to discriminate between images of the $K$ classes under consideration. Second, we ask GPT-4 to provide the values for all these attributes for all the $K$ classes as sentences. An example for the CUB dataset is show in left side of fig \ref{fig:illustrative}.


The last row in table \ref{table_visual} shows that the GPT-4 generated visual sentences are close to that generated from the class attribute vectors annotated by domain experts. We follow the same simple strategy for all the datasets in the benchmark suite to generate visually descriptive sentences in a scalable and flexible manner and use them to construct prompt ensembles.



\subsection{Simple few-shot adapters for visual sentences}

CLIP-A is prone to overfitting when trained on a few-shot training dataset due to the number of parameters. The authors circumvent this via residual connections that combine the fine-tuned knowledge with the 0-shot domain knowledge from the CLIP encoders.  
%Write about how just using visual information in few-shot transfer would be hard and result in overfitting to spurious features that are not relevant visual identifiers of the class. Is this correct though?  
Few-shot adapters trained using cross-entropy have a fundamental problem in that it's hard to recognize the discriminative features from only a few images of that class resulting in spurious visual features like the environment, co-occurring objects, etc. being used to identify the object. We argue that incorporating VDT can remove this issue and ensure that only the relevant visual concepts for identifying the class are learned by the adapter.
We design two simple adapters that can make use of VDT information and improve the few-shot transfer of CLIP to target datasets. We append a small set of learnable parameters to the output of the word encoder and train the network using cross-entropy loss. The two different versions are:
\begin{enumerate}
\item  CLIP-A-mlp is a 3-layer MLP that takes the average of sentence embeddings as input.
\item CLIP-A-self adapter is a self-attention layer that applies attention over the embeddings of the different sentences for each class and averages the output. 
\end{enumerate}
Given we have $M$ GPT generated sentences for each of the $K$ classes \(t_{m,k}\), we first construct the $M$ prompts by appending each sentence to the prompt template like  \(H(classname(y_{i,k} ), \left \{ t_{m,k} \right \})\) and pass them through CLIP's word encoder to get the classification weight matrix \(W^{sent}\in \mathbb{R}^{D \times M \times K}\). Then for CLIP-A-mlp we use a 3-layer MLP \(B_{mlp}\) as follows:

\begin{align}
    W_{avg} = 1/M \sum _{m=1}^{M} W_{m,k}^{sent}  \\
    W^{\star} = \beta B_{mlp}(\mathbf{W_{avg}})^{T} + (1 - \beta) \mathbf{W_{avg}}
\end{align}

For the self-attention adapter, we apply vanilla self-attention \cite{VaswaniAttentionNeed} over all the visual descriptive sentences such that during training it learns to pick the most relevant visual sentences for identifying each class. Just like before, we first obtain the classification vector for all sentences \(W^{sent}\in \mathbb{R}^{K \times M \times D}\) and pass them as the key, query, and value to the self-attention module \(B_{self}\) and average out the output tokens to get the final classification vector \(W^{\star}\). Here the attention is applied over the $M$ different visually descriptive sentences. 

\begin{align}
    W_{avg} = 1/M \sum _{m=1}^{M} W_{m,k}^{sent}  \\
    W^{adapted}_{m,k} = B_{self}(\left \{ W^{sent}_{m,k} \right \}_{1}^{M}, \left \{ W^{sent}_{m,k} \right \}_{1}^{M}, \left \{ W^{sent}_{m,k} \right \}_{1}^{M})\\
    W_{adapted mean} = 1/M \sum _{m=1}^{M} W_{m,k}^{adapted}  \\
    W^{\star} = \beta \mathbf{W_{adapted mean}}^{T} + (1 - \beta) \mathbf{W_{avg}}
\end{align}

In either case, we finally obtain the new adapter classifier weights \(W^{\star}\in \mathbb{R}^{D \times K}\) that have been adapted to focus on the most visually discriminative information among the $M$ visually descriptive sentences. We make use of \ref{logits} to calculate the probabilities and predict the image category by selecting the class with the highest probability.

During the few-shot training only the weights of the adapter network (either \(B_{mlp}\) or \(B_{self}\)) are trained using cross-entropy loss.


\begin{table*}[t]
    \tabstyle{6pt}
    \caption{\textbf{Comparison of GPT-Adapters with CLIP, CoOp and CoCoOp in the Base-to-New generalization setting}. For prompt learning-based methods (CoOp and CoCoOp), their prompts are learned from the base classes (16 shots). The results strongly justify the importance of including extra visual information. H denotes Harmonic mean (to highlight the generalization trade-off~\cite{xian2017zero}).}
    \label{table_results_generalization}
    \begin{subtable}[t]{.3\textwidth}
    \centering
    \caption{\textbf{Average over 12 datasets}.}
    \begin{tabular}{l cc|c}
    \toprule
    & Base & New & H \\
    \midrule
    CLIP & 69.34 & 74.22 & 71.70 \\
    CoOp & \textbf{82.69} & 63.22 & 71.66 \\
    CoCoOp & 80.47 & 71.69 & 75.83 \\
    CLIP-A &  78.90 & 72.14 & 75.07 \\
    \rowcolor{tabhighlight}
    GPT-A-mlp &  80.06 & \textbf{75.24} & 77.42 \\
    \rowcolor{tabhighlight}
    GPT-A-attn &  82.12 & 74.20 & \textbf{77.78} \\
    \bottomrule
    \end{tabular}
    \end{subtable}
    \vspace{1em}  
    \begin{subtable}[t]{.3\textwidth}
    \centering
    \caption{CUB.}
    \begin{tabular}{l cc|c}
    \toprule
    & Base & New & H \\
    \midrule
    CLIP &  58.7 & 70.3 & 63.90 \\
    CoOp & 79.2 & 53.3 & 63.71 \\
    CoCoOp & 67.1 & 74.1 & 70.40 \\
    CLIP-A &  68.3 & 70.8 & 69.53 \\
    \rowcolor{tabhighlight}
    GPT-A-mlp &  74.1 & 72.8 & 73.44 \\
    \rowcolor{tabhighlight}
    GPT-A-attn &  \textbf{78.6} & \textbf{71.3} & \textbf{74.77} \\
    \bottomrule
    \end{tabular}
    \end{subtable}
    ~
    \begin{subtable}[t]{.3\textwidth}
    \centering
    \caption{Caltech101.}
    \begin{tabular}{l cc|c}
    \toprule
    & Base & New & H \\
    \midrule
    CLIP & 96.84 & 94.00 & 95.40 \\
    CoOp & 98.00 & 89.81 & 93.73 \\
    CoCoOp & 97.96 & 93.81 & 95.84 \\
    CLIP-A &  97.7 & 93.6 & 95.61 \\
    \rowcolor{tabhighlight}
    GPT-A-mlp &  97.5 & 95.7 & 96.59 \\
    \rowcolor{tabhighlight}
    GPT-A-attn &  \textbf{98.3} & \textbf{95.9} & \textbf{97.09} \\
    \bottomrule
    \end{tabular}
    \end{subtable}
    ~
    \begin{subtable}[t]{.3\textwidth}
    \centering
    \caption{OxfordPets.}
    \begin{tabular}{l cc|c}
    \toprule
    & Base & New & H \\
    \midrule
    CLIP & 91.17 & 97.26 & 94.12 \\
    CoOp & 93.67 & 95.29 & 94.47 \\
    CoCoOp & \textbf{95.20} & \textbf{97.69} & \textbf{96.43} \\
    CLIP-A &  94.8 & 97.0 & 95.89 \\
    \rowcolor{tabhighlight}
    GPT-A-mlp &  95.5 & 96.9 & 96.19 \\
    \rowcolor{tabhighlight}
    GPT-A-attn &  94.4 & 97.0 & 95.68 \\
    \bottomrule
    \end{tabular}
    \end{subtable}
    \vspace{1em}
    \begin{subtable}[t]{.3\textwidth}
    \centering
    \caption{StanfordCars.}
    \begin{tabular}{l cc|c}
    \toprule
    & Base & New & H \\
    \midrule
    CLIP & 63.37 & \textbf{74.89} & 68.65 \\
    CoOp & \textbf{78.12} & 60.40 & 68.13 \\ 
    CoCoOp & 70.49 & 73.59 & 72.01 \\
    CLIP-A &  70.5 & 73.3 & 71.87 \\
    \rowcolor{tabhighlight}
    GPT-A-mlp &  72.2 & 73.1 & 72.65 \\
    \rowcolor{tabhighlight}
    GPT-A-attn &  76.8 & 72.9 & \textbf{74.80} \\
    \bottomrule
    \end{tabular}
    \end{subtable}
    ~
    \begin{subtable}[t]{.3\textwidth}
    \centering
    \caption{Flowers102.}
    \begin{tabular}{l cc|c}
    \toprule
    & Base & New & H \\
    \midrule
    CLIP & 72.08 & \textbf{77.80} & 74.83 \\
    CoOp & \textbf{97.60} & 59.67 & 74.06 \\
    \rowcolor{tabhighlight}
    CoCoOp & 94.87 & 71.75 & 81.71 \\
    CLIP-A &  94.6 & 71.5 & 81.44 \\
    \rowcolor{tabhighlight}
    GPT-A-mlp &  98.1 & 76.2 & \textbf{85.77} \\
    \rowcolor{tabhighlight}
    GPT-A-attn &  97.4 & 75.3 & \textbf{84.94} \\
    \bottomrule
    \end{tabular}
    \end{subtable}
    ~
    \begin{subtable}[t]{.3\textwidth}
    \centering
    \caption{Food101.}
    \begin{tabular}{l cc|c}
    \toprule
    & Base & New & H \\
    \midrule
    CLIP & 90.10 & 91.22 & 90.66 \\
    CoOp & 88.33 & 82.26 & 85.19 \\
    CoCoOp & \textbf{90.70} & \textbf{91.29} & \textbf{90.99} \\
    CLIP-A &  90.3 & 91.2 & 90.75 \\
    \rowcolor{tabhighlight}
    GPT-A-mlp &  90.6 & 91.6 & 91.10 \\
    \rowcolor{tabhighlight}
    GPT-A-attn &  90.4 & 91.2 & 90.80 \\
    \bottomrule
    \end{tabular}
    \end{subtable}
    \vspace{1em}
    \begin{subtable}[t]{.3\textwidth}
    \centering
    \caption{FGVCAircraft.}
    \begin{tabular}{l cc|c}
    \toprule
    & Base & New & H \\
    \midrule
    CLIP & 27.19 & \textbf{36.29} & 31.09 \\
    CoOp & \textbf{40.44} & 22.30 & 28.75 \\
    CoCoOp & 33.41 & 23.71 & 27.74 \\
    CLIP-A &  34.9 & 33.5 & 34.19 \\
    \rowcolor{tabhighlight}
    GPT-A-mlp &  35.0 & 35.0 & 35.00 \\
    \rowcolor{tabhighlight}
    GPT-A-attn &  37.8 & 33.0 & \textbf{35.24} \\
    \bottomrule
    \end{tabular}
    \end{subtable}
    ~
    \begin{subtable}[t]{.3\textwidth}
    \centering
    \caption{SUN397.}
    \begin{tabular}{l cc|c}
    \toprule
    & Base & New & H \\
    \midrule
    CLIP & 69.36 & 75.35 & 72.23 \\
    CoOp & \textbf{80.60} & 65.89 & 72.51 \\
    \rowcolor{tabhighlight}
    CoCoOp & 79.74 & 76.86 & 78.27 \\
    CLIP-A &  80.1 & 75.9 & 77.94 \\
    \rowcolor{tabhighlight}
    GPT-A-mlp &  80.2 & \textbf{78.4} & \textbf{79.29} \\
    \rowcolor{tabhighlight}
    GPT-A-attn &  81.4 & 76.8 & 79.03 \\
    \bottomrule
    \end{tabular}
    \end{subtable}
    ~
    \begin{subtable}[t]{.3\textwidth}
    \centering
    \caption{DTD.}
    \begin{tabular}{l cc|c}
    \toprule
    & Base & New & H \\
    \midrule
    CLIP & 53.24 & 59.90 & 56.37 \\
    CoOp & 79.44 & 41.18 & 54.24 \\
    CoCoOp & 77.01 & 56.00 & 64.85 \\
    CLIP-A &  74.9 & 53.0 & 62.08 \\
    \rowcolor{tabhighlight}
    GPT-A-mlp &  75.8 & 62.0 & 68.21 \\
    \rowcolor{tabhighlight}
    GPT-A-attn &  \textbf{81.8} &\textbf{62.3} & \textbf{70.73} \\
    \bottomrule
    \end{tabular}
    \end{subtable}
    ~
    \begin{subtable}[t]{.3\textwidth}
    \centering
    \caption{EuroSAT.}
    \begin{tabular}{l cc|c}
    \toprule
    & Base & New & H \\
    \midrule
    CLIP & 56.48 & 64.05 & 60.03 \\
    CoOp & \textbf{92.19} & 54.74 & 68.69 \\
    CoCoOp & 87.49 & 60.04 & 71.21 \\
    CLIP-A &  82.5 & 62.4 & 71.06 \\
    \rowcolor{tabhighlight}
    GPT-A-mlp &  85.7 & \textbf{72.6} & \textbf{78.61} \\
    \rowcolor{tabhighlight}
    GPT-A-attn &  88.5 & 70.5 & 78.48 \\
    \bottomrule
    \end{tabular}
    \end{subtable}
    ~
    \begin{subtable}[t]{.3\textwidth}
    \centering
    \caption{UCF101.}
    \begin{tabular}{l cc|c}
    \toprule
    & Base & New & H \\
    \midrule
    CLIP & 70.53 & \textbf{77.50} & 73.85 \\
    CoOp & \textbf{84.69} & 56.05 & 67.46 \\
    CoCoOp & 82.33 & 73.45 & 77.64 \\
    CLIP-A &  82.9 & 74.9 & 78.70 \\
    \rowcolor{tabhighlight}
    GPT-A-mlp &  80.2 & 78.6 & 79.39 \\
    \rowcolor{tabhighlight}
    GPT-A-attn &  84.1 & 76.4 & \textbf{80.07} \\
    \bottomrule
    \end{tabular}
    \end{subtable}
    ~
    \begin{subtable}[t]{.3\textwidth}
    \centering
    \caption{ImageNet.}
    \begin{tabular}{l cc|c}
    \toprule
    & Base & New & H \\
    \midrule
    CLIP & 72.43 & 68.14 & 70.22 \\
    CoOp & \textbf{76.47} & 67.88 & 71.92\\
    \rowcolor{tabhighlight}
    CoCoOp & 75.98 & \textbf{70.43} & \textbf{73.10} \\
    CLIP-A &  75.4 & 68.6 & 71.84 \\
    \rowcolor{tabhighlight}
    GPT-A-mlp &  75.9 & 75.0 & 72.83 \\
    \rowcolor{tabhighlight}
    GPT-A-attn &  76.4 & 68.3 & 72.12 \\
    \bottomrule
    \end{tabular}
    \end{subtable}
    
\end{table*}

% \begin{table}[]
% \begin{tabular}{llll}
% Dataset & Class   & Top 3                                                                                                                             & Bottom 3                                                                                                   \\
% FGVC    & 707-100 & The aircraft's most unique visual identifier is its distinctive upper deck 'hump,' which is often associated with the 747 series. & The color scheme varies greatly as it depends on the airline operating the aircraft.                       \\
%         &         & The Boeing 747-100 does not feature canards.                                                                                      & The model number of this aircraft is 747-100, indicating its specific version within Boeing's 747 series.  \\
%         &         & It sports a traditional tail type, consisting of a horizontal stabilizer and a vertical fin.                                      & Primarily, this aircraft serves a commercial function, transporting passengers.                            \\
% Cars    & BMW M5  & The fenders are contoured and designed to accommodate the wide tires for performance.                                             & The interior is luxurious, with leather seats and a driver-focused cockpit.                                \\
%         &         & The body shape is a four-door sedan, recognized for its sleek and aerodynamic design.                                             & The body details include M5 badging and unique side vents, distinguishing it from regular 5 Series models. \\
%         &         & The rear spoiler is subtle and integrated into the trunk lid for added downforce.                                                 & The brand logo is the distinctive blue and white roundel of BMW.                                           \\
% UCF-101 & Diving  & diving requires sports equipment, particularly a diving suit and sometimes an oxygen tank.                                        & Diving uses whole body muscles.                                                                            \\
%         &         & interaction with equipment like diving suit, oxygen tank, and sometimes, underwater photography gear.                             & The use of force is usually medium to high.                                                                \\
%         &         & The posture during diving can be anything from standing (before the dive) to swimming postures underwater.                        & The main types of motion involved are jumping and swimming.'                                              
% \end{tabular}
% \end{table}

\begin{table*}[]
\begin{NiceTabular}{l|l|l}
\toprule
\textbf{Dataset} & \textbf{Top 3 attributes selected}                                                                                        & \textbf{Bottom 3 attributes selected}                                                    \\
\midrule
\rule{0pt}{2ex}
FGVC    & Unique visual identifier, presence of canards, tail type.                                     & Color scheme, model number, commercial or cargo.             \\
\rule{0pt}{3ex}    
Cars    & Body shape, fender description, spoiler description,                                          & Interior description, brand logo description, color scheme   \\
\rule{0pt}{3ex}    
UCF-101 & \makecell[tl]{Description of equipment involved, \\
Posture of person during action,  \\ Interaction information.} & \makecell[tl]{Body muscles used, force involved, \\ speed and range of motion}
% \bottomrule
\end{NiceTabular}
\caption{The top 3 and bottom 3 attributes selected by the attention mechanism in GPT-A-self for 3 different datasets. For UCF101, We see that attention learns to pick visually descriptive sentences like posture and description of objects over temporal information like speed of motion and force applied.}
\label{table:attn_wts}
\end{table*}


\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|llll}
\toprule
Prompting      & ZS   & \begin{tabular}[c]{@{}l@{}}Base\end{tabular} & \begin{tabular}[c]{@{}l@{}}New\end{tabular} & \begin{tabular}[c]{@{}l@{}}H\end{tabular} \\
\midrule
Default        & 54.7 & NA                                                        & NA                                                       & NA                                                     \\
OpenAssistant & 56.0 & 78.3                                                      & 69.8                                                     & 73.80                                                  \\
GPT-3.5        & 55.7 & 78.1                                                      & 70.6                                                     & 74.16                                                  \\
GPT-4          & 57.4 & 78.6                                                      & 71.3                                                     & 74.77                                                 
\end{tabular}%
}
\caption{Comparing different GPT models for obtaining the VDT information. We see that the larger models provide higher quality VDT information but CLIP-A-self is capable of producing generalizable classifiers even with smaller models like OpenAssistant.}
\label{table:gpt_models}
\end{table}

\section{Experiments}
We evaluate the importance of visual sentence ensembles in the following two problem settings: (i) We evaluate the quality of the visual sentences themselves by comparing an ensemble of visual sentence prompts with that of the default CLIP prompt for each dataset on a benchmark suite of 12 datasets; (ii)  We compare the performance of adapters that make use of these visual sentence prompts with other few-shot transfer methods on generalization from Base-to-New classes within a dataset. Before discussing the results we provide information about the datasets and experimental setup.

\subsection{Datasets}
For both settings, we make use of the 11 datasets described in \cite{ZhouLearningModelsb}, which cover a diverse range of image recognition tasks from fine to coarse-grained, as well as image recognition tasks ranging in various degrees of similarity with CLIP's pretraining dataset. We also evaluate all methods on the CUB dataset of bird species \cite{Caltech-UCSD200} which is frequently used in the 0-shot and few-shot learning literature \cite{SchonfeldGeneralizedAutoencoders, Maniparambil2022BaseTransformers:Learning} to expand the dataset suite to 12 datasets. Specifically, the datasets are ImageNet~\cite{deng2009imagenet} and Caltech101~\cite{fei2004learning} for classification on generic objects; OxfordPets~\cite{parkhi2012cats}, StanfordCars~\cite{krause20133d}, Flowers102~\cite{nilsback2008automated}, Food101~\cite{bossard2014food} and FGVCAircraft~\cite{maji2013fine} for fine-grained classification; SUN397~\cite{xiao2010sun} for scene recognition; UCF101~\cite{soomro2012ucf101} for action recognition; DTD~\cite{cimpoi2014describing} for texture classification; EuroSAT~\cite{helber2019eurosat} for satellite imagery recognition; and ~\cite{Caltech-UCSD200} for bird species identification. For 0-shot transfer with visual sentences, we test on the test set of all these datasets including all the classes in each dataset. For the Base-to-New setting, we follow \cite{ZhouConditionalModels}, where we sample an equal number of classes for the base set and the new set with no overlap. For the CUB dataset we use the common split for generalized 0-shot learning from ZSL literature \cite{XianZero-ShotUglyc} where 150 classes belong to the base set and 50 classes belong to the new set. Similar to \cite{ZhouConditionalModels} we evaluate our adapters on the 16-shot setting to enable easy comparison with other methods that evaluate on the Base-to-New setting. Plots detailing the performance of our method on lower shot settings for all datasets are presented in the supplementary material.

\subsection{Baselines}
We compare the performance of visual sentences ensemble on 0-shot transfer against the CLIP model \cite{RadfordLearningSupervision} whose default prompts for each dataset have been extensively fine-tuned using a test set.
We compare our adapter models against two prompt learning methods CoOp \cite{ZhouLearningModelsb} which learns static prompts and CoCoOp \cite{ZhouConditionalModels} which learns a dynamic prompt that is specifically designed to improve Base-to-New transfer. We also compare our GPT-Adapters against CLIP-A \cite{gao2021clip} due to the similarity in architecture and to show that the performance improvements are from making use of the visual sentences and not from the architecture itself.
\subsection{Training settings}
Our implementation is based on CoOp's and CLIP-A's code. \footnote{\url{https://github.com/KaiyangZhou/CoOp}, \url{https://github.com/gaopengcuhk/CLIP-Adapter}}
 We make all our comparisons on the best CLIP backbone i.e., VIT-B/16. We take the results for CoOp and CoCoOp for all datasets (except CUB) from their respective papers, while we make use of practices from the respective papers like context length set to 4 and context initialization to ``a photo of" to ensure the best results on the CUB dataset. For CLIP-A, we re-run all experiments on VIT-B/16 backbone as they were not reported in the paper. For all adapter models including ours, we only tune the residual ratio \(\beta\) hyper-parameter. For CLIP-A, we use the version where the MLP is applied on top of the visual encoder as it performed the best \cite{GaoCLIP-Adapter:Adapters}.

\subsection{GPT generated visual sentences improve 0-shot transfer.}

We compare the performance of CLIP-gpt prompt ensemble with the default prompts of CLIP in Table \ref{table_main_results_zs}. We can see that the GPT-generated prompt ensemble improves upon the performance of CLIP 0-shot by ~2\(\%\) on average over 12 datasets. The improvement over CLIP-ZS is significant; over 5\(\%\) for specialized fine-grained datasets like CUB, SUN397, EuroSAT, and DTD and over 2\(\%\) for oxford-flowers and oxford-pets. This shows that CLIP does not recognize several of the classnames in these datasets and describing the class in the form of visually descriptive sentences results in better classifiers from the text-encoder and better classification accuracy. It is also worth noting that only including the visually descriptive sentences in the prompts can help improve the performance of general datasets like Imagenet (over 4\(\%\)) and Caltech-101 (over 1\(\%\)) too. For all other datasets, the transfer performance matches that of CLIP, with the exception being the action recognition dataset UCF-101. We inspected the sentences generated for UCF-101 and notice that several of the sentences generated by GPT involves temporal information instead of visual descriptions and we believe this could be the reason for the drop in accuracy. However, we notice in section \ref{sec:attn_wts} that the self-attention module of the few-shot adapter learns to pick only the visual sentences out of the generated sentences which might explain the improvement in performance of few-shot adapters in the new setting in section \ref{sec:adapter_results}. 


% \subsection


\subsection{GPT-Adapters improve few-shot transfer performance.}
\label{sec:adapter_results}

We compare the performance of our GPT-Adapters against CLIP, CoOp, and CoCoOp on the benchmark suite of 12 datasets in the Base-to-New setting in table \ref{table_results_generalization}. Here we see that GPT-Adapters that make use of the VDT information outperform CoCoOp by \(3\%\) in the new setting while maintaining similar performance to that of CoOp in the base setting on the average accuracy over 12 datasets. This is impressive considering that CoCoOp makes use of a meta-network and forward pass through the text encoder making it computationally intensive to train. CoCoOp takes up to 5 hours to train on 16-shot ImageNet for VIT-B/16 encoder, in comparison,  our CLIP-A-self takes only 10 mins (on an RTX 3090 GPU). The Base-to-New generalization ability of our adapters is even more impressive for fine-grained, specialized datasets as evidenced by the gains over CoCoOp in Harmonic mean of base and new accuracy. For example, CLIP-A-self demonstrates gains in datasets like FGVCAircraft (~7.5\(\%\)), EuroSat (~7.4\(\%\)), DTD (~5.8\(\%\)), CUB (~4.3\(\%\)), Flowers102 (~4\(\%\)),  Stanford Cars (~2.4\(\%\)) and UCF-101 (~2.4\(\%\) ). This demonstrates that our adapters make use of semantic information in the form of visually descriptive sentences and fuse this with CLIP's 0-shot knowledge to build more generalizable classifiers that transfer well to unseen classes within the same dataset. It is also worth noting that even though the same set of VDT did not provide any improvements in 0-shot domain transfer for datasets like FGVC-Aircraft, Stanford-Cars and UCF-101, our self-attention adapter was able to choose the most informative subset of VDT and produce few-shot classifiers that provide substantial few-shot transfer performance gains in comparison to CoCoOp. We show in section \ref{sec:attn_wts} the sentences picked by the attention mechanism for these datasets to qualitatively verify this.

%% comparison between mlp and attn adapter 
%%% what to write here.. Artificial experiment where we add non-visual information and it shows how accuracy of mlp reduces while accuracy of attn stays the same.

\subsubsection{Attention weights Analysis}
\label{sec:attn_wts}
We note that even though CLIP-gpt ensembles were outperformed by CLIP default prompt on FGVC Aircraft, UCF-101, and Stanford Cars dataset, we see that GPT-A-attn outperforms CLIP-A and CoCoOp \cite{ZhouConditionalModels} on these datasets in the few-shot transfer settings. We believe that this is because, during few-shot training, the self-attention mechanism learns to select the most relevant visual sentences out of the set of visually descriptive text and helps produce generalizable classifiers. In Table, we show the top 3 and bottom 3 attributes picked by attention scores for each of these datasets and show that the sentences with the highest attention scores correspond to visually descriptive attributes in the set and vice versa for the lowest scored attributes. For example, for both Stanford Cars and FGVC it is interesting to see that the color scheme is one of the least used attributes as it's difficult to identify a car or a plane from its color alone, while for datasets like Oxford-Flowers and CUB, the color of the class is the most important attribute. For UCF-101, information like the force involved or temporal information like speed and range of motion of the action is unlikely to be encoded in the image and hence is not selected by the attention mechanism. Information regarding the subject and the object of the action, like the posture of the person, description of the object, and interaction between objects are visible in the images and hence weighted highly by the attention mechanism.

\subsection{Ablation over different GPT models}
In this section, we see if other GPT models like GPT-3.5 and open-source model, OpenAssistant, are as capable as GPT-4 in generating visually descriptive information. We explore this on the CUB dataset as it is fine-grained and specialized. The results are presented in Table \ref{table:gpt_models}. We find that the performance improves with larger models which are more capable of memorizing accurate class information with less hallucination \cite{YuKoLA:Models}. Even though we obtain decent performance with the open-source model OpenAssistant, the outputs were always inconsistent and noisy, resulting in a lot of clean-up effort in comparison to GPT-3.5 and GPT-4 where the outputs were in the form of concise sentences following a dictionary format.
It is worth noting that our few-shot adapter is capable of picking out the the best VDT information even from a noisy set, pushing the Base-to-New generalization performance of OpenAssistant, and GPT-3.5 close to that of GPT-4. We provide results for all datasets in the benchmark using GPT-3.5 in the supplementary material.

% \subsection{Ablation over the number of heads in self-attention}


% \subsection{Lower shots results}

%% run imagenet for coop and co-co-op at 1 shot and 5-shot and 10-shot and show how our method is better? %% not so impressive. so leave it out, add the plots in supplementary. write about it in results




%------------------------------------------------------------------------



%-------------------------------------------------------------------------




%-------------------------------------------------------------------------

%------------------------------------------------------------------------
\section{Conclusion}
In this work, we show that using visually descriptive textual (VDT) information can improve the 0-shot domain transfer performance of CLIP over non-visual information and the default prompts. We demonstrate GPT-4 to be an accurate and flexible source of VDT information by improving the 0-shot domain transfer performances on a suite of 12 benchmark datasets. Our few-shot adapter CLIP-A-self learns to pick the best VDT information from the GPT generated set and improve the few-shot domain transfer in the Base-to-New setting even when the quality of the generated text deteriorates. We release all prompts and VDT information for all 12 datasets to promote further research in the fertile research direction of using LLMs for learning multi-modal adapters for foundation models.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}