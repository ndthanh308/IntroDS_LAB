\section{Differentiable optimization}
In this section, we study the derivative of convex optimization programs over the parameters of the objective function. We show that the gradient in Eq.~\ref{eq:chain-rule} is often zero outside of the optimum, and hence it causes gradient-following methods to get stuck in suboptimal solutions. In the second part of this section, we introduce a method to solve this problem. 

Without loss of generality, we consider a single instance of the problem, i.e.,~one sample $(o, w)\in\mathcal{D}.$ Everywhere in this section, we denote the prediction by $\hat{w}=\phi_\theta(o).$ Then, the decision is computed as a solution of the internal optimization problem defined as follows:
\begin{equation}
    x^\ast(\hat{w}) = \argmax_{x} f(x, \hat{w}) \text{ s.t. } x\in\mathcal{C}.
    \label{eq:int}
\end{equation}
We use $\hat{x}$ to denote the value of $x^\ast(\hat{w})$ for a given prediction $\hat{w}.$ As we are interested in convex optimization problems, we make the following assumptions:
% f(x) is convex
\begin{assumption} 
The objective function $f(x, w)$ is concave and twice continuously differentiable in $x$ for any ${w}.$ \end{assumption}
% C is convex, i.e., it is defined as g_i(x) <= 0 for some convex g_i.
\begin{assumption}
    The feasibility region $\mathcal{C}$ is convex, i.e., $\{\mathcal{C}=\{x|g_i(x)\leq 0, i=1,\dots,l\},$ where $g_i(x)$ are convex differentiable functions. Moreover, for any $x\in\mathcal{C},$ the gradients $\{\nabla_{x}g_i(x)|g_i(x)=0\}$ of the active constraints are linearly independent. \footnote{As is, Assumption 2 does not allow equality constraints. For clarity, we use this formulation in the main body of the paper. In the appendix, we show that our results hold for the equality constraints as well.}
\end{assumption}
Additionally, we make an assumption about how $f$ depends on $w$, which holds for many real-world problems, including linear and quadratic optimization problems.

\begin{assumption}
    The objective function $f(x, w)$ is twice continuously differentiable in $w.$
\end{assumption}

Throughout this paper, we use derivatives of different objects. For clarity, we first provide an overview of them: the gradient of the true objective function over the decision, $\nabla_{x} f(\hat{x},w);$ the Jacobian of the decision over the prediction, $\nabla_{\hat{w}}x^\ast(\hat{w});$ the Jacobian of the prediction over the ML model parameters, $\nabla_{\theta}\hat{w};$ and the gradient of the predicted objective in the internal problem, $\nabla_x f(x, \hat{w}).$ In the next section, we establish some crucial properties of the Jacobian $\nabla_{\hat{w}}x^\ast(\hat{w}).$

\subsection{The zero-gradient theorem}
We begin by investigating the relation between the values of the function $x^\ast(\hat{w})$ and the gradient of the internal objective, $\nabla_{x}f(x, \hat{w}).$
Let $n_i:=\nabla_{x}g_i(\hat{x}),\,i=1,\dots,\,l$ be the normal vectors of the constraints at $\hat{x},$ Then, the KKT conditions \cite{kkt} at $\hat{x}$ state that there exist real values $\alpha_1,\ldots,\alpha_l$ such that the following holds:
\begin{equation*}
\nabla_{x}f(\hat{x}, \hat{w}) = \sum_{i=1}^{l}\alpha_in_i,\quad
\alpha_ig_i(\hat{x})=0, \quad \alpha_i \geq 0, \quad g_i(\hat{x})\leq0,\quad i=1,\dots,l.    
\end{equation*}

%$$g_i(\hat{x})\leq0, i=1,\dots,l.$$
Under Assumptions 1 and 2, the KKT multipliers $\alpha_i$ are uniquely defined by $\hat{w}$ and $\hat{x}.$ Thus, as $\hat{x}$ is defined by $\hat{w},$ we sometimes write $\alpha_i(\hat{w})$ to emphasize that it is, in fact, a function of $\hat{w}.$
To provide a geometrical perspective on the KKT conditions, we introduce the following definition:

\begin{definition} 
Let $x\in\mathcal{C}$ and let ${I(x)=\{i|g_i(x)=0\}}$ be the set of indices of the constraints active at $x.$ Let $n_i=\nabla_{x} g_i(x),\,\forall i\in I(x)$, be the normal vectors of these constraints. The \textnormal{gradient cone},
$ G(x):=\Big\{\sum_{i\in I}\alpha_in_i|\alpha_i\geq 0 \Big\}, $ 
is the positive linear span of normal vectors $n_i.$
\end{definition}

Combining the KKT conditions with Definition 3.1, we immediately arrive at the following property:
% For x inside C, G(x) is degenerate

\begin{property}
    Let $x\in\mathcal{C}$ and let $\nabla_{x}f(x, \hat{w})$ be the internal gradient at $x.$ Then, $x$ is a solution to the problem in Eq. \ref{eq:int} if and only if \,$\forall i \in I(x),\exists \alpha_i\geq 0,$ such that 
    $\nabla_{x}f(x,\hat{w})=\sum_{i\in I(x)} \alpha_in_i\in G(x),$
    where $I(x)$ is the set of indices of active constraints, $I(x)=\{i|g_i(x)=0\}.$
\end{property}

While trivial, this property provides a geometrical interpretation of the problem. Effectively, a point $x$ is a solution to the problem in Eq. \ref{eq:int} if and only if the internal gradient at this point lies inside its gradient cone. Figure \ref{fig:zg_cone} illustrates this property. 

Before studying the Jacobian $\nabla_{\hat{w}}x^\ast(\hat{w}),$ we first need to address the question of when this Jacobian exists. Sufficient conditions for existence are given in \citet{fiacco1976sensitivity}. Under Assumptions 1-3, these conditions can be reformulated as follows:

\begin{lemma}[Theorem 2.1 in \citet{fiacco1976sensitivity}]
    Let Assumptions 1-3 hold and let 
    \begin{equation*}
    \nabla_{x}f(\hat{x},\hat{w})=\sum_{i\in I(\hat{x})}\alpha_i (\hat{w})n_i
    %,\quad\alpha_i(\hat{w})\geq0, \;\forall i\in I(\hat{x})\,,
    \end{equation*}
    be the representation of the internal gradient with the normals of the active constraints. Then, suppose that the \textnormal{strict complementary slackness condition} holds, i.e., $\alpha_i(\hat{w})>0,\,\forall i\in I(\hat{x}).$
    Then, the Jacobian $\nabla_{\hat{w}}x^\ast(\hat{w})$ exists at $\hat{w}.$ Moreover, $\alpha_i(\cdot)$ is continuous around $\hat{w}$ for any $i\in I(\hat{x}).$
\end{lemma}

%\begin{wrapfigure}{l}{0.5 \textwidth}
% Figure environment removed
%\end{wrapfigure}

Proof of this lemma is given in \citet{fiacco1976sensitivity}. This result establishes that strict complementary slackness is sufficient for the Jacobian $\nabla_{\hat{w}}x^\ast(\hat{w})$ to exist. 
In most cases, the points that violate strict complementary slackness form a zero-measure set and hence can be neglected in practice. 

Now, we have all the necessary tools to describe the structure of the Jacobian $\nabla_{\hat{w}}x^\ast(\hat{w}).$
Suppose that the strict complementary slackness condition holds at $\hat{x}$ and hence the Jacobian exists. 
Assume that we perturb $\hat{w}$ and obtain $\hat{w}'.$ Let  $\hat{x}'=x^\ast(\hat{w}')$ denote the solution corresponding to $\hat{w}'.$ What can be said about $\hat{x}'?$ Strict complementary slackness implies that the constraints active at $\hat{x}$ will remain active at $\hat{x}'$ if the difference $\|\hat{w}' - \hat{w}\|_2^2$ is small enough. Therefore, the decision $\hat{x}'$ can only move within the tangent space of $\mathcal{C}$ at $\hat{x}$, i.e., orthogonally to all $n_i,\, i\in I(\hat{x}.)$  Hence, when more constraints are active, $\hat{x}'$ can move in less directions. Formally, we obtain the following lemma:

\begin{lemma}
Suppose that the strict complementary slackness conditions hold at $\hat{x}$ and let $\nabla_{x}f(\hat{x}, \hat{w})=\sum_{i\in I(\hat{x})}\alpha_in_i,$ $ \alpha_i> 0,\; \forall i \in I(\hat x)$
be the internal gradient. Let 
$\mathcal{N}(\hat{x})=span(\{n_i \,|\, i\in I(\hat{x})\})$
be the linear span of the gradient cone.
Then $\mathcal{N}(\hat{x})$ is contained in the left null space of $\nabla_{\hat{w}} x^\ast(\hat{w}),$ i.e., $v\,\nabla_{\hat{w}}x^\ast(\hat{w})=0,\,\forall v\in\mathcal{N}(\hat{x})$
\end{lemma}
The formal proof of this result can be found in the appendix. Lemma 3.4 is very important, as it specifies in what directions $x^\ast(\hat{w})$ \textit{can move} as a consequence of changing $\hat{w}.$ Now, the first term in the chain rule in Eq. \ref{eq:chain-rule}, $\nabla_{x}f(\hat{x}, w),$ specifies in what directions $x^\ast(\hat{w})$ \textit{should} move in order for the true objective to increase. Naturally, if these directions are contained in the null space of $\nabla_{\hat{w}}x^\ast(\hat{w}),$ then the total gradient in Eq.~\ref{eq:chain-rule} is zero. This observation constitutes the main theorem of this paper -- the zero-gradient theorem.

% THE THEOREM!
\begin{theorem}[Zero-gradient theorem] Let $\hat{w}$ be a prediction, and let $\hat{x}$ be the solution of the internal optimization problem defined in Eq. \ref{eq:int}. Suppose that the strict complementary slackness conditions hold at $\hat{x}$ and let
$\mathcal{N}(\hat{x})=span(\{n_i \,|\, i\in I(\hat{x})\})$
be the linear span of the gradient cone at $\hat{x}.$
Then, 
$\nabla_{x} f(\hat{x}, w)\in\mathcal{N}(\hat{x})\implies
\nabla_{\theta} f(\hat{x}, w) = 0.$
\end{theorem}

The proof of this theorem is obtained by simply applying Lemma 3.4 to the chain rule in Eq.~\ref{eq:chain-rule}. 
The theorem claims that the gradient of the P\&O loss in Eq. \ref{eq:po-loss} can be zero in the points outside of the optimal solution. Hence, any gradient-following method ``shall not pass'' these points.
In particular, the zero-gradient phenomenon happens in such points $\hat{x}$ where the true gradient $\nabla_{x} f(\hat{x}, w)$ is contained in the space $\mathcal{N}(\hat{x})$ spanned by the gradient cone $G(\hat{x}).$ As the dimensionality of this space grows with the number of active constraints, the zero-gradient issue is particularly important for problems with a large number of constraints. 
In the worst case, $\mathcal{N}(\hat{x})$ can be as big as the whole decision space $\R^n,$ thereby making the total gradient $\nabla_{\theta}f(\hat{x}, w)$ from Eq.~\ref{eq:chain-rule} zero for any value of the true gradient $\nabla_{x}f(\hat{x}, w)$.
In the following sections, we introduce a method that resolves the zero-gradient problem and provides theoretical guarantees for its performance.

\subsection{Quadratic programming approximation}
The fundamental assumption of the predict and optimize framework is that training $\phi_{\theta}$ using the task performance loss is better than fitting it to the true values of $w.$ Hence, the models trained with predict and optimize might output $\hat{w}$ that is significantly different from the true $w$ and yet produces good decisions. Taking this argument one step further, we claim that the objective function $f(x, \hat{w})$ in the internal optimization problem in Eq. \ref{eq:int} does not need to be the same as the true objective $f(x, w).$ In particular, we suggest computing decisions using a simple quadratic program (QP):
\begin{equation}
    x^\ast_{QP}(\hat{w}) = \argmax_{x} - \|x-\hat{w}\|^2_2 \text{ s.t. } x\in\mathcal{C}.
    \label{eq:qp}
\end{equation}

The reasons for this choice are manyfold. First, the internal objective $f_{QP}(x, \hat{w})=- \|x-\hat{w}\|^2_2,$ is strictly concave and hence $x^\ast_{QP}(\hat{w})$ is always uniquely-defined. Moreover, the range of $x_{QP}(\hat{w})$ is $\mathcal{C},$ i.e., $
\forall x\in\mathcal{C},\,\exists \hat{w}$ such that $x=x^\ast_{QP}(\hat{w}).$ Hence, it can represent any optimal solution. However, the most important property of QP is that its Jacobian is very simple, which we explain below.

The problem in Eq.~\ref{eq:qp} has a simple geometrical interpretation: the point $x=\hat{w}$ is the unconstrained maximum of $f_{QP}(x, \hat{w})$ and $x^\ast_{QP}(\hat{w})$ is its Euclidean projection on the feasibility set $\mathcal{C},$ see Figure~\ref{fig:qp}. To compute the Jacobian $\nabla_{\hat{w}}\,x^\ast_{QP},$ we need to understand how perturbations of $\hat{w}$ affect $x^\ast_{QP}.$ Employing the geometrical intuition above, we obtain the following lemma:

\begin{lemma}
    Let $\hat{w}$ be a prediction and $\hat{x}$ be the optimal solution of the QP problem defined in Eq. \ref{eq:qp}. Let the strict complementary slackness condition hold and let $\{n_i|i\in I(\hat{x})\}$ be the normals of the active constraints. Let
    $
        \{e_j|j=1,\ldots,n-|I(\hat{x}|) \}
    $
    be an orthogonal complement of vectors $\{n_i| i \in I(\hat{x})\}$ to a basis of $\R^n.$ Then, the representation of the Jacobian $\nabla_{\hat{w}}x_{QP}(\hat{w})$ in the basis $\{n_i\}\cup \{e_j\}$ is a diagonal matrix. Its first $|I(\hat{x})|$ diagonal entries are zero, and the others are one.
\end{lemma} 

Proof of this lemma can be found in the appendix. Lemma 3.6 implies that the Jacobian $\nabla_{\hat{w}}x_{QP}(\hat{w})$ has a simple form and can be easily computed by hand. While providing computational benefits, this approach does not address the zero-gradient problem. In the next section, we introduce a method to compute an approximate of the Jacobian $\nabla_{\hat{w}}x_{QP}(\hat{w})$ that has a strictly one-dimensional null space. Combined with the QP approximation, it is guaranteed to at least not decrease the task performance.

\subsection{Local smoothing}
% Figure environment removed
We identified a fundamental issue of differential optimization -- the zero-gradient problem. We showed that the null space of the Jacobian $\nabla_{\hat{w}}x(\hat{w})$ depends on the number of constraints active at $\hat{x}.$ Generally, this number can be as large as the number of optimized variables $n,$ and the gradient-descent algorithms can get stuck in certain points on the boundary of the feasibility region.

%A potential solution to this issue is to approximate the feasibility region $\mathcal{C}$ in such a way that all gradient cones become one-dimensional (see Figure \ref{fig:smooth-c}). 
%It is known that any convex set can be approximated with a convex polytope \cite{bronstein2008approximation}, and a convex polytope can be approximated with a smooth convex set \cite{ghomi2004}. By combining these results we could approximate $\mathcal{C}$ with a smooth $\mathcal{C}'$. Then, the null space of the Jacobian of the resulting problem over $\mathcal{C}'$ would be strictly one-dimensional. Therefore, it is possible to derive an arbitrarily close approximation of the problem that suffers from the zero-gradient problems much less. 

% Theoretically, this approach seems promising but it might be hard to use in practice. 
In this section, we propose a simple way to modify the feasibility region -- we smooth $\mathcal{C}$ locally around the point for which we compute the Jacobian, thereby ensuring that its null space becomes one dimensional. First, we define a method for the general setup, without imposing any assumptions on the optimization problem. Then, we demonstrate that combined with the QP approximation from Section 3.2, this smoothing approach has theoretical guarantees.

We begin with the general case -- the problem in Eq. \ref{eq:int}.
%Assume that it has a unique solution for any $\hat{w}.$ Then, let $\hat{w}$ be a prediction and let $\hat{x}$ denote the optimal decision.
Let
$\nabla_{x}f(\hat{x},\hat{w})=\sum_{i\in I(\hat{x})}\alpha_in_i$ be the internal gradient at $\hat{x}$ for some $\alpha_i\geq 0,\; \forall i \in I(\hat x).$ Then, we introduce the following definition:
\begin{definition}
    Let $r>0$ be a positive real number. Let $c=\hat{x} - r\frac{\nabla_{x}f(\hat{x},\hat{w})}{\|\nabla_{x}f(\hat{x},\hat{w})\|_2}.$
    \textnormal{The local $r$-smoothed feasibility region},
    $\mathcal{C}_r(\hat{x},\hat{w}):=\{y|y\in\R^n, \|y - c\|_2\leq r\},$
    is a ball of radius $r$ around $c.$
    \textnormal{The local $r-$smoothed problem} $P_r(\hat{x}, \hat{w})$ with parameters $\hat{x}, \hat{w}$ is defined as 
    ${x^\ast_r(\hat{w}):=\argmax_{x\in\mathcal{C}_r(\hat{x}, \hat{w})}f(x, \hat{w}).}$
\end{definition}
Figure \ref{fig:qp} shows an example of the local $r-$smoothed problem. Now, let $\hat{x}_r=x^\ast_r(\hat{w})$ denote the solution of $P_r(\hat{x}, \hat{w})$. By construction, the internal gradient at $\hat{x}_r$ lies in the one-dimensional gradient cone, and hence, by Property 3.2, $\hat{x}_r=\hat{x}.$
The main purpose of smoothing is to approximate the gradient in Eq. \ref{eq:chain-rule} by substituting $\nabla_{\hat{w}}x^\ast(\hat{w})$ with $\nabla_{\hat{w}}x^\ast_r(\hat{w}).$ We highlight that the decisions are still computed using the non-smoothed problem $x^\ast(\hat{w})$ and $x^\ast_r(\hat{x}, \hat{w})$ is used exclusively to perform the gradient update step. In other words, we use the following expression to compute the gradient:
\begin{equation}
    \nabla_{\theta}f(x^\ast(\hat{w}), w) \approx \nabla_{x}f\big(\hat{x}, w\big)\; \nabla_{\hat{w}}x^\ast_r(\hat{w})\; \nabla_{\theta}\hat{w}
\end{equation}
It is worth mentioning that the strict complementary slackness in the original problem is a stronger condition than the strict complementary slackness on $P_r(\hat{x}, \hat{w}).$ Therefore, the Jacobian of the $r-$smoothed problem can exist even for predictions $\hat{w}$ where the true Jacobian does not.

Generally, the efficiency of $r-$smoothing depends on the form of the internal problem in Eq. \ref{eq:int}. Below, we show that combining $r-$smoothing with the QP approximation has guarantees on its performance. First, we notice that Lemma 3.6 prescribes the Jacobian of the $r-$smoothed QP problem:
\begin{property}
Let $\hat{x}=x^\ast_{QP}(\hat{w})$ be a decision derived via QP. Suppose that the complementary slackness conditions hold for $P_r(\hat{x}, \hat{w})$ and let $e_1=\nabla_{x}f_{QP}(\hat{x}, \hat{w})$ be the internal gradient. Let $\{e_2,\ldots,e_{n}\}$ be a complement of $e_1$ to an orthogonal basis of $\R^n.$
Then, the Jacobian
$\nabla_{\hat{w}}x^\ast_r(\hat{w})$ of the local $r-$smoothed problem expressed in the basis $\{e_1, e_2,\ldots,e_{n}\}$ is a diagonal matrix. Its first entry is zero, others are ones.
\end{property}
As $C_r(\hat{x}, \hat{w})$ is defined by a single constraint, the null space of $\nabla_{\hat{w}}x^\ast_r(\hat{x},\hat{w})$ is always one-dimensional. Hence, the zero-gradient problem can only occur when the internal gradient $\nabla_{x}f_{QP}(\hat{x}, \hat{w})$ and the true gradient $\nabla_{x}f(\hat{x}, w)$ are exactly collinear. Hence, we expect $r-$smoothing to significantly improve upon the zero-gradient problem. Next, we show that the $r-$smoothed Jacobian is actually a good approximation. In the following theorem, we demonstrate that the local $r-$smoothing of the QP approach indeed yields a ``good'' direction for the gradient steps.
\begin{theorem}
    Let $\hat{x}=x^\ast_{QP}(\hat{w})$ be the decision obtained via QP and let $\nabla_{\hat{w}}x^\ast_r(\hat{w})$ be the Jacobian of the $r-$smoothed QP problem. Let $\Delta\hat{w}=\nabla_{x}f(\hat{x},w)\;\nabla_{\hat{w}}x^\ast_r(\hat{w})$ be the prediction perturbation obtained by using this Jacobian and let $\hat{w}'(t)=\hat{w} + t\Delta\hat{w}$ be the updated prediction.
    Then, for $t\to0^+,$ using $\hat{w}'(t)$ results in a non-decrease in the task performance. In other words,
    $f\big(x^\ast_{QP}(\hat{w}'(t)), w\big)\geq f\big(x^\ast_{QP}(\hat{w}), w\big).$
\end{theorem}
Interestingly, this result does not depend on $r.$ However, this is to be expected -- no matter the radius of $\mathcal{C}_r,$ the Jacobian of $P_r(\hat{x}, \hat{w})$ is still the same by Lemma 3.6. 
Theorem 3.10 shows that using $r-$smoothing together with the QP approximation results in analytically computable Jacobian that has a strictly one-dimensional null space. Therefore, we are much less likely to encounter the zero-gradient problem when using this approximation. 
However, the resulting one-dimensional null space contains
the only direction that can move the prediction $\hat{w}$, and hence the decision $\hat{x},$ inside $\mathcal{C}$. This might become crucial, for example, when the optimal solution with respect to the true objective lies in the interior of $\mathcal{C}.$ To resolve this problem, we use the projection distance regularization method first suggested in \cite{Chen2021}. Specifically, we add a penalty term
\begin{equation}
    p(\hat{w}) = \alpha\|\hat{x} -\hat{w}\|_2^2,
    \label{eq:reg}
\end{equation}
where $\alpha\in\R^+$ is a hyperparameter. Minimizing this term, we push $\hat{w}$ along the null-space of the Jacobian towards the feasibility region and eventually move $\hat{x}$ inside $\mathcal{C}.$
\subsection{The training process}
In this section, we summarize the results of Sections 3.1-3.3 and describe the final algorithm we use to solve the P\&O problems. For each problem instance $(o, w),$ we first compute the prediction, $\hat{w}=\phi_{\theta}(o),$ and the decision using the QP approximation method, $\hat{x}=x^\ast_{QP}(\hat{w}).$ Then, we obtain the achieved objective value, $f(\hat{x}, w).$ During training, we update the model parameters $\theta$ by performing the steps described in Algorithm \ref{alg:training}.

\begin{algorithm}[h!]
\setstretch{1.4}
\caption{}
\label{alg:training}
\begin{algorithmic}
\For{$(o,w)\in\mathcal{D}$}
 \State $\hat{x}\gets x_{QP}^\ast\big(\phi(o)\big)$\Comment{Compute the decision}
 \State $f_x\gets \nabla_{x}f(\hat{x}, w)$\Comment{Compute the true gradient}
 \State $\hat{f}_x\gets\nabla_{x}f(\hat{x}, \hat{w})$\Comment{Compute the internal gradient}
 \State $f^0 \gets \frac{f_x^\top \hat{f}_x}{\|\hat{f}_x\|_2}$ \Comment{Project the true gradient on the null space of $\nabla_{\hat{w}}x_r^\ast(\hat{w})$}
 \State $\Delta\hat{w}\gets f_x\,\nabla_{\hat{w}}x^\ast_r(\hat{w})= f_x - f^0.$ \Comment{Compute the prediction perturbation}
 \State $\Delta\hat{w}^{reg}\gets2\alpha(\hat{x}-\hat{w})$\Comment{Compute the anti-gradient of the penalty from Eq. \ref{eq:reg}}
 \State$\Delta\theta \gets (\Delta\hat{w} + \Delta\hat{w}^{reg}) \nabla_{\theta}\,\phi_\theta(o)$\Comment{Approximate the total gradient}
 \State $\theta \gets \theta + \eta \Delta\theta$\Comment{Perform the gradient step of size $\eta$}
 \EndFor
\end{algorithmic}
\end{algorithm}

