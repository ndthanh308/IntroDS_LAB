\section{Conclusion}
%The sub-optimal performance reported in some earlier work using predict and optimize can be explained by the large null space of the Jacobian in cones of the solution space, spanned by the normal vectors of the active constraints.
In this work, we discover and explain the zero-gradient problem in P\&O for convex optimization. In particular, we show that the null space of the Jacobian of a convex optimization problem can get arbitrarily large, especially in the case with numerous constraints. This phenomenon prevents gradient-following algorithms from learning optimal solutions in convex P\&O problems. 

To resolve this issue, we introduce a method to compute an approximation of the Jacobian. It is done by smoothing the feasibility region around the current solution and thereby reducing the dimensionality of the null space to one. We prove that the combination of smoothing with the QP approximation results in the gradient update steps that at least do not decrease the task performance, but often allow to escape the zero-gradient cones. To enable movement along the remaining one-dimensional null space, we add a projection distance regularization term.
The suggested method leads to significantly better results for the convex P\&O problems that suffer from the zero-gradient problem the most -- those with many constraints and with the true optimum lying on the boundary of the feasibility set.

%For future work, we aim to investigate the zero-gradient problem in the context of sequential decision-making. We plan to study new challenges arising from the sequentiality of the problem and study the applicability of our smoothing method.