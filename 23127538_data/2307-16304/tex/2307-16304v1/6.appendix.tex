\section{Proofs}
\begin{proof}[Proof of Lemma 3.4]
Let $\Delta\hat{w}$ denote an arbitrary direction and let $d=\nabla_{\hat{w}}\,x^\ast(\hat{w})\,\Delta\hat{w}$ be the corresponding directional derivative of the decision. The existence of $d$ is guaranteed by the strict complementary slackness conditions and Lemma 3.3.
Let $t\to0^+.$ Then, we have
\begin{equation*}
\hat{x}'(t):=x^\ast(\hat{w}+t\Delta\hat{w})=\hat{x}+td+o_x(t),
\end{equation*} where $o_x(t)$ is the ``little $o$'' notation, i.e., $\lim_{t \to 0^+} \frac{\|o_x(t)\|_2}{t}=0.$ To prove the lemma, we first want to show that $d^\top n_i =0,\;\forall i \in I(\hat{x}).$ Then, we will show that it implies the lemma's claim.

By definition, $n_i=\nabla_{x}g_i(\hat{x}).$ Then, since $g_i(\cdot)$ is differentiable and $g_i(\hat{x})=0,\,\forall i\in I(\hat{x}),$ we have the following first-order approximation for $g_i\big(\hat{x}'(t)\big):$
\begin{equation*}    g_i\big(\hat{x}'(t)\big)=g_i\big(\hat{x}+td+o(t)\big)=g_i(\hat{x}) + tn_i^\top d  + o_g(t) = tn_i^\top d + o_g(t).
\end{equation*}
Since $\hat{x}'$ is the solution of the internal optimization problem, the inequality $g_i(\hat{x}'(t))\leq 0$ holds. Hence, the equation above implies that $n_i^\top d \leq 0.$
Now, we want to show that, in fact, $n_i^\top d = 0.$ For a proof by contradiction, suppose that $n_i^\top d < 0.$
Then, by definition of $o_g(t)$, there exists $\epsilon>0,$ such that 
\begin{equation*}
    0< t<\epsilon\implies g_i\big(\hat{x}'(t)\big)<0.
\end{equation*}
Now, we will to show that $g_i\big(\hat{x}'(t)\big)<0$ contradicts the complementary slackness condition at $\hat{x}.$ From Lemma 3.3, we know the KKT multiplier, $\alpha'_i(t):=\alpha_i(\hat{w}+t\Delta\hat{w}),$ is a continuous function of $t.$ On the one hand, from the KKT conditions, we know that $g_i\big(\hat{x}'(t)\big)<0\implies \alpha'_i(t)=0.$ Therefore, $\alpha'_i(t)=0$ for $t<\epsilon.$ Hence, we have
\begin{equation*}
    \lim_{t \to 0^+} \alpha'_i(t)=0.
\end{equation*} 
On the other hand, the continuity implies that
$\lim_{t \to 0^+} \alpha'_i(t)=\alpha'_i(0)=\alpha_i$ and, due to strict complementary slackness, $\alpha_i>0.$ Hence, we also have
\begin{equation*}
    \lim_{t \to 0^+} \alpha'_i(t)>0.
\end{equation*}

We arrived at a contradiction and therefore can claim that ${d^\top n_i=0}$ for all $n_i.$ 
Since ${\{n_i|i\in I(\hat{x})\}}$ is a basis of $\mathcal{N}(\hat{x}),$ this implies that for any direction $v\in\mathcal{N}(\hat{x})$ and for any $\Delta\hat{w},$ we have $v^\top\,\nabla_{\hat{w}}\,x^\ast(\hat{w})\,\Delta\hat{w}=0.$ 
In other words, vector $v^\top\,\nabla_{\hat{w}}\,x^\ast(\hat{w})$ is orthogonal to the whole space of $\hat{w}$ and hence it must be zero, $v^\top\,\nabla_{\hat{w}}\,x^\ast(\hat{w})=0,\,
\forall v\in\mathcal{N}(\hat{x}).$
Hence $\mathcal{N}(\hat{x})$ is contained in the left null space of $\nabla_{\hat{w}}\,x^\ast(\hat{w}).$
\end{proof}

\begin{proof}[Proof of Lemma 3.6]
First, consider the case when the unconstrained maximum $\hat{w}$ is in the interior of $\mathcal{C}.$ By definition of $x^\ast_{QP},$ it means that $\hat{x}=x^\ast_{QP}(\hat{w})$ is also in the interior of $\mathcal{C}$ and $\hat{x}=\hat{w}$. Then, $x^\ast_{QP}$ is the identity function around $\hat{w},$ and hence 
$x^\ast_{QP}(\hat{w}+\Delta\hat{w})=x(\hat{w})+\Delta\hat{w}$ for small enough $\Delta\hat{w}.$ Hence, $\nabla_{\hat{w}}x^\ast_{QP}(\hat{w})=I.$ Since no constraints are active in this case ($I(\hat{x})=\emptyset$), the lemma's claim holds.

Now, consider the case when some constraints are active, and thus $\hat{x}$ lies on the boundary of $\mathcal{C}.$ 
To get the exact form of the Jacobian $\nabla_{x}\,x_{QP}^\ast(\hat{w}),$ we will compute $\lim_{t\to0}x^\ast_{QP}(\hat{w}+t\Delta\hat{w})$ for all possible $\Delta\hat{w}.$ 
As in the QP case the predictions $\hat{w}$ lie in the same space as $\hat{x}$, we can do it first for $\Delta\hat{w}\in\mathcal{N}(\hat{x})$ and then for $\Delta\hat{w}\perp\mathcal{N}(\hat{x}).$

\paragraph{1. ${\Delta\hat{w}\in\mathcal{N}(\hat{x}).}$} For $\Delta\hat{w}\in\mathcal{N}(\hat{x}),$ we want to show that the corresponding directional derivative is zero. We begin by computing the internal gradient $\nabla_{x}f_{QP}(\hat{x}, \hat{w}):$ 
\begin{equation*}
\nabla_{x}f_{QP}(\hat{x}, \hat{w})=-\nabla_{x}\,\|x-w\|^2_2 = 2(\hat{w}-\hat{x}).    
\end{equation*}
Using this formula, we can write the internal gradient for the perturbed prediction $\hat{w}+t\Delta\hat{w}$ at the same point $\hat{x}$:
\begin{equation*}
    \nabla_{x}f_{QP}(\hat{x}, \hat{w} + t\Delta\hat{w}) = \nabla_{x}f_{QP}(\hat{x}, \hat{w}) + 2t\Delta\hat{w}. 
\end{equation*}
By definition, $\mathcal{N}(\hat{x})$ is a linear span of the vectors $\{ n_i|i\in I(\hat{x})\}.$ Hence, since $\Delta\hat{w}\in\mathcal{N}(\hat{x}),$ it can be expressed as 
\begin{equation*}
\Delta\hat{w}=\sum_{i\in I(\hat{x})}\delta_in_i,\quad\delta_i\in\R.
\tag{$\ast$}
\end{equation*}
 
By Property 3.2, the internal gradient has the following representation: 
\begin{equation*}
    \nabla_{x}f_{QP}(\hat{x}, \hat{w}) = \sum_{i\in I(\hat{x})}\alpha_in_i,\quad \alpha_i>0.
    \tag{$\ast\ast$}
\end{equation*}
Then, combining $(\ast)$ and $(\ast\ast),$ we obtain
\begin{equation*}
 \nabla_{x}f_{QP}(\hat{x}, \hat{w} + t\Delta\hat{w}) = \nabla_{x}f_{QP}(\hat{x}, \hat{w}) + 2t\Delta\hat{w} = \sum_{i\in I(\hat{x})}(\alpha_i + 2t\delta_i)n_i
\end{equation*}
Since $\alpha_i>0,\,\forall i \in I(\hat{x})$, there exists $\epsilon>0,$ such that $\alpha_i -2t\delta_i > 0$ for $|t|<\epsilon.$ Therefore, $\nabla_{x}f_{QP}(\hat{x}, \hat{w} + t\Delta\hat{w})$ lies in the gradient cone of $\hat{x},$ and hence, by Property 3.2, $x_{QP}^\ast(\hat{w}+t\Delta\hat{w})=\hat{x}$ for $|t|<\epsilon.$ Therefore, the directional derivative of $x_{QP}^\ast(\hat{w})$ along $\Delta\hat{w}\in\mathcal{N}(\hat{x})$ is zero.
\paragraph{2. $\Delta\hat{w}\perp\mathcal{N}(\hat{x}).$ } Next, let $\Delta\hat{w}$ be orthogonal to $\mathcal{N}(\hat{x}).$ We begin with the first order approximation of $\hat{x}'(t):$
\begin{equation*}
    \hat{x}'(t)=\hat{x} + td + o(t).
\end{equation*} 
From the proof of Lemma 3.3, we can know that $d\perp \mathcal{N}.$ By definition of $x^\ast_{QP}$, we know that 
$\hat{x}$ is the point on $\mathcal{C}$ closest to $\hat{w}.$ Likewise, $\hat{x}'(t)$ is the point on $\mathcal{C}$ closest to $\hat{w}+t\Delta\hat{w}.$ Hence, $d=\Delta\hat{w}.$ Therefore, for any $\Delta\hat{w}\perp\mathcal{N},$ the directional derivative of $x_{QP}(\hat{w})$ along $\Delta\hat{w}$ is one. 

So, we have shown that 
\begin{equation*}
    \nabla_{\hat{w}}\,x^\ast_{QP}(\hat{w})\,\Delta\hat{w}=\begin{cases}
0 &\text{for } \Delta\hat{w}\in\mathcal{N}(\hat{x}) \\
\Delta\hat{w} & \text{for }  \Delta\hat{w}\perp\mathcal{N}(\hat{x}).
\end{cases}
\end{equation*}
Therefore, the lemma is proven.
\end{proof} 

\begin{proof}[Proof of Theorem 3.9]
First, we want to construct an orthogonal basis $\{e_1,\ldots e_n\}$ of $\R^n$ that will greatly simplify the calculations. We start by including the internal gradient in this basis, i.e., we define $e_1=\nabla_{x}f_{QP}(\hat{x}, \hat{w}).$ Then, let $I(\hat{x})=\{i|g_i(\hat{x})=0\}$ be the set of indices of the active constraints of the original problem and let $\mathcal{N}(\hat{x})=span(\{n_i|i\in I(\hat{x})\})$ be a linear span of their normals. By the liner independence condition from Assumption 2, $dim\big(\mathcal{N}(\hat{x})\big)=|I(\hat{x})|.$ Moreover, by Property 3.2, we know that $e_1\in\mathcal{N}(\hat{x}).$ Then, we can choose vectors $e_2,\ldots, e_{|I(\hat{x})|}$ that complement $e_1$ to an orthogonal basis of $\mathcal{N}(\hat{x}).$
The remaining vectors $e_{|I(\hat{x})| +1},\ldots,e_n,$ are chosen to complement $e_1,\ldots,e_{|I(\hat{x})|}$ to an orthogonal basis of $\R^n$. The choice of this basis is motivated by Lemma 3.6: $e_1$ is a basis of the null-space of the $r-$smoothed Jacobian, $e_1,\ldots,e_{|I(\hat{x})|}$ form a basis of the null space of the true QP Jacobian, and the remaining vectors form a basis of space in which we can move $x^\ast_{QP}(\hat{w}).$

For brevity, let $f_x=\nabla_{x}f(\hat{x}, w)$ denote the true gradient vector. By definition,
${\Delta\hat{w}=f_x\,\nabla_{\hat{w}}x^\ast_r(\hat{x}, \hat{w})}$
is obtained via the $r-$smoothed problem.
From Property 3.8, we know that $\Delta\hat{w}$ is a projection of $f_x$ on the vectors $e_2,\ldots,e_n.$ Then, since $e_1,\ldots,e_n$ is an orthogonal basis, we have
\begin{equation*}
    \Delta\hat{w}=\sum_{i=2}^n\beta_ie_i,\quad \beta_i=f_x^\top e_i,\, i=2,\ldots,n.
\end{equation*}

Now, let's see how this $\Delta\hat{w}$ affects the true decision $x^\ast_{QP}(\hat{w} + t\Delta\hat{w})$ for $t\to 0^+.$ First, we have a first-order approximation 
\begin{equation*}
x^\ast_{QP}(\hat{w} + t\Delta\hat{w})=\hat{x} + td + o(t),
\end{equation*}
for some $d\in\R.$ From Lemma 3.6, we know that $d$ is actually a projection of $\Delta\hat{w}$ onto the vectors $e_{|I(\hat{x})|+1},\ldots,e_n.$ 
Therefore, we have
\begin{equation*}
    x^\ast_{QP}(\hat{w} + t\Delta\hat{w})=\hat{x} + \sum_{i=|I(\hat{x}|+1}^n\beta_ie_i + o(t).
\end{equation*}

Finally, the change in the true objective can be expressed as
\begin{align*}
    f\Big(x^\ast_{QP}(\hat{w}+t\Delta\hat{w}), w\Big)-f\Big(x^\ast_{QP}(\hat{w}), w\Big)=tf^\top_x \Big(\sum_{i=|I(\hat{x}|+1}^n\beta_ie_i\Big) + o(t)=\\=t\sum_{i=|I(\hat{x}|+1}^n\beta_if^\top_xe_i
+o(t)= t\sum_{i=|I(\hat{x}|+1}^n\beta_i ^2 +o(t)\geq 0.
\end{align*}
    
Therefore, perturbing prediction along $\Delta\hat{w}$ does not decrease the true objective $f(\hat{x}, w),$ and hence \begin{equation*}
    f\big(x^\ast_{QP}(\hat{w}+t\Delta\hat{w}), w\big)\geq f\big(x^\ast_{QP}(\hat{w}), w\big) 
\end{equation*}
for $t\to 0^+.$
\end{proof}

\section{Equality constraints}
Assumption 2 postulates that for any $x\in\mathcal{C},$ the gradients of active constraints, $\{\nabla_{x}g_i(x)|g_i(x)=0\},$ are linearly independent. Now, suppose we include equality constraints in our problem. e.g., we have a constraint $g^{eq}(x)\leq0$ and $-g^{eq}(x)\leq 0$ for some $g.$ Clearly, the gradients of $g^{eq}(x)$ and $-g^{eq}(x)$ violate the independence assumption. However, we claim that it does not affect our results.
Let $\hat{w}$ and $\hat{x}$ be a prediction and a corresponding decision and let $n^{eq}=\nabla_{x}\,g^{eq}(\hat{x}).$ Suppose the equality constraint $g^{eq}(\hat{x})=0$ is active. Let $I(\hat{x})$ be the set of indices of the active constraints \textit{not including} $g^{eq}(x).$ Then, we have a representation of the internal gradient, 
\begin{equation*}
    \nabla_{x}f(\hat{x},\hat{w})=\alpha^{eq}_1n^{eq} - \alpha^{eq}_2n^{eq} + \sum_{i\in I(\hat{x})}\alpha_in_i.
\end{equation*}
Suppose that $\alpha^{eq}_1\neq\alpha_2^{eq},$ e.g., without loss of generality, $\alpha^{eq}_1>\alpha^{eq}_2.$ Then, 
\begin{equation*}
    \nabla_{x}f(\hat{x},\hat{w})=(\alpha^{eq}_1-\alpha^{eq}_2)n^{eq} + \sum_{i\in I(\hat{x})}\alpha_in_i
\end{equation*} and hence removing the constraint $-g^{eq}(x)\leq 0$ would not change the optimality of $\hat{x}.$ The remaining problem would satisfy complementary slackness and hence would have all the properties demonstrated in Section 3. Therefore, for the case with equality constraints, we need to extend the complementary slackness conditions by demanding $\alpha_1^{eq}\neq\alpha_2^{eq}.$ 

\section{Experimental details}
In this section, we provide the details of the experiments reported in the paper. All experiments were conducted on a machine with 32gb RAM and NVIDIA GeForce RTX 3070. The code is written in Python 3.8, and neural networks are implemented in \textit{PyTorch} 1.12. For methods requiring differentiation of optimization problems (those, without $r-$smoothing), we use the implementation by Agrawal et.\ al [2019a]. The code can be found at \textit{placeholder for GitHub link. For the reviewers, the code is submitted through OpenReview.}
\subsection{Portfolio optimization problem}
\begin{table}[t]
\centering
\begin{tabular}{l|l}
%\cline{2-2}
& Search space \\ \hline
\multicolumn{1}{l|}{\textit{Learning rate}} 
&$\{5\times10^{-6}, 10^{-5}, 2\times10^{-5}, 5\times10^{-5}, 10^{-4}, 5\times10^{-4}\}$  \\ %\hline

\multicolumn{1}{l|}{\textit{Batch size}} & $\{1, 2, 4, 8, 32\}$    \\ %\hline
\multicolumn{1}{l|}{\textit{Proj. distance weight} $\alpha$ from Eq. (6)} & 
$\{0.001, 0.0025, 0.005, 0.01, 0.05, 0.1, 1\}$   \\ %\hline
%\multicolumn{1}{|l|}{\textit{Training epochs}} &  \\ \hline
\multicolumn{1}{l|}{$x_{shift}$} & $\{0, .1, 1\}$ \\ %\hline
\multicolumn{1}{l|}{$x_{scale}$} & $\{0.1, 1, 5\}$ \\ %\hline
\end{tabular}
\caption{Search space for different hyperparameters for the portfolio optimization problem}
\label{tab:po_search}
\end{table}
In the portfolio optimization problem, the predictor $\phi_\theta$ is represented by a fully connected neural network with two hidden layers of 100 neurons each, and \textit{ReLU} activation functions. The output layer has no activation function. Instead, the output of the neural network is scaled by the factor $x_{scale}$ and shifted by $x_{shift}$.  For the methods using the QP approximation, the output layer predicts only $\hat{w}$ and consists of 200 neurons, one per the decision variable $x_i$. For the method that uses the original problem formulation and predicts both $\hat{p}$ and $\hat{Q}$, the output layer additionally predicts a $200\times200$ matrix $L$ and then sets $\hat{Q}:=(0.9L+0.1I)(0.9L+0.1I)^\top$, where $I$ is the identity matrix.

For training, we used the \textit{Adam} optimizer from PyTorch, with custom learning rate and otherwise default parameters.
The values of different hyperparameters were determined by a grid search procedure, summarized in Table~\ref{tab:po_search}.
The values used in the experiments are reported in Table~\ref{tab:po_results}. These values may vary across experiments with different $\lambda'$s. For each $\lambda,$ however, the four studied methods use the same values of the hyperparameters (the only exception is the projection distance weight $\alpha,$ which is always zero for methods without regularization).

\begin{table}[t]
\centering
%\begin{tabular}{l|l|l|l|l|l|l|l}
\begin{tabular}{lrrrrrrrr}
%\cline{2-7}
                         & $\lambda=2$&$\lambda=1$&$\lambda=0.5$&$\lambda=0.25$&$\lambda=0.1$&$\lambda=0$ \\ \hline
\multicolumn{1}{l|}{\textit{Learning rate}} 
&$10^{-5}$ &$10^{-5}$ &$2\times10^{-5}$&$2\times10^{-5}$ & 
$2\times10^{-5}$& $5\times10^{-5}$\\ %\hline

\multicolumn{1}{l|}{\textit{Batch size}} & $1$ &$1$&$1$&$1$&$1$&$1$    \\ %\hline
\multicolumn{1}{l|}{\textit{Penalty weight} $\alpha$ from Eq. (6)} &$0.1$&$0.1$&$0.02$&$0.005$&$0.005$&$0.0025$    \\ %\hline
\multicolumn{1}{l|}{\textit{Training epochs}} &$180$&$180$&$180$&$180$&$180$&$180$    \\ %\hline
\multicolumn{1}{l|}{$x_{shift}$} &$1$&$1$&$1$&$1$&$1$&$1$  \\ %\hline
\multicolumn{1}{l|}{$x_{scale}$} &$0.1$&$0.1$&$0.1$&$0.1$&$0.1$&$0.1$\\ %\hline
\end{tabular}
\caption{Best performing values of the hyperparameters for the portfolio optimization problem with different $\lambda'$s}
\label{tab:po_results}
\end{table}

\subsection{Optimal power flow problem}
\begin{table}[h]
\centering
\begin{tabular}{l|l}
%\cline{2-2}
& Search space \\ \hline
\multicolumn{1}{l|}{\textit{Learning rate}} 
&$\{5\times10^{-6}, 10^{-5}, 2\times10^{-5}, 5\times10^{-5}, 10^{-4}, 5\times10^{-4}\}$  \\ %\hline

\multicolumn{1}{l|}{\textit{Batch size}} & $\{1, 2, 4, 8, 32\}$    \\ %\hline
\multicolumn{1}{l|}{\textit{Proj. distance weight} $\alpha$ from Eq. (6)} & 
$\{0.0001, 0.00025, 0.0005, 0.001, 0.005, 0.01, 0.1\}$   \\ %\hline
%\multicolumn{1}{|l|}{\textit{Training epochs}} &  \\ \hline
\multicolumn{1}{l|}{$x_{shift}$} & $\{0, .1, 1\}$ \\ %\hline
\multicolumn{1}{l|}{$x_{scale}$} & $\{0.1, 1, 5\}$ \\ %\hline
\end{tabular}
\caption{Search space for different hyperparameters in the DC OPF problem}
\label{tab:opf_search}
\end{table}
\paragraph{Data generation process.}
Data for the DC OPF problem is generated artificially. First, we randomly generate a grid topology, see Figure~\ref{fig:grid} for an example. For each line, its admittance is set to $6S$. Nodal voltages are bounded between 325V and 375V, and the reference node has a fixed voltage of $v_0=350V.$ The demand in loads (power upper-bound), generators capacity (power lower-bound), and line current limits are sampled randomly from the following normal distributions: $\mathcal{N}(8000, 2500)\times$watt-hour, $\mathcal{N}(-14000, 2500)\times$watt-hour, $\mathcal{N}(25, 5)\times$ampere.
The coefficients $w$ are also sampled form the normal distributions: $\mathcal{N}(1.2, 1)$ for loads, and $\mathcal{N}(0.8, 0.1)$ for generators. Finally, all values are normalized such that $v_0$ becomes $7V$ (surprisingly, it performed better numerically than scaling $v_0$ to $1V$).
The observations $o$ consist of the true coefficients $w,$ demand of the loads, the capacity of the generators, and line current limits.

The predictor in the optimal power flow problem is the same as the one in the portfolio optimization, except for its hidden layers consisting of 256 neurons and using \textit{LeakyReLU} activation functions. The hyperparameters search space and final values are reported in Tables~\ref{tab:opf_search},\ref{tab:opf_results}

\begin{table}[t]
\centering
%\begin{tabular}{l|l|l|l|l|l|l|l}
\begin{tabular}{lr}
%\cline{2-7}
                         & Value  \\ \hline
\multicolumn{1}{l|}{\textit{Learning rate}} 
&$5\times10^{-5}$ \\ %\hline
\multicolumn{1}{l|}{\textit{Batch size}} 
&$1$ \\ %\hline
\multicolumn{1}{l|}{\textit{Penalty weight} $\alpha$ from Eq. (6)}
&$0.0001$ \\ %\hline
\multicolumn{1}{l|}{\textit{Training epochs}} 
&$250$ \\ %\hline
\multicolumn{1}{l|}{$x_{shift}$}
&$7$ \\ %\hline
\multicolumn{1}{l|}{$x_{scale}$} 
&$1$ \\ %\hline
\end{tabular}
\caption{Best performing values of the hyperparameters for the DC OPF problem}
\label{tab:opf_results}
\end{table}

% Figure environment removed
