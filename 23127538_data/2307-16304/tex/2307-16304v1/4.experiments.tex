\section{Experiments}
% Figure environment removed
The main result of Section 3 is the zero-gradient theorem, which describes when the gradient $\nabla_{\theta}f\big(x^\ast(\hat{w}), w\big)$ is zero.  To deal with it, we introduced the QP approach for computing the decisions, $r-$smoothing for approximating the Jacobian $\nabla_{\hat{w}}x^\ast(\hat{w}),$ and projection distance regularization to deal with the remaining null space dimension. Our solution deals with the zero gradient problem by combining these methods. In this section, we use two real-world P\&O problems to evaluate the efficiency of our method.
\subsection{Portfolio optimization}
Following \cite{Wang2020}, we apply the predict and optimize framework to the Markowitz mean-variance stock market optimization problem \cite{markowitz2000mean}. In this problem, we act as an investor who seeks to maximize the immediate return but minimize the risk penalty. The decision variable, $x\in\R^n,$ is a positive vector representing our investment in different securities. The budget constraint forces the investments to add up to one, i.e., $\sum_ix_i=1.$
The objective is defined as $f(x, p, Q) = p^\top x - \lambda x^\top Q x,$ where $p\in\R^n$ is the immediate return of the securities, $\lambda\geq0$ is the risk-aversion weight, and $Q\in\R^{n\times n}$ is the positive definite matrix of covariance between securities. The portfolio optimization problem is then defined as follows:
\vspace{-2mm}
\begin{equation}
    \argmax_{x} \; \underbrace{p^\top x - \lambda \, x^\top Qx}_{f(x,p,Q)}
    \qquad\text{ 
    s. t.}\qquad {\textstyle\sum\limits_{i=1}^n}\, x_i=1, \quad x\geq0.
    \label{eq:po}
\end{equation}
This is a quadratic optimization problem with unknown parameters $(p, Q),$ as neither the immediate return nor the true covariance matrix is known at the decision-making moment. Following \cite{Wang2020}, we use historical data from QUANDL WIKI prices \cite{quandl} for 505 largest companies on the American market for the period 2014-2017. The dataset is processed and for every day we obtain a feature vector summarizing the recent price dynamic. For further details on the processing we refer readers to the code\footnote{Placeholder for the link to the GitHub repository} and to \cite{Wang2020}. For each run, we randomly split data into train, validation, and test sets by using 70\%, 15\%, and 15\% of the whole dataset respectively. To evaluate the performance of different algorithms, we use \textit{regret,} defined as 
\begin{equation}
    \text{regret}(o, w) = f\Big(x^\ast\big(\phi_\theta(o), w\big)\Big) - \max_{x}f\big(x, w\big).
    \label{eq:regret}
\end{equation}

In the experiments, we used $\lambda$ from the set $\{0, 0.1, 0.25, 0.5, 1, 2\}.$ For the larger value of $\lambda,$ the true objective $f(x, p, Q)$ is ``more'' quadratic, and hence its maximum is more likely to lie in the interior of $\mathcal{C}.$ For smaller $\lambda$'s, on the other hand, the true objective becomes almost linear and hence it usually attains its maximum on the boundary of $\mathcal{C}.$

First of all, we define the QP approximation of the portfolio optimization problem:
\begin{equation}
    x^\ast(\hat{w})=\argmax_{x}-(x-\hat{w})^\top I (x-\hat{w}) \qquad\text{ s. t. } \qquad
    {\textstyle\sum\limits_{i=1}^n  x_i}=1, \quad x\geq0.
    \label{eq:po_qp}
\end{equation}

\begin{wraptable}{r}{7cm}
%\begin{table}
  \caption{Performance of the QP approximation}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{3}{c}{Final test regret}                   \\
    \cmidrule(r){2-3}
    $\lambda$     & QP approximation     & Predict both $Q$ and $p$ \\
    \midrule
    $0$ & $\textbf{0.061}\pm\textbf{0.002} $  & $0.064\pm0.002 $     \\
    $0.1$ & $\textbf{0.047}\pm\textbf{0.002} $  & $0.052\pm0.002 $     \\
    $0.25$ & $\textbf{0.040}\pm\textbf{0.002} $  & $0.045\pm0.002 $     \\
    $0.5$ & $\textbf{0.039}\pm\textbf{0.001} $  & $0.041\pm0.001 $     \\
    $1$ & $\textbf{0.040}\pm\textbf{0.002} $  & $0.041\pm0.002 $     \\
    $2$ & $\textbf{0.039}\pm\textbf{0.001}$  & $0.04\pm0.001 $     \\
    \bottomrule
  \end{tabular}
\label{tab:qp}
%\end{table}
\vspace{-5mm}
\end{wraptable}


As the  problem is quadratic, the only difference introduced by the QP approximation comes from using the identity matrix $I$ instead of $Q.$ The results in Table \ref{tab:qp} indicate that it performs at least as well as learning to predict both $p$ and $Q,$ and hence in all other experiments we used the QP approximation to compute the decisions.

To investigate the zero-gradient effect, we compared four ways to train the predictor: with/without $r-$smoothing and with/without the penalty term from Eq.~\ref{eq:reg}. The model used for the predictor is a 2-layer neural network, further details on the training process are described in the appendix.
The results in Figure \ref{fig:exp1} indicate that $r-$smoothing significantly improves the performance when the true objective is more linear. This result matches the theory from Section 3, as linear true objective pushes the decision $\hat{x}$ towards the boundary of $\mathcal{C},$ and hence it is more likely to enter points with a large gradient cone. For the more quadratic objectives, the true maximum is often in the interior of $\mathcal{C},$ and hence $r-$smoothing alone is not sufficient to reach it. In this case, the regularization term from Eq~\ref{eq:reg} becomes crucial, as it is the only method that can push $\hat{x}$ inside $\mathcal{C}.$



\subsection{Optimal power flow in a DC grid}
\begin{wrapfigure}{t!}{5cm}
\centering
\includegraphics[width=5
cm]{figures_images_eps/opf.eps}
\caption{Comparison of different methods on the DC grid OPF problem. $y-$axis represents the mean and standard deviation of the test regret for twelve random seeds.}
\label{fig:opf}
\vspace{-30pt}
\end{wrapfigure}

To further understand the zero-gradient phenomenon, we consider the optimal power flow problem (OPF) for DC grids \citep{Li2017}. Due to power losses, the constraints in this problem are non-linear, thus making it computationally hard. In our experiments, we used a linearized version of the problem that represents a DC grid without power losses. The decision variable is the vector of nodal voltages $v\in\R^n,$ and the unknown parameter $w$ represents either the value gained by serving power to a customer or the price paid for utilizing a generator. The reference voltage $v_0\in\R,$ the admittance matrix $Y,$ and the constraint bounds represent the physical properties of the grid. 
\begin{equation}
    \begin{aligned}
    \max_{v} &\quad& f(v, w)= -v_0w^\top (Yv)&& & \notag \\
    \text{subject to:} &\quad& \ushort{V}\leq v \leq \bar{V} && & \\ 
    &\quad&  \ushort{P} \leq-v_0Yv \leq \bar{P} && & \\
    &\quad&  \ushort{I} \leq Y_{ij}(v_i-v_j) \leq \bar{I} && & \\
    \end{aligned}
\end{equation}
We refer the reader to \cite{Li2017} for further details of the problem. Importantly, the feasibility region is defined by multiple linear constraints, and therefore we expect it to have numerous vertices with large gradient cones. The objective function $f(v,w)$ quantifies the social welfare \citep{veviurko2022surrogate} generated by all the users of the power grid. Importantly, $f(v,w)$ is linear, and hence its maximum lies on the boundary of the feasibility region.

We compared the same four methods as before on this problem using randomly generated grids with four generators and twelve loads. Same as before, we use QP approximation, $x^\ast_{QP}(\hat{w})=\argmax_{x\in\mathcal{C}}(-\|\hat{w}-x\|_2^2),$ to compute the decisions. The results in Figure \ref{fig:opf} confirm our hypothesis -- even though we differentiate through a quadratic problem, the linearity of the true objective causes the zero-gradient effect as the decision is pushed towards the boundary of the feasibility region. Then, due to a large number of constraints, it is likely to enter a vertex with a large gradient cone and get stuck there. In this case, $r-$smoothing greatly outperforms the standard differential optimization method from \cite{Agrawal2019}, while the projection distance regularization does not help a lot.