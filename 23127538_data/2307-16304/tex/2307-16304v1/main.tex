\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission



% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:

%\usepackage{neurips_2023}
\usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{svg}
\usepackage{bm}
\theoremstyle{plain}
\usepackage{wrapfig}
\usepackage{ushort}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{property}[theorem]{Property}
\newtheorem{assumption}{Assumption}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}{Corollary}[theorem]
\theoremstyle{definition}
\DeclareMathOperator*{\argmax}{\text{arg\,max}}
\DeclareMathOperator*{\argmin}{\text{arg\,min}}
\newcommand{\ve}[1]{\bm #1}
\newcommand{\mat}[1]{\text{\bf #1}}
\newcommand{\Set}[1]{\mathcal #1}
\newcommand{\R}{{\mathbb R}}%
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\usepackage{setspace}
%\usepackage{algorithm2e}
%\SetKwComment{Comment}{/* }{ */}
%\newcommand{\atcp}[1]{\tcp*[r]%{\makebox[\commentWidth]{#1\hfill}}}

%\newlength\algowd
%\def\savewd#1{\setbox0=\hbox{#1\hspace{.7in}}\algowd=\wd0\relax#1}
%\newcommand\algolines[2]{\savewd{#1}%
% \tcp*{\parbox[t]{\dimexpr\algowidth-\algowd}{#2}}}
%\RestyleAlgo{ruled}
\title{You Shall not Pass: the Zero-Gradient Problem in Predict and Optimize for
Convex Optimization}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Grigorii Veviurko \\
  Delft University of Technology\\
  \texttt{g.veviurko@tudelft.nl} \\
  \And
  Wendelin Boehmer \\
  Delft University of Technology\\
  \texttt{j.w.bohmer@tudelft.nl} \\
  \And
  Mathijs de Weerdt \\
  Delft University of Technology\\
  \texttt{m.m.deweerdt@tudelft.nl} \\
}


\begin{document}


\maketitle


\begin{abstract}
Predict and optimize is an increasingly popular decision-making paradigm that employs machine learning to predict unknown parameters of optimization problems. Instead of minimizing the prediction error of the parameters, it trains predictive models using task performance as a loss function. In the convex optimization domain, predict and optimize has seen significant progress due to recently developed methods for differentiating optimization problem solutions over the problem parameters. This paper identifies a yet unnoticed drawback of this approach -- the zero-gradient problem -- and introduces a method to solve it. The suggested method is based on the mathematical properties of differential optimization and is verified using two real-world benchmarks.
\end{abstract}


\input{1.introduction}
\input{2.background}
\input{3.zg}
\input{4.experiments}
\input{5.conclusion}
\bibliographystyle{plainnat}
\bibliography{references}
\newpage
\appendix
\input{6.appendix}
\end{document}
