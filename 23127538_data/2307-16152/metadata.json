{
  "title": "Variance Control for Distributional Reinforcement Learning",
  "authors": [
    "Qi Kuang",
    "Zhoufan Zhu",
    "Liwen Zhang",
    "Fan Zhou"
  ],
  "submission_date": "2023-07-30T07:25:18+00:00",
  "revised_dates": [],
  "abstract": "Although distributional reinforcement learning (DRL) has been widely examined in the past few years, very few studies investigate the validity of the obtained Q-function estimator in the distributional setting. To fully understand how the approximation errors of the Q-function affect the whole training process, we do some error analysis and theoretically show how to reduce both the bias and the variance of the error terms. With this new understanding, we construct a new estimator \\emph{Quantiled Expansion Mean} (QEM) and introduce a new DRL algorithm (QEMRL) from the statistical perspective. We extensively evaluate our QEMRL algorithm on a variety of Atari and Mujoco benchmark tasks and demonstrate that QEMRL achieves significant improvement over baseline algorithms in terms of sample efficiency and convergence performance.",
  "categories": [
    "cs.LG"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.16152",
  "pdf_url": "https://arxiv.org/pdf/2307.16152v1",
  "comment": "ICML 2023",
  "num_versions": null,
  "size_before_bytes": 9762136,
  "size_after_bytes": 1608770
}