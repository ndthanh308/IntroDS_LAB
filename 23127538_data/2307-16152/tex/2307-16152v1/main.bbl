\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anschel et~al.(2017)Anschel, Baram, and Shimkin]{AVERAGED-DQN}
Anschel, O., Baram, N., and Shimkin, N.
\newblock Averaged-dqn: Variance reduction and stabilization for deep
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  176--185. PMLR, 2017.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and Munos]{C51}
Bellemare, M.~G., Dabney, W., and Munos, R.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  449--458. PMLR, 2017.

\bibitem[Bellemare et~al.(2023)Bellemare, Dabney, and Rowland]{DRLbook}
Bellemare, M.~G., Dabney, W., and Rowland, M.
\newblock \emph{Distributional Reinforcement Learning}.
\newblock MIT Press, 2023.
\newblock \url{http://www.distributional-rl.org}.

\bibitem[Bellman(1966)]{bellman1966dynamic}
Bellman, R.
\newblock Dynamic programming.
\newblock \emph{Science}, 153\penalty0 (3731):\penalty0 34--37, 1966.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Cornish \& Fisher(1938)Cornish and Fisher]{Fisher1938}
Cornish, E.~A. and Fisher, R.~A.
\newblock Moments and cumulants in the specification of distributions.
\newblock \emph{Revue de l'Institut international de Statistique}, pp.\
  307--320, 1938.

\bibitem[Dabney et~al.(2018{\natexlab{a}})Dabney, Ostrovski, Silver, and
  Munos]{IQN}
Dabney, W., Ostrovski, G., Silver, D., and Munos, R.
\newblock Implicit quantile networks for distributional reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1096--1105, 2018{\natexlab{a}}.

\bibitem[Dabney et~al.(2018{\natexlab{b}})Dabney, Rowland, Bellemare, and
  Munos]{QRDQN}
Dabney, W., Rowland, M., Bellemare, M.~G., and Munos, R.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2018{\natexlab{b}}.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and Meger]{TD3}
Fujimoto, S., Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1587--1596. PMLR, 2018.

\bibitem[Greensmith et~al.(2004)Greensmith, Bartlett, and
  Baxter]{greensmith2004variance}
Greensmith, E., Bartlett, P.~L., and Baxter, J.
\newblock Variance reduction techniques for gradient estimates in reinforcement
  learning.
\newblock \emph{Journal of Machine Learning Research}, 5\penalty0 (9), 2004.

\bibitem[Hsu et~al.(2011)Hsu, Kakade, and Zhang]{RandomDesign}
Hsu, D., Kakade, S.~M., and Zhang, T.
\newblock An analysis of random design linear regression.
\newblock \emph{arXiv preprint arXiv:1106.2363}, 2011.

\bibitem[Koenker(2005)]{Koenker2005Quantile}
Koenker.
\newblock \emph{Quantile regression}.
\newblock Cambridge University Press, 2005.

\bibitem[Kuznetsov et~al.(2020)Kuznetsov, Shvechikov, Grishin, and Vetrov]{TQC}
Kuznetsov, A., Shvechikov, P., Grishin, A., and Vetrov, D.
\newblock Controlling overestimation bias with truncated mixture of continuous
  distributional quantile critics.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5556--5566. PMLR, 2020.

\bibitem[Luo et~al.(2021)Luo, Liu, Duan, Schulte, and Poupart]{SPL-DQN}
Luo, Y., Liu, G., Duan, H., Schulte, O., and Poupart, P.
\newblock Distributional reinforcement learning with monotonic splines.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Lyle et~al.(2019)Lyle, Bellemare, and Castro]{lyle2019}
Lyle, C., Bellemare, M.~G., and Castro, P.~S.
\newblock A comparative analysis of expected and distributional reinforcement
  learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  4504--4511, 2019.

\bibitem[Ma et~al.(2020)Ma, Xia, Zhou, Yang, and Zhao]{dsac}
Ma, X., Xia, L., Zhou, Z., Yang, J., and Zhao, Q.
\newblock Dsac: distributional soft actor critic for risk-sensitive
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.14547}, 2020.

\bibitem[Mavrin et~al.(2019)Mavrin, Yao, Kong, Wu, and Yu]{DLTV}
Mavrin, B., Yao, H., Kong, L., Wu, K., and Yu, Y.
\newblock Distributional reinforcement learning for efficient exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4424--4434, 2019.

\bibitem[Nguyen-Tang et~al.(2021)Nguyen-Tang, Gupta, and Venkatesh]{MMDRL}
Nguyen-Tang, T., Gupta, S., and Venkatesh, S.
\newblock Distributional reinforcement learning via moment matching.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  9144--9152, 2021.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and Van~Roy]{osband2016}
Osband, I., Blundell, C., Pritzel, A., and Van~Roy, B.
\newblock Deep exploration via bootstrapped dqn.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Ostrovski et~al.(2017)Ostrovski, Bellemare, Oord, and
  Munos]{ostrovski2017count}
Ostrovski, G., Bellemare, M.~G., Oord, A., and Munos, R.
\newblock Count-based exploration with neural density models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2721--2730. PMLR, 2017.

\bibitem[Rowland et~al.(2018)Rowland, Bellemare, Dabney, Munos, and
  Teh]{rowland2018analysis}
Rowland, M., Bellemare, M., Dabney, W., Munos, R., and Teh, Y.~W.
\newblock An analysis of categorical distributional reinforcement learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  29--37. PMLR, 2018.

\bibitem[Rowland et~al.(2019)Rowland, Dadashi, Kumar, Munos, Bellemare, and
  Dabney]{EDRL}
Rowland, M., Dadashi, R., Kumar, S., Munos, R., Bellemare, M.~G., and Dabney,
  W.
\newblock Statistics and samples in distributional reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5528--5536. PMLR, 2019.

\bibitem[Rowland et~al.(2023{\natexlab{a}})Rowland, Munos, Azar, Tang,
  Ostrovski, Harutyunyan, Tuyls, Bellemare, and Dabney]{rowland2023analysis}
Rowland, M., Munos, R., Azar, M.~G., Tang, Y., Ostrovski, G., Harutyunyan, A.,
  Tuyls, K., Bellemare, M.~G., and Dabney, W.
\newblock An analysis of quantile temporal-difference learning.
\newblock \emph{arXiv preprint arXiv:2301.04462}, 2023{\natexlab{a}}.

\bibitem[Rowland et~al.(2023{\natexlab{b}})Rowland, Tang, Lyle, Munos,
  Bellemare, and Dabney]{rowland2023statistical}
Rowland, M., Tang, Y., Lyle, C., Munos, R., Bellemare, M.~G., and Dabney, W.
\newblock The statistical benefits of quantile temporal-difference learning for
  value estimation.
\newblock \emph{arXiv preprint arXiv:2305.18388}, 2023{\natexlab{b}}.

\bibitem[Sobel(1982)]{sobel1982}
Sobel, M.~J.
\newblock The variance of discounted markov decision processes.
\newblock \emph{Journal of Applied Probability}, 19\penalty0 (4):\penalty0
  794--802, 1982.

\bibitem[Sutton(1988)]{Sutton1988LearningTP}
Sutton, R.~S.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine Learning}, 3:\penalty0 9--44, 1988.

\bibitem[Villani(2009)]{villani2009optimal}
Villani, C.
\newblock \emph{Optimal transport: old and new}, volume 338.
\newblock Springer, 2009.

\bibitem[Watkins(1989)]{watkins1989learning}
Watkins, C. J. C.~H.
\newblock Learning from delayed rewards.
\newblock \emph{PhD thesis}, 1989.

\bibitem[Yang et~al.(2019)Yang, Zhao, Lin, Qin, Bian, and Liu]{FQF}
Yang, D., Zhao, L., Lin, Z., Qin, T., Bian, J., and Liu, T.-Y.
\newblock Fully parameterized quantile function for distributional
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6193--6202, 2019.

\bibitem[Zhang \& Zhu(2023)Zhang and Zhu]{qcm}
Zhang, N. and Zhu, K.
\newblock Quantiled conditional variance, skewness, and kurtosis by
  cornish-fisher expansion.
\newblock \emph{arXiv preprint arXiv:2302.06799}, 2023.

\bibitem[Zhou et~al.(2020)Zhou, Wang, and Feng]{NC-QRDQN}
Zhou, F., Wang, J., and Feng, X.
\newblock Non-crossing quantile regression for distributional reinforcement
  learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15909--15919, 2020.

\bibitem[Zhou et~al.(2021)Zhou, Zhu, Kuang, and Zhang]{NDQFN}
Zhou, F., Zhu, Z., Kuang, Q., and Zhang, L.
\newblock Non-decreasing quantile function network with efficient exploration
  for distributional reinforcement learning.
\newblock \emph{International Joint Conference on Artificial Intelligence},
  pp.\  3455--3461, 2021.

\end{thebibliography}
