\section{On the Evaluation of Generative Data}
\label{sec:genbench}
In this section, we investigate how generative data could improve pre-trained visual recognition models on a wider range of downstream datasets, with \gb as our evaluation benchmark.

\subsection{Datasets}
Inherited from existing benchmarks such as Elevater~\cite{li2022elevater} and VTAB~\cite{zhai2019large}, \gb encompasses a total of 22 datasets with a broad spectrum of visual recognition concepts consisting of 2548 categories. These datasets are categorized into the following three main groups. More statistical information on these datasets is listed in the appendix.

\noindent \textbf{\common~Concepts} mainly cover generic objects that are commonly seen in daily life and in the Internet data. In this group, we can evaluate whether the generative models could cover those most common categories effectively. This group includes: ImageNet-1K~\cite{russakovsky2015imagenet}, CIFAR-10~\cite{krizhevsky2009learning}, CIFAR-100~\cite{krizhevsky2009learning}, Caltech-101~\cite{fei2004learning}, VOC-2007~\cite{everingham2009pascal}, MNIST~\cite{lecun1998gradient} and SUN-397~\cite{xiao2010sun}. 


\noindent \textbf{\finegrained~Concepts} cover subcategories within a meta-category, such as different breeds of dogs within the dog category or different models of airplanes within the airplane category.
In this group, we can assess the ability of generative models to capture the subtle visual differences between different subcategories and their capacity to generate objects with more specialized names. This task can help identify the limitations of generative models in capturing the complexity and variability of fine-grained object categories. This group includes: Food-101~\cite{bossard2014food}, Oxford Pets~\cite{parkhi2012cats}, Oxford Flowers~\cite{nilsback2008automated}, Standford Cars~\cite{krause20133d}, FGVC Aircraft~\cite{maji2013fine}, Country-211~\cite{radford2021learning}. 

\noindent \textbf{\rare~Concepts} covers less common objects in the real world, such as medical images (e.g., Patch-Camelyon~\cite{veeling2018rotation}) and remote sensing images (e.g., RESISC-45~\cite{cheng2017remote}, EuroSAT~\cite{helber2017eurosat}). Obtaining such data can be challenging, making it important to assess whether generative models can effectively synthesize this type of data, which presents a unique challenge to computer vision research. This group includes PatchCamelyon~\cite{veeling2018rotation}, EuroSAT~\cite{helber2017eurosat}, GTSRB~\cite{stallkamp2011german}, Rendered-SST2~\cite{radford2021learning}, FER 2013~\cite{fer2013}, RESISC-45~\cite{cheng2017remote}, Hateful Memes~\cite{kiela2020hateful}, Describable Textures~\cite{cimpoi2014describing}, and KITTI Distance~\cite{fritsch2013new}.

\subsection{External Data} 
\noindent \textbf{Generative Data.} Although there exists various large-scale pretrained text-to-image generative models~\cite{saharia2022photorealistic,ramesh2022hierarchical}. Our investigation in ~\gb is centered around two readily available and accessible generative models, namely GLIDE ~\cite{nichol2021glide} and Stable Diffusion 2.1~\cite{rombach2022high}. We use the names of categories in the dataset and combine them with various prompt strategies in~\cref{subsec:prompt_strategy} as textual prompts to generative model and obtain corresponding sampled images. If the number of required generated images exceeds the initial prompts, we randomly augment the prompt list to reach the required number.
% We choose the prompt strategies to be repeated based on the number of images required to be generated for each category.

\noindent \textbf{Retrieval Data.} Similar to retrieval data, we utilize category names along with defined templates specific to each dataset as retrieval queries, without employing additional prompt strategies as used in generative data. We employ text-to-text retrieval on LAION-400M to obtain the top-$k$ results using these queries and select the required number of images based on their similarity to each result. This approach aligns with generative data acquisition to facilitate our evaluation and analysis of data quality. It may differ from other retrieval-based works~\cite{liu2023react}, which we will discuss in \cref{sec: ext_data}. 

\noindent \textbf{Original Data.}  It refers to the training data of each individual dataset. This data type is considered the highest quality among all different types of external data and serves as the upper bound when evaluating on the same amount of shots.
However, it is also the most expensive to acquire.


To compare the cost-effectiveness of different types of data, we also analyze the cost of external data. For generative and retrieval data, it is calculated based on the time needed for sampling generative models or performing top-$k$ text-to-text search and the cost of running a CPU/GPU instance on Azure\footnote{\href{https://instances.vantage.sh/azure/}{Azure Pricing}}. For original data, we provide a reference value for human labeling cost obtained from Amazon Turk\footnote{\href{https://aws.amazon.com/sagemaker/data-labeling/pricing/}{Amazon Turk}}.
We list the estimated cost for each type of external data in ~\cref{tab:data_collection}.

% Overall, the acquisition methods and the corresponding cost per image for each type of external data are listed in ~\cref{tab:data_collection}.

\begin{table}[ht]
\centering
\caption{Data acquisition cost for different types of external data (measured in US dollars per image). See appendix for details.}
\label{tab:data_collection}
\resizebox{\columnwidth}{!}{%
\setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c|c|c}
\toprule
\rowcolor{COLOR_MEAN}
\textbf{Data Type} & \textbf{Collection Source} & \textbf{Est. Cost / Image} \\ \midrule
Generative Data & Model Inference & $2.54 \times 10^{-4}$ USD \\
Retrieval Data & Web/Database Query & $3.93 \times 10^{-5}$ USD \\
Original Data & Human Label & $1.20 \times 10^{-2} $ USD \\ \bottomrule
\end{tabular}%
}
\end{table}

% \textcolor{red}{Bo: definition of original data}
% Recently, REACT~\cite{liu2023react} proposes to learn customized visual models by finetuning pretrained vision-language models on retrieved data from large image-text database according to task definition. There are two major distinctions between REACT and our approach: (1) REACT finetunes pretrained vision language models on the retrieved data, while our CLER score does not require finetuning; (2) REACT directly uses the retrieved data for customizing visual models, while our approach can use the retrieved data as image prompts for the generative model and prompt for new data for classification.

% \footnote{\href{https://github.com/openai/glide-text2im}{OpenAI/GLIDE}}
% \footnote{\href{https://github.com/huggingface/diffusers/tree/main/examples/text_to_image}{Diffusers/StableDiffusion}}

\subsection{Prompt Strategy} 
\label{subsec:prompt_strategy}
Initially, we convert the category name into a textual prompt and feed it into the generative models to produce the corresponding image. The different specific descriptions added to the prompt to generate better images are referred to as prompt strategies. In our benchmark, we use several prompt strategies, which we present below:

\noindent \textbf{Simple Template (ST)} refers the basic prompt format consisting of \textit{a photo of \{\}}, where the category name is inserted into the brackets. 

\noindent \textbf{Defined Template (DT)} refers using simple category names as prompts, we also followed  the practice of CLIP\footnote{\href{https://github.com/openai/CLIP/blob/main/data/prompts.md}{CLIP Prompts}}, by defining dataset-specific prompt formats for each dataset. For example, in FVGC-Aircraft dataset, we extend the simple prompt with a description \textit{a type of aircraft}. 

\noindent \textbf{Category Enhancement (CE)} was introduced in~\cite{he2022synthetic}, where an off-the-shelf word-to-sentence T5~\cite{raffel2020exploring} model is utilized to expand each category into a complete sentence, such as expanding \textit{airplane} into \textit{A large, sleek, white airplane with dual engines is soaring through the clear blue skies, leaving behind a trail of white clouds.}. In the appendix, we will provide examples of generated sentences for different datasets.

\noindent \textbf{Restrictive Description (RD)} are additional, specific phrases added to the prompt in order to guide the generative model to produce images of higher quality. Examples of such phrases include \textit{hi-res, highly detailed, sharp focus}. Additionally, some special restrictive symbols, such as enclosing the category with parentheses, \eg \textit{((airplane))}, can be added to the prompt to make the generative model more focused on the category rather than other descriptive words. These ideas mainly come from the Stable Diffusion community, and the results show that this approach can lead to better generation results, but there is still no rigorous conclusion.

\noindent \textbf{Negative Prompts (NP)} are input arguments that guide the Stable Diffusion model to deliberately exclude particular objects, styles, or abnormalities from the generated image. This significant feature empowers users to eliminate undesired or redundant elements from the final output. Moreover, there are several quality constraints that are applicable to all datasets, such as restricting words like \textit{bad shape} and \textit{misfigured} to prevent the generative model from producing low-quality data.

% \noindent \textbf{Retrieval Augmented (RA)} refers to obtaining few-shot retrieval images from a retrieval system (specified in \cref{subsec:tradeoffs}), and using them as image prompts for the generative model. These images will serve as a starting point for the generative model's sampling process to better guide the model in generating images in combination with text prompts. This strategy is similar to TIG and we specify it in \cref{sec:tag}.

All prompt strategies aforementioned will be presented with more details for each dataset in the appendix.

\subsection{Evaluation Protocol}

We aim to measure the \textit{improvement} of generative data over the baseline zero-shot accuracy for a diverse range of categories. However, existing metrics such as FID primarily assess the aesthetic quality and visual coherence. While current diffusion-based generative models excel in these metrics, they are inadequate for evaluating generative data's downstream task quality. Typically, the accuracy of a model on test data is used to measure this improvement. Our experiments primarily use CLIP VIT/B-32 as the baseline visual recognition model.
% , and leave the results with larger-scale CLIP models in the appendix.

\begin{algorithm}[t]
\caption{Pseudocode of CLER in PyTorch style.}
\label{alg:code}
\algcomment{\fontsize{7.2pt}{0em}\selectfont \texttt{mm}: matrix multiplication; \texttt{T}: transpose.
%\vspace{-1.em}
}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\lstdefinestyle{python}{
  language=python,
  morekeywords={mm,encode_image,encode_text,group,mean,argmax},
  keywordstyle={\fontsize{7.2pt}{7.2pt}\selectfont\textcolor{blue!90!black}}
}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
  mathescape
}
\begin{lstlisting}[style=python]
def CLER_score(gen_images, gen_labels, test_images, test_labels, class_names):
    n_classes = len(class_names)
    gen_emb = CLIP.encode_image(gen_images) $~~$# [M,F]
    test_emb = CLIP.encode_image(test_images) $$# [N,F]
    text_emb = CLIP.encode_text(class_names) $~$# [C,F]
    gen_emb_grp = group(gen_emb, gen_labels) $~$# [C,K,F]
    class_centers = gen_emb_grp.mean(dim=1) $~~$# [C,F]

    preds_CLER = mm(test_emb, class_centers.T)
    preds_CLIP = mm(text_emb, text_features.T)
    preds_ensemble = (preds_CLER + preds_CLIP) / 2

    CLER = sum(preds_CLER.argmax(dim=1) == test_labels)
    CLER_ensemble = sum(preds_ensemble.argmax(dim=1) == test_labels)
\end{lstlisting}
\end{algorithm}

Motivated by CLIP zero-shot evaluation, we propose a training-free metric, Class-centered Recognition (CLER) score, an approximation to the improvement of generative data over a given downstream task. The core idea is to replace the averaged language embeddings of each class in CLIP zero-shot evaluation, with the averaged image embeddings from each class in the target downstream dataset. Specifically, given a set of images $\mathcal{I}$ with corresponding class labels $\mathcal{C}$, a downstream dataset $\mathcal{D}$ containing $C$ classes, CLER score measures the improvement $\mathcal{I}$ can bring to the classification task on $\mathcal{D}$.

First, we extract embeddings of images $\mathcal{I}$ using CLIP $\mathcal{E}_i = \text{CLIP}(\mathcal{I}_i)$.
Then, we group embeddings $\mathcal{E}$ according to labels $\mathcal{T}$, and compute the average embedding within each group: $\mathcal{M}_j = \text{average}(\{\mathcal{E}_i; \mathcal{T}_i = j\})$, where $1~\leq j \leq~C$.
For each test image $\mathcal{D}_k$, we obtain its image embedding $\mathcal{E}_k$ and class prediction $\mathcal{P}_k = \arg\max_{j}(\mathcal{E}_k^\top \cdot \mathcal{E}_j)$.  Then we calculate the mean accuracy of class prediction $\mathcal{P}_k$ for each test image in \cref{eq:cler_score}, where $\mathcal{L}_k$ is the ground truth label of the test image $\mathcal{D}_k$ and $\mathds{1}$ is the indicator function that returns $1$ if $\mathcal{P}_k = \mathcal{L}_k$ or $0$ otherwise.
\begin{equation}
    \text{CLER} = \frac{1}{N} \sum^{1}_{N}\mathds{1}(\mathcal{P}_k = \mathcal{L}_k)
    \label{eq:cler_score}
\end{equation}
 If we specify the CLER score evaluation with the CLIP model,  we can further enhance to a more accurate version, by ensembling the prediction with CLIP contrastive prediction: $\hat{\mathcal{P}}_k = \arg\max_{j}(\mathcal{E}_k^\top \cdot (\mathcal{E}_j + \mathcal{L}_j) / 2)$, where $\mathcal{L}_j$ is the text embeddings obtained from CLIP text encoder for class $j$. We provide the pseudo code for CLER score in ~\cref{alg:code}.
 
 In our experiments and analysis, we primarily use CLIP ViT-B/32 as the recognition model for evaluation. We will also discuss the performance of finetuning the CLIP model on generative data, as well as the evaluation of other backbones (\eg ResNet, larger ViTs, \etc) in the appendix.

% Initially, we focus on conducting experiments and analyzing the results using the CLIP VIT/B-32 model. We generated up to 500 images per category and assessed their accuracy by training a linear classifier on a frozen CLIP model and evaluating its performance on the same test set. 

% Figure environment removed

We assess the correlation between CLER score, CLIP score, and FID with linear probing accuracy using a correlation analysis on 18 datasets with 100-shot generative data per category, excluding four datasets with a small number of categories. Results in~\cref{fig:metric_correlation} show that CLER score has the best positive correlation with linear probing accuracy, followed by CLIP score. FID, which is often used to assess image quality, does not exhibit a clear correlation with linear probing accuracy. \textcolor{black}{Moreover, it is important to note that CLER computes dot products between cluster centers, significantly reducing complexity compared to computing dot products for individual images. Consequently, evaluating the CLER score is considerably more cost-effective than obtaining linear probing accuracy, which requires tuning models on generated data, as demonstrated in~\cref{fig:efficient_evaluation}.}

In addition, CLER score can be employed to assess the performance improvement of other types of data, such as original training data or retrieval data from the Internet. In the appendix, we provide a correlation analysis for original and retrieval data. 

Given the advantages of CLER score, we primarily rely on this metric in subsequent analyses, it enables us to efficiently draw indicative conclusions.

% Figure environment removed

\begin{table*}[tp]
\centering
\caption{Results of GLIDE and Stable Diffusion, along with various corresponding strategies, on the \textbf{GenBench} dataset, are presented in the table. We report the CLER scores of the 20-shot generated images on each dataset. For simplicity, we use the following abbreviations: ST for Simple Templates, CE for Category Enhancement, DT for Defined Template, NP for Negative Prompts, RD for Restrictive Descriptions, RA for Retrieval Augmented. We denote the \colorbox{best}{best} and \colorbox{second_best}{second best} with specified colors.}
\label{tab:strategy_comparison}
\resizebox{\textwidth}{!}{%
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{2}
\begin{tabular}{c|c|cccccccccccccccccccccc|c}
\toprule
\rowcolor{COLOR_MEAN}
\multicolumn{1}{c|}{\rot{\textbf{Model}}} & \rot{\textbf{Prompt Strategy}} & \rot{\textbf{ImageNet-1K}} & \rot{\textbf{Caltech-101}} & \rot{\textbf{CIFAR-10}} & \rot{\textbf{CIFAR-100}} & \rot{\textbf{Country-211}} & \rot{\textbf{Desc. Textures}} & \rot{\textbf{EuroSAT}} & \rot{\textbf{FER-2013}} & \rot{\textbf{FGVC-Aircraft}} & \rot{\textbf{Food-101}} & \rot{\textbf{GTSRB}} & \rot{\textbf{Hateful Memes}} & \rot{\textbf{KITTI Distance}} & \rot{\textbf{MNIST}} & \rot{\textbf{Oxford Flowers}} & \rot{\textbf{Oxford-IIIT Pets}} & \rot{\textbf{PatchCamelyon}} & \rot{\textbf{Rendered-SST2}} & \rot{\textbf{RESISC-45}} & \rot{\textbf{Stanford Cars}} & \rot{\textbf{SUN-397}} & \rot{\textbf{VOC-2007}} & \rot{\textbf{Mean}} \\ \midrule
\multirow{3}{*}{GLIDE~\cite{nichol2021glide}} & ST & 43.66 & 78.75 & 85.70 & 45.54 & 11.07 & 22.07 & \colorbox{best}{51.36} & \colorbox{second_best}{35.30} & 15.86 & 71.33 & 18.05 & \colorbox{best}{58.61} & 37.27 & 18.98 & 62.23 & 64.24 & \colorbox{best}{62.53} & 52.94 & 49.52 & 51.20 & 49.95 & 80.34 & \textbf{48.48} \\
 & CE & 57.35 & 81.35 & 82.35 & 47.83 & 10.16 & 29.84 & 40.48 & 22.82 & 13.35 & 70.74 & 10.48 & 51.87 & \colorbox{best}{43.32} & 16.29 & 49.15 & 56.89 & \colorbox{second_best}{61.98} & 54.97 & 48.87 & 45.27 & \colorbox{best}{58.57} & 78.62 & \textbf{46.93} \\ 
 & DT & 57.55 & 85.38 & 88.64 & 57.64 & 13.17 & 36.38 & 41.64 & 23.79 & \colorbox{best}{18.96} & 79.58 & 17.75 & \colorbox{second_best}{54.62} & 40.79 & 15.91 & \colorbox{second_best}{65.66} & \colorbox{best}{83.96} & 58.36 & \colorbox{second_best}{55.57} & \colorbox{best}{54.87} & 54.33 & \colorbox{second_best}{58.56}  & 81.73 & \colorbox{second_best}{\textbf{52.04}} \\ \hline
\multirow{6}{*}{Stable Diffusion~\cite{rombach2022high}} & ST & 47.90 & 85.79 & 85.20 & 58.63 & 13.34 & 38.14 & 48.42 & 19.17 & 15.09 & \colorbox{best}{82.39} & 20.99 & 46.05 & 8.30 & 19.80 & 64.87 & 77.41 & 51.72 & 47.83 & 51.31 & 58.35 & 54.35 & 79.20 & \textbf{48.83} \\
 & CE & 60.04 & 81.30 & 88.22 & 56.09 & 11.27 & 30.69 & \colorbox{second_best}{50.00} & 29.81 & 13.24 & 73.95 & 8.73 & 50.67 & 24.47 & 22.15 & 45.95 & 57.01 & 54.70 & 50.58 & 49.15 & 54.11 & 55.90 & 80.97 & \textbf{47.68} \\
 & DT & \colorbox{second_best}{61.07} & 86.31 & 88.90 & 60.19 & 13.85 & \colorbox{best}{46.12} & 40.72 & 27.81 & 16.30 & \colorbox{second_best}{82.29} & \colorbox{best}{30.09} & 49.02 & 9.14 & 19.70 & \colorbox{best}{66.13} & \colorbox{second_best}{83.36} & 53.09 & 50.08 & 52.73 & \colorbox{second_best}{58.45} & 57.48 & \colorbox{best}{82.35} & \textbf{51.60} \\
 & w/ NP & 60.19 & \colorbox{second_best}{86.70} & \colorbox{second_best}{89.43} & \colorbox{second_best}{62.95} & \colorbox{best}{14.25} & 44.36 & 35.70 & 28.11 & 15.95 & 81.64 & 22.03 & 44.75 & 22.08 & 17.49 & 63.31 & 80.87 & 50.54 & 53.27 & 52.34 & 58.41 & 57.55 & \colorbox{second_best}{82.32} & \textbf{51.10} \\
 & w/ NP, RD & \colorbox{best}{61.45} & \colorbox{best}{87.26} & \colorbox{best}{89.22} & \colorbox{best}{64.90} & \colorbox{second_best}{14.14} & \colorbox{second_best}{45.69} & 27.94 & 27.42 & 15.23 & 80.99 & 22.03 & 43.30 & 37.27 & \colorbox{second_best}{26.22} & 64.89 & 80.10 & 53.04 & 55.41 & 50.83 & 57.29 & 57.43 & 82.16 & \textbf{52.01} \\  
% & w/ RA & 59.37 & 84.15 & 87.17 & 57.92 & 12.19 & 40.74 & 44.46 & \colorbox{best}{38.17} & \colorbox{second_best}{17.55} & 68.56 & \colorbox{second_best}{25.82} & 43.10 & \colorbox{second_best}{38.82} & \colorbox{best}{29.68} & 63.93 & 74.79 & 53.13 & \colorbox{best}{55.74} & \colorbox{second_best}{54.11} & \colorbox{best}{58.58} & 57.92 & 79.66 & \colorbox{best}{\textbf{52.07}} \\ 
\bottomrule
% \hline
% \rowcolor{ROW_COLOR}
% \multicolumn{2}{c|}{Max.} &  & 87.26 & 82.35 & 64.90 & 13.34 & 46.12 & 49.90 & 28.11 & 16.30 & 82.39 & 30.09 & 51.87 & 43.32 & 26.22 & 66.13 & 83.36 & 61.98 & 55.41 & 52.73 & 58.45 &  & 82.35 \\ 
\end{tabular}%
}
\end{table*}