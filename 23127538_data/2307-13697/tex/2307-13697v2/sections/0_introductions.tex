\section{Introduction}
Large-scale pretrained generative models~\cite{rombach2022high,nichol2021glide,ramesh2022hierarchical,saharia2022photorealistic} have advanced the synthesis of realistic images, opening possibilities for improving visual recognition with generative data. Some studies~\cite{he2022synthetic,bansal2023leaving,shipard2023boosting} have explored the use of generative data for training visual systems with positive results. In addition, the development of cost-efficient and controllable generative models~\cite{zhang2023adding,li2023gligen,hu2021lora,chen2022analog,song2020denoising,kong2021fast,blattmann2022retrieval} has gained more and more attention, offering the potential for richer and more cost-effective external data. To determine whether large pre-trained generative models can serve as efficient data generators and to understand the unique characteristics of generative data in comparison to human-annotated and retrieval data (as depicted in \cref{fig:teaser}), a comprehensive study is required.

% Figure environment removed

To address it, we introduce \gb, a benchmark with 22 datasets and 2548 categories, to evaluate the effectiveness of generative data in a broad range of scenarios. Formally, we measure the quality of generative data on \gb by its test accuracy after linear probing on CLIP~\cite{radford2021learning} model and compare it with other types of external data. The datasets on \gb are grouped into three, namely \common, \finegrained, and \rare~concepts, facilitating our analysis of the generative model's generation capabilities on different types of data.

% Figure environment removed

To validate the quality of generative images as well as other types of external data, we propose the Class-centered Recognition (\textbf{CLER}) score. It is directly correlated with the test accuracy from linear probing CLIP~\cite{radford2021learning} and is an efficient training-free measure for assessing the \textit{improvements} of external data.

In~\cref{sec:analysis}, we provide a detailed analysis of generative data. In~\cref{subsec:tradeoffs}, we compare the performance and cost (in USD) of different data types with a maximum of 500 shots per category, totaling over 1M images. Our results in~\cref{fig:scaling_effect} show that generative data is cost-effective and performs well for common concepts. However, it offers no apparent advantage over original data for fine-grained and rare concepts. 

% Combining~\cref{fig:rel_improvement} and~\cref{fig:scaling_effect}, we demonstrate the advantages of generative data in comparison to CLIP's strong zero-shot baseline performance on various concepts. ~\cref{fig:rel_improvement} reveals that while generative data may not significantly enhance CLIP's performance on common concepts, it exhibits distinct benefits concerning fine-grained concepts and rare concepts that present challenges for CLIP's effective transfer learning. ~\cref{fig:scaling_effect} offers valuable insights into the scaling effect of different types of external data. Notably, generative data emerges as highly advantageous and cost-effective when compared to both original data and retrieval data. The incorporation of generative data showcases its potential to bolster model performance effectively. However, it is important to acknowledge the limitations associated with rare concepts, as they are infrequently represented in stable diffusion's training data, thereby impacting the efficacy of generative data on such concepts.

Our integration of~\cref{fig:rel_improvement} and~\cref{fig:scaling_effect} illustrates generative data's superiority over CLIP's zero-shot baseline in varied conceptual realms. ~\cref{fig:rel_improvement} suggests that although generative data scarcely improves CLIP's aptitude for common concepts, it significantly bolsters its handling of nuanced and rare concepts. ~\cref{fig:scaling_effect} emphasizes generative data's cost-effectiveness and advantage over both original and retrieval data. Notwithstanding, limitations surface with rare concepts due to their sparse representation in stable diffusion's training data, impacting the generative data's efficacy.

We then conducted an examination to delineate the possible scenarios for generative data usage and the particular reasons for its infeasibility in certain circumstances. We first delved deeper into the susceptibility of present text-to-image generative models to different prompt strategies, as detailed in ~\cref{subsec:exp_prompt_strategy}. The evidence unearthed highlights the imperative for custom prompt strategies tailored to specific datasets due to the variability of the optimal strategy. Then our investigation in ~\cref{subsec:domain_gap} shines a light on the potential relationship between the amplification effect of generative data and the average text resemblance between the dataset and the pre-training dataset of generative models. We hypothesize the existence of an inherent bias in generative models, which favors categories that either frequently occur or display high query similarity in their pre-training dataset.

Aiming for the augmentation of generative data, we incorporated external knowledge into pre-trained models by fine-tuning specialized token embeddings for each dataset category via Textual Inversion ~\cite{gal2022image}. Upon injecting varied types of reference data, a consistent performance improvement was observed across most of the 17 datasets examined. Despite these advances, certain limitations were observed that the method did not improve generative data performance on specific datasets with only low-resolution reference images.

In summary, our contributions are as follows:
\vspace{-\topsep}
\begin{enumerate}
  \setlength{\parskip}{1pt}
  \setlength{\itemsep}{0pt plus 1pt}
    \item[(1)] A benchmark, \gb, that comprehensively evaluates the benefits of generative data across a diverse range of visual concepts.
    \item[(2)] A analytical training-free metric \textbf{CLER} score for fast evaluation of the image quality of different types of external data.
    \item[(3)] A detailed examination on the potential applications for generative data, along with the explicit rationales for its infeasibility in specified situations.
    \item[(4)] A comprehensive investigation into the effectiveness of generative data through the injection of external knowledge into pretrained generative models.
\end{enumerate}
\vspace{-\topsep}

% Figure environment removed