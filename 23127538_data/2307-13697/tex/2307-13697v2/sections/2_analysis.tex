\section{Systematic Analysis on Generative Data}
\label{sec:analysis}
In this section, we analyze: \textbf{(1)} the effectiveness and cost trade-offs of generative data compared to other external data, \textbf{(2)} the performance of different generative models with various prompt strategies on a wide range of categories, and \textbf{(3)} the advantages and disadvantages of using generative data on different datasets and their reasons.

\subsection{Trade-offs between Performance and Cost}
\label{subsec:tradeoffs}
Data efficiency refers to the amount of labeled data required for a model to achieve a desired performance. While more data generally improves recognition accuracy and generalization ability, it also means higher annotation costs. Thus, obtaining higher-quality training data through efficient means is a goal of ML practitioners.

With the information of estimated cost per image in~\cref{tab:data_collection}, in~\cref{fig:scaling_effect}, we demonstrate the impact of adding different amount of external data on performance across different concept groups. This figure provides a detailed breakdown and suggests that there are significant differences in the scaling effect across different concept groups. 

For example, in the \common~concepts, the gap between original and generation is gradually reduced with an increasing amount of data per category. Meanwhile, generative data also significantly outperforms the usage of retrieval data. From the cost perspective, as the amount of generative data increases, the advantage of using generative data over original data becomes more significant in \common~concepts. As the figure suggests, obtaining 500-shot original data on \common~concepts costs around 10k USD while generating the same amount of data would only cost 208 USD. The cost difference between the two is around 9,800 USD, but the performance gap is not significant.

However, in the other two groups, \finegrained~and \rare~concepts, using original data is still advantageous, neither generative data nor retrieval data can reach the performance of original data in these concepts.

% why failed in providing analysis on proxy-a-distance.

\subsection{Performance of Various Prompt Strategies}
\label{subsec:exp_prompt_strategy}
We carried out a comprehensive assessment of the generative data obtained from GLIDE and Stable Diffusion using various prompt strategies on 22 datasets. Table~\ref{tab:strategy_comparison} presents the CLER scores of the 20-shot generative data obtained from different prompt strategies with GLIDE and Stable Diffusion on 22 datasets. 

Our findings indicate that while Stable Diffusion 2.1 with dataset-specific prompts generally yielded better results, the optimal strategy varied across different datasets. Therefore, it is important to consider prompt strategy on a case-by-case basis, as there is no universal optimal strategy. To summarize, our observations are as follows:

% This difference in performance may be attributed to the varying capability of the generative models to generate images of different categories. In some object-centric datasets such as CIFAR-10 and ImageNet-1k, we found that the restrictive description strategy was particularly effective. By adding limiting words such as \textit{sharp focus} the generative model was better able to generate object-focused images. 

First, Stable Diffusion and GLIDE perform comparably well with a simple prompt strategy. Despite Stable Diffusion using a larger pretraining dataset and being considered to create higher-quality images, the ability on creating useful generative data for downstream tasks is not much more significant than GLIDE.


Second, the category enhancement strategy in~\cite{he2022synthetic} may result in performance degradation when evaluated on more datasets due to the introduced noise from expanding a category name into a sentence containing other categories. This approach can enhance data diversity but may lead to images corresponding to multiple concepts but a single label. Compared to GLIDE, Stable Diffusion may be better at handling this kind of noise because it incorporates Open CLIP's Text encoder, which has a better ability to attend to information related to the original category in the sentence.

% Second, the category enhancement strategy that was considered useful in~\cite{he2022synthetic} appears to result in some performance degradation in when evaluated on more datasets. This may be due to that simply expanding a category name into a sentence may result in the sentence containing other categories, leading to images that contain multiple concepts but still correspond to a single label. This approach can enhance data diversity to some extent, but it also introduces more noise. Compared to GLIDE, Stable Diffusion may be better at handling this kind of noise because it incorporates Open CLIP's Text encoder, which has better ability to attend to information related to the original category in the sentence. Compared to GLIDE, Stable Diffusion may be better at handling this kind of noise because it incorporates Open CLIP's Text encoder, which has better ability to attend to information related to the original category in the sentence.

Third, advanced prompt strategies can further enhance Stable Diffusion's generative ability. Negative prompts and restrictive descriptions both perform well on average. The latter is especially effective in object-centric datasets like CIFAR-10 and ImageNet-1K, where limiting words such as \textit{sharp focus} improve the model's ability to generate object-focused images.

Fourth, the retrieval augmented strategy improves performance for fine-grained and rare concept datasets such as FGVC-Aircraft, Stanford-Cars, and RESISC45. The categories in these datasets are less frequent in the pretraining dataset of the generative models. Retrieval images as image prompt provide more hints for the generative models to generate high-quality data for these rare categories.

Overall, the above analysis mainly stems from an observational perspective. In \cref{subsec:domain_gap}, we will quantitatively analyze the correlation between the CLER score of the generated data and the dataset-level mean text similarity on a specific dataset.

\subsection{Correlation with Domain Gap}
\label{subsec:domain_gap}
From Figure~\ref{fig:rel_improvement} and Figure~\ref{fig:scaling_effect}, it can be seen that the effectiveness of generative data varies across different groups. As these groups were defined based on empirical observations, in this section, we will further quantify the correlation between the performance improvement and the properties of each specific dataset.

We speculate that the capability of a generative model may differ across different categories, which mainly depends on whether the pre-training data it was trained on covers these categories and their related information. We consider measuring the capability of a generative model when facing different categories with {\it mean text similarity} (MTS). 

For a specific downstream dataset, we used text-to-text retrieval on LAION-400M, which is a subset of LAION-5B, to retrieve the top-$k$ results using category names as queries. Then, we computed the average similarity among these $k$ retrieval results to obtain the MTS for each category. The category-level MTS measures the similarity between the top-$k$ retrieval results obtained from LAION-400M and the category name query. Common categories such as \textit{airplane} tend to have higher MTS scores, whereas rare categories such as \textit{lymph node} tend to have significantly lower MTS scores. We averaged MTS scores for all categories within a dataset to obtain the dataset-level MTS. From~\cref{fig:mean_text_similarity}, we observed the positive correlation between the changes in CLER score and the dataset-level mean text similarity.

% Figure environment removed

\section{Injecting External Knowledge to Generative Models}
\label{sec:tag}

After observing the improvement that simple data generation can bring to various datasets, we remain interested in exploring whether a more effective method for generating higher quality data can be achieved through efficient fine-tuning using a small amount of data.

Building upon this, we consider injecting (1) retrieval data and (2) original data into generative model using Textual Inversion~\cite{gal2022image}. As shown in the conclusion drawn from \cref{fig:scaling_effect}, the cost of obtaining retrieval data is low, while the cost of acquiring original data is relatively high. Therefore, we consider conducting our experiments using a larger quantity of retrieval data and a limited number of original data.

We adopt the method of Textual Inversion~\cite{gal2022image} to finetune the token embeddings $V*$ (a continous vector representations) for each category using external data.
%
For example in aeroplane category from VOC-2007 dataset, for retrieval data, we retrieve 100 images related to \textit{aeroplane} from LAION-400M using text-to-text similarity matching. We then use BLIP-2~\cite{li2023blip} to caption these images, and calculate the similarity between the BLIP-2 generated captions and the word \textit{aeroplane} using BERT~\cite{devlin2018bert}. We select the subset of images whose similarity scores are above threshold of 0.8 (the number of remaining images may vary slightly for each category, typically ranging from 5-30 images). The data will be used to train the token embeddings specific to VOC-2007 aeroplane. During training, the conditional text prompt will follow the naming rule of \texttt{\textless|dataset  category|\textgreater} , etc. During sampling, we add defined prompts on those special tokens, forming the conditional sampling prompts \textit{a photo of \texttt{\textless|voc-2007 aeroplane|\textgreater}}.

The visual results are provided in \cref{fig:demo}. From a direct comparison between the generation results and Stable Diffusion direct sampling, the results generated after Textual Inversion fine-tuning are closer to the style of the reference data. This approach has the potential to gradually move the sampling space of Stable Diffusion towards the target reference data, thus obtaining data that is more similar to the downstream task.

In \cref{tab:add_external_data}, we evaluated 17 datasets from GenBench. It can be seen that adding retrieval data and original data through Textual Inversion finetuning in Stable Diffusion consistently improves performance across the 17 datasets. We highlighted the column for the datasets where the inclusion of external data further improved the performance. Due to space limit, we provide more detailed analysis in appendix by listing the visual examples of generated images for datasets with significant improvement. We also provide an analysis for why finetuning on original images was not able to improve model performance on low-resolution datasets (\eg for CIFAR-10). These results are presented to provide readers with a reference.

Moreover, Textual Inversion is a very lightweight fine-tuning method. We trained 1000 steps for each category, and the training process takes less than 10 minutes on average per category on a A100 GPU.

% Afterwards, we integrate the obtained token embeddings as external knowledge into a Stable Diffusion framework for sampling.


% As generative data's effectiveness is highly correlated with the mean text similarity (MTS) of datasets, it varies significantly in different scenarios, especially in those with large domain gaps. 

% To address this issue, we discuss a practical strategy in this section on how to adapt the generation model to produce images closer to downstream tasks.

% Previous works, like Textual Inversion \cite{gal2022image} and Dreambooth \cite{ruiz2022dreambooth}, fine-tune generative models with a small subset of downstream task images to guide the model to generate relevant images. However, this method requires thousands of training steps and showed limited applicability in our scenario with no significant improvements in our preliminary experiments. 

% Motivated by recent image-to-image generation techiniques~\cite{saharia2022palette,rombach2022high,bansal2023leaving}, we propose Target-Initialized Generation for generating images closer to the target task. Specifically, our method involves using a few target images $\mathbf{x}_{t}$ as the starting point for the generative model sampling process, which produces images that are closer to the target images. To achieve this, we first encode the target images with a pretrained encoder $\tau_{\theta}(\mathbf{x}_{t})$ and run them through a forward diffusion process for $T$ steps, resulting in an approximately normal distribution $\hat{z}_{T}(\mathbf{x}_{t})$. We then generate new images by denoising $\hat{z}_{T}(\mathbf{x}_{t})$ while conditioning on the category name. This approach is equivalent to using target images as the starting point for sampling in the generative model, resulting in the generated images being closer to the target images.

\begin{table*}[htp]
\centering
% \caption{Incorporating retrieval and original data.}
\caption{Analysis of injecting different types of external knowledge by finetuning special token embedding for each category through Textual Inversion across 17 datasets. The \colorbox{best}{entries} indicate improved performance while others indicate degraded performance.}
\label{tab:add_external_data}
\resizebox{\textwidth}{!}{%
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{2}
\begin{tabular}{c|cccc|ccccccccccccccccc}
\toprule
\rowcolor{COLOR_MEAN}
\textbf{Model} & \textbf{FT Data} & \textbf{FT Shot} & \textbf{Gen. Shot} & \textbf{Mean} & \rot{\textbf{Caltech-101}} & \rot{\textbf{Country-211}} & \rot{\textbf{Desc. Textures}} & \rot{\textbf{EuroSAT}} & \rot{\textbf{FER-2013}} & \rot{\textbf{FGVC-Aircraft}} & \rot{\textbf{Food-101}} & \rot{\textbf{GTRSB}} & \rot{\textbf{Hateful Memes}} & \rot{\textbf{Kitti Distance}} & \rot{\textbf{Oxford Flowers}} & \rot{\textbf{Oxford Pets}} & \rot{\textbf{PatchCamelyon}} & \rot{\textbf{Rendered-SST2}} & \rot{\textbf{RESISC-45}} & \rot{\textbf{Stanford Cars}} & \rot{\textbf{VOC-2007}} \\ \hline
Stable Diffusion & - & - & 5-shot & 45.92 & 84.68 & \cellcolor[HTML]{48cae4}11.20 & \cellcolor[HTML]{48cae4}40.27 & 37.88 & \cellcolor[HTML]{48cae4}21.04 & \cellcolor[HTML]{48cae4}14.26 & 72.97 & 23.29 & \cellcolor[HTML]{48cae4}44.16 & \cellcolor[HTML]{48cae4}28.69 & \cellcolor[HTML]{48cae4}52.03 & \cellcolor[HTML]{48cae4}72.94 & 51.61 & 49.54 & \cellcolor[HTML]{48cae4}50.48 & \cellcolor[HTML]{48cae4}50.76 & \cellcolor[HTML]{48cae4}74.79 \\ \hline
 & retrieval & 5$\sim$30-shot & 5-shot & 46.46 & 83.70 & \cellcolor[HTML]{48cae4}9.90 & \cellcolor[HTML]{48cae4}36.91 & 23.82 & \cellcolor[HTML]{48cae4}39.78 & \cellcolor[HTML]{48cae4}13.46 & 69.33 & 22.40 & \cellcolor[HTML]{48cae4}49.02 & \cellcolor[HTML]{48cae4}30.80 & \cellcolor[HTML]{48cae4}58.72 & \cellcolor[HTML]{48cae4}74.63 & 50.32 & 50.08 & \cellcolor[HTML]{48cae4}45.64 & \cellcolor[HTML]{48cae4}51.43 & \cellcolor[HTML]{48cae4}79.94 \\ \cline{2-22} 
\multirow{-2}{*}{+ TI} & original & 5-shot & 5-shot & 50.52 & 80.04 & \cellcolor[HTML]{48cae4}14.65 & \cellcolor[HTML]{48cae4}45.05 & 28.56 & \cellcolor[HTML]{48cae4}35.00 & \cellcolor[HTML]{48cae4}17.43 & 69.93 & 16.93 & \cellcolor[HTML]{48cae4}52.10 & \cellcolor[HTML]{48cae4}42.90 & \cellcolor[HTML]{48cae4}71.06 & \cellcolor[HTML]{48cae4}85.50 & 54.45 & 51.13 & \cellcolor[HTML]{48cae4}56.71 & \cellcolor[HTML]{48cae4}57.25 & \cellcolor[HTML]{48cae4}80.16 \\ \hline
Stable Diffusion & - & - & 100-shot & 48.64 & 86.23 & \cellcolor[HTML]{48cae4}12.65 & \cellcolor[HTML]{48cae4}46.38 & 40.04 & \cellcolor[HTML]{48cae4}28.25 & \cellcolor[HTML]{48cae4}13.16 & 74.42 & 26.98 & \cellcolor[HTML]{48cae4}45.20 & \cellcolor[HTML]{48cae4}25.74 & \cellcolor[HTML]{48cae4}57.42 & \cellcolor[HTML]{48cae4}81.82 & 50.27 & 50.80 & \cellcolor[HTML]{48cae4}52.19 & \cellcolor[HTML]{48cae4}55.07 & \cellcolor[HTML]{48cae4}80.29 \\ \hline 
 & retrieval & 5$\sim$30-shot & 100-shot & 47.75 & 85.68 & \cellcolor[HTML]{48cae4}10.79 & \cellcolor[HTML]{48cae4}37.50 & 24.12 & \cellcolor[HTML]{48cae4}46.58 & \cellcolor[HTML]{48cae4}11.88 & 69.24 & 24.45 & \cellcolor[HTML]{48cae4}49.73 & \cellcolor[HTML]{48cae4}32.22 & \cellcolor[HTML]{48cae4}59.42 & \cellcolor[HTML]{48cae4}76.53 & 50.63 & 51.81 & \cellcolor[HTML]{48cae4}47.68 & \cellcolor[HTML]{48cae4}53.21 & \cellcolor[HTML]{48cae4}80.22 \\ \cline{2-22} 
\multirow{-2}{*}{+ TI} & original & 5-shot & 100-shot & 53.90 & 84.40 & \cellcolor[HTML]{48cae4}19.20 & \cellcolor[HTML]{48cae4}48.41 & 34.74 & \cellcolor[HTML]{48cae4}45.86 & \cellcolor[HTML]{48cae4}17.85 & 70.62 & 25.98 & \cellcolor[HTML]{48cae4}55.51 & \cellcolor[HTML]{48cae4}47.65 & \cellcolor[HTML]{48cae4}72.57 & \cellcolor[HTML]{48cae4}84.98 & 55.51 & 52.19 & \cellcolor[HTML]{48cae4}57.85 & \cellcolor[HTML]{48cae4}61.20 & \cellcolor[HTML]{48cae4}81.83 \\ \bottomrule
\end{tabular}%
}
\end{table*}

% Figure environment removed

% % Figure environment removed


% In \cref{fig:demo}, we directly compare the images generated by Stable Diffusion with and without the TIG method. The second row in the figure (\textit{wo/ TIG}) shows images generated directly by Stable Diffusion using the given textual prompt at the bottom. The third row (\textit{w/ TIG}) displays images generated by Stable Diffusion using TIG with 5-shot original images (as shown in the first row) as reference. The TIG-generated images are noticeably closer to the original images.

% In ~\cref{fig:tag_comparison}, we compare the performance improvement of different methods on four datasets where vanilla generative data (\textit{wo/ TIG}) underperforms. To ensure a fair comparison, we also include the results of using 5 original images per category. The figure shows that TIG significantly improves the quality of generated images and, in some datasets, outperforms the performance of 5-shot original data (gray dotted line).