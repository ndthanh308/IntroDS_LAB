\section{Details on Datasets}
In \cref{tab:data_details}, we include the basic statistics of 22 datasets in \textbf{GenBench}. We categorized the dataset into concept groups and listed the number of validation sets and task types for each dataset. 

The majority of the datasets are multi-class tasks, where each image is associated with a single label among multiple categories. VOC 2007 dataset is an exception, as it is a multi-label task, where each image could be associated with multiple labels. Due to the limitations of current generative models and the scope of our study, multi-label task is treated as multi-class task in both for both generative data and retrieval data. Specifically, for each category, we only generate images containing this category and do not consider involving multiple objects.

\begin{table*}[htp]
\centering
\caption{Dataset details, grouped by concepts.}
\label{tab:data_details}
\resizebox{\textwidth}{!}{%
\setlength{\tabcolsep}{12pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c|c|c|c|c|c}
\toprule
\textbf{Division} & \textbf{Datasets} & \textbf{\#Categories} & \textbf{Validation Size} & \textbf{Evaluation   Metric} & \textbf{Task Type} \\ \midrule
\multirow{7}{*}{\common} & ImageNet-1K & 1,000 & 50,000 & Accuracy & Multi-Class \\
 & CIFAR-10 & 10 & 10,000 & Accuracy & Multi-Class \\
 & CIFAR-100 & 100 & 10,000 & Accuracy & Multi-Class \\
 & Caltech-101 & 101 & 6,085 & Mean Per Class & Multi-Class \\
 & VOC 2007 & 20 & 4,952 & 11-point mAP & Multi-Label \\
 & MNIST & 10 & 10,000 & Accuracy & Multi-Class \\
 & SUN397 & 397 & 19,850 & Accuracy & Multi-Class \\ \midrule
\textbf{Total} & \textbf{-} & \textbf{1,638} & \textbf{110,887} & \textbf{-} & \textbf{-} \\ \midrule
\multirow{6}{*}{\finegrained} & Food-101 & 101 & 25,250 & Accuracy & Multi-Class \\
 & Oxford-IIIT Pets & 37 & 3,669 & Mean Per Class & Multi-Class \\
 & Oxford Flowers & 102 & 6,149 & Mean Per Class & Multi-Class \\
 & Stanford Cars & 196 & 8,041 & Accuracy & Multi-Class \\
 & FGVC Aircraft & 100 & 3,333 & Mean Per Class & Multi-Class \\
 & Country-211 & 211 & 21,100 & Accuracy & Multi-Class \\ \midrule
\textbf{Total} & \textbf{-} & \textbf{747} & \textbf{67,542} & \textbf{-} & \textbf{-} \\ \midrule
\multirow{9}{*}{\rare} & PatchCamelyon & 2 & 32,768 & Accuracy & Multi-Class \\
 & EuroSAT & 10 & 5,000 & Accuracy & Multi-Class \\
 & GTSRB & 43 & 12,630 & Accuracy & Multi-Class \\
 & Rendered-SST2 & 2 & 1,821 & Accuracy & Multi-Class \\
 & FER 2013 & 2 & 3,574 & Accuracy & Multi-Class \\
 & RESISC-45 & 45 & 25,200 & Accuracy & Multi-Class \\
 & Hateful Memes & 2 & 500 & ROC AUC & Multi-Class \\
 & Describable Textures & 47 & 1,880 & Accuracy & Multi-Class \\
 & KITTI Distance & 4 & 711 & Accuracy & Multi-Class \\ \midrule
\textbf{Total} & \textbf{-} & \textbf{157} & \textbf{84,084} & \textbf{-} & \textbf{-} \\ \bottomrule
\end{tabular}%
}
\end{table*}

\section{Details on Measuring Data Aquisation Cost}
In this section, we provide a detailed explanation of how we calculated the acquisition costs for different types of external data.

For generative data, we use the NC24ads A100 v4 instance available on the Azure platform and the Stable Diffusion 2.1 model from~\url{https://github.com/huggingface/diffusers}. The machine contains one A100 GPU, 24 vCPUs, and 220 GB of memory. We estimated the inference time required to generate 1,000 images and calculated the corresponding time and cost per image. 

For retrieval data, since text-to-text search does not require GPU computation, we employed a CPU-based instance, E32ads v5, for this purpose. This machine contains 32 vCPUs and 256 GB of memory, and is less expensive. Due to the fast speed of text-to-text retrieval, we estimated the time required to acquire 10,000 images and scaled it down to the time required for 1,000 images.

To better approximate real-world usage, we used the spot price (in USD) on Azure of both GPU/CPU instance as a reference value. In~\cref{tab:data_acquisation}, we present the prices and the estimated time required to acquire 1,000 images, as well as the calculated cost per image.

It should be noted that the estimated acquisition cost may be affected by the specific GPU and CPU environment, as well as the implementation of model acceleration techniques. In our implementation of the Stable Diffusion inference, we utilized xformers (available at~\url{https://github.com/facebookresearch/xformers}) to optimize memory usage, but did not employ any other quantization strategies.

For original data collected by human labeler, we utilized the reference values provided on~\url{https://aws.amazon.com/cn/sagemaker/data-labeling/pricing/}. It should be noted that the \textit{original data} used in our study refers to the labeled training data provided by the datasets on \textbf{GenBench}, which we consider as equivalent to manually collected human-labeled data.

\begin{table*}[htp]
\centering
\caption{Breakdown on data acquisation cost for different types of external data.}
\label{tab:data_acquisation}
\resizebox{\textwidth}{!}{%
\setlength{\tabcolsep}{12pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{@{}c|c|c|c|c|c@{}}
\toprule
\textbf{Data Type} & \textbf{Collection Source} & \textbf{Instance Type} & \textbf{Price/Hours} & \textbf{Estimate Hours/1K Images} & \textbf{Estimate Cost/Image} \\ \midrule
Generative & Model Inference & NC24ads A100 v4 & 1.47 & 1.74 & 2.54 $\times$ $10^{-4}$ \\
Retrieval & Text to Text Search & E32ads v5 & 0.21 & 0.60 & 3.93 $\times$ $10^{-5}$ \\
Original & Human Labeling & - & - & - & 1.20 $\times 10^{-2}$ \\ \bottomrule
\end{tabular}%
}
\end{table*}

\section{Details on Prompt Strategies}

In~\cref{tab:prompt_examples}, we list the specific prompts for 5 categories under different prompt strategies. Specifically, the categories are: airplane (CIFAR-10), British Shorthair (Oxford-IIT Pets), Acura TL Sedan 2012 (Stanford Cars), and red and white circle no truck entry (GTRSB). The names in brackets refer to corresponding datasets for each category, they also respectively correspond to \common, \finegrained, and \rare~concepts on GenBench.

Specifically, with simple template strategy, only the category name (\eg \textit{airplane}) is given as the text prompt for generative model. With defined template strategy, a more dataset-specific text prompt is created using the CLIP templates~\footnote{\href{https://github.com/openai/CLIP/blob/main/data/prompts.md}{CLIP Templates}}, such as \textit{a black and white photo of the airplane}. In some datasets, there may also be corresponding explanations, such as in FGVC Aircraft dataset, where the template for 737-400 includes the explanation \textit{a type of aircraft}. 

The idea behind the restrictive description strategy mainly comes from suggestions for generating images from the Stable Diffusion community. We consider using limiting words such as \textit{hires} and \textit{sharp focus} and adding \textit{(())} to increase the text encoder's attention weight on these restrictive words. Usually, this type of prompt strategy can help the model generate more object-centric images.

For negative prompts, we consider adding names of other categories within the same dataset during the inference process of Stable Diffusion, in order to restrict the model from generating images of other categories. Empirically, for datasets with a large number of categories, we only consider using a subset of categories (such as randomly selecting 5 categories) as negative prompts.

For category enhancement, the same strategy has been considered in~\cite{he2022synthetic}, we use the same word-to-sentence model to expand a category name into a complete sentence. This approach increases the diversity of text prompts, as well as the final generated images.

% In Section~\ref{sec:generated_examples}, 
We present the images generated using the example prompts listed in Table~\ref{tab:prompt_examples}. By comparing the generated images for each category, the audience can check the differences in image quality generated by different prompts for the same category. This provides a way for evaluating the effectiveness of the different prompt strategies in terms of their ability to produce diverse and high-quality images.

\begin{table*}[htp]
\centering
\caption{The specific prompt example under different prompt strategies.}
\label{tab:prompt_examples}
\resizebox{\textwidth}{!}{%
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{@{}c|c|c@{}}
\toprule
\textbf{Prompt   Strategy} & \textbf{Example} & \textbf{Category} \\ \midrule
\multirow{5}{*}{Simple Template} & airplane & airplane \\
 & British Shorthair & British Shorthair \\
 & Acura TL Sedan 2012 & Acura TL Sedan 2012 \\
 & 737-400 & 737-400 \\
 & red and white circle no truck entry & red and white circle no turck entry \\ \midrule
\multirow{5}{*}{Defined Template} & a black and white   photo of the airplane. & airplane \\
 & a photo of a British Shorthair, a type of pet. & British Shorthair \\
 & a photo of my old Acura TL Sedan 2012. & Acura TL Sedan 2012 \\
 & a photo of a 737-400, a type of aircraft. & 737-400 \\
 & a close up photo of a red and white circle no truck entry, a traffic   sign. & red and white circle no turck entry \\ \midrule
\multirow{5}{*}{Category Enhancement} & An airplane is sitting on the runway. & airplane \\
 & An all white soldier shows off a shorthaired British & British Shorthair \\
 & a little boy in an Acura TL Sedan in 2012. & Acura TL Sedan 2012 \\
 & a aircraft is reported to be sitting in the plane class for aircraft number 737-400 & 737-400 \\
 & a truck is stamped in red and white during entry & red and white circle no turck entry \\ \midrule
\multirow{5}{*}{Restrictive Description} & a photo of the big   airplane., ((sharp focus)), ((highly detailed)), ((hires)) & airplane \\
 & a photo of a British Shorthair, a type of pet., ((sharp focus)),   ((highly detailed)), ((hires)) & British Shorthair \\
 & a photo of my clean Acura TL Sedan 2012., ((sharp focus)), ((highly   detailed)), ((hires)) & Acura TL Sedan 2012 \\
 & a photo of a 737-400, a type of aircraft., ((sharp focus)), ((highly   detailed)), ((hires)) & 737-400 \\
 & a close up photo of a red and white circle no truck entry, a traffic   sign., ((sharp focus)), ((highly detailed)), ((hires)) & red and white circle no turck entry \\ \midrule
\multirow{5}{*}{Negative Prompts} & \textbf{Prompt:} a good photo   of the airplane. \textbf{Negative Prompt:} automobile, ship, truck, car, forg & airplane \\
 & \textbf{Prompt:} a photo of a British Shorthair, a type of pet. \textbf{Negative Prompt:} Bombay, boxer, leonberger, Persian, Siamese & British Shorthair \\
 & \textbf{Prompt:} a photo of my new Acura TL Sedan 2012. \textbf{Negative Prompt:} Audi V8 Sedan 1994, Bentley Arnage Sedan 2009… & Acura TL Sedan 2012 \\
 & \textbf{Prompt:} a photo of a 737-400, a type of aircraft. \textbf{Negative Prompt:} 737-800, 747-100, 757-200, 767-200, II-76 & 737-400 \\
 & \textbf{Prompt:} a centered photo of a red and white circle no truck entry, a traffic sign. \textbf{Negative Prompt:} empty red and whilte circle & red and white circle no turck entry \\ \bottomrule
\end{tabular}%
}
\end{table*}

% \subsection{Examples with Various Prompt Strategies}
% \label{sec:generated_examples}
% In~\cref{fig:appendix_example}, we present a comparison of the images generated by different prompt strategies for different categories. The figure shows that different prompt strategies have varying impacts on the generated images. For instance, adding restrictive descriptions tends to produce more object-centric images, but for some rare concepts (e.g. red and white no entry from GTSRB dataset), these descriptions may be more distracting than helpful. 

% Regarding the generated images from category enhancement, we should note that using this language expansion technique can potentially alter the original semantic information of the category name. For example, the category \textit{British Shorthair} (a breed of cat) could potentially be expanded to \textit{An all-white soldier shows off a shorthaired British}, which alters the intended meaning of the original category and results in poor-quality images (see second row in~\cref{fig:appendix_example}).

% As mentioned in the main paper, it cannot be confirmed that there is an optimal prompt strategy that is suitable for all categories. Therefore, when selecting prompt strategies for specific datasets or categories, some human expertise is still necessary. We plan to further improve this aspect in our future work.

\section{Examples on Injecting Different External Data}

In \cref{fig:positive_example1} and \cref{fig:positive_example2}, we select four datasets (FER-2013, Oxford-Flowers, Hateful-Memes, and KITTI-Distance) and provided a comparison of the generated images via direct sampling Stable Diffusion and after Textual Inversion finetuning with different types of external data. These four datasets showed a significant improvement in CLER Score after incorporating external data, as shown in Table 3 from main paper.

For the FER-2013 dataset shown in \cref{fig:positive_example1}, most of the content is related to human faces. The results generated directly by Stable Diffusion for faces were relatively poor (due to privacy concerns, many faces in the training data have varying degrees of mosaic obscuration). However, after incorporating retrieval and original data, the generated images improved significantly for the face region. Among them, the use of original data greatly enhanced the generation of realistic style faces. For the Hateful Memes dataset, it may also be due to a similar reason, where the results generated by Stable Diffusion itself are not good enough.

For fine-grained classification datasets such as Oxford Flowers, incorporating external knowledge can greatly enhance the model's ability to capture category differences and reduce misunderstandings on specific terms. For example, in \cref{fig:positive_example1}, the term \textit{air plant} and \textit{clot foot} in the context of flowers can be confusing because \textit{air}, \textit{plant} and \textit{foot} can be understood separately. This can increase the difficulty for Stable Diffusion to generate the correct category, and thus leading to incorrect results. After introducing external retrieval or original data, we can observe that the generated images become significantly more accurate.

For another type of specific dataset like KITTI Distance, where all data comes from street scenes, we can observe that both the Stable Diffusion direct sampling and incoporating retrieval data can generate decent images but relatively low CLER Score. However, after incorporating original data, the generated images style naturally adapts to the street scenes in the KITTI Distance dataset, which can explain the significant performance improvement on this dataset.


\section{Failures on Finetuning with Low-res Original Data}

% Figure environment removed

During finetuning on original data, we also encountered some failure cases~\ref{fig:low_res_failures}. Here, we share them to prevent others from conducting redundant experiments. We found that during Textual Inversion fine-tuning, when the resolution of the reference data is below a certain threshold (less than 32x32 pixels), this subset cannot be generated well by Stable Diffusion. The reconstruction loss during training is difficult to converge, and the generated results, as well as their CLER Scores, are relatively low. This may be because the resolution of the training data used by Stable Diffusion is generally higher than this threshold, resulting in a large domain gap when fine-tuning on low-resolution images. In Figure 2, we share a failure case on the CIFAR-10 dataset.

\section{Extended Analysis}
In order to demonstrate that the CLER Score can effectively verify data quality and quantify the improvement of downstream tasks after adding the data to training, we conducted correlation analysis on linear probing accuracy in the main paper. In this section, we further investigate the CLER score performance when evaluating on different architectures.

% % Figure environment removed

% Figure environment removed

\subsection{Correlation with Other External Data}

% Figure environment removed

In~\cref{fig:other_external}, we examined the correlation between CLER score and linear probe accuracy on other types of external data, including both original and retrieval data. As depicted in the figure, we observed a strong correlation between CLER score and linear probe accuracy for the original data, while the correlation was slightly weaker for the retrieval data. These results indicate that CLER score can be applied more broadly to assess the quality of external data, including both human-labeled and machine-generated data. Moreover, the efficient validation of external data quality provided by CLER score can be particularly beneficial for downstream tasks that require large amounts of labeled data, as it can reduce the time and cost of collecting and labeling data. Additionally, since CLER score does not require any training, it can be used as a quick and easy method to evaluate the quality of external data before incorporating it into downstream tasks. However, it should be noted that while CLER score is strongly correlated with linear probe accuracy, further research is still needed to fully explore its ability to represent finetuned accuracy on downstream tasks.

\subsection{Correlation with Retrieval Metrics}

From a conceptual standpoint, the CLER Score essentially quantifies how closely generated images align with the per-class cluster centers of downstream test data within the embedding space. A smaller distance leads to a higher CLER score, thereby enabling generative models to optimize further and produce high-quality images beneficial for downstream tasks.

Moreover, we acknowledge that the image-to-image retrieval precision-recall metric shares a similar concepts. Thus, we conduct experiments on ImageNet-1k datasets, utilizing generated images from various generative models and configurations. For each dataset, we performed image-to-image retrieval per class by computing CLIP embeddings of both generated and test images, retrieving the top-5 nearest test embeddings based on cosine similarity, and calculating precision@5 and recall@5 metrics. Subsequently, we conducted a correlation analysis between retrieval precision and the CLER Score. These additional experiments and discussions have been incorporated into Section 5.1 (highlighted in blue), providing a comparative analysis between CLER and retrieval precision-recall metrics to further validate the effectiveness of the CLER metric.

\subsection{Evaluation on Other Architectures}

To evaluate performance on different architectures, we selected several other pre-trained models provided by CLIP, including ResNet-50, VIT-B/32, VIT-B/16, and VIT-L/14, ranked by the number of model parameters. Due to computational resource constraints, the experiments are conducted on six datasets (Caltech-101, EuroSAT, FGVC Aircraft, Hateful Memes, MNIST, and VOC-2007). From~\cref{fig:arch_scaling}, as observed in the results, a strong correlation was observed between CLER score and linear probing accuracy for all the pre-trained models, even when the model parameters were increased. Although the ViT-L/14 model has relatively larger errors on some datasets, its CLER score and linear probing accuracy remained highly correlated. However, for larger model parameters (\eg exceeding 1 billion parameters), it remains unclear whether CLER score can continue to indicatively evaluate the quality of external data. Further investigation is needed to address this question.

% Figure environment removed

% Figure environment removed

\subsection{Discussion on Distilling Pretrained Knowledge into Downstream Models}

\textcolor{black}{Indeed, we acknowledge that our approach primarily focuses on distilling pretrained image generation models into downstream recognition models. Given the rapid proliferation of pretrained models trained extensively on large-scale datasets with substantial computational resources, our method explicitly leverages this prevailing trend. Specifically, we outline how our approach effectively capitalizes on the rich knowledge embedded in these larger generative models, highlighting practical trade-offs and efficiency improvements for downstream recognition tasks.}

\subsection{Discussion on Limitations on Rare Concepts}

\textcolor{black}{As mentioned above, we also face limitations when pretrained generation models do not have enough knowledge about rare concepts. These limitations mainly appear as poor performance when generating images from rare categories, leading to lower effectiveness in downstream tasks. However, we consider this issue temporary, as generative models will likely improve with larger training datasets and more computational resources. Therefore, our method of distilling knowledge from these models remains effective and useful despite current limitations. We have also clearly explained dataset biases and the specific difficulties involved in rare and fine-grained categories. Moreover, we highlight that using external knowledge injection methods like Textual Inversion can help reduce these biases, providing a practical way to overcome current challenges and improve future model performance.}

\section*{Acknowledgement}
This study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOE-T2EP20221-0012, MOE-T2EP20223-0002), and under the RIE2020 Industry Alignment Fund – Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).
% % Figure environment removed

\begin{IEEEbiography}[{% Figure removed}]{Bo Li} received the B.S. from Harbin Institute of Technology in 2020. He is currently working towards the PhD degree in the College of Computer and Data Science, Nanyang Technological University, Singapore. His research interests mainly include multimodal learning and foundation models.
\end{IEEEbiography}


\begin{IEEEbiography}[{% Figure removed}]{Haotian Liu} received the B.E. (with honors) from Zhejiang University and the Ph.D. degree from the University of Wisconsin–Madison in 2024, under the supervision of Prof. Yong Jae Lee. He is currently a member of technical staff at xAI. His research interests include computer vision, multimodal learning, and foundation models.
\end{IEEEbiography}


\begin{IEEEbiography}[{% Figure removed}]{Liangyu Chen} received his B.Eng. from Nanyang Technological University, Singapore, in 2022. He worked at MMLab@NTU from 2022 to 2024. He is currently pursuing a Ph.D. in Computer Science at Stanford University. He researches multimodal foundation models and data-centric machine learning.
\end{IEEEbiography}


\begin{IEEEbiography}[{% Figure removed}]{Yong Jae Lee} is an Associate Professor in the Department of Computer Sciences at the University of Wisconsin-Madison. His main research interests are in computer vision and machine learning. He is particularly interested in creating robust AI systems that can understand our multimodal world with minimal human supervision. He received his Ph.D. from UT-Austin, and was a postdoc at CMU and UC Berkeley.
\end{IEEEbiography}

\begin{IEEEbiography}[{% Figure removed}]{Chunyuan Li}’s recent focus is large multimodal models in vision-and-language. His contributions include the development of LLaVA and the series of model families, as well as earlier works such as Oscar, GLIP, Grounding DINO, GLIGEN and Florence. He has worked with xAI, ByteDance, Microsoft Research, and obtained his PhD at Duke University.

\end{IEEEbiography}


\begin{IEEEbiography}[{% Figure removed}]{Ziwei Liu} is currently an Associate Professor at Nanyang Technological University, Singapore. His research revolves around computer vision, machine learning and computer graphics. He has published extensively on top-tier conferences and journals in relevant fields, including CVPR, ICCV, ECCV, NeurIPS, ICLR, SIGGRAPH, TPAMI, TOG and Nature - Machine Intelligence. He is the recipient of PAMI Mark Everingham Prize, CVPR Best Paper Award Candidate, Asian Young Scientist Fellowship, International Congress of Basic Science Frontiers of Science Award and MIT Technology Review Innovators under 35 Asia Pacific. He serves as an Area Chair of CVPR, ICCV, ECCV, NeurIPS and ICLR, as well as an Associate Editor of IJCV.
\end{IEEEbiography}