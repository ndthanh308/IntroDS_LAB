\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{fer2013}
{FER} 2013: Kaggle challenges in representation learning facial expression
  recognition.
\newblock \url{https://www.kaggle.com/}.

\bibitem{bansal2023leaving}
Hritik Bansal and Aditya Grover.
\newblock Leaving reality to imagination: Robust classification via generated
  datasets.
\newblock {\em arXiv preprint arXiv:2302.02503}, 2023.

\bibitem{besnier2020dataset}
Victor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu Cord, and Patrick
  P{\'e}rez.
\newblock This dataset does not exist: training models from generated images.
\newblock In {\em ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 1--5. IEEE, 2020.

\bibitem{blattmann2022retrieval}
Andreas Blattmann, Robin Rombach, Kaan Oktay, and Bj{\"o}rn Ommer.
\newblock Retrieval-augmented diffusion models.
\newblock {\em arXiv preprint arXiv:2204.11824}, 2022.

\bibitem{borgeaud2021improving}
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza
  Rutherford, Katie Millican, George van~den Driessche, Jean-Baptiste Lespiau,
  Bogdan Damoc, Aidan Clark, et~al.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock {\em arXiv preprint arXiv:2112.04426}, 2021.

\bibitem{bossard2014food}
Lukas Bossard, Matthieu Guillaumin, and Luc~Van Gool.
\newblock Food-101--mining discriminative components with random forests.
\newblock In {\em ECCV}, 2014.

\bibitem{brock2018large}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale gan training for high fidelity natural image synthesis.
\newblock {\em arXiv preprint arXiv:1809.11096}, 2018.

\bibitem{chen2022making}
Liangyu Chen, Yutong Bai, Siyu Huang, Yongyi Lu, Bihan Wen, Alan~L Yuille, and
  Zongwei Zhou.
\newblock Making your first choice: To address cold start problem in vision
  active learning.
\newblock {\em arXiv preprint arXiv:2210.02442}, 2022.

\bibitem{chen2022analog}
Ting Chen, Ruixiang Zhang, and Geoffrey Hinton.
\newblock Analog bits: Generating discrete data using diffusion models with
  self-conditioning.
\newblock {\em arXiv preprint arXiv:2208.04202}, 2022.

\bibitem{chen2022murag}
Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William~W Cohen.
\newblock Murag: Multimodal retrieval-augmented generator for open question
  answering over images and text.
\newblock {\em arXiv preprint arXiv:2210.02928}, 2022.

\bibitem{cheng2017remote}
Gong Cheng, Junwei Han, and Xiaoqiang Lu.
\newblock Remote sensing image scene classification: Benchmark and state of the
  art.
\newblock {\em Proceedings of the IEEE}, 2017.

\bibitem{cimpoi2014describing}
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea
  Vedaldi.
\newblock Describing textures in the wild.
\newblock In {\em CVPR}, 2014.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{dosovitskiy2015flownet}
Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas,
  Vladimir Golkov, Patrick Van Der~Smagt, Daniel Cremers, and Thomas Brox.
\newblock Flownet: Learning optical flow with convolutional networks.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2758--2766, 2015.

\bibitem{everingham2009pascal}
Mark Everingham, Luc Van~Gool, Christopher~KI Williams, John Winn, and Andrew
  Zisserman.
\newblock The pascal visual object classes (voc) challenge.
\newblock {\em International journal of computer vision}, 88:303--308, 2009.

\bibitem{fei2004learning}
Li Fei-Fei, Rob Fergus, and Pietro Perona.
\newblock Learning generative visual models from few training examples: An
  incremental bayesian approach tested on 101 object categories.
\newblock In {\em 2004 conference on computer vision and pattern recognition
  workshop}, pages 178--178. IEEE, 2004.

\bibitem{fritsch2013new}
Jannik Fritsch, Tobias Kuehnl, and Andreas Geiger.
\newblock A new performance measure and evaluation benchmark for road detection
  algorithms.
\newblock In {\em ITSC}. IEEE, 2013.

\bibitem{gal2022image}
Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit~H Bermano, Gal
  Chechik, and Daniel Cohen-Or.
\newblock An image is worth one word: Personalizing text-to-image generation
  using textual inversion.
\newblock {\em arXiv preprint arXiv:2208.01618}, 2022.

\bibitem{goodfellow2020generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial networks.
\newblock {\em Communications of the ACM}, 63(11):139--144, 2020.

\bibitem{guu2020realm}
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.
\newblock Realm: Retrieval-augmented language model pre-training.
\newblock {\em arXiv preprint arXiv:2002.08909}, 2020.

\bibitem{he2022synthetic}
Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song
  Bai, and Xiaojuan Qi.
\newblock Is synthetic data from generative models ready for image recognition?
\newblock {\em arXiv preprint arXiv:2210.07574}, 2022.

\bibitem{helber2017eurosat}
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth.
\newblock Eurosat: A novel dataset and deep learning benchmark for land use and
  land cover classification, 2017.

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6840--6851, 2020.

\bibitem{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em arXiv preprint arXiv:2106.09685}, 2021.

\bibitem{jahanian2021generative}
Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola.
\newblock Generative models as a data source for multiview representation
  learning.
\newblock {\em arXiv preprint arXiv:2106.05258}, 2021.

\bibitem{khandelwal2019generalization}
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.
\newblock Generalization through memorization: Nearest neighbor language
  models.
\newblock {\em arXiv preprint arXiv:1911.00172}, 2019.

\bibitem{kiela2020hateful}
Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh,
  Pratik Ringshia, and Davide Testuggine.
\newblock The hateful memes challenge: Detecting hate speech in multimodal
  memes.
\newblock {\em NeurIPS}, 2020.

\bibitem{kong2021fast}
Zhifeng Kong and Wei Ping.
\newblock On fast sampling of diffusion probabilistic models.
\newblock {\em arXiv preprint arXiv:2106.00132}, 2021.

\bibitem{krause20133d}
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In {\em ICCV workshops}, 2013.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{lewis2020retrieval}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
  Karpukhin, Naman Goyal, Heinrich K{\"u}ttler, Mike Lewis, Wen-tau Yih, Tim
  Rockt{\"a}schel, et~al.
\newblock Retrieval-augmented generation for knowledge-intensive {NLP} tasks.
\newblock {\em NeurIPS}, 2020.

\bibitem{li2022elevater}
Chunyuan Li, Haotian Liu, Liunian~Harold Li, Pengchuan Zhang, Jyoti Aneja,
  Jianwei Yang, Ping Jin, Yong~Jae Lee, Houdong Hu, Zicheng Liu, et~al.
\newblock Elevater: A benchmark and toolkit for evaluating language-augmented
  visual models.
\newblock {\em arXiv preprint arXiv:2204.08790}, 2022.

\bibitem{li2023blip}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock {\em arXiv preprint arXiv:2301.12597}, 2023.

\bibitem{li2023gligen}
Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao,
  Chunyuan Li, and Yong~Jae Lee.
\newblock Gligen: Open-set grounded text-to-image generation.
\newblock {\em arXiv preprint arXiv:2301.07093}, 2023.

\bibitem{liu2023react}
Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong~Jae Lee, and
  Chunyuan Li.
\newblock Learning customized visual models with retrieval-augmented knowledge.
\newblock 2023.

\bibitem{liu2020k}
Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping
  Wang.
\newblock K-{BERT}: Enabling language representation with knowledge graph.
\newblock In {\em AAAI}, 2020.

\bibitem{long2022retrieval}
Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait,
  Ravi Garg, Alan Blair, Chunhua Shen, and Anton van~den Hengel.
\newblock Retrieval augmented classification for long-tail visual recognition.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 6959--6969, 2022.

\bibitem{maji2013fine}
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi.
\newblock Fine-grained visual classification of aircraft.
\newblock {\em arXiv preprint arXiv:1306.5151}, 2013.

\bibitem{marino2021krisp}
Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach.
\newblock Krisp: Integrating implicit and symbolic knowledge for open-domain
  knowledge-based {VQA}.
\newblock In {\em CVPR}, 2021.

\bibitem{nichol2021glide}
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
  Bob McGrew, Ilya Sutskever, and Mark Chen.
\newblock Glide: Towards photorealistic image generation and editing with
  text-guided diffusion models.
\newblock {\em arXiv preprint arXiv:2112.10741}, 2021.

\bibitem{nichol2021improved}
Alexander~Quinn Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock In {\em International Conference on Machine Learning}, pages
  8162--8171. PMLR, 2021.

\bibitem{nilsback2008automated}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In {\em Indian Conference on Computer Vision, Graphics \& Image
  Processing}. IEEE, 2008.

\bibitem{parkhi2012cats}
Omkar~M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar.
\newblock Cats and dogs.
\newblock In {\em CVPR}, 2012.

\bibitem{peng2017visda}
Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate
  Saenko.
\newblock Visda: The visual domain adaptation challenge.
\newblock {\em arXiv preprint arXiv:1710.06924}, 2017.

\bibitem{peters2019knowledge}
Matthew~E Peters, Mark Neumann, Robert~L Logan~IV, Roy Schwartz, Vidur Joshi,
  Sameer Singh, and Noah~A Smith.
\newblock Knowledge enhanced contextual word representations.
\newblock {\em arXiv preprint arXiv:1909.04164}, 2019.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em ICML}, 2021.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em The Journal of Machine Learning Research}, 21(1):5485--5551,
  2020.

\bibitem{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock {\em arXiv preprint arXiv:2204.06125}, 2022.

\bibitem{reed2016generative}
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and
  Honglak Lee.
\newblock Generative adversarial text to image synthesis.
\newblock In {\em International conference on machine learning}, pages
  1060--1069. PMLR, 2016.

\bibitem{richter2016playing}
Stephan~R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun.
\newblock Playing for data: Ground truth from computer games.
\newblock In {\em Computer Vision--ECCV 2016: 14th European Conference,
  Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14},
  pages 102--118. Springer, 2016.

\bibitem{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10684--10695, 2022.

\bibitem{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International journal of computer vision}, 115:211--252, 2015.

\bibitem{saharia2022photorealistic}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily
  Denton, Seyed Kamyar~Seyed Ghasemipour, Burcu~Karagol Ayan, S~Sara Mahdavi,
  Rapha~Gontijo Lopes, et~al.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock {\em arXiv preprint arXiv:2205.11487}, 2022.

\bibitem{shipard2023boosting}
Jordan Shipard, Arnold Wiliem, Kien~Nguyen Thanh, Wei Xiang, and Clinton
  Fookes.
\newblock Boosting zero-shot classification with synthetic data diversity via
  stable diffusion.
\newblock {\em arXiv preprint arXiv:2302.03298}, 2023.

\bibitem{sohl2015deep}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In {\em International Conference on Machine Learning}, pages
  2256--2265. PMLR, 2015.

\bibitem{song2020denoising}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock {\em arXiv preprint arXiv:2010.02502}, 2020.

\bibitem{stallkamp2011german}
Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel.
\newblock The german traffic sign recognition benchmark: a multi-class
  classification competition.
\newblock In {\em IJCNN}, 2011.

\bibitem{veeling2018rotation}
Bastiaan~S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling.
\newblock Rotation equivariant cnns for digital pathology.
\newblock In {\em MICCAI}, 2018.

\bibitem{wu2021multi}
Jialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh Mottaghi.
\newblock Multi-modal answer validation for knowledge-based {VQA}.
\newblock {\em arXiv preprint arXiv:2103.12248}, 2021.

\bibitem{xiao2010sun}
Jianxiong Xiao, James Hays, Krista~A Ehinger, Aude Oliva, and Antonio Torralba.
\newblock Sun database: Large-scale scene recognition from abbey to zoo.
\newblock In {\em 2010 IEEE computer society conference on computer vision and
  pattern recognition}, pages 3485--3492. IEEE, 2010.

\bibitem{yang2021empirical}
Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and
  Lijuan Wang.
\newblock An empirical study of {GPT}-3 for few-shot knowledge-based {VQA}.
\newblock {\em arXiv preprint arXiv:2109.05014}, 2021.

\bibitem{yasunaga2022retrieval}
Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec,
  Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih.
\newblock Retrieval-augmented multimodal language modeling.
\newblock {\em arXiv preprint arXiv:2211.12561}, 2022.

\bibitem{yu2021dict}
Wenhao Yu, Chenguang Zhu, Yuwei Fang, Donghan Yu, Shuohang Wang, Yichong Xu,
  Michael Zeng, and Meng Jiang.
\newblock Dict-bert: Enhancing language model pre-training with dictionary.
\newblock {\em arXiv preprint arXiv:2110.06490}, 2021.

\bibitem{zhai2019large}
Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos
  Riquelme, Mario Lucic, Josip Djolonga, Andre~Susano Pinto, Maxim Neumann,
  Alexey Dosovitskiy, et~al.
\newblock A large-scale study of representation learning with the visual task
  adaptation benchmark.
\newblock {\em arXiv preprint arXiv:1910.04867}, 2019.

\bibitem{zhang2023adding}
Lvmin Zhang and Maneesh Agrawala.
\newblock Adding conditional control to text-to-image diffusion models.
\newblock {\em arXiv preprint arXiv:2302.05543}, 2023.

\end{thebibliography}
