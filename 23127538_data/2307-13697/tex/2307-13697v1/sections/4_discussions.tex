\section{Discussions}

\paragraph{Takeaway Messages}
In this paper, we present the following key findings:

\noindent \textbf{(1)} Generative data can improve downstream tasks on the most common categories. Obtaining generative data is not significantly more expensive than retrieval data, and it can lead to better performance in downstream tasks.

\noindent \textbf{(2)} The effectiveness of generative data is uncertain for fine-grained and rare categories, and careful selection of prompt strategies is required in these scenarios.

\noindent \textbf{(3)} We found that the effectiveness of generative data is closely related to the mean text similarity (MTS) of downstream tasks. In scenarios with low MTS, using Target-Initialized Generation (TIG) can generate images that are more suitable for downstream tasks.

\noindent \textbf{(4)} Using a few target images as the starting point for generation in TIG can significantly improve the quality of generated images and even outperform the use of original images in some cases.

\vspace{-4mm}
\paragraph{Future Directions and Limitations}
We view generative models as a cost-effective and controllable approach for obtaining high-quality external data. With retrieval augmentation and other methods, we can enhance the generative model's ability to enhance its performance on fine-grained and rare concepts, which promotes trustworthiness and fairness. Our method potentially improves learning performance in the long run, e.g., as the initial query in active learning~\cite{chen2022making}.

In our study, we explored the use of up to 500-shot per category (totaling over 1 million images on GenBench) and observed an upward trend in performance in~\cref{fig:scaling_effect}. It would be worthwhile to investigate the scaling law in this direction with more computing resources and identify key factors in generative data and explore their characteristics, as well as the best scenarios for training downstream models. We also believe that increasing the prompt diversity of the images will enhance the usability of the generative data for training models. However, ensuring that the generative data still contain the main semantic information while increasing this diversity is a challenging and worthwhile research problem.