\vspace{-2mm}
\section{Related Works}
\subsection{Text-to-Image Generative Models}
 \label{sec: gen_models}
Text-to-image generative models, based on generative adversarial networks \cite{goodfellow2020generative, reed2016generative} or diffusion models \cite{sohl2015deep, ho2020denoising, nichol2021improved}, sample images conditioned on text prompts, \ie to match the given natural language description. 
Several emergent text-to-image models such as Stable Diffusion \cite{rombach2022high}, GLIDE \cite{nichol2021glide}, DALL-E2 \cite{ramesh2022hierarchical}, and Imagen \cite{saharia2022photorealistic} generate high-fiedility synthesized images. Albeit such images achieve perceptual results for human visual communications, their potential utilization for machine visual recognition tasks is yet under-explored. In this paper, we adopt two large-scale pretrained text-to-image generative models, Stable Diffusion and GLIDE, to benchmark and analyze the effect of synthesized images for visual recognition tasks.

% \subsection{Few-shot Finetuned Generative Models}
% Textual inversion \cite{gal2022image} freezes a text-to-image model but learns to encode visual concepts by pseudo-words in text embedding space. Albeit low computes, its expressiveness is bounded by the textual modality. Contrastive Prompt-Tuning \cite{dong2022dreamartist} trains both positive and negative embeddings as pseudo-words jointly.
% Dreambooth \cite{ruiz2022dreambooth} Ô¨Åne-tunes the model in order to represent the subject within the output domain of the model, which unlocks novel image generation capabilities to preserve object identities. 
% While previous works focus on the perceptual effect of synthesized images, we propose Targe-adapted Generation to synthesize images tailored to the target task from several original images.



\subsection{External Knowledge for Visual Recognition}
 \label{sec: ext_data}

In NLP, several works augment large language models with external knowledge~\cite{peters2019knowledge,guu2020realm,lewis2020retrieval,liu2020k,yu2021dict,borgeaud2021improving,khandelwal2019generalization}.
Motivated by retrieval-augmented models in NLP, several recent works leverage visual and textual knowledge to improve image classification~\cite{long2022retrieval,liu2023react} and multi-modal tasks like question answering~\cite{wu2021multi,marino2021krisp,yang2021empirical,chen2022murag,yasunaga2022retrieval}.
 
A few early attempts at exploring generative data for visual recognition \cite{dosovitskiy2015flownet, peng2017visda, richter2016playing}, usually employ a simulation pipeline with a specific data source, 
 \eg synthetic 2D renderings of 3D models or scenes from graphics engines. Inheriting the limitations of graphics engines, this paradigm usually suffers from the considerable gap between the generated images and real-world observations, as well as the bounded diversity of the specific data sources \cite{he2022synthetic}. Generative models close these gaps with potentially unlimited synthetic data size with a scalable storage cost. \cite{besnier2020dataset} trains image classifiers solely based on the synthesized images generated by a class-conditional BigGAN \cite{brock2018large} model.
\cite{jahanian2021generative} produces multi-view image pairs based on GAN priors to augment contrastive learning.
ImageNet-G-v1 \cite{bansal2023leaving}, a generative dataset based on Stable Diffusion, augments real data to achieve higher accuracy and effective robustness against natural distribution shifts. The most relevant work to ours is \cite{he2022synthetic}, which shows synthetic images are effective in zero-shot/few-shot settings and more performant than ImageNet-1k as pretraining data for downstream tasks. Extending the evaluation of generated images to more domains, we are the first to systematically benchmark their effectiveness along with retrieved and original images. We reveal the failure cases of generated images, and further, propose TIG to improve performance.