

% Figure environment removed




\section{Dataset}
\label{sec: Dataset}
In this section, we will describe our data processing pipeline. We focus on utilizing single-view portrait images from the internet due to their easy accessibility and abundance. However, utilizing face recognition and face reconstruction methods to gather annotated data from all camera angles is challenging, since the necessary facial features required for accurate recognition may be obscured. Therefore, we propose to use body features (e.g., shoulders) that are more distinctive to collect data.
%
In particular, we introduce a novel data processing method based on an off-the-shelf body reconstruction method to extract camera parameters and body poses from in-the-wild images, enabling us to obtain aligned portraits.

%
We begin by making the initial assumption that we have a human body SMPL \cite{DBLP:journals/tog/LoperM0PB15} template mesh in the local space, denoted as $M$, with the standard body shape. 
Its neck joint is aligned to the origin point $[0,0,0]$, and there is no additional global rotation or translation performed on $M$. We denote the template mesh with body pose parameters $\vec{\theta} \in \mathbb{R}^{69}$ as $M(\vec{\theta})$.
%
As we aim to solely preserve the head, neck, and shoulders region of the input portrait, we only consider the neck pose $p_n \in \mathbb{R}^{3}$ and head pose $p_h \in \mathbb{R}^{3}$ in $\vec{\theta}$, while disregarding the remaining body pose. Thus, we define neck pose and head pose as $p = [p_n,p_h] \in \mathbb{R}^{6}$ and denote the template mesh with neck and head pose $p$ as $M(p)$.
%
Regarding camera settings, we assume that our camera is always positioned on a sphere with radius $r = 2.7$, directed towards a fixed point. Additionally, intrinsic camera parameters are fixed as constant values.


Given an in-the-wild portrait image, our aim is to find the camera parameters, neck pose, and head pose of the portrait, allowing the rendering result of the template mesh $M$ to be aligned with the head, neck, and shoulders region of the input portrait.
%
Using an off-the-shelf body reconstruction method, 3DCrowdNet \cite{DBLP:conf/cvpr/ChoiMPL22}, we extract the input portrait's SMPL parameters (global rotation $rot$ and translation $trans$, shape parameters $\vec{\beta} \in \mathbb{R}^{10}$, and pose parameters $\vec{\theta} \in \mathbb{R}^{69}$), resulting in an estimated mesh $\tilde{M}(trans, rot, \vec{\beta},\vec{\theta})$ in the world space (with a fixed camera).
% 
We apply the neck pose and head pose of the estimated mesh to our template mesh, resulting in $M(p)$. Then we compute the transformation matrix that could transform $\tilde{M}(trans, rot, \vec{\beta},\vec{\theta})$ to align its head, neck, and shoulders joints with those of $M(p)$. 
%
Next, we apply the same transformation matrix to the fixed camera and normalize its camera parameters according to our camera assumption, obtaining the final camera parameters. 
%
The final camera parameters, denoted as $c \in \mathbb{R}^{25}$, comprises an extrinsic camera matrix $e \in \mathbb{R}^{16}$ and an intrinsic camera matrix $k \in \mathbb{R}^{9}$. Note that $k$ remains fixed as a constant matrix.
%
Then the raw image is cropped and aligned based on the obtained parameters, resulting in an aligned image denoted as $I$ (see Fig. \ref{fig: dataset_sample}). 
%


% Figure environment removed


To filter out the images with inaccurately estimated camera parameters, we render the mesh $M(p)$ on $I$ using the camera parameters $c$. We then manually examine the rendering results and remove any images where the mesh rendering is not well-aligned with $I$, as well as blurry or noisy images.
%
However, due to the limitations of the body reconstruction method, we encounter cases where the neck and head rendering results are not aligned with $I$, even though shoulder reconstruction is more accurate. During manual selection, we also avoid using the neck and head alignment as a criterion for manual selection and only consider the shoulder alignment.
%
As a result, the estimated camera parameters can render the template mesh in the local space to have aligned shoulders with the portraits. However, the estimated neck and head pose $p$ is ``coarse'' and inaccurate.
%
Therefore, instead of being used directly as the training label, this coarse body pose is only employed to calculate a regularization loss during the early stages of the training process, which we will explain in later sections.




In sum, we collect 41,767 raw portrait images from Pexels\footnote{\hyperref[]{https://www.pexels.com}} and Unsplash\footnote{\hyperref[]{https://unsplash.com}}, finally getting \textbf{54,000} aligned images as our \textit{$\it{360}^{\circ}$PHQ} dataset.  Samples of these images can be found in Fig. \ref{fig: dataset_sample} as well as the supplementary file.
The datasets are augmented by a horizontal flip.
%
We convert the camera positions in our dataset to the spherical coordinate system ($\mu$ and $\nu$, or yaw and pitch), and visualize the distribution of camera positions in Fig. \ref{fig: data_distribution}. 
%Our dataset exhibits a wide range of camera distributions, and the images in our dataset are high-quality, and feature variations in gender, age, race, expression, and lighting.
Our dataset contains a diverse set of camera distributions, and the images are of high quality, with variations in gender, age, race, expression, and lighting.
%
The analysis of the distribution of semantic attributes (gender, race, age, etc) can be found in the supplementary file. 







% Given an in-the-wild portrait image, our goal is to find the camera pose and body pose of the portrait, these parameters can then be used to render the $M(\vec{\theta})$ that aligned with the input portrait.
% We first employ an off-the-shelf body reconstruction method \cite{DBLP:conf/cvpr/ChoiMPL22} to extract the input portrait's SMPL parameters (global rotation $rot$ and translation $trans$, shape parameters $\vec{\beta} \in \mathbb{R}^{10}$ and pose parameters $\vec{\theta} \in \mathbb{R}^{69}$), getting an estimated mesh $\tilde{M}(trans, rot, \vec{\beta},\vec{\theta})$ in the world space.
% %
% Then we apply the pose parameters $\vec{\theta}$ of the estimated mesh to our template mesh as $M(\vec{\theta})$.
% % 
% The body reconstruction method \cite{DBLP:conf/cvpr/ChoiMPL22} outputs a mesh $\tilde{M}(trans, rot, \vec{\beta},\vec{\theta})$ that is globally rotated and translated (in the world space), along with a fixed camera to render it. Our aim is to determine the camera parameter to render the mesh $M(\vec{\theta})$ (in the local space) onto the portrait image, resulting in similar rendering outcomes as the estimated mesh.


% To accomplish this, we first translate $\tilde{M}(trans, rot, \vec{\beta},\vec{\theta})$ to align its neck joint to the origin point $[0,0,0]$, obtaining the translation matrix $T$. We then compute the rotation matrix that rotates $\tilde{M}(trans, rot, \vec{\beta},\vec{\theta})$ to align it with $M(\vec{\theta})$, obtaining the rotation matrix $R$.
% %
% Next, we apply the transformation matrix $RT$ to the fixed camera and normalize its camera pose according to our camera assumption, obtaining the final camera parameter. 
% %
% The final camera parameter, denoted as $c \in \mathbb{R}^{25}$, comprises an extrinsic camera matrix $e \in \mathbb{R}^{16}$ and an intrinsic camera matrix $k \in \mathbb{R}^{9}$. Note that $k$ remains fixed as a constant matrix.





% As a result, for each in-the-wild portrait image, we obtain its pose parameters $\vec{\theta}$, and camera pose $c$.
% %In this paper, we omit the shape parameters $\vec{\beta}$. 
% As we solely aim to preserve the head, neck, and shoulders region of the input portrait, we only consider the neck pose $p_n \in \mathbb{R}^{3}$ and head pose $p_h \in \mathbb{R}^{3}$ in $\vec{\theta}$, while disregarding the remaining body pose.
% %
% As a result, we define neck pose and head pose $p = [p_n,p_h] \in \mathbb{R}^{6}$ as the body pose, and denote the template mesh with body pose $p$ as $M(p)$ throughout the rest of the paper.
% Then all the raw images are cropped and aligned based on the obtained parameters, resulting in an aligned image denoted as $I$ (see Fig. \ref{fig: dataset_sample}). 




