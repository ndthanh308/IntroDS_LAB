






\section{Methodology}
\label{sec: Methodology}
We first present a comprehensive overview of our proposed approach. Firstly, we introduce the design of our body pose-aware discriminator in Sec. \ref{sec: Discriminator}, which is capable of extracting body poses and corresponding scores from the input images.
%
Subsequently, we elaborate on our 3DPortraitGAN in  Sec. \ref{sec: 3DPortraitGAN}, including the pose predictor in generator (Sec. \ref{sec: pose sampling}), the generator backbone (Sec. \ref{sec: EG3D Backbone}) and our deformation module (Sec. \ref{sec: Deformation Module}).
%
Finally, we discuss the losses utilized in our training process in  Sec. \ref{sec: Losses}, along with the details of the training in  Sec. \ref{sec: Training Details}.


% Figure environment removed



\subsection{Body Pose-aware Discriminator}
\label{sec: Discriminator}
In Sec. \ref{sec: Dataset}, we mentioned that the body poses in our dataset are inaccurate and cannot be directly used for training.
Drawing inspiration from the pose-free 3D-aware generator, Pof3D \cite{shi2023pof3d}, we propose employing the discriminator to predict more accurate body pose $\hat{p}$ from real/generated images.

Taking a real image $I_{real}$ as an example, we denote its camera parameters in the \textit{$\it{360}^{\circ}$PHQ} dataset as $c_{real}$.
As shown in Fig. \ref{fig: discriminator}, the convolutional layers first extract features from image $I_{real}$. Then the features and the camera parameters are fed into a pose predictor branch $\Gamma_D$, yielding a predicted body pose:
\begin{equation}
	\begin{split}
    \label{eqn: pose_predict_D}
     \hat{p}_{real} &= \Gamma_D (\mathrm{Conv}(I_{real}), c_{real}),
     \end{split}
\end{equation}
where $\mathrm{Conv}$ denotes the convolutional layers.
We observe that $\Gamma_D$ faces difficulty in accurately predicting symmetry poses for symmetry images. To explicitly maintain the symmetry of $\Gamma_D$, we propose an explicit symmetry strategy for $\Gamma_D$.
%
Specifically, once fed into $\Gamma_D$, we flip the input image horizontally when the spherical coordinate $\mu$ of its camera position falls within the range of $[-\frac{1}{2}\pi,\frac{1}{2}\pi]$ (which indicates that the camera is on the left-hand side of the subject, see Fig. \ref{fig: data_distribution}). We also flip the resulting predicted body pose so as to obtain the final predicted body pose. This operation guarantees that two symmetrical images have the symmetry value of the predicted body pose.




Next, we feed $c_{real}$ and $\hat{p}_{real}$ into a pose feature mapping network, and the image features are fed into an image feature mapping network. Then the outputs of the two feature mapping networks are multiplied to get the final score of the discriminator:
\begin{equation}
	\begin{split}
    \label{eqn: discriminator_real}
     score_{real} & = D(I_{real}\vert c_{real}, \hat{p}_{real} ), 
     \end{split}
\end{equation}
where $D$ denotes the discriminator. 

Likewise, $D$ extract scores and body pose from generated images as:
\begin{equation}
	\begin{split}
    \label{eqn: discriminator_gen}
    \hat{p}_{gen} &= \Gamma_D (\mathrm{Conv}(I_{gen}), c_{gen}), \\
     score_{gen} & = D(I_{gen}\vert c_{gen}, \hat{p}_{gen} ),
     \end{split}
\end{equation}
where $I_{gen}$ refers to the image generated by our generator (which we will describe in the following section) using camera parameters $c_{gen}$ and a specific body pose. $c_{gen}$ is sampled from the \textit{$\it{360}^{\circ}$PHQ} dataset, while $\hat{p}_{gen}$ denotes the predicted body pose of $I_{gen}$.



% Figure environment removed



\subsection{3DPortraitGAN}
\label{sec: 3DPortraitGAN}



\subsubsection{Pose Sampling}
\label{sec: pose sampling}
In this paper, we propose to generate image $I_{gen}$ using latent code $z$, camera parameters $c_{gen}$ and a certain body pose $p_{gen}$ as :
\begin{equation}
	\begin{split}
    \label{eqn: generate}
     I_{gen} = G(z, c_{gen}, p_{gen}) \sim P(I_{gen} \vert z, c_{gen}, p_{gen}).
     \end{split}
\end{equation}
In order to train our generator, we need to sample camera parameters and body poses from the pose distribution of \textit{$\it{360}^{\circ}$PHQ} dataset. 


However, in \textit{$\it{360}^{\circ}$PHQ} dataset, while the camera parameters we extract from real images are somewhat precise (after our manual selection), the body poses are ``coarse'' and cannot be used as training data (Sec. \ref{sec: Dataset}).
%
Taking inspiration from Pof3D \cite{shi2023pof3d}, we employ a pose prediction network $\Gamma_G$ to estimate the conditional distribution of body pose based on randomly sampled latent codes and camera parameters drawn from the \textit{$\it{360}^{\circ}$PHQ} dataset. More specifically, we employ a predictor $\Gamma_G$  to predict body pose from both latent code and camera parameters:
\begin{equation}
	\begin{split}
    \label{eqn: pose_predict_G}
     {p}_{gen} &= \Gamma_G (z, c_{gen})  \sim P({p}_{gen} \vert z, c_{gen}).
     \end{split}
\end{equation}
Similarly to $\Gamma_D$, we want the predicted body poses that correspond to symmetry camera poses to have symmetry distribution. Thus we apply the explicit symmetry strategy to $\Gamma_G$.
We horizontally flip the camera parameters $c_{gen}$ with $\mu \in [-\frac{1}{2}\pi,\frac{1}{2}\pi]$, and then the predicted ${p}_{gen}$ is flipped to obtain the final predicted body pose.  



Then we can generate images from $z$ and $c_{gen}$ by:
\begin{equation}
	\begin{split}
    \label{eqn: generate_new}
     I_{gen} = G(z, c_{gen}, \Gamma_G (z, c_{gen})) \sim P(I_{gen} \vert z, c_{gen}, \Gamma_G (z, c_{gen})).
     \end{split}
\end{equation}
In the next section, we will introduce how our generator renders image ${I}_{gen}$ with certain camera parameters ${c}_{gen}$ and body pose ${p}_{gen}$ in detail.

% % Figure environment removed






\subsubsection{EG3D Backbone}
\label{sec: EG3D Backbone}
As shown in Fig. \ref{fig: pipeline},  we utilize EG3D \cite{DBLP:conf/cvpr/ChanLCNPMGGTKKW22} as our backbone. 
After predicting body pose ${p}_{gen}$ from latent code $z$ and camera parameters $c_{gen}$, 
% we input $z$ into the mapping network, while using $c_{gen}$ and  ${p}_{gen}$ as the conditional input of the mappin network. 
%
we input $z$ into the mapping network, where ${p}_{gen}$ and $c_{gen}$ serve as its conditional inputs.
The resulting $w$ latent code is then used to modulate the generator to synthesize feature maps, which are then reshaped to tri-planes.
In this paper, we assume that our tri-planes are ``normalized'' and canonical. 
This indicates that if we directly render the results using the tri-planes, we will obtain a canonical human body geometry representation with a neutral body pose.


During volumetric rendering, each point $\bold{x} = (x,y,z)$ on the ray is projected onto the tri-planes as $(x,y),(y,z),(z,y)$. These projections are used to sample features from the tri-planes, 
which are then fed into a decoder to obtain the color and density of point $\bold{x}$ and to perform volumetric rendering. 
After that, the rendered image is fed to a super-resolution network to obtain the final high-resolution image.



\subsubsection{Deformation Module}
\label{sec: Deformation Module}
In Sec. \ref{sec: EG3D Backbone}, we assume that the tri-planes generated by our generator are canonical. 
Given a body pose $p_{gen}$, to achieve a final portrait that conforms to $p_{gen}$, we utilize the deformed mesh $M(p_{gen})$ to produce a deformation field that maps each sampled point $\bold{x} = (x, y, z)$ in the observation space to a corresponding point $\bold{x}' = (x', y', z') = (x + \Delta x, y + \Delta y, z + \Delta z) $ in the canonical space, where $ \Delta \bold{x} =  (\Delta x, \Delta y,  \Delta z) $ denotes the deformation field value.


In RigNeRF \cite{DBLP:conf/cvpr/AtharXSSS22}, the deformation field is computed using 3DMM, and a neural network is utilized to predict the residual non-rigid deformation value. 
To enhance the efficiency of our training process, we exclude the NN-based non-rigid deformation training employed in RigNeRF and directly use a canonical SMPL mesh $M(0)$ ($0$ refer to neutral body pose) and the deformed SMPL mesh $M(p_{gen})$ to compute a deformation field. Similar to RigNeRF, the SMPL deformation field value at a point $\bold{x}$ on the ray is defined as follows:
\begin{equation} 
	\begin{split}
    \label{eqn: original SMPLDef}
    & \Delta \bold{x}  = SMPLDef(\bold{x}, p_{gen}) = \frac{SMPLDef(\hat{\bold{x}},p_{gen})}{\exp(\Vert\bold{x} , \hat{\bold{x}}\Vert^2)}, \\
    & SMPLDef(\hat{\bold{x}},p_{gen}) = \hat{\bold{x}}_{M(0)} - \hat{\bold{x}}, \enspace
     \end{split}
\end{equation}
where $\hat{\bold{x}}$ is the closest point on the deformed mesh $M(p_{gen})$ to $\bold{x}$, and  $\Vert\bold{x} , \hat{\bold{x}}\Vert^2$ is the Euclidean Distance between $\bold{x}$ and $\hat{\bold{x}}$. $\hat{\bold{x}}_{M(0)}$ is the position of point $\hat{\bold{x}}$ on $M(0)$.
%As a result, the deformation direction of point $\bold{x}$ is the translation direction of its closest point on the deformed SMPL mesh. Moreover, the magnitude of the deformation is determined by the distance between $\bold{x}$ and $\hat{\bold{x}}$.

However, the deformation field in RigNeRF may encounter issues when the NN-based non-rigid deformation is disabled and the body pose $p_{gen}$ is large. 
% 
The computation of RigNeRF's deformation field $\Delta \bold{x}$ depends only on the translation of the point, ignoring the relative positioning between the sample point and the mesh. This approach produces an ``offset face'' as depicted in Fig. \ref{fig: ablation_deform}.


To tackle this issue, as shown in Fig. \ref{fig: pipeline}, we utilize a deformation field that accounts for the positional relationship between $\bold{x}$ and its nearest face on the mesh:
\begin{equation}
\begin{split}
\label{eqn: new SMPLDef}
& \Delta \bold{x}  = SMPLDef(\bold{x},p_{gen}) = \left\{
\begin{aligned}
&\check{\bold{x}} - \bold{x}, \enspace \quad  \Vert\bold{x}, \hat{f}\Vert^2<\alpha\\
&0, \enspace \quad \Vert\bold{x}, \hat{f}\Vert^2 \geq \alpha \\
\end{aligned}
\right. \\
&\bold{x}  = C_{\hat{f}}(u,v,h) ,\quad  \check{\bold{x}}  = C_{\hat{f}}^{M(0)}(u,v,h), \enspace 
\end{split}
\end{equation}
where $\hat{f}$ is the face on $M(p_{gen})$ that is closest to $\bold{x}$, and  $ \Vert\bold{x}, \hat{f}\Vert^2$ denotes the Euclidean distance between $\bold{x}$ and  $\hat{f}$, $\alpha$ is a hyper-parameter that controls the ``thickness'' of the geometry beyond the mesh (we empirically set $\alpha$ as 0.25).

We first obtain the local coordinate system $C_{\hat{f}}$ of $\hat{f}$ using its vertices, which yields the local coordinates $(u,v,h)$ of $\bold{x}$ in $C_{\hat{f}}$.  Next, we obtain the local coordinate system $C_{\hat{f}}^{M(0)}$ of the same face on the template mesh $M(0)$ and use $(u,v,h)$ to compute the new global coordinates $\check{\bold{x}}$. 
In other words, if $\bold{x}$ is close to the mesh, its position relative to its closest face on the mesh remains unchanged.




\subsection{Losses}
\label{sec: Losses}

\subsubsection{Discriminator Loss}
We define the loss of the discriminator as:
\begin{equation}
\begin{split}
    \label{eqn: D discriminator loss}
    L_D = & -\mathbb{E} [\log(1-D(I_{gen}\vert c_{gen}, \hat{p}_{gen} ))] \\
          & - \mathbb{E} [\log (D(I_{real}\vert c_{real}, \hat{p}_{real} ) ] \\
          &  +  \lambda \mathbb{E} [ ||\nabla_{I_{real}} D(I_{real}\vert c_{real}, \hat{p}_{real} ) ||_2] + \lambda_p L_{p},
    \end{split}
\end{equation}
where $\lambda \mathbb{E} [ ||\nabla_{I_r} D(I_{real}\vert c_{real}, \hat{p}_{real} ) ||_2]$ is the gradient penalty, $\lambda_{p}$ represents the weight of $L_{p}$, $L_{p}$ is the body pose loss, which is used to optimize the pose predictor $\Gamma_D$ in the discriminator as:
\begin{equation}
	\begin{split}
    \label{eqn: body pose loss}
    L_{p} & = L_2({p}_{gen}, \hat{p}_{gen}),
     \end{split}
\end{equation}
where ${p}_{gen}$ could be regarded as the ground truth body pose of $I_{gen}$ (since ${p}_{gen}$ is used to perform deformation), and $\hat{p}_{gen}$ is the body pose that is predicted by the discriminator from $I_{gen}$, $L_2$ denotes the $L_2$ distance.


\begin{table*}[t]
\begin{tabular}{@{}ccccccc@{}}
\toprule
                         & Image            & $L_{preg}$                    & $L_{rear}$            & Neural Rendering Resolution     & $\Gamma_G$              \\ \midrule
\multirow{3}{*}{Stage 1} & 0$\sim$0.2M          & \CheckmarkBold & \CheckmarkBold  (w/o normalization)  & $64^2$                          & training          \\
                         & 0.2M$\sim$0.8M       & \XSolidBrush   & \CheckmarkBold  (w/ normalization)   & $64^2$                          & training        \\
                         & 0.8M$\sim$4M     & \XSolidBrush   & \CheckmarkBold  (w/ normalization)   & $64^2$                          & training         \\\midrule
Stage 2                  & 4M$\sim$10M  & \XSolidBrush   & \CheckmarkBold  (w/ normalization)   & $64^2$                          & freeze           \\ \midrule
\multirow{2}{*}{Stage 3} & 10M$\sim$11M & \XSolidBrush   & \CheckmarkBold  (w/ normalization)   & increase from $64^2$ to $128^2$ & freeze       \\ 
                         & 11M$\sim$14M & \XSolidBrush   & \CheckmarkBold  (w/ normalization)   & $128^2$                         & freeze       \\\bottomrule
\end{tabular}
\caption{The training details of our three-stage training. M means million images.}
  \label{tab: training_details}
\end{table*}



\subsubsection{Generator Loss}
We define the generator's loss as follows:
\begin{equation}
\begin{split}
    \label{eqn: G discriminator loss}
    L_G = & - \mathbb{E} [\log (D(I_{gen}\vert c_{gen}, \hat{p}_{gen} )) ] \\
    & + \lambda_{preg} L_{preg} + \lambda_{rear} L_{rear},
    \end{split}
\end{equation}
where $L_{preg}$ represents the body pose regularization loss, and $L_{rear}$ represents the rear-view depth regularization loss, $\lambda_{preg}$ and $\lambda_{rear}$ represents the weights of the regularization losses.
The body pose regularization loss $L_{preg}$ will be only employed in the very early stage of our training process, we provide more details in Sec. \ref{sec: Training Details}.


 \subsubsection{Body Pose Regularization Loss}
Although our network can learn the relative body pose between different images, it struggles to predict the absolute body pose due to the lack of prior information. The predicted pose can be viewed as a ``deviated'' pose, which has an offset from the true value. This offset does not affect camera parameters prediction (as in Pof3D \cite{shi2023pof3d}) since the camera system can be globally rotated. However, we use an SMPL model in the canonical space, which means a ``deviated'' body pose will result in an unnatural mesh deformation. To address this issue, we propose using $L_{preg}$ to constrain the value of the predicted pose as follows:
\begin{equation}
	\begin{split}
    \label{eqn: body pose reg loss}
    {p}_{gen} &= \Gamma_G (z, c_{gen}), \\
     L_{preg} & =  L_2({p}_{gen}, p_{coarse}), 
     \end{split}
\end{equation}
where $p_{coarse}$ represents the coarse body pose in the \textit{$\it{360}^{\circ}$PHQ} dataset, 
and $c_{gen}$ and $p_{coarse}$ are from the same real image.





 \subsubsection{Rear-view Depth Regularization Loss}
We observe the presence of a face on the back of the head (refer to Fig. \ref{fig: rear-view-reg} in ablation studies). Despite the generator's ability to learn the texture of hair at the back of the head, the face geometry is apparent in the shape extracted by the marching cubes algorithm. Such an artifact also exists in the comparison results in Rodin \cite{DBLP:journals/corr/abs-2212-06135}, where they adapted the official implementation of EG3D to 360-degree generation and retrain EG3D using their rendered dataset.
%
We attribute this phenomenon to the geometric ambiguity when using single-view images as training data. The discriminator can only assess the rendered images, leading to an underdetermined problem in characterizing the geometry of the back of the head. In the absence of constraints, our model may converge to a suboptimal solution marked by front-back symmetric geometry.








To tackle this issue, we introduce a rear-view depth regularization loss, which is incorporated into the generator's training to prevent it from falling into suboptimal solutions. Specifically, we generate the depth image of $M(0)$'s occiput (the SMPL mesh with neutral body pose) from the camera parameters $c_{back}$, where $\mu = -\frac{1}{2} \pi$ and $\nu = \frac{1}{2} \pi$. We then apply depth image constraints to the image generated by the generator $G$ from $c_{back}$:
\begin{equation}
	\begin{split}
    \label{eqn: rear-view depth regularization loss}
     L_{rear} & =  L_2(m \odot depth_{M(0)}, m \odot depth_{G}). \\ 
     \end{split}
\end{equation}
Here, $depth_{M(0)}$ represents $M(0)$'s depth image, $depth_{G}$ represents the depth image produced by the generator, $m$ represents the valid mask of $M(0)$'s depth image, and $\odot$ denotes element-wise multiplication. 




\subsubsection{Overrall Loss}
Our full objective function is:
\begin{equation}
\begin{split}
    \label{eqn: full objective}
    L =  L_D + L_G 
    \end{split}.
\end{equation}







 \subsection{Training Details}
\label{sec: Training Details}
Our model is trained on 8 NVIDIA A40 GPUs. It takes 7 days to train our full model.
Taking into account the computational cost of the deformation field, we retain only the SMPL faces that fall within the bounding box of the volumetric rendering. 
We set gamma as 1.0, and the resolution of the training dataset is $256^2$.
The body pose predictors $\Gamma_G$ and $\Gamma_D$ are composed of fully-connected layers with leaky ReLU as the activation functions. 

The proposed training strategy for our 3DPortraitGAN is composed of three stages, as shown in Tab. \ref{tab: training_details}. 
%to first warm up our networks, then finetune our model using the rebalanced dataset, and finally increase the neural rendering resolution.


\paragraph{Stage1 - Warm Up} 
    The first stage is the warm-up period, from 0 to 4M images.
    %
    We employ the regularization loss $L_{preg}$ during the initial phase of this stage. 
    Specifically, we employ the $L_{preg}$ regularization loss to the first 0.2M images while linearly decaying $\lambda_{preg}$ to 0 over the subsequent 0.2M images. This helps prevent the coarse body pose from negatively affecting the entire training process. 
    %
    Meanwhile,  we compute $L_{rear}$ regularization loss by directly utilizing $depth_{M(0)}$ and $depth_{G}$ according to Eq. (\ref{eqn: rear-view depth regularization loss}) from 0 to 0.8M images. After generator $G$ has acquired a rudimentary understanding of portrait geometry, we normalize $depth_{M(0)}$ with $depth_{G}$ to match their mean depth values in order to enhance head depth diversity.
    %
    We utilize the swapping regularization method proposed by EG3D \cite{DBLP:conf/cvpr/ChanLCNPMGGTKKW22}. Specifically, we begin by randomly swapping the conditioning pose of $G$'s mapping network with another random pose with 100\% probability, then the swapping probability is linearly decayed to 60\% over the first 1M images. For the remainder of this stage, we maintain a 60\% swapping probability. The neural rendering resolution remains fixed at $64^2$.
     
    
    
\paragraph{Stage2 - Low Neural Rendering Resolution} 
    From 4M to 10M images, we encounter issues with the collapse of $\Gamma_G$ during training. 
    % %
    Therefore, we freeze $\Gamma_G$ while continuing training 3DPortraitGAN for 10M images.
    We randomly swap the conditioning pose of $G$'s mapping network with another random pose with 60\% probability during this stage, and neural rendering resolution remains fixed at $64^2$.
    
    
\paragraph{Stage3 - Increase Neural Rendering Resolution}  
    This stage represents the training from 10M images to 14M images of our full training process.
    %
    In this stage, we gradually increase the neural rendering resolution of 3DPortraitGAN while other training settings are identical to those of Stage 2.
    %
    Specifically, we linearly increase the neural rendering resolution from $64^2$ to $128^2$ from 10M to 11M images, then we retain the neural rendering resolution as $128^2$ until finishing training.
    We randomly swap the conditioning pose of $G$'s mapping network with another random pose with 60\% probability during this stage.
    



    
