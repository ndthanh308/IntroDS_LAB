

% Figure environment removed
\section{Introduction}
\label{sec: Introduction}

%In recent years, there has been a significant advancement in the development of 3D-aware generators.
There has been significant progress in the development of 3D-aware generators in recent years.
Unlike 2D GANs, which can only produce high-quality single-view images, 3D-aware generators  \cite{DBLP:conf/cvpr/ChanLCNPMGGTKKW22,DBLP:conf/iccv/HenzlerM019,DBLP:journals/corr/abs-2112-11427,DBLP:conf/iclr/GuL0T22} utilize voxel rendering or NeRF rendering to acquire knowledge of the 3D geometry from 2D image collections. 3D-aware generators have been instrumental in facilitating image and video editing tasks \cite{DBLP:journals/corr/abs-2205-15517, nerffaceediting,DBLP:conf/siggrapha/JinRKBC22,DBLP:journals/corr/abs-2301-02700,DBLP:journals/corr/abs-2203-13441,DBLP:journals/corr/abs-2302-04871,10.1145/3597300} as they are able to produce multi-view consistent results with realistic geometry.


Most 3D-aware face generators only require single-view face images as training data. These single-view face datasets, such as \textit{FFHQ} \cite{DBLP:conf/cvpr/KarrasLAHLA20} and \textit{CelebA} \cite{liu2015faceattributes}, are usually derived from in-the-wild images on the internet. This is because 3D-aware generators need extensive and diverse datasets, and internet images are readily accessible and abundant.
%
Despite their usefulness, single-view face datasets have certain limitations. 
%
First, the datasets primarily consist of frontal or near-frontal views, with limited views from larger poses and no views from behind the head. 
As a result of the use of  \textit{FFHQ} and \textit{CelebA}, 3D-aware face generators only produce facial and frontal-head area data, and the the back of the head is missing. \cite{DBLP:conf/cvpr/ChanLCNPMGGTKKW22,DBLP:conf/iccv/HenzlerM019,DBLP:journals/corr/abs-2112-11427,DBLP:conf/iclr/GuL0T22,DBLP:conf/nips/SchwarzSNL022}.
%
Second, even PanoHead \cite{An_2023_CVPR} is able to achieve full-head generation by collecting back-head and large-pose images (\textit{FFHQ-F}) and utilizing self-adaptive image alignment, \textit{FFHQ-F} only includes facial data and does not contain complete data for the neck and shoulder regions. 
Consequently, the geometry in PanoHead's results is still limited to the facial region, and the neck and shoulders are incomplete.
%
The reasons behind these dataset limitations are twofold. 
Firstly, the availability of such single-view datasets mainly depends on the accuracy of face-recognition technology used to extract faces from in-the-wild images and the accuracy of face reconstruction methods utilized to extract camera parameters. In some cases where cameras are positioned at a large camera pose or even behind the head, the necessary facial features required for accurate recognition may be obscured, making it difficult to extract data from all angles.
%
Secondly, for in-the-wild real portraits that include the neck and shoulder regions, diverse body poses are always present. Unfortunately, current 3D-aware generators require portrait images in a canonical space. Otherwise, the dimensionality of the data distribution becomes prohibitively high, resulting in significant distortion in the results.
%
Therefore, the lack of sufficient data and appropriate methods creates a significant challenge for developing a full-head 3D-aware portrait generator using a single-view dataset.



In addition to single-view face data, there is a line of research focused on multi-view face data. 
To reconstruct 3D portrait geometry, researchers have developed 3D portrait datasets \cite{DBLP:conf/cvpr/Yang0WHSYC20,wuu2022multiface,DBLP:journals/corr/abs-1904-00168} that consist of high-quality multi-view labeled portrait images. Nevertheless, the diversity of these datasets is constrained by the challenges involved in data collecting and processing.  
%
Synthetic face datasets \cite{DBLP:journals/corr/abs-2212-06135,DHFdataset,DBLP:conf/iccv/WoodBHD0S21} offer a convenient solution to generate portrait data with diverse environments and camera parameters. 
Given its capacity to regulate data creation, as well as produce reliable ground truth labels, such as segmentation masks and landmarks, synthetic data has become a popular tool for training computer vision models.
Rodin \cite{DBLP:journals/corr/abs-2212-06135} utilizes rendered portrait images as its training dataset and achieves a full-head portrait generation model. However, its results are constrained by the unrealistic rendering style of the training data. Additionally, Rodin demands an ample, multi-view image dataset of avatars (consisting of at least 300 multi-view images for each 100K synthetic individuals) to fit tri-planes, which makes this method highly data-dependent.
%
In summary, there is no suitable multi-view face data available for training a realistic 3D-aware portrait generator.


In this paper, we propose a novel 3D-aware full-head portrait generator, \textbf{3DPortraitGAN}, that can learn a canonical 3D avatar distribution from a collection of single-view real portraits with body pose self-learning. The generator is capable of producing realistic, $360^{\circ}$ view-consistent portrait images.
%
Regarding the training data, we focus on utilizing single-view portrait images from the Internet. Considering the challenges associated with collecting data from all camera angles using face recognition methods, we propose to use more distinctive body features to collect data. 
%
Specifically, we introduce an innovative data processing method based on an off-the-shelf body reconstruction method to extract camera parameters and body poses from in-the-wild images and identify desired portrait regions. 
The resulting dataset, named  {$\bf{360^{\circ}}$}-\textbf{P}ortrait-\textbf{HQ} (\textbf{$\bf{360^{\circ}}$PHQ}), comprises \textbf{54,000} high-quality single-view portraits with a wide range of camera angles. 
%
While we aim to generate human geometry within a canonical space using our generator, the diverse body poses in the \textit{$\it{360}^{\circ}$PHQ} dataset pose a challenge for learning a canonical 3D avatar representation. 
%
To address this issue, we  employ a deformation module to deform the generated human geometry in the canonical space, ensuring that the volumetric rendering results display the desired body pose to fit within the real portrait distribution.
%
Since the estimated body poses in the dataset are imprecise, we incorporate two pose predictors into both the generator and discriminator to achieve body pose self-learning.
%
As depicted in Fig. \ref{fig: motivation}, the generator's pose predictor learns a distribution of body pose which the generator utilizes to generate portraits. The generated portrait is then processed by the body-pose-aware discriminator, where the pose predictor predicts its body pose. The difference between the estimated and input body pose of the generated portrait is utilized to train the pose predictor in the discriminator.
Additionally, the body-pose-aware discriminator is conditioned on the predicted body pose to score the generated (or real) portraits, which are further used to train the generator and the discriminator networks.


To the best of our knowledge, our delicately designed framework, coupled with the novel portrait dataset, enables our 3DPortraitGAN model to become the first 3D-aware full-head GAN capable of effectively learning $360^{\circ}$ canonical 3D portraits from single-view and body-pose-varied 2D data, while also achieving body pose self-learning.
%
Through experiments, we demonstrate that our framework can generate view-consistent, realistic portrait images with complete geometry from a wide range of camera angles and accurately predict portrait body pose.


In summary, our work makes the following major contributions:
\begin{itemize}

     \item The first 3D-aware full-head GAN framework that can learn $360^{\circ}$ canonical 3D portrait distribution from a single-view and body-pose-various dataset with body pose self-learning. 

     
    \item A high-quality, large-scale, and single-view real portrait dataset featuring diverse camera parameters and body poses.


\end{itemize}



