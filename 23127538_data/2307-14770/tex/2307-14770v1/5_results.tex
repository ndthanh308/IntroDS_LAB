


\begin{table}[]
\begin{tabular}{@{}cccccc@{}}
\toprule
             & VoxGRAF &StyleNeRF  & EG3D  & Ours                             \\ \midrule
FID          & 39.41   &22.16      & 14.52 & \textbf{12.98}$^\star$ /\textbf{13.30}$^\diamond$    \\  
% FID$^*$      & 52.21   &N/A        & 16.55 & \textbf{14.48}$^\star$ /\textbf{15.36}$^\diamond$    \\ 
\midrule
% Identity (full range)     & 0.44    &           & 0.48  & \textbf{0.58}  \\ 
Identity     & 0.52    &0.48       & 0.59  & \textbf{0.71}  \\   \bottomrule
\end{tabular}
\caption{FID ($\downarrow$) and face identity ($\uparrow$) for our model and SOTA methods.
$^\star$ means the body poses that are used to generate images are predicted by $\Gamma_D$ from real images in \textit{$\it{360}^{\circ}$PHQ},  $^\diamond$ means the body poses that are used to generate images are sampled from the pose distribution predicted by $\Gamma_G$. 
%FID$^*$ ($\downarrow$) means that the conditional label of the generator and rendering label in volume rendering are independent of one another.
}
  \label{tab: metrics}
  %\vspace{-10pt}
\end{table}



% Figure environment removed







\section{Results}
\label{sec: Results}

Fig. \ref{fig: generation-2} displays a selection of random samples generated by our model. Real images from the \textit{$\it{360}^{\circ}$PHQ} dataset are randomly sampled (1st col) and passed through $\Gamma_D$ to obtain the predicted body pose. Latent codes are then randomly sampled, and the camera parameters and predicted body pose of the real images are used to generate images (2nd-9th cols).
We also show results that rendered from steep camera parameters in Figs. \ref{fig:yaw}-\ref{fig:pitch}.

To illustrate 3DPortraitGAN's performance in generating novel views for real images, we perform latent code optimization in $W$ latent space to real images using our 3DPortraitGAN model. The input real images are never seen during training. As shown in Fig.~\ref{fig: inversion}, 3DPortraitGAN produces reasonable reconstructed portrait geometry and appearance.




\section{Comparison}
\label{sec: Comparison}
%To be consistent with the scope of our method, we compare only to methods that
To be consistent with the scope of our method, we choose VoxGRAF \cite{DBLP:conf/nips/SchwarzSNL022}, StyleNeRF \cite{DBLP:conf/iclr/GuL0T22} and EG3D \cite{DBLP:conf/cvpr/ChanLCNPMGGTKKW22} as state-of-the-art representations of the previous 3D-aware generators to compare. We adopt the official implementation of those works to our dataset.
%
Rodin \cite{DBLP:journals/corr/abs-2212-06135} has a similar objective as ours, however, Rodin requires a multi-view image dataset of avatars (300 multi-view images for each 100K synthetic individuals independently) to fit tri-planes, which cannot be applied to our single-view dataset. Therefore, we do not conduct a comparison with Rodin.
%


\subsection{Qualitative Comparison}
In Fig. \ref{fig: comparison}, we present a qualitative comparison between the SOTA methods and 3DPortraitGAN. The results show that VoxGRAF and EG3D suffer from distortion and artifacts due to their attempt to directly learn 3D portraits from a dataset with diverse body poses. The results of StyleNeRF exhibit interpolated faces, which can be more vividly observed in our supplementary video.
%
In contrast, 3DPortraitGAN shows a clear advantage in terms of the quality of results. Our approach generates high-quality multi-view rendering results and reasonable 3D geometry.

\subsection{Quantitative Comparison}
Regarding the quantitative results, we compare our method to SOTA alternatives using Fr\'echet Inception Distance (FID) \cite{DBLP:conf/nips/HeuselRUNH17} and facial identity metrics (refer to Tab. \ref{tab: metrics}).

To assess the rendering quality of models, we use FID, which computes the distance between the distribution of the generated images and that of the real images to evaluate the quality and diversity of the generated images.
For each model, we generate 50K images using camera parameters sampled from the \textit{$\it{360}^{\circ}$PHQ} dataset. 
For our 3DPortraitGAN, we utilize pose predictor $\Gamma_D$ to predict body poses from real images in \textit{$\it{360}^{\circ}$PHQ} ($^\star$ in Tab. \ref{tab: metrics}), and also sample body poses from the pose distribution that is predicted by the pose predictor $\Gamma_G$ in the generator ($^\diamond$ in Tab. \ref{tab: metrics}). 
% As explained by \cite{DBLP:journals/corr/abs-2303-14407}, the FID value of pose-conditional generator architecture (VoxGRAF, EG3D, and 3DPortraitGAN) is heavily affected by the conditional pose labels fed into the generator.
% In order to better indicate the quality of geometry to some extent, we apply the novel FID computation approach proposed by \cite{DBLP:journals/corr/abs-2303-14407}. This is achieved by allowing the conditional label of the generator and rendering label in volume rendering to be independent of one another. We apply this novel FID computation to EG3D and 3DPortraitGAN, see FID$^*$ in Tab. \ref{tab: metrics}.
%
Our 3DPortraitGAN model shows notable improvements in FID.
Moreover, we obtain similar FID scores when utilizing the body poses predicted by both $\Gamma_D$ and $\Gamma_G$ in our model, implying that the  body pose distribution predicted by $\Gamma_G$ closely resembles the genuine distribution.
%
 
%
We employ ArcFace \cite{DBLP:conf/cvpr/DengGXZ19} to evaluate the ability of models in maintaining facial identity. Specifically, we produce a pair of images with different camera views for 1,024 randomly selected latent codes for each model.
We observe that the performance of ArcFace is affected by extreme camera angles, in which the face is completely occluded. To address this, we solely sample camera positions for each view with $\mu \in[0.25\pi,0.75\pi]$ from the \textit{$\it{360}^{\circ}$PHQ} dataset, ensuring unobstructed face regions.
%
To ensure a fair comparison, we set the body pose to a neutral position for our model and set the conditional camera parameters of VoxGRAF, EG3D, and 3DPortraitGAN as the average camera parameters. We also set the truncation of all models as 1.0.
%
As shown in Tab. \ref{tab: metrics}, our model presents improvements in facial identity consistency, indicating that our model attains superior performance in generating realistic view-consistent results.



% Figure environment removed

% Figure environment removed







\section{Ablation Study}
\label{sec: Ablation Study}



\subsection{Pose prediction}
As illustrated in Fig. \ref{fig: pose_predict}, we present the coarse body poses from \textit{$\it{360}^{\circ}$PHQ} alongside the body poses predicted by our body pose-aware discriminator. Our results exhibit a notable enhancement in the accuracy of predicted body poses, surpassing the precision of the coarse body poses acquired from real images through an off-the-shelf body reconstruction approach \cite{DBLP:conf/cvpr/ChoiMPL22}.


To demonstrate that our pose predictor generates more accurate body poses and improves model performance, we conduct an experiment by directly utilizing the coarse body poses from \textit{$\it{360}^{\circ}$PHQ} to train a baseline model. In particular, we remove the pose predictors from the generator and discriminator and train the model using the coarse body poses obtained from \textit{$\it{360}^{\circ}$PHQ}.
%
For both baseline and 3DPortraitGAN models
%(both are trained for 6M images)
, we generate 1,024 portrait images by using randomly selected latent codes, average camera parameters with $\mu = \frac{1}{2} \pi$ and $\nu = \frac{1}{2} \pi$, and neutral body pose (i.e. no deformation is performed). 
To compare the performance of the two models in generating canonical 3D representation, we utilize $\Gamma_D$ from our full model to predict the body poses of the randomly generated portraits. Then we calculate the mean and standard deviation of the predicted body poses' absolute values for each model.

As outlined in Tab. \ref{tab: ablation Pose prediction}, our 3DPortraitGAN model produces lower mean and standard deviation values for body poses when the deformation is disabled. This indicates that our pose learning and prediction module aids the generator in better learning the canonical 3D representation.
%
The higher body pose mean and deviation observed in the baseline model may be due to some distortion cases in the random outputs. 
As coarse body poses are used as conditional labels for the discriminator to compute image scores and for the generator to compute the deformation field, their inaccuracy will lead the generator to learn some distorted samples to fit the real image distribution.
We will present some distorted results in the supplementary file.


 
\begin{table}[t]
\begin{tabular}{@{}ccc@{}}
\toprule
                    & w/o pose learning  & Ours     \\ \midrule
mean                & 4.36$^\circ$             & \textbf{2.51}$^\circ$    \\  
standard deviation  & 3.29$^\circ$             & \textbf{1.73}$^\circ$    \\ \bottomrule
\end{tabular}
\caption{ 
 We compare the performance of our 3DPortraitGAN model against the baseline model by generating 1,024 portrait images without performing deformation and computing the mean and standard deviation values for the predicted body pose.
}
  \label{tab: ablation Pose prediction}
\end{table}





% \subsection{Dataset rebalance}
% In Sec. \ref{sec: Training Details}, we mentioned that the imbalanced \textit{$\it{360}^{\circ}$PHQ} dataset will cause artifacts on the back of the head, which could be addressed by dataset rebalancing.


\subsection{Rear-view Depth Regularization Loss}
In Sec. \ref{sec: Losses}, we discussed the geometric ambiguity issue that may result in face artifacts on the back of the head. To address this problem, we introduced a novel regularization loss $L_{rear}$. Fig. \ref{fig: rear-view-reg} demonstrates the effectiveness of $L_{rear}$. When $L_{rear}$ is not applied (left), the generator simply learns a face geometry for the back of the head. Conversely, when $L_{rear}$ is applied (right), the generator learns a smoother, more natural-looking geometry. 
%
The baseline model (left) is trained using the same training strategy as our full model but without the use of $L_{rear}$.



\subsection{Mesh-guided deformation field}
In Sec. \ref{sec: Deformation Module}, we mentioned that the mesh-guided deformation field in RigNeRf \cite{DBLP:conf/cvpr/AtharXSSS22} will cause an ``offset face'' artifact. We show this phenomenon in Fig. \ref{fig: ablation_deform}. We deform the portraits using body pose $p_{n} = [0,\frac{\pi}{2},0]$ and $p_{h} = [0,0,0]$, it can be seen that the deformed portrait using RigNeRf deformation field suffer from the ``offset face'' artifact, while ours is more realistic.


% Figure environment removed



% \section{User Study}
% \label{sec: User Study}
