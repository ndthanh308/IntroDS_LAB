





% Figure environment removed









\section{Results}
\label{sec: Results}

Fig. \ref{fig: generation-2} displays a selection of random samples generated by our model. Real images from the \textit{$\it{360}^{\circ}$PHQ} dataset are randomly sampled (1st col) and passed through $\Gamma_D$ to predict the %obtain the predicted 
body poses. Latent codes are then randomly sampled, and the camera parameters and predicted body poses of the real images are used to generate images (2nd-7th cols).
We also show results rendered from steep camera parameters in Figs. \ref{fig:yaw}-\ref{fig:pitch}.

To illustrate 3DPortraitGAN's performance in generating novel views for real images, we perform latent code optimization in the $W$ latent space to real images using our 3DPortraitGAN model. The input real images are never seen during training. As shown in Fig.~\ref{fig: inversion}, 3DPortraitGAN produces reasonable reconstructed portrait geometry and appearance. See more discussion about our results in Sec. \ref{sec: Discussion}. {Further details on the inversion process can be found in the supplementary file.}
%\hbc{but artifacts are more visible in such results. Are you going to discuss them here or in the limitation part?} \onethousand{[Yiqian: Yes. We have pointed out in the limitation part that our model still lacks the expressive power to accurately represent portrait images in real-life.]}



% Figure environment removed


\section{Comparison}
\label{sec: Comparison}
%To be consistent with the scope of our method, we compare only to methods that
To be consistent with the scope of our method, we choose VoxGRAF \cite{DBLP:conf/nips/SchwarzSNL022}, StyleNeRF \cite{DBLP:conf/iclr/GuL0T22}, EG3D \cite{DBLP:conf/cvpr/ChanLCNPMGGTKKW22}, and PanoHead \cite{An_2023_CVPR} as state-of-the-art representations of the previous 3D-aware generators to compare. We {re-train those works on our dataset using their official implementation.} %\hbc{adapt? what do you mean here? You re-train them on our dataset?}
%
Rodin \cite{DBLP:journals/corr/abs-2212-06135} has a similar objective as ours. However, Rodin requires a multi-view image dataset of avatars to fit tri-planes, which cannot be applied to our single-view dataset. Furthermore, the dataset and code of Rodin are not publicly available. Therefore, we only show random Rodin results in our qualitative comparison.
%


\subsection{Qualitative Comparison}
In Fig. \ref{fig: comparison}, we present a qualitative comparison between the SOTA methods and 3DPortraitGAN. The results show that VoxGRAF, EG3D, and PanoHead suffer from distortion and artifacts due to their attempt to directly learn 3D portraits from a dataset with diverse body poses. The results of StyleNeRF exhibit interpolated faces, which can be more visible in our supplementary video. Rodin generates rendering-style results that lack realism.
%
In contrast, 3DPortraitGAN shows an enhancement in terms of the quality of results. Our approach generates high-quality multi-view rendering results and reasonable 3D geometry.

\begin{table}[t]
\caption{FID and face identity for our model and SOTA methods. %\hbc{Why not include Rodin? I assume this is explained in the main text.}
$^\star$ means the body poses used  %that are used 
to generate images are predicted by $\Gamma_D$ from real images in \textit{$\it{360}^{\circ}$PHQ},  $^\diamond$ means the body poses used to generate images are sampled from the pose distribution predicted by $\Gamma_G$. 
%FID$^*$ ($\downarrow$) means that the conditional label of the generator and rendering label in volume rendering are independent of one another.
}
\scalebox{0.85}{
\begin{tabular}{@{}ccccccc@{}}
\toprule
             & VoxGRAF &StyleNeRF  & EG3D  &PanoHead & Ours                             \\ \midrule
% FID $\downarrow$          & 39.41   &22.16      & 14.52 & 17.52   &\textbf{12.98}$^\star$ /\textbf{13.30}$^\diamond$    \\  
% FID$^*$      & 52.21   &N/A        & 16.55 &      & \textbf{14.48}$^\star$ /\textbf{15.36}$^\diamond$    \\ 
% Identity (full range)     & 0.44    &           & 0.48  & \textbf{0.58}  \\ 
%Identity $\uparrow$    & 0.52    &0.48       & 0.59  & 0.52   & \textbf{0.71}  \\   
FID $\downarrow$          & 39.41   &22.16      & 14.52 & 14.49   &\textbf{12.88}$^\star$ /\textbf{13.75}$^\diamond$    \\ \midrule 
Identity $\uparrow$    &  0.52   &0.46       & 0.57  & 0.60  & \textbf{0.68}  \\ 
\bottomrule
\end{tabular}
}
  \label{tab: metrics}
  %\vspace{-10pt}
\end{table}

\subsection{Quantitative Comparison}
Regarding the quantitative results, we compare our method to SOTA alternatives using Fr\'echet Inception Distance (FID) \cite{DBLP:conf/nips/HeuselRUNH17} and facial identity metrics (refer to Tab. \ref{tab: metrics}).

To assess the rendering quality of models, we use FID, which computes the distance between the distribution of the generated images and that of the real images to evaluate the quality and diversity of the generated images.
For each model, we generate 50K images using the camera parameters sampled from the \textit{$\it{360}^{\circ}$PHQ} dataset. 
For our 3DPortraitGAN, we utilize pose predictor $\Gamma_D$ to predict body poses from real images in \textit{$\it{360}^{\circ}$PHQ} ($^\star$ in Tab. \ref{tab: metrics}), and also sample body poses from the pose distribution %that is 
predicted by the pose predictor $\Gamma_G$ in the generator ($^\diamond$ in Tab. \ref{tab: metrics}). 
% As explained by \cite{DBLP:journals/corr/abs-2303-14407}, the FID value of pose-conditional generator architecture (VoxGRAF, EG3D, and 3DPortraitGAN) is heavily affected by the conditional pose labels fed into the generator.
% In order to better indicate the quality of geometry to some extent, we apply the novel FID computation approach proposed by \cite{DBLP:journals/corr/abs-2303-14407}. This is achieved by allowing the conditional label of the generator and rendering label in volume rendering to be independent of one another. We apply this novel FID computation to EG3D and 3DPortraitGAN, see FID$^*$ in Tab. \ref{tab: metrics}.
%
Our 3DPortraitGAN model shows notable improvements in FID.
Moreover, we obtain similar FID scores when utilizing the body poses predicted by both $\Gamma_D$ and $\Gamma_G$ in our model, implying that the  body pose distribution predicted by $\Gamma_G$ closely resembles the genuine distribution.
%
 
%
We employ ArcFace \cite{DBLP:conf/cvpr/DengGXZ19} to evaluate the ability of models to maintain facial identity. Specifically, we produce a pair of images with different camera views for 1,024 randomly selected latent codes for each model.
We observe that the performance of ArcFace is affected by extreme camera angles, in which the face is completely occluded. To address this, we solely sample camera positions for each view with $\mu \in[0.25\pi,0.75\pi]$ from the \textit{$\it{360}^{\circ}$PHQ} dataset, ensuring unobstructed face regions.
%
To ensure a fair comparison, we set the body pose to a neutral position for our model and set the conditional camera parameters of all models as the average camera parameters. We also set the truncation of all models as 0.6.
%
As shown in Tab. \ref{tab: metrics}, our model presents improvements in facial identity consistency, indicating that our model attains superior performance in generating realistic view-consistent results.





\begin{table}[t]
\caption{ 
 We compare the performance of our 3DPortraitGAN model against the baseline model by generating 1,024 portrait images without performing deformation and computing the mean and standard deviation values for the predicted body poses.
}
\begin{tabular}{@{}ccc@{}}
\toprule
                    & w/o pose learning  & Ours     \\ \midrule
% mean                & 4.36$^\circ$             & \textbf{2.51}$^\circ$    \\  
% standard deviation  & 3.29$^\circ$             & \textbf{1.73}$^\circ$    \\ \bottomrule
% mean                &  2.19$^\circ$             & \textbf{1.76}$^\circ$    \\  
% standard deviation  &  1.58$^\circ$             & \textbf{1.42}$^\circ$    \\ \bottomrule
mean                &  3.19$^\circ$             & \textbf{2.51}$^\circ$    \\  
standard deviation  &  2.88$^\circ$             & \textbf{2.49}$^\circ$    \\ \bottomrule
\end{tabular}
  \label{tab: ablation Pose prediction}
\end{table}







\section{Ablation Study}
\label{sec: Ablation Study}

% Figure environment removed


\subsection{Pose prediction}
\label{sec: ablation pose prediction}
As illustrated in Fig. \ref{fig: pose_predict}, we present the coarse body poses from \textit{$\it{360}^{\circ}$PHQ} alongside the body poses predicted by our body pose-aware discriminator. Our results exhibit a notable enhancement in the accuracy of the predicted body poses, surpassing the precision of the coarse body poses acquired from real images through an off-the-shelf body reconstruction approach \cite{DBLP:conf/cvpr/ChoiMPL22}.


To demonstrate that our pose predictor generates more accurate body poses and improves model performance, we conduct an experiment by directly utilizing the coarse body poses from \textit{$\it{360}^{\circ}$PHQ} to train a baseline model. In particular, we remove the pose predictors from the generator and discriminator and train the model using the coarse body poses obtained from \textit{$\it{360}^{\circ}$PHQ}.
%
For both baseline and 3DPortraitGAN models,
%(both are trained for 6M images)
we generate 1,024 portrait images by using randomly selected latent codes, camera parameters with $\mu = \frac{1}{2} \pi$ and $\nu = \frac{1}{2} \pi$ (i.e., frontal view), and neutral body pose (i.e., no deformation performed). 
To compare the performance of the two models in generating the canonical 3D representation, we utilize $\Gamma_D$ from our full model to predict the body poses of the randomly generated portraits. Then we calculate the mean and standard deviation of the predicted body poses' absolute values for each model.

As outlined in Tab. \ref{tab: ablation Pose prediction}, our 3DPortraitGAN model produces lower mean and standard deviation values for body poses when the deformation is disabled. This indicates that our pose learning and prediction module aids the generator in better learning the canonical 3D representation.
%
The higher body pose mean and deviation observed in the baseline model may be due to some distortion cases in the random outputs. 
As coarse body poses are used as conditional labels for the discriminator to compute image scores and for the generator to compute the deformation field, their inaccuracy will lead the generator to learn some distorted samples to fit the real image distribution.
We %will 
present some distorted results in the supplementary file.





{\subsection{Tri-grid and Mask Guidance}
We observe the presence of a face in the rear view (refer to Fig. \ref{fig: rear-view-reg}), which we call a ``rear-view-face'' artifact. Despite the generator's ability to learn the texture of hair at the back of the head, the face geometry is sometimes apparent in the shape extracted by the marching cubes algorithm. Such an artifact also exists in the comparison results in Rodin \cite{DBLP:journals/corr/abs-2212-06135} and PanoHead \cite{An_2023_CVPR}, where they retrain EG3D using their own dataset.
% %
We attribute this phenomenon to the geometric ambiguity when using single-view images as training data. The discriminator can only assess the rendered images, leading to an under-determined problem in characterizing the geometry of the back of the head. In the absence of constraints, our model may converge to a suboptimal solution marked by front-back symmetric geometry.


To address this problem, in Sec. \ref{sec: Methodology}, we employ the tri-grid 3D representation and mask guidance proposed by PanoHead \cite{An_2023_CVPR}.
The tri-grid helps alleviate mirror feature artifacts, and the foreground masks provide more accurate 3D prior to some extent.
%
Fig. \ref{fig: rear-view-reg} demonstrates the effectiveness of tri-grid and mask guidance. When tri-grid and mask guidance are not applied (left), the generator simply learns a face geometry for the back of the head. Conversely, with 
tri-grid and mask guidance %is applied 
(right), the generator learns a smoother, more natural-looking geometry. 
% 
 
}





\subsection{Mesh-guided Deformation Field}
In Sec. \ref{sec: Deformation Module}, we mentioned that the mesh-guided deformation field in RigNeRf \cite{DBLP:conf/cvpr/AtharXSSS22} would 
cause an ``offset face'' artifact. We show this phenomenon in Fig. \ref{fig: ablation_deform}. We deform the portraits using body pose $p_{n} = [0,\frac{\pi}{2},0]$ and $p_{h} = [0,0,0]$, it can be seen that the deformed portrait using RigNeRf deformation field suffer from the ``offset face'' artifact, while ours is more realistic.


% Figure environment removed



% \section{User Study}
% \label{sec: User Study}
                                                                                                                                                                                                                                                                                                                                                                                                                           