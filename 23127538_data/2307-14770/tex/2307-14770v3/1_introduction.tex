

% Figure environment removed
\section{Introduction}
\label{sec: Introduction}

%In recent years, there has been a significant advancement in the development of 3D-aware generators.
There has been significant progress in the development of 3D-aware generators in recent years.
Unlike 2D GANs, which can only produce high-quality single-view images, 3D-aware generators  \cite{DBLP:conf/cvpr/ChanLCNPMGGTKKW22,DBLP:journals/corr/abs-2112-11427,DBLP:conf/iclr/GuL0T22} utilize voxel rendering or NeRF rendering to acquire knowledge of the 3D geometry from 2D image collections. 3D-aware generators have been instrumental in facilitating image and video editing tasks \cite{DBLP:journals/corr/abs-2205-15517, nerffaceediting,DBLP:conf/siggrapha/JinRKBC22,DBLP:journals/corr/abs-2301-02700,DBLP:journals/corr/abs-2203-13441,DBLP:journals/corr/abs-2302-04871,10.1145/3597300} since they are able to produce multi-view consistent results with realistic geometry.


Most 3D-aware face generators only require single-view face images as training data. These single-view face datasets, such as \textit{FFHQ} \cite{DBLP:conf/cvpr/KarrasLAHLA20} and \textit{CelebA} \cite{liu2015faceattributes}, usually consist of in-the-wild images, which are readily accessible and abundant on the Internet. 
%
Despite their usefulness, these currently widespread single-view face datasets %\hbc{I'm not sure if it's appropriate to criticize single-view face datasets since ours is also a single-view dataset? There is nothing wrong with `single-view'?} 
have certain limitations. 
%
First, the datasets primarily consist of frontal or near-frontal views, with limited views from larger poses and no views from behind the head. 
As a result of using \textit{FFHQ} and \textit{CelebA}, 3D-aware face generators \cite{DBLP:conf/cvpr/ChanLCNPMGGTKKW22,DBLP:journals/corr/abs-2112-11427,DBLP:conf/iclr/GuL0T22,DBLP:conf/nips/SchwarzSNL022} only produce 
%facial and 
frontal-head area data and fail to produce the back of the head. 
%
Second, although PanoHead \cite{An_2023_CVPR} is able to achieve full-head generation by collecting back-head and large-pose images (\textit{FFHQ-F}) and utilizing self-adaptive image alignment, \textit{FFHQ-F} only includes %facial
{head}
data and does not contain complete data for the neck and shoulder regions. 
Consequently, the geometry in PanoHead's results is still limited to the facial region, and the neck and shoulders are incomplete.
%


The reasons behind these dataset limitations are twofold. 
First, the availability of such single-view face datasets mainly depends on the accuracy of face-recognition technology used to extract faces from in-the-wild images and the accuracy of face reconstruction methods utilized to extract camera parameters. In some cases where cameras are positioned at a large camera pose or even behind the head, the necessary facial features required for accurate recognition may be obscured, making it difficult to extract data from all angles.
%
Second, for in-the-wild \textit{one-quarter headshot}\footnote{\hyperref[]{https://www.backstage.com/magazine/article/types-of-headshots-75557/}} portraits that include the neck and shoulder regions, diverse body poses are always present. Unfortunately, current 3D-aware portrait generators require portrait images in a canonical space where all objects are uniformly positioned, have semantically meaningful correspondence and similar scale, and undergo no significant deformations.
%\hbc{Is `canonical space' known to everybody? Since `canonical' is emphasized in the paper title, this should be made clear.}. 
Otherwise, the dimensionality of the data distribution becomes prohibitively high, resulting in significant distortion in the results.
%
Therefore, the lack of sufficient data and appropriate methods creates a significant challenge for developing a one-quarter headshot 3D-aware portrait generator using a single-view dataset.



Another line of research focuses on multi-view portrait data. 
To reconstruct 3D portrait geometry, researchers have developed 3D portrait datasets \cite{DBLP:conf/cvpr/Yang0WHSYC20,wuu2022multiface,DBLP:journals/corr/abs-1904-00168} that consist of high-quality multi-view labeled portrait images. Nevertheless, the diversity of these datasets is constrained by the challenges involved in data collecting and processing.  
%
Synthetic portrait datasets \cite{DBLP:journals/corr/abs-2212-06135,DHFdataset,DBLP:conf/iccv/WoodBHD0S21} offer a convenient solution to generate portrait data with diverse environments and camera parameters. 
Given its capacity to regulate data creation and produce reliable ground-truth labels, such as segmentation masks and landmarks, synthetic data has become a popular tool for training computer vision models.
Rodin \cite{DBLP:journals/corr/abs-2212-06135} utilizes rendered portrait images as its training dataset and achieves a one-quarter headshot portrait generation model. However, its results are constrained by the unrealistic rendering style (see Fig. \ref{fig: comparison}) %\hbc{are we going to compare with it? If yes, you may refer to specific figures containing such comparisons} 
%\onethousand{[Yiqian: Ok, we have added Rodin's results in comparison.]}
of the training data. Additionally, Rodin demands an ample, multi-view image dataset of avatars (consisting of at least 300 multi-view images for each 100K synthetic individuals) to fit tri-planes, making this method highly data-dependent.
%
In summary, no suitable multi-view face data is available for training a realistic 3D-aware portrait generator.


This paper proposes a novel realistic 3D-aware one-quarter headshot portrait generator, \textbf{3DPortraitGAN}, which can learn a canonical 3D avatar distribution from a collection of single-view real portraits with body pose self-learning. The generator is capable of producing realistic, view-consistent portrait images from $360^{\circ}$ camera angles, including complete head, neck, and shoulder geometry. 
%
Regarding the training data, we focus on utilizing single-view portrait images from the Internet. Considering the challenges associated with collecting data from all camera angles using face recognition methods, we propose to use more distinctive body features to collect data. 
%
Specifically, we introduce a new data processing method based on an off-the-shelf body reconstruction method, 3DCrowdNet \cite{DBLP:conf/cvpr/ChoiMPL22}. We apply 3DCrowdNet to extract camera parameters and body poses from in-the-wild images. Then, we identify the desired one-quarter headshot portrait regions to obtain aligned images.
The resulting dataset, named  {$\bf{360^{\circ}}$}-\textbf{P}ortrait-\textbf{HQ} (\textbf{$\bf{360^{\circ}}$PHQ}), comprises \textbf{54,000} high-quality single-view portraits with a wide range of camera angles {(the yaw angles span the entire $360^{\circ}$ range). %\hbc{Here you're talking about `camera poses', it's weird to have `body poses'? You meant the yaw angles span the entire body poses?}.} \onethousand{[Yiqian: Have removed.]}
%
%\hbc{This paragraph is rather long. Shall we break it here?}

{Our framework is built on the backbone of EG3D \cite{DBLP:conf/cvpr/ChanLCNPMGGTKKW22} and incorporates the tri-grid 3D representation and mask guidance from PanoHead \cite{An_2023_CVPR}.}
While we aim to generate human geometry within a canonical space using our generator, the diverse body poses in the \textit{$\it{360}^{\circ}$PHQ} dataset pose a challenge for learning a canonical 3D avatar representation. 
%
To address this issue, we employ a deformation module to deform the generated human geometry in the canonical space. This ensures that the volume rendering results display the desired body pose to fit within the real portrait distribution.
%
Since the estimated body poses in the dataset are imprecise, we incorporate two pose predictors into both the generator and discriminator to achieve body pose self-learning.
%
As depicted in Fig. \ref{fig: motivation}, the generator's pose predictor learns a distribution of body pose, which the generator utilizes to generate portraits. The generated portrait, along with its foreground mask, 
is then processed by the body-pose-aware discriminator, where the pose predictor predicts its body pose. The difference between the estimated and input body poses of the generated portrait is utilized to train the pose predictor in the discriminator.
Additionally, the body-pose-aware discriminator is conditioned on the predicted body pose to score the generated (or real) portraits, which are further used to train the generator and the discriminator networks.



To the best of our knowledge, our delicately designed framework, coupled with the novel portrait dataset, enables our 3DPortraitGAN model to become the first 3D-aware one-quarter headshot GAN capable of effectively learning $360^{\circ}$ canonical 3D portraits from single-view and body-pose-varied 2D data, while also achieving body pose self-learning.
%
Through extensive experiments, we demonstrate that our framework can generate view-consistent, realistic portrait images with complete geometry from a wide range of camera angles and accurately predict portrait body poses.


In summary, our work makes the following major contributions:
\begin{itemize}

    \item A large-scale dataset of high-quality single-view real portrait images featuring diverse camera parameters and body poses.

    
     \item The first 3D-aware one-quarter headshot GAN framework that can learn $360^{\circ}$ canonical 3D portrait distribution from the proposed dataset with body pose self-learning. 
     % A 3D-aware full-head GAN framework, the first one capable of learning a $360^{\circ}$ canonical 3D portrait distribution from the proposed dataset, with the added benefit of self-learning body pose.
     % \onethousand{
     % A 3D-aware GAN framework, 3DPortraitGAN, can learn canonical 3D portraits containing full-head, neck, and shoulders data. To the best of what we know, our 3DPortraitGAN is the first one to learn canonical 3D portraits from a body-pose-varied dataset with body pose self-learning.
     % }
     
     
     
     

     
    


\end{itemize}



