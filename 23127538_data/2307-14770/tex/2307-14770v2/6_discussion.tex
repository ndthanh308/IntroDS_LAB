\section{Discussion}
\label{sec: Discussion}

% Our method has limitations.
% % 
% First, we employed a deformation field that is solely governed by a non-shaped SMPL model, with no regard for the shape of the resulting portrait geometry. While this deformation method did not interfere with our training process, it resulted in artifacts during inference, such as long hair being truncated due to deformation. Also the computation of the deformation field is time-costly and memory intensive, making our training at half the speed of EG3D. 
% %
% Secondly, training the pose predictor $\Gamma_G$ in the generator is prone to collapsing. To mitigate this issue, we freeze $\Gamma_G$ after a certain training duration.
% %
% Thirdly, we introduced a depth regularization loss $L_{rear}$ to regulate the depth of the occiput. However, the SMPL model is incapable of representing the geometry of every portrait in the real distribution. As such, the weight of $L_{rear}$ cannot be set too high. Although our approach significantly mitigates face artifacts on the occiput, it is not completely eliminated. This issue could be addressed by using a more appropriate constraint by rendering depth images using models with hair and clothes modeled realistically.
% %
% Finally, we directly use the camera poses in our \textit{$\it{360}^{\circ}$PHQ} dataset as training labels, where the  inaccurate estimation may lead to geometric artifacts. This limitation can be overcome by applying a more accurate body pose estimation method in the future.

%\hbc{Consider breaking it into 2-3 shorter paragraphs since it's too long and there is only one paragraph under a section.}
Despite its noteworthy performance, our method has some limitations. 
Firstly, we employ a deformation field solely governed by the SMPL model, which does not account for the shape of the resulting geometry of a portrait. Although this deformation method does not interfere with our training process, it leads to some artifacts during inference, such as truncated long hair {(highlighted by the green box in Fig. \ref{fig: limitation} (a))}. Additionally, the computation of the deformation field is time- and memory-intensive, making our training much slower than EG3D and costing almost twice as much time.

Secondly, the pose predictor $\Gamma_G$ within the generator is prone to collapsing and heavily influences the entire framework during the training phase.  To prevent this, we freeze $\Gamma_G$ after it is trained for a certain duration. 
Furthermore, while the pose self-learning strategy in our framework can assist in predicting more precise body poses and alleviate distortions in the results, as shown in Tab. \ref{tab: ablation Pose prediction}, our model still cannot achieve entirely canonical representations {(see distorted samples in Fig. \ref{fig: limitation} (c))}. This is because of the inaccurate predictions in our pose learning method, even if the outcome is better than the coarse body poses in our dataset.
During training, we directly used the camera parameters in our \textit{$\it{360}^{\circ}$PHQ} dataset as training labels. However, the inaccurate estimation of these camera parameters might cause geometric artifacts. This issue can be overcome by using a more accurate body pose estimation method in the future.

{
Thirdly, we employ tri-grid and foreground mask guidance to alleviate the ``rear-view-face'' artifacts. However, as the rear-view data in our dataset is significantly less than the frontal data, our approach does not entirely eradicate them (highlighted by the blue boxes in Fig. \ref{fig: limitation}(b)). This issue might be addressed by adding more rear-view data to the training set.
}

Finally, our model still lacks the expressive power to accurately represent real-life portrait images, as seen in the real image inversion results presented in Fig. \ref{fig: inversion}. 
{We attribute this to the limited resolution ($256^2$) of the tri-grids and the training images. This issue could be addressed by increasing the resolution and seeking more efficient training strategies to prevent the model from consuming excessive computation to train high-resolution tri-grids and final results.}

% 




% Figure environment removed