

% Figure environment removed




\section{Dataset}
\label{sec: Dataset}
In this section, we will describe our data processing pipeline. We focus on utilizing single-view portrait images online due to their easy accessibility and abundance. However, using face recognition and face reconstruction methods to gather annotated data from all camera angles is challenging since the facial features required for accurate recognition may be obscured. Therefore, we propose {using more distinctive} body features (e.g., shoulders) to collect data.
%
In particular, we introduce a novel data processing method based on an off-the-shelf body reconstruction method \cite{DBLP:conf/cvpr/ChoiMPL22} to extract camera parameters and body poses from in-the-wild images, enabling us to obtain aligned portraits.

%
We begin by assuming that we have a human body SMPL \cite{DBLP:journals/tog/LoperM0PB15} template mesh in the local space, denoted as $M$, with the standard body shape. 
Its neck joint is aligned to the origin point $[0,0,0]$, and no additional global rotation or translation is performed on $M$. We denote the template mesh with body pose parameters %\hbc{there are additional parameters? they are not used? why?} 
%\onethousand{[Yiqian: To clarify, we only use body poses as parameters in our framework, and there are no additional parameters included. We do not perform any global rotation or translation of SMPL, as we aim to keep the mesh in a local space. Additionally, we fixed the shape parameters of SMPL as a standard value since we only need a simple deformation field for training, and a standard body shape is enough.]}
$\vec{\theta} \in \mathbb{R}^{69}$ as $M(\vec{\theta})$.
%
As we aim to solely preserve the head, neck, and shoulder regions of the input portrait, we only consider the neck pose $p_n \in \mathbb{R}^{3}$ and head pose $p_h \in \mathbb{R}^{3}$ in $\vec{\theta}$, 
%while disregarding  the remaining body pose. 
{while setting the remaining body pose as zero.}
%\hbc{what do you mean by `disregarding' here? fixing the remaining parameters? if you don't consider them, does SMPL still work?} 
Thus, we define the neck pose and head pose as $p = [p_n,p_h] \in \mathbb{R}^{6}$ and denote the template mesh with neck and head pose $p$ as $M(p)$.
%
Regarding camera settings, similar to EG3D \cite{DBLP:conf/cvpr/ChanLCNPMGGTKKW22}, we assume that our camera is always positioned on a sphere with radius $r = 2.7$, directed towards a fixed point. Additionally, intrinsic camera parameters are fixed as constant values. 
% \hbc{is this a reasonable assumption? what if this assumption does not hold?}
% \onethousand{[Yiqian: We aligned and cropped all the raw images based on this assumption. While we believe that this assumption may not hold for all images, the training results demonstrate that it is sufficient.
% In addition, we adopted the camera setting ($r = 2.7$) from EG3D as a default value. If it is not essential, we can remove this information and simply state that the camera radius is fixed for all images in the dataset.]}\hbc{Maybe cite EG3D here?}
% \onethousand{[Yiqian: OK.]}




Given an in-the-wild portrait image, our aim is to find the camera parameters, neck pose, and head pose of the portrait, 
% {allowing us to render the mesh $M$ using the estimated camera parameters and body poses to render a result aligning with the input portrait's head, neck, and shoulder regions.}
{allowing us to render the mesh $M$ aligning with the input portrait's head, neck, and shoulder regions.}
%
%
Using an off-the-shelf body reconstruction method, 3DCrowdNet \cite{DBLP:conf/cvpr/ChoiMPL22}, we extract the input portrait's SMPL parameters (global rotation $rot$ and translation $trans$, shape parameters $\vec{\beta} \in \mathbb{R}^{10}$, and pose parameters $\vec{\theta} \in \mathbb{R}^{69}$), resulting in an estimated mesh $\tilde{M}(trans, rot, \vec{\beta},\vec{\theta})$ in the world space (with a fixed camera).
% 
%We apply the neck pose and head pose of the estimated mesh to our template mesh
{We set the neck pose and head pose of $M$ to be the same as those of the estimated mesh}, 
%\hbc{How to apply? I'm not sure. Why not simply use the estimated mesh?} \onethousand{[Yiqian: We have clarified that we set the neck and head pose of the template mesh to be the same as those of the estimated mesh. We do not simply use the estimated mesh in our method because the estimated mesh may contain body poses (such as legs and hands) that may affect the computation of the deformation field.]}, 
resulting in $M(p)$. Then we compute the transformation matrix that could transform $\tilde{M}(trans, rot, \vec{\beta},\vec{\theta})$ to align its head, neck, and shoulders joints with those of $M(p)$. 
%
Next, we apply the same transformation matrix to the fixed camera and normalize its camera parameters according to our camera assumption, obtaining the final camera parameters. 
%
The final camera parameters, denoted as $c \in \mathbb{R}^{25}$, comprise an extrinsic camera matrix $e \in \mathbb{R}^{16}$ and an intrinsic camera matrix $k \in \mathbb{R}^{9}$. Note that $k$ remains fixed as a constant matrix.
%
Then the raw image is cropped and aligned based on the obtained parameters. To ensure that the head, neck, and shoulders are fully covered, we set a uniform crop region for all images. This process results in an aligned one-quarter headshot image denoted as $I$ (see Fig. \ref{fig: dataset_sample}). 
% \hbc{Are we supposed to see the aligned effect in Fig. 3?}
% \onethousand{[Yiqian: I showed some aligned results in the supplementary files. If necessary, I can add some of them to Fig. 3.]} \hbc{Okay, otherwise you should refer to the supp. mat. instead of Fig. 3.} \onethousand{[Yiqian: Have added.]}
%


% Figure environment removed


To filter out the images with inaccurately estimated camera parameters, we render the mesh $M(p)$ on $I$ using the camera parameters $c$. We then manually examine the rendering results and remove any images where the mesh rendering is not well-aligned with $I$, as well as blurry or noisy images.
%
However, due to the limitations of the body reconstruction method, we encounter cases where the neck and head rendering results are not aligned with $I$, even though shoulder reconstruction is more accurate. During manual selection, we avoid using the neck and head alignment as a criterion for manual selection and only consider the shoulder alignment.
%
As a result, the estimated camera parameters can render the template mesh in the local space to have aligned shoulders with the portraits. However, the estimated neck and head pose $p$ is ``coarse'' and inaccurate.
%
Therefore, instead of being used directly as the training label, this coarse body pose is only employed to calculate a regularization loss during the early stages of the training process, which we will explain in later sections (Sec. \ref{sec: Body Pose Regularization Loss}).
%\hbc{maybe make it clear which sections they are?}.




In sum, we collected 41,767 raw portrait images from Pexels\footnote{\hyperref[]{https://www.pexels.com}} and Unsplash\footnote{\hyperref[]{https://unsplash.com}} and finally got \textbf{54,000} aligned images as our \textit{$\it{360}^{\circ}$PHQ} dataset. The number of aligned images is greater than that of raw images, since a single raw image may contain multiple people.  
%
Samples of these images can be found in Fig. \ref{fig: dataset_sample} as well as the supplementary file {(Sec. E)}.
% Our dataset is augmented by a horizontal flip
Our dataset is augmented by a horizontal flip.
We also extract the portrait {foreground segmentation mask} 
%\hbc{mask(s)? if there are one or more masks depending on the number of people in each image?}\onethousand{[Yiqian: We only extract the foreground segmentation mask for a person in the image. Have revised it.]} 
from each aligned image using the DeepLabV3 ResNet101 network \cite{DBLP:journals/corr/ChenPSA17}. These segmentation masks will be used as training guidance, as explained in Sec. \ref{sec: Discriminator}.
The images in our dataset are of high quality, with variations in gender, age, race, expression, and lighting. The analysis of the distribution of semantic attributes (gender, race, age, etc) can be found in the supplementary file (Sec. D). 

%
We convert the camera positions in our dataset to the spherical coordinate system ($\mu$ and $\nu$, or yaw and pitch), and visualize the distribution of camera positions in Fig. \ref{fig: data_distribution}. 
%
Our dataset contains a diverse set of camera distributions, {with yaw angles spanning the entire $360^{\circ}$ range.}









% Given an in-the-wild portrait image, our goal is to find the camera pose and body pose of the portrait, these parameters can then be used to render the $M(\vec{\theta})$ that aligned with the input portrait.
% We first employ an off-the-shelf body reconstruction method \cite{DBLP:conf/cvpr/ChoiMPL22} to extract the input portrait's SMPL parameters (global rotation $rot$ and translation $trans$, shape parameters $\vec{\beta} \in \mathbb{R}^{10}$ and pose parameters $\vec{\theta} \in \mathbb{R}^{69}$), getting an estimated mesh $\tilde{M}(trans, rot, \vec{\beta},\vec{\theta})$ in the world space.
% %
% Then we apply the pose parameters $\vec{\theta}$ of the estimated mesh to our template mesh as $M(\vec{\theta})$.
% % 
% The body reconstruction method \cite{DBLP:conf/cvpr/ChoiMPL22} outputs a mesh $\tilde{M}(trans, rot, \vec{\beta},\vec{\theta})$ that is globally rotated and translated (in the world space), along with a fixed camera to render it. Our aim is to determine the camera parameter to render the mesh $M(\vec{\theta})$ (in the local space) onto the portrait image, resulting in similar rendering outcomes as the estimated mesh.


% To accomplish this, we first translate $\tilde{M}(trans, rot, \vec{\beta},\vec{\theta})$ to align its neck joint to the origin point $[0,0,0]$, obtaining the translation matrix $T$. We then compute the rotation matrix that rotates $\tilde{M}(trans, rot, \vec{\beta},\vec{\theta})$ to align it with $M(\vec{\theta})$, obtaining the rotation matrix $R$.
% %
% Next, we apply the transformation matrix $RT$ to the fixed camera and normalize its camera pose according to our camera assumption, obtaining the final camera parameter. 
% %
% The final camera parameter, denoted as $c \in \mathbb{R}^{25}$, comprises an extrinsic camera matrix $e \in \mathbb{R}^{16}$ and an intrinsic camera matrix $k \in \mathbb{R}^{9}$. Note that $k$ remains fixed as a constant matrix.





% As a result, for each in-the-wild portrait image, we obtain its pose parameters $\vec{\theta}$, and camera pose $c$.
% %In this paper, we omit the shape parameters $\vec{\beta}$. 
% As we solely aim to preserve the head, neck, and shoulders region of the input portrait, we only consider the neck pose $p_n \in \mathbb{R}^{3}$ and head pose $p_h \in \mathbb{R}^{3}$ in $\vec{\theta}$, while disregarding the remaining body pose.
% %
% As a result, we define neck pose and head pose $p = [p_n,p_h] \in \mathbb{R}^{6}$ as the body pose, and denote the template mesh with body pose $p$ as $M(p)$ throughout the rest of the paper.
% Then all the raw images are cropped and aligned based on the obtained parameters, resulting in an aligned image denoted as $I$ (see Fig. \ref{fig: dataset_sample}). 




