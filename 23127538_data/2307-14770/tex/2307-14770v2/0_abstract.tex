\begin{abstract}

3D-aware face generators are typically trained on 2D real-life face image datasets that primarily consist of near-frontal face data, and as such, they are unable to construct \textit{one-quarter headshot} 3D portraits with complete head, neck, and shoulder geometry. Two reasons account for this issue: First, existing facial recognition methods struggle with extracting facial data captured from large camera angles or back views. Second, it is challenging to learn a distribution of 3D portraits covering the one-quarter headshot region from single-view data due to significant geometric deformation caused by diverse body poses. To this end, we first create the dataset $\it{360}^{\circ}$-\textit{Portrait}-\textit{HQ} (\textit{$\it{360}^{\circ}$PHQ} for short) which consists of high-quality single-view real portraits annotated with a variety of camera parameters (the yaw angles span the entire $360^{\circ}$ range) and body poses. We then propose \textit{3DPortraitGAN}, the first 3D-aware one-quarter headshot portrait generator that learns a canonical 3D avatar distribution from the \textit{$\it{360}^{\circ}$PHQ} dataset with body pose self-learning. Our model can generate view-consistent portrait images from all camera angles with a canonical one-quarter headshot 3D representation. Our experiments show that the proposed framework can accurately predict portrait body poses and generate view-consistent, realistic portrait images with complete geometry from all camera angles. 
We will release our \textit{$\it{360}^{\circ}$PHQ} dataset, code and pre-trained models for reproducible research.

\end{abstract}


% original abstract
% \begin{abstract}

% 3D-aware face generators have significantly improved the performance of various downstream applications, such as talking heads and multi-view consistent semantic editing.
% % They are commonly trained on 2D real-life face image datasets. Nevertheless, existing facial recognition methods often struggle to extract face data captured from various camera angles. Furthermore, in-the-wild images with diverse body poses introduce a high-dimensional challenge \hbc{is `high-dimensional challenge' commonly known? It's also not clear why this challenge makes it difficult for neck and shoulder regions.} for 3D-aware generators, making it difficult to utilize data that contains complete neck and shoulder regions. Consequently, these face image datasets often contain only near-frontal face data, which poses challenges for 3D-aware face generators to construct \textit{full-head} 3D portraits. 
% %
% {These generators are typically trained on 2D real-life face image datasets that primarily consist of near-frontal face data, and as such, they are unable to construct ``full-head'' 3D portraits. This can be attributed to two reasons: extracting face data captured from large camera angles or back views is challenging for existing facial recognition methods, and learning a distribution of 3D portraits from single-view data that covers complete neck and shoulder regions is difficult due to the significant geometric deformation caused by diverse body poses.}
% %\hbc{is `high-dimensional challenge' commonly known? It's also not clear why this challenge makes it difficult for neck and shoulder regions.}
% %
% To this end, we first create the dataset 
% {$\it{360}^{\circ}$}-\textit{Portrait}-\textit{HQ} (\textit{$\it{360}^{\circ}$PHQ} for short)
% % \hbc{do we want to emphasize this dataset-related contribution in the paper title? Can `single-view portrait dataset' clearly reflect the uniqueness of this dataset?}
% % \onethousand{[Yiqian: I think we should highlight the challenge posed by diverse body poses in our dataset as it significantly affects the model training process. Maybe `Learning Full-Head 3D GANs from a Single-View Body-Pose-Various Portrait Dataset' ?]}, \hbc{Body-Pose-Various sounds weird. But I don't have a better name at this moment. Can we use 360? But I'm afraid if use 360 (e.g., in 360 PHQ), readers might first think of 360 photos... That's why I'm not sure of 360-PHD would also be confusing.}
% % \onethousand{[Yiqian: Or `Learning Full-Head 3D GANs from a Single-View Portrait Dataset with Diverse Body Poses'? I will try to find a more appropriate name for the dataset. ]}\hbc{This seems clearer but you want to emphasize Diverse Camera Poses too?}
% %\onethousand{[TODO: 1. Find a more appropriate name for the dataset. 2. Find a better title to emphasize diverse body poses and camera poses. ]}
% %
% which consists of high-quality single-view real portraits annotated with a variety of camera parameters {(the yaw angles span the entire $360^{\circ}$ range)} and body poses.
% %
% We then propose \textit{3DPortraitGAN}, the first 3D-aware full-head portrait generator that learns a canonical 3D avatar distribution from the 
% %
% \textit{$\it{360}^{\circ}$PHQ} dataset with body pose self-learning. Our model can generate view-consistent portrait images from all camera angles
% %(${360}^{\circ}$)
% %\hbc{Previously 360 means the 360-degree yaw range. Do you mean the same thing here?}
% %\onethousand{[Yiqian: No. Strictly speaking, our dataset only contains the full range of yaw angles, but not the full range of pitch angles. However, this does not prevent our model from rendering from all camera angles. I removed the $\it{360}^{\circ}$ here so as not to cause ambiguity. ]}
% with a full-head 3D representation.
% %
% We incorporate a mesh-guided deformation field into volumetric rendering to produce deformed results and generate portrait images that conform to the body pose distribution of the dataset using our canonical generator.
% We integrate two pose predictors into our framework to predict more accurate body poses and thus address the issue of inaccurately estimated body poses in our dataset. 
% %
% Our experiments show that the proposed framework can accurately predict portrait body poses and generate view-consistent, realistic portrait images with complete geometry from all camera angles. 
  
% \end{abstract}
 