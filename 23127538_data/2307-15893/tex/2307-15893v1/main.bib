
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries


@inproceedings{pmlr-v15-chu11a,
  title = 	 {Contextual Bandits with Linear Payoff Functions},
  author = 	 {Chu, Wei and Li, Lihong and Reyzin, Lev and Schapire, Robert},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {208--214},
  year = 	 {2011},
  editor = 	 {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {11--13 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v15/chu11a/chu11a.pdf},
  url = 	 {https://proceedings.mlr.press/v15/chu11a.html},
  abstract = 	 {In this paper we study the contextual bandit problem (also known as the multi-armed bandit problem with expert advice) for linear payoff functions.  For $T$ rounds, $K$ actions, and d dimensional feature vectors, we prove an $O\left(\sqrt{Td\ln^3(KT\ln(T)/\delta)}\right)$ regret bound that holds with probability $1-\delta$ for the simplest known (both conceptually and computationally) efficient upper confidence bound algorithm for this problem.  We also prove a lower bound  of $\Omega(\sqrt{Td})$ for this setting, matching the upper bound up to logarithmic factors.}
}

@inproceedings{deshpande2012,
  author={Deshpande, Yash and Montanari, Andrea},
  booktitle={2012 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
  title={Linear bandits in high dimension and recommendation systems}, 
  year={2012},
  volume={},
  number={},
  pages={1750-1754},
  doi={10.1109/Allerton.2012.6483433}}

@inproceedings{wang2017,
author = {Wang, Huazheng and Wu, Qingyun and Wang, Hongning},
year = {2017},
month = {01},
pages = {},
title = {Factorization Bandits for Interactive Recommendation}
}

@inproceedings{Li_2010,
	doi = {10.1145/1772690.1772758},
  
	url = {https://doi.org/10.1145%2F1772690.1772758},
  
	year = 2010,
	month = {apr},
  
	publisher = {{ACM}
},
  
	author = {Lihong Li and Wei Chu and John Langford and Robert E. Schapire},
  
	title = {A contextual-bandit approach to personalized news article recommendation},
  
	booktitle = {Proceedings of the 19th international conference on World wide web}
}

@inproceedings{bigtable-osdi06,
title	= {Bigtable: A Distributed Storage System for Structured Data},
author	= {Fay Chang and Jeffrey Dean and Sanjay Ghemawat and Wilson C. Hsieh and Deborah A. Wallach and Mike Burrows and Tushar Chandra and Andrew Fikes and Robert E. Gruber},
year	= {2006},
booktitle	= {7th {USENIX} Symposium on Operating Systems Design and Implementation (OSDI)},
pages	= {205--218}
}

@inproceedings{delayfeedback-JoulaniGS13, title = 	 {Online Learning under Delayed Feedback}, author = 	 {Joulani, Pooria and Gyorgy, Andras and Szepesvari, Csaba}, booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning}, pages = 	 {1453--1461}, year = 	 {2013}, editor = 	 {Dasgupta, Sanjoy and McAllester, David}, volume = 	 {28}, number =       {3}, series = 	 {Proceedings of Machine Learning Research}, address = 	 {Atlanta, Georgia, USA}, month = 	 {17--19 Jun}, publisher =    {PMLR}, pdf = 	 {http://proceedings.mlr.press/v28/joulani13.pdf}, url = 	 {https://proceedings.mlr.press/v28/joulani13.html}, abstract = 	 {Online learning with delayed feedback has received increasing attention recently due to its several applications in distributed, web-based learning problems. In this paper we provide a systematic study of the topic, and analyze the effect of delay on the regret of online learning algorithms. Somewhat surprisingly, it turns out that delay increases the regret in a multiplicative way in adversarial problems, and in an additive way in stochastic problems. We give meta-algorithms that transform, in a black-box fashion, algorithms developed for the non-delayed case into ones that can handle the presence of delays in the feedback loop. Modifications of the well-known UCB algorithm are also developed for the bandit problem with delayed feedback, with the advantage over the meta-algorithms that they can be implemented with lower complexity.} }

@inproceedings{Song2022,
author = {Song, Yu and Sun, Shuai and Lian, Jianxun and Huang, Hong and Li, Yu and Jin, Hai and Xie, Xing},
title = {Show Me the Whole World: Towards Entire Item Space Exploration for Interactive Personalized Recommendations},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498459},
doi = {10.1145/3488560.3498459},
abstract = {User interest exploration is an important and challenging topic in recommender systems, which alleviates the closed-loop effects between recommendation models and user-item interactions.Contextual bandit (CB) algorithms strive to make a good trade-off between exploration and exploitation so that users' potential interests have chances to expose. However, classical CB algorithms can only be applied to a small, sampled item set (usually hundreds), which forces the typical applications in recommender systems limited to candidate post-ranking, homepage top item ranking, ad creative selection, or online model selection (A/B test). In this paper, we introduce two simple but effective hierarchical CB algorithms to make a classical CB model (such as LinUCB and Thompson Sampling) capable to explore users' interest in the entire item space without limiting to a small item set. We first construct a hierarchy item tree via a bottom-up clustering algorithm to organize items in a coarse-to-fine manner. Then we propose ahierarchical CB (HCB) algorithm to explore users' interest on the hierarchy tree. HCB takes the exploration problem as a series of decision-making processes, where the goal is to find a path from the root to a leaf node, and the feedback will be back-propagated to all the nodes in the path. We further propose aprogressive hierarchical CB (pHCB) algorithm, which progressively extends visible nodes which reach a confidence level for exploration, to avoid misleading actions on upper-level nodes in the sequential decision-making process. Extensive experiments on two public recommendation datasets demonstrate the effectiveness and flexibility of our methods.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {947–956},
numpages = {10},
keywords = {contextual bandit, interest exploration, recommender system},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@misc{liu2022monolith,
      title={Monolith: Real Time Recommendation System With Collisionless Embedding Table}, 
      author={Zhuoran Liu and Leqi Zou and Xuan Zou and Caihua Wang and Biao Zhang and Da Tang and Bolin Zhu and Yijie Zhu and Peng Wu and Ke Wang and Youlong Cheng},
      year={2022},
      eprint={2209.07663},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{chandramouli2011streamrec,
author = {Chandramouli, Badrish and Levandoski, Justin and Eldawy, Ahmed and Mokbel, Mohamed F.},
title = {StreamRec: A Real-Time Recommender System},
booktitle = {ACM SIGMOD International Conference on Management of Data (SIGMOD 2011)},
year = {2011},
month = {June},
abstract = {This demonstration proposes StreamRec, a novel approach to building recommender systems that leverages a stream processing system capable of handling an end-to-end recommendation process in order to produce real-time recommendations. We demonstrate several popular collaborative filtering recommendation methods within StreamRec by providing an application scenario that uses StreamRec as the underlying recommendation engine.},
publisher = {ACM SIGMOD},
url = {https://www.microsoft.com/en-us/research/publication/streamrec-a-real-time-recommender-system/},
edition = {ACM SIGMOD International Conference on Management of Data (SIGMOD 2011)},
}

@inproceedings{Chaney2018,
author = {Chaney, Allison J. B. and Stewart, Brandon M. and Engelhardt, Barbara E.},
title = {How Algorithmic Confounding in Recommendation Systems Increases Homogeneity and Decreases Utility},
year = {2018},
isbn = {9781450359016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240323.3240370},
doi = {10.1145/3240323.3240370},
abstract = {Recommendation systems are ubiquitous and impact many domains; they have the potential to influence product consumption, individuals' perceptions of the world, and life-altering decisions. These systems are often evaluated or trained with data from users already exposed to algorithmic recommendations; this creates a pernicious feedback loop. Using simulations, we demonstrate how using data confounded in this way homogenizes user behavior without increasing utility.},
booktitle = {Proceedings of the 12th ACM Conference on Recommender Systems},
pages = {224–232},
numpages = {9},
keywords = {algorithmic confounding, recommendation systems},
location = {Vancouver, British Columbia, Canada},
series = {RecSys '18}
}

@article{auer2002,
author = {Auer, Peter and Cesa-Bianchi, Nicol\`{o} and Fischer, Paul},
title = {Finite-Time Analysis of the Multiarmed Bandit Problem},
year = {2002},
issue_date = {May-June 2002},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {47},
number = {2–3},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1013689704352},
doi = {10.1023/A:1013689704352},
abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
journal = {Mach. Learn.},
month = {may},
pages = {235–256},
numpages = {22},
keywords = {finite horizon regret, bandit problems, adaptive allocation rules}
}

@inproceedings{Chapelle2011,
author = {Chapelle, Olivier and Li, Lihong},
title = {An Empirical Evaluation of Thompson Sampling},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off, but it is surprisingly unpopular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2249–2257},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@InProceedings{Jeremie2015,
author="Mary, J{\'e}r{\'e}mie
and Gaudel, Romaric
and Preux, Philippe",
editor="Pardalos, Panos
and Pavone, Mario
and Farinella, Giovanni Maria
and Cutello, Vincenzo",
title="Bandits and Recommender Systems",
booktitle="Machine Learning, Optimization, and Big Data",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="325--336",
abstract="This paper addresses the on-line recommendation problem facing new users and new items; we assume that no information is available neither about users, nor about the items. The only source of information is a set of ratings given by users to some items. By on-line, we mean that the set of users, and the set of items, and the set of ratings is evolving along time and that at any moment, the recommendation system has to select items to recommend based on the currently available information, that is basically the sequence of past events. We also mean that each user comes with her preferences which may evolve along short and longer scales of time; so we have to continuously update their preferences. When the set of ratings is the only available source of information, the traditional approach is matrix factorization. In a decision making under uncertainty setting, actions should be selected to balance exploration with exploitation; this is best modeled as a bandit problem. Matrix factors provide a latent representation of users and items. These representations may then be used as contextual information by the bandit algorithm to select items. This last point is exactly the originality of this paper: the combination of matrix factorization and bandit algorithms to solve the on-line recommendation problem. Our work is driven by considering the recommendation problem as a feedback controlled loop. This leads to interactions between the representation learning, and the recommendation policy.",
isbn="978-3-319-27926-8"
}

@inproceedings{paul2016,
title	= {Deep Neural Networks for YouTube Recommendations},
author	= {Paul Covington and Jay Adams and Emre Sargin},
year	= {2016},
booktitle	= {Proceedings of the 10th ACM Conference on Recommender Systems},
address	= {New York, NY, USA}
}

@inproceedings{Okura2017EmbeddingbasedNR,
author = {Okura, Shumpei and Tagami, Yukihiro and Ono, Shingo and Tajima, Akira},
title = {Embedding-Based News Recommendation for Millions of Users},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1933–1942},
numpages = {10},
keywords = {news recommendations, large-scale services, distributed representations, neural networks},
location = {Halifax, NS, Canada},
series = {KDD ’17}
}

@inproceedings{Liu2017,
author = {Liu, David C. and Rogers, Stephanie and Shiau, Raymond and Kislyuk, Dmitry and Ma, Kevin C. and Zhong, Zhigang and Liu, Jenny and Jing, Yushi},
title = {Related Pins at Pinterest: The Evolution of a Real-World Recommender System},
year = {2017},
booktitle = {WWW 2017},
}

@inproceedings{Zhai2017,
author = {Zhai, Andrew and Kislyuk, Dmitry and Jing, Yushi and Feng, Michael and Tzeng, Eric and Donahue, Jeff and Du, Yue Li and Darrell, Trevor},
title = {Visual Discovery at Pinterest},
year = {2017},
booktitle = {WWW 2017},
}

@inproceedings{yi2019samplingbias,
author = {Yi, Xinyang and Yang, Ji and Hong, Lichan and Cheng, Derek Zhiyuan and Heldt, Lukasz and Kumthekar, Aditee and Zhao, Zhe and Wei, Li and Chi, Ed},
title = {Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations},
year = {2019},
booktitle = {RecSys 2019},
}


@inproceedings{yang2020,
author = {Yang, Ji and Yi, Xinyang and Zhiyuan Cheng, Derek and Hong, Lichan and Li, Yang and Xiaoming Wang, Simon and Xu, Taibai and Chi, Ed H.},
title = {Mixed Negative Sampling for Learning Two-Tower Neural Networks in Recommendations},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3386195},
doi = {10.1145/3366424.3386195},
abstract = {Learning query and item representations is important for building large scale recommendation systems. In many real applications where there is a huge catalog of items to recommend, the problem of efficiently retrieving top k items given user’s query from deep corpus leads to a family of factorized modeling approaches where queries and items are jointly embedded into a low-dimensional space. In this paper, we first showcase how to apply a two-tower neural network framework, which is also known as dual encoder in the natural language community, to improve a large-scale, production app recommendation system. Furthermore, we offer a novel negative sampling approach called Mixed Negative Sampling (MNS). In particular, different from commonly used batch or unigram sampling methods, MNS uses a mixture of batch and uniformly sampled negatives to tackle the selection bias of implicit user feedback. We conduct extensive offline experiments using large-scale production dataset and show that MNS outperforms other baseline sampling methods. We also conduct online A/B testing and demonstrate that the two-tower retrieval model based on MNS significantly improves retrieval quality by encouraging more high-quality app installs.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {441–447},
numpages = {7},
keywords = {Extreme Classification, Neural Networks, Information Retrieval, Context-aware Recommender Systems},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{Yao2021,
author = {Yao, Tiansheng and Yi, Xinyang and Cheng, Derek Zhiyuan and Yu, Felix and Chen, Ting and Menon, Aditya and Hong, Lichan and Chi, Ed H. and Tjoa, Steve and Kang, Jieqi (Jay) and Ettinger, Evan},
title = {Self-Supervised Learning for Large-Scale Item Recommendations},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481952},
doi = {10.1145/3459637.3481952},
abstract = {Large scale recommender models find most relevant items from huge catalogs, and they play a critical role in modern search and recommendation systems. To model the input space with large-vocab categorical features, a typical recommender model learns a joint embedding space through neural networks for both queries and items from user feedback data. However, with millions to billions of items in the corpus, users tend to provide feedback for a very small set of them, causing a power-law distribution. This makes the feedback data for long-tail items extremely sparse. Inspired by the recent success in self-supervised representation learning research in both computer vision and natural language understanding, we propose a multi-task self-supervised learning (SSL) framework for large-scale item recommendations. The framework is designed to tackle the label sparsity problem by learning better latent relationship of item features. Specifically, SSL improves item representation learning as well as serving as additional regularization to improve generalization. Furthermore, we propose a novel data augmentation method that utilizes feature correlations within the proposed framework.We evaluate our framework using two real-world datasets with 500M and 1B training examples respectively. Our results demonstrate the effectiveness of SSL regularization and show its superior performance over the state-of-the-art regularization techniques. We also have already launched the proposed techniques to a web-scale commercial app-to-app recommendation system, with significant improvements top-tier business metrics demonstrated in A/B experiments on live traffic. Our online results also verify our hypothesis that our framework indeed improves model performance even more on slices that lack supervision.},
booktitle = {Proceedings of the 30th ACM International Conference on Information and Knowledge Management},
pages = {4321–4330},
numpages = {10},
keywords = {neural networks, self-supervised learning, contrastive learning, recommender systems},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{pre-training,
title={Pre-training Tasks for Embedding-based Large-scale Retrieval},
author={Wei-Cheng Chang and Felix X. Yu and Yin-Wen Chang and Yiming Yang and Sanjiv Kumar},
booktitle={ICLR 2020},
year = {2020},
}

@inproceedings{zhe19watchnext,
author = {Zhao, Zhe and Hong, Lichan and Wei, Li and Chen, Jilin and Nath, Aniruddh and Andrews, Shawn and Kumthekar, Aditee and Sathiamoorthy, Maheswaran and Yi, Xinyang and Chi, Ed},
title = {Recommending What Video to Watch next: A Multitask Ranking System},
booktitle = {RecSys 2019},
year = {2019},
}

@inproceedings{heng16,
author = {Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and Anil, Rohan and Haque, Zakaria and Hong, Lichan and Jain, Vihan and Liu, Xiaobing and Shah, Hemal},
title = {Wide \& Deep Learning for Recommender Systems},
series = {DLRS 2016},
year = {2016},
}

@inproceedings{Gao2021,
author = {Gao, Weihao and Fan, Xiangjun and Wang, Chong and Sun, Jiankai and Jia, Kai and Xiao, Wenzi and Ding, Ruofan and Bin, Xingyan and Yang, Hui and Liu, Xiaobing},
title = {Learning An End-to-End Structure for Retrieval in Large-Scale Recommendations},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482362},
doi = {10.1145/3459637.3482362},
abstract = {One of the core problems in large-scale recommendations is to retrieve top relevant candidates accurately and efficiently, preferably in sub-linear time. Previous approaches are mostly based on a two-step procedure: first learn an inner-product model, and then use some approximate nearest neighbor (ANN) search algorithm to find top candidates. In this paper, we present Deep Retrieval (DR), to learn a retrievable structure directly with user-item interaction data (e.g. clicks) without resorting to the Euclidean space assumption in ANN algorithms. DR's structure encodes all candidate items into a discrete latent space. Those latent codes for the candidates are model parameters and learnt together with other neural network parameters to maximize the same objective function. With the model learnt, a beam search over the structure is performed to retrieve the top candidates for reranking. Empirically, we first demonstrate that DR, with sub-linear computational complexity, can achieve almost the same accuracy as the brute-force baseline on two public datasets. Moreover, we show that, in a live production recommendation system, a deployed DR approach significantly outperforms a well-tuned ANN baseline in terms of engagement metrics. To the best of our knowledge, DR is among the first non-ANN algorithms successfully deployed at the scale of hundreds of millions of items for industrial recommendation systems.},
booktitle = {Proceedings of the 30th ACM International Conference on Information and Knowledge Management},
pages = {524–533},
numpages = {10},
keywords = {recommendation systems, deep retrieval},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{scam_paper,
title	= {Accelerating Large-Scale Inference with Anisotropic Vector Quantization},
author	= {Ruiqi Guo and Philip Sun and Erik Lindgren and Quan Geng and David Simcha and Felix Chern and Sanjiv Kumar},
year	= {2020},
URL	= {https://arxiv.org/abs/1908.10396}
}

@inproceedings{Chen2022,
author = {Chen, Minmin and Xu, Can and Gatto, Vince and Jain, Devanshu and Kumar, Aviral and Chi, Ed},
title = {Off-Policy Actor-Critic for Recommender Systems},
year = {2022},
isbn = {9781450392785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523227.3546758},
doi = {10.1145/3523227.3546758},
booktitle = {Proceedings of the 16th ACM Conference on Recommender Systems},
pages = {338–349},
numpages = {12},
keywords = {recommender systems, pessimism, reinforcement learning, off-policy actor-critic, batch RL},
location = {Seattle, WA, USA},
series = {RecSys '22}
}

@inproceedings{Gilotte2018,
author = {Gilotte, Alexandre and Calauz\`{e}nes, Cl\'{e}ment and Nedelec, Thomas and Abraham, Alexandre and Doll\'{e}, Simon},
title = {Offline A/B Testing for Recommender Systems},
year = {2018},
isbn = {9781450355810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159652.3159687},
doi = {10.1145/3159652.3159687},
abstract = {Online A/B testing evaluates the impact of a new technology by running it in a real production environment and testing its performance on a subset of the users of the platform. It is a well-known practice to run a preliminary offline evaluation on historical data to iterate faster on new ideas, and to detect poor policies in order to avoid losing money or breaking the system. For such offline evaluations, we are interested in methods that can compute offline an estimate of the potential uplift of performance generated by a new technology. Offline performance can be measured using estimators known as counterfactual or off-policy estimators. Traditional counterfactual estimators, such as capped importance sampling or normalised importance sampling, exhibit unsatisfying bias-variance compromises when experimenting on personalized product recommendation systems. To overcome this issue, we model the bias incurred by these estimators rather than bound it in the worst case, which leads us to propose a new counterfactual estimator. We provide a benchmark of the different estimators showing their correlation with business metrics observed by running online A/B tests on a large-scale commercial recommender system.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
pages = {198–206},
numpages = {9},
keywords = {importance sampling., off-policy evaluation, recommender system, counterfactual estimation},
location = {Marina Del Rey, CA, USA},
series = {WSDM '18}
}

@inproceedings{Thorsten2017,
author = {Joachims, Thorsten and Swaminathan, Adith and Schnabel, Tobias},
title = {Unbiased Learning-to-Rank with Biased Feedback},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3018699},
doi = {10.1145/3018661.3018699},
abstract = {Implicit feedback (e.g., clicks, dwell times, etc.) is an abundant source of data in human-interactive systems. While implicit feedback has many advantages (e.g., it is inexpensive to collect, user centric, and timely), its inherent biases are a key obstacle to its effective use. For example, position bias in search rankings strongly influences how many clicks a result receives, so that directly using click data as a training signal in Learning-to-Rank (LTR) methods yields sub-optimal results. To overcome this bias problem, we present a counterfactual inference framework that provides the theoretical basis for unbiased LTR via Empirical Risk Minimization despite biased data. Using this framework, we derive a Propensity-Weighted Ranking SVM for discriminative learning from implicit feedback, where click models take the role of the propensity estimator. In contrast to most conventional approaches to de-biasing the data using click models, this allows training of ranking functions even in settings where queries do not repeat. Beyond the theoretical support, we show empirically that the proposed learning method is highly effective in dealing with biases, that it is robust to noise and propensity model misspecification, and that it scales efficiently. We also demonstrate the real-world applicability of our approach on an operational search engine, where it substantially improves retrieval performance.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {781–789},
numpages = {9},
keywords = {implicit feedback, propensity weighting, click models, learning to rank, ranking svm},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inproceedings{Adith2015,
author = {Swaminathan, Adith and Joachims, Thorsten},
title = {The Self-Normalized Estimator for Counterfactual Learning},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback (BLBF), and proposes the use of an alternative estimator that avoids this problem. In the BLBF setting, the learner does not receive full-information feedback like in supervised learning, but observes feedback only for the actions taken by a historical policy. This makes BLBF algorithms particularly attractive for training online systems (e.g., ad placement, web search, recommendation) using their historical logs. The Counterfactual Risk Minimization (CRM) principle [1] offers a general recipe for designing BLBF algorithms. It requires a counterfactual risk estimator, and virtually all existing works on BLBF have focused on a particular unbiased estimator. We show that this conventional estimator suffers from a propensity overfitting problem when used for learning over complex hypothesis spaces. We propose to replace the risk estimator with a self-normalized estimator, showing that it neatly avoids this problem. This naturally gives rise to a new learning algorithm - Normalized Policy Optimizer for Exponential Models (Norm-POEM) - for structured output prediction using linear rules. We evaluate the empirical effectiveness of Norm-POEM on several multi-label classification problems, finding that it consistently outperforms the conventional estimator.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3231–3239},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{Philip2016,
author = {Thomas, Philip S. and Brunskill, Emma},
title = {Data-Efficient off-Policy Policy Evaluation for Reinforcement Learning},
year = {2016},
publisher = {JMLR.org},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {2139–2148},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{Ma2020,
author = {Ma, Jiaqi and Zhao, Zhe and Yi, Xinyang and Yang, Ji and Chen, Minmin and Tang, Jiaxi and Hong, Lichan and Chi, Ed H.},
title = {Off-Policy Learning in Two-Stage Recommender Systems},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380130},
doi = {10.1145/3366423.3380130},
abstract = {Many real-world recommender systems need to be highly scalable: matching millions of items with billions of users, with milliseconds latency. The scalability requirement has led to widely used two-stage recommender systems, consisting of efficient candidate generation model(s) in the first stage and a more powerful ranking model in the second stage. Logged user feedback, e.g., user clicks or dwell time, are often used to build both candidate generation and ranking models for recommender systems. While it’s easy to collect large amount of such data, they are inherently biased because the feedback can only be observed on items recommended by the previous systems. Recently, off-policy correction on such biases have attracted increasing interest in the field of recommender system research. However, most existing work either assumed that the recommender system is a single-stage system or only studied how to apply off-policy correction to the candidate generation stage of the system without explicitly considering the interactions between the two stages. In this work, we propose a two-stage off-policy policy gradient method, and showcase that ignoring the interaction between the two stages leads to a sub-optimal policy in two-stage recommender systems. The proposed method explicitly takes into account the ranking model when training the candidate generation model, which helps improve the performance of the whole system. We conduct experiments on real-world datasets with large item space and demonstrate the effectiveness of our proposed method.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {463–473},
numpages = {11},
keywords = {Off-policy Learning, Neural Networks, Two-stage Systems, Recommender Systems},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{Chen2021,
author = {Chen, Minmin},
title = {Exploration in Recommender Systems},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3474601},
doi = {10.1145/3460231.3474601},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {551–553},
numpages = {3},
keywords = {online learning, reinforcement learning, uncertainty, recommender systems, bandits, serendipity, exploration},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}


