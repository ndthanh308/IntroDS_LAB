\section{Introduction}

Recommender systems have become indispensable for users to explore and access content in the era of information overload. Matching users with items to fulfill their information need by leveraging user and item data is a fundamental task in recommender systems.
Compared to other machine learning domains such as language and vision where data is static,  recommender systems are distinguished by dynamic user-system interactions that can give rise to a feedback loop, because new system policies are trained primarily based on data generated from users' past interactions. This feedback loop can further lead to the \textsl{rich gets richer} problem, i.e., future recommendations may excessively prioritize items with high engagement in the past, thus emphasizing the need for exploration. Furthermore, real-world applications must be capable of processing substantial amounts of fresh content. For instance, millions of new videos are uploaded to YouTube on a daily basis. As users demand up-to-date information, it is important for recommender systems to explore their preferences on new items and promptly incorporate their feedback into the decision-making processes. 

In recent years, deep learning-based models have emerged as a dominant class of approaches for building recommender systems. Notably, the majority of such models still rely on supervised learning from users' past interactions in \textsl{batch} mode, and their ability to explore and adapt in real-time is limited, if at all. In recent years, a line of work (e.g.~\cite{Gilotte2018, Adith2015, Philip2016, Chen2022}) applied offline reinforcement learning (RL) for deep learning-based models to address system bias in the training data. Nonetheless, such offline RL approaches suffer from large latency in model updates, since they still rely on learning from logged user feedback in the \textsl{batch} training framework, rather than through real-time interactions with the environment. As a result, offline RL is typically inadequate to explore new items in a timely and efficient manner, or to respond quickly to system changes such as shifts in user preferences.

In this paper, we focus on building a system with the capabilities of real-time learning and exploration. While online learning (e.g., multi-armed bandits) is well suited for the aforementioned considerations, the applications of it to large-scale real-world systems are much less explored compared to batch modeling. Practically, the challenges are mainly three-folds:

\begin{itemize}
\item Large exploration space. The exploration space typically grows linearly with the number of candidate items in the system, which poses a significant challenge for most real-world systems that need to retrieve from millions of items. In the absence of an appropriately designed space pruning technique, exploration can come at a high cost to the user experience.

\item Limited exploration traffic. Compared to traditional recommender systems that focus on \textsl{exploiting} known learnings, exploration by its nature can lead to a short-term user experience degradation/regret. Therefore it is crucial to perform online exploration in a \textsl{cost-efficient} manner, typically within a limited budget such as 1\% of randomly shuffled daily user traffic.

\item Lack of real-time learning systems. Existing systems are mainly designed for training models in batch mode and serve them online with fixed parameters. It is certainly non-trivial to build a custom highly scalable system for real-time learning with large data throughput.
\end{itemize}


This paper aims to tackle these challenges by introducing a real-time bandit system for item exploration by learning users' direct feedback on items in a closed loop. The proposed system, called Online Matching, adopts a hybrid \textsl{offline and online} learning approach in order to make the exploration system efficient, scalable, and responsive.  The offline learning component of Online Matching is designed for pruning the large exploration space and enabling cost-efficient exploration. 
In particular, we train a two-tower neural network model to co-embed users and items into the same space, similar to some existing neural retrieval systems such as \cite{yi2019samplingbias}. Then we discretize the embedding space into a specified number of ``user clusters'' using off-the-shelf embedding clustering approaches. Finally, we build a sparse bipartite graph between user clusters and the items to be explored based on their embedding similarities.
The key insight is that for a particular user cluster, items nearby in the embedding space can be good candidates to explore.
Note that all the aforementioned steps are performed offline. User cluster-item edges in the sparse bipartite graph essentially encode most promising user cluster-item pairs to explore. The online learning component of Online Matching is based on our novel extension of the classic LinUCB algorithm \cite{pmlr-v15-chu11a}, called Diag-LinUCB, with a user's real-time distribution over all the user clusters as the \textsl{context}.
With this novel algorithmic design, our bandit parameters simply correspond to user cluster-item edges in the sparse bipartite graph and can be updated in real-time in a fully distributed manner. The resulting online system boasts a very low policy update latency, which refers to the amount of time it takes from user interaction to incorporating the feedback in the decision-making process.

Online Matching has been successfully deployed to YouTube for the use cases of fresh content discovery and item exploration. In the first case, our goal is to quickly identify high-quality fresh items through exploration with small traffic, and amplify their impact through exploitation with major traffic. For item exploration, we aim to substantially increase discoverable corpus by leveraging large exploration traffic while having minimum regret on user experience. We provide the experimental frameworks for running these two experiments, and demonstrate the effectiveness of Online Matching through improved topline metrics in these two cases.

In a nutshell, our contributions are:
\begin{itemize}
\item \textbf{Algorithmic methods.} We provide a novel algorithmic framework that combines offline batch learning and online learning for building large-scale real-time bandit systems. Particularly, we propose Diag-LinUCB, a novel bandit algorithm that enables distributed bandit updates, allowing the system to scale and serve billions of users and millions of items.
\item \textbf{System design.} We present a complete system design, starting from the offline learning pipeline for creating a sparse bipartite graph, to the real-time bandit parameters aggregation performed by an online agent.
\item \textbf{Experiment design.} We study two use cases of Online Matching -- \textsl{Fresh Content Discovery} (Type-I) and \textsl{Corpus Exploration} (Type-II), and present the experiment frameworks for impact measurements in both cases. This includes the application of a user-corpus partition framework to measure the growth of the discoverable item corpus.
\item \textbf{Live experiments.} We conduct live experiments for both Type-I and Type-II use cases in YouTube with significant top-line metric gains. We demonstrate the value of Online Matching in improving user experiences through discovering high-quality fresh videos and growing discoverable item corpus.
\end{itemize}