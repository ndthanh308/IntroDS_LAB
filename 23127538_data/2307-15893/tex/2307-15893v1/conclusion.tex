\section{Conclusion}

In this paper, we introduced Online Matching, a large-scale real-time bandit system for item exploration and recommendation.
To scale up to serving billions of users and millions of items, we proposed an algorithm-system co-design that combines offline and online learning, introduces a novel Diagonal LinUCB algorithm, and devises a high-throughput online agent system. Empirically, we applied Online Matching to two important use cases in YouTube and demonstrated its effectiveness for increasing fresh content adoption and growing discoverable corpus.


We hope our exercise of building a real-world bandit system can provide some valuable insights for future endeavors to address practical challenges. For example, we used a mixture of exploration and exploitation modes for our Fresh Content Discovery use case. In contrast to the classic bandit setup where minimizing regret in a single mode is the primary goal, more research needs to be done to study the mixture setup. In addition, real-world applications often have to deal with a dynamic exploration corpus, where items are being constantly added or removed in a streaming fashion. Designing principled bandit algorithms for dynamic arms remains an interesting open research problem.