\section{Experiment Results}
We conduct our experiments on one major recommendation surface of YouTube -- one of the world’s largest video content discovery, creation and sharing platforms. Similar to many industry-scale recommender systems, the recommendation surface we considered consists of multiple stages including candidate generation and a multi-task ranking system \cite{zhe19watchnext}. Given a user and a corresponding context, the candidate generation stage employs multiple video retrieval systems to generate a few hundred/thousand candidate videos with a primary focus of optimizing the recall performance. The ranking stage then takes the set of retrieved videos as input and generates the final ranking. In the following two use cases, Online Matching is added as a new retrieval source to the candidate generation stage.

\subsection{Use Cases}
We primarily consider two use cases of Online Matching:
\begin{itemize}
\item \textsl{Fresh Content Discovery (Type-I).} Content creators upload millions of videos to Youtube on a daily basis. These videos vary a lot in quality and traditional batch-learning based recommender systems are limited in their capability to identify high-quality fresh videos and recommend them to the appropriate audience in a timely manner.
Intuitively, an efficient exploration system on fresh content can supplement traditional recommender systems by identifying and leveraging quality fresh content to improve user experience with satisfied engagement, particularly with fresh content. To compensate the quality loss from exploration, the idea is to use a very small traffic for exploration, and conduct exploitation (i.e., amplifying high-quality candidates) for the rest of traffic.
\item \textsl{Corpus Exploration (Type-II).} Real-world recommender systems typically have a long-tail distribution over the recommendation corpus. Traditional recommender systems are primarily exploitation-based due to a feedback loop that reinforces the existing ``winners'' in the corpus. Exploring and leveraging the long-tail portion of the corpus is both challenging given its vast size and rewarding. In this case, the goal is to grow the \textsl{discoverable corpus}. Having a larger discoverable corpus is argued to bring long-term value to recommenders, e.g., through reducing system uncertainty on sparse data region \cite{Chen2021}. It can also provide torso or long-tailed content creators more opportunities to showcase their content to a wider audience. However, similar to Type-I, explicitly exploring long-tail and fresh items can lead to short-term loss of user engagement and satisfaction. Therefore, the goal of this use case is to enlarge discoverable corpus while having minimal regret on short-term engagement.
\end{itemize}

Online Matching is a fitting solution for the above use cases thanks to its efficiency in both the exploration algorithm and system. On one hand, Online Matching trims the exploration space through offline learning, making it possible to only use small traffic for exploration in the first use case. On the other hand, the real-time property of Online Matching system helps to minimize exploration errors, so that it can effectively enlarge the discoverable corpus while maintaining a small regret on short-term engagement in the second use case.



\subsection{Fresh Content Discovery}
\subsubsection{Experiment Setup.}

We select up to $O(1M)$ fresh videos that were uploaded within the past certain days (referred as X days in the rest of this paper) as the exploration corpus. Multiple offline and online filters are applied to make sure the explored videos are safe and satisfy minimum quality requirements. We use a small size of frequently shuffled user traffic ($\leq 2\%$) to explore this corpus and learn the bandit parameters. In the exploration mode, for getting unbiased user feedback, the picked candidate from Online Matching bypasses the ranking layer and is shown at a fixed position in the UI. We call one experiment running in this exploration mode as \textsl{one exploration slot}. The reward we use is a combination of multiple signals representing user's happiness with the recommended videos. If any video is filtered during exploration, the filtering information is also used as part of the reward in bandits. It's worth noting that, besides hard filtering, the sparse graph construction can also control candidate quality through the offline learnt embeddings.

To amplify the learnt good fresh videos to all users, we also set up another online agent that works in an ``exploitation'' mode for the rest of traffic (98-99\%). Particularly, it reuses the corpus and bandit parameters from the aforementioned online agent and ranks videos only based on the estimated reward in Eq. \eqref{eq:diag-exploit}. In exploitation, since there is no need to collect user feedback, multiple top candidates by mean reward are passed to the ranking layer together with candidates from other sources. Exploration corpus is being updated on a rolling basis -– new eligible videos are continuously injected to the exploration corpus and videos uploaded beyond X days continuously graduate from the corpus.

\textbf{Top-k randomization.} Rather than getting feedback instantly in the theoretical bandit model, our system still has a nontrivial policy update latency. This latency can cause the system to overly explore certain items before collecting their reward. To address this issue, we introduce a randomization mechanism to uniformly sample a video from the top $k$ videos measured by UCB. We set $k = 5$ in the following experiments.

\subsubsection{Results.}
We ran an online user-diverted A/B testing for a week to understand the effectiveness of our \textsl{explore-and-amplify} framework.
For both exploration and exploitation modes, the control is the production recommender system without adding Online Matching agent as an additional candidate generator.

\textbf{Gains from exploitation}. The results of Online Matching exploitation mode are shown in Table \ref{tbl:freshcontentdiscovery}. We use the setup where all user clusters are treated equally as the baseline to show the value of our user context modeling. We report two Diag-LinUCB arms where the second one uses a larger corpus and more clusters. Note that with the larger graph, we had to increase exploration traffic from 1\% to 2\% to avoid under exploration. Overall, we see improved topline satisfied engagement metric from Online Matching and significant gains on the fresh item slice.
To study long-term impact, we have run a holdback for several weeks. As demonstrated in Fig. \ref{fig:dau} shows, there is a +0.04\% improvement on daily active users, a long-term metric that is much more difficult to improve than short-term user engagement.

\begin{table*}
  \begin{tabular}{ |c|c|c|c|c|c|c| }
    \hline
     & \shortstack{Satisfied \\ user engagement} & \shortstack{Engagement with \\ fresh content} & \# clusters & Fresh corpus size & \shortstack{Graph size \\ (\# edges)}\\
    \hline
    Equal-weight Bandit & +0.03\% & +3.61\% & 15k & 1x & $\sim$ 4M \\ \hline
    Diag-LinUCB         &+0.08\%  & +5.25\% & 15k & 1x & $\sim$ 4M \\ \hline
    Diag-LinUCB (Larger Graph)  &+0.15\%  &+8.33\% & 30k & 3x & $\sim$ 20M \\ \hline  
  \end{tabular}
  \captionof{table}{Fresh Content Discovery exploitation A/B testing results. Improvements over the production system without Online Matching are reported. The first two arms used $1\%$ traffic for exploration, and the 3rd arm used $2\%$ traffic for exploration due to graph being larger.}
  \label{tbl:freshcontentdiscovery}
\end{table*}

% Figure environment removed

\textbf{Cost of exploration}. We observed -0.16\% and -0.19\% satisfied user engagement loss from the 1\% and 2\% exploration slots for the 2nd and 3rd rows in Table \ref{tbl:freshcontentdiscovery}. By discounting the engagement loss with the small traffic proportion, it is clear that the value added by exploitation far outweighs the cost of exploration.



% Figure environment removed
\subsection{Corpus Exploration}
\subsubsection{Experiment Setup}
In the traditional A/B testing setup, user traffic is randomly assigned to a control and treatment group. This user-diverted setup can enable measuring user-side metric changes. In this use case, we want to measure the growth of discoverable corpus. The user-diverted experiment is not an appropriate method for measuring corpus changes because the control and treatment group share the same corpus. As a result, any treatment effect on the corpus can be leaked to the control group. In order to correctly measure corpus changes, we employ a user-corpus co-diverted setup where we partition the entire corpus (e.g., by hashing item id) into multiple disjoint slices and expose each slice to a faction of user traffic in one experiment.

To largely explore the long-tailed proportion of our corpus, we curate a large corpus of fresh videos,  that is $O(10M)$ videos uploaded within X days. We divide the entire corpus into 10 slices and one slice in each experiment that has $6\%$ user traffic. Type-II experiment shares many similar configurations such as the two-tower model and number of clusters as Type-I, except that the exploration corpus is much larger
and the online agent only runs in the \textsl{exploration} mode in this use case.

\subsubsection{Experiment Results}


We compare daily unique videos greater than different impression threshold values to measure the discoverable corpus sizes across control and treatment groups, each of which contains 1/10 of the entire fresh corpus.
Fig. \ref{fig:duiv} shows the relative corpus size improvement across multiple impression values. Note that we can naively boost the impressions of more unique videos regardless of their performance. But overly showing low-quality videos to users can cause very negative user experience. Overall, we only observed -0.05\% topline user engagement loss with corpus size gain from Fig. \ref{fig:duiv}. Comparing to Type-I, here the cost of the exploration is much lower. 

This is expected because in one exploration slot, Type-II has a slightly smaller exploration corpus than Type-I, but its exploration user traffic is significantly larger ($6\%$ vs $2\%$). Intuitively, this means that in Type-II bandits can discover high-quality items more quickly.