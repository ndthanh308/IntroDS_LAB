\section{Related Work}

\subsection{Neural Recommenders and Off-policy Learning}
Neural modeling has emerged as the dominant approach for constructing large-scale recommender systems in a range of industry applications, including video discovery \cite{paul2016}, news recommendations \cite{Okura2017EmbeddingbasedNR}, and social networks \cite{Liu2017, Zhai2017}. These models cover a broad range of tasks, from retrieval \cite{yi2019samplingbias} to ranking \cite{heng16, zhe19watchnext}.
The main focus of this paper is on the retrieval problem, where the goal is to find a few related items from a large item corpus. This problem is also known as \textsl{deep retrieval} and has been extensively studied in the past. For example, a line of work \cite{yi2019samplingbias, yang2020, pre-training, Yao2021} studied two-tower models for learning user and item representations in the same embedding space, allowing converting the problem of retrieval to maximum-inner-product-search (MIPS) \cite{scam_paper} with sub-linear complexity. Compared to matrix factorization and extreme classification models \cite{paul2016}, two-tower architecture can leverage content features of items, making it more suitable for generating reasonable representations of fresh items with little user engagement. Although a large amount of training data is often available in real-world applications, neural models (including aforementioned two-tower models) are typically trained offline on logged user feedback in a supervised fashion, making them biased toward existing recommendation policies and hard to promptly adapt to system changes.
In this paper, we propose an efficient exploration space pruning strategy, using two-tower models as a key part of our offline learning framework. Benefiting from the generalization capabilities of two-tower models, our strategy makes online learning feasible under strict latency and traffic constraint. 

To alleviate the aforementioned bias problem, there has been a body of work bringing offline reinforcement learning (RL) techniques to neural recommender systems. The idea of off-policy learning is to estimate the value of a target policy or to train it using data collected by a different behavior policy. A series of work (see e.g. \cite{Gilotte2018, Adith2015, Philip2016}) focused on developing off-policy estimators through inverse-propensity weighting. For target policy learning, Chen et al. \cite{Chen2022} applied off-policy correction to training a neural sequence model for video retrieval. There are also various papers (see e.g. \cite{Thorsten2017, zhe19watchnext}) studying learning unbiased ranking models from biased logged feedback. More recently, Ma et al. \cite{Ma2020} proposed using off-policy learning for two-stage recommender systems. One challenge in off-policy learning is that data collected from the existing behavior policy might not accurately represent the target policy. This is especially true for fresh items with minimal or no engagement data, where the inverse propensity weighting can have a high variance.
Offline RL, like traditional batch trained neural recommenders, is also subject to significant delays in data logging, model training, and deployment, as it is still based on the batch training paradigm.

\subsection{Online Learning and Exploration}
Online learning provides a useful framework for building recommenders that are adaptable to user feedback. One commonly used type of online learning algorithm is bandit algorithms, such as UCB \cite{auer2002}, Thompson Sampling \cite{Chapelle2011}, and LinUCB \cite{pmlr-v15-chu11a}. These algorithms are designed to balance the exploration of new options with the exploitation of known good options, allowing the system to learn and adapt to user behavior over time.
The application of bandits to recommenders dates back to the seminal work by Li et al. \cite{Li_2010} on using contextual bandits for news recommendation. The work in \cite{Chapelle2011} provided an evaluation of Thompson Sampling on real-world datasets.  Jeremie et al. \cite{Jeremie2015} studied bridging matrix factorization and bandits to solve the cold-start problem in recommenders. Besides the use of bandits for exploring new items, more recently, Song et al. \cite{Song2022} proposed creating a hierarchical representation of item space to help explore new user interests. To the best of our knowledge, most of the works on bandits for recommendations conduct experiments on offline synthetic or real-world datasets with simulated environment, and how to scale online learning system to billions of users and millions of items with real-time updates is not well studied. In contrast, a key contribution of this paper is on scaling in real-world environment, and we provide the engineering details on how we build the Online Matching system. 

% Figure environment removed

\subsection{Real-time Recommenders}
Building real-time systems that can quickly process user feedback is critical for recommenders, especially in applications where new items are quickly added. StreamRec \cite{chandramouli2011streamrec} is a demo for event-driven fast data processing for training recommender models. Recently, Monolith was introduced in \cite{liu2022monolith} as a system for fast model training with collisionless embedding table. In addition to the real-time update capability, our Online Matching system is further built with explicit exploration strategies to address the feedback loop problem \cite{Chaney2018} that is not addressed in this stream of work.