
\section{A unified class of plug-in estimators} \label{sec:uniform}

In this section, we introduce a new class of estimators for $m_0$ (or equivalently $\pi_0$) that mathematically guarantees plug-in FDR control. 
{It is based on sums of suitably transformed $p$-values, allowing us to   recover classical estimators, such as the Storey \eqref{def:m0:Storey} and the PC (slightly modified) \eqref{def:m0:PC:2006} estimators, and also to define new estimators. We first present a general result for single estimators and then show that plug-in FDR control is also preserved for convex combinations of such estimators. }
%\iq{This new class of estimator provides a generic formula to construct estimators based on sums of suitably transformed $p$-values, allowing to recover classical estimators, such as the Storey \eqref{def:m0:Storey} or the PC (slightly modified) \eqref{def:m0:PC:2006} estimator, and also to define new estimators.
%We first present the mathematical guarantees verified by the general class and then present a useful property allowing to combine estimators of the class.\\}
%
% and includes the classical Storey estimator \eqref{def:m0:Storey} as well as a very simple modification of the PC estimator \eqref{def:m0:PC:2006}, which we compare to the ZZD estimator \eqref{def:PCZZD}. 
%Using our general result, we also define and investigate some further estimators.
%\subsection{Definition and plug-in FDR control} \label{ssec:def:plugin:general}
To start, assume that the $p$-values are transformed by certain functions $g \in \mathcal{G}$, with %\footnote{TO DO seb : actually we work with $f(x)=g(x)/\nu(g)$, which is a density on $[0,1]$. Is the interpretation as a likelihood helpful for motivating the estimators below? \iq{see the remark below}} 
\begin{align} \label{eq:def:class:transformations}
	\mathcal{G}&= \{g:[0,1] \rightarrow [0,1] : \text{ $g$ is non-decreasing and $\E g(U) > 0 $,  where $U \sim \unifrv[0,1]$}\}.
\end{align} 
%In the classical setting (see e.g. \ref{label}) the same transformation (and rescaling) is applied to each $p$-value. 
Accordingly we define the class of estimators $\mathcal{F}_0$ as
\begin{align}
	\mathcal{F}_0&= \left\{ \widehat{m}_0 : [0,1]^m \rightarrow  [0,\infty) \vert\: \widehat{m}_0 (p_1, \ldots, p_m) = \frac{1}{{\nu(g)}} \left(1+ \sum_{i=1}^m g(p_i) \right), g \in \mathcal{G}
	\right\}, \label{eq:def:class:estimators:0}
\end{align} 
where $\nu(g)= \E g(U)$ for any $g \in \mathcal{G}$ with $U \sim \unifrv[0,1]$ (for brevity we sometimes omit the $g$ in $\nu$ when there is no ambiguity concerning the function $g$).
The class $\mathcal{F}_0$ contains the classical estimator $\mStorey$ \eqref{def:m0:Storey} by taking $g(u)=\ind{u>\lambda}$ and  $\nu=1-\lambda$. 
It also contains a slightly modified version $\mPCNew$  of the classical estimator $\mPCOrig$ \eqref{def:m0:PC:2006} obtained from taking $g(u)=u$ with $\nu=1/2$, i.e. 
\begin{align}
	\mPCNew &= 2 + 2 \sum_{i=1}^{m} p_i  \label{eq:def:PC:new}.
\end{align}
In Section~\ref{sec:example:homogeneous} we will introduce some additional estimators and discuss $\mPCNew$ in more detail. 
{The rationale behind the definitions of the classes $\mathcal{G}$ and $\mathcal{F}_0$ is two-fold. Requiring that $g$ is non-decreasing ensures that $\widehat{m}_0$ is coordinatewise non-decreasing, allowing  us to apply Theorem \ref{thm:IMC}. 
The quantity $g(p_i)/\nu$ can be interpreted as the (local) contribution of $p_i$ to the estimate of $m_0$. 
If we expect large $p$-values to provide evidence for null hypotheses, then it seems reasonable to require $g$ to be non-decreasing. Rescaling $g(p_i)$ by $\nu= \E g(U)$ is a simple way of ensuring that $\sum_{i=1}^m g(p_i)/\nu$ is conservatively biased in the sense that $\E(\sum_{i=1}^m g(p_i)/\nu)\ge m_0$ in any constellation of null and alternative hypotheses. 
This type of conservativeness may however not be strong enough for plug-in control. 
As our main result -- Proposition \ref{prop:plugin:control:general:g} below -- shows, simply adding $1/\nu$ as a 'safety margin' to the above estimate is enough for ensuring plug-in FDR control.}\\
%\footnote{TO DO seb :here or earlier: more motivation ... all our estimators are rescaled estimators .... under \eqref{superunif} we have conservativeness: $\E  \widehat{m}_0 \ge m_0$. This is not enough however, for establishing plug-in control. Proposition \ref{prop:plugin:control:general:g} shows that adding $\frac{1}{min(\nu_1, \ldots, \nu_m ) }$  is sufficent. ... In order to provide some more intuition for the rescaling approach in estimators $\widehat{m}_0$ in \eqref{def:m0:Bin:general}, it is easily seen that under \eqref{superunif} these estimators are always conservative. }\footnote{with a slight abuse of notation since we identify the mapping with the estimator...}
{In some situations $p$-values under the null may be heterogeneous, i.e. the $p$-values may  have different distributions under the null, so that using an individual transformation for each $p$-value may be helpful. 
%\footnote{TO DO iqraa : try it on weighted $p$-values so we have an example for continuous heterogeneous setting}. 
To this end, we introduce the following  richer and more flexible class of estimators. }
%In some situations, e.g. when multiple discrete tests are performed (see Section \ref{sec:discretestimators}), the distribution of the  $p$-values under the null may be heterogeneous. 
%In that case it may be helpful to use an individual transformation for each $p$-value, which leads to the richer and more flexible class of estimators

%\begin{equation}
%	\begin{split}
%		\mathcal{F}= \left\{ &\widehat{m}_0 : [0,1]^m \rightarrow  [0,\infty) \vert\: \widehat{m}_0 (p_1, \ldots, p_m) = \frac{1}{min(\nu_1, \ldots, \nu_m ) } + \sum_{i=1}^m \frac{g_i(p_i)}{\nu_i}, \\
%	 	& \left. \text{ $g_i \in \mathcal{G}$ and $\nu_{i} = \E[g_{i}(U)]$, with $U \sim \unifrv[0,1]$ for all $i$} \right\}.
%	\end{split}
%	\label{eq:def:class:estimators}
%\end{equation} 

\begin{equation}
	\begin{split}
		\mathcal{F} = \biggl\{ \widehat{m}_0 : [0,1]^m &\rightarrow [0,\infty) \ \bigg\vert \ \widehat{m}_0 (p_1, \ldots, p_m) = \frac{1}{\min(\nu_1, \ldots, \nu_m)} \\
		&+ \sum_{i=1}^m \frac{g_i(p_i)}{\nu_i}, \quad \text{with $g_i \in \mathcal{G}$ and $\nu_{i} = \E[g_{i}(U)]$, $U \sim \unifrv[0,1]$ for all $i$} \biggr\}.
	\end{split}
	\label{eq:def:class:estimators}
\end{equation}

We state our main result on plug-in FDR control for this {more general} class below.
Clearly,  $\mathcal{F}_0 \subset \mathcal{F}$, so that the results stated for $\mathcal{F}$ {also hold} for $\mathcal{F}_0$. 
%\iq{\seb{Thanks Iqraa, for this remark. I thought about it some more and then wrote the above explanation without referring to the likelihood aspect since the connection is not so clear to me  ... We can talk about how/what we want to deal with this next week.}
%\begin{remark}
%	$\frac{g_i(p_i)}{\nu_i}$ defines a density on $[0, 1]$ so that we can interpret the sum part of the estimator as a likelihood.
%	A good choice of transformation $g$ should allow to distinguish potential nulls from the signal, so we expect $\frac{g_i(p_i)}{\nu_i}$ to be closer to one when $p_{i}$ is truly null thus allowing to increase the likelihood.
%	In that sense, $\frac{g_i(p_i)}{\nu_i} $ can be interpreted as a local estimate of \seb{$m_0$}$\widehat{m}_0$  \footnote{ or of $\mathcal{H}_{0}$, (or $\widehat{m}_0/m$?)}.  
%\end{remark}
%}
%This class of estimators is simple but still flexible enough to deal with heterogenous null $p$-values, as we shall see in Section \ref{later}. In classical settings, the same transformation is applied to all $p$-values. We also introduce a more constrained but simpler class of estimators $\mathcal{F}_0 \subset \mathcal{F}$ where the same transformation is applied to all $p$-values:
%It says that any positive, inc.  reasing and bounded transformation of $p$-values can be used to obtain valid plug-in BH procedures for FDR control.
%Our main technical tool is the following upper bound in the convex ordering of a transformed uniform rv by Bernoulli rv's. 
We now present an upper bound in the convex order  for transformed uniform random variables in terms of  Bernoulli random variables, which is the main technical tool  we use for proving  plug-in FDR control for the class $\mathcal{F}$.
%This result provides convex ordering upper bound on the for Bernoulli transformed uniform random variables.
\begin{lemma} \label{lemma:Bernoulli:cx}
	For any $g \in \mathcal{G}$ we have $g(U) \cxorder \Bin (1,\nu)$ and $\nu = \E g(U)$, where $U \sim \unifrv[0,1]$, {and the notation $\cxorder$ denotes the convex ordering (see Definition~\ref{app:def:cxorder})}.
\end{lemma} 

\begin{proof}
	For $U \sim \unifrv[0,1]$ define $X=g(U)$, so that $\E(X)=\nu$.
	Let  $l_X = \inf_{x \in [0, 1]} g(x)$, $u_X= \sup_{x \in [0, 1]} g(x)$ be the lower and upper endpoints of the support of $X$, and define a two-point distribution $Y$ concentrated on $\{l_X,u_X\}$ by $P(Y=l_X)=(u_X-\nu)/(u_X-l_X)$ and $P(Y=u_X)=(\nu-l_X)/(u_X-l_X)$.
	By Lemma~\ref{app:lemma:SS:3-A-24} 
%	Theorem 3.A.24 in \citetalias{shaked2007stochastic} 
	we have $X \cxorder Y$. \\
	Now let $Z \sim \Bin \left(1, \nu \right)$ and denote the distribution function of $Y$ and $Z$ by $F$ and $G$. 
	Clearly, $\E Y = \nu = \E Z$.  
%	\footnote{TO DO iqraa: need to specify that the important fact is the sign change, not the ordering of positive and negative}
	Since $[l_X,u_X] \subset [0,1]$ and both $Y$ and $Z$ are two-point distributions, the function $G-F$ posesses one crossing point on $[0, 1)$.
%	 and the sign sequence of $G-F$ is $+,-$. 
	Indeed, for $t \in [0, l_{X}), \quad F(t) = 0$ while $G(t) = 1 - \nu$ so that $G-F$ is positive, and for $ t \in [u_{X}, 1), F(t) = 1$ while $G(t) = 1 - \nu$ so that $G-F$ is negative. 
	For $t \in [l_{X}, u_{X})$, $G-F$ can be positive or negative depending on $\nu$. 
	Overall, the sign sequence of $G-F$ is $+, -$ so that Lemma~\ref{app:lemma:SS:3-A-44} 
%	Theorem 3.A.44 in \citetalias{shaked2007stochastic} 
	implies that $Y \cxorder Z$ and the claim follows.
\end{proof}

The following proposition is our main result on plug-in FDR control.

\begin{proposition}\label{prop:plugin:control:general:g}
%	Let $g: [0,1] \rightarrow  [0,1]$ be non-decreasing with $g(0)=0$ and $g(1)=1$ and let $\nu=\int_{0}^{1} g(x) dx$. 	
	Assume that $p_1, \ldots, p_m$ are mutually independent and \eqref{superunif} holds. 
	Then \eqref{eq:IMC} holds true for any estimator $\widehat{m}_0 \in \mathcal{F}$, where $\mathcal{F}$ is defined by \eqref{eq:def:class:estimators}. 
	In particular, the BH plug-in procedure \eqref{eq:khat:BH} using $\widehat{m}_0$ controls FDR at level $\alpha$. 
\end{proposition} 

%Since $\mathcal{F}_0 \subset \mathcal{F}$ the proposition immediately implies FDR control for the BH procedure for estimators $\widehat{m}_0 \in \mathcal{F}_0$.

\begin{proof}
	Since $\widehat{m}_0$ is coordinatewise non-decreasing, it is sufficient to verify \eqref{eq:IMC}.  
	For {any} $h \in \nullset$, monotonicity and super-uniformity give us $\widehat{m}_0( p_{0, h})  \gest 1/\nu+S_0$, where $\nu= min_{l \in \nullset \setminus \{ h \}} \nu_{l} $, and $S_0 = \sum_{\ell \in \nullset \setminus \{h\}} g_\ell(U_\ell)/\nu_\ell $ with $(U_\ell)_{\ell \in \nullset}$ i.i.d random variables distributed according to $\unifrv[0,1]$. 
	By Lemma \ref{lemma:Bernoulli:cx} we have $g_\ell(U_\ell) \cxorder \Bin (1,\nu_\ell)$ and Lemma~\ref{app:lemma:SS:3-A-48} gives $\Bin(1,\nu_i)/\nu_i \cxorder \Bin(1,\nu)/\nu$. 
	Since the convex ordering is preserved under convolutions (see Lemma~\ref{app:lemma:SS:3-A-12}) we obtain $ \nu S_0 \cxorder \Bin (m_0-1, \nu)$. 
	Finally, the mapping $x \mapsto \nu/(1+x)$  is convex on $[0, \infty)$ and therefore from the Definition~\ref{app:def:cxorder} of $\cxorder$ we obtain that 
	\begin{align} \label{eq:proofprop3.1}
		\E \left( \frac{1}{\widehat{m}_0( p_{0, h})}\right) &\le \E \left( \frac{1}{\frac{1}{\nu}+S_0}\right) = \E \left( \frac{\nu}{1+\nu S_0}\right) \le \E \left( \frac{\nu}{1+\Bin (m_0-1, \nu)}\right) \le \frac{1}{m_0},
	\end{align}
	where the last bound is a well-known result for the inverse moment of Binomial distributions (see e.g. \ref{lemma:IM:exp:bin} in Appendix) so that \eqref{eq:IMC} is proved. 
	The statement on plug-in FDR control now follows from Theorem \ref{thm:IMC}.
\end{proof}
By taking $g(u)=\ind{u>\lambda}$ we have $\nu S_0 \sim \Bin (m_0-1, \nu)$ and therefore the second inequality from the right  in \eqref{eq:proofprop3.1} can be replaced by an equality. Thus, it may be tempting to conclude that $\mStorey$ is optimal. In the case of a Dirac-Uniform constellation of $p$-values (see \cite{BR2009})  this is indeed true, since $\nu \mStorey \sim 1 + \Bin (m_0-1, \nu)$ and therefore the left inequality in \eqref{eq:proofprop3.1} can also be replaced by an equality. In more general settings however,  other choices of $g$ may be better, as the results in Section~\ref{ssec:numerical:results} show. 



%\iq{The second to last inequality of \eqref{eq:proofprop3.1} could be replaced by an equality by taking $g(u)=\ind{u>\lambda}$ providing the sharpest possible bound. 
%Thus it may be tempting to conclude that Storey estimator must be optimal, however this would only holds true in the Dirac-Uniform setting. 
%The proof suggest that taking $g(u)=\ind{u>\lambda}$ would be the best possible choice to provide the sharpest upper bound on $m_{0}$, however this only holds true in the Dirac-Uniform case. 
%Indeed, as the results in Section~\ref{ssec:numerical:results} suggest, other choices of $g$ may be better in non Dirac-Uniform setting.}
We highlight that introducing a general class of estimators as \eqref{eq:def:class:estimators} with Proposition~\ref{prop:plugin:control:general:g} allows a unified  proof of plug-in FDR control for known estimators like $\mStorey$ and $\mPCNew$ and also for new estimators that we will define  in Section \ref{sec:example:homogeneous}.
Additionally, the classes $\mathcal{F}_0$ and $\mathcal{F}$ possess stability properties that make it possible to combine various  plug-in estimators while maintaining FDR control.
\begin{proposition}\label{prop:convexity}
	Let  $\widehat{m}_1, \widehat{m}_2 \in \mathcal{F}$, where $\mathcal{F}$ is defined by \eqref{eq:def:class:estimators} and let $\lambda \in [0,1]$.   
	Then the BH plug-in procedure \eqref{eq:khat:BH} using $\widehat{m}_0 = \lambda \widehat{m}_1 + (1- \lambda) \widehat{m}_2$ controls FDR at level $\alpha$.
\end{proposition}

\begin{proof} We show that $\widehat{m}_0$ satisfies \eqref{eq:IMC}. Let $\widehat{m}_1, \widehat{m}_2 \in \mathcal{F}$ have the representation	
\begin{align*}
\widehat{m}_1 &= \frac{1}{\nu }+ \sum_{i=1}^m \frac{g_i(p_i)}{\nu_i} \qquad \text{and} \qquad 
\widehat{m}_2 = \frac{1}{\mu }+ \sum_{i=1}^m \frac{h_i(p_i)}{\mu_i},
\intertext{where $\nu=min(\nu_1, \ldots, \nu_m )$ and $\mu=min(\mu_1, \ldots, \mu_m )$ so that}
\widehat{m}_0 &= \frac{\lambda}{\nu }+\frac{1-\lambda}{\mu } + \sum_{i=1}^m \frac{\lambda g_i(p_i)}{\nu_i} + \frac{(1-\lambda)h_i(p_i)}{\mu_i}
%\intertext{and define}
%\kappa_i&= \frac{\lambda \mu_i}{\lambda \mu_i+ (1-\lambda)\mu_i}\\
%f_i &= \kappa_i g_i + (1-\kappa_i)h_i.
\end{align*}	
and define weights $\kappa_i = \frac{\lambda \mu_i}{\lambda \mu_i+ (1-\lambda)\nu_i}$ and transformations $f_i = \kappa_i g_i + (1-\kappa_i)h_i$. Clearly, $\kappa_i \in [0,1]$ and  $f_i \in \mathcal{G}$ and we introduce $\epsilon_i=\E (f_i)=\kappa_i \nu_i + (1-\kappa_i)\mu_i$. From the above definitions we obtain with some straightforward algebra
\begin{align}
\lambda &= \frac{\kappa_i \nu_i}{\epsilon_i} \qquad \text{and} \qquad 1- \lambda = \frac{(1-\kappa_i) \mu_i}{\epsilon_i} \label{eq:kappa:lambda}
\intertext{which yields}
\frac{\lambda}{\nu_i}g_i& + \frac{(1-\lambda)}{\mu_i}h_i = \frac{\kappa_i }{\epsilon_i}g_i + \frac{(1-\kappa_i) }{\epsilon_i} h_i=\frac{f_i}{\epsilon_i}. \label{eq:convex:functions}
\intertext{From \eqref{eq:kappa:lambda} we have}
\frac{\lambda}{\nu }&= \max \left(  \frac{\lambda}{\nu_1}, \ldots, \frac{\lambda}{\nu_m} \right) =  \max \left(  \frac{\kappa_1}{\epsilon_1}, \ldots, \frac{\kappa_m}{\epsilon_m} \right) \qquad \text{and} \notag \\
\frac{1-\lambda}{\mu }&= \max \left(  \frac{1-\lambda}{\mu_1}, \ldots, \frac{1-\lambda}{\mu_m} \right) = \max \left(  \frac{1-\kappa_1}{\epsilon_1}, \ldots, \frac{1-\kappa_m}{\epsilon_m} \right) \notag
\intertext{so that the sub-additivity of the $\max$ function now yields the bound}
\frac{\lambda}{\nu }&+\frac{1-\lambda}{\mu } \ge \frac{1}{\epsilon}, \label{eq:convex:coefficients}
%\intertext{where $\epsilon= \min(\epsilon_1, \ldots, \epsilon_m)$. 
\end{align}
where $\epsilon= \min(\epsilon_1, \ldots, \epsilon_m)$. Combining \eqref{eq:convex:functions} and \eqref{eq:convex:coefficients} now gives us
\begin{align*}
\widehat{m}_0 &= \frac{\lambda}{\nu }+\frac{1-\lambda}{\mu } + \sum_{i=1}^m \frac{\lambda g_i(p_i)}{\nu_i} + \frac{(1-\lambda)h_i(p_i)}{\mu_i} \ge 
\frac{1}{\epsilon} + \sum_{i=1}^m \frac{f_i(p_i)}{\epsilon_i} := \widetilde{m}_0
\end{align*}
with $\widetilde{m}_0 \in  \mathcal{F}$ and from Proposition \ref{prop:plugin:control:general:g} we know that \eqref{eq:IMC} holds true for $\widetilde{m}_0$ and therefore also for $\widehat{m}_0$.
\end{proof}
The proof shows that $\mathcal{F}$ is ``almost'' convex in the sense that whenever equality holds in \eqref{eq:convex:coefficients}  we have $\widehat{m}_0= \widetilde{m}_0 \in \mathcal{F}$. If $\widehat{m}_1, \widehat{m}_2 \in \mathcal{F}_0$, i.e. each estimator uses only a single transformation function then it is easy to see that equality holds in \eqref{eq:convex:coefficients} which leads to the following result:

\begin{proposition}\label{prop:convexity:0}%\footnote{\iq{Shouldn't we state the result for a finite number of estimators $m_{1}, \dots, m_{l}$ since convexity allows for convex combination}}
	The class of estimators $ \mathcal{F}_0$ given by \eqref{eq:def:class:estimators:0} is convex. 
	In particular this implies that for any $\widehat{m}_1, \widehat{m}_2 \in \mathcal{F}_0$  and $\lambda \in [0,1]$ the BH plug-in procedure  \eqref{eq:khat:BH} controls FDR at level $\alpha$ for the estimator  $\widehat{m}_0 = \lambda \widehat{m}_1 + (1- \lambda) \widehat{m}_2$.
\end{proposition}

%\begin{proof}
%For $j \in \{1,2\}$ let $\widehat{m}_j \in \mathcal{F}$ with $g_j$ (and associated $\nu_j$) satisfying (T). For $0 \le \lambda \le  1$ define $\widehat{m}_0 = \lambda \widehat{m}_1 + (1- \lambda) \widehat{m}_2$, $\kappa = \frac{\lambda \nu_2}{\lambda \nu_2 + (1-\lambda) \nu_1}$ anf $g_0 = \kappa g_1 + (1 - \kappa) g_2$.
%
%Clearly, $\kappa \in [0,1]$ and it is easily seen that $\lambda$ can be expressed $\lambda = \frac{\kappa \nu_1}{\kappa \nu_1 + (1-\kappa) \nu_2}$ and $g_0$ satisfies (T) with $\nu_0=\E g_0(U)= \kappa \nu_1 + (1-\kappa) \nu_2$. Thus we have
%\begin{align*}
%\widehat{m}_0 &= \lambda \widehat{m}_1 + (1- \lambda) \widehat{m}_2= \frac{1}{\kappa \nu_1 + (1-\kappa) \nu_2} \cdot \left[ \kappa \nu_1 \widehat{m}_1 + (1-\kappa) \nu_2 \widehat{m}_2\right]\\
%&= \frac{1}{\kappa \nu_1 + (1-\kappa) \nu_2} \cdot \left[ \kappa \left(1+ \sum_{i=1}^{m} g_1(p_i)\right) + (1-\kappa) \left(1+ \sum_{i=1}^{m} g_2(p_i)\right) \right]\\
%&=\frac{1}{\nu_0} \cdot \left[ 1+ \sum_{i=1}^{m} g_0(p_i) \right] 
%\end{align*}
%and so $\widehat{m}_0 \in \mathcal{F}$.
%\end{proof}


%Proposition \ref{prop:convexity} combined with Proposition \ref{prop:plugin:control:general:g} immediately  implies that weighted estimators from $\mathcal{F}$ provide valid FDR control.
%
%\begin{corollary}\label{coro:weighted:estimators}
%If we take any estimators $\widehat{m}_1, \ldots, \widehat{m}_K \in \mathcal{F}$ and any weights $\lambda_1, \dots, \lambda_K \in [0,1]$ with $\sum_{k=1}^{K} \lambda_k=1$, the BH plug-in procedure \eqref{eq:khat:BH} using the weighted estimator $\widehat{m}_0 = \sum_{k=1}^{K} \lambda_k \widehat{m}_k $ controls FDR at level $\alpha$.	
%\end{corollary} 

%Discussion (to do):
%
%\begin{itemize}
%	\item Heesen + Janssen: Data dependent convex combinations, weighted estimators, inspection points, a practical guide for weighting Storey's estimators $g(u)=...$
%	\item Liang + Nettleton
%	\item Sarkar (2008)? 
%\end{itemize}

