\section{Homogeneous estimators} \label{sec:example:homogeneous}
In this section we focus on the class  {of homogeneous estimators} $\mathcal{F}_0$  given by \eqref{eq:def:class:estimators:0}, i.e. on estimators of the form
\begin{align*}
	\widehat{m}_0  &= \widehat{m}_0  (p_1, \ldots, p_m)= \frac{1}{\nu} \left(1+ \sum_{i=1}^m g(p_i) \right),
\end{align*}
where $g \in \mathcal{G}$ and $\nu= \nu(g)= \E g(U)  > 0$,  with $U \sim \unifrv[0,1]$. 
As mentioned before, this class includes the classical estimator $\mStorey$ \eqref{def:m0:Storey} and the new estimator $\mPCNew$  \eqref{eq:def:PC:new}, and also gives the scientist freedom to define new estimators with proven plug-in FDR control thanks to  Proposition \ref{prop:plugin:control:general:g}
%\iq{A practical transformation $g$ could be a weighting function that weighs down small $p$-values and weighs up large $p$-values.
%{The transformation $g$ can be interpreted as a weighting function that weighs down small $p$-values and weighs up large $p$-values.}
}
%Proposition \ref{prop:plugin:control:general:g} gives the data analyst great freedom for defining new estimators that come with guarantees for plug-in FDR control. 
%The function $g$ can be interpreted as a weighting function that weighs down small $p$-values and weighs up large $p$-values. 
There are many conceivable ways in which this can be done. 
As an ad hoc example, we define a polynomial estimator of degree $r \ge 0$ and thresholding parameter $\lambda \in [0, 1)$ by taking $\widehat{m}_0$ as above in \eqref{eq:def:class:estimators:0} with $g(u)= g_{r, \lambda}(u) = u^{r} \cdot \ind{u>\lambda}$, so that $\nu= \frac{1 - \lambda^{r+1}}{r+1}$. 
This gives us 
	\begin{align} \label{eq:def:m0poly}
		\mPoly = \frac{r+1}{1 - \lambda^{r+1}}  + \frac{r+1}{1 - \lambda^{r+1}}  \sum_{i =1}^{ m} p_{i}^{r} \cdot \ind{p_{i}>\lambda}.
	\end{align}
It is easily seen that the classical estimators $\mStorey$ and $\mPCNew$ are particular instances of $\mPoly$ with $r=0$ for $\mStorey$ and $r=1$ and $\lambda = 0$ for $\mPCNew$.
Taking $r=1$ and $\lambda > 0$ yields a hybrid estimator which combines $\mStorey$ and $\mPCNew$ which has the potential to combine the strengths of both methods. For all estimators $\mPoly$ plug-in FDR control follows immediately from Proposition~\ref{prop:plugin:control:general:g}. 
%\footnote{should we present these estimators in equations to be able to refer to them later ? \seb{may be a good idea, i think it depens in wether we need it ... i propose to wait for now. Depeneding on figure 2 we should decide which ones to introduce formally}}
%\begin{enumerate}
%	\item[(iii)] $g(u)=u \cdot \ind{u>\lambda}$ with $\nu=(1-\lambda^2)/2$ yields a hybrid combination of $\mStorey$ and $\mPCNew$ which may combine the strengths of $\mStorey$ and $\mPCNew$. 
%	This idea can be extended to formally define a polynomial estimator 
%	Since we always set $\lambda=1/2$ throughout the paper, we only consider the degree $r$ as a parameter 
%	\item[(iv)] $g(u)=u^2$ with $\nu=1/3$ yields an estimator based on quadratic transformation of $p$-values. 
%	This example could be broadened into a class of polynomial estimators that we investigate in Section~\ref{} \footnote{TO DO later : define polynomial estimator to be more general. Dedicate a section to it ? \seb{Could make sense, depending on how much results we have on this.}}
%\end{enumerate}
%The rationale for estimators (iii) and (iv) is ad-hoc and heuristic. 
%\sout{Figure~\ref{fig:g_functions} illustrates the function $u \mapsto g_{r, \lambda}(u)$ for various choices of $r$.} 
For illustrational purposes we effectively only use $r$ as a parameter and set the thresholding parameter to  the classical value of $\lambda=1/2$ throughout the paper. 
These examples are primarily meant to illustrate the freedom and flexibility Proposition~\ref{prop:plugin:control:general:g} {allows for the class $\mathcal{F}_0$ and should not be interpreted as {recommendations for} optimal choices. These examples are investigated further in the following section.
	 
%\begin{center}
%	% Figure environment removed
%\end{center}


\subsection{Numerical results} \label{ssec:numerical:results}
%To guide the choice of the estimator, we provide in this section a comparison of the performance of the different estimators introduced above. 
%We first propose to 
%In this section we analyze numerically the performances of the estimators introduced above on simulated and real data. 
%In this section we investigate numerically the properties of in the setting of one-sided gaussian testing, i.e. we assume that $X_1, \ldots, X_m \sim N(\mu,1)$ ... \footnote{TO DO iqraa :more details needed}

%\subsubsection{Simulation results} \label{sssec:simu:bias:variance:mse}


Here we compare the performance of several estimators from the class $\mathcal{F}_0$  in a Gaussian one-sided testing setting. We assume that we observe $X_{1}, \dots, X_{m}$  independent random variables with $X_{i} \sim \mathcal{N}(0, 1)$  for $i \in \nullset$ and $X_{i} \sim \mathcal{N}(\mu, 1)$  for $i \in \altset$, with $\mu > 0$, and we test $H_{0,i}: \mu = 0$ vs. $H_{1,i}: \mu > 0$. For a given signal strength $\mu >0$ under the alternative, closed-form expressions for the expectation and variance of $\widehat{m}_0$ are available, see Appendix~\ref{appendix:sec:one:sided:gaussian:testing} for more details. Thus we can numerically compare the mean squared error (MSE) of estimators from  $\mathcal{F}_0$.
%To compare the performance of estimators from class $\mathcal{F}$, we compute the analytical mean squared error (MSE). 
%This is possible because the Gaussian setting is simple enough to provide a closed form formula for the expectation and variance of transformed $p$-values under the alternative, see Appendix~\ref{appendix:sec:one:sided:gaussian:testing} for more details.}


For this analysis, we fix $m=10 \: 000$ and first compare the MSE w.r.t. to the signal strength $\mu$ of the alternatives with a fixed proportion of true nulls $\pi_{0} = 0.6$. 
Then, we compare the MSE w.r.t. the proportion of true nulls $\pi_{0}$ with a fixed signal strength $\mu=1.5$. 
The considered estimators for this comparison are $\mStorey$ \eqref{def:m0:Storey}, $\mPCNew$ \eqref{eq:def:PC:new}, $\mPol(1, 1/2)$, and $\mPol(2, 1/2)$ (see \eqref{eq:def:m0poly} for both).
The MSE is evaluated in terms of $\pi_{0}$ for  better readability and displayed in Figure~\ref{fig:gaussian:mse}. 
The qualitative comparison between the estimators remains consistent across both panels of Figure~\ref{fig:gaussian:mse}: $\mPCNew$ has the poorest performance, characterized by the largest MSE, followed by $\mStorey$.
While the polynomial approach shows some benefits, the improvement is not particularly remarkable except for small to moderate values of $\pi_{0}$. 
For larger values of $\pi_{0}$ or $\mu$, there are no noticeable differences in performance.
%The qualitative comparison between the estimators is consistent between both panels of Figure~\ref{fig:gaussian:mse}:  $\mPCNew$ has the worst performance with the largest MSE, followed by $\mStorey$. 
%Although, the polynomial approach seems to be beneficial, the improvement is not striking besides for small to moderate values of $\pi_{0}$, and for large values of $\pi_{0}$ or $\mu$ there actually no more noticeable differences between the performance.
%Figure~\ref{fig:gaussian:mse} corroborates the estimation results presented in Figure~\ref{fig:gaussian:estim} : we can see that the hybrid estimator combining quadratic transformation of $p$-values and thresholding has the smallest MSE while $\mPCNew$ has the largest one. 


\begin{center}
	% Figure environment removed
\end{center}


%\begin{itemize}
%	\item consistency between both plots 
%	\item PC is the worst, followed by Storey
%	\item polynomial approach seems to be beneficial but no striking improvement 
%	\item for $\pi_{0}$ large or strong signal no more difference 
%\end{itemize}




%\begin{center}
%	% Figure environment removed
%\end{center}






%\footnote{TO DO iqraa : restructure the section : remove the subsection about mse just present simu, plots and then mse plot }

%\iq{We first analyze the estimator on simulated data in the one-sided Gaussian testing setting.
%In this setting we assume that we observe data $X_{1}, \dots X_{m}$ as i.i.d realizations of $\mathcal{N}(\mu, 1)$, and we test the null hypothesis $\mathcal{H}_{0, i} : \mu = 0$ vs the alternative $\mathcal{H}_{1, i} : \mu \geq 0$ simultaneously for all $i \in {1, \dots, m}$.
%Following this setting, we simulate $m=10 \: 000$ $p$-values and carry the estimations over $1 \: 000$ Monte-Carlo simulations. 
%The thresholding parameter of Storey type estimators is set to $\lambda = 0.5$, and the nominal control level is set to $\alpha = 0.05$.
%The point wise performance of the estimators and the power of the plug-in BH procedure, for a signal strength of $\mu= 1.5$ under the alternative, are presented in Figure~\ref{fig:gaussian:estim}\footnote{TO DO iqraa : add ticks in the x-axis, and uniformize colors of procedures between plots} (results for other lower and higher signal strength are displayed in the Appendix~ ). 
%For a better readability, we present the results in terms of estimation of $\pi_{0}$.}
%
%\iq{From the point estimation comparison (left panel of Figure~\ref{fig:gaussian:estim}) we can see that that both version of the \cite{PC2006} estimator $\mPCNew$ and $\mZZKB$ are the most over-conservative estimator throughout the whole range of true $\pi_{0} \in [0.1, 0.9]$.
%For small values of true $\pi_{0}$ (say $\pi_{0} < 0.5$), the hybrid estimator combining quadratic transformation of $p$-values and thresholding provides the best performance, followed by the hybrid combining $\mPCNew$  and $\mStorey$ or the raw $\mStorey$. 
%This indicates that the in the uniform setting, thresholding based estimator are better suited to estimate the proportion of true null, and combining this thresholding with a polynomial transformation of the $p$-values further improves the estimation. 
%In section ? we provide a more detailed investigation of the polynomial-thresholding based estimator that provide some insights for the choice of the degree of the polynomial.
%Each point estimation was plug-in the Bh procedure to compare the power of the adaptive BH procedure with these different estimators. 
%Along with the plug-in BH procedures we also run the raw BH and the oracle adaptive BH procedures to allow a better readability of the results: power of adaptive BH procedures close to the power of the raw BH procedure indicates that there is no improvement using adaptivity, while power of adaptive BH procedures close to the power of the oracle adaptive BH procedure indicates that the adaptivity has an impact on the procedure. \footnote{Should we crop the plot to a smaller range of true $\pi_{0}$ because after 0.5 everything is very close to 0 ...}}
%\begin{center}
%	% Figure environment removed
%\end{center}
%






%
%
%
%\begin{center}
%	% Figure environment removed
%\end{center}

%\begin{center}
%	% Figure environment removed
%\end{center}

%Here we present an investigation into the bias, variance and MSE of the estimators presented in Section \ref{ssec:def:plugin:general} in the setting of one-sided gaussian testing. If $X_0 \sim g(p_i)|H_0$ and $X_1 \sim g(p_i)|H_1 \sim f_1$ we obtain ...





%Using the formulas in Appendix \ref{appendix:ssec:one:sided:gaussian:testing} we calculate $\bias, \var ,MSE$ for   $g=...$.  Figure \ref{label} shows that... 

%\subsubsection{Simulation results.} \label{sssec:bias:variance:mse}
%To do:  describe simulation  results for $\widehat{m}_0$'s and power in the settings of \cite{ZZD2011}.



%\subsubsection{Analysis of empirical data} 
%\footnote {To DO iqraa : try the estimators on this real dataset and discuss if keep it here or in the appendix. Also make a plot with varying lambda }
%\footnote{TO DO iqraa : look for other data sets maybe the one used in PC}

%Maybe use the Hedenfalk data.

%\footnote{To Do later : maybe make a section about optimal polynomial estimator}

\subsection{More details on the Pounds and Cheng estimator} \label{ssec:ComparePCNew:PCZZD}


While FDR control for $\mStorey$ is a classical result following from Theorem \ref{thm:IMC} (\cite{BR2009,benjamini2006adaptive}), much less is known about the validity of  $\mPCOrig$ as a plug-in estimator. 
{Indeed, \cite{PC2006} introduced their estimator \eqref{def:m0:PC:2006} primarily to obtain  a robust estimate of FDR.} 
To the best of our knowlegde, the only  related result on plug-in FDR control was obtained by \cite{ZZD2011}, who defined the following  modified version of $\mPCOrig$:
\begin{align}
	\mZZKB& = C(m) \cdot \min \left[ m, \max \left(s(m), 2 \cdot \sum_{i =1}^m p_i \right) \right], \label{def:PCZZD}
\end{align}
where the correction factors $C(m)$ and $s(m)$ are chosen in such a way so that \eqref{eq:IMC} holds. 
{However, determining} these factors is non-trivial and requires extensive use of numerical integration and approximations methods (see Supplement B in \cite{ZZD2011} for further details) so that no simple representation of $C(m)$ and $s(m)$ is available (for selected values of $m$, Table S1 in \cite{ZZD2011} lists values for the correction factors). 

By contrast, our new modification \eqref{eq:def:PC:new}  is extremely simple and, as we show in Section \ref{sec:discretestimators}, can be adapted easily to e.g. discrete tests, thus confirming a conjecture in \cite{PC2006}.
%By contrast, the modified estimator $\mPCNew$ belonging to our class of estimators is extremely simple and can be adapted easily to e.g. discrete tests (confirming an unproven claim by \cite{PC2006}).
Its validity for plug-in FDR control follows directly from Proposition \ref{prop:plugin:control:general:g} and involves no sophisticated asymptotic or numerical approximations. 
Supplementary material in Appendix \ref{appendix:ssec:ComparePCNew:PCZZD} shows that the two versions of the PC estimator behave more or less identically. 
Nevertheless, we argue in favor of using $\mPCNew$ since it is both conceptually and computationally much simpler than $\mZZKB$.


%Suppose we are interested in how likely the new estimator $\mPCNew$ is more conservative than $\mZZKB$.  
%\begin{align*}
%	\mZZKB& = C(m) \cdot \min \left[ m, \max \left(s(m), 2 \cdot \sum_{i =1}^m p_i \right) \right]
%\end{align*}


% As a particular example, we follow \cite{ZZD2011} who chose $m=500$ in their simulation study.  From their Table S1 we can read off the  values $C(500)=1.011709$ and $s(500)=98$ so that here \eqref{def:PCZZD} becomes
%	\begin{align}
%		\mZZKB & = 1.011709 \cdot \min [ 500, \max (98, 2 \sum_{i =1}^m p_i)]   \label{eq:def:zeisel:estimtor:500}
%	\end{align}
%	and  we can conclude that  $\mPCNew> \mZZKB$ iff $2 \sum_{i =1}^{500} p_i \in (C(500)\cdot 98 -2, 2/(C(500)-1) \cup (500\cdot C(500)-2,1000)$. 
%	
%	
%	We now present a rough asymptotic approximation for the probability that $\mPCNew$ is more conservative than $\mZZKB$. In typical applications, most hypotheses are nulls. If we additionally assume that the signals are strong, we have $2 \sum_{i =1}^m p_i \approx 2 \sum_{i \in \nullset} p_i =:S$ 
%	and this distribution can be approximated  by the CLT so that we obtain for a given $m_0$ 
%	\begin{align*}
%		\P (\mPCNew> \mZZKB) &= \P(S>500\cdot C(500)-2) \approx \overline{\Phi} \left(\sqrt{\frac{3}{m_0}}\cdot (500\cdot C(500)-2-m_0)\right)
%	\end{align*}
%	Figure \ref{fig:comparisonzzd}  illustrates that in the range $m_0=450, \ldots, 500$ these probabilities are quite small and even in the worst case scenario $m_0=500$ the probability is less than $1/3$. Simulation findings (see Section \ref{ssec:one:sided:gaussian:testing}) show that the two versions of the PC estimator behave more or less identically, however we argue for using $\mPCNew$ since it is both conceptually and computationally much simpler than $\mZZKB$.

%\seb{remove this comment?}In our modification of the original PC estimator we have concentrated on deriving a simple and elegant result + easy implementation!. Truncating the estimator, as \cite{zeisel2011fdr} do, makes thing more complicated. 			However, it is worth noting that if we could bound the inverse moment of a truncated Erlang-rv we coiuld still use our approach, since the mapping $x \mapsto \frac{1}{\min (1+x,a)}$ is convex for any $a>0$... 

%\begin{lemma}\label{lemma:basic}
%	For $\lambda \in [0,1]$ let $F_\lambda \sim \unifrv \left[ 1 - \frac{1-\lambda}{1+\lambda}, 1 + \frac{1-\lambda}{1+\lambda}\right]$ (with the convention $F_1 \sim \delta_{\{1\}}$). Then we have
%	\begin{align}
%	F_\lambda & \cxorder F_0 \cxorder \expov(1),
%	\end{align}
%	where $\cxorder$ denotes the usual convex order (see \cite{shaked2007stochastic}).
%\end{lemma}

%\begin{proof}
%For the  bound on the l.h.s.  we show that the family $(F_\lambda)_\lambda$ is monotone w.r.t $\cxorder$ in the sense that $F_\lambda  \cxorder F_{\lambda'}$ for $\lambda' \le \lambda$. For $\lambda' \le \lambda$ the function $F_{\lambda'}-F_{\lambda}$  has exactly one sign change (at $x=1$) and the sign sequence is $+, -$\footnote{present graph?}. Since the two distributions have the same mean, \cite[Theorem 3.A.44]{shaked2007stochastic} implies that 		$F_\lambda  \cxorder F_{\lambda'}$. 
%
%For the bound on the r.h.s. we observe that $F_0 \sim \unifrv [0,2]$. Since $\expov(1)$ has decreasing density on $[0, \infty)$ with mean 1, the claim follows from Theorem 3.A.46 b) in \cite{shaked2007stochastic}.
%\end{proof}



%\newpage