
%\section{Estimators for heterogeneous $p$-values / From uniform to discrete plug-in estimators / New (Adapting ?) estimators  for discrete $p$-values} \label{sec:discretestimators}
\section{Adjusted estimators for discrete $p$-values} \label{sec:discretestimators}

In this section we assume -- additionally to mutual independence of the $p$-values -- that the null distribution functions $F_1, \ldots, F_m$ are known. 
As a particular application we consider the setting of discrete $p$-values (see Section~\ref{ssec:background} for more detailed references).
%which has attracted some research interest in recent years \footnote{TO DO iqraa : add ref}. 
%\seb{\sout{For simplicity of exposition we assume throughout that  $\widehat{m}_0 \in \mathcal{F}_0$ even though the results in this section also hold for  $\widehat{m}_0 \in \mathcal{F}$.} I removed this, because we dont need $\widehat{m}_0 \in \mathcal{F}_0$ in Section 5.3}
The classical plug-in estimators, like the \cite{Storey2002} estimator defined in \eqref{def:m0:Storey}, were developed for uniformly distributed $p$-values under the nulls, 
and can thus suffer of an inflated bias when computed under \eqref{superunif} assumption. 

{To illustrate the problem, we compare the bias of an arbitrary estimator $\widehat{m}_0 \in \mathcal{F}_0$ under the uniform setting with the bias under the super-uniform setting. In the classical uniform case,} 
%As a motivating point, we formulate  the bias for any estimator $\widehat{m}_0 \in \mathcal{F}_0$ and compare it between the uniform and the super-uniform setting. 
%For any estimator $\widehat{m}_0 \in \mathcal{F}_0$, under the classical uniform setting, 
{ considering marginally independent $p$-values {$p_{i} \sim X_0$ for $i\in \nullset$, and $p_{i} \sim X_1$ for $i\in \altset$}, for some variables $X_{0}, X_{1}$ defined on $[0, 1]$}, the bias is seen to be
\begin{align}
 	\bias [\widehat{m}_0 ] &= \E[\widehat{m}_0] - m_{0} = \frac{1}{\nu} (1 + m_{1} \E [g(X_{1})]).  \label{eq:uniformbias}
	\intertext{In contrast, under the super-uniform setting, still considering {independent} $p$-values under the null and the alternative, the bias is }
	\bias [\widehat{m}_0 ] &= \frac{1}{\nu} (1 +  m_{1} \E [g(X_{1})]) +  \frac{1}{\nu}  m_{0} ( \E [g(X_{0})] - \nu).\label{eq:superuniformbias}
\end{align}

%\begin{equation} 
% 	 
%\end{equation}
 Recall that $\nu = \E [g(U)]$ with $ U \sim \unifrv [0, 1]$, thus under super-uniformity {$\E [g(X_{0})] \geq \nu$ (see the characterization of the usual stochastic order in Appendix \ref{appendix:auxres})}, which shows that an additional source of conservativeness is {present} in this case. In general, practitioners use classical estimators without worrying about $p$-values distributions, ingenuously expecting the estimator to perform according to the ``uniform''  bias \eqref{eq:uniformbias} when in fact it often performs according to the ``super-uniform'' bias \eqref{eq:superuniformbias}.
This motivates the need for a correction in the estimator that will aim at deflating \eqref{eq:superuniformbias}.\\
%\seb{remove the following?} to get closer to \eqref{eq:uniformbias}, which is the simplest way to fight the over conservativeness.\footnote{to do iqraa : reference/discuss previous/known approaches.}
Super-uniformity does not solely appear in the discrete setting, it also occurs e.g. when testing composite nulls,  
however in the discrete setting additional information on the $p$-values c.d.f (as defined by \eqref{equ:Fi}) may be available and leveraged to correct the over-conservativeness. 
In this section, we present two such approaches that incorporate the available knowledge of $F_{i}$ -- the $p$-value c.d.f under the null -- in the estimators.
The standard way of defining $p$-values for discrete tests leads to distribution functions that satisfy \eqref{discrete} and \eqref{superunif}. 
{As we later introduce transformed $p$-values with transformed distribution, for clearer distinguishability the c.d.f associated to these standard discrete $p$-values are denoted  by $\Fdu_1, \ldots, \Fdu_m$ (where the upper-script ``sd'' denotes ``standard discrete'')}.


%\iq{TO DO 
%\begin{itemize}
%	\item add ref to talk about dealing with discreteness 
%	\item
%	\item
%\end{itemize}
%}

\subsection{{Transformations of discrete $p$-values}} \label{ssec:SuperUniformity:Discrete}
%Correcting means to make it behave in the expected way by the user, indeed the practitioner uses classical estimator without worrying about the distribution of the $p$-values. 
%So under super-uniformity the practitioner might expect the estimator to perform according to the ``uniform''  bias \eqref{eq:uniformbias} while in fact it performs according to \eqref{eq:superuniformbias}.
%The correction aims at deflating \eqref{eq:superuniformbias} to be the closest to \eqref{eq:uniformbias} which is the simplest way to fight the over conservativeness.
%\begin{itemize}
%	\item make it feels like the only quality criterion is the bias 
%	\item 
%\end{itemize}
%\seb{T.b.d. I think we need no more detailed  definitions of discrete df's, therefore i have removed them.}
%\iq{hence we start by formally defining these latter.
%\begin{definition} \label{def:discrete:cdf}
%	A function $F$ is said to be to be a discrete c.d.f of random variable valued in $[0, 1]$ if it is right continuous, and piecewise constant with \iq{jumping ?} points given by a set $\mathcal{A}_{i}$ called the support. 
%	It means that $\mathcal{A}_{i}$ is defined by points $0 = x_{i, 0} < x_{i, 1} < \ldots < x_{i, K_{i}} < x_{i, K_{i} + 1} =1$ such that $ F_{i}(x_{i, j}) = x_{i, j} \mbox{ for all } 1 \le j \le K_{i} + 1 $.
%\end{definition}
%}
%
%\seb{\begin{definition} \label{def:discrete:cdf:a}
%		A distribution function $F$ is said to be the c.d.f of a \emph{discrete-uniform} random variable  if
%		\begin{itemize}
%			\item it is piecewise constant,
%			\item the discontinuity points of $F$ are given by a finite (or countable?) set $\{0,1\} \subset \mathcal{A} \subset [0,1]$ called the \emph{support} of $F$
%			\item $F(x)=x$ for all $x \in \mathcal{A}$.
%		\end{itemize}
%\end{definition}}\footnote{to do: make this consistent with (Discrete) in Section 2?}
%
In order to reduce the individual conservatism of $p$-values caused by super-uniformity, various transformation of discrete $p$-values have been proposed, see e.g. \cite{habiger2015multiple}. 
Perhaps the most popular transformation is the so-called \emph{mid-$p$-value}. 
For the realization $x$ of the random variable $X$, let $p(x)$ be the (realized) standard $p$-value. 
Now define the \emph{mid-$p$-value} \citep{RubindelanchyHeardLawson2017} $q(x)$ given the observation $x$ as
\begin{align} \label{eq:def:mip}
	q(x) &= p(x) - \frac{1}{2} P_0 (p(X)=p(x)),
\end{align}
where $P_0$ denotes the distribution of $X$ under the null (for simplicity, we assume that such a unique distribution exists).
%After this transformation, the distribution function of the mid-$p$-value $q$, is no longer super-uniform but ``shrunk'' towards zero (for an example see Figure \ref{label} \footnote{TO DO iqraa : do a figure comparing p and mi p values}). \footnote{To do Iqraa: more on expectation}
{Transforming the $p$-value through \eqref{eq:def:mip} helps to mimic the behavior of a uniform random variable in expectation. Indeed, we always have $\mathbb{E}[q(X)] = 1/2$, see \cite{berry1995mid} for more details. 
However, the distribution of the mid-$p$-value is no longer super-uniform but shrunk toward 0 as displayed in Figure~\ref{fig:midpvsp}.}
In what follows, we denote by $\Fmid_1, \ldots, \Fmid_m$ the distribution functions of the mid-$p$-values associated with the distribution functions $\Fdu_1, \ldots, \Fdu_m$ of the standard $p$-values.  
In Section \ref{ssec:adjusting:rescaling} we show how the distribution functions of standard discrete or mid-$p$-values can be used in $m_0$-estimators introduced in Section \ref{sec:uniform}, while preserving plug-in FDR control. 

Another transformation to reduce the conservativeness of discrete $p$-values uses so-called \emph{randomized $p$-values} which are defined in our context by
\begin{align}\label{eq:def:randomp}
	r(x,u) &= p(x) - u \cdot P_0 (p(X)=p(x)),
\end{align}
where $u$ is the realization of a uniform random variable $U \sim \unifrv[0,1]$, independent of $X$. 
Alternatively to the notation $r(x,u)$, we will also use (with a slight abuse) $r(p,u)$, where $p=p(x)$ is the standard $p$-value obtained from observation $x$.
Randomized $p$-values and mid-$p$ values are related via the conditional expectation on the observations $q(x)=\E_U [r(x,U) | X = x]$.
{Randomization leads to an (unconditional) uniform behavior, however at the cost of introducing an additional source of randomness which makes its use controversial for decisions on individual hypotheses, see e.g. \cite{habiger2011randomised} for a discussion. }
We show in Section \ref{ssec:randomization:approach} that for estimation purposes however, {randomized $p$-values can be beneficial for obtaining an efficient non-randomized estimator.}
\begin{center}
	% Figure environment removed
\end{center}



%when combined in a generic way with estimators from . 

%Throughout this whole section, we assume that the set of c.d.f $\mathcal{F} = \{F_{i}, 1\leq i \leq m\}$, associated to the $p$-values $p_{1}, \dots, p_{m}$, is \emph{known} and verifies \eqref{superunif} for each $1\leq i \leq m\ $. 
%Note that these are classical assumptions made in the discrete multiple testing setting \iq{ref ?}.
%In the discrete setting, each $F_{i} \in \mathcal{F}$ can be seen as a discretized version of a uniform random variable on $[0, 1]$, where the whole probability mass between two points $(x_\ell,x_{\ell+1}]$ is placed at $x_{\ell+1}$, see Figure~ for an example of  $F_{i}$ obtained from Fisher exact tests \footnote{present a graph?} 
%\iq{This is the standard way in which (conservative) $p$-values are constructed from discrete test statistics \cite{bibid}.\footnote{Finite support ... no accumulation points. Problem for poisson test? We only need this assumption in the proof of rescaled PC estimator.}}
%
%
%\begin{definition}[generalized inverse distribution function]
%	Let $F: \R \rightarrow [0,1]$ be a distribution function. Define the generalized inverse as
%	\begin{align*}
%	F^{-1} (y) &= \inf \{x \in \R: F(x) \ge y \} .
%	\end{align*}  
%\end{definition}

\subsection{Adjusting the rescaling constants}\label{ssec:adjusting:rescaling}

{The first approach for adjusting estimators to discrete $p$-values is tailored  to estimators from the class $\mathcal{F}_0$ and adjusts the rescaling constant $\nu$ in $\widehat{m}_0$. In fact, this approach is not limited to  the discrete setting, and can also be applied for  arbitrary $p$-value distributions.}
%\sout{We present a slightly more general result for arbitrary heterogeneous (not necessarily discrete) distributions.} 
%\iq{This approach is in fact general and can allow the use of any arbitrary super-uniform\footnote{I think you cannot go with non super-uniform distribution is you want plug-in FDR control, bc we need each $\nu_{i} > 0$ which can be guaranteed only with super-uniformity, see the proof.}, not necessarily discrete, heterogeneous distributions.}

\begin{proposition}\label{prop:plugin:discrete:control:general:g}
	%	Let $g: [0,1] \rightarrow  [0,1]$ be non-decreasing with $g(0)=0$ and $g(1)=1$ and let $\nu=\int_{0}^{1} g(x) dx$. 	
	Assume that $p_1, \ldots, p_m$ are mutually independent and the null distribution functions $F_1, \ldots, F_m$ are known. 
	For any $\widehat{m}_0 \in \mathcal{F}_0$ \eqref{eq:def:class:estimators:0} with 
	\begin{align}
		\widehat{m}_0 (p_1, \ldots, p_m)&= \frac{1}{\nu(g)} \left(1+ \sum_{i=1}^m g(p_i) \right) \label{eq:def:discrete:nonadjusted:estimator}
		\intertext{define the adjusted estimator}
		\widehat{m}^{\text{adj}}_0 (p_1, \ldots, p_m)&= \frac{1}{min(\nu^{\text{adj}}_1, \ldots, \nu^{\text{adj}}_m ) }+ \sum_{i=1}^m \frac{g(p_i)}{\nu^{\text{adj}}_i} \label{eq:def:discrete:adjusted:estimator}
	\end{align}
	where $\nu^{\text{adj}}_i = \E_{p_i \sim F_i} [g(p_i)]$, is the expectation {of the transformed p-value}  taken w.r.t. $F_i$. 
	Then the BH plug-in procedure \eqref{eq:khat:BH} using $\widehat{m}^{\text{adj}}_0$ controls  FDR at level $\alpha$.
\end{proposition} 

%\seb{
%Comments:
%\begin{itemize}
%	\item  The proposition is a general result, not limited to the discrete setting. Can this be useful/interesting?
%	\item The null distribution functions $F_1, \ldots, F_m$ need not be super-uniform! The price we have to pay for this may be smaller rescaling constants...
%\end{itemize}
%}


\begin{proof}

%We show that $\widehat{m}^{\text{a}}_0 \in \mathcal{F}$ and conclude using Proposition~\eqref{prop:plugin:control:general:g}.\\
Without loss of generality we assume that $\nu^{\text{adj}}_i > 0$, otherwise the sum and minimum in  \eqref{eq:def:discrete:adjusted:estimator} is to  be taken over the index set $\{i : \nu^{\text{adj}}_i > 0\}$.
	
For any $i \in \{1, \dots, m\}$ define $g_i : [0,1] \rightarrow [0,1]$ by $g_i(y) = g \circ F^{-1}_i (y)$ for $y \in (0, 1]$, where $F^{-1}_i (y) = \inf \{x \in \R: F_i(x) \ge y \}$ is the generalized inverse of $F_{i}$, and set $g_i(0)=g(0)$.
Since $g \in \mathcal{G}$ and $F^{-1}_i $ are both nondecreasing, so is $g_i$.
For $i \in \mathcal{H}_{0}$, with $U \sim \unifrv[0, 1]$, we have  $p_{i} \sim F^{-1}_i (U)$ {by Proposition 2 in \cite{EmbrechtsHofert2013}}, so that $g_{i}(U) \sim g(p_{i})$, which implies that $ \E [g_{i}(U)] = \E_{p_i \sim F_i} [g(p_i)] = \nu^{\text{adj}}_i $, so that \eqref{eq:def:discrete:adjusted:estimator} %restricted to $\nullset$
 belongs to the class $\mathcal{F}$.\\
Now let $z_{1}, \dots z_{m}$ be independent random variables with $z_{i} \sim \unifrv[0, 1]$ for $i \in \nullset$ and $z_{i} \sim \delta_{0}$ for $ i \in \altset$ (Dirac-Uniform configuration). 
Since $g_{i}(U) \sim g(p_{i})$ for $i \in \nullset$, we have 
\begin{align*}
	\widetilde{m}^{\text{adj}}_0 (z_1, \ldots, z_m)  &= \frac{1}{min(\nu^{\text{adj}}_1, \ldots, \nu^{\text{adj}}_m ) }+ \sum_{i \in \nullset} \frac{g_{i}(z_i)}{\nu^{\text{adj}}_i} \\
									  &\sim \frac{1}{min(\nu^{\text{adj}}_1, \ldots, \nu^{\text{adj}}_m ) }+ \sum_{i \in \nullset} \frac{g(p_i)}{\nu^{\text{adj}}_i}
									  \leq \widehat{m}^{\text{adj}}_0 (p_1, \ldots, p_m) \quad \mbox{(a.s.)}
\end{align*}
Since $\widetilde{m}^{\text{adj}}_0 (z_1, \ldots, z_m) \in \mathcal{F}$, by Proposition~\ref{prop:plugin:control:general:g} \eqref{eq:IMC} holds for $\widetilde{m}^{\text{adj}}_0 (z_1, \ldots, z_m)$, and since $\widetilde{m}^{\text{adj}}_0 (z_1, \ldots, z_m) \leq \widehat{m}^{\text{adj}}_0 (p_1, \ldots, p_m)$ (a.s.), \eqref{eq:IMC} also holds for $\widehat{m}^{\text{adj}}_0 (p_1, \ldots, p_m)$.
\end{proof}
%For any $y \in [0,1]$ define the generalized inverse of $F_i$ as $F^{-1}_i (y) = \inf \{x \in \R: F_i(x) \ge y \}$ (and $F^{-1}_i (0)= - \infty$). 
%Define the function $g_i : [0,1] \rightarrow [0,1]$ by  $g_i(y) = g \circ F^{-1}_i (y)$ for $y \in (0,1]$ and set $g_i(0)=g(0)$.
%Since $g \in \mathcal{G}$, for each $i \in [\![ 1, m]\!]$, $g_i(y)$ is non-decreasing, and for $U \sim \unifrv[0,1]$, $\E [g \circ F^{-1}_i (U)] = \E_{p_i \sim F_i} [g(p_i)]$ since $F^{-1}_i (U) \sim F_{i}$. 
%Finally,  $\E_{p_i \sim F_i} [g(p_i)] \geq  \E [g(U)]$ because $F_{i}$ is super-uniform, so $\nu^{\text{a}}_i = \E_{p_i \sim F_i} [g(p_i)] > 0$ so $g_i \in \mathcal{G}$ and $\widehat{m}^{\text{a}}_0 \in \mathcal{F}$.


% Then $g_i \in \mathcal{G}$.
	
%Let $\widetilde{p}_1, \ldots, \widetilde{p}_m$ be independent rv's taking values in $[0,1]$, so that $p_i \sim F^{-1}_i (\widetilde{p}_i)$. If $i \in \nullset$, Proposition 2 in \cite{EmbrechtsHofert2013} guarantees that taking  $\widetilde{p}_i \sim \unifrv[0,1]$ works. For $i \in \altset$ the distribution of $\widetilde{p}_i$ can be constructed by ...
%\seb{Iqraa: Could you complete the proof?}

%Thus we have 
%\begin{align*}
%\widehat{m}^{\text{a}}_0 (p_1, \ldots, p_m)&= \frac{1}{min(\nu^{\text{a}}_1, \ldots, \nu^{\text{a}}_m ) }+ \sum_{i=1}^m \frac{g(p_i)}{\nu^{\text{a}}_i} \sim 
%\frac{1}{min(\nu^{\text{a}}_1, \ldots, \nu^{\text{a}}_m ) }+ \sum_{i=1}^m \frac{g_i(\widetilde{p}_i)}{\nu^{\text{a}}_i} = \widetilde{m}_0 (\widetilde{p}_1, \ldots, \widetilde{p}_m)
%\end{align*}
%and since the rescaling constants are evaluated unter the nulls we have $\nu^{\text{a}}_i = \E_{p_i \sim F_i} g(p_i)= \E_{\widetilde{p}_i \sim \unifrv[0,1]} g_i(\widetilde{p}_i) = \nu (g_i)$ so that  $\widetilde{m}_0 \in \mathcal{F}$. Since the $(\widetilde{p}_1, \ldots, \widetilde{p}_m)$ are independent and super-uniform under the null the claim now follows from Proposition \ref{prop:plugin:control:general:g}.



%In the case of super-uniform $p$-values, the 'offset term' in $\widehat{m}^{\text{a}}_0$ can be replaced by the simpler but more conservative quantity $1/\nu$.
% In appendix~\eqref{}, we present for completeness, a version of $\widehat{m}^{\text{a}}_0$ where the 'offset term' is replaced by the simpler but more conservative quantity $1/\nu$.
%This version may be interesting from a mathematical viewpoint,  but it provides no practical benefit  since the $\nu^{\text{a}}_i$'s need to be computed in any case.
% While this result, which we have presented for completeness,  may be interesting from a mathematical viewpoint,  it provides no practical benefit,  since the  $\nu^{\text{a}}_i$'s need to be computed in any case. 
%\sout{For practical applications, we therefore prefer to work with $ \widehat{m}^{\text{a}}_0$, defined in \eqref{eq:def:discrete:adjusted:estimator}. }


Following Proposition \ref{prop:plugin:discrete:control:general:g}, we define the discrete-uniform estimator using \eqref{eq:def:discrete:adjusted:estimator} with {standard} discrete $p$-values  $p_1, \ldots, p_m$ and their distribution functions  $\Fdu_1, \ldots, \Fdu_m$
%\footnote{The upperscript \textit{du} indicates that we use the standard discrete distribution function $F_i$ of $p$-values under the null} 

%For the special case of standard  we follow Proposition  by defining for a given $\widehat{m_0} \in \mathcal{F}_0$ with $g \in \mathcal{G}$  the \emph{discrete-uniform}\footnote{to do iqraa : Can we find a better name?} estimator 
\begin{align}
	\mdu (p_1, \ldots, p_m) &= \frac{1}{min(\nudu_1, \ldots, \nudu_m ) }+ \sum_{i=1}^m \frac{g(p_i)}{\nudu_i}, \label{eq:def:m0du}
\end{align}
where $\nudu_i=\E_{p_i \sim \Fdu_{i}} [g(p_i)]$. \begin{corollary}
	Assume that $p_1, \ldots, p_m$ are mutually independent {and  \eqref{superunif} holds} with null distribution functions $\Fdu_1, \ldots, \Fdu_m$ that are known. 
	Then the BH plug-in procedure  \eqref{eq:khat:BH} using $\mdu$ as in  \eqref{eq:def:m0du}controls  FDR at level $\alpha$. 
	Moreover $\mdu \le \widehat{m}_0$ (a.s.), {where $\widehat{m}_0$ is the base non-adjusted estimator \eqref{eq:def:discrete:nonadjusted:estimator}}.
\end{corollary}	
{The last statement of the corollary shows that for standard discrete $p$-values the estimator $\mdu$ is guaranteed to perform better than $\widehat{m_0}$.}
{This follows from the fact that $\nudu_1, \ldots, \nudu_m \ge \nu$ because $g$ is non-decreasing and \eqref{superunif} holds (see Appendix~\ref{appendix:auxres}).}


%and the expectation is taken using the standard discrete-uniform distribution function $\Fdu_i$ under the null. 
For classical estimators, the adjusted rescaling constants {can be computed easily, using}
\begin{itemize}
	\item $\nuduStorey_i=1-\Fdu_i(\lambda)$;
	\item $\nuduPC_i = \sum_{x \in \mathcal{A}_i} x \cdot P (p_i=x)$, where $\mathcal{A}_i$ denotes the support of $\Fdu_i$.
\end{itemize}


Similarly {to \eqref{eq:def:m0du}}, we define a mid $p$-value estimator using \eqref{eq:def:discrete:adjusted:estimator} with mid-$p$-values $q_1, \ldots, q_m$ and their distribution functions $\Fmid_1, \ldots, \Fmid_m$ 
\begin{align}\label{eq:def:rescmidp}
	\mmidp (q_1, \ldots, q_m) &= \frac{1}{min(\numid_1, \ldots, \numid_m ) }+ \sum_{i=1}^m \frac{g(q_i)}{\numid_i}
\end{align}
where $\numid_i=\E_{q_i \sim \Fmid_i} [g(q_i)]$ is the expectation taken under the null using the  mid $p$-value distribution function $\Fmid_i$. 
For $\mStorey$ we have $\numidStorey_i = 1 - \Fmid_i(\lambda) \le 1-\Fdu_i(\lambda)=\nuduStorey_i$ and  $g(q_i)=\ind{q_i > \lambda} \le \ind{p_i > \lambda}=g(p_i)$ so that $\mmidp$ can be smaller or larger than $\mdu$, depending on the specific constellation. 
In the case of the PC estimator {we have $g(x)=x$ and since $\E q_i = 1/2$ for any mid-$p$-value (see \cite{berry1995mid}) we have $\numid_1=\ldots =\numid_m = 1/2$} so that in this case the mid-$p$ estimator has a particularly simple representation. 
{Combining this with the fact that $q_i \le p_i$ $(a.s.)$ gives us the following result.}

\begin{corollary}
	Assume that $p_1, \ldots, p_m$ are mutually independent {and super-uniform under the null (i.e. \eqref{superunif} holds),} and let $q_1, \ldots, q_m$ denote the corresponding mid-$p$-values. 
	Then the mid-$p$ estimator of $\mPCNew$ is given by
	\begin{align*}
		\mPCmidp (q_1, \ldots, q_m) &= 2+ 2\cdot  \sum_{i=1}^m q_i 
	\end{align*}
	and  the BH plug-in procedure  \eqref{eq:khat:BH} using $\mPCmidp (q_1, \ldots, q_m)$ 	controls  FDR at level $\alpha$. 
	Moreover, $\mPCmidp \le \widehat{m_0} $ $(a.s.)$, {where $\widehat{m}_0$ is the base non-adjusted estimator \eqref{eq:def:discrete:nonadjusted:estimator}}.
\end{corollary}

This result implies that for the PC estimator with discrete data we can simply use $2+ 2\cdot  \sum_{i=1}^m q_i$ instead of the more conservative  $2+ 2\cdot  \sum_{i=1}^m p_i$ estimator without losing plug-in FDR control.  
We point out that the mid-$p$-values are used exclusively for estimating $m_0$ in the plug-in procedure defined by \eqref{eq:khat:BH}  while the (ordered) standard discrete $p$-values $p_{(k)}$ are used in the final decision step. 

%\begin{proof} \seb{\sout{For the PC estimator we have $g(x)=x$ and since $\E q_i = 1/2$ for any mid-$p$-value (see \ref{label}) we have $\numid_1=\ldots =\numid_m = 1/2$ and the result follows.}} \end{proof}

\subsection{A randomization approach} \label{ssec:randomization:approach}
Here we describe an approach related to \cite{dickhaus2012analyze} who argue for using randomization methods in estimating $m_0$ on discrete data.
For any estimator $\widehat{m}_0$, not necessarily belonging to $\mathcal{F}_0$ define the associated \emph{expected randomized estimator} as 
\begin{align}
	\mrand (p_1, \ldots, p_m) &=  \left[ \E_{(U_1, \ldots, U_m)} \left(\frac{1}{\widehat{m}_0 (r(p_1,U_1), \ldots, r( p_m,U_m))}\right)\right]^{-1} \label{def:m0:rand}
\end{align}
where $U_1, \ldots, U_m \sim \unifrv[0,1]$ denote i.i.d uniform random variables independent of $ (p_1, \ldots, p_m)$. 
Thus, for fixed $(p_1, \ldots, p_m)$ this estimator is obtained by taking the expectation over the randomized $p$-values associated with $(p_1, \ldots, p_m)$. %\footnote{to do seb : Comments: reproducibility,No unpleasant side effects (additional randomness introduced) due to randomization due to taking the expectation. Not 'full' randomization but 'conditional in expectation' (?), describe similarities and ifferences to Dickhaus ...} 
In most cases \eqref{def:m0:rand} is analytically intractable, we therefore use Monte-Carlo approximation of $\mrand$ obtained by averaging a large number of simulations of 
$\widetilde{m}_0 (r(p_1,U_1), \ldots, r( p_m,U_m))$ (the vector $(U_1, \ldots, U_m)$ is simulated many times, while $(p_1, \ldots, p_m)$ is kept fixed). 
Again, this approach comes with guaranteed FDR plug-in control.

\begin{corollary} \label{coro:randomized:plugin:control}
Assume that $p_1, \ldots, p_m$ are mutually independent {and super-uniform under the null (i.e. \eqref{superunif} holds)} and let $\widehat{m}_0$   {satisfy the conditions of Theorem \ref*{thm:IMC}}. 
	Then the BH plug-in procedure  \eqref{eq:khat:BH} using $\mrand (p_1, \ldots, p_m) $ defined by \eqref{def:m0:rand} controls  FDR at level $\alpha$. {Moreover $\mrand \le \widehat{m}_0$ (a.s.).}
\end{corollary}

%\seb{I have  rephrased the corollary since rand works for any $\widehat{m}_0$ which satisfies Theorem \ref{thm:IMC}.}

\begin{proof}
The proof uses Theorem \ref{thm:IMC}. First, we show that $\mrand (p_1, \ldots, p_m)$ is coordinatewise non-decreasing. 
For fixed $(u_1, \ldots, u_m) \in [0,1]^m$ each (realized) randomized $p$-value $r_i=r(p_i,u_i)$ is non-decreasing in $p_i$. Since $\widehat{m}_0 \in \mathcal{F}$ is coordinatewise non-decreasing in $(p_1, \ldots, p_m)$, the function $1/\widehat{m}_0(r(\cdot,u_1), \ldots, r(\cdot,u_m))$ is coordinatewise decreasing for all $(u_1, \ldots, u_m) \in [0,1]^m$ and so is its expectation which implies that $\mrand$ is coordinatewise non-decreasing.
To establish \eqref{eq:IMC},   we denote for $h \in \nullset$ by $r_{0,h}$ the set of randomized $p$-values $(r_1, \ldots,r_m)$, where $r_h$ has been replaced by $0$. 
By the definition of $\mrand$ we have
\begin{align*}
	\E_{(p_1, \ldots, p_m)} \left[\frac{1}{\mrand(p_{0,h})}\right] &= \E_{(p_1, \ldots, p_m)} \left[ \E_{(U_1, \ldots, U_m)} \frac{1}{\widehat{m}_0(r_{0,h})} \right]=\E_{(r_1, \ldots, r_m)} \left[\frac{1}{\widehat{m}_0(r_{0,h})} \right].
\end{align*}
where the second equality follows from the fact that for super-uniform $p$-value $p_{h} = 0$,  the associated randomized $p$-value $r(p_{h}, u) = 0$ (a.s) by Definition~\eqref{eq:def:randomp}.
Since the $(r_1, \ldots, r_m)$ are mutually independent and uniform under the null and $\widehat{m}_0 \in \mathcal{F}$, the bound \eqref{eq:IMC} for $\widehat{m}_0(r_{0,h})$ now follows %from Proposition \ref{prop:plugin:control:general:g} so that 
since $\widehat{m}_0$ satisfies the conditions of Theorem \ref{thm:IMC}. Therefore, the r.h.s. of the last equation can be bounded by $1/m_0$ and {plug-in FDR control} for $\mrand$ now follows from Theorem \ref{thm:IMC}. 
{To see that the last statement of the corollary holds true, observe that since $\widehat{m}_0$ is coordinatewise non-decreasing and $r(p_i, U_i) \le r(p_i,0)=p_i$ we have $\widehat{m}_0 (r(p_1,U_1), \ldots, r( p_m,U_m)) \le \widehat{m}_0(p_1,\ldots,p_m)$ and therefore the r.h.s. of \eqref{def:m0:rand} is bounded by $\widehat{m}_0(p_1,\ldots,p_m)$ $(a.s.)$.}
\end{proof}

\cite{dickhaus2012analyze} argue for using randomized $p$-values in (essentially) Storey's estimator, i.e. applying $\mStorey$ to $(r_1, \ldots,r_m)$ instead of $(p_1, \ldots, p_m)$ which yields a random estimate that should provide  a better estimate for $m_{0}$. 
They show that plugging this estimator  into the Bonferroni procedure yields asymptotic control of the Familywise Error Rate (FWER) under certain assumptions. 
They also point out that if fully reproducible results are desired it may be more appropriate to work with the conditional expectation w.r.t. randomization, i.e. using $\E_{(U_1, \ldots, U_m)} \left(\mStorey (r_1, \ldots,r_m)\right)$. 
Corollary \ref{coro:randomized:plugin:control} shows that we can obtain similar guarantees w.r.t. to plug-in FDR control in a finite-sample setting for {any estimator  $\widehat{m}_0$ satisfying  the conditions of Theorem \ref*{thm:IMC} and in particular for  $\widehat{m}_0 \in \mathcal{F}_0$ by using conditional expectation w.r.t. randomization.}  
The slightly complicated form of  \eqref{def:m0:rand} is a natural consequence of Theorem \ref{thm:IMC}, but if the variance of $\widehat{m}_0 (r(p_1,U_1), \ldots, r( p_m,U_m))$ w.r.t. $U_1, \ldots, U_m$  is small we have the approximation $\mrand (p_1, \ldots, p_m) \approx \E_{(U_1, \ldots, U_m)} \widehat{m}_0 (r(p_1,U_1), \ldots, r( p_m,U_m))$.
%\begin{align*}
%	\mrand (p_1, \ldots, p_m) &\approx \E_{(U_1, \ldots, U_m)} \widehat{m}_0 (r(p_1,U_1), \ldots, r( p_m,U_m)).
%\end{align*}



\subsection{Simulation results}\label{ssec:simu:discrete}

In this section, we analyze how the discrete adjustments can improve the base estimators on simulated data. 
%More specifically, we simulate $m = 500$ experiments in which the goal is to detect differences between two groups by counting the number of successes/failures in each group.
More specifically, we follow \cite{DDR2018} by simulating a two-sample problem in which a vector of $m= 500$ independent binary responses is observed for $N=25$ subjects in both groups. 
The goal is to test the $m$ null hypotheses $H_{0,i}$: '$p_{1i} = p_{2i}$', $i = 1,...,m$ where $p_{1i}$ and $p_{2i}$ are the success probabilities for the $i^{th}$ binary response in group A and B respectively. 
Thus, for each hypothesis $i$, the data can be summarized by a $2 \times 2$ contingency table, and we use (two-sided) Fisher's exact tests (FETs) for testing $H_{0i}$. 
The  $m$ hypotheses are split in three groups of size 
$m_1$, $m_2$, and $m_3$ such that $m = m_1 + m_2 + m_3$.
Then, the binary responses are generated as i.i.d Bernoulli of probability 0.01 ($\Bin(1,0.01)$) at $m_1$ positions for both groups,
i.i.d $\Bin(1,0.10$) at $m_2$ positions for both groups, 
and i.i.d $\Bin(1,0.10)$ at $m_3$ positions for one group
and i.i.d $\Bin(1,p_3)$ at $m_3$ positions for the other group.
Thus, null hypotheses are true for $m_1 + m_2$ positions, while they are false for $m_3$ positions (set $\cH_1$). 
We interpret $p_3$ as the strength of the signal and set it to 0.4, while $\pi_{1} = \frac{m_3}{m}$, corresponds to the proportion of signal. 
Also, $m_1$ and $m_2$ are both taken equal to $\frac{m - m_3}{2}$.

We first compare  the base estimators  $\mStorey$ \eqref{def:m0:Storey}, $\mPCNew$ \eqref{eq:def:PC:new}, and $\mPol(2, 1/2)$ \eqref{eq:def:m0poly}  with their standard discrete rescaled versions.
 %calculated following $\mdu$ as in  \eqref{eq:def:m0du}. 
 Figure~\ref{fig:FETestim_base_vs_resc} displays the estimation results for a grid of true $\pi_{0} \in \{0.1, \dots, 0.9 \}$.
 We can see that incorporating discreteness leads to considerable improvements for all estimators over the entire range of $\pi_{0}$ values. This is particularly relevant for large values of $\pi_{0}$ where the base estimators may lead to a strong deterioration in the power of the plug-in BH procedure.
 {On another note, among the base estimators we can see that $\mPol(2, 1/2)$ performs poorly compared to the results of Section~\ref{ssec:numerical:results}. 
 This seems plausible since a large portion of $p$-values are equal to 1 in the discrete setting in contrast to the Gaussian setting. 
 For these $p$-values, the contribution in the $\mPol(2, 1/2)$ estimator is equal to the constant $\nu= \frac{3}{1 - 1/2^{3}} = \frac{24}{7} $ which is much larger than the corresponding contribution of $\nu=2$ in the Storey estimator.
% Since the rescaling constant $\nu$ of $\mPol(2, 1/2)$, $\nu= \frac{1 - 1/2^{3}}{3} = \frac{24}{7} $ is much larger than the rescaling constant  of 2 for the Storey estimator, it inflates 
% longer compensated by the sum in \eqref{eq:def:m0poly}
 }  
 %a significant improvement across the entire range of $\pi_{0}$ values, particularly for large values of $\pi_{0}$.
 %This further motivates the use of discrete adjustments as the base estimators tend to be excessively conservative and consistently exceed 1 for larger values of $\pi_{0}$, rendering them useless.
\begin{center}
	% Figure environment removed
\end{center}
In a second step, we compare the different discrete adjustments $\mdu$ as in  \eqref{eq:def:m0du}, $\mmidp$ as in \eqref{eq:def:rescmidp}, and $\mrand$ as in \eqref{def:m0:rand} in Figure~\ref{fig:FETestim_all_disc}, where we display the estimation results for three values of true $\pi_{0} \in \{0.2, 0.5, 0.7 \}$.
We can see that there are no relevant differences between the different adjustments. Therefore, there is no strong reason to advocate a specific type of adjustment since they yield similar outcomes.
\begin{center}
	% Figure environment removed
\end{center}

\subsection{Real data analysis}

Finally, we compare the performance of base and discrete estimators on three different datasets. 
The first dataset consists of data provided by the International Mice Phenotyping Consortium (IMPC)  \citep{karp2017prevalence}, which coordinates studies on the genotype influence on mouse phenotype. 
This dataset includes, for each of the $m=266952$ studied genes, the counts of normal and abnormal 
phenotypes thus providing multiple two by two contingency tables, which can be analysed using FETs. 
Then we analyze the methylation dataset for cytosines of Arabidopsis in Lister et al. (2008) which is part of the R-package \texttt{fdrDiscreteNull} of \cite{chen2015fdrdiscretenull}.
This dataset  contains $m=3525$ counts for a biological entity under two different biological conditions or treatments also analyzed using FETs.
Finally, the third dataset, provided by the Regulatory Agency in the United Kingdom, includes adverse drug reactions due to medicines and healthcare products. It contains the number of reported cases of amnesia as well as the total number of adverse events reported for each of the $m = 2446$ drugs in the database.  For more details we refer to \cite{heller2011false} and to the accompanying R-package \texttt{discreteMTP} of \cite{heller2012discretemtp}, which also contains the data. \cite{heller2011false} investigate the association between reports of amnesia and suspected drugs by performing for each drug (one-sided) FETs.


From the results in Table~\ref{tab:SummaryRandmized} we can see that taking discreteness into account is always beneficial, regardless of the adjustment used.
Depending on the type of discreteness and the amount of signal contained in the data, adjusting for discreteness  can provide a great  improvement in some cases. Indeed, as the example of the IMPC data shows, base estimators may not be able to recognize the presence of any alternatives. However, the discrete estimators clearly suggest that a considerable amount of alternatives is present. 



%Indeed, without discrete adjustments the estimators are unable to reveal the signal. 

%\begin{itemize}
%	\item describe the datasets (see, online paper, and paper with Guillermo)
%	\item taking discreteness into account is always beneficial regardless of the method used
%	\item depending on the type of discreteness and the amount of signal the adjustment can lead to big improvement (see diff between IMPC)
%	\item for IMPC signal get lost if no adjustment, continuous estimators obscure the amount of signal, discrete estimator reveals the signal
%
%\end{itemize}


\begin{table}[!ht]
		\caption{$\pi_{0}$-estimates for base estimators and adjusted discrete estimators on three different datasets containing discrete data.}\label{tab:SummaryRandmized}
	\centering
	\begin{tabular}{llccc}
		\toprule
		~ & ~ & \multicolumn{3}{c}{Dataset} \\ \cmidrule{3-5}
		Adjustment & Estimator & IMPC  & Arabidopsis & Pharmacovigilance \\ \midrule
		standard (none) & Storey                 & 1.26     &     0.67        &     1.79   \\
		& PC                      & 1.26    &     0.73      &       1.79 \\
		& Poly$(2, 1/2)$         & 2.16     &      0.75       &        2.97\\\midrule[0em]
		rescaled (du) & Storey                 & 0.63     &      0.59       &      1.05  \\ 
		& PC                      &  0.63    &      0.64       &       1.04 \\
		& Poly$(2, 1/2)$        &  0.63    &      0.57       &        1.10\\ \midrule[0em]
		rescaled (mid) & Storey                 & 0.63   &       0.63      &       1.03 \\
		& PC                      &  0.63    &      0.64       &        1.05\\
		& Poly$(2, 1/2)$         & 0.63    &       0.58      &        1.11\\ \midrule[0em]
		randomized & Storey                  &  0.63    &     0.58        &       1.08 \\
		& PC                       &   0.63   &        0.64     &        1.06\\
		& Poly$(2, 1/2)$         &    0.63  &       0.56      &       1.14 \\
		\bottomrule
	\end{tabular}
\end{table}



\newpage