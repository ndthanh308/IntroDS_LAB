\section{Introduction}\label{sec:intro}

\subsection{Background} \label{ssec:background}
When many statistical tests are performed simultaneously, an ubiquituous way to account for the false rejections is the false discovery rate (FDR), that is the expected proportion of errors among the rejections. 
The seminal \cite{BH95} procedure (abbreviated in the sequel as BH procedure) works by rejecting $H_{(1)}, \ldots, H_{(\hat{k})}$, where $\hat{k}$ is determined in the following \emph{step-up} manner
\begin{align}\label{eq:def:BH:critvalues}
	\hat{k} &= \max \left\{ \l \in \{0, \ldots, m \} : p_{(\l)} \le \frac{\l}{m} \cdot \alpha \right\},
\end{align} 
where $ p_{(1)} \le \ldots \le p_{(m)}$ denote the ordered $p$-values, and $H_{(1)} , \ldots, H_{(m)}$ the corresponding null hypotheses and  $p_{(0)} := 0$. 
According to results in \cite{BH95} and \cite{BY2001}, this procedure guarantees that $\FDR \le \pi_{0}\alpha$ when the $p$-values are independent or positively dependent, while
for arbitrarily dependent $p$-values the \cite{BY2001} procedure is available. The simplicity of the BH procedure  and its many useful theoretical properties have made it an indispensable tool in modern high dimensional data analysis, see e.g. \cite{benjamini2010simultaneous}.  
Much work has gone into analyzing, extending and adapting this procedure to various settings.

In this context, \cite{Storey2004} showed that the \emph{plug-in BH procedure}
\begin{align}
	\hat{k} &= \max \left\{ \l \in \{0, \ldots, m \} : p_{(\l)} \le \frac{\l}{\widehat{m}_0} \cdot \alpha \right\}, \label{eq:khat:BH} 
\end{align} 
obtained by replacing  $m$ in \eqref{eq:def:BH:critvalues} by an estimate $\widehat{m_0}$ of $m_0$ still provides so called \emph{adaptive} or \emph{plug-in} $\FDR$ control at level $\alpha$ while allowing for more power. % \sout{under certain conditions}.   
 Classical examples of such estimates were proposed by \cite{Storey2004}
\begin{align}
	\mStorey&= \frac{1 + \sum_{i =1}^m \ind{p_i > \lambda}}{1-\lambda}, \label{def:m0:Storey}\\
	\intertext{where $\lambda \in [0, 1)$ is a tuning parameter, and by \cite{PC2006} }
	\mPCOrig&= 1 \wedge  \left( 2\sum_{i =1}^m  p_i \right). \label{def:m0:PC:2006} 
\end{align}

%Further specific contributions include \citep{ benjamini2000adaptive,  Gavrilov2009, dickhaus2012analyze, ZZD2011, heesen2016dynamic}.
While the focus of this work is on plug-in FDR control, estimates of $m_0$ (or equivalently $\pi_0=m_0/m$) can also be used for FDR estimation purposes (see \cite{Storey2002}). 
Thus, there is a large body of literature on this topic and numerous methods for establishing plug-in FDR control are available, see e.g. \citet{benjamini2006adaptive, Sarkar2008, BR2009, heesen2016dynamic, DitzhausJanssen} and references therein. 
{
%To prove plug-in control, sufficient conditions on the distribution of the estimator have been proposed by . 
%These sufficient conditions are of the same flavor, relying on bounding the inverse moment of the estimators \footnote{TO DO iqraa: check}, with difference relying more on the ... \footnote{TO DO iqraa : check what are the differences}. 
In this paper, we use a classical condition proposed by \cite{BR2009} for establishing plug-in FDR control for a unified class of $m_0$-estimators, see Section~\ref{ssec:imc} for more details.
%\cite{BR2009} sufficient condition is investigated for most of the estimators (see previous references) but is rarely verified (up to now only \cite{Storey2002} verifies the condition), and there exist no generic method to build efficient estimator verifying the condition.
}

{Previous work on plug-in FDR control has focused on} continuous test statistics, for which  the null $p$-values are distributed according to the uniform distribution. 
{Considering the abundance of super-uniform $p$-values in real life applications, the uniformity assumption is often violated which may lead to undesirable conservatism of the $m_0$-estimators, see Section~\ref{sec:discretestimators} for more details.}
% to a loss of FDR control guarantee. \footnote{TO DO iqraa : is that true, add a ref ? } }
{Super-uniform $p$-values can be observed when testing composite null hypotheses or when dealing with discrete tests, the latter being the setting of our interest in this work.}
%%\begin{itemize}
%%	\item Introduce FDR and BH 
%%	\item BH has extra undesired stringency: the procedure's FDR is at most $\pi_{0} \alpha$ instead of $\alpha$
%%	\item Adaptivity to counteract this stringency : incorporate $\pi_{0}$ in the critical thresholds to uplift the FDR to $\alpha$. In practice $\pi_{0}$ is unknown so we need to estimate it.
%%	\item The estimator $\hat{\pi}_{0}$ is built using the same data as the one used to get the $p$-values : additional randomness, so the conservativity of the plug-in BH procedure might be at risk
%%	\item One sufficient way to verify the conservativeness of the plug-in BH procedure is to verify the inverse moment bound condition defined bellow 
%%\end{itemize}
%%
%%\begin{itemize}
%%	\item Multiple testing regain an important interest since the work of \cite{BH95} introducing the False Discovery Rate (FDR), a more liberal error criterion allowing the number of error to grow with the number of discoveries made. 
%%	\item In this seminal work, \cite{BH95} not only propose this new error criterion but also a testing procedure, so called the BH procedure, that allow to make discoveries while controlling the FDR at a user chosen testing level $\alpha \in (0, 1)$.
%%	\item This procedure is the most popular in the MT field and can be considered the state of the art in a number of simple scenarios. However, the BH procedure still suffers from undesired conservativeness since the FDR of the procedure is actually equal to $\pi_{0} \alpha \leq \alpha$, where $\pi_{0} denotes the proportion of true nulls.$
%%\end{itemize}

%\seb{Iqraa's text (still to be merged with the above)}
%
%Multiple Testing (MT) has gained significant attention since the work of \cite{BH95} introducing the False Discovery Rate (FDR), a more liberal error criterion that allow that allows for a greater number of errors in proportion to the number of discoveries made.  Alongside this new criterion \cite{BH95} also proposed a procedure called the BH procedure to control the FDR at a prespecified level $\alpha \in (0, 1)$. The BH procedure is widely known in the MT field and is often preferred procedure in a lot of scenarios. However, the procedure suffers from undesired conservativeness that can limit its power to make interesting discoveries. Indeed, the FDR of the procedure is actually equal to $\frac{m_{0}}{m} \alpha$, where $\frac{m_{0}}{m} = \pi_{0}$ denotes the proportion of true nulls, which can be much smaller than $\alpha$ when $\pi_{0}$ is small. One way to address this issue is to incorporate $\pi_{0}$ in the testing thresholds to uplift the FDR of the procedure to $\alpha$. In practice, $\pi_{0}$ is unknown so an estimate $\hat{\pi}_{0}$ is incorporated instead, designing an adaptive BH procedure. The issue now is of providing good estimates of $\pi_{0}$ while guaranteeing the FDR control of the adaptive procedure. Indeed, the estimate is generally built using the same date as for the testing adding more randomness into the procedure that might jolt the FDR control.

%%\begin{itemize}
%%	\item How to verify the FDR control quickly ? The \cite{BR2009} \cite{benjamini2006adaptive} condition: sufficient condition for the FDR control of the adaptive BH procedure. This condition is a condition on the inverse moment of the estimator 
%%	\item Adaptive procedure ongoing area of research, here focus on p-values based MT so estimation methods are also p-value based. Most popular work is the one of \cite{Storey2004}, uses a user defined parameter $\lambda \in (0, 1), count the number of p-values above that $\lambda$ to consider them as true nulls, verifies the BR condition 
%%	\item Another widely known estimate is the one proposed by \cite{PC2006}, no need to use additional parameter tuning, should be better for one sided testing, BUT do not verifies the BR condition, and more broadly speaking do not has the FDR control guarantee. 
%%	\item Modification of this estimator have been proposed, to provided FDR control, but modifications break the simplicity of the base estimator involving user defined parameter see the work of \cite{ZZD2011} ....
%%	\item sentence about the importance of FDR control: cite application 
%%\end{itemize}

%Along with the requirement of simpleness for the estimator and conservativeness for the plug-in BH procedure, an estimator should be efficient to improve the detection of interesting variables, but this feature can also be harmed when dealing with difficult data structure as it is the case for discrete data. 

%\begin{itemize}
%	\item Discrete data naturally appear in applications involving counts 
%	\item ref of papers with discrete testings 
%	\item Basic example : contingency tables with FET, but also Poisson tests ...
%	\item Discrete $p$-values are super-uniform instead of uniform (see Section~\ref{sec:distribassumption}), which make them more difficult to test, they are naturally more conservative then needed, but have distrib info available 
%\end{itemize}

Discrete tests often originate when the tests are based on counts or contingency tables:
for example in clinical studies, the efficacy or safety of drugs is determined by counting patients who survive a certain period, or experience a certain type of adverse drug reaction after being treated, see e.g. \cite{chavant2011memory}; 
and also in biology, where the genotype effect on the phenotype can be analyzed by knocking out genes and counting the number of individuals with a changed phenotype, see e.g. \cite{munoz2018international}.
In discrete testing, each $p$-value is super-uniform and (potentially) has its own support, thus producing heterogeneous $p$-values. \cite{PC2006} recognized the need for developing methods tailored  to discrete $p$-values and  introduced $\mPCOrig$  as a simple and robust $m_0$-estimate in this setting. They did not, however, provide a proof of plug-in FDR control, not even in  the uniform setting. Further works adressing the discreteness and heterogeneity include \cite{chen2018multiple} who introduced a $m_0$-estimator for discrete $p$-values based on averaging Storey type estimators for plug-in control. \cite{Biswas2020} pointed out an error in the proof of \cite{chen2018multiple} and provided a corrected version. 
However it is unclear whether this estimator actually provides an improvement over the  classical (uniform) Storey estimator in practice. Thus, there are gaps to be filled on $m_0$-estimation both for the uniform and discrete case.
%These two characteristics are not taken into account by classical estimators like \eqref{def:m0:Storey} or \eqref{def:m0:PC:2006}, which is why 
%\iq{Nevertheless, 
%Some recent work addresses the super-uniformity issue. %Indeed, \cite{PC2006} introduced their estimator~\eqref{def:m0:PC:2006} and an associated discrete estimator with a view toward estimating FDR, but make no statement about plug-in FDR control. 

%\iq{\cite{chen2018multiple} introduced a \seb{$m_0$-estimator} method for discrete $p$-values based on averaging Storey type estimators for plug-in control, however \cite{Biswas2020} pointed out an error in the proof and provided a corrected version but for which plug-in FDR control is met only under a strong assumption that is hard to met \seb{verify/check?} in practice.
%Therefore, providing suitable discrete estimators with proven FDR control is still motivated and needed. 
%}
%\seb{Thus, there are gaps to be filled on $m_0$-estimation both for the uniform and discrete case.}
%\iq{but all of them fail to provide a realistic adaptive FDR control \footnote{TO DO iqraa : check}.}
%}
% and negatively affect the efficiency of any statistical process involving them such as the estimation process we are interested in. \\
%Ref for discrete estimators 
%\begin{itemize}
%	\item PC (deal with discreteness and one sided pvalues)
%	\item Biswas and Chattopadhyay ( average of Storey type estimators in the same spirit as Liang and Nettleton)
%	\item Chen Doerge Sarkar 
%\end{itemize}




%MOTIVATE THE GENERAL CLASS 

%To verify the FDR control of the adaptive procedure, a sufficient condition (formally defined in Theorem~\eqref{thm:IMC}) is to bound the inverse moment of the estimator as proposed in the work of \cite{BR2009} and \cite{benjamini2006adaptive}. 
%Although, the condition is only sufficient, it has the benefit to be simple and to focus solely on the estimator without reconsidering the entire testing procedure. 
%The development of true nulls proportion estimators is an ongoing area of research, with the most popular work being the one of \cite{Storey2004} where the estimator is build by counting $p$-values above a certain prespecified threshold $\lambda \in (0, 1)$ (see \eqref{def:m0:Storey}). 
%This estimator verifies easily the inverse moment condition (see \cite{BR2009} for a simple proof) and is intuitive. 
%However if the additional parameter $\lambda$ is poorly chosen, the overall adaptive procedure will not bring any improvement for the power. 
%This motivates the use of estimator without any parameter tuning as the one proposed by \cite{PC2006} who use the empirical average of the $p$-values. 
%However, this estimator does not come with guarantee for the adaptive BH procedure, which can restrain the willingness to use this estimator. Following, the work of \cite{ZZD2011} (abbreviated in the following as \citetalias{ZZD2011}) has focused on providing a guarantee for a modified version of \cite{PC2006}'s estimator, but at the price of a more complicated estimator involving more than one parameter tuning (see \eqref{def:PCZZD}). Additionally, the efficiency of estimators to improve the detection of meaningful variables may also be compromised when dealing with difficult data structures, such as discrete data.

%
%PUT SOME OF THSE ON THE DISCRETE PARAGRAPH 
%To quantify the quality of an estimator we can compare the bias. 
%Indeed, with Jensen's inequality, we know that an estimator verifying Theorem~\eqref{thm:IMC} is necessarily upwardly biased, 
%however the excess amount of overestimation can vary with the nature of the $p$-value data. 
%%Nonetheless, by comparing the excess amount of overestimation, the bias can still provide some insights about the quality of an estimator. 
%Thus, in the discrete setting 
%%where $p$-values are super-uniform (see Section~\ref{sec:distribassumption}) instead of uniform under the null, 
%the excess amount of the bias is inflated in comparison to the classic uniform setting, see Section~\eqref{sec:discretestimators} for an explicit comparison between both settings for the Storey estimator. 
%This inflation comes from the super-uniformity of the $p$-values which does not solely happen in the discrete setting, but the latter provide distributional information about the $p$-values under the null that can be used 
%to adapt the estimators to better suit the discrete data. To our knowledge, such adaptation has not been thoroughly studied with the only notable work being the one of  \ref{PC2006} and \iq{other refs ?} .	





%In this work we propose ...
%
%\iq{Should we move the BH procedure def and the motivating point for estimation in the introduction ? \\}
%\seb{Done}
%
%\seb{To Do: Define FDR}
%\seb{Reference to Schweder-Spotvoll}
%
%\seb{To Do: 'Adaptive BH procedure'}


%\iq{Why do we have this equation here ? }\seb{i removed the equation, hope it's ok now}






%{\color{gray}
%\begin{itemize}
%	\item This works focuses on the discrete setting 
%	\item motivate use of discrete testing : when does it happens, happens a lot, 	
%	\item How to better take into account the intrinsic nature of the data when estimating the proportion of nulls ? For instance when dealing with discrete date, we know that the p-values are super-uniform so more difficult to test but we still have info about the p-value distribution, how to use that info? 
%	\item Only work to our knowledge is the one of \cite{PC2006} but as stated before, does not come with guarantee for the FDR of the plug-in procedure.
%\end{itemize}
%
%\begin{itemize}
%	\item What we propose to do : our contributions 
%	\item provide a slightly modified version of PC estimator, no additional tuning parameter, that verifies the BR condition 
%	\item provide transformation of known estimator to better suit discrete data, and comes with simple verifications of the BR condition 
%	\item general recipe to design estimator verifying the BR condition ? 
%	\item Simulations to compare, and try on real data application 
%\end{itemize}
%}



%%Multiple inference is a crucial issue in many modern, high dimensional, and massive data sets, for which a large amount of variables are considered altogether questions naturally emerge simultaneously.
%%To tackle this issue, the domain of Multiple Testing (MT) provide statistical methods to achieve these inferences while maintaining the number of error one can do under a certain level. 
%%One of the most widely known method is the one introduced by \cite{BH95} (so called the BH method hereafter) which allows to test 
%%
%%Multiple Testing (MT) is a statistical method allowing researchers to investigate modern data sets to answer question related to health, sociology etc 
%%MT regain a new interest since the work of \cite{BH95} introducing the False Discovery Rate (FDR) as a new metric to quantify the type I error and a procedure to control it, the so called BH procedure.
%%The BH procedure works by first sorting in ascending order the $p$-values and then selecting the $\hat{k}$ first $p$-values, with $\hat{k} = \max \{1 \leq k \leq m,  p_{(k)} \leq \frac{\alpha k}{m} \}$, where $p_{(k)}$ denotes the 
%%k-th smallest $p$-value, $m$ the number of hypotheses tested, and $\alpha$ the level of control desired. Although, the BH procedure is the standard procedure used to deal with multiplicity while allowing for discoveries, it is still stringent because the type I error is in fact controlled at level $\frac{\alpha m_{0}}{m}$, which can be way less smaller than $\alpha$ when $m_{0}$, the number of null hypotheses, is small. To deal with this unnecessary stringency, one can incorporate $m_{0}$ in the critical levels thus providing a type I error control at the desired level $\alpha$. Since the underlying truth is in general not available, $m_{0}$ remains unknown and needs to be estimated. \\
%%
%%The estimation of the number of null hypotheses is a line of research very active dating back to the work of \cite{} who propose to estimate $m_{0}$ by counting the $p$-values too large with respect to a threshold $\lambda$ chosen by the user. The plug-in Storey-BH procedure effectively provides a control at level $\alpha$ and possibly allows to make more discoveries when $m_{0}$ is small. More recently, \cite{} proposed another estimator by summing the raw p-values (what's the rational behind doing that ?). However, this estimator has no proved guarantee as for the BH plug-in procedure using this estimator. One way to 

%Literature:
%\begin{itemize}
%	\item Krieger et al.
%	\item BR
%	\item Sarkar ?
%	\item PC
%	\item Storey
%	\item Zeisel
%\end{itemize}

%%
%%Much efforts have been done to improve the BH procedure in terms of power by adapting the procedure to 
%%leverage more informations contained in the data. One idea is to adapt the critical levels used for the individual decision to be more or less stringent 
%
%%\paragraph{Contributions} 
%%In this work we propose, first, to provide a proof for the conservativeness of the BH procedure plug-in with the PC estimator based on the BR criterion. 
%%Then, we propose three new estimator designed for taking into account the discrete nature of the data either by using the CDF of the p-values, or by using transformations like mid-pvalues or randomized p-values.


\subsection{Contributions}
{In this paper we address some of the gaps and limitations  mentioned above by introducing a simple and flexible class of $m_{0}$ estimators which has the following properties:}
%\iq{In this paper, we propose to address the gaps and limits mentioned earlier.}
%\iq{For that, our main contribution is to introduce a simple and flexible class of $m_{0}$ estimators which has the following properties:}
{
%Our main contribution is the introduction of a simple, yet flexible class of estimators for $m_0$ which has the following properties:
	\begin{itemize}
		\item Plug-in FDR control is guaranteed for all estimators contained in this class under independence of $p$-values. We give a unified proof using simple convex ordering arguments {(for the reader's convenience we restate some definitions and classical results on stochastic and convex ordering in Appendix~\ref{appendix:auxres})}.
		\item {It provides a simple and flexible generic formulation {which is useful for designing } new estimators.} In particular, we obtain a simple modification of $\mPCOrig$ with guaranteed plug-in FDR control.
		\item Additional distributional information  on the $p$-values like heterogeneity and super-uniformity can be incorporated easily into estimators from the class.
		{In particular, the estimators of this class can be used in conjunction with classical discrete $p$-value transformations like the mid-p transformation.}
		%\item {It is flexible enough to {incorporate additional distributional information (e.g. heterogeneity, super-uniformity) on the $p$-values} }
%		everage available distributional assumption (e.g. heterogeneity, super-uniformity) of $p$-values;}
		\item {Combining several weighted estimators from the class preserves {plug-in} FDR control.} 
		% When weighted estimators from the class are added together, plug-in FDR control is preserved.
%			\item Using the general result, statements on plug-in FDR control follow easily not only for cases where this has already been established in the past, but also for cases like the Pounds and Chen estimator, where the situation is less understood. In addition, new specific plug-in estimators that come with mathematical guarantees can be constructed easily. 
%			\item Both homogeneous and heterogeneous $p$-values under the null can be incorporated into estimators. By allowing local  heterogeneity the estimators are suited for settings where additional (covariate) information on the null distribution of the $p$-values is available. In Section \ref{sec:discretestimators} we present two approaches for adapting arbitrary plug-in estimators that are valid under \eqref{superunif}\footnote{check} to discrete data. 
	\end{itemize}
}

The paper is organized as follows: Section \ref{sec:setting} presents the statistical setting and {restates} {a classical sufficient} criterion for plug-in FDR control. 
Section \ref{sec:uniform} introduces the new class of estimators, and presents the main mathematical results on plug-in FDR control, followed by some numerical results
%\footnote{maybe include a real data example? maybe estimation path for BRAC data?}  
in Section \ref{sec:example:homogeneous}.
In Section~\ref{sec:discretestimators} we present approaches for adjusting estimators to discreteness, {and investigate their performance on simulated and real data. 
%The following two sections investigate the case of continuous and discrete tests and present results both for simulated and real data. 
%For discrete tests we present two generic approaches for adapting estimators that were designed for continuous $p$-values (with uniform sitribution under the nulls) to the discrete case. 
The paper concludes with a  discussion in Section \ref{sec:conclusion}. {Technical details -- including classical results on stochastic and convex ordering-- and further analyses are deferred to the Appendices}.



%\iq{Should we reformulated these, and put it in the introduction to state our contribution? }
%\seb{i suggest to do this later.}
%Purpose of this work is two-fold:
%\begin{enumerate}
%	\item Continuous case: We propose the following simple modification of $\mPCOrig$
%	\begin{align}
%	\mPCNew &= 2 + 2 \cdot \sum_{i=1}^m p_i \label{eq:def:m0:PC:New}
%	\end{align}
%	which requires no ... and provide a short proof of \ref{eq:IMC}.  
%	We also give some result for a new estimator... and provide a new and short proof of another estimator proposed by \cite{ZZD2011}. 
%	Our proofs are based on techniques and results from stochastic ordering that  allow ... practical and mathematical/aesthetical aspects...
%	\item Discrete case: Often we have to deal with discrete $p$-values. ... conservativeness  ... We investigate several approaches for mitigating the inherent conservativeness when discrete $p$-values are used for estimating 	$m_0$. It is clear, that for classical estimators like $\mStorey$ and  $\mPCOrig$ naively using the discrete $p$-values ...
%\end{enumerate}
%
%\iq{ Where should we introduce these notations : 
%	\begin{itemize}
%		%	\item Denote $p$-values $p_{1}, \dots, p_{m}$, with $p_{i} \in (0, 1),$ for all $i \geq 1$
%		%	\item Denote $\nullset$ as the set of true null hypotheses with $|\nullset| = m_{0}$
%		%	\item Denote by $p_{-h}$ the collection of p-values $p$ restricted to $\mathcal{H} \backslash {h}$ , that is, $p_{h} = (p_{h'}, h' \neq h)$. 
%		%	\item Denote $p_{0, h} = (p_{-h}, 0)$ the collection $p_{h}$ where $p_{h}$ has been replaced by 0.
%		%	\item Denote $G : \left[ 0, 1\right]^{\mathcal{H}} \rightarrow (0, \infty)$ an estimator of $\pi_{0}^{-1}$, where $\pi_{0} = \frac{m_{0}}{m}$ denotes the proportion of true nulls.
%		%	$G$ is assumed to be a measurable and coordinate-wise non-increasing function.
%		\item The symbol $\stoorder$ denotes the stochastic order. 
%		\item The symbol $\cxorder$ denotes the convex order.
%	\end{itemize}
%}

