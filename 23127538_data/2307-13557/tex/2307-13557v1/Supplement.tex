\pagebreak
\section{Auxiliary definitions and results}\label{appendix:auxres}
%Here we present some definitions and results from stochastic ordering.
%Bounds for inverse moments of sume distributions.
%We follow the 
%assume univariate r.v
%Definitions and results from \citetalias{shaked2007stochastic}
In this appendix we recall some definitions and results of stochastic ordering following the presentation in  \citetalias{shaked2007stochastic}, to which we also refer the reader for further details.
We also recall a well-known bound on the inverse moment of the Binomial distribution.
\begin{definition}[Stochastic order]
	Let $X$ and $Y$ be two random variables such that
	\begin{align*}
		\P(X > x) \leq \P(Y > x) \quad \text{for all } x \in (-\infty, \infty),
	\end{align*}
	Then $X$ is said to be smaller than $Y$ in the usual stochastic order denoted by $X \stoorder Y$.
\end{definition}
	
An equivalent characterization of the stochastic order is that  $X \stoorder Y$ $\Leftrightarrow$ $\E[g(X)] \leq \E[g(Y)]$, for  all  non-decreasing functions $g : \R \rightarrow \R$ for which the expectations exist (see (1.A.7) in \citetalias{shaked2007stochastic})).	
%\begin{corollary}\label{app:cor:stoorder}
%	
%\end{corollary}

\begin{definition}[Convex order] \label{app:def:cxorder}
	Let $X$ and $Y$ be two random variables such that 
		\begin{align*}
		\E(\phi( X)) \leq \E(\phi( Y)) \quad \text{for all convex functions } \phi : \R \rightarrow \R,
	\end{align*}
	provided the expectations exist. Then $X$ is said to be smaller than $Y$ in the convex order denoted as $X \cxorder Y$.
\end{definition}

The next results follows from the definition of convex ordering, see Chapter~3 of \citetalias{shaked2007stochastic}.
%The convex ordering has a number of characterization
%We restate here the ones we use in the paper.

\begin{lemma}[Theorem 3.A.24 in \citetalias{shaked2007stochastic}] \label{app:lemma:SS:3-A-24}
	Let $X$ be a random variable with mean $\E X$. 
	Denote the left (right) endpoint of the support of $X$ by $l_X\left[u_X\right]$. 
	Let $Z$ be a random variable such that $\P\left\{Z=l_X\right\} = \left(u_X-\E X\right) /\left(u_X-l_X\right)$ and $\P\left\{Z=u_X\right\}=(\E X-$ $\left.l_X\right) /\left(u_X-l_X\right)$. 
	Then
	$$
		\E X \cxorder X \cxorder Z
	$$
	where $\E X$ denotes a random variable that takes on the value $\E X$ with probability 1 (the left handside just restates Jensen's inequality).	
\end{lemma}


\begin{lemma}[Theorem 3.A.44 in \citetalias{shaked2007stochastic}] \label{app:lemma:SS:3-A-44}
	Let $X$ and $Y$ be two random variables with equal means, density functions $f$ and $g$, distribution functions $F$ and $G$, and survival functions $\bar{F}$ and $\bar{G}$, respectively. 
	Denote by $S^{-}(a)$ the number of sign changes for function $a$. 
	Then $X \leq_{\mathrm{cx}} Y$ if any of the following conditions hold:
	\begin{align*}
		& S^{-}(g-f)=2 \mbox{ and the sign sequence is } +,-,+;\\ 
		& S^{-}(\bar{F}-\bar{G})=1 \mbox{ and the sign sequence is } +,- ;\\ 
		& S^{-}(G-F)=1 \mbox{ and the sign sequence is } +, -.
	\end{align*}
\end{lemma}

\begin{proposition}[Theorem 3.A.12 d) in \citetalias{shaked2007stochastic}] \label{app:lemma:SS:3-A-12}
	Let $X_1, X_2, \ldots, X_m$ be a set of independent random variables and let $Y_1, Y_2, \ldots, Y_m$ be another set of independent random variables. 
	If $X_i \cxorder Y_i$ for $i=1,2, \ldots, m$, then
	$$
		\sum_{j=1}^m X_j \cxorder \sum_{j=1}^m Y_j .
	$$
	That is, the convex order is closed under convolutions.
\end{proposition}


\begin{lemma} [Example 3.A.48 in \citetalias{shaked2007stochastic}] \label{app:lemma:SS:3-A-48}
	Let $X$ and $Y$ be Bernoulli random variables with parameters $p$ and $q$, respectively, with $0<p \leq q \leq 1$. 
	Then
	$$
		\frac{X}{p} \geq_{\mbox{cx}} \frac{Y}{q}.
	$$
%	Let $X_1, X_2, \ldots, X_n$ be nonnegative exchangeable random variables, and let $I_{p_1}, I_{p_2}, \ldots, I_{p_n}$ and $I_{q_1}, I_{q_2}, \ldots, I_{q_n}$ be independent Bernoulli 	random variables that are independent of $X_1, X_2, \ldots, X_n$. If $\boldsymbol{p} \prec \boldsymbol{q}$, then
%	$$
%	 \sum_{i=1}^n I_{q_i} X_i \cxorder \sum_{i=1}^n I_{p_i} X_i 
%	$$
\end{lemma}

\begin{lemma}[Inverse moment for the Binomial distribution]\label{lemma:IM:exp:bin}
Let $B_1, \ldots, B_k \sim \Bin(1,q)$. Then $\E [ 1/(1+\sum_{i=1}^k B_i) ]\le 1/((k+1)q)$.
\end{lemma}

\begin{proof} See e.g. \cite{benjamini2006adaptive}.
\end{proof}	

%\begin{corollary}
%	In the setting  of Proposition \ref{prop:plugin:discrete:control:general:g}  assume additionally that 
% 	$p_1, \ldots, p_m$ are  super-uniform under the null (i.e. \eqref{superunif} holds). Define the estimator
% 	\begin{align*}
%		 \widetilde{m}^{\text{a}}_0 (p_1, \ldots, p_m)&= \frac{1}{\nu }+ \sum_{i=1}^m \frac{g(p_i)}{\nu^{\text{a}}_i}.
%	 \end{align*}
%  	Then the BH plug-in procedure  \eqref{eq:khat:BH} using $\widetilde{m}^{\text{a}}_0$ controls FDR at level $\alpha$
%	
%\end{corollary}
%
%\begin{proof}
%	If the $F_i$ are super-uniform we have (under each null) $U \stoorder p_i$, where $U \sim \unifrv [0,1]$ and since $g \in \mathcal{G}$ is non-decreasing this yields $\nu^{\text{a}}_i = \E g(p_i) \ge \E g(U) = \nu $ for all $i$ and thus $\widetilde{m}^{\text{a}}_0 \ge \widehat{m}^{\text{a}}_0$.
%\end{proof}


\section{Complements to Section \ref{ssec:numerical:results}} \label{appendix:sec:one:sided:gaussian:testing} 

In the context of Gaussian one-sided testing described in Section~\ref{ssec:numerical:results}, let $\widehat{m}_0 (p_1, \ldots, p_m) = \frac{1}{\nu} \left(1+ \sum_{i=1}^m g(p_i) \right)  \in \mathcal{F}_0$. Define $X_0 \sim g(p_i)$ for $i \in \nullset$ and $X_1 \sim g(p_i)$ where $ i \in \altset$. Then we have
\begin{align*}
	\bias (\widehat{m}_0) &= \frac{1}{\nu} \E \left(1+ \sum_{i \in \altset}g(p_i)\right)=(1+(m-m_0)\cdot \E X_1)/\nu,\\
	\var (\widehat{m}_0) &= \frac{1}{\nu^2} \var \left( \sum_{i \in \nullset}g(p_i)+ \sum_{i \in \altset}g(p_i)\right)= (m_0 \cdot \var(X_0)+ (m-m_0)\cdot \var(X_1))/\nu^2, \qquad \text{with}\\
	\var(X_0) &= \int_{0}^1 g(u)^2 du - \left[\int_{0}^1 g(u) du\right]^2,\\
	\var(X_1) &= \int_{0}^1 g(u)^2 f_1(u) du - \left[\int_{0}^1 g(u) f_1(u) du\right]^2.
\end{align*}
where $f_1(t)= \exp \left(- \mu \cdot \Phi^{-1}(t)-\mu^2/2\right)$ denotes the density of the $p$-values under the alternative.



\section{Complements to Section \ref{ssec:ComparePCNew:PCZZD}} \label{appendix:ssec:ComparePCNew:PCZZD}

Here we present some numerical results, comparing the performance of $\mPCNew$ (see \eqref{eq:def:PC:new}) and $\mZZKB$ (see \eqref{def:PCZZD}) for $m=500$, where the correction factors $C(500)=1.011709$ and $s(500)=98$ are taken from Table S1 in \cite{ZZD2011}. 

We first analyze the two estimators on simulated data in a one-sided Gaussian testing setting where we observe realizations of independent rv's $X_{1}, \dots, X_{m_0} \sim N(0,1)$ and $X_{m_0 +1}, \dots, X_{500} \sim N(1.5,1)$ for $1000$ Monte-Carlo simulation runs and a varying range of $m_0 =50,100, \dots, 450$. 
We obtain $500$ $p$-values by testing the null hypotheses $H_{0, i} : \mu = 0$ vs. the alternatives $H_{1, i} : \mu  > 0$ simultaneously for all $i \in \{1, \dots, 500\}$ and calculate $\mPCNew$ and $\mZZKB$ as well as the number of rejections obtained from the plug-in BH procedure in \eqref{eq:khat:BH} with $\alpha=0.05$. 
\begin{center}
	% Figure environment removed
\end{center}
Figure~\ref{fig:PCzzd_vs_our} shows that over a wide range of true $m_0$ values, $\mPCNew$ and $\mZZKB$ yield comparable results both w.r.t. the point estimates and for the number of  rejections. 
In fact, $\mPCNew$ appears to be slightly more efficient than $\mZZKB$.
 
Another comparison can be obtained when we assume that the signal under the alternative is strong and that most hypotheses are nulls. 
In this case  we have $2 \sum_{i =1}^m p_i \approx 2 \sum_{i \in \nullset} p_i =:S$ so that we can use the Central Limit Theorem to quantify the probability that $\mPCNew$ is more conservative than $ \mZZKB$
\begin{align*}
	\P (\mPCNew > \mZZKB) & = \P(S > m \cdot C(m) - 2) \approx \overline{\Phi} \left(\sqrt{\frac{3}{m_0}}\cdot (m\cdot C(m)-(m_0+2))\right).
\end{align*}
Figure \ref{fig:comparisonzzd} shows that this probability, for various values of the true $m_{0}$, is quite small and even under the complete null ($m_{0} = 500$) it is bounded by $1/3$.

% Figure environment removed	
 

%An alternative proof of plug-in control for $\mPCNew$ using the exponential distribution, instead of the Bernoulli distribution, as a bounding device is also possible (see \ref{label}\footnote{to do seb : in the appendix: exponential bound does not require bounded $g$ + the exponential distribution can also be used as a bounding device to give a stremalined proof for ZZD's exponential estimator.}). 
 
 

%\subsection{Additional Figures for numerical calculations}

\section{Additional Figures for simulated data of Section~\ref{sec:example:homogeneous}}
We provide additional results on simulated data in the Gaussian one-sided testing setting described in Section~\ref{ssec:numerical:results}, with $m=10000$ and $\mu=1.5$.
Figure~\ref{fig:gaussian_simu} displays estimation results for  $\pi_{0}$ over $1000$ Monte-Carlo replications.  
They are in line with the analytical comparisons of the MSE provided in Figure~\ref{fig:gaussian:mse}. 
Alongside, we also provide results on power, defined as the ratio of the number of true discoveries to the number of alternatives, for the corresponding plug-in BH (abbreviated in ABH for adaptive BH) procedures using each of the estimators, the raw BH and oracle plug-in BH (using the true $m_{0}$). 
The procedures are run for a fixed level $\alpha = 0.05$.
%The power results show that the raw BH provides the worst performance while the oracle BH provides the best performance as expected.
{The power enhancement among the different plug-in estimators' is not striking except perhaps for very small values of $\pi_{0}$ were where we recover the same performance ranking as in Figure~\ref{fig:gaussian:mse}. 
For larger values of $\pi_{0}$, the differences in power is not perceptible anymore, every procedure behaves poorly as there is less and less signal.}
%\footnote{todo: BH worst, oracle best, plug-in in between, however for $pi>1/2$ everything is bad...}
\begin{center}
	% Figure environment removed
\end{center}

%\subsection{Additional Figures for simulated data os Section~\ref{ssec:simu:discrete}}

\section{Upper and lower bounds for the inverse moment of the uniform sum distribution }

The Pounds and Cheng estimator is closely related to the sum of independent uniform random variables. 	
This distribution plays a role in various contexts and is also known as the \emph{Irwin-Hall} distribution (for more details, see \cite{JohnsonKotz1970}). 
As an auxiliary result, we give lower and upper bounds for the inverse moment of this distribution.
\begin{lemma}[Inverse moments for Erlang distributions]\label{lemma:IM:Erlang}
	Let $E_1, \ldots, E_k \sim \expov(1)$ be independent exponentially distributed random variables. 
	Then
	 $\E [ 1/\sum_{i=1}^k E_i]\le 1/(k-1).$	
\end{lemma}

\begin{proof} Since $X=\sum_{i=1}^k E_i$ is Gamma-distributed with shape  $\alpha=k$ and  inverse scale parameter $\beta=1$  then $1/X$ is Inverse-gamma distributed with mean $\beta/(\alpha-1)$, see \cite{gelman2013bayesian}.
\end{proof}	

\begin{proposition}(Inverse moment for sums of uniforms)\label{prop:IMSumUnif}
	For $k \ge 2$ let $U_1, U_2, \ldots,U_k \sim \unifrv[0,1] $ iid. Then we have
	\begin{align}
	\frac{2}{k} &\le \E \left( \frac{1}{\sum_{i=1}^{k} U_i}\right) \le 	\frac{2}{k-1}
	\end{align}
\end{proposition}

\begin{proof} Let $E_1,E_2, \ldots,E_k \sim \expov(1)$ iid. From Theorems 3.A.24 and 3.A.46 in \citetalias{shaked2007stochastic} we have for $i=1, \ldots, ,k$
	\begin{align*}
	1 & \cxorder 2U_i \cxorder E_i
	\intertext{and since the  convex ordering is preserved under convolutions (see \citetalias[Theorem 3.A.12.]{shaked2007stochastic}) we obtain}
	k & \cxorder \sum_{i=1}^{k} 2U_i \cxorder \sum_{i=1}^{k} E_i. 
	\intertext{Together with the convexity of the mapping $x \mapsto 1/x$ on $(0,1)$ this yields}
	\frac{1}{k} &\le \E \left( \frac{1}{\sum_{i=1}^{k} 2U_i}\right) \le \E \left( \frac{1}{\sum_{i=1}^{k} E_i}\right) \le	\frac{1}{k-1},
		\end{align*}
		where the last inequality follows from  Lemma \ref{lemma:IM:Erlang}.
\end{proof}

