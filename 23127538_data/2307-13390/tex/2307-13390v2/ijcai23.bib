@book{ abelson-et-al:scheme,
  author = "Harold Abelson and Gerald~Jay Sussman and Julie Sussman",
  title = "Structure and Interpretation of Computer Programs",
  publisher = "MIT Press",
  address = "Cambridge, Massachusetts",
  year = "1985"
}


@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},



booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}
@inproceedings{ bgf:Lixto,
  author = "Robert Baumgartner and Georg Gottlob and Sergio Flesca",
  title = "Visual Information Extraction with {Lixto}",
  booktitle = "Proceedings of the 27th International Conference on Very Large Databases",
  pages = "119--128",
  publisher = "Morgan Kaufmann",
  address = "Rome, Italy",
  month = "September",
  year = "2001"
}

@article{ brachman-schmolze:kl-one,
  author = "Ronald~J. Brachman and James~G. Schmolze",
  title = "An overview of the {KL-ONE} knowledge representation system",
  journal = "Cognitive Science",
  volume = "9",
  number = "2",
  pages = "171--216",
  month = "April--June",
  year = "1985"
}

@article{ gottlob:nonmon,
  author = "Georg Gottlob",
  title = "Complexity results for nonmonotonic logics",
  journal = "Journal of Logic and Computation",
  volume = "2",
  number = "3",
  pages = "397--425",
  month = "June",
  year = "1992"
}

@article{ gls:hypertrees,
  author = "Georg Gottlob and Nicola Leone and Francesco Scarcello",
  title = "Hypertree Decompositions and Tractable Queries",
  journal = "Journal of Computer and System Sciences",
  volume = "64",
  number = "3",
  pages = "579--627",
  month = "May",
  year = "2002"
}

@article{ levesque:functional-foundations,
  author = "Hector~J. Levesque",
  title = "Foundations of a functional approach to knowledge representation",
  journal = "Artificial Intelligence",
  volume = "23",
  number = "2",
  pages = "155--212",
  month = "July",
  year = "1984"
}

@inproceedings{ levesque:belief,
  author = "Hector~J. Levesque",
  title = "A logic of implicit and explicit belief",
  booktitle = "Proceedings of the Fourth National Conference on Artificial Intelligence",
  publisher = "American Association for Artificial Intelligence",
  pages = "198--202",
  address = "Austin, Texas",
  month = "August",
  year = "1984"
}

@article{ nebel:jair-2000,
  author = "Bernhard Nebel",
  title = "On the compilability and expressive power of propositional planning formalisms",
  journal = "Journal of Artificial Intelligence Research",
  volume = "12",
  pages = "271--315",
  year = "2000"
}

 @misc{proceedings,
  author = {{IJCAI Proceedings}},
  title = {{IJCAI} Camera Ready Submission},
  howpublished = {\url{https://proceedings.ijcai.org/info}},
}


@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

\section{Conclusion}
\label{sec:con}
This paper presents a novel model-agnostic algorithm for finding CEs via linear interpolation in latent space. Our method implements a framework that searches in a Gaussian mixture distributed latent space of an auto-encoder for CEs given a query sample. We demonstrated our methodâ€™s advantages and disadvantages by comparing it to two similar methods on three different datasets (MNIST, Adult income, and Lending Club default loan). We show that our method is faster and produces CEs closer to the original dataset. 

Based on the presented comparison, we suggest that our work could be improved by (1) disentangling relevant dimensions in latent space from the irrelevant ones for the predicted label \cite{hadad2020,zheng2019} and only enforcing the Gaussian mixture distribution to focus on the relevant dimensions so that the original characteristics of the query sample can be preserved better during CE search, (2) improving the latent representation and interpolation via an adversarial regularizer \cite{berthelot2018}, and (3) adding sparsity and proximity constraints to the optimization function. 
   %For example, we could change the architecture to include 


@inproceedings{amini2019,
  title = {Uncovering and {{Mitigating Algorithmic Bias}} through {{Learned Latent Structure}}},
  booktitle = {Proceedings of the 2019 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Amini, Alexander and Soleimany, Ava P. and Schwarting, Wilko and Bhatia, Sangeeta N. and Rus, Daniela},
  year = {2019},
  month = jan,
  series = {{{AIES}} '19},
  pages = {289--295},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3306618.3314243},
  abstract = {Recent research has highlighted the vulnerabilities of modern machine learning based systems to bias, especially towards segments of society that are under-represented in training data. In this work, we develop a novel, tunable algorithm for mitigating the hidden, and potentially unknown, biases within training data. Our algorithm fuses the original learning task with a variational autoencoder to learn the latent structure within the dataset and then adaptively uses the learned latent distributions to re-weight the importance of certain data points while training. While our method is generalizable across various data modalities and learning tasks, in this work we use our algorithm to address the issue of racial and gender bias in facial detection systems. We evaluate our algorithm on the Pilot Parliaments Benchmark (PPB), a dataset specifically designed to evaluate biases in computer vision systems, and demonstrate increased overall performance as well as decreased categorical bias with our debiasing approach.},
  isbn = {978-1-4503-6324-2},
  keywords = {algorithmic bias,deep learning,facial detection,neural networks}
}

@article{balasubramanian2021,
  title = {Latent-{{CF}}: {{A Simple Baseline}} for {{Reverse Counterfactual Explanations}}},
  shorttitle = {Latent-{{CF}}},
  author = {Balasubramanian, Rachana and Sharpe, Samuel and Barr, Brian and Wittenbach, Jason and Bruss, C. Bayan},
  year = {2021},
  month = jun,
  journal = {arXiv:2012.09301 [cs]},
  eprint = {2012.09301},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In the environment of fair lending laws and the General Data Protection Regulation (GDPR), the ability to explain a model's prediction is of paramount importance. High quality explanations are the first step in assessing fairness. Counterfactuals are valuable tools for explainability. They provide actionable, comprehensible explanations for the individual who is subject to decisions made from the prediction. It is important to find a baseline for producing them. We propose a simple method for generating counterfactuals by using gradient descent to search in the latent space of an autoencoder and benchmark our method against approaches that search for counterfactuals in feature space. Additionally, we implement metrics to concretely evaluate the quality of the counterfactuals. We show that latent space counterfactual generation strikes a balance between the speed of basic feature gradient descent methods and the sparseness and authenticity of counterfactuals generated by more complex feature space oriented techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{barr2021,
  title = {Counterfactual {{Explanations}} via {{Latent Space Projection}} and {{Interpolation}}},
  author = {Barr, Brian and Harrington, Matthew R. and Sharpe, Samuel and Bruss, C. Bayan},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.00890 [cs]},
  eprint = {2112.00890},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Counterfactual explanations represent the minimal change to a data sample that alters its predicted classification, typically from an unfavorable initial class to a desired target class. Counterfactuals help answer questions such as "what needs to change for this application to get accepted for a loan?". A number of recently proposed approaches to counterfactual generation give varying definitions of "plausible" counterfactuals and methods to generate them. However, many of these methods are computationally intensive and provide unconvincing explanations. Here we introduce SharpShooter, a method for binary classification that starts by creating a projected version of the input that classifies as the target class. Counterfactual candidates are then generated in latent space on the interpolation line between the input and its projection. We then demonstrate that our framework translates core characteristics of a sample to its counterfactual through the use of learned representations. Furthermore, we show that SharpShooter is competitive across common quality metrics on tabular and image datasets while being orders of magnitude faster than two comparable methods and excels at measures of realism, making it well-suited for high velocity machine learning applications which require timely explanations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{berthelot2018,
  title = {Understanding and {{Improving Interpolation}} in {{Autoencoders}} via an {{Adversarial Regularizer}}},
  author = {Berthelot, David and Raffel, Colin and Roy, Aurko and Goodfellow, Ian},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.07543 [cs, stat]},
  eprint = {1807.07543},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can "interpolate": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{datta2016,
  title = {Algorithmic {{Transparency}} via {{Quantitative Input Influence}}: {{Theory}} and {{Experiments}} with {{Learning Systems}}},
  shorttitle = {Algorithmic {{Transparency}} via {{Quantitative Input Influence}}},
  booktitle = {2016 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Datta, Anupam and Sen, Shayak and Zick, Yair},
  year = {2016},
  pages = {598--617},
  issn = {2375-1207},
  doi = {10.1109/SP.2016.42},
  abstract = {Algorithmic systems that employ machine learning play an increasing role in making substantive decisions in modern society, ranging from online personalization to insurance and credit decisions to predictive policing. But their decision-making processes are often opaque-it is difficult to explain why a certain decision was made. We develop a formal foundation to improve the transparency of such decision-making systems. Specifically, we introduce a family of Quantitative Input Influence (QII) measures that capture the degree of influence of inputs on outputs of systems. These measures provide a foundation for the design of transparency reports that accompany system decisions (e.g., explaining a specific credit decision) and for testing tools useful for internal and external oversight (e.g., to detect algorithmic discrimination). Distinctively, our causal QII measures carefully account for correlated inputs while measuring influence. They support a general class of transparency queries and can, in particular, explain decisions about individuals (e.g., a loan decision) and groups (e.g., disparate impact based on gender). Finally, since single inputs may not always have high influence, the QII measures also quantify the joint influence of a set of inputs (e.g., age and income) on outcomes (e.g. loan decisions) and the marginal influence of individual inputs within such a set (e.g., income). Since a single input may be part of multiple influential sets, the average marginal influence of the input is computed using principled aggregation measures, such as the Shapley value, previously applied to measure influence in voting. Further, since transparency reports could compromise privacy, we explore the transparency-privacy tradeoff and prove that a number of useful transparency reports can be made differentially private with very little addition of noise. Our empirical validation with standard machine learning algorithms demonstrates that QII measures are a useful transparency mechanism when black box access to the learning system is available. In particular, they provide better explanations than standard associative measures for a host of scenarios that we consider. Further, we show that in the situations we consider, QII is efficiently approximable and can be made differentially private while preserving accuracy.},
  keywords = {Algorithm design and analysis,Atmospheric measurements,Correlation,Decision making,fairness,machine learning,Machine learning algorithms,Particle measurements,Privacy,transparency}
}

@inproceedings{dhurandhar2018,
  title = {Explanations Based on the Missing: Towards Contrastive Explanations with Pertinent Negatives},
  shorttitle = {Explanations Based on the Missing},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Dhurandhar, Amit and Chen, Pin-Yu and Luss, Ronny and Tu, Chun-Chen and Ting, Paishun and Shanmugam, Karthikeyan and Das, Payel},
  year = {2018},
  month = dec,
  series = {{{NIPS}}'18},
  pages = {590--601},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  abstract = {In this paper we propose a novel method that provides contrastive explanations justifying the classification of an input by a black box classifier such as a deep neural network. Given an input we find what should be minimally and sufficiently present (viz. important object pixels in an image) to justify its classification and analogously what should be minimally and necessarily absent (viz. certain background pixels). We argue that such explanations are natural for humans and are used commonly in domains such as health care and criminology. What is minimally but critically absent is an important part of an explanation, which to the best of our knowledge, has not been explicitly identified by current explanation methods that explain predictions of neural networks. We validate our approach on three real datasets obtained from diverse domains; namely, a handwritten digits dataset MNIST, a large procurement fraud dataset and a brain activity strength dataset. In all three cases, we witness the power of our approach in generating precise explanations that are also easy for human experts to understand and evaluate.}
}

@inproceedings{dilokthanakul2016,
  title = {{Deep Unsupervised Clustering withGaussian Mixture Variational Autoencoders}},
  author = {Dilokthanakul, Nat and Mediano, Pedro A. M. and Garnelo, Marta and Lee, Matthew C. H. and Salimbeni, Hugh and Arulkumaran, Kai and Shanahan, Murray},
  year = {2016},
  month = nov,
  annotation = {\_eprint: 1611.02648}
}

@inproceedings{ding2020,
  title = {{{CcGAN}}: {{Continuous Conditional Generative Adversarial Networks}} for {{Image Generation}}},
  shorttitle = {{{CcGAN}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Ding, Xin and Wang, Yongwei and Xu, Zuheng and Welch, William J. and Wang, Z. Jane},
  year = {2020},
  month = sep,
  abstract = {This work proposes the continuous conditional generative adversarial network (CcGAN), the first generative model for image generation conditional on continuous, scalar conditions (termed regression...},
  langid = {english}
}

@misc{fernandez2020,
  title = {Full {{Text}}: {{Explaining Data-Driven Decisions}} Made by {{AI Systems}}: {{The Counterfactual Approach}}},
  shorttitle = {Full {{Text}}},
  author = {Fernandez, Carlos and Provost, Foster and Han, Xintian},
  year = {2020},
  month = jan,
  journal = {Onikle},
  abstract = {Fulltext:   Lack of understanding of the decisions made by model-based AI systems is an important barrier for their adoption. We examine counterfactual explanations as an alternative for explaining AI decisions. The counterfactual approach defines an explanation as a set of the system's data inputs that causally drives the decision (meaning that removing them changes the decision) and is irreducible (meaning that removing any subset of the inputs in the explanation does not change the decision). We generalize previous work on counterfactual explanations, resulting in a framework that (a) is model-agnostic, (b) can address features with arbitrary data types, (c) is able explain decisions made by complex AI systems that incorporate multiple models, and (d) is scalable to large numbers of features. We also propose a heuristic procedure to find the most useful explanations depending on the context. We contrast counterfactual explanations with another alternative: methods that explain model predictions by weighting features according to their importance (e.g., SHAP, LIME). This paper presents two fundamental reasons why explaining model predictions is not the same as explaining the decisions made using those predictions, suggesting we should carefully consider whether importance-weight explanations are well-suited to explain decisions made by AI systems. Specifically, we show that (1) features that have a large importance weight for a model prediction may not actually affect the corresponding decision, and (2) importance weights are insufficient to communicate whether and how features influence system decisions. We demonstrate this using several examples, including three detailed studies using real-world data that compare the counterfactual approach with SHAP and illustrate various conditions under which counterfactual explanations explain data-driven decisions better than feature importance weights.},
  howpublished = {https://onikle.com/articles/48299}
}

@article{goodfellow2015,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  year = {2015},
  month = mar,
  journal = {arXiv:1412.6572 [cs, stat]},
  eprint = {1412.6572},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  journal = {arXiv:1406.2661 [cs, stat]},
  eprint = {1406.2661},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{goyal2019,
  title = {Counterfactual {{Visual Explanations}}},
  author = {Goyal, Yash and Wu, Ziyan and Ernst, Jan and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  year = {2019},
  month = jun,
  journal = {arXiv:1904.07451 [cs, stat]},
  eprint = {1904.07451},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this work, we develop a technique to produce counterfactual visual explanations. Given a 'query' image \$I\$ for which a vision system predicts class \$c\$, a counterfactual visual explanation identifies how \$I\$ could change such that the system would output a different specified class \$c'\$. To do this, we select a 'distractor' image \$I'\$ that the system predicts as class \$c'\$ and identify spatial regions in \$I\$ and \$I'\$ such that replacing the identified region in \$I\$ with the identified region in \$I'\$ would push the system towards classifying \$I\$ as \$c'\$. We apply our approach to multiple image classification datasets generating qualitative results showcasing the interpretability and discriminativeness of our counterfactual explanations. To explore the effectiveness of our explanations in teaching humans, we present machine teaching experiments for the task of fine-grained bird classification. We find that users trained to distinguish bird species fare better when given access to counterfactual explanations in addition to training examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{goyal2019b,
  title = {Counterfactual {{Visual Explanations}}},
  author = {Goyal, Yash and Wu, Ziyan and Ernst, Jan and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  year = {2019},
  month = jun,
  journal = {arXiv:1904.07451 [cs, stat]},
  eprint = {1904.07451},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this work, we develop a technique to produce counterfactual visual explanations. Given a 'query' image \$I\$ for which a vision system predicts class \$c\$, a counterfactual visual explanation identifies how \$I\$ could change such that the system would output a different specified class \$c'\$. To do this, we select a 'distractor' image \$I'\$ that the system predicts as class \$c'\$ and identify spatial regions in \$I\$ and \$I'\$ such that replacing the identified region in \$I\$ with the identified region in \$I'\$ would push the system towards classifying \$I\$ as \$c'\$. We apply our approach to multiple image classification datasets generating qualitative results showcasing the interpretability and discriminativeness of our counterfactual explanations. To explore the effectiveness of our explanations in teaching humans, we present machine teaching experiments for the task of fine-grained bird classification. We find that users trained to distinguish bird species fare better when given access to counterfactual explanations in addition to training examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{hadad2020,
  title = {A {{Two-Step Disentanglement Method}}},
  author = {Hadad, Naama and Wolf, Lior and Shahar, Moni},
  year = {2020},
  month = jan,
  journal = {arXiv:1709.00199 [cs, stat]},
  eprint = {1709.00199},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We address the problem of disentanglement of factors that generate a given data into those that are correlated with the labeling and those that are not. Our solution is simpler than previous solutions and employs adversarial training. First, the part of the data that is correlated with the labels is extracted by training a classifier. Then, the other part is extracted such that it enables the reconstruction of the original data but does not contain label information. The utility of the new method is demonstrated on visual datasets as well as on financial data. Our code is available at https://github.com/naamahadad/A-Two-Step-Disentanglement-Method},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{kingmaAutoEncodingVariationalBayes2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  month = may,
  journal = {arXiv:1312.6114 [cs, stat]},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{kohavi1996,
  title = {Scaling up the Accuracy of {{Naive-Bayes}} Classifiers: A Decision-Tree Hybrid},
  shorttitle = {Scaling up the Accuracy of {{Naive-Bayes}} Classifiers},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Kohavi, Ron},
  year = {1996},
  month = aug,
  series = {{{KDD}}'96},
  pages = {202--207},
  publisher = {{AAAI Press}},
  address = {{Portland, Oregon}},
  abstract = {Naive-Bayes induction algorithms were previously shown to be surprisingly accurate on many classification tasks even when the conditional independence assumption on which they are based is violated. However, most studies were done on small databases. We show that in some larger databases, the accuracy of Naive-Bayes does not scale up as well as decision trees. We then propose a new algorithm, NBTree, which induces a hybrid of decision-tree classifiers and Naive-Bayes classifiers: the decision-tree nodes contain univariate splits as regular decision-trees, but the leaves contain Naive-Bayesian classifiers. The approach retains the interpretability of Naive-Bayes and decision trees, while resulting in classifiers that frequently outperform both constituents, especially in the larger databases tested.}
}

@article{kusner2018,
  title = {Counterfactual {{Fairness}}},
  author = {Kusner, Matt J. and Loftus, Joshua R. and Russell, Chris and Silva, Ricardo},
  year = {2018},
  month = mar,
  journal = {arXiv:1703.06856 [cs, stat]},
  eprint = {1703.06856},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{lang2021,
  title = {Explaining in {{Style}}: {{Training}} a {{GAN}} to Explain a Classifier in {{StyleSpace}}},
  shorttitle = {Explaining in {{Style}}},
  booktitle = {Proc. {{ICCV}} 2021},
  author = {Lang, Oran and Gandelsman, Yossi and Yarom, Michal and Wald, Yoav Itzhak and Elidan, Gal and Hassidim, Avinatan and Freeman, Bill and Isola, Phillip and Globerson, Amir and Irani, Michal and Mosseri, Inbar},
  year = {2021}
}

@article{larsen2016,
  title = {Autoencoding beyond Pixels Using a Learned Similarity Metric},
  author = {Larsen, Anders Boesen Lindbo and S{\o}nderby, S{\o}ren Kaae and Larochelle, Hugo and Winther, Ole},
  year = {2016},
  month = feb,
  journal = {arXiv:1512.09300 [cs, stat]},
  eprint = {1512.09300},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{lash2017,
  title = {A Budget-Constrained Inverse Classification Framework for Smooth Classifiers},
  author = {Lash, Michael T. and Lin, Qihang and Street, W. Nick and Robinson, Jennifer G.},
  year = {2017},
  month = jun,
  journal = {arXiv:1605.09068 [cs, stat]},
  eprint = {1605.09068},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Inverse classification is the process of manipulating an instance such that it is more likely to conform to a specific class. Past methods that address such a problem have shortcomings. Greedy methods make changes that are overly radical, often relying on data that is strictly discrete. Other methods rely on certain data points, the presence of which cannot be guaranteed. In this paper we propose a general framework and method that overcomes these and other limitations. The formulation of our method can use any differentiable classification function. We demonstrate the method by using logistic regression and Gaussian kernel SVMs. We constrain the inverse classification to occur on features that can actually be changed, each of which incurs an individual cost. We further subject such changes to fall within a certain level of cumulative change (budget). Our framework can also accommodate the estimation of (indirectly changeable) features whose values change as a consequence of actions taken. Furthermore, we propose two methods for specifying feature-value ranges that result in different algorithmic behavior. We apply our method, and a proposed sensitivity analysis-based benchmark method, to two freely available datasets: Student Performance from the UCI Machine Learning Repository and a real world cardiovascular disease dataset. The results obtained demonstrate the validity and benefits of our framework and method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,H.2.8,Statistics - Machine Learning}
}

@article{laugel2017,
  title = {Inverse {{Classification}} for {{Comparison-based Interpretability}} in {{Machine Learning}}},
  author = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.08443 [cs, stat]},
  eprint = {1712.08443},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In the context of post-hoc interpretability, this paper addresses the task of explaining the prediction of a classifier, considering the case where no information is available, neither on the classifier itself, nor on the processed data (neither the training nor the test data). It proposes an instance-based approach whose principle consists in determining the minimal changes needed to alter a prediction: given a data point whose classification must be explained, the proposed method consists in identifying a close neighbour classified differently, where the closeness definition integrates a sparsity constraint. This principle is implemented using observation generation in the Growing Spheres algorithm. Experimental results on two datasets illustrate the relevance of the proposed approach that can be used to gain knowledge about the classifier.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{le2020,
  title = {{{GRACE}}: {{Generating Concise}} and {{Informative Contrastive Sample}} to {{Explain Neural Network Model}}'s {{Prediction}}},
  shorttitle = {{{GRACE}}},
  author = {Le, Thai and Wang, Suhang and Lee, Dongwon},
  year = {2020},
  month = oct,
  journal = {arXiv:1911.02042 [cs, stat]},
  eprint = {1911.02042},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Despite the recent development in the topic of explainable AI/ML for image and text data, the majority of current solutions are not suitable to explain the prediction of neural network models when the datasets are tabular and their features are in high-dimensional vectorized formats. To mitigate this limitation, therefore, we borrow two notable ideas (i.e., "explanation by intervention" from causality and "explanation are contrastive" from philosophy) and propose a novel solution, named as GRACE, that better explains neural network models' predictions for tabular datasets. In particular, given a model's prediction as label X, GRACE intervenes and generates a minimally-modified contrastive sample to be classified as Y, with an intuitive textual explanation, answering the question of "Why X rather than Y?" We carry out comprehensive experiments using eleven public datasets of different scales and domains (e.g., \# of features ranges from 5 to 216) and compare GRACE with competing baselines on different measures: fidelity, conciseness, info-gain, and influence. The user-studies show that our generated explanation is not only more intuitive and easy-to-understand but also facilitates end-users to make as much as 60\% more accurate post-explanation decisions than that of Lime.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{lundberg2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  author = {Lundberg, Scott and Lee, Su-In},
  year = {2017},
  month = dec,
    booktitle={Advances in Neural Information Processing Systems 30 (NIPS 2017)} ,
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.}
}

@incollection{menzies2020,
  title = {Counterfactual {{Theories}} of {{Causation}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Menzies, Peter and Beebee, Helen},
  editor = {Zalta, Edward N.},
  year = {2020},
  edition = {Winter 2020},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  abstract = {The basic idea of counterfactual theories of causation is that themeaning of causal claims can be explained in terms of counterfactualconditionals of the form ``If A had not occurred,C would not have occurred''. Most counterfactualanalyses have focused on claims of the form ``event ccaused event e'', describing `singular' or`token' or `actual' causation. Such analyseshave become popular since the development in the 1970s of possibleworld semantics for counterfactuals. The best-known counterfactualanalysis of causation is David Lewis's (1973b) theory. However,intense discussion over forty years has cast doubt on the adequacy ofany simple analysis of singular causation in terms of counterfactuals.Recent years have seen a proliferation of different refinements of thebasic idea; the `structural equations' or `causalmodelling' framework is currently the most popular way ofcashing out the relationship between causation and counterfactuals.},
  keywords = {causation: and manipulability,causation: backward,causation: probabilistic,causation: the metaphysics of,conditionals: counterfactual,determinism: causal,events,facts,Hume; David,implicature,intrinsic vs. extrinsic properties,possible worlds,probability; interpretations of,rationalism vs. empiricism,scientific explanation,time: thermodynamic asymmetry in}
}

@article{pawelczyk2020,
  title = {Learning {{Model-Agnostic Counterfactual Explanations}} for {{Tabular Data}}},
  author = {Pawelczyk, Martin and Haug, Johannes and Broelemann, Klaus and Kasneci, Gjergji},
  year = {2020},
  month = apr,
  journal = {Proceedings of The Web Conference 2020},
  eprint = {1910.09398},
  eprinttype = {arxiv},
  pages = {3126--3132},
  doi = {10.1145/3366423.3380087},
  abstract = {Counterfactual explanations can be obtained by identifying the smallest change made to a feature vector to qualitatively influence a prediction; for example, from 'loan rejected' to 'awarded' or from 'high risk of cardiovascular disease' to 'low risk'. Previous approaches often emphasized that counterfactuals should be easily interpretable to humans, motivating sparse solutions with few changes to the feature vectors. However, these approaches would not ensure that the produced counterfactuals be proximate (i.e., not local outliers) and connected to regions with substantial data density (i.e., close to correctly classified observations), two requirements known as counterfactual faithfulness. These requirements are fundamental when making suggestions to individuals that are indeed attainable. Our contribution is twofold. On one hand, we suggest to complement the catalogue of counterfactual quality measures [1] using a criterion to quantify the degree of difficulty for a certain counterfactual suggestion. On the other hand, drawing ideas from the manifold learning literature, we develop a framework that generates attainable counterfactuals. We suggest the counterfactual conditional heterogeneous variational autoencoder (C-CHVAE) to identify attainable counterfactuals that lie within regions of high data density.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{ph.d2006,
  title = {Simplification and {{Understanding}} of {{Models}}},
  author = {Ph.D, Robert},
  year = {2006},
  month = dec,
  journal = {System Dynamics Review},
  volume = {5},
  pages = {51--68},
  doi = {10.1002/sdr.4260050105},
  abstract = {The theory of model simplification is presented as a means of increasing model understanding. Simplification is based on a selection of the behavior modes defined by the linearized representation of the model and results in a smaller and more easily understood model. To allow understanding, the variables in the simplified model must be easy to interpret relative to those in the original model. This interpretation is complete in an exact simplification, a concept used to derive measures of the importance of different variables in generating selected behavior modes. These measures are used to select which variables to retain and which to omit in forming the simplified model. Issues in the application to nonlinear models are considered, and software that facilitates model simplification is discussed.}
}

@article{purohit2020,
  title = {Deep {{Autoencoding GMM-based Unsupervised Anomaly Detection}} in {{Acoustic Signals}} and Its {{Hyper-parameter Optimization}}},
  author = {Purohit, Harsh and Tanabe, Ryo and Endo, Takashi and Suefusa, Kaori and Nikaido, Yuki and Kawaguchi, Yohei},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.12042 [cs, eess, stat]},
  eprint = {2009.12042},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  abstract = {Failures or breakdowns in factory machinery can be costly to companies, so there is an increasing demand for automatic machine inspection. Existing approaches to acoustic signal-based unsupervised anomaly detection, such as those using a deep autoencoder (DA) or Gaussian mixture model (GMM), have poor anomaly-detection performance. In this work, we propose a new method based on a deep autoencoding Gaussian mixture model with hyper-parameter optimization (DAGMM-HO). In our method, the DAGMM-HO applies the conventional DAGMM to the audio domain for the first time, with the idea that its total optimization on reduction of dimensions and statistical modelling will improve the anomaly-detection performance. In addition, the DAGMM-HO solves the hyper-parameter sensitivity problem of the conventional DAGMM by performing hyper-parameter optimization based on the gap statistic and the cumulative eigenvalues. Our evaluation of the proposed method with experimental data of the industrial fans showed that it significantly outperforms previous approaches and achieves up to a 20\% improvement based on the standard AUC score.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@article{rezende2016,
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  year = {2016},
  month = jun,
  journal = {arXiv:1505.05770 [cs, stat]},
  eprint = {1505.05770},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology}
}

@article{rezende2016a,
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  year = {2016},
  month = jun,
  journal = {arXiv:1505.05770 [cs, stat]},
  eprint = {1505.05770},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology}
}

@article{ribeiro2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  journal = {arXiv:1602.04938 [cs, stat]},
  eprint = {1602.04938},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{samangouei2018,
  title = {{{ExplainGAN}}: {{Model Explanation}} via {{Decision Boundary Crossing Transformations}}},
  shorttitle = {{{ExplainGAN}}},
  booktitle = {{{ECCV}} (10)},
  author = {Samangouei, Pouya and Saeedi, Ardavan and Nakagawa, Liam and Silberman, Nathan},
  year = {2018},
  month = jan,
  abstract = {We introduce a new method for interpreting computer vision models: visually perceptible, decision-boundary crossing transformations. Our goal is to answer a simple question: why did a model...},
  langid = {english}
}

@article{sharma2020,
  title = {{{CERTIFAI}}: {{Counterfactual Explanations}} for {{Robustness}}, {{Transparency}}, {{Interpretability}}, and {{Fairness}} of {{Artificial Intelligence}} Models},
  shorttitle = {{{CERTIFAI}}},
  author = {Sharma, Shubham and Henderson, Jette and Ghosh, Joydeep},
  year = {2020},
  month = feb,
  journal = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  eprint = {1905.07857},
  eprinttype = {arxiv},
  pages = {166--172},
  doi = {10.1145/3375627.3375812},
  abstract = {As artificial intelligence plays an increasingly important role in our society, there are ethical and moral obligations for both businesses and researchers to ensure that their machine learning models are designed, deployed, and maintained responsibly. These models need to be rigorously audited for fairness, robustness, transparency, and interpretability. A variety of methods have been developed that focus on these issues in isolation, however, managing these methods in conjunction with model development can be cumbersome and timeconsuming. In this paper, we introduce a unified and model-agnostic approach to address these issues: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models (CERTIFAI). Unlike previous methods in this domain, CERTIFAI is a general tool that can be applied to any black-box model and any type of input data. Given a model and an input instance, CERTIFAI uses a custom genetic algorithm to generate counterfactuals: instances close to the input that change the prediction of the model. We demonstrate how these counterfactuals can be used to examine issues of robustness, interpretability, transparency, and fairness. Additionally, we introduce CERScore, the first black-box model robustness score that performs comparably to methods that have access to model internals.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{shrikumar2019,
  title = {Learning {{Important Features Through Propagating Activation Differences}}},
  author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  year = {2019},
  month = oct,
  journal = {arXiv:1704.02685 [cs]},
  eprint = {1704.02685},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{vanlooveren2020,
  title = {Interpretable {{Counterfactual Explanations Guided}} by {{Prototypes}}},
  author = {Van Looveren, Arnaud and Klaise, Janis},
  year = {2020},
  month = feb,
  journal = {arXiv:1907.02584 [cs, stat]},
  eprint = {1907.02584},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a fast, model agnostic method for finding interpretable counterfactual explanations of classifier predictions by using class prototypes. We show that class prototypes, obtained using either an encoder or through class specific k-d trees, significantly speed up the the search for counterfactual instances and result in more interpretable explanations. We introduce two novel metrics to quantitatively evaluate local interpretability at the instance level. We use these metrics to illustrate the effectiveness of our method on an image and tabular dataset, respectively MNIST and Breast Cancer Wisconsin (Diagnostic). The method also eliminates the computational bottleneck that arises because of numerical gradient evaluation for \$\textbackslash textit\{black box\}\$ models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{verma2020,
  title = {Counterfactual {{Explanations}} for {{Machine Learning}}: {{A Review}}},
  shorttitle = {Counterfactual {{Explanations}} for {{Machine Learning}}},
  author = {Verma, Sahil and Dickerson, John and Hines, Keegan},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.10596 [cs, stat]},
  eprint = {2010.10596},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine-learning-based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{wachter2018,
  title = {Counterfactual {{Explanations}} without {{Opening}} the {{Black Box}}: {{Automated Decisions}} and the {{GDPR}}},
  shorttitle = {Counterfactual {{Explanations}} without {{Opening}} the {{Black Box}}},
  author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  year = {2018},
  month = mar,
  journal = {arXiv:1711.00399 [cs]},
  eprint = {1711.00399},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@article{wan2018,
  title = {Rethinking {{Feature Distribution}} for {{Loss Functions}} in {{Image Classification}}},
  author = {Wan, Weitao and Zhong, Yuanyi and Li, Tianpeng and Chen, Jiansheng},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.02988 [cs]},
  eprint = {1803.02988},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose a large-margin Gaussian Mixture (L-GM) loss for deep neural networks in classification tasks. Different from the softmax cross-entropy loss, our proposal is established on the assumption that the deep features of the training set follow a Gaussian Mixture distribution. By involving a classification margin and a likelihood regularization, the L-GM loss facilitates both a high classification performance and an accurate modeling of the training feature distribution. As such, the L-GM loss is superior to the softmax loss and its major variants in the sense that besides classification, it can be readily used to distinguish abnormal inputs, such as the adversarial examples, based on their features' likelihood to the training feature distribution. Extensive experiments on various recognition benchmarks like MNIST, CIFAR, ImageNet and LFW, as well as on adversarial examples demonstrate the effectiveness of our proposal.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{zheng2019,
  title = {Disentangling {{Latent Space}} for {{VAE}} by {{Label Relevant}}/{{Irrelevant Dimensions}}},
  author = {Zheng, Zhilin and Sun, Li},
  year = {2019},
  month = mar,
  journal = {arXiv:1812.09502 [cs]},
  eprint = {1812.09502},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {VAE requires the standard Gaussian distribution as a prior in the latent space. Since all codes tend to follow the same prior, it often suffers the so-called "posterior collapse". To avoid this, this paper introduces the class specific distribution for the latent code. But different from CVAE, we present a method for disentangling the latent space into the label relevant and irrelevant dimensions, \$\textbackslash bm\{\textbackslash mathrm\{z\}\}\_s\$ and \$\textbackslash bm\{\textbackslash mathrm\{z\}\}\_u\$, for a single input. We apply two separated encoders to map the input into \$\textbackslash bm\{\textbackslash mathrm\{z\}\}\_s\$ and \$\textbackslash bm\{\textbackslash mathrm\{z\}\}\_u\$ respectively, and then give the concatenated code to the decoder to reconstruct the input. The label irrelevant code \$\textbackslash bm\{\textbackslash mathrm\{z\}\}\_u\$ represent the common characteristics of all inputs, hence they are constrained by the standard Gaussian, and their encoder is trained in amortized variational inference way, like VAE. While \$\textbackslash bm\{\textbackslash mathrm\{z\}\}\_s\$ is assumed to follow the Gaussian mixture distribution in which each component corresponds to a particular class. The parameters for the Gaussian components in \$\textbackslash bm\{\textbackslash mathrm\{z\}\}\_s\$ encoder are optimized by the label supervision in a global stochastic way. In theory, we show that our method is actually equivalent to adding a KL divergence term on the joint distribution of \$\textbackslash bm\{\textbackslash mathrm\{z\}\}\_s\$ and the class label \$c\$, and it can directly increase the mutual information between \$\textbackslash bm\{\textbackslash mathrm\{z\}\}\_s\$ and the label \$c\$. Our model can also be extended to GAN by adding a discriminator in the pixel domain so that it produces high quality and diverse images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{zotero-844,
  year= {1998},
  author= {LeCun,Yann and
 Cortes,Corinna and
Burges, J.C., Christopher},
  
  key = {MNIST},
  title = {{Handwritten Digit Database}},
  howpublished = {\url{http://yann.lecun.com/exdb/mnist/}}
}

@misc{zotero-952,
  year= {2020},
  key = {Lending Club},
  title = {{Lending Club 2007-2020Q3}},
  howpublished = {\url{https://www.kaggle.com/datasets/ethon0426/lending-club-20072020q1}}
}

@misc{zotero-970,
  title = {Comparison-{{Based Inverse Classification}} for {{Interpretability}} in {{Machine Learning}} | Springerprofessional.De},
  howpublished = {https://www.springerprofessional.de/en/comparison-based-inverse-classification-for-interpretability-in-/15801486}
}

@misc{zotero-971,
  title = {{{ViCE}} | {{Proceedings}} of the 25th {{International Conference}} on {{Intelligent User Interfaces}}},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/3377325.3377536}
}

@misc{zotero-987,
  title = {{{ExplainGAN}}: {{Model Explanation}} via {{Decision Boundary Crossing Transformations}}},
  shorttitle = {{{ExplainGAN}}},
  journal = {springerprofessional.de},
  abstract = {We introduce a new method for interpreting computer vision models: visually perceptible, decision-boundary crossing transformations. Our goal is to answer a simple question: why did a model classify an image as being of class A instead of class B?},
  howpublished = {https://www.springerprofessional.de/en/explaingan-model-explanation-via-decision-boundary-crossing-tran/16177130},
  langid = {english}
}


@misc{samoilescuModelagnosticScalableCounterfactual2021,
  title = {Model-Agnostic and {{Scalable Counterfactual Explanations}} via {{Reinforcement Learning}}},
  author = {Samoilescu, Robert-Florian and Van Looveren, Arnaud and Klaise, Janis},
  year = {2021},
  month = jun,
  number = {arXiv:2106.02597},
  eprint = {2106.02597},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.02597},
  abstract = {Counterfactual instances are a powerful tool to obtain valuable insights into automated decision processes, describing the necessary minimal changes in the input space to alter the prediction towards a desired target. Most previous approaches require a separate, computationally expensive optimization procedure per instance, making them impractical for both large amounts of data and high-dimensional data. Moreover, these methods are often restricted to certain subclasses of machine learning models (e.g. differentiable or tree-based models). In this work, we propose a deep reinforcement learning approach that transforms the optimization procedure into an end-to-end learnable process, allowing us to generate batches of counterfactual instances in a single forward pass. Our experiments on real-world data show that our method i) is model-agnostic (does not assume differentiability), relying only on feedback from model predictions; ii) allows for generating target-conditional counterfactual instances; iii) allows for flexible feature range constraints for numerical and categorical attributes, including the immutability of protected features (e.g. gender, race); iv) is easily extended to other data modalities such as images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{klaiseAlibiExplainAlgorithms2021,
  title = {Alibi {{Explain}}: {{Algorithms}} for {{Explaining Machine Learning Models}}},
  shorttitle = {Alibi {{Explain}}},
  author = {Klaise, Janis and Van Looveren, Arnaud and Vacanti, Giovanni and Coca, Alexandru},
  year = {2021},
  month = jun,
  journal = {Journal of Machine Learning Research},
  volume = {22},
  number = {181},
  pages = {1--7},
  abstract = {Algorithms for explaining machine learning models},
  copyright = {Apache-2.0}
}

@misc{wachterCounterfactualExplanationsOpening2018a,
  title = {Counterfactual {{Explanations}} without {{Opening}} the {{Black Box}}: {{Automated Decisions}} and the {{GDPR}}},
  shorttitle = {Counterfactual {{Explanations}} without {{Opening}} the {{Black Box}}},
  author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  year = {2018},
  month = mar,
  number = {arXiv:1711.00399},
  eprint = {1711.00399},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1711.00399},
  abstract = {There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@misc{zotero-1220,
  title = {Lending {{Club}} 2007-{{2020Q3}}},
  year= {2020},
  author= {Yash},
  abstract = {Consolidated Data of All the years},
  howpublished = {https://www.kaggle.com/datasets/ethon0426/lending-club-20072020q1},
  langid = {english}
}

@misc{zotero-1218,
  title = {{{UCI Machine Learning Repository}}: {{Adult Data Set}}},
   author= { Kohavi, Ronny and  Becker,Barry},
   year= {1996},
  howpublished = {https://archive.ics.uci.edu/ml/datasets/adult}
}



@inproceedings{GuyomardFBG21,
  author       = {Victor Guyomard and
                  Fran{\c{c}}oise Fessant and
                  Tassadit Bouadi and
                  Thomas Guyet},
  editor       = {Michael Kamp and
                  Irena Koprinska and
                  Adrien Bibal and
                  Tassadit Bouadi and
                  Beno{\^{\i}}t Fr{\'{e}}nay and
                  Luis Gal{\'{a}}rraga and
                  Jos{\'{e}} Oramas and
                  Linara Adilova and
                  Yamuna Krishnamurthy and
                  Bo Kang and
                  Christine Largeron and
                  Jefrey Lijffijt and
                  Tiphaine Viard and
                  Pascal Welke and
                  Massimiliano Ruocco and
                  Erlend Aune and
                  Claudio Gallicchio and
                  Gregor Schiele and
                  Franz Pernkopf and
                  Michaela Blott and
                  Holger Fr{\"{o}}ning and
                  G{\"{u}}nther Schindler and
                  Riccardo Guidotti and
                  Anna Monreale and
                  Salvatore Rinzivillo and
                  Przemyslaw Biecek and
                  Eirini Ntoutsi and
                  Mykola Pechenizkiy and
                  Bodo Rosenhahn and
                  Christopher L. Buckley and
                  Daniela Cialfi and
                  Pablo Lanillos and
                  Maxwell Ramstead and
                  Tim Verbelen and
                  Pedro M. Ferreira and
                  Giuseppina Andresini and
                  Donato Malerba and
                  Ib{\'{e}}ria Medeiros and
                  Philippe Fournier{-}Viger and
                  M. Saqib Nawaz and
                  Sebasti{\'{a}}n Ventura and
                  Meng Sun and
                  Min Zhou and
                  Valerio Bitetta and
                  Ilaria Bordino and
                  Andrea Ferretti and
                  Francesco Gullo and
                  Giovanni Ponti and
                  Lorenzo Severini and
                  Rita P. Ribeiro and
                  Jo{\~{a}}o Gama and
                  Ricard Gavald{\`{a}} and
                  Lee A. D. Cooper and
                  Naghmeh Ghazaleh and
                  Jonas Richiardi and
                  Damian Roqueiro and
                  Diego Saldana Miranda and
                  Konstantinos Sechidis and
                  Guilherme Gra{\c{c}}a},
  title        = {Post-hoc Counterfactual Generation with Supervised Autoencoder},
  booktitle    = {Machine Learning and Principles and Practice of Knowledge Discovery
                  in Databases - International Workshops of {ECML} {PKDD} 2021, Virtual
                  Event, September 13-17, 2021, Proceedings, Part {I}},
  series       = {Communications in Computer and Information Science},
  volume       = {1524},
  pages        = {105--114},
  publisher    = {Springer},
  year         = {2021},
  url          = {https://doi.org/10.1007/978-3-030-93736-2\_10},
  doi          = {10.1007/978-3-030-93736-2\_10},
  timestamp    = {Tue, 15 Mar 2022 10:21:47 +0100},
  biburl       = {https://dblp.org/rec/conf/pkdd/GuyomardFBG21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



