%%%% ijcai23.tex

\typeout{IJCAI--23 Instructions for Authors}

% These are the instructions for authors for IJCAI-23.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai23.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai23}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}


\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{hyperref}

\usepackage[switch]{lineno}
\usepackage{hyperref}
\usepackage{url}


\usepackage{multicol}


\usepackage{paralist}
\usepackage{float,bbold}
\usepackage{dsfont}
%\usepackage{algorithmic}

\usepackage{algpseudocode}

\usepackage{todonotes}
\usepackage{ verbatim,subcaption}

% Comment out this line in the camera-ready submission
%\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.
% Please **do not** include Title and Author information
\pdfinfo{
/TemplateVersion (IJCAI.2023.0)
}

\title{Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space}


% Single author syntax
%\author{
    %Xuan Zhao
    %\affiliations
    %SCHUFA Holding AG
    %\emails
    %xuan.zhao@schufa.de
%}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
%\iffalse
\author{
Xuan Zhao$^1$
\and
Klaus Broelemann$^1$\and
Gjergji Kasneci$^{2}$
\affiliations
$^1$SCHUFA Holding AG\\
$^2$Technical University of Munich\\
%$^3$Third Affiliation\\
%$^4$Fourth Affiliation
\emails
%\{first, second\}@example.com,
xuan.zhao@schufa.de,
klaus.broelemann@schufa.de,
gjergji.kasneci@tum.de
}
%\fi

\begin{document}

\maketitle

\begin{abstract}
Counterfactual Explanations (CEs) are an important tool in Algorithmic Recourse for addressing two questions: 1. What are the crucial factors that led to an automated prediction/decision? 2. How can these factors be changed to achieve a more favorable outcome from a user's perspective? Thus, guiding the user's interaction with AI systems by proposing easy-to-understand explanations and easy-to-attain feasible changes is essential for the trustworthy adoption and long-term acceptance of AI systems. In the literature, various methods have been proposed to generate CEs, and different quality measures have been suggested to evaluate these methods. However, the generation of CEs is usually computationally expensive, and the resulting suggestions are unrealistic and thus non-actionable. In this paper, we introduce a new method to generate CEs for a pre-trained binary classifier by first shaping the latent space of an autoencoder to be a mixture of Gaussian distributions. CEs are then generated in latent space by linear interpolation between the query sample and the centroid of the target class. We show that our method maintains the characteristics of the input sample during the counterfactual search. In various experiments, we show that the proposed method is competitive based on different quality measures on image and tabular datasets -- efficiently returns results that are closer to the original data manifold compared to three state-of-the-art methods, which are essential for realistic high-dimensional machine learning applications.
\end{abstract}

\section{Introduction}\label{sec:intro}

Machine learning models have recently become ubiquitous in our society, and the demand for explainability is rising, especially in high-stake applications like healthcare, finance, and employment. Explainability of the model's behavior is the prerequisite for establishing trust in machine learning-based decision systems. Researchers have developed various techniques to explain the relationships between the input and output of a model. While models with relatively simple design (e.g., logistic regression, decision trees, rule fit algorithm) can be interpreted straightforwardly, more complex models are analyzed based on simpler surrogate models. %These surrogate models emulate the more complex model in a post-hoc fashion locally and provide explanations in terms of local feature importance (i.e., for a given input). For example, LIME \cite{ribeiro2016} perturbs the input features and measures the effects of the perturbation through a local linear approximation of the complex model to assess the importance of the input features. On the other hand, SHAP \cite{lundberg2017} applies a game-theoretic approach to evaluate the contribution of input features to the output. While LIME and SHAP have their specific advantages and disadvantages, they have a shared objective: estimate the effect of each input feature on a given prediction. 
Another important line of research aims to answer the following question in a binary classification setting -- "How can the input be changed to achieve a prediction representing the favored class instead of the unfavored one?". The exploration of outcomes in alternative similar yet non-occurring worlds is called counterfactual analysis \cite{menzies2020}. With a pre-trained and fixed model, the only way for the model to produce a different output is by altering the input. To this end, counterfactual explanations (CEs) provide prescriptive suggestions on the features of the query sample (i.e., input) that have to change (and also by how much they need to change) to achieve the desired outcome. We require this change to be minimal (i.e., associated with low costs) and feasible (i.e., realistic). With a clear semantic implication and a common logical grounding, CEs are generally easy to understand by end-users~\cite{fernandez2020}. In fact, in literature, the most direct possible usage of CEs is the guidance to the end-users.

 %There are many benefits to this form of explanation. Counterfactual reasoning is a common logical approach for humans and therefore is easy to interpret Fernandez et al. [2020].
However, CEs have their downsides. High-dimensional input spaces lead to high-dimensional CEs with limited utility for the potential users of the explanation (less intuitive and less feasible). %Braun et al. [1956]. 
In addition, searching through the high-dimensional space for CEs is computationally expensive. Other than the problem caused by the dimensionality, without proper constraints, it is possible to generate out-of-sample CEs close to the original data distribution. Out-of-sample CEs can result in explanations/suggestions that are not feasible (i.e., unlikely to be achieved since they do not correspond to the training data distribution). Adversarial samples \cite{goodfellow2015} which resemble the original sample but have changed imperceptibly to fool the classifier, might be a good example to illustrate this situation even though they are designed for a different purpose (deceive human perception and pre-trained classifiers). We show in Section \ref{sec:eva} that a search in the original space for CEs might cause this situation. An feasible CE should stay close to the data manifold and suggest meaningful (i.e., realistic and easily achievable) changes to a query sample.   %The majority of the current methods focus on providing explanations to people impacted by the decisions made by these models. Our work focuses on providing explanations to a neglected audience - the intermediaries ultimately responsible for their use, situated between model developers and the individuals that are subjected to the models decisions. We focus on filling this gap with Our method. It is essential to provide those audiences with methods to guide building, understanding,and implementing models. We emphasize their use as explanations for the model a necessary step before trying to afford recourse.
Various requirements are proposed in the literature for generating useful CEs: feasible, sparse, valid, proximate, and computationally efficient to generate~\cite{verma2020}. It is also clear that there might be a trade-off between these requirements. 

We introduce a method for generating CEs by using interpolation within the latent representations of the input data to achieve the requirements mentioned above. We perform experiments with an image dataset -- MNIST and two tabular datasets -- the Adult income and Lending Club loan default. Our main contributions are: (1) a model agnostic framework for finding feasible CEs that are prominent in scalability in data with a low computational expense, (2) a novel strategy for manipulating the latent space for the counterfactual search, (3) a comparison of methods across the image and tabular datasets. 

The remainder of this paper is structured as follows: First, we briefly review the existing methods of counterfactual explanation (Section \ref{sec:back}). Then, in Section \ref{sec:method}, we propose a CE method via autoencoder-aided search in Gaussian Mixture (GM) distributed latent space. In Section \ref{sec:eva}, we present the evaluation results of our method against three state-of-the-art methods. Finally, Section \ref{sec:con} concludes the paper.

\section{Background and Related Work}
\label{sec:back}
\subsection{Generating Counterfactuals by Perturbing the Original Input Space}
One large branch of the literature generates counterfactuals by perturbing the input feature space. Inverse classification \cite{lash2017} maintains sparsity by partitioning the features into immutable and mutable features and imposing budgetary constraints on the mutable features. A sampling approach is proposed by \cite{laugel2017} with the growing spheres method to traverse the input space for CEs. Gradient descent is utilized in input space to find contrastive explanations \cite{dhurandhar2018}, which are separated into pertinent positives and pertinent negatives. They included an autoencoder loss to keep the explanations in sample. Gradient descent methods are improved with the introduction of prototypes, guiding the gradient descent towards the average value of the target class by averaging the training sets representation in the latent space \cite{vanlooveren2020}. GRACE \cite{le2020} is designed for neural networks on tabular data that combines the concepts of contrastive explanations with interventions by performing constrained gradient descent adding an additional loss that is a measure of information gain to keep the resulting explanations sparse. Several methods mentioned above already include generative models to maintain in sample. The criteria of the XAI for CEs are included in the methods either as constraints or backbone of the loss design, which might lead to difficult optimization especially when the input is high dimensional. 

\subsection{Generating Counterfactuals by Perturbing the Latent Space}
Input feature perturbation methods without proper regularization can generate counterfactuals unconvincing \cite{goyal2019b} and infeasible, resembling adversarial samples. To address this problem, latent space perturbations methods could be a solution since they utilize generative and probabilistic models in the algorithm design to ensure counterfactuals have a high probability under the data distribution $p(X)$. One example is StylEx \cite{lang2021}, which includes the classifier in the generative model and manipulates the latent space to visualize the counterfactual search. Moreover, ExplainGAN \cite{samangouei2018} is a method for finding counterfactual explanations for images by training multiple autoencoders and using the signal from the classifier and discriminators to inform the learned representations. Growing sphere search is performed in the latent space of a conditional variational autoencoder to generate counterfactuals \cite{pawelczyk2020}. \cite{GuyomardFBG21} generates counterfactuals by using class prototypes obtained from a supervised auto-encoder. The novelty of this approach is that these class prototypes are obtained using a supervised auto-encoder. Gradient descent in the latent space of a variational autoencoder is applied with regularization terms \cite{balasubramanian2021}. These methods, however, could be computationally expensive. Sharpshooter \cite{barr2021} searchs counterfactuals by linear interpolation in the latent space with aid from two separate autoencoders which is less expensive, but it does not guarantee the search maintains the label-irrelevant features.

\section{Our method}
\label{sec:method}
% \begin{comment}
% % Figure environment removed
% \end{comment}

\subsection{Problem Setting}
Our method requires access to the dataset and a previously trained classifier that we want to explain with counterfactual explanations. We only consider a binary classification in this paper. In the problem setting of counterfactual explanation, there is usually a base class where instances belong that class wants to seek feasible counterfactual explanation toward the target class. The dataset is composed $\mathcal{D} = (x_i,y_i)_{i=1}^K$ where we can split $x$ into two classes where $x_i$ belongs to the base class has $\hat{y_i}= 0$ and $x_{i}$ belongs to the target class has $\hat{y_i} = 1$ under the classifier $\hat{y} = f(x)$. $x_q$ is a query sample with an output 0 from the classifier $f$. 1 is the desired target output. Hence, CEs are needed for the query sample $x_q$ under the classifier $f$.


%For simplicity, we assume constant prior probabilities $p(c)$, and we ignore the correlation among different dimensions of $z$, $\sigma_c$ is assumed to be diagonal.

%Formally, the training dataset of the pre-trained bianry classifier $\hat{y_i} = f(x_i)$ is given by $\mathcal{D} = (x_i,y_i)_{i=1}^K$, where $y_i\in \mathbb{R}$ is a continuous label and $\hat{y_i}$ is the regression output/prediction. $x_q$ is a query sample with an output $\hat{y}_q$ from the regressor $f$. $\hat{y}_t$ is the desired target output. Hence, CEs are needed for the query sample $x_q$ under the regressor $f$.

% Figure environment removed



\subsection{Desiderata for CE Search}
\label{Desiderata}

Our approach is driven by the following main desiderata:

\textbf{Desideratum 1}
Intrinsic and label-irrelevant characteristics of the input query should be retained during the search process, i.e., the target label should be reached based on intrinsic properties of the input and with low effort.

\textbf{Desideratum 2} The generating step, should return highly realistic, i.e., feasible CEs. 

\textbf{Desideratum 3} The CEs should be retrieved efficiently in high-dimensional settings to ensure practical application to real-life settings.

\subsection{Learning Components}
We design a CE generation method based on two main steps: a Training Step in which we use the same training dataset used in training the classifier to learn an autoencoder with its latent space shaped by enforcing a Gaussian-mixture distribution on the embeddings and a Generating Step in which we use interpolation in the latent space to find a relatively less computational expensive counterfactual explanation. The algorithm flow is shown in Figure \ref{fig:1}. It only requires access to the training dataset and prediction of the pre-trained classifier $f$ that we aim to explain through appropriate CEs. For our Training Step, we form a new training dataset $\mathcal{D}_t = (x_i,\hat{y}_i)_{i=1}^K$ by replacing original $y$ in $\mathcal{D}$ with $\hat{y}$.

In the following part of this subsection, we will describe each component of the Training Step in Figure \ref{fig:1}. The details of Generating Step are described in Section \ref{sec:algorithm}.






\textbf{Label Relevant Branch: Gaussian Mixture Distribution} 
%A Gaussian mixture distribution is comprised of several Gaussian distributions, each identified by $ k \in \{ 1, \ldots , K \} $, where K is the number of clusters of the dataset.
Gaussian Mixture models are usually used in a unsupervised way, but in our usage, it is supervised by the classification label produced by the pre-trained classifier. Our goal for generating a CE is to cross the decision boundary of the pre-trained classifier by taking advantage of Gaussian mixture distribution in the latent space, in which case, we could generate a CE without having a process of optimization for each query sample. Intuitively, in the latent space, data points with the same class label should cluster closer to each other. Inspired by GM loss \cite{wan2018}, we could use a classification constraint and likelihood constraints to 'push' the latent space to a Gaussian mixture distribution for further manipulation.  %One thing to notice here is that we have a hidden assumption that the datapoint with the same class label is generated by the same component(check gmm for better reference)
After proper training, we could `force' the extracted embedding $z$ on the training set following a Gaussian mixture distribution expressed in Equation \ref{eq:z}, in which $\mu_{c}$ and $\sigma_{c}$ are the mean and covariance of class c in the latent space, and $p(c)$ is the prior probability of class $c$. %, which we assume is fixed. 
 In the binary classification setting, $c\in [0,1]$.

\begin{align}
    \label{eq:z}
        p(z) & =\sum_{c}p(z \mid c)p(c)=\sum_{c}\mathcal{N}(z;\mu_{c},\sigma_{c})p(c)
        %\\
%    \label{eq:2}
%        p(z \mid c) &=\mathcal{N}(z;\mu_{c},\Sigma_{c})p(c)
\end{align}

% \begin{equation} \label{eq:1}
% p(z)=\sum_{c}p(z \mid c)p(c)=\sum_{c}\mathcal{N}(z;\mu_{c},\Sigma_{c})p(c)
% \end{equation}

% \begin{equation} \label{eq:2}
% p(z \mid c)=\mathcal{N}(z;\mu_{c},\Sigma_{c})p(c)
% \end{equation}








If the latent space follows a Gaussian mixture distribution, the conditional probability distribution of a latent embedding $z$ given its class label c can be expressed in Equation \ref{eq:zc}. The corresponding posterior probability distribution can be expressed in Equation \ref{eq:cz}.
\begin{align}
    \label{eq:zc}
p(z \mid c)=\mathcal{N}(z;\mu_{c},\sigma_{c})p(c)
        \\
\label{eq:cz}
p(c \mid z) = \frac{\mathcal{N}( z;\mu_{c},\Sigma_{c})p(\mu_{c})}{\sum_{c=1}^{C}\mathcal{N}(z;\mu_{c},\sigma_{c})}
\end{align}
\begin{comment}
\begin{equation} \label{eq:2}
p(z \mid c)=\mathcal{N}(z;\mu_{c},\Sigma_{c})p(c)
\end{equation}

\begin{equation} \label{eq:3}
p(c \mid z) = \frac{\mathcal{N}( z;\mu_{c},\Sigma_{c})p(\mu_{c})}{\sum_{c=1}^{C}\mathcal{N}(z;\mu_{c},\Sigma_{c})}
\end{equation}
\end{comment}
A \textbf{classification loss} $\mathcal{L}_{cls}$ is then calculated as the cross-entropy between the posterior probability distribution and the class label as is shown in Equation \ref{eq:cls}.



\begin{equation} \label{eq:cls}
\mathcal{L}_{cls} = -\frac{1}{N}\sum_{i=1}^{N}\text{log}\frac{\mathcal{N}( z_i;\mu_{\hat{y}},\Sigma_{\hat{y}}) p(\mu_{c=\hat{y}})}{\sum_{c=1}^{C}\mathcal{N}(z_i;\mu_{c},\Sigma_{c})}
\end{equation}

Applying the classification loss only cannot reach our goal of forcing the latent space to be a Gaussian mixture distribution. There will be situations where a $z_i$ can be far away from the corresponding target class centroid $\mu _c$ and still be correctly classified since it is relatively closer to $\mu _c$ than to the means of the other classes in multiple classifications--which could be an outlier. To fix this problem, we then use a likelihood to measure the extent to of the training data fits the Gaussian mixture distribution. The \textbf{likelihood} for $\{z, c\}$ is expressed in Equation \ref{eq:lkd}. The likelihood could serve as a constraint to the original classification loss.

\begin{equation} \label{eq:lkd}
\mathcal{L}_{lkd} = -\sum_{i=1}^{N}\text{log }\mathcal{N}(z_i;\mu_{{z}_{i}},\Sigma_{{z_{i}}})
\end{equation}

Gaussian mixture loss $\mathcal{L}_{GM}$ we optimize to update the parameters of Encoder, $\mu_{c}$ and $\Sigma_{c}$, during Step 1, is defined in Equation \ref{eq:gm}, in which $\lambda$ is a weighting coefficient.

\begin{equation} \label{eq:gm}
\mathcal{L}_{GM}=\mathcal{L}_{cls}+\lambda_{lkd}\mathcal{L}_{lkd}
\end{equation}

\textbf{Label Irrelevant Branch: Encoder$_u$ and Adversarial Classifier}
We notice that generative models usually try to generate various unseen samples. However, the generation of CEs needs to satisfy different criteria, as mentioned in Section \ref{sec:intro}. In our problem setting, to satisfy \emph{Desideratum 1}, which requires maintaining the characteristics of the query sample during the search, it is intuitive to adopt disentanglement methods in the latent space of an autoencoder.

Inspired by a Two-Step Disentanglement Method \cite{hadad2020}, we introduce an Adversarial Classifier to ensure that the embedding $z_u$ captured by the Encoder\textsubscript{u} is classification label-irrelevant. %The motivation for us to utilize this component is that a CE should maintain the query sample's characteristics according to \emph{Desideratum 1}.
It is inspired by GANs, where the discriminator gradually loses the capacity to tell the generated data from the real data during the training phase. %We borrow the idea of generating CEs for which the regression labels are more favorable than the original ones. 
While GANs are usually used to improve the quality of generated output -- telling fake from real, the adversarial component in our design encourages the Encoder\textsubscript{u} to dismiss information about the labels, leading to disentanglement. With the Adversarial Classifier, we could guarantee that the $z_u$ and $\hat{y}$ are disentangled and independent, which prepares for the interpolation in the Generating Step. $\hat{y}'$ is the classification label of $z_u$ through the Adversarial Classifier. The \textbf{adversarial classification loss} is shown in Equation \ref{eq:acl} as binary cross entropy loss.

\begin{equation} \label{eq:acl}
\mathcal{L}_{adv} = -\frac{1}{N}\sum_{i=1}^{N}\hat{y_i}\text{log}\hat{y_i}'+(1-\hat{y_i})\text{log}(1-\hat{y_i}')
\end{equation}

\textbf{Autoencoder} 
%We notice that all of the generative models mentioned in Section \ref{sec:back} try to generate new samples with the help of random noise to increase generation diversity.
 %However, We add a Gaussian Distribution as a constraint to train the plain autoencoder. By doing so, we also gain better control of the latent space for interpolation in the algorithm. 
The reconstruction error of the autoencoder is shown in Equation \ref{eq:rec}. %For future work, we could consider using VAE instead to see if the representation is improved for counterfactual searching.

\begin{equation} \label{eq:rec}
\mathcal{L}_{rec}=\frac{1}{N}\sum_{i=1}^{N}||x_i-x'_i||^2
\end{equation}
\textbf{Summary} The configuration of the network in the Training Step is composed of three network branches: first, in the \textbf{Label Relevant Branch}, the $\mathcal{L}_{GM}$ forces $z$ to be Gaussian Mixture Distribution. Second,  in the \textbf{Label Irrelevant Branch}, the Adversarial Classifier is trained to minimize the \textbf{adversarial classification loss} $\mathcal{L}_{adv}$ in Equation \ref{eq:acl} -- it is trained to classify $z_u$ to $\hat{y}$. Third, the autoencoder network is trained to minimize the \textbf{total loss} $\mathcal{L}$, the sum of three terms as shown in Equation \ref{eq:recon}: (i) the \textbf{reconstruction loss} $\mathcal{L}_{rec}$ as shown in Equation \ref{eq:rec}, (ii) \textbf{likelihood} in Equation \ref{eq:lkd} and (iii) minus the \textbf{adversarial classification loss} $\mathcal{L}_{adv}$ in Equation \ref{eq:acl}. 
\begin{equation} \label{eq:recon}
\mathcal{L} = \mathcal{L}_{rec}+\lambda_{lkd} \mathcal{L}_{lkd}-\lambda_{adv} \mathcal{L}_{adv}
\end{equation}

%\textbf{Interpolation} We perform a linear interpolation during Step 2. We are motivated to do a linear interpolation because it is relatively less computationally expensive. With the Gaussian mixture distribution enforcement, we already have a regular distributed latent space which makes linear actions reasonable (see Figure \ref{fig:pca}). The encoder is used to embed a base class sample(query sample) $z_b$ in a learned representation forced to be distributed Gaussian mixture. We then use linear interpolation to sample along the line between a base sample and its target component centroid in latent space $z_t = (1- \alpha)z_b + \alpha z_1$ with $\alpha$ in $(0,1]$. The search is guided by the line between $z_b$ and the centroid of the target class. The guided search could be less computationally expensive. We use the autoencoder to estimate the data distribution and its latent space to ensure that counterfactuals both lie close to the original data manifold and are relatively close to their original query sample in latent space. 



\subsection{Algorithm} \label{sec:algorithm}
Algorithm \ref{alg:main} and \ref{alg:main2}  show pseudo code to process our method in addition to Figure \ref{fig:1} which is used to generate the counterfactual in Section \ref{sec:eva}. 

\begin{algorithm}[bt]
\caption{Training Step of our proposed architecture }
   \begin{algorithmic}[1]
   
   \Require $\psi$, $\pi$, $\phi$ and $\omega$ the initial parameters of Encoder, Encoder$_u$,  Decoder and Adversarial Classifier; $\mu_{c}$ and $\Sigma_{c}$ the initial mean and covariance of the Gaussian distribution of $z$; $n$  the number of iterations; $\lambda_{lkd}$ and $\lambda_{adv}$ the weights of regularization terms $\mathcal{L}_{lkd}$ and $\mathcal{L}_{adv}$; $c\in [0,1]$
 
  \While {not converged} 
    \For {$i=0$ to $n$}
      \State Sample $\{x, y\}$ a batch from dataset $\mathcal{D}_t$.
          \State $\omega \stackrel{+}{\gets} -\bigtriangledown_\omega \mathcal{L}_{adv}$
      \State $\mathcal{L}_{GM} \gets \mathcal{L}_{cls}+\lambda_{lkd} \mathcal{L}_{lkd}$
     \State $\psi, \mu_{c}, \Sigma_{c}\stackrel{+}{\gets}-\bigtriangledown_{\psi, \mu_{c}, \sigma_{c}} (\mathcal{L}_{GM}-\lambda_{adv}\mathcal{L}_{adv})$
     \State Sample $\{x, y\}$ a batch from dataset $\mathcal{D}_t$.
     \State $\mathcal{L} \gets \mathcal{L}_{rec}+\lambda_{lkd} \mathcal{L}_{lkd}-\lambda_{adv}\mathcal{L}_{adv}$
     \State $\psi,\pi,\phi \stackrel{+}{\gets} -\bigtriangledown_{\psi,\pi,\phi} \mathcal{L}$
     \EndFor 
  \EndWhile

   \end{algorithmic}
   \label{alg:main}
   \end{algorithm}


\begin{algorithm}[bt]

\caption{Generating Step of our proposed architecture}
   \begin{algorithmic}[1]

     
   
   %\Require $\psi$,  $\phi$ , $\omega$ and $\sigma$ the initial parameters of Encoder\textsubscript{u}, Decoder, Adversarial Regressor and Discriminator;  $n$  the number of iterations; $\lambda_{Adv}$  the weight of regularization term $\mathcal{L}_{Adv}$;
  \Require $S$ samples $\alpha=\alpha_{s=1}^S$ in $(0,1]$; $x_q$ the query sample;  $x_s$ potential CE;  $f$ classifier; $tol$ tolerance; $T$ probability of target counterfactual class ($0.5$ for decision boundary); $\mu_1$ the mean of target class in latent space of the label relevant branch
  \State $z_q \gets Encoder(x_q)$
  \State $z_{u,q} \gets Encoder_u(x_q)$
    \For {$\alpha_s$ in $\alpha$}
    \State $z_s = (1- \alpha)z_q + \alpha \mu_1$
    \State $x_s \gets Decoder(z_s,z_{u,q})$
    \If {$|f(x_s)-T|< tol$ }
     \State $x_{cf}=x_s$
     \EndIf
   \EndFor
   \State\Return {$x_{cf}$}
   
   \end{algorithmic}
   \label{alg:main2}
   \end{algorithm}

% Figure environment removed


For the Training Step, we sample a batch from the training dataset to update the parameters of Adversarial Classifier $\omega$, Encoder $\psi$ and mean and covariance $\mu_b$, $\mu_t$, and $\sigma_b$, $\sigma_t$ -- base class and target class combined as training data. Then we sample another batch to update the Encoder, Encoder$_u$, and Decoder parameters together($\psi$, $\pi$, and $\phi$). We iterate this procedure until convergence of the loss functions is reached. During the end of the Training Step: we should get a \textbf{label relevant} latent space $z$ following Gaussian Mixture distribution (see Figure \ref{fig:pca} for PCA of $z$ in Appendix) and a \textbf{label irrelevant} latent space $z_u$ to capture the label irrelevant information of samples.  
%separate the dataset according to the class and train the target VAE φt and the unified VAE ψb,t. We denote
%their encoders ϕt and ϕb,t, and their decoders ψt and ψb,t respectively. 
In Generating Step, where we generate a CE of a query sample $x_q$, We first pass $x_q$ from the base class through the Encoder and the Encoder$_u$ to obtain the sample’s embeddings $z_q$ and $z_{u,q}$. Then we get the searched potential $z_s$ by linear interpolation in latent space: $z_s = (1- \alpha)z_q + \alpha \mu_1$ ( $\alpha$ in $(0, 1]$ ), where $\mu_1$ is the mean of target class in latent space of the label relevant branch. $z_s$ and $z_{u,q}$ combined are decoded through the Decoder to get $x_s$, and the pre-trained classifier $f$ assesses its classification score. The counterfactual search stops if it crosses the decision boundary and is within a user (end-user or developer of the algorithm) specified tolerance $tol$, which is the desired maximum distance from $T$ for the generated counterfactual’s classification score. For the experiments in Section \ref{sec:eva}, the decision boundary $T$ is set to be 0.5. The search is performed by sampling along the line with a finite number of $\alpha$. 

Note that we demonstrate our method in the scenario with binary decision. However, it is straight-forward to extend our method to handle a multi-categorical decision. %The search interval could be designed with more refinement -- smaller when it is closer to the decision boundary $T$ and larger when further away from the decision boundary. %An alternative formulation of this
%search can also be done using a one-dimensional gradient descent which is demonstrated in Algorithm
%2 in the appendix. Gradient descent can be advantageous over a simple line search depending on the
%user criteria (tighter tolerance can be hard to hit with sampling) and the use case (the presence of
%categoricals can make the probabilities along the line either discontinuous and/or non-monotonic). 
\begin{table*}[bt]
\centering
\caption{\label{tab:mnist}Summary of metrics for MNIST}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Method     & time(s)           & reconstruction           & sparsity   & validity(\%) & proximity          \\ \hline
Our Method & \textbf{0.007$\pm$0.001} & \textbf{0.012$\pm$0.003} & 2.119 $\pm$0.857        & \textbf{89.5 }    & 0.193  $\pm$0.075       \\
GDL        & 1.568 $\pm$0.047         & 0.247 $\pm$0.048        & 2.121 $\pm$0.085       & 77.9    & \textbf{0.172$\pm$0.068 }         \\
Prototype        & 1.345 $\pm$0.095         & 0.926   $\pm$0.175       & \textbf{0.014$\pm$0.005} & 58.7   & 1.075$\pm$0.055 \\
RL        & 0.085   $\pm$0.012      & 0.857$\pm$0.058         & 0.015$\pm$0.026 & 60.5  & 0.834$\pm$0.047 \\ \hline
\end{tabular}
\end{table*}


\begin{table*}[bt]
\centering
\caption{\label{tab:adult}Summary of metrics for Adult income}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Method     & time(s)            & reconstruction           & sparsity   & validity(\%) & proximity           \\ \hline
Our Method & \textbf{0.008$\pm$0.002} &  \textbf{0.012$\pm$0.005}          & 0.343$\pm$0.127          & \textbf{90.3}   & \textbf{0.193$\pm$0.045}          \\
GDL        & 1.568$\pm$0.224       & 0.520$\pm$0.135 & 0.090$\pm$0.002          & 84.2   & 2.170$\pm$0.835          \\
Prototype        & 7.345$\pm$0.784         & 4.463$\pm$0.563          & 0.014$\pm$0.005 & 75.3   & 1.075$\pm$0.235 \\
RL        & 1.804$\pm$0.112       & 5.453$\pm$0.673         & \textbf{0.012$\pm$0.008} & 65.3  & 0.875$\pm$0.132 \\
\hline
\end{tabular}
\end{table*}






\begin{comment}
% Figure environment removed
\end{comment}


\begin{comment}
Figure 3: Visualization of the decision boundary in latent space, with classifier prediction
values being drawn from samples along the interpolation line between the encoding of the base
class and their target class' centroid in the latent space. The black points represent the location of the chosen
counterfactual samples. Blue areas represent regions of high base classification where red represents
regions of high target class classification.
\end{comment}

\begin{comment}
% Figure environment removed
\end{comment}

\section{Experiments and Evaluation} \label{sec:eva}

We compare our method to three other counterfactual methods introduced in Section \ref{sec:back}: Gradient Descent Method improved with Prototypes (Prototype) \cite{vanlooveren2020}, CE Generation with Reinforcement Learning (RL) \cite{samoilescuModelagnosticScalableCounterfactual2021} and Gradient Descent in the Latent
Space of a VAE (GDL) \cite{balasubramanian2021} on three datasets: MNIST \cite{zotero-844} (image), Adult Dataset \cite{zotero-1218} (tabular) and Lending Club \cite{zotero-1220} (tabular). The reason why we choose Prototype and RL is that they are both designed not only for image datasets but also for tabular datasets. Besides, they both use autoencoders to learn the representation to remain close to the data distribution, similar to our design, while they use other constraints to ensure desiderata like sparsity. We use the package ALIBI \cite{klaiseAlibiExplainAlgorithms2021} for Prototype and RL to stay close to the original design. GDL is a relatively simple baseline, but it is similar to our method because both operate in latent space. We implement GDL from scratch since the source code is not available. Our networks are trained on an Intel(r) Core(TM) i7-8700 CPU and the networks in our experiments are built based on Pytorch \cite{NEURIPS2019_9015}

\subsection{Measures for Comparison} We evaluate the quality of CEs by the measures taken from literature \cite{verma2020,barr2021,wachterCounterfactualExplanationsOpening2018a}:  (i) \textbf{counterfactual generation time}, time required to find a CE for a given query sample (ii) \textbf{validity}, the
percentage success in generating CEs that the target labels requested by the users are reached (iii) \textbf{proximity}, distance($L_2$
norm) from query samples to CEs in original space (iv) \textbf{sparsity}, $L_1$
norm of the change vector in original space (v) \textbf{reconstruction loss} -- a measure of the CE being in sample. We pass a CE through the autoencoder and measure its loss. A smaller loss indicates closer to the original data distribution because the autoencoder is trained on the same training dataset as the pre-trained classifier. Further details about the metrics used in the experiments can be found in the appendix.



\subsection{Datasets, Training and Evaluation}




{\bf{MNIST}} The MNIST database is a large database of handwritten digits. For MNIST, we adjust the problem as a binary classification problem of predicting ones (our base class) and sevens (our target class) while the original problem setting is multi-calcification. MNIST provides a naturally intuitive visualization of the result of a gradual change from the base class to the target class crossing the decision boundary given a query sample. The counterfactual generated shown in Figure \ref{fig:grad} exhibits a combination of characteristics from the query sample (e.g., the tilt of long-stroke) and from the target class (e.g., longer leveled stroke in sevens). Our \emph{Desideratum 1} -- generating a CE  while keeping the characteristics of the query sample which are not related to the classification prediction is reached. In our problem setting with MNIST, the tilt and length of long strokes of query sample ones are kept during the interpolation process. Table \ref{tab:mnist} (mean±SD) shows the quality of counterfactual explanations as measured by the above metrics. Our method outperforms the other methods in time, reconstruction, and validity but not in proximity and sparsity (very close to GDL in proximity, though). Our method outperforms Prototype and RL in the time dimension since it searches through the latent space with lower dimensions. It is also more substantial than GDL because it performs interpolation instead of optimization for a single query sample. It indicates that our method is suitable for high-dimensional applications which require intensive computation. 

\begin{table*}[bt]
\caption{\label{tab:lc}Summary of metrics for Lendidng Club default loan}
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Method     & time(s)           & reconstruction           & sparsity   & validity(\%) & proximity          \\ \hline
Our Method & \textbf{0.007$\pm$0.001} &  \textbf{0.132$\pm$0.081}          & 1.713$\pm$0.627          & \textbf{92.1}   & 3.532$\pm$0.258          \\
GDL        & 3.569 $\pm$0.087         & 0.142 $\pm$0.046        & 1.731 $\pm$0.068       & 76.3    & 1.652$\pm$0.668         \\
Prototype        & 3.137 $\pm$0.915         & 0.546   $\pm$0.235       & \textbf{1.016$\pm$0.035} & 75.7   & 1.975$\pm$1.045 \\
RL        & 0.035   $\pm$0.042      & 0.673$\pm$0.124         & 1.017$\pm$0.076 & 67.5  & \textbf{1.034$\pm$0.127}  \\ \hline
\end{tabular}
\end{table*}

For MNIST, we use a CNN-based network, and we train the model for 20 epochs by stochastic gradient descent using the Adam optimizer and a batch size of 100 on the training dataset. Our model uses two hyperparameters, $\lambda_{adv}$ and $\lambda_{lkd}$, for regularizing the loss functions for the network. We varied $\lambda_{adv}$ and $\lambda_{lkd}$ between 0.01 and 1 during training. During the training process, we want the three loss terms $\mathcal{L}_{adv}$, $\mathcal{L}_{lkd}$ and $\mathcal{L}_{cls}$ to remain approximately on the same scale. We found $\lambda_{adv}= 0.05$ and $\lambda_{lkd}= 0.1$ to give us a numerical balance among the three loss terms by checking the testing dataset. We use these values to report our results. The details of the hyper-parameters of the Discriminator are shown in the Appendix. 

% Figure environment removed



%validity, classifier shift, reconstruction score, and time, but lagging in proximity and sparsity.
%Note that the thin layer of changed pixels in the counterfactuals for GDI in Figure 3 is reflected by the method’s poor performance in classifier shift and reconstruction score. The noisy transformations seen in those counterfactuals is not reproduced in UVAE reconstructions - leading to the lowerperformance in CS and RECON. This is why we introduce these two measures as metrics ofrealism of counterfactuals: the UVAE smooths out samples far from the underlying data distribution.
%Meanwhile, SharpShooter counterfactuals lie closer to the data distribution, and appear more like the
%target class in Figure 3.





{\bf{ADULT}} The Adult dataset was drawn from the 1994 United States Census Bureau data. It used personal information such as education level and working hours per week to predict whether an individual earns more or less than \$50,000 per year \cite{kohavi1996}. We train a classifier with age, years of education, capital gain, capital loss, hours-per-week as continuous features, and education level as a categorical feature for simplicity (considering mutable and immutable features are not the focus of this paper). The dataset is imbalanced -- the instances made less than \$50,000 constitute 25\% of the dataset, and the instances made more than \$50,000 constitute 75\% of the dataset. To avoid the situation that the accuracy of the classifier for an imbalanced dataset only reflects the distribution of the training dataset, we train a classifier with re-weighting according to the proportion of base and target class. For tabular datasets, more prepossessing is performed compared to image datasets. We normalize the continuous features and use one-hot encoding to deal with the categorical features for the input of the autoencoder. We train the model for 100 epochs by stochastic gradient descent using the Adam optimizer and a batch size of 100. $\lambda_{adv}$ and $\lambda_{lkd}$ are set at 0.05 and 0.5. For tabular datasets, we use a multi-layer perceptron-based network. The details of the architectures of the networks for the Adult income dataset are shown in the appendix. Based on the comparison results shown in Table \ref{tab:adult}, our method outperforms in time, reconstruction dimensions and proximity, while RL is stronger in sparsity. Intuitively, perturbation in original space without much guidance could quickly end up as adversarial samples \cite{goodfellow2015}, which provide non-actionable CEs. In contrast, operations in latent space (our method and GDL) could be closer to the original datasets. 



{\bf{Lending Club}} Lending Club is a peer-to-peer lending company that allows individuals to lend to other individuals \cite{zotero-952}. This tabular dataset includes whether a borrower defaulted on their loan size, annual income, debt-to-income ratio, FICO score, loan length, etc. We train a classifier to predict default using five continuous and one categorical feature. Since this dataset is also imbalanced -- the default class constitutes around 15\% of the population, we adjust the loss functions of the classifier and the autoencoder to re-weighted versions based on the proportion of each class. From Table \ref{tab:lc}, we find that similar to the Adult income dataset, our method is much faster at generating CEs and excels at reconstruction and validity. The training details of this dataset are similar to ADULT. Furthermore, more details are included in the Appendix. 







\section{Conclusion}
\label{sec:con}
This paper presents a novel model-agnostic algorithm for finding CEs via linear interpolation in latent space. Our method implements a framework that first disentangles the label relevant and label irrelevant dimensions and then searches in a Gaussian mixture distributed latent space of the label relevant latent dimensions for CEs given a query sample. We demonstrated our method’s advantages and disadvantages by comparing it to three similar methods (GDL, Prototype and RL) on three different datasets (MNIST, ADULT income, and Lending Club default loan). We show that our method is faster and provides more valid CEs which are closer to the original dataset (in the dimensions of time, validity and reconstruction). Based on the presented comparison, we suggest that our work could evolve around improving latent representation.


\section*{Acknowledgement}
This work has received funding from the European Union’s Horizon 2020 research and innovation programme under Marie Sklodowska-Curie Actions (grant agreement number 860630) for the project “NoBIAS - Artificial Intelligence without Bias”.

\begin{comment}
\section{Introduction}

The {\it IJCAI--23 Proceedings} will be printed from electronic
manuscripts submitted by the authors. These must be PDF ({\em Portable
        Document Format}) files formatted for 8-1/2$''$ $\times$ 11$''$ paper.

\subsection{Length of Papers}


All paper {\em submissions} to the main track must have a maximum of seven pages, plus at most two for references / acknowledgements / contribution statement / ethics statement.

The length rules may change for final camera-ready versions of accepted papers and
differ between tracks. Some tracks may disallow any contents other than the references in the last two pages, whereas others allow for any content in all pages. Similarly, some tracks allow you to buy a few extra pages should you want to, whereas others don't.

If your paper is accepted, please carefully read the notifications you receive, and check the proceedings submission information website\footnote{\url{https://proceedings.ijcai.org/info}} to know how many pages you can use for your final version. That website holds the most up-to-date information regarding paper length limits at all times.


\subsection{Word Processing Software}

As detailed below, IJCAI has prepared and made available a set of
\LaTeX{} macros and a Microsoft Word template for use in formatting
your paper. If you are using some other word processing software, please follow the format instructions given below and ensure that your final paper looks as much like this sample as possible.

\section{Style and Format}

\LaTeX{} and Word style files that implement these instructions
can be retrieved electronically. (See Section~\ref{stylefiles} for
instructions on how to obtain these files.)

\subsection{Layout}

Print manuscripts two columns to a page, in the manner in which these
instructions are printed. The exact dimensions for pages are:
\begin{itemize}
    \item left and right margins: .75$''$
    \item column width: 3.375$''$
    \item gap between columns: .25$''$
    \item top margin---first page: 1.375$''$
    \item top margin---other pages: .75$''$
    \item bottom margin: 1.25$''$
    \item column height---first page: 6.625$''$
    \item column height---other pages: 9$''$
\end{itemize}

All measurements assume an 8-1/2$''$ $\times$ 11$''$ page size. For
A4-size paper, use the given top and left margins, column width,
height, and gap, and modify the bottom and right margins as necessary.

\subsection{Format of Electronic Manuscript}

For the production of the electronic manuscript, you must use Adobe's
{\em Portable Document Format} (PDF). A PDF file can be generated, for
instance, on Unix systems using {\tt ps2pdf} or on Windows systems
using Adobe's Distiller. There is also a website with free software
and conversion services: \url{http://www.ps2pdf.com}. For reasons of
uniformity, use of Adobe's {\em Times Roman} font is strongly suggested.
In \LaTeX2e{} this is accomplished by writing
\begin{quote}
    \mbox{\tt $\backslash$usepackage\{times\}}
\end{quote}
in the preamble.\footnote{You may want to also use the package {\tt
            latexsym}, which defines all symbols known from the old \LaTeX{}
    version.}

Additionally, it is of utmost importance to specify the {\bf
        letter} format (corresponding to 8-1/2$''$ $\times$ 11$''$) when
formatting the paper. When working with {\tt dvips}, for instance, one
should specify {\tt -t letter}.

\subsection{Papers Submitted for Review vs. Camera-ready Papers}
In this document, we distinguish between papers submitted for review (henceforth, submissions) and camera-ready versions, i.e., accepted papers that will be included in the conference proceedings. The present document provides information to be used by both types of papers (submissions / camera-ready). There are relevant differences between the two versions. Find them next.

\subsubsection{Anonymity}
For the main track and some of the special tracks, submissions must be anonymous; for other special tracks they must be non-anonymous. The camera-ready versions for all tracks are non-anonymous. When preparing your submission, please check the track-specific instructions regarding anonymity.

\subsubsection{Submissions}
The following instructions apply to submissions:
\begin{itemize}
\item If your track requires submissions to be anonymous, they must be fully anonymized as discussed in the Modifications for Blind Review subsection below; in this case, Acknowledgements and Contribution Statement sections are not allowed.

\item If your track requires non-anonymous submissions, you should provide all author information at the time of submission, just as for camera-ready papers (see below); Acknowledgements and Contribution Statement sections are allowed, but optional.

\item Submissions must include line numbers to facilitate feedback in the review process . Enable line numbers by uncommenting the command {\tt \textbackslash{}linenumbers} in the preamble \footnote{New in IJCAI--23}.

\item The limit on the number of  content pages is \emph{strict}. All papers exceeding the limits will be desk rejected.
\end{itemize}

\subsubsection{Camera-Ready Papers}
The following instructions apply to camera-ready papers:

\begin{itemize}
\item Authors and affiliations are mandatory. Explicit self-references are allowed. It is strictly forbidden to add authors not declared at submission time.

\item Acknowledgements and Contribution Statement sections are allowed, but optional.

\item Line numbering must be disabled. To achieve this, comment or disable {\tt \textbackslash{}linenumbers} in the preamble.

\item For some of the tracks, you can exceed the page limit by purchasing extra pages.
\end{itemize}

\subsection{Title and Author Information}

Center the title on the entire width of the page in a 14-point bold
font. The title must be capitalized using Title Case. For non-anonymous papers, author names and affiliations should appear below the title. Center author name(s) in 12-point bold font. On the following line(s) place the affiliations.

\subsubsection{Author Names}

Each author name must be followed by:
\begin{itemize}
    \item A newline {\tt \textbackslash{}\textbackslash{}} command for the last author.
    \item An {\tt \textbackslash{}And} command for the second to last author.
    \item An {\tt \textbackslash{}and} command for the other authors.
\end{itemize}

\subsubsection{Affiliations}

After all authors, start the affiliations section by using the {\tt \textbackslash{}affiliations} command.
Each affiliation must be terminated by a newline {\tt \textbackslash{}\textbackslash{}} command. Make sure that you include the newline after the last affiliation, too.

\subsubsection{Mapping Authors to Affiliations}

If some scenarios, the affiliation of each author is clear without any further indication (\emph{e.g.}, all authors share the same affiliation, all authors have a single and different affiliation). In these situations you don't need to do anything special.

In more complex scenarios you will have to clearly indicate the affiliation(s) for each author. This is done by using numeric math superscripts {\tt \$\{\^{}$i,j, \ldots$\}\$}. You must use numbers, not symbols, because those are reserved for footnotes in this section (should you need them). Check the authors definition in this example for reference.

\subsubsection{Emails}

This section is optional, and can be omitted entirely if you prefer. If you want to include e-mails, you should either include all authors' e-mails or just the contact author(s)' ones.

Start the e-mails section with the {\tt \textbackslash{}emails} command. After that, write all emails you want to include separated by a comma and a space, following the order used for the authors (\emph{i.e.}, the first e-mail should correspond to the first author, the second e-mail to the second author and so on).

You may ``contract" consecutive e-mails on the same domain as shown in this example (write the users' part within curly brackets, followed by the domain name). Only e-mails of the exact same domain may be contracted. For instance, you cannot contract ``person@example.com" and ``other@test.example.com" because the domains are different.


\subsubsection{Modifications for Blind Review}
When submitting to a track that requires anonymous submissions,
in order to make blind reviewing possible, authors must omit their
names, affiliations and e-mails. In place
of names, affiliations and e-mails, you can optionally provide the submission number and/or
a list of content areas. When referring to one's own work,
use the third person rather than the
first person. For example, say, ``Previously,
Gottlob~\shortcite{gottlob:nonmon} has shown that\ldots'', rather
than, ``In our previous work~\cite{gottlob:nonmon}, we have shown
that\ldots'' Try to avoid including any information in the body of the
paper or references that would identify the authors or their
institutions, such as acknowledgements. Such information can be added post-acceptance to be included in the camera-ready
version.
Please also make sure that your paper metadata does not reveal
the authors' identities.

\subsection{Abstract}

Place the abstract at the beginning of the first column 3$''$ from the
top of the page, unless that does not leave enough room for the title
and author information. Use a slightly smaller width than in the body
of the paper. Head the abstract with ``Abstract'' centered above the
body of the abstract in a 12-point bold font. The body of the abstract
should be in the same font as the body of the paper.

The abstract should be a concise, one-paragraph summary describing the
general thesis and conclusion of your paper. A reader should be able
to learn the purpose of the paper and the reason for its importance
from the abstract. The abstract should be no more than 200 words long.

\subsection{Text}

The main body of the text immediately follows the abstract. Use
10-point type in a clear, readable font with 1-point leading (10 on
11).

Indent when starting a new paragraph, except after major headings.

\subsection{Headings and Sections}

When necessary, headings should be used to separate major sections of
your paper. (These instructions use many headings to demonstrate their
appearance; your paper should have fewer headings.). All headings should be capitalized using Title Case.

\subsubsection{Section Headings}

Print section headings in 12-point bold type in the style shown in
these instructions. Leave a blank space of approximately 10 points
above and 4 points below section headings.  Number sections with
Arabic numerals.

\subsubsection{Subsection Headings}

Print subsection headings in 11-point bold type. Leave a blank space
of approximately 8 points above and 3 points below subsection
headings. Number subsections with the section number and the
subsection number (in Arabic numerals) separated by a
period.

\subsubsection{Subsubsection Headings}

Print subsubsection headings in 10-point bold type. Leave a blank
space of approximately 6 points above subsubsection headings. Do not
number subsubsections.

\paragraph{Titled paragraphs.} You should use titled paragraphs if and
only if the title covers exactly one paragraph. Such paragraphs should be
separated from the preceding content by at least 3pt, and no more than
6pt. The title should be in 10pt bold font and to end with a period.
After that, a 1em horizontal space should follow the title before
the paragraph's text.

In \LaTeX{} titled paragraphs should be typeset using
\begin{quote}
    {\tt \textbackslash{}paragraph\{Title.\} text} .
\end{quote}

\subsection{Special Sections}

\subsubsection{Appendices}
You may move some of the contents of the paper into one or more appendices that appear after the main content, but before references. These appendices count towards the page limit and are distinct from the supplementary material that can be submitted separately through CMT. Such appendices are useful if you would like to include highly technical material (such as a lengthy calculation) that will disrupt the flow of the paper. They can be included both in papers submitted for review and in camera-ready versions; in the latter case, they will be included in the proceedings (whereas the supplementary materials will not be included in the proceedings).
Appendices are optional. Appendices must appear after the main content.
Appendix sections must use letters instead of Arabic numerals. In \LaTeX,  you can use the {\tt \textbackslash{}appendix} command to achieve this followed by  {\tt \textbackslash section\{Appendix\}} for your appendix sections.

\subsubsection{Ethical Statement}

Ethical Statement is optional. You may include an Ethical Statement to discuss  the ethical aspects and implications of your research. The section should be titled \emph{Ethical Statement} and be typeset like any regular section but without being numbered. This section may be placed on the References pages.

Use
\begin{quote}
    {\tt \textbackslash{}section*\{Ethical Statement\}}
\end{quote}

\subsubsection{Acknowledgements}

Acknowledgements are optional. In the camera-ready version you may include an unnumbered acknowledgments section, including acknowledgments of help from colleagues, financial support, and permission to publish. This is not allowed in the anonymous submission. If present, acknowledgements must be in a dedicated, unnumbered section appearing after all regular sections but before references.  This section may be placed on the References pages.

Use
\begin{quote}
    {\tt \textbackslash{}section*\{Acknowledgements\}}
\end{quote}
to typeset the acknowledgements section in \LaTeX{}.


\subsubsection{Contribution Statement}

Contribution Statement is optional. In the camera-ready version you may include an unnumbered Contribution Statement section\footnote{New in IJCAI--23}, explicitly describing the contribution of each of the co-authors to the paper. This is not allowed in the anonymous submission. If present, Contribution Statement must be in a dedicated, unnumbered section appearing after all regular sections but before references.  This section may be placed on the References pages.

Use
\begin{quote}
    {\tt \textbackslash{}section*\{Contribution Statement\}}
\end{quote}
to typeset the Contribution Statement section in \LaTeX{}.

\subsubsection{References}

The references section is headed ``References'', printed in the same
style as a section heading but without a number. A sample list of
references is given at the end of these instructions. Use a consistent
format for references. The reference list should not include publicly unavailable work.

\subsubsection{Order of Sections}
Sections should be arranged in the following order:
\begin{enumerate}
    \item Main content sections (numbered)
    \item Appendices (optional, numbered using capital letters)
    \item Ethical statement (optional, unnumbered)
    \item Acknowledgements (optional, unnumbered)
    \item Contribution statement (optional, unnumbered)
    \item References (required, unnumbered)
\end{enumerate}

\subsection{Citations}

Citations within the text should include the author's last name and
the year of publication, for example~\cite{gottlob:nonmon}.  Append
lowercase letters to the year in cases of ambiguity.  Treat multiple
authors as in the following examples:~\cite{abelson-et-al:scheme}
or~\cite{bgf:Lixto} (for more than two authors) and
\cite{brachman-schmolze:kl-one} (for two authors).  If the author
portion of a citation is obvious, omit it, e.g.,
Nebel~\shortcite{nebel:jair-2000}.  Collapse multiple citations as
follows:~\cite{gls:hypertrees,levesque:functional-foundations}.
\nocite{abelson-et-al:scheme}
\nocite{bgf:Lixto}
\nocite{brachman-schmolze:kl-one}
\nocite{gottlob:nonmon}
\nocite{gls:hypertrees}
\nocite{levesque:functional-foundations}
\nocite{levesque:belief}
\nocite{nebel:jair-2000}

\subsection{Footnotes}

Place footnotes at the bottom of the page in a 9-point font.  Refer to
them with superscript numbers.\footnote{This is how your footnotes
    should appear.} Separate them from the text by a short
line.\footnote{Note the line separating these footnotes from the
    text.} Avoid footnotes as much as possible; they interrupt the flow of
the text.

\section{Illustrations}

Place all illustrations (figures, drawings, tables, and photographs)
throughout the paper at the places where they are first discussed,
rather than at the end of the paper.

They should be floated to the top (preferred) or bottom of the page,
unless they are an integral part
of your narrative flow. When placed at the bottom or top of
a page, illustrations may run across both columns, but not when they
appear inline.

Illustrations must be rendered electronically or scanned and placed
directly in your document. They should be cropped outside \LaTeX{},
otherwise portions of the image could reappear during the post-processing of your paper.
When possible, generate your illustrations in a vector format.
When using bitmaps, please use 300dpi resolution at least.
All illustrations should be understandable when printed in black and
white, albeit you can use colors to enhance them. Line weights should
be 1/2-point or thicker. Avoid screens and superimposing type on
patterns, as these effects may not reproduce well.

Number illustrations sequentially. Use references of the following
form: Figure 1, Table 2, etc. Place illustration numbers and captions
under illustrations. Leave a margin of 1/4-inch around the area
covered by the illustration and caption.  Use 9-point type for
captions, labels, and other text in illustrations. Captions should always appear below the illustration.

\section{Tables}

Tables are treated as illustrations containing data. Therefore, they should also appear floated to the top (preferably) or bottom of the page, and with the captions below them.

\begin{table}
    \centering
    \begin{tabular}{lll}
        \hline
        Scenario  & $\delta$ & Runtime \\
        \hline
        Paris     & 0.1s     & 13.65ms \\
        Paris     & 0.2s     & 0.01ms  \\
        New York  & 0.1s     & 92.50ms \\
        Singapore & 0.1s     & 33.33ms \\
        Singapore & 0.2s     & 23.01ms \\
        \hline
    \end{tabular}
    \caption{Latex default table}
    \label{tab:plain}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{lrr}
        \toprule
        Scenario  & $\delta$ (s) & Runtime (ms) \\
        \midrule
        Paris     & 0.1          & 13.65        \\
                  & 0.2          & 0.01         \\
        New York  & 0.1          & 92.50        \\
        Singapore & 0.1          & 33.33        \\
                  & 0.2          & 23.01        \\
        \bottomrule
    \end{tabular}
    \caption{Booktabs table}
    \label{tab:booktabs}
\end{table}

If you are using \LaTeX, you should use the {\tt booktabs} package, because it produces tables that are better than the standard ones. Compare Tables~\ref{tab:plain} and~\ref{tab:booktabs}. The latter is clearly more readable for three reasons:

\begin{enumerate}
    \item The styling is better thanks to using the {\tt booktabs} rulers instead of the default ones.
    \item Numeric columns are right-aligned, making it easier to compare the numbers. Make sure to also right-align the corresponding headers, and to use the same precision for all numbers.
    \item We avoid unnecessary repetition, both between lines (no need to repeat the scenario name in this case) as well as in the content (units can be shown in the column header).
\end{enumerate}

\section{Formulas}

IJCAI's two-column format makes it difficult to typeset long formulas. A usual temptation is to reduce the size of the formula by using the {\tt small} or {\tt tiny} sizes. This doesn't work correctly with the current \LaTeX{} versions, breaking the line spacing of the preceding paragraphs and title, as well as the equation number sizes. The following equation demonstrates the effects (notice that this entire paragraph looks badly formatted, and the line numbers no longer match the text):
%
\begin{tiny}
    \begin{equation}
        x = \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i
    \end{equation}
\end{tiny}%

Reducing formula sizes this way is strictly forbidden. We {\bf strongly} recommend authors to split formulas in multiple lines when they don't fit in a single line. This is the easiest approach to typeset those formulas and provides the most readable output%
%
\begin{align}
    x = & \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \nonumber \\
    +   & \prod_{i=1}^n \sum_{j=1}^n j_i.
\end{align}%

If a line is just slightly longer than the column width, you may use the {\tt resizebox} environment on that equation. The result looks better and doesn't interfere with the paragraph's line spacing: %
\begin{equation}
    \resizebox{.91\linewidth}{!}{$
            \displaystyle
            x = \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i
        $}.
\end{equation}%

This last solution may have to be adapted if you use different equation environments, but it can generally be made to work. Please notice that in any case:

\begin{itemize}
    \item Equation numbers must be in the same font and size as the main text (10pt).
    \item Your formula's main symbols should not be smaller than {\small small} text (9pt).
\end{itemize}

For instance, the formula
%
\begin{equation}
    \resizebox{.91\linewidth}{!}{$
            \displaystyle
            x = \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j + \prod_{i=1}^n \sum_{j=1}^n j_i + \prod_{i=1}^n \sum_{j=1}^n i_j
        $}
\end{equation}
%
would not be acceptable because the text is too small.

\section{Examples, Definitions, Theorems and Similar}

Examples, definitions, theorems, corollaries and similar must be written in their own paragraph. The paragraph must be separated by at least 2pt and no more than 5pt from the preceding and succeeding paragraphs. They must begin with the kind of item written in 10pt bold font followed by their number (e.g.: {\bf Theorem 1}),
optionally followed by a title/summary between parentheses in non-bold font and ended with a period (in bold).
After that the main body of the item follows, written in 10 pt italics font (see below for examples).

In \LaTeX{} we strongly recommend that you define environments for your examples, definitions, propositions, lemmas, corollaries and similar. This can be done in your \LaTeX{} preamble using \texttt{\textbackslash{newtheorem}} -- see the source of this document for examples. Numbering for these items must be global, not per-section (e.g.: Theorem 1 instead of Theorem 6.1).

\begin{example}[How to write an example]
    Examples should be written using the example environment defined in this template.
\end{example}

\begin{theorem}
    This is an example of an untitled theorem.
\end{theorem}

You may also include a title or description using these environments as shown in the following theorem.

\begin{theorem}[A titled theorem]
    This is an example of a titled theorem.
\end{theorem}

\section{Proofs}

Proofs must be written in their own paragraph(s) separated by at least 2pt and no more than 5pt from the preceding and succeeding paragraphs. Proof paragraphs should start with the keyword ``Proof." in 10pt italics font. After that the proof follows in regular 10pt font. At the end of the proof, an unfilled square symbol (qed) marks the end of the proof.

In \LaTeX{} proofs should be typeset using the \texttt{\textbackslash{proof}} environment.

\begin{proof}
    This paragraph is an example of how a proof looks like using the \texttt{\textbackslash{proof}} environment.
\end{proof}


\section{Algorithms and Listings}

Algorithms and listings are a special kind of figures. Like all illustrations, they should appear floated to the top (preferably) or bottom of the page. However, their caption should appear in the header, left-justified and enclosed between horizontal lines, as shown in Algorithm~\ref{alg:algorithm}. The algorithm body should be terminated with another horizontal line. It is up to the authors to decide whether to show line numbers or not, how to format comments, etc.

In \LaTeX{} algorithms may be typeset using the {\tt algorithm} and {\tt algorithmic} packages, but you can also use one of the many other packages for the task.

\begin{algorithm}[tb]
    \caption{Example algorithm}
    \label{alg:algorithm}
    \textbf{Input}: Your algorithm's input\\
    \textbf{Parameter}: Optional list of parameters\\
    \textbf{Output}: Your algorithm's output
    \begin{algorithmic}[1] %[1] enables line numbers
        \STATE Let $t=0$.
        \WHILE{condition}
        \STATE Do some action.
        \IF {conditional}
        \STATE Perform task A.
        \ELSE
        \STATE Perform task B.
        \ENDIF
        \ENDWHILE
        \STATE \textbf{return} solution
    \end{algorithmic}
\end{algorithm}

\section{\LaTeX{} and Word Style Files}\label{stylefiles}

The \LaTeX{} and Word style files are available on the IJCAI--23
website, \url{https://ijcai-23.org/}.
These style files implement the formatting instructions in this
document.

The \LaTeX{} files are {\tt ijcai23.sty} and {\tt ijcai23.tex}, and
the Bib\TeX{} files are {\tt named.bst} and {\tt ijcai23.bib}. The
\LaTeX{} style file is for version 2e of \LaTeX{}, and the Bib\TeX{}
style file is for version 0.99c of Bib\TeX{} ({\em not} version
0.98i). The {\tt ijcai23.sty} style differs from the {\tt
        ijcai22.sty} file used for IJCAI--22.

The Microsoft Word style file consists of a single file, {\tt
        ijcai23.docx}. This template differs from the one used for
IJCAI--22.

These Microsoft Word and \LaTeX{} files contain the source of the
present document and may serve as a formatting sample.

Further information on using these styles for the preparation of
papers for IJCAI--23 can be obtained by contacting {\tt
        proceedings@ijcai.org}.

\end{comment}

\bibliographystyle{named}
\bibliography{ijcai23}

\appendix





\subsection{Metrics for Comparison}

% Figure environment removed
We evaluate the quality of counterfactuals by the metrics taken from literature: 
\begin{inparaenum}[1)]
\item time - time required to find a counterfactual for a query sample. Time is one of the most significant strengths of our method. We use the time spent to generate a counterfactual explanation for a query sample on average. Our approach is particularly fast compared to other methods because generating a CE requires a search through a relatively lower dimension and projection through the decoder for no more than a fixed number of interpolated points (search stops when criteria are met). By comparison, iterative search based on gradients or perturbing in the input space can be quite expensive if the search distance is large, the learning rate is low, or the mutable dimensions are high. We do not include the time of training the autoencoder with Gaussian mixture latent space due to the fact that this training time is not trivial. It is a one-time cost and is not related to the scalability of CEs generation for a query sample. Ideally, our method triumphs even more, when the query sample is high dimensional. 
\item validity - the percentage of success in generating counterfactuals cross the decision boundary. The goal for generating counterfactuals is that they are counterfactuals, which requires them to be classified as the target class. We evaluate validity by including the pre-trained classifier in the algorithm. Since linear search is performed with the classifier is ended when certain criteria are met for the classifier. We could see, in general, that the performance of this dimension for the three methods is satisfying since all three methods employ the classifier as the end search criteria. \item proximity - a measure of distance from query sample to counterfactual in latent space. Proximity is used as a constraint to promote the performance of counterfactual generation \cite{verma2020}. This metric measures how close a counterfactual is to the query sample it is generated from. Usually, this measure is calculated by the $L2$ norm in the input space. \item sparsity - a measure of features changed. Sparsity is a common metric in the counterfactual literature \cite{verma2020}. Sparsity is a measure of how sparse the change vector is. We calculate the $L1$ norm of the change vector in input space. \item reconstruction loss - a measure of the CE being close to data manifold \cite{barr2021}. We can measure the closeness of a CE to its original dataset by passing a counterfactual through the autoencoder and measuring its reconstruction loss. During the training process of the autoencoder, we try to minimize the reconstruction loss. Intuitively, a sample with a smaller loss should be more in-sample and closer to the original data distribution. An unseen sample should have a larger reconstruction loss concerning the autoencoder. 
\end{inparaenum} 
\subsection{PCA of latent space}


\subsection{Further Thoughts about the Comparison Results}
We could see that our method performs better on three dimensions above: counterfactual generation time, validity and reconstruction loss due to its design. We design the architecture based on our three desiderata and in the end, through the experiment, we show that the desiderata are achieved. Our method performs linear interpolation in the latent space instead of optimization in the original space, it has the highest validity, and also because the linear interpolation happens in latent space, we cannot guarantee the sparsity and proximity in the original space as shown in the tables, which is stated in Section \ref{sec:back}.


\subsection{Algorithms of GDL}

In GDL, instead of updating parameters of DNN which is commonly applied, we update the input query sample and the latent projection of the query sample correspondingly until it cross the decision boundary and the tolerance is satisfied. 


\begin{algorithm}
\caption{Gradient Descent Latent Space}
   \begin{algorithmic}[1]
   \label{alg:gdl}
   \Require $\psi$ and $\phi$  the initial parameters of Encoder and Decoder; $f$ classifier; $x_b$ the query sample; $tol$ tolerance; $p$ probability of target counterfactual class ($0.5$ for decision boundary)
  
      \State $z_b \gets \psi(x)$
      \State $z_t \gets z_b $
      
      \State $z_t \stackrel{+}{\gets} \bigtriangledown_{z_{t}} (T -f(\phi(z)))^2$
      \State $x_t \gets \phi(z_t)$
      
    
     
    \If {$|f(x_t)-T|< tol$ \bf{and} $f(x_t)>p$} 

     
    
   \State $x_{cf}=\phi(z_t)$
   \State\Return $x_{cf}$
   \EndIf
   \end{algorithmic}
   \end{algorithm}

\subsection{Repetition} We repeat experiments on each dataset five times. Before each repetition, we randomly split data into training data (80\%) and test data (20\%) for the computation of the standard errors of the metrics. 



\subsection{Details of Autoencoders and Classifer}

We tabulate the detailed structures of autoencoders and classifiers during experiment in Section \ref{sec:eva}. For tabular datasets, to avoid break the differentiation of $(T -f(\phi(z)))^2$, instead of argmax for decoder output, we apply temperature annealing softmax to have a sharper reconstructed sample where we set $t=0.5$. 

\begin{table}[H]
\centering
\caption{\label{tab:adultc_1} The network structure of MNIST
classifier}
\begin{tabular}{|l|}
\hline
Input 28*28            \\ \hline
Linear 28*28-128, ReLu \\ \hline
Linear 128-64, ReLu    \\ \hline
Linear 64-32, ReLu     \\ \hline
Linear 32-1, Sigmoid   \\ \hline
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{\label{tab:adultautoencoder} The network structure of Adult income autoencoder}
\begin{tabular}{|l|l|}
\hline
Encoder                                               & Decoder                                                       \\ \hline
Input(1,28,28)                                        & Linear(25,128), Relu                                    \\ \hline
Conv2d(1,8,2), stride 1,  ReLu                  & Linear(128, 29*29*32), unflatten                    \\ \hline
Conv2d(8,16,2), stride 1,  BatchNorm2d 16,ReLu & convtranspose2d(32,16,2),stride 1, batchnorm2d 16,  ReLu \\ \hline
Conv2d(16,32,2), stride 1, Leaky ReLu, flatten        & convtranspose2d(16,8,2),stride 1, batchnorm2d 8, ReLu   \\ \hline
Linear(22912, 128), ReLu,                       & convtranspose2d(8,1,2), stride 1, sigmoid                     \\ \hline
Linear(128,15)                                        & output(1,28,28)                                               \\ \hline
\end{tabular}
\end{table}

The structure of Encoder$_u$ is similar to Encoder but with the last layer of 25 nodes.
\begin{table}[H]
\centering
\caption{\label{tab:adultc} The network structure of Adult income classifier}
\begin{tabular}{|l|}
\hline
Input $x$(cont\_in 6 + cat\_in 16/one-hot encoding) \\ \hline
Linear 21-10, ReLu                                \\ \hline
Linear 10-4, ReLu                                 \\ \hline
Linear 4-2, ReLu                                  \\ \hline
Linear 2-1, Sigmoid                               \\ \hline
\end{tabular}
\end{table}


\begin{table}[H]
\centering

\caption{\label{tab:adultautoencoder_2} The network structure of Adult income autoencoder}

\begin{tabular}{|l|ll|}
\hline
Encoder                                                   & Decoder                                &                                                  \\ \hline
Input x(cont\_in 6/min-max + cat\_in 16/one-hot encoding) & Linear 3-6, LeakyReLu                  &                                                  \\ \hline
Linear 21-12, LeakyReLu                                   & Linear 6-12, LeakyReLu                 &                                                  \\ \hline
Linear 12-24, LeakyRelu                                   & Linear 12-24, LeakyReLu                &                                                  \\ \hline
Linear 24-12, LeakyReLu                                   & \multicolumn{1}{l|}{Linear 24-5, Tanh} & Linear 24-16, softmax \\ \hline
Linear 12-6, LeakyReLu                                    & \multicolumn{1}{l|}{cont\_output 5}    & cat\_output 16                                   \\ \hline
Linear 6-3                                                & \multicolumn{1}{l|}{}                  &                                                  \\ \hline
\end{tabular}
\end{table}




%% The file named.bst is a bibliography style file for BibTeX 0.99c


\end{document}

