
@inproceedings{kipf2017semi,
	title={Semi-Supervised Classification with Graph Convolutional Networks},
	author={Kipf, Thomas N. and Welling, Max},
	booktitle={International Conference on Learning Representations (ICLR)},
	year={2017}
}

@book{Wendt,
	author = {John F. Wendt},
	title = "Computational Fluid Dynamics - An Introduction",
	publisher = "Springer-Verlag",
	year = "2009",
	address  = "Berlin Heidelberg",
}

@book{Versteeg,
	author = {H. K. Versteeg and W. Malalasekera},
	title = "An Introduction to Computational Fluid Dynamics - The Finite Volume Method, Second Edition",
	publisher = "Pearson Education Limited",
	year = "2007",
	address  = "Harlow",
}

@article{naca75,
	author = {Ladson, Charles L. and Brooks, Cuyler W., Jr },
	title = {Development of a Computer Program to Obtain Ordinates for NACA 4-Digit, 4-Digit Modified, 5-Digit, and 16-Series Airfoils},
	journal = {NASA Technical Memorandum X-3284},
	year = {1975}
}

@article{Bhatnagar_2019,
	title={Prediction of aerodynamic flow fields using convolutional neural networks},
	volume={64},
	ISSN={1432-0924},
	DOI={10.1007/s00466-019-01740-0},
	number={2},
	journal={Computational Mechanics},
	publisher={Springer Science and Business Media LLC},
	author={Bhatnagar, Saakaar and Afshar, Yaser and Pan, Shaowu and Duraisamy, Karthik and Kaushik, Shailendra},
	year={2019},
	month={Jun},
	pages={525–545}
}

@inproceedings{cfdnet2020,
	author = {Obiols-Sales, Octavi and Vishnu, Abhinav and Malaya, Nicholas and Chandramowliswharan, Aparna},
	title = {CFDNet: A Deep Learning-Based Accelerator for Fluid Simulations},
	year = {2020},
	isbn = {9781450379830},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3392717.3392772},
	abstract = {CFD is widely used in physical system design and optimization, where it is used to predict engineering quantities of interest, such as the lift on a plane wing or the drag on a motor vehicle. However, many systems of interest are prohibitively expensive for design optimization, due to the expense of evaluating CFD simulations.To render the computation tractable, reduced-order or surrogate models are used to accelerate simulations while respecting the convergence constraints provided by the higher-fidelity solution. This paper introduces CFDNet - a physical simulation and deep learning coupled framework, for accelerating the convergence of Reynolds Averaged Navier-Stokes simulations. CFDNet is designed to predict the primary physical properties of the fluid including velocity, pressure, and eddy viscosity using a single convolutional neural network at its core. We evaluate CFDNet on a variety of use-cases, both extrapolative and interpolative, where test geometries are observed/not-observed during training. Our results show that CFDNet meets the convergence constraints of the domain-specific physics solver while outperforming it by 1.9 - 7.4X on both steady laminar and turbulent flows. Moreover, we demonstrate the generalization capacity of CFDNet by testing its prediction on new geometries unseen during training. In this case, the approach meets the CFD convergence criterion while still providing significant speedups over traditional domain-only models.},
	booktitle = {Proceedings of the 34th ACM International Conference on Supercomputing (ICS 20)},
	articleno = 3,
	
	numpages = 12,
	keywords = {AI for science, turbulent flows, computational fluid dynamics, deep learning, physics-machine learning coupled framework},
	
	location = {Barcelona, Spain}
}

@Article{Kashefi2021,
	author    = {Kashefi, Ali and Rempe, Davis and Guibas, Leonidas J.},
	journal   = {Physics of Fluids},
	title     = {A point-cloud deep learning framework for prediction of fluid flow fields on irregular geometries},
	year      = {2021},
	issn      = {1070-6631},
	month     = feb,
	number    = {2},
	pages     = {027104},
	volume    = {33},
	doi       = {10.1063/5.0033376},
	file      = {Full Text PDF:https\://aip.scitation.org/doi/pdf/10.1063/5.0033376:application/pdf},
	publisher = {American Institute of Physics},
}
@inproceedings{FeyLenssen2019,
	title={Fast Graph Representation Learning with {PyTorch Geometric}},
	author={Fey, Matthias and Lenssen, Jan E.},
	booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
	year={2019},
	journal   = {CoRR},
	volume    = {abs/1903.02428},
	eprint    = {1903.02428},
}

@inproceedings{Aulich2019,
	author    =  {Aulich, Marcel and Kueppers, Fabian and Schmitz, Andreas and Voß, Christian},
	title     = {Surrogate {Estimations} of {Complete} {Flow} {Fields} of {Fan} {Stage} {Designs} via {Deep} {Neural} {Networks}},
	volume = { 2D: Turbomachinery},
	address="Phoenix, USA",
	booktitle = {Turbo Expo: Power for Land, Sea, and Air},
	publisher = {American Society of Mechanical Engineers Digital Collection},
	year = {2019},
	month = {06},
	doi = {10.1115/GT2019-91258},
}

@Article{Brunton2020,
	author   = {Brunton, Steven L. and Noack, Bernd R. and Koumoutsakos, Petros},
	journal  = {Annual Review of Fluid Mechanics},
	title    = {Machine {Learning} for {Fluid} {Mechanics}},
	year     = {2020},
	number   = {1},
	pages    = {477--508},
	volume   = {52},
	abstract = {The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful information-processing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications.},
	doi      = {10.1146/annurev-fluid-010719-060214},
	file     = {:/data/Nextcloud/literature/Brunton-Noak-ML4FluidMechanics-annurev-fluid-010719-060214.pdf:PDF;Full Text PDF:http\://www.annualreviews.org/doi/pdf/10.1146/annurev-fluid-010719-060214:application/pdf},
	groups   = {Review of surrogate models},
	url      = {https://doi.org/10.1146/annurev-fluid-010719-060214},
	urldate  = {2020-12-03},
}
@InProceedings{Pointnet2017,
	author     = {Charles, R. Q. and Su, H. and Kaichun, M. and Guibas, L. J.},
	booktitle  = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	title      = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	year       = {2017},
	month      = jul,
	address="Honolulu, USA",
	note       = {ISSN: 1063-6919},
	pages      = {77--85},
	abstract   = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	doi        = {10.1109/CVPR.2017.16},
	issn       = {1063-6919},
	keywords   = {data structures, data visualisation, feature extraction, image classification, image segmentation, learning (artificial intelligence), rendering (computer graphics), deep learning, 3D classification, geometric data structure, neural network, object classification, part segmentation, scene semantic parsing, PointNet, regular 3D voxel grids, Three-dimensional displays, Shape, Computer architecture, Feature extraction, Machine learning, Semantics},
	shorttitle = {{PointNet}},
}

@article{thuerey2020deepFlowPred,
	title={Deep learning methods for Reynolds-averaged Navier--Stokes simulations of airfoil flows},
	author={Thuerey, Nils and Wei{\ss}enow, Konstantin and Prantl, Lukas and Hu, Xiangyu},
	journal={AIAA Journal}, 
	year={2020},
	volume={58}, 
	number={1}, 
	pages={25--36},
	publisher={American Institute of Aeronautics and Astronautics}, 
	doi = {10.2514/1.J058291}
}

@InProceedings{Belbute-Peres2020,
	author    = {De Avila Belbute-Peres, Filipe and Economon, Thomas and Kolter, Zico},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	title     = {Combining Differentiable {PDE} Solvers and Graph Neural Networks for Fluid Flow Prediction},
	year      = {2020},
	month     = {13--18 Jul},
	pages     = {2402--2411},
	publisher = {PMLR},
	series    = {Proceedings of Machine Learning Research},
	volume    = {119},
	abstract  = {Solving large complex partial differential equations (PDEs), such as those that arise in computational fluid dynamics (CFD), is a computationally expensive process. This has motivated the use of deep learning approaches to approximate the PDE solutions, yet the simulation results predicted from these approaches typically do not generalize well to truly novel scenarios. In this work, we develop a hybrid (graph) neural network that combines a traditional graph convolutional network with an embedded differentiable fluid dynamics simulator inside the network itself. By combining an actual CFD simulator (run on a much coarser resolution representation of the problem) with the graph network, we show that we can both generalize well to new situations and benefit from the substantial speedup of neural network CFD predictions, while also substantially outperforming the coarse CFD simulation alone.},
	file      = {:/data/Nextcloud/literature/CFD_Surrogate/de-avila-belbute-peres20a.pdf:PDF},
	groups    = {ML Prediction},
	pdf       = {http://proceedings.mlr.press/v119/de-avila-belbute-peres20a/de-avila-belbute-peres20a.pdf},
	url       = {http://proceedings.mlr.press/v119/de-avila-belbute-peres20a.html},
}

@article{hamilton2020,
	author={Hamilton, William L.},
	title={Graph Representation Learning},
	journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
	volume={14},
	number={3},
	year={2020},
	pages={1-159},
	publisher={Morgan and Claypool},
	address="New York"
} 


@article{LiuZhou2020IntroGNN,
	
	author = {Liu, Zhiyuan and Zhou, Jie},
	
	title = {Introduction to Graph Neural Networks},
	
	journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	
	volume = {14},
	
	number = {2},
	
	pages = {1-127},
	
	year = {2020},
	
	DOI = {10.2200/S00980ED1V01Y202001AIM045},
	
	URL = { https://doi.org/10.2200/S00980ED1V01Y202001AIM045
	},
	
}
@article{GilmerSRVD17,
	author    = {Justin Gilmer and
	Samuel S. Schoenholz and
	Patrick F. Riley and
	Oriol Vinyals and
	George E. Dahl},
	title     = {Neural Message Passing for Quantum Chemistry},
	journal   = {CoRR},
	keyword    = {abs/1704.01212},
	year      = {2017},
	eprint    = {1704.01212},
	timestamp = {Mon, 13 Aug 2018 16:48:42 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/GilmerSRVD17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Gori2005,
	author    = {Gori, M. and Monfardini, G. and Scarselli, F.},
	booktitle = {Proceedings {IEEE} {International} {Joint} {Conference} on {Neural} {Networks}, 2005.},
	title     = {A new model for learning in graph domains},
	doi       = {10.1109/IJCNN.2005.1555942},
	pages     = {729--734 vol. 2},
	volume    = {2},
	abstract  = {In several applications the information is naturally represented by graphs. Traditional approaches cope with graphical data structures using a preprocessing phase which transforms the graphs into a set of flat vectors. However, in this way, important topological information may be lost and the achieved results may heavily depend on the preprocessing stage. This paper presents a new neural model, called graph neural network (GNN), capable of directly processing graphs. GNNs extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs. A learning algorithm for GNNs is proposed and some experiments are discussed which assess the properties of the model.},
	
	groups    = {Foundations},
	
	issn      = {2161-4407},
	
	keywords  = {Neural networks, Focusing, Application software, Machine learning, Recurrent neural networks, Encoding, Data structures, Machine learning algorithms, Tree graphs, Software engineering},
	
	month     = jul,
	
	year      = {2005},
	
}


@InProceedings{Scarselli2004,
	
	author="Scarselli, Franco
	and Tsoi, Ah Chung
	and Gori, Marco
	and Hagenbuchner, Markus",
	
	editor="Fred, Ana
	and Caelli, Terry M.
	and Duin, Robert P. W.
	and Campilho, Aur{\'e}lio C.
	and de Ridder, Dick",
	
	title="Graphical-Based Learning Environments for Pattern Recognition",
	
	booktitle="Structural, Syntactic, and Statistical Pattern Recognition",
	
	year="2004",
	
	publisher="Springer Berlin Heidelberg",
	
	address="Berlin, Heidelberg",
	
	pages="42--56",
	
	abstract="In this paper, we present a new neural network model, called graph neural network model, which is a generalization of two existing approaches, viz., the graph focused approach, and the node focused approach. The graph focused approach considers the mapping from a graph structure to a real vector, in which the mapping is independent of the particular node involved; while the node focused approach considers the mapping from a graph structure to a real vector, in which the mapping depends on the properties of the node involved. It is shown that the graph neural network model maintains some of the characteristics of the graph focused models and the node focused models respectively. A supervised learning algorithm is derived to estimate the parameters of the graph neural network model. Some experimental results are shown to validate the proposed learning algorithm, and demonstrate the generalization capability of the proposed model.",
	
	isbn="978-3-540-27868-9"
	
}



@Article{Scarselli2009,
	
	author   = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
	
	title    = {The {Graph} {Neural} {Network} {Model}},
	
	doi      = {10.1109/TNN.2008.2005605},
	
	issn     = {1941-0093},
	
	number   = {1},
	
	pages    = {61--80},
	
	volume   = {20},
	
	abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
	
	groups   = {Foundations},
	
	journal  = {IEEE Transactions on Neural Networks},
	
	keywords = {Neural networks, Biological system modeling, Data engineering, Computer vision, Chemistry, Biology, Pattern recognition, Data mining, Supervised learning, Parameter estimation, Graphical domains, graph neural networks (GNNs), graph processing, recursive neural networks},
	
	month    = jan,
	
	year     = {2009},
	
}

@ARTICLE{Bronstein2017,
	author={Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
	journal={IEEE Signal Processing Magazine}, 
	title={Geometric Deep Learning: Going beyond Euclidean data}, 
	year={2017},
	volume={34},
	number={4},
	pages={18-42},
	doi={10.1109/MSP.2017.2693418}}

@inbook{pytorch2019,
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K\"{o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	year = {2019},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems (NIPS'19)},
	articleno = {721},
	numpages = {12},
	url = {https://dl.acm.org/doi/10.5555/3454287.3455008}
}



@book{Goodfellow2016,
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	title = {Deep learning},
	address = {Cambridge, Massachusetts and London, England},
	publisher = {{The MIT Press}},
	isbn = {9780262035613},
	series = {Adaptive computation and machine learning},
	institution = {{MIT Press}}
}

@article{lstm,
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	year = {1997},
	month = {12},
	pages = {1735-1780},
	title = {Long Short-term Memory},
	volume = {9},
	journal = {Neural computation},
	doi = {10.1162/neco.1997.9.8.1735}
}

@article{inception,
	author    = {Christian Szegedy and
	Wei Liu and
	Yangqing Jia and
	Pierre Sermanet and
	Scott E. Reed and
	Dragomir Anguelov and
	Dumitru Erhan and
	Vincent Vanhoucke and
	Andrew Rabinovich},
	title     = {Going Deeper with Convolutions},
	journal   = {CoRR},
	year      = {2014},
	eprint    = {1409.4842},
	timestamp = {Mon, 13 Aug 2018 16:48:52 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/SzegedyLJSRAEVR14},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{ResNet,
	author    = {Christian Szegedy and
	Sergey Ioffe and
	Vincent Vanhoucke},
	title     = {Inception-v4, Inception-ResNet and the Impact of Residual Connections
	on Learning},
	journal   = {CoRR},
	volume    = {abs/1602.07261},
	year      = {2016},
	archivePrefix = {arXiv},
	eprint    = {1602.07261},
	timestamp = {Mon, 13 Aug 2018 16:48:39 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/SzegedyIV16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{AdamOptim2015,
	author    = {Diederik P. Kingma and
	Jimmy Ba},
	editor    = {Yoshua Bengio and
	Yann LeCun},
	title     = {Adam: {A} Method for Stochastic Optimization},
	booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
	San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
	year      = {2015},
	url       = {http://arxiv.org/abs/1412.6980},
	timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pfaff2021learning,
	author    = {Tobias Pfaff and Meire Fortunato and Alvaro Sanchez-Gonzalez and Peter Battaglia},
	booktitle = {International Conference on Learning Representations},
	title     = {Learning Mesh-Based Simulation with Graph Networks},
	url       = {https://arxiv.org/abs/2010.03409},
    eprinttype = {arXiv},
    eprint    = {2010.03409},
	groups    = {Graph Representation - Unsteady},
	year      = {2021},
}
@Article{Sekar2019,
	author    = {Sekar, Vinothkumar and Jiang, Qinghua and Shu, Chang and Khoo, Boo Cheong},
	title     = {Fast flow field prediction over airfoils using deep learning approach},
	doi       = {10.1063/1.5094943},
	issn      = {1070-6631},
	number    = {5},
	pages     = {057103},
	url       = {https://aip.scitation.org/doi/abs/10.1063/1.5094943},
	urldate   = {2021-07-01},
	volume    = {31},
	file      = {Full Text PDF:https\://aip.scitation.org/doi/pdf/10.1063/1.5094943?journalCode=phf:application/pdf},
	groups    = {Subsample to fixed grid},
	journal   = {Physics of Fluids},
	month     = may,
	publisher = {American Institute of Physics},
	year      = {2019},	
}

@inproceedings{
	Ummenhofer2020Lagrangian,
	title={Lagrangian Fluid Simulation with Continuous Convolutions},
	author={Benjamin Ummenhofer and Lukas Prantl and Nils Thuerey and Vladlen Koltun},
	booktitle={International Conference on Learning Representations (ICLR)},
	year={2020},
}

@InProceedings{NIPS2016_04df4d43,
	
	author    = {Defferrard, Micha\"{e}l and Bresson, Xavier and Vandergheynst, Pierre},
	booktitle = {Advances in Neural Information Processing Systems},
	title     = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},
	publisher = {Curran Associates, Inc.},
	url       = {https://proceedings.neurips.cc/paper/2016/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf},
	volume    = {29},
	groups    = {Graph Convolutional Layers},
	year      = {2016},
}

@InProceedings{NIPS2017_5dd9db5e,
	
	author    = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
	
	booktitle = {Advances in Neural Information Processing Systems},
	
	title     = {Inductive Representation Learning on Large Graphs},
	
	editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	
	publisher = {Curran Associates, Inc.},
	
	url       = {https://proceedings.neurips.cc/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},
	
	volume    = {30},
	
	groups    = {Graph Convolutional Layers},
	
	year      = {2017},
	
}

@Article{Morris2019,
	author     = {Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L. and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
	title      = {Weisfeiler and {Leman} {Go} {Neural}: {Higher}-{Order} {Graph} {Neural} {Networks}},
	doi        = {10.1609/aaai.v33i01.33014602},
	issn       = {2374-3468},
	language   = {en},
	number     = {01},
	pages      = {4602--4609},
	url        = {https://ojs.aaai.org/index.php/AAAI/article/view/4384},
	urldate    = {2021-07-01},
	volume     = {33},
	copyright  = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	file       = {Full Text PDF:https\://ojs.aaai.org/index.php/AAAI/article/download/4384/4262:application/pdf},
	groups     = {Graph Convolutional Layers},
	journal    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	month      = jul,
	shorttitle = {Weisfeiler and {Leman} {Go} {Neural}},
	year       = {2019},
}

@article{raissi2019physics,
	title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
	journal={Journal of Computational Physics},
	volume={378},
	pages={686--707},
	year={2019},
	publisher={Elsevier},
	doi = {10.1016/j.jcp.2018.10.045}
}

@article{Raissi2018,
	title={Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations},
	author={Raissi, Maziar and Yazdani, Alireza and Karniadakis, George Em},
	journal={Science},
	volume={367},
	number={6481},
	pages={1026--1030},
	year={2020},
	publisher={American Association for the Advancement of Science}
}

@article{spalart,
	author = {P. Spalart and S. Allmaras},
	title = {A one-equation turbulence model for aerodynamic flows},
	journal = {30th Aerospace Sciences Meeting and Exhibit},
	chapter = {},
	pages = {},
	year = {1992},
	doi = {10.2514/6.1992-439},
}

@article{DropoutSrivastava2014,
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	year = {2014},
	issue_date = {January 2014},
	publisher = {JMLR.org},
	pages = {1929–1958},
	volume = {15},
	number = {1},
	doi ={10.5555/2627435.2670313},
	issn = {1532-4435}}


@article{sergeev2018horovod,
  Author = {Alexander Sergeev and Mike Del Balso},
  Journal = {arXiv:1802.05799},
  Title = {Horovod: fast and easy distributed deep learning in {TensorFlow}},
  Year = {2018}
}

@inproceedings{kahip,
             AUTHOR = {Sanders, Peter and Schulz, Christian},
             TITLE = {{Think Locally, Act Globally: Highly Balanced Graph Partitioning}},
             BOOKTITLE = {Proceedings of the 12th International Symposium on Experimental Algorithms (SEA'13)},
             SERIES = {LNCS},
             PUBLISHER = {Springer},
             YEAR = {2013},
             VOLUME = {7933},
             PAGES = {164--175}
}

@inproceedings{Halos,
author = {Ding, Chris and He, Yun},
title = {A Ghost Cell Expansion Method for Reducing Communications in Solving PDE Problems},
year = {2001},
isbn = {158113293X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/582034.582084},
booktitle = {Proceedings of the 2001 ACM/IEEE Conference on Supercomputing},
pages = {50},
numpages = {1},
keywords = {ghost cells, latency, PDE, bandwidth, near neighbor communication},
location = {Denver, Colorado},
series = {SC '01}
}

@inproceedings{pbg,
  title={{PyTorch-BigGraph: A Large-scale Graph Embedding System}},
  author={Lerer, Adam and Wu, Ledell and Shen, Jiajun and Lacroix, Timothee and Wehrstedt, Luca and Bose, Abhijit and Peysakhovich, Alex},
  booktitle={Proceedings of the 2nd SysML Conference},
  year={2019},
  address={Palo Alto, CA, USA}
}

 @inproceedings{hamilton2017inductive,
     author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
     title = {Inductive Representation Learning on Large Graphs},
     booktitle = {NIPS},
     year = {2017}
   }
   
@article{choquette_nvidia_2021,
	title = {{NVIDIA} {A100} {Tensor} {Core} {GPU}: {Performance} and {Innovation}},
	volume = {41},
	issn = {1937-4143},
	shorttitle = {{NVIDIA} {A100} {Tensor} {Core} {GPU}},
	doi = {10.1109/MM.2021.3061394},
	number = {2},
	journal = {IEEE Micro},
	author = {Choquette, Jack and Gandhi, Wishwesh and Giroux, Olivier and Stam, Nick and Krashinsky, Ronny},
	month = mar,
	year = {2021},
	keywords = {A100, Artificial intelligence, Bandwidth, Benchmark testing, C++20, CUDA, Deep Learning, GPU, Graphics processing units, NVLink, Parallel processing, Tensor Core, Tensors, Throughput},
	pages = {29--35},
}

@InProceedings{Gocht2020,
  author    = {Andreas Gocht and Robert Sch{\"o}ne and Jan Frenzel},
  booktitle = {{Tools for High Performance Computing 2018 / 2019}},
  title     = {{Advanced Python Performance Monitoring with Score-P}},
  year      = {2021},
  note      = {\href{https://doi.org/10.1007/978-3-030-66057-4_14}{\mbox{DOI: 10.1007/978-3-030-66057-4\_14}}},
  pages     = {261-270},
  publisher = {Springer International Publishing},
  isbn      = {978-3-030-66057-4},
}

@InProceedings{vampir,
  author    = {Kn{\"u}pfer, Andreas and Brunst, Holger and Doleschal, Jens and Jurenz, Matthias and Lieber, Matthias and Mickler, Holger and M{\"u}ller, Matthias S. and Nagel, Wolfgang E.},
  title     = {{The Vampir Performance Analysis Tool-Set}},
  booktitle = {{Tools for High Performance Computing}},
  year      = {2008},
  note      = {\href{https://doi.org/10.1007/978-3-540-68564-7_9}{\mbox{DOI: 10.1007/978-3-540-68564-7\_9}}},
}

@INPROCEEDINGS{Gocht2021,
  author={Gocht-Zech, Andreas and Grund, Alexander and Schöne, Robert},
  booktitle={2021 IEEE/ACM International Workshop on Programming and Performance Visualization Tools (ProTools)}, 
  title={Controlling the Runtime Overhead of Python Monitoring with Selective Instrumentation}, 
  year={2021},
  volume={},
  number={},
  doi={10.1109/ProTools54808.2021.00008}
}

@misc{scorep_python,
    author = {Andreas Gocht-Zech and  Alexander Grund and Sebastian (NanoNabla) and  Bert Wesarg and  Michael Knobloch},
    title = {scorep - scorep\_binding\_python},
    howpublished = {\url{https://github.com/score-p/scorep_binding_python}, {Accessed 2022-11-15}}
}

@InProceedings{Knuepfer2012,
  author    = {Kn{\"u}pfer, Andreas and R{\"o}ssel, Christian and Mey, Dieter an and Biersdorff, Scott and Diethelm, Kai and Eschweiler, Dominic and Geimer, Markus and Gerndt, Michael and Lorenz, Daniel and Malony, Allen and Nagel, Wolfgang E. and Oleynik, Yury and Philippen, Peter and Saviankou, Pavel and Schmidl, Dirk and Shende, Sameer and Tsch{\"u}ter, Ronny and Wagner, Michael and Wesarg, Bert and Wolf, Felix},
  title     = {{Score-P: A Joint Performance Measurement Run-Time Infrastructure for Periscope,Scalasca, TAU, and Vampir}},
  booktitle = {{Tools for High Performance Computing 2011}},
  year      = {2012},
  note      = {\href{https://doi.org/10.1007/978-3-642-31476-6_7}{\mbox{DOI: 10.1007/978-3-642-31476-6\_7}}},
  abstract  = {This paper gives an overview about the Score-P performance measurement infrastructure which is being jointly developed by leading HPC performance tools groups. It motivates the advantages of the joint undertaking from both the developer and the user perspectives, and presents the design and components of the newly developed Score-P performance measurement infrastructure. Furthermore, it contains first evaluation results in comparison with existing performance tools and presents an outlook to the long-term cooperative development of the new system.},
  isbn      = {978-3-642-31476-6},
}

@inproceedings{stroenischPASC22,
	author = {Str\"{o}nisch, Sebastian and Meyer, Marcus and Lehmann, Christoph},
	title = {Flow Field Prediction on Large Variable Sized 2D Point Clouds with Graph Convolution},
	year = {2022},
	isbn = {9781450394109},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3539781.3539789},
	booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
	articleno = {6},
	numpages = {10},
	keywords = {surrogate model, graph convolution, machine learning, CFD},
	location = {Basel, Switzerland},
	series = {PASC '22}
}


@techreport{metis97,
	author =  {George Karypis and Vipin Kumar},
	institution = {Computer Science \& Engineering (CS\&E) Technical Reports},
	title = {METIS: A Software Package for Partitioning Unstructured Graphs, Partitioning Meshes, and Computing Fill-Reducing Orderings of Sparse Matrices},
	year = 1997,
	number = {97-061}
}

@manual{DDPDesignNote,
	author = "{PyTorch Contributors}",
	year = 2022,
	title = "Distributed Data Parallel - Internal Design",
	address = {\url{https://pytorch.org/docs/master/notes/ddp.html}, {Accessed 2022-11-19}}}
				
@manual{NvidiaNCCL,
	author = "{NVIDIA Corporation}",
	year = 2022,
	title = "NVIDIA Collective Communications Library (NCCL)",
	address = {\url{https://developer.nvidia.com/nccl}, {Accessed 2022-11-19}}}


@manual{torchscript,
	author = "{PyTorch Contributors}",
	year = 2022,
	title = "TorchScript",
	address = {\url{https://pytorch.org/docs/stable/jit.html}, {Accessed 2022-11-19}}}

@manual{torchAMP,
	author = "{PyTorch Contributors}",
	year = 2022,
	title = "Automatic Mixed Precision package - torch.amp",
	address = {\url{https://pytorch.org/docs/stable/amp.html}, {Accessed 2022-11-19}}}


@article{SchwarzeURANS2006,
	author = {Schwarze, Rüdiger and Obermeier, Frank},
	title = {Performance and limitations of the unsteady RANS approach},
	journal = {PAMM},
	volume = {6},
	number = {1},
	pages = {543-544},
	doi = {10.1002/pamm.200610252},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/pamm.200610252},
	year = {2006}
}

@inproceedings{HYDRA, 
	author = {Lapworth, Leigh}, 
	year = {2004}, 
	month = {07}, 
	pages = {}, 
	title = {HYDRA-CFD: A Framework for Collaborative CFD Development} }

@manual{NCCL,
	author = "{Nvidia Corporation}",
	year = 2022,
	title = "Optimized primitives for inter-GPU communication.",
	address = {\url{https://github.com/NVIDIA/nccl}, {Accessed 2022-11-17}}}

@article{Lino2021,
	title={Simulating Continuum Mechanics with Multi-Scale Graph Neural Networks},
	author={Mario Lino and Chris D. Cantwell and Anil Anthony Bharath and Stathi Fotiadis},
	journal={ArXiv},
	year={2021},
	date        = {2021-06-09},
	volume={abs/2106.04900}
}

@Article{Lino2022,
	author      = {Mario Lino and Stathi Fotiadis and Anil A. Bharath and Chris Cantwell},
	title       = {Towards Fast Simulation of Environmental Fluid Mechanics with Multi-Scale Graph Neural Networks},
	date        = {2022-05-05},
	year        = {2022},
	eprint      = {2205.02637v1},
	eprintclass = {physics.flu-dyn},
	eprinttype  = {arXiv},
	doi         = {10.48550/arxiv.2205.02637},
	file        = {online:http\://arxiv.org/pdf/2205.02637v1:PDF},
	groups      = {Multiscale Graphs},
	keywords    = {physics.flu-dyn, cs.LG},
}

@Article{Lino2022a,
	author    = {Mario Lino and Stathi Fotiadis and Anil A. Bharath and Chris D. Cantwell},
	title     = {Multi-scale rotation-equivariant graph neural networks for unsteady Eulerian fluid dynamics},
	journal   = {Physics of Fluids},
	year      = {2022},
	volume    = {34},
	number    = {8},
	pages     = {087110},
	month     = {aug},
	doi       = {10.1063/5.0097679},
	groups    = {Multiscale Graphs},
	publisher = {{AIP} Publishing},
}

@Article{Fortunato2022,
	author       = {Meire Fortunato and Tobias Pfaff and Peter Wirnsberger and Alexander Pritzel and Peter Battaglia},
	title        = {MultiScale MeshGraphNets},
	date         = {2022-10-02},
	year        = {2022},
	eprint       = {2210.00612v1},
	eprintclass  = {cs.LG},
	eprinttype   = {arXiv},
	file         = {online:http\://arxiv.org/pdf/2210.00612v1:PDF},
	groups       = {Multiscale Graphs},
	Journal = {2nd AI4Science Workshop at the 39th International Conference on Machine Learning (ICML)},
	keywords     = {cs.LG, cs.CE},
}

@Article{Cao2022UNDERREVIEW,
	author      = {Yadi Cao and Menglei Chai and Minchen Li and Chenfanfu Jiang},
	title       = {Bi-Stride Multi-Scale Graph Neural Network for Mesh-Based Physical Simulation},
	date        = {2022-10-05},
	year        = {2022},
	eprint      = {2210.02573v1},
	eprintclass = {cs.LG},
	eprinttype  = {arXiv},
	file        = {online:http\://arxiv.org/pdf/2210.02573v1:PDF},
	groups      = {Multiscale Graphs},
	keywords    = {cs.LG},
}

@inproceedings{deepspeedZERO20,
	author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
	title = {ZeRO: Memory Optimizations toward Training Trillion Parameter Models},
	year = {2020},
	isbn = {9781728199986},
	publisher = {IEEE Press},
	doi = {10.5555/3433701.3433727},
	booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	articleno = {20},
	numpages = {16},
	location = {Atlanta, Georgia},
	series = {SC '20}
}

@inproceedings{TensorFlow,
	title	= {TensorFlow: A system for large-scale machine learning},
	author	= {Martin Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
	year	= {2016},
	URL	= {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
	booktitle	= {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
	pages	= {265--283}
}

