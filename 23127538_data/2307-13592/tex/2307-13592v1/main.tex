\documentclass[conf]{new-aiaa}
%\documentclass[journal]{new-aiaa} for journal papers
\usepackage[utf8]{inputenc}

\usepackage{varioref}% smart page, figure, table, and equation referencing
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[version=4]{mhchem}
\usepackage{siunitx}
\usepackage{longtable,tabularx}
% style of minipage footnotes
\renewcommand{\thempfootnote}{\fnsymbol{mpfootnote}}
\setlength\LTleft{0pt} 
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\usepackage[nolist,printonlyused,nohyperlinks]{acronym}
\usepackage{todonotes}
\usepackage{booktabs}
\usepackage{comment}
\usepackage[format=hang]{caption}
\usepackage{subfigure}% subcaptions for subfigures
\usepackage{subfigmat}% matrices of similar subfigures, aka small mulitples
\usepackage{changepage} %margins for centered figures
\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{0pt}
\usepackage[ruled,vlined]{algorithm2e}

%\captionsetup[subfigure]{position=top, singlelinecheck=off,justification=raggedright}

\title{Multi-GPU Approach for Training of Graph ML Models on large CFD Meshes}

\fancypagestyle{firstpage}
{
	\fancyhead[L]{}    
	\fancyhead[C]{\textcolor{red}{\large\textbf{AIAA SciTech Forum, January 23-27 2023}\\ \normalsize Reprinted by permission of the American Institute of Aeronautics and Astronautics, Inc..\\
		Original work available at \url{https://arc.aiaa.org/doi/10.2514/6.2023-1203}}}    
	\fancyhead[R]{}
}

\author{Sebastian Strönisch\footnote{Research Assistant, Center for Information Services and High Performance Computing (ZIH), sebastian.stroenisch@tu-dresden.de}}
%\affil{ZIH, Technische Universität Dresden, 01062 Dresden, Germany}
\author{Maximilian Sander\footnote{Research Assistant, Center for Information Services and High Performance Computing (ZIH), maximilian.sander1@tu-dresden.de}}
\author{Andreas Knüpfer\footnote{Deputy Director, Center for Information Services and High Performance Computing (ZIH), andreas.knuepfer@tu-dresden.de}}
\affil{ZIH, Technische Universität Dresden, 01062 Dresden, Germany}
\author{Marcus Meyer\footnote{Specialist Aerothermal Methods, marcus.meyer@rolls-royce.com}}
\affil{Rolls-Royce Deutschland, 15827 Blankenfelde-Mahlow, Germany}

% single lines (widows and orphans)
\widowpenalty10000
\clubpenalty10000

\begin{document}

\thispagestyle{firstpage}
\vspace*{-25pt}
\maketitle

% https://www.aiaa.org/SciTech/presentations-papers/technical-presenter-resources

\begin{abstract}
Mesh-based numerical solvers are an important part in many design tool chains.
However, accurate simulations like computational fluid dynamics are time and resource consuming which is why surrogate models are employed to speed-up the solution process.
Machine Learning based surrogate models on the other hand are fast in predicting approximate solutions but often lack accuracy.
Thus, the development of the predictor in a predictor-corrector approach is the focus here, where the surrogate model predicts a flow field and the numerical solver corrects it.
%
This paper scales a state-of-the-art surrogate model from the domain of graph-based machine learning to industry-relevant mesh sizes of a numerical flow simulation. 
The approach partitions and distributes the flow domain to multiple GPUs and provides halo exchange between these partitions during training. 
The utilized graph neural network operates directly on the numerical mesh and is able to preserve complex geometries as well as all other properties of the mesh.
The proposed surrogate model is evaluated with an application on a three dimensional turbomachinery setup and compared to a traditionally trained distributed model. 
%
The results show that the traditional approach produces superior predictions and outperforms the proposed surrogate model. %\todo[color=red]{der neue Ansatz kann zeitliche variabilität leider genauso wenig}
Possible explanations, improvements and future directions are outlined.
\end{abstract}

%\section{Nomenclature}
\begin{acronym}[URANS] %longest acronym
 	\acro{amp}[AMP]{Automatic Mixed Precision}
	\acro{cfd}[CFD]{Computational Fluid Dynamics}
	\acro{ddp}[DDP]{distributed data parallel}	
	\acro{dnn}[DNN]{Deep Neural Network}
	\acrodefplural{dnn}[DNNs]{Deep Neural Networks}
	%
	\acro{gnn}[GNN]{Graph Neural Network}
	\acrodefplural{gnn}[GNNs]{Graph Neural Networks}
	\acro{gcn}[GCN]{graph convolutional network}
	\acro{gpu}[GPU]{Graphic Processing Unit}
	\acrodefplural{gpu}[GPUs]{Graphic Processing Units}
	%
	\acro{ml}[ML]{Machine Learning}
	\acro{mlp}[MLP]{multi layer perceptron}
	\acro{mgn}[MGN]{\textsc{{M}esh{G}raph{N}ets}}
	%
	\acro{dl}[DL]{Deep Learning}
	%
	\acro{nn}[NN]{neural network}
	\acro{cnn}[CNN]{convolutional neural network}
	\acro{rans}[RANS]{Reynolds-Average Navier-Stokes}
	\acro{les}[LES]{Large Eddy Simulation}
	%\acro{pyg}[PyG]{PyTorch Geometric} %% NO PyG in torchDDP
	\acro{2d}[2D]{two-dimensional}
	\acro{3d}[3D]{three-dimensional}
	\acro{aoa}[AoA]{angle of attack}
	\acro{knn}[$k$-NN]{$k$-nearest neighbor}
	\acro{lstm}[LSTM]{Long Short-Term Memory}
	\acro{mse}[MSE]{mean square error}
	\acro{pinn}[PINN]{physics-informed neural network}
	\acro{oom}[OOM]{out-of-memory}
	\acro{rmse}[RMSE]{root mean square error}
	\acro{urans}[URANS]{unsteady Reynolds-Average Navier-Stokes}
	%
	\acro{l}[$l$]{latent vector size}
	\acro{K}[$K$]{number of message passing steps}
	\acro{t}[$t$]{time step}
	
	\acrodefplural{t}[$t$]{time steps}
\end{acronym}
%printed nomenclature
%{\renewcommand\arraystretch{1.0}
%	\noindent\begin{longtable*}{@{}l @{\quad=\quad} l@{}}
%		$t$  & latent vector size \\
%		$N$ &    number of message passing steps \\
%		$t$& time step \\
%\end{longtable*}}
%\todo[inline,color=magenta]{Nomenclature \textbf{tbc}}

%% https://www.aiaa.org/publications/books/Publication-Policies#how-can-i-share-my-research-guidelines-for-authors

\input{./content/intro.tex}
\input{./content/model.tex}
\input{./content/methods.tex}
\input{./content/results.tex}

\section{Conclusions and Future Work}\label{ch:conclusion}
Recent implementations of \ac{ml} architectures to predict fluid flow achieve already outstanding results on academic examples. 
Building upon a state-of-the-art \ac{dnn} this work outlines and implements an approach for graph-based \ac{ml} models to predict fluid flow on industrially-relevant mesh sizes.
As halo exchanges between devices and adjacent message-passing steps is introduced, this approach is considered a logical extension of the single \ac{gpu} implementation of \ac{mgn}.
In order to validate the proposed approach a pre-study on a two-dimensional cylinderflow dataset from \cite{pfaff2021learning} is carried out and their results presented in section \ref{subsec:CYL}.
The proposed approach \textsc{MGN-Halo} performs slightly worse for single and multi-step rollouts than the single \ac{gpu} implementation.
Using gradient accumulation as a batching strategy for the single device implementation as well, the single step rollout results are comparable, which is why the pre-study was declared successful.
However, the multi step rollout results of \textsc{MGN-Halo} are still worse.

In the subsequent section \textsc{MGN-Halo} is applied to a dataset of a representative state-of-the-art turbine stator with $0.75\times10^6$ points in average.
A classic distributed learning strategy using \textsc{Horovod}~\cite{sergeev2018horovod} on partitioned samples without additional communication is used as a comparison and denoted \textsc{MGN-NoComm} in the following.
Both training approaches are compared after half of the number of optimizer steps used in~\cite{pfaff2021learning}. 
For single and multi-step rollouts as well as for 1-step rollouts starting from any timestep (\textit{Nextstep} prediction) \textsc{MGN-NoComm} is superior.
Visual inspections of 1-step rollouts of a validation sample confirmed the low prediction quality of \textsc{MGN-Halo} as areas of large error values as well as prediction artifacts are visible.
A possible explanation is that here, a classical halo exchange was performed, transferring only the node features.
But \ac{mgn} learns on node and edge features. An extension to exchange edge features additionally is therefore the mandatory next step to achieve sufficient results. 
%A more sophisticated and \ac{cfd} specific error measure is part of future improvements

Then, the capabilities of the superior \textsc{MGN-NoComm} model are examined and further prediction shortcomings were discovered.
The examined model configuration is not able to predict temporal variability that is present in the training. 
Instead, the model learned mean flow fields of the different geometries provided.
The relative error $e$ of the temporal deviation $\sigma_{\tau}$ is around $91\%$ for prediction on the training set. 
Prediction on the validation set are similarly stationary.
First evaluation of the amount of temporal deviation that was present in the dataset lead to the assumption that the model might suffer from vanishing gradients during gradient synchronization.
When the majority of the model replicas see quasi stationary flow, their gradients are close to zero and will dominate the gradient average that is computed.
Another possible explanation is that through the increased number of points in the domain the number of message passing steps is too low to exchange relevant flow information beyond the local neighborhood.
In \cite{pfaff2021learning} the correlation between lower message passing steps and accuracy deterioration is already presented.
Here, memory and resource constraints forced a lower message passing step number than in the original implementation which probably further lowered the accuracy of the proposed approach.
Recent advances in multiscale \ac{mgn} approaches \cite{Lino2021,Lino2022,Lino2022a,Fortunato2022} could resolve this issue as on coarse grids the number of message passing steps that is required for flow information to travel to distant parts of the domain is reduced.

Even though the proposed approach fell short of traditional distributed training, valuable experience with the training and inferencing of \acp{gnn} on large \ac{cfd} meshes was gained.
To the best of the authors knowledge, this is the first attempt of applying \ac{mgn} to train on a domain of about $10^6$ points.
Even though the proposed method turns out worse than the traditional training approach with \textsc{Horovod}, the experience of how to train on graph domains of this size is essential for future applications to even larger domains.
Furthermore, multiple mandatory improvements for the detected shortcomings of the models presented here were outlined and presumably lead to the proposed method being a useful extension to \ac{mgn} for the training on large \ac{cfd} domains after all. 
Future work will besides the evaluation of the improved halo exchange method include hybrid approaches using multiscale \ac{mgn} approaches, physics-based loss functions and transfer learning for sustainable usage of the expensively trained model.

Not limited to fluid flow, \ac{gnn} architectures can also be adapted to support other mesh-based numerical methods e.g. FEM.
This opens new possibilities for the integration of \ac{ml} models into design pipelines or into numerical solvers directly in order to speed-up convergence and decrease the required computational resources overall.

\newpage
\section*{Appendix}
\input{./content/appendix.tex}

\newpage
\section*{Acknowledgments}
The work presented in this paper was conducted within the framework of the DARWIN research project (20D1911C), funded by Rolls-Royce Deutschland Ltd \& Co KG and the Bundesministerium für Wirtschaft und Klimaschutz. 
Rolls-Royce Deutschland’s permission to publish this work is greatly acknowledged.
The authors gratefully acknowledge the GWK's support for funding this project by providing computing time through the Center for Information Services and HPC (ZIH) at TU Dresden.
\bibliography{literature}

\end{document}

