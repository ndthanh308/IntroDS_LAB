
\section{Introduction}
The use of \ac{cfd} in many scientific disciplines for parameter studies, design optimization studies and uncertainty quantification is increasing significantly over the recent years. Since hundreds or even thousands of CFD calculations are needed for those types of studies even small improvements in solver convergence time can have a serious impact on the overall time of the design study, its computational resources and its costs. 
Data-driven approaches like \ac{ml} may help to address this issue: 
The main objective of this paper is a contribution towards the acceleration of the convergence process of \ac{cfd} simulations of industrially relevant size by applying a \ac{gnn} \cite{kipf2017semi} architecture to generate a tailored ML-based initial flow field prediction.

In recent years, graph-based \ac{ml} algorithms that preserve even complex geometries like airfoils were successfully applied for fluid flow field prediction \cite{Aulich2019,Belbute-Peres2020,thuerey2020deepFlowPred,Kashefi2021,pfaff2021learning,stroenischPASC22}. 
However, graph-based \ac{ml} approaches face the challenge of high \ac{gpu} memory consumption which limits the numerical mesh size of the CFD setup for which they can be utilized.
Therefore, scaling approaches like the one presented here that use multiple nodes with multiple \acp{gpu} need to be studied to enable the usage of graph-based \ac{ml} for relevant mesh sizes.
%
Recent advances in large graph \ac{ml} techniques mainly focus on graph embeddings \cite{hamilton2017inductive,pbg}.
To the best of the authors knowledge, there exists no multi-GPU training approach for regression problems like flow field prediction on large graphs using message-passing based model architectures.

The key contribution of this paper is the implementation of a graph-based multi-GPU learning approach and its application for the training and prediction of three-dimensional flow fields of CFD setups with up to one million cells while preserving the full numerical mesh. 
Furthermore, a comparison to a traditional distributed training approach reveals several limitations that need further investigation in the future.

Each flow domain is represented as a graph where the grid vertices are viewed as points and their connections as edges. 
This way graph convolution~\cite{kipf2017semi} can be utilized directly on the numerical mesh.
Flow values as well as coordinates are stored on each point as node features. 
The model can operate on graphs of arbitrary size but is constrained by the hardware that it is trained on \cite{stroenischPASC22}.
Here, up to 8 Nvidia A100~\cite{choquette_nvidia_2021} are used to train the model, whose main principles are outlined in chapter \ref{ch:model}.
The distributed learning approach is applied on the state-of-the-art \ac{mgn} from \cite{pfaff2021learning}.
Details on the proposed approach are given in the subsequent chapter \ref{ch:method}.

There exist two datasets on which the distributed learning approach is tested.
%The Navier-Stokes equations govern fluid flow and there exist a multitude of numerical methods to solve them for different spatial or temporal resolution of the flow, see \cite{Versteeg, Wendt} for further details.
The first dataset consists of the cylinderflow dataset from \cite{pfaff2021learning} and is used for validation of the approach. 
The results are outlined in section \ref{subsec:CYL}.
The second dataset is created from \ac{urans} simulations of geometry variations of a representative state-of-the-art turbine stator (see \cite{SchwarzeURANS2006} for more details on URANS).
The performance of the distributed \ac{mgn} will be evaluated according to its generalization capabilities on unseen geometry.
Results and discussions are given in section \ref{subsec:H01S}.

Finally, section \ref{subsec:perf} analyzes the performance aspect of the proposed approach and presents the communication overhead for multi-node training before deficiencies and future directions are discussed in section \ref{ch:conclusion}.
