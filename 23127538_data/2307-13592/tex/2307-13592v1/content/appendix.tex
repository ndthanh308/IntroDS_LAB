% Figure environment removed

\begin{algorithm}
	\caption{Training of distributed MeshGraphNets}
	\label{alg:distr_meshgn_training}
	\textbf{Input:} Model $\textit{f}_{\boldsymbol{\theta}} =$ $\{\textit{EncNode}_{\boldsymbol{\theta}}$,
	 $\textit{EncEdge}_{\boldsymbol{\theta}}$,
	 $\textit{GraphNetBlock}_{\boldsymbol{\theta}}$,
	 $\textit{DecNode}_{\boldsymbol{\theta}}\}$,
	 time series data on the nodes $\{\boldsymbol{X},\boldsymbol{Q}\} = \{\boldsymbol{x}^n,\Delta\boldsymbol{q}^{n}\}_{n=1}^{N}$,
	 edges and edge features describing the geometry $ \{\boldsymbol{\mathcal{E}}^n,\mathbf{e}^n\}_{n=1}^{N}$,
	 blocks and neighbors describing the mesh partitions $ \{\boldsymbol{b}^n,\boldsymbol{nb}^n\}_{n=1}^{N}$ and
 	 loss mask excluding boundary conditions  $\{\boldsymbol{\textit{mask}}^n_\textit{loss}\}_{n=1}^{N}$ of length $N$;
 	 Number of epochs: $M$;
 	 Gradient accumulation step: \textit{acc};
 	 Learning rate: $\eta$;
 	 Number of Processes:~\textit{CS};
     Own rank: \textit{pid}; \\

	\For{\text{epoch} $= 1$ \KwTo $M$}
	{
        $\nabla \boldsymbol{\theta} \gets 0$\;
		\For{$n = 1$ \KwTo $N$}
		{	
			$\Delta \boldsymbol{q}^n_{\textit{norm}},\boldsymbol{x}^n_{norm},\mathbf{e}^n_{norm} \gets \text{Normalize}(\Delta \boldsymbol{q}^n,\boldsymbol{x}^n,\mathbf{e}^n)$\;
			$\boldsymbol{x}^n_{enc} \gets \textit{EncNode}_{\boldsymbol{\theta}}(\boldsymbol{x}^n_{norm})$\;
			$\mathbf{e}^n_{enc} \gets \textit{EncEdge}_{\boldsymbol{\theta}}(\mathbf{e}^n_{norm})$\;
			$\textit{buff}_\textit{send},\textit{buff}_\textit{recv} \gets \textit{CS}\times[]$\;
			$\textit{mask}_\textit{send}, \textit{mask}_\textit{recv} \gets \textit{CS}\times[]$\;
			
			\For{$\textit{com} \gets 1$ \KwTo $\textit{CS}$}
			{
				\If{$\textit{com} \in \boldsymbol{nb}^n$}
				{
					$\textit{mask}_\textit{send}$[com], $\textit{mask}_\textit{recv}$[com] $\gets$ create\_masks($\boldsymbol{b}^n$, $\boldsymbol{\mathcal{E}}^n$, \textit{pid}, \textit{com}) \;
				}
			}
			\For{$k \gets 1$ \KwTo $K$}
			{
			    $\boldsymbol{x}^{n,k+1}_{enc}, \mathbf{e}^{k+1}_{enc} \gets \textit{GraphNetBlock}_{\boldsymbol{\theta}}(\boldsymbol{x}^{n,k}_{enc}, \mathbf{e}^{k}_{enc}, \boldsymbol{\mathcal{E}}^n)$\;
				\For{$\textit{com} \in \boldsymbol{nb}^n$}
				{
					$\textit{buff}_\textit{send}[\textit{com}] \gets \boldsymbol{x}^{n,k+1}_{enc}[\textit{mask}_\textit{send}[\textit{com}]]$\;
				}
				$\textit{buff}_\textit{recv} \gets \text{all\_to\_all}(\textit{buff}_\textit{send})$\;
				\For{$\textit{com} \in \boldsymbol{nb}^n$}
				{
					$\boldsymbol{x}^{n,k+1}_{enc}[\textit{mask}_\textit{recv}[\textit{com}]] \gets \textit{buff}_\textit{recv}[\textit{com}]$\;
				}
			}
			
			$\mathbf{p}^n \gets \textit{DecNode}_{\boldsymbol{\theta}}(\boldsymbol{x}^{n,k+1}_{enc})$\;
			$\mathcal{L}_{local} \gets SSE(\mathbf{p}^n,\Delta \boldsymbol{q}^n_{norm})$\;

			$\mathcal{L} \gets \textit{loss\_allreduce}(\mathcal{L}_{local},\textit{mask}^{n}_{\textit{loss}})$\;
			$\nabla \boldsymbol{\theta} \gets \nabla \boldsymbol{\theta} + \text{Backprop}(\mathcal{L})$\;
			\If{$Mod(n, \mathrm{\textit{acc}})=0$}
			{
				$\boldsymbol{\theta} \gets \boldsymbol{\theta} - \eta \nabla \boldsymbol{\theta}$\;
				$\nabla \boldsymbol{\theta} \gets 0$\;
			}
		}
	}
\textbf{Output:} Trained model $\textit{f}_{\boldsymbol{\theta}} =$ $\{\textit{EncNode}_{\boldsymbol{\theta}}$,
$\textit{EncEdge}_{\boldsymbol{\theta}}$,
$\textit{GraphNetBlock}_{\boldsymbol{\theta}}$,
$\textit{DecNode}_{\boldsymbol{\theta}}\}$
\end{algorithm}

