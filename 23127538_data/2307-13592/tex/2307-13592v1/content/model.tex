\section{Model}\label{ch:model}
The \ac{gnn} used in this work is trained to model the temporal progression of a flow field around an object.
Both datasets at hand consist of temporal trajectories of different geometries that are simulated with constant boundary conditions for a fixed number of \acp{t}.
The simulation mesh at time $t \in \mathcal{T}$ is denoted $\mathcal{M}^\text{t} = (\mathcal{N},\mathcal{E})$ with a set of nodes $\mathcal{N}$ that are connected by a set of edges $\mathcal{E}$.
Each node $i \in \mathcal{N}$ has an associated set of coordinates $\mathbf{x}_\text{i}$ and dynamical quantities $\mathbf{q}_\text{i,t}$ that are to be modeled.
Summarized, the learning task is to model the evolution of time-dependent dynamical quantities over a fixed mesh $\mathcal{M} = \mathcal{M}^\text{t}, t\in \mathcal{T}$ and sample $\mathbf{q}_\text{i,t}$ at the mesh nodes. 

The \ac{mgn} model that is used here was developed by \cite{pfaff2021learning} and can be utilized for all kinds of graph-based prediction tasks.
Its core architecture consists of an encoder, a series of processing blocks and a decoder. 
The simulation mesh and the node features of a time step $t$ are encoded into a graph $\mathcal{G}_\text{t} = (\mathcal{N}_\text{t},\mathcal{E})$. 
Analogous to \cite{pfaff2021learning} positional features $\mathbf{x}_\text{i}$ are provided as relative edge features to achieve spatial equivariance. 
Edge features $\mathtt{e}_\text{ij} \in \mathcal{E}$ of an edge between nodes $i$ and $j$ hold the relative displacement vector $\mathbf{x}_\text{ij} = \mathbf{x}_\text{i} - \mathbf{x}_\text{j}$ as well as its norm $|\mathbf{x}_\text{ij}|$.
The input node features $\mathbf{f}_\text{i,t} \in \mathcal{N}_\text{t}$ are concatenated from dynamical quantities $\mathbf{q}_\text{i,t}$ and a one-hot vector $\mathbf{n}_\text{i}$ indicating the node type, such as e.g. inflow, outflow or wall.
Node features $\mathbf{f}_\text{i,t}$ and Edge Features $\mathtt{e}_\text{ij}$ are encoded separately into a $l$-sized latent vector by the encoder block. \acused{l}
The encoded node and edge features are then passed to the processor which consists of $K$ processing blocks. 
A processing block is also called message passing block as features are exchanged between neighboring nodes.
Each block is modeled as an independent \ac{mlp} which is applied to the output of the previous block. 
After all \ac{K} are executed, a decoder transforms the node features in latent space to the desired output features $\mathbf{p}_\text{i}$.

A message-passing-like scheme called neighborhood aggregation is used in the processor block so that each node gathers and scatters node features from itself to its connected neighbors in the graph.
This can be viewed as a generalization of the convolutional operator to irregular domains.
For \ac{cfd} data this scheme is executed multiple times so that information flow across a large part of the domain is ensured.
In \cite{pfaff2021learning} prediction capabilities are demonstrated on simulation data from a flag, a deforming plate and two-dimensional flows around objects.
The \ac{cfd} datasets used by those authors consisted of samples with mesh sizes of around five thousand cells which were easily trained on a single \ac{gpu}. 

The learning task regarding CFD that is addressed with this architecture can be described by
\begin{align}
	f\left( \mathcal{E}, \mathcal{N}_t \right) \rightarrow \Delta \mathbf{q}_\text{t} 
\end{align}
so that the predicted flow field of the next time step $\mathbf{\tilde{q}}_\text{t+1} = f\left( \mathcal{E}, \mathcal{N}_t \right) + \mathbf{q}_\text{t} = \mathbf{p}_\text{i,t} + \mathbf{q}_\text{t}$.
The mesh is constant for a given trajectory so that $\mathcal{G}_\text{t+1} = (\mathcal{N}_\text{t+1},\mathcal{E})$. 

In this work, a custom PyTorch~\cite{pytorch2019} \ac{mgn} version is deployed on three-dimensional samples with roughly one million cells using the distribution approach presented in section \ref{ch:method}.

