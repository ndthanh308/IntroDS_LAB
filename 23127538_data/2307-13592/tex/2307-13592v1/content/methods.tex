\section{Methodology} \label{ch:method}

Two approaches are considered and compared in the present paper for application of a \ac{dnn} to industry-relevant mesh sizes. 
The first and trivial one is training a neural network (NN) on a small part of the flow domain and combining predictions of all parts of the domain for the full flow field. 
The downside of this approach is the constraint information flow which is disrupted between the different parts. For evaluating a complete flow field, multiple predictions have to be stitched together. 
On the other side, already established scaling approaches like the one by Horovod \cite{sergeev2018horovod} can be used to increase the number of samples per training batch through utilizing multiple \ac{gpu}s. 

The second approach which is developed here combines classic high performance computing strategies with the machine learning approach at hand. 
The flow domain is partitioned using a current version of Metis \cite{metis97} and then distributed on multiple GPUs that train separate models in parallel. 
The parameters of \ac{ml} optimizer and model are kept separate but use synchronized gradients utilizing the \ac{ddp} framework of PyTorch \cite{pytorch2019,DDPDesignNote}.
This way the optimizers are expected to be synchronized as well.
However, this approach keeps separate copies of model and optimizer parameter on each \ac{gpu} which increases the per-GPU memory usage significantly \cite{deepspeedZERO20}.

% Figure environment removed
In order to ensure information flow across the whole domain, each partition additionally shares all nodes that are directly connected to other partitions. 
These cells are typically called halo cells \cite{Halos}.
In this work the halo cell layer is of size one.
For demonstration purposes a partitioned sample from the cylinder dataset from \cite{pfaff2021learning} is shown in figure~\vref{f:partitioned}.
Furthermore the halo layer for one arbitrary partition (here bright green) is marked by red dots in figure~\ref{f:halosGreen}.

Figure \ref{fig:MGN_multiGPU} visualizes one optimizer step with a single backward step including the communication between all parallel model replicas.
Distributed model training already uses communication for gradient synchronization over all parameters with an asynchronous \textit{allreduce} after the loss backward-pass.
In addition, a global mean squared error loss calculation is used as well as a forward halo exchange and a backward gradient exchange between halos is introduced.
The additional communication is colored red in Fig. \ref{fig:MGN_multiGPU}.
Information exchange between halos happens between adjacent message passing steps.
% Figure environment removed

This counter-intuitive approach forces synchronization between GPUs for each sample and each message passing step. 
The time that each device idles before all \acp{gpu} resume is called imbalance and section \ref{subsec:perf} visualizes it's impact on overall runtime.
Against expectations the communication overhead for a distributed two-dimensional sample on eight Nvidia A100 was only around 18\% compared to the single GPU application of the same dataset. 
This comparison is only possible in small cases, where a sample can fit in a single \ac{gpu} memory. 
The assumption is that the latency of the communication dominates for small samples, but for large graphs the computational part should predominate significantly.


For a detailed performance analysis Score-P~\cite{Knuepfer2012} in conjunction with Vampir~\cite{vampir} is used. 
Score-P is a tool used to record event traces and originates in the domain of High Performance Computing. 
It supports but is not limited to C/C++, Fortran, CUDA and MPI.
Recently, a detailed analysis of Python code with the focus on low overhead was made available~\cite{Gocht2021}.
