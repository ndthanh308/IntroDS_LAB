
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proof}{Proof}
\newtheorem{observation}{Observation}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\Nitem}{M}
%

\newcommand{\nitem}{m}
\newcommand{\Nround}{T}
\newcommand{\nround}{t}

\newcommand{\rigel}[1]{{\color{magenta}[\textsc{Rigel}: \emph{#1}]}}
\newcommand{\negin}[1]{{\color{red}[\textsc{NG}: \emph{#1}]}}
%\newcommand{\rigel}[1]{{[#1]}}
%\newcommand{\negin}[1]{{[#1]}}

%\usepackage{natbib}
% \bibpunct[, ]{(}{)}{,}{a}{}{,}%
% \def\bibfont{\small}%
 %\def\bibsep{\smallskipamount}%
 %\def\bibhang{10pt}%
 %\def\newblock{\ }%
 %\def\BIBand{and}%

%\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}  
%\documentclass[format=acmsmall, review=false]{acmart}
\documentclass{article}
%\usepackage[toc,page]{appendix}
\usepackage{hyperref}
\usepackage{url}
 \usepackage{natbib}
 \bibpunct[, ]{(}{)}{,}{a}{}{,}%
 \def\bibfont{\scriptsize}%
 \def\bibsep{\smallskipamount}%
 \def\bibhang{24pt}%
 \def\newblock{\ }
%\documentclass[12pt]{report}
\usepackage{bm}
%\usepackage{setspace}
%\renewcommand{\baselinestretch}{1.5} 
\usepackage{zref-totpages}
\usepackage{wrapfig} % This line includes the wrapfig package

\usepackage{pgfplots}
\usepgfplotslibrary{fillbetween}

\usepackage{booktabs} % For better table rules
\usepackage{booktabs} % For formal tables
\usepackage[ruled]{algorithm2e} % For algorithms
%\usepackage{graphicx}
%\usepackage{caption}
\usepackage{subcaption}
\renewcommand{\algorithmcfname}{ALGORITHM}
\usepackage[margin=1in]{geometry}
%\SetAlFnt{\small}
%\SetAlCapFnt{\small}
%\SetAlCapNameFnt{\small}
%\SetAlCapHSkip{0pt}
%\IncMargin{-\parindent}
%\pagenumbering{gobble}
% Choose a citation style by commenting/uncommenting the appropriate line:
%\setcitestyle{acmnumeric}
%\setcitestyle{authoryear}
%\OneAndAHalfSpacedXI
%\SingleSpacedXI
\usepackage{setspace}
\onehalfspacing

\RequirePackage{amssymb,amsmath,ifthen,url,graphicx,color,array}
%\TheoremsNumberedThrough     % Preferred (Theorem 1,
%\ECRepeatTheorems

\begin{document}
% Title. Note the optional short title for running heads. In the interest of anonymization, please do not include any acknowledgements.

% Anonymized submission.
%\author{Submission XYZ}



\title{Learning in Repeated Multi-Unit Pay-As-Bid Auctions}
\author{Rigel Galgana$^1$ \and Negin Golrezaei$^2$}
\date{%
    $^1$MIT Operations Research Center\\%
    $^2$MIT Sloan School of Management\\[2ex]%
}

\maketitle


\begin{abstract}
    Motivated by Carbon Emissions Trading Schemes, Treasury Auctions, Procurement Auctions, and Wholesale Electricity Markets, which all involve the auctioning of homogeneous multiple units, we consider the problem of learning how to bid in repeated multi-unit pay-as-bid auctions. In each of these auctions, a large number of (identical) items are to be allocated to the largest submitted bids, where the price of each  of the winning bids is equal to the bid itself. In this work, we study the problem of optimizing bidding strategies from the perspective of a single bidder.

    Effective bidding in PAB auctions is complex due to the combinatorial nature of the action space. We address this by concentrating on the offline setting, where bidders optimize bids based on past participant bids. We show that the optimal solution to  the offline problem can be obtained using a polynomial time dynamic programming (DP) scheme under which the bidder's utility is decoupled across units. We leverage the structure of the DP scheme to design online learning algorithms with polynomial time and space complexity under full information and bandit feedback settings. Under these two feedback structures, we achieve an upper bound on regret of $O(\Nitem \sqrt{\Nround \log \Nround})$ and $O(\Nitem \Nround^{\frac{2}{3}} \sqrt{\log \Nround})$ respectively, where $\Nitem$ is the number of units demanded by the bidder and  $\Nround$ is the total number of auctions. {\color{black}We accompany these results with a regret lower bound of $\Omega(M\sqrt{T})$ for the full information setting and $\Omega (M^{2/3}T^{2/3})$ for the bandit setting. We also present additional findings on the characterization of equilibria within the PAB auction format.} 
    
    Numerical results suggest that with our no-regret learning algorithms, market dynamics converge to a welfare-maximizing equilibrium with uniform bids when all agents adopt them. Increased competition reduces strategization's impact, leading to faster convergence toward higher revenue and welfare levels. Experiments consistently show that PAB outperforms the uniform price auction in generating significantly higher revenue, making it an appealing choice where revenue maximization, such as in ETS, holds significant social value.
    \noindent
    
    \textbf{Keywords.} 
    Multi-unit pay-as-bid auctions, Bidding strategies, Regret analysis, Market dynamics.
\end{abstract}




% Title page for title and abstract only.
%\begin{titlepage}

%\maketitle

%\end{titlepage}

% Paper body
\input{introduction.tex}
\input{problem_statement3.tex}
\input{online_reps4.tex}
\input{experiments.tex}
\input{timevarying}
\input{conclusions_2.tex}
\bibliographystyle{ACM-Reference-Format}
\footnotesize{
\bibliography{ref.bib}
}
\newpage
% \begin{APPENDICES}
\input{appendix.tex}
% \section{Switching times}
% \input{response_letter}
% \section{Supplementary materials}
% \end{APPENDICES}
\end{document}
