\section{Online Learning Algorithms: Mirror Descent} 
\label{sec:bandit}


In this section, we propose our second online learning algorithm. Instead of mimicking the exponential weights algorithm, we reformulate the problem as online linear optimization over node probabilities in our DP graph. We solve this using OMD and construct a policy that sequentially samples bids based on these probabilities. We first present the regret analysis for the bandit setting, followed by the full information setting. We provide a single algorithm for both settings, with only one line changing based on the feedback structure. Our OMD algorithm is regret optimal in both feedback structures (up to a factor of $\sqrt{|\mathcal{B}|\log|\mathcal{B}|}$). However, it requires solving a convex optimization problem at each iteration, which may slow it down in practice. Nonetheless, this algorithm is preferred when prioritizing regret optimality over computational complexity.
\subsection{Algorithm Statement}



Recall that in the DP graph, we have $\Nitem$ layers, where in each layer there are $|\mathcal B|$ nodes and $|\mathcal B|^2$ edges. Given the structure of the DP graph, one idea is to  maintain some policy $\bm{\pi}: [\Nitem] \times \mathcal{B} \times \mathcal{B} \to [0, 1]$ which induces a family of probability measures $\bm{\rho}: [\Nitem] \times \mathcal{B} \times \mathcal{B} \to [0, 1]$ over the edges in the DP graph. In particular, let $\pi((\nitem, b), b') = \prob(b_{\nitem+1} = b' \mid b_\nitem = b)$ be the probability that the agent selects bid $b'$ for slot $\nitem+1$ conditional on having already selected bid $b$ for slot $\nitem$. Further, define $\rho((\nitem, b), b') = \prob(b_\nitem = b, b_{\nitem+1} = b')$ as  the unconditional probability that agent selects bids $b$ and $b'$ for slots $\nitem$ and $\nitem+1$, respectively. Following this idea, one can transform the bid optimization problem as an equivalent online linear optimization (OLO) problem over the space of possible $\bm{\rho}$, which we will show in the following section. However, this approach would lead to an algorithm with sub-optimal regret of $O(\Nitem |\mathcal{B}| \sqrt{\Nround \log |\mathcal{B}|})$ as it fails to capture the additional structure within the DP graph; cf. \citep{CMAB2013, PathKernel2003, OREPS2013}.
We show later that it is possible to improve this regret by a factor of $\sqrt{|\mathcal{B}|}$.

In this section, as our main contribution, instead of maintaining probability measures $\bm{\rho}$ over the edges in the DP graph, we maintain probability measures $\bm{q}$ over nodes. This idea is based on an important observation that, in the DP formulation, the weight of a path depends only on the nodes traversed and not the edges. In other words, regardless of the value of $b_\nitem$, selecting the edge from $(\nitem, b_\nitem)$ to $(\nitem+1, b_{\nitem+1})$ at round $t$ always yields the same utility $w_{\nitem+1}^\nround(b_{\nitem+1})$. We then construct some policy $\bm{\pi}$ that generates the desired node probability measures $\bm{q}$, where there may be many such choices of $\bm{\pi}$, though we argue the specific choice will not affect the regret. 
Consequently, we can reduce the higher dimensional problem of regret minimization over policies to the simpler one of regret minimization over node measures.



\textbf{Algorithm Summary.} Our algorithm (Algorithm \ref{alg: OMD}) consists of four steps. First, we recursively sample $\bm{b}^\nround$ according to the policy $\bm{\pi}^\nround$. Second, we compute node utility estimates $\widehat{w}_m^t(b)$, either as the true loss $w_m^t(b)$ in the full information setting or the inverse probability weighted version $\frac{w_m^t(b)}{q_m^{t-1}(b)}\textbf{1}_{b = b_{m}^t}$ in the bandit setting. Third, we optimize the negentropy-regularized expected estimated utility with respect to the probability measure over states $\bm{q}^\nround$, using OMD or Follow-the-Negentropy-Regularized-Leader updates. We show how this update can be efficiently computed by projecting the unconstrained optimizer of the regularized utility to the feasible space of $\bm{q}$, denoted by $\mathcal Q$, where the set $\mathcal Q$
    \begin{align}        \mathcal{Q} = \Big\{\bm{q} \in [0, 1]^{M \times |\mathcal{B}|}:  \sum_{b \in \mathcal{B}} q_m(b) = 1, \sum_{b \leq b'} q_{m+1}(b) \geq \sum_{b \leq b'} q_m(b) \forall b, b' \in \mathcal{B}, m \in [M]\Big\}\,. \label{eq:Q}
    \end{align}
consists of probability distributions over bids satisfying certain stochastic dominance constraints {which reflect the bid monotonicity constraints}. That is, under $\mathcal Q$, $\bm{q}_{\nitem+1}$ stochastically dominates $\bm{q}_\nitem$ for all $\nitem \in [\Nitem-1]$. 
Fourth, we convert $\bm{q}^\nround$ to a corresponding policy representation $\bm{\pi}^\nround$, ensuring that a feasible solution $\bm{\pi}^\nround$ exists as long as $\bm{q}^\nround \in \mathcal{Q}$.  
    

    
    We next discuss the main ideas and provide insights regarding our algorithm design.

 \begin{algorithm}[t]
 \footnotesize
	\KwIn{Learning rate $\eta > 0$, Valuation $\bm{v} \in [0, 1]^{+\Nitem}$ } 
	\KwOut{The aggregate utility $\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b}^\nround)$.}
	$\pi_0((m, b), b') \gets \frac{1}{|\{b" \in \mathcal{B}: b" \leq b, b" \leq v_{m+1}\}|}$ for all $\nitem \in [\Nitem], b \geq b' \in \mathcal{B}, b' \leq v_{m+1}$. Let $\bm{q}^0 \in [\Nitem] \times \mathcal{B} \to [0, 1]$ be the corresponding unit-bid value pair occupancy measure\;
	\For{$\nround \in [\Nround]$:}{
            \textbf{Determining the Bid Vector $\bm {b}^t$ recursively.} Set $b_1$ to $b \in \mathcal{B}$ with probability  $q^t_1(b)$\;
            \textbf{for} $m \in [1,\ldots,M-1], b \in \mathcal{B}: b_{m+1} \gets b$ with probability $\pi^t((m, b_\nitem), b)$\;
            Receive reward $\mu^\nround_n(\bm{b}^\nround) = \sum_{\nitem=1}^\Nitem w_\nitem^\nround(b^\nround_\nitem)$ and observe $w_\nitem^\nround(b^\nround_\nitem)$ where $w_\nitem^\nround(b) = (v_\nitem - b)\textbf{1}_{b \geq b^\nround_{-\nitem}}$\;
            \textbf{Update Reward Estimates}\;
            \textbf{for} $\nitem \in [\Nitem], b \in \mathcal{B}: \widehat{w}_\nitem^\nround(b) \gets \frac{w_\nitem^\nround(b)}{q^{\nround-1}_\nitem(b)} \textbf{1}_{b = b^{\nround}_\nitem}$ if \emph{Bandit Feedback}, $\widehat{w}_\nitem^\nround(b) \gets w_\nitem^\nround(b)$ if \emph{Full Information}\;
            \textbf{Determining Probability Measure $\bm{q}^t$ over any unit-bid value pair $(m, b)$} Set
            \[\bm{q}^\nround \gets \text{argmin}_{\bm{q} \in \mathcal{Q}} \eta\langle \bm{q}, -\widehat{\bm{w}}^\nround\rangle + D(\bm{q} || \bm{q}^{\nround-1})\,,\] where $\mathcal{Q}$ is as in Equation ~\eqref{eq:Q} and $D(\bm{q} || \bm{q}') = \sum_{\nitem \in [\Nitem], b \in \mathcal{B}} q_\nitem(b)\log \frac{q_\nitem(b)}{q'_\nitem(b)} - (q_\nitem(b) - q'_\nitem(b))$.
            
            \textbf{Convert $\bm{q}^t$ to Policy $\bm{\pi}^t$}\;
            Compute any feasible solution $\bm{\pi}^t$ to constraints $q^t_m(b) = \sum_{b' \geq b} q^t_m(b') \pi^t((m-1, b'), b)$ and $\sum_{b" \leq b} \pi^t((m, b), b") = 1$ for all $m \in [M], b \in \mathcal{B}$.
        }
        \textbf{Return $\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b}^\nround)$.} 
	\caption{\textsc{OMD - Bid Optimization in Multi-Unit Pay as Bid Auctions}}
	\label{alg: OMD}
\end{algorithm}

\textbf{Main Idea: Using the DP formulation to reduce our problem to online linear optimization.}  To design our algorithm, we observe that our DP formulation  allows us to reduce the bidding problem to OLO over the space of possible node probability measures $\bm{q}$. Of course, we must justify why it is reasonable for our optimization procedure to only consider node probability measures instead of the larger space of possible policies $\bm{\pi}$. Recall that the reward at round $\nround$ for bidding $\bm{b}$ is given by the sum of utilities of unit-bid values $\mu_n^\nround(\bm{b}) = \sum_{\nitem=1}^\Nitem w_\nitem^\nround(b_\nitem)$. We then take expectations over the bid vector $\bm{b}$, sampled from the following policy $\bm{\pi}$ which induces probability measures $\bm{q}_1,\ldots,\bm{q}_{\Nitem}$ over bid values; i.e., $q_m(b) = \prob_{\bm{b} \sim \bm{\pi}}(b_m = b)$: 
\begin{align}
    \mathbb{E}_{\bm{b} \sim \bm{\pi}}\left[ \mu^{\nround}_n(\bm{b}) \right] = \sum_{\nitem=1}^\Nitem \mathbb{E}_{\bm{b} \sim \bm{\pi}}\left[ w_\nitem^\nround(b_\nitem) \right] = \sum_{\nitem=1}^\Nitem \mathbb{E}_{b_\nitem \sim \bm{q}_\nitem}\left[ w_\nitem^\nround(b_\nitem) \right] = \sum_{\nitem=1}^\Nitem  \sum_{b \in \mathcal{B}} q_\nitem(b) w_\nitem^\nround(b) = \langle \bm{q}, \bm{w}^\nround \rangle\,.
    \label{eq: Loss of policy}
\end{align}
Here, the last term is an inner product over the space $[\Nitem] \times \mathcal{B}$, and in the first equation, we invoke the linearity of bid vector utilities on its unit-bid value utilities. The second equality is justified because we are taking an expectation over possible bid vectors $\bm{b}$, as the $\bm{q}_m$'s are by definition the  probabilities of selecting bid $b$ and unit $m$. This addresses the question of why we concern ourselves only with the node probability measures $\bm{q}$ when optimizing, as the regret depends only on $\bm{q}$, rather than the associated policy. In other words, for a fixed $\bm{q}$, any policy $\bm{\pi}$ that induces node probability measures $\bm{q}$ will yield the same expected utility. Intuitively, this reflects the fact that the utilities are associated with nodes and not edges in our DP graph. 

Letting $\bm{q}^\nround$ denote the probability measures induced by the (condensed) policy at round $\nround$, the instantaneous utility at round $\nround$ is given by $\langle \bm{q}, \bm{w}^\nround \rangle$. Seeing this inner product begs use of OLO algorithms. However, most OLO algorithms require convexity of the action space which is, in our setting, the space of possible $\bm{q}$. To show that this space is convex, we invoke the following lemma.

\begin{lemma}[$\mathcal{Q}$-Space Equivalence]
    \label{lem: QSpace Equivalence}
    Let \[\Pi = \Big\{{\pi} \in [0,1]^{M\times |\mathcal B|\times |\mathcal B|}: \pi((m, b), b') = 0 ~~\forall b' > b, m\in [M], \sum_{b' \leq b} \pi((m, b), b') = 1, m\in [M]\Big\}\] denote the space of policies on our DP graph. With a slight abuse of notation, for any $\pi\in  \Pi$, define 
    \[q(\pi) = \{\mathbf{q} \in [0, 1]^{M\times |\mathcal B|}: \forall b \in \mathcal{B},  q_1(b) =\pi((0, b_0), b), q_{m+1}(b) = \sum_{b' \in \mathcal B} q_m(b')\pi((m, b'), b), m\in[M-1]\}\,\] as the node probabilities induced by $\pi$. Here, $b_0= \max \mathcal B$. Let $\mathcal{Q}_{\Pi} = \cup_{\pi \in \Pi} q(\pi)$. Then,   $\mathcal{Q}_{\Pi}$ is equivalent to the set $\mathcal{Q}$ where $\mathcal{Q}$ is defined in Equation \eqref{eq:Q}. 
\end{lemma}

At a high level, the proof requires constructing a bijection between elements of $\mathcal{Q}_\Pi$ and $\mathcal{Q}$. Showing that $\bm{q} \in \mathcal{Q}_\Pi$ implies $\bm{q} \in \mathcal{Q}$ follows straightforwardly by applying the linear transform $q$ to the $\bm{\pi}$ associated with $\bm{q}$. The reverse direction requires a careful construction of a sequence of nested, non-empty subsets of $\Pi$ that satisfy the $q_{m+1}(b) = \sum_{b' \in \mathcal B} q_m(b')\pi((m, b'), b)$ constraints. 


Lemma \ref{lem: QSpace Equivalence} establishes that during the execution of Algorithm \ref{alg: OMD}, we can focus on the node probabilities in set $\mathcal{Q}$ without loss of generality. We recall that within $\mathcal{Q}$, the stochastic dominance conditions are enforced solely over node probabilities across layers. In other words, when determining $\mathbf{q}^t$ in Algorithm \ref{alg: OMD}, it is sufficient to consider the feasible set restricted to $\mathcal{Q}$, which is a convex set as  $\mathcal{Q}$ is a polyhedron.


Now, we argue that we only need to consider optimizing over $\mathcal{Q}$ as opposed to $\Pi$, as the regret can be rewritten strictly in terms of $\bm{q}$, independently of the corresponding $\bm{\pi}$.


 
 \begin{lemma}
    \label{lem: Online Linear Optimization}
     Any sequence of policies $\bm{\pi}^1,\ldots,\bm{\pi}^\Nround$ over our DP graph with associated node probability measures $\bm{q}^1,\ldots,\bm{q}^\Nround$ has discretized regret $\textsc{Regret}_{\mathcal{B}} = \max_{\bm{q} \in \mathcal{Q}} \sum_{\nround=1}^\Nround \langle \bm{q} - \bm{q}^\nround, \bm{w}^\nround\rangle$. Here, $\bm{w}^\nround = \{w^\nround_m(b)\}_{m \in [M], b \in \mathcal{B}}$ represents vector of the round $\nround$ rewards for all possible $(m, b)$ unit-bid value pairs.
 \end{lemma}

 

Having shown convexity of our action space and the mapping to an equivalent OLO problem, we are ready to state the key result for Algorithm~\ref{alg: OMD}, for the bandit setting.

\begin{theorem}[Online Mirror Descent: Bandit Feedback] \label{thm: OMD}    With $\eta = \Theta(\sqrt{\frac{\log |\mathcal{B}|}{|\mathcal{B}|\Nround}})$, Algorithm~\ref{alg: OMD} achieves (discretized) regret $O(\Nitem \sqrt{|\mathcal{B}| \Nround \log |\mathcal{B}|})$, with total time and space complexity polynomial in $\Nitem$, $|\mathcal{B}|$, and $\Nround$. Optimizing for discretization error from restricting the bid space to $\mathcal{B}$, we obtain a continuous regret of $O(\Nitem \Nround^{\frac{2}{3}})$. 
\end{theorem}

Under full information, we recover the regret bound of Algorithm \ref{alg: Decoupled Exponential Weights} by replacing the node weight estimates with the true weights.

\begin{corollary}[Online Mirror Descent: Full Information] \label{cor}
    With $\eta = \Theta(\sqrt{\frac{\log |\mathcal{B}|}{T}})$, Algorithm \ref{alg: OMD} achieves (discretized) regret $O(\Nitem \sqrt{ \Nround \log |\mathcal{B}|})$, with total time and space complexity polynomial in $\Nitem$, $|\mathcal{B}|$, and $\Nround$. Optimizing for discretization error from restricting the bid space to $\mathcal{B}$, we obtain a continuous regret of $O(\Nitem \sqrt{\Nround \log \Nround})$.
\end{corollary}





\section{Regret Lower Bound}

\label{sec: lower bound}

We remark that our OMD algorithms, under both full information and bandit settings, were designed to be robust to adversarial environments and incur discretized regret linear in $M$. In this section, we show that this is the best one can do, even in the stochastic setting. More specifically, we construct a corresponding (discretized) regret lower bound for our online bid optimization problem. At a high level, we will construct two bid vectors with nearly optimal expected utility under stochastic highest other bids. We derive the precise distribution of highest other bids and, using Le Cam's method, show that no algorithm in the full information or bandit feedback setting can learn the optimal bid vector quickly enough to avoid incurring $O(\Nitem \sqrt{\Nround})$ regret. 
\begin{theorem}\label{thm:lower}
    Under the full information setting, the discretized regret is lower bounded with $\textsc{Regret}_{\mathcal{B}} \in \Omega(\Nitem \sqrt{\Nround})$. This implies an equivalent regret lower bound in the bandit feedback setting.
\end{theorem}


We remark that our regret lower bound matches our upper bound for the OMD algorithm in the full information setting (up to a $\sqrt{\log |\mathcal{B}|}$ factor), as well as in the bandit setting up to a factor of $\sqrt{|\mathcal{B}| \log |\mathcal{B}|}$ factor.


                                    
   







