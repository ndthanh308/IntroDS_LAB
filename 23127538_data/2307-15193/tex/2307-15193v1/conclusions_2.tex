\section{Concluding Remarks}

We have provided low-regret learning algorithms for PAB auctions in the full information and bandit settings with corresponding polynomial time and space complexities. In particular, we utilize our DP formulation and its equivalent graph representation to decouple the utility associated with bidding $b_\nitem = b$ for all $\nitem \in [\Nitem], b \in \mathcal{B}$. We derived two algorithms, one that mimics the exponential weights algorithm and another based on OMD, both of which allowed us to achieve polynomial (in $\Nitem$, $|\mathcal{B}|$, and $\Nround$) regret upper bounds, as well as time and space complexities, despite the combinatorially large bid space.

There are several intriguing avenues for future research that can be explored based on the current work. A promising direction is to leverage the structure induced by bid monotonicity in PAB auctions. 
Recent advancements in a simpler single-unit setting have demonstrated the efficacy of cross-learning between bids under certain feedback structures \citep{OptimalNoRegretFPA2020, LearningBidOptimallyAdversarialFPA2020}. It would be intriguing to investigate the potential benefits of applying cross-learning techniques  in our multi-unit setting. By incorporating such methods, we can explore whether they can enhance our regret bounds. Furthermore, inspired by our numerical results---where we show that the winning bids in PAB market dynamics converge to the same value---we can explore the design of online learning algorithms for the setting where bidders are restricted to a simplified bidding interface, wherein they are only allowed to submit a single  price and quantity for the units demanded rather than an entire vector of bids. 

%we do not utilize this additional side information and assume that agents only observe the utility corresponding to their submitted bid vector. It is unclear how to utilize the side information to construct meaningfully lower variance, unbiased utility estimates of either entire bid vectors or of individual slot-bid pairs. Under  the decoupled sampler algorithm for the full information setting, we have perfect cross learning between bids \cite{ContextBanditsCrossLearning2019}, the bandit setting makes Hedge style algorithms break down and it is difficult to make use of side information. In particular, our bandit setting algorithm did not utilize any cross learning over bids. Cross learning over bids by applying algorithms such as $\textsc{Exp3.G}$ or $\textsc{Exp3.SET}$ is difficult as the feedback graph is never revealed, i.e. the agent never observes the reserve nor the adversary's lowest winning bid and highest losing bid. Without this knowledge, the agent cannot construct unbiased utility estimates of bid vectors, let alone per-slot utility estimates. Conversely, cross learning over valuations is difficult as FTRL algorithms have an updating step which is difficult to decouple due to the projection step. This is one interesting future avenue of investigation.

%Regarding the difficulty of cross learning assuming both non-stationary valuations and adversarial adversary actions, we note that \cite{LearningBidOptimallyAdversarialFPA2020, OptimalNoRegretFPA2020} proved a related result regarding the impossibility of achieving sublinear (in $\Nround$) regret when the adversary's actions $\bm{b}^{\nround}_-$ are dependent on $\bm{v}$ in the bandit setting, regardless if these valuations were generated adversarially or stochastically. They give a simple example---that extends straightforwardly to the multi-unit setting---with guaranteed linear regret under any learning algorithm. They provide an efficient algorithm that utilizes graph feedback for an arm elimination algorithm in the case of $\Nitem = 1$. It is unknown whether their result can be extended to the case of higher dimensional $\bm{b}^{\nround}_-$ as in our setting. Interestingly, their results showed that it is possible to utilize cross learning without always observing the entire graph feedback structure, which many existing similar cross-learning algorithms require. In contrast to their adversarial valuation and i.i.d. environment assumption, we assumed adversarial environment and fixed valuation profile in the bandit setting. As such, an interesting open question is how to utilize cross learning to allow our algorithms to operate under the adversarial or non-stationary valuations.