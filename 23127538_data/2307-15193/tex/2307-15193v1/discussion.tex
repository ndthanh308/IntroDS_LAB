\section{Discussions and Extensions}
\label{sec: discussion}

\rigel{TODO:
\begin{enumerate}
    \item Do path kernels proof from scratch
    \item Do the discussions and extensions section
    \item Run experiments
    \item Clarify the DP over time?
\end{enumerate}
}

\subsection{Time Varying Valuations}

\rigel{May not be as easy in the bandit setting since the $q$'s might need to be recalculated each round. Moreover, the algorithm used in Golrezaei's paper requires integrating/summing over the context distribution multiplied by the probability of selecting an action inside this distribution. It might be possible in our setting, since the valuations enter into the utility additively}

Thus far, we have assumed that the valuation profile $\bm{v}$ remains static throughout all $\Nround$ iterations. This can be unrealistic in many real world scenarios, as demands can change over time, either in a stochastic or adversarial fashion. As such, it may be desirable to construct efficient, no-regret algorithms that allow for time varying valuations $\{\bm{v}^\nround\}_{\nround \in [T]}$. While it is known that there no online learning algorithm can obtain vanishing regret assuming adversarially generated contexts and adversarial rewards, there exist efficient learning algorithms for the stochastic contexts setting \cite{ContextBanditsCrossLearning2019}. Fortunately, the structure of our decoupled exponential weights algorithms, namely Algorithm~\ref{alg: Decoupled Exponential Weights} and Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels}, naturally generalizes to the case of time varying, stochastic valuations. To be more concrete, we formally define the regret as the contextual regret w.r.t. the (unknown) distribution $F_v$ over valuation vectors as contexts. The contextual regret is defined as the difference between the optimal fixed bid vector and utility under their learning strategy, in expectation over the realized valuation vectors $\bm{v}^1,\ldots,\bm{v}^\Nround$ and the randomness of the learning algorithm.
\begin{align}
\tag{Continuous Contextual Regret}
    \textsc{Regret}(F_v) = \max_{\bm{b} \in [0, 1]^{+\Nitem}} \mathbb{E}_{F_v}\left[\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}; \bm{v}^\nround) - \mathbb{E}\left[\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround; \bm{v}^\nround)\right]\right]
\end{align}
and
\begin{align}
\tag{Discretized Contextual Regret}
    \textsc{Regret}_\mathcal{B}(F_v) = \max_{\bm{b} \in \mathcal{B}^{+\Nitem}} \mathbb{E}_{F_v}\left[\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}; \bm{v}^\nround) - \mathbb{E}\left[\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround; \bm{v}^\nround)\right]\right]\,,
\end{align}
Here, $\mu_n^\nround(\bm{b}; \bm{v}^\nround) = \sum_{\nitem=1}^\Nitem (v^t_m - b_m)1_{b \geq b^\nround_{-m}}$ is simply the utility that bidder $n$ receives with bid $\bm{b}$ and valuation profile $\bm{v}^\nround$ at round $\nround$. The key algorithmic modification that enables cross-learning between valuations (see Section 3.2.2. of \cite{ContextBanditsCrossLearning2019} on Algorithm $\textsc{EXP3.CL-U}$) is maintaining a sampling distribution over bid vectors for each possible valuation profile. At a high level, for a particular valuation profile, the probability of selecting bid vector $\bm{b}$ is proportional to its exponentiated cumulative utility, normalized by the probability of selecting this action in previous rounds under the realized valuations. Of course, blindly applying $\textsc{EXP3.CL-U}$ will result in exponential time complexity and regret, as there are a combinatorially large number of arms. We face two immediate challenges when generalizing Algorithm $\textsc{EXP3.CL-U}$ to our setting. First, the valuation space may be continuous, whereas $\textsc{EXP3.CL-U}$ only works for finite context space. This may be addressed via discretization, as we did to our bid space, and balancing the discretization error with our regret bound. For simplicity, we assume a discretization of $\mathcal{B}$, i.e., $\bm{v}^\nround \in \mathcal{B}^{+\Nitem}$. Second, and more importantly, due to he combinatorial nature of the multi-unit setting, we must once decouple the utilities associated with the per-unit bid \textit{and} valuation. This leads naturally to the following generalization of our decoupled exponential weights algorithm (for the bandit feedback setting, though the extension to the full information setting is straightforward):


 \begin{algorithm}[t]
        \KwIn{Learning rate $\eta > 0$, Valuation Distribution $F_v$, Exploration rate $\alpha > 0$, and Bias corrector $\beta > 0$}	
        \KwOut{The aggregate utility $\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b}^{\nround}; \bm{v}^\nround)$}
	$\hat{W}_\nitem^0(b) \gets 0, \hat{\textsc{NumUse}}_\nitem^0(b) \gets 0, \hat{\textsc{NumWon}}_\nitem^0(b) \gets 0$ for all $\nitem \in [\Nitem], b \in \mathcal{B}$\;
        $b_{0}^t \gets \max \mathcal B$, and $\hat{S}_{M+1}^t (b)=0$ for any $t\in[T]$ and $b\in \mathcal B$\;
 
	\For{$\nround \in [\Nround]$:}{
            Observe Valuation Profile $\bm{v}^\nround \sim F_v$ \rigel{Left off here; complete later}\;
            %Adversary selects $\bm{b}^{\nround}_-$.
            \textbf{Recursively Computing Exponentially Weighted Partial Utilities $\bm{S}^t$}\;
             \For{$\nitem \in [\Nitem-1,\ldots,1]$}{
            $\hat{S}^t_\nitem(b) \gets \exp(\eta \hat{W}_\nitem^\nround(b)) \sum_{b' \leq b} \hat{S}_{\nitem + 1}^\nround(b'), \quad b \in \mathcal{B}$. \hspace{30mm} $\backslash \backslash$ $\textsc{Compute}-\hat{S}_\nitem$
            }
        \textbf{Determining the Bid Vector $\bm{b}^\nround$ Recursively}\;
        \For{$\nitem \in [1, 2, \ldots, M]$:}{
            Set $b_\nitem^\nround$ to $b \in \mathcal B, b \le b_{m-1}^t$ with probability $ \frac{\hat{S}^t_\nitem(b)}{\sum_{b' \leq b_{\nitem-1}^t} \hat{S}^t_{\nitem}(b')}; \,.$ \hspace{16mm} $\backslash \backslash$ $\textsc{Sample}-\bm{b}$
        }
        %$\bm{b}^{\nround} \gets \textsc{Sampler}(\bm{W}^{\nround-1}, \eta)$\;
        Observe $\bm{b}^{\nround}_-$ and receive reward $\mu_n^\nround(\bm{b}^{\nround})$\;
        \textbf{Recursively Computing Probability Measure $\bm{q}$}\;
        $q^t_1(b) \gets \frac{\hat{S}^\nround_m(b)}{\sum_{b' \in \mathcal{B}} \hat{S}^\nround_m(b')}$ for all $b \in \mathcal{B}$\;
        \For{$\nitem \in [2, \ldots, M]$:}{
            $q_\nitem^\nround(b) \gets \sum_{b' \geq b} \frac{q_{\nitem-1}^t(b')\hat{S}^\nround_{\nitem}(b)}{\sum_{b" \geq b'} \hat{S}^\nround_\nitem(b")}$ for all $b \in \mathcal{B}$\;
        }
        \textbf{Update Weight Estimates}\;
        \For{$\nitem \in [\Nitem], b \in \mathcal{B}:$}{
            $\hat{\textsc{NumUse}}_\nitem^\nround(b) \gets \hat{\textsc{NumUse}}_\nitem^{\nround-1}(b) + \frac{\textbf{1}_{b = b_m^\nround}}{q_m^\nround(b)}$\;
            $\hat{\textsc{NumWon}}_\nitem^\nround(b) \gets \hat{\textsc{NumWon}}_\nitem^{\nround-1}(b) + \frac{\textbf{1}_{b = b_m^\nround > b^{\nround}_{-\nitem}}}{q_m^\nround(b)}$\;
            $\hat{W}^{\nround+1}_{\nitem}(b) \gets \nround - \hat{\textsc{NumUse}}_\nitem^\nround(b) +  \hat{\textsc{NumWon}}_\nitem^{\nround}(b) (v_\nitem - b)$\;
        }
        }
        \textbf{Return} $\sum_{\nround=1}^\Nround \mu_n^{\nround}(\bm{b}^{\nround})$
	\caption{\textsc{Decouplsed Exponential Weights - Bandit Feedback, Stochastic Valuations}}
	\label{alg: Decoupled Exponential Weights - Time Varying}
\end{algorithm}

\begin{theorem}[Learning to Bid in Pay-As-Bid Auctions under Bandit Feedback Using Decoupled Exponential Weights] \label{thm:decoupled exp - time varying}
    With $\eta \propto \sqrt{\frac{\log |\mathcal{B}|}{T}}$ such that $\eta \lesssim \frac{1}{M}$, Algorithm \ref{alg: Decoupled Exponential Weights - Time Varying} achieves (discretized) regret $O(\Nitem^{\frac{3}{2}} \sqrt{ |\mathcal{{B}|}\Nround \log |\mathcal{B}|})$, with total time and space complexity polynomial in $\Nitem$, $|\mathcal{B}|$, and $\Nround$. Optimizing for discretization error from restricting the bid space to $\mathcal{B}$, we obtain a continuous regret of $O(M^{\frac{4}{3}}T^{\frac{2}{3}} \sqrt{\log \Nround})$.
\end{theorem}

\begin{proof}
    \rigel{Add later possibly? Might not be worth it, just keep this section high level? Or alternatively, I can add this subsection to the end of the path kernels algorithm section since we aren't using the OMD setup at all; let me know what you think}
\end{proof}




\rigel{Finish/(formalize?) this later; basically just say that we can fully cross learn in these two algorithms because of the $\textsc{NumWon}$ and $\textsc{NumUse}$ variables. Perhaps include pseudocode with the minor change? Or should I just say that we replace $v_m$ with $v_m^t$ in both algorithms? We will also need to update the set of individually rational bid vectors depending on the valuation vector. Argue that the OMD algorithm doesn't generalize. However, the FTRL algorithm generalizes to time varying valuations straightforwardly, but we need to argue that FTRL obtains the same iterates as OMD, and then analyze the contextual regret. Might take a long time, especially for this FTRL bit.}

\subsection{Ordered Slates} \rigel{Find and cite all ordered slates papers; essentially the optimal upper regret bounds on ordered slate is $O(M^{\frac{3}{2}}\sqrt{|\mathcal{B}| T}$, which we can improve using OMD. Might be state of art?}

\rigel{Can try to just say more generally instead of PAB, we can also deal with the GSP and GFP settings}

\subsection{Properties of Nash and Coarse Correlated Equilibria} 

\begin{enumerate}
    \item Pure strategy Nash equilibria (PSNE) and Bayes Nash Equilibria BNE are known and characterized in (https://arxiv.org/pdf/1303.1646.pdf). The PSNE is efficient, albeit allows for weakly dominated strategy (overbidding in losing positions); the BNE are not necessarily efficient.
    \item The optimal strategy against a stochastic adversary need not be $(b, m)$-reducible; i.e. 2 bidders, 2 items, bidder has valuations $[1, 1]$ but adversary bids uniformly equal probabilities between $[0.2, 0]$ and $[0.1, 0.1]$. The Bayes optimal bid vector is $[0.2+\epsilon, 0.1+\epsilon]$.
    \item However, it is hard to say anything about CCE. Think about this more.
\end{enumerate}

\if 0

Thus far, we have concerned ourselves with the online learning setting for bid optimization. From the traditional economics standpoint, characterizing the equilibria of PAB analytically may provide equally valuable predictive insight as to the long term behavior of agents in these repeated auctions. As such, we ask ourselves whether the Nash equilibria (NE), and similarly the Coarse Correlated Equilibria (CCE) to which no-regret learning is guaranteed to converge to, can be computed. It turns out that both of these objects can be explicitly described and take on a surprisingly compact form.

\subsubsection{Nash Equilibria of PAB}

In a (pure) Nash equilibrium, agents have no incentive to deviate from their equilibrium strategy. In our setting, this means that each bidder's equilibrium bid vector is weakly utility maximizing w.r.t. the other bidders' equilibrium bid vectors. While in the general game setting, there may be multiple or zero such NE, we can show that the NE of PAB both exists and is unique. In particular, we compute the Nash equilibria of PAB when each agent $n$'s bid space is unrestricted; i.e., $[0, 1]$ instead of $\mathcal{B}$.

\begin{theorem}
    \label{thm: Nash Equilibria}
    \rigel{Reference https://arxiv.org/pdf/1303.1646.pdf for characterization; all bidders bid at $v_{(\overline{M})}$ for items in the first $m_n^*$ slots and $v_{(\overline{M})} - \epsilon$ for all other slots. This equilibrium is unstable/weakly dominated when considering randomness, because bidders potentially overbid for slots $m_n^*+1$ through $M$, thus cannot be reached by our learning algorithms, which assume individual rationality.}
    In a PAB auction with $\overline{M}$ items and $N$ bidders, where each bidder $n$ is endowed with valuation profile $\bm{v}^n \in [0, 1]^{+M_n}$ and must submit bid vector $\bm{b}^n \in [0, 1]^{+M_n}$, there exists a pure strategy Nash Equilibrium. Letting $v_{(1)} > v_{(2)} > \ldots$ denote the order statistics of all per-unit valuations, we define $v^*_n$ to be the largest valuation less than $v_{(\overline{M})}$ among all bidders except bidder $n$. Furthermore, we define $m^*_n$ to be the number of valuations of bidder $n$ at least $v_{(\overline{M})}$. Then, in any Nash equilibrium, each bidder $n$ submits bids of $v^*_n$ for the first $m^*_n$ slots and their valuation for the remaining slots. We note that this Nash equilibrium is unique up to the behavior in the first $m_n^*$ slots \rigel{Need better way to phrase this last bit}.
\end{theorem}

\begin{proof}
    \rigel{Fill in later; basically goes like this. Start from bidding exactly your valuation for each item. In fixed order, for each bidder, compute their utility maximizing bid vector. Repeat until there exists no incentive to improve for any bidder (or prove that this quantity does not exist/fluctuates between lowering price/sacrificing an item)}
\end{proof}

\begin{theorem}
    \label{thm: CCE}
    \rigel{Tentative, need to make sure this is true} All CCE of PAB are such that each bidder's equilibrium bidding strategy is characterized by an $(m_n^*, b_n^*)$ pair, where the bidders all bid $b_n^*$ for the first $m_n^*$ units. \rigel{Is it the case that $b_n^* = b^*$ for all $n$?}
\end{theorem}

Furthermore, we claim that all CCE of PAB are characterized by a $(m_n^*, b_n^*)$ pair in the stochastic highest other bids setting. This leads us to have the following simplified algorithm, wherein bidders submit a bid vector $\bm{b}$ which is characterized by a price $b \in \mathcal{B}$ and quantity $m \in [M]$, where the first $m$ bids are $b$ and are 0 for the remaining slots.

\fi





\begin{enumerate}
    \item Add the full information algorithm with bandit feedback (path kernels) here, and OMD algorithm with full information feedback as well
    \item Time varying valuations (path kernels and full information algorithm; talk about the context/cross learning across contexts, what the new definition of regret would be under these different contexts)
    \item Relationship to ordered slate problem (flesh out the ordered slate problem); show that the OMD algorithm actually improves the state of the art regret upper bound.
    \item Talk about how in CCE, the welfare is guaranteed to be optimal (show the proof for this) \rigel{Check the proof for this}. Talk about the possible revenue guarantees in the CCE vs NE; mention that the PAB is smooth auction, (robust to aftermarkets) so the revenue under NE is guaranteed to to have PoA lower bounded by $\frac{e-1}{e}\textsc{OptWelfare}$. 
    \item Implement FTRL and compare the iterates vs that of OMD
\end{enumerate}