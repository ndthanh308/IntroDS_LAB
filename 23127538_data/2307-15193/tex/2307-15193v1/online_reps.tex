\section{Bid Optimization: Bandit Feedback}

In this section, we now consider the more realistic feedback setting where agent $n$ only observes $H^{\nround}$ regarding each auction $\nround$'s allocation $x^\nround = x^\nround_n(\bm{b}^\nround)$. We apply ideas from solutions to the Stochastic Shortest Paths problem (SSP) to instead associate weights with each node given by a $(\nitem, B)$ pair, which can be interpreted as the action of bidding bid $B \in \mathcal{B}$ at slot $\nitem \in [\Nitem]$. \rigel{Fix the flow here} Using vanilla $\textsc{Exp3}$ algorithm will result in exponential regret and time complexity. Instead, one may be tempted to use generalizations of $\textsc{Exp3}$ that exploit additional information for cross-learning between bid vectors. Note that any bid above $c^\nround = b^{\nround}_{x^\nround}$ in the first $x^{\nround}$ slots is guaranteed to win the corresponding item, and any bid below $d^\nround = b^{\nround}_{x^\nround + 1}$ is guaranteed to lose the item. This implies that agent $n$ can determine the counterfactual utility of any bid vector $\bm{b}$ at round $\nround$ if $b_{x^{\nround} \geq c^\nround}$ and $b_{x^{\nround}+1} \leq d^\nround$. These generalized $\textsc{Exp3}$ algorithms will construct importance sampling estimators of the utility of bid vectors as follows: Let $\bm{b}$ denote the bid vector sampled at round $\nround$ according to distribution $F^{\nround}$, with corresponding $x^\nround, c^\nround, d^\nround$. Then, we define utility estimates $\hat{\mu}^{\nround}_n(\bm{B}') = \mu^{\nround}_n(\bm{B}') \textbf{1}(\text{Observe $\bm{B}'$ | $\bm{b}^{\nround} = \bm{B}$}) \prob(\text{Observe $\bm{B}'$ | $\bm{b}^{\nround} \sim F^{\nround}$})^{-1}$. However, this last term $\prob(\text{Observe $\bm{B}'$ | $\bm{b}^{\nround} \sim F^{\nround}$})$ is problematic. Recalling from above that the utility of $\bm{B}'$ is only observable under $\bm{b}^{\nround}$ if the following three conditions hold:
\begin{enumerate}
    \item The allocation $x^{\nround}_n(\bm{B}') = \sum_{\nitem=1}^\Nitem \textbf{1}_{B'_\nitem \geq \max(b^{\nround}_{-\nitem}, \pi^\nround}$ is equal to $x^\nround$.
    \item The clearing price $c^\nround = B'_{x^\nround}$ under $\bm{B}'$ is at least as large as $b^{\nround}_{x^\nround}$.
    \item The guaranteed losing price $d^\nround = '_{x^\nround+1}$ under $\bm{B}'$ is at most $b^{\nround}_{x^\nround+1}$.
\end{enumerate}
The set of $\bm{b}^{\nround}$ that satisfies the above conditions is never revealed to the bidder and hence, one of the terms required to define the utility estimates cannot be computed. As such, we resort to learning algorithms that construct utility estimates that do not depend on the entire set of observable bid vectors. Such algorithms include the FTRL based $\textsc{O-REPS}$ algorithm among other techniques to solve SSP problems. In these methods, the utility estimates depend only on the per-slot utilities of the selected bid vector. We first explain how to model our bid optimization problem as an SSP problem and then briefly describe an algorithm to solve it. Then, we show a variant of this algorithm that achieves better time and space complexity.

\subsection{Shortest Paths Formulation}

At a high level, the agent $n$ only updates their utility estimates corresponding to the slot-bid value pairs corresponding to their action $\{w^\nround_\nitem(b^\nround_\nitem)\}_{\nitem \in [\Nitem]}$ in addition to the aggregate utility $\mu^{\nround}_n(\bm{b}^\nround)$. This information feedback system is common among many shortest paths problems, where agents observe the utility derived from each selected edge in addition to the total path utility. Using this commonality, we frame our bid learning problem as an instance of episodic learning over Markov Decision Processes (MDPs) with adversarially changing costs.
\begin{enumerate}
    \item We define 2 types of states: source node $S_0$, followed by $\Nitem$ layers of states of width $|\mathcal{B}|$ between them which we denote by $\{(\nitem, B)\}_{\nitem \in [\Nitem], B \in \mathcal{B}}$. We let $\mathcal{S}$ denote the set of all states and let $s_\nitem$ denote which $B$ bid value the agent is at on layer $\nitem$.
    \item There are 2 types of actions available at each state which describe which state in the subsequent layer the agent will move to: actions from $S_0$ to the entire first layer of nodes $\{b_{1,B}\}_{B \in \mathcal{B}}$ and actions between layers $\nitem$ to layer $\nitem+1$. In particular, the latter set of actions requires non-decreasing bid value---i.e. $(\nitem, B)$ only has an action to $(\nitem+1, B')$ for $B' \leq B$. This reflects the bid monotonicity assumption. With probability 1, the agent will transition to the state prescribed by their action. We let $\mathcal{A} \equiv \mathcal{B}$ denote the set of possible actions with subsets $\mathcal{A}_B = \{B' \in \mathcal{B}: B' \leq B\}$ being the actions available at state $(\nitem, B)$ for $\nitem \in [\Nitem]$. Let $a_\nitem$ denote which action in $\mathcal{A}$ was taken at layer $\nitem$.
    \item There are 3 parameters: the valuation vector $\bm{v}$, adversarial (sorted) bid vector $\bm{b}^{\nround}_-$, and reserve price $\pi^\nround$. With these parameters, the weight of any incoming edge to state $(\nitem, B)$ is given by $w^{\nround}_{\nitem}(B) = 1_{B \geq \max(b^{\nround}_{-\nitem}, \pi^\nround)} (v^{\nround}_\nitem - B)$. This is precisely the same as the definition of $w^{\nround}_{\nitem}(B)$ as in offline bid optimization. The total reward of episode $\nround$ is equal to the sum of the edge weights traversed.
    \item We define MDP $\mathcal{M}^\nround = (\mathcal{S}, \mathcal{A}, \mathcal{P}, \{\bm{w}^\nround\})$ to have states and actions as prescribed above, deterministic transitions $\mathcal{P}$ such that $\prob(s_{\nitem+1} = B' \mid s_{\nitem} = B, a_\nitem = B') = 1$, and edge weights defined by $\bm{w}^\nround$.
\end{enumerate}
Let $\pi: \mathcal{S} \times \mathcal{A} \to [0, 1]$ denote a policy over MDP $\mathcal{M}^\nround$, where $\pi((\nitem, B), B')$ denotes the probability of selecting action $B' \leq B$ at state $(\nitem, B)$. We abuse notation and let $B \sim \pi$ denote a sequence of bid values $(s_1,\ldots,s_\Nitem)$ within a path sampled according to policy $\pi$. We then define a corresponding state-action occupancy measure $\rho^\pi((\nitem, B), B') = \prob_{\bm{B} \sim \pi}(s_\nitem = B, a_\nitem = B')$, with $\rho^\pi(S_0, B') = \prob_{\bm{B} \sim \pi}(s_0 = S_0, a_0 = B_1)$. \rigel{Look up Online network flow bandit problem} Since we are operating under a deterministic Markovian environment with (directed) edges between consecutive layers, we have:
\begin{align}
    \sum_{B' \leq B} \rho^\pi((\nitem, B), B') = \sum_{B" \geq B} \rho^\pi((\nitem-1, B"), B) \quad \text{and} \quad \sum_{B \in \mathcal{B}} \sum_{B' \leq B} \rho^\pi((\nitem, B), B') = 1
\end{align}
Let $\Pi$ denote the set of all policies on $\mathcal{M}_\nround$. With the above equalities and the obvious non-negativity constraints $\rho^\pi((\nitem, B), B') \geq 0$ for all $B' \leq B$, we can now define the set $\Delta$ of all possible state-action occupancy measures.
\begin{definition}
    Let $\Delta(\Pi)$ denote the set of all $\rho \in \mathcal{S} \times \mathcal{A} \to [0, 1]$ with the following properties:
    \begin{enumerate}
        \item $\sum_{B \in \mathcal{B}} \sum_{B' \leq B} \rho((\nitem, B), B') = 1$ for all $\nitem \in [\Nitem]$
        \item $\sum_{B \in \mathcal{B}} \rho(S_0, B) = 1$
        \item $\sum_{B' \leq B} \rho((\nitem, B), B') = \sum_{B" \geq B} \rho((\nitem-1, B"), B)$ for all $B \in \mathcal{B}, \nitem \in [\Nitem]$
        \item $\sum_{B' \leq B} \rho((1, B), B') = \rho(S_0, B)$
    \end{enumerate}
    This is equivalent to the condition that exists a policy $\pi \in \Pi$ such that $\rho((\nitem, B), B') = \prob_{\bm{B} \sim \pi}(s_\nitem = B_\nitem, a_\nitem = B_{\nitem+1})$ for all $B \geq B' \in \mathcal{B}$, $\nitem \in [\Nitem]$ and $\rho(S_0, B') = \prob_{\bm{B} \sim \pi}(s_0 = S_0, a_0 = B_1)$.
\end{definition}
We say that policy $\pi$ generates state-action occupancy measure $\rho$ if $\pi(s, a) = \frac{\rho(s, a)}{\sum_{b: \mathcal{A}(s)} \rho(s, b)}$ for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$ where $\mathcal{A}(s)$ denotes the set of valid actions at state $s$. From the definition of $\Delta(\Pi)$, we can see that each $\rho$ uniquely determines $\pi$ and vice versa. We let $\pi^\rho$ denote the policy that generates the occupancy measure $\rho$. Hence, computing the regret minimizing policy $\pi^\rho \in \Pi$ is equivalent to minimizing the regret minimizing state-action occupancy measure $\rho$. Once computing $\rho$, we can compute the corresponding $\pi^\rho$ and then simulate a bid vector according to $\pi^\rho$. The key idea is that optimizing with respect to $\rho$, as opposed to $\pi$, can be framed as instance of online linear optimization. More specifically, we can compute the (expected) loss function at round $\nround$ as follows:
\begin{align}
    \mathbb{E}_{\bm{B} \sim \pi^\rho}\left[ \mu^{\nround}_n(\bm{B}) \right] = \mathbb{E}_{\bm{B} \sim \pi^\rho}\left[ \sum_{\nitem=1}^\Nitem w_\nitem^\nround(B_\nitem) \right] = \sum_{\nitem=1}^\Nitem \prob_{\bm{B} \sim \pi^\rho}(s_{\nitem-1} = B_{\nitem-1}, a_{\nitem-1} = B_{\nitem}) w_\nitem^\nround(B_\nitem)
\end{align}
Substituting in the definition of $\pi^\rho$ and the corresponding $\rho$, we have:
\begin{align}
    \mathbb{E}_{\bm{B} \sim \pi^\rho}\left[ \mu^{\nround}_n(\bm{B}) \right] = \sum_{\nitem=1}^\Nitem \sum_{B \in \mathcal{B}} \sum_{B' \leq B} \rho((\nitem-1, B), B') w_\nitem^\nround(B') = \langle \bm{\rho}, \bm{w}^\nround \rangle 
\end{align}
Where we assume $s_0 = S_0$ and define $\bm{w}^\nround \equiv \{w_\nitem^\nround(B')\}_{\nitem \in [\Nitem], B \geq B' \in \mathcal{B}}$. Assuming that the learner selects occupancy measure $\bm{\rho}^\nround$ at round $\nround$ with fixed valuation profile $\bm{v}$, the regret can then be written as:
\begin{align}
    \textsc{Regret}_\mathcal{B}^{\Nround} &= \max_{\bm{b} \in \mathcal{B}^{+\Nitem}} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}) - \mathbb{E}_{\bm{b}^\nround \sim \bm{\rho}^\nround} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)\\
    &\leq \max_{\bm{\rho} \in \Delta(\Pi)} \mathbb{E}_{\bm{b} \sim \bm{\rho}} \sum_{\nround=1}^\Nround \mu(\bm{b}) - \mathbb{E}_{\bm{b}^\nround \sim \bm{\rho}^\nround} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)\\
    &= \max_{\bm{\rho} \in \Delta(\Pi)} \sum_{\nround=1}^\Nround \langle \bm{\rho} - \bm{\rho}^\nround, \bm{w}^\nround \rangle = \max_{\bm{\rho} \in \Delta(\Pi)} \sum_{\nround=1}^\Nround \langle  \bm{\rho}^\nround - \bm{\rho}, -\bm{w}^\nround \rangle
\end{align}
\rigel{Add condition that $\bm{\rho}^\nround$ is in $\Delta(\Pi)$ for all $\nround$} Where the inequality follows as any deterministic bid can be represented as an expectation over a deterministic policy. Notice that to keep consistent with the SSP literature, we negate both terms in the dot product to represent the problem as loss minimization rather than utility maximization. Defining $D(\bm{\rho} || \bm{\rho}') = \sum_{s \in \mathcal{S}, a \in \mathcal{A}(s)} \rho(s, a)\frac{\log \rho(s, a)}{\rho'(s, a)} - (\rho(s, a) - \rho'(s, a))$, we are now ready to state the $\textsc{O-REPS}$ algorithm formally and give the corresponding performance guarantees.

\begin{algorithm}[t]
	\KwIn{Valuation $\bm{v} \in [0, 1]^\Nitem$, Learning rate $\eta > 0$, Adaptive Adversarial Environment $\textsc{Env}^\nround: \mathcal{H}^\nround \to \mathcal{B}^{-\Nitem} \times \mathcal{B}$ where $\mathcal{H}^\nround$ denotes the set of all possible historical auction results $H^\nround$ up to round $\nround$ for all $\nround \in [\Nround]$.}
	\KwOut{The aggregate utility $\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)$ corresponding to a sequence of bid vectors $\bm{b}^{1},\ldots,\bm{b}^{\Nround}$ sampled according to $\textsc{O-REPS}$.}
	$\pi^0(s, a) \gets \frac{1}{|\mathcal{A}(s)|}$ for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$. Let $\bm{\rho}^0$ be the corresponding state-action occupancy measure \;
        $H^0 \gets \emptyset$ \;
	\For{$\nround \in [\Nround]$:}{
            $(\bm{b}^{\nround}_-, \pi^\nround) \gets \textsc{Env}^{\nround-1}(H^{\nround-1})$ and $\bm{b}^{\nround} \sim \bm{\pi}^{\nround-1}$\;
            Observe $\bm{b}^{\nround}_-, \pi^\nround$ and receive reward $\mu^\nround_n(\bm{b}^\nround))$\;
            $\hat{w}_\nitem^\nround(B, B') \gets \frac{w_\nitem^\nround(B')}{\rho^{\nround-1}((\nitem-1, B), B')} \textbf{1}_{B = b^{\nround}_{\nitem-1},  B' = b^{\nround}_\nitem}$ for all $\nitem \in [\Nitem]$ and $B \geq B' \in \mathcal{B}$\;
            $\bm{\rho}^\nround \gets \text{argmin}_{\bm{\rho} \in \Delta(\Pi)} \eta\langle \bm{\rho}, -\hat{\bm{w}}^\nround\rangle + D(\bm{\rho} || \bm{\rho}^{\nround-1})$ and $\pi^\nround(s, a) \gets \frac{\rho^\nround(s, a)}{\sum_{b: \mathcal{A}(s)} \rho^\nround(s, b)}$\;
        }
        \textbf{Return} $\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)$
	\caption{\textsc{O-REPS}}
	\label{alg: O-REPS}
\end{algorithm}

\begin{theorem}
    Under bandit feedback, Algorithm~\ref{alg: O-REPS} achieves regret rate $\textsc{Regret}_\mathcal{B}^{\Nround} \lesssim O(\Nitem |\mathcal{B}| \sqrt{T \log |\mathcal{B}|})$ using $\eta = |\mathcal{B}|^{-1}\sqrt{\frac{\log |\mathcal{B}|}{T}}$ with respect to the discretized benchmark.
\end{theorem}

\begin{proof}
    At a high level, we want to bound the regret of Follow the (Entropy) Regularized leader. We first do this by upper bounding the regret by the regret of the unconstrained Be the (Entropy) Regularized leader (see Lemma 13 of \rigel{Cite Rakhlin 2009}. In particular, we define the corresponding unconstrained optimization problem:
    \begin{align}
        \tilde{\bm{\rho}}^{\nround+1} = \text{argmin}_{\bm{\rho} \in \mathcal{S} \times \mathcal{A} \to \mathbb{R}^+} \left( \eta \langle \bm{\rho}, -\hat{\bm{w}}^{\nround} \rangle + D(\bm{\rho} || \bm{\rho}^{\nround}) \right)
    \end{align}
    Where we are optimizing over all non-negative functions over state-action pairs rather than $\Delta(\Pi)$. Using the unbiasedness of our estimators $\hat{\bm{w}}^\nround$, we can replace $\bm{w}^\nround$ with $\hat{\bm{w}}^\nround$ in the definition of regret. Now, as it is shown in Lemma 13 of \rigel{Cite Rakhlin 2009}, we can upper bound the expected estimated regret as a function of the unconstrained optimizer $\tilde{\bm{\rho}}^{\nround+1}$ and the unregularized relative entropy with respect to the initial state-action occupancy measure $\bm{\rho}^1$. 
    \begin{align}
        \textsc{Regret}_\mathcal{B}^{\Nround} = \max_{\bm{\rho} \in \Delta(\Pi)} \mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{\rho}^{\nround} - \bm{\rho}, -\hat{\bm{w}}^{\nround} \rangle \right] \leq \max_{\bm{\rho} \in \Delta(\Pi)}\mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{\rho}^{\nround} - \tilde{\bm{\rho}}^{ \nround+1}, -\hat{\bm{w}}^{\nround} \rangle + \eta^{-1}D(\bm{\rho} || \bm{\rho}^{1}) \right]
    \end{align}
    Furthermore, the unconstrained optimizer can be solved with $\tilde{\bm{\rho}}^{ \nround+1} = \bm{\rho}^{\nround} \exp(\eta \hat{\bm{w}}^{\nround})$. Applying $\exp(x) \geq 1 + x$ for $x = \exp(\eta \hat{\bm{w}}^{\nround})$, we obtain $\tilde{\bm{\rho}}^{ \nround+1} \exp(\eta \hat{\bm{w}}^{\nround}) \geq \bm{\rho}^{\nround} + \bm{\rho}^{\nround}\eta \hat{\bm{w}}^{\nround}$. Plugging this back in:
    \begin{align}
        \textsc{Regret}_\mathcal{B}^{\Nround} &\leq \mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{\rho}^{\nround} - \bm{\rho}^{\nround} \exp(\eta \hat{\bm{w}}^{\nround}), -\hat{\bm{w}}^{\nround} \rangle + \eta^{-1}D(\bm{\rho} || \bm{\rho}^{1}) \right]\\
        &= \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{B \geq B' \in \mathcal{B}} \rho^{\nround}((\nitem-1, B), B') \hat{w}^{\nround}_\nitem(B, B')^2 + \eta^{-1}D(\bm{\rho} || \bm{\rho}^{1}) \right] \label{eq: node diff}
    \end{align}
    Note that $\hat{w}^{\nround}_\nitem(B, B') = \frac{w_\nitem^\nround(B')}{\rho^{\nround-1}((\nitem-1, B), B')} \textbf{1}_{B = b^{\nround}_{\nitem-1},  B' = b^{\nround}_\nitem}$ for all $\nitem \in [\Nitem]$ and $B \geq B' \in \mathcal{B}$ by definition. Since $w^{\nround}_\nitem(B) \leq 1$ and $\textbf{1}_{B = b^{\nround}_{\nitem-1},  B' = b^{\nround}_\nitem} \leq 1$ we have $\hat{w}^{\nround}_\nitem(B, B') \leq \frac{1}{\rho^{\nround}_\nitem((\nitem-1, B), B')}$ and we continue the above chain of inequalities with:
    \begin{align}
        \textsc{Regret}_\mathcal{B}^{\Nround} &\leq \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{B \geq B' \in \mathcal{B}} \rho^{\nround}((\nitem-1, B), B') \hat{w}^{\nround}_\nitem(B, B') \frac{1}{\rho^{\nround}((\nitem-1, B), B')}  + \eta^{-1}D(\bm{\rho} || \rho^{1}) \right] \label{eq: full info difference}\\
        &= \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{B \geq B' \in \mathcal{B}} \hat{w}^{\nround}_\nitem(B, B')  + \eta^{-1}D(\bm{\rho} || \bm{\rho}^{1}) \right]
    \end{align}
    Note that $D(\bm{\rho} || \bm{\rho}') \leq \sum_{\nitem=1}^\Nitem \sum_{B \geq B' \in \mathcal{B}} \left[-\rho((\nitem-1, B), B') \log \rho((\nitem-1, B), B') \right]$. Now, we use the fact that $\sum_{B \geq B' \in \mathcal{B}} \rho((\nitem-1, B), B') = 1$ for any $\rho \in \Delta(\Pi)$ and $\nitem \in [\Nitem]$, we have that $\rho((\nitem, \cdot), \cdot)$ is a valid probability mass function and has entropy upper bounded by $O(\log |\mathcal{B}|)$ which is the entropy of the uniform distribution over $|\mathcal{B}|^2$ items. Hence, $D(\bm{\rho} || \bm{\rho}') \lesssim \Nitem \log |\mathcal{B}|$. Plugging this back in:
    \begin{align}
        \textsc{Regret}_\mathcal{B}^{\Nround} &\lesssim \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{B \geq B' \in \mathcal{B}} \hat{w}^{\nround}_\nitem(B, B')  + \eta^{-1}\Nitem \log |\mathcal{B}| \right]\\
        &\leq \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem=1} \sum_{B \geq B' \in \mathcal{B}} \hat{w}^{\nround}_\nitem(B, B') + \eta^{-1}\Nitem \log |\mathcal{B}| \right]\\
        &\leq \eta \sum_{\nround=1}^\Nround \sum_{\nitem=1} \sum_{B \geq B' \in \mathcal{B}} w^{\nround}_\nitem(B, B') + \eta^{-1}\Nitem \log |\mathcal{B}|\\
        &= \eta \Nround \Nitem |\mathcal{B}|^2 + \eta^{-1}\Nitem \log |\mathcal{B}|
    \end{align}
    Where in the last equality, we used the unbiasedness property of $\hat{\bm{w}}^{\nround}$. Setting $\eta = |\mathcal{B}|^{-1}\sqrt{\frac{\log |\mathcal{B}|}{T}}$, we obtain $\textsc{Regret}_\mathcal{B}^{\Nround} \leq \Nitem |\mathcal{B}| \sqrt{\Nround \log |\mathcal{B}|}$. To handle the full information case, we note that we can improve line~\ref{eq: full info difference} by instead replacing $\hat{\bm{w}}^{\nround}$ with $\bm{w}^{\nround}$ in the previous line to obtain: 
    \begin{align}
        \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem \sum_{B \geq B' \in \mathcal{B}} \rho^{\nround}((\nitem-1, B), B') \hat{w}^{\nround}_\nitem(B)^2 &= \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem \sum_{B \geq B' \in \mathcal{B}} \rho^{\nround}((\nitem-1, B), B') w^{\nround}_\nitem((\nitem-1, B), B')^2\\
        &\leq \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem \sum_{B \geq B' \in \mathcal{B}} \rho^{\nround}_\nitem(B, B') = \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem 1 = \Nround \Nitem
    \end{align}
    Setting $\eta = \sqrt{\frac{ \log |\mathcal{B}|}{T}}$, we obtain in the full information setting $\textsc{Regret}_\mathcal{B}^{\Nround} \leq \Nitem \sqrt{\Nround \log |\mathcal{B}|}$.
\end{proof}

One may wonder how to efficiently update the state-action occupancy measures by computing the minimizer of $\eta\langle \bm{\rho}, -\hat{\bm{w}}^\nround\rangle + D(\bm{\rho} || \bm{\rho}^{\nround-1})$. While we relegate the details to \rigel{Cite O-REPS paper here}, the idea is to first solve the unconstrained entropy regularized minimizer with $\tilde{\bm{\rho}}^{ \nround+1} = \bm{\rho}^{\nround} \exp(\eta \hat{\bm{w}}^{\nround})$. We then project this unconstrained minimizer to $\Delta(\Pi)$ with:
\begin{align}
    \bm{\rho}^{\nround + 1} = \text{argmin}_{\bm{\rho} \in \Delta(\Pi)} D(\bm{\rho}||\tilde{\bm{\rho}}^{\nround + 1})
\end{align}
As shown in the analysis of \rigel{Cite O-REPS again}, this can be solved efficiently as an unconstrained convex optimization problem in $\mathbb{R}^{|\mathcal{B}|^2}$. Unfortunately, this projection prevents the straightforward generalization of our method to handle time varying valuation profiles.










\subsection{Reduction to State-Occupancy Measures}

One may realize that as the rewards associated with each edge are independent of its initial location, we may hope to further simplify the problem by assuming proportional conditional probability mass functions. That is, letting $B' \leq B$, we enforce that $\{\pi((\nitem, B), B")\}_{B" \leq B'} \propto \{\pi((\nitem, B'), B")\}_{B" \leq B'}$ for all $\nitem \in [\Nitem]$. We let $\Pi'$ denote the set of all $\Pi$ that fulfill this condition. This condition can be written succinctly as $\frac{\pi((\nitem, B), B')}{\pi((\nitem, B), B")} = \frac{\pi((\nitem, B'), B')}{\pi((\nitem, B'), B")}$ for all $\nitem$ and $B" \leq B' \leq B$. Letting $B_0$ denote the maximal possible bid, these conditions can be written even more concisely as $\bigcap_{\nitem=1}^\Nitem \bigcap_{B' \in \mathcal{B}} \bigcap_{B" \leq B'} \{\frac{\pi((\nitem, B_0), B')}{\pi((\nitem, B_0), B")} = \frac{\pi((\nitem, B'), B')}{\pi((\nitem, B'), B")}\}$. Using the fact that $\sum_{B' \leq B} \pi((\nitem, B), B') = 1$, we have:
\begin{align}
    &\frac{\pi((\nitem, B_0), B')}{\pi((\nitem, B_0), B")} = \frac{\pi((\nitem, B'), B')}{\pi((\nitem, B'), B")} \\
    &\leftrightarrow \pi((\nitem, B_0), B')\pi((\nitem, B'), B") = \pi((\nitem, B'), B')\pi((\nitem, B_0), B")\\
    &\to \sum_{B" \leq B'} \pi((\nitem, B_0), B')\pi((\nitem, B'), B") = \sum_{B" \leq B'} \pi((\nitem, B'), B')\pi((\nitem, B_0), B") \\
    &\leftrightarrow \pi((\nitem, B_0), B') = \pi((\nitem, B'), B') \sum_{B" \leq B'} \pi((\nitem, B_0), B")\\
    &\leftrightarrow \pi((\nitem, B'), B') = \frac{\pi((\nitem, B_0), B')}{\sum_{B" \leq B'} \pi((\nitem, B_0), B")}\\
    &\to \pi((\nitem, B), B') = \frac{\pi((\nitem, B_0), B')}{\sum_{B" \leq B} \pi((\nitem, B_0), B")}
\end{align}
Where in the last equality, we plugged in the value of $\pi((\nitem, B'), B')$ back into the proportionality constraint. At a high level, this allows us to describe the entire policy with only $\{\pi(S_0, B')\}_{B' \in \mathcal{B}} \cup \{\pi((\nitem, B_0), B')\}_{\nitem \in [\Nitem], B' \in \mathcal{B}}$, which is a set of size $O(\Nitem |\mathcal{B}|)$. Letting $\psi_\nitem(B) = \pi((\nitem-1, B_0), B)$, we see that we can rewrite $\pi((\nitem, B), B') = \frac{\psi(\nitem+1, B')}{\sum_{B" \leq B} \psi(\nitem+1, B")}$. Once again abusing notation, let $\bm{B} \sim \psi$ denote a bid vector sampled according to the policy generated from $\psi$. Furthermore, letting $\Psi: [\Nitem] \times \mathcal{B} \to [0, 1]$ such that $\sum_{B \in \mathcal{B}} \psi_\nitem(B) = 1$ denote the set of possible condensed policies $\psi$, we can recursively define the state occupancy measure $q^\psi(\nitem, B) = \prob_{\bm{B} \sim \psi}(s_\nitem = B_\nitem)$. With base case $q^\psi(1, B') = \psi(S_0, B')$, we have:
\begin{align}
    q^\psi(\nitem, B') = \sum_{B \geq B'} q^\psi(\nitem - 1, B)\pi((\nitem-1, B), B') = \psi_\nitem(B') \sum_{B \geq B'}  \frac{q^\psi(\nitem - 1, B)}{\sum_{B" \leq B} \psi_\nitem(B")}
\end{align}
By strong induction, it is straightforward to show that indeed $q^\psi(\nitem, B) = \prob_{\bm{B} \sim \psi}(s_\nitem = B_\nitem)$. We define $\mathcal{Q}$ to be the set of all possible state occupancy measures $\bm{q}$ such that there exists a $\psi \in \Psi$ that generates $\bm{q}$. More formally,
\begin{align}
    \mathcal{Q} \equiv \{\bm{q} \in [\Nitem] \times \mathcal{B} \to [0, 1]: \exists \psi \in \Psi \text{ such that } q^\psi(\nitem, B') = \psi_\nitem(B') \sum_{B \geq B'}  \frac{q^\psi(\nitem - 1, B)}{\sum_{B" \leq B} \psi_\nitem(B")} \}
\end{align}
As such, the loss can be rewritten as:
\begin{align}
    \mathbb{E}_{\bm{B} \sim \psi}\left[ \mu^{\nround}_n(\bm{B}) \right] = \mathbb{E}_{\bm{B} \sim \psi}\left[ \sum_{\nitem=1}^\Nitem w_\nitem^\nround(B_\nitem) \right] = \sum_{\nitem=1}^\Nitem \prob_{\bm{B} \sim \psi}(s_\nitem = B_\nitem) w_\nitem^\nround(B_\nitem)
\end{align}
Substituting in our definition $q^\psi(\nitem, B) = \prob_{\bm{B} \sim \psi}(s_\nitem = B_\nitem)$, we obtain:
\begin{align}
    \mathbb{E}_{\bm{B} \sim \psi}\left[ \mu^{\nround}_n(\bm{B}) \right] = \sum_{\nitem=1}^\Nitem \sum_{B \in \mathcal{B}} q^\nround(\nitem, B_\nitem) w_\nitem^\nround(B_\nitem) = \langle \bm{q}^\nround, \bm{w}^\nround\rangle
\end{align}
Where $\bm{w}^\nround = \{w_\nitem^\nround(B)\}_{\nitem \in [\Nitem], B \in \mathcal{B}}$ which is different as in the previous section where it was defined additionally over $B' \leq B$. Following a similar argument, we obtain:
\begin{align}
    \textsc{Regret}_\mathcal{B}^{\Nround} \leq \max_{\bm{q} \in \mathcal{Q}} \sum_{\nround=1}^\Nround \langle  \bm{q}^\nround - \bm{q}, -\bm{w}^\nround \rangle
\end{align}
We can now repeat the same algorithm and analysis as in above, except using state occupancy measures $\bm{q}$ as opposed to state-action occupancy measures $\bm{\rho}$. The primary benefit is that in the regret analysis, we can replace the summation over $(\nitem, B, B')$ with a summation over $(\nitem, B)$. This allows us to obtain sharper regret bounds of $O(\Nitem \sqrt{|\mathcal{B} \Nround \log |\mathcal{B}|})$. Note that in order to sample a monotone bid vector, we must first reconstruct the policy $\bm{\pi}$ corresponding to the recovered $\bm{q}$. To do this, we first compute $\bm{\psi}$ recursively corresponding to $\bm{q}$, and from this, compute $\bm{\pi}$ using $\pi((\nitem, B), B') = \frac{\psi(\nitem+1, B')}{\sum_{B" \leq B} \psi(\nitem+1, B")}$. To compute $\bm{\psi}$ from $\bm{q}$, notice that for any $\nitem$:
\begin{align}
    q(\nitem, B_0) = \psi_\nitem(B_0) \sum_{B \geq B_0} \frac{q(\nitem-1, B)}{\sum_{B" \leq B} \psi_\nitem(B")} = \psi_\nitem(B_0) \frac{q(\nitem-1, B_0)}{\sum_{B" \leq B_0} \psi_\nitem(B")} = \psi_\nitem(B_0) q(\nitem-1, B_0)
\end{align}
Where the last equality follows as $\sum_{B" \leq B_0} \psi_\nitem(B") = \sum_{B" \in \mathcal{B}} \psi_\nitem(B") = 1$. In the recursive case, we have:
\begin{align}
    q(\nitem, B') = \psi_\nitem(B')\sum_{B \geq B'} \frac{q(\nitem-1, B)}{\sum_{B" \leq B} \psi_\nitem(B")} = \psi_\nitem(B')\sum_{B \geq B'} \frac{q(\nitem-1, B)}{1 - \sum_{B" > B} \psi_\nitem(B")}
\end{align}
Hence, we can solve for $\psi_\nitem(B') = q(\nitem, B')\left(\sum_{B \geq B'} \frac{q(\nitem-1, B)}{1 - \sum_{B" > B} \psi_\nitem(B")}\right)^{-1} $ in terms of $\bm{q}$ (which is known) and $\psi_\nitem(B")$ for $B" \geq B'$, which is known from induction. A more serious complication is that the projection step is now more involved, requiring the proportional policy constraints. What we can do is first convert $\bm{\psi}^\nround$ into an equivalent $\bm{\pi}^\nround$ and $\bm{\rho}^\nround$. We then compute the maximizer w.r.t. all state-action occupancy measures in $\bm{\rho} \in \Delta(\Pi)$ with the additional constraint that $\bm{\rho}$ must have a corresponding $\bm{\pi}^\rho \in \Pi'$. As there are only polynomially many $O(\Nitem |\mathcal{B}|)$ constraints on $\pi$ required for proportionality, and that there are polynomially many $O(\Nitem |\mathcal{B}|)$ constraints required for some arbitrary $\rho \in \mathcal{S} \times \mathcal{A} \to [0, 1]$ to have a corresponding generating policy, then the projection step can still be solved efficiently. \rigel{Big claim, can't prove easily} Once we have computed $\bm{\rho}^{\nround+1}$, we translate this back into $\bm{q}^{\nround+1}$ and the corresponding $\bm{\psi}^{\nround + 1}$. 


\begin{algorithm}[t]
	\KwIn{Valuation $\bm{v} \in [0, 1]^\Nitem$, Learning rate $\eta > 0$, Adaptive Adversarial Environment $\textsc{Env}^\nround: \mathcal{H}^\nround \to \mathcal{B}^{-\Nitem} \times \mathcal{B}$ where $\mathcal{H}^\nround$ denotes the set of all possible historical auction results $H^\nround$ up to round $\nround$ for all $\nround \in [\Nround]$.}
	\KwOut{The aggregate utility $\sum_{\nround=1}^\Nround \mu(\bm{b}^\nround)$ corresponding to a sequence of bid vectors $\bm{b}^{1},\ldots,\bm{b}^{\Nround}$ sampled according to $\textsc{Node O-REPS}$.}
	$\psi^0(\nitem, B) \gets \frac{1}{|\mathcal{B}|}$ for all $\nitem \in [\Nitem], B \in \mathcal{B}$. Let $\bm{q}^0$ be the corresponding state-action occupancy measure \;
        $H^0 \gets \emptyset$ \;
	\For{$\nround \in [\Nround]$:}{
            $(\bm{b}^{\nround}_-, \pi^\nround) \gets \textsc{Env}^{\nround-1}(H^{\nround-1})$ and $\bm{b}^{\nround} \sim \bm{\pi}^{\nround-1}$\;
            Observe $\bm{b}^{\nround}_-, \pi^\nround$ and receive reward $\mu^\nround_n(\bm{b}^\nround)$\;
            $\hat{w}_\nitem^\nround(B') \gets \frac{w_\nitem^\nround(B')}{q^{\nround-1}(\nitem, B')} \textbf{1}_{B' = b^{\nround}_\nitem}$ for all $\nitem \in [\Nitem]$ and $B' \in \mathcal{B}$\;
            $\bm{q}^\nround \gets \text{argmin}_{\bm{q} \in \mathcal{Q}} \eta\langle \bm{q}, -\hat{\bm{w}}^\nround\rangle + D(\bm{q} || \bm{q}^{\nround-1})$ \;
            Recursively compute for all $\nitem$, $B$ in decreasing order $\psi^\nround_\nitem(B') \gets q^\nround(\nitem, B')\left(\sum_{B \geq B'} \frac{q^\nround(\nitem-1, B)}{1 - \sum_{B" > B} \psi^\nround_\nitem(B")}\right)^{-1}$ \;
            Compute $\pi^\nround((\nitem, B), B') \gets \frac{\psi^\nround(\nitem+1, B')}{\sum_{B" \leq B} \psi^\nround(\nitem+1, B")}$ for all $\nitem \in [\Nitem], B \geq B' \in \mathcal{B}$\;
        }
        \textbf{Return} $\sum_{\nround=1}^\Nround \mu(\bm{b}^\nround)$
	\caption{\textsc{Node O-REPS}}
	\label{alg: Node O-REPS}
\end{algorithm}

\begin{theorem}
    Under bandit feedback, Algorithm~\ref{alg: Node O-REPS} achieves regret rate $\textsc{Regret}_\mathcal{B}^{\Nround} \lesssim O(\Nitem \sqrt{|\mathcal{B}| \Nround \log |\mathcal{B}|})$ using $\eta = \sqrt{\frac{\log |\mathcal{B}|}{|\mathcal{B}|\Nround}}$.
\end{theorem}

\begin{proof}
    The justification follows that of Algorithm~\ref{alg: O-REPS} and only changes at line~\ref{eq: node diff}, where we instead have:
    \begin{align}
        \textsc{Regret}_\mathcal{B}^{\Nround} \leq \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{B \in \mathcal{B}} q^{\nround}(\nitem, B') \hat{w}^{\nround}_\nitem(B)^2 + \eta^{-1}D(\bm{q} || \bm{q}^{1}) \right]
    \end{align}
    Similarly, bounding the entropy term $D(\bm{q} || \bm{q}^1)$ only requires taking a sum over $(\nitem, B)$ pairs rather than $(\nitem, B, B')$. Nonetheless, since the entropy bound is logarithmic in the distribution size, we achieve the same bound of $O(\Nitem |\mathcal{B}|)$. Plugging this back in, we obtain:
    \begin{align}
        \textsc{Regret}_\mathcal{B}^{\Nround} \leq \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{B \in \mathcal{B}} q^{\nround}(\nitem, B') \hat{w}^{\nround}_\nitem(B)^2 + \eta^{-1}\Nitem |\mathcal{B}| \right]
    \end{align}
    We can upper bound the first term with $\eta \Nround \Nitem |\mathcal{B}|$ in the bandit setting and $\eta \Nround \Nitem $ in the full information setting using the same arguments as in $\textsc{O-REPS}$. Setting $\eta = \sqrt{\frac{\log |\mathcal{B}|}{|\mathcal{B}|\Nround}}$ yields the desired regret bound in the bandit setting. Similarly, setting $\eta = \sqrt{\frac{\log |\mathcal{B}|}{\Nround}}$ in the full information setting yields regret rate $O(\Nitem\sqrt{\Nround \log|\mathcal{B}|})$.
\end{proof}



\begin{theorem}
    Under bandit feedback, Algorithm~\ref{alg: Node O-REPS} achieves regret rate $\textsc{Regret}_\mathcal{B}^{\Nround} \lesssim O(\Nitem \sqrt{|\mathcal{B}| \Nround \log |\mathcal{B}|})$ using $\eta = \sqrt{\frac{\log |\mathcal{B}|}{|\mathcal{B}|\Nround}}$.
\end{theorem}

