\section{Preliminaries}\label{sec:model}

\textbf{Notation.} We let $|\mathcal{S}|$ denote the size of set $\mathcal{S}$, and define $[k] = \{1,\ldots,k\}$ to be the set of the first $k$ positive integers. We define $\mathcal{S}^{+k}$ to be the set of non-increasing $k$-vectors of elements from set $\mathcal{S}$. Similarly, $\mathcal{S}^{-k}$ denotes the set of non-decreasing $k$-vectors of elements from set $\mathcal{S}$. We let $\Delta(\mathcal{S})$ denote the set of valid probability measures over set $\mathcal{S}$. We also say that the quantity $x$ is $\lesssim$, $\propto$, or $\gtrsim$ than $f(a_1,\ldots,a_k)$ some function of $k$ algorithm parameters if $x \in \mathcal{O}(f(a_1,\ldots,a_k))$, $x \in \Theta(f(a_1,\ldots,a_k))$, or $x \in \Omega(f(a_1,\ldots,a_k))$,  respectively.



\textbf{Auction format: Pay-as-bid.} 
 Consider a scenario in which there are $N$ bidders and $\overline{M}$ identical units available for auction in a PAB  format. In this context, we assume that each agent $n$ within the set $[N]$ desires a maximum of $M$ units. (It should be noted that our results can be extended to a situation where each agent $n$ may demand a different maximum number of units, denoted by $M_n$, which are not necessarily identical.)
 
 Let $\bm{v}_n = (v_{n, \nitem})_{\nitem \in [M]} \in [0, 1]^{+M}$ represent agent $n$'s non-increasing marginal valuation profile. This implies that for any given $n$ in the set $[N]$, the following conditions hold: (i) {valuation monotonicity} $v_{n,1}\ge v_{n, 2}\ge \ldots \ge v_{n, \Nitem}$ and (ii) the total valuation of agent $n$ after receiving $\nitem$ units is given by $\sum_{k=1}^{m}v_{n, k}$.

 Let $\bm{b}_n = \{b_{n, \nitem}\}_{\nitem \in [\Nitem]} \in [0, 1]^{+\Nitem}$  represent the non-increasing bids submitted by bidder $n$. Here, $b_{n, \nitem}$ refers to the bid made by bidder $n$ for the $\nitem$-th slot or, equivalently, the $\nitem$-th unit. Similar to $\bm{v}_n$, we have the following conditions for any $n \in [N]$: (i) {bid monotonicity}
$b_{n,1}\ge b_{n, 2}\ge \ldots \ge b_{n, \Nitem}$ and  (ii) individual rationality (IR) $b_{n, \nitem} \leq v_{n, \nitem}$ for all $m \in [M]$. 
It is important to note that the bid monotonicity condition is not an assumptions; it is implied by the auction rule that will be stated shortly. Consequently, the total payment made by bidder $n$ after receiving $\nitem$ units is given by $\sum_{k=1}^{m} b_{n, k}$. We define $\bm{b}_{-n} = (b_{-n,\nitem})_{\nitem \in [\overline{\Nitem}]} \in [0, 1]^{-\overline{M}}$  as the set of the $\overline{\Nitem}$ largest bids not belonging to agent $n$, arranged in increasing order (i.e., $b_{-n, 1} \leq \ldots \leq b_{-n, \overline{\Nitem}}$).



 The auction operates according to the following rules:
 In a PAB auction, all bids submitted across the $N$ bidders are arranged in descending order. The $\nitem$-th unit is assigned to the bidder with the $\nitem$-th highest bid, and they are charged the amount of their bid. We denote the allocation to agent $n$ as $x_n(\bm{b}_n) = x(\bm{b}_{n}, \bm{b}_{-n})$, and the (quasi-linear) utility as $\mu_n(\bm{b}_n) = \mu(\bm{b}_{n}, \bm{b}_{-n})$, where
\begin{align}
    \label{eq: allocation and utility definition}
    x(\bm{b}_{n}, \bm{b}_{-n}) = \sum_{\nitem=1}^\Nitem \textbf{1}_{b_{n, \nitem} \geq b_{-n, \nitem}} \quad \text{and} \quad \mu(\bm{b}_{n}, \bm{b}_{-n}) = \sum_{\nitem=1}^{x_n(\bm{b}_n)} (v_{n, \nitem} - b_{n, \nitem})\,.
\end{align}
respectively. Here, $b_{-n, \nitem}$ represents the $\nitem$-th smallest bid among the $\overline{M}$ largest bids of all other bidders except bidder $n$.
It should be noted that $x(\bm{b}_{n}, \bm{b}_{-n})$ denotes the number of units that agent $n$ receives in the auction. In the case of tied bids, we assume the use of an arbitrary, publicly known deterministic tie-breaking rule, denoted as $\textsc{TieBreak}_n: \mathcal{R}^{+\Nitem} \times \mathcal{R}^{-\overline{\Nitem}} \to [\Nitem]$, to determine the allocation for agent $n$. This tie-breaking rule is incorporated into the allocation as $x(\bm{b}_n, \bm{b}_{-n}) = \sum_{\nitem=1}^\Nitem \textbf{1}_{b_{n, \nitem} > b_{-n, \nitem}} + \textsc{TieBreak}_n(\bm{b}_n, \bm{b}_{-n})$, and we use this shorthand notation in Equation \eqref{eq: allocation and utility definition}.\medskip




\textbf{Online/Repeated Setting.} Consider a repeated setting where the PAB auction is conducted over $\Nround$ rounds. In this repeated setting, we will focus on the perspective of agent $n \in [N]$ and remove additional indexing when it is evident from the context. For each auction round, agent $n$ has a fixed valuation profile represented by 
$\bm{v} = (v)_{\nitem \in [\Nitem]} \in [0, 1]^{+\Nitem}$, and their
 bid vector in the $\nround$-th auction is denoted by $\bm{b}^\nround = (b^\nround_\nitem)_{\nitem \in [\Nitem]} \in [0, 1]^{+\Nitem}$.\footnote{In Section \ref{sec: time varying}, we extend our results to the case of time-varying valuations.} 
Similarly, $\bm{b}^\nround_{-} = (b^\nround_{-m})_{m \in [\overline{\Nitem}]} \in [0, 1]^{-\overline{\Nitem}}$ represents the competing bids in round $\nround$. In each round, agent $n$ receives $x^\nround_n(\bm{b}^\nround)$ units and earns a utility of $\mu^\nround_n(\bm{b}^\nround)$, where:
\begin{align}
    \label{eq: allocation and utility definition repeated}
    x^\nround_n(\bm{b}^\nround) = x(\bm{b}^\nround, \bm{b}^\nround_{-}) \quad \text{and} \quad \mu^\nround_n(\bm{b}^\nround) = \mu(\bm{b}^\nround, \bm{b}^\nround_{-})\,.
\end{align}
Recall that functions $x$ and $\mu$ are defined in Equation \eqref{eq: allocation and utility definition}. 
The goal of agent $n$ is to choose a sequence of bid vectors $(\bm{b}^\nround)_{\nround \in [\Nround]}$ that maximizes their total utility, given by $\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)$. However, the main challenge is that the vectors $\bm{b}^{\nround}_{-}$, representing the competing bids, are not known in advance. Instead, they are revealed in an online manner. Consequently, agents must learn how to bid optimally throughout the sequence of auctions, taking into account their previous allocations $H^{\nround-1} = (x^\tau(\bm{b}^\tau))_{\tau \in [\nround-1]}$ and their valuation profile $\bm{v}$. The performance of an agent's learning strategy is evaluated in terms of regret, which quantifies the difference between their expected utility using their learning strategy and the optimal utility achievable with perfect knowledge of the competing  bidding vectors in hindsight:
\begin{align}
\tag{Continuous Regret}
    \textsc{Regret} = \max_{\bm{b} \in [0, 1]^{+\Nitem}} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}) - \mathbb{E}\left[\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)\right]\,.
\end{align}
Here, $\bm{b}^{\nround}_-$ can be selected by an adaptive adversary; i.e. an adversary who can select $\bm{b}^\nround_-$ as a function of the entire auction history, which includes $\bm{b}^1,\ldots,\bm{b}^{\nround-1}$, but does not have access to the possible randomness when selecting $\bm{b}^\nround$. One example is when the other competitors are also behaving according to no-regret learning algorithms. 
We note that it is known that the time averaged iterates in the game dynamics induced by agents running no-regret learning algorithms converges to a coarse correlated equilibrium (CCE), which have desirable revenue and welfare guarantees via smooth-auction PoA analysis \citep{inefficiency2013, feldman2017correlated}. As few theoretical results are known regarding the structure of CCE's in PAB auctions, we hope our proposed algorithms and experiments will provide useful insights in this direction. We will discuss this further in Section~\ref{sec: experiments}.



The benchmark, $\max_{\bm{b} \in [0, 1]^{\Nitem}} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b})$ used in the definition of continuous regret, is constructed considering all possible $\bm{b}^{\nround}_{-}$ for every round $\nround \in [\Nround]$, where in the hindsight optimal solution, the bid vector can be chosen from any vector $\bm{b} \in [0, 1]^{+\Nitem}$. However, in practice, bid vectors are often restricted to a discretization of $[0, 1]$ denoted by $\mathcal{B} = (B_1,\ldots,B_{|\mathcal{B}|})$, where $0 = B_1 < \ldots < B_{|\mathcal{B}|} = 1$. For such cases, we define an analogous version of regret:
\begin{align}
\tag{Discretized Regret}
    \textsc{Regret}_\mathcal{B} = \max_{\bm{b} \in \mathcal{B}^{+\Nitem}} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}) - \mathbb{E}\left[\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)\right]\,.
\end{align}
In both the definitions of continuous and discretized regret, we henceforth implicitly assume that $b_m\le v_m$ for any $m\in [M]$; that is, we have  the bid vector $\bm{b}$ is subject to individual rationality and overbidding is not allowed. Observe that the benchmark in defining $\textsc{Regret}_\mathcal{B}$ is weaker than that in $\textsc{Regret}$. Nevertheless, they are not too far from each other as we discuss in the following sections.  We wish to derive a learning algorithm that achieves an upper bound on $\textsc{Regret}$ that is polynomial  in $\Nitem$ and sub-linear in $\Nround$. To do so,  we consider the discretized setting, and bound $\textsc{Regret}_\mathcal{B}$; an upper bound on $\textsc{Regret}$ will be obtained by accounting for the discretization errors. 

We consider two feedback structures: (i) full information and (ii) bandit. In the full information setting, the agent's allocation and the values of $\bm{b}^\nround_-$ are revealed after the end of each round, whereas in the bandit setting, only the agent's allocation is revealed. 

\section{Hindsight Optimal Offline Solution}\label{sec:offline}


In the offline setting, our goal is to determine agent $n$'s optimal fixed bidding strategy for the $\Nround$ rounds of PAB auctions. Recall the following optimization problem with bid space $\mathcal{B}$:
\begin{align}
\tag{Offline}
\label{eq:offline}
\max_{\bm{b} \in \mathcal{B}^{+\Nitem}} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b})\,,
\end{align}
where $\mu^\nround_n(\bm{b})$ (defined in Equation \eqref{eq: allocation and utility definition repeated}) represents the utility of agent $n$ in round $\nround$ given bid vector $\bm{b}$ and competing bids $\bm{b}_-^t$. As mentioned earlier, the solution to this optimization problem serves as a benchmark for evaluating the performance of online learning algorithms in the repeated setting. Furthermore, it provides valuable insights for designing algorithms with polynomial time and space complexity for the repeated setting.

  

To solve  Problem \eqref{eq:offline}, we take advantage of the following decomposition: 
\begin{align}
    \label{eq: decomposition}
    \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}) &= \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem (v_\nitem - b_\nitem)\textbf{1}_{b_\nitem \geq b_{-\nitem}^t} = \sum_{\nitem=1}^\Nitem \sum_{\nround=1}^\Nround (v_\nitem - b_\nitem)\textbf{1}_{b_\nitem \geq b_{-\nitem}^t}\\ &:= \sum_{\nitem=1}^\Nitem \sum_{\nround=1}^\Nround w_\nitem^\nround(b_\nitem) := \sum_{\nitem=1}^\Nitem W^{\Nround+1}_\nitem(b_\nitem)\,,
    \label{def: def mu w W}
\end{align}
where $w^\nround_\nitem(b) = \textbf{1}_{b \geq b^{\nround}_{-\nitem}} (v_\nitem - b)$ represents the utility in the $\nround$-th auction for winning the $\nitem$-th item with bid $b$, and $W^{\Nround+1}_\nitem(b) = \sum_{\nround=1}^\Nround w^{\nround}_\nitem(b)$ represents the cumulative utility gained from winning the $\nitem$-th item with bid $b$ across the $\Nround$ auctions. {(Here, in $w^\nround_\nitem(b)$, the same tie-breaking rule in Equation \eqref{eq: allocation and utility definition} is applied)}. To solve Problem \eqref{eq:offline}, we develop a polynomial-time DP scheme utilizing these $w^\nround_\nitem(b)$ and $W^{\Nround+1}_\nitem(b)$. In particular, for any $\nitem \in [\Nitem]$ and any bid $b \in \mathcal B$, let $U_\nitem(b)$ be the optimal cumulative utility of the agents from units $\nitem, \nitem+1, \ldots, \Nitem$ over $\Nround$ auctions assuming that bids for unit $\nitem$ is less than or equal to $b$.


We then have
\begin{align}
    U_\nitem(b) = \max_{b'\le b, b'\in \mathcal B}\left\{ W_{\nitem}^{\Nround+1} (b') + U_{\nitem+1}(b')\right\} \quad b\in \mathcal B, m\in [M] \quad \text{and} \quad U_{\Nitem+1}(b) = 0 \quad b \in \mathcal{B}\,.
\end{align}
Algorithm \ref{alg: Offline Full} uses the aforementioned  DP scheme to devise an optimal solution to Problem \eqref{eq:offline}. The following theorem, proven in Section~\ref{sec: offline proof}, shows the optimality of Algorithm \ref{alg: Offline Full}. 

\begin{theorem}\label{thm:offline}
Algorithm~\ref{alg: Offline Full} returns the optimal solution to problem \eqref{eq:offline} {with time and space complexity of $O(\Nitem |\mathcal{B}|^2)$ and $O(\Nitem|\mathcal{B}|)$ respectively.}
\end{theorem}






\begin{algorithm}[t]
\footnotesize
	\KwIn{Valuation $\bm{v}$ for $\bm{v} \in [0, 1]^{+\Nitem}$, Other bids $\{\bm{b}^{\nround}_-\}_{\nround \in [\Nround]}$ for $\bm{b}^{\nround}_- \in \mathcal{B}^{-\overline{\Nitem}}$.}
	\KwOut{Optimal bid vector $\bm{b}^* = \text{argmax}_{\bm{b} \in \mathcal{B}^\Nitem} \mu^{ \Nround}_n(\bm{b})$ and its corresponding utility.}
	Let $W^{\Nround+1}_\nitem(b) \gets \sum_{\nround=1}^\Nround \textbf{1}_{b \geq b^{\nround}_{-\nitem}} (v_\nitem - b)$,  $b \in \mathcal{B}, \nitem \in [\Nitem]$,   define 
        $U_{\Nitem+1}(b) \gets 0$, $b \in \mathcal{B}$, and set $b^*_{0} = \max(\mathcal B)$\;
        \textbf{for} $m \in [M,\ldots,1], b \in \mathcal{B}: U_\nitem(b) \gets \text{max}_{b' \in \mathcal{B}; b' \leq b}W^{\Nround+1}_\nitem(b') + U_{\nitem+1}(b')$\;
        \textbf{for} $m \in [1,\ldots,M]: b_m^\star \gets \arg\max_{b \le b_{m-1}^*} U_\nitem(b)$\;
        \textbf{Return} $U_1(\max(\mathcal{B}))$ and $\bm{b}^{*} =(b_{m}^*)_{m\in [M]}$.
	\caption{\textsc{Offline}$(\bm{v}, \{\bm{b}^{\nround}_-\}_{\nround \in [\Nround]})$ \label{alg: Offline Full}}	
\end{algorithm}


This algorithm to solve the offline bid optimization problem enables us to compute the hindsight optimal utility, which serves as a benchmark for evaluating the effectiveness of our online learning algorithms. It is worth mentioning that we can represent our DP algorithm as an equivalent graph with $\Nitem$ layers, with $|\mathcal{B}|$ nodes in each. More precisely, we define the (offline) DP graph as follows:
\begin{enumerate}
    \item \textbf{DP nodes/states.} There are $\Nitem$ layers, each with    $|\mathcal{B}|$ nodes in each, denoted by $\{(\nitem, b)\}_{\nitem \in [\Nitem], b \in \mathcal{B}}$. % We let $\mathcal{S}$ denote the set of all nodes/states.
    \item \textbf{DP edges.} In this graph, there are only (directed) edges between two consecutive layers, i.e., from layer $m$ to layer $m+1$ for any $m \in [M-1]$. 
    In particular, 
    node $(\nitem, b)$ only has an edge to node $(\nitem+1, b')$ for $b' \leq b$ and if $b' \leq v_{m+1}, b \leq v_m$. 
    \item \textbf{DP weights.} We define the weight of node/state $(\nitem, b)$ to be $W_{\nitem}^{T+1}(b) = \sum_{\tau=1}^{\Nround} \textbf{1}_{b \geq b^{\tau}_{-\nitem}} (v_\nitem - b)$.
\end{enumerate}

% Figure environment removed




For the online setting, we also note that we can define the DP graph at time $t$, as opposed to $T$, by setting $W_{\nitem}^t(b) = \sum_{\tau=1}^{\nround-1} \textbf{1}_{b \geq b^{\tau}_{-\nitem}} (v_\nitem - b)$. This allows us to construct algorithms for the full information and bandit settings by taking advantage of the structure of the DP graph to enhance efficiency and optimize storage of necessary computations. 








\section{Decoupled Exponential Weights Algorithms}

\label{sec: decoupled exp weights section}

In this section, we present our first algorithm for learning in the online setting. In particular, we construct a decoupled version of the Exponential Weights algorithm which circumvent the large space and time complexity of maintaining and updating the sampling distributions of all possible bid vectors. Our algorithms instead sequentially sample a singular bid value from each layer of our DP graph such that the probability of sampling a particular vector of bids $\bm{b}^\nround$ is precisely equal to the probability of the exponential weights algorithm selecting $\bm{b}^\nround$.\footnote{In Section~\ref{sec: time varying}, we present a generalization of our decoupled exponential weights algorithm to the setting with time varying valuations, where the valuations are drawn from some known, finite support distribution $F_{\bm{v}}$.} 
 

In the following sections, we provide a description of our algorithm in both the full information  and the bandit settings. It is important to note that our decoupled exponential weights algorithm achieves regret that is sub-optimal by a factor of $O(\sqrt{M})$.  Nonetheless, we present an alternative regret optimal algorithm based on OMD in a subsequent section. Despite this, our decoupled exponential weights algorithm remains practical as it does not necessitate solving a convex optimization problem at each time step $t \in [T]$.


\subsection{Full Information Setting}\label{sec:full}



Now let us focus on learning optimal bidding in an online fashion with full information feedback. One straightforward approach in this context is to apply the exponential weights algorithm \citep{DBLP:journals/iandc/LittlestoneW94} to the entire set of bid vectors. This algorithm guarantees per-round rewards within the range of $[-\Nitem, \Nitem]$. However, the challenge lies in the exponentially large bid space $\mathcal{B}^{+\Nitem}$. Tracking and updating weights for all possible bid vectors naively would lead to a non-polynomial time and space complexity. Although this approach achieves a small regret of $O(\Nitem^\frac{3}{2} \sqrt{T \log |\mathcal{B}|})$, we need a more efficient solution with a polynomial time and space complexity.

To do so, we leverage the DP scheme developed in Section \ref{sec:offline}. By utilizing the DP graph and the information it provides about bid vector utilities, we can effectively mimic the exponential weights algorithm without explicitly tracking weights for every bid vector. 
In Algorithm~\ref{alg: Decoupled Exponential Weights}, instead of 
 associating weights to each  possible bid vector, we  associate weights with each $(\nitem, b)$ pair for any $m\in [M]$ and $b\in \mathcal B$. These weights are then updated via variables $S_m^t(b)$ for $b\in \mathcal B, m\in[M]$, which are inspired by the DP scheme. For any round $t$, we define 
 \begin{align*} S^t_\nitem(b) &= \exp(\eta W_\nitem^\nround(b)) \sum_{\bm{b}'_{\nitem+1:\Nitem} \in \mathcal{B}^{+(\Nitem-\nitem)}, b'_{\nitem+1} \leq b'_\nitem = b} \exp(\eta \sum_{\nitem'=\nitem+1}^\Nitem W_{\nitem'}^\nround(b'_{\nitem'}))\\
 &= \exp(\eta W_\nitem^\nround(b)) \sum_{b' \in \mathcal{B}; b' \leq b} S_{\nitem+1}(b')\,.\end{align*}
 Here, $\eta> 0$ is the learning rate of the algorithm and we  recall that  $W_m^t(b)= \sum_{\tau=1}^{t-1} w^{\tau}_\nitem(b)$ is  cumulative utility gained across the first $t-1$ auctions from the winning the $\nitem$'th item with bid $b$, respectively. Computing $S_m^t(\cdot)$ is done in step $\textsc{Compute}-S_\nitem$ of the algorithm.  In step $\textsc{Sample}-\bm{b}$, the bid vector is then sampled  according to $S_m^t$'s subject to bid monotonicity.  To disallow overbidding, we initialize weights  $W^0_m(b) = -\infty$ for all $m \in [M], b \in \mathcal{B}$ such that $b > v_m$.
 
 

The concept of utility decoupling shares similarities with solutions used in combinatorial bandits, tabular reinforcement learning \citep{CMAB2013, OREPS2013}, and problems such as shortest path algorithms involving weight pushing or path kernels \citep{PathKernel2003, Hedging2010}. These methods are employed to solve variants of the shortest path or maximum weight path problems, where costs or weights are associated with edges rather than nodes. By exploiting the graph structure and the linearity of utilities with respect to the weights of each edge, these algorithms efficiently compute path weights based on edge weights, similar to how our algorithm computes path weights based on node weights. In addition to investigating a fundamentally different problem,   the key distinction is that our approach  considers weights associated with nodes instead of edges. In our setting, the reward associated with selecting bid $b'$ in slot $\nitem+1$ is independent of selecting bid $b \geq b'$ in slot $\nitem$. This allows us to get an improved regret bound and save a factor of $|\mathcal{B}|$ in terms of time and space complexity. Instead of storing and updating weights for $O(\Nitem |\mathcal{B}|^2)$ possible $(\nitem, b, b')$ slot-value-next value triplets, we only need to handle $O(\Nitem |\mathcal{B}|)$ possible $(\nitem, b)$ unit-bid pairs.

The following statement is the main result of this section. 

\begin{theorem}[Decoupled Exponential Weights: Full Information] \label{thm:full}
    With $\eta = \Theta(\sqrt{\frac{\log |\mathcal{B}|}{MT}})$, Algorithm \ref{alg: Decoupled Exponential Weights} achieves (discretized) regret $O(\Nitem^\frac{3}{2} \sqrt{ \Nround \log |\mathcal{B}|})$, with total time and space complexity polynomial in $\Nitem$, $|\mathcal{B}|$, and $\Nround$. Optimizing for discretization error from restricting the bid space to $\mathcal{B}$, we obtain a continuous regret of $O(\Nitem^\frac{3}{2} \sqrt{\Nround \log \Nround})$.
\end{theorem}
%\proof{}
%See  Section \ref{sec: path kernels regret}.
%\endproof

%It is worth noting that in both the discrete and continuous worst-case regret, the scaling with respect to $\Nitem$ is linear. This implies that the per-unit regret remains constant regardless of the number of items. 
 It is worth noting that both the time and space complexity exhibit polynomial scaling with $\Nitem$ and $|\mathcal{B}|$. Given that $\Nitem$ can be large in practical scenarios, such as carbon emissions license auctions or electricity markets, it becomes crucial to minimize the dependence on $\Nitem$.  




 \begin{algorithm}[t]
 \footnotesize
	\KwIn{Learning rate $0<\eta < \frac{1}{M}$, $\bm{v} \in [0, 1]^{+\Nitem}$.}
	\KwOut{The aggregate utility $\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b}^{\nround})$}
	$W_\nitem^0(b) \gets 0$ for all $\nitem \in [\Nitem], b \in \mathcal{B}$ such that $b \leq v_m$; else $W_\nitem^0(b) \gets -\infty$\;
        $b_{0}^t \gets \max \mathcal B$, and $S_{M+1}^t (\min \mathcal{B})=1$ for any $t\in[T]$\;
 
	\For{$\nround \in [1,\ldots,\Nround]$:}{
            %Adversary selects $\bm{b}^{\nround}_-$.
            \textbf{Recursively Computing Exponentially Weighted Partial Utilities $\bm{S}^t$}\;
            \textbf{for} $m \in [M,\ldots,1], b \in \mathcal{B}: S^t_\nitem(b) \gets \exp(\eta W_\nitem^\nround(b)) \sum_{b' \leq b} S_{\nitem + 1}^\nround(b')$. \label{eq: compute s} \hspace{0mm} $\backslash \backslash$ $\textsc{Compute}-S_\nitem$\;
        \textbf{Determining the Bid Vector $\bm{b}^\nround$ Recursively}\;
        \textbf{for} $m \in [1,\ldots,M], b \leq b_{m-1}^t: b_\nitem^\nround \gets b$ with probability $ \frac{S^t_\nitem(b)}{\sum_{b' \leq b_{\nitem-1}^t} S^t_{\nitem}(b')};$ \hspace{1mm} $\backslash \backslash$ $\textsc{Sample}-\bm{b}$\;
            %$\bm{b}^{\nround} \gets \textsc{Sampler}(\bm{W}^{\nround-1}, \eta)$\;
            Observe $\bm{b}^{\nround}_-$ and receive reward $\mu_n^\nround(\bm{b}^{\nround})$\;
        \textbf{Update Weight Estimates} \;
        \textbf{for} $\nitem \in [\Nitem], b \in \mathcal{B}: W^{\nround+1}_{\nitem}(b) \gets W^{\nround}_{\nitem}(b) +  (v_\nitem - b)\textbf{1}_{b \geq b^t_{-m}}$ if $b \leq v_m$; else $W^{\nround+1}_{\nitem}(b) \gets -\infty$\;
        }
        
        
        \textbf{Return} $\sum_{\nround=1}^\Nround \mu_n^{\nround}(\bm{b}^{\nround})$
	\caption{\textsc{Decoupled Exponential Weights - Full Information}}
	\label{alg: Decoupled Exponential Weights}
\end{algorithm}



\subsection{Bandit Feedback Setting}

\label{sec: decoupled exp weights, bandit feedback}


We extend Algorithm~\ref{alg: Decoupled Exponential Weights} for the bandit feedback setting. In the bandit feedback setting, the bidder's allocation and utility are not available for all possible bid vectors, unlike in the full information setting. Instead, the agent only observes their utility for the submitted bid vector. To handle this, we use inverse probability weighted (IPW) node weight estimates $\widehat{w}^t_m(b)$ instead of the node weights $w^t_m(b)$ in Algorithm~\ref{alg: Decoupled Exponential Weights}. This adaptation results in a regret of $O(M^{\frac{3}{2}}\sqrt{|\mathcal{B}| T \log |\mathcal{B}|})$, as shown in Theorem~\ref{thm:decoupled exp - bandit feedback}. This regret includes an additional factor of $\sqrt{|\mathcal{B}|}$ compared to the full information setting. 


The structure of Algorithm \ref{alg: Decoupled Exponential Weights - Path Kernels} is similar to that of Algorithm~\ref{alg: Decoupled Exponential Weights}. Both algorithms maintain node weight estimates, 
%, set $\widehat{W}^t_m(b) = -\infty$ for all $m \in [M], b \in \mathcal{B}$ such that $b > v_m$ to prevent overbidding,
compute the sum of exponentiated partial bid vector estimated utilities recursively, and sample bids for each unit recursively proportional to these summed exponentiated utilities. %The second and third steps are identical between the two algorithms. 
Specifically, Algorithm \ref{alg: Decoupled Exponential Weights - Path Kernels} samples bid vectors with probabilities proportional to the sum of the cumulative \textit{estimated} utility $\widehat{W}_{\nitem}^\nround(b) = \sum_{\tau=1}^{t-1} \widehat{w}_{\nitem}^\tau(b)$ over each unit-bid value pair, where   $\widehat{w}_{m}^\nround(b) = 1 - \frac{1 - (v_m - b)\textbf{1}_{b \geq b_{-m}^{\nround}}}{q_m^\nround(b)}\textbf{1}_{b_m^\nround = b}$ and $q_m^\nround(b)$ is the (unconditional) probability that bid $b$ is chosen for unit $m$ at time $t$.

Note that this bid vector utility estimator is a slightly different estimator {than the one} used in the standard $\textsc{Exp3}$ algorithm (See Chapter 11 of \cite{Lattimore2020}). In particular, the standard IPW estimator $\widehat{w}_m^\nround(b) = \frac{v_m-b}{q_m^\nround(b)} \textbf{1}_{b = b_m^t > b_{-m}^t}$, while unbiased, can be unboundedly large when $q_m^\nround(b)$ approaches 0, whereas our proposed estimator is bounded above by 1. As a consequence, we have that $\widehat{\mu}_n^\nround(\bm{b})$ is upper bounded by $M$, and therefore $ =\eta \widehat{\mu}^\nround(\bm{b}) = \eta \sum_{m=1}^M \widehat{w}_m^\nround(b)$ is upper bounded by 1 for $\eta < \frac{1}{M}$, which we crucially use in the proof. 

 \begin{algorithm}[t]
 \footnotesize
	\KwIn{Learning rate $0 < \eta < \frac{1}{M}$, $\bm{v} \in [0, 1]^{+\Nitem}$}
	\KwOut{The aggregate utility $\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b}^{\nround})$}
	$\widehat{W}_\nitem^0(b) \gets 0$ for all $\nitem \in [\Nitem], b \in \mathcal{B}$ such that $b \leq v_\nitem$; else $\widehat{W}_\nitem^0(b) \gets -\infty$.\;
        $b_{0}^t \gets \max \mathcal B$, and $\widehat{S}_{M+1}^t (\min \mathcal{B})=1$ for any $t\in[T]$\;
 
	\For{$\nround \in [1,\ldots,\Nround]$:}{
            \textbf{Recursively Computing Exponentially Weighted Partial Utilities $\bm{S}^t$}\;
            \textbf{for} $m \in [M,\ldots,1], b \in \mathcal{B}: \widehat{S}^t_\nitem(b) \gets \exp(\eta \widehat{W}_\nitem^\nround(b)) \sum_{b' \leq b} \widehat{S}_{\nitem + 1}^\nround(b')$ \hspace{0mm} $\backslash \backslash$ $\textsc{Compute}-\widehat{S}_\nitem$\;
        \textbf{Determining the Bid Vector $\bm{b}^\nround$ Recursively}\;
        \textbf{for} $m \in [1,\ldots,M], b \leq b_{m-1}^t: b_\nitem^\nround \gets b$ with probability $\frac{\widehat{S}^t_\nitem(b)}{\sum_{b' \leq b_{\nitem-1}^t} \widehat{S}^t_{\nitem}(b')}; $ \hspace{1mm} $\backslash \backslash$ $\textsc{Sample}-\bm{b}$\;
        Observe $\bm{b}^{\nround}_-$ and receive reward $\mu_n^\nround(\bm{b}^{\nround})$\;
        \textbf{Recursively Computing Probability Measure $\bm{q}$}\;
        $q^t_1(b) \gets \frac{\widehat{S}^\nround_m(b)}{\sum_{b' \in \mathcal{B}} \widehat{S}^\nround_m(b')}$ for all $b \in \mathcal{B}$\;
        \textbf{for} $m \in [2,\ldots,M], b \in \mathcal{B}: q_\nitem^\nround(b) \gets \sum_{b' \geq b} \frac{q_{\nitem-1}^t(b')\widehat{S}^\nround_{\nitem}(b)}{\sum_{b" \geq b'} \widehat{S}^\nround_\nitem(b")}$ for all $b \in \mathcal{B}$\;
        \textbf{Update Weight Estimates}\;
        \textbf{for} $m \in [M], b \in \mathcal{B}: \widehat{W}^{\nround+1}_{\nitem}(b) \gets \widehat{W}^{\nround}_{\nitem}(b) + (1 - \frac{1 - (v_m - b)\textbf{1}_{b \geq b^t_m}}{q^t_m(b)} \textbf{1}_{b^t_m = b})$ if $b \leq v_m$; else $\widehat{W}^{\nround+1}_{\nitem}(b) \gets -\infty$\;
        }
        \textbf{Return} $\sum_{\nround=1}^\Nround \mu_n^{\nround}(\bm{b}^{\nround})$
	\caption{\textsc{Decoupled Exponential Weights - Bandit Feedback}}
	\label{alg: Decoupled Exponential Weights - Path Kernels}
\end{algorithm}


   
    
    The primary difference in the implementation of Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels} as compared to Algorithm~\ref{alg: Decoupled Exponential Weights} is that we require additional steps in order to obtain unbiased node weight estimates $\widehat{W}^{\nround+1}_{\nitem}(b) = \sum_{\tau=1}^{t} \widehat{w}_m^\tau(b)$ which we compute using an IPW estimator. In order to do this, we must compute $q_m^t(b)$---the  probabilities of selecting bid $b$ at slot $m$. 
    
   

\begin{theorem}[Decoupled Exponential Weights: Bandit Feedback] \label{thm:decoupled exp - bandit feedback}
    With $\eta = \Theta(\sqrt{\frac{\log |\mathcal{B}|}{M|\mathcal{B}|T}})$ such that $\eta < \frac{1}{M}$, Algorithm \ref{alg: Decoupled Exponential Weights - Path Kernels} achieves (discretized) regret $O(\Nitem^{\frac{3}{2}} \sqrt{ |\mathcal{{B}|}\Nround \log |\mathcal{B}|})$, with total time and space complexity polynomial in $\Nitem$, $|\mathcal{B}|$, and $\Nround$. Optimizing for discretization error from restricting the bid space to $\mathcal{B}$, we obtain a continuous regret of $O(M^{\frac{4}{3}}T^{\frac{2}{3}} \sqrt{\log \Nround})$.
\end{theorem}



