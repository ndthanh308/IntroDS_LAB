\newpage 

\section{Appendix}

%\section{}

%In the appendix, we address the missing proofs and algorithms from the main body. We first begin by completing the proof of the decoupled exponential weights algorithm (Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels}) in the bandit setting. We then move to the proof of correctness and regret analysis of the OMD algorithm (Algorithm~\ref{alg: OMD}).

\subsection{Proof of Theorem \ref{thm:offline}: Offline Bid Optimization Algorithm}
\label{sec: offline proof}

We give a proof of correctness of the offline bid optimization algorithm used to compute the hindsight optimal bid vector across $T$ rounds of PAB auctions. Our proof shows that the variables $U_m(b)$ are path weights of the optimal partial bid vector with weights $W_m^{T+1}(b)$. Thus, $U_1(b)$ is the optimal bid vector and $b^*_m$'s can be used to back out the optimal bid vector recursively in polynomial time.

\begin{proof}{Proof of Theorem \ref{thm:offline}}
   By definition, 
    \begin{align*}
        U_\nitem(b) &= \max_{ b\ge b_\nitem\geq\ldots\geq b_\Nitem} \sum_{\nitem' = \nitem}^\Nitem W^{\Nround+1}_{\nitem'}(b_{\nitem'})\\
        &= \max_{ b\ge b_\nitem\geq\ldots\geq b_\Nitem}  W^{\Nround+1}_{\nitem}(b_\nitem ) +  \sum_{\nitem' = \nitem+1}^\Nitem W^{\Nround+1}_{\nitem'}(b_{\nitem'})\\
        &= \max_{ b' \leq b} W^{\Nround+1}_\nitem(b') + U_{\nitem+1}(b')\,.
    \end{align*}
    Since we have that $U_{\Nitem}(b ) = W^{\Nround+1}_{\Nitem} (b )$ trivially correct from the base case, and the optimality of $U_\nitem(b)$ follows from induction. Consequently, optimality of $b_\nitem^*$ follows from induction. The base case trivially holds as $b_1^* = \text{argmax}_{b \in \mathcal{B}} U_1(b)$. The recursive case also follows straightforwardly by definition of $b^*_\nitem$:
    \begin{align*}
        b^*_\nitem = \text{argmax}_{b \leq b^*_{\nitem-1}} U_\nitem(b)\,.
    \end{align*}
    As $b^*_{\nitem-1}$ was optimal by the induction hypothesis, then $b_\nitem^*$ must also be optimal.
    



    
    We finish this proof by discussing the time and space complexity of Algorithm \ref{alg: Offline Full}.  {Table $U$ is of size $O(\Nitem |\mathcal{B}|)$, with each entry requiring taking a maximum over $O(|\mathcal{B}|)$ terms, yielding time and space complexities of $O(\Nitem |\mathcal{B}|^2)$ and $O(\Nitem|\mathcal{B}|)$ respectively.}

 
\end{proof}


\subsection{Proof of Theorems \ref{thm:full} and \ref{thm:decoupled exp - bandit feedback}: Decoupled Exponential Weights Algorithm}
\label{sec: path kernels regret}

\begin{proof}{}
    We give the proofs of correctness, complexity analysis, and regret analysis for the decoupled exponential weights algorithms for both the full information (Algorithm~\ref{alg: Decoupled Exponential Weights}) and the bandit setting (Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels}). Our proof comes in 5 parts. We first prove correctness of the bandit version of our algorithm. In particular, we show that defining the node and bid vector utility estimates to be $\widehat{w}_m^\nround(b) = 1 - \frac{1-(v_m-b)1_{b \geq b_{-m}^\nround}}{q_m^t(b)}1_{b = b^\nround_m}$ and $\widehat{\mu}^\nround(\bm{b}) = \sum_{\nitem=1}^\Nitem \widehat{w}_\nitem^\nround(b_m)$, our algorithm samples bid vector $\bm{b}^\nround$ with probability proportional to $\exp(\eta \sum_{\tau=1}^{t-1} \widehat{\mu}^\tau(\bm{b}))$ via the same recursive sampling procedure as in Algorithm~\ref{alg: Decoupled Exponential Weights}. In the second part and third parts, we derive a corresponding regret upper bound and obtain the time and space complexities of our algorithm with bandit feedback. In the fourth part, we optimize the continuous regret w.r.t. the selection of $\mathcal{B}$. In the fifth part, we show how to extend our algorithm and results to the full information setting.

    \textbf{Part 1: Algorithm Correctness.} In this part of the proof, we argue that our choice of estimator is unbiased and that Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels} samples bid vectors with the same probability that the exponential weights algorithm would have, given the same node utility estimates $\widehat{w}_m^t(b)$. To show unbiasedness of $\widehat{w}_m^t(b)$, we have:
    \begin{align*}
        \mathbb{E}\left[\widehat{w}_m^{t}(b)\right] = \mathbb{E}\left[1 - \frac{1-w^t_m(b)}{q^t_m(b)} \textbf{1}_{b^t_m = b}\right] = \mathbb{E}\left[1 - \frac{\textbf{1}_{b^t_m = b}}{q^t_m(b)} + \frac{\textbf{1}_{b^t_m = b} \cdot w^t_m(b)}{q^t_m(b)}\right] = w^t_m(b)\,.
    \end{align*}
    Now, it remains to show that our sampling procedure $\textsc{Sample}-\bm{b}$ w.r.t. $\widehat{S}^\nround_m$ indeed samples bid vectors $\bm{b}$ with the same probability as the exponential weights algorithm under weights $\widehat{\mu}^\nround_n(\bm{b})$. In particular, we want to show that our algorithm samples bid vectors $\bm{b}^\nround$ with probability proportional to $\exp(\eta \sum_{\tau=1}^{t-1}\widehat{\mu}_m^\tau(b_m))$ for any $m\in [M]$. This follows from analyzing the dynamic programming variables that represent the sum of exponentiated (estimated) partial bid vector utilities, $\widehat{S}$.

    In exponential weights, the bidder selects at round $\nround+1$ some action $\bm{b}$ with probability $P^\nround(\bm{b})$ proportional to $\sum_{\tau=1}^{\nround} \widehat{\mu}_n^{\nround}(b)$. Using our representation of $\widehat{\mu}_n^{\nround}(\bm{b})$ as a function $\widehat{w}_\nitem^{\tau}(b)$, we have:
    \begin{align*}
        P^{\nround}(\bm{b}) = \frac{\exp(\eta \sum_{\tau=1}^{t-1} \widehat{\mu}_n^{\tau}(\bm{b}))}{\sum_{\bm{b}' \in \mathcal{B}^{+\Nitem}} \exp(\eta \sum_{\tau=1}^{t-1} \widehat{\mu}_n^\tau(\bm{b}'))} = \frac{\exp(\eta \sum_{\nitem=1}^\Nitem \widehat{W}_\nitem^\nround(b_\nitem))}{\sum_{\bm{b}' \in \mathcal{B}^{+\Nitem}} \exp(\eta \sum_{\nitem=1}^\Nitem \widehat{W}_\nitem^\nround(b'_\nitem))}\,.
    \end{align*}
    Hence, we wish to construct a sampler that samples $\bm{b}$ with the above probability. Defining $b_0 = \max_{b \in \mathcal{B}} b$, we begin by decomposing the denominator as follows:
    \begin{align*}
        &\sum_{\bm{b} \in \mathcal{B}^{+\Nitem}} \exp(\eta \sum_{\nitem=1}^\Nitem \widehat{W}_\nitem^\nround(b_\nitem)) = \sum_{b_1 \in \mathcal{B}, b_1 \leq b_0} \sum_{b_2 \in \mathcal{B}, b_2 \leq b_1} \ldots \sum_{b_\Nitem \in \mathcal{B}, b_\Nitem \leq b_{\Nitem-1}} \exp(\eta \sum_{\nitem=1}^\Nitem \widehat{W}_\nitem^\nround(b_\nitem))\\
        &= \sum_{b_1 \in \mathcal{B}, b_1 \leq b_0} \exp(\eta \widehat{W}_1^\nround(b_1)) \sum_{b_2 \in \mathcal{B}, b_2 \leq b_1}\exp(\eta \widehat{W}_2^\nround(b_2)) \ldots \sum_{b_\Nitem \in \mathcal{B}, b_\Nitem \leq b_{\Nitem-1}} \exp(\eta \widehat{W}_\Nitem^\nround(b_\Nitem))\,.
    \end{align*}
   Recall a key object $\widehat{S}^t_\nitem(b)$, which is the sum of exponentially weighted utilities of partial bid vectors $\bm{b}'_{\nitem:\Nitem} \in \mathcal{B}^{+(\Nitem - \nitem + 1)}$ over slots $\nitem,\ldots,\Nitem$ subject to $b_\nitem = b$.
    \begin{align}
        \widehat{S}^t_\nitem(b) &= \exp(\eta \widehat{W}_\nitem^\nround(b)) \sum_{\bm{b}'_{\nitem+1:\Nitem} \in \mathcal{B}^{+(\Nitem-\nitem)}, b'_{\nitem+1} \leq b'_\nitem = b} \exp(\eta \sum_{\nitem'=\nitem+1}^\Nitem \widehat{W}_{\nitem'}^\nround(b'_{\nitem'}))\\
        &= \exp(\eta \widehat{W}_\nitem^\nround(b)) \sum_{b' \in \mathcal{B}; b' \leq b} \widehat{S}^t_{\nitem+1}(b')\,.
    \end{align}
    With the trivial base case $\widehat{S}^t_\Nitem(b) = \exp(\eta \widehat{W}_\Nitem^\nround(b))$, we can recover all of the exponentially weighted partial utilities $\{\widehat{S}^t_{\nitem}(b)\}_{\nitem \in [\Nitem], b \in \mathcal{B}}$ given $\bm{W}^\nround$. Once we have computed $\{\widehat{S}^t_{\nitem}(b)\}_{\nitem \in [\Nitem], b \in \mathcal{B}}$, we can sample $\bm{b}$ according to its exponentially weighted utility $\exp(\eta \widehat{\mu}^{\nround}_\nitem(\bm{b}))$ by sequentially sampling each $b_1,\ldots,b_\Nitem$.
    
    Let $P_{\textsc{D}}^t(\bm{b})$ be the probability that our Algorithm \ref{alg: Decoupled Exponential Weights - Path Kernels} returns bid vector $\bm{b} \in \mathcal{B}^{+\Nitem}$ in round $t$. Recall that we sample $\bm{b}$ by setting $b_\nitem^\nround$ to $b \in \mathcal B, b \le b_{m-1}^t$ with probability $ \frac{\widehat{S}^t_\nitem(b)}{\sum_{b' \leq b_{\nitem-1}^t} \widehat{S}^t_{\nitem}(b')}$. Hence, the probability of selecting $\bm{b}$ is the product of $\nitem$ conditional probability mass functions (pmf's) and we have 
    \begin{align*}
        P_{\textsc{D}}^\nround(\bm{b}) = \prod_{\nitem=1}^\Nitem \frac{\widehat{S}^t_\nitem(b_\nitem)}{\sum_{b' \leq b_{\nitem-1}} \widehat{S}^t_{\nitem}(b')} = \left(\prod_{\nitem=1}^{M-1} \frac{\exp(\eta \widehat{W}_\nitem^\nround(b_\nitem)) \sum_{b \leq b_{\nitem}} \widehat{S}^t_{\nitem+1}(b)}{\sum_{b' \leq b_{\nitem-1}} \widehat{S}^t_{\nitem}(b')}\right)\left(\frac{\exp(\eta \widehat{W}_M^t(b_M))}{\sum_{b' \leq b_{M-1}} \widehat{S}_M^t(b')}\right)\,.
    \end{align*}
    Moving the $\exp(\eta \widehat{W}_m^t(b_m))$ outside of the product, we obtain:
    \begin{align*}
        P_{\textsc{D}}^\nround(\bm{b}) &= \left(\prod_{\nitem=1}^{\Nitem-1} \exp(\eta \widehat{W}_\nitem^\nround(b_\nitem)) \right) \left(\prod_{\nitem=1}^{M-1} \frac{\sum_{b \leq b_{\nitem}} \widehat{S}^t_{\nitem+1}(b)}{\sum_{b' \leq b_{\nitem-1}} \widehat{S}^t_{\nitem}(b')}\right)\left(\frac{\exp(\eta \widehat{W}_M^t(b_M))}{\sum_{b' \leq b_{M-1}} \widehat{S}_M^t(b')}\right)\\
        &= \left(\prod_{\nitem=1}^{\Nitem} \exp(\eta \widehat{W}_\nitem^\nround(b_\nitem)) \right) \left(\frac{\sum_{b \leq b_{M-1}} \widehat{S}^t_{M}(b)}{\sum_{b' \leq b_{0}} \widehat{S}^t_{1}(b')}\right)\left(\frac{1}{\sum_{b' \leq b_{M-1}} \widehat{S}_M^t(b')}\right)\\
        &= \frac{\prod_{\nitem=1}^{\Nitem} \exp(\eta \widehat{W}_\nitem^\nround(b_\nitem))}{\sum_{b' \leq b_{0}} \widehat{S}_1^t(b')}\,.
    \end{align*}
    We now rearrange the last expression to see that our algorithm samples $\bm{b}$ with the same probability as the exponential weights algorithm: 
    \begin{align*}
        P_{\textsc{D}}^\nround(\bm{b}) = \frac{\prod_{\nitem=1}^\Nitem \exp(\eta \widehat{W}_\nitem^\nround(b_\nitem))}{\sum_{b \leq b_0} S_1^t(b)} = \frac{\exp(\eta \sum_{\nitem=1}^\Nitem \widehat{W}_\nitem^\nround(b_\nitem))}{\sum_{\bm{b}' \in \mathcal{B}^{+\Nitem}} \exp(\eta \sum_{m=1}^M \widehat{W}_m^t(b'_m))} = P^\nround(\bm{b})\,.
    \end{align*}

    \textbf{Part 2: Regret Analysis.} We are now ready to derive the regret upper bound on Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels}. First, we show that the bid vector utility estimators $\widehat{\mu}^\nround(\bm{b})$ are both unbiased and have a finite upper bound. To show the upper bound, we take expectation with respect to the bid vectors selected by our algorithm and observe that 
    \begin{align*}
        \mathbb{E}[\widehat{\mu}^\nround(\bm{b})] &= \sum_{m=1}^\Nitem \mathbb{E}[\widehat{w}_m^t(b_m)]= \sum_{m=1}^M \mathbb{E}\left[1 - \frac{1-(v_m-b_m)1_{b_m > b_{-m}^\nround}}{q_m^t(b_m)}1_{b_m = b^\nround_m}\right]
    \end{align*}
    As we are considering the expectation ex-post, keeping the $b_{-m}^t$'s fixed, we have independence between the two indicator functions and we obtain:
    \begin{align*}
        = \sum_{m=1}^M \mathbb{E}\left[1 - \frac{1-w^\nround_m(b_m)}{q_m^t(b_m)}1_{b_m = b^\nround_m}\right]= M - \sum_{m=1}^M \frac{1 - w^\nround_m(b_m)}{q_m^t(b_m)}\mathbb{E}\left[1_{b_m = b^\nround_m}\right]= \mu^\nround(\bm{b})\,.
    \end{align*}
 
    
    As for the finite upper bound, we have that $\widehat{\mu}^\nround(\bm{b}) = \sum_{m=1}^\Nitem \widehat{w}_m^t(b_m)$ is the sum over $M$ node utility estimators, each of which is upper bounded by 1. Hence, $\widehat{\mu}^\nround(\bm{b}) \leq \Nitem$ for all $\bm{b} \in \mathcal{B}^{+\Nitem}$. Now, we make the following claim:
    \begin{lemma}\label{lem:regret_bound}
        Let $\widehat{\mu}^\nround(\bm{b}) = \sum_{m=1}^M (1 - \frac{1-(v_m-b_m)1_{b_m > b_{-m}^\nround}}{q_m^t(b_m)}1_{b_m = b^\nround_m})$ be our bid vector utility estimate as discussed. Then, any algorithm which samples bid vectors $\bm{b}$ with probability proportional to $\exp(\eta \sum_{\tau=1}^{t-1} \widehat{\mu}^\nround(\bm{b}))$ at round $\nround$ for $\eta \leq \frac{1}{M}$ has regret upper bound
        \begin{align}
            \label{eq: ExpWeights Analysis 2}
            \textsc{Regret}_{\mathcal{B}} \lesssim \eta^{-1}M\log|\mathcal{B}| + \eta \sum_{\nround=1}^\Nround \sum_{\bm{b}} \prob(\bm{b}^\nround =\bm{b}) \mathbb{E}[(\sum_{m=1}^M \widehat{w}^\nround_m(b_m))^2]\,.
        \end{align}
    \end{lemma}
    \proof{Proof of Lemma \ref{lem:regret_bound}}
        We will closely follow the analysis of the $\textsc{Exp3}$ algorithm as presented in Chapter 11.4 of \cite{Lattimore2020}. In particular, we follow their regret analysis until Equation 11.13. Define $\Phi^t = \sum_{\bm{b} \in \mathcal{B}^{+\Nitem}} \exp(\eta \sum_{\tau=1}^{t} \widehat{\mu}^\tau(\bm{b}))$ to be the \textit{potential} at round $\nround$. As per our initial conditions in Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels}, we have $\widehat{\mu}^0(\bm{b}) = 0$, and consequently, $\Phi^0 = |\mathcal{B}^{+\Nitem}|$. While it is not immediately apparent how the potentials $\Phi^\nround$ relate to the regret, we begin by upper bounding $\exp(\eta \sum_{t=1}^{T} \widehat{\mu}^t(\bm{b}))$ for a fixed $\bm{b}'$:
        \begin{align}
            \label{eq: Potentials}
            \exp(\eta \sum_{t=1}^{T} \widehat{\mu}^t(\bm{b}')) \leq \sum_{\bm{b} \in \mathcal{B}^{+\Nitem}} \exp(\eta \sum_{t=1}^{T} \widehat{\mu}^t(\bm{b})) = \Phi^T = \Phi^0 \prod_{t=1}^T \frac{\Phi^t}{\Phi^{t-1}}\,.
        \end{align}
        Now, we upper bound each $\frac{\Phi^t}{\Phi^{t-1}}$:
        \begin{align*}
            \frac{\Phi^t}{\Phi^{t-1}} = \sum_{\bm{b} \in \mathcal{B}^{+M}} \frac{\exp(\eta \sum_{\tau=1}^{t} \widehat{\mu}^\tau(\bm{b}))}{\Phi^{t-1}} = \sum_{\bm{b} \in \mathcal{B}^{+M}} \frac{\exp(\eta \sum_{\tau=1}^{t-1} \widehat{\mu}^\tau(\bm{b}))}{\Phi^{t-1}} \exp(\eta \widehat{\mu}^t(\bm{b})) = \sum_{\bm{b} \in \mathcal{B}^{+M}} \prob(\bm{b}^\nround = \bm{b}) \exp(\eta \widehat{\mu}^t(\bm{b}))\,,
        \end{align*}
        where in the last equality, we used the condition that our algorithm samples bid vector $\bm{b}$ with probability proportional to $\exp(\eta \sum_{\tau=1}^{t-1} \widehat{\mu}^\nround(\bm{b}))$ at round $\nround$. In order to continue the chain of inequalities, we note that for $\eta \leq \frac{1}{M}$, we have that the quantity $\eta \widehat{\mu}^t(\bm{b})$ is upper bounded by 1 as $\eta \widehat{\mu}^t(\bm{b}) \leq \eta M \leq 1$. In the first inequality, we used the fact that $\widehat{\mu}^\nround(\bm{b}) \leq \Nitem$. Now, we can apply the inequalities $\exp(x) \leq 1 + x + x^2$ and $1 + x \leq \exp(x)$ for all $x \leq 1$, with $x= \eta \widehat{\mu}^t(\bm{b})$ and $x = \eta \prob(\bm{b}^\nround = \bm{b}) \widehat{\mu}^t(\bm{b})$, respectively,  to obtain:
        \begin{align*}
            \frac{\Phi^t}{\Phi^{t-1}} &\leq \sum_{\bm{b} \in \mathcal{B}^{+M}} \prob(\bm{b}^\nround = \bm{b}) \exp(\eta \widehat{\mu}^t(\bm{b}))\\
            &\leq \sum_{\bm{b} \in \mathcal{B}^{+M}} \prob(\bm{b}^\nround = \bm{b}) \left[1 + \eta\widehat{\mu}^t(\bm{b}) + \eta^2 \widehat{\mu}^t(\bm{b})^2 \right]\\
            &= 1+ \sum_{\bm{b} \in \mathcal{B}^{+M}} \prob(\bm{b}^\nround = \bm{b}) \left[\eta\widehat{\mu}^t(\bm{b}) + \eta^2 \widehat{\mu}^t(\bm{b})^2 \right]\\
            &\leq \exp(\sum_{\bm{b} \in \mathcal{B}^{+M}} \prob(\bm{b}^\nround = \bm{b}) \left[\eta\widehat{\mu}^t(\bm{b}) + \eta^2 \widehat{\mu}^t(\bm{b})^2 \right])\,.
        \end{align*}
        Combining this with Equation~\eqref{eq: Potentials} and then taking logarithms, we obtain:
        \begin{align*}
            \eta \sum_{t=1}^{T} \widehat{\mu}^t(\bm{b}') \leq \log \Phi^0 + \eta \sum_{t=1}^{T} \sum_{\bm{b} \in \mathcal{B}^{+M}} \prob(\bm{b}^\nround = \bm{b}) \widehat{\mu}^t(\bm{b}) + \eta^2 \sum_{t=1}^T \sum_{\bm{b} \in \mathcal{B}^{+M}}\prob(\bm{b}^\nround = \bm{b}) \widehat{\mu}^t(\bm{b})^2\,.
        \end{align*}
        Dividing both sides by $\eta$, applying the upper bound on $\Phi^0$, and rearranging, we obtain that for any $\bm{b}' \in \mathcal{B}^{+M}$:
        \begin{align*}
            \sum_{t=1}^{T} \widehat{\mu}^t(\bm{b}') - \sum_{t=1}^{T} \sum_{\bm{b} \in \mathcal{B}^{+M}} \prob(\bm{b}^\nround = \bm{b})  \widehat{\mu}^t(\bm{b}) \lesssim \eta^{-1} \Nitem \log |\mathcal{B}| + \eta \sum_{t=1}^T \sum_{\bm{b} \in \mathcal{B}^{+M}}\prob(\bm{b}^\nround = \bm{b}) \widehat{\mu}^t(\bm{b})^2\,.
        \end{align*}
        Replacing $\widehat{\mu}^\nround(\bm{b})$ with its definition in terms of $\widehat{w}^\nround_m(b_m)$ and taking expectations, we obtain the right hand side of the lemma:
        \begin{align*}
            \mathbb{E}\left[\sum_{t=1}^{T} \widehat{\mu}^t(\bm{b}') - \sum_{t=1}^{T} \sum_{\bm{b} \in \mathcal{B}^{+M}} \prob(\bm{b}^\nround = \bm{b})  \widehat{\mu}^t(\bm{b})\right] \lesssim \eta^{-1}M\log|\mathcal{B}| + \eta \sum_{\nround=1}^\Nround \sum_{\bm{b}} \prob(\bm{b}^\nround =\bm{b}) \mathbb{E}[(\sum_{m=1}^M \widehat{w}^\nround_m(b_m))^2]\,.
        \end{align*}
        Replacing $\sum_{\bm{b} \in \mathcal{B}^{+M}} \prob(\bm{b}^\nround = \bm{b})  \widehat{\mu}^t(\bm{b})$ with $\mathbb{E}[\widehat{\mu}^\nround(\bm{b}^\nround)]$ and recalling that the bid vector utility estimates $\widehat{\mu}^\nround$ were unbiased, we have:
        \begin{align*}
            \sum_{t=1}^{T} \mu^t(\bm{b}') - \sum_{t=1}^{T} \mathbb{E}[\mu^t(\bm{b}^\nround)] \lesssim \eta^{-1}M\log|\mathcal{B}| + \eta \sum_{\nround=1}^\Nround \sum_{\bm{b}} \prob(\bm{b}^\nround =\bm{b}) \mathbb{E}[(\sum_{m=1}^M \widehat{w}^\nround_m(b_m))^2]\,.
        \end{align*}
        Notice that as this is true for any $\bm{b}'$, we can replace it with the bid vector that maximizes the true cumulative utility $\sum_{t=1}^T \mu^\nround(\bm{b}')$ and see that the left hand side becomes precisely $\textsc{Regret}_\mathcal{B}$, which completes the proof.
        \Halmos
    \endproof

    Now, it remains to show an upper bound on the second moment of the bid vector utility estimate $\mathbb{E}[(\sum_{m=1}^M \widehat{w}^\nround_m(b_m))^2]$. A crude attempt would be to say that $\mathbb{E}[(\sum_{m = 1}^M \widehat{w}^\nround_m(b_m))^2] \leq \Nitem \sum_{m = 1}^M \mathbb{E}[\widehat{w}^\nround_m(b_m)^2]$:
    \begin{align}
        &\sum_{\nround=1}^\Nround \sum_{\bm{b}} \prob(\bm{b}^\nround = \bm{b}) \mathbb{E}[(\sum_{m = 1}^M \widehat{w}^\nround_m(b_m))^2] \leq \Nitem \sum_{\nround=1}^\Nround \sum_{\bm{b}} \prob(\bm{b}^\nround = \bm{b}) \sum_{m = 1}^M \mathbb{E}[\widehat{w}^\nround_m(b_m)^2] \label{eq: full info difference}\\
        &= \Nitem \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} \mathbb{E}[\widehat{w}^\nround_m(b)^2] \sum_{\bm{b}: b_m = b} \prob(\bm{b}^\nround = \bm{b}) = \Nitem \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} \mathbb{E}[\widehat{w}^\nround_m(b)^2] q^\nround_m(b) \notag\\
        &\leq \Nitem \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} \frac{2}{q_m^t(b)} q_m^t(b) = O(\Nitem^2 |\mathcal{B}| \Nround)\,. \notag
    \end{align}
    Where the last inequality follows from:
    \begin{align*}
        \mathbb{E}[\widehat{w}_m^t(b)^2] = \mathbb{E}\left[\left( 1 - \frac{1-w_m^t(b)}{q_m^t(b)} \textbf{1}_{b_m^t = b} \right)^2 \right] = 1 - 2\mathbb{E}\left[\frac{1-w_m^t(b)}{q_m^t(b)}\textbf{1}_{b_m^t=b}\right] + \mathbb{E}\left[\left(\frac{1-w_m^t(b; \bm{v})}{q_m^t(b)}\right)^2\textbf{1}_{b_m^t=b}\right]\,.
    \end{align*}
    Evaluating the expectations with $\mathbb{E}\left[\textbf{1}_{b_m^t = b}\right] = q_m^t(b)$, we have:
    \begin{align*}
        \mathbb{E}[\widehat{w}_m^t(b)^2] = 1 - \left[2 - 2w_m^t(b)\right] + \left[\frac{(1 - w_m^t(b))^2}{q_m^t(b)}\right] = 2w_m^t(b) - 1 + \frac{1}{q_m^t(b)} \leq 1 + \frac{1}{q_m^t(b)} \leq \frac{2}{q_m^t(b)}\,.
    \end{align*}    
    Plugging this back into our upper bound yields stated regret bound for $\eta = \Theta(\sqrt{\frac{\log |\mathcal{B}|}{M|\mathcal{B}|T}})$ such that $\eta < \frac{1}{M}$:
    
    \begin{align*}
        \textsc{Regret}_\mathcal{B} \lesssim \eta^{-1}\Nitem \log |\mathcal{B}| + \eta\Nitem^2 |\mathcal{B}|T = O(\Nitem^{\frac{3}{2}}\sqrt{|\mathcal{B}| \Nround \log |\mathcal{B}|})\,.
    \end{align*}

    \textbf{Part 3: Complexity Analysis.} We note that the time and space complexity analysis is identical to that of Algorithm~\ref{alg: Decoupled Exponential Weights}, as the only additional computational work being done is computing the normalization terms $q_m^\nround(b)$, which requires $O(M|\mathcal{B}|)$ space and $O(MT|\mathcal{B}|)$ time respectively. Hence, discarding old tables, the total time and space complexities of Algorithm \ref{alg: Decoupled Exponential Weights - Path Kernels} are $O(\Nitem |\mathcal{B}| \Nround)$ and $O(\Nitem |\mathcal{B}|)$ respectively. As this is polynomial in $\Nitem, |\mathcal{B}|, \Nround$, we have proven the claim of polynomial space and time complexities.

    \textbf{Part 4: Selecting $\mathcal{B}$.} We claim that the sub-optimality due to the discretization is on the order of $\frac{MT}{|\mathcal{B}|}$. Assume that $\mathcal{B} \equiv \{\frac{i}{|\mathcal{B}|}\}_{i \in [|\mathcal{B}|]}$ is an even discretization of $[0, 1]$., and recall the continuous regret benchmark:
    \begin{align*}
        \textsc{Regret} = \max_{\bm{b} \in [0, 1]^{+\Nitem}} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}) - \mathbb{E}\left[\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)\right],
    \end{align*}
    where the maximum is taken over the entire space $[0, 1]^{+\Nitem}$ rather than $\mathcal{B}^{+\Nitem}$. Let $\bm{b}^*$ denote the maximizer of the continuous regret. Then, bidder $n$ could have obtained at least the same allocation by rounding up each bid in $\bm{b}^*$ to the next largest multiple of $\frac{1}{|\mathcal{B}|}$. Let this rounded bid vector be denoted by $\bm{b}^+$. As their allocation, thus value for the set of items received, does not decrease, and their total payment increases by a maximum of $\frac{M}{|\mathcal{B}|}$ at each round, then we have that $\mu_n^t(\bm{b}^+) \geq \mu_n^t(\bm{b}^*) - \frac{M}{|\mathcal{B}|}$. Let $\bm{b}_{\mathcal{B}}^* \in \mathcal{B}^{+\Nitem}$ denote the hindsight optimal utility vector returned by our offline dynamic programming (Algorithm~\ref{alg: Offline Full}), which serves as the regret benchmark in the definition of discretized regret. Noting that $\bm{b}^+ \in \mathcal{B}^{+\Nitem}$, we have that the total utility of bidding $\bm{b}^*_{\mathcal{B}}$ must be at least that of $\bm{b}^+$. Thus,
    \begin{align*}
        \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}_{\mathcal{B}}^*) \geq \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^+)\geq \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^*) - \frac{MT}{|\mathcal{B}|}
    \end{align*}
    We balance this with the discretized regret $O(\Nitem^{\frac{3}{2}}\sqrt{|\mathcal{B}| \Nround \log |\mathcal{B}|})$ with $|\mathcal{B}| = M^{-\frac{1}{3}}T^{\frac{1}{3}}$. This yields continuous regret $\textsc{Regret} = O(M^{\frac{4}{3}}T^{\frac{2}{3}} \sqrt{\log \Nround})$.

    \textbf{Part 5: Extending to the Full Information Setting.} Thus far, we have only discussed the bandit feedback algorithm. Fortunately, the full information setting algorithm is exactly the same except for two differences: 1) we do not need to compute $\bm{q}$ and 2) we can replace the reward estimates $\widehat{\mu}^t(\bm{b})$ with the true rewards $\mu^t(\bm{b})$ in Equation~\ref{eq: full info difference}. The first difference can only serve to improve the time and space complexity of our algorithm. The second difference allows us to improve the bound on $\sum_{\nround=1}^\Nround \sum_{\bm{b}} \prob(\bm{b}^\nround = \bm{b}) \mathbb{E}[(\sum_{m = 1}^M \widehat{w}^\nround_m(b_m))^2]$ in the left hand sight of Equation~\ref{eq: full info difference} by replacing $\widehat{w}^\nround_m(b_m))$ with $w^t_m(b_m)$:
    \begin{align*}
        \sum_{\nround=1}^\Nround \sum_{\bm{b}} \prob(\bm{b}^\nround = \bm{b}) \mathbb{E}[(\sum_{m = 1}^M \widehat{w}^\nround_m(b_m))^2] = \sum_{\nround=1}^\Nround \sum_{\bm{b}} \prob(\bm{b}^\nround = \bm{b}) \mathbb{E}[(\sum_{m = 1}^M \widehat{w}^\nround_m(b_m))^2] \leq M^2 \sum_{\nround=1}^\Nround \sum_{\bm{b}} \prob(\bm{b}^\nround = \bm{b}) = M^2T
    \end{align*}
    
    Notice that this bound is a factor of $|\mathcal{B}|$ improvement over that in the bandit setting. Consequently, we obtain our stated regret bound of $O(M^\frac{3}{2} \sqrt{T \log |\mathcal{B}|})$ with the choice of $\eta = \Theta(\sqrt{\frac{\log |\mathcal{B}|}{MT}})$. Balancing this regret with the error term, which is of order $O\left(\frac{M|\mathcal{B}|}{T}\right)$, the optimal choice of $|\mathcal{B}|$ is given by $\Theta(\sqrt{\frac{T}{M}})$. This yields corresponding continuous regret of $O(M^{\frac{3}{2}}\sqrt{T \log T})$.
    

\end{proof}

\subsection{Proof of Lemma \ref{lem: QSpace Equivalence}} \label{sec:QSpace Equivalence} 
      
\begin{proof}{Proof of Lemma \ref{lem: QSpace Equivalence}}
In order to show equivalence, we show that (1) for any $\pi \in \Pi$, that $q(\pi) \in \mathcal{Q}$ and (2) for any $\bm{q} \in \mathcal{Q}$, there exists a $\pi \in \Pi$ such that $q(\pi) = \bm{q}$. We first prove (1). To do this, we simply need to check that for a given $\pi \in \Pi$, that $q^\pi = q(\pi)$ satisfies the constraints prescribed by $\mathcal{Q}$.

    The non-negativity constraint holds trivially as each $\pi((m, b), b')$ is non-negative. Since all $q^\pi_1(b) = \pi((0, \max\mathcal{B}), b) \geq 0$ for all $b \in \mathcal{B}$, by induction, $q^\pi_{m+1}(b) = \sum_{b" \geq b} q^\pi_m(b") \pi((m, b"), b)$ is also non-negative.
    
    Now we prove that each layer $m$ sums to 1, i.e., $\sum_{b \in \mathcal{B}} q^\pi_m(b) = 1$. Since $\sum_{b \in \mathcal{B}} q^\pi_1(b)$, the policy has total node probability 1 in the first layer, we can prove $\sum_{b \in \mathcal{B}} q^\pi_m(b) = 1$, that the policy has total node probability 1 in the $m$'th layer, via induction. This follows immediately from the fact that the DP graph is layered, i.e., edges exist only from nodes in layer $m$ to nodes in layer $m+1$, thus the only edges leading to layer $m+1$ are from layer $m$, in which there are no other edges. Hence, the total node probability in layer $m+1$ must be exactly that of layer $m$. More formally, we have:
    \begin{align*}
        \sum_{b \in \mathcal{B}} q^\pi_{m+1}(b) = \sum_{b \in \mathcal{B}} \sum_{b" \geq b} q^\pi_m(b") \pi((m, b"), b) = \sum_{b" \in \mathcal{B}} q^\pi_m(b") \sum_{b \leq b"} \pi((m, b"), b) = \sum_{b" \in \mathcal{B}} q^\pi_m(b")\,.
    \end{align*}

     To show the stochastic domination constraint $\sum_{b \leq b'} q^\pi_{m+1}(b) \geq \sum_{b \leq b'} q^\pi_m(b)$, we use the bid monotonicity constraint; i.e., the fact that the edges between layers are only from larger bids to (weakly) smaller bids. Recall that $\pi((m,b'), b")$ is the probability of transitioning from unit-bid value pair $(m, b')$ to $(m+1, b")$ and that the only edges leading to $(m+1, b")$ come from nodes $(m, b')$ for $b' \geq b"$. Then, we have:
     \begin{align*}
         \sum_{b \leq b'} q^\pi_{m+1}(b) &= \sum_{b \leq b'} \sum_{b" \geq b} q^\pi_m(b")\pi((m, b"), b) \\
         &=\sum_{b" > b'} q^\pi_m(b") \sum_{b \leq b'} \pi((m, b"), b) + \sum_{b" \leq b'} q^\pi_m(b") \sum_{b \leq b"} \pi((m, b"), b)\\
         &= \sum_{b" > b'} q^\pi_m(b") \sum_{b \leq b'} \pi((m, b"), b) + \sum_{b \leq b'} q^\pi_m(b)\\
         &\geq \sum_{b \leq b'} q^\pi_m(b)\,.
     \end{align*}
     Hence, we have shown that for any $\pi \in \Pi$, that the corresponding $q(\pi) \in \mathcal{Q}$.
     
     Now we show the other direction (2), that for any $\bm{q} \in \mathcal{Q}$, there exists a $\pi \in \Pi$ such that $q(\pi) = \bm{q}$. We proceed by showing that for all $m, b^*$, there exists $\{\pi((m, b), b')\}_{b, b' \in \mathcal{B}}$ such that the following conditions hold:
     \begin{enumerate}
         \item $\pi((m, b), b') \geq 0$ for all $b, b' \geq b^*$.
         \item $\pi((m, b), b') = 0$ for all $b' > b \geq b^*$.
         \item $\sum_{b' \leq b, b' \geq b^*} \pi((m, b), b') \leq 1$ for all $b^* \in \mathcal{B}$, with equality if and only if $b^* =  b_{\min}$ where $b_{\min} = \min \mathcal{B}$.
         \item $\sum_{b' \geq b^*} \sum_{b \geq b'} q_m(b)\pi((m, b), b') = \sum_{b' \geq b^*} q_{m+1}(b')$.
     \end{enumerate}
Let $\Pi(b^*; \mathbf{q})$, $b^*\in \mathcal B$, be the set of all policies under which the four conditions hold at $b^*$ and ${\mathbf q} \in \mathcal Q$. 
     
    These conditions trivially hold for $m = 0$, as we can set $\pi((0, \max \mathcal{B}), b) = q_1(b)$ and $\pi((0, b), b') = \textbf{1}_{b = b'}$. To solve for general $m$, we must show that there exists $\{\pi((m, b), b')\}_{b, b' \in \mathcal{B}}$ that satisfies the constraints prescribed by $\Pi$ and that $\sum_{b \geq b'} q_m(b)\pi((m, b), b') = q_{m+1}(b')$ for all $b' \in \mathcal{B}$. In order to do this, we show that conditions (1), (2), (3), and (4) for each $b^* \in \mathcal{B}$. In particular, if we show conditions (1) and (2) for $b^* = b_{\min}$, then we have already satisfied the first two conditions of $\Pi$. If we show that (3) holds for $b^* = b_{\min}$, then by condition (1), then (3) holds for all $b^* \in \mathcal{B}$ as well, as the summation only includes fewer terms as $b^*$ increases. Similarly, if we show condition (4) holds for two adjacent values of $b_-^* < b^*$, then we have that $\sum_{b \geq b' \geq b^*} q_m(b)\pi((m, b), b') = q_{m+1}(b^*)$. Thus, if condition (4) holds for all possible pairs of adjacent bid values, then we have that $\sum_{b \geq b'} q_m(b)\pi((m, b), b') = q_{m+1}(b')$ for all $b'$. These observations suggest use of induction over $b^*$, and indeed, we begin by showing that these conditions hold for $b^* = b_{\min}$. We then show that this implies that the conditions hold for the next smallest value of $b^*$, which would complete the induction proof.

    \textbf{Base Case}: Recall $b^* = b_{\min}$. We now show that there exists $\{\pi((m, b), b')\}_{b, b' \in \mathcal{B}}$ satisfying all four conditions. For any $m\in[M]$, let we set $\pi((m, b), b') = \textbf{1}_{b = b'}$. Then, 
 condition (4) is clearly satisfied: 
    \begin{align*}
        \sum_{b' \geq b^*} \sum_{b \geq b'} q_m(b)\pi((m, b), b') = \sum_{b' \geq b^*} q_{m+1}(b') \leftrightarrow \sum_{b \in \mathcal{B}} q_m(b) \sum_{b' \leq b} \pi((m, b), b') = \sum_{b \in \mathcal{B}} q_{m+1}(b) = 1\,.
    \end{align*}
    It is also easy to check that conditions (1)-(3) are also satisfied when we set $\pi((m, b), b') = \textbf{1}_{b = b'}$ for any $m$. This shows that $\Pi(b^*; {\mathbf{q}})$ is non-empty, as desired. 
   
    
    \textbf{Recursive Case}:
    For any $b\in \mathcal B$, let $b_-$ be the largest $b' \in \mathcal B$, which is strictly smaller than $b$. Here, we assume that 
    $\Pi(b^*_-; \bf{q})$ is not empty, and under this assumption, we show that set $\Pi(b^*; \bf{q})$ is not empty, where $\Pi(b^*; {\bf{q}}) \subseteq \Pi(b^*_-; \bf{q})$. 
    Let us start with condition (4). 
    We would like to show that there exists a $\bm{\pi}$ that satisfies condition (4) at $b^*$ along with the other three conditions. By the induction assumption, we have  
    \begin{align*}
        &\sum_{b' \geq b_-^*} \sum_{b \geq b'} q_m(b)\pi((m, b), b') = \sum_{b' \geq b_-^*} q_{m+1}(b') \to\\
        &\sum_{b' \geq b^*} \sum_{b \geq b'} q_m(b)\pi((m, b), b') + \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*) = \sum_{b' \geq b^*} q_{m+1}(b') + q_{m+1}(b_-^*) \to\\
        &\sum_{b' \geq b^*} \sum_{b \geq b'} q_m(b)\pi((m, b), b') = \sum_{b' \geq b^*} q_{m+1}(b') + \left[q_{m+1}(b_-^*) - \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*)\right] \to\\
        &\sum_{b \geq b^*} q_m(b) \sum_{b' \leq b; b' \geq b^*} \pi((m, b), b') = \sum_{b' \geq b^*} q_{m+1}(b') + \left[q_{m+1}(b_-^*) - \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*)\right] 
    \end{align*}
    Thus, we can satisfy condition (4) if $q_{m+1}(b_-^*) = \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*)$. We now observe that the latter summation depends linearly (and hence, continuously) in the values of $\pi((m, b), b_-^*)$. If we can show that there exists an assignment of these variables that satisfy $q_{m+1}(b_-^*) \geq \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*)$ and also $q_{m+1}(b_-^*) \leq \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*)$, then by the intermediate value theorem, there must be some assignment that achieves exact equality. 
    
    
    In order to show the first inequality, notice that if we set $\pi((m, b), b_-^*) = 1 - \sum_{b' < b_-^*} \pi((m, b), b')$ for all $b \geq b_-^*$ (this is required in order to guarantee conditions (1) and (3) are satisfied), then:
    \begin{align*}
        \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*) &= \sum_{b \geq b_-^*} q_m(b) - \sum_{b \geq b_-^*} q_m(b)\sum_{b' < b_-^*} \pi((m, b), b') \\
        &= \sum_{b \geq b_-^*} q_{m}(b) - \sum_{b \in \mathcal{B}} q_m(b) \sum_{b' < b_-^*} \pi((m, b), b') + \sum_{b < b_-^*} q_m(b) \sum_{b' < b_-^*} \pi((m, b), b')\\
        &= \sum_{b \geq b_-^*} q_{m}(b) - \sum_{b' < b_-^*} q_{m+1}(b) + \sum_{b < b_-^*} q_m(b) \sum_{b' < b_-^*} \pi((m, b), b')\\
        &= \sum_{b \geq b_-^*} q_{m}(b) - \sum_{b' < b_-^*} q_{m+1}(b) + \sum_{b < b_-^*} q_m(b)\\
        &\geq \sum_{b \geq b_-^*} q_{m+1}(b) - \sum_{b' < b_-^*} q_{m+1}(b) + \sum_{b < b_-^*} q_{m+1}(b)\\
        &= \sum_{b \geq b_-^*} q_{m+1}(b)\\
        &\geq q_{m+1}(b_-^*)\,.
    \end{align*}
    Here, the third equality follows from the (strong) inductive hypothesis, and the first inequality is a result of the stochastic domination constraint in $\mathcal{Q}$. We also note that the values $\sum_{b' < b_-^*} \pi((m, b), b')$ have already been fixed
    as these were required to satisfy condition (4) in the previous iterates, and as condition (3) holds for $b^*_-$ by the inductive hypothesis, then $1 - \sum_{b' < b_-^*} \pi((m, b), b') \geq 0$. Conversely, if we set $\pi((m, b), b_-^*) = 0$ for all $b \geq b_-^*$, then:
    \begin{align*}
        \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*) = 0 \leq q_{m+1}(b_-^*)\,.
    \end{align*}
    As the sum $\sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*)$ linearly (thus, continuously) depends on the values of $\pi((m, b), b_-^*)$, by the intermediate value theorem, there exists an assignment of $\{\pi((m, b), b_-^*)\}_{b \geq b_-^*}$ with each $\pi((m, b), b_-^*) \in [0, 1 - \sum_{b' < b_-^*} \pi((m, b), b')]$ such that the sum is precisely equal to $q_{m+1}(b_-^*) \in [0, 1]$. Now we observe that these values of $\pi((m, b), b_-^*) \in [0, 1 - \sum_{b' < b_-^*} \pi((m, b), b')]$ do not violate conditions (1), (2), or (3). Furthermore, note that any $\bm{\pi} \in \Pi$ also satisfied conditions (1), (2), and (3) under $b^*_-$ for $\{\pi((m, b), b')\}_{b \geq b_-^*, b' \leq b_-^*}$, then the assignment to $\{\pi((m, b), b')\}_{b \geq b_-^*, b' < b_-^*}$ will not violate these conditions as our new constraint on the variables $\{\pi((m, b), b_-^*)\}_{b \geq b_-^*}$ is independent of the values of $\{\pi((m, b), b')\}_{b \geq b_-^*, b' < b_-^*}$. Thus, the set $\Pi(b^*)$ is non-empty:
    \begin{align*}
        \Pi(b^*) = \{\{\pi((m, b), b')\}_{b, b' \in \mathcal{B}} \in \Pi(b_-^*): \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*) = q_{m+1}(b_-^*)\} \neq \emptyset
    \end{align*}    
    With this, we have proven via induction that our four conditions hold for all $b^* \in \mathcal{B}$, implying that for a fixed $m$, every constraint in $\Pi$ pertaining to variables $\pi((m, b), b')$ is satisfied, as well as the node-measure constraints $\sum_{b \geq b'} q_m(b)\pi((m, b), b') = q_{m+1}(b')$ for all $b'$. By induction, this works for all $m \in [M]$, which concludes the proof.

\end{proof}

\subsection{Proof of Lemma \ref{lem: Online Linear Optimization}}
    We have by the definition of discretized regret:
    \begin{align*}
        \textsc{Regret}_\mathcal{B} &= \max_{\bm{b} \in \mathcal{B}} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}) - \mathbb{E}\left[\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)\right] = \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{q}, \bm{w}^\nround\rangle - \sum_{\nround=1}^\Nround \langle \bm{q}^\nround, \bm{w}^\nround\rangle\right]\,, 
    \end{align*}
    where in the first equality, we applied Equation~\eqref{eq: Loss of policy} which equated the dot product of utilities $\bm{w}^\nround$ and node probability weights $\bm{q}$ to the expected utility of bid vector $\bm{b} \sim \bm{\pi}$ with utilities $\{w_m^t(b)\}_{m \in [M], b \in \mathcal{B}} = \bm{w}^\nround$. Combining the two summations yields the desired result.

\subsection{Proof of Theorem~\ref{thm: OMD}: Online Mirror Descent Algorithm }

\label{sec: Proof of OMD}

\begin{proof}{Proof of Theorem~\ref{thm: OMD}: Online Mirror Descent Algorithm}

    The proof is divided into four parts, similar to the analysis of Algorithm~\ref{alg: Decoupled Exponential Weights}. In the first part, we rigorously show how our algorithm achieves the stated regret. In the second, we verify correctness of our procedure that recovers a policy $\bm{\pi}^\nround$ from $\bm{q}^\nround$. Then, we show the corresponding time and space complexity of our algorithm. Afterwards, we optimize over discretization error to obtain the continuous regret. %Lastly, we show the extension to the full information setting.
    

    \textbf{Part 1: Regret of Online Linear Optimization.} Recall that from Lemma~\ref{lem: Online Linear Optimization}, we have
    \begin{align}
        \textsc{Regret}_\mathcal{B} = \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[ \sum_{\nround=1}^\Nround \langle \bm{q} - \bm{q}^\nround, \bm{w}^\nround \rangle\right] = \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[\sum_{\nround=1}^\Nround \langle  \bm{q}^\nround - \bm{q}, -\bm{w}^\nround \rangle\right]\, ,
    \end{align}
    where we negate the utility function into a loss function to be consistent with the OLO convention. We follow a standard analysis of OMD, which shows that the optimization step can be solved efficiently and the resulting iterates have bounded regret. For the former, we show that solution to the $\bm{q}$ optimization step in our algorithm $\bm{q}^{\nround} = \text{argmin}_{\bm{q} \in \mathcal{Q}} \eta\langle \bm{q}, -\bm{w}^\nround\rangle + D(\bm{q} || \bm{q}^{\nround-1})$ can be obtained as the projection of the unconstrained minimizer of \[\tilde{q}^{\nround}= \text{argmin}_{\bm{q} \in [0, 1]^{M \times |\mathcal{B}|}} \eta\langle \bm{q}, -\bm{w}^\nround\rangle + D(\bm{q} || \bm{q}^{\nround-1})\] to the space $\mathcal{Q}$ (See Projection Lemma, Lemma 8.6 of \cite{BartokLecNotes2011}). 
    Having characterized the exact form of the OMD iterates, all that remains is to upper bound the regret of OMD with the regret of Be-the-regularized-leader.
    % that relates $\eta \bm{w}^t$ to $D(\bm{q}^\nround||\tilde{\bm{q}}^\nround)$
    \begin{lemma}[Lemma 9.2 of \cite{BartokLecNotes2011}]
        \label{lem: Be Regularized leader regret}
        Letting $D(\bm{q} || \bm{q}')$ denote the unnormalized KL divergence between $\bm{q}$ and $\bm{q}'$, we have:
        \begin{align*}
            \textsc{Regret}_\mathcal{B} \leq \max_{\bm{q} \in\mathcal{Q}} \mathbb{E}\Big[\eta^{-1} D(\bm{q} || \bm{q}^1) + \sum_{\nround=1}^\Nround \langle \bm{q}^\nround - \tilde{\bm{q}}^{\nround+1}, \bm{w}^\nround \rangle\Big]\,.
        \end{align*}
    \end{lemma} 
     The remainder of the regret analysis closely follows that of Theorem 1 in \cite{OREPS2013}. At a high level, we want to bound the regret of Online Mirror Descent by the regret of the unconstrained Be the
     (Negentropy) Regularized leader, via Lemma~\ref{lem: Be Regularized leader regret} (see Lemma 13 of \cite{LectureNotes2009} for the more general statement and proof of this lemma). 
     We then upper the contribution of the summation term by using the specific definition of the node weight estimators. Similarly, we upper bound the divergence term as a function of the dimension of the space $\mathcal{Q}$.
     
    
    
    To begin, note that our node utility estimators $\widehat{w}_\nitem^\nround(b)$ are unbiased:
    \begin{align}
        \mathbb{E}_{\bm{b} \sim \bm{\pi}^\nround}[\widehat{w}_\nitem^\nround(b)] = \mathbb{E}_{\bm{b} \sim \bm{\pi}^\nround}[\frac{w_\nitem^\nround(b)}{q^{\nround}_\nitem(b)} \textbf{1}_{b = b^{\nround}_\nitem}] = \frac{w_\nitem^\nround(b)}{q^{\nround}_\nitem(b)} \prob_{\bm{b} \sim \bm{\pi}^\nround} (b = b^\nround_\nitem) = \frac{w_\nitem^\nround(b)}{q^{\nround}_\nitem(b)} q^{\nround-1}_\nitem(b) = w_\nitem^\nround(b)\ .
        \label{proof: part1}
    \end{align}
    Now, consider the right hand side of the inequality in  Lemma \ref{lem: Be Regularized leader regret}. As the node utility estimators are unbiased, so we can replace $\bm{w}^\nround$ with $\widehat{\bm{w}}^\nround$. 
    Now, as per Lemma \ref{lem: Be Regularized leader regret}, we can upper bound the expected estimated regret as a function of the unconstrained optimizer $\tilde{\bm{q}}^{\nround+1}$ and the unregularized relative entropy  with respect to the initial state-edge occupancy measure $\bm{q}^1$. Applying the aforementioned lemma to Equation \eqref{proof: part1}, we obtain:
    \begin{align}
        \textsc{Regret}_\mathcal{B} = \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{q}^{\nround} - \bm{q}, -\widehat{\bm{w}}^{\nround} \rangle \right] \leq \max_{\bm{q} \in \mathcal{Q}}\mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{q}^{\nround} - \tilde{\bm{q}}^{ \nround+1}, -\widehat{\bm{w}}^{\nround} \rangle + \eta^{-1}D(\bm{q} || \bm{q}^{1}) \right]
    \end{align}
    Applying $\exp(x) \geq 1 + x$ for $x = \exp(\eta \widehat{\bm{w}}^{\nround})$, we obtain $\tilde{\bm{q}}^{ \nround+1} = \bm{q}^\nround \exp(\eta \widehat{\bm{w}}^{\nround}) \geq \bm{q}^{\nround} + \eta \bm{q}^{\nround} \widehat{\bm{w}}^{\nround}$, which yields $\bm{q}^t - \bm{q}^t\exp(\eta\widehat{\bm{w}}^t) \ge  -\eta \bm{q}^t \widehat{\bm{w}}^t$. Plugging this back in:
    \begin{align}
        \textsc{Regret}_\mathcal{B} &\leq \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{q}^{\nround} - \bm{q}^{\nround} \exp(\eta \widehat{\bm{w}}^{\nround}), -\widehat{\bm{w}}^{\nround} \rangle + \eta^{-1}D(\bm{q} || \bm{q}^{1}) \right]\\
        &\le  \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}_\nitem(b) \widehat{w}^{\nround}_\nitem(b)^2 + \eta^{-1}D(\bm{q} || \bm{q}^{1}) \right] \,.\label{eq: node diff}
    \end{align}
    Note that $\widehat{w}^{\nround}_\nitem(b) = \frac{w_\nitem^\nround(b)}{q^{\nround-1}_{\nitem}(b)} \textbf{1}_{b = b^{\nround}_{\nitem}}$ for all $\nitem \in [\Nitem]$ and $b \in \mathcal{B}$ by definition. Since $w^{\nround}_\nitem(b) \leq 1$ and $\textbf{1}_{b = b^{\nround}_{\nitem}} \leq 1$ we have $\widehat{w}^{\nround}_\nitem(b) \leq \frac{1}{q^{\nround}_\nitem(b)}$ and we continue the above chain of inequalities with:
    \begin{align}
        \textsc{Regret}_\mathcal{B} &\leq \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}_{\nitem}(b) \widehat{w}^{\nround}_\nitem(b) \frac{1}{q^{\nround}_{\nitem}(b)}  + \eta^{-1}D(\bm{q} || q^{1}) \right] \label{eq: full info difference appendix}\\
        &= \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} \widehat{w}^{\nround}_\nitem(b)  + \eta^{-1}D(\bm{q} || \bm{q}^{1}) \right] \, .
    \end{align} 
    
    Recalling that $D(\bm{q} || \bm{q}^1) = \sum_{\nitem \in [\Nitem], b \in \mathcal{B}} q_\nitem(b)\log\frac{ q_\nitem(b)}{q^1_\nitem(b)} - (q_\nitem(b) - q^1_\nitem(b))$, we note that:
    
    \begin{align*}
        D(\bm{q} || \bm{q}^1) &= \sum_{m = 1}^M \sum_{b \in \mathcal{B}} q_m(b)\frac{\log q_m(b)}{\log q^1_m(b)} - q_m(b) + q^1_m(b) \\
        &= \sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q_{\nitem}(b)\log q_m(b) - q_m(b)\log q^1_m(b)\,,
    \end{align*} 
    where in the second equality, we used the fact that the elements both $\bm{q}$ and $\bm{q}^1$ all sum to $M$. Selecting $\bm{q}^1_m(\cdot)$ to be the uniform distribution over all $b \in \mathcal{B}$ and using the fact that the entropy of a discrete distribution over $|\mathcal{B}|$ items is $\log |\mathcal{B}|$, we obtain:
    \begin{align*}
        D(\bm{q} || \bm{q}^1) &= -\sum_{\nitem=1}^\Nitem H(\bm{q}_m) + \log |\mathcal{B}|\sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q_{\nitem}(b) \\
        &\leq \sum_{\nitem=1}^\Nitem \log |\mathcal{B}| + \log |\mathcal{B}|\sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q_{\nitem}(b) = \Theta(M\log|\mathcal{B}|)\,,
    \end{align*}
    where $H(\bm{x}) = -\sum_{x \in \bm{x}} x \log x $ denotes the discrete entropy function.  
    Plugging this back in:
    \begin{align}
        \textsc{Regret}_\mathcal{B} &\lesssim \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} \widehat{w}^{\nround}_\nitem(b)  + \eta^{-1}\Nitem \log |\mathcal{B}| \right]\\
        &\leq \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem=1} \sum_{b \in \mathcal{B}} \widehat{w}^{\nround}_\nitem(b) + \eta^{-1}\Nitem \log |\mathcal{B}| \right]\\
        &\leq \eta \sum_{\nround=1}^\Nround \sum_{\nitem=1} \sum_{b \in \mathcal{B}} w^{\nround}_\nitem(b) + \eta^{-1}\Nitem \log |\mathcal{B}|\\
        &= \eta \Nround \Nitem |\mathcal{B}| + \eta^{-1}\Nitem \log |\mathcal{B}|\,,
    \end{align}
    where in the last equality, we used the unbiasedness property of $\widehat{\bm{w}}^{\nround}$. Setting $\eta = \sqrt{\frac{\log |\mathcal{B}|}{|\mathcal{B}|T}}$, we obtain $\textsc{Regret}_\mathcal{B}(\Nround) \leq \Nitem \sqrt{|\mathcal{B}| \Nround \log |\mathcal{B}|}$. 

    
    \textbf{Part 2: Determining Policy $\bm{\pi}$ from Node Probability Measures $\bm{q}$.} 
    Notice that in our regret analysis for both the bandit and full information setting, we do not require explicit knowledge of the policy $\bm{\pi}^t$, so long as it generates the desired node occupancy measure $\bm{q}^t$. In particular, we require a method of converting $\bm{q}^\nround$ to policy $\bm{\pi}^\nround$ which, in turn, is required in order to sample $\bm{b}^\nround$. Recall from Lemma~\ref{lem: QSpace Equivalence} that the mapping from the space of policies $\Pi$ to the space of node weight measures $\mathcal{Q}_\Pi = \mathcal{Q}$ is injective. Thus, for any $\bm{q} \in \mathcal{Q}$, there must exists a $\bm{\pi} \in \Pi$ such that $q(\bm{\pi}) = \bm{q}$. Moreover, the set $\Pi(\bm{q})$ of such $\bm{\pi}$ can be written as the intersection of two polyhedrons, and hence a polyhedron, from which a feasible solution can be computed efficiently (e.g., ellipsoid method), where  $ \Pi(\bm{q})$ is the set of policies $\pi \in [0,1]^{M\times |\mathcal B|\times |\mathcal B|}$ such that 
    \begin{itemize}
        \item $\pi((0, \max \mathcal{B}), b) = q_1(b)$, for any $b \in \mathcal{B}$;
        \item $\pi((0, b), b') = \textbf{1}_{b = b'}$ for any $b, b' < \max \mathcal{B}$;
        \item $q_{m+1}(b') = \sum_{b \in \mathcal{B}} q_m(b) \pi((m, b), b')\}$ for any $b'\in \mathcal B$ and $m \in [M-1]$.
    \end{itemize}



    \textbf{Part 3: Complexity analysis.} One may wonder how to efficiently update the state occupancy measures by computing the minimizer of $\eta\langle \bm{q}, -\widehat{\bm{w}}^\nround\rangle + D(\bm{q} || \bm{q}^{\nround-1})$. The idea is to first solve the unconstrained entropy regularized minimizer with $\tilde{\bm{q}}^{ \nround+1} = \bm{q}^{\nround} \exp(\eta \widehat{\bm{w}}^{\nround})$. We then project this unconstrained minimizer to $\mathcal{Q}$ with:
    \begin{align}
        \bm{q}^{\nround + 1} = \text{argmin}_{\bm{q} \in \mathcal{Q}} D(\bm{q}||\tilde{\bm{q}}^{\nround + 1})
    \end{align}
    Relegating the details to \cite{OREPS2013}, the above constrained optimization problem can be solved as the minimizer of an equivalent unconstrained convex optimization problem with a polynomial (in $\Nitem$ and $|\mathcal{B}|$) number of variables, and therefore, can be computed efficiently. Combining with finding an initial feasible solution to $\Pi(\bm{q})$ as well as the optimization step, we achieve polynomial in $\Nitem, |\mathcal{B}|, \Nround$ total time complexity. For the space complexity, we only need store the values of $\bm{\pi}^\nround$, $\bm{q}^\nround$, and $\widehat{\bm{w}}^\nround$, for a total space complexity of $O(\Nitem |\mathcal{B}|^2)$.   
    
    \textbf{Part 4: Continuous Regret.} To obtain the continuous regret, recall that the discretization error is $O(\frac{\Nitem \Nround}{|\mathcal{B}|})$. As the discretized regret is $O\left(\Nitem \sqrt{|\mathcal{B}| \Nround \log |\mathcal{B}|}\right)$ in the bandit feedback setting, the optimal choice of $|\mathcal{B}|$ is $\Theta(\Nround^{\frac{1}{3}})$, which achieves continuous regret $\textsc{Regret} = O(\Nitem \Nround^{\frac{2}{3}} \sqrt{\log \Nround})$.

\end{proof}

\subsubsection{Proof of Corollary \ref{cor}}
\label{sec: Proof of cor}

We can straightforwardly extend Algorithm~\ref{alg: OMD} to the full information setting. To do this, we note that we can improve Equation~\eqref{eq: full info difference appendix} by instead replacing $\widehat{\bm{w}}^{\nround}$ with $\bm{w}^{\nround}$ in Equation~\eqref{eq: node diff} to obtain:
\begin{align*}
    \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}_\nitem(b) \widehat{w}^{\nround}_\nitem(b)^2 = \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}_\nitem(b) w^{\nround}_\nitem(b)^2 \leq \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}_\nitem(b) = \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem 1 = \Nround \Nitem\,.
\end{align*}
Setting $\eta = \sqrt{\frac{ \log |\mathcal{B}|}{T}}$, we obtain in the full information setting $\textsc{Regret}_\mathcal{B} = O(\Nitem \sqrt{\Nround \log |\mathcal{B}|})$. We can also compute the optimal choice of $|\mathcal{B}|$ to obtain optimal continuous regret. Using the optimal choice of $|\mathcal{B}|$ being $\Theta(\sqrt{T})$, we achieve continuous regret of $\textsc{Regret} = O(\Nitem \sqrt{\Nround \log \Nround})$. Note that due to the complexity of the optimization sub-routine in the projection step of OMD, for the full information setting, it is preferable to use Algorithm~\ref{alg: Decoupled Exponential Weights} instead.










 



\subsection{Proof of Theorem \ref{thm:lower}: Regret Lower Bound}

 To construct our lower bounds, we construct a stochastic adversary whose distribution across their bids makes it difficult for the bidder to determine their optimal bid, and thus, occurs $\Omega(M\sqrt{T})$ regret while doing so. We define $\bm{b}'_- = (0,\ldots,0,c,\ldots,c)$, where there are $k$ and $\Nitem - k$ values of 0 and $c$ each. We additionally define $\bm{b}"_- = (c,\ldots,c)$ as the $\Nitem$-vector of bids at $c$. Restricting the adversary's bid vectors to be in $\{\bm{b}'_-, \bm{b}"_-\}$, we construct two adversary bid vector distributions $F$ and $G$ over $\{\bm{b}'_-, \bm{b}"_-\}^\Nround$ such that   under $F$, we have $\prob(\bm{b}_-^\nround = \bm{b}'_-) = \frac{1}{2} + \delta$  and $\prob(\bm{b}_-^\nround = \bm{b}"_-) = \frac{1}{2} - \delta$ 
 and under $G$, we have $\prob(\bm{b}_-^\nround = \bm{b}'_-) = \frac{1}{2} - \delta$ and $\prob(\bm{b}_-^\nround = \bm{b}"_-) = \frac{1}{2} + \delta$  for some $\delta \in [0, \frac{1}{2}]$ to be optimized over later. 
 
 
 Assume that $\bm{v} = (1,\ldots,1)$, all tiebreaks are won for simplicity, and the competitors' bids over time are independent. Then, for certain choices of $c$ and $k$ (which we show below), the expected utility maximizing bid vector under $\{\bm{b}_-^\nround\}_{\nround \in [\Nround]} \sim F$ is $(0,\ldots,0)$ and under $\{\bm{b}_-^\nround\}_{\nround \in [\Nround]} \sim G$ is $(c,\ldots,c)$. 
 In particular, we can compute precisely the expected value of bidding $\bm{b}^\nround = \bm{b}$ for all $\nround \in [\Nround]$ under both $F$ and $G$. Note that as adversary bid values only take values in $\{0, c\}$ and bidder $n$ wins all tiebreaks, then the bidder only need consider bid vectors consisting only of all $0$ or $c$. Letting $\nitem$ denote the number of bids in $\bm{b}$ equal to $c$, we have:
    \begin{align*}
        \mathbb{E}_F\left[\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b})\right] = \Nround \left[ (\frac{1}{2} + \delta)\left((1 - c)m + \max(0, M - k - m)\right)  + (\frac{1}{2} - \delta)(1 - c)m   \right]\,.
    \end{align*}
    Where $\mathbb{E}_F$ denotes the expectation with respect to the adversary bids drawn from $F$, namely $\{\bm{b}_-^\nround\}_{\nround \in [\Nround]} \sim F$ (and similarly for $\mathbb{E}_G$ below). In particular, we have that with probability $\frac{1}{2} + \delta$, the adversary will select bid $\bm{b}'_-$. We are then guaranteed to win $m$ units at a price of $c$, for a utility of $1 - c$ per unit. If $\nitem < k$, then $M - k - m$ of the items were won with price 0, for a utility of 1 per unit. With probability $\frac{1}{2}-\delta$, all of the adversary bids are $c$, and we obtain $\nitem$ units at a cost of $c$ each, which corresponds to utility $1 - c$. Similarly, we have: 
    \begin{align*}
        \mathbb{E}_G\left[\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b})\right] = \Nround \left[ (\frac{1}{2} - \delta)\left((1 - c)m + \max(0, M - k - m)\right)  + (\frac{1}{2} + \delta)(1 - c)m   \right]
    \end{align*}
    For the case $m + k \leq M$, we have that $(1-c)m + \max(0, M - k - m) = M - k - mc$, and the above two equations simplify to:
    \begin{align*}
        \mathbb{E}_F[\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b})] = T\left[ (\frac{1}{2} + \delta)(M - k) + m(\frac{1}{2} - \delta - c) \right]\,;\\\mathbb{E}_G[\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b})] = T\left[ (\frac{1}{2} - \delta)(M - k) + m(\frac{1}{2} + \delta - c) \right]\,.
    \end{align*}
    In the case where $m + k \geq M$, we have that $(1-c)m + \max(0, M - k - m) = m - mc$ and we obtain:
    \begin{align*}
        \mathbb{E}_F\left[\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b})\right] = \mathbb{E}_G\left[\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b})\right] = T(1 - c)m\,.
    \end{align*}
    Note that in either case, in the case where we sample $\{\bm{b}^\nround_-\}_{\nround \in [\Nround]}$ according to the mixture $\frac{F+G}{2}$, this corresponds to the case where $\delta = 0$, i.e., the probability of observing either $\bm{b}'_-$ or $\bm{b}"_-$ is equal. We have for all $\bm{b}$:
    \begin{align*}
        \mathbb{E}_{(F+G)/2}[\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b})] = \frac{1}{2}((1 - c)m + \max(0, M-k-m)) + \frac{1}{2}(1 -c)m  \geq (1 - c)m
    \end{align*}
    Note that under $F$, the optimal occurs at the all 0's vector for $c > \frac{1}{2} - \delta$ and $(\frac{1}{2} + \delta)(M - k) > (1-c)m = 0$. Similarly, the optimal occurs at the all $c$'s vector for $c > \frac{1}{2} - \delta$ and $(\frac{1}{2} - \delta)(M - k) > (1 - c)M$. These obtain utilities of $(\frac{1}{2}+\delta)(M-k)$ and $M - Mc$ respectively. One choice of $c$ and $k$ is $\frac{2}{3}$ and $\frac{M}{3}$, with $0 < \delta < \frac{1}{6}$. Looking at the regret incurred each step of the algorithm by selecting any action $\bm{b}$, we have:
    \begin{align*}
        &\max_{\bm{b}'} \left(\mathbb{E}_F[\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b}') - \mu_n^\nround(\bm{b})] \right) + \max_{\bm{b}'} \left(\mathbb{E}_G[\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b}') - \mu_n^\nround(\bm{b})] \right) \\
        &\geq \max_{\bm{b}'} \left(\mathbb{E}_F[\sum_{\nround=1}^\Nround  \mu_n^\nround(\bm{b}')]\right) + \max_{\bm{b}'} \left(\mathbb{E}_G[\sum_{\nround=1}^\Nround  \mu_n^\nround(\bm{b}')]\right) - 2\max_{\bm{b}'} \mathbb{E}_{(F+G)/2}\left(\mathbb{E}_F[\sum_{\nround=1}^\Nround  \mu_n^\nround(\bm{b}')]\right)\\
        &\geq \mathbb{E}_F[\sum_{\nround=1}^\Nround  \mu_n^\nround((0,\ldots,0))] + \mathbb{E}_G[\sum_{\nround=1}^\Nround  \mu_n^\nround((c,\ldots,c))] - 2\max_{\bm{b}'} \mathbb{E}_{(F+G)/2}\left(\mathbb{E}_F[\sum_{\nround=1}^\Nround  \mu_n^\nround(\bm{b}')]\right)\\
        &= (\frac{1}{2}+\delta)(M-k) + \left(M - Mc\right) - 2\max_{\bm{b}'} \mathbb{E}_{(F+G)/2}\left(\mathbb{E}_F[\sum_{\nround=1}^\Nround  \mu_n^\nround(\bm{b}')]\right)\\
        &\geq (\frac{1}{2}+\delta)(M-k) + \left(M - Mc\right) - 2(1-c)M\,.
    \end{align*}
    Now, for example, we can set $k = \frac{M}{3}$ and $c = \frac{2}{3}$ to obtain a per step incurred regret of $\Theta(\Nitem \delta)$.     
    We invoke the useful lemma relating the regret under $(F+G)/2$ to the Kullback-Leilber divergence:
    \begin{lemma}[\cite{NonparametricEstimation2008} Theorem 2.2.]
        We have for any two discrete distributions $F$ and $G$:
        \begin{align}
            \mathbb{E}_{(F+G)/2} \left[\textsc{\emph{Regret}}_\mathcal{B}(T)\right] = \Omega\left( \frac{\Delta}{2} \exp(-D_{\emph{KL}(F || G)}) \right)
        \end{align}
        where $\Delta$ denotes the sum of the total regret incurred under $F$ or $G$.
    \end{lemma} 
    When $F$ and $G$ are independent Bernoulli processes with parameters $\frac{1}{2}+\delta$ and $\frac{1}{2}-\delta$ respectively, then $D_{\text{KL}}(F || G) \leq cT\delta^2$ for some constant $c$. Using $\Delta \in \Theta(MT\delta)$, we have that the previous lemma implies:
    \begin{align}
        \textsc{Regret}_{\mathcal{B}} \in \Omega \left(\Nitem \sqrt{\Nround}\right)
    \end{align}                             
    where $\delta$ is chosen to be $\Omega(\frac{1}{\sqrt{T}})$.


\subsection{Appendix to Section \ref{sec: experiments}: Additional Experiments }

In this section, we run additional experiments to (1) show the impact of the modified $\textsc{EXP3-IX}$ estimator, and (2) empirically verify the regret guarantees of Algorithms \ref{alg: Decoupled Exponential Weights - Path Kernels} and \ref{alg: OMD}. First, we explain the modified, $\textsc{EXP3-IX}$ based versions of the algorithms as used in the experiments section, as well as why we chose to use these modified versions instead of the original ones. We show that the change in the algorithms is marginal, as the only step that is different between Algorithm~\ref{alg: OMD} and its $\textsc{EXP3-IX}$ variant is in updating the node weights. We then run experiments in the $M=1$ unit setting to illustrate the impact of our modified algorithm. Second, in order to empirically gauge the regret guarantees of our proposed algorithms, we compare their performance against an adversary that bids stochastically. We analytically derive the optimal bidding strategy and compare how quickly our algorithms converge to this optimal solution. We repeat similar experiments for the uniform price auctions. 

 
\subsubsection{$\textsc{EXP3-IX}$ vs. Unbiased Reward Estimator}

\label{sec: IX}

In the experiments section, we ran a slightly modified version of our existing algorithms in the bandit feedback setting. We do this as the variance of the accumulated regret of our algorithms are high, as the node weight estimators normalize over vanishingly small probabilities $q^t_m(b)$. To mitigate the effect of such normalization, we use the $\textsc{EXP3-IX}$ estimator as described in \cite{neu2015explore, Lattimore2020}. Under this estimator, rather than normalizing the probability of selecting bid $b^t_m$ for unit $m$ at time $t$ by $q^t_m(b_m^t)$, we instead normalize it by $q^t_m(b_m^t) + \gamma$ for some constant $\gamma > 0$. In the standard $K$-armed bandit setting, despite being a biased estimator, still achieves the same sublinear expected regret guarantee  with a smaller variance. This smaller variance indeed allows for stronger high probability guarantees on the magnitude of our regret; i.e., for $\delta > 0$ and $\gamma = \sqrt{\frac{\log(K) + \log(\frac{K+1}{\delta})}{4K\Nround}}$, the $\textsc{EXP3-IX}$ algorithm guarantees with probability at least $1 - \delta$ that the regret is upper bounded by $C\sqrt{KT\log K}$ for some absolute constant $c > 0$. We extend this algorithm to the multi-unit PAB setting algorithms, where for each node $(m, b)$, we set $\gamma = \sqrt{\frac{\log(K) + \log(\frac{K+1}{\delta})}{4K\Nround}}$ and $K = |\{b \in \mathcal{B}: b \leq v_m\}|$, for $\delta = 0.05$. Aside from the change in node weight estimators, the $\textsc{EXP3-IX}$ versions of Algorithms \ref{alg: Decoupled Exponential Weights - Path Kernels} and \ref{alg: OMD} are exactly the same.\\


\subsubsection{Empirical Performance of Original Algorithms vs. $\textsc{EXP3-IX}$ Variants}

In this section, we empirically analyze the modified variants of our algorithms which use the biased, but lower variance $\textsc{EXP3-IX}$ node-weight estimators (see Appendix~\ref{sec: IX}). We compare the distribution of the regret recovered by these modified algorithms versus the non-modified versions when the number of units is one. The bidder, endowed with valuation vector $\bm{v} = [1]$, will compete against a single adversary over the course of $T$ rounds for $\overline{M}=M=1$ item. This is the standard first price auction (FPA). Here, we compare performance when the adversary is stochastic (bids drawn uniformly random from $[0, 1]$) versus adaptive adversary (running the same algorithm, with a valuation drawn uniformly random from $[0, 1]$). 


We plot the regret of the bidder against the stochastic and adversarial competitors for moderate $T \in \{100, 500, 2000, 10000\}$. The stochastic adversary setting is shown in Figure~\ref{fig:eval_IX} (a) and the adversarial setting is shown in Figure~\ref{fig:eval_IX} (b). 
We observe that while the $\textsc{EXP3-IX}$ variants marginally worsens regret for small values of $T \in \{100, 500\}$ for both the stochastic and adaptive settings, it significantly mitigates the heavy tailed distribution of regret for large $T \in \{2000, 10000\}$, especially in the adversarial setting.



% Figure environment removed


\subsubsection{Stochastic Setting} \label{sec:stochastic}

To verify our algorithms' theoretical regret guarantees, we consider the setting where the bidder competes in a stochastic setting with multi-unit. Here, the bidder, endowed with valuation vector $\bm{v} = [1, 1, 1]$, will compete over the course of $T=10^4$ rounds for $\overline{M}=M=3$ items. The competing bids are  $\bm{b}^{-1} = [0.1, 0.1, 0.1]$, $[0.3,0.3, 1.0]$, or $[0.4, 1.0, 1.0]$ with probabilities $\frac{1}{4}, \frac{1}{4}$, and $\frac{1}{2}$, respectively. Assuming that the bidder receives priority in tiebreaks, with $\mathcal{B} = \{\frac{i}{10}\}_{i \in [10]}$, the expected utility $\sum_{m=1}^3 \prob(b_m \geq b^{-1}_m) (1 - b_m)$ maximizing bid vector is given by $\bm{b} = [0.4, 0.3, 0.1]$, which yields utility $(1)(1-0.4) + (0.75)(1 - 0.3) + (0.5)(1-0.1) = 0.6 + 0.525 + 0.45 = 1.575$. We select learning rates $\eta = \sqrt{\tfrac{\log(|\mathcal{B}|)}{|\mathcal{B}|T}} \approx 0.005$ and $\eta = \sqrt{\tfrac{\log(|\mathcal{B}|)}{T}} = 0.002$ for the full information and bandit settings respectively (and for the $\textsc{EXP3-IX}$ estimator, we choose an exploration rate of $\sqrt{\frac{2\log(|\mathcal{B}|/\delta)}{4|\mathcal{B}|T}} \approx 0.003$, for high probability bound parameter $\delta = 0.05$). 

% Figure environment removed

In Figure \ref{fig:exp2_bids}, we plot the average value of each bid over time. Here, the bidder's objective is to learn the optimal bid vector under each of our three algorithms:  decoupled exponential weights algorithm (Algorithm \ref{alg: Decoupled Exponential Weights}) for the full information, modified (i.e., EXP3-IX) version of decoupled exponential weights algorithm (Algorithm \ref{alg: Decoupled Exponential Weights - Path Kernels}) for the bandit setting, and modified version of the OMD algorithm  (Algorithm \ref{alg: OMD}) for the bandit setting. In this figure, we further compare the rate of convergence to the optimal bid vector of $[0.4, 0.3, 0.1]$ with our three algorithms. We observe that the full information decoupled exponential weights algorithm converges the fastest to the optimal bid, and the bandit feedback decoupled exponential weights algorithm converges the slowest, and the bandit feedback mirror descent algorithm is in between. This behavior is consistent with our theoretical findings. 

We repeat this experiment for the algorithms to learn in uniform price auctions described in \cite{brnzei2023online}. Though we do not perform the calculations, the optimal bid vector in the uniform price setting is still $[0.4, 0.3, 0.1]$. We note that it takes noticeably longer for the bandit algorithm to converge as compared to either its full information variant or our Algorithms \ref{alg: Decoupled Exponential Weights - Path Kernels} or \ref{alg: OMD}, as predicted by the looser regret upper bounds:
\begin{theorem}[(Discrete) Regret in Uniform Price Auctions, \cite{brnzei2023online}] \label{thm: uniform price regret full}
    Under full information feedback (resp. bandit feedback), there exists an algorithm which achieves $O(M^{\frac{3}{2}}\sqrt{T \log |\mathcal{B}|})$ (resp. $O(M^{\frac{5}{2}}|\mathcal{B}|T^{\frac{1}{2}}\log|\sqrt{\log |\mathcal{B}|} + M^2 \log |\mathcal{B}|)$) discrete regret.
\end{theorem}


\subsection{Uniform Price Market Dynamic Analysis}
\label{sec: uniform dynamics}

In this section, we provide some additional experiments or analyses in the market dynamics for the uniform price as prescribed in Section \ref{sec:dynamic} in order to directly compare to the dynamics of the PAB auction. In particular, we run a more complete analysis of the uniform price auction bidding dynamics that parallels Section \ref{sec:dynamic}.

\textbf{Uniform Price Learning Dynamics}: Here, we repeat the setup and analyses of Section \ref{sec:dynamic} except using the uniform price auction with bidders bidding according to the learning algorithms as prescribed in \cite{brnzei2023online}. 

\textbf{Bid Dynamics.} In Figure \ref{fig:unif_exp3_bids}, we observe that the winning bids and largest losing bids noticeably diverge, indicating that the regret minimizing bid strategies are non-uniform (and so are the true hindsight optimal bid vectors which we verified using an offline optimization protocol described in \cite{brnzei2023online}). In particular, as in Section~\ref{sec:dynamic}, there are $N=3$ bidders, $\overline{M} = M = 5$ items, the bid space is $\mathcal{B} = \{\frac{i}{20}\}_{i \in [20]}$. The valuations (which are drawn i.i.d. $\text{Unif}(0, 1)$ which are then sorted) for this specific instance are given by $\bm{v}_1 = [0.89, 0.7, 0.55, 0.51, 0.29], v_2 = [0.89, 0.44, 0.2, 0.12, 0.05], v_3 = [0.67, 0.64, 0.45, 0.27, 0.02]$. For convenience and ease of comparison, we include the PAB bidding dynamics counterpart (Figure~\ref{fig:exp3_bids}) as well.

% Figure environment removed

\textbf{Welfare and Revenue Over Time.} In Figure \ref{fig:unif_rev_wel_over_time}, we compare the distribution of welfare and revenue (normalized by maximum welfare) of the uniform price auction over time showing the 10th, 25th, 50th, 75th, and 90th percentiles in different shades. In particular, we run the full information and bandit feedback learning algorithms for the uniform price auction \cite{brnzei2023online}. Note that this figure parallels Figure \ref{fig:rev_wel_over_time} under PAB. We observe that the welfare rapidly converges to 1 in both the full information and bandit feedback settings. However, the revenue under bandit feedback has noticeably larger variance compared to the full information revenue. Once again, to compare the welfare and revenue evolution over time with the PAB auction, we include the PAB counterpart (Figure~\ref{fig:rev_wel_over_time}) for convenience.

% Figure environment removed


\subsection{Time Varying Valuations}

\label{sec: time varying}

We extend Algorithms \ref{alg: Decoupled Exponential Weights} and \ref{alg: Decoupled Exponential Weights - Path Kernels} to the time varying valuations setting. In particular, we assume that the valuations $\bm{v}$ are no longer fixed, and instead, in every round $t$, $\bm{v}^\nround$ is independently  drawn i.i.d. from some known distribution $F_{\bm{v}}$ with discrete, finite support $\mathcal{V}$. 
This contextual setting requires a  stronger  benchmark oracle in comparison to our original setup with a fixed valuation. The new benchmark oracle, which we will formalize shortly, possesses knowledge of the hindsight optimal bid vector for each context. That is, under this benchmark, we have the optimal mapping from any context (valuation vector) to an action (bid vector).
Consequently, our current definitions of $\textsc{Regret}$ and $\textsc{Regret}_{\mathcal{B}}$ need to be updated to accommodate these contextual factors:
\begin{align}
\tag{Continuous Contextual Regret}
    \textsc{Regret}(F_{\bm{v}}) = \max_{\bm{b}: \mathcal{V} \to [0, 1]^{+\Nitem}} \sum_{\nround=1}^\Nround \mathbb{E}_{\bm{v} \sim F_{\bm{v}}}[\mu^\nround_n(\bm{b}(\bm{v}); \bm{v})] - \mathbb{E}\left[\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround; \bm{v}^\nround)\right]\,.
\end{align}
Here, $\mu^\nround_n(\bm{b}; \bm{v})$ denotes the utility of bidder $n$ by submitting bid vector $\bm{b}$ with valuations $\bm{v}$ at round $t$ where the competing bids are $\bm{b}_{-}^{t}$. Observe that in the benchmark of $ \textsc{Regret}(F_{\bm{v}})$, i.e., $\max_{\bm{b}: \mathcal{V} \to [0, 1]^{+\Nitem}} \sum_{\nround=1}^\Nround \mathbb{E}_{\bm{v} \sim F_{\bm{v}}}[\mu^\nround_n(\bm{b}(\bm{v}); \bm{v})]$, we abuse notation and define valuation-to-bid vector mapping $\bm{b}: \mathcal{V} \to [0, 1]^{+M}$.  %$\bm{b}^*(\bm{v})$ denotes the hindsight optimal bid under valuation vector $\bm{v}$.
We have an equivalent definition of discretized contextual regret:
\begin{align}
\tag{Discretized Contextual Regret}
    \textsc{Regret}_\mathcal{B}(F_{\bm{v}}) = \max_{\bm{b}:\mathcal {V}\to
    \mathcal{B}^{+\Nitem}} \sum_{\nround=1}^\Nround \mathbb{E}_{\bm{v} \sim F_{\bm{v}}}[\mu^\nround_n(\bm{b}(\bm{v}); \bm{v})] - \mathbb{E}\left[\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround; \bm{v}^\nround)\right]\,.
\end{align}
An agent's goal is to minimize their contextual regret with respect to their valuation distribution $F_{\bm{v}}$. Using naive contextual bandit algorithms would lead to a large regret, as the regret of these algorithms  scales  with the square root of the number of contexts. However, we make an observation that we have complete cross-learning over these contexts as in \cite{ContextBanditsCrossLearning2019}. As such, we borrow  from the results described in \cite{ContextBanditsCrossLearning2019}; specifically those explaining the cross-learning-across-contexts generalizations of the $\textsc{EXP3}$ algorithm in the stochastic contexts (valuations) and adversarial rewards setting (adversarial competing bids). %We assume a known, finite valuation distribution.

We assume that the agent has access to their valuation distribution. Moreover,  as stated earlier, we assume that the support of this valuation distribution is finite; i.e., $|\mathcal{V}| < \infty$. This scenario occurs often in practice where bidders' valuations depend naturally on some natural events. For example, investors may prescribe a `low' or `high' value to certain assets depending on various market indices. 

We generalize the $\textsc{EXP3-CL}$ algorithm described in \cite{ContextBanditsCrossLearning2019} to our PAB setting, specifically Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels}, and  achieve exactly the same regret rates as our non-contextual variants, albeit requiring an additional $O(|\mathcal{V}|)$ factor of memory and computation.

In order to make the generalization more clear, at a high level, the $\textsc{EXP3-CL}$ algorithm on a set of $K$ arms and $C$ contexts with full cross learning constructs a reward estimator $\hat{r}(k; c) = \frac{r(k; c)}{\sum_{c}\prob(c)\prob(k^t = k | c^t = c)}\textbf{1}_{k^t = k}$ for each arm $k$ and context $c$ pair. Here, the term $\sum_{c}\prob(c)\prob(k^t = k | c^t = c)$ is the expected probability that arm $k^t = k$ was selected under context $c^t = c$, where in the summation we take   expectation  with respect to the stochasticity over contexts $c$. This estimator mirrors that of standard $\textsc{EXP3}$ using the IPW estimator, except that the IPW is averaged over the context distribution. 

To apply this to our setting, we wish to mimic the behavior of the $\textsc{EXP3-CL}$ algorithm with our decoupled exponential weights algorithm. This can be done by running the $\textsc{EXP3-CL}$ estimator on all of the nodes $b \in \mathcal{B}$ within each layer $m \in [M]$. In particular, we use the following estimator  $\widehat{w}_m^t(b; \bm{v}) = 1 - \frac{1 - w_m^t(b; \bm{v})}{Q_m^t(b)} \textbf{1}_{b_m^t = b}$, where the  normalizer $Q_m^t(b) = \sum_{\bm{v} \in \mathcal{V}} \prob(\bm{v}^t = \bm{v}) q_m^t(b; \bm{v})$ in this estimator 
is the expected probability of selecting bid $b$ with corresponding valuation $\bm{v} = \bm{v}^t$, where the expectation is taken with respect to all valuation vectors $\bm{v} \in \mathcal{V}$. This procedure, formally described in Algorithm~\ref{alg: Decoupled Exponential Weights - Time Varying Known Finite} yields the following regret upper bound:

\begin{theorem}[Time Varying Valuations - Decoupled Exponential Weights] \label{thm: time varying known finite}
    Under bandit feedback (resp. full information feedback), Algorithm~\ref{alg: Decoupled Exponential Weights - Time Varying Known Finite}, with appropriately chosen $\eta$, achieves contextual continuous regret $\textsc{Regret}(F_{\bm{v}})$ of order $O(\Nitem^\frac{3}{2} \sqrt{\Nround \log \Nround})$ (resp. $O(\Nitem \sqrt{\Nround \log \Nround})$) with total time time and space complexity polynomial in $M$, $|\mathcal{B}|$, $|\mathcal{V}|$, and $\Nround$.
\end{theorem}

\begin{proof}{Proof of Theorem \ref{thm: time varying known finite}}
    
To begin, we can once again `decouple' the utility per unit-bid pair, but this time conditional on the valuation vector context. In particular, we have:
\begin{align*}
    \mu_n^t(\bm{b}; \bm{v}) =  \sum_{m=1}^M w_m^t(b_m; \bm{v}) = \sum_{m=1}^M (v_m - b_m)1_{b_m \geq b^t_{-m}} \quad \text{and} \quad \widehat{\mu}_n^t(\bm{b}; \bm{v}) =  \sum_{m=1}^M \widehat{w}_m^t(b_m; \bm{v})\,.
\end{align*}
%As such, one need only maintain one weight estimate per unit-bid-valuation triple. In other words, for each $m \in [M], b \in \mathcal{B}, v \in \mathcal{V}_m$, where $\mathcal{V}_m$ denotes the support of $v_m$ in $F_{\bm{v}}$, w
As stated earlier, we define reward-weight estimates based on Equation (6) of \cite{ContextBanditsCrossLearning2019} and our Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels}: %\negin{This seems like a description of the algorithm, rather than a proof }
\begin{align*}
    \widehat{w}_m^t(b; \bm{v}) = 1 - \frac{1 - w_m^t(b; \bm{v})}{\sum_{\bm{v} \in \mathcal{V}} \prob(\bm{v}^t = \bm{v}) q_m^t(b; \bm{v})} \textbf{1}_{b_m^t = b} = 1 - \frac{1 - w_m^t(b; \bm{v})}{Q_m^t(b)} \textbf{1}_{b_m^t = b}\,.
\end{align*}
Here, $q_m^t(b; \bm{v}) = \prob(b^t_m = b | \bm{v}^t = \bm{v}) = \sum_{\bm{b}: b^t_m = b} \prob(\bm{b}^t = \bm{b} | \bm{v}^t = \bm{v})$ is the probability of selecting bid $b$ in slot $m$ with valuation $\bm v$. Similarly, $Q_m^t(b)$ is the probability of selecting bid $b$ for unit $m$, averaged across all possible valuations. One can verify unbiasedness of this estimator $\mathbb{E}[\hat{w}_m^t(b; \bm{v})] = w_m^t(b; \bm{v})$ for all $m \in [M], b \in \mathcal{B}, \bm{v} \in \mathcal{V}$. The second moment can similarly be computed as:
\begin{align*}
    \mathbb{E}[\widehat{w}_m^t(b; \bm{v})^2] = \mathbb{E}\left[\left( 1 - \frac{1-w_m^t(b; \bm{v})}{Q_m^t(b)} \textbf{1}_{b_m^t = b} \right)^2 \right] = 1 - 2\mathbb{E}\left[\frac{1-w_m^t(b; \bm{v})}{Q_m^t(b)}\textbf{1}_{b_m^t=b}\right] + \mathbb{E}\left[\left(\frac{1-w_m^t(b; \bm{v})}{Q_m^t(b)}\right)^2\textbf{1}_{b_m^t=b}\right]\,.
\end{align*}

Evaluating the expectations and recalling that $\mathbb{E}[\textbf{1}_{b_m^t = b}] = Q_m^t(b)$, we have:
\begin{align*}
    \mathbb{E}[\widehat{w}_m^t(b; \bm{v})^2] = 1 - \left[2 - 2w_m^t(b; \bm{v})\right] + \left[\frac{(1 - w_m^t(b; \bm{v}))^2}{Q_m^t(b)}\right] = 2w_m^t(b; \bm{v}) - 1 + \frac{1}{Q_m^t(b)} \leq 1 + \frac{1}{Q_m^t(b)} \leq \frac{2}{Q_m^t(b)}\,.
\end{align*}

Using this, the proof largely follows that of Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels} up until Equation (\ref{eq: full info difference}). In particular, we have that the contextual regret can be written as:
\begin{align*}
    \textsc{Regret}_\mathcal{B}(F_{\bm{v}}) &= \mathbb{E}_{F_{\bm{v}}}\left[\sum_{t=1}^{T} \mu_n^t(\bm{b}'; \bm{v}^t) - \sum_{t=1}^{T} \mathbb{E}[\mu^t(\bm{b}^\nround; \bm{v}^t)]\right]\\
    &\lesssim \eta^{-1}M\log|\mathcal{B}| + \eta \mathbb{E}_{F_{\bm{v}}}\left[\sum_{\nround=1}^\Nround \sum_{\bm{b}} \prob(\bm{b}^\nround =\bm{b}| \bm{v}^t = \bm{v}) \mathbb{E}[(\sum_{m=1}^M \widehat{w}^\nround_m(b_m; \bm{v}))^2]\right]\\
    &= \eta^{-1}M\log|\mathcal{B}| + \eta \left[\sum_{\nround=1}^\Nround \sum_{\bm{v} \in \mathcal{V}} \prob(\bm{v}^t = \bm{v})\sum_{\bm{b}} \prob(\bm{b}^\nround =\bm{b}| \bm{v}^t =  \bm{v}) \mathbb{E}[(\sum_{m=1}^M \widehat{w}^\nround_m(b_m; \bm{v}))^2]\right]\\
    &= \eta^{-1}M\log|\mathcal{B}| + \eta M\left[\sum_{\nround=1}^\Nround \sum_{m=1}^M \sum_{\bm{v} \in \mathcal{V}} \prob(\bm{v}^t = \bm{v})\sum_{b \in \mathcal{B}} \mathbb{E}[\widehat{w}_m^t(b; \bm{v})^2]  \sum_{\bm{b}: b_m = b} \prob(\bm{b}^\nround =\bm{b}| \bm{v}^t = \bm{v})\right]\\
    &= \eta^{-1}M\log|\mathcal{B}| + \eta M\left[\sum_{\nround=1}^\Nround \sum_{m=1}^M \sum_{\bm{v} \in \mathcal{V}} \prob(\bm{v}^t = \bm{v})\sum_{b \in \mathcal{B}} \mathbb{E}[\widehat{w}_m^t(b; \bm{v})^2]  q_m^t(b; \bm{v})\right]\\
    &= \eta^{-1}M\log|\mathcal{B}| + 2\eta M\left[\sum_{\nround=1}^\Nround \sum_{m=1}^M \sum_{\bm{v} \in \mathcal{V}} \prob(\bm{v}^t = \bm{v})\sum_{b \in \mathcal{B}} \frac{1}{Q_m^t(b)}  q_m^t(b; \bm{v})\right]\\
    &= \eta^{-1}M\log|\mathcal{B}| + 2\eta M\left[\sum_{\nround=1}^\Nround \sum_{m=1}^M \sum_{b \in \mathcal{B}} \frac{1}{Q_m^t(b)} \sum_{\bm{v} \in \mathcal{V}} \prob(\bm{v}^t = \bm{v}) q_m^t(b; \bm{v})\right]\\
    &= \eta^{-1}M\log|\mathcal{B}| + 2\eta M\left[\sum_{\nround=1}^\Nround \sum_{m=1}^M \sum_{b \in \mathcal{B}} \frac{1}{Q_m^t(b)} Q_m^t(b)\right]\\
    &\leq \eta^{-1}M\log|\mathcal{B}| + \eta M^2|\mathcal{B}|T\,.
\end{align*}
(We will show the first inequality shortly.)
With $\eta = \Theta(\sqrt{\frac{\log |\mathcal{B}|}{M|\mathcal{B}|T}})$,  this yields  the  discretized contextual regret upper bounds of $O(M^{\frac{3}{2}}\sqrt{|\mathcal{B}| T \log |\mathcal{B}|})$ under the bandit setting. Accounting for the rounding error of order $O(\frac{MT}{|\mathcal{B}|})$, we obtain the stated continuous contextual regret upper bounds. 
To obtain the full information results, we simply replace $\widehat{w}_m^t(b_m; {\bm v}^t)$ with $w_m^t(b_m; {\bm v}^t)$ in the second line of the above equations, which leads to  the discretized contextual regret upper bounds of $O(M^{\frac{3}{2}}\sqrt{ T \log |\mathcal{B}|})$, as desired.


Next, following the proof of Algorithm \ref{alg: Decoupled Exponential Weights - Path Kernels}, we show   the first inequality. 
We define the potentials with respect to a fixed valuation vector $\bm{v}$: $\Phi^t(\bm{v}) = \sum_{\bm{b} \in \mathcal{B}^{+\Nitem}} \exp(\eta \sum_{\tau=1}^{t} \widehat{\mu}^\tau(\bm{b}; \bm{v}^\tau))$. Taking the ratio of adjacent terms, we obtain:
\begin{align*}
    \frac{\Phi^t(\bm{v})}{\Phi^{t-1}(\bm{v})} = \sum_{\bm{b} \in \mathcal{B}^{+M}} \frac{\exp(\eta \sum_{\tau=1}^{t-1} \widehat{\mu}^\tau(\bm{b}; \bm{v}^\tau))}{\Phi^{t-1}(\bm{v})} \exp(\eta \widehat{\mu}^t(\bm{b}; \bm{v}^t)) = \sum_{\bm{b} \in \mathcal{B}^{+M}} \prob(\bm{b}^\nround = \bm{b} | \bm{v}^\nround = \bm{v}) \exp(\eta \widehat{\mu}^t(\bm{b}; \bm{v}^\nround))\,,
\end{align*}
Where in the last equality, we used the condition that our algorithm samples bid vector $\bm{b}$ with probability proportional to $\exp(\eta \sum_{\tau=1}^{t-1} \widehat{\mu}^\nround(\bm{b}; \bm{v}))$ at round $\nround$ with valuations $\bm{v}^t = \bm{v}$. Combining this with inequalities $\exp(x) \leq 1 + x + x^2$ and $1 + x \leq \exp(x)$ for all $x \leq 1$, we obtain:
\begin{align*}
    \frac{\Phi^t(\bm{v})}{\Phi^{t-1}(\bm{v})} \leq \sum_{\bm{b} \in \mathcal{B}^{+M}} \prob(\bm{b}^\nround = \bm{b} | \bm{v}^\nround = \bm{v}) \exp(\eta \widehat{\mu}^t(\bm{b}; \bm{v})) \leq \exp(\sum_{\bm{b} \in \mathcal{B}^{+M}} \prob(\bm{b}^\nround = \bm{b} | \bm{v}^\nround = \bm{v}) \left[\eta\widehat{\mu}^t(\bm{b}; \bm{v}) + \eta^2 \widehat{\mu}^t(\bm{b}; \bm{v})^2 \right])\,.
\end{align*}
Combining this with Equations~\ref{eq: Potentials} and the fact that $\Phi^0(\bm{v}) = M\log |\mathcal{B}|$, for any fixed bid vector $\bm{b}'$, we have:
\begin{align*}
    \sum_{t=1}^{T} \widehat{\mu}^t(\bm{b}'; \bm{v}) - \sum_{t=1}^{T} \sum_{\bm{b}} \prob(\bm{b}^\nround = \bm{b}| \bm{v}^t = \bm{v})  \widehat{\mu}^t(\bm{b}; \bm{v}) &\lesssim \eta^{-1} \Nitem \log |\mathcal{B}| + \eta \sum_{t=1}^T \sum_{\bm{b}}\prob(\bm{b}^\nround = \bm{b} | \bm{v}^\nround = \bm{v}) \widehat{\mu}^t(\bm{b}; \bm{v})^2\\
    &= \eta^{-1} \Nitem \log |\mathcal{B}| + \eta \sum_{t=1}^T \sum_{\bm{b}}\prob(\bm{b}^\nround = \bm{b} | \bm{v}^\nround = \bm{v}) (\sum_{m=1}^M \widehat{w}_m^t(b_m; \bm{v}))^2\,.
\end{align*}
Taking expectations over $\bm{b}$ and the supremum over all $\bm{b}'$ yields the desired first crucial regret inequality.

As for the time and space complexity, notice that the only algorithmic difference between Algorithm~\ref{alg: Decoupled Exponential Weights - Time Varying Known Finite} and Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels} is precisely in computing the estimator, which in the former, requires having to compute the weights $Q_m^t(b)$ by iterating over all $\bm{v} \in \mathcal{V}$. As we also have to store reward estimates for each possible valuations, both the time complexity and space complexity of Algorithm~\ref{alg: Decoupled Exponential Weights - Time Varying Known Finite} are a factor $|\mathcal{V}|$ larger than in Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels}, which are $O(M|\mathcal{B}| |\mathcal{V}| T)$ and $O(M|\mathcal{B}| |\mathcal{V}|)$ respectively.


\end{proof}

\begin{algorithm}[t]
\footnotesize
	\KwIn{Learning rate $0 < \eta < \frac{1}{M}$, Valuation Distribution $F_{\bm{v}}$}
	\KwOut{The aggregate utility $\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b}^{\nround}; \bm{v}^\nround)$}
	$\widehat{W}_\nitem^0(b; \bm{v}) \gets 0$ for all $\nitem \in [\Nitem], b \in \mathcal{B}, \bm{v} \in \mathcal{V}$ such that $b \leq v_m$; else $\widehat{W}_\nitem^0(b; \bm{v}) \gets -\infty$.\; 
	\For{$\nround \in [1,\ldots,\Nround]$:}{
            \textbf{Observe Valuation Vector $\bm{v}^t \sim F_{\bm{v}}$}\;
            $b_{0}^t \gets \max \mathcal B$, and $\widehat{S}_{M+1}^t (\min \mathcal{B}; \bm{v}^t)=1$ for any $t\in[T]$\;
            %Adversary selects $\bm{b}^{\nround}_-$.
            \textbf{Recursively Computing Exponentially Weighted Partial Utilities $\bm{S}^t$}\;
            \textbf{for} $m \in [M,\ldots,1], b \in \mathcal{B}: \widehat{S}^t_\nitem(b; \bm{v}^t) \gets \exp(\eta \widehat{W}_\nitem^\nround(b; \bm{v}^t)) \sum_{b' \leq b} \widehat{S}_{\nitem + 1}^\nround(b'; \bm{v}^t)$ \hspace{0mm} $\backslash \backslash$ $\textsc{Compute}-\widehat{S}_\nitem$\;
        \textbf{Determining the Bid Vector $\bm{b}^\nround$ Recursively}\;
        \textbf{for} $m \in [1,\ldots,M], b \leq b_{m-1}^t: b_\nitem^\nround \gets b$ with probability $\frac{\widehat{S}^t_\nitem(b; \bm{v}^t)}{\sum_{b' \leq b_{\nitem-1}^t} \widehat{S}^t_{\nitem}(b'; \bm{v}^t)}; $ \hspace{1mm} $\backslash \backslash$ $\textsc{Sample}-\bm{b}$\;
        %$\bm{b}^{\nround} \gets \textsc{Sampler}(\bm{W}^{\nround-1}, \eta)$\;
        Observe $\bm{b}^{\nround}_-$ and receive reward $\mu_n^\nround(\bm{b}^{\nround}; \bm{v}^t)$\;
        $Q_m^t(b) \gets 0$ for all $m \in [M], b \in \mathcal{B}$\;
        \For{$\bm{v} \in \mathcal{V}$:}{
            \textbf{Recursively Computing Probability Measure $\bm{q}$ Under $\bm{v}\in {\mathcal V}$}\;
            $\widehat{S}^t_{M+1}(b; \bm{v}) \gets 1$ for all $m \in [M], b \in \mathcal{B}$\;
            \textbf{for} $m \in [M,\ldots,1], b \in \mathcal{B}: \widehat{S}^t_\nitem(b; \bm{v}) \gets \exp(\eta \widehat{W}_\nitem^\nround(b; \bm{v})) \sum_{b' \leq b} \widehat{S}_{\nitem + 1}^\nround(b'; \bm{v})$\;
            $q^t_1(b; \bm{v}) \gets \frac{\widehat{S}^\nround_m(b; \bm{v})}{\sum_{b' \in \mathcal{B}} \widehat{S}^\nround_m(b'; \bm{v})}$ for all $b \in \mathcal{B}$\;
            \textbf{for} $m \in [2,\ldots,M], b \in \mathcal{B}: q_\nitem^\nround(b; \bm{v}) \gets \sum_{b' \geq b} \frac{q_{\nitem-1}^t(b'; \bm{v})\widehat{S}^\nround_{\nitem}(b; \bm{v})}{\sum_{b" \geq b'} \widehat{S}^\nround_\nitem(b"; \bm{v})}$ for all $b \in \mathcal{B}$\;
            $Q_m^t(b) \gets Q_m^t(b) + \prob(\bm{v}^t = \bm{v})q_m^t(b; \bm{v})$
        }
        \textbf{Update Weight Estimates}\;
        \textbf{if} $\textsc{Bandit Feedback}$, \textbf{for} $m \in [M], b \in \mathcal{B}, \bm{v} \in \mathcal{V}$\; 
        $\widehat{W}^{\nround+1}_{\nitem}(b; \bm{v}) \gets \widehat{W}^{\nround}_{\nitem}(b; \bm{v}) + (1 - \frac{1 - (v - b)\textbf{1}_{b \geq b^t_m}}{Q_m^t(b)} \textbf{1}_{b^t_m = b})$ if $b \leq v$; else $\widehat{W}^{\nround+1}_{\nitem}(b; \bm{v}) \gets -\infty$\;
        \textbf{if} $\textsc{Full Information}$, \textbf{for} $m \in [M], b \in \mathcal{B}, \bm{v} \in \mathcal{V}$\;
        $\widehat{W}^{\nround+1}_{\nitem}(b; \bm{v}) \gets \widehat{W}^{\nround}_{\nitem}(b; \bm{v}) + (v - b)\textbf{1}_{b \geq b^t_m}$ if $b \leq v$; else $\widehat{W}^{\nround+1}_{\nitem}(b; \bm{v}) \gets -\infty$\;
        }
        \textbf{Return} $\sum_{\nround=1}^\Nround \mu_n^{\nround}(\bm{b}^{\nround}; \bm{v}^\nround)$
	\caption{\textsc{Decoupled $\textsc{EXP3-CL}$ - Time Varying Valuations}}
	\label{alg: Decoupled Exponential Weights - Time Varying Known Finite}
\end{algorithm}
