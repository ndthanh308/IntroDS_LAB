
\section{Conclusions}

\subsection{Regret Lower Bound}

Observations:
\begin{enumerate}
    \item For a fixed $m$, we have $w_\nitem^\nround(b) \leq w_\nitem^\nround(b + \epsilon) + \epsilon$. This will also hold in expectation over $t$. This is because of the monotone allocation rule.
    \item For a fixed $b$, we have $w_\nitem^\nround(b) - v_\nitem \geq w_{\nitem+1}^\nround(b) - v_{\nitem+1}$. This holds also in expectation over $t$. This is due to the monotonicity of the adversary bids. In particular, we have that in expectation, $\prob(b_{-\nitem} \leq x) \geq \prob(b_{-(\nitem+1)} \leq x)$.
    \item 
\end{enumerate}

\subsection{Granularity Selection}

One important outstanding question is how bidders should select $\mathcal{B}$. In general, unless the auctioneer has specified a discrete set of allowable bid values (and tie-breaking rule), the participants do not need to select the same discretization. As such, agent $n$ selects $\mathcal{B}$ as a function of their theoretical revenue sub-optimality due to discretization and their regret guarantees under the assumption that $\bm{b}^{-n, \nround} \in \mathcal{B}^{-\Nitem} \equiv [0, 1]^{-\Nitem}$ and $\pi^\nround \in [0, 1]$. More specifically, we consider a stronger regret benchmark of the following form:
\begin{align*}
    \textsc{Regret}^{n, \Nround} = \max_{\bm{b}^n \in [0, 1]^\Nitem} \sum_{\nround=1}^\Nround \mu(\bm{v}^{n, \nround}, \bm{b}^n, \bm{b}^{-n, \nround}, \pi^{\nround}) - \mathbb{E}_{\bm{b}^{n, \nround} \sim F^{n, \nround}} \sum_{\nround=1}^\Nround \mu (\bm{v}^{n, \nround}, \bm{b}^{n, \nround}, \bm{b}^{-n, \nround}, \pi^\nround)
\end{align*}
To address this, we first derive the set of possible (approximate) Nash equilibria in this auction when agent $n$'s bid space is $\mathcal{B}^\Nitem$ and all others with $[0, 1]^{\Nitem}$. For simplicity, let $\mathcal{B} \equiv \{\frac{i}{D}\}_{i \in [D]}$ be an even discretization of $[0, 1]$. Let $v^\nround_{(\nitem)}$ denote the $m$'th largest valuation among bidders at round $\nround$ and let $\lfloor v \rfloor_D$ denote the largest multiple of $\frac{1}{D}$ at most $v$ and similarly, let $\lceil v \rceil$ denote the smallest multiple of $\frac{1}{D}$ at least $v$. We argue that the $\Nitem$ largest bids at time $\nround$ denoted by $b^\nround_{(1)} \leq \ldots \leq b^\nround_{(\Nitem)}$ in the auction must be between $\lfloor v^\nround_{(\nitem+1)} \rfloor_D$ and $\lceil v^\nround_{(\nitem+1)} \rceil_D$. If bid $b^\nround_{(\nitem)}$ is strictly less than $v^\nround_{(\Nitem+1)}$, then the bidder whose valuation corresponds to $v^\nround_{(\Nitem + 1)}$ could benefit by increasing their highest losing bid to $v^\nround_{(\Nitem+1)} - \epsilon > b^\nround_{(\nitem)}$ and gaining $\epsilon$ utility. Hence, all other bidders aside from $n$ will bid at exactly $v^\nround_{(\Nitem+1)}$ for slots corresponding to valuations larger than $v^\nround_{(\Nitem+1)}$. Bidder $n$ will bid $b^{n, \nround}_\nitem = \lceil v^\nround_{(\Nitem+1)} \rceil_D$---their lowest possible winning value---for all slots such that $v^{n, \nround}_\nitem \geq \lceil v^\nround_{(\Nitem+1)} \rceil_D$. If instead, bidder $n$ were allowed to bid any value in $[0, 1]$, their optimal strategy would follow that of other bidders. Thus, the aggregate sub-optimality of restricting one's bids to discrete set $\mathcal{B}$ is equal to the extra paid for units won minus the utility lost for items whose marginal values lied between $v^\nround_{(\Nitem+1)}$ and $\lceil v^\nround_{(\Nitem+1)}\rceil_D$. Note that in either case, the loss per unit is upper bounded by $\frac{1}{D}$, and thus, the total loss is upper bounded by $\frac{1}{D} \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem (v^{n, \nround}_\nitem \geq v^\nround_{(\Nitem+1)})$. Hence, the loss is larger for agents with larger valuations. Nonetheless, we can upper bound this loss by $\frac{\Nitem \Nround}{D} = \frac{\Nitem \Nround}{|\mathcal{B}|}$. Letting  $\mathcal{B} \equiv \{\frac{i}{D}\}_{i \in [D]}$ with $|\mathcal{B}| = D$, we balance this sub-optimality term with the regret and achieve the following results:
\begin{corollary}
    Under the full information case, selecting $|\mathcal{B}| = \sqrt{T}$, Algorithm~\ref{alg: Decoupled Exponential Weights} achieves $\textsc{Regret}^{n, \Nround} \lesssim \Nitem \sqrt{\Nround \log \Nround}$.
\end{corollary}

\begin{corollary}
    Under bandit feedback and selecting $|\mathcal{B}| = \Nround^{\frac{1}{3}}$, Algorithm~\ref{alg: Decoupled Exponential Weights - Bandit Feedback} achieves $\textsc{Regret}^{n, \Nround} \lesssim \Nitem \Nround^{\frac{2}{3}} \sqrt{\log \Nround}$.
\end{corollary}

\subsection{Regret Lower Bounds}

\begin{theorem}
    With respect to the continuous regret benchmark, we can obtain a regret lower bound of $\Omega(\Nitem^{\frac{2}{3}}\Nround^{\frac{2}{3}}$, which is a factor of $\Nitem^{\frac{1}{3}}$ away from our regret upper bound up to logarithmic factors. 
\end{theorem}

\begin{proof}
    We partition $\mathcal{B}$ into $\Nitem+1$ 
\end{proof}


\rigel{A note: For the case of single item FPA with valuation $v$ and bid space $\mathcal{B}$, the regret lower bound is $\Omega(\sqrt{|\mathcal{B}|\Nround})$. The proof from "http://www.cs.cornell.edu/courses/cs683/2007sp/papers/oppa.pdf" can be adapted since the reward function of the posted price auction from auctioneer's perspective is simply the reversal of that of the FPA from the bidder's perspective (flip the domain [0, 1]).}

\rigel{Still incomplete. General formula to get lower bounds: Construct a hard family of instances, such that there is a small number (or single) of actions with optimal utility that is $\epsilon$ better than that of the remaining actions. 1) For each instance, compute the expected regret under a single $\pi$ as a function of $\epsilon$ and the probability of selecting these optimal actions dictated by the instance. 2) Lower bound the maximum regret over all instances by the mean regret over all instances. 3) Upper bound the number of times the optimal action(s) are selected. Compute the an upper bound on the loss in each instance. This usually involves information theoretic arguments or concentration inequalities. 4) Optimize over $\epsilon$.}

\begin{enumerate}
    \item Say we have valuations $v_m = 1$ for all $m$. The adversary bids are $b_{-m} = 1 - \frac{1}{m}$, except for one $m^*$ for which their bid is $b_{-m^*} = 1 - \frac{1}{m^*} - \frac{\epsilon}{m^*}$. Note that $\epsilon < \frac{1}{m^* - 1} - \frac{1}{m^*} = O((m^*)^{-2})$. Assume that $\mathcal{B}$ is fine grained enough to have negligible error due to discretization.
    \item Note that bidding at any value (assuming we win all tiebreaks) of $1 - \frac{1}{m}$ will win exactly $m$ items, each one yielding a utility of $\frac{1}{m}$ per item, with a total overall utility of 1. However, for $m^*$, we can win $m^*$ items to achieve utility of $m^*(\frac{1}{m^*} + \frac{\epsilon}{m^*}) = 1 + \epsilon$.
    \item Alternatively, we can split the bid space into $\Nitem$ continguous, disjoint regions of size $\frac{|\mathcal{B}|}{\Nitem}$. Learning each of these separately yields a regret of $O(\sqrt{\frac{|\mathcal{B}|}{\Nitem}\Nround}$  each, and since there are $\Nitem$ of them, has regret of $O(\sqrt{\Nitem |\mathcal{B}|\Nround})$. This requires that $|\mathcal{B}| > \Nitem$ though.
\end{enumerate}


\begin{conjecture}
    The hindsight optimal bid vector (assuming fixed valuations as in the bandit setting) may not necessarily be characterized by a price and quantity, despite the optimal action each round being able to be characterized as such.
\end{conjecture}

\begin{proof}
    Let valuations be [1.0, 0.5] for all rounds, with $|\mathcal{B}| = 100$. Let there be 2 items. The adversary bids are [0.5, 0.5], [0.25, 0.25]. The optimal bid would be [0.51, 0.26], which yields utilities of $0.49$ from the first auction and $0.49 + 0.01$ from the second for a total of $0.99$.
\end{proof}

\rigel{Note that Node-O-REPS algorithm works more generally for arbitrary layered graph with arbitrary reward structure within each layer, which we can use to take off the bid and valuation monotonicity assumption. However, we would need to give the allocation rule in the auction more thought in this case; perhaps greedily allocating to the feasible highest bid per item combination?}

\subsection{Discussion on Side Information}

While some results utilize the observation that any bids larger/smaller than winning/losing bids they will also be winning/losing bids \cite{LearningBidOptimallyAdversarialFPA2020, OptimalNoRegretFPA2020} \rigel{Cite cross learning negin paper}, we do not utilize this additional side information and assume that agents only observe the utility corresponding to their submitted bid vector. To our knowledge, it is unclear how to utilize the side information to construct meaningfully lower variance, unbiased utility estimates of either entire bid vectors or of individual slot-bid pairs. While the decoupled sampler algorithm for the full information setting works for any adversarially selected valuation profiles as, in the full information setting, we have perfect cross learning between valuations \rigel{Cite cross-learning across contexts paper}, the bandit setting makes Hedge style algorithms break down and it is difficult to make use of side information. In particular, both variants of the $\textsc{O-REPS}$ algorithms did not utilize any cross learning over valuations or over bids. Cross learning over bids by applying algorithms such as $\textsc{Exp3.G}$ or $\textsc{Exp3.SET}$ is difficult as the feedback graph is never revealed, i.e. the agent never observes the reserve nor the adversary's lowest winning bid and highest losing bid. Without knowledge of $\mathcal{V}^{-1}_\nround$, the agent cannot construct unbiased utility estimates of bid vectors, let alone per-slot utility estimates. Conversely, cross learning over valuations is difficult as FTRL algorithms, such as $\textsc{O-REPS}$, have an updating step which is difficult to decouple due to the projection step. This is one interesting future avenue of investigation. \rigel{Are there any results on FTRL with contexts? If there are, there is hope for decoupled contextual O-REPS}

Regarding the difficulty of cross learning assuming both non-stationary valuations and adversarial adversary actions, we note that \rigel{Cite Yanjun Han paper} proved a related result regarding the impossibility of achieving sublinear (in $\Nround$) regret when the adversary's actions $\bm{b}^{-n, \nround}$ and $\pi^\nround$ are dependent on $\bm{v}^{n, \nround}$ in the bandit setting, regardless if these valuations were generated adversarially or stochastically. They give a simple example---that extends straightforwardly to the multi-unit setting---with guaranteed linear regret under any learning algorithm. They provide an efficient algorithm that utilizes graph feedback for an arm elimination algorithm in the case of $\Nitem = 1$ under i.i.d. $b^{-n, \nround}_1$. It is unknown whether their result can be extended to the case of adversarially generated $b^{-n, \nround}_1$ or higher dimensional $\bm{b}^{-n, \nround}$ as in our setting. Interestingly, their results showed that it is possible to utilize cross learning without always observing the entire graph feedback structure, which many existing similar cross-learning algorithms require. In contrast to their adversarial valuation and i.i.d. environment assumption, we assumed adversarial environment and fixed valuation profile in the bandit setting. As such, an interesting open question is how to utilize cross learning to allow our algorithms to operate under the adversarial or non-stationary valuations.



\section{Time Varying-Valuations}



\newpage

\section{Materials for 1/9/2023}

\textbf{Learning Across \textit{m} Blocks}

We can achieve a regret lower bound of $O(M\sqrt{|\mathcal{B}|T})$ by using a blocking argument, assuming $|\mathcal{B}| = \Omega(M^2)$. We generalize this to a strategy that achieves a regret lower bound of $O(\sqrt{M^k|\mathcal{B}|T})$ assuming $|\mathcal{B}| = \Omega(M^{k+1})$.


\begin{enumerate}
    \item For the case of $|\mathcal{B}| = \Omega(M^2)$, we partition $\mathcal{B}$ into $M+1$ disjoint, contiguous regions $I_\nitem = \{B_{\nitem(M-1) + 1},\ldots,B_{\nitem M}\}$, with $I_{\Nitem+1} = \{B_{M^2+1},\ldots,B_{|\mathcal{B}|}\}$, where $B_1 > \ldots > B_{|\mathcal{B}|}$. Assume that $B_{\Nitem^2} = c$ for $c \neq 0$ and all valuations $v_1,\ldots,v_M$ are equal to 1.
    \item We can construct a distribution over highest-other-bids such that the expected rewards for slot $\nitem$ at any bid value $B \in I_\nitem$ are equal, say at $\frac{1}{2}$, except for $B^*_\nitem \in I_\nitem$, for which the reward is $\frac{1}{2} + \epsilon$. \rigel{Reconstruct the proof for the Kleinberg knowing the worth of demand curve paper; construct in such a way that the reward outside of these intervals is strictly less than $\frac{1}{2}$}{\color{red} in your construction, what is the reward for any $B\notin I_m$?}
    \item As the $I_\nitem$'s are disjoint, we can consider the task of learning in $M$ separate but simultaneous first price auctions. In particular, we reveal to the learner that the optimal bid vector $\bm{B}^*$ satisfies $B^*_\nitem \in I_\nitem$ for all $\nitem \in [\Nitem]$. Hence, from monotonicity and disjointness of the $I_\nitem$, the learner participating in $M$ separate first price auctions needs only consider bids that satisfies monotonicity.
    \item Since each auction yields a regret of $O(\sqrt{|\mathcal{B}| T})$ {\color{red} present the reduction clearly to explain how you obtain the regret of $O(\sqrt{|\mathcal{B}| T})$}, the aggregate regret is given by $O(M\sqrt{|\mathcal{B}|T})$ {\color{red} why?}. We can extend the above argument by assuming that $|\mathcal{B}| = \Omega(M^{k+1})$ and letting $I_\nitem = \{B_{\nitem(M^k - 1) + 1},\ldots,B_{\nitem M^k}\}$, which will yield a regret of $O(\sqrt{M^k|\mathcal{B}|T})$. {\color{red} for such a claim, you need to show your constructions can be extended.} \rigel{See where this generalization can fail}
\end{enumerate}

% Figure environment removed

\newpage

\textbf{Constructing \textit{m} Blocks with Equal Reward Distribution per Block}: We now construct a probability distribution $\bm{p}$ over adversary bid vectors, with marginal probability distributions $\bm{p}_1,\ldots,\bm{p}_M$ with corresponding expected rewards $r_1,\ldots,r_M$. Here, $p_m(b)$ is the probability that the adversary's $m$'th bid $b_{-m}$ takes on value $b$, which yields expected reward $r_m = \mathbb{E}_{\bm{p}_1,\ldots,\bm{p}_M}[1_{b > b_{-m}}(v_m - b)] = \prob_{b_{-m} \sim \bm{p}_m}(b > b_{-m}) (v_m - b)$. Here, we assume $v_1 = 1, v_2 = 1 - \frac{1}{2M}, \ldots, v_M = \frac{M+1}{2M}$. Additionally, we assume $|\mathcal{B}| = 2MK$ for some positive integer $K$, so that each block is of size $K$. In particular, we let $B_m = \frac{m}{2MK}$ for $m \in [2MK]$. We let block $m$ be denoted as $I_m = \{B_j\}_{(M-m)K < j \leq (M-m+1)K}$. These blocks corresponds to regions on $\mathbb{R}$; e.g. $I_1$ corresponds to bids in the region $(\frac{1}{2} - \frac{1}{2M}, \frac{1}{2}]$, $I_2$ to $(\frac{1}{2} - \frac{2}{2M}, \frac{1}{2} - \frac{1}{2M}]$, $\ldots$, and $I_M$ to $(0, \frac{1}{2M}]$. We assume that the optimal bid vector $\bm{b}^* = (b^*_1,\ldots,b^*_M)$ is such that $b^*_m \in I_m$, and this information is revealed to the bidder, so they only have to consider bid vectors $\bm{b} = (b_1,\ldots,b_M)$ such that $b_m \in I_m$. 

\textbf{Regret Lower Bound for Each Slot} We now bound the regret obtained for each slot $m$, assuming that the distribution of adversary bids for slot $m$ were contained strictly within block $I_m$ (we will show shortly that this is actually impossible given the equal-reward requirement, though we will address this in the next paragraph). Following the proof in Kleinberg's "The Value of Knowing a Demand Curve: Bounds on Regret for On-line Posted-Price Auctions" regarding the regret of a posted price auction (and the reduction to first price auctions in "Contextual Bandits with Cross Learning"), the regret of learning in a first price auction with $K$ possible evenly-spaced bids in $[0, 1]$ over $T$ rounds is lower bounded with $\Omega(\sqrt{KT})$. This was shown by constructing an adversary bid distribution such that the reward distribution of each bid is equal to the $\textsc{Unif}(\{0, 1\})$ distribution, except for one bid $b^*$, whose reward is equal to 1 with probability $\frac{1}{2} + \epsilon$ and 0 otherwise. This reward is, on average, $\epsilon$ larger than that of the other arms. In their construction, there was an inherent constraint that $\epsilon \leq \frac{1}{K}$, as the average reward cannot grow by more than $\frac{1}{K}$ on intervals of size $\frac{1}{K}$. Now, we map their construction to our setting. In our setting, we have $K$ uniformly spaced bids over regions of size $\frac{1}{2M}$, each of whose rewards is at most $\frac{1}{2}$ by construction of our $v_1,\ldots,v_M$. Scaling up our bid space by a factor of $2M$, we have $K$ uniformly spaced bids over a region over $[0, 1]$, each of whose rewards is at most $M$. Scaling the rewards down by a factor of $M$, we obtain exactly the first price auction setting of Kleinberg/Golrezaei. Hence, we obtain a regret of $\Omega(\sqrt{KT})$ for each slot, which multiplying by $M$, yields regret lower bound $\Omega(M\sqrt{KT}) = \Omega(M \sqrt{\frac{|\mathcal{B}|T}{2M}}) = \Omega(\sqrt{M|\mathcal{B}|T})$, which is a factor of $\sqrt{M}$ away from optimal. Unfortunately, as mentioned at the beginning of the paragraph, this is true only if the adversary bids were contained within $I_m$ almost surely. Nonetheless, we can still obtain this regret bound with some additional work.

\textbf{Constructing $\bm{p}$}: We first consider a trivial construction that we will use to construct $\bm{p}$. We construct useful distribution $\bm{p}^c = (p_1,\ldots,p_K)$ over bids in $\mathcal{B}^c = \{c + \frac{1}{2MK}, \ldots, c + \frac{2K}{2MK}\}$. We will run a first price auction over this region, with valuation $v^c = c + \frac{M+1}{2M}$, such that the adversary distribution is given by $\bm{p}^{\textsc{base}}$ and yields $\prob_{b_c \sim \bm{p}^{c}}(b' > b_c) (v^c - b') = \prob_{b_c \sim \bm{p}^{c}}(b > b_c) (v^c - b)$ for all $b, b' \in \mathcal{B}^c$. By construction, we can assume $c = 0$, since $c$ only specifies a horizontal shift of the bid space and valuation. We claim that the distribution $\bm{p}^c$ defined with $p_{M-i} = \frac{MK}{(MK+i)(MK + i + 1)}$ yields that the expected reward $\prob_{b_c \sim \bm{p}^{c}}(b > b_c) (v^c - b) = \frac{1}{2}$. One may then think that we can set each $\bm{p}_m$ to be the appropriately $c$-shifted $\bm{p}^c$, however, the adversary bid monotonicity prevents us from doing so. That being said, we can define a secondary distribution $\bm{p}^d$ that will help us to construct our desired $\bm{p}$, with equal expected reward for fixed $m$, varying $b$. In particular, we select an adversary bid as follows: select $m \in [M]$ with probability $q_m$ (probability distribution $\bm{q}$), let all of the adversary bids in the first $m-1$ slots follow distribution $\bm{p}^d$ (with appropriate shifting $d$), the adversary bid in slot $m$ to follow distribution $\bm{p}^c$, and the remaining bids to be equal to 0.

\rigel{TODO}: Recompute the bid distributions as a function of $d$ and $m$





\newpage

\textbf{Optimal Static Reserve}

We now consider the problem of selecting a reserve price as to obtain good expected revenue guarantees. We must make a distinction, though, on what information is known to the auctioneer. If the auctioneer is aware of the valuation distributions of the bidders and is allowed to select reserves for each item $\nitem$, then we can employ static pricing policies based on multi-unit prophet inequalities (see Alaei 2011, Chawla 2020) that obtain competitive ratio $O(1 - \sqrt{\frac{\log \Nitem}{\Nitem}})$. Of course, this is optimistic, as the auctioneer often does not have access to the valuation distributions of the bidders. Instead, the auctioneer must sequentially learn the optimal reserve price(s) by joining the bid optimization game as an agent who seeks to optimize their reserve price(s). In the static (announced) reserve setting, the optimal reserve would be equal to $\pi^* = \max_{\nitem \in [\Nitem]} \nitem v_{(\nitem)}$, where $v_{(\nitem)}$ is the $\nitem$'th highest valuation. The auctioneer, restricting their choice of reserve to $\Pi$, achieves regret $O(\sqrt{|\Pi|T})$. However, there are two shortcomings of this approach. First, this benchmark is too strong when the reserves are not announced before the auction, as the agents cannot bid 'rationally' with respect to the selected reserve---i.e. bidders might bid underneath the reserve even if their valuation exceeds it. Second, announcing the reserve prior to the auction introduces reserves as contexts, which would change how the learning algorithms for the bidders would work.


\subsection{Reduction to State-Occupancy Measures}

One may realize that as the rewards associated with each edge are independent of its initial location, we may hope to further simplify the problem by assuming proportional conditional probability mass functions. That is, letting $B' \leq B$, we enforce that $\{\pi((\nitem, B), B")\}_{B" \leq B'} \propto \{\pi((\nitem, B'), B")\}_{B" \leq B'}$ for all $\nitem \in [\Nitem]$. We let $\Pi'$ denote the set of all $\Pi$ that fulfill this condition. This condition can be written succinctly as $\frac{\pi((\nitem, B), B')}{\pi((\nitem, B), B")} = \frac{\pi((\nitem, B'), B')}{\pi((\nitem, B'), B")}$ for all $\nitem$ and $B" \leq B' \leq B$. Letting $B_0$ denote the maximal possible bid, these conditions can be written even more concisely as $\bigcap_{\nitem=1}^\Nitem \bigcap_{B' \in \mathcal{B}} \bigcap_{B" \leq B'} \{\frac{\pi((\nitem, B_0), B')}{\pi((\nitem, B_0), B")} = \frac{\pi((\nitem, B'), B')}{\pi((\nitem, B'), B")}\}$. Using the fact that $\sum_{B' \leq B} \pi((\nitem, B), B') = 1$, we have:
\begin{align}
    &\frac{\pi((\nitem, B_0), B')}{\pi((\nitem, B_0), B")} = \frac{\pi((\nitem, B'), B')}{\pi((\nitem, B'), B")} \\
    &\leftrightarrow \pi((\nitem, B_0), B')\pi((\nitem, B'), B") = \pi((\nitem, B'), B')\pi((\nitem, B_0), B")\\
    &\to \sum_{B" \leq B'} \pi((\nitem, B_0), B')\pi((\nitem, B'), B") = \sum_{B" \leq B'} \pi((\nitem, B'), B')\pi((\nitem, B_0), B") \\
    &\leftrightarrow \pi((\nitem, B_0), B') = \pi((\nitem, B'), B') \sum_{B" \leq B'} \pi((\nitem, B_0), B")\\
    &\leftrightarrow \pi((\nitem, B'), B') = \frac{\pi((\nitem, B_0), B')}{\sum_{B" \leq B'} \pi((\nitem, B_0), B")}\\
    &\to \pi((\nitem, B), B') = \frac{\pi((\nitem, B_0), B')}{\sum_{B" \leq B} \pi((\nitem, B_0), B")}
\end{align}
Where in the last equality, we plugged in the value of $\pi((\nitem, B'), B')$ back into the proportionality constraint. At a high level, this allows us to describe the entire policy with only $\{\pi(S_0, B')\}_{B' \in \mathcal{B}} \cup \{\pi((\nitem, B_0), B')\}_{\nitem \in [\Nitem], B' \in \mathcal{B}}$, which is a set of size $O(\Nitem |\mathcal{B}|)$. Letting $\psi_\nitem(B) = \pi((\nitem-1, B_0), B)$, we see that we can rewrite $\pi((\nitem, B), B') = \frac{\psi(\nitem+1, B')}{\sum_{B" \leq B} \psi(\nitem+1, B")}$. Once again abusing notation, let $\bm{B} \sim \psi$ denote a bid vector sampled according to the policy generated from $\psi$. Furthermore, letting $\Psi: [\Nitem] \times \mathcal{B} \to [0, 1]$ such that $\sum_{B \in \mathcal{B}} \psi_\nitem(B) = 1$ denote the set of possible condensed policies $\psi$, we can recursively define the state occupancy measure $q^\psi(\nitem, B) = \prob_{\bm{B} \sim \psi}(s_\nitem = B_\nitem)$. With base case $q^\psi(1, B') = \psi(S_0, B')$, we have:



\newpage

Agenda for 1/3/2023:

\begin{enumerate}
    \item Get rid of reserves? Doesn't add much if we're not considering revenue or welfare
    \item We can get a regret lower bound of $O(\sqrt{M|\mathcal{B}T})$ by using the learning across $M$ disjoint blocks argument
\end{enumerate}

We can try generalizing the above argument to have shrinking blocks with each $\nitem$. For example, the first row goes from $B_0$ to $\frac{1}{2}$, and the remaining rows go from $B_{\nitem \Nitem}$ to $\frac{1}{2}$. It is possible to construct a distribution of highest other bids such that the expected reward for each slot-value pair inside of these blocks is equal and bounded above by $\frac{1}{2}$, and that the slots to the right also have reward that is bounded above by $\frac{1}{2}$ but decreasing linearly (though we ignore these). It is possible to slightly perturb this distribution so that the reward at specific bid value index $i_m$ is $\epsilon$ larger than that of the remaining bid values in the same layer (where $i_m$ is monotone increasing in $m$). We can do this without affecting the validity of the other bids by only perturbing downwards the bid distribution where there is a highest other bid at exactly the index selected for that layer at exactly and only this index. Hence, the excess reward of any bid vector is going to be equal to $\epsilon$ times the number of correctly selected bid value indices. Now we have to figure out the KL divergence of this distribution, yikes.


TODO:
\begin{enumerate}
    \item Get rid of $\bm{v}^t$ altogether, assume that it's the same throughout and then change at theend mentioning that the full information setting can generalize to arbitrary $v$
    \item Define macros for $W$ to be similar to $\mu$
    \item State in each result which regret benchmark is used
    \item Change the approximation factor in our algorithm by restricting ourselves to quantity * price. What's the worst a fixed quantity * price can do.
    \item Move the granularity selection stuff inside of the algorithms themselves and then refer to the appendix.
    \item get rid of $\pi$ and use this as a reward
    \item How do the CCE and NE differ with/without reserve?
    \item In Algorithm 3, remove environment and just say adversary selects. Simplify the wording
    \item Get rid of the O-REPS algorithm and make notation consistent
\end{enumerate}

\if 0
\begin{enumerate}
    \item General idea: construct a family of hard instances, indexed by a price $B \in \mathcal{B}$ and a quantity $\nitem \in [\Nitem]$. The adversary bids are fixed to be $\lfloor B \rfloor$ for the first $\nitem$ bids and set to be 1 for the remaining bids, where $\lfloor B \rfloor$ denotes the largest bid $B' \in \mathcal{B}$ that is \textit{strictly} smaller than $B$. Assume that your valuations for all items is $B + \epsilon < \lceil B \rceil$.
    \item If the agent chooses the correct $B$... but bids for more items, then they get maximum utility... So if the agent chooses the incorrect $B$:
    \begin{enumerate}
        \item Say bids at $B' > B$ too high (assuming does not violate bidding over valuation constraint). Say $m'$ items are won. If $m > m'$ (because bid for fewer items), then loss is $m'(B' - B) + (m'-m)$
    \end{enumerate}
\end{enumerate} 

\fi


\if 0
\subsection{Regret Lower Bounds}



We will prove that the regret of any learning algorithm for bandit feedback bid optimization is lower bounded with $\Omega(\Nitem \sqrt{ |\mathcal{B}|\Nround})$, which matches the regret upper bound of $\textsc{Node O-REPS}$ up to a logarithmic $\sqrt{\log |\mathcal{B}|}$ factor. We do this by constructing a hard family of MDPs as in \rigel{Cite Domingues 2021, Osband and Van Roy 2016, Foster 2020}. In their work, they consider the task of learning in episodic MDPs with horizon $H$, states $\mathcal{S}$, actions $\mathcal{A}$, stage-dependent transition probabilities or rewards, and $\Nround$ episodes. They provide a regret lower bound of $\Omega(\sqrt{H^3 SA\Nround})$ where $S = |\mathcal{S}|$ and $A = |\mathcal{A}|$. However, as our setting is simpler, we expect to obtain a lower regret bound. In particular, the transitions/rewards are stage independent, the transitions are deterministic, the MDP has edges with a layered and monotonic structure, and the rewards of each action-state pair only depends on the action and follow a piece-wise zero or linearly decreasing function of the action taken. As such, we adapt their proof of learning over the set of all models $\mathcal{M}$ only considering this simpler setting. We now construct a hard instance of MDPs $\mathcal{M}' \subset \mathcal{M}$ and lower bound the regret of the previous task of learning over all models $\mathcal{M}$ with learning over all models in $\mathcal{M}'$.

\subsection{Constructing a Hard Family of MDPs}

\rigel{The fact that the utilities obtained each slot is monotonic (since valuations and adversary bids are also monotonic) really might make this problem easy. That is, we need to bid higher for later items which have lower marginal value. This makes it a very special type of MDP.}

\rigel{As an aside, maybe can try something similar to this except where valuations and bids are allowed to be non-monotonic, and the auctioneer instead selects the largest feasible bid/number of items ratio to allocate the items to. Knapsack learning in combinatorial auctions?}

\rigel{Describe the MDP/edge/reward structure again here}

Consider the set of MDPs in $\mathcal{M}$ where the reward is set to be exactly 0 for all actions $((\nitem-1, B), B')$ for $\nitem \in [\Nitem-1]$ and $B \geq B' \in \mathcal{B}$. This corresponds to the case where $v_\nitem = b_{-\nitem} = 1$ for all $\nitem \in [\Nitem-1]$. We let the reward for the last slot $\Nitem$ be given by the piece-wise zero or linearly decreasing function for $v_\Nitem = B + \epsilon$ and $b_\Nitem = B$ for any $B \in \mathcal{B}$ and some small $\epsilon > 0$ such that $B = \max\{B' \in \mathcal{B}: B' \leq B + \epsilon\})$.


\if 0
\begin{proposition}
    \rigel{Cite Domingues Paper?} Let $\mathcal{M}$ denote the class of tabular MDPs with $S$ states, $A$ actions, time horizon $H$, and $\sum_{h=1}^H r_h \in [0, 1]$. If $S \geq 2, A \geq 2, H \geq 2\log(\frac{S}{2})$, and $\gamma \in \Theta(HS)$, then for any decision making algorithm $\textsc{DM}$ learning on $\mathcal{M}$ over $\Nround$ rounds, we have that:
    \begin{align}
        \mathbb{E}(\textsc{Regret}_{\textsc{DM}}^\Nround) \geq \Omega(\sqrt{HS\Nround})
    \end{align}
\end{proposition}

\begin{proof}
    Define the class of MDPs $\mathcal{M}' \equiv \{\mathcal{M}_s\}_{h \in \mathcal{H}', s \in \mathcal{S}'}$.
\end{proof}
\fi

\newpage

% \rigel{Ok this is significantly more involved than I thought, come back to this later (thoughts commented out). Using Domingues's method (which is the largest regret lower bound that I've seen for episodic MDPs with stage-independent transition probability), we have a lower bound of regret of $O(\sqrt{H^2 SAT}$, where there are $T$ rounds, $H = M$, state space size $S = O(\Nitem |\mathcal{B}|)$, action space size $A = O(|\mathcal{B}|)$, for a total of $O(\Nitem^{\frac{3}{2}}|\mathcal{B}|\sqrt{T}$. We need to somehow incorporate 1. known deterministic transition probability transitions and 2. layered MDP structure, and 3. strictly action-dependent rewards.}

\fi