\section{Experiments}
\label{sec: experiments}



In our experiments, we simulate the market dynamics induced by our learning algorithms; see the performance of our algorithms under a stochastic setting in Section \ref{sec:stochastic}. To better give context into the meaning of these experiments, we briefly discuss the notions of coarse correlated equilibria. We also provide specifics as to the slight modifications made in our algorithms to improve its empirical performance.


\textbf{Coarse Correlated Equilibrium (CCE).} In our experiments, we simulate the market dynamics in which every agent behaves according to our algorithms. This will allow us  to obtain some insight into the structure, welfare, and revenues of the PAB CCEs recovered by our algorithms. CCEs are  solution concepts that generalize Nash equilibria by allowing for dependence between bidder strategies. It is well known that the time-averaged behavior of agents running no-regret learning algorithms converges to a CCE and that these CCEs possess strong welfare and revenue guarantees in smooth games, such as PAB auctions \cite{syrgkanis2012composable, InefficiencyStandard2013, roughgarden2015smooth, PriceOfAnarchyInAuctions2017, feldman2017correlated}. While there are some limiting results describing the efficiency, revenue, and structure of Bayes-Nash equilibria of PAB auctions \citep{OptimalBidding1995, InefficiencyStandard2013, Homogeneous2020}, the CCEs have eluded an analytic characterization. We conduct several simulations of market dynamics under these no-regret learning algorithms to better understand the properties of these CCEs.

\textbf{Algorithm Implementation.} In our experiments, we run a slightly modified version of our  algorithms in the bandit feedback setting. We do this as the variance of the regret of our algorithms is high, as the node weight estimators normalize over small probabilities $q^t_m(b)$. To mitigate the effect of such normalization, we use the implicit  $\textsc{EXP3-IX}$ estimator as described in \citep{neu2015explore, Lattimore2020}. Under this estimator, rather than reward estimate $\widehat{w}_m^t(b^t_m)$ of selecting bid $b^t_m$ for unit $m$ at time $t$ by $q^t_m(b_m^t)$, we instead normalize it by $q^t_m(b_m^t) + \gamma$. That is, we use node reward estimator $\widehat{w}_m^t(b) = \frac{w_m^t(b)}{q_m^t(b) + \gamma}\textbf{1}_{b=b_m^t}$ for specially chosen $\gamma > 0$ (see Section (\ref{sec: IX})) in our OMD algorithm. (Note that in the standard $K$-armed bandit setting, despite being a biased estimator, this algorithm still achieves the same sublinear expected regret guarantee with a smaller variance.) 
Aside from this modified estimator, the remainder of the Algorithm \ref{alg: OMD} remains the same.  

\textbf{Experiments.} To that end, we analyze the bidding behavior of multiple learning agents and the induced market dynamics under full information with Algorithm~\ref{alg: Decoupled Exponential Weights} and under bandit feedback with Algorithm~\ref{alg: OMD} (see Section \ref{sec:dynamic}). Note that we omit the bandit feedback decoupled exponential weights (Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels}) as its regret guarantees are dominated by the OMD variant both theoretically and empirically (See Section~\ref{sec:stochastic}). Similarly, we omit the full information OMD algorithm as the convex optimization step is prohibitively computationally expensive, considering the marginal improvement in regret. We additionally compare the PAB market dynamics to the uniform pricing dynamics recovered under the no-regret learning algorithms described in \cite{brânzei2023online}.

\subsection{PAB Market Dynamics} \label{sec:dynamic}

In this experiment, we let there be $N=3$ bidders, $\overline{M} = M = 5$ items, all valuations drawn from $\text{Unif}(0, 1)$ which are then sorted, and the bid space is $\mathcal{B} = \{\frac{i}{20}\}_{i \in [20]}$, with higher indexed bidders receiving tie-break priority. We run the market dynamics where each agent is using the decoupled exponential weights algorithm under full information (Algorithm~\ref{alg: Decoupled Exponential Weights}, with $T=10^4, \eta = \sqrt{\frac{\log |\mathcal{B}|}{T}} = 0.036$)
and the OMD  algorithm under bandit feedback (Algorithm~\ref{alg: OMD} with $T = 10^5$ and $\eta = \sqrt{\tfrac{\log |\mathcal{B}|}{|\mathcal{B}|T}} = 0.0008$).
 


\textbf{Bids Dynamics.} In Figure \ref{fig:exp3_bids}, we analyze the bids over time of the market dynamics induced when all agents bid according to our  Algorithms \ref{alg: Decoupled Exponential Weights}  and \ref{alg: OMD}. 
We observe  that the winning bids (and largest losing bid) converge to approximately the same value. Informally, while the converged prices are slightly different,  our learning algorithms induce market dynamics under which prices are almost the same for all the bidders. (See also the left plot of Figure  \ref{fig: bid ratios} for a similar observation.) 

% Figure environment removed

\textbf{Utilities Dynamics.} In Figure  \ref{fig:exp3_utility},   we analyze the time averaged utility of the agent in the market dynamics induced when all agents bid according to  Algorithm \ref{alg: Decoupled Exponential Weights} under the full info setting and Algorithm  \ref{alg: OMD} under the bandit setting. In both the full information and bandit feedback settings, the utilities converge to the optimal utilities  overtime; albeit significantly faster in the full information setting. As a consequence, the algorithms converge to a welfare optimal CCE. We also note that the hindsight optimal utility for each bidder differs across the two settings, potentially due to the different prices converged to.  
One interesting observation is that the shape of the regret curves are similar for both settings, indicating that the agents learned similar sets of strategies under, albeit more slowly and with higher variance in the bandit feedback setting. 

% Figure environment removed

\textbf{Distribution of Revenues.}
In Figure \ref{fig: rev distribution}, we compare the distribution of per-item time averaged revenue under Algorithm ~\ref{alg: Decoupled Exponential Weights} for the full information setting and Algorithm~\ref{alg: OMD} for the bandit setting. Here, we normalize by either the per-item average welfare or by the $\overline{M}$'th and $\overline{M}+1$ largest valuations (i.e., $v_{(\overline{M})}$ and $v_{(\overline{M}+1)}$). We also plot both the time averaged and last iterate versions to see that the bids did indeed converge last-iterate wise. We note that while the revenues are generally lower in the bandit setting, the distribution of revenue of both algorithms generally maintain the same shape and it is not too heavy tailed towards low revenue. In fact, in most cases, the revenue returned is at least half of the maximum welfare. This is hopeful considering that the impact of strategizing and bid shading is large as there are a small number $N=3$ agents. We further note that revenue recovered is actually \textit{smaller} than both $v_{(\overline{M})}$ and $v_{(\overline{M}+1)}$. This suggests that in some cases, there exists an agent whose valuation exceeds the clearing price, but does not actually learn to win this item; as they have foregone exploring this possibility in order to lower their payments for units they are already winning. Indeed, in several instances, some agents fail to learn their hindsight optimal bid and incur large regret. 
\footnote{We note that the occasional non-convergence to 0 regret does not contradict our results as our algorithms only guarantee convergence of the \textit{expected} regret. See Section 11.5 in \cite{Lattimore2020}.} 


% Figure environment removed




\textbf{Welfare and Revenue Over Time.} In Figure \ref{fig:rev_wel_over_time}, we further  compare the distribution of welfare and revenue over time showing the 10th, 25th, 50th, 75th, and 90th percentiles in different shades under Algorithm~\ref{alg: Decoupled Exponential Weights} and Algorithm~\ref{alg: OMD}. We normalize both welfare and revenue by the maximum possible welfare (sum of the largest $M$ valuations) in each instance. In comparison to the full information version, it takes longer for the bidders to settle to an approximately welfare maximizing steady state in the bandit setting. Furthermore, the revenue at the recovered steady state under bandit feedback is lower than that of the full information setting, albeit with lower variance.%, consistent with the findings in the previous revenue distribution plots.


% Figure environment removed



\textbf{Impact of Competition.} In Figure \ref{fig:comp}, we compare the distribution of welfare and revenue over time showing the 10th, 25th, 50th, 75th, and 90th percentiles  in different shades under  Algorithm~\ref{alg: OMD} for the bandit setting for a varying number of market participants $N \in \{12, 48\}$ with $T = 10^4$. Compared with the previous experiment with $N = 3$ bidders, we see that as $N$ grows larger, there is less incentive for agents to bid strategically as there is more competition, and thus, the agents' bids, welfare, and revenue converge more quickly.  Additionally, the increased competition also improves the revenue, as expected.


% Figure environment removed

\subsection{PAB vs. Uniform Price Auction Market Dynamics}\label{sec:experiment:uniform}

In the following set of experiments, we compare the bids, welfare, and revenue of the no-regret market dynamics recovered in the uniform price auction (using the same parameterizations $N=3, M=5, |\mathcal{B}| = 20, T=10^5$ and valuations as for the PAB learning algorithms). More specifically, we implement the no-regret learning algorithm for the uniform price auction as described in \cite{brânzei2023online}, with the payment equal to the smallest winning bid. (An alternative approach would be to set the payment equal to the largest losing bid. However, as argued in \cite{LastAcceptedBid2020, brânzei2023online}, this strategy yields low revenue under Nash Equilibria. This insight has inspired us to set the payment equal to the smallest winning bid in our implementation of uniform price auctions.) 

In our implementation of these algorithms, we impose no-overbidding constraint just as we had in the PAB setting, which prevents the learning dynamics from converging to a non-IR equilibrium. Similarly, we also use the $\textsc{EXP3-IX}$ based estimator in the bandit uniform price algorithm to be consistent with the PAB implementation. {In Sections~\ref{sec:stochastic} and \ref{sec: uniform dynamics}, we run additional experiments that provide more detail regarding the regret rates, bid dynamics, and the evolution of revenue and welfare of the uniform price auction.}



\textbf{Time Averaged Welfare and Revenue.} In Figure \ref{fig:welfare_revenue_comparison_box_plot}, we present a direct comparison of the time-averaged welfare and revenue (normalized by maximum welfare) between the PAB and uniform price auctions, considering both full information and bandit feedback scenarios. The plot includes the median (represented by a horizontal line), mean (indicated by an `x'), 25th and 75th percentiles, as well as the minimum and maximum values for welfare and revenue.
Our observations reveal that in both full information and bandit feedback settings, the uniform price dynamics yield slightly higher welfare but significantly lower revenue compared to the PAB dynamics. This disparity is further amplified in the bandit feedback scenario. Specifically, in bandit feedback, the mean welfare for the uniform price auction is 0.980 (with a standard deviation of 0.028), while the PAB mechanism achieves a mean welfare of 0.952 (with a standard deviation of 0.049). Similarly, the mean revenue for the uniform price auction is 0.481 (with a standard deviation of 0.194), whereas the PAB mechanism achieves a mean revenue of 0.626 (with a standard deviation of 0.091). It is worth noting that the variance in uniform price revenue under bandit feedback is particularly concerning, as the revenue can be as low as 8\% of the maximum welfare, compared to 39\% under the PAB mechanism. Conversely, the minimum welfare achieved under bandit feedback for the PAB mechanism is 77\%, while the uniform price auction attains a minimum welfare of 86\%.






The slight decrease in welfare observed in PAB can be attributed to the increased incentive for bidders to understate their bids in order to reduce their payment. In PAB, the payment and utility for each individual unit directly depend on the bid placed on that specific unit. On the other hand, in the uniform price auction, the payment is determined by the lowest winning bid, and there is consequently less motivation to manipulate higher bids since they are less correlated with the payment. This observation aligns with findings from other variants of discriminatory versus uniform pricing, such as generalized first and second price auctions. Conversely, the improved stability and higher revenue observed in PAB can be attributed to the ability of bidders to strategically shade their bids on a per-unit basis. In PAB, bidders have more direct control over their payments compared to the uniform price auction, where the payment per unit can fluctuate significantly depending on a single clearing price. This increased control allows bidders to optimize their bids strategically and leads to enhanced stability and higher revenue in the PAB mechanism.





% Figure environment removed

\textbf{Convergence of Bids.} In Figure \ref{fig: bid ratios}, we compare the ratios of (i) the largest to the smallest winning bid and (ii) the smallest winning bid to the largest losing bid. We plot these ratios over time  for both the PAB and uniform price auctions, under bandit feedback only (we exclude full information feedback as the differences in welfare and revenue are negligible). We make the crucial observation that while both ratios converge to 1 ($\log_2 1=0$ in the plot) in the PAB auction, the ratio of the largest to smallest winning bid in the uniform price auction does not converge to 1. 
The convergence of these ratios for the PAB setting indicate that the optimal bidding strategy for the PAB auction is uniform bidding. In contrast, the non-convergence for the uniform price setting suggests that the optimal bidding in the uniform price auction requires discriminatory bidding (different prices for each unit). See our discussions in Section \ref{sec:insights} regarding the practical implications of this observation.

Furthermore, we observe that  the ratio of the smallest winning to largest losing bids converges noticeably more slowly under the uniform price auction. This slower convergence can be attributed to the increased number of edge weights $O(M|\mathcal{B}|^2)$ required to learn in the uniform price auction as compared to the $O(M|\mathcal{B}|)$ node weights in the PAB auction. 
% Figure environment removed





