\section{Bid Optimization: Bandit Feedback}


{\color{red}
\begin{itemize}
\item explain what we did in the full info setting and why that idea does not work here when we have bandit setting
\item at the high level, we want to convert the EXP3 algorithm we designed with the help of the DP formulation to the bandit setting.
\item on natural idea that has been presented in the prior work is the path kernel method that uses the idea of weight pushing. See also XXX (such as combinatorial bandits paper)
\item if we apply these algorithms, we get sub-optimal regret in terms of $M$.
\item this is because our problem has additional structure that we can use. 
\item explain the structure and may want to  present a lemma (remark or observation works too)
\item our algorithm presented in XXX uses this structure 
\item Explain the algorithm. 
\item Tell how/where it uses the structure/the DP formulation 
\item talk about the optimization problem
\item at a high level, this alg bears some resemblance to Component hedge (weight pushing), combinatorial m-sets, O-REPS. But, that algorithm (as it was the case for the kernel method) fails to use the structure and leads to a sub-optimal regret.  
\end{itemize}
}


\begin{algorithm}[t]
	\KwIn{Family of probability measures over states $\bm{\psi}: [\Nitem] \times \mathcal{B} \to [0, 1]$. }
	\KwOut{Bid vector $\bm{b}$.}
        \textbf{Recover Policy $\bm{\pi}$ from Condensed Policy $\bm{\psi}$}: $\pi(S_0, b) \gets \psi_1(b)$ and $\pi((m, b), b') \gets \frac{\psi_{m+1}(b')}{\sum_{b" \leq b} \psi_{m+1}(b)}$ for all $m \in [\Nitem], b \geq b', b, b' \in \mathcal{B}$.\;
        \textbf{Sampling $\bm{b}$ Recursively}: Set $b_1$ to $b \in \mathcal{B}$ with probability  $\pi(S_0, b)$.\;
        \For{$\nitem = 1,\ldots,\Nitem-1$:}{
            Set $b_{\nitem+1}$ to $b \in \mathcal{B}$ with probability $\pi((\nitem, b_\nitem), b)$
        }
        \textbf{Return} $\bm{b}$
	\caption{\textsc{Monotone Bid Sampling in Multi-Unit Pay-as-Bid Auctions}}
	\label{alg: Sample Node O-REPS}
\end{algorithm}

\begin{algorithm}[t]
	\KwIn{Valuation $\bm{v} \in [0, 1]^\Nitem$, and Learning rate $\eta > 0$. } %Adaptive Adversarial Environment $\textsc{Env}^\nround: \mathcal{H}^\nround \to \mathcal{B}^{-\Nitem} \times \mathcal{B}$ where $\mathcal{H}^\nround$ denotes the set of all possible historical auction results $H^\nround$ up to round $\nround$ for all $\nround \in [\Nround]$.}
	\KwOut{The aggregate utility $\sum_{\nround=1}^\Nround \mu(\bm{b}^\nround)$.}
	$\psi^0(\nitem, b) \gets \frac{1}{|\mathcal{B}|}$ for all $\nitem \in [\Nitem], b \in \mathcal{B}$. Let $\bm{q}^0 \in [\Nitem] \times \mathcal{B} \to [0, 1]$ be the corresponding state-action {\color{red} let's call this ``unit-bid" pair or sth like that} occupancy measure.%, where $q^\nround_\nitem(b)$ is the probability of selecting bid $b$ at state $\nitem$ \;
	\For{$\nround \in [\Nround]$:}{
            %Adversary selects $\bm{b}^{\nround}_-$ and 
            \textbf{Determining the Bid Vector $\bm {b}^t$ recursively.} \rigel{Write this as its own algorithm} Without observing  $\bm{b}^{\nround}_-$, choose $\bm{b}^{\nround} \sim \bm{\psi}^{\nround-1}$ is sampled according to the policy generated by $\bm{\psi}^{\nround-1}$ {\color{red}The same way we did in the previous section, we need to explain how this sampling is done. I think we agreed to do this in the previous meeting}\;
            Receive reward {\color{red} let's be consistent: $\mu^\nround_n$ versus $\mu^\nround$. We have both in this section.} $\mu^\nround_n(\bm{b}^\nround) = \sum_{\nitem=1}^\Nitem w_\nitem^\nround(b^\nround_\nitem)$ and observe $w_\nitem^\nround(b^\nround_\nitem)$ where $w_\nitem^\nround(b) = (v_\nitem - b)\textbf{1}_{b \geq b^\nround_{-\nitem}}$\;
            $\hat{w}_\nitem^\nround(b) \gets \frac{w_\nitem^\nround(b)}{q^{\nround-1}_\nitem(b)} \textbf{1}_{b = b^{\nround}_\nitem}$ for all $\nitem \in [\Nitem]$ and $b \in \mathcal{B}$\;
            \textbf{Determining Probability Measure $\bm{q}^t$ over any pair of $(m, b)$}
            $\bm{q}^\nround \gets \text{argmin}_{\bm{q} \in \mathcal{Q}} \eta\langle \bm{q}, -\hat{\bm{w}}^\nround\rangle + D(\bm{q} || \bm{q}^{\nround-1})$ where $\mathcal{Q}$ is as in Definition~\ref{def: Qspace} and \[D(\bm{q} || \bm{q}') = \sum_{\nitem \in [\Nitem], b \in \mathcal{B}} q_\nitem(b)\frac{\log q_\nitem(b)}{q'_\nitem(b)} - (q_\nitem(b) - q'_\nitem(b))\,.\]
            \textbf{Convert $\bm{q}^t$ to Policy $\bm{\psi}^t$.}
            {\color{red} write a for loop for that}
            Recursively compute for all $\nitem$, $b$ in decreasing order $\psi^\nround_\nitem(b) \gets q^\nround_\nitem(b)\left(\sum_{b' \geq b} \frac{q^\nround_{\nitem-1}(b')}{1 - \sum_{b" > b'} \psi^\nround_\nitem(b")}\right)^{-1}$\;
            % Compute $\pi^\nround((\nitem, b), b') \gets \frac{\psi^\nround(\nitem+1, b')}{\sum_{b" \leq b} \psi^\nround_{\nitem+1}(b")}$ for all $\nitem \in [\Nitem], b \geq b' \in \mathcal{B}$ \rigel{Get rid of $\pi$ here and then just explicitly write $\frac{\psi^\nround(\nitem+1, b')}{\sum_{b" \leq b} \psi^\nround(\nitem+1, b")}$ in the sampling $\bm{b}$ step}\;
        }
        \textbf{Return} $\sum_{\nround=1}^\Nround \mu(\bm{b}^\nround)$
	\caption{\textsc{Bandit Bid Optimization in Multi-Unit Pay as Bid Auctions}}
	\label{alg: Node O-REPS}
\end{algorithm}

In this section, we now consider the more realistic feedback setting where agent $n$ only observes each auction $\nround$'s allocation $x^\nround = x^\nround_n(\bm{b}^\nround)$. \negin{ why are talking about cross-learning between actions that we are not even using. That would raise questions and could hurt us} In the full information setting, we were able to run a decoupled version of exponential weights which allowed us to obtain the counterfactual utility for every possible bid vector. However, in the bandit feedback setting, $\bm{b}^\nround_-$ is not revealed to the learner and thus, the bidder's allocation (and hence utility) would not be available to the learner for all possible bid vectors and instead only observes their utility for their submitted bid vector. Vanilla $\textsc{Exp3}$ here would result in exponential in $\Nitem$ regret as the bid space is exponentially large. 

Rather than running a variant of an $\textsc{Exp3}$ algorithm, our algorithm, which is presented in Algorithm \ref{alg: Node O-REPS}, leverages the DP formulation in Section 
\ref{sec:offline}. Recall that in the DP formulation, we have $M$ layers, where in each layer there are $|\mathcal B|$ nodes and {\color{red}XXX edges}. Given the structure of the DP, one idea is maintain some policy $\pi$ which induces a family of probability measures $\rho$ over the edges in the DP graph. {\color{red} add some figure for the DP section. make sure the notion of "DP graph" is well-defined. Formally define  } More precisely, {\color{red} explain $\pi$ and $\rho$.} 

Following this idea would lead to an algorithm with regret of XXX.  In this section, as our main contribution, instead of maintaining probability measures over the edges in the DP graph, we maintain probability measures over nodes. This idea is based on an important observation we made. In particular, in the DP formulation, we know that regardless of the value of $b_\nitem$, transitioning to state $b_{\nitem+1}$ always yields the same utility $w_\nitem^\nround(b_{\nitem+1})$ {\color{red} explain the previous sentence using nodes and edges and is the the (immediate) reward we obtain when we go from some $(m, b)$ to another node}. {\color{red} this translates this property on $\pi$ and maybe $\rho$: todo add that property}\rigel{Add something here about the connection to $\pi$ and then relate back to the algorithm} 
{\color{red} given that, define $\psi$ and maybe $q$ based on $\rho$ and $\pi$.}

Now, we are ready to explain our alg in more details. The algorithm has three steps. {\color{red} todo: complete.}



\newpage 
It may be tempting to apply $\textsc{Exp3}$ variants \rigel{Cite Exp3.G paper, Exp3.Set} that  by constructing importance sampling estimators using the observation that any bid above $c^\nround = b^{\nround}_{x^\nround}$ in the first $x^{\nround}$ slots is guaranteed to win the corresponding item, and any bid below $d^\nround = b^{\nround}_{x^\nround + 1}$ is guaranteed to lose the item. However, as the feedback graph is not revealed to the learner, such methods do not apply to our setting. As such, we resort to learning algorithms that construct utility estimates that do not depend on the entire set of rewards of cross-learnable bid vectors. Such algorithms include weight pushing \rigel{cite Mehryar Mohri (1998)}, combinatorial bandit algorithms (\rigel{Cite Chen 2014}), or FTRL (Follow the Regularized Leader) or OMD (Online Mirror Descent) based stochastic shortest path (SSP) solvers \rigel{Cite O-REPS, Component Hedge}. In these methods, the utility estimates depend only on the per-slot  utilities of the selected bid vector \negin{What do you mean by `` per-slot  utilities of the selected bid vector".I found this discussion confusing}. However, applying these methods directly results in sub-optimal regret as they fail to exploit additional structure within the bid optimization problem. Instead, we restate our bid optimization problem as a node-weight-maximization problem over a layered graph. We then describe an algorithm to solve this problem and construct upper and lower bounds on regret.

\subsection{Graphical Formulation}

Similar to our dynamic program in the offline setting, the learner maintains a table of utility estimates corresponding to $\{w^\nround_\nitem(b^\nround_\nitem)\}_{\nitem \in [\Nitem]}$ instead of explicitly storing the utility estimates corresponding to entire bid vectors. In order to recover the utility estimate of a bid vector, the learner sums the utility estimates of each of the components of the bid vector. The learner wishes to maximize their cumulative utility across $\Nround$ rounds. This interpretation of the bid optimization problem lends itself naturally to a graphical interpretation, where we construct a graph $\mathcal{G} = (\mathcal{S}, \mathcal{A})$ where we associate with each node in $s \in \mathcal{S}$ a weight and we seek to find the maximum weight path in $\mathcal{G}$. More concretely, we define $\mathcal{G}$ as follows:
\begin{enumerate}
    \item We define 2 types of states: source node $S_0$, followed by $\Nitem$ layers of states of width $|\mathcal{B}|$ between them which we denote by $\{(\nitem, b)\}_{\nitem \in [\Nitem], b \in \mathcal{B}}$. We let $\mathcal{S}$ denote the set of all states.
    \item There are 2 types of actions available at each state which describe which state in the subsequent layer the agent will move to: actions $a_0$ from $S_0$ to the entire first layer of nodes and actions $a_\nitem$ between layers $\nitem$ to layer $\nitem+1$. In particular, the latter set of actions requires non-decreasing bid value---i.e. $(\nitem, b)$ only has an action to $(\nitem+1, b')$ for $b' \leq b$. This reflects the bid monotonicity assumption. With probability 1, the agent will transition to the state prescribed by their action, where we say that the agent took (valid) action $a_\nitem = b$ if they selected action $(\nitem, b_\nitem) \to (\nitem+1, b)$. We let $\mathcal{A} = \mathcal{B}$ denote the set of all possible actions.
    \item There are 2 parameters: the valuation vector $\bm{v}$ and adversarial bid vector $\bm{b}^{\nround}_-$. With these parameters, the weight of state $(\nitem, B)$ is given by $w^{\nround}_{\nitem}(B) = 1_{b \geq b^{\nround}_{-\nitem}} (v_\nitem - b)$. This is precisely the same as the definition of $w^{\nround}_{\nitem}(b)$ as in offline bid optimization. The total reward of episode $\nround$ is equal to the sum of the node weights traversed. We show an example graph in figure~\ref{fig: valid_bids_graph}.
\end{enumerate}

% Figure environment removed

\subsection{Reduction to Online Linear Optimization with Semi-Bandit Feedback}

The agent's objective is to maximize their cumulative reward across $\Nround$ rounds using only feedback in each round given as $\{w_\nitem^\nround(b^\nround_\nitem)\}_{\nitem \in [\Nitem]}$. In order to balance exploration and exploitation, the agent stores, updates, and samples bid vectors according to policies $\pi^\nround: \mathcal{S} \times \mathcal{A} \to [0, 1]$, where $\pi^\nround((\nitem, b), b')$ denotes the probability of selecting action $b' \leq b$ at state $(\nitem, b)$ and round $\nround \in [\Nround]$. Abusing notation, let $\bm{b} \sim \pi$ denote a sequence of bid values $(b_1,\ldots,b_\Nitem)$ within a path sampled according to policy $\pi$. A common idea in the literature regarding episodic learning in MDP's is to construct a corresponding state-action occupancy measure $\rho^\pi((\nitem, b), b') = \prob_{\bm{b} \sim \pi}(b_\nitem = b, a_\nitem = b')$, with $\rho^\pi(S_0, b') = \prob_{\bm{b} \sim \pi}(s_0 = S_0, a_0 = b_1)$. Since we are operating under a deterministic Markovian environment with (directed) edges between consecutive layers, we have:{\color{red} not obvious why we have this. Please explain why this holds }
\begin{align}
    \sum_{b' \leq b} \rho^\pi((\nitem, b), b') = \sum_{b" \geq b} \rho^\pi((\nitem-1, b"), b) \quad \text{and} \quad \sum_{b \in \mathcal{B}} \sum_{b' \leq b} \rho^\pi((\nitem, b), b') = 1
\end{align}{\color{red} it is not a good practice to explain things using certain variables ($\rho$ here) that do nor even show up in the algorithm }
The key idea of selecting state-action pairs with marginal probabilities given by $\bm{\rho}$ is that we can rephrase the bid optimization problem as an instance of online linear optimization with semi-bandit feedback. More specifically, we can compute the (expected) loss function at round $\nround$ as follows: {\color{red} maybe her we oversimplify our method. We had a DP with value to goes. We need to also explain how we took care of it. To me, this is hidden in the sampling procedure that we did not explain well   }
\begin{align}
    \mathbb{E}_{\bm{b} \sim \pi^\rho}\left[ \mu^{\nround}_n(\bm{b}) \right] = \mathbb{E}_{\bm{b} \sim \pi^\rho}\left[ \sum_{\nitem=1}^\Nitem w_\nitem^\nround(b_\nitem) \right] = \sum_{\nitem=1}^\Nitem \prob_{\bm{b} \sim \pi^\rho}(s_{\nitem-1} = b_{\nitem-1}, a_{\nitem-1} = b_{\nitem}) w_\nitem^\nround(b_\nitem)
\end{align}
Substituting in the definition of $\pi^\rho$ and the corresponding $\rho$, we have:
\begin{align}
    \mathbb{E}_{\bm{b} \sim \pi^\rho}\left[ \mu^{\nround}_n(\bm{b}) \right] = \sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} \sum_{b' \leq b} \rho((\nitem-1, b), b') w_\nitem^\nround(b') = \langle \bm{\rho}, \bm{w}^\nround \rangle 
\end{align}
Where we assume $s_0 = S_0$ and define $\bm{w}^\nround \equiv \{w_\nitem^\nround(b')\}_{\nitem \in [\Nitem], b \geq b' \in \mathcal{B}}$. Assuming that the learner selects occupancy measure $\bm{\rho}^\nround$ at round $\nround$, the regret can then be written as:
\begin{align}
    \textsc{Regret}_\mathcal{B}(\Nround) &= \max_{\bm{b} \in \mathcal{B}^{+\Nitem}} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}) - \mathbb{E}_{\bm{b}^\nround \sim \bm{\rho}^\nround} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)\\
    &\leq \max_{\bm{\rho} \in \Delta(\Pi)} \mathbb{E}_{\bm{b} \sim \bm{\rho}} \sum_{\nround=1}^\Nround \mu(\bm{b}) - \mathbb{E}_{\bm{b}^\nround \sim \bm{\rho}^\nround} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)\\
    &= \max_{\bm{\rho} \in \Delta(\Pi)} \sum_{\nround=1}^\Nround \langle \bm{\rho} - \bm{\rho}^\nround, \bm{w}^\nround \rangle
\end{align}
{\color{red}I think that in your writing, you try to bring some parts of the proofs to the main text. This is good to do for the main idea. But, perhaps you are over doing it. Here is how I would write it. I would say this is our  algorithm. The algorithm has 3 main steps. Step 1- bidding recursively according to $\boldsymbol{\psi}^t$. In this step, we fist compute $b_t^1$, and then XXX. This step  returns a monotone bid vector. Step 2- computing $\textbf{q}^t$, which is XXX. While $\textbf{q}^t$ is useful, it cannot determine the bid vectors. This will be handled in Step 3. Step 3- we convert $\textbf{q}^t$, we convert $\textbf{q}^t$ to a policy  $\boldsymbol{\psi}^t$ that will be used to determine the bid vector for the next round. Then, you can talk about the novelty/intuitions/ideas in the design of the algorithm.   You can say sth like this: In the design of this algorithm, we used two/three main ideas: (I would then try to relate these ideas to the three steps you mentioned above) \\
Idea 1: maybe your first idea is the sampling method that helps you not be worried about value to goes in the DP. The sampling lets you determine bids sequentially while maintaining monotonicity.  Idea 2: maybe idea 2 is using the properties of the DP formulation that can be viewed as a deterministic MDP to XXXX.  Maybe you used this idea in computing $\bm{q}^t$.}
Where the inequality follows as any deterministic bid can be represented as an expectation over a deterministic policy, where the space of all measures on the set of all policies is given by $\Delta(\Pi)$. Existing algorithms for online linear optimization \rigel{Cite Lattimore 2020, Audibert 2013, O-REPS paper} over dimension $O(\Nitem |\mathcal{B}|^2)$ guarantee a regret upper bound of $O(\Nitem |\mathcal{B}| \sqrt{T \log \mathcal{B}})$. However, in our bid optimization setting, these algorithms achieve sub-optimal regret as the state-action occupancy measure does not fully exploit the structure of the problem. In particular, we know that regardless of the value of $b_\nitem$, transitioning to state $b_{\nitem+1}$ always yields the same utility $w_\nitem^\nround(b_{\nitem+1})$. \rigel{Add something here about the connection to $\pi$ and then relate back to the algorithm} Consequently, it is natural to remove the dependence on the value of $b_\nitem$ in the policy, subject to bid monotonicity. We do this by assuming a proportionality constraint on $\Pi$, the space of all possible policies, in the sense that the probability of transitioning to $(\nitem+1, b)$ versus $(\nitem, b')$ proportional under $b_\nitem$ and $b'_\nitem$. That is,
\[
\frac{\pi((\nitem, b), b')}{\pi((\nitem, b), b")} = \frac{\pi((\nitem, b'), b')}{\pi((\nitem, b'), b")} \text{ for all } \nitem \in [\Nitem], b" \leq b' \leq b, b", b', b \in \mathcal{B}
\]
Using the fact that $\sum_{b' \leq b} \pi((\nitem, b), b') = 1$, we have:
\begin{align}
    &\frac{\pi((\nitem, b_0), b')}{\pi((\nitem, b_0), b")} = \frac{\pi((\nitem, b'), b')}{\pi((\nitem, b'), b")} \\
    &\leftrightarrow \pi((\nitem, b_0), b')\pi((\nitem, b'), b") = \pi((\nitem, b'), b')\pi((\nitem, b_0), b")\\
    &\to \sum_{b" \leq b'} \pi((\nitem, b_0), b')\pi((\nitem, b'), b") = \sum_{b" \leq b'} \pi((\nitem, b'), b')\pi((\nitem, b_0), b") \\
    &\leftrightarrow \pi((\nitem, b_0), b') = \pi((\nitem, b'), b') \sum_{b" \leq b'} \pi((\nitem, b_0), b")\\
    &\leftrightarrow \pi((\nitem, b'), b') = \frac{\pi((\nitem, b_0), b')}{\sum_{b" \leq b'} \pi((\nitem, b_0), b")}\\
    &\to \pi((\nitem, b), b') = \frac{\pi((\nitem, b_0), b')}{\sum_{b" \leq b} \pi((\nitem, b_0), b")}
\end{align}
Where in the last equality, we plugged in the value of $\pi((\nitem, b'), b')$ back into the proportionality constraint. At a high level, this allows us to describe the entire policy with only $\{\pi(S_0, b')\}_{b' \in \mathcal{B}} \cup \{\pi((\nitem, b_0), b')\}_{\nitem \in [\Nitem], b' \in \mathcal{B}}$, which is a set of size $O(\Nitem |\mathcal{B}|)$. Letting $\psi_\nitem(b) = \pi((\nitem-1, b_0), b)$ denote the probability of transitioning from the maximum possible bid to $b$ at slot $\nitem$, we see that we can rewrite $\pi((\nitem, b), b') = \frac{\psi(\nitem+1, b')}{\sum_{b" \leq b} \psi(\nitem+1, b")}$. Furthermore, letting $\Psi: [\Nitem] \times \mathcal{B} \to [0, 1]$ such that $\sum_{b \in \mathcal{B}} \psi_\nitem(b) = 1$ denote the set of possible condensed policies $\psi$, we can recursively define the corresponding state occupancy measure $\bm{q}$ where $q_\nitem(b) = \prob_{\bm{b} \sim \psi}(b_\nitem = b)$ denotes the marginal probability that $b_\nitem = b$ when following the policy $\pi^\psi$ generated by $\psi$. With base case $q_1(b) = \psi(S_0, b)$, we have:
\begin{align}
    q_\nitem(b) = \sum_{b' \geq b} q_{\nitem-1}(b')\pi((\nitem-1, b'), b) = \psi_\nitem(b) \sum_{b' \geq b}  \frac{q_{\nitem - 1}(b')}{\sum_{b" \leq b'} \psi_\nitem(b")}
\end{align}
By strong induction, we have $q_\nitem(b) = \prob_{\bm{b} \sim \psi}(b_\nitem = b)$. We define $\mathcal{Q}$ to be the set of all possible state occupancy measures $\bm{q}$ such that there exists a $\psi \in \Psi$ that generates $\bm{q}$. More formally:
\begin{definition}
    \label{def: QSpace}
    Let $\mathcal{Q}$ be the set of all state occupancy measures $\bm{q} \in [\Nitem] \times \mathcal{B} \to [0, 1]$ that satisfy the following properties:
    \begin{enumerate}
        \item \textit{Probability measure validity}: $\sum_{b \in \mathcal{B}} q_\nitem(b) = 1$ for all $\nitem \in [\Nitem]$.
        \item \textit{Detailed balance and proportionality constraint}: There exists a function $\bm{\psi}: [\Nitem] \times \mathcal{B} \to [0, 1]$ with $\sum_{b \in \mathcal{B}} \psi_\nitem(b) = 1$ such that $q_1(b) = \psi(S_0, b)$ and $q_\nitem(b) = \psi_\nitem(b) \sum_{b' \geq b}  \frac{q_{\nitem - 1}(b')}{\sum_{b" \leq b'} \psi_\nitem(b")}$ for all $\nitem \in [\Nitem], b \in \mathcal{B}$.
    \end{enumerate}
\end{definition}

{\color{red} this should be a part of the proof, not the main text.}All that remains to show is how to recover $\bm{\psi}$ (from which we can recover $\pi$) given $q$. To do this, we first compute $\bm{\psi}$ recursively corresponding to $\bm{q}$, and from this, compute $\bm{\pi}$ using $\pi((\nitem, b), b') = \frac{\psi_{\nitem+1}(b')}{\sum_{b" \leq b} \psi_{\nitem+1}(b")}$. To compute $\bm{\psi}$ from $\bm{q}$, notice that for any $\nitem$:{\color{red}determine the range of $b_0$}
\begin{align}
    q_\nitem(b_0) = \psi_\nitem(b_0) \sum_{b \geq b_0} \frac{q_{\nitem-1}(b)}{\sum_{b" \leq b} \psi_\nitem(b")} = \psi_\nitem(b_0) \frac{q_{\nitem-1}(b_0)}{\sum_{b" \leq b_0} \psi_\nitem(b")} = \psi_\nitem(b_0) q_{\nitem-1}(b_0)
\end{align}
Where the last equality follows as $\sum_{b" \leq b_0} \psi_\nitem(b") = \sum_{b" \in \mathcal{B}} \psi_\nitem(b") = 1$. In the recursive case, we have:
\begin{align}
    q(\nitem, b') = \psi_\nitem(b')\sum_{b \geq b'} \frac{q_{\nitem-1}(b)}{\sum_{b" \leq b} \psi_\nitem(b")} = \psi_\nitem(b')\sum_{b \geq b'} \frac{q_{\nitem-1}(b)}{1 - \sum_{b" > b} \psi_\nitem(b")}
\end{align}
Hence, we can solve for $\psi_\nitem(b') = q_\nitem(b')\left(\sum_{b \geq b'} \frac{q_{\nitem-1}(b)}{1 - \sum_{b" > b} \psi_\nitem(b")}\right)^{-1} $ in terms of $\bm{q}$ (which is known) and $\psi_\nitem(b")$ for $b" \geq b'$, which is known from induction. Now that we have obtained concise description of the marginal probabilities of the event that $(\nitem, b)$ is observed in a bid vector sampled according to $\pi^\psi$, we can rewrite the regret as a function of the state occupancy measure $\bm{q}$. Rewriting the loss, we have:
\begin{align}
    \mathbb{E}_{\bm{b} \sim \psi}\left[ \mu^{\nround}_n(\bm{b}) \right] = \mathbb{E}_{\bm{b} \sim \psi}\left[ \sum_{\nitem=1}^\Nitem w_\nitem^\nround(b_\nitem) \right] = \sum_{\nitem=1}^\Nitem \prob_{\bm{b} \sim \psi}(b_\nitem = b) w_\nitem^\nround(b_\nitem)
\end{align}
Substituting in our definition $q_\nitem(b) = \prob_{\bm{b} \sim \psi}(b_\nitem = b)$, we obtain:
\begin{align}
    \mathbb{E}_{\bm{b} \sim \psi}\left[ \mu^{\nround}_n(\bm{b}) \right] = \sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q^\nround_\nitem(b_\nitem) w_\nitem^\nround(b_\nitem) = \langle \bm{q}^\nround, \bm{w}^\nround\rangle
\end{align}
Where $\bm{w}^\nround = \{w_\nitem^\nround(b)\}_{\nitem \in [\Nitem], b \in \mathcal{B}}$ which is different as in the previous section where it was defined additionally over $b' \leq b$. Following a similar argument, we obtain:
\begin{align}
    \textsc{Regret}_\mathcal{B}(\Nround) \leq \max_{\bm{q} \in \mathcal{Q}} \sum_{\nround=1}^\Nround \langle  \bm{q}^\nround - \bm{q}, -\bm{w}^\nround \rangle.
\end{align}
Here, the underlying dimension is of size $O(\Nitem |\mathcal{B}|)$ as opposed to $O(\Nitem |\mathcal{B}|^2)$ as in the previous section. As such, the regret corresponding to semi-bandit feedback negative-entropy regularized FTRL or OMD algorithms decrease by a factor of $\sqrt{|\mathcal{B}|}$ and we obtain:
\begin{align}
    \textsc{Regret}_\mathcal{B}(\Nround) \lesssim \Nitem \sqrt{|\mathcal{B}| \Nround \log |\mathcal{B}|}.
\end{align}

\subsection{ Regret Analysis}



\begin{theorem}
    Under bandit feedback, Algorithm~\ref{alg: Node O-REPS} achieves regret rate $\textsc{Regret}_\mathcal{B}(\Nround) \lesssim O(\Nitem \sqrt{|\mathcal{B}| \Nround \log |\mathcal{B}|})$ using $\eta = \sqrt{\frac{\log |\mathcal{B}|}{|\mathcal{B}|\Nround}}$. \rigel{Include time and space complexity here}
\end{theorem}

\begin{proof}
    The proof closely follows that of Theorem 1 in \rigel{Cite O-REPS paper}. At a high level, we want to bound the regret of Follow the (Entropy) Regularized leader. We first do this by upper bounding the regret by the regret of the unconstrained Be the (Entropy) Regularized leader (see Lemma 13 of \rigel{Cite Rakhlin 2009}. In particular, we define the corresponding unconstrained optimization problem:
    \begin{align}
        \tilde{\bm{q}}^{\nround+1} = \text{argmin}_{\bm{q} \in [\Nitem] \times \mathcal{B} \to \mathbb{R}^+} \left( \eta \langle \bm{q}, -\hat{\bm{w}}^{\nround} \rangle + D(\bm{q} || \bm{q}^{\nround}) \right)
    \end{align}
    Where $D(\bm{q} || \bm{q}') = \sum_{\nitem \in [\Nitem], b \in \mathcal{B}} q_\nitem(b)\frac{\log q_\nitem(b)}{q'_\nitem(b)} - (q_\nitem(b) - q'_\nitem(b))$. Here, notice that we are optimizing over all non-negative functions over states pairs rather than the space of state occupancy measures $\mathcal{Q}$. We continue by observing that the estimators $\hat{w}_\nitem^\nround(b) \gets \frac{w_\nitem^\nround(b)}{q^{\nround-1}_\nitem(b)} \textbf{1}_{b = b^{\nround}_\nitem}$ are unbiased:
    \begin{align}
        \mathbb{E}_{\bm{b} \sim \bm{\psi}^\nround}[\hat{w}_\nitem^\nround(b)] = \mathbb{E}_{\bm{b} \sim \bm{\psi}^\nround}[\frac{w_\nitem^\nround(b)}{q^{\nround-1}_\nitem(b)} \textbf{1}_{b = b^{\nround}_\nitem}] = \frac{w_\nitem^\nround(b)}{q^{\nround-1}_\nitem(b)} q^{\nround-1}_\nitem(b) = w_\nitem^\nround(b)\ .
    \end{align}
    Using the unbiasedness of our estimators $\hat{\bm{w}}^\nround$, we can replace $\bm{w}^\nround$ with $\hat{\bm{w}}^\nround$ in the definition of regret. Now, as it is shown in Lemma 13 of \rigel{Cite Rakhlin 2009}, we can upper bound the expected estimated regret as a function of the unconstrained optimizer $\tilde{\bm{q}}^{\nround+1}$ and the unregularized relative entropy with respect to the initial state-action occupancy measure $\bm{q}^0$. 
    \begin{align}
        \textsc{Regret}_\mathcal{B}(\Nround) = \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{q}^{\nround} - \bm{q}, -\hat{\bm{w}}^{\nround} \rangle \right] \leq \max_{\bm{q} \in \mathcal{Q}}\mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{q}^{\nround} - \tilde{\bm{q}}^{ \nround+1}, -\hat{\bm{w}}^{\nround} \rangle + \eta^{-1}D(\bm{q} || \bm{q}^{1}) \right]
    \end{align}
    Furthermore, the unconstrained optimizer can be solved with $\tilde{\bm{q}}^{ \nround+1} = \bm{q}^{\nround} \exp(\eta \hat{\bm{w}}^{\nround})$. Applying $\exp(x) \geq 1 + x$ for $x = \exp(\eta \hat{\bm{w}}^{\nround})$, we obtain $\tilde{\bm{q}}^{ \nround+1} \exp(\eta \hat{\bm{w}}^{\nround}) \geq \bm{q}^{\nround} + \eta \bm{q}^{\nround} \hat{\bm{w}}^{\nround}$. Plugging this back in:
    \begin{align}
        \textsc{Regret}_\mathcal{B}(\Nround) &\leq \mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{q}^{\nround} - \bm{q}^{\nround} \exp(\eta \hat{\bm{w}}^{\nround}), -\hat{\bm{w}}^{\nround} \rangle + \eta^{-1}D(\bm{q} || \bm{q}^{1}) \right]\\
        &= \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}_\nitem(b) \hat{w}^{\nround}_\nitem(b)^2 + \eta^{-1}D(\bm{q} || \bm{q}^{1}) \right] \label{eq: node diff}
    \end{align}
    Note that $\hat{w}^{\nround}_\nitem(b) = \frac{w_\nitem^\nround(b)}{q^{\nround-1}_{\nitem}(b)} \textbf{1}_{b = b^{\nround}_{\nitem}}$ for all $\nitem \in [\Nitem]$ and $b \in \mathcal{B}$ by definition. Since $w^{\nround}_\nitem(b) \leq 1$ and $\textbf{1}_{b = b^{\nround}_{\nitem}} \leq 1$ we have $\hat{w}^{\nround}_\nitem(b) \leq \frac{1}{q^{\nround}_\nitem(b)}$ and we continue the above chain of inequalities with:
    \begin{align}
        \textsc{Regret}_\mathcal{B}(\Nround) &\leq \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}_{\nitem-1}(b) \hat{w}^{\nround}_\nitem(b) \frac{1}{q^{\nround}_{\nitem}(b)}  + \eta^{-1}D(\bm{q} || q^{1}) \right] \label{eq: full info difference}\\
        &= \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} \hat{w}^{\nround}_\nitem(b)  + \eta^{-1}D(\bm{q} || \bm{q}^{1}) \right]
    \end{align}
    Noting that $D(\bm{q} || \bm{q}') \leq \sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} \left[-q_{\nitem}(b) \log q_\nitem(b) \right] = \sum_{\nitem=1}^\Nitem H(\bm{q}_\nitem(\cdot))$ and $\sum_{b \in \mathcal{B}} q_\nitem(b) = 1$ for any $q \in \mathcal{Q}$ and $\nitem \in [\Nitem]$, we have that $\bm{q}_\nitem(\cdot)$ is a valid probability mass function. Thus, its entropy $H(\bm{q}_\nitem(\cdot))$ is upper bounded by $O(\log |\mathcal{B}|)$ which is the entropy of the uniform distribution over $|\mathcal{B}|^2$ items. Hence, $D(\bm{q} || \bm{q}') \lesssim \Nitem \log |\mathcal{B}|$. Plugging this back in:
    \begin{align}
        \textsc{Regret}_\mathcal{B}(\Nround) &\lesssim \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} \hat{w}^{\nround}_\nitem(b)  + \eta^{-1}\Nitem \log |\mathcal{B}| \right]\\
        &\leq \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem=1} \sum_{b \in \mathcal{B}} \hat{w}^{\nround}_\nitem(b) + \eta^{-1}\Nitem \log |\mathcal{B}| \right]\\
        &\leq \eta \sum_{\nround=1}^\Nround \sum_{\nitem=1} \sum_{b \in \mathcal{B}} w^{\nround}_\nitem(b) + \eta^{-1}\Nitem \log |\mathcal{B}|\\
        &= \eta \Nround \Nitem |\mathcal{B}| + \eta^{-1}\Nitem \log |\mathcal{B}|
    \end{align}
    Where in the last equality, we used the unbiasedness property of $\hat{\bm{w}}^{\nround}$. Setting $\eta = \sqrt{\frac{\log |\mathcal{B}|}{|\mathcal{B}|T}}$, we obtain $\textsc{Regret}_\mathcal{B}(\Nround) \leq \Nitem \sqrt{|\mathcal{B}| \Nround \log |\mathcal{B}|}$. We can also extend this algorithm to the full information setting. To handle the full information case, we note that we can improve line~\ref{eq: full info difference} by instead replacing $\hat{\bm{w}}^{\nround}$ with $\bm{w}^{\nround}$ in the previous line to obtain: 
    \begin{align}
        \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}_\nitem(b) \hat{w}^{\nround}_\nitem(b)^2 &= \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}_\nitem(b) w^{\nround}_\nitem(b)^2\\
        &\leq \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}_\nitem(b) = \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem 1 = \Nround \Nitem
    \end{align}
    Setting $\eta = \sqrt{\frac{ \log |\mathcal{B}|}{T}}$, we obtain in the full information setting $\textsc{Regret}_\mathcal{B}(\Nround) \leq \Nitem \sqrt{\Nround \log |\mathcal{B}|}$.
\end{proof}


One may wonder how to efficiently update the state occupancy measures by computing the minimizer of $\eta\langle \bm{q}, -\hat{\bm{w}}^\nround\rangle + D(\bm{q} || \bm{q}^{\nround-1})$. While we relegate the details to \rigel{Cite O-REPS paper here}, the idea is to first solve the unconstrained entropy regularized minimizer with $\tilde{\bm{q}}^{ \nround+1} = \bm{q}^{\nround} \exp(\eta \hat{\bm{w}}^{\nround})$. We then project this unconstrained minimizer to $\mathcal{Q}$ with:
\begin{align}
    \bm{q}^{\nround + 1} = \text{argmin}_{\bm{q} \in \mathcal{Q}} D(\bm{\rho}||\tilde{\bm{q}}^{\nround + 1})
\end{align}
Similar to how \rigel{Cite O-REPS} shows that this projection step (for state-action occupancy measures) can be solved as the solution to an unconstrained convex optimization problem in $\mathbb{R}^{|\mathcal{B}|^2}$, we have a slightly more complicated convex optimization problem over $\mathbb{R}^{|\mathcal{B}|}$ with a polynomial number of constraints as prescribed in Definition~\ref{def: Qspace}. Perhaps more simply, we can instead solve a modified bid optimization shortest path problem instance. In particular, we can set existing edge costs to 0, duplicate each layer with a single edge between same-bid pairs $b$ in row $\nitem$ and the duplicated row $\nitem$ with reward $w^\nround_\nitem(b)$ (see Figure~\ref{fig: modified_o_reps}). We can then invoke existing stochastic shortest path solvers such as Component Hedge or $\textsc{O-REPS}$ on this modified problem in order to obtain the state-action occupancy measure $\bm{\rho}^{\nround+1}$. Once we have computed $\bm{\rho}^{\nround+1}$, we can translate this into $\bm{\pi}^{\nround+1}$ or back into $\bm{q}^{\nround+1}$ and the corresponding $\bm{\psi}^{\nround + 1}$. Unfortunately, regardless of the optimization procedure used to update the policies, this projection step prevents the straightforward generalization of our method to handle non-stationary valuation profiles. 

Additionally, we comment on a regret lower bound. We know that the full information setting had a regret lower bound of $\Omega(\Nitem \sqrt{\Nround \log |\mathcal{B}|})$, which is linear in $\Nitem$ and a factor of $\sqrt{|\mathcal{B}|}$ off from the bandit setting upper bound. We note that it is difficult to construct

% Figure environment removed







\if 0

\newpage







Let $\Pi$ denote the set of all possible policies. With the above equalities and the obvious non-negativity constraints $\rho^\pi((\nitem, b), b') \geq 0$ for all $b' \leq b$, we can now define the set $\Delta$ of all possible state-action occupancy measures.
\begin{definition}
    Let $\Delta(\Pi)$ denote the set of all $\rho \in \mathcal{S} \times \mathcal{A} \to [0, 1]$ with the following properties:
    \begin{enumerate}
        \item $\sum_{B \in \mathcal{B}} \sum_{B' \leq B} \rho((\nitem, B), B') = 1$ for all $\nitem \in [\Nitem]$
        \item $\sum_{B \in \mathcal{B}} \rho(S_0, B) = 1$
        \item $\sum_{B' \leq B} \rho((\nitem, B), B') = \sum_{B" \geq B} \rho((\nitem-1, B"), B)$ for all $B \in \mathcal{B}, \nitem \in [\Nitem]$
        \item $\sum_{B' \leq B} \rho((1, B), B') = \rho(S_0, B)$
    \end{enumerate}
    This is equivalent to the condition that exists a policy $\pi \in \Pi$ such that $\rho((\nitem, B), B') = \prob_{\bm{B} \sim \pi}(s_\nitem = B_\nitem, a_\nitem = B_{\nitem+1})$ for all $B \geq B' \in \mathcal{B}$, $\nitem \in [\Nitem]$ and $\rho(S_0, B') = \prob_{\bm{B} \sim \pi}(s_0 = S_0, a_0 = B_1)$.
\end{definition}





We say that policy $\pi$ generates state-action occupancy measure $\rho$ if $\pi(s, a) = \frac{\rho(s, a)}{\sum_{b: \mathcal{A}(s)} \rho(s, b)}$ for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$ where $\mathcal{A}(s)$ denotes the set of valid actions at state $s$. From the definition of $\Delta(\Pi)$, we can see that each $\rho$ uniquely determines $\pi$ and vice versa. We let $\pi^\rho$ denote the policy that generates the occupancy measure $\rho$. Hence, computing the regret minimizing policy $\pi^\rho \in \Pi$ is equivalent to minimizing the regret minimizing state-action occupancy measure $\rho$. Once computing $\rho$, we can compute the corresponding $\pi^\rho$ and then simulate a bid vector according to $\pi^\rho$. The key idea is that optimizing with respect to $\rho$, as opposed to $\pi$, can be framed as instance of online linear optimization. More specifically, we can compute the (expected) loss function at round $\nround$ as follows:
\begin{align}
    \mathbb{E}_{\bm{b} \sim \pi^\rho}\left[ \mu^{\nround}_n(\bm{b}) \right] = \mathbb{E}_{\bm{b} \sim \pi^\rho}\left[ \sum_{\nitem=1}^\Nitem w_\nitem^\nround(b_\nitem) \right] = \sum_{\nitem=1}^\Nitem \prob_{\bm{b} \sim \pi^\rho}(s_{\nitem-1} = b_{\nitem-1}, a_{\nitem-1} = b_{\nitem}) w_\nitem^\nround(b_\nitem)
\end{align}
Substituting in the definition of $\pi^\rho$ and the corresponding $\rho$, we have:
\begin{align}
    \mathbb{E}_{\bm{b} \sim \pi^\rho}\left[ \mu^{\nround}_n(\bm{b}) \right] = \sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} \sum_{b' \leq b} \rho((\nitem-1, b), b') w_\nitem^\nround(b') = \langle \bm{\rho}, \bm{w}^\nround \rangle 
\end{align}
Where we assume $s_0 = S_0$ and define $\bm{w}^\nround \equiv \{w_\nitem^\nround(b')\}_{\nitem \in [\Nitem], b \geq b' \in \mathcal{B}}$. Assuming that the learner selects occupancy measure $\bm{\rho}^\nround$ at round $\nround$, the regret can then be written as:
\begin{align}
    \textsc{Regret}_\mathcal{B}(\Nround) &= \max_{\bm{b} \in \mathcal{B}^{+\Nitem}} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}) - \mathbb{E}_{\bm{b}^\nround \sim \bm{\rho}^\nround} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)\\
    &\leq \max_{\bm{\rho} \in \Delta(\Pi)} \mathbb{E}_{\bm{b} \sim \bm{\rho}} \sum_{\nround=1}^\Nround \mu(\bm{b}) - \mathbb{E}_{\bm{b}^\nround \sim \bm{\rho}^\nround} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)\\
    &= \max_{\bm{\rho} \in \Delta(\Pi)} \sum_{\nround=1}^\Nround \langle \bm{\rho} - \bm{\rho}^\nround, \bm{w}^\nround \rangle = \max_{\bm{\rho} \in \Delta(\Pi)} \sum_{\nround=1}^\Nround \langle  \bm{\rho}^\nround - \bm{\rho}, -\bm{w}^\nround \rangle
\end{align}
Where the inequality follows as any deterministic bid can be represented as an expectation over a deterministic policy. We additionally assume that that $\bm{\rho}^\nround$ is in $\Delta(\Pi)$ for all $\nround$. Notice that to keep consistent with the SSP literature, we negate both terms in the dot product to represent the problem as loss minimization rather than utility maximization. Defining $D(\bm{\rho} || \bm{\rho}') = \sum_{s \in \mathcal{S}, a \in \mathcal{A}(s)} \rho(s, a)\frac{\log \rho(s, a)}{\rho'(s, a)} - (\rho(s, a) - \rho'(s, a))$, we are now ready to state the $\textsc{O-REPS}$ algorithm formally and give the corresponding performance guarantees.

\begin{algorithm}[t]
	\KwIn{Valuation $\bm{v} \in [0, 1]^\Nitem$, Learning rate $\eta > 0$, Adaptive Adversarial Environment $\textsc{Env}^\nround: \mathcal{H}^\nround \to \mathcal{B}^{-\Nitem} \times \mathcal{B}$ where $\mathcal{H}^\nround$ denotes the set of all possible historical auction results $H^\nround$ up to round $\nround$ for all $\nround \in [\Nround]$.}
	\KwOut{The aggregate utility $\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)$.}
	$\pi^0(s, a) \gets \frac{1}{|\mathcal{A}(s)|}$ for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$. Let $\bm{\rho}^0$ be the corresponding state-action occupancy measure \;
        $H^0 \gets \emptyset$ \;
	\For{$\nround \in [\Nround]$:}{
            $\bm{b}^{\nround}_- \gets \textsc{Env}^{\nround-1}(H^{\nround-1})$ and $\bm{b}^{\nround} \sim \bm{\pi}^{\nround-1}$\;
            Receive reward $\mu^\nround_n(\bm{b}^\nround) = \sum_{\nitem=1}^\Nitem w_\nitem^\nround(b^\nround_\nitem)$ and observe $w_\nitem^\nround(b^\nround_\nitem)$ where $w_\nitem^\nround(b)$ is as defined in Equation~\ref{def: def mu w W}\;            $\hat{w}_\nitem^\nround(B, B') \gets \frac{w_\nitem^\nround(B')}{\rho^{\nround-1}((\nitem-1, B), B')} \textbf{1}_{B = b^{\nround}_{\nitem-1},  B' = b^{\nround}_\nitem}$ for all $\nitem \in [\Nitem]$ and $B \geq B' \in \mathcal{B}$\;
            $\bm{\rho}^\nround \gets \text{argmin}_{\bm{\rho} \in \Delta(\Pi)} \eta\langle \bm{\rho}, -\hat{\bm{w}}^\nround\rangle + D(\bm{\rho} || \bm{\rho}^{\nround-1})$ and $\pi^\nround(s, a) \gets \frac{\rho^\nround(s, a)}{\sum_{b: \mathcal{A}(s)} \rho^\nround(s, b)}$\;
        }
        \textbf{Return} $\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)$
	\caption{\textsc{O-REPS}}
	\label{alg: O-REPS}
\end{algorithm}

\begin{theorem}
    Under bandit feedback, Algorithm~\ref{alg: O-REPS} achieves regret rate $\textsc{Regret}_\mathcal{B}(\Nround) \lesssim O(\Nitem |\mathcal{B}| \sqrt{T \log |\mathcal{B}|})$ using $\eta = |\mathcal{B}|^{-1}\sqrt{\frac{\log |\mathcal{B}|}{T}}$ with respect to the discretized benchmark.
\end{theorem}

\begin{proof}
    The proof follows directly from \rigel{Cite O-REPS paper}. At a high level, we want to bound the regret of Follow the (Entropy) Regularized leader. We first do this by upper bounding the regret by the regret of the unconstrained Be the (Entropy) Regularized leader (see Lemma 13 of \rigel{Cite Rakhlin 2009}. In particular, we define the corresponding unconstrained optimization problem:
    \begin{align}
        \tilde{\bm{\rho}}^{\nround+1} = \text{argmin}_{\bm{\rho} \in \mathcal{S} \times \mathcal{A} \to \mathbb{R}^+} \left( \eta \langle \bm{\rho}, -\hat{\bm{w}}^{\nround} \rangle + D(\bm{\rho} || \bm{\rho}^{\nround}) \right)
    \end{align}
    Where we are optimizing over all non-negative functions over state-action pairs rather than $\Delta(\Pi)$. Using the unbiasedness of our estimators $\hat{\bm{w}}^\nround$, we can replace $\bm{w}^\nround$ with $\hat{\bm{w}}^\nround$ in the definition of regret. Now, as it is shown in Lemma 13 of \rigel{Cite Rakhlin 2009}, we can upper bound the expected estimated regret as a function of the unconstrained optimizer $\tilde{\bm{\rho}}^{\nround+1}$ and the unregularized relative entropy with respect to the initial state-action occupancy measure $\bm{\rho}^1$. 
    \begin{align}
        \textsc{Regret}_\mathcal{B}(\Nround) = \max_{\bm{\rho} \in \Delta(\Pi)} \mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{\rho}^{\nround} - \bm{\rho}, -\hat{\bm{w}}^{\nround} \rangle \right] \leq \max_{\bm{\rho} \in \Delta(\Pi)}\mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{\rho}^{\nround} - \tilde{\bm{\rho}}^{ \nround+1}, -\hat{\bm{w}}^{\nround} \rangle + \eta^{-1}D(\bm{\rho} || \bm{\rho}^{1}) \right]
    \end{align}
    Furthermore, the unconstrained optimizer can be solved with $\tilde{\bm{\rho}}^{ \nround+1} = \bm{\rho}^{\nround} \exp(\eta \hat{\bm{w}}^{\nround})$. Applying $\exp(x) \geq 1 + x$ for $x = \exp(\eta \hat{\bm{w}}^{\nround})$, we obtain $\tilde{\bm{\rho}}^{ \nround+1} \exp(\eta \hat{\bm{w}}^{\nround}) \geq \bm{\rho}^{\nround} + \bm{\rho}^{\nround}\eta \hat{\bm{w}}^{\nround}$. Plugging this back in:
    \begin{align}
        \textsc{Regret}_\mathcal{B}(\Nround) &\leq \mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{\rho}^{\nround} - \bm{\rho}^{\nround} \exp(\eta \hat{\bm{w}}^{\nround}), -\hat{\bm{w}}^{\nround} \rangle + \eta^{-1}D(\bm{\rho} || \bm{\rho}^{1}) \right]\\
        &= \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \geq b' \in \mathcal{B}} \rho^{\nround}((\nitem-1, b), b') \hat{w}^{\nround}_\nitem(b, b')^2 + \eta^{-1}D(\bm{\rho} || \bm{\rho}^{1}) \right] \label{eq: node diff}
    \end{align}
    Note that $\hat{w}^{\nround}_\nitem(b, b') = \frac{w_\nitem^\nround(b')}{\rho^{\nround-1}((\nitem-1, b), b')} \textbf{1}_{b = b^{\nround}_{\nitem-1},  b' = b^{\nround}_\nitem}$ for all $\nitem \in [\Nitem]$ and $b \geq b' \in \mathcal{B}$ by definition. Since $w^{\nround}_\nitem(b) \leq 1$ and $\textbf{1}_{b = b^{\nround}_{\nitem-1},  b' = b^{\nround}_\nitem} \leq 1$ we have $\hat{w}^{\nround}_\nitem(b, b') \leq \frac{1}{\rho^{\nround}_\nitem((\nitem-1, b), b')}$ and we continue the above chain of inequalities with:
    \begin{align}
        \textsc{Regret}_\mathcal{B}(\Nround) &\leq \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \geq b' \in \mathcal{B}} \rho^{\nround}((\nitem-1, b), b') \hat{w}^{\nround}_\nitem(b, b') \frac{1}{\rho^{\nround}((\nitem-1, b), b')}  + \eta^{-1}D(\bm{\rho} || \rho^{1}) \right] \label{eq: full info difference}\\
        &= \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \geq b' \in \mathcal{B}} \hat{w}^{\nround}_\nitem(b, b')  + \eta^{-1}D(\bm{\rho} || \bm{\rho}^{1}) \right]
    \end{align}
    Note that $D(\bm{\rho} || \bm{\rho}') \leq \sum_{\nitem=1}^\Nitem \sum_{b \geq b' \in \mathcal{B}} \left[-\rho((\nitem-1, b), b') \log \rho((\nitem-1, b), b') \right]$. Now, we use the fact that $\sum_{b \geq b' \in \mathcal{B}} \rho((\nitem-1, b), b') = 1$ for any $\rho \in \Delta(\Pi)$ and $\nitem \in [\Nitem]$, we have that $\rho((\nitem, \cdot), \cdot)$ is a valid probability mass function and has entropy upper bounded by $O(\log |\mathcal{B}|)$ which is the entropy of the uniform distribution over $|\mathcal{B}|^2$ items. Hence, $D(\bm{\rho} || \bm{\rho}') \lesssim \Nitem \log |\mathcal{B}|$. Plugging this back in:
    \begin{align}
        \textsc{Regret}_\mathcal{B}(\Nround) &\lesssim \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \geq b' \in \mathcal{B}} \hat{w}^{\nround}_\nitem(b, b')  + \eta^{-1}\Nitem \log |\mathcal{B}| \right]\\
        &\leq \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem=1} \sum_{b \geq b' \in \mathcal{B}} \hat{w}^{\nround}_\nitem(b, b') + \eta^{-1}\Nitem \log |\mathcal{B}| \right]\\
        &\leq \eta \sum_{\nround=1}^\Nround \sum_{\nitem=1} \sum_{b \geq b' \in \mathcal{B}} w^{\nround}_\nitem(b, b') + \eta^{-1}\Nitem \log |\mathcal{B}|\\
        &= \eta \Nround \Nitem |\mathcal{B}|^2 + \eta^{-1}\Nitem \log |\mathcal{B}|
    \end{align}
    Where in the last equality, we used the unbiasedness property of $\hat{\bm{w}}^{\nround}$. Setting $\eta = |\mathcal{B}|^{-1}\sqrt{\frac{\log |\mathcal{B}|}{T}}$, we obtain $\textsc{Regret}_\mathcal{B}(\Nround) \leq \Nitem |\mathcal{B}| \sqrt{\Nround \log |\mathcal{B}|}$. To handle the full information case, we note that we can improve line~\ref{eq: full info difference} by instead replacing $\hat{\bm{w}}^{\nround}$ with $\bm{w}^{\nround}$ in the previous line to obtain: 
    \begin{align}
        \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem \sum_{B \geq B' \in \mathcal{B}} \rho^{\nround}((\nitem-1, B), B') \hat{w}^{\nround}_\nitem(B)^2 &= \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem \sum_{B \geq B' \in \mathcal{B}} \rho^{\nround}((\nitem-1, B), B') w^{\nround}_\nitem((\nitem-1, B), B')^2\\
        &\leq \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem \sum_{B \geq B' \in \mathcal{B}} \rho^{\nround}_\nitem(B, B') = \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem 1 = \Nround \Nitem
    \end{align}
    Setting $\eta = \sqrt{\frac{ \log |\mathcal{B}|}{T}}$, we obtain in the full information setting $\textsc{Regret}_\mathcal{B}(\Nround) \leq \Nitem \sqrt{\Nround \log |\mathcal{B}|}$.
\end{proof}

One may wonder how to efficiently update the state-action occupancy measures by computing the minimizer of $\eta\langle \bm{\rho}, -\hat{\bm{w}}^\nround\rangle + D(\bm{\rho} || \bm{\rho}^{\nround-1})$. While we relegate the details to \rigel{Cite O-REPS paper here}, the idea is to first solve the unconstrained entropy regularized minimizer with $\tilde{\bm{\rho}}^{ \nround+1} = \bm{\rho}^{\nround} \exp(\eta \hat{\bm{w}}^{\nround})$. We then project this unconstrained minimizer to $\Delta(\Pi)$ with:
\begin{align}
    \bm{\rho}^{\nround + 1} = \text{argmin}_{\bm{\rho} \in \Delta(\Pi)} D(\bm{\rho}||\tilde{\bm{\rho}}^{\nround + 1})
\end{align}
As shown in the analysis of \rigel{Cite O-REPS again}, this can be solved efficiently as an unconstrained convex optimization problem in $\mathbb{R}^{|\mathcal{B}|^2}$. Unfortunately, this projection prevents the straightforward generalization of our method to handle time varying valuation profiles.










\subsection{Reduction to State-Occupancy Measures}

One may realize that as the rewards associated with each edge are independent of its initial location, we may hope to further simplify the problem by assuming proportional conditional probability mass functions. That is, letting $b' \leq b$, we enforce that $\{\pi((\nitem, b), b")\}_{b" \leq b'} \propto \{\pi((\nitem, b'), b")\}_{b" \leq b'}$ for all $\nitem \in [\Nitem]$. We let $\Pi'$ denote the set of all $\Pi$ that fulfill this condition. This condition can be written succinctly as $\frac{\pi((\nitem, b), b')}{\pi((\nitem, b), b")} = \frac{\pi((\nitem, b'), b')}{\pi((\nitem, b'), b")}$ for all $\nitem$ and $b" \leq b' \leq b$. Letting $b_0$ denote the maximal possible bid, these conditions can be written even more concisely as $\bigcap_{\nitem=1}^\Nitem \bigcap_{b' \in \mathcal{B}} \bigcap_{b" \leq b'} \{\frac{\pi((\nitem, b_0), b')}{\pi((\nitem, b_0), b")} = \frac{\pi((\nitem, b'), b')}{\pi((\nitem, b'), b")}\}$. Using the fact that $\sum_{b' \leq b} \pi((\nitem, b), b') = 1$, we have:
\begin{align}
    &\frac{\pi((\nitem, b_0), b')}{\pi((\nitem, b_0), b")} = \frac{\pi((\nitem, b'), b')}{\pi((\nitem, b'), b")} \\
    &\leftrightarrow \pi((\nitem, b_0), b')\pi((\nitem, b'), b") = \pi((\nitem, b'), b')\pi((\nitem, b_0), b")\\
    &\to \sum_{b" \leq b'} \pi((\nitem, b_0), b')\pi((\nitem, b'), b") = \sum_{b" \leq b'} \pi((\nitem, b'), b')\pi((\nitem, b_0), b") \\
    &\leftrightarrow \pi((\nitem, b_0), b') = \pi((\nitem, b'), b') \sum_{b" \leq b'} \pi((\nitem, b_0), b")\\
    &\leftrightarrow \pi((\nitem, b'), b') = \frac{\pi((\nitem, b_0), b')}{\sum_{b" \leq b'} \pi((\nitem, b_0), b")}\\
    &\to \pi((\nitem, b), b') = \frac{\pi((\nitem, b_0), b')}{\sum_{b" \leq b} \pi((\nitem, b_0), b")}
\end{align}
Where in the last equality, we plugged in the value of $\pi((\nitem, b'), b')$ back into the proportionality constraint. At a high level, this allows us to describe the entire policy with only $\{\pi(S_0, b')\}_{b' \in \mathcal{B}} \cup \{\pi((\nitem, b_0), b')\}_{\nitem \in [\Nitem], b' \in \mathcal{B}}$, which is a set of size $O(\Nitem |\mathcal{B}|)$. Letting $\psi_\nitem(b) = \pi((\nitem-1, b_0), b)$, we see that we can rewrite $\pi((\nitem, b), b') = \frac{\psi(\nitem+1, b')}{\sum_{b" \leq b} \psi(\nitem+1, b")}$. Once again abusing notation, let $\bm{b} \sim \psi$ denote a bid vector sampled according to the policy generated from $\psi$. Furthermore, letting $\Psi: [\Nitem] \times \mathcal{B} \to [0, 1]$ such that $\sum_{b \in \mathcal{B}} \psi_\nitem(b) = 1$ denote the set of possible condensed policies $\psi$, we can recursively define the state occupancy measure $q^\psi(\nitem, b) = \prob_{\bm{b} \sim \psi}(s_\nitem = b_\nitem)$. With base case $q^\psi(1, b') = \psi(S_0, b')$, we have:
\begin{align}
    q^\psi(\nitem, b') = \sum_{b \geq b'} q^\psi(\nitem - 1, b)\pi((\nitem-1, b), b') = \psi_\nitem(b') \sum_{b \geq b'}  \frac{q^\psi(\nitem - 1, b)}{\sum_{b" \leq b} \psi_\nitem(b")}
\end{align}
By strong induction, it is straightforward to show that indeed $q^\psi(\nitem, b) = \prob_{\bm{b} \sim \psi}(s_\nitem = b_\nitem)$. We define $\mathcal{Q}$ to be the set of all possible state occupancy measures $\bm{q}$ such that there exists a $\psi \in \Psi$ that generates $\bm{q}$. More formally,
\begin{align}
    \mathcal{Q} \equiv \{\bm{q} \in [\Nitem] \times \mathcal{B} \to [0, 1]: \exists \psi \in \Psi \text{ such that } q^\psi(\nitem, b') = \psi_\nitem(b') \sum_{b \geq b'}  \frac{q^\psi(\nitem - 1, b)}{\sum_{b" \leq b} \psi_\nitem(b")} \}
    \label{def: Qspace}
\end{align}
As such, the loss can be rewritten as:
\begin{align}
    \mathbb{E}_{\bm{b} \sim \psi}\left[ \mu^{\nround}_n(\bm{b}) \right] = \mathbb{E}_{\bm{b} \sim \psi}\left[ \sum_{\nitem=1}^\Nitem w_\nitem^\nround(b_\nitem) \right] = \sum_{\nitem=1}^\Nitem \prob_{\bm{b} \sim \psi}(s_\nitem = b_\nitem) w_\nitem^\nround(b_\nitem)
\end{align}
Substituting in our definition $q^\psi(\nitem, b) = \prob_{\bm{b} \sim \psi}(s_\nitem = b_\nitem)$, we obtain:
\begin{align}
    \mathbb{E}_{\bm{b} \sim \psi}\left[ \mu^{\nround}_n(\bm{b}) \right] = \sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q^\nround(\nitem, b_\nitem) w_\nitem^\nround(b_\nitem) = \langle \bm{q}^\nround, \bm{w}^\nround\rangle
\end{align}
Where $\bm{w}^\nround = \{w_\nitem^\nround(b)\}_{\nitem \in [\Nitem], b \in \mathcal{B}}$ which is different as in the previous section where it was defined additionally over $b' \leq b$. Following a similar argument, we obtain:
\begin{align}
    \textsc{Regret}_\mathcal{b}(\Nround) \leq \max_{\bm{q} \in \mathcal{Q}} \sum_{\nround=1}^\Nround \langle  \bm{q}^\nround - \bm{q}, -\bm{w}^\nround \rangle
\end{align}
We can now repeat the same algorithm and analysis as in above, except using state occupancy measures $\bm{q}$ as opposed to state-action occupancy measures $\bm{\rho}$. The primary benefit is that in the regret analysis, we can replace the summation over $(\nitem, b, b')$ with a summation over $(\nitem, b)$. This allows us to obtain sharper regret bounds of $O(\Nitem \sqrt{|\mathcal{B}| \Nround \log |\mathcal{B}|})$. Note that in order to sample a monotone bid vector, we must first reconstruct the policy $\bm{\pi}$ corresponding to the recovered $\bm{q}$. To do this, we first compute $\bm{\psi}$ recursively corresponding to $\bm{q}$, and from this, compute $\bm{\pi}$ using $\pi((\nitem, b), b') = \frac{\psi(\nitem+1, b')}{\sum_{b" \leq b} \psi(\nitem+1, b")}$. To compute $\bm{\psi}$ from $\bm{q}$, notice that for any $\nitem$:
\begin{align}
    q(\nitem, b_0) = \psi_\nitem(b_0) \sum_{b \geq b_0} \frac{q(\nitem-1, b)}{\sum_{b" \leq b} \psi_\nitem(b")} = \psi_\nitem(b_0) \frac{q(\nitem-1, b_0)}{\sum_{b" \leq b_0} \psi_\nitem(b")} = \psi_\nitem(b_0) q(\nitem-1, b_0)
\end{align}
Where the last equality follows as $\sum_{b" \leq b_0} \psi_\nitem(b") = \sum_{b" \in \mathcal{B}} \psi_\nitem(b") = 1$. In the recursive case, we have:
\begin{align}
    q(\nitem, b') = \psi_\nitem(b')\sum_{b \geq b'} \frac{q(\nitem-1, b)}{\sum_{b" \leq b} \psi_\nitem(b")} = \psi_\nitem(b')\sum_{b \geq b'} \frac{q(\nitem-1, b)}{1 - \sum_{b" > b} \psi_\nitem(b")}
\end{align}
Hence, we can solve for $\psi_\nitem(b') = q(\nitem, b')\left(\sum_{b \geq b'} \frac{q(\nitem-1, b)}{1 - \sum_{b" > b} \psi_\nitem(b")}\right)^{-1} $ in terms of $\bm{q}$ (which is known) and $\psi_\nitem(b")$ for $b" \geq b'$, which is known from induction. A more serious complication is that the projection step is now more involved, requiring the proportional policy constraints. What we can do is first convert $\bm{\psi}^\nround$ into an equivalent $\bm{\pi}^\nround$ and $\bm{\rho}^\nround$. We then compute the maximizer w.r.t. all state-action occupancy measures in $\bm{\rho} \in \Delta(\Pi)$ with the additional constraint that $\bm{\rho}$ must have a corresponding $\bm{\pi}^\rho \in \Pi'$. As there are only polynomially many $O(\Nitem |\mathcal{B}|)$ constraints on $\pi$ required for proportionality, and that there are polynomially many $O(\Nitem |\mathcal{B}|)$ constraints required for some arbitrary $\rho \in \mathcal{S} \times \mathcal{A} \to [0, 1]$ to have a corresponding generating policy, then the projection step can still be solved efficiently. \rigel{This next part of the paragraph makes it seem like our approach is too straightforward} Perhaps more simply, we can instead solve a modified bid optimization shortest path problem instance. In particular, we can set each existing edge cost to 0, duplicate each layer with a single edge between same-bid pairs $b$ in row $\nitem$ and the duplicated row $\nitem$ with reward $w^\nround_\nitem(b)$ (see Figure. We can then invoke standard $\textsc{O-REPS}$ on this modified problem in order to obtain the policy $\bm{\rho}^{\nround+1}$ whilst maintaining the lower regret of $\textsc{Node O-REPS}$. Once we have computed $\bm{\rho}^{\nround+1}$, we translate this back into $\bm{q}^{\nround+1}$ and the corresponding $\bm{\psi}^{\nround + 1}$.

% Figure environment removed


\begin{algorithm}[t]
	\KwIn{Valuation $\bm{v} \in [0, 1]^\Nitem$, Learning rate $\eta > 0$, Adaptive Adversarial Environment $\textsc{Env}^\nround: \mathcal{H}^\nround \to \mathcal{B}^{-\Nitem} \times \mathcal{B}$ where $\mathcal{H}^\nround$ denotes the set of all possible historical auction results $H^\nround$ up to round $\nround$ for all $\nround \in [\Nround]$.}
	\KwOut{The aggregate utility $\sum_{\nround=1}^\Nround \mu(\bm{b}^\nround)$.}
	$\psi^0(\nitem, b) \gets \frac{1}{|\mathcal{B}|}$ for all $\nitem \in [\Nitem], b \in \mathcal{B}$. Let $\bm{q}^0$ be the corresponding state-action occupancy measure, where $q^\nround(\nitem, b)$ is the probability of selecting bid $b$ at state $\nitem$ \;
        $H^0 \gets \emptyset$ \;
	\For{$\nround \in [\Nround]$:}{
            $\bm{b}^{\nround}_-\gets \textsc{Env}^{\nround-1}(H^{\nround-1})$ and $\bm{b}^{\nround} \sim \bm{\pi}^{\nround-1}$ \rigel{Explain that we need to be more explicit about sampling from $\psi$ since it needs to have monotonicity, get rid of $\psi$}\;
            Receive reward $\mu^\nround_n(\bm{b}^\nround) = \sum_{\nitem=1}^\Nitem w_\nitem^\nround(b^\nround_\nitem)$ and observe $w_\nitem^\nround(b^\nround_\nitem)$ where $w_\nitem^\nround(b)$ is as defined in Equation~\ref{def: def mu w W}\;
            $\hat{w}_\nitem^\nround(b') \gets \frac{w_\nitem^\nround(b')}{q^{\nround-1}(\nitem, b')} \textbf{1}_{b' = b^{\nround}_\nitem}$ for all $\nitem \in [\Nitem]$ and $b' \in \mathcal{B}$\;
            $\bm{q}^\nround \gets \text{argmin}_{\bm{q} \in \mathcal{Q}} \eta\langle \bm{q}, -\hat{\bm{w}}^\nround\rangle + D(\bm{q} || \bm{q}^{\nround-1})$ where $\mathcal{Q}$ is as in Equation~\ref{def: Qspace} \;
            Recursively compute for all $\nitem$, $b$ in decreasing order $\psi^\nround_\nitem(b') \gets q^\nround(\nitem, b')\left(\sum_{b \geq b'} \frac{q^\nround(\nitem-1, b)}{1 - \sum_{b" > b} \psi^\nround_\nitem(b")}\right)^{-1}$ \rigel{Move the $\nitem$ in the subscript} \;
            Compute $\pi^\nround((\nitem, b), b') \gets \frac{\psi^\nround(\nitem+1, b')}{\sum_{b" \leq b} \psi^\nround(\nitem+1, b")}$ for all $\nitem \in [\Nitem], b \geq b' \in \mathcal{B}$ \rigel{Get rid of $\pi$ here and then just explicitly write $\frac{\psi^\nround(\nitem+1, b')}{\sum_{b" \leq b} \psi^\nround(\nitem+1, b")}$ in the sampling $\bm{b}$ step}\;
        }
        \textbf{Return} $\sum_{\nround=1}^\Nround \mu(\bm{b}^\nround)$
	\caption{\textsc{Node O-REPS}}
	\label{alg: Node O-REPS}
\end{algorithm}

\begin{theorem}
    Under bandit feedback, Algorithm~\ref{alg: Node O-REPS} achieves regret rate $\textsc{Regret}_\mathcal{B}(\Nround) \lesssim O(\Nitem \sqrt{|\mathcal{B}| \Nround \log |\mathcal{B}|})$ using $\eta = \sqrt{\frac{\log |\mathcal{B}|}{|\mathcal{B}|\Nround}}$.
\end{theorem}

\begin{proof}
    The justification follows that of Algorithm~\ref{alg: O-REPS} and only changes at line~\ref{eq: node diff}, where we instead have:
    \begin{align}
        \textsc{Regret}_\mathcal{B}(\Nround) \leq \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}(\nitem, b') \hat{w}^{\nround}_\nitem(b)^2 + \eta^{-1}D(\bm{q} || \bm{q}^{1}) \right]
    \end{align}
    Similarly, bounding the entropy term $D(\bm{q} || \bm{q}^1)$ only requires taking a sum over $(\nitem, b)$ pairs rather than $(\nitem, b, b')$. Nonetheless, since the entropy bound is logarithmic in the distribution size, we achieve the same bound of $O(\Nitem |\mathcal{B}|)$. Plugging this back in, we obtain:
    \begin{align}
        \textsc{Regret}_\mathcal{B}(\Nround) \leq \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}(\nitem, b') \hat{w}^{\nround}_\nitem(b)^2 + \eta^{-1}\Nitem |\mathcal{b}| \right]
    \end{align}
    We can upper bound the first term with $\eta \Nround \Nitem |\mathcal{B}|$ in the bandit setting and $\eta \Nround \Nitem $ in the full information setting using the same arguments as in $\textsc{O-REPS}$. Setting $\eta = \sqrt{\frac{\log |\mathcal{B}|}{|\mathcal{B}|\Nround}}$ yields the desired regret bound in the bandit setting. Similarly, setting $\eta = \sqrt{\frac{\log |\mathcal{B}|}{\Nround}}$ in the full information setting yields regret rate $O(\Nitem\sqrt{\Nround \log|\mathcal{B}|})$.
\end{proof}



\begin{theorem}
    Under bandit feedback, Algorithm~\ref{alg: Node O-REPS} achieves regret rate $\textsc{Regret}_\mathcal{B}(\Nround) \lesssim O(\Nitem \sqrt{|\mathcal{B}| \Nround \log |\mathcal{B}|})$ using $\eta = \sqrt{\frac{\log |\mathcal{B}|}{|\mathcal{B}|\Nround}}$.
\end{theorem}

\fi