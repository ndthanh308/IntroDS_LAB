\newpage

\TITLE{Learning in Repeated Multi-Unit Pay-As-Bid Auctions}

\ARTICLEAUTHORS{
\AUTHOR{Rigel Galgana}
\AFF{Operations Research Center, Massachusetts Institute of Technology, \EMAIL{rgalgana@mit.edu}, \URL{}}
\AUTHOR{Negin Golrezaei}
\AFF{Sloan School of Management, Massachusetts Institute of Technology,  \EMAIL{golrezaei@mit.edu}, \URL{}}
} 

\begin{center}
    {\LARGE{Learning in Repeated Multi-Unit Pay-As-Bid Auctions}}

    {\small{Rigel Galgana}}

    {\scriptsize{Operations Research Center, Massachusetts Institute of Technology, rgalgana@mit.edu}}

    {\small{Negin Golrezaei}}

    {\scriptsize{Sloan School of Management, Massachusetts Institute of Technology, golrezaei@mit.edu}}
\end{center}

\maketitle

In this online accompaniment, we include all of the omitted proofs of several of our key results as well as additional discussion and experiments regarding algorithm implementation and market dynamics. We conclude with a discussion regarding both the feasibility and practicality of the PAB versus Uniform Price auction formats.

\section{Online Appendix - Missing Proofs}

In this section, we include proofs of several of our key results. We first precisely characterize the PAB equilibrium under our $c = \lfloor v_{(M)}\rfloor_\delta = c_{-n}$ competitiveness assumption. Second, we flesh out the details of our OMD algorithm as well as its full proof, regret analysis, and complexity analysis. Lastly, we prove our time-varying valuations generalization of our decoupled exponential weights algorithm.


\subsection{Proof of Theorem \ref{thm: PNE existence}: Existence of an Approximately Efficient PNE}
\label{sec: equilibrium proof}

\textsc{Theorem 1} \textbf{(Existence of an Apprixxmately Efficient PNE)}

\emph{Define the clearing price $c = \lfloor v_{(M)} \rfloor_{\delta}$ to be the $M$'th largest valuation among all bidders, denoted by $v_{(M)}$,  rounded down to the nearest multiple of $\delta$, and similarly, define $c_{-n} = \lfloor v_{(-n, M)} \rfloor_{\delta}$ to be the rounded $M$'th largest valuation among all bidders except $n$. If $c = c_{-n}$ for all $n\in [N]$ and ties are broken in favor of higher indexed bidders, then there exists a PNE $(\bm{b}_1,\ldots,\bm{b}_N)$ where each bidder $n\in [N]$:  
    \begin{enumerate}
        \item submits bids of either all $c$ or all $c + \delta$ for units such that $v_{n,m} \geq c + \delta$,
        \item submit bids of $c$ for all units such that $v_{n,m} \in [c, c + \delta)$,
        \item submit bids smaller than $c$ for all other units.
    \end{enumerate}
    Moreover, this PNE is $M\delta$-approximately welfare optimal:
    \begin{align*}
        \sum_{m=1}^M v_{(m)} - \sum_{n=1}^N \sum_{m=1}^{x(\bm{b}_n, \bm{b}_{-n})} v_{n,m} \leq M\delta\,.
    \end{align*}}

{\color{black}
\begin{proof}{Proof of Theorem~\ref{thm: PNE existence}}

To prove Theorem~\ref{thm: PNE existence}, we proceed in two steps. First, we demonstrate that if all other bidders adhere to the three properties outlined in Theorem~\ref{thm: PNE existence}, then bidder $n$'s optimal bids must also satisfy these properties. Afterwards, using a monotonicity argument, we show that under our particular deterministic tie-breaking rule, there exists a specific configuration of bids—either $c$ or $c + \delta$—among each bidder’s winning bids that constitutes a PNE.

\textbf{First Part of the Proof.} {We first show the three properties. The first property implies that if for every bidder $i\ne n$, we have  bids of either all $c$ or all $c + \delta$ for units such that $v_{i,m} \geq c + \delta$, then bidder $n$ also submits bids of  either all $c$ or all $c + \delta$ for units such that $v_{n,m} \geq c + \delta$. This 
follows immediately from Lemma~\ref{lem: PNE uniform bidding} and recalling the definition of $c$.

 To demonstrate the second property, we argue that there is no incentive for bidder $n$ to decrease their winning bids below $c$, specifically for any unit $m$ with $v_{n, m} \in [c, c+\delta)$. Given that there are at least $M$ other valuations equal to or greater than $c = c_{-n}$, and considering the PNE characterization which states that for any bidder $i \neq n$, all units with a valuation at or above the clearing price (i.e., $v_{i, m} \geq c$, $i \in [N], i\ne n$) must be accompanied by a bid of either $c$ or $c + \delta$, it follows that there are at least $M$ bids of at least $c = c_{-n}$ submitted by other bidders. Therefore, reducing any of bidder's $n$ winning bids below $c$ would not result in winning a unit, making it sub-optimal.


Lastly, to show the final property, we show that there exists no incentive for bidder $n$ to increase their bid for the remaining items (i.e., any item $m$ with $v_{n, m}< c$) to at least $c$, we recall the definition of $c= c_{-n}$ being the $M$'th largest valuation among all bidders except bidder $n$. Thus, bidding above $c$ for these units violates the no-overbidding assumption.}

\textbf{Second Part of the Proof.} Now that we have shown the three properties, we fully characterize the PNE w.r.t. each bidders' largest bids. In particular, consider the perspective of bidder $n$, who has tie-break priority over bidders $1,\ldots,n-1$ but behind $n+1,\ldots,N$. Thus, any bids of $c+\delta$ submitted by the first $n-1$ bidders, and any bids of $c$ or $c + \delta$ submitted by bidders $n+1,\ldots,N$, take priority over any bids of $c$ submitted by bidder $n$. Let $M_n$ denote the number of bids of $c+\delta$ submitted by bidder $n$, which by our PNE characterization, is between $0$ and $\sum_{m=1}^M 1_{v_{n, m} \geq c + \delta}$. Fixing $M_{1:n-1}$ and $M_{n+1:N}$, we claim that the optimal $M_n$ is precisely either 0 or $\sum_{m=1}^M 1_{v_{n, m} \geq c + \delta}$---they either submit bids of all $c$ or all $c+\delta$ for all items they value at least $c + \delta$. 

To show this, notice that bidder $n$ can win all of the items they value at least $c + \delta$ by bidding at $c + \delta$, as there are fewer than $M$ values at least $c+\delta$ across all bidders. In addition to this, they can win some number of items by bidding at $c$ for all items with value at least $c$ via tie-break. To be more specific, 
there are $M$ items with $\sum_{n' \leq n} M_{n'}$ bids of $c + \delta$ and $\sum_{n' > n} \sum_{m=1}^M 1_{v_{n',m} \geq c}$ bids of at least $c$ (by the PNE characterization) that take priority over any bids of $c$ submitted by bidder $n$, where we note that the number of bids of $c$ submitted by bidder $n$ is at most $\sum_{m=1}^M 1_{v_{n,m} \geq c}$. Thus, bidder $n$'s allocation and utility as a function of $M_n$ for any fixed $\bm{M}_{-n} = (M_{-1},\ldots,M_{-(n-1)},M_{-(n+1)},\ldots,M_{-N})$ is given by:

\begin{align*}
    x_n(M_n | \bm{M}_{-n}) &= \min\left(\sum_{m=1}^M 1_{v_{n,m} \geq c}~,~ M_n + \max\big(0, \theta(\bm{M}_{-n}) - M_n\big)\right)\\
    \hspace{1mm} \mu_n(M_n | \bm{M}_{-n}) &= - M_n\delta +\sum_{m=1}^{x_n(M_n | \bm{M}_{-n})} (v_{n,m} - c)\,,
\end{align*}
where $\theta(\bm{M}_{-n}) = M - \sum_{n' < n} M_{n'} - \sum_{n' > n} \sum_{m=1}^M 1_{v_{n',m} \geq c}$. Here, $\min\left(\sum_{m=1}^M 1_{v_{n,m} \geq c}, \theta(\bm{M}_{-n})\right)$ denotes the number of units bidder $n$ would have won in tie-break by submitting $M_n = 0$ bids of $c + \delta$.
Now, let's consider the general case where $M_n\ge 0$. For  all $M_n \geq \theta(\bm{M}_{-n})$, the $\max(0, \theta(\bm{M}_{-n}) - M_n)$ term in the allocation function is 0, and thus, the allocation function  increases linearly in $M_n$ until $M_n = \sum_{m=1}^M 1_{v_{n,m} \geq c+\delta}$. In contrast, for $M_n <\theta(\bm{M}_{-n})$, the second term (i.e., $\max(0, \theta(\bm{M}_{-n}) - M_n)$) is non-zero, and the $M_n$ within the summation of the second term cancels out with the first term of $M_n$. Thus, the allocation function $x_n(M_n | \bm{M}_{-n}) = \theta(\bm{M}_{-n})$ is constant for all $M_n \leq \theta(\bm{M}_{-n})$, at which point it becomes precisely $x_n(M_n | \bm{M}_{-n}) = \theta(\bm{M}_{-n}) = M_n$ until $M_n = \sum_{m=1}^M 1_{v_{n, m} \geq c + \delta}$. This reflects the fact that each additional bid at $c + \delta$ submitted by bidder $n$ consumes an item that could have been won in tie-break at price $c$, which we illustrate in Figure~\ref{fig: allocation visualization PNE}. From Figure~\ref{fig: allocation utility vs mn}, we see that the optimal $M_{-n}$ is always achieved at $M_{-n} \in \{0, \sum_{m=1}^M 1_{v_{n,m} \geq c + \delta}\}$.

% Figure environment removed



% Figure environment removed

    
    Now that we have shown that the optimal number of bids to submit at $c + \delta$ of each bidder is either $M_n = 0$ or $M_n = \sum_{m=1}^M 1_{v_{n, m} \geq c + \delta}$, we finish the argument by claiming that the utility corresponding to $M_n = 0$ is weakly decreasing in $\bm{M}_{-n}$. This is because $x_n(\cdot | \bm{M}_{-n})$, and similarly $\mu_n(\cdot | \bm{M}_{-n})$, are weakly decreasing in these quantities. Because of this monotonicity, agents can run best response dynamics beginning with $(M_1,\ldots,M_N) = (0, \ldots, 0)$ and converge to a PNE w.r.t. $M_1,\ldots,M_N$. That is, each agent that switches from $M_n = 0$ to $M_n = \sum_{m=1}^M 1_{v_{n, m} \geq c + \delta}$ can only incentivize other bidders to also switch away from 0 and never towards 0.
    
\end{proof}

}

\subsection{Lemmas \ref{lem: QSpace Equivalence} and \ref{lem: Online Linear Optimization}, and their Proofs} \label{sec:QSpace Equivalence} 

Now, we complete the description and analysis of our OMD based algorithm. In Algorithm~\ref{alg: OMD}, we require that the space of all possible node probability measure $\bm{q}$ encompasses the set of node probability measures that correspond to any policy $\bm{\pi}$ over our DP graph.

\begin{lemma}[$\mathcal{Q}$-Space Equivalence]
    \label{lem: QSpace Equivalence}
    Let \[\Pi = \Big\{{\pi} \in [0,1]^{M\times |\mathcal B|\times |\mathcal B|}: \pi((m, b), b') = 0 ~~\forall b' > b, m\in [M], \sum_{b' \leq b} \pi((m, b), b') = 1, m\in [M]\Big\}\] denote the space of policies on our DP graph. With a slight abuse of notation, for any $\pi\in  \Pi$, define 
    \[q(\pi) = \{\mathbf{q} \in [0, 1]^{M\times |\mathcal B|}: \forall b \in \mathcal{B},  q_1(b) =\pi((0, b_0), b), q_{m+1}(b) = \sum_{b' \in \mathcal B} q_m(b')\pi((m, b'), b), m\in[M-1]\}\,\] as the node probabilities induced by $\pi$. Here, $b_0= \max \mathcal B$. Let $\mathcal{Q}_{\Pi} = \cup_{\pi \in \Pi} q(\pi)$. Then,   $\mathcal{Q}_{\Pi}$ is equivalent to the set $\mathcal{Q}$ where $\mathcal{Q}$ is defined in Equation \eqref{eq:Q}. 
\end{lemma}



Lemma \ref{lem: QSpace Equivalence} establishes that during the execution of Algorithm \ref{alg: OMD}, we can focus on the node probabilities in set $\mathcal{Q}$ without loss of generality. We recall that within $\mathcal{Q}$, the stochastic dominance conditions are enforced solely over node probabilities across layers. 
We now argue that we only need to consider optimizing over $\mathcal{Q}$ as opposed to the space of policies $\Pi$, as the regret can be rewritten strictly in terms of $\bm{q}$, independently of the corresponding $\bm{\pi}$.


\begin{lemma}
    \label{lem: Online Linear Optimization}
     Any sequence of policies $\bm{\pi}^1,\ldots,\bm{\pi}^\Nround$ over our DP graph with associated node probability measures $\bm{q}^1,\ldots,\bm{q}^\Nround$ has discretized regret $\textsc{Regret}_{\mathcal{B}} = \max_{\bm{q} \in \mathcal{Q}} \sum_{\nround=1}^\Nround \langle \bm{q} - \bm{q}^\nround, \bm{w}^\nround\rangle$. Here, $\bm{w}^\nround = \{w^\nround_m(b)\}_{m \in [M], b \in \mathcal{B}}$ represents vector of the round $\nround$ rewards for all possible $(m, b)$ unit-bid value pairs.
 \end{lemma}
      
\subsubsection{Proof of Lemma \ref{lem: QSpace Equivalence}}

In order to show equivalence, we show that (1) for any $\pi \in \Pi$, that $q(\pi) \in \mathcal{Q}$ and (2) for any $\bm{q} \in \mathcal{Q}$, there exists a $\pi \in \Pi$ such that $q(\pi) = \bm{q}$. We first prove (1). To do this, we simply need to check that for a given $\pi \in \Pi$, that $q^\pi = q(\pi)$ satisfies the constraints prescribed by $\mathcal{Q}$.

    The non-negativity constraint holds trivially as each $\pi((m, b), b')$ is non-negative. Since all $q^\pi_1(b) = \pi((0, \max\mathcal{B}), b) \geq 0$ for all $b \in \mathcal{B}$, by induction, $q^\pi_{m+1}(b) = \sum_{b" \geq b} q^\pi_m(b") \pi((m, b"), b)$ is also non-negative.
    
    Now we prove that each layer $m$ sums to 1, i.e., $\sum_{b \in \mathcal{B}} q^\pi_m(b) = 1$. Since $\sum_{b \in \mathcal{B}} q^\pi_1(b)$, the policy has total node probability 1 in the first layer, we can prove $\sum_{b \in \mathcal{B}} q^\pi_m(b) = 1$, that the policy has total node probability 1 in the $m$'th layer, via induction. This follows immediately from the fact that the DP graph is layered, i.e., edges exist only from nodes in layer $m$ to nodes in layer $m+1$, thus the only edges leading to layer $m+1$ are from layer $m$, in which there are no other edges. Hence, the total node probability in layer $m+1$ must be exactly that of layer $m$. More formally, we have:
    \begin{align*}
        \sum_{b \in \mathcal{B}} q^\pi_{m+1}(b) = \sum_{b \in \mathcal{B}} \sum_{b" \geq b} q^\pi_m(b") \pi((m, b"), b) = \sum_{b" \in \mathcal{B}} q^\pi_m(b") \sum_{b \leq b"} \pi((m, b"), b) = \sum_{b" \in \mathcal{B}} q^\pi_m(b")\,.
    \end{align*}

     To show the stochastic domination constraint $\sum_{b \leq b'} q^\pi_{m+1}(b) \geq \sum_{b \leq b'} q^\pi_m(b)$, we use the bid monotonicity constraint; i.e., the fact that the edges between layers are only from larger bids to (weakly) smaller bids. Recall that $\pi((m,b'), b")$ is the probability of transitioning from unit-bid value pair $(m, b')$ to $(m+1, b")$ and that the only edges leading to $(m+1, b")$ come from nodes $(m, b')$ for $b' \geq b"$. Then, we have:
     \begin{align*}
         \sum_{b \leq b'} q^\pi_{m+1}(b) &= \sum_{b \leq b'} \sum_{b" \geq b} q^\pi_m(b")\pi((m, b"), b) \\
         &=\sum_{b" > b'} q^\pi_m(b") \sum_{b \leq b'} \pi((m, b"), b) + \sum_{b" \leq b'} q^\pi_m(b") \sum_{b \leq b"} \pi((m, b"), b)\\
         &= \sum_{b" > b'} q^\pi_m(b") \sum_{b \leq b'} \pi((m, b"), b) + \sum_{b \leq b'} q^\pi_m(b)\\
         &\geq \sum_{b \leq b'} q^\pi_m(b)\,.
     \end{align*}
     Hence, we have shown that for any $\pi \in \Pi$, that the corresponding $q(\pi) \in \mathcal{Q}$.
     
     Now we show the other direction (2), that for any $\bm{q} \in \mathcal{Q}$, there exists a $\pi \in \Pi$ such that $q(\pi) = \bm{q}$. We proceed by showing that for all $m, b^*$, there exists $\{\pi((m, b), b')\}_{b, b' \in \mathcal{B}}$ such that the following conditions hold:
     \begin{enumerate}
         \item $\pi((m, b), b') \geq 0$ for all $b, b' \geq b^*$.
         \item $\pi((m, b), b') = 0$ for all $b' > b \geq b^*$.
         \item $\sum_{b' \leq b, b' \geq b^*} \pi((m, b), b') \leq 1$ for all $b^* \in \mathcal{B}$, with equality if and only if $b^* =  b_{\min}$ where $b_{\min} = \min \mathcal{B}$.
         \item $\sum_{b' \geq b^*} \sum_{b \geq b'} q_m(b)\pi((m, b), b') = \sum_{b' \geq b^*} q_{m+1}(b')$.
     \end{enumerate}
Let $\Pi(b^*; \mathbf{q})$, $b^*\in \mathcal B$, be the set of all policies under which the four conditions hold at $b^*$ and ${\mathbf q} \in \mathcal Q$. 
     
    These conditions trivially hold for $m = 0$, as we can set $\pi((0, \max \mathcal{B}), b) = q_1(b)$ and $\pi((0, b), b') = \textbf{1}_{b = b'}$. To solve for general $m$, we must show that there exists $\{\pi((m, b), b')\}_{b, b' \in \mathcal{B}}$ that satisfies the constraints prescribed by $\Pi$ and that $\sum_{b \geq b'} q_m(b)\pi((m, b), b') = q_{m+1}(b')$ for all $b' \in \mathcal{B}$. In order to do this, we show that conditions (1), (2), (3), and (4) for each $b^* \in \mathcal{B}$. In particular, if we show conditions (1) and (2) for $b^* = b_{\min}$, then we have already satisfied the first two conditions of $\Pi$. If we show that (3) holds for $b^* = b_{\min}$, then by condition (1), then (3) holds for all $b^* \in \mathcal{B}$ as well, as the summation only includes fewer terms as $b^*$ increases. Similarly, if we show condition (4) holds for two adjacent values of $b_-^* < b^*$, then we have that $\sum_{b \geq b' \geq b^*} q_m(b)\pi((m, b), b') = q_{m+1}(b^*)$. Thus, if condition (4) holds for all possible pairs of adjacent bid values, then we have that $\sum_{b \geq b'} q_m(b)\pi((m, b), b') = q_{m+1}(b')$ for all $b'$. These observations suggest use of induction over $b^*$, and indeed, we begin by showing that these conditions hold for $b^* = b_{\min}$. We then show that this implies that the conditions hold for the next smallest value of $b^*$, which would complete the induction proof.

    \textbf{Base Case}: Recall $b^* = b_{\min}$. We now show that there exists $\{\pi((m, b), b')\}_{b, b' \in \mathcal{B}}$ satisfying all four conditions. For any $m\in[M]$, let we set $\pi((m, b), b') = \textbf{1}_{b = b'}$. Then, 
 condition (4) is clearly satisfied: 
    \begin{align*}
        \sum_{b' \geq b^*} \sum_{b \geq b'} q_m(b)\pi((m, b), b') = \sum_{b' \geq b^*} q_{m+1}(b') \leftrightarrow \sum_{b \in \mathcal{B}} q_m(b) \sum_{b' \leq b} \pi((m, b), b') = \sum_{b \in \mathcal{B}} q_{m+1}(b) = 1\,.
    \end{align*}
    It is also easy to check that conditions (1)-(3) are also satisfied when we set $\pi((m, b), b') = \textbf{1}_{b = b'}$ for any $m$. This shows that $\Pi(b^*; {\mathbf{q}})$ is non-empty, as desired. 
   
    
    \textbf{Recursive Case}:
    For any $b\in \mathcal B$, let $b_-$ be the largest $b' \in \mathcal B$, which is strictly smaller than $b$. Here, we assume that 
    $\Pi(b^*_-; \bf{q})$ is not empty, and under this assumption, we show that set $\Pi(b^*; \bf{q})$ is not empty, where $\Pi(b^*; {\bf{q}}) \subseteq \Pi(b^*_-; \bf{q})$. 
    Let us start with condition (4). 
    We would like to show that there exists a $\bm{\pi}$ that satisfies condition (4) at $b^*$ along with the other three conditions. By the induction assumption, we have  
    \begin{align*}
        &\sum_{b' \geq b_-^*} \sum_{b \geq b'} q_m(b)\pi((m, b), b') = \sum_{b' \geq b_-^*} q_{m+1}(b') \to\\
        &\sum_{b' \geq b^*} \sum_{b \geq b'} q_m(b)\pi((m, b), b') + \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*) = \sum_{b' \geq b^*} q_{m+1}(b') + q_{m+1}(b_-^*) \to\\
        &\sum_{b' \geq b^*} \sum_{b \geq b'} q_m(b)\pi((m, b), b') = \sum_{b' \geq b^*} q_{m+1}(b') + \left[q_{m+1}(b_-^*) - \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*)\right] \to\\
        &\sum_{b \geq b^*} q_m(b) \sum_{b' \leq b; b' \geq b^*} \pi((m, b), b') = \sum_{b' \geq b^*} q_{m+1}(b') + \left[q_{m+1}(b_-^*) - \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*)\right] 
    \end{align*}
    Thus, we can satisfy condition (4) if $q_{m+1}(b_-^*) = \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*)$. We now observe that the latter summation depends linearly (and hence, continuously) in the values of $\pi((m, b), b_-^*)$. If we can show that there exists an assignment of these variables that satisfy $q_{m+1}(b_-^*) \geq \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*)$ and also $q_{m+1}(b_-^*) \leq \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*)$, then by the intermediate value theorem, there must be some assignment that achieves exact equality. 
    
    
    In order to show the first inequality, notice that if we set $\pi((m, b), b_-^*) = 1 - \sum_{b' < b_-^*} \pi((m, b), b')$ for all $b \geq b_-^*$ (this is required in order to guarantee conditions (1) and (3) are satisfied), then:
    \begin{align*}
        \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*) &= \sum_{b \geq b_-^*} q_m(b) - \sum_{b \geq b_-^*} q_m(b)\sum_{b' < b_-^*} \pi((m, b), b') \\
        &= \sum_{b \geq b_-^*} q_{m}(b) - \sum_{b \in \mathcal{B}} q_m(b) \sum_{b' < b_-^*} \pi((m, b), b') + \sum_{b < b_-^*} q_m(b) \sum_{b' < b_-^*} \pi((m, b), b')\\
        &= \sum_{b \geq b_-^*} q_{m}(b) - \sum_{b' < b_-^*} q_{m+1}(b) + \sum_{b < b_-^*} q_m(b) \sum_{b' < b_-^*} \pi((m, b), b')\\
        &= \sum_{b \geq b_-^*} q_{m}(b) - \sum_{b' < b_-^*} q_{m+1}(b) + \sum_{b < b_-^*} q_m(b)\\
        &\geq \sum_{b \geq b_-^*} q_{m+1}(b) - \sum_{b' < b_-^*} q_{m+1}(b) + \sum_{b < b_-^*} q_{m+1}(b)\\
        &= \sum_{b \geq b_-^*} q_{m+1}(b)\\
        &\geq q_{m+1}(b_-^*)\,.
    \end{align*}
    Here, the third equality follows from the (strong) inductive hypothesis, and the first inequality is a result of the stochastic domination constraint in $\mathcal{Q}$. We also note that the values $\sum_{b' < b_-^*} \pi((m, b), b')$ have already been fixed
    as these were required to satisfy condition (4) in the previous iterates, and as condition (3) holds for $b^*_-$ by the inductive hypothesis, then $1 - \sum_{b' < b_-^*} \pi((m, b), b') \geq 0$. Conversely, if we set $\pi((m, b), b_-^*) = 0$ for all $b \geq b_-^*$, then:
    \begin{align*}
        \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*) = 0 \leq q_{m+1}(b_-^*)\,.
    \end{align*}
    As the sum $\sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*)$ linearly (thus, continuously) depends on the values of $\pi((m, b), b_-^*)$, by the intermediate value theorem, there exists an assignment of $\{\pi((m, b), b_-^*)\}_{b \geq b_-^*}$ with each $\pi((m, b), b_-^*) \in [0, 1 - \sum_{b' < b_-^*} \pi((m, b), b')]$ such that the sum is precisely equal to $q_{m+1}(b_-^*) \in [0, 1]$. Now we observe that these values of $\pi((m, b), b_-^*) \in [0, 1 - \sum_{b' < b_-^*} \pi((m, b), b')]$ do not violate conditions (1), (2), or (3). Furthermore, note that any $\bm{\pi} \in \Pi$ also satisfied conditions (1), (2), and (3) under $b^*_-$ for $\{\pi((m, b), b')\}_{b \geq b_-^*, b' \leq b_-^*}$, then the assignment to $\{\pi((m, b), b')\}_{b \geq b_-^*, b' < b_-^*}$ will not violate these conditions as our new constraint on the variables $\{\pi((m, b), b_-^*)\}_{b \geq b_-^*}$ is independent of the values of $\{\pi((m, b), b')\}_{b \geq b_-^*, b' < b_-^*}$. Thus, the set $\Pi(b^*)$ is non-empty:
    \begin{align*}
        \Pi(b^*) = \{\{\pi((m, b), b')\}_{b, b' \in \mathcal{B}} \in \Pi(b_-^*): \sum_{b \geq b_-^*} q_m(b)\pi((m, b), b_-^*) = q_{m+1}(b_-^*)\} \neq \emptyset
    \end{align*}    
    With this, we have proven via induction that our four conditions hold for all $b^* \in \mathcal{B}$, implying that for a fixed $m$, every constraint in $\Pi$ pertaining to variables $\pi((m, b), b')$ is satisfied, as well as the node-measure constraints $\sum_{b \geq b'} q_m(b)\pi((m, b), b') = q_{m+1}(b')$ for all $b'$. By induction, this works for all $m \in [M]$, which concludes the proof.



\subsubsection{Proof of Lemma \ref{lem: Online Linear Optimization}}
We have by the definition of discretized regret:
\begin{align*}
    \textsc{Regret}_\mathcal{B} &= \max_{\bm{b} \in \mathcal{B}} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}) - \mathbb{E}\left[\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)\right] = \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{q}, \bm{w}^\nround\rangle - \sum_{\nround=1}^\Nround \langle \bm{q}^\nround, \bm{w}^\nround\rangle\right]\,, 
\end{align*}
where in the first equality, we applied Equation~\eqref{eq: Loss of policy} which equated the dot product of utilities $\bm{w}^\nround$ and node probability weights $\bm{q}$ to the expected utility of bid vector $\bm{b} \sim \bm{\pi}$ with utilities $\{w_m^t(b)\}_{m \in [M], b \in \mathcal{B}} = \bm{w}^\nround$. Combining the two summations yields the desired result.

\subsection{Proof of Theorem~\ref{thm: OMD}: Online Mirror Descent Algorithm }

\label{sec: Proof of OMD}

\begin{proof}{Proof of Theorem~\ref{thm: OMD}: Online Mirror Descent Algorithm}

    The proof is divided into four parts, similar to the analysis of Algorithm~\ref{alg: Decoupled Exponential Weights}. In the first part, we rigorously show how our algorithm achieves the stated regret. In the second, we verify correctness of our procedure that recovers a policy $\bm{\pi}^\nround$ from $\bm{q}^\nround$. Then, we show the corresponding time and space complexity of our algorithm. Afterwards, we optimize over discretization error to obtain the continuous regret. 
    

    \textbf{Part 1: Regret of Online Linear Optimization.} Recall that from Lemma~\ref{lem: Online Linear Optimization}, we have
    \begin{align}
        \textsc{Regret}_\mathcal{B} = \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[ \sum_{\nround=1}^\Nround \langle \bm{q} - \bm{q}^\nround, \bm{w}^\nround \rangle\right] = \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[\sum_{\nround=1}^\Nround \langle  \bm{q}^\nround - \bm{q}, -\bm{w}^\nround \rangle\right]\, ,
    \end{align}
    where we negate the utility function into a loss function to be consistent with the OLO convention. We follow a standard analysis of OMD, which shows that the optimization step can be solved efficiently and the resulting iterates have bounded regret. For the former, we show that solution to the $\bm{q}$ optimization step in our algorithm $\bm{q}^{\nround} = \text{argmin}_{\bm{q} \in \mathcal{Q}} \eta\langle \bm{q}, -\bm{w}^\nround\rangle + D(\bm{q} || \bm{q}^{\nround-1})$ can be obtained as the projection of the unconstrained minimizer of \[\tilde{q}^{\nround}= \text{argmin}_{\bm{q} \in [0, 1]^{M \times |\mathcal{B}|}} \eta\langle \bm{q}, -\bm{w}^\nround\rangle + D(\bm{q} || \bm{q}^{\nround-1})\] to the space $\mathcal{Q}$ (See Projection Lemma, Lemma 8.6 of \cite{BartokLecNotes2011}). 
    Having characterized the exact form of the OMD iterates, all that remains is to upper bound the regret of OMD with the regret of Be-the-regularized-leader.
    \begin{lemma}[Lemma 9.2 of \cite{BartokLecNotes2011}]
        \label{lem: Be Regularized leader regret}
        Letting $D(\bm{q} || \bm{q}')$ denote the unnormalized KL divergence between $\bm{q}$ and $\bm{q}'$, we have:
        \begin{align*}
            \textsc{Regret}_\mathcal{B} \leq \max_{\bm{q} \in\mathcal{Q}} \mathbb{E}\Big[\eta^{-1} D(\bm{q} || \bm{q}^1) + \sum_{\nround=1}^\Nround \langle \bm{q}^\nround - \tilde{\bm{q}}^{\nround+1}, \bm{w}^\nround \rangle\Big]\,.
        \end{align*}
    \end{lemma} 
     The remainder of the regret analysis closely follows that of Theorem 1 in \cite{OREPS2013}. At a high level, we want to bound the regret of Online Mirror Descent by the regret of the unconstrained Be the
     (Negentropy) Regularized leader, via Lemma~\ref{lem: Be Regularized leader regret} (see Lemma 13 of \cite{LectureNotes2009} for the more general statement and proof of this lemma). 
     We then upper the contribution of the summation term by using the specific definition of the node weight estimators. Similarly, we upper bound the divergence term as a function of the dimension of the space $\mathcal{Q}$.
     
    
    
    To begin, note that our node utility estimators $\widehat{w}_\nitem^\nround(b)$ are unbiased:
    \begin{align}
        \mathbb{E}_{\bm{b} \sim \bm{\pi}^\nround}[\widehat{w}_\nitem^\nround(b)] = \mathbb{E}_{\bm{b} \sim \bm{\pi}^\nround}[\frac{w_\nitem^\nround(b)}{q^{\nround}_\nitem(b)} \textbf{1}_{b = b^{\nround}_\nitem}] = \frac{w_\nitem^\nround(b)}{q^{\nround}_\nitem(b)} \prob_{\bm{b} \sim \bm{\pi}^\nround} (b = b^\nround_\nitem) = \frac{w_\nitem^\nround(b)}{q^{\nround}_\nitem(b)} q^{\nround-1}_\nitem(b) = w_\nitem^\nround(b)\ .
        \label{proof: part1}
    \end{align}
    Now, consider the right hand side of the inequality in  Lemma \ref{lem: Be Regularized leader regret}. As the node utility estimators are unbiased, so we can replace $\bm{w}^\nround$ with $\widehat{\bm{w}}^\nround$. 
    Now, as per Lemma \ref{lem: Be Regularized leader regret}, we can upper bound the expected estimated regret as a function of the unconstrained optimizer $\tilde{\bm{q}}^{\nround+1}$ and the unregularized relative entropy  with respect to the initial state-edge occupancy measure $\bm{q}^1$. Applying the aforementioned lemma to Equation \eqref{proof: part1}, we obtain:
    \begin{align}
        \textsc{Regret}_\mathcal{B} = \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{q}^{\nround} - \bm{q}, -\widehat{\bm{w}}^{\nround} \rangle \right] \leq \max_{\bm{q} \in \mathcal{Q}}\mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{q}^{\nround} - \tilde{\bm{q}}^{ \nround+1}, -\widehat{\bm{w}}^{\nround} \rangle + \eta^{-1}D(\bm{q} || \bm{q}^{1}) \right]
    \end{align}
    Applying $\exp(x) \geq 1 + x$ for $x = \exp(\eta \widehat{\bm{w}}^{\nround})$, we obtain $\tilde{\bm{q}}^{ \nround+1} = \bm{q}^\nround \exp(\eta \widehat{\bm{w}}^{\nround}) \geq \bm{q}^{\nround} + \eta \bm{q}^{\nround} \widehat{\bm{w}}^{\nround}$, which yields $\bm{q}^t - \bm{q}^t\exp(\eta\widehat{\bm{w}}^t) \ge  -\eta \bm{q}^t \widehat{\bm{w}}^t$. Plugging this back in:
    \begin{align}
        \textsc{Regret}_\mathcal{B} &\leq \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[\sum_{\nround=1}^\Nround \langle \bm{q}^{\nround} - \bm{q}^{\nround} \exp(\eta \widehat{\bm{w}}^{\nround}), -\widehat{\bm{w}}^{\nround} \rangle + \eta^{-1}D(\bm{q} || \bm{q}^{1}) \right]\\
        &\le  \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}_\nitem(b) \widehat{w}^{\nround}_\nitem(b)^2 + \eta^{-1}D(\bm{q} || \bm{q}^{1}) \right] \,.\label{eq: node diff}
    \end{align}
    Note that $\widehat{w}^{\nround}_\nitem(b) = \frac{w_\nitem^\nround(b)}{q^{\nround-1}_{\nitem}(b)} \textbf{1}_{b = b^{\nround}_{\nitem}}$ for all $\nitem \in [\Nitem]$ and $b \in \mathcal{B}$ by definition. Since $w^{\nround}_\nitem(b) \leq 1$ and $\textbf{1}_{b = b^{\nround}_{\nitem}} \leq 1$ we have $\widehat{w}^{\nround}_\nitem(b) \leq \frac{1}{q^{\nround}_\nitem(b)}$ and we continue the above chain of inequalities with:
    \begin{align}
        \textsc{Regret}_\mathcal{B} &\leq \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}_{\nitem}(b) \widehat{w}^{\nround}_\nitem(b) \frac{1}{q^{\nround}_{\nitem}(b)}  + \eta^{-1}D(\bm{q} || q^{1}) \right] \label{eq: full info difference appendix}\\
        &= \max_{\bm{q} \in \mathcal{Q}} \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} \widehat{w}^{\nround}_\nitem(b)  + \eta^{-1}D(\bm{q} || \bm{q}^{1}) \right] \, .
    \end{align} 
    
    Recalling that $D(\bm{q} || \bm{q}^1) = \sum_{\nitem \in [\Nitem], b \in \mathcal{B}} q_\nitem(b)\log\frac{ q_\nitem(b)}{q^1_\nitem(b)} - (q_\nitem(b) - q^1_\nitem(b))$, we note that:
    
    \begin{align*}
        D(\bm{q} || \bm{q}^1) &= \sum_{m = 1}^M \sum_{b \in \mathcal{B}} q_m(b)\frac{\log q_m(b)}{\log q^1_m(b)} - q_m(b) + q^1_m(b) \\
        &= \sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q_{\nitem}(b)\log q_m(b) - q_m(b)\log q^1_m(b)\,,
    \end{align*} 
    where in the second equality, we used the fact that the elements both $\bm{q}$ and $\bm{q}^1$ all sum to $M$. Selecting $\bm{q}^1_m(\cdot)$ to be the uniform distribution over all $b \in \mathcal{B}$ and using the fact that the entropy of a discrete distribution over $|\mathcal{B}|$ items is $\log |\mathcal{B}|$, we obtain:
    \begin{align*}
        D(\bm{q} || \bm{q}^1) &= -\sum_{\nitem=1}^\Nitem H(\bm{q}_m) + \log |\mathcal{B}|\sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q_{\nitem}(b) \\
        &\leq \sum_{\nitem=1}^\Nitem \log |\mathcal{B}| + \log |\mathcal{B}|\sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q_{\nitem}(b) = \Theta(M\log|\mathcal{B}|)\,,
    \end{align*}
    where $H(\bm{x}) = -\sum_{x \in \bm{x}} x \log x $ denotes the discrete entropy function.  
    Plugging this back in:
    \begin{align*}
        \textsc{Regret}_\mathcal{B} \leq \mathbb{E}\left[\eta \sum_{\nround=1}^\Nround \sum_{\nitem = 1}^\Nitem \sum_{b \in \mathcal{B}} \widehat{w}^{\nround}_\nitem(b)  + \eta^{-1}\Nitem \log |\mathcal{B}| \right] \leq \eta \sum_{\nround=1}^\Nround \sum_{\nitem=1} \sum_{b \in \mathcal{B}} w^{\nround}_\nitem(b) + \eta^{-1}\Nitem \log |\mathcal{B}| = \eta \Nround \Nitem |\mathcal{B}| + \eta^{-1}\Nitem \log |\mathcal{B}|\,,
    \end{align*}
    where we used unbiasedness of $\widehat{\bm{w}}^{\nround}$. Setting $\eta = \sqrt{\frac{\log |\mathcal{B}|}{|\mathcal{B}|T}}$, we obtain $\textsc{Regret}_\mathcal{B}(\Nround) \leq \Nitem \sqrt{|\mathcal{B}| \Nround \log |\mathcal{B}|}$. 

    
    \textbf{Part 2: Determining Policy $\bm{\pi}$ from Node Probability Measures $\bm{q}$.} 
    Notice that in our regret analysis for both the bandit and full information setting, we do not require explicit knowledge of the policy $\bm{\pi}^t$, so long as it generates the desired node occupancy measure $\bm{q}^t$. In particular, we require a method of converting $\bm{q}^\nround$ to policy $\bm{\pi}^\nround$ which, in turn, is required in order to sample $\bm{b}^\nround$. Recall from Lemma~\ref{lem: QSpace Equivalence} that the mapping from the space of policies $\Pi$ to the space of node weight measures $\mathcal{Q}_\Pi = \mathcal{Q}$ is injective. Thus, for any $\bm{q} \in \mathcal{Q}$, there must exists a $\bm{\pi} \in \Pi$ such that $q(\bm{\pi}) = \bm{q}$. Moreover, the set $\Pi(\bm{q})$ of such $\bm{\pi}$ can be written as the intersection of two polyhedrons, and hence a polyhedron, from which a feasible solution can be computed efficiently (e.g., ellipsoid method), where  $ \Pi(\bm{q})$ is the set of policies $\pi \in [0,1]^{M\times |\mathcal B|\times |\mathcal B|}$ such that 
    \begin{itemize}
        \item $\pi((0, \max \mathcal{B}), b) = q_1(b)$, for any $b \in \mathcal{B}$;
        \item $\pi((0, b), b') = \textbf{1}_{b = b'}$ for any $b, b' < \max \mathcal{B}$;
        \item $q_{m+1}(b') = \sum_{b \in \mathcal{B}} q_m(b) \pi((m, b), b')\}$ for any $b'\in \mathcal B$ and $m \in [M-1]$.
    \end{itemize}



    \textbf{Part 3: Complexity analysis.} One may wonder how to efficiently update the state occupancy measures by computing the minimizer of $\eta\langle \bm{q}, -\widehat{\bm{w}}^\nround\rangle + D(\bm{q} || \bm{q}^{\nround-1})$. The idea is to first solve the unconstrained entropy regularized minimizer with $\tilde{\bm{q}}^{ \nround+1} = \bm{q}^{\nround} \exp(\eta \widehat{\bm{w}}^{\nround})$. We then project this unconstrained minimizer to $\mathcal{Q}$ with:
    \begin{align}
        \bm{q}^{\nround + 1} = \text{argmin}_{\bm{q} \in \mathcal{Q}} D(\bm{q}||\tilde{\bm{q}}^{\nround + 1})\,.
    \end{align}
    Relegating the details to \cite{OREPS2013}, the above constrained optimization problem can be solved as the minimizer of an equivalent unconstrained convex optimization problem with a polynomial (in $\Nitem$ and $|\mathcal{B}|$) number of variables, and therefore, can be computed efficiently. Combining with finding an initial feasible solution to $\Pi(\bm{q})$ as well as the optimization step, we achieve polynomial in $\Nitem, |\mathcal{B}|, \Nround$ total time complexity. For the space complexity, we only need store the values of $\bm{\pi}^\nround$, $\bm{q}^\nround$, and $\widehat{\bm{w}}^\nround$, for a total space complexity of $O(\Nitem |\mathcal{B}|^2)$.   
    
    \textbf{Part 4: Continuous Regret.} To obtain the continuous regret, recall that the discretization error is $O(\frac{\Nitem \Nround}{|\mathcal{B}|})$. As the discretized regret is $O\left(\Nitem \sqrt{|\mathcal{B}| \Nround \log |\mathcal{B}|}\right)$ in the bandit feedback setting, the optimal choice of $|\mathcal{B}|$ is $\Theta(\Nround^{\frac{1}{3}})$, which achieves continuous regret $\textsc{Regret} = O(\Nitem \Nround^{\frac{2}{3}} \sqrt{\log \Nround})$.

\end{proof}

\subsubsection{Proof of Corollary \ref{cor}}
\label{sec: Proof of cor}

We can straightforwardly extend Algorithm~\ref{alg: OMD} to the full information setting. To do this, we note that we can improve Equation~\eqref{eq: full info difference appendix} by instead replacing $\widehat{\bm{w}}^{\nround}$ with $\bm{w}^{\nround}$ in Equation~\eqref{eq: node diff} to obtain:
\begin{align*}
    \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}_\nitem(b) \widehat{w}^{\nround}_\nitem(b)^2 = \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}_\nitem(b) w^{\nround}_\nitem(b)^2 \leq \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem \sum_{b \in \mathcal{B}} q^{\nround}_\nitem(b) = \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem 1 = \Nround \Nitem\,.
\end{align*}
Setting $\eta = \sqrt{\frac{ \log |\mathcal{B}|}{T}}$, we obtain in the full information setting $\textsc{Regret}_\mathcal{B} = O(\Nitem \sqrt{\Nround \log |\mathcal{B}|})$. We can also compute the optimal choice of $|\mathcal{B}|$ to obtain optimal continuous regret. Using the optimal choice of $|\mathcal{B}|$ being $\Theta(\sqrt{T})$, we achieve continuous regret of $\textsc{Regret} = O(\Nitem \sqrt{\Nround \log \Nround})$. Note that due to the complexity of the optimization sub-routine in the projection step of OMD, for the full information setting, it is preferable to use Algorithm~\ref{alg: Decoupled Exponential Weights} instead.









{\color{black}
\subsection{Proof of Theorem \ref{thm: time varying known finite}}
We now prove the regret bounds of the contextualized version of our decoupled hedge algorithm (Algorithm~\ref{alg: Decoupled Exponential Weights - Time Varying Known Finite}) to handle time-varying valuations.
To begin, we can once again `decouple' the utility per unit-bid pair, but this time conditional on the valuation vector context. In particular, we have:
\begin{align*}
    \mu_n^t(\bm{b}; \bm{v}) =  \sum_{m=1}^M w_m^t(b_m; \bm{v}) = \sum_{m=1}^M (v_m - b_m)1_{b_m \geq b^t_{-m}} \quad \text{and} \quad \widehat{\mu}_n^t(\bm{b}; \bm{v}) =  \sum_{m=1}^M \widehat{w}_m^t(b_m; \bm{v})\,.
\end{align*}
As stated earlier, we define reward estimates based on Equation (6) of \cite{ContextBanditsCrossLearning2019} and our Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels}: 
\begin{align*}
    \widehat{w}_m^t(b; \bm{v}) = 1 - \frac{1 - w_m^t(b; \bm{v})}{\sum_{\bm{v} \in \mathcal{V}} \prob(\bm{v}^t = \bm{v}) q_m^t(b; \bm{v})} \textbf{1}_{b_m^t = b} = 1 - \frac{1 - w_m^t(b; \bm{v})}{Q_m^t(b)} \textbf{1}_{b_m^t = b}\,.
\end{align*}
Here, $q_m^t(b; \bm{v}) = \prob(b^t_m = b | \bm{v}^t = \bm{v}) = \sum_{\bm{b}: b^t_m = b} \prob(\bm{b}^t = \bm{b} | \bm{v}^t = \bm{v})$ is the probability of selecting bid $b$ in slot $m$ with valuation $\bm v$. Similarly, $Q_m^t(b)$ is the probability of selecting bid $b$ for unit $m$, averaged across all possible valuations. One can verify unbiasedness of this estimator $\mathbb{E}[\hat{w}_m^t(b; \bm{v})] = w_m^t(b; \bm{v})$ for all $m \in [M], b \in \mathcal{B}, \bm{v} \in \mathcal{V}$. The second moment can similarly be computed as:
\begin{align*}
    \mathbb{E}[\widehat{w}_m^t(b; \bm{v})^2] = \mathbb{E}\left[\left( 1 - \frac{1-w_m^t(b; \bm{v})}{Q_m^t(b)} \textbf{1}_{b_m^t = b} \right)^2 \right] = 1 - 2\mathbb{E}\left[\frac{1-w_m^t(b; \bm{v})}{Q_m^t(b)}\textbf{1}_{b_m^t=b}\right] + \mathbb{E}\left[\left(\frac{1-w_m^t(b; \bm{v})}{Q_m^t(b)}\right)^2\textbf{1}_{b_m^t=b}\right]\,.
\end{align*}

Evaluating the expectations and recalling that $\mathbb{E}[\textbf{1}_{b_m^t = b}] = Q_m^t(b)$, we have:
\begin{align*}
    \mathbb{E}[\widehat{w}_m^t(b; \bm{v})^2] = 1 - \left[2 - 2w_m^t(b; \bm{v})\right] + \left[\frac{(1 - w_m^t(b; \bm{v}))^2}{Q_m^t(b)}\right] = 2w_m^t(b; \bm{v}) - 1 + \frac{1}{Q_m^t(b)} \leq 1 + \frac{1}{Q_m^t(b)} \leq \frac{2}{Q_m^t(b)}\,.
\end{align*}

Using this, the proof largely follows that of Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels} up until Equation (\ref{eq: full info difference}). In particular, we have that the contextual regret can be written as:
\begin{align*}
    \textsc{Regret}_\mathcal{B}(F_{\bm{v}}) &= \mathbb{E}_{F_{\bm{v}}}\left[\sum_{t=1}^{T} \mu_n^t(\bm{b}'; \bm{v}^t) - \sum_{t=1}^{T} \mathbb{E}[\mu^t(\bm{b}^\nround; \bm{v}^t)]\right]\\
    &\lesssim \eta^{-1}M\log|\mathcal{B}| + \eta \mathbb{E}_{F_{\bm{v}}}\left[\sum_{\nround=1}^\Nround \sum_{\bm{b}} \prob(\bm{b}^\nround =\bm{b}| \bm{v}^t = \bm{v}) \mathbb{E}[(\sum_{m=1}^M \widehat{w}^\nround_m(b_m; \bm{v}))^2]\right]\\
    &= \eta^{-1}M\log|\mathcal{B}| + \eta \left[\sum_{\nround=1}^\Nround \sum_{\bm{v} \in \mathcal{V}} \prob(\bm{v}^t = \bm{v})\sum_{\bm{b}} \prob(\bm{b}^\nround =\bm{b}| \bm{v}^t =  \bm{v}) \mathbb{E}[(\sum_{m=1}^M \widehat{w}^\nround_m(b_m; \bm{v}))^2]\right]\\
    &= \eta^{-1}M\log|\mathcal{B}| + \eta M\left[\sum_{\nround=1}^\Nround \sum_{m=1}^M \sum_{\bm{v} \in \mathcal{V}} \prob(\bm{v}^t = \bm{v})\sum_{b \in \mathcal{B}} \mathbb{E}[\widehat{w}_m^t(b; \bm{v})^2]  \sum_{\bm{b}: b_m = b} \prob(\bm{b}^\nround =\bm{b}| \bm{v}^t = \bm{v})\right]\\
    &= \eta^{-1}M\log|\mathcal{B}| + \eta M\left[\sum_{\nround=1}^\Nround \sum_{m=1}^M \sum_{\bm{v} \in \mathcal{V}} \prob(\bm{v}^t = \bm{v})\sum_{b \in \mathcal{B}} \mathbb{E}[\widehat{w}_m^t(b; \bm{v})^2]  q_m^t(b; \bm{v})\right]\\
    &= \eta^{-1}M\log|\mathcal{B}| + 2\eta M\left[\sum_{\nround=1}^\Nround \sum_{m=1}^M \sum_{\bm{v} \in \mathcal{V}} \prob(\bm{v}^t = \bm{v})\sum_{b \in \mathcal{B}} \frac{1}{Q_m^t(b)}  q_m^t(b; \bm{v})\right]\\
    &= \eta^{-1}M\log|\mathcal{B}| + 2\eta M\left[\sum_{\nround=1}^\Nround \sum_{m=1}^M \sum_{b \in \mathcal{B}} \frac{1}{Q_m^t(b)} \sum_{\bm{v} \in \mathcal{V}} \prob(\bm{v}^t = \bm{v}) q_m^t(b; \bm{v})\right]\\
    &= \eta^{-1}M\log|\mathcal{B}| + 2\eta M\left[\sum_{\nround=1}^\Nround \sum_{m=1}^M \sum_{b \in \mathcal{B}} \frac{1}{Q_m^t(b)} Q_m^t(b)\right]\\
    &\leq \eta^{-1}M\log|\mathcal{B}| + \eta M^2|\mathcal{B}|T\,.
\end{align*}
(We will show the first inequality shortly.)
With $\eta = \Theta(\sqrt{\frac{\log |\mathcal{B}|}{M|\mathcal{B}|T}})$,  this yields  the  discretized contextual regret upper bounds of $O(M^{\frac{3}{2}}\sqrt{|\mathcal{B}| T \log |\mathcal{B}|})$ under the bandit setting. Accounting for the rounding error of order $O(\frac{MT}{|\mathcal{B}|})$, we obtain the stated continuous contextual regret upper bounds by optimizing with $|\mathcal{B}| = M^{-\frac{1}{3}}T^{\frac{1}{3}}$. 
To obtain the full information results, we simply replace $\widehat{w}_m^t(b_m; {\bm v}^t)$ with $w_m^t(b_m; {\bm v}^t)$ in the second line of the above equations, which leads to  the discretized contextual regret upper bounds of $O(M^{\frac{3}{2}}\sqrt{ T \log |\mathcal{B}|})$, as desired.


Next, following the proof of Algorithm \ref{alg: Decoupled Exponential Weights - Path Kernels}, we show   the first inequality. 
We define the potentials with respect to a fixed valuation vector $\bm{v}$: $\Phi^t(\bm{v}) = \sum_{\bm{b} \in \mathcal{B}^{+\Nitem}} \exp(\eta \sum_{\tau=1}^{t} \widehat{\mu}^\tau(\bm{b}; \bm{v}^\tau))$. Taking the ratio of adjacent terms, we obtain:
\begin{align*}
    \frac{\Phi^t(\bm{v})}{\Phi^{t-1}(\bm{v})} = \sum_{\bm{b} \in \mathcal{B}^{+M}} \frac{\exp(\eta \sum_{\tau=1}^{t-1} \widehat{\mu}^\tau(\bm{b}; \bm{v}^\tau))}{\Phi^{t-1}(\bm{v})} \exp(\eta \widehat{\mu}^t(\bm{b}; \bm{v}^t)) = \sum_{\bm{b} \in \mathcal{B}^{+M}} \prob(\bm{b}^\nround = \bm{b} | \bm{v}^\nround = \bm{v}) \exp(\eta \widehat{\mu}^t(\bm{b}; \bm{v}^\nround))\,,
\end{align*}
Where in the last equality, we used the condition that our algorithm samples bid vector $\bm{b}$ with probability proportional to $\exp(\eta \sum_{\tau=1}^{t-1} \widehat{\mu}^\nround(\bm{b}; \bm{v}))$ at round $\nround$ with valuations $\bm{v}^t = \bm{v}$. Combining this with inequalities $\exp(x) \leq 1 + x + x^2$ and $1 + x \leq \exp(x)$ for all $x \leq 1$, we obtain:
\begin{align*}
    \frac{\Phi^t(\bm{v})}{\Phi^{t-1}(\bm{v})} \leq \sum_{\bm{b} \in \mathcal{B}^{+M}} \prob(\bm{b}^\nround = \bm{b} | \bm{v}^\nround = \bm{v}) \exp(\eta \widehat{\mu}^t(\bm{b}; \bm{v})) \leq \exp(\sum_{\bm{b} \in \mathcal{B}^{+M}} \prob(\bm{b}^\nround = \bm{b} | \bm{v}^\nround = \bm{v}) \left[\eta\widehat{\mu}^t(\bm{b}; \bm{v}) + \eta^2 \widehat{\mu}^t(\bm{b}; \bm{v})^2 \right])\,.
\end{align*}
Combining this with Equations~\eqref{eq: Potentials} and the fact that $\Phi^0(\bm{v}) = M\log |\mathcal{B}|$, for any fixed bid vector $\bm{b}'$, we have:
\begin{align*}
    \sum_{t=1}^{T} \widehat{\mu}^t(\bm{b}'; \bm{v}) - \sum_{t=1}^{T} \sum_{\bm{b}} \prob(\bm{b}^\nround = \bm{b}| \bm{v}^t = \bm{v})  \widehat{\mu}^t(\bm{b}; \bm{v}) &\lesssim \eta^{-1} \Nitem \log |\mathcal{B}| + \eta \sum_{t=1}^T \sum_{\bm{b}}\prob(\bm{b}^\nround = \bm{b} | \bm{v}^\nround = \bm{v}) \widehat{\mu}^t(\bm{b}; \bm{v})^2\\
    &= \eta^{-1} \Nitem \log |\mathcal{B}| + \eta \sum_{t=1}^T \sum_{\bm{b}}\prob(\bm{b}^\nround = \bm{b} | \bm{v}^\nround = \bm{v}) (\sum_{m=1}^M \widehat{w}_m^t(b_m; \bm{v}))^2\,.
\end{align*}
Taking expectations over $\bm{b}$ and the supremum over all $\bm{b}'$ yields the desired first crucial regret inequality.

As for the time and space complexity, notice that the only algorithmic difference between Algorithm~\ref{alg: Decoupled Exponential Weights - Time Varying Known Finite} and Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels} is precisely in computing the estimator, which in the former, requires having to compute the weights $Q_m^t(b)$ by iterating over all $\bm{v} \in \mathcal{V}$. As we also have to store reward estimates for each possible valuations, both the time complexity and space complexity of Algorithm~\ref{alg: Decoupled Exponential Weights - Time Varying Known Finite} are a factor $|\mathcal{V}|$ larger than in Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels}, which are $O(M|\mathcal{B}| |\mathcal{V}| T)$ and $O(M|\mathcal{B}| |\mathcal{V}|)$ respectively.


}



\section{Online Appendix - Additional Discussion and Experiments}

In this section, we run several additional experiments and include further discussion of our previous results as well as these new experiments. First, we discuss the interaction between the underlying parameters $N, M, |\mathcal{B}|$ and the $c = c_{-n}$ condition required to guarantee existence of a PNE in Theorem~\ref{thm: PNE existence}. We then justify our use of the $\textsc{EXP3-IX}$ weight estimator rather than the unbiased estimator in the implementation of our algorithms in Section~\ref{sec: experiments}. We also discuss a method that achieves uniform exploration per item as per the $\textsc{EXP3.P}$ algorithm described in \cite{Lattimore2020}. Thirdly, we include several experiments omitted in the main body regarding faster convergence with a larger, more competitive market. We then conclude with a discussion of the practicality of the PAB vs. Uniform Price auction formats from the market design perspective.


\subsection{Nash Equilibrium Existence}

\label{sec: Nash equilibrium existence further discussion}

The assumption of competitiveness $c = c_{-n}$ used in Theorem~\ref{thm: PNE existence} can be relaxed. The key idea behind this assumption is that no individual bidder $n$ cannot lower their bids such that their decreased payment offsets their decreased allocation. To see this in effect without requiring the $c = c_{-n}$ condition, consider the following example. 
\begin{enumerate}
    \item Let $N=3, M=3$, and $\mathcal{B} = \{\frac{i}{10}\}_{i \in [10]}$.
    \item Let $v_1 = [1, 0, 0]$, $v_2 = [1, 0.7 - \epsilon, 0], v_3 = [1, 0.4, 0]$ for some small $\epsilon > 0$.
    \item Here, $c = 1$ but $c_{-1} = c_{-3} = 0.7 - \epsilon \neq c_{-2} = 0.4$. Despite $c \neq c_{-n}$, we have that a PNE exists in the form $[0.6, 0, 0], [0.5, 0.5, 0], [0.5, 0.4, 0]$. To verify that this is indeed a PNE, each of the bidders obtains an allocation of 1, with corresponding utilities of 0.4, 0.5, and 0.5 respectively. Considering only uniform winning bids, which contain the set of optimal responses as per Lemma~\ref{lem: near-uniform optimal bidding}, we see the bidders are bidding at Nash:
    \begin{enumerate}
        \item Bidder 1 only demands one item. Here is $b_{2,2} = 0.5$ is bidder 1's $M$'th largest competing bid. Since this belongs to a higher tie-break priority bidder, they must bid strictly higher, yielding a clearing price of 0.6 for bidder 1. Thus, bidder 1 cannot increase or decrease their bids and therefore must be playing their optimal response.
        \item Bidder 2 can win two items at a price of 0.6, which yields utility $0.5-\epsilon < 0.5$. Similarly, in order to win one item, their $M$'th highest competing bid is $b_{3,2} = 0.4$. Since this belongs to a higher tie-break priority bidder, they must bid strictly higher, yielding a clearing price of 0.5. Thus, bidder 2 cannot increase or decrease their bids and therefore must be playing their optimal response.
        \item Bidder 3 can win two items at a price of 0.6, which yields utility $0.2 < 0.5$. Similarly, in order to win one item, their $M$'th highest competing bid is $b_{2,2}=0.5$. Since this belongs to a lower tie-break priority bidder, this yields a clearing price of $0.5$ for bidder 3. Thus, bidder 3 cannot increase or decrease their bids and therefore must be playing their optimal response.
    \end{enumerate}
\end{enumerate}

We have shown that the $c = c_{-n}$ condition is not necessary for PNE existence. Unfortunately, giving a simple characterization of conditions that guarantee PNE existence is non-trivial. However, as mentioned at the beginning of this section, any equilibrium bids $(\bm{b}_1^*,\ldots,\bm{b}_N^*)$ requires that individual bidders cannot i) profitably sacrifice allocation in order to reduce costs and ii) profitably increase their bids to increase allocation. Let $\tilde{b}_{-n,m}$ be as in Lemma~\ref{lem: near-uniform optimal bidding}: $\tilde{b}_{-n,m}$ denotes bidder $n$'s $m$'th smallest competing bid rounded up to the next multiple of $\frac{1}{|\mathcal{B}|}$ if belonging to bidder $n' > n$ due to tie-breaking. In other words, submitting $\tilde{b}_{-n,m}$ for the first $m$ items minimizes the cost required to win $m$ items. As such, we can construct an efficient frontier $\bm{\mu}_n(\bm{b}_{-n}^*) \doteq \{\sum_{i=1}^m v_{n,i} - m\tilde{b}_{-n,m}\}_{m \in [M]}$ of bidder $n$'s utility by minimizing the cost required to win $m$ items. If this frontier is maximized at $\bm{b}_n^*$ for all $n$, then $(\bm{b}_1^*,\ldots,\bm{b}_N^*)$ constitutes a PNE. More formally, a PNE exists at $(\bm{b}_1^*,\ldots,\bm{b}_N^*)$ if:
\begin{align}
    b_{n,m}^* = \min\left(\tilde{b}_{-n,m^*}, \lfloor v_{n,m} \rfloor_{\delta}\right) \quad \text{where} \quad m^* = \text{argmax}_{m \in [M]} \bm{\mu}_n(\bm{b}_{-n}^*) \quad \forall m \in [M], n \in [N]\,.
\end{align}

With this, we can interpret the relaxed condition as saying that bidders prefer to win as many units as possible (subject to positive marginal per-unit utility) over winning fewer units at a reduced cost. This can equivalently interpreted as bidders being price-takers and having little manipulative power over the market price. While even this weaker assumption can be easily violated in artificial examples, e.g., the example in our bid convergence remark, we claim that it is a reasonable assumption in many of the PAB auction's real world applications and markets.
 
\begin{enumerate}
    \item Bidders only have high utility for a small number of units, i.e., the number of units $M$ each bidder $n$ demands is much smaller than the supply $\overline{M}$.
    This is reasonable in many relevant markets, e.g. electricity or emissions, as the total supply far exceeds any single firm's power usage or pollution capacity. 
    \item The total supply is smaller than the aggregate demand, i.e., $\sum_{n \in [N]} \overline{M}_n \gg M$.
    For example, electricity supply in the US in the early 2020's has been strained due to increasing power requirements across many industries, due in part to the rise of energy intensive AI technologies, cloud computing, and cryptocurrency mining. This is also the case in carbon markets, the supply $M$ is artificially limited so as to keep the prices from falling too low and becoming too weak of a disincentive (e.g. carbon markets) to pollute.
\end{enumerate}
In conjunction, these conditions imply that no individual bidder can sufficiently impact prices as their demand is too small compared to the aggregate demand and supply, thus, their reduced costs from underbidding is outweighed by their reduction in allocation. We do note that this result differs from existing characterizations of efficient Pure Nash Equilibria in PAB auctions \cite{inefficiency2013}. However, the Nash equilibria described in the latter require overbidding (bidding above one's marginal valuations) for units, so long as they are guaranteed not to win one of these units. In particular, if every bidder submits the same vector of $[b,\ldots,b]$ where $b$ is the $M$'th largest valuation among all bidders, and ties are broken in favor of bidders with the highest value, then this is also an efficient Pure Strategy Nash equilibrium. 

To illustrate this more quantitatively, we analyze the effects of changing $M, |\mathcal{B}|, N$ on the probability that a PNE (assuming no overbidding) exists in Figure~\ref{fig: PNE existence vs M B N}. Of course as $|\mathcal{B}|$ gets smaller and the number of agents $N$ becomes larger, the probability that the rounded-down $M$'th highest-other-valuation is equal for all $n$ increases. Curiously, the probability is also increasing, albeit slowly, as a function of $M$.

% Figure environment removed



\subsection{$\textsc{EXP3-IX}$ vs. Unbiased Reward Estimator}

\label{sec: IX}

In the experiments section, we ran a slightly modified version of our existing algorithms in the bandit feedback setting. We do this as the variance of the accumulated regret of our algorithms are high, as the node weight estimators normalize over vanishingly small probabilities $q^t_m(b)$. To mitigate the effect of such normalization, we use the $\textsc{EXP3-IX}$ estimator as described in \cite{Lattimore2020}. Under this estimator, rather than normalizing the probability of selecting bid $b^t_m$ for unit $m$ at time $t$ by $q^t_m(b_m^t)$, we instead normalize it by $q^t_m(b_m^t) + \gamma$ for some constant $\gamma > 0$. In the standard $K$-armed bandit setting, despite being a biased estimator, still achieves the same sublinear expected regret guarantee  with a smaller variance. This smaller variance indeed allows for stronger high probability guarantees on the magnitude of our regret; i.e., for $\delta > 0$ and $\gamma = \sqrt{\frac{\log(K) + \log(\frac{K+1}{\delta})}{4K\Nround}}$, the $\textsc{EXP3-IX}$ algorithm guarantees with probability at least $1 - \delta$ that the regret is upper bounded by $C\sqrt{KT\log K}$ for some absolute constant $c > 0$. We extend this algorithm to the multi-unit PAB setting algorithms, where for each node $(m, b)$, we set $\gamma = \sqrt{\frac{\log(K) + \log(\frac{K+1}{\delta})}{4K\Nround}}$ and $K = |\{b \in \mathcal{B}: b \leq v_m\}|$, for $\delta = 0.05$. Aside from the change in node weight estimators, the $\textsc{EXP3-IX}$ versions of Algorithms \ref{alg: Decoupled Exponential Weights - Path Kernels} and \ref{alg: OMD} are exactly the same.\\


\subsubsection{Empirical Performance of Original Algorithms vs. $\textsc{EXP3-IX}$ Variants}

In this section, we empirically analyze the modified variants of our algorithms which use the biased, but lower variance $\textsc{EXP3-IX}$ node-weight estimators (see Appendix~\ref{sec: IX}). We compare the distribution of the regret recovered by these modified algorithms versus the non-modified versions when the number of units is one. The bidder, endowed with valuation vector $\bm{v} = [1]$, will compete against a single adversary over the course of $T$ rounds for $\overline{M}=M=1$ item. This is the standard first price auction (FPA). Here, we compare performance when the adversary is stochastic (bids drawn uniformly random from $[0, 1]$) versus adaptive adversary (running the same algorithm, with a valuation drawn uniformly random from $[0, 1]$). 

We plot the regret of the bidder against the stochastic and adversarial competitors for moderate $T \in \{100, 500, 2000, 10000\}$. The stochastic adversary setting is shown in Figure~\ref{fig:eval_IX} (a) and the adversarial setting is shown in Figure~\ref{fig:eval_IX} (b). 
We observe that while the $\textsc{EXP3-IX}$ variants marginally worsens regret for small values of $T \in \{100, 500\}$ for both the stochastic and adaptive settings, it significantly mitigates the heavy tailed distribution of regret for large $T \in \{2000, 10000\}$, especially in the adversarial setting.



% Figure environment removed

\subsubsection{$\textsc{EXP3.P}$ vs. Unbiased Reward Estimator}

\label{sec: EXP3P}

One downside of the $\textsc{EXP3-IX}$ node weight estimator approach is that the added exploration aggregates over layers $m \in [M]$ in an uneven manner, as bid vectors must stay monotonic. As such, this per-node re-weighting does not guarantee convergence of the regret distribution in probability and only maintains the weaker guarantee over expectation convergence. To mitigate the effect of such uneven exploration aggregation, we instead modify the $\textsc{EXP3.P}$ algorithm as described in \cite{ Lattimore2020}. This algorithm explicitly mixes in uniform noise into the decisions made by the algorithm, and then normalizes accordingly. How does one sample uniformly from the exponentially large space of all monotone, individually rational bid vectors? We claim that the following procedure straightforwardly achieves such uniform mixing:
\begin{enumerate}
    \item Select $b_1$ uniformly at random from $\mathcal{B}$ subject to $b_1 < v_1$.
    \item If $b_m < v_{m+1}$, then set $b_{m+1} = b_m$. Otherwise, select $b_{m+1}$ uniformly at random from $\mathcal{B}$ subject to $b_{m+1} < v_{m+1}$.
\end{enumerate}
With a random exploration probability of $\gamma \in (0, 1)$, the $\textsc{EXP3.P}$ variant of our algorithm performs the above uniform exploration with probability $\gamma$ and follows the procedure in Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels} otherwise. Now, we must also account for this in our node weight estimators. In particular, under the $\textsc{EXP3.P}$ variant, the probability of selecting bid $b^t_m$ for unit $m$ is given by $\hat{q}^t_m(b_m^t) = (1-\gamma)q^t_m(b_m^t) + \frac{\gamma}{|b \in \mathcal{B}: b < v_{m}|} = (1-\gamma)q^t_m(b_m^t) + \frac{\gamma \delta}{\lfloor v_{m}\rfloor_\delta}$. We then update the rewards of all bids $b < v_m$ in layer $m$ as:
\begin{align*}
    \widehat{W}^{\nround+1}_{\nitem}(b) \gets \widehat{W}^{\nround}_{\nitem}(b) + \frac{(v_m - b)\textbf{1}_{b \geq b^t_{-m}} + \beta_m}{\hat{q}^t_m(b)} \textbf{1}_{b^t_m = b})
\end{align*}
where $\beta_m = \Theta(\sqrt{\frac{\log(|\mathcal{B}| T/\delta)}{|\mathcal{B}|T}})$ for some high probability parameter $\delta \in (0, 1)$. In the standard $K$-armed bandits problem, the $\textsc{EXP3.P}$ algorithm guarantees that the regret is bounded above by $C\sqrt{KT\log(K/\delta)}$ with probability at least $1-\delta$ for some universal constant $C$. This high probability bound follows immediately from bounding the variance of the weight estimators. Of course, we cannot blindly apply this bound in our multi-unit setting, as there are an exponentially large number of possible bid vectors. Fortunately, we may apply the same utility decoupling trick as in the analyses of Algorithms~\ref{alg: Decoupled Exponential Weights - Path Kernels} and ~\ref{alg: OMD}. That is, we upper bound the variance of the reward estimate of a bid vector by the sum of the variances of each of its constituent bids, yielding a $1-\delta$ high probability bound of $CM^{\frac{3}{2}}\sqrt{|\mathcal{B}|T\log(|\mathcal{B}|/\delta)}$ in the bandit setting.


\subsection{Experiments with Larger $N$}

In this section, we empirically show the faster convergence of and superior welfare and revenue of more competitive PAB markets. More specifically, we evaluate the impact of competition by running the same revenue and welfare over time experiments for both the PAB and uniform price auctions with varying values of $N$. In Figure \ref{fig:comp}, we compare the distribution of welfare and revenue over time showing the 10th, 25th, 50th, 75th, and 90th percentiles for the bandit setting for a varying number of market participants $N \in \{2, 6, 12, 48\}$ with $T = 10^4$. Compared with the previous revenue and welfare plots with $N = 3$ bidders (Figure~\ref{fig:rev_wel_over_time}), we see that as $N$ grows larger, the welfare and revenue more smoothly, and with lower variance, increase towards their equilibrium values. In addition to the increased competition reducing the incentive for agents to strategically shade their bids, the final revenue is higher as expected.

% Figure environment removed


\subsection{Concluding Remarks}

In this work, we have shown that pay-as-bid (PAB) auctions have several advantages over its more commonly used uniform price counterpart. These advantages include simpler Nash bidding structure requiring only uniform bids (see Lemma~\ref{lem: PNE uniform bidding} and \ref{lem: near-uniform optimal bidding}) where this structure empirically extends to the dynamic setting. Additionally, we show that our utility decoupling insight enables online learning algorithms for PAB that are computationally and regret dominant compared to learning in uniform price auctions. Lastly, we showed that regardless of the parameterizations of $M, N, |\mathcal{B}|$, the PAB auction routinely out-performed uniform price in terms of revenue whilst remaining competitive in terms of welfare (see Table~\ref{table: learning dynamics full info} and ~\ref{table: learning dynamics full uniform}). This superior revenue and welfare trade-off can be seen more closely in Figure~\ref{fig: welfare_revenue_comparison_box_plot_intro}.

% Figure environment removed

Despite these computational and economic advantages of PAB over uniform price, we must still be careful in practice. For example, as PAB achieves higher revenue for the auctioneer, this conversely implies lower consumer surplus. Thus, switching from a uniform price auction to PAB may drive away participants from the platform and join a competing platform. While this is a non-issue for markets where the auctioneer holds a monopoly over a product---e.g., government issued pollution licenses or treasury bills--- this makes switching impractical in certain settings, such as electricity markets. As such, we leave the analysis of learning dynamics in such markets operating under uniform price or markets with multiple platforms for important future work.