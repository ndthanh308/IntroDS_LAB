\section{Preliminaries}\label{sec:model}

\textbf{Notation.} We let $|\mathcal{S}|$ denote the size of set $\mathcal{S}$, and define $[k] = \{1,\ldots,k\}$ to be the set of the first $k$ positive integers. We define $\mathcal{S}^{+k}$ to be the set of non-increasing $k$-vectors of elements from set $\mathcal{S}$. Similarly, $\mathcal{S}^{-k}$ denotes the set of non-decreasing $k$-vectors of elements from set $\mathcal{S}$. We let $\Delta(\mathcal{S})$ denote the set of valid probability measures over set $\mathcal{S}$. We also say that the quantity $x$ is $\lesssim$, $\propto$, or $\gtrsim$ than $f(a_1,\ldots,a_k)$ some function of $k$ algorithm parameters if $x \in \mathcal{O}(f(a_1,\ldots,a_k))$, $x \in \Theta(f(a_1,\ldots,a_k))$, or $x \in \Omega(f(a_1,\ldots,a_k))$,  respectively. We use $\tilde{O}$, $\tilde{\Theta}$, and $\tilde{\Omega}$ to hide logarithmic factors.
Lastly, given a set of elements $a_1,\ldots,a_k$, we define $a_{s:r} = (a_s,\ldots,a_r)$ for $r \leq k$ to be the set of elements from $s$ to $r$ inclusive. 


\textbf{Auction format: Pay-as-bid.} 
 Consider a scenario in which there are $N$ bidders and $\overline{M}$ identical units available for auction in a PAB  format. In this context, we assume that each agent $n$ within the set $[N]$ desires a maximum of $M$ units. (It should be noted that our results can be extended to a situation where each agent $n$ may demand a different maximum number of units, denoted by $M_n$, which are not necessarily identical.)
 
 Let $\bm{v}_n = (v_{n, \nitem})_{\nitem \in [M]} \in [0, 1]^{+M}$ represent agent $n$'s non-increasing marginal valuation profile. This implies that for any given $n$ in the set $[N]$, the following conditions hold: (i) {valuation monotonicity} $v_{n,1}\ge v_{n, 2}\ge \ldots \ge v_{n, \Nitem}$ and (ii) the total valuation of agent $n$ after receiving $\nitem$ units is given by $\sum_{k=1}^{m}v_{n, k}$.

 Let $\bm{b}_n = \{b_{n, \nitem}\}_{\nitem \in [\Nitem]} \in [0, 1]^{+\Nitem}$  represent the non-increasing bids submitted by bidder $n$. Here, $b_{n, \nitem}$ refers to the bid made by bidder $n$ for the $\nitem$-th slot/unit. Similar to $\bm{v}_n$, we have the following conditions for any $n \in [N]$: (i) {bid monotonicity}
$b_{n,1}\ge b_{n, 2}\ge \ldots \ge b_{n, \Nitem}$ and  (ii) individual rationality (IR) $b_{n, \nitem} \leq v_{n, \nitem}$ for all $m \in [M]$. 
It is important to note that the bid monotonicity condition is not an assumptions; it is implied by the auction rule that will be stated shortly. Consequently, the total payment made by bidder $n$ after receiving $\nitem$ units is given by $\sum_{k=1}^{m} b_{n, k}$. We define $\bm{b}_{-n} = (b_{-n,\nitem})_{\nitem \in [\overline{\Nitem}]} \in [0, 1]^{-\overline{M}}$  as the set of the $\overline{\Nitem}$ largest bids not belonging to agent $n$, arranged in increasing order (i.e., $b_{-n, 1} \leq \ldots \leq b_{-n, \overline{\Nitem}}$).



 The auction operates according to the following rules:
 In a PAB auction, all bids submitted across the $N$ bidders are arranged in descending order. The $\nitem$-th unit is assigned to the bidder with the $\nitem$-th highest bid, and they are charged the amount of their bid. We denote the allocation to agent $n$ as $x_n(\bm{b}_n) = x(\bm{b}_{n}, \bm{b}_{-n})$, and the (quasi-linear) utility as $\mu_n(\bm{b}_n) = \mu(\bm{b}_{n}, \bm{b}_{-n})$, where
\begin{align}
    \label{eq: allocation and utility definition}
    x(\bm{b}_{n}, \bm{b}_{-n}) = \sum_{\nitem=1}^\Nitem \textbf{1}_{b_{n, \nitem} \geq b_{-n, \nitem}} \quad \text{and} \quad \mu(\bm{b}_{n}, \bm{b}_{-n}) = \sum_{\nitem=1}^{x_n(\bm{b}_n)} (v_{n, \nitem} - b_{n, \nitem})\,.
\end{align}
respectively. Here, $b_{-n, \nitem}$ represents the $\nitem$-th smallest bid among the $\overline{M}$ largest bids of all other bidders except bidder $n$.
It should be noted that $x(\bm{b}_{n}, \bm{b}_{-n})$ denotes the number of units that agent $n$ receives in the auction. We assume that ties are broken in favor of higher indexed bidders and that this is public knowledge.

{\color{black}In the following sections, we study PAB in two settings. We first consider the one-shot setting, wherein we characterize properties of PNEs, CCEs, and CEs. Then, we define and discuss the repeated setting and derive corresponding no-regret learning algorithms.}

{\color{black}\section{Equilibria of Pay-as-Bid Auctions}
\label{sec: equilibria}

We provide partial results regarding the properties and structure of common solution concepts such as PNE and CCE. More specifically, we show near-uniformity of the winning bids and each bidder's submitted bids under PNEs and we show these properties do not extend to CCEs, or even CEs. We first define these equilibria  formally:


\begin{definition}[Pure Nash Equilibria]
    In the PAB setting, a Pure Nash Equilibrium (PNE) is defined as a collection of $N$ bid vectors $\bm{b}_n \in \mathcal{B}^{+M}$ for all $n \in [N]$ that satisfies
    \begin{align*}
        \mu_i(\bm{b}_i, \bm{b}_{-i}) \geq \mu_i(\bm{b}'_i, \bm{b}_{-i})\,.
    \end{align*}
\end{definition}


PNE can be interpreted as a collection of \textit{independent} strategies under which no individual has an incentive to deviate. While PNEs are the most commonly studied and desirable solution concept in much of the economics and computer science literature, they are often intractable computationally and, in the case of PNE, may not necessarily even exist \citep{PredictionLearningGames}. As such, we will study a broader class of solution concepts known as CCEs, which contains the set of PNEs. These solution concepts generalize PNEs by allowing for dependencies between bidders' strategies. To better understand this, we first formally define CCEs, which are well known to comprise the limit cycles of no-regret learning algorithms, and CEs, which are a class of solutions between PNEs and CCEs that comprise the time-averaged joint bid distributions limit cycles of no-swap-regret learning \citep{blum2007externalinternalregret}.

\begin{definition}[Coarse correlated equilibria and correlated equilibria]
    In the PAB setting, a coarse correlated equilibrium (CCE) is defined as a joint probability distribution $p: \bigotimes_{n=1}^N \mathcal{B}^{+M} \to [0, 1]$ over all $N$ agents' bid vectors $\bm{b}_{1:N}$ that satisfies the following set of linear constraints for all $n \in [N]$:
    \begin{align*}
       \mathbb{E}_{\bm{b}_{1:N}\sim p}[  \mu_n(\bm{b}_n; \bm{b}_{-n})] \geq \mathbb{E}_{\bm{b}_{1:N}\sim p}  [\mu_n(\bm{b}'_n; \bm{b}_{-n})] \quad \text{ for any } \bm{b}'_n \in \mathcal{B}^{+M}\,.
    \end{align*}
    A correlated equilibrium (CE) is similarly defined as a joint probability distribution $p: \bigotimes_{n=1}^N \mathcal{B}^{+M} \to [0, 1]$ over all $N$ agents' bid vectors $\bm{b}_{1:N}$ that satisfies the more stringent set of linear constraints for all $n \in [N]$:
   \begin{align*}
        \mathbb{E}_{\bm{b}_{1:N}\sim p}[  \mu_n(\bm{b}_n; \bm{b}_{-n}) | \bm{b}_n] \geq    \mathbb{E}_{\bm{b}_{1:N}\sim p}[\mu_n(\bm{b}'_n; \bm{b}_{-n})| \bm{b}_n] \quad \text{ for any }  \bm{b}_n, \bm{b}'_n \in \mathcal{B}^{+M}\,.
    \end{align*}
    
\end{definition}

CCEs (resp. CEs) can be interpreted as a joint distribution $p(\cdot)$ over all strategy profiles such that it is ex-ante (resp. ex-post) optimal for bidders to adhere to the strategy prescribed by a ``signal" drawn from this distribution. Beyond the well known fact that the class of CCEs (resp. CEs) comprise the limit cycle of no external (resp. internal) regret learning dynamics in repeated games \cite{PredictionLearningGames}, CCEs and CEs are of major interest they possess strong welfare guarantees in smooth games, such as PAB auctions \citep{syrgkanis2012composable, InefficiencyStandard2013, roughgarden2015smooth, PriceOfAnarchyInAuctions2017, feldman2017correlated}. 
Moreover, the linear constraint representation of CCEs and CEs is particularly useful as one may define a linear program with a corresponding linear objective function in order to test certain hypotheses; as we do in the proof of Lemma \ref{lem: CCE non uniform winning}.
Having formally defined PNEs, CCEs, and CEs, we are ready to state our main results regarding the equilibria of PAB, assuming an even discretization $\mathcal{B} = \{0, \delta,\ldots, 1-\delta, 1\}$.

\subsection{Pure Nash Equilibria}

We begin by showing that PNEs, if they exist, must have nearly uniform winning bids; at most one discretization factor $\delta$ (which is equal to $\frac{1}{|\mathcal{B}|}$ under an even discretization $\mathcal{B}$) apart.

\begin{lemma}
    \label{lem: PNE uniform bidding} Any PNE (if one exists) requires that the highest bid $b_{(1)}$ is at most \(\delta\) above the \(M\)'th largest bid, denoted by \(b_{(M)}\). Here, $\delta$ is the discretization factor.
\end{lemma}

Lemma \ref{lem: PNE uniform bidding}, detailed in Section \ref{sec:proof:lem: PNE uniform bidding}, establishes that any PNE (if existing) necessitates nearly uniform winning bids, differing by at most a discretization factor of $\delta$. Additionally, the subsequent lemma, proved in Section \ref{sec:proof:lem: near-uniform optimal bidding}, illustrates that uniform bidding is optimal for any bidder.

\begin{lemma}
    \label{lem: near-uniform optimal bidding} 
    Consider any fixed bids \(\bm{b}_{-n}\) with tie-breaking favoring higher-indexed bidders. Let $\tilde{b}_{n,m}$ denote the minimum bid required for bidder \(n\) to win the \(m\)'th item under the tie-breaking rule. Specifically, \(\tilde{b}_{n,m} = b_{-n,m} + \delta\) if \(b_{-n,m}\) belongs to a bidder with a higher index than \(n\), and \(\tilde{b}_{n,m} = b_{-n,m} \) otherwise. Assuming no overbidding, bidder \(n\)'s optimal bids takes the following form:  \(b_j= \tilde{b}_{n,m}\) for all \(j\in [m]\), and \(b_j \leq v_j\) for all \(j>m\) for all $m$ such that $\tilde{b}_{n,m} < v_m$, where for convenience, $\tilde{b}_{n,0} = 0$. Consequently, under any PNE, if one exists, bidder \(\bm{b}_n\) should take the aforementioned form. 
\end{lemma}



This result states that under any fixed $\bm{b}_{-n}$, the optimal bid vector for bidder $n$, under which they are allocated $m$ items, is such that they pay the minimum possible price required to win those $m$ items.
We now present our main result of this section, where we provide sufficient conditions to fully characterize an approximately welfare-efficient PNE.

\begin{theorem}[Existence of an Approximately Efficient PNE]
    \label{thm: PNE existence}
     Define the clearing price $c = \lfloor v_{(M)} \rfloor_{\delta}$ to be the $M$'th largest valuation among all bidders, denoted by $v_{(M)}$,  rounded down to the nearest multiple of $\delta$, and similarly, define $c_{-n} = \lfloor v_{(-n, M)} \rfloor_{\delta}$ to be the rounded $M$'th largest valuation among all bidders except $n$. If $c = c_{-n}$ for all $n\in [N]$ and ties are broken in favor of higher indexed bidders, then there exists a PNE $(\bm{b}_1,\ldots,\bm{b}_N)$ where each bidder $n\in [N]$:  
    \begin{enumerate}
        \item submits bids of either all $c$ or all $c + \delta$ for units such that $v_{n,m} \geq c + \delta$,
        \item submit bids of $c$ for all units such that $v_{n,m} \in [c, c + \delta)$,
        \item submit bids smaller than $c$ for all other units.
    \end{enumerate}
    Moreover, this PNE is $M\delta$-approximately welfare optimal:
    \begin{align*}
        \sum_{m=1}^M v_{(m)} - \sum_{n=1}^N \sum_{m=1}^{x(\bm{b}_n, \bm{b}_{-n})} v_{n,m} \leq M\delta\,.
    \end{align*}
\end{theorem}
{\color{black}At a high level, the $c = c_{-n}$ constraint implies that bidders act as price-takers and cannot increase their utility by under-bidding for the units allocated to them under the PNE condition. In this competitive environment, there exists an approximately welfare-optimal PNE where all bidders submit bids close to the clearing price $c$, which represents the $M$'th largest valuation.

For a detailed proof, we refer the reader to the online appendix (arXiv:2307.15193v3). 
In Section \ref{sec:discuss:assumption}, we include discussion of this assumption as well as present an example where this assumption holds, but even a minor alteration leads to its breakdown. As demonstrated in Figure~\ref{fig: bid cycling main body}, this impacts bid convergence in our algorithm, resulting in cyclic bidding behavior for certain bidders. However, it's crucial to note that while the $c = c_{-n}$ condition suffices for existence of PNEs, PNE existence is broader, dependent on bidders acting as price-takers with minimal market influence. This holds true in markets where individual demands are much smaller than the total supply and the total demand exceeds the total supply.}

\subsection{Coarse Correlated and Correlated Equilibria}
Thus far, we have showed some structural results for PNEs in PAB auctions. Unfortunately, this structure does not extend to CCEs, or even the tighter class of CEs. 
\begin{lemma}
    \label{lem: CCE non uniform winning}
    There are CCEs (and CEs) of PAB where winning bids differ by more than one discretization factor, 
$\delta$, with a non-zero probability.
\end{lemma}



Because of this non-winning bid uniformity in CCEs, we cannot hope to recover such an approximate welfare optimality result as we did for PNE. Not all hope is lost, however, as we can still apply a smoothness-based argument to lower bound the expected welfare under any CCE of PAB \cite{syrgkanis2012composable, roughgarden2015smooth}. This result shows that in PAB auctions, the expected welfare under any CCE is at least $\frac{1}{2}(1 - \frac{1}{e})$ of the optimal welfare. Perhaps more importantly, despite the existence of CCEs with non-uniform winning bids, our no-regret learning algorithms consistently converge to CCEs which possess such uniformity (see Table~\ref{table: learning dynamics full info}). We leave such characterization of these CCEs as the result of our learning algorithms as future work.


}


\section{Learning to Bid in a Repeated Setting}
\label{sec:repeated}


In this section, we explore the repeated PAB setting where bidders participate over $T$ rounds, aiming to maximize their aggregate utility or minimize their regret. A limitation of the equilibria analysis from the previous section is its ambiguity on the attainability of these equilibria through natural learning dynamics like fictitious play or no-regret learning. For instance, while Lemmas~\ref{lem: near-uniform optimal bidding} and \ref{lem: PNE uniform bidding} depend on the existence of a PNE, which may not be present, bidders' learning behavior might still converge towards a specific subset of CCEs with certain  structures.\footnote{For example, when bidders behave according to a certain class of no-regret learning algorithms known as \textit{mean-based} algorithms, they may even \textit{last-iterate} converge to Mixed NE or even PNE of certain kinds of auctions \cite{braverman2018noregretbuyer, Deng_2022}.}
We respond to these insights by designing no-regret learning algorithms and empirically examining the dynamics and equilibria  they converge towards.

\subsection{Repeated Setting}

We consider a repeated setting where the PAB auction is conducted over $\Nround$ rounds. In this repeated setting, we will focus on the perspective of agent $n \in [N]$ and remove additional indexing when it is evident from the context. For each auction round, agent $n$ has a fixed valuation profile represented by 
$\bm{v} = (v)_{\nitem \in [\Nitem]} \in [0, 1]^{+\Nitem}$, and their
 bid vector in the $\nround$-th auction is denoted by $\bm{b}^\nround = (b^\nround_\nitem)_{\nitem \in [\Nitem]} \in [0, 1]^{+\Nitem}$.\footnote{In Section \ref{sec: time varying}, we extend our results to the case of time-varying valuations.} 
Similarly, $\bm{b}^\nround_{-} = (b^\nround_{-m})_{m \in [\overline{\Nitem}]} \in [0, 1]^{-\overline{\Nitem}}$ represents the competing bids in round $\nround$. In each round, agent $n$ receives $x^\nround_n(\bm{b}^\nround)$ units and earns a utility of $\mu^\nround_n(\bm{b}^\nround)$, where:
\begin{align}
    \label{eq: allocation and utility definition repeated}
    x^\nround_n(\bm{b}^\nround) = x(\bm{b}^\nround, \bm{b}^\nround_{-}) \quad \text{and} \quad \mu^\nround_n(\bm{b}^\nround) = \mu(\bm{b}^\nround, \bm{b}^\nround_{-})\,.
\end{align}
Recall that functions $x$ and $\mu$ are defined in Equation \eqref{eq: allocation and utility definition}. 
The goal of agent $n$ is to choose a sequence of bid vectors $(\bm{b}^\nround)_{\nround \in [\Nround]}$ that maximizes their total utility, given by $\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)$. However, the main challenge is that the vectors $\bm{b}^{\nround}_{-}$, representing the competing bids, are not known in advance. Instead, they are revealed in an online manner. Consequently, agents must learn how to bid optimally throughout the sequence of auctions, taking into account their previous allocations $H^{\nround-1} = (x^\tau(\bm{b}^\tau))_{\tau \in [\nround-1]}$ and their valuation profile $\bm{v}$. The performance of an agent's learning strategy is evaluated in terms of regret, which quantifies the difference between their expected utility using their learning strategy and the optimal utility achievable with perfect knowledge of the competing  bidding vectors in hindsight:
\begin{align}
\tag{Continuous Regret}
    \textsc{Regret} = \max_{\bm{b} \in [0, 1]^{+\Nitem}} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}) - \mathbb{E}\left[\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)\right]\,.
\end{align}
Here, $\bm{b}^{\nround}_-$ can be selected by an adaptive adversary; i.e. an adversary who can select $\bm{b}^\nround_-$ as a function of the entire auction history, which includes $\bm{b}^1,\ldots,\bm{b}^{\nround-1}$, but does not have access to the possible randomness when selecting $\bm{b}^\nround$.


{\color{black}The benchmark, $\max_{\bm{b} \in [0, 1]^{\Nitem}} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b})$ used in the definition of continuous regret, is constructed considering all possible $\bm{b}^{\nround}_{-}$ for every round $\nround \in [\Nround]$, where in the hindsight optimal solution, the bid vector can be chosen from any vector $\bm{b} \in [0, 1]^{+\Nitem}$. However, as is common in the literature (see, e.g., \citep{LearningBidOptimallyAdversarialFPA2020, OptimalNoRegretFPA2020, ContextBanditsCrossLearning2019}) for learning to bid in first-price auctions (PAB auctions with $M=1$), characterizing the continuous regret as defined above requires discretizing the action space while considering discretization errors and characterizing the discrete regret, which we will define shortly, in order to bound the continuous regret. The level of discretization is then optimized to balance the discrete regret and discretization errors.

In light of this, let us confine ourselves to a discretization of $[0, 1]$, denoted by $\mathcal{B} = (B_1,\ldots,B_{|\mathcal{B}|})$, where $0 = B_1 < \ldots < B_{|\mathcal{B}|} = 1$. In such instances, we define an analogous version of regret:
\begin{align}
\tag{Discretized Regret}
    \textsc{Regret}_\mathcal{B} = \max_{\bm{b} \in \mathcal{B}^{+\Nitem}} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}) - \mathbb{E}\left[\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround)\right]\,.
\end{align}}
In both  definitions of continuous and discretized regret, we henceforth implicitly assume that $b_m\le v_m$ for any $m\in [M]$; that is, we have  the bid vector $\bm{b}$ is subject to individual rationality and overbidding is not allowed. 
{\color{black}We  wish to derive a learning algorithm that achieves a  continuous regret  (i.e., $\textsc{Regret}$) that is polynomial  in $\Nitem$ and sub-linear in $\Nround$. To do so,  as stated above,  we consider the discretized setting, and bound $\textsc{Regret}_\mathcal{B}$; an upper bound on the continuous regret  will be obtained by accounting for the discretization errors, which we show in Section~\ref{sec: path kernels regret} is of order $O(\frac{MT}{|\mathcal{B}|})$.}


\subsection{Offline Setting} \label{sec:offline}


In this section, we provide an algorithm that efficiently computes the hindsight optimal bid vector $\max_{\bm{b} \in \mathcal{B}^{+\Nitem}} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b})$ which is precisely our benchmark for regret. In order to compute agent $n$'s optimal fixed bidding strategy for the $\Nround$ rounds of PAB auctions when fixing all other bids $\bm{b}_{-n}^1,\ldots,\bm{b}_{-n}^T$, we design an algorithm based on path weight optimization. Recall the following optimization problem with bid space $\mathcal{B}$:
\begin{align}
\tag{Offline}
\label{eq:offline}
\max_{\bm{b} \in \mathcal{B}^{+\Nitem}} \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b})\,,
\end{align}
where $\mu^\nround_n(\bm{b})$ (defined in Equation \eqref{eq: allocation and utility definition repeated}) represents the utility of agent $n$ in round $\nround$ given bid vector $\bm{b}$ and competing bids $\bm{b}_-^t$. As mentioned earlier, the solution to this optimization problem serves as a benchmark for evaluating the performance of online learning algorithms in the repeated setting. Furthermore, it provides valuable insights for designing algorithms with polynomial time and space complexity for the repeated setting.

  

To solve  Problem \eqref{eq:offline}, we take advantage of the following decomposition: 
\begin{align}
    \label{eq: decomposition}
    \sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}) &= \sum_{\nround=1}^\Nround \sum_{\nitem=1}^\Nitem (v_\nitem - b_\nitem)\textbf{1}_{b_\nitem \geq b_{-\nitem}^t} = \sum_{\nitem=1}^\Nitem \sum_{\nround=1}^\Nround (v_\nitem - b_\nitem)\textbf{1}_{b_\nitem \geq b_{-\nitem}^t}\\ &:= \sum_{\nitem=1}^\Nitem \sum_{\nround=1}^\Nround w_\nitem^\nround(b_\nitem) := \sum_{\nitem=1}^\Nitem W^{\Nround+1}_\nitem(b_\nitem)\,,
    \label{def: def mu w W}
\end{align}
where 
$w^\nround_\nitem(b) = \textbf{1}_{b \geq b^{\nround}_{-\nitem}} (v_\nitem - b)$ represents the utility in the $\nround$-th auction for winning the $\nitem$-th item with bid $b$, and $W^{\Nround+1}_\nitem(b) = \sum_{\nround=1}^\Nround w^{\nround}_\nitem(b)$ represents the cumulative utility gained from winning the $\nitem$-th item with bid $b$ across the $\Nround$ auctions. \footnote{Here, in $w^\nround_\nitem(b)$, the same tie-breaking rule in Equation \eqref{eq: allocation and utility definition} is applied.} To solve Problem \eqref{eq:offline}, we develop a polynomial-time DP scheme utilizing these $w^\nround_\nitem(b)$ and $W^{\Nround+1}_\nitem(b)$. In particular, for any $\nitem \in [\Nitem]$ and any bid $b \in \mathcal B$, let $U_\nitem(b)$ be the optimal cumulative utility of the agents from units $\nitem, \nitem+1, \ldots, \Nitem$ over $\Nround$ auctions assuming that bids for unit $\nitem$ is less than or equal to $b$. We then have
\begin{align}
    U_\nitem(b) = \max_{b'\le b, b'\in \mathcal B}\left\{ W_{\nitem}^{\Nround+1} (b') + U_{\nitem+1}(b')\right\} \quad b\in \mathcal B, m\in [M] \quad \text{and} \quad U_{\Nitem+1}(b) = 0 \quad b \in \mathcal{B}\,.
\end{align}
Algorithm \ref{alg: Offline Full} uses the aforementioned  DP scheme to devise an optimal solution to Problem \eqref{eq:offline}. The following theorem, proven in Section~\ref{sec: offline proof}, shows the optimality of Algorithm \ref{alg: Offline Full}. 

\begin{theorem}\label{thm:offline}
Algorithm~\ref{alg: Offline Full} returns the optimal solution to Problem \eqref{eq:offline} {with time and space complexity of $O(\Nitem |\mathcal{B}|^2)$ and $O(\Nitem|\mathcal{B}|)$, respectively.}
\end{theorem}






\begin{algorithm}[t]
\footnotesize
	\KwIn{Valuation $\bm{v}$ for $\bm{v} \in [0, 1]^{+\Nitem}$, Other bids $\{\bm{b}^{\nround}_-\}_{\nround \in [\Nround]}$ for $\bm{b}^{\nround}_- \in \mathcal{B}^{-\overline{\Nitem}}$.}
	\KwOut{Optimal bid vector $\bm{b}^* = \text{argmax}_{\bm{b} \in \mathcal{B}^\Nitem} \mu^{ \Nround}_n(\bm{b})$ and its corresponding utility.}
	Let $W^{\Nround+1}_\nitem(b) \gets \sum_{\nround=1}^\Nround \textbf{1}_{b \geq b^{\nround}_{-\nitem}} (v_\nitem - b)$,  $b \in \mathcal{B}, \nitem \in [\Nitem]$,   define 
        $U_{\Nitem+1}(b) \gets 0$, $b \in \mathcal{B}$, and set $b^*_{0} = \max(\mathcal B)$\;
        \textbf{for} $m \in [M,\ldots,1], b \in \mathcal{B}: U_\nitem(b) \gets \text{max}_{b' \in \mathcal{B}; b' \leq b}W^{\Nround+1}_\nitem(b') + U_{\nitem+1}(b')$\;
        \textbf{for} $m \in [1,\ldots,M]: b_m^\star \gets \arg\max_{b \le b_{m-1}^*} U_\nitem(b)$\;
        \textbf{Return} $U_1(\max(\mathcal{B}))$ and $\bm{b}^{*} =(b_{m}^*)_{m\in [M]}$.
	\caption{\textsc{Offline}$(\bm{v}, \{\bm{b}^{\nround}_-\}_{\nround \in [\Nround]})$ \label{alg: Offline Full}}	
\end{algorithm}


This algorithm to solve the offline bid optimization problem enables us to compute the hindsight optimal utility, which serves as a benchmark for evaluating the effectiveness of our online learning algorithms. It is worth mentioning that we can represent our DP algorithm as an equivalent graph with $\Nitem$ layers, with $|\mathcal{B}|$ nodes in each. More precisely, we define the (offline) DP graph as follows:
\begin{enumerate}
    \item \textbf{DP nodes.} There are $\Nitem$ layers, each with    $|\mathcal{B}|$ nodes in each, denoted by $\{(\nitem, b)\}_{\nitem \in [\Nitem], b \in \mathcal{B}}$.
    \item \textbf{DP edges.} In this graph, there are only (directed) edges between two consecutive layers, i.e., from layer $m$ to layer $m+1$ for any $m \in [M-1]$. 
    In particular, 
    node $(\nitem, b)$ only has an edge to node $(\nitem+1, b')$ for $b' \leq b$ and if $b' \leq v_{m+1}, b \leq v_m$. 
    \item \textbf{DP weights.} We define the weight of node $(\nitem, b)$ to be $W_{\nitem}^{T+1}(b) = \sum_{\tau=1}^{\Nround} \textbf{1}_{b \geq b^{\tau}_{-\nitem}} (v_\nitem - b)$.
\end{enumerate}

% Figure environment removed


Now, we extend our DP formulation for solving for the hindsight optimal to the online setting. We consider two feedback structures: (i) full information and (ii) bandit. In the full information setting, the agent's allocation and the values of $\bm{b}^\nround_-$ are revealed after the end of each round, whereas in the bandit setting, only the agent's allocation is revealed. As the bidder can only see information up to round $t$, we can define the DP graph at time $t$, as opposed to $T$, by setting $W_{\nitem}^t(b) = \sum_{\tau=1}^{\nround-1} \textbf{1}_{b \geq b^{\tau}_{-\nitem}} (v_\nitem - b)$. This allows us to construct algorithms for the full information and bandit settings by taking advantage of the structure of the DP graph to enhance efficiency and optimize storage of necessary computations. 


\subsection{Online Setting: Decoupled Exponential Weights}

\label{sec: decoupled exp weights section}

In this section, we present our first algorithm for learning in the online setting. In particular, we construct a decoupled version of the Exponential Weights algorithm which circumvent the large space and time complexity of maintaining and updating the sampling distributions of all possible bid vectors. Our algorithms instead sequentially sample a singular bid value from each layer of our DP graph such that the probability of sampling a particular vector of bids $\bm{b}^\nround$ is precisely equal to the probability of the exponential weights algorithm selecting $\bm{b}^\nround$.\footnote{In Section~\ref{sec: time varying}, we present a generalization of our decoupled exponential weights algorithm to the setting with time varying valuations, where the valuations are drawn from some known, finite support distribution $F_{\bm{v}}$.} 
 
In the following sections, we describe our algorithm for both full information and bandit settings. It's important to note that while our decoupled exponential weights algorithm achieves regret suboptimal by a factor of \(O(\sqrt{M})\), we introduce an alternative regret-optimal algorithm based on Online Mirror Descent (OMD) in a subsequent section. Despite this, the decoupled exponential weights algorithm remains practical, as it avoids the need to solve a convex optimization problem at each time step \(t \in [T]\). 

\subsubsection{Full Information Setting}\label{sec:full}

Now let us focus on learning optimal bidding in an online fashion with full information feedback. One straightforward approach in this context is to apply the exponential weights algorithm \citep{DBLP:journals/iandc/LittlestoneW94} to the entire set of bid vectors. This algorithm guarantees per-round rewards within the range of $[-\Nitem, \Nitem]$. However, the challenge lies in the exponentially large bid space $\mathcal{B}^{+\Nitem}$. Tracking and updating weights for all possible bid vectors naively would lead to a non-polynomial time and space complexity. Although this approach achieves a small regret of $O(\Nitem^\frac{3}{2} \sqrt{T \log |\mathcal{B}|})$, we need a more efficient solution with a polynomial time and space complexity.

To do so, we leverage the DP scheme developed in Section \ref{sec:offline}. By utilizing the DP graph and the information it provides about bid vector utilities, we can effectively mimic the exponential weights algorithm without explicitly tracking weights for every bid vector. 
In Algorithm~\ref{alg: Decoupled Exponential Weights}, instead of 
 associating weights to each  possible bid vector, we  associate weights with each $(\nitem, b)$ pair for any $m\in [M]$ and $b\in \mathcal B$. These weights are then updated via variables $S_m^t(b)$ for $b\in \mathcal B, m\in[M]$, which are inspired by the DP scheme. For any round $t$, we define 
 \begin{align*} S^t_\nitem(b) &= \exp(\eta W_\nitem^\nround(b)) \sum_{\bm{b}'_{\nitem+1:\Nitem} \in \mathcal{B}^{+(\Nitem-\nitem)}, b'_{\nitem+1} \leq b'_\nitem = b} \exp(\eta \sum_{\nitem'=\nitem+1}^\Nitem W_{\nitem'}^\nround(b'_{\nitem'}))\\
 &= \exp(\eta W_\nitem^\nround(b)) \sum_{b' \in \mathcal{B}; b' \leq b} S_{\nitem+1}(b')\,.\end{align*}
 Here, $\eta> 0$ is the learning rate of the algorithm and we  recall that  $W_m^t(b)= \sum_{\tau=1}^{t-1} w^{\tau}_\nitem(b)$ is  cumulative utility gained across the first $t-1$ auctions from the winning the $\nitem$'th item with bid $b$, respectively. Computing $S_m^t(\cdot)$ is done in step $\textsc{Compute}-S_\nitem$ of the algorithm.  In step $\textsc{Sample}-\bm{b}$, the bid vector is then sampled  according to $S_m^t$'s subject to bid monotonicity.  To disallow overbidding, we initialize weights  $W^0_m(b) = -\infty$ for all $m \in [M], b \in \mathcal{B}$ such that $b > v_m$.
 
 

The concept of utility decoupling shares similarities with solutions used in combinatorial bandits, tabular reinforcement learning \citep{CMAB2013, OREPS2013}, and problems such as shortest path algorithms involving weight pushing or path kernels \citep{PathKernel2003, Hedging2010}. These methods are employed to solve variants of the shortest path or maximum weight path problems, where costs or weights are associated with edges rather than nodes. By exploiting the graph structure and the linearity of utilities with respect to the weights of each edge, these algorithms efficiently compute path weights based on edge weights, similar to how our algorithm computes path weights based on node weights. In addition to investigating a fundamentally different problem,   the key distinction is that our approach  considers weights associated with nodes instead of edges. In our setting, the reward associated with selecting bid $b'$ in slot $\nitem+1$ is independent of selecting bid $b \geq b'$ in slot $\nitem$. This allows us to get an improved regret bound and save a factor of $|\mathcal{B}|$ in terms of time and space complexity. Instead of storing and updating weights for $O(\Nitem |\mathcal{B}|^2)$ possible $(\nitem, b, b')$ slot-value-next value triplets, we only need to handle $O(\Nitem |\mathcal{B}|)$ possible $(\nitem, b)$ unit-bid pairs. The following statement is the main result of this section. 

\begin{theorem}[Decoupled Exponential Weights: Full Information] \label{thm:full}
    With $\eta = \Theta(\sqrt{\frac{\log |\mathcal{B}|}{MT}})$, Algorithm \ref{alg: Decoupled Exponential Weights} achieves (discretized) regret $O(\Nitem^\frac{3}{2} \sqrt{ \Nround \log |\mathcal{B}|})$, with total time and space complexity polynomial in $\Nitem$, $|\mathcal{B}|$, and $\Nround$. {\color{black}Optimizing for discretization error of $O(\frac{MT}{|\mathcal{B}|})$ from restricting the bid space to $\mathcal{B}$, we obtain a continuous regret of $O(\Nitem^\frac{3}{2} \sqrt{\Nround \log \Nround})$.}
\end{theorem}

Note that both the time and space complexity scale polynomially in $\Nitem$ and $|\mathcal{B}|$. {\color{black}For the special case of $M=1$ corresponding to the standard first price auction, we obtain the regret of $\tilde{O}(\sqrt{\Nround})$.}





 \begin{algorithm}[t]
 \footnotesize
	\KwIn{Learning rate $0<\eta < \frac{1}{M}$, $\bm{v} \in [0, 1]^{+\Nitem}$.}
	\KwOut{The aggregate utility $\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b}^{\nround})$}
	$W_\nitem^0(b) \gets 0$ for all $\nitem \in [\Nitem], b \in \mathcal{B}$ such that $b \leq v_m$; else $W_\nitem^0(b) \gets -\infty$\;
        $b_{0}^t \gets \max \mathcal B$, and $S_{M+1}^t (\min \mathcal{B})=1$ for any $t\in[T]$\;
 
	\For{$\nround \in [1,\ldots,\Nround]$:}{
            \textbf{Recursively Computing Exponentially Weighted Partial Utilities $\bm{S}^t$}\;
            \textbf{for} $m \in [M,\ldots,1], b \in \mathcal{B}: S^t_\nitem(b) \gets \exp(\eta W_\nitem^\nround(b)) \sum_{b' \leq b} S_{\nitem + 1}^\nround(b')$. \label{eq: compute s} \hspace{0mm} $\backslash \backslash$ $\textsc{Compute}-S_\nitem$\;
        \textbf{Determining the Bid Vector $\bm{b}^\nround$ Recursively}\;
        \textbf{for} $m \in [1,\ldots,M], b \leq b_{m-1}^t: b_\nitem^\nround \gets b$ with probability $ \frac{S^t_\nitem(b)}{\sum_{b' \leq b_{\nitem-1}^t} S^t_{\nitem}(b')};$ \hspace{1mm} $\backslash \backslash$ $\textsc{Sample}-\bm{b}$\;
            Observe $\bm{b}^{\nround}_-$ and receive reward $\mu_n^\nround(\bm{b}^{\nround})$\;
        \textbf{Update Weight Estimates} \;
        \textbf{for} $\nitem \in [\Nitem], b \in \mathcal{B}: W^{\nround+1}_{\nitem}(b) \gets W^{\nround}_{\nitem}(b) +  (v_\nitem - b)\textbf{1}_{b \geq b^t_{-m}}$ if $b \leq v_m$; else $W^{\nround+1}_{\nitem}(b) \gets -\infty$\;
        }
        
        
        \textbf{Return} $\sum_{\nround=1}^\Nround \mu_n^{\nround}(\bm{b}^{\nround})$
	\caption{\textsc{Decoupled Exponential Weights - Full Information}}
	\label{alg: Decoupled Exponential Weights}
\end{algorithm}


\subsubsection{Bandit Feedback Setting}

\label{sec: decoupled exp weights, bandit feedback}


We extend Algorithm~\ref{alg: Decoupled Exponential Weights} for the bandit feedback setting. In the bandit feedback setting, the bidder's allocation and utility are not available for all possible bid vectors, unlike in the full information setting. Instead, the agent only observes their utility for the submitted bid vector. To handle this, we use inverse probability weighted (IPW) node weight estimates $\widehat{w}^t_m(b)$ instead of the node weights $w^t_m(b)$ in Algorithm~\ref{alg: Decoupled Exponential Weights}. This adaptation results in a regret of $O(M^{\frac{3}{2}}\sqrt{|\mathcal{B}| T \log |\mathcal{B}|})$, as shown in Theorem~\ref{thm:decoupled exp - bandit feedback}. This regret includes an additional factor of $\sqrt{|\mathcal{B}|}$ compared to the full information setting. 


The structure of Algorithm \ref{alg: Decoupled Exponential Weights - Path Kernels} is similar to that of Algorithm~\ref{alg: Decoupled Exponential Weights}. Both algorithms maintain node weight estimates, 
compute the sum of exponentiated partial bid vector estimated utilities recursively, and sample bids for each unit recursively proportional to these summed exponentiated utilities.
Specifically, Algorithm \ref{alg: Decoupled Exponential Weights - Path Kernels} samples bid vectors with probabilities proportional to the sum of the cumulative \textit{estimated} utility $\widehat{W}_{\nitem}^\nround(b) = \sum_{\tau=1}^{t-1} \widehat{w}_{\nitem}^\tau(b)$ over each unit-bid value pair, where   $\widehat{w}_{m}^\nround(b) = 1 - \frac{1 - (v_m - b)\textbf{1}_{b \geq b_{-m}^{\nround}}}{q_m^\nround(b)}\textbf{1}_{b_m^\nround = b}$ and $q_m^\nround(b)$ is the (unconditional) probability that bid $b$ is chosen for unit $m$ at time $t$.

Note that this bid vector utility estimator is a slightly different estimator {than the one} used in the standard $\textsc{Exp3}$ algorithm (See Chapter 11 of \cite{Lattimore2020}). In particular, the standard IPW estimator $\widehat{w}_m^\nround(b) = \frac{v_m-b}{q_m^\nround(b)} \textbf{1}_{b = b_m^t > b_{-m}^t}$, while unbiased, can be unboundedly large when $q_m^\nround(b)$ approaches 0, whereas our proposed estimator is bounded above by 1. As a consequence, we have that $\widehat{\mu}_n^\nround(\bm{b})$ is upper bounded by $M$, and therefore $ \eta \widehat{\mu}^\nround(\bm{b}) = \eta \sum_{m=1}^M \widehat{w}_m^\nround(b)$ is upper bounded by 1 for $\eta < \frac{1}{M}$, which we crucially use in the proof. 

 \begin{algorithm}[t]
 \footnotesize
	\KwIn{Learning rate $0 < \eta < \frac{1}{M}$, $\bm{v} \in [0, 1]^{+\Nitem}$}
	\KwOut{The aggregate utility $\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b}^{\nround})$}
	$\widehat{W}_\nitem^0(b) \gets 0$ for all $\nitem \in [\Nitem], b \in \mathcal{B}$ such that $b \leq v_\nitem$; else $\widehat{W}_\nitem^0(b) \gets -\infty$.\;
        $b_{0}^t \gets \max \mathcal B$, and $\widehat{S}_{M+1}^t (\min \mathcal{B})=1$ for any $t\in[T]$\;
 
	\For{$\nround \in [1,\ldots,\Nround]$:}{
            \textbf{Recursively Computing Exponentially Weighted Partial Utilities $\bm{S}^t$}\;
            \textbf{for} $m \in [M,\ldots,1], b \in \mathcal{B}: \widehat{S}^t_\nitem(b) \gets \exp(\eta \widehat{W}_\nitem^\nround(b)) \sum_{b' \leq b} \widehat{S}_{\nitem + 1}^\nround(b')$ \hspace{0mm} $\backslash \backslash$ $\textsc{Compute}-\widehat{S}_\nitem$\;
        \textbf{Determining the Bid Vector $\bm{b}^\nround$ Recursively}\;
        \textbf{for} $m \in [1,\ldots,M], b \leq b_{m-1}^t: b_\nitem^\nround \gets b$ with probability $\frac{\widehat{S}^t_\nitem(b)}{\sum_{b' \leq b_{\nitem-1}^t} \widehat{S}^t_{\nitem}(b')}; $ \hspace{1mm} $\backslash \backslash$ $\textsc{Sample}-\bm{b}$\;
        Observe $\bm{b}^{\nround}_-$ and receive reward $\mu_n^\nround(\bm{b}^{\nround})$\;
        \textbf{Recursively Computing Probability Measure $\bm{q}$}\;
        $q^t_1(b) \gets \frac{\widehat{S}^\nround_m(b)}{\sum_{b' \in \mathcal{B}} \widehat{S}^\nround_m(b')}$ for all $b \in \mathcal{B}$\;
        \textbf{for} $m \in [2,\ldots,M], b \in \mathcal{B}: q_\nitem^\nround(b) \gets \sum_{b' \geq b} \frac{q_{\nitem-1}^t(b')\widehat{S}^\nround_{\nitem}(b)}{\sum_{b" \geq b'} \widehat{S}^\nround_\nitem(b")}$ for all $b \in \mathcal{B}$\;
        \textbf{Update Weight Estimates}\;
        \textbf{for} $m \in [M], b \in \mathcal{B}: \widehat{W}^{\nround+1}_{\nitem}(b) \gets \widehat{W}^{\nround}_{\nitem}(b) + (1 - \frac{1 - (v_m - b)\textbf{1}_{b \geq b^t_m}}{q^t_m(b)} \textbf{1}_{b^t_m = b})$ if $b \leq v_m$; else $\widehat{W}^{\nround+1}_{\nitem}(b) \gets -\infty$\;
        }
        \textbf{Return} $\sum_{\nround=1}^\Nround \mu_n^{\nround}(\bm{b}^{\nround})$
	\caption{\textsc{Decoupled Exponential Weights - Bandit Feedback}}
	\label{alg: Decoupled Exponential Weights - Path Kernels}
\end{algorithm}


   
    
    The primary difference in the implementation of Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels} as compared to Algorithm~\ref{alg: Decoupled Exponential Weights} is that we require additional steps in order to obtain unbiased node weight estimates $\widehat{W}^{\nround+1}_{\nitem}(b) = \sum_{\tau=1}^{t} \widehat{w}_m^\tau(b)$ which we compute using an IPW estimator. In order to do this, we must compute $q_m^t(b)$---the  probabilities of selecting bid $b$ at slot $m$. 
    
   

\begin{theorem}[Decoupled Exponential Weights: Bandit Feedback] \label{thm:decoupled exp - bandit feedback}
    With $\eta = \Theta(\sqrt{\frac{\log |\mathcal{B}|}{M|\mathcal{B}|T}})$ such that $\eta < \frac{1}{M}$, Algorithm \ref{alg: Decoupled Exponential Weights - Path Kernels} achieves (discretized) regret $O(\Nitem^{\frac{3}{2}} \sqrt{ |\mathcal{{B}|}\Nround \log |\mathcal{B}|})$, with total time and space complexity polynomial in $\Nitem$, $|\mathcal{B}|$, and $\Nround$. {\color{black}Optimizing for discretization error of order $O(\frac{MT}{|\mathcal{B}|})$ from restricting the bid space to $\mathcal{B}$, we obtain a continuous regret of $O(M^{\frac{4}{3}}T^{\frac{2}{3}} \sqrt{\log \Nround})$.}
\end{theorem}

{\color{black}For the case of $M=1$---the standard first price auction---the (continuous) regret scales with $\tilde{O}(T^{\frac{2}{3}})$, which matches the (tight) regret bound of \cite{ContextBanditsCrossLearning2019} for the bandit setting.}



