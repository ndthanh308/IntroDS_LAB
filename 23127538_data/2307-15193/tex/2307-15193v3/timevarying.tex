\section{Extension: Time Varying Valuations}

\label{sec: time varying}
{\color{black}
We extend Algorithms \ref{alg: Decoupled Exponential Weights} and \ref{alg: Decoupled Exponential Weights - Path Kernels} to the time varying valuations setting. In particular, we assume that the valuations $\bm{v}$ are no longer fixed, and instead, in every round $t$, $\bm{v}^\nround$ is independently  drawn from some known distribution $F_{\bm{v}}$ with discrete, finite support $\mathcal{V}$. 
This contextual setting requires a  stronger  benchmark oracle in comparison to our original setup with a fixed valuation. The new benchmark oracle, which we will formalize shortly, possesses knowledge of the hindsight optimal bid vector for each context. That is, under this benchmark, we have the optimal mapping from any context (valuation vector) to an action (bid vector). 
Consequently, our current definitions of $\textsc{Regret}$ and $\textsc{Regret}_{\mathcal{B}}$ need to be updated to accommodate these contextual factors:
\begin{align}
\tag{Continuous Contextual Regret}
    \textsc{Regret}(F_{\bm{v}}) = \max_{\bm{b}: \mathcal{V} \to [0, 1]^{+\Nitem}} \sum_{\nround=1}^\Nround \mathbb{E}_{\bm{v} \sim F_{\bm{v}}}[\mu^\nround_n(\bm{b}(\bm{v}); \bm{v})] - \mathbb{E}\left[\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround; \bm{v}^\nround)\right]\,.
\end{align}
Here, $\mu^\nround_n(\bm{b}; \bm{v})$ denotes the utility of bidder $n$ by submitting bid vector $\bm{b}$ with valuations $\bm{v}$ at round $t$ where the competing bids are $\bm{b}_{-}^{t}$. Observe that in the benchmark of $ \textsc{Regret}(F_{\bm{v}})$, i.e., $\max_{\bm{b}: \mathcal{V} \to [0, 1]^{+\Nitem}} \sum_{\nround=1}^\Nround \mathbb{E}_{\bm{v} \sim F_{\bm{v}}}[\mu^\nround_n(\bm{b}(\bm{v}); \bm{v})]$, we abuse notation and define valuation-to-bid vector mapping $\bm{b}: \mathcal{V} \to [0, 1]^{+M}$.  
We have an equivalent definition of discretized contextual regret:
\begin{align}
\tag{Discretized Contextual Regret}
    \textsc{Regret}_\mathcal{B}(F_{\bm{v}}) = \max_{\bm{b}:\mathcal {V}\to
    \mathcal{B}^{+\Nitem}} \sum_{\nround=1}^\Nround \mathbb{E}_{\bm{v} \sim F_{\bm{v}}}[\mu^\nround_n(\bm{b}(\bm{v}); \bm{v})] - \mathbb{E}\left[\sum_{\nround=1}^\Nround \mu^\nround_n(\bm{b}^\nround; \bm{v}^\nround)\right]\,.
\end{align}
An agent's goal is to minimize their contextual regret with respect to their valuation distribution $F_{\bm{v}}$. Using naive contextual bandit algorithms would lead to a large regret, as the regret of these algorithms  scales  with the square root of the number of contexts. However, we make an observation that we have \emph{complete cross-learning} over these contexts as in \cite{ContextBanditsCrossLearning2019}. That is,
 whenever the agents chooses bid $\bm b$ in round $t$ while having context/value $\bm v$ and receives reward $\mu_{n}^{t}({\bm b}; \mathbf v)$, they also learn the value of $\mu_{n}^{t}({\bm b}; \mathbf v')$ for any  contexts/values $\bm v'$
 This is because of the functional form of $\mu_n(\bm{b}; {\bm v}) = \sum_{\nitem=1}^{x_n(\bm{b}, {\bm b}_{-})} (v_{n, \nitem} - b_{n, \nitem})$.

As such, we borrow  from the results described in \cite{ContextBanditsCrossLearning2019}; specifically those explaining the cross-learning-across-contexts generalizations of the $\textsc{EXP3}$ algorithm in the stochastic contexts (valuations) and adversarial rewards setting (adversarial competing bids). 
We assume that the agent has access to their valuation distribution. Moreover,  as stated earlier, we assume that the support of this valuation distribution is finite; i.e., $|\mathcal{V}| < \infty$. This scenario occurs often in practice where bidders' valuations depend naturally on some natural events. For example, investors may prescribe a `low' or `high' value to certain assets depending on various market indices. 

We generalize the $\textsc{EXP3-CL}$ algorithm described in \cite{ContextBanditsCrossLearning2019} to our PAB setting, specifically Algorithm~\ref{alg: Decoupled Exponential Weights - Path Kernels}, and  achieve exactly the same regret rates as our non-contextual variants, albeit requiring an additional $O(|\mathcal{V}|)$ factor of memory and computation.

In order to make the generalization more clear, at a high level, the $\textsc{EXP3-CL}$ algorithm on a set of $K$ arms and $C$ contexts with full cross-learning constructs a reward estimator $\hat{r}(k; c) = \frac{r(k; c)}{\sum_{c}\prob(c)\prob(k^t = k | c^t = c)}\textbf{1}_{k^t = k}$ for each arm $k$ and context $c$ pair. Here, the term $\sum_{c}\prob(c)\prob(k^t = k | c^t = c)$ is the expected probability that arm $k^t = k$ was selected under context $c^t = c$, where in the summation we take expectation over the stochasticity of contexts $c$. This estimator mirrors that of standard $\textsc{EXP3}$ using the IPW estimator, except that the IPW is averaged over the context distribution. 

To apply this to our setting, we wish to mimic the behavior of the $\textsc{EXP3-CL}$ algorithm with our decoupled exponential weights algorithm. This can be done by running the $\textsc{EXP3-CL}$ estimator on all of the nodes $b \in \mathcal{B}$ within each layer $m \in [M]$. In particular, we use the following estimator  $\widehat{w}_m^t(b; \bm{v}) = 1 - \frac{1 - w_m^t(b; \bm{v})}{Q_m^t(b)} \textbf{1}_{b_m^t = b}$, where the  normalizer $Q_m^t(b) = \sum_{\bm{v} \in \mathcal{V}} \prob(\bm{v}^t = \bm{v}) q_m^t(b; \bm{v})$ in this estimator 
is the expected probability of selecting bid $b$, where the expectation is taken with respect to all valuation vectors $\bm{v} \in \mathcal{V}$. This procedure, formally described in Algorithm~\ref{alg: Decoupled Exponential Weights - Time Varying Known Finite} and analyzed in the online appendix (arXiv:2307.15193v3), yields the following regret upper bound:

\begin{theorem}[Time Varying Valuations - Decoupled Exponential Weights] \label{thm: time varying known finite}
    Under bandit feedback (resp. full information feedback), Algorithm~\ref{alg: Decoupled Exponential Weights - Time Varying Known Finite}, with appropriately chosen $\eta$, achieves contextual continuous regret $\textsc{Regret}(F_{\bm{v}})$ of order $O(\Nitem^\frac{4}{3} \Nround^{\frac{2}{3}} \sqrt{\log \Nround})$ (resp. $O(\Nitem^\frac{3}{2} \sqrt{\Nround \log \Nround})$ with total time time and space complexity polynomial in $M$, $|\mathcal{B}|$, $|\mathcal{V}|$, and $\Nround$.
\end{theorem}



\begin{algorithm}[t]
\footnotesize
	\KwIn{Learning rate $0 < \eta < \frac{1}{M}$, Valuation Distribution $F_{\bm{v}}$}
	\KwOut{The aggregate utility $\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b}^{\nround}; \bm{v}^\nround)$}
	$\widehat{W}_\nitem^0(b; \bm{v}) \gets 0$ for all $\nitem \in [\Nitem], b \in \mathcal{B}, \bm{v} \in \mathcal{V}$ such that $b \leq v_m$; else $\widehat{W}_\nitem^0(b; \bm{v}) \gets -\infty$.\; 
	\For{$\nround \in [1,\ldots,\Nround]$:}{
            \textbf{Observe Valuation Vector $\bm{v}^t \sim F_{\bm{v}}$}\;
            $b_{0}^t \gets \max \mathcal B$, and $\widehat{S}_{M+1}^t (\min \mathcal{B}; \bm{v}^t)=1$ for any $t\in[T]$\;
            \textbf{Recursively Computing Exponentially Weighted Partial Utilities $\bm{S}^t$}\;
            \textbf{for} $m \in [M,\ldots,1], b \in \mathcal{B}: \widehat{S}^t_\nitem(b; \bm{v}^t) \gets \exp(\eta \widehat{W}_\nitem^\nround(b; \bm{v}^t)) \sum_{b' \leq b} \widehat{S}_{\nitem + 1}^\nround(b'; \bm{v}^t)$ \hspace{0mm} $\backslash \backslash$ $\textsc{Compute}-\widehat{S}_\nitem$\;
        \textbf{Determining the Bid Vector $\bm{b}^\nround$ Recursively}\;
        \textbf{for} $m \in [1,\ldots,M], b \leq b_{m-1}^t: b_\nitem^\nround \gets b$ with probability $\frac{\widehat{S}^t_\nitem(b; \bm{v}^t)}{\sum_{b' \leq b_{\nitem-1}^t} \widehat{S}^t_{\nitem}(b'; \bm{v}^t)}; $ \hspace{1mm} $\backslash \backslash$ $\textsc{Sample}-\bm{b}$\;
        Observe $\bm{b}^{\nround}_-$ and receive reward $\mu_n^\nround(\bm{b}^{\nround}; \bm{v}^t)$\;
        $Q_m^t(b) \gets 0$ for all $m \in [M], b \in \mathcal{B}$\;
        \For{$\bm{v} \in \mathcal{V}$:}{
            \textbf{Recursively Computing Probability Measure $\bm{q}$ Under $\bm{v}\in {\mathcal V}$}\;
            $\widehat{S}^t_{M+1}(b; \bm{v}) \gets 1$ for all $m \in [M], b \in \mathcal{B}$\;
            \textbf{for} $m \in [M,\ldots,1], b \in \mathcal{B}: \widehat{S}^t_\nitem(b; \bm{v}) \gets \exp(\eta \widehat{W}_\nitem^\nround(b; \bm{v})) \sum_{b' \leq b} \widehat{S}_{\nitem + 1}^\nround(b'; \bm{v})$\;
            $q^t_1(b; \bm{v}) \gets \frac{\widehat{S}^\nround_m(b; \bm{v})}{\sum_{b' \in \mathcal{B}} \widehat{S}^\nround_m(b'; \bm{v})}$ for all $b \in \mathcal{B}$\;
            \textbf{for} $m \in [2,\ldots,M], b \in \mathcal{B}: q_\nitem^\nround(b; \bm{v}) \gets \sum_{b' \geq b} \frac{q_{\nitem-1}^t(b'; \bm{v})\widehat{S}^\nround_{\nitem}(b; \bm{v})}{\sum_{b" \geq b'} \widehat{S}^\nround_\nitem(b"; \bm{v})}$ for all $b \in \mathcal{B}$\;
            $Q_m^t(b) \gets Q_m^t(b) + \prob(\bm{v}^t = \bm{v})q_m^t(b; \bm{v})$
        }
        \textbf{Update Weight Estimates}\;
        \textbf{if} $\textsc{Bandit Feedback}$, \textbf{for} $m \in [M], b \in \mathcal{B}, \bm{v} \in \mathcal{V}$\; 
        $\widehat{W}^{\nround+1}_{\nitem}(b; \bm{v}) \gets \widehat{W}^{\nround}_{\nitem}(b; \bm{v}) + (1 - \frac{1 - (v - b)\textbf{1}_{b \geq b^t_m}}{Q_m^t(b)} \textbf{1}_{b^t_m = b})$ if $b \leq v$; else $\widehat{W}^{\nround+1}_{\nitem}(b; \bm{v}) \gets -\infty$\;
        \textbf{if} $\textsc{Full Information}$, \textbf{for} $m \in [M], b \in \mathcal{B}, \bm{v} \in \mathcal{V}$\;
        $\widehat{W}^{\nround+1}_{\nitem}(b; \bm{v}) \gets \widehat{W}^{\nround}_{\nitem}(b; \bm{v}) + (v - b)\textbf{1}_{b \geq b^t_m}$ if $b \leq v$; else $\widehat{W}^{\nround+1}_{\nitem}(b; \bm{v}) \gets -\infty$\;
        }
        \textbf{Return} $\sum_{\nround=1}^\Nround \mu_n^{\nround}(\bm{b}^{\nround}; \bm{v}^\nround)$
	\caption{\textsc{Decoupled $\textsc{EXP3-CL}$ - Time Varying Valuations}}
	\label{alg: Decoupled Exponential Weights - Time Varying Known Finite}
\end{algorithm}}