\subsection{Online Setting: Online Mirror Descent} 
\label{sec:bandit}


We propose our second online learning algorithm which achieves faster regret rates than our decoupled exponential weights algorithms at the cost of additional computation. Instead of mimicking the exponential weights algorithm, we reformulate the problem as online linear optimization over node probabilities in our DP graph. We solve for these probabilities using OMD and construct a corresponding policy that sequentially samples bids based on these probabilities.


 \begin{algorithm}[t]
 \footnotesize
	\KwIn{Learning rate $\eta > 0$, Valuation $\bm{v} \in [0, 1]^{+\Nitem}$ } 
	\KwOut{The aggregate utility $\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b}^\nround)$.}
	$\pi_0((m, b), b') \gets \frac{1}{|\{b" \in \mathcal{B}: b" \leq b, b" \leq v_{m+1}\}|}$ for all $\nitem \in [\Nitem], b \geq b' \in \mathcal{B}, b' \leq v_{m+1}$. Let $\bm{q}^0 \in [\Nitem] \times \mathcal{B} \to [0, 1]$ be the corresponding unit-bid value pair occupancy measure\;
	\For{$\nround \in [\Nround]$:}{
            \textbf{Determining the Bid Vector $\bm {b}^t$ recursively.} Set $b_1$ to $b \in \mathcal{B}$ with probability  $q^t_1(b)$\;
            \textbf{for} $m \in [1,\ldots,M-1], b \in \mathcal{B}: b_{m+1} \gets b$ with probability $\pi^t((m, b_\nitem), b)$\;
            Receive reward $\mu^\nround_n(\bm{b}^\nround) = \sum_{\nitem=1}^\Nitem w_\nitem^\nround(b^\nround_\nitem)$ and observe $w_\nitem^\nround(b^\nround_\nitem)$ where $w_\nitem^\nround(b) = (v_\nitem - b)\textbf{1}_{b \geq b^\nround_{-\nitem}}$\;
            \textbf{Update Reward Estimates}\;
            \textbf{for} $\nitem \in [\Nitem], b \in \mathcal{B}: \widehat{w}_\nitem^\nround(b) \gets \frac{w_\nitem^\nround(b)}{q^{\nround-1}_\nitem(b)} \textbf{1}_{b = b^{\nround}_\nitem}$ if \emph{Bandit Feedback}, $\widehat{w}_\nitem^\nround(b) \gets w_\nitem^\nround(b)$ if \emph{Full Information}\;
            \textbf{Determining Probability Measure $\bm{q}^t$ over any unit-bid value pair $(m, b)$.} Set \vspace{-2mm}
            \begin{align}
                \bm{q}^\nround \gets \text{argmin}_{\bm{q} \in \mathcal{Q}} \eta\langle \bm{q}, -\widehat{\bm{w}}^\nround\rangle + D(\bm{q} || \bm{q}^{\nround-1}) \label{eq:q_t}\,
            \end{align} 
            where $\mathcal{Q}$ is as in Equation ~\eqref{eq:Q} and $D(\bm{q} || \bm{q}') = \sum_{\nitem \in [\Nitem], b \in \mathcal{B}} q_\nitem(b)\log \frac{q_\nitem(b)}{q'_\nitem(b)} - (q_\nitem(b) - q'_\nitem(b))$.
            
            \textbf{Convert $\bm{q}^t$ to Policy $\bm{\pi}^t$.}
            Compute any feasible solution $\bm{\pi}^t$ to constraints $q^t_m(b) = \sum_{b' \geq b} q^t_m(b') \pi^t((m-1, b'), b)$ and $\sum_{b" \leq b} \pi^t((m, b), b") = 1$ for all $m \in [M], b \in \mathcal{B}$.
        }
        \textbf{Return $\sum_{\nround=1}^\Nround \mu_n^\nround(\bm{b}^\nround)$.} 
	\caption{\textsc{OMD - Bid Optimization in Multi-Unit Pay as Bid Auctions}}
	\label{alg: OMD}
\end{algorithm}


\textbf{Algorithm Ideas. }Recall that $\bm{q}: [M] \times \mathcal{B} \to [0, 1]$ denotes the node probabilities; i.e., $q_m^t(b)$ is the probability of selecting bid $b$ for unit $m$ at round $t$. Utilizing PAB's reward structure, we can rewrite the aggregate utility---and consequently, the regret---as a function of $\{\bm{q}^t, \bm{w}^t\}_{t \in [T]}$
\begin{align}
    \mathbb{E}_{\bm{b} \sim \bm{\pi}}\left[ \sum_{t=1}^T \mu^{\nround}_n(\bm{b}) \right] = \sum_{t=1}^T \sum_{\nitem=1}^\Nitem \mathbb{E}_{b_\nitem \sim \bm{q}^t_\nitem}\left[ w_\nitem^\nround(b_\nitem) \right] = \sum_{t=1}^T \sum_{\nitem=1}^\Nitem  \sum_{b \in \mathcal{B}} q^t_\nitem(b) w_\nitem^\nround(b) = \sum_{t=1}^T \langle \bm{q}^t, \bm{w}^\nround \rangle\,.
    \label{eq: Loss of policy}
\end{align}
We make the important observation that the rewards are \textit{linear} in the $\bm{q}^t$'s, which immediately hints towards the use of online linear optimization (OLO). It is also straightforward to verify linearity\footnote{While showing that the linearity of set $\mathcal{Q}$ is trivial, we must also show for completeness that any valid policy over our DP graph $\bm{\pi}$ yields node probabilities $\bm{q} \in \mathcal{Q}$. We leave the proof of this in the online appendix, Lemma~\ref{lem: QSpace Equivalence}.} of the feasible region $\mathcal{Q}$ of $\bm{q}$, as they only contain non-negativity, probability mass, and stochastic dominance constraints:
\begin{align}
    \mathcal{Q} = \Big\{\bm{q} \in [0, 1]^{M \times |\mathcal{B}|}:  \sum_{b \in \mathcal{B}} q_m(b) = 1, \sum_{b \leq b'} q_{m+1}(b) \geq \sum_{b \leq b'} q_m(b) \forall b, b' \in \mathcal{B}, m \in [M]\Big\}\,.
    \label{eq:Q}
\end{align}
Now, we make a second important observation that there is no explicit dependence of the regret or feasible region $\mathcal{Q}$ on the policy $\bm{\pi}$ that generated the node probabilities $\bm{q}$. Indeed, one of our primary contributions is the insight that our algorithm's regret guarantee is agnostic w.r.t. the exact policy $\bm{\pi}$, conditional upon having the same node probabilities $\bm{q}$. Unlike existing algorithms that optimize over policies $\bm{\pi}^t$ over a graph's node-edge pairs \cite{OREPS2013, PathKernel2003} which is of size $O(M|\mathcal{B}|^2)$ in our setting, our approach of optimizing over node probabilities saves an additional factor of $|\mathcal{B}|$ in both regret and computation. Using negentropy regularization as in Equation \eqref{eq:q_t} of Algorithm~\ref{alg: OMD}, we achieve continuous regret \textit{linear} in $M$:

\begin{theorem}[Online Mirror Descent: Bandit Feedback] \label{thm: OMD}    With $\eta = \Theta(\sqrt{\frac{\log |\mathcal{B}|}{|\mathcal{B}|\Nround}})$, Algorithm~\ref{alg: OMD} achieves (discretized) regret $O(\Nitem \sqrt{|\mathcal{B}| \Nround \log |\mathcal{B}|})$, with total time and space complexity polynomial in $\Nitem$, $|\mathcal{B}|$, and $\Nround$. {\color{black}Optimizing for discretization error of order $O(\frac{MT}{|\mathcal{B}|})$ from restricting the bid space to $\mathcal{B}$, we obtain a continuous regret of $O(\Nitem \Nround^{\frac{2}{3}})$.} 
\end{theorem}


We defer the proof to the online appendix (arXiv:2307.15193v3). Under full information, we recover the regret of Algorithm \ref{alg: Decoupled Exponential Weights} by replacing the node weight estimates with the true weights.

\begin{corollary}[Online Mirror Descent: Full Information] \label{cor}
    With $\eta = \Theta(\sqrt{\frac{\log |\mathcal{B}|}{T}})$, Algorithm \ref{alg: OMD} achieves (discretized) regret $O(\Nitem \sqrt{ \Nround \log |\mathcal{B}|})$, with total time and space complexity polynomial in $\Nitem$, $|\mathcal{B}|$, and $\Nround$. {\color{black}Optimizing for discretization error of order $O(\frac{MT}{|\mathcal{B}|})$ from restricting the bid space to $\mathcal{B}$, we obtain a continuous regret of $O(\Nitem \sqrt{\Nround \log \Nround})$.}
\end{corollary}


\section{Regret Lower Bound}
\label{sec: lower bound}
{\color{black}In this section, we complement our upper bound results on regret with corresponding lower bounds. We introduce two bounds: for the full information setting, we establish a regret lower bound of \(\Omega(\Nitem \sqrt{\Nround})\), which is also a valid lower bound for the bandit setting. This lower bound demonstrates that even in a full-information, stochastic setting, continuous regret should scale linearly with \(\Nitem\), reflecting the linear dependency observed in our OMD-based algorithms (Algorithm \ref{alg: OMD}) that achieve regret \(O(\Nitem \sqrt{\Nround \log \Nround})\). Furthermore, our regret lower bound aligns with the continuous regret upper bound for the OMD algorithm in the full information setting, up to logarithmic factors. This matching bound indicates  that no algorithm, with or without discretization, can achieve smaller regret. For the bandit setting, we additionally present a regret lower bound of \(\Omega(\Nitem^{\frac{2}{3}} \Nround^{\frac{2}{3}})\), matching the regret lower bounds of our two algorithms in terms of \(\Nround\)'s dependence for the bandit setting.}


\begin{theorem}[Regret Lower Bound for the Full Information Setting]\label{thm:lower}
    In the full information setting, the continuous regret of learning to bid in PAB is $\Omega(\Nitem \sqrt{\Nround})$. This implies an equivalent lower bound in the bandit setting.
\end{theorem}

{\color{black}To establish lower bounds, we devise a stochastic adversary whose bidding distribution makes it challenging for the bidder to optimize bids, resulting in $\Omega(M\sqrt{T})$ regret. We define bid vectors $\bm{b}'_{-}$ and $\bm{b}"_{-}$ and construct two adversary bid distributions $F$ and $G$. Under $F$, $\prob(\bm{b}_-^\nround = \bm{b}'_{-}) = \frac{1}{2} + \delta$, and under $G$, $\prob(\bm{b}_{-}^\nround = \bm{b}'_{-}) = \frac{1}{2} - \delta$. We analyze the expected utilities for bidding vectors under $F$ and $G$ and derive expressions for expected utilities. By evaluating these expressions, we compute a per-step incurred regret of $\Theta(\Nitem \delta)$. Employing Le Cam's method and relating regret under $(F+G)/2$ to the Kullback-Leibler divergence between $F$ and $G$, we establish a regret bound of $\Omega(\Nitem \sqrt{\Nround})$. This bound holds when $\delta$ is chosen to be $\Omega(\frac{1}{\sqrt{T}})$. 

The main novelty in the proof is the structure of  the highest  competing bids $\bm{b}'_{-}$ and $\bm{b}"_{-}$, where we set
$\bm{b}'^{-} = (0,\ldots,0,c,\ldots,c)$, with $k$ values of 0 and $\Nitem - k$ values of $c$. Additionally, we define  $\bm{b}"^{-} = (c,\ldots,c)$ as the $\Nitem$-vector of bids at $c$. We then show that  for certain choices of $c$ and $k$, the expected utility-maximizing bid vector under ${\bm{b}_{-}^\nround} \sim F$ is $(0,\ldots,0)$ and under ${\bm{b}_{-}^\nround} \sim G$ is $(c,\ldots,c)$. This leads to the desired results after choosing $c$ and $k$ carefully.}

{\color{black}

\begin{theorem}[Regret Lower Bound for the Bandit Setting] \label{thm:lower_bound_bandit}
Assuming a time horizon of \(T\), a number of units \(M\), and that \(T \geq M^2\), the regret of learning to bid in PAB auctions is lower bounded by \(\Omega(M^{\frac{2}{3}} T^{\frac{2}{3}})\).
\end{theorem}

Theorems \ref{thm:lower} and \ref{thm:lower_bound_bandit}, which shown in Section \ref{sec:proof:lower:bandit}, together provide the regret lower bound of \(\Omega(\max\{ M^{\frac{2}{3}} T^{\frac{2}{3}}, M\sqrt{T} \})\) for the bandit setting. 
To construct the regret lower bound of \(\Omega(M^{\frac{2}{3}} T^{\frac{2}{3}})\) for the bandit setting, we begin with a base hypothesis that specifies a distribution over competing bids. Under this base hypothesis, the per-unit utility of unit \(m\) for each possible bid value is the same and equals some constant \(c_m\), which decays with \(m\). For each unit \(m\), we then construct \(O\left(\frac{|\mathcal{B}|}{M}\right)\) different hypotheses by perturbing the base distribution for one of the bids in a certain partition of \(\mathcal{B}\), denoted by \(\mathcal{B}_m\). See Figure \ref{fig:partitions} for an illustration of the partitions.

In fact, under the alternative hypothesis, the perturbed bid is optimal, and hence any learning algorithm should be able to identify the perturbed bid for any unit \(m\). The main challenge in such a construction is to ensure no ``cross-learning" occurs across units. That is, by learning the utility for a given unit, we do not learn anything about any other units. In our construction, the way we construct the alternative hypotheses ensures that their perturbations are disjoint across units, allowing us to prevent any cross-learning between units.
  
% Figure environment removed
}

