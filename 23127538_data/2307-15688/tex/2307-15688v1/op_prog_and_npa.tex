\section{NPA Hierarchy}
\subsection{Phrasing the Local Hamiltonian problem as an Operator Program}

Operator programs are a powerful and flexible way of stating difficult problems.  These are generally stated as the problem of optimization over non-commuting (nc) polynomials over sets of non-commuting variables ($\{a_i\}$).  In this context the variables $\{a_i\}$ are unspecified complex matrices of some finite, fixed, unspecified size ($a_i$ has the same size as $a_j$ so that multiplication is well defined).  It could be the case that the objective goes to infinity as the matrices get larger or that the objective converges to some fixed value in the limit of large matrices, but for the cases we will consider here an optimal feasible solution to the problem will consist of matrices of finite size, so the programs discussed here are all well-defined and explicitly obtain their maxima/minima.  %In fact, each of the programs described here has an explicit upper bound on the optimal matrix size which is exponential in the size of the problem.  
Depending on the convention \cite{pir10con}, one often also includes variables $\{a_i^*\}$ for denoting the complex conjugate of the matrix variables, however, in this paper these are redundant since we will always optimize over Hermitian matrices.  Polynomials in these variables will consist of linear combinations of monomials in the nc variables.  The set of monomials of degree $\leq \ell$ is denoted $\Gamma_\ell=\{a_{i_1} a_{i_2} ...a_{i_q}: q \leq \ell\}$ so an arbitrary degree-$\ell$ nc polynomial can be denoted $\theta(\{a_i\})=\sum_{\phi \in \Gamma_\ell} \theta_\phi \phi$ where $\theta_\phi \in \mathbb{C}$ for all $\phi$.  $\Gamma_\ell$ will always contain a term of degree $0$, $\mathbb{I}$.  $\mathbb{I}$ varies inside the program since it will have size matching the $\{a_i\}$ but will always denote an identity of the appropriate size.  
\begin{definition}\label{def:op_prog} Given nc polynomials $\theta$ and $\{\eta_i\}$ with $\theta^*=\theta$, an operator program $\mathcal{O}$ is an optimization problem of the following form:
% \begin{align}
% \min/\max \,\,\, \braket{\boldsymbol{\phi}| q(\{\mathbf{A}_i\})|\boldsymbol{\phi}}\\
% s.t. \,\,\,\, p_j(\{\mathbf{A}_i\})&=0 \,\, \text{  for all $j$}\\
% \braket{\boldsymbol{\phi}| \boldsymbol{\phi}}&=1\\
% \label{eq:herm_ness}\mathbf{A}_i&=\mathbf{A}_i^* \,\,\, \forall i
% \end{align}
\begin{align}
\min/\max \quad & \braket{g| \theta(\{a_i\})|g}\\
\mathrm{s.t.} \quad & \eta_j(\{a_i\}) =0 \,\, \text{  for all $j$},\\
& \label{eq:herm_ness}a_i =a_i^*,\, \forall i, \\
& \braket{g| g} =1.
\end{align}
\end{definition}

To build intuition we will first consider an ncp optimization problem where the constraints force the variables to be commuting, and hence the problem reduces to a combinatorial optimization problem.  Given a graph $(V, w)$, the {\scshape MaxCut} problem is equivalent to the following optimization problem:
\begin{align}
MC(V, w):=\max \,\, \sum_{ij} w_{ij} \frac{1-z_i z_j}{2}\,\,s.t. \,\, z_i \in \{\pm 1\} \,\, \forall \,\, i \in [n]. 
\end{align}
We could also have phrased {\scshape MaxCut} as a local Hamiltonian problem where the Local terms of the Hamiltonian are diagonal in the $Z$ basis \cite{woc03}.  In this case the largest eigenvalue would be $MC(V, w)$.  Since the matrix is diagonal, the extremal eigenvector can be assumed to be a computational basis state WLOG and this basis state provides the optimal assignment for Max Cut: 
\begin{align}\label{eq:ham_max_cut_def}
    MC(V, w)=\max \,\, \bra{g} \sum_{ij} w_{ij} \frac{\mathbb{I}-Z_i Z_j}{2} \ket{g} \,\, s.t. \,\, \braket{g|g}=1.
\end{align}
Note that the operators above are {\it not} variables, they are the explicitly defined Pauli matrices from \cref{sec:notation}.  Additionally the vector $\ket{g}$ is a vector varaible of fixed size, unlike \Cref{def:op_prog}.  A natural direction for stating \cref{eq:ham_max_cut_def} as a ncp optimization problem is ``promoting'' the actual Pauli matrices $Z_i$ to matrix variables $z_i$.  This would lead to a {\it relaxation} where the optimal solution to the operator program would be at least the solution to the relevant {\scshape MaxCut} instance. 
 To get the objectives to match we will need to explicitly enforce constraints on $z_i$ which are satisfied by $Z_i$.  We must demand that the $z_i$ commute, as well as that they square to the identity.  The resulting operator program is
\begin{align}\label{eq:op_max_cut_def}
    \max \quad & \bra{g} \sum_{ij} w_{ij} \frac{\mathbb{I}-z_i z_j}{2} \ket{g} \\ 
    \label{eq:mc1}\mathrm{s.t.}  \quad &z_i^2=\mathbb{I} \,\, \forall \,\, i \in[n],\\
     \label{eq:mc2}&z_i z_j-z_j z_i=0 \,\, \forall \,\,i, j \in [n],\\
     \label{eq:mc_end}&z_i^*=z_i\,\, \forall \,\, i \in [n],\\
     &\braket{g|g}=1.
\end{align}

 \begin{proposition}
     The program defined in \cref{eq:op_max_cut_def} has optimal objective $MC(V, w)$.
 \end{proposition}
\begin{proof}
    Let $z_i=Z_i'$ and $\ket{g}=\ket{\psi}$ be the optimal solution to \cref{eq:op_max_cut_def}.  $Z_i'$ all square to the identity and are Hermitian so they have at most two eigenvalues, $\{\pm 1\}$.  Since the $Z_i'$ all commute we can construct a basis which simultaneously diagonalizes all the $Z_i'$.  The objective is diagonal in this basis so we may assume WLOG that $\ket{\psi}$ is one of these basis elements and that $\bra{\psi} Z_i' \ket{\psi} \in \{\pm 1\}$.  Let us define $z_i'=\bra{\psi} Z_i' \ket{\psi} \in \{\pm 1\}$, so $(z_i')^2=1$.  By the eigenvector property,
    \begin{align*}
    \bra{\psi} \sum_{ij} w_{ij} \frac{\mathbb{I}-Z_i' Z_j'}{2} \ket{\psi}
    %=\sum_{ij} w_{ij} \frac{1-\bra{\psi}Z_i Z_j\ket{\psi}}{2} 
    =\sum_{ij} w_{ij} \frac{1-\bra{\psi}Z_i'\ket{\psi} \bra{\psi} Z_j'\ket{\psi}}{2} 
    =\sum_{ij} w_{ij} \frac{1-z_i' z_j'}{2},
    \end{align*}
    so the optimal objective of \cref{eq:op_max_cut_def} is less than or equal to $MC(V, w)$.  We already know that optimal objective of \cref{eq:op_max_cut_def} is greater than or equal to $MC(V, w)$ since it is a relaxation.
    
\end{proof}

Naturally we may consider a generic $2$-Local Hamiltonian problem and ask similar questions.  Arbitrary $2$-Local Hamiltonians may be written as $H=\sum_{ij} H_{ij}$ where $H_{ij}$ acts only on qubits $i$ and $j$.  We can express each $H_{ij}$ in the Pauli basis as 
\begin{equation}
    H_{ij}=\sum_{\sigma, \gamma \in \{\mathbb{I}, X, Y, Z\}} c_{\sigma, \gamma}^{ij} \,\, \sigma_i \gamma_j,
\end{equation}
for $c_{\sigma, \gamma}^{ij} \in \mathbb{R}$.  This lets us express the overall Hamiltonian as
\begin{equation}
    H=\sum_{ij} \sum_{\sigma, \gamma \in \{\mathbb{I}, X, Y, Z\}} c_{\sigma, \gamma}^{ij} \,\, \sigma_i \gamma_j.
\end{equation}
The maximum eigenvalue problem is then
\begin{equation}
   \mu_{max}(H)= \max \,\, \bra{g} \sum_{ij} \sum_{\sigma, \gamma \in \{\mathbb{I}, X, Y, Z\}} c_{\sigma, \gamma}^{ij} \,\, \sigma_i \gamma_j \ket{g} \,\, s.t. \,\, \braket{g|g}=1.
\end{equation}
We may promote the Pauli matrices above to operator variables, $X_i \rightarrow x_i, Y_i \rightarrow y_i, Z_i \rightarrow z_i$, to get a relaxation, but we will need to know what constraints to enforce to ensure that the operator problem has the same objective as the explicit local Hamiltonian problem we have in mind, just as for {\scshape MaxCut}.  Enforcing constraints of the form \cref{eq:mc1,eq:mc2,eq:mc_end} plus additional anti-commutation constraints is sufficient: 
\begin{definition}
Given a $2$-Local Hamiltonian $H$ on $n$ qubits,  

\begin{align}
\mathcal{Pauli}(H):= \max\quad & \bra{g}  \left(\sum_{ij} \sum_{\sigma, \gamma \in \{\mathbb{I}, x, y, z\}} c_{\sigma, \gamma}^{ij}\,\, \sigma_i \gamma_i\right)\ket{g}\\
\nonumber \mathrm{s.t.} \quad &\text{for all distinct $j, k\in [n]$}:\\
&\mathbb{I}=x_j^2=y_j^2=z_j^2, \\
\label{pauli_anti_commute_const} &\{x_j, y_j\}=0, \,\,\{x_j, z_j\}=0, \,\,\{y_j, z_j\}=0,\\
&a_j b_k-b_k a_j=0 \,\, \forall \,\,a, b \in \{x, y, z\},\\
&x_j^*=x_j, \,\,y_j^*=y_j, \,\,z_j^*=z_j,\\
&\braket{g|g}=1.
\end{align}
\end{definition}
\begin{proposition}[Theorem 2.3 in \cite{cha17}]
    $\mu_{max}(H)=\mathcal{Pauli}(H)$
\end{proposition}
The proof of this statement proceeds by showing that {\it any} operators which satisfy the relations above must be equal to the Pauli matrices up to overall unitary and tensoring with identity matrices.  In a sense the smallest feasible solution to $\mathcal{Pauli}(H)$ are the Pauli matrices themselves and larger solutions must have the same objective. 

\subsection{{\scshape QMaxCut} as an Operator Program}\label{subsec:qmcop}

While the $\mathcal{Pauli}$ program is very nice because of its generality, Hamiltonians are often best studied with the natural symmetry present taken into account.  Our interest is in a specific family of Local Hamiltonians known as ``Quantum Max Cut'' ({\scshape QMaxCut}) in many works \cite{gha19alm, par22opt, ans20bey, par21app}, so our aim is to produce the ``natural'' operator programs for these Hamiltonians.  Given a weighted graph $(V, w)$ with non-negative weights $w_{ij}\geq0$, the corresponding {\scshape QMaxCut} instance is defined on $n=|V|$ qubits \footnote{In later sections, $n$ is not always $|V|$ depending on the graph we focus on, which should be clear from the context.} by 
\begin{equation}\label{eq:QMCHamDef}
QMC(V, w):= \mu_{max} \left( \sum_{ij} w_{ij} H_{ij}\right),
\end{equation}
where $H_{ij} := \frac{1}{4} \left(\mathbb{I} - X_i X_j-Y_i Y_j -Z_i Z_j \right)$. 
The term $H_{ij}$ is a projector to the singlet state $|\psi^-_{ij}\rangle := (|0_i1_j\rangle - |1_i0_j\rangle)/\sqrt{2}$. Note that the singlet state is order sensitive ($|\psi^-_{ij}\rangle = - |\psi^-_{ji}\rangle$), but the Hamiltonian is not ($H_{ij}=H_{ji}$). 
This Hamiltonian has been well-studied in physics for decades, serving as central model for quantum magnetism. It has the nice property that it is rotation-invariant; that is, for any single-qubit unitary $U$, we have $(U^\dag)^{\otimes n} H_{ij} U^{\otimes n} = H_{ij}$. $H \succeq 0$ since we only consider non-negative weights $w$ ($\succeq 0$ denotes that a matrix is positive semidefinite.).  

It will be convenient for us to have a definition of another Hamiltonian which is simply an affine shift of the {\scshape QMaxCut} Hamiltonian.  If we define the usual quantum SWAP operators as 
\begin{equation}\label{eq:swap_def}
P_{ij}=\begin{bmatrix} 
1 & 0 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 0 & 1
\end{bmatrix}_{ij}=\frac{\mathbb{I} +X_i X_j +Y_i Y_j +Z_i Z_j}{2},
\end{equation}
we can then define
\begin{equation}
\SWAP(V, w)=\mu_{min} \left(\sum_{ij \in E} w_{ij} P_{ij} \right).
\end{equation}
The extremal eigenvalues are related as:
\begin{equation}
\SWAP (V, w)= \sum_{jk } w_{jk} -2 QMC (V, w).
\end{equation}

Our approach is to promote the operators $H_{ij}$ and $P_{ij}$ to variables, but we are left with the same question of deciding what constraints to include to accurately capture the local Hamiltonian problem.  Our work naturally extends that of ~\cite{par22opt}, who used such operators to obtain an optimal approximation for {\scshape QMaxCut} using product states.

One of the results of this work is showing the sufficiency of the following sets of constraints for {\scshape QMaxCut} and SWAP Hamiltoniants respectively:
\begin{comment}
By expressing the local terms in the Pauli basis we can go through a similar procedure 




optimal cut would correspond to the computational basis state which would also be the eigenvector corresponding to the largest eigenvalue, and 

The varaiables $\{\mathbf{z}_i\}$ are scalar (commuting) varaibles.  If we wanted to phrase this as an operator program the natural direction would be to promote 

We will assume that the matrices $\mathbf{A}_i$ are all Hermitian matrices of the same size (hence the product of matrices is well-defined).  The set of monomials of degree $\leq \ell$ is denoted $\Gamma_\ell=\{\mathbf{A}_{i_1} \mathbf{A}_{i_1} ...\mathbf{A}_{i_q}: q \leq \ell\}$ so an arbitrary degree-$\ell$ nc polynomial can be denoted $r(\{\mathbf{A}_i\})=\sum_{\mathbf{\Phi} \in \Gamma_\ell} r_\mathbf{\Phi} \mathbf{\Phi}$ where $r_\mathbf{\Phi} \in \mathbb{C}$ for all $\mathbf{\Phi}$.  

While their primary application is in optimizing over non-commuting operator variables, generally in the context of information, they can also be used to optimize over commuting operators.  Evaluating the solution to these commuting operator progmras is generally NP-hard.  As an example we may express the Max Cut problem as an operator program



All matrix varaibles will be assumed Hermitian, sp $(\mathbf{A}_{i_1} ... \mathbf{A}_{i_q})^*=\mathbf{A}_{i_q}^* ... \mathbf{A}_{i_1}^*=\mathbf{A}_{i_q} ... \mathbf{A}_{i_1}$, but note that degree-$\ell$ nc polynomials are not generally Hermitian (unless $\ell=1$): $r^*(\{\mathbf{A}_i\})=\sum_{\mathbf{\Phi} \in \Gamma_\ell} r_\mathbf{\Phi}^* \mathbf{\Phi}^*$.  If $\{A_i\}$ is some assignment to the variables $\{\mathbf{A}_i\}$ and $\mathbf{\Phi} =\mathbf{A}_{i_1} \mathbf{A}_{i_2} ...\mathbf{A}_{i_q} \in \Gamma_l$ then we will denote $\mathbf{\Phi}(\{A_i\})=A_{i_1} A_{i_2} ... A_{i_q}$.  A nc polynomial optimization problem is then an optimization problem with nc polynomials for the objective and constraints.  Specifically the objective will be the optimization of an extremal eigenvalue of a nc polynomial:

\noindent All of the operator programs in this paper satisfy $q^*=q$, so the objective is well-defined and equal to $\mu_{min}/\mu_{max}$ of $q(\{\mathbf{A}_i\})$.  

In our study of QMC we will be interested in three kinds of operator programs.  The first, $\mathcal{Pauli}$, is studied in all the existing works on approximation algorithms for QMC \cite{gha19alm,hwa22uni,par22opt,par21app}.  The other two operator programs, $\mathcal{Perm}$ and $\mathcal{Proj}$, are first presented in this work (although a ``low level'' of $\mathcal{Perm}$ is implicitly studied in \cite{par22opt}).

\begin{definition}
Given a weighted graph $(V, \{w\})$ on $n$ vertices,  

\begin{align}
\mathcal{Pauli}(G, w):= \,\,\max\bra{\boldsymbol{\phi}}  \left(\sum_{jk } w_{jk} \frac{\mathbb{I}-\mathbf{X}_j \mathbf{X}_k -\mathbf{Y}_j \mathbf{Y}_k- \mathbf{Z}_j \mathbf{Z}_k}{4}\right)\ket{\boldsymbol{\phi}}\\
s.t. \,\,\, \mathbb{I}=\mathbf{X}_j^2=&\mathbf{Y}_j^2=\mathbf{Z}_j^2 \,\, \forall j \in [n], \\
\label{pauli_anti_commute_const} \mathbf{X}_j \mathbf{Y}_j =&i \mathbf{Z}_j \,\, \forall j\in [n],\\
\mathbf{A}_j \mathbf{B}_k-\mathbf{B}_k \mathbf{A}_j=0 \,\, \forall \,\,\mathbf{A}, \mathbf{B} \in \{\mathbf{X}, &\mathbf{Y}, \mathbf{Z}\}, \text{ and distinct }j, k\in [n],\\
\braket{\boldsymbol{\phi}|\boldsymbol{\phi}}&=1,\\
\mathbf{X}_i^*=\mathbf{X}, \,\,\mathbf{Y}_i^*=\mathbf{Y},& \,\,\mathbf{Y}_i^*=\mathbf{Y} \,\,\, \forall i \in [n].
\end{align}
In contrast to the usual setting when $X$, $Y$ and $Z$ are fixed matrices, we emphasize that in the above program they should be thought of as unspecified matrix variables.  Note that \Cref{pauli_anti_commute_const} implies that $\mathbf{X}_j \mathbf{Y}_j +\mathbf{Y}_j \mathbf{X}_j=0$ (the usual anti-commutation constraint).  We will primarily be interested in the $\mathcal{Proj}$ and $\mathcal{Perm}$ operator programs since they are essentailly ``tailored'' to the QMC problem.  We define these programs as follows:
\end{definition}
\end{comment} 

\begin{definition}[$\mathcal{Proj}(V, w)$]  Given Hamiltonian $H=\sum_{ij} w_{ij} H_{ij}$ corresponding to graph $(V, w)$, define 
\begin{align}
\mathcal{Proj}(H)=\mathcal{Proj}(V, w):= \max \quad & \bra{g} \left(\sum_{jk} w_{jk} h_{jk}  \right) \ket{g}&\\
\nonumber \mathrm{s.t.}\quad &\forall \text{ distinct }\,\, i, j, k, l\in [n]:\\
  &h_{ij}^2 = h_{ij}, \label{eq:singproj1}\\
  &h_{ij}h_{kl} = h_{kl} h_{ij}, \label{eq:singprojcomm}\\
  &h_{ij} h_{jk} +h_{jk} h_{ij}= \frac{1}{2}(h_{ij}+h_{jk} -h_{ik}),\label{eq:anticommproj}\\
  &h_{ij}^* =h_{ij},\label{eq:singproj2}\\
  &\braket{g|g}=1.\label{eq:singprojnormalize}
\end{align}
\end{definition}

\begin{definition}[$\mathcal{Perm}(V, w)$]  Given Hamiltonian $H=\sum_{ij} w_{ij} P_{ij}$ corresponding to graph $(V, w)$, define 
\begin{align}
\mathcal{Perm}(H)=\mathcal{Perm}(V, w):=\min \quad & \bra{g} \left(  \sum_{jk} w_{jk} p_{jk}  \right)\ket{g}&\\
\nonumber \mathrm{s.t.} \quad & \forall \text{ distinct }\,\, i, j, k, l\in [n]:\\
\label{eq:sym_const_1}&p_{ij}^2=\mathbb{I},\\
\label{eq:sym_const_2}&p_{ij} p_{kl}=p_{kl}p_{ij},\\
\label{eq:anti_comm}&p_{ij} p_{jk} +p_{jk} p_{ij} =p_{ij} +p_{jk} +p_{ik}-\mathbb{I},\\
\label{eq:herm_const} &p_{ij}^*=p_{ij},\\
&\braket{g|g}=1.
\end{align}
\end{definition}

It is easy to verify that $\mathcal{Proj}$ and $\mathcal{Perm}$ are equivalent in the sense that optimal objectives of $\mathcal{Perm}$ and $\mathcal{Proj}$ are affine shifts of one another: 
\begin{equation}
\mathcal{Perm} (V, w)= \sum_{jk \in E} w_{jk} -2 \mathcal{Proj} (V, w).
\end{equation}  This can be verified by observing that if $\{P_{jk}'\}$ is a feasible solution for $\mathcal{Perm}$ then $\{(\mathbb{I}- P_{jk}')/2\}$ is a feasible solution for $\mathcal{Proj}$ and that if $\{H_{jk}'\}$ is feasible for $\mathcal{Proj}$ then $\{\mathbb{I}-2 H_{jk}'\}$ is feasible for $\mathcal{Perm}$.  

The intuition behind the $\mathcal{Perm}$ constraints can most easily be understood in the context of the representation theory of the symmetric group.  It is well known that the symmetric group has a ``finite presentation''.  Loosely speaking this means that there is a finite set of generators such that any element of the symmetric group can be written as the product of generators, and any product of elements from the symmetric group can be inferred from some finite set of multiplication rules on those generators.  Using standard notation, the symmetric group $S_n$ is generated by transpositions $(i, j)$ subject to the following rules:
\begin{align}
    \nonumber \forall \text{ distinct }\,\, i, j, k, l\in [n]:\\
\label{eq:fin_pres_1}(i, j)^2&=1,\\
\label{eq:fin_pres_2}(i, j) (k, l)&=(k, l)(i, j),\\
\label{eq:fin_pres_3}(i, j) (j, k) (i, j)&=(j, k)(i, j)(j, k).
\end{align}
Since all multiplicative identities can be derived from these rules, if we have operators $p_{ij}$ which satisfy analogous relations then multiplication of products of monomials in $\{p_{ij}\}$ must behave exactly like products of transpositions.  Hence feasible solutions $\{p_{ij}'\}$ must correspond to a representation of the symmetric group (see Appendix \ref{sec:rep_theory}).
%Note that \cref{eq:sym_const_1} corresponds to \cref{eq:fin_pres_1}, and \cref{eq:sym_const_2} corresponds to \cref{eq:fin_pres_2}, but \cref{eq:fin_pres_3} has no corresponding equation. 
Note that there is a correspondence between \cref{eq:sym_const_1} and \cref{eq:fin_pres_1} as well as between \cref{eq:sym_const_2} and \cref{eq:fin_pres_2}, but apparently none for \cref{eq:fin_pres_3}.
Instead, the operator program $\mathcal{Perm}$ contains an additional ``anti-commuting constraint'' \cref{eq:anti_comm}.  $\mathcal{Perm}$ actually has an implicit constraint corresponding to \cref{eq:fin_pres_3} (\Cref{prop:add_const}), so the constraints present enforce that the operators $p_{ij}$ ``look like'' a finite presentation of the symmetric group, plus an additional anti-commuting constraint. 
%An understanding of 
This fact is crucial for understanding why the operator programs listed are accurately capturing the relevant local Hamiltonian problems (\cref{thm:perm_is_opt}).  \Cref{eq:sym_const_1,eq:sym_const_2,eq:anti_comm,eq:herm_const}
%\jnote{Just to confirm, this one also needs Eq 35 (AC) right?}\knote{Yes, thank you for checking.  Prop 3.7 explicitly uses Hermitianness of the variables.  You have a simpler proof int eh appendix though which doesnt use it?  }\jnote{Yes, I have a shorter proof with less assumptions used for a stronger result. I replaced the proof with that because I think it's simply better and it makes things easier in \cref{app:relationderivation}. I left the previous proof in the comments.} 
force the operators to correspond to a representation of the Symmetric group, and \cref{eq:anti_comm} further forces the operators to correspond to the correct representation for the Hamiltonian.  

\begin{proposition}\label{prop:add_const}
    For all distinct $i, j, k\in [n]$, $\mathcal{Perm}$ satisfies the additional constraint 
    \begin{equation}\label{eq:totalswap}
    p_{ij} p_{jk}p_{ij}
    %=p_{jk}p_{ij}p_{jk}
    =p_{ik},
    \end{equation}
    and $\mathcal{Proj}$ satisfies the additional constraint
    \begin{equation}\label{eq:quarterformula}
    4 h_{ij}h_{jk}h_{ij}=h_{ij}.
    %=4h_{jk}h_{ij}h_{jk}-h_{jk}=0.
    \end{equation}
\end{proposition}
\begin{proof}
    From the anticommutation relation \cref{eq:anticommproj}, we can expand $h_{jk}=h_{ij}+h_{ik}-2(h_{ij}h_{ik}+h_{ik}h_{ij})$ to obtain
    \begin{eqnarray}
    h_{ij}h_{jk}h_{ij}&=&
    h_{ij}\bigl(h_{ij}+h_{ik}-2(h_{ij}h_{ik}+h_{ik}h_{ij})\bigr)h_{ij}\\
        &=&h_{ij}-3h_{ij}h_{ik}h_{ij},
    \end{eqnarray}
    where we used \cref{eq:singprojnormalize} as well. 
    Repeating the same substitution for $h_{ik}$ in the second term gives 
$h_{ij}h_{jk}h_{ij}=h_{ij}-3\bigl(h_{ij}-3h_{ij}h_{jk}h_{ij}\bigr)$,  
    which results in \cref{eq:quarterformula} after solving the linear equation. To obtain \cref{eq:totalswap}, apply the same proof with $h_{ij}=(\mathbb{I}-p_{ij})/2$. 
\end{proof}

This additional constraint \cref{eq:quarterformula} is actually one of the basic relations in the Temperley-Lieb algebra \cite{tem71rel} describing the 1-dimensional Heisenberg chain and variants. The factor of 4 could be understood in relation with other algebraic structures used for analyzing the Heisenberg model \cite{san05gro,bea06som}. 
Furthermore, an immediate corollary of \cref{prop:add_const} is that 
\begin{equation}\label{eq:sym_const_3}
p_{ij}p_{jk}p_{ij}=p_{ik}=p_{jk}p_{ij}p_{jk},
\end{equation}giving the constraint corresponding to \cref{eq:fin_pres_3}.
This lets us prove our first main result. 

\begin{comment}
\begin{proof}
By \cref{eq:anti_comm},
\begin{align}\label{eq:removing_const}
    &\quad p_{ij} p_{jk}p_{ij}-p_{jk}p_{ij}p_{jk}\\
    \nonumber &=(-p_{jk}p_{ij}+p_{ij}+p_{jk}+p_{ik}-\mathbb{I}) p_{ij}-(-p_{ij} p_{jk}+p_{ij}+p_{jk}+p_{ik}-\mathbb{I})p_{jk}\\
    \nonumber &=(p_{ij}+p_{jk}+p_{ik}) p_{ij}-(p_{ij}+p_{jk}+p_{ik})p_{jk}.
\end{align}
By applying \cref{eq:anti_comm} to every term on the expanded R.H.S we may derive:
\begin{align}
    &\quad (p_{ij}+p_{jk}+p_{ik}) p_{ij}-(p_{ij}+p_{jk}+p_{ik})p_{jk}\\
    &=-p_{ij}(p_{ij}+p_{jk}+p_{ik}) +p_{jk}(p_{ij}+p_{jk}+p_{ik})\\
    \nonumber 
    &=-((p_{ij}+p_{jk}+p_{ik}) p_{ij}-(p_{ij}+p_{jk}+p_{ik})p_{jk})^*,
\end{align}
where the last equality follows from the Hermitian assumption on the variables $p_{ij}$.  It is easily seen that the L.H.S of \cref{eq:removing_const} is Hermitian, and we have demonstrated that the R.H.S of \cref{eq:removing_const} is skew-Hermitian hence they are both zero.  For the $\mathcal{Proj}$ constraint apply the same proof with $h_{ij}=(\mathbb{I}-p_{ij})/2$.
\end{proof}
\end{comment}


\begin{theorem}\label{thm:perm_is_opt}
$\mathcal{Perm}(V, w)=\SWAP(V, w)$.
\end{theorem}
\begin{proof}
See \Cref{sec:perm_conv}.
\end{proof}

\begin{corollary}
$\mathcal{Perm}$ is a {\sf QMA}-complete operator program.  
\end{corollary}
\begin{proof}
    Determining $\SWAP(G, w)$ is {\sf QMA}-complete \cite{pid17} and by \Cref{thm:perm_is_opt} finding $\SWAP(G, w)$ is equivalent to finding $\mathcal{Perm}(G, w)$.
\end{proof}


\Cref{thm:perm_is_opt} establishes that the optimal operator variables in $\mathcal{Perm}$ and actual quantum operators in $\SWAP$ are the same, or that $\mathcal{Perm}$ can be optimized using the fixed assignments $p_{ij}=P_{ij}$ WLOG. 
Similarly, the abstract operator variables in $\mathcal{Proj}$ denoted by $h_{ij}$ become indistinguishable from the actual quantum operators of {\scshape QMaxCut} denoted by $H_{ij}$ when they are optimized according to the program. 
This is somewhat surprising, since in general, there are infinite amount of nontrivial relations between singlet projectors $H_{ij}$ that include higher order terms. The theorem implies that all of them must be derivable purely from relations in the program, namely \crefrange{eq:singproj1}{eq:singproj2} for $h_{ij}$, and \crefrange{eq:sym_const_1}{eq:herm_const} for $p_{ij}$. 
Proposition \ref{prop:add_const} could be seen as one such derivation, and we show other examples in Appendix \ref{app:relationderivation}. 
Later in section \ref{sec:exact}, we will take advantage of this equivalence, and identify $h_{ij}$ and $H_{ij}$ for practical purposes. If we explicitly add the constraints \cref{eq:sym_const_3} to the program then the above conclusions hold even if only a single anti-commuting constraint is included.  So if we only enforced $p_{12}p_{23}+p_{23}p_{12}=p_{12}+p_{23}+p_{13}-\mathbb{I}$ and did not include any other anti-commuting constraints then we would still have $\SWAP(V, w)=\mathcal{Perm}(V, w)$, essentially because a single constraint rules out all the incorrect representations.


\subsection{The Hierarchy}\label{sec:hierarchy}
The NPA hierarchy \cite{pir10con} gives a general procedure for constructing a family of semidefinite optimization programs parameterized by ``level'' which provide increasingly accurate relaxations on operator programs $\mathcal{O}$ (\Cref{def:op_prog}).  The definition given in \cite{pir10con} is more general than the one presented here we have simplified it for our particular application.  We motivate the definition of the hierarchy by considering a ``moment matrix''.  Let $\{A_i\}$, $\ket{G}$ be some feasible solution to $\mathcal{O}$.  $M_\ell$ is defined to be a complex Hermitian matrix with rows/columns indexed by elements of $\Gamma_\ell :=\{a_{i_1} a_{i_2} ...a_{i_m}: m \leq \ell\}$, the set of monomials of degree $\leq \ell$.
Entries of $M_\ell$ are defined so that 
\begin{align}
M_\ell(a_{i_1} ... a_{i_m} , a_{j_1}...a_{j_r})&:=\braket{G | (A_{i_1}...A_{i_m})^* A_{j_1} ... A_{j_r} |G }\\
\nonumber &= \braket{G| A_{i_m} ... A_{i_1} A_{j_1} ... A_{j_r}| G}.
\end{align}
By definition $M_\ell$ gives consistent values for all monomials in $\Gamma_{2\ell}$, and we can unambiguously define 
\begin{align}\label{eq:val_def}
 M_\ell( a_{i_1} ... a_{i_r} ):=\begin{cases}
     M_{\ell} (\mathbb{I}, a_{i_1} ... a_{i_r}) \text{ if $r\leq \ell$},\\
     M_\ell(a_{i_\ell} ... a_{i_1}, a_{i_{\ell+1}} ... a_{i_r}) \text{ otherwise}.
 \end{cases}   
\end{align}
Similarly for any degree-$\ell$ nc polynomials $\beta(\{A_i\})=\sum_{\phi \in \Gamma_\ell} \beta_\phi \phi(\{A_i\})$ and $\gamma(\{A_i\})=\sum_{\phi \in \Gamma_\ell}\gamma_\phi \phi(\{A_i\})$,  with $ \beta_\phi, \gamma_\phi\in \mathbb{C}$ for all $\phi$, we define 
\begin{align}
    M_\ell(\beta, \gamma):=\sum_{\phi, \phi' \in \Gamma_\ell} {\beta_\phi}^* \gamma_{\phi'} \braket{G | \phi^*(\{A_i\}) \phi'(\{A_i\})|G} = \sum_{\phi, \phi' \in \Gamma_\ell} {\beta_\phi}^* \gamma_{\phi'} M_\ell(\phi, \phi') = \sum_{\phi, \phi' \in \Gamma_\ell} (\beta_\phi)^* \gamma_{\phi'} M_\ell(\phi^*\phi') .
\end{align}
\begin{comment}
For degree-$\ell$ polynomials in $\{A_i\}$ we may define $M_\ell(\beta, \gamma)=\sum_{\phi, \phi'} \beta_\phi^* \gamma_{\phi'} \braket{G | \phi^*(\{A_i\}) \phi'(\{A_i\})|G}$.  Further, for any polynomial $\beta$ of degree-$2\ell$ we can unambiguously define $M_\ell(\beta) =\sum_{\phi \in \Gamma_{2 \ell}} \beta_{\phi}\braket{G | \phi (\{A_i\})|G}$ (for any monomial in $\theta$ of degree $>\ell$ we can split it up into two operators of size $\leq \ell$ and evaluate that term from an entry of $M_\ell$).  It can be easily verified that $M_\ell$ satisfies the following constraints:
\end{comment}
Furthermore, since the polynomials $\{\eta_k\}$ corresponding to the constraints must evaluate to zero for the variables $\{A_i\}$, we must also have that $0=\eta_k(\{A_i\})=\bra{G} \eta_k(\{A_i\}) \ket{G}=M_\ell(\eta_k)$.  More generally, feasibility for $\mathcal{O}$ implies that for any nc polynomials $\beta, \gamma$ with $\deg(\beta\eta_k\gamma)\leq 2 \ell$, 
\begin{equation}\label{eq:npa_const_0}
    M_\ell(\beta \eta_k \gamma)=0, 
\end{equation}
since $\bra{G}\beta \eta_k \gamma \ket{G}=\bra{G}\beta 0 \gamma \ket{G}$. The matrix $M_{\ell}$ also naturally satisfies 
\begin{align}\label{eq:npa_const_M}
    M_\ell=M_{\ell}^*  \text{ ~~~and~~~ }
    M_\ell \succeq 0,
\end{align}
where the latter constraint can be seen from the fact that for any complex column vector $v$, $v^* M_\ell v=M_{\ell} (\beta, \beta)=\bra{G} \beta^* \beta \ket{G}\geq 0$ for some $\beta$ that is a degree-$\ell$ nc polynomial.  

\begin{comment}
\begin{align}
M_\ell(\phi, \phi')&=M_\ell(\gamma, \gamma') \text{ if } \phi, \phi', \gamma, \gamma' \in \Gamma_\ell \\ \nonumber &\text{ and } \phi^* \phi' = \gamma^* \gamma',\\
M_\ell &=M_\ell^*, \\
M_\ell(\beta, \beta) \geq 0 \,\forall\,\,\, \text{degree-$\ell$ }&\beta \Leftrightarrow M_\ell \succeq 0,\\
\label{op_prog_const_p_i} M_\ell(\beta \eta_i \gamma)&=0 \text{ if } deg(\beta)+deg(\gamma)+deg(\eta_i) \leq 2\ell.
\end{align}

\begin{align}
\label{eq:mom_consistency}M_\ell(\phi, \phi')&=M_\ell(\phi^* \phi') \,\, \forall \,\, \phi, \phi'\in \Gamma_\ell,\\
M_\ell &=M_\ell^*, \\
M_\ell(\beta, \beta) \geq 0 \,\forall\,\,\, \text{degree-$\ell$ }&\beta \Leftrightarrow M_\ell \succeq 0,\\
\label{op_prog_const_p_i} M_\ell(\beta \eta_i \gamma)&=0 \text{ if } deg(\beta)+deg(\gamma)+deg(\eta_i) \leq 2\ell.
\end{align}
In the first set of constraints, \cref{eq:mom_consistency}, $M_\ell(\phi^* \phi')$ is meant to 

Note that $\eta_i$ are the nc polynomial constraints defined in \Cref{def:op_prog}.  \Cref{op_prog_const_p_i} applies to all polynomials $\beta$, $\gamma$ of sufficiently low degree, however linearity implies all constraints of the form \Cref{op_prog_const_p_i} can be simultaneously enforced simply by enforcing them on $\beta, \gamma\in \Gamma_{2\ell}$.  
\end{comment}

%Given an operator program $\mathcal{O}$, the $\ell$-th level of the NPA hierarchy, $NPA_\ell (\mathcal{O})$, is simply optimization of the variable $M_\ell$ subject to the above constraints without reference to a legitimate assignment to the variables.  
Given an operator program $\mathcal{O}$, the $\ell$-th level of the NPA hierarchy, denoted by $NPA_\ell (\mathcal{O})$, is the optimization of the matrix $M_\ell$ as a variable, subject to the above properties viewed as constraints, not requiring a corresponding legitimate vector $\ket{G}$ or operators $\{A_i\}$. $NPA_\ell (\mathcal{O})$ relaxes $\mathcal{O}$ since we could have used the optimal $\ket{G}, \{A_i\}$ to define $M_\ell$, so if $\mathcal{O}$ is a maximization problem, $NPA_\ell(\mathcal{O})\geq \mathcal{O}$. 
For a given monomial $a_{i_1}...a_{i_r}$, $M_\ell(a_{i_1}...a_{i_r})$ is defined as a specific entry of $M_\ell$ according to \cref{eq:val_def}. 
However, there are many other distinct entries of $M_\ell$ which will match. For example, $M_2(a_1a_2):= M_2(\mathbb{I}, a_1 a_2)$, but $ M_2(\mathbb{I}, a_1 a_2)=M_2(a_1, a_2)=M_2(a_2 a_1, \mathbb{I})$.  In $NPA_\ell$ we will force the value of $M_\ell(a_{i_1}...a_{i_r})$ to be consistent with the other distinct entires by enforcing that these other entries of $M_\ell$ are equal to the ``cannonical'' value $M_2(a_1a_2):= M_2(\mathbb{I}, a_1 a_2)$.  Note that $M_\ell$ will be {\it indexed} by variables $\phi \in \Gamma_\ell$ of the original operator program, but these $\phi$ do not vary inside $NPA_\ell$. 
\begin{definition}[$NPA_\ell (\mathcal{O})$]
%Given an operator program $\mathcal{O}$ with objective to $\max/\min $ the extremal $\max/\min$ eigenvalue of $\theta\in \text{span}(\Gamma_{2 \ell})$ subject to constraints $\{\eta_i\}$ 
Given an operator program $\mathcal{O}$ with objective to max / min the extremal eigenvalue of a nc polynomial $\theta\in \text{span}(\Gamma_{2 \ell})$ satsifying $\theta^*=\theta$ and subject to constraints $\{\eta_k\}$, define
\begin{comment}
\begin{align}
NPA_\ell(\mathcal{O}):= \max /\min \,\,&M_\ell(q)\\
\nonumber s.t. \\
\label{eq:id_const}&M_\ell(\mathbb{I})=1\\
\label{concat_const}&M_\ell(\phi, \phi')=M_\ell(\gamma, \gamma') \text{ if } \phi, \phi', \gamma, \gamma' \in \Gamma_\ell \\ \nonumber &\text{ and } \phi^* \phi' = \gamma^* \gamma'\\
\label{npa_const_p_i} &M_\ell (\beta \eta_i \gamma)=0 \,\,\,\,\forall \beta, \gamma \in \Gamma_{2 \ell}, \eta_i: deg(\eta_i )+deg(\beta)+deg(\gamma) \leq 2\ell\\
&M_\ell\succeq 0, Hermitian
\end{align}
\end{comment}
\begin{align}
NPA_\ell(\mathcal{O}):= \max /\min \quad & M_\ell(\theta)\\
\label{eq:id_const}\mathrm{s.t.}\quad & M_\ell(\mathbb{I})=1,\\
\label{concat_const}&M_\ell(\phi, \phi')=M_\ell(\phi^* \phi') ~~~~ \forall  \phi, \phi'\in \Gamma_\ell,\\
%\label{npa_const_p_i} &M_\ell (\beta \eta_k \gamma)=0 ~~~~ \forall \beta, \gamma \in \Gamma_{2 \ell}, \eta_k: \deg(\eta_k )+\deg(\beta)+\deg(\gamma) \leq 2\ell,\\
\label{npa_const_p_i} &M_\ell (\beta \eta_k \gamma)=0 ~~~~ \forall \beta, \gamma \in \Gamma_{2 \ell}, \eta_k: \deg(\beta\eta_k\gamma)\leq 2 \ell,\\
&M_\ell\succeq 0, ~\mathrm{Hermitian}.
\end{align}
\end{definition}
\begin{comment}
It is important to note that if a constraint polynomial has $deg(\eta_i) > 2 \ell$ then it is \textbf{not} included in $NPA_\ell$ (we ignore that constraint for that level).  
\end{comment}
In several works \cite{par21app, ans20bey, par22opt} a ``real version'' of $NPA_\ell$ is studied, $NPA_\ell^{\mathbb{R}}$.  The key difference is that $NPA_\ell^{\mathbb{R}}$ takes only the real portion of the moment matrix, effectively ``zeroing out'' skew-Hermitian polynomials.
\begin{definition}[$NPA_\ell^{\mathbb{R}} (\mathcal{O})$]
Given an operator program $\mathcal{O}$ with objective to max / min the extremal eigenvalue of a nc polynomial $\theta\in \text{span}(\Gamma_{2 \ell})$ satsifying $\theta^*=\theta$ and subject to constraints $\{\eta_k\}$, define\begin{align}
NPA_\ell^{\mathbb{R}} (\mathcal{O}):= \max /\min \quad &M_\ell^{\mathbb{R}}(\theta)\\
\mathrm{s.t.} \quad & M_\ell^{\mathbb{R}} (\mathbb{I})=1,\\
&M_\ell^{\mathbb{R}} (\phi, \phi')=M_\ell^{\mathbb{R}} (\phi^* \phi') ~~~~ \forall  \phi, \phi'\in \Gamma_\ell,\\
&\label{npa_const_p_i_real}M_\ell^{\mathbb{R}} ((\beta \eta_k \gamma + \gamma^* \eta_k^* \beta^*)/2)=0 ~~~~ \forall \beta, \gamma \in \Gamma_{2 \ell}, \eta_k: \deg(\beta\eta_k\gamma )\leq 2\ell,\\
%&\label{npa_const_p_i_real}M_\ell^{\mathbb{R}} ((\beta \eta_k \gamma + \gamma^* \eta_k^* \beta^*)/2)=0 ~~~~ \forall \beta, \gamma \in \Gamma_{2 \ell}, \eta_k: \deg(\eta_k )+\deg(\beta)+\deg(\gamma) \leq 2\ell,\\
%&M_\ell ~~\mathrm{Hermitian},\\ 
%&M_\ell^{\mathbb{R}}=\frac{1}{2}(M_\ell+(M_\ell^*)^T),\\
&M_\ell^\mathbb{R} \succeq 0, ~\mathrm{Symmetric}.
\end{align}
\end{definition}
As noted in other works \cite{bra19, gha19alm}, $ NPA_\ell^\mathbb{R}(\mathcal{O})$ is a relaxation on $NPA_\ell(\mathcal{O})$ ($NPA_\ell(\mathcal{O}) \leq NPA_\ell^\mathbb{R}(\mathcal{O})$ if $\mathcal{O}$ is a maximization problem) since given a feasible $M_\ell$ for $NPA_\ell(\mathcal{O})$, we can take $M_\ell^\mathbb{R}:=(M_\ell+(M_\ell^*)^T)/2$ to obtain a feasible solution to $NPA_\ell^\mathbb{R}$ with the same objective, but not necessarily the other way around. 
%$NPA_\ell^\mathbb{R}(\mathcal{O})$ is a relaxation on $NPA_\ell(\mathcal{O})$, so it generally upper/lower bounds $NPA_\ell(\mathcal{O})$ for max/min problems. 
We show explicit examples in \cref{subsubsec:smallstat} where the separation is strict for the $\mathcal{Pauli}$ case. 
%However, for the specific operator program we focus on most in this paper $\mathcal{Proj}$, this is a distinction without a difference:
However, for the specific SDP we focus on most in this paper, $NPA_1(\mathcal{Proj})$, this is a distinction without a difference:
\begin{proposition}
    For any weighted graph $(V, w)$, $NPA_1(\mathcal{Proj}(V, w))=NPA_1^\mathbb{R}(\mathcal{Proj}(V, w))$.
\end{proposition}
\begin{proof}
     Let $M_1^\mathbb{R}$ be a feasible solution to $NPA_1^\mathbb{R}(\mathcal{Proj})$.  We claim that $M_1^\mathbb{R}$ is feasible for $NPA_1(\mathcal{Proj})$ and hence $NPA_1^\mathbb{R}(\mathcal{Proj}) \leq NPA_1(\mathcal{Proj})$.  This is demonstrated by showing that all the constraints $\eta$ enforced on $M_1^\mathbb{R}$ (constraints of the form \Cref{npa_const_p_i_real}) are Hermitian and hence $M_1^\mathbb{R}(\eta+\eta^*)=0$ implies $M_1^\mathbb{R}(\eta)=0$.  It is easy to see that the nc polynomials $\eta$ of the form $h_{ij}^2-h_{ij}$ and $h_{ij}h_{jk}+h_{jk}h_{ij}-(h_{ij}+h_{jk}-h_{ik})/2$ are Hermitian, hence $M_1^\mathbb{R}(\eta)=0$ for these polynomials.  Since we are looking at moment matrices with a maximum degree $2$ if $\beta \eta \gamma$ has max degree $2$ for $\eta$ one of the polynomials above $\beta=\mathbb{I}=\gamma$ so there are no other constraints to check and $M_1^\mathbb{R}$ must be feasible for $NPA_1(\mathcal{Proj})$
     
     %This is shown by demonstrating that all the constraints enforced on $M_1^\mathbb{R}$ are actually Hermitian polynomials, The constraint $\eta=h_{ij}-h_{ij}^2=0$ is Hermitian ($\eta^*=\eta$) and we know $M_1^\mathbb{R}(\eta+\eta^*)=0$ by the constraints of $NPA_1^\mathbb{R}$ so $M_1^\mathbb{R}(\eta)=0$. Since $M_1$ is Hermitian and by the constraints $M_1(h_{ij})=M_1(h_{ij}^2)=M_1(h_{ij}, h_{ij})\in \mathbb{R}$, $M_1^\mathbb{R}(h_{ij}^2)=M_1^\mathbb{R}(h_{ij})$ for all distinct $i, j$.  Similarly, $M_1(h_{ij}, h_{kl})=M_1(h_{kl}, h_{ij})$ by the constraints, and since $M_1$ is Hermitian $M_1(h_{ij}, h_{kl})\in \mathbb{R}$, $M_1^\mathbb{R}(h_{ij}, h_{kl})=M_1^\mathbb{R}(h_{kl}, h_{ij})$ for all distinct $i, j, k, l$.  Lastly, 
    %\begin{equation}
    %        M_1(h_{ij}, h_{jk})+M_1(h_{jk}, h_{ij})=\frac{1}{2}(M_1(h_{ij})+M_1(h_{jk})-M_1(h_{ik})).
    %\end{equation}
    %$M_1$ being Hermitian implies that the right hand side is strictly real, so the L.H.S must also be strictly real.  Hence the above also holds with $M_1^\mathbb{R}$ in place of $M_1$.  We have demonsrataed that $M_1^\mathbb{R}$ satisfies all the constraints of $NPA_1(\mathcal{Proj})$.    
\end{proof}


Each $NPA_\ell$ can be solved via semidefinite programming, and the dual set of programs is called the {\it Sum of Squares} (SoS) hierarchy due to the interpretation of the hierarchy as an optimization over {\it sum of squares proofs}.  To motivate this imagine we are trying to maximize an nc polynomial $\theta$ and obtain an expression of the form
\begin{equation}\label{eq:sos_mot}
    \lambda \mathbb{I}-\theta=\sum_i \psi_i^* \psi_i+\sum_j \beta_j \eta_j \gamma_j,
\end{equation}
where $\lambda\in \mathbb{R}$, $\psi_i\in \mathrm{span}_\mathbb{C}(\Gamma_\ell)$ for all $i$ and $\deg(\beta_j\eta_j \gamma_j)\leq 2 \ell$ for all $j$.  Since $\eta_k$ are constraints of $\mathcal{O}$ we must have that 
\begin{equation}
    \lambda \mathbb{I}-\theta=\sum_i \psi_i^* \psi_i 
\end{equation}
for {\it any} feasible solution to $\mathcal{O}$.  $\sum_i \psi^* \psi_i$ is manifestly $\succeq 0$ so for all feasible solutions $\bra{G} \lambda \mathbb{I}-\theta\ket{G} \geq 0\Rightarrow \lambda \geq \bra{G} \theta \ket{G}$ which implies $\lambda \geq NPA_\ell(\mathcal{O})$.  An expression of the form \cref{eq:sos_mot} is generally referred to as a {\it sum of squares proof}.  The SoS optimization problem at level $\ell$ is to find the smallest $\lambda$ such that $\lambda \mathbb{I}-\theta$ can be deformed via the constraints to an expression of the form $\sum_i \psi^* \psi$: $\lambda \mathbb{I}-\theta-\sum_j \beta_j \eta_j \gamma_j =\sum_i \psi_i^* \psi_i$.  Crucially, the constraints that the SoS proof uses are of degree at most $\ell$ at the corresponding level ($\deg(\beta_j\eta_j\gamma_j)\leq 2\ell$).  For ease of reference, define the set of constraint deformation polynomials as
\begin{equation}
    U^\ell(\{\eta_k\}):= \mathrm{span}_\mathbb{C}\{\beta \eta_k \gamma: \beta, \gamma\in \Gamma_\ell, \deg(\beta\eta_k\gamma)\leq 2\ell\}
\end{equation}


\begin{comment}
In the interest of providing an explicit SDP for which the dual can be easily verified we will define a number of matrices.  These will be all be ``constraint matrices'' used to enforce  \Cref{eq:id_const}-\Cref{npa_const_p_i}, so they will all have rows and columns indexed by $\Gamma_\ell$.  For ease of reference let $U_1^\ell:=\{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')\in \Gamma_\ell \times \Gamma_\ell \times\Gamma_\ell \times \Gamma_\ell: \mathbf{\Phi}^* \mathbf{\Phi}'=\mathbf{\Theta}^* \mathbf{\Theta}'\}$, and let $U_2:= \{ p\in span(\Gamma_{2\ell}): \,\,p=r p_i s \text{ with } r, s\in \Gamma_{2\ell}  \text{ and } deg(s)+deg(r)+deg(p_i) \leq 2\ell\}$.  Let
\begin{equation}\label{eq:b_I}
    B_\mathbb{I}(\mathbf{\Phi}, \mathbf{\Phi}')=\begin{cases}
    1 \text{  if } \mathbf{\Phi}=\mathbb{I}=\mathbf{\Phi}'\\
    0 \text{   otherwise}
    \end{cases}.
\end{equation}
For all $(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')\in U_1$ define $C_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}, C_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}'$\footnote{We will require matrices $C_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}$ and  $C_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}'$ to enforce \Cref{concat_const} using only \textbf{Hermitian} constraint matrices .} according to:
\begin{equation}
    C_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}(\mathbf{\Psi}, \mathbf{\Psi}')=\begin{cases}
    1 \text{  if  $\mathbf{\Psi}=\mathbf{\Phi} $ and $\mathbf{\Psi}'=\mathbf{\Phi}'$}\\
    1 \text{  if  $\mathbf{\Psi}=\mathbf{\Phi}' $ and $\mathbf{\Psi}'=\mathbf{\Phi}$}\\
    -1 \text{  if  $\mathbf{\Psi}=\mathbf{\Theta} $ and $\mathbf{\Psi}'=\mathbf{\Theta}'$}\\
    -1 \text{  if  $\mathbf{\Psi}=\mathbf{\Theta}' $ and $\mathbf{\Psi}'=\mathbf{\Theta}$}\\
    0 \text{  otherwise}\\
    \end{cases},
\end{equation}
and 
\begin{equation}
    C_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}'(\mathbf{\Psi}, \mathbf{\Psi}')=\begin{cases}
    i \text{  if  $\mathbf{\Psi}=\mathbf{\Phi} $ and $\mathbf{\Psi}'=\mathbf{\Phi}'$}\\
    -i \text{  if  $\mathbf{\Psi}=\mathbf{\Phi}' $ and $\mathbf{\Psi}'=\mathbf{\Phi}$}\\
    -i \text{  if  $\mathbf{\Psi}=\mathbf{\Theta} $ and $\mathbf{\Psi}'=\mathbf{\Theta}'$}\\
    i \text{  if  $\mathbf{\Psi}=\mathbf{\Theta}' $ and $\mathbf{\Psi}'=\mathbf{\Theta}$}\\
    0 \text{  otherwise}\\
    \end{cases}.
\end{equation}
For all $p\in U_2$ define $D_p, D_p'$ as follows:  Let $p=\sum_{\mathbf{\Phi}\in \Gamma_{2\ell}}c_\mathbf{\Phi} \mathbf{\Phi}$ for $c_\mathbf{\Phi} \in \mathbb{C}$ for all $\mathbf{\Phi}$.  For all $\mathbf{\Phi} \in \Gamma_{2\ell}$ choose a pair $(\mathbf{\Theta}_\mathbf{\Phi}, \mathbf{\Theta}_{\mathbf{\Phi}}')\in \Gamma_\ell \times \Gamma_\ell$ such that $\mathbf{\Theta}_\mathbf{\Phi}^* \mathbf{\Theta}_\mathbf{\Phi}'=$ Then,
\begin{equation}
    D_p(\mathbf{\Psi}, \mathbf{\Psi}')=\begin{cases}
        c_\mathbf{\Phi} \text{  if $\mathbf{\Psi}=\mathbf{\Theta}_\mathbf{\Phi}$ and $\mathbf{\Psi}'=\mathbf{\Theta}_\mathbf{\Phi}'$}\\
        c_\mathbf{\Phi}^* \text{  if $\mathbf{\Psi}=\mathbf{\Theta}_\mathbf{\Phi}'$ and $\mathbf{\Psi}'=\mathbf{\Theta}_\mathbf{\Phi}$}\\
        0 \text{  otherwise}
    \end{cases},
\end{equation}
and 
\begin{equation}\label{eq:dp_p}
    D_p(\mathbf{\Psi}, \mathbf{\Psi}')'=\begin{cases}
        i c_\mathbf{\Phi} \text{  if $\mathbf{\Psi}=\mathbf{\Theta}_\mathbf{\Phi}$ and $\mathbf{\Psi}'=\mathbf{\Theta}_\mathbf{\Phi}'$}\\
        -i c_\mathbf{\Phi}^* \text{  if $\mathbf{\Psi}=\mathbf{\Theta}_\mathbf{\Phi}'$ and $\mathbf{\Psi}'=\mathbf{\Theta}_\mathbf{\Phi}$}\\
        0 \text{  otherwise}
    \end{cases}.
\end{equation}
Using these matrices and denoting $A\cdot B:= Tr(A^* B)$, $NPA_\ell$ can be succinctly written as:
\begin{align}
\label{eq:npa_as_sdp} NPA_\ell(\mathcal{O})= \max /\min \,\,&\mathbf{M}_\ell \cdot \frac{1}{2} D_q\\
\nonumber s.t. \\
 \label{eq:sdp_const1}&B_\mathbb{I} \cdot \mathbf{M}_\ell =1\\
 \label{eq:sdp_const2}&C_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')} \cdot \mathbf{M}_\ell=0 \text{  for all $(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')\in U_1$}\\
 \label{eq:sdp_const3}&C_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}' \cdot \mathbf{M}_\ell=0 \text{  for all $(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')\in U_1$}\\
 \label{eq:sdp_const4}&D_p \cdot \mathbf{M}_\ell =0 \text{ for all $p \in U_2$}\\
 \label{eq:sdp_const5}&D_p' \cdot \mathbf{M}_\ell =0 \text{ for all $p\in U_2$}\\ 
\nonumber & \mathbf{M}_\ell\succeq 0, Hermitian
\end{align}
Note that \Cref{eq:sdp_const1} enforces \Cref{eq:id_const}, \Cref{eq:sdp_const2}-\Cref{eq:sdp_const3} enforce \Cref{concat_const}, and \Cref{eq:sdp_const4}-\Cref{eq:sdp_const5} enforce \Cref{npa_const_p_i}.  Assuming $\mathcal{O}$ is a maximization problem, the dual of \Cref{eq:npa_as_sdp} is 
\begin{gather}
    \min \boldsymbol{\lambda}\\
    \nonumber s.t.\\
    \boldsymbol{\lambda} B_\mathbb{I}+\sum_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')\in U_1} \mathbf{y}_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')} C_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}+\mathbf{y}_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}' C_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}'+\sum_{p \in U_2} \mathbf{z}_p D_p +\mathbf{z}_p' D_p' \succeq \frac{1}{2} D_q \\
    \mathbf{y}_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}, \mathbf{y}_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}', \mathbf{z}_p, \mathbf{z}_p' \in \mathbb{R} \,\, \forall \,\,(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}') \in U_1, \,\, p \in U_2,
\end{gather}
which is equivalent to:
\begin{gather}
    \min \boldsymbol{\lambda}\\
    \nonumber s.t.\\
    \boldsymbol{\lambda} B_\mathbb{I}- \frac{1}{2}D_q = \mathbf{S}+\mathbf{T}\\
    \mathbf{T}=\sum_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')\in U_1} \mathbf{y}_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')} C_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}+\mathbf{y}_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}' C_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}'+\sum_{p \in U_2} \mathbf{z}_p D_p +\mathbf{z}_p' D_p'\\
    \mathbf{y}_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}, \mathbf{y}_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}', \mathbf{z}_p, \mathbf{z}_p' \in \mathbb{R} \,\, \forall \,\,(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}') \in U_1, \,\, p \in U_2\\
    \mathbf{S}\succeq 0, \,\,Hermitian.
\end{gather}

The above program can be interpreted as trying to find the smallest $\boldsymbol{\lambda}$ such that $\boldsymbol{\lambda} B_\mathbb{I}- \frac{1}{2}D_q$ may be deformed to a PSD matrix $\mathbf{S}$ using constraint matrices of $NPA_\ell(\mathcal{O})$.  $\mathbf{T}$ is such a ``constraint deformation'' matrix.  We will further simplify the above to make the interpretation more clear.  Let $(\lambda, S, T)$ be a feasible solution.  $S$ is PSD so it has a decomposition $S=\sum_i v_i^* v_i$ where $v_i$ are unnormalized complex row vectors.  Let $\mathbf{w}$ be a vector of variables indexed by elements of $\Gamma_\ell$ and define $s_i=v_i \cdot{} \mathbf{w}=\sum_{\mathbf{\Phi} \in \Gamma_\ell} (v_i)_{\mathbf{\Phi}} \,\, \mathbf{\Phi}$.  We can evaluate\knote{May want a notation which makes it clear polynomials are a function of the variables but are not variate themselves.  Also double check the below, could be off by a factor of $2$}:
\begin{align}
    \mathbf{w}^* \left( \lambda B_\mathbb{I}- \frac{1}{2}D_q \right) \mathbf{w}=\lambda \mathbb{I}-q(\{\mathbf{A}_i\})\\
    \nonumber \text{and}\\
    \mathbf{w}^* \left( S+T\right) \mathbf{w}=\sum_i s_i^* s_i+\sum_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')\in U_1}y_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')} (\mathbf{\Phi}^* \mathbf{\Phi}'-\mathbf{\Theta}^* \mathbf{\Theta}'+(\mathbf{\Phi}')^*\mathbf{\Phi} -(\mathbf{\Theta}')^* \mathbf{\Theta})\\
     \nonumber + y_{(\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')}' i(\mathbf{\Phi}^* \mathbf{\Phi}'- \mathbf{\Theta}^* \mathbf{\Theta}'+(\mathbf{\Theta}')^* \mathbf{\Theta}- (\mathbf{\Phi}')^* \mathbf{\Phi})+\sum_{p\in U_2}z_p (p+p^*)+z_p i (p-p^*).
\end{align}
Note that for any feasible solution to $\mathcal{O}$ all of the terms in $\mathbf{w}^* T \mathbf{w}$ will evaluate to zero, so a feasible solution to $NPA_\ell$ provides a way to write $\lambda \mathbb{I}-q=\sum_i s_i^* s_i$ where equality is defined modulo the constraints.  Since $\sum_i s_i^* s_i$ will always be PSD this provides an algebraic proof that $\lambda \geq \mu_{max}(q)$ independent of the values that the variables $\{\mathbf{A}_i\}$ take.  The set of duals to programs in $NPA_\ell$ is generally referred to as a ``Sum of Squares'' hierarhcy for this reason.  It may be interpreted as looking for an algebraic deformation of $\lambda \mathbb{I}- q$ to a sum of squares via the constraints which is then guaranteed to be $\succeq 0$.  Let us define $U^\ell$ as the real span of all terms of the form 
\begin{align}
   (\mathbf{\Phi}^* \mathbf{\Phi}'-\mathbf{\Theta}^* \mathbf{\Theta}'+(\mathbf{\Phi}')^*\mathbf{\Phi} -(\mathbf{\Theta}')^* \mathbf{\Theta}) \text{ for }  (\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')\in U_1^\ell,\\
   i(\mathbf{\Phi}^* \mathbf{\Phi}'- \mathbf{\Theta}^* \mathbf{\Theta}'+(\mathbf{\Theta}')^* \mathbf{\Theta}- (\mathbf{\Phi}')^* \mathbf{\Phi}) \text{ for }  (\mathbf{\Phi}, \mathbf{\Phi}', \mathbf{\Theta}, \mathbf{\Theta}')\in U_1^\ell,\\
   (p+p^*) \text{ for } p \in U_2^\ell, \\
   \text{ and } i(p-p^*) \text{ for } p \in U_2^\ell.
\end{align}


Each $NPA_\ell(\mathcal{O})$ may be expressed as a semidefinite program (SDP), so we may construct a dual hierarchy where each level corresponds to the SDP dual of $NPA_\ell(\mathcal{O})$.  This ``dual hierarchy'' is generally referred to as the ``Sum of Squares'' (SoS) hierarchy and can be interpreted as searching for a degree$-\ell$ nc polynaomil in $span(\Gamma_\ell)$ which provides an explicit (rigorous) bound on the objective of the polynomial $q$.  Each level in the dual hierarchy will use constraints present at that level to write $\boldsymbol{\lambda} \mathbb{I}-q$ as a nc polynomial of the form $\sum_i s_i^* s_i+ t$ for $s_i \in span(\Gamma_\ell)$ for all $i$ and $t$ a linear combination of constraints from $NPA_\ell(\mathcal{O})$.  Since $t$ will be zero for any feasible solution and $\sum_i s_i^* s_i \succeq 0$, this implies that $\boldsymbol{\lambda} \geq \mu_{max}(q)$.  Define the following lists of operators:
\begin{align}
    U_1^\ell=[\mathbf{\Phi}^* \mathbf{\Phi}'-\mathbf{\Theta}^* \mathbf{\Theta}'+(\mathbf{\Phi}')^* \mathbf{\Phi}-(\mathbf{\Theta}')^* \mathbf{\Theta}: \mathbf{\Phi}, \mathbf{\Phi'}, \mathbf{\Theta}, \mathbf{\Theta}' \in \Gamma_\ell \text{ with } \mathbf{\Phi}^* \mathbf{\Phi}'=\mathbf{\Theta}^* \mathbf{\Theta}']\\
    U_2^\ell=[i(\mathbf{\Phi}^* \mathbf{\Phi}'-\mathbf{\Theta}^* \mathbf{\Theta}'-(\mathbf{\Phi}')^* \mathbf{\Phi}+(\mathbf{\Theta}')^* \mathbf{\Theta}): \mathbf{\Phi}, \mathbf{\Phi'}, \mathbf{\Theta}, \mathbf{\Theta}' \in \Gamma_\ell \text{ with } \mathbf{\Phi}^* \mathbf{\Phi}'=\mathbf{\Theta}^* \mathbf{\Theta}']\\
    U_3^\ell=[r p_j +(r p_j)^*: r\in \Gamma_{2 \ell},\,\, p_j \text{ is a constraint of $\mathcal{O}$}, \,\,deg(r)+deg(p_j)\leq 2\ell]\\
    U_4^\ell=[i(r p_j -(r p_j)^*): r\in \Gamma_{2 \ell},\,\, p_j \text{ is a constraint of $\mathcal{O}$}, \,\,deg(r)+deg(p_j)\leq 2\ell]
\end{align}
\knote{I dont know howq to say waht I want, but it's technically important that these be lists rather than sets.}  Let us further define $U^\ell=span_\mathbb{R}(U_1^\ell \cup U_2^\ell \cup U_3^\ell \cup U_4^\ell)$ as the set of nc polynomials corresponding to all real linear combinations of the constraints of $\mathcal{O}$.
\end{comment}


\begin{definition}[$SoS_\ell(\mathcal{O})$]\label{def:SoS}
Given an operator program $\mathcal{O}$ with objective to max/min the extremal eigenvalue of $\theta$ subject to the constraints $\{\eta_k\}$,
\begin{align}
    SoS_\ell(\mathcal{O}):= \min/\max \quad & \lambda\\
    \mathrm{s.t.}\quad & \sum_i \psi_i^* \psi_i +\beta =\begin{cases} \lambda \mathbb{I}-\theta \text{ if $\mathcal{O}$ is a maximization problem}\\ \theta-\lambda \mathbb{I} \text{ if $\mathcal{O}$ is a minimization problem} \end{cases},\\
    &\psi_i\in \mathrm{span}_\mathbb{C}(\Gamma_\ell) \,\, \forall i,\\
    &\beta \in U^\ell(\{\eta_k\}),\\
    &\lambda \in \mathbb{R}.
\end{align}
\end{definition}

\textbf{Equivalence}.  SDPs for $\mathcal{Perm}$ and $\mathcal{Proj}$ are equivalent since a feasible solution for $NPA_\ell(\mathcal{Perm})$ can be used to construct a feasible solution to $NPA_\ell(\mathcal{Proj})$ and vice versa.  The reasoning for this is analogous to the reasoning behind the operator programs $\mathcal{Perm}$ and $\mathcal{Proj}$ being equivalent: Given $M_\ell$ feasible for $NPA_\ell$ one can construct $M_\ell'$ feasible for $NPA_\ell(\mathcal{Proj})$ according to
$$
%M_\ell'(h_{i_1, j_1}  ... h_{i_a, j_a}, h_{k_1, m_1} ... h_{k_b, m_b}):= M_\ell\left(\frac{\mathbb{I}+p_{i_1, j_1}}{2}...\frac{\mathbb{I}+p_{i_a, j_a}}{2}, \frac{\mathbb{I}+p_{k_1, m_1}}{2} ... \frac{\mathbb{I}+p_{k_b, m_b}}{2} \right).
M_\ell'(h_{i j}  ... h_{k l}, h_{m n} ... h_{o p}):= M_\ell\left(\frac{\mathbb{I}+p_{i j}}{2}...\frac{\mathbb{I}+p_{k l}}{2}, \frac{\mathbb{I}+p_{m n}}{2} ... \frac{\mathbb{I}+p_{o p}}{2} \right).
$$
Note that the R.H.S is evaluated using the expression for $M_\ell(\beta, \gamma)$ for nc polynomials $\beta$ and $\gamma$ as in \cref{sec:hierarchy}.  The primal/dual pair $NPA_\ell/SoS_\ell$ satisfies strong duality, so it holds that $NPA_\ell(\mathcal{Perm})=SoS_\ell(\mathcal{Perm})$, $NPA_\ell(\mathcal{Proj})=SoS_\ell(\mathcal{Proj})$, and that the objectives of the two sets of programs differ by a known affine shift.  

\textbf{Convergence.  }It is simple to check that any $\mathcal{Perm}$ satisfies a boundedness condition (it is {\it Archimedean} \cite{pir10con}).  For this we need to show the existence of a constant $C$ such that $C^2 \mathbb{I}-\sum_{i<j} P_{ij}\succeq 0$ for any feasible assignment to the variables $p_{ij}=P_{ij}$.  Since $P_{ij}^2=\mathbb{I}$, $P_{ij} \preceq \mathbb{I}$ so $C^2 \mathbb{I}-\sum_{i<j} P_{ij}\succeq \left(C^2-\binom{n}{2}\right) \mathbb{I}$.  Hence we may choose $C=\sqrt{\binom{n}{2}}$.  The results of \cite{pir10con} then imply that $NPA_{\ell}(\mathcal{Perm})$ converges to the optimal objective of $\mathcal{Perm}$ in the limit of large $\ell$.  In fact, the constraints present are strong enough to guarantee finite convergence of $NPA_\ell$.  Essentially the proof of this statement involves showing that moment matrices will satisfy the ``rank condition'' of \cite{pir10con} hence an optimal operator solution can be constructed from the optimal solution to $NPA_\ell(\mathcal{Perm})$ at some finite $\ell$.
\begin{proposition}\label{prop:fin_conv}
    Let $\ell^*=\binom{n}{2}$, then $NPA_{\ell^*}(\mathcal{Perm}(V, w)) = NPA_{\ell^*+1}(\mathcal{Perm}(V, w))$ 
    
    \noindent $ = NPA_{\ell^* + k}(\mathcal{Perm}(V, w))$ for any $k\in \mathbb{Z}_{\geq 0}$.
\end{proposition}
\begin{proof}
    We show this by demonstrating that the entries of a feasible moment matrix for $NPA_{\ell^*+k}$ are in fact determined by the submatrix corresponding to $NPA_{\ell^*}$.  Let $\phi\in \Gamma_{\ell^*+k}$ of degree $\ell^*+j$ for $0\leq j\leq k$.  Since there are only $\binom{n}{2}$ many varaibles $p_{ij}$, $\phi$ must contain $j$ varaibles which are repeated.  Using the anti-commuting constraint \cref{eq:anti_comm} we can commute the variables $p_{ij}$ through each other while potentially picking up lower degree monomials ($p_{ij} p_{jk}\rightarrow -p_{jk} p_{ij} +p_{ij} +p_{jk}+p_{ik}-\mathbb{I}$).  We can commute repeated $p_{ij}$ through to cancel them using \cref{eq:sym_const_1} and proceed inductively to cancel any repeating transpositions in each monomial in the linear combination of monomials generated by applying the anti-commuting constraint.  It follows that $\phi$ may be written as a linear combination of monomials in the variables $\{p_{ij}\}$ containing only distinct $p_{ij}$.  Hence, the constraints imply that for any $ \phi, \phi'\in \Gamma_{\ell^*+k}$, $M_{\ell^*+k}(\phi, \phi')=\sum_{\theta, \theta' \in \Gamma_{\ell^*}}c_{\theta, \theta'} M_{\ell^*+k}(\theta, \theta')$ 
    %\lunote{Should it be $M_{\ell^*+k}(\phi, \phi')=\sum_{\theta, \theta' \in \Gamma_{\ell^*}}c_{\theta, \theta'} M_{\ell^*}(\theta, \theta')$?}\knote{No, I think in the context of this proof $M_{\ell^*}$ isnt defined.  The point is to show that the matrix $M_{\ell^*+k}$ is really determined by a submatrix.} 
    for scalars $c_{\theta, \theta'}$.
\end{proof}
A feasible solution of $NPA_{\ell^*}(\mathcal{Perm})$ is ``effectively'' of size $ 2^{\binom{n}{2}}$.  This is because we may use the anti-commuting constraint to relate any row of $M_{\ell^*}$ to a row in which the $p_{ij}$ are given in some predetermined canonical order. Hence a linearly independent row is given by choosing whether or not each $p_{ij}$ is included in this fixed order. 
$NPA_{\ell}(\mathcal{Pauli})$ is known to have finite convergence at a lower level $\ell^*=n$ with effective SDP size $4^n \ll 2^{n \choose 2}$.  However, we suspect that \cref{prop:fin_conv} is loose and $NPA_{\ell}(\mathcal{Perm})$ actually has finite convergence at $\ell=n/2$\footnote{Indeed, this has been established by an independent group of researchers \cite{other_guys} for a slightly different hierarchy.}. As a matter of fact, for many Hamiltonians of interest $NPA_\ell(\mathcal{Perm})$ converges with smaller SDP sizes.  
For example, most of the graphs we address in the following section (star graphs, complete (bipartite) graphs, crown graphs, etc.) will be exactly solved by $NPA_1(\mathcal{Perm})$\footnote{Recall that solvability with $NPA_1(\mathcal{Perm})$ is equivalent to $NPA_1(\mathcal{Proj})$. While we will stick to $\mathcal{Proj}$ notation in the following sections, here we are using $\mathcal{Perm}$ for the simplicity of the finite convergence proof.} but not by $NPA_1(\mathcal{Pauli})$. 
The matrix size required for convergence then reads
$\binom{n}{2}3^2+3n+1$ and $\binom{n}{2}+1$ for $\mathcal{Pauli}$ and $\mathcal{Perm}$ respectively, where the latter is more efficient by an approximate factor of 9, and is far less than $2^{n \choose 2}$. 
%\lunote{Why are we using $NPA_1(\mathcal{Perm})$ instead of $NPA_1(\mathcal{Proj})$ in the proposition and this paragraph?}\knote{$\mathcal{Perm}$ is nice because you can cancel out same factors, i.e. $p_{ij}^2=\mathbb{I}$.  Without that you have to actually use the fact that you have every possible $(i, j)$. Perm is easier for an inductive proof, but its moslty a matter of taste.  }
%For instance the weighted/unweighted star graphs require a $\mathcal{Pauli}$ SDP of size $\binom{n}{2}3^2+3n+1$ for convergence while $\mathcal{Perm}$ requires only $\ell=1$, or size $1+\binom{n}{2}$. 
%The same could be said for any other family of graphs that are not exactly solvable by $NPA_1(\mathcal{Pauli})$ but is solvable by $NPA_1(\mathcal{Perm})$, which includes 


\begin{comment}
\subsection{Hierarchies as Relaxations of Quantum Systems}

In our context both the operator programs and the SDP hierarchies have the property taht they ``relax'' the local Hamiltonian problem.  This means taht the optimal eigenvector for the local Hamiltonian problem can be used to construct wfeasible solutions to the operator programs/SDPs in such a way that the extermal eigenvalue equals the objective of the corresponding feasible solution.  If $\ket{\psi}$ is the extremal eigenvecotr of a SWAP Hamiltonian then we may set $\ket{\boldsymbol{\phi}}=\ket{\psi}$ and $\mathbf{P}_{ij}= P_{ij}$ (defined in equation BLANK) to obtain a feasible solution for the program corresponding to $\mathcal{Perm}(G, \{w\})$ with objective matching the extremal eigenvalue.  Similarly, we can set
\begin{equation}
    \mathbf{M}_\ell(\mathbf{\Phi}, \mathbf{\Theta})=Tr[\mathbf{\Phi}^* (\{P_{ij}\} \mathbf{\Theta}(\{P_{ij}\}) \ket{\psi} \bra{\psi}]
\end{equation}
to obtain a feasible solution to $NPA_\ell(\mathcal{Perm})$ with objective matching the extremal eigenvalue.  It follows that $\SWAP(G, \{w\})\leq \mathcal{Perm}(G, \{w\})$ and $\SWAP(G, \{w\})\leq NPA_\ell(\mathcal{Perm}(G, \{w\})$\knote{Notation is failing here}.  Similar considerations hold for $\mathcal{Proj}$ and $\mathcal{Pauli}$ for QMC instances and general $2-$Local Hamiltonian instances respectively.

Both $NPA_\ell$ and $SoS_\ell$ provide computationally efficient upper bounds on the corresponding local Hamiltonian problem, but $SoS_\ell$ has the additional feature that the optimal solution to the SDP provides an explicit algebraic proof that $OPT(\mathcal{O}) \leq SoS_\ell$.


In order to explicitly give an SDP for $NPA_\ell$ we will define constraint matrices\knote{uinsg this expression for two different things} and express the above in SDP ``standard form''.  Let $B_\mathbb{I}$ be a matrix indexed by $\Gamma_\ell$ such that
\begin{equation}\label{eq:b_I}
    B_\mathbb{I}(\Phi, \Phi')=\begin{cases}
    1 \text{  if } \Phi=\mathbb{I}=\Phi'\\
    0 \text{   otherwise}
    \end{cases}.
\end{equation}
If $(\Phi, \Phi', \Theta, \Theta')\in \Gamma_\ell \times \Gamma_\ell \times \Gamma_\ell \times \Gamma_\ell $ is a tuple satisfying $\Phi^* \Phi'=\Theta^* \Theta'$ which can seen by concatenation, define $C_{(\Phi, \Phi', \Theta, \Theta')}, C_{(\Phi, \Phi', \Theta, \Theta')}'$\footnote{We will require matrices $C_{(\Phi, \Phi', \Theta, \Theta')}$ and  $C_{(\Phi, \Phi', \Theta, \Theta')}'$ to enforce \Cref{concat_const} using only \textbf{Hermtian} constraint matrices .}\knote{ToDo: Use D matrix notation to enforce these $C$ things should be shorter/less clunky.  One issue is that the (theta, theta') pair cannot be chosen arbitrarily anymore for enforcing the C conditions.  In general the notation is kind of bad here so think about how to make it better.  } as Hermitian matrices indexed by $\Gamma_\ell$ such that
\begin{equation}
    C_{(\Phi, \Phi', \Theta, \Theta')}(\Psi, \Psi')=\begin{cases}
    1 \text{  if  $\Psi=\Phi $ and $\Psi'=\Phi'$}\\
    1 \text{  if  $\Psi=\Phi' $ and $\Psi'=\Phi$}\\
    -1 \text{  if  $\Psi=\Theta $ and $\Psi'=\Theta'$}\\
    -1 \text{  if  $\Psi=\Theta' $ and $\Psi'=\Theta$}\\
    0 \text{  otherwise}\\
    \end{cases},
\end{equation}
and 
\begin{equation}
    C_{(\Phi, \Phi', \Theta, \Theta')}'(\Psi, \Psi')=\begin{cases}
    i \text{  if  $\Psi=\Phi $ and $\Psi'=\Phi'$}\\
    -i \text{  if  $\Psi=\Phi' $ and $\Psi'=\Phi$}\\
    -i \text{  if  $\Psi=\Theta $ and $\Psi'=\Theta'$}\\
    i \text{  if  $\Psi=\Theta' $ and $\Psi'=\Theta$}\\
    0 \text{  otherwise}\\
    \end{cases}.
\end{equation}
For any $p\in span(\Gamma_{2 \ell})$ define $D_p, D_p'$ as matrices indexed by $\Gamma_\ell$ as follows:  Let $p=\sum_{\Phi\in \Gamma_{2\ell}}c_\Phi \Phi$ for $c_\Phi \in \mathbb{C}$ for all $\Phi$.  For all $\Phi \in \Gamma_{2\ell}$ choose a pair $(\Theta_\Phi, \Theta_{\Phi}')\in \Gamma_\ell \times \Gamma_\ell$ such that $\Theta_\Phi^* \Theta_\Phi'=$ Then,
\begin{equation}
    D_p(\Psi, \Psi')=\begin{cases}
        c_\Phi \text{  if $\Psi=\Theta_\Phi$ and $\Psi'=\Theta_\Phi'$}\\
        c_\Phi^* \text{  if $\Psi=\Theta_\Phi'$ and $\Psi'=\Theta_\Phi$}\\
        0 \text{  otherwise},
    \end{cases}
\end{equation}
and 
\begin{equation}\label{eq:dp_p}
    D_p(\Psi, \Psi')'=\begin{cases}
        i c_\Phi \text{  if $\Psi=\Theta_\Phi$ and $\Psi'=\Theta_\Phi'$}\\
        -i c_\Phi^* \text{  if $\Psi=\Theta_\Phi'$ and $\Psi'=\Theta_\Phi$}\\
        0 \text{  otherwise}.
    \end{cases}
\end{equation}
For ease of reference let $U_1:=\{(\Phi, \Phi', \Theta, \Theta')\in \Gamma_\ell \times \Gamma_\ell \times\Gamma_\ell \times \Gamma_\ell: \Phi^* \Phi'=\Theta^* \Theta'\}$ and $U_2:= \{ p\in span(\Gamma_{2\ell}): \,\,p=r p_i \text{ with } r\in \Gamma_{2\ell}  \text{ and } deg(r)+deg(p_i) \leq 2\ell\}$.  Using these sets and denoting $A\cdot B:= Tr(A^* B)$, $NPA_\ell$ can be succinctly written as:
\begin{align}
NPA_\ell(\mathcal{O}):= \max /\min \,\,&M_\ell \cdot \frac{1}{2} D_q\\
\nonumber s.t. \\
\nonumber B_\mathbb{I} \cdot M_\ell &=1\\
\nonumber \,\, C_{(\Phi, \Phi', \Theta, \Theta')} \cdot M_\ell&=0 \text{  for all $(\Phi, \Phi', \Theta, \Theta')\in U_1$}\\
\nonumber C_{(\Phi, \Phi', \Theta, \Theta')}' \cdot M_\ell&=0 \text{  for all $(\Phi, \Phi', \Theta, \Theta')\in U_1$}\\
\nonumber D_p \cdot M_\ell &=0 \text{ for all $p \in U_2$}\\
\nonumber D_p' \cdot M_\ell &=0 \text{ for all $p\in U_2$}\\ 
\nonumber M_\ell&\succeq 0, Hermitian
\end{align}

The dual hierarhcy, where individual levels are SDP dual to the corresponding NPA level, is normally referred to as the ``sum of squares'' (SoS) hierarchy.  To motivate this hierarchy we will need to note a natural correspondence between PSD matrices indexed by $\Gamma_\ell$ and ``sums of squares'' of operators in $span(\Gamma_\ell)$. The SoS hierarchy will be slightly different depending on whether the objective of $\mathcal{O}$ is to maximize or minimize.  We will motivate it for the maximization version and give a general definition for $\max/\min$.  Let $\sum_i S_i^* S_i$ be a degree $\leq 2 \ell$ sum of squares.  This means each $S_i=\sum_{\Phi\in Gamma_\ell} c_\Phi^i \Phi$ for some constants $c_\Phi^i\in \mathbb{C}$.  While this is not obviously a sum of squares, if $c_\Phi^i\in \mathbb{R}$ for all $i$, $\Phi$ then $S_i^*=S_i$ and $\sum_i S_i^* S_i=\sum_i S_i^2$.  Having $\sum_i S_i^* S_i$ allows us to generalize this notion to complex numbers.  Let $Q$ be a matrix indexed by $\Gamma_\ell$  such that $Q(\Phi, \Phi')=\sum_i (c_\Phi^i)^* c_{\Phi'}^i$.  $Q$ is easily seen to be PSD: For each $i$ define a row vector $v_i$ indexed by $\Gamma_\ell$ such that $v_i(\Phi)=c_\Phi^i$.  Then, $Q=\sum_i v_i^* v_i$.  Observe also that any $Q\succeq 0$ has a corresponding sum of squares $\sum_i S_i^* S_i$ which can be calculated via diagonalization.  We will also need to define a ``constraint deformation matrix'' \knote{think of better thing to call this}.  Given the matrices in \Cref{eq:b_I}-\Cref{eq:dp_p} a constraint deformation matrix is simply a matrix of the form:
\begin{align}
T=\sum_{(\Phi, \Phi', \Theta, \Theta')\in U_1} y_{(\Phi, \Phi', \Theta, \Theta')} C_{(\Phi, \Phi', \Theta, \Theta')}+y_{(\Phi, \Phi', \Theta, \Theta')}' C_{(\Phi, \Phi', \Theta, \Theta')}'+\sum_{p \in U_2} z_p D_p +z_p' D_p',
\end{align}
where all $y_{(\Phi, \Phi', \Theta, \Theta')}, y_{(\Phi, \Phi', \Theta, \Theta')}', z_p, z_p' \in \mathbb{R}$.

Now suppose we are able to obtain an expression of the form $\lambda B_\mathbb{I}-\frac{1}{2}D_q= Q+T $ where $Q$ is an SoS matrix ($\succeq 0$) and $T$ is a constraint deformation matrix.  We claim that such an expression implies that the optimal value of $\mathcal{O}$ is $\leq \lambda$ (recall $\mathcal{O}$ is currently a maximization problem).  The left and right side are both matrices indexed by $\Gamma_\ell$ so by summing up matrix entries an multiplying by the corresponding operators we may derive:
\begin{align} 
\lambda\mathbb{I}-\frac{1}{2}\sum_{\Phi, \Phi' \in \Gamma_\ell}D_q(\Phi, \Phi') =\sum_{\Phi, \Phi'} Q(\Phi, \Phi') +T(\Phi, \Phi')\\
\Rightarrow \lambda\mathbb{I}- q =\sum_i S_i^* S_i+\sum_{(\Phi, \Phi', \Theta, \Theta')\in U_1}y_{(\Phi, \Phi', \Theta, \Theta')} (\Phi^* \Phi'-\Theta^* \Theta'+(\Phi')^*\Phi -(\Theta')^* \Theta)\\ 
\nonumber + y_{(\Phi, \Phi', \Theta, \Theta')}' i(\Phi^* \Phi'- \Theta^* \Theta'+(\Theta')^* \Theta- (\Phi')^* \Phi)+\sum_{p\in U_2}z_p (p+p^*)+z_p i (p-p^*).
\end{align}
The algebraic constraints of $\mathcal{O}$ imply the second and third sums on the R.H.S evaluate to $0$.  So for feasible operators for $\mathcal{O}$, $\lambda \mathbb{I}-q=\sum_i S_i^* S_i \succeq 0$, hence $\mu_{max}(q) \leq \lambda$.  Essentially a level-$\ell$ SoS proof involves taking a SoS of operator in $span(\Gamma_\ell)$ and deforming it using constraints which hold \textbf{at that level} to obtain an expression of the form $\lambda \mathbb{I}-q$.  Higher levels give access to more constraints (the set $U_2$ gets larger), and hence stronger proofs.  The optimal SoS proof at a level-$\ell$ can be computed by SDP and is in fact the dual SDP to $NPA_\ell$:

\begin{definition}[$SoS_\ell(\mathcal{O})$] Given an operator program of the form BLANK, define matrices as in \Cref{eq:b_I}-\Cref{eq:dp_p}.  If $\mathcal{O}$ is a maximization problem then 
\begin{align}
    SoS_\ell(\mathcal{O}):= \min \lambda\\
    s.t.\\
    \lambda B_\mathbb{I}-\frac{1}{2} D_q= Q+T\\
    T=\sum_{(\Phi, \Phi', \Theta, \Theta')\in U_1} y_{(\Phi, \Phi', \Theta, \Theta')} C_{(\Phi, \Phi', \Theta, \Theta')}+y_{(\Phi, \Phi', \Theta, \Theta')}' C_{(\Phi, \Phi', \Theta, \Theta')}'+\sum_{p \in U_2} z_p D_p +z_p' D_p'\\
    Q \succeq, Hermitian\\
    y_{(\Phi, \Phi', \Theta, \Theta')}, y_{(\Phi, \Phi', \Theta, \Theta')}', z_p, z_p' \in \mathbb{R} \,\, \forall \,\, (\Phi, \Phi', \Theta, \Theta')\in U_1, p\in U_2
\end{align}
If $\mathcal{O}$ is a minimization problem then:
\begin{align}
    SoS_\ell(\mathcal{O}):= \max \lambda\\
    s.t.\\
    \frac{1}{2} D_q-\lambda B_\mathbb{I}= Q+T\\
    T=\sum_{(\Phi, \Phi', \Theta, \Theta')\in U_1} y_{(\Phi, \Phi', \Theta, \Theta')} C_{(\Phi, \Phi', \Theta, \Theta')}+y_{(\Phi, \Phi', \Theta, \Theta')}' C_{(\Phi, \Phi', \Theta, \Theta')}'+\sum_{p \in U_2} z_p D_p +z_p' D_p'\\
    Q \succeq, Hermitian\\
    y_{(\Phi, \Phi', \Theta, \Theta')}, y_{(\Phi, \Phi', \Theta, \Theta')}', z_p, z_p' \in \mathbb{R} \,\, \forall \,\, (\Phi, \Phi', \Theta, \Theta')\in U_1, p\in U_2
    \end{align}
\end{definition}


\subsection{Convergence}
Under relatively mild assumptions \cite{pir10con} it is know that $NPA_\ell(\mathcal{O})$ converges to the optimal solution of $\mathcal{O}$ as $\ell \rightarrow \infty$.  So it is generally possible to solve a high enough level $NPA_\ell$ (corresponding to an exponentially large SDP in $\ell$) to obtain the optimal solution for $\mathcal{O}$.  However, it is unclear what the relationship is between the optimal solution of $\mathcal{O}$ and the extremal eigenvalue of the explicit Hamiltonian problem we have in mind (i.e. QMC).  One issue is that the optimal solution to the operator program could have operator assignments with size which are larger or smaller than the relevant Hilbert space, so it is not obvious how to interpret a the optimal solution of the operator program in the context of the Hamiltonian problem.  

Luckily for $\mathcal{Pauli}$ operator programs it is known by representation theory that the optimal solution to the operator program exactly equals the optimal solution to the local Hamiltonian problem (even for Hamiltonians outside QMC).  This is because any representation of the Pauli operators must be isomorphic to the Pauli operators themselves up to a global unitary and tensoring with the identity \cite{cha17}\knote{find better ref for this}.  In a sense the ``only'' representation of the Pauli group is the matrices themselves.  Additionally there is an operator program not discussed here for fermionic creation and annihilation operators \cite{pir10con} which is known to converge to the optimal eigenvalue under similar reasoning.  In both these cases we can understand the optimal solution to the operator program as the extremal eigenvalue because the representation theory is strict enough to guarantee that a set of optimal operators will be equal to the explicit operators we have in mind (again up to tensoring with identity and overall unitary).  

It is natural to ask if there is an operator program with optimal solution achieving the extremal quantum eigenvalue, \textbf{without} the representation theory of the underlying group being as strict.  Indeed one of the results of this paper is that $\mathcal{Perm}/\mathcal{Proj}$ achieves the optimal solution of the corresponding $\SWAP/QMC$ instance.  In contrast to $\mathcal{Pauli}$ the proof demonstrates that $\mathcal{Perm}$ obtains the optimal value with operators which are strictly smaller than the corresponding quantum SWAP operators.

\begin{theorem}\label{thm:perm_is_opt}
$\mathcal{Perm}(G, w)=\SWAP(G, w)$.
\end{theorem}
\begin{proof}
See \Cref{sec:perm_conv}.
\end{proof}

\begin{corollary}
$\mathcal{Perm}$ is a QMA-complete operator program.  
\end{corollary}
\begin{proof}
    Determining $\SWAP(G, w)$ is QMA-complete \cite{pid17} and by \Cref{thm:perm_is_opt} finding $\SWAP(G, w)$ is equivalent to finding $\mathcal{Perm}(G, w)$.
\end{proof}
\end{comment}