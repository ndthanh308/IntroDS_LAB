%!TEX root = ../main.tex

\section{Methods} % (fold)
\label{sec:methods}

In this paper, we introduce an automated pipeline for moth species detection and classification.
As visualized in Figure~\ref{fig:pipeline}, the moth detector identifies bounding boxes around the insects given an input image.
Afterward, the image patches identified by the detected bounding boxes are fed into a CNN classifier.
The pipeline performs the classification either only on the input image or extracts informative regions, called parts, which it uses as additional information.
In our experiments, we show that this additional information improves the classification performance (see Sect.~\ref{sub:results_on_eumoths}).

In the following, we introduce the two stages of the pipeline: (1) moth detection based on the single-shot detector (Sect.~\ref{sub:single_shot_detector}), and (2) part-based classification with the help of classification-specific parts (Sect.~\ref{sub:part_based_classification}).

% Figure environment removed

\subsection{Single-shot Detector} % (fold)
\label{sub:single_shot_detector}
As already mentioned in Sect.~\ref{sub:related:detection}, the main idea of the single-shot MultiBox detector (SSD) proposed by Liu~\etal~\cite{liu2016ssd} is to utilize feature maps from multiple intermediate stages of the backbone CNN to predict location offsets and class confidences for a set of prior locations.
More precisely, given a feature map with $F \in \mathbb{R}^{N \times M \times P}$ with $P$ channels, $K$ prior bounding boxes with different scales and aspect ratios are defined for each of the $N \cdot M$ locations.
The feature map is transformed by a $3 \times 3 $ convolution with $(C+4) \cdot K$ output channels.
This results for each of the $K$ prior boxes in $C$ per-class scores and four offset values $\Delta=\{dx, dy, dw, dh \}$.
The offsets $dx$ and $dy$ describe the positional offset to the center of the prior box.
The change of the width and the height of a prior box is modeled by $dw$ and $dh$.
% Figure~\ref{fig:prior_boxes} visualizes the idea of the prior boxes.

% Figure environment removed

During training, a prior box is selected as positive when there was a ground truth bounding box with an IoU score higher than~\num{0.5}.
As visualized in Figure~\ref{fig:prior_boxes}, prior boxes at different locations are assigned to different objects.
Additionally, for a single object multiple prior boxes may be selected.
The training objective for estimated location offsets $\Delta$ and class scores $c$ given ground truth bounding boxes $g$ is defined as a sum of the confidence loss and the localization loss:
\begin{equation}
	L(\Delta,c,g) = \dfrac{1}{\mathcal{K}} \left(L_{conf}(c) + L_{loc}(\Delta,g) \right)
\end{equation}
with $\mathcal{K}$ being the number of matched prior boxes, and if no boxes are matched, the loss is set to \num{0}.
The class scores are unnormalized log-likelihoods of a class identified in a certain location.
In our case, the detector distinguishes only the general \emph{moth} class from the background.
For more details about the loss functions, we refer to the original paper of Liu~\etal~\cite{liu2016ssd}.

\begin{comment}
% \begin{equation}
% 	Pos = \{(i,j)~|~\forall i~\exists j: \mathrm{IoU}(d_i,g_j) > 0.5 \} \quad .
% \end{equation}


If $z^p_{ij} = \{0,1\}$ is an indicator for $i$-th prior box to match the $j$-th ground truth box of class $p$, then
This reduces the multi-class softmax cross-entropy loss to a binary log loss:

\begin{align}
L_{conf}(z,c)
	& = - z^{moth}_{i,j}\cdot \log \left( \hat{c}^{moth}_i \right) - \log \left(1 - \hat{c}^{moth}_i \right)\\
	\mathrm{with}~\hat{c}^{moth}_i &= \frac{ \exp \left(c^{moth}_i\right) }{\exp \left(c^{moth}_i\right) + \exp \left(c^{bg}_i\right)} \quad \nonumber.
\end{align}

The localization loss is a Smooth-L1 loss~\cite{girshick2015fast} between the predicted ($l$) and the ground truth ($\hat{g}$) offsets, calculated from the ground truth bounding box ($g$) and a prior box ($d$):
% \todo[inline]{Gleichung Ã¼berarbeiten!}
\begin{align}
	L_{loc}(z,l,g)
		& = \sum_{(i,j) \in Pos}~\sum_{m \in \Delta}
		 z_{ij}^k \cdot \mathrm{L1}_{smooth} (l_i^m - \hat{g}^m_j) \\
	\mathrm{with}~\nonumber
	\hat{g}^{dx}_j
		& = \frac{g^{x}_j - d^{x}_i}{d^{w}_i}
	, \quad {~}\hat{g}^{dy}_j
		= \frac{g^{y}_j - d^{y}_i}{d^{h}_i}  \\
	\nonumber
	\hat{g}^{dw}_j
		& = \log \left(\frac{g^{w}_j}{d^{w}_i}\right)
	,\quad \hat{g}^{dh}_j
		= \log \left(\frac{g^{h}_j}{d^{h}_i}\right) \quad .
\end{align}

\end{comment}

% subsection single_shot_detector (end)

\subsection{Part-based Classification} % (fold)
\label{sub:part_based_classification}

% \noident\emph{Implicit feature selection:}
Nowadays, neural networks like CNNs yield the best results in classification by extracting high-level features from the input image in the form of a high-dimensional feature vector (e.g., $D=2048$ in case of InceptionV3).
In the context of a fine-grained recognition task, the classifier has to focus on a specific feature dimension to distinguish a class from the others.
Therefore, we first estimate the most informative features for the current classification task.
It is realized by utilizing a linear classifier with a sparsity-inducing L1-regularization.
An optimization with L1-regularization forces the classifier's decisions to perform the classification on a small subset of feature dimensions.
This kind of implicit feature selection is classification-specific.
Furthermore, it allows identifying for each class the feature dimensions that best distinguish this class from all other classes by selecting dimensions with classifier weights above a certain threshold.

\noindent\textbf{Informative Image Regions:}\quad
% In this step, we identify most important regions in the image with the respect to the classification decision.
We utilize gradient maps~\cite{simonyan2013deep} to estimate the most informative pixels in the image, identified by large gradients.
As described previously, we restrict the computation of the gradients only to the feature dimensions used by the L1-regularized classifier.
Thus, we incorporate the initial classification in the estimation of the part regions.
Like Simonyan \etal~\cite{simonyan2013deep} and Simon \etal~\cite{Simon_2015_ICCV}, we use back-propagation through the CNN to identify the regions of interest for each selected feature dimension.
We compute a saliency map $\vec{M}(\vec{I})$ for an image $\vec{I}$ based on the feature dimension subset $\mathfrak{D} \subset \{1, \dots, D\}$ as follows:

\begin{equation}
	\label{eq:saliency_sum}
	M_{x,y}(\vec{I})
	  = \dfrac{1}{|\mathfrak{D}|} \sum_{d \in \mathfrak{D}} \left| \dfrac{\partial}{\partial I_{x,y}} f^{(d)}(\vec{I}) \right|
	  % = \dfrac{1}{|\mathfrak{D}|} \sum_{d \in \mathfrak{D}} \left| \dfrac{\partial}{\partial I_{x,y}} \dfrac{1}{s \cdot u}\sum_{j=1}^s \sum_{j'=1}^u F_{j,j'}^{(d)}(\vec{I}) \right|
	  \quad  .
\end{equation}

\noindent\textbf{Part Estimation:}\quad
Next, we normalize the values of the saliency map to the range $[0 \dots 1]$, and discard regions of low saliency by setting values beneath the mean saliency value to $0$.
We use the resulting sparse saliency map to estimate the spatial extent of coherent regions.
Like Zhang \etal~\cite{zhang2019unsupervised}, we achieve this by \mbox{$k$-means} clustering of pixel coordinates $(x, y)$ and the saliencies $M_{x,y}$ (Eq.~\ref{eq:saliency_sum}).
Additionally, we also consider the RGB values at the corresponding positions in the input image.
The clusters are initialized with $k$ peaks computed by non-maximum suppression, identifying locations with the largest saliencies.
Consequently, the number of peaks determines the number of parts to detect.
Finally, it is straightforward to identify a bounding box around each estimated cluster, and the resulting bounding boxes serve as parts for the following classification.

\noindent\textbf{Extraction and Aggregation of Part Features:}\quad
In the final step, we extract image patches with the help of the estimated bounding boxes and treat them as regular images.
The neural network should extract different features from these image patches than from the original image because the level of detail varies between these types of input.
Therefore, we process the part images by the same CNN architecture as the original image but with a separate set of weights.
Afterward, for every part image, the CNN extracts a feature vector, resulting in a set of part features for every single image.
There are different ways to aggregate these features to a single feature vector and perform the classification.
We have chosen to average over the part features, which results in a single feature vector with the same dimension as for the original image.
This aggregation strategy yielded better results in our experiments than, for example, concatenation of part features.
Finally, classification is performed based on the global feature and the aggregated part feature.
For joint optimization of both CNNs, we average the cross-entropy losses of the global prediction $\vec{p}$ and part prediction $\vec{q}$.
It equals to computing the geometrical mean of normalized class probabilities and enforces both classifiers to be certain about the correct class:

\begin{align}
	L_{final}\left(\left\{\vec{p},\vec{q}\right\}, y\right)
		&= \frac{1}{2}\left(L\left(\vec{p}, y\right) + L\left(\vec{q}, y\right) \right) \\
		&= -\frac{1}{2}\left(
			\sum^{C}_{i=1} y_i \cdot \log (p_{i}) +
			\sum^{C}_{i=1} y_i \cdot \log (q_{i}) \right)\\
		&= -\sum^{C}_{i=1} y_i \cdot \log \left(
			\sqrt{p_i \cdot q_i}
		\right) \quad .
\end{align}


% subsection part_based_classification (end)

% section methods (end)

\endinput

Better structure for this subsection:
\begin{enumerate}
	\item Estimation of interesting regions
	\begin{itemize}
		\item Given image and CNN-extracted features, with an saliency map, we can compute the impact of each pixel w.r.t. the computed features
		\item We are interested in features, that contribute to the classification the most
		\item Sparsity-inducing classifier training results in implicit feature selection
		\item Features used in the classification encode the most information for the classification and saliency map for these features identifies the pixels used most
		\item This saliency map results in a more precise attention of interesting image regions
	\end{itemize}
	\item Separation into parts
	\begin{itemize}
		\item given a saliency map (independently how it was computed), we want to separate it in coherent regions
		\item We estimate coherency by the position, the saliency at a certain position, and pixel value at a certain position
		\item utilize an unsupervised clustering method to identify coherent regions
		\item these regions are then used to estimate bounding boxes around there regions
	\end{itemize}
	\item Aggregation of part information
	\begin{itemize}
		\item Encode each part location with a CNN, same as the image itself
		\item use a separate network, since it operates at a different granularity level
		\item combine resulting features in a part feature representation
		\item compute class scores from the global and the part feature representations
		\item combine the losses for the predictions (arithmetic mean)
		\item averaging losses arithmetically is the same as averaging class probabilities geometrically.
		\item Other aggregation methods possible. We chose the most simplest one to this point.
	\end{itemize}
\end{enumerate}
