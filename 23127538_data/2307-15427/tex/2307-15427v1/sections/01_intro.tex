%!TEX root = ../main.tex

\section{Introduction} % (fold)
\label{sec:introduction}

The discussions and concerns about environmental changes nowadays are both ubiquitous and necessary.
We live in times in which ecosystems change drastically in a short time, and we, as humans, play a significant role in this development.
One of these negative developments is the dramatic loss of insects~\cite{hallmann2017more,wagner2021insect}.
One step towards a better understanding of insect die-off is monitoring species populations, which is time-consuming and often requires experts in that field.
With about one million named species~\cite{stork2018many}, insects represent the vast majority of animals on our planet.
Hence, it is also clear why manual counting of individuals for abundance estimations is not realistic.
Though current developments in big data analysis and computer vision improved a lot, these technologies are not established in insect monitoring as they are, for example, in marketing or entertainment.

Besides others, there are monitoring systems to observe insects~\cite{jonason2014lighttrap,svenningsen2020contrasting,bjerge2021automated}, great apes~\cite{Freytag16_CFW,Brust2017AVM,Kaeding18_ALR,yang2019great,sakib2020visual}, elephants~\cite{Koerschens18:Elephants,Koerschens19:ELPephants}, or sharks~\cite{hughes2017automated}.
Though such monitoring systems are already camera-assisted, a vast amount of data needs to be evaluated.
Unfortunately, the daily work of many ecologists nowadays is still the manual inspection of hundreds or thousands of images, which is exhausting and time-consuming.

As a part of the AMMOD project\footnote{AMMOD = \textbf{A}utomated \textbf{M}ultisensor Station for \textbf{M}onitoring \textbf{o}f Bio\textbf{d}iversity (\url{https://ammod.de/})}, we aim at automated monitoring of species in our immediate vicinity assisted by a computer vision system.
In this paper, we cover the task of categorizing moth species (subset of \emph{Lepidoptera}) by a non-invasive approach.
Within the project, a so-called \emph{moth~scanner} is developed, which consists of an illuminated planar surface and an automated camera system.
During the nighttime, special light sources illuminate the planar area to attract different moth species in the surrounding area.
The automated camera system captures the attracted individuals that land on the illuminated surface.
Finally, our task is to detect and classify the individuals in the taken images.
With the detection and classification results, we assist the ecologists in analyzing the insect population trends.

For automatically analyzing the images, we propose a prototype for a deep learning pipeline consisting of two steps: (1)~localization of individuals via moth detection and (2)~species identification by classification.
For the detection, we use a well-established detection model capable of identifying multiple objects in an image, namely the single-shot MultiBox detector (SSD)~\cite{liu2016ssd}.
The mean average precision (mAP) of our moth detector is~\pcent{88.88} and~\pcent{99.01} for intersection over union (IoU) values above \num{0.75} and \num{0.5}, respectively.

The subsequent classification of \num{200} common moth species in Central Europe is performed with the help of a convolutional neural network (CNN).
In our experiments, we show the benefits of different design decisions for a classifier trained on copped images, namely images depicting a single insect.
The selection of a fine-tuning strategy, the pre-training dataset, and the extension of the classifier with an unsupervised part estimator improve the classification accuracy from~\pcent{63.28} to~\pcent{93.13}.

Finally, we show that the classification accuracy of our proposed pipeline improves with a preceding moth detector on uncropped images.
These are the images captured by the moth scanner, where we cannot ensure that only a single insect has been photographed.
Furthermore, in these images, the insects allocate only a small portion of the entire area.
Hence, preceding a moth classifier with a moth detector, we can improve the classification performance from~\pcent{79.62} to~\pcent{88.05}.

\endinput


First, fine-tuning of the entire model instead of the final classification layer introduces a computational overhead but increases the classification accuracy remarkably from~\pcent{63.28} to~\pcent{89.46}.
Next, the selection of proper pre-trained weights for the classification model is crucial as well.
While widely provided weights pre-trained on the ImageNet dataset~\cite{ImageNet} already yield good recognition results, using a more domain-related dataset like iNaturalist~\cite{iNaturalist} boosts the accuracy to~\pcent{91.50}.
Finally, we utilize an unsupervised part detection approach proposed by Korsch~\etal~\cite{Korsch19_CSPARTS} to enrich the data fed into the CNN by additional image patches.
Since the approach is unsupervised, it requires no further part annotations.
Nevertheless, using the detected parts increases the classification accuracy even further to~\pcent{93.13}.
% An extension of this classifier with a preceding detector improves the classification from~\pcent{79.62} to~\pcent{88.05}.

\redmark{Viell. zu viele Zahlen aus den Experimenten hier in der Intro vorweggenommen? Auf jeden Fall im letzten Absatz aufpassen: zu Beginn sprichst du von einer Klassif. mit 79-88\%, weil du da die gesamte Pipeline meinst. Sp√§ter beziehst du dich nur auf die Klassif. und berichtest von hoeheren Accuracies (91.5 und 93.13). Das kann verwirred auf den (fachfremden) Leser wirken}

% section intro (end)
