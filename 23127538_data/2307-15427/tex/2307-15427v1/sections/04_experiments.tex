%!TEX root = ../main.tex

\section{Experiments}
\label{sec:experiments}

In the following, we evaluate the two parts of our moth scanner pipeline.
We perform each experiment ten times and provide in Sections~\ref{sub:results_on_mcc}, and~\ref{sub:results_on_eumoths} the mean and the standard deviation of the evaluation metrics across the different runs.
We fine-tune all CNNs for \num{60} epochs with the RMSProp~\cite{tieleman2012lecture} optimizer and L2-regularization with a weight decay of~\num{5e-4}.
The starting learning rate of \num{1e-4} is reduced by \num{0.1} after \num{20} and \num{40} epochs.
Furthermore, we utilize standard image augmentation methods: random cropping, random horizontal and vertical flipping, and color jittering (contrast, brightness, and saturation).
In the case of the classification, we further utilize label smoothing regularization~\cite{inception} with a smoothing factor of \num{0.1}.

First, we evaluate the performance of the moth detector presented in Sect.~\ref{sub:single_shot_detector}.
In Table~\ref{tab:detection_results}, we report the mean average precision (mAP) as the evaluation metric for the detections.
The precision is based on predictions of detected objects, where the intersection over union (IoU) of predicted and ground truth bounding boxes is above a certain threshold.
The IoU describes how well two bounding boxes match by computing the ratio between the intersection and the union of the areas of the two bounding boxes.
The two typical evaluation metrics used in one of the most common object detection benchmarks~\cite{lin2014mscoco} are \emph{mAP@0.5} and \emph{mAP@0.75}, with IoU thresholds of \num{0.5} and \num{0.75}, respectively.
We use a standard version of the detection network with an input size of $300\times300$ and the VGG16~\cite{simonyan2014very} architecture as a backbone that is pre-trained on ImageNet~\cite{ImageNet}.
All additional layers included for the detection task and not initially present in the VGG architecture are initialized randomly.

Second, we evaluate the classification performance.
Since the datasets we use have a balanced sample distribution across the classes, we use accuracy as a primary evaluation metric.
To be independent of the underlying detector, we use the ground truth bounding boxes and perform the classification on the cropped image patches.
Further, we extract additional parts, as described in Sect.~\ref{sub:part_based_classification}, and combine the predictions on these parts with the predictions on the entire image.
For the classification, we use the InceptionV3 CNN architecture~\cite{inception}.
Here, we also decided on the default input size of $299\times299$ for both the images and parts.
Furthermore, we investigate the effect of two different pre-training methods.
As the typical choice, we use ImageNet~\cite{ImageNet} pre-training since most of the deep learning frameworks implementing different CNN architectures also provide weights for these architectures pre-trained on the ImageNet dataset.
Additionally, we use pre-training on the iNaturalist~\cite{iNaturalist} dataset provided by Cui~\etal~\cite{Cui_2018_CVPR_large}.
Data used in this kind of pre-training is more related to the domain of animals, in our case to the domain of insects, which can also be seen in the improvement of the classification accuracy in Table~\ref{tab:classification_results:eumoths}.

Finally, we evaluate the proposed pipeline as a whole: given an uncropped image, like in Figure~\ref{fig:examples:eumoths}, we first apply the moth detector and then perform the classification on the resulting image patches.
We compare this setup to a classifier that performs the classification on the initial uncropped images.

We further evaluate the classifier on the dataset provided by Bjerge~\etal~\cite{bjerge2021automated}.
Unfortunately, the authors report only F1-scores of their classification and do not provide their training-test split.
Nevertheless, we performed five-fold cross-validation with the same train-test ratios as in the original paper.
For each of the folds, we repeated the training ten times, like in the previous experiments.


\subsection{Datasets}
\label{sub:datasets}

\input{sections/04_results/10_examples}

\noindent\textbf{Moth Classification and Counting (MCC) Dataset\footnote{\url{https://github.com/kimbjerge/MCC-trap}}:}\quad Created by Bjerge~\etal~\cite{bjerge2021automated}, the subset for the classification consists of eight moth species with \num{250} images for each species, resulting in a dataset of \num{2000} images.
The authors used an 80:20 training-test split of the data but did not provide their specific split.
Additionally, there are two more classes: a background class and a class for a wasp species.
We ignore these classes and perform the training and the evaluation only on the eight moths species.
An individual from every class is shown in Figure~\ref{fig:examples:mcc}.
Compared to the EU-Moths dataset, the images are of lower quality since the authors captured them with an eight-megapixel web camera connected to a Raspberry Pi 4.

\noindent\textbf{European Moths (EU-Moths) Dataset\footnote{\url{https://www.inf-cv.uni-jena.de/eu_moths_dataset}}:}\quad From roughly \num{4700} moth species present in Central Europe\footnote{\url{http://lepiforum.org/} (accessed on \nth{6} July, 2021)}, this dataset consists of \num{200} species most common in this region.
Each of the species is represented by approximately \num{11} images.
We considered a random but balanced training-test split in eight training and three test images per species, resulting in roughly \num{1600} training and \num{600} test images in total.
To evaluate the detector, we manually annotated the bounding boxes around the insects.
Some examples of the images are shown in Figure~\ref{fig:examples:eumoths}.
The insects are photographed manually and mainly on a relatively homogeneous background.
About \pcent{92} of the images contain only a single individual like it is shown in the first two rows of Figure~\ref{fig:examples:eumoths}.
The last row of the same image depicts images with more than one insect of interest.
This scenario may happen in the final moth scanner installation, and it is crucial to test how the detector performs in this case.

This dataset yields different challenges for the detector and the classifier.
On the one hand, the detector should be able to detect insects of different sizes.
Furthermore, we require a detector with MultiBox support.
Both of these properties are fulfilled by the SSD.
On the other hand, designing a classifier that can predict species from these raw images is difficult.
As proposed in this paper, we decided to use a moth detector to locate single insects and classify these separately.
Hence, the classifier is trained on images cropped to the bounding boxes identifying a single individual.

\input{sections/04_results/20_detection_examples}

\subsection{Results on the MCC Dataset} % (fold)
\label{sub:results_on_mcc}

\input{sections/04_results/30_classification_mcc}

\paragraph{Detection:} Unfortunately, the authors do not provide any bounding box annotations.
Hence, we were not able to evaluate the performance of our moth detector on this dataset.
Nevertheless, we provide a qualitative evaluation on some of the images in Figure~\ref{fig:example_detections:mcc}.


\paragraph{Classification:}
In Table~\ref{tab:classification_results:mcc}, we compare  our classification method with the one proposed by Bjerge~\etal~\cite{bjerge2021automated} on the MCC dataset.
% Though the authors report only the F1-score, one can see that our classification accuracy achieves a near-perfect classification.
One can see that our classifier achieves near-perfect accuracies and F1-scores.
We assume the reason for these results is in the composition of the dataset.
Since Bjerge~\etal do not provide any training-test split, we have used a random split with the same ratio (80:20) as the authors in their work.
Nevertheless, one can see in Figure~\ref{fig:example_detections:mcc} some of the moth individuals do not move in different images captured in short intervals.
As a result, extracting image crops from these images would result in near-identical images in different splits after the random splitting.
Consequently, one would train and test the classifier on nearly the same data.
Nevertheless, to be comparable to the results of Bjerge~\etal, we chose the same splitting strategy, even though it may not represent the correct evaluation of the model.

\subsection{Results on the EU-Moths Dataset} % (fold)
\label{sub:results_on_eumoths}

\input{sections/04_results/41_detection_eumoths}

\paragraph{Detection:}
We split this experiment into two parts: (1) we evaluated the detection performance on the entire dataset, and (2) we split the dataset into two subsets of distinct classes.
The first part evaluates the standard performance of the detector.
The second part of the experiment evaluates how good the detector performs on classes not seen during the training.
This scenario is essential since not all species may be available at training time in the real-world setup.
One could train the detector on a dataset captured at one location and deploy it at another one.
Furthermore, a detector able to localize moth species not seen at training time is beneficial for novelty detection, active learning, and incremental learning algorithms~\cite{Brust2017AVM,Brust2020AIL}.

Table~\ref{tab:detection_results} shows the detection results for both experiments.
As previously mentioned, we report the mean	average precision for IoU thresholds 0.75 and 0.5 (mAP@0.75 and mAP@0.5).
The first row shows the results for the first experiment.
The detector seems to be quite precise if we consider the challenges of the dataset.
The lower part of Table~\ref{tab:detection_results} further shows the cross-subset results.
Here we can see that the mAP@0.75 performance drops compared to the previous experiment, and the standard deviation increases.
Both are explainable because, for the second experiment, we used only half of the classes for the training.
Furthermore, mAP@0.5 performance remains comparable to the first experiment, which shows the moth detector's reliability for unseen classes.

Additionally, Figure~\ref{fig:example_detections} depicts qualitative results of the detector.
In Figure~\ref{fig:example_detections:eumoth}, we estimated the bounding boxes (in \emph{black}) for some of the images of the EU-Moths dataset.
We visualized the ground truth bounding boxes (in \emph{blue}) and the resulting IoU between the prediction and the ground truth.
The detector's most significant challenge seems to be insects located too close to each other (second last example in the final row).
% Since we had no ground truth bounding-box annotations for the MCC dataset, we visualized in Figure~\ref{fig:example_detections:mcc} only the estimates of our moth detector.

\input{sections/04_results/42_classification_eumoths}
\paragraph{Classification:}
Table~\ref{tab:classification_results:eumoths} shows the results of the classification.
We compare different training and pre-training methods and whether the additional information in the form of parts benefits the classification accuracy.

First, one can see that fine-tuning the entire CNN instead of using it only as feature extractor results in an improvement of the recognition rate by roughly \pcent{26} and \pcent{4} for CNNs pre-trained on ImageNet~\cite{ImageNet} and iNaturalist 2017~\cite{iNaturalist} datasets, respectively.
Though training the entire CNN results in longer training times and is computationally more expensive, the improvements are visible.

Second, the choice of the pre-training dataset is also crucial.
Replacing the typical CNN weights provided by almost every deep learning framework pre-trained on the ImageNet dataset with the ones proposed by Cui~\etal~\cite{Cui_2018_CVPR_large} leads to a further improvement.
The later pre-training increases the accuracy by approximately \pcent{1} if the entire CNN is trained.
It also yields remarkable benefits if choosing computationally cheaper training of only the final classification layer, namely an improvement of \pcent{20}.

Finally, employing additional information in the form of parts improves the classification accuracies by approximately \SIrange[range-units=single]{2}{2.6}{\%} depending on the chosen pre-training.
We achieved the best results with the part-based setup: \pcent{91.50} and \pcent{93.13} with ImagenNet and iNaturalist pre-training, respectively.



\paragraph{Entire Pipeline:}
\input{sections/04_results/43_pipeline_eumoths}
In this experiment, we evaluate the entire pipeline as presented in Sect.~\ref{sec:methods}.
For this purpose, we couple every trained detector with every trained classifier and observe the resulting classification performance.
This way, we report the mean accuracy of \num{100} detector-classifier combinations in the last row of Table~\ref{tab:pipeline_results}.
As a baseline method, we trained ten CNN classifiers on the original uncropped images.
One can see that the preceding detector improves the classification accuracy by approximately \pcent{9}.

