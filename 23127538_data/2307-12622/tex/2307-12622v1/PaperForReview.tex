% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{svg}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{diagbox}
\usepackage{bbding}

\newtheorem{assumption}{Assumption}
%\renewcommand{\theassumption}{\arabic{assumption}.}
\newtheorem{corollary}{Corollary}
%\renewcommand{\thecorollary}{\arabic{corollary}.}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Phase Match for Out-of-Distribution Generalization}

\author{Chengming Hu \quad Rui Wang \quad Hao Chen \quad Zhouwang Yang\thanks{Corresponding author}\\
University of Science and Technology of China\\
{\tt\small \{cmhu,rui\_wang,ch330822\}@mail.ustc.edu.cn, yangzw@ustc.edu.cn}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
The Fourier transform, serving as an explicit decomposition method for visual signals, has been employed to explain the out-of-distribution generalization behaviors of Convolutional Neural Networks (CNNs). Previous research and empirical studies have indicated that the amplitude spectrum plays a decisive role in CNN recognition, but it is susceptible to disturbance caused by distribution shifts. On the other hand, the phase spectrum preserves highly-structured spatial information, which is crucial for visual representation learning. In this paper, we aim to clarify the relationships between Domain Generalization (DG) and the frequency components by introducing a Fourier-based structural causal model. Specifically, we interpret the phase spectrum as semi-causal factors and the amplitude spectrum as non-causal factors. Building upon these observations, we propose Phase Match (PhaMa) to address DG problems. Our method introduces perturbations on the amplitude spectrum and establishes spatial relationships to match the phase components. Through experiments on multiple benchmarks, we demonstrate that our proposed method achieves state-of-the-art performance in domain generalization and out-of-distribution robustness tasks.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

%Under the typical assumption that the training data and the testing data are independent and identically distributed (i.i.d.)~\cite{lecun2015deep,goodfellow2016deep}, Convolutional Neural Networks (CNNs) have achieved excellent results on many visual downstream tasks. However, in real-world scenarios, CNNs usually have poor performance due to the random and uncertain distribution shift (a.k.a. domain shift) between training data and testing data. As a result, Domain Generalization (DG)~\cite{muandet2013domain}, which aims to learn a machine learning model that can generalize to unseen data distributions, has been proposed and has gained increasing attention.
%
Convolutional Neural Networks (CNNs) have demonstrated exceptional performance on various visual downstream tasks, assuming the typical independent and identically distributed (i.i.d.) setting for training and testing data~\cite{lecun2015deep,goodfellow2016deep}. However, in real-world scenarios, the CNNs often exhibit subpar performance due to random and uncertain distribution shifts, also known as domain shifts, between the training and testing data. Consequently, researchers have introduced Domain Generalization (DG)~\cite{muandet2013domain}, an approach that aims to enable machine learning models to generalize to unseen data distributions, attracting increasing attention in recent times.

%Since the target domain is inaccessible during training, mainstream DG studies~\cite{arjovsky2019invariant,li2018domain,mahajan2021domain,ilse2020diva,peng2019domain,mahajan2021domain,lv2022causality} mainly focus on extracting invariant representation from source domains, which can be well generalized to the target domain. Along another line, data augmentation~\cite{hendrycks2019augmix,Yun_2019_ICCV,zhang2018mixup,huang2017arbitrary,nuriel2021permuted,zhou2021domain,li2022uncertainty}, a way to simulate domain shifts or attacks without changing the label, can also be thought of as a way to enforce the network to extract invariant representations under perturbations (\eg, flip, brightness, contrast, style.). 

Mainstream Domain Generalization (DG) studies~\cite{arjovsky2019invariant,li2018domain,mahajan2021domain,ilse2020diva,peng2019domain,mahajan2021domain,lv2022causality} primarily focus on extracting invariant representations from source domains that can be effectively generalized to the target domain, which remains inaccessible during training. Another approach involves data augmentation~\cite{hendrycks2019augmix,Yun_2019_ICCV,zhang2018mixup,huang2017arbitrary,nuriel2021permuted,zhou2021domain,li2022uncertainty}, a technique to simulate domain shifts or attacks without altering the label. Data augmentation can also be viewed as a means to compel the network to extract invariant representations under various perturbations (\eg, flip, brightness, contrast, style.).

% Figure environment removed

%Several recent studies~\cite{chen2021amplitude,guo2018low,liu2021spatial,sharma2019effectiveness,wang2020high} have specifically examined the explanations for CNN's generalizability in frequency domain. These studies' experiments have demonstrated the sensitivity of CNNs to the amplitude spectrum. \Cref{fig:toy_exp} illustrates our toy-example experiment, where we initialize a random amplitude spectrum and reconstruct the images using their phase spectrum, respectively. We observe that the ResNet-18~\cite{he2016deep} pre-trained on ImageNet-1K~\cite{deng2009imagenet} (semi)correctly predicts the categories of raw images (green arrows), but wrongly predicts the category of reconstructed images (yellow arrows). While for the human visual system, despite extreme corruptions, humans can still accurately recognize the object, (\ie, house and dog) in the reconstructed image with a high degree of certainty, suggesting that the amplitude spectrum does not affect recognition and localization. Besides, the reconstructed images still perserve highly structured spatial information, \eg, contour, edge, from the original image with a high consistency. Based on the above observations, we can draw two conclusions: (1) existing pre-trained CNNs rely excessively on the amplitude spectrum, and (2) the amplitude information is redundant as a visual signal for the human visual system. Hence, removing the impacts of amplitude information can significantly improve the signal-to-noise ratio (SNR) and thus enhance the network's representation learning capabilities. 

Recent studies~\cite{chen2021amplitude,guo2018low,liu2021spatial,sharma2019effectiveness,wang2020high} have focused on exploring the explanations for CNN's generalizability in the frequency domain. These experiments have demonstrated the sensitivity of CNNs to the amplitude spectrum. In \cref{fig:toy_exp}, we present our toy-example experiment, initializing a random amplitude spectrum, and reconstructing the images using their phase spectrum. Our observations reveal that the ResNet-18~\cite{he2016deep} pre-trained on ImageNet-1K~\cite{deng2009imagenet} (semi)correctly predicts the categories of raw images (indicated by green arrows), but inaccurately predicts the category of the reconstructed images (indicated by yellow arrows). In contrast, the human visual system remains proficient in recognizing objects (\eg, house and dog) in the reconstructed image despite extreme corruptions, suggesting that the amplitude spectrum does not significantly impact recognition and localization. Based on these observations, we draw two conclusions: (1) existing pre-trained CNNs heavily rely on the amplitude spectrum, and (2) the amplitude information is redundant as a visual signal for the human visual system. Consequently, eliminating the influence of amplitude information can substantially improve the signal-to-noise ratio (SNR) and enhance the network's representation learning capabilities.

%Moreover, note that the reconstructed images still perserve the highly structured spatial information, \ie, for each \textit{patch} from the image pairs, structural information (\eg, countour, edge) is highly consistent, which is crucial for recognition and positioning. However, existing studies have not focused on the spatial information perserved in the image patches. Considering the invariance of phase spectra and human's prediction outcomes, we argue that the secret of robust visual systems is the utilization of the phase spectrum perserving spatial information of an image. Therefore, buliding spatial relationships for the phase spectrum is a key for a generalizable and robust network.

Additionally, it is essential to note that the reconstructed images still preserve highly structured spatial information. For each \textit{patch} in the image pairs, the structural information (\eg, contour, edge) remains remarkably consistent, which significantly contributes to recognition and positioning tasks. However, existing studies have largely overlooked the spatial information preserved in the image patches. Taking into account the invariance of phase spectra and humans' successful prediction outcomes, we assert that the secret to robust visual systems lies in the utilization of the phase spectrum, which effectively preserves spatial information in an image. Consequently, establishing spatial relationships for the phase spectrum becomes a critical aspect of achieving a generalizable and robust network.

%This concept aligns with how the human visual system filters out amplitude information and further utilize the phase information to obtain a more robust image representation.

%In this paper, we seek to address DG problems from the perspective of frequency. We present \textit{Phase Match} (PhaMa) as shown in \cref{fig:framework}. To enhance generalizability against amplitude perturbations caused by domain shifts, we first introduce perturbations on the amplitude spectrum for adversarial training. We randomly select two images from source domains and mix their amplitude spectra by linear interpolation, and both send the original images and the augmenteds to the network. Then, we introduce a patch contrastive loss~\cite{park2020contrastive} to \textit{match} the patch representations from the image pairs. The impact from the amplitude component is further alleviated and the spatial relationship of the phase spectrum is built, thus the network can focus more on the phase spectrum of the image.

This paper addresses Domain Generalization (DG) problems from the frequency perspective and introduces \textit{Phase Match} (PhaMa) as shown in \cref{fig:framework}. To enhance generalizability against amplitude perturbations caused by domain shifts, we adopt perturbations on the amplitude spectrum for adversarial training. Specifically, we randomly select two images from the source domains and mix their amplitude spectra using linear interpolation. Both the original images and the augmented versions are then fed into the network. Subsequently, we introduce a patch contrastive loss~\cite{park2020contrastive} to encourage matching of patch representations from the image pairs. This approach further alleviates the impact of the amplitude component and establishes the spatial relationship of the phase spectrum, allowing the network to prioritize the phase spectrum of the image.

Our contributions are summarized as: (1) We present an intuitive causal view for domain generalization with the Fourier transform and specify the causal/non-causal factors with association to the Fourier spectrum, (2) a method called \textit{Phase Match} is proposed to enforce the network to focus more on the phase spectrum for generalizable representation learning, and (3) results from comprehensive experiments show that our method outperforms state-of-the-art methods on many DG benchmarks.
%------------------------------------------------------------------------
\section{Related Work}
\label{sec:relwo}

\noindent \textbf{Domain Generalization.} Domain generalization (DG) aims to learn a model from the source domains that has high performance in unseen target domains. Data augmentation is a widely used technique in machine learning to enhance the out-of-distribution generalization ability of the models~\cite{wang2022generalizing,zhou2022domain}. MixUP~\cite{zhang2018mixup} adopts linear interpolations between two input samples and smooth the label. CutMix~\cite{Yun_2019_ICCV} generates training images by cutting and pasting from raw images. From the frequency perspective, recent works~\cite{chen2021amplitude,xu2021fourier,lv2022causality} utilize the property of phase and amplitude of the Fourier spectrum  into DG~\cite{xu2021fourier,lv2022causality} and Robustness~\cite{chen2021amplitude}. Motivated by the observation that image style can be captured from latent feature statistics~\cite{ulyanov2016instance,huang2017arbitrary}, many DG methods utilize adaptive instance normalization (AdaIN)~\cite{huang2017arbitrary} and its variants~\cite{nuriel2021permuted,zhou2021domain,li2022uncertainty} to synthesize novel feature statistics. Another way to tackle DG problems is domain-invariant representation learning. For example, MMD-AAE~\cite{li2018domain} regularizes a multi-domain autoencoder by minimizing the Maximum Mean Discrepancy (MMD) distance. \cite{zhao2020domain} minimizes the KL divergence between the conditional distributions of different training domains. DAL~\cite{peng2019domain} disentangles domain-specific features using adversarial losses. Recently, contrastive learning is introduced into DG. PDEN~\cite{li2021progressive} utilizes contrastive learning for single domain generalization. SelfReg~\cite{kim2021selfreg} aligns the positive pairs by contrastive learning. PCL~\cite{yao2022pcl} proposes a proxy-based contrastive learning method for DG.

\vskip 5pt
\noindent \textbf{CNN Behaviours from Frequency Perspective.} A wide range of frequency-based researches on CNN have beeen conducted. \cite{guo2018low} conducts adversarial attacks on low-frequency components and reveals that CNN only utilize the low-frequency components for prediction. \cite{sharma2019effectiveness} demonstrates that CNN is vulnerable under low-frequency perturbations. On the other hand, \cite{wang2020high} observes that high-frequency components is important to the generalization of CNN. Further, APR~\cite{chen2021amplitude} offers a qualitative study for both amplitude and phase spectrum, and argues that the phase spectrum is crucial for robust recognition. However, the spatial relationships of the phase spectrum remains unclear, which we aim to explore from a contrastive view.

\vskip 5pt
\noindent \textbf{Contrastive Learning.} As a simple and powerful tool for visual representation learning, there has been a surge of impressive studies~\cite{caron2021emerging,chen2020simple,he2020momentum,wu2018unsupervised} on contrastive learning~\cite{hadsell2006dimensionality}. The only prerequisite of contrastive learning is a definition of positive-negative pairs. InstDisc~\cite{wu2018unsupervised} uses a memory-bank to store the features for contrast. MoCo~\cite{he2020momentum} builds a dictionary with a quene and a momentum upadte encoder. SimCLR~\cite{chen2020simple} introduces a learnable projection head between encoder networks and the contrastive loss. DINO~\cite{caron2021emerging} trains the Vision Transformer (ViT)~\cite{dosovitskiy2020vit}, which contains explicit information about the semantic segmentation.

%------------------------------------------------------------------------
\section{Method}
\label{sec:method}

\subsection{Problem Definition}
\label{sec:pd}
Given a training set consisting of $M$ source domains $\mathcal{D}_{s}=\left\{ \mathcal{D}_{i}|i=1,\dots,M \right\}$ where $\mathcal{D}_{i}=\{(x_{i}^{j}, y_{i}^{j})\}_{j=1}^{n_{i}}$ denotes the $i$-th domain. The goal of domain generalization is to learn a robust and generalizable model $g:\mathcal{X}\to \mathcal{Y}$ from the $M$ source domains and achieve a minimum prediction error on the traget domain $\mathcal{S}_{t}$, which is ininaccessible during training:
\begin{equation}
	\min_{g} \mathbb{E}_{(x, y)\in \mathcal{D}_{t}}\left[\ell(g(x),y) \right].
\end{equation}

In this paper, we consider an object recognition model $g(\cdot;\theta):\mathcal{X}\to \mathbb{R}^{N}$ comprising an encoder $E$ and a classifier $C$, where $\theta$ is the model parameters, and $N$ denotes the number of catrgories in the target domain.

\subsection{A Frequency Causal View for DG}
\label{sec:dg_fcv}

%To better understand the relationship between the Fourier spectra and DG, we first consider domain generalization as a domain-specific image generation task from a causal view. As illustrated in \cref{fig:scm_g}, given information about an \textit{object} (O) and a \textit{domain} (D), we generate a domain-specific image $X$ with category $Y$. The generating process first samples O and D, which are mutually independent. The pixels of the image $X$ are constructed using both latent embeddings of O and D, while the corresponding label $Y$ is only caused by O. We assume the latent embeddings caused by O and D as causal factors (C) and non-causal factors (N), respectively.

To gain deeper insights into the relationship between the Fourier spectra and DG, we initially consider domain generalization as a domain-specific image generation task from a causal perspective. As depicted in \cref{fig:scm_g}, given information about an \textit{object} (O) and a \textit{domain} (D), we generate a domain-specific image $X$ with its corresponding category label $Y$. The image generating process first samples O and D, which are mutually independent. The pixels of the image $X$ are then constructed using the latent embeddings of both O and D, whereas the category label $Y$ is solely influenced by O. In this context, we consider the latent embeddings caused by O and D as causal factors (C) and non-causal factors (N), respectively.

% Figure environment removed

Based on \textit{Reichenbach’s Common Cause Principle}~\cite{reichenbach1956direction}, the SCM of domain-specific image generation process can be formulated as follows:
\begin{equation}
	\begin{aligned}
		C &= U_{O}, N = U_{D},\\
		X &= g_{x}(C, N; \theta) + U_{X}, \\
		Y &= g_{y}(C; \theta) + U_{Y},
		\label{eq:scm_g}
	\end{aligned}
\end{equation}
where $U=\{U_{O}, U_{D}, U_{X}, U_{Y} \}$ denotes the \textit{exogenous variables}, and $V=\{X, Y, C, N \}$ denotes the \textit{endogenous variables}. Note that $C$ and $N$ satisfy the following conditions: \textbf{(1)} $C \not \! \perp \!\!\! \perp O$, $N \not \! \perp \!\!\! \perp D$; \textbf{(2)} $C \perp \!\!\! \perp D \mid O$, $N \perp \!\!\! \perp O \mid D$. The latter condition ensures that $C$ is invariant with the same object across different domains, and $N$ is independent of the object. Unfortunately, due to the unobservation of causal/non-causal factors, we cannot directly formulate $X = g_{x}(C, N; \theta) + U_{X}$, which remains a challenging problem for causal inference~\cite{gelman2011causality} and poses a further obstacle to modeling the distribution from $X$ to $Y$. 

Based on our observation in \cref{sec:intro} and the well-known property of Fourier transform: The phase spectrum preserves high-level semantics of the image while the amplitude spectrum contains low-level statistics~\cite{hansen2007structural,lv2022causality,oppenheim1979phase,oppenheim1981importance,xu2021fourier,yang2020fda}. We believe that introducing Fourier transform into causal inference might help learn causal representations. Hence, with reference to~\cite{chen2021amplitude}, we make the following assumption for components of the Fourier transform:

\begin{assumption}
	The phase component of the Fourier spectrum is dependent on both the object and domain information. The amplitude component is only dependent on the domain information.
	\label{ass:1}
\end{assumption}

With Assumption~\ref{ass:1}, we can have a formal statement for the generation process:
\begin{corollary} 
	The category label of the generated image is only dependent on the phase spectrum, the pixels of the image is constructed from both the phase and amplitude spectrum.
\label{coro:1}
\end{corollary}

The proof is omitted because \Cref{coro:1} has been empirically  verified in \cref{sec:intro,} and previous work~\cite{chen2021amplitude}. Therefore the corollary can serve as the causal explanation to generalizability of CNN in DG. 

With \Cref{coro:1}, we treat the phase spectrum $\mathcal{P}$ as the semi-causal factors (note that a cusal relation between domain (D) and the phase spectrum ($\mathcal{P}$) is introduced) and the amplitude spectrum $\mathcal{A}$ as the redundant non-causal factors. We transform the general form \cref{eq:scm_g} to the following specified form (depicted in \cref{fig:scm_s}):
\begin{equation}
	\begin{aligned}
		\mathcal{P} &= U_{O} + U_{D}, \mathcal{A} = U_{D},\\
		X &= g_{x}(\mathcal{P}, \mathcal{A}; \theta) + U_{X}, \\
		Y &= g_{y}(\mathcal{P}; \theta) + U_{Y}.
	\end{aligned}
\label{eq:scm_s}
\end{equation}

%In this way, we present an intuitive view for the SCM of human visual system from the Fourier perspective. With accessibility to the factors in causal inference, we can conduct operations on the specified factors instaed of blindly exploring in the highly-entangled latent space. Since the amplitude spectrum has no association with the catrgory label, reducing or even eliminating its impact can greatly improve the SNR of the image, thus helping the model to learn the intrinsic features of objects.

Here, we present an intuitive view of the Structural Causal Model (SCM) of the human visual system from the frequency perspective. By having access to the factors in causal inference, we can perform operations on the specified factors instead of blindly exploring the highly-entangled latent space. As the amplitude spectrum has no association with the category label, reducing or even eliminating its impact can significantly improve the Signal-to-Noise Ratio (SNR) of the image, thereby aiding the model in learning the intrinsic features of objects.

\subsection{Phase Match}
\label{sec:sicreg}

%From the above perspective, DG is a form of generating domain-specific images, and the phase spectrum of the image is specified to be the \textit{semi-causal factors}. Our hypothesis is that a roubust representation is invariant to the phase spectrum of the object under huge perturbations of the amplitude spectrum. Based on this motivation, we present Phase Match as described next.

From the above perspective, Domain Generalization (DG) can be viewed as a process of generating domain-specific images, where the phase spectrum of the image is considered the \textit{semi-causal factors}. Our hypothesis is that a robust representation remains invariant to the phase spectrum of the object despite significant perturbations in the amplitude spectrum. Based on this motivation, we introduce Phase Match, as described next.

\vskip 5pt
\subsubsection{Data Augmentation with Amplitude Perturbation}
\label{sec:aada}
As is discussedd in \cref{sec:intro} and \cref{sec:dg_fcv}, the network is vulnerable to the amplitude perturbations, which is usually caused by domain shifts. To make the network robust to this perturbation, an intuitive way is to add attacks for the training examples to get adversarial gradients. Considering the semantic-preserving property of Fourier phase spectrum~\cite{oppenheim1979phase,oppenheim1981importance,piotrowski1982demonstration}, we introduce perturbations upon the amplitude information by linearly interpolating between the amplitude spectrums of two randomly-sampled images, while maintaining the phase spectrums unchanged as in~\cite{xu2021fourier,lv2022causality}.

Formally, given an image $x\in \mathbb{R}^{H\times W\times 3}$, we can obtain the complex spectrum $\mathcal{F}\in \mathbb{C}^{H\times W\times 3}$ computed across the spatial dimension within each channel by FFT~\cite{nussbaumer1981fast}:
\begin{equation}
	\mathcal{F}(x)(u, v) = \sum_{h=0}^{H-1}\sum_{w=0}^{W-1} x(h, w) \cdot e^{-j2\pi(\frac{h}{H}u+\frac{w}{W}v)}, 
\label{eq:fft}
\end{equation}
where $H$ and $W$ represent the height and width of the image respectively. 

We then perturb the amplitude components of two images $x_{o}, x_{o}'$ from arbitrary source domains by the way as MixUP~\cite{zhang2018mixup}:
\begin{equation}
	\hat{\mathcal{A}}_{o}^{o'}=(1-\lambda) \mathcal{A}\left(x_{o}\right)+\lambda \mathcal{A}\left(x_{o}'\right),
	\label{eq:amp_mix}
\end{equation}
where $\lambda \sim U(0, \eta)$ and $\eta$ is a hyperparameter which controls the scale of perturbation. The pahse-invariant image $x_{a}$ is then generated from the combination of oroginal phase component and mixed amplitude component:
\begin{equation}
	x_{a} = \mathcal{F}^{-1}(\hat{\mathcal{A}}_{o}^{o'} \otimes e ^{-j \cdot \mathcal{P}(x_{o})}).
	\label{eq:ifft}
\end{equation}

The augmented image pairs and the corresponding original labels are both fed to the model for training. The prediction loss is formulated as the standard Cross Entropy Loss:
\begin{equation}
	\mathcal{L}^{o(a)}_{cls} = -y^{T} \log (\sigma(g(x_{o(a)};\theta))),
\end{equation}
where $\sigma$ denotes the \textit{softmax} function. 

%By leveraging this simple operation, we make the network robust to unknown amplitude shifts. Since the phase component is intact during this operation, we actually bulid positive pairs, which paves the way for our subsequent contrastive regularization for improveing the network's ability to capture the phase features.

By leveraging this simple operation, we enhance the network's robustness to unknown amplitude shifts. Importantly, since the phase component remains intact during this operation, we effectively build positive pairs, which, in turn, paves the way for our subsequent contrastive regularization to improve the network's ability to capture the phase features.

% Figure environment removed

\vskip 5pt
\subsubsection{Matching Phase with Cross Patch Contrast} 
\label{sec:pm}
%At the core of our method is \textit{matching} the phase component of the original image and the corresponding augmented sample, \ie, the representation of the image pairs should be similar or even same. According to our research, currently there is no neural network-based method that specifically focuses on extracting phase spectrum features, which means that we cannot dierctly align the phase embeddings with common metrics (L1, L2, KLD, \etc). Hence, we introduce contrastive learning~\cite{hadsell2006dimensionality}, an unsupervised learning method to measure the similarities of sample pairs in a representation space, to our method.

At the core of our method is \textit{matching} the phase component of the original image with the corresponding augmented sample; in other words, the representation of the image pairs should exhibit similarity or even be the same. Our research reveals that there is currently no neural network-based method that specifically focuses on extracting phase spectrum features. Consequently, we face challenges in directly matching the phase embeddings with common metrics such as L1, L2, KLD. To address this, we incorporate contrastive learning~\cite{hadsell2006dimensionality}, an unsupervised learning method that measures the similarities of sample pairs in a representation space, into our method.

Given the encoded hierarchal feature maps of an image $f_{j}(x)$, where $j$ denotes the index of the feature map, \eg, for a ResNet-like network, $j\in\left\{1, 2, 3, 4 \right\}$. Our choice of the feature representations is the \textit{last-two-levels} in a hierarchal network, in that the high-level features of the network are more tend to extract semantic-related information~\cite{zeiler2014visualizing}. Specifically, for the two hierarchal representations $f_{3}\in \mathbb{R}^{C_{3}\times H_{3}\times W_{3}}$ and $f_{4}\in \mathbb{R}^{C_{4}\times H_{4}\times W_{4}}$, we resize $f_{4}$ by bilinear interpolation and concatenate them in the channel dimension, denoted as $z$, and send them to a 2-layer nonlinear projection head $p(z) = W_{1}\sigma(W_{2}z)$ as in~\cite{chen2020simple}.

Inspired by the high consistency of the phase spectrum in preserving spatial structures, \ie, for the same position in the image pairs in \cref{sec:intro}, contours and edges are highly consistent. We aim to establish associations for each patch in the spatial dimension, \ie, make the patch representations from the same location \textit{similar}, and \textit{push away} those from different positions as far as possible. In this way, the encoded representations from each patch are consistent under the amplitude perturbation, and the network can learn from the invariant phase spectrum. Therefore, the following PatchNCE loss~\cite{park2020contrastive} is considered:
\begin{equation}
		\mathcal{L}_{patch}^{o2a'}= -\sum_{i} \log \frac{\exp \left(p_{o}^{i}\cdot p_{a'}^{i} / \tau\right)}{\exp \left(p_{o}^{i}\cdot p_{a'}^{i} / \tau\right) + \sum_{j} \exp \left(p_{o}^{i}\cdot p_{a'}^{j} / \tau\right)},
		\label{eq:loss_match}	
\end{equation}
where $p_{o}$ and $p_{a'}$ are from the encoder and the mumentum encoder respectively, $i(j)\in\left\{1, \ldots, P \right\}$ denotes the index of the patch, $\left(\cdot \right)$ denotes the innear product, $\tau$ is a temperature parameter. For the $i$th patch in the original image $p_{o}^{i}$, patches in other location in the augmented image $p_{a'}^{j}(j\ne i)$ are treated as negative samples, the contrast can then be set as a $P$-way classification problem (\cref{fig:patchnce}).

%where the objective distinguishes the patch representation. The spatial relationship between different patches is thus built to further match the phase information.

However, existing pre-trained networks~\cite{he2016deep,dosovitskiy2020vit} extract very different representations (\cref{sec:intro}) under huge perturbations of amplitude information, which tends to cause gradient collapsing during the back-propagation in our experiments (\cref{sec:abl}). To alleviate the impact caused by amplitude perturbations:
\begin{itemize}
\item For both the original and augmented images, we adopt a momentum-updated encoder to ensure consistent representation extraction, following the approach proposed in~\cite{he2020momentum}. Specifically, we update the parameters of the encoder, denoted as $\theta$, and the momentum encode, denoted as $\theta_{m}$, using the following update rule:
	\begin{equation}
		\theta_{m} \leftarrow m \theta_{m} + (1-m) \theta.
		\label{eq:moup}
	\end{equation}
\item We perform the patch contrast operation (\cref{eq:loss_match}) \textbf{\textit{across}} the patch representation of the original image and the agumented from the encoder and the momentum encoder, respectively. The contrastive loss is defined as:
	\begin{equation}
		\mathcal{L}_{contr} = \mathcal{L}_{patch}^{o2a'} + \mathcal{L}_{patch}^{a2o'}.
	\end{equation} 
\end{itemize}

% Figure environment removed

The overall objective function of our proposed method can be formulated as follows:
\begin{equation}
	\mathcal{L}_{PhaMa}=\frac{1}{2}(\mathcal{L}_{cls}^{o}+\mathcal{L}_{cls}^{a})+\beta \mathcal{L}_{contr},
	\label{eq:loss_total}
\end{equation}
where $\beta$ is a trade-off parameter.
%------------------------------------------------------------------------
\section{Experiment}
\label{sec:exp}
In this section, we conduct experiments on several benchmarks to evaluate the effectiveness of our method in improving generalization ability of networks, including multi-domain classification and robustness towards corruptions. More details about the implementation, results analysis and ablation studies will be discussed in the following.

\subsection{Multi-domain Classification}
\label{sec:mdc}

\subsubsection{Implementation Details}
\label{sec:mdc_imp}

\noindent \textbf{Datasets.} We evaluate the generalization ability of our method on the following 3 datasets:
(1)\textbf{Digits-DG}~\cite{zhou2020deep}, consisting of four datasets MNIST~\cite{lecun1998gradient}, MNIST-M~\cite{ganin2015unsupervised}, SVHN~\cite{37648}, SYN~\cite{ganin2015unsupervised};
(2)\textbf{PACS}~\cite{li2017deeper}, a commonly used benchmark for domain generalization, comprising of 9991 images from four distinct domains: Art Painting, Cartoon, Photo, Sketch; 
(3)\textbf{Office-Home}~\cite{venkateswara2017deep}, which contains around 15,500 images of 65 categories from four domains: Artistic, Clipart, Product and Real World.

\vskip 5pt
\noindent \textbf{Training.} For all DG benchmarks, we follow the leave-one-domain-out protocol with officical train-val split, and report the classification accuracy (\%) on the entire held-out target domain. We also use the standard augmentation, which consists of random resized cropping, horizontal flipping and color jittering. For Digits-DG, all images are resized to $32\times 32$. We train the encoder (same as in~\cite{zhou2020deep}) from scratch using SGD, batch size 64 and weight decay of 5e-4, the learning rate is initially 0.05 and is decayed by 0.1 every 20 epochs. For PACS and Office-Home, all images are resized to $224\times 224$. We use the ImageNet pretrained ResNet~\cite{he2016deep} as the encoder, and train the network with SGD, batch size 64, momentum 0.9 and weight decay 5e-4 for 50 epochs. The initial learning rate is 0.001 and decayed by 0.1 at 80\% of the total epochs.

\vskip 5pt
\noindent \textbf{Method-specific.} We use a sigmoid ramp-up \cite{tarvainen2017mean} for $\beta$ within first 5 epochs in all experiments. The trade-off parameter $\beta$ is set to 0.1 for Digits-DG, and 0.5 for PACS and Office-Home. We also follow the common setting~\cite{wu2018unsupervised} to let $\tau=0.07$.

\vskip 5pt
\noindent \textbf{Model Selection.} We use training-domain validation set for model selection. Specifically, we train our model on the training splits of all source domains and choose the model maximizing the accuracy on the overall validation set.


\subsubsection{Results Analysis}
\label{sec:mdc_ra}

\begin{table}[t]
\caption{Leave-one-domain-out classification accuracy (\%) on Digits-DG.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccc|c}
	\toprule
	Method&  MNIST&  MNIST-M&  SVHN&  SYN&  Avg. \\
	\midrule
	Baseline&  95.8&  58.8&  61.7&  78.6&  73.7 \\
	CCSA&  95.2&  58.2&  65.5&  79.1&  74.5 \\
	MMD-AAE&  96.5&  58.4&  65.0&  78.4&  74.6 \\
	CrossGrad&  96.7&  61.1&  65.3&  80.2&  75.8 \\
	DDAIG&  96.6&  \textbf{64.1}&  68.6&  81.0&  77.6 \\
	Jigen&  96.5&  61.4&  63.7&  74.0&  73.9 \\
	L2A-OT&  96.7&  63.9&  68.6&  83.2&  78.1 \\
	MixStyle&  96.5&  63.5&  64.7&  81.2&  76.5 \\
	FACT&  96.8&  63.2  &\textbf{73.6}  &89.3  &80.7  \\
	\midrule
	PhaMa (\textit{ours})&  \textbf{97.3}&  63.9&  73.2&  \textbf{90.2}&  \textbf{81.1} \\
	\bottomrule
\end{tabular}}
\label{tab:digits_dg}
\end{table}

\begin{table}[t]
\caption{Leave-one-domain-out classification accuracy (\%) on PACS with ResNet pretrained on ImageNet.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccc|c}
	\toprule
	Method&  Art&  Cartoon&  Photo&  Sketch&  Avg.\\
	\midrule
	\multicolumn{6}{c}{ResNet-18} \\
	\midrule
	Baseline&  77.6&  76.7&  95.8&  69.5&  79.9 \\
	MixUP&  76.8&  74.9&  95.8&  66.6&  78.5 \\
	CutMix&  74.6&  71.8&  95.6&  65.3&  76.8 \\
	pAdaIN&  81.7&  76.6&  96.3&  75.1&  82.5 \\
	MixStyle&  82.3&  79.0&  96.3&  73.8&  82.8 \\
	DSU&  83.6&  79.6&  95.8&  77.6&  84.1 \\
	MetaReg&  83.7& 77.2& 95.5& 70.3& 81.7 \\
	JiGen&  79.4&  75.2&  96.0&  71.3&  80.5 \\
	MASF&  80.2&  77.1&  94.9&  71.6&  81.1 \\
	L2A-OT&  83.3&  78.0&  96.2&  73.6&  82.8 \\
	RSC (\textit{our imp.})&  80.5&  78.6&  94.4&  76.0&  82.4 \\
	MatchDG&  81.3&  \textbf{80.7}&  96.5&  79.7&  84.5 \\
	SelfReg&  82.3&  78.4&  96.2&  77.4&  83.6 \\
	FACT& \textbf{85.3}&  78.3&  95.1&  79.1&  84.5 \\
	\midrule
	PhaMa (\textit{ours})&  84.8&  79.1&  \textbf{96.6}&  \textbf{79.7}&  \textbf{85.1} \\
	\midrule
	\multicolumn{6}{c}{ResNet-50} \\
	\midrule
	Baseline&  84.9&  76.9&  97.6&  76.7&  84.1 \\
	MetaReg&  87.2&  79.2&  97.6&  70.3&  83.6 \\
	MASF&  82.8&  80.4&  95.0&  72.2&  82.7 \\
	RSC (\textit{our imp.})&  83.9&  79.5&  95.1&  82.2&  85.2 \\
	MatchDG&  85.6&  82.1&  \textbf{97.9}&  78.7&  86.1 \\
	FACT&  \textbf{89.6}&  81.7&  96.7&  \textbf{84.4}&  88.1 \\
	\midrule
	PhaMa (\textit{ours})&  \textbf{89.6}&  \textbf{82.7}&  97.2&  83.7&  \textbf{88.3} \\
	\bottomrule
\end{tabular}}
\label{tab:pacs}
\end{table}

\begin{table}[t]
\caption{Leave-one-domain-out classification accuracy (\%) on Office-Home with ResNet-18 pretrained on ImageNet.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccc|c}
	\toprule
	Method&  Art&  Clipart&  Product&  Real&  Avg.\\
	\midrule
	Baseline&  57.8&  52.7&  73.5&  74.8&  64.7 \\
	CCSA&  59.9&  49.9&  74.1&  75.7&  64.9 \\
	MMD-AAE&  56.5&  47.3&  72.1&  74.8&  62.7 \\
	CrossGrad&  58.4&  49.4&  73.9&  75.8&  64.4 \\
	DDAIG&  59.2&  52.3&  74.6&  76.0&  65.5 \\
	L2A-OT&  \textbf{60.6}&  50.1&  74.8&  \textbf{77.0}&  65.6 \\
	Jigen&  53.0&  47.5&  71.4&  72.7&  61.2 \\
	MixStyle&  58.7&  53.4&  74.2&  75.9&  65.5 \\
	DSU&  60.2&  \textbf{54.8}&  74.1&  75.1&  66.1 \\
	FACT&  60.3&  \textbf{54.8}&  74.4&  76.5&  \textbf{66.5} \\
	\midrule
	PhaMa (\textit{ours})&  60.2&  54.0&  \textbf{75.2}&  76.4&  \textbf{66.5}  \\
	\bottomrule
\end{tabular}}
\label{tab:office}
\end{table}

\noindent \textbf{Digits-DG.} 
Results are shown in \cref{tab:digits_dg}. PhaMa achieves great improvement over the baseline method and surpasses previous domain-invariant methods by a large margin. Since our method adopts the same augmentation technique as FACT~\cite{xu2021fourier}, we compare our method with it. Our method shows slight improvement over FACT, which means that matching the patch feature is more suitable for cross-domain representation.

\vskip 5pt
\noindent \textbf{PACS.} %The experiment results shown in \cref{tab:pacs} demonstrate our significant improvement over the baseline method. Our PhaMa has a huge improvement in average accuracy, especially in Art, Cartoon and Sketch. Previous DG methods tend to drop a little in Photo since it has similar domain characteristics as ImageNet dataset and the slight drop might be due to ImageNet pretraining. Nevertheless, our method can also lift/maintain performance in Photo, which means making contrast between phase components does not degrade raw representations.
The experimental results presented in \cref{tab:pacs} show a significant improvement of our PhaMa method compared to the baseline approach. Particularly, PhaMa exhibits substantial improvements in average accuracy, notably in Art, Cartoon, and Sketch domains. It is worth noting that previous DG methods tend to experience a slight drop in accuracy for the Photo domain, which shares similar domain characteristics with the ImageNet dataset, and this drop might be attributed to ImageNet pretraining. However, our method is capable of either maintaining or lifting the performance in the Photo domain, demonstrating that contrasting phase components does not degrade raw representations.

%Note that other contrast-based methods (MatchDG, SelfReg) also show competitive performance on Photo and other domains. This indicates that contrastive learning is a powerful tool for DG. Meanwhile, our PhaMa’s great performance reveals that making spatial contrast for phase component can achieve more promising results.
Note that other contrast-based methods such as MatchDG and SelfReg also demonstrate competitive performance on Photo and other domains. This observation highlights the effectiveness of contrastive learning as a powerful tool for DG. Meanwhile, the exceptional performance of our PhaMa method suggests that introducing spatial contrast for the phase component can yield even more promising results.

\vskip 5pt
\noindent \textbf{Office-Home.} Results are shown in \cref{tab:office}. It can be observed that our method brings obvious improvement over the baseline method and also achieves competitive results aganist FACT. By introducing the amplitude perturbation and patch contrastive loss, the model can learn to alleviate the amplitude impacts and focus on the phase information.

\subsection{Robustness Towards Corruptions}
\label{sec:rtc}

\subsubsection{Implementation Details}
\label{sec:rtc_imp}

\noindent \textbf{Datasets.}
We evaluate the robustness towards corruptions of our method on CIFAR-10-C, CIFAR-100-C~\cite{hendrycks2019robustness}. The two datasets are constructed by corrupting the test split of original CIFAR dataset with a total of 15 corruption types (\textit{noise, blur, weather} and \textit{digital}). Note that the 15 corruptions are not introduced in training.

\vskip 5pt
\noindent \textbf{Training.} Following~\cite{chen2021amplitude}, we evaluate our method on various architectures of networks, including ResNet-18~\cite{he2016deep}, 40-2 Wide-ResNet~\cite{zagoruyko2016wide}, DenseNet-BC ($k=2, d=100$)~\cite{huang2017densely}, and ResNeXt-29 ($32\times4$)~\cite{xie2017aggregated}. All networks use an initial learning rate of 0.1 which decay every 60 epochs. We train all models from scratch for 200 epochs using SGD, batch size 128, momentum 0.9. All input images are randomly processed with resized cropping and horizontal flipping.

\vskip 5pt
\noindent \textbf{Method-specific.} All configurations are in line with \cref{sec:mdc_imp}.

\vskip 5pt
\noindent \textbf{Model Selection.} We select the last-epoch model for evaluations on corrupted datasets.

\subsubsection{Results Analysis}
\label{sec:rtc_ra}

\begin{table}[t]
	\caption{Mean Corruption Error (\%) on CIFAR-10(100)-C.}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{l|cccc}
			\toprule
			\diagbox{Method}{Arch.}&  ResNet&  DenseNet&  WideResNet&  ResNeXt\\
			\midrule
			\multicolumn{5}{c}{CIFAR-10-C} \\
			\midrule
			Standard&  -&  30.7&  26.9&  27.5 \\
			Cutout&  -&  32.1&  26.8&  28.9 \\
			MixUP&  -&  24.6&  22.3&  22.6 \\
			CutMix&  -&  33.5&  27.1&  29.5 \\
			Adv Training&  -&  27.6&  26.2&  27.0 \\
			APR&  \textbf{16.7}&  \textbf{20.3}&  18.3&  18.5 \\
			\midrule
			PhaMa (\textit{ours})&  17.5&  \textbf{20.3}&  \textbf{17.5}&  \textbf{17.9} \\
			\midrule
			\multicolumn{5}{c}{CIFAR-100-C} \\
			\midrule
			Standard&  -&  59.3&  53.3&  53.4 \\
			Cutout&  -&  59.6&  53.5&  54.6 \\
			MixUP&  -&  55.4&  50.4&  51.4 \\
			CutMix&  -&  59.2&  52.9&  54.1 \\
			Adv Training&  -&  55.2&  55.1&  54.4 \\
			APR&  \textbf{43.8}&  49.8&  44.7&  44.2 \\
			\midrule
			PhaMa (\textit{ours})&  \textbf{43.8}&  \textbf{48.5}&  \textbf{43.9}&  \textbf{41.5} \\
			\bottomrule
	\end{tabular}}
	\label{tab:cifar}
\end{table}

% Figure environment removed

%We report the mean Corruption Error (\%) on CIFAR-10-C and CIFAR-100-C. Results are shown in \cref{tab:cifar}. Compared to conventional data augmentation methods (Cutout, MixUP, CutMix), our PhaMa surpasses them by a large margin. In addition, PhaMa has slight improvements over APR, which adopts a similar technique in frequency domain like us. This demonstrates that build spatial relationships for the phase information can facilitate the model intrinsic representation extraction.

We present the mean Corruption Error (\%) for CIFAR-10-C and CIFAR-100-C in \cref{tab:cifar}. Our PhaMa method outperforms conventional data augmentation techniques (Cutout, MixUP, CutMix) by a significant margin. Furthermore, PhaMa shows slight improvements over APR, which also employs a similar technique in the frequency domain like ours. This observation demonstrates that building spatial relationships for the phase information can further enhance the extraction of intrinsic representations by the model.

\subsection{Ablation Studies}
\label{sec:abl}

\begin{table*}[t]
	\caption{Effects of different modules on PACS with ResNet-18.}
	\centering
	\begin{tabular}{l|cccc|cccc|c}
		\toprule
		Method&  APDA&  $\mathcal{L}_{patch}^{o2a}$&  $\mathcal{L}_{patch}^{a2o}$&  MoEnc&  Art&  Cartoon&  Photo&  Sketch&  Avg. \\
		\midrule
		Baseline& -&  -&  -&  -&  77.6&  76.7&  95.8&  69.5&  79.9 \\
		\midrule
		Variant A& \Checkmark&  -&  -&  -&  83.9&  76.9&  95.5&  77.6&  83.4 \\
		Variant B& \Checkmark&  \Checkmark&  \Checkmark&  -&  83.2&  77.1&  95.5&  79.0&  83.7 \\
		Variant C& \Checkmark&  \Checkmark&  -&  \Checkmark&  84.2&  78.7&  96.1&  79.5&  84.6 \\
		Variant D& \Checkmark&  -&  \Checkmark&  \Checkmark&  84.1&  78.4&  95.5&  79.1&  84.5 \\
		\midrule
		PhaMa&  \Checkmark&  \Checkmark&  \Checkmark&  \Checkmark&  \textbf{84.8}&  \textbf{79.1}&  \textbf{96.6}&  \textbf{79.7}&  \textbf{85.1} \\
		\bottomrule
	\end{tabular}
	\label{abl:module}
\end{table*}

\noindent \textbf{Effects of Different Modules.} We conduact ablation studies to investigate the impact of each module in our method in \cref{abl:module}. Compared with baseline, the amplitude perturbation data augmentation module (APDA) plays a significant role in our method, which lifts the performance by a margin of 4.5\%. We exclude the momentum-updated encoder (MoEnc) for variant B, and the performance draws by nearly 1\%, demonstrating the network's the over-dependence on the amplitude spectrum makes the feature extraction inefficient. With the cross contrast operation for the image pairs, PhaMa surpass variant C and D, suggesting that keeping the consistency between the image pairs is important for the training of contrastive learning.

\vskip 5pt
\noindent \textbf{Sensitivity of Trade-off Parameter.} %The hyper-parameter $\beta$ is to trade off between classification loss and patch contrastive loss. We set $\beta$ from $\left\{0, 0.1, 0.5, 1.0, 2.0, 5.0 \right\}$. As shown in \cref{fig:abl_beta}, when $\beta$ is small, the results have silght oscillations. But when $\beta$ is big, the training is collapsed. We suppose the crash is due to the over-dependence on the amplitude spectrum of ImageNet pretrained weights. When $\beta$ grows, the raw representation is corrupted by the intense contrast objective.
The hyperparameter $\beta$ controls the trade-off between the classification loss and the patch contrastive loss. We experiment with different values of $\beta$ from the set $\left\{0, 0.1, 0.5, 1.0, 2.0, 5.0 \right\}$. The results, depicted in \cref{fig:abl_beta}, show that for small $\beta$ values, there are slight oscillations in the results. However, when $\beta$ is set to a large value, the training process collapses. We hypothesize that the collapse is attributed to an over-dependence on the amplitude spectrum of the ImageNet pretrained weights. As $\beta$ increases, the intense contrast objective tends to corrupt the raw representation.

\vskip 5pt
\noindent \textbf{Choices of Match Loss.} %Since we aim to match the representation of each patch from the image pairs, we evaluate different types of matching loss, including SmoothL1, MSE, and PatchNCE. As shown in \cref{tab:abl_loss}, PatchNCE loss outperforms the others on Art Painting, Cartoon, and Sketch, while shows competitive performance on Photo. The worse performance of SmoothL1 and MSE is probably due to simple anlinment of the representation, which is unweighted for the network to discriminate the positive patch from the others.
Since our objective is to match the representation of each patch from the image pairs, we evaluate various types of matching loss, including SmoothL1, MSE, and PatchNCE. As demonstrated in \cref{tab:abl_loss}, the PatchNCE loss outperforms the others on Art Painting, Cartoon, and Sketch domains, while also showing competitive performance on the Photo domain. The inferior performance of SmoothL1 and MSE losses is likely attributed to the simplistic alignment of the representation, which lacks weighting to enable the network to effectively discriminate the positive patch from the others.

\begin{table}
	\caption{Evaluation of different types of matching loss.}
	\centering
	\resizebox{0.9\linewidth}{!}{
	\begin{tabular}{c|cccc|c}
			\toprule
			Type&  Art&  Cartoon&  Photo&  Sketch&  Avg.\\
			\midrule
			SmoothL1&  83.7&  76.6&  96.4&  72.3&  82.3 \\
			MSE&  81.8&  77.4&  96.6&  76.1&  83.0 \\
			PatchNCE&  84.8&  79.1&  96.6&  79.7&  85.1 \\
			\bottomrule
	\end{tabular}}
	\label{tab:abl_loss}
\end{table}

\vskip 5pt
\noindent \textbf{Impacts of Hierarchies.} \Cref{tab:abl_position} compares different hierarchical positions (indexed with 1-4) in the ResNet bolcks for contrastive leraning. As can be seen, the representation extracted from the last two layers occupies the least GPU memory while achieving the best effect performance.

\begin{table}
	\caption{Average Accuracy (\%) and GPU Memory-Usage (GB) of different posiotions.}
	\centering
	\resizebox{0.6\linewidth}{!}{
		\begin{tabular}{c|c|c}
			\toprule
			Position&  Avg Acc&  Mem-Usage \\
			\midrule
			1, 2&  82.2&  $\sim$43.3 \\
			2, 3&  83.5&  $\sim$12.1 \\
			3, 4&  85.1&  $\sim$11.8 \\
			\bottomrule
	\end{tabular}}
	\label{tab:abl_position}
\end{table}

\subsection{Visualization}
To intuitively present PhaMa's effects on feature representations, we visualize the feature representation vectors of different categories in unseen domain with t-SNE~\cite{van2008visualizing} in \cref{fig:vis}. Compared with baseline method, features from the same category become more compact with our method. The clustered representations illustrate that our method can alleviate perturbations caused by domain shifts and extract more domain-invariant features.

% Figure environment removed

%------------------------------------------------------------------------
\section{Discussion and Conclusion}
\label{sec:dis&clu}
In this paper, we consider DG from a frequency perspective and present PhaMa. The main idea is establishing spatial relationships for the phase spectrum. Our method shows promising results of introducing spatial relationships for phase information on many DG benchmarks. 

Moreover, several questions are worth rethinking. Even though the cross-domain impact of amplitude information is allievated, our method doesn't actually build relationships between cross-domain samples, \ie, the intrinsic domain-invariant representation learning is still weakly reflected. As shown in \cref{fig:vis}, minority features are still outliers from the cluster. The above limitations might result in the slight improvement of PhaMa on many DG benchmarks. Beyond domain generalization, PhaMa draws on MoCo\cite{he2020momentum} for many module designs. Considering the great property of the Fourier transform, it is worth thinking that (1) can the phase information be a prior embedding, \eg, position embedding in Transformer~\cite{van2008visualizing}, to augment the original visual signal, (2) whether the Fourier transform and the spatial relationship of the phase spectrum can be extended to unsupervised visual representation learning or even multimodal learning~\cite{radford2021learning}. We hope our work can bring more inspirations into the community.


{\small
	\bibliographystyle{ieee_fullname}
	\bibliography{egbib}
}
\end{document}
