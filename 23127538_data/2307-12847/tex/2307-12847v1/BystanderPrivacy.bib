@online{AA1,
year = {2023},
title={What is mixed reality?},
url={https://learn.microsoft.com/en-us/windows/mixed-reality/discover/mixed-reality.}, 
author={Microsoft},
lastaccessed ={May 14, 2023},
}

@inproceedings{AA2,
author = {Pierce, James and Weizenegger, Claire and Nandi, Parag and Agarwal, Isha and Gram, Gwenna and Hurrle, Jade and Liao, Hannah and Lo, Betty and Park, Aaron and Phan, Aivy and Shumskiy, Mark and Sturlaugson, Grace},
title = {Addressing Adjacent Actor Privacy: Designing for Bystanders, Co-Users, and Surveilled Subjects of Smart Home Cameras},
year = {2022},
isbn = {9781450393584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3532106.3535195},
abstract = {Many consumer Internet Things (IoT) devices involve spatial sensors such as cameras and microphones. These affect the privacy of nearby people. A prime example is smart home security cameras. We present our work developing scenarios, use cases, and design proposals for addressing smart camera privacy. Preliminary findings from a concept evaluation with 11 participants is presented. The outcomes of this research through design project foreground the importance and challenges of designing to support the privacy of nearby users. We outline actionable design responses while also raising limitations of technology approaches alone to address these issues.},
booktitle = {Designing Interactive Systems Conference},
pages = {26–40},
numpages = {15},
keywords = {Internet of Things (IoT), Privacy, Research through Design, Interaction Design, Smart home},
location = {Virtual Event, Australia},
series = {DIS '22}
}

@article{AA3,
author = {Lehman, Sarah M. and Alrumayh, Abrar S. and Kolhe, Kunal and Ling, Haibin and Tan, Chiu C.},
title = {Hidden in Plain Sight: Exploring Privacy Risks of Mobile Augmented Reality Applications},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {4},
issn = {2471-2566},
doi = {10.1145/3524020},
abstract = {Mobile augmented reality systems are becoming increasingly common and powerful, with applications in such domains as healthcare, manufacturing, education, and more. This rise in popularity is thanks in part to the functionalities offered by commercially available vision libraries such as ARCore, Vuforia, and Google’s ML Kit; however, these libraries also give rise to the possibility of a hidden operations threat, that is, the ability of a malicious or incompetent application developer to conduct additional vision operations behind the scenes of an otherwise honest AR application without alerting the end-user. In this article, we present the privacy risks associated with the hidden operations threat and propose a framework for application development and runtime permissions targeted specifically at preventing the execution of hidden operations. We follow this with a set of experimental results, exploring the feasibility and utility of our system in differentiating between user-expectation-compliant and non-compliant AR applications during runtime testing, for which preliminary results demonstrate accuracy of up to 71%. We conclude with a discussion of open problems in the areas of software testing and privacy standards in mobile AR systems.},
journal = {ACM Trans. Priv. Secur.},
month = {jul},
articleno = {26},
numpages = {35},
keywords = {Augmented reality, mobile system security, user privacy}
}

@article{AA4,
author = {O'Hagan, Joseph and Saeghe, Pejman and Gugenheimer, Jan and Medeiros, Daniel and Marky, Karola and Khamis, Mohamed and McGill, Mark},
title = {Privacy-Enhancing Technology and Everyday Augmented Reality: Understanding Bystanders' Varying Needs for Awareness and Consent},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
doi = {10.1145/3569501},
abstract = {Fundamental to Augmented Reality (AR) headsets is their capacity to visually and aurally sense the world around them, necessary to drive the positional tracking that makes rendering 3D spatial content possible. This requisite sensing also opens the door for more advanced AR-driven activities, such as augmented perception, volumetric capture and biometric identification - activities with the potential to expose bystanders to significant privacy risks. Existing Privacy-Enhancing Technologies (PETs) often safeguard against these risks at a low level e.g., instituting camera access controls. However, we argue that such PETs are incompatible with the need for always-on sensing given AR headsets' intended everyday use. Through an online survey (N=102), we examine bystanders' awareness of, and concerns regarding, potentially privacy infringing AR activities; the extent to which bystanders' consent should be sought; and the level of granularity of information necessary to provide awareness of AR activities to bystanders. Our findings suggest that PETs should take into account the AR activity type, and relationship to bystanders, selectively facilitating awareness and consent. In this way, we can ensure bystanders feel their privacy is respected by everyday AR headsets, and avoid unnecessary rejection of these powerful devices by society.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {Jan},
articleno = {177},
numpages = {35},
keywords = {Bystanders, Biometrics, Privacy, Extended Perception, Augmented Reality, Altered Reality}
}

@misc{AA5,
title={Katz v. United States},
url={https://supreme.justia.com/cases/federal/us/389/347/},
journal={389 U.S. 347, 350-51},
author={U.S. Supreme Court},
year={1967},
} 


@online{AA6,
year = {2017},
title={The Desert of the Unreal: Inequality in Virtual and Augmented Reality},
url={https://repository.law.miami.edu/fac_articles/539/}, 
author={Mary Anne Franks},
lastaccessed ={May 14, 2023},
}

@online{AA7,
year = {2021},
title={Balancing User Privacy and Innovation in Augmented and Virtual Reality},
url={https://itif.org/publications/2021/03/04/balancing-user-privacy-and-innovation-augmented-and-virtual-reality/}, 
author={Ellysse Dick},
lastaccessed ={May 14, 2023},
}

@online{AA8,
year = {2020},
title={How to Address Privacy Questions Raised by the Expansion of Augmented Reality in Public Spaces},
url={https://itif.org/publications/2020/12/14/how-address-privacy-questions-raised-expansion-augmented-reality-public/ }, 
author={Ellysse Dick},
lastaccessed ={May 14, 2023},
}

@article{AA9,
title = {Blending the real world and the virtual world: Exploring the role of flow in augmented reality experiences},
journal = {Journal of Business Research},
volume = {122},
pages = {423-436},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.08.041},
author = {Jennifer {Brannon Barhorst} and Graeme McLean and Esta Shah and Rhonda Mack},
keywords = {Augmented reality, AR, Customer satisfaction, Flow, Experiential marketing},
abstract = {This study examines the ‘sweet spot’ of augmented reality (AR) through the lens of flow theory and has two primary objectives. First, the study seeks to determine whether investment in AR technologies is warranted by exploring flow in both an AR and a traditional shopping context. Second, the study examines the unique capabilities of AR to facilitate an enhanced state of flow and its positive influence across several consumer outcomes. To achieve these objectives, a commercially available AR app was utilized to conduct an online, between-subjects experiment with 500 participants. Partial least squares structural equation modeling was used to analyze the predictor variables of consumer flow, as well as the impact of flow across several consumer outcomes. Managerial and practical conclusions for marketers and designers are provided to supportthe creation and execution of AR technology within consumer contexts.}
}

@inproceedings{AA10,
author = {Corbett, Matthew and David-John, Brendan and Shang, Jiacheng and Hu, Y. Charlie and Ji, Bo},
title = {BystandAR: Protecting Bystander Visual Data in Augmented Reality Systems},
year = {2023},
isbn = {9798400701108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3581791.3596830},
abstract = {Augmented Reality (AR) devices are set apart from other mobile devices by the immersive experience they offer. While the powerful suite of sensors on modern AR devices is necessary for enabling such an immersive experience, they can create unease in bystanders (i.e., those surrounding the device during its use) due to potential bystander data leaks, which is called the bystander privacy problem. In this paper, we propose BystandAR, the first practical system that can effectively protect bystander visual (camera and depth) data in real-time with only on-device processing. BystandAR builds on a key insight that the device user's eye gaze and voice are highly effective indicators for subject/bystander detection in interpersonal interaction, and leverages novel AR capabilities such as eye gaze tracking, wearer-focused microphone, and spatial awareness to achieve a usable frame rate without offloading sensitive information. Through a 16-participant user study,we show that BystandAR correctly identifies and protects 98.14% of bystanders while allowing access to 96.27% of subjects. We accomplish this with average frame rates of 52.6 frames per second without the need to offload unprotected bystander data to another device.},
booktitle = {Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services},
pages = {370–382},
numpages = {13},
keywords = {visual data, augmented reality, bystander privacy, eye tracking},
location = {Helsinki, Finland},
series = {MobiSys '23}
}

@inproceedings{AA11,
author = {Shu, Jiayu and Zheng, Rui and Hui, Pan},
title = {Cardea: Context-Aware Visual Privacy Protection for Photo Taking and Sharing},
year = {2018},
isbn = {9781450351928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3204949.3204973},
abstract = {The growing popularity of mobile and wearable devices with built-in cameras and social media sites are now threatening people's visual privacy. Motivated by recent user studies that people's visual privacy concerns are closely related to context, we propose Cardea, a context-aware visual privacy protection mechanism that protects people's visual privacy in photos according to their privacy preferences. We define four context elements in a photo, including location, scene, others' presences, and hand gestures. Users can specify their context-dependent privacy preferences based on the above four elements. Cardea will offer fine-grained visual privacy protection service to those who request protection using their identifiable information. We present how Cardea can be integrated into: a) privacy-protecting camera apps, where captured photos will be processed before being saved locally; and b) online social media and networking sites, where uploaded photos will first be examined to protect individuals' visual privacy, before they become visible to others. Our evaluation results on an implemented prototype demonstrate that Cardea is effective with 86\% overall accuracy and is welcomed by users, showing promising future of context-aware visual privacy protection for photo taking and sharing.},
booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
pages = {304–315},
numpages = {12},
keywords = {photo capturing and sharing, context-aware computing, visual privacy protection},
location = {Amsterdam, Netherlands},
series = {MMSys '18}
}

@inproceedings{AA12,
author = {Steil, Julian and Koelle, Marion and Heuten, Wilko and Boll, Susanne and Bulling, Andreas},
title = {PrivacEye: Privacy-Preserving Head-Mounted Eye Tracking Using Egocentric Scene Image and Eye Movement Features},
year = {2019},
isbn = {9781450367097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3314111.3319913},
abstract = {Eyewear devices, such as augmented reality displays, increasingly integrate eye tracking, but the first-person camera required to map a user's gaze to the visual scene can pose a significant threat to user and bystander privacy. We present PrivacEye, a method to detect privacy-sensitive everyday situations and automatically enable and disable the eye tracker's first-person camera using a mechanical shutter. To close the shutter in privacy-sensitive situations, the method uses a deep representation of the first-person video combined with rich features that encode users' eye movements. To open the shutter without visual input, PrivacEye detects changes in users' eye movements alone to gauge changes in the "privacy level" of the current situation. We evaluate our method on a first-person video dataset recorded in daily life situations of 17 participants, annotated by themselves for privacy sensitivity, and show that our method is effective in preserving privacy in this challenging setting.},
booktitle = {Proceedings of the 11th ACM Symposium on Eye Tracking Research \& Applications},
articleno = {26},
numpages = {10},
keywords = {gaze behaviour, privacy protection, egocentric vision},
location = {Denver, Colorado},
series = {ETRA '19}
}

@INPROCEEDINGS{AA13,
  author={Hasan, Rakibul and Crandall, David and Fritz, Mario and Kapadia, Apu},
  booktitle={2020 IEEE Symposium on Security and Privacy (SP)}, 
  title={Automatically Detecting Bystanders in Photos to Reduce Privacy Risks}, 
  year={2020},
  volume={},
  number={},
  pages={318-335},
  doi={10.1109/SP40000.2020.00097}}


@online{AA14,
year = {2021},
title={Facebook Is Considering Facial Recognition For Its Upcoming Smart Glasses},
url={https://www.buzzfeed.com/ryanmac/facebook-considers-facial-recognition-smart-glasses}, 
author={Ryan Mack},
lastaccessed ={May 14, 2023},
}

@online{AA15,
year = {2023},
title={Introducing Project Aria},
url={https://about.meta.com/realitylabs/projectaria/}, 
author={Meta},
lastaccessed ={May 14, 2023},
}

@online{AA16,
year = {2023},
title={Google AR and VR},
url={https://arvr.google.com/}, 
author={Google},
lastaccessed ={May 14, 2023},
}

@online{AA17,
year = {2023},
title={Building and testing helpful AR experiences},
url={https://blog.google/products/google-ar-vr/building-and-testing-helpful-ar-experiences/}, 
author={Juston Payne},
lastaccessed ={May 14, 2023},
}

@online{AA18,
year = {2013},
title={Google Glass Cons: How the Camera-Embedded Eyeglasses Could Shatter Privacy},
url={https://variety.com/2013/biz/news/google-glass-cons-how-the-camera-embedded-eyeglasses-could-shatter-privacy-1200563731/}, 
author={Sarah Downey},
lastaccessed ={May 14, 2023},
}

@inproceedings{AA19,
author = {David-John, Brendan and Peacock, Candace and Zhang, Ting and Murdison, T. Scott and Benko, Hrvoje and Jonker, Tanya R.},
title = {Towards Gaze-Based Prediction of the Intent to Interact in Virtual Reality},
year = {2021},
isbn = {9781450383455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3448018.3458008},
abstract = {With the increasing frequency of eye tracking in consumer products, including head-mounted augmented and virtual reality displays, gaze-based models have the potential to predict user intent and unlock intuitive new interaction schemes. In the present work, we explored whether gaze dynamics can predict when a user intends to interact with the real or digital world, which could be used to develop predictive interfaces for low-effort input. Eye-tracking data were collected from 15 participants performing an item-selection task in virtual reality. Using logistic regression, we demonstrated successful prediction of the onset of item selection. The most prevalent predictive features in the model were gaze velocity, ambient/focal attention, and saccade dynamics, demonstrating that gaze features typically used to characterize visual attention can be applied to model interaction intent. In the future, these types of models can be used to infer user’s near-term interaction goals and drive ultra-low-friction predictive interfaces.},
booktitle = {ACM Symposium on Eye Tracking Research and Applications},
articleno = {2},
numpages = {7},
keywords = {eye tracking, interaction, intent prediction, virtual reality, mixed reality},
location = {Virtual Event, Germany},
series = {ETRA '21 Short Papers}
}

@inproceedings{AA20,
author = {Jonker, Tanya and Desai, Ruta and Carlberg, Kevin and Hillis, James and Keller, Sean and Benko, Hrvoje},
title = {The Role of AI in Mixed and Augmented Reality Interactions},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {CHI 2020 Extended Abstracts},
series = {CHI 2020 Extended Abstracts}
}

@article{AA21,
title = {Facebook's Project Aria indicates problems for responsible innovation when broadly deploying AR and other pervasive technology in the Commons},
journal = {Journal of Responsible Technology},
volume = {5},
pages = {100010},
year = {2021},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2021.100010},
url = {https://www.sciencedirect.com/science/article/pii/S2666659621000032},
author = {Sally A. Applin and Catherine Flick},
keywords = {Ethics, Responsible innovation, Augmented reality, Pervasive technology, Social media, Surveillance, Anthropology},
abstract = {Nearly every week, a technology company is introducing a new surveillance technology, varying from applying facial recognition to observing and cataloguing behaviours of the public in the Commons and private spaces, to listening and recording what we say, or mapping what we do, where we go, and who we're with—or as much of these facets of our lives as can be accessed. As such, the general public writ-large has had to wrestle with the colonization of publicly funded space, and the outcomes to each of our personal lives as a result of the massive harvesting and storing of our data, and the potential machine learning and processing applied to that data. Facebook, once content to harvest our data through its website, cookies, and apps on mobile phones and computers, has now planned to follow us more deeply into the Commons by developing new mapping technology combined with smart camera equipped Augmented Reality (AR) eyeglasses, that will track, render and record the Commons—and us with it. The resulting data will privately benefit Facebook's continued goal to expand its worldwide reach and growth. In this paper, we examine the ethical implications of Facebook's Project Aria research pilot through the perspectives of Responsible Innovation, comparing both existing understandings of Responsible Research and Innovation and Facebook's own Responsible Innovation Principles; we contextualise Project Aria within the Commons through applying current social multi-dimensional communications theory to understand the extensive socio-technological implications of Project Aria within society and culture; and we address the potentially serious consequences of the Facebook Project Aria experiment, inspiring countless other companies to shift their focus to compete with Project Aria, or beat it to the consumer marketplace.}
}

@online{AA22,
year = {2023},
title={Image of the Google Glass},
url={https://g.co/kgs/Hu7WJE}, 
author={Google},
lastaccessed ={May 24, 2023},
}

@inproceedings{AA23,
author = {Palen, Leysia and Salzman, Marilyn and Youngs, Ed},
title = {Going Wireless: Behavior\& Practice of New Mobile Phone Users},
year = {2000},
isbn = {1581132220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/358916.358991},
abstract = {We report on the results of a study in which 19 new mobile phone users were closely tracked for the first six weeks after service acquisition. Results show that new users tend to rapidly modify their perceptions of social appropriateness around mobile phone use, that actual nature of use frequently differs from what users initially predict, and that comprehension of service-oriented technologies can be problematic. We describe instances and features of mobile telephony practice. When in use, mobile phones occupy multiple social spaces simultaneously, spaces with norms that sometimes conflict: the physical space of the mobile phone user and the virtual space of the conversation.},
booktitle = {Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work},
pages = {201–210},
numpages = {10},
keywords = {mobile, qualitative research, wireless communications, digital telephony, cellular, communicative practice},
location = {Philadelphia, Pennsylvania, USA},
series = {CSCW '00}
}