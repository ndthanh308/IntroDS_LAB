\section{Survey Study}
\label{sec:survey_design}

To define the relative value of scenarios, we design a survey to ask participants the degree to which they agree or disagree with the decisions of a fictional social media platform, SocialNet.
%
These scenarios represent TP, TN, FP, FN, and rejected predictions.
%
The TP and TN scenarios imply that SocialNet successfully detects whether a post is hateful or not hateful, respectively.
%
The FP scenario means that SocialNet incorrectly predicts a non-hateful post as hateful, and conversely for the FN scenario.
%
For example, in the FN scenario, the survey shows a hateful post to the subject and explains that SocialNet did not identify the post as hate speech.
%
%Then, participants indicate the degree of agreement/disagreement using some scale, and the answers per scenario are aggregated to obtain the relative values.
%
%For example, the FN scenario is considered $q$ times as worse as an FP scenario, where $q$ is computed based on the scale.


\subsection{Choice of the Scale}

We use ME as the primary scale.
%
A Likert scale was initially considered, as it is widely used in research for retrieving participant opinions and is perhaps more intuitive for participants~\cite{boone2012analyzing}.
%
%For example, we may use seven Likert-scale items ranging from ``strongly disagree'' to ``strongly agree'' with a ``neutral'' midpoint.
%
However, a Likert scale is not suitable in our case, as Likert-type items are ordinal, meaning that we only know the ranks but not the exact distances between the items~\cite{allen2007likert}.
%
In our case, computing the relative values (i.e., ratios) of our scenarios requires measuring the distances between different items, which cannot be provided by a Likert scale.
%
On the contrary, the ME scale allows us to measure ratios by asking participants to provide numerical ratings.
%
ME originated from psychophysics, where participants gave quantitative estimates of sensory magnitudes~\cite{stevens1956direct}.
%
For sound loudness, a sound twice as loud as the previous one, should ideally receive a rating twice as large.



Researchers have previously applied the ME scale to different physical stimuli (e.g. line length, brightness, or duration) and proved that the results are reproducible, as well as that the data has ratio properties~\cite{moskowitz1977magnitude}.
%
Other works have shown that the ME technique is also helpful for rating abstract types of stimuli, such as judging the relevance of documents~\cite{maddalena2017crowdsourcing}, the linguistic acceptability of sentences~\cite{bard1996magnitude}, and the usability of system interfaces~\cite{mcgee2004master}.
%
Thus, we conclude that ME is a promising method for judging hate speech.


\subsection{Normalization and Validation of the Scale}

The ME scale is unbounded.
%
For example, suppose we first show a scenario and the participant provides a value (e.g., 100) to indicate the degree of agreement.
%
Suppose we next present a scenario that the participant agrees with more.
%
The participant can always provide a higher value (e.g., 125) and not be restricted within a fixed range. %(e.g., between 0 and 100). 
%
%However, there are drawbacks to using the ME scale.
%
The results need to be normalized as different participants rate the agreement/disagreement degree differently.
%
%And it can be hard to validate if the ME scale measures the participants' judgments for different scenarios of hate speech detection consistently.


Multiple solutions exist for normalizing the ME scale, such as modulus normalization, which uses geometric averaging to preserve the ratio information~\cite{moskowitz1977magnitude, mcgee2004master}.
%
Unlike the unipolar ME scales used in previous research~\cite{bard1996magnitude, mcgee2004master}, we use bipolar scales.
% (disagree-agree) with 0 (neutral) and negative values (disagree).
%
%Thus, we cannot use geometric averaging since it uses logarithm calculations.
%
Using arithmetic averaging is inappropriate since it uses logarithmic calculations and would disrupt the ratio scale properties~\cite{moskowitz1977magnitude}.
%
% Another normalization approach is external calibration, which keeps the ratio scale properties~\cite{moskowitz1977magnitude}.
%
%This approach asks participants at the end of the survey to indicate which verbal label (e.g., ``strongly agree'') corresponds to which numerical value (e.g., 125) they used before.
%
Therefore, we normalize the results by dividing the magnitude estimates of each subject by their maximum estimate.
%
We multiply the normalized magnitude estimates by 100 for the sake of clarity.
%
This way, all magnitudes estimates are in the range $[-100, 100]$ while maintaining their ratio properties.
% Then, the average value is calculated as the pivot, and all the ME results are divided by the pivot value.
%
% In this way, external calibration normalizes ME results while maintaining the ratio properties.


%\subsection{Validation of the Scale}

Most previous research using the ME scale applies validation, such as cross-modality validation, where estimated magnitudes are compared to the physical stimuli using correlation analysis~\cite{bard1996magnitude}.
%
%For example, when asking a participant to estimate line lengths in two scenarios, we can use two lines where one's length is $r$ times longer than the other.
%
%Then, we can vary $r$ for each participant to compare the relationship between the ME results and the exact line lengths.
Cross-modality validation is difficult in domains that do not have exact measures of stimuli, such as hate speech.
%
%One alternative is to compare different non-physical stimuli, which has been proven suitable to validate the ME scale~\cite{mcgee2004master, lodge1979comparisons}.
%
%Some previous work analyzed the correlation between different ME scales~\cite{bard1996magnitude, lodge1976calibration}. %, such as handgrip measurements or drawing lines \cite{bard1996magnitude, lodge1976calibration}.
%
Some previous work compared ME with other validated scales~\cite{maddalena2017crowdsourcing}.
%that can be of any type
%
In our case, we use the 100-level scale to validate the ME scale by analyzing their correlation~\cite{roitero2018fine}, which is a form of convergent validation~\cite{fitzner2007reliability}.
%
%The 100-level scale is bounded, consists of 100 numerical values from 0 to 100, and is previously validated~\cite{roitero2018fine}.

\iffalse
\color{blue}
\subsection{Variables}%Independent, Confounding, Control, and Dependent Variables}
The independent variables are the possible machine decision scenarios (TP, TN, FP, FN, and rejection).
%
%The true positive (TP) and true negative (TN) scenarios mean that SocialNet successfully detects whether a post is hateful or not, respectively.
%
%The false positive (FP) scenario means that SocialNet incorrectly predicts a non-hateful post as hateful, and conversely for the false negative (FN) scenario.
%
We inform participants in the survey that when hate speech is detected (i.e., the TP and FP scenarios), SocialNet ranks the hateful post lower so that it takes much more effort for the users to find the post (e.g., longer page scrolling).
%
%The rejection scenario means that SocialNet is uncertain whether the post was hateful or non-hateful and thus the post requires human decision-making.
%
For the rejection scenario, we inform the participants in the survey that a moderator needs to check the post within 24 hours, and meanwhile, the post remains visible with its original rank on the page.
%
The design decision of using 24 hours is based on the German NetzDG law, which allows the government to fine social media platforms if they do not remove illegal hate speech within 24 hours~\cite{Tworek2019AnAO}.


%\subsection{Confounding Variables}

%Confounding variables are agraphics, such as gender, education, location, and age.
%
%Participants from different backgrounds may define hate speech differently and have various perspectives about handling hate speech.
%
Some previous work focuses on studying confounding variables such as demographics. For example, \citet{gold2018women} showed that there is no significant difference when perceiving hate between genders.
%
Our research does not study the effect of confounding variables though we may report them to inform the readers about the study context.
%
%We leave the investigation of the effects of demographics on the outcomes to future research.
%\subsection{Control variables}
Our study has two control variables: the measurement scales and the content of posts.
%
%Regarding the content of the post, due to limited resources, we manually select 40 posts from existing datasets containing hateful, neutral, and non-hateful tweets.
%
%The selection procedure is explained in a later subsection.
%
Regarding scales, as described before, we choose ME as our primary scale and use the 100-level scale for validation.
%
We leave the study of other scales to future work.
%\subsection{Dependent variables}
Our dependent variables are reliability, validity, and value ratios.
%
We use Krippendorff's alpha to compute reliability, where a value equal to or larger than 0.8 and 0.6 indicates reliable and tentative conclusions, respectively~\cite{krippendorff2004reliability, maddalena2017crowdsourcing}.
%
Regarding validity, we use convergent validity~\cite{fitzner2007reliability} to assess if the two different scales measure the same phenomenon by calculating the correlation between the results measured using the ME and 100-level scales.
%
The value ratio variable describes the perceived value of the scenarios, which is measured by calculating the median of the normalized magnitude estimates of each decision scenario.
\color{black}\fi

\subsection{Participants and Data}
We use Prolific to recruit crowd workers for the study.\footnote{Approved by the ethics committee of our organization.}
%
Participants need to be at least 18 years of age, be fluent in English, and have an approval rating of over 90\%.
%
Participants also need to have experience using a social media platform regularly (at least once a month). %one of the following social media platforms regularly (at least once a month): Facebook, Twitter, YouTube, LinkedIn, Pinterest, Google Plus, Tumblr, Instagram, Reddit, VK, Flickr, Vine.co, Meetup, ask.fm, Snapchat, TikTok, or Medium.
%
Every participant is paid an hourly wage of 9 GBP, exceeding the UK minimum wage at the time of the study.
%
Regarding sample size, we recruit 24 participants for the pilot study and 136 participants for the official study.
%
Of the recruited participants, 50\% identified as female, though \citet{gold2018women} showed that there is no significant difference when perceiving hate between genders.
%
Half of the participants are assigned the ME scale and the other half the 100-level scale.
%
We choose a 90\% Confidence Interval (CI) and 10\% Margin of Error (MoE) for this study due to budget limitations.
%
There are billions of social media users, and according to~\citet{muller2014survey}, we need a sample size of 68 participants per measurement scale, i.e., 136 participants, to reach the desired CI and MoE.

%\subsection{Hate Speech Data}
The final dataset consists of 20 hateful and 20 non-hateful social media posts from a public dataset~\cite{basile2019semeval} to build the machine decision scenarios (TP, TN, FP, FN, and rejection).
%
The dataset contains 13,000 English tweets, and each tweet is annotated with three categories: hate speech (yes/no), target (group/individual), and aggressiveness (yes/no).
%
We first exclude tweets that are replies or contained mentions or URLs since they have unclear contexts.
%
Finally, we use clustering analysis to select 40 tweets for our study.
%
We use a cluster size of 20 for the non-hateful tweets and sample one tweet per cluster by taking the nearest sample to each cluster centroid to obtain each cluster's most representative tweets.
%
For the hateful tweets, we first divide them into four groups using the target and aggressiveness categories.
%
Similarly, for each hateful tweet group, we use a cluster size of 5 and sample one tweet per cluster.
%
We perform latent semantic analysis (LSA), which is a combination of term frequency-inverse document frequency (TF-IDF) and Singular Value Decomposition (SVD), and k-means clustering on each group of tweets.
%
We calculate the silhouette coefficient to determine the optimal cluster size ($k$ value) for the neutral tweets and the four groups of hateful tweets.
%
%\color{red}The silhouette analysis indicated to set $k$ as large as possible. \color{black}
%
We manually select one tweet per cluster using a majority vote from three members of our group to choose representative tweets and create the final set of 40 tweets.

Additional information on the study's variables, pilot study, demographics, as well as example tasks may be found in~\cref{app:b}. 


\subsection{Procedure and Data Quality Control}
The survey first presents the informed consent policy and excludes participants that do not agree with it.
%
Next, introductory texts are shown to explain the possible machine decisions.
%
In the case of using the ME scale, participants are presented with a warm-up task to estimate different line lengths.
%
Then, the survey asks 40 randomly shuffled question sets regarding the TP, TN, FP, FN, and rejection scenarios (with 8 question sets per scenario).
%
%Each set contains several questions with the same structure.
%
The first question is about whether participants think the post is hateful (yes/no).
%
The second question is whether participants agree or disagree with the decision made by the machine, which may be correct or incorrect, or are neutral towards it.
%
In the case of a non-neutral decision, the survey asks the third question about the degree to which participants agree or disagree with the machine's decisions, using either the ME or 100-level scale, depending on their group.
%
There is no time limit for the survey.


%\subsection{Data Quality Control}

In the middle of the question sets, we use two Instructional Manipulation Checks to determine if the user is paying attention\footnote{\href{https://researcher-help.prolific.co/hc/en-gb/articles/360009223553}{Prolific's Attention and Comprehension Check Policy}}.
%
These attention checks ask participants to select a specific option from multiple choices (e.g., "You must select Orange").
%
We exclude responses from the participants who fail the attention checks or do not complete all questions.
%
For the ME scale, we discard responses that do not perform well in the line length warm-up task.

\iffalse
\subsection{Pilot Study}
\color{blue}
We conducted a pilot study with 24 participants to test the survey and estimate the required completion time.
%
The pilot study showed low inter-rater reliability, which may be due to unmatched expectations and unclear scenario descriptions.
%
Initially, our scenarios mentioned that machine-detected hateful posts would be removed, which could be controversial.
%
Thus, we changed scenarios to rank the hateful posts lower on the feed. % so that it takes more effort for users to scroll the page to access the posts.
%
Moreover, we added more lengthy descriptions to explain the potential consequences of all scenarios in a neutral tone, which prepares the participants to focus on evaluating harm (instead of giving rewards).
%
%After the changes, the official survey study shows higher inter-rater reliability.
\color{black}\fi

\subsection{Analysis}
\label{sec:analysis}

We first compute the values for the TP, TN, FP, FN, and rejection scenarios using the survey study data.
%
For both scales, we convert disagreement (with the machine decision) ratings to negative values, neutral stances to 0, and agreement ratings to positive values.
%
%Then, the ME values are used to compute the optimal rejection threshold, as described in section~\ref{s:3}).
%
%Next, we use the 100-level values to validate the ME values. 
%
We apply convergent validity, in which a correlation analysis between different scales (i.e., the ME and 100-level scales) is conducted to determine if they measure the same phenomenon \cite{fitzner2007reliability}.
%
We expect a medium-large correlation between both scales, meaning that ME responses small in magnitude should correspond to 100-level scale responses small in magnitude and vice versa.
%
Finally, we analyze reliability, which determines whether we can trust our results and achieve consistent outcomes~\cite{fitzner2007reliability}.
%
In our case, we use inter-rater reliability to investigate whether different subjects give approximately the same judgments to the same scenarios and, thus, whether the degree to which hate speech is subjective.
%
It is measured using Krippendorff's alpha, which we calculate using the normalized ME and 100-level values for all scenarios.