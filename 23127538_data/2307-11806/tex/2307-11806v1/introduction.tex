\section{Introduction}
\label{s:1}

Hateful content spread online through social media remains a significant problem.
%
Ignoring its presence can lead to psychological harm and even result in violence and other conflicts~\cite{columbia-facebook-linked-to-violence, mujib-mashal-india, paul-mozur-2018, muller2021fanning}.
%
Governmental institutions and social media platforms are increasingly aware of these risks and are combating hate speech.
For example, the European Union developed a Code of Conduct on countering hate speech~\cite{eu-code-of-conduct}, requesting large social media companies to moderate hate speech and report their progress yearly. However, results reported so far are not yet satisfactory, as, for example, less than 5\% of hateful content has been removed from Facebook~\cite{noah2021giansiracusa}.


Hateful content moderation is either carried out manually or automatically by computational algorithms, where manual moderation may be more reliable but is not scalable to handle the deluge of user-generated content ~\cite{klonick2017new}. Further, continuous exposure to harmful content can be harmful to moderators as it can induce mental issues and potentially even lead to acts of self-harm~\cite{solonFacebook}. Computational solutions are, therefore, urgently in demand by online platforms~\cite{fortuna2018survey}. The methods considered best suited to this task are mainly based on machine learning, which has achieved reasonable performance at scale~\cite{gagliardone2015countering}. Yet, machine learning methods are far from being reliable, especially in dealing with hateful content previously unseen in the training data, which is often limited in size and biased~\cite{balayn2021automatic}. Several recent studies on hate speech have shown a significant drop in machine learning performance when assessed on different data from those captured in the training phase~\cite{arango2019hate,grondahl2018all}. 


An approach that can combine the strengths of both previously mentioned approaches is human-AI collaboration, where humans are involved to solve AI-hard tasks, typically by taking over decisions where machines are unreliable~\cite{cheng2015flock,Law_hearth_cscw18}.
%
Such an approach is favorable in applications where decisions involve high-stakes and incorrect decisions can lead to damaging effects, as is the case for hate speech detection. 
%
Human-AI collaboration has been advocated in the human computation community~\cite{cheng2015flock,Raghu2019,WilderHK20} and, likely, is also an approach widely being used in enterprise applications such as search and conversational agents~\cite{Khodabandeh2020ExpandingAI}. Despite this, methods for implementing human-AI collaboration so far are limited to predefined heuristics and have largely ignored the complexity of real-world problems, especially the cost of incorrect predictions being context-dependent. 


Common heuristics of task handover from machines to humans are based on machine confidence: humans take over the task when the confidence of the machine in its decision is lower than a predefined threshold \cite{Law_hearth_cscw18}. Such heuristics assume that machine confidence is well-calibrated, that is, a decision with high confidence should be more likely to be reliable and vice versa. This assumption however does not hold for many machine learning models, especially deep learning models, which may indicate high confidence when decisions are incorrect or vice versa~\cite{Guo2017,Balda2020}. An improved approach is proposed by~\citet{geifman2017selective} which determines the appropriate confidence threshold based on empirical evidence of machine correctness, e.g., based on the accuracy-threshold curve obtained on an empirical dataset. Such an approach, however, does not take into account the implications of right or wrong decisions. Incorrect decisions in high-stakes domains have a larger impact that, in turn, should pose a stricter constraint on accepting machine decisions, e.g., via a higher confidence threshold. Similar ideas have recently been discussed in position papers that advocate the adoption of the notion of context-dependent \emph{value} as a replacement of accuracy, the most common metric in machine decisions assessment~\cite{Sayin2021reject,casati2021value}. Value, however, is an abstract term -- it can be interpreted from social, ethical, or commercial perspectives~\cite{cummings2006ethics,zhu2018algo, gilliland2020value} -- yet the discussion on what creates value and how to measure it, specifically in a machine learning context, is limited due to it depending on the application. %In specific contexts, the question remains how value should be operationalized and embedded into the human-AI collaboration framework. 

In this paper, we study the problem of operationalizing value perception of machine decisions and its integration into human-AI collaboration in the specific context of hate speech detection. % \rmj{We first introduce a formal framework of human-AI collaboration for the chosen task where value is integrated as an abstract parameter that determines the optimal confidence threshold for rejecting machine decisions (and subsequently passing the decision to human moderators).} 
We start by identifying several factors that may affect the value definition, %\rmj{ used for such a framework}, 
namely the selection of a specific stakeholder's standpoint and the relativity of value perception as affected by stakeholder expectation or regulation. We then operationalize user-perceived value in hate speech moderation scenarios, where a decision with a corresponding confidence has been made by a machine. To measure these perceptions, we explore several measurement scales and propose to select Magnitude Estimation (ME)~\cite{stevens1956direct} as the primary scale. ME allows the measurement of the magnitude of user (dis)agreement using an unbounded scale and makes it possible to obtain the relative ratios between the magnitudes of different machine decisions. These ratios are essential to determine the optimal confidence threshold for rejecting machine decisions (see \cref{s:3}).

To validate ME in value operationalization, we designed a survey study where we recruited 160 participants. Each participant's perception regarding a dataset of 40 selected hateful and non-hateful tweets and their (dis)agreement regarding the corresponding machine decisions were evaluated. Through a between-subject study, we show that Magnitude Estimation returns results with significantly higher inter-rater reliability compared to other scales, showing its suitability in measuring user perception. Our results show that the inter-rater reliability is significantly higher for incorrect decisions than for correct decisions, indicating a strong consensus among participants regarding the consequences of harm, as well as disagreements on what constitutes hate online.
Further, users appear to be more negatively affected when a non-hateful post is subject to moderation than when an instance of hate speech is classified as non-hateful, implying that users would rather contend with an instance of hate speech than have an innocent user punished for a non-hateful post. 
%\jie{say also the higher agreement in value rating for incorrect decisions, and what it entails} \Philippe{done, correct?} % Our results also show that users tend to disagree more on false prediction for hateful content classified as non-hateful than the other way around. 

To demonstrate the utility of value integration in human-AI collaboration, we evaluate the effect of rejecting machine decisions made by three machine learning-based hate speech detection models -- including traditional, deep learning, and BERT-based models \cite{devlin2018bert} -- in handling data from both seen and unseen sources. Our results show that for all three models, when evaluated on unseen data, the optimal confidence thresholds determined by the model-delivered value are much higher than the optimal thresholds on seen data. These results confirm the findings from previous studies on machine biases and demonstrate the effectiveness of using value as a target for optimally rejecting machine decisions. We further show that when selecting the optimal model, using value as the criterion returns different results compared to using accuracy. Note, that our approach to measuring value perception can be applied to different tasks and is model-agnostic.

In summary, we make the following key contributions:
\begin{itemize}
    % \item \rmj{We introduce the human-AI collaboration framework with a rejection mechanism for machine decisions to the problem of hate speech detection;}
    \item We introduce Magnitude Estimation as a scale for measuring user perception of machine decisions in scenarios where these decisions are correct and incorrect;
    \item We demonstrate the applicability of Magnitude Estimation through a between-subject survey study, as well as the utility of value for optimally rejecting machine decisions;
    % \item As part of our survey study we find that users tend to be more in agreement regarding what constitutes a misclassified hate speech instance than what constitutes a correctly classified instance. Users also appear to be more negatively affected when a non-hateful post is subject to moderation than when an instance of hate speech is classified as non-hateful.
    \item We contribute a set of insights into user-perceived value of automated machine decisions, especially their  attitudes towards different types of (mis)classifications.
\end{itemize}


% In our case, the handover from machine to human is handled by a rejector, which rejects making a prediction regarding an instance of potential hate speech at low confidence, and passes it onto a human instead.
% %
% The appropriate confidence threshold for rejection of a prediction depends on context, as the amount of potential benefit or harm stemming from a prediction varies. 
% %
% %To address this challenge we employ value. 
% %
% To determine the value of a decision we must first consider the different possible scenarios (tolerate, ban, or reject making a choice) and, in the case when a choice is made, whether it is correct or not.
% %
% %Thus, for these decisions, we have to define the relative values of the possible scenarios: true positive (TP), true negative (TN), false positive (FP), false negative (FN), and rejected predictions for hate speech detection.
% %
% This paper aims to build the first \textit{value-sensitive smart rejector} for hate speech detection, leading to our research question: \textbf{how can we maximize the value of machine learning models in hate speech detection using a reject option?}

% Expressing the relative values for each of the scenarios for a specific instance of hate speech in money saved or spent is infeasible for the case of hate speech detection due to many uncertainties \cite{sunstein2018does}.
% %
% We, therefore, conduct a crowdsourced survey to define the relative values subjectively. The crowd workers provide their judgment on hypothetical hate speech moderation scenarios, where a decision to ban content, tolerate it, or reject making the decision and redirect it to a human moderation team has been made by a machine.
% %
% We then aggregate their opinions to quantify their perceived level of (dis)agreement with regards to the choice made by the machine, before determining the relative values of each scenario and the optimal confidence threshold, below which the machine should reject making a prediction.


% In this paper, we find by means of a survey study that the Magnitude Estimation (ME) scale can be used for collecting subjects' opinions regarding different hate speech detection scenarios. We found that the ME scale provides reliable results and validated the scale by showing the correlation with a bounded scale that consists of 100 rating levels, used in a separate survey.
% %
% We provide a method for measuring the total value of a model with a reject option that takes the values of true positive (TP), true negative (TN), false positive (FP), false negative (FN), and rejected predictions into account.
% %
% Further, we show that we can incorporate how humans feel towards machine decision-making into our formulation by conducting an empirical study in which we measure the total value of three different hate speech classification models with a reject option.
% %
% We find that the total value of all models can be maximized by the use of our rejector when there is a focus on reducing the harm done to users of social media by hate speech, especially when the models are applied to previously unseen data.











