\section{Discussion}
\label{sec:discussion}

\subsection{Value Ratios, Reliability, and Validity}
%\textbf{value ratio}
Our results show that TP and TN scenarios are highly valued. 
%
Participants seem to value correct predictions more than incorrect predictions across all scenarios, regardless of whether they are positive or negative.
%
%\color{red}
The value of rejected predictions is the closest to 0 (neutral), as expected, due to them not contributing any benefit or harm, but just delaying the publishing of the post due to the additional human moderation effort.
%\color{black}
%\ych{Maybe explain why this is expected?}
%
For both scales, we observe the same relation of scenarios in terms of values (FN$<$FP$<$Rejection$<$TP$<$TN).
The fact that correct decisions receive higher value ratings indicates strong user appreciation of correct machine decisions. 
%
%\color{red}
The value of FN having a larger magnitude than the value of FP is noteworthy, as users appear to be more negatively affected when a non-hateful post is subject to moderation than when an instance of hate speech is classified as non-hateful. This implies that users would rather contend with an instance of hate speech than have an innocent user punished for a non-hateful post. This phenomenon may be explained by the Blackstone principle from the domain of criminal law: ``Better that ten guilty persons escape, than that one innocent suffer''~\cite{epps2014consequences}. % Our findings This further strengthens our assumption that effective moderation through human-AI collaboration is key to maximizing total value.
% \color{black}
%\ych{Maybe explain why this is expected?}
%
% Participants perceive the impact of incorrectly classifying hateful content as greater than incorrectly classifying non-hateful content.
%We expected that the value of FN would be lower than of FP, so the impact of incorrectly classifying hateful content is considered greater than incorrectly classifying non-hateful content.
%
However, we do consider it surprising that the value of TN is greater than the value of TP.
%
One possible reason could be that people disagree more on what is considered hateful among the TP scenarios.
%
We also encountered this phenomenon in the survey results where most people rated TN cases as non-hateful, while for the TP cases there were more disagreements.

%\textbf{reliability}
Regarding reliability, Krippendorff's alpha, $\alpha$, for the 100-level scale being lower than the one for the ME scale is unexpected, as the 100-level scale is bounded with fewer possible options.
%
The stronger agreement for the ME scale indicates that it is indeed suitable for this task.
%
Since $\alpha$ compares the expected difference with the observed difference, it follows that the alpha values for the entire scale should be greater than for the individual scenarios.
%
Generally, participants tend to have low agreement on TP, TN, and rejection cases while they have a high agreement regarding the FP and FN cases. Users tend to agree more regarding what constitutes a misclassified instance than what constitutes a correctly classified instance. For the ME scale, we even observe systematic disagreement for the rejection case, as can be seen by its negative $\alpha$ value. 
This indicates that users are lower in agreement than one would expect by chance, showing the wide variety of opinions regarding rejection cases by users.
%\jie{isn't the result the other way around?} \jie{also, we need to explain what the result entails.}
%
%\color{red}
By considering all answers, instead of answers for certain scenarios, we observe a greatly increased $\alpha$, as the observed difference between ratings is closer to the difference expected by chance. % \jie{what do we mean by expected difference here?}
%
%This does not hold when computing the value for only one scenario.
%
%\color{red}
For example, participants tend to agree on the classification of a single scenario, e.g. TP, but may give different values on both scales, resulting in lower $\alpha$ for the scenario but greater $\alpha$ across all scenarios.
%\color{black}
%\ych{Please check if the red-marked one is correct}
%
Beyond this, the low reliability for the positive compared to negative predictions indicates that participants disagree on what constitutes hate speech in the first place.
    
%\textbf{validity}
Regarding validity, we observe a strong correlation between scales, demonstrating that the ME scale is validated for measuring people's opinions about different hate speech detection scenarios.
%
The almost S-shaped curve for the data points in~\cref{fig:validity} is due to the lower and upper bounds of the 100-level scale that restrict the participants' choices, making them more likely to assign the lowest or highest value.
%
Meanwhile, the data points corresponding to the ME scale are skewed towards 0 because of the normalization.
    
\subsection{Value Function for Rejection}

The purpose of the reject option is to reject predictions where the risk of an incorrect prediction is too high.
%
However, when we use all values obtained from the survey to measure the value function $V(\tau)$, the total value of a model with a reject option is maximized by accepting all predictions.
%
As shown in~\cref{fig:metric-values-seen,fig:metric-values-unseen}, values are positive at the beginning, decline steadily as the rejection threshold increases, and eventually become negative as more predictions are rejected.
%In~\cref{fig:metric-values-seen,fig:metric-values-unseen} we can see how the value steadily declines and eventually becomes negative as more predictions are rejected. 
%
This observation is not surprising, as the absolute values of correct predictions are greater than the absolute values of incorrect predictions (see~\cref{tab:costs-reliability}).


However, instead of rewarding correct predictions, we believe it is more critical to emphasize penalizing incorrect predictions, as hate speech should be moderated effectively to minimize harm.
%users of a social media platform expect hate speech to be moderated effectively but will be adversely impacted should this not be the case.
%
%\color{red}
To study the effects of this we also analyze the behavior of $V(\tau)$ when users do not experience an increase in value through correct classifications, i.e. TP and TN. To achieve this, we set the scenario values $v$ of TP and TN equal to zero. This results in correct predictions effectively only increasing the total value by the $v$ of rejection when accepted and decreasing when rejected, as can be seen in~\cref{for:final-V}.
%\color{black}
%\ych{This sentence is hard to read and not clear to me. Would you mind rewriting it using an easier-to-understand wording?}
%
The result in~\cref{fig:metric-values-seen-tptn0} shows a steady increase in value before it peaks for each of the three models, eventually falling again and becoming negative as almost all predictions are rejected.
%
Hence, there is a strong incentive to reject some (but not all) predictions for the \textit{seen} data.
%
At the points where values are maximized, we found an optimal balance between accepting and rejecting predictions.
%
Figure~\ref{fig:metric-values-unseen-tptn0} shows that the values continually rise for all three models, only peaking as the rejection threshold approaches 1.
%
This indicates that the model is very uncertain regarding its predictions for the \textit{unseen} data, which may be expected.
%considering the evaluation on the unseen data.
%
Initially, at the 0.5 rejection threshold, the value is negative as all predictions are accepted.
%
When the rejection threshold increases, the value rises steadily since too many incorrect predictions are made.
%
%\color{red}
This indicates that the model is not performing well at the task (i.e., high confidence false predictions), and thus the optimal condition to reject most predictions makes the unviable model.
%\color{black}
%\ych{Please check if the red-marked sentence is correct.}
%This indicates that the model is not performing well at the task, as rejecting the vast majority of cases put the model's viability into question.

The results show that by penalizing incorrect predictions without rewarding correct predictions, a significant fraction of the predictions can be accepted from all three models. 
%The results show that by focusing on penalizing incorrect predictions, all three models perform well on seen data since we accept a large fraction of its predictions as a result of high confidence.
%
For unseen data, however, very few predictions from these models can be accepted and the majority are rejected. %with a reject option may still be valuable since they achieve high accuracies when a small fraction of the most confident predictions are accepted.
%
% On the other hand, the risk of making incorrect predictions for all three models for unseen data is higher compared to seen data,
% \color{red}
Such a result confirms the bias in the dataset as also found in previous studies \cite{arango2019hate,grondahl2018all}. The results also show the utility of value as a metric in guiding the decision on when to reject machine predictions. Value utility is further confirmed in the results in \cref{tab:metric2} from our experiment on optimal model selection: the best model selected by value is different compared to using accuracy as the metric.
% \%color{black}
%\ych{Where did we mention this? Maybe say things like in section X}

\subsection{Findings, Implications, and Limitations}
%\textbf{RQ}
Our survey study uncovers several interesting findings. First, social media users are more appreciative of correct decisions made by the platform, with an absolute magnitude higher than the (negative) perception of incorrect decisions. Among the correct decisions, users especially appreciate that non-hateful content is correctly identified and not banned. On the other hand, users show a much higher agreement on the negative value of incorrect decisions than correct ones, indicating a strong consensus over the harm (from both identifying hateful content to be non-hateful, and vice versa). These results indicate that while users appreciate correct decisions, minimizing incorrect decisions remains an important task for social media platforms. On the methodological side, we also believe our proposal of using ME for rating human perception can be particularly relevant for research that aims to tackle social science problems through quantitative approaches, like machine learning. 

By integrating value as a parameter into the human-AI collaboration framework for rejecting machine decisions, we show that value can help guide the decision of when to accept machine decisions to reach the optimal value a model can deliver. By showing how the number of acceptable machine decisions changes when the model is applied to a dataset different from the training data, our results confirm findings from previous research that such datasets are biased and hence the trained models are as well. Our results also show that when considering value as an optimization target, the best model selected can be different compared with using accuracy as the metric. We believe these findings can benefit the research community and industry alike, as they present a novel way of using a value-sensitive reject option to increase the utility of human-AI collaboration across domains.
%
% Regarding our research question, we show that the value of a machine learning model can be maximized using the human-based values derived from a crowdsourced survey study to find the optimal rejection threshold. 
% %Regarding our research question, we show that the value of a machine learning model can be maximized using a reject option which makes use of values derived from a crowdsourced study to find the optimal rejection threshold.
% %
% The rejector is therefore both value-sensitive and smart.
% %
% Whether the rejector is beneficial depends on the underlying confidence in machine predictions and how stakeholders who provide the values for different scenarios view their implications and impacts.
% %The actual usefulness of such a rejector depends on the underlying confidence in predictions by the machine learning model and how the stakeholders that provide the values for different scenarios view their potential implications.
% %
% %Further, we show that a key assumption we make towards the baseline of the user.
% %
% It could be that social media users do not expect the model to perform well in hate speech detection.
% %
% Thus, their perspectives may focus on the positive sides of the model rather than evaluating potential harm, which is the phenomenon that we found in the survey study.
%If the user does not expect the model to perform its task as a default, then their responses will focus on the positives of the model, rather than on an evaluation of potential harm.



%\textbf{Limitations}
Our work is limited to a relatively small sample size (68 subjects per scale). %resulting in a 90\% confidence and 10\% MoE.
%
We expect the results to be more reliable at a larger sample size.
%
%Nevertheless, having 95\% confidence and 5\% MoE requires 384 participants per scale given the size of our population, which would greatly increase the cost of our study for potentially a marginal gain.
%
Besides, optimal confidence threshold determination relies heavily on empirical data, which may not be available in real applications. An easier way for selecting the optimal threshold would be using well-calibrated models, for which the optimal threshold is only dependent on the human-perceived value. % which means more correct predictions with high confidence and fewer wrong predictions with high confidence. 
%
Although techniques such as Temperature Scaling can help improve the calibration of existing neural networks or transformer models such as DistilBERT, we still observe that all models are predisposed to producing high-confidence errors.
%
Finally, due to taking the users' standpoint, we do not fully capture the cost of the moderation team being exposed to hate speech. We leave this as possible future work.
