\section{Related Work}
\label{s:2}

\subsection{Hate Speech Detection} 
Online hate speech content refers to ``online messages demeaning people on the basis of their race/ethnicity, gender, national origin, or sexual preference''~\cite{lee2020source}.
%
Its characterizing features are properties of the target of the language, as compared to other types of online conflictual languages, which are defined by the intention of the author such as cyberbullying or flaming~\cite{burbank1994cross, rayner1997summary}.
%
%It also differs from conflictual languages whose definitions put more emphasis on language style but are not necessarily mutually exclusive, such as abusive, profane, or toxic languages. 
%
A large body of discussion can be found on conflictual languages from social sciences, political science, and computer science~\cite{matias2019preventing,tsesis2001hate,waldron2012harm}.
%
Hate speech-related research in computer science has identified mismatches between the formalization of hateful content and how people perceive such languages~\cite{balayn2021automatic}.
%
These mismatches conceptually are further reflected in the technical biases of the machine learning systems used for filtering hateful content.
%
For instance,~\citet{grondahl2018all} found that F1 scores were reduced by up to 69\% when training a hate speech detection model on one dataset and evaluating it using another dataset from a similar source.
%
Similarly,~\citet{arango2019hate} found that most research in hate speech detection overestimates the performance of the automated methods due to dataset bias.
%
In response to these findings, our work aims to explore a human-AI collaborative approach for effective hate speech detection. 


\subsection{Human-AI Collaboration and Rejection}
Human-AI collaboration aims to exploit the complementarity between the cognitive ability of humans and the scalability of machines to solve complex tasks at scale~\cite{vaughan2017making, Bansal2021team}. 
%The most common way of utilising human computation for machine learning is perhaps data generation. 
%
%For example,~\citet{deng2009imagenet} and~\citet{heilman2010rating} explored ways of annotating data with crowd workers to tackle computer vision and natural language processing tasks.
%Related work includes early explorations of crowdsourced data annotation for model training, such as ImageNet for vision tasks~\cite{deng2009imagenet}, but also efforts of data annotation in language tasks~\cite{heilman2010rating}.
%
%In human computation for machine learning, research has focused on reducing the annotation effort~\cite{yang2018leveraging}, as well as noises arising from varying levels of worker expertise, skills, motivation, task difficulty, or clarity~\cite{dawid1979maximum,sheng2008get,whitehill2009whose}.
%
%Active learning is also used to reduce the annotation effort or cost~\cite{yan2011active,yang2018leveraging}.
%A focal point of research has been reducing the annotation noises arising from varying levels of worker expertise, skills, and motivation, and task difficulty or clarity~\cite{dawid1979maximum,sheng2008get,whitehill2009whose}, or reducing the annotation effort or costs through active learning approaches \cite{yan2011active,yang2018leveraging}.
%
Some work proposed new ways of collaboration, such as learning crowd vote aggregation models from features of the crowd task~\cite{Kamar_2012_combining} and leveraging crowds to learn features of ML models~\cite{flock_2015,Carlos_pattern}.
%
Recent work has shifted attention to human involvement in providing interpretations of model decisions and evaluating these interpretations~\cite{kyunglee2018perception, ribeiro2016should}.%, as well as debugging the system or the data~\cite{nushi2017human,yang2019scalpel}.
%
%Work can also be found on other ways of human-AI collaboration from learning crowd vote aggregation models from ``features'' of the crowd task by~\citet{Kamar_2012_combining}, to leveraging crowds to learn features of ML models, as in the pioneering paper by \citet{flock_2015} as well as~\citet{Carlos_pattern}.
%
%The line of work more relevant to ours is hybrid human-AI decision-making that we briefly review as follows.
%
%\subsection{Rejecting Machine Decisions}
%
A notable idea for hybrid human-AI decision-making was recently proposed by~\citet{Law_hearth_cscw18}: humans are involved after a machine decision is observed to have low confidence.
%
%paper below doesn't even discuss that?
%Similar ideas regarding the rejection of machine decisions have been discussed in the past \cite{de2000reject}.
%
Following works can be categorized in several dimensions, namely \emph{when} rejection happens, on \emph{what models}, and based on \emph{what criteria}~\cite{hendrickx2021machine}.
%
Regarding the ``when'', rejection can be implemented in three ways: the preemptive way where whether a data item needs to be handled by a human is decided beforehand~\cite{coenen2020probability}; the integrated way which uses a rejector inside the machine learning model (e.g., a rejection layer in a neural network) to decide whether a decision should be rejected~\cite{geifman2019reject}; and the dependent way, which is also the most common, which analyzes the rejection option after model decisions~\cite{geifman2017selective, de2000reject, grandvalet2008reject}.
%
%The dependent way is most often used, as we also study in our work.
%
In terms of ``what models'', work has been done on rejecting decisions made by a range of models, such as SVMs~\cite{grandvalet2008reject,coenen2020probability} and different neural networks~\cite{de2000reject,geifman2019reject}.
%
%One should note that in practice neural networks are often poorly calibrated~\cite{Guo2017,Balda2020}, making their decision confidence unreliable.
%An important consideration of rejecting decisions from neural networks is that they are often poorly calibrated~\cite{Guo2017,Balda2020}, making their decision confidence unreliable.
%
In our case, we apply the dependent way to reject models that are based on neural networks.
%
In terms of ``what criteria'',~\citet{geifman2017selective} proposed a rejection function based on a predefined risk value, an idea also explored in~\cite{nadeem2009reject}.
%
But unlike ours, their proposals do not consider the impact of machine decisions in a specific context.
%
The most relevant proposal to our work is from~\citet{de2000reject}, who studied a confidence metric for determining the optimal rejection threshold.
%
In their work, the threshold is calculated with simulations based on a set of predictions.
%
Going beyond defining cost values from simulations, our approach determines cost values based on users' perception of machine decisions using a survey study with crowd workers.
%
%Thus, we obtain a threshold that captures the implications of machine decisions from a human perspective.



\subsection{Value Assessment and Measurement}
%Due to algorithmic biases, value of machine learning systems has recently been intensively discussed across the ethics, design, and computer science domains.
%
Value is generally defined as desirable properties of an entity~\cite{birhane2021values}. Specifically for machine learning systems \citet{yurrita2022towards} have identified relevant properties, including individual empowerment, conservation, universalism, and openness.
%
%A big challenge in creating machine learning systems that conform to these properties is the operationalization of value, which means identifying context-specific values and translating them into concrete system specifications~\cite{shahin2021operationalizing}.
%
%Efforts have been made from both social and technological sides.
%
Examples include outlining ethical principles of algorithmic systems~\cite{fjeld2020principled}, developing value-based assessment frameworks~\cite{yurrita2022towards}, and proposing new metrics for evaluating machine learning systems that incorporate value parameters~\cite{casati2021value}. 
%
%Multiple measurement scales, including ME, were compared by \citet{checco2016mag} in a crowdsourced rating setting.
%
However, a research gap in measuring value in social contexts has been identified by~\citet{olteanu2017detection}, who investigated human-centered metrics for machine learning evaluation in hate speech detection.
%
Their work highlights the gap between accuracy-based evaluation metrics and user perception.
%
Our work represents a first step towards filling the gap in the context of hate speech detection using ME with a crowdsourced survey.
%
%To do so, we specifically consider and compare three types of measurement scales as part, namely Likert, magnitude estimation \cite{stevens1956direct}, and a 100-level scale \cite{roitero2018fine}, which we discuss in detail in the next sections.
