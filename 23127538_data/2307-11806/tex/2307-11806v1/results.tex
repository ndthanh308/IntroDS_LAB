\section{Results}
\label{s:5}

%This section shows the survey results of value ratios, reliability, and validity (defined in~\cref{sec:survey_design}).
%
%Then, we present the value of different models at different rejection thresholds.\footnote{All our code and data are submitted in the supplementary files.}
%As explained in~\cref{sec:survey_design}, we will evaluate the survey results by focusing on three different aspects: value ratios, reliability, and validity, before showing the total value of different models at all thresholds.
\subsection{Reliability and Validity}
%\subsection{Value Ratios}
\label{sec:results-costs}

First, for each survey question set, we calculate the median of all responses.
%
This step yields 40 values (eight values per scenario).
%
We use the median since data from both scales are highly skewed.
%
Then, we calculate the mean of the values ($V_{TP}, V_{TN}, V_{FP}, V_{FN}, V_r$) within each scenario, giving us the final five values for the TP, TN, FP, FN, and rejection cases.
%
The results for both scales can be seen in~\cref{tab:costs-reliability}.
%
The total value, $V$, is calculated at a later point in this section using the different values.
\begin{table}[t]
\centering
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{\textbf{ME}} & \multicolumn{2}{c}{\textbf{S100}}\\
\cmidrule(l){2-3} \cmidrule(l){4-5}
& $\boldsymbol{\alpha}$ & $\textbf{v}$ & $\boldsymbol{\alpha}$  & $\textbf{v}$ \\
\midrule
\textbf{TP}          & 0.07    & 18.15  & 0.04 & 77.00                                \\
\textbf{TN}          & 0.10    & 36.32  & 0.11  & 86.31                               \\
\textbf{FP}          & 0.39    & -16.69 & 0.07  & -51.00                               \\
\textbf{FN}          & 0.92    & -28.08 & 0.14  & -62.43                               \\
\textbf{Rejection}   & -0.31   & -4.82  & 0.07  & -16.37                               \\
\midrule
\textbf{All}         & 0.78    & ---       & 0.44                               &    ---                            \\
\bottomrule
\end{tabular}
\caption{Krippendorff's alpha ($\alpha$) and the scenario values ($v$) for TP, TN, FP, FN, and rejection scenarios. ME refers to Magnitude Estimation, and S100 refers to the 100-level scale.}
\label{tab:costs-reliability}
\end{table}

%\subsection{Reliability and Validity}
We calculate Krippendorff's alpha to measure the inter-rater reliability of all scenarios for each scale, as shown in~\cref{tab:costs-reliability}.
%
The last row of the table contains the $\alpha$ values for the entire scale, measuring the inter-rater reliability for all answers.
%
We observe that the ME scale has high inter-rater reliability while the 100-level scale is less reliable.
%
Also, participants using the ME scale tend to exhibit higher agreement regarding the FP and FN cases and systematically disagree on the rejected cases.
%
For the 100-level scale, we observe that participants have low agreement on all scenarios. 

%\subsection{Validity}

We analyze the validity of the ME scale by comparing the median normalized magnitude estimates with the median 100-level scores for each question set.
%
Figure~\ref{fig:validity} presents the correlation plot between the two scales.
%
A Shapiro-Wilk test indicates that the data of both scales do not follow a normal distribution ($p<0.05$).
%
Thus, we use the Spearman and Kendall rank correlation coefficients since these are non-parametric tests.
%
Spearman returned a $0.98$ and Kendall a $0.89$ correlation between the ME and the 100-level scales ($p<0.05$).
%
Finally, a Mann-Whitney U test between the ME and 100-level scales gives a large p-value, indicating no statistically significant difference between the two scales.
%shows that there the distributions of both scales are identical.

% Figure environment removed

\subsection{Total Model Value due to Threshold}
We evaluate the $V(\tau)$ function (i.e., the value at different rejection thresholds) using the values from the survey study obtained using the ME scale.
%
We train three different binary hate speech classification models on the~\citet{waseem2016hateful} dataset.
%
The used models are Logistic Regression (LR) with Character N-gram~\cite{waseem2016hateful}, a Convolutional Neural Network (CNN) based on~\citet{agrawal2018deep}, and a DistilBERT transformer~\cite{sanh2019distilbert}.
%
We use Temperature Scaling to calibrate the CNN and the DistilBERT models following the approach from~\citet{Guo2017}.
%
The model predictions are based on two different test datasets: the \textit{seen} dataset and the \textit{unseen} dataset.
%
The \textit{seen} dataset is the test set of~\citet{waseem2016hateful} and the \textit{unseen} dataset is a test set from a separate but similar source~\cite{basile2019semeval}.
%
We use the \textit{unseen} dataset to simulate how the models would perform in a more challenging, realistic use case.
%
Using unseen data that is similar but separate from the training set, we also investigate the impact of bias.
%
Finally, we calculate the total value as a function of the threshold, $V(\tau)$, for all models with the reject option at all possible rejection thresholds ($\tau$).
%
When $\tau\in[0.0, 0.5]$, all predictions are accepted since the confidence of all predictions is above 0.5 in the case of binary classification.
%
On the other hand, $\tau=1.0$ implies that all predictions are rejected.
%
We use the $v$ values of the ME scale from~\cref{tab:costs-reliability} to plot the results of all three models in~\cref{fig:metric-values-seen,fig:metric-values-unseen} using~\cref{for:final-V}.
%
The diamond-shaped markers indicate the optimal confidence thresholds for rejection at which the model achieves the highest total value.

% Figure environment removed

% Figure environment removed

Participants ascribe higher absolute values to TP and TN scenarios compared to FP and FN ones (see~\cref{tab:costs-reliability}), which results in all but one model having the highest value when all predictions are accepted (see~\cref{fig:metric-values-seen,fig:metric-values-unseen}).
%
%This effect can also be observed in~\cref{fig:metric-values-seen,fig:metric-values-unseen}, where for all but one model (LR for \textit{unseen} data) we achieve the highest value when all predictions are accepted.
%
The rejection rates (i.e., the percentage of rejected predictions) and accuracies of accepted predictions at the optimal threshold across the three classifiers can be seen in the first two rows of~\cref{tab:metric}.
%
\begin{table*}[]
\small
\centering
\begin{tabular}{lcccccccccc}
\toprule
& \multicolumn{3}{c}{\textbf{LR}} & \multicolumn{3}{c}{\textbf{DistilBERT}} & \multicolumn{3}{c}{\textbf{CNN}}\\
\cmidrule(l){2-4} \cmidrule(l){5-7} \cmidrule(l){8-10}
& $\boldsymbol{\tau}$ & \textbf{Acc}  & \textbf{RR} & $\boldsymbol{\tau}$ & \textbf{Acc} & \textbf{RR} & $\boldsymbol{\tau}$ & \textbf{Acc} & \textbf{RR} \\
\midrule
\textbf{Seen data}    & 0.500 & 0.853 & 0.000    & 0.500 & 0.853 & 0.000    & 0.500 &  0.845 & 0.000\\
\textbf{Unseen data}  & 0.531 & 0.646 & 0.043    & 0.500 & 0.643 & 0.000    & 0.500 &  0.624 & 0.000\\
\midrule
\textbf{Seen data ($V_{TP}=0, V_{TN}=0)$}   & 0.829 & 0.925 & 0.316    & 0.786 & 0.923 & 0.202    & 0.815 & 0.934 & 0.299 \\
\textbf{Unseen data ($V_{TP}=0, V_{TN}=0)$} & 0.999 & 0.818 & 0.991    & 0.974 & 1.000   & 0.996    & 0.961 & 0.833 & 0.980 \\
\bottomrule
\end{tabular}
\caption{The optimal rejection thresholds ($\tau$), the accuracy of the accepted predictions (Acc), and rejection rates (RR) of all models for both datasets using the values from the survey.}
\label{tab:metric}
\end{table*}
%
\begin{table*}[]
\small
\centering
\begin{tabular}{lccccccc}
\toprule
& \multicolumn{2}{c}{\textbf{LR}} & \multicolumn{2}{c}{\textbf{DistilBERT}} & \multicolumn{2}{c}{\textbf{CNN}}\\
\cmidrule(l){2-3} \cmidrule(l){4-5} \cmidrule(l){6-7}
& $\boldsymbol{V(\tau_O)}$ & \textbf{Acc} & $\boldsymbol{V(\tau_O)}$ & \textbf{Acc} & $\boldsymbol{V(\tau_O)}$ & \textbf{Acc} \\
\midrule
\textbf{Seen data}                  & 45534 & 0.853     & 45250 & 0.853    & 44893 & 0.845 \\
\textbf{Unseen data}                & 18563 & 0.631     & 19132 & 0.643    & 18385 & 0.624 \\
\midrule
\textbf{Seen data ($V_{TP}=0, V_{TN}=0)$}     & 4325 & 0.853      & 5172 & 0.853     & 5460 & 0.845 \\
\textbf{Unseen data ($V_{TP}=0, V_{TN}=0)$}   & 4404 & 0.631      & 4213 & 0.643     & 5291  & 0.624\\
\bottomrule
\end{tabular}
\caption{The total values $V(\tau_O)$ and the accuracies (Acc) of all models. Here, $\tau_O$ is the optimal rejection threshold.}
\label{tab:metric2}
\end{table*}
%
If we were to take the view that the users' baseline expectation is correct machine decisions, then we can set the value of TP and TN to 0.0 and repeat our analysis to examine how $V(\tau)$ behaves as we consider only punishing incorrect predictions without rewarding correct predictions made by the model (considering the regulation effect discussed in~\cref{s:3}).
%To examine how $V(\tau)$ behaves when we only consider the value of FP, FN, and rejected predictions, therefore focusing on punishing incorrect predictions instead of rewarding correct predictions, we set the value of TP and TN equal to 0.0 in~\cref{fig:metric-values-seen-tptn0,fig:metric-values-unseen-tptn0}.
%
Figures~\ref{fig:metric-values-seen-tptn0} and~\ref{fig:metric-values-unseen-tptn0} demonstrate that the optimal values are achieved at increased rejection thresholds ($\tau$).
%
The last two rows of~\cref{tab:metric} show that the optimal $\tau$ values result in higher accuracies for the \textit{seen} data while rejecting 31.6\% of predictions.
%
For the \textit{unseen} data, we achieve high accuracies but reject a large fraction of the predictions.

We also compare the effect of using value and the widely-used accuracy metric in selecting the best model, shown in~\cref{tab:metric2}. We observe that both metrics return the same optimal model when correct predictions are rewarded, though there is a difference between \emph{seen} and \emph{unseen} cases. When only incorrect predictions are punished, the optimal models are different as measured by the two metrics: in the case of \emph{seen} data, both LR and DistilBERT perform better than CNN when measured by accuracy, while CNN delivers the highest value; the same observation holds true in the case of \emph{unseen} data -- where the optimal model switches from DistilBERT to CNN when we consider the value they deliver instead of  accuracy.

% xx discuss results xx. Second, we created the Accuracy-Rejection curves of both models evaluated on both datasets and included the optimal rejection threshold points as well in \cref{fig:accuracy-rejection}. xx discuss results, perhaps we can use these plots to show that only focusing on accuracy metrics is not enough. perhaps the plots indicate we can even reject fewer samples and keep high accuracy while the optimal rejection threshold indicates that we should reject more samples because the stakes are too high xx. Finally, in \cref{fig:metric-values}, we present the resulting metric values for all possible rejection thresholds between 0\% (rejecting nothing) and 100\% (rejecting everything). The optimal rejection thresholds are indicated with points on the curves.




% % Figure environment removed

