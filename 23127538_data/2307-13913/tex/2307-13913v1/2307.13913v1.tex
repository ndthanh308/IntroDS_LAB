\documentclass[11pt]{amsart}
%\documentclass{jams-l}
%\documentstyle[11pt]{article}
%\input{amsmath}
%\overfullrule=5pt
\usepackage{amsmath,amssymb,mathrsfs,color}
\usepackage[title]{appendix}
\usepackage{amssymb}
\allowdisplaybreaks
%%%%    Macros used in the article: you can change if you like.

\topmargin=0pt \oddsidemargin=0pt \evensidemargin=0pt
\textwidth=16cm \textheight=23.0cm \raggedbottom

\let\cal=\mathcal
\def\N{{\mathbb N}}
\def\Z{{\mathbb Z}}
\def\R{{\mathbb R}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\T{{\mathbb T}}
\def\var{\varepsilon}
\def\eps{\epsilon}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{conj}[thm]{Conjecture}
\theoremstyle{definition}
\newtheorem{de}[thm]{Definition}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{exam}[thm]{Example}
\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\ab}[1]{ |{\mbox det}( #1 )|}
\newcommand{\sgn}{{\rm Sgn\,}}
\newcommand{\defi}{\par\medbreak\it Definition : \rm}
\newcommand{\lep}{<\cdot\,}
\newcommand{\I}{\Cal I}
\newcommand{\J}{\Cal J}
\newcommand{\rmd}{{\rm d}}
\newcommand{\rme}{{\rm e}}
\allowdisplaybreaks

\begin{document}


\title[Wasserstein convergence rates in the invariance principle]{Wasserstein convergence rates in the invariance principle for sequential dynamical systems}

\author{Zhenxin Liu}
\address{Z. Liu: School of Mathematical Sciences, Dalian University of Technology, Dalian
116024, P. R. China}
\email{zxliu@dlut.edu.cn}

\author{Zhe Wang}
\address{Z. Wang: School of Mathematical Sciences, Dalian University of Technology, Dalian
116024, P. R. China}
 \email{zwangmath@hotmail.com; wangz1126@mail.dlut.edu.cn}


\date{July 25, 2023}

\subjclass[2010]{37A50, 60F17, 37C99, 60B10}

\keywords{Invariance principle, rate of convergence, Wasserstein distance, sequential dynamical systems}

\begin{abstract}
In this paper, we consider the convergence rate with respect to Wasserstein distance in the invariance principle for sequential dynamical systems. We utilize and modify the techniques previously employed for stationary sequences to address our non-stationary case. Given some mild assumptions, we can apply our result to a large class of dynamical systems, including sequential $\beta_n$-transformations, piecewise uniformly expanding maps with additive noise in one-dimensional and multidimensional case, and so on.
\end{abstract}

\maketitle

\section{Introduction}
\setcounter{equation}{0}

There is considerable interest in the study of statistical properties for deterministic dynamical systems exhibiting hyperbolicity, wherein the same map is iterated all along the time. Due to the presence of an absolutely continuous invariant measure, the observable processes along the orbit become stationary. However,
in many physical applications, it is often the case that different maps, close to a fixed one, are iterated randomly. This situation can be described as a (discrete) time-dependent dynamical system. Over the past few decades, there has been a growing interest in proving statistical properties for time-dependent dynamical systems, including sequential dynamical systems and random dynamical systems. Unlike the time-independent systems, time-dependent systems lack a universal invariant measure across all maps. Due to the lack of invariant measure and the fact that maps change with time, the processes are non-stationary, which causes some difficulties in study.

Sequential dynamical systems, as introduced by Berend and Bergelson \cite{BB84}, consist of a composition of different self-maps defined on a space $X$, represented by $T_k\circ T_{k-1}\circ\cdots\circ T_1$. The literature on statistical properties for such systems is already extensive. Conze and Raugi's seminal paper \cite{Conze07} explored the dynamical Borel-Cantelli lemma and the central limit theorem (CLT) for a sequence of one-dimensional piecewise expanding maps. Haydn et al \cite{Haydn17} further investigated the almost sure invariance principle (ASIP) for sequential dynamical systems and some other non-stationary systems, which implies the CLT, the law of the iterated logarithm and their functional forms. Additionally, the extreme value theory \cite{FFV17} and concentration inequality \cite{AR16} were also obtained for the systems considered in \cite{Haydn17}. Notably, for the systems discussed in the mentioned references, their transfer operator with respect to the Lebesgue measure is quasi-compact on a suitable Banach space. However, when considering the composition of Pomeau-Manneville-like maps, obtained by perturbing the slope at the indifferent fixed point 0, the transfer operator is not quasi-compact. Noteworthy results in this situation include discussions on
the loss of memory \cite{AHNTV15,KL22}, the CLT \cite{NTV18}, the ASIP \cite{S19}, the large deviation \cite{Nicol21}, among others.

In the present paper, we focus on the rate of convergence with respect to Wasserstein distance in the invariance principle for sequential dynamical systems, whose transfer operator is quasi-compact in the setting of \cite{Conze07,Haydn17}. The invariance principle (also known as the functional CLT) states that a stochastic process constructed by the sums of random variables with suitable scale converges weakly to a Brownian motion. Here, we employ the Wasserstein distance to measure the rate of weak convergence.  For $p\ge 1$, we denote by $\cal W_p(P,Q)$ the Wasserstein distance between the distributions $P$ and $Q$ on a Polish space $(\cal X,d)$ (see \cite[Definition~6.1]{V09}):
\[
\cal W_p(P,Q)= \inf \{ [\mathbb{E} {d(X,Y)}^p]^{1/p}; \hbox{law} (X)=P, \hbox{law} (Y)=Q \}.
\]
In comparison to the L\'{e}vy-Prokhorov distance, the Wasserstein distance is stronger and contains more information since it involves the metric of the underlying space. This distance finds important applications in the fields of optimal transport, geometry, partial differential equations etc; see e.g.\ Villani~\cite{V09} for details.

To the best of our knowledge, there are rare results in the literature regarding the convergence rate in the invariance principle for non-stationary dynamical systems. In the probability theory literature, we refer to \cite{H23} for the convergence rate (in the L\'evy-Prokhorov distance) in the invariance principle for $\alpha$-mixing triangular arrays. Dedecker et al \cite{DMR22} provided rates of convergence (in the
$\cal W_1$-distance and the Kolmogorov distance) in the CLT for martingale in the non-stationary setting.  Turning to the dynamical systems literature, Hella and Lepp\"anen \cite{HL20} obtained the convergence rate (in the $\cal W_1$-distance) in the CLT for time-dependent intermittent maps. Furthermore, Antoniou and Melbourne \cite{AM19} established the convergence rate in the L\'evy-Prokhorov distance in the invariance principle for nonuniformly hyperbolic systems in the stationary case, while Liu and Wang \cite{Liu23} obtained the Wasserstein convergence rate in
the same setting.

In this paper, we obtain the Wasserstein convergence rate $O(n^{-\frac{1}{4}+\delta})$ in the invariance principle for sequential dynamical systems, where $\delta$ can be arbitrarily small. To derive the convergence rate, we employ techniques developed for stationary systems, particularly the martingale approximation methods and the martingale Skorokhod embedding theorem. The convergence rate we obtain may not be optimal. However, it is well known that one cannot get a better result than $O(n^{-\frac{1}{4}})$ by means of the Skorokhod embedding theorem; see \cite{B73,S72} for some details. In the non-stationary setting, the variance may not exhibit linear growth. However, in our case, a linear growth condition on the variance is necessary, since this condition guarantees the convergence in the Wasserstein distance, which is explained in Remark~\ref{lin}. Furthermore, this condition can be satisfied for a large class of one dimensional and multidimensional sequential expanding maps as demonstrated in \cite{Conze07}.

To establish the related convergence rate in the invariance principle for sequential dynamical systems, whose transfer operator is not quasi-compact,  a second martingale-coboundary decomposition \cite{KKM18}, similar to that in the stationary case, may be the key. However, the decomposition is currently unavailable and it is the ongoing focus of our research.

The remainder of this paper is organized as follows. In Section 2, we introduce the setting and main result of this paper. In Section 3, we recall the martingale decomposition for sequential dynamical systems and give a result on moment estimate.
In Section 4, we prove the main result. In the last section, we give some applicatioins to explain our result.

Throughout the paper, we use $1_A$ to denote the indicator function of measurable set $A$. As usual, $a_n=O(b_n)$ means that there exists a constant $C>0$ such that $|a_n|\le C |b_n|$ for all $n\ge 1$, and $\|\cdot\|_{p}$ means the $L^p$-norm. For simplicity we write $C$ to denote constants independent of $n$ and $C$ may change from line to line.  We use $\rightarrow_{w}$ to denote the weak convergence in the sense of probability measures \cite{Bill99}. We denote by $C[0,1]$ the space of all continuous functions on $[0,1]$ equipped with the supremum distance $d_C$, that is
\[
d_C(x,y):=\sup_{t\in [0,1]}|x(t)-y(t)|, \quad x,y\in C[0,1].
\]
We use $\P_X$ to denote the law/distribution of random variable $X$ and use $X=_d Y$ to mean $X, Y$ sharing the same distribution. We use the notation $\mathcal{W}_p(X,Y)$ to mean $\mathcal{W}_p(\P_X, \P_Y)$ for the sake of simplicity.
\section{Setting and main result}
In this section, we first recall an introduction to sequential dynamical systems and some basic assumptions, which were described in detail
in~\cite{Conze07,Haydn17}, and then we state our main result.
\subsection{Sequential dynamical systems}
Let $M$ be a compact subset of $\R^d$ or a torus $\T^d$ with the Lebesgue measure $m$. Consider a family of non-invertible maps $T_\alpha:M\to M$, which are non-singular with respect to $m$ (i.e. $m(T_\alpha^{-1}E)=0$ if and only if $m(E)=0$ for all Borel measurable sets $E\subset M$).
We take a countable sequence of maps
$\{T_k\}_{k\ge 1}$ from it; this sequence defines a sequential dynamical system.

We denote by $\{\cal{T}^n\}_{n\ge 0}$ the sequence of composed maps
\[
\cal{T}^n:=T_n\circ T_{n-1}\circ\cdots\circ T_1 \quad\text{for~} n\ge 1, \text{~and~} \cal T^0:=Id.
\]
The transfer operator $P_\alpha$ corresponding to $T_\alpha$ is defined by
\[
\int_{M}P_\alpha f\cdot g\rmd m=\int_{M} f\cdot g\circ T_\alpha\rmd m \quad\hbox{for~all~} f\in L^1, g\in L^\infty.
\]
Similar to $\cal{T}^n$, we can define the composition of operators as
\[
\cal{P}^n:=P_n\circ P_{n-1}\circ\cdots\circ P_1 \quad\text{for~} n\ge 1, \text{~and~} \cal P^0:=Id.
\]
Then it is easy to check that
\[
\int_{M} \cal{P}^n f\cdot g\rmd m=\int_{M} f\cdot g\circ \cal{T}^n\rmd m \quad\hbox{for~all~} f\in L^1, g\in L^\infty.
\]

For a fixed sequence $\{\cal T^n\}_{n\ge 0}$, we set $\cal B_n:=(\cal T^n)^{-1}\cal B$, the $\sigma$-algebra associated with $n$-fold pull back of the Borel $\sigma$-algebra $\cal B$. Since the transformations $T_n$ are non-invertible,
we obtain a decreasing sequence of $\sigma$-algebras $\{\cal B_n\}_{n\ge 0}$, i.e. $\cal B_n\subset \cal B_m$ for $n\ge m\ge 0$. It was described
in~\cite{Conze07} that for $f\in L^{\infty}$, the quotients $|\cal{P}^n f/\cal{P}^n 1|$ are bounded by $\|f\|_\infty$ on the set
$\{\cal{P}^n 1>0\}$ and we have $\cal{P}^n f(x)=0$ on $\{\cal{P}^n 1=0\}$. Then we can define $|\cal{P}^n f/\cal{P}^n 1|=0$ on $\{\cal{P}^n 1=0\}$.
Therefore, we have
\begin{align}
\E(f|\cal B_k)=\big(\frac{\cal{P}^k f}{\cal{P}^k 1}\big)\circ \cal T^k,
\end{align}
and,
\begin{align}\label{cexp}
\E(f\circ \cal T^l|\cal B_k)=\big(\frac{P_k\cdots P_{l+1}(f\cal{P}^l 1)}{\cal{P}^k 1}\big)\circ \cal T^k,\quad 0\le l\le k\le n.
\end{align}
Here, the expectation is taken with respect to the Lebesgue measure $m$.
\subsection{Assumptions}
Let $\cal V\subset L^1 (1\in \cal V)$ be a Banach space of  functions from $M$ to $\R$ with norm $\|\cdot\|_\alpha$, such that $\|v\|_\infty\le C_1\|
v\|_\alpha$. Moreover, we assume that for $v_1, v_2\in \cal V$, we have $\|v_1v_2\|_\alpha\le C_2\|
v_1\|_\alpha\|v_2\|_\alpha$. For example, we can let $\cal V$ be the Banach space of bounded variation functions on a compact interval of $\R$ with the norm $\|\cdot\|_{BV}$ given by the sum of the $L^1$ norm and the total variation $|\cdot|_{bv}$, or we can take $\cal V$ to be the space of $\alpha$-H\"older functions on a compact set of $\R^d$ with the norm $\|\cdot\|_\alpha=\|\cdot\|_\infty+|\cdot|_\alpha$, where $|\cdot|_\alpha$
denotes the H\"older semi-norm.

Following the setting described in~\cite{Conze07} and \cite{Haydn17}, we now recall the required properties (DEC) and (MIN).

{\bf Property (DEC).} There exist constants $C>0$, $\gamma\in(0,1)$ such that for any $n\ge 1$, any sequence $P_1, P_2,\ldots, P_n$ and
any $v\in \cal V$ with $\int_M v\rmd m=0$, we have
\[
\|P_n\circ P_{n-1}\circ\cdots\circ P_1v\|_\alpha\le C\gamma^n\|v\|_\alpha.
\]


%\begin{rem}
%In~\cite{Conze07} and \cite{Haydn17}, the authors introduced a Banach space $\cal{V}\subset L^1 (1\in \cal{V})$ of functions over $M$ with norm $\|\cdot\|_\alpha$ such that $\|v\|_\infty\le C\|v\|_\alpha$. Typical examples of Banach space $\cal{V}$ are the space of bounded variation functions, or the space of $\alpha$-H\"older functions. In this paper, we consider the space of $\alpha$-H\"older functions with norm
%$\|v\|_\alpha=\|v\|_\infty+|v|_\alpha$.
%\end{rem}

{\bf Property (MIN).} There exists $\delta>0$ such that for any sequence $P_1, P_2,\ldots, P_n$, we have the uniform lower bound
\[
\inf_{x\in M}P_n\circ P_{n-1}\circ\cdots\circ P_11(x)\ge \delta, \quad\forall n\ge 1.
\]
\subsection{Main result}
Let $v_n:M\to\R$ be a family of functions in $\cal V$ such that $\sup_n\|v_n\|_\alpha<\infty$.
Denote $S_n:=\sum_{i=0}^{n-1}\bar v_i\circ \cal T^i$, $\Sigma_n^2:=\E(\sum_{i=0}^{n-1}\bar v_i\circ \cal T^i)^2$, where
$\bar v_i:=v_i-\int_M v_i\circ \cal T^i\rmd m$. For every $t\in[0,1]$, set
\[
N_n(t):=\min\{1\le k\le n: t\Sigma_n^2\le\Sigma_{k}^2\}.
\]
Consider the following continuous processes $W_{n}(t)\in C[0,1]$ defined by
\begin{equation}\label{wnt}
W_{n}(t):=\frac{1}{\Sigma_n}\bigg[\sum_{i=0}^{N_n(t)-1}\bar v_i\circ \cal T^i+\frac{t\Sigma_n^2-\Sigma_{N_n(t)-1}^2}{\Sigma_{N_n(t)}^2-\Sigma_{N_n(t)-1}^2}\bar v_{N_n(t)}\circ \cal T^{N_n(t)}\bigg],\quad t\in[0,1].
\end{equation}

If the variance $\Sigma_n^2$ satisfies an additional growth condition, Haydn et al \cite{Haydn17} obtained that the ASIP holds.
\begin{prop}\cite[Theorem~3.1]{Haydn17}\label{asip}
Assume that $\{\cal T^n\}$ satisfies (DEC) and (MIN) and $\Sigma_n\ge n^{\frac{1}{4}+\delta}$ for some $0<\delta<\frac{1}{4}$. Then $\{v_n\circ \cal T^n\}$ satisfies the ASIP, i.e.\ enlarging our probability space if necessary it is possible to find a sequence of independent centered Gaussion variables $\{Z_k\}$
such that for any $\beta<\delta$,
\[
\sup_{1\le k\le n}\big|\sum_{i=1}^{k}\bar v_i\circ \cal T^i-\sum_{i=1}^{k}Z_i\big|=o(\Sigma_n^{1-\beta}) \quad m-a.s.
\]
Furthermore, $\sum_{j=1}^{n}\E(Z_j^2)=\Sigma_n^2+O(\Sigma_n)$.
\end{prop}
We can deduce from Proposition~\ref{asip} that the weak invariance principle holds, i.e. $W_n\to_w W$ in $C[0,1]$, where $W$ is a standard Brownian motion. Now, we introduce our main result on the Wasserstein convergence rate in the invariance principle.
\begin{thm}\label{thnon}
Assume that $\{\cal T^n\}$ satisfies (DEC) and (MIN) and that $\Sigma_n^2$ grows linearly, that is, $\Sigma_n^2$ satisfies $c_1n\le \Sigma_n^2\le c_2n$
for some $c_1, c_2>0$ and all $n$ large enough. Let $\{v_n\}$ be a sequence of functions in $\cal V$ with $\sup_n\|v_n\|_\alpha<\infty$. Then for any $\delta>0$, there exists a constant $C>0$ such that $\mathcal{W}_{p}(W_{n},W)\leq C n^{-(\frac{1}{4}-\delta)}$ for $n\gg 1$ and $p\ge 2$, where $W$ is a standard Brownian motion.
\end{thm}
\begin{rem}\label{lin}
 It should be noted that the linear growth condition on $\Sigma_n^2$ is necessary for the convergence in Wasserstein distance. Indeed, by the moment estimates on $W_n$ (see Proposition~\ref{mom}),
we can see that the linear growth condition on $\Sigma_n^2$ guarantees the boundedness of $p$-moment of $W_n$ for all $p\ge 2$.
%Based on the fact that $W_n\to_{w} W$ in $C[0,1]$, it follows from the proof of Theorem~3.3 in~\cite{Liu23} that $\mathcal{W}_{p}(W_{n},W)\to 0$ in $C[0,1]$ for all $p\ge 1$.
If the variance $\Sigma_n^2$ is growing at a slower rate compared to the linear growth, the Wasserstein distance between the distributions of $W_n$ and Brownian motion does not converge. Moreover, we refer to the proof of Theorem~3.3 in~\cite{Liu23} for an explicit proof of the convergence result.
\end{rem}

\begin{rem}
Note that our method does not work for the estimate of $\mathcal{W}_1(W_n, W)$. But $\mathcal{W}_{q}(W_{n},W)$ $\le \mathcal{W}_{p}(W_{n},W)$ for $q\le p$, so $\mathcal{W}_{1}(W_{n},W)$ can be controlled by $\mathcal{W}_{q}(W_{n},W)$ for $q>1$. It seems an interesting question to estimate the convergence rate for $\mathcal{W}_1(W_n, W)$ directly, which probably produces a better rate.
\end{rem}

\section{Martingale decomposition}
%\setcounter{equation}{0}
In the following, we assume that $\{\cal T^n\}$ satisfies the conditions (DEC) and (MIN).
As in \cite{Conze07}, we define the operator $Q_n$ by $Q_nv:=\frac{P_n(v\cal P^{n-1}1)}{\cal P^{n}1}$. Set $h_0:=0$ and for $n\ge 1$,
\begin{align*}
h_n&:=Q_n\bar v_{n-1}+Q_n\circ Q_{n-1}\bar v_{n-2}+\cdots+Q_n\circ Q_{n-1}\circ \cdots \circ Q_1\bar v_{0}\\
&=\frac{1}{\cal P^{n}1}\big[P_n(\bar v_{n-1}\cal P^{n-1}1)+P_n\circ P_{n-1}(\bar v_{n-2}\cal P^{n-2}1)+\cdots+P_n\circ P_{n-1}\circ \cdots \circ P_{1}(\bar v_{0}\cal P^{0}1)\big].
\end{align*}
Since $\{\bar v_{n-k}\cal P^{n-k}1\}_{1\le k\le n}$ belongs to $\cal V$ with mean zero, by the properties (DEC) and (MIN), $\|h_n\|_\alpha$ is uniformly bounded. In particular, $h_n\in L^{\infty}(M)$.

Define $\psi_n=\bar v_{n}+h_n-h_{n+1}\circ T_{n+1}$. Then $\|\psi_n\|_\infty\le \|\bar v_{n}\|_\infty+2\|h_n\|_\infty<\infty$.
 It follows from~\cite{Conze07} that $\{\psi_{n}\circ \mathcal T^n\}_{n\ge0}$ is a sequence of reverse martingale differences for the filtration $\{\cal B_n\}_{n\ge 0}$, and we have
\[
\sum_{i=0}^{n-1}\bar v_{i}\circ \mathcal T^i=\sum_{i=0}^{n-1}\psi_{i}\circ \mathcal T^i+h_{n}\circ \mathcal T^{n}.
\]


\begin{prop}\label{var}
$\sigma_{n}^{2}=\Sigma_{n}^{2}+O(1)$, where $\sigma_n^2=\E(\sum_{i=0}^{n-1}\psi_i\circ \cal T^i)^2$.
\end{prop}
\begin{proof}
It follows from \cite[Lemma~2.1]{NTV18} that $\sigma_n^2=\Sigma_n^2-\int h_{n}^2\circ \cal T^{n}$. Since $h_n$ is uniformly bounded, we obtain the conclusion.
\end{proof}

\begin{prop}\label{mom}
$\left\|\max_{1\le k\leq n}|\sum_{i=0}^{k-1}\bar v_{i}\circ \mathcal T^i|\right\|_{2p}\leq Cn^{1/2}$ for all $n\ge 1$ and $p\ge1$.
\end{prop}

\begin{proof}
Fix $n\ge1$. We take $X_i=\bar v_{n-i}\circ \mathcal T^{n-i}$, $Y_i=\psi_{n-i}\circ \mathcal T^{n-i}$ and
$H_i=h_{n-i}\circ \mathcal T^{n-i}$ for $1\le i\le n$. Then $Y_i=X_i+H_i-H_{i-1}$, and $\{Y_i,1\le i\le n\}$ is
a sequence of martingale differences for the filtration $\cal G_i=(\cal T^{n-i})^{-1}\cal B$.

By the formula of $Y_i$, we can write
\begin{align*}
\sum_{k=i}^{l}\E(X_k|\cal G_i)&=\sum_{k=i}^{l}\big[\E(Y_k|\cal G_i)-\E(H_k|\cal G_i)+\E(H_{k-1}|\cal G_i)\big]\\
&=\sum_{k=i}^{l}\E(Y_k|\cal G_i)+\E(H_{i-1}|\cal G_i)-\E(H_l|\cal G_i).
\end{align*}
Since $\E(Y_k|\cal G_i)=0$ for $k>i$, we have
\[
\sum_{k=i}^{l}\E(X_k|\cal G_i)=\E(Y_i|\cal G_i)+\E(H_{i-1}|\cal G_i)-\E(H_l|\cal G_i).
\]
This implies that
\[
\max_{1\le i\le l\le n}\bigg\|\sum_{k=i}^{l}\E(X_k|\cal G_i)\bigg\|_{p}\le \max_{1\le i\le l\le n}(\|Y_i\|_{p}+\|H_{i-1}\|_{p}+\|H_l\|_{p}).
\]
Since $\{X_i\}_{1\le i\le n}$ are uniformly bounded, we have
\[
\max_{1\le i\le l\le n}\bigg\|X_i\sum_{k=i}^{l}\E(X_k|\cal G_i)\bigg\|_{p}\le \max_{1\le i\le l\le n} \|X_i\|_\infty(\|Y_i\|_{p}+\|H_{i-1}\|_{p}+\|H_l\|_{p})<\infty.
\]
Hence, by Rio's inequality \cite{Rio17} (see Proposition~\ref{rio}),
\begin{align}\label{mar}
\E\bigg(\max_{1\le j\le n}\big|\sum_{i=1}^{j}\bar v_{n-i}\circ \mathcal T^{n-i}\big|^{2p}\bigg)\le Cn^p.
\end{align}
Writing $\sum_{i=0}^{k-1}\bar v_{i}\circ \mathcal T^i=\sum_{i=1}^{n}\bar v_{n-i}\circ \mathcal T^{n-i}-\sum_{j=1}^{n-k}\bar v_{n-j}\circ \mathcal T^{n-j}$, we obtain
\[\bigg\|\max_{k\leq n}\big|\sum_{i=0}^{k-1}\bar v_{i}\circ \mathcal T^i\big|\bigg\|_{2p}\leq Cn^{1/2}.\]
\end{proof}
\begin{rem}\label{mar}
Since $\{\psi_{n-i}\circ \mathcal T^{n-i},1\le i\le n\}$ is a sequence of martingale differences for the filtration $\cal G_i=(\cal T^{n-i})^{-1}\cal B$. By Burkholder's inequality \cite{Bu73}, for $p\ge1$, we have
\[\bigg\|\max_{k\leq n}\big|\sum_{i=1}^{k}\psi_{n-i}\circ \mathcal T^{n-i}\big|\bigg\|_{2p}\leq Cn^{1/2}\max_{1\le i\leq n}\|\psi_{n-i}\|_{2p}.\]
\end{rem}

\section{Proof of Theorem \ref{thnon}}
\setcounter{equation}{0}
Recall that $\sigma_n^2=\E(\sum_{i=0}^{n-1}\psi_i\circ \cal T^i)^2=\sum_{i=0}^{n-1}\E(\psi_i^2\circ \cal T^i)$. For every $t\in[0,1]$, set
\[
r_n(t):=\min\{1\le k\le n: t\sigma_n^2\le\sigma_{k}^2\}.
\]
Similar to $W_n$, we define the following continuous processes $M_{n}(t)\in C[0,1]$ by
\begin{equation}\label{exp}
M_{n}(t):=\frac{1}{\sigma_n}\bigg[\sum_{i=0}^{r_n(t)-1}\psi_i\circ \cal T^i+\frac{t\sigma_n^2-\sigma_{r_n(t)-1}^2}{\sigma_{r_n(t)}^2-\sigma_{r_n(t)-1}^2}\psi_{r_n(t)}\circ \cal T^{r_n(t)}\bigg],\quad t\in[0,1].
\end{equation}

{\bf Step 1. Estimation of the convergence rate between $W_n$ and $M_n$.}
\begin{lem}\label{wmn}
Let $p\ge 2$. Then there exists a constant $C>0$ such that for all $n\gg1$,
\[\bigg\|\sup_{t\in[0,1]}|W_{n}(t)-M_n(t)|\bigg\|_{p}\leq Cn^{-\frac{1}{2}}.\]
\end{lem}
\begin{proof}
We can estimate that
\begin{align*}
&\bigg\|\sup_{t\in[0,1]}\big|\frac{1}{\Sigma_n}\sum_{i=0}^{N_n(t)-1}\bar v_i\circ \cal T^i-\frac{1}{\sigma_n}\sum_{i=0}^{r_n(t)-1}
\psi_i\circ \cal T^i\big|\bigg\|_{p}\\
\le&\big|\frac{1}{\Sigma_n}-\frac{1}{\sigma_n}\big|\bigg\|\sup_{t\in[0,1]}\big|\sum_{i=0}^{N_n(t)-1}\bar v_i\circ \cal T^i\big|\bigg\|_{p}+\frac{1}{\sigma_n}\bigg\|\sup_{t\in[0,1]}\big|\sum_{i=0}^{N_n(t)-1}\bar v_i\circ \cal T^i-\sum_{i=0}^{r_n(t)-1}
\psi_i\circ \cal T^i\big|\bigg\|_{p}.
\end{align*}
For the first term, by Propositions~\ref{var} and~\ref{mom}, we have for $n\gg1$,
\begin{align*}
&\big|\frac{1}{\Sigma_n}-\frac{1}{\sigma_n}\big|\bigg\|\sup_{t\in[0,1]}\big|\sum_{i=0}^{N_n(t)-1}\bar v_i\circ \cal T^i\big|\bigg\|_{p}\\
= &\big|\frac{\Sigma_n-\sigma_n}{\Sigma_n\cdot \sigma_n}\big|\bigg\|\max_{1\le k\le n}\big|\sum_{i=0}^{k-1}\bar v_i\circ \cal T^i\big|\bigg\|_{p}\\
\le &C\frac{1}{\Sigma_n^2}\cdot n^\frac{1}{2}\le Cn^{-\frac{1}{2}}.
\end{align*}
To deal with the second term, we first note that
\begin{align*}
&\max_{1\le j\le n}\big|\frac{\Sigma_j^2}{\Sigma_n^2}-\frac{\sigma_j^2}{\sigma_n^2}\big|
\le \max_{1\le j\le n}\big|\frac{\Sigma_j^2}{\Sigma_n^2}-\frac{\sigma_j^2}{\Sigma_n^2}\big|
       +\max_{1\le j\le n}\big|\frac{\sigma_j^2}{\Sigma_n^2}-\frac{\sigma_j^2}{\sigma_n^2}\big|\\
\le&\frac{1}{\Sigma_n^2}\max_{1\le j\le n}|\Sigma_j^2-\sigma_j^2|+\big|\frac{1}{\Sigma_n^2}-\frac{1}{\sigma_n^2}\big|\max_{1\le j\le n} |\sigma_j^2|\\
\le&\frac{1}{\Sigma_n^2}\max_{1\le j\le n}|\Sigma_j^2-\sigma_j^2|+\big|\frac{1}{\Sigma_n^2}-\frac{1}{\sigma_n^2}\big|\sigma_n^2\\
\le& \frac{2}{\Sigma_n^2}\max_{1\le j\le n}|\Sigma_j^2-\sigma_j^2|\le \frac{M}{\Sigma_n^2}
\end{align*}
by Proposition~\ref{var}. Since $n$ is sufficiently large, $\frac{M}{\Sigma_n^2}$ can be sufficiently small. Then
\begin{align*}
&\bigg\|\sup_{t\in[0,1]}\big|(\sum_{i=0}^{N_n(t)-1}-\sum_{i=0}^{r_n(t)-1})\psi_i\circ \cal T^i\big|\bigg\|_{p}\\
\le &\bigg\|\sup_{t\in[0,1]}\big|(\sum_{i=0}^{N_n(t)-1}-\sum_{i=0}^{r_n(t)-1})\psi_i\circ \cal T^i\big|1_{\big\{\max_{1\le j\le n}\big|\frac{\Sigma_j^2}{\Sigma_n^2}-\frac{\sigma_j^2}{\sigma_n^2}\big|\le \frac{M}{\Sigma_n^2}\big\}}\bigg\|_{p}\\
\le &\bigg\|\sup_{|s-t|<\frac{2M}{\Sigma_n^2}}\big|(\sum_{i=0}^{r_n(t)-1}-\sum_{i=0}^{r_n(s)-1})\psi_i\circ \cal T^i\big|\bigg\|_{p}\\
\le &2\max_{0\le j\le n-1}\|\psi_i\circ \cal T^i\|_p.
\end{align*}
Since $\psi_n=\bar v_{n}+h_n-h_{n+1}\circ T_{n+1}$ and $\psi_n$, $h_n$ are uniformly bounded, we can estimate the second term that
\begin{align*}
&\frac{1}{\sigma_n}\bigg\|\sup_{t\in[0,1]}\big|\sum_{i=0}^{N_n(t)-1}\bar v_i\circ \cal T^i-\sum_{i=0}^{r_n(t)-1}
\psi_i\circ \cal T^i\big|\bigg\|_{p}\\
=&\frac{1}{\sigma_n}\bigg\|\sup_{t\in[0,1]}\big|\sum_{i=0}^{N_n(t)-1}(\psi_i+h_{i+1}\circ T_{i+1}-h_i)\circ \cal T^i-\sum_{i=0}^{r_n(t)-1}
\psi_i\circ \cal T^i\big|\bigg\|_{p}\\
\le &\frac{1}{\sigma_n}\bigg\|\sup_{t\in[0,1]}\big|(\sum_{i=0}^{N_n(t)-1}-\sum_{i=0}^{r_n(t)-1})\psi_i\circ \cal T^i\big|\bigg\|_{p}+\frac{1}{\sigma_n}\bigg\|\sup_{t\in[0,1]}\big|h_{N_n(t)}\circ \cal T^{N_n(t)}-h_1\circ \cal T^1\big|\bigg\|_{p}\\
\le &\frac{2}{\sigma_n}\max_{0\le j\le n-1}\|\psi_i\circ \cal T^i\|_p+\frac{2}{\sigma_n}\max_{0\le j\le n-1}\|h_i\circ \cal T^i\|_p
\le C\sigma_n^{-1}\le Cn^{-1/2}.
\end{align*}
Since $\psi_n$, $\bar v_n$ are uniformly bounded, we have
\[
\bigg\|\sup_{t\in[0,1]}|W_{n}(t)-\frac{1}{\Sigma_n}\sum_{i=0}^{N_n(t)-1}\bar v_i\circ \cal T^i|\bigg\|_{\infty}\le
\frac{1}{\Sigma_n}\big\|\max_{0\le i\le n-1 }|\bar v_i\circ \cal T^i|\big\|_{\infty}\le Cn^{-1/2},
\]
\[
\bigg\|\sup_{t\in[0,1]}|M_{n}(t)-\frac{1}{\sigma_n}\sum_{i=0}^{r_n(t)-1}\psi_i\circ \cal T^i|\bigg\|_{\infty}\le
\frac{1}{\sigma_n}\big\|\max_{0\le i\le n-1 }|\psi_i\circ \cal T^i|\big\|_{\infty}\le Cn^{-1/2}.
\]
It follows from the above estimates that $\big\|\sup_{t\in[0,1]}|W_{n}(t)-M_n(t)|\big\|_{p}\leq Cn^{-\frac{1}{2}}$ for all $n\gg1$.
\end{proof}

Define
\[
\xi_{n,j}:=\frac{1}{\sigma_n}\psi_{n-j}\circ \mathcal T^{n-j},\ \mathcal{G}_{n,j}:=\cal T^{-(n-j)}\mathcal{B}, \quad\hbox{for } 1\le j\le n.
\]
Then $\{\xi_{n,j}, \mathcal{G}_{n,j}; 1\le j\le n\}$ is a martingale difference array.

For $1\le l\le n$, define the conditional variance
\[
V_{n,l}:=\sum_{j=1}^{l}\E(\xi_{n,j}^2|\mathcal{G}_{n,j-1}).
\]
For the convenience, we set $V_{n,0}=0$.

Define the following stochastic processes $X_n$ with sample paths in $C[0,1]$ by
\begin{eqnarray}\label{xn}
X_{n}(t):=\sum_{j=1}^{k}\xi_{n,j}+\frac{tV_{n,n}-V_{n,k}}{V_{n,k+1}-V_{n,k}}\xi_{n,k+1}, \qquad \textrm{if}~~~ V_{n,k}\leq tV_{n,n}<V_{n,k+1}.
\end{eqnarray}

\medskip
{\bf Step 2. Estimation of the Wasserstein convergence rate between $X_n$ and $W$.}
\begin{prop}\label{vn1}
Let $p\ge 2$. Then there exists a constant $C>0$ such that for all $n\gg1$,
\[\big\|V_{n,n}-1\big\|_{p}\le Cn^{-\frac{1}{2}}.\]
\end{prop}
\begin{proof}
For $1\le j\le n$, we denote $\alpha_{j}^{2}=\sum_{i=1}^{j}\int\psi_{n-i}^2\circ \mathcal T^{n-i}\rmd m$. Then $\alpha_{n}^{2}=\sigma_{n}^{2}$ and
\begin{align*}
\big\|V_{n,n}-1\big\|_{p}=\big\|V_{n,n}-\frac{\alpha_{n}^{2}}{\sigma_{n}^{2}}\big\|_{p}.
\end{align*}

To deal with it, we denote $[\psi_{n-i}^2]=\psi_{n-i}^2-\E(\psi_{n-i}^2\circ \cal T^{n-i})$ for $1\le i\le n$. Then
\begin{align}\label{vnn1}
\big\|V_{n,n}-1\big\|_{p}&=\frac{1}{\sigma_n^2}\bigg\|\sum_{i=1}^{n}\E(\psi_{n-i}^2\circ \mathcal T^{n-i}|\mathcal{G}_{n,i-1})-\sum_{i=1}^{n}\E(\psi_{n-i}^2\circ \mathcal T^{n-i})\bigg\|_{p}\\\nonumber
&=\frac{1}{\sigma_n^2}\bigg\|\sum_{i=1}^{n}\E\big(\psi_{n-i}^2\circ \mathcal T^{n-i}-\E(\psi_{n-i}^2\circ \mathcal T^{n-i})\big|\mathcal{G}_{n,i-1}\big)\bigg\|_{p}\\\nonumber
&=\frac{1}{\sigma_n^2}\bigg\|\sum_{i=1}^{n}\E\big([\psi_{n-i}^2]\circ \mathcal T^{n-i}\big|\mathcal{G}_{n,i-1}\big)\bigg\|_{p}\\\nonumber
&=\frac{1}{\sigma_n^2}\bigg\|\sum_{i=1}^{n}\frac{P_{n-i+1}([\psi_{n-i}^2]\cdot\cal{P}^{n-i} 1)}{\cal{P}^{n-i+1} 1}\circ \cal T^{n-i+1}\bigg\|_{p},
\end{align}
where the last equation is due to \eqref{cexp}.

We claim that $\frac{P_{n-i+1}([\psi_{n-i}^2]\cdot\cal{P}^{n-i} 1)}{\cal{P}^{n-i+1} 1}\in \cal V$ and
$\E\big(\frac{P_{n-i+1}([\psi_{n-i}^2]\cdot\cal{P}^{n-i} 1)}{\cal{P}^{n-i+1} 1}\circ \cal T^{n-i+1}\big)=0$.
Then by Remark~\ref{mar}, for $n\gg1$,
\[
\big\|V_{n,n}-1\big\|_{p}\le C\sigma_n^{-2}n^{\frac{1}{2}}\le Cn^{-\frac{1}{2}}.
\]

Next, we verify the claim. It is obvious that
\begin{align*}
&\int\frac{P_{n-i+1}([\psi_{n-i}^2]\cdot\cal{P}^{n-i} 1)}{\cal{P}^{n-i+1} 1}\circ \cal T^{n-i+1}\rmd m =
\int P_{n-i+1}([\psi_{n-i}^2]\cdot\cal{P}^{n-i} 1)\rmd m\\
&=\int [\psi_{n-i}^2]\cdot\cal{P}^{n-i} 1\rmd m=\int [\psi_{n-i}^2]\circ \cal T^{n-i}\rmd m=0.
\end{align*}
Since $\inf_{x\in M}\cal P^n1(x)\ge \delta$ for all $n\ge 1$, we have
\[
\left\|\frac{P_{n-i+1}(\psi_{n-i}^2\cdot\cal{P}^{n-i} 1)}{\cal{P}^{n-i+1} 1}\right\|_\alpha
=\left\|\frac{P_{n-i+1}[(\bar v_{n-i}+h_{n-i}-h_{n-i+1}\circ T_{n-i+1})^2\cdot\cal{P}^{n-i} 1]}{\cal{P}^{n-i+1} 1}\right\|_\alpha<\infty.
\]
%It suffices to prove $[\psi_{n-i}^2]\cdot\cal{P}^{n-i} 1\in C^\alpha(M)$. Since $\cal{P}^{n-i} 1\in C^\alpha(M)$, we aim to prove
%$[\psi_{n-i}^2]\in C^\alpha(M)$. It is obvious that for any $x,y\in M$,
%\begin{align*}
%&|[\psi_{n-i}^2](x)-[\psi_{n-i}^2](y)|=|\psi_{n-i}^2(x)-\psi_{n-i}^2(y)|\\
%&\le 2\max_{1\le i\le n}\|\psi_i\|_\infty|\psi_{n-i}(x)-\psi_{n-i}(y)|\le C|x-y|^\alpha.
%\end{align*}
Hence
\[
\frac{P_{n-i+1}([\psi_{n-i}^2]\cdot\cal{P}^{n-i} 1)}{\cal{P}^{n-i+1} 1}\in \cal V.
\]
The claim holds.
\end{proof}

\begin{lem}\label{xnw}
Let $p\ge 2$. Then for any $\delta>0$ there exists a constant $C>0$ such that $\mathcal{W}_{p}(X_{n},W)\leq C n^{-(\frac{1}{4}-\delta)}$ for all $n\gg 1$.
\end{lem}

\begin{proof}
The proofs are based on the ideas employed in the stationary case in \cite[Lemma~4.4]{Liu23}. To obtain the convergence rate, we have to produce a bound of $\mathcal{W}_{p}(X_{n},W)$ for fixed $n\gg1$. It suffices to deal with a single row of the array $\{\xi_{n,j},\mathcal{G}_{n,j}, 1\le j\le n\}$.

By the Skorokhod embedding theorem (see Theorem \ref{ske}), there exists a probability space (depending on $n$) supporting a standard Brownian motion, still denoted by $W$ which should not cause confusion, and a sequence of nonnegative random variables $\tau_1,\ldots, \tau_n$ such that for $T_i=\sum_{j=1}^{i}\tau_j$ we have $\sum_{j=1}^{i}\xi_{n,j}=W(T_i)$ with $1\le i\le n$. In particular, we set $T_{0}=0$. Then on this probability space and for this Brownian motion, we aim to show that for any $\delta>0$ there exists a constant $C>0$ such that
\begin{align*}
\bigg\|\sup_{t\in[0,1]}|X_{n}(t)-W(t)|\bigg\|_{p}\leq Cn^{-(\frac{1}{4}-\delta)}.
\end{align*}
Thus the result follows from the definition of the Wasserstein distance.

For ease of exposition when there is no ambiguity, we will write $\xi_j$ and $V_k$ instead of $\xi_{n,j}$ and $V_{n,k}$ respectively. Then by the Skorokhod embedding theorem, we can write \eqref{xn} as
\begin{align}\label{set}
X_{n}(t)=W(T_{k})+\bigg(\frac{tV_{n}-V_{k}}{V_{k+1}-V_{k}}\bigg)\big(W(T_{k+1})-W(T_{k})\big),\quad \hbox{if}~V_{k}\leq tV_{n}<V_{k+1}.
\end{align}

1. We first estimate $|X_{n}-W|$ on the set $\{|T_{n}-1|\ge 1\}$.
 Note that Theorem \ref{ske}(3) implies
\[
T_k-V_k=\sum_{i=1}^{k}\big(\tau_{i}-\mathbb{E}(\tau_{i}|\mathcal{F}_{i-1})\big),\quad 1\le k\le n,
\]
where $\mathcal{F}_{i}$ is the $\sigma$-field generated by all events up to $T_i$ for $1\le i\le n$. Therefore $\{T_k-V_k, \mathcal{F}_{k}, 1\le k\le n\}$ is a martingale. By the Burkholder inequality and conditional Jensen inequality, for $p\ge 2$, we have
\begin{align*}\label{mar}
\left\|\max_{1\le k\le n}|T_k-V_{k}|\right\|_{p} &\le Cn^{\frac{1}{2}} \max_{1\le k\le n}\big\|\tau_{k}-\mathbb{E}(\tau_{k}|\mathcal{F}_{k-1})\big\|_{p}\\
&\le Cn^{\frac{1}{2}} \max_{1\le k\le n}\big\|\tau_{k}\big\|_{p}.
\end{align*}
It follows from Theorem \ref{ske}(4) that $\mathbb{E}(\tau_{k}^{p})\le 2\Gamma(p+1)\mathbb{E}(\xi_{k}^{2p})$ for each $k$. Since $\xi_k$ is uniformly bounded, we have
\begin{equation}\label{TkVk}
\bigg\|\max_{1\le k\le n}|T_k-V_{k}|\bigg\|_{p} \le Cn^{\frac{1}{2}} \max_{1\le k\le n}\big\|\xi_{k}\big\|_{2p}^2=
Cn^{\frac{1}{2}}\sigma_n^{-2}\max_{1\le k\le n}\big\|\psi_{n-k}\circ \mathcal T^{n-k}\big\|_{2p}^{2}\le Cn^{-\frac{1}{2}}.
\end{equation}
On the other hand, it follows from Proposition~\ref{vn1} that
\begin{align}\label{Vn1}
\|V_{n}-1\|_{p}\le  C n^{-\frac{1}{2}}.
\end{align}
Based on the above estimates, by Chebyshev's inequality we have
\begin{equation}\label{eqq}
\begin{split}
&m(|T_{n}-1|>1)\le \mathbb{E}|T_{n}-1|^{p} \\
&\le 2^{p-1}\left\{\mathbb{E}|T_{n}-V_{n}|^{p}+\mathbb{E}|V_{n}-1|^{p}\right\}\le Cn^{-\frac{p}{2}}.
\end{split}
\end{equation}
Hence, by the H\"{o}lder inequality, \eqref{eqq} and Remark~\ref{mar}, we deduce that
\begin{align*}
I:=&\bigg\| 1_{\{|T_{n}-1|>1\}}\sup_{t\in[0,1]}|X_{n}(t)-W(t)|\bigg\|_{p}\\
\le &\big(m(|T_{n}-1|>1)\big)^{1/2p}\bigg\| \sup_{t\in[0,1]}|X_{n}(t)-W(t)|\bigg\|_{2p}\\
\le &\big(m(|T_{n}-1|>1)\big)^{1/2p}\bigg(\bigg\|\sup_{t\in[0,1]}|X_{n}(t)|\bigg\|_{2p}+\bigg\|\sup_{t\in[0,1]}|W(t)|\bigg\|_{2p}\bigg)\\
\le &Cn^{-\frac{1}{4}}.
\end{align*}

2. We now estimate $|X_{n}-W|$ on the set $\{|T_{n}-1|\le 1\}$:
\begin{align*}
&\bigg\| 1_{\{|T_{n}-1|\le 1\}}\sup_{t\in[0,1]}|X_{n}(t)-W(t)|\bigg\|_{p}\\
\le &\bigg\| 1_{\{|T_{n}-1|\le 1\}}\sup_{t\in[0,1]}|X_{n}(t)-W(T_{k})|\bigg\|_{p}+\bigg\|1_{\{|T_{n}-1|\le 1\}}\sup_{t\in[0,1]}|W(T_{k})-W(t)|\bigg\|_{p}\\
 =: & I_1 + I_2.
\end{align*}
For $I_1$, it follows from \eqref{set} that
\[
\sup_{t\in[0,1]}|X_{n}(t)-W(T_{k})|\le \max_{0\le k\le n-1}|W(T_{k+1})-W(T_{k})|=\max_{0\le k\le n-1}|\xi_{k+1}|.
\]
Since $\psi_n$ is uniformly bounded, we have
\begin{align*}\label{zeta}
I_1=&\bigg\| 1_{\{|T_{n}-1|\le 1\}}\sup_{t\in[0,1]}|X_{n}(t)-W(T_{k})|\bigg\|_{p}\\
\le &\bigg\| 1_{\{|T_{n}-1|\le 1\}}\max_{0\le k\le n-1}|\xi_{k+1}|\bigg\|_{p}\\
\le &\bigg\| \max_{0\le k\le n-1}|\xi_{k+1}|\bigg\|_{p}\\
\le &\frac{1}{\sigma_n}\max_{0\le k\le n-1}\big\|\psi_{k}\circ\cal T^{k}\big\|_{\infty}\le Cn^{-\frac{1}{2}}.
\end{align*}

3. Finally, We consider $I_{2}$ on the set $\{|T_{n}-1|\le 1\}$. Take $p_1>p$, then it is well known that
\begin{equation}\label{bts}
\mathbb{E}|W(t)-W(s)|^{2p_1}\le c|t-s|^{p_1}, \quad \text{for~ all~} s,t\in [0,2].
\end{equation}
So it follows from Kolmogorov's continuity theorem that for each $0<\gamma<\frac{1}{2}-\frac{1}{2p_1}$, the process $W(\cdot)$ admits a version, still denoted by $W$, such that for almost all $\omega$ the sample path $t\mapsto W(t,\omega)$ is H\"{o}lder continuous with exponent $\gamma$ and
\begin{equation*}
\bigg\|\sup_{s,t\in[0,2]\atop s\neq t}\frac{|W(s)-W(t)|}{|s-t|^{\gamma}}\bigg\|_{2p_1}< \infty.
\end{equation*}
In particular,
\begin{equation}\label{holder}
\bigg\|\sup_{s,t\in[0,2]\atop s\neq t}\frac{|W(s)-W(t)|}{|s-t|^{\gamma}}\bigg\|_{2p}< \infty.
\end{equation}

As for $|T_{k}-t|$, by some calculations (see \cite[Lemma~4.4]{Liu23} for details), we have
\begin{align*}
&\sup_{t\in[0,1]}|T_{k}-t|\le \max_{0\le k\le n-1}\sup_{t\in[\frac{V_{k}}{V_{n}},\frac{V_{k+1}}{V_{n}})}|T_{k}-t|\\
%\le &\max_{0\le k\le n-1}\bigg|T_{k}-\frac{V_{k}}{V_{n}}\bigg|+\max_{0\le k\le n-1}\sup_{t\in[\frac{V_{k}}{V_{n}},\frac{V_{k+1}}{V_{n}})}\bigg|\frac{V_{k}}{V_{n}}-t\bigg|\\
%\le &\max_{0\le k\le n}\bigg|T_{k}-\frac{V_{k}}{V_{n}}\bigg|+\max_{0\le k\le n-1}\left|\frac{V_{k+1}}{V_{n}}-\frac{V_{k}}{V_{n}}\right|\\
%\le &\max_{0\le k\le n}\big|T_{k}-V_{k}\big|+\max_{0\le k\le n}\bigg|V_{k}-\frac{V_{k}}{V_{n}}\bigg|+\max_{0\le k\le n-1}\left|\frac{V_{k+1}}{V_{n}}-V_{k+1}\right|\\
%&\quad +\max_{0\le k\le n-1}\left|V_{k+1}-V_{k}\right|+\max_{0\le k\le n-1}\left|V_{k}-\frac{V_{k}}{V_{n}}\right|\\
&\le \max_{0\le k\le n}\big|T_{k}-V_{k}\big| + 3\max_{0\le k\le n}\left|V_{k}-\frac{V_{k}}{V_{n}}\right| +\max_{0\le k\le n-1}\big|V_{k+1}-V_{k}\big|.
\end{align*}
Note that $T_{0}=V_{0}=0$ and $\gamma\le1$, so
\[
\sup_{t\in[0,1]}|T_{k}-t|^{\gamma}\le \max_{1\le k\le n}\left|T_{k}-V_{k}\right|^{\gamma} + 3^\gamma\max_{1\le k\le n}\left|V_{k}-\frac{V_{k}}{V_{n}}\right|^{\gamma} +\max_{0\le k\le n-1}\left|V_{k+1}-V_{k}\right|^{\gamma}.
\]
Hence we have
\begin{align}
&\bigg\|\sup_{t\in[0,1]}|T_{k}-t|^{\gamma}\bigg\|_{2p}\nonumber\\
\le & \bigg\|\max_{1\le k\le n}\big|T_{k}-V_{k}\big|\bigg\|_{2\gamma p}^{\gamma} +3^\gamma \bigg\|\max_{1\le k\le n}\big|V_{k}-\frac{V_{k}}{V_{n}}\big|\bigg\|_ {2\gamma p}^{\gamma}
 +\bigg\|\max_{0\le k\le n-1}\big|V_{k+1}-V_{k}\big|\bigg\|_ {2\gamma p}^{\gamma}.
\end{align}
For the first term, since $\gamma< \frac{1}{2}$, it follows from \eqref{TkVk} that
\begin{equation}\label{Tgam}
\bigg\|\max_{1\le k\le n}|T_{k}-V_{k}|\bigg\|_{2\gamma p}^{\gamma}\le Cn^{-\frac{\gamma}{2}}.
\end{equation}
For the second term, since $|V_{k}-\frac{V_{k}}{V_{n}}|=V_k|1-\frac1{V_n}|$, we have
\[\max_{1\le k\le n}\bigg|V_{k}-\frac{V_{k}}{V_{n}}\bigg|=V_n \bigg|1-\frac1{V_n}\bigg|=|V_{n}-1|.\]
Hence by \eqref{Vn1},
\begin{align}\label{Vgam}
\bigg\|\max_{1\le k\le n}\big|V_{k}-\frac{V_{k}}{V_{n}}\big|\bigg\|_ {2\gamma p}^{\gamma}
=\big\|V_{n}-1\big\|_{2\gamma p}^{\gamma}\le Cn^{-\frac{\gamma}{2}}.
\end{align}
As for the last term, note that for all $1\le k\le n$,
\begin{align*}
|V_{k}-V_{k-1}|&=\mathbb{E}(\xi_{k}^2|\mathcal{F}_{k-1})
=\frac{1}{\sigma_n^2}\mathbb{E}\big(\psi_{n-k}^2\circ \mathcal T^{n-k}|\mathcal{G}_{k-1}\big)\\
&=\frac{1}{\sigma_n^2}\cdot\frac{P_{n-k+1}(\psi_{n-k}^2\cdot\cal{P}^{n-k} 1)}{\cal{P}^{n-k+1} 1}\circ \cal T^{n-k+1}.
\end{align*}
Since $\frac{P_{n-k+1}(\psi_{n-k}^2\cdot\cal{P}^{n-k} 1)}{\cal{P}^{n-k+1} 1}\in L^\infty$, we have
\begin{align}\label{mgam}
&\bigg\|\max_{0\le k\le n-1}\big|V_{k+1}-V_{k}\big|\bigg\|_ {2\gamma p}^{\gamma}=\frac{1}{\sigma_n^{2\gamma}}\bigg\|\max_{1\le k\le n}\big|\frac{P_{n-k+1}(\psi_{n-k}^2\cdot\cal{P}^{n-k} 1)}{\cal{P}^{n-k+1} 1}\circ \cal T^{n-k+1}\big|\bigg\|_{2\gamma p}^{\gamma}\\\nonumber
&\le \sigma_n^{-2\gamma}n^{\frac{\gamma}{p}}\max_{1\le k\le n}\bigg\|\frac{P_{n-k+1}(\psi_{n-k}^2\cdot\cal{P}^{n-k} 1)}{\cal{P}^{n-k+1} 1}\circ \cal T^{n-k+1}\bigg\|_{p}^{\gamma}\le Cn^{-\left(\gamma-\frac{\gamma}{p}\right)},
\end{align}
where the inequality follows from Proposition~\ref{mpe}.

Based on the above estimates \eqref{Tgam}--\eqref{mgam}, we have
\begin{align}\label{te}
\bigg\|\sup_{t\in[0,1]}|T_{k}-t|^{\gamma}\bigg\|_{2p}
\le C\bigg(n^{-\frac{\gamma}{2}}+n^{-\left(\gamma-\frac{\gamma}{p}\right)}\bigg)
\le C n^{-\frac{\gamma}{2}},
\end{align}
where the last inequality holds since
$\gamma< \frac{1}{2}$, $1-\frac{1}{p}\ge\frac{1}{2}$.

On the set $\{|T_{n}-1|\le 1\}$, note that
\[
\sup_{t\in[0,1]}|W(T_{k})-W(t)|\le \bigg[\sup_{s,t\in[0,2]\atop s\neq t}\frac{|W(s)-W(t)|}{|s-t|^{\gamma}}\bigg]\bigg[\sup_{t\in[0,1]}|T_{k}-t|^{\gamma}\bigg].
\]
Since $0<\gamma<\frac{1}{2}-\frac{1}{2p_1}$, by the H\"{o}lder inequality and \eqref{holder}, \eqref{te}, we have
\begin{align*}
I_2=&\bigg\|1_{\{|T_{n}-1|\le 1\}}\sup_{t\in[0,1]}|W(T_{k})-W(t)|\bigg\|_{p}\\
\le &\bigg\|\bigg[\sup_{s,t\in[0,2]\atop s\neq t}\frac{|W(s)-W(t)|}{|s-t|^{\gamma}}\bigg]\bigg[\sup_{t\in[0,1]}|T_{k}-t|^{\gamma}\bigg]\bigg\|_{p}\\
\le &\bigg\|\sup_{s,t\in[0,2]\atop s\neq t}\frac{|W(s)-W(t)|}{|s-t|^{\gamma}}\bigg\|_{2p}\bigg\|\sup_{t\in[0,1]}|T_{k}-t|^{\gamma}\bigg\|_{2p}\\
\le & C n^{-\frac{\gamma}{2}}.
\end{align*}
Note that $p_1$ can be taken arbitrarily large in \eqref{bts}, which implies that $\gamma$ can be chosen sufficiently close to $\frac{1}{2}$. So for any $\delta>0$, we can choose $p_1$ large enough such that $I_2\le Cn^{-\frac{1}{4}+\delta}$. The result now follows from the above estimates for $I,I_1$ and $I_2$.
\end{proof}


{\bf Step 3. Estimation of the Wasserstein convergence rate between $M_n$ and $X_n$.}

\vskip3mm

Define a continuous transformation $g:C[0,1]\rightarrow C[0,1]$ by $g(u)(t):=u(1)-u(1-t)$.

\begin{lem}\label{wnxn}
Let $p\ge2$. Then there exists a constant $C>0$ such that $\mathcal{W}_{p}(g\circ M_{n}, X_{n})\leq Cn^{-\frac{1}{4}}$ for all $n\gg 1$.
\end{lem}

\begin{proof}
For $1\le j\le n$, we recall that $\alpha_{j}^{2}=\sum_{i=1}^{j}\int\psi_{n-i}^2\circ \mathcal T^{n-i}\rmd m$. Then $\alpha_{n}^{2}=\sigma_n^2$. We define
\[
\widetilde M_n(t):=\frac{1}{\sigma_n}\bigg[\sum_{i=1}^{l_n(t)}\psi_{n-i}\circ\cal T^{n-i}+\frac{t\alpha_{n}^{2}-\alpha_{l_n(t)}^{2}}{\alpha_{l_n(t)+1}^{2}-\alpha_{l_n(t)}^{2}}\psi_{n-l_n(t)-1}\circ\cal T^{n-l_n(t)-1}\bigg], \quad \textrm{if}~~~~ \alpha_{l_n(t)}^{2}\leq t\alpha_{n}^{2}<\alpha_{l_n(t)+1}^{2}.
\]
For $\eps>0$ sufficiently small, we have
\begin{align*}
&\bigg\|\sup_{t\in[0,1]}|\widetilde M_n(t)-X_n(t)|\bigg\|_{p}\\
\le& \bigg\|1_{\big\{\max_{1\le j\le n}\big|\frac{\alpha_j^2}{\alpha_n^2}-\frac{V_{n,j}}{V_{n,n}}\big|>\eps\big\}}\sup_{t\in[0,1]}|\widetilde M_n(t)-X_n(t)|\bigg\|_{p}\\
&\quad\quad\quad+\bigg\|1_{\big\{\max_{1\le j\le n}\big|\frac{\alpha_j^2}{\alpha_n^2}-\frac{V_{n,j}}{V_{n,n}}\big|\le\eps\big\}}\sup_{t\in[0,1]}|\widetilde M_n(t)-X_n(t)|\bigg\|_{p}\\
=:&I_1+I_2.
\end{align*}

To deal with $I_1$, by Chebyshev's inequality, we have
\begin{align*}
&m(\max_{1\le j\le n}\big|\frac{\alpha_j^2}{\alpha_n^2}-\frac{V_{n,j}}{V_{n,n}}\big|>\eps)\le \frac{\E\max_{1\le j\le n}\big|\frac{\alpha_j^2}{\alpha_n^2}-\frac{V_{n,j}}{V_{n,n}}\big|^p}{\eps^p}\\
\le&\frac{2^{p-1}(\E\max_{1\le j\le n}\big|\frac{\alpha_j^2}{\alpha_n^2}-V_{n,j}\big|^p+\E\max_{1\le j\le n}\big|V_{n,j}-\frac{V_{n,j}}{V_{n,n}}\big|^p)}{\eps^p}\\
\le &\frac{2^{p}\frac{1}{\alpha_n^{2p}}\E\max_{1\le j\le n}\big|\alpha_j^2-V_{n,j}\cdot \alpha_n^2\big|^p}{\eps^p},
\end{align*}
where the last inequality is due to
\[\max_{1\le j\le n}\bigg|V_{n,j}-\frac{V_{n,j}}{V_{n,n}}\bigg|=V_{n,n}\bigg|1-\frac1{V_{n,n}}\bigg|=|V_{n,n}-1|\le \max_{1\le j\le n}\big|V_{n,j}-\frac{\alpha_j^2}{\alpha_n^2}\big|.\]
Then it follows from the proof of Proposition~\ref{vn1} and Remark~\ref{mar} that
\begin{align*}
&\E\max_{1\le j\le n}\big|\alpha_j^2-V_{n,j}\cdot \alpha_n^2\big|^p\\
=&\E\max_{1\le j\le n}\big|\sum_{i=1}^{j}\E(\psi_{n-i}^2\circ \mathcal T^{n-i}|\mathcal{G}_{n,i-1})-\sum_{i=1}^{j}\E(\psi_{n-i}^2\circ \mathcal T^{n-i})\big|^p\\
%=&\E\max_{1\le j\le n}\big|\sum_{i=1}^{j}\E\big(\psi_{n-i}^2\circ \mathcal T^{n-i}-\E(\psi_{n-i}^2\circ \mathcal T^{n-i})\big|\mathcal{G}_{n,i-1}\big)\big|^p\\
=&\E\max_{1\le j\le n}\big|\sum_{i=1}^{j}\E\big([\psi_{n-i}^2]\circ \mathcal T^{n-i}\big|\mathcal{G}_{n,i-1}\big)\big|^{p}\\\nonumber
=&\E\max_{1\le j\le n}\big|\sum_{i=1}^{j}\frac{P_{n-i+1}([\psi_{n-i}^2]\cdot\cal{P}^{n-i} 1)}{\cal{P}^{n-i+1} 1}\circ \cal T^{n-i+1}\big|^{p}\\
\le &Cn^{\frac{p}{2}}.
\end{align*}
So for $n\gg1$,
\[
m(\max_{1\le j\le n}\big|\frac{\alpha_j^2}{\alpha_n^2}-\frac{V_{n,j}}{V_{n,n}}\big|>\eps)\le C \frac{1}{\alpha_n^{2p}}n^{\frac{p}{2}}\le Cn^{-\frac{p}{2}}.
\]
Then by the H\"older inequality and Remark~\ref{mar}, we have
\begin{align*}
I_1&\le \big(m(\max_{1\le j\le n}\big|\frac{\alpha_j^2}{\alpha_n^2}-\frac{V_{n,j}}{V_{n,n}}\big|>\eps)\big)^{\frac{1}{2p}}\bigg(\bigg\|\sup_{t\in[0,1]}|\widetilde M_{n}(t)|\bigg\|_{2p}
+\bigg\|\sup_{t\in[0,1]}|X_n(t)|\bigg\|_{2p}\bigg)\\
&\le Cn^{-\frac{1}{4}}.
\end{align*}

As for the term $I_2$, we have
\begin{align*}
I_2&=\bigg\|1_{\big\{\max_{1\le j\le n}\big|\frac{\alpha_j^2}{\alpha_n^2}-\frac{V_{n,j}}{V_{n,n}}\big|\le\eps\big\}}\sup_{t\in[0,1]}|\widetilde M_n(t)-X_n(t)|\bigg\|_{p}\\
&\le \bigg\|\sup_{|s-t|<2\eps}|\widetilde M_n(t)-\widetilde M_n(s)|\bigg\|_{p}\\
&\le \frac{2}{\sigma_n}\big\|\max_{1\le i\le n}|\psi_{n-i}\circ\cal T^{n-i}|\big\|_p\le C\sigma_n^{-1}\le Cn^{-\frac{1}{2}}.
\end{align*}
Hence
\[
\bigg\|\sup_{t\in[0,1]}|\widetilde M_n(t)-X_n(t)|\bigg\|_{p}\le Cn^{-\frac{1}{4}}.
\]

On the other hand, we note that
\begin{align*}
&g\circ M_{n}(t)=M_n(1)-M_n(1-t)\\
&=\frac{1}{\sigma_n}\sum_{i=0}^{n-1}\psi_{i}\circ\cal T^{i}-\frac{1}{\sigma_n}\sum_{i=0}^{r_n(1-t)-1}\psi_{i}\circ\cal T^{i}+F_n(t)\\
&=\frac{1}{\sigma_n}\sum_{i=r_n(1-t)}^{n-1}\psi_{i}\circ\cal T^{i}+F_n(t)\\
&=\frac{1}{\sigma_n}\sum_{i=1}^{n-r_n(1-t)}\psi_{n-i}\circ\cal T^{n-i}+F_n(t),
\end{align*}
where $\|F_n(t)\|_{\infty}\le \sigma_n^{-1}\max_{0\le i\le n-1}\|\psi_i\|_{\infty}\le Cn^{-\frac{1}{2}}$.

To compare $n-r_n(1-t)$ with $l_n(t)$, we can find that
\[\sigma_{r_n(1-t)-1}^2<(1-t)\sigma_n^2\le\sigma_{r_n(1-t)}^2.\]
Since $\sigma_n^2=\alpha_n^2$, we have
\[\alpha_n^2-\alpha_{n-r_n(1-t)+1}^2<(1-t)\alpha_n^2\le \alpha_n^2-\alpha_{n-r_n(1-t)}^2,\]
i.e.
\[\alpha_{n-r_n(1-t)}^2\le t\alpha_n^2<\alpha_{n-r_n(1-t)+1}^2.\]
By the definition of $l_n(t)$, we also have
$\alpha_{l_n(t)}^2\le t\alpha_n^2<\alpha_{l_n(t)+1}^2$. So $l_n(t)=n-r_n(1-t)$.
Hence
\[
\big\|\sup_{t\in[0,1]}|g\circ M_{n}(t)-\widetilde M_n(t)|\big\|_{\infty}\le C\frac{1}{\sigma_n}\big\|\max_{0\le i\le n-1 }|\psi_i\circ \cal T^i|\big\|_{\infty}\le Cn^{-1/2}.
\]

Combining the above estimates, by the definition of Wasserstein distance, we obtain that for all $n\gg1$,
\begin{align*}
\mathcal{W}_{p}(g\circ M_{n}, X_{n})&\le \mathcal{W}_{p}(g\circ M_{n}, \widetilde M_n)+\mathcal{W}_{p}(\widetilde M_n, X_{n})\\
&\le Cn^{-1/2}+Cn^{-\frac{1}{4}}\le Cn^{-\frac{1}{4}}.
\end{align*}
\end{proof}


\begin{proof}[Proof of Theorem \ref{thnon}]
Recall that $g:C[0,1]\rightarrow C[0,1]$ is a continuous transformation defined by $g(u)(t)=u(1)-u(1-t)$. we note that $g\circ g= Id$ and $g$ is Lipschitz with Lip$g$ $\le 2$. It follows from Lipschitz mapping theorem that
\[
\mathcal{W}_{p}(M_{n},W)= \mathcal{W}_{p}(g(g\circ M_{n}),g(g\circ W))\le 2\mathcal{W}_{p}(g\circ M_{n},g\circ W).
\]
Since $g(W)=_d W$, by Lemmas \ref{xnw} and \ref{wnxn}, for $p\ge 2$ we have
\begin{align*}
&\mathcal{W}_{p}(g\circ M_{n},g\circ W)\le\mathcal{W}_{p}(g\circ M_{n}, X_n)+\mathcal{W}_{p}(X_n, W)\\
&\le Cn^{-\frac{1}{4}}+Cn^{-\frac{1}{4}+\delta}\le Cn^{-\frac{1}{4}+\delta}
\end{align*}
with $\delta$ sufficiently small and $n\gg1$.

Finally, by Lemma~\ref{wmn}, we conclude that
\begin{align*}
\mathcal{W}_{p}(W_{n},W)&\le \mathcal{W}_{p}(W_{n},M_n)+\mathcal{W}_{p}(M_{n},W)\\
&\le Cn^{-\frac{1}{2}}+Cn^{-\frac{1}{4}+\delta}\le Cn^{-\frac{1}{4}+\delta}
\end{align*}
with $\delta$ sufficiently small and $n\gg1$.
\end{proof}

\section{Applications of Theorem~\ref{thnon}}
In this section, we introduce a large class of systems investigated in~\cite{Haydn17} as concrete examples to which the Wasserstein convergence
rate in the invariance principle (Theorem~\ref{thnon}) applies. In order to guarantee the conditions in Theorem~\ref{thnon}, a few assumptions are needed. For the convenience, we recall the assumptions first and then provide a list of examples. We refer to~\cite{Conze07} and \cite[Section~7]{Haydn17} for more details.

We say that a transfer operator $P$ is exact if $\lim_{n\to \infty}\|P^nv\|_1=0$, $\forall v\in \cal V$ with mean zero (w.r.t. Legesgue measure). We define a distance
between two transfer operators $P$ and $Q$ by taking
\[
d(P,Q)=\sup_{v\in \cal V, \|v\|_{\alpha}\le 1}\|Pv-Qv\|_1.
\]
In the following the maps we consider will be close to a given map $T_0$. Roughly speaking, the word ``close" means that $d(P_n, P_0)\to 0$, as $n\to \infty$. We will give a detailed description below.

One of the basic assumptions is a ``quasi-compactness" condition:\\
{\bf Uniform Doeblin-Fortet-Lasota-Yorke inequality (DFLY).} There exist constants $A,B<\infty$, $\gamma\in(0,1)$ such that for any $n\in \N$, any sequence $P_1, P_2,\ldots, P_n$ and
any $v\in \cal V$, we have
\begin{align}\label{dfly}
\|P_n\circ P_{n-1}\circ\cdots\circ P_1v\|_\alpha\le A\gamma^n\|v\|_\alpha+B\|v\|_1.
\end{align}
In particular, the bound~\eqref{dfly} is valid when it is applied to $P_0^n$. Namely, we require:\\
{\bf Exactness property (Exa).} The operator $P_0$ has a spectral gap, which implies that there exist constants $C<\infty$, $\gamma_0\in(0,1)$ such that for any $n\ge 1$ and $v\in \cal V$,
\[
\|P_0^nv\|_\alpha\le C\gamma_0^n\|v\|_\alpha.
\]

By the definition of $\|\cdot\|_\alpha$, we know immediately that the transfer operator $P_0$ is exact. To verify the (DEC) condition, a useful criterion was given in \cite[Proposition~2.10]{Conze07}. It says that if $P_0$ is exact, then there exists
$\delta_0>0$ such that the set $\{P: d(P,P_0)<\delta_0\}$ satisfies the (DEC) condition.

Next, to guarantee the linear growth of the variance $\Sigma_n^2$, we need some quantitative assumptions.\\
{\bf Lipschitz continuity property (Lip).} Assume that the maps (and their transfer operators) are parametrised by a sequence of numbers $\eps_k$, $k\in\N$, such that $\lim_{k\to\infty}\eps_k=\eps_0$ $(P_{\eps_0}=P_0)$. We assume that there exists a constant $C_1<\infty$ such that
\[
d(P_{\eps_k}, P_{\eps_j})\le C_1|\eps_k-\eps_j|, \quad \hbox{for~ all~} k,j\ge 0.
\]
In the following, the maps we consider are restricted to a  subclass of maps; that is $\{T_{\eps_k}: |\eps_k-\eps_0|< C_1^{-1}\delta_0\}$. Then the maps in this subclass satisfy the (DEC) condition. Besides, we also need a stronger assumption:\\
{\bf Convergence property (Conv).}  There exist constants $C_2<\infty$, $\kappa>0$ such that
\[
|\eps_n-\eps_0|\le C_2\frac{1}{n^\kappa} \quad\forall n\ge 1.
\]

Finally, we also require:\\
{\bf Positivity property (Pos).} The density $h$ for the limiting map $T_0$ is strictly positive. Namely,
\[
\inf_x h(x)>0.
\]

The above properties can be summarized to obtain the linear growth of the variance $\Sigma_n^2$ in the following result.
\begin{lem}\cite[Lemma~7.1]{Haydn17}
Assume the assumptions (Exa), (Lip), (Conv) and (Pos) are satisfied. If $v$ is not a coboundary for $T_0$, then $\Sigma_n^2/n$ converges as
$n\to\infty$ to $\Sigma^2$ which is given by
\[
\Sigma^2=\int \hat P[Gv-\hat PGv]^2(x)h(x)\rmd x,
\]
where $\hat Pv=\frac{P_0(hv)}{h}$ is the normalized transfer operator of $T_0$ and $Gv=\sum_{k\ge 0}\frac{P_0^k(hv)}{h}$.
\end{lem}
%\begin{proof}
%The proof is based on the Section~5 in~\cite{Conze07}, in particular, based on Lemmas~5.6 and~5.7.
%\end{proof}
\subsection{$\beta$-transformations}
Let $\beta>1$ and denote by $T_\beta(x)=\beta x\mod 1$ the $\beta$-transformation on the unit circle. Let $c>0$ and $\beta_k$ be real number such
that $\beta_k\ge 1+c$, $k\ge 1$. Then $\{T_{\beta_k}: k\ge1\}$ is the family of functions we want to consider here. We take the functional space $\cal V$ to be the Banach space of bounded variation functions with norm $\|\cdot\|_{BV}$. The property (DEC) and (MIN) were proved in \cite[Theorem~3.4(c)]{Conze07} and  \cite[Proposition~4.3]{Conze07}, respectively.
The invariant density of $T_\beta$ is bounded below, and the continuity (Lip) was introduced in \cite[Lemma~3.9]{Conze07}. It follows
from \cite[Corollary~5.4]{Conze07} and \cite[Theorem~7.2]{Haydn17} that the variance $\Sigma_n^2$ grows linearly and the standard ASIP holds with variance $\Sigma^2$. Then by Theorem~\ref{thnon}, we obtain
\begin{thm}
Assume that $|\beta_n-\beta|\le n^{-\theta}$, $\theta>1/2$. Let $v\in BV$ be such that $m(hv)=0$, where $m$ is the Lebesgue measure and $v$ is not a coboundary for $T_\beta$. Let $W_n$ be defined by \eqref{wnt} and $W$ a standard Brownian motion. Then for any $\delta>0$, there exists a constant $C>0$ such that
$\mathcal{W}_{p}(W_{n},W)\leq C n^{-(\frac{1}{4}-\delta)}$ for all $n\gg 1$ and $p\ge 2$.
\end{thm}
\subsection{Random additive noise}
Let $T$ be a piecewise uniformly expanding map on the unit interval $M=[0,1]$. We assume that $T$ is locally injective on the open intervals $A_k$, $k=1,\ldots,m$, that give a partition $\cal A=\{A_k: k\}$ of the unit interval (up to zero measure sets). The map $T$ is $C^2$ on each $A_k$ and has a $C^2$ extension to the boundaries. Moreover, there exist $\Lambda>1$, $C<\infty$ such that $\inf_{x\in M}|DT(x)|\ge \Lambda$ and $\sup_{x\in M} \left|\frac{D^2T(x)}{DT(x)}\right|\le C$.

The family of maps we consider here are constructed with local additive noise starting from $T$. On each interval $A_k$, we define $T_\eps=T(x)+\eps$, where $|\eps|<1$ and we restrict the values of $\eps$ such that the images $T_\eps A_k$, $k=1,\ldots,m$ are strictly included in [0,1]. We also suppose that there exists an element $A_\omega\in \cal A$ such that\\
(i) $A_\omega\subset T_\eps A_k$ for all $T_\eps$ and $k=1,\ldots,m$; \\
(ii) The map $T$ sends $A_\omega$ to the whole unit interval. In particular, there exists $1>L'>0$ such that for all $T_\eps$ and $k=1,\ldots,m$, $|T_\eps(A_\omega)\cap A_k|>L'$.

We take the functional space $\cal V$ to be the Banach space of bounded variation functions with norm $\|\cdot\|_{BV}$. It follows from \cite[Lemma~7.5]{Haydn17} that the maps $T_\eps$ satisfy the conditions (DFLY), (MIN), (Pos) and (Lip). Hence the variance $\Sigma_n^2$ grows linearly and the standard ASIP holds with variance $\Sigma^2$ by \cite[Theorem~7.6]{Haydn17}. Further, by Theorem~\ref{thnon}, we obtain
\begin{thm}\label{c2}
Let $T$ be a map of the unit interval defined above and such that it has only one absolutely continuous invariant measure, which is also mixing. Assume that $\{T_{\eps_k}\}$ is the sequence of maps, where the sequence $\{\eps_k\}_{k\ge 1}$ satisfies $|\eps_k|\le k^{-\theta}$, $\theta>1/2$. If $v\in BV$ is not a coboundary for $T$, then for any $\delta>0$, there exists a constant $C>0$ such that
$\mathcal{W}_{p}(W_{n},W)\leq C n^{-(\frac{1}{4}-\delta)}$ for all $n\gg 1$ and $p\ge 2$.
\end{thm}

\begin{rem}
We can also consider $C^2$ uniformly expanding maps on the circle $\mathbb{T}$ satisfying the above assumptions. Then Theorem~\ref{c2} holds without any restriction on the values of $\eps_k$. We refer to Section~7.2 in \cite{Haydn17} for more details.
\end{rem}

\subsection{Multidimensional maps}

We give here multidimensional piecewise expanding maps, which were extensively investigated in \cite{AFLV11,AFV15,HNVZ13,S00}. Let $M$ be a compact subset of $\R^N$ with $\overline{\text{int} M}=M$. We take a map $T:M\to M$ and Let $\cal A=\{A_i\}_{i=1}^{m}$ be a finite family of disjoint open sets such that
the Lebesgue measure of $M\setminus\bigcup_iA_i$ is zero, and there exist open sets $\tilde A_i\supset\bar A_i$ and $C^{1+\alpha}$ maps
$T_i:\tilde A_i\to \R^N$, for some real number $0<\alpha\le 1$ and some sufficiently small real number $\eps_1>0$, such that\\
(1) $T_i(\tilde A_i)\supset B_{\eps_1}(T(A_i))$ for each $i$, where $B_\eps(V)$ denotes a neighbourhood of size $\eps$ of the set $V$, and the maps $T_i$ are the local extensions of $T$ to the $\tilde A_i$.\\
(2) There exists a constant $C_1$ such that for each $i$ and $x,y\in T(A_i)$ with dist$(x,y)\le \eps_1$, we have
\[
|\det DT_i^{-1}(x)-\det DT_i^{-1}(y)|\le C_1|\det DT_i^{-1}(x)|\text{dist}(x,y)^\alpha.
\]
(3) There exists $s=s(T)<1$ such that $\forall x,y\in T(\tilde A_i)$ with dist$(x,y)\le \eps_1$, we have
\[
\text{dist}(T_i^{-1}x,T_i^{-1}y)\le s\;\text{dist}(x,y).\]
(4) Each $\partial A_i$ is a codimension-one embedded compact piecewise $C^1$ submanifold and
\[
s^\alpha+\frac{4s}{1-s}Z(T)\frac{\gamma_{N-1}}{\gamma_{N}}<1,
\]
where $Z(T)=\sup_x\sum_{i}\sharp\{$smooth pieces intersecting $\partial A_i$ containing $x\}$ and $\gamma_{N}$ is the volume of the unit ball in $\R^N$.

Given such a map $T$, we define locally on each $A_i$ the map $T_\eps$ by $T_\eps(x)=T(x)+\eps$, where $\eps$ is an $N$-dimensional vector with all the components of absolute value less than one. As in the previous example we choose a suitable $\eps$ such that the image $T_\eps A_i$ remains in $M$. As in the one dimensional case, we shall make the following assumption. We suppose that there exists $A_\omega\in \cal A$ satisfying:\\
(i) $A_\omega\subset T_\eps A_k$ for all $T_\eps$ and $k=1,\ldots,m$. \\
(ii) $TA_\omega$ is the whole $M$, which in turn implies that there exists $1>L'>0$ such that for all $T_\eps$ and $k=1,\ldots,m$, diameter$(T_\eps(A_\omega)\cap A_k)>L'$.

In this case, we take the functional space $\cal V$ to be the space of quasi-H\"older functions, for which we refer to \cite{AFV15,HNVZ13,S00} for details. It follows from the proof of \cite[Theorem~7.7]{Haydn17} that the maps $T_\eps$ satisfy the conditions (DFLY), (MIN), (Pos) and (Lip). So the variance $\Sigma_n^2$ grows linearly and the standard ASIP holds with variance $\Sigma^2$. Further, by Theorem~\ref{thnon}, we obtain
\begin{thm}
 Let $T:M\to M$ be a map defined above and such that it has only one absolutely continuous invariant measure, which is also mixing. Assume that $\{T_{\eps_k}\}$ is the sequence of maps satisfying the conditions (i) and (ii), and the sequence $\{\eps_k\}_{k\ge 1}$ satisfies $|\eps_k|\le k^{-\theta}$, $\theta>1/2$. If $v$ is not a coboundary for $T$, then for any $\delta>0$, there exists a constant $C>0$ such that
$\mathcal{W}_{p}(W_{n},W)\leq C n^{-(\frac{1}{4}-\delta)}$ for all $n\gg 1$ and $p\ge 2$.
\end{thm}

\subsection{Covering maps: A general class}
We now present a more general class of examples which were introduced in \cite{BV13}. As before the maps we consider here will be constructed around a given map $T:M\to M$ with $M=[0,1]$. We take the functional space $\cal V$ to be the Banach space of bounded variation functions with norm $\|\cdot\|_{BV}$. Now we introduce such a initial map $T$.

{\bf(H1)} There exists a partition $\cal A=\{A_i\}_{i=1}^{m}$ of $M$, which consists of pairwise disjoint intervals $A_i$. Let
$\bar A_i:=[c_{i,0},c_{i+1,0}]$ and there exists $\delta>0$ such that $T_{i,0}:=T|_{(c_{i,0},c_{i+1,0})}$ is $C^2$ and extends to a $C^2$ function
$\bar T_{i,0}$ on a neighbourhood $[c_{i,0}-\delta,c_{i+1,0}+\delta]$ of $\bar A_i$.

{\bf(H2)} There exists $\beta_0<\frac{1}{2}$ such that $\inf_{x\in I\setminus \cal C_0}|T'(x)|\ge \beta_0^{-1}$, where $\cal C_0=\{c_{i,0}\}_{i=1}^{m}$.

Next, we construct the perturbed map $T_\eps$ in the following way. Each map $T_\eps$ has a partition $\{A_{i,\eps}\}_{i=1}^{m}$ of $M$, which consists of pairwise disjoint intervals $A_{i,\eps}$,
$\bar A_{i,\eps}:=[c_{i,\eps},c_{i+1,\eps}]$ such that

(i) for each $i$ we have $[c_{i,0}+\delta,c_{i+1,0}-\delta]\subset [c_{i,\eps},c_{i+1,\eps}]\subset[c_{i,0}-\delta,c_{i+1,0}+\delta]$; whenever
$c_{1,0}=0$ or $c_{m+1,0}=1$, we do not move them with $\delta$. In this way we establish a one-to-one correspondence between the unperturbed and the perturbed boundary points of $A_i$ and $A_{i,\eps}$. (The quantity $\delta$ is from the assumption (H1) above.)

(ii) The map $T_\eps$ is locally injective over the closed intervals $\bar A_{i,\eps}$, of class $C^2$ in their interiors, and expanding with
$\inf_{x}|T'_\eps(x)|> 2$. Moreover, if $c_{i,0}$ and $c_{i,\eps}$ are two (left or right) corresponding points, we assume that there exists $\sigma>0$ such that $\forall \eps>0$, $\forall i=1,\ldots,m$ and
$\forall x\in [c_{i,0}-\delta,c_{i+1,0}+\delta]\cap\bar A_{i,\eps}$, we have
\begin{align}\label{c}
|c_{i,0}-c_{i,\eps}|\le \sigma
\end{align}
and
\begin{align}\label{T12}
|\bar T_{i,0}(x)-T_{i,\eps}(x)|\le \sigma.
\end{align}

We note that the assumption (H2), more precisely the fact that $\beta_0^{-1}$ is strictly bigger than $2$ instead of $1$, is sufficient to get the uniform Doeblin-Fortet-Lasota-Yorke inequality
(DFLY), as explained in Section~4.2 of \cite{GHW11}. In order to deal with the lower bound condition (MIN), we need to require the following condition. We refer to \cite[Section~2.6]{AR16} or \cite[Section~7.4]{Haydn17} for more details.\\
{\bf Covering property.} There exist $n_0$ and $N(n_0)$ such that:\\
(i) the partition into sets $A_{k_1,\ldots,k_{n_0}}^{\eps_1,\cdots,\eps_{n_0}}$ has diameter less than $\frac{1}{2au}$, where we use the notation
$A_{i,\eps_k}$ to denote the $i$ domain of injectivity of the map $T_{\eps_k}$, and
\[
A_{k_1,\ldots,k_{n}}^{\eps_1,\cdots,\eps_{n}}:=T_{k_1,\eps_1}^{-1}\circ\cdots\circ T_{k_{n-1},\eps_{n-1}}^{-1}A_{k_n,\eps_n}\cap\cdots\cap
T_{k_1,\eps_1}^{-1}A_{k_2,\eps_2}\cap A_{k_1,\eps_1}.
\]
(ii) For any sequence $\eps_1,\cdots,\eps_{N(n_0)}$ and $k_1,\ldots,k_{n_0}$ we have
\[
T_{\eps_{N(n_0)}}\circ \cdots \circ T_{\eps_{n_0}+1}A_{k_1,\ldots,k_{n_0}}^{\eps_1,\cdots,\eps_{n_0}}=M.
\]
Meanwhile, the (Pos) condition also follows from the above covering condition. As for the continuity (Lip), we can extend the continuity for the expanding maps of the intervals to the general case if we can get the following bounds:
\begin{align}\label{TDT}
\left.
\begin{matrix}
|T_{\eps_1}^{-1}(x)-T_{\eps_2}^{-1}(x)|\\
|DT_{\eps_1}(x)-DT_{\eps_2}(x)|\\
\end{matrix}
\right\}=O(|\eps_1-\eps_2|),
\end{align}
where the point $x$ is in the same domain of injective of the maps $T_{\eps_1}$ and $T_{\eps_2}$, the comparison of the same functions and
derivative in two different points being controlled by the condition \eqref{c}. The bounds \eqref{TDT} follow easily by adding to \eqref{c},
\eqref{T12} the further assumptions that $\sigma=O(\eps)$ and requiring a continuity condition for derivatives like \eqref{T12} and with
$\sigma$ again being order of $\eps$.

Combining the above statements, we obtain
\begin{thm}
 Let $T:M\to M$ be a map defined above. Assume that $\{T_{\eps_k}\}$ is the sequence of maps satisfying the above conditions, and the sequence $\{\eps_k\}_{k\ge 1}$ satisfies $|\eps_k|\le k^{-\theta}$, $\theta>1/2$. If $v$ is not a coboundary for $T$, then for any $\delta>0$, there exists a constant $C>0$ such that
$\mathcal{W}_{p}(W_{n},W)\leq C n^{-(\frac{1}{4}-\delta)}$ for all $n\gg 1$ and $p\ge 2$.
\end{thm}
\appendix

\section{}

%\begin{thm}[Kolmogorov continuity criterion \cite{kunita}]\label{kcc}
%Let $X=\{X(t), t\in [0,T] \}$ be an n-dimensional stochastic process such that
%\begin{align*}
%E|X(t)-X(s)|^\beta\le C|t-s|^{1+\alpha}
%\end{align*}
%for constants $\beta, \alpha>0$, $C\ge 0$ and  for all $0\le s,t \le T$. Then $X$ has a continuous version $\widetilde{X}$.
%
%Further for each $0<\gamma<\frac{\alpha}{\beta}$, there exists a positive random variable $K(\omega)$ with $E(K^{\beta})<\infty$ such that
%\[
%|\widetilde{X}(t,\omega)- \widetilde{X}(s,\omega)|\le K(\omega)|s-t|^{\gamma},\quad \hbox{for~every~} s,t \in [0,T]
%\]
%holds for almost all $\omega$.
%\end{thm}

\begin{thm}[Skorokhod embedding theorem \cite{HH80}]\label{ske}
Let $\{S_n=\sum_{i=1}^{n}X_{i},\mathcal{F}_{n}, n\ge 1\}$ be a zero-mean, square-integrable martingale. Then there exist a probability space supporting a (standard) Brownian motion $W$ and a sequence of nonnegative variables $\tau_{1}, \tau_{2},\ldots$ with the following properties: if $T_{n}=\sum_{i=1}^{n}\tau_{i}$, $S'_{n}=W(T_n)$, $X'_1=S'_1$, $X'_n=S'_{n}-S'_{n-1}$ for $n\ge 2$, and $\mathcal{B}_{n}$ is the $\sigma$-field generated by $S'_1,\ldots, S'_{n}$ and $W(t)$ for $0\le t\le T_n$, then
\begin{enumerate}
\item $\{S_{n}, n\ge 1\}=_{d} \{S'_n, n\ge 1\}$;
\item $T_n$ is a stopping time with respect to $\mathcal{B}_n$;
\item $\E(\tau_{n}|\mathcal{B}_{n-1})=\E(|X'_{n}|^{2}|\mathcal{B}_{n-1}) $ a.s.;
\item for any $p>1$, there exists a constant $C_p<\infty$ depending only on $p$ such that
\[
\E(\tau_{n}^{p}|\mathcal{B}_{n-1})\leq C_{p}\E(|X'_{n}|^{2p}|\mathcal{B}_{n-1})=C_{p}\E(|X'_{n}|^{2p}|X'_1,\ldots,X'_{n-1}) \quad \hbox{a.s.},
\]
where $C_p=2(8/\pi^2)^{p-1}\Gamma(p+1)$, with $\Gamma$ being the usual Gamma function.
\end{enumerate}
\end{thm}

\begin{prop}\label{mpe}
Let $X_1, X_2,\ldots, X_n$ be real-valued random variables defined on a common probability space and $\|X_i\|_{p}<\infty$ for $1\le i\le n$, $p\ge 1$. Then
\[\big\|\max_{1\le k\le n}| X_k|\big\|_{p}\le n^{\frac{1}{p}}\max\{\|X_k\|_{p}: 1\le k\le n\}.\]
\end{prop}

\begin{proof}
We have $\max_{1\le k\le n}|X_k|^p\le \sum_{i=1}^{n}|X_i|^p$, and the proposition follows by taking expectation of both sides.
\end{proof}
\begin{prop}\cite[Proposition~7]{MPU06}\label{rio}
Let $X_1, X_2,\ldots, X_n$ be a sequence of real-valued square integrable random variables defined on a common probability space,
and $\cal F_i=\sigma(X_j, 0\le j\le i)$ for $1\le i\le n$. Define $S_n=\sum_{i=1}^{n}X_i$ and
\[
b_{i,n}=\max_{i\le l\le n}\|X_i\sum_{k=i}^{l}\E(X_k|\cal F_i)\|_{p/2}.
\]
Then for any $p\ge 2$, the following inequality holds
\[
\E|S_n|^p\le (2p\sum_{i=1}^{n}b_{i,n})^{p/2}.
\]
In addition for $p\ge 2$,
\[
\E(\max_{1\le j\le n}|S_j|^p)\le C_p(\sum_{i=1}^{n}b_{i,n})^{p/2}.
\]
\end{prop}

\section*{Acknowledgements}
This work is supported by NSFC Grants 11871132, 11925102, and Dalian High-level Talent Innovation Project (Grant 2020RD09).


%\bibliography{ref}
%\bibliographystyle{plain}
\begin{thebibliography}{xx}


\bibitem{AHNTV15}
R. Aimino, H. Hu, M. Nicol, A. T\"{o}r\"{o}k and S. Vaienti,
\newblock Polynomial loss of memory for maps of the interval with a neutral
  fixed point,
\newblock {\em Discrete Contin. Dyn. Syst.} {\bf35} (2015), 793--806.

\bibitem{AR16}
R. Aimino and J. Rousseau,
\newblock Concentration inequalities for sequential dynamical systems of the
  unit interval,
\newblock {\em Ergodic Theory Dynam. Systems} {\bf36} (2016), 2384--2407.

\bibitem{AFLV11}
J. F. Alves, J. M. Freitas, S. Luzzatto and S. Vaienti,
\newblock From rates of mixing to recurrence times via large deviations,
\newblock {\em Adv. Math.} {\bf228} (2011), 1203--1236.

\bibitem{AM19}
M. Antoniou and I. Melbourne,
\newblock Rate of convergence in the weak invariance principle for
  deterministic systems,
\newblock {\em Comm. Math. Phys.} {\bf369} (2019), 1147--1165.

\bibitem{AFV15}
H. Ayta\c{c}, J. M. Freitas and S. Vaienti,
\newblock Laws of rare events for deterministic and random dynamical systems,
\newblock {\em Trans. Amer. Math. Soc.} {\bf367} (2015), 8229--8278.

\bibitem{BV13}
W. Bahsoun and S. Vaienti,
\newblock Escape rates formulae and metastability for randomly perturbed maps,
\newblock {\em Nonlinearity} {\bf 26} (2013), 1415--1438.

\bibitem{BB84}
D. Berend and V. Bergelson,
\newblock Ergodic and mixing sequences of transformations,
\newblock {\em Ergodic Theory Dynam. Systems} {\bf 4} (1984), 353--366.

\bibitem{Bill99}
P. Billingsley,
\newblock {\em Convergence of Probability Measures,} 2nd edition,
Wiley Series in Probability and Statistics: Probability and Statistics. A Wiley-Interscience Publication. New York, 1999. x+277 pp.

\bibitem{B73}
A. A. Borovkov,
\newblock The rate of convergence in the invariance principle,
\newblock {\em Teor. Verojatnost. i Primenen.} {\bf 18} (1973), 217--234.

\bibitem{Bu73}
D. L. Burkholder,
\newblock Distribution function inequalities for martingales,
\newblock {\em Ann. Probability} {\bf 1} (1973), 19--42.

\bibitem{Conze07}
J. P. Conze and A. Raugi,
\newblock Limit theorems for sequential expanding dynamical systems on
  {$[0,1]$},
\newblock {\em Ergodic theory and related fields}, {\em
  Contemp. Math.} vol. 430, Amer. Math. Soc., Providence, RI, 2007, pp. 89--121.

\bibitem{DMR22}
J. Dedecker, F. Merlev\`ede and E. Rio,
\newblock Rates of convergence in the central limit theorem for martingales in
  the non stationary setting,
\newblock {\em Ann. Inst. Henri Poincar\'{e} Probab. Stat.} {\bf 58} (2022), 945--966.


\bibitem{FFV17}
A. C. M. Freitas, J. M. Freitas and S. Vaienti,
\newblock Extreme value laws for non stationary processes generated by
  sequential and random dynamical systems,
\newblock {\em Ann. Inst. Henri Poincar\'{e} Probab. Stat.} {\bf 53} (2017), 1341--1370.

\bibitem{GHW11}
C. Gonz\'{a}lez-Tokman, B. R. Huntand and P. Wright,
\newblock Approximating invariant densities of metastable systems,
\newblock {\em Ergodic Theory Dynam. Systems} {\bf31} (2011), 1345--1361.

\bibitem{H23}
Y. Hafouta,
\newblock Convergence rates in the functional {CLT} for {$\alpha$}-mixing
  triangular arrays,
\newblock {\em Stochastic Process. Appl.} {\bf161} (2023), 242--290.

\bibitem{HH80}
P. Hall and C. C. Heyde,
\newblock {\em Martingale Limit Theory and Its Application},
Probability and Mathematical Statistics. Academic Press, Inc. [Harcourt Brace Jovanovich, Publishers], New York-London, 1980. xii+308 pp.

\bibitem{Haydn17}
N. Haydn, M. Nicol, A. T\"{o}r\"{o}k and S. Vaienti,
\newblock Almost sure invariance principle for sequential and non-stationary
  dynamical systems,
\newblock {\em Trans. Amer. Math. Soc.} {\bf369} (2017), 5293--5316.

\bibitem{HNVZ13}
N. Haydn, M. Nicol, S. Vaienti and L. Zhang,
\newblock Central limit theorems for the shrinking target problem,
\newblock {\em J. Stat. Phys.} {\bf 153} (2013), 864--887.

\bibitem{HL20}
O. Hella and J. Lepp\"{a}nen,
\newblock Central limit theorems with a rate of convergence for time-dependent
  intermittent maps,
\newblock {\em Stoch. Dyn.} {\bf20} (2020), 2050025, 28 pp.

\bibitem{KKM18}
A. Korepanov, Z. Kosloff and I. Melbourne,
\newblock Martingale-coboundary decomposition for families of dynamical
  systems,
\newblock {\em Ann. Inst. H. Poincar\'{e} C Anal. Non Lin\'{e}aire}
  {\bf 35} (2018), 859--885.

\bibitem{KL22}
A. Korepanov and J. Lepp\"{a}nen,
\newblock Loss of memory and moment bounds for nonstationary intermittent
  dynamical systems,
\newblock {\em Comm. Math. Phys.} {\bf385} (2021), 905--935.

\bibitem{Liu23}
Z. Liu and Z. Wang,
\newblock {Wasserstein convergence rates in the invariance principle for
  deterministic dynamical systems}, to appear in 
\newblock {\em Ergodic Theory Dynam. Systems}.
http://dx.doi.org/10.1017/etds.2023.40.

\bibitem{MPU06}
F. Merlev\`ede, M. Peligrad and S. Utev,
\newblock Recent advances in invariance principles for stationary sequences,
\newblock {\em Probab. Surv.} {\bf3} (2006), 1--36.

\bibitem{Nicol21}
M. Nicol, F. P. Pereira and A. T\"{o}r\"{o}k,
\newblock Large deviations and central limit theorems for sequential and random
  systems of intermittent maps,
\newblock {\em Ergodic Theory Dynam. Systems} {\bf41} (2021), 2805--2832.

\bibitem{NTV18}
M. Nicol, A. T\"{o}r\"{o}k and S. Vaienti,
\newblock Central limit theorems for sequential and random intermittent
  dynamical systems,
\newblock {\em Ergodic Theory Dynam. Systems} {\bf38} (2018), 1127--1153.

\bibitem{Rio17}
E. Rio,
\newblock {\em Asymptotic theory of weakly dependent random processes},
Probability Theory and Stochastic Modelling, 80. Springer, Berlin, 2017. xviii+204 pp.

\bibitem{S00}
B. Saussol,
\newblock Absolutely continuous invariant measures for multidimensional
  expanding maps,
\newblock {\em Israel J. Math.} {\bf116} (2000), 223--248.

\bibitem{S72}
S. Sawyer,
\newblock Rates of convergence for some functionals in probability,
\newblock {\em Ann. Math. Statist.} {\bf43} (1972), 273--284.

\bibitem{S19}
Y. Su,
\newblock Almost surely invariance principle for non-stationary and random
  intermittent dynamical systems,
\newblock {\em Discrete Contin. Dyn. Syst.} {\bf 39} (2019), 6585--6597.

\bibitem{V09}
C. Villani,
\newblock {\em Optimal Transport. Old and New.} Grundlehren der mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences], 338. Springer-Verlag, Berlin, 2009. xxii+973 pp.

\end{thebibliography}

\end{document}
