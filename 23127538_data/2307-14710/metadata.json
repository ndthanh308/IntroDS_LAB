{
  "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
  "authors": [
    "Ryo Nakamura",
    "Hirokatsu Kataoka",
    "Sora Takashima",
    "Edgar Josafat Martinez Noriega",
    "Rio Yokota",
    "Nakamasa Inoue"
  ],
  "submission_date": "2023-07-27T08:58:39+00:00",
  "revised_dates": [
    "2023-07-31T01:06:05+00:00"
  ],
  "abstract": "Formula-driven supervised learning (FDSL) is a pre-training method that relies on synthetic images generated from mathematical formulae such as fractals. Prior work on FDSL has shown that pre-training vision transformers on such synthetic datasets can yield competitive accuracy on a wide range of downstream tasks. These synthetic images are categorized according to the parameters in the mathematical formula that generate them. In the present work, we hypothesize that the process for generating different instances for the same category in FDSL, can be viewed as a form of data augmentation. We validate this hypothesis by replacing the instances with data augmentation, which means we only need a single image per category. Our experiments shows that this one-instance fractal database (OFDB) performs better than the original dataset where instances were explicitly generated. We further scale up OFDB to 21,000 categories and show that it matches, or even surpasses, the model pre-trained on ImageNet-21k in ImageNet-1k fine-tuning. The number of images in OFDB is 21k, whereas ImageNet-21k has 14M. This opens new possibilities for pre-training vision transformers with much smaller datasets.",
  "categories": [
    "cs.CV"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.14710",
  "pdf_url": "https://arxiv.org/pdf/2307.14710v2",
  "comment": "Accepted to ICCV 2023",
  "num_versions": null,
  "size_before_bytes": 12765166,
  "size_after_bytes": 283854
}