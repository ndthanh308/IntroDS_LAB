
\documentclass[10pt]{article} %


\usepackage{hyperref}
\usepackage{url}



\usepackage{subcaption}
\usepackage{multirow}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{mathtools}
\usepackage{tabulary}
\usepackage{booktabs}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{booktabs}
\usepackage{pifont}

\usepackage{times}


\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother
\usepackage{tikz}
\usepackage{graphicx,verbatimbox}
\usepackage{tcolorbox}
\usepackage{color}


\usepackage{times}
\usepackage{epsfig}



\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, arrows.meta}
\usepackage{pgfplots}
\pgfplotsset{width=7.5cm,compat=1.12}
\usepgfplotslibrary{fillbetween}

\newcommand{\cmmnt}[1]{}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\cmarkg}{\textcolor{lightgray}{\ding{51}}}%
\newcommand{\xmarkg}{\textcolor{lightgray}{\ding{55}}}%

\usepackage{svg}
\usepackage{adjustbox}
\usepackage{longtable}

\usepackage{color, colortbl}

\definecolor{battleshipgrey}{rgb}{0.45, 0.45, 0.45}
\definecolor{asparagus}{rgb}{0.50, 0.70, 0.40}
\definecolor{cadmiumorange}{rgb}{1, 0.65, 0.2}
\definecolor{cornflowerblue}{rgb}{0.39, 0.58, 0.93}
\definecolor{coralred}{rgb}{0.99, 0.3, 0.3}

\definecolor{3modalities}{rgb}{1, 0.88, 0.88}
\definecolor{unified}{rgb}{1.0, 1.0, 0.8}


\newcommand{\unival}{\textbf{\textcolor{battleshipgrey}{Un}\textcolor{cornflowerblue}{I}\textcolor{cadmiumorange}{V}\textcolor{coralred}{A}\textcolor{asparagus}{L}}}


\newcommand{\cor}[1]{\textcolor{magenta}{[\textbf{Corentin}: #1]}}
\newcommand{\corr}[1]{\textcolor{magenta}{#1}}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\usepackage[preprint]{tmlr}


\title{\unival: Unified Model for Image, Video, Audio and Language Tasks 
}


\author{\name Mustafa Shukor \email mustafa.shukor@sorbonne-universite.fr \\
      \addr Sorbonne University\\
       MLIA, ISIR, Paris, France
      \AND
      \name Corentin Dancette \email corentin.dancette@sorbonne-universite.fr \\
      \addr Sorbonne University \\
       MLIA, ISIR, Paris, France
      \AND
      \name Alexandre Rame \email  alexandre.rame@isir.upmc.fr\\
      \addr Sorbonne University \\
       MLIA, ISIR, Paris, France
      \AND
      \name Matthieu Cord \email  matthieu.cord@sorbonne-universite.fr \\
      \addr Sorbonne University,  MLIA, ISIR, Paris, France \\
      Valeo.ai, Paris, France
      }


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  %
\def\year{YYYY} %
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} %


\begin{document}


\maketitle

\begin{abstract}
 Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification, allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (\emph{e.g.}, Flamingo \citep{alayrac2022flamingo}), trained on massive datasets, can support more than two modalities, current small to mid-scale unified models are still limited to 2 modalities, usually image-text or video-text. The question that we ask is: \textit{is it possible to build efficiently a unified model that can support all modalities?} To answer this, we propose \unival{}, a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters, the $\sim$ 0.25B parameter \unival{} model goes beyond two modalities and unifies text, images, video, and audio into a single model. Our model is efficiently pretrained on many tasks, based on task balancing and multimodal curriculum learning. \unival{} shows competitive performance to existing state-of-the-art approaches, across image and video-text tasks. The feature representations learned from image and video-text modalities,  allows the model to achieve competitive performance when finetuned on audio-text tasks, despite not being pretrained on audio. Thanks to the unified model, we propose a novel study on multimodal model merging via weight interpolation of models trained on different multimodal tasks, showing their benefits in particular for out-of-distribution generalization. Finally, we motivate unification by showing the synergy between tasks. The model weights and code are released here \href{https://github.com/mshukor/UnIVAL}{https://github.com/mshukor/UnIVAL}. 
 
 
\end{abstract}


\section{Introduction}


The advent of Large Language Models (LLMs) \citep{brown2020languagegpt3, rae2021scalinggopher, chowdhery2022palm, tay2022unifyingul2} represents a significant step towards the development of generalist models. Generally based on the Transformer architecture \citep{vaswani2017attention} and a single next-token prediction objective, they continue to astound the world with their remarkable performances in text understanding and generation. 

Nevertheless, their current limitation to a single modality (text) restricts their understanding and interaction with the world. This highlights the need for robust multimodal models handling diverse tasks across numerous modalities. 
Recently, many works have tried to go beyond single modality, and build powerful multimodal models \citep{huang2023languagekosmos, driess2023palme, li2023blip2} that surpass previous task/modality-specific approaches. However, most of these works focus on image-text tasks and only a handful of approaches aim to incorporate more than two modalities, such as image/video-text \citep{alayrac2022flamingo, wang2022omnivl}.





The prevailing approach for pretraining multimodal models revolves around training them on large, noisy image-caption datasets \citep{schuhmann2021laion, jia2021scaling_align, radford2021learning}, where the model is tasked with generating or aligning image-captions through causal generation or unmasking. However, this approach encounters a significant challenge: it relies on extensive datasets to compensate for the inherent noise and the relatively simple task of caption generation. In contrast, multitask learning \citep{Caruana1997MultitaskL} on relatively small yet high-quality datasets presents an alternative solution to learn efficient models capable of competing with their large-scale counterparts \citep{alayrac2022flamingo,chen2022pali,reed2022generalistgato}.

Current small to mid-scale (less than couple of hundred million parameters) vision-language models \citep{li2019visualbert, vicha, meter, li2022blip} still have task-specific modules/heads, many training objectives, and support a very small number of downstream tasks due to the different input/output format. 
Recently, the sequence-to-sequence OFA \citep{wang2022unifyingofa} and Unified-IO \citep{lu2022unifiedio} have made a noticeable step towards more unified systems that can support a wide range of image and image-text tasks, with more reasonable scales (\emph{e.g.} can fit on user-grade GPU). These models are pretrained on many good quality, public benchmarks. On video-text tasks, LAVENDER \citep{li2022lavender} takes a similar direction by unifying the pretraining tasks as Masked Language Modeling (MLM).
Sequence-to-sequence unified models are particularly well-suited for open-ended text generation tasks and can readily incorporate recent LLMs.
To guide the model in solving a specific task, a textual prompt resembling an instruction \citep{raffel2020exploringt5} is added at the beginning of the input sequence.
They have the capability to unify tasks across different modalities, and thus easily supporting new tasks, by representing all inputs and outputs as sequences of tokens, utilizing an unified input/output format and vocabulary. These tokens can represent various modalities such as text, image patches, bounding boxes, audio, video, or any other modality, without the need for task-specific modules/heads.
These strategies are straightforward to scale and manage, as they involve a single training objective and a single model.


However, existing works are still limited to downstream tasks with no more than 2 modalities (image-text or video-text).
Providing unification across a larger number of tasks and modalities would offers additional advantages.
First, we would benefit from the knowledge transfer across them, by harnessing their collaborative strengths.
Second, once pretraining is done, the model can be finetuned on many different datasets: because of the wider range of more diverse pretraining data, unification across more tasks would enable better and more efficient generalization after finetuning on novel tasks and modalities.
In this paper, we thus ask the following question.
\begin{center}
    \textit{Is it possible to build efficiently a unified model that can support all modalities?}
\end{center}
A positive answer to this question will pave the way for building generalist models that can potentially solve any task. 
To answer this question, we propose \unival{}, a step further towards generalist modality-agnostic models. \unival{} (illustrated in Fig.\ref{fig:unival}) goes beyond two modalities and unifies text, images, video, and audio into a single model.



Our contributions are multiple: 
\begin{itemize}
    \item To the best of our knowledge, \unival{} is the first model, with unified architecture, vocabulary, input/output format, and training objective, that is able to tackle image, video, and audio language tasks, without relying on large scale training or large model size. Our 0.25B parameter model achieves competitive performance to existing modality-customized work. With comparable model sizes, we achieves new SoTA on some tasks (\emph{e.g.} +1.4/+0.98/+0.46 points accuracy on RefCOCO/RefCOCO+/RefCOCOg Visual Grounding, +3.4 CIDEr on Audiocaps) .
    \item We show the benefits of multimodal curriculum learning with task balancing, for efficiently training the model beyond two modalities. 
    \item We show the importance of multitask pretraining, compared to the standard single task one, and study the synergy and knowledge transfer between pretrained tasks and modalities. In addition, we find that pretraining on more modalities makes the model generalizes better to new ones. In particular, without any audio pretraining, \unival{} is able to attain competitive performance to SoTA when finetuned on audio-text tasks.
    \item We propose a novel study on multimodal model merging via weight interpolation \citep{izmailov2018,wortsman2022modelsoups,rame2022diversediwa}. We show that, when weights are finetuned on different multimodal tasks from our unified pretrained model, interpolation in the weight space can effectively combine the skills of the various finetuned weights, creating more robust multitask models without any inference overhead. Thus, in addition to multitask pretraining, averaging differently finetuned weights is another way to leverage and recycle \citep{rame2022recyclingratatouille} the diversity of multimodal tasks, enabling their collaboration. This is the first study of weight interpolation showing its effectiveness with multimodal foundation models.    
\end{itemize}
























% Figure environment removed
\section{Related Work}
We provide a brief related work, further detailed in Appendix \ref{app:related}.








\paragraph{Multimodal pretraining.} So far, most of the efforts to build multimodal models have been focused on vision-language  pretraining. Contrastive-based approaches \citep{radford2021learning, jia2021scaling_align} try to learn shared and aligned latent space by training on hundreds of millions of pairs. More data-efficient approaches \citep{vicha, li2021alignalbef, li2022blip, meter, singh2022flava} relied on additional multimodal interaction modules and variety of training objectives such as image-text matching, masked language modeling and image-text contrastive \citep{chen2020uniter, kim2021vilt, lu2019vilbert, zhang2021vinvl}.  In the video-language community, similar approaches have tried to model the interaction between language and frames sequences \citep{cheng2022vindlu, wang2022allinone, fu2021violet, zellers2021merlot, yang2021justask}. Few works have targeted both image and video language pretraining \citep{wang2022omnivl}.




\begin{table*}[h]
    \small
	\centering	
 \setlength\tabcolsep{4pt}
 
	\resizebox{\linewidth}{!}{
 
	\begin{tabular}	{lc@{\hspace{15pt}}c@{\hspace{15pt}}c@{\hspace{5pt}}c@{\hspace{15pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}}
		\toprule	 	
	 \multirow{2}{*}{Method}  & \multirow{2}{*}{PT examples. I (V)} & \multirow{2}{*}{Model Size} & \multicolumn{2}{c@{\hspace{15pt}}}{Param. init} & \multicolumn{2}{c@{\hspace{25pt}}}{PT Modalities} & \multicolumn{3}{c@{\hspace{20pt}}}{DS Modalities} & \multicolumn{4}{c@{\hspace{20pt}}}{Unified}  \\
    &  &  & V & L & I-T & V-T & I-T & V-T & A-T & Arch. & I/O & Tasks & Objective  \\
  \midrule

GIT/2 \citep{wang2022git} & 0.8B/12.9B & 0.7B/5.1B & Florence/DaViT & Random  & \cmark & & \cmark & \cmark &  & \cmmnt{encoder?} &  & \cmark & \cmark \\
PaLI \citep{chen2022pali}  & 12B+ & 3B/15B/17B & ViT-G & mT5 & \cmark & & \cmark & & & \cmmnt{(encoder?)} & & \cmark & \cmark \\
CoCa \citep{yu2022coca} & 4.8B & 2.1B & Random & Random & \cmark & & \cmark & \cmark & & \cmmnt{(encoder?)} & & \cmmnt{classif} &  \\
Unified-IO \citep{lu2022unifiedio} & 130M+ & 0.2B/0.8B/2.8B & Random & T5 & \cmark & & \cmark & & & \cmark & \cmark & \cmark & \cmark \\
OmniVL \citep{wang2022omnivl}  & 15.3M (2.8M) & 0.2B & TimeSformer & BERT & \cmark & \cmark & \cmark &  \cmark & &  &  & \cmark &  \\
VIOLET \citep{fu2021violet}  & 3.3M (182.5M)& 0.2B & VideoSwin & BERT & \cmark & \cmark &  &  \cmark & & \cmark &  &  &  \\
Merlot Reserve \citep{zellers2022merlot} & (960M) & $\sim$ 0.3B/0.7B & ViT/AST & - & & \cmark & & \cmark & & & \cmark \cmmnt{(MASK?)} & \cmark &    \\
LAVENDER \citep{li2022lavender} & 19M (14.4M) & $\sim$ 0.2B & VidSwin & BERT & \cmark & \cmark &  & \cmark & & & \cmark & \cmark & \cmark     \\
 BLIP-2 \citep{li2023blip2} & 129M+ & 12.1B & EVA/CLIP & FlanT5/OPT & \cmark & & \cmark & & & \cmmnt{encoder} & & \cmark & \\
 FLamingo \citep{alayrac2022flamingo} & 2.3B (27M) & 3.2B/9.3B/80B & CLIP & Chinchilla & \cmark & \cmark & \cmark & \cmark & & \cmark \cmmnt{(encoder)} & & \cmark & \cmark\\
 OFA \citep{wang2022unifyingofa} & 60M+ & 0.2B/0.5B/0.9B & ResNet & BART & \cmark & & \cmark & & & \cmark & \cmark & \cmark & \cmark \\
 Gato \citep{reed2022generalistgato} & 2.2B+ & 1.2B & ResNet & N/A & \cmark & & \cmark &  & & \cmark & \cmark & \cmark & \cmark \\ \midrule
 \unival{} (ours) & 21.4M (5M) & 0.25B & ResNet/ResNeXt & BART & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark \\
	\bottomrule
	\end{tabular} 
 }
	\caption
	{
	\footnotesize	
    Comparison of different foundation models. Our \unival{} approach is pretrained on a relatively small dataset, tackles image/video/audio-text modalities, while unifying the 4 different aspects explained in Sec.\ref{sec:method} and in Appendix \ref{app:background}: unified model, input/output format,  pretraining tasks, and training objective.}
	\label{tab:foundations}
\end{table*}

\paragraph{Unified models.}
Building unified systems has been explored first in the NLP community. \citet{raffel2020exploringt5} proposed the T5 transformer model, a text-to-text framework that solves many NLP tasks, each one being described by a task-specific textual prefix. Since then, building general textual models has been heavily explored with LLMs \citep{brown2020languagegpt3, rae2021scalinggopher, chowdhery2022palm}. This inspired other communities to build unified models. In the vision community, the work of \citet{chen2022unifiedpix2seqall}, proposed a pixel-to-sequence framework to unify different vision tasks such as object detection and instance segmentation. For multimodal tasks, \citet{cho2021unifyingvlt5} proposed to unify vision-language tasks as conditional text generation. OFA \citep{wang2022unifyingofa} then proposed a large-scale sequence-to-sequence framework and extended previous approaches to more image-text tasks, including text-to-image generation. Similarly, Unified-IO \citep{lu2022unifiedio}, in addition to image-text tasks, targets many visual tasks including dense prediction ones. 
The closest works to us are indeed OFA \citep{wang2022unifyingofa} and Unified-IO \citep{lu2022unifiedio}, however, we propose to unify tasks across more modalities, with significantly smaller model and dataset sizes. The differences are clarified in Tab.\ref{tab:foundations}, where we compare different foundation models involving unification.













\paragraph{Weight averaging across multimodal tasks.}
To combine multiple expert models with diverse specializations, we leverage a simple yet practical strategy: \emph{linear interpolation in the weight space}.
We follow \citet{2022arXiv221204089I,daheim2023elastic,ortiz2023task} suggesting that averaging networks in weights can combine their abilities without any computational overhead.
In particular, this weight averaging (WA) strategy was shown useful in model soups approaches \citep{wortsman2022modelsoups,rame2022diversediwa} to improve out-of-distribution generalization as an approximation of the more costly averaging of predictions \citep{Lakshminarayanan2017}.
Recent works extended WA to weights fine-tuned with different losses \citep{rame2022diversediwa,rame2023rewarded,croce2023seasoning} or on different datasets \citep{matena2022merging,choshen2022fusing,choshen2022cold,rame2022recyclingratatouille}.
In addition, some techniques try to leverage the features learned on different auxiliary tasks for a given target task. Fusing \citep{choshen2022fusing} averages multiple auxiliary weights to serve as an initialization for the unique finetuning on the target task. In contrast, ratatouille \citep{rame2022recyclingratatouille} delays the averaging after the multiple finetunings on the target tasks: each auxiliary model is finetuned independantly on the target task, and then all fine-tuned weights are averaged. These approaches consider classification tasks, for a given modality (usually images): interpolating weights of models trained on different multimodal tasks is very little investigated. The most similar and concurrent work is the recent \citet{sung2023empirical} applying a complex architecture-specific merging strategy. This work differs from us, as we explore WA during finetuning on multimodal downstream tasks, where they merge models pretrained on different modalities.




\section{Background on Unified Foundation Models: 4 Unification axes}
\label{app:background}
While many previous works have attempted to build unified models, they still have some customization in terms of architectures and tasks. Our work tries to unify most aspects of the model, following a recent line of work \citep{wang2022unifyingofa}. In the following, we detail the 4 unification axes that distinguish our work from previous ones.



\paragraph{Unified input/output.} To have a unified model, it is important to have the same input and output format across all tasks and modalities. The common approach is to cast everything to sequence of tokens as in language models. Multimodal inputs, such as images, videos and audios can be transformed to tokens by patchifying or using shallow modality-specific projections. Multimodal outputs can also be discritized, by using VQ-GAN for images and discrete pixel locations for visual grounding. A unified vocabulary is used when training the model.

\paragraph{Unified model.} The unified input/output representation allows to use a single model to solve all tasks, without the need to any adaptation when transitioning from the pretraining to the finetuning phase (\emph{e.g.}, no need for task-specific heads). In addition, the current advances in LLMs, especially their generalization to new tasks, make it a good choice to leverage these models to solve multimodal tasks. The common approach is to have a language model as the core model, with light-weight modality-specific input projections.

\paragraph{Unified tasks.} To seamlessly evaluate the model on new unseen tasks, it is essential to reformulate all tasks in the same way. For sequence-to-sequence frameworks, this can be done via prompting, where each task is specified by a particular textual instruction. In addition, discriminaive tasks can be cast to generation ones, and thus having only sequence generation output.

\paragraph{Unified training objective.} Due to the success of next token prediction in LLMs, it is common to use this objective to train also unified models. An alternative, is to use an equivalent to the MLM loss. The same loss is used during pretraining and finetuning.

\section{Pretraining of \unival{}}
\label{sec:method}










Current multimodal models are pretrained on massive noisy datasets with a limited number of tasks (\emph{e.g.}, image-conditioned text generation). We focus on the challenge of achieving reasonable performance without relying on vast amounts of data. Our approach involves multi-task pretraining on many good-quality datasets. We hope that the quality mitigates the need for massive datasets, thereby reducing computational requirements, while enhancing the model's generalization capabilities to novel tasks. The adoption of such an approach has become increasingly easy due to the growing availability of public, human-annotated, or automatically generated datasets. \unival{} is unified along the following 4 axes (more detailed in Appendix \ref{app:background}); model, input/output format, pretraining tasks, and training objective.


\subsection{Unified model}

Our model's core is a LM designed to process abstract representations. It is enhanced with lightweight modality-specific projections that enable the mapping of different modalities to a shared and more abstract representation space, which can then be processed by the LM. We use the same model during pretraining and finetuning of all tasks, without any task-specific heads. We detail below key components of this architecture, that are further detailed in Appendix~\ref{app:arch}.


\paragraph{Shared module.} To tackle multimodal tasks at small to mid-scale, we employ an encoder-decoder LM, due to its effectiveness for multimodal tasks and zero-shot generalization after multitask training. Another advantage of this architecture is the inclusion of bidirectional attention mechanisms in addition to unidirectional causal attention. This is particularly beneficial for processing various non-textual modalities. Our model accepts a sequence of tokens representing different modalities as input and generates a sequence of tokens as output.


\paragraph{Light-weight specialized modules.} To optimize data and compute requirements, it is crucial to map different modalities to a shared representation space, before feeding them into the encoder of the LM. To achieve this, we employ lightweight modality-specific encoders. Each encoder extracts a feature map, which is then flattened to generate a sequence of tokens. These tokens are linearly projected to match the input dimension of the LM. It is important to strike a balance in the choice of encoder complexity. Using overly simplistic encoders, such as linear projections, may disrupt the LM, impede training speed, and necessitate larger datasets and then computational resources. Conversely, employing excessively complex encoders can hinder the benefits of learning a unified representation in the shared module. In our approach, we opt for CNN encoders as they scale effectively with high-resolution inputs, minimize the number of output tokens, and exhibit improved efficiency during both inference and training compared to transformers. 


\subsection{Unified input/output format}

The input/output of all tasks consists of a sequence of tokens, where we use a unified vocabulary that contains text, location, and discrete image tokens.

\subsection{Unified pretraining tasks}

To train a single model on many tasks, a unified representation of these tasks is necessary. As our model's core is a LM, we transform all tasks into a sequence-to-sequence format, where each task is specified by a textual prompt (\emph{e.g.}, "What does the video describe?" for video captioning).
 For pretraining tasks, we pretrain only on relatively small public datasets, such as image captioning (COCO \citep{lin2014microsoftcoco}, Visual Genome (VG) \citep{krishna2017visualgenome}, SBU \citep{sbu}, CC3M \citep{Sharma2018ConceptualCA} and CC12M \citep{changpinyo2021conceptual} (only in the first stage)), VQA (VQAv2 \citep{goyal2017makingvqav2}, GQA \citep{hudson2019gqa}, VG \citep{krishna2017visualgenome}), Visual Grounding (VGround) and referring expression comprehension (RefCOCO, RefCOCO+, RefCOCOg \citep{yu2016modelingrefcoco+}), video captioning (WebVid2M \citep{bain2021frozenwebvid}) and video question answering (WebVidQA \citep{yang2021justask}). Note that we only use the training sets during pretraining. 
Pretraining tasks are further detailed in Appendix \ref{app:pret_tasks}. 





\subsection{Unified training objective}

We follow other approaches \citep{wang2022unifyingofa, alayrac2022flamingo} and optimize the model for conditional next token prediction. Specifically, we use a cross-entropy loss.

\subsection{Efficient pretraining}

Besides unification across tasks and modalities, we detail different techniques for efficient pretraining.

% Figure environment removed

\paragraph{Multimodal curriculum learning (MCL).} Other works train the model on all tasks and modalities simultaneously \citep{wang2022unifyingofa, li2022lavender}. However, we have observed that models trained on more modalities tend to exhibit better generalization to new ones. To capitalize on this, we employ a different strategy wherein we gradually introduce additional modalities during training (shown in Fig.\ref{fig:curriculum}). This approach facilitates a smoother transition to new modalities by providing a better initialization. Furthermore, this paradigm significantly reduces computational requirements compared to training on the entire dataset at once. Previous studies \citep{wang2022omnivl} have demonstrated notable performance enhancements when employing this paradigm for shared visual encoders (applied to both images and videos). In our work, we extend this setting beyond shared visual encoders, and show its effectiveness for modality-specific projections and unified models. This approach mainly yields gains in training efficiency. This is important as it allows us to leverage existing pretrained multimodal models to incorporate new modalities. To validate the approach, we train the same model on image-text and video-text data for 20 epochs using 2 training approaches; the one-stage approach where we train on all data from the beginning, and  our 2-stage curriculum training where we start to train on image-text for 10 epochs then we continue training on all data for the next 10 epochs. Tab.\ref{tab:curr}, shows that the performance of both approaches are comparable. However, the 2-stage approach is more efficient in terms of training time (18\% faster) and memory (25\% less GPU~memory).


\begin{table*}[h]
\centering
  \label{tab:vid_cap}
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l|ccccccc}
    \toprule
    Method & Train. time & Avg. bs & COCO & VQA v2 & RefCOCO+ & MSR-VTT & MSRVTT-QA\\
    \midrule
    
     One-stage  & 2h04m & 4K & 127.9 & 73.21 & 70.89 & 55.9 & 42.38  \\ %
     MCL  & 1h42m  & 3K & 128 & 73.24 & 70.19 & 56.3 & 42.27   \\
     
    
    \bottomrule
\end{tabular*}
}
\caption{\footnotesize \textbf{Multimodal Curriculum learning (MCL).} We show that our multi-stage training is more efficient than the one stage one and leads to on par results. The training time is for one epoch on the same number of GPUs.}
\label{tab:curr}
\end{table*}








\paragraph{Multimodal task balancing.} Contrary to previous work \citep{wang2022unifyingofa}, we find it more beneficial to balance the tasks in the batch, especially when using highly unbalanced datasets. Tab.\ref{tab:balancing} shows some results. We compare models trained without balancing, where in each batch the number of examples for each task is proportional to the corresponding dataset size, and with task balancing, where the tasks have similar number of examples. The results show a consistent improvement after balancing especially with highly unbalanced datasets (\emph{e.g.}, when adding CC12M, the overall performance drops significantly (B+CC12M)). 

\begin{table*}[h]
\centering
  
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l |cccc@{}}
    \toprule
     Data & Task Balancing &  COCO & VQA v2 & RefCOCO+  \\
    \midrule
      B & \xmark & 127.0  & 72.93 &  66.03    \\ 
      B+CC12M  & \xmark &  126.8  & 72.79  &  68.04      \\
      \midrule
      B+VQA+Ground.  & \xmark &  129.9  & 74.43  &   78.78    \\
      B+VQA+Ground.  & \cmark & 130.3  & 75.44  & 78.99       \\
      \midrule
      B+VQA+Ground.+CC12M  & \xmark & 129.9  & 75.21   &   78.85     \\
      B+VQA+Ground.+CC12M  & \cmark & 131.3  & 75.34  &   79.47 \\
    \bottomrule
\end{tabular*}
}

\caption{\footnotesize \textbf{Multimodal task balancing.} Task balancing significantly improve the performance, especially when using datasets that largely differ in size (\emph{e.g.}, CC12M). The baseline (B) consists of; VQAv2, RefCOCO+/CC3M/SBU/COCO/VG. VQA; GQA/VG. Ground.: RefCOCO/RefCOCOg.}
\label{tab:balancing}
\end{table*}














\paragraph{Implementation details for pretraining.}
The architecture of the LM is a typical encoder-decoder transformer initialized by BART-base \citep{lewis2020bart} with few modifications, following the implementation details of other work \citep{wang2022unifyingofa}. The modality-specific encoders are ResNet-101 pretrained on ImageNet as image encoder, 3D ResNext-101 \citep{hara2018can3dresnext} pretrained on kinetics 400 as video encoder and PANN encoder pretrained for audio classification as audio encoder, we do not skip the last block as done by previous approaches \citep{wang2022unifyingofa}. We use Adam optimizer with weight decay 0.01 and linear decay scheduler for the learning rate starting from $2e-4$. All model parameters are pretrained in 2 stages; first we train only on image-text tasks for 150k steps and batch size 3200, then we add video-text tasks and continue training (after removing CC12M) for ~90K steps with batch size 4K (2k for each modality). At the end of the last stage, we train the model for additional epoch after increasing the resolution of images from 384 to 480 and the videos from $224 \times 224$ and 8 frames to $384 \times 384$  and 16 frames. More details in Appendix \ref{app:details}.




\begin{table}[h]
\centering
  
    \resizebox{0.9\linewidth}{!}{
    
    \begin{tabular}{l|ccccccc}
    \toprule
    Data (Modality)  & Data size (\# of examples) & Method &  COCO & VQA v2 & RefCOCO+ & MSR-VTT & MSRVTT-QA  \\
    \midrule
    CC3M (I)  & 2.8M & \multirow{4}{*}{One-task pretraining} & 117.3  & 69.5  & 55.2  &  - &   -   \\
    CC12M (I)  & 10M & & 120.2  & 71.6  & 56.7  &  - &   -   \\
    CC3M+CC12M (I)  &  12.8M & & 123.6   & 71.7  & 59.8  &  - &   -   \\ 
    COCO+SBU+VG+CC3M (I) & 5M & &125.8 & 72.0 & 56.1 & - & -\\
    \midrule
    B (I)  & 5.6M & \multirow{5}{*}{Multitask pretraining} & 127.0  & 72.9 &  66.0 & -  &   -   \\
    B+VQA (I)  & 7.94M &  & 128.9  & 73.2 &  71.0 &  - & -   \\
    B+Ground (I)  & 9.3M &  & 129.8 & 74.4 &  77.6 &  - &  -    \\
    B+VQA+Ground (I)  & 11.6M & &129.9  & 75.1 & 78.8 & -  &  -     \\
    B+VQA+Ground+CC12M (I)  & 21.6M & & 130.0  & 75.2  &  78.9 &   -  &  - \\ \midrule
    
     
    B (I+V)  & 8.1M & \multirow{4}{*}{Multitask pretraining} & 128.8  & 73.2  & 70.1 &  54.6  &  42.1    \\
    B+WebVidQA (I+V)  & 10.6M & & 128.0 & 73.2 & 70.2 & 56.3  & 42.3    \\
    B+VQA+WebVidQA (I+V) & 13.9M & &131.7 & 75.0 &  77.9 & 57.0  &  42.6  \\
    B+Ground.+WebVidQA (I+V)  & 17.6M & & 131.1  & 75.1 & 78.1 & 56.2  &  42.5   \\

     
    
    \bottomrule
\end{tabular}
}
\caption{\footnotesize \textbf{Knowledge transfer across tasks and datasets.} We show the synergy between different tasks and datasets. Multitask learning is efficient as it leverages the collaboration across tasks. Models are trained longer on I+V tasks.}
\label{tab:data_contrib}
\end{table}

\paragraph{Knowledge transfer across tasks and modalities.} We investigate the knowledge transfer between tasks/modalities. We train for 10 epochs on image-text (I) datasets, followed by 10 epochs on image/video-text (I+V) datasets. The results are shown in Tab.\ref{tab:data_contrib}. We first compare between single and multitask learning. For single task, the models are trained on different image captioning datasets. For multitask learning, the models are trained for several tasks such as captioning, VQA or grounding. Overall, multitask learning is more efficient. as with comparable number of examples, it significantly outperforms models trained on single task. 

Second, we investigate the synergy between tasks and datasets. For image-text pretraining, there is a clear benefit of multitask training. Specifically, training on VQA helps to get +1.9 points on Captioning and ~4 points for Visual Grounding. Similarly training on VGround, we have larger improvements on captioning and VQA. For image-text and video-text pretraining, VideoQA helps Video Caption and interestingly, Image VQA helps video tasks. We noticed that large datasets like CC12M does not bring significant improvements, compared to adding additional task with smaller number of examples. This also demonstrates that multitask learning is more efficient than large-scale single task learning.

We put in Appendix \ref{app:ablation} our experiments that study further the \textbf{knowledge transfer across modalities.} 


\section{\unival{} on Downstream Tasks}



In this section, we present the experimental results of \unival{} following different setups; finetuning on downstream datasets and direct evaluation without finetuning (\emph{e.g.} zero-shot). Other unified approaches are highlighted in yellow, and models targeting more than 2 modalities in red.


\subsection{Finetuning on multimodal tasks}
\label{sec:experiments_sota}
For \textbf{downstream tasks}, we finetune on standard image-text, video-text and audio-text benchmarks (Appendix \ref{app:details} contains more implementation details). To have a fairer comparison with OFA, we finetune the author's released checkpoint (denoted as $\text{OFA}\rm_{Base}^\dagger$) using the same hyperparametres as \unival{}.

\subsubsection{Image-text tasks}


\begin{table*}[h]
\center
\begin{adjustbox}{max width=0.7\textwidth}
\begin{tabular}{@{\extracolsep{\fill}}lcccccccc}
\toprule
  \multirow{2}*{Model}
  &\multicolumn{3}{c}{RefCOCO}
  &\multicolumn{3}{c}{RefCOCO+}
  &\multicolumn{2}{c}{RefCOCOg}
  \\
  & val & testA & testB
  & val & testA & testB
  &val-u & test-u
  \\
\midrule
  VL-T5 \citep{cho2021unifyingvlt5}
  &- & - & -
  &- & - & -
  &- & 71.3
  \\
  UNITER \citep{chen2020uniter}
  &81.41 & 87.04 & 74.17
  &75.90 & 81.45 & 66.70
  &74.86 & 75.77
  \\
  VILLA \citep{gan2020largevilla}
  &82.39 & 87.48 & 74.84
  &76.17 & 81.54 & 66.84
  &76.18 & 76.71
  \\
  MDETR \citep{kamath2021mdetr}
  &86.75 & 89.58 & 81.41
  &79.52 & 84.09 & 70.62
  &81.64 & 80.89
  \\
  \rowcolor{unified}
  UniTAB \citep{yang2021crossingunitab}
  &88.59 & 91.06 & 83.75 & 80.97 & 85.36 & 71.55 & 84.58 & 84.70 
  \\
  \rowcolor{unified}
  $\text{OFA}\rm_{Base}$ \citep{wang2022unifyingofa}
  & 88.48 & 90.67 & 83.30
  & 81.39 & \textbf{87.15} & 74.29
  &82.29 & 82.31
  \\
  \midrule
  \unival{} (ours)
  & \textbf{89.12} &  \textbf{91.53} &  \textbf{85.16} &   \textbf{82.18} &  86.92 &  \textbf{75.27} &  \textbf{84.70} &   \textbf{85.16}
  \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{\footnotesize \textbf{Finetuning for Visual Grounding on RefCOCO, RefCOCO+, and RefCOCOg datasets.} \unival{} achieves the new SoTA results among comparable model sizes.}
\label{tab:refcoco}
\end{table*}
\paragraph{Visual Grounding.} We evaluate the ability of the model to localise spatially the text in the image. This task consists of predicting the coordinates of bounding box given an input text. The task is cast as sequence generation task, where the model outputs a sequence of 4 pixel locations corresponding to the 4 corners of the bounding box. Tab.\ref{tab:refcoco} shows that we achive new SoTA results on all 3 benchmarks. Interestingly, our scores are better than the reported OFA scores, which additionally pretrain for object detection. 




\begin{table}[h]
\centering
    \begin{minipage}{.49\linewidth}
      \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{@{\extracolsep{\fill}}lcccc}
        \toprule
          \multirow{2}*{Model}
          & \multicolumn{2}{c}{VQAv2}
          & \multicolumn{2}{c}{SNLI-VE}
          \\
          & test-dev & test-std
          & dev & test
          \\
        \midrule
          UNITER \citep{chen2020uniter}
          & 73.8 & 74.0
          & 79.4 & 79.4
          \\
          OSCAR \citep{li2020oscar}
          &73.6 & 73.8
          &- & -
          \\
          VILLA \citep{gan2020largevilla}
          & 74.7 & 74.9
          &80.2 & 80.0
        
          \\
          VinVL \citep{zhang2021vinvl}
          &76.5 & 76.6
          &- & -
          \\
          UNIMO \citep{li2020unimo}
          &75.0 & 75.3
          &81.1 & 80.6
          \\
          ALBEF \citep{li2021alignalbef}
          &75.8 & 76.0
          &80.8 & 80.9
          \\
            ViCHA \citep{vicha}
          & 75.0 & 75.1
          & 79.9 & 79.4
          \\
          METER \citep{meter}
          &77.7 & 77.6
          &80.9 & 81.2
          \\
          \midrule
          \textcolor{gray}{\textit{Text-generation approaches}} \\
          \rowcolor{unified}
          VL-T5 \citep{cho2021unifyingvlt5}
          & - & 70.3
          &- & -
          \\    
          \rowcolor{unified}
          UniTAB \citep{yang2021crossingunitab} & 70.7 & 71.0 & - & - \\
        GIT-L \citep{wang2022git} & 75.5 & - & - & - \\
          \rowcolor{3modalities}
            OmniVL \citep{wang2022omnivl}
          &78.3 & 78.4
          &- & - \\
        \rowcolor{unified}
        $\text{OFA}\rm_{Base}^\dagger$ \citep{wang2022unifyingofa} 
            & 77.0  &  77.1
          & 78.8  &  78.6
          \\
        \midrule
        \textcolor{gray}{\textit{Large-scale pretraining}} \\
          SimVLM$_{Large}$ \citep{wang2021simvlm}
          &79.3 & 79.6
          &85.7 & 85.6
          \\
          Florence \citep{yuan2021florence}
          &80.2 & 80.4
          &- & -
          \\
        PaLM-E 84B \citep{driess2023palme} 
          & 80.5 & --
          &- & -
          \\
          \midrule
        
        \unival{} (ours) & 77.0  & 77.1 & 78.2 & 78.6 \\
        \bottomrule
        \end{tabular}
            }   
      
    \end{minipage}%
    \hfill
    \begin{minipage}{.49\linewidth}
      \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{@{\extracolsep{\fill}}lcccc}
        \toprule
          \multirow{2}*{Model}
          &\multicolumn{4}{c}{Cross-Entropy Optimization}
          \\
          &BLEU@4  & METEOR & CIDEr & SPICE
          \\
        \midrule
          VL-T5 \citep{cho2021unifyingvlt5}
          &34.5 & 28.7 & 116.5 & 21.9
          \\
          OSCAR \citep{li2020oscar}
          &37.4 & 30.7 & 127.8 & 23.5
          \\
          \rowcolor{unified}
          UniTAB \citep{yang2021crossingunitab}
          &36.1 & 28.6 & 119.8 & 21.7
          \\
          VinVL \citep{zhang2021vinvl}
          &38.5 & 30.4 & 130.8 & 23.4
          \\
          UNIMO \citep{li2020unimo}
          &39.6 &  - & 127.7 & -
          \\
        GIT-L \citep{wang2022git} &42.0 & 30.8 & 138.5 & 23.8
        \\
        \rowcolor{3modalities}
        OmniVL \citep{wang2022omnivl}
        &39.8 & - & 133.9 & -
        \\
         \rowcolor{unified}
         $\text{OFA}\rm_{Base}^\dagger$ \citep{wang2022unifyingofa}
          & 42.5  & 30.6  &  138.1 & 23.7  
          \\
        \midrule
        \textcolor{gray}{\textit{Large-scale pretraining}} \\
          LEMON \citep{hu2022scalinglemon}
          &41.5 &  30.8 & 139.1 & 24.1
          \\
          SimVLM$_{Large}$ \citep{wang2021simvlm}
          &40.3 & 33.4 & 142.6 & 24.7
          \\
        PaLM-E 84B \citep{driess2023palme} &-- & -- & 138.0 & --
          \\
        \midrule
         \unival{} (ours)
          & 42.0 & 30.5 & 137.0  & 23.6
          \\
        \bottomrule
        \end{tabular}
            }
    \label{tab:caption}
    \end{minipage} 

\caption{\footnotesize \textbf{Finetuning on Image-Text understanding and generation tasks such as VQAv2, SNLI-VE and Image Captioning.} Our text-generation based approach is competitive with other SoTA, while using less pretraining data.}
\label{tab:vqa_ve_caption}
\end{table}

\paragraph{Multimodal understanding tasks.} We evaluate on VQA and Visual entailment tasks, that we cast as text generation. Tab.\ref{tab:vqa_ve_caption} shows a comparison with other approaches. Despite pretraining on less data for less number of steps, our approach is on par with the previous unified model OFA \citep{wang2022unifyingofa} finetuned from the author's released checkpoint ($\text{OFA}\rm_{Base}^\dagger$). For comparable scale, we significantly outperform GIT$_L$ \citep{wang2022git} that uses CLIP-ViT-L as image encoder. Our model is competitive with other SoTA models trained on large datasets that casted the task as classification. Note that, we evaluate both our model and OFA, with beam search for VQA, instead of all-candidate evaluation. For SNLI-VE, our approach uses only the image and the text hypothesis, without the text premise as previously done in OFA \citep{wang2022unifyingofa}. The results on SNLI-VE suggest that unified models such OFA and our models underperform on the visual entailment task.








\paragraph{Multimodal generation tasks.} We evaluate the model for image captioning on COCO dataset \citep{lin2014microsoftcoco}, and report the scores on the Karpathy test split.  Tab.\ref{tab:vqa_ve_caption} shows that we are comparable with OFA. Compared to the previous OmniVL model \citep{wang2022omnivl} that pretrain on both image and video text datasets, we largely outperform it by more than 3 points CIDEr. Our model is very close to other SoTA such as GIT-L and large-scale trained ones such as LEMON and PaLM-E 84B.











\subsubsection{Video-Text tasks}

Here we evaluate the model on different video-text tasks.  


\begin{table}[h]
\centering
  
  \small
  \resizebox{0.6\linewidth}{!}{
    \begin{tabular}{lccc@{}}
    \toprule
    \textbf{Method} & \textbf{\#PT images/videos} & \textbf{MSRVTT-QA} & \textbf{MSVD-QA} \\
    \midrule
    ClipBERT~\citep{lei2021lessclipbert} &  0.15M/- & 37.4 & - \\
    JustAsk~\citep{yang2021justask} & -/69M & 41.5 & 46.3 \\
    ALPRO~\citep{li2022alignandpromptalpro} & 3M/2.5M & 42.1 & 45.9 \\
    MERLOT~\citep{zellers2021merlot} & -/180M & 43.1 & - \\
    VIOLET~\citep{fu2021violet} & 3.3M/182M & 43.9 & 47.9 \\
    All-in-one~\citep{wang2022allinone} & -/283M & 46.8 & 48.3  \\ 
    GIT~\citep{wang2022git} & 800M/- & 43.2 & 56.8  \\
    \rowcolor{3modalities}
    OmniVL \citep{wang2022omnivl} & 14M/2.8M & 44.1 & 51.0 \\ 
    \rowcolor{unified}
    LAVENDER \citep{li2022lavender} & 14M/14.4M & 45.0 & 56.6 \\
    \midrule
    \unival{} (ours) & 14M/2.5M & 43.48 & 49.55 \\
  \bottomrule
\end{tabular}
}

\caption{\footnotesize \textbf{Finetuning for VideoQA on MSRVTT-QA and MSVD-QA datasets}. The text-generation based \unival{} model is competitive with SoTA models customized for videos or trained on significantly larger datasets.
}
\label{tab:videoqa}
\end{table}



\paragraph{Video question answering.} We evaluate for VideoQA on MSRVTT-QA and MSVD-QA \citep{msvd_msrvtt} datasets. Tab.\ref{tab:videoqa} shows a comparison with other approaches. On MSRVTT-QA, we outperform large scale pretrained models like GIT, including models trained on more videos (MERLOT) and customised for VideoQA (JustAsk). We are competitive with the unified video model LAVENDER with heavier vision encoder (Video Swin), trained on more videos (and restrict the generated answers to one word), and the ununified OmniVL targeting both images and videos. On MSVD-QA, we have competitive performance to previous work. 










\begin{table}[h]
    
    \begin{minipage}{.55\linewidth}
      \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llcccc}

        \toprule
        ~ & ~ & \multicolumn{4}{c}{\textbf{MSRVTT}}  \\
        \cmidrule(lr){3-6} 
        Method & \#PT Image (Video) Data & B@4 & M & R & C   \\
        \midrule
        UniVL \citep{luo2020univl} & (136M) & 42.2 & 28.2 & 61.2 & 49.9 \\
        SwinBERT \citep{lin2022swinbert} & - & 41.9 & 29.9 & 62.1 & 53.8   \\
        CLIP4Caption \citep{tang2021clip4caption} & - & 46.1 & 30.7 & 63.7 & 57.7  \\
        MV-GPT$^T$ \citep{seo2022endmvgpt} & (53M) & 48.9 & 38.7 & 64.0 & 60.0 \\
        \rowcolor{unified}
        LAVENDER \citep{li2022lavender} & 14M (14.4M) &  - & - & - & 60.1  \\
        \midrule
        \unival{} (ours) & 14M (2.5M) &  46.42 & 29.01 & 62.92 & 60.5    \\
        \bottomrule
    \end{tabular}
    }
    \end{minipage}%
    \hfill
    \begin{minipage}{.4\linewidth}
      \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccc}

        \toprule
        ~  & \multicolumn{3}{c}{\textbf{ActivityNet-Captions}}  \\
        \cmidrule(lr){2-4} 
        Method &  B@3 & B@4 & M   \\
        \midrule
        DCEV \citep{krishna2017densedcev} & 4.09 &1.60 & 8.88\\
        DVC \citep{li2018jointlydvc} & 4.51 &1.71 & 9.31\\
        Bi-SST \citep{wang2018bidirectional} & -- & -- & 10.89 \\
        HACA \citep{wang-etal-2018-watchhaca} & 5.76 & 2.71 & 11.16\\
        MWSDEC \citep{rahman2019watchmwsdec} & 3.04 & 1.46 & 7.23\\
        MDVC \citep{iashin2020multimdvc} & -- & 1.46 & 7.23\\
        BMT \citep{iashin2020betterbmt} & 4.63 & 1.99 & 10.90 \\
        MV-GPT$^T$ \citep{seo2022endmvgpt} & -- & 6.84 & 12.31\\
        \midrule

        \unival{} (ours) & 7.67 & 4.76 & 10.51      \\
        \bottomrule
    \end{tabular}
    }
    \end{minipage} 
    \caption{\footnotesize \textbf{Finetuning for Video Captioning on MSRVTT and ActivityNet-Captions.} \unival{} is competitive with other task/modality-customized SoTA that are trained on larger datasets. $^T$: uses in addition text transcript. For ActivityNet-Captions we use ground-truth action proposals.}
    \label{tab:video_caption}
\end{table}




\paragraph{Video captioning.} We evaluate our model for Video Captioning. Tab.\ref{tab:video_caption} shows that our model is very competitive with other approaches customized for videos, trained on much larger datasets (LAVENDER) and use speech transcript as additional input (MV-GPT). On ActivityNet-Caption with ground truth proposal, we outperform previous approaches by significant margin as per the B@4 metric and we are competitive with the current SoTA MV-GPT.

\subsubsection{Audio-Text Tasks}

\begin{table*}[h]
\centering
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{ccccccc}
\hline
 Dataset & Method &  BLEU$_{1}$ & BLEU$_{2}$ & METEOR & CIDEr & SPICE  \\
\toprule
 \multirow{4}{*}{Audiocaps} & \citep{kim-etal-2019-Audiocaps} &  0.614 & 0.446 & 0.203 & 0.593 & 0.144  \\
 & \citep{xu2021investigating} & 0.655 & 0.476 & 0.229 & 0.660 & 0.168  \\ 
 & \citep{mei2021audio} &  0.647 & 0.488 & 0.222 & \underline{0.679} & 0.160  \\
 & \citep{liu2022leveraging} &  \underline{0.671} & \underline{0.498} & \underline{0.232} & 0.667 & \underline{0.172}  \\ \midrule
  & \unival{} (ours) & \textbf{0.690} & \textbf{0.515} & \textbf{0.237} & \textbf{0.713} & \textbf{0.178}  \\
\midrule
 \multirow{7}{*}{Clotho v1} 
 
 &  \citep{takeuchi2020effects} &  0.512 & 0.325 & 0.145 & 0.290 & 0.089  \\
 & \citep{koizumi2020transformer} &  0.521 & 0.309 & 0.149 & 0.258 & 0.097  \\
 & \citep{Chen2020transcnn} &  0.534 & 0.343 & 0.160 & 0.346 & 0.108  \\
 & \citep{xu2020crnn} &  0.561 & 0.341 & 0.162 & 0.338 & 0.108  \\
 & \citep{eren_sememb2020} &  \textbf{0.590} & 0.350 & \textbf{0.220} & 0.280 & -  \\ 
 & \citep{xu2021investigating} &  0.556 & 0.363 & 0.169 & \underline{0.377} & \textbf{0.115}  \\
 & \citep{koh2022automated} & 0.551 & \textbf{0.369} & 0.165 & \textbf{0.380} & 0.111  \\ \midrule
 & \unival{} (ours) & \underline{0.569} & \underline{0.367} & \underline{0.178} & \textbf{0.380} & \underline{0.114}  \\
\bottomrule
\end{tabular}
}
\caption{\footnotesize \textbf{Finetuning on the new audio-text modality for audio-captioning.} We compare \unival{} to other audio-text models on Audiocaps and Clotho v1 datasets. Despite not using audio-text during pretraining \unival{} is very competitive with other customized SoTA. We compare with models that rely only on audio as input. The best and next best scores are \textbf{bolded} and \underline{underlined} respectively.}
\label{tab:audio_cap}
\end{table*}

Even though we do not pretrain on audio-text data, we evaluate the generalization ability of our model to the new audio modality. We use an additional audio encoder pretrained on audio classification and finetune directly the encoder and core model pretrained on our image/video-text data. 

\paragraph{Audio captioning.}  We evaluate the model on standard audio captioning datasets; Clotho v1 and Audiocaps. Tab.\ref{tab:audio_cap} shows a comparison with other approaches that take solely the audio as input. Interestingly, we significantly outperform other approaches on Audiocaps, and we are competitive with the current SoTA on the small Clotho v1 dataset.  

    
   


















\subsection{Evaluation without finetuning}


\begin{table}[h]
    
    \begin{minipage}{.45\linewidth}
      \centering
        \resizebox{\linewidth}{!}{
        \begin{tabular}{lccc}
        \toprule
          \multirow{2}*{Model}
          & VQAv2
          & COCO Caption
          & RefCOCO+
          \\
          & test-dev Acc 
          & Val/Test CIDEr & Val Acc@0.5
          \\
        \midrule

        \rowcolor{unified}
          Unified-IO$_{Base}$ \citep{lu2022unifiedio}
          & 61.8 & 104.0/-- & --
          \\
        
        
        \rowcolor{unified}
        $\text{OFA}\rm_{Base}$ \citep{wang2022unifyingofa}
            &  68.91 &  74.47/75.27  &  30.45
          \\
          \midrule
        
        \unival{} & 70.18 & 90.07/91.04 & 70.81 \\
    
    
        \bottomrule
        \end{tabular}
        }
        
        \caption{\footnotesize \textbf{Evaluation without finetuning.} \unival{} outperforms OFA and competitive with Unified-IO trained on more data.}
        \label{tab:noft}
    \end{minipage}%
    \hfill
    \begin{minipage}{.5\linewidth}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccc}
    
        \toprule
      \multirow{2}*{Model}
      & OKVQA
      & VizWiz
      & NoCaps
      & MSRVTT-QA
      & MSVD-QA
      \\
      & Val Acc 
      & Val Acc & CIDEr (out-domain) & Test Acc & Test Acc
      \\
    \midrule
    \rowcolor{unified}
      Unified-IO$_{Base}$ \citep{lu2022unifiedio}
      & \textcolor{gray}{37.8} & \textcolor{gray}{45.8}  & -- & -- & --
      \\
    \rowcolor{unified}
    $\text{OFA}\rm_{Base}$ \citep{wang2022unifyingofa}
        &  40.16 &  17.33  &   \cmmnt{54.08/60.47} 48.95 & -- & --
    
      \\
      \rowcolor{unified}
    $\text{LAVENDER}$ \citep{li2022lavender}
        &  -- &  --  &   -- & 4.5 & 11.6
      \\
      \rowcolor{3modalities}
    $\text{Flamingo-3B}$ \citep{alayrac2022flamingo}
        &  41.2 &  28.9  &   -- & 11.0 & 27.5
    
      \\
      \midrule
    
    \unival{} & 38.91 & 20.22 &  \cmmnt{50.02/51.52} 47.68 & 5.84 & 21.15 \\
    \bottomrule
    \end{tabular}
    }
    
    \caption{\footnotesize \textbf{Zero-Shot Evaluation.} Scores in gray means the dataset is used during pretraining. \unival{} is competitive with modality-specific models.}
    \label{tab:zs}
    \end{minipage} 
\end{table}
    











\paragraph{Evaluation on seen datasets.} Following \citet{lu2022unifiedio}, we directly evaluate the representation learned during pretraining without task-specific finetuning. This setup is similar to the zero-shot evaluation, except that the evaluation tasks are seen during pretraining. We compare our model to different baselines following the same setup, with the main difference that other baselines pretrain longer, on significantly larger datasets and more tasks. Tab.\ref{tab:noft} shows that our approach outperforms the most similar baseline OFA on all tasks. Compared to Unified-IO, we are significantly better on VQAv2, despite pretraining on less VQA datasets.  








\paragraph{Evaluation on unseen datasets (zero-shot).} We follow the same previous setup, but we evaluate the model on new datasets, unseen during pretraining. Tab.\ref{tab:zs} shows a comparison with other models on several image and video-text datasets. Our model is very competitive to OFA, and close to Unified-IO (grayed scores) on OKVQA. However, Unified-IO pretrains on both OKVQA and VizWiz. Compared to the unified video-language model LAVENDER, we significantly outperform it on video tasks. Our approach attains close performance to the large-scale Flamingo-3B model on OKVQA and MSVD-QA. 


\subsection{Generalization to new tasks and modalities}
In this section we investigate the importance of pretraining on different modalities for the generalization to new tasks and modalities. Specifically, we want to validate the following hypothesis; \textit{pretraining on more modalities, and thus on more tasks, allows to learn more modality and task-agnostic representation}.

\begin{table}[h]
    \begin{minipage}{.5\linewidth}
      \centering
      \resizebox{0.8\linewidth}{!}{
      \centering
        \begin{tabular}{l|cc}
        \toprule
        Modality & Multitask &  Audiocaps  \\
        \midrule
         Image-Text  & \xmark  & 54.4  \\
         Image-Text   & \cmark & \textbf{57.6}  \\
         \midrule
         Text & \xmark & 53.2 \\ 
         Image-Text & \cmark  &   58.4  \\
         Video-Text & \cmark  &   57.4  \\
         Image-Text+Video-Text & \cmark  &  \textbf{58.8}  \\ 
        \bottomrule
    \end{tabular}
    }
      \caption{\footnotesize \textbf{Finetuning for Audio Captioning on the Audiocaps dataset.} We compare different initialization (after pretraining on Images-Text (I), Videos-Text (V), or Text (T)) for audio captioning. Pretraining on more modalities leads to better results when finetuning on audio captioning, a task not seen during pretraining.
      }
      \label{tab:new_modality}
    \end{minipage}%
    \hfill
    \begin{minipage}{.4\linewidth}
      \centering
      \resizebox{0.8\linewidth}{!}{
      \centering
        \begin{tabular}{l|c}
        \toprule
        \textbf{Method} &  CLIP score $\uparrow$  \\
        \midrule
         Text & 31.0 \\
         Image-Text   &  \textbf{31.6}  \\
         Image-Text+Video-Text   &  31.3 \\ 
        
        \bottomrule
    \end{tabular}
    }
    \caption{\footnotesize \textbf{Finetuning for text-to-image generation on COCO dataset}. Multimodal pretraining improves the results when finetuning on new text-to-image generation, a task not seen during pretraining. }
    \label{tab:image_generation}
    \end{minipage} 
\end{table}
    
\paragraph{Better initialization for new modalities: from vision-language to audio-language tasks.} We finetune our model for audio captioning on the Audiocaps dataset. To compare the effect of pretraining on more tasks and modalities, we evaluate the same model with different initialization; pretraining on text (the model initialized from BART), pretraining on image-text (with and without multitask pretraining), pretraining on video-text and pretraining on both image and video-text. We pretrain for the same number of epochs. Tab.\ref{tab:new_modality} shows that pretraining on image-text and video-text data leads to better scores on Audiocaps, compared to the model pretrained on text. Interestingly, the model pretrained on both modalities attain the best scores. This support our underlying hypothesis. We also show the importance of multitask pretraining, by comparing two models trained on image-text tasks; one with single task on CC3M and CC12M (12.8M examples) and another one with multitask on COCO, VG, SBU, CC3M, VQAv2 and RefCOCO+ (5.6M examples). The results validates again the importance of multitasking in generalization to new modalities/tasks.




    

     
    



    


\paragraph{Better initialization for new tasks: from multimodal input to multimodal output.} Here, we investigate if our pretrained model can be a good initialization to add new tasks. We experiment with a more challenging scenario; text-to-image generation. We finetune the model with different initialization on the COCO dataset and report the CLIP score \citep{wu2022nuwa}. Tab.\ref{tab:image_generation} shows that pretraining on either image-text or video-text data helps to get additional improvement, with more improvement coming from pretraining on image-text tasks.


\section{Weight Interpolation of \unival{} Models} %

Previously, we showed the synergy between tasks and modalities that results from multitask pretraining. Here, instead, we use interpolation in the weight space to leverage this synergy.
We follow the literature on weight interpolation \citep{izmailov2018,wortsman2022modelsoups,rame2022diversediwa} to merge models finetuned on different multimodal tasks, without inference overhead.
Our framework is an ideal candidate for this investigation, due to the unified architecture and the shared pretraining \citep{neyshabur2020being}, which naturally enforces linear mode connectivity \citep{Frankle2020} and thus averegability (without requiring weight permutation as in \cite{ainsworth2022git}) across finetuned weights.
We consider 4 image-text tasks; Image Captioning (IC), VQA, Visual Grounding (VGround) and Visual Entailment (VE), and provide similar results for video tasks in Appendix \ref{app:results}.
Then, given two models with weights $W_1$ and $W_2$ finetuned on 2 different tasks among those 4 image-text tasks, we analyze the performance of a new model whose weights are $W = \lambda \cdot W_1 + (1-\lambda) \cdot W_2$, where $\lambda \in [0,1]$.
We propose to study the following questions.%

\paragraph{\textit{Scenario 1}: can we trade off between 2 different multimodal tasks by weight interpolation?}
In Fig.\ref{fig:wa_main} we plot the performances of the model whose weights are defined by interpolation ($\lambda \in [0,1]$) between different finetuned weights.
In more details, all weights were initialized from the same \enquote{pretrain init}, which performs badly on the considered tasks (grey star usually at the bottom left of the plots).
By vanilla finetuning (blue stars at the endpoints of the blue line) on a target task, we consistently improve results on the specific metric, yet at the cost of severe performance degradation on other tasks; this suggests that the different tasks are in tension, and optimizing one degrades another.
Then, by weight interpolation between these two vanilla finetuned endpoints, we reveal a convex front of solutions that trade-off between the different abilities, validating that \emph{we can effectively combine the skills of expert models finetuned on diverse multimodal tasks}.
Actually, it is even possible to find an interpolating coefficient $\lambda$ so that the interpolated model outperforms the specialized one: \emph{e.g.}, in the middle subplot, the CIDEr score of the model for $\lambda=0.8$ with weights $0.8 \cdot \theta_{Cap} + 0.2 \cdot \theta_{VQA}$ is 138.51 vs. 136.52 for the captioning expert $\theta_{Cap}$ corresponding to $\lambda=1$. We speculate the interpolated model benefits from the synergy between different tasks.


 




















% Figure environment removed%
Besides, we also experiment a more costly finetuning approach, ratatouille \citep{rame2022recyclingratatouille}, where each finetuning leverages the other tasks as auxiliary tasks.
For example, when considering VQA as the target task; (i) we first finetune weights on the auxiliary tasks (VGround, VE and IC); (ii) then we launch multiple finetunings on VQA from these auxiliary weights; (iii) we uniformly average all the weights finetuned on VQA to obtain $W_{VQA}^{r}$. Similarly, to obtain $W_{IC}^{r}$ specialized on IC, we apply the same recipe but this time the final finetunings are on the target task IC. Then, as shown on the left subplot from Fig.\ref{fig:wa_main}, we plot the performances for $W^{r} = \lambda \cdot W_{VQA}^{r} + (1-\lambda) \cdot W_{IC}^{r}$ where $\lambda \in [0,1]$.
The obtained (yellow) front is to the right and above the vanilla (blue) front, validating the superior performances of ratatouille.












% Figure environment removed





\paragraph{\textit{Scenario 2}: given $N$ models trained on different multimodal tasks, can we obtain a single model that is good on seen, and new unseen tasks?}
We consider $N=4$ models with weights $\{W_i\}_{i=1}^N$ finetuned independently, one for each task. This time, we simply average them uniformly: $\lambda=1/N$, and then we consider the weight average (WA) $\frac{1}{N} \sum_{i=1}^N W_i$ and plot its performance in Fig.\ref{tab:zeroshot_wa_merging}.
First, we observe that this vanilla WA outperforms the \enquote{pretrain init}, evaluated on training tasks (in-distribution or ID setting) but also when evaluated on new tasks (out-of-distribution or OOD setting), without any additional training.
Performances are improved when averaging uniformly ratatouille finetunings $\frac{1}{N} \sum_{i=1}^N W_i^r$.
We also consider the fusing \citep{choshen2022fusing} strategy, which considers the average the auxiliary weights $\frac{1}{N} \sum_{i=1}^N W_i$ as the initialization for a second step of finetunings on the target tasks to obtain $\{W_i^f\}_{i=1}^{N}$, and then report the performance for $\frac{1}{N} \sum_{i=1}^N W_i^f$; fusing performs better than vanilla fine-tuning in ID, but not OOD.
When comparing fusing and ratatouille, they perform similarly ID; yet, in the OOD setting, ratatouille outperforms fusing, validating the results from \citet{rame2022recyclingratatouille}.
In conclusion, these experiments show that uniform averaging can merge different finetuned models to get one general model that performs well on all seen and unseen tasks.
Though, results could be improved with tailored selection of the interpolation coefficients $\lambda$.





























\section{Discussion}

\paragraph{Limitations and discussion.} Despite the good quantitative results, we find that \unival{} suffers from several limitations. 
First, \unival{} can \textbf{hallucinate}. Specifically, it may generate new objects in image descriptions (object bias \citep{rohrbach2018objecthallucination}) prioritizing coherence in its generation rather than factuality. In the case of VQA, the model can generate plausible response that are not directly evident in the given image. A similar challenge arises in visual grounding, where \unival{} may ground objects that are not mentioned in the text or not present in the image. Nonetheless, in comparison to other large models like Flamingo \citep{alayrac2022flamingo}, we show in Appendix \ref{app:discussion} that \unival{} demonstrates a reduced inclination towards hallucinations. This distinction can be attributed to using smaller LM, a component that is known to be particularly susceptible to this issue when scaled. Second, it struggles in \textbf{complex instruction following.} We have observed that the model's performance is suboptimal when confronted with intricate instructions, such as identifying a specific object in the presence of similar alternatives, detecting small or distant objects, and recognizing numerals. In Appendix \ref{app:discussion}, we provide a detailed discussion on the limitations (\emph{e.g.}, hallucinations, abstention and other biases, instruction following and efficient finetuning) and interesting future directions (\emph{e.g.}, scaling, adding more modalities, embodiment, and better training schemes). 

\paragraph{Conclusion.} In this study, we introduce \unival{}, the first unified model capable of supporting image, video, and audio-text tasks. We achieve this with a relatively small model with $\sim$ 0.25B parameter on dataset of relatively small sizes. Our unified system, pretrained with multitasking, offers several advantages. It harnesses the synergies between diverse tasks and modalities, enables more data-efficient training, and exhibits strong generalization capabilities to novel modalities and tasks. The unification aspect of our strategy paves the way to interesting techniques to merge models finetuned on different multimodal tasks: we demonstrate that, in addition to multitask pretraining, merging by weight interpolation can further exploit the tasks diversity.
Ultimately, we aspire that our work inspires the research community and accelerates the progress toward constructing modality-agnostic generalist assistant agents.

\section{Acknowledgments}
This work was supprted by HPC resources of CINES and GENCI. The authors would like to thank the staff of CINES for technical support in managing the Adastra GPU cluster, in particular; Jean-Christophe Penalva, Johanne Charpentier, Mathieu Cloirec, 
Jerome Castaings, Grard Vernou, Bertrand Cirou and Jos Ricardo Kouakou.
This work was also partly supported by ANR grant VISA DEEP (ANR-20-CHIA-0022).

\newpage
\appendix

\section*{Appendix}

The Appendix is organized as follows:
\begin{itemize}
    \item Section \ref{app:model_card}: model card.
    \item Section \ref{app:related}: detailed discussion about related work.
    \item Section \ref{app:arch}: details about model architecture.
    \item Section \ref{app:pret_tasks}: image and video-text pretraining tasks.
    \item Section \ref{app:mcl}: illustration and details about multimodal curriculum learning.
    \item Section \ref{app:details}: datasets and implementation details.
    \item Section \ref{app:peft}: finetuning only the linear connection (Parameter-Efficient Finetuning).
    \item Section \ref{app:ablation}: ablation study including knowledge transfer across modalities and training efficiency.
    \item Section \ref{app:results}: additional quantitative results.
    \item Section \ref{app:discussion}: discussion of several limitations and future directions.
    \item Section \ref{app:qual}: qualitative results of several image-text tasks.
\end{itemize}


\section{Model Card}
\label{app:model_card}

In the following table, we detail our model card \citep{mitchell2019model}.



\begin{center}
\begin{longtable}{p{0.35\linewidth} | p{0.6\linewidth}}
    
    \toprule
    \noalign{\vskip 2mm}
    \multicolumn{2}{c}{\textbf{Model Details}} 
    \vspace{2mm}\\
    \toprule
    Model Date & July 2023 \\
    \midrule
    Model Type & Transformer encoder-decoder pretrained on text and trained end-to-end to be conditioned on image, video and audio input. Modality-specific encoders are based on convnets and pretrained from classification on public benchmarks. All input tokens are concatenated and fed to the encoder. The text generation is conditioned on other modalities via cross-attention.
    (See Section for details.)  \\
    \vspace{1mm} \\
    
    \toprule
    \noalign{\vskip 2mm}
    \multicolumn{2}{c}{\textbf{Intended Uses}} 
    \vspace{2mm} \\
    \toprule
    Primary Intended Uses &

    The primary use is research on unified multimodal models that span a wide range of applications such as; image/video/audio captioning, image/video question answering, grounding/detection and image generation. In addition, the study of the limitation and biases of such kind of model, and novel approach for efficient training and adaptation. Other similar multimodal applications can also be considered, like multimodal dialogue, and text-guided robotics applications.
    \\
    \midrule
    Primary Intended Users &  The research community. The model will be made public. \\
    Out-of-Scope Uses &

    Any downstream applications that can cause harm to society, or without mitigation of associative safety measures.
    
    \\
    
    \toprule
    \noalign{\vskip 2mm}
    \multicolumn{2}{c}{\textbf{Factors}} 
    \vspace{2mm} \\
    \toprule
    Card Prompts -- Relevant Factor &
    The model is trained on english and based on BART \citep{lewis2020bart} language model. The model should not be used any downstream application without propoer factor analysis.
     \\
    \midrule
    Card Prompts -- Evaluation Factors &
      The model inherits the biases and risks of the pretrained language model \citep{lewis2020bart}. It may also hallucinates some information not present in the conditioned modality. On some tasks we constraints the text generation to predifined set of answers, however, generally, there is no mechanism that force it to not produce toxic or racist output on all tasks.
    \vspace{1mm} \\
    
    \toprule
    \noalign{\vskip 2mm}
    \multicolumn{2}{c}{\textbf{Metrics}} 
    \vspace{2mm} \\
    \toprule
    Model Performance Measures &

The performance using standard metrics to evaluate the model performance on several public benchmarks, such as; Visual Question Answering (accuracy on VQAv2, OKVQA, ,VizWiz, MSVD-QA and MSRVTT-QA), Visual Grounding (IoU>0.5 on RefCOCO, RefCOCO+ and RefCOCOg), Image Captioning (CIDEr, METEOR, BLEU, SPICE on MSCOCO, MSR-VTT, Audiocaps and Clotho v1) and Text to Image Generation (CLIP score on MSCOCO). 
\\
    \midrule
    Decision thresholds & N/A \\
    \midrule
    Approaches to Uncertainty and Variability &
    The relatively costly pretraining prevent from doing several runs, however the different ablation study and the evaluation on many datasets validate the overall performance of the model.
    \vspace{1mm} \\
    
    \toprule
    \noalign{\vskip 2mm}
    \multicolumn{2}{c}{\textbf{Evaluation Data}} 
    \vspace{2mm} \\
    \toprule
    Datasets & Check Tab.~\ref{tab:downstream} for more details. \\
    \midrule
    Motivation &
    The datasets span different standard benchamrks across image, video and audio modalities. This show the overall capability of the model to process different modalities.\\
    \midrule
    Preprocessing &
    Text is process with BPE tokenizers, audio is transformer to mel spectorgram and we randomly sample some frames from videos. Some addition data augmentation techniques are used during training. 
    \vspace{1mm} \\

    \toprule
    \noalign{\vskip 2mm}
    \multicolumn{2}{c}{\textbf{Training Data}} 
    \vspace{2mm} \\
    \toprule
    Datasets & We only use public datasets, such as image captioning (COCO \citep{lin2014microsoftcoco}, Visual Genome (VG) \citep{krishna2017visualgenome}, SBU \citep{sbu}, CC3M \citep{Sharma2018ConceptualCA} and CC12M \citep{changpinyo2021conceptual} (only in the first stage)), VQA (VQAv2 \citep{goyal2017makingvqav2}, GQA \citep{hudson2019gqa}, VG \citep{krishna2017visualgenome}), Visual Grounding (VGround) and referring expression comprehension (RefCOCO, RefCOCO+, RefCOCOg \citep{yu2016modelingrefcoco+}), video captioning (WebVid2M \citep{bain2021frozenwebvid}) and video question answering (WebVidQA \citep{yang2021justask}). We only use the training sets during pretraining.
    \vspace{1mm} \\

    \toprule
    \noalign{\vskip 2mm}
    \multicolumn{2}{c}{\textbf{Quantitative Analyses}}
    \vspace{2mm}\\
    \toprule
    Unitary Results & Our unified model is competitive to state of the art approaches customized for less modalities. It attains state of the art results on Visual Grounding and Audio Captioning. Please check Sec.\ref{sec:experiments_sota} for more details.
    \\ 
    \midrule
    Intersectional Results & N/A. 
    \vspace{1mm} \\

    \toprule
    \noalign{\vskip 2mm}
    \multicolumn{2}{c}{\textbf{Ethical Considerations}} 
    \vspace{2mm}\\
    \toprule
    Data & 
    We use only public benchmarks, however some benchmarks are not filtered from racist, sexist or otherwise harmful content. \\
    \midrule
    Human Life &
    The model is not intended to be used for safety critical applications. \\
    \midrule
    Mitigations &
    Constrained text generation can be adapted for some tasks. However, for open-ended generation post processing or some engineered prompts might mitigate some of the biases. Overall, filtering the pretraining data can be ver effective approach.
    \\
    \midrule
    Risks and Harms & 
    We use public datasets. Not all of them are filtered from from toxic and personal data. \\
    \midrule
    Use Cases &
    Forcing the model (finetuning or prompting) to generate harmful or racist text. Other use cases regarding general language models are also relevant.\\
    
    \bottomrule
    
    \caption{\footnotesize \textbf{\unival{} Model Card.} We follow the framework of \citep{mitchell2019model}.}
    \label{tab:model_card}
\end{longtable}
\end{center}


\section{Related Work}
\label{app:related}
\paragraph{Unimodal pretraining}
Pretraining on large uncurated datasets has been a substantial ingredients in the vision and NLP communities to develop powerful models that generalize to a wide range of tasks. For vision models, supervised \citep{touvron2021trainingdeit, dehghani2023scalingvit22b} and self supervised \citep{chen2020simple_simclr, caron2020unsupervised, zbontar2021barlow, he2022maskedmae} techniques have extensively investigated , while for NLP, the widely used training objective is next token prediction \citep{brown2020languagegpt3, hoffmann2022trainingchinchilla, touvron2023llama}.

Recently, these domains started to converge on a simple training paradigm; joint scaling of the pretraining data, model size and compute, while using a unified architecture and training objective. Surpassing a certain scaling threshold has elicited new emergent capabilities, especially in LLMs \citep{brown2020languagegpt3, chowdhery2022palm}, that allows such models to solve new reasoning tasks that were out of reach few years ago. Once such models are available, they can be seamlessly adapted without retraining, via prompting such zero-shot or few-shot In Context Learning. Scaling vision transformer models \citep{dehghani2023scalingvit22b} lead to be more robust and aligned to human object recognition.

While being very successful, training such models is hard, extremely costly and need dedicated infrastructure. However, the public release of many of these models allow to leverage them for variety of tasks.
In this work we leverage unimodal pretrained models for multimodal tasks.





\paragraph{Multimodal pretraining.}

So far, most of the effort to build multimodal models have been focused on vision-language  pretraining. Contrastive based approaches \citep{radford2021learning, jia2021scaling_align} try to learn shared and aligned latent space by training on hundred of millions of data. More data efficient approaches \citep{vicha, li2021alignalbef, li2022blip, meter, singh2022flava}, have relied on additional multimodal interaction modules and variety of training objectives such as image-text matching, masked language modeling and image-text contrastive \citep{chen2020uniter, kim2021vilt, lu2019vilbert, zhang2021vinvl}.  In the video-language community, similar approaches have been mildly adapted to model the interaction between language and frames sequences \citep{cheng2022vindlu, wang2022allinone, fu2021violet, zellers2021merlot, yang2021justask}. Few work have targeted both image and video language pretraining \citep{wang2022omnivl}. 

These works have been following the scaling trend as in unimodal pretraining. Scaling the model went from couple of billions of parameters \citep{yu2022coca, wang2022imagebeit3, wang2022git} to tens of billions \citep{chen2022pali, alayrac2022flamingo}. 





 
    


\paragraph{Unified models}
Building unified systems has been triggered first in the NLP community. \citep{raffel2020exploringt5} proposed the T5 transformer model, a text-to-text framework, where the same pretrained model is used to solve many NLP tasks, each one is described by task-specific textual prefix. Since then, building general textual models has been heavily investigated by LLMs \citep{brown2020languagegpt3, rae2021scalinggopher, chowdhery2022palm}. The success of unified Language models, have inspired other communities. In the vision community, \citep{chen2022unifiedpix2seqall} proposed a pixel-to-sequence framework to unify different vision tasks such as object detection and instance segmentation. For multimodal tasks, \citep{cho2021unifyingvlt5} proposed to unify vision-language tasks, including discriminative ones, as conditional text generation. This was followed by \citep{yang2021crossingunitab}, which targets also grounded tasks and does not rely on an object detection model. OFA \citep{wang2022unifyingofa} then proposed a large scale sequence-to-sequence framework, and extended previous approaches to more image-text tasks, including text to image generation. Similarly, Unified-IO \citep{lu2022unifiedio}, in addition to image-text tasks, targets many visual tasks including dense prediction such as depth estimation and image segmentation. 
The most closest to us is the work of OFA and Unified-IO, however, we propose to unify tasks across many modalities, and use smaller model and dataset sizes.








\paragraph{Efficient multimodal learning}
The current paradigm in training multimodal models is to train all model parameters, even when using pretrained models \citep{chen2022pali, wang2022unifyingofa, li2022blip}. Despite attaining SoTA, these approaches are extremely costly to train. To overcome this, recent approaches showed that pretrained models, generalize well to multimodal tasks, where it is possible to use a frozen LM with a powerful multimodal encoder such as CLIP, and train only a handful of parameters, such as the vision encoder \citep{eichenberg2021magma}, the vision connector \citep{merullo2022linearlylimber, manas2022mapl, koh2023groundingfromage, li2023blip2} or additionally the Adapters \citep{eichenberg2021magma, yang2022zerofrozenbilm}. This paradigm was then generalized in \citep{shukor2023epalm}, to other modalities, such video and audio, where the authors showed that it is even possible train only a linear projection layer to adapt pretrained unimodal encoder (\emph{e.g.}, pretrained on ImageNet) and a language decoder to do multimodal tasks.  

Another line of research, is data-efficient approaches, recent work shows that it is possible to get comparable results by training on significantly less data, by designing better training objectives \citep{vicha}, data augmentation \citep{li2021supervisiondeclip} and curriculum learning \citep{srinivasan2022curriculum}. In this work, we focus on parameter-efficient finetuning, especially, training only the linear connection.





\paragraph{Weight interpolation and mutltimodal tasks.}
Our strategy enable the training of multiple expert models with diverse specializations. To combine them, we leverage a simple yet practical strategy: \emph{linear interpolation in the weight space}, despite the non-linearities in the network's architecture. This weight averaging (WA) strategy is in line with recent findings on linear mode connectivity \citep[LMC]{Frankle2020,neyshabur2020being}: weights fine-tuned from a shared pre-trained initialization remain linearly connected. 
Recent works \citep{2022arXiv221204089I,daheim2023elastic,ortiz2023task} suggest that averaging networks in weights can combine their abilities without any computational overhead; for instance, the average of an English summarizer and an English-to-French translator will behave as a French summarizer \citep{jang2023exploring}.
Model soups approaches \citep{wortsman2022modelsoups,rame2022diversediwa} improve out-of-distribution generalization and show that weight averaging actually approximates predictions averaging \citep{Lakshminarayanan2017} when the LMC holds.
The LMC was extended to weights fine-tuned with different losses \citep{rame2022diversediwa,croce2023seasoning,rame2023rewarded} or on different datasets \citep{matena2022merging,ilharco2022patching,choshen2022fusing,choshen2022cold,rame2022recyclingratatouille,dimitriadis2022pareto}.
Moreover, several other merging approaches \citep{matena2022merging,yadav2023resolving} have been proposed, though with arguably minor empirical gains over the simpler linear interpolation. For example, \citep{matena2022merging} considers the Fisher information; \citep{yadav2023resolving} resolve updates conflicts across weights. Neuron permutations strategies \citep{entezari2022the, ainsworth2022git, jordan2022repair} address the ambitious challenge of enforcing connectivity across weights with different random initializations, though so far with moderate empirical results. Most of exisiting WA approaches consider very similar tasks, such as image classifications from different datasets or text classification/generation. Interpolating weights of models finetuned on different multimodal tasks, is little investigated, with no work exploring this technique in multimodal foundation models. The most similar and concurrent work is the recent \citet{sung2023empirical} applying a complex architecture-specific merging strategy involving weight averaging for models pretrained on different modalities. Another difference, is that we explore WA for multimodal downstream tasks.






\section{Model Architecture}
\label{app:arch}
% Figure environment removed

To tackle multimodal tasks at small to mid-scale, we employ an encoder-decoder LM \citep{vaswani2017attention, lewis2020bart} (shown in Fig.\ref{fig:unival_arch}), as its effectiveness for multimodal tasks has been demonstrated compared to decoder-only models \citep{wang2021simvlm}, and their superiority in zero-shot generalization after multitask training \citep{wang2022language}.
The encoder consists of stack of blocks of Self-Attention (SA), Layer Normalization (LN), GELU activations and Feed Forward Network (FFN) layers. The decoder blocks contains additionally cross-attention (CA) layers to attend to the encoder last layer tokens. Specifically, the output tokens of the encoder are considered as keys and values in the CA, while the text generated in the decoder is considered as queries. Following other approaches \citep{wang2022unifyingofa}, and to stabilize the training, we add LN layers after the SA and the FFN, and head scaling to the SA. We use independent absolute and relative position embeddings for text, images, videos and audios. We add different modality token embeddings to distinguish text from other modalities. The model parameters are initialized from BART-base model \citep{lewis2020bart}.

For each modality, we use light-weight convolution architectures (\emph{e.g.}, the encoders in orange and green in Fig.\ref{fig:unival_arch}). For images, we follow other work \citep{wang2021simvlm, wang2022unifyingofa} and use ResNet-101 trained on ImageNet. For videos, we use 3D ResNext-101 \citep{hara2018can} trained on Kinetics-400 \citep{kay2017kinetics}, and for audio, we use PANN-CNN14 \citep{kong2020panns} trained on AudioSet \citep{audioset}. We do not skip the last block in the encoders \citep{wang2022unifyingofa}, as we find that it reduces the number of tokens and accelerate the training (see Tab.\ref{tab:ab_eff}). 

Each modality is encoded in the modality projection (for text we use linear embedding layer), and then concatenated to form a sequence of tokens (\emph{e.g.}, textual and visual) before being  passed to the encoder (for some tasks such as VQA, we pass also the question to the decoder). After encoding, the output of the encoder interact with the decoder via cross-attention. The decoder generates the response auto-regressively starting from a special BOS token. 





\section{Pretraining Tasks}
\label{app:pret_tasks}
We pretrain \unival{} on the following image/video-text tasks:

\paragraph{Image Captioning.} The model takes as input an image and "what does the image describe?" as text and generate a textual description of the image.

\paragraph{Visual Question Answering (VQA).} The model takes as input an image and a question and generates a textual answer based on the image.

\paragraph{Visual Grounding (VGround.).} The model takes an image and "Which region does the <text>  describe?" as text and the model generates the coordinates of the bounding box described by the <text>. 

\paragraph{Grounded Captioning (GC).} This is similar to image captioning, but the model should generate a description of a specific region in the image. Specifically, the model takes an image and "what does the region describe? region: <x1, y1, x2, y2>" as text and generates a caption of the region. <x1, y1, x2, y2> are coordinates of the region bounding box.

\paragraph{Image-Text Matching (ITM).} The model takes an image and a text and should predict if the text corresponds to the image. For a given image we randomly sample a caption as negative text and consider the original caption as positive. The input text is "Does the image describe <text>?" and the output is either "Yes" or "No".

\paragraph{Video Captioning.} Similarly to image captioning, the model takes a video and "what does the video describe?" and generates a video description.

\paragraph{Video Question Answering (VideoQA).} The model takes a video and question and should answer the question based on the video.

\paragraph{Video-Text Matching (VTM).} The model should predict if a text corresponds to a given video or not. 



\section{Multimodal Curriculum Learning}
\label{app:mcl}
Training on many tasks and modalities is computationally expensive, especially when considering long videos ore audios. To overcome this, we propose a multistage curriculum training approach (depicted in Fig.\ref{fig:curriculum}) in which we progressively add more modalities. In stage 1, the model is trained on large corpus of text following typical next token prediction or other LM training. Thanks to the many open sourced pretrained language models, it is easier to leverage and initialize from existing LMs (\emph{e.g.}, BART \citep{lewis2020bart} as in our case). In stage 2, the model is trained on many tasks of images and texts. Afterwards,  video-text datasets are added and the model is trained on both image-text and video-text data. This is a general paradigm to efficiently train multimodal models on many modalities. Training on many tasks is more efficient, however, the standard training on image-text alignment on image captioning can be also considered. Note that, to keep good performance on unimodal tasks, it is better to add also unimodal data.


While this training scheme is more efficient than training on all data from the beginning, using more efficient approaches from the continual learning community \citep{wang2023comprehensivecontinual} is extremely useful in this context, to limit the number of examples as we add more modalities, especially if the objective is to obtain modality agnostic models. Training only on the new modalities will make the model forget about previous ones.











\section{Data and Implementation Details}
\label{app:details}



\begin{table*}[h]
\centering
\caption{Downstream tasks and datasets. We show the size of different splits used in our work. }
  \label{tab:vidqa}
  \small
  \resizebox{0.8\linewidth}{!}{
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lccc@{}}
    \toprule
    \textbf{Dataset} & \textbf{Modality} & \textbf{Task} & \textbf{Size (Train/Val/Test)} \\
    \midrule
    COCO \citep{lin2014microsoftcoco} & Image-Text & Image Captioning & 113K/5K/5K  \\
    nocaps \citep{agrawal2019nocaps} & Image-Text & Image Captioning & --/4.5K/-- \\
    VQAv2 \citep{goyal2017makingvqav2} & Image-Text & VQA & 443K/214K/453K\\
    OKVQA \citep{okvqa} & Image-Text & VQA & --/5K/-- \\
    VizWiz \citep{gurari2018vizwiz} & Image-Text & VQA & --/4.3K/-- \\
    SNLI-VE \citep{xie2019visualsnli} & Image-Text & Visual Entailment & 30K/1K/1K \\
    RefCOCO \citep{yu2016modelingrefcoco+} & Image-Text & Visual Grounding & 120K/6K/5K\\
    RefCOCO+ \citep{yu2016modelingrefcoco+} & Image-Text & Visual Grounding & 120K/6K/5K \\
    RefCOCOg \citep{yu2016modelingrefcoco+} & Image-Text & Visual Grounding & 80K/5K/10K\\
    COCO \citep{lin2014microsoftcoco} & Image-Text & Text to Image Generation & 80K/64K/30K \\
    \midrule
    MSR-VTT \citep{Xu_2016_CVPR_msrvtt} & Video-Text & Video Captioning & 6.5K/0.5K/3K \\
    ActivityNet-Caption \citep{krishna2017densedcev} & Video-Text & Video Captioning & 37.5K/--/17K \\
    MSRVTT-QA \citep{msvd_msrvtt} & Video-Text & VideoQA & 156K/12K/70K \\
    MSVD-QA \citep{msvd_msrvtt} & Video-Text & VideoQA & 30K/6K/12K \\
    \midrule
    Audiocaps \citep{audiocaps} & Audio-Text & Audio Captioning & 47K/0.5K/1K \\
    Clotho v1 \citep{drossos2020clotho} & Audio-Text & Audio Captioning & 17.5K/1K/-- \\   
  \bottomrule
\end{tabular*}
}
\label{tab:downstream}
\end{table*}



\subsection{Implementation details of downstream tasks.}
For image-text tasks, we keep the hyperparameters during finetuning close to those in OFA \citep{wang2022unifyingofa}. The downstream datasets are detailed in Tab.\ref{tab:downstream}.

\paragraph{VQA.} We finetune on VQAv2 dataset and cast the task as text generation. The model is trained for 5 epochs with a batch size of 256 using Adam optimizer. We use a learning rate of $1e-4$ with linear decay and label smoothing of 0.1. The image resolution is increased to 480 and we use exponential moving average with 0.9999 decay. We use Trie based search to constraint the generated answers to the top 3.1k answers. We freeze the encoder and decoder embeddings during finetuning. The question is passed to both the encoder and decoder as prompt.

\paragraph{Image Captioning.} We finetune on MSCOCO karpathy split and report standard captioning metrics. The model is trained for 4 epochs with a batch size of 128. The image resolution is set to 480 and the learning rate to $1e-5$ with linear decay. We use an encouraging \citep{zhao2022wellencouraging} cross entropy loss with label smoothing of 0.1. We freeze the encoder and decoder embeddings during finetuning.

\paragraph{Visual Grounding.} We finetune on RefCOCO, RefCOCO+ and RefCOCOg for 8 epochs with batch size of 256. The images are resized to 512 and the learning rate start with $5e-5$ and decreases linearly. We train with cross entropy and label smoothing of 0.1. We limit the generation length to 4 and report the Acc@0.5.

\paragraph{Visual Entailment.} The model is trained for 4 epochs with batch size of 256 and learning rate of 5e-5 that deacreases linearly. The image resolution is set to 480. The model takes only the image and the text hypothesis, without the text premise, and the generation is constrained to yes/maybe/no using Trie-based search. The text is passed to both the encoder and decoder as prompt.


\paragraph{VideoQA.} The model is trained for 25 epochs on MSRVTT-QA and 40 epochs on MSVD-QA with a batch size of 128 and learning rate of $1e-4$ that decreases linearly. We sample randomly 8 frames with resolution 384. We train with cross entropy with encouraging loss and label smoothing of 0.1. We use exponential moving averaging model pass the question to both the encoder and the decoder. The answer generation is constrained to the set of possible answers via Trie-based search. We freeze the encoder and decoder embedding layers. 


\paragraph{Video Captioning.} We train on MSR-VTT for 15 epochs and a batch size of 256 with a starting learning rate of $1e-5$ that decreases linearly. We randomly sample 16 frames with resolution 384 and train with an encouraging cross entropy loss and label smoothing of 0.1. We freeze both the encoder and the decoder embedding layers.  

\paragraph{Audio Captioning.} We train for 10 epochs on Audiocaps and Clotho v1 with a batch size of 128 and starting learning rate of $1e-4$ ($5e-5$ for clotho v1). The mel bins is set to 64 and the hop size to 200. We train with encouraging cross entropy loss with label smoothing of 0.1 and freeze the encoder and decoder embedding layers. 

\paragraph{Text-to-Image Generation.} We follow previous work \citep{wang2022unifyingofa} and finetune the model on the train set of MSCOCO and evaluate on 30K images from its validation set. We start by training with cross-entropy loss for 50K steps and batch size of 512 ($\sim$ 60 epochs) and lr 1e-3, followed by CLIP score optimization for 5K steps and batch size of 128 and lr 1e-6. When evaluating the model we select the best image, among 24 generations based on CLIP score. We report Inception score (IS) \citep{salimans2016improved}, Frchet Inception Distance (FID) \citep{heusel2017gans} and CLIP simliarity score (CLIPSIM) \citep{wu2022nuwa}.




\section{Parameter Efficient Fine-Tuning (PEFT): Training only the Linear Connection.}
\label{app:peft}


\begin{table}[h]
\centering
  \resizebox{0.9\linewidth}{!}{
  
    \begin{tabular}{lccccccc}
    
    \toprule
    Method & PT modality & Model size & COCO & VQA v2 val & MSR-VTT & MSRVTT-QA  & Audiocaps \\
    \midrule
    PromptFuse \citep{liang2022modularpromptfuse} & Text & 0.22B & -  & 34.1  & -    & - & -  \\
     FrozenBiLM \citep{yang2022zerofrozenbilm} & Video-Text & 0.89B & -  & -  & -    & \textcolor{gray}{47.0} & -  \\
     eP-ALM \citep{shukor2023epalm} & Text & 2.8B & 97.2  & 53.3  &  50.7 & 36.7   &  63.6 \\ \midrule
     \unival{} (ours) & Image-Text (S1)  &  0.25B  &  \textcolor{gray}{129.8}    &  \textcolor{gray}{71.6} & 39.8 & 19.1 & 47.5 \\
     \unival{} (ours) & Image+Video-Text (S2)  & 0.25B & \textcolor{gray}{132.7}  & \textcolor{gray}{71.6}  & \textcolor{gray}{51.8} & \textcolor{gray}{33.6} &  49.5 \\
    \bottomrule
\end{tabular}
}
\caption{\footnotesize \textbf{Finetuning only the linear connection on different image/video/audio-text tasks.} Despite the significantly smaller size of \unival{}, the model can achieve reasonable performance when finetuned on new modalities. Scores in gray are for models pretrained on the same target modality.}
\label{tab:peft_linear}
\end{table}

Once we have powerful pretrained models, it becomes important to develop highly efficient approaches that can be adapted to various tasks and modalities. Recent studies \citep{shukor2023epalm, merullo2022linearlylimber} have demonstrated the possibility of efficiently adapting unimodal pretrained models to multimodal tasks, by training \textit{only a linear layer}. The key idea is to project modality-specific tokens onto the input text space of a language model, effectively transforming them into textual tokens, while keeping all the pretrained parameters frozen. While this approach has proven effective with large models containing billions of parameters, in this section, we explore this setup with smaller models comprising several hundred million parameters. Following \unival{} pretraining, we train only the linear projection responsible for mapping the output of the modality-specific encoders to the input of the LM encoder. 

As shown in Tab.\ref{tab:peft_linear}, \unival{} achieves reasonable performance on new tasks and modalities despite the smaller parameter count. However, these results suggest that achieving competitive performance with only the linear connection may require larger models or training on larger datasets.




\section{Ablation Study}
\label{app:ablation}

\begin{table}[h]
\centering

    \resizebox{0.6\linewidth}{!}{
    
    \begin{tabular}{c|ccccc}
    \toprule
    Pretrain Modality &  COCO & VQA v2 & RefCOCO+ & MSR-VTT & MSRVTT-QA   \\
    \midrule
    \xmark & 37.9 & 62.1 & 6.4 & 47.7 & 23.0 \\
     I   &  128.0 & 73.1  & 70.5  & 47.3    & 29.0  \\
     V   & 96.6  & 68.4  & 24.3  &  54.5   & 41.9   \\
     I+V   & 128.0  & 73.2 & 70.2 &  56.3 &  42.3  \\
     
    
    \bottomrule
\end{tabular}
}
\caption{\footnotesize \textbf{Knowledge transfer across modalities.} Training on images helps significantly the video tasks. However, training on videos does seem to have a significant effect on image tasks.
}
\label{tab:trans_modal}
\end{table}


\paragraph{Knowledge transfer across modalities.} Here we investigate the knowledge transfer between modalities, in other words, how learning a new modality can affect the performance of the model on other modalities. We test the following hypothesis; \textit{pretraining on more modalities should improve the overall performance on all tasks}.

Tab.\ref{tab:trans_modal} shows that in general learning a new modality, improves the performance on other modalities. Besides, it significantly helps to solve the downstream tasks of the same modality. Compared to model initialized from scratch, training solely on image-text datasets help VideoQA. In addition, training on video-text datasets (V) significantly helps image-text tasks on VQAv2, COCO and RefCOCO+. Finally, training on both image and video-text datasets improve the performance on video-text task (w.r.t to pretraining on video) and did not degrade the performance on image-text tasks.


\paragraph{Efficiency during training.}
Another important aspect of our approach is the significantly shorter training time. In Tab.\ref{tab:ab_eff}, we compare the training time (finetuning for one epoch) with the previous unified model OFA \citep{wang2022unifyingofa}. Compare to OFA, our training time is significantly reduced, especially with tasks requiring high image resolution (\emph{e.g.}, 512$\times$512 with RefCOCO+). This is mainly due to the small number of visual tokens passed to the LM, that results from using additional convolution block in the image encoder.

\begin{table}[h]
\centering
    \resizebox{0.35\linewidth}{!}{
    \begin{tabular}{l|ccc@{}}
    \toprule
    \textbf{Method} & COCO & VQA v2 & RefCOCO+    \\
    \midrule
    
     OFA  &  5.7 & 11.5 & 1.3       \\ %
     \unival{}  & 3.1   & 8.0 & 0.7     \\

    
    
    \bottomrule
\end{tabular}
}
\caption{\footnotesize \textbf{Finetuning time in GPUh for one epoch training.} \unival{} is significantly more efficient than OFA, especially with tasks using high image resolution.}
\label{tab:ab_eff}
\end{table}

\section{Additional Results}
\label{app:results}

\subsection{Text-to-image generation} 
\begin{table}[h]
\center
\resizebox{0.6\linewidth}{!}{
\begin{tabular}{@{}lcccccc@{}}
\toprule
  Model & Model Size 
  & Pretrain
  & FID$\downarrow$  & CLIPSIM$\uparrow$ & IS$\uparrow$
  \\
\midrule
  DALLE 
  \citep{ramesh2021zero}
  & 12B
  & \cmark
  & 27.5
  & -
  & 17.9

  \\
  CogView 
  \citep{ding2021cogview}
  & 4B
  & \cmark 
  & 27.1 
  & 33.3
  &18.2
      \\
  GLIDE 
  \citep{nichol2022glide}
  & 3.5B
  & \cmark
  & 12.2
  & -
    & -
    \\
  Unifying 
  \citep{huang2021unifying}
  & 0.2B
  & \xmark
  & 29.9
  & 30.9
  &-
  \\
  N\"UWA
  \citep{wu2022nuwa}
  & 0.9B
  & \cmark
  & 12.9 
  & 34.3
  &27.2


  \\

  $\text{OFA}\rm_{Base}^\dagger$ \citep{wang2022unifyingofa}
  & 0.2B
  & \cmark
  & 13.9
  & 34.0
  & 26.7
  \\ \midrule
  \unival{} (ours) & 0.2B & \xmark & 15.4 & 33.6 & 25.7 \\
\bottomrule
\end{tabular}
}
\caption{Text-to-image generation on MSCOCO. Pretrain: image generation is included during pretraining.}
\label{tab:image_gen}
\end{table}

We finetune \unival{} on MSCOCO train set and compare the performance with other approaches. Tab.\ref{tab:image_gen}, shows that our model is competitive with previous approaches, despite being significantly smaller in size and does rely on image generation during pretraining. Compared to OFA, we have very close performance, especially w.r.t the CLIPSIM score.




\subsection{Linear interpolation of weights}

To complement our study from the main paper, we show in Fig.\ref{fig:wa_app} more results when interpolating weights finetuned on different multimodal tasks. These results (on both image-text and video-text tasks) confirm those previously reported in the main paper.

% Figure environment removed


\subsection{Finetuning for OOD generalization.}
Fig.\ref{tab:zeroshot_wa} explores the generalization abilities of different finetunings strategies after zero-shot evaluation.
We evaluate the OOD performances on 3 datasets that were not seen during pretraining nor during finetuning; OKVQA (a VQA task), VizWiz (a VQA task) and nocaps (an IC task).
We use the model trained on VQAv2 for OKVQA/VizWiz and on COCO Captioning for nocaps.
While fusing outperforms on VizWiz the vanilla finetuning (on VQAv2), it lags behind on the other 2 evaluation datasets.
Ratatouille, significantly outperforms both vanilla and fusing finetuning on all OOD datasets, which echos the observation from \cite{rame2022recyclingratatouille}. 
The reason being that features diversity (promoted by delayed averaging in ratatouille) increases robustness, and thus helps OOD generalization.






\begin{table}[h]
    \centering
    \resizebox{0.5\linewidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
      \multirow{2}*{Model}
      & OKVQA
      & VizWiz
      & NoCaps
      \\
      & Val Acc 
      & Val Acc & CIDEr (out-domain)  
      \\
    \midrule
    
    
    Vanilla &  38.06 & 13.57 &  94.39        \\
    Fusing & 35.12  & 15.63 &  93.58        \\
    Ratatouille & 38.97  & 18.48 &  95.28       \\
    
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Zero-shot evaluation of different finetunings strategy}.
    }
    \label{tab:zeroshot_wa}
\end{table}







 



    

\section{Discussion}
\label{app:discussion}
In this section we discuss some of the limitations and interesting future directions. 

\subsection{Limitations}

\paragraph{Hallucinations, abstention and other biases.} We find that our model suffers from different kind of \textit{hallucinations} (Fig.\ref{fig:qual_limitations}), however, similar to OFA, it is less inclined to hallucinate compared to other foundation models (Tab.\ref{tab:hall}), including larger models like Flamingo \citep{alayrac2022flamingo} (Fig.\ref{fig:hall_compare}). Reducing hallucinations remains an ongoing challenge within the research community, which has become more prominent with the emergence of large-scale multimodal models. While certain recent studies \citep{biten2022let, dai2022plausible} have proposed partial solutions to address this problem, an effective approach for mitigating hallucinations in large-scale pretrained models has yet to be established. Additionally, refraining from generating answers \citep{Dancette_2023_CVPR} or visual grounding can be promising directions to enhance factuality and diminish hallucinations. Nevertheless, despite the progress made by the research community, there is still much work to be done in this area. Other biases and limitations that are crucial to address, and have not been covered in our work are; social biases, toxic generation, and explainable generation. Some recent interesting works \citep{rame2023rewarded} can be considered to address some of these issues.

\begin{table*}[h]
\center
\begin{adjustbox}{max width=0.6\textwidth}
\begin{tabular}{@{\extracolsep{\fill}}lccc}
\toprule
Method & CIDEr$\uparrow$ & CHAIR$_S \downarrow$ & CHAIR$_I \downarrow$ \\
\midrule
  OSCAR$_{Base}$ \citep{li2020oscar} & 117.6 & 13.0 & 7.1 \\
VinVL$_{Larg}$ \citep{zhang2021vinvl} & 130.8 & 10.5  & 5.5 \\
  BLIP$_{Large}$ \citep{li2022blip}  & 136.70 & 8.8 & 4.7 \\
  \midrule
  OFA \citep{wang2022unifyingofa} & 75.27 & \textbf{4.36} & 3.98 \\
  UnIVAL  & \textbf{91.04} & 4.44 & \textbf{3.64} \\
  \midrule
  OFA Ft \citep{wang2022unifyingofa} & \textbf{138.1} & \textbf{3.06} & \textbf{2.03} \\
  UnIVAL Ft   & 137.0 & 3.26 & 2.20 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{\footnotesize \textbf{Hallucinations}. Comparison with different foundation models.  SoTA results from \citep{dai2023plausible}. }
\label{tab:hall}
\end{table*}

% Figure environment removed

\paragraph{Complex instructions following.} 
\unival{} exhibits good performance when presented with straightforward instructions commonly encountered in standard benchmarks. However, it encounters difficulties when faced with complex instructions, such as delivering intricate image descriptions or providing explanations for memes. To overcome this challenge, finetuning the model using a substantial number of diverse instructions can serve as a potential solution \citep{xu2022multiinstruct, liu2023visualllava, dai2023instructblip}.


% Figure environment removed






\paragraph{Unimodal tasks.} We noticed that training solely on aligned multimodal tasks can degrade the performance of the model in tackling unimodal ones. This problem is usually addressed by adding unimodal data, such as corpus of text or image, during pretraining \citep{singh2022flava, lu2022unifiedio, wang2022unifyingofa}.


\paragraph{Zero-shot evaluation and efficient finetuning.} The ideal scenario is for the model to demonstrate strong performance and generalization across multiple tasks following the pretraining phase. However, we have observed that refraining from finetuning or solely training the linear connection \citep{shukor2023epalm} results in unsatisfactory performance compared to SoTA approaches. This issue can be tackled by training larger models on a greater number of instructions/tasks or by employing alternative parameter-efficient finetuning techniques \citep{hu2021lora, lester2021powerprompttuning}.







    
\subsection{Future Directions} 

\paragraph{Model scaling and better LM initialization.} In this study, we conduct experiments using a relatively small BART-initialized encoder-decoder transformer. Nonetheless, numerous intriguing language models have recently been introduced \citep{raffel2020exploringt5, zhang2022opt, touvron2023llama}, which could potentially enhance performance when fine-tuned for multimodal tasks. Another aspect involves reasonably scaling the model size and training it on larger datasets, which could unveil more capabilities like In-Context Learning \citep{dong2022survey} and the ability to tackle more complex tasks \citep{lu2022learnscienceqa}.

\paragraph{More modalities and tasks.} Our study demonstrated the feasibility of training a unified model capable of addressing tasks involving image, video, audio, and text modalities. As a result, we posit that incorporating additional modalities, either during the pretraining phase or solely during finetuning, can be accomplished straightforwardly. Furthermore, expanding the scope of tasks within each modality, such as incorporating a broader range of visual tasks \citep{lu2022unifiedio, zou2023generalized} or tasks necessitating complex reasoning abilities \citep{liu2023visualllava}, represents a natural extension of this work. Ideally, we hope that in the future, there will be modality-agnostic models, bridging the gap between domains and modalities.

\paragraph{Towards embodied and generalist multimodal assistant agents.} Modality-agnostic models hold the potential to facilitate the development of embodied agents capable of addressing real-world challenges, including navigation and robotics manipulation, which demand the simultaneous handling of multiple modalities. Furthermore, while there has been notable progress in the NLP community regarding the construction of generalist agents, such as chatbots \citep{liu2023summary}, these advancements remain constrained in terms of their ability to accept diverse input modalities and generate outputs beyond textual form.


\paragraph{Better training schemes for multitask multimodal training.} While growing the number of tasks and modalities, it is important to devise new efficient training schemes to better leverage the collaboration between tasks, and continually support more modalities. We believe that there is more efficient approaches than our multimodal curriculum learning, to continually add more modalities while avoiding forgetting previous ones.
























% Figure environment removed



\section{Qualitative Results}
\label{app:qual}


% Figure environment removed


% Figure environment removed


% Figure environment removed




















\clearpage
\clearpage
\bibliography{main}
\bibliographystyle{tmlr}


\end{document}
