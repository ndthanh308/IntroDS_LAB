\begin{thebibliography}{161}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal et~al.(2019)Agrawal, Desai, Wang, Chen, Jain, Johnson, Batra,
  Parikh, Lee, and Anderson]{agrawal2019nocaps}
Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark
  Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson.
\newblock nocaps: novel object captioning at scale.
\newblock In \emph{ICCV}, 2019.

\bibitem[Ainsworth et~al.(2022)Ainsworth, Hayase, and
  Srinivasa]{ainsworth2022git}
Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa.
\newblock Git re-basin: Merging models modulo permutation symmetries.
\newblock In \emph{ICLR}, 2022.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
  Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
  et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Bain et~al.(2021)Bain, Nagrani, Varol, and
  Zisserman]{bain2021frozenwebvid}
Max Bain, Arsha Nagrani, G{\"u}l Varol, and Andrew Zisserman.
\newblock Frozen in time: A joint video and image encoder for end-to-end
  retrieval.
\newblock In \emph{ICCV}, 2021.

\bibitem[Biten et~al.(2022)Biten, Gomez, and Karatzas]{biten2022let}
Ali~Furkan Biten, Lluis Gomez, and Dimosthenis Karatzas.
\newblock Let there be a clock on the beach: Reducing object hallucination in
  image captioning.
\newblock In \emph{Winter Conference on Applications of Computer Vision}, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020languagegpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron2020unsupervised}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Caruana(1997)]{Caruana1997MultitaskL}
Rich Caruana.
\newblock Multitask learning.
\newblock \emph{Machine Learning}, 1997.

\bibitem[Changpinyo et~al.(2021)Changpinyo, Sharma, Ding, and
  Soricut]{changpinyo2021conceptual}
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
\newblock Conceptual 12m: Pushing web-scale image-text pre-training to
  recognize long-tail visual concepts.
\newblock In \emph{CVPR}, 2021.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Wu, Wang, Zhang, Nian, Li, and
  Shao]{Chen2020transcnn}
Kun Chen, Yusong Wu, Ziyue Wang, Xuan Zhang, Fudong Nian, Shengchen Li, and
  Xi~Shao.
\newblock Audio captioning based on transformer and pre-trained cnn.
\newblock In \emph{DCASE}, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Kornblith, Norouzi, and
  Hinton]{chen2020simple_simclr}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{ICML}, 2020{\natexlab{b}}.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Saxena, Li, Lin, Fleet, and
  Hinton]{chen2022unifiedpix2seqall}
Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David~J Fleet, and Geoffrey~E
  Hinton.
\newblock A unified sequence interface for vision tasks.
\newblock \emph{NeurIPS}, 2022{\natexlab{a}}.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Wang, Changpinyo, Piergiovanni,
  Padlewski, Salz, Goodman, Grycner, Mustafa, Beyer, et~al.]{chen2022pali}
Xi~Chen, Xiao Wang, Soravit Changpinyo, AJ~Piergiovanni, Piotr Padlewski,
  Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer,
  et~al.
\newblock Pali: A jointly-scaled multilingual language-image model.
\newblock \emph{arXiv preprint}, 2022{\natexlab{b}}.

\bibitem[Chen et~al.(2020{\natexlab{c}})Chen, Li, Yu, El~Kholy, Ahmed, Gan,
  Cheng, and Liu]{chen2020uniter}
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El~Kholy, Faisal Ahmed, Zhe Gan,
  Yu~Cheng, and Jingjing Liu.
\newblock Uniter: Universal image-text representation learning.
\newblock In \emph{ECCV}, 2020{\natexlab{c}}.

\bibitem[Cheng et~al.(2022)Cheng, Wang, Lei, Crandall, Bansal, and
  Bertasius]{cheng2022vindlu}
Feng Cheng, Xizi Wang, Jie Lei, David Crandall, Mohit Bansal, and Gedas
  Bertasius.
\newblock Vindlu: A recipe for effective video-and-language pretraining.
\newblock \emph{arXiv preprint}, 2022.

\bibitem[Cho et~al.(2021)Cho, Lei, Tan, and Bansal]{cho2021unifyingvlt5}
Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal.
\newblock Unifying vision-and-language tasks via text generation.
\newblock In \emph{ICML}, 2021.

\bibitem[Choshen et~al.(2022)Choshen, Venezian, Slonim, and
  Katz]{choshen2022fusing}
Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav Katz.
\newblock Fusing finetuned models for better pretraining.
\newblock \emph{arXiv preprint}, 2022.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint}, 2022.

\bibitem[Croce et~al.(2023)Croce, Rebuffi, Shelhamer, and
  Gowal]{croce2023seasoning}
Francesco Croce, Sylvestre-Alvise Rebuffi, Evan Shelhamer, and Sven Gowal.
\newblock Seasoning model soups for robustness to adversarial and natural
  distribution shifts.
\newblock In \emph{CVPR}, 2023.

\bibitem[Daheim et~al.(2023)Daheim, Dziri, Sachan, Gurevych, and
  Ponti]{daheim2023elastic}
Nico Daheim, Nouha Dziri, Mrinmaya Sachan, Iryna Gurevych, and Edoardo~M Ponti.
\newblock Elastic weight removal for faithful and abstractive dialogue
  generation.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Dai et~al.(2022)Dai, Liu, Ji, Su, and Fung]{dai2022plausible}
Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale Fung.
\newblock Plausible may not be faithful: Probing object hallucination in
  vision-language pre-training.
\newblock \emph{arXiv preprint}, 2022.

\bibitem[Dai et~al.(2023{\natexlab{a}})Dai, Li, Li, Tiong, Zhao, Wang, Li,
  Fung, and Hoi]{dai2023instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng~Huat Tiong, Junqi Zhao,
  Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
\newblock Instructblip: Towards general-purpose vision-language models with
  instruction tuning.
\newblock \emph{arXiv preprint}, 2023{\natexlab{a}}.

\bibitem[Dai et~al.(2023{\natexlab{b}})Dai, Liu, Ji, Su, and
  Fung]{dai2023plausible}
Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale Fung.
\newblock Plausible may not be faithful: Probing object hallucination in
  vision-language pre-training.
\newblock In \emph{Proceedings of the 17th Conference of the European Chapter
  of the Association for Computational Linguistics}, pp.\  2128--2140,
  2023{\natexlab{b}}.

\bibitem[Dancette et~al.(2023)Dancette, Whitehead, Maheshwary, Vedantam,
  Scherer, Chen, Cord, and Rohrbach]{Dancette_2023_CVPR}
Corentin Dancette, Spencer Whitehead, Rishabh Maheshwary, Ramakrishna Vedantam,
  Stefan Scherer, Xinlei Chen, Matthieu Cord, and Marcus Rohrbach.
\newblock Improving selective visual question answering by learning from your
  peers.
\newblock In \emph{CVPR}, 2023.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek,
  Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin,
  et~al.]{dehghani2023scalingvit22b}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan
  Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim
  Alabdulmohsin, et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Dimitriadis et~al.(2022)Dimitriadis, Frossard, and
  Fleuret]{dimitriadis2022pareto}
Nikolaos Dimitriadis, Pascal Frossard, and Fran{\c{c}}ois Fleuret.
\newblock Pareto manifold learning: Tackling multiple tasks via ensembles of
  single-task models.
\newblock \emph{arXiv preprint}, 2022.

\bibitem[Ding et~al.(2021)Ding, Yang, Hong, Zheng, Zhou, Yin, Lin, Zou, Shao,
  Yang, et~al.]{ding2021cogview}
Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da~Yin, Junyang
  Lin, Xu~Zou, Zhou Shao, Hongxia Yang, et~al.
\newblock Cogview: Mastering text-to-image generation via transformers.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Don-Yehiya et~al.(2023)Don-Yehiya, Venezian, Raffel, Slonim, Katz, and
  Choshen]{choshen2022cold}
Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and
  Leshem Choshen.
\newblock {ColD} fusion: Collaborative descent for distributed multitask
  finetuning.
\newblock In \emph{ACL}, 2023.

\bibitem[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and
  Sui]{dong2022survey}
Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun,
  Jingjing Xu, and Zhifang Sui.
\newblock A survey for in-context learning.
\newblock \emph{arXiv preprint}, 2022.

\bibitem[Dou et~al.(2021)Dou, Xu, Gan, Wang, Wang, Wang, Zhu, Liu, Zeng,
  et~al.]{meter}
Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang,
  Chenguang Zhu, Zicheng Liu, Michael Zeng, et~al.
\newblock An empirical study of training end-to-end vision-and-language
  transformers.
\newblock \emph{arXiv preprint}, 2021.

\bibitem[Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter,
  Wahid, Tompson, Vuong, Yu, et~al.]{driess2023palme}
Danny Driess, Fei Xia, Mehdi~SM Sajjadi, Corey Lynch, Aakanksha Chowdhery,
  Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et~al.
\newblock Palm-e: An embodied multimodal language model.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Drossos et~al.(2020)Drossos, Lipping, and Virtanen]{drossos2020clotho}
Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen.
\newblock Clotho: An audio captioning dataset.
\newblock In \emph{ICASSP}, 2020.

\bibitem[Eichenberg et~al.(2021)Eichenberg, Black, Weinbach, Parcalabescu, and
  Frank]{eichenberg2021magma}
Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, and
  Anette Frank.
\newblock Magma--multimodal augmentation of generative models through
  adapter-based finetuning.
\newblock \emph{arXiv preprint}, 2021.

\bibitem[Entezari et~al.(2022)Entezari, Sedghi, Saukh, and
  Neyshabur]{entezari2022the}
Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur.
\newblock The role of permutation invariance in linear mode connectivity of
  neural networks.
\newblock In \emph{ICLR}, 2022.

\bibitem[Eren \& Sert(2020)Eren and Sert]{eren_sememb2020}
Ayşegül~Özkaya Eren and Mustafa Sert.
\newblock Audio captioning based on combined audio and semantic embeddings.
\newblock In \emph{ISM}, 2020.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and Carbin]{Frankle2020}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel~M. Roy, and Michael
  Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{ICML}, 2020.

\bibitem[Fu et~al.(2021)Fu, Li, Gan, Lin, Wang, Wang, and Liu]{fu2021violet}
Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William~Yang Wang, Lijuan Wang, and
  Zicheng Liu.
\newblock Violet: End-to-end video-language transformers with masked
  visual-token modeling.
\newblock \emph{arXiv preprint}, 2021.

\bibitem[Gan et~al.(2020)Gan, Chen, Li, Zhu, Cheng, and Liu]{gan2020largevilla}
Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu~Cheng, and Jingjing Liu.
\newblock Large-scale adversarial training for vision-and-language
  representation learning.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Gemmeke et~al.(2017)Gemmeke, Ellis, Freedman, Jansen, Lawrence, Moore,
  Plakal, and Ritter]{audioset}
Jort~F. Gemmeke, Daniel P.~W. Ellis, Dylan Freedman, Aren Jansen, Wade
  Lawrence, R.~Channing Moore, Manoj Plakal, and Marvin Ritter.
\newblock Audio set: An ontology and human-labeled dataset for audio events.
\newblock In \emph{ICASSP}, 2017.

\bibitem[Goyal et~al.(2017)Goyal, Khot, Summers-Stay, Batra, and
  Parikh]{goyal2017makingvqav2}
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
\newblock Making the v in vqa matter: Elevating the role of image understanding
  in visual question answering.
\newblock In \emph{CVPR}, 2017.

\bibitem[Gurari et~al.(2018)Gurari, Li, Stangl, Guo, Lin, Grauman, Luo, and
  Bigham]{gurari2018vizwiz}
Danna Gurari, Qing Li, Abigale~J Stangl, Anhong Guo, Chi Lin, Kristen Grauman,
  Jiebo Luo, and Jeffrey~P Bigham.
\newblock Vizwiz grand challenge: Answering visual questions from blind people.
\newblock In \emph{CVPR}, 2018.

\bibitem[Hara et~al.(2018{\natexlab{a}})Hara, Kataoka, and Satoh]{hara2018can}
Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh.
\newblock Can spatiotemporal 3d cnns retrace the history of 2d cnns and
  imagenet?
\newblock In \emph{CVPR}, 2018{\natexlab{a}}.

\bibitem[Hara et~al.(2018{\natexlab{b}})Hara, Kataoka, and
  Satoh]{hara2018can3dresnext}
Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh.
\newblock Can spatiotemporal 3d cnns retrace the history of 2d cnns and
  imagenet?
\newblock In \emph{CVPR}, 2018{\natexlab{b}}.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and
  Girshick]{he2022maskedmae}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{CVPR}, 2022.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark,
  et~al.]{hoffmann2022trainingchinchilla}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint}, 2022.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint}, 2021.

\bibitem[Hu et~al.(2022)Hu, Gan, Wang, Yang, Liu, Lu, and
  Wang]{hu2022scalinglemon}
Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and
  Lijuan Wang.
\newblock Scaling up vision-language pre-training for image captioning.
\newblock In \emph{CVPR}, 2022.

\bibitem[Huang et~al.(2023)Huang, Dong, Wang, Hao, Singhal, Ma, Lv, Cui,
  Mohammed, Liu, et~al.]{huang2023languagekosmos}
Shaohan Huang, Li~Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma,
  Tengchao Lv, Lei Cui, Owais~Khan Mohammed, Qiang Liu, et~al.
\newblock Language is not all you need: Aligning perception with language
  models.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Huang et~al.(2021)Huang, Xue, Liu, and Lu]{huang2021unifying}
Yupan Huang, Hongwei Xue, Bei Liu, and Yutong Lu.
\newblock Unifying multimodal transformer for bi-directional image and text
  generation.
\newblock In \emph{ICM}, 2021.

\bibitem[Hudson \& Manning(2019)Hudson and Manning]{hudson2019gqa}
Drew~A Hudson and Christopher~D Manning.
\newblock Gqa: A new dataset for real-world visual reasoning and compositional
  question answering.
\newblock In \emph{CVPR}, 2019.

\bibitem[Iashin \& Rahtu(2020{\natexlab{a}})Iashin and
  Rahtu]{iashin2020betterbmt}
Vladimir Iashin and Esa Rahtu.
\newblock A better use of audio-visual cues: Dense video captioning with
  bi-modal transformer.
\newblock \emph{arXiv preprint}, 2020{\natexlab{a}}.

\bibitem[Iashin \& Rahtu(2020{\natexlab{b}})Iashin and
  Rahtu]{iashin2020multimdvc}
Vladimir Iashin and Esa Rahtu.
\newblock Multi-modal dense video captioning.
\newblock In \emph{CVPR}, 2020{\natexlab{b}}.

\bibitem[Ilharco et~al.(2022)Ilharco, Wortsman, Gadre, Song, Hajishirzi,
  Kornblith, Farhadi, and Schmidt]{ilharco2022patching}
Gabriel Ilharco, Mitchell Wortsman, Samir~Yitzhak Gadre, Shuran Song, Hannaneh
  Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt.
\newblock Patching open-vocabulary models by interpolating weights.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Ilharco et~al.(2023)Ilharco, {Tulio Ribeiro}, {Wortsman},
  {Gururangan}, {Schmidt}, {Hajishirzi}, and {Farhadi}]{2022arXiv221204089I}
Gabriel Ilharco, Marco {Tulio Ribeiro}, Mitchell {Wortsman}, Suchin
  {Gururangan}, Ludwig {Schmidt}, Hannaneh {Hajishirzi}, and Ali {Farhadi}.
\newblock Editing models with task arithmetic.
\newblock In \emph{ICLR}, 2023.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and {Andrew
  Gordon} Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In \emph{UAI}, 2018.

\bibitem[Jang et~al.(2023)Jang, Kim, Ye, Kim, Logeswaran, Lee, Lee, and
  Seo]{jang2023exploring}
Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran,
  Moontae Lee, Kyungjae Lee, and Minjoon Seo.
\newblock Exploring the benefits of training expert language models over
  instruction tuning.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and
  Duerig]{jia2021scaling_align}
Chao Jia, Yinfei Yang, Ye~Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
  Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In \emph{ICML}, 2021.

\bibitem[Jordan et~al.(2023)Jordan, Sedghi, Saukh, Entezari, and
  Neyshabur]{jordan2022repair}
Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, and Behnam Neyshabur.
\newblock {REPAIR}: Renormalizing permuted activations for interpolation
  repair.
\newblock In \emph{ICLR}, 2023.

\bibitem[Kamath et~al.(2021)Kamath, Singh, LeCun, Synnaeve, Misra, and
  Carion]{kamath2021mdetr}
Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and
  Nicolas Carion.
\newblock Mdetr-modulated detection for end-to-end multi-modal understanding.
\newblock In \emph{ICCV}, 2021.

\bibitem[Kay et~al.(2017)Kay, Carreira, Simonyan, Zhang, Hillier,
  Vijayanarasimhan, Viola, Green, Back, Natsev, et~al.]{kay2017kinetics}
Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra
  Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et~al.
\newblock The kinetics human action video dataset.
\newblock \emph{arXiv preprint}, 2017.

\bibitem[Kim et~al.(2019{\natexlab{a}})Kim, Kim, Lee, and Kim]{audiocaps}
Chris~Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim.
\newblock Audiocaps: Generating captions for audios in the wild.
\newblock In \emph{NAACL-HLT}, 2019{\natexlab{a}}.

\bibitem[Kim et~al.(2019{\natexlab{b}})Kim, Kim, Lee, and
  Kim]{kim-etal-2019-Audiocaps}
Chris~Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim.
\newblock {A}udio{C}aps: Generating captions for audios in the wild.
\newblock In \emph{ACM}, 2019{\natexlab{b}}.

\bibitem[Kim et~al.(2021)Kim, Son, and Kim]{kim2021vilt}
Wonjae Kim, Bokyung Son, and Ildoo Kim.
\newblock Vilt: Vision-and-language transformer without convolution or region
  supervision.
\newblock In \emph{ICML}, 2021.

\bibitem[Koh et~al.(2022)Koh, Fuzhao, and Siong]{koh2022automated}
Andrew Koh, Xue Fuzhao, and Chng~Eng Siong.
\newblock Automated audio captioning using transfer learning and reconstruction
  latent space similarity regularization.
\newblock In \emph{ICASSP}, 2022.

\bibitem[Koh et~al.(2023)Koh, Salakhutdinov, and
  Fried]{koh2023groundingfromage}
Jing~Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.
\newblock Grounding language models to images for multimodal generation.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Koizumi et~al.(2020)Koizumi, Masumura, Nishida, Yasuda, and
  Saito]{koizumi2020transformer}
Yuma Koizumi, Ryo Masumura, Kyosuke Nishida, Masahiro Yasuda, and Shoichiro
  Saito.
\newblock A transformer-based audio captioning model with keyword estimation.
\newblock \emph{Proc. Interspeech}, 2020.

\bibitem[Kong et~al.(2020)Kong, Cao, Iqbal, Wang, Wang, and
  Plumbley]{kong2020panns}
Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark~D
  Plumbley.
\newblock Panns: Large-scale pretrained audio neural networks for audio pattern
  recognition.
\newblock \emph{ACM}, 2020.

\bibitem[Krishna et~al.(2017{\natexlab{a}})Krishna, Hata, Ren, Fei-Fei, and
  Carlos~Niebles]{krishna2017densedcev}
Ranjay Krishna, Kenji Hata, Frederic Ren, Li~Fei-Fei, and Juan Carlos~Niebles.
\newblock Dense-captioning events in videos.
\newblock In \emph{ICCV}, 2017{\natexlab{a}}.

\bibitem[Krishna et~al.(2017{\natexlab{b}})Krishna, Zhu, Groth, Johnson, Hata,
  Kravitz, Chen, Kalantidis, Li, Shamma, et~al.]{krishna2017visualgenome}
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
  Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David~A Shamma, et~al.
\newblock Visual genome: Connecting language and vision using crowdsourced
  dense image annotations.
\newblock \emph{IJCV}, 2017{\natexlab{b}}.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{Lakshminarayanan2017}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Lei et~al.(2021)Lei, Li, Zhou, Gan, Berg, Bansal, and
  Liu]{lei2021lessclipbert}
Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara~L Berg, Mohit Bansal, and
  Jingjing Liu.
\newblock Less is more: Clipbert for video-and-language learning via sparse
  sampling.
\newblock In \emph{CVPR}, 2021.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and
  Constant]{lester2021powerprompttuning}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{EMNLP}, 2021.

\bibitem[Lewis et~al.(2020)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{lewis2020bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock In \emph{ACL}, 2020.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Li, Li, Niebles, and
  Hoi]{li2022alignandpromptalpro}
Dongxu Li, Junnan Li, Hongdong Li, Juan~Carlos Niebles, and Steven~CH Hoi.
\newblock Align and prompt: Video-and-language pre-training with entity
  prompts.
\newblock In \emph{CVPR}, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Selvaraju, Gotmare, Joty, Xiong, and
  Hoi]{li2021alignalbef}
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong,
  and Steven Chu~Hong Hoi.
\newblock Align before fuse: Vision and language representation learning with
  momentum distillation.
\newblock \emph{NeurIPS}, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Li, Xiong, and Hoi]{li2022blip}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified
  vision-language understanding and generation.
\newblock \emph{arXiv preprint}, 2022{\natexlab{b}}.

\bibitem[Li et~al.(2023)Li, Li, Savarese, and Hoi]{li2023blip2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Li et~al.(2022{\natexlab{c}})Li, Gan, Lin, Lin, Liu, Liu, and
  Wang]{li2022lavender}
Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce~Liu, and Lijuan
  Wang.
\newblock Lavender: Unifying video-language understanding as masked language
  modeling.
\newblock \emph{arXiv preprint}, 2022{\natexlab{c}}.

\bibitem[Li et~al.(2019)Li, Yatskar, Yin, Hsieh, and Chang]{li2019visualbert}
Liunian~Harold Li, Mark Yatskar, Da~Yin, Cho-Jui Hsieh, and Kai-Wei Chang.
\newblock Visualbert: A simple and performant baseline for vision and language.
\newblock \emph{arXiv preprint}, 2019.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Gao, Niu, Xiao, Liu, Liu, Wu, and
  Wang]{li2020unimo}
Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and
  Haifeng Wang.
\newblock Unimo: Towards unified-modal understanding and generation via
  cross-modal contrastive learning.
\newblock \emph{arXiv preprint}, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Yin, Li, Zhang, Hu, Zhang, Wang, Hu,
  Dong, Wei, et~al.]{li2020oscar}
Xiujun Li, Xi~Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan
  Wang, Houdong Hu, Li~Dong, Furu Wei, et~al.
\newblock Oscar: Object-semantics aligned pre-training for vision-language
  tasks.
\newblock In \emph{ECCV}, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Liang, Zhao, Cui, Ouyang, Shao, Yu,
  and Yan]{li2021supervisiondeclip}
Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao,
  Fengwei Yu, and Junjie Yan.
\newblock Supervision exists everywhere: A data efficient contrastive
  language-image pre-training paradigm.
\newblock \emph{arXiv preprint}, 2021{\natexlab{b}}.

\bibitem[Li et~al.(2018)Li, Yao, Pan, Chao, and Mei]{li2018jointlydvc}
Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and Tao Mei.
\newblock Jointly localizing and describing events for dense video captioning.
\newblock In \emph{CVPR}, 2018.

\bibitem[Liang et~al.(2022)Liang, Zhao, and
  Sch{\"u}tze]{liang2022modularpromptfuse}
Sheng Liang, Mengjie Zhao, and Hinrich Sch{\"u}tze.
\newblock Modular and parameter-efficient multimodal fusion with prompting.
\newblock In \emph{ACL}, 2022.

\bibitem[Lin et~al.(2022)Lin, Li, Lin, Ahmed, Gan, Liu, Lu, and
  Wang]{lin2022swinbert}
Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu,
  Yumao Lu, and Lijuan Wang.
\newblock Swinbert: End-to-end transformers with sparse attention for video
  captioning.
\newblock In \emph{CVPR}, 2022.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'a}r, and Zitnick]{lin2014microsoftcoco}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{ECCV}, 2014.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Li, Wu, and
  Lee]{liu2023visualllava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock \emph{arXiv preprint}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2022)Liu, Mei, Huang, Sun, Zhao, Liu, Plumbley, Kilic, and
  Wang]{liu2022leveraging}
Xubo Liu, Xinhao Mei, Qiushi Huang, Jianyuan Sun, Jinzheng Zhao, Haohe Liu,
  Mark~D Plumbley, Volkan Kilic, and Wenwu Wang.
\newblock Leveraging pre-trained bert for audio captioning.
\newblock In \emph{EUSIPCO}, 2022.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Han, Ma, Zhang, Yang, Tian, He, Li,
  He, Liu, et~al.]{liu2023summary}
Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian,
  Hao He, Antong Li, Mengshen He, Zhengliang Liu, et~al.
\newblock Summary of chatgpt/gpt-4 research and perspective towards the future
  of large language models.
\newblock \emph{arXiv preprint}, 2023{\natexlab{b}}.

\bibitem[Lu et~al.(2019)Lu, Batra, Parikh, and Lee]{lu2019vilbert}
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
\newblock Vilbert: Pretraining task-agnostic visiolinguistic representations
  for vision-and-language tasks.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Lu et~al.(2022{\natexlab{a}})Lu, Clark, Zellers, Mottaghi, and
  Kembhavi]{lu2022unifiedio}
Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha
  Kembhavi.
\newblock Unified-io: A unified model for vision, language, and multi-modal
  tasks.
\newblock \emph{arXiv preprint}, 2022{\natexlab{a}}.

\bibitem[Lu et~al.(2022{\natexlab{b}})Lu, Mishra, Xia, Qiu, Chang, Zhu,
  Tafjord, Clark, and Kalyan]{lu2022learnscienceqa}
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu,
  Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.
\newblock Learn to explain: Multimodal reasoning via thought chains for science
  question answering.
\newblock In \emph{NeurIPS}, 2022{\natexlab{b}}.

\bibitem[Luo et~al.(2020)Luo, Ji, Shi, Huang, Duan, Li, Li, Bharti, and
  Zhou]{luo2020univl}
Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason
  Li, Taroon Bharti, and Ming Zhou.
\newblock Univl: A unified video and language pre-training model for multimodal
  understanding and generation.
\newblock \emph{arXiv preprint}, 2020.

\bibitem[Ma{\~n}as et~al.(2022)Ma{\~n}as, Rodriguez, Ahmadi, Nematzadeh, Goyal,
  and Agrawal]{manas2022mapl}
Oscar Ma{\~n}as, Pau Rodriguez, Saba Ahmadi, Aida Nematzadeh, Yash Goyal, and
  Aishwarya Agrawal.
\newblock Mapl: Parameter-efficient adaptation of unimodal pre-trained models
  for vision-language few-shot prompting.
\newblock \emph{arXiv preprint}, 2022.

\bibitem[Marino et~al.(2019)Marino, Rastegari, Farhadi, and Mottaghi]{okvqa}
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
\newblock Ok-vqa: A visual question answering benchmark requiring external
  knowledge.
\newblock In \emph{CVPR}, 2019.

\bibitem[Matena \& Raffel(2022)Matena and Raffel]{matena2022merging}
Michael Matena and Colin Raffel.
\newblock Merging models with {Fisher}-weighted averaging.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Mei et~al.(2021)Mei, Liu, Huang, Plumbley, and Wang]{mei2021audio}
Xinhao Mei, Xubo Liu, Qiushi Huang, Mark~David Plumbley, and Wenwu Wang.
\newblock Audio captioning transformer.
\newblock In \emph{Workshop DCASE}, 2021.

\bibitem[Merullo et~al.(2022)Merullo, Castricato, Eickhoff, and
  Pavlick]{merullo2022linearlylimber}
Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick.
\newblock Linearly mapping from image to text space.
\newblock \emph{arXiv preprint}, 2022.

\bibitem[Mitchell et~al.(2019)Mitchell, Wu, Zaldivar, Barnes, Vasserman,
  Hutchinson, Spitzer, Raji, and Gebru]{mitchell2019model}
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman,
  Ben Hutchinson, Elena Spitzer, Inioluwa~Deborah Raji, and Timnit Gebru.
\newblock Model cards for model reporting.
\newblock In \emph{Proceedings of the conference on fairness, accountability,
  and transparency}, 2019.

\bibitem[Neyshabur et~al.(2020)Neyshabur, Sedghi, and
  Zhang]{neyshabur2020being}
Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang.
\newblock What is being transferred in transfer learning?
\newblock \emph{NeurIPS}, 2020.

\bibitem[Nichol et~al.(2022)Nichol, Dhariwal, Ramesh, Shyam, Mishkin, Mcgrew,
  Sutskever, and Chen]{nichol2022glide}
Alexander~Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela
  Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen.
\newblock Glide: Towards photorealistic image generation and editing with
  text-guided diffusion models.
\newblock In \emph{ICML}, 2022.

\bibitem[Ordonez et~al.(2011)Ordonez, Kulkarni, and Berg]{sbu}
Vicente Ordonez, Girish Kulkarni, and Tamara~L Berg.
\newblock Im2text: Describing images using 1 million captioned photographs.
\newblock In \emph{NeurIPS}, 2011.

\bibitem[Ortiz-Jimenez et~al.(2023)Ortiz-Jimenez, Favero, and
  Frossard]{ortiz2023task}
Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard.
\newblock Task arithmetic in the tangent space: Improved editing of pre-trained
  models.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{ICML}, 2021.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, et~al.]{rae2021scalinggopher}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  et~al.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{arXiv preprint}, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploringt5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{JMLR}, 2020.

\bibitem[Rahman et~al.(2019)Rahman, Xu, and Sigal]{rahman2019watchmwsdec}
Tanzila Rahman, Bicheng Xu, and Leonid Sigal.
\newblock Watch, listen and tell: Multi-modal weakly supervised dense event
  captioning.
\newblock In \emph{ICCV}, 2019.

\bibitem[Rame et~al.(2022)Rame, Kirchmeyer, Rahier, Rakotomamonjy, Gallinari,
  and Cord]{rame2022diversediwa}
Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy,
  Patrick Gallinari, and Matthieu Cord.
\newblock Diverse weight averaging for out-of-distribution generalization.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Rame et~al.(2023{\natexlab{a}})Rame, Ahuja, Zhang, Cord, Bottou, and
  Lopez-Paz]{rame2022recyclingratatouille}
Alexandre Rame, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, L{\'e}on Bottou, and
  David Lopez-Paz.
\newblock {Model Ratatouille}: Recycling diverse models for out-of-distribution
  generalization.
\newblock In \emph{ICML}, 2023{\natexlab{a}}.

\bibitem[Rame et~al.(2023{\natexlab{b}})Rame, Couairon, Shukor, Dancette, Gaya,
  Soulier, and Cord]{rame2023rewarded}
Alexandre Rame, Guillaume Couairon, Mustafa Shukor, Corentin Dancette,
  Jean-Baptiste Gaya, Laure Soulier, and Matthieu Cord.
\newblock Rewarded soups: towards pareto-optimal alignment by interpolating
  weights fine-tuned on diverse rewards.
\newblock \emph{arXiv preprint}, 2023{\natexlab{b}}.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{ICML}, 2021.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov,
  Barth-Maron, Gimenez, Sulsky, Kay, Springenberg,
  et~al.]{reed2022generalistgato}
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio~Gomez Colmenarejo, Alexander
  Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay,
  Jost~Tobias Springenberg, et~al.
\newblock A generalist agent.
\newblock \emph{arXiv preprint}, 2022.

\bibitem[Rohrbach et~al.(2018)Rohrbach, Hendricks, Burns, Darrell, and
  Saenko]{rohrbach2018objecthallucination}
Anna Rohrbach, Lisa~Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate
  Saenko.
\newblock Object hallucination in image captioning.
\newblock In \emph{EMNLP}, 2018.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training gans.
\newblock \emph{NeurIPS}, 2016.

\bibitem[Schuhmann et~al.(2021)Schuhmann, Vencu, Beaumont, Kaczmarczyk, Mullis,
  Katta, Coombes, Jitsev, and Komatsuzaki]{schuhmann2021laion}
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk,
  Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran
  Komatsuzaki.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text
  pairs.
\newblock \emph{arXiv preprint}, 2021.

\bibitem[Seo et~al.(2022)Seo, Nagrani, Arnab, and Schmid]{seo2022endmvgpt}
Paul~Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and Cordelia Schmid.
\newblock End-to-end generative pretraining for multimodal video captioning.
\newblock In \emph{CVPR}, 2022.

\bibitem[Sharma et~al.(2018)Sharma, Ding, Goodman, and
  Soricut]{Sharma2018ConceptualCA}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In \emph{ACL}, 2018.

\bibitem[Shukor et~al.(2022)Shukor, Couairon, and Cord]{vicha}
Mustafa Shukor, Guillaume Couairon, and Matthieu Cord.
\newblock Efficient vision-language pretraining with visual concepts and
  hierarchical alignment.
\newblock In \emph{BMVC}, 2022.

\bibitem[Shukor et~al.(2023)Shukor, Dancette, and Cord]{shukor2023epalm}
Mustafa Shukor, Corentin Dancette, and Matthieu Cord.
\newblock ep-alm: Efficient perceptual augmentation of language models.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Singh et~al.(2022)Singh, Hu, Goswami, Couairon, Galuba, Rohrbach, and
  Kiela]{singh2022flava}
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech
  Galuba, Marcus Rohrbach, and Douwe Kiela.
\newblock Flava: A foundational language and vision alignment model.
\newblock In \emph{CVPR}, 2022.

\bibitem[Srinivasan et~al.(2022)Srinivasan, Ren, and
  Thomason]{srinivasan2022curriculum}
Tejas Srinivasan, Xiang Ren, and Jesse Thomason.
\newblock Curriculum learning for data-efficient vision-language alignment.
\newblock \emph{arXiv preprint}, 2022.

\bibitem[Sung et~al.(2023)Sung, Li, Lin, Gan, Bansal, and
  Wang]{sung2023empirical}
Yi-Lin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal, and Lijuan Wang.
\newblock An empirical study of multimodal model merging.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Takeuchi et~al.(2020)Takeuchi, Koizumi, Ohishi, Harada, and
  Kashino]{takeuchi2020effects}
Daiki Takeuchi, Yuma Koizumi, Yasunori Ohishi, Noboru Harada, and Kunio
  Kashino.
\newblock Effects of word-frequency based pre-and post-processings for audio
  captioning.
\newblock \emph{arXiv preprint}, 2020.

\bibitem[Tang et~al.(2021)Tang, Wang, Liu, Rao, Li, and
  Li]{tang2021clip4caption}
Mingkang Tang, Zhanyu Wang, Zhenhua Liu, Fengyun Rao, Dian Li, and Xiu Li.
\newblock Clip4caption: Clip for video caption.
\newblock In \emph{ICM}, 2021.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Tran, Garcia, Bahri, Schuster, Zheng,
  Houlsby, and Metzler]{tay2022unifyingul2}
Yi~Tay, Mostafa Dehghani, Vinh~Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster,
  Huaixiu~Steven Zheng, Neil Houlsby, and Donald Metzler.
\newblock Unifying language learning paradigms.
\newblock \emph{arXiv preprint}, 2022.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou]{touvron2021trainingdeit}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In \emph{ICML}, 2021.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Yang, Hu, Li, Lin, Gan, Liu, Liu,
  and Wang]{wang2022git}
Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan,
  Zicheng Liu, Ce~Liu, and Lijuan Wang.
\newblock {GIT}: A generative image-to-text transformer for vision and
  language.
\newblock \emph{Transactions on Machine Learning Research}, 2022{\natexlab{a}}.
\newblock ISSN 2835-8856.

\bibitem[Wang et~al.(2018{\natexlab{a}})Wang, Jiang, Ma, Liu, and
  Xu]{wang2018bidirectional}
Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, and Yong Xu.
\newblock Bidirectional attentive fusion with context gating for dense video
  captioning.
\newblock In \emph{CVPR}, 2018{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Ge, Yan, Ge, Lin, Tsutsui, Lin,
  Cai, Wu, Shan, et~al.]{wang2022allinone}
Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Kevin~Qinghong Lin, Satoshi
  Tsutsui, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, et~al.
\newblock All in one: Exploring unified video-language pre-training.
\newblock In \emph{CVPR}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Chen, Wu, Luo, Zhou, Zhao, Xie,
  Liu, Jiang, and Yuan]{wang2022omnivl}
Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao,
  Yujia Xie, Ce~Liu, Yu-Gang Jiang, and Lu~Yuan.
\newblock Omnivl: One foundation model for image-language and video-language
  tasks.
\newblock \emph{arXiv preprint}, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Zhang, Su, and
  Zhu]{wang2023comprehensivecontinual}
Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu.
\newblock A comprehensive survey of continual learning: Theory, method and
  application.
\newblock \emph{arXiv preprint}, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{c}})Wang, Yang, Men, Lin, Bai, Li, Ma,
  Zhou, Zhou, and Yang]{wang2022unifyingofa}
Peng Wang, An~Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma,
  Chang Zhou, Jingren Zhou, and Hongxia Yang.
\newblock Unifying architectures, tasks, and modalities through a simple
  sequence-to-sequence learning framework.
\newblock \emph{arXiv preprint}, 2022{\natexlab{c}}.

\bibitem[Wang et~al.(2022{\natexlab{d}})Wang, Roberts, Hesslow, Le~Scao, Chung,
  Beltagy, Launay, and Raffel]{wang2022language}
Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le~Scao, Hyung~Won Chung,
  Iz~Beltagy, Julien Launay, and Colin Raffel.
\newblock What language model architecture and pretraining objective works best
  for zero-shot generalization?
\newblock In \emph{ICML}, 2022{\natexlab{d}}.

\bibitem[Wang et~al.(2022{\natexlab{e}})Wang, Bao, Dong, Bjorck, Peng, Liu,
  Aggarwal, Mohammed, Singhal, Som, et~al.]{wang2022imagebeit3}
Wenhui Wang, Hangbo Bao, Li~Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti
  Aggarwal, Owais~Khan Mohammed, Saksham Singhal, Subhojit Som, et~al.
\newblock Image as a foreign language: Beit pretraining for all vision and
  vision-language tasks.
\newblock \emph{arXiv preprint}, 2022{\natexlab{e}}.

\bibitem[Wang et~al.(2018{\natexlab{b}})Wang, Wang, and
  Wang]{wang-etal-2018-watchhaca}
Xin Wang, Yuan-Fang Wang, and William~Yang Wang.
\newblock Watch, listen, and describe: Globally and locally aligned cross-modal
  attentions for video captioning.
\newblock In \emph{ACL}, 2018{\natexlab{b}}.

\bibitem[Wang et~al.(2021)Wang, Yu, Yu, Dai, Tsvetkov, and Cao]{wang2021simvlm}
Zirui Wang, Jiahui Yu, Adams~Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.
\newblock Simvlm: Simple visual language model pretraining with weak
  supervision.
\newblock \emph{arXiv preprint}, 2021.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes,
  Morcos, Namkoong, Farhadi, Carmon, Kornblith, and
  Schmidt]{wortsman2022modelsoups}
Mitchell Wortsman, Gabriel Ilharco, Samir~Yitzhak Gadre, Rebecca Roelofs,
  Raphael Gontijo-Lopes, Ari~S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair
  Carmon, Simon Kornblith, and Ludwig Schmidt.
\newblock Model soups: averaging weights of multiple fine-tuned models improves
  accuracy without increasing inference time.
\newblock In \emph{ICML}, 2022.

\bibitem[Wu et~al.(2022)Wu, Liang, Ji, Yang, Fang, Jiang, and Duan]{wu2022nuwa}
Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan
  Duan.
\newblock N{\"u}wa: Visual synthesis pre-training for neural visual world
  creation.
\newblock In \emph{ECCV}, 2022.

\bibitem[Xie et~al.(2019)Xie, Lai, Doran, and Kadav]{xie2019visualsnli}
Ning Xie, Farley Lai, Derek Doran, and Asim Kadav.
\newblock Visual entailment: A novel task for fine-grained image understanding.
\newblock \emph{arXiv preprint}, 2019.

\bibitem[Xu et~al.(2017)Xu, Zhao, Xiao, Wu, Zhang, He, and Zhuang]{msvd_msrvtt}
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting
  Zhuang.
\newblock Video question answering via gradually refined attention over
  appearance and motion.
\newblock In \emph{ICM}, MM '17, New York, NY, USA, 2017. Association for
  Computing Machinery.
\newblock ISBN 9781450349062.
\newblock \doi{10.1145/3123266.3123427}.
\newblock URL \url{https://doi.org/10.1145/3123266.3123427}.

\bibitem[Xu et~al.(2016)Xu, Mei, Yao, and Rui]{Xu_2016_CVPR_msrvtt}
Jun Xu, Tao Mei, Ting Yao, and Yong Rui.
\newblock Msr-vtt: A large video description dataset for bridging video and
  language.
\newblock In \emph{CVPR}, June 2016.

\bibitem[Xu et~al.(2020)Xu, Dinkel, Wu, and Yu]{xu2020crnn}
Xuenan Xu, Heinrich Dinkel, Mengyue Wu, and Kai Yu.
\newblock A crnn-gru based reinforcement learning approach to audio captioning.
\newblock In \emph{DCASE}, 2020.

\bibitem[Xu et~al.(2021)Xu, Dinkel, Wu, Xie, and Yu]{xu2021investigating}
Xuenan Xu, Heinrich Dinkel, Mengyue Wu, Zeyu Xie, and Kai Yu.
\newblock Investigating local and global information for automated audio
  captioning with transfer learning.
\newblock In \emph{ICASSP}, 2021.

\bibitem[Xu et~al.(2022)Xu, Shen, and Huang]{xu2022multiinstruct}
Zhiyang Xu, Ying Shen, and Lifu Huang.
\newblock Multiinstruct: Improving multi-modal zero-shot learning via
  instruction tuning.
\newblock \emph{arXiv preprint}, 2022.

\bibitem[Yadav et~al.(2023)Yadav, Tam, Choshen, Raffel, and
  Bansal]{yadav2023resolving}
Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal.
\newblock Resolving interference when merging models.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Yang et~al.(2021{\natexlab{a}})Yang, Miech, Sivic, Laptev, and
  Schmid]{yang2021justask}
Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid.
\newblock Just ask: Learning to answer questions from millions of narrated
  videos.
\newblock In \emph{ICCV}, 2021{\natexlab{a}}.

\bibitem[Yang et~al.(2022)Yang, Miech, Sivic, Laptev, and
  Schmid]{yang2022zerofrozenbilm}
Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid.
\newblock Zero-shot video question answering via frozen bidirectional language
  models.
\newblock In \emph{NeurIPS 2022-36th Conference on Neural Information
  Processing Systems}, 2022.

\bibitem[Yang et~al.(2021{\natexlab{b}})Yang, Gan, Wang, Hu, Ahmed, Liu, Lu,
  and Wang]{yang2021crossingunitab}
Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu,
  Yumao Lu, and Lijuan Wang.
\newblock Crossing the format boundary of text and boxes: Towards unified
  vision-language modeling.
\newblock \emph{arXiv preprint}, 2021{\natexlab{b}}.

\bibitem[Yu et~al.(2022)Yu, Wang, Vasudevan, Yeung, Seyedhosseini, and
  Wu]{yu2022coca}
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and
  Yonghui Wu.
\newblock Coca: Contrastive captioners are image-text foundation models.
\newblock \emph{arXiv preprint}, 2022.

\bibitem[Yu et~al.(2016)Yu, Poirson, Yang, Berg, and
  Berg]{yu2016modelingrefcoco+}
Licheng Yu, Patrick Poirson, Shan Yang, Alexander~C Berg, and Tamara~L Berg.
\newblock Modeling context in referring expressions.
\newblock In \emph{ECCV}, 2016.

\bibitem[Yuan et~al.(2021)Yuan, Chen, Chen, Codella, Dai, Gao, Hu, Huang, Li,
  Li, et~al.]{yuan2021florence}
Lu~Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao,
  Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et~al.
\newblock Florence: A new foundation model for computer vision.
\newblock \emph{arXiv preprint}, 2021.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{zbontar2021barlow}
Jure Zbontar, Li~Jing, Ishan Misra, Yann LeCun, and St{\'e}phane Deny.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In \emph{ICML}, 2021.

\bibitem[Zellers et~al.(2021)Zellers, Lu, Hessel, Yu, Park, Cao, Farhadi, and
  Choi]{zellers2021merlot}
Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae~Sung Park, Jize Cao,
  Ali Farhadi, and Yejin Choi.
\newblock Merlot: Multimodal neural script knowledge models.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Zellers et~al.(2022)Zellers, Lu, Lu, Yu, Zhao, Salehi, Kusupati,
  Hessel, Farhadi, and Choi]{zellers2022merlot}
Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza
  Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi.
\newblock Merlot reserve: Neural script knowledge through vision and language
  and sound.
\newblock In \emph{CVPR}, 2022.

\bibitem[Zhang et~al.(2021)Zhang, Li, Hu, Yang, Zhang, Wang, Choi, and
  Gao]{zhang2021vinvl}
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
  Yejin Choi, and Jianfeng Gao.
\newblock Vinvl: Revisiting visual representations in vision-language models.
\newblock In \emph{CVPR}, 2021.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint}, 2022.

\bibitem[Zhao et~al.(2022)Zhao, Yang, Ren, Li, Wu, and
  Sun]{zhao2022wellencouraging}
Guangxiang Zhao, Wenkai Yang, Xuancheng Ren, Lei Li, Yunfang Wu, and Xu~Sun.
\newblock Well-classified examples are underestimated in classification with
  deep neural networks.
\newblock In \emph{AAAI}, 2022.

\bibitem[Zou et~al.(2023)Zou, Dou, Yang, Gan, Li, Li, Dai, Behl, Wang, Yuan,
  et~al.]{zou2023generalized}
Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang
  Dai, Harkirat Behl, Jianfeng Wang, Lu~Yuan, et~al.
\newblock Generalized decoding for pixel, image, and language.
\newblock In \emph{CVPR}, 2023.

\end{thebibliography}
