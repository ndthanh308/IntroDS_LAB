\section{Method}
\label{sec:Method}
% Figure environment removed

In this section, we introduce the proposed RAW image watermarking method named RAWIW. 
The schematic overview of the proposed method, as depicted in Fig \ref{section:framework}, comprises a watermarking module, a deep ISP module, and a distortion network. The watermarking module aims at embedding and extracting the watermarking, while the deep ISP module converts RAW images to RGB images. The distortion network introduces simulated noises into the framework, thereby enhancing the robustness of the watermarking against a range of adversarial attacks. The following subsections will describe these modules respectively in detail.

\subsection{Problem Formulation}
Our proposed methods embed a copyright message $\mathbf{M}$ within a RAW image $\mathbf{R}$ via a RAW image encoder $\mathbf{E}$ with weights $\mathbf{\theta_{E}}$ to produce an encoded RAW image $\mathbf{R_{e}}$. The ISP pipeline $\mathbf{P}$ is subsequently utilized to convert the encoded RAW image $\mathbf{R_{e}}$ to an RGB image $\mathbf{I_{e}}$ and also convert the original RAW image $\mathbf{R}$ to  RGB image $\mathbf{I_{o}}$. Finally, our proposed method utilizes an RGB image decoder $\mathbf{D}$ with weights $\mathbf{\theta_{D}}$ to extract the embedded message $\mathbf{M_{d}}$ from the RGB image $\mathbf{I_{e}}$. The above process can be expressed by the following four formulas:
\begin{equation}
    \mathbf{R_{e}} = \mathbf{E}_{\theta_{E}}(\mathbf{R}, \mathbf{M}), \mathbf{I_{e}} = \mathbf{P}(\mathbf{R_{e}}), \mathbf{I_{o}} = \mathbf{P}(\mathbf{R}), \mathbf{M_{d}} = \mathbf{D}_{\theta_{D}}(\mathbf{I_{e}}).
\end{equation}
For the purpose of the copyright protection for RAW images, it is imperative to achieve the congruence between $\mathbf{M_{d}}$ and $\mathbf{M}$, while simultaneously ensuring that the visual quality of both images remain uncompromised, Therefore, we need to optimize the parameters during the training process:
\begin{equation}
    \arg \min_{\theta_{E} , \theta_{D}} d_{rgb}(\mathbf{I_{o}}, \mathbf{I_{e}}) + d_{raw}(\mathbf{R}, \mathbf{R_{e}}) + d_{bin}(\mathbf{M}, \mathbf{M_{d}}),
\end{equation}
where $d_{rgb}$ stands for the RGB image distance, and analogously for $d_{raw}$, $d_{bin}$.

\subsection{Image Watermarking}
The image watermarking module is composed of three parts: encoder, decoder, and discriminator. The encoder is responsible for embedding the watermarking information into the RAW image, while the decoder extracts the watermarking information from the corresponding RGB image. The discriminator and encoder modules are trained adversarially to enhance the concealment of the watermarking information.

\textbf{Encoder}: 
The primary function of the encoder is to integrate watermarking information into a RAW image $\mathbf{R}$, producing an encoded RAW image $\mathbf{R_{e}}$ that exhibits the minimal perceptual differentiation compared to $\mathbf{R}$. Furthermore, the RGB images $\mathbf{I_{e}}$ derived from $\mathbf{R_{e}}$ exhibit the minimal perceptual differentiation compared to $\mathbf{I_{o}}$.
The watermarking message $\mathbf{M} \in \{0, 1\}^{L}$ is represented as a binary vector of length $\mathbf{L}$. This binary vector is first processed through a fully connected layer, resulting in a tensor $\mathbf{T} \in \mathbb{R}^{\frac{H}{4} \times \frac{W}{4} \times 1}$. Subsequently, the tensor $\mathbf{T}$ is upsampled to form a tensor $\mathbf{T^{'} } \in \mathbb{R}^{H \times W \times 1 }$, which is then concatenated with the RAW image $\mathbf{R} \in \mathbb{R}^{H \times W \times 1}$. This preprocess of watermarking message benefits the convergence of encoder and decoder\cite{tancik2020stegastamp}.
Considering the properties of bayer pattern in RAW images, we use two U-Net alike networks to compose our encoder, where one is used to process the concatenated tensor $\mathbf{T^{'}} \otimes \mathbf{R}$ of RAW image and watermarking, where $\otimes$ denotes the concatenation operation of tensor. The other is used to concatenated tensor $\mathbf{T^{'}} \otimes \mathbf{R_{d}}$ of demosaicing  RAW image $\mathbf{R_{d}} \in \mathbb{R}^{H \times W \times 4}$ and watermarking. Where demosaicing RAW image $\mathbf{R_{d}} = U( \mathbf{R_{re}} \otimes \mathbf{R_{b}} \otimes \mathbf{R_{gr}} \otimes \mathbf{R_{gb}})$, $\mathbf{R_{re}}$, $\mathbf{R_{b}}$, $\mathbf{R_{gr}}$, $\mathbf{R_{gb}}$ denotes the subimages of red pixels, blue pixels, green pixels in upper left and green pixels in lower right respectively and $U$ means the upsampling 2 times. 
% In subsequent ablation experiments, we also performed ablation experiments using only RAW images, only demosaiced images and both.
Finally, the results from two networks are averaged to get a single channel RAW residual $\mathbf{R_{r}} \in \mathbb{R}^{ H \times W \times 1}$, which will add to the original RAW image to get container RAW image $\mathbf{R_{e}} \in \mathbb{R}^{ H \times W \times 1}$.
We present examples of encoded images and residuals in Fig \ref{figresidual}. In subsequent ablation experiments, we also conduct the experiments that only uses RAW images, only demosaiced images and both. The motivation behind the utilization of the two-streamed encoder lies in our aspiration to incorporate supplementary pixel spatial information into the encoder through the implementation of a demosaicing module. It is crucial to emphasize that similar modules have been developed for existing deep ISP pipelines, indicating their effectiveness and relevance. By leveraging these modules, our objective is to enhance the overall performance and capabilities of our framework, resulting in improved image quality and fidelity of the encoded images. The ablation experiment serves as an illustration, demonstrating that the introduction of additional pixel spatial information can indeed enhance image quality and decoding accuracy.

% Figure environment removed


\textbf{Decoder}: 
The function of the decoder is to extract the watermarking from the RGB image produced by the deep ISP pipeline (image will also go through the distortion network if using it in training). We adopt the same decoder structure as Stegastamp~\cite{tancik2020stegastamp}. To extract the output message from the RGB images, we resort multiple convolutional layers and multiple Multilayer Perceptron (MLP), which are designed to extract relevant features and patterns from the image to recover watermarking. In the end, we use a Sigmoid function to map output values to a probability score between 0 and 1. The length of the output message is determined by the number of units in final MLP, which have the same length as the input message. The decoder network is supervised using the cross-entropy loss between copyright watermarking $\mathbf{M}$ and embeded watermarking $\mathbf{M_{d}}$.

\textbf{Discriminator}: 
Since the adversarial training improves the visual quality of encoded images~\cite{Zhu_2018_ECCV}, we introduce a WGAN-style discriminator~\cite{arjovsky2017wasserstein} in our RAWIW to supervise the training of our encoder. The discriminator network comprises a series of convolutional layers followed by a max pooling. It is used to predict whether a watermarking is encoded in the RGB image which is used as a perceptual loss for the whole framework. The discriminator is trained using an input image and its corresponding encoded image, where the Wasserstein loss function is employed as a supervised signal. 


\subsection{Deep ISP Pipeline}
RAWIW requires a module to transform RAW images into RGB images. Given the complexity of the traditional ISP pipeline, which includes some non-differentiable operations, we have chosen to utilize the deep ISP pipeline in our investigation. Specifically, we employ MW-ISPNet~\cite{ignatov2020aim} and AWNet~\cite{dai2020awnet}, as mentioned above. Additionally, we trained a UNet~\cite{ronneberger2015u}-style architecture ISP pipeline for conducting experiments. During the training process of the watermarking module, the parameters of the entire deep ISP pipeline are kept fixed. The reason for not modifying the parameters of the ISP pipeline to enhance the concealment of watermarking is that our primary objective is to enable the encoder to learn how to directly encode information into RAW images and the decoder to learn how to decode information from RGB images. Moreover, modifying the ISP pipeline could introduce additional computational complexity, potentially impacting the overall system performance. To compare the impact of different ISP pipelines on the entire framework, we conducted relevant ablation experiments, and the specific experimental results are presented in Section \ref{section:abst}.




\subsection{Distortion Network}
Since the deep ISP pipeline is trained to establish a mapping between RAW images and RGB images, this mapping can be significantly influenced by the ISP pipeline of data collection devices. However, for RAW image copyright protection, the watermarking must remain robust to traditional ISP pipelines as well. To enhance the decoder's robustness to these distortions (we consider the disparity between the deep ISP pipeline and traditional ISP pipelines as distortion), we introduce a distortion network in front of the decoder during training. In our approach, we address the visual gap between RGB images generated by different ISP pipelines, primarily manifested in differences in color temperature and compression. Additionally, to ensure the watermarking's robustness to transmission noises, we incorporate operations such as differentiable color temperature adjustment, JPEG compression~\cite{zhang2020towards}, Gaussian noise, saturation, contrast, and brightness adjustment into the distortion network. During training, the degrees of these distortions are randomly chosen within a range related to the training epoch, facilitating the decoder's convergence. Fig \ref{figdistortion} visually illustrates the image distortion process. The introduction of the distortion network enhances the decoder's capability to handle various distortions, ensuring robustness and maintaining the effectiveness of the watermarking even when faced with the challenges posed by traditional ISP pipelines. The details of the distortions are as follows:

% Figure environment removed

\textbf{Color temperature}: 
% Calculating correlated color temperatures across the entire gamut of daylight and skylight chromaticities
In order to adjust the color temperature of images, we use a Kelvin table which comprises of Kelvin values~\cite{hernandez1999calculating} of different color temperatures.
\begin{equation}
    (r^{'}, g^{'}, b^{'}) = (r \times \frac{r_{t}}{255}, g \times \frac{g_{t}}{255}, b \times \frac{b_{t}}{255}),
\end{equation}
where $r_{t}$, $g_{t}$, $b_{t}$ are the Kelvin values in color temperatures $t$. The color temperature $t$ is randomly selected from:$[6500 - \frac{5500}{\lambda  -\epsilon}, 6500 + \frac{5500}{\lambda  -\epsilon}]$, where $\epsilon$ is the current epoch number, $\lambda$ is the total epoch number of third stage. The meaning behind the article remains the same as stated above.

\textbf{JPEG compression}:
% Jpeg-resistant Adversarial Images
We use the differentiable JPEG compression proposed by~\cite{shin2017jpeg} to approximate rounding function:
\begin{equation}
    rounding(x) =\left\{\begin{matrix}
     x^{3} & |x| < 0.5 \\
     x & |x| \ge 0.5
\end{matrix}\right. .
\end{equation}
The JPEG quality factor is randomly selected from $[60+\frac{40}{\epsilon+1}, 100]$.

\textbf{Brightness, contrast, saturation, and noise}:
For the brightness, contrast and saturation, we use the functions in Kornia. The brightness, contrast and saturation factor is randomly selected from 
$[0-\frac{0.3}{\lambda  - \epsilon}, 0+ \frac{0.3}{ \lambda  - \epsilon}]$, $[0-\frac{0.1}{\lambda  - \epsilon}, 0+\frac{0.1}{\lambda  - \epsilon}]$ and $[0, 0+ \frac{0.1}{\lambda  - \epsilon}]$ respectively. At the same time, we use the Gaussian noise and the standard deviation is randomly selected from $[0,0+ \frac{0.05}{\lambda  - \epsilon}]$.

\subsection{Three-Stage Training and Loss Function}
\label{section:Three}
To strike an optimal balance between decoding accuracy, image quality, and robustness to noise, we have designed a three-stage training process. Each stage focuses on optimizing a specific goal: decoding accuracy, image quality, and robustness to noise, respectively. In the first stage, we prioritize achieving successful watermarking decoding, which comes at the cost of noticeable degradation in both RAW and RGB image quality. Moving to the second stage, we observe improvements in image quality, but this might result in increased vulnerability of the watermarking to distortion. In the third stage, we reinforce the resilience of the watermarking by gradually introducing noise augmentation. It is essential to note that our experimental findings demonstrate that attempting to simultaneously optimize all goals from the beginning leads to non-convergence of the loss function. Hence, our step-by-step approach allows us to effectively enhance different aspects of the watermarking process and achieve the desired trade-offs.

\textbf{The First Stage}:
At this stage, the optimization is solely focused on the RGB image decoder. The loss function for optimizing the decoder is defined as $L_{S1}$:
\begin{equation}
    L_{S1}(M, M_{d}) = L_{dec}(M, M_{d}) = Cross-Entropy(M, M_{d}).
\end{equation}


\textbf{The Second Stage}:
After the initial stage, our model can retrieve the hidden watermarking message from the encoded RGB images generated by the ISP pipeline. However, it should be noted that the appearance of the encoded RGB images closely resembles Quickly Respond (QR) code images. The observed phenomenon can be attributed to the absence of the visual quality constraints imposed on the model. Therefore, in this stage, we use $L_{S2}$ to the second stage as follow: 
\begin{equation}
    \begin{split}
    & L_{S2}(M, M_{d},R,R_{e},I_{o},I_{e}) = \lambda_{1} L_{dec}(M, M_{d}) + \lambda_{2}L_{2}(R,R_{e})  \\
    & + \lambda_{3}L_{2}(I_{o},I_{e}) + \lambda_{4}L_{P}(I_{o},I_{e}) + \lambda_{5}L_{d}(I_{o},I_{e}),
    \end{split}
\end{equation}
where $L_{2}$ is L2 norm loss, $L_{P}$ is the LPIPS~\cite{zhang2018perceptual} loss and $L_{d}$ is the discriminator loss.
$\lambda_{1}$, $\lambda_{2}$, $\lambda_{3}$, $\lambda_{4}$ and $\lambda_{5}$ are hyperparameter set to 2, 1, 1, 1, and 1, respectively.

\textbf{The Third Stage}:
Following the aforementioned two stages of training, the model has acquired the ability to encode the information into RAW images and extract the information from RGB images, simultaneously preserving the visual fidelity of both RAW and RGB images. However, the current watermarking is not robust to different distortions (\textit{e.g.,} Gaussian Noise, JPEG compression). 
To effectively extract the embeded watermarking from the distorted RGB images. We train our encoder and decoder using $L_{S3}$ as follows:
\begin{equation}
    \begin{split}
    & L_{S3}(M, M^{'}_{d},R,R_{e},I_{o},I_{e}) = \lambda_{1} L_{dec}(M, M^{'}_{d}) + \lambda_{2}L_{2}(R,R_{e})  \\
    & + \lambda_{3}L_{2}(I_{o},I_{e}) + \lambda_{4}L_{P}(I_{o},I_{e}) + \lambda_{5}L_{d}(I_{o},I_{e}).
    \end{split}
\end{equation}
The $L_{S3}$ is very similar to the $L_{S2}$, only the second parameter is different, where $M^{'}_{d}$ means decoded binary message from distorted RGB images and the hyperparameters setting is same as second stage. 
\begin{equation}
    M^{'}_{d} = D_{\theta_{D}}(\mathbf{N}(I_{e})),
\end{equation}
where $\mathbf{N}$ is present the distortion network, which can help the encoder to add anti-distortion watermarking into RAW images. 


% Figure environment removed
