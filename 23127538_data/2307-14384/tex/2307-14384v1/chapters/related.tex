\section{Related Work}

\subsection{Federated Learning for Non-IID Data}
%
In terms of the goal of optimization, there are mainly three categories of common FL work that tackles non-IID data:
% 
(1) Global performance, which modifies the local objectives with a regularization term to obtain a
well-performed global model~\cite{li2020federated}.
% 
FedProx~\cite{li2020federated} proposes an additional proximal term to local objective, which penalizes the updated local model that is far away from the global model.
% 
FedDYN~\cite{acar2020federated} and MOON~\cite{li2021model} regularize the model change with both historical global and local models simultaneously.
% 
(2) Local performance, which trains a personalized model for individual clients to enhance the performance of local models~\cite{t2020personalized};
% 
and (3) Global and local performance, which empirically decomposes the network in FL into the body for universal representation, and the head for personalized classification. 
%
 Fed-RoD~\cite{chen2021bridging} consists of two classifiers to maintain the local and global performance, respectively.
 % 
 However, no above work takes action to avoid class shifting.
 % 
 Few work fixes the classifier to fill this gap, e.g., FedBABU~\cite{oh2021fedbabu} and SphereFed~\cite{dong2022spherefed}.
 % 
FedBABU randomly initializes and fixes the classifier during training FL, which cannot guarantee the separation of different classes is distinguishable enough.
% 
SphereFed considers fixing the classifier in hyperspherical space.
% 
But SphereFed either lacks scalability in low-dimensional space, or generates sparse representation in high-dimensional space.
 % 
VFGNN~\cite{CCIjcai22} utilizes graph sturcture in vertical FL rather than horizontal FL.
Moreover, few existing work considers utilizing hierarchical structure and consistent aggregation, which degrades FL with non-IID data.

\subsection{Hyperbolic Representation Learning}
% 
Hyperbolic geometry is a non-Euclidean geometry, which can be constructed by various isomorphic models, e.g., Poincar\'e model~\cite{nickel2017poincare}. 
%
% 
Hyperbolic modeling has been leveraged in various deep networks, such as fully-connected layers~\cite{shimizu2020hyperbolic}, convolutional layers~\cite{shimizu2020hyperbolic}, recurrent layers~\cite{ganea2018hyperbolic}, classification layers~\cite{cho2019large,weber2020robust}, graph neural networks~\cite{liu2019hyperbolic,tan2022towards} and Transformer~\cite{ermolov2022hyperbolic}.
% 
However, the existing work overlooks taking the advantage of hyperbolic learning in FL.
% 
\cite{shen2021spherical,mettes2019hyperspherical,ghadimi2021hyperbolic}
treat additional prior using orthogonal basis or the prior knowledge 
 embeddings as hyperbolic prototypes, which are positioned as distant as possible from the origin, to avoid frequently updating in prototype learning.
% 
These motivate us to utilize hyperbolic prototypes
% take the hyperbolic prototypes as class prototypes 
in \modelname.
% 
% 
On the contrary, we contract the class prototypes away from the bound of the Poincar\'e ball model, 
% for the purpose of 
aiming to obtain more general class semantic information.
