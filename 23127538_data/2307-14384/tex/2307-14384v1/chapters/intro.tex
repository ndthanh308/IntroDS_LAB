\section{Introduction}
% background for FL, and why PFL
% Out of the privacy-leakage concerns,\xenia{del not main topic} 
Federated Learning (FL) trains a global model by collaboratively modeling decentralized data in local clients~\cite{mcmahan2017communication}.
%
Disappointingly, FL 
comes into a performance bottleneck in many real-world applications, where clients contain data with 
non-identical and independent distributions (non-IID) ~\cite{li2019convergence,zhao2018federated}. 
% 
Taking a hand-written recognition system as an example, 
different people have their personalized writing styles, making hand-written characters and letters differ in shape, size, and so on.
% 

% Figure environment removed
% \lwm{
% 1. Fig1, prototype shift; scattered; non-iid setting lead to ... SphereFed ...  }
% \lwm{
% 1. Fig1, hyperbolic space ... }

Existing FL work with non-IID data either improves the performance of the general global model or enhances the personalized local model.
% 
First, to obtain better global performance, a number of work tries to modify the local objectives by adding regularization, so as to make them consistent with generic global performance, e.g., FedProx~\cite{li2020federated}.
% 
Second, to enhance the performance of the local models, a series of studies encourage training a personalized model for individual clients with meta-learning~\cite{fallah2020personalized}, transfer learning\cite{luo2022disentangled}, and so on.
% 
Recently, FedBABU~\cite{oh2021fedbabu} and Fed-RoD~\cite{chen2021bridging} find it possible to enhance global and local models simultaneously, which decouple model in FL into two parts, i.e., one for global generalization and the other for personalization.
% 
% Though decomposition brings blessings with better performance, it is non-trivial to constructing an ideal personalized predictor that is scalable and determinant to inference.


Nevertheless, most existing methods overlook three issues in FL with non-IID data.
% 
Firstly, \textit{ class statistics shifting \textbf{(Issue 1)}} happens in FL with non-IID data.
% 
Clients have different class statistics information of local data distributions, i.e., the class prototypes, which will shift and bring trivial solutions in FL without fixing.
%
% 
As Fig.~\ref{fig:motivation}(a) shows, ignoring fixing class prototypes brings two inevitable limits.
% 
Client A fails to recognize the missing class, i.e., data corresponding to some class is missing, while client B causes the class overlapping, i.e., gathering the prototypes of blue and gray classes too tight to discriminate their image features.
% 
Though FedBABU~\cite{oh2021fedbabu} and SphereFed~\cite{dong2022spherefed} contribute to addressing this issue, they suffer from the \textit{dimension dilemma} problem, i.e., they either lack scalability in low-dimensional space, or generate sparse representation in high-dimensional space. 
% 
Secondly, current work only captures the semantic information of non-IID data, having \textit{the insufficient hierarchical information utilization} issue \textbf{(Issue 2)}.
% 
% 
As depicted in Fig.~\ref{fig:motivation}(b), it is hard to gather the data samples of the same class together without hierarchical information.
% 
Hierarchical information can be helpful to group data samples and generate fine-grained representations, which can further bring prediction gains.
% 
% 
To take this advantage, hyperbolic models are used for continuously capturing the hierarchical structure of data in the low-dimensional space~\cite{liu2020hyperbolic,linial1995geometry}, whose effectiveness is proved in computer vision~\cite{khrulkov2020hyperbolic}, recommender systems~\cite{tan2022enhancing}, and natural language processing~\cite{chen2022fully}. 
% 
% As depicted in Fig.~\ref{fig:motivation}(a), both Euclidean tree and hyperbolic model, e.g., Poincar\'e ball, can capture the partial order structure inherent in data, which tights the data samples of the same class together for determined prediction.
% 
% For example, it is vital for prediction with the structural information, since the partial-order relationship pulls the pairs of the same class closer than the pairs with different classes. \ccc{}
% 
% Compared with modeling data in Euclidean tree, hyperbolic model is more effective in continuously capturing the hierarchical structure of data in the low-dimensional space~\cite{liu2020hyperbolic,linial1995geometry}.
% 
However, how to integrate existing FL methods with the hierarchical information in hyperbolic space remains unknown.
% 
Lastly, \textit{the inconsistency in aggregating clients \textbf{(Issue 3)}} deteriorates the performance of current FL methods as well. 
% 
In practice, clients usually have statistically heterogeneous data.
% 
% It is practical to involve some clients with statistically heterogeneous data in real-world FL applications. \ccc{In practice, clients usually have statistically heterogeneous data. } 
% 
Existing aggregation methods, e.g., weighted average with data amounts, result in the aggregated global model deviating from the optimal global model. 
% 


In this work, we propose \modelname~which contains three modules to address the above issues, respectively. 
% , which explores hyperbolic prototypes and consistency updating in FL. 
%
% Specifically, \modelname consists of three modules, i.e., \textbf{hyperbolic prototype Tammes initialization (HPTI)}, \textbf{hyperbolic prototype learning (HPL)}, and \textbf{consistent aggregation (CA)} to address three issues in non-IID setting.
% (sol2)
To solve \textbf{\textit{Issue 1}} and avoid dimension dilemma, \modelname~contains \textbf{hyperbolic prototype Tammes initialization (HPTI)} module in server.
% 
The server first uses Tammes prototype initialization (TPI) to construct uniformly distributed class prototypes for the whole class set in hyperbolic space. 
% 
Then the server fixes the position of class prototypes, and shares class prototypes to initialize the hyperbolic predictors of clients.
% 
In this way, \modelname~not only guides the consistent and separated criteria, but also introduces the statistics of missing class, both of which encourage discriminative and fine-grained feature representation for non-IID data.
%(sol1) 
To avoid \textbf{\textit{Issue 2}}, 
% (advantage2) 
with the supervision of hyperbolic prototypes, \textbf{hyperbolic prototype learning (HPL)} module in each client pulls the data sample close to its ground truth class prototype, and pushes other data samples away.
% 
Thus \modelname~enjoys the benefits of predicting with hierarchical information.
% (sol3) 
% To minimize the performance deterioration caused by 
To tackle 
\textbf{\textit{Issue 3}}, \modelname~has a \textbf{consistent aggregation (CA)} module that resolves the inconsistent deviations from clients to server by solving a multi-objective optimization with Pareto constraints.
% (advantage3) 
Hence \modelname~obtains consistent updating direction among clients, without the need of cumbersome grid search for aggregating clients. 


In summary, we are the first, as far as we know, to explore hyperbolic prototypes in FL with non-IID Data. 
%
We contribute in:
%
(1)
We adopt uniformly distributed and fixed class prototypes in hyperbolic space to alleviate the 
impact of statistics shifting. 
%
(2)
We 
sufficiently leverage hyperbolic representation space to capture hierarchical information for FL.
% 
(3) We optimize the aggregation of different client model to a Pareto stationary point, minimizing the impact of inconsistent clients deviations.
%
(4) Extensive experiments on four benchmark datasets prove the effectiveness of \modelname.







