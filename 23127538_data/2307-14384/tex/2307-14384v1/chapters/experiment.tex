\section{Experiments and Discussion}

% Questions

\subsection{Experimental Setup}
\paragraph{Datasets.}
% ref Personalized fedrated learning (self-FL)
% ref sampling from MOON
We use four public datasets in torchvision\footnote{https://pytorch.org/vision/stable/index.html}, i.e., EMNIST by Letters~\cite{cohen2017emnist}, Fashion-MNIST (FMNIST)~\cite{xiao2017fashion}, Cifar10, and Cifar100~\cite{krizhevsky2009learning}, which are widely-used in the recent FL work~\cite{chen2021bridging,oh2021fedbabu,li2021model}.
%  
% 
There are two evaluation goals for FL with non-IID data, i.e., global performance (G-FL) and local personalized performance (P-FL).
% 
G-FL tests the global model aggregated in the server by evaluating the \textit{test set} published in the torchvision.
% 
In P-FL, we simulate local data distribution using the \textit{train set} published in torchvision to evaluate the  local models.
% 
For all datasets, we simulate the non-IID data distributions following \cite{hsu2019measuring,li2021model}. 
%
%  
Specifically, we sample a proportion of instances of class $j$ to client $k$ with Dirichlet distribution, i.e., $p_{j,k} \sim {Dir}_N (\alpha)$,  where $\alpha$ denotes the non-IID degree of every class among the clients.
% 
The smaller $\alpha$ indicates the more heterogeneous data distribution.
% 
% 
We 
sample 75\% of local data as \textit{local training set} and the remaining as \textit{local test set}.
% 


\paragraph{Comparison Methods.}
% ref Federated Learning with Label Distribution Skew via Logits Calibration
% 
We compare \modelname~with three categories of state-of-the-art approaches according to their optimization goals, i.e., (1) optimizing global model: \textbf{FedAvg}~\cite{mcmahan2017communication}, \textbf{FedProx}~\cite{li2020federated}, \textbf{SCAFFOLD}~\cite{karimireddy2019scaffold}, \textbf{FedDYN}~\cite{acar2020federated}, \textbf{MOON}~\cite{li2021model}, (2) optimizing local personalized models: \textbf{FedMTL}~\cite{smith2017federated}, \textbf{FedPer}~\cite{arivazhagan2019federated}, \textbf{pFedMe}~\cite{t2020personalized}, \textbf{Ditto}~\cite{li2021ditto},
\textbf{APPLE}~\cite{luo2021adapt}, and (3) optimizing both global and local models: \textbf{Fed-RoD}~\cite{chen2021bridging}, \textbf{FedBABU}~\cite{oh2021fedbabu}, and \textbf{SphereFed}~\cite{dong2022spherefed}. 
% 
% We provide the details of these methods in Appendix B.



\paragraph{Implementation Details.} 
% 
We adopt ConvNet~\cite{lecun1998gradient} as a feature extractor for EMNIST and FMNIST, while ResNet~\cite{he2016deep} for Cifar10 and Cifar100.
% 
% 
We set all of the datasets with batch size as 128 and embedding dimension as 20.
% 
 For \modelname, we choose RSGD~\cite{bonnabel2013stochastic} as the optimizer, set the learning rate $lr=0.3$, the margin $m=3$, and the slope degree $s=0.9$.
% 
% 
% In our basic setting,
We conduct training for all of the methods with 5 local epochs per round until converge.
% 
% 
We evaluate both G-FL and P-FL by top-1 accuracy.
% 
We set the non-IID degree $\alpha=\{0.1, 0.5,5\}$, respectively, for evaluating the performance of different methods.
% 
% We depict the data distributions in Appendix A.
% % 
% % 
% We tune the hyper-parameters of all methods to their best values for a fair comparison, which is detailed in Appendix B.


% Figure environment removed



\begin{table*}[]
\renewcommand\arraystretch{0.5}
\setlength\tabcolsep{6pt}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|ccc|ccc|ccc|ccc}
\toprule
Dataset       & \multicolumn{3}{c}{EMNIST}   & \multicolumn{3}{c}{FMNIST}   & \multicolumn{3}{c}{Cifar10} & \multicolumn{3}{c}{Cifar100} \\
\midrule
Method \textbackslash NonIID        & Dir(0.1) & Dir(0.5) & Dir(5) & Dir(0.1) & Dir(0.5) & Dir(5) & Dir(0.1) & Dir(0.5) & Dir(5) & Dir(0.1)  & Dir(0.5) & Dir(5) \\
\midrule
FedAvg & 88.86  & 92.86  & 93.31  & 78.51  & 86.75  & 88.75  & 36.82  & 62.44  & 67.53  & 27.61  & 29.26  & 30.12  \\
FedProx                        & 90.75   & 93.27   & 93.65   & 78.49   & 86.57   & 88.68   & 37.95   & 63.48   & 67.23   & 26.91   & 29.77   & 29.98   \\
SCAFFOLD                       & 90.35   & 93.38   & 93.70   & 79.20   & 87.03   & 88.80   & 33.10   & 66.99   & 70.53   & 29.57   & 33.25   & 33.86    \\
FedDYN                         & 91.30   & 92.63   & 93.13   & 85.25   & 89.46   & 90.51   & 35.16   & 65.07   & 69.04   & 29.16   & 31.49   & 32.21   \\
MOON                           & 91.97   & 93.50   & 93.91   & 83.78   & 90.27   & 91.08   & 33.54   & 60.23   & 62.45   & 22.86   & 24.49   & 25.99   \\
\midrule
Fed-RoD                        & 89.42  & 92.80  & 93.52  & 77.26  & 89.17  & 90.78  & 37.79  & 66.90  & 70.81  & 17.06  & 24.32  & 31.99  \\
FedBABU & 86.34 & 91.95 & 92.74 & 74.31 & 82.44 & 85.21 & 37.90 & 60.11 & 65.16 & 22.98 & 24.56 & 23.89 
\\
SphereFed                      & 93.25   & 93.93   & 94.07   & 88.55   & 90.55   & 91.17   & 32.41   & 70.02   & 70.13   & 22.37   & 24.96   & 24.72   \\
\midrule
\modelname-\texttt{Geodesic}          & 93.35   & 94.13   & 94.43   & 79.00   & 90.76   & 91.48   & 34.26   & 65.53   & 68.65   & 26.05   & 29.07   & 26.62   \\
% \modelname-$\mathbb{E}$                     & 92.81   & 93.74   & 94.34   & 84.34   & 90.63   & 91.09   & 37.22   & 68.48   & 71.94   & 28.87   & 28.25   & 27.37   \\
\modelname-\texttt{Shared}                & 93.82   & 93.98   & 94.16   & 85.29   & 90.48   & 91.23   & 36.44   & 69.21   & 73.16   & 33.62   & 35.31   & 36.30   \\
\modelname-\texttt{Averaged}              & 93.65   & 94.29   & 94.45   & 87.91   & 90.98   & 91.69   & 36.69   & 70.12   & 70.81   & 33.39   & 36.33   & 36.93   \\
\modelname & \textbf{94.00}     &   \textbf{94.33}    &   \textbf{94.46} &   \textbf{89.38}   &   \textbf{91.16}   &   \textbf{91.83} & \textbf{38.03}   &   \textbf{71.25}   &   \textbf{75.22} &   \textbf{33.93}    &   \textbf{38.89}   &   \textbf{37.50}\\
 \bottomrule
\end{tabular}
}
\caption{G-FL accuracy ($\%$) of the global model. We bold  the best result.}
\label{tb:G-FL}
\end{table*}

\begin{table*}[]

\renewcommand\arraystretch{0.5}
\setlength\tabcolsep{6pt}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|ccc|ccc|ccc|ccc}
\toprule
Dataset       & \multicolumn{3}{c}{EMNIST}    & \multicolumn{3}{c}{FMNIST}    & \multicolumn{3}{c}{Cifar10}  & \multicolumn{3}{c}{Cifar100} \\
\midrule
Method \textbackslash NonIID        & Dir(0.1) & Dir(0.5) & Dir(5)  & Dir(0.1) & Dir(0.5) & Dir(5)  & Dir(0.1) & Dir(0.5) & Dir(5)  & Dir(0.1) & Dir(0.5) & Dir(5)  \\
\midrule
FedMTL                         & 96.28  & 91.08  & 88.21  & 97.39  & 90.89  & 85.71  & 90.57  & 65.68  & 48.31  & 46.28  & 24.72  & 11.64  \\
FedPer                         & 97.23  & 94.19  & 92.97  & 96.87  & 90.54  & 87.21  & 91.93  & 75.63  & 67.20  & 49.69  & 32.75  & 23.89  \\
pFedMe                         & 97.23  & 94.03  & 92.62  & 96.03  & 88.57  & 84.95  & 92.45  & 77.64  & 66.09  & 55.20  & 36.34  & 28.19  \\
Ditto                          & 97.72  & 95.32  & 94.30  & 97.79  & 93.75  & 92.05  & 91.34  & 74.35  & 69.94  & 47.08  & 31.89  & 27.04  \\
APPLE    &
97.19  & 94.08  & 92.63  & 96.79  & 90.49  & 86.59  & 89.85  & 67.96  & 56.13  & 42.94  & 24.61  & 20.39  \\
\midrule
Fed-RoD                        & 97.76  & 95.31  & 93.99  & 97.21  & 93.48  & 91.62  & 91.24  & 72.11  & 71.33  & 35.03  & 27.70  & 32.22  \\
FedBABU                        & 97.36  & 94.17  & 92.81  & 96.64  & 89.59  & 85.57  & 92.04  & 64.05  & 63.46  & 29.30  & 25.05  & 24.01  \\
SphereFed     & 93.61  & 94.11  & 94.34  & 88.91  & 90.87  & 91.88  & 91.68  & 78.39  & 72.86  & 40.68 & 32.63  & 34.90    \\
\midrule
\modelname-\texttt{Geodesic}          & 97.97   & 95.60   & 94.48   & 97.00  & 93.98   & 91.89   & 91.68   & 72.20   & 69.02   & 29.70   & 27.11   & 27.03   \\
% \modelname-$\mathbb{E}$                     & 92.80   & 93.82   & 93.99   & 84.88   & 90.72   & 91.80   & 92.66   & 76.55   & 69.67   & 27.73   & 25.76   & 26.18   \\
\modelname-\texttt{Fixed}                 & 97.96   & 95.65   & 94.45   & 96.53   & 92.66   & 90.85   & 90.57   & 79.66   & 59.38   & 49.08   & 20.20   & 11.31   \\
\modelname-\texttt{Shared}                & 97.50   & 95.66   & 94.39   & 97.50   & 93.00   & 91.52   & 92.69   & 77.43   & 71.62   & 47.97   & 33.16   & 35.19   \\
\modelname-\texttt{Averaged}              & 97.71   & 95.13   & 94.46   & 97.83   & 93.73   & 91.96  & 92.63   & 70.53    & 69.02    & 53.50   & 36.49    & 35.79    \\
\modelname & \textbf{98.76}  & \textbf{96.11} & \textbf{94.53}  & \textbf{98.13}  & \textbf{94.19}  & \textbf{92.08}  & \textbf{93.49} & \textbf{83.17}  & \textbf{74.94}  & \textbf{59.85} & \textbf{40.94}  & \textbf{36.80}  \\
\bottomrule
\end{tabular}
}
\caption{P-FL accuracy ($\%$) of the local model. We bold  the best result.}
\label{tb:P-FL}
\end{table*}

% Figure environment removed

% Figure environment removed




% Figure environment removed



% Figure environment removed


% Please add the following required packages to your document preamble:
\begin{table}[]
\renewcommand\arraystretch{0.5}
\setlength\tabcolsep{6pt}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccccccccc}
\toprule
Dataset  & Goal & 2       & 5       & 10      & 20      & 25      & 50      & 100     \\
\midrule
FMNIST
% \multirow{2}{*}{ } 
& P-FL & 88.91  & 93.20  & 93.84  & 93.98  & 93.70  & 93.73  & 93.66  \\
 ($\alpha$=0.5)       & G-FL    & 88.59  & 91.07  & 90.91  & 91.16 & 90.74  & 90.74  & 90.74  \\
\midrule
Cifar10
% \multirow{2}{*}{} 
& P-FL & 72.82  & 80.55  & 82.83  & 83.17  & 82.50  & 83.15  & 82.76  \\
  ($\alpha$=0.5)        & G-FL    & 60.87  & 68.82  & 69.84  & 71.25  & 70.67  & 71.51  & 71.69 
        \\
\bottomrule
\end{tabular}
}
\caption{Effect of the representation dimension in \modelname.}
\label{tb:dimension}
\end{table}

\subsection{Empirical Results}

\paragraph{Performance Comparison.}
We run each method 5 times and report the performance of G-FL and P-FL in Tab.~\ref{tb:G-FL}-\ref{tb:P-FL} with the average value, respectively.
% 
% 
There are mainly three observations.
% 
(1) \textbf{In terms of G-FL} evaluated in Tab.~\ref{tb:G-FL}, generally speaking, the performance of G-FL is increasing along with the increase of non-IID degree $\alpha$.
% 
It states that the heterogeneity of data distribution deteriorates the performance of FL methods.
% 
FedAvg takes no measure to handle non-IID, which is worse than most methods.
% 
Similarly, FedBABU is a variant of FedAvg that fixes the randomly initialized classifier, perform inferior as well.
% 
It means that simply fixing the classifier initialized by random is not enough for FL with non-IID data.
% 
% MOON is effective on small EMNIST and FMNIST, but they have much worse accuracy on Cifar10 and Cifar100.
% % 
% FedDYN and Fed-ROD maintain comparable results on four datasets.
% % 
% FedBABU is less effective, which means that simply fixing the classifier initialized by random is not enough for FL with non-IID data.
%
% SphereFed is the runner-up on EMNIST and FMNIST, but it fails to be effective in complicated datasets, i.e., Cifar10 and Cifar100.
% % 
% It states simply fixing the classifier head but not utilizing the hierarchical information, will hinder the scalability of FL methods for complicated datasets.
% % 
% SCAFFOLD is the runner-up on Cifar10 ($\alpha=0.5$ and $\alpha=5$) and Cifar100 ($\alpha=0.5$ and $\alpha=5$), which means it can handle complicated datasets with less heterogeneous data distribution.
% % 
(2) \textbf{In terms of P-FL} evaluated in Tab.~\ref{tb:P-FL},
% 
the methods of the third category can achieve accuracy similar as the second category on FMNIST and EMNIST, but cannot maintain the personalization on Cifar100.
% 
This indicates that simply fixing class statistics in FL is  less promising than personalization strategies on complex dataset. 
% 
SphereFed is the runner-up of Cifar100 ($\alpha=5$), but it fails to perform well on more heterogeous setting, i.e., Cifar100 ($\alpha=\{0.1, 0.5\}$).
% 
This phenomenon shows that \modelname~performs better for FL with non-IID data, as a FL method of the third category.
% 
% 
% (3) \textbf{In terms of the performance of methods simultaneously optimizing the local and global model}, FedBABU and SphereFed achieve inferior performance on Cifar10 and Cifar100 than global model personalization methods in G-FL and learning personalized model in P-FL.
% 
% This indicates that naively fixing the classifier head will hinder the scalability of FL methods for complicated datasets.
(3) \textbf{In terms of the performance of \modelname}, \modelname~achieves the best results in all kinds of non-IID degrees and different datasets, which verifies the efficacy of \modelname~for non-IID problems. 
% 
% Compared with EMNIST and FMNIST, 
% \modelname~achieves fairly more performance enhancement on Cifar10 and Cifar100.
% % % 
% It states the necessity of improving the sufficient representation for \xenia{complex datasets}.
% better PFL performance.
% % 
 In the results of Cifar100,
\modelname~significantly outperforms all of the comparison algorithms listed both in G-FL and P-FL, in terms of at least 10.75\%  and 5.44\%, respectively.
% 
This shows that \modelname~can capture more fine-grained representation to improve performance, especially on large and complicated datasets with low dimensional representation.


\noindent\textbf{Visualization. }
To verify the benefits of fixing class statistics in hyperbolic space, we utilize UMAP~\cite{mcinnes2018umap} to visualize the G-FL hidden representations of the global model in \modelname, the runner-up of G-FL,i.e., SphereFed, and P-FL, i.e., Ditto, and FedBABU on FMNIST ($\alpha=0.5$) in Fig.~\ref{fig:visualization}. 
% 
we can find that: (1) Compared with Ditto, methods fixing the class statistics attain more gathered representations.
% 
(2) Though FedBABU fixes the classifier, the randomly initialized classifier is limited in resolving class overlapping.
% 
(3) Compared with SphereFed whose classifier is initialized with a set of orthogonal basis in hypersphere,  \modelname~concentrates the hidden representation tighter due to sufficiently utilizing the hierarchical information.


\paragraph{Ablation Studies.}
% 
% hyperbolic vs Euclidean 
% triplet -> representation no hyperbolic
% triplet for euclidean
% 
We consider four variants of \modelname: 
% (1). \modelname~using fixed and shared prototypes with contrastive learning in Euclidean space (Cosine Distance), i.e., \modelname-$\mathbb{E}$,
(1) \modelname~uses geodesic
% (Eq.~\eqref{eq:geo_dis}) 
as metric, i.e., \modelname-\texttt{Geodesic},
(2) \modelname~uses fixed class prototypes only, i.e., \modelname-\texttt{fixed}, (3) \modelname~uses shared class prototypes only, i.e., \modelname-\texttt{shared}, and (4) \modelname~uses weighted average aggregation by data amounts,
% using the amounts of data as weights to average aggregation,
i.e., \modelname-\texttt{Averaged}.
% 
% The former is built by substituting the triplet loss in Eq.~\eqref{eq:loss} with the original geodesic distance in Eq.~\eqref{eq:geo_dis}.
% % 
% The latter is a Euclidean version of \modelname, i.e., a federated learning framework optimizes cosine distance with fixed Euclidean classifier. 
% 
% 
From Tab.~\ref{tb:G-FL}-\ref{tb:P-FL}, we can discover that:
% 
All of the variants decrease their performance compared with \modelname.
% 
These results identify that all of the components of \modelname, 
% i.e., hyperbolic triplet loss, fixed and shared class prototypes, and consistent updating, 
contribute to performance enhancement.
% 

\noindent\textbf{Hyper-parameters Sensitivity.}
To study the sensitivity of hyper-parameters, we compare the performance of FMNIST and Cifar10 by varying the local epochs $E= \{5, 10,20, 50\}$ in Fig.~\ref{fig:hyperparameter_E}, slope degree $s=\{0.2, 0.5, 0.8, 0.9, 0.95, 1\}$ in Fig.~\ref{fig:slope-degree}, margin $m=\{1, 1.5, 2,2.5, 3, 3.5, 4, 4.5, 5\}$ in Fig.~\ref{fig:margin_hyper}, number of clients $K= \{5, 10, 20,50,100\}$ in Fig.~\ref{fig:n_clients}, and the dimension of features representation $d=\{2,5,10,20,25,50,100\}$ in Tab.~\ref{tb:dimension},  respectively.
% 
We can conclude that:
(1) The increase of local epochs per communication round decreases the performance of FL methods. 
% 
(2) \modelname~nearly converges when $s=0.9$, which indicates that positioning the class prototypes in 90\% of radius achieves a balance between the best model performance and the most general class information. 
% 
% We evaluate the behavior of impacts of local epochs and find the performance is shrinking with additionally more epochs per round.
% 
(3) The performance of \modelname~varying by $m$ forms a slight bell curve.
% , which means $m=3$ contributes to best performance.
% 
(4) \modelname~outperforms the runner-up methods in all cases of different number of clients.
% 
(5) The dimension changes slightly affect the performance of \modelname, proving that \modelname~is proficient in low-dimensional representation. 
% 



% local epochs 
% dim of model
% reg constant (e.g., margin)
% clients number
% model architecture