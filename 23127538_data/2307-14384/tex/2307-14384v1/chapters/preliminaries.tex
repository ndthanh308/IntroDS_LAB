\section{Preliminary: Poincar\'e Ball Model}
% 用自己的话表达 Hyperbolic Image Segmentation
% ref Hyperbolic visual embedding learning for zero-shot recognition
Poincar\'e ball model is one of the common models in hyperbolic space, which is a type of Riemannian manifold $\mathcal{M}$ with constant negative curvature.
%
In this work, $||\boldsymbol{x}||_2 = \sqrt{\Sigma_{i=1}^n x_i^2}$ is a Euclidean norm.
% 
The Poincar\'e is an open ball model in $n$-dimensional hyperbolic space defined as $(\mathbb{P}^n, g^{\mathbb{P}})$, where $\mathbb{P}^n = \{\boldsymbol{x}\in \mathbb{R}^n: ||\boldsymbol{x}||_2 < 1\}$ 
and $g^{\mathbb{P}}$ is the Riemannian metric of a Poincar\'e ball.
% 
$g^{\mathbb{P}}$ is conformal to the metric of Euclidean space $g^\mathbb{E}$, i.e.,
$
g^{\mathbb{P}}_{\boldsymbol{x}} = \lambda_{\boldsymbol{x}}^2 g^\mathbb{E},
$
where $\lambda_{\boldsymbol{x}}=\frac{2}{1-||\boldsymbol{x}||_2^2}$ is the conformal factor.
% \xenia{and $g^\mathbb{E}$ is the metric of Euclidean space}.
%
Given two points in the Poincar\'e ball model, i.e., $\boldsymbol{x}_1, \boldsymbol{x}_2\in \mathbb{P}^n$, the M\"obious addition is defined as:
$$\boldsymbol{x}_1\oplus \boldsymbol{x}_2 = \frac{(1+2\langle\boldsymbol{x}_1, \boldsymbol{x}_2\rangle + ||\boldsymbol{x}_2||_2^2)\boldsymbol{x}_1 +(1-||\boldsymbol{x}_1||_2)\boldsymbol{x}_2^2}{1+2\langle\boldsymbol{x}_1, \boldsymbol{x}_2\rangle + ||\boldsymbol{x}_1||_2^2||\boldsymbol{x}_2||_2^2 }$$
%
We can define the geodesic, i.e., the shortest distance between these two points in the Poincar\'e ball as below:
\begin{equation}\label{eq:geo_dis}
    d(\boldsymbol{x}_1, \boldsymbol{x}_2) = \operatorname{arcosh}(1 + \frac{2||\boldsymbol{x}_1-\boldsymbol{x}_2||_2^2}{(1 -  ||\boldsymbol{x}_1||_2^2)(1 - ||\boldsymbol{x}_2||_2^2)}).
\end{equation}
%
%
For a point $\boldsymbol{x}$ in a manifold $\mathcal{M}$, the tangent space $T_{\boldsymbol{x}} \mathcal{M}$ is a vector space comprising all directions that are tangent to $\mathcal{M}$ at $\boldsymbol{x}$.
% \xenia{For a point $x$ in a manifold $\mathcal{M}$, one can define the tangent space $T_x \mathcal{M}$ of $\mathcal{M}$ at $x$ as a vector space that contains all possible directions in which one can tangentially pass through. An inner product can be defined on $T_x \mathcal{M}$. }
% 
Exponential map $\exp_{\boldsymbol{x}}: T_{\boldsymbol{x}} \mathcal{M} \rightarrow \mathcal{M}$ is a transformation that 
projects any point $\boldsymbol{u}$ from the Euclidean tangent space to the Poincar\'e ball referred by point $\boldsymbol{x}$, defined as:
\begin{equation}
\exp _{\boldsymbol{x}}(\boldsymbol{u})=\boldsymbol{x} \oplus\left(\tanh \left(\frac{\lambda_{\boldsymbol{x}}\|\boldsymbol{u}\|_2}{2}\right) \frac{\boldsymbol{u}}{\|\boldsymbol{u}\|_2}\right).
\end{equation}
%
The reverse of $\exp_{\boldsymbol{x}}$ is logarithmic map, i.e., $\log _{\boldsymbol{x}}: \mathcal{M} \rightarrow T_{\mathbf{x}} \mathcal{M}$, which projects hyperbolic vector back to Euclidean space. 
% 
% %
% \subsection{Riemannian Stochastic Gradient Descent}
% In Euclidean space, the stochastic gradient descent (SGD) algorithm for optimizing an objective loss function $\mathcal{L}$ is:
% %
% \begin{equation}\label{eq:SGD}
% \boldsymbol{\theta}^{t+1} = \boldsymbol{\theta}^t - \eta \nabla\mathcal{L},     
% \end{equation}
% %
% where $\boldsymbol{\theta}^t$ represents the parameters at iteration step $t$, $\eta$ is the learning rate and $\nabla\mathcal{L}$ is the Euclidean gradient of the loss function.
% %
% To generalize it in a Riemannian manifold $\mathcal{M}$, \cite{bonnabel2013stochastic} proposes the Riemannian stochastic gradient descent(RSGD), which is defined as:
% %
% \begin{equation} \label{eq:RSGD}    
% \boldsymbol{\theta}^{t+1} = \exp_{\boldsymbol{\theta}^t}(-\eta\nabla_R\mathcal{L}).
% \end{equation}
% %
% In Poincar\'e ball model, the Riemannian gradient $\nabla_R\mathcal{L} = (g^{\mathbb{P}}_{\boldsymbol{\theta}^t})^{-1}\nabla \mathcal{L}$ is obtained by rescaling the Euclidean gradient with the inverse of the Poincar\'e ball metric tensor. 
% %
% % It is noticeable that Euclidean space is the 0-curvature manifold, i.e., $\exp_{\boldsymbol{x}}(\boldsymbol{u})=\boldsymbol{x} + \boldsymbol{u}$, at which Eq.~\eqref{eq:SGD} and Eq.~\eqref{eq:RSGD} are identical.


% % To find the optimal solution of a loss function $\mathcal{L}: \mathbb{P}^n\rightarrow \mathbb{R}$ with parameters $\boldsymbol{\theta}$, the Rienmannian Stochastic Gradient Descent is defined as 
% % \begin{equation} \label{eq:RSGD}    
% % \boldsymbol{\theta}^{t+1} = \exp_{\boldsymbol{\theta}}(-\eta_t\nabla_R\mathcal{L})
% % \end{equation}
% % ~\cite{bonnabel2013stochastic},
% % %
% % where $\eta$ is the learning rate and $\nabla_R \mathcal{L}=((\lambda_{\boldsymbol{x}})^2 \mathbb{I}^n)^{-1} \nabla\mathcal{L}$ is the Riemannian gradient.


