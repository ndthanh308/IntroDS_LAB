\section{Method}

\subsection{Problem Statement}
% ref Model-Contrastive Federated Learning
% ref https://www.aaai.org/AAAI21Papers/AAAI-5802.HuangY.pdf
% ref Learning to Generalize in Heterogeneous Federated Networks
In FL with non-IID setting, we assume there are $K$ clients, containing their own models and local datasets, and a central server with global model aggregated from clients.
% 
Suppose a dataset $\mathcal{D}$ has $C$ classes indexed by $[C]$, where $[C]$ means the full set of labels in $\mathcal{D}$.
% , and each sample in $\mathcal{D}$ denotes as $(\boldsymbol{x}, y) \in \mathcal{X} \times [C]$.}
%
Each client $k$ has access to its private local dataset $\mathcal{D}_k=\{ \boldsymbol{x}_{k,i}, y_{k,i}\}_{i=1}^{N_k}$, containing $N_k$ instances sampled from distinct distributions.
% 
Therefore, we get $\mathcal{D} =\cup_{k\in [K]} \mathcal{D}_k$, where the data distributions of different $\mathcal{D}_k$ are different.
%
The overall objective in FL with non-IID setting is defined as below: 
\begin{equation}
\small
{\operatorname{min}} \mathcal{L}(\boldsymbol{\theta}_{1}, \dots, \boldsymbol{\theta}_{K}; \boldsymbol{p})=\Sigma_{k=1}^K p_k \mathbb{E}_{(\boldsymbol{x}, {y})\sim D_k}[\mathcal{L}_k(\boldsymbol{\theta}_{k}; (\boldsymbol{x}, {y})],
\label{eq:pfl_problem}
\end{equation}
where $\mathcal{L}_k (\cdot)$ is the model loss at client $k$, and $p_k$ indicates its weight ratio for aggregating.
% makes all of the local Poincar\'e models coincide with each other

% Figure environment removed



\subsection{Framework Overview}
%
To explain how \modelname~solve the problem in FL with non-IID data, i.e., Eq.~\eqref{eq:pfl_problem}, we introduce the framework overview of \modelname.
% 
% 
In Fig.~\ref{fig:HyperbolicFed}, there are a server and K clients.
% 
Each client or server, similarly consists of a feature extractor, an exponential map, and a Poincar\'e ball predictor. 
% 
%
%
The feature extractor $\mathcal{F}(\cdot): \mathcal{X}\rightarrow \mathbb{R}^d$ maps an input data $\boldsymbol{x}$ into a $n$-dimensional vector $\boldsymbol{z}=\mathcal{F}(\boldsymbol{x})$ as feature representation.
% 
Then we get $\operatorname{exp}_{\boldsymbol{o}} (\boldsymbol{z})$ by
leveraging exponential map on the feature representation $\boldsymbol{z}$ to the Poincar\'e ball space referred by the origin $\boldsymbol{o}$. 
% 
Finally, the Poincar\'e ball predictor $h(\cdot):\mathbb{R}^d \rightarrow \mathcal{Y}$ decides class label for input data based on the representation $\operatorname{exp}_{\boldsymbol{o}} (\boldsymbol{z})$ in Poincar\'e ball. 
% 
All Poincar\'e ball predictors of server and clients are fixed and shared. % in FL modeling.

As Fig.~\ref{fig:HyperbolicFed} shows, there are mainly three steps in  \modelname.
% 
(1) The server in \modelname~leverages hyperbolic prototype Tammes
initialization (HPTI) module to construct a full set of uniformly-distributed class prototypes 
for the Poincar\'e ball predictor by Tammes prototype initialization (TPI), contracts class prototypes close to origin,
and shares Poincar\'e ball predictor with fixed class prototypes to all of the clients.
% 
(2) Each client models local data distribution independently with the hyperbolic prototype learning (HPL) module, then sends the parameters of the local model to the server for aggregation.
% 
(3) Consistent aggregation (CA) module in the server updates the global model parameters using consistent updating (CU) to mitigate the inconsistent deviations from clients to server. After that, the server sends the new global model parameters back to clients.
% 
This communication between server and clients, i.e., steps 2-3, iterates until the performance converges.
% 

\subsection{Hyperbolic Prototype Tammes Initialization}
% \subsection{Hyperbolic Prototype Modelling}
\paragraph{Motivation.} In this section, we devise HPTI module in server to resolve two limits brought from the class statistics shifting, i.e., missing class and class overlapping, as described in Fig.~\ref{fig:motivation}(a).
% 
To bypass the dilemma of choosing a dimension, HPTI explores the class statistics in the hyperbolic space, which is scalable and effective in modeling data with low-dimensional space. 
% 
% 
Firstly, HPTI uses Tammes prototype
initialization (TPI) to construct uniformly distributed and distinguishable class prototypes for the entire class set. 
% 
Then HPTI fixes the position of the class prototypes on the Poincar\'e ball predictor.
% 
Lastly, HPTI sends the Poincar\'e ball predictor with fixed hyperbolic class prototypes, including the missing classes, to clients.
% , obtaining consistent criteria for local modeling in the next section. 
% 
We describe them in detail below.

% % 
HPTI first constructs uniformly distributed hyperbolic prototypes with TPI, which is available in data without prior semantic information and efficient in computation.
% 
Most work relies on prior semantic knowledge about classes to discover the positions and representations of class prototypes.
% 
However, not all datasets contain prior semantic knowledge.
% 
Motivated by \cite{ghadimi2021hyperbolic}, we randomly sample points on the boundary of Poincar\'e ball for assigning class prototypes, and optimize these points to be uniformly distributed in a ball-shaped space.
% 
In this way, we incorporate the prior with large margin separation for Poincar\'e ball predictor.
% 
To be specific, searching for uniformly distributed class prototypes can be formulated as a Tammes problem~\cite{tammes1930origin}, i.e., arranging a set of points on a unit sphere that maximizes the minimum distance between any two points. 
% 
%
TPI optimizes this Tammes problem to obtain the class prototypes for all $C$ classes in a dataset, i.e., $\boldsymbol{W}^* \in \mathbb{R}^{C\times n}$ with $n$ denoting the dimension of prototype:
\begin{equation}
\small
\begin{gathered}
\label{eq:tammes}
\boldsymbol{W}^*=\underset{\boldsymbol{W} \in \mathbb{P}^n}{\operatorname{argmin}}\left(\max _{(i, j, i \neq j) \in[C]} \boldsymbol{w}_i {\boldsymbol{w}_j}^{\top}\right), \\
\text { s.t. } \forall i \in [C] \quad\left\|\boldsymbol{w}_i\right\|_2=1,
\end{gathered}
\end{equation}
where $\boldsymbol{w}_i$ ($\boldsymbol{w}_j$) is the $i-$th ($j-$th) row of $\boldsymbol{W}$ representing as the $i-$th ($j-$th) class prototype.
% 
We choose cosine similarity to measure this distance,
because Poincar\'e ball model space is conformal to the Euclidean space~\cite{ganea2018hyperbolic}.
% 

Optimizing Eq.~\eqref{eq:tammes} requires computing pairwise similarity of class prototypes iteratively, which is inefficient.
% 
To mitigate it, we utilize the similarity of Poinecar\'e ball and hyper-sphere to follow~\cite{mettes2019hyperspherical},  and minimize the largest cosine similarity for each prototype in the form of matrix, thus accelerating the optimization:
\begin{equation}
\small
\begin{gathered}
\mathbf{L}_{P}=\frac{1}{C} \Sigma_{i=1}^C \max _{j \in [C]} \mathbf{M}_{i j}, \mathbf{M}=\boldsymbol{W} \boldsymbol{W}^T-2 \mathbf{I}, \\
\text{ s.t. } \forall i \in [C] \quad \left\|\boldsymbol{w}_i\right\|_2=1.
\end{gathered}
\end{equation}

Next, we find the position to fix the hyperbolic class prototypes.
% 
As Fig.~\ref{fig:motivation}(b) shows, in Poincar\'e ball model, the closer the distance from the referred origin to the node, the more general the semantic information of node represents~\cite{liu2020hyperbolic}.
% 
But the uniformly distributed class prototypes are initially positioned on the boundary of the Poincar\'e ball, which is against the distribution of hierarchical structure in Poincar\'e ball.
%
% 
% 
% 
In order to enjoy the benefits of uniformity and generality simultaneously, we contract the class prototypes along with the radius to the origin $\boldsymbol{o}$ by a slope degree $s$, i.e., $\boldsymbol{W}_{\mathbb{P}}= s 
 \boldsymbol{W}^* \in \mathbb{R}^{C\times n}$.
% 
Lastly, HPTI~shares and fixes the Poincar\'e ball model to clients, which encourages local clients to model local data sufficiently with the supervision of consistent and separated hyperbolic prototypes.

% 
% Hence, each client can consistently optimize objective in Eq.~\eqref{eq:loss} and predict its local data samples according to the geodesic distance, i.e., Eq.~\eqref{eq:geo_dis}, between themselves and the shared class prototypes:
% \begin{equation}
% \label{eq:pred}
% y^{\star}=\arg \min _{\boldsymbol{w}_{y} \in \boldsymbol{W}_{\mathbb{P}}} d \left(\exp _{\boldsymbol{o}}(\mathcal{F}_{\boldsymbol{\theta}} (\mathbf{x})), \boldsymbol{w}_{y}\right).
% \end{equation} 
% 
 
\subsection{Hyperbolic Prototype Learning}
\paragraph{Motivation.}
In this part, we provide the details of HPL, which utilizes the hierarchical information inherent in data to obtain fine-grained and gathered data representations.
% 
To utilize the hierarchical information of data, HPL uses Poincar\'e ball model for the benefits of continuous optimization and effective representation in low-dimensional space.
%
To start with, HPL extracts the feature for data samples and applies an exponential map referred by the origin shared with class prototypes. 
% 
According to the supervision of shared class prototypes in hyperbolic space, HPL next represents the data features of the same class according to hyperbolic triplet loss.
% 

% \nosection{Personalized  
% Hyperbolic Representation}
% 【每个技术点都需要介绍清楚，它是什么，因为什么原因要用或者怎么起到了作用的】
% P1: 生成prototype ref busemann 
% 
In the following, we present how to model the hyperbolic representations of data samples for each client $k$ locally. 
% 
Specifically, we expect to learn a projection of local data $\mathcal{D}_k$ to the local Poincar\'e ball model, i.e., $\mathbb{P}^n_k$, in which we compute the similarity between data samples and class prototypes for each client.
% 
As introduced in Fig.~\ref{fig:HyperbolicFed}, we take a feature extractor $\mathcal{F}_{\boldsymbol{\theta}_k}(\cdot)$ in Euclidean space to obtain the feature representations of local data samples, i.e., $\boldsymbol{z}=\mathcal{F}_{\boldsymbol{\theta}_k}(\boldsymbol{x})$ for an instance pair $(\boldsymbol{x}, y)$ in $\mathcal{D}_k$.
% 
Referred by origin $\boldsymbol{o}$, we apply an exponential map from tangent space $\mathcal{T}_{\boldsymbol{o}} \mathcal{M}$ to the Poincar\'e ball model $\mathbb{P}^n_k$ shared with class prototypes.
% 
Hence, the representation of data samples in Poincar\'e ball model $\mathbb{P}^n_k$ can be:
\begin{equation}\label{eq:exp_map}
\exp_{\boldsymbol{0}}(\boldsymbol{z})=\tanh(||\boldsymbol{z}||_2)\frac{\boldsymbol{z}}{||\boldsymbol{z}||_2}.
\end{equation}

% 
As mentioned ahead, we seek to construct the hierarchical structure between the feature representations of data samples and their corresponding class prototypes.
% 
Triplet loss~\cite{movshovitz2017no,weimingNeurips21,weimingWWW23}
optimizes the distances among a set of triplets, denoted as \{anchor point, positive point, negative point\}, by creating a fixed margin, i.e., $m$, between the anchor-positive points difference and the anchor-negative points difference.
%
% 
% % 
% In detail, triplet loss takes a hinge function to create a fixed margin, i.e., $m$, between the anchor-positive points difference and the anchor-negative points difference:
% \begin{small}
%     \begin{equation}
% \mathcal{L}_{\text{triplet}}
% % (a,p_\text{pos},p_\text{neg})
% = \max(\operatorname{diff}(a,p_\text{pos})+m-\operatorname{diff}(a,p_\text{neg}),0 ). 
% \end{equation}
% \end{small}
% % 
Motivated by this, we choose each data sample representation in the Poincar\'e ball model as \textit{anchor point}, the ground truth class prototype as \textit{positive point}, and the remaining prototypes of the full class set as \textit{negative points}.
% 
% 
%
In this way, each client incorporates the prototypes of its missing class to feature representation, by randomly sampling negative points.
% \xenia{todel:In this way, the loss function for data sample $(\boldsymbol{x}, y) \in \mathcal{D}_k$ is free of the limitation of missing classes of data.} 
% 
We define hyperbolic triplet loss for client $k$ as below:
% \begin{align}
%     \mathcal{L} &= \max(d_{pos}-d_{neg}  + m, 0) \nonumber\\
%     d_{pos} &= d(\exp_{\boldsymbol{0}}(g_k(\boldsymbol{x}_i)), W_{k, y_i}) \nonumber\\
%     d_{neg} &= d(\exp_{\boldsymbol{0}}(g_k(\boldsymbol{x}_i)), W_{k, y_j}) \nonumber
% \end{align}
\begin{equation}
\small
    \label{eq:loss}
\mathbf{L}_{k}^{R}
% (\boldsymbol{z}, \boldsymbol{W}_{\mathbb{P}})
=\max(d(\exp_{\boldsymbol{0}}(\boldsymbol{z}), \boldsymbol{w}_y)-d(\exp_{\boldsymbol{0}}(\boldsymbol{z}), \boldsymbol{w}_{y^{\prime}})  + m, 0),
\end{equation}
where $\boldsymbol{z} = \mathcal{F}_{\boldsymbol{\theta}_k}(\boldsymbol{x})$, $\boldsymbol{w}_{y^{\prime}}$ is randomly sampled negative class prototype, $d(\cdot,\cdot)$ is the geodesic distance defined in Eq.~\eqref{eq:geo_dis}, and margin $m$ is a hyper-parameter.
% 
We obtain fine-grained representation with sufficient hierarchical information, by simultaneously minimizing the positive geodesic, e.g., the green curve in Fig.~\ref{fig:HyperbolicFed}, and maximizing the negative geodesic, e.g., the red curve in Fig.~\ref{fig:HyperbolicFed}.
% 
In this way, \modelname~utilizes the data hierarchical information to enhance the prediction.
% for FL with non-IID data.

\begin{algorithm}[h]
    \caption{Training procedure of \modelname}
    \label{alg:HyperFed}
    \textbf{Input}: Batch size $B$, communication rounds $T$, number of clients $K$, local steps $E$, dataset $\mathcal{D} =\cup_{k\in [K]} \mathcal{D}_k$\\
    % , where $\mathcal{D}_k=\{ \boldsymbol{x}_{k,i}, y_{k,i}\}_{i=1}^{N_k}$ \\
    \textbf{Output}: hyperbolic class prototypes $\boldsymbol{W}_{\mathbb{P}}$, model parameters, i.e., $\boldsymbol{\theta}^T \text{and} \{\boldsymbol{\theta}_k^T\}$
    \begin{algorithmic}[1]
        \STATE \textbf{Server executes():}
        \STATE Initialize 
 $\boldsymbol{\theta}^{0}$ with random distribution and $\boldsymbol{W}_{\mathbb{P}}$ by \textit{\textbf{HPTI}} 
        % \STATE Send model parameters to all participating clients
        \FOR{$t=0,1,...,T-1$}
            \FOR{$k=1,2,...,K$ \textbf{in parallel}} 
                \STATE Send $\{\boldsymbol{\theta}^t,\boldsymbol{W}_{\mathbb{P}}\}$ to client $k$ if $t=0$ else $\boldsymbol{\theta}^t$
                \STATE $\boldsymbol{\theta}_k^{t+1} \leftarrow$ \textit{\textbf{HPL:}} \textbf{Client executes}($k$, $\boldsymbol{\theta}^t$)
            \ENDFOR
        \STATE \textit{\textbf{CA:}} optimize Eq.~\eqref{eq:mgda} with CU and update parameters of $\boldsymbol{\theta}^{t+1}$ by Eq.~\eqref{eq:agg}
        \ENDFOR
        \STATE \textbf{return} $\boldsymbol{\theta}^T,\boldsymbol{W}_{\mathbb{P}}, \{\boldsymbol{\theta}_k^T\}$
        % \STATE
        \STATE \textit{\textbf{HPL:}} \textbf{Client executes}($k$, $\boldsymbol{\theta}^t$)\textbf{:}
        \STATE Assign global model to the local model $\boldsymbol{\theta}_k^t \leftarrow \boldsymbol{\theta}^t$
        \FOR{each local epoch $e= 1, 2,..., E$}
            \FOR{batch of samples $(\boldsymbol{x}_{k, 1:B}, \boldsymbol{y}_{k, 1:B}) \in \mathcal{D}_{k}$}
                \STATE Feature extraction $\boldsymbol{z}_{k, 1:B} \leftarrow \mathcal{F}_{\boldsymbol{\theta}_k^e} (\boldsymbol{x}_{k, 1:B})$
                \STATE Project $\boldsymbol{z}_{k, 1:B}$ to Poincar\'e ball by Eq.~\eqref{eq:exp_map}
                \STATE Compute loss $\mathbf{L}_k^{R}$ by Eq.~\eqref{eq:loss}
                % \STATE $d_{pos} \leftarrow d(\exp_{\boldsymbol{0}}(g_k(\boldsymbol{x}_i)), W_{k, y_i})$ 
                % \STATE $d_{neg} \leftarrow d(\exp_{\boldsymbol{0}}(g_k(\boldsymbol{x}_i)), W_{k, y_j}), y_j \ne y_i$  
                % \STATE $\mathcal{L} \leftarrow max(d_{pos} - d_{neg} + m, 0)$
                % \STATE $g_k^{t} \leftarrow \exp_{g_k^t}(-\eta_t\Delta_R \mathcal{L})$
                \STATE Update parameters of $\boldsymbol{\theta}_k^e$
                by RSGD
                % ~\cite{bonnabel2013stochastic} %Eq.~\eqref{eq:RSGD}
            \ENDFOR
        \ENDFOR
        \STATE \textbf{return} $\boldsymbol{\theta}_k^E$ to server
    \end{algorithmic}
\end{algorithm}


\subsection{Consistent Aggregation}
% \lwm{Adaptive Strategy}
% Lastly, the conflicting deviations of the extremely
% divergent clients (Issue 3) deteriorates the current FL meth-
% ods as well. It is inevitable to include some clients with ex-
% tremely statistically heterogeneous data distribution in the FL
% for real-world applications. The extremely divergent client
% will cause the heuristic aggregation, e.g., averaging weighted
% by the data amounts, to depart from the optimal global model,
% thus impacting the sub-stream training of clients.
% 
\paragraph{Motivation.}
% 
Finally, we introduce CA which mitigates the inconsistent deviations from clients to server caused by the statistically heterogeneous data distributions.
% 
% 
In FL aggregation,
% \xenia{In FL aggregation,}
% 
CA first formulates the aggregation of local feature extractors in \modelname~ 
as a multi-objective optimization.
% , i.e., alleviating the inconsistent deviations from different clients to server. 
% 
Then CA applies consistent updating (CU) to pick the toughest client, i.e., the client with the most divergent deviation, and alleviate the inconsistency between the toughest client and the remaining clients. 
% 
Lastly, CU iteratively optimizes this multi-objective optimization to yield a Pareto optimal solution and obtain the 
weight ratio of different client models.
% model parameter weights of different clients.
% 

We formulate the aggregation as a multi-objective optimization in the following. 
% 
Specifically, we first compute the different deviations from clients to server as multiple objectives, then the goal of alleviating the inconsistency of these deviations can be achieved by multiple-objective optimization.
% 
We obtain the combination of local parameters in server: 
\begin{equation}
\label{eq:agg}
\boldsymbol{\theta}^{t+1} = \boldsymbol{\theta}^{t}+\Sigma_{k=1}^K p_k\left(\boldsymbol{\theta}_k^{t+1}-\boldsymbol{\theta}^{t}\right),
\end{equation}
where $\boldsymbol{\theta}^{t}$ is the global model and $\boldsymbol{\theta}_k^{t}$ is the local model of the client $k$ at $t-$th communication.
% 
Next, we denote global and client deviations, i.e., $\Delta_{\boldsymbol{\theta}}^{t+1}=\boldsymbol{\theta}^{t+1} - \boldsymbol{\theta}^{t}$ and $\Delta_{\boldsymbol{\theta}_{k}}^{t+1}=\boldsymbol{\theta}_k^{t+1}-\boldsymbol{\theta}^{t}$, respectively, and rewrite Eq.~\eqref{eq:agg} as:
% to obtain Eq.~\eqref{eq:agg} in the form of deviations:
\begin{equation}
\label{eq:agg_grad}
    \Delta_{\boldsymbol{\theta}}^{t+1} = \Sigma_{k=1}^K p_k \Delta_{\boldsymbol{\theta}_{k}}^{t+1}.
\end{equation}
% 
% The goal of consistent optimization is to alleviate the inconsistency of updating deviations from the different local models to the global model.
% % 
% Taking the different deviations from clients to server as multiple objectives, the goal of alleviating the inconsistency of these devitations can be achieved by multiple-objective optimization.
%
Then CA solves this multiple-objective optimization to Pareto stationary point, i.e., minimizing the minimum possible convex combination of inconsistent deviations:
\begin{small}
    \begin{equation}
\min \frac{1}{2}\left\| \Sigma_{k=1}^K p_k \Delta_{\boldsymbol{\theta}_{k}}^{t+1}\right\|^2_2 \text {, s.t. }  \Sigma_{k=1}^K p_k=1 \text {, and } \forall k, p_k \geq 0.
\label{eq:mgda}
\end{equation}
\end{small}
% 

Next, we introduce CU~which derives from Multiple Gradient Descent Algorithm (MGDA)~\cite{desideri2012multiple,sener2018multi} to solve this optimization.
% 
The optimization problem defined in Eq.~\eqref{eq:mgda} is equivalent to finding a minimum-norm point in the convex hull of the set of input points, i.e., a convex quadratic problem with linear constraints.
% 
CU iteratively optimizes Eq.~\eqref{eq:mgda} by linear search, which can be solved analytically~\cite{jaggi2013revisiting}.
% 
In detail, we find the toughest client, treat the combination of the remaining clients as a virtual client, and analyze the solution according to the directions of the toughest client and the virtual client. 
% 
Firstly,
we initialize $\boldsymbol{p}_0$ with the weight of data samples, i.e., $p_k= \nicefrac{N_k}{\Sigma_{k=1}^{K}N_k} $, and 
precompute the consistency of deviations $\boldsymbol{V}, s.t. \boldsymbol{V}_{k,k^{\prime}}= {\Delta_{\boldsymbol{\theta}_{k}}}^{\top}{\Delta_{\boldsymbol{\theta}_{k^{\prime}}}}$. 
% 
Then we find the toughest client by $\tau = \operatorname{arg}\operatorname{min}_{k^{\prime}} \Sigma_{k=1}^{K} p_k\boldsymbol{V}_{k^{\prime},k}$ with deviation $\Delta_{\tau}$, and remain the combination of others with historical weights to be a virtual client, i.e., $\Delta_{\text{vir}} = \Sigma_{k=1, k\neq \tau }^{K} p_k \Delta_{\boldsymbol{\theta}_{k}}$.
% 
Thus we simplify Eq.~\eqref{eq:mgda}, i.e., $\operatorname{min}_{p_{\tau}\in [0,1]} \frac{1}{2} \|p_{\tau}\Delta_{{\tau}}+(1-p_{\tau})\Delta_{{\text{vir}}}\|^2_2$.
% 
According to the directions of $\Delta_{\tau}$ and $\Delta_{\text{vir}}$, we can obtain the analytical solution for $p_{\tau}$:
\begin{equation}
    p_{\tau}= \operatorname{CU}^{+}\left[\nicefrac{{(\Delta_{\text{vir}}-\Delta_{\tau})}^{\top} \Delta_{\text{vir}}}{\|\Delta_{\tau}-\Delta_{\text{vir}}\|^2_2}\right],
    \label{eq:p_two_clients}
\end{equation}
where $\operatorname{CU}^{+}[\cdot] = \operatorname{max}(\operatorname{min} (\cdot, 1),0)$.
% 
In Appendix C, we present the analysis of optimization for obtaining $\operatorname{CU}^{+}[\cdot]$  based on the computational geometry~\cite{sekitani1993recursive} of minimum-norm in the convex hull. 
% 
Given $p_{\tau}$, we update the weight ratio $\boldsymbol{p}=(1-p_{\tau})\boldsymbol{p}+ p_{\tau}\boldsymbol{e}$, where $\boldsymbol{e}$ is the one-hot vector with 1 in the $\tau-$th position.
% 
In order to obtain the Pareto stationary point,  
% 
CU iterates the process of finding the toughest client several times to obtain the best combination $\boldsymbol{p}^*$ that alleviates the inconsistent deviations of clients.
% 
Finally, we find the consistent optimization direction with the Pareto optimal solution $\boldsymbol{p}^*$ for aggregating in Eq.~\eqref{eq:agg}.
% 
% Hence, we obtain the approximately consistent global optimum and local optimum from both the optimization objective and optimization direction.
% 

Given three main modules, i.e., HPTI, HPL, and CA, we illustrate the overall algorithm of modeling \modelname~in Algo.~\ref{alg:HyperFed}.
% 
Steps 1-10 are the server execution.
% 
In step2, the server initializes model parameters and HPTI in it initializes the hyperbolic class prototypes.
% 
Then for each communication round, all clients use HPL to train their local model with the shared Poincar\'e ball predictor in step 6.
% 
After that, in step 8, CA in server receives the model parameters of all clients, and mitigates the inconsistency of client deviations in aggregation.
% 
The details of client execution is listed in steps 11-21.







% \xenia{Theorem1 communication error bound}
% \xenia{Theorem2 Convergence rounds in total}

% \nosection{Privacy-preserving}
% % P4: for privacy concerns, 加了差分隐私
% % 1. 差分隐私在user-level 的情况下的privacy bound 
% \subsection{Global Aggregation}
% P1: 为什么在server上进行聚合，具体的聚合操作是什么,聚合对PFL这个问题的好处是什么
%P2: Algorithm
