\documentclass[%
 reprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
superscriptaddress,
 amsmath,amssymb,
 aps,
 prx,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-2}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage[colorlinks]{hyperref}% add hypertext capabilities
\usepackage{physics}
\usepackage[squaren]{SIunits}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{mathrsfs}
\usepackage{times}
\usepackage[normalem]{ulem}
\usepackage{algorithm2e}
% \usepackage{algorithmicx}
% \usepackage{algpseudocode}
% \usepackage[normalem]{ulem}
% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{comment}
% \usepackage{minitoc}
\usepackage{titletoc}

\usepackage{tocloft}

\addtolength{\cftsecnumwidth}{10pt}

% \makeatletter
% \renewcommand*\l@subsection{\@dottedtocline{2}{3.8em}{3.5em}}% Used to be {3.8em}{3.2em}
% \makeatother


% \renewcommand{\tablename}{\textbf{TABLE}}
% \renewcommand{\figurename}{\textbf{FIG.}}
% \renewcommand{\thefigure}{\textbf{\arabic{figure}}}
% \newcommand{\beginsupplement}{
%         \setcounter{table}{0}
%         \setcounter{figure}{0}
%         \setcounter{equation}{0}
%         \setcounter{page}{0}
%         \renewcommand{\tablename}{Table}
%         \renewcommand{\figurename}{Fig.}
%         \renewcommand{\thetable}{S\arabic{table}}
%         \renewcommand{\thefigure}{S\arabic{figure}}
%         \renewcommand{\thepage}{S\arabic{page}} 
%         \renewcommand{\thesection}{S\arabic{section}} 
%         \renewcommand{\thesubsection}{\thesection.\arabic{subsection}} 
%         \renewcommand{\theequation}{\thesection.\arabic{equation}}
%      }


% \setlength\parindent{0pt}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}
\newtheorem{definition}[theorem]{Definition}

\hypersetup{
	colorlinks=true,
	linkcolor=red,
	filecolor=blue,      
	urlcolor=blue,
	citecolor=blue
}
% \input{preamble}
\setlength{\parskip}{0.5em}

\newcommand{\rev}[1]{{\textcolor{black}{#1}}}
% \newcommand{\rev}[1]{{\textcolor{blue}{#1}}} % uncomment to show changes
\newcommand{\fh}[1]{{\textcolor{orange}{[FH: #1]}}}
\newcommand{\sak}[1]{{\textcolor{brown}{#1}}}
\newcommand{\ga}[1]{{\textcolor{violet}{[GA: #1]}}}
\newcommand{\het}[1]{{\textcolor{orange}{[HET: #1]}}}
\newcommand{\et}[1]{{\textcolor{cyan}{[ET: #1]}}}
\newcommand{\lb}[1]{\textcolor{teal}{[LB: #1]}}
\newcommand{\mv}[1]{\textcolor{olive}{[MV: #1]}}

\newcommand{\Eq}[1]{Eq.\,(\ref{#1})}
\newcommand{\Eqr}[1]{(\ref{#1})}
\newcommand{\Eqs}[2]{Eqs.\,(\ref{#1}-\ref{#2})}
\newcommand{\Sec}[1]{Section~\ref{#1}}
\newcommand{\Fig}[1]{Fig.\,\ref{#1}}
\newcommand{\ceil}[1]{\lceil {#1} \rceil}


% Frequently used symbols
% \newcommand{\NS}{N_\text{S}}
\newcommand{\NS}{S}
% \newcommand{\NQ}{N_\text{Q}}
\newcommand{\NQ}{L}
\newcommand{\NI}{N}
\newcommand{\NR}{N_\mathrm{R}}
\newcommand{\ND}{d}
\newcommand{\UI}{\bm{u}}
\newcommand{\EC}{C_T}
\newcommand{\ETC}{\bar{\mathcal{T}}}
\newcommand{\regmat}{\widetilde{\mathbf{F}}_N}
\newcommand{\RM}{\mathbf{F}_N}
\newcommand{\Xs}{\mathcal{X}}

\newcommand{\cu}{\mathbf{\Sigma}}  % covariance as func of u
\newcommand{\ci}{\mathbf{V}}       % covariance after integration
\newcommand{\gr}{\mathbf{G}}       % gram matrix or graph matrix
\newcommand{\gh}{\mathbf{\Lambda}}  % generalized hilbert matrix
\newcommand{\nsr}{\mathbf{R}}      % ratio of noise-to-signal

\newcommand{\Es}[1]{\mathbb{E}_{\Xs}\!\!\left[#1\right]}
\newcommand{\Covs}{\mathrm{Cov}_{\Xs}}
\newcommand{\Vars}{\mathrm{Var}_{\Xs}}
\newcommand{\EUI}[1]{\mathbb{E}_{\bm{u}}\!\!\left[#1\right]}
\newcommand{\Eu}[1]{\mathbb{E}_{u}\!\!\left[#1\right]}


\begin{document}

%\title[]{Fundamental Limits to Expressive Capacity of Finitely Sampled Qubit-Based Systems}
%\title[]{Quantifying the Expressive Capacity of Quantum Systems: Fundamental Limits and Eigentasks}
\title[]{Tackling Sampling Noise in Physical Systems for Machine Learning Applications: Fundamental Limits and Eigentasks}

%\thanks{Footnote to title of article.}
% Discuss principal functions

% Re-evaluate use of 'fundamental'

\makeatletter
%\long\def\@makefntext#1{%
%  \parindent 1em\hb@xt@ 1.8em{\hss \textsuperscript(\kern-0.1ex\@makefnmark\kern-0.1ex\textsuperscript)}#1}
% \let\@fnsymbol\@fnsymbol@latex
% \@booleanfalse\altaffilletter@sw
% \makeatother


\author{Fangjun Hu}
\thanks{These three authors contributed equally}
% \email{fhu@princeton.edu}
\affiliation{Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ 08544, USA}

\author{Gerasimos Angelatos}
\thanks{These three authors contributed equally}
\affiliation{Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ 08544, USA}
\affiliation{Raytheon BBN, Cambridge, MA 02138, USA}

\author{Saeed A. Khan}
\thanks{These three authors contributed equally}
\affiliation{Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ 08544, USA}

\author{Marti Vives}
\affiliation{Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ 08544, USA}
\affiliation{Q-CTRL, Santa Monica, CA 90401, USA}

\author{Esin T\"ureci}
\affiliation{Department of Computer Science, Princeton University, Princeton, NJ 08544, USA}

\author{Leon Bello}
\affiliation{Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ 08544, USA}

\author{Graham E. Rowlands}
\affiliation{Raytheon BBN, Cambridge, MA 02138, USA}

\author{Guilhem J. Ribeill}
\affiliation{Raytheon BBN, Cambridge, MA 02138, USA}

\author{Hakan E. T\"ureci}
\affiliation{Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ 08544, USA}


\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified


\begin{abstract}

% 196 words
The expressive capacity of physical systems employed for learning is limited by the unavoidable presence of noise in their extracted outputs. Though present in physical systems across both the classical and quantum regimes, the precise impact of noise on learning remains poorly understood. Focusing on supervised learning, we present a mathematical framework for evaluating the resolvable expressive capacity (REC) of general physical systems under finite sampling noise, and provide a methodology for extracting its extrema, the eigentasks. Eigentasks are a native set of functions that a given physical system can approximate with minimal error. We show that the REC of a quantum system is limited by the fundamental theory of quantum measurement, and obtain a tight upper bound for the REC of any finitely-sampled physical system. We then provide empirical evidence that extracting low-noise eigentasks can lead to improved performance for machine learning tasks such as classification, displaying robustness to overfitting. We present analyses suggesting that correlations in the measured quantum system enhance learning capacity by reducing noise in eigentasks. The applicability of these results in practice is demonstrated with experiments on superconducting quantum processors. Our findings have broad implications for quantum machine learning and sensing applications.
% this version is (also) 209 words 
% \rev{The expressive capacity of physical systems for machine learning is limited by sampling noise incurred during information extraction. Although it is generally believed that noise limits the resolvable capacity of physical and biological systems, the precise impact of noise on learning is not yet fully understood. Focusing on supervised learning, we present a mathematical framework for evaluating the resolvable expressive capacity (REC) of general physical systems for a finite number of measurements, and provide a methodology for extracting its extrema, the eigentasks. Eigentasks are a native set of functions that a given physical system can approximate with minimal error. A fundamental limit can be found for the REC of a physical system sampled $S$ times that is allowable by the quantum theory. We arrive at such a tight upper bound for quantum systems and adopting a general description of sampling noise provided by quantum measurement theory. We then show that extracting low-noise eigentasks leads to improved performance for machine learning tasks such as classification, displaying robustness to overfitting. We present analyses suggesting that correlations in the measured quantum system enhance learning capacity by reducing noise in eigentasks. These results are supported by experiments on superconducting quantum processors. Our findings have broad implications for quantum machine learning and sensing applications.}
\end{abstract}


\maketitle

% \doparttoc % Tell to minitoc to generate a toc for the parts
% \faketableofcontents % Run a fake tableofcontents command for the partocs

% \part{} % Start the document part
% \parttoc % Insert the document TOC


%\pacs{TBD}% PACS, the Physics and Astronomy
                             % Classification Scheme.
%\keywords{TBD}%Use showkeys class option if keyword
                              %display desired

%Learning with quantum systems is a promising application of near-term quantum processors, with several recent demonstrations in both quantum machine learning (QML)~\cite{grant_hierarchical_2018, havlicek_supervised_2019, tacchino_quantum_2020} and quantum sensing~\cite{meyer_fisher_2021, ma_adaptive_2021, Marciniak_Monz_2022}. A broad class of such data-driven applications proceed by embedding data into a quantum system, where the embedding, dynamics, and extracted outputs via measurement are all governed by a set of general parameters $\bm{\theta}$. Depending on the learning scheme, different components of this general framework may be trained for optimal performance of a given task. Irrespective of the scheme, however, the training of such quantum systems encounters the problem of barren plateaus in the optimization landscape~\cite{mcclean_barren_2018, cerezo_cost_2021, sack_avoiding_2022}, highlighting the need for targeted design strategies for system primitives. In this work, we develop a framework that can guide the design of quantum systems for computation, by quantifying their expressive capacity in the presence of fundamental quantum sampling noise.


\section{Introduction}

%%%%% CHANGES FOR NEXT REVISION
% Appendix reference on page 6 points to wrong figure in appendix:
 % We have observed very similar behaviour using completely different quantum system models~(
% see Appendix Fig.\,\ref{fig:NSR_Tilde_Renormalization} <- wrong
% see Appendix Fig.\,\ref{fig:app_Features_and_Capacxity}
% ~\cite{Giovannetti2006, martinez2021dynamical}).

 % figure 5, in Appendix B, corrected last line of caption:
 % original: For each input $\UI$ and shots $S$, the simulation is conducted for $100$ repetition. 
 % correction: For each input $\UI$ and shots $S$, the simulation is repeated $100$ times, resulting in the distribution shown. 

% \fh{Referee 2 asked us to cite this paper on linear memory capacity \cite{jager2001short}.}

% \ga{at some point most of the various quantum rc and qelm references were removed.  I think important ones to include (like where we introduce our QRC scheme) etc are (explicitly circuit QELMS: \cite{wilson_quantum_2019}, \cite{innocenti_potential_2022}), \cite{suzuki_natural_2022} (possibly \cite{chen_temporal_2020}) (already present, but maybe not referenced in the right place: \cite{mujal_opportunities_2021}, \cite{wright_capacity_2019} \cite{martinez-pena_information_2020}, \cite{garcia-beni_scalable_2022}) (some references on learning with parameterized quantum circuits, including experiments, which should be mentioned in the appropriate place: (\cite{cerezo_variational_2021}, \cite{grant_hierarchical_2018} (supervisedm classifiers), \cite{rudolph_generation_2022} (generative), \cite{Marciniak_Monz_2022} (sensing)}
% \fh{previously we introduce them in setion of experiment, but R2 says that it should be replaced by non-temporal elm, thus the papers listed here can be added, only if QRC is explicitly introduced. } \ga{several of them are either explicitly ELMs, or are really ELMS, and they are very related in that they consider very similar things, namely learning with untrained parameterized circuits, so I do think they should be mentioned.}

A physical system receiving an input stimulus typically evolves in response to it, such that its degrees of freedom become dependent on said input after a certain period of interaction with it. This everyday observation has a profound implication: 
any dynamical system can be viewed as performing a transformation of its input, realizing an input-output map~\cite{boyd_fading_1985}. This functional map can in principle be optimized, inspiring an emerging approach to learning with analog physical systems, which we will collectively refer to as Physical Neural Networks (PNN) \cite{Wright2022, nakajima_physical_2022, markovic_physics_2020}. 
PNNs employ a wide variety of analog physical systems to compute a trainable transformation on an input signal~\cite{Tanaka2019, mujal_opportunities_2021, cerezo_variational_2021, ortin_unified_2015, lopez-pastor_self-learning_2021, wilson_quantum_2019, garcia-beni_scalable_2022, havlicek_supervised_2019, rowlands_reservoir_2021, canaday_rapid_2018, shen_deep_2017, lin_all-optical_2018, pai_experimentally_2023}.    
% PNN refs, order of relavance roughly:  Wright2022 (general) markovic_physics_2020 (good PNN review) mujal_opportunities_2021 (QRC review) Tanaka2019 (physical RC) lin_all-optical_2018 (optical ML) pai_experimentally_2023 (photonic ML with backprop) shen_deep_2017 (MZ intererometers for multiplication - deep nn) lopez-pastor_self-learning_2021 (florian group physical learning) canaday_rapid_2018 (Dan's group, FPGAs) Marciniak_Monz_2022 (programmable quantum sensors ie learning with qubits for a sensing task) ortin_unified_2015 (elm and RC with time delay optical system) khan_physical_2021 (self-explanitory) angelatos_reservoir_2021 (likewise) cerezo_variational_2021 (variational algorithms on quantum systems review) benedetti_parameterized_2019 (more specifically review on parameterized circuits for ml, but also we dont cite it anymore) chen_temporal_2020 (a popular QRC reference for us) 
More precisely, the role of an idealized (\textit{i.e.}\,completely deterministic, noise-free) physical system in these approaches is that of a high-dimensional feature generator. 
% in the idealized case a completely deterministic, noise-free one. 
Given inputs $\bm{u}$, the measured degrees of freedom $x_k(\bm{u})$ for $k \in [K]$, generated by the system, act as an input-dependent vector of features. These features are used to approximate a function $f(\bm{u})$ via a learned linear projection with sufficient accuracy, as dictated by a chosen loss function (See Fig.\,\ref{fig:Gen_Schematic}). Different characteristics of the physical system, described by a set of hyperparameters $\bm{\theta}$, may determine its ability to approximate a particular function. Consequently, the relationship between a specific physical system and the classes of functions it can express with high accuracy is a fundamental question in this paradigm of machine learning
~\cite{dambre_information_2012, sheldon_computational_2022, schuld_effect_2021, wu_expressivity_2021, wright_capacity_2019, pai_experimentally_2023}. 

 % This everyday observation has a profound implication: 
%any evolving physical system can be viewed as performing a transformation of its input, realizing an input-output map that computes functions of the input~\cite{boyd_fading_1985}.
% any dynamical system can 

No physical system however exists in isolation, and is therefore necessarily subject to noise. Noise can enter at the input, whereby it evolves under the same dynamical law governing the evolution of the physical system. There may also be variability in this very dynamics of the physical system itself. Finally, there is typically noise associated with the measurement of output features from the physical system. 
As a consequence of these noise sources, the resulting feature map is stochastic: even under identical preparations and inputs $\bm{u}$, the outcome of a measurement $X_k^{(s)}(\bm{u})$ of a feature $k$ can vary between repetitions, each of which is referred to as a ``shot'' $s$. By empirically averaging the outcomes of $\NS$ shots, one can
generally reduce this stochasticity. We will refer to the resulting noise as “sampling noise”. Theoretical analysis and experimental implementations of PNNs have already demonstrated that sampling noise can have a substantial role in the ultimate performance of a physical learning machine~\cite{garcia-beni_scalable_2022, shen_deep_2017, havlicek_supervised_2019}. However, it is also known that this role may be more subtle than a limitation on performance across the board, as evidenced for example in the effective use of noise for regularization to aid generalizability in learning~\cite{bishop_training_1995, neelakantan_adding_2015, noh_regularizing_2017}. 

%\ga{I think some of this is repeated at the start of section 2, so I simplified a bit} \fh{Agreed, we can simplify it a little bit. }

Often, heuristic descriptions are used to theoretically model such sampling noise and explain its effects on learning~\cite{dambre_information_2012, ortin_unified_2015, Rumyantsev2020, garcia-beni_scalable_2022}. However, when considering physical \textit{quantum} systems for learning, a fundamental microscopic model for sampling noise is provided, and in fact imposed by the quantum theory of measurement. Explicitly, for a quantum system prepared in an initial state density matrix $\hat{\rho}_0$ and evolving under an input-parameterized quantum channel $\mathcal{U}(\bm{\theta},\bm{u})$, the final state is $\hat{\rho}(\UI) = \mathcal{U}(\bm{\theta},\bm{u})\hat{\rho}_0$~\footnote{A few things to note here. 1. The initial state preparation is in practice often realized by an act of measurement as well. Then, the input-evolution-output sequence can be described as the sequence of measurement-evolution-measurement sequence. 2. Some PNN realizations view input as provided through an input state $|\Psi (\UI) \rangle$. Within the framework we adopt, this can be described as a parametric evolution $\mathcal{U}(\UI)$ acting on an initial $\UI$-independent state.}. Sampling noise in measured features from this quantum system is constrained by the choice of measurement projectors $\hat{M}_k$ associated with $\hat{\rho}(\UI)$. Unless the physical transformation defined by the quantum channel is optimized to yield only specific highly localized $\hat{\rho}(\UI)$ in the eigenspace of $\hat{M}_k$ (as in quantum algorithms such as Grover's or Shor's \cite{nielsen2002quantum}) -- a significant design restriction -- or an excessively large number of shots $\NS$ is used -- a significant hardware restriction -- such quantum sampling noise will be an intrinsic component of learning with quantum systems. 

% Statistically identical conditions is expressed in terms of an initial state density matrix $\hat{\rho}_0$ for the system, the statistically identical conditions for the evolution (in the presence of an environment with statistically well-defined conditions) is described in terms of a quantum channel $\mathcal{U}$, with its dependence on input being described by the parametric dependence of $\mathcal{U}(\UI)$, yielding after the fixed interaction time the statistically well-defined system state $\hat{\rho}(\UI)$. Finally, the output features $x_k$ are extracted from a set of measurement projectors $\hat{M}_k$, namely $x_k({\UI}) \equiv {\rm Tr}\{\hat{M}_k \hat{\rho}({\UI})\}$
% ~.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Figure environment removed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Therefore, a framework is required that can account for sampling noise across generic physical systems, and provide tools for learning when sampling noise is unavoidable. In this paper, we address the following question directly: what is the resolvable function space of an arbitrary physical system when regarded as an input-output machine \textit{in the presence of sampling noise}? This simple objective leads us to a general mathematical framework with important consequences for statistical learning theory, which we now overview.  Our analysis is centered around a specific metric, the Resolvable Expressive Capacity (REC), which is a generalization of the information processing capacity introduced in Ref.\,\cite{dambre_information_2012} (see also the earlier work in Ref.\,\cite{jager2001short}) to account for the presence of sampling noise.  
% (and to which it reduces in the $S\to\infty$ limit). %\ga{this point is made elsewhere}
% \ga{the original paper calls it computational capacity, not sure we should use IPC} \fh{In the original report from R2 and EE, they both used terminology information processing capacity, not computational capacity. } \ga{thats probably because we called it IPC? } \fh{I'm fine with either. }
Specifically, the REC is a quantitative measure of the accuracy with which $K$ system-specific orthogonal functions can be constructed from $K$ stochastic features $\bar{X}_k(\bm{u})$. Remarkably, this accuracy has a tight, calculable $\NS$-dependent upper-bound. 
The special functions, referred to as the eigentasks $y^{(k)}(\bm{u})$ of the physical system, define the maximally-resolvable function space under $\NS$ shots, which sets the stage for the introduction of a learning methodology in the presence of sampling noise. %Going beyond this, the knowledge of these maximally resolvable functions allows the introduction of a learning scheme that only uses combinations of stochastic features that are least effected by sampling noise. This construction presents several advantages for supervised learning, which we demonstrate using simple examples.

Crucially, our framework can be applied to an arbitrary physical system via the solution of a simple matrix eigenproblem. The matrices in question are standard Gram matrix $\gr$ and covariance matrix $\ci$, which can be estimated using stochastic samples from the system as a function of inputs $\bm{u}$ over the domain of interest; the analysis can thus be directly implemented in experimental settings without an internal model of the system. The solution of this linear eigenproblem yields both the eigentasks $\{y^{(k)}(\bm{u})\}$ and associated ``noise-to-signal'' eigenvalues $\{\beta_k^2\}$, which codify the normalized noise power in a construction of $y^{(k)}(\bm{u})$ from finite-$S$ $\bar{X}_k(\bm{u})$. The REC of the system is then only a function of $\{\beta_k^2\}$ and $S$.

In the second part of this paper we develop Eigentask Learning, a means of learning in physical systems where sampling noise dominates, by using the noise-ordered eigentasks to construct a maximally resolvable basis of measured features. Our approach affects a systematic removal of high-noise features during training, which we demonstrate in experiments. These experimental demonstrations provide empirical evidence of robustness to overfitting in supervised learning, enhancing generalizability in the presence of sampling noise. Such a learning scheme built on avoiding features identified as having large noise may in fact be at play in natural physical systems such as biological neural circuits~\cite{montijn_population-level_2016}. 
%Behaviors elicited by the noisy neural activities can be robust, implying the existence of down-stream modes that are robust to (correlated) noise present in single neuron activities.
A well-studied example is that of neural vision: here input visual stimuli drive stochastic dynamics of sensory neurons in the visual cortex, which must together elicit a target response, such as the brain correctly distinguishing two images. Studies have shown that the dynamics of individual neurons under nominally-identical stimuli can exhibit great variability on a shot-by-shot basis~\cite{faisal_noise_2008}; however in spite of the significant noise, the overall driven behavior remains capable of distinguishing visual stimuli with high fidelity. Studies analyzing the robust neural code despite noisy neural activity have found emergent global coding directions in the population activity that evade ``modes" with maximal noise~\cite{montijn_population-level_2016, Rumyantsev2020}. The eigentask construction introduced here can be viewed as a generalization of this idea of noise-ordered modes to function spaces over an arbitrary input domain, and for an arbitrary physical system.

The Eigentask Learning framework is  sensitive not just to the properties of the noise itself (encoded in $\ci$), but also any dependence between it and the noise-free features via the Gram matrix $\gr$.  Such a situation typically prevails when the dominant source of sampling noise is either part of, or evolves under, the same input-output map defined by the physical system, as opposed to a completely uncorrelated noise source downstream. A simple example illustrating this, and one we analyze in detail, is that of a classical optical system, with features measured via photodetection. Here, the sampled features -- the integrated photocurrents -- are subject to shot noise whose variance is related to the mean of the photocurrents themselves.

In the quantum regime of operation of a physical system -- our ultimate focus -- this relation between sampling noise and the state of the physical system emerges in the most general description of quantum measurement as a positive operator-valued measure (POVM). We formulate the computation of REC and eigentasks for arbitrary quantum systems in the presence of this fundamental sampling noise structure; we focus on qubit-based quantum systems (including gate-based circuits and quantum annealers) operated as 
%Extreme Learning Machines (ELMs) \cite{GuangBinHuang, ortin_unified_2015} under static inputs, but our analysis is applicable to far more general quantum sensing and learning platforms.  
untrained PNNs under static inputs (so-called Extreme Learning Machines (ELMs) \cite{GuangBinHuang, ortin_unified_2015}), but our analysis is applicable to far more general quantum sensing and learning platforms. 
To validate the theoretical findings and emphasize their ready applicability to experimental scenarios, we implemented an ELM through a parameterized quantum circuit encoding on an IBMQ superconducting processor: demonstrating the calculation of REC, the construction of eigentasks, and the application of Eigentask Learning to a classification task. In all cases excellent agreement is seen with numerical simulation, and direct correlation is observed between REC and success at the considered classification task. This invites the exploration of principles to maximize the finite-sampling REC of a quantum system; for the qubit-based systems analyzed here, we show that an increase in measured quantum correlations can aid this goal. 

% \ga{Note that paper quotes in the response letter may need to be updated (eg I think the reviewer requested the caption of figure 1)} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Figure environment removed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The remainder of this paper is organized as follows. Section \ref{sec:TheoreticalAnalysis} provides the general theory of REC and eigentasks with respect to sampling noise in generic supervised physical learning systems, and presents a calculation for a basic classical optical PNN. Section \ref{sec:QSN} applies REC theory and eigentask construction to machine learning with quantum systems, which is then validated and demonstrated with experiments performed on a 7-qubit IBMQ superconducting processor
in Sec.~\ref{sec:ibmq}. Finally, conclusions are presented in Sec.~\ref{sec:discussion}.
%%% Original
% \rev{
% Therefore, a framework is required that can account for sampling noise across generic physical systems, and provide tools for learning when sampling noise is unavoidable. In this paper, we address the following question directly: what is the resolvable function space of an arbitrary physical system when regarded as an input-output machine \textit{in the presence of sampling noise}? This simple objective leads us to a general mathematical framework with important consequences for statistical learning theory, which we now overview.  Our analysis is centered around a specific metric, the Resolvable Expressive Capacity (REC), which is an generalization of the computational capacity introduced in Ref.\,\cite{dambre_information_2012} to the presence of sampling noise.
% Specifically, the REC is a quantitative measure of the accuracy with which $K$ system-specific orthogonal functions can be constructed from $K$ stochastic features $\bar{X}_k(\bm{u})$. Remarkably, this accuracy has a tight, calculable $\NS$-dependent upper-bound. 
% The special functions, which we call the eigentasks $y^{(k)}(\bm{u})$ of the physical system, define the maximally-resolvable function space under $\NS$ shots. %Going beyond this, the knowledge of these maximally resolvable functions allows the introduction of a learning scheme that only uses combinations of stochastic features that are least effected by sampling noise. This construction presents several advantages for supervised learning, which we demonstrate using simple examples.
% }


% \rev{
% Crucially, the REC framework for an arbitrary physical system achieves these goals via the solution of a simple matrix eigenproblem. The matrices in question are standard Gram matrix $\gr$ and covariance matrix $\ci$ that can be estimated using stochastic samples from the system as a function of inputs $\bm{u}$ over the domain of interest. The possibility of this construction is somewhat surprising, since the eigentasks $y^{(k)}(\bm{u})$, being the eigenvectors of the spectral problem, are generally nonlinear functions of the input. The associated ordered eigenvalues $\{\beta_k^2\}$ then codify the normalized error in approximating the corresponding eigentasks, defining a set of functions ordered by their resolvability under finite sampling. The REC of the system is then only a function of $\{\beta_k^2\}$ and $S$. Together, the linearity and simplicity makes the REC framework not only valid for arbitrary physical systems, but also practically calculable.
% }

% \rev{
% Most importantly, we show that the noise-ordered eigentasks provide means of learning with physical systems where sampling noise dominates. An example of a physical input-output system where the ideas of learning and sampling noise are inextricable is that of biological neural circuits. A well-studied example is that of neural vision: here input visual stimuli drive stochastic dynamics of sensory neurons in the visual cortex, which must together elicit a desired (or target) response, for example by the brain distinguishing two images. However, studies have shown that the neuron dynamics to nominally-identical stimuli can exhibit great variability on a shot-by-shot basis~\cite{montijn_population-level_2016}, thus exhibiting significant sampling noise. In spite of this stochasticity, the overall neural response can be very accurate, distinguishing visual stimuli with high fidelity past an input threshold. Studies analyzing this robustness to noisy neuron dynamics have found signatures attributing it to the coding of visual signals in features that evade ``modes'' with maximal noise~\cite{montijn_population-level_2016, Rumyantsev2020}. The eigentask construction introduced here can be viewed as a generalization of the idea of noise-ordered modes to function spaces over an arbitrary input domain, and for an arbitrary physical system.
% }


% \rev{
% Furthermore, the eigentask construction is not limited just to the strength or correlations of the noise in sampled features, encoded in $\ci$. It is also sensitive to any dependence between the noise statistics $\ci$ and the measured features via the Gram matrix $\gr$. Such a dependence will typically prevail when the dominant source of sampling noise is either part of, or evolves under, the same input-output map defined by the physical system (as opposed to a completely uncorrelated noise source downstream). A simple example illustrating this, analyzed in detail, is that of a classical optical system, with features measured via photodetection. Then, the sampled features -- the integrated photocurrents -- are subject to shot noise whose variance is related to the mean of the photocurrents themselves.
% }

% \rev{
% In the quantum regime of operation of a physical system -- our ultimate focus -- this relation between sampling noise and the state of the physical system is microscopically prescribed by the most general form of measurement, positive operator-valued measure (POVM), in quantum measurement theory. We demonstrate the computation of REC and eigentasks for quantum systems in the presence of this prescribed sampling noise structure, focusing on qubit-based quantum systems (including gate-based circuits and quantum annealers) operated as Extreme Learning Machines (ELMs) \cite{GuangBinHuang, ortin_unified_2015} under static inputs. 
% % \ga{Maybe mention here that 'We emphasize however that this analysis encompasses arbitrary quantum systems employing any input-encoding quantum channel and subject to the most general form of measurement, a positive operator-valued measure (POVM); it is therefore applicable to more general quantum sensing and learning platforms.'}
% To validate the theoretical findings and their applicability to experimental scenarios, we perform experiments on IBMQ superconducting processors to demonstrate the calculation of REC and the construction of eigentasks in a practical setting, comparing numerical simulation and experimental results. Finally, to assess how useful our framework may be for practical learning problems, we perform a simple binary classification task for linearly-inseparable classes. In the examples we consider, strong correlation is observed between quantum systems with a higher REC and success at the considered classification task. This invites the exploration of principles to maximize the finite-sampling REC of a quantum system towards its upper bound. For system analyzed in this paper, we show that an increase in measured correlations of features sampled from the quantum system can aid this goal. 
% % Furthermore, we provide an example which demonstrates \fh{or suggests?} that the calculated eigentasks provide a means to systematically remove high noise features during training, which we show can be harnessed effectively to establish robustness to overfitting, enhancing generalizability. 
% Furthermore, we show empirical evidence that calculated eigentasks provide a means to systematically remove high noise features during training which can be harnessed effectively to establish robustness to overfitting, enhancing generalizability. 
% % This provides important evidence for the utility of eigentasks in optimizing learning with quantum systems under sampling noise.
% }


%% older stuff
% \rev{Physical systems evolving under an input stimulus $\bm{u}$ can be regarded as performing non-trivial computation on its input. Considering the physical system as an input-output system~\cite{boyd_fading_1985}, a given physical system can be thought as transforming an input to an output $x_k$, where $x_k$ are the measured degrees of freedom of the physical system after a certain period of interaction with the input. The resulting natural evolution can be regarded as the physical system computing the functions $x_k(\bm{u})$. This basic observation is behind an emerging approach to machine learning that employs (analog) physical systems for learning~\cite{Wright2022, manypapers}. A broad class of such physical neural networks (PNNs) proceed by embedding data into the evolution of a physical system, where the embedding, dynamics, and extracted outputs via measurement are all governed by a set of general parameters $\bm{\theta}$. Depending on the specific learning scheme, different components $\bm{\theta}$ of this general framework may be trained for optimal performance of a given task~\footnote{To keep the discussion focused, we consider specifically supervised ML. A broader class of learning schemes involve a mechanism for adapting parameters of the physical system to properties of the input. We do not consider these here, though some subset of our results are relevant for such more general context as well.}. In all cases the fundamental role of the physical system is that of a high-dimensional feature generator: given inputs $\bm{u}$, the measured features $x_k$ act as a parameterized feature vector to approximate a function $f (\UI)$ that minimizes a chosen loss function (see Fig.\,\ref{fig:Gen_Schematic}). The relationship between the physical structure of the model and the function classes that can be expressed with high accuracy is a fundamental question of basic importance and integral to a more informed understanding of how to design a successful PNN. Recent results have begun to shed light on this important question~\cite{?}. Yet when it comes to experimental implementations, the presence of noise is found to substantially curtail theoretical expectations for performance~\cite{?}.} \het{Need to have more general references of PNNs and reservoir realizations here, not just quantum refs}

% \rev{Is the sole role of noise to just blur the features of a PNN and hence increase the error in the PNN approximating a desired function of its input? Some findings and observations indicate that noise may play a more subtle role, on account of noise in physical systems generally having a correlated character. From the point of view of sensory input to neural circuits, noise in the input as well as in the output is a natural consequence of the laws of physics governing the signal and the neural circuit components. Therefore evolutionary adaptations to dealing with noise is presumably an important aspect of learning in biological systems. Recent findings in massive imaging studies of the visual cortex provide some evidence in this direction. Even in machine learning algorithms, noise injection is found to improve the ... of DNNs \cite{Goodfellow-et-al-2016, Bishop1995}. The role of regularization in learning, critical to the generalization performance of DNN algorithms, can be regarded as a way to emulate an optimal amount of noise. More broadly speaking, any learning task humans would be interested in ultimately involves electromagnetic, chemical, mechanical etc signals which are subject to noise that is dictated by physical interactions. This aspect is in a certain way swept under the rug when dealing with digital data, but decidedly play an important role in leading to training and inference stages that are highly resource-hungry. The consideration of analog physical systems for learning requires a more nuanced understanding of the impact of noise and how it can be dealt with, offering an opportunity to develop a more detailed and systematic understanding of the role of physical noise in statistical learning theory. Such a new perspective may in turn provide new insight for designing vastly more energy-efficient and faster learning systems that are unlike any of our current digital computing systems.}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % Figure environment removed

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \rev{Given the physical nature of noise, i.e.\,that one can in principle mathematically describe the evolution of the noise associated with the evolution of the physical system itself~\footnote{It is important to realize that inputs are also of physical origin, originating from physical systems that evolve under similar conditions.}, can one come up with systematic strategies to mitigate its impact? This is the basic question that we address in this paper for  appropriately defined models of physical systems. We show that the correlated nature of noise in physical systems allows for extraction of high-SNR functions in physical systems with a large number of degrees of freedom.} 

% \rev{While a heuristic description of noise either in the input or in the operation of the physical system is possible and has been often found to limit the computational capacity of physical~\cite{?} and biological~\cite{Rumyantsev2020} systems for learning, it is quantum theory that provides a fundamental description of {\it sampling noise}. By $S$-shot sampling noise we refer to the stochastic fluctuation in the measured features with the experiment repeated $S$ times under nominally identical conditions for the {\it same input}. Quantum theory describes the act of information processing with a physical system in three steps: System is prepared in an initial state under statistically identical conditions, evolves under a well-defined quantum channel $\mathcal{U}$ in the presence of the input stimulus, and is subject to a well-defined measurement to extract information (output). Quantum measurement theory provides the most general mathematical framework to describe this information processing in three acts: Statistically identical conditions is expressed in terms of an initial state density matrix $\hat{\rho}_0$ for the system, the statistically identical conditions for the evolution (in the presence of an environment with statistically well-defined conditions) is described in terms of a quantum channel $\mathcal{U}$, with its dependence on input being described by the parametric dependence of $\mathcal{U}(\UI)$, yielding after the fixed interaction time the statistically well-defined system state $\hat{\rho}(\UI)$. Finally, the output features $x_k$ are extracted from a set of measurement projectors $\hat{M}_k$, namely $x_k({\UI}) \equiv {\rm Tr}\{\hat{M}_k \hat{\rho}({\UI})\}$}
% ~\footnote{A few things to note here. 1. The initial state preparation can be described as an act of measurement as well. Therefore the input-evolution-output sequence can be described as the sequence of measurement-evolution-measurement sequence. 2. Some PNN realizations view input as provided through an input state $|\Psi (\UI) \rangle$. Within the framework we adopt, this can be described as a parametric evolution $\mathcal{U}(\UI)$ acting on an initial $\UI$-independent state.}.
% \rev{Sampling noise then refers to the statistical properties of the noise in the measured features $X_k$ when $S$ measurements (`shots') are conducted. Only in the asymptotic limit $S \rightarrow \infty$ does $X_k$ self-average to $x_k$. As we show in Section \ref{sec:QSN} this is uniquely described by $\hat{\rho}_0$, $\mathcal{U} (\UI)$ and $\hat{M}_k$ and the noise properties strongly depend on what one would identify as the signal.} 

% \rev{The noise mitigation methodology introduced here, {\it Eigentask learning}, is based on the maximization of a metric we introduce here, the {\it Resolvable Expressive Capacity} (REC), that quantifies the resolvable function space that the physical system can approximate when regarded as an input-output system $x_k(\UI)$ under finite shots $S$. This metric is inspired by a related construction, the {\it Information Processing Capacity}, introduced in Ref.\,\cite{dambre_information_2012} to which it reduces in the asymptotic limit of infinite shots $S \rightarrow \infty$. Given an input $\bm{u}$ to a general dynamical system, we define its REC as a measure of the accuracy with which $K$ orthogonal functions $\{f_\ell (\UI)\}$ of the input can be constructed from $K$ measured features constructed from averaging over $S$ shots. One difficulty in the maximization problem is that functions $\{f(\UI)\}$ are generally (and in practically useful cases, as discussed in the main text) non-linear functions of $\UI$ and these functions live in an essentially infinite dimensional space, as for instance expressed by the fact that the number of orthogonal basis functions to expand $f(\UI)$ is infinite in number. We show here that this non-linear optimization problem can be turned into a linear problem, i.e.\,a quadratic optimization problem, under conditions to be stated in the proof, and an orthogonal set of functions $y^{(k)} (\UI)$ and associated eigenvalues $\beta^2_k$ can be found that is native to the given physical system. These eigentasks can be shown to provide a unique basis which is maximally resolvable, i.e.\,a set of functions that can be approximated with a normalized error given by $1/(1+\beta^2_k/S)$. We then use these results to find a tight bound on the $S$-shot REC given the physical system. The set of eigentasks represent the native set of functions that saturate that bound, and we provide a numerical method of construction from noisy measurements performed on a physical system.}  

% \rev{It is important to compare the contributions of this paper to some earlier work on learning in the presence of noise, that is a particularly important problem for information processing systems that operate in the low signal-to-noise ratio regime. Two notable categories are quantum computers and biological neural circuits.} 

% \rev{Quantum computers operate by encoding information $\UI$ into electromagnetic signals that are close to the lowest limits allowable by quantum theory~\footnote{Here we are referring to qubit-based superconducting quantum processors for concreteness, where information is encoded in microwave pulses that on average contains a single photon at the transition frequency of individual qubits. Higher powers are possible, though little explored, in the presence of larger qubit non-linearities.}. In gate-based systems, measurements are designed to act as projective measurements in the qubit-basis, and hence the shot-noise in individual measurements is highly qubit-state dependent, and can be in the regime SNR $< 1$. In quantum computing, such potentially large fluctuations are dealt with by averaging over as many experiments as is practically possible, or using a large ensemble of identical quantum systems, or more desirably offloaded to the design of algorithms so that the result of computation is in the SNR $\gg 1$ regime (e.g.\,Schor's and Grover's algorithm). It appears however very difficult to evade the issue of dealing with sampling noise in a systematic way for general ML applications with current gate-based schemes~\cite{}. The contribution of this work is to (1) lay the foundations of a general mathematical framework for a systematic analysis of the impact of sampling noise in qubit-based computing systems within the supervised learning framework of Reservoir Computing and Extreme Learning Machines~\cite{fujii_harnessing_2017, chen_temporal_2020, Kalfus_2022, wright_capacity_2019, garcia-beni_scalable_2022, ELMpapers}. (2) Provide a mathematical procedure to extract a native set of maximally resolvable functions, the eigentasks, for a given given physical system and encoding, (3) Introduce a learning scheme based on eigentasks. Eigentask construction entails the construction of a reduced set of measured features that maximally projects out sampling noise as allowed by quantum theory for an $S$-shot measurement. For a simple supervised ML toy problem on continuous-valued inputs, we show that this construction displays robustness to overfitting. (4) Provide a methodology for analyzing the role of quantum correlations in qubit-based quantum systems. An important problem that is left out in our analysis is the determination of the optimal hyperparameters $\bm{\theta}$ to maximize the REC for a given physical system, circumvented here by adopting an approach inspired by reservoir computing. The approach in recent QML work is based on optimizing additionally the {\it internal} parts of the hyperparameters $\bm{\theta}$ {\it for a given supervised ML task}. The present work offers an alternative approach based on a more universal metric that accounts for a finite set of measurements, the $S$-shot REC, rather than the accuracy for a specific ML task. It also suggests, as a second stage, the optimization of the $S$-shot REC itself in the space of $\bm{\theta}$. The properties of that optimization problem, i.e.\,the functional properties of $REC(\bm{\theta})$ will be taken up in future work. We present preliminary evidence for why this may for practical applications be a potent approach.}
% \het{I am referring here to the calculations showing the properties of REC for 2-designs, the evidence for correlation of high REC and a high accuracy in the 1d ML task, the simple hyperparameter-optimization calculation for the correlated vs product circuit problem (optimizing J)}


% \rev{Evolutionary adaptations to dealing with noise in biological neural circuits have been focused on sensory cortical coding of visual signals in the visual cortex. Recent experiments suggest that correlated noise in sensory cortical neural ensembles fundamentally limits the information that brain can extract from the noisy dynamics of sensory neurons, and recent theoretical work proposes that how a neural ensemble’s sensory tuning properties relate statistically to its correlated noise patterns is a greater determinant of coding accuracy than is absolute noise strength. Availability of massive imaging modalities that allows the extraction of simultaneous recording from a large set of neurons has provided unique insight into modes of information encoding that is variable in the noise. Most notably, it is found in recent experimental studies that neural connectivity appears to be adopted so that visual signals were encoded in a space orthogonal to largest noise modes, which therefore limited coding fidelity in a way that is far less detrimental than what would be expected from the absolute noise strength. The eigentask construction introduced here formalizes this idea of information-rich modes, and generalizes it to the function space that can be resolved by a given physical system. The quantum theoretical description is merely to provide a mathematical underpinning, as one has to address the problem of a fundamental (microscopically accurate) description of sampling noise to correctly describe its correlated statistical nature. The results, REC and eigentask construction are very general, and is applicable to any physical or biological system once a consistent model of noise is introduced. We illustrate this with the application of the same mathematical machinery to an optical imaging setup operating in the classical electromagnetic regime that is frequently used in machine learning applications.}



% \rev{Finally, the strategy for defining the noise-constrained REC naturally focuses on accessible noisy output features under a specified measurement scheme, as opposed to unmeasured degrees of freedom. This renders the REC and the set of eigentasks efficiently computable from experimental data without an accurate noise model, as we demonstrate using both numerical simulations and experiments on IBM Quantum's superconducting multi-qubit processors~\cite{IBMQ_2022}. We also demonstrate how this toolbox can be utilized as effective diagnostic tools, showing for instance that the current processors on which our experiments were conducted appear to be limited in capacity by sampling noise (i.e.\,by the available number of shots) rather than other noise sources.}

%A central challenge in determining the REC for \textit{quantum} systems is the fundamentally stochastic nature of measurement outcomes. Even when technical noise due to system parameter fluctuations is minimized as in an error-corrected quantum computer, there is a fundamental level of noise, the quantum sampling noise (QSN), which cannot be eliminated in learning with quantum systems. Quantum sampling noise therefore sets a fundamental limit to the REC of any physical system. Although quantum sampling noise is well-understood theoretically, a formulation of its impact on learning is a challenging task as it is strongly determined by the quantum state of the system relative to the measurement basis, and is highly correlated when quantum coupling is present. Consequently, the impact of quantum sampling noise is often ignored~\cite{fujii_harnessing_2017, benedetti_parameterized_2019, cerezo_variational_2021,schuld_machine_2021, martinez-pena_information_2020} (with a few exceptions~\cite{wright_capacity_2019, arrasmith_effect_2021, garcia-beni_scalable_2022, anshu_sample-efficient_2021}), even though it can place strong constraints on practical optimization~\cite{arrasmith_effect_2021} and performance~\cite{garcia-beni_scalable_2022}. 


%Learning with quantum systems is a promising application of near-term quantum processors, with several recent demonstrations in both quantum machine learning (QML)~\cite{grant_hierarchical_2018, havlicek_supervised_2019, tacchino_quantum_2020, chen_temporal_2020, suzuki_natural_2022, rudolph_generation_2022} and quantum sensing~\cite{meyer_fisher_2021, ma_adaptive_2021, Marciniak_Monz_2022}. A broad class of such data-driven applications proceed by embedding data into the evolution of a quantum system, where the embedding, dynamics, and extracted outputs via measurement are all governed by a set of general parameters $\bm{\theta}$ \cite{benedetti_parameterized_2019,cerezo_variational_2021, schuld_machine_2021}. 
% \ga{moved refs around a bit: added generative qml experiment ref \cite{rudolph_generation_2022}, another quantum sampling noise-aware ref \cite{anshu_sample-efficient_2021}.  Also not sure if \cite{meyer_fisher_2021} belongs above }
%Depending on the learning scheme, different components $\bm{\theta}$ of this general framework may be trained for optimal performance of a given task. In all cases the fundamental role of the quantum system is that of a high-dimensional feature generator: given inputs $\bm{u}$, a set of frequencies for the occurrence of different measurement outcomes act as a parameterized feature vector implementing a function $f (\UI)$ that minimizes a chosen loss function (see Fig.\,\ref{fig:NISQRC_Schematic}). The relationship between the physical structure of the model and the function classes that can be expressed with high accuracy is a fundamental question of basic importance to the success of quantum models. Recent results have begun to shed light on this important question and provide guidance on the choice of parameterized quantum models~\cite{wright_capacity_2019, Du2020, du_efficient_2022, sim_expressibility_2019, wu_expressivity_2021, larose_robust_2020, schuld_effect_2021, Abbas_power_2021,  Holmes2022}.
%Yet when it comes to experimental implementations, the presence of noise is found to substantially curtail theoretical expectations for performance~\cite{grant_hierarchical_2018, havlicek_supervised_2019, tacchino_quantum_2020}.
% \ga{might need to address distinction with works like \cite{Du2020, du_efficient_2022} more directly}


%In this article, we develop a mathematical framework to quantify the REC that exactly accounts for the structure of quantum sampling noise, providing a tight bound for a quantum system with $K$ measurement outcomes under $S$ samples, and illustrate how a mathematical framework for its quantification can guide experimental design for QML applications. While the strength of the REC lies in its generality, we provide numerical examples and experimental results confirming that higher REC is typically indicative of improved performance on specific tasks. As such, the REC provides a metric to guide ans\"atze-design for improved learning performance in a task-agnostic and parameter-independent manner. Specifically, our work identifies enhancement in measurable quantum correlations as a general principle to increase the REC of quantum systems under finite sampling.  
% \ga{I moved these from the following paragraph, not sure why it was part of the eigentask discussion when its referring to REC as a metric.  Also want to be careful with what we mean by `parameter-independent', so tried to clarify with `ansatz-design'.}

%Our work goes beyond simply defining the REC as a figure of merit for parameterized quantum systems, however. In particular, we offer a reliable methodology to identify the native function set that is most accurately resolvable by a given encoding under finite sampling parameterization under quantum sampling noise.
% \ga{I changed this because its unclear what 'encoding' means (also its specialized to supervised learning) but we have parameters defined above.}
%Equivalently, we show that this defines a construction of measured features spanning the accessible information which is optimally robust to noise in readout, thereby furnishing a critical tool employable in any QML framework to improve learning in experimental settings.
% \ga{trying to connect to more general context, also want to add that eigentasks are also the `minimal basis spanning the accessible information' (ie capturing all the functions of the data available via measurement) since 'most accurately realizable', and 'optimally robust to noise' mean the same thing}



% \ga{I think the thought that REC isnt an theoretical metric for an algorithm which may not be realizable in practise (like expressibility), but one of the few that reflects experimental reality, while being task-agnostic (ie its not just accuracy at a specific task) is an important thought to fit in here to help motivate why its more broadly relevant and so want to add:  ``... REC an efficiently-computable quantity in practice, and one which captures the performance of the entire ``quantum computational stack'' -- from an abstract algorithm, to the quantum hardware on which its implemented and the classical electronics used for control and readout.''}


% \section{\sout{Learning with Quantum Systems} 
\section{Theoretical Analysis}
\label{sec:TheoreticalAnalysis}

\subsection{Sampling Noise in Learning with Physical Systems}

The most general approach to supervised learning from classical data using a generic physical system is depicted schematically in Fig.\,\ref{fig:Gen_Schematic}. A table with symbols and abbreviations used in the text can be found in Appendix \ref{app:table}. We consider a scheme that begins with ``embedding'' the classical input data $\bm{u}$, sampled from a distribution $p(\UI)$, into the physical system to be used for learning. The form of this embedding is unrestricted beyond the requirement of being physical, and its precise nature will influence the REC and eigentasks; some concrete examples will be provided shortly.

In order to access information from the physical system after its interaction with the input, measurements must be performed on its $K$ accessible  degrees of freedom. For a fixed input $\bm{u}$, a single measurement or ``shot'' $s$ yields \textit{single-shot} random-valued features $\{X_k^{(s)} (\UI)\}$ for each $k \in [K]$. We define the \textit{measured features} $\bar{X}_k$ as $\NS$-shot sample means of $\{X_k^{(s)} \}$:
\begin{align}
    \bar{X}_k(\UI) = \frac{1}{\NS} \sum_{s=1}^{S} X_k^{(s)} (\UI)
    \label{eq:Xsum}
\end{align}
whose expectation (equivalently via the central limit theorem, the $S$-infinite limit) is given by
\begin{align}
    x_k({\UI}) \equiv \Es{ \bar{X}_k(\UI) } = \lim_{S\to\infty} \bar{X}_k({\UI})
    \label{eq:mapping0}
\end{align}
where $\UI$ is regarded as a free variable. To be more precise, the expectation is evaluated over the product distribution of $S$ independent and identically distributed (i.i.d.)~vectors $\Xs(\UI) \equiv \{(X_{0}^{(s)}(\UI), X_{1}^{(s)}(\UI), \cdots, X_{K-1}^{(s)}(\UI))\}_{s \in [S]}$, conditioned on a fixed $\UI$.

With the definition of their expectation in Eq.\,(\ref{eq:mapping0}), the measured features $\bar{\bm X}(\UI) \in \mathbb{R}^K$, a column vector consisting of $\bar{X}_k(\UI)$,
% \het{Later uses of X e.g. above and below Eq.5 suggests that X is a column vector?} 
can be conveniently decomposed by extracting its deterministic mean value, together with a zero-mean, input-dependent noise term $\boldsymbol{\zeta}(\UI)$:
\begin{equation}
    \bar{\bm X}(\UI) =  {\bm x}(\UI) +\frac{1}{\sqrt{S}}  \boldsymbol{\zeta}(\UI).
    \label{eq:xbar}
\end{equation}
Here $\boldsymbol{\zeta}$ encodes the statistics of sampling; it generally has nontrivial cumulants of all orders, of which the covariances take the particular $S$-independent form $\cu(\UI) \in \mathbb{R}^{K \times K}$:
\begin{align}
    \cu_{jk}(\UI) \equiv \Covs [{\zeta}_j(\UI), {\zeta}_k(\UI)]
    \label{eq:sigmajku}
\end{align}
and only depend on input $\UI$. We note that Eq.\,(\ref{eq:xbar}) is exact. The factor of $1/\sqrt{\NS}$ is merely extracted for convenience of the analysis to follow, and is \textit{not} meant to suggest an expansion for large $\NS$ at this stage; cumulants of $\boldsymbol{\zeta}$ beyond second-order inherit a complicated $S$-dependence. 

The general input-output relationship $\bm{u} \rightarrow \bar{X}_k(\UI)$ above can be made concrete by considering three example physical systems, depicted in Fig.\,\ref{fig:NISQRC_Schematic}. For an {\it optical system}, the input $\UI$ could for instance be embedded as a collection of pixel values on a spatial light modulator (SLM) in the path of a propagating beam of light. The individual single-shot features $\{X_k^{(s)}\}$ could be generated by integrating the photocurrent from each pixel of a number-resolving CCD camera for a certain hold time. For a {\it biological neural circuit}, the input $\UI$ might be a static-in-time visual stimulus, representing the electromagnetic field intensity incident on photoreceptors in the eye, and $\{X_k^{(s)} (\UI)\}$ can be the action potential of the $k$th neuron integrated over a certain time-period, e.g.\,measured through Ca$^{2+}$ imaging \cite{Grienberger2022}. Finally, for a {\it superconducting quantum processor}, inputs may be embedded via a suitable quantum channel, implemented for example via parameterized quantum gates. The single-shot features are simply the indicator functions of the possible outcome labels after quantum measurement. In all cases, the measured features $\bar{X}_k(\bm{u})$ may be obtained by repeating each experiment $S$ times with the same $\UI$  and constructing the $S$-shot histogram.

The randomness of the measured features derives from the quantum mechanical or the thermodynamical nature of the processes that the physical system is subject to during its evolution, but more importantly in the measurement/detection phase. In the case of neural circuits for instance, even when great care is exercised by presenting identical stimuli, the timing of action potentials of individual neurons can vary significantly over repeated trials on a scale that can be physiologically relevant. 
This noise can be traced to various sources~\cite{faisal_noise_2008} including dynamical changes of internal states of neurons between trials, and random processes neurons are subject to. The source of sampling noise for the optical system discussed in Sec.~\ref{sec:photonicRC} is the shot noise related to the discrete nature of energy exchange between the EM field and the photodetector, an electronic system. For an ideal  quantum computing system, the noise process we consider is due to shot noise in projective measurement, which we refer to as quantum sampling noise. We note that in qubit systems, there are many other potential noise sources, but in modern quantum processors these ought to be sub-leading at least for shallow circuits.  Indeed, in experiments reported in Sec.~\ref{subsec:RECqc} we observe that sampling noise dominates even at the maximum available $S$. Quantum sampling noise will still be the limiting source of noise after the advancement of error-corrected quantum computers.

A last important source of noise is the noise in the input signal to be processed. Visual neural circuits for instance involve the absorption of photons that arrive at the photoreceptors from EM sources that are subject to quantum mechanical or thermodynamical fluctuations. Here we are not concerned with a precise description of the physical nature of the input stimuli, and account for it by assuming an underlying probability distribution $p(\UI)$ from which the inputs are sampled. The most complete treatment of such a process requires a quantum mechanical description of both the signal generating system and its coherent coupling to the physical system that processes it, as has been introduced and analyzed in Ref.\,\cite{khan_quantum_2021}.


\subsection{Resolvable Expressive Capacity and Eigentasks}
\label{sec:defwwast}

Returning to the situation depicted in Fig.\,\ref{fig:Gen_Schematic}, supervised learning in physical systems can generically be cast as encoding data in the system, and then using measurement outputs to approximate a desired function $f(\UI)$ (here assumed to be square-integrable $\EUI{f^2} < \infty$), where the expectation over input data $\mathbb{E}_{\UI}$ is defined with respect to the distribution $p(\UI)$: $\EUI{f} \equiv \int \dd \UI \, p(\UI) f(\UI)$. 
%We also make the reasonable specialization to learning target functions that are square-integrable over the input distribution: $\mathbb{E}_{\UI} [|f(\UI)|^2] < \infty$. Recalling that features $\bar{\bm{X}}(\UI)$ are estimators of POVM expectation values -- the linear combination of which can be used to construct all accessible observables --
The introduction of the symbol $\mathbb{E}_{\UI}$ for expectation over $\UI$ is necessitated by the use of two types of averages in the analysis of the loss function: over the output samples ($\mathbb{E}_{\Xs}$) and over the input domain ($\mathbb{E}_{\UI}$).

Within the PNN approach considered here, $f(\UI)$ is approximated for finite $S$ as $f_{\bm{W}}(\UI) = \bm{W}^T \bar{\bm{X}}(\UI) = \sum_{k} W_k \bar{X}_k(\UI)$. To quantify the fidelity of this approximation, we introduce a statistical variant of the function capacity~\cite{dambre_information_2012, wright_capacity_2019, martinez-pena_information_2020}, which is the normalized mean-squared accuracy of the estimate $f_{\bm{W}}$,
% to construct the target function as the minimum achievable (normalized) mean squared error between the target and its estimate:
\begin{align}
    C[f] =  1- \min_{\boldsymbol{W} \in \mathbb{R}^K} \frac{ \EUI{ \mathbb{E}_{\mathcal{X}}[ (f(\UI) - f_{\bm{W}}(\UI) )^2 ]} }{\EUI{f(\UI)^2}}. 
    \label{eq:fcap}
\end{align}
This quantity differs from that introduced in Refs.\,\cite{dambre_information_2012, wright_capacity_2019, martinez-pena_information_2020} in that the squared error term $(f(\UI) - f_{\bm{W}}(\UI) )^2$ is stochastic, and thus both the expectation over the output samples $\mathcal{X}$ and the expectation over the inputs $\bm{u}$ are needed to ensure that Eq.\,(\ref{eq:fcap}) is a deterministic value.
% the right hand side of Eq.\,(\ref{eq:fcap}) is an integral over random variables -- the stochastic readout features $\bar{\bm{X}}(\UI)$. However, through the use of concentration inequalities in probability theory, one can show that it converges to a well-defined deterministic value~\footnote{In physics, such quantities are referred to as self-averaging quantities. (see Eq.\,(\ref{eq:Lambdatilde}) in Appendix \ref{sec:Information_capacity_saturation}). }.
Minimizing the error in the approximation of $f(\UI)$ by $f_{\bm{W}}(\UI)$ over the input domain to determine capacity thus requires finding 
\begin{align}
    \boldsymbol{w} = \underset{\bm{W} \in \mathbb{R}^{K}}{\mathrm{argmin}} \, \EUI{\Es{(f - \bm{W}^T \bar{\bm{X}})^2}}.  \label{eq:optimalw}
\end{align}
This minimization can always be expressed analytically via a pseudoinverse operation (see Appendix \ref{sec:DefCap}). This function capacity is constructed such that $0 \leq C[f]\leq 1$, with the upper limit indicating a perfect approximation. 

The choice of a linear estimator and a mean squared error loss function may appear restrictive at first glance, but the generality of our formalism averts such limitations. The use of a linear estimator applied directly to readout features appears to preclude nonlinear post-processing of measurements; this is intentional and simply meant to ensure the calculated functional capacity is a measure of the ability of the physical system itself, and not of a nonlinear processing layer. Furthermore, the mean squared loss effectively describes the first term in a Taylor expansion of a wide range of arbitrary nonlinear post-processing and non-quadratic loss functions. The most well-known example is that of logistic regression for supervised classification problems, where the sigmoid function $\sigma(\bm{W}^T \bar{\bm{X}})$ (\textit{i.e.,} $\sigma(z)=1/(1+\mathrm{exp}(-z))$) is used for post-processing, while the cross-entropy loss function is used for optimization~(for further details and analysis of non-linear post-processing, see Appendix \ref{app:ComplxNonLin}). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To extend the notion of capacity to a task-independent metric representing how much classical information about an input can be extracted from a system in the presence of sampling noise, we sum the function capacity over a basis of functions $\{ f_\ell \}_{\ell \in \mathbb{N}}$ which are complete and orthonormal with respect to the input distribution, i.e.\,equipped with the inner product $\langle f_{\ell}, f_{\ell'} \rangle_{p} = \int f_{\ell}(\UI) f_{\ell'}(\UI) p(\UI) \dd \UI = \delta_{\ell \ell'}$. The total \textit{Resolvable Expressive Capacity} (REC) is then $\EC \equiv \sum_{\ell=0}^{\infty} C[ f_{\ell} ]$, which effectively quantifies how many linearly-independent functions can be expressed from a linear combination of $\{\bar{X}_k (\UI) \}$. Our main result -- proven in detail in Appendix \ref{app:EC} -- is that given any $S \in \mathbb{N}^{+}$, the REC for a physical system whose measured features are stochastic variables of the form of Eq.\,(\ref{eq:xbar}) is given by
\begin{align}
    \!\!\EC(\bm{\theta}) %= \sum_{\ell} C[f_{\ell})]
    = \mathrm{Tr} \left(\! \left( \gr + \frac{1}{\NS} \ci \right)^{\!\! - 1}\!\! \gr \right) = \sum_{k=0}^{K-1} \frac{1}{1 + \beta_k^2(\bm{\theta})/\NS}. 
    \label{eq:EC}
\end{align}
Here we made explicit the dependence on $\bm{\theta}$, the hyperparameters of the input embedding to indicate the important dependence of the $S$-shot REC on the input encoding. 

The first equality, arrived at through straight-forward algebraic manipulation, is written in terms of the expected feature Gram and covariance matrices $\gr \equiv \EUI{\bm{x}\bm{x}^T}$ and $\ci \equiv \EUI{\boldsymbol\Sigma}$ respectively. 
First, we are able to conclude that $\lim_{S\to\infty} C_T = \mathrm{Rank}\{\gr\} \leq K$, recovering the bound of Ref.\,\cite{dambre_information_2012}. Importantly, the rank of the Gram matrix is always equal to the maximal number of linearly-independent functions in the set $\{x_k(\UI)\}$ (see Appendix \ref{app:eigentasks}) In this article, we only consider the case where $\gr$ is full-rank, which is the most interesting case: maximizing the rank of $\gr$ maximizes the highest achievable (i.e.\,infinite-$\NS$) REC for a physical system. Furthermore, this condition is typically met unless the physical system is constrained by special symmetries; in such cases where some features $\{x_k(\UI)\}$ are linearly dependent, the matrix inverse in Eq.\,(\ref{eq:EC}) should be modified to a pseudo-inverse.
We also later demonstrate that both $\gr$ and $\ci$ can be estimated efficiently and accurately in experiment and consequently under finite $\NS$ (see Appendix \ref{app:Spectral_finite_statistics}). 
The second equality in Eq.\,(\ref{eq:EC}) remarkably provides a closed-form expression for $\EC$ at any $S$, which is independent of the specific choice of the generally {\it infinite} set $\{ f_\ell \}_{\ell \in \mathbb{N}}$ (and thus not subject to numerical challenges associated with its evaluation over such a set~\cite{dambre_information_2012}).
Instead, the REC is entirely captured by the function capacity of $K$ distinct functions, and for a given physical system is fully characterized by the spectrum of eigenvalues $\{ \beta^{2}_k \}_{k \in [K]}$ satisfying the generalized eigenvalue problem
\begin{align}
    \ci \bm{r}^{(k)} = \beta_k^2 \gr \bm{r}^{(k)}. \label{eq:eigenprob}
\end{align}
In the above, all quantities depend on $\boldsymbol\theta$ and thus the specific physical system and input embedding via the Gram ($\gr$) and covariance ($\ci$) matrices.
Associated with each $\beta_k^2$ is an eigenvector $\bm{r}^{(k)}$ living in the space of measured features and thus defining a set of $K$ orthogonal functions via the linear transformation
\begin{align}
    y^{(k)} (\UI) = \sum_{j} r_{j}^{(k)} x_{j} (\UI). 
    \label{eq:eigentaskproj}
\end{align}
We refer to $\{y^{(k)}\}$ as {\it eigentasks}, as they form the minimal set of orthonormal functions ($\EUI{y^{(j)} y^{(k)}} = \delta_{jk}$) which saturates the available REC of a physical system and thus the accessible information content present in its measured features. Specifically, the capacity to approximate a given $y^{(k)}$ with $S$ shots is $C[y^{(k)}]=1/(1+\beta^2_k/S)$: the REC in Eq.\,\eqref{eq:EC} is simply a sum of eigentask capacities. This further highlights that a given parameterized system can only approximate a target function to the degree that it can be written as a linear combination of $\{y^{(k)}\}$.  The eigentasks thus serve as a powerful basis for learning, as shall be explored in Sec.\,\ref{sec:eigentasklearning}.

\subsection{Resolvable Expressive Capacity and Eigentasks in practice: measured eigentasks}

Our use of the expectation over distributions of the input, $\EUI{\cdot}$, and finitely-sampled measured features, $\Es{\cdot}$, in principle implies the availability of an infinite number of input and measured samples respectively. Of course, for the practical implementation of any PNN, both these values are finite. However, as we will demonstrate via calculations of the REC and eigentasks using both theoretical and experimental systems, our framework can be applied when these values are constrained to be finite. 

More precisely, we note that in practice only a finite number of values $N$ can be i.i.d.~sampled from the input distribution, namely $\UI^{(n)} \sim p(\UI)$ for any $n \in [N]$. For each discrete input, one set of measured output features constructed from finite $\NS$ is obtained, a single sample from the distribution $\mathcal{X}(\bm{u}^{(n)})$. The collection of both input and output samples constitutes the \textit{complete dataset}, which we denote as $\mathcal{D} \equiv \{(\bm{u}^{(n)}, \mathcal{X}(\bm{u}^{(n)}))\}_{n \in [N]}$. Our calculation of REC and eigentasks will have some dependence on $\mathcal{D}$ via $N$ and $\NS$.

In particular, the practically computed optimal weights in the capacity calculation are not the deterministic weights $\bm{w}$, but $\bm{w}^{\ast}$ computed on a given set of input samples and measured features, and hence depend on the dataset $\mathcal{D}$:
\begin{align}
    \bm{w}^{\ast}(\mathcal{D}) \equiv \underset{\bm{W} \in \mathbb{R}^{K}}{\mathrm{argmin}} \frac{1}{N} \! \sum_{n=1}^{N} \! \left( f(\UI^{(n)}) - \bm{W}^T \bar{\bm{X}}(\UI^{(n)}) \right)^2 \!. 
\end{align}
$\bm{w}^{\ast}(\mathcal{D})$ will vary due to changes in $\mathcal{D}$.
Generally, when $N,S$ are simultaneously finite and $S$ is fixed, a study of the $N$-scaling behavior of the difference between $\bm{w}$ in Eq.\,(\ref{eq:optimalw}) and the average optimized weight $\mathbb{E}_{\mathcal{D}}[\bm{w}^{\ast}(\mathcal{D})]$, as well as the variation of $\bm{w}^{\ast}(\mathcal{D})$ for different $\mathcal{D}$, falls in the realm of training and generalization errors over the input domain, an important area of research in {theoretical} machine learning~\cite{Seung1992, Canatar2021}. We leave this problem for future work; for all calculations and experiments in this paper, we consider the case - always realized in practice (and of particular relevance where \textit{sampling}, and thus the time and resource cost of processing with physical systems, is concerned) - where the dataset consists of a finite number $N$ of input samples.
% and only consider a fixed number of input samples $N$ for all our calculations in the numerical and experimental parts of this paper.
% \ga{Could this section and its connection to the following paragraph be worded in a way that emphasizes the finite N case we consider is what is meaningful in practice?  ie any real dataset consists of a finite number of points; and indeed sampling noise is only relevant due to the fact that it takes a finite amount of time to process each data point (otherwise we could take N to infinity along with S).  ie the changes in braces}

We do address the important problem of REC and eigentask calculation when this fixed value of $N$ is finite, and using only a given set of measured features constructed under finite sampling $\NS$. As alluded to earlier, in Appendix~\ref{app:Spectral_finite_statistics} we demonstrate how the eigenproblem Eq.\,(\ref{eq:eigenprob}) can be constructed for finite $N$ and $\NS$, and present corrections to the eigenvalues and eigenvectors due to the finiteness of $\NS$. Numerical examples presented in Appendix~\ref{app:Spectral_finite_statistics} demonstrate a favorable match between this correction method and numerical simulations of eigenvalues and eigenvectors (see Fig.\,\ref{fig:NSR_Tilde_Renormalization} and Fig.\,\ref{fig:Tilde_Renormalization}). 

Importantly, we define a set of {\it measured} eigentasks $\bar{y}^{(k)}(\UI) = \sum_j {r}_j^{(k)} {\bar{X}}_j(\UI)$ constructed from a given set of measured features. For these measured eigentasks, we find (see Appendix \ref{sec:noisy_ET}) that $\{\bm{r}^{(k)}\}$ specify a unique linear transformation that simultaneously orthogonalizes not only the signal, but also the associated noise: $\EUI{\Es{\bar{y}^{(j)} \bar{y}^{(k)}} } = \delta_{jk}(1+\beta^2_{k}/S)$. The term $\beta^2_k/S$ is thus the mean squared error, or noise power, associated with the approximation of eigentask $y^{(k)}$; equivalently,  $\bar{y}^{(k)}$ has a signal-to-noise ratio of $S/\beta^2_k$. This leads to a natural interpretation of $\{\beta_k^2\}$ as noise-to-signal (NSR) eigenvalues. The eigentasks, ordered in increasing noise strength $0 \leq \beta^2_0 \leq \beta^2_1 \leq \cdots \leq \beta^2_{K-1} < \infty$, are the orthogonal set of functions maximally robust to sampling noise. 

% Defining {\it measured} eigentasks $\bar{y}^{(k)}(\UI) = \sum_j {r}_j^{(k)} {\bar{X}}_j(\UI)$, we find (see Appendix \ref{sec:noisy_ET}) that $\{\bm{r}^{(k)}\}$ specify a unique linear transformation that simultaneously orthogonalizes not only the signal, but also the associated noise: $\EUI{\Es{\bar{y}^{(j)} \bar{y}^{(k)}} } = \delta_{jk}(1+\beta^2_{k}/S)$. The term $\beta^2_k/S$ is thus the mean squared error, or noise power, associated with the approximation of eigentask $y^{(k)}$; equivalently,  $\bar{y}^{(k)}$ has a signal-to-noise ratio of $S/\beta^2_k$. This leads to a natural interpretation of $\{\beta_k^2\}$ as noise-to-signal (NSR) eigenvalues. The eigentasks, ordered in increasing noise strength $0 \leq \beta^2_0 \leq \beta^2_1 \leq \cdots \leq \beta^2_{K-1} < \infty$, are the orthogonal set of functions maximally robust to sampling noise. 

% \rev{The pragmatic training process involves determining an optimal weight denoted as $\bm{w}^{\ast}$, a value that relies on the complete dataset $\mathcal{D}$
% \begin{align}
%     \bm{w}^{\ast}(\mathcal{D}) \equiv \underset{\bm{W} \in \mathbb{R}^{K}}{\mathrm{argmin}} \frac{1}{N} \sum_{n=1}^{N} \left( f(\UI^{(n)}) - \bm{W}^T \bar{\bm{X}}(\UI^{(n)}) \right)^2. 
% \end{align}
% The stochastic nature of the weight $\bm{w}^{\ast}$ as defined above originates from the inherent randomness present within the specific complete dataset $\mathcal{D}$. Consequently, it becomes logical to conceptualize its expectation, denoted as $\mathbb{E}_{\mathcal{D}}[\bm{w}^{\ast}(\mathcal{D})]$, as the \textit{average learned weight}. $\mathrm{\mathbb{E}_{\mathcal{D}}}[\bm{w}^{\ast}(\mathcal{D})]$ is supposed to be closed to $\bm{w}$ when taking sufficiently large $N$. Investigating their difference for finite $N$ (while also considering finite $S$) introduces the challenge of addressing training and generalization errors. In prior research, a preliminary exploration was conducted in Ref.\,\cite{Canatar2021}, where the impact of Gaussian noise in the dataset was examined. However, when dealing with more generalized forms of noise affecting readout features, the implications stemming from the finite nature of $N$ and $S$ remain somewhat obscure. Nevertheless, during theoretical presentations, we often disregard the distinction between $\mathbb{E}_{\mathcal{D}}[\bm{w}^{\ast}(\mathcal{D})]$ and $\bm{w}$ when $N$ is large enough. As a result, our primary focus remains on the weight $\bm{w}$. 
% }

% \rev{Eigenproblem solving under finite N and S...eigentasks to conclude...}



\subsection{Example: Resolvable Expressive Capacity and Eigentasks for a Classical Optical Learning System}
\label{sec:photonicRC}

Before presenting more involved examples of physical quantum systems, we discuss an example of the presented framework for noisy \textit{classical} dynamical systems, within the popular PNN platform of photonic ELM \cite{pierangeli_photonic_2021, ortin_unified_2015} and reservoir computing (RC)~\cite{Tanaka2019, dong_optical_2020}.
% \ga{if dont define RC in intro, do here: ..RC } %probably dont need all the elm refs \fh{OK}
The specific setup we consider is illustrated in Fig.\,\ref{fig:photonicEC}(a), where computation of inputs $\bm{u}$ is performed via the encoding, propagation, and measurement of propagating electromagnetic (EM) waves in a medium.  Here the entire 3-D space is defined by coordinates $(q^1,q^2,q^3)$, and EM fields of wavelength $\lambda$ propagate in the $q^3$ direction. The electric field distribution is then completely defined by the position vector $\vec{d}$ defined in the plane orthogonal to the propagation direction, so that $\vec{d} = (q^1,q^2)$.~\footnote{We assume the validity of the parabolic approximation here.}

The input embedding of $\bm{u}$ is performed using a spatial light modulator that modulates the amplitude and/or phase of the electric field of the radiation as it passes through. We will restrict this example to 1D inputs $u$ that are uniformly distributed, $p(u)=\mathrm{Unif}[-1,1]$. The scalar $u$ is then mapped to all the pixels of the SLM through a specific mapping discussed in Appendix~\ref{app:photonicRC1}. We consider this rather artificial input encoding for two reasons: for ease of visualization of the computed eigentasks (see \Fig{fig:photonicEC}), and to ensure the distribution is sufficiently sampled. 
In the simulation of the classical optical system we consider here, we choose $N=300$. This is also the number of input samples used in our analysis of qubit-based quantum systems in Sec.\,\ref{sec:ibmq}.

The spatial profile of the electric field $E_0(u;\vec{d})$ following the SLM can be written generally in the form $E_0(u;\vec{d}) \!=\! A_0 \cos (\frac{\varphi_1(u;\vec{d})}{2} ) \exp \! \left\{\! i \!\left(\frac{\varphi_1(u;\vec{d})+2\varphi_2(u;\vec{d})}{2}\right) \!\right\}$, where $A_0$ is the initial electric field amplitude and $\varphi_l(u;\vec{d})$ are input encoding functions~\cite{zhu_arbitrary_2014} (cf.\,\Eqs{eq:inputenc1}{eq:inputenc2}). Following the input encoding, the radiation propagates through free space and then past a thin lens. The electric field in the focal plane of the lens, $E(u;\vec{d})$, can be shown to be related to the initial field $E_0(u;\vec{d})$ via a Fourier transform~\cite{saleh_fundamentals_1991, yariv_photonics_2007}, $E(u;\vec{d}) = \int\!\!\int \dd^2 \vec{d}'~E_0(u;\vec{d}')\exp\left\{\frac{i2\pi}{\lambda f}\left( \vec{d}\cdot\vec{d}' \right) \right\}$, where $f$ is the focal length of the lens. The choice of the optical propagation medium as a lens is again for convenience of analysis, not a limitation. More complex optical systems can be analyzed using the same techniques outlined here. 

Finally, output features are extracted via photodetection (using a CCD camera) in the focal plane of this lens. Modeling this effectively requires us to address the important question of the measurement noise associated with photodetection. First, we consider the camera plane as being comprised of a discrete set of $K=P^2$ photodetectors (here $P=8$), arranged in a $P$-by-$P$ square spatial grid, such that the $k$th photodetector is identified with coordinates $\vec{d}_k = (q^1_k,q^2_k)$, and $k\in[K]$ (See Fig.\,\ref{fig:photonicEC}(a)). This spatial grid ultimately defines the coarse-graining level at which the propagating fields can be probed, and is set by the spatial resolution of the photodetection apparatus, as expected. Then the differential, stochastic photocurrent generated in a given photodetector in a single measurement -- namely the increment in photodetector counts in the time window $[t,t+\dd t]$, which we denote as $\dd N(\vec{d}_k,t)$, follows a Poisson point process (commonly referred to as shot noise)~\cite{wiseman_quantum_2009}. This instantaneous photocurrent $\dd N(\vec{d}_k,t)$ is often integrated over a finite time $T_{\mathrm{int}}$ in so-called integrate-and-dump photodetectors. This defines the single-shot features of this RC scheme, 
\begin{align}
    X^{(s)}_k = \int_{0}^{T_{\mathrm{int}}}~\dd N^{(s)}(\vec{d}_k,t) \in \mathbb{N}.
    \label{eq:XkphotonicRC}
\end{align}
% where $M = \mathcal{T}/\dd t$ defines the total number of $\dd t$ intervals in the total window $\mathcal{T}$. 
Note that $X^{(s)}_k$ are stochastic, integer quantities, simply counting the total number of photo-generated carriers in a time window $T_{\mathrm{int}}$ of a single measurement.  
% \sout{Again, this is a typical choice for illumination by classical sources, but not unique. Different construction of the measured features can be chosen as well.} \ga{feel its already clear you are giving a specific non-unique example here}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Figure environment removed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% This stochastic increment has the specific statistical properties:
% \begin{subequations}
% \begin{align}
%     \mathbb{E}[\dd N(\vec{d}_k,t)] &= \eta~P(\vec{d}_k)~\dd t \label{appeq:meandN} \\
%     \mathbb{E}[\dd N(\vec{d}_k,t)\dd N(\vec{d}_{k'},t')] &= \mathbb{E}[\dd N(\vec{d}_k,t)]\mathbb{E}[\dd N(\vec{d}_{k'},t')], \quad {\rm if }~\vec{d}_k\neq \vec{d}_{k'} \\
%     \mathbb{E}[\dd N^2(\vec{d}_k,t)] &= \eta~P(\vec{d}_k)~\dd t + (\eta~P(\vec{d}_k)~\dd t)^2 = \eta~P(\vec{d}_k)~\dd t + (\mathbb{E}[\dd N(\vec{d}_k,t)])^2 \label{appeq:vardN}
%     % dN^2(x_i,y_j,t) &= dN(x_i,y_j,t) 
% \end{align} 
% \end{subequations}
% % $\delta(t-t')$ is the Dirac $\delta$-function, $\delta_{l,l'}$ is the Kronecker $\delta$-function, and 
% where expectation values are formally computed over the distribution of Poisson point processes $dN(\vec{d}_k,t)$. Importantly, $P(\vec{d}_k)$ is the power incident on the photodetector with spatial coordinate. Then, the expectation value of the increment in counts is directly proportional to the intensity of the incident radiation and a measurement efficiency factor $\eta$. The second term indicates that counts on spatially distinct photodetectors and at distinct times are uncorrelated. Finally, the third line provides the second-order moment of photodetector increments. The second term here appears at $O(\dd t^2)$, but is essential to retain as it is simply equal to the square of the expectation value of the increments, and therefore cancels out in the calculation of variances.
% }

We now make the important physical connection between the measured photocurrents and the propagating fields reaching the photodetector. The power incident on the $k$th photodetector is simply set by the Poynting flux of the propagating fields, and is proportional to the electric field intensity, $\alpha|E(u;\vec{d}_k)|^2$, where $\alpha$ is a dimensionful constant that depends for example on the speed of light through the medium of traversal. Then, the expected value of the photocurrent in a time interval $\dd t$ is simply proportional to the incident power, up to a factor $\eta$ that encapsulates the efficiency of photodetection, $\mathbb{E}[\dd N(\vec{d}_k,t)] = \eta \alpha|E(u;\vec{d}_k)|^2~\dd t$.

% This also allows us to easily right down the statistical properties of the integrated photocurrent using Eqs.~(\ref{appeq:meandN})-(\ref{appeq:vardN}):
% \begin{subequations}
% \begin{align}
%     \mathbb{E}[I(\vec{d}_k)] &= \eta~P(\vec{d}_k)~T_{\rm int}  \\
%     \mathbb{E}[I(\vec{d}_k)I(\vec{d}_{k'})] &= \mathbb{E}[I(\vec{d}_k)] \mathbb{E}[I(\vec{d}_{k'})], \quad {\rm if }~\vec{d}_k\neq\vec{d}_{k'} \\
%     \mathbb{E}[I^2(\vec{d}_k)] &= \eta~P(\vec{d}_k)~T_{\rm int} + (\eta~P(\vec{d}_k)~T_{\rm int})^2 = \eta~P(\vec{d}_k)~T_{\rm int} + (\mathbb{E}[I(\vec{d}_k)])^2
% \end{align} 
% \end{subequations}
% The final expression then provides the variance of the integrated photocurrent for a given photodetector indexed by $k$:
% \begin{align}
%     \sigma_{\rm I}^2(\vec{d}_k) = \mathbb{E}[I^2(\vec{d}_k)]-(\mathbb{E} [I(\vec{d}_k)])^2 = \eta~P(\vec{d}_k)~T_{\rm int}
% \end{align}

% It is common to calculate the measured power signal to noise ratio of the integrated photocurrent, ${\rm SNR}_I$, which takes the form
% \begin{align}
%     {\rm SNR}_I = \frac{(\mathbb{E}[I(\vec{d}_k)])^2}{\sigma_{\rm I}^2(\vec{d}_k)} = \eta~P(\vec{d}_k)~T_{\rm int},
% \end{align}
% and therefore grows with incident power $P(\vec{d}_k)$ and integration time $T_{\rm int}$ (assuming an unsaturated photodetector).

% To emphasize this, we now define the measured features $\bar{X}_k(u)$ extracted from this classical machine analogously to the case of measured features extracted from quantum systems, namely Eq.\,(\ref{eq:Xsum}). Precisely, we define $\bar{X}_k(u)$ as averages over individual shots $s$ of integrated photocurrents for each photodetector,
% \begin{align}
%     \bar{X}_k(u) = \frac{1}{\NS}\sum_s I^{(s)}(u;\vec{d}_k),
% \end{align}

% $\bar{X}_k(u)$ are therefore sums over i.i.d. random variables $I^{(s)}(u;\vec{d}_k)$. Hence we can directly write for the mean and covariance of these measured features calculated from infinitely-many samples:
% \begin{subequations}
% \begin{align}
%     \mathbb{E}[\bar{X}_k(u)] &= \mathbb{E}[I(u;\vec{d}_k)] = \eta\alpha|E(u;\vec{d}_k)|^2T_{\rm int}  \\
%     {\rm Cov}[\bar{X}_j, \bar{X}_k](u) &= \frac{1}{\NS} \delta_{jk}{\rm Cov}[\bar{I}_j, \bar{I}_k](u) = \frac{1}{\NS} \delta_{jk} \eta\alpha|E(u;\vec{d}_k)|^2T_{\rm int}
% \end{align}  
% \end{subequations}

% To connect with our prior notation, we further write:
% \begin{align}
%     \bar{X}_k(u) = x_k(u) + \frac{1}{\sqrt{\NS}}\zeta_k(u)
%     \label{appeq:xbarCML}
% \end{align}
% which is simply Eq.\,(\ref{eq:xbar}) of the main text. 

The complete input-output map defined above fits within our very general framework. In particular, we define measured features $\bar{X}_k(u)$ as $\NS$-shot sample means of $\{X_k^{(s)}\}$, as in Eq.\,(\ref{eq:Xsum}).
% Note that for most classical PNN schemes, $\NS=1$; however, in our analysis we allow the shot number $\NS$ to be a general value. 
Note that in most classical PNN schemes $\NS=1$; here we consider the shot number $\NS \geq 1$. % \ga{minor rephrasing}
We then express the measured features in terms of the decomposition in Eq.\,(\ref{eq:xbar}). First, using Eq.\,(\ref{eq:XkphotonicRC}) and the definition of $x_k(u)$ in Eq.\,(\ref{eq:mapping0}), we find:
\begin{align}
    x_k(u) = \eta\alpha|E(u;\vec{d}_k)|^2 T_{\mathrm{int}}. 
    \label{eq:xCML}
\end{align}
Then, the remaining term in Eq.\,(\ref{eq:xbar}), $\frac{1}{\sqrt{\NS}}\zeta_k(u)$, is a stochastic process with zero mean, and its second-order moment encodes the variance of the Poisson point process in one shot of the experiment, namely that its variance is equal to its mean~(see Appendix~\ref{app:photonicRC} for details),
\begin{align}
    % {\rm Cov}[{\zeta}_j, {\zeta}_k](u) 
    % \equiv
    \cu_{jk}(u) = \delta_{jk} x_k(u).
    \label{eq:sigmaCML}
\end{align}
%We emphasize once more that the factor of $\frac{1}{\sqrt{\NS}}$ extracted from the stochastic term is \textit{not} indicative of any expansion in $\NS$, but merely a convenient choice. 
The form of the covariance matrix here is specific to the Poisson nature of the noise process inherited from the classical nature of the source (e.g.\,a coherent light source such as a laser) generating the beam of light. Other types of noise processes will yield distinct covariance matrices, as we will see in examples of quantum systems.

This is an appropriate place to remark that for such a classical description of a physical system for learning, the stochastic photocurrent $\dd N$ for any shot is determined by the \textit{deterministic} electric field incident on the camera plane. In a fully quantum description, on the other hand, the power incident on the photodetectors in a given shot will be determined by the expectation value of the field excitation number operator $\hat{\Psi}^{\dagger}\hat{\Psi}$ (in second-quantized notation) with respect to the \textit{conditional} density matrix $\hat{\rho}^{(c)}$, describing the measurement-conditioned state of the propagating radiation field for that shot. We are compelled to consider a description of this sort when describing physical \textit{quantum} systems in Sec.\,\ref{sec:QSN}. 

% Note that the specific form of the covariance matrix is calculable due to the Poisson nature of the noise process, and takes a distinct form in comparison to the multinomial distribution for measurements of an $L$-qubit system, Eq.\,(\ref{eq:cov}).

Eqs.\,(\ref{eq:xCML}) and (\ref{eq:sigmaCML}) are sufficient to calculate the feature Gram and covariance matrices $\gr$ and $\mathbf{V}$ respectively, as per the discussion following Eq.\,(\ref{eq:EC}). We are thus set up to solve the eigenproblem of Eq.\,(\ref{eq:eigenprob}) and obtain the NSR spectrum and eigentasks for this toy model of a classical optical PNN. We first present the spectrum of NSR eigenvalues $\beta_k^2$ in Fig.\,\ref{fig:photonicEC}(b). The NSR spectrum allows calculation of the REC as a function of $\NS$ using Eq.\,(\ref{eq:EC}); this is shown in Fig.\,\ref{fig:photonicEC}(c). At finite $\NS$, we clearly observe that the REC remains below its upper bound of $K=64$, only approaching it when $\NS$ is increased, reducing the impact of sampling noise on measured features.  


Finally, we discuss eigentask construction, also obtained by solving the eigenproblem of Eq.\,(\ref{eq:eigenprob}).  
% \sout{The use of 1D inputs $u$ makes the eigentasks readily visualizable.} 
% We thus show a selection of $\NS\to\infty$ eigentasks ${y}^{(k)}(u)$ defined in Eq.\,(\ref{eq:eigentaskproj}). Also shown are the measured eigentasks using sampled features $\bar{y}^{(k)}(u)$, here for a single-shot $\NS = 1$, in Fig.\,\ref{fig:photonicEC}(c).  
In Fig.\,\ref{fig:photonicEC}(c) we visualize as a function of $u$ a selection of both the $\NS\to\infty$ eigentasks ${y}^{(k)}(u)$ defined in Eq.\,(\ref{eq:eigentaskproj}), and the measured eigentasks $\bar{y}^{(k)}(u)$ obtained from $\NS = 1$ sampled features (i.e.~single-shot).
% \ga{reworded for verbosity}
We note that the sampled features are obtained by numerically integrating the stochastic differential equation defining independent measurements of the stochastic photocurrents in Eq.~(\ref{eq:XkphotonicRC}). These measured eigentasks exhibit sampling noise, which is evident when compared against the infinite shot eigentasks. 
% We clearly see that eigentasks that are higher order in $k$ demonstrate higher noise across the input domain, as also encapsulated by the larger NSR eigenvalues. The ordered eigentasks therefore represent the functions that are optimally-resolvable using this classical optical setup subject to the sampling noise it is naturally, and unavoidably, subject to under finite $\NS$.
We clearly see that eigentasks which are higher-order in $k$ are increasingly noisy across the input domain, as also encapsulated by the larger associated NSR eigenvalues. The ordered eigentasks therefore represent the functions that are optimally-resolvable using this classical optical setup in the presence of the sampling noise that it is naturally, and unavoidably, subject to for finite $\NS$.


% \subsection{REC analysis of classical photonic reservoir computer}

% For the Poisson noise process of photodetection, the noise matrix $\ci$ is different compared to the case of the multinomial noise process. Fortunately, for the specific case we have considered with spatially uncorrelated detectors, $\ci$ is in fact simpler. We note that:
% \begin{align}
%     \ci_{jk} = \int du~p(u) \cu_{jk}(u) = \int du~p(u) \delta_{jk}x_k(u) 
% \end{align}
% Hence $\ci$ is itself now diagonal. The eigenproblem in question, Eq.\,(\ref{eq:eigenprob}), can therefore be simplified to:
% \begin{align}
%     \ci \bm{r}^{(k)} = \beta_k^2 \gr \bm{r}^{(k)} \implies \ci^{-1}\gr \bm{r}^{(k)} = \frac{1}{\beta_k^2}\bm{r}^{(k)}
%     \label{appeq:eigenprobCML}
% \end{align}
% where we have computed the inverse of $\ci$ since it is a diagonal matrix; we also assume none of its diagonal entries vanish. This is not a strong constraint, since these entries are simply equal to the measured features, which are the sum of intensities incident on the photodetector, and will typically be nonzero (assuming integrated photocurrents from any `dead' photodetectors or pixels are excluded from the measured features). 

% Solving Eq.\,(\ref{appeq:eigenprobCML}) allows us to calculate the infinite-shot eigentasks and the $\EC$ as a function of $\NS$ for the photonic reservoir computer.

\hspace{0.5cm}

\section{Learning with Quantum Systems}
\label{sec:QSN}

% Our formulation of REC hence encompasses general quantum states, to the best of our knowledge the first of its kind, going beyond characterizations of noise-constrained capacity that have been attempted for linear classical systems~\cite{hermans_memory_2010} and Gaussian quantum systems~\cite{garcia-beni_scalable_2022}. The eigentasks then reveal the set of orthogonal functions best approximated by the quantum system, and hence are sensitive to properties such as the degree of quantum correlations. Finally, the fidelity of approximation of these native functions -- determined by NSR eigenvalues -- is constrained fundamentally by quantum sampling noise.

\subsection{Sampling Noise in Quantum Systems}

Having developed our framework for REC in the most general context, in the remainder of this paper we will use it to analyze \textit{quantum} systems in greater depth. The same quantitative metrics -- REC, eigentasks, and NSR eigenvalues -- now carry the significance of being determined by a parameterized quantum state. To be more specific,
the classical data $\bm{u}$ is now encoded through a quantum channel parameterized by $\bm{\theta}$ acting on a known initial state, 
\begin{align}
    \hat{\rho}(\UI;\bm{\theta}) = \mathcal{U}(\UI;\bm{\theta} )\hat{\rho}_0, \label{eq:qchannel} 
\end{align}
whose data-dependence may be hard to model classically. The quantum channel $\mathcal{U}$ includes all quantum operations applied to the input data; to obtain the computational output or perform further classical processing, one must extract information from the quantum system via a set of measurements described most generally as a positive operator-valued measure (POVM). Specifically, we define a set of $K$ POVM elements \{$\hat{M}_k$\}, each associated with a distinct measurement outcome indexed $k$, and  constrained only by the normalization condition $\sum_{k=0}^{K-1}\hat{M}_k = \hat{\mathbf{I}}$ (and hence not necessarily commuting). 

Each shot then yields a discrete index $k^{(s)}(\UI)$ specifying the observed outcome: for input $\UI$, if outcome $k$ is observed in shot $s$ then $k^{(s)}(\UI) \gets k$. In this case, the single-shot random-valued feature $X^{(s)}_k(\UI)$ is exactly the indicator $\delta(k^{(s)}(\UI), k)$ of index $k$, so that the {\it measured features} are given by:
\begin{align}
    \bar{X}_k(\UI) = \frac{1}{\NS} \sum_s \delta(k^{(s)}(\UI), k). 
    \label{eq:Xsum0}
\end{align}
Hence $\bar{X}_k(\UI)$ in this case is the empirical frequency of occurrence of the outcome $k$ in $S$ repetitions of the experiment with the same input $\UI$. These measured features are formally random variables that are unbiased estimators of the expected value of the corresponding element $\hat{M}_k$ as computed from $\hat{\rho}(\UI)$. Explicitly
\begin{align}
    x_k({\UI}) = {\rm Tr}\{\hat{M}_k \hat{\rho}({\UI};\bm{\theta})\}, \label{eq:mapping}
\end{align}
so that $x_k$ is the probability of occurrence of the $k$th outcome as specified by the quantum state.  These probability amplitudes encompass the accessible information in $\hat{\rho}({\UI};\bm{\theta})$: any observable under this set can be written as a linear combination of POVM elements $\hat{O}_{\bm{W}} = \sum_k W_k \hat{M}_k$, such that $\langle \hat{O}_{\bm{W}} \rangle = \bm{W}^T \bm{x}$.

In quantum machine learning (QML) theory, it is standard to consider the limit $\NS \to \infty$, and to thus use expected features $\{x_k(\UI)\}$ for learning. In any actual implementation however, measured features $\{\bar{X}_k(\UI)\}$ must be constructed under finite $\NS$, in which case their fundamentally quantum-stochastic nature can no longer be ignored. The decomposition Eq.\,(\ref{eq:xbar}) is still applicable $\bar{\bm X}(\UI) =  {\bm x}(\UI) +  \boldsymbol{\zeta}(\UI)/\sqrt{S}$, where now $\bm{x}$ are the quantum-mechanical event
probabilities, and $\boldsymbol{\zeta}$ encodes the multinomial statistics of quantum sampling noise, whose covariance is explicitly
\begin{align}
    % {\rm Cov}[{\zeta}_j, {\zeta}_k](\UI) 
    % \equiv
    \cu_{jk}(\UI) = \delta_{jk} x_k(\UI) - x_j(\UI) x_{k}(\UI). 
    \label{eq:cov}
\end{align}
This is simply the expression for the covariance of multinomial distribution with  $\NS$ trials and $K$ mutually exclusive outcomes  with probabilities $p_k=x_k$. For arbitrary orders of cumulants of multinomial statistics, we refer to Ref.\,\cite{Wishart1949}. For the quadratic loss function considered here, only the cumulants up to second order turn out to be sufficient.

One may wonder what specifically distinguishes a quantum system from a classical stochastic system that can generate a multinomial distribution in its output. %One specific distinction is that certain $\{ p_k(u) \}$ combinations can be generated by only a quantum system. In other words, a quantum system can access a far larger set of $\{ p_k \}$ than a classical stochastic system and hence, as will be discussed later, the accessible space of functions is far richer. 
Firstly, certain $\{ p_k(u) \}$ combinations can be generated efficiently by only a quantum system. That is to say, given equal resources, a quantum system can access some $\{ p_k \}$ that may be inaccessible to any classical stochastic system and hence, as will be discussed later, the accessible space of functions is far richer. 
% \ga{reword "one ...specific distinct.. x 2"}
However, we will also find that resolvability of that function space in $S$ measurements is the key determinant in learning.  
Note that all statistical properties of stochastic readout features $\bar{\bm X}(\UI)$ -- namely first-order cumulants $\bm{x}(\UI)$, second-order cumulants $\bm{\Sigma}(\UI)$, and all higher-order cumulants -- are determined fully by the quantum state $\hat{\rho}({\UI})$, which itself may be hard to generate classically.

%From Eq.\,(\ref{eq:cov}), we can conclude that the quantum nature beyond the general decomposition Eq.\,(\ref{eq:xbar}) is incorporated in the fact that for quantum systems  

% Before continuing applying our capacity analysis to quantum system, we note that when viewed in isolation Eq.\,\ref{eq:xbar} defines an extremely general map between inputs $\UI$ and outputs $\bar{\bm X}(\UI)$ assembled from $S$ measurements. The readout features it describes could therefore have been extracted from \textit{any} dynamical system with stochastic outputs, including systems that are entirely classical. This is no restriction -- Eq.\,(\ref{eq:xbar}) also applies to a very broad class of quantum systems: ultimately, measurement outcomes from quantum systems are also recorded by an observer as classical stochastic variables. Where, then, is the quantum nature of measured features apparent? 

% The framework we develop here allows characterization of the function-learning capacity of any noisy dynamical system satisfying Eq.\,\eqref{eq:xbar}, and provides a practical, experimentally applicable methodology to optimize learning by avoiding overfitting to noise in ML tasks. When applied to classical systems, it can be viewed as a means of statistical inference based on data containing classical noise~\cite{cox_principles_2006}. However, our primary interest is the study of quantum systems, where the fundamental model for the noise process depends nontrivially on the quantum state, a dependence we account for exactly. This enables us to extract the limits on function-learning capacity set by quantum sampling noise and its dependence on the encoding. In this paper, using both theoretical studies and experiments on real quantum devices, we analyze how this capacity depends on quantum properties such as the degree of measured correlations, and how our framework can be applied for optimal learning in practical QML tasks such as classification.

To proceed with our REC analysis in quantum systems, we write down the generalized eigenproblem Eq.\,(\ref{eq:eigenprob}) by computing $\gr = \EUI{\bm{x}\bm{x}^T}$ and $\ci = \EUI{\boldsymbol\Sigma}$. Eq.\,(\ref{eq:cov}) enables us to simplify the exact form of $\ci$, namely $\ci = \mathbf{D}-\gr$, where $\mathbf{D} \in \mathbb{R}^{K \times K}$ is a diagonal matrix with elements $\mathbf{D}_{kk} = \EUI{x_k}$.
Alternatively, for any encoding state ensemble $\{ p (\bm{u}) \dd \bm{u}, \hat{\rho} (\bm{u}) \}$, the matrices $\mathbf{D}$ and $\gr$ can be compactly expressed as (see Appendix\,\ref{sec:DefCap})
\begin{align}
    \mathbf{D}_{k k} & = \mathrm{Tr} \{\hat{M}_k \hat{\rho}^{(1)}\}, \\
    \gr_{j k} & = \mathrm{Tr} \{(\hat{M}_j \otimes \hat{M}_{k}) \hat{\rho}^{(2)}\}
\end{align}
by defining the $t$-th order \textit{quantum ensemble moment} $\hat{\rho}^{(t)} = \int \hat{\rho} (\bm{u})^{\otimes t} p (\bm{u}) \dd \bm{u}$ in the $t$-copy space of the quantum state \cite{Harrow2009}.

From Eq.\,\eqref{eq:EC}, we have $\lim_{S\to\infty} \EC = {\rm Rank}\{\gr\}$, where ${\rm Rank}\{\gr\} = K$, the number of measured features, provided no special symmetries exist. This important result reveals that in the absence of sampling noise all quantum systems -- independent of pararameterization -- have a capacity which is simply the number of independent accessible degrees of freedom \cite{dambre_information_2012, hermans_memory_2010}. 
The generic exponential scaling of measured degrees of freedom with the size of the quantum system (e.g.\,$K=2^L$ for $L$-qubit systems subject to a computational basis measurement) is often-cited as a motivator for studying ML with quantum systems~\cite{Kalfus_2022, wright_capacity_2019, martinez-pena_information_2020}. 
However, as will be demonstrated shortly, the REC of quantum systems can be significantly reduced from this limit for finite $S$ in a way that strongly depends on the encoding. By evaluating the ability of quantum systems to accurately express functions in the presence of quantum sampling noise, the capacity analysis above provides an important metric to assess the utility of quantum platforms for learning in practice.


\subsection{Resolvable Expressive Capacity of Quantum 2-designs}
\label{sec:twodesign}

We first consider the REC of quantum 2-designs: systems with fixed $\bm{\theta}$ that map inputs to a unitrary ensemble $\{p(\UI) \dd \UI, \hat{U}(\UI;\bm{\theta})\}$ whose first and second moments agree with those from a uniform (Haar) distribution of unitaries. Quantum 2-designs are important to recent QML studies \cite{cerezo_variational_2021, Holmes2022} due to their role in defining and studying ``expressibility'' \cite{sim_expressibility_2019, wu_expressivity_2021}: a metric quantifying how close a parameterized quantum system is to such a 2-design. The capacity eigenproblem Eq.\,(\ref{eq:eigenprob}) for any quantum 2-design over $K$-dimensions can be solved analytically (see Appendix \ref{app:2designsol}), yielding a flat spectrum of NSR eigenvalues $\beta^2_k = K (1-\delta_{k0})$.  This results in an REC
\begin{align}
    C_T = K \cdot \frac{S + 1}{S + K},
\end{align}
which at finite $\NS$ can be significantly lower than $K$.  For quantum systems with $K=2^L$, all $k\neq 0$ eigentasks have a noise strength $2^L/\NS$, requiring $S$ to grow exponentially with qubit-number $L$ in order to extract useful features.

A quantum 2-design is thought of as having maximal ``expressibility'', however we see that its REC always vanishes exponentially with system size for a fixed finite $S$. 
% It is exactly such systems that have been shown to lead to barren plateaus which preclude learning \cite{Holmes2022}. \fh{We should remove this sentence if it is too confusing.} 
To emphasize the distinction with ``expressibility'', we note that REC reflects how much classical information can be extracted from  the entire ``quantum computational stack'' in practice: from an abstract algorithm, to the quantum hardware on which its implemented, and the classical electronics used for control and readout.  REC requires only noisy computational outputs $\{\bar{X}_k (\UI) \}$ and is thus efficiently-computable in experiment -- unlike more abstract metrics \cite{sim_expressibility_2019, wu_expressivity_2021, meyer_fisher_2021} -- yielding a directly relevant metric for learning with quantum hardware.

\begin{comment}
\rev{
\subsection{Application to a Broader Set of QML Schemes (to be moved to response to External Experts)}
For supervised learning, e.g.\,PQC, QNN, quantum Kernel, the input $\UI$ is simply the training/testing data. However, the encoding process described in Eq.\,(\ref{eq:qchannel}) can also be applicable within a broader range of QML schemes, as long as input data following a prior distribution is embedded into a quantum system, which is immediately followed by any quantum measurement. One particulart example is the quantum generative adversarial network (qGAN). In qGAN, one has a distribution of true data $\bm{z} \sim p_{\mathrm{true}}(\bm{z})$ which can be embedded inside the state $\hat{\sigma}(\bm{z})$, and also a prior distribution $\bm{z}' \sim p_{\mathrm{prior}}(\bm{z}')$. Then one can transform the variable sampled $\bm{z}'$ through a parameterized \textit{generator} $\hat{G}(\bm{\theta}_G)$ to yield a state $\hat{\gamma}(\bm{z}') = \hat{G}\ket{\bm{z}'} \! \bra{\bm{z}'} \hat{G}^\dagger$.}

\rev{In order to fit the formalism of Eq.\,(\ref{eq:qchannel}), we define $\bm{u} = (\bm{z}, \bm{z}', c)$ where $c \in \{0, 1\}$ is an extra random binary variable with half-half probability, namely $q(0)=q(1)=1/2$. Now the distribution $p(\bm{u}) = p_{\mathrm{true}}(\bm{z}) p_{\mathrm{prior}}(\bm{z}') q(c)$ is a product distribution. If $c=0$, the encoding state is the true state $\hat{\rho}(\UI) = \hat{\sigma}(\bm{z})$; while if $c=1$, the encoding state is the generated fake state $\hat{\rho}(\UI) = \hat{\gamma}(\bm{z}')$. The target of qGAN is to optimize the generator $\hat{G}(\bm{\theta}_G)$ so that given a $\hat{\rho}(\bm{u})$, it is hard to distinguish whether it is from $\hat{\sigma}(\bm{z})$ or from $\hat{\gamma}(\bm{z}')$. In classical GAN, the prior $\bm{z}'$ is necessary for generating a mimic distribution, but it can be null in qGAN. To formally quantify hardness of distinguishability, the encoded state is fed into a binary classifier \textit{discriminator} $\hat{D}(\bm{\theta}_D)$ which is parameterized by $\bm{\theta}_D$. Finally, one more POVM $\{\hat{M}_k\}$ is conducted on $\hat{D} \hat{\rho}(\bm{u}) \hat{D}^\dagger$ to produce a discriminant label $c'$ via some function, say a sigmoid function $c'=\sigma(\bm{W}^T \bar{\bm{X}}(\bm{u}))$. The training of qGAN can be expressed as a game
\begin{align}
    \min_{\bm{\theta}_G} \max_{\bm{\theta}_D, \bm{W}} \mathbf{E}_{\bm{u}} [\mathcal{L}(\bm{u})],
\end{align}
where $\mathcal{L}(\bm{u})$ is a loss function that captures the success probability of the discriminator correctly classifying the label $c'=c$. In this work, we are mostly interested in the optimization of $\bm{w}$ which is directly related to the quantum sampling noise, when parameters $\bm{\theta}_G, \bm{\theta}_R$ are given. From the perspective of function fitting, the part of maximization $\max_{\bm{w}} \mathbf{E}_{\bm{u}} [\mathcal{L}(\bm{u})]$ is approximating the conditional probability $f(\bm{u}) = \mathrm{Pr}[c=1|\bm{u}]$ with $\sigma(\bm{W}^T \bar{\bm{X}}(\bm{u}))$ as much as possible. }
\end{comment}


\section{Experimental Results in Quantum systems} 
\label{sec:ibmq}

In this section we discuss the implementation of the Eigentask construction in experiments we carried out on a 7-qubit IBMQ superconducting quantum processor \texttt{ibmq\_perth}. 

\subsection{The Quantum Circuit Ansatz implemented in Experiments}

To demonstrate the practical utility of our framework, we now show how the spectrum $\{\beta_k^2\}$, the REC, and eigentasks can all be computed for real quantum devices in the presence of parameter fluctuations and device noise. 
{We {reiterate} at the outset that our approach for quantifying the REC of a quantum system is very general, and can be applied to a variety of quantum system models.} 
For practical reasons, we perform experiments on $L$-qubit IBM Quantum (IBMQ) processors, whose dynamics is described by a parameterized quantum circuit containing single and two-qubit gates. However, as an example of the broad applicability of our approach, in Appendix \ref{app:H-ansztz} we compute the REC for $L$-qubit quantum annealers via numerical simulations, governed by the markedly different model of continuous-time Hamiltonian dynamics. 

On IBMQ devices, each input $u$ will generate a quantum circuit, hence the maximal number of distinct circuits places a resource constraint on input size. Specially, our experiment and computation is limited to $N=300$ 1D inputs $u$ that are also uniformly distributed, $p(u)=\mathrm{Unif}[-1,1]$, see Fig.\,\ref{fig:Genc1}(a). A 1D distribution then ensures features $\{\bar{X}_k (\UI) \}$ are sufficiently densely sampled to approach the continuum limit, and are also easy to visualize, as in the classical optical RC in Sec.\,\ref{sec:photonicRC}. We emphasize that this analysis can be straightforwardly extended to multi-dimensional and arbitrarily-distributed inputs given suitable hardware resources, without modifying the form of the Gram and covariance matrices.

We are only now required to specify the model of the quantum system, and choose an ansatz  tailored to be natively implementable on IBMQ processors (see Appendix \ref{sec:NISQRC_Architecture_Detail}). 
We fix $\hat{\rho}_0 = \ketbra{0}{0}^{\otimes L}$; note, however, that any other initial state may be implemented via an additional unitary and absorbed into the ``encoding'', i.e.\,the quantum channel $\mathcal{U}(u;\bm{\theta})$ of Eq.\,\eqref{eq:qchannel}. In this way, the dependence of REC on initial states  could be explored in future studies.

The circuit we choose consists of $\tau \in \mathbb{N}$ repetitions of the same input-dependent circuit block depicted in Fig.\,\ref{fig:Genc1}(a). The block itself is of the form $\mathcal{R}_{x}(\bm{\theta}^x/2) \mathcal{W}(J) \mathcal{R}_{z}(\bm{\theta}^z +\bm{\theta}^I u ) \mathcal{R}_{x}(\bm{\theta}^x/2) $, where $ \mathcal{R}_{x/z}$ are Pauli-rotations applied qubit-wise, e.g.\,$ \mathcal{R}_{z} = \bigotimes_{l} \hat{R}_{z}({\theta}^z_l +{\theta}^I _l u) $. A two-qubit coupling gate acts between physically connected qubits in the device and can be written as $\mathcal{W}(J) = \prod_{\langle l, l' \rangle} \mathcal{W}_{l, l'}(J) = \prod_{\langle l, l' \rangle} \mathrm{exp}\{- i \frac{J}{2} \hat{\sigma}^z_{l} \hat{\sigma}^z_{l'}\} $.
Within the structure of this ansatz, we will choose all single-qubit rotation parameters randomly: $\theta^{x/z}_l \sim \mathrm{Unif}[0,2\pi]$ and  $\theta^{I}_l\sim \mathrm{Unif}[0,10\pi]$, generally representing a circuit trained for a particular unspecified task. Each instance of random parameters, along with associated dissipative processes, specifies the quantum channel  $\mathcal{U}(u;\bm{\theta})$ which we refer to as an ``encoding''.  We will study the performance of an overall ansatz by looking at the behavior averaged across encodings as hyperparameters such as $J$ are varied.  In this work we also choose $\tau=3$, which limits circuit depth and associated prevalence of gate errors, while still generating a complex state with correlation generally distributed throughout all qubits.

% \ga{Felt I needed to add a note in this section about why choosing random parameters and that instance of a circuit with random parameters is called an encoding, might need some work.  Also should have parameters in text since they apply to all figures, rather than just fig 2}

Finally, we consider feature extraction via a computational basis measurement as is standard in quantum information processing: the POVM elements are the $K=2^L$ projectors $\hat{M}_k = \ketbra{\boldsymbol{b}_k }$, where $\boldsymbol{b}_k$ is the $L$-bit binary representation of the integer $k$. However, as with state preparation, measurements in any other basis can be (and in practice, are) realized using an additional unitary prior to computational basis readout, whose effect can similarly be analyzed as part of the general encoding $\mathcal{U}(u;\bm{\theta})$.



% For the specific case of an $L$-qubit system which is projectively measured -- relevant to most current forms of quantum information processing -- the POVM elements are simply projectors onto $K=2^L$ computational basis states $\hat{M}_k = \ketbra{\boldsymbol{b}_k }$, where $\boldsymbol{b}_k$ is the $L$-bit binary representation of the integer $k$. \fh{I suggest move to Sec 2, since $K=2^L$ is mentioned many times there. }

Note that for this ansatz, the choice $J=0~(\mbox{mod } \pi)$ yields either $\mathcal{W}_{l, l'}(J) = \hat{I}$ or $\hat{\sigma}^z \otimes \hat{\sigma}^z$, both of which ensure $\hat{\rho}(u)$ is a product state and measured features are simply products of uncorrelated individual qubit observables -- equivalent to a noisy classical system. Starting from this \textit{product system} (PS), tuning the coupling $J\neq 0~(\mbox{mod } \pi)$ provides a controllable parameter to realize a \textit{quantum correlated system} (CS), for which the $2^L$-dimensional multinomial distribution $\bm{x}(u)$ cannot be represented as a tensor product of $L$ marginal binomial distributions on each qubit. In general, such non-product systems are intuitively expected to result in $u$-dependent quantum states which exhibit entanglement and can potentially be more difficult to describe classically.
% contain quantum states which can be more difficult to describe classically}
% \ga{I think we need to connect non-zero correlations to  non-classicality. Also note 'equivalent', as opposed to any classical probabilistic system.  again we need to motivate why we control J, and fundamentally its to make states that are non-classical}
This control enables us to address a natural question regarding REC of quantum systems under finite $\NS$: what is the dependence of REC and realizable eigentasks on $J$, and hence on quantum correlations? 
% \ga{I think we could try to say a bit more to connect correlations to quantumness ie more general expression of entanglement}


\subsection{Resolvable Expressive Capacity of Quantum Circuits}
\label{subsec:RECqc}

% This calculation of REC requires extracting measured features from the quantum circuit under input $u$, one example of which is shown for the IBMQ \texttt{ibmq\_perth} device in Fig.\,\ref{fig:Genc1}(a), for $\NS=2^{14}$.
% \sout{This calculation of REC requires} 
To perform the capacity analysis, one must extract measured features from the quantum system as the input $u$ is varied, as exemplified in Fig.\,\ref{fig:Genc1}(a) for the IBMQ \texttt{ibmq\_perth} device.  For comparison, we also show ideal-device simulations (unitary evolution, no device noise), where slight deviations are observed. The agreement with experimental results is improved when the effects of gate errors, readout errors, and qubit relaxation are included, hereafter referred to as ``device noise'' simulations, highlighting both the non-negligible role of device nonidealities, and that our analysis incorporates them.

The measured features under finite $\NS$ are used to estimate the Gram and covariance matrices, and to therefore solve the eigenproblem Eq.\,(\ref{eq:eigenprob}) for NSR eigenvalues $\{\bar{\beta}_k^2\}$ and eigenvectors $\{\bar{\bm{r}}^{(k)}\}$, as estimators of $\{\beta_k^2\}$ and $\{\bm{r}^{(k)}\}$ (see Eqs.\,(\ref{eq:barbeta2k}-\ref{eq:barrk}) in Appendix \ref{app:tilde_correction} for detailed techniques). Typical NSR spectra computed for a random encoding (i.e.~set of rotation parameters) on the device are shown in Fig.\,\ref{fig:Genc1}(b), for $J=0$ (PS) and $J=\pi/2$ (CS), together with corresponding spectra from device noise simulations, with which they agree well. We note that at lower $k$, the device NSR eigenvalues are larger than those from ideal simulations, and at larger $k$ deviate from the direct exponential increase (with order) seen in ideal simulations. Both these effects are captured by device noise simulations as well and can therefore be attributed to device errors and dissipation. The NSR spectra therefore can serve as an effective diagnostic tool for quantum processors and encoding schemes. 
% \sout{More examples will be provided later in the discussion.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Figure environment removed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The NSR spectra can be used to directly compute the REC of the corresponding quantum device for finite $\NS$, via Eq.\,(\ref{eq:EC}). Practically, at a given $\NS$ only NSR eigenvalues $\bar{\beta}_k^2 \lesssim \NS$ contribute substantially to the REC. An NSR spectrum with a flatter slope therefore has more NSR eigenvalues below $\NS$, which gives rise to a higher capacity. Fig.\,\ref{fig:Genc1}(b) shows that the CS generally exhibits an NSR spectrum with a flatter slope than the PS, yielding a larger capacity for function approximation across all sampled $\NS$.

To more precisely quantify the role of quantum correlations in REC, we introduce the \textit{expected total correlation} (ETC) of the measured state over the input domain of $u$ \cite{Vedral2002, Modi2010},
\begin{align}
   \bar{\mathcal{T}} = \Eu{ \sum_{l = 1}^L \mathrm{S} ( \hat{\rho}_l^{M} (u) ) - \mathrm{S} (\hat{\rho}^{M} (u) ) }, 
\end{align}
where $\hat{\rho}^{M} (u) \equiv \sum_{k} \hat{\rho}_{kk}(u) \ket{\bm{b}_k}\!\bra{\bm{b}_k}$ is the post-measured state, $\mathrm{S}(\cdot)$ is the von Neumann entropy~(see Appendix \ref{app:QCM}),
and $\hat{\rho}_l = \mathrm{Tr}_{[L] \backslash \{ l \}} \{ \hat{\rho} \}$ is the reduced density matrix obtained by tracing over all qubits except qubit $l$. Therefore, non-zero ETC indicates the generation of quantum states over the input domain $u$ that on average have nontrivial correlations amongst their constituents, including for example pure many-body states that are entangled.

We now compute REC and ETC using $S=2^{14}$ in Fig.\,\ref{fig:Genc1}(c) as a function of $J$, for the same random encoding considered above on the device. We note that the experimental results show excellent agreement in both cases with the corresponding device noise simulation. We also show average REC at $S=2^{14}$ and ETC across 8 random encodings in both ideal and device noise simulations. We find that the influence of individual encodings, i.e.\,random rotation parameters, leads only to small deviations from the overall REC trend when global hyperparameters are held fixed. This implies that no crucial features of the REC are missed by us foregoing fine-tuning (e.g.\,via gradient descent) of individual rotation parameters \textit{in lieu} of sampling them from a given uniform probability distribution.

We note that product states by definition have $\bar{\mathcal{T}}=0$~\cite{nielsen2002quantum}; this is seen in ideal simulations for $J=0~(\mbox{mod}~ \pi)$. However, the actual device retains a small amount of correlation at this operating point, which is reproduced by device noise simulations. This can be attributed to gate or measurement errors as well as cross-talk, the latter being especially relevant for the transmon-based IBMQ platform with a parasitic always-on ZZ coupling \cite{sheldon_procedure_2016}. 
With increasing $J$, $\bar{\mathcal{T}}$ increases and peaks around $J \approx \pi/2~(\mbox{mod } \pi)$; interestingly, $\EC$ also peaks for the same coupling range. From the analogous plot of REC, we clearly see that at finite $S$, increased ETC appears directly correlated with higher REC. We have observed very similar behaviour using completely different quantum system models (see Appendix Fig.\,\ref{fig:app_Features_and_Capacity}~\cite{Giovannetti2006, martinez2021dynamical}). This indicates the utility of enhancing quantum correlations as a means of improving the general expressive capability of quantum systems.


We raise two notes of caution here. First, our analysis across different quantum system implementations has often (though not always) found that a certain threshold number of shots $\NS$ is required before the finite-$\NS$ capacity of a CS overtakes that of the corresponding PS (See Appendix~\ref{app:H-ansztz}). This higher resolvability of functions using a PS under restricted shots may be due to the comparative ease of estimating probabilities from an effectively product distribution, and merits further exploration. At a sufficiently large $\NS$, the increased complexity of the $u$-dependence imposed by the input-output map of a CS results in an REC that eventually surpasses that of the PS.

Secondly, we caution that the connection between measurement correlations and REC is an observed trend, rather than a law derived from first principles.  One can come up with contrived situations where increasing correlation has no effect on REC: for example, appending a layer of CNOT gates directly prior to measurement will generally increase the ideal ETC of any ansatz. For measured features however this amounts to a simple shuffling of labels $x_k(\UI)\leftrightarrow x_{k'}(\UI)$, thus yielding the same NSR spectrum and REC. The input, quantum-state, and feature mapping ultimately governs REC: only increases in correlation that also increase the complexity of the measured features' $u$-dependence (as achieved via the intermediate $\mathcal{W}$ gates here) are beneficial from the perspective of information processing.

% We also emphasize here the above observation is valid on some specific natural system like quantum annealing ansatz (Appendix \ref{app:H-ansztz}) or the circuit ansatz here. For example, one may consider a circuit with an array of single qubit rotation gates. The correlation among the qubits in the post-measured state will definitely increase if we add some CNOT gates afterwards; however, such additional CNOT gates merely reshuffle the feature functions $\{x_k(\UI)\}$ thus still give the same eigen-NSRs and REC values.

As a final important point, note that at finite $\NS$, even with increased quantum correlations, the maximum REC is still substantially lower than the upper bound of $K=64$. This remains true even for ideal simulations, and over several random encodings, so the underperformance cannot be attributed to device noise or poor ansatz choice respectively.
It is worth emphasizing that the impact of device noise is captured in the small REC gap between the ideal and noisy simulation curves, with the remainder of the reduction from $K=64$ attributable to quantum sampling noise alone.
These results clearly indicate that the resulting sampling noise at finite $\NS$ is the fundamental limitation for QML applications on this particular IBM device, rather than other types of noise sources and errors. 

% \section{A Robust Approach to Learning}
\subsection{A Robust Approach to Learning}
\label{sec:eigentasklearning}

While we have demonstrated the REC as an efficiently-computable metric of general expressive capability of a noisy quantum system, some important practical questions arise. First, does the general REC metric have implications for practical performance on \textit{specific} ML tasks? Secondly, given the limiting -- and unavoidable -- nature of correlated sampling noise, does the REC provide any insights on optimal learning using a particular noisy quantum system and the associated encoding?

Our formulation addresses both these important questions naturally, as we now discuss. {Recall that} beyond being a simple figure of merit, the REC is precisely the sum of capacities to approximate a particular set of orthogonal functions native to the given noisy quantum system: the eigentasks. {Furthermore}, these eigentasks 
$\bar{y}^{(k)}(u)$ can be directly estimated from a noisy quantum system via the generalized eigenvectors $\{\bar{\bm{r}}^{(k)}\}$, and are ordered by their associated NSR eigenvalues $\{\bar{\beta}_k^2\}$. In Fig.\,\ref{fig:Genc2}(a) show a selection of estimated eigentasks from the device for the CS $(J=\pi/2)$ and PS $(J=0)$ encodings of Fig.\,\ref{fig:Genc1}(b). For both systems, the increase in noise with eigentask order is apparent when comparing two sampling values, $\NS=2^{10}$ and $\NS=2^{14}$. Furthermore, for any order $k$, eigentasks for the PS are visibly noisier than the CS; this is consistent with NSR eigenvalues for PS being larger than those for CS (Fig.\,\ref{fig:Genc1}(b)). The higher resolvable expressive capacity of the CS can be interpreted the ability to accurately resolve more eigentasks at fixed $S$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Figure environment removed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The resolvable eigentasks of a finitely-sampled quantum system are intimately related to its performance at specific QML applications. To demonstrate this result, we consider a concrete application: a binary classification task that is not linearly-separable. The domain $u \in [-1,1]$ over which REC was evaluated is separated into two classes, as depicted in Fig.\,\ref{fig:Genc2}(b). A selection of $N_{\rm train}=150$ total samples -- with equal numbers from each class -- are input to the IBMQ device, and 
eigentasks $\{\bar{y}^{(k)}(u^{(n)})\}$ are estimated using $S=2^{14}$ shots. A linear estimator applied to this set of eigentasks is then trained using logistic regression to learn the class label associated with each input. Finally, the trained IBMQ device is used to predict class labels of $N_{\rm test}=150$ distinct input samples for testing. Note that we use the \textit{random} circuits of the previous section to draw more direct comparisons between REC and task performance.  By training only external weights instead of internal parameters $\boldsymbol{\theta}$ we are employing the framework of quantum ELM \cite{mujal_opportunities_2021, wright_capacity_2019, wilson_quantum_2019, innocenti_potential_2022}, which allows one to avoid the computational overhead and difficulty associated with training quantum systems while still achieving comparable performance.
% \cite{chen_temporal_2020, Kalfus_2022, wright_capacity_2019, garcia-beni_scalable_2022}.

This task can equivalently be cast as one of learning the likelihood function that discriminates the two input distributions, shown in Fig.\,\ref{fig:Genc2}(c), with minimum error. The set of up to $K_{\rm L}$ eigentasks $\{\bar{y}^{(k)}(u)\}_{k \in [K_{\rm L}]}$, where $K_{\rm L} \leq K$, serves as the native orthonormal basis of readout features used to approximate \textit{any} target function using the quantum system. Importantly, the basis is \textit{ordered}, with eigentasks at higher $k$ contributing more noise, as dictated by the NSR eigenvalues $\bar{\beta}_{k}^2$. In particular, at any level of sampling $\NS$, there exists an eigentask order $K_c(\NS)$ after which the NSR eigenvalues $\bar{\beta}_k^2/\NS$ first drops below unity: $K_{c}(S) \equiv \max_k\{\bar{\beta}_{k}^2 < \NS\}$. Heuristically, including eigentasks $k > K_c(\NS)$ should contribute more `noise' to the function approximation task than `signal'. In Fig.\,\ref{fig:Genc2}(c), we plot the learned estimates of the likelihood function using $K_{\rm L} = K_c(\NS)$ eigentasks for both the CS and PS. First, we note that $K_c$ is lower for the PS than the CS; the former has fewer resolvable eigentasks at a given $\NS$. This limitation on resolvable features limits function approximation capacity: the learned estimate of the likelihood function using $K_c$ eigentasks is visibly worse for the PS than the CS. 

% \sout{The noisier eigentasks of the PS therefore limit the accuracy with which it can be used to learn the target, in comparison to the ES. This is clear from the learned estimates shown in Fig.\,\ref{fig:Genc2}(c), using an equal number of $K_{\rm L} = 36$ eigentasks to ensure a fair comparison.}

% Importantly, the higher approximation capacity translates to improved classification performance. In Fig.\,\ref{fig:Genc3}(a) we show the training and testing classification accuracy for both CS and PS, as a function of the number of eigentasks $K_{\rm L}$ used for learning. We find that the maximum testing accuracy using the CS exceeds that of the PS. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Figure environment removed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this way, higher REC allows noisy quantum systems to better approximate more functions, which translates to improved learning performance -- this result is explored systemically in Fig.\,\ref{fig:Genc3}(b). Of course, it is natural to ask whether using $K_c(\NS)\leq K$ eigentasks is optimal: exactly this question is investigated in Fig.\,\ref{fig:Genc3}(a), where we plot the training and test accuracy of both device encodings as a function of the number of measured eigentasks $K_{\rm L}$. The performance on the specific training and test set shown in  Fig.\,\ref{fig:Genc2}(b) is indicated with markers, and solid lines indicate the average performance over $10$ distinct divisions of the data into training and test sets. This permutation of the learning task is a standard technique to optimize hyperparameters in ML, and is done here to eliminate the sensitivity of these results to the choice of training set. First note that in all cases, using all eigentasks ($K_{\rm L} = K$) -- or equivalently all measured features $\{\bar{\bm{X}}\}$ -- leads to far lower test accuracy than is found in training.  The observed deviation is a distinct signature of overfitting: the optimized estimator learns noise in the training set (comprised of noisy eigentask estimates $\bar{y}^{(k)}(u^{(n)})$), and thus loses generalizability {to unseen samples} in testing.  

Improvements in model training performance with added features are only meaningful insofar as they also lead to better performance on new data: in both encodings we see test set classification accuracy peaks near $K_c(S)$. This is particularly clear for the averaged results, but even for individual datasets the test accuracy at $K_c(S)$ is within $\approx\!2\%$ of its maximum, thus confirming our heuristic reasoning that eigentasks beyond this order, with an NSR eigenvalues $<\!\!1$, hinder learning. The eigentask-learning approach naturally allows one to decompose the outputs from quantum measurements into a compressed basis with known noise properties, and then select the set of these which exactly captures the resolvable information at a given $S$.  This robust approach to learning enabled by the capacity analysis maximizes the ability of a noisy quantum system to approximate functions without overfitting to noise, in this case fundamental quantum sampling noise.


% Furthermore, we now see a concrete example of how eigentasks ordered by NSR provide a path towards optimal learning using noisy quantum systems. First, note that using all eigentasks ($K_{\rm L} = K$) yields a test accuracy far lower than that found in training. The observed deviation is a distinct signature of overfitting: the optimized estimator learns noise in the training set, and thus loses generalizability to unseen samples in testing. The discrepancy between training and testing starts to emerge roughly around $K_{\rm L} = K_c(\NS)$, the eigentask order beyond which the NSR at finite $\NS$ drops below unity: this confirms our heuristic reasoning that eigentasks beyond this order contribute mostly noise to the learning task. Excluding these eigentasks of a quantum system therefore optimizes performance by maximizing learning from the training set without overfitting to noise, in this case fundamental quantum sampling noise.


Finally, Fig.\,\ref{fig:Genc3}(b) shows the classification accuracy for this device encoding as $J$ is varied, where following the above approach, the optimal $K_c(S)$ set of eigentasks are used for each encoding.  We also show the performance of a similar-scale ($K_{\rm L}=64$ node) software neural network and ideal simulations in the $S\to\infty$ limit ($K_c(\infty)=64$) for comparison.  Note that only these infinite-shot results approach the classical neural network, with quantum sampling noise imposing a significant performance penalty even for $J \approx \pi/2~(\mbox{mod } \pi)$. We highlight the striking similarity with Fig.\,\ref{fig:Genc1}(c): encodings with larger quantum correlations and thus higher resolvable expressive capacity will perform generically better on learning tasks in the presence of noise, because they generate a larger set of eigentasks that can be resolved at a given sampling $\NS$. Resolvable Expressive Capacity is \textit{a priori} unaware of the specific problem considered here; this example thus emphasizes its power as a general metric predictive of performance on arbitrary tasks.

\section{Discussion}
\label{sec:discussion}

We have developed a straightforward approach to quantify the resolvable expressive capacity of any physical system in the presence of fundamental sampling noise. Crucially, this analysis extends to physical quantum systems where sampling noise is fundamentally imposed by quantum measurement theory. Our analysis is built upon an underlying framework that determines the native function set that can be most robustly realized by a finitely-sampled physical system: its eigentasks. We use this framework to introduce a methodology for optimal learning that we demonstrate using noisy quantum systems, which centers around identifying the minimal number of eigentasks required for a given learning task. The resulting learning methodology is resource-efficient, and the empirical evidence we provide indicates that it is also robust to overfitting. We demonstrate that eigentasks can be efficiently estimated from experiments on real devices using a limited number of training points and finite shots. We also demonstrate across two distinct qubit-based ans\"atze that the presence of measured quantum correlations enhances resolvable expressive capacity. 
% We note that final review of the present manuscript, work was posted to the arXiv~\cite{Polloreno2022} exploring limits to noisy reservoir computers using an approach closely aligned with our methods here


We believe our work opens up several avenues of exploration in the field of learning with physical \textit{quantum} systems in particular. Firstly, our approach provides the tools to understand the limitations of sampling noise in noisy reservoir computing schemes (e.g.\,quantum reservoir computing~\cite{dambre_information_2012, fujii_harnessing_2017, chen_temporal_2020, wright_capacity_2019, garcia-beni_scalable_2022, Kalfus_2022}). In fact, during the final review of the present manuscript, work was posted to the arXiv~\cite{Polloreno2022} exploring limits to noisy reservoir computers using an approach closely aligned with our methods here. Secondly, our work has direct application to the design of circuits for learning with qubit-based systems. In particular, we propose the optimization of resolvable expressive capacity as a meaningful goal for the design of quantum circuits with finite measurement resources. This importantly includes the utilization of the eigentask formulation and eigentask learning as a useful tool for understanding the performance of physical quantum systems in practical learning tasks. Finally, the practical demonstration of our scheme under restrictions of finite input and output samples means that it can prove useful for studies on generalization and training. For example, any difference in REC and eigentasks computed with optimal weights estimated using only a finite number of input samples - as opposed to the ideal but impractical infinite input sampling limit - would constitute a generalization error over the input domain, which one can seek to minimize for optimal learning in future work.

% \rev{Furthermore, in Sec.\,\ref{sec:defwwast}, we differentiate between two distinct categories of learned weights, denoted as $\bm{w}$ and $\mathbb{E}_\mathcal{D}[\bm{w}^{\ast}(\mathcal{D})]$. Exploring the scenario where both $N$ and $S$ are simultaneously finite emerges as a promising avenue for future research, enhancing our understanding of the learning curve (encompassing training and generalization errors) in physical system-based machine learning.}


\section*{Acknowledgement}
We express our sincere gratitude to the anonymous reviewers for their invaluable guidance, which significantly contributed to the refinement and enhancement of the final manuscript. We would like to thank Ronen Eldan, Fatih Dinç, Daniel Gauthier, Michael Hatridge, Benjamin Lienhard, Peter McMachon, Sridhar Prabhu, Shyam Shankar, Francesco Tacchino, Logan Wright, Xun Gao for stimulating discussions about the work that went into this manuscript. This research was developed with funding from the DARPA contract HR00112190072, AFOSR award FA9550-20-1-0177, and AFOSR MURI award FA9550-22-1-0203. The views, opinions, and findings expressed are solely the authors' and not the U.S. government's. 

\appendix

%% References
% \bibliography{bibtex,NISQRC}
\bibliography{bibtex}

% \addcontentsline{toc}{section}{Appendix} % Add the appendix text to the document TOC
% \part{Appendices} % Start the appendix part
% \parttoc % Insert the appendix TOC

\begin{widetext}
    \include{appendix}
\end{widetext}

\end{document}
