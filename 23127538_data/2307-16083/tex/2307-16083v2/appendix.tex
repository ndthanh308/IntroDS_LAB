% \documentclass[%
%  reprint,
% %superscriptaddress,
% %groupedaddress,
% %unsortedaddress,
% %runinaddress,
% %frontmatterverbose, 
% %preprint,
% %preprintnumbers,
% %nofootinbib,
% %nobibnotes,
% %bibnotes,
%  superscriptaddress,
%  amsmath,amssymb,
%  aps,
%  prx,
% %prb,
% %rmp,
% %prstab,
% %prstper,
% %floatfix,
%  onecolumn
% ]{revtex4-2}


% \usepackage{graphicx}% Include figure files
% \usepackage{dcolumn}% Align table columns on decimal point
% \usepackage{bm}% bold math
% \usepackage[colorlinks]{hyperref}% add hypertext capabilities
% \usepackage{physics}
% \usepackage[squaren]{SIunits}
% \usepackage{mathtools}
% \usepackage{amsthm}
% \usepackage{xcolor}
% \usepackage{mathrsfs}
% \usepackage{times}
% % \usepackage{algorithm}
% % \usepackage{algorithmicx}
% % \usepackage{algpseudocode}
% % \usepackage[normalem]{ulem}
% % \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% % \renewcommand{\algorithmicensure}{\textbf{Output:}}
% \usepackage{comment}

% \renewcommand{\tablename}{Table}
% \renewcommand{\figurename}{Fig.}
% \renewcommand{\thetable}{S\arabic{table}}
% \renewcommand{\thefigure}{S\arabic{figure}}
% \renewcommand{\thepage}{S\arabic{page}} 
% \renewcommand{\thesection}{\Roman{section}} 
% \renewcommand{\thesubsection}{\Alph{subsection}} 
% \renewcommand{\theequation}{S\arabic{equation}}
% \renewcommand*{\citenumfont}[1]{S#1}
% \renewcommand*{\bibnumfmt}[1]{(S#1).}
% \def\bibsection{\section*{Supplementary References}} 
% % \renewcommand{\theequation}{\thesection.\arabic{equation}}


% % \setlength\parindent{0pt}
% \newtheorem{theorem}{Theorem}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{proposition}[theorem]{Proposition}

% \hypersetup{
% 	colorlinks=true,
% 	linkcolor=red,
% 	filecolor=blue,      
% 	urlcolor=blue,
% 	citecolor=blue
% }
% % \input{preamble}
% \setlength{\parskip}{0.5em}

% \newcommand{\fh}[1]{{\textcolor{orange}{[FH: #1]}}}
% \newcommand{\sak}[1]{{\textcolor{brown}{#1}}}
% \newcommand{\ga}[1]{{\textcolor{violet}{[GA: #1]}}}
% \newcommand{\het}[1]{{\textcolor{blue}{[HET: #1]}}}
% \newcommand{\et}[1]{{\textcolor{cyan}{[ET: #1]}}}
% \newcommand{\rev}[1]{{\textcolor{red}{[\textbf{Comments}: #1]}}}
% \newcommand{\lb}[1]{\textcolor{teal}{[LB: #1]}}
% \newcommand{\mv}[1]{\textcolor{olive}{[MV: #1]}}

% \newcommand{\Eq}[1]{Eq.\,(\ref{#1})}
% \newcommand{\Eqr}[1]{(\ref{#1})}
% \newcommand{\Eqs}[2]{Eqs.\,(\ref{#1}-\ref{#2})}
% \newcommand{\ceil}[1]{\lceil {#1} \rceil}


% % Frequently used symbols
% \newcommand{\NS}{S}
% \newcommand{\NQ}{L}
% \newcommand{\NI}{N}
% \newcommand{\NR}{N_\mathrm{R}}
% \newcommand{\ND}{d}
% \newcommand{\UI}{\bm{u}}
% \newcommand{\EC}{C_T}
% \newcommand{\ETC}{\bar{\mathcal{T}}}
% \newcommand{\regmat}{\widetilde{\mathbf{F}}_N}
% \newcommand{\RM}{\mathbf{F}_N}

% \newcommand{\cu}{\mathbf{\Sigma}}  % covariance as func of u
% \newcommand{\ci}{\mathbf{V}}       % covariance after integration
% \newcommand{\gr}{\mathbf{G}}       % gram matrix or graph matrix
% \newcommand{\gh}{\mathbf{\Lambda}}  % generalized hilbert matrix
% \newcommand{\nsr}{\mathbf{R}}      % ratio of noise-to-signal



% \begin{document}

% \title[]{``Fundamental Limits to Learning with Finitely Sampled Qubit-Based Systems'' \\
% Supplementary Material}

% \maketitle


% \tableofcontents

% \makeatletter
% \let\toc@pre\relax
% \let\toc@post\relax
% \makeatother


\newpage

% \addcontentsline{toc}{section}{Appendix} % Add the appendix text to the document TOC
% \part{Appendix} % Start the appendix part

% \makeatletter
% \let\parttoc@pre\relax
% \let\parttoc@post\relax
% \makeatother

% \parttoc % Insert the appendix TOC



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section S1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\startcontents[appendices]
\printcontents[appendices]{l}{1}{\section*{Appendices}\setcounter{tocdepth}{2}}
% \printcontents[appendices]{l}{1}{\setcounter{tocdepth}{2}}

\makeatletter
\let\toc@pre\relax
\let\toc@post\relax
\makeatother

\newpage

\section{Table of main notations}
\label{app:table}

% See Table \ref{tab}.

\begin{table}[htb]
\begin{tabular}{lp{0.72\textwidth}}

\toprule
    \multicolumn{2}{c}
    {\textbf{Abbreviations}}  \\
    \hline
    % NISQ & Noisy Intermediate Scale Quantum \\
    \hspace{8mm} REC & Resolvable Expressive Capacity, $C_T$ \\
    \hspace{8mm} (Q)ML & (Quantum) Machine Learning \\
    \hspace{8mm} PNN & Physical Neural Network \\
    % \hspace{8mm} IPC & Information Processing Capacity\\
    % VQC & Variational Quantum Circuits \\
    \hspace{8mm} POVM & Positive Operator-Valued Measure \\
    \hspace{8mm} ELM & Extreme Learning Machine \\
    \hspace{8mm} RC  & Reservoir Computing \\
    \hspace{8mm} SLM & Spatial Light Modulator \\
    % \hspace{8mm} QSN & Quantum Sampling Noise \\
    \hspace{8mm} NSR & Noise-to-Signal Ratio \\
    \hspace{8mm} PS  & Product System \\
    \hspace{8mm} CS  & Correlated System \\
    \hspace{8mm} ETC & Expected Total Correlation, $\bar{\mathcal{T}}$ \\
    \hline
    % \multicolumn{2}{>{\hsize=\dimexpr2\hsize+2\tabcolsep+\arrayrulewidth\relax}X}
    \multicolumn{2}{c}
    {\textbf{Symbols and Notation}} \\
    \hline    
    \hspace{8mm} $S$ & Number of shots \\
    \hspace{8mm} $N$ & Number of inputs; for each input we obtain $S$ output samples or shots \\
    \hspace{8mm} $L$ & Number of qubits \\
    \hspace{8mm} $K$ & Number of measured features; $K= 2^L$ for computational-basis projective measurement \\
    \hspace{8mm} $\UI$ & Input \\
    \hspace{8mm} $p$ & Input distribution \\
    \hspace{8mm} $X_k^{(s)}$ & Single-shot random-valued features in any physical system \\
    \hspace{8mm} $\Xs(\UI)$ & Collection of $k$ random-valued features for $S$ shots, $\equiv \{(X_{0}^{(s)}(\UI), X_{1}^{(s)}(\UI), \cdots, X_{K-1}^{(s)}(\UI))\}_{s \in [S]}$ \\
    \hspace{8mm} $\mathcal{D}$ & Complete dataset, $\equiv \{(\bm{u}^{(n)}, \mathcal{X}(\bm{u}^{(n)}))\}_{n \in [N]}$ \\
    \hspace{8mm} $\mathbb{E}_{\Xs}$ & Expectation over the output samples, conditioned on some fixed $\UI$\\
    \hspace{8mm} $\mathbb{E}_{\UI}$ & Expectation over the input, with underlying prior distribution $p(\UI)$, $\EUI{f} \equiv \int \dd \UI \, p(\UI) f(\UI)$\\
    \hspace{8mm} $\bar{X}_k$ & Empirical observed features, $(1/S) \sum_s X_k^{(s)}$ \\
    \hspace{8mm} $x_k$ & Expected features, $\Es{\bar{X}_k}$ \\
    \hspace{8mm} $\zeta_{k}$ & Noise component of $\bar{X}_{k}$ \\
    \hspace{8mm} $\bm{W}$ & General output weights \\
    \hspace{8mm} $\bm{w}$ & Learned optimal output weights for finite-$S$ features $\{\bar{X}_k\}$ \\
    \hspace{8mm} $\mathscr{L}$ & Loss function \\
    \hspace{8mm} $\gr$ & Gram matrix of expected features $\{x_k\}$ \\
    \hspace{8mm} $\ci$ & Expected covariance matrix of random variables $X^{(s)}_k(\UI)$ over input distribution \\
    \hspace{8mm} $\mathbf{D}$ & Expected second-order moment matrix of random variable $X^{(s)}_k(\UI)$ over input distribution, it is diagonal if $X^{(s)}_k(\UI)$ obeys multinomial distribution \\
    \hspace{8mm} $y^{(k)}$ & Eigentasks, $\sum_{k'} r_{k'}^{(k)} x_{k'}$ \\
    \hspace{8mm} $\beta_k^2$ & NSR eigenvalue associated with eigentask $y^{(k)}$ \\
    \hspace{8mm} $\bm{r}^{(k)}$ & Linear combination coefficients of expected features $\{x_{k'}\}$ forming  $y^{(k)}$ \\
    \hspace{8mm} $\bar{\beta}_k^2$ & Finite-$S$ estimate of $\beta_k^2$ \\
    \hspace{8mm} $\bar{\bm{r}}^{(k)}$ & Finite-$S$ estimate of $\bm{r}^{(k)}$ \\
    \hspace{8mm} $\bar{y}^{(k)}$ & Finite-$S$ estimate of eigentasks, $\sum_{k'} r_{k'}^{(k)} \bar{X}_{k'}$ \\
    \hspace{8mm} $\bm{\theta}$ & Quantum system parameters \\
    \hspace{8mm} $\hat{\rho}$ & Generated quantum state \\
    \hspace{8mm} $\mathcal{U}$ & Quantum channel \\
    \hspace{8mm} $\hat{M}_k$ & POVM elements, $\equiv \ketbra{\boldsymbol{b}_k }$ for computational-basis projective measurement\\
    \hspace{8mm} ${\bm b}_k$ & Computational basis eigenstate labels\\
    \hspace{8mm} $k^{(s)}$ & Measurement outcome for shot $s$ \\
    % $\xi^{(k)}$ & $\equiv \sum_{k'} r_{k'}^{(k)} \zeta_{k'}$, noise part in $\bar{y}^{(k)}$ \\
    % $\hat{O}_k$ & $\equiv \sum_{k'} r_{k'}^{(k)} \ketbra{\boldsymbol{b}_{k'}}$, optimal measurement basis \\
    \hspace{8mm} $\hat{\rho}^{M}$ & Diagonal post-measurement state, $\sum_{k}\hat{\rho}_{kk}(\UI) \ket{\bm{b}_k}\!\bra{\bm{b}_k}$\\
    \hspace{8mm} $K_c(\NS)$ \hspace{8mm} & Cutoff index where $\beta_k^2$ approaches $\NS$, $\max_k\{\beta_{k}^2 < \NS\}$\\
    % $\widetilde{(\cdot)}_N$ & Quantity obtained from finite $N$ sampling data \\
    % $\widetilde{(\cdot)}$ & Large $N$ limit, that is $\lim_{N \to \infty} \widetilde{(\cdot)}_N$ \\
    \hline
    \label{tab}
\end{tabular}
\caption{Table of abbreviations and symbols used in main text and appendices}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section S2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{Features, output weights and noise covariances}
\section{Feature maps generated by quantum systems}
\label{sec:NISQRC_Architecture_Detail}

% \subsection{Details of underlying systems}
% \subsection{Details of input encodings into quantum systems}
\label{DetailsEncodings}

In the main text, we introduce the idea of encoding inputs into the state of a quantum system via a parameterized quantum channel, reproduced below:
\begin{align}
    \hat{\rho}(\bm{u};\bm{\theta}) = \mathcal{U}(\bm{u};\bm{\theta})\hat{\rho}_0,
    \label{appeq:quantumChannel}
\end{align}
one then measures this state to approximate desired functions of the input. %Our analysis of REC presented in this work does not depend on the precise details of the quantum channel $\mathcal{U}$. 
Fig.\,\ref{fig:Schematic_4Inputs} gives a simple example of this mapping from classical inputs $\UI$ (here in a 2D compact domain) to a quantum state generated by a $\bm{u}$-dependent encoding, and finally to the measured features in a $2$-qubit system undergoing commuting local measurements in the computational basis. The measurement outcomes are therefore bitstrings, of which there are $K= 2^L = 4$, namely: $\bm{b}_k \in \{00,01,10,11\}$. A given shot will yield one of these possible bitstrings.

On the right we plot samples of $S$-shot features $\{\bar{X}_k\}$ constructed for different numbers of shots $S=100, 1000, 10000$ (here we enumerate the feature $k$ with the associated bitstring $\bm{b}_k$). The feature-space for a 2-qubit system is four-dimensional. Owing to the normalization condition $\sum_k \bar{X}_{k} = 1$, only three of these dimensions are independent. For ease of visualization we only plot a two-dimensional projection in the  $\bar{X}_{00} - \bar{X}_{11}$ plane. Each dot in this plot is an average (cf. \Eq{eq:Xsum}) over the associated $S$ shots holding the input $\UI = (u_1, u_2)$ identical over those experiments.


As expressed in Eq.\,\eqref{eq:xbar}, the structure of the noise and thus the correlations in the distribution is determined by the associated quantum state, subject to an overall scaling with $S$. It is important to notice here that as $S\to \infty$ this distribution collapses to a single deterministic point, the corresponding quantum probability $\bm{x}(\UI)$.  It is also evident from this plot that the shape and orientation of these clusters depends on the underlying quantum state $\hat{\rho}(\bm{u};\bm{\theta})$ and associated probabilities $\bm{x}(\UI)$ via Eq.\,\eqref{eq:cov}.  In the remainder of this section, we will consider more complex quantum models, such that they generate mappings which can be useful for learning. A descriptive pseudo-algorithm for learning scheme based circuit-ansatz can be found in Algorithm\,\ref{alg:arc_prob}.
% its like starting a sentance with an acronym :)
% In Fig.\,\ref{fig:Schematic_4Inputs}, we provide a toy example in $2$-qubits system, for different shots $S$. The quantum sampling noise nature forces the $\bar{\bm{X}}_k$ be noisy for finite $S$. The smaller the shots $S$ is, the more strongly the cluster disperses; and as $S \to \infty$, the noisy features $\bar{\bm{X_k}}(\UI)$ should be able to converge to the corresponding quantum probability $\bm{x}(\UI)$. We also emphasize here that the shape and orientation of these clusters, under quantum sampling noise, also highly depends on the $S$-infinity feature values $\bm{x}$ itself, which is different from typical classical noise like white or pink noise.}
% For practical calculations, however, we have to consider more concrete and complicated models, about which we provide more details in this section.

To describe these models, we begin by first limiting to 1-D inputs $u$ as analyzed in the main text; generalizations to multi-dimensional inputs $\bm{u}$ are straightforward. Then, we write Eq.\,(\ref{appeq:quantumChannel}) in the form
\begin{align}
    \hat{\rho}(u;\bm{\theta}) = \hat{U}(u; \boldsymbol{\theta}) \hat{\rho}_0 \hat{U}^{\dagger}(u; \boldsymbol{\theta}).
\end{align}
In the main text, we have considered a model for dynamics of an $L$-qubit quantum system that is natively implementable on modern quantum computing platforms: namely an ansatz of quantum circuits with single and two-qubit gates. We refer to this encoding as the \textit{circuit ansatz} (or \textit{C-ansatz} for short) for which the operator $\hat{U}(u; \boldsymbol{\theta})$ takes the precise form
\begin{align}
    \hat{U}(u; \boldsymbol{\theta}) = \left[ \mathcal{R}_{x} \!\left(\frac{\bm{\theta}^x}{2}\right) \mathcal{W}(J) \mathcal{R}_{z} \!\left(\bm{\theta}^z +\bm{\theta}^I u \right) \mathcal{R}_{x} \!\left(\frac{\bm{\theta}^x}{2} \right)
    \right]^{\tau}~~~~~~~\textit{(C-ansatz)}
    \label{appeq:cansatz}
\end{align}
% Figure environment removed

For completeness, we recall that $ \mathcal{R}_{x/z}$ are Pauli-rotations applied qubit-wise, e.g.\,$ \mathcal{R}_{z} = \bigotimes_{l} \hat{R}_{z}({\theta}^z_l +{\theta}^I _l u) $, while the coupling gate acts between physically connected qubits in the device and can be written as $\mathcal{W}(J) = \prod_{\langle l, l' \rangle} \mathrm{exp}\{- i \frac{J}{2} \hat{\sigma}^z_{l} \hat{\sigma}^z_{l'}\} $. We emphasize here again that $\tau \in \mathbb{N}^+$ is an integer, representing the number of repeated blocks in the C-ansatz encoding. We note that the actual operations implemented on IBMQ processors also include dynamics due to noise, gate, and measurement errors, and thus must be represented as a general quantum channel as in Eq.\,\eqref{appeq:quantumChannel}. As discussed in the main text, the REC of a quantum system can be computed in the presence of these more general dynamics, and is sensitive to the limitations introduced by them.

An alternative ansatz analyzed here is the \textit{Hamiltonian ansatz} (or \textit{H-ansatz} for short) where the operator $\hat{U}(u; \boldsymbol{\theta})$ describes continuous Hamiltonian dynamics. This ansatz is relevant to computation with general quantum devices, such as quantum annealers and more generally quantum simulators: 
\begin{align}
    \hat{U}(u; \boldsymbol{\theta}) = {\rm exp}\{-i\hat{H}(u)t \},~\hat{H}(u) = \hat{H}_0 + u \cdot \hat{H}_1~~~~~~~\textit{(H-ansatz)}
    \label{eq:Hu=H0+uH1}
\end{align}
Here $t$ is a continuous parameter defining the evolution time; and $\hat{H}_0 = \sum^L_{\langle l,l' \rangle} J_{l, l'} \hat{\sigma}^z_l \hat{\sigma}^z_{l'} + \sum^L_{l=1} h^x_{l} \hat{\sigma}^x_l  + \sum^L_{l=1} h^z_{l} \hat{\sigma}^z_l$ and $\hat{H}_1 = \sum^L_{l=1} h^I_{l} \hat{\sigma}^z_l$. The transverse $x$-field strength $h^x_{l} = \bar{h}^x + \varepsilon^x_{l}$ and longitudinal $z$-drive strength $h^{z,I}_{l} = \bar{h}^{z,I} + \varepsilon^{z,I}_{l}$ are all randomly chosen and held fixed for a given realization of the quantum system,
\begin{align}
    \varepsilon^{x,z,I}_{l} \sim h^{x,z,I}_{\mathrm{rms}}~\mathcal{N}(0, 1),
\end{align}
where $\mathcal{N}(0,1)$ defines the standard normal distribution with zero mean and unit variance. We consider nearest-neighbor interactions $J_{l,l'}$, which can be constant $J_{l,l'} \equiv J$, or drawn from $J_{l,l'} \sim \mathrm{Unif}[0, J_{\rm max}]$, where $\mathrm{Unif}[a,b]$ is a uniform distribution with non-zero density within $[a,b]$. 

As an aside, we note that the C-ansatz quantum channel described by Eq.\,(\ref{appeq:cansatz}) can be considered a Trotterization-inspired implementation of the H-ansatz in Eq.\,(\ref{eq:Hu=H0+uH1}). In particular, if we set $\theta^{x/z/I} = h^{x/z/I} \Delta\cdot\tau$, where $t=\Delta \cdot \tau$, and consider the limit $\Delta \to 0$ while keeping $t$ fixed, Eq.\,(\ref{appeq:cansatz}) corresponds to a Trotterized implementation of Eq.\,(\ref{eq:Hu=H0+uH1}). This correspondence is chosen for practical reasons, but is not necessary in our analysis.

% \sout{The parameterized quantum channel characterizes how information is injected into the quantum system and processed by it; however, to extract information from the quantum system, one must apply an appropriate and feasible quantum measurement. To extract information efficiently, we consider a wide family of observables $\hat{M}_k$: the only restriction on these observables is that they must be a product of local observables,  $\hat{M}_k = \hat{o}_1 \otimes \cdots \otimes \hat{o}_L$, which \textit{mutually commute} with each other (meaning they are are simultaneously measurable). We consider two general schemes. The first one is the probability representation $\hat{o}_l \in \{\ket{0}\!\bra{0}, \ket{1}\!\bra{1}\}$, while the second is the spin moments representation, $\hat{o}_l \in \{\hat{I}, \hat{\sigma}^z\}$; the former representation is used throughout the main text. We will show below that these two readout schemes are equivalent up to a unitary transformation.  For H-ansatz considered in the main text, the subroutine for generating quantum features with sampling noise is shown in Algorithm \ref{alg:arc_prob} (in quantum probability representation) and Algorithm \ref{alg:arc} (in quantum moment representation).}

\RestyleAlgo{ruled}
\SetKwComment{Comment}{/* }{ */}
\SetKwInOut{input}{Input}
\SetKwInOut{output}{Output}
\SetKwFor{For}{For}{}{EndFor}
\SetKwFor{If}{If}{}{EndIf}
\begin{algorithm}[t]
    \caption{Measured features under multinomial sampling in quantum system}
    \label{alg:arc_prob}
    \input{$u \in [-1, +1]$}
    \output{$\bar{\boldsymbol{X}}(u)$, which approximates $x_k(u) := \mathrm{Tr}\left\{\hat{\rho}(u) \ket{\boldsymbol{b}_k} \! \bra{\boldsymbol{b}_k}\right\}$}
    \For{$s \gets 1$ to $\NS$}{
        Initialize overall state $\hat{\rho}_{0} \gets \ket{0}\bra{0}^{\otimes \NQ}$\;
        Evolve under quantum channel $\mathcal{U}(u)$: $\hat{\rho}(u) \gets \mathcal{U}(u) \hat{\rho}_0$\;
        Measure all $\NQ$ qubits: $\boldsymbol{b}^{(s)}(u) \gets \boldsymbol{b}_k =  \left(b_{k,1}, b_{k,2} \cdots, b_{k,L}\right) \in \{0, 1\}^{\NQ}$\;
    }
    \For{$k \gets 0$ to $K-1$}{
        Take the ensemble averages as readout features:\\
        \hspace{5mm} $\bar{X}_k(u) \gets \frac{1}{\NS}\sum_{s=1}^{\NS} \delta (\boldsymbol{b}_k, \boldsymbol{b}^{(s)}(u))$ \Comment*[r]{Notice $x_k(u) := \mathrm{Tr}\left\{\hat{\rho}(u) \ket{\boldsymbol{b}_k} \! \bra{\boldsymbol{b}_k}\right\} = \lim_{\NS \to \infty} \bar{X}_k(u)$}
    }
\end{algorithm}

\begin{comment}
\begin{algorithm}[t]
    \caption{Measured features in the \textbf{spin moments} representation} \label{alg:arc}
    \input{$u \in [-1, +1]$}
    \output{$\bar{\boldsymbol{X}}(u)$, which approximates $x_k(u) := \mathrm{Tr}\left\{\hat{\rho}(u) \hat{M}_k \right\}$}
    \For{$s \gets 1$ to $\NS$}{
        Initialize overall state $\hat{\rho}_{0} \gets \ket{0}\bra{0}^{\otimes \NQ}$\;
        Evolve under quantum channel $\mathcal{U}(u)$: $\hat{\rho}(u) \gets \mathcal{U}(u) \hat{\rho}_0$\;
        Measure all $\NQ$ qubits: $\boldsymbol{b}^{(s)}(u) \gets \boldsymbol{b}_k =  \left(b_{k,1}, b_{k,2} \cdots, b_{k,L}\right) \in \{0, 1\}^{\NQ}$\;
        Compute $z^{(s)}_l(u) \gets (-1)^{b^{(s)}_l(u)} \in \{+1, -1\}$ for each $l \in [L]$\;
        \For{$k \gets 1$ to $K$}{
            \If{$\hat{M}_k = \hat{\sigma}_{l_1}^z \hat{\sigma}_{l_2}^z \cdots \hat{\sigma}_{l_m}^z$}{
                $X^{(s)}_k(u) \gets z^{(s)}_{l_1}(u) z^{(s)}_{l_2}(u) \cdots z^{(s)}_{l_m}(u) \in \{+1, -1\} $;
            }
        }
    }
    \For{$k \gets 0$ to $K-1$}{
        Take the ensemble averages as readout features:\\
        \hspace{5mm} $\bar{X}_k(u) \gets \frac{1}{\NS} \sum_{s=1}^{\NS} X^{(s)}_{k} (u)$ \Comment*[r]{Notice $x_k(u) := \mathrm{Tr}\left\{\hat{\rho}(u) \hat{M}_k \right\} = \lim_{\NS \to \infty} \bar{X}_k(u)$}
    }
\end{algorithm}
\end{comment}

\begin{algorithm}[t]
    \caption{Training of output weights}\label{alg:training}
    \input{$\{ u^{(1)}, \cdots, u^{(N)} \} \in [-1, +1]^N$}
    \output{$\widetilde{\boldsymbol{w}}_N$, such that $y = \widetilde{\boldsymbol{w}}_N \cdot \bar{\boldsymbol{X}}(u)$ can approximate $f(u)$}
    \For{$n \gets 1$ to $N$}{
        Generate features $\bar{\boldsymbol{X}}(u^{(n)})$  through Algorithm 1
    }
    Collect the features into a regression matrix $\regmat \in \mathbb{R}^{N \times K}$\;
    Compute empirical Gram matrix $\bar{\gr} \gets \frac{1}{N} \regmat^T \regmat$ \Comment*[r]{For finite $S$, $\lim_{N \to \infty} \bar{\gr} = \tilde{\gr} := \gr + \frac{1}{S} \ci$}
    Compute target vector $\boldsymbol{Y} \gets \left(f(u^{(1)}), \cdots, f(u^{(N)}) \right)^T$\;
    $\boldsymbol{w}^{\ast} \gets (\regmat^T \regmat)^{-1} \regmat^T \boldsymbol{Y} $ \Comment*[r]{For finite $S$, $\lim_{N \to \infty} \boldsymbol{w}^{\ast} = \boldsymbol{w} :=$ Eq.\,(\ref{eq:wopt})}
\end{algorithm}

\begin{comment}
\subsection{Extracting output features under finite sampling: expressions for features and covariances}
\label{app:analytic_expressions_for_covariances}

Following the evolution of the quantum system under the input-dependent Hamiltonian given by Eq.\,(\ref{eq:Hu=H0+uH1}), we extract certain measurable observables that are used as outputs for any learning task. The form of observables is again chosen for compliance with measurement protocols native to near-term quantum computing implementations: we consider Pauli $z$ basis measurements only (although this can be generalized easily). This means our algorithm has access only to diagonal terms in $\hat{\rho} (u)$. We abbreviate vectors $\vec{M}_k, \vec{\rho} (u) \in \mathbb{R}^{K}$ such that $(\vec{M}_{k'})_{k} = (\hat{M}_{k'})_{kk}$ and $\left( \vec{\rho} (u) \right)_{k} = \hat{\rho} (u)_{kk}$. Then one can check for $\{ + 1, - 1 \}$ readout (spin moment representation): $\vec{M}_k \cdot \vec{M}_{k'} = K \delta_{j j'}$, and the readout features can be expressed into dot product form $x_k (u) = \mathrm{Tr} \! \left\{ \hat{M}_k \hat{\rho}(u) \right\} = \vec{M}_k \cdot \vec{\rho} (u)$. In QRC, we hope to make full use of all functions in family $\left\{ \left( \vec{\rho} (u) \right)_k \right\}_{k \in [K]}$ as readout features. The collection of all readout features
\begin{equation}
  \bm{x} (u) = \left(\begin{array}{c}
    x_0 (u)\\
    x_1 (u)\\
    \vdots\\
    x_{K-1} (u)
  \end{array}\right) = \left(\begin{array}{c}
    \vec{M}^T_0\\
    \vec{M}^T_1\\
    \vdots\\
    \vec{M}^T_{K-1}
  \end{array}\right) \vec{\rho} (u) =:
  U \vec{\rho} (u), 
\end{equation}
The orthonormality of $\{ \vec{M}_k \}_{k \in [K]}$ implies that $U$ is unitary up to an overall constant (in fact, $U = \left( 
\begin{array}{cc}
    1 & 1 \\
    1 & -1
\end{array}
\right)^{\otimes \NQ}$ is the \textit{Hadamard matrix}~\cite{nielsen2002quantum}). This unitarity implies that the above transformation is information-preserving. In particularly, this guarantees the ability to reconstruct the \textit{diagonal} QRC density matrix elements (via tomography), $\vec{\rho}(u) = U^{-1} \bm{x} (u)$, simply computing the required inverse via the numerically-robust relationship $U^{- 1} = \frac{1}{K} U^{T}$. If each qubit has a readout error $\epsilon$, that is, it will flip $\ket{0} \leftrightarrow \ket{1}$. Then the transition probability of reading out $\ket{\bm{b}_{k'}}$ from $\ket{\bm{b}_{k}}$ will be $\epsilon^{d(\bm{b}_{k}, \bm{b}_{k'})} (1-\epsilon)^{L-d(\bm{b}_{k}, \bm{b}_{k'})}$ where $d(\bm{b}_{k}, \bm{b}_{k'})$ is the Hamming distance between $\bm{b}_{k}$ and $\bm{b}_{k'}$. Thus, readout errors can furthermore be mathematically modeled by one more transition matrix (more precisely, a \textit{stochastic matrix}):
\begin{align}
    \bm{x} (u) = U \left( 
    \begin{array}{cc}
        1-\epsilon & \epsilon \\
        \epsilon & 1-\epsilon
    \end{array} 
    \right)^{\otimes \NQ} \vec{\rho} (u).
\end{align}
The covariance of the $\boldsymbol{X}(u) \in \{+1, -1\}^{\NQ}$ (the random features for individual shot $\NS=1$) can also be expressed easily: 
\begin{align}
    \mathbb{V}[\boldsymbol{X}(u)] = U \left(\mathrm{diag} \! \left(\vec{\rho} (u)\right) - \vec{\rho} (u) \cdot \vec{\rho} (u)^T \right) U^T \label{eq:CovXu}
\end{align}
where ${\rm diag}(\vec{v})$ is a diagonal matrix that has the elements of $\vec{v}$ as entries. To prove this expression, it suffices to verify that the second order moments are entries
\begin{equation}
    \mathbb{V}[\boldsymbol{X} (u)]_{k_1 k_2} \equiv \mathrm{Tr} \left\{\hat{M}_{k_1} \hat{M}_{k_2} \hat{\rho} (u) \right\} = \sum_{k = 0}^{K-1} (\hat{M}_{k_1} \hat{M}_{k_2})_{kk} \, \hat{\rho}_{kk} (u) = \sum_{k = 0}^{K-1} \left( U \right)_{k_1 k} \left( U \right)_{k_2 k} \hat{\rho}_{kk} (u) = \left( U \mathrm{diag} \left( \vec{\rho} (u) \right) U^T \right)_{k_1 k_2} .
\end{equation}
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section S3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Information capacity with sampling noise}
\label{sec:Information_capacity_saturation}

In this Appendix \ref{sec:Information_capacity_saturation}, we provide a detailed \textit{theoretical} construction of the resolvable expessive capacity (REC) analysis. Important related \textit{numerical} techniques required for the \textit{practical} calculation of REC are addressed in Appendix \ref{app:Spectral_finite_statistics}.

We start building the theory by rewriting the functional capacity Eq.\,(\ref{eq:fcap}) into a generalized Rayleigh quotient Eq.(\ref{eq:Rayleigh_quotient}) in Appendix \ref{sec:DefCap}. By solving for the critical point of this generalized Rayleigh quotient, we prove in Appendix \ref{app:eigentasks} that it naturally defines a set of orthonormal functions -- the \textit{eigentasks} $\{y^{(k)}\}$. Each eigentask of the Rayleigh quotient is associated with a particular eigenvalue $\{\beta^2_k\}$, called eigen-noise-to-signal ratio (NSR), whose meaning is interpreted in Appendix \ref{sec:noisy_ET}. In Appendix \ref{app:EC}, we derive a formula $\EC = \sum_{k} 1/(1 + \beta_k^2/S)$ to compute the resolvable expressive capacity, which reproduces the main result Eq.\,(\ref{eq:EC}) in the main text. 
In many practical scenarios in machine learning, like classification problems, there is usually one more nonlinear postprocessing function $\sigma_{\mathrm{NL}}$ acting on $\bm{W}\cdot\bar{\bm{X}}(u)$. We generalize our methodology to those cases in Appendix \ref{app:ComplxNonLin}, via an approximation truncating the third and higher order derivatives of $\sigma_{\mathrm{NL}}$. 
Finally, we give a simplified, equivalent form of eigenproblem in Appendix \ref{app:IntroD} specifically for systems obeying multinomial statistics, which allows solving for both the eigentasks and NSR eigenvalue more simply.

\subsection{Definition of capacity for physical systems with sampling noise}
\label{sec:DefCap}
Suppose an arbitrary probability distribution $p(u)$ for a random (scalar) variable $u$ defined in $D \subseteq \mathbb{R}$. This naturally defines a function space $L^2_p (D)$ containing all functions $f:D \to \mathbb{R}$ with $\int f^2(u) p(u) \dd u < \infty$. The space is equipped with the inner product structure $\langle f_1, f_2 \rangle_{p} = \int f_1(u) f_2(u) p(u) \dd u$. We first review the definition of the function approximation capacity known as the {\it Information Processing Capacity} (IPC) introduced in Ref.~\cite{dambre_information_2012}. This deterministic quantity is based on a metric quantifying the accuracy of a physical system to approximate one of the functions $f_\ell(u)$ of its input through a linear estimator based on its accessible (measurable) degrees of freedom $x_k (u)$:
\begin{equation}
    C [f_\ell] = 1 - \min_{\boldsymbol{W}_\ell \in \mathbb{R}^K}  \frac{\int \left( \sum_{k = 0}^{K-1} W_{\ell k} x_k (u) - f_{\ell} (u) \right)^2 p(u) \dd u}{\int f_{\ell} (u)^2 p(u)  \dd u},
\end{equation}
where functions $f_{\ell}(u)$ are orthogonal target functions $\langle f_{\ell}, f_{\ell'} \rangle_{p} = \int f_{\ell}(u) f_{\ell'}(u) p(u) \dd u = 0$ for $\ell \neq \ell'$. The IPC is defined as $\EC \equiv \sum_{\ell=0}^{\infty} C[f_{\ell}]$, capturing the ability of what type of function the linear combination of physical system readout features can produce. Ref.~\cite{dambre_information_2012} provides an upper bound for the IPC; the IPC of any generic dynamical system is bounded by the accessible degrees of freedom, $\EC \leq K$. 

While this result is quite general, it neglects the limitations due to noise in readout features, which is unavoidable when using physical systems in the presence of finite computational and measurement resources. It is generally accepted that the capacity is reduced in the presence of additive noise, but there are no general results on how to quantify that reduction in the presence of {\it general physical noise}. This is our goal here, to arrive at an exact result for capacity reduction under well-defined but sufficiently general conditions given the physical system. Additional desideratum on this metric is that it provides a practical, calculable metric that can be calculated (1) either from numerical solution of the dynamics of the physical system incorporating a sufficiently accurate noise model, or (2) if desired, from experimental data extracted from said physical system. In the main text, we provide a comparative analysis of these two modalities for a 7-qubit superconducting quantum processor.

We start from considering the noisy readout features $\bar{\bm{X}}(u)$ whose expectations and covariances are
\begin{align}
    \Es{\bar{\boldsymbol{X}} (u)} & \equiv \boldsymbol{x}(u), \\
    \Covs [\bar{\boldsymbol{X}} (u)] & \equiv \frac{1}{\NS} \cu(u),
\end{align}
where the expectation and covariance are evaluated under the product distribution of $S$-shot i.i.d.\,variable $\{X^{(s)}_k(u)\}$. To determine the optimal capacity to compute an arbitrary normalized function $f (u)$ using the noisy readout features $\bar{\bm{X}}(u)$ extracted from the physical system, we need to find an optimal $\boldsymbol{W}$ such that
\begin{equation}
    C [f] = 1 - \frac{\min_{\boldsymbol{W}}  \int \Es{\left( \sum_{k = 0}^{K-1} W_k \bar{X}_k (u) - f (u) \right)^2} p(u) \dd u}{\int f^2 (u) p(u) \dd u}. \label{eq:defIPC}
\end{equation}

By expanding the numerator of the right-hand side for a given, finite number of shots $\NS$, we find
\begin{align}
     & \int f^2 (u) p(u) \dd u - \int \Es{\left( \sum_{k = 0}^{K-1} W_k \bar{X}_k (u) - f (u) \right)^2} p(u) \dd u \nonumber\\
     =~&\! - \sum_{k_1 = 0}^{K-1} \sum_{k_2 = 0}^{K-1} W_{k_1} W_{k_2}  \int \Es{\bar{X}_{k_1} (u) \bar{X}_{k_2} (u)} p(u) \dd u + 2 \sum_{k = 0}^{K-1} W_k  \int \Es{\bar{X}_k (u)} f (u) p(u) \dd u \nonumber\\
    % \approx~&\! - \frac{1}{\NI} \sum_{k_1 = 0}^{K-1} \sum_{k_2 = 0}^{K-1} W_{k_1} W_{k_2}  \sum_{n = 1}^{\NI} \bar{X}_{k_1} (u^{(n)}) \bar{X}_{k_2} (u^{(n)}) + \frac{2}{\NI} \sum_{k = 0}^{K-1} W_k  \sum_{n = 1}^{\NI} \bar{X}_k (u^{(n)}) f (u^{(n)}) . 
    =~& - \sum_{k_1 = 0}^{K-1} \sum_{k_2 = 0}^{K-1} W_{k_1} W_{k_2}  \int \left( x_{k_1} (u) x_{k_2} (u) + \frac{1}{\NS} \cu(u)_{k_1 k_2} \right) p(u) \dd u + 2 \sum_{k = 0}^{K-1} W_k  \int x_k (u) f (u) p(u) \dd u. \label{eq:Lambdatilde}
\end{align}
where the final lines comes from the property of covariance matrix $\Es{\bar{X}_{j} (u) \bar{X}_{k} (u)} - \Es{\bar{X}_{j} (u)} \Es{\bar{X}_{k} (u)} = \frac{1}{\NS} \cu(u)_{j k}$.
% where we have approximated the integral over the input domain by a finite sum in the limit of a large number of inputs $\NI$. Next, note that if $n \neq n'$, then $X_{k_1} (u^{(n)})$ and $X_{k_2} (u^{(n')})$ are independent random variables (though not necessarily identically distributed). The sums over $N$ on the right hand side are therefore sums of bounded independent random variables. In the limit of large $\NI \gg 1$, the deviation between stochastic realizations of these sums and their expectation values is exponentially suppressed, as determined by the Hoeffding inequality. Then, with large probability, the sums over $N$ may be replaced by their expectation values,
% \begin{align}
    %  & \int f^2 (u) p(u) \dd u - \int \rev{\Es{\left( \sum_{k = 0}^{K-1} W_k \bar{X}_k (u) - f (u) \right)^2}} p(u) \dd u \nonumber\\
    % \approx~& - \frac{1}{\NI} \sum_{k_1 = 0}^{K-1} \sum_{k_2 = 0}^{K-1} W_{k_1} W_{k_2}  \sum_{n = 1}^{\NI} \Es{\bar{X}_{k_1} (u^{(n)}) \bar{X}_{k_2} (u^{(n)})} + \frac{2}{\NI} \sum_{k = 0}^{K-1} W_{k}  \sum_{n = 1}^{\NI} \Es{\bar{X}_k (u^{(n)}) f (u^{(n)})} \nonumber\\
    % =~& - \frac{1}{\NI} \sum_{k_1 = 0}^{K-1} \sum_{k_2 = 0}^{K-1} W_{k_1} W_{k_2}  \sum_{n = 1}^{\NI} \left( x_{k_1} (u^{(n)}) x_{k_2} (u^{(n)}) + \frac{1}{\NS} \cu(u^{(n)})_{k_1 k_2} \right) + \frac{2}{\NI} \sum_{k = 0}^{K-1} W_k \sum_{n = 1}^{\NI} x_k (u^{(n)}) f (u^{(n)}) \nonumber\\
% \end{align}
% The first approximation above comes from the Hoeffding inequality. For the first term in the second line, since $\bar{X}_{k_1} (u^{(n)}) \bar{X}_{k_2} (u^{(n)}) \in [0, 1]$, we have
% \begin{align}
%     \mathrm{Prob}\!\left[ \left| \frac{1}{\NI} \sum_{n = 1}^{\NI} \bar{X}_{k_1} (u^{(n)}) \bar{X}_{k_2} (u^{(n)}) - \Es{\frac{1}{\NI} \sum_{n = 1}^{\NI} \bar{X}_{k_1} (u^{(n)}) \bar{X}_{k_2} (u^{(n)})} \right| \geq \epsilon \right] \leq 2 e^{- 2 N \epsilon^2},
% \end{align}
% where the probability of large deviation is suppressed exponentially with $\NI$ (similarly for the second term in the second line). In going from the second to the third line,  Thus we can say the original integral in Eq.\,(\ref{eq:defIPC}) is approximately equal to Eq.\,(\ref{eq:Lambdatilde}) to $O (1/\sqrt{\NI})$. In the limit of a large number of input samples, $N \to \infty$, we conclude that all approximations can be replaced by exact equalities. The final expression is obtained by rewriting sums over $u$ as integrals, with an error proportional to $1/\sqrt{\NI}$ once more.

The goal of the remaining part of this section is deducing a more compact generalized Rayleigh quotient form of functional capacity. The dependence of readout features $x_k(u)$ on the input $u$ can always be written in the form of a Taylor expansion,
\begin{align}
    x_k (u) = \sum_{j = 0}^{\infty} (\mathbf{T})_{k j} u^j
    \label{eq:TMat}
\end{align}
where we define the \textit{transfer matrix} $\mathbf{T}(\bm{\theta})\equiv \mathbf{T} \in \mathbb{R}^{K \times \infty}$ that depends on the density matrix $\hat{\rho}(u)$, and in particular on parameters $\bm{\theta}$ characterizing the physical system. 
The first term in Eq.\,(\ref{eq:Lambdatilde}) does not depend explicitly on the function $f(u)$ being constructed, and introduces quantities that are determined entirely by the response of the physical system of interest to inputs over the entire domain of $u$. In particular, we introduce the \textit{Gram matrix} $\gr \in \mathbb{R}^{K \times K}$ as
\begin{align}
    (\gr)_{k_1k_2} &= \int x_{k_1}(u) x_{k_2} (u) p(u) \dd u  = \sum_{j_1 = 0}^{\infty} \sum_{j_2 = 0}^{\infty} (\mathbf{T})_{k_1 j_1} \left( \int u^{j_1 + j_2} p(u) \dd u \right) (\mathbf{T})_{k_2 j_2} \equiv (\mathbf{T}\gh\mathbf{T}^T)_{k_1k_2}
\end{align}
where we have also introduced the \textit{generalized Hilbert matrix} $\gh \in \mathbb{R}^{\infty \times \infty}$ as
\begin{equation}
    (\gh)_{j_1 j_2} = \int u^{j_1 + j_2} p(u) \dd u .
\end{equation}
Secondly, we introduce the noise matrix $\ci \in \mathbb{R}^{K \times K}$,
\begin{align}
    (\ci)_{k_1 k_2} & = \int \cu(u)_{k _1k_2}~p(u) \dd u
    \label{eq:V}
\end{align}
The second term in Eq.\,(\ref{eq:Lambdatilde}) depends on $f(u)$ and can be simplified using the $\gh$ matrix as well. Introducing the Taylor series expansion $f (u) = \sum_{j = 0}^{\infty} (\mathbf{Y})_j u^j$
\begin{align} 
    \Eu{x_k f} = \int x_{k} (u) f (u) p(u) \dd u & = \sum_{j_1 = 0}^{\infty} \sum_{j_2 = 0}^{\infty} (\mathbf{T})_{k j_1} \left( \int u^{j_1 + j_2} p(u) \dd u \right) (\mathbf{Y})_{j_2} = (\mathbf{T}\gh\mathbf{Y})_{k}.
\end{align}
With these definitions, Eq.\,(\ref{eq:defIPC}) can be compactly written in matrix form as a Tikhonov regularization problem: 
\begin{equation}
    C[f] =
    % = \max_{\boldsymbol{W}} \left( \frac{-\boldsymbol{W}^T \left( \mathbf{T} \gh\mathbf{T}^T + \frac{1}{\NS} \ci \right) \boldsymbol{W} + 2\boldsymbol{W}^T \mathbf{T}\gh\mathbf{Y}} {\mathbf{Y}^T\gh\mathbf{Y}} \right) 
    1 - \min_{\boldsymbol{W}} \left( \frac{ \left\| \gh^{\frac{1}{2}}\mathbf{T}^T \boldsymbol{W} - \gh^{\frac{1}{2}}\mathbf{Y} \right\|^2 + \frac{1}{\NS} \boldsymbol{W}^T \ci \boldsymbol{W}} {\mathbf{Y}^T\gh\mathbf{Y}} \right). \label{eq:Cf}
\end{equation}
The least-squares form ensures that the optimal value (argmin) $\boldsymbol{w}$ of $\boldsymbol{W}$ has closed form 
\begin{equation}
    \boldsymbol{w} = \left( \gr + \frac{1}{\NS} \ci \right)^{- 1} \Eu{\bm{x}f} = \left( \mathbf{T}\gh\mathbf{T}^T + \frac{1}{\NS} \ci \right)^{- 1} \mathbf{T}\gh\mathbf{Y}. \label{eq:wopt}
\end{equation}
Substituting $\bm{w}$ into the expression for $C$, we obtain the optimal capacity with which a function $f$ can be constructed, which takes the form of a \textit{generalized Rayleigh quotient}
\begin{equation}
    C[f] = \frac{\Eu{\bm{x}^Tf} \left( \gr + \frac{1}{\NS} \ci \right)^{- 1} \Eu{\bm{x}f}}{\Eu{f^2}} = \frac{\mathbf{Y}^T \gh\mathbf{T}^T \left( \gr + \frac{1}{\NS} \ci \right)^{- 1} \mathbf{T}\gh\mathbf{Y}}{\mathbf{Y}^T\gh\mathbf{Y}}. \label{eq:Rayleigh_quotient}
\end{equation}

In scenario of fundamental quantum measurement noise, or quantum sampling noise, given $u$ and $\NS$, the quantum readout features $\bar{X}_k (u) = \frac{1}{\NS} \sum_{s=1}^{\NS} \delta(k^{(s)} (u), k)$ are stochastic variables. The expectation vector and covariance matrix of $\bar{\bm{X}} (u)$ can be expressed in terms of $\hat{\rho} (u)$
\begin{align}
    \Es{\bar{\boldsymbol{X}} (u)} & \equiv \boldsymbol{x}(u) = \mathrm{Tr}\{\hat{M}_k \hat{\rho}(u)\}, \label{eq:Ux}\\
    \Covs [\bar{\boldsymbol{X}} (u)] & \equiv \frac{1}{\NS} \cu(u) = \frac{1}{\NS} \left(\mathrm{diag}(\bm{x})-\bm{x}\bm{x}^T \right). \label{eq:Sigma}
\end{align}
To understand Eq.\,(\ref{eq:Sigma}), we provide brief proofs of some important identities involving the second order statistics of multinomial distribution. In the case of quantum measurement noise, the single-shot random-valued feature is the indicator $X^{(s)}_k (u) = \delta (k^{(s)} (u), k)$ for shot $s$ and input $u$. By definition, for any $s$, the expectation of indicator is always the probability of obtaining index $k$, given the input $u$: $\Es{X^{(s)}_k (u)} = x_k (u)$. If $s \neq s'$, the experiment for different shots must be independent, so we have $\Es{X^{(s)}_k (u) X^{(s')}_{k'} (u)} = x_k (u) x_{k'} (u)$; while if $s = s'$, the mutual exclusion for getting different indices in one shot implies that $\Es{X^{(s)}_k (u) X^{(s)}_{k'} (u)} = \delta_{k k'} x_k (u)$. Thus, we can unify them into one equation
\begin{equation}
    \Es{X^{(s)}_k (u) X^{(s')}_{k'} (u)} = \delta_{s s'} \delta_{k k'} x_k (u) + (1 - \delta_{s s'}) x_k (u) x_{k'} (u) = x_k (u) x_{k'} (u) + \delta_{s s'} (\delta_{k k'} x_k (u) - x_k (u) x_{k'} (u)) .
\end{equation}
Then, the expectation of the product of $\bar{X}_k$ and $\bar{X}_{k'}$ is 
\begin{align}
    \Es{\bar{X}_k (u) \bar{X}_{k'} (u)} & = \frac{1}{S^2} \sum_{s, s'} (x_k (u) x_{k'} (u) + \delta_{s s'} (\delta_{k k'} x_k (u) -  x_k (u) x_{k'} (u))) \nonumber\\
    & = x_k (u) x_{k'} (u) + \frac{1}{S} (\delta_{k k'} x_k (u) - x_k (u) x_{k'} (u)), \label{eq:2ndMat}
\end{align}
which can be directly used to derive Eq.\,(\ref{eq:Sigma}), namely the element $\cu_{k k'}(\UI)$ is:
\begin{align}
    \Es{ [(\bar{X}_k (u) - x_k (u)) (\bar{X}_{k'} (u) - x_{k'} (u))} & = \Es{\bar{X}_k (u) \bar{X}_{k'} (u)} - \Es{\bar{X}_k (u) x_{k'} (u)} -\Es{x_k (u) \bar{X}_{k'} (u)} + \Es{x_k (u) x_{k'} (u)} \nonumber\\
    & = x_k (u) x_{k'} (u) + \frac{1}{S} (\delta_{k k'} x_k (u) - x_k (u) x_{k'} (u)) - x_k (u) x_{k'} (u) \nonumber\\
    & = \frac{1}{S} (\delta_{k k'} x_k (u) - x_k (u) x_{k'} (u)) . 
\end{align}
Therefore for quantum sampling noise, we can express the noise matrix $\ci$ more explicitly,
\begin{align}
    (\ci)_{k_1 k_2} & = \int \cu(u)_{k _1k_2}~p(u) \dd u = \int (\delta_{k_1 k_2} x_{k_1} (u) - x_{k_1}\!(u) x_{k_2}\!(u)) p(u) \dd u \equiv (\mathbf{D})_{k_1k_2}-(\gr)_{k_1k_2}
    \label{eq:defV}
\end{align}
Here we have also introduced the \textit{second-order-moment} matrix $\mathbf{D} \in \mathbb{R}^{K \times K}$ such that
\begin{align}
    (\mathbf{D})_{k_1 k_2} = \delta_{k_1 k_2} \sum_{k} \gr_{k k_1} = \delta_{k_1 k_2} \int x_{k_1} (u) p(u) \dd u. \label{eq:defD}
\end{align}
Then, the noise matrix simply defines the covariance of readout features, and is therefore given by $\ci = \mathbf{D} - \gr$.

In the study of quantum machine learning, it is convenient to define the $t$-th order \textit{quantum ensemble moment} of ensemble $\mathcal{E} = \{ p (\bm{u}) \dd \bm{u}, \hat{\rho} (\bm{u}) \}$ in a $t$-copy space \cite{Harrow2009}:
\begin{equation}
  \hat{\rho}^{(t)} = \int \hat{\rho} (\bm{u})^{\otimes t} p (\bm{u}) \dd \bm{u} .
\end{equation}
The Gram matrix and second-order moment matrix can then be compactly expressed as
\begin{align}
  \mathbf{D}_{k k} & = \int x_k (\bm{u}) p (\bm{u}) \dd \bm{u} = \int \mathrm{Tr} \{\hat{M}_k \hat{\rho} (\bm{u})\} p (\bm{u}) \dd \bm{u} = \mathrm{Tr} \! \left\{ \int \hat{M}_k \hat{\rho} (\bm{u}) p (\bm{u}) \dd \bm{u} \right\} = \mathrm{Tr} \{\hat{M}_k \hat{\rho}^{(1)}\}, \\
  \gr_{k k'} & = \int x_k (\bm{u}) x_{k'} (\bm{u}) p (\bm{u}) \dd \bm{u} = \int \mathrm{Tr} \{\hat{M}_k \hat{\rho} (\bm{u})\} \mathrm{Tr} \{\hat{M}_k \hat{\rho} (\bm{u})\} p (\bm{u}) \dd \bm{u} \nonumber\\
  & = \mathrm{Tr} \! \left\{ (\hat{M}_k \otimes \hat{M}_{k'}) \left( \int \hat{\rho} (\bm{u}) \otimes \hat{\rho} (\bm{u}) p (\bm{u}) \dd \bm{u} \right) \right\} = \mathrm{Tr} \{(\hat{M}_k \otimes \hat{M}_{k'}) \hat{\rho}^{(2)}\}. 
\end{align}

While most of the results in our paper do not utilize this representation, we do find that it provides a compact and natural representation and we use this representation to derive the analytical results in \Sec{sec:twodesign}, see Appendix~\ref{app:2designsol}.

\subsection{Eigentasks}
\label{app:eigentasks}
Eq.\,(\ref{eq:Rayleigh_quotient}) defines the optimal capacity of approximating an arbitrary function $f (u) = \sum_{j = 0}^{\infty} (\mathbf{Y})_j u^j$. We can therefore naturally ask which functions $f$ maximize this optimal capacity. To this end, we first note that the denominator of Eq.\,(\ref{eq:Rayleigh_quotient}) is simply a normalization factor that can be absorbed into the definition of the function $f(u)$ being approximated, without loss of generality. More precisely, we consider:
\begin{align}
    \langle f, f \rangle_p = 1 = \left( \gh^{\frac{1}{2}} \mathbf{Y} \right)^T \left( \gh^{\frac{1}{2}} \mathbf{Y} \right) = \mathbf{Y}^T\gh\mathbf{Y}.
\end{align}
Then, we can rewrite the optimal capacity from Eq.\,(\ref{eq:Rayleigh_quotient}) as
\begin{align}
    C[f] = \mathbf{Y}^T\gh^{\frac{1}{2}} \mathbf{Q} \gh^{\frac{1}{2}}\mathbf{Y}.
    \label{eq:Cp}
\end{align}
Here we have defined the matrix $\mathbf{Q} \in \mathbb{R}^{\infty \times \infty}$ as
\begin{align}
    \mathbf{Q} &= \mathbf{B}\left(\mathbf{I} + \frac{1}{\NS} \mathbf{R} \right)^{- 1} \!\!\! \mathbf{B}^{T}, \label{eq:Q} \\
    \mathbf{B} &= \gh^{\frac{1}{2}} \mathbf{T}^T\gr^{-\frac{1}{2}},
    \label{eq:B} \\
    \mathbf{R} & = \gr^{-\frac{1}{2}}\mathbf{V}\gr^{-\frac{1}{2}}
\end{align}
by introducing the matrix square root of $\gr^{\frac{1}{2}} \in \mathbb{R}^{K \times K}$, and the \textit{noise-to-signal ratio} (NSR) matrix $\mathbf{R}$. The decomposition in Eq.\,(\ref{eq:Q}) may be verified by direct substitution into Eq.\,(\ref{eq:Cp}). The ability to calculate matrix powers and in particular the inverse of $\gr$ requires constraints on its rank. 
% We show in Appendix \ref{app:Function-independence} that the rank of $\gr$ is equal to the number of linearly-independent functions in $\{x_k(u)\}_{k \in [K]}$. 

Before we analytically find the eigenvectors of $\mathbf{Q}$, we need to show in general that the number of linearly independent features always equals to the rank of the Gram matrix $\gr$, no matter what symmetries the system is subject to. Let us consider any vector $\bm{c}\in \mathbb{R}^K$, the quadratic form
\begin{align}
    \sum_{k_1,k_2=0}^{K-1} c_{k_1} c_{k_2} (\gr)_{k_1, k_2} =  \int \left( \sum_{k_1=1}^{K} c_{k_1} x_{k_1} (u) \right)\!\! \left( \sum_{k_2=1}^{K} c_{k_2} x_{k_2} (u) \right) p(u) \dd u = \left\langle \sum_{k=0}^{K-1} c_{k} x_{k}, \sum_{k=0}^{K-1} c_{k} x_{k} \right\rangle_p. 
\end{align}
gives the norm of function $\sum_{k=0}^{K-1} c_{k} x_{k} (u)$ in RHS. The summation $\sum_{k_1,k_2=1}^{K} c_{k_1} c_{k_2} (\gr)_{k_1, k_2}=0$ vanishes if and only if function $\sum_{k=0}^{K-1} c_{k} x_{k} (u)$ is a zero function, namely $\bm{c}$ is in the null space of $\gr$. We conclude that the rank Gram matrix $\gr$ is equal to the number of linearly independent features. One way for the rank of $\gr$ to be deficient is through the use of the input encoding that leads to identical dependence of the features in a way that is not broken by the rest of the system's interactions (dictated by $\bm{\theta}$). We do not consider such symmetries. All simulations utilize encodings that result in a full rank $\gr$. In case of a rank-deficient $\gr$, one should replace all appearances of  $\gr^{-1}$ above with the pseudoinverse $\gr^{+}$.

%For a generic system with no particular symmetry, this is always true. For the special case when the Gram matrix does not have a full rank, one should replace the inverse $\gr^{-1}$ with the Moore-Penrose pseudoinverse $\gr^{+}$. Therefore, instead of defining a redundant symbol for the rank of $\gr$, we only consider the full rank $\gr$, and always use $\gr^{-1}$ instead of $\gr^{+}$. 
% This is consistent with the same notation used in Eq.\,(6) in Ref.\,\cite{dambre_information_2012}.

We now consider the measure-independent part of the eigenvectors of $\mathbf{Q}$, indexed $\mathbf{Y}^{(k)}$, satisfying the standard eigenvalue problem:
\begin{align}
    \mathbf{Q} \gh^{\frac{1}{2}} \mathbf{Y}^{(k)} = C_k\gh^{\frac{1}{2}}\mathbf{Y}^{(k)}. \label{eq:eigQ}
\end{align}
where $k=0,\cdots,K-1$. From Eq.\,(\ref{eq:Cp}), it is clear that these eigenvectors have a particular meaning. Consider the function $y^{(k)}(u)$ defined by the eigenvector $\mathbf{Y}^{(k)}$, namely
\begin{align}
    y^{(k)}(u) = \sum_{j = 0}^{\infty} \mathbf{Y}^{(k)}_{j} u^j, \label{eq:fkdef}
\end{align}
which we will refer to from now on as \textit{eigentasks}. Suppose we wish to construct the function $y^{(k)}(u)$ using outputs obtained from the physical system defined by $\mathbf{Q}$ in the $\NS\to\infty$ limit (namely, with \textit{deterministic} outputs). At a first glance, before we dive into solving the eigenproblem Eq.(\ref{eq:eigQ}), we do not know any relationship between $y^{(k)}$ and $\bm{x}(u)$.The rest part of this subsection is aiming to  prove that $y^{(k)}$ must be a specific linear combination of features $\bm{x}(u)$. Then, the physical system's capacity for this construction is simply given by the corresponding eigenvalue $C_{k}$, as may be seen by substituting Eq.\,(\ref{eq:eigQ}) into Eq.\,(\ref{eq:Cp}). Formally, the $y^{(k)}(u)$ serves as the \textit{critical point} (or \textit{stationary point}) of the generalized Rayleigh quotient in Eq.\,(\ref{eq:Rayleigh_quotient}). Consequently, the function that is constructed with largest capacity then corresponds to the nontrivial eigenvector with largest eigenvalue. 

To obtain these eigentasks, we must solve the eigenproblem defined by Eq.\,(\ref{eq:eigQ}). Here, the representation of $\mathbf{Q}$ in Eq.\,(\ref{eq:Q}) becomes useful, as we will see that the eigensystem of $\mathbf{Q}$ is related closely to that of the NSR matrix $\mathbf{R}$. In particular, we first define the eigenproblem of $\mathbf{R}$,
\begin{align}
    \nsr \gr^{\frac{1}{2}}\bm{r}^{(k)} & = \beta_k^2 \gr^{\frac{1}{2}}\bm{r}^{(k)} \label{eq:Reigen}
\end{align}
with NSR eigenvalues $\beta_k^2$ and corresponding eigenvectors $\bm{r}^{(k)}$, which satisfy the orthogonality relation $\bm{r}^{(k')T} \gr \bm{r}^{(k)} = \delta_{k, k'}$. Here the $\bm{r}^{(k)}$ can also be computed from the solution to a simpler generalized eigen-problem, where matrix square root operation $\gr^{\frac{1}{2}}$ is not needed:
\begin{align}
    \ci \bm{r}^{(k)} = \beta^2_k \gr \bm{r}^{(k)}. \label{eq:eigenproblem}
\end{align}
This is because $ \ci \bm{r}^{(k)} = \gr^{\frac{1}{2}} \nsr \gr^{\frac{1}{2}} \bm{r}^{(k)} = \beta_k^2 \gr^{\frac{1}{2}} \gr^{\frac{1}{2}} \bm{r}^{(k)} = \beta^2_k \gr \bm{r}^{(k)}$. The prefactor $\gr^{\frac{1}{2}}$ is introduced for later convenience. Eq.\,(\ref{eq:Reigen}) then allows us to define the related eigenproblem
\begin{align}
    \left(\mathbf{I} + \frac{1}{\NS} \mathbf{R} \right)^{- 1}\!\!\! \gr^{\frac{1}{2}}\bm{r}^{(k)}  = \left( 1 + \frac{\beta_k^2}{\NS}  \right)^{-1} \gr^{\frac{1}{2}}\bm{r}^{(k)}
\label{eq:eigRinvSol}
\end{align}
Next, we note that $\mathbf{Q}$ is related to the matrix in brackets above via a \textit{generalized} similarity transformation defined by $\mathbf{B}$, Eq.\,(\ref{eq:Q}). In particular, $\mathbf{B}^T \mathbf{B} = \gr^{-\frac{1}{2}} \gr \gr^{-\frac{1}{2}} = \mathbf{I} \in \mathbb{R}^{K \times K}$, while we remark that $\mathbf{B}\mathbf{B}^T \neq \mathbf{I}$ since it is in $\mathbb{R}^{\infty \times \infty}$. This connection allow us to show that
\begin{align}
    \mathbf{Q} \mathbf{B} \gr^{\frac{1}{2}} \bm{r}^{(k)} = \mathbf{B} \left( \mathbf{I} + \frac{1}{\NS} \mathbf{R} \right)^{- 1}\!\!\! \mathbf{B}^T \mathbf{B} \gr^{\frac{1}{2}} \bm{r}^{(k)} = \frac{1}{1+\beta_k^2/S} \mathbf{B} \gr^{\frac{1}{2}} \bm{r}^{(k)}. 
    \label{eq:eiqQSol}
\end{align}
Comparing with Eq.\,(\ref{eq:eigQ}), we can now simply read off both the eigenvalues and eigenvectors of $\mathbf{Q}$,
\begin{align}
    \left. \begin{array}{rl}
        C_k & = \frac{1}{1+\beta_k^2/\NS}  \\
        \gh^{\frac{1}{2}}\mathbf{Y}^{(k)} & = \mathbf{B} \gr^{\frac{1}{2}} \bm{r}^{(k)} 
    \end{array} \right\}
    \implies \mathbf{Y}^{(k)} = \mathbf{T}^T \bm{r}^{(k)}
\end{align}
where we have used the definition of $\mathbf{B}$ from Eq.\,(\ref{eq:B}). The functions defined by the eigenvectors $\mathbf{Y}^{(k)}$ are automatically orthonormal:
\begin{equation}
    \left\langle y^{(k_1)}, y^{(k_2)} \right\rangle_{p} = \left( \gh^{\frac{1}{2}} \mathbf{Y}^{(k_1)} \right)^T\!\! \left( \gh^{\frac{1}{2}} \mathbf{Y}^{(k_2)} \right) = \boldsymbol{r}^{(k_1)T} \gr^{\frac{1}{2}} \mathbf{B}^T \mathbf{B} \gr^{\frac{1}{2}} \boldsymbol{r}^{(k_2)} = \boldsymbol{r}^{(k_1)T} \gr \boldsymbol{r}^{(k_2)} = \delta_{k_1 k_2}. 
\end{equation}

\subsection{Noisy eigentasks from readout features}
\label{sec:noisy_ET}

We can now also discuss the interpretation of $\{\beta_k^2\}$ for a physical system for which $\{\bm{r}^{(k)}\}$ are known. Consider the evaluation by the physical system (for a given $u$) under finite shots $S$, which yields a single instance of the readout features $\bar{\bm{X}}(u)$. We can simply construct a \textit{noisy} estimator of the $k$th eigentask, $\bar{y}^{(k)}(u)$
\begin{align}
    \bar{y}^{(k)} (u) = \sum_{k' = 0}^{K-1} r_{k'}^{(k)} \bar{X}_{k'} (u)
\end{align}
which is equivalent to requiring the output weights $\bm{W} = \bm{r}^{(k)}$.The corresponding set of noisy function is also orthogonal, this is because $\ci \bm{r}^{(k)} = \beta^2_k \gr \bm{r}^{(k)}$ implies $\bm{r}^{(k)T} \ci \bm{r}^{(k')} = \beta^2_k \delta_{k, k'}$ and hence
\begin{align}
    \Eu{\Es{\bar{y}^{(k_1)} \bar{y}^{(k_2)} }} = \boldsymbol{r}^{(k_1)T} \left( \gr +\frac{1}{S} \ci \right) \boldsymbol{r}^{(k_2)} = \left(1+\frac{\beta^2_k}{S}\right) \delta_{k_1 k_2} \label{eq:baryk1baryk2}
\end{align}
Let us define $\bar{y}^{(k)}(u) = y^{(k)}(u) + \xi^{(k)}(u)$. It means: for each $k$, the noisy eigentask $\bar{y}^{(k)}(u)$ contains a signal part $y^{(k)}(u)$ and a noise part in $\xi^{(k)}(u)$, where the latter one is computed from the linear combination $\xi^{(k)}(u) = \frac{1}{\sqrt{S}} \sum_{k=0}^{K-1} r^{(k)}_{k'} \zeta_{k'}(u)$. One can check $\Eu{\Es{y^{(k_1)} \xi^{(k_2)} }} = \Eu{\Es{\xi^{(k_1)} y^{(k_2)} }} =0$, and 
\begin{align}
    \Eu{ y^{(k_1)} y^{(k_2)} } & =
    % \left\langle y^{(k_1)}, y^{(k_2)} \right\rangle_{p} =
    \boldsymbol{r}^{(k_1)T} \gr \boldsymbol{r}^{(k_2)} = \delta_{k_1 k_2}, \\
    \Eu{\Es{\xi^{(k_1)} \xi^{(k_2)} }} & = 
    % \left\langle \xi^{(k_1)}, \xi^{(k_2)} \right\rangle_{p} = 
    \frac{1}{S} \boldsymbol{r}^{(k_1)T} \ci \boldsymbol{r}^{(k_2)} = \frac{\beta^2_{k_1}}{S} \delta_{k_1 k_2}. \label{eq:ortnoise}
\end{align}
It means that taking linear combinations of $\{x_k(u)\}$ and $\{\zeta_k(u)/\sqrt{S}\}$ with coefficients $\{\bm{r}^{(k)}\in\mathbb{R}^K\}_{k \in [K]}$, not only produces orthonormal eigentasks $\{ y^{(k)}(u) \}$ for signal, but also induces a set of orthogonal noise functions $\{ \xi^{(k)} (u) \}$.

If the physical system can be run multiple times for a given $S$, multiple instances of $\bar{\bm{X}}(u)$ can be obtained, from each of which an estimate of the $k$th eigentask $\bar{y}^{(k)}(u)$ can be constructed. The expectation value of these estimates then simply yields
\begin{align}
    \Es{\bar{y}^{(k)} (u)} = \sum_{k' = 0}^{K-1} r_{k'}^{(k)} \Es{\bar{X}_{k'} (u)} = \sum_{k' = 0}^{K-1} r_{k'}^{(k)} {x}_{k'} (u) = {y}^{(k)}(u)
\end{align}

If we have access to only a single instance of $\bar{\bm{X}}(u)$, however, and thus only one estimate $\bar{y}^{(k)}(u)$ (as $y^{(k)}(u)$ and $\bar{y}^{(k)}(u)$ depicted in Fig.\,\ref{fig:app_Features_and_Capacity}), it is useful to know the expected error in this estimate. This error can be extracted from Eq.\,(\ref{eq:Cf}). In particular, requiring $\mathbf{Y}^{(k)} = \mathbf{T}^T \bm{r}^{(k)}$, we have
\begin{align}
    % 1-C[y^{(k)}] = 
    &\frac{ \left\| \gh^{\frac{1}{2}} \mathbf{T}^T \bm{r}^{(k)} - \gh^{\frac{1}{2}} \mathbf{Y}^{(k)} \right\|^2 + \frac{1}{\NS} \bm{r}^{(k)T} \ci \bm{r}^{(k)} }{\mathbf{Y}^{(k)T} \gh \mathbf{Y}^{(k)}} = \frac{1}{S} \bm{r}^{(k)T} \ci \bm{r}^{(k)} = \frac{\beta_k^2}{S}. 
\end{align}
This mean squared error in using $\bar{y}^{(k)}(u)$ to estimate ${y}^{(k)}(u)$ over the domain of $u$ decreases to zero for $S\to\infty$ as expected, since the noise in $\bar{\bm{X}}$ decreases with $S$. However, $\beta_k^2$ defines the $S$-independent contribution to the error. In particular, this indicates that at a given $S$, certain functions with lower NSR eigenvalues $\beta_k^2$ may be better approximated using this physical system than others. We present in Fig.\,\ref{fig:app_Features_and_Capacity} the measured features $\bar{\bm{X}}$, the eigentasks $\bm{y}$ and their $S$-finite version $\bar{\bm{y}}$ in a 6-qubit Hamiltonian based system. The associated NSR spectrum, resolvable expressive capacity, and total correlations are also depicted for both CS ($J \neq 0$) and PS ($J = 0$) encodings. 



\subsection{$S$-shot resolvable expressive apacity: derivation of the bound}
\label{app:EC}
Given an arbitrary set of complete orthonormal basis functions $f_{\ell} (u) = \sum_{j = 0}^{\infty} (\mathbf{Y}_{\ell})_j u^j$,
\begin{equation}
     \langle f_{\ell}, f_{\ell'} \rangle_p = \left( \gh^{\frac{1}{2}} \mathbf{Y}_{\ell} \right)^T \left( \gh^{\frac{1}{2}} \mathbf{Y}_{\ell'} \right) = \delta_{\ell \ell'} . \label{eq:S-shot-eigentask}
\end{equation}
The total capacity is independent of the basis choice
\begin{align}
    \EC(\NS) & = \sum_{\ell = 0}^{\infty} C [f_{\ell}] = \sum_{\ell = 0}^{\infty} \mathbf{Y}_{\ell}^T \gh^{\frac{1}{2}} \left( \gh^{\frac{1}{2}} \mathbf{T}^T \left( \mathbf{T}\gh\mathbf{T}^T + \frac{1}{\NS} \ci \right)^{- 1} \!\!\! \mathbf{T}\gh^{\frac{1}{2}} \right) \gh^{\frac{1}{2}} \mathbf{Y}_{\ell} \nonumber\\
    & = \mathrm{Tr} \left( \gh^{\frac{1}{2}} \mathbf{T}^T \left( \mathbf{T}\gh\mathbf{T}^T + \frac{1}{\NS} \ci \right)^{- 1} \!\!\! \mathbf{T}\gh^{\frac{1}{2}} \right) = \mathrm{Tr} \left( \left( \gr + \frac{1}{\NS} \ci \right)^{- 1} \!\!\! \gr \right) = \sum_{k=0}^{K-1} \frac{1}{1 + \frac{\beta_k^2}{\NS}}. 
\end{align}

\subsection{Eigentask learning training procedure for a nonlinear postprocessing layer}
\label{app:ComplxNonLin}

The definition of REC metric and the training scheme underlying Eigentask Learning considered in the main text is based on a linear estimator $\bm{W} \cdot \bar{\bm{X}}$ fed into a quadratic loss function. Note that in an experimental context, their calculations are performed on a classical processor after the measurement results are collected. This is what is done in the experiments on the superconducting quantum processor in \Sec{sec:ibmq}. This choice of a linear estimator and a quadratic loss function may seem arbitrary, but the rationale behind it, as explained in the main text, is the desire to quantify the function expression capacity of solely the physical system itself, rather than the classical post-processing layer.   

The Eigentask Learning training methodology introduced in the present work is however sufficiently general to be adapted to non-linear post-processing scenarios as well, which is the subject of this Appendix. The central finding is that a cumulant expansion of the non-linear loss function produces extra regularization terms whose magnitudes can be characterized by the NSR spectra $\{\beta^2_k\}$. The non-linear training loss can then be well-approximated using a truncated set of eigentasks. To be more specific, we are going to demonstrate that those eigentasks $y^{(k)}$ whose corresponding $\beta^2_k/S$ is larger should \textit{qualitatively} contribute a larger penalty to the loss function. 

The most general case of the output layer can involve a nonlinear activation function or kernel which may subsequently be fed into a nonlinear loss function. A unified description of these two aspects can be achieved through the usage of a differentiable non-linear function $\sigma_{\mathrm{NL}}$ 
\begin{align}
    \mathscr{L} = \Eu{\Es{\sigma_{\mathrm{NL}}(\bar{\bm{X}})}}. \label{eq:L=EusigmaX}
\end{align} 
For our proof, we do not consider the most general form of $\sigma_{\mathrm{NL}}$, but employ some reasonable assumptions on it that allow us to qualitatively demonstrate the role of eigentasks in the presence of nonlinear post-processing. First, we assume that it is legitimate to truncate a series expansion of $\sigma_{\mathrm{NL}}$ to second order; that is, all terms of the third and higher order derivatives of $\sigma_{\mathrm{NL}}$ are assumed to be much smaller in comparison. Secondly, we assume that the second order derivative (namely the \textit{Hessian matrix}) $\nabla_{\bm{x}}\nabla_{\bm{x}}^T \sigma_{\mathrm{NL}}(u)$ does not vary too strongly with respect to $u$. For mean-square loss in Eq.\,(\ref{eq:fcap}), the Hessian matrix of $\nabla_{\bm{x}}\nabla_{\bm{x}}^T \sigma_{\mathrm{NL}}(u) = 2 \bm{W}\bm{W}^T$ is a constant matrix.
Another typical example of $\sigma_{\mathrm{NL}}$ is the cross-entropy loss function of logistic regression used in the toy binary classification problem considered in \Sec{sec:eigentasklearning}. Here the target function is the conditional probability distribution $f(u) := \mathrm{Prob}[u \in C_1 | \, u]$, where $C_1$ represents the class labeled by $1$. The eventual loss function contains a softmax layer and a cross-entropy function $\mathscr{L} = \Eu{\Es{\mathrm{H} (f(u), \sigma(\bm{W} \cdot \bar{\bm{X}}(u)))}}$ where $\sigma$ is sigmoid function (\textit{e.g.} softmax function $\sigma(z) = 1/(1+\mathrm{exp}(-z))$), and $\mathrm{H}(p,q) = - p \ln q - (1-p) \ln (1-q)$ is the cross-entropy. One can check that $\nabla_{\bm{x}}\nabla_{\bm{x}}^T \sigma_{\mathrm{NL}} = \sigma(\bm{W}\cdot \bm{x}) (1-\sigma(\bm{W}\cdot \bm{x})) \bm{W} \bm{W}^T$, where $\sigma(\bm{W}\cdot \bm{x}) (1-\sigma(\bm{W}\cdot \bm{x})) \in [0, 1/4]$ is a bounded function. 
% \rev{We will check that all second and higher order of $\sigma_{\mathrm{NL}} = \mathrm{H} (f, \sigma(\bm{W} \cdot \bm{x}))$ is independent of $f$. }

Suppose the eigentasks $\bar{\bm{y}}$ have been determined by solving the generalized eigenvalue problem \Eq{eq:eigenprob}. We proceed by expressing the non-linear loss function \Eq{eq:L=EusigmaX} in terms of $\bar{\bm{y}}$. This can be done by expressing $\bar{\bm{X}}$ in terms of $\bar{\bm{y}}$, $\bar{\bm{X}} = \bm{\Gamma} \bar{\bm{y}}$, with $\bm{\Gamma}^T = (\bm{r}^{(0)}, \cdots, \bm{r}^{(K-1)})^{-1}$. This is possible by virtue of $\{\bm{r}^{(k)}\}$ being the eigenvectors of the problem Eq.\,(\ref{eq:eigenproblem}). All noisy measured features $\{\bar{X}_k\}$ can now be expressed in terms of the orthogonal signal-basis $\{y^{(k)}\}$ and the noise-basis $\{\xi^{(k)}\}$
\begin{align}
    \bar{X}_{k'}(u) \equiv \sum_{k=0}^{K-1} \Gamma_{k'k} (y^{(k)}(u) + \xi^{(k)} (u)).
\end{align}
Using a cumulant expansion for the non-linear loss function and recalling that $\Es{\xi^{(k)}(u)} = 0$ and $\Es{\xi^{(k)} \xi^{(k')} }=\bm{r}^{(k)T} \mathbf{\Sigma} \bm{r}^{(k')}$ where $\mathbf{\Sigma}$ is the covariance of original sampling noise $\bm{\zeta}$,
\begin{align}
    \mathscr{L} & = \Eu{\Es{\sigma_{\mathrm{NL}} (\bar{\bm{X}})}} = \Eu{\Es{\sigma_{\mathrm{NL}} (\mathbf{\Gamma} \bar{\bm{y}})}} = \Eu{\Es{\sigma_{\mathrm{NL}} \! \left( \sum_k \Gamma_{0, k} (y^{(k)} + \xi^{(k)}), \cdots, \sum_k \Gamma_{K - 1, k} (y^{(k)} + \xi^{(k)}) \right) }} \nonumber\\
    & = \Eu{\sigma_{\mathrm{NL}} (\mathbf{\Gamma} \bm{y})} + \sum_{k = 0}^{K - 1} \Eu{\Es{\frac{\partial \sigma_{\mathrm{NL}}}{\partial y^{(k)}} \xi^{(k)} }} + \frac{1}{2} \sum_{k_1 = 0}^{K - 1} \sum_{k_2 = 0}^{K - 1} \Eu{\Es{\frac{\partial^2 \sigma_{\mathrm{NL}}}{\partial y^{(k_1)} \partial y^{(k_2)}} \xi^{(k_1)} \xi^{(k_2)} }} + \mathcal{O}\!\left(\frac{1}{S^2}\right) \nonumber\\
    & = \Eu{\sigma_{\mathrm{NL}} (\mathbf{\Gamma} \bm{y})} + \sum_{k = 0}^{K - 1} \Eu{\frac{\partial \sigma_{\mathrm{NL}}}{\partial y^{(k)}} \Es{\xi^{(k)}}} + \frac{1}{2} \sum_{k_1 = 0}^{K - 1} \sum_{k_2 = 0}^{K - 1} \Eu{\frac{\partial^2 \sigma_{\mathrm{NL}}}{\partial y^{(k_1)} \partial y^{(k_2)}} \Es{\xi^{(k_1)} \xi^{(k_2)}} } + \mathcal{O}\!\left(\frac{1}{S^2}\right) \nonumber\\
    & = \Eu{\sigma_{\mathrm{NL}} (\mathbf{\Gamma} \bm{y})} + \frac{1}{2} \sum_{k_1 = 0}^{K - 1} \sum_{k_2 = 0}^{K - 1} \Eu{ \frac{\partial^2 \sigma_{\mathrm{NL}}}{\partial y^{(k_1)} \partial y^{(k_2)}} \bm{r}^{(k_1)T} \mathbf{\Sigma} \bm{r}^{(k_2)} } + \mathcal{O}\!\left(\frac{1}{S^2}\right), \label{eq:Lapprox}
\end{align}
% Here the third line still uses Hoeffding inequality by replacing all $u$-dependent random variables with their $u$-dependent expectations. 
We see here that all terms including third and higher order derivatives of $\sigma_{\mathrm{NL}}$ are of $\mathcal{O}\!\left(\frac{1}{S^2}\right)$, which can be neglected in comparison to lower order terms for large enough $\NS$, in accordance with the discussion following Eq.\,(\ref{eq:L=EusigmaX}).
Secondly, the slow variation of the second order derivative $\nabla_{\bm{y}}\nabla_{\bm{y}}^T \sigma_{\mathrm{NL}}(u) = \mathbf{\Gamma}^T \nabla_{\bm{x}}\nabla_{\bm{x}}^T \sigma_{\mathrm{NL}}(u) \mathbf{\Gamma}$ with respect to $u$ allow us to make a further simplification of Eq.\,(\ref{eq:Lapprox}) by taking the mean-value approximation 
\begin{align}
    \Eu{ \frac{\partial^2 \sigma_{\mathrm{NL}}}{\partial y^{(k_1)} \partial y^{(k_2)}} \bm{r}^{(k_1)T} \mathbf{\Sigma} \bm{r}^{(k_2)} } \approx \Eu{ \frac{\partial^2 \sigma_{\mathrm{NL}}}{\partial y^{(k_1)} \partial y^{(k_2)}} } \Eu{\bm{r}^{(k_1)T} \mathbf{\Sigma} \bm{r}^{(k_2)} }
\end{align}
and using the equality $\Eu{\bm{r}^{(k_1)T} \mathbf{\Sigma} \bm{r}^{(k_2)}} = \delta_{k_1 k_2} \beta_{k_1}^2 / S$:
\begin{equation}
   \mathscr{L} \approx 
   \Eu{\sigma_{\mathrm{NL}} (\mathbf{\Gamma} \bm{y})} + \sum_{k = 0}^{K - 1} \frac{\beta_k^2}{S} \cdot \Eu{\frac{\partial^2 \sigma_{\mathrm{NL}}}{(\partial y^{(k)})^2} } =
   \Eu{\sigma_{\mathrm{NL}} (\mathbf{\Gamma} \bm{y})} + \sum_{k} \frac{\beta_k^2}{S} \cdot (\mathbf{\Gamma}^T \Eu{ \nabla_{\bm{x}}\nabla_{\bm{x}}^T \sigma_{\mathrm{NL}}} \mathbf{\Gamma})_{k k}. \label{eq:Lapprox-1}
\end{equation} 

In typical scenarios, such as for instance the case of logistic regression, the loss function depends on the trainable parameters $W_k$ through a linear combination $\bm{W} \cdot \bar{\bm{X}}(u)$. For such scenarios, it proves convenient to introduce $\bm{\Omega}$ such that $\bm{\Omega} = \bm{\Gamma}^T \bm{W}$
\begin{align}
    \bm{W} \cdot \bar{\bm{X}}(u) \equiv \sum_{k=0}^{K-1} \Omega_{k} \cdot (y^{(k)}(u) + \xi^{(k)} (u)) ,
\end{align}
For any loss function of the form $\sigma_{\mathrm{NL}}(\bm{x}) = g(\bm{W} \cdot \bm{x})$, the second term of Eq.\,(\ref{eq:Lapprox-1}) can then be expressed as $\mathbf{\Gamma}^T \nabla_{\bm{x}}\nabla_{\bm{x}}^T \sigma_{\mathrm{NL}} \mathbf{\Gamma} = g''(\bm{\Omega}\cdot \bm{y}) \mathbf{\Omega} \mathbf{\Omega}^T$. In the case of logistic regression $\sigma_{\mathrm{NL}}(\bm{x}) = g(\bm{W} \cdot \bm{x}) = \operatorname{H} (f, \sigma(\bm{W} \cdot \bm{x}))$. The final loss function can be further simplified to 
\begin{align}
    \mathscr{L} & \approx 
    \Eu{\operatorname{H} \! \left(f, \sigma\!\left(\bm{\Omega} \cdot \bm{y}\right)\right)} + \left(\sum_{k=0}^{K-1} \frac{\beta_k^2}{\NS} \Omega^2_{k}\right) \cdot \Eu{ \sigma(\bm{\Omega} \cdot \bm{y}) (1-\sigma(\bm{\Omega} \cdot \bm{y})) }. \label{eq:LapproxCls}
\end{align}
Here the first term is the $S$-infinity value of the loss function, $\lim_{S \to \infty} \mathscr{L} = \Eu{\operatorname{H} \! \left(f, \sigma\!\left(\bm{\Omega} \cdot \bm{y}\right)\right) }$. For the softmax function, we have $\sigma(\bm{\Omega} \cdot \bm{y}(u)) (1-\sigma(\bm{\Omega} \cdot \bm{y}(u)))\in[0,1/4]$, which is assumed not to fluctuate strongly as a function of $u$, compared to the fluctuation of $\bm{r}^{(k_1)T} \mathbf{\Sigma}(u) \bm{r}^{(k_2)}$. 


While the final result \Eq{eq:LapproxCls} is obtained under certain assumptions on $\sigma_{\mathrm{NL}}$ as detailed at the outset, the form of the last term suggests the interpretation of $\beta_k^2 / \NS$ as a natural regularization, and the use of $\beta_k^2 / \NS$ as a metric for truncation. This truncation is successfully employed in calculations behind the generation of plots in \Fig{fig:Genc2} and \Fig{fig:Genc3} for the binary classification task, using the cross-entropy loss function of logistic regression. \Eq{eq:LapproxCls} therefore provides some theoretical justification for the use of this truncation scheme when a nonlinear post-processing layer is employed.


%The approximation in the second term of Eq.\,(\ref{eq:LapproxCls}) is rough, but it still gives us a sufficient reason to do the following manipulation: for optimized $\mathscr{L}$, the dependence on $y^{(k)}$ with $\beta_k^2 / \NS>1$ will be strongly suppressed in large-$N$ limit, hence we can pre-exclude the eigentasks whose $\beta_k^2 / \NS>1$.
% We will leave the detailed discussion of this important application in Appendix \ref{app:PCA} and Appendix \ref{sec:Single_step_QRC}.

% \subsection{Proof that number of linearly independent features equals to rank of Gram matrix $\gr$} 
% \label{app:Function-independence}


%Furthermore, in quantum system, thanks to the linearity of readout, we can think of under what condition the rank of Gram matrix $\gr$ is not $K$. Suppose there exists coefficients $\{ c_k \}$ such that
%\begin{align}
%    \sum_{k = 0}^{K-1} c_k x_k (u) = \mathrm{Tr} \left\{ \left( \sum_{k = 0}^{K-1} c_k \hat{M}_k  \right) \mathcal{U} (u) \hat{\rho}_0 \right\} = 0. 
%\end{align}
%However, this means that the quantum observable $\sum_{k = 0}^{K-1} c_k \hat{M}_k$ is a zero-expectation readout-qubit quantity for any state $\mathcal{U} (u) \hat{\rho}_0$ under arbitrary input $u$. \rev{We restrict our discussion in the meaningful encoding system where it does not exist such conserved quantity for the chosen $\mathcal{U} (u) \hat{\rho}_0$ and POVM set $\{\hat{M}_k\}$. It is equivalent to assume that $\gr$ has no zero eigenvalues, namely $\operatorname{Rank}\{\gr\}=K$. }

%\rev{Last but not least, we must remark here that the rank of the $\ci$ matrix is quite different. It is easy to verify that for a $\ci$ associated with sampling from a quantum measurement (see Eq.\,(\ref{eq:defV})), all row-sum and all column-sum are zeros. Thus $\ci$ has a trivial eigenvector $(1,1,...,1)^T$ with eigenvalue $0$: even for a generic system with no symmetry at all, the matrix $\ci$ is still not of full rank, thus it is invalid to use the notion $\ci^{-1}$ in quantum sampling noise (see Eq.\,(\ref{eq:defD})). }
% ga{this zero eigenvector is only true for sampling noise correct? also should point to an eqn above where one can easily verify this} \fh{Only true for quantum noise. }

\subsection{Simplifying the noise-to-signal matrix and its eigenproblem for quantum systems}
\label{app:IntroD}

We have shown that the problem of obtaining the eigentasks for a generic quantum system, and deducing its resolvable expressive capacity under finite measurement resources, can be reduced simply to solving the eigenproblem of its NSR matrix $\mathbf{R}$, Eq.\,(\ref{eq:Reigen}). Note that constructing $\mathbf{R}=\gr^{-\frac{1}{2}}\mathbf{V}\gr^{-\frac{1}{2}}$ requires computing the inverse of $\gr$. However, $\gr$ can have small (although always nonzero) eigenvalues, especially for larger systems, rendering it ill-conditioned and making the computation of $\mathbf{R}$ numerically unstable. Fortunately, certain simplifications can be made to derive an equivalent eigenproblem that is much easier to solve. We begin by employing the fact that the second-order moment matrix $\mathbf{D}$ of multinomial sampling is diagonal.
% Practically, the probability representation is native to measurement schemes in contemporary quantum processors, and therefore minimizes the required post-processing of readout features obtained from a real device. More importantly, the strength of the probability representation lies in the fact that it renders the second-order moment matrix $\mathbf{D}$ diagonal. 
In particular,
\begin{align}
    (\mathbf{D})_{k_1k_2} = \left\{
    \begin{array}{cc}
        \sum_{k = 0}^{K-1} (\gr)_{k k_1}, &\text{if } k_1=k_2 \\
        0, & \text{if } k_1 \neq k_2
    \end{array} \right.
    % ~~~\text{(in~probability~representation~of~readout~features)}
\end{align}

Using $\mathbf{V} =\mathbf{D}-\gr$, we can rewrite the eigenproblem for $\mathbf{R}$,
\begin{align}
    \nsr \left( \gr^{\frac{1}{2}}\bm{r}^{(k)} \right) & = \beta_k^2 \gr^{\frac{1}{2}}\bm{r}^{(k)} \nonumber \\
    \implies \gr^{-\frac{1}{2}}(\mathbf{D}-\gr)\gr^{-\frac{1}{2}} \left( \gr^{\frac{1}{2}}\bm{r}^{(k)} \right) &= \beta_k^2 \gr^{\frac{1}{2}}\bm{r}^{(k)} \nonumber \\
    \implies \gr^{-1}\mathbf{D} \bm{r}^{(k)} &= (1+\beta_k^2) \bm{r}^{(k)}
\end{align}
Finally, considering the inverse of the matrix on the left hand side, we obtain the simplified eigenproblem for the matrix $\mathbf{D}^{-1}\gr$,
\begin{align}
    \mathbf{D}^{-1}\gr \bm{r}^{(k)} = (1+\beta_k^2)^{-1} \bm{r}^{(k)} \equiv \alpha_k \bm{r}^{(k)},
    \label{eq:DinvGeigen}
\end{align}
which shares eigenvectors with $\nsr$, and whose eigenvalues are a simple transformation of the NSR eigenvalues $\beta_k^2$. Importantly, constructing $\mathbf{D}^{-1}\gr$ no longer requires calculating any powers of $\gr$, and it relies only on the straightforward inversion of a diagonal matrix $\mathbf{D}$.

\begin{comment}
The matrix $\mathbf{D}^{-1}\gr$ also has significance in spectral graph theory, when interpreting the Gram matrix $\gr$ as the adjacency matrix of a weighted graph. This connection is elaborated upon in Appendix \ref{sec:graph}.


\subsection{Connections to spectral graph theory}
\label{sec:graph}

Let us have a small digression to the graphic theoretic meaning of $\gr$ and $\mathbf{\mathbf{D}}^{-1} \gr$. Now we consider a weighted graph with adjacency matrix $\gr$. In spectral graph theory, the matrix $\mathbf{\mathbf{D}}^{- 1} \gr$ is exactly the random walk matrix associated with graph $\gr$, and then the second order matrix $\mathbf{D}$ happens to be the \textit{degree matrix} of this graph since $(\mathbf{D})_{k k}= \sum_{k' = 0}^{K-1} (\mathbf{G})_{k k'}$. Then the eigentask combination coefficient $\boldsymbol{r}^{(k)}$ is precisely the right eigenvector of random walk matrix.  Another concept associated with a graph is $\mathbf{I} - \mathbf{D}^{-\frac{1}{2}} \gr \mathbf{D}^{-\frac{1}{2}}$, the \textit{normalized Laplacian matrix} of $\gr$, while the matrix $\mathbf{D}^{-\frac{1}{2}} \gr \mathbf{D}^{-\frac{1}{2}}$ is always referred to be \textit{normalized adjacency matrix} in many literatures. The eigenproblem of normalized adjacency matrix can also be solved easily, because 
\begin{align}
    \mathbf{D}^{-\frac{1}{2}} \gr \mathbf{D}^{-\frac{1}{2}} \left( \mathbf{D}^{\frac{1}{2}} \bm{r}^{(k)} \right) = \mathbf{D}^{\frac{1}{2}} \mathbf{D}^{-1} \gr \bm{r}^{(k)} = \alpha_k \left( \mathbf{D}^{\frac{1}{2}} \bm{r}^{(k)} \right).
\end{align}
From perspective of spectral graph theory, roughly speaking, a reservoir with stronger ability to resist noise are those who has more ``bottlenecks" in graph $\gr$'s connectivity. The extreme case is supposing that $\alpha_k=1$ (or $1-\alpha_k=0$) for all $k$. According the basic conclusion in spectral graph theory, the normalized Laplacian matrix has $K$ zero eigenvalues iff the graph $\gr$ is fully disconnected. This gives us the condition when noisy information capacity obtain its upper bound $K$: there exists a partition $\{ \mathrm{Dom}_k \}_{k \in [K]}$ of domain $\mathrm{Dom}=[-1,1]$ such that $\hat{\rho}_{kk}(u) = 1$ iff $u \in \mathrm{Dom}_k$.

\end{comment}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section S4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Spectral analysis based on finite statistics in quantum systems}
\label{app:Spectral_finite_statistics}

While Eq.\,(\ref{eq:DinvGeigen}) is a numerically simpler eigenproblem to solve than Eq.\,(\ref{eq:Reigen}), it still requires the approximation of $\gr$ (recall that $\mathbf{D}$ can be obtained from $\gr$) from readout features $\bar{\bm{X}}(u)$ under finite sampling of the input ($N$) and finite shots ($\NS$). To be more precise, in experiment one only has access to measured features sampled at finite-$S$ $\bar{\bm{X}}$ (indeed, this distinction is the underlying premise of this article).  However, in Eq.\,\eqref{eq:eigenprob} $\gr$ and $\ci$ are defined with respect to the ideal $\bm{x}$. Let $\widetilde{\gr} \equiv \EUI{\Es{\bar{\bm{X}}\bar{\bm{X}}^T}}$ and $\widetilde{\ci} \equiv \EUI{\Es{\mathrm{diag}(\bar{\bm{X}})-\bar{\bm{X}}\bar{\bm{X}}^T}}$. The objective of Appendix \ref{app:tilde_correction} is showing that the eigen-analysis $\{\beta^2_k, \bm{r}^{(k)}\}$ can be accurately expressed with
\begin{align}
    \beta_k^2 = \frac{S \cdot \tilde{\beta}_k^2}{(S-1)-\tilde{\beta}_k^2}, \label{eq:btilde}
\end{align}
and $\bm{r}^{(k)} = \tilde{\bm{r}}^{(k)}$ from solving generalized eigenvalue problem $\widetilde{\ci} \tilde{\bm{r}}^{(k)} = \tilde{\beta}_k^2 \widetilde{\gr} \tilde{\bm{r}}^{(k)}$. In what follows, we show how an approximation $\widetilde{\gr}_N$ of $\gr$ can be constructed from finitely-sampled readout features, as relevant for practical quantum devices. Secondly, we also describe an approach in Appendix \ref{sec:GramMatrix-free} to obtain the eigentasks $y^{(k)}(u)$ and corresponding NSR eigenvalues $\beta_k^2$ in a singlular-value decomposition (SVD) problem that avoids explicit construction of the Gram matrix, and is thus numerically even more robust. 


\subsection{Approximating eigentasks and NSR eigenvalues under finite $S$ and $N$}
\label{app:tilde_correction}

For practical computations, readout features $\bar{\bm{X}}(u)$ from the quantum system for finite $\NS$ can be computed for a discrete set of $u^{(n)} \in [-1,1]$ for $n = 1,\ldots,N$. Labelling the corresponding readout features $\bar{\bm{X}}(u^{(n)})$, we can define the \textit{regression matrix} constructed from these readout features,
\begin{equation}
    \regmat \equiv (\bar{\bm{X}} (u^{(1)}), \bar{\bm{X}} (u^{(2)}), \cdots, \bar{\bm{X}} (u^{(N)}))^T = \left(\begin{array}{ccc}
        \bar{X}_0 (u^{(1)}) & \cdots & \bar{X}_{K-1} (u^{(1)})\\
        \vdots &  & \vdots\\
        \bar{X}_0 (u^{(N)}) & \cdots & \bar{X}_{K-1} (u^{(N)})
    \end{array}\right) . \label{eq:regmat}
\end{equation}
Here, $\regmat \in \mathbb{R}^{N\times K}$, with subscript $N$ indicating its construction from a finite set of $N$ inputs, is a random matrix due to the stochasticity of readout features; in particular it can be written as:
\begin{align}
    \regmat = \RM  + \frac{1}{\sqrt{\NS}}\mathbf{Z}(\RM)
\end{align}
where $(\RM)_{nk} = \Es{\bar{X}_k(u^{(n)})} = x_k(u^{(n)})$, and $\mathbf{Z}$ is the centered multinomial stochastic process, so that $\Es{\widetilde{\mathbf{F}}_N} = \RM$.

Using this regression matrix $\regmat$, we can obtain an estimation of the Gram matrix and second order moment matrix, which we denote $\widetilde{\gr}_N$ and $\widetilde{\mathbf{D}}_N$, and whose matrix elements are defined via
\begin{align}
     (\widetilde{\gr}_N)_{k_1 k_2} & \equiv \frac{1}{N}\sum_{n=1}^N \bar{X}_{k_1}(u^{(n)})\bar{X}_{k_2}(u^{(n)}) = \frac{1}{N} (\regmat^T \regmat)_{k_1k_2} \approx \int \bar{X}_{k_1}(u)\bar{X}_{k_2}(u) p(u) \dd u, \\
     (\widetilde{\mathbf{D}}_N)_{k_1 k_2} & \equiv \delta_{k_1, k_2} \frac{1}{N}\sum_{n=1}^N \bar{X}_{k_1}(u^{(n)}) \approx \delta_{k_1, k_2} \int \bar{X}_{k_1}(u) p(u) \dd u.
\end{align}
While the quantities $\widetilde{\gr}_N$ and $\widetilde{\mathbf{D}}_N$ are computed from stochastic readout features, their stochastic contributions are suppressed in the large $N$ limit by the Hoeffding inequality for sums of bounded stochastic variables. 

In what follows, our goal is to prove that by solving the eigenproblem $\widetilde{\mathbf{D}}_N^{-1} \widetilde{\gr}_N \tilde{\bm{r}}_{N}^{(k)} = (1 + \tilde{\beta}_{N, k}^2)^{-1} \tilde{\bm{r}}_{N}^{(k)}$, the true NSR eigenvalue $\beta_k^2$ and eigentask coefficients $\bm{r}^{(k)}$ can be well approximated by $\NS \tilde{\beta}_{N,k}^2/(\NS - \tilde{\beta}_{N,k}^2 - 1)$ and $\tilde{\bm{r}}_{N}^{(k)}$, respectively. In particular, to achieve the goal stated above, the first step is still taking the $N \to \infty$ limit and defining the deterministic limit of $\widetilde{\gr}_N$ and $\widetilde{\mathbf{D}}_N$, according to Eq.\,(\ref{eq:2ndMat}), as
\begin{align}
    \widetilde{\gr} &\equiv \lim_{N \to \infty} \frac{1}{N} (\regmat^T \regmat) = \gr + \frac{1}{\NS} \ci = \gr + \frac{1}{\NS} (\mathbf{D}-\gr), \label{eq:Gtilde1}
    \\ \widetilde{\mathbf{D}} &\equiv \lim_{N \to \infty} \widetilde{\mathbf{D}}_N = \mathbf{D}. \label{eq:Dtilde1}
\end{align}
In a generic physical system, the covariance matrix $\ci$ can be reconstructed by computing the empirical covariance of measured features and employing the well-known rule of \textit{Bessel's correction}, $\Es{ \frac{1}{S} \sum_s (X^{(s)}_k (u) - \bar{X}_k (u)) (X^{(s)}_{k'} (u) - \bar{X}_{k'} (u)) } = \frac{S-1}{S} \mathbf{\Sigma} (u)$. In this way, $\mathbf{V}$ can be approximated from the whole record of single-shot random-valued features $\Xs(u) = \{ X_k^{(s)} (u) \}_{k \in [K], s \in [S]}$
\begin{equation}
    \lim_{N \rightarrow \infty} \frac{1}{N (S - 1)} \sum_{n = 1}^N \sum_{s = 1}^S (X^{(s)}_k (u^{(n)}) - \bar{X}_k (u^{(n)})) (X^{(s)}_{k'} (u^{(n)}) - \bar{X}_{k'} (u^{(n)})) = \mathbf{V}_{k k'}. \label{eq:Bessel}
\end{equation}
However, in the scenario of quantum sampling noise, the covariance matrix has a special structure. In particular, $\cu$ is not independent of the first order moments $\bm{x}(u)$. This allows us to invert Eq.\,(\ref{eq:Gtilde1}) and Eq.\,(\ref{eq:Dtilde1}) to express the Gram matrix $\gr$ and second-order moment matrix $\mathbf{D}$ in terms of the estimators $\widetilde{\mathbf{G}}$ and $\widetilde{\mathbf{D}}$ computed using a finite number of shots $\NS$, in a numerically cheap way without using full readout record of $\Xs(u) = \{ X_k^{(s)} (u) \}_{k \in [K], s \in [S]}$,
\begin{align}
    \gr &= \frac{\NS}{\NS-1}\widetilde{\mathbf{G}} - \frac{1}{\NS-1}\widetilde{\mathbf{D}},  \\
    \mathbf{D} &= \widetilde{\mathbf{D}}.
\end{align}
We see that to lowest order in $\frac{1}{\NS}$, $\gr \approx \widetilde{\gr}$ and $\mathbf{D} \approx \widetilde{\mathbf{D}}$, which is what one might expect naively. However, we clearly see that the estimation of $\gr$ can be improved by including a higher-order correction in $\frac{1}{\NS}$. This contribution arises due to the highly-correlated nature of noise and signal for quantum systems: we are able to estimate the noise matrix $\widetilde{\mathbf{G}}$ and $\widetilde{\mathbf{D}}$ using knowledge of the readout features, and correct for the contribution to $\widetilde{\gr}$ and $\widetilde{\mathbf{D}}$ that arises from this noise matrix. We will see that this contribution will be important in more accurately approximating quantities of interest derived from $\gr$, $\mathbf{D}$.

% Figure environment removed

To this end, we recall that our ultimate aim is not just to estimate $\gr$ and $\mathbf{D}$, but to solve the eigenproblem of Eq.\,(\ref{eq:DinvGeigen}). Using the above relation, we can then establish $\widetilde{\mathbf{D}}^{-1}\widetilde{\mathbf{G}} = \frac{\NS-1}{\NS}\mathbf{D}^{-1}\gr +\frac{1}{\NS} \mathbf{I}$, and write Eq.\,(\ref{eq:DinvGeigen}) in a form entirely in terms of $\widetilde{\mathbf{G}}$ and $\widetilde{\mathbf{D}}$,
\begin{align}
    \mathbf{D}^{-1}\gr \bm{r}^{(k)} &= (1+\beta_k^2)^{-1} \bm{r}^{(k)}, \nonumber \\
    \implies \widetilde{\mathbf{D}}^{-1}\widetilde{\mathbf{G}}\bm{r}^{(k)} &= \left[\frac{\NS-1}{\NS}(1+\beta_k^2)^{-1} +\frac{1}{\NS} \right]\bm{r}^{(k)}. 
\end{align}
Note that the final form is conveniently another eigenproblem, now for the finite-$\NS$ matrix $\widetilde{\mathbf{D}}^{-1}\widetilde{\mathbf{G}}$:
\begin{align}
    \widetilde{\mathbf{D}}^{-1}\widetilde{\mathbf{G}} \tilde{\bm{r}}^{(k)} = (1+\tilde{\beta}_k^2)^{-1} \tilde{\bm{r}}^{(k)} \equiv \tilde{\alpha}_k \tilde{\bm{r}}^{(k)},
    \label{eq:DinvGeigenFinS}
\end{align}
whose eigenvalues and eigenvectors can be easily related to the desired eigenvalues $\beta_k^2$ and eigenvectors $\bm{r}^{(k)}$ of Eq.\,(\ref{eq:DinvGeigen}). Following some algebra, we find:
\begin{align}
    \beta_k^2 &= \frac{S}{(\NS-1) -\tilde{\beta}_k^2}\cdot \tilde{\beta}_k^2 = \tilde{\beta}_k^2 +  
    \sum_{j=1}^{\infty} \tilde{\beta}_k^2 \left( 1+\tilde{\beta}_k^2 \right)^{j} \left( \frac{1}{S} \right)^j, \label{eq:betaEst} \\
    \bm{r}^{(k)} &= \tilde{\bm{r}}^{(k)}. 
    \label{eq:rEst}
\end{align}
From Eq.\,(\ref{eq:betaEst}), we see that to lowest order in $\frac{1}{\NS}$, $\beta_k^2 \approx \tilde{\beta}_k^2$. We note that $\tilde{\beta}_k^2$ are always smaller than $S-1$, and the zero-th order truncation of Taylor series expression for $\beta_k^2$ above is valid only for those $k$ satisfying $\tilde{\beta}_k^2 \ll \NS-1$ (see Fig.\,\ref{fig:NSR_Tilde_Renormalization}). However, this expression also supplies corrections to higher orders in $\frac{1}{S}$, which are non-negligible for $\tilde{\beta}_k^2 \lesssim S-1$, as we see in example of Fig.\,\ref{fig:NSR_Tilde_Renormalization}. In contrast, the estimated eigenvectors $\tilde{\bm{r}}^{(k)}$ to \textit{any} order in $\frac{1}{S}$ equal the desired eigenvectors ${\bm{r}}^{(k)}$ without any corrections.

% Figure environment removed

Of course, in practice we do not have access to the matrices $\widetilde{\gr}$ and $\widetilde{\mathbf{D}}$, as these are only defined precisely in the limit where $N\to\infty$. However, for sufficiently large $N$, we can approximate these matrices to lowest order by their finite $N$ values, $\widetilde{\gr} = \widetilde{\gr}_N + \mathcal{O}\left(\frac{1}{N}\right)$ and $\widetilde{\mathbf{D}} = \widetilde{\mathbf{D}}_N + \mathcal{O}\left(\frac{1}{N}\right)$. 
Then, the eigenproblem in Eq.\,(\ref{eq:DinvGeigenFinS}) can be expressed in the final form,
\begin{align}
    \widetilde{\mathbf{D}}_N^{-1} \widetilde{\gr}_N \tilde{\bm{r}}_{N}^{(k)} = (1 + \tilde{\beta}_{N, k}^2)^{-1} \tilde{\bm{r}}_{N}^{(k)} \equiv \tilde{\alpha}_{N,k} \tilde{\bm{r}}_{N}^{(k)}, 
    \label{eq:DinvGeigenApprox}
\end{align}
where the eigenvalues $\tilde{\beta}_{N,k}^2, \tilde{\alpha}_{N,k}$ and eigenvectors $\tilde{\bm{r}}_{N}^{(k)}$ in the large $N$ limit must satisfy
\begin{align}
    \lim_{N \to \infty} \tilde{\beta}_{N,k}^2 = \tilde{\beta}_{k}^2, \quad \lim_{N \to \infty} \tilde{\alpha}_{N,k} = \tilde{\alpha}_k, \quad 
    \lim_{N \to \infty} \tilde{\bm{r}}_{N}^{(k)} = \tilde{\bm{r}}^{(k)} \equiv \bm{r}^{(k)}. 
\end{align}
Here the invertibility of the empirically-computed matrix $\widetilde{\mathbf{D}}_N$ required for Eq.\,(\ref{eq:DinvGeigenApprox}) is numerically checked, based on which we can establish a better numerical method in Appendix \ref{sec:GramMatrix-free}. 

Eq.\,(\ref{eq:DinvGeigenApprox}) represents the eigenproblem whose eigenvalues $\tilde{\beta}_{N,k}^2$ and eigenvectors $\tilde{\bm{r}}_{N}^{(k)}$ we actually calculate. For large enough $N$ and under finite $\NS$, we can use these as valid approximations to the eigenvalues and eigenvectors of Eq.\,(\ref{eq:DinvGeigenFinS}). According to using Eqs.\,(\ref{eq:betaEst}), (\ref{eq:rEst}), we are finally able to directly estimate the $N,\NS \to \infty$ quantities $\beta_k^2$ and $\bm{r}^{(k)}$ by the following two quantities:
\begin{align}
    \bar{\beta}_k^2 &\equiv \frac{\NS \cdot \tilde{\beta}_{N,k}^2}{(\NS - 1) - \tilde{\beta}_{N,k}^2} = \frac{1 - \tilde{\alpha}_{N,k}}{\tilde{\alpha}_{N,k} - \frac{1}{\NS}}, \label{eq:barbeta2k} \\
    \bar{\bm{r}}^{(k)} &\equiv \tilde{\bm{r}}_N^{(k)}. \label{eq:barrk} 
\end{align}
It is clear that the approximation of $\beta_k^2$ to lowest order will be an underestimate, as the contribution of order $\frac{1}{\NS}$ is positive. 
In Fig.\,\ref{fig:NSR_Tilde_Renormalization}, we demonstrate a match between $\bar{\beta}_k^2 = (\NS \cdot \tilde{\beta}_{N,k}^2)/((\NS - 1) - \tilde{\beta}_{N,k}^2)$ and $\beta_k^2$ for a wide range of eigentasks with lower order $k$.
In Fig.\,\ref{fig:Tilde_Renormalization}, we plot the estimated eigenvectors $\tilde{\bm{r}}_N^{(k)}$ computed under finite statistics ($N=300,S=1000$, where these two numbers are relevant for IBM quantum processors) in H-encoding, together with the $N,\NS \to \infty$ eigenvectors ${\bm{r}}^{(k)}$, and the estimated eigenvalues.


\subsection{Gram matrix-free construction to approximate eigentasks and NSR eigenvalues}
\label{sec:GramMatrix-free}

% In large $N$ limit, we have approximation
% \begin{align}
%     \widetilde{\mathbf{D}}_N \to \mathbf{D}, \quad \text{and} \quad \frac{1}{N} \regmat^T \regmat \to \widetilde{\gr} = \gr + \frac{1}{S} \ci. 
% \end{align}

If we consider Eq.\,(\ref{eq:DinvGeigenApprox}) and multiply through by $\mathbf{D}_N^{-\frac{1}{2}}$, the resulting equation can be written as an equivalent eigenproblem, 
\begin{align}
    \frac{1}{N} \widetilde{\mathbf{D}}_N^{-\frac{1}{2}} \regmat^T \regmat \widetilde{\mathbf{D}}_N^{-\frac{1}{2}} \left(\widetilde{\mathbf{D}}_N^{\frac{1}{2}} \tilde{\bm{r}}_N^{(k)}\right) = \tilde{\alpha}_{N,k}\left(\widetilde{\mathbf{D}}_N^{\frac{1}{2}} \tilde{\bm{r}}_{N}^{(k)}\right) %\approx \mathbf{D}^{-\frac{1}{2}} \widetilde{\gr} \mathbf{D}^{-\frac{1}{2}} \left(\mathbf{D}^{\frac{1}{2}} \bm{r}^{(k)}\right)  = \tilde{\alpha}_k \left(\mathbf{D}^{\frac{1}{2}} \bm{r}^{(k)}\right),
\end{align}
where we have also written $\widetilde{\gr}_N = \frac{1}{N}\widetilde{\mathbf{F}}_N^T\widetilde{\mathbf{F}}_N$ as in the previous section. Note that as written above, the eigenproblem is entirely equivalent to obtaining the singular value decomposition of the matrix $\frac{1}{\sqrt{N}} \widetilde{\mathbf{D}}_N^{-\frac{1}{2}} \regmat^T = U \Sigma V^T$, where $U\in \mathbb{R}^{K\times K}$ and $V\in \mathbb{R}^{N\times N}$ are unitary matrix, and $\Sigma$ is a non-negative diagonal matrix with non-increasing diagonal entries: 
\begin{align}
    \Sigma = \mathrm{diag}(\tilde{\alpha}_{N,0}^{\frac{1}{2}}, \cdots, \tilde{\alpha}_{N, K-1}^{\frac{1}{2}}) \approx \mathrm{diag}(\tilde{\alpha}_0^{\frac{1}{2}}, \cdots, \tilde{\alpha}_{K-1}^{\frac{1}{2}}).
\end{align}
To obtain the estimation of combination coefficients $\bm{r}^{(k)}$, let $\bar{\bm{t}}^{(k)} \in \mathbb{R}^K$ be the normalized left singular vector of $\frac{1}{\sqrt{N}} \widetilde{\mathbf{D}}_N^{-\frac{1}{2}} \regmat^T$ (which is also the eigenvector of $\frac{1}{N} \widetilde{\mathbf{D}}_N^{-\frac{1}{2}} \regmat^T \regmat \widetilde{\mathbf{D}}_N^{-\frac{1}{2}} \approx \mathbf{D}^{-\frac{1}{2}} \widetilde{\gr} \mathbf{D}^{-\frac{1}{2}}$ in the large $N$ limit). Then $\bm{r}^{(k)} \approx \tilde{\alpha}^{-\frac{1}{2}}_{N,k} \widetilde{\mathbf{D}}_N^{-\frac{1}{2}} \bar{\bm{t}}^{(k)} = \bar{\bm{r}}^{(k)} \in \mathbb{R}^K$, and 
\begin{align}
    U = (\bar{\bm{t}}^{(0)}, \cdots, \bar{\bm{t}}^{(K-1)}) = \widetilde{\mathbf{D}}_N^{\frac{1}{2}} ( \tilde{\alpha}^{\frac{1}{2}}_{N,0} \bar{\bm{r}}^{(0)}, \cdots, \tilde{\alpha}^{\frac{1}{2}}_{N,K-1} \bar{\bm{r}}^{(K-1)}). 
\end{align}
Here $\bar{\bm{r}}^{(k)} = \tilde{\alpha}^{-\frac{1}{2}}_{N,k} \widetilde{\mathbf{D}}_N^{-\frac{1}{2}} \bar{\bm{t}}^{(k)}$ can be treated as the combination prefactor of $\hat{M}_k$, to obtain the observables which correspond to the eigentasks. The merit of an SVD analysis of $\frac{1}{\sqrt{N}} \widetilde{\mathbf{D}}_N^{-\frac{1}{2}} \regmat^T$ is that we only need to work with a $K$-by-$N$ matrix of features $\regmat$, which is numerically cheaper than further constructing a Gram matrix $\frac{1}{N} \regmat^T \regmat$. Therefore,
\begin{align}
    \Sigma V^T = U^T \frac{1}{\sqrt{N}} \widetilde{\mathbf{D}}_N^{-\frac{1}{2}} \regmat^T = \frac{1}{\sqrt{N}} \left(
    \begin{array}{c}
        \tilde{\alpha}^{\frac{1}{2}}_{N,0} \bar{\bm{r}}^{(0)T}\\
        \vdots\\
        \tilde{\alpha}^{\frac{1}{2}}_{N,K-1} \bar{\bm{r}}^{(K-1)T}
    \end{array}\right) \widetilde{\mathbf{D}}_N^{\frac{1}{2}} \widetilde{\mathbf{D}}_N^{-\frac{1}{2}} \regmat^T = \frac{1}{\sqrt{N}} \left(
    \begin{array}{c}
        \tilde{\alpha}^{\frac{1}{2}}_{N,0} \bar{\bm{r}}^{(0)T}\\
        \vdots\\
        \tilde{\alpha}^{\frac{1}{2}}_{N,K-1} \bar{\bm{r}}^{(K-1)T}
    \end{array}\right) \regmat^T \label{eq:SigmaVT}
\end{align}
The entries of Eq.\,(\ref{eq:SigmaVT}) are 
\begin{align}
    (\Sigma V^T)_{k,n} = \frac{1}{\sqrt{N}} \tilde{\alpha}^{\frac{1}{2}}_{N,k} \sum_{n=1}^{N} \bar{r}^{(k)}_{k'} (\regmat)_{n,k'} = \frac{1}{\sqrt{N}} \tilde{\alpha}^{\frac{1}{2}}_{N,k} \sum_{n=1}^{N} \bar{r}^{(k)}_{k'} \bar{X}_{k'}(u^{(n)}) \propto \bar{y}^{(k)}(u^{(n)}).
\end{align}
This means that for each data sample $u^{(n)}$, the value of the $k$th order eigentask $\bar{y}^{(k)}(u^{(n)})$ is exactly the principal component coordinate of $\frac{1}{\sqrt{N}} \widetilde{\mathbf{D}}_N^{-\frac{1}{2}} \regmat^T$, up to a constant factor.

The appearance of the SVD above brings comparisons to a popular, powerful data-compressing tool: \textit{principal component analysis}, or PCA, which is used to project a relatively high-dimensional data set into a smaller space, without losing much information. In standard PCA, the original data set is cast into a feature matrix $F \in \mathbb{R}^{N \times K}$, representing $N$ data samples and $K$ features. Let $\mu_k = \frac{1}{N} \sum_n F_{n,k}$ and $\sigma^2_k = \frac{1}{N} \sum_n (F_{n,k}-\mu_k)^2$. Then the standard-scored (or the z-scored) matrix $F'$ is defined by 
\begin{align}
    F'_{n,k} = \frac{F_{n,k} - \mu_{k}}{\sigma_k}. \label{z-score}
\end{align}
The SVD of $F'^{T} = U \Sigma V^T$ gives the well-known principal component analysis. Each row of $\Sigma V^T \in \mathbb{R}^{K \times N}$ is called a \textit{principal component} of the standard score data set $F'$. To be more specific, for each data sample labeled by $n$, the coordinate of its $k$-th principal component is $(\Sigma V^T)_{k,n}$.

While classical PCA focuses on how to \textit{reconstruct} a data set with minimal representative features, REC analysis focuses on \textit{minimizing} the effect from quantum sampling noise. 
%In the particular normalization factor $\frac{1}{\sqrt{N}} \widetilde{\mathbf{D}}_N^{-\frac{1}{2}}$ of REC analysis in quantum system, is different from the standard score of principal components analysis. 
Since the aim of REC analysis is about the affect from quantum sampling noise, the normalization factor in  REC is taking the reciprocal of $\sqrt{N}(\widetilde{\mathbf{D}}_N^{\frac{1}{2}})_{kk} = (\sum_n \bar{X}_{k}(u^{(n)}))^{1/2}$, which is quite different from usual PCA, where one uses the reciprocal of the standard deviation $\sigma_k$ of each feature over the whole data set samples for normalization, as is described by the form of Eq.\,(\ref{z-score}). 

% \rev{\subsection{Relation between REC analysis and principal component analysis}
% \textit{Principal component analysis}, or PCA, is a powerful data-compressing tool to project a relatively large data set into a smaller space, without losing to much information. In classical PCA, the orignal data set is in a feature matrix $F \in \mathbb{R}^{N \times K}$, representing $N$ data samples and $K$ features. Let $\mu_k = \frac{1}{N} \sum_n F_{n,k}$ and $\sigma^2_k = \frac{1}{N} \sum_n (F_{n,k}-\mu_k)^2$. Then the standard-scored (or the z-scored) matrix $F'$ is defined by 
% \begin{align}
%     F'_{n,k} = \frac{F_{n,k} - \mu_{k}}{\sigma_k}. \label{z-score}
% \end{align}
% The SVD of $F'^{T} = U \Sigma V^T$ gives the well-known principal component analysis. Each row of $\Sigma V^T \in \mathbb{R}^{K \times N}$ is called a \textit{principal component} of the standard score data set $F'$. To be more specific, for each data sample labeled by $n$, the coordinate of its $k$-th principal component is $(\Sigma V^T)_{k,n}$. }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section S5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{H-ansatz quantum systems: NSR spectra, resolvable expressive capacity, and eigentasks}
\label{app:H-ansztz}

In this section, we evaluate the REC for quantum systems described by the H-ansatz introduced in Appendix \ref{DetailsEncodings}, as an example of how REC can be efficiently computed for a variety of general quantum systems, and is not just restricted to parameterized quantum circuits.  The results of the analysis are compiled in Fig.\,\ref{fig:app_Features_and_Capacity}, and discussed below.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Figure environment removed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Fig.\,\ref{fig:app_Features_and_Capacity}(a) presents the set of features $\{\bar{X}_k(u)\}$ for typical $L=6$ qubit CS and PS at $\NS=1000$ with randomly chosen parameters (referred to as encodings, see caption). The resultant noisy eigentasks $\{\bar{y}^{(k)}(u)\}$ and NSR spectra $\{\beta_k^2\}$ extracted via the eigenvalue analysis are shown in  Figs.\,\ref{fig:app_Features_and_Capacity}(b) and \ref{fig:app_Features_and_Capacity}(c) respectively. In the side-by-side comparison in Fig.\,\ref{fig:app_Features_and_Capacity}(b), we clearly see the $J=0$ ansatz transitioning to a regime with more noise at much lower $k$ than the $J \neq 0$ ansatz. This is reflected in Fig.\,\ref{fig:app_Features_and_Capacity}(c), the $\beta_k^2$ spectrum, having a much flatter slope for larger $k$ (note the plot is semilog). Finally, Fig.\,\ref{fig:app_Features_and_Capacity}(d) shows the REC of both systems as a function of $\NS$. REC rapidly rises for small $S$ for both systems, but the rise of the $J=0$ system is steeper. After a certain threshold in $S$, however, the CS grows more rapidly, approaching the upper bound $2^6=64$ with $\NS \sim 10^8$; in contrast, the PS has a significantly lower $\EC$.

% For $J \to \infty$ we also have $\bar{\mathcal{T}}=0$ because $\hat{\rho}_0 = \ket{0}\!\bra{0}^{\otimes L}$ is an eigenstate of the encoding ($\hat{\rho}(u)=\hat{\rho}_0$). This implies there must be a peak at some intermediate $J$, which for both REC and ETC occurs when the coupling is proportional to the transverse field $J \sim h^x$. 

Just like the case of parameterized quantum circuits considered in the main text, we also explore how the REC $\EC$ changes with $J$ for the H-ansatz, and compare it to the total correlation ETC $\bar{\mathcal{T}}$,  as shown in Fig.\,\ref{fig:app_Features_and_Capacity}(f). For $J \to 0$ we have a PS with $\bar{\mathcal{T}}=0$, whereas in the $J \to \infty$ we also have $\bar{\mathcal{T}}=0$ because $\hat{\rho}_0 = \ket{0}\!\bra{0}^{\otimes L}$ is an eigenstate of the encoding ($\hat{\rho}(u)=\hat{\rho}_0$).  This implies there must be a peak at some intermediate $J$, which for both REC and ETC occurs when the coupling is proportional to the transverse field $J \sim h^x$.  At finite $S$, increased ETC is directly related to a higher REC.

Another interesting aspect is the clear trend seen in the maximization of REC around $J \sim h^x_{\mathrm{rms}}$ for various $h^x_{\mathrm{rms}}$, possibly hinting at the role of increased correlation around the MBL phase transition in random spin systems~\cite{martinez2021dynamical}. 
This trend is consistent with results in quantum metrology -- in general, the SNR obtained from averaging $L$ uncorrelated probes scales as $1/\sqrt{L}$. This scaling can become favorable in the presence of quantum correlation and other non-classical correlations, in which case the scaling of the SNR can show up as a quadratic improvement $1/L$~\cite{Giovannetti2006}. 
For even larger $J$, we find that $\hat{\rho}(u)\to\hat{\rho}_0 = \ket{0}\!\bra{0}^{\otimes L}$, which clearly reduces $\ETC$, but also $\EC$ as the quantum system state becomes $u$-independent.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section S6
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\section{Scaling with quantum system size}
\label{app:syssize}

An important question in quantum machine learning applications is the possible advantage of using larger quantum systems for information processing. In this section, we present preliminary results of scaling with quantum system size. The left panel of Fig.\,\ref{fig:Qubit_Scaling} shows REC vs $L$ at select $S$ values for H-ansatz, while the right panel shows two encodings in the C-ansatz device, as well as their noisy simulations. In both plots, the dashed line indicates the $S\to\infty$ result $\EC=2^L$. We see that the REC increases when adding more qubits into the Ising chain for the H-ansatz, or when increasing the number of circuit qubits $L$ for the C-ansatz. Note, however, that at any finite $S$ the noise-constrained REC falls off the exponential bound for $\NS\to\infty$. The dropoff is particularly severe for the IBMQ device, where we are limited to just $S \sim 10^4$, which significantly suppresses the REC even for $L=7$ qubits. Note, however, that even if one is well below $\EC=2^L$ due to this finite sampling constraint, increasing the dimension of the quantum system is always an effective way to increase the REC, particularly when compared to the logarithmic growth with $S$ of Fig.\,\ref{fig:Genc1} of Main Text. 

% Figure environment removed

\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section S7
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Analytic solution to the quantum 2-design resolvable expressive capacity}
\label{app:2designsol}

% \ga{need to edit for grammar still}

There are many system-specific factors that can contribute to the scaling of resolvable expressive capacity with system size, making it challenging to create a general model that describes all systems.
%The scaling of resolvable expressive capacity with system size in general is hard to quantify, and is likely best approached with a numerical or experimental study for a specific quantum model. 
However, we can analytically solve for the REC of a class of quantum models for a specific system: \textit{$2$-design parametric quantum circuits} $\{ p(\bm{u}) \dd \bm{u}, \hat{U} (\bm{\theta} ; \bm{u}) \}$.  We clarify that we are referring here to systems with specific parameters $\bm{\theta}$ which result in 2-designs with respect to the input distribution $p(\bm{u})$; the ensemble average is taken with respect to  inputs $u$.  Quantum literature \cite{Holmes2022}  often refers to general ans\"atze which form $2$-designs with respect to parameters $\bm{\theta}$ instead, which is not what we are considering here.

To be more specific, an ensemble $\{ p(\bm{u}) \dd \bm{u}, \hat{U} (\bm{\theta} ; \bm{u}) \}$ is a $2$-design if the following two quantum channels, defined on \textit{any} $2L$-qubit state $\hat{\tau}$ are equal
% as opposed to parameters $\bm{\theta}$.  Ansatze which form a $2$-design with respect to parameter variations are frequently studied in QML liturature
% Even though qubit scaling in general is hard to quantify. But it turns out we can analytically solve the REC for any a \textit{$2$-design parametric quantum circuit} $\{ p(\bm{u}) \dd \bm{u}, \hat{U} (\bm{\theta} ; \bm{u}) \}$. Here we emphasize that we fix the parameters $\bm{\theta}$, and the ensemble average is taken with respect to the randomness of inputs $\UI$ instead of the parameters $\bm{\theta}$. This is different from most parametric quantum circuit literature where there is no concept of inputs $\UI$. Let $\hat{\rho}_0 = \ket{\bm{b}_0} \bra{\bm{b}_0}$ be the initial state. By definition of $2$-design
\begin{equation}
    \mathcal{C} (\hat{\tau}) = \int \hat{U} (\bm{\theta} ; \bm{u})^{\otimes 2} \hat{\tau} (\hat{U} (\bm{\theta} ; \bm{u})^{\dagger})^{\otimes 2} p (\bm{u}) \dd \bm{u} = \int \hat{U}^{\otimes 2} \hat{\tau} (\hat{U}^{\dagger})^{\otimes 2} \dd \mu_H (\hat{U}) .
\end{equation}
where $\mu_{H}$ is the uniform (Haar) measure. We can verify that all information in the Gram matrix is explicitly contained in the elements of $\mathcal{C} (\hat{\rho}_0 \otimes \hat{\rho}_0)$. To be more specific,
\begin{align}
    & \bra{\bm{b}_{k_1}, \bm{b}_{k_2}} \mathcal{C} (\hat{\rho}_0 \otimes \hat{\rho}_0) \ket{\bm{b}_{k_1}, \bm{b}_{k_2}} \nonumber\\
    =~& \bra{\bm{b}_{k_1}, \bm{b}_{k_2}} \left( \int (\hat{U} (\bm{\theta} ; \bm{u}) \otimes \hat{U} (\bm{\theta} ; \bm{u})) \ket{\bm{b}_0, \bm{b}_0} \bra{\bm{b}_0, \bm{b}_0} (\hat{U} (\bm{\theta} ; \bm{u})^{\dagger} \otimes \hat{U} (\bm{\theta} ; \bm{u})^{\dagger}) p (\bm{u}) \dd \bm{u} \right) \ket{\bm{b}_{k_1}, \bm{b}_{k_2}} \nonumber\\
    % =~& \int \left( \bra{\bm{b}_{k_1}} U (\bm{\theta} ; \bm{u}) \ket{\bm{b}_0} \bra{\bm{b}_{k_2}} U (\bm{\theta} ; \bm{u}) \ket{\bm{b}_0} \right) \cdot \left( \bra{\bm{b}_0} U (\bm{\theta} ; \bm{u})^{\dagger} \ket{\bm{b}_{k_1}} \bra{\bm{b}_0} U (\bm{\theta} ; \bm{u})^{\dagger} \ket{\bm{b}_{k_2}} \right) p (\bm{u}) \dd \bm{u} \nonumber\\
    =~& \int \left| \bra{\bm{b}_{k_1}} \hat{U} (\bm{\theta} ; \bm{u}) \ket{\bm{b}_0} \right|^2 \cdot \left| \bra{\bm{b}_{k_2}} \hat{U} (\bm{\theta} ; \bm{u}) \ket{\bm{b}_0} \right|^2 p (\bm{u}) \dd \bm{u} \nonumber\\
    =~& \int x_{k_1} (\bm{u}) x_{k_2} (\bm{u}) p (\bm{u}) \dd \bm{u} = (\mathbf{G})_{k_1 k_2} . 
\end{align}
However, $\mathcal{C} (\hat{\rho}_0 \otimes \hat{\rho}_0) = \int U^{\otimes 2} (\hat{\rho}_0 \otimes \hat{\rho}_0) (U^{\dagger})^{\otimes 2} \dd \mu_H (U)$ implies that we can compute the Gram matrix by instead integrating over the Haar measure \cite{Puchaa2017}:
\begin{align}
  (\mathbf{G})_{k_1 k_2} = \int | U_{0, k_1} |^2 | U_{0, k_2} |^2 \dd \mu_H (U) = \left\{\begin{array}{ll}
    \frac{2}{K (K + 1)}, & \text{if } k_1 = k_2,\\
    \frac{1}{K (K + 1)}, & \text{if } k_1 \neq k_2 .
  \end{array}\right. 
\end{align}
Then the corresponding second-order moment matrix $\mathbf{D}$ is given by
\begin{equation}
    (\mathbf{D})_{k k} = \frac{2}{K (K + 1)} + (K - 1) \times \frac{1}{K (K + 1)} = \frac{1}{K} .
\end{equation}
It is self-consistent that the matrix $\mathbf{D}= \mathrm{diag} \left( \frac{1}{K}, \frac{1}{K}, \cdots, \frac{1}{K} \right)$ obeys the normalization condition $\mathrm{Tr} (\mathbf{D}) = K \cdot \frac{1}{K} = 1$. Then we can solve the eigenvalues $\{ \alpha_k \}_{k \in [K]}$ of random walk matrix $(\mathbf{D}^{- 1} \mathbf{G})_{k_1 k_2} = \frac{1}{K + 1} (1+\delta_{k_1 k_2})$. It gives
\begin{equation}
     \alpha_k = \frac{1}{K + 1}(1 + K \delta_{k0}).
\end{equation}
Furthermore, we use $\alpha_k = \frac{1}{1 + \beta_k^2}$ or $\beta^2_k = \frac{1}{\alpha_k} - 1$ to compute the NSR eigenvalue:
\begin{equation}
    (\beta^2_0, \beta^2_1, \beta^2_2, \cdots, \beta^2_{K - 2}, \beta^2_{K - 1}) = (0, K, K, \cdots, K, K) .
\end{equation}
Then the resolvable expressive capacity of any 2-design system is given by
\begin{equation}
    C_T = 1 + \frac{K - 1}{1 + \frac{K}{S}} = K \times \frac{1 + \frac{1}{S}}{1 + \frac{K}{S}} = 2^L \times \frac{S + 1}{S + 2^L} .
\end{equation}
% or equivalently
% \begin{equation}
%     L = \log_2 \left( \frac{C_T \cdot S}{S - (C_T - 1)} \right).
% \end{equation}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section S8
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Quantum correlation metrics}
\label{app:QCM}

There is no one standard metric to quantify correlation in a many-body state. The metric we would like to utilize here, the \textit{quantum total correlation}, is a quantity inspired by the classical total correlation of $L$ random variables $(b_1, \cdots, b_L)$, that is $\sum_{l = 1}^L \mathrm{H} (b_l) - \mathrm{H} (b_1 , \cdots, b_L )$. Using the chain rule of Shannon entropy $\mathrm{H} (b_1, b_2, \cdots, b_L) = \mathrm{H} (b_1) + \mathrm{H} (b_2 | b_1 ) + \cdots + \mathrm{H} (b_L | b_1, b_2, \cdots, b_{L - 1} )$
\begin{align}
    & \sum_{l = 2}^L \mathrm{H} (b_l) - \mathrm{H} (b_1, b_2, \cdots, b_L) = \sum_{l = 1}^L \mathrm{H} (b_l) - \sum_{l = 1}^L \mathrm{H} (b_l | b_1, b_2, \cdots, b_{l - 1} ) = \sum_{l = 2}^L \mathrm{I}(b_1, \cdots, b_{l - 1} ; b_l) \in [0, L-1], 
\end{align}
we can see that the classical total correlation tells us how a set of random variables reveals information about each other. Similarly, the quantum total correlation can be defined as \cite{Vedral2002, Modi2010}
\begin{align}
    \mathcal{T} ( \hat{\rho} ) = \sum_{l = 1}^L \mathrm{S} ( \hat{\rho}_l ) - \mathrm{S} ( \hat{\rho} )
\end{align}
where $\mathrm{S}$ is von Neumann entropy and $\hat{\rho}_l := \mathrm{Tr}_{[L] \backslash \{ l \}} \left\{ \hat{\rho} \right\} $ is the subsystem state at qubit $l$. Due to the subadditivity of von-Neumann entropy $\sum_{l = 1}^L \mathrm{S} ( \hat{\rho}_l )  \geq \mathrm{S} ( \hat{\rho} )$, the quantum total correlation is non-negative, and is zero iff the state $\hat{\rho} = \bigotimes_{l=1}^{L} \hat{\rho}_l$ is a product state. 

In this paper's measurement scheme, the specific readout POVMs are the projectors onto the computational states $\{ \ket{\boldsymbol{b}_k} \bra{\boldsymbol{b}_k} \}_{k \in [K]}$. Thus, we are in particular interested in analyzing the post-measurement state $\hat{\rho}^M (u) = \sum_k \rho_{k k} (u)  \ket{\boldsymbol{b}_k} \bra{\boldsymbol{b}_k}$ whose subsystems are correspondingly in states $\hat{\rho}_l^M (u) = \mathrm{Tr}_{[L] \backslash \{ l \}} \left\{ \hat{\rho}^M (u) \right\}$. We compute the average or \textit{expected} quantum total correlation over the input domain $u$ with respect to the input probability distribution $p(u)$: 
\begin{align}
    \bar{\mathcal{T}}\! \left( \hat{\rho}^M \right) = \Eu{ \sum_{l = 1}^L \mathrm{S} ( \hat{\rho}_l^M (u) ) - \mathrm{S} (\hat{\rho}^M (u) ) } = \Eu{ \sum_{l = 1}^L \mathrm{H} (b_l (u)) - \mathrm{H} (b_1 (u), \cdots, b_L (u)) }
\end{align}
where the second equality comes from the diagonal nature of the post-measurement state which reduces the quantum total correlation to a normal classical total correlation.

The post-measurement quantum total correlation always reaches its maximum $L-1$ when the post-measurement state (which just constitutes the diagonal entries of the pre-measurement state) is a GHZ-typed state. As an additional example, for a $W$-state $\ket{W} = \frac{1}{\sqrt{L}} \left( \ket{10 \cdots 0} + \ket{01 \cdots 0} + \cdots + \ket{00 \cdots 1} \right)$, the post-measurement quantum total correlation $\mathcal{T} ( \ket{W} \! \bra{W} )$ is
\begin{align}
     L \left( - \left( \frac{1}{L} \right) \log_2 \left( \frac{1}{L} \right) - \left( \frac{L - 1}{L} \right) \log_2 \left( \frac{L - 1}{L} \right) \right) - L \left( - \left( \frac{1}{L} \right) \log_2 \left( \frac{1}{L} \right) \right) = (L - 1) \log_2 \left( \frac{L}{L - 1} \right) ,
\end{align}
which is upper bounded by $\lim_{L \rightarrow \infty} \mathcal{T} ( \ket{W}  \! \bra{W} ) = \frac{1}{\ln (2)} \approx 1.443$.


\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section S9
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Guidance from REC theory: principal component analysis with respect to quantum noise}
\label{app:PCA}

Our proposed capacity spectrum analysis has another significant benefit: it provides a natural truncation scale for eigentasks. In machine learning theory, the technique of projection of a high-dimensional data to a subspae of reduced dimensionality is called \textit{principal component analysis}. Within the computing architecture we are discussing, we are interested in carrying out a PCA in the function space. 
 More specifically, consider a given function $f(u)$, we hope to find $K'$ functions $\{G^{(k)}(u)\}_{k \in [K']}$ where $G^{(k)}(u)=\sum_{k' = 0}^{K-1} g^{(k)}_{k'} x_{k'}(u)$ lies in the space spanned by measured features $G^{(k)}(u) \in \mathrm{Span}\{\bm{x}\}$, such that the relative mean square error 
\begin{align}
    \min_{\bm{W}} \frac{\mathbb{E}_u \! \left[\left|f - \sum_{k=1}^{K'} W_k \left(\sum_{k' = 0}^{K-1} g^{(k)}_{k'} \bar{X}_{k'}\right) \right|^2\right]}{\mathbb{E}_u[|f|^2]}
\end{align}
is much smaller as possible. According to Appendix \ref{sec:Information_capacity_saturation}, the solution to $\{\bm{g}^{(k)}\}_{k \in [K']}$ is exactly $\bm{g}^{(k)} = \bm{r}^{(k)}$. Fig.\,\ref{fig:PCA} supplies a concrete example of fitting linear function $f(u)=u$, by setting $K'=40$ in a $6$-qubit system (and thus $K=64$). 
% Figure environment removed

Fig.\,\ref{fig:PCA}(a) shows the projection onto the space spanned by the dominant $40$ readout features. Here, by ``dominant" we mean one can first train by least square regression to get an output weight $\bm{w} \in \mathbb{R}^{K}$, and then select corresponding $w_k$ with the leading $K'$ largest $w^2_k \cdot \mathbb{E}_u[|x_k|^2]$. Then we need to use these $K'$ features to retrain and obtain a new output weight $\bm{w}' \in \mathbb{R}^{K'}$. In such particular example, $\bm{g}^{(k)}$ are some one-hot vectors where the index of $1$ are chosen by the sorting $K'$ largest $w^2_k \cdot \mathbb{E}_u[|x_k|^2]$ as we described before. We can compare the relative mean square error with the case of $\bm{g}^{(k)} = \bm{r}^{(k)}$, the eigentasks. As illustrated in Fig.\,\ref{fig:PCA}(b) the latter is able to achieve an approximation of the desired function (here a linear function) with a decidedly smaller relative mean square error. 

One important question is: what would be an appropriate choice of $K'$ in practice? In Appendix \ref{app:Spectral_finite_statistics} we claim that it is determined by the set of eigentasks for which $\beta_k^2/S < 1$, those for which the signal is larger than the noise. Namely we should define the cut-off $K_{c}(S)$ such that 
\begin{align}
    K_{c}(S) = \max_{\beta_{k}^2 < \NS} k. 
\end{align}
Based on this observation, we can further explore the trend of $K_{c}(S)$ when qubit number $L$ is scaled. As we show in the main text, the NSR spectra grow much slower when $L$ increases. Then the quantum system is able to provide much more eigentasks with more signal than noise. Fig.\,\ref{fig:PCA_L}(a) shows spectrum in H-encoding quantum system with sizes ranging from $L=3$ to  $L=8$ with fixed hyperparameters. Notice that number of shots $\NS=5000$ here is not a large number, which means that we cannot sample enough shots so that features converge to their mean values in the $2^8=256$ dimensional Hilbert space. But applying eigentasks analysis in this example still shows a fast decay of relative error $\min_{\bm{W}} \mathbb{E}_u  [|f - \sum_{k=0}^{K_c(S)} W_k \bar{y}^{(k)} |^2]/\mathbb{E}_u[|f|^2]$ until the fitting accuracy saturates at $L=8$.  

% Figure environment removed


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section S10
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Quantum-noise-PCA in classification problem}
\label{sec:Single_step_QRC}

The highly nonlinear readout feature $x_k(u)$ should have Taylor expansion $x_k(u) = \sum_j^{\infty} (\mathbf{T})_{kj} u^j$. Such complicated functions will span a certain functional space. One fundamental question is what the limit of approximation ability is based on the architecture we proposed. 
Hereby we first show that this architecture \textit{under infinite sampling} is capable of approximating \textit{any} continuous function on the domain $[-1, 1]$ to arbitrary precision. 
Furthermore, the linearity of quantum moment readout and complexity of quantum evolution will help us to understand why such a quantum system has capability to approximate a highly nonlinear function, under finite and bounded computational resources. Exploring the capacity for function approximation under finite measurement resources, as is done in the main text and Appendix \ref{sec:Information_capacity_saturation}, highlights the fundamental limitations placed by quantum noise on computation using the reservoir computing approach. 

\subsection{Universal Function Approximation}
\label{FuncApxUniv}

\rev{A very general question is that, given a specific class $\mathcal{C}$ of functions, whose member is generated from a particular type of physical systems, what type of functions can be approximated by this class.} One conclusion which can be drawn is the \textit{universal function approximation} property. That is, give any continuous function from space of continuous functions on domain $[-1, 1]$, \textit{i.e.} $\phi \in \mathscr{C}([-1, 1], \mathbb{R})$, for any given error $\varepsilon > 0$, there always exists a function $\varphi(u) = \boldsymbol{w} \cdot \boldsymbol{x}(u) \in \mathcal{C}$ such that 
\begin{align}
    |\varphi(u) - \phi(u)| \leq \varepsilon
\end{align}
for any input $u \in [-1,1]$. 
\rev{It is well-known that this is true for Ising-typed Hamiltonian system under a specific way of encoding. One can find it in Appendix A3 of Ref.\,\cite{chen_temporal_2020} which provides a more general proof when more complicated temporal classical input data is considered.} The proof employs the well-known Stone-Weierstrass theorem. 

For our particular architecture, \rev{we illustrate universality holds for the class $\mathcal{C}$ of output functions $y(u) = \sum_k W_k x_k(u)$ generated by \textit{any} Hamiltonian with the form of Eq.\,(\ref{eq:Hu=H0+uH1}). First of all, $D=[-1, 1]$ is obviously a compact space. Then the first thing one need to show is that this class satisfies the so-called \textit{point-separation}: it means for any two distinct $u_1, u_2 \in [-1, 1]$ there exists one weighted feature function $y(u) = \sum_k W_k x_k(u)$ in this class such that $y(u_1) \neq y(u_2)$. For $\mathcal{C}$, the point-separation can also be trivially fulfilled by a single qubit system ($L=1$). In fact, for any two distinct $u_1, u_2 \in [-1, 1]$, one can always find one single qubit Hamiltonian $\hat{H}(u)$ with form of Eq.\,(\ref{eq:Hu=H0+uH1}), evolution time $\tau$ and a single-qubit POVM $\hat{M}$ such that
\begin{align}
    \mathrm{Tr}(e^{- i \tau \hat{H}(u_1)} \hat{\rho}_0 e^{i \tau \hat{H}(u_1)} \hat{M}) \neq \mathrm{Tr}(e^{- i \tau \hat{H}(u_2)} \hat{\rho}_0 e^{i \tau \hat{H}(u_2)} \hat{M})
\end{align}}
\rev{The second thing to be proven is that $\mathcal{C}$ contains a subalgebra. A subalgebra means a subset of functions in $\mathcal{C}$ that is a closure under addition and multiplication.} The subalgebra structure of the function family generated by quantum systems is automatically satisfied by the subset of all product systems, as long as we use a \textit{Walsh-Hadamard transformation} to convert from the quantum probability $\bm{x}(u)$ to the many-body Pauli-$z$ products $\{\langle \prod_i \hat{\sigma}^z_{l}: l \in B \rangle_{\hat{\rho}}\}$ for all $B \subseteq [L]$. 

\rev{For the class of output functions $y(u)$ generated by \textit{any} circuit with the form of Eq.\,(\ref{appeq:cansatz}), one can use the same reason to repeat the proof of point-separation and subalgebra structure, hence show the universality of this class. }

% Figure environment removed

\subsection{1D classification as function approximation for noiseless measured features}
\label{app:1D_clf_func_approx}
In this section, we will show how the universal function approximation property of the architecture described in Appendix \ref{FuncApxUniv} enables it to perform -- among others -- paradigmatic machine learning tasks such as classification. 
% Figure environment removed

Suppose two classes $C_0$ and $C_1$ of samples, each of which is generated from distributions $p_0(u)$ and $p_1(u)$ respectively. The probability of occurrence of $C_0$ and $C_1$ are both $50\%$, and we simply let each class equally contain $5000$ samples and thus $N = 10000$ samples in total. Both distribution are artificially defined by summing several Gaussian distributions with different amplitudes and widths together. Domain of both distributions are restricted in $[-1, 1]$ and both distributions are also normalized. Due to the overlap of two distributions, there is some theoretical maximal classical accuracy to distribution whether a given $u$ belongs to either $C_0$ or $C_1$. 

During the training, we feed each sample $u^{(n)}$ (belonging to class $C_{c^{(n)}}$) into a $5$-qubit quantum system. The quantum system will be read out with $K_{\rm eff} = \sum_{m=0}^{m_{\rm max}}\big(^L_{m}\big)$ different features $\{x_k(u^{(n)})\}_{k \in [K_{\rm eff}]}$. Then features of $N$ sample forms the regressor matrix. According to the standard supervised learning procedure, we simply train based on $(\boldsymbol{x}(u^{(n)}), c^{(n)})$ by logistics regression where one should minimize the cross-entropy loss 
\begin{align}
    \mathscr{L} (\boldsymbol{W}) =~ \frac{1}{N} \sum_{n = 1}^{N} \bigg[- c^{(n)} \mathrm{log}\!\left( \sigma (\boldsymbol{W}\cdot\boldsymbol{x}(u^{(n)})) \right) - \left( 1 - c^{(n)} \right) \mathrm{log}\!\left( 1 - \sigma (\boldsymbol{W}\cdot\boldsymbol{x}(u^{(n)})) \right)\bigg]
    \label{eq:cross-entropy}
\end{align}
where $\sigma$ is the sigmoid function $\sigma(y) = \frac{1}{1+e^{-y}}$. A small $L_2$ penalty $\lambda \|\boldsymbol{W} \|^2$ (where $\lambda=10^{-6}$) is added to Eq.\,(\ref{eq:cross-entropy}) for preventing overfitting. The optimal $\bm{W}$ is then simply the set of weights that minimizes this cost function,
\begin{align}
    \bm{w} = {\rm argmin}_{\bm{W}}~\{\mathscr{L} (\boldsymbol{W})\}
\end{align}

% Figure environment removed

We test the fidelity of learning the classification task by determining the accuracy of classification on a testing set formed by drawing $N = 10000$ new samples (independent of the training set) as a function of the order of output moments extracted, $m_{\rm max} = 1,2,3,4,5$, corresponding to reading out $K_{\rm eff}=6, 16, 26, 31, 32$ features respectively. The resulting testing accuracy is plotted in the left panel of Fig.\,\ref{fig:Approximate_conditional_probability}). We see that the testing accuracy converges to the theoretical maximal accuracy (dashed green) with increase in readout features.

Importantly, one can show that this improvement in learning performance coincides with training of optimal weights $\bm{w}$ such that the QRC is able to approximate the conditional distribution $\mathrm{Pr}[u \in C_1 | u]$ of the two classes with increasing accuracy (lower error). To verify this, we first numerically compute all $K=32$ readout feature functions $\boldsymbol{x}(u)$ of the system, by sweeping $500$ equidistant values of $u \in [-1, 1]$. Effectively learning the conditional distribution means that $\sigma(\boldsymbol{w} \cdot \boldsymbol{x}(u)) \approx \mathrm{Pr}[u \in C_1 | u]$. It is equivalent to use $\boldsymbol{w} \cdot \boldsymbol{x}(u)$ to approximate the following function: 
\begin{equation}
    \boldsymbol{w} \cdot \boldsymbol{x}(u) \approx \sigma^{-1}(\mathrm{Pr}[u \in C_1 | u]). 
    \label{eq:classifier_approx}
\end{equation}
We therefore see that the function approximation universality property of the architecture discussed in Appendix \ref{FuncApxUniv} enables its use as a generic classifier.


\subsection{Solving classification problem by quantum-noise-PCA}

% Figure environment removed

Now we can solve the classification task above by using the quantum-noise princilpal component analysis we learn from capacity analysis. Suppose a physical system with $L=5$ qubits and ring connectivity, we choose the hyperparameter to be $J=2$, $ h^x_{\mathrm{rms}} = h^z_{\mathrm{rms}} = h^I_{\mathrm{rms}} = 5$ and $t=3$. In this H-encoding scheme, we can obtain $K=32$ measured features on each of $N=10^5$ samples $\{u^{(n)}\}$ ($5000$ in class $C_0$ and $5000$ in class $C_1$). We emphasize here that the underlying marginal distribution $p(u)$ is no longer uniform here, and it will make both $\{\beta_k^2\}$ and $\{\bm{r}^{(k)}\}$ very different. 

Given the number of shots $S \in [10^1, 10^5]$, we can still compute the empirical $\tilde{\bm{r}}_N^{(k)}$ and estimating $\beta_k^2$ by using the correction techniques we used in Appendix 
 \ref{app:Spectral_finite_statistics}. By comparing the estimated $(1 - \tilde{\alpha}_{N,k})/(\tilde{\alpha}_{N,k} - \frac{1}{\NS})$ and $S$, we can figure out the cutoff order $K_c(S)$ and combination coefficients $\tilde{\bm{r}}_N^{(k)}$, based on which we can define a set of observables
\begin{align}
    \hat{O}_k = \sum_{k' = 0}^{K-1} \tilde{\bm{r}}^{(k)}_{N,k'} \hat{M}_{k'} \quad k = 0, 1, \cdots, K_c(S).
\end{align}
It is equivalent to say, by measuring $\hat{O}_k$, we can effectively obtain eigentasks $\tilde{\bm{r}}_N^{(k)} \cdot \bar{\bm{X}}_{\mathrm{Train}}$. Then we can apply standard logistics regression on those eigentasks as we did in Eq.\,\ref{eq:cross-entropy}. The only difference is we no longer need any regularization term as penalty like $\lambda \|\boldsymbol{W} \|^2$. The training procedure eventual yield $\bm{w}_{\mathrm{Train}} \in \mathbb{R}^{K_c(S)}$, together with $\tilde{\bm{r}}_N^{(k)}$ and $K_c(S)$. 

Now we generate a totally new and independent set of $u$'s for testing purpose. By measuring $\hat{O}_k$, one get eigentasks $\tilde{\bm{r}}_N^{(k)} \cdot \bar{\bm{X}}_{\mathrm{Test}}$. By plugging $\bm{w}_{\mathrm{Train}} \in \mathbb{R}^{K_c(S)+1}$, together with $\tilde{\bm{r}}_N^{(k)}$ and $K_c(S)$ in training, we can achieve the testing accuracy. The agreement between training and testing accuracy show that the quantum measurement noise effectively works as a regularizer, and do a pretty good job (see Fig.\,\ref{fig:Classification_ES}).

\end{comment}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section S11
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\section{Finite sampling bound and uncertainty propagation}
\label{app:Uncertainty_propagation}
% We conclude that the principle advantage brought about by correlation in this section. There we observe that for certain inputs $u$ (that depend on the input encoding) the measurement of an CS when mapped into the moment space can generate distributions that can be highly anisotropic at finite $\NS$. While for PS these distributions are generally isotropic unless they are close to the boundaries of the output domain (when the encoding produces outputs that are eigenstates of the measurement basis). We observe that this trend is also present in the experimental system despite non-idealities. The origin of higher resolvable expressive capacity at large $\NS$ provided by CSs can be traced back to this basic feature. 

\rev{In this section, we give a rough intuition of that why resolvable expressive capacity in a CS is usually higher than the resolvable expressive capacity in a PS.} For notation convenience, we will adopt the quantum moment representation instead of quantum probability representation. To be more specific, Let $\hat{N}_k = \hat{\sigma}_{l_1}^z \hat{\sigma}_{l_2}^z \cdots \hat{\sigma}_{l_m}^z$, and $\bar{X}_{k} (u)$ be empirical mean based on $\NS$ sampling of Pauli-$z$ strings. Notice that the variance of $\bar{X}_k$ now has an alternative form of
\begin{equation}
    \Vars [\bar{X}_k] = \frac{1}{\NS} \left( \langle (\hat{\sigma}_{l_1}^z \hat{\sigma}_{l_2}^z \cdots \hat{\sigma}_{l_m}^z)^2 \rangle - \langle \hat{\sigma}_{l_1}^z \hat{\sigma}_{l_2}^z \cdots \hat{\sigma}_{l_m}^z \rangle^2 \right) = \frac{1}{\NS}(1 - x^2_k (u)) .
\end{equation}
Thus,
\begin{equation}
    \bar{X}_{k} (u) = x_k (u) + \delta_{k}(u) = x_k (u) + \frac{1}{\sqrt{\NS}} \zeta_{k}(u),
\end{equation}
where random sampling noise $\zeta_{k}(u) \approx \sqrt{1 - x_k^2 (u)} \epsilon$ and $\epsilon$ is a random fluctuation with variance $1$. For quantum moment readout in a generic CS, the amplitude of relative error is
\begin{equation}
    \left| \frac{\delta_{k}(u)}{x_k (u)} \right| \approx \sqrt{\frac{1 - x_k^2 (u)}{x_k^2 (u)}} \frac{1}{\sqrt{\NS}} \propto \frac{1}{\sqrt{\NS}} . \label{K3}
\end{equation}
\rev{If there is no correlation in quantum system, then the readout features in PS is $\langle \hat{\sigma}_{l_1}^z \hat{\sigma}_{l_2}^z \cdots \hat{\sigma}_{l_m}^z \rangle = \langle \hat{\sigma}_{l_1}^z \rangle \langle \hat{\sigma}_{l_2}^z \rangle \cdots \langle \hat{\sigma}_{l_m}^z \rangle$ under infinite $S$. At a first glance, it seems that for each single $\langle \hat{\sigma}_{l}^z \rangle$, it can be well-evaluated from fewer shots $S$. However, we need to point out that for PS it is still hard to reconstruct higher order terms, which are the product of single site terms. There two possible ways of doing that. The first one is taking the classical polynomial readout, the amplitude of relative error is obtained by rule of uncertainty propagation
\begin{align}
  & \left| \frac{(x_{l_1} (u) + \delta_{l_1}) \cdots (x_{l_m} (u) + \delta_{l_m}) - x_{l_1} (u) \cdots x_{l_m} (u)}{x_{l_1} (u) \cdots x_{l_m} (u)} \right|
  \approx \left| \frac{\delta_{l_1}}{x_{l_1} (u)} + \cdots + \frac{\delta_{l_m}}{x_{l_m} (u)} \right| \nonumber\\
  \approx & \left( \sqrt{\frac{1 - x^2_{l_1} (u)}{x^2_{l_1} (u)}} + \cdots + \sqrt{\frac{1 - x^2_{l_m} (u)}{x^2_{l_m} (u)}} \right) \times \frac{1}{\sqrt{\NS}}
  \propto \, m \times \frac{1}{\sqrt{\NS}} . 
  \label{eq:prod-uncertainty-propagation}
\end{align}
which scales linearly when site number $m$ increase. The second way is directly counting the product of $\pm 1$ in outcomes from each qubit. According to Eq.\,(\ref{K3}), we have
\begin{align}
    \left| \frac{\delta_{k}}{x_{k} (u)} \right| \approx \frac{1}{x_k (u)} \frac{1}{\sqrt{\NS}} = \frac{1}{\langle \hat{\sigma}_{l_1}^z \rangle \langle \hat{\sigma}_{l_2}^z \rangle \cdots \langle \hat{\sigma}_{l_m}^z \rangle} \frac{1}{\sqrt{\NS}} \propto 2^m \times \frac{1}{\sqrt{\NS}}.
\end{align}
The last term comes from that $\langle \hat{\sigma}_{l_1}^z \rangle \langle \hat{\sigma}_{l_2}^z \rangle \cdots \langle \hat{\sigma}_{l_m}^z \rangle$ should decrease quickly to to zero when $m$ increases (see a simple example in Fig.\,\ref{fig:EC_vs_PC}). As a result, in PS, both ways do not give a good reconstruction of readout features from low $S$, which illustrates that product systems do not have advantage when quantum sampling noise is present. 
}

% Figure environment removed

\end{comment}

\section{REC and eigentasks for classical systems: a basic optical PNN example}
\label{app:photonicRC}

% % Figure environment removed

In this appendix section, we present some additional details of the REC analysis of the optical setup considered in Sec.~\ref{sec:photonicRC} of the main text. Our focus here is (1) on the details of the sampling noise statistics, which are different when compared to quantum systems, and (2) the form of the REC eigenproblem for this special case.

\subsection{Sampling noise statistics for a classical optical system}
\label{app:photonicRC1}

For convenience, we recall the form of the electric field of propagating radiation presented in Sec.~\ref{sec:photonicRC}. The electric field after the SLM can be written generally in the form~\cite{zhu_arbitrary_2014}:
\begin{align}
    E_0(u;\vec{d}) = A_0 \cos \left(\frac{\varphi_1(u;\vec{d})}{2} \right) \exp\left\{i\left(\frac{\varphi_1(u;\vec{d})+2\varphi_2(u;\vec{d})}{2}\right)\right\}
\end{align}
where $\varphi_l(u;\vec{d})$ are input encoding functions, and $\vec{d}$ is the position vector describing the coordinates where the electric field is evaluated in the plane orthogonal to the propagation direction; in particular $\vec{d} = (q^1,q^2)$. The specific form of the encoding functions is given by:
\begin{subequations}
    \begin{align}
    \varphi_1(u;\vec{d}) &= B\left( \cos u \left[A_1(\vec{d}) \cos q^1 + A_2(\vec{d}) \sin q^2 \right] + \sin u\left[A_1(\vec{d}) \sin q^1 + A_2(\vec{d}) \cos q^2 \right] \right) \label{eq:inputenc1} \\
    \varphi_2(u;\vec{d}) &= B u( A_1(\vec{d}) q^1 + A_2(\vec{d}) q^2) \label{eq:inputenc2}
\end{align}
\end{subequations}
where we set $B=3.75$, and $A_{1,2}(\vec{d})$ are fixed \textit{input-independent} spatial mask functions whose values are sampled from a normal distribution with zero mean and unit variance; more precisely, $A_{1,2}(\vec{d}) \sim \mathcal{N}(0,1)$ for every $\vec{d} = (q^1,q^2)$.

Following the input encoding, the light propagates through a lens and is measured in the lens' focal plane. The electric field in the focal plane $E(u;\vec{d})$ can be shown to be related to the initial field $E_0(u;\vec{d})$ via a Fourier transform~\cite{saleh_fundamentals_1991, yariv_photonics_2007}:
\begin{align}
    E(u;\vec{d}) = \int\int \dd^2 \vec{d}'~E_0(u;\vec{d}')~\exp\left\{\frac{i2\pi}{\lambda f}\left( \vec{d}\cdot\vec{d}' \right) \right\} 
\end{align}
where $\lambda = \frac{2\pi}{k}$ is the wavelength of the propagating field with wavevector $k$, and $f$ is the focal length of the lens being used.

Finally, information must be extracted from this optical system via measurement for its use as a PNN, which also requires us to address the associated measurement noise in a classical setting. We consider photodetection using a camera in the focal plane of the lens. Furthermore, we consider the camera plane as being comprised of a discrete set of $K=P^2$ photodetectors, arranged in an $P$-by-$P$ square spatial grid, such that the $k$th photodetector is identified with coordinates $\vec{d}_k = (q^1_k,q^2_k)$. This spatial grid ultimately defines the coarse-graining level at which the propagating fields can be probed, and is set by the spatial resolution of the photodetection apparatus, as expected. Then, it is known~\cite{wiseman_quantum_2009} that the differential, stochastic photocurrent generated in a given photodetector in a single measurement, which we name for reference $\dd I(\vec{d}_k,t)$, can be written as a Poisson point process,
\begin{align}
    \dd I(\vec{d}_k,t) = \dd N(\vec{d}_k,t)
\end{align}
where $\dd N(\vec{d}_k,t)$ describes the increment in photodetector counts in a time $\dd t$. The stochastic increments, which are independent at different time, has the specific statistical properties:
%%%%%%%%%%%%%% Definition from Fangjun
\begin{subequations}
\begin{align}
    \mathrm{Prob}[\dd N(\vec{d}_k,t) = 1] &= \eta~P(\vec{d}_k)~\dd t + o({\dd t}), \label{appeq:meandN} \\
    \mathrm{Prob}[\dd N(\vec{d}_k,t) \geq 2] & = o({\dd t}), \\
    \mathbb{E}[\dd N(\vec{d}_k,t)\dd N(\vec{d}_{k'},t')] &= \mathbb{E}[\dd N(\vec{d}_k,t)]\mathbb{E}[\dd N(\vec{d}_{k'},t')], \quad {\rm if }~\vec{d}_k\neq \vec{d}_{k'} \label{appeq:vardN}
    % dN^2(x_i,y_j,t) &= dN(x_i,y_j,t) 
\end{align} 
\end{subequations}
where expectation values are formally computed over the distribution of Poisson point processes $\dd N(\vec{d}_k,t)$. Importantly, $P(\vec{d}_k)$ is the power incident on the photodetector with spatial coordinate $\vec{d}_k$. Then, the expectation value of the increment in counts is directly proportional to the intensity of the incident radiation and a measurement efficiency factor $\eta$. The second line defines the fact that the probability of more than a single increment in counts in the time interval $\dd t$ is $o(\dd t)$, and hence higher-order. Finally, the third line indicates that counts on spatially distinct photodetectors and at distinct times are uncorrelated. 

%%%%%%%%%%%%%% Alternative definition from Saeed
% \begin{subequations}
% \begin{align}
%     \mathbb{E}[\dd N(\vec{d}_k,t)] &= \eta~P(\vec{d}_k)~\dd t \label{appeq:meandN} \\
%     \mathbb{E}[\dd N(\vec{d}_k,t)\dd N(\vec{d}_{k'},t')] &= \mathbb{E}[\dd N(\vec{d}_k,t)]\mathbb{E}[\dd N(\vec{d}_{k'},t')], \quad {\rm if }~\vec{d}_k\neq \vec{d}_{k'} \\
%     \mathbb{E}[\dd N^2(\vec{d}_k,t)] &= \eta~P(\vec{d}_k)~\dd t + (\eta~P(\vec{d}_k)~\dd t)^2 = \eta~P(\vec{d}_k)~\dd t + (\mathbb{E}[\dd N(\vec{d}_k,t)])^2 \label{appeq:vardN}
%     % dN^2(x_i,y_j,t) &= dN(x_i,y_j,t) 
% \end{align} 
% \end{subequations}
% where expectation values are formally computed over the distribution of Poisson point processes $\dd N(\vec{d}_k,t)$. Importantly, $P(\vec{d}_k)$ is the power incident on the photodetector with spatial coordinate \rev{$\vec{d}_k$}. Then, the expectation value of the increment in counts is directly proportional to the intensity of the incident radiation and a measurement efficiency factor $\eta$. The second line indicates that counts on spatially distinct photodetectors and at distinct times are uncorrelated. Finally, the third line provides the second-order moment of photodetector increments. The second term here appears at $\mathcal{O}(\dd t^2)$, but is essential to retain as it is simply equal to the square of the expectation value of the increments, and therefore cancels out in the calculation of variances.

The single-shot measured features of this photonic learning scheme become the integrated photocurrent values over an integration time $T_{\rm int}$ (the input $u$ is not written explicitly for notational simplicity): 
\begin{align}
    X^{(s)}_k \equiv I(\vec{d}_k) = \int_0^{T_{\rm int}}~\dd I(\vec{d}_k,t) \in \mathbb{N}
\end{align}
which are once again stochastic quantities, as they vary from one measurement to the next. This also allows us to easily write down the statistical properties of the integrated photocurrent using Eqs.~(\ref{appeq:meandN})-(\ref{appeq:vardN}):
\begin{subequations}
\begin{align}
    \mathbb{E}[I(\vec{d}_k)] &= \eta~P(\vec{d}_k)~T_{\rm int}  \\
    \mathbb{E}[I(\vec{d}_k)I(\vec{d}_{k'})] &= \mathbb{E}[I(\vec{d}_k)] \mathbb{E}[I(\vec{d}_{k'})], \quad {\rm if }~\vec{d}_k\neq\vec{d}_{k'} \\
    \mathbb{E}[I^2(\vec{d}_k)] &= \eta~P(\vec{d}_k)~T_{\rm int} + (\eta~P(\vec{d}_k)~T_{\rm int})^2 = \eta~P(\vec{d}_k)~T_{\rm int} + (\mathbb{E}[I(\vec{d}_k)])^2
\end{align} 
\end{subequations}
The final expression then provides the variance of the integrated photocurrent for a given photodetector indexed by $k$:
\begin{align}
    \sigma_{\rm I}^2(\vec{d}_k) = \mathbb{E}[I^2(\vec{d}_k)]-(\mathbb{E} [I(\vec{d}_k)])^2 = \eta~P(\vec{d}_k)~T_{\rm int}
\end{align}
As an aside, we calculate the measured power signal-to-noise ratio of the integrated photocurrent, ${\rm SNR}_I$, which takes the form
\begin{align}
    {\rm SNR}_I = \frac{(\mathbb{E}[I(\vec{d}_k)])^2}{\sigma_{\rm I}^2(\vec{d}_k)} = \eta~P(\vec{d}_k)~T_{\rm int},
\end{align}
and therefore grows with incident power $P(\vec{d}_k)$ and integration time $T_{\rm int}$ (assuming an unsaturated photodetector). 

We now make the connection between the measured photocurrents and the propagating fields reaching the photodetector, described by $E(u;\vec{d}_k)$. More precisely, the power incident on the photodetector is simply set by the Poynting flux of the propagating fields, and can be related to the electric field intensity, $P(\vec{d}_k) = \alpha|E(u;\vec{d}_k)|^2$, where $\alpha$ is as introduced in Sec.~\ref{sec:photonicRC}.

The complete input-output map defined above fits within our very general framework. To emphasize this, we now define the measured features $\bar{X}_k(u)$ extracted from this classical machine analogously to the case of measured features extracted from quantum systems, namely Eq.~(\ref{eq:Xsum}). Precisely, we define $\bar{X}_k(u)$ as averages over individual shots $s$ of integrated photocurrents for each photodetector,
\begin{align}
    \bar{X}_k(u) = \frac{1}{\NS}\sum_s X_k^{(s)}(u),
\end{align}
We have here restored the $u$-dependence of $X_k$, which arises via the encoded amplitudes and phases in the electric field $E(u;\vec{d}_k)$. 

$\bar{X}_k(u)$ are therefore sums over i.i.d.\,random variables $X_k(u)$. Hence we can directly write for the mean and covariance of these measured features calculated from infinitely-many samples:
\begin{subequations}
\begin{align}
    \Es{\bar{X}_k(u)} &= \eta\alpha|E(u;\vec{d}_k)|^2T_{\rm int}  \\
    \Covs[\bar{X}_j, \bar{X}_k](u) & = 
    % \frac{1}{\NS} \delta_{jk} \Covs [{X}_j, {X}_k](u) 
    \frac{1}{\NS} \delta_{jk} \eta\alpha|E(u;\vec{d}_k)|^2T_{\rm int}
\end{align}  
\end{subequations}
To connect with our prior notation, we further write:
\begin{align}
    \bar{X}_k(u) = x_k(u) + \frac{1}{\sqrt{\NS}}\zeta_k(u)
    \label{appeq:xbarCML}
\end{align}
which is simply Eq.~(\ref{eq:xbar}) of the main text. Note that for most classical machine learning schemes, $\NS=1$; however, in our analysis we allow that the shot number $S$ can be any integer. Here $x_k(u)$ are deterministic quantities defined as:
\begin{align}
    x_k(u) \equiv \Es{\bar{X}_k(u)} = \eta\alpha|E(u;\vec{d}_k)|^2T_{\rm int}.
    \label{appeq:xCML}
\end{align}
Then, it follows that the remaining term $\frac{1}{\sqrt{\NS}}\zeta_k(u)$ is a stochastic process with zero mean (as taking expectation values on both sides of Eq.~(\ref{appeq:xbarCML}) will demonstrate) and whose second-order moment (equivalent to the variance as it has zero mean) encodes the variance of the Poisson point process in one shot of experiment,
\begin{align}
    \Covs [\bar{X}_j, \bar{X}_k](u)  = \frac{1}{\NS} \Covs [{\zeta}_j, {\zeta}_k](u) = \frac{1}{\NS}\eta\alpha|E(u;\vec{d}_k)|^2 T_{\rm int} \delta_{jk} = \frac{1}{\NS} \delta_{jk}x_k(u)
\end{align}
where we have used Eq.~(\ref{appeq:xCML}). This finally yields
\begin{align}
    \Covs[{\zeta}_j, {\zeta}_k](u) 
    \equiv
    \cu_{jk}(u)
    = \delta_{jk} x_k(u)
    \label{appeq:sigmaCML}
\end{align}
as presented in Sec.~\ref{sec:photonicRC}.


\subsection{REC analysis for a classical optical system}

For the Poisson noise process of photodetection, the noise matrix $\ci$ is different compared to the case of the multinomial noise process. Fortunately, for the specific case we have considered with spatially uncorrelated detectors, $\ci$ is in fact simpler. We note that:
\begin{align}
    \ci_{jk} = \int du~p(u) \cu_{jk}(u) = \int du~p(u) \delta_{jk}x_k(u) .
\end{align}
Hence $\ci$ is itself now diagonal. The eigenproblem in question, Eq.~(\ref{eq:eigenprob}), can therefore be simplified to:
\begin{align}
    \ci \bm{r}^{(k)} = \beta_k^2 \gr \bm{r}^{(k)} \implies \ci^{-1}\gr \bm{r}^{(k)} = \frac{1}{\beta_k^2}\bm{r}^{(k)}
    \label{appeq:eigenprobCML}
\end{align}
where we have computed the inverse of $\ci$ since it is a diagonal matrix; we also assume none of its diagonal entries vanish. This is not a strong constraint, since these entries are simply equal to the measured features, which are the sum of intensities incident on the photodetector, and will typically be nonzero (assuming integrated photocurrents from any `dead' photodetectors or pixels are excluded from the measured features). 

Solving Eq.~(\ref{appeq:eigenprobCML}) allows us to calculate the infinite-shot eigentasks and the $\EC$ as a function of $\NS$ for the photonic learning system. We can also construct the Gram and covariance matrices using finitely-sampled features $\bar{X}_k(u)$ over the input domain, as would be done in a real experiment. One do not need to employ the general Eq.\,(\ref{eq:Bessel}) to estimate the eigentasks. In fact, there is simpler procedure which is similar to Eq.\,(\ref{eq:Gtilde1}) and Eq.\,(\ref{eq:Dtilde1}). The following two quantities computed from the $S$-finite statistics 
\begin{align}
     (\widetilde{\gr}_N)_{k_1 k_2} & \equiv \frac{1}{N}\sum_{n=1}^N \bar{X}_{k_1}(u^{(n)})\bar{X}_{k_2}(u^{(n)}) = \frac{1}{N} (\regmat^T \regmat)_{k_1k_2} \approx \int \bar{X}_{k_1}(u)\bar{X}_{k_2}(u) p(u) \dd u, \\
     (\widetilde{\mathbf{V}}_N)_{k_1 k_2} & \equiv \delta_{k_1, k_2} \frac{1}{N}\sum_{n=1}^N \bar{X}_{k_1}(u^{(n)}) \approx \delta_{k_1, k_2} \int \bar{X}_{k_1}(u) p(u) \dd u.
\end{align}
Similar to the scenario in Appendix \ref{app:tilde_correction}. We can show that by solving the eigenproblem $\widetilde{\ci}_N^{-1} \widetilde{\gr}_N \tilde{\bm{r}}_N^{(k)} = \tilde{\alpha}_{N,k} \tilde{\bm{r}}_N^{(k)}$, the true eige-NSR $\beta_k^2$ and eigentasks coefficients $\bm{r}^{(k)}$ can be well approximated by $1/(\tilde{\alpha}_{N,k} - 1/S)$ and $\tilde{\bm{r}}_{N}^{(k)}$, respectively. The derivation still starts from taking their limits for $N \to \infty$:
\begin{align}
    \widetilde{\gr} &= \lim_{N\to \infty}\widetilde{\gr}_N = \gr + \frac{1}{\NS} \mathbf{V}, \label{eq:Gtilde2}
    \\ \widetilde{\mathbf{V}} &= \lim_{N\to \infty}\widetilde{\mathbf{V}}_N = \mathbf{V}. \label{eq:Dtilde2}
\end{align}
Therefore, $\widetilde{\gr}$ and $\widetilde{\mathbf{V}}$ will provide an eigen-problem:
\begin{align}
    \widetilde{\ci}^{-1} \widetilde{\gr} \bm{r}^{(k)} = \tilde{\alpha}_k \bm{r}^{(k)}
\end{align}
where $\tilde{\alpha}_k = \frac{1}{\beta_k^2} + \frac{1}{S}$, or equivalently $\beta^2_k = \frac{1}{\tilde{\alpha}_k - 1/S}$. It means that by numerically solving $\widetilde{\ci}_N^{-1} \widetilde{\gr}_N \tilde{\bm{r}}_N^{(k)} = \tilde{\alpha}_{N,k} \tilde{\bm{r}}_N^{(k)}$, one can use
\begin{align}
    \bar{\beta}_k^2 &\equiv \frac{1}{\tilde{\alpha}_{N,k} - 1/S}, \\
    \bar{\bm{r}}^{(k)} &\equiv \tilde{\bm{r}}_N^{(k)}.  
\end{align}
to accurately approxiamte to $\beta_k^2$ and $\bm{r}^{(k)}$, which finishes the proof of the desired statement.

\begin{center}
    \rule{30mm}{1pt}
\end{center}

\stopcontents[appendices]

%% References
% \bibliography{bibtex}

% \end{document}