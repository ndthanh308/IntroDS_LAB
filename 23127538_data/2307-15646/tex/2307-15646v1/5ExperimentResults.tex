\section{EXPERIMENT RESULTS}
\label{sec: results}


We conduct experiments to evaluate our pipeline's performance in estimating the four properties of the solid particles inside the container. In this section, we show the results for both seen and unseen particles. We also evaluate the influence of three key components of our system: the choice of sensors, actions, and features to extract from the raw data. 

% the following questions: (1) How well does our approach estimate those four particle properties (content mass, volume, particle size, and particle shape)? (2) Could our method estimate unseen particles with different volumes? (3) In our method, there are three key components different with other works: the newly designed high-speed GelSight, the well-defined actions, and the physical-inspired feature extraction process. How helpful to use them for this task?
% \wenzhen{these components are confusing to me-- it seems that they are not the major results, but only some secondary conclusion} 
% \feng{How about now?}
% we will show three key components of our pipeline that contributes to the results: (1) The high-speed GelSight which can sense the fingertip vibration signals. (2) The well-defined actions to activate the whole system to get better sensory data; (3) The feature extraction process inspired by the physical understanding.

% approach the following questions: (1) How well could our approach estimate particle properties? (2) Can our approach generalize to other unseen particles?


\subsection{Experiment Setup and Dataset}
\label{sec: setup}

The robot experiment setup is shown in Fig. \ref{fig:experimentsetup}a. We use a 6 DoF robotic arm (UR5E from Universal Robotics) and a parallel gripper (WSG50 from Weiss Robotics) to grasp the bottle. One Fingertip GelSight \cite{dong2017improved} and one newly designed High-Speed GelSight are mounted on the gripper. A 6-axis Force/Torque sensor (NRS-D50 from Nordbo Robotics) is mounted at the wrist.


% Figure environment removed

% % Figure environment removed


% Figure environment removed


% \wenzhen{avoid passive voice if you can} 


We collect a dataset of $37$ different common solid particles, as shown in Fig. \ref{fig:dataset}. These particles are different in terms of density, size, and shape. They are selected to cover a wide variety of particles in daily life. The particles are stored inside a common plastic cuboid bottle. In this work, we only focus on this particular type of container. We collect data at three levels of content heights $H_p$: $30 \text{mm}, 50\text{mm}, \text{and } 70\text{mm}$.
We perform the data collection process three times for each particle-height setting, formulating a dataset with $333$ data points. 

% \wenzhen{I'm confused here: in one of the experiment, you tested on new particles. Which means the division is in a different way, right?}\feng{yes, the division od VIB and VIC are different.}
% \wenzhen{It looks weird to me if this "dataset" part is only about the dataset used in VI.B, but not VI.C. Why don't you combine them?}
% \feng{I moved the splition part to each of the subsections}\wenzhen{The same with height estimation}

\subsection{Property Estimation with Seen Particles}

We first evaluate our method for estimating particle properties on seen particles. The experiment is conducted on all the $37$ particles in the dataset. Here we randomly split the $333$ data points into $80\%$ training data points and $20\%$ testing data points. The results are shown in Fig. \ref{fig:property_estimation_result}.

\subsubsection{Mass Estimation}
% We estimate mass $M_p$ using Equation \ref{eq:mass}. 
Our method estimates $M_p$ with mean absolute error (MAE) of $1.8$g, and mean absolute percentage error (MAPE) of $1.4$\%.
% The error mainly comes from the hardware noise, such as the inconsistent cable tension.\wenzhen{The second one is confusing. I suggest you to remove it if you don't want to explain why that's causing problems-- therotically it shouldn't}

\subsubsection{Volume Estimation}
We estimate $V_p$ by estimating $H_p$. Our approach estimates $H_p$ with MAE of $2.1$mm and MAPE of $4.3$\%, which represents the $V_p$ estimation MAE is about $6.1$ml and MAPE is $4.3$\%. Note that the ground truth of $H_p$ has a measurement error of roughly $1$ mm.

\subsubsection{Size Estimation}
The particle size $D_p$ estimation MAE is about $1.1$ mm, which is around the size difference between barley and green beans. For most spherical particles, the estimation error is small, while for the irregular particle, the estimation error is larger. There is a measurement error of $0.5$ mm for the groundtruth measurement. %The ground truth of the size has a measurement error of roughly 0.5 mm due to individual particle differences. 
The complex particle dynamic increases the particle behavior variance under small disturbances. In addition, multiple other particle parameters such as texture or friction coefficient would also influence the particle behavior. Some of the daily particles such as rosemary or crushed pepper are very light, decreasing the sensory data magnitude. All the above contribute to the estimation error.


\subsubsection{Shape Estimation}
We classify the particles into different shape groups, as defined in Sec. \ref{sec: problemstatement}. The classification accuracy is $75.6$\%. Our method recognizes most of the particles well. Most of the misclassification of particles happens to the particles whose shape is closer to spherical. This is because the sphericity difference of those particles is smaller and the other particle property such as stickiness would also affect the particle behavior.

% Most of the misclassification of particles happens to the adjacent class. 
% \joe{I see class 2 to class 4 though, why?}
% We train a 3-layer MLP to estimate the particle shape $S_p$. We use MSE loss and ask the model to predict the descriptor $S_p$ instead of the class type. Our approach estimates $S_p$ with MAE of $0.53$. \joe{While I know what you are talking about, this $S_p$ estimation error is particularly confusing.} 

% Figure environment removed


% \subsection{Generalize to Unseen Particles}
\subsection{Property Estimation with Unseen Particles}
% To show the generalizability of our approach, we test our method on particles not used for training. 
To evaluate the generalizability of our approach, we utilize it to estimate the properties of unseen particles. Among our dataset, we select $5$ types of particles for testing: ground coffee, rosemary, orzo, navy beans, and green beans. We use the data collected from the other $32$ particle types for training. To test the generalizability with volume estimation, for each test particle, we also collected 6 data points with different random volumes. So we have $288$ training data points and $75$ test data points.

The results are shown in Fig. \ref{fig:generalization_result}: MAEs of $M_p$, $H_p$, and $D_p$ estimation are $1.4$g, $2.0$mm, and $0.8$ mm, respectively. The particle shape classification accuracy is $64$\%. The result shows that our approach can estimate unseen particles' properties. For most particles, the estimation error of mass, volume, and size is in a similar range to the error testing on seen particles. The algorithm works well to recognize the shape of spherical particles such as navy beans and green beans but performs worse when estimating the shape of irregular particles. Most of the misclassification happens to adjacent groups, which are harder to distinguish.  

% \wenzhen{Check whether the analysis is updated according to the latest experiments}


% Figure environment removed


% In our method, we estimate the particle size and shape based on the prior estimation of mass and volume. To show the benefit of the prior estimation, here we make a comparison on size and shape estimation. We use the same training and test set in Sec\wenzhen{?} \joe{This paragraph is in general weird to me. Maybe need to rewrite it.}\wenzhen{Agree. Can't understand it at all.} but use different features to estimate particle size and shape. 1. We don't include the mass and volume in the input features; 2. We include the estimated mass and volume in the input features; 3. We include the ground truth mass and volume in the input features. The results of these three methods are 1.7 mm, 1.6 mm, 1.4 mm, individually. The results show that the prior estimation of mass and volume is helpful for estimating particle size and shape, and how precise the prior estimation is will influence the latter estimation. 

\begin{table*}[]
    \caption{Comparison of using different sensors, different actions, and different signal processing methods}
    \label{tab:comparison}
    \centering
    \begin{tabular}{c|c|c|c|c}
    \hline
    Tactile sensor & Actions & Signal processing method & size estimation MAE (mm) & shape classification accuracy \\
    \hline \hline
    Normal GelSight & \makecell{two dynamic robot motion \\ (our action)} & \makecell{feature engineering \\ (our method)} & 1.5 & 63.3\%\\
    \hline
    High-speed GelSight & horizontal shaking \cite{chen2016learning} & spectrum features \cite{chen2016learning} & 2.3 & 36.1\% \\
    \hline 
    High-speed GelSight & \makecell{two dynamic robot motion \\ (our action)} & MFCC + LSTM \cite{jonetzko2020multimodal} & 2.9 & 37.3\% \\
    \hline
    \textbf{High-speed GelSight} & \makecell{ \textbf{two dynamic robot motion} \\ \textbf{(our action)}} & \makecell{ \textbf{feature engineering} \\ \textbf{(our method)}} & \textbf{1.1} & \textbf{75.6\%} \\
    \hline
    \end{tabular}

\end{table*}


\subsection{Comparison with Other Methods}
\label{sec: comparison}
The good performance of our method relies on three key components: the use of high-speed GelSight, the optimized actions, and the physical-inspired feature extraction process. Here we evaluate the necessity of those components by comparing the property estimation results using different settings, and the results are shown in Table \ref{tab:comparison}. %conduct multiple comparisons to show their benefits.

\noindent\textbf{Choice of Sensor: }
We first make a comparison with if we only use the traditional GelSight sensor~\cite{dong2017improved} with a sampling rate of $30$ Hz. Using this setting, we lose some high-frequency vibration features during the fast rotation action. The result shows that using high-speed GelSight improves both size estimation and shape estimation accuracy. It also shows the high-frequency vibration features benefit the task.% and the newly-designed high-speed GelSight is helpful for the particle property estimation task.

\noindent\textbf{Choice of Action: }
In our method, we designed two rotation motions based on the desired perception of particle macro-scale behavior to estimate particle size and shape. Here we compare it with the baseline motion: shaking in one direction, which is a common action used to classify solid particles in previous work \cite{chen2016learning}, \cite{eppe2018deep}. Specifically, we ask the robot to shake the container horizontally at $2$ Hz and record dynamic tactile signals by high-speed GelSight. The magnitude of each frequency band is commonly used as the spectrum features of the vibration signal \cite{chen2016learning}. Here we extract the spectrum features from tactile signals of horizontal shaking and input it to a 4-layer MLP to estimate particle size and shape. The results show that by using our actions, the property estimation results improved significantly.
% \wenzhen{Here you also use a different signal processing method. Why that one? Explain it}
%Our estimation error is smaller, showing our designed actions get more sensory data than using horizontal shaking, proving that choosing a proper action to interact with the container is important. 
% \joe{I think this section can be simplified and shortened. You can condense the first 5 sentences into 1 or 2 sentences: We use x and y actions to trigger dynamic tactile signal to estimate x. In this section, we compare it with x.}
% We make a comparison using that motion on particle size and shape estimation. Fig. \ref{} shows the results by horizontal shaking motion. The size estimation error is around 2.8 mm, which is nearly random guessing. 

\noindent\textbf{Choice of Features: }
In our method, based on the physical inspiration, we choose vibration-related features and topple-related features for estimating the particle shape and size. We manually designed the signal processing and feature extraction process. 
%If lacking the physical understanding, a common way, as done by the previous work,
Instead, a common way to process temporal data is to use recurrent neural networks. Specifically, previous works \cite{eppe2018deep}, \cite{jonetzko2020multimodal} on particle classification employed Mel Frequency Cepstral Coefficients (MFCC) preprocessing to the signal and then input the Mel coefficients to LSTM for classification. Here we compare our results with their method. We apply MFCC to the tactile signals we collected during the two exploration processes mentioned in Sec. \ref{section: shape_size} and use LSTM for the estimation of particle size and shape.
% is to input the temporal sequence directly to the neural network. Table \ref{tab:comparison} shows the particle size and shape estimation results if we input the raw signals into LSTM. 
It turns out that the estimation error is significantly larger than ours. This shows that the physics-inspired features we use effectively extracted the important information relevant to the particle dynamics from the tactile signal.
% showing our hand-designed features are proper for estimating corresponding particle properties.
% \joe{I think the first two sentences can be condensed. You just need to mention that you are comparing your processing method with LSTM.}

\subsection{Sugar Humidity Estimation}


Other than the properties we described before, some properties are specific to only certain types of particles but are highly useful at the function level. A typical example is the humidity of sugar, which helps humans to handle them properly. 
% not commonly used to describe all solid particles, such as particle stickiness and humidity, but the estimation of them to some specific particles is also important.
% \feng{jump to the most important thing}
% \wenzhen{the logic flow is weird}
% \joe{I think you can replace the previous sentence to "We show that our pipeline can estimate other particle properties such as sugar stickiness."}
In this subsection, we apply our method to estimate sugar humidity. According to \cite{fraysse1999humidity}, the granular media AoR is also related to particle humidity, inspiring us to use the topple-related features to infer the sugar humidity since these features reflect the particle AoR. Intuitively, higher humidity force sugar to be sticky and hard to collapse, while the dry sugar would flow more like the liquid.
To magnify the collapse pattern of the sticky sugar, we use a rotation motion from $-135$ to $135$ degrees for the exploratory procedure and record the tactile signals by the high-speed GelSight. Then we extract the topple-related features in the same way described in Sec. \ref{section: shape_size}. We then input those features into a 3-layer MLP to estimate the sugar humidity.

To collect a dataset for the task, we manually drop $0.1$, $0.2$, $0.3$, $0.4$, and $0.5$ ml of water into the $150$ g fine sugar powder to create the sugar with $5$ different humidity levels. We use the volume of water to denote the humidity level in this specific setting. 
% Our objective is to estimate the volume of water mixed with the sugar. 
Fig. \ref{fig:sugar_stacking_pattern} shows the step-like torque signals collected in $3$ groups of humid sugar. Sugar with higher humidity has a larger portion to be sticky to the container wall and never collapses, resulting in a lesser variation in the step-like signals. We repeat the exploratory procedure on the $5$ groups of humid sugar with 10 trials. Fig. \ref{fig:sugar_result}a shows the estimation result if we randomly split the whole dataset into $80$\% training dataset and $20$\% testing dataset. Our estimation of the added water has an MAE of $0.026$ ml. Fig. \ref{fig:sugar_result}b shows the estimation result if we train on humid sugar with $0.1$, $0.3$, $0.5$ ml of water and test on humid sugar with $0.2$, $0.4$ ml of water. Our estimation of the added water has an MAE of $0.043$ ml. This experiment shows the potential of our methodology to estimate other particle properties.

% \wenzhen{Can you describe the difference in the signal cased by the sugar humidity?}
%Our previous experiments show that the solid particle dynamic is very complex\wenzhen{no you didn't show that} and a prior estimation of some property (particle mass, volume) can help in estimating other properties (size). We wonder what if we could have stronger prior knowledge such as the specific particle type, could we reduce the variance and get a better estimation?\wenzhen{This motivation part is very weird. I suggest rewriting it.}\joe{okay}\joe{Can we motivate this from the application point of view? In the kitchen, we will need to know the sugar humidity} \wenzhen{That should work. Try to make it intuitive} \joe{It will be slightly disconnect from the two previous experiments though. Is that okay?}\wenzhen{You are doing what you can in two hours}Specifically, we target the sugar particle and try to estimate the humidity of the sugar. 
% Recall that the AoR is also related to particle humidity. Therefore, here we use the stacking pattern feature to estimate how much water is there in the sugar powder. 
% Given the fact that the humid sugar is too sticky to have movement when the end-effector only rotates from -60 degrees to 60 degrees, w
%We enlarge the rotation range to -135 degrees to 135 degrees for generating a more obvious movement. We manually drop 0.1 ml to 0.5 ml of water into the 150g fine sugar powder and shake the bottle vigorously to make the sugar with different humidity. Fig. \ref{}\wenzhen{...} shows the stacking pattern signals. We repeat each group with 10 trials. Fig. \ref{} shows the estimation result if we random split and whole dataset into 80\% training set and 20\% test. The water estimation MAE is 0.26 ml. Fig. \ref{} shows the estimation result if we trained on 0.1 ml, 0.3 ml, 0.5 ml water and tested on 0.2 ml, 0.4 ml water. The water estimation MAE is 0.43 ml.







% Figure environment removed
\feng{include the insight in the captions as well}
\feng{modified}

% Figure environment removed

\wenzhen{caption of fig.14 is unclear}
\feng{modified}
%   (a) Water amount estimation results if we randomly split training and test set. (b) Water amount estimation results if tests on the unseen humid sugar
% \subsection{Comparison}


% \subsection{Discussion}


% \subsection{Comparison with Audio Sensing}
