\UseRawInputEncoding
\documentclass{article}

\pdfoutput=1
% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib, preprint]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{multirow}


 \title{Fashion Matrix: Editing Photos by Just Talking}



\author{
  Zheng Chong$^{1,2}$   \quad  Xujie Zhang$^{1}$   \quad  Fuwei Zhao$^{1}$ \quad 
  Zhenyu Xie$^{1}$  \quad    Xiaodan Liang$^{1,2}$\thanks{Corresponding author is Xiaodan Liang (\texttt{xdliang328@gmail.com}).}   \\
  $^1$Shenzhen Campus of Sun Yat-Sen University  \\
  $^2$Peng Cheng Laboratory\\
  \url{https://zheng-chong.github.io/FashionMatrix} \\
}

\newif\ifcomments
% \commentstrue % Include comments
\commentsfalse %Remove comments

% \newcommand{\commentZXJ}[1]{\textcolor{blue}{[#1]}}

% \definecolor{red}{rgb}{.9,0.1,0.1}
% \newcommand{\commentF}[1]{\ifcomments {\color{red}F: \bf\color{red}#1\color{black}}\fi}
% \newcommand{\commentCZ}[1]{\textcolor{green}{[#1]}}

% \newcommand{\commentZY}[1]{\textcolor{purple}{[#1]}}


\begin{document}

\maketitle
\begin{center}
    \captionsetup{type=figure}
    % Figure removed
    \captionof{figure}{Fashion Matrix demonstrates the capacity for engaging in multiple rounds of user dialogue, enabling proficient and precise photo editing of individuals based on provided instructions.}
\end{center}


% \maketitle



\begin{abstract}
The utilization of Large Language Models (LLMs) for the construction of AI systems has garnered significant attention across diverse fields. The extension of LLMs to the domain of fashion holds substantial commercial potential but also inherent challenges due to the intricate semantic interactions in fashion-related generation.
% Using LLMs as an agent to build AI Systems has received extensive attention in various fields. 
% Extending purely language-based models to fashion is commercially valuable but challenging as fashion-related generation often involves more complex semantic interactions.
To address this issue, we developed a hierarchical AI system called \textbf{Fashion Matrix} dedicated to editing photos by just talking. This system facilitates diverse prompt-driven tasks, encompassing garment or accessory replacement, recoloring, addition, and removal.
% To solve this problem, we designed a hierarchical AI system, named Fashion Matrix, for conversational fashion editing. It supports a wide range of prompt instruction tasks including region-specific replacement, recoloring, addition,
% \commentZXJ{<done> The expression is strange.maybe delete region-specific replacement, the meaning is a bit repetitive} 
%and removal of garments or accessories. 
Specifically, Fashion Matrix employs LLM as its foundational support and engages in iterative interactions with users. It employs a range of Semantic Segmentation Models (e.g., Grounded-SAM, MattingAnything, etc.) to delineate the specific editing masks based on user instructions. Subsequently, Visual Foundation Models (e.g., Stable Diffusion, ControlNet, etc.) are leveraged to generate edited images from text prompts and masks, thereby facilitating the automation of fashion editing processes.
% Specifically, Fashion Matrix is comprised of three modules: (1) \textit{Fashion Assistant}, (2) \textit{Fashion Designer}, and (3) \textit{AutoMasker}. Fashion Assistant focuses on dialogue with users, and detects and summarizes users' editing requirements; Fashion Designer analyzes the requirements and disassembles them into executable task sequences, and then utilizes AutoMasker and Visual Foundation Models (VFMs) such as StableDiffusion and ControlNet to generate edited images. AutoMasker employs multiple human parsing, pose estimation, and open domain segmentation models (referred to as Semantic Segmentation Models, SSMs)\commentZXJ{maybe: remove open domain, for example Semantic Segmentation Models(SSMs)} to obtain refined fashion semantics to provide guidance for the editing process.
Experiments demonstrate the outstanding ability of Fashion Matrix to explores the collaborative potential of functionally diverse pre-trained models in the domain of fashion editing.
%, with a clear hierarchical division of system roles, 
% explores the collaborative potential of functionally diverse pre-trained models in the domain of fashion editing.
% \commentZXJ{<done> 
% Fashion Matrix as a clear hierarchical division of system roles shows it outstanding ability to explores the collaborative potential of functionally diverse pre-trained models in the domain of fashion editing}
The code is available at \url{https://github.com/Zheng-Chong/FashionMatric}.

  % ChatGPT has garnered much attention from a diverse range of fields due to its impressive conversational responses and problem-solving proficiency. Extending this purely language-based model to the fashion domain is commercially valuable but challenging because fashion is typically evaluated through visual features instead of text. To this end, we integrate ChatGPT with pre-trained vision models such as Stable Diffusion, BLIP, and ControlNet tailored for fashion, and create a dialogic fashion editing system named \textbf{Fashion ChatGPT}, which supports extensive prompt-instructed tasks including garments/accessories replacement, outfit style transfer, and partial clothing editing.

  % Specifically, we disassemble the system based on the business logic and divide it into three modules: (1) \textit{Fashion Asistant}, (2) \textit{Fashion Designer}, and (3) \textit{AutoMasker}. The information transfer between modules only needs a few task parameters, which avoids the system redundancy caused by complex logic. The Fashion Assistant focuses on the dialogue with users and detects the user editing instruction. Fashion Designer analyzes the instructions and organizes them into multiple executable task sequences and distributes them to AutoMasker and VFMs. AutoMasker is the key to achieving fine-grained fashion editing. This module uses multiple human body segmentation models to generate the required mask according to a given editing task.
  % Experiments show that with clear role division, Fashion ChatGPT assisted by visual models and LLMs effectively explores the ability of pre-training model cooperation in fashion editing.
  % Experiments show that Fashion ChatGPTenables the exploration of ChatGPT's visual capabilities in the field of fashion editing and virtual try-on. 
    % prompt-instructed (1) virtual garments/accessories try-on, (2) partial clothing editing, (3) logo addition/removal, (4) outfit style transfer, and so on.
  
  % Specifically, we set ChatGPT as a conversational hub for fashion tasks, designed with \textit{prompt rules} that enable it to break down complex user queries into multiple executable standard sub-tasks and invoke the corresponding visual models. To fulfill fine-grained fashion editing such as embellishing the neckline, we further propose a \textit{CoSeg} module that jointly uses multiple human segmentation models to yield the desired body parsing.
\end{abstract}

% Figure environment removed


\section{Introduction}

% As show in Fig.1, ...
% ChatGPT is great but it is a purely language-based model. It need fashion vision models to process fashion editing tasks.
% Recently, large language models (LLMs) such as PaLM\cite{chowdhery2022palm}, LLaMA\cite{touvron2023llama}, and GPT-3\cite{brown2020language} have proven highly effective for various Natural Language Processing (NLP) tasks. 
% Large language Models finetuned for human-machine dialogue, such as ChatGPT, have attracted wide attention. Furthermore, innovators begin extending LLMs, especially ChatGPT, to domain-specific tasks of interest [CITE].

Recently, large language models (LLMs) such as PaLM\cite{chowdhery2022palm}, LLaMA\cite{touvron2023llama}, and ChatGPT\cite{InstructGPT} have proven highly effective for various Natural Language Processing (NLP) tasks, such as knowledge graphs, code completion, and chat robots, etc.
% \commentZY{[done](e.g., ... etc.) (maybe show some NLP using cases here to make the description more concrete.)} 
Moreover, innovators begin extending LLMs to domain-specific tasks and producing agent systems that demonstrate remarkable proficiency in handling complex problems. These works typically endow LLMs with the functionalities of other models or tools, thereby augmenting their capacity to address diverse application scenarios beyond NLP. 
% For instance, Visual ChatGPT\cite{wu2023visual} integrates a spectrum of Visual Foundation Models, empowering LLMs with adeptness in reading, editing, and reconstructing images.
% Visual ChatGPT exhibits satisfactory performance in general-purpose neutral image editing, but it falls short when employed for fashion-related editing tasks, highlighting the need for further optimization and specialization in the domain of fashion. 

For instance, Visual ChatGPT\cite{wu2023visual} integrates a spectrum of Visual Foundation Models, empowering LLMs with adeptness in reading, editing, and reconstructing images.
Visual ChatGPT exhibits satisfactory performance in general-purpose neutral image editing. However, for the fashion-related domain, which focuses on human-centric generation and editing, Visual ChatGPT obtains inferior performance due to the lack of dedicated semantic perception for human body (e.g., human pose, human parsing, etc.).
This highlights the need for the combination of cutting-edge image generation model with the advanced semantic modeling model to further facilitate the improvement of LLM for fashion-related application scenario.
% \commentZY{[done]Maybe we should provide some analysis here to explain why Visual ChatGPT performs poorly for fashion domain. The reasons could be }
% \commentZY{[done]For instance, Visual ChatGPT\cite{wu2023visual} integrates a spectrum of Visual Foundation Models, empowering LLMs with adeptness in reading, editing, and reconstructing images.
% Visual ChatGPT exhibits satisfactory performance in general-purpose neutral image editing. However, for the fashion-related domain, which focuses on human-centric generation and editing, Visual ChatGPT obtains inferior performance due to the lack of dedicated semantic perception for human body (e.g., human pose, human parsing, etc.).
% This highlights the need for the combination of cutting-edge image generation model with the advanced semantic modeling model to further facilitate the improvement of LLM for fashion-related application scenario.}

% People habitually perceive fashion in a visual way, developing a system that can manipulate fashion images by just talking is more crucial for dialogic fashion understanding. 
% Representative work such as Visual ChatGPT investigates the visual roles of LLMs with the help of Visual Foundation Models. 
% and ChatGPT~\cite{InstructGPT} for fashion is one of the most commercially valuable directions.
% Due to the lack of image understanding ability, the current use case limits to text-based fashion recommendation, e.g., ``What should I wear on a rainy day?'' 
% This AI System that combines various pre-trained models through LLMs has attracted great attention.
% While Visual ChatGPT is applied to neutral images editing for general purpose with plenty of VFMs with different functions, 

% \commentZXJ{Recently, large language models (LLMs) such as PaLM , LLaMA, and ChatGPT  have demonstrated high effectiveness in various Natural Language Processing (NLP) tasks. Moreover, innovators have started extending LLMs, particularly ChatGPT, to domain-specific tasks of interest, and ChatGPT for fashion is one of the most commercially valuable directions. Previous works, such as Visual ChatGPT , have explored the visual capabilities of LLMs with the assistance of VFMs. Visual ChatGPT which combines different pre-trained models through LLMs, has garnered significant attention. While it is commonly used for general-purpose editing of neutral images with a variety of VFMs offering different functions, its performance is unsatisfactory when applied to fashion-related editing tasks. Since people typically perceive fashion visually, the development of a system that can manipulate fashion images through conversation is crucial for enhancing dialogic fashion understanding.}

To tackle this concern, we have developed a multi-round dialogue AI system named \textbf{Fashion Matrix}, expressly tailored for fashion-centric applications. Serving as the pioneer in conversational fashion editing, this innovative framework integrates LLMs with cutting-edge image generation models (e.g. Stable Diffusion\cite{rombach2021highresolution}, ControlNet\cite{zhang2023Controlnet}, etc.) and semantic segmentation models (e.g., Grounded-SAM\cite{kirillov2023segany,liu2023grounding}, MattingAnything\cite{li2023matting}, etc.) facilitating expeditious and accurate guidance for multiple editing tasks (as shown in Fig. \ref{fig:teaser}). 

% To alleviate this problem, we developed a multi-round dialogue AI system named \textbf{Fashion Matrix} focusing on fashion-related tasks. Being the first conversational fashion editor, it integrates LLMs with the phenomenal image generation models including Stable Diffusion and ControlNet\cite{zhang2023Controlnet} to accomplish prompt-instructed area-specified replacement, addition, recoloring, removal and AI model tasks (as shown in Fig.\ref{fig:teaser}). 
% \commentZXJ{[done]To address this issue, we have created a multi-round dialogue AI system called \textbf{Fashion Matrix}, which specifically targets fashion-related tasks. As the first conversational fashion editor, it combines LLMs with state-of-the-art image generation models such as Stable Diffusion and ControlNet to enable quick and precise instructions for mutli edit task(this expresssion maybe need replace)}
% These tasks have been explored extensively in human-centric computer vision research. But most of them only support a single and specified input format. Here, we aim to achieve a multi-round dialogue  system that can receive multi-modal inputs for fashion editing.

% This requires ChatGPT to have the visual understanding ability.


 % (1) garments/accessories replacement/addition, (2) partial clothing editing, (3) outfit style transfer as well as their combinations (see Fig.~\ref{fig:vis}). 


% Meantime, there has been a significant proliferation of cross-modal visual models, such as BLIP\cite{li2022blip} for visual question answering (VQA), as well as Stable Diffusion for text-to-image generation. These advancements have laid the foundation for establishing a connection between language and vision domains. This growing body of research aims to bridge the gap between text and image modalities, facilitating seamless interaction and understanding between language and vision. \commentF{I would replace this paragraph with one that describes the cons of vision-only fashion editing models. And move BLIP to the ``Specifically'' part.}

% % Figure environment removed



% To compensate for the absence of visual understanding of ChatGPT, recent research endeavors, such as Visual ChatGPT, have aimed to bridge this gap by integrating a Visual Foundation Model (VFM) into ChatGPT. While the visual capabilities of ChatGPT have been explored to some extent, its application in specific domains, such as fashion, has not been extensively investigated. \commentF{This paragraph may be considered for integration into the second paragraph}

% Leveraging the potential of ChatGPT in the fashion domain could prove to be highly advantageous from a commercial standpoint. In the past, people habitually perceive fashion in a visual way, but the intelligent fashion is moving towards a development trajectory where the desired visual effects can be achieved through text or dialogue. \commentF{This paragraph may be considered for integration into the second paragraph}

% As the pioneer in the field of conversational fashion editing, our system combines the power of Language Models (LLMs) with vision-language models such as Stable Diffusion and ControlNet\cite{zhang2023adding}. This integration enables us to perform a range of tasks based on prompt instructions, including (1) replacing and adding garments/accessories , (2) conducting partial clothing editing, (3) executing outfit style transfers, as well as various combinations of these tasks. \commentF{This paragraph may be considered for integration into the second paragraph}

Specifically, our Fashion Matrix is composed of three modules as shown in Fig.~\ref{fig:system}: (1) \textit{Fashion Assistant} , (2) \textit{Fashion Designer}, and (3) \textit{AutoMasker}. 
% The Fashion Assistant is responsible for receiving user input in the form of text and images and providing intelligent responses. Its main task is to gather fashion editing requirements from users and submit them to the Fashion Designer. 
The Fashion Assistant employs LLM to engage in dialogues with users, and gathers their editing requirements.
% The Fashion Designer, which is designed for logical control, plays a crucial role in the entire system. It analyzes the user requirements and transforms them into system instructions, effectively utilizing Virtual Fitting Models (VFMs) and AutoMasker to achieve precise image editing. 
Fashion Designer designed for logic control plays a core role in the whole system. It partitions the user's editing requirements into discrete editing tasks, followed by prompt standardization for each task and utilizes AutoMasker and the corresponding visual fondation models to iteratively process the tasks.
AutoMasker is crucial for achieving fine-grained and open-vocabulary editing capabilities. It combines results form multiple semantic segmentation models 
% \commentZXJ{[done[delete abbreviation,just once in paper}, to obtain more precise semantic masks that may have overlapping areas. These masks are then composed to align with the user's given instructions. 
% \commentZXJ{[done]The Fashion Assistant utilizes LLM to engage in conversations with users and gather their editing requirements. The Fashion Designer, designed for logic control, plays a central role in the entire system. It separates the user's instructions from the Fashion Assistant into individual editing tasks, standardizes prompts for each task, and calls the corresponding AutoMasker and VFMs to process the tasks iteratively. The AutoMasker is crucial for achieving fine-grained and open-vocabulary editing capabilities. It combines results from multiple Semantic Segmentation Models, including Grounded-SAM, MattingAnything, Graphonomy, and DensePose, to obtain more precise semantic masks that may have overlapping areas. These masks are then composed to align with the user's given instructions}

% We compare it to a variety of LLMs such as the ChatGPT, the New Bing ... Fashion ChatGPT achieves the best performance on ...
% Figure environment removed


To summarize, we present three main contributions:
\begin{itemize}
    \item We propose Fashion Matrix, a conversational system with a structured hierarchical architecture. It can address diverse fashion editing tasks bolstered by the integration of LLM,  Semantic Segmentation Models and Visual Foundation Models 
    % \commentZXJ{[done]delete abbreviation,use Unified expression} tailored for fashion.
    % \item We propose Fashion ChatGPT, a conversational system with a structured hierarchical architecture. It can address a range of fashion editing tasks bolstered by the integration of ChatGPT and Visual Foundation Models (VFMs) tailored for fashion.
    \item We propose an \textit{AutoMasker} module that integrates various human parsing, pose estimation and general semantic segmentation models to from a new fine-grained human segmentation map named CoSegmentation and generate task-oriented semantic masks, facilitating a wide range of fashion editing tasks.
    \item Extensive zero-shot experiments have demonstrated the exceptional performance of our Fashion Matrix. Its versatility makes it valuable for both professional designers and casual users who wish to explore various outfit combinations and styles.
    % Extensive zero-shot experiments highlight the superiority of our Fashion Matrix. And its versatility would be helpful for both professional designers and regular users who want to play around with different outfit combinations and styles. 
    % \commentZXJ{[done]Extensive zero-shot experiments have demonstrated the exceptional performance of our Fashion Matrix. Its versatility makes it valuable for both professional designers and casual users who wish to explore various outfit combinations and styles}
    % It would also be an important tool for e-commerce companies looking to showcase their products in a new and innovative way.
\end{itemize}

\section{Related Work}

\subsection{Agent System}

AutoGPT\cite{richards2023auto}, GPT-Engineer\cite{gpt-engineer}, HuggingGPT\cite{shen2023hugginggpt}, BabyAG\cite{babyagi3}, and other projects have demonstrated to a certain extent the ability to use a large language models (LLMs) as the core controller to build an Agent System. The potential of LLMs is not limited to generating content, stories, papers, etc. It also has powerful general problem-solving capabilities that can be applied in various fields.
In the LLM-driven AI Agent System, LLM is the "brain" of the system, which uses Chain-of-Thought (CoT)\cite{li2020manigan-cot, zhang2022automatic-cot}, ReAct\cite{yao2022react} , and other ways to think about the specified target person and obtain the target result by calling external tools. Although there are plenty of studies on Agent Systems, the incorporation of LLM capabilities into fashion-related domains remains a relatively underexplored area.

% This paper not only uses LLM as a tool for dialogue with users focus on the fashion domain but also applies its logical reasoning ability to task-oriented information extraction, classification, and Matching.

\subsection{Human Parsing and Pose Estimation}

Human parsing and pose estimation belong to the human-centered subdivision of dense prediction tasks, which supports the development of virtual try-on and fashion-related generation. OpenPose\cite{openpose}, MMPose\cite{mmpose2020}, and other methods\cite{alphapose, li2019crowdpose} identify the specified keypoints of the human body in the picture and form a pose heatmap in the form of a skeleton. DensePose\cite{Guler2018DensePose} realizes the mapping of 2D RGB images to 3D models, which has richer information than skeleton, and its prediction segmentation map also has clothing-agnostic features. Graphonomy\cite{Gong2019Graphonomy} and some other works\cite{gong2018instance-cihp, Gong_2017_CVPR-ssl, liang2018look-JPPnet} can identify and segment parts with specified semantics (such as top, coats, hair, etc.), but their segmentation is limited to specified labels, making it difficult to perform finer-grained division.
Recently, SAM\cite{kirillov2023segany} achieves open-domain segmentation when providing prompts (such as boxes / points), which is a landmark progress in the field of dense prediction. Grounded-SAM achieves open-domain segmentation through text prompts by combining GroundingDINO\cite{liu2023grounding} and SAM\cite{kirillov2023segany} without manually labeling the bounding box. Then MattingAnything\cite{li2023matting} imitated Grounded-SAM to achieve matting for any object with richer details than segmentation.
Nevertheless, relying solely on human-centered dense prediction proves inadequate to meet the demands for fine-grained fashion tasks. It is necessary to investigate the integration of multiple Semantic Segmentation Models with the aim of accomplishing open-vocabulary fashion segmentation.

% The AutoMasker module proposed in this paper utilizes a variety of human-centered or general-purpose detection and segmentation models to construct a new semantic segmentation, called CoSegmentation, with overlapping parts and more fine-grained semantics,  thereby supporting subsequent fashion editing.


\subsection{Fashion Synthesis and Editing}

Previous work on human synthesis and editing usually focuses on image-to-image virtual try-on \cite{choi2021vitonhd, wang2018toward-cp,  xie2022pasta, zhu2023tryondiffusion}, or unconditional human generation \cite{fu2022stylegan}, with limited granularity and degree of control over the generation process. Recently, text-to-image fashion editing, such as Text2Human\cite{jiang2022text2human}, and HumanDiffusion\cite{zhang2022humandiffusion}, realizes the generation of human images based on pose or segmentation under the guidance of text (or labels), but these methods cannot maintain the identity of the person. FICE\cite{pernuvs2023fice} uses GAN Inversion to realize the modification of human photos based on text prompts while maintaining the characteristics of the person, but it is unable to guarantee the editing effect of images outside the distribution.
However, these methods encounter challenges in achieving meticulous control over the generated photos, or suffer from simplistic control conditions, consequently leading to functional limitations.
% By leveraging the fine-grained segmentation provided by AutoMasker, along with the integration of diverse Visual Foundation Models, our system can achieve more controllable synthesis and editing of human photos based on text prompts while preserving the attributes of non-target regions to the greatest extent possible.

% \subsection{Text-to-Human Generation}
% Previous fashion editing tasks mainly focused on image-to-image virtual try-on, but text-to-image fashion editing is a topic that garners limited attention. 
% Recently, Text2Human\cite{jiang2022text2human} adds text descriptions and labels based on the DeepFashion\cite{liuLQWTcvpr16DeepFashion} dataset and is enhanced with the ability to generate high-quality human images from posture or segmentation under the guidance of labels.
% HumanDiffusion\cite{zhang2022humandiffusion} realizes human generation driven by open vocabulary based on the new dataset.
% FICE\cite{pernuvs2023fice} leverages GAN Inversion to enable human image modifications based on text prompts. Moreover, FashionTex\cite{lin2023fashiontex} introduces the integration of texture image blocks as additional conditions, alongside text, enabling the transformation of clothing to align with desired textures and descriptions.

% While the above approaches successfully achieve the controllable generation of human images based on textual input, they have limitations when it comes to fine-grained modifications such as neckline and sleeve adjustments. Additionally, it only focuses on clothing replacement and lacks the capability to edit accessories like hats and necklaces. Fashion ChatGPT surpasses these limitations by offering more intricate and versatile modifications without the need for training or fine-tuning .

% \subsection{Domain Augmentation for Pre-trained LLMs}
% Pre-trained LLMs have shown superior performance in handling language-based problems, ranging from text summarization and translation to sentiment analysis and question-answering. However, when faced with more complex scenarios such as  multimodal inputs, these models will reveal their limitations due to the lack of domain-specific knowledge or visual understanding capabilities. To solve this, GeneGPT\cite{jin2023genegpt} augments LLMs with domain-specific tools such as database utilities to help facilitate better access to specialized knowledge. Concretely, it uses Web Application Programming Interfaces (APIs) of the National Center for Biotechnology Information (NCBI) to answer genomics questions. They test GeneGPT on the GeneTuring dataset and show that it achieves state-of-the-art results on both one-shot and zero-shot tasks, outperforming other biomedical LLMs and general-purpose LLMs. For multimodal tasks, Visual ChatGPT\cite{wu2023visual} uses prompts to integrate visual model information into ChatGPT, and combines the strengths of both ChatGPT and Visual Foundation Models (VFMs), allowing people to communicate with ChatGPT using both language and images. In this paper, we augment LLMs with fashion-specific vision models, and propose a conversational fashion editing system to accomplish complex dialogic multimodal tasks.


\section{Fashion Matrix}
\label{method}

% % Figure environment removed

\subsection{Overview}

% The intention of Fashion Matrix is to make fine-grained and controllable fashion editing of given photo by combining various pre-training models under the condition of conforming to human interaction habits.
% % \commentZXJ{Maybe you need to start with "our proposed Fashion chatgpt balabala "to be clearer,
% % Just a sentence or two to introduce the model}
% To make the framework more consistent with the team collaborative workflow like humans, we divided Fashion Matrix into modules according to different functional roles, the pipeline of Fashion Matrix is demonstrated in Fig. \ref{fig:system}. This division not only reduces the complexity of a single module function but also makes each module more focused and efficient in its functional responsibilities. Specifically, we divide Fashion Matrix into three modules:


The purpose of the Fashion Matrix is to enable precise and controllable fashion editing of a given photo by integrating various pre-trained models while adhering to human interaction habits. To align the framework with the collaborative workflow of a team, we have divided the Fashion Matrix into modules based on different functional roles. The pipeline of the Fashion Matrix is illustrated in Fig. \ref{fig:system}. This division not only simplifies the complexity of each module's function but also enhances the focus and efficiency of each module in its specific responsibilities. Specifically, the Fashion Matrix is divided into three modules:

% \commentZXJ{[done]The purpose of the Fashion Matrix is to enable precise and controllable fashion editing of a given photo by integrating various pre-trained models while adhering to human interaction habits. To align the framework with the collaborative workflow of a team, we have divided the Fashion Matrix into modules based on different functional roles. The pipeline of the Fashion Matrix is illustrated in Figure 3. This division not only simplifies the complexity of each module's function but also enhances the focus and efficiency of each module in its specific responsibilities. Specifically, the Fashion Matrix is divided into three modules:}
\begin{itemize}
    % \item \textbf{Fashion Assistant}: As a module that communicates directly with users and communicates to the real core editing business according to the needs of sorting users, \textit{Fashion Assistant} plays the role of "customer service" or "front desk" to establish the connection between users and the framework.\textit{Fashion Assistant} focuses on talking to users and collates and passes their fashion editing requirements to \textit{Fashion Designer} when users put forward them.
    \item \textbf{Fashion Assistant}: As a module that directly interacts with users and conveys their specific editing needs to the core editing functionality, the Fashion Assistant serves as the "customer service" or "front desk" of the framework, establishing a connection between users and the system. The Fashion Assistant primarily engages in conversations with users, collects and organizes their fashion editing requirements, and forwards them to the Fashion Designer module for further processing.
    % \commentZXJ{[done]As a module that directly interacts with users and conveys their specific editing needs to the core editing functionality, the Fashion Assistant serves as the "customer service" or "front desk" of the framework, establishing a connection between users and the system. The Fashion Assistant primarily engages in conversations with users, collects and organizes their fashion editing requirements, and forwards them to the Fashion Designer module for further processing.} 
    \item \textbf{Fashion Designer}: As its name indicates, \textit{Fashion Designer} will process and optimize according to the photos to be processed and editing instructions submitted by \textit{Fashion Assistant}, and utilize BLIP\cite{li2022blip}, \textit{AutoMasker} to obtain image information, target mask, and standardized editing instructions according to the standardized processing flow. Finally, the edited image results are obtained by using various Visual Foundation Models.
    \item \textbf{AutoMasker}: It uses different human-centered parsing and pose estimation models to obtain finer-grained human semantic information and form it into CoSegmentation. Besides, AutoMasker utilizes Grounded-SAM\cite{kirillov2023segany, liu2023grounding} for open-domain segmentation to be suitable for more general fashion tasks and uses MattingAnything\cite{li2023matting} for fine-tuning of boundaries.
\end{itemize}

\subsection{Fashion Assistant}

% Figure environment removed

Fashion Assistant plays the role of account manager in the team, which does not directly contact the image editing business but only plays the role of docking with users and Fashion Designer. The Fashion Assistant can have natural conversations, including providing users with basic information about Fashion Matrix and answering users' questions. 

After the user uploads the image and indicates the editing instruction, the Fashion Assistant will submit the image to be edited and the editing instruction to the Fashion Designer to start the fashion editing process, then submit the editing result returned by the Fashion Designer to the user, reorganize the image to be edited according to the user's feedback on the result and submit it to the Fashion Designer according to the user's requirements, and so on. 

This clear role division and black box design avoids the confusion caused by directly letting LLMs take on too many functions, makes the processing flow clearer and clearer, and avoids designing too many systematic definitions in advance. 

\subsection{Fashion Designer}

% % Figure environment removed

We define Fashion Designer as a hub for receiving, processing, and distributing fashion editing tasks. The name "Designer" vividly expresses its function. 
For more controllable execution, we divide fashion tasks into 4 categories:
1) Replacement: Replace an item or partial area with another and its shape and appearance may be changed, such as modifying the neckline style.
2) Recoloring: Modify the appearance (mainly the color) of a certain part while retaining its shape, such as changing the color of pants.
3) Addition: Add an accessory or clothing that does not exist in the photo, such as add a coat, watch.
4) Removal: Erase an accessory or part, such as removing necklaces, bracelets.

After receiving the edited image $I$ and editing requirements $R$ from Fashion Assistant, Fashion Designer leverages LLM to decompose $R$ into an executable task sequence $\{T_0, ..., T_n\}$.  Each task is parameterized as $T = \{c, t_o, t_e\}$, where $c$ represents the category of the task and $t_o$ and $t_e$ represent the original and target description of the part to be edited respectively. 
$T$ and $I$ will be passed as parameters to AutoMasker to get the binary Mask $M$ used to guide image editing:

\begin{equation}
\label{eq1}
	M=AutoMasker(I,T)=AutoMasker(I,\{c,t_o,t_e\})
\end{equation}

The generation process is completed by the collaborative work of Stable Diffusion and ControlNet with different conditions. Since this process is a text-guided generation, it is necessary to obtain a suitable text prompt $t$.
For this purpose, BLIP is used to obtain more detailed information about $t_o$ in $I$, and then LLM summarizes a more appropriate text prompt from this information and $R$ for the text-to-image generation:

\begin{equation}
\label{eq2}
	t=LLM(c, BLIP(I,t_o), t_e)
\end{equation}

For recoloring, the SoftEdge version of ControlNet\cite{zhang2023Controlnet} uses the extracted edge sketch (PiDiNet\cite{su2021pidi}) to keep the shape of the target part unchanged and uses $t$ to recolor the sketch along with the Inpainting version of ControlNet. For replacement, addition, and removal, the Inpainting version of ControlNet is adopted directly:

\begin{equation}
\label{eq3}
	I_e=\left\{  
             \begin{array}{lr}  
             G(I,M, PiDi(I),t),&  if\; c \;is\; recoloring\\  
             G(I,M,t),&  else\\  
             \end{array}  
\right.  
\end{equation}
where $\bigodot$ is the element-wise multiplication, $G(\Theta)$ represents the Stable Diffusion\cite{rombach2021highresolution} generation process controlled by ControlNets\cite{zhang2023Controlnet}.

% it can use BLIP\cite{li2022blip} to understand the content of the edited image, split the complex fashion editing requirements of users into several executable standard tasks, and standardize the prompts used to guide the generation for each task according to the original image caption from BLIP and editing instructions provided by users. After the above operations are completed, the corresponding visual model is adaptively called for processing.

% After receiving the image and the user's editing instruction submitted by Fashion Assistant, the operation workflow of Fashion Designer is as follows: 

% Firstly, Fashion Designer splits the user's editing instruction $I_t$ into operation sequence $O=\left\{o_0,...o_n\}\right.$ according to semantics, each operation $o_i$ in $O$ is an independent editing operation for a specified action of a certain part. 

% Secondly, Fashion Designer utilizes BLIP\cite{li2022blip} for the visual question and answer to understand the outfits worn by the character in the image to be edited and organize the information into a descriptive text $I_i$. 

% Then, Fashion Designer will generate standardized the text prompt $p_i$ according to operation $o_i$ and image information $I_i$, which can be used to guide the subsequent generation process.

% Finally, $O_i$ is passed to AutoMasker to generate the Mask of the target editing area $M_i$, and $(M_i, P_i)$ is input to the corresponding VFMs to generate the result $R_i$ of the current operation. Subsequently, $R_i$ is used as a reference image to perform subsequent operations until all operations are performed.


\subsection{AutoMasker}

% % Figure environment removed


To make use of fine-grained mask semantics and control mask generation, we propose an AutoMask module to balance the degree of original information retention and the naturalness of generation fusion. 

For an input image $I$, AutoMasker first processes the human semantic segmentation (Graphonomy) and pose estimation (DensePose), and combine them to obtain a more fine-grained semantic segmentation map named CoSegmentation $S_{co}=\{s_0:m_0,…,s_n:m_n \}$, where $s_i$ represents the semantic of certain part, and $m_i$ represents the mask corresponding to the semantic. Despite the absence of certain common semantics in the original image, they can be effectively estimated by employing various operations such as combination, cropping, and pooling, based on the existing semantics. This, in turn, greatly facilitates the Addition task. The visualization of CoSegmentation is as Fig.\ref{fig:cosegm}:

% Figure environment removed

To make full use of the task information $T=\{c,t_o,t_e\}$ from Fashion Designer, AutoMasker adopts different mask schemes according to the task category $c$.

For recoloring, replacement, and removal tasks, AutoMasker utilizes LLM to judge if the semantic $s_i$ corresponding to the original part $t_o$ in $S_{co}$. If in, $m_i$ is adopted as the original part mask $m_o$ directly. If not, AutoMasker will utilize GroundingSAM to obtain the original part mask $m_o$ from $I$ and $t_i$.Therefore $m_o$ can be obtained by the following formula:

\begin{equation}
\label{eq4}
	m_o=\left\{  
             \begin{array}{lr}  
             S_{co}[s_i]=   S_{co}[LLM(t_o)],&  if\; s_i \;in\;  S_{co}\\  
             GroundingSAM(t_o),&  else\\  
             \end{array}  
\right.  
\end{equation}

For the removal task, there is no need to consider the target object or part, so its final mask can be obtained from:

\begin{equation}
\label{eq5}
    M_{remove}=MaxPool(m_o)
\end{equation}


where $MaxPool$ represents the boundary expansion of Mask by using maximum pooling, which can make local editing more consistent and coordinated with the surrounding context.

For the recoloring task, it is necessary to ensure that only the target area is recolored. Therefore, MAM is additionally used to alleviate the problem of $m_o$ boundary blur and possible overlap at the junction of the background. The process can be expressed as: 


\begin{equation}
\label{eq6}
    M_{recolor}=MAM(I) \bigodot m_o
\end{equation}

For the replacement task, besides $m_o$, other parts may also be occluded by the target object. For instance, when replacing a vest with a t-shirt, part of the arms may also be obstructed.  To address this issue, AutoMasker uses LLM to logically infer the body parts that may be masked by the target object $S_m=\{m_0^m,…,m_k^m\}$ from $S_{co}$ which are merged with $m_o$ to form a more reasonable target mask:
\begin{equation}
\label{eq7}
    S_m=LLM(S_{co}, t_e)=\{m_0^m,…,m_k^m\}
\end{equation}

\begin{equation}
\label{eq8}
    M_{replace}=MaxPool(m_o+\sum_{m_i^m\in S_m}m_i^m)
\end{equation}

For the addition task, the target object or area does not exist in $S_co$, so it is only necessary to infer the parts that the target object may mask:
\begin{equation}
\label{eq9}
    M_{add}=MaxPool(\sum_{m_i^m\in S_m}m_i^m)
\end{equation}
% Firstly, the module can use HPMs such as Graphonomy\cite{Gong2019Graphonomy} and DensePose\cite{Guler2018DensePose} to parse the input image. By combining different human body parsing semantics, finer-grained semantic masks are generated. Then, according to the editing operation and the original image information, the image is divided into 1) mask area, 2) protection area, and 3) neutral area with the help of LLM.

% AutoMasker merges masks of the Masked Regions, applying a pooling operation to ensure complete coverage without residue. The Protected Area which is not influenced by pooling preserves identifiable information about characters and non-target outfits. The Neutral Area, which has little influence on quality, is partially masked near the masked region but preserved elsewhere. This allows for greater freedom and natural background fusion in the final editing results.


\section{Experiment}
\subsection{Implementation Details}

\textbf{Visual Foundation Models.} We choose Realistic Vision V4.0 finetuned from Stable Diffusion V1.5\cite{rombach2021highresolution} as the base generator. This model can largely alleviate the unrealistic problems of characters' faces and hands. The SoftEdge and Inpainting variants of ControlNet v1.1\cite{zhang2023Controlnet} are employed for conditional control purposes. We employ BLIP\cite{li2022blip} for visual question answering.

\textbf{Semantic Segmentation Models.} To obtain human-centric dense predictions, we first employ Graphonomy\cite{Gong2019Graphonomy} and DensePose\cite{Guler2018DensePose}. Subsequently, we utilize Grounded-SAM\cite{kirillov2023segany, liu2023grounding} and MattingAnything\cite{li2023matting} to facilitate open-vocabulary segmentation acquisition and edge refinement.

\textbf{Large Language Models.} As the logical reasoning and dialogue with users of the system is supported by LLMs, we have conducted a series of evaluations on various open-source LLMs configured with distinct parameter level, encompassing FastChat-T5-3B\cite{zheng2023judging}, ChatGLM-6B\cite{zeng2022glm}, ChatGLM2-6B\cite{zeng2022glm}, Vicuna-7B\cite{zheng2023judging}, Vicuna-13B\cite{zheng2023judging}, and Baichuan-13B-Chat\cite{Baichuan-13B}.



% \subsection{SetUp}

% We have employed the LLMs using ChatGPT with the "text-davinci-003" version for online interactions, and utilize ChatGLM\cite{du2022glm} with the "chatglm-6b" version for offline purposes. We have BLIP\cite{li2022blip}, Stable Diffusion (finetuned for inpainting), and ControlNet\cite{zhang2023Controlnet} as Visual Foundation Models. The full deployment of all the ChatGLM and VFMs requires 2 Nvidia 3090 GPUs. However, if the ChatGPT interface is employed, the system can be deployed on a single Nvidia 3090 GPU.

\subsection{Comparison with Text-to-Image Baselines}
Currently, there is a lack of a text-based fashion editing method for images. In this regard, we compare our system with two existing text-based try-on approaches: Text2Human\cite{jiang2022text2human} and FICE\cite{pernuvs2023fice}. Text2Human is capable of generating try-on results based on the pose and parsing conditions. However, it should be noted that text-based try-on constitutes only a minor component of the functionalities offered by Fashion Matrix.
In our comparative analysis, we observed that Fashion Matrix outperformed these two methods in terms of CLIP Score\cite{radford2021clipscore} and IS\cite{salimans2016improved_is}.
Furthermore, we conducted a human evaluation to assess the naturalness and text-image matching of the generated images. For each criterion, we randomly assembled 30 sets of result images, and volunteers were tasked with selecting the option that appeared more natural or exhibited superior text-image matching. We obtained 25 responses for assessing naturalness and 20 responses for evaluating text-image matching.
The evaluation showed in Table \ref{tab:quant_comparison} demonstrates that Fashion Matrix produces outputs with improved realism, naturalness, and adherence to text descriptions.
% % Figure environment removed

% % Figure environment removed


% \begin{table}
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{l|cccc}
%     \toprule
%     Method                   & CLIPScore\uparrow & IS\uparrow & Naturalness\uparrow & Text-Image Matching\uparrow \\
%     \midrule
%     FICE                     & 23.74             & 2.54        & -    & -  \\
%     Text2Human(from parsing) & 26.49             & 3.10        &      &    \\
%     Text2Human(from pose)    & 26.63             & 3.04        &      &    \\
%     Ours                     & 27.78             & 3.14        &      &    \\
%     \bottomrule
%   \end{tabular}
% \end{table}

\begin{table}[]
    \centering
    \caption{\textbf{Quantitative comparison} with Text2Human\cite{jiang2022text2human} and FICE\cite{pernuvs2023fice}.In addition to assessing the CLIP Score\cite{radford2021clipscore} and Inception Score (IS)\cite{salimans2016improved_is}, we conducted evaluations on the naturalness and text-image matching. Our system possess advantages across all these metrics.}
    \vspace{3mm}
    \begin{tabular}{c|cccc}
        \toprule
        \multirow{2}{*}{Method} & \multicolumn{1}{c}{\multirow{2}{*}{CLIP Score$\uparrow$}} & \multicolumn{1}{c}{\multirow{2}{*}{IS$\uparrow$}} & \multicolumn{2}{c}{Human Evalution} \\ \cline{4-5} 
                                  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & Naturalness & Text-Image Matching \\ \midrule
        FICE                      & 23.74                & 2.54                 & -           &   -                  \\
        Text2Human (from parsing) & 26.49                & 3.10                 & 23.33\%     &  28.13\%             \\
        Text2Human (from pose)    & 26.63                & 3.04                 & 21.33\%     &  25.20\%              \\
        Ours                      & \textbf{27.78 }      & \textbf{3.14}        & \textbf{55.33\%}     & \textbf{ 46.67\%}          \\
        \bottomrule
    \end{tabular}
    \label{tab:quant_comparison}
\end{table}



\subsection{Ablation Studies}
In our study, we employed ChatGPT\cite{InstructGPT} to generate requirements with limited examples from users. Subsequently, we classified these instances into three distinct categories: single-task, dual-task, and multi-task requirements, comprising 100, 70, and 50 instances, respectively. In task classification, we exclusively employ the set of 100 single-task requirements. We proceeded to manually rectify the task splitting and classification results generated by LLMs, utilizing these cases as a benchmark for evaluating the efficacy of various LLMs in handling fashion-related tasks.
% We used ChatGPT\cite{InstructGPT} as an assistant to generate requirements with limited examples from users. These examples are then categorized these into single-, dual-, and multi-task instructions, totaling 100, 70, and 50, respectively.
% Then, we manually corrected the task splitting and classification results generated by LLMs. These cases were used to assess the ability of different LLMs for dealing with fashion tasks.
We conduct evaluations on the 6 aforementioned open-source LLMs without individually optimizing prompts for each model. Nevertheless, it is essential to acknowledge that the stochastic nature of LLMs and the inherent bias towards specific prompts may lead to the fact that our evaluation results do not fully reflect the capabilities of LLMs.
% which are represented as \textit{text-davinci-003}, \textit{chatglm-6b} and \textit{oasst-sft-6-llama-30b} respectively.

As shown in Table \ref{ablation-table}, for task classification, there exists a positive correlation between the accuracy rate and the number of model parameters. Specifically, both the 13B models attained an accuracy rate exceeding 75\%, whereas the performance of the remaining models was comparatively lackluster.
However, in the case of task splitting, it is noteworthy that the accuracy rate does not exhibit a significant correlation with the number of model parameters. For instance, despite having relatively less parameters, Vicuna-7B\cite{zheng2023judging} and FastChat-T5-3B\cite{zheng2023judging} demonstrated impressive performance. Conversely, Baichuan-13B-Chat\cite{Baichuan-13B} struggled to produce accurate results.
Overall, considering all the factors, Vicuna-13B\cite{zheng2023judging} emerges as the most suitable option for supporting Fashion Matrix.
% the accuracy of each model in editing instruction partition and mask selection is positively correlated with its parameter quantity. However, during the experiment, we found that ChatGPT is more stable than the other two models for output formats (such as the standard list format when dividing instructions), especially the failure case of OpenAssistant is usually caused by the mismatch of output formats.



\begin{table}
  \caption{\textbf{Accuracy comparison} of different LLMs for Task Splitting and Classification.For Task Splitting, we divide the test cases into single-task, dual-task, and multi-task requirements, which are represented by 1, 2 and 3 + in the table respectively.}
  \label{ablation-table}
  \centering
  \begin{tabular}{c|ccccc}
    \toprule
    \multirow{2}{*}{LLM} & \multicolumn{4}{c}{Task Splitting} & \multicolumn{1}{c}{\multirow{2}{*}{Task Classification}}\\ 
    \cline{2-5} 
    \multicolumn{1}{c|}{} & 1 & 2 & 3+ & Average & \multicolumn{1}{c}{}  \\ 
    \midrule
    % LLM            &  1 operation   & 2 operation & 3+operation     & Average     & Mask Selection\\
    Vicuna-13B\cite{zheng2023judging}   & 73.00\%       & 87.14\%     & 78.00\%         & 78.64\%     &\textbf{78.00\%}  \\
    Baichuan-13B-Chat\cite{Baichuan-13B}& 10.00\%       & 64.29\%     & 10.00\%         & 27.27\%     &75.00\%  \\
    Vicuna-7B\cite{zheng2023judging}    & 86.00\%       & \textbf{94.29\%} & \textbf{88.00\%} & \textbf{89.09\%}    &45.00\%  \\
    ChatGLM-6B\cite{zeng2022glm}        & 70.00\%       & 81.43\%     & 78.00\%         & 75.45\%     &60.00\%  \\
    ChatGLM2-6B\cite{zeng2022glm}       & 75.00\%       & 65.71\%     & 62.00\%         & 69.09\%     &42.00\%  \\
    FastChat-T5-3B\cite{zheng2023judging}& \textbf{87.00\%} & 81.43\%     & 86.00\%         & 85.00\%     &53.00\%  \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Conclusion}
In this work, we propose Fashion Matrix, a multi-round dialogue AI system that integrates LLM, Visual Foundation Models, and Semantic Segmentation Models to realize text-guided fashion editing tasks with open vocabulary. 
This system innovatively integrates various Semantic Segmentation Models to construct a  more detailed semantic map called CoSegmentation and adaptively generates task-specific masks for the editing area, which effectively address complex text-guided fashion editing tasks.
% This system creatively combines multiple Semantic Segmentation Models to form a new finer-grained semantic map-CoSegmentation, and adaptively generates task-specified masks of editing area which are used to tackle intricate text-guided fashion editing tasks.
Extensive experiment and specific cases have demonstrated the remarkable potential and proficiency of Fashion Matrix in various fashion editing tasks.
% However, the absence of an LLM specifically tailored for semantic comprehension in the fashion domain, coupled with the limitation of SSMs, imposes bad influence on the capability of Fashion Matrix to effectively handle intricate and specialized editing instructions.
Nevertheless, the optimization of LLMs specifically for the fashion domain, alongside the development of detailed Semantic Segmentation Models for both human subjects and fashion items, harbors considerable potential in elevating and broadening the functionalities of the Fashion Matrix.

% \bibliographystyle{plain}
% \bibliography{refs}
\begin{thebibliography}{10}

\bibitem{openpose}
Z.~{Cao}, G.~{Hidalgo Martinez}, T.~{Simon}, S.~{Wei}, and Y.~A. {Sheikh}.
\newblock Openpose: Realtime multi-person 2d pose estimation using part
  affinity fields.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2019.

\bibitem{choi2021vitonhd}
Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo.
\newblock Viton-hd: High-resolution virtual try-on via misalignment-aware
  normalization.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 14131--14140, 2021.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{mmpose2020}
MMPose Contributors.
\newblock Openmmlab pose estimation toolbox and benchmark.
\newblock \url{https://github.com/open-mmlab/mmpose}, 2020.

\bibitem{alphapose}
Hao-Shu Fang, Jiefeng Li, Hongyang Tang, Chao Xu, Haoyi Zhu, Yuliang Xiu,
  Yong-Lu Li, and Cewu Lu.
\newblock Alphapose: Whole-body regional multi-person pose estimation and
  tracking in real-time.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2022.

\bibitem{fu2022stylegan}
Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen~Change Loy,
  Wayne Wu, and Ziwei Liu.
\newblock Stylegan-human: A data-centric odyssey of human generation.
\newblock In {\em European Conference on Computer Vision}, pages 1--19.
  Springer, 2022.

\bibitem{Gong2019Graphonomy}
Ke~Gong, Yiming Gao, Xiaodan Liang, Xiaohui Shen, Meng Wang, and Liang Lin.
\newblock Graphonomy: Universal human parsing via graph transfer learning.
\newblock In {\em CVPR}, 2019.

\bibitem{gong2018instance-cihp}
Ke~Gong, Xiaodan Liang, Yicheng Li, Yimin Chen, Ming Yang, and Liang Lin.
\newblock Instance-level human parsing via part grouping network.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 770--785, 2018.

\bibitem{Gong_2017_CVPR-ssl}
Ke~Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen, and Liang Lin.
\newblock Look into person: Self-supervised structure-sensitive learning and a
  new benchmark for human parsing.
\newblock In {\em The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, July 2017.

\bibitem{Guler2018DensePose}
R{\i}za~Alp G{\"u}ler, Natalia Neverova, and Iasonas Kokkinos.
\newblock Densepose: Dense human pose estimation in the wild.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7297--7306, 2018.

\bibitem{jiang2022text2human}
Yuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu, Chen~Change Loy, and Ziwei Liu.
\newblock Text2human: Text-driven controllable human image generation.
\newblock {\em ACM Transactions on Graphics (TOG)}, 41(4):1--11, 2022.

\bibitem{kirillov2023segany}
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
  Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C. Berg, Wan-Yen Lo, Piotr
  Doll{\'a}r, and Ross Girshick.
\newblock Segment anything.
\newblock {\em arXiv:2304.02643}, 2023.

\bibitem{li2020manigan-cot}
Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip~HS Torr.
\newblock Manigan: Text-guided image manipulation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 7880--7889, 2020.

\bibitem{li2023matting}
Jiachen Li, Jitesh Jain, and Humphrey Shi.
\newblock Matting anything.
\newblock {\em arXiv: 2306.05399}, 2023.

\bibitem{li2019crowdpose}
Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, and Cewu Lu.
\newblock Crowdpose: Efficient crowded scenes pose estimation and a new
  benchmark.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 10863--10872, 2019.

\bibitem{li2022blip}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified
  vision-language understanding and generation.
\newblock In {\em ICML}, 2022.

\bibitem{liang2018look-JPPnet}
Xiaodan Liang, Ke~Gong, Xiaohui Shen, and Liang Lin.
\newblock Look into person: Joint body parsing \& pose estimation network and a
  new benchmark.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2018.

\bibitem{liu2023grounding}
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan
  Li, Jianwei Yang, Hang Su, Jun Zhu, et~al.
\newblock Grounding dino: Marrying dino with grounded pre-training for open-set
  object detection.
\newblock {\em arXiv preprint arXiv:2303.05499}, 2023.

\bibitem{babyagi3}
Yohei Nakajima.
\newblock Babyagi.
\newblock \url{https://github. com/yoheinakajima/babyagi}, 2023.

\bibitem{gpt-engineer}
Anton Osika.
\newblock Gpt engineer.
\newblock \url{https://github.com/AntonOsika/gpt-engineer}, 2023.

\bibitem{InstructGPT}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke~E. Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul~Francis Christiano, Jan Leike, and Ryan~J. Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock {\em ArXiv}, abs/2203.02155, 2022.

\bibitem{pernuvs2023fice}
Martin Pernu{\v{s}}, Clinton Fookes, Vitomir {\v{S}}truc, and Simon
  Dobri{\v{s}}ek.
\newblock Fice: Text-conditioned fashion image editing with guided gan
  inversion.
\newblock {\em arXiv preprint arXiv:2301.02110}, 2023.

\bibitem{radford2021clipscore}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International conference on machine learning}, pages
  8748--8763. PMLR, 2021.

\bibitem{richards2023auto}
Toran~Bruce Richards.
\newblock Auto-gpt: An autonomous gpt-4 experiment, 2023.

\bibitem{rombach2021highresolution}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models, 2021.

\bibitem{salimans2016improved_is}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training gans.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{shen2023hugginggpt}
Yongliang Shen, Kaitao Song, Xu~Tan, Dongsheng Li, Weiming Lu, and Yueting
  Zhuang.
\newblock Hugginggpt: Solving ai tasks with chatgpt and its friends in
  huggingface.
\newblock {\em arXiv preprint arXiv:2303.17580}, 2023.

\bibitem{su2021pidi}
Zhuo Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao, Qi~Tian, Matti
  Pietik{\"a}inen, and Li~Liu.
\newblock Pixel difference networks for efficient edge detection.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 5117--5127, 2021.

\bibitem{Baichuan-13B}
Baichuan~Intelligent Technology.
\newblock Baichuan-13b.
\newblock \url{https://github.com/baichuan-inc/Baichuan-13B}, 2023.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
  Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{wang2018toward-cp}
Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin Chen, Liang Lin, and Meng Yang.
\newblock Toward characteristic-preserving image-based virtual try-on network.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 589--604, 2018.

\bibitem{wu2023visual}
Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan
  Duan.
\newblock Visual chatgpt: Talking, drawing and editing with visual foundation
  models.
\newblock {\em arXiv preprint arXiv:2303.04671}, 2023.

\bibitem{xie2022pasta}
Zhenyu Xie, Zaiyu Huang, Fuwei Zhao, Haoye Dong, Michael Kampffmeyer, Xin Dong,
  Feida Zhu, and Xiaodan Liang.
\newblock Pasta-gan++: A versatile framework for high-resolution unpaired
  virtual try-on.
\newblock {\em arXiv preprint arXiv:2207.13475}, 2022.

\bibitem{yao2022react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
  and Yuan Cao.
\newblock React: Synergizing reasoning and acting in language models.
\newblock {\em arXiv preprint arXiv:2210.03629}, 2022.

\bibitem{zeng2022glm}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
  Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et~al.
\newblock Glm-130b: An open bilingual pre-trained model.
\newblock {\em arXiv preprint arXiv:2210.02414}, 2022.

\bibitem{zhang2022humandiffusion}
Kaiduo Zhang, Muyi Sun, Jianxin Sun, Binghao Zhao, Kunbo Zhang, Zhenan Sun, and
  Tieniu Tan.
\newblock Humandiffusion: a coarse-to-fine alignment diffusion framework for
  controllable text-driven person image generation.
\newblock {\em arXiv preprint arXiv:2211.06235}, 2022.

\bibitem{zhang2023Controlnet}
Lvmin Zhang and Maneesh Agrawala.
\newblock Adding conditional control to text-to-image diffusion models, 2023.

\bibitem{zhang2022automatic-cot}
Zhuosheng Zhang, Aston Zhang, Mu~Li, and Alex Smola.
\newblock Automatic chain of thought prompting in large language models.
\newblock {\em arXiv preprint arXiv:2210.03493}, 2022.

\bibitem{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
  Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric.~P Xing, Hao Zhang, Joseph~E.
  Gonzalez, and Ion Stoica.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

\bibitem{zhu2023tryondiffusion}
Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, Chitwan Saharia,
  Mohammad Norouzi, and Ira Kemelmacher-Shlizerman.
\newblock Tryondiffusion: A tale of two unets.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 4606--4615, 2023.

\end{thebibliography}

\newpage

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed



\end{document}