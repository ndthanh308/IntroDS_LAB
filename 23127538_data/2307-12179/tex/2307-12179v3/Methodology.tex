\section{Methodology}
\label{sec:method}
% Figure environment removed


%\vspace*{0.2cm}\noindent\textbf{Problem formulation.} 
% Let $O$ be the set of Objects,  $S$ be the set of
%  States and $\textit{I}$ the set of
%   Images which consists of the disjoint sets ${I^{S}}$ and ${I^{U}}$ that are used during the training and testing phase respectively. 
%    Each image $i_{k} \in \textit{I}$  contains an object $o_{i} \in O$ which  is situated in a state $s_{j} \in S$. The OSC task deals with the yielding  of a  predicted state label $sp_{j} \in S$ for an image $i_{k} \in {I^U}$ that has been given as an input. In the zero-shot variation of OSC, ${S^{S}} 
%   \not \supseteq {S^{U}}$, i.e. some of the states contained in the testing images do not appear in the training images. 
% Let $O$ denote a set of objects, 
% \textcolor{red}{$S^S$ as the set of known object states found in the training images, $S^U$ as the test, yet unknown, object state labels} and $I$ the set of images, which is partitioned into the training set $I^T$ and the test set $I^U$. 
% % Each image $i \in I$ contains an object $o \in O$ in a state $s \in S$. 
% \textcolor{red}{Each image $i \in I^T$ contains an object $o \in O$ in a state $s \in S^S$, while an image $i \in I^U$ contains an object $o \in O$ in any state $s \in S = \{S^S \cup S^U\}$. }
% The goal of OSC is to predict the state $s \in S$, given the object $o$ in $i \in I^U$. In the zero-shot variation of OSC, the set of states observed in the test images $S^U$ is not a subset of the set of states observed in the training images $S^S$, i.e., there exists some states in the test image set that do not appear in the training set. Furthermore, the task should be addressed in an object-agnostic manner, i.e. no information concerning the object classes is to be utilized explicitly.  However,  although the set of object classes does not directly affect the task of OaSC, its size is proportional to the complexity of the problem. 
% The workflow of the proposed method is shown in \autoref{fig:pipeline}.

Let $O$ denote a set of objects, $S$ denote the set of states and $I$ denote the set of images, which is partitioned into the training set $I^T$ and the testing set $I^U$. Each image $i \in I$ contains an object $o \in O$ in a state $s \in S$. 
The goal of OSC is to predict the state $s \in S$, given the object $o$ in $i \in I^U$. In the zero-shot variation of OSC, the set of states observed in the test images $S^U$ is not a subset of the set of states observed in the training images $S^S$, i.e., there exists some states in the test image set that do not appear in the training set. Furthermore, the task should be addressed in an object-agnostic manner, i.e. no information concerning the object classes is to be utilized explicitly.  However,  although the set of object classes does not directly affect the task of OaSC, its size is proportional to the complexity of the problem. 
The workflow of the proposed method is shown in \autoref{fig:pipeline}.




% and will be analyzed in the following sections.

% \vspace*{0.2cm}\noindent\textbf{Approach.}

\subsection{Overview}
% Our method is inspired by works that address the problem of zero-shot object classification \cite{}. The main idea behind this line of work is that the necessary information for the classification of the unseen classes can be found in a Knowledge Graph (KG) if processed appropriately by a Graph Neural Network (GNN). Obviously, the most crucial component of this approach lies in the combination of the visual information stemming from the training images and referring to the seen classes with the semantic information stemming from the KG  and referring to the unseen classes.

We are inspired by prior research on zero-shot object classification and leverage the potential of KGs and GNNs to classify previously unseen objects~\cite{Kampffmeyer2019,nayak:tmlr22}. 
The core idea is that semantic information that is stored in the KG can be used by GNNs to learn graph embeddings that can be utilized jointly with visual information extracted from training images. 
This enables the model to generalize to new object classes by leveraging the semantic and contextual information encoded in the graph embeddings of the KG.

% More in detail, the GNN architecture is adopted to the architecture of the Classifier  that is used for the training on seen classes, the GNN last layer has the same size  with the Classifier last layer. This way the GNN can produce semantic embedding features that correspond to all the classes, both seen and unseen, that will be encountered during the inference. These embedding features  replace the last layer of the Classifier. Holding this layer fixed, the body of the Classifier is then fine-tuned with the training images.

GNNs are designed to operate on graph-structured data, such as KGs~\cite{kipf2016semi,Monka2022}. KGs are typically represented as labeled multi-graphs, where nodes correspond to entities, and edges represent entity relationships. GNNs process this graph by iteratively aggregating information from neighboring nodes, using neural network-based operations.

At each iteration, a GNN receives a feature vector for each graph node, which is initially set to the node's embedding vector. Then, the GNN performs a message-passing step that aggregates information from neighboring nodes, based on the edge weights and the features of the nodes. This message-passing operation can be formulated as a neural network layer, which applies a learnable function to the features of the neighboring nodes and returns an aggregated message for each node. After the message-passing step, the GNN updates the node features by applying a learnable transformation that takes into account the original features of the node and the received messages from its neighbors. This updated feature vector is then passed to the next iteration of the message-passing step. The process continues until a fixed number of epochs or convergence.
%%%AAA: Endexetai na mas rethrown gia tis times aytwn twn parametrwn?
% KP edw anaferetai genika mia diadikasia GNN training. Na anaferoume edw times parametrwn h sto 4 - see implementation details ?

The proposed method leverages GNN training using a visual classifier that is trained on seen state classes as supervision. In particular, the last layer of the GNN is designed to have the same size as the last layer of the classifier. This enables the GNN to generate semantic embedding features that correspond to all classes, including both seen and unseen classes that will be encountered during inference. Subsequently, the semantic embedding features replace the last layer of the classifier while this layer is kept fixed. The body of the classifier is then fine-tuned with the training images to optimize the overall model for state recognition.

% \vspace*{0.2cm}\noindent\textbf{GNN Details.} 
Overall, we experimented with four different model architectures and opted for the Transformer Graph Convolutional network (Tr-GCN)~\cite{nayak:tmlr22}. Further details are provided in Section~\ref{sec:abl} and the supplementary material of this work. 
The Tr-GCN mode is capable of combining input sets non-linearly by utilizing multilayer perceptrons and self-attention. Tr-GCN refers to an inductive model that can learn node representations by aggregating local neighborhood features allowing the trained model to make predictions on new graph structures without retraining. 
We leverage the aforementioned property of the Tr-GCN to train a permutation invariant non-linear aggregator that captures the intricate structure of a common sense knowledge graph. 
% , rendering it well-suited for zero-shot learning. 
% It is worth noting that a similar network architecture has been effectively employed for zero-shot object classification~\cite{nayak:tmlr22}.

% A critical aspect of the proposed method involves calibrating the weights of the GNN in a manner that its predictions in the semantic space are useful for the classifier deployed in the visual space. To accomplish this, we adopt an approach based on prior research \cite{Kampffmeyer2019, Wang2018b, nayak:tmlr22} that involves learning the semantic class representations by minimizing the L2 distance between the learned class representations and the weights of a fully connected layer in a ResNet classifier pre-trained on the ILSVRC 2012 dataset \cite{russakovsky2015imagenet}. Once the class representations are learned, we fix them and fine-tune the ResNet backbone using the training images from the dataset.




% \vspace*{0.2cm}\noindent\textbf{Building of the KG.}
% The KG is created by the querying  of a common sense repository. The repositories that we are ConceptNet \cite{} and WordNet\cite{}. The procedure takes place as follows. Initially we create a set of nodes that correspond to the target stace classes. Subsequently, the repository is queried for each of these nodes and its neighbours in the repository of  added to the KG if  certain criteria are met (see ablation section for more details). This procedure is repeated for the newly added nodes and henceforth until a number of hops has been reached.  

\subsection{The proposed OaSC approach}
\label{sec:pipeline}
Overall, the proposed method consists of four stages, as shown in \autoref{fig:pipeline}: (1) construction of the KG, (2) GNN training and learning of semantic graph embeddings, (3) fine-tuning of the visual classifier and (4) deployment of the fine-tuned state classifier.

\vspace*{0.0cm}\noindent\textbf{Construction of the KG (Stage 1)}:
To create the KG, we query a common sense repository to compile a generic solution and to avoid the construction of a task-specific KG, tailored to the entities at hand and their relationships. First, a set of nodes that correspond to the words of the target state classes $S^U$ and $S^S$ is generated. Then, we query the repository for each of these nodes and add their neighbors in the KG, if they meet specific criteria (see also Section~\ref{sec:abl}). This process is repeated for the newly added nodes until a specified number of node hops is reached.

This technique for building a generic KG offers several advantages in comparison to other problem-specific approaches. First, it allows the same KG to be used for different variations of the task. It also enables transfer learning since KGs can be reused to tackle other related problems. Moreover, the construction of such a KG does not rely on expert knowledge. Besides, the structured representation of relationships between entities and concepts that KGs provide can be leveraged to generate robust embeddings for zero-shot learning.
% which is expensive and time-consuming.  
The trade-off is that such KGs are prone to noisy information in the used repositories. 

% In comparison, language models, such as BERT~\cite{devlin2018bert}, often rely on large amounts of unstructured text data to generate embeddings. While language models are highly effective at capturing semantic relationships between words and phrases, they can also be prone to create associations between concepts that are not actually related. This can lead to noisy or unreliable embeddings, which can in turn degrade the performance of zero-shot learning models. By contrast, the structured nature of KGs allows for more accurate and precise capture of relationships between entities and concepts, leading to more robust embeddings that can improve the accuracy and reliability of zero-shot learning models~\cite{brown2020language}.


\vspace*{0.0cm}\noindent\textbf{Computation of  Graph Embeddings (Stage 2)}:
% Given the KG constructed in Stage 1, a word features embedding matrix corresponding to the KG nodes is created by utilizing the pre-computed word features of GloVe~\cite{pennington2014glove}. 
% % Subsequently,  random walks are performed in the KG and a sample of neighbors for each node is obtained.
% By taking the word features embedding matrix, the KG topology, and a target node as inputs, the GNN estimates the node's embeddings: the features of the node and its neighbors are  aggregated and submitted to a series of convolutions and pooling operations before the  output is produced in the form of a feature vector, the length of which is tailored to be the same as the size dimension of the last layer of a ResNet-101 classifier. 
% This procedure is repeated for all KG nodes and results in the computation of the semantic embeddings for all target state classes with each embedding being a feature vector of length equal to 2048. By combining these embeddings for the \mathcal{d} target classes  a  $ d \times 2048$ features matrix is created which serves as the last layer of a CNN classifier that is utilized during Stages 3 and 4.
% which serves as the last layer of a CNN classifier that is utilized during Stages 3 and 4 .
% We employ an established approach~\cite{Kampffmeyer2019, Wang2018b} that involves training of a transformer-based Graph Convolutional Model using graph embeddings of a set of semantic entities acquired by a common sense repository by minimizing the L2 distance between the learned class representations and the weights of a fully connected layer in a ResNet classifier, pre-trained on the ILSVRC 2012 dataset~\cite{russakovsky2015imagenet}, ensuring that the semantic class representations are meaningfully embedded.
We employ an established approach~\cite{Kampffmeyer2019, Wang2018b} that involves the training of a transformer-based Graph Convolutional Network (GCN)
 \textcolor{black}{ that utilizes a KG as input  %Training is performed %using features of a set of semantic entities acquired by a common sense repository, \textcolor{red}{(e.g. the ConceptNet, CSKG, or other)}  
 and generates an embedding vector for each node of the  KG. %. For the production of the embeddings vectors the GCM employs a sequence of transformations to the semantic features that correspond to the concepts linked to each node.
This process defines pre-computed GloVe word, i.e. semantic features~\cite{pennington2014glove}, for the KG nodes with each node representing a concept class.
% To compute node embeddings, the GNN is applied to encode the KG topology and the word feature embedding matrix. 
The GNN  aggregates each node's and its neighbors' features through a sequence of convolutions and pooling operations. %This results in the generation of a feature vector having a length equal to the dimension of the last layer in a visual CNN-based classifier that is instantiated using a ResNet-101 model. 
%By pre-training the visual classifier in a set of target classes 
The visual classifier is pre-trained on a set of target classes and using the weights of its fully connected layer, the GCN learns to produce visual feature representations, i.e. visual embeddings,  corresponding to the concept classes of the KG`s nodes.}
\textcolor{black}{
Formally,  the training involves the minimization of the L2 distance   $\mathcal{L_G}$ between the generated visual embeddings and the ground truth visual embeddings stemming from the visual classifier.} 
\textcolor{black}{In notation, 
\begin{equation}
\mathcal{L_G} = \frac{1}{2N} \sum_{n \in N} \sum_{p \in P} (W_{n,p} - \tildea{W}_{n,p})^2,
 \end{equation}
where $\tildea{W} \in \mathbb{R}^{|N|xP}$ denotes the weights of the GCN for the set of known concept classes $N$ and the dimensionality $P$ of the weight vector. Similar to~\cite{Kampffmeyer2019}, the ground truth weights, denoted as $W \in \mathbb{R}^{|N|xP}$, are obtained by extracting the last layer weights of a pre-trained CNN.}
% This process is repeated for all KG nodes corresponding to $S^U$ and $S^S$, generating semantic graph embeddings for all target state classes. 
%Each embedding comes in the form of a feature vector of length 2048. 


%By combining these embeddings for the $d$ target classes, a  $d \times 2048$ features matrix is defined that is integrated as the final layer of the visual CNN-based classifier that is employed in Stages~3-4.
%A critical aspect of this process is adjusting the GNN weights to align its predictions with the semantic space. This ensures that the semantic embeddings effectively aid the classifier used in Stages 3 and 4, operating in the visual space. 



\textcolor{black}{ 
The KG  given as an input to the GCN model is a hierarchical graph created for the requirements of the   ILSVRC 2012 dataset~\cite{russakovsky2015imagenet} and represents the WordNet hierarchical structure of the $1,000$ classes comprising the dataset. These 1,000 concept labels constitute the set of classes upon which the visual classifier used for the extraction of the ground truth visual embeddings is pre-trained.
}
% A critical aspect of this process is adjusting the GNN weights to align its predictions with the semantic space. This ensures that the semantic embeddings effectively aid the classifier used in Stages 3 and 4, operating in the visual space. 
% The concepts  that are used for the training refer to a set of 1K object classes of the ILSVRC 2012 dataset~\cite{russakovsky2015imagenet}, while the pre-trained ResNet101-based classifier is used for supervision to ensure that the GNN outputs, thus the semantic object class representations, are meaningfully embedded into the visual feature space. 
After the training is completed, the GCN model is employed to process the KG (constructed in Stage 1) and generate visual embeddings for the KG nodes that correspond to the object state classes,  by taking as input the  KG that was constructed during Stage 1. Each embedding comes in the form of a feature vector of length 2048, i.e. dimension of the last layer of the  pre-trained visual CNN-based classifier.
By combining these embeddings for the $d$ target classes, a  $d \times 2048$ features matrix is defined that is integrated as the final layer of the visual CNN-based classifier that is employed in Stages~3-4.
% First, using their pre-computed GloVe word features~\cite{pennington2014glove}, a matrix of word, i.e. semantic, features embeddings is defined for each of the KG nodes.
% % To compute node embeddings, the GNN is applied to encode the KG topology and the word feature embedding matrix. 
% Subsequently,  the GNN takes as input every target node that corresponds to any class in $S^U$ and $S^S$ and aggregates the features about the node and its neighbors through a sequence of convolutions and pooling operations. This results in the generation of a feature vector having a length equal to the dimension of the last layer in the visual CNN-based classifier that is instantiated using a ResNet-101 model.
% % This process is repeated for all KG nodes corresponding to $S^U$ and $S^S$, generating semantic graph embeddings for all target state classes. 
% Each embedding comes in the form of a feature vector of length 2048. By combining these embeddings for the $d$ target classes, a  $d \times 2048$ features matrix is defined that is integrated as the final layer of the visual CNN-based classifier that is employed in Stages~3-4.
% A critical aspect of this process is adjusting the GNN weights to align its predictions with the semantic space. This ensures that the semantic embeddings effectively aid the classifier used in Stages 3 and 4, operating in the visual space. 

% A critical aspect of this procedure involves calibrating the weights of the GNN to embed its predictions in the semantic space, i.e. semantic embeddings, are useful for the classifier deployed in the visual space during Stages 3 and 4. To accomplish this, we adopt an approach based on prior research~\cite{Kampffmeyer2019, Wang2018b} that involves learning the semantic class representations by minimizing the L2 distance between the learned class representations and the weights of a fully connected layer in a ResNet classifier pre-trained on the ILSVRC 2012 dataset~\cite{russakovsky2015imagenet}.  

\vspace*{0.0cm}\noindent\textbf{Fine-tuning of the Visual Classifier (Stage 3)}:
The estimated semantic embeddings are integrated into a visual CNN classifier that relies on the ResNet backbone and is initially pre-trained for object classification. The embeddings serve as the final layer of the network, encapsulating the representations essential for predicting the train state classes $S^S$. To enable this adaptation, the visual classifier undergoes re-training, specifically tailored to the classification of the train classes. 
During this fine-tuning process, input images $I^T$ contain states sourced exclusively from the training set $S^S$, i.e. ``seen states''. The primary objective is to harness the classifier capabilities to classify these familiar states, accurately. Notably, fine-tuning involves keeping the weights of the last layer fixed, safeguarding the integrity of the acquired semantic representations from Stage 2. Consequently, adjustments are only applied to the weights of preceding layers to ensure they effectively match the ``frozen'' last-layer weights.
% Apart from this detail, the procedure takes place in the same manner as the training of a CNN classifier.
% in every training epoch a loss is computed the value of which guides the update of all layers weights except the last one. 
Following the notation introduced 
\textcolor{black}{in the beginning of Section~\ref{sec:method}, the loss function is defined as:}
\begin{equation}
% \mathcal{L} = -\sum_{i \in S^{S}} y_i \cdot \log(P(y=i|X))
\mathcal{L_V} = -\sum_{s \in S^S, i \in I^{T}} y_s \cdot \log(P(s|i)),
 \end{equation}
\textcolor{black}{for the predicted \textit{$y_s$} state label in the \textit{$S^S$} set of state labels. $P(s|i)$ denotes the probability of state label \textit{s} based on the softmax vector given an image \textit{i} from the $I^T$ training set.}

\noindent\textbf{Zero-shot OaSC (Stage 4)}:
Upon the completion of fine-tuning, the visual state classifier can be utilized for  prediction by choosing the most likely class
\begin{equation}
% \^y = \arg\max_{i \in S} \left( P(y=i|X) \right)
\hat{y} = \arg\max_{s \in S^U i \in I^{U}} \left( P(s|i) \right),
\end{equation}
\textcolor{black}{where $I^U$ denotes the test image set and $S^U$ the test state classes respectively.} 
We highlight that the classifier is well-suited for predicting either only unseen classes, i.e. zero-shot classification, or both seen and unseen classes, i.e. generalized zero-shot classification.
\vspace{-.15cm}
% \subsection{Pipeline}

% Overall, the pipeline of our method consists of four stages (\autoref{fig:pipeline}}). During \textbf{Stage 1}, the KG is constructed.

% \vspace*{0.2cm}\noindent\textbf{Construction of the KG (Stage 1)}:
% The KG creation process involves querying a common sense repository to enable generalization instead of creating a custom KG tailored to specific entities and relationships. Initially, nodes corresponding to the target state classes are generated. The repository is then queried for each node, and neighbors meeting specific criteria are added to the knowledge graph. This process continues for the newly added nodes until a specified number of hops is reached. More details can be found in the ablation section.


% \vspace*{0.2cm}\noindent\textbf{Computation of semantic embeddings (Stage 2)}:


% \vspace*{0.2cm}\noindent\textbf{Finetuning of the Classfier (Stage 3)}:

% \vspace*{0.2cm}\noindent\textbf{Deployment  (Stage 4)}: