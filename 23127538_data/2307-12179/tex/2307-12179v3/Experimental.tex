\section{Experimental Evaluation}

\newcommand{
  \pbt}{\color{blue}{}
}

% \begin{table}[t]

%   \resizebox{\columnwidth}{!}{
% \centering
%     \begin{tabular}{|l|rrr|rrrrrr|}

% \hline
% \textbf{Dataset} & \textbf{TR}  &  \textbf{V}   &  \textbf{TE} & \textbf{S} & \textbf{U} & \textbf{O} & \textbf{VO} & \textbf{TO} & \textbf{A} \\ \hline \hline

% OSDD \cite{gouidis2022} &   6,977 & 1,124  & 5,275 & 5 & 4 & 14 & 35 &126 &2.36 \\ \hline 
% CGQA \cite{Mancini2022} &   509 & 65 & 806 & 2 & 3 & 18  &41 & 90 & 1.71  \\ \hline
% MIT \cite{Isola2015} &  171 & 42 & 349 & 2 & 3 & 14 & 20 &  70  & 1.57 \\ \hline
%  VAW \cite{Pham2021CVPR} &  2,752 & 516 & 1,584 & 4 & 5 & 23 & 51 &  207  & 2.61 \\
%    \hline
%   \end{tabular}
%     \caption{Dataset details. TR/V/TE: Number of Training/Validation/Testing Images. S/U: Number of seen/unseen State classes. O: Number of Object classes. VO/TO: Valid/Total Object-State combinations. A: Average number of states per object.}


%   }

%     \label{tab:datasets}
  

% % \end{subtable}

% % \hspace{\fill}

% \end{table}

%\begin{subtable}{0.4\linewidth}

%\tiny
%\centering


% Our work involves a series of experiments that entails a comparison of our approach with other SoA models, as well as an extensive ablation study to investigate various aspects of the problem. Specifically, we aim at an in-depth exploration of three Hypotheses. Specifically, we investigate the role of the constructed KG in performance gains for the OSC task and the impact of the GNN architecture on the method's overall performance. Additionally, we investigate whether knowledge of the object class has an effect on the performance of the OSC task. 

% The previous hypotheses can be formulated as follows:

% \vspace*{0.1cm}\noindent\textbf{Hypothesis 1}: The KG is beneficial to the task. Its impact depends primarily on the type of knowledge it contains. 

% \vspace*{0.1cm}\noindent\textbf{Hypothesis 2}: The GNN architecture is crucial  to the performance of the method. 

% \vspace*{0.1cm}\noindent\textbf{Hypothesis 3}:\label{hyp:3} Knowledge of an object's class is not decisive for the prediction of its state. Therefore, a method that is agnostic to the object class, can perform equally well to  a method that relies on it.


\begin{table*}[t]
	

    \centering
     % \resizebox{1\textwidth}{!}
      %  \resizebox{0.76\textwidth}{!}{\begin{minipage}{\textwidth} 
      \resizebox{2\columnwidth}{!}{
     \begin{threeparttable}
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
   %  \begin{tabular}{|l|c|c|c|c|c|c|c|c|} 
   \hline
%\tiny
      
 % \diagbox[innerleftsep=.5cm,innerrightsep=0pt]{{\bf \multirow{2}{*}{Method}}}{{\bf \multirow{2}{*}{Dataset}}} 
 
 


 
\textbf{\centering  Method}  &\multicolumn{4}{|c|}{\textbf{OSDD}}  & \multicolumn{4}{|c|}{\textbf{CGQA-States}}  & \multicolumn{4}{|c|}{\textbf{MIT-States}}    & \multicolumn{4}{|c|}{\textbf{VAW-States}} \\      \cline{2-17} 

% \textbf{\centering  Method}  &\multicolumn{2}{|c}{\textbf{OSDD}~\cite{gouidis2022}}  & \multicolumn{2}{|c|}{\textbf{CGQA}~\cite{Mancini2022}}  & \multicolumn{2}{|c|}{\textbf{MIT}~\cite{Isola2015}}    & \multicolumn{2}{|c|}{\textbf{VAW}~\cite{Pham2021CVPR}} \\      \cline{2-9} 

 &\textbf{S} & \textbf{U}   
 & \textbf{HM} & \textbf{A} 
 &\textbf{S} & \textbf{U}  
 & \textbf{HM} & \textbf{A}
 &\textbf{S} & \textbf{U}  
& \textbf{HM} & \textbf{A}
 &\textbf{S} & \textbf{U}  
& \textbf{HM} & \textbf{A}
 \\ \hline \hline
 

   



\hline  

% AoP\cite{nagarajan2018attributes}   & 84.2 &  18.2  & 14.0 &  6.7
%  & 99.3 & 8.7 & 8.9 & 1.7
% & 100.0 &  55.5 &  37.5 &  30.6 \\ 

 AoP
\cite{nagarajan2018attributes}   
& 43.2 &  26.1  
& 20.7 &  7.4
 & 100.0 & 19.6 
 &22.9 & 13.3
& 100.0 &  11.6
&  13.2 &  7.0 
& 32.4 &  9.4 
&  9.5 &  2.0 \\ 

% LE+\cite{misra2017red} & 30.5 &     31.9  &  14.0 & 4.3
%      & 97.7 & 13.0 & 13.6 & 6.0
%  & 100.0 & 20.5 &   19.4 & 10.0 \\  
 

 LE+
\cite{misra2017red} 
& 30.5 &     31.9  
&  14.0 & 4.3
     & 97.7 & 12.5 
     & 12.8 & 5.5
 & 100.0 & 20.5
 &   19.4 & 10.0  
  & 56.5 & 16.8 
  &   15.9 & 5.8 \\  
%  TMN\cite{purushwalkam2019task} & 83.1 &  66.5  & 38.5 &  27.5
%  & 99.2 & 40.2 & 25.6 & 15.0
% & 100.0 &  17.7 & 17.8 &  11.0 \\   

  TMN
 \cite{purushwalkam2019task} 
 & 83.1 &  66.5  
 & \underline{38.5} &  \underline{27.5}
 & 99.2 & 40.2 
 & 25.6 & 15.0
& 100.0 &  17.8
& 17.7 & 11.0 
 & 86.7 &  55.2 
 & \bf 38.1 & \underline{27.1} \\  %cw
 % SymNet\cite{Li2020} &83.2 &     36.7  &  28.3 & 16.3 
 %     & 98.5 & 70.4  & 44.2 & 31.1  
 %     % & 49.3 & 41.7
 % & 94.1 & 22.3 &   29.6 & 17.3 \\  

  SymNet
  \cite{Li2020}
  &83.8 &     37.3  
  &  33.5 & 19.8
     & 99.2 & 24.5 
     & \underline{36.6} & \underline{20.6} 
     & 94.1 & 21.4 
     &   23.2 & 13.2
& 87.8 & 31.6 
&   37.3 & 21.5 \\  

%%.| Seen Acc attr: 78.91% | Unseen Acc attr: 49.73 | HM_attr: 32.10% | AUC_attr: 20.69% cw
%   Compcos\cite{Mancini2022}  & 86.0 &  60.0  & 36.0 &  25.4
%  & 99.2 & 22.8 & 21.9 & 10.1
% & 100.0 &  23.7 &  20.2 &  11.0 \\ 


  Compcos
 \cite{Mancini2022} 
 & 86.5 &  43.7  
 & 26.9 &  15.9 
 & 89.9& 17.1
 & 14.7 & 6.1
& 100.0 &  52.2
& \bf 36.4 &  \bf 25.8 
& 88.3 &  32.1
& 27.7 &  17.2 \\
%   KG-SP\cite{karthik2022open}  & 80.0 &  39.8  & 26.7 &  12.4
%  & 96.9 & 8.2 & 10.7 & 4.5
% & 100.0 &  6.5 &  12.2 &  6.3 \\ 
   KG-SP
  \cite{karthik2022open} 
  & 80.0 &  39.8  
  & 26.7 &  12.4
 & 96.9 & 8.2 
 & 10.7 & 4.5
& 100.0 &  7.1 
&  9.0 &  4.0 

& 83.9 &  11.4 
&  17.7 &  8.1 \\

  SCEN
\cite{li2022siamese} 
  & 77.8 &  41.5 
  & 35.2 &  22.5
 & 100.0 & 13.0 
 & 12.9 & 5.9
& 100.0 &  22.03
&  20.6 &  12.6 
& 89.6 &  37.4
&  28.2 &  17.3 \\ 


%   SCEN-NET\cite{li2022siamese}  & 49.2 &  38.3  & 19.8 &  7.1
%   & 100.0 & 64.9 & 46.4 & 39.5
% & 100.0 &  22.3 &  20.6 &  12.6 \\ 

%   IVR\cite{zhang2022learning}  & 85.8 &  38.5  & 36.0 &  23.1
%  & 98.5 & 18.8 & 17.1 & 8.4
% & 100.0 &  11.3 &  14.1 &  5.2 \\ 


   IVR
  \cite{zhang2022learning}  
  & 85.8 &  37.8  
  & 35.1 &  22.1
 & 98.4 & 18.8 
 & 17.1 & 8.4
& 100.0 &  11.3 
&  14.1 &  5.4  

& 88.9 &  11.0
&  16.2 &  7.8 \\
%   OADiS\cite{Saini_2022_CVPR}  & 80.9 &  65.8  & 26.1 &  16.5
%  & 96.1 & 11.7 & 11.1 & 5.6
% & 100.0 &  28.5 &  23.0 &  11.5 \\ 


   OADiS
  \cite{Saini_2022_CVPR} 
  & 72.7 &  55.5 
  & 23.1 &  13.0
 & 97.7 & 11.7
 & 11.9 & 4.8
& 94.1 &  30.0 
&  \underline{23.3} &  12.5 
& 83.3 &  53.5
&  33.8 &  23.9 \\ 
%   CANET\cite{wang2023learning}  & 85.6 &  36.4  & 20.2 &  12.1
%  & 100.0 & 9.5 & 11.3 & 5.1
% &  100.0 &  17.2 &  19.4 &  9.7 \\ 


%    CSP
%   \cite{csp2023} 
%  % & 85.6 &  36.4  
%   % & 14.8 &  6.1
%     & 28.0 &  15.0
%  %& 100.0 & 9.5 
%  & 37.8 &  28.6
%  %  45.03 &38.3
% %&  100.0 &  16.9 
% % &  44.48 &    39.48
% &   &    
% %&  87.8 &  53.4
% &  30.6 &  18.3 \\ 
% % &  37.7 &  26.6  \\ 
% %   ADE\cite{hao2023ade}  & 90.4 &  67.8  & 39.8 &  29.8
% %  & 100.0  & 68.5 & 44.9 & 38.0
% % & 100.0 &  23.7 &  28.3 &  17.2 \\ 

   CANET
  \cite{wang2023learning} 
  & 85.6 &  36.4  
  & 20.2 &  12.1
 & 100.0 & 9.5 
 & 11.3 & 5.0 
&  100.0 &  16.9 
&  23.1 &  11.9  
&  87.8 &  53.4
&  35.6 &  25.6  \\ 
%   ADE\cite{hao2023ade}  & 90.4 &  67.8  & 39.8 &  29.8
%  & 100.0  & 68.5 & 44.9 & 38.0
% & 100.0 &  23.7 &  28.3 &  17.2 \\ 
  
   ADE
  \cite{hao2023learning} 
  & 91.4 &  67.1 
  & \bf 40.5 &   \bf 30.3
 & 100.0  & 68.7
 &\bf 40.0 & \bf 33.3
& 100.0 &  24.9
&  22.6 & \underline{12.6}  
 % & 89.1 &  56.9 &  37.4 & 27.9   %cw


& 89.35 & 56.9 
& \underline{36.9} & \bf 27.6 \\ %ow
   
  \hline  \hline 
 
\centering  {\bf OaSC (ours)}
% (Ours)
  & 87.7 & 69.9 
  & \bf  \textcolor{red}{48.6} & \bf \textcolor{red} {39.8} 
   &97.1 & 73.4 
  & \bf  \textcolor{red}{43.6} & \bf  \textcolor{red}{36.5}
    &85.7 & 69.9 
  & \bf  \textcolor{red}{51.1} & \bf  \textcolor{red}{41.2}  
  
  &83.7 & 58.6 
  & \bf  \textcolor{red}{42.9} & \bf  \textcolor{red}{32.8}  

 \\  \hline 
% Improvement 
$\Delta$ \quad (gain) &{\cellcolor{gray}}  &{\cellcolor{gray}}  & +8.1 &  +9.5 & {\cellcolor{gray}} & {\cellcolor{gray}} &+3.6 &  +3.2 
 & {\cellcolor{gray}} & {\cellcolor{gray}} &+14.7 &  +15.4  & {\cellcolor{gray}} &{\cellcolor{gray}}  &+3.8 &  +5.2   
  
  \\  \hline  

%  \hline  
%   % CLIP  & 66.0 &  58.8  &  46.8 &   31.7
% % CLIP  & 57.5 &  69.8  &  29.5 &   22.3
% CLIP-ViTB16~\cite{radford2021learning} 
% %& 85.8 &  66.2 
% &  45.4 &   39.7
% %& 61.9 &  71.8 
% &  54.9 &   40.3
% %& 58.5 &  80.0 
% &  63.4 &   39.8

% %& 87.3 &  66.9 
% &  43.4 &   36.3 \\

% CLIP-ViTB32~\cite{radford2021learning} 
% %& 85.8 &  66.2 
% &  43.1 &   35.4
% %& 61.9 &  71.8 
% &  48.2 &   30.4
% %& 58.5 &  80.0 
% &  63.9 &   39.6

% %& 87.3 &  66.9 
% &  41.4&   33.4 \\
  


% CLIP-RN101~\cite{radford2021learning} 
% %& 85.8 &  66.2 
% &  33.6&   22.2 
% %& 61.9 &  71.8 
% &  51.8&   36.5
% %& 58.5 &  80.0 
% &  63.5 &   45.6

% %& 87.3 &  66.9 
% &  37.4 &   27.1 \\  \hline 
  \end{tabular}
 \caption{Aggregate results for the  Object Agnostic Setting. 
    Seen: Best Accuracy on seen classes. Unseen: Best accuracy on unseen classes. 
   HM: Best harmonic mean. A: Area under curve for the pairs of accuracy for seen and unseen classes.  Red/Bold/Underlined text indicates best/2nd best/3rd best performance.
   }
      \label{tab:table_aggr}
        \end{threeparttable}
       }
\end{table*}

%\vspace{-.35cm}
\subsection{Implementation and evaluation issues}
%\vspace{-.15cm}

\noindent\textbf{Implementation details}: 
The GNN was trained following the method outlined in Nayak et al. \cite{Kampffmeyer2019}. The model was trained for 1000 epochs on 950 randomly selected classes from the ILSVRC 2012 dataset \cite{russakovsky2015imagenet}, while the remaining 50 classes comprise the validation set. The model with the lowest validation loss was chosen to generate the seen and unseen class embeddings using the graph. For the seen classes, the embeddings were frozen, and a pre-trained ResNet101-backbone was fine-tuned on the individual datasets for 50 epochs using stochastic gradient descent  with a learning rate of 0.0001 and momentum of 0.9.

% For the training of the  GNN we follow the strategy propose in \cite{nayak:tmlr22} and train the model for 1000 epochs on 950 random classes
% from the ILSVRC 2012 \cite{russakovsky2015imagenet} while the remaining 50 classes are used for validation. The model with
% the least loss on the validation classes is used to generate the seen and unseen class embeddings with
% the graph.  We freeze the class embeddings for the seen classes and fine-tune a pretrained ResNet101-backbone on the individual datasets for 50 epochs using SGD with a learning rate 0.0001 and
% momentum of 0.9. 


% Currently with the exception of the OSDD dataset \cite{},  there are not exclusive object states dataset available, but rather attributes  dataset which include among their classes and object states. Therefore, we adopt two of the most popular attributes  datasets \cite{Isola2015} \cite{Mancini2022}  to our needs by taking the subsets that refer to object states in order to be used in the context of the experimental evaluation.


\vspace*{0.0cm}\noindent\textbf{Datasets}: 
Currently, there is a scarcity of datasets specifically designed for characterizing object states, 
% At present, there is a lack of datasets exclusively dedicated to object states, 
except for the OSDD~\cite{gouidis2022} which is a dataset tailored for state detection. Instead, existing attribute datasets include object states among their classes. To address this, we utilized two of the most widely used attribute datasets CGQA~\cite{Mancini2022} and MIT~\cite{Isola2015}, and extracted subsets that are specifically related to object states. We also experimented with VAW~\cite{Pham2021CVPR} which is a recently published object detection dataset that provides object state annotations for some of its samples. Regarding the OSDD and VAW, we extracted the bounding boxes of the original images to create images suitable for the OSC task. The complexity of each dataset can be assessed mainly by the number of unseen state classes and the average number of states per object class. More details on these datasets are presented in the supplementary section. 

\vspace*{0.0cm}\noindent\textbf{Metrics}: 
Our evaluation protocol follows the standard generalized zero-shot evaluation described in~\cite{Purushwalkam}, i.e., we calculate the Area Under the Curve (AUC) measuring the accuracy on both seen and unseen compositions at different operating points based on the bias term that is added to the scores of the unseen classes. The optimal zero-shot performance occurs when the bias term is positive, leading the classifier to prioritize the unseen labels. Conversely, the best seen performance is achieved with a negative bias term, which results in a focus on the seen labels. Additionally, we report the best harmonic mean (HM) which expresses a balance between the seen and unseen accuracy, respectively.



\noindent\textbf{Comparison with SOTA object-aware CZSL methods for state classification}: 
% Since there are no actual zero-shot state classifiers, we utilize SoA methods from the fields of  czsl and zero-shot attribute classification. In the case of czsl, they only method that can be used for ZS-OSC is  Compcos  which also exhibits the best performance in the czsl task. 
% As there are currently no pure zero-shot state classifiers available, we utilize  the state-of-the-art method from the field of CZSL\cite{Mancini2022} which involves both the prediction of object and state label for an image is  the most related field to to OSC.  We report the methods performance for three different settings: closed world, open world and object-oracle. For the first setting the method has to predict only among the valid object-state pairs, whereas for the second the method has to predict among all object-state pairs. For the third setting, all object labels are set to the generic term \'object\' enabling thus the method to predict only the state label. Although the closed world setting violates the zero-shot assumptions we include it as a baseline.
Given that there are currently no zero-shot state classifiers available, we resort to employing 11 state-of-the-art models \cite{misra2017red,nagarajan2018attributes,purushwalkam2019task,Li2020,Mancini2022,karthik2022open,li2022siamese,zhang2022learning,Saini_2022_CVPR,wang2023learning,hao2023learning}
from the field of Compositional Zero-Shot Learning (CZSL). These methods deal with predicting both object and state labels. As such, they are relevant to OSC - however, they are object-aware and not object-agnostic as the proposed OaSC method. 
% As these models are capable of producing state labels, they can be used in the context of OSC without any modifications.
We evaluate the performance of this approach on three different versions: %Object Agnostic, Closed World, and Open World:
\begin{itemize}[noitemsep,topsep=0pt]
\item {\bf Object Agnostic (OA)  version}: All object labels are replaced with the generic term ``object'', allowing the method to solely predict the state label.
\item {\bf Closed World (CW) version}: The method is tasked with predicting only among the valid object-state pairs. 
\item  {\bf Open World  (OW) version}: The method is tasked with predicting among all object-state pairs. 
\end{itemize}
In all three settings, we focus exclusively on the predictions concerning the states labels. It's important to emphasize that both the CW and OW versions of the models deviate from the principles of zero-shot conditions. Specifically, the CW version relies on pre-existing knowledge of valid states for each object, while the OW version considers a closed set of object labels corresponding to the states. These assumptions, although informative, limit the generality of the approach. Unlike these versions, our method remains entirely impartial to such constraints, demonstrating its versatility by maintaining consistency between training and inference. 

Additionally, it's noteworthy that both the CW and OW versions of the models incorporate knowledge about object categories, which is contrary to the object-agnostic assumption. In contrast, our approach remains consistent with the object-agnostic principle. Given these considerations, the fairest comparison to our method is the OA  version of the models. Nevertheless, for reference, we present the results of both the CW and OW versions of each model. This comprehensive approach provides a frame of reference while highlighting the distinct strengths of our method.



% It should be noted that both the CW and the OW versions violate the zero-shot conditions.  Namely, for the CW the valid states for each object are  known in advance, whereas, for the OW version, the set of object labels to which the states correspond is considered closed, which assumption sets it less generic than our approach, i.e. the same during training and inference, whereas our method is totally agnostic to this. In addition, both the CW and the OW  versions of the models use the knowledge for object categories, something that violates the object-agnostic assumption.  Therefore, the most fair comparison to our method is the GO version of the models. However, we report  the results of the  CW  and OW versions of each model as frames of reference. 
% Specifically, in the former case we can only use the Compcos method \cite{Mancini2022}, which has demonstrated the best performance in the context of czsl.


% The best zero-shot performance is achieved when the bias term is large, predicting only the unseen labels. The best seen performance is achieved when the bias term is negative, predicting only the seen labels.

% % Figure environment removed


\subsection{Experimental results}

\noindent{\bf Intra-dataset evaluation:}
\autoref{tab:table_aggr} summarizes the results of the OA versions evaluation for the four employed datasets (the results for the CW and OW versions are presented as supplementary material due to space limitations). We report the performance of the version of
our model that was selected by the ablation study described in the next section. It is important to note that this version of the model does not
exhibit the best performance in all dataset categories.
% (further information about the specific characteristics of the two models are provided in the next section).
% Each model is based on a different KG and/or a different calibration. Further information about the specific characteristics of the four models are provided in the next section.   
% The results indicate that under the GO  versions, our method clearly outperforms the competing methods in both metrics, namely AUC  and HM, across  all three datasets. Furthermore, our method achieves superior performance in the OW  setting for all three datasets and ranks first, third, and fourth among the CW versions of the competing methods in the MIT-States, OSDD and CGQA-States datasets respectively. This attests to the robustness of our method since the CW setting makes use of additional information regarding the object classes  and the valid object-state combinations. 
% % In the following, we refer only to the performance of the GO versions of the competing methods.
% More in detail, as regards the GO versions the largest performance gain for our method is observed in the MIT-states dataset, with an increase of  15.1\%/ for AUC  and  16.3\% for HM  in comparison to the scores of the best-performed competing method (CompCos). In the case of the OSDD dataset, there is a difference of  5.1\% for AUC and 5.1\% for HM in favor of our method w.r.t.  ADE method which is the competing method that performs best in this dataset. Finally, for the CGQA-States dataset, an improvement of  1.7\%   for AUC and 3.0\%  for HM of our method is achieved compared to the ADE model, which scores best among the competing methods in this setting.  
Based on the obtained results, we observe that our method outperforms by a significant performance gain every other competing method. Specifically, in the MIT-States dataset OaSC outperforms by a margin of 15.4\% (14.7\% for HM)  the second best-performing method, which is the CompCos approach. Regarding the OSDD dataset, our  method outperforms the leading competitor, ADE,  with a gain of 9.5\% (8.1\% for HM). In the case of VAW, the gain in favor of our method is 5.2\% (3.8\% for HM) in comparison ADE which is the second-best method.  Lastly, in the CGQA-States dataset, our method demonstrates an improvement of 3.2\% (3.6\% for HM), surpassing the ADE model, which is the second best-performing among the competing methods in this scenario.
The substantial margin by which OaSC outperforms the competing methods in the OA setting indicates that the lack of information related to objects classes is detrimental for the CZSL methods. 
Moreover,  the fact that the CZSL methods in the OW and CW settings, although improved, are still inferior to  our method, suggests that the leveraging of KGs  can serve as a substitute for object-aware information.

%can be attributed to the difficulty of the task and to the benefits that our method gains by  the leveraging of the KGs.

% \begin{table}[t]
% 	\small
%     \centering
%     \resizebox{1\columnwidth}{!}{
%    % \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
%      \begin{tabular}{|l|c|c|c|c|} 
%    \hline\hline



% {\textbf{Variant}}  & {\textbf{OSDD}}~ \cite{gouidis2022}  & {\textbf{CGQA}}~\cite{Mancini2022}   & {\textbf{MIT}}~\cite{Isola2015}    &{\textbf{VAW}} \cite{Pham2021CVPR}  \\   

% \hline\hline

% {\bf RN101}
% % %& 85.8 &  66.2 
% % &  33.6
% &   22.2 
% % %& 61.9 &  71.8 
% % &  51.8
% &   36.5
% % %& 58.5 &  80.0 
% % &  63.5
% &   45.6

% % %& 87.3 &  66.9 
% % &  37.4
% &   27.1 \\  \hline 

% {\bf ViT-B/16}%
% % %& 85.8 &  66.2 
% % &  45.4 
% &   39.7
% % %& 61.9 &  71.8 
% % &  54.9 
% &   40.3
% % %& 58.5 &  80.0 
% % &  63.4 
% &   39.8

% % %& 87.3 &  66.9 
% % &  43.4 &   
% &36.3 \\ \hline

% {\bf ViT-B/32} %~\cite{radford2021learning} 
% % %& 85.8 &  66.2 
% % &  43.1 &   
% &35.4
% % %& 61.9 &  71.8 
% % &  48.2 &  
% &30.4
% % %& 58.5 &  80.0 
% % &  63.9 &   
% &39.6

% % %& 87.3 &  66.9 
% % &  41.4&   
% &33.4 \\
  
% \hline

%   \end{tabular}
%   }

%    \caption{AUC performance of CLIP for three different visual backbones. The models are fine-tuned following the procedure described in \cite{wortsman2021robust}. The  dataset splits  are the same as the ones used in the experiments reported in \autoref{tab:table_aggr}.
%    % The variant that is more close to our model is RN-101
%    }
% \label{table:clip}
% \end{table}



% \begin{table}[t]
% 	\small
%     \centering
%     \resizebox{1\columnwidth}{!}{
%    % \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
%      \begin{tabular}{|l|c|c|c|c|} 
%    \hline\hline

% {\textbf{Dataset}}  & {\textbf{OSDD}} \cite{gouidis2022}  & {\textbf{CGQA}} \cite{Isola2015}  & {\textbf{MIT}}\cite{Mancini2022}    &{\textbf{VAW}} \cite{Pham2021CVPR}  \\   

% \hline\hline

% {\textbf{OSDD}} \cite{gouidis2022}  & \\ \hline
%   {\textbf{CGQA}} \cite{Isola2015}  & \\ \hline
%   {\textbf{MIT}}\cite{Mancini2022}  \\\hline
%   &{\textbf{VAW}} \cite{Pham2021CVPR} \hline
% \hline

%   \end{tabular}

%    \caption{  Cross Validation
%    % The variant that is more close to our model is RN-101
%    }
% \label{table:cross}
% \end{table}

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}


\noindent{\bf Cross-dataset evaluation:} A further series of experiments was conducted concerning cross-dataset evaluation. \autoref{table:cross} reports the results obtained by our method and the ADE~\cite{hao2023learning}  model, which overall is the second-best model in the intra-dataset evaluation.  We can see that our method outperforms ADE in all cross settings when OSDD, CGQA and VAW  are used as training datasets, whereas ADE is better when MIT is used. %A plausible explanation for this is that MIT  is very distinct, visually, from the other 3 datasets and is also the smallest in terms of samples, which entails that the fine-tuning of a model in this dataset renders the classification in the other 3 datasets
%very challenging. 
This likely is due to   MIT  being very distinct, visually, from the other 3 datasets and being also the smallest in terms of samples, which entails that the fine-tuning of a model in this dataset renders the classification in the other 3 datasets
%very challenging. 
%Therefore, fine-tuning a model in this dataset renders the classification in the other  datasets very challenging. 
% The  backbone of ADE is a visual transformer (ViT) which is much more
% effective in learning representations than our CNN back-bone (ResNet101). %due to visual transformers having access to more sub-space global information across multi-head
% %attentions than CNNs . 
% Î¤hese more powerful  representations result in higher accuracy for  the  classes of the cross datasets in comparison to our CNN back-bone. 
ADE uses as a backbone a visual transformer (ViT) which is
much more effective in learning representations than our
CNN backbone (ResNet101), since visual transformers
have access to more sub-space global information across
multi-head attentions than CNNs. Therefore, 
the difference in backbones is crucial in this context.


%of the OA versions can be deemed only average, given the significantly smaller search space (i.e., set of states) in comparison to the search spaces of the Closed-CW (i.e., set of valid object-state pairs) and  Open-OW (i.e., set of all object-state pairs) worlds, respectively. 
% Overall, only the performance of the CW versions of some of the methods can be considered satisfactory.

 % The substantial margin by which our proposed method outperforms the object-based methods in every experiment supports the conclusion in the object-agnostic setting 
 % that the information of the object class is not a decisive factor in the task.  Additionally, by examining more closely the behavior of the three versions of the competing models (OA, OW, CW - see supplementary material) we can make the following observations. The OW versions perform in general very poorly, while the performance of the OA versions can be deemed only average, given the significantly smaller search space (i.e., set of states) in comparison to the search spaces of the Closed-CW (i.e., set of valid object-state pairs) and  Open-OW (i.e., set of all object-state pairs) worlds, respectively. 
% Overall, only the performance of the CW versions of some of the methods can be considered satisfactory.

\noindent{\bf Comparison with LPMs:}
We also report the performance of three variations of the CLIP~\cite{radford2021learning} model which is considered one of the best-performing Large Pre-Trained Models (LPMs) and is used extensively for a variety of downstream tasks such as state classification and BLIP~\cite{li2022blip} which also supports diverse downstream tasks but utilized mainly in the context of  Visual Question Answering. It is important to stress that although LPMs  are considered zero-shot learning models, they are rather classifiers in the wild since these models have been presented during their training with samples containing the target classes to which they are tested. However, since these models are witnessing wide popularity and are considered SoA methods, we opted to report these variants to serve as an additional frame of reference. The obtained results are summarized in~\autoref{table:clip}. We observe that OaSC performs better than CLIP-RN101 which is the CLIP variant that uses the same visual backbone as our classifier. In more detail, our method outperforms CLIP-RN101 by a margin of 17.7\% in OSDD  and by  5.1\% in the VAW, while it achieves the same performance in CGQA and falls short by  -4.4\% in MIT. Moreover, our model outscores  BLIP by  margins ranging from 10.4\% (CGQA) to 26.2 \%  (OSDD) across all datasets.  Overall,  these results provide a further indication of the power of our method. %This outcome suggests that LPMs tailored to the VQA task are not fitted to address the zero shot OaSC problem.
\begin{table}[t]


     \resizebox{\columnwidth}{!}{
     \centering
\begin{tabular}{|l|c|c|c|c|} 
   \hline



{\textbf{Variant}}  & {\textbf{OSDD}}~ \cite{gouidis2022}  & {\textbf{CGQA}}~\cite{Mancini2022}   & {\textbf{MIT}}~\cite{Isola2015}    &{\textbf{VAW}} \cite{Pham2021CVPR}  \\   

\hline\hline

{\bf RN101}
% %& 85.8 &  66.2 
% &  33.6
&   22.2 
% %& 61.9 &  71.8 
% &  51.8
&   36.5
% %& 58.5 &  80.0 
% &  63.5
&   45.6

% %& 87.3 &  66.9 
% &  37.4
&   27.1 \\  \hline 

{\bf ViT-B/16}%
% %& 85.8 &  66.2 
% &  45.4 
&   39.7
% %& 61.9 &  71.8 
% &  54.9 
&   40.3
% %& 58.5 &  80.0 
% &  63.4 
&   39.8

% %& 87.3 &  66.9 
% &  43.4 &   
&36.3 \\ \hline

{\bf ViT-B/32} %~\cite{radford2021learning} 
% %& 85.8 &  66.2 
% &  43.1 &   
&35.4
% %& 61.9 &  71.8 
% &  48.2 &  
&30.4
% %& 58.5 &  80.0 
% &  63.9 &   
&39.6

% %& 87.3 &  66.9 
% &  41.4&   
&33.4 \\
  
\hline
{\bf BLIP}  
& 13.6
&   26.1
&  27.2
  
& 16.1\\
\hline

  \end{tabular}
 } 
   \caption{AUC performance of CLIP for three different visual backbones. The models are fine-tuned as described in \cite{wortsman2021robust}. %The dataset splits are as in the experiments reported in \autoref{tab:table_aggr}.
   % The variant that is more close to our model is RN-101
   }
%\vspace{-0.2cm}
\label{table:clip}


\end{table}


\begin{table}[t]
\centering
%\tiny
%\begin{subtable}{0.48\linewidth}


     \resizebox{\columnwidth}{!}{
\begin{tabular}{|ll|llll|}
\hline
\multicolumn{2}{|l|}{{\bf Datasets}}           & \multicolumn{4}{c|}{{\bf OaSC (ours) vs ADE~\cite{hao2023learning} }}                                                         \\ \hline
\multicolumn{2}{|l|}{{\bf \diagbox[innerleftsep=.05cm,innerrightsep=4pt]{{\bf {Training}}}{{\bf {Testing}}}}} & \multicolumn{1}{c|}{\bf OSDD} & \multicolumn{1}{c|}{\bf CGQA} & \multicolumn{1}{c|}{\bf MIT} & \multicolumn{1}{c|}{\bf VAW} \\ \hline\hline
\multicolumn{2}{|l|}{{\bf OSDD}}       &\multicolumn{1}{|l|}{\cellcolor{gray}}  & 
 \multicolumn{1}{r|}{\textbf{24.8}/14.1}& 
 
\multicolumn{1}{r|}{\textbf{47.4}/27.3}       &  \multicolumn{1}{r|}{\textbf{16.9}/15.5}  \\ \hline
 \multicolumn{2}{|l|}{{\bf CGQA}}            & \multicolumn{1}{r|}{\textbf{35.0}/17.2}&\multicolumn{1}{r|}{\cellcolor{gray}}       &   \multicolumn{1}{r|}{\textbf{21.6}/12.8}        &  \multicolumn{1}{r|}{\textbf{34.4}/20.3}        \\ \hline
\multicolumn{2}{|l|}{{\bf MIT}}            &\multicolumn{1}{r|}{17.1/\textbf{19.1}} &\multicolumn{1}{r|}{6.1/\textbf{21.4}} &        \multicolumn{1}{r|}{\cellcolor{gray}}  &      \multicolumn{1}{r|}{10.1/\textbf{18.9}}   \\ \hline
\multicolumn{2}{|l|}{{\bf VAW}}     &          \multicolumn{1}{r|}{\textbf{23.0}/5.9}    & \multicolumn{1}{r|}{\textbf{29.3}/22.3}&\multicolumn{1}{r|}{\textbf{28.2}/3.1}    &          \multicolumn{1}{r|}{\cellcolor{gray}}   \\ \hline
\end{tabular}%
}
\caption{Cross-dataset evaluation of OaSC and ADE (AUC metric) for pairs of training (rows) and testing (columns) datasets.}

\label{table:cross}
% \end{subtable}
%\hspace{\fill}
%\vspace{-0.65cm}

\end{table}



% The largest performance margin in favor of our method is observed in the MIT-states dataset, with an increase of 15.2\%/19.7\% for HM  and   10.3\%/15.9\% for AUC between the two versions method and the best of the competing methods (TMN). In the case of the OSDD dataset, there is an increase of  7.9\%/4.8\% for AUC and 8.8\%/7.1\%  for HM. Finally, for the CGQA-States dataset  a difference of 24.9\% for AUC and 9.7\%/5.5\%  for HM is observed. 
% This pattern indicates that our method performs even better than the competing method as the complexity of the dataset increases. 
  % Regarding the absolute performance of the method across the three datasets we see that  it is proportional to  the complexity of each dataset with the greater scores achieved for the MIT-states and the lowest for the CGQA-states respectively. This behavior is also observed in the performance of the majority of the competing methods.
% Overall, the best performance for the OSDD is achieved by the model CN+WN\_H2\_TH, while the model CN\_H2 scores best for the two other datasets.  


% The substantial margin by which our proposed object-agnostic OaSC method outperforms the competing object-based methods in every experiment supports strongly the {\bf Hypothesis 3}, indicating that object information is not decisive in the context of zero-shot OSC.
% Additionally, examining more closely the behavior of the three  versions of the competing models we can make the following observations. The OW versions perform in general very poorly, while the performance of the GO versions can be deemed only average, given the significantly smaller search space (i.e., set of states) in comparison to the search spaces of the Closed (i.e., set of valid object-state pairs) and Open (i.e., set of all object-state pairs) Worlds, respectively. Overall, only the performance of the CW versions of some of the methods can be considered satisfactory.


% % Figure environment removed
% Finally, examining the  performance of the models across the three datasets we can see that for  most of the models the scores achieved are proportional to  the complexity of each dataset with the greater scores achieved for the MIT-states and the lowest for the OSDD respectively. 


% The results strongly support \textbf{Hypothesis 3}, which suggests that object information does not confer any advantages in the context of zero-shot Object State Classification, as evidenced by the clear margin of outperformance of our object-agnostic method over the competing object-based method in all experiments.



% Furthermore, the behavior of the three different versions of CompCos provides further insights regarding the problem. Specifically, we observe that the performance of the object-oracle version  
\vspace{-.1cm}

\subsection{Ablation Study}\label{sec:abl}
\vspace{-.1cm}

% We ablate our method across the following categories: the GNN architecture, the KG source, the number of max hops that were used for the creation of the KG and the policy that were followed w.r.t. the inclusion of nodes to the KG. For each ablated model we report the best accuracy achieved on seen and unseen classes, the best harmonic mean and the best AUC for each of the three datasets. The results are presented in \autoref{tab:ablation}.

We conducted a host of ablation experiments across several problem dimensions to select the optimal parameters for our model.  Specifically, we explored the impact of varying the GNN architecture, the KG source, the maximum number of hops used for KG creation and the policy for including nodes in the KG. Due to space limitations, the performance exhibited by every ablated model is provided in the supplementary material. Here, we present aggregated means of all models across each of the ablated dimensions reporting the best harmonic mean and the AUC for each of the four datasets, respectively. 
% The results of these experiments are presented in \autoref{tab:abl1} - \autoref{tab:abl3}.

%%%AAA: Parakatw, prwta parousiazeis oles tis diastaseis tou ablation, kai meta ola ta relevant results. Gia na veltiwseis to locality of reference kanto alliws: ablation diastasi 1 - results 1, ablation diastasi 2 - results 2...
 


\vspace*{0.0cm}\noindent\textbf{GNN architecture}:
We conduct experiments using 4 different GNN architectures:  GCN \cite{kipf2016semi}, R-GCN \cite{schlichtkrull2018modeling},   LSTM \cite{hamilton2017inductive} and Tr-GCN \cite{nayak:tmlr22}. The ablation results for the different architectures are presented in \autoref{tab:abl1}. 
The Tr-GCN framework outperforms the other frameworks in all datasets w.r.t. AUC metric, whereas it scores best w.r.t. HM metric in the OSSD and VAW and comes second in the two other datasets. The R-GCN framework exhibits the second-best performance, while the GCN framework comes in third and the LSTM framework exhibits the worst performance. 
% These findings are consistent with prior research in the domain of zero-shot object classification and validate the view that the architecture of .



\vspace*{0.0cm}\noindent\textbf{KG source}:
% The KG sources that we use are: ConceptNet\cite{speer2017conceptnet} and WordNet\cite{fellbaum2010wordnet}. We also include in our experiments some KGs that were created using information from both sources. We mention also that we attempted to utilize other sources such as Dbpedia\cite{} and WikiData\cite{} but did not succeed at finding the necessary information for the creation of a KG.
We employed two KG sources, namely ConceptNet~\cite{speer2017conceptnet} and WordNet~\cite{fellbaum2010wordnet}, and also experimented with combining information from both sources. Other sources such as Dbpedia~\cite{auer2007dbpedia} and WikiData~\cite{vrandevcic2014wikidata} were also considered, but the necessary information for constructing a KG could not be obtained. To better assess the contribution of the KGs, we include a ConceptNet-based model in which the target state classes were mapped to other unrelated state embeddings of the KG and a random model where the embeddings corresponding to the target state classes were generated by a random process. 

Based on the results reported in \autoref{tab:abl2}, the ConceptNet-based model outperforms WordNet across all four datasets, while combining both sources results in performance gains for the HM metric across all four datasets and for the AUC metric in three of the datasets. The difference in favor of ConceptNet can be attributed to the difference between the type of information that each KG holds. ConceptNet contains mainly common-sense knowledge and also includes some lexicographic information, while WordNet contains only lexicographic information. Still, the fact that the best results are achieved by a model that uses both sources suggests their complementarity.  
% Taken together, these findings offer substantial support for \textbf{Hypothesis 1}.  



\begin{table}[t]
%	\small
    \centering
  
 %\begin{subtable}{0.48\linewidth}

   \resizebox{\columnwidth}{!}{
    \begin{tabular}{|l|c|c|c|c|}

\hline
 \bf \diagbox[innerleftsep=.05cm,innerrightsep=4pt]{{\bf {Dataset}}}{{\bf {Arch}}} & 
  \textbf{LSTM}& \textbf{GCN}   & \textbf{R-GCN}& \textbf{Tr-GCN}\\
 
\cline{2-5}
\hline
\hline
\textbf{OSDD}  & 39.0 / 25.7 & 40.0 / 27.0 & 42.9 / 29.9 & \bf 43.2 / 30.3  \\
\hline  
\textbf{CGQA}  & 28.3 / 37.8 & 30.6 / 40.2 &  \textbf{29.0} / 38.1 &28.2 / \textbf{38.5} 
  \\ \hline  



\textbf{MIT} & 47.7 / 30.7 & 50.7 / 34.3  & \textbf{53.7} / 36.6 & 51.2 / \textbf{39.8}  \\   \hline  
\textbf{VAW} & 32.2 / 22.1 & 34.3 / 23.4  & 36.5 / 25.6 & \textbf{39.2} / \textbf{27.6}  \\   \hline  
\end{tabular}
%\end{minipage}
}
\caption{Ablation results for the framework architecture.  The first (second) value in each cell corresponds to the best HM (AUC). } 

% All values are aggregate averages.}
\label{tab:abl1}
 %\vspace{-0.4cm} 

% \end{subtable}
\end{table}

\begin{table}[t]
	%\tiny
     \centering
  %     \resizebox{1\columnwidth}{!}{
%\begin{subtable}[t]{0.48\linewidth}

     \resizebox{\columnwidth}{!}{
    \begin{tabular}{|l|c|c|c|c|c|}

\hline
 \bf \diagbox[innerleftsep=.05cm,innerrightsep=4pt]{{\bf {Dataset}}}{{\bf {KG }}} & 
  \textbf{CN}& \textbf{WN}   & \textbf{CN+WN}&  \textbf{IE}&   \textbf{RN} \\
 

\hline\hline

\textbf{OSDD}  &43.5/30.5 & 32.6/18.5 &  \bf 45.4/34.7  & 19.4/9.3   & 8.2/3.1\\ \hline

\textbf{CGQA}  &  39.2/29.1  & 37.9/27.4 & \bf 44.5/34.7 &  20.1/9.0  & 11.1/5.7 
  \\ \hline

\textbf{MIT} & 53.3/\textbf{42.6} & 38.5/26.6  & \textbf{54.0}/42.1    & 33.8/22.1 & 18.6/13.0 \\   \hline  
\textbf{VAW} & 41.0/28.1 & 31.0/17.3  & \textbf{39.2}/\textbf{32.1}    & 15.3/19.2 & 7.3/3.5 \\   \hline
% \textbf{AGR} & 43.5/30.5 & 32.6/18.5 &  45.4/34.7 & 39.2/29.1  & 37.9/27.4 & 44.5/34.7 & 53.3/42.6 & 38.5/26.6  & 54.0/42.1 \\     


\end{tabular}
}
  \caption{Ablation results for the KG source. The first (second) value in each cell corresponds to the best HM (AUC). CN: ConceptNet. WN: WordNet, WN+CN: Model based on both ConceptNet and WordNet. IE: ConceptNet-Based Model (irrelevant embeddings).  RN:  Model with random embeddings. }

% The values in the three fist columns are aggregate averages.}
\label{tab:abl2}
%\end{subtable}
 % \vspace{-0.5cm} 
 \end{table}

 

Furthermore, the performance of the model using the random embeddings is very low, whereas the  ConceptNet-based model using unrelated state embeddings achieves a clearly better performance which remains significantly lower than that of the other CN-based models. 
The distinction between these approaches can be attributed to the
distribution of their embeddings: the former model employs a balanced and representative distribution enabled by GNN which permits the model to map the learned representations to the visual information of seen classes during the fine-tuning procedure. In contrast, the latter
model has a completely random distribution that cannot be mapped to the semantic representations. The unrelated embeddings do not leverage the recognition of unseen classes, thus resulting in the lower performance of the model. This is further supported by the results included in the supplementary material where the best seen and unseen accuracies are also
reported.  
% The distinction between these approached lies in their
% distribution: the former model employs a balanced and rep-
% resentative distribution enabled by GNN, while the latter
% model has a completely random distribution. This suggests
% that the fine-tuning process can yield competitive seen accuracy even with unrelated embeddings to the target labels as long as the distribution is appropriate. In contrast, achieving accuracy for unseen classes requires an exact mapping
% between the embeddings and the target states


\begin{table}[t]
%	\small
     \centering
    %   \resizebox{1\columnwidth}{!}{
%\begin{subtable}[t]{0.48\linewidth}


         \resizebox{\columnwidth}{!}{
    \begin{tabular}{|l|c|c||c|c|}

\hline
 \bf \diagbox[innerleftsep=.05cm,innerrightsep=3pt]{{\bf {Dataset}}}{{\bf {Hops/Policy}}} & 
  \textbf{Hop 2}& \textbf{Hop 3}   &  \textbf{NP}& \textbf{THR}
 
\\
\hline\hline

\textbf{OSDD}  & \bf 43.1/30.6  &   41.0/27.6 & 38.8/25.3   & \bf 42.5/28.5 \\
\hline
\textbf{CGQA-States}  &30.3/39.5 & \bf 31.4/41.0 &  25.9/36.0  &   \bf 29.8/39.5
  \\ \hline



\textbf{MIT-States} & 52.3/\textbf{36.9} &  \textbf{54.8}/36.5 & 45.9/31.7 & \bf 56.0/42.3 \\   \hline  
\textbf{VAW} & \textbf{37.5}/\textbf{27.8} &  35.5/24.3 & 31.5/20.7 &  \bf 34.1/\bf 23.0  \\   \hline  






% \textbf{Aggr} &42.3/29.6  &  43.2/30.4 & 39.6/26.6  & 41.0/27.6 &
% 30.1/39.8  & 28.4/37.7 & 22.1/32.3  &  31.4/41.0 &
% 51.7/35.5 & 52.5/36.8 & 44.0/32.1 & 54.8/36.5 \\

\end{tabular}
}
    \caption{Ablation results. 
    1st (2nd) column, number of hops: average performance of models that are based on a KG with a number of hops equal to 2 (3).
    3rd (4th) column, threshlold policy:  average performance of models that are based on a KG created without (with) threshold policy. The 1st (2nd) value in each cell corresponds to the best HM (AUC).}
    
%}

% All values are aggregate averages.}
\label{tab:abl3}
%\end{subtable}
\vspace{-0.3cm} 
%\vspace{-0.65cm}

\end{table}




\vspace*{0.0cm}\noindent\textbf{Number of max node hops}:
We experiment with a hop count equal to 2 and to 3 for both KGs. The results are shown in \autoref{tab:abl3}. No consistent pattern can be identified.  The best average performance is achieved for the OSDD and VAW datasets at hop 2, while the best average performance is exhibited for the CGQA-State dataset at hop 3. In MIT-States there is no clear winner, as hop 2 shows superior AUC and hop 3 exhibits superior HM. This suggests that introducing nodes beyond a certain limit may introduce noise and potentially deteriorate the overall performance in specific cases, as observed in the OSDD dataset. 
% This outcome is consistent with the \textbf{Hypothesis 1}.

\vspace*{0.0cm}\noindent\textbf{Node policy}:
We investigate two strategies for adding nodes to our knowledge graph, indiscriminate inclusion of all neighboring nodes and selective inclusion of only relevant nodes. To determine relevance in ConceptNet, we use the edge weight between the queried node and its neighbors as the inclusion criterion. In WordNet, we use the Wu-Palmer Similarity metric~\cite{wu1994verb} 
between the two nodes. Additionally, in WordNet, we explore a hierarchical policy of accepting candidate nodes only if their ancestors belong to certain generic categories, such as attributes or objects. 
The results (last two columns of \autoref{tab:abl3}) show that adopting this policy leads to significant performance improvements across all three datasets. This finding complements the previous observation regarding the number of hops and further strengthens the notion that the presence of noisy nodes can have a detrimental effect on model performance. 
% These results align with \textbf{Hypothesis 1}.
 

% More details about the characteristics of the different KGs that were used during the ablation  can be found in \autoref{tab:KGS}.

% \begin{table*}[t]
% %	\small
%     % \centering
%       \resizebox{0.73\textwidth}{!}{\begin{minipage}{\textwidth} 
%     \begin{tabular}{|c|cccc|cccc|cccc|}

% \hline\hline
%  \bf \multirow{2}{*}{Method}& \multicolumn{4}{|c|}{\textbf{OSDD}}  & \multicolumn{4}{|c|}{\textbf{CGQA-States}}  & \multicolumn{4}{|c|}{\textbf{MIT-States}}  \\      \cline{2-13} 
%  & \textbf{LSTM}& \textbf{GCN}   & \textbf{R-GCN}& \textbf{Tr-GCN}& \textbf{LSTM}& \textbf{GCN}   & \textbf{R-GCN}& \textbf{Tr-GCN}& \textbf{LSTM}& \textbf{GCN}   & \textbf{R-GCN}& \textbf{Tr-GCN} \\
 
% \cline{2-13}
% \hline

% \textbf{H2\_WN}  &   33.0/18.1 &   30.8/15.4  &   35.1/21.5 &  28.6/13.0  & 
%  37.7/27.5 &   39.9/32.9 &  40.4/31.5 &  38.2/28.1 & 
% 31.1/11.4  &  35.2/22.3 & 43.0/20.0 & 39.7/27.3 \\ 

% \textbf{H2\_CN}  
%  & 38.1/24.6 &  44.0/31.0 &  42.2/29.2 &   40.2/27.7
%  & 43.0/34.0 &   36.2/26.7 & 34.5/24.5 &   39.2/28.8 
% & 44.0/20.1 & 59.2/29.3  & 51.2/30.3 &  53.6/43.7 \\

% \textbf{H2\_CNWN} & 38.4/27.8 & 47.7/35.0 &  37.9/25.2 &  47.1/33.4 & 
%  43.0/32.2&42.6/35.2 &   43.5/33.7 &  35.8/24.5 & 

% 54.0/47.1& 53.8/2.1 &  47.8/37.2 &  52.7/42.7 \\  

% \textbf{H2\_RN}  & 15.7 / 7.1 &  12.3 / 5.4 &  16.9 / 8.0 & 19.4 / 9.3 & 
%  33.7 / 19.0 &   13.1 / 6.8 &     12.8 / 5.8 &   20.1 / 9.0 & 
% 23.0 / 4.3  &   34.7 / 22.1 &  24.7 / 17.6  &  33.8 / 21.1 \\   \hline \hline  

% \textbf{Aggr} & 39.0/25.7 & 40.0/27.0 & 42.9/29.9 &  43.2/30.3 & 28.3/37.8 & 30.6/40.2 &  29.0/38.1 &28.2/38.5 & 47.7/30.7 & 50.7/34.3  & 53.7/36.6 & 51.2/39.8  \\   \hline


    
% \hline
% \end{tabular}
% \end{minipage}}
% \label{tab:abl1}
% \caption{Ablation results for the framework architecture. The first four rows present the performance of four different models for each architecture, whereas the last row presents the aggregated average of all models. The first (second) value in each cell corresponds to the HM(AUC).}

% \end{table*}









% \begin{table*}[t]
%	\small
    % \centering
%       \resizebox{0.74\textwidth}{!}{\begin{minipage}{\textwidth} 
%     \begin{tabular}{|c|cccc|cccc|cccc|}

% \hline\hline
%  \bf \multirow{2}{*}{Method}& \multicolumn{4}{|c|}{\textbf{OSDD}}  & \multicolumn{4}{|c|}{\textbf{CGQA-States}}  & \multicolumn{4}{|c|}{\textbf{MIT-States}}  \\      \cline{2-13} 
%  & \textbf{H2\_NP}& \textbf{H2\_TH}   & \textbf{H3\_NP}&  \textbf{H3\_TH}&  \textbf{H2\_NP}& \textbf{H2\_TH}   & \textbf{H3\_NP}
%  &   \textbf{H3\_TH}&\textbf{H2\_NP}& \textbf{H2\_TH}   & \textbf{H3\_NP} & \textbf{H3\_TH} \\
 
% \cline{2-13}
% \hline
% \textbf{CN} & 
% 39.2 /25.5 &   43.0 /28.5 &  40.7 /26.1 & 43.5/31.5
% & 43.0 /32.2 &  38.8 /31.9 & 39.6 /31.5 &  38.5/26.4  &
%  42.5 /17.4 &   48.2 /25.3 & 52.4 /26.1  & 49.2/38.7 \\ 
% \textbf{WN}  & 43.5/30.6 & 43.7/30.7 &   42.4/28.2 &   40.1/26.3 & 
%  42.6/35.2 & 40.5/32.0  &  37.0/27.6 & 36.4/27.1 &
%  59.0/29.7 & 56.6/29.4 & 56.9/27.2   & 54.8/25.6  \\
% \textbf{CN+WN}  & 41.2/28.7 &   41.2/28.5 &   40.0/27.3 & 47.3/32.3 & 
% 43.5/33.7 &   41.6/31.6 &     46.2/39.2 &    38.8/28.6 &
% 51.2/30.2 &  51.8/30.0  &  58.4/28.4  & 57.2/46.5 \\  \hline 
% \textbf{Aggr} &42.3/29.6  &  43.2/30.4 & 39.6/26.6  & 41.0/27.6 &
% 30.1/39.8  & 28.4/37.7 & 22.1/32.3  &  31.4/41.0 &
% 51.7/35.5 & 52.5/36.8 & 44.0/32.1 & 54.8/36.5 \\
% \hline
% \end{tabular}
% \label{tab:abl3}
% \end{minipage}}
% \caption{Ablation results for the number of hops and the threshold policy. The first three rows present the average performance of each KG source combination for each option, whereas the last row presents the aggregated average of all models. The first (second) value in each cell corresponds to the best HM (AUC).}
% \end{table*}


% \begin{table}[t]
% %	\small
%     % \centering
%        \resizebox{0.86\textwidth}{!}{\begin{minipage}{\textwidth} 
%     \begin{tabular}{|c|cc||cc|}

% \hline\hline
%  \bf \diagbox[innerleftsep=.05cm,innerrightsep=4pt]{{\bf {Dataset}}}{{\bf {Arch}}} & 
%   \textbf{H2\_NP}& \textbf{H2\_THR}   &  \textbf{H3\_NP}& \textbf{H3\_THR}
 
% \cline{2-13}
% \hline

% \textbf{OSDD}  & 42.3/29.6  &  \bf 43.2/30.4 & 39.6/26.6  & 41.0/27.6 \\
% \hline
% \textbf{CGQA-States}  &30.1/39.8  & 28.4/37.7 & 22.1/32.3  &  \bf 31.4/41.0 
%   \\ \hline



% \textbf{MIT-States} & 51.7/35.5 & 52.5/\textbf{36.8} & 44.0/32.1 & \textbf{54.8}/36.5 \\   \hline  

% % \textbf{Aggr} &42.3/29.6  &  43.2/30.4 & 39.6/26.6  & 41.0/27.6 &
% % 30.1/39.8  & 28.4/37.7 & 22.1/32.3  &  31.4/41.0 &
% % 51.7/35.5 & 52.5/36.8 & 44.0/32.1 & 54.8/36.5 \\
% \hline
% \end{tabular}
% \label{tab:abl3}
% \end{minipage}}
% \caption{Ablation results for the number of hops and the threshold policy. The first three rows present the average performance of each KG source combination for each option, whereas the last row presents the aggregated average of all models. The first (second) value in each cell corresponds to the best HM (AUC).}
% \end{table*}



% \vspace*{0.2cm}\noindent\textbf{Results Analysis}:
% Regarding the GNN architecture, we can see that the Tr-GCN framework outperforms the rest of the framework for every different model combination by a clear margin in every dataset.
% The ablation for the different GNN architectures is presented in \autoref{tab:abl1}. We can see that   the Tr-GCN framework  outperforms the other frameworks in all datasets w.r.t. AUC metric. whereas it scores best w.r.t. HM metric in the OSSD and comes second in the two other datasets. The R-GCN framework exhibits the second-best performance, while the GCN framework comes in third and the LSTM framework exhibits the worst performance respectively. These findings are consistent with prior research in the domain of zero-shot object classification and  substantiate   \textbf{Hypothesis 2}.



% Furthermore, we can see that the model with the unrelated embeddings (CN\_H3\_UN\_Tr-GCN) achieves an accuracy in seen classes a performance similar to the model of the same characteristics and standard embeddings (CN\_H3\_Tr-GCN). However, CN\_H3\_UN\_Tr-GCN accuracy in unseen classes and its HM and AUC score is about three to four times inferior to CN\_H3\_Tr-GCN. In contrast, the random model performs poor in all four metrics.  The difference between the embeddings of the CN\_H3\_UN\_Tr-GCN and the random model concerns their distribution: in the former case the GNN allows a balanced and representative distribution, whereas in the latter case it is totally random.
% This indicates that the fine-tuning process can result in competing seen accuracy even if the embeddings utilized are unrelated to the target labels  as long as they are  distributed adequately. In contrast, the accuracy for the unseen classes depends on an exact mapping between the embeddings and the target states. 
% Additionally, it can be observed that the model employing irrelevant embeddings (CN\_H3\_UN\_Tr-GCN) performs similarly to the model using standard embeddings (CN\_H3\_Tr-GCN) in terms of accuracy for seen classes. However, the accuracy for unseen classes, as well as HM and AUC scores, are three to four times lower in CN\_H3\_UN\_Tr-GCN compared to CN\_H3\_Tr-GCN. On the other hand, the random model exhibits poor performance in all four metrics. The distinction between the embeddings of CN\_H3\_UN\_Tr-GCN and the random model lies in their distribution: the former model employs a balanced and representative distribution enabled by GNN, while the latter model has a completely random distribution. This suggests that the fine-tuning process can yield competitive seen accuracy even with unrelated embeddings to the target labels as long as the distribution is appropriate. In contrast, achieving accuracy for unseen classes requires an exact mapping between the embeddings and the target states. 


% Regarding the maximum number of hops (as shown in the first two columns of \autoref{tab:abl3}), no consistent pattern can be identified.  The best average performance is achived for the OSDD dataset at a hop count of 2, while best average performance is exhibited for  the CGQA-State dataset   at a hop count of 3. In the case of MIT-States, there is no clear winner, as hop 2 shows superior AUC and hop 3 exhibits superior HM. This suggests that introducing additional nodes beyond a certain limit may introduce noise, potentially  impacting negatively overall performance in specific cases, as observed in the OSDD dataset. This outcome is consistent with the \textbf{Hypothesis 1}.

% In terms of the maximum number of hops (first two columns of \autoref{tab:abl3}),  no consistent pattern detect can be detected , since in the case of OSDD the best average performance is achieved for hop equal to 2, while in the case of CGQA-State  the best average performance is achieved for hop equal to 3 and in the case of MIT-States there is no obvious winner(superior AUC for hop 2 and superior HM for hop 3). This outcome suggests that the introduction of additional nodes beyond a certain limit introduces noise that could affect adversely overall performance in some instances(in our case, this happens for OSDD dataset). 


% Concerning the node inclusion policy (as shown in the last two columns of \autoref{tab:abl3}),   it is evident that adopting this policy leads to significant performance improvements across all three datasets. This finding complements the previous observation regarding the number of hops and further strengthens the notion that the presence of noisy nodes can have a detrimental effect on model performance. These results align with the hypothesis stated earlier (\textbf{Hypothesis 1}).

% From the previous, it follows that the optimal version of our method should employ  the Tr-GCN arhictecture for its GNN module. Moreover, for the construction of its KG both  Concept and WordNet should be queried and a node inclusion policy should be followed during this process. Finally, regarding the number of maximum hops the optimal number seems to depend on the size of the KG and the type of the dataset. Determining the ideal number of hops may require experimentation and careful consideration of the specific requirements of the task at hand.

% Based on the previous findings, the optimal version of our method would utilize the Tr-GCN architecture for its GNN module. Additionally, for constructing the knowledge graph (KG), querying both Concept and WordNet would be beneficial. During this process, it is recommended to follow a node inclusion policy to ensure comprehensive representation.
% Regarding the number of maximum hops in the KG, the optimal value appears to be dependent on the size of the KG and the characteristics of the dataset. Finding the ideal number of hops may require experimentation and considering the specific requirements of the task at hand.


% Based on our previous findings, we recommend using the Tr-GCN architecture as the GNN module in our method, as it has demonstrated superior performance. Furthermore, querying both the Concept and WordNet repositories is advantageous for constructing the knowledge graph (KG). It is advisable to follow a node inclusion policy during the construction process to ensure a comprehensive representation of the data.
% Regarding the number of maximum hops in the KG, the optimal value depends on factors such as the size of the KG and the characteristics of the dataset. Determining the ideal number of hops may require experimentation and careful consideration of the specific requirements of the task at hand.


% \begin{table}[t]
	
%     \centering
%       \resizebox{0.7\textwidth}{!}{\begin{minipage}{\textwidth} 
%     \begin{tabular}{|l|r|r|r|l|}

% \hline\hline
% \textbf{KG} &   \textbf{N}  &  \textbf{E}   &  \textbf{RT} & \textbf{RC}\\ \hline\hline



% WN\_H2 & 70/54/49 &  321/223/105 &       5 &LX\\ \hline
% WN\_H3 & 429/311/295 &  873/680/655 &       5 &LX\\ \hline\hline


% CN\_H2 &   715/552/504  & 2,132/1,981/1,864  &    13 &CS    \\ \hline
% CN\_H3 & 2,139/1,872/1,788  &  2,542/2,194/2,103 &       24 &CS    \\ \hline 
% CN\_H2\_TH & 611/505/485 & 1710/1521/1415  &       12 &CS  \\ \hline
% CN\_H3\_TH & 12,733/9,839/9,212 &  29,794/25,105/24,292&       29 & CS \\ \hline \hline
  
  
% CN+WN\_H2 & 667/581/506 & 1,906/1,682/1,602  &       13 &CS \\ \hline
% CN+WN\_H2\_TH & 590/492/431 & 1,442/1,167/1,089 &       12 &CS/LX    \\ \hline
% CN+WN\_H3\_TH &  10,165/8,842/7,948 &  26,735/23,176/22,602   &       29 & CS/LX   \\ \hline 
%   \end{tabular}
  

   
%      \end{minipage}}
  
%    \caption{KGs Details. N: Number of Nodes. E: Number of Edges.  RN: Number of Different Relation Types between nodes. RC: Category of Relation Types. CS: Common-Sense. LX: Lexicographic. First/Second/Third number in the N and E columns refers to the KG for OSDD/CGQA-States/Mit-States dataset respectively.}

%     \label{tab:KGS}
        
% \end{table}


 



