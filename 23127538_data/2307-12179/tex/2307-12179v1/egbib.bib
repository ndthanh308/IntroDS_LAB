
@inproceedings{speer2017conceptnet,
  title={Conceptnet 5.5: An open multilingual graph of general knowledge},
  author={Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}

@article{zhang2019integrating,
  title={Integrating semantic knowledge to tackle zero-shot text classification},
  author={Zhang, Jingqing and Lertvittayakumjorn, Piyawat and Guo, Yike},
  journal={arXiv preprint arXiv:1903.12626},
  year={2019}
}

@inproceedings{lee2018multi,
  title={Multi-label zero-shot learning with structured knowledge graphs},
  author={Lee, Chung-Wei and Fang, Wei and Yeh, Chih-Kuan and Wang, Yu-Chiang Frank},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1576--1585},
  year={2018}
}


@inproceedings{duan2012discovering,
  title={Discovering localized attributes for fine-grained recognition},
  author={Duan, Kun and Parikh, Devi and Crandall, David and Grauman, Kristen},
  booktitle={2012 IEEE conference on computer vision and pattern recognition},
  pages={3474--3481},
  year={2012},
  organization={IEEE}
}
@inproceedings{yu2017semantic,
  title={Semantic jitter: Dense supervision for visual comparisons via synthetic images},
  author={Yu, Aron and Grauman, Kristen},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={5570--5579},
  year={2017}
}

@article{xian2018zero,
  title={Zero-shot learningâ€”a comprehensive evaluation of the good, the bad and the ugly},
  author={Xian, Yongqin and Lampert, Christoph H and Schiele, Bernt and Akata, Zeynep},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={9},
  pages={2251--2265},
  year={2018},
  publisher={IEEE}
}


@inproceedings{lampert2009learning,
  title={Learning to detect unseen object classes by between-class attribute transfer},
  author={Lampert, Christoph H and Nickisch, Hannes and Harmeling, Stefan},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={951--958},
  year={2009},
  organization={IEEE}
}
@inproceedings{patterson2016coco,
  title={Coco attributes: Attributes for people, animals, and objects},
  author={Patterson, Genevieve and Hays, James},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14},
  pages={85--100},
  year={2016},
  organization={Springer}
}

@inproceedings{singh2016end,
  title={End-to-end localization and ranking for relative attributes},
  author={Singh, Krishna Kumar and Lee, Yong Jae},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14},
  pages={753--769},
  year={2016},
  organization={Springer}
}

@book{Radenovic2021,
abstract = {We study the problem of learning how to predict attribute-object compositions from images, and its generalization to unseen compositions missing from the training data. To the best of our knowledge, this is a first large-scale study of this problem, involving hundreds of thousands of compositions. We train our framework with images from Instagram using hashtags as noisy weak supervision. We make careful design choices for data collection and modeling, in order to handle noisy annotations and unseen compositions. Finally, extensive evaluations show that learning to compose classifiers outperforms late fusion of individual attribute and object predictions, especially in the case of unseen attribute-object pairs.},
archivePrefix = {arXiv},
arxivId = {2105.11373},
author = {Radenovic, Filip and Sinha, Animesh and Gordo, Albert and Berg, Tamara and Mahajan, Dhruv},
booktitle = {Proceedings of ACM Conference (Conference'17)},
eprint = {2105.11373},
file = {:home/philo/Documents/Mendeley Desktop/Radenovic et al. - 2021 - Large-Scale Attribute-Object Compositions.pdf:pdf},
keywords = {54,attribute-object compositions,attribute-object compositions, classification, dat,by our approach in,classification,datasets,example attribute-object compositions predicted,figure 1,the yfcc100m dataset},
number = {1},
publisher = {Association for Computing Machinery},
title = {{Large-Scale Attribute-Object Compositions}},
url = {http://arxiv.org/abs/2105.11373},
volume = {1},
year = {2021}
}

@inproceedings{reed2016learning,
  title={Learning deep representations of fine-grained visual descriptions},
  author={Reed, Scott and Akata, Zeynep and Lee, Honglak and Schiele, Bernt},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={49--58},
  year={2016}
}
@article{socher2013zero,
  title={Zero-shot learning through cross-modal transfer},
  author={Socher, Richard and Ganjoo, Milind and Manning, Christopher D and Ng, Andrew},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@inproceedings{xian2018feature,
  title={Feature generating networks for zero-shot learning},
  author={Xian, Yongqin and Lorenz, Tobias and Schiele, Bernt and Akata, Zeynep},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5542--5551},
  year={2018}
}

@inproceedings{akata2013label,
  title={Label-embedding for attribute-based classification},
  author={Akata, Zeynep and Perronnin, Florent and Harchaoui, Zaid and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={819--826},
  year={2013}
}


@article{Lampert2014,
abstract = {We study the problem of object recognition for categories for which we have no training examples, a task also called zero-data or zero-shot learning. This situation has hardly been studied in computer vision research, even though it occurs frequently; the world contains tens of thousands of different object classes, and image collections have been formed and suitably annotated for only a few of them. To tackle the problem, we introduce attribute-based classification: Objects are identified based on a high-level description that is phrased in terms of semantic attributes, such as the object's color or shape. Because the identification of each such property transcends the specific learning task at hand, the attribute classifiers can be prelearned independently, for example, from existing image data sets unrelated to the current task. Afterward, new classes can be detected based on their attribute representation, without the need for a new training phase. In this paper, we also introduce a new data set, Animals with Attributes, of over 30,000 images of 50 animal classes, annotated with 85 semantic attributes. Extensive experiments on this and two more data sets show that attribute-based classification indeed is able to categorize images without access to any training images of the target classes. {\textcopyright} 2014 IEEE.},
author = {Lampert, Christoph H. and Nickisch, Hannes and Harmeling, Stefan},
doi = {10.1109/TPAMI.2013.140},
file = {:home/philo/Documents/Mendeley Desktop/lampert13attributes.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object recognition,vision and scene understanding},
number = {3},
pages = {453--465},
pmid = {24457503},
publisher = {IEEE},
title = {{Attribute-based classification for zero-shot visual object categorizationa}},
volume = {36},
year = {2014}
}


@article{Purushwalkam,
author = {Purushwalkam, Senthil and Nickel, Maximilian and Gupta, Abhinav and Ranzato, Marc Aurelio},
file = {:home/philo/Documents/Mendeley Desktop/Purushwalkam et al. - Unknown - Task-Driven Modular Networks for Zero-Shot Compositional Learning.pdf:pdf},
pages = {3593--3602},
title = {{Task-Driven Modular Networks for Zero-Shot Compositional Learning}}
}


@inproceedings{schlichtkrull2018modeling,
  title={Modeling relational data with graph convolutional networks},
  author={Schlichtkrull, Michael and Kipf, Thomas N and Bloem, Peter and Van Den Berg, Rianne and Titov, Ivan and Welling, Max},
  booktitle={The Semantic Web: 15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3--7, 2018, Proceedings 15},
  pages={593--607},
  year={2018},
  organization={Springer}
}

@inproceedings{shang2019end,
  title={End-to-end structure-aware convolutional networks for knowledge base completion},
  author={Shang, Chao and Tang, Yun and Huang, Jing and Bi, Jinbo and He, Xiaodong and Zhou, Bowen},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={3060--3067},
  year={2019}
}
@article{xiong2019imposing,
  title={Imposing label-relational inductive bias for extremely fine-grained entity typing},
  author={Xiong, Wenhan and Wu, Jiawei and Lei, Deren and Yu, Mo and Chang, Shiyu and Guo, Xiaoxiao and Wang, William Yang},
  journal={arXiv preprint arXiv:1903.02591},
  year={2019}
}
@inproceedings{yao2019graph,
  title={Graph convolutional networks for text classification},
  author={Yao, Liang and Mao, Chengsheng and Luo, Yuan},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={7370--7377},
  year={2019}
}

@article{wu1994verb,
  title={Verb semantics and lexical selection},
  author={Wu, Zhibiao and Palmer, Martha},
  journal={arXiv preprint cmp-lg/9406033},
  year={1994}
}
@article{Monka2022,
abstract = { KGs can represent auxiliary knowledge either in an underlying graph-structured schema or in a vector-based knowledge graph embedding. Intending to enable the reader to solve visual transfer learning problems with the help of specific KG-DL configurations we start with a description of relevant modeling structures of a KG of various expressions, such as directed labeled graphs, hypergraphs, and hyper-relational graphs. We explain the notion of feature extractor, while specifically referring to visual and semantic features. We provide a broad overview of knowledge graph embedding methods and describe several joint training objectives suitable to combine them with high dimensional visual embeddings. The main section introduces four different categories on how a KG can be combined with a DL pipeline: 1) Knowledge Graph as a Reviewer; 2) Knowledge Graph as a Trainee; 3) Knowledge Graph as a Trainer; and 4) Knowledge Graph as a Peer. To help researchers find meaningful evaluation benchmarks, we provide an overview of generic KGs and a set of image processing datasets and benchmarks that include various types of auxiliary knowledge. Last, we summarize related surveys and give an outlook about challenges and open issues for future research.},
archivePrefix = {arXiv},
arxivId = {2201.11794},
author = {Monka, Sebastian and Halilaj, Lavdim and Rettinger, Achim},
doi = {10.3233/SW-212959},
eprint = {2201.11794},
file = {:home/philippos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Monka, Halilaj, Rettinger - 2022 - A survey on visual transfer learning using knowledge graphs.pdf:pdf},
issn = {22104968},
journal = {Semantic Web},
keywords = {Knowledge graph, visual transfer learning, knowled,knowledge-based machine learning,visual transfer learning},
number = {3},
pages = {477--510},
title = {{A survey on visual transfer learning using knowledge graphs}},
volume = {13},
year = {2022}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@article{bhagavatula2019abductive,
  title={Abductive commonsense reasoning},
  author={Bhagavatula, Chandra and Bras, Ronan Le and Malaviya, Chaitanya and Sakaguchi, Keisuke and Holtzman, Ari and Rashkin, Hannah and Downey, Doug and Yih, Scott Wen-tau and Choi, Yejin},
  journal={arXiv preprint arXiv:1908.05739},
  year={2019}
}
@inproceedings{zhang2019tgg,
  title={TGG: Transferable graph generation for zero-shot and few-shot learning},
  author={Zhang, Chenrui and Lyu, Xiaoqing and Tang, Zhi},
  booktitle={Proceedings of the 27th ACM International Conference on Multimedia},
  pages={1641--1649},
  year={2019}
}
@article{bosselut2019comet,
  title={COMET: Commonsense transformers for automatic knowledge graph construction},
  author={Bosselut, Antoine and Rashkin, Hannah and Sap, Maarten and Malaviya, Chaitanya and Celikyilmaz, Asli and Choi, Yejin},
  journal={arXiv preprint arXiv:1906.05317},
  year={2019}
}

@article{bastings2017graph,
  title={Graph convolutional encoders for syntax-aware neural machine translation},
  author={Bastings, Jasmijn and Titov, Ivan and Aziz, Wilker and Marcheggiani, Diego and Sima'an, Khalil},
  journal={arXiv preprint arXiv:1704.04675},
  year={2017}
}


@article{adhikari2020learning,
  title={Learning dynamic belief graphs to generalize on text-based games},
  author={Adhikari, Ashutosh and Yuan, Xingdi and C{\^o}t{\'e}, Marc-Alexandre and Zelinka, Mikul{\'a}{\v{s}} and Rondeau, Marc-Antoine and Laroche, Romain and Poupart, Pascal and Tang, Jian and Trischler, Adam and Hamilton, Will},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3045--3057},
  year={2020}
}

@inproceedings{wu2019simplifying,
  title={Simplifying graph convolutional networks},
  author={Wu, Felix and Souza, Amauri and Zhang, Tianyi and Fifty, Christopher and Yu, Tao and Weinberger, Kilian},
  booktitle={International conference on machine learning},
  pages={6861--6871},
  year={2019},
  organization={PMLR}
}
@article{vashishth2019composition,
  title={Composition-based multi-relational graph convolutional networks},
  author={Vashishth, Shikhar and Sanyal, Soumya and Nitin, Vikram and Talukdar, Partha},
  journal={arXiv preprint arXiv:1911.03082},
  year={2019}
}

@article{hamilton2017inductive,
  title={Inductive representation learning on large graphs},
  author={Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}


@inproceedings{gouidis2022,
  title={ Detecting Object States vs Detecting Objects: A New Dataset and a Quantitative Experimental Study},
  author={Gouidis, F. and  Patkos, T. and Argyros, A. and Plexousakis, D. },
  booktitle={ Proceedings of the 17th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications  (VISAPP)},
  year={2022},
  volume={5},
  pages={590--600}
}

@incollection{fellbaum2010wordnet,
  title={WordNet},
  author={Fellbaum, Christiane},
  booktitle={Theory and applications of ontology: computer applications},
  pages={231--243},
  year={2010},
  publisher={Springer}
}

@article{nayak:tmlr22,
  Author = {Nayak, N. V. and Bach, S. H.},
  Title = {Zero-Shot Learning with Common Sense Knowledge Graphs},
  Journal = {Transactions on Machine Learning Research (TMLR)},
  Year = {2022}}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer}
}
@inproceedings{akata2015evaluation,
  title={Evaluation of output embeddings for fine-grained image classification},
  author={Akata, Zeynep and Reed, Scott and Walter, Daniel and Lee, Honglak and Schiele, Bernt},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2927--2936},
  year={2015}
}


@article{vrandevcic2014wikidata,
  title={Wikidata: a free collaborative knowledgebase},
  author={Vrande{\v{c}}i{\'c}, Denny and Kr{\"o}tzsch, Markus},
  journal={Communications of the ACM},
  volume={57},
  number={10},
  pages={78--85},
  year={2014},
  publisher={ACM New York, NY, USA}
}


@inproceedings{auer2007dbpedia,
  title={Dbpedia: A nucleus for a web of open data},
  author={Auer, S{\"o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  booktitle={The Semantic Web: 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007+ ASWC 2007, Busan, Korea, November 11-15, 2007. Proceedings},
  pages={722--735},
  year={2007},
  organization={Springer}
}

@article{atzmon2020causal,
  title={A causal view of compositional zero-shot recognition},
  author={Atzmon, Yuval and Kreuk, Felix and Shalit, Uri and Chechik, Gal},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1462--1473},
  year={2020}
}

@inproceedings{purushwalkam2019task,
  title={Task-driven modular networks for zero-shot compositional learning},
  author={Purushwalkam, Senthil and Nickel, Maximilian and Gupta, Abhinav and Ranzato, Marc'Aurelio},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3593--3602},
  year={2019}
}
@article{Li2020,
abstract = {Attributes and objects can compose diverse compositions. To model the compositional nature of these general concepts, it is a good choice to learn them through transformations, such as coupling and decoupling. However, complex transformations need to satisfy specific principles to guarantee the rationality. In this paper, we first propose a previously ignored principle of attribute-object transformation: Symmetry. For example, coupling peeled-apple with attribute peeled should result in peeled-apple, and decoupling peeled from apple should still output apple. Incorporating the symmetry principle, a transformation framework inspired by group theory is built, i.e. SymNet. SymNet consists of two modules, Coupling Network and Decoupling Network. With the group axioms and symmetry property as objectives, we adopt Deep Neural Networks to implement SymNet and train it in an end-to-end paradigm. Moreover, we propose a Relative Moving Distance (RMD) based recognition method to utilize the attribute change instead of the attribute pattern itself to classify attributes. Our symmetry learning can be utilized for the Compositional Zero-Shot Learning task and outperforms the state-of-the-art on widely-used benchmarks. Code is available at https://github.com/DirtyHarryLYL/SymNet.},
  title={Symmetry and group in attribute-object compositions},
  author={Li, Yong-Lu and Xu, Yue and Mao, Xiaohan and Lu, Cewu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11316--11325},
  year={2020}
}



@inproceedings{yang2020learning,
  title={Learning unseen concepts via hierarchical decomposition and composition},
  author={Yang, Muli and Deng, Cheng and Yan, Junchi and Liu, Xianglong and Tao, Dacheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10248--10256},
  year={2020}
}
@inproceedings{misra2017red,
  title={From red wine to red tomato: Composition with context},
  author={Misra, Ishan and Gupta, Abhinav and Hebert, Martial},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1792--1801},
  year={2017}
}


@inproceedings{nagarajan2018attributes,
  title={Attributes as operators: factorizing unseen attribute-object compositions},
  author={Nagarajan, Tushar and Grauman, Kristen},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={169--185},
  year={2018}
}

@inproceedings{changpinyo2016synthesized,
  title={Synthesized classifiers for zero-shot learning},
  author={Changpinyo, Soravit and Chao, Wei-Lun and Gong, Boqing and Sha, Fei},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5327--5336},
  year={2016}
}

@article{Wang2018b,
abstract = {We consider the problem of zero-shot recognition: learning a visual classifier for a category with zero training examples, just using the word embedding of the category and its relationship to other categories, which visual data are provided. The key to dealing with the unfamiliar or novel category is to transfer knowledge obtained from familiar classes to describe the unfamiliar class. In this paper, we build upon the recently introduced Graph Convolutional Network (GCN) and propose an approach that uses both semantic embeddings and the categorical relationships to predict the classifiers. Given a learned knowledge graph (KG), our approach takes as input semantic embeddings for each node (representing visual category). After a series of graph convolutions, we predict the visual classifier for each category. During training, the visual classifiers for a few categories are given to learn the GCN parameters. At test time, these filters are used to predict the visual classifiers of unseen categories. We show that our approach is robust to noise in the KG. More importantly, our approach provides significant improvement in performance compared to the current state-of-the-art results (from 2 $\sim$ 3% on some metrics to whopping 20% on a few).},
archivePrefix = {arXiv},
arxivId = {arXiv:1803.08035v2},
author = {Wang, Xiaolong and Ye, Yufei and Gupta, Abhinav},
doi = {10.1109/CVPR.2018.00717},
eprint = {arXiv:1803.08035v2},
file = {:home/philo/Documents/Mendeley Desktop/Wang, Ye, Gupta - 2018 - Zero-Shot Recognition via Semantic Embeddings and Knowledge Graphs.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Object recognition - detection/Zero shot - Few shot},
pages = {6857--6866},
title = {{Zero-Shot Recognition via Semantic Embeddings and Knowledge Graphs}},
year = {2018}
}



@article{Kampffmeyer2019,
abstract = {Graph convolutional neural networks have recently shown great potential for the task of zero-shot learning. These models are highly sample efficient as related concepts in the graph structure share statistical strength allowing generalization to new classes when faced with a lack of data. However, multi-layer architectures, which are required to propagate knowledge to distant nodes in the graph, dilute the knowledge by performing extensive Laplacian smoothing at each layer and thereby consequently decrease performance. In order to still enjoy the benefit brought by the graph structure while preventing dilution of knowledge from distant nodes, we propose a Dense Graph Propagation (DGP) module with carefully designed direct links among distant nodes. DGP allows us to exploit the hierarchical graph structure of the knowledge graph through additional connections. These connections are added based on a node's relationship to its ancestors and descendants. A weighting scheme is further used to weigh their contribution depending on the distance to the node to improve information propagation in the graph. Combined with finetuning of the representations in a two-stage training approach our method outperforms state-of-the-art zero-shot learning approaches.},
archivePrefix = {arXiv},
arxivId = {1805.11724},
author = {Kampffmeyer, Michael and Chen, Yinbo and Liang, Xiaodan and Wang, Hao and Zhang, Yujia and Xing, Eric P.},
doi = {10.1109/CVPR.2019.01175},
eprint = {1805.11724},
file = {:home/philo/Documents/Mendeley Desktop/Kampffmeyer et al. - 2019 - Rethinking knowledge graph propagation for zero-shot learning.pdf:pdf;:home/philo/Documents/Mendeley Desktop//Kampffmeyer et al. - 2019 - Rethinking knowledge graph propagation for zero-shot learning.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Categorization,Deep Learning,Recognition: Detection,Retrieval},
pages = {11479--11488},
title = {{Rethinking knowledge graph propagation for zero-shot learning}},
volume = {2019-June},
year = {2019}
}
@inproceedings{chuang2018learning,
  title={Learning to act properly: Predicting and explaining affordances from images},
  author={Chuang, Ching-Yao and Li, Jiaman and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={975--983},
  year={2018}
}

@article{Soucek2022,
abstract = {We aim to learn to temporally localize object state changes and the corresponding state-modifying actions by observing people interacting with objects in long uncurated web videos. We introduce three principal contributions. First, we explore alternative multi-task network architectures and identify a model that enables efficient joint learning of multiple object states and actions such as pouring water and pouring coffee. Second, we design a multi-task self-supervised learning procedure that exploits different types of constraints between objects and state-modifying actions enabling end-to-end training of a model for temporal localization of object states and actions in videos from only noisy video-level supervision. Third, we report results on the large-scale ChangeIt and COIN datasets containing tens of thousands of long (un)curated web videos depicting various interactions such as hole drilling, cream whisking, or paper plane folding. We show that our multi-task model achieves a relative improvement of 40\% over the prior single-task methods and significantly outperforms both image-based and video-based zero-shot models for this problem. We also test our method on long egocentric videos of the EPIC-KITCHENS and the Ego4D datasets in a zero-shot setup demonstrating the robustness of our learned model.},
archivePrefix = {arXiv},
arxivId = {2211.13500},
author = {Sou{\v{c}}ek, Tom{\'{a}}{\v{s}} and Alayrac, Jean-Baptiste and Miech, Antoine and Laptev, Ivan and Sivic, Josef},
eprint = {2211.13500},
file = {:home/philippos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sou{\v{c}}ek et al. - 2022 - Multi-Task Learning of Object State Changes from Uncurated Videos.pdf:pdf},
mendeley-groups = {State Detection},
title = {{Multi-Task Learning of Object State Changes from Uncurated Videos}},
url = {http://arxiv.org/abs/2211.13500},
year = {2022}
}

@inproceedings{liu2017jointly,
  title={Jointly recognizing object fluents and tasks in egocentric videos},
  author={Liu, Yang and Wei, Ping and Zhu, Song-Chun},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2924--2932},
  year={2017}
}


@article{Sawatzky2019,
abstract = {When humans have to solve everyday tasks, they simply pick the objects that are most suitable. While the question which object should one use for a specific task sounds trivial for humans, it is very difficult to answer for robots or other autonomous systems. This issue, however, is not addressed by current benchmarks for object detection that focus on detecting object categories. We therefore introduce the COCO-Tasks dataset which comprises about 40,000 images where the most suitable objects for 14 tasks have been annotated. We furthermore propose an approach that detects the most suitable objects for a given task. The approach builds on a Gated Graph Neural Network to exploit the appearance of each object as well as the global context of all present objects in the scene. In our experiments, we show that the proposed approach outperforms other approaches that are evaluated on the dataset like classification or ranking approaches.},
archivePrefix = {arXiv},
arxivId = {1904.03000},
author = {Sawatzky, Johann and Souri, Yaser and Grund, Christian and Gall, Juergen},
eprint = {1904.03000},
file = {:home/philippos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sawatzky et al. - 2019 - What Object Should I Use - Task Driven Object Detection.pdf:pdf},
mendeley-groups = {Affordances - Functionality},
title = {{What Object Should I Use? - Task Driven Object Detection}},
url = {http://arxiv.org/abs/1904.03000},
year = {2019}
}


@article{Isola2015,
abstract = {Objects in visual scenes come in a rich variety of transformed states. A few classes of transformation have been heavily studied in computer vision: mostly simple, parametric changes in color and geometry. However, transformations in the physical world occur in many more flavors, and they come with semantic meaning: e.g., bending, folding, aging, etc. The transformations an object can undergo tell us about its physical and functional properties. In this paper, we introduce a dataset of objects, scenes, and materials, each of which is found in a variety of transformed states. Given a novel collection of images, we show how to explain the collection in terms of the states and transformations it depicts. Our system works by generalizing across object classes: states and transformations learned on one set of objects are used to interpret the image collection for an entirely new object class.},
author = {Isola, Phillip and Lim, Joseph J. and Adelson, Edward H.},
doi = {10.1109/CVPR.2015.7298744},
file = {:home/philo/Documents/Mendeley Desktop/Isola, Lim, Adelson - 2015 - Discovering states and transformations in image collections.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {State Detection},
pages = {1383--1391},
title = {{Discovering states and transformations in image collections}},
volume = {07-12-June},
year = {2015}
}

@article{Mancini2022,
abstract = {Compositional Zero-Shot learning (CZSL) aims to recognize unseen compositions of state and object visual primitives seen during training. A problem with standard CZSL is the assumption of knowing which unseen compositions will be available at test time. In this work, we overcome this assumption operating on the open world setting, where no limit is imposed on the compositional space at test time, and the search space contains a large number of unseen compositions. To address this problem, we propose a new approach, Compositional Cosine Graph Embedding (Co-CGE), based on two principles. First, Co-CGE models the dependency between states, objects and their compositions through a graph convolutional neural network. The graph propagates information from seen to unseen concepts, improving their representations. Second, since not all unseen compositions are equally feasible, and less feasible ones may damage the learned representations, Co-CGE estimates a feasibility score for each unseen composition, using the scores as margins in a cosine similarity-based loss and as weights in the adjacency matrix of the graphs. Experiments show that our approach achieves state-of-the-art performances in standard CZSL while outperforming previous methods in the open world scenario.},
author = {Mancini, Massimiliano and Naeem, Muhammad Ferjad and Xian, Yongqin and Akata, Zeynep},
doi = {10.1109/TPAMI.2022.3163667},
file = {:home/philo/Documents/Mendeley Desktop/Mancini et al. - 2022 - Learning Graph Embeddings for Open World Compositional Zero-Shot Learning.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Compositional Zero-Shot Learning,Convolutional neural networks,Dogs,Graph Neural Networks,Open-World Recognition,Scene Understanding,Smoothing methods,Standards,Task analysis,Training,Visualization},
number = {c},
pages = {1--15},
publisher = {IEEE},
title = {{Learning Graph Embeddings for Open World Compositional Zero-Shot Learning}},
volume = {8828},
year = {2022}
}

@inproceedings{Wang2016b,
abstract = {What defines an action like 'kicking ball'? We argue that the true meaning of an action lies in the change or transformation an action brings to the environment. In this paper, we propose a novel representation for actions by modeling an action as a transformation which changes the state of the environment before the action happens (precondition) to the state after the action (effect). Motivated by recent advancements of video representation using deep learning, we design a Siamese network which models the action as a transformation on a high-level feature space. We show that our model gives improvements on standard action recognition datasets including UCF101 and HMDB51. More importantly, our approach is able to generalize beyond learned action categories and shows significant performance improvement on cross-category generalization on our new ACT dataset.},
archivePrefix = {arXiv},
arxivId = {1512.00795},
author = {Wang, Xiaolong and Farhadi, Ali and Gupta, Abhinav},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.291},
eprint = {1512.00795},
file = {:home/philippos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Farhadi, Gupta - 2016 - Actions $\sim$ Transformations.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
mendeley-groups = {Action Recognition/Low},
month = {jun},
pages = {2658--2667},
publisher = {IEEE},
title = {{Actions $\sim$ Transformations}},
url = {http://ieeexplore.ieee.org/document/7780660/},
volume = {2016-Decem},
year = {2016}
}

@article{jamone2016affordances,
  title={Affordances in psychology, neuroscience, and robotics: A survey},
  author={Jamone, Lorenzo and Ugur, Emre and Cangelosi, Angelo and Fadiga, Luciano and Bernardino, Alexandre and Piater, Justus and Santos-Victor, Jos{\'e}},
  journal={IEEE Transactions on Cognitive and Developmental Systems},
  volume={10},
  number={1},
  pages={4--25},
  year={2016},
  publisher={IEEE}
}


@article{Farhadi2009,
abstract = {We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object ("spotty dog", not just "dog"); to say something about unfamiliar objects ("hairy and four-legged", not just "unknown"); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic ("spotty") or discriminative ("dogs have it but sheep do not"). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attributebased framework. {\textcopyright} 2009 IEEE.},
author = {Farhadi, Ali and Endres, Ian and Hoiem, Derek and Forsyth, David},
doi = {10.1109/CVPRW.2009.5206772},
file = {:home/philo/Documents/Mendeley Desktop/Farhadi et al. - 2009 - Describing objects by their attributes.pdf:pdf},
isbn = {9781424439935},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2009},
mendeley-groups = {Attributes},
pages = {1778--1785},
publisher = {IEEE},
title = {{Describing objects by their attributes}},
year = {2009}
}
