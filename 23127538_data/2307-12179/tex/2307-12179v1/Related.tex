\section{Related Work}

\noindent\textbf{State/Attribute Classification}:
The most generally accepted definition of ``visual attributes" is that they are visual concepts which are detectable by machines and can be comprehended by humans~\cite{duan2012discovering}.
The current approach for learning attributes is similar to that of object classes, where a convolutional neural network is trained with discriminative classifiers using annotated image datasets~\cite{singh2016end}. However, labeled attribute image datasets often lack the data scale found in object datasets, contain a limited number of generic attributes, or cover only a few specific categories~\cite{lampert2009learning, Isola2015,patterson2016coco, yu2017semantic, Mancini2022}. The number of  works focusing exclusively on  state classification is limited~\cite{gouidis2022}. Most of them  are based on the same assumptions that are used for the task of attribute classification. 
% \vspace*{0.2cm}\noindent\textbf{Open-vocabulary Attribute Detection} 

% \vspace*{0.2cm}\noindent\textbf{Zero-shot Learning} 
% Zero-Shot Learning (ZSL) is a task that involves recognizing new classes that were not seen during training, by leveraging side information such as attributes\cite{Lampert2014}, text descriptions\cite{reed2016learning} or word embeddings \cite{socher2013zero}. Previous approaches have attempted to learn a compatibility function between image and class embeddings\cite{akata2013label} or generate image features for novel classes \cite{xian2018feature}.

\vspace*{0.2cm}\noindent\textbf{Zero-shot Object Classification}:
Zero-shot object classification has gained increasing attention in recent years due to its practical importance in real-world applications, where it is often difficult to obtain training data for all possible object classes\cite{xian2018zero}. A number of approaches have been proposed to address this problem, including semantic embedding-based methods\cite{Wang2018b, xian2018feature}, attribute-based methods\cite{Lampert2014},  generative models\cite{xian2018feature,changpinyo2016synthesized} and learning of a compatibility function between image and class embeddings \cite{akata2015evaluation}. Semantic embedding-based methods utilize a low-dimensional semantic space to represent objects and their attributes, and use this representation to map between seen and unseen object classes. Attribute-based methods leverage a set of attributes that describe object classes, and use these attributes to infer the class of an unseen object. Generative models generate samples of unseen object classes by synthesizing images that are similar to images of seen object classes. In addition to these approaches, recent work has also explored the use of knowledge graphs\cite{Kampffmeyer2019,nayak:tmlr22}, which capture semantic relationships between objects and can be used to facilitate zero-shot learning. Prior methods in zero-shot learning have typically utilized predetermined attributes or pretrained embeddings, in contrast to our proposed approach which centers on acquiring class representations directly from the knowledge graph during the task.


% Overall, these approaches have achieved promising results in zero-shot object classification and hold great potential for real-world applications such as text classification, video action recognition, and machine translation.   n

% There has been significant research on zero-shot object classification using graph neural networks, with recent works exploring the use of common sense knowledge graphs to generate class representations. However, previous methods relied on predefined attributes or pretrained embeddings, while our approach focuses on explicitly learning class representations from the knowledge graph in the task. Other notable works in zero-shot learning include text classification, video action recognition, and machine translation.  


\vspace*{0.2cm}\noindent\textbf{Compositional Zero-shot Learning}:
Compositional Zero-Shot Learning (CZSL) aims to generalize to unseen combinations of object and state primitives by learning their compositionality from the training set. Two groups of CZSL approaches have been proposed: the first group models individual classifiers of states and objects or learns a hierarchical decomposition and composition of visual primitives~\cite{misra2017red, nagarajan2018attributes,yang2020learning}, while the second group learns a joint compatibility function with respect to the image, the state, and the object by conditioning modular networks on each composition~\cite{purushwalkam2019task,atzmon2020causal}. The work in~\cite{atzmon2020causal} recently proposed learning the visual transformation through a causal graph, where the latent representations of primitives are independent of each other, as a way to achieve generalization in CZSL. The work in~\cite{Li2020} presented a transformation framework consisting of two modules inspired by group theory that incorporates the principle of symmetry in attribute-object transformations. Mancini~\cite{Mancini2022} utilizes a graph convolutional neural network to model the dependency between states, objects and their compositions, and estimates a feasibility score for each unseen composition to improve representations in open-world CZSL scenarios.  

\vspace*{0.2cm}\noindent\textbf{Graph Neural Networks}:
Graph Neural Networks have gained popularity due to their ability to learn node embeddings that reflect the structure of the graph \cite{kipf2016semi}. These networks have shown significant improvements in downstream tasks, such as node classification and graph classification \cite{hamilton2017inductive,wu2019simplifying, shang2019end,vashishth2019composition}. In this work, we make use of the transformer graph convolutional networks which has been recently used in the context of zero-shot object classification~\cite{nayak:tmlr22}. Prior works have considered transformers as a method to learn meta-paths in heterogeneous graphs rather than as a neighborhood aggregation technique.  Furthermore, graph neural networks have been applied to various applications, including fine-grained entity typing \cite{xiong2019imposing}, text classification \cite{yao2019graph}, reinforcement learning \cite{adhikari2020learning}, and neural machine translation \cite{bastings2017graph}. 


% Graph Convolutional Networks (GCN) \cite{} are a special type of neural networks that leverage the interdependencies of data (nodes) that are defined in a graph. However, current methods \cite{}  have limitations in terms of network depth as over-smoothing at deeper layers of the network can cause all nodes to converge to the same value \cite{} . To address this limitation, several approaches have been proposed, such as dense skip connections \cite{} , randomly dropping edges [\cite{} , and applying a linear combination of neighbor features \cite{} . Recent works in this direction have combined residual connections with identity mapping \cite{} or utilized GCNs for zero-shot learning. 

% For instance, \cite{}  propose to directly predict the classifier weights of novel classes using a GCN applied on an external knowledge graph such as WordNet \cite{} . \cite{}  enhance this approach by introducing a dense graph to learn a shallow GCN as a solution for the Laplacian smoothing problem.

\vspace*{0.2cm}\noindent\textbf{Leveraging common sense Knowledge Graphs}: 
Common sense knowledge graphs have been extensively utilized in various tasks including transductive zero-shot text classification \cite{zhang2019integrating} and object classification \cite{Kampffmeyer2019,xian2018zero}. Previous works such as \cite{bhagavatula2019abductive} and \cite{bosselut2019comet}  have explored the application of common sense knowledge graphs in diverse settings. The notable work in~\cite{zhang2019integrating}  used ConceptNet for transductive zero-shot text classification as shallow features for class representation.
Another related work~\cite{zhang2019tgg}  also utilized common sense knowledge graphs and graph neural networks for transductive zero-shot object classification. This approach learns to model seen-unseen relations with a graph neural network and requires knowledge of unseen classes during training, utilizing hand-crafted attributes.   Drawing inspiration from~\cite{nayak:tmlr22} which proposed a novel GNN architecture capable of generating dense vector representations from ConceptNet, we further extend this approach in a novel context.

% We draw inspiration from  where  a novel GNN architecture  is proposed capable of generating dense vector representations from ConceptNet. We extend this approach further and use it a novel context. 

% In contrast, our method does not require explicit knowledge of unseen classes during training, but  learns instead class representations from the common sense knowledge graph. Overall, common sense knowledge graphs have shown great potential in improving the performance of various tasks, and our work extends this by applying them to a novel task.