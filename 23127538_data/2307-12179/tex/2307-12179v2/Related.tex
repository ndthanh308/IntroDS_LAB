\section{Related Work}

\noindent\textbf{State/Attribute Classification}:
The most generally accepted definition of ``visual attributes" refers to visual concepts that are detectable by machines and can be comprehended by humans~\cite{duan2012discovering}.
The current approach for learning attributes in images is similar to that of object classes, where a convolutional neural network is trained with discriminative classifiers using annotated image datasets~\cite{singh2016end}. However, labeled attribute image datasets often lack the data scale found in object datasets, contain a limited number of generic attributes, or cover only a few specific categories~\cite{lampert2009learning, Isola2015,patterson2016coco, yu2017semantic, Mancini2022}. %Most of these works are based on the same assumptions that are used for the task of attribute classification, with 
%the number of works focusing exclusively on state classification being limited~\cite{gouidis2022, DBLP:conf/visigrapp/GouidisPPAP24}. 
% \vspace*{0.2cm}\noindent\textbf{Open-vocabulary Attribute Detection} 
% \vspace*{0.2cm}\noindent\textbf{Zero-shot Learning} 
% Zero-Shot Learning (ZSL) is a task that involves recognizing new classes that were not seen during training, by leveraging side information such as attributes\cite{Lampert2014}, text descriptions\cite{reed2016learning} or word embeddings \cite{socher2013zero}. Previous approaches have attempted to learn a compatibility function between image and class embeddings\cite{akata2013label} or generate image features for novel classes \cite{xian2018feature}.
Few studies  address explicitly state classification~\cite{gouidis2022,DBLP:conf/visigrapp/GouidisPPAP24}, with most adopting assumptions from attribute classification. Zero-shot learning has emerged as a prominent approach, leveraging semantic embeddings for object representation~\cite{Wang2018b}, and recent works integrate Knowledge Graphs (KGs) or combine KGs with Large Language Models (LLMs)~\cite{gouidis2024llm,DBLP:conf/aaaiss/GouidisPPPAP24}. Other methods focus on compositional image generation~\cite{saini2023chop} or conditioned diffusion models for object state transformations~\cite{soucek2024genhowto}. In the context of videos object state changes provide meaningful context for video-based human action recognition (HAR), complementing visual action representations. Methods often detect object states explicitly~\cite{fathi2013modeling, soucek2022look} or indirectly via scene changes~\cite{alayrac2017joint}. Notable works include frameworks for discovering object states and manipulation actions~\cite{alayrac2017joint}, modeling object fluents in egocentric videos~\cite{Liu2017}, and analyzing multi-object interactions~\cite{Ma2018}. Recent methods leverage self-supervised learning for temporal localization~\cite{soucek2022look}, open-world object part segmentation~\cite{xue2024learning},  disentangling embeddings for object-state recognition~\cite{saini2022disentangling} and anticipation of object states changes~\cite{manousaki2024anticipating}.%, with models like InternVideo~\cite{chen2022ego4d} adapted for object state change and action anticipation.


\noindent\textbf{Zero-shot Object Classification}:
Zero-shot object classification has gained increasing attention due to its practical importance in real-world applications, where it is often difficult to obtain training data for all possible object classes~\cite{xian2018zero}. Several approaches were proposed to address this problem, including semantic embedding-based methods~\cite{Wang2018b, xian2018feature,fu2015zero}, attribute-based methods~\cite{Lampert2014},  generative models~\cite{xian2018feature,changpinyo2016synthesized} and learning of a compatibility function between image and class embeddings ~\cite{akata2015evaluation}. Semantic embedding-based methods employ compact semantic spaces or attribute sets to bridge seen and unseen object classes. Attribute-based methods leverage a set of attributes that describe object classes and use these attributes to infer the class of an unseen object. Generative models generate samples of unseen object classes by synthesizing images that are similar to images of seen object classes. In addition to these approaches, recent work has explored the use of knowledge graphs~\cite{Kampffmeyer2019,nayak:tmlr22}, which capture semantic relationships between objects and can be used to facilitate zero-shot learning. Prior methods in zero-shot learning utilized predetermined attributes or pretrained embeddings, in contrast to our approach which centers on acquiring class representations directly from the knowledge graph during the task.
In a similar vein, some recent works~\cite{DBLP:conf/aaaiss/GouidisPPPAP24,gouidis2024llm} have explored the role of Large Language Models (LLMs) in the context of zero-shot classification.
% Overall, these approaches have achieved promising results in zero-shot object classification and hold great potential for real-world applications such as text classification, video action recognition, and machine translation. 
% There has been significant research on zero-shot object classification using graph neural networks, with recent works exploring the use of common sense knowledge graphs to generate class representations. However, previous methods relied on predefined attributes or pretrained embeddings, while our approach focuses on explicitly learning class representations from the knowledge graph in the task. Other notable works in zero-shot learning include text classification, video action recognition, and machine translation.  

\noindent\textbf{Compositional Zero-shot Learning}: Compositional Zero-shot Learning (CZSL) aims to generalize to unseen combinations of object and state primitives by learning compositionality from the training set. Approaches are grouped into two types: one models individual classifiers for states and objects or learns hierarchical visual primitives~\cite{misra2017red, nagarajan2018attributes, yang2020learning, karthik2022open}, while the other learns a joint compatibility function between image, state, and object~\cite{purushwalkam2019task,atzmon2020causal}. For instance, \cite{atzmon2020causal} introduced a causal graph ensuring primitive independence, while \cite{Li2020} used a symmetry-based framework inspired by group theory. Graph CNNs were employed by~\cite{Mancini2022} to model dependencies and estimate composition feasibility. More recent works explore disentanglement and external knowledge integration, such as ConceptNet for predicting primitives~\cite{karthik2022open}, generative models for creating novel compositions~\cite{li2022siamese}, and attribute-object invariant domains~\cite{zhang2022learning}. Others focus on learning conditional attribute embeddings~\cite{wang2023learning} or disentangled embeddings via cross-attentions~\cite{hao2023learning}. A key limitation in existing CZSL methods is their reliance on training samples containing attribute-object labels. By contrast, our method models states object-agnostically, enabling generalization to unseen state classes.
% Compositional Zero-Shot Learning (CZSL) aims to generalize to unseen combinations of object and state primitives by learning their compositionality from the training set. Two groups of CZSL approaches have been proposed. The first group models individual classifiers of states and objects or learns a hierarchical decomposition and composition of visual primitives~\cite{misra2017red, nagarajan2018attributes, yang2020learning, karthik2022open}. The methods in the second group learn a joint compatibility function concerning the image, the state, and the object by conditioning modular networks on each composition~\cite{purushwalkam2019task,atzmon2020causal}. The work in~\cite{atzmon2020causal} recently proposed learning the visual transformation through a causal graph, where the latent representations of primitives are independent of each other, as a way to achieve generalization in CZSL. \cite{Li2020} presented a transformation framework consisting of two modules inspired by group theory that incorporates the principle of symmetry in attribute-object transformations. \cite{Mancini2022} utilized a graph CNN to model the dependency between states, objects and their compositions in order to estimate a feasibility score for each unseen composition. % to improve representations in open-world CZSL scenarios.  
% More recently, \cite{karthik2022open} 
% % calculate the feasibility relying on external semantic knowledge. Contrary to directly classifying compositions, simple primitive methods predict primitive concepts independently and then construct the compositional probability distribution for classification jointly. It  follows the Visual Product \cite{misra2017red} and
%  predicted state and object primitives independently with non-linear feature extractors
%  % To refine the relation between independent primitives, 
%  using ConceptNet~\cite{speer2017conceptnet} as  an external knowledge source. 
% The work in~\cite{li2022siamese} 
% investigated the disentanglement of states and objects on real-world settings by using contrastive loss and the utilization of a generative network which creates novel compositions.
% % that bridge the gap between seen and unseen compositions. 
% % loss and proposes to use compositions with shared primitives as positive samples and others as negative samples.  It learns concept-constant samples contrastively without constructing concept embed- ding prototypes to avoid learning irrelevant concepts shared by positive samples.
% % \cite{li2022siamese}  proposed the utilization of a generative network to create novel compositions to bridge the gap between seen and unseen compositions. 
% \cite{zhang2022learning} treated CZSL as a domain generalization task, learning attribute- and object-invariant domains which are mapped into ideal concept-invariant domains.  
% % \cite{zhang2022learning} disentangles visual features into ideal concept-invariant domains.  
% \cite{Saini_2022_CVPR} extracted visual similarity from spatial features to disentangle attributes and objects.  \cite{wang2023learning} propose the learning of conditional attribute embeddings via an attribute learning framework consisting of an attribute hyper learner and an attribute base learner. \cite{hao2023learning} learns disentangled concept embeddings by utilizing cross-attentions as compositional disentanglers. 
% A notable limitation of the existing CZSL methods is the assumption that the labels of each element pair, i.e. the attribute and the object and a subset of their possible combinations, are included in the training samples. In contrast, our method focuses on the {\sl object-agnostic} modeling of states. In other words, it represents a state class regardless of objects' classes that it can be related with and can generalize to new, previously unseen state classes that are not included in the training set.  

\noindent\textbf{Graph Neural Networks}:
Graph Neural Networks (GNNs) have gained popularity due to their ability to learn node embeddings that reflect the structure of the graph \cite{kipf2016semi}. These networks have shown significant improvements in downstream tasks, such as node classification and graph classification \cite{hamilton2017inductive,wu2019simplifying, shang2019end,vashishth2019composition}. In this work, we use the GNN transformers that have recently been used for zero-shot object classification~\cite{nayak:tmlr22}. Prior works have considered transformers as a method to learn meta-paths in heterogeneous graphs rather than as a neighborhood aggregation technique~\cite{yun2022graph,liu2023meta}.  Furthermore, GNNs have been applied to various problems including fine-grained entity typing~\cite{xiong2019imposing}, text classification~\cite{yao2019graph}, reinforcement learning~\cite{adhikari2020learning} and neural machine translation~\cite{bastings2017graph}.



% Graph Convolutional Networks (GCN) \cite{} are a special type of neural networks that leverage the interdependencies of data (nodes) that are defined in a graph. However, current methods \cite{}  have limitations in terms of network depth as over-smoothing at deeper layers of the network can cause all nodes to converge to the same value \cite{} . To address this limitation, several approaches have been proposed, such as dense skip connections \cite{} , randomly dropping edges [\cite{} , and applying a linear combination of neighbor features \cite{} . Recent works in this direction have combined residual connections with identity mapping \cite{} or utilized GCNs for zero-shot learning. 

% For instance, \cite{}  propose to directly predict the classifier weights of novel classes using a GCN applied on an external knowledge graph such as WordNet \cite{} . \cite{}  enhance this approach by introducing a dense graph to learn a shallow GCN as a solution for the Laplacian smoothing problem.

\vspace*{0.0cm}\noindent\textbf{Common Sense Knowledge Graphs}: 
Common sense KGs have been extensively utilized in various tasks including transductive zero-shot text classification \cite{zhang2019integrating} and object classification \cite{Kampffmeyer2019,xian2018zero}. Works such as~\cite{bhagavatula2019abductive} and~\cite{bosselut2019comet}  have explored the application of common sense KGs in diverse settings. The work in~\cite{zhang2019integrating}  used ConceptNet~\cite{speer2017conceptnet} for transductive zero-shot text classification as shallow features for class representation. 
Another work~\cite{zhang2019tgg} also utilized common sense knowledge graphs and GNNs for transductive zero-shot object classification. This approach learns to model seen-unseen relations with a graph neural network and requires knowledge of unseen classes during training, utilizing hand-crafted attributes.   Drawing inspiration from~\cite{nayak:tmlr22} which proposed a novel GNN architecture capable of generating dense vector representations from ConceptNet, we extend this approach in a novel context.

% \vspace*{0.0cm}\noindent\textbf{Large Pre-trained Models}: 
% While Large Language Models (LLMs) have achieved remarkable success in natural language processing tasks, their application to vision tasks has been limited. A recent development concerns the development of LLMs that are adapted to the visual domain. These models, which are typically called Large Pre-trained Models (LPMs), have been trained on massive amounts of text and visual data and exploit contrastive learning in order to understand relationships between these two modalities. This approach enables the utilization of LPMs in downstream tasks such as zero-shot classification, visual question answering and image captioning. Two prominent examples of LPMs are  \cite{radford2021learning} and \cite{jia2021scaling}. 

% We draw inspiration from  where  a novel GNN architecture  is proposed capable of generating dense vector representations from ConceptNet. We extend this approach further and use it a novel context. 

% In contrast, our method does not require explicit knowledge of unseen classes during training, but  learns instead class representations from the common sense knowledge graph. Overall, common sense knowledge graphs have shown great potential in improving the performance of various tasks, and our work extends this by applying them to a novel task.which