%\section{Supplementary Section}

% We present material that due to space limitations could not be included in the main paper. In Section~\ref{exp} we present and discuss the results for the CW and OW versions evaluation (section 4.2 main paper), whereas in Section~\ref{abl} we present and discuss the full results of the ablation study (section 4.3 of main paper). 


\section*{Supplementary Material}

\subsection{Datasets Details}
\autoref{tab:datasets} 
presents the following details for each dataset: i) the number of the training, validation and test samples; ii) the number of state and object classes; iii) the valid and  iv) the total object-state combinations and v) the average
  number of states in which an object can be situated.
\subsection{Evaluation of the CW and OW versions}

\label{exp}
The results for the Open World (OW) and Closed World (CW) versions of the models are shown in \autoref{tab:table_aggr_open} and  \autoref{tab:table_aggr_closed},   respectively.
For the OW settings our method continues to outperform the competing methods, although the performance gain has predictably been decreased. Moreover, w.r.t  OSDD dataset, the 2nd  best method is  IVR~\cite{zhang2022learning}, whereas CANET~\cite{wang2023learning} is the 3rd best method. In the case of the CGQA-States dataset, the 2nd and 3rd best method is  IVR~\cite{zhang2022learning}  and   CANET~\cite{wang2023learning}, respectively. Concerning the MIT-States dataset the 2nd  best method is the IVR\cite{zhang2022learning}, whereas KG-SP~\cite{karthik2022open} exhibits the 3rd best AUC score and   CANET~\cite{wang2023learning} the 3rd best HM score. Finally, in the case of the VAW dataset, the 2nd  best performance  is achieved by  CANET~\cite{wang2023learning}, while IVR~\cite{zhang2022learning} ranks 3rd.

Regarding the CW settings, our method ranks 1st for the OSDD, VAW and MIT-states datasets and 4th for the  CGQA-states dataset. Regarding the OSDD dataset,  IVR~\cite{zhang2022learning} exhibits the 2nd best performance and  KG-SP~\cite {karthik2022open} the 3rd best performance. In the case of MIT-States dataset,   CompCos~\cite{Mancini2022}  achieves the 2nd best performance and   ADE~\cite{hao2023learning}  the 3rd best performance. Concerning the CGQA-states dataset, the best performance is achieved by   CANET~\cite{wang2023learning}, the 2nd best by   CompCos~\cite{Mancini2022}  and the 3rd best by  OADiS\cite{Saini_2022_CVPR}. Finally, regarding VAW, the 2nd best method is ADE~\cite{hao2023learning} and the 3rd best method is CANET~\cite{wang2023learning}.



\subsection{Additional Results of the Ablation Study}\label{abl}
% \autoref{tab:KGS} presents the characteristics of the different KGs upon which the different ablated models that were tested during the ablation study    were based.
% \autoref{tab:KGS} provides details regarding the employed KGs. \autoref{tab:ablation} presents the details of the performance of all ablated models across the four datasets. 

% \noindent {\bf 1st sub-table, GNN architectures:} the Tr-GCN based model CN+WN\_H2\_TH\_GCN exhibits the best performance overall. 

% \noindent {\bf 2nd sub-table, KGs:} the ConceptNet-based model CN\_H2\_TH\_Tr-GCN  achieves the best scores.

% \noindent {\bf 3rd sub-table, hops:} for most of the models the best performance is obtained for a number of hops equal to 2. 

% \noindent {\bf 4th sub-table, node policy:} For most models the adoption of a node policy improves  slightly their performance. 

% It should be noted that although  CN\_H2\_TH\_Tr-GCN  achieves the best scores in two out of the three datasets, the model that was selected for the comparison with the competing methods was CN+WN\_H2\_TH\_GCN, since  the selection was based on the aggregate averages over the four categories.
  
% Additionally, in seen classes, the model with the unrelated embeddings (CN\_H3\_UN\_Tr-GCN) achieves an accuracy  similar to the model of the same characteristics and standard embeddings (CN\_H3\_Tr-GCN). However, CN\_H3\_UN\_Tr-GCN accuracy in unseen classes and its HM and AUC score is about three to four times inferior to CN\_H3\_Tr-GCN. In contrast, the random model performs poor in all four metrics.  The difference between the embeddings of the CN\_H3\_UN\_Tr-GCN and the random model concerns their distribution: in the former case the GNN allows a balanced and representative distribution, whereas in the latter case it is totally random. This indicates that the fine-tuning process can result in competing seen accuracy even if the embeddings utilized are unrelated to the target labels  as long as they are  distributed adequately. In contrast, the accuracy for the unseen classes depends on an exact mapping between the embeddings and the target states. 
\autoref{tab:KGS} outlines the details of the employed KGs, while \autoref{tab:ablation} summarizes the performance of all ablated models across the four datasets.

\noindent 1st Sub-table (GNN Architectures): The Tr-GCN-based model CN+WN\_H2\_TH\_GCN demonstrates the best overall performance.

\noindent 2nd Sub-table (KGs): The ConceptNet-based model CN\_H2\_TH\_Tr-GCN achieves the highest scores.

\noindent 3rd Sub-table (Hops): Most models achieve their best performance with two hops.

\noindent 4th Sub-table (Node Policy): Adopting a node policy slightly improves the performance of most models.

Notably, while CN\_H2\_TH\_Tr-GCN achieves the best scores on two of the three datasets, CN+WN\_H2\_TH\_GCN was selected for comparison with competing methods, as this selection was based on aggregate averages across all four categories.

In seen classes, the model using unrelated embeddings (CN\_H3\_UN\_Tr-GCN) achieves similar accuracy to its counterpart with standard embeddings (CN\_H3\_Tr-GCN). However, CN\_H3\_UN\_Tr-GCN performs significantly worse in unseen classes, with its HM and AUC scores being three to four times lower than those of CN\_H3\_Tr-GCN. In contrast, the random model performs poorly across all metrics.

The key distinction between CN\_H3\_UN\_Tr-GCN and the random model lies in their embedding distributions: in the former, the GNN enables a balanced and representative distribution, while in the latter, the distribution is entirely random. This suggests that fine-tuning can yield competitive accuracy for seen classes even when embeddings are unrelated to target labels, provided they are distributed effectively. However, for unseen classes, accuracy depends on a precise mapping between embeddings and target labels.




\begin{table*}[t]
    \centering
%\resizebox{\textwidth}{!}{ 
    \begin{tabular}{|l|rrr|rrrrr|}

\hline 
\textbf{Dataset} & \textbf{Train}  &  \textbf{Val}   &  \textbf{Test} & \textbf{States} & \textbf{Objects} & \textbf{VOSC} & \textbf{TOSC} & \textbf{S\textbackslash O} \\ \hline \hline

OSDD \cite{gouidis2022} &   6,977 & 1,124  & 5,275 & 9 & 14 & 35 &126 &2.36 \\ \hline 
CGQA-states \cite{Mancini2022} &   244 & 46 & 806 & 5 & 17  &41 & 75 & 1.71  \\ \hline
MIT-states \cite{Isola2015} &  170 & 34 & 274 & 5 & 14 & 20 &  70  & 1.57 \\\hline
VAW \cite{Pham2021CVPR} &  2,752 & 516 & 1,584 & 9 & 23 & 51 &  207  & 2.61 \\


   \hline
  \end{tabular}
%}   
   \caption{Details about the four image datasets utilized in this work. Train/Val/Test: Number of Training/Validation/Testing Images. States:  Number of State classes, Objects: Number of Object classes. VOSC/TOSC: Valid/Total Object-State combinations. S\textbackslash O: Average number of states than an Object can be situated in.}
    \label{tab:datasets}
\end{table*}

% Additionally, it can be observed that the model employing irrelevant embeddings (CN\_H3\_UN\_Tr-GCN) performs similarly to the model using standard embeddings (CN\_H3\_Tr-GCN) in terms of accuracy for seen classes. However, the accuracy for unseen classes, as well as HM and AUC scores, are three to four times lower in CN\_H3\_UN\_Tr-GCN compared to CN\_H3\_Tr-GCN. On the other hand, the random model exhibits poor performance in all four metrics. 
% The distinction between the embeddings of CN\_H3\_UN\_Tr-GCN and the random model lies in their distribution: the former model employs a balanced and representative distribution enabled by GNN, while the latter model has a completely random distribution. This suggests that the fine-tuning process can yield competitive seen accuracy even with unrelated embeddings to the target labels as long as the distribution is appropriate. In contrast, achieving accuracy for unseen classes requires an exact mapping between the embeddings and the target states. 

\begin{table*}[t]
	%\tiny
   
 \resizebox{0.9\textwidth}{!}{\begin{minipage}{\textwidth} 

     \begin{threeparttable}
      \centering
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
     
   \hline 

      
 % 


 
\textbf{\centering  Method}  &\multicolumn{4}{|c|}{\textbf{OSDD}}  & \multicolumn{4}{|c|}{\textbf{CGQA-States}}  & \multicolumn{4}{|c|}{\textbf{MIT-States}}  & \multicolumn{4}{|c|}{\textbf{VAW}} \\      \cline{2-17} 

 &\textbf{S} & \textbf{Un}   & \textbf{HM} & \textbf{AUC} & \textbf{S} & \textbf{Un}   & \textbf{HM} & \textbf{AUC}
& \textbf{S} &\textbf{Un}  & \textbf{HM} & \textbf{AUC}
& \textbf{S} &\textbf{Un}  & \textbf{HM} & \textbf{AUC}
 \\ \hline \hline
 

   



\hline  

AoP~\cite{nagarajan2018attributes}   &69.9 &  33.3  & 31.6 & 13.3
 & 14.5 & 4.3 & 4.4 & 0.3
& 36.4 &  4.8 &  8.4 &  1.3

& 59.6 &  5.4 &  6.1 &  1.3\\


LE+~\cite{misra2017red} & 71.6 &     14.3  &  20.8 & 6.5
     & 29.1 & 4.0 & 7.0 & 0.6
 & 45.5 & 14.9 &   15.1 & 4.3    
  & 23.7 & 12.3 &   13.7 & 0.4 \\  
 TMN~\cite{purushwalkam2019task} & 73.4 &  43.6  & 33.7 &  19.0
 & 45.5 & 29.7 & 19.3 & 6.1
& 69.7&  18.4 & 22.4 &  6.3   
& 77.6&  35.5 & 26.8 &  14.3 \\  

 
 SymNet~\cite{Li2020} &77.7 &  14.0 & 21.1 & 7.5 
     & 94.0 & 7.1 & 13.7 & 6.1
 & 97.0 & 1.9 &   2.1 & 0.9 
& 82.2 &  3.1 &  3.5 &  1.2\\
 



  CompCos~\cite{Mancini2022}  &  78.7 &  31.5  & 42.0 & 22.1
   & 95.5 & 4.0 & 7.7 & 3.4  
   &   75.8 & 2.5 &   4.9&  1.2 
   &   75.8 & 2.5 &   4.9&  1.2 \\
  KG-SP\cite{karthik2022open}  & 77.0 &  29.8 & 35.4 &  17.9
 % & 95.5 & 2.6 & 1.1 & 0.5
  & 94.0 & 16.9 & 26.1 & 12.7
% & 97.0 &  1.2 &  1.9 &  0.4 \\ 
& 97.0 &  15.5 & 22.6 & 12.0
& 74.3 &  12.3 & 17.6 & 8.6
 \\ 

  SCEN-NET~\cite{li2022siamese}  & 75.8 &  25.5  & 26.3 &  10.7
 & 83.6 & 7.4 & 13.6 & 5.9
& 36.4 &  8.5 &  13.0 &  1.6  
& 22.0 &  12.0 &  11.1 &  2.5 \\ 

  IVR~\cite{zhang2022learning}  & 78.8 &  61.6  & \bf 44.2 &  \bf 30.8
 & 94.0 & 40.3 & \bf {37.4} & \bf {26.4}
& 96.9 &  22.5 &  \textbf{24.5} & \bf 14.9 
& 87.2 &  37.4 &  \underline{29.7} &  \underline{18.2} \\ 

  OADiS~\cite{Saini_2022_CVPR}  & 76.5 &  20.5  & 27.1 &  10.7
   % & 96.1 & 11.7 & 11.1 & 5.6
 & 94.8 & 26.3 & 20.3 & 12.0
& 93.9 &  29.1 &  23.4 &  \underline{12.5} 
& 82.8 &  8.9 &  11.0 &  4.2 \\ 

% & 100.0 &  28.5 &  23.0 &  11.5 \\ 
  CANET~\cite{wang2023learning}  & 79.2 &  43.9  & \underline{43.7} &  \underline{27.2}
 & 95.5 & 51.3 &  \underline{41.9} &  \underline{26.1}
&  96.9 &  19.3 &  \underline{22.7} &  11.4 

&  90.1 &  53.9 &   \bf  40.4 &  \bf 29.7 \\ 
  ADE~\cite{hao2023learning}  & 80.2 &  27.6 & 32.3 &  12.3
 & 95.5  & 16.3 & 25.7 & 12.8
& 78.8 &  4.5 &  4.7 &  0.8 
  
& 80.8&  22.3 &  14.3&  8.4 \\ 
  
    \hline  \hline 
 

\centering  OaSC (Ours)
  & 87.7 & 69.9 & \bf  \textcolor{red}{48.6} & \bf \textcolor{red} {39.8} & 
  97.1 & 73.4 & \bf  \textcolor{red}{43.6} & \bf  \textcolor{red}{36.5} & 
  85.7 & 69.9 & \bf  \textcolor{red}{51.1} & \bf  \textcolor{red}{41.2} 
&83.7 & 58.6   & \bf  \textcolor{red}{42.9} & \bf  \textcolor{red}{32.8}  \\  \hline 


 



  
  
  \end{tabular}
  

   \caption{   \label{tab:table_aggr_open}Aggregate results for Open World Versions. S: Best Accuracy on seen classes. UN: Best accuracy on unseen classes. HM: Best harmonic mean. AUC: Area under curve for the  pairs of accuracy for seen and unseen classes. Red/Bold/Underlined text indicates best/2nd best/3rd best performance.
   }
        \end{threeparttable}
       \end{minipage} }
   \end{table*}


\begin{table*}[h!]
	
   
  \resizebox{0.9\textwidth}{!}{\begin{minipage}{\textwidth} 
     \begin{threeparttable}
      \centering
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
    
   \hline  

      
 % 


 
\textbf{\centering  Method}  &\multicolumn{4}{|c|}{\textbf{OSDD}}  & \multicolumn{4}{|c|}{\textbf{CGQA-States}}  & \multicolumn{4}{|c|}{\textbf{MIT-States}}  & \multicolumn{4}{|c|}{\textbf{VAW}} \\      \cline{2-17} 

 &\textbf{S} & \textbf{UN}   & \textbf{HM} & \textbf{AUC} & \textbf{S} & \textbf{UN}   & \textbf{HM} & \textbf{AUC}
& \textbf{S} &\textbf{UN}  & \textbf{HM} & \textbf{AUC}
& \textbf{S} &\textbf{UN}  & \textbf{HM} & \textbf{AUC}
 \\ \hline \hline





AoP~\cite{nagarajan2018attributes}   &75.9 &  53.5  & 32.2& 19.5
 & 95.5 & 50.0 & 35.9 & 27.8
& 48.5 &  20.9 &  15.1 &  4.1
& 55.1 &  44.7 &  24.1 &  11.6\\



LE+~\cite{misra2017red} & 68.6 &     31.7  &  34.5 & 16.9
     & 93.5 & 16.1 & 16.1 & 8.1
 & 63.6 & 14.6 &   20.3 & 7.1   
  & 41.6 & 2.3 &   2.6 & 1.2  \\ 
 TMN\cite{purushwalkam2019task} & 71.5 &  49.8  & 35.0 &  20.8
 & 97.0 & 76.0 & 39.9 & 32.2
& 84.9 &  30.7 & 27.4 &  16.1    
& 82.6 &  55.5 & 37.3 &  25.6 \\  

 
 SymNet~\cite{Li2020} &77.7 &  59.4 & 44.2 & \underline {31.0} 
     & 95.5 & 27.4 & 39.4 & 24.4
 & 96.9 & 27.5 &   26.8 & 15.7  
  
 & 89.2 & 46.6 &   40.0 & 27.4 \\ 



  Compcos~\cite{Mancini2022}  &  76.3 &  45.3  & 38.7 & 23.8
    & 92.5 & 73.9 & \bf 48.1 & \bf 41.5  

   &   100.0 & 44.9 &  \textbf{32.3} &   \textbf{23.8} 
   &   88.4 & 51.4 &  39.3 &   29.1 \\
  KG-SP~\cite{karthik2022open}  & 78.0 &  55.0  & \bf  {47.6} & 29.7
 & 95.5 & 17.7 & 27.2 & 13.5
& 97.1 &  15.5 &  22.6 &  12.0  
& 89.4 &  37.3& 39.3 & 23.4\\ 
  SCEN-NET\cite{li2022siamese}  & 75.1 &  45.6  & 39.4 &  22.7
 & 94.1 & 53.4 & 41.1 & 31.0
& 84.9 &  23.1 &  22.1 &  11.5 
& 90.5&  44.2 &  37.7&  23.5 \\ 

  IVR~\cite{zhang2022learning}  & 78.4 &  60.5  & \underline{46.0} &  \bf {31.8} 
 & 94.0 & 43.4 & 35.2 & 25.2
& 87.9 &  28.8 &  27.1 &  14.0  
& 86.7 &  38.2 &  30.5 &  18.5 \\ 

%   OADiS\cite{Saini_2022_CVPR}  & 78.7 &  59.7  & 38.3 &  26.2
%  & 96.1 & 11.7 & 11.1 & 5.6
% & 100.0 &  28.5 &  23.0 &  11.5 \\ 

  OADiS~\cite{Saini_2022_CVPR}  & 78.7 &  59.7  & 38.3 &  26.2
 & 95.5 & 78.6 & 43.5 & \underline{36.7}
& 93.9 &  29.4 &  28.3 &  17.2 
& 89.9 & 61.8 &  39.8&  \underline{30.5}\\ 

  CANET~\cite{wang2023learning}  & 80.3 &  43.6  & 45.1 &  27.9
 & 95.5 & 64.9 & \bf  \textcolor{red}{50.0} & \bf  \textcolor{red}{43.3}
&  96.9 &  23.0 &  28.2 &  15.9  
&  90.3 &  54.6 &  \underline{40.8} & \underline{30.5} \\ 
  ADE\cite{hao2023learning}  & 82.0 &  42.5  & 35.9  &  20.6
 & 94.8  & 58.3 & \underline{45.5} & 34.9
& 93.9 &  27.5 &  \underline{30.4} & \underline{19.2} 
& 90.7&  45.0 & \bf 40.9& \bf 30.6 \\ 

  
    \hline  \hline 
 
\centering  OaSC (Ours)
  & 87.7 & 69.9 & \bf  \textcolor{red}{48.6} & \bf \textcolor{red} {39.8} & 
  97.1 & 73.4 &   43.6 &  36.5 & 
  85.7 & 69.9 & \bf  \textcolor{red}{51.1} & \bf  \textcolor{red}{41.2} 
&83.7 & 58.6   & \bf  \textcolor{red}{42.9} & \bf  \textcolor{red}{32.8}
\\  \hline 

 



  
  
  \end{tabular}
  

   \caption{   \label{tab:table_aggr_closed}Aggregate results for Closed World Versions. S: Best Accuracy on seen classes. UN: Best accuracy on unseen classes. HM: Best harmonic mean. AUC: Area under curve for the  pairs of accuracy for seen and unseen classes. 
 Red/Bold/Underlined text indicates best/2nd best/3rd best performance.
   }
        \end{threeparttable}

        \end{minipage}}
   \end{table*}


\begin{table*}[h!]
	%\small
    
      \resizebox{0.95\textwidth}{!}{\begin{minipage}{\textwidth} 
      \centering
    \begin{tabular}{|l|c|c|r|l|}

\hline
\textbf{KG} &   \textbf{N}  &  \textbf{E}   &  \textbf{RT} & \textbf{RC}\\ \hline\hline



% WN\_H2 & 70 / 54 / 49 &  321 / 223 / 105 &       5 &LX\\ \hline
% WN\_H3 & 429 / 311 / 295 &  873 / 680 / 655 &       5 &LX\\ \hline\hline
WN\_H2 & 70 / 54 / 49  / 79&  321 / 223 / 105 / 365 &       5 &LX\\ \hline
WN\_H3 & 429 / 311 / 295 / 465&  873 / 680 / 655 / 912 &       5 &LX\\ \hline\hline

CN\_H2 &   715 / 552 / 504 / 743 / & 2,132 / 1,981 / 1,864  / 2,342 &    13 &CS    \\ \hline
CN\_H3 & 2,139 / 1,872 / 1,788 /2,349 /  &  2,542 / 2,194 / 2,103 / 2,874 &       24 &CS    \\ \hline 
CN\_H2\_TH & 611 / 505 / 485  / 785 & 1,710 / 1,521 / 1,415 / 1,956 &       12 &CS  \\ \hline
CN\_H3\_TH & 12,733 / 9,839 / 9,212  / 13,045&  29,794 / 25,105 / 24,292 / 32,456 &       29 & CS \\ \hline \hline
  
  
CN+WN\_H2 & 667 / 581 / 506 / 845 & 1,906 / 1,682 / 1,602 / 2,136 &       13 &CS \\ \hline
CN+WN\_H2\_TH & 590 / 492 / 431 / 705 & 1,442 / 1,167 / 1,089 / 1,673&       12 &CS/LX    \\ \hline
CN+WN\_H3\_TH &  10,165 / 8,842 / 7,948 / 12,116 &  26,735 / 23,176 / 22,602 / 29,672  &       29 & CS/LX   \\ \hline 
  \end{tabular}
  

   
     % \end{minipage}}
  
   \caption{KGs Details. N: Number of Nodes. E: Number of Edges.  RT: Number of Different Relation Types between nodes. RC: Category of Relation Types. CS: Common-Sense. LX: Lexicographic. First/Second/Third/Fourth number in the N and E columns refers to the KG for OSDD/CGQA-States/MIT-States/ VAW dataset, respectively.}

    \label{tab:KGS}
     \end{minipage}}  
\end{table*}



\begin{table*}[h!] 
    \centering
     % \resizebox{0.90\textwidth}{!}{\begin{minipage}{\textwidth} 
\resizebox{\textwidth}{!}{     
    \begin{tabular}{|l|cccc|cccc|cccc|cccc|}

\hline
 \bf \multirow{2}{*}{Method}& \multicolumn{4}{|c|}{\textbf{OSDD}}  & \multicolumn{4}{|c|}{\textbf{CGQA-States}}  & \multicolumn{4}{|c|}{\textbf{MIT-States}}   & \multicolumn{4}{|c|}{\textbf{VAW}}  \\      \cline{2-17} 

\cline{2-13} 

& \textbf{S} & \textbf{Un}   & \textbf{HM} & \textbf{AUC} & \textbf{S} & \textbf{UN}   & \textbf{HM} & \textbf{AUC}
& \textbf{S} &\textbf{UN}  & \textbf{HM} & \textbf{AUC}
& \textbf{S} &\textbf{UN}  & \textbf{HM} & \textbf{AUC}
 \\ \hline \hline

 %\parbox[t]{2mm}{\multirow{12}{*}{\rotatebox[origin=c]{90}{GN}}}
 CN\_H3\_LSTM & 85.1 & 38.0 & 38.0 & 24.3 & 96.4 & 57.1 & 37.3 & 27.0 & 92.9 & 65.4 & 50.9 & 36.9 
&55.7 
&43.9 
&22.1 
&12.5 
 \\ 
CN\_H3\_GCN & 86.7 & 58.5 & \color{blue}\textbf{44.1} & \color{blue}\textbf{34.0}   & 95.7 & 62.5 & 40.0 & 28.7 & 88.1 & 66.7 & 47.1 & 32.2 

&70.3 
&49.5 
&30.2 
&20.8 

\\ 
CN\_H3\_R-GCN & 87.7 & 49.0 & 42.7 & 30.4 & 95.7 & 71.4 &\color{blue}\textbf{40.9}  & \color{blue} \textbf{34.0} & 78.6 & 73.4 &  47.4 & 32.9 
&79.5 
&57.5 
&38.9 
&28.8 
\\ 
 CN\_H3\_Tr-GCN & 87.4 & 42.2 & 40.2 & 27.7 & 93.6 & 56.3 & 39.2 & 28.8 & 88.1 & 67.0 &  \textbf{53.6} & \textbf{43.7} 
 &80.2 
&56.8 
&\color{blue} \textbf{40.7} 
& \color{blue} \textbf{29.9} 
 \\ 

\hline
WN\_H3\_LSTM & 86.0 & 60.0 & \color{blue} \textbf{43.3} & \color{blue} \textbf{33.9} & 96.4 & 13.4 & 16.6 & 8.7 & 90.5 & 24.4 & 24.2 & 13.2 
&37.4 &55.6 & 18.1 & 10.2 

\\ 
WN\_H3\_GCN & 86.8 & 39.5 & 36.7 & 21.2 & 86.4 & 49.0 & 34.2 & 24.1 & 88.1 & 54.8 & \color{blue} \textbf{50.1} & \color{blue} \textbf{37.9} 
&64.2 &38.3 & 24.4 &19.4

\\ 
WN\_H3\_R-GCN & 85.5 & 36.0 & 36.5 & 22.1 & 93.6 & 52.9 & \color{blue} \textbf{40.5} &  \color{blue}\textbf{28.9} & 78.6 & 47.4 & 42.9 & 21.4 

&69.7 &56.0 & 38.9 &28.8

\\ 
WN\_H3\_Tr-GCN & 89.2 & 48.4 & 36.6 & 23.9 & 86.4 & 56.6 & 37.6 & 26.6 & 88.1 & 44.2 & 37.3 & 25.9
& 65.0 & 54.5 & \color{blue} \textbf{31.8} & \color{blue} \textbf{21.3}
\\ 

\hline

CN\_H2\_TH\_LSTM & 86.5 & 50.0 & 43.0 & 28.8 & 97.1 & 71.7 & 38.8 & 31.9 & 78.6 & 60.3 & 47.8 & 26.0 

& 61.0 & 52.6 & 27.9 & 17.9 

\\ 
%CN+WN\_H2\_TH\_GCN
CN\_H2\_TH\_GCN & 84.6 & 52.8 & 43.7 & 30.7 & 95.7 & 67.5 & 40.5 & 32.0 & 85.7 & 73.1 & 46.6 & 29.4 

 & 74.3 & 48.3 & 36.4 & 27.4

\\ 
 %CN+WN\_H2\_TH\_R-GCN
CN\_H2\_TH\_R-GCN & 85.9 & 48.0 & 41.2 & 28.5 & 95.0 & 63.6 & 41.6 & 31.6 & 81.0 & 69.2 & 51.8 & 30.0

&82.4 
&57.6 
& 40.5
& 31.5 
\\ 


CN\_H2\_TH\_Tr-GCN & 85.7 & 63.7 & \textbf{45.6} & \textbf{34.5} & 97.1 & 70.0 &\textbf{43.5} & \textbf{35.6} & 85.7 & 70.2 & \color{blue} \textbf{51.6} & \color{blue} \textbf{40.5}   
% &82.4 &59.4 &38.0 & 32.6
&82.4&59.4 &\textbf{38.0}  & \textbf{32.6}  \\
\hline
\hline\hline

WN\_H2\_Tr-GCN & 87.9 & 23.0 & 28.6 & 13.0 & 92.9 & 53.8 & \color{blue}\textbf{38.2} & \color{blue}\textbf{28.1} & 83.3 & 45.8 & \color{blue} \textbf{39.7} & \color{blue}\textbf{27.3}  

& 69.7 & 45.8 & 30.5 & 18.3 \\
WN\_H3\_Tr-GCN & 89.2 & 48.4 & \color{blue}\textbf{36.6} & \color{blue}\textbf{23.9} & 86.4 & 56.6 & 37.6 & 26.6 & 88.1 & 44.2 & 37.3 & 25.9 
& 65.0 & 54.5 & \color{blue}  \textbf{31.8}
& \color{blue}  \textbf{21.3}

\\ \hline

CN\_H2\_Tr-GCN & 86.4 & 60.6 & \color{blue}\textbf{45.1} & \color{blue} \textbf{34.3} & 97.1 & 73.4 &  \textbf{46.3} & \textbf{39.5} & 88.1 & 69.6 & \textbf{56.2} & \textbf{43.5} 
&82.4 
&58.9 
&\color{blue}\textbf{37.3}
&\color{blue}\textbf{32.0}
\\ 
CN\_H3\_Tr-GCN & 87.4 & 42.2 & 40.2 & 27.7 & 93.6 & 56.3 & 39.2 & 28.8 & 88.1 & 67.0 & 53.6 & 43.7 
&81.1 
&48.3 
&36.9
&26.3
 
\\ \hline

CN\_H3\_UN\_Tr-GCN & 85.7 & 14.8 &\color{blue} \textbf{17.0} & \color{blue}\textbf{7.6} & 93.6 & 13.2 & \color{blue} \textbf{15.1} &\color{blue} \textbf{7.4} & 83.3 & 26.6 & \color{blue}\textbf{20.6} & \color{blue} \textbf{7.6} 
&83.1 
&10.2 
&\color{blue} \textbf{14.8} 
&\color{blue} \textbf{5.3}

\\  
RN\_Tr-GCN & 12.9 & 11.3 & 3.2 & 1.6 & 15.7 & 9.7 & 5.1 & 2.5 & 26.7 & 24.2 & 12.5 & 4.6   
& 12.0 & 9.8 & 3.0 & 1.3 \\
\hline
CN+WN\_H2\_Tr-GCN & 85.7 & 60.9 & 45.2 & 33.9 & 97.1 & 72.0 & \color{blue}\textbf{46.0} & \color{blue} \textbf{38.9} & 88.1 & 68.9 & 55.3 & 43.3 
&82.0 
&58.9 
&39.8 
&32.6 



\\ 
CN+WN\_H2\_TH\_Tr-GCN  

& 87.7 & 69.9 & \bf  \textcolor{black}{48.6} & \bf \textcolor{black} {39.8} & 
  97.1 & 73.4 &  43.6 & {36.5} & 
  85.7 & 69.9 & \bf  \textcolor{blue}{51.1} & \bf  \textcolor{blue}{41.2} 
  &83.7
&58.6 
& \textbf{42.9} 
& \textbf{32.8}   
  
  
  \\
\hline\hline
\hline
 WN\_H2\_Tr-GCN  & 87.9 & 23.0 & 28.6 & 13.0 & 92.9 & 53.8 & \color{blue}\textbf{38.2} & \color{blue} \textbf{28.1} & 83.3 & 45.8 & \color{blue} \textbf{39.7} & \color{blue}\textbf{27.3}  
& 69.7 & 45.8 & 30.5 & 18.3 \\
  WN\_H3\_Tr-GCN & 89.2 & 48.4 & \color{blue} \textbf{36.6} &\color{blue}  \textbf{23.9} & 86.4 & 56.6 & 37.6 & 26.6 & 88.1 & 44.2 & 37.3 & 25.9 
& 65.0 & 54.5 & \color{blue}  \textbf{31.8} 
& \color{blue}  \textbf{21.3}\\
  
  \hline
CN\_H2\_Tr-GCN & 86.4 & 60.6 & 45.1 & 34.3 & 97.1 & 73.4 &  \textbf{46.3} &  \textbf{39.5} & 88.1 & 69.6 & \textbf{56.2} & \textbf{43.5} 
&82.4 
&58.9 
&\color{blue}\textbf{37.3}
&\color{blue}\textbf{32.0}

\\ 
CN\_H3\_Tr-GCN & 87.4 & 42.2 & \color{blue} \textbf{40.2} &  \color{blue}\textbf{27.7} & 93.6 & 56.3 & 39.2 & 28.8 & 88.1 & 67.0 & 
53.6 & 43.7 
&80.2 
&56.8 
&40.7
&29.9


\\  \hline
CN+WN\_H2\_TH\_Tr-GCN &  87.7 & 69.9 & \bf  \textcolor{black}{48.6} & \bf \textcolor{black} {39.8} & 
  97.1 & 73.4 &  \textcolor{blue}{43.6} & \textcolor{blue}{36.5} & 
  85.7 & 69.9 &  {51.1} &  {41.2} 
  
  &83.7
&58.6 
&\color{black}\textbf{42.9 }
&\color{black}\textbf{32.8 } 
  \\



CN+WN\_H3\_TH\_Tr-GCN & 87.1 & 56.3 & 44.6 & 31.9 & 97.1 & 60.5 & 41.0 & 32.5 & 83.3 & 68.6 & \color{blue} \textbf{55.9} & \color{blue} \textbf{41.0}   
&80.6 
&59.2 
&38.8 
&30.6  \\
\hline
  \hline\hline

WN\_H3\_Tr-GCN & 87.3 & 46.4 &  \color{blue} \textbf{35.7} & 23.0 & 85.5 & 53.6 & 35.3 & 25.2 & 87.2 & 44.3 &  \color{blue} \textbf{37.4} & 25.7 
& 65.0 & 54.5 & 31.8 & 21.3\\ 
WN\_H3\_TH\_Tr-GCN & 89.2 & 48.4 & 36.6 &  \color{blue} \textbf{23.9} & 86.4 & 56.6 &  \color{blue} \textbf{37.6} &  \color{blue} \textbf{26.6} & 88.1 & 44.2 & 37.3 &  \color{blue} \textbf{25.9} 
& 68.1 & 56.0 &  \color{blue} \textbf{32.7} &  \color{blue} \textbf{23.4}\\   \hline
CN\_H2\_Tr-GCN & 86.4 & 60.6 & 45.1 & 34.3 & 97.1 & 73.4 & \textbf{46.3} & \textbf{39.5} & 88.1 & 69.6 & \textbf{56.2} & \textbf{43.5} 
&82.4 
&58.9 
&37.3
&32.0
\\   
CN\_H2\_TH\_Tr-GCN & 85.7 & 63.7 & \textbf{45.6} & \textbf{34.5} & 97.1 & 70.0 & 43.5 & 35.6 & 85.7 & 70.2 & 51.6 & 40.5 
&82.4&59.4 &\textbf{38.0}  & \textbf{32.6}  
 \\ 


\hline 

  \end{tabular}
%  

}  

\caption{Ablation Study. 
1st  section of the table: comparison for the GNN architecture. 
2nd section: comparison for the KG source. 
3rd section: comparison for max number of hops. 
4th section: comparison for the node inclusion policy. 
Bold font indicates top performance across ablation category. Blue colour indicates top performance across ablation subcategory.
S: Best Accuracy on seen classes. UN: Best accuracy on unseen classes. HM: Best harmonic mean. AUC: Area under curve for the  pairs of accuracy for seen and unseen classes.  CN: ConceptNet-based model. WN: WordNet-based model. UN: Embeddings corresponding to concepts unrelated to the target classes. RN: Random embeddings. H2(3): Maximum number of hops equal to 2(3). TH: Thresholding policy for the nodes of the KG.}
     % \end{minipage}}
   \label{tab:ablation}
   
\end{table*}












