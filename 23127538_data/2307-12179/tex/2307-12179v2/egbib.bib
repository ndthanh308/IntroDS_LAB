
@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International Conference on Machine Learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}
@inproceedings{
  csp2023,
  title={Learning to Compose Soft Prompts for Compositional Zero-Shot Learning},
  author={Nihal V. Nayak and Peilin Yu and Stephen H. Bach},
  booktitle={International Conference on Learning Representations},
  year={2023}
}
@article{wortsman2021robust,
  title={Robust fine-tuning of zero-shot models},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Kim, Jong Wook and Li, Mike and Kornblith, Simon and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Hajishirzi, Hannaneh and Farhadi, Ali and Namkoong, Hongseok and Schmidt, Ludwig},
  journal={arXiv preprint arXiv:2109.01903},
  note={\url{https://arxiv.org/abs/2109.01903}},
  year={2021}
}
@InProceedings{Saini_2022_CVPR,
    author    = {Saini, Nirat and Pham, Khoi and Shrivastava, Abhinav},
    title     = {Disentangling Visual Embeddings for Attributes and Objects},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {13658-13667}
}
@inproceedings{wang2016actions,
  title={Actions\~{} transformations},
  author={Wang, Xiaolong and Farhadi, Ali and Gupta, Abhinav},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages={2658--2667},
  year={2016}
}
@InProceedings{Pham2021CVPR,
    author    = {Pham, Khoi and Kafle, Kushal and Lin, Zhe and Ding, Zhihong and Cohen, Scott and Tran, Quan and Shrivastava, Abhinav},
    title     = {Learning To Predict Visual Attributes in the Wild},
    booktitle = {Proceedings of the IEEE/CVF CVPR},
    month     = {June},
    year      = {2021},
    pages     = {13018-13028}
},
@inproceedings{jia2021scaling,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={International conference on machine learning},
  pages={4904--4916},
  year={2021},
  organization={PMLR}
}
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{speer2017conceptnet,
  title={Conceptnet 5.5: An open multilingual graph of general knowledge},
  author={Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}

@article{zhang2019integrating,
  title={Integrating semantic knowledge to tackle zero-shot text classification},
  author={Zhang, Jingqing and Lertvittayakumjorn, Piyawat and Guo, Yike},
  journal={arXiv preprint arXiv:1903.12626},
  year={2019}
}

@inproceedings{lee2018multi,
  title={Multi-label zero-shot learning with structured knowledge graphs},
  author={Lee, Chung-Wei and Fang, Wei and Yeh, Chih-Kuan and Wang, Yu-Chiang Frank},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1576--1585},
  year={2018}
}


@inproceedings{duan2012discovering,
  title={Discovering localized attributes for fine-grained recognition},
  author={Duan, Kun and Parikh, Devi and Crandall, David and Grauman, Kristen},
  booktitle={2012 IEEE conference on computer vision and pattern recognition},
  pages={3474--3481},
  year={2012},
  organization={IEEE}
}
@inproceedings{yu2017semantic,
  title={Semantic jitter: Dense supervision for visual comparisons via synthetic images},
  author={Yu, Aron and Grauman, Kristen},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={5570--5579},
  year={2017}
}

@article{xian2018zero,
  title={Zero-shot learningâ€”a comprehensive evaluation of the good, the bad and the ugly},
  author={Xian, Yongqin and Lampert, Christoph H and Schiele, Bernt and Akata, Zeynep},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={9},
  pages={2251--2265},
  year={2018},
  publisher={IEEE}
}


@inproceedings{lampert2009learning,
  title={Learning to detect unseen object classes by between-class attribute transfer},
  author={Lampert, Christoph H and Nickisch, Hannes and Harmeling, Stefan},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={951--958},
  year={2009},
  organization={IEEE}
}
@inproceedings{patterson2016coco,
  title={Coco attributes: Attributes for people, animals, and objects},
  author={Patterson, Genevieve and Hays, James},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14},
  pages={85--100},
  year={2016},
  organization={Springer}
}

@inproceedings{singh2016end,
  title={End-to-end localization and ranking for relative attributes},
  author={Singh, Krishna Kumar and Lee, Yong Jae},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14},
  pages={753--769},
  year={2016},
  organization={Springer}
}

@book{Radenovic2021,
abstract = {We study the problem of learning how to predict attribute-object compositions from images, and its generalization to unseen compositions missing from the training data. To the best of our knowledge, this is a first large-scale study of this problem, involving hundreds of thousands of compositions. We train our framework with images from Instagram using hashtags as noisy weak supervision. We make careful design choices for data collection and modeling, in order to handle noisy annotations and unseen compositions. Finally, extensive evaluations show that learning to compose classifiers outperforms late fusion of individual attribute and object predictions, especially in the case of unseen attribute-object pairs.},
archivePrefix = {arXiv},
arxivId = {2105.11373},
author = {Radenovic, Filip and Sinha, Animesh and Gordo, Albert and Berg, Tamara and Mahajan, Dhruv},
booktitle = {Proceedings of ACM Conference (Conference'17)},
eprint = {2105.11373},
file = {:home/philo/Documents/Mendeley Desktop/Radenovic et al. - 2021 - Large-Scale Attribute-Object Compositions.pdf:pdf},
keywords = {54,attribute-object compositions,attribute-object compositions, classification, dat,by our approach in,classification,datasets,example attribute-object compositions predicted,figure 1,the yfcc100m dataset},
number = {1},
publisher = {Association for Computing Machinery},
title = {{Large-Scale Attribute-Object Compositions}},
url = {http://arxiv.org/abs/2105.11373},
volume = {1},
year = {2021}
}

@inproceedings{reed2016learning,
  title={Learning deep representations of fine-grained visual descriptions},
  author={Reed, Scott and Akata, Zeynep and Lee, Honglak and Schiele, Bernt},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={49--58},
  year={2016}
}
@article{socher2013zero,
  title={Zero-shot learning through cross-modal transfer},
  author={Socher, Richard and Ganjoo, Milind and Manning, Christopher D and Ng, Andrew},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@inproceedings{xian2018feature,
  title={Feature generating networks for zero-shot learning},
  author={Xian, Yongqin and Lorenz, Tobias and Schiele, Bernt and Akata, Zeynep},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5542--5551},
  year={2018}
}

@inproceedings{akata2013label,
  title={Label-embedding for attribute-based classification},
  author={Akata, Zeynep and Perronnin, Florent and Harchaoui, Zaid and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={819--826},
  year={2013}
}


@article{Lampert2014,
abstract = {We study the problem of object recognition for categories for which we have no training examples, a task also called zero-data or zero-shot learning. This situation has hardly been studied in computer vision research, even though it occurs frequently; the world contains tens of thousands of different object classes, and image collections have been formed and suitably annotated for only a few of them. To tackle the problem, we introduce attribute-based classification: Objects are identified based on a high-level description that is phrased in terms of semantic attributes, such as the object's color or shape. Because the identification of each such property transcends the specific learning task at hand, the attribute classifiers can be prelearned independently, for example, from existing image data sets unrelated to the current task. Afterward, new classes can be detected based on their attribute representation, without the need for a new training phase. In this paper, we also introduce a new data set, Animals with Attributes, of over 30,000 images of 50 animal classes, annotated with 85 semantic attributes. Extensive experiments on this and two more data sets show that attribute-based classification indeed is able to categorize images without access to any training images of the target classes. {\textcopyright} 2014 IEEE.},
author = {Lampert, Christoph H. and Nickisch, Hannes and Harmeling, Stefan},
doi = {10.1109/TPAMI.2013.140},
file = {:home/philo/Documents/Mendeley Desktop/lampert13attributes.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object recognition,vision and scene understanding},
number = {3},
pages = {453--465},
pmid = {24457503},
publisher = {IEEE},
title = {{Attribute-based classification for zero-shot visual object categorizationa}},
volume = {36},
year = {2014}
}


@article{Purushwalkam,
author = {Purushwalkam, Senthil and Nickel, Maximilian and Gupta, Abhinav and Ranzato, Marc Aurelio},
file = {:home/philo/Documents/Mendeley Desktop/Purushwalkam et al. - Unknown - Task-Driven Modular Networks for Zero-Shot Compositional Learning.pdf:pdf},
pages = {3593--3602},
title = {{Task-Driven Modular Networks for Zero-Shot Compositional Learning}}
}


@inproceedings{schlichtkrull2018modeling,
  title={Modeling relational data with graph convolutional networks},
  author={Schlichtkrull, Michael and Kipf, Thomas N and Bloem, Peter and Van Den Berg, Rianne and Titov, Ivan and Welling, Max},
  booktitle={The Semantic Web: 15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3--7, 2018, Proceedings 15},
  pages={593--607},
  year={2018},
  organization={Springer}
}

@inproceedings{shang2019end,
  title={End-to-end structure-aware convolutional networks for knowledge base completion},
  author={Shang, Chao and Tang, Yun and Huang, Jing and Bi, Jinbo and He, Xiaodong and Zhou, Bowen},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={3060--3067},
  year={2019}
}
@article{xiong2019imposing,
  title={Imposing label-relational inductive bias for extremely fine-grained entity typing},
  author={Xiong, Wenhan and Wu, Jiawei and Lei, Deren and Yu, Mo and Chang, Shiyu and Guo, Xiaoxiao and Wang, William Yang},
  journal={arXiv preprint arXiv:1903.02591},
  year={2019}
}
@inproceedings{yao2019graph,
  title={Graph convolutional networks for text classification},
  author={Yao, Liang and Mao, Chengsheng and Luo, Yuan},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={7370--7377},
  year={2019}
}
article{zhang2019integrating,
  title={Integrating semantic knowledge to tackle zero-shot text classification},
  author={Zhang, Jingqing and Lertvittayakumjorn, Piyawat and Guo, Yike},
  journal={arXiv preprint arXiv:1903.12626},
  year={2019}
}
@article{wu1994verb,
  title={Verb semantics and lexical selection},
  author={Wu, Zhibiao and Palmer, Martha},
  journal={arXiv preprint cmp-lg/9406033},
  year={1994}
}
@article{Monka2022,
abstract = { KGs can represent auxiliary knowledge either in an underlying graph-structured schema or in a vector-based knowledge graph embedding. Intending to enable the reader to solve visual transfer learning problems with the help of specific KG-DL configurations we start with a description of relevant modeling structures of a KG of various expressions, such as directed labeled graphs, hypergraphs, and hyper-relational graphs. We explain the notion of feature extractor, while specifically referring to visual and semantic features. We provide a broad overview of knowledge graph embedding methods and describe several joint training objectives suitable to combine them with high dimensional visual embeddings. The main section introduces four different categories on how a KG can be combined with a DL pipeline: 1) Knowledge Graph as a Reviewer; 2) Knowledge Graph as a Trainee; 3) Knowledge Graph as a Trainer; and 4) Knowledge Graph as a Peer. To help researchers find meaningful evaluation benchmarks, we provide an overview of generic KGs and a set of image processing datasets and benchmarks that include various types of auxiliary knowledge. Last, we summarize related surveys and give an outlook about challenges and open issues for future research.},
archivePrefix = {arXiv},
arxivId = {2201.11794},
author = {Monka, Sebastian and Halilaj, Lavdim and Rettinger, Achim},
doi = {10.3233/SW-212959},
eprint = {2201.11794},
file = {:home/philippos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Monka, Halilaj, Rettinger - 2022 - A survey on visual transfer learning using knowledge graphs.pdf:pdf},
issn = {22104968},
journal = {Semantic Web},
keywords = {Knowledge graph, visual transfer learning, knowled,knowledge-based machine learning,visual transfer learning},
number = {3},
pages = {477--510},
title = {{A survey on visual transfer learning using knowledge graphs}},
volume = {13},
year = {2022}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@article{bhagavatula2019abductive,
  title={Abductive commonsense reasoning},
  author={Bhagavatula, Chandra and Bras, Ronan Le and Malaviya, Chaitanya and Sakaguchi, Keisuke and Holtzman, Ari and Rashkin, Hannah and Downey, Doug and Yih, Scott Wen-tau and Choi, Yejin},
  journal={arXiv preprint arXiv:1908.05739},
  year={2019}
}
@inproceedings{zhang2019tgg,
  title={TGG: Transferable graph generation for zero-shot and few-shot learning},
  author={Zhang, Chenrui and Lyu, Xiaoqing and Tang, Zhi},
  booktitle={Proceedings of the 27th ACM International Conference on Multimedia},
  pages={1641--1649},
  year={2019}
}
@article{bosselut2019comet,
  title={COMET: Commonsense transformers for automatic knowledge graph construction},
  author={Bosselut, Antoine and Rashkin, Hannah and Sap, Maarten and Malaviya, Chaitanya and Celikyilmaz, Asli and Choi, Yejin},
  journal={arXiv preprint arXiv:1906.05317},
  year={2019}
}

@article{bastings2017graph,
  title={Graph convolutional encoders for syntax-aware neural machine translation},
  author={Bastings, Jasmijn and Titov, Ivan and Aziz, Wilker and Marcheggiani, Diego and Sima'an, Khalil},
  journal={arXiv preprint arXiv:1704.04675},
  year={2017}
}


@article{adhikari2020learning,
  title={Learning dynamic belief graphs to generalize on text-based games},
  author={Adhikari, Ashutosh and Yuan, Xingdi and C{\^o}t{\'e}, Marc-Alexandre and Zelinka, Mikul{\'a}{\v{s}} and Rondeau, Marc-Antoine and Laroche, Romain and Poupart, Pascal and Tang, Jian and Trischler, Adam and Hamilton, Will},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3045--3057},
  year={2020}
}

@inproceedings{wu2019simplifying,
  title={Simplifying graph convolutional networks},
  author={Wu, Felix and Souza, Amauri and Zhang, Tianyi and Fifty, Christopher and Yu, Tao and Weinberger, Kilian},
  booktitle={International conference on machine learning},
  pages={6861--6871},
  year={2019},
  organization={PMLR}
}
@article{vashishth2019composition,
  title={Composition-based multi-relational graph convolutional networks},
  author={Vashishth, Shikhar and Sanyal, Soumya and Nitin, Vikram and Talukdar, Partha},
  journal={arXiv preprint arXiv:1911.03082},
  year={2019}
}

@article{hamilton2017inductive,
  title={Inductive representation learning on large graphs},
  author={Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}


@inproceedings{gouidis2022,
  title={ Detecting Object States vs Detecting Objects: A New Dataset and a Quantitative Experimental Study},
  author={Gouidis, F. and  Patkos, T. and Argyros, A. and Plexousakis, D. },
  booktitle={ Proceedings of the 17th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications  (VISAPP)},
  year={2022},
  volume={5},
  pages={590--600}
}

@incollection{fellbaum2010wordnet,
  title={WordNet},
  author={Fellbaum, Christiane},
  booktitle={Theory and applications of ontology: computer applications},
  pages={231--243},
  year={2010},
  publisher={Springer}
}

@article{nayak:tmlr22,
  Author = {Nayak, N. V. and Bach, S. H.},
  Title = {Zero-Shot Learning with Common Sense Knowledge Graphs},
  Journal = {Transactions on Machine Learning Research (TMLR)},
  Year = {2022}}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer}
}
@inproceedings{akata2015evaluation,
  title={Evaluation of output embeddings for fine-grained image classification},
  author={Akata, Zeynep and Reed, Scott and Walter, Daniel and Lee, Honglak and Schiele, Bernt},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2927--2936},
  year={2015}
}
article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}

@article{vrandevcic2014wikidata,
  title={Wikidata: a free collaborative knowledgebase},
  author={Vrande{\v{c}}i{\'c}, Denny and Kr{\"o}tzsch, Markus},
  journal={Communications of the ACM},
  volume={57},
  number={10},
  pages={78--85},
  year={2014},
  publisher={ACM New York, NY, USA}
}


@inproceedings{auer2007dbpedia,
  title={Dbpedia: A nucleus for a web of open data},
  author={Auer, S{\"o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  booktitle={The Semantic Web: 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007+ ASWC 2007, Busan, Korea, November 11-15, 2007. Proceedings},
  pages={722--735},
  year={2007},
  organization={Springer}
}

@article{atzmon2020causal,
  title={A causal view of compositional zero-shot recognition},
  author={Atzmon, Yuval and Kreuk, Felix and Shalit, Uri and Chechik, Gal},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1462--1473},
  year={2020}
}

@inproceedings{purushwalkam2019task,
  title={Task-driven modular networks for zero-shot compositional learning},
  author={Purushwalkam, Senthil and Nickel, Maximilian and Gupta, Abhinav and Ranzato, Marc'Aurelio},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3593--3602},
  year={2019}
}
@article{Li2020,
abstract = {Attributes and objects can compose diverse compositions. To model the compositional nature of these general concepts, it is a good choice to learn them through transformations, such as coupling and decoupling. However, complex transformations need to satisfy specific principles to guarantee the rationality. In this paper, we first propose a previously ignored principle of attribute-object transformation: Symmetry. For example, coupling peeled-apple with attribute peeled should result in peeled-apple, and decoupling peeled from apple should still output apple. Incorporating the symmetry principle, a transformation framework inspired by group theory is built, i.e. SymNet. SymNet consists of two modules, Coupling Network and Decoupling Network. With the group axioms and symmetry property as objectives, we adopt Deep Neural Networks to implement SymNet and train it in an end-to-end paradigm. Moreover, we propose a Relative Moving Distance (RMD) based recognition method to utilize the attribute change instead of the attribute pattern itself to classify attributes. Our symmetry learning can be utilized for the Compositional Zero-Shot Learning task and outperforms the state-of-the-art on widely-used benchmarks. Code is available at https://github.com/DirtyHarryLYL/SymNet.},
  title={Symmetry and group in attribute-object compositions},
  author={Li, Yong-Lu and Xu, Yue and Mao, Xiaohan and Lu, Cewu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11316--11325},
  year={2020}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}


@inproceedings{yang2020learning,
  title={Learning unseen concepts via hierarchical decomposition and composition},
  author={Yang, Muli and Deng, Cheng and Yan, Junchi and Liu, Xianglong and Tao, Dacheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10248--10256},
  year={2020}
}
@inproceedings{misra2017red,
  title={From red wine to red tomato: Composition with context},
  author={Misra, Ishan and Gupta, Abhinav and Hebert, Martial},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1792--1801},
  year={2017}
}


@inproceedings{nagarajan2018attributes,
  title={Attributes as operators: factorizing unseen attribute-object compositions},
  author={Nagarajan, Tushar and Grauman, Kristen},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={169--185},
  year={2018}
}

@inproceedings{changpinyo2016synthesized,
  title={Synthesized classifiers for zero-shot learning},
  author={Changpinyo, Soravit and Chao, Wei-Lun and Gong, Boqing and Sha, Fei},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5327--5336},
  year={2016}
}

@article{Wang2018b,
abstract = {We consider the problem of zero-shot recognition: learning a visual classifier for a category with zero training examples, just using the word embedding of the category and its relationship to other categories, which visual data are provided. The key to dealing with the unfamiliar or novel category is to transfer knowledge obtained from familiar classes to describe the unfamiliar class. In this paper, we build upon the recently introduced Graph Convolutional Network (GCN) and propose an approach that uses both semantic embeddings and the categorical relationships to predict the classifiers. Given a learned knowledge graph (KG), our approach takes as input semantic embeddings for each node (representing visual category). After a series of graph convolutions, we predict the visual classifier for each category. During training, the visual classifiers for a few categories are given to learn the GCN parameters. At test time, these filters are used to predict the visual classifiers of unseen categories. We show that our approach is robust to noise in the KG. More importantly, our approach provides significant improvement in performance compared to the current state-of-the-art results (from 2 $\sim$ 3% on some metrics to whopping 20% on a few).},
archivePrefix = {arXiv},
arxivId = {arXiv:1803.08035v2},
author = {Wang, Xiaolong and Ye, Yufei and Gupta, Abhinav},
doi = {10.1109/CVPR.2018.00717},
eprint = {arXiv:1803.08035v2},
file = {:home/philo/Documents/Mendeley Desktop/Wang, Ye, Gupta - 2018 - Zero-Shot Recognition via Semantic Embeddings and Knowledge Graphs.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Object recognition - detection/Zero shot - Few shot},
pages = {6857--6866},
title = {{Zero-Shot Recognition via Semantic Embeddings and Knowledge Graphs}},
year = {2018}
}



@article{Kampffmeyer2019,
abstract = {Graph convolutional neural networks have recently shown great potential for the task of zero-shot learning. These models are highly sample efficient as related concepts in the graph structure share statistical strength allowing generalization to new classes when faced with a lack of data. However, multi-layer architectures, which are required to propagate knowledge to distant nodes in the graph, dilute the knowledge by performing extensive Laplacian smoothing at each layer and thereby consequently decrease performance. In order to still enjoy the benefit brought by the graph structure while preventing dilution of knowledge from distant nodes, we propose a Dense Graph Propagation (DGP) module with carefully designed direct links among distant nodes. DGP allows us to exploit the hierarchical graph structure of the knowledge graph through additional connections. These connections are added based on a node's relationship to its ancestors and descendants. A weighting scheme is further used to weigh their contribution depending on the distance to the node to improve information propagation in the graph. Combined with finetuning of the representations in a two-stage training approach our method outperforms state-of-the-art zero-shot learning approaches.},
archivePrefix = {arXiv},
arxivId = {1805.11724},
author = {Kampffmeyer, Michael and Chen, Yinbo and Liang, Xiaodan and Wang, Hao and Zhang, Yujia and Xing, Eric P.},
doi = {10.1109/CVPR.2019.01175},
eprint = {1805.11724},
file = {:home/philo/Documents/Mendeley Desktop/Kampffmeyer et al. - 2019 - Rethinking knowledge graph propagation for zero-shot learning.pdf:pdf;:home/philo/Documents/Mendeley Desktop//Kampffmeyer et al. - 2019 - Rethinking knowledge graph propagation for zero-shot learning.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Categorization,Deep Learning,Recognition: Detection,Retrieval},
pages = {11479--11488},
title = {{Rethinking knowledge graph propagation for zero-shot learning}},
volume = {2019-June},
year = {2019}
}
@inproceedings{chuang2018learning,
  title={Learning to act properly: Predicting and explaining affordances from images},
  author={Chuang, Ching-Yao and Li, Jiaman and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={975--983},
  year={2018}
}
@inproceedings{schoonbeek2024industreal,
  title={IndustReal: A Dataset for Procedure Step Recognition Handling Execution Errors in Egocentric Videos in an Industrial-Like Setting},
  author={Schoonbeek, Tim J and Houben, Tim and Onvlee, Hans and van der Sommen, Fons and others},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={4365--4374},
  year={2024}
}
@INPROCEEDINGS {10376814,
author = {N. Saini and H. Wang and A. Swaminathan and V. Jayasundara and B. He and K. Gupta and A. Shrivastava},
booktitle = {2023 IEEE/CVF International Conference on Computer Vision (ICCV)},
title = {Chop \& Learn: Recognizing and Generating Object-State Compositions},
year = {2023},
volume = {},
issn = {},
pages = {20190-20201},
abstract = {Recognizing and generating object-state compositions has been a challenging task, especially when generalizing to unseen compositions. In this paper, we study the task of cutting objects in different styles and the resulting object state changes. We propose a new benchmark suite Chop &amp; Learn, to accommodate the needs of learning objects and different cut styles using multiple viewpoints. We also propose a new task of Compositional Image Generation, which can transfer learned cut styles to different objects, by generating novel object-state images. Moreover, we also use the videos for Compositional Action Recognition, and show valuable uses of this dataset for multiple video tasks. Project website: https://chopnlearn.github.io.},
keywords = {computer vision;three-dimensional displays;image recognition;image synthesis;benchmark testing;icebergs;task analysis},
doi = {10.1109/ICCV51070.2023.01852},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV51070.2023.01852},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {oct}
}

@InProceedings{Grauman_2022_CVPR,
    author    = {Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and Martin, Miguel and Nagarajan, Tushar and Radosavovic, Ilija and Ramakrishnan, Santhosh Kumar and Ryan, Fiona and Sharma, Jayant and Wray, Michael and Xu, Mengmeng and Xu, Eric Zhongcong and Zhao, Chen and Bansal, Siddhant and Batra, Dhruv and Cartillier, Vincent and Crane, Sean and Do, Tien and Doulaty, Morrie and Erapalli, Akshay and Feichtenhofer, Christoph and Fragomeni, Adriano and Fu, Qichen and Gebreselasie, Abrham and Gonz\'alez, Cristina and Hillis, James and Huang, Xuhua and Huang, Yifei and Jia, Wenqi and Khoo, Weslie and Kol\'a\v{r}, J\'achym and Kottur, Satwik and Kumar, Anurag and Landini, Federico and Li, Chao and Li, Yanghao and Li, Zhenqiang and Mangalam, Karttikeya and Modhugu, Raghava and Munro, Jonathan and Murrell, Tullie and Nishiyasu, Takumi and Price, Will and Ruiz, Paola and Ramazanova, Merey and Sari, Leda and Somasundaram, Kiran and Southerland, Audrey and Sugano, Yusuke and Tao, Ruijie and Vo, Minh and Wang, Yuchen and Wu, Xindi and Yagi, Takuma and Zhao, Ziwei and Zhu, Yunyi and Arbel\'aez, Pablo and Crandall, David and Damen, Dima and Farinella, Giovanni Maria and Fuegen, Christian and Ghanem, Bernard and Ithapu, Vamsi Krishna and Jawahar, C. V. and Joo, Hanbyul and Kitani, Kris and Li, Haizhou and Newcombe, Richard and Oliva, Aude and Park, Hyun Soo and Rehg, James M. and Sato, Yoichi and Shi, Jianbo and Shou, Mike Zheng and Torralba, Antonio and Torresani, Lorenzo and Yan, Mingfei and Malik, Jitendra},
    title     = {Ego4D: Around the World in 3,000 Hours of Egocentric Video},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {18995-19012}
}
@article{Soucek2022,
abstract = {We aim to learn to temporally localize object state changes and the corresponding state-modifying actions by observing people interacting with objects in long uncurated web videos. We introduce three principal contributions. First, we explore alternative multi-task network architectures and identify a model that enables efficient joint learning of multiple object states and actions such as pouring water and pouring coffee. Second, we design a multi-task self-supervised learning procedure that exploits different types of constraints between objects and state-modifying actions enabling end-to-end training of a model for temporal localization of object states and actions in videos from only noisy video-level supervision. Third, we report results on the large-scale ChangeIt and COIN datasets containing tens of thousands of long (un)curated web videos depicting various interactions such as hole drilling, cream whisking, or paper plane folding. We show that our multi-task model achieves a relative improvement of 40\% over the prior single-task methods and significantly outperforms both image-based and video-based zero-shot models for this problem. We also test our method on long egocentric videos of the EPIC-KITCHENS and the Ego4D datasets in a zero-shot setup demonstrating the robustness of our learned model.},
archivePrefix = {arXiv},
arxivId = {2211.13500},
author = {Sou{\v{c}}ek, Tom{\'{a}}{\v{s}} and Alayrac, Jean-Baptiste and Miech, Antoine and Laptev, Ivan and Sivic, Josef},
eprint = {2211.13500},
file = {:home/philippos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sou{\v{c}}ek et al. - 2022 - Multi-Task Learning of Object State Changes from Uncurated Videos.pdf:pdf},
mendeley-groups = {State Detection},
title = {{Multi-Task Learning of Object State Changes from Uncurated Videos}},
url = {http://arxiv.org/abs/2211.13500},
year = {2022}
}

@inproceedings{liu2017jointly,
  title={Jointly recognizing object fluents and tasks in egocentric videos},
  author={Liu, Yang and Wei, Ping and Zhu, Song-Chun},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2924--2932},
  year={2017}
}


@article{Sawatzky2019,
abstract = {When humans have to solve everyday tasks, they simply pick the objects that are most suitable. While the question which object should one use for a specific task sounds trivial for humans, it is very difficult to answer for robots or other autonomous systems. This issue, however, is not addressed by current benchmarks for object detection that focus on detecting object categories. We therefore introduce the COCO-Tasks dataset which comprises about 40,000 images where the most suitable objects for 14 tasks have been annotated. We furthermore propose an approach that detects the most suitable objects for a given task. The approach builds on a Gated Graph Neural Network to exploit the appearance of each object as well as the global context of all present objects in the scene. In our experiments, we show that the proposed approach outperforms other approaches that are evaluated on the dataset like classification or ranking approaches.},
archivePrefix = {arXiv},
arxivId = {1904.03000},
author = {Sawatzky, Johann and Souri, Yaser and Grund, Christian and Gall, Juergen},
eprint = {1904.03000},
file = {:home/philippos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sawatzky et al. - 2019 - What Object Should I Use - Task Driven Object Detection.pdf:pdf},
mendeley-groups = {Affordances - Functionality},
title = {{What Object Should I Use? - Task Driven Object Detection}},
url = {http://arxiv.org/abs/1904.03000},
year = {2019}
}


@article{Isola2015,
abstract = {Objects in visual scenes come in a rich variety of transformed states. A few classes of transformation have been heavily studied in computer vision: mostly simple, parametric changes in color and geometry. However, transformations in the physical world occur in many more flavors, and they come with semantic meaning: e.g., bending, folding, aging, etc. The transformations an object can undergo tell us about its physical and functional properties. In this paper, we introduce a dataset of objects, scenes, and materials, each of which is found in a variety of transformed states. Given a novel collection of images, we show how to explain the collection in terms of the states and transformations it depicts. Our system works by generalizing across object classes: states and transformations learned on one set of objects are used to interpret the image collection for an entirely new object class.},
author = {Isola, Phillip and Lim, Joseph J. and Adelson, Edward H.},
doi = {10.1109/CVPR.2015.7298744},
file = {:home/philo/Documents/Mendeley Desktop/Isola, Lim, Adelson - 2015 - Discovering states and transformations in image collections.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {State Detection},
pages = {1383--1391},
title = {{Discovering states and transformations in image collections}},
volume = {07-12-June},
year = {2015}
}

@article{Mancini2022,
abstract = {Compositional Zero-Shot learning (CZSL) aims to recognize unseen compositions of state and object visual primitives seen during training. A problem with standard CZSL is the assumption of knowing which unseen compositions will be available at test time. In this work, we overcome this assumption operating on the open world setting, where no limit is imposed on the compositional space at test time, and the search space contains a large number of unseen compositions. To address this problem, we propose a new approach, Compositional Cosine Graph Embedding (Co-CGE), based on two principles. First, Co-CGE models the dependency between states, objects and their compositions through a graph convolutional neural network. The graph propagates information from seen to unseen concepts, improving their representations. Second, since not all unseen compositions are equally feasible, and less feasible ones may damage the learned representations, Co-CGE estimates a feasibility score for each unseen composition, using the scores as margins in a cosine similarity-based loss and as weights in the adjacency matrix of the graphs. Experiments show that our approach achieves state-of-the-art performances in standard CZSL while outperforming previous methods in the open world scenario.},
author = {Mancini, Massimiliano and Naeem, Muhammad Ferjad and Xian, Yongqin and Akata, Zeynep},
doi = {10.1109/TPAMI.2022.3163667},
file = {:home/philo/Documents/Mendeley Desktop/Mancini et al. - 2022 - Learning Graph Embeddings for Open World Compositional Zero-Shot Learning.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Compositional Zero-Shot Learning,Convolutional neural networks,Dogs,Graph Neural Networks,Open-World Recognition,Scene Understanding,Smoothing methods,Standards,Task analysis,Training,Visualization},
number = {c},
pages = {1--15},
publisher = {IEEE},
title = {{Learning Graph Embeddings for Open World Compositional Zero-Shot Learning}},
volume = {8828},
year = {2022}
}

@inproceedings{Wang2016b,
abstract = {What defines an action like 'kicking ball'? We argue that the true meaning of an action lies in the change or transformation an action brings to the environment. In this paper, we propose a novel representation for actions by modeling an action as a transformation which changes the state of the environment before the action happens (precondition) to the state after the action (effect). Motivated by recent advancements of video representation using deep learning, we design a Siamese network which models the action as a transformation on a high-level feature space. We show that our model gives improvements on standard action recognition datasets including UCF101 and HMDB51. More importantly, our approach is able to generalize beyond learned action categories and shows significant performance improvement on cross-category generalization on our new ACT dataset.},
archivePrefix = {arXiv},
arxivId = {1512.00795},
author = {Wang, Xiaolong and Farhadi, Ali and Gupta, Abhinav},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.291},
eprint = {1512.00795},
file = {:home/philippos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Farhadi, Gupta - 2016 - Actions $\sim$ Transformations.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
mendeley-groups = {Action Recognition/Low},
month = {jun},
pages = {2658--2667},
publisher = {IEEE},
title = {{Actions $\sim$ Transformations}},
url = {http://ieeexplore.ieee.org/document/7780660/},
volume = {2016-Decem},
year = {2016}
}

@article{jamone2016affordances,
  title={Affordances in psychology, neuroscience, and robotics: A survey},
  author={Jamone, Lorenzo and Ugur, Emre and Cangelosi, Angelo and Fadiga, Luciano and Bernardino, Alexandre and Piater, Justus and Santos-Victor, Jos{\'e}},
  journal={IEEE Transactions on Cognitive and Developmental Systems},
  volume={10},
  number={1},
  pages={4--25},
  year={2016},
  publisher={IEEE}
}


@article{Farhadi2009,
abstract = {We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object ("spotty dog", not just "dog"); to say something about unfamiliar objects ("hairy and four-legged", not just "unknown"); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic ("spotty") or discriminative ("dogs have it but sheep do not"). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attributebased framework. {\textcopyright} 2009 IEEE.},
author = {Farhadi, Ali and Endres, Ian and Hoiem, Derek and Forsyth, David},
doi = {10.1109/CVPRW.2009.5206772},
file = {:home/philo/Documents/Mendeley Desktop/Farhadi et al. - 2009 - Describing objects by their attributes.pdf:pdf},
isbn = {9781424439935},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2009},
mendeley-groups = {Attributes},
pages = {1778--1785},
publisher = {IEEE},
title = {{Describing objects by their attributes}},
year = {2009}
}


@inproceedings{karthik2022open,
  title={KG-SP: Knowledge Guided Simple Primitives for Open World Compositional Zero-Shot Learning},
  author={Karthik, S and Mancini, M and Akata, Zeynep},
  booktitle={35th IEEE Conference on Computer Vision and Pattern Recognition},
  year={2022},
  organization={IEEE}
}
@inproceedings{li2022siamese,
  title={Siamese Contrastive Embedding Network for Compositional Zero-Shot Learning},
  author={Li, Xiangyu and Yang, Xu and Wei, Kun and Deng, Cheng and Yang, Muli},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9326--9335},
  year={2022}
}


@inproceedings{wang2023learning,
  title={Learning Conditional Attributes for Compositional Zero-Shot Learning},
  author={Wang, Qingsheng  and Liu, Lingqiao  and Jing, Chenchen and Chen, Hao and  Liang, Guoqiang and Wang,  Peng and Shen, Chunhua },
  booktitle={CVPR},
  year={2023}
}

@inproceedings{hao2023learning,
  title={Learning Attention as Disentangler for Compositional Zero-shot Learning},
  author={Hao, Shaozhe and Han, Kai and Wong, Kwan-Yee K},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15315--15324},
  year={2023}
}




@inproceedings{DBLP:conf/aaaiss/GouidisVPABP20,
  author       = {Filippos Gouidis and
                  Alexandros Vassiliades and
                  Theodore Patkos and
                  Antonis A. Argyros and
                  Nick Bassiliades and
                  Dimitris Plexousakis},
  editor       = {Andreas Martin and
                  Knut Hinkelmann and
                  Hans{-}Georg Fill and
                  Aurona Gerber and
                  Doug Lenat and
                  Reinhard Stolle and
                  Frank van Harmelen},
  title        = {A Review on Intelligent Object Perception Methods Combining Knowledge-based
                  Reasoning and Machine Learning},
  booktitle    = {Proceedings of the {AAAI} 2020 Spring Symposium on Combining Machine
                  Learning and Knowledge Engineering in Practice, {AAAI-MAKE} 2020,
                  Palo Alto, CA, USA, March 23-25, 2020, Volume {I}},
  series       = {{CEUR} Workshop Proceedings},
  volume       = {2600},
  publisher    = {CEUR-WS.org},
  year         = {2020},
  url          = {https://ceur-ws.org/Vol-2600/paper7.pdf},
  timestamp    = {Fri, 10 Mar 2023 16:22:39 +0100},
  biburl       = {https://dblp.org/rec/conf/aaaiss/GouidisVPABP20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/i-semantics/VassiliadesBGP20,
  author       = {Alexandros Vassiliades and
                  Nick Bassiliades and
                  Filippos Gouidis and
                  Theodore Patkos},
  editor       = {Eva Blomqvist and
                  Paul Groth and
                  Victor de Boer and
                  Tassilo Pellegrini and
                  Mehwish Alam and
                  Tobias K{\"{a}}fer and
                  Peter Kieseberg and
                  Sabrina Kirrane and
                  Albert Mero{\~{n}}o{-}Pe{\~{n}}uela and
                  Harshvardhan J. Pandit},
  title        = {A Knowledge Retrieval Framework for Household Objects and Actions
                  with External Knowledge},
  booktitle    = {Semantic Systems. In the Era of Knowledge Graphs - 16th International
                  Conference on Semantic Systems, SEMANTiCS 2020, Amsterdam, The Netherlands,
                  September 7-10, 2020, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {12378},
  pages        = {36--52},
  publisher    = {Springer},
  year         = {2020},
  url          = {https://doi.org/10.1007/978-3-030-59833-4\_3},
  doi          = {10.1007/978-3-030-59833-4\_3},
  timestamp    = {Fri, 14 May 2021 08:34:14 +0200},
  biburl       = {https://dblp.org/rec/conf/i-semantics/VassiliadesBGP20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/visapp/GouidisPAP22,
  author       = {Filippos Gouidis and
                  Theodore Patkos and
                  Antonis A. Argyros and
                  Dimitris Plexousakis},
  editor       = {Giovanni Maria Farinella and
                  Petia Radeva and
                  Kadi Bouatouch},
  title        = {Detecting Object States vs Detecting Objects: {A} New Dataset and
                  a Quantitative Experimental Study},
  booktitle    = {Proceedings of the 17th International Joint Conference on Computer
                  Vision, Imaging and Computer Graphics Theory and Applications, VISIGRAPP
                  2022, Volume 5: VISAPP, Online Streaming, February 6-8, 2022},
  pages        = {590--600},
  publisher    = {{SCITEPRESS}},
  year         = {2022},
  url          = {https://doi.org/10.5220/0010898400003124},
  doi          = {10.5220/0010898400003124},
  timestamp    = {Tue, 06 Jun 2023 14:58:00 +0200},
  biburl       = {https://dblp.org/rec/conf/visapp/GouidisPAP22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{DBLP:conf/icaart/GouidisVBP22,
  author       = {Filippos Gouidis and
                  Alexandros Vassiliades and
                  Nena Basina and
                  Theodore Patkos},
  editor       = {Ana Paula Rocha and
                  Luc Steels and
                  H. Jaap van den Herik},
  title        = {Towards a Formal Framework for Social Robots with Theory of Mind},
  booktitle    = {Proceedings of the 14th International Conference on Agents and Artificial
                  Intelligence, {ICAART} 2022, Volume 3, Online Streaming, February
                  3-5, 2022},
  pages        = {689--696},
  publisher    = {{SCITEPRESS}},
  year         = {2022},
  url          = {https://doi.org/10.5220/0010893300003116},
  doi          = {10.5220/0010893300003116},
  timestamp    = {Tue, 06 Jun 2023 14:58:00 +0200},
  biburl       = {https://dblp.org/rec/conf/icaart/GouidisVBP22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{DBLP:conf/visigrapp/GouidisPPAP24,
  author       = {Filippos Gouidis and
                  Konstantinos E. Papoutsakis and
                  Theodore Patkos and
                  Antonis A. Argyros and
                  Dimitris Plexousakis},
  editor       = {Petia Radeva and
                  Antonino Furnari and
                  Kadi Bouatouch and
                  A. Augusto de Sousa},
  title        = {Exploring the Impact of Knowledge Graphs on Zero-Shot Visual Object
                  State Classification},
  booktitle    = {Proceedings of the 19th International Joint Conference on Computer
                  Vision, Imaging and Computer Graphics Theory and Applications, VISIGRAPP
                  2024, Volume 2: VISAPP, Rome, Italy, February 27-29, 2024},
  pages        = {738--749},
  publisher    = {{SCITEPRESS}},
  year         = {2024},
  url          = {https://doi.org/10.5220/0012434800003660},
  doi          = {10.5220/0012434800003660},
  timestamp    = {Sun, 06 Oct 2024 21:15:54 +0200},
  biburl       = {https://dblp.org/rec/conf/visigrapp/GouidisPPAP24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{DBLP:conf/aaaiss/GouidisPPPAP24,
  author       = {Filippos Gouidis and
                  Katerina Papantoniou and
                  Konstantinos E. Papoutsakis and
                  Theodore Patkos and
                  Antonis A. Argyros and
                  Dimitris Plexousakis},
  editor       = {Ron P. A. Petrick and
                  Christopher W. Geib},
  title        = {Fusing Domain-Specific Content from Large Language Models into Knowledge
                  Graphs for Enhanced Zero Shot Object State Classification},
  booktitle    = {Proceedings of the {AAAI} 2024 Spring Symposium Series, Stanford,
                  CA, USA, March 25-27, 2024},
  pages        = {115--124},
  publisher    = {{AAAI} Press},
  year         = {2024},
  url          = {https://doi.org/10.1609/aaaiss.v3i1.31190},
  doi          = {10.1609/AAAISS.V3I1.31190},
  timestamp    = {Mon, 03 Jun 2024 16:39:33 +0200},
  biburl       = {https://dblp.org/rec/conf/aaaiss/GouidisPPPAP24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{
@inproceedings{DBLP:conf/aaaiss/GouidisVPABP20,
  author       = {Filippos Gouidis and
                  Alexandros Vassiliades and
                  Theodore Patkos and
                  Antonis A. Argyros and
                  Nick Bassiliades and
                  Dimitris Plexousakis},
  editor       = {Andreas Martin and
                  Knut Hinkelmann and
                  Hans{-}Georg Fill and
                  Aurona Gerber and
                  Doug Lenat and
                  Reinhard Stolle and
                  Frank van Harmelen},
  title        = {A Review on Intelligent Object Perception Methods Combining Knowledge-based
                  Reasoning and Machine Learning},
  booktitle    = {Proceedings of the {AAAI} 2020 Spring Symposium on Combining Machine
                  Learning and Knowledge Engineering in Practice, {AAAI-MAKE} 2020,
                  Palo Alto, CA, USA, March 23-25, 2020, Volume {I}},
  series       = {{CEUR} Workshop Proceedings},
  volume       = {2600},
  publisher    = {CEUR-WS.org},
  year         = {2020},
  url          = {https://ceur-ws.org/Vol-2600/paper7.pdf},
  timestamp    = {Fri, 10 Mar 2023 16:22:39 +0100},
  biburl       = {https://dblp.org/rec/conf/aaaiss/GouidisVPABP20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/i-semantics/VassiliadesBGP20,
  author       = {Alexandros Vassiliades and
                  Nick Bassiliades and
                  Filippos Gouidis and
                  Theodore Patkos},
  editor       = {Eva Blomqvist and
                  Paul Groth and
                  Victor de Boer and
                  Tassilo Pellegrini and
                  Mehwish Alam and
                  Tobias K{\"{a}}fer and
                  Peter Kieseberg and
                  Sabrina Kirrane and
                  Albert Mero{\~{n}}o{-}Pe{\~{n}}uela and
                  Harshvardhan J. Pandit},
  title        = {A Knowledge Retrieval Framework for Household Objects and Actions
                  with External Knowledge},
  booktitle    = {Semantic Systems. In the Era of Knowledge Graphs - 16th International
                  Conference on Semantic Systems, SEMANTiCS 2020, Amsterdam, The Netherlands,
                  September 7-10, 2020, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {12378},
  pages        = {36--52},
  publisher    = {Springer},
  year         = {2020},
  url          = {https://doi.org/10.1007/978-3-030-59833-4\_3},
  doi          = {10.1007/978-3-030-59833-4\_3},
  timestamp    = {Fri, 14 May 2021 08:34:14 +0200},
  biburl       = {https://dblp.org/rec/conf/i-semantics/VassiliadesBGP20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/visapp/GouidisPAP22,
  author       = {Filippos Gouidis and
                  Theodore Patkos and
                  Antonis A. Argyros and
                  Dimitris Plexousakis},
  editor       = {Giovanni Maria Farinella and
                  Petia Radeva and
                  Kadi Bouatouch},
  title        = {Detecting Object States vs Detecting Objects: {A} New Dataset and
                  a Quantitative Experimental Study},
  booktitle    = {Proceedings of the 17th International Joint Conference on Computer
                  Vision, Imaging and Computer Graphics Theory and Applications, VISIGRAPP
                  2022, Volume 5: VISAPP, Online Streaming, February 6-8, 2022},
  pages        = {590--600},
  publisher    = {{SCITEPRESS}},
  year         = {2022},
  url          = {https://doi.org/10.5220/0010898400003124},
  doi          = {10.5220/0010898400003124},
  timestamp    = {Tue, 06 Jun 2023 14:58:00 +0200},
  biburl       = {https://dblp.org/rec/conf/visapp/GouidisPAP22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{DBLP:conf/icaart/GouidisVBP22,
  author       = {Filippos Gouidis and
                  Alexandros Vassiliades and
                  Nena Basina and
                  Theodore Patkos},
  editor       = {Ana Paula Rocha and
                  Luc Steels and
                  H. Jaap van den Herik},
  title        = {Towards a Formal Framework for Social Robots with Theory of Mind},
  booktitle    = {Proceedings of the 14th International Conference on Agents and Artificial
                  Intelligence, {ICAART} 2022, Volume 3, Online Streaming, February
                  3-5, 2022},
  pages        = {689--696},
  publisher    = {{SCITEPRESS}},
  year         = {2022},
  url          = {https://doi.org/10.5220/0010893300003116},
  doi          = {10.5220/0010893300003116},
  timestamp    = {Tue, 06 Jun 2023 14:58:00 +0200},
  biburl       = {https://dblp.org/rec/conf/icaart/GouidisVBP22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




inproceedings{DBLP:conf/visigrapp/GouidisPPAP24,
  author       = {Filippos Gouidis and
                  Konstantinos E. Papoutsakis and
                  Theodore Patkos and
                  Antonis A. Argyros and
                  Dimitris Plexousakis},
  editor       = {Petia Radeva and
                  Antonino Furnari and
                  Kadi Bouatouch and
                  A. Augusto de Sousa},
  title        = {Exploring the Impact of Knowledge Graphs on Zero-Shot Visual Object
                  State Classification},
  booktitle    = {Proceedings of the 19th International Joint Conference on Computer
                  Vision, Imaging and Computer Graphics Theory and Applications, VISIGRAPP
                  2024, Volume 2: VISAPP, Rome, Italy, February 27-29, 2024},
  pages        = {738--749},
  publisher    = {{SCITEPRESS}},
  year         = {2024},
  url          = {https://doi.org/10.5220/0012434800003660},
  doi          = {10.5220/0012434800003660},
  timestamp    = {Sun, 06 Oct 2024 21:15:54 +0200},
  biburl       = {https://dblp.org/rec/conf/visigrapp/GouidisPPAP24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



inproceedings{DBLP:conf/aaaiss/GouidisPPPAP24,
  author       = {Filippos Gouidis and
                  Katerina Papantoniou and
                  Konstantinos E. Papoutsakis and
                  Theodore Patkos and
                  Antonis A. Argyros and
                  Dimitris Plexousakis},
  editor       = {Ron P. A. Petrick and
                  Christopher W. Geib},
  title        = {Fusing Domain-Specific Content from Large Language Models into Knowledge
                  Graphs for Enhanced Zero Shot Object State Classification},
  booktitle    = {Proceedings of the {AAAI} 2024 Spring Symposium Series, Stanford,
                  CA, USA, March 25-27, 2024},
  pages        = {115--124},
  publisher    = {{AAAI} Press},
  year         = {2024},
  url          = {https://doi.org/10.1609/aaaiss.v3i1.31190},
  doi          = {10.1609/AAAISS.V3I1.31190},
  timestamp    = {Mon, 03 Jun 2024 16:39:33 +0200},
  biburl       = {https://dblp.org/rec/conf/aaaiss/GouidisPPPAP24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{gouidis2024llm,
  title={LLM-aided Knowledge Graph construction for Zero-Shot Visual Object State Classification},
  author={Gouidis, Filippos and Papantoniou, Katerina and Papoutsakis, Konstantinos and Patkos, Theodore and Argyros, Antonis and Plexousakis, Dimitris},
  booktitle={2024 14th International Conference on Pattern Recognition Systems (ICPRS)},
  pages={1--7},
  year={2024},
  organization={IEEE}
}



%  url          = {https://doi.org/10.1007/978-3-031-22695-3\_13},
%  doi          = {10.1007/978-3-031-22695-3\_13},
%  timestamp    = {Wed, 04 Sep 2024 08:13:04 +0200},
@inproceedings{DBLP:conf/ausai/Gouidis,
  author       = {Filippos Gouidis and
                  Konstantinos Papoutsakis and
                Theodore  Patkos  and 
               Antonis    Argyros  and 
               Dimitris  Plexousakis },

  title        = {Enabling Visual Intelligence by Leveraging Visual Object States },
  booktitle    = {{A4} 2024: Advances in Artificial Intelligence - 37th Australasian
                  Joint Conference, {AI} 2024, Perth, WA, Australia, November 24-28, 2024,
                  Proceedings},
  series       = {Lecture Notes in Computer Science},
  publisher    = {Springer},
  year         = {2024},
  biburl       = {https://dblp.org/rec/conf/ausai/OwenDW22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{Gouidis2025WACVrecognizing,
  title={Recognizing Unseen States of Unknown Objects by Leveraging Knowledge Graphs},
  author={Gouidis, Filipos and Papoutsakis, Konstantinos and Patkos, Theodore and Argyros, Antonis and Plexousakis, Dimitris},
  booktitle={2024 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year={2025},
    organization={IEEE}
}



,
  title={LLM-aided Knowledge Graph construction for Zero-Shot Visual Object State Classification},
  author={Gouidis, Filippos and Papantoniou, Katerina and Papoutsakis, Konstantinos and Patkos, Theodore and Argyros, Antonis and Plexousakis, Dimitris},
  booktitle={2024 14th International Conference on Pattern Recognition Systems (ICPRS)},
  pages={1--7},
  year={2024},
  organization={IEEE}
}



%  url          = {https://doi.org/10.1007/978-3-031-22695-3\_13},
%  doi          = {10.1007/978-3-031-22695-3\_13},
%  timestamp    = {Wed, 04 Sep 2024 08:13:04 +0200},
@inproceedings{DBLP:conf/ausai/Gouidis,
  author       = {Filippos Gouidis and
                  Konstantinos Papoutsakis and
                Theodore  Patkos  and 
               Antonis    Argyros  and 
               Dimitris  Plexousakis },

  title        = {Enabling Visual Intelligence by Leveraging Visual Object States },
  booktitle    = {{A4} 2024: Advances in Artificial Intelligence - 37th Australasian
                  Joint Conference, {AI} 2024, Perth, WA, Australia, November 24-28, 2024,
                  Proceedings},
  series       = {Lecture Notes in Computer Science},
  publisher    = {Springer},
  year         = {2024},
  biburl       = {https://dblp.org/rec/conf/ausai/OwenDW22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}









@inproceedings{zhang2022learning,
  title={Learning Invariant Visual Representations for Compositional Zero-Shot Learning},
  author={Zhang, Tian and Liang, Kongming and Du, Ruoyi and Sun, Xian and Ma, Zhanyu and Guo, Jun},
  booktitle={ECCV},
  year={2022}
}

InProceedings{Saini_2022_CVPR,
    author    = {Saini, Nirat and Pham, Khoi and Shrivastava, Abhinav},
    title     = {Disentangling Visual Embeddings for Attributes and Objects},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {13658-13667}
}

article{Lu2022,
abstract = {Compositional Zero-Shot Learning (CZSL) aims to recognize novel concepts formed by known states and objects during training. Existing methods either learn the combined state-object representation, challenging the generalization of unseen compositions, or design two classifiers to identify state and object separately from image features, ignoring the intrinsic relationship between them. To jointly eliminate the above issues and construct a more robust CZSL system, we propose a novel framework termed Decomposed Fusion with Soft Prompt (DFSP)1, by involving vision-language models (VLMs) for unseen composition recognition. Specifically, DFSP constructs a vector combination of learnable soft prompts with state and object to establish the joint representation of them. In addition, a cross-modal decomposed fusion module is designed between the language and image branches, which decomposes state and object among language features instead of image features. Notably, being fused with the decomposed features, the image features can be more expressive for learning the relationship with states and objects, respectively, to improve the response of unseen compositions in the pair space, hence narrowing the domain gap between seen and unseen sets. Experimental results on three challenging benchmarks demonstrate that our approach significantly outperforms other state-of-the-art methods by large margins.},
archivePrefix = {arXiv},
arxivId = {2211.10681},
author = {Lu, Xiaocheng and Liu, Ziming and Guo, Song and Guo, Jingcai},
eprint = {2211.10681},
file = {:home/philo/Documents/Mendeley Desktop/2211.10681.pdf:pdf},
title = {{Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning}},
url = {http://arxiv.org/abs/2211.10681},
year = {2022}
}


@article{nayak2022learning,
  title={Learning to compose soft prompts for compositional zero-shot learning},
  author={Nayak, Nihal V and Yu, Peilin and Bach, Stephen H},
  journal={arXiv preprint arXiv:2204.03574},
  year={2022}
}


@inproceedings{soucek2024genhowto,
    title={GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos},
    author={Sou\v{c}ek, Tom\'{a}\v{s} and Damen, Dima and Wray, Michael and Laptev, Ivan and Sivic, Josef},
    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month={June},
    year={2024}
}

@inproceedings{saini2023chop,
  title={Chop \& Learn: Recognizing and Generating Object-State Compositions},
  author={Saini, Nirat and Wang, Hanyu and Swaminathan, Archana and Jayasundara, Vinoj and He, Bo and Gupta, Kamal and Shrivastava, Abhinav},
  booktitle={Proceedings of the International Conference on Computer Vision (ICCV)},
  year={2023},
  organization={IEEE}
}


@article{soucek2022multi,
    title={Multi-Task Learning of Object States and State-Modifying Actions from Web Videos},
    author={Sou\v{c}ek, Tom\'{a}\v{s} and Alayrac, Jean-Baptiste and Miech, Antoine and Laptev, Ivan and Sivic, Josef},
    journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
    year={2024},
    doi={10.1109/TPAMI.2024.3362288}
}

@inproceedings{soucek2022look,
  title={Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos},
  author={Sou{\v{c}}ek, Tom{\'a}{\v{s}} and Alayrac, Jean-Baptiste and Miech, Antoine and Laptev, Ivan and Sivic, Josef},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022}
}


@inproceedings{alayrac2017joint,
  title={Joint Discovery of Object States and Manipulation Actions},
  author={Alayrac, Jean-Baptiste and Sivic, Josef and Laptev, Ivan and Lacoste-Julien, Simon},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  pages={407--416},
  year={2017}
}

@article{xue2024learning,
  title={Learning Object State Changes in Videos: An Open-World Perspective},
  author={Xue, Zihui and others},
  journal={arXiv preprint arXiv:2312.11782},
  year={2024}
}


@inproceedings{saini2022recognizing,
  title={Recognizing Actions Using Object States},
  author={Saini, Nirat and He, Bo and Shrivastava, Gaurav and Rambhatla, Sai Saketh and Shrivastava, Abhinav},
  booktitle={ICLR Workshop on Objects, Structure and Causality},
  year={2022}
}

@inproceedings{saini2022disentangling,
  title={Disentangling Visual Embeddings for Attributes and Objects},
  author={Saini, Nirat and Pham, Khoi and Shrivastava, Abhinav},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022}
}

@InProceedings{Lu_2018_CVPR,
author = {Lu, Cewu and Su, Hao and Li, Yonglu and Lu, Yongyi and Yi, Li and Tang, Chi-Keung and Guibas, Leonidas J.},
title = {Beyond Holistic Object Recognition: Enriching Image Understanding With Part States},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
} 
@inproceedings{Alayrac17,
abstract = {Many human activities involve object manipulations aiming to modify the object state. Examples of common state changes include full/empty bottle, open/closed door, and attached/detached car wheel. In this work, we seek to automatically discover the states of objects and the associated manipulation actions. Given a set of videos for a particular task, we propose a joint model that learns to identify object states and to localize state-modifying actions. Our model is formulated as a discriminative clustering cost with constraints. We assume a consistent temporal order for the changes in object states and manipulation actions, and introduce new optimization techniques to learn model parameters without additional supervision. We demonstrate successful discovery of seven manipulation actions and corresponding object states on a new dataset of videos depicting real-life object manipulations. We show that our joint formulation results in an improvement of object state discovery by action recognition and vice versa.},
archivePrefix = {arXiv},
arxivId = {1702.02738},
author = {Alayrac, Jean-Baptiste Baptiste and Sivic, Josef and Laptev, Ivan and Lacoste-Julien, Simon},
booktitle = {International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.234},
eprint = {1702.02738},
file = {:home/philo/Documents/Mendeley Desktop/Alayrac et al. - 2017 - Joint Discovery of Object States and Manipulation Actions(2).pdf:pdf;:home/philo/Documents/Mendeley Desktop/Alayrac et al. - 2017 - Joint Discovery of Object States and Manipulation Actions.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
mendeley-groups = {State Detection},
pages = {2146--2155},
title = {{Joint Discovery of Object States and Manipulation Actions}},
volume = {2017-Octob},
year = {2017}
}


@article{Liu2017,
abstract = {This paper addresses the problem of jointly recognizing object fluents and tasks in egocentric videos. Fluents are the changeable attributes of objects. Tasks are goal-oriented human activities which interact with objects and aim to change some attributes of the objects. The process of executing a task is a process to change the object fluents over time. We propose a hierarchical model to represent tasks as concurrent and sequential object fluents. In a task, different fluents closely interact with each other both in spatial and temporal domains. Given an egocentric video, a beam search algorithm is applied to jointly recognizing the object fluents in each frame, and the task of the entire video. We collected a large scale egocentric video dataset of tasks and fluents. This dataset contains 14 categories of tasks, 25 object classes, 21 categories of object fluents, 809 video sequences, and approximately 333,000 video frames. The experimental results on this dataset prove the strength of our method.},
author = {Liu, Yang and Wei, Ping and Zhu, Song Chun},
doi = {10.1109/ICCV.2017.318},
file = {:home/philo/Documents/Mendeley Desktop/Liu, Wei, Zhu - 2017 - Jointly Recognizing Object Fluents and Tasks in Egocentric Videos.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
mendeley-groups = {State Detection},
pages = {2943--2951},
title = {{Jointly Recognizing Object Fluents and Tasks in Egocentric Videos}},
volume = {2017-Octob},
year = {2017}
}


@article{Ma2018,
abstract = {Human actions often involve complex interactions across several inter-related objects in the scene. However, existing approaches to fine-grained video understanding or visual relationship detection often rely on single object representation or pairwise object relationships. Furthermore, learning interactions across multiple objects in hundreds of frames for video is computationally infeasible and performance may suffer since a large combinatorial space has to be modeled. In this paper, we propose to efficiently learn higher-order interactions between arbitrary subgroups of objects for fine-grained video understanding. We demonstrate that modeling object interactions significantly improves accuracy for both action recognition and video captioning, while saving more than 3-times the computation over traditional pairwise relationships. The proposed method is validated on two large-scale datasets: Kinetics and ActivityNet Captions. Our SINet and SINet-Caption achieve state-of-the-art performances on both datasets even though the videos are sampled at a maximum of 1 FPS. To the best of our knowledge, this is the first work modeling object interactions on open domain large-scale video datasets, and we additionally model higher-order object interactions which improves the performance with low computational costs.},
archivePrefix = {arXiv},
arxivId = {1711.06330},
author = {Ma, Chih Yao and Kadav, Asim and Melvin, Iain and Kira, Zsolt and Alregib, Ghassan and Graf, Hans Peter},
doi = {10.1109/CVPR.2018.00710},
eprint = {1711.06330},
file = {:home/philo/Documents/Mendeley Desktop/Ma et al. - 2018 - Attend and Interact Higher-Order Object Interactions for Video Understanding.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Action Recognition/Low},
pages = {6790--6800},
title = {{Attend and Interact: Higher-Order Object Interactions for Video Understanding}},
year = {2018}
}


article{zhang2019integrating,
  title={Integrating semantic knowledge to tackle zero-shot text classification},
  author={Zhang, Jingqing and Lertvittayakumjorn, Piyawat and Guo, Yike},
  journal={arXiv preprint arXiv:1903.12626},
  year={2019}
}

@inproceedings{lee2018multi,
  title={Multi-label zero-shot learning with structured knowledge graphs},
  author={Lee, Chung-Wei and Fang, Wei and Yeh, Chih-Kuan and Wang, Yu-Chiang Frank},
  booktitle={Proceedings of the IEEE CVPR},
  pages={1576--1585},
  year={2018}
}

@inproceedings{fathi2013modeling,
  title={Modeling actions through state changes},
  author={Fathi, Alireza and Rehg, James M},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2579--2586},
  year={2013}
}

inproceedings{alayrac2017joint,
  title={Joint Discovery of Object States and Manipulation Actions},
  author={Alayrac, Jean-Baptiste and Sivic, Josef and Laptev, Ivan and Lacoste-Julien, Simon},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  pages={407--416},
  year={2017}
}
@article{manousaki2024anticipating,
  title={Anticipating Object State Changes},
  author={Manousaki, Victoria and Bacharidis, Konstantinos and Gouidis, Filippos and Papoutsakis, Konstantinos and Plexousakis, Dimitris and Argyros, Antonis},
  journal={arXiv preprint arXiv:2405.12789},
  year={2024}
}

@article{yun2022graph,
  title={Graph Transformer Networks: Learning meta-path graphs to improve GNNs},
  author={Yun, Seongjun and Jeong, Minbyul and Yoo, Sungdong and Lee, Seunghun and Sean, S Yi and Kim, Raehyun and Kang, Jaewoo and Kim, Hyunwoo J},
  journal={Neural Networks},
  volume={153},
  pages={104--119},
  year={2022},
  publisher={Elsevier}
}

@article{liu2023meta,
  title={Meta-HGT: Metapath-aware HyperGraph Transformer for heterogeneous information network embedding},
  author={Liu, Jie and Song, Lingyun and Wang, Guangtao and Shang, Xuequn},
  journal={Neural Networks},
  volume={157},
  pages={65--76},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{fu2015zero,
  title={Zero-shot object recognition by semantic manifold distance},
  author={Fu, Zhenyong and Xiang, Tao and Kodirov, Elyor and Gong, Shaogang},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2635--2644},
  year={2015}
}
