\documentclass[11pt, a4paper]{article}
% \documentclass[12pt, a4paper, draft]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsfonts, amsmath, amssymb, bm}
\usepackage{tabu}
\usepackage{enumerate}
\usepackage{tgcursor}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{listings}
\usepackage{stackengine}
\usepackage{caption}
\usepackage{subcaption}
% \usepackage{mathptmx}
\usepackage{setspace}
\usepackage{footnote}
\usepackage{comment}
\usepackage{footmisc}
\usepackage{hyperref}
% \numberwithin{equation}{section}
\usepackage{eucal}
\usepackage{mathrsfs}
\usepackage{natbib}
\usepackage{authblk}
\usepackage[mathlines]{lineno} % line numbers, https://tex.stackexchange.com/questions/461186/how-to-use-lineno-with-
\usepackage{amsthm}


\usepackage{enumitem}

\def\arraystretch{1.1}
\doublespacing



%%%%%%%%%%%%%%%%%%%%%%%% to number equations
%% Patch 'normal' math environments:
\newcommand*\linenomathpatch[1]{%
  \cspreto{#1}{\linenomath}%
  \cspreto{#1*}{\linenomath}%
  \csappto{end#1}{\endlinenomath}%
  \csappto{end#1*}{\endlinenomath}%
}
%% Patch AMS math environments:
\newcommand*\linenomathpatchAMS[1]{%
  \cspreto{#1}{\linenomathAMS}%
  \cspreto{#1*}{\linenomathAMS}%
  \csappto{end#1}{\endlinenomath}%
  \csappto{end#1*}{\endlinenomath}%
}

%% Definition of \linenomathAMS depends on whether the mathlines option is provided
\expandafter\ifx\linenomath\linenomathWithnumbers
  \let\linenomathAMS\linenomathWithnumbers
  %% The following line gets rid of an extra line numbers at the bottom:
  \patchcmd\linenomathAMS{\advance\postdisplaypenalty\linenopenalty}{}{}{}
\else
  \let\linenomathAMS\linenomathNonumbers
\fi

\linenomathpatch{equation}
\linenomathpatchAMS{gather}
\linenomathpatchAMS{multline}
\linenomathpatchAMS{align}
\linenomathpatchAMS{alignat}
\linenomathpatchAMS{flalign}
%%%%%%%%%%%%%%%%%%%%%%%% to number equations


\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}



%%%%%%%% Functions
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\etr}[1]{\text{etr}\left\{{#1}\right\}}

\newcommand{\trp}{\,\prime}

%%%%%%%% Bold Letters
\newcommand{\va}{\mathbf{a}}
\newcommand{\vA}{\mathbf{A}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vB}{\mathbf{B}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vC}{\mathbf{C}}
\newcommand{\vd}{\mathbf{d}}
\newcommand{\vD}{\mathbf{D}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vE}{\mathbf{E}}
\newcommand{\veh}{\hat{\mathbf{e}}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vF}{\mathbf{F}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vG}{\mathbf{G}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vH}{\mathbf{H}}
\newcommand{\vi}{\mathbf{i}}
\newcommand{\vI}{\mathbf{I}}
\newcommand{\vj}{\mathbf{j}}
\newcommand{\vJ}{\mathbf{J}}
\newcommand{\vk}{\mathbf{k}}
\newcommand{\vK}{\mathbf{K}}
\newcommand{\vl}{\mathbf{l}}
\newcommand{\vL}{\mathbf{L}}
\newcommand{\vm}{\mathbf{m}}
\newcommand{\vM}{\mathbf{M}}
\newcommand{\vn}{\mathbf{n}}
\newcommand{\vN}{\mathbf{N}}
\newcommand{\vo}{\mathbf{o}}
\newcommand{\vO}{\mathbf{O}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vP}{\mathbf{P}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vQ}{\mathbf{Q}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vR}{\mathbf{R}}
\newcommand{\vs}{\mathbf{s}}
\newcommand{\vS}{\mathbf{S}}
\newcommand{\vt}{\mathbf{t}}
\newcommand{\vT}{\mathbf{T}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vU}{\mathbf{U}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vV}{\mathbf{V}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vW}{\mathbf{W}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vX}{\mathbf{X}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vyh}{\hat{\mathbf{y}}}
\newcommand{\vY}{\mathbf{Y}}
\newcommand{\vYh}{\hat{\mathbf{Y}}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vZ}{\mathbf{Z}}

%%%%%%%% Statistical Functions
\newcommand{\var}{\text{var}}
\newcommand{\cov}{\text{cov}}

%%%%%%%% Bold Greek Letters
\newcommand{\valpha}{\boldsymbol{\alpha}}
\newcommand{\vAlpha}{\boldsymbol{\Alpha}}
\newcommand{\vbeta}{\boldsymbol{\beta}}
\newcommand{\vBeta}{\boldsymbol{\Beta}}
\newcommand{\vchi}{\boldsymbol{\chi}}
\newcommand{\vChi}{\boldsymbol{\Chi}}
\newcommand{\vdelta}{\boldsymbol{\delta}}
\newcommand{\vDelta}{\boldsymbol{\Delta}}
\newcommand{\vepsilon}{\boldsymbol{\epsilon}}
\newcommand{\vEpsilon}{\boldsymbol{\Epsilon}}
\newcommand{\veta}{\boldsymbol{\eta}}
\newcommand{\vEta}{\boldsymbol{\Eta}}
\newcommand{\vgamma}{\boldsymbol{\gamma}}
\newcommand{\vGamma}{\boldsymbol{\Gamma}}
\newcommand{\viota}{\boldsymbol{\iota}}
\newcommand{\vIota}{\boldsymbol{\Iota}}
\newcommand{\vkappa}{\boldsymbol{\kappa}}
\newcommand{\vKappa}{\boldsymbol{\Kappa}}
\newcommand{\vlambda}{\boldsymbol{\lambda}}
\newcommand{\vLambda}{\boldsymbol{\Lambda}}
\newcommand{\vmu}{\boldsymbol{\mu}}
\newcommand{\vMu}{\boldsymbol{\Mu}}
\newcommand{\vnu}{\boldsymbol{\nu}}
\newcommand{\vNu}{\boldsymbol{\Nu}}
\newcommand{\vomega}{\boldsymbol{\omega}}
\newcommand{\vOmega}{\boldsymbol{\Omega}}
\newcommand{\vphi}{\boldsymbol{\phi}}
\newcommand{\vPhi}{\boldsymbol{\Phi}}
\newcommand{\vpi}{\boldsymbol{\pi}}
\newcommand{\vPi}{\boldsymbol{\Pi}}
\newcommand{\vpsi}{\boldsymbol{\psi}}
\newcommand{\vPsi}{\boldsymbol{\Psi}}
\newcommand{\vrho}{\boldsymbol{\rho}}
\newcommand{\vRho}{\boldsymbol{\Rho}}
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\newcommand{\vSigma}{\boldsymbol{\Sigma}}
\newcommand{\vtau}{\boldsymbol{\tau}}
\newcommand{\vTau}{\boldsymbol{\Tau}}
\newcommand{\vtheta}{\boldsymbol{\theta}}
\newcommand{\vTheta}{\boldsymbol{\Theta}}
\newcommand{\vupsilon}{\boldsymbol{\upsilon}}
\newcommand{\vUpsilon}{\boldsymbol{\Upsilon}}
\newcommand{\vxi}{\boldsymbol{\xi}}
\newcommand{\vXi}{\boldsymbol{\Xi}}
\newcommand{\vzeta}{\boldsymbol{\zeta}}
\newcommand{\vZeta}{\boldsymbol{\Zeta}}

% bold variation Greek letters
\newcommand{\vvepsilon}{\boldsymbol{\varepsilon}}
\newcommand{\vvkappa}{\boldsymbol{\varkappa}}
\newcommand{\vvphi}{\boldsymbol{\varphi}}
\newcommand{\vvpi}{\boldsymbol{\varpi}}
\newcommand{\vvrho}{\boldsymbol{\varrho}}
\newcommand{\vvsigma}{\boldsymbol{\varsigma}}
\newcommand{\vvtheta}{\boldsymbol{\vartheta}}



\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Change title, author, and affilitaion text
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand\Authfont{\normalfont}
\renewcommand\Affilfont{\itshape\small}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Title
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\title{\vspace{-2cm}\LARGE\centering\normalfont{A Flexible Prior for Orthonormal Matrices}}
% Key terms for the title: prior, orthonormal, structured, uncertainty quant, fast
\title{\vspace{-2cm}\LARGE\centering\normalfont{A flexible class of priors for conducting posterior inference on structured orthonormal matrices}}
\date{\vspace{-10mm}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Authors
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author[1,*]{Joshua S. North}
\author[1]{Mark D. Risser}
\author[2]{F. Jay Breidt}
\affil[1]{Climate and Ecosystem Sciences Division, Lawrence Berkeley National Laboratory, 1 Cyclotron Road, Berkeley, CA 94720}
\affil[2]{Department of Statistics and Data Science, NORC at the University of Chicago}

\affil[*]{Corresponding author: jsnorth@lbl.gov}

\begin{document}

\maketitle

% \linenumbers

\begin{abstract}
%-----------------------------------------------------
% Super helpful resource for writing good abstracts: 
% https://www.nature.com/documents/nature-summary-paragraph.pdf
%-----------------------------------------------------
% One or two sentences providing a basic introduction to the field,  comprehensible to a scientist in any discipline. 
The big data era of science and technology motivates statistical modeling of matrix-valued data using a low-rank representation that simultaneously summarizes key characteristics of the data and enables dimension reduction for data compression and storage.
% Two to three sentences of more detailed background, comprehensible to scientists in related disciplines.
Low-rank representations such as singular value decomposition factor the original data into the product of orthonormal basis functions and weights, where each basis function represents an independent feature of the data.
However, the basis functions in these factorizations are typically computed using algorithmic methods that cannot quantify uncertainty or account for explicit structure beyond what is implicitly specified via data correlation. 
% One sentence summarizing the main result (with the words “here we show” or their equivalent).
We propose a flexible prior distribution for orthonormal matrices that can explicitly model structure in the basis functions.
% Two or three sentences explaining what the main result reveals in direct comparison to what was thought to be the case previously, or how the main result adds to previous knowledge.
The prior is used within a general probabilistic model for singular value decomposition to conduct posterior inference on the basis functions while accounting for measurement error and fixed effects.
To contextualize the proposed prior and model, we discuss how the prior specification can be used for various scenarios and relate the model to its deterministic counterpart.
We demonstrate favorable model properties through synthetic data examples and apply our method to sea surface temperature data from the northern Pacific, enhancing our understanding of the ocean's internal variability.

    \begin{center}
        \textit{Key Words: Bayesian Singular Value Decomposition, Probabilistic Low-Rank Representation, Probabilistic Basis Functions, Stiefel Manifold, Spatio-Temporal Random Effect}
    \end{center}
    
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%===================================
% Point-of-paragraph:
% Orthonormal matrices in statistics -- what are they and their general importance 
%===================================
% Things to cover: directional statistics, multivariate analysis, matrix decompositions, data compression
Orthonormal matrices are an important aspect of multivariate statistics \citep{Hammarling1985}, directional statistics \citep{Pewsey2021}, and matrix decomposition methods \citep{VanDerMaaten2009}.
The set of orthonormal matrices $\mathcal{V}_{k,n}=\{\vY\in\mathbb{R}^{n\times k}:\vY' \vY = \vI_k\}$,  where $\vI_k$ is the $k \times k$ identity matrix, is called the Stiefel manifold \citep{Chikuse2003}.
Due to the importance of orthonormal matrices in statistical analysis, considerable effort has been put into understanding theoretical properties associated with their distributions and optimal methods for computation and sampling \citep{Mardia1999, Chikuse2003, Hoff2007, Hoff2009, Byrne2013, Wang2013, Wang2014, Hernandez-Stumpfhauser2017, Pourzanjani2021, Jauch2021}.
More specifically, being able to specify and sample from a distribution for a matrix with columns that are of unit length and orthogonal remains a challenging and important body of active research.



%===================================
% Point-of-paragraph:
% Use of orthonormal matrices as low-rank process for statistics specifically
%===================================
Within the field of statistics, orthonormal matrices are used for a variety of modeling purposes, including exploratory data analysis, factor analysis \citep{Harman1976, Mulaik2009}, principal component analysis \citep[PCA;][]{Hotelling1933, Jolliffe2002}, singular value decomponsition \citep[SVD;][]{Stewart1993}, proper orthogonal decomposition \citep[POD;][]{Berkooz1993}, and spatially-indexed basis functions (see \citealt{Cres} and \citealt{Wikle2019} and references therein).
In general, any stochastic process can be represented as the infinite linear expansion of orthogonal basis functions and random variables \citep{Daw2022}.
In practice, it is impossible to work with an infinite sum of basis functions, and so the summation is truncated at some specified value, say $k$.
Among all basis function representations, the Karhunen-Lo\'eve expansion (KLE) has been shown to be optimal in that it minimizes the mean squared error between the truncation and the infinite expansion \citep{Karhunen1946, Loeve1955}.
In regard to orthonormal matrices in statistics, the KLE has been shown to be closely related, or equivalent under certain conditions, to SVD and PCA \citep{Gerbrands1981, Daw2022}.



%===================================
% Point-of-paragraph:
% Discuss how SVD and PCA fit in
%===================================
Both PCA and SVD involve similar matrix factorizations that produce low-rank representations of data.
PCA is a technique of summarizing data by projecting it on a new coordinate system where the first coordinate describes the most variation in the data, the second coordinate is orthogonal to the first and describes the second most variation, and so on.
More formally, for mean-zero $\vY \in \mathbb{R}^{n \times m}$, PCA decomposes $\vY \vY' = \vA \vB \vA'$ where $\vA \in \mathbb{R}^{n \times l}$ is an orthonormal matrix (i.e., $\vA \vA' = \vI_n$) whose columns are the eigenvectors of $\vY \vY'$, $\vB \in \mathbb{R}^{l \times l}$ is a diagonal matrix whose elements are the eigenvalues of $\vY \vY'$, and $l = \text{min}\{n, m\}$.
Alternatively, SVD generalizes the eigendecomposition of a matrix by decomposing $\vY = \vU \vD \vV'$ where $\vU \in \mathbb{R}^{n \times l}$ is an orthonormal matrix, $\vD \in \mathbb{R}^{l \times l}$ is a diagonal matrix, $\vV \in \mathbb{R}^{m \times l}$ is an orthonormal matrix, and $l = \text{min}\{n, m\}$.
The equivalence between SVD and PCA comes from $ \vY \vY'=\left(\vU \vD \vV'\right)\left( \vV \vD' \vU'\right)=\vU \vD \vD' \vU'$,
resulting in $\vU \equiv \vA$ and $\vB \equiv \vD \vD'$, where $\vD$ can be recognized as the square root of the eigenvalues of $\vY \vY'$ and the columns of $\vU$ are the eigenvectors of $\vY \vY'$.


%===================================
% Point-of-paragraph:
% Focus on matrix decomposition uses in all of science
%===================================
More generally, for the broader class of matrix decomposition methods (of which PCA and SVD are special cases), orthonormal matrices are commonly used to represent matrix-valued data using a low-rank process.
The low-rank process decomposes the original data as a product of basis functions ($\vU$) and weights ($\vD\vV'$), where each basis function represents a different feature, or summary of multiple features, of the data.
For both PCA and SVD, the low-rank representation can be obtained by letting $k < l$ (typically $k \ll l$) where $k$ is some pre-specified value.
This is crucial for ``big-data'' applications because the low-rank representation provides an important foundation for summarizing key characteristics of the data as well as dimension reduction \citep{Kambhatla1997, Wikle2010} and data compression \citep{Yu2019, Chen2022}.
Across many areas of science, technology, and medicine, orthonormal matrices and their corresponding low-rank representations are highly useful because the measurements of interest in these fields commonly arise from lower-dimensional processes that have physically interpretable structures.
For example, factor analysis in physiological studies \citep{FORD1986, Fabrigar1999}, PCA in geography \citep{Demsar2013, Roden2015} and ecology \citep{Jackson1993, Jackson2004, Peres-Neto2003}, and SVD and PCA for medical imaging \citep{Mauldin2011, Smith2014}.
%===================================
% Point-of-paragraph:
% Focus on orthonormal matrices for climate
%===================================
% Within the climate sciences, o
In climate sciences, orthonormal matrices play a major role  through the use of empirical orthogonal functions \citep[EOFs;][]{Lorenz1956, North1982, Hannachi2007}, which are analogous to PCA.
EOFs are used to compute indices that summarize modes of climate variability, e.g., the Pacific Decadal Oscillation \citep[PDO;][]{Mantua2002} and the Arctic Oscillation \citep[][]{Thompson2000}. 
EOFs are also used to identify large-scale meteorological patterns that drive extreme weather and climate events \citep{Grotjahn2016} as well as quantify human-induced changes to natural variability in the global climate system \citep{OBrien2023}.


%===================================
% Point-of-paragraph:
% Traditional approaches to PCA/SVD do not impose structure; do no quantify uncertainty
%===================================
SVD and PCA are useful in statistical modeling because they can incorporate spatial (or temporal, depending on the orientation of the data) information into the model through spatially-indexed basis functions and spatial random effects \citep{Stroud2001, Nychka2002, Cressie2006, Cressie2008}.
Traditionally, SVD is computed through an iterative procedure \citep{Golub1965, Demmel1990}, which we refer to as classical SVD (C-SVD or C-PCA) henceforth.
These classical procedures have several important limitations.
First, when $n$ is large with respect to $m$, the basis functions estimated from C-SVD can be noisy and therefore lose their physical interpretation \citep{Wang2017}.
C-SVD and C-PCA are not able to distinguish between measurement and signal variation, which means that estimates of the basis functions are heavily influenced by the presence of measurement noise \citep{Bailey2012, Epps2019}.
Second, since their algorithms are deterministic, C-PCA and C-SVD do not provide measures of uncertainty in either the basis functions or their weights.
Finally, the estimated basis functions only exhibit dependence or structure implicitly via data correlations since C-PCA and C-SVD cannot take advantage of explicit structure that may be present in the data generating mechanisms. 

%===================================
% Point-of-paragraph:
% Summarize approaches that impose spatial structure
%===================================
% Give a bit more detail than you have right now.
To address the issue of noise, large $n$ with small $m$, and structure in the basis functions, a regularized PCA approach can be adopted \citep{Shen2008, Zou2006, Jolliffe2002b}.
However, regularization may produce either non-orthogonal bases or isolated areas of zero and nonzero components (i.e., not spatially dependent).
\citet{Wang2017} extend the regularization approach by incorporating smoothness and local features into their penalization using smoothing splines and an $\ell_1$ penalty, producing spatially explicit orthogonal basis functions.
While a viable modeling choice, the penalized PCA basis functions are deterministic and do not provide a measure of uncertainty quantification.

%===================================
% Point-of-paragraph:
% Summarize approaches that quantify uncertainty
%===================================
To account for uncertainty quantification in the basis function, one possibility is to take a Bayesian approach and specify a prior distribution for the orthonormal matrices.
This requires a suitable prior for the basis functions (i.e., preserving the orthonormal requirement) and then sampling from the posterior distribution to obtain distributional properties for the basis functions.
\citet{Hoff2007} developed a uniform prior for orthonormal basis functions (the invariant or uniform measure on the Steifel manifold) that enables the specification of a Bayesian SVD model, and showed how to sample from the full conditional distributions of the model.
However, the approach in \citet{Hoff2007} requires sampling from the von Mises-Fisher (or Bingam-von Mises-Fisher) distributions, which can be difficult, and does not allow for the basis functions to be structured.
Additionally, support for these distributions in probabilistic programming languages such as Stan is limited \citep{Carpenter2017}, providing yet another barrier for implementation.
Work has been done to make sampling from these densities tractable \citep{Hoff2009, Byrne2013}, but this requires the underlying statistical model to abide by specific conditions and/or forms, limiting the application areas.
To overcome some of the sampling difficulty issues and increase the flexibility of application problems, recent work by \citet{Pourzanjani2021} and \citet{Jauch2021} has focused on the computational aspects of distributions on the Stiefel manifold.
Specifically, they simulate unconstrained random vectors (i.e., not orthogonal and not unit-length) and then transform these draws to be orthonormal via an appropriate Jacobian.
This approach is convenient because the transformation can be incorporated into probabilistic programming languages and used for a variety of modeling applications, including the incorporation of structure into the basis functions.


%===================================
% Point-of-paragraph:
% What are we doing that's new
%===================================
Here, we develop a prior distribution for orthonormal matrices that is conjugate with the normal distribution and construct a probabilistic model for SVD.
The resulting full conditional distributions for the basis functions are available in closed form, yielding analytically straightforward posterior sampling for the orthonormal basis functions, and we furthermore discuss how the prior can be used for a variety of modeling purposes.
Additionally, our prior is in general not uniformly distributed on $\mathcal{V}_{k,n}$ (although the uniform distribution is a special case) and we are able to impart information into the prior through our specification of a covariance matrix.
We show how the covariance matrix can be specified to either impart smoothing onto the basis functions \citep[producing results similar to][]{Wang2017} or recover the prior developed by \citet{Hoff2007}, and also demonstrate how the mean of the full conditional distributions for the basis functions of our probabilistic SVD model coincides with the classical approach (C-SVD) under certain conditions.
Our resulting prior, along with the proposed Bayesian hierarchical model, is quite general and allows each basis function to have a unique dependence structure that is learned from the data, which has not been previously possible.

While our approach to generalizing \citet{Hoff2007} to allow for dependently structured orthonormal matrices is similar in flavor to the approach developed by \citet{Jauch2021}, there are two notable differences.
First, our prior distribution is formulated conditionally on a basis function by basis function case, resulting in a distribution on the Stiefel manifold rather than a general method for sampling from distributions on the Stiefel manifold.
Specifically, the prior distribution we develop includes an explicit orthonormality constraint, while \citet{Jauch2021} and \citet{Pourzanjani2021} sample from distributions on an unconstrained space and use the polar or Givens transformation, respectively, to obtain samples on the constrained space.
Second, we are able to estimate a unique dependence structure for each basis function due to the underlying difference in distribution, whereas the approach in \citet{Pourzanjani2021} does not consider dependence and the approach in \citet{Jauch2021} can only impose a single dependence structure on all basis functions.

%===================================
% Point-of-paragraph:
% Paper organization
%===================================
The remainder of the manuscript is organized as follows. 
Section~\ref{sec:Prior} develops the prior distribution for orthonormal matrices.
Section~\ref{sec:GPM} proposes a general probabilistic model for matrix factorizations with a specific focus on SVD and then expands other possible modeling choices.
A simulation study is conducted in section~\ref{sec:simulations}, where we show the impact of the covariance specification across varying degrees of structure in the data generating mechanism and signal-to-noise ratios.
We present a real-world application Section~\ref{sec:SST}, applying our probabilistic model for SVD to reconstruct the PDO index from sea surface temperature measurements from the north Pacific and provide uncertainty bounds for the PDO index, allowing for us to better understand the strength of the PDO signal.
Section~\ref{sec:discussion} concludes the paper.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A flexible class of priors for orthonormal matrices}\label{sec:Prior}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Here, we construct a prior distribution for matrices on the Stiefel manifold $\mathcal{V}_{k,n}$ that is conjugate with a normal likelihood model.
The prior is constructed from the projected normal distribution (see \citealt{Wang2013, Wang2014} and \citealt{Hernandez-Stumpfhauser2017} and the references therein for details on the Projected Normal) where each column is restricted to be of unit length and orthogonal to the other columns.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generating mechanism}\label{sec:GeneratingMechanism}

One method of drawing an orthonormal matrix from the uniform distribution on $\mathcal{V}_{k,n}$ is outlined in the appendix of \citet{Hoff2007}.
As part of the construction, the underlying normal distribution from which the orthonormal matrix is generated specifies the identity matrix as the covariance, and the resulting distribution is uniform on $\mathcal{V}_{k,n}$.
Here, we extend this generating mechanism to allow for structure in its covariance, such that the prior implied by \cite{Hoff2007} is a special case.
By construction, the resulting distribution is not necessarily uniform on $\mathcal{V}_{k,n}$. 


For fixed $k$, let $\vz_i$ independent $ N_n(\vec{0}, \vOmega_i)$ for $i=1,2,\ldots,k$. Define $\vP_0=\vI_n$, $\vx_1=\vP_0\vz_1$, and 
\begin{align*}
    \vX_i= [\vx_1, ..., \vx_i], \quad \vP_i= \vI_n - \vX_i (\vX'_i \vX_i)^{-1} \vX'_i, \quad \vx_{i+1}=\vP_i\vz_{i+1}
\end{align*}
for $i=1,2,\ldots,k-1$.
Then $p(\vx_i|\vX_{i-1}) \sim N_n(\vec{0}, \vP_{i-1}\vOmega_i\vP'_{i-1})$ and $\vx'_i \vx_j = 0$ for $i \neq j$.
Further, define
\begin{align}\label{eqn:orthomatrix}
    \vw_i=\frac{\vx_i}{(\vx_i'\vx_i)^{1/2}},\quad \vW_i=[\vw_1,\ldots,\vw_i]
\end{align}
for $i=1,2,\ldots,k$.
By construction, $\vW_k \in \mathcal{V}_{k,n}$ is an orthonormal matrix.
The conditional distributions of each column given the preceding columns are $p(\vw_i |\vW_{i-1}, \vOmega_i) \sim PN_n(\vec{0}, \vP_{i-1}\vOmega_i\vP'_{i-1})$, where $PN_n(\cdot, \cdot)$ denotes the $n$-dimensional projected normal distribution \citep{Wang2013, Wang2014, Hernandez-Stumpfhauser2017}.

Let $\overset{d}{=}$ denote equality in distribution.
We now provide two key properties associated with the distribution of $\vW \equiv \vW_k$ based on the constructed matrix $\vX \equiv \vX_k$, with proofs deferred to appendix~\ref{sec:props}. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proposition}\label{prop:one}
    The matrix $\vW = \vX (\vX' \vX)^{-1/2}$ is invariant to right-orthogonal transformation. That is, for $\vH:\mathbb{R}^k \rightarrow \mathbb{R}^k$ an orthogonal transformation matrix, $\vW \vH \overset{d}{=} \vW$.
\end{proposition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proposition}\label{prop:two}
    $\vw_i|\vW_{i-1} \overset{d}{=} \vN_{i-1}\widetilde{\vw}_i|\vW_{i-1}$ where the columns of $\vN_{i-1}$ form an orthonormal basis for the null space of $\vW_{i-1}$ and $\widetilde{\vw}_i|\vW_{i-1} \sim PN_{n-i+1}(\vec{0}, \vN_{i-1}'\vOmega_i\vN_{i-1})$ is the projected weight function.
\end{proposition}
% \textbf{We should think of what to call $\widetilde{\vw}_i$. Maybe the projected column/projected basis function?}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Proposition~\ref{prop:one} implies the columns of $\vW$ are exchangeable, and therefore the conditional distribution $\vw_i|\vW_{Q}$ is invariant to the choice of subset of columns $Q \subset \{1, ..., k\}$.
When proposition~\ref{prop:one} is taken with proposition~\ref{prop:two}, the conditional distribution of $\vw_i|\vW_{Q}$ given any subset of columns $Q$ is equal in distribution to $\vN_{Q}\widetilde{\vw}_i$, where $\vN_{Q}$ is an orthonormal basis for the null space of $\vW_{Q}$ and $\widetilde{\vw}_i|\vW_{Q} \sim PN_{n-|Q|+1}(\vec{0}, \vN'_{Q}\vOmega_i\vN_{Q})$.
Therefore, we now focus on a prior distribution for $\widetilde{\vw}_i$, the \textit{projected weight function}, where $\widetilde{\vw}_i|\vW_{-i} \sim PN_{n-k+1}(\vec{0}, \vN'_{i}\vOmega_i\vN_{i})$ (i.e., $Q = \{1, ..., i-1, i+1, ..., k\}$) and the columns of $\vN_{i}$ span the null space of $\vW_{-i}$.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Orthonormal Matrix}\label{sec:OrthonormalMatrix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Projected normal prior distribution}\label{sec:PriorDistribution}

From the construction in Section~\ref{sec:GeneratingMechanism}, we have $\widetilde{\vw}_i|\vW_{-i} \sim PN_{n-k+1}(\vec{0}, \vN_{i}'\vOmega_i\vN_{i})$.
However, sampling from a high-dimensional projected normal distribution is difficult because of the form of the density function.
To make sampling from the projected normal tractable, we augment the distribution $\widetilde{\vw}_i|\vW_{-i}$ using a latent length variable $r_i$.
The joint distribution of $(r_i, \widetilde{\vw}_i) | \vW_{-i}$ can be derived by transforming the random variable $\vx_i$ to spherical coordinates (see supplement~\ref{sec:PND}), where the density function is
\begin{align*}
    p(r_i, \widetilde{\vw}_i|\vW_{-i}) = (2 \pi)^{-n^*/2}|\vN_{i}'\vOmega_i\vN_{i}|^{-1/2}\exp\left\{-\frac{1}{2}(r_i \widetilde{\vw}_i)'(\vN_{i}'\vOmega_i\vN_{i})^{-1}(r_i \widetilde{\vw}_i) \right\}r_i^{n^*-1}\mathbb{I}(\widetilde{\vw}_i \in \mathcal{V}_{1,n^*}),
\end{align*}
which we denote as $p(r_i, \widetilde{\vw}_i) \sim N_{n^*}(\vec{0}, \vN_{i}'\vOmega_i\vN_{i})r_i^{n^*-1}$ with $n^* = n-k+1$.
The indicator function $\mathbb{I}(\widetilde{\vw}_i \in \mathcal{V}_{1,n^*})$ is an integrating constant that is independent of the angle of $\widetilde{\vw}_i$ and dependent only on its length.
Note for $k=1$, the Stiefel manifold $\mathcal{V}_{1,n}$ is the $n-1$-dimensional unit sphere and $\mathcal{V}_{1,n} \equiv \mathbb{S}^{n-1}$.
The length variable $r_i$ can be sampled using either a slice sampler \citep{Hernandez-Stumpfhauser2017} or a Metropolis-Hastings algorithm.
However, we have found the slice sampler has numerical issues when $n$ is large, and use a Metropolis-Hastings within Gibbs algorithm (see supplement~\ref{sec:FCD}) for all examples presented herein.


The $PN$ prior is convenient because if the data distribution is normal, the resulting full conditional distribution is proportional to a normal, which is easy to sample from (see Section~\ref{sec:PSVD} and supplement~\ref{sec:FCD} for more detail).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Incorporating explicit structure into the prior}\label{sec:PriorStructure}


From the formulation of the prior, we have the ability to specify or estimate the covariance structure for the projected basis functions.
The non-informative choice is $\vOmega_i \propto \vI_n$, implying there is no dependence between the elements of the basis functions.
As discussed in the supplement (\ref{sec:IC}), when $\vOmega_i \equiv \vI$ the generating mechanism is equivalent to that proposed by \citet{Hoff2007}, resulting in $\widetilde{\vw}_i$ being distributed uniformly on the $(n-k+1)$-dimensional sphere and the prior being equivalent to \citet{Hoff2007}.


A more general choice is to model $\vOmega_i = \sigma^2_{i}\vC$, where $\vC$ is a non-diagonal positive-definite matrix that specifies structure in the basis functions.
In most cases, $\vC \equiv \vC(\theta)$ will depend on hyperparameters $\theta$ that can either be specified or learned within the hierarchical model.
Across many areas of science, including spatial statistics, machine learning, and emulation of complex physical models, the elements of $\vC$ are modeled via kernel functions $C_\theta(\cdot, \cdot)$ that are positive definite on the domain specified by the input space $\mathcal{S}$.
For example, when $\mathcal{S} \subset \mathbb{R}^d$, a popular choice is the Mat\'ern kernel
\begin{align}\label{eqn:matern}
    C_{\nu, \rho}(\vs, \vs') = \frac{2^{1-\nu}}{\Gamma(\nu)}\left(2\nu \frac{||\vs-\vs'||}{\rho}\right)^{\nu}J_{\nu}\left(2\nu \frac{||\vs-\vs'||}{\rho}\right),
\end{align}
defined for $\vs,\vs' \in \mathcal{S}$,
where $\Gamma$ is the gamma function, $J_{\nu}$ is the Bessel function of the second kind, and $\{\nu, \rho\}$ are hyperparameters that describe the differentiability and length-scale of the implied stochastic process, respectively. 
Special cases of the Mat\'ern kernel are for $\nu = 0.5$, in which (\ref{eqn:matern}) simplifies to the exponential kernel $C_{0.5, \rho}(\vs, \vs') = \exp\{-||\vs - \vs'||/\rho\}$, and the limit as $\nu\rightarrow\infty$, in which (\ref{eqn:matern}) reduces to the squared exponential or radial basis function kernel $C_{\infty, \rho}(\vs, \vs') = \exp\{-||\vs - \vs'||^2/\rho\}$ \citep[the latter of which is used in 90\% of Gaussian process applications;][]{Pilario2019}.
Kernel functions like the Mat\'ern are useful for modeling generic dependence because they are highly flexible, depend on only a few hyperparameters (each of which are interpretable), yield data-driven smoothing that can characterize nonlinear structures in the underlying data, and require minimal \textit{a priori} or subjective specification.
Furthermore, such kernel functions do not require offline tuning of bandwidth or regularization parameters \citep[as is needed in, e.g., smoothing splines; see][]{Wang2017} since these aspects of the kernel can be inferred from the data within the Bayesian hierarchical model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{General probabilistic model}\label{sec:GPM}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Define $\vZ \in \mathbb{R}^{n \times m}$ to be the observed data which is modeled as
\begin{align}\label{eqn:data}
    \vZ = \vM + \vY + \vA\vXi\vB,
\end{align}
where $\vM \in \mathbb{R}^{n \times m}$, $\vY \in \mathbb{R}^{n \times m}$, $\vSigma = \vA \vA' \in \mathbb{R}^{n \times n}$, $\vPhi = \vB \vB' \in \mathbb{R}^{m \times m}$, and $\vXi = [\vxi_1, ..., \vxi_m]$ with $\vxi_i$ independent $N_n(0, \vI_m)$ for $i = 1, ..., m$.
This is succinctly written $\vZ|\vM, \vY, \vSigma, \vPhi \sim \mbox{MN}_{n \times m}(\vM + \vY, \vSigma, \vPhi)$ where $\mbox{MN}$ is the matrix normal distribution, $\vM + \vY$ is the mean of $\vZ$, $\vSigma$ is the covariance matrix for the rows of $\vZ$, $\vPhi$ is the covariance matrix for the columns of $\vZ$, and the density function is
\begin{align*}%\label{eqn:MVN}
    p(\vZ|\vM, \vY, \vSigma, \vPhi) = \frac{1}{(2\pi)^{nm/2}|\vPhi|^{n/2}|\vSigma|^{m/2}}\exp\left\{-\frac{1}{2} \text{tr}\left[ \vPhi^{-1}(\vZ - \vM - \vY)'\vSigma^{-1}(\vZ - \vM - \vY) \right] \right\}.
\end{align*}
Equation (\ref{eqn:data}) is a mixed-effects model, where $\vM$ is a fixed-effect mean structure that is dependent on observed covariates, which we discuss in Section~\ref{sec:LinearTrend}, and $\vY$ is a ``smooth'' random-effect that we will represent using basis functions and weights.
Generally, we assume $\vY$ is a mean zero random-effect and explains any discrepancy in $\vZ$ not explained by $\vM$.
For example, if $\vZ$ is oriented such that the rows index spatial locations and the columns index temporal observations (or replications), $\vY$ would be considered spatial random effects.
We now specify a non-parametric model for the random effects $\vY$ using singular value decomposition and the prior distribution proposed in Section~\ref{sec:PriorDistribution}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A probabilistic model for singular value decomposition}\label{sec:PSVD}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For now, we assume the mean of $\vZ$ is zero (i.e., $\vM = \vec{0}$) and focus on a model for $\vY$.
In models such as (\ref{eqn:data}), the process $\vY$ can be represented as a reduced-rank process.
One such example of a reduced-rank model is that of singular value decomposition (SVD), where $\vY = \vU \vD \vV'$ with $\vU \in \mathbb{R}^{n \times l}$ is an orthonormal matrix (i.e., $\vU' \vU = \vI_l$), $\vD \in \mathbb{R}^{l \times l}$ is a diagonally structured matrix, $\vV \in \mathbb{R}^{m \times l}$ is an orthonormal matrix, and $l = \text{min}\{n, m\}$.
As discussed in the introduction, to reduce the dimension of the process, take $k < l$ (typically $k \ll l$) where $k$ is some pre-specified value.
This results in $\vY \approx \vU \vD \vV'$, where now $\vU \in \mathbb{R}^{n \times k}$, $\vD \in \mathbb{R}^{k \times k}$, and $\vV \in \mathbb{R}^{m \times k}$ are of a reduced dimension.
In traditional SVD (similarly in PCA), the amount of variance explained by each component can be used to inform the value of $k$.
For now, we will assume $k$ is fixed and refer the reader to the Section~\ref{sec:ModelSpecifications} for further discussion.


In (\ref{eqn:data}), $\vPhi = \vB \vB'$ represents the covariance between replicate observations (columns).
We make the simplifying assumption $\vPhi = \vI_m$ (i.e., independence between replicates) and model all variation in the data through $\vSigma$, which represents the covariance between observations (rows).
The resulting probability model is $\vZ \sim MN_{n \times m}(\vU \vD \vV', \vSigma, \vI_m)$, where $\vSigma$ now accounts for the approximation of choosing $k \ll l$ and the density function is
\begin{align}\label{eqn:SVD_data}
    p(\vZ|\vU, \vD, \vV, \vSigma, \vI_m) = \frac{1}{(2\pi)^{nm/2}|\vSigma|^{m/2}}\exp\left\{-\frac{1}{2} \text{tr}\left[(\vZ - \vU \vD \vV')'\vSigma^{-1}(\vZ - \vU \vD \vV') \right] \right\}.
\end{align}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Model priors}

To complete our model specification, we assign priors to $\vU, \vD, \vV,$ and $\vSigma$, and estimate the model parameters using Bayesian techniques.
In order to use the prior developed in Section~\ref{sec:PriorDistribution}, we need conditional distributions of the form $[\vu_i|\vU_{-i}, \cdot]$ and $[\vv_i|\vV_{-i}, \cdot]$, which will result in estimating each column of $\vU$ and $\vV$ and each element of $\vD$ individually.
Define $\vU_{-i} \equiv [\vu_1, ..., \vu_{i-1}, \vu_{i+1}, ..., \vu_k]$, $\vV_{-i} \equiv [\vv_1, ..., \vv_{i-1}, \vv_{i+1}, ..., \vv_k]$, $\vD_{-i} \equiv \text{diag}(d_1, ..., d_{i-1}, d_{i+1}, ..., d_{k})$, and $\vE_{-i} \equiv \vZ - \vU_{-i}\vD_{-i}\vV'_{-i}$, so that $\vZ - \vU\vD\vV' = \vE_{-i} - d_i\vu_i\vv'_i$.
Factoring the trace of the exponent of (\ref{eqn:SVD_data}),
\begin{align*}
    \text{tr}[(\vZ - \vU\vD\vV')'\vSigma^{-1}(\vZ - \vU\vD\vV')] & = \text{tr}[(\vE_{-i} - d_i\vu_i\vv'_i)'\vSigma^{-1}(\vE_{-i} - d_i\vu_i\vv'_i)] \\
            & = \text{tr}[\vE'_{-i}\vSigma^{-1}\vE_{-i} - 2d_i\vv_i\vu'_i\vSigma^{-1}\vE_{-i} + d^2_i\vv_i\vu'_i\vSigma^{-1}\vu_i\vv'_i] \\
            & = \text{tr}[\vE'_{-i}\vSigma^{-1}\vE_{-i} - 2d_i\vu'_i\vSigma^{-1}\vE_{-i}\vv_i + d^2_i\vv'_i\vv_i\vu'_i\vSigma^{-1}\vu_i] \\
            & = \text{tr}[\vE'_{-i}\vSigma^{-1}\vE_{-i} - 2d_i\vu'_i\vSigma^{-1}\vE_{-i}\vv_i + d^2_i\vu'_i\vSigma^{-1}\vu_i],
\end{align*}
where the second to last equality holds by properties of the trace operator.

Using this reformulation of the exponent, the distribution $\vZ \sim MN_{n \times m}(\vU \vD \vV', \vSigma, \vI_m)$ can be written
\begin{align}\label{eqn:SVDfactored}
    p(\vZ|\vu_i, \vv_i, d_i, \vU_{-i}, \vD_{-i}, \vV_{-i}, \vSigma) = & \\ \frac{1}{(2\pi)^{nm/2}|\vSigma|^{m/2}}\exp\biggl\{-\frac{1}{2} \text{tr}\Bigl[&\vE'_{-i}\vSigma^{-1}\vE_{-i} - 2d_i\vu'_i\vSigma^{-1}\vE_{-i}\vv_i + d^2_i\vu'_i\vSigma^{-1}\vu_i\Bigr] \biggr\} \nonumber,
\end{align}
which enables inference on the columns of $\vU$ and $\vV$ and the elements of $\vD$ individually (e.g., inference on $\vu_i$ and $\vv_i$).
Recall from Section~\ref{sec:PriorDistribution} that $\vu_i|\vU_{-i} \overset{d}{=} \vN^u_{i} \widetilde{\vu}_i$ and $\vv_i|\vV_{-i} \overset{d}{=} \vN^v_{i} \widetilde{\vv}_i$ where the columns of $\vN^u_{i}$ and $\vN^v_{i}$ span the null space of $\vU_{-i}$ and $\vV_{-i}$, respectively.
We specify the prior distributions
\begin{align}\label{eqn:SVDpriors}
\begin{split}
    d_{i}\widetilde{\vu}_i|\vU_{-i} & \sim N_{n-k+1}(\vec{0}, \vN^{u \trp}_{i}\vOmega^u_i\vN^u_{i})d_{i}^{n-k}\mathbb{I}(\widetilde{\vu}_i \in \mathcal{V}_{1,n-k+1}) \\
    d_{i}\widetilde{\vv}_i|\vV_{-i} & \sim N_{m-k+1}(\vec{0}, \vN^{v \trp}_{i}\vOmega^v_i\vN^v_{i})d_{i}^{m-k} \mathbb{I}(\widetilde{\vv}_i \in \mathcal{V}_{1,m-k+1}) \\
    d_i & \sim U(0, \infty).
\end{split}
\end{align}
For simplicity, we assume $\vSigma = \vsigma^2\vI_n$, but this simplification can be relaxed if desired, e.g., by allowing $\vSigma$ to be a structured non-diagonal covariance matrix.
Last, we specify $\vOmega^u_i = \sigma^2_{u,i}\vC_u(\theta_{u,i})$ and $\vOmega^v_i = \sigma^2_{v,i}\vC_v(\theta_{v,i})$ where $\vC_u(\theta_{u,i})$ and $\vC_v(\theta_{v,i})$ are valid correlation matrices (i.e., the matrices are positive semi-definite; see Section~\ref{sec:PriorStructure}) and $\sigma_{u,i}^2$ and $\sigma_{v,i}^2$ are variance parameters.
For $\vsigma^2, \sigma_{u,i}^2$ and $\sigma_{v,i}^2$ we assign the non-informative half-t prior on the standard deviation as proposed by \citet{Huang2013}; specifically $\sigma \sim \text{\textit{Half-t}}(1, A)$, $\sigma_{u,i} \sim \text{\textit{Half-t}}(1, A_{u,i})$ and $\sigma_{v,i} \sim \text{\textit{Half-t}}(1, A_{v,i})$.


One major benefit of our proposed prior is now realized: the full conditional distribution of $\widetilde{\vu}_i$ and $\widetilde{\vv}_i$ is proportional to a normal distribution (see supplement~\ref{sec:FCD}).
This results in a Gibbs update step for both $\widetilde{\vu}_i$ and $\widetilde{\vv}_i$ within the larger Markov chain Monte Carlo (MCMC) sampling scheme (shown in supplement~\ref{sec:FCD}), with computational benefits coming from known tricks for sampling from the normal distribution (e.g., the Cholesky decomposition).
Additionally, we have the ability to specify, or learn, unique covariance matrices $\vOmega^u_i$ and $\vOmega^v_i$ for each basis function which, to the best of our knowledge, has not been previously considered.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Special cases}\label{sec:specialCases}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As discussed in Section~\ref{sec:PriorStructure}, when the prior covariance structure $\vOmega_i \equiv \vI$, the prior distribution is equivalent to that proposed by \citet{Hoff2007}.
Additionally, when $\vOmega_i \equiv \vI$ our specified probabilistic model for SVD is equivalent to the fixed-rank SVD model proposed by \citet{Hoff2007}.
Another interesting property is the relationship to the classic algorithmic approach, C-SVD. 
As discussed and shown empirically through simulation in the supplement (\ref{sec:ASVD}), when $\vOmega_i = \vI$ the mean of the full conditional distribution for the basis functions is equivalent to the estimates obtained by C-SVD.
% Of course, in a Bayesian setting \citep[e.g.,][]{Hoff2007} one can quantify uncertainty in the basis function estimates.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Model specification}\label{sec:ModelSpecifications}

The SVD model has several parameters that need to be specified: the number of basis functions $k$, the covariance matrices $\vC_u(\theta_{u,i})$ and $\vC_v(\theta_{v,i})$, and any hyperparameters associated with the covariance matrices $\theta_{u,i}$ and $\theta_{v,i}$.
While in principle the value $k$ can be estimated either informally, e.g., scree plots \citep{Cattell1966}, or formally, e.g., cross-validation \citep{Wold1978} or the variable-rank model proposed by \citet{Hoff2007}, that is not the focus of this work.
Through empirical testing, we have found that if the true $k^*$ is less than the specified $k$, then the last $k-k^*$ basis functions of both $\vU$ and $\vV$ will have posterior credible intervals that cover zero at all, or nearly all, observations.
Conversely, if the true $k^*$ is greater than the specified $k$, there is little to no impact on the basis functions (i.e., the $k$th basis function is not biased to account for the lost information by not estimating the remaining $k^* - k$ basis functions).
In choosing $k$ for the proposed model, an empirical Bayes approach can be taken.
Specifically, one could compute the C-SVD, compute the cumulative amount of variance explained by the basis function, and inform the value of $k$ based on this ``traditional'' approach.


Regarding the covariance matrices, as previously mentioned the hyperparameters $\vtheta_{u,i}$ and $\vtheta_{v,i}$ can either be specified directly or learned within the broader hierarchical model.
The latter choice would involve specifying a prior $p(\vtheta_{u,i}, \vtheta_{v,i})$ for these quantities and subsequently updating them within the MCMC algorithm.
In the case of using the Mat\'ern kernel to specify $\vC_u(\theta_{u,i})$ and $\vC_v(\theta_{v,i})$, recall that $\theta_{u,i} = \{\nu_{u,i}, \rho_{u,i}\}$ and $\theta_{v,i} = \{\nu_{v,i}, \rho_{v,i}\}$, where $\nu_{(\cdot)}$ describes the differentiability of the implied stochastic process and $\rho_{(\cdot)}$ describes the length-scale of the basis functions.
We generally recommend setting $\nu_{(\cdot)} = 3.5$ so the basis functions are third-order continuous but not over- or under- smoothed (e.g., infinitely differentiable with $\nu = \infty$ or nondifferentiable with $\nu = 0.5$, respectively).
If the length-scale parameters are not estimated within the MCMC algorithm, they could be estimated offline via geostatistical techniques, e.g., estimating a semivariogram separately across both the rows and columns.
In the simulations presented in Section~\ref{sec:simulations} we opt to estimate the length-scale parameter within the MCMC algorithm, while for the real-world example in Section~\ref{sec:SST} we pre-specify these quantities due to computational considerations (see Section~\ref{sec:scale}).
While it is difficult to ``perfectly'' estimate the length-scale parameters, any minor discrepancies from the ``true'' length-scale and the estimated length-scale can be offset by the fact that we infer the component-specific variance parameters $\{ \sigma^2_{u,i}, \sigma^2_{v,i}:i =1, \dots, k\}$.
More discussion on this point is provided in Section \ref{sec:discussion}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Other modeling choices}\label{sec:submodels}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Section~\ref{sec:PSVD} proposes a general model for observed data using a low-rank approach.
However, there are other model specifications and corresponding matrix factorizations 
that can be seen as special cases of the SVD model. 
We discuss a few of these choices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Principal components}\label{sec:PC}

As discussed in the introduction, PCA and SVD can be shown to produce an equivalent matrix factorization.
To this end, we can analogously represent the process $\vY = \vU \vA$ where $\vU$ is an orthonormal matrix of the eigenvectors of $\vY \vY'$, also known as the \textit{principal components}, $\vA = \vD\vV' = [\va_1, ..., \va_k]$ where $\va_i \sim N(0, \lambda_i\vI_m)$, and $\vLambda = diag(\lambda_1, ..., \lambda_k)$ are the eigenvalues of $\vY \vY'$, also known as the \textit{principal loadings}.
To estimate $\vU, \vA,$ and $\vLambda$ under this parameterization, there are two choices: (1) factor $\vE_{-i} = \vZ - \vU_{-i}\vA_{-i}$ in (\ref{eqn:SVDfactored}) and we assign the prior $\lambda_{i}\widetilde{\vu}_i|\vU_{i} \sim N_{n-k+1}(\vec{0}, \vN^{u \trp}_{i}\vOmega^u_i\vN^u_{i})\lambda_{i}^{n-1}$, or (2) estimate the parameters from the SVD model and compute $\vA$ as the posterior produce of $\vD$ and $\vV'$.
For choice (1), only the columns of $\vU$ are dependent where the elements of $\vA$ are independent, resulting in only the principal components having dependence.
If choice (2) is taken, then the columns of $\vU$ and rows $\vA$ can be modeled dependently, where $\vA$ is dependent through the specification of $\vV$.
For PCA parameterization, we advocate for choice (2) as there is more ``control'' over the model than choice (1).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Including covariates}\label{sec:LinearTrend}

The general model (\ref{eqn:data}) allows for more complex model structure, such as including covariates.
Traditionally, data are centered, or de-trended, prior to computing the SVD/PCA decomposition.
However, within (\ref{eqn:data}) a mean term can be accommodated by modeling, or parameterizing, $\vM$.
A naive choice is to let $\vM$ be a global mean, akin to the centering value in a traditional approach.
A slightly more informed choice would be to model $\vM$ as a linear trend that is dependent on some set of observed characteristics.

To account for a linear trend in the probabilistic SVD model, let $\text{vec}(\vM) = \vX \vbeta$ where $\vX \in \mathbb{R}^{nm \times p}$ is a matrix of covariates and $\vbeta \in \mathbb{R}^{p}$ is a vector of parameters.
To estimate $\vU, \vD, \vV$ under this parameterization, $\vE_{-i} = \vZ - [\vX \vbeta] - \vU_{-i}\vD_{-i}\vV'_{-i}$ in (\ref{eqn:SVDfactored}), where $[\vX \vbeta]$ denotes the reconstructed matrix of size $n \times m$.
To estimate $\vbeta$, we vectorize the model to get $\text{vec}(\vZ) \sim MVN_{nm}(\vX \vbeta + \text{vec}(\vU\vD\vV'), \vI_m \otimes \vSigma)$, assign the diffuse normal prior $\vbeta \sim MVN_p(\vec{0}, \sigma^2_{\beta}\vI_p)$, with $\sigma^2_{\beta}$ large, and get a standard normal-normal conjugate update for $\vbeta$.

This idea can be extended to a nonlinear function, say $\text{vec}(\vM) = f(\vX, \vbeta)$, where $f()$ is a nonlinear function.
For example, generalized additive models \citep{Hastie2017} or differential equations \citep{Berliner1996, Wikle2003a} could be used to model the nonlinear function.
However, care will likely need to be taken for the nonlinear case such that the nonlinear function is not too flexible, there-by conflicting with the random effect.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Computation and scalability}\label{sec:scale}

While the proposed prior is relatively simple to specify and implement, there are some computational aspects to consider.
On one hand, the fact that the prior is conjugate with a normal data distribution means that MCMC updates for the columns of $\vU$ and $\vV$ can be obtained in a straightforward manner.
On the other hand, calculating the full conditional distributions (from which the Gibbs draws are sampled) is computationally intensive for large $n$.  
From the formulation in Section~\ref{sec:PSVD}, the full conditional distributions for the columns of $\vU$ and $\vV$ involve matrix inverses $(\vN^{u \trp}_i \vOmega^u_i \vN^u_i)^{-1}$ and $(\vN^{v \trp}_i \vOmega^v_i \vN^v_i)^{-1}$, respectively (see supplement \ref{sec:FCD}), each of which are dense $(n-k+1)\times(n-k+1)$ and $(m-k+1)\times(m-k+1)$ matrices, respectively.
Therefore, in order to update $\vU$ and $\vV$ once in an MCMC iteration, we need to calculate $2k$ matrix inverses (one for each of the $k$ columns of $\vU$ and $\vV$), which is computationally challenging for large $n$ or $m$.
Furthermore, updating the hyperparameters of the kernel (e.g., the length-scale parameters $\rho_{u,i}$ and $\rho_{v,i}$) requires Metropolis-Hastings steps.
In this case, the likelihood involves a multivariate Normal density: when the covariance of the multivariate Normal is non-diagonal and dense (as is the case here), the number of flops associated with evaluating the determinant and solving quadratic forms scales with $\mathcal{O}(n^3)$.
Again, each iteration of the MCMC requires $2k$ of these calculations.
As such, without significant computing resources, the required computation for the model as-is proves difficult for data where either $n$ or $m$ is greater than 1000. 
More specifically, Figure~\ref{fig:scalability} shows an estimate of the amount of time needed to update all parameters in a single iteration of the MCMC for the special case of $k=5$ across different sample sizes $n$ and $m$ on a personal laptop.

% Figure environment removed

In spite of these apparent limitations, there are a variety of approaches one could take to reduce the associated computational burdens of this model.
The simplest approach would be to parameterize the covariance matrix as $\vOmega_i = \sigma^2_i\vC(\vtheta)$ where $\vtheta$ is specified and not estimated (as is done in Section~\ref{sec:SST}): this would remove the Metropolis-Hastings steps required to estimate $\rho_{u,i}$ and $\rho_{v,i}$.
Alternatively, one could use a compactly-supported kernel function \citep[see, e.g.,][]{Wendland1995, Buhmann2000, Genton2000, Gneiting2002, Melkumyan2009} and leverage sparse matrix techniques or, since there is a direct connection between the scaling of our methodology and the scaling of Gaussian processes, one could utilize one of the broad suite of approximate GP methods \citep[see, e.g.,][]{Heaton2019}. 
For now, we leave these considerations to future work.
Ultimately, these computational challenges are the price for full Bayesian uncertainty quantification for structured orthonormal matrices, and any simplifications or shortcuts 
should be fully vetted for impacts on the method's robustness.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Synthetic data example}\label{sec:simulations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We now conduct a simulation study to illustrate the impact of various parameterizations of the proposed prior with the Bayesian SVD model on basis function and data recovery.
For all simulations, the target ``true'' basis functions $\vU$ and $\vV$ are simulated according to the generating mechanism described in section~\ref{sec:GeneratingMechanism} (e.g., to produce the orthonormal matrix in (\ref{eqn:orthomatrix})) with $\vOmega^u_i = \sigma^2_{u,i}\vC_u(\vtheta_{u,i})$ and $\vOmega^v_i = \sigma^2_{v,i}\vC_v(\vtheta_{v,i})$ where the elements of $\vC_u(\vtheta_{u,i})$ and $\vC_v(\vtheta_{v,i})$ are defined by the Mat\'ern correlation function with $\vtheta_{u,i} = (\nu_{u,i}, \rho_{u,i})$ and $\vtheta_{v,i} = (\nu_{v,i}, \rho_{v,i})$ and the diagonal matrix $\vD$ is specified.
For all simulations, we specify the true number of basis function $k = 5$ with covariance parameters $(\nu_{(\cdot),i}, \rho_{(\cdot),i}) = (3.5, 3)$ for both $\vU$ and $\vV$ and for all $i=1,\dots,k$, and diagonal matrix $\vD = diag(40, 30, 20, 10, 5)$.


Data are simulated according to (\ref{eqn:SVD_data}) with $\vSigma = \sigma^2 \vI_n$, $n = 100$, $m = 100$, $\vs_1, ..., \vs_n$ equally spaced in $\mathcal{S} = [-5, 5]$, $t_1, ..., t_m$ are equally spaced in $\mathcal{T} = [0, 10]$.
To determine the effect of measurement error, we generate data sets with $\sigma^2$ chosen such that the signal-to-noise ratio (SNR) is $SNR = [10, 5, 2, 1, 0.5, 0.1]$.
Specifically, let $\veta$ be a random $n \times m$ matrix of iid standard normal random variables.
Then, $\sigma = \sqrt{\frac{var(\vY)}{SNR * var(\veta)}}$, and the simulated data is $\vZ = \vY + \sigma \veta$ (see the supplement Figure~\ref{fig:simdata1D}) with $var(\vY)/var(\vZ-\vY) = SNR$.
One realization of the simulated data with a SNR of 1 and the $\vU$ and $\vV$ basis functions are shown in Figure~\ref{fig:data1D}.

% Figure environment removed

As discussed in Section~\ref{sec:ModelSpecifications}, using this model only requires specification of $k$, the number of basis functions used in $\vU$ and $\vV$.
To investigate how the number of basis functions impacts model recovery, we estimate the model specifying $k = [3, 4, 5, 6, 7]$ (recall the true $k=5$) for each level of SNR.
For each SNR and $k$ combination, we obtain $10000$ posterior samples of the model parameters, discarding the first $5000$ as burn-in.
To understand the variability in the specification of $k$ and SNR on model recovery, we repeat this process 100 times.
Due to the large number of models, convergence was assessed graphically for a random subset of the fits with no issues detected.


For each posterior simulation, we calculated the 95\% coverage rate (CR) and root mean square error (RMSE) for $\vU$, $\vV$, and the ``true'' surface $\vY = \vU \vD \vV'$.
If the true $k^*$ is greater than the specified $k$, the empirical CR and RMSE are computed only for the first $k$ basis functions and then averaged over the $k$ estimates (e.g., we do not consider the last $k^* - k$ basis functions when computing CR and RMSE).
If the true $k^*$ is less than the specified $k$, the empirical CR and RMSE for the ``extra'' $k-k^*$ basis functions are compared to the zero line and the reported CR and RMSE values are obtained by averaging over the $k$ estimates.
% CR and RMSE are computed on a pointwise basis and then averaged over $k$ and all data points.
Additionally, for each simulation we computed the C-SVD using the base linear algebra library, \textit{LinearAlgebra.jl}, in Julia \citep[][]{Bezanson2017} and computed the RMSE of the calculated $\vU, \vV$, and reconstructed surface $\vY$ assuming the same truncation value $k$.
% For simulations where the specified $k$ was larger than the true value, we compared the estimated value of the ``extra'' basis functions to the zero line.
The coverage rates and the RMSE are shown in Figure~\ref{fig:simulation_CR_RMSE}.
The results of one simulation are shown in Figure~\ref{fig:estimatedBasis1D} based on the data shown in Figure~\ref{fig:data1D}.



% Figure environment removed

% Figure environment removed

From Figure~\ref{fig:simulation_CR_RMSE}(a), we see our median coverage rate for the $\vU$ (middle row) and $\vV$ (bottom row) basis functions (blue line) is near the nominal level (horizontal black line) and the 95\% Monte Carlo uncertainty bounds (MCUB) for the coverage rate (blue shaded region) covers the nominal level for all SNR levels and regardless of the specification of $k$.
This implies that posterior uncertainties are well calibrated and robust to mis-specifications of the number of estimated basis functions, regardless of the magnitude of the noise.
For the recovered data (top row), we see the 95\% MCUB cover the nominal level for all SNR levels with $k$ greater than 5.
However, for $k$ less than 5, achieving the nominal coverage depends on SNR: in low signal cases (e.g., SNR = 0.1), the uncertainties are well calibrated, while posterior uncertainties are too small (i.e., coverage of the truth is much less than the nominal level) when the signal is stronger (SNR $>$ 0.5).
This counterintuitive result is due to the impact of unaccounted signal for higher-order basis functions ($i=4$ and/or $i=5$) on the signal: for large SNR, individual basis functions both (a) contribute more to the overall uncertainty in the data and also (b) have narrower posterior distributions, such that ignoring one or more true basis functions causes the model to underestimate data uncertainties (e.g., see Figure~\ref{fig:estimatedBasis1D}).
Conversely, for smaller SNR, there is more uncertainty in each basis function estimate and the impact of higher-order basis functions on the estimated surface is reduced, to the extent that the model can recover the nominal coverage of the data.

For the RMSE, shown in Figure~\ref{fig:simulation_CR_RMSE}(b), the most notable result is that the median RMSE for our approach (blue line) is systematically lower than the corresponding RMSE from the algorithmic C-SVD approach for both data (top row) and basis functions (middle and bottom rows), across SNR levels and specification of $k$.
In other words, estimates of the basis functions in both $\vU$ and $\vV$ and the recovered data have systematically lower errors than what one can obtain from the algorithmic approach. 
Regarding RMSE for estimates of the recovered surface, the median error (blue line) decreases as a function of SNR, as expected, and interestingly the data RMSE is relatively insensitive to specification of $k$. 
For the $\vU$ and $\vV$ basis functions (middle and bottom rows, respectively, of Figure~\ref{fig:simulation_CR_RMSE}), we see that trajectories of RMSE estimates for our proposed approach and the C-SVD mirror each other, with our estimates being systematically, but not significantly, lower.
However, across SNR levels, the RMSE actually increases as one moves from $k=3$ to $k=7$ (even though the true $k=5$).
For SNR equal to 5 and 10, we see a dramatic spike in the RMSE estimate and uncertainty for the $\vU$ and $\vV$ basis functions for $k = 6$ and $7$.
This is because we are comparing against the zero line for these cases: while the uncertainty bounds for these basis function covers the zero line (as seen in the coverage results in Figure~\ref{fig:simulation_CR_RMSE}a.), there is a lot of variability in these estimates (with relatively lower uncertainty due to larger signal), leading to inflated RMSE values.



In conclusion, this synthetic data example shows the proposed method has well calibrated uncertainty and significantly reduces the impact of measurement noise on the basis function estimates.
However, there is a significant trade-off in choosing $k$ to be too small or large based on the magnitude of the SNR.
Based on our simulation, there will be significant bias in the recovered surface but \textit{not} in the estimated basis functions if $k$ is too small and the SNR is low.
Additionally, there will \textit{not} be significant bias in the recovered surface or in the estimated basis functions if either $k$ is too small and the SNR is large or $k$ is too large.
The only trade-off for too large of $k$ are inflated RMSE's for the extraneous basis function which could lead to underestimated RMSE's in the recovered surface.
Therefore, we suggest erring on the side of choosing $k$ to be too large.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sea surface temperature}\label{sec:SST}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As discussed in the introduction, empirical orthogonal functions or EOFs are commonly used in climate sciences to compute (among other things) indices that summarize the modes of large-scale variability in the Earth system.
One such index is the \textit{Pacific Decadal Oscillation} \citep[PDO;][]{Mantua1997}.
Mathematically speaking, the PDO is defined as the leading EOF of the North Pacific (poleward of 20$^\circ$N) sea-surface temperature (SST) monthly averaged anomalies after the global climatological mean has been removed (see supplement~\ref{sec:PDO} for specific details).
When the PDO has a positive value, the interior SST of the Pacific is anomalously cool and the west coast of the Americas is anomalously warm.
Conversely, when the PDO has a negative value, the interior SST of the Pacific is anomalously warm and the coast of the Americas is anomalously cool \citep{Mantua2002}.
While it is known that the PDO is highly correlated with other physical climate processes \citep[e.g., the El Ni\~no/Southern Oscillation and the Pacific-North American teleconnection pattern;][]{Newman2016, Risser2021}, we ignore these relationships for this example.

We use gridded monthly SST data from the Met Office Hadley Centre\footnote{https://www.metoffice.gov.uk/hadobs/hadisst/} (HadISST) at 1$^\circ$ resolution from January 1870 to June 2022 \citep{Rayner2003}.
The data are detrended according to the process described in the supplement \citep[\ref{sec:PDO}; derived from][]{Hartmann2015} and we focus on the subset to only include data from 20$^\circ$- 60$^\circ$N and 110$^\circ$- 280$^\circ$E for a total of 4267 spatial locations across 1830 time points.
While it is possible to create a design matrix that is equivalent to the detrending step and incorporate the matrix into the model structure through $\vM$ in (\ref{eqn:data}), this would have resulted in a substantial number of parameters to estimate.
Therefore, we detrend the data \textit{a priori} to parameter estimation.
The detrended data is shown in the supplement (Figure~\ref{fig:PDOoriginal}) along with the first EOF and the PDO index time series as computed using C-SVD.


Our goal is to understand the uncertainty in the PDO index when the spatial basis functions are modeled dependently.
However, we do not want to impose temporal dependence on the PDO index: in climate science applications that utilize the index \citep[e.g.][]{Zhang2010}, one is typically interested in the month-to-month variability of the index.
Therefore, we parameterize the covariance matrix for the prior of the spatial basis functions using the Mat\'ern kernel with $\rho = 560$km and $\nu = 3.5$ and the covariance matrix for the prior of the temporal basis functions using the identity matrix.
The effective range $\rho = 560$km is chosen based on empirical variogram estimates.
Last, while our interest is only on the leading basis function, we specify $k=5$ so that the estimate of the leading basis function is not biased by failure to account for the lower order signals.
We obtain $10000$ samples from the posterior, discarding the first $5000$ as burn-in, where convergence is assessed graphically with no issues detected.

The posterior samples of the PDO (P-PDO) index from January 2010 to June 2022 are shown in Figure~\ref{fig:PDOFigure} c), where the black line indicates the posterior mean and the yellow shaded region is the 95\% CI.
The P-PDO index preserves the general structure of the PDO index (Figure~\ref{fig:PDOFigure} a)), which is to be expected as the PDO signal has been shown to be robust to the data source (e.g., HadISST, COBE, ERSST, Kaplan) and sample period \citep[e.g., analysis of different time periods;][]{Newman2016}.
To further support this claim, we found no significant departure between the P-PDO and PDO index based on the 95\% CI (Figure~\ref{fig:PDOFigure} e)).
As was expected, the posterior mean of the leading spatial EOF (Figure~\ref{fig:PDOFigure} d); P-EOF) is smoother than the algorithmic estimate of the leading spatial EOF as seen by observing the difference in individual contours (Figure~\ref{fig:PDOFigure} b); A-EOF), but the general spatial structure is the same.
The posterior difference between the P-EOF and A-EOF (Figure~\ref{fig:PDOFigure} f)) highlights areas where the P-EOF is smoother than the A-EOF estimate, with the most visually apparent being in the spatial region of 30$^\circ$- 45$^\circ$N and 153$^\circ$- 129$^\circ$W, corresponding to the transition from a positive to negative value.
Interestingly, we see a ``banding'' pattern in the difference plot which is due to the P-EOF values smoothing the rougher estimates of the A-EOF values - effectively bisecting the A-EOF values.



% Figure environment removed


In effect, our Bayesian estimate samples from the distribution of the PDO index while accounting for the measurement error in the SST data and the spatial dependence present in the spatial basis functions under the model assumptions used to generate the HadISST gridded data product.
The primary benefit of using a Bayesian approach to modeling the PDO index is now apparent: posterior draws of the spatial basis functions, together with posterior draws of all other hyperparameters, imply a posterior distribution over the PDO index itself. While this may seem like an obvious statistical conclusion, this is a nontrivial result: we have now generated an observational ``ensemble'' of a dominant mode of climate variability (instead of just a central estimate with standard errors). The members of this ensemble (the different realizations of the PDO index calculated from each posterior draw) can be plugged into an analysis such as \cite{Zhang2010}, providing a straightforward way to propagate uncertainty in the PDO index through to the resulting scientific conclusions (potentially obviating challenges associated with error-in-variable regression approaches). Importantly, as the output of a statistical model, this observational ensemble of PDO estimates is calculated at considerably less computational cost than other approaches to generating observational ensembles (e.g., the ensemble component of ERA5, \citealp{Hersbach2020}).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}\label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We proposed a novel prior distribution for structured orthonormal matrices that is an extension of \citet{Hoff2007}, where the basis functions are able to be modeled dependently.
The prior is based on the projected normal distribution which we augment with a latent length.
When our prior is combined with a normal data model, the resulting full conditional distributions for the basis functions are conjugate, resulting in analytically straightforward MCMC sampling.
We describe how the prior can be used to conduct posterior inference on a general class of probabilistic SVD models and how to extend the proposed model to various other applications.
We discussed various mathematical properties of our probabilistic SVD model (supplement~\ref{sec:IC}) and illustrated its capability through a simulation study.
% Through simulation studies we illustrate the capability of our probabilistic SVD model and discuss the mathematical properties of the model (supplement~\ref{sec:IC}).
The model is then used to draw inference on the PDO index, an important metric for characterizing the state of Earth's climate.



The synthetic data examples and real world example presented in Sections~\ref{sec:simulations} and \ref{sec:SST}, respectively, all highlighted the model's efficacy on gridded data.
However, the model is equally well suited for non-uniformly spaced data so long as the spacing is consistent within space and within time.
If the data are not spaced consistently within space and within time, this would constitute a missing data problem, which we plan to explore in future work.


Another area for possible extension relies on computational improvements.
For all synthetic data examples and the real world example, we relied on the support of a super computer.
While each individual model combination in Section~\ref{sec:simulations} could be run on a personal computer, the overall simulation in Section~\ref{sec:simulations} as well as the real world example in Section~\ref{sec:SST} were impractical for a personal computer due to the time demand.
As discussed in Section~\ref{sec:scale}, this is because each iteration of the MCMC algorithm requires the inversion of $\vN^{u \trp}_{i}\vOmega^u_i\vN^u_{i}$ and $\vN^{v \trp}_{i}\vOmega^v_i\vN^v_{i}$ for $i = 1, ..., k$ in addition to the Metropolis-Hastings step required to estimate $\rho_{u,i}$ and $\rho_{v,i}$.
While possible to run on a personal computer from a memory stand-point, the amount of time required to run the simulation and real world example is the impractical aspect.
Computational work focusing on improving the speed of this calculation would prove beneficial for model feasibility.
For example, using sparse kernels, which have favorable computational properties for large data sets \citep{Noack2023}, can be used to reduce the computational burden present with dense matrices.


The choice of $k$, the number of basis functions, is the only major subjective choice in our proposed probabilistic SVD model.
While we show the mis-specification of $k$ does not have a negative impact when erring on the side of $k$ being too large, a more flexible model estimating $k$ is attractive.
To estimate $k$, \citet{Hoff2007} proposed a variable-rank model utilizing the so-called spike-and-slab variable selection prior \citep{Mitchell1988}.
However, because of the difference in our prior compared to the prior proposed by \citet{Hoff2007}, incorporating the spike-and-slab prior into our proposed model would require extra theoretical work.
Work focused on estimating the rank $k$ with our framework would produce a very flexible approach for modeling spatio-temporal random effects.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


All code is written in Julia \citep{Bezanson2017} and is available publicly on GitHub at %\textit{redacted for review}.
\url{https://github.com/jsnowynorth/BayesianSpatialBasisFunctions}.
Additionally, the Bayesian SVD model has been developed into a Julia package 
%\textit{redacted for review}, 
\textit{BayesianSVD.jl}, 
which can also be downloaded from GitHub at 
%\textit{redacted for review} 
\url{https://github.com/jsnowynorth/BayesianSVD.jl} 
for easy use.


This research was supported by the Director, Office of Science, Office of Biological and Environmental Research of the U.S.\ Department of Energy under Contract No.\ DE-AC02-05CH11231 and by the Regional and Global Model Analysis Program area within the Earth and Environmental Systems Modeling Program.
This research was also supported by the National Science Foundation [OAC-1931363, ACI-1553685] and the National Institute of Food \& Agriculture
[COL0-FACT-2019].
The research used resources of the National Energy Research Scientific Computing Center (NERSC), also supported by the Office of Science of the U.S.\ Department of Energy, under Contract No.\ DE-AC02-05CH11231.

This document was prepared as an account of work sponsored by the United States Government. While this document is believed to contain correct information, neither the United States Government nor any agency thereof, nor the Regents of the University of California, nor any of their employees, makes any warranty, express or implied, or assumes any legal responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by its trade name, trademark, manufacturer, or otherwise, does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof, or the Regents of the University of California. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof or the Regents of the University of California.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setstretch{1}
\bibliographystyle{apalike}
% \bibliography{references, extraRefs}
% \bibliography{references}
\bibliography{referencesClean}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{section}{0}
\setcounter{equation}{0}
\renewcommand{\thesection}{A}
\renewcommand{\theequation}{A.\arabic{equation}}
\section{Appendix}\label{sec:appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proofs of propositions}\label{sec:props}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


We now prove the propositions describing the properties of the orthogonal matrix constructed in section~\ref{sec:GeneratingMechanism}.

\noindent \rule{\linewidth}{0.1em}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Proposition 1.} 
\textit{
    The matrix $\vW = \vX (\vX' \vX)^{-1/2}$ is invariant to right-orthogonal transformation. That is, for $\vH:\mathbb{R}^k \rightarrow \mathbb{R}^k$ an orthogonal transformation matrix, $\vW \vH \overset{d}{=} \vW$.
}


% \textit{Proof of Proposition 1.} 
\begin{proof}
From Corollary 2.4.7 of \citet{Chikuse2003}, the distribution of $\vW$ is invariant to right-orthogonal transformation if 
\begin{itemize}
    \item[(a)] $\vX$ is an $n \times k$ matrix that is invariant to right-orthogonal transformation and 
    \item[(b)] the density of $\vX$ has the form
\begin{align*}
    f_X(X) = |\vSigma|^{-k/2}g(\vX' \vSigma^{-1} \vX),
\end{align*}
with $\vSigma$ a positive-definite matrix.
\end{itemize}


First, for $\vX_i, i = 1, ..., K$, we show invariance to right-orthogonal transformation by induction.
Let $\vH_i:\mathbb{R}^i \rightarrow \mathbb{R}^i$ be an orthogonal transformation.

\begin{enumerate}
    \item For $i=1$, $\vX_1\vH_1 = \vx_1\vH_1 \overset{d}{=} \vx_1 = \vX_1$.
    \item Assume for $i=j$, $\vX_j\vH_j|\vX_{j-1} \overset{d}{=} \vX_j|\vX_{j-1}$.
    \item By the characteristic function of $\vX_{j+1}\vH_{j+1} = \vX_{j+1}^{\star}$, we have
        \begin{align*}
            \varphi(\vX_{j+1}\vH_{j+1}|\vX_{j}) & = E\left[ \left.\exp\left\{ i \sum_{\ell=1}^{j+1}\vt_{\ell}'\vx_{\ell}^{\star} \right\} \right|\vX_{j} \right] \\
            & = \exp\left\{ i \sum_{\ell=1}^{j}\vt_{\ell}'\vx_{\ell}^{\star} \right\} E\left[ \exp\left\{ i\vt_{j+1}'\vx_{j+1}^{\star} \right\}| \vX_{j}\right],
        \end{align*}
        where $\vt_{j+1}'\vx_{j+1}^{\star} = \vt_{j+1}'\vP_{j}^{\star}\vz_{j+1}$, $\vP_{j}^{\star} = \vI - (\vX_j\vH_j) ((\vX_j\vH_j)' (\vX_j\vH_j))^{-1} (\vX_j\vH_j)'$, and $\vz_{j+1} \sim N_n(\vec{0}, \vOmega_j)$ is independent of $\vX_{j}$.
        We also have
        \begin{align*}
            \vP_{j}^{\star} & = \vI - (\vX_j\vH_j) \{(\vX_j\vH_j)' (\vX_j\vH_j)\}^{-1} (\vX_j\vH_j)' \\
            & = \vI - \vX_j\vH_j \vH_j^{-1}('\vX_j' \vX_j)^{-1}\vH_j^{\trp-1} \vH_j'\vX_j \\
            & = \vI - \vX_j(\vX_j' \vX_j)^{-1}\vX_j \\
            & = \vP_{j}.
        \end{align*}
        Therefore, the characteristic function is
        \begin{align}\label{eqn:ROC}
            \varphi(\vX_{j+1}\vH_{j+1}|\vX_j) & = \exp\left\{ i \sum_{\ell=1}^{j}\vt_{\ell}'\vx_{\ell}^{\star} \right\} E\left[ \exp\left\{ i\vt_{j+1}'\vP_{j}^{\star}\vz_{j+1} \right\} |\vX_j \right]  \nonumber \\
            & = \exp\left\{ i \sum_{\ell=1}^{j}\vt_{\ell}'\vx_{\ell}^{\star} \right\} \exp\left\{ -\frac{1}{2}\vt_{j+1}'\vP_{j}^{\star}\vOmega_j\vP_{j}^{\trp \star} \vt_{j+1}\right\}  \nonumber \\
            & = \exp\left\{ i \sum_{\ell=1}^{j}\vt_{\ell}'\vx_{\ell}^{\star} \right\} \exp\left\{ -\frac{1}{2}\vt_{j+1}'\vP_{j}\vOmega_j\vP_{j}^{\trp} \vt_{j+1}\right\}.
        \end{align}
        Similarly, the characteristic function for $\vX_{j+1}$ is
        \begin{align}\label{eqn:OC}
            \varphi(\vX_{j+1}|\vX_j) = \exp\left\{ i \sum_{\ell=1}^{j}\vt_{\ell}'\vx_{\ell} \right\} \exp\left\{ -\frac{1}{2}\vt_{j+1}'\vP_{j}\vOmega_j\vP_{j}^{\trp} \vt_{j+1}\right\}.
        \end{align}
        By assumption, $\vX_j\vH_j \overset{d}{=} \vX_j$, so $\{\vx_1^{\star}, ..., \vx_j^{\star}, \vP_j^{\star}\} \overset{d}{=} \{\vx_1, ..., \vx_j, \vP_j\}$ and the characteristic function (\ref{eqn:ROC}) and (\ref{eqn:OC}) are equal. 
        Because the characteristic functions specify the distributions, $\vX_{j+1}\vH_{j+1} \overset{d}{=} \vX_{j+1}$.        
\end{enumerate}

Next, we show the density of $\vX$ is of the form
\begin{align*}
    f_{\vX}(\vX) = |\vSigma|^{-k/2}g(\vX' \vSigma^{-1} \vX)
\end{align*}
using induction.

\begin{enumerate}
    \item 
        For $i=1$, $\vX_1 = \vx_1$ and
        \begin{align*}
            f_{\vx_1}(\vx_1) \propto |\vOmega_1|^{-1/2}g(\vx_1' \vOmega_{1}^{-1} \vx_1).
        \end{align*}
        
    \item 
        For $i=2$, $f_{\vx_2|\vX_1}(\vx_2|\vX_1) \sim N(\vec{0}, \vP_1 \vOmega_2 \vP_1')$.
        Taking the Schur complement, $\vP_1 \vOmega_2 \vP_1' = \vSigma_{11} - \vSigma_{12}\vSigma_{22}^{-1}\vSigma_{21}$ and the joint distribution is 
        \begin{align*}
            \begin{bmatrix}
                \vx_1 \\
                \vx_2
            \end{bmatrix} \sim N\left(\vec{0}, 
            \begin{bmatrix}
                \vSigma_{11} & \vSigma_{12} \\
                \vSigma_{21} & \vSigma_{22}\\
            \end{bmatrix}
            \right).
        \end{align*}
        From known properties of the multivariate normal and the matrix normal, $[\vx_1, \vx_2] \equiv \vX_2 \sim MN_{n \times 2}(\vec{0}, \vA, \vB)$ where 
        \begin{align*}
            \begin{bmatrix}
                \vSigma_{11} & \vSigma_{12} \\
                \vSigma_{21} & \vSigma_{22}\\
            \end{bmatrix} = \vB \otimes \vA,
        \end{align*}
        where $\vA \in \mathbb{R}^{2 \times 2}$, $\vB \in \mathbb{R}^{n \times n}$.
        Also, because $\vB \otimes \vA$ is invertible, $(\vB \otimes \vA)^{-1} = \vB^{-1} \otimes \vA^{-1}$ and both $\vA$ and $\vB$ are invertible.
        Thus,
        \begin{align*}
            f_{\vX_2}(\vX_2) \propto |\vB|^{-1/2}g(\vX_2' \vB^{-1} \vX_2).
        \end{align*}
        
        
    \item 
        For $i = j$, $f_{\vx_j|\vX_{j-1}}(\vx_j|\vX_{j-1}) \sim N(\vec{0}, \vP_{j-1} \vOmega_j \vP_{j-1}')$, where the Schur complement is $\vP_{j-1} \vOmega_j \vP_{j-1}' = \vSigma_{11} - \vSigma_{12}\vSigma_{22}^{-1}\vSigma_{21}$.
        The joint distribution  
        \begin{align*}
            \begin{bmatrix}
                \vX_{j-1} \\
                \vx_j
            \end{bmatrix} \sim N\left(\vec{0}, 
            \begin{bmatrix}
                \vSigma_{11} & \vSigma_{12} \\
                \vSigma_{21} & \vSigma_{22}\\
            \end{bmatrix}
            \right)
        \end{align*}
        can be written as a matrix normal $[\vX_{j-1}, \vx_j] \equiv \vX_j \sim MN_{n \times j}(\vec{0}, \vA, \vB)$ where 
        \begin{align*}
            \begin{bmatrix}
                \vSigma_{11} & \vSigma_{12} \\
                \vSigma_{21} & \vSigma_{22}\\
            \end{bmatrix} = \vB \otimes \vA
        \end{align*}
        with
        \begin{align*}
            f_{\vX_j}(\vX_j) \propto |\vB|^{-1/2}g(\vX_j' \vB^{-1} \vX_j).
        \end{align*}    
\end{enumerate}

Therefore, the density of $\vX$ is of the form
\begin{align*}
    f_{\vX}(\vX) = |\vSigma|^{-k/2}g(\vX' \vSigma^{-1} \vX).
\end{align*}
\end{proof}

\noindent \rule{\linewidth}{0.1em}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Proposition 2.} 
\textit{ 
    $\vw_i|\vW_{i-1} \overset{d}{=} \vN_{i-1}\widetilde{\vw}_i|\vW_{i-1}$ where the columns of $\vN_{i-1}$ form an orthonormal basis for the null space of $\vW_{i-1}$ and $\widetilde{\vw}_i|\vW_{i-1} \sim PN_{n-i+1}(\vec{0}, \vN_{i-1}'\vOmega_i\vN_{i-1})$ is the projected weight function.
}


\begin{proof}
The following argument is similar to \citet{Hoff2007}, except now we account for dependence structure and the resulting distribution is different.
By construction, $\vw_i = \vP_{i-1}\vz_i/(\vz_i'\vP_{i-1}'\vP_{i-1}\vz_i)^{1/2}$ where $\vP_{i-1}$ has $n-i+1$ eigenvalues equal to 1 and the rest being 0.
Let the eigenvalue decomposition be $\vP_{i-1} = \vN_{i-1}\vN_{i-1}'$ where $\vN_{i-1}$ is an $n \times (n-i+1)$ matrix whose columns span the null space of $\vW_i$.
Making the substitution $\vP_{i-1} = \vN_{i-1}\vN_{i-1}'$,
\begin{align*}
    \vw_i & = \frac{\vP_{i-1}\vz_i}{(\vz_i'\vP_{i-1}'\vP_{i-1}\vz_i)^{1/2}} \\
          & = \frac{\vN_{i-1}\vN_{i-1}'\vz_i}{(\vz_i'\vN_{i-1}'\vN_{i-1}\vN_{i-1}\vN_{i-1}'\vz_i)^{1/2}} \\
          & = \vN_{i-1}\frac{\vN_{i-1}'\vz_i}{(\vz_i'\vN_{i-1}\vN_{i-1}'\vz_i)^{1/2}}.
\end{align*}
Note that $\vP_{i-1} = \vI - \vW_{i-1}\vW_{i-1}'$, so $\vw_i|\vW_{i-1} \overset{d}{=} \vN_{i-1}\frac{\vN_{i-1}'\vz_i}{(\vz_i'\vN_{i-1}\vN_{i-1}'\vz_i)^{1/2}}$.
Because $\vz_i \sim N_n(\vec{0}, \vOmega_i)$, we have $\vN_{i-1}'\vz_i|\vW_{i-1} \sim N_n(\vec{0}, \vN_{i-1}'\vOmega_i\vN_{i-1})$ and $\frac{\vN_{i-1}'\vz_i}{(\vz_i'\vN_{i-1}\vN_{i-1}'\vz_i)^{1/2}} \equiv \widetilde{\vw}_i|\vW_{i-1} \sim PN(\vec{0}, \vN_{i-1}'\vOmega_i\vN_{i-1})$.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{section}{0}
\renewcommand{\thesection}{S}
\renewcommand{\thetable}{S}
\renewcommand{\thefigure}{S.\arabic{figure}}
\renewcommand{\theequation}{S.\arabic{equation}}
\setcounter{figure}{0}
\section{Supplemental Material}\label{sec:supplement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Full conditional distributions}\label{sec:FCD}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The diagonal matrix $\vD$ and the length-scale parameters $\rho_{u,i}$ and $\rho_{v,i}$ do not have conjugate updates and so we use a Metropolis-within-Gibbs step to estimate these parameters.
For all Metropolis steps, we use a truncated normal for the proposal distribution with the mean set to the most recently accepted value.
For $\vD$ the upper truncation bound is set to infinity and for $\rho_{u,i}$ and $\rho_{v,i}$ the upper truncation bound is set to the max distance for $\vU$ (e.g., greatest distance between spatial locations) and $\vV$ (e.g., greatest span between time points) divided by 2, respectively.
Because the variance of the proposal can influence the acceptance rate, we automatically tune the proposal variance for each parameter individually such that the acceptance rate is between 25\% and 45\%.

\vspace{2em}

\textbf{Sampling Algorithm}

For each iteration of the MCMC algorithm, do:
\begin{enumerate}
    \item Update $\vD$ using a Metropolis step
    
    \item For $i \in \{1, ..., k\}$ update $\vu_i|\vU_{-i} \overset{d}{=} \vN^u_{i} \widetilde{\vu}_i$ where $\widetilde{\vu}_i$
        \begin{align*}
            [\widetilde{\vu}_i|\cdot]  & \sim N(\vS_u^{-1}\vm_u, \vS_u^{-1}) \mathbb{I}(\widetilde{\vu}_i \in \mathcal{V}_{1,n}) \\
                              & \vm_u = d_i \vN_i^{u \trp} \vSigma^{-1} \vE_i \vv_i \nonumber \\
                              & \vS_u = d_i^2 (\vN^{u \trp}_{i}\vOmega^u_i\vN^u_{i})^{-1} + d_i^2 \vN_i^{u \trp} \vSigma^{-1} \vN_i^{u}. \nonumber
        \end{align*}
    \item For $i \in \{1, ..., k\}$ update $\vv_i|\vV_{-i} \overset{d}{=} \vN^v_{i} \widetilde{\vv}_i$ where $\widetilde{\vv}_i$
        \begin{align*}
            [\widetilde{\vv}_i|\cdot] & \sim N(\vS_v^{-1}\vm_v, \vS_v^{-1}) \mathbb{I}(\widetilde{\vv}_i \in \mathcal{V}_{1,m}) \\
                              & \vm_v = d_i \vN_i^{v \trp} \vE_i' \vSigma^{-1} \vu_i \nonumber\\
                              & \vS_v = d_i^2 (\vN^{v \trp}_{i}\vOmega^v_i\vN^v_{i})^{-1} + d_i^2 \vu_i' \vSigma^{-1} \vu_i \vI_m. \nonumber
        \end{align*}
    
    \item Recall, we parameterize $\vSigma = \sigma^2 \vI_n$.
            The full conditional distribution for $\sigma^2$ is
            \begin{align*}
                [a|\cdot] & \sim IG((\xi + 1)(2), (1/A^2) + \xi/\sigma) \\
                [\sigma^2|\cdot] & \sim IG((nm + \xi)/2, \xi/a + (\vZ - \vU \vD \vV')'(\vZ - \vU \vD \vV')/2).
            \end{align*}
            We specify $\xi = 1$ and $A = 10^5$ which corresponds to the prior $\sigma \sim \mbox{Half-}t(\xi, A) \equiv \mbox{Half-cauchy}(A)$.
    
    \item Recall, we parameterize $\vOmega_u(\theta_{u,i}) = \sigma^2_{u,i} \vC_u(\theta_{u,i})$.
            For $i \in \{1, ..., k\}$ update $\sigma^2_{u,i}$ from
            \begin{align*}
                [a_{u,i}|\cdot] & \sim IG((\xi + 1)(2), (1/A^2) + \xi/\sigma_{u,i}) \\
                [\sigma^2_{u,i}|\cdot] & \sim IG((n - k + 1 + \xi)/2, \xi/a_{u,i} + (d_i^2 \widetilde{\vu}_i' (\vN^{u \trp}_{i}\vOmega^u_i\vN^u_{i})^{-1} \widetilde{\vu}_i)/2),
            \end{align*}
            with $\xi = 1$ and $A = 10^5$.
    
    \item Recall, we parameterize $\vOmega_v(\theta_{v,i}) = \sigma^2_{v,i} \vC_v(\theta_{v,i})$.
            For $i \in \{1, ..., k\}$ update $\sigma^2_{v,i}$ from
            \begin{align*}
                [a_{v,i}|\cdot] & \sim IG((\xi + 1)(2), (1/A^2) + \xi/\sigma_{v,i}) \\
                [\sigma^2_{v,i}|\cdot] & \sim IG((m - k + 1 + \xi)/2, \xi/a_{v,i} + (d_i^2 \widetilde{\vv}_i' (\vN^{v \trp}_{i}\vOmega^v_i\vN^v_{i})^{-1} \widetilde{\vv}_i)/2),
            \end{align*}
        with $\xi = 1$ and $A = 10^5$.
    
    \item For $i \in \{1, ..., k\}$ update $\rho^2_{u,i}$ using a Metropolis step
    
    \item For $i \in \{1, ..., k\}$ update $\rho^2_{v,i}$ using a Metropolis step
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Identity correlation}\label{sec:IC}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

When $\vOmega_i = \vI_n$, $\widetilde{\vw}_k$ in proposition 2 is uniformly distributed on the Stiefel manifold.
To see this, note that for $\vz_k \sim N(0, \vI)$, $\vN_{k-1}'\vz_k \sim N_{n-k+1}(0, \vI)$, and $\frac{\vN_{k-1}'\vz_k}{(\vz_i'\vN_{i-1}\vN_{i-1}'\vz_i)^{1/2}}$ is uniformly distributed on the $n-k+1$ sphere.
Also, we see proposition 1 is now equivalent to \citet{Hoff2007}, and $\vW$ is the uniform probability measure on $\mathcal{V}_{k,n}$.

The resulting full conditional distributions for $\widetilde{\vu}_i$ and $\widetilde{\vv}_i$ when $\vOmega^u_i = \vI_{n}$ and $\vOmega^v_i = \vI_{m}$ for the SVD model in~\ref{sec:FCD} become the von-Mises Fisher distribution, which is equivalent to the full conditionals of \citet{Hoff2007}.
To see this, note the mean of $[\widetilde{\vu}_i|\cdot]$ is $\vS_u^{-1}\vm_u = \frac{1}{\sigma^2 + 1}\frac{1}{d_i} \vN_i^{u \trp} \vE_i \vv_i$ and the covariance is $\vS_u^{-1} = \left(d_i^2 + \frac{d_i^2}{\sigma^2}\right)^{-1}$.
The full conditional distribution
\begin{align*}
    [\widetilde{\vu}_i|\cdot] & \propto \exp\left\{-\frac{1}{2}tr\left[-2\widetilde{\vu}'_i d_i \vN_i^{u \trp} \vE_i \vv_i \left( \frac{1}{\sigma^2 + 1} + \frac{1}{\sigma^4 + \sigma^2} \right)\right] \right\} \\
    & = \exp\left\{-\frac{1}{2}tr\left[-2\widetilde{\vu}'_i d_i \vN_i^{u \trp} \vE_i \vv_i \left( \frac{1}{\sigma^2} \right)\right] \right\},
\end{align*}
which is the kernel of the von-Mises Fisher distribution.
The same result holds for $\widetilde{\vv}_i$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Relationship to algorithmic SVD}\label{sec:ASVD}

Computing the SVD of $\vY$,
\begin{align*}
    \vY & = \vU \vD \vV' \\
    \vY & = \vU_{-i} \vD_{-i} \vV'_{-i} + d_{i} \vu_{i} \vv'_{i} \\
    \vY - \vU_{-i} \vD_{-i} \vV'_{-i} & = \vE_{-i} =  d_{i} \vu_{i} \vv'_{i} \\
    \frac{1}{d_i}\vE_{-i}\vv_{i} & = \vu_{i},
\end{align*}
so the $i$th basis function can be expressed as a function of the data and other basis functions.
The mean of the full conditional distribution $[\widetilde{\vu}_i|\cdot]$ is $\vS_u^{-1}\vm_u = \frac{1}{\sigma^2 + 1}\frac{1}{d_i} \vN_i^{u \trp} \vE_i \vv_i$, and $E[\widetilde{\vu}_i|\cdot] \rightarrow \frac{1}{d_i} \vN_i^{u \trp} \vE_i \vv_i$ as $\sigma^2 \rightarrow 0$.
Mapping to the original space, $\vN_i^{u} E[\widetilde{\vu}_i|\cdot] = \frac{1}{d_i} \vN_i^{u} \vN_i^{u \trp} \vE_i \vv_i = \frac{1}{d_i} \vE_i \vv_i$.
While not shown here, the same argument applies for $\vV$.
Therefore, when the covariance is taken to be the identity matrix, the posterior mean of the basis functions is equivalent to the C-SVD basis functions.

To see the relationship visually, we repeat one of the simulation conducted in Section~\ref{sec:simulations} with $SNR = 5$, $k = 5$, and set the correlation matrices $\vC_u$ and $\vC_v$ to be the identity.
Here, we still estimate the basis function specific variance $\sigma^2_{u,i}$ and $\sigma^2_{v,i}$.
We obtain 10000 samples from the posterior, discarding the first 5000 as burnin.
The resulting estimates for the $\vU$ and $\vV$ basis functions are shown in Figure~\ref{fig:IdentityBasis}, where the posterior mean of the basis functions (blue) is nearly identical to the C-SVD estimates (red).
In all cases, we see the 95\% intervals (blue shaded region) cover the C-SVD estimates but has ~95\% coverage of the true line (black).



% Figure environment removed


\FloatBarrier

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Projected normal distribution}\label{sec:PND}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let $\vz_j \sim N_n(\vec{0}, \vOmega), j = 1, ..., K$ and $\vZ = [\vz_1, ..., \vz_K]$.
Then $\vW = \vZ/ \|\vZ\| \sim PN(\vec{0}, \vOmega)$ where $\vw_j$ is a length-$n$ directional vector with $n-1$ angles $\vtheta_j = [\theta_{1,j}, \theta_{2,j}, ..., \theta_{n-1,j}]$.
Using spherical coordinates, $r_j = \|\vz_j\| = \sqrt{z_{1,j}^2 + \cdots + z_{n,j}^2}$,
\begin{align*}
    w_{1,j} & = \cos(\theta_{1, j}) \\
    w_{2,j} & = \sin(\theta_{1, j}) \cos(\theta_{2, j}) \\
     & \vdots \\
    w_{n-1,j} & = \sin(\theta_{1, j}) \cdots \sin(\theta_{n-2, j})\cos(\theta_{n-1, j}) \\
    w_{n,j} & = \sin(\theta_{1, j}) \cdots \sin(\theta_{n-2, j})\sin(\theta_{n-1, j})
\end{align*}
and
\begin{align*}
    z_{1,j} & = r_j \cos(\theta_{1, j}) \\
    z_{2,j} & = r_j \sin(\theta_{1, j}) \cos(\theta_{2, j}) \\
     & \vdots \\
    z_{n-1,j} & = r_j \sin(\theta_{1, j}) \cdots \sin(\theta_{n-2, j})\cos(\theta_{n-1, j}) \\
    z_{n,j} & = r_j \sin(\theta_{1, j}) \cdots \sin(\theta_{n-2, j})\sin(\theta_{n-1, j})
\end{align*}
with $r_j \geq 0$, $\theta_{1,j}, \theta_{2,j}, ..., \theta_{n-2,j} \in [0, \pi]$, and $\theta_{n-1,j} \in [0, 2\pi]$.
Augmenting the distribution with its latent length $r_j$, we get the joint density of $(r_j, \vw_j)$ is
\begin{align*}
    p(r_j, \vw_j) = (2 \pi)^{-n/2}|\vOmega|^{-1/2}\exp\left\{-\frac{1}{2}(r_j \vw_j)'\vOmega^{-1}(r_j \vw_j) \right\}r_i^{n-1} \mathbb{I}(\vw_i \in \mathcal{V}_{1,n}),
\end{align*}
where the area element on the unit sphere is $r_j^{n-1}sin^{n-2}(\theta_{1,j})sin^{n-3}(\theta_{2,j})...sin(\theta_{n-2,j})dr_jd\theta_{1,j}d\theta_{2,j}...d\theta_{n-1,j}$.
For properties of this distribution, see \citet{Hernandez-Stumpfhauser2017}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Additional simulation figures}\label{sec:SimFigs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Figure environment removed

\FloatBarrier

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Pacific decadal oscillation}\label{sec:PDO}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Figure environment removed


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsubsection{Computing the PDO}\label{sec:PDOcomputation}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


There are various approaches to computing the PDO from monthly SST data - we provide one such approach.
Assume the data is organized as $SST(\vs,t)$ for $\vs \in \mathcal{D}, t \in T$.
\begin{enumerate}
    \item Compute the mean SST for each month at each location (i.e., the average January, February, ... temperature at location $\vs$ across all years). For each location, $\overline{SST}_J(\vs) = E(SST(\vs,t))$ for $t \in J \subset T, J =$ January, ..., December
    \item Center the data by month (subtract the monthly mean), to produce a monthly anomaly data set. For each location, $SSTM(\vs, t) = SST(\vs,t) - \overline{SST}_J(\vs)$ for  $t \in J \subset T, J =$ January, ..., December
    \item Subtract the global mean of SST making sure to weight each cell appropriately based on its cell size. For each location and time, $SSTC(\vs,t) = SSTM(\vs,t) - E(SSTM(\vs, t))$
    \item Remove the temporal trend - For each location, regress the time-series on an intercept and time vector and then subtract the temporal trend
    \begin{enumerate}
        \item Let $\vX = [\vec{1}, 1:T]$
        \item Compute the BLUP estimate at each location as $\vbeta(\vs) = \vX(\vX'\vX)^{-1}\vX'SSTC(\vs,:)$
        \item $SSTA(\vs, :) = SSTC(\vs,:) - \vX\vbeta(\vs)$
    \end{enumerate}
\end{enumerate}

Then $SSTA(\vs, t)$ $\vs \in \mathcal{D}, t \in T$, which is a data set containing monthly sea surface temperature anomalies, can be used to compute the PDO index.








\end{document}

