 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
\documentclass[letter,twocolumn]{autart}    % Enable this line and disable the 
                                     % preceding line to obtain a two-column 
                                     % document whose style resembles the
                                     % printed Automatica style.

\pdfminorversion=4

\footskip = 37pt
\textheight = 652pt

\addtolength\voffset{-25pt}
\addtolength\textheight{52pt}
\addtolength\footskip{-10pt}
\addtolength\hoffset{-8pt}
\setlength\marginparsep{0pt}
\setlength\marginparwidth{0pt}
% I added 20 pts to the following!
\addtolength\textwidth{13pt}
\addtolength\columnsep{-10pt}

%For indicating what is recently edited
\newcommand{\new}[1]{{\color{blue} #1}}
%\renewcommand{\new}[1]{#1}
\newcommand{\red}[1]{{\color{red} #1}}

% The following packages can be found on http:\\www.ctan.org
%\usepackage[english]{babel}
\usepackage[normalem]{ulem}
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  
\usepackage{cite}
\usepackage{color}
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[numbers]{natbib}
\usepackage{wrapfig} 

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remarks}[theorem]{Remarks}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}  
\newtheorem{problem}[theorem]{Problem}
%\newtheorem*{standaloneproof}[theorem]{Algorithm}
\newenvironment{problemformulation} {\paragraph*{Problem Formulation}}{\hfill$\square$}
%\newenvironment{problemfurmulation} {\paragraph*{Proof of {#1} {#2} :}}{\hfill$\square$}

\usepackage{enumitem}
\renewcommand{\theenumi}{\alph{enumi}}

\newcommand{\naturals}{\mathbb{N}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\cplx}{\mathbb{C}}
\newcommand{\EX}{\mathbb{E}}
\newcommand{\range}{\mathcal{R}}


\newcommand{\Ac}{\mathcal{A}}
\newcommand{\Bc}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\Ec}{\mathcal{E}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Ic}{\mathcal{I}}
\newcommand{\Jc}{\mathcal{J}}
\newcommand{\Kc}{\mathcal{K}}
\newcommand{\Lc}{\mathcal{L}}
\newcommand{\Mc}{\mathcal{M}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Qc}{\mathcal{Q}}
%\newcommand{\Rc}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Tc}{\mathcal{T}}
\newcommand{\Uc}{\mathcal{U}}
\newcommand{\Vc}{\mathcal{V}}
\newcommand{\Wc}{\mathcal{W}}
\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Zc}{\mathcal{Z}}
\newcommand{\abf}{\mathbf{a}}
\newcommand{\bbf}{\mathbf{b}}
\newcommand{\cbf}{\mathbf{c}}
\newcommand{\dbf}{\mathbf{d}}
\newcommand{\fbf}{\mathbf{f}}
\newcommand{\nbf}{\mathbf{n}}
\newcommand{\ubf}{\mathbf{u}}
\newcommand{\vbf}{\mathbf{v}}
\newcommand{\xbf}{\mathbf{x}}
\newcommand{\ybf}{\mathbf{y}}
\newcommand{\zbf}{\mathbf{z}}
\newcommand{\Abf}{\mathbf{A}}
\newcommand{\Bbf}{\mathbf{B}}
\newcommand{\Dbf}{\mathbf{D}}
\newcommand{\Ibf}{\mathbf{I}}
\newcommand{\Kbf}{\mathbf{K}}
\newcommand{\Mbf}{\mathbf{M}}
\newcommand{\Pbf}{\mathbf{P}}
\newcommand{\Qbf}{\mathbf{Q}}
\newcommand{\Rbf}{\mathbf{R}}
\newcommand{\Wbf}{\mathbf{W}}
\newcommand{\alphab}{{\boldsymbol{\alpha}}}
\newcommand{\deltab}{\boldsymbol{\delta}}
\newcommand{\sigmab}{{\boldsymbol{\sigma}}}
\newcommand{\Sigmab}{\boldsymbol{\Sigma}}
\newcommand{\Gammab}{\boldsymbol{\Gamma}}
\newcommand{\xib}{\boldsymbol{\xi}}
\newcommand{\nub}{\boldsymbol{\nu}}
\newcommand{\thetab}{\boldsymbol{\theta}}
\newcommand{\Pib}{\boldsymbol{\Pi}}
\newcommand{\etab}{\boldsymbol{\eta}} 
\newcommand{\phib}{\boldsymbol{\phi}}
\newcommand{\Pf}{\mathfrak{P}}

\newcommand{\EDMD}[3]{\operatorname{EDMD}(#1,#2,#3)}
\newcommand{\Kapprox}{K_{\operatorname{approx}}}
\newcommand{\Kedmd}{K_{\operatorname{EDMD}}}
\newcommand{\Kexact}{K_{\operatorname{exact}}}
\newcommand{\Knoisy}{K_{\operatorname{noisy}}}
\newcommand{\Knredmd}{K_{\operatorname{NREDMD}}}
\newcommand{\until}[1]{\{1,\dots,#1\}}
\newcommand{\zuntil}[1]{\{0,\dots,#1\}}
\newcommand{\rank}[1]{\operatorname{rank}(#1)}
\newcommand{\map}[3]{#1:#2 \rightarrow #3}
\newcommand{\deltaxx}{\Delta D^N(X,\Delta X)}
\newcommand{\deltayy}{\Delta D^N(Y,\Delta Y)}
\newcommand{\ybias}{Y_{\bias}^N}
\newcommand{\xbias}{X_{\bias}^N}
\newcommand{\ycov}{Y_{\cov}}
\newcommand{\xcov}{X_{\cov}}
%\newcommand{\dnx}{D^N(X)}
%\newcommand{\dny}{D^N(Y)}
%\newcommand{\tdnx}{\tilde{D}^N(X)}
%\newcommand{\tdny}{\tilde{D}^N(Y)}
\newcommand{\dx}{D(X)}
\newcommand{\dy}{D(Y)}
\newcommand{\psix}{\Psi(X)}
\newcommand{\psixp}{\Psi(X^+)}
\newcommand{\dxs}{D(X_s)}
\newcommand{\dys}{D(Y_s)}
\newcommand{\dxn}[1]{D(X_{#1})}
\newcommand{\dyn}[1]{D(Y_{#1})}
\newcommand{\tdx}{\tilde{D}(X)}
\newcommand{\tdy}{\tilde{D}(Y)}
\newcommand{\Span}{\operatorname{span}}
\newcommand{\blkdiag}{\operatorname{blkdiag}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\re}{{\operatorname{re}}}
\newcommand{\im}{{\operatorname{im}}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\noisy}{{\operatorname{noisy}}}
\newcommand{\bias}{{\operatorname{bias}}}
\newcommand{\exact}{{\operatorname{exact}}}
\newcommand{\cov}{{\operatorname{cov}}}
\newcommand{\rows}{{\operatorname{rows}}}
\newcommand{\cols}{{\operatorname{cols}}}
\newcommand{\rown}{{\operatorname{\sharp rows}}}
\newcommand{\coln}{{\operatorname{\sharp cols}}}
\newcommand{\basis}{{\operatorname{basis}}}
\newcommand{\dist}{{\operatorname{dist}}}
\newcommand{\ssd}{{\operatorname{SSD}}}
\newcommand{\tssd}{{\operatorname{T-SSD}}}
\newcommand{\pred}{{\operatorname{pred}}}
\newcommand{\edmd}{\operatorname{EDMD}}

\newcommand{\fbedmd}[3]{\operatorname{FB-EDMD}(#1,#2,#3)}
\newcommand{\Kfi}[1]{K_{F,#1}}
\newcommand{\Kbi}[1]{K_{B,#1}}
\newcommand{\Mci}[1]{M_{C,#1}}
\newcommand{\ic}{\mathcal{I}_C}


\newcommand{\restr}[2]{#1 \!\! \restriction_{#2}}


\newcommand{\cssd}{C_{\ssd}}
\newcommand{\dssd}{D_{\ssd}}
\newcommand{\dtssd}{D_{\tssd}}
\newcommand{\Kssd}{K_\ssd}
\newcommand{\Ktssd}{K_\tssd}
\newcommand{\ssddx}{\dssd(X)}
\newcommand{\ssddy}{\dssd(Y)}
\newcommand{\dtssdx}{\dtssd(X)}
\newcommand{\dtssdy}{\dtssd(Y)}
\newcommand{\ctssd}{C_{\tssd}}
% command for in-neigbor
%\newcommand{\inn}[1]{{\operatorname{\Nc_{\operatorname{in}}(#1)}}}
\newcommand{\inn}{\Nc_{\operatorname{in}}} 
%\newcommand{\outn}[1]{{\operatorname{\Nc_{\operatorname{out}}(#1)}}}
\newcommand{\outn}{\Nc_{\operatorname{out}}}
\newcommand{\flag}{{\operatorname{flag}}}
%\newcommand{\null}{{\operatorname{null}}}
\newcommand{\symmintersection}{{\operatorname{Symmetric-Intersection}}}
\newcommand{\cmax}{C_{\max}}
%\newcommand{\ctssd}{C_{\operatorname{T-SSD}}}
\newcommand{\mspec}{\operatorname{mspec}}
\newcommand{\mspecnz}{\operatorname{mspec}_{\neq 0}}
\newcommand{\spec}{\operatorname{spec}}
\newcommand{\specnz}{\operatorname{spec}_{\neq 0}}
\newcommand{\sprad}{\operatorname{sprad}}
\newcommand{\rrmse}{\operatorname{RRMSE}}
\newcommand{\rrmsemax}{\operatorname{RRMSE_{\max}}}
\newcommand{\aug}{\operatorname{aug}}
\newcommand{\Faug}{\Fc^{\aug}}
\newcommand{\Taug}{\Tc^{\aug}}
\newcommand{\Kaug}{\Kc^{\aug}}
\newcommand{\innerprod}[2]{\langle #1, #2 \rangle}
\newcommand{\innerprodF}[2]{\langle #1, #2 \rangle_{\Fc}}
\newcommand{\innerprodFaug}[2]{\langle #1, #2 \rangle_{\Faug}}


\newcommand{\longthmtitle}[1]{\mbox{}{\textit{(#1):}}}
\newcommand{\setdef}[2]{\{#1 \; | \; #2\}}
\newcommand{\setdefb}[2]{\big\{#1 \; | \; #2\big\}}
\newcommand{\setdefB}[2]{\Big\{#1 \; | \; #2\Big\}}
\newcommand{\oprocendsymbol}{\hbox{$\square$}}
\newcommand{\oprocend}{\relax\ifmmode\else\unskip\hfill\fi\oprocendsymbol}
\def\eqoprocend{\tag*{$\square$}}


\parskip 1.1ex
\renewcommand{\baselinestretch}{.98}

\newcommand{\myclearpage}{\clearpage}
\renewcommand{\myclearpage}{}

\allowdisplaybreaks

\newcommand{\marginJC}[1]{\marginpar{\color{red}\tiny\ttfamily#1}}
\newcommand{\marginMH}[1]{\marginpar{\color{blue}\tiny\ttfamily MH: #1}}

\graphicspath{{epsfiles/}}

\begin{document}
\begin{frontmatter}

  % Conference version title: Data-driven approximation of Koopman-
  % invariant subspaces with tunable accuracy

  \title{Modeling Nonlinear Control Systems via Koopman Control Family: 
  	Universal Forms and Subspace Invariance Proximity}

  \thanks{This work was supported by ONR Award N00014-23-1-2353 and
  	NSF Award IIS-2007141.}
    
  \vspace{-10pt}
  
    \author[First]{Masih Haseli}%
    \author[First]{\quad Jorge Cort\'es}
  
    \address[First]{Department of Mechanical and Aerospace
      Engineering, University of California, San Diego,
      \{mhaseli,cortes\}@ucsd.edu}
          
\begin{abstract}
  This paper introduces the Koopman Control Family (KCF), a
  mathematical framework for modeling general discrete-time nonlinear
  control systems with the aim of providing a solid theoretical
  foundation for the use of Koopman-based methods in systems with
  inputs.  We demonstrate that the concept of KCF captures the
  behavior of nonlinear control systems on a (potentially
  infinite-dimensional) function space. By employing a generalized
  notion of subspace invariance under the KCF, we establish a
  universal form for finite-dimensional models, which encompasses the
  commonly used linear, bilinear, and linear switched models as
  specific instances. In cases where the subspace is not invariant
  under the KCF, we propose a method for approximating models in
  general form and characterize the model's accuracy using the concept
  of invariance proximity.  We end by discussing how
  % the proposed
  % framework naturally lends itself to the incorporation of data-driven
  % methods in modeling and control.
  the proposed framework naturally lends itself to data-driven
  modeling of control systems.
\end{abstract}

%% WILL ADD THIS FOR THE FINAL VERSION
% \begin{keyword}
%   Nonlinear system identification, Koopman operator, Dynamic Mode
%   Decomposition, accurate prediction
% \end{keyword}
  
\end{frontmatter}

\section{Introduction}
The Koopman operator approach to dynamical systems has gained
widespread attention in recent years. While traditional state-space
methods for nonlinear systems rely on the description of system
trajectories, the Koopman viewpoint provides an equivalent formulation
of the system behavior using a linear operator acting on a vector
space of functions.
% The Koopman paradigm leads to advantageous algebraic structures
% that can be exploited for computationally efficient learning and
% predictions.
The Koopman operator framework yields beneficial algebraic
constructions that can be leveraged for efficient computational
learning and prediction.  These benefits have motivated researchers to
consider extending the framework to control systems.  However, unlike
the case of autonomous systems\footnote{Consistent with the literature
	on the Koopman operator, we use the term ``autonomous'' to describe
	systems without input.}, this has proven difficult due to the fact
that the role of input is fundamentally different from the state's
role: without a priori knowledge of the input signal, there is not
enough information to predict the system's trajectories since the
choice of input can drastically alter the system behavior.
% the theoretical structures enabling the use of the Koopman operator
% for control systems have proven to be more difficult to construct.
% are still at their infancy and there are many problems that yet have
% to be explored.
% 
% \marginJC{This feels a bit generic: what are the many problems?  What
	%   are they related to? We could mention specific areas, we could also
	%   explain that the role of the input is fundamentally different from
	%   the state, and this raises many challenges in the context of the
	%   Koopman operator, we could say both. } \marginMH{Please see the
	%   changes. I rather not to talk about the unsolved problems since many
	%   people have claimed to have solved them and it might cause some
	%   emotional distress for them.}
% 
Our aim here is to provide a comprehensive mathematical framework for
Koopman operator-based modeling of control systems.

\textit{Literature Review:} The Koopman operator~\citep{BOK:31}
provides an alternative description of nonlinear \emph{autonomous}
systems that encodes the system behavior through the evolution of
functions (a.k.a., observables) belonging to a vector space. Even
though the system might be nonlinear, the Koopman operator is always
linear. This inherent linearity gives rise to favorable algebraic
properties, leading to powerful tools to analyze complex dynamical
systems~\citep{IM:05,CWR-IM-SB-PS-DSH:09,MB-RM-IM:12} for which typical
state-space and geometric methods are cumbersome. However, linearity
comes at the expense of the infinite--dimensional nature of the
operator. To make possible its direct and efficient implementation on
digital computers, one needs to develop finite-dimensional
descriptions for it. This generally relies on the concept of subspace
invariance~\citep{SLB-BWB-JLP-JNK:16}. If a finite-dimensional subspace
is invariant under the operator, then one can restrict the operator to
the subspace and represent its action with a matrix given a chosen
basis. This has led to a search for invariant subspaces through a
variety of techniques, including the direct identification of
eigenfunctions (which span invariant
subspaces)~\citep{MK-IM:20,EK-JNK-SLB:21}, optimization and neural
network-based
methods~\citep{NT-YK-TY:17,BL-JNK-SLB:18,EY-SK-NH:19,SEO-CWR:19,SP-NAM-KD:21,MS:21-L4DC,MK:23-tac},
and efficient algebraic searches~\citep{MH-JC:22-tac,MH-JC:21-tcns}.

Even without a finite-dimensional invariant subspace available, one
can still approximate the action of the Koopman operator on any
finite-dimensional subspace via an orthogonal projection.  Prominent
data-driven methods in this category are Dynamic Mode Decomposition
(DMD)~\citep{PJS:10,JHT-CWR-DML-SLB-JNK:13} and its variant, Extended
Dynamic Mode Decomposition (EDMD)~\citep{MOW-IGK-CWR:15,MK-IM:18}.
% In this case, the quality of the approximation depends on the
% quality of the chosen finite-dimensional space and therefore
% characterizing the accuracy of approximation and finding
% appropriate subspaces which give rise to accurate approximation
% become important.
For such methods, criteria that must be balanced to choose the
finite-dimensional space are the relevance of the dynamical
information captured by the subspace and the accuracy of the resulting
approximation.  The work~\citep{MH-JC:23-csl} provides a tight upper
bound on the worst-case prediction error on a subspace, providing a
measure of the quality of the subspace independently of the chosen
basis.  The works~\citep{HL-DMT:20,FN-SP-FP-MS-KW:23} provide several
error bounds for accuracy of DMD, EDMD, and extensions to
Koopman-based control models.
%The works~\citep{HL-DMT:20,FN-SP-FP-MS-KW:23} provide
%error bounds for Koopman-based predictions while~\citep{MH-JC:23-csl}
%provides a tight upper bound on the worst-case prediction error on a
%subspace, providing a measure of the quality of the subspace
%independently of the chosen basis. 
The work~\citep{MH-JC:23-auto} provides an algebraic algorithm to
approximate Koopman-invariant subspaces of an arbitrary
finite-dimensional space with tunable accuracy.

The algebraic properties of the Koopman operator have been used in a
myriad applications, including fluid
dynamics~\citep{CWR-IM-SB-PS-DSH:09}, stability
analysis~\citep{AM-IM:16,BY-IRM:21,SAD-AMV-CJT:22,CMZ-AM:21}, reachability
analysis~\citep{SB-SB-PSD-ARG-KP:21,BU-DT-UV:22,HB-AA-ZL-NO:23,WS-NS-MK-YTC-SH:23},
safety-critical control~\citep{CF-YC-ADM-JWB:20,VZ-EB:23,MB-DP:22},
%cryptography~\cite{SS-RS-FA:22}, 
and robotics~\citep{DB-XF-RBG-CDR-RC:20,LS-KK:21,GM-IA-TDM:23}.
% 
%\marginMH{To Jorge: Should I put a reference to Fabio's work here?
	%  {\color{red} JC: Yes!}}
%\marginMH{Done! Also, I removed one reference related to cryptography and added another reference form the same authors that is related to control.}
% 
% Despite the lack of a well-defined Koopman operator for control
% systems Therefore, extending the Koopman theory to control systems
% is naturally of utmost interest to the controls community.
The fact that the Koopman operator is only formally defined for
autonomous systems has not been an obstacle for the development of
many data-based methods inspired by it to construct low-complexity
representations of control systems.  Many advances do not directly
require an operator-theoretic approach for the open control system,
but instead rely on the Koopman operator of the unforced system and
address control design based on control Lyapunov
functions~\citep{AN-SHS-JSK:22,VZ-EB:23-neurocomputing} or feedback
linearization~\citep{DG-VK-FP:22}.  A significant amount of attention
has been devoted to deriving finite-dimensional forms by lifting to
higher dimensions.  Due to their simplicity, lifted linear models are
the most popular in the
literature~\citep{SLB-BWB-JLP-JNK:16,MK-IM-automatica:18}
and lead to highly efficient computational algorithms. Such models,
however, are not capable of capturing certain structures, such as
terms containing the products of inputs and states, which are
prevalent in control-affine nonlinear systems. For these, the
works~\citep{BH-XM-UV:18,SP-SEO-CWR:20,DG-DAP:21,RS-JB-FA:23} propose
the use of bilinear lifted models based on geometric arguments relying
on the control-affine structure. The work~\citep{LCI-RT-MS:22} proposes
a different lifted form based on invariant subspaces for the Koopman
operator associated with the unforced system. An interesting
alternative is to model the system by switching between several
Koopman operators, each associated with the system under a different
constant~\citep{SP-SK:19,MB-JPH:23} or piecewise
constant~\citep{AS-DE:17,AS-AM-DE:18} input signal.  The
work~\citep{MK-IM-automatica:18} takes a different approach and
considers the system behavior under all possible infinite input
sequences, defining a Koopman operator for the control system on a
function space whose members' domain is the Cartesian product of the
state space and all possible input sequences. This is perhaps the most
direct approach in terms of an operator-theoretic viewpoint for
controls systems. However, given the reliance on infinite input
sequences, working with finite-time restrictions should be done with
care. Here, we take a different operator-theoretic approach to capture
the behavior of control systems that we find easier to work with on
finite-dimensional subspaces with only finitely many input steps
available.

\textit{Statement of Contributions:} Our goal here is to provide a
solid theoretical framework to model general discrete-time nonlinear
\emph{control} systems based on Koopman operator theory.  The starting
point of our approach is the observation made in the literature that
if the input is a constant signal, then the control system becomes an
autonomous dynamics.
% The main intuition behind our approach, building on the observation
% made in the literature, is that if the input is a constant signal,
% then the control system becomes an autonomous dynamics.
Therefore, any Koopman-theoretic model for the control system must
reduce to the conventional Koopman operator associated with the
corresponding autonomous system.  Motivated by this, we define the
concept of \emph{Koopman control family (KFC)} as the collection of
all Koopman operators associated with constant-input autonomous
dynamics derived from the control system. We show that the KCF can
completely capture the control system's behavior on a potentially
infinite-dimensional function space.  Since dealing with
infinite-dimensional operators is computationally intractable, we
provide a theoretical structure for finite-dimensional models whose
construction is based on projection operators, analogous to the case
of autonomous systems.  To find a general finite-dimensional form for
Koopman-based models for the control system, we rely on a generalized
notion of subspace invariance. Specifically, we show that on a
common-invariant subspace for the KCF, the finite-dimensional model
always has a specific \emph{``input-state separable''}
form. Remarkably, the linear, bilinear, and switched linear models
commonly used in the literature are all special cases of the
input-state separable form.  Since KCF contains uncountably many
operators (given the infinite choices for the constant input signal),
directly finding a common invariant subspace is challenging.  To
tackle this, we parametrize the KCF with one operator, termed
augmented Koopman operator, and show that invariant subspaces under
this augmented operator lead to common invariant subspaces for the
KCF. As a result, the problem of working with uncountably many
operators simplifies to working with a single linear operator on an
extended function space.  Similarly to the case of autonomous systems,
finding an exact and informative finite-dimensional common invariant
subspace for the KCF is generally challenging and in some cases might
not even exist. To address this, we define the concept of invariance
proximity under an operator, which enables us to extended our results
to approximations on non-invariant subspaces and provide a bound on
the accuracy of the resulting approximated models for the control
system.  Our final contribution shows how the results of the paper can
directly be used in data-driven modeling applications.

\textit{Notation:} The symbols $\naturals$, $\real$, and $\cplx$,
represent the sets of natural, real, and complex numbers,
respectively.  Given $A \in \cplx^{m \times n}$, we denote its
transpose, pseudo-inverse, conjugate transpose, Frobenius norm and
range space by $A^T$, $A^\dagger$, $A^H$, $\|A\|_F$, and $\range(A)$,
respectively. If $A$ is square, $A^{-1}$ and $\Tr(A)$ denote its
inverse and trace respectively. When all eigenvalues of $A$ are real,
$\lambda_{\min}(A)$ and $\lambda_{\max}(A)$ denote the smallest and
largest eigenvalues of~$A$.  We use $I_m$ and $\mathbf{0}_{m\times n}$
to denote the $m \times m$ identity matrix and $m\times n$ zero matrix
(we drop the indices when appropriate). We denote the $2$-norm of the
vector $v \in \cplx^n$ by $\|v\|_2$. Given sets $S_1$ and $S_2$, their
union and intersection are represented by $S_1 \cup S_2$ and
$S_1 \cap S_2$, respectively. Also, $S_1 \subseteq S_2$ and
$S_1 \subsetneq S_2$ mean that $S_1$ is a subset and proper subset of
$S_2$, respectively. Given the vector space $\Vc$ defined on the field
$\cplx$, $\dim \Vc$ denotes its dimension. Given a set
$\Sc \subseteq \Vc$, $\Span(\Sc)$ is a vector space comprised of all
linear combinations of elements in $\Sc$. Given vector spaces $\Vc_1$
and $\Vc_2$, we define
$\Vc_1+\Vc_2 := \setdef{v_1 + v_2}{v_1 \in \Vc_1, \, v_2 \in \Vc_2}$.
Given functions $f$ and $g$ with appropriate domains and co-domains,
$f \circ g$ denotes their composition.  Let $f: A \times B \to C$ be a
multivariable function yielding $f(a,b)$ for $(a,b) \in A\times
B$. Then, $\restr{f}{b=b^*}:A \to C$ is defined as
$\restr{f}{b=b^*}(a) := f(a,b^*)$ for $a \in A$. If $F$ consists of
multivariable functions of the form $f: A \times B \to C$, then
$\restr{F}{b = b^*}:=\setdef{\restr{f}{b=b^*}}{f \in F}$. Given a
positive measure $\mu$ on a set $A$ and functions $f,g: A \to \cplx$,
we define their $L_2$ inner product as
$\innerprod{f}{g}_{L_2(\mu)}:= \int_A f(x)\bar{g}(x) d\mu(x)$, where
$\bar{g}$ is the complex conjugate of $g$. The $L_2$ norm of $f$ is
defined as $\|f\|_{L_2(\mu)} = \sqrt{\innerprod{f}{f}_{L_2(\mu)}}$.
We drop the dependency on the measure $\mu$ when the context is clear.

\section{Preliminaries}\label{sec:preliminaries}
In this section, we review notions and results regarding the Koopman
operator, Extended Dynamic Mode Decomposition, and the concept of
consistency index.

\subsection{Koopman Operator}\label{sec:prelim-Koopman}
Here, we briefly explain the Koopman operator associated with a
dynamical system and its properties following the terminology
in~\cite{MB-RM-IM:12}. Consider a discrete-time system
\begin{align}\label{eq:dynamical-sys}
	x^+ = T(x),
\end{align}
with state space $\Xc \subseteq \real^n$.  Consider a linear function
space $\Fc$ (defined on the field $\cplx$) comprised of
functions form $\Xc$ to $\cplx$ and assume it is closed under
composition with $T$, i.e., $f \circ T \in \Fc$ for all $f \in
\Fc$. We define the Koopman operator $\Kc: \Fc \to \Fc$ as
\begin{align}\label{eq:Koopman-def}
	\Kc f = f \circ T.
\end{align}
It is easy to verify that~\eqref{eq:Koopman-def} is a linear operator,
i.e.,
\begin{align}\label{eq:Koopman-spatial-linear}
	\Kc (\alpha f + \beta g) = \alpha \Kc f + \beta \Kc g, \; \forall
	f,g \in \Fc, \; \forall \alpha, \beta \in \cplx. 
\end{align}
The Koopman operator's action on a given function can be viewed as
pushing forward the function values across all system's trajectories
by one time step. We can repeatedly apply the Koopman operator on a
function $f \in \Fc$ to predict its evolution on any system trajectory
$\{x(i)\}_{i=0}^\infty$ as
\begin{align}\label{eq:multistep-Koopman-nocontrol}
	f(x(i)) = \Kc^i f(x(0)), \quad \forall i \in \naturals_0.
\end{align}
Since $\Kc$ is a linear operator, we can define its
eigendecomposition. We say the function $\phi \in \Fc$ is an
\emph{eigenfunction} of $\Kc$ with \emph{eigenvalue} $\lambda$ if
\begin{align}\label{eq:Koopman-eig-def}
	\Kc \phi = \lambda \phi.
\end{align}
By comparing~\eqref{eq:multistep-Koopman-nocontrol}
and~\eqref{eq:Koopman-eig-def}, one can see that Koopman
eigenfunctions evolve linearly on system trajectories,
\begin{align}\label{eq:eigs-temporal-linearity}
	\phi(x(i)) = \lambda \phi(x(i-1)), \quad \forall i \in \naturals.
\end{align}
We refer to~\eqref{eq:eigs-temporal-linearity} as \emph{temporal
	linear evolution} of eigenfunctions. This temporal linearity of
eigenfunctions combined with the
linearity~\eqref{eq:Koopman-spatial-linear} of the operator on the
space $\Fc$ enables us to linearly predict function values on system
trajectories. Specifically, given eigenfunctions
$\{\phi_k\}_{k=1}^{N_k}$ with corresponding eigenvalues
$\{\lambda_k\}_{k=1}^{N_k}$, one can write the evolution of the
function $f = \sum_{k=1}^{N_k} c_k \phi_k$ on the system's
trajectories as
\begin{align*}%\label{eq:function-Koopman-eig-prediction}
	f(x(i)) = \sum_{k=1}^{N_k} c_k \lambda_k^i \phi_k (x(0)), \; \forall i \in \naturals_0.
\end{align*}
This equation is of utmost importance since it provides a linear
structure facilitating the prediction of nonlinear
systems~\cite{MK-IM-automatica:18,MK-IM:20} as well as learning the
system's behavior from
data~\cite{CWR-IM-SB-PS-DSH:09,MH-JC:22-tac,MH-JC:23-auto}.
% 
One should keep in mind that, in general, to capture the full state of
the system, one might need $\Fc$ to be infinite dimensional since it
must be closed under composition with~$T$.

Next, we define the concept of subspace invariance under the Koopman
operator. 
A subspace $\Gc \subseteq \Fc$ is \emph{Koopman invariant}
if $\Kc f \in \Gc$ for all $f \in \Gc$. Koopman eigenfunctions
trivially span invariant subspaces. 

\begin{remark}\longthmtitle{Simplifying Notation For Vector-Valued
    Functions}\label{r:notation-overload}
  {\rm 
    For convenience, we introduce some notation simplifying the
    operation of the Koopman operator on finite-dimensional spaces.  Let
    $\Psi: \Xc \to \cplx^s$ be a vector-valued map represented as
    $\Psi(\cdot) = [\psi_1(\cdot),\ldots, \psi_s(\cdot)]^T$, where
    $\psi_i: \Xc \to \cplx$ for all $i \in \until{s}$. We define the
    span of the elements of $\Psi$ and action of Koopman operator on the
    elements of $\Psi$ as
    \begin{align*}
      \Span(\Psi) & := \Span(\{\psi_1,\ldots, \psi_s\}),
      \\
      \Kc \Psi & := [\Kc \psi_1, \ldots, \Kc \psi_s]^T = \Psi \circ T.
    \end{align*}
    Given a finite-dimensional subspace $\Hc \subset \Fc$, we
    often describe a basis for it by a vector-valued map
    $\Phi: \Xc \to \cplx^{\dim(\Hc)}$ satisfying
    $\Hc = \Span(\Phi)$.  \oprocend }
\end{remark}

An important property of finite-dimensional Koopman-invariant
subspaces is that one can capture the action of the operator by a
matrix once a basis is chosen. Formally, given the invariant subspace
$\Sc \subseteq \Fc$ with basis $\Psi: \Xc \to \cplx^{\dim(\Sc)}$,
there exists a unique matrix
$K \in \cplx^{\dim(\Sc) \times \dim(\Sc)}$ such that
\begin{align}\label{eq:Koopman-matrix-invariant}
	\Kc \Psi= \Psi \circ T= K \Psi.
\end{align}
This equation combined with the linearity of the operator allow us to
easily calculate the action of the operator on any function in
$\Sc$. Formally, given any function $f \in \Sc$ with description
$f (\cdot) = w^T \Psi(\cdot)$ where $w \in \cplx^{\dim(\Sc)}$, one has
\begin{align}\label{eq:Koopman-predictor-invariant}
	\Kc f = w^T K \Psi.
\end{align}

% \marginMH{To Jorge: Please do not remove this section (specially the
	%   remark at the end). We later will heavily use this part which
	%   shortens the rest of the paper and gives our analysis some
	%   structure.}

The concept of subspace invariance is of utmost importance since it
allows us to operate on finite-dimensional subspaces and use numerical
matrix computation for prediction, as laid out in
equations~\eqref{eq:Koopman-matrix-invariant}-\eqref{eq:Koopman-predictor-invariant}.

Even if the subspace $\Sc \subseteq \Fc$ is \emph{not} invariant under
$\Kc$, it is still possible to the use the notion of subspace
invariance to \emph{approximate} the action of $\Kc$ on $\Sc$. To
achieve this, one usually utilizes $\Pc_{\Sc}: \Fc \to \Fc$, the
orthogonal projection operator (given an inner product on $\Fc$) on
$\Sc$. Observe that the space $\Sc$ is invariant under the operator
$\Pc_{\Sc} \Kc: \Fc \to \Fc$; hence,
equations~\eqref{eq:Koopman-matrix-invariant}-\eqref{eq:Koopman-predictor-invariant}
are valid when we substitute in them the Koopman operator $\Kc$ by
$\Pc_{\Sc} \Kc$. Let $\Kapprox$ be the matrix calculated by applying
equation~\eqref{eq:Koopman-matrix-invariant} to the operator
$\Pc_{\Sc} \Kc$. Then, this matrix provides an approximation for the
action of $\Kc$ on $\Sc$ as follows
\begin{align}\label{eq:Koopman-matrix-noninvariant}
	\Kc \Psi= \Psi \circ T \approx \Pc_{\Sc} \Kc \Psi= \Kapprox \Psi.
\end{align}
Moreover, the analogous but approximated version of function
prediction in~\eqref{eq:Koopman-predictor-invariant} is given by
\begin{align}\label{eq:Koopman-predictor-noninvariant}
	\Kc f \approx \Pc_{\Sc} \Kc f = w^T \Kapprox \Psi.
\end{align}

% We conclude this section by a remark regarding the general form of
% finite-dimensional models.

\begin{remark}\longthmtitle{General Linear Form and Subspace
    Invariance}\label{r:invariance-general-form}
  {\rm When dealing with the action of the Koopman operator on
    finite-dimensional spaces, we use linear models that are either
    exact,
    cf.~\eqref{eq:Koopman-matrix-invariant}-\eqref{eq:Koopman-predictor-invariant},
    or approximated,
    cf.~\eqref{eq:Koopman-matrix-noninvariant}-\eqref{eq:Koopman-predictor-noninvariant}.
    Note that in either case the model has the same form. It is in
    this sense that we say that the general finite-dimensional form of
    Koopman-based models is linear. Note that this general form is a
    consequence of the notion of subspace invariance (whether the
    subspace is actually Koopman-invariant or not). \oprocend }
\end{remark}


\subsection{Extended Dynamic Mode Decomposition}\label{subsec:EDMD}
% The Koopman operator is generally infinite dimensional and its exact
% implementation on digital hardware would require infinite
% resources. At the same time, we are interested in inferring
% properties of the Koopman operator (i.e., the system) from data.
In many engineering applications, the system dynamics is unknown and
we only have access to data from the system's trajectories.  The
Extended Dynamic Mode Decomposition (EDMD)
method~\cite{MOW-IGK-CWR:15} reviewed here
% addresses both issues of infinite dimensionality and information
% extraction from data.
uses data to approximate the action of the Koopman operator on a given
\emph{finite-dimensional} space of functions.
% Before introducing this function space we remark an important
% convention regarding the data-driven applications.

\begin{remark}\longthmtitle{Use of Real-valued Basis Functions in
    Data-Driven Applications}\label{r:real-dictionary}
  {\rm All the
	systems in this paper are defined on state and input spaces with
	real-valued elements. Consequently, the Koopman operator's action on
	any pair of complex-conjugate functions leads to another
	complex-conjugate pair, which can be captured by a pair of
	real-valued functions. Hence, even though we develop our theory
	based on complex functions, in data-driven applications, we work
	with bases with real-valued elements to simplify the numerical
	operations, without loss of generality\footnote{Given a
		vector-valued function $\Psi$ with real-valued elements,
		$\Span(\Psi)$ contains complex-valued functions since we employ
		$\cplx$ as the underlying field.}.  \oprocend
              }
\end{remark}

To specify the function space, EDMD uses a dictionary comprised of
$N_{\Psi}$ functions form $\Xc$ to $\real$. Formally, we define our
dictionary as a vector-valued function
\begin{align*}
	\Psi(\cdot) = [\psi_1(\cdot),\ldots, \psi_{N_{\Psi}}(\cdot)]^T,
\end{align*}
where $\psi_1, \ldots, \psi_{N_{\Psi}} \in \Fc$ are the dictionary
elements. To approximate the behavior of the Koopman operator (and
therefore the system) on $\Span(\Psi)$, EDMD uses data snapshots from
the trajectories in two matrices $X,X^+ \in \real^{n \times N}$ such
that
\begin{align}\label{eq:data-snapshots}
	x_i^+ = T(x_i), \; \forall i \in \until{N},
\end{align}
where $x_i$ and $x_i^+$ are the $i$th columns of matrices $X$ and
$X^+$ respectively. For convenience, we define the action of the
dictionary on data matrix $X$ (similarly for any data matrix) as
\begin{align*}
	\psix = [\Psi(x_1), \Psi(x_2), \ldots, \Psi(x_n)] \in \real^{N_{\psi} \times N}.
\end{align*}
Note that based on~\eqref{eq:Koopman-def}
and~\eqref{eq:data-snapshots}, one can see
$ \psixp = \Psi \circ T(X) = \Kc \Psi(X)$.  Hence, the dictionary
matrices $\psix$ and $\psixp$ capture the behavior of the Koopman
operator on $\Span(\Psi)$. EDMD approximates the action of the
operator by solving a least-squares problem
\begin{align}\label{eq:EDMD-optimization}
	\underset{K}{\text{minimize}} \| \psixp -  K \psix \|_F,
\end{align} 
with the following closed-form solution
\begin{align}\label{eq:EDMD-closed-form}
	\Kedmd = \psixp  \psix^\dagger.
\end{align}
Throughout this paper, we make the following assumption.

\begin{assumption}\longthmtitle{Full Rank Dictionary
		Matrices}\label{a:full-rank}
	$\psix$ and $\psixp$ have full row rank.  \oprocend
\end{assumption}

Note that Assumption~\ref{a:full-rank} implies that the element of
$\Psi$ are linearly independent. Also, it implies that data in $X$ and
$X^+$ are diverse enough to distinguish between functions in
$\Span(\Psi)$. Moreover, if Assumption~\ref{a:full-rank} holds,
$\Kedmd$ is the unique solution of~\eqref{eq:EDMD-optimization}.

The matrix $\Kedmd$ captures relevant information about the system's
behavior and can be used to approximate the action of the Koopman
operator on $\Span(\Psi)$. Formally, we define the EDMD predictor of
$\Kc \Psi$ by EDMD as
\begin{align}\label{eq:EDMD-dictionary-predictor}
	\Pf_{\Kc \Psi}^{\edmd} := \Kedmd \Psi.
\end{align}
Similarly, for an arbitrary function $f \in \Span(\Psi)$ with
description $f(\cdot) = w^T \Psi(\cdot)$ for $w \in \cplx^{N_{\Psi}}$,
one can define the EDMD predictor of $\Kc f$ as
\begin{align}\label{eq:EDMD-function-predictor}
	\Pf_{\Kc f}^{\edmd} := w^T \Kedmd \Psi.
\end{align}
The
predictors~\eqref{eq:EDMD-dictionary-predictor}-\eqref{eq:EDMD-function-predictor}
are special cases of the approximations
in~\eqref{eq:Koopman-matrix-noninvariant}-\eqref{eq:Koopman-predictor-noninvariant},
where the orthogonal projection corresponds to the $L_2(\mu_X)$ inner
product, with empirical measure
\begin{align}\label{eq:empirical-measure}
	\mu_X = \frac{1}{N} \sum_{i=1}^{N} \delta_{x_i},
\end{align}
where $\delta_{x_i}$ is the Dirac measure defined on the $i$th column
of $X$ (see e.g.,~\cite{MK-IM:18}). The quality of predictors
in~\eqref{eq:EDMD-dictionary-predictor}-\eqref{eq:EDMD-function-predictor}
depends on the quality of $\Span(\Psi)$ in terms of being close to
invariant under $\Kc$. If $\Span(\Psi)$ is invariant under $\Kc$, then
the predictors
in~\eqref{eq:EDMD-dictionary-predictor}-\eqref{eq:EDMD-function-predictor}
are exact and match
equations~\eqref{eq:Koopman-matrix-invariant}-\eqref{eq:Koopman-predictor-invariant},
respectively. Determining closeness to invariance necessitates an
appropriate metric, which is the concept we review next.
% In order to find an appropriate dictionary whose span is close to
% being invariant under the Koopman operator, we need a data-driven
% measure that can characterize the quality of the dictionary. We
% address this next.

\subsection{Consistency Index Measures The Dictionary's Quality}\label{subsec:consistency-index}
We recall the concept of temporal forward-backward consistency to
measure how close a dictionary span is to being Koopman invariant.
Given a dictionary $\Psi$ with real-valued elements and data matrices
$X, X^+$, the \emph{consistency index}~\cite{MH-JC:23-csl}~is
% $ \ic (\Psi,X,X^+) = \lambda_{\max} (I - K_F K_B)$ 
\begin{align*}
	\ic (\Psi,X,X^+) = \lambda_{\max} (I - K_F K_B),
\end{align*}
where $K_F = \psixp \psix^\dagger$ and $K_B = \psix \psixp^\dagger$
are EDMD matrices applied forward and backward in time\footnote{Note
	that this definition is equivalent but different
	from~\cite[Definition~1]{MH-JC:23-csl}. The data matrices in this
	paper are transpose of the ones in~\cite{MH-JC:23-csl}; however,
	this transposition does not affect the value of the consistency
	index.}. When the context is clear, we drop the arguments and
use~$\ic$.

The intuition behind the consistency index is that when $\Span(\Psi)$
is Koopman-invariant, the forward and backward EDMD matrices $K_F$ and
$K_B$ are inverse of each other. Otherwise, their product will deviate
from the identity matrix, with the consistency index providing a
measure for this deviation.  The consistency index is easy to compute
based on data and its value only depends on the vector space, not on
the choice of particular basis. The following result states a key
property of relevance to the ensuing discussion.

\begin{theorem}\longthmtitle{\cite[Theorem~1]{MH-JC:23-csl}:
		$\sqrt{\ic}$ Bounds the Relative 
		$L_2$-norm error for EDMD's Koopman
		Predictions}\label{t:RRMSE-bound-sprad-consistency} Given
	Assumption~\ref{a:full-rank} for dictionary $\Psi$, data matrices
	$X,X^+$, and the empirical measure~\eqref{eq:empirical-measure},
	%  define the empirical measure based on data in $X$
	%  as $\mu_X = \frac{1}{N} \sum_{i=1}^{N} \delta_{x_i}$, where
	%  $\delta_{x_i}$ is the Dirac measure defined on the $i$th column of
	%  $X$. Then
	\begin{align*}
		\sqrt{\ic(\Psi,X,X^+)} = \max_{f \in \Span(\Psi)} \frac{\|\Kc f -
			\Pf_{\Kc f}^{\edmd}\|_{L_2(\mu_X)}}{{\| \Kc f \|_{L_2(\mu_X)}}}  ,
	\end{align*}
	where 
	%  $x_i$ is the $i$th column of $X$, 
	the maximum is taken over
	all functions where the denominator is nonzero (when denominator is
	zero, the EDMD prediction is exact and there is no prediction
	error), and $\Pf_{\Kc f}^{\edmd}$ is defined
	in~\eqref{eq:EDMD-function-predictor}.  \oprocend
\end{theorem}
\smallskip

Theorem~\ref{t:RRMSE-bound-sprad-consistency} provides a crucial tool
for the approximation of Koopman-invariant subspaces. The consistency
index provides a closed-form formula that can be used as a cost
function in data-driven dictionary learning. Since
$\Pf_{\Kc f}^{\edmd}$ does not depend on the choice of basis for
$\Span(\Psi)$ (cf.~\cite[Lemma 7.1]{MH-JC:23-auto}), based on
Theorem~\ref{t:RRMSE-bound-sprad-consistency}, the consistency index
does not depend on the choice of the basis of $\Span(\Psi)$ and
provides a \emph{tight} upper bound for the relative prediction error
for all (uncountably many) functions in the space\footnote{The
	residual error of EDMD $\| \psixp - \Kedmd \psix \|_F$ depends on
	the choice of basis and is not suitable for measuring quality of
	$\Span(\Psi)$. In fact, it is easy to
	show~\cite[Example~1]{MH-JC:23-csl} that, if $\Span(\Psi)$ is not
	invariant but contains one exact eigenfunction, then one can find a
	linear transformation on the dictionary to make the residual error
	\emph{arbitrarily} close to zero.}.
	
\section{Motivation and Problem Statement}
Consider the discrete-time control system
\begin{align}\label{eq:control-system}
	x^+ = \Tc(x,u), \quad x \in \Xc \subseteq \real^n, \; u \in \Uc
	\subseteq \real^m, 
\end{align}
where $x$ and $u$ are the state and input vectors, and $\Xc$ and $\Uc$
are the state and input spaces, respectively. Note that no special
structure (e.g., control affine) is assumed on the
system~\eqref{eq:control-system}. Our goal is to provide a Koopman
operator theory description of the nonlinear control system. The
challenge for extending the concept of the Koopman operator to systems
with inputs is that unlike the autonomous
system~\eqref{eq:dynamical-sys}, the behavior of the control
system~\eqref{eq:control-system} cannot be determined without
knowledge of the input sequence\footnote{Given an infinite input
  sequence, one can determine~\cite{MK-IM-automatica:18} the system's
  behavior completely and define a Koopman operator for it. Moreover,
  if one closes the loop by means of feedback, the system takes the
  autonomous form~\eqref{eq:dynamical-sys} and hence has a
  well-defined Koopman operator, see~e.g.,~\cite{EK-JNK-SLB:21}.}.
%{\color{red} Even though there have been many advances regarding the
	%  extension of Koopman theory to control systems, there is still a
	%  vacuum}
%
%\marginJC{Mentioning ``advances'', we should at the very least provide
	%  references. But then, this becomes more of a lit review, and should
	%  be in the intro. I'd rather move it there, and here simply state
	%  what our primary aim is: provide a rigorous mathematical description
	%  of how to employ the Koopman operator for control systems in both
	%  the infinite- and finite-dimensional cases. We aim to build on this
	%  description to articulate how data-driven methods can be used in the
	%  modeling and identification of control systems.}
%\marginMH{For space reasons, I removed the references to the literature and mention them in the introduction.}
% 
%regarding a general mathematical structure that is rigorous in both
%infinite- and finite-dimensional cases and can also be used in
%data-driven cases. Formally, our goal in this paper is to address the
%following problem.
Here, we aim to provide a rigorous mathematical description of how to
employ the Koopman operator for control systems in both infinite- and
finite-dimensional cases, and articulate its application in
data-driven modeling of control systems. We formalize the problem
next.

\begin{problem}\longthmtitle{Challenges Regarding the Extension of
    Koopman Theory to Control Systems}
  We aim to provide a mathematical framework based on the Koopman
  operator to
  \begin{enumerate}
  \item capture the behavior of control
    system~\eqref{eq:control-system};
  \item provide a generalized notion of subspace invariance, leading
    to a general form for finite-dimensional Koopman-based models that
    \begin{enumerate}[label=(\roman*)]
    \item encompasses commonly used linear, bilinear, and linear switched
      control models;
    \item is consistent with the linear form for autonomous systems
      in~\eqref{eq:Koopman-matrix-invariant}
      and~\eqref{eq:Koopman-matrix-noninvariant}: if we set the
      input to be constant (which yields an autonomous system), the
      finite-dimensional form should reduce to the lifted linear
      model in~\eqref{eq:Koopman-matrix-invariant}
      and~\eqref{eq:Koopman-matrix-noninvariant};
    \end{enumerate}
    % encompasses commonly used linear, bilinear, and linear switched
    % control models;
  \item evaluate the accuracy of such finite-dimensional models;
  \item use for data-driven identification of control systems.
  \end{enumerate}
\end{problem}

% \red{
	%\begin{remark}\longthmtitle{Autonomous vs. Homogeneous}
	%  In the Koopman community the terms ``non-autonomous'' and
	%  ``autonomous'' are used to describe systems with and without
	%  input. However, in classical control theory these words are used to describe
	%  time-varying and time-invariant systems, respectively. In this
	%  paper, we follow the classical terminology and use the words
	%  ``non-homogeneous'' and ``homogeneous'' to describe system with and
	%  without input.  \oprocend
	%\end{remark}
	%}
%%
%\marginJC{Instead of homogeneous/non-homogeneous (which does not seem
	%  natural to me!), how about simply dynamics for (1) and controlled
	%  dynamics for (14)?}
%%
%\marginMH{I have changed homogeneous to autonomous throughout the
	%  text. I will remove the remark when I write the introduction and add
	%  a footnote explaining our use of the word ``autonomous'' (I have not
	%  removed the remark so I do not forget).}
%%


\section{Koopman Control Family and General Form for
  Finite-Dimensional Models}
Here, we take the first step towards extending Koopman theory to the
control system case and providing a generalized notion of subspace
invariance. As we show below, this ultimately leads to a
finite-dimensional model that is the extension of the linear form
in~\eqref{eq:Koopman-matrix-invariant}.  We start from the observation
that, if we fix the input as a constant, we get an autonomous system
in the form of~\eqref{eq:dynamical-sys} which admits a well-defined
Koopman operator. Motivated by this idea, one can model the
system~\eqref{eq:control-system} by switching between constant input
autonomous systems\footnote{The idea of modeling a control system via
  constant input systems has already been considered several times in
  the literature~\cite{AS-DE:17,AS-AM-DE:18,SP-SK:19}.  }.  Formally,
consider the family of autonomous systems created by setting the input
as a constant signal
\begin{align}\label{eq:switched-family}
	x^+ = \Tc_{u^*} (x) := \Tc(x, u \equiv u^*), \quad u^* \in \Uc.
\end{align}
Note that any trajectory $\{x_k\}_{k=0}^L \subset{\Xc}$ of
system~\eqref{eq:control-system} generated with input sequence
$\{u_k\}_{k=0}^{L-1} \subset{\Uc}$, can be generated by applying the
autonomous systems $\Tc_{u_k}$, $k \in \{0,\ldots,L-1\}$ subsequently
on the initial condition $x_0$. Hence, we have
\begin{subequations}
	\begin{align}
		x_k
		&= \Tc_{u_{k-1}} \circ \cdots \circ \Tc_{u_0}
		(x_0) \label{eq:mulitstep-control-family}
		\\
		&= \Tc_{u_{k-1}} (x_{k-1}), \quad k \in
		\until{L}. \label{eq:switched-constant-input-evolution} 
	\end{align}  
\end{subequations}
Noting that the members of the family $\{\Tc_{u^*}\}_{u^* \in \Uc}$
are all autonomous, we can define Koopman operators for each of them
as in~\eqref{eq:Koopman-def}, leading to the following definition.

\begin{definition}\longthmtitle{Koopman Control Family (KCF)}
	Let $\Fc$ be a vector space (over the field $\cplx$) of
	complex-valued functions with domain $\Xc$ that is closed under
	composition with members of $\{\Tc_{u^*}\}_{u^* \in \Uc}$.  The
	associated \textbf{Koopman control family (KCF)} is the family of
	operators $\{\Kc_{u^*} : \Fc \to \Fc\}_{u^* \in \Uc}$ where, for
	each $u^* \in \Uc$, $\Kc_{u^*}$ defined by
	$ \Kc_{u^*} f = f \circ \Tc_{u^*}$, for all $f \in \Fc $, is the
	Koopman operator corresponding to the dynamics~$\Tc_{u^*}$.
	\oprocend
\end{definition}

Similarly to the multi-step
prediction~\eqref{eq:multistep-Koopman-nocontrol} under the Koopman
operator of an autonomous system, one can
use~\eqref{eq:mulitstep-control-family} and the definition of KCF to
provide a similar identity for the non-autonomous case,
\begin{align*}
	f(x_k) = \Kc_{u_0} \Kc_{u_1} \ldots \Kc_{u_{k-1}} f (x_0), \quad
	\forall f \in \Fc. 
\end{align*}
Note that the identity above is \emph{exact} and general and can be
utilized for all trajectories of~\eqref{eq:control-system}.

Even though a KCF on an appropriate function space can completely
capture the behavior of control system~\eqref{eq:control-system}, the
infinite-dimensional nature of the function space $\Fc$ can make its
implementation on digital computers impossible. To address this issue,
we need finite-dimensional representations for KCF. A simple guiding
observation in this regard is that if we have an exact
finite-dimensional representation and fix the input to be constant,
then the system is autonomous and the model should reduce to a linear
finite-dimensional case similar to
equation~\eqref{eq:Koopman-matrix-invariant}. This leads us to the
concept of common invariant subspaces under KCF.

\begin{definition}\longthmtitle{Common Invariant Subspaces Under the
		Koopman Control Family}
	The space $\Lc \subseteq \Fc$ is a \textbf{common invariant
		subspace} under the KCF if $\Kc_{\bar{u}} f \in \Lc$, for all
	$\Kc_{\bar{u}} \in \{\Kc_{u^*}\}_{u^* \in \Uc}$ and all $f \in \Lc$.
	\oprocend
\end{definition}

Finite-dimensional common invariant subspaces under the KCF
$\{\Kc_{u^*}\}_{u^* \in \Uc}$ are of utmost importance because the
action of all its members on such subspaces can be captured
\emph{exactly} by matrices. This provides a general framework for
treating control systems. Next, we show that finite-dimensional common
invariant subspaces under KCF lead to a universal form of models that
can be viewed as a generalization
of~\eqref{eq:Koopman-matrix-invariant} to the case of control
systems\footnote{Similarly to the case of autonomous systems
	(cf.~Remark~\ref{r:invariance-general-form} and its preceding
	discussions), we rely on a notion of subspace invariance to find a
	general finite-dimensional form. In the following sections, we also
	rigorously investigate approximations on non-invariant subspaces.}.
% \marginMH{I added the footnote above for impatient
	%   readers. The reader might think the assumption of common invariance
	%   is strong (which is). But, they might not read the next 10 pages to
	%   reach the non-invariant case (I once reviewed a paper that got
	%   rejected because the reviewers did not read the entire paper and got
	%   mad at an assumption that was relaxed later in the paper. I do not
	%   want this to happen to us!)}
% \marginJC{Ok. Let's eliminate it for the final version}

\begin{theorem}\longthmtitle{General Form on Common Invariant
		Subspaces: \textbf{Input-State
			Separable}}\label{t:common-invariant-separates}
	The Koopman control family has a finite-dimensional (of dimension
	$s$) common invariant subspace if and only if there exist functions
	$\Psi:\Xc \to \cplx^s$ and $A: \Uc \to \cplx^{s \times s}$ such that
	for all $(x,u) \in \Xc \times \Uc$,
	\begin{align}\label{eq:composition-to-separation}
		\Psi(x^+) = \Psi \circ \Tc (x,u) = \Ac(u) \Psi(x).
	\end{align}
	In this formulation, the common invariant subspace under the KCF is
	described by $\Span(\Psi)$.
\end{theorem}
\begin{pf}
	$(\Rightarrow):$ Let $\Sc \subset \Fc$, with $\dim \Sc = s$, be a
	common invariant subspace of the Koopman control family
	$\{\Kc_{u^*}\}_{u^* \in \Uc}$.  Let functions
	$\{\psi_1, \ldots, \psi_s\}$ be a basis for $\Sc$ and define the
	vector-valued function $\Psi: \Xc \to \cplx^s$ as
	$\Psi(x) = [\psi_1(x),\ldots, \psi_s(x)]^T$ for all $x \in
	\Xc$. Since $\Sc = \Span(\Psi)$ is invariant under the KCF, for each
	$u^* \in \Uc$, there exists a matrix
	$K_{u^*} \in \cplx^{s \times s}$
	% capturing the effect of the Koopman control family on $\Sc$ with
	% respect to the basis $\Psi$, i.e.,
	such that 
	\begin{align}\label{eq:operator-by-matrix}
		\Kc_{u^*} \Psi(x) = K_{u^*} \Psi (x), \; \forall x \in \Xc, 
	\end{align}
	where we have used~\eqref{eq:Koopman-matrix-invariant} and the
	notation in Remark~\ref{r:notation-overload}.  Define then the
	matrix-valued function $\Ac: \Uc \to \cplx^{s \times s}$ as
	\begin{align*}
		\Ac(u) = K_u, \; \forall u \in \Uc.
	\end{align*}
	Noting that equation~\eqref{eq:operator-by-matrix} holds for all
	$u^* \in \Uc$, one can use the definition of
	$\Ac: \Uc \to \cplx^{s \times s}$ and write
	\begin{align*}
		\Psi \circ \Tc(x,u) = K_u \Psi(x) = \Ac(u) \Psi(x), \; \forall x \in \Xc, \, \forall u \in \Uc,
	\end{align*}
	hence proving equation~\eqref{eq:composition-to-separation}.
	
	$(\Leftarrow):$ Assume equation~\eqref{eq:composition-to-separation}
	holds. Hence, for all $u^* \in \Uc$,
	\begin{align*}
		\Psi \circ \Tc(x, u \equiv u^*) = \Psi \circ \Tc_{u^*} (x) = \Ac(u
		\equiv u^*) \Psi(x), \forall x \in \Xc. 
	\end{align*}
	Given that $A (u \equiv u^*)$ is a constant matrix, for any function
	$f \in \Span(\Psi)$ in the form of $f = v_f^T \Psi$ with
	$v_f \in \cplx^s$, we have
	\begin{align*}
		\Kc_{u^*} f = f \circ \Tc_{u^*} = v_f^T \Psi \circ \Tc_{u^*} =
		v_f^T \Ac(u \equiv u^*) \Psi \in \Span(\Psi). 
	\end{align*}
	This equality holds for all $u^* \in \Uc$; hence, $\Span(\Psi)$
	is invariant under the Koopman control family
	$\{\Kc_{u^*}\}_{u^* \in \Uc}$. \qed
\end{pf}

The input-state separable form~\eqref{eq:composition-to-separation}
(note the composition on the left and the matrix product on the right)
can be viewed as a generalization
of~\eqref{eq:Koopman-matrix-invariant}, which describes the exact
action of the Koopman operator on an invariant subspace.  Importantly,
the condition in Theorem~\ref{t:common-invariant-separates} is
necessary and sufficient; therefore the input-state separable form is
general. In fact, as we show next, it provides a mathematical
framework encompassing common Koopman-inspired descriptions of the
control system~\eqref{eq:control-system}.
% such as linear switched, linear, and bilinear models.
It is easy to see that the linear switched systems used
in~\cite{SP-SK:19} are a special case of the input-state separable
form where the input space $\Uc$ contains finitely many elements. We
formalize this observation in the following result that follows
directly from the definition of input-state separable form.

\begin{lemma}\longthmtitle{Linear Switched Form is a Special Case of
		Input-State Separable Form}\label{l:switched-linear-separable}
	For system~\eqref{eq:control-system}, let $\Uc = \{u_1,\ldots,u_l\}$
	and assume the system has a lifted linear switched
	representation of the form
	\begin{align}\label{eq:linear-switched}
		\Psi(x^+) \!=\! A_u \Psi(x), \; A_u \in \{A_{u_1}, \dots, A_{u_l}
		\}\subset \real^{N_\Psi \times N_\Psi}, 
	\end{align}
	where $\Psi: \Xc \to \real^{N_{\Psi}}$ with $N_\Psi \in \naturals$
	and $u \in \Uc$.
	% Then the KCF associated with the system has a finite-dimensional
	% common invariant subspace and has an input-state separable
	% representation.
	Then, $\Span(\Psi)$ is a finite-dimensional common invariant
	subspace under the KCF associated with the system
	and~\eqref{eq:linear-switched} is an input-state separable
	representation.  \oprocend
\end{lemma}
\smallskip

Next, we show that the commonly used linear and bilinear Koopman-based
models are also special cases of the input-state separable form.

\begin{lemma}\longthmtitle{Linear and Bilinear Forms are Special Cases
		of Input-State Separable Form}\label{l:linear-bilinear-separable}
	Assume the system~\eqref{eq:control-system} has a finite-dimensional
	lifted representation of the form
	% $\psi(x^+) = A \psi(x) + \sum_{i=1}^m B_i \psi(x) u_i + C u$
	\begin{align}\label{eq:2nd-order-model}
		\psi(x^+) = A \psi(x) + \sum_{i=1}^m  B_i \psi(x) u_i + C u,
	\end{align}
	where $\psi: \Xc \to \real^{N_{\psi}}$ with $N_\psi \in
	\naturals$. Moreover, $A, B_i \in \real^{N_\psi \times N_\psi}$ for
	$i \in \until{m}$ and $C \in \real^{N_\psi \times m}$ where $m$ is
	the dimension of the input vector. 
	% Then the associated KCF has a finite-dimensional common invariant
	% subspace and has an input-state separable representation.
	Then $\Span(\psi) + \Span(1_\Xc)$ is a finite-dimensional
        common invariant subspace under the KCF associated with the
        system\footnote{Here, $1_\Xc: \Xc \to \cplx$ is the constant
          function defined by $1_\Xc(x) = 1$ for all $x \in \Xc$.},
        which has the input-state separable representation
	\begin{align}\label{eq:linear-bilinear}
          \begin{bmatrix}
            \psi(x^+)
            \\
            1_\Xc(x^+)
          \end{bmatrix}
          = \begin{bmatrix}
              A+\sum_{i=1}^m  u_i B_i
              & Cu
              \\
              0
              &
                1
            \end{bmatrix}
            % 
          \begin{bmatrix}
            \psi(x)
            \\
            1_\Xc(x)
          \end{bmatrix}.
          % 
	\end{align}
      \end{lemma}
\begin{pf}
  % First, consider the constant function $1_\Xc: \Xc \to \cplx$, such
	% that $1_\Xc(x) = 1$ for all $x \in \Xc$. Without loss of
	% generality, we assume that $1_\Xc \in \Fc$ (if that is not the
	% case, one can replace $\Fc$ with
	% $\tilde{\Fc} = \Fc + \Span(1_\Xc)$ which leads to a well defined
	% KCF since constant functions are trivial eigenfunctions of any
	% Koopman operator).
	Using the constant function $1_\Xc$, one can rewrite the
	dynamics~\eqref{eq:2nd-order-model} as~\eqref{eq:linear-bilinear},
	which is in input-state separable form. Therefore, based on
	Theorem~\ref{t:common-invariant-separates},
	$\Span(\psi) + \Span(1_\Xc)$ is a finite-dimensional common
	invariant subspace under the KCF associated with the system. \qed
\end{pf}

% \marginMH{I added this remark since it is important and I also believe that some (most in my opinion) reviewers are not familiar with basic boolean logic. Actually, one of the reviewers had an issue with this! Asked for a linear system while say common invariant subspace is too strong.}
\begin{remark}\longthmtitle{Existence of Linear or Bilinear Forms
    Implies Common Invariant Subspaces of KCF} {\rm
    Lemma~\ref{l:linear-bilinear-separable} shows that if a system
    has a linear or bilinear lifted form, then its associated KCF
    has a common invariant subspace. However, the converse does not
    hold, as corroborated by the necessary and sufficient condition
    in Theorem~\ref{t:common-invariant-separates}. Therefore, for a
    system to have a linear or bilinear lifted form, stronger
    conditions than the existence of common invariant subspace under
    KCF are required. \oprocend }
\end{remark}

Note that linear and bilinear models are special cases of the model
in~\eqref{eq:2nd-order-model}. Therefore, the input-state separable
model captures these important special cases.
% We demonstrate the previous results using a simple example.

\begin{example}\longthmtitle{Input-State Separable
		Form}\label{ex:input-states-separable}
{\rm
	Consider
	\begin{align}\label{eq:ex-control-system}
		x_1^+
		&= a x_1 +b u
		\nonumber
		\\
		x_2^+
		&= c x_2 + d x_1^2  + e x_1 u + fu + g \sin(u) + h
	\end{align}
	where $x_1, x_2$ are the state variables and $u$ is the input. The
	system has the input-state separable form
	\begin{align}\label{eq:ex-input-state-separable-form}
		\begin{bmatrix}
			x_1
			\\
			x_2
			\\
			x_1^2
			\\
			1
		\end{bmatrix}^+
		\!\!= 
		\begin{bmatrix}
			a & 0 & 0 & b \,u 
			\\
			e \, u & c & d & fu + g \sin(u) + h 
			\\
			2ab \, u & 0 & a^2 & b^2 u^2 
			\\
			0 & 0 & 0 & 1
		\end{bmatrix}
		\begin{bmatrix}
			x_1
			\\
			x_2
			\\
			x_1^2
			\\
			1
		\end{bmatrix}.
	\end{align}
	Note that for any constant $u\equiv u^*$, the system turns into an
	exact lifted linear form on a Koopman invariant subspace (compare
	with the linear switched model in~\cite{SP-SK:19} and
	Lemma~\ref{l:switched-linear-separable}). If $b = g = 0$,
	\eqref{eq:ex-input-state-separable-form} turns into the following
	bilinear form (cf.~Lemma~\ref{l:linear-bilinear-separable})
	\begin{align*}
		\begin{bmatrix}
			x_1 \\ x_2 \\ x_1^2 \\ 1
		\end{bmatrix}^+
		=  
		\begin{bmatrix}
			a & 0 & 0 & 0 
			\\
			0 & c & d & h 
			\\
			0 & 0 & a^2 & 0
			\\
			0 & 0 & 0 & 1
		\end{bmatrix}
		\begin{bmatrix}x_1 \\ x_2 \\ x_1^2 \\ 1 \end{bmatrix}
		+
		\begin{bmatrix}
			0 & 0 & 0 & 0 
			\\
			e & 0 & 0 & f 
			\\
			0 & 0 & 0 & 0
			\\
			0 & 0 & 0 & 0
		\end{bmatrix}
		\begin{bmatrix}x_1 \\ x_2 \\ x_1^2 \\ 1 \end{bmatrix} u.
	\end{align*}
	If in addition we have $e = 0$, the previous equation can be
	written in linear form. \oprocend
}
\end{example}
\smallskip

So far, we have established the KCF modeling can completely capture
the behavior of the control
system~\eqref{eq:control-system}. Moreover, we have found the general
form of finite-dimensional models on the common invariant subspaces
of~KCF. However, given that, in general, the KCF contains uncountably
many linear operators, one needs to find tractable ways to find or
approximate finite-dimensional common invariant subspaces under the
KCF. We tackle this task in the following sections.


\section{Parameterizing the Koopman Control Family}
%
% Here, we take the first step towards finding a finite-dimensional
% common invariant subspace for the KCF. 
Here we provide a way to parametrize a Koopman Control Family via a
single linear operator defined on an augmented function space. This
allows us to provide an equivalent characterization for a common
invariant subspace under the KCF.

\subsection{Augmented Koopman Operator}
To parametrize the KCF, we first parametrize the family of constant
input systems in~\eqref{eq:switched-family} as the
following augmented dynamical system
\begin{align*}
	\begin{bmatrix}
		x
		\\
		u
	\end{bmatrix}^+
	=
	\begin{bmatrix}
		\Tc(x,u)
		\\
		u
	\end{bmatrix}.
\end{align*}
For convenience, we define the following tuple notation for the system
above
\begin{align}\label{eq:augmented-system}
	(x^+,u^+) =  \Taug(x,u) := (\Tc(x,u), u), 
\end{align}
for $(x,u) \in \Xc \times \Uc$.  Note that
in~\eqref{eq:augmented-system}, $u$ is a part of the state vector and
not an input. The next result shows that this augmented system
captures the behavior of all members of constant-input systems defined
in~\eqref{eq:switched-family}.

\begin{lemma}\longthmtitle{Augmented System Parametrizes the
		Constant-Input Family}\label{l:augsystem-constantinputs}
	For the augmented system~\eqref{eq:augmented-system}, the following
	hold:
	\begin{enumerate}
		\item the set $\Xc \times \{u^*\}$ is forward invariant
		under~\eqref{eq:augmented-system} for all $u^* \in \Uc$;
		\item for any $u^* \in \Uc$, let $\{x_i\}_{i=1}^\infty$ be a
		trajectory of $\Tc_{u^*}$ in~\eqref{eq:switched-family} with
		initial condition $x_0 \in \Xc$ and let
		$\{ (x_i^{\aug}, u_i^{\aug}) \}_{i=1}^\infty$ be a trajectory of
		$\Taug$ starting from $(x_0^{\aug}, u^*) \in \Xc \times \Uc$ with
		$x_0^{\aug} = x_0$. Then, $x_i = x_i^{\aug}$ for all
		$i \in \naturals$. \oprocend
	\end{enumerate}
\end{lemma}
%\begin{proof}
%  (a) Consider the point $(x,u) \in \Xc \times \{u^*\}$. Note that by
%  definition $u = u^*$. Hence,
%  $\Taug(x,u) = \Taug(x,u^*) = (\Tc(x,u^*),u^*)$. As a result, by
%  noting that $\Tc(x,u^*) \in \Xc$
%  (cf. equation~\eqref{eq:control-system}), we have
%  $\Taug(x,u) \in \Xc \times \{u^*\}$.
%
%  (b) We prove this part by induction. For $i=0$, we have
%  $x_i^{\aug} = x_i$ by hypothesis. Now, assuming $x_k^{\aug} = x_k$
%  for $k \in \naturals_0$, we need to prove the same for $k+1$. First,
%  based on part~(a) and noting that
%  $(x_0^{\aug}, u^*) \in \Xc \times \{u^*\}$, we have
%  \begin{align}\label{eq:u-constant}
	%    u_i^{\aug} = u^*, \quad \forall i \in \naturals_0.
	%  \end{align}
%  Now, let $(x_{k+1}^{\aug}, u_{k+1}) = \Taug(x_k^{\aug},u_k)$. By the
%  induction's hypothesis ($x_k^{\aug} = x_k$),
%  equation~\eqref{eq:u-constant}, and the definition of $\Taug$
%  (cf. equation~\eqref{eq:augmented-system}), we have
%  $(x_{k+1}^{\aug}, u_{k+1}) = \Taug(x_k^{\aug},u_k) = (\Tc(x_k,
%  u^*),u^*)$. Hence, $x_{k+1}^{\aug} = \Tc(x_k,u^*)$. However, by
%  comparing $\Tc(x_k,u^*)$ with the definition of $\Tc_{u^*}$
%  in~\eqref{eq:switched-family}, one can conclude
%  $x_{k+1}^{\aug} = \Tc(x_k,u^*) = \Tc_{u^*}(x_k) = x_{k+1}$.
%\end{proof}

\smallskip The proof of Lemma~\ref{l:augsystem-constantinputs}
directly follows from the definition of
system~\eqref{eq:augmented-system} and is omitted for space reasons.
As a result of Lemma~\ref{l:augsystem-constantinputs}(a), if we
restrict the state space of~\eqref{eq:augmented-system} to
$\Xc \times \{u^*\}$ for any $u^* \in \Uc$, we get a well-defined
dynamics, which we denote by $\restr{\Taug}{\Xc \times
	\{u^*\}}$. Moreover, by Lemma~\ref{l:augsystem-constantinputs}(b),
$\restr{\Taug}{\Xc \times \{u^*\}}$ captures the behavior of
$\Tc_{u^*}$ for all $u^* \in \Uc$. It is in this sense that we say
that $\Taug$ on the state space $\Xc \times \Uc$ parametrizes the
family of constant-input systems $\{\Tc_{u^*}\}_{u^* \in \Uc}$.

%Since the augmented system is a homogeneous dynamics in the form
%of~\eqref{eq:dynamical-sys}, we can define a Koopman operator as given
%in~\eqref{eq:Koopman-def}.  To do this, we first choose a proper
%function space.

Since the augmented system is an autonomous dynamics in the form
of~\eqref{eq:dynamical-sys}, we can define a Koopman operator as given
in~\eqref{eq:Koopman-def}.  Appropriately defined, this operator would
encompass the KCF's information, as supported by
Lemma~\ref{l:augsystem-constantinputs}, which connects the augmented
system~\eqref{eq:augmented-system} to the constant-input
systems~\eqref{eq:switched-family}. Nonetheless, before making this
connection, we must first bridge the gap between the state-space of
constant-input systems ($\Xc$) and that of the augmented system
($\Xc \times \Uc$), and define a proper function space. To do this, we
first provide the following definition.

%
%\marginJC{Re: footnote, why? Make it explicit: b/c keeping the
%  argument $u$ on the right later allows consistent manipulation with
%  input-state separable forms??}
%\marginMH{Thank you for your comment. I made the necessary changes.}
%
\begin{definition}\longthmtitle{Control-Independent Extension of
    Functions in $\Fc$ to Domain $\Xc \times
    \Uc$}\label{def:cont-indep-extenstion} 
  Given the function $\phi \in \Fc$ where $\phi: \Xc \to \cplx$, we
  define its \textbf{control-independent extension} to the domain
  $\Xc \times \Uc$ as $\phi_e: \Xc \times \Uc \to \cplx$,
  \begin{align*}
    \phi_e(x,u) = \phi(x) 1_\Uc(u) , \quad \forall (x,u) \in \Xc
    \times \Uc ,     
  \end{align*}
  where $1_\Uc: \Uc \to \cplx$ is defined as $1_\Uc(u) = 1$ for all
  $u \in \Uc$. Similarly, for a vector-valued function
  $\Phi(x) = [\phi_1(x), \ldots, \phi_n(x)]^T$, where $\phi_i \in \Fc$
  for all $i \in \until{n}$, we define
  $\Phi_e(x,u) = [\phi_1(x)1_\Uc(u), \ldots, \phi_n(x)1_\Uc(u)]^T$.
  \oprocend
\end{definition}
\smallskip

One could equivalently define the control-independent extension as
$\phi_e(x,u) = \phi(x)$ for $(x,u) \in \Xc \times \Uc$. However, the
structure of Definition~\ref{def:cont-indep-extenstion} is consistent
with input-state separable forms, which is particularly convenient in
our forthcoming theoretical analysis.  Next, we state straightforward
but useful properties of control-independent extensions that follow
from the definition.

\begin{lemma}\longthmtitle{Control-Independent Extensions'
    Properties}\label{l:control-indep-properties}
  Let $\phi: \Xc \to \cplx$ and $\Phi: \Xc \to \cplx^n$, and let
  $I_\Uc^{n \times n}: \Uc \to \cplx^{n \times n}$ be a constant
  function returning the identity matrix, i.e.,
  $I_\Uc^{n \times n}(u) = I_{n \times n}$ for all $u \in
  \Uc$. Then, for all $(x,u) \in \Xc \times \Uc$,
  \begin{enumerate}
  \item $\phi_e(x,u) = \phi(x)$ and $\Phi_e(x,u) = \Phi(x)$;
  \item $\Phi_e(x,u) = I_\Uc^{n \times n}(u) \Phi(x)$;
  \item for all $f \in \Span(\Phi)$ with description
    $f = v_f^T \Phi$ where $v_f \in \cplx^n$, we have
    $f_e = v_f^T \Phi_e$.  \oprocend
  \end{enumerate}
\end{lemma}
\smallskip

We next define a proper function space for the Koopman operator
associated with the augmented system~\eqref{eq:augmented-system}.

\begin{definition}\longthmtitle{Function Space for
    $\Taug$}\label{def:augmented-operator}
  Let $\Faug$ be a linear space (on the field $\cplx$) of
  complex-valued functions with domain $\Xc \times \Uc$ such that
  \begin{enumerate}
  \item is closed under composition with $\Taug$;
  \item contains $ f \circ \Tc$ for all $f \in \Fc$;
  \item contains the control-independent extension $f_e$ for all
    $f \in \Fc$; % (cf. Definition~\ref{def:cont-indep-extenstion});
    % \item contains the $u$-independent extensions of functions in
    %   $\Fc$: for all $f \in \Fc$, define the function
    %   $f_e: \Xc \times \Uc \to \cplx$ by $f_e(x,u) = f(x)1_\Uc(u)$
    %   for all $(x,u) \in \Xc \times \Uc$ where $1_\Uc(u) \equiv
    %   1$. Then $f_e \in \Faug$;
    % \item for all $u^* \in \Uc$, $\restr{\Faug}{u=u^*} \subseteq \Fc$. \oprocend
  \item for all $u^* \in \Uc$, $\restr{\Faug}{u=u^*} = \Fc$.
    \oprocend
  \end{enumerate}
\end{definition}
%%
%\marginJC{It might make sense to directly define (d) as equality (so
%  that we save ourselves the following lemma, a reviewer pointed this
%  out and it's not a bad idea to get to the more important results
%  earlier).}
%%

Note that, as long as we allow the function spaces $\Fc$ and $\Faug$
to be infinite-dimensional, the conditions in
Definition~\ref{def:augmented-operator} are easy to satisfy. Also,
note that at this time, there are no inner products, norms, or metrics
on these function spaces.  

%The next result shows for the function
%space $\Faug$, the inclusion in (d) must be an equality.
%
%\begin{lemma}\longthmtitle{$\restr{\Faug}{u=u^*} =
%		\Fc$}\label{l:Faug-restriction-is-F}
%	Consider $\Faug$ as defined in
%	Definition~\ref{def:augmented-operator}. Then,
%	$\restr{\Faug}{u=u^*} = \Fc$ for all $u^* \in \Uc$.
%\end{lemma}
%\begin{pf}
%Consider an arbitrary function $f \in
%\Fc$. Definition~\ref{def:augmented-operator}(c) implies that $f_e$
%belongs to $\Faug$ and hence
%$\restr{f_e}{u = u^*} \in \restr{\Faug}{u=u^*}$ for all
%$u^* \in \Uc$. However, $\restr{f_e}{u = u^*} = f$ for all
%$u^* \in \Uc$ since $f_e$ is control-independent. Therefore,
%$f \in \restr{\Faug}{u=u^*}$ for all $u^* \in \Uc$, which proves
%$\Fc \subseteq \restr{\Faug}{u=u^*}$ for all $u^* \in \Uc$.  This,
%combined with Definition~\ref{def:augmented-operator}(d), implies
%the statement. \qed
%\end{pf}

With the function space in place, we define the \textbf{augmented
	Koopman operator} $\Kaug: \Faug \to \Faug$ as
\begin{align}\label{eq:augmented-Koopman}
	\Kaug g = g \circ \Taug, \quad \forall g \in \Faug. 
\end{align}
The augmented operator $\Kaug$ encodes the behavior of the augmented
system~\eqref{eq:augmented-system}.
% As a result, two related natural questions arise
% \begin{enumerate}
	% \item[(i)] Given that $\Taug$ parametrizes the family
	%   $\{\Tc_{u^*}\}_{u^* \in \Uc}$, does the Koopman operator of $\Taug$
	%   parametrize the KCF $\{\Kc_{u^*}\}_{u^* \in \Uc}$?
	% \item[(ii)] If the answer to (i) is affirmative, can invariant
	%   subspaces of the Koopman operator for $\Taug$ help us find common
	%   invariant subspaces for the KCF?
	% \end{enumerate}
	
	
\subsection{Augmented Koopman Operator Parametrizes the Koopman
	Control Family}

Here, we investigate the connection between the augmented operator and
the KCF, and the implications for the search of common invariant
subspaces for the KCF.  The next result shows how $\Kaug$
parameterizes the KCF $\{\Kc_{u^*}\}_{u^* \in \Uc}$.

\begin{lemma}
  \longthmtitle{$\Kaug$ Parametrizes the
    KCF}\label{l:augmentedKoopman-captures-individuals}
  Let $f \in \Faug$. Then for all $u^* \in \Uc$ we have
  $\restr{(\Kaug f)}{u=u^*} = \Kc_{u^*} (\restr{f}{u=u^*})$.
  \oprocend
\end{lemma}
% % 
% \marginJC{Since proof is fairly straightforward, how about commenting
%   it out? I think we still want to keep the lemma statement, b/c it's
%   important to formalize the property.}
% %
% \marginMH{I understand your point. However, this makes me very
%   uncomfortable. Every time I read the statement of the lemma, it
%   seems too good to be true (note that it holds for all $u^*$ and it
%   is really counter intuitive because $f$ can be any arbitrary
%   extension that changes based on $u$ (not just constant input
%   extension) of $\restr{f}{u=u^*}$ to domain $\Xc \times \Uc$). If I
%   was a reviewer, I would directly reject the paper if this lemma was
%   without a proof. This lemma and the next one are the foundation of
%   the entire paper. I do not see how removing proofs improves the
%   quality of a scientific paper. I do understand that many control
%   engineers/practitioners oppose the use of logical arguments and see
%   proofs as a way of showing their skills. However, I believe we
%   should follow the correct scientific method and provide evidence for
%   our claims.}
% 
%\begin{pf}
%  By definition, $\Kaug f(x,u) = f(\Tc(x,u),u)$.~Hence,
%  \begin{align*}
%    \restr{(\Kaug f(x,u))}{u=u^*}
%    % = \restr{\big(f \circ       \Taug(x,u)\big)}{u = u^*}
%    & =
%      f(\Tc(x,u^*), u^*)  
%      = f(\Tc_{u^*}(x), u^*)
%    \\
%    &
%      = \restr{f}{u=u^*} (\Tc_{u^*}(x)) = \Kc_{u^*} [\restr{f}{u=u^*}] (x),
%  \end{align*}
%  for all $u^* \in \Uc$ and all $x \in \Xc$. \qed
%\end{pf}

%Even though its proof is straightforward,
The proof of Lemma~\ref{l:augmentedKoopman-captures-individuals}
directly follows from the definition of $\Kaug$ and is omitted for
space reasons.  Lemma~\ref{l:augmentedKoopman-captures-individuals}
establishes the important fact that the action of $\Kaug$ on $\Faug$
completely captures the effect of $\Kc_{u^*}$ on
$\restr{\Faug}{u=u^*} = \Fc$
(cf.~Definition~\ref{def:augmented-operator}) for all $u^* \in
\Uc$. This shows that $\Kaug$ can be viewed as a parametrization of
the KCF, i.e., by knowing the effect of $\Kaug$ on $\Faug$, one can
calculate the effect of all (potentially uncountably many) members of
the KCF. The next result shows how the augmented Koopman operator can
capture relevant information regarding the evolution of functions in
$\Fc$ under the trajectories of the control
system~\eqref{eq:control-system}.

\begin{lemma}\longthmtitle{Augmented Koopman Operator Predicts the
    Functions Evolutions on  System's
    Trajectories}\label{l:Kaug-captures-function-evolutions-in-F} 
  Let $f \in \Fc$ and denote by $f \circ \Tc \in \Faug$ the function
  created by pushing the values of $f$ one time-step forward through
  the trajectories of $\Tc$.  Let $f_e$ be the control-independent
  extension of $f$ to $\Xc \times \Uc$.  Then,
  $f \circ \Tc = \Kaug f_e$.
  \oprocend
\end{lemma}
% %
% \marginJC{Same comment as above the  proof.}
% \marginMH{Please see my response above.}
%
%\begin{pf}
%  For all $(x,u) \in \Xc \times \Uc$, one can write
%  \begin{align*}
%    \Kaug f_e (x,u)
%    &= f_e(\Taug(x,u)) = f_e (\Tc(x,u), u) 
%    \\
%    % &= f(\Tc(x,u))1_\Uc(u) = f \circ \Tc (x,u),
%      & = f(\Tc(x,u)) = f \circ \Tc (x,u),
%  \end{align*}
%  % where in the second equality we have
%  % used~\eqref{eq:augmented-system} and
%  % Lemma~\ref{l:control-indep-properties}(a).
%  where we have used~\eqref{eq:augmented-system} in the second
%  equality and Lemma~\ref{l:control-indep-properties}(a) in the third
%  equality. \qed
%\end{pf}

The proof of Lemma~\ref{l:Kaug-captures-function-evolutions-in-F}
directly follows from the definition of $\Kaug$ and
Lemma~\ref{l:control-indep-properties}(a), and is omitted for space
reasons.  Lemma~\ref{l:Kaug-captures-function-evolutions-in-F}
provides a crucial tool to analyze the behavior of functions in $\Fc$
on the trajectories of the control system~\eqref{eq:control-system}
(note the similarity of the composition $f \circ \Tc$ with the
definition of the Koopman operator~\eqref{eq:Koopman-def} for
autonomous systems). In this result, observe that even though $\Kaug$
is the Koopman operator associated with~\eqref{eq:augmented-system},
its action on control-independent function extensions leads to the
prediction of the function values on trajectories of the actual
control system~\eqref{eq:control-system}.

The next result provides a link between the invariant subspaces of
$\Kaug$ and common invariant subspaces of the KCF.

\begin{proposition}\longthmtitle{Invariant Subspaces of $\Kaug$
    Characterize Common Invariant Subspaces for the
    KCF}\label{p:family-augmented-invariance} 
  Let $\Sc \subseteq \Faug$ be an invariant subspace under
  $\Kaug$. Then,
  \begin{enumerate}
  \item for all $u^* \in \Uc$, $\restr{\Sc}{u=u^*}$ is an invariant
    subspace of $\Kc_{u^*}$;
  \item if $\restr{\Sc}{u=u_1} = \restr{\Sc}{u=u_2}$ for all
    $u_1, u_2 \in \Uc$, then $\restr{\Sc}{u=u^*}$ (for any
    $u^* \in \Uc$) is a common invariant subspace under the Koopman
    control family $\{\Kc_{u^*}\}_{u^* \in \Uc}$.
  \end{enumerate}
\end{proposition}
\begin{pf}
  (a) First note that $\restr{\Sc}{u=u^*}$ is a vector space for all
  $u^* \in \Uc$. Given any $u^* \in \Uc$, consider an arbitrary
  function $g \in \restr{\Sc}{u=u^*}$. By definition of
  $\restr{\Sc}{u=u^*}$, there exists a function $\tilde{g} \in \Sc$
  such that $g = \restr{\tilde{g}}{u = u^*}$ (note that $\tilde{g}$
  might not be unique). By
  Lemma~\ref{l:augmentedKoopman-captures-individuals}, one can write
	\begin{align*}
		\Kc_{u^*} g = \Kc_{u^*} (\restr{\tilde{g}}{u=u^*})= \restr{(\Kaug
			\tilde{g})}{u=u^*} \in \restr{\Sc}{u=u^*},
	\end{align*}
	where we have used the fact that $\Kaug \tilde{g} \in \Sc$ because
	$\Sc$ is invariant under $\Kaug$. Therefore, $\restr{\Sc}{u=u^*}$ is
	an invariant subspace of~$ \Kc_{u^* }$.
	
	(b) This is a direct consequence of part~(a) and the definition of
	common invariant subspace for the KCF. \qed
\end{pf}

%
% \marginJC{Does $\Sc$ invariant subspace under $\Kaug$ imply that
	% $\restr{\Sc}{u=u_1} = \restr{\Sc}{u=u_2}$ for all
	% $u_1, u_2 \in \Uc$? I guess the answer is not in general, but worth
	% commenting on it -- and maybe provide a simple counterexample?}
% \marginMH{I don't know the answer. I cannot prove it or provide
	% counterexamples. I will address this if I find out. In the following
	% sections we provide additional conditions that make the converse
	% hold.}
%

%Lemma~\ref{l:Kaug-captures-function-evolutions-in-F} and
%Proposition~\ref{p:family-augmented-invariance} provide preliminary
%tools to find general input-state separable models laid out in
%Theorem~\ref{t:common-invariant-separates}. We continue our search for
%such models next.
Proposition~\ref{p:family-augmented-invariance} provides a tool for
the identification of common invariant subspaces under KCF based on
the invariant subspaces of the augmented Koopman operator. However,
checking the condition in
Proposition~\ref{p:family-augmented-invariance}(b) requires one to
compare different vector spaces, which can be cumbersome. In the
following section, we provide more direct conditions that can be
checked easily and lead to input-state separable models, as laid out
in Theorem~\ref{t:common-invariant-separates}.
	
	
	
\section{Input-State Separable Forms via the Augmented Koopman
	Operator} 
Here, we aim to build on
Proposition~\ref{p:family-augmented-invariance} and
Theorem~\ref{t:common-invariant-separates} to provide more specific
practical criteria to identify common invariant subspaces of the KCF
and derive input-state separable models. Based on
Theorem~\ref{t:common-invariant-separates} we know that on a common
invariant subspace, function composition with $\Tc$ leads to functions
that can be written as a linear combination of separable functions in
$x$ and $u$. For convenience, we provide the following definition.

\begin{definition}\longthmtitle{Input-State Separable Functions and
		Their Linear Combinations}\label{def:separable}
	A function $f \in \Faug$ is \textbf{input-state separable} if there
	exist $g: \Uc \to \cplx$ and $h: \Xc \to \cplx$ such that
	$f(x,u) = g(u) h(x)$ for all $x \in \Xc$ and $u \in \Uc$.  A
	function $J$ is an \textbf{input-state separable combination} (or
	\textbf{separable combination} for short) if it can be written as a
	\emph{finite} linear combination of input-state separable functions.
	\oprocend
\end{definition}

Next, we show a property of the bases for spaces of separable
combinations.

\begin{proposition}\longthmtitle{Spaces of Separable Combinations Have
		Separable Bases}\label{p:separable-basis}
	Let $\Sc \subseteq \Faug$ be a finite-dimensional (of dimension
	$s \in \naturals$) subspace comprised of input-state separable
	combinations. Then, for any arbitrary basis
	$\{\phi_1,\ldots, \phi_s\}$ of $\Sc$, the vector-valued function
	$\Phi(x,u) = [\phi_1(x,u), \ldots, \phi_s(x,u)]^T$ can be decomposed
	as the product of two functions as follows
	\begin{align}\label{eq:basis-product-decomposition}
		\Phi(x,u) = G(u) H(x),  \; \forall (x,u)\in \Xc \times \Uc.
	\end{align}
	where $G: \Uc \to \cplx^{s \times l}$ and $H: \Xc \to \cplx^{l}$ for
	some $l \in \naturals$.
	% \footnote{Observe that $G$ produces a matrix of size $s \times l$
		% and $H$ gives a column vector of length $l$.}
\end{proposition}
\begin{pf}
	By hypothesis, for each $i \in \until{s}$, there exists $n_i$ such
	that
	\begin{align}\label{eq:phi_decomposition}
		\phi_i (x,u) = \sum_{j_i = 1}^{n_i} p_{j_i}^i (u) q_{j_i}^i (x),
		\; \forall (x,u)\in \Xc \times \Uc. 
	\end{align}
	for some functions $p_{j_i}^i: \Uc \to \cplx$,
	$ q_{j_i}^i: \Xc \to \cplx$.  Now, consider the space
	\begin{align*}
		Q = \Span \{ q_{j_i}^i: \Xc \to \cplx \; | \; i \in \until{s}, j_i \in \until{n_i} \}.
	\end{align*}
	By construction, $Q$ is finite dimensional, with
	$l:= \dim Q \leq \sum_{i=1}^s n_i$. Let $\{h_1, \ldots, h_l\}$ be a
	basis for $Q$ and construct the vector-valued function
	$H:\Xc \to \cplx^l$ as
	\begin{align*}
		H(\cdot) = [h_1(\cdot), \ldots, h_l(\cdot)]^T.
	\end{align*}
	By construction of $Q$, all functions $q_{j_i}^i$ can be written as
	linear combinations of $\{h_1, \ldots, h_l\}$. Hence, there exist
	vectors $v_{j_i}^i \in \cplx^l$ such that
	\begin{align}\label{eq:q-based-on-H}
		q_{j_i}^i (\cdot)= (v_{j_i}^i)^T H(\cdot), \; \forall i \in \until{s}, \,  j_i \in \until{n_i}.
	\end{align}
	Now, based on~\eqref{eq:phi_decomposition}-\eqref{eq:q-based-on-H},
	for all $i \in \until{s}$, one can write
	\begin{align}\label{eq:phi-new-decomposition}
		\phi_i (x,u) = \sum_{j_i = 1}^{n_i} p_{j_i}^i (u) (v_{j_i}^i)^T
		H(x), \; \forall (x,u) \in \Xc \times \Uc. 
	\end{align}
	Defining the function $G: \Uc \to \cplx^{s \times l}$ as
	\begin{align*}
		G(u) = \begin{bmatrix}
			\sum_{j_1 = 1}^{n_1} p_{j_1}^1 (u) (v_{j_1}^1)^T
			\\
			% \sum_{j_2 = 1}^{n_2} p_{j_2}^2 (u) (v_{j_2}^2)^T
			% \\
			\vdots
			\\
			\sum_{j_s = 1}^{n_s} p_{j_s}^s (u) (v_{j_s}^s)^T
		\end{bmatrix}, \; \forall u \in \Uc,
	\end{align*}
	it follows from~\eqref{eq:phi-new-decomposition} that
	$\Phi(x,u) = G(u) H(x)$. \qed
\end{pf}


With this result in place, we can show how to obtain a common
invariant subspace of the KCF using the invariant subspaces of the
augmented Koopman operator.

\begin{theorem}\longthmtitle{Rank Condition for Identification of
    Common Invariant Subspaces of KCF via
    $\Kaug$}\label{t:rank-common-invariant}
  Let $\Sc \subseteq \Faug$ be a finite-dimensional (of dimension
  $s \in \naturals$) subspace comprised of input-state separable
  combinations that is invariant under~$\Kaug$ and let
  $\Phi(x,u) = G(u) H(x)$ be a decomposition of a basis for $\Sc$,
  where $G: \Uc \to \cplx^{s \times l}$ and $H: \Xc \to \cplx^{l}$.
  % where $H(x) = [h_1(x), \ldots, h_l(x)]^T$.
  If $G(u)$ has full column rank for all $u \in \Uc$, then the space
  $\Hc = \Span(H)$ is a common invariant subspace for the~KCF.
\end{theorem}
\begin{pf}
  Since $\Sc$ is a finite-dimensional invariant subspace under
  $\Kaug$, given the basis $\Phi$, one can represent the action of
  $\Kaug$ on $\Sc$ by a matrix $A \in \cplx^{s \times s}$ as
  \begin{align}\label{eq:Kaug-basis-matrix}
    \Kaug \Phi = A \Phi,
  \end{align}
  where we have used the compact notation in
  Remark~\ref{r:notation-overload}.  Using this and the decomposition
  $\Phi(x,u) = G(u) H(x)$, 
  \begin{align*}
    \Kaug \big(G(\cdot) H(\cdot)  \big) = A \, G(\cdot) H(\cdot).
  \end{align*}
  With this compact description, in order to invoke
  Proposition~\ref{p:family-augmented-invariance}(b), we need to show
  that
  $\restr{\big[ \Span( G(u) H(\cdot) )\big]}{u=u^*} = \Span(G(u^*) H)$
  is the same for all $u^* \in \Uc$.
  % Note that in this equation $G(u^*)$ is a constant matrix
  % and we have used the compact notation in
  % Remark~\ref{r:notation-overload}.
  We show this by establishing
  \begin{align}\label{eq:subspace-restriction-equality}
    \Span(G(u^*) H )= \Span(H) ,  \; \forall u^* \in \Uc. 
  \end{align}
  To show the inclusion from left to right, consider
  $g: \Xc \to \cplx$ with $g \in \Span(G(u^*) H)$.  Hence, there is a
  vector $v_g \in \cplx^s$ such that
  $g(\cdot) = v_g^T G(u^*) H(\cdot)$. Defining
  $w_g = G(u^*)^T v_g \in \cplx^l$, one can write
  $g(\cdot) = w_g^T H(\cdot) \in \Span(H)$, proving
  \begin{align}\label{eq:GH-sub-H}
    \Span(G(u^*) H) \subseteq \Span(H), \; \forall u^* \in \Uc. 
  \end{align}
  To prove the inclusion from right to left, consider
  $p: \Xc \to \cplx$ with $p \in \Span(H)$. Hence, there is a vector
  $v_p \in \cplx^l$ such that $p(\cdot) = v_p^T H(\cdot)$. For a given
  $u^*$, we need to show that there exists a vector $w_p \in \cplx^s$
  such that $p(\cdot) = w_p^T G(u^*) H(\cdot)$. In other words, we
  have to show the following linear equation holds for some
  $w_p \in \cplx^s$
  \begin{align}\label{eq:linear-equation-wp}
    G(u^*)^T w_p = v_p.
  \end{align}
  Given that $G(u^*)$ has full column rank,
  equation~\eqref{eq:linear-equation-wp} always have at least one
  solution, which might not be unique. Therefore,
  $p(\cdot) = w_p^T G(u^*) H(\cdot) \in \Span(G(u^*) H)$ and
  consequently
  \begin{align}\label{eq:H-sub-GH}
    \Span(H) \subseteq \Span(G(u^*) H), \; \forall u^* \in \Uc.
  \end{align}
  Combining~\eqref{eq:GH-sub-H} and~\eqref{eq:H-sub-GH} yields the
  subspace equality~\eqref{eq:subspace-restriction-equality}. By
  Proposition~\ref{p:family-augmented-invariance}(b), we conclude that
  $\Span(H)= \Hc$ is a common invariant subspace for the KCF. \qed
\end{pf}

Theorem~\ref{t:rank-common-invariant} provides an algebraic rank
condition that is far easier to check than the condition in
Proposition~\ref{p:family-augmented-invariance}.
% Next, we remark that under some circumstances one might be able to
% indirectly use Theorem~\ref{t:rank-common-invariant} when the rank
% condition does not hold everywhere on $\Uc$.

\begin{remark}\longthmtitle{A Note on Rank Condition in
    Theorem~\ref{t:rank-common-invariant}}
  {\rm 
    In Theorem~\ref{t:rank-common-invariant}, if the matrix $G(u)$ is
    column-rank deficient only for some $u \in \Uc$, one might be able
    to use the result with a slight relaxation. In particular, define
    \begin{align*}
      \tilde{\Uc} :=\setdef{u \in \Uc}{G(u)\: \text{has full column rank}}.
    \end{align*}
    If the control system~\eqref{eq:control-system} exhibits favorable control
    properties, e.g., controllability, reachability, or stabilizability,
    etc., on $\tilde{\Uc}$, then one can restrict the input space to
    $\tilde{\Uc}$ and apply Theorem~\ref{t:rank-common-invariant}. A
    notable example of this restriction is the case of switched linear
    modeling, see e.g.,~\cite{SP-SK:19}, that only requires $\tilde{\Uc}$
    to contain finitely many predetermined inputs.  \oprocend
  }
\end{remark}

\begin{example}\longthmtitle{Revisiting
		Example~\ref{ex:input-states-separable} -- Invariant Subspace for
		$\Kaug$}\label{ex:Kaug-invariant-decomposition}
{\rm
	For the system~\eqref{eq:ex-control-system}, one can derive a
	lifted linear form on an invariant subspace of $\Kaug$ as
	\begin{align*}
		\begin{bmatrix}
			x_1 \\ x_2 \\ x_1^2 \\ 1 \\ x_1 u \\ u \\ u^2 \\ \sin(u)
		\end{bmatrix}^+
		= 
		\begin{bmatrix} %
			a & 0 & 0 & 0 & 0 & b &  0 & 0 %
			\\
			0 & c & d & h & e & f &  0 & g  %
			\\
			0 & 0 & a^2 & 0 & 2 ab & 0 &  b^2 & 0  %
			\\
			0 & 0 & 0 & 1 & 0 & 0 &  0 & 0  %
			\\
			0 & 0 & 0 & 0 & a & 0 &  b & 0  %
			\\
			0 & 0 & 0 & 0 & 0 & 1 &  0 & 0  %
			\\
			0 & 0 & 0 & 0 & 0 & 0 &  1 & 0  %
			\\
			0 & 0 & 0 & 0 & 0 & 0 &  0 & 1  %
		\end{bmatrix}
		\begin{bmatrix}
			x_1 \\ x_2 \\ x_1^2 \\ 1 \\ x_1 u \\ u \\ u^2 \\ \sin(u)
		\end{bmatrix}.
	\end{align*}
	Note that the evolution is based on the augmented
	system~\eqref{eq:augmented-system}, which does not evolve $u$. One
	can decompose the basis
	$\Phi(x,u) = [x_1, x_2, x_1^2, 1, x_1 u, u, u^2, \sin(u)]^T$ as
	$\Phi(x,u) = G(u) H(x)$, with
	$G(u) = [I_{4 \times 4}, \tilde{G}(u)^T]^T$ where
	\begin{align*}
		H(x) =
		\begin{bmatrix}
			x_1
			\\
			x_2
			\\
			x_1^2
			\\
			1
		\end{bmatrix}
		\; \text{and} \; 
		\tilde{G}(u) = 
		\begin{bmatrix}
			u & 0 & 0 & 0 
			\\
			0 & 0 & 0 & u 
			\\
			0 & 0 & 0 & u^2
			\\
			0 & 0 & 0 & \sin(u)
		\end{bmatrix}.
	\end{align*}
	In this decomposition, the rank condition in
	Theorem~\ref{t:rank-common-invariant} holds (note the presence of
	$I_{4 \times 4}$ in $G(u)$). Hence, $\Span(H)$ is a common invariant
	subspace for the KCF, which is in agreement with
	Example~\ref{ex:input-states-separable}. \oprocend
}
\end{example}


According to Theorem~\ref{t:common-invariant-separates}, a common
invariant subspace for the KCF comes with an associated input-state
separable model for the control system~\eqref{eq:control-system}. The
next result specifies how to obtain it under the conditions of
Theorem~\ref{t:rank-common-invariant}.

\begin{proposition}\longthmtitle{Deriving Input-State Separable Models
		using Invariant Subspaces of
		$\Kaug$}\label{p:separable-form-invariant-augmented} 
	Let $\Sc \subseteq \Faug$ be a finite-dimensional (of dimension
	$s \in \naturals$) subspace comprised of input-state separable
	combinations that is invariant under~$\Kaug$ and
	$\Phi: \Xc \times \Uc \to \cplx^s$ a basis of $\Sc$. Let
	$A \in \cplx^{s \times s}$ be\footnote{The existence of this matrix
		is a direct consequence of the fact that $\Sc$ is invariant under
		$\Kaug$.} such that $ \Kaug \Phi = A \Phi $.  Let
	$\Phi(x,u) = G(u) H(x)$ be a decomposition of a basis for $\Sc$,
	where $G: \Uc \to \cplx^{s \times l}$ and $H: \Xc \to \cplx^{l}$.
	If $G(u)$ has full column rank for all $u \in \Uc$, then the
	matrix-valued map $\Ac: \Uc \to \cplx^{l \times l}$ given by
	\begin{align*}
		\Ac(u) = G(u)^\dagger A G(u) =\big( G(u)^H G(u) \big) ^{-1} G(u)^H A G(u),
	\end{align*}
	turns the common-invariant subspace $\Hc = \Span(H)$ for the KCF
	into the input-state separable form of
	Theorem~\ref{t:common-invariant-separates}, i.e., for all
	$(x,u) \in \Xc \times \Uc$
	\begin{align*}
		H(x^+) = H \circ \Tc (x,u) = \Ac(u) H(x).
	\end{align*}
\end{proposition}
\begin{pf}
Using the definition of $\Taug$,
cf. equation~\eqref{eq:augmented-system}, one can write
$\Kaug \Phi (x,u)= \Phi \circ \Taug(x,u) = \Phi(\Tc(x,u),u) = A
\Phi(x,u)$ for all $(x,u) \in \Xc \times \Uc$. Now, by using
$\Phi(x,u) = G(u) H(x)$,
\begin{align*}
	G(u) H(\Tc(x,u)) = A G(u) H(x), \quad \forall (x,u) \in \Xc \times \Uc.
\end{align*}
Keeping in mind that $G(u)$ has full column rank, one can multiply
both sides from the left by
$G(u)^\dagger = \big( G(u)^H G(u) \big) ^{-1} G(u)^H$, use
$G(u)^\dagger G(u) = I$, and reorder the terms to write
\begin{align*}
	H \circ \Tc(x,u) = H(\Tc(x,u)) = G(u)^\dagger A G(u) H(x) = \Ac(u) H(x), 
\end{align*}
for all $(x,u) \in \Xc \times \Uc$. \qed
\end{pf}

Theorem~\ref{t:rank-common-invariant} and
Proposition~\ref{p:separable-form-invariant-augmented} provide us with
a way to leverage the augmented Koopman operator $\Kaug$ to identify
common invariant subspaces for the KCF and derive input-state
separable models for the control system.

\section{Input-State Separable Forms on Normal Spaces}
In this section, we turn our attention to a special case of subspaces
that are of practical significance. This focus is motivated
%
% {\color{red} A closer look at the rank condition in
	% Theorem~\ref{t:rank-common-invariant} reveals that this condition
	% specifies a structural property of the subspace $\Sc$ and its basis
	% and does not depend on the operator~$\Kaug$.}
%
% \new{A closer look at the rank condition on $G(u)$ in
	% Theorem~\ref{t:rank-common-invariant} and noting that the matrix
	% value function $G$ is a part of the basis description for subspace
	% $\Sc$ reveals that this condition specifies a structural property of
	% the subspace $\Sc$ and its basis and does not depend on the operator
	% $\Kaug$.}
by examining the rank condition on~$G(u)$ presented in
Theorem~\ref{t:rank-common-invariant}, and observing that the
matrix-valued function $G$ constitutes an element of the basis
description for subspace~$\Sc$.  It becomes then clear that this
condition specifies a structural characteristic of the subspace $\Sc$
and its basis, which is independent of the operator $\Kaug$.
Therefore, here we study a specific class of subspaces and their bases
that always satisfy the rank condition in
Theorem~\ref{t:rank-common-invariant}.

\begin{definition}\longthmtitle{Vector-Valued Function of Separable
		Combinations in Normal Form and Normal
		Spaces}\label{def:normal-form-dictionary}
	Let $\Phi: \Xc \times \Uc \to \cplx^s$ be a vector-valued function
	of separable combinations. Moreover, let the set of elements of
	$\Phi$ be linearly independent. $\Phi$ is in \textbf{normal form} if
	it has a decomposition as one of the following:
	% \begin{align*}
		%   \Phi(x,u) = \begin{bmatrix} I_{\Uc}^{l \times l} (u)
			%     \\
			%     \tilde{G}(u) \end{bmatrix} H(x), \; \text{or} \; \Phi(x,u) =
		%   I_{s \times s} (u) H(x)
		% \end{align*}
	\begin{subequations}\label{eq:normal-form-dictionary}
		\begin{align}
			&\Phi(x,u) =
			\begin{bmatrix}
				I_{\Uc}^{l \times l}(u)
				\\
				\tilde{G}(u)
			\end{bmatrix}
			H(x),
			&&\text{ $s> l$}, 
			\label{eq:normal-form-s-gt-l}
			\\
			&\Phi(x,u) =  I_{\Uc}^{l \times l}(u)  H(x),
			&&
			\text{ $s = l$},
		\end{align}
	\end{subequations}
	where $H: \Xc \to \cplx^l$ and
	$\tilde{G}: \Uc \to \cplx^{(s-l)\times l}$ for some $l \leq s$ and
	the elements of $H$ are linearly independent.  Moreover,
	$I_{\Uc}^{l \times l} : \Uc \to \cplx^{l \times l}$ is the constant
	identity function, $I_{\Uc}^{l \times l}(u) \equiv I$.
	% (we omit the dependency on $u$ when the context is clear).
	A finite-dimensional space of separable combinations is
	\textbf{normal} if it has a basis that can be written as a
	vector-valued function of normal form.  \oprocend
\end{definition}

From Definition~\ref{def:normal-form-dictionary}, it is clear that a
basis in normal form satisfies the rank condition in
Theorem~\ref{t:rank-common-invariant}. The next result shows a useful
property of normal spaces.

\begin{proposition}\longthmtitle{Normal Spaces Capture Control-Independent
    Functions}\label{p:normal-space-u-independent}
  Let $\Sc \subset \Faug$ be a finite-dimensional space of input-state
  separable combinations and let $\Phi(x,u)= G(u) H(x)$ be a
  decomposition of a basis for $\Sc$, where
  $G: \Uc \to \cplx^{s \times l}$ and $H: \Xc \to \cplx^{l}$ for some
  $l \leq s$ (here, $s \in \naturals$ is the dimension of
  $\Sc$). Then, $\Sc$ is normal if and only if $h_e \in \Sc$ (cf.~Definition~\ref{def:cont-indep-extenstion}) for all
  $h \in \Span(H)$.
	% \new{where $h_e$ is the control-independent
        % extension of $h$
        % (cf.~Definition~\ref{def:cont-indep-extenstion}).
\end{proposition}
\begin{pf}
  $(\Rightarrow):$ Since $\Sc$ is normal, it has a basis with one of
  the following forms:
  \begin{align}\label{eq:normal-forms}
    &\hat{\Phi}(x,u) =
      \begin{bmatrix}
        I_{\Uc}^{l \times l} (u)
        \\
        \tilde{G}(u)
      \end{bmatrix}
      \tilde{H}(x),
    &&\text{if $s> l$},
       \nonumber
    \\
    &\hat{\Phi}(x,u) = I_{\Uc}^{l \times l} (u) \tilde{H}(x),
    &&\text{if
       $s = l$},
  \end{align}
  where $\Span(\tilde{H}) = \Span(H)$ (this is a direct consequence of
  the fact that $\Phi$ and $\hat{\Phi}$ are bases for the same
  subspace). Then, for every $h \in \Span(H)$, there exists a vector
  $w_h \in \cplx^l$, such that $h(\cdot) = w_h^T
  \tilde{H}(\cdot)$. Based on~\eqref{eq:normal-forms}, one can write
  \begin{align*}
    &h_e(x,u)  = [w_h^T, 0_{1 \times (s-l)}] \hat{\Phi}(x,u) \in \Sc,
    &&\text{if $s> l$}, 
    \\
    &h_e(x,u) = w_h^T \hat{\Phi}(x,u) \in \Sc, &&\text{if $s = l$}.
  \end{align*}
  This concludes the proof of this part.
  
  $(\Leftarrow):$ Let $H(\cdot) = [h_1(\cdot), \ldots,
  h_l(\cdot)]^T$. By hypothesis, we have $h_i(x) 1_\Uc(u) \in \Sc$ for
  all $i \in \until{l}$. As a result, there exist vectors
  $\{w_1, \ldots, w_l\} \subset \cplx^s$ such that for all
  $(x,u) \in \Xc \times \Uc$ we have
  \begin{align}\label{eq:u-independent-representation}
    h_i(x) 1_\Uc(u) = w_i^T G(u) H(x), \; \forall i \in \until{l}.
  \end{align}
  Let $W = [w_1, \ldots, w_l]^T \in \cplx^{l \times s}$ and consider
  two cases:
  
  Case~(i): Suppose $s = l$. Define the vector-valued function
  $\tilde{\Phi}(\cdot,\cdot) = W \Phi(\cdot,\cdot)$. This function can
  be written as
  \begin{align*}
    \tilde{\Phi}(x,u) = W G(u) H(x) = I_{\Uc}^{l \times l}(u) H(x), \quad
    \forall (x,u) \in \Xc \times \Uc, 
  \end{align*}
  where we have used~\eqref{eq:u-independent-representation}. Therefore
  $\tilde{\Phi}$ is a normal-form basis and hence $\Sc$ is normal.
  
  Case~(ii) Suppose $s > l$ and decompose
  $G(u) = [G_1^T(u), G_2^T(u)]^T$, where
  $G_1: \Uc \to \cplx^{l \times l}$ and
  $G_2: \Uc \to \cplx^{(s-l) \times l}$. Define the vector-valued
  function $\hat{\Phi}: \Xc \times \Uc \to \cplx^{s \times l}$,
  \begin{align*}
    \hat{\Phi}(x,u) =
    \begin{bmatrix}
      W \\ B
    \end{bmatrix}
    \Phi(x,u)
    & =
      \begin{bmatrix}
        W  \\ B
			\end{bmatrix}
      \begin{bmatrix}
        G_1(u) \\ G_2(u)
      \end{bmatrix}
      H(x) 
    \\
    &
      = \begin{bmatrix}
          I_{\Uc}^{l \times l} (u) \\ G_2(u)
        \end{bmatrix}
      H(x),
  \end{align*}
  where
  $B = [0_{(s-l) \times l}, I_{(s-l) \times (s-l)}] \in \cplx^{(s-l)
    \times s}$, and in the third equality we have
  used~\eqref{eq:u-independent-representation}. Therefore,
  $\hat{\Phi}$ is a normal-form basis and hence $\Sc$ is normal. \qed
\end{pf}

	%
	% \marginJC{So in a sense, this result is also the answer to my margin
        % above re: Prop. 5.6: if $\Sc$ is invariant and normal, then
        % $\restr{\Sc}{u=u_1} = \restr{\Sc}{u=u_2}$ for all
        % $u_1, u_2 \in \Uc$, no?}  \marginMH{That is correct.}
	%

Proposition~\ref{p:normal-space-u-independent} reveals a useful
property of normal spaces that allows us to directly predict the
evolution of functions in $\Fc$ under the system's trajectories by
applying $\Kaug$ on control-independent extensions through
Lemma~\ref{l:Kaug-captures-function-evolutions-in-F}, as we explain
next.

\begin{theorem}\longthmtitle{Identification of Common Invariant
    Subspaces of the KCF and Input-State Separable Forms on
    Normal Spaces}\label{t:normal-space-common-invariant}
  Let $\Sc \subset \Faug$ be a finite-dimensional normal space of
  input-state separable combinations that is invariant under
  $\Kaug$. Let $\Phi(x,u) = G(u) H(x)$ be a decomposition of a basis for
  $\Sc$ where $G: \Uc \to \cplx^{s \times l}$ and
  $H: \Xc \to \cplx^{l}$ for some $l \leq s$ (here, $s \in \naturals$
  is the dimension of $\Sc$). Then,
  \begin{enumerate}
  \item $\Span(H) \subset \Fc$ is a common invariant subspace under
    the Koopman control family $\{\Kc_{u^*}\}$;
  \item for all $h \in \Span(H)$ and for all
    $(x,u) \in \Xc \times \Uc$, it holds that
    $h(x^+) = h\circ \Tc (x,u) = \Kaug h_e(x,u)$;
    % where $h_e$ is the control-independent extension of $h$
    % (cf. Definition~\ref{def:cont-indep-extenstion}).
  \item without loss of generally, assume $\Phi$ is in normal form,
    i.e., $G(u) = I_{\Uc}^{l \times l}(u)$ if $l = s$ or
    $G(u) = [I_{\Uc}^{l \times l}(u)^T, \tilde{G}(u)^T]^T$ if $s >l$.
    % (cf. Definition~\ref{def:normal-form-dictionary}).
    Moreover, let $A \in \cplx^{s \times s}$ be a matrix such that
    $\Kaug \Phi = A \Phi$ (note that $A$ exists because $\Sc$ is
    invariant under $\Kaug$). If $s > l$, consider the
    block-decomposition of $A$,
    \begin{align*}
      A = \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix}, 
    \end{align*}
    where $A_{11} \in \cplx^{l \times l}$,
    $A_{12} \in \cplx^{l \times (s-l)}$,
    $A_{21} \in \cplx^{(s-l) \times l}$, and
    $A_{22} \in \cplx^{(s-1) \times (s-l)}$. Then, the associated
    input-state separable dynamics can be written as
    \begin{align}\label{eq:normal-separable-form}
      H(x^+) = H \circ \Tc(x,u) = \Ac(u) H(x),
    \end{align}
    where, for each $u \in \Uc$,
    \begin{align*}
      &\Ac(u) = A_{11} + A_{12} \tilde{G}(u), &&\text{if $s > l$}. 
      \\
      &\Ac(u) = A \, I_{\Uc}^{l \times l}(u) = A, &&\text{if $s = l$}. 
    \end{align*}
  \end{enumerate}
\end{theorem}
\begin{pf}
  (a) Since $\Sc$ is normal, one can do a linear transformation of the
  basis $\Phi(x,u) = G(u) H(x)$ to put it in normal form. Hence, there
  is a nonsingular square matrix $E$, such that
  $E\Phi(x,u) = EG(u) H(x)$ is in normal form. Therefore, by
  Definition~\ref{def:normal-form-dictionary}, $EG(u)$ has full column
  rank for all $u \in \Uc$. Since $E$ is nonsingular, we deduce that
  $G(u)$ has full column rank for all $u \in \Uc$. As a result, we can
  invoke Theorem~\ref{t:rank-common-invariant} to deduce that
  $\Span(H) \subset \Fc$ is a common invariant subspace under the KCF.
  
  (b) This part is the direct consequence of
  Lemma~\ref{l:Kaug-captures-function-evolutions-in-F}.
  
  (c) Using the definition of $\Taug$,
  cf. equation~\eqref{eq:augmented-system}, one can write
  $ \Phi \circ \Taug(x,u) = \Phi(\Tc(x,u),u) = A \Phi(x,u)$ for all
  $(x,u) \in \Xc \times \Uc$. Now, using $\Phi(x,u) = G(u) H(x)$,
  \begin{align}\label{eq:normal-invariant-separation}
    G(u) H(x^+) = G(u) H(\Tc(x,u)) = A G(u) H(x).
  \end{align}
  The case $s = l$ is trivial since $G(u)$ is an identity map. For the
  case $s > l$, the proof directly follows by multiplying both sides
  of~\eqref{eq:normal-invariant-separation} from the left by the
  matrix $[I_{l \times l}, 0_{l \times (s-l)}]$ and using the
  decompositions of $G(u)$ and~$A$. \qed
\end{pf}

\begin{example}\longthmtitle{Examples~\ref{ex:input-states-separable}
    and~\ref{ex:Kaug-invariant-decomposition} Revisited}
  {\rm
    The basis decomposition in
    Example~\ref{ex:Kaug-invariant-decomposition} is in normal form. One
    can readily use the formula in
    Theorem~\ref{t:normal-space-common-invariant}(c) with this
    decomposition to calculate the input-state separable
    form~\eqref{eq:ex-input-state-separable-form}.  \oprocend
  }
\end{example}

Theorem~\ref{t:normal-space-common-invariant} has significant
practical implications: not only it connects the invariant subspaces
of $\Kaug$ to common invariant subspaces of the KCF, but more
importantly, unlike
Proposition~\ref{p:separable-form-invariant-augmented}, it provides a
direct way of predicting the evolution of observables in $\Fc$ under
the control system based on the application of $\Kaug$ on
control-independent extensions. This direct computation does not
require taking a pseudo-inverse (cf.
Proposition~\ref{p:separable-form-invariant-augmented}) and is helpful
to find accuracy bounds when we have to approximate invariant
subspaces of~$\Kaug$, as we explain next.

\section{Non-Invariant Subspaces, Invariance Proximity, and
  Approximation Error}

In the sections above we have provided results connecting the
finite-dimensional invariant subspaces of $\Kaug$ to common invariant
subspaces of the Koopman control family $\{\Kc_{u^*}\}_{u^* \in \Uc}$,
and how these can be used in predicting the evolution of functions on
the common invariant subspace under the trajectories of the control
system. In practice, however, finding exact invariant subspaces that
capture proper information is an arduous task and one might need to
settle for an approximately invariant subspace. In such case, three
fundamental questions immediately arise:
\begin{enumerate}
\item[(Q1)] How can we measure the closeness of a subspace to being
  invariant?
\item[(Q2)] How does this measure characterize the approximation error
  of the action of the operator on a non-invariant subspace?
\item[(Q3)] How do the previous results regarding the prediction of
  observables on the trajectories of the control system extend to the
  case of non-invariant subspaces?
\end{enumerate}
These are the questions we tackle in this section. To determine
whether a finite-dimensional subspace $\Sc \subset \Faug$ is invariant
under $\Kaug$ we only need the concept of set inclusion. However, in
order to quantify how close to invariant a subspace is, we need to be
able to measure angles, lengths, and distances. Therefore, we equip
the space $\Faug$ with an inner product, that induces a norm and, in
turn, a metric\footnote{Even though we aim to approximate a common
  invariant subspace $\Hc \subset \Fc$ under the Koopman control
  family, our end goal is to predict the evolution of observables
  under the system's trajectories, i.e., we aim to predict
  $h(x^+) = h \circ \Tc(x,u)$ for all $h \in \Hc$ and
  $(x,u) \in \Xc \times \Uc$. Since $h\circ \Tc \in \Faug$, we need to
  reason with~$\Faug$.}.

\begin{definition}\longthmtitle{Inner Product, Norm, and Metric on
		$\Faug$}\label{def:innerprod-norm-metric}
	An arbitrary inner product\footnote{Since we are working with
		finite-dimensional subspaces, we do not require the metric space
		to be complete (Hilbert).}
	$\innerprod{\cdot}{\cdot}: \Faug \times \Faug \to \cplx$ on $\Faug$
	induces a norm $\| \cdot \|: \Faug \to [0,\infty)$ and a metric
	$\dist: \Faug \times \Faug \to [0,\infty)$ as
	\begin{align*}
		\| f \| = \sqrt{\innerprod{f}{f}}\, , \qquad 
		\dist(f,g) = \|f -
		g\|. \eqoprocend
	\end{align*}
\end{definition}

Since we work with a finite-dimensional subspace that is not
necessarily invariant under the operator, we have to approximate the
action of the operator on the subspace. This approximation is
generally done by performing an orthogonal projection on the subspace,
% (see Section~\ref{sec:prelim-Koopman})
as explained next.

\begin{definition}\longthmtitle{Linear Predictors on
		Finite-Dimensional Subspaces}\label{def:linear-predictors}
	Consider the finite-dimensional subspace $\Sc \subset \Faug$ and let
	$\Pc_{\Sc}: \Faug \to \Faug$ be the orthogonal projection
	operator\footnote{Given an orthonormal basis $\{e_1,\ldots,e_n\}$
		for $\Sc$, one can calculate the orthogonal projection of
		$g \in \Faug$ on $\Sc$ by
		$\Pc_{\Sc}(g) = \sum_{i=1}^{n} \innerprod{g}{e_i}e_i$.} on
	$\Sc$. We define the predictor for the function $\psi \in \Faug$ on
	$\Sc$ as
	\begin{align*}
		\psi \approx \Pf_\psi^\Sc := \Pc_{\Sc} \psi.
	\end{align*}
	For a vector-valued function $\Psi =
        [\psi_1,\ldots,\psi_n]^T$, where $\psi_i \in \Faug$ for
        $i \in \until{n}$, we define the linear predictor
        $ \Psi \approx \Pf_{ \Psi}^\Sc := [\Pf_{\psi_1}^\Sc, \ldots,
        \Pf_{\psi_n}^\Sc]$.  We remove the superscript $\Sc$ when the
        choice of subspace is clear from the context.  \oprocend
\end{definition}

%\new{
	%\begin{definition}\longthmtitle{Linear Predictors on Finite-Dimensional Subspaces}%\label{def:linear-predictors}
	%Consider the finite-dimensional (with dimension $s$) subspace $\Sc \subset \Faug$ with the vector-valued basis $\Phi: \Xc \times \Uc \to \cplx^s$. Moreover, let $\Pc_{\Sc}: \Faug \to \Faug$ be the orthogonal projection operator on $\Sc$ and let the matrix $\tilde{A}$ be a matrix such that $\Pc_{\Sc} \Kaug \Phi = \tilde{A} \Phi$ (note that $\tilde{A}$ exists since $\Sc$ is invariant under $\Pc_{\Sc} \Kaug$). Then, following~\eqref{eq:Koopman-matrix-noninvariant}-\eqref{eq:Koopman-predictor-noninvariant}, we define the dictionary predictor $\Pf_{\Kaug \Phi}$ as
	%\begin{align*}
	%\Kaug \Phi = \Phi \circ \Taug \approx \Pf_{\Kaug \Phi} := \Pc_{\Sc} \Kaug \Phi = \tilde{A} \Phi.
	%\end{align*}
	%Moreover, for any function $f \in \Sc$ with description $f = w^T \Phi$ for $w \in \cplx^s$, we define the linear predictor $\Pf_{\Kaug f}$ as
	%\begin{align*}
	%\Kaug f = f \circ \Taug \approx \Pf_{\Kaug f} := \Pc_{\Sc} \Kaug f = w^T \tilde{A} \Phi.
	%\end{align*}
	%\oprocend
	%\end{definition}
	%}

The properties of the operator $\Pc_{\Sc}$ lead to useful properties
of the linear predictors defined in
Definition~\ref{def:linear-predictors}.

\begin{lemma}\longthmtitle{Properties of Linear
		Predictors}\label{l:predictor-properties}
	Linear predictors on the finite-dimensional subspace
	$\Sc \subset \Faug$ satisfy:
	\begin{enumerate}
		\item $\Pf_f \in \Sc$ is the best approximation for $f \in \Faug$ on
		$\Sc$, i.e., $\| f - \Pf_f\| \leq \|f - g\|$ for all $g \in \Sc$;
		\item $ \Pf_{c_1 f_1 + c_2 f_2} = c_1 \Pf_{f_1} + c_2 \Pf_{f_2}$ for
		all $f_1,f_2 \in \Faug$ and $c_1,c_2 \in \cplx$;
		\item let $\Psi$ be a vector-valued function with
		$\Span(\Psi) \subset \Faug$
		% (cf. Remark~\ref{r:notation-overload}),
		and let $f = v_f^T \Psi$, where $v_f$ is a complex vector of
		appropriate size. Then, $\Pf_f = v_f^T \Pf_\Psi$.  \oprocend
	\end{enumerate}
\end{lemma}

The proof of Lemma~\ref{l:predictor-properties} is a direct
consequence of the properties of orthogonal projections and is omitted
for space reasons. Lemma~\ref{l:predictor-properties}(a) states that
the predictor defined in Definition~\ref{def:linear-predictors} is the
best predictor on the subspace: in this sense, we use the notation
$f \approx \Pf_f$ when we aim to emphasize that we approximate $f$
with~$\Pf_f$.

We next use the linear predictors to approximate the action of
the operator $\Kaug$ on a non-invariant finite-dimensional subspace
and provide a matrix notation for it.

\begin{lemma}\longthmtitle{Approximating an Operator's Action using
		Linear Predictors}\label{l:operator-predictor-matrix}
	Any finite-dimensional subspace $\Sc \subset \Faug$ is invariant
	under $\Pc_{\Sc}\Kaug$.  Let $\Phi: \Xc \times \Uc \to \cplx^s$ be a
	basis for $\Sc$ and let $\tilde{A} \in \cplx^{s \times s}$ be a
	matrix such that $\Pc_{\Sc} \Kaug \Phi = \tilde{A} \Phi$. Then,
	\begin{enumerate}
		\item $\Pf_{\Kaug \Phi} = \tilde{A} \Phi$;
		\item for $f \in \Sc$ with description $f = v_f^T \Phi$, where
		$v_f \in \cplx^s$, we have $\Pf_{\Kaug f} = v_f^T \tilde{A} \Phi$.
		\oprocend
	\end{enumerate}
\end{lemma}

Note the parallelism of Lemma~\ref{l:operator-predictor-matrix}
with~\eqref{eq:Koopman-matrix-noninvariant}-\eqref{eq:Koopman-predictor-noninvariant}. Its
proof is a direct consequence of the linearity of $\Kaug$ and
Lemma~\ref{l:predictor-properties}, and is omitted for space reasons.
The prediction error associated with the predictors in
Lemma~\ref{l:operator-predictor-matrix} directly depends on how close
to invariant the space is under the operator $\Kaug$. To capture this,
we define the concept of invariance proximity under an operator.

\begin{definition}\longthmtitle{Invariance
		Proximity}\label{def:invariance-proximity}
	The \textbf{invariance proximity} of a finite-dimensional subspace
	$\Sc \subset \Faug$ under the operator $\Kaug$, denoted
	$I_{\Kaug} (\Sc)$, is
	\begin{align*}
		I_{\Kaug} (\Sc) = \sup_{f \in \Sc, \|\Kaug f\| \neq 0} \frac{ \|
			\Kaug f -  \Pf_{\Kaug f}\|}{\| \Kaug f \|}.  \eqoprocend
	\end{align*}
\end{definition}

Invariance proximity measures the worst-case relative error of
approximation by projecting the action of $\Kaug$ on~$\Sc$ and
provides an answer to Q2 above.  Invariance proximity does not depend
on the specific basis for the subspace, and is instead a property of
the linear space $\Sc$ and the operator~$\Kaug$.

\begin{proposition}\longthmtitle{Properties of Invariance
    Proximity}\label{prop:prop-inv-prox}
  Given a finite-dimensional subspace $\Sc \subset \Faug$,
  \begin{enumerate}
  \item $I_{\Kaug} (\Sc) \in [0,1]$;
  \item $I_{\Kaug} (\Sc) = 0$ if~\footnote{The converse also holds if
      $\|f-g\|=0$ implies $f=g$ everywhere. This might not hold for
      typical norms on function spaces that operate on equivalence
      classes and allow for violations of equality on measure-zero
      sets.} $\Sc$ is invariant under~$\Kaug$.
  \end{enumerate}
\end{proposition}
\begin{pf}
  (a) Let $f \in \Faug$ with $\|\Kaug f\| \neq 0$.  Noting that
  $\Pf_{\Kaug f} = \Pc_{\Sc} \Kaug f$ is an orthogonal projection on
  $\Sc$, we can decompose $\Kaug f$ as $\Kaug f = \Pf_{\Kaug f} + e$,
  where $ \langle \Pf_{\Kaug f}, e \rangle = 0$.
  % % 
  % %   % This decomposition is always possible since the operations are
  % % confined to the finite-dimensional vector space $\Sc + \Span(\Kaug f)$.
  % % 
	% Hence,
	% \begin{align}\label{eq:projection-error-norm}
        %   \| \Kaug f -  \Pf_{\Kaug f}\| = \|e\|.
        % \end{align}
            %      Moreover,
  Using the definition of the norm induced by the inner product then
  yields $ \|\Kaug f\|^2 = \|\Pf_{\Kaug f}\|^2 + \|e\|^2$.  Therefore,
  $\|e\| \leq \|\Kaug f\|$ and we can write 
  \begin{align*}
    \frac{ \| \Kaug f -  \Pf_{\Kaug f}\|}{\| \Kaug f \|} = \frac{ \| e
    \|}{\| \Kaug f \|} \leq 1. 
  \end{align*}
  Since this inequality holds for all functions $f \in \Faug$ where
  $\Pf_{\Kaug f} \neq 0$, we deduce $I_{\Kaug} (\Sc) \leq
  1$. Moreover, by definition of $I_{\Kaug} (\Sc)$ and the fact that
  norms are nonnegative, we conclude $I_{\Kaug} (\Sc) \geq 0$,
  completing the proof.
  
  (b) If $\Sc$ is invariant under $\Kaug$, we have $\Kaug f \in \Sc$
  and therefore $\| \Kaug f - \Pf_{\Kaug f}\| =0$ for all $f \in
  \Sc$. Hence, $I_{\Kaug} (\Sc) = 0$. \qed
\end{pf}

% Now, we define the approximate input-state separable form on normals spaces by using the construction in  Theorem~\ref{t:normal-space-common-invariant}(c) on normal space $\Sc$ and the operator $\Pc_{\Sc} \Kaug$.
%
%\new{
	%\begin{definition}\longthmtitle{Approximate Input-State Separable Form on Normal Spaces}
	% Let $\Sc \subset \Faug$ be a finite-dimensional (with dimension $s$) normal space of
	%input-state separable combinations with a normal form decomposition $\Phi(x) = G(u) H(x)$ be a decomposition of basis of the form~\eqref{eq:normal-form-dictionary}. Moreover, let $A \in \cplx^{s \times s}$ be a matrix such that $\Pc_{\Sc}\Kaug \Phi = A \Phi$ ($A$ exists since $\Sc$ is invariant under $\Pc_{\Sc}\Kaug$). If $s > l$ decompose $A$ as (cf.~Theorem~\ref{t:normal-space-common-invariant}(c))
	%\begin{align*}
	%A = \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix}, 
	%\end{align*}
	%where $A_{11} \in \cplx^{l \times l}$, $A_{12} \in \cplx^{l \times (s-l)}$, $A_{21} \in \cplx^{(s-l) \times l}$, and $A_{22} \in \cplx^{(s-1) \times (s-l)}$. Then, the approximate input-state separable dynamics can be written as 
	%\begin{align*}
	%H(x^+) = H \circ \Tc(x,u) \approx \Ac(u) H(x^+),
	%\end{align*}
	%where for all $u \in \Uc$,
	%\begin{align*}
	%&\Ac(u) = A_{11} + A_{12} \tilde{G}(u), &&\text{if $s > l$}. 
	%\\
	%&\Ac(u) = A, &&\text{if $s = l$}. 
	%\end{align*}
	%\oprocend
	%\end{definition}
	%}

%\red{write a result and mention for all functions in $\Span(H)$ this is the correct predictor $\Pf_{\Kaug f}$.}

Proposition~\ref{prop:prop-inv-prox} means that invariance
proximity provides an answer to Q1 above.  The next result extends to
non-invariant subspaces the results on prediction of the evolution of
functions in $\Fc$ under the control system~\eqref{eq:control-system},
providing an answer to~Q3.


%\begin{theorem}\longthmtitle{Invariance Proximity on Normal Spaces Bounds Prediction Error of Function Evolutions under Control System's Trajectories}\label{t:invariance-proximity-bounds-error-in-F}
%Let $\Sc \subset \Faug$ be a finite-dimensional normal subspace comprised of input-state separable combinations. Let $\Phi(x,u)= G(u) H(x)$ be a decomposition of a basis for $\Sc$ \new{where $G: \Uc \to \cplx^{s \times l}$ and $H: \Xc \to \cplx^{l}$ for some $l \leq s$}. \new{Moreover, for all $h \in \Span(H) \subset \Fc$ consider their control independent extension $h_e \in \Faug$ (cf. Definition~\ref{def:cont-indep-extenstion}). Similarly define $H_e(x,u):= I_{\Uc}^{l \times l}(u)H(x)$ where $I_{l\times l}(u) \equiv I$ always returns the identity matrix.} Then,
%%Then, for all $h \in \Span(H) \subset \Fc$, we have
%\begin{enumerate}
%\item the function $\Pf_{h \circ \Tc} := \Pf_{\Kaug h_e} = \Pc_{\Sc} \Kaug h_e$ is the best predictor of the function $h(x^+) = h\circ \Tc(x,u)$ on $\Sc$, i.e., $\|h\circ \Tc - \Pf_{h \circ \Tc}\| \leq \|h\circ \Tc - g\|$ for all $g \in \Sc$;
%\new{
	%\item Define the vector-valued predictor $\Pf_{H \circ \Tc}:= \Pf_{\Kaug H_e}$ (this is equivalent of stacking the predictors in part (a) for each element of $H\circ \Tc$ \red{prove this!}), then for all $h \in \Span(H)$ with description $h = v_h^T H$ with $v_h \in \cplx^l$, we have
	%\begin{align*}
	%\Pf_{h\circ \Tc} = v_h^T \Pf_{H \circ \Tc};
	%\end{align*}
	%
	%\item without loss of generally assume $\Phi$ is in normal form, i.e., $G(u) = I_{\Uc}^{l \times l}(u)$ if $l = s$ or $G(u) = [I_{\Uc}^{l \times l}(u)^T, \tilde{G}(u)^T]^T$ if $s >l$ (cf. Definition~\ref{def:normal-form-dictionary}). Moreover, let $\tilde{A} \in \cplx^{s \times s}$ be a matrix such that $\Pc_{\Sc} \Kaug \Phi = \tilde{A} \Phi$ ($\tilde{A}$ exists since $\Sc$ is invariant under $\Pc_{\Sc}\Kaug$). If $s > l$ decompose $A$ as
	%\begin{align*}
	%\tilde{A} = \begin{bmatrix} \tilde{A}_{11} & \tilde{A}_{12} \\ \tilde{A}_{21} & \tilde{A}_{22} \end{bmatrix}, 
	%\end{align*}
	%where $\tilde{A}_{11} \in \cplx^{l \times l}$, $\tilde{A}_{12} \in \cplx^{l \times (s-l)}$, $\tilde{A}_{21} \in \cplx^{(s-l) \times l}$, and $\tilde{A}_{22} \in \cplx^{(s-1) \times (s-l)}$. Then, the approximate input-state separable dynamics can be written as 
	%\begin{align}\label{eq:approx-separable-form}
	%H(x^+) = H \circ \Tc(x,u) :\approx \Pf_{H \circ \Tc} (x,u)= \Ac(u) H(x),
	%\end{align}
	%where for all $u \in \Uc$,
	%\begin{align*}
	%&\Ac(u) = \tilde{A}_{11} + \tilde{A}_{12} \tilde{G}(u), &&\text{if $s > l$}. 
	%\\
	%&\Ac(u) = \tilde{A} \, I_{\Uc}^{l \times l}(u) \equiv \tilde{A}, &&\text{if $s = l$}. 
	%\end{align*}
	%
	%}
%\item the relative error of the prediction by the aforementioned predictor is bounded by the invariance proximity of $\Sc$ under $\Kaug$, i.e.,
%\begin{align*}
%\frac{\left\| h \circ \Tc - \Pf_{h \circ \Tc} \right\|}{\| h \circ \Tc \|} \leq I_{\Kaug}(\Sc), \forall h \in \Span(H); \, \|h \circ \Tc\| \neq 0.
%\end{align*}
%\end{enumerate}
%\end{theorem}
%\begin{proof}
%(a) As a basic consequence of $\Pc_{\Sc}$ being an orthogonal projection, $\Pc_{\Sc} \Kaug h_e$ is the best predictor of $\Kaug h_e$ on $\Sc$ (one can readily see this by taking an orthonormal basis for $\Sc$ and extend it to build an orthonormal basis for $\Sc + \Span(\Kaug h_e)$ and compare the function decompositions with respect to this basis). The rest of the proof follows from the fact that $\Kaug h_e = h \circ \Tc$ based on Lemma~\ref{l:Kaug-captures-function-evolutions-in-F}.
%
%(b) The proof directly follows from the stacked notation in Remark~\ref{r:notation-overload} and the fact that the operator $\Pc_{\Sc} \Kaug$ is linear.
%
%(c) We need to prove the rightmost equality, i.e., $\Pf_{H \circ \Tc}(x,u) = \Ac(u) H(x)$, in~\eqref{eq:approx-separable-form} (the rest are definitions). Based on the definition in part~(b) and Definition~\ref{def:linear-predictors}, we have 
%\begin{align}\label{eq:predictor-He}
%\Pf_{H \circ  \Tc} = \Pf_{\Kaug H_e} = \Pc_{\Sc} \Kaug H_e.
%\end{align}
%For the case $s = l$, note that $\Phi(x,u) = I_{\Uc}^{l \times l}(u) H(x) = H_e(x,u)$. Hence, noting that $\Pc_{\Sc} \Kaug \Phi = \tilde{A} \Phi$, we have $\Pc_{\Sc} \Kaug H_e (x,u) = \tilde{A} H_e(x,u) = \tilde{A}  I_{\Uc}^{l \times l}(u) H(x)$. Therefore, one can use~\eqref{eq:predictor-He} and write $\Pf_{H \circ  \Tc}(x,u) = \tilde{A} H_e (x,u)= \tilde{A}  I_{\Uc}^{l \times l}(u) H(x)$ which completes the proof.
%
%Now, we turn our attention to the case $s>l$. In this case note that
%\begin{align}\label{eq:phi-decomposition}
%\Phi(x,u) = [H_e(x,u)^T, (\tilde{G}(u) H(x))^T]^T.
%\end{align}
%By multiplying both sides of $\Pc_{\Sc} \Kaug \Phi = \tilde{A} \Phi$ from the left by $W = [I_{l \times l}, 0_{l \times (s-l)}]$, and using~\eqref{eq:phi-decomposition}, the decomposition of $\tilde{A}$, the properties of the vector-valued notation in Remark~\ref{r:notation-overload}, and the linearity of the operator $\Pc_{\Sc} \Kaug$, one can write
%\begin{align*}
%\Pc_{\Sc} \Kaug H_e &= W \Pc_{\Sc} \Kaug \Phi = W A \Phi 
%\\
%&= (\tilde{A}_{11} I_{\Uc}^{l \times l}(u) + \tilde{A}_{12}\tilde{G}(u)) H(x).
%\end{align*}
%The rest of the proof follows from equation~\eqref{eq:predictor-He} and the fact that $I_{\Uc}^{l \times l}(u) = I$ for all $u \in \Uc$.
%
%(d) By definition of normal spaces and invariance proximity measure, for every $h \in \Span(H)$, one can write
%\begin{align*}
%\frac{\| \Kaug h_e - \Pc_{\Sc} \Kaug h_e\|}{\| \Kaug h_e \|} \leq I_{\Kaug}(\Sc).
%\end{align*}
%The rest of the proof follows from the fact that $\Kaug h_e = h \circ \Tc $ (cf.~Lemma~\ref{l:Kaug-captures-function-evolutions-in-F}) and $\Pf_{h \circ \Tc} =\Pc_{\Sc} \Kaug h_e$.
%\end{proof}

\begin{theorem}\longthmtitle{Approximate Input-State Separable Form
		and Accuracy
		Bound}\label{t:invariance-proximity-bounds-error-in-F}
	Let $\Sc \subset \Faug$ be a finite-dimensional normal subspace
	comprised of input-state separable combinations. Let
	$\Phi(x,u)= G(u) H(x)$ be a decomposition of a basis for $\Sc$ where
	$G: \Uc \to \cplx^{s \times l}$ and $H: \Xc \to \cplx^{l}$ for some
	$l \leq s$ (here, $s \in \naturals$ is the dimension of $\Sc$). Let
	$H_e$ and $h_e$ be the control-independent extensions of $H$ and
	$h \in \Span(H)$ respectively. Then,
	\begin{enumerate}
		\item $\Pf_{h \circ \Tc} = \Pf_{\Kaug h_e}$ for all
		$h \in \Span(H)$, and $\Pf_{H \circ \Tc} = \Pf_{\Kaug H_e}$;
		\item without loss of generally, assume $\Phi$ is in normal form,
		i.e., $G(u) = I_{\Uc}^{l \times l}(u)$ if $l = s$ or
		$G(u) = [I_{\Uc}^{l \times l}(u)^T, \tilde{G}(u)^T]^T$ if $s
		>l$. Moreover, let $\tilde{A} \in \cplx^{s \times s}$ be a matrix
		such that $\Pc_{\Sc} \Kaug \Phi = \tilde{A} \Phi$ (note that
		$\tilde{A}$ exists because $\Sc$ is invariant under
		$\Pc_{\Sc}\Kaug$). If $s > l$, consider the block-decomposition of
		$\tilde{A}$,
		\begin{align*}
			\tilde{A} =
			\begin{bmatrix}
				\tilde{A}_{11} & \tilde{A}_{12}
				\\
				\tilde{A}_{21} & \tilde{A}_{22}
			\end{bmatrix},  
		\end{align*}
		where $\tilde{A}_{11} \in \cplx^{l \times l}$,
		$\tilde{A}_{12} \in \cplx^{l \times (s-l)}$,
		$\tilde{A}_{21} \in \cplx^{(s-l) \times l}$, and
		$\tilde{A}_{22} \in \cplx^{(s-1) \times (s-l)}$. Then, the associated
		approximate input-state separable dynamics can be written as
		\begin{align}\label{eq:approx-separable-form}
			H(x^+) = H \circ \Tc(x,u) \approx \Pf_{H \circ \Tc} (x,u)= \Ac(u) H(x),
		\end{align}
		where,  for each $u \in \Uc$,
		\begin{align*}
			&\Ac(u) = \tilde{A}_{11} + \tilde{A}_{12} \tilde{G}(u),
			&&\text{if $s > l$}. 
			\\ 
			&\Ac(u) = \tilde{A} \, I_{\Uc}^{l \times l}(u) = \tilde{A},
			&&\text{if $s = l$}.  
		\end{align*}
		
		\item for all $h \in \Span(H)$ with description $h = v_h^T H$,
		$v_h \in \cplx^l$,
		\begin{align*}
			h(x^+) = h \circ \Tc(x,u) \approx  \Pf_{h \circ \Tc} (x,u) =
			v_h^T \Ac(u) H(x); 
		\end{align*}
		
		\item for all $h \in \Span(H)$ with $ \|h \circ \Tc\| \neq 0$, the
		predictor's relative error is bounded by the invariance proximity
		of $\Sc$ under $\Kaug$,
		\begin{align*}
			\frac{\left\| h \circ \Tc - \Pf_{h \circ \Tc} \right\|}{\| h
				\circ \Tc \|} \leq I_{\Kaug}(\Sc) .
		\end{align*}
	\end{enumerate}
\end{theorem}
\begin{pf}
	(a) By Definition~\ref{def:linear-predictors},
	$\Pf_{h \circ \Tc} = \Pc_{\Sc} (h \circ \Tc)$. Using
	Lemma~\ref{l:Kaug-captures-function-evolutions-in-F}, we have
	$h \circ \Tc = \Kaug h_e$. Hence,
	$\Pf_{h \circ \Tc} = \Pc_{\Sc} \Kaug h_e = \Pf_{\Kaug h_e}$. The
	statement regarding $H$ follows directly by applying this to each
	element of the equality $\Pf_{H \circ \Tc} = \Pf_{\Kaug H_e}$.
	
	(b) We need to prove the rightmost equality
	in~\eqref{eq:approx-separable-form}, since the rest follow directly
	from their definitions. From part~(a), and using the vector-valued
	notation in Remark~\ref{r:notation-overload}, we have
	\begin{align}\label{eq:predictor-He}
		\Pf_{H \circ  \Tc} = \Pf_{\Kaug H_e} = \Pc_{\Sc} \Kaug H_e.
	\end{align}
	For the case $s = l$, we use
	Lemma~\ref{l:control-indep-properties}(b) to write
	$\Phi(x,u) = I_{\Uc}^{l \times l}(u) H(x) = H_e(x,u)$. Hence, noting
	that $\Pc_{\Sc} \Kaug \Phi = \tilde{A} \Phi$, we have
	$\Pc_{\Sc} \Kaug H_e (x,u) = \tilde{A} H_e(x,u) = \tilde{A}
	I_{\Uc}^{l \times l}(u) H(x)$. Using~\eqref{eq:predictor-He}, we can
	write
	$\Pf_{H \circ \Tc}(x,u) = \tilde{A} H_e (x,u)= \tilde{A} I_{\Uc}^{l
		\times l}(u) H(x)$, which completes the proof.
	
	Next, we turn our attention to the case $s>l$. Using
	Lemma~\ref{l:control-indep-properties}(b), one can write
	\begin{align}\label{eq:phi-decomposition}
		\Phi(x,u) = [H_e(x,u)^T, (\tilde{G}(u) H(x))^T]^T.
	\end{align}
	Multiplying both sides of $\Pc_{\Sc} \Kaug \Phi = \tilde{A} \Phi$
	from the left by $W = [I_{l \times l}, 0_{l \times (s-l)}]$, and
	using~\eqref{eq:phi-decomposition}, the decomposition of
	$\tilde{A}$, the properties of the vector-valued notation in
	Remark~\ref{r:notation-overload}, and the linearity of the operator
	$\Pc_{\Sc} \Kaug$, one can write
	\begin{align*}
		\Pc_{\Sc} \Kaug H_e
		&= W \Pc_{\Sc} \Kaug \Phi = W \tilde{A} \Phi 
		\\
		&= (\tilde{A}_{11} I_{\Uc}^{l \times l}(u) + \tilde{A}_{12}\tilde{G}(u)) H(x).
	\end{align*}
	The statement then follows from equation~\eqref{eq:predictor-He} and
	the fact that $I_{\Uc}^{l \times l}(u) = I$ for all $u \in \Uc$.
	
	(c) We need to prove the rightmost equality
	$\Pf_{h \circ \Tc}(x,u) = v_h^T\Ac(u) H(x)$, since the rest follow
	directly from their definitions.  By hypothesis
	$h \circ \Tc = v_h^T H \circ \Tc$; hence, from
	Lemma~\ref{l:predictor-properties}(c), we have
	$\Pf_{h \circ \Tc} = v_h^T \Pf_{H \circ \Tc}$. The result then
	follows from~\eqref{eq:approx-separable-form}.
	
	(d) By Proposition~\ref{p:normal-space-u-independent}, and using the
	definition of invariance proximity, for all $h \in \Span(H)$ with
	$\|h \circ \Tc\| \neq 0$, one can write
	\begin{align*}
		\frac{\| \Kaug h_e - \Pf_{\Kaug h_e}\|}{\| \Kaug h_e \|} \leq I_{\Kaug}(\Sc).
	\end{align*}
	The statement then follows from the fact that
	$\Kaug h_e = h \circ \Tc $
	(cf.~Lemma~\ref{l:Kaug-captures-function-evolutions-in-F}) and
	part~(a). \qed
\end{pf}

This result can be viewed as an analog of
Theorem~\ref{t:normal-space-common-invariant} for non-invariant
subspaces. Theorem~\ref{t:invariance-proximity-bounds-error-in-F} is a
central result, as it allows to approximate models in the input-state
separable form (cf. Theorem~\ref{t:common-invariant-separates}) by
approximating a single normal invariant subspace of $\Kaug$, which is
significantly easier than working with the KCF directly. Moreover, the
concept of invariance proximity provides a bound for approximation
errors on the entire subspace.  This has important implications for
the validity and approximation accuracy of common Koopman-inspired
descriptions of the control system~\eqref{eq:control-system},
cf. Lemmas~\ref{l:switched-linear-separable}
and~\ref{l:linear-bilinear-separable}.

\section{Implications for Robust Data-driven Learning}
In this section we illustrate how the results of the paper can be used
in data-driven modeling of control systems. We provide an algorithmic
description that specifies how to process the data, the choice of
inner product space, and the formulation for the dictionary learning.
%
% \marginMH{I added this sentence to make sure that the reviewers will
	% not ask us to remove main results of the paper in favor of
	% simulations. Especially since we only consider a very specific inner
	% product but our results are for general inner product spaces.}
%

\subsection{Gathering Data for the Augmented Koopman Operator}
Our strategy for learning relies on using
Theorem~\ref{t:invariance-proximity-bounds-error-in-F} to
approximate an input-state separable form and bound the prediction
error for all functions in the identified subspace. This result
employs the augmented Koopman operator associated with the augmented
system~\eqref{eq:augmented-system} and, instead, we can only collect
trajectory data from the original control
system~\eqref{eq:control-system}. This mismatch can be easily
reconciled as we explain next. 
% It is important to keep in mind that unlike the original
% system~\eqref{eq:control-system} that can have a physical
% manifestation from which we can gather experimental data, the
% augmented system is simply a mathematical construct, therefore one
% cannot directly measure its trajectories.
% As a result, in this section, we explain how we can build a data set
% for the augmented system~\eqref{eq:augmented-system} from the
% trajectories of the original control
% system~\eqref{eq:control-system}.

Let $\{x_i\}_{i=1}^N \subset \Xc$, $\{u_i\}_{i=1}^N \subset \Uc$, and
$\{x_i^+\}_{i=1}^N \subset \Xc$ be state and input data from
trajectories of system~\eqref{eq:control-system} such that
\begin{align}\label{eq:data-snapshots-control}
	x_i^+ = \Tc(x_i, u_i), \quad \forall i \in \until{N}.
\end{align}
A close look at the definition of $\Taug$
in~\eqref{eq:augmented-system} reveals that it does not alter the
input signal, i.e., if we apply it on the state-input pair $x_i, u_i$
for all $i \in \until{N}$, we get
$\Taug(x_i,u_i) = (\Tc(x_i,u_i),u_i) = (x_i^+, u_i)$.  Therefore, we
already have access to all the information $\Taug$ generates: the
first element returned by $\Taug$ is exactly the action of the control
system $\Tc$ that we have measured
in~\eqref{eq:data-snapshots-control} and the second element is exactly
the input (without any change) to $\Tc$, again measured
in~\eqref{eq:data-snapshots-control}. For convenience, we gather these
data snapshots for $\Taug$ in snapshot matrices as follows
\begin{align}\label{eq:data-augmented-system}
	&X = [x_1, \ldots, x_N] \in \real^{n \times N},
	&&
	X^+ =[x_1^+,
	\ldots,
	x_N^+]\in
	\real^{n \times
		N}, 
	\nonumber
	\\
	&U = [u_1, \ldots, u_N]\in \real^{m \times N},  &&U^+ = U.
\end{align}
Note that even though matrix $U^+$ does not capture additional
information we have created it, since it is a part of the
corresponding state for $\Taug$. To apply existing numerical methods
such as EDMD on $\Kaug$, we gather the augmented state snapshots of
$\Taug$ as
\begin{align}\label{eq:state-snapshots-augmented-system}
	&Z = [X^T, U^T]^T \in \real^{(n+m) \times N},  
	\nonumber \\
	&Z^+ = [(X^+)^T, (U^+)^T]^T \in \real^{(n+m) \times N}.
\end{align}

\subsection{Choice of Inner Product Space}
The results in the previous sections can be used for subspace learning
on any arbitrary inner product space. Here we focus on the most
popular inner product space in the literature that is used for the
EDMD method~\cite{MOW-IGK-CWR:15,MK-IM:18}. Consider the empirical
measure $\mu_Z$ defined by
\begin{align}\label{eq:empirical-measure-z}
	\mu_Z = \frac{1}{N} \sum_{i=1}^{N} \delta_{z_i},
\end{align}
where $\delta_{z_i}$ is the Dirac measure at point $z_i$, the $i$th
column of matrix $Z$ defined
in~\eqref{eq:state-snapshots-augmented-system}. We then choose the
space $L_2(\mu_Z)$ comprised of functions on the domain
$\Xc \times \Uc$.  Under this choice, given any basis
$\Phi: \Xc \times \Uc \to \real^s$ with \emph{real-valued} elements
(cf. Remark~\ref{r:real-dictionary}) for the finite-dimensional (with
dimension $s$) normal subspace $\Sc$, the matrix $\tilde{A}$ in the
hypotheses of Theorem~\ref{t:invariance-proximity-bounds-error-in-F}
is the EDMD solution applied on dictionary $\Phi$ and data
in~\eqref{eq:state-snapshots-augmented-system} (cf.
Section~\ref{subsec:EDMD}), i.e.,
\begin{align}\label{eq:data-driven-EDMD}
	\tilde{A} = \Phi(Z^+) \Phi(Z)^ \dagger.
\end{align}
Moreover, under the condition that $\Phi(Z)$ and $\Phi(Z^+)$ have full
row rank, the invariance proximity turns into the square root of the
consistency index (cf.  Section~\ref{subsec:consistency-index}) and
has the following closed-form expression
\begin{align}\label{eq:data-driven-invariance-proximity}
	I_{\Kaug}(\Sc)
	&= \sqrt{\ic(\Phi,Z,Z^+)} 
	\nonumber \\
	&= \sqrt{\lambda_{\max}\big(I - \Phi(Z^+) \Phi(Z)^ \dagger \Phi(Z)
		\Phi(Z^+)^\dagger\big)} .
\end{align}
We use~\eqref{eq:data-driven-invariance-proximity} to formulate an
optimization-based learning problem for modeling the control system.

\subsection{Optimization-Based Subspace Learning}
Based on Theorem~\ref{t:invariance-proximity-bounds-error-in-F}(d),
the invariance proximity determines the accuracy of the model
provided on a given normal subspace. Hence, we formulate an
optimization problem to find an accurate model by minimizing the
invariance proximity over a parametric family of normal spaces with
basis $\Phi$ in normal form~\eqref{eq:normal-form-dictionary} as
\begin{align}\label{eq:optimization}
	&\underset{ \Phi \in \text{PF}}{\text{minimize}} \; I_{\Kaug}(\Sc)
	& \Leftrightarrow
	&&\underset{ \Phi \in
		\text{PF}}{\text{minimize}} \;
	\sqrt{\ic(\Phi,Z,Z^+)} \;, 
\end{align}
where $\text{PF}$ is the parametric family of choice (e.g.,~neural
networks, polynomials), $\Sc = \Span(\Phi)$, and one can use the
closed-form solution of the invariance proximity
in~\eqref{eq:data-driven-invariance-proximity}.  Note that depending
on the choice of the parametric family, the optimization
problem~\eqref{eq:optimization} is generally non-convex.
%%
%\marginJC{One reviewer mentioned we should point out that this problem
%is nonconvex. I agree! Also below, for (53).}
%\marginMH{I added this for~\eqref{eq:optimization}-\eqref{eq:edmd-residual}.}
%%

We make the following observations regarding the optimization
problem~\eqref{eq:optimization} and its properties:

\paragraph*{Alternative formulation for efficiency and numerical
	resiliency to finite-precision errors}
% \longthmtitle{Using Trace instead of $\lambda_{\max}$ in
	% Optimization~\eqref{eq:optimization}}
Using the closed-form expression for invariance proximity
in~\eqref{eq:data-driven-invariance-proximity} requires calculating
the maximum eigenvalue of
$M_C = I - \Phi(Z^+) \Phi(Z)^ \dagger \Phi(Z)
\Phi(Z^+)^\dagger$. This matrix has spectrum in $[0,1]$,
cf.~\cite[Lemma~1]{MH-JC:23-csl}. Many software packages for finding
maximum eigenvalues rely on iterative methods that are sensitive to
the separation between the largest and second largest
eigenvalues. To avoid numerical issues, one can use $\Tr(M_C)$
instead of $\lambda_{\max}(M_C)$, as justified by
\begin{align*}
	\frac{1}{s} \Tr(M_C) \leq \lambda_{\max}(M_C) \leq \Tr(M_C),
\end{align*}
where $s$ is the dimension of $M_C$. Note that the inequalities
follow from the fact that the spectrum of $M_C$ belongs to $[0,1]$.

\paragraph*{Equivalence to robust minimax problem}
% \longthmtitle{Optimization~\eqref{eq:optimization} is a Minimax
	% Problem}
Based on Theorem~\ref{t:RRMSE-bound-sprad-consistency}, the
optimization problem~\eqref{eq:optimization} is equivalent to the
following robust minimax problem
\begin{align*}
	\underset{ \Phi \in \text{PF}}{\text{minimize}} \max_{f \in \Sc,
		\| \Kaug f  \|_{L_2(\mu_Z)} \neq 0} \frac{\| \Kaug f - 
		\Pf_{\Kaug f} \|_{L_2(\mu_Z)}}{\| \Kaug f  \|_{L_2(\mu_Z)}},
\end{align*}
where $\Sc = \Span(\Phi)$ and $\mu_Z$ is defined
in~\eqref{eq:empirical-measure-z}. This equivalence makes it clear
that optimization~\eqref{eq:optimization} minimizes the worst-case
error on the subspace, does not depend on the choice of basis, and
is not sensitive to the scaling of variables.

\paragraph*{Differences with respect to minimizing residual error of
	EDMD}
% \longthmtitle{Optimization~\eqref{eq:optimization} vs Minimizing
	% EDMD's Residual Error}
A widely popular method for dictionary learning consists of
minimizing the residual error of EDMD as
\begin{align}\label{eq:edmd-residual}
	\underset{ \Phi \in \text{PF}}{\text{minimize}} \quad \|\Phi(Z^+)
	- \tilde{A} \Phi(Z) \|_F, 
\end{align}
where $\tilde{A} = \Phi(Z^+) \Phi(Z)^ \dagger$ is the solution of EDMD
applied on dictionary $\Phi$ and data $Z$ and $Z^+$.  Note that even
though~\eqref{eq:edmd-residual} might lead to reasonable accuracy for
dictionary elements, unlike optimization~\eqref{eq:optimization}, it
does \emph{not} necessarily lead to a `close to invariant'
subspace~\footnote{\cite[Example~1]{MH-JC:23-csl} provides an instance
  of a non-invariant subspace with arbitrarily close to zero residual
  error depending on the choice of basis.}, is sensitive to the choice
of basis for the subspace, and also to the scaling of optimization
variables.  Similarly to~\eqref{eq:optimization}, the optimization
problem~\eqref{eq:edmd-residual} is also generally non-convex
depending on the choice of the parametric family.

For the readers' convenience, Algorithm~\ref{algo:data-driven-model}
summarizes the steps described above to learn input-state separable
models.

\begin{algorithm}
	\caption{Learning Input-State Separable Models} \label{algo:data-driven-model}
	\begin{algorithmic}[1] % The number tells where the line numbering should start
		\State Gather data according
		to~\eqref{eq:data-augmented-system}-\eqref{eq:state-snapshots-augmented-system}
		% 
		\State Choose a parametric family  of normal dictionaries
		(e.g., neural networks, polynomials)
		with real-valued elements in the form~\eqref{eq:normal-form-dictionary}
		% 
		\State Obtain $\Phi^*$  by solving~\eqref{eq:optimization} using
		the closed-form expression for the
		cost in~\eqref{eq:data-driven-invariance-proximity}
		% 
		\State Calculate
		$\tilde{A} = \Phi^*(Z^+) \Phi^*(Z)^ \dagger$
		% 
		\State Find the input-state separable form via
		Theorem~\ref{t:invariance-proximity-bounds-error-in-F}(b)
	\end{algorithmic}
\end{algorithm}

%\subsection{Pendulum with Nonlinear Friction and Saturated Input}
%
%
% \marginMH{I turned the following into an example. Subsection did not
% seem appropriate.} \marginJC{Sounds good!}
\begin{example}\longthmtitle{Pendulum with Nonlinear Friction and
    Saturated Input}
%Here we provide an example of the application of our approach to
%data-driven modeling of control systems.  
{\rm
Consider the following
nonlinear pendulum
\begin{align}\label{eq:nonlinear-pendulum}
	\dot{x}_1
	&= x_2, 
	\nonumber
	\\
	\dot{x}_2
	&= -9.81 \sin x_1 - x_2 - 0.1\, x_2^3 + 5 \tanh(0.5\, u),
\end{align}
where $x = [x_1, x_2]^T$ is the state vector and $u$ is the input. The
term $\tanh(0.5\, u)$ models the actuator's nonlinear saturation. Note
that the system is nonlinear in the input and therefore linear and
bilinear methods associated with control-affine systems do not apply.
%
% \marginJC{Have you tried compared our plots with what one would get
%   using a linear/bilinear method?}  \marginMH{No. The Theory of lifted
%   bilinear models comes from differential geometry (not related to
%   Koopman Operator but is called Koopman lift!) and requires the
%   system to be control affine. So the assumption of the so-call
%   Koopman linear/bilinear lifting do not hold. Regarding the
%   simulations, no I have not done that. If you think it is necessary
%   it would take around three weeks to write all the code and train the
%   neural networks and create comparisons. We can wait for the
%   reviewers to complain then we add it (this way we have something
%   positive to say:-) ).}
% %
%
% \marginJC{The weakness in the presentation is that we do not compare
%   our method to anything: we might have discussed the reasons back
%   when we originally submitted the paper, but I don't remember
%   now. E.g., why not to (53)?}
%

\textit{Data:} We run $10^4$ experiments with constant inputs and
length $2.5$ seconds starting from uniformly selected initial
conditions from $[-\frac{\pi}{2},\frac{\pi}{2}]^2$ and inputs from
$[-6,6]$. We sample the trajectories with time step $\Delta t = 25$
milliseconds, resulting in a total of $10^6$ data snapshots. Out of
this data set, we randomly select $N = 5 \times 10^4$ snapshots as the
training data set and the same amount as the test data set.

\textit{Parametric Family:} To model the normal basis in the
form~\eqref{eq:normal-form-dictionary}, we choose the dimension of the
normal space $s = 16$ and the dimension of the input-state separable
model as $l = 8$. We model the functions $H(x)$ and $\tilde{G}(u)$
in~\eqref{eq:normal-form-dictionary} by two feedforward neural
networks, each with 5 hidden layers and 200 neuron per layer and
exponentially linear unit (ELU) activation function. We also fix the
first two elements of $H(x)$ to be the state vector corresponding to
the system.

\textit{Training:} We simultaneously train the neural networks (whose
weights and biases are randomly initialized) using the (relaxed) trace
version of the consistency index mentioned above for 2000 epochs based
on the Adam method with batch size of 500 data points and learning
rate $5 \times 10^{-4}$. The final invariance proximity calculated
by~\eqref{eq:data-driven-invariance-proximity} on the training and
test data sets are $0.028$ and $0.03$ respectively.
%%
%\marginJC{How do we initialize the optimization?}
%\marginMH{Please see above.}
%%

Finally, we use the formula in
Theorem~\ref{t:invariance-proximity-bounds-error-in-F}(b) to build an
input-state separable model. To evaluate the accuracy of the model, we
create a piecewise constant random input with time step $\Delta t$ for
$10s$ and compare the model's response with the actual system
trajectories generated by~\eqref{eq:nonlinear-pendulum} starting from
three different initial conditions as illustrated in
Figure~\ref{fig:pendulum-random-input}. As the plots show, the
obtained model accurately predicts the system behavior.
% Figure environment removed
\oprocend
}
\end{example}

\subsection{Special Case of Linear Models}
In many control applications, one only requires reasonably accurate
models since, in general, the use of feedback creates robustness
against model mismatches. For instance, in model predictive control
(MPC) schemes, one only requires models that provide reasonably
accurate \emph{finite-horizon} predictions. A sensible strategy in
such cases is to use simpler (and possibly less accurate) models with
favorable structure to gain significant improvement in computational
cost. This renders linear lifted models,
e.g.,~\cite{MK-IM-automatica:18}, a powerful tool for predictive
control applications. The next result explains how to use the results
in the paper to derive such models.

\begin{lemma}\longthmtitle{Normal Basis Form for Linear
		Models}\label{l:linear-model}
	Consider the dictionary in normal form
	in~\eqref{eq:normal-form-s-gt-l} with $s = m+l$, where $m$ is the
	dimension of the input vector.  Impose the additional structure
	$H(x) = [\Psi(x)^T, 1_\Xc(x)^T]^T$ and
	$\tilde{G}(u) = [0_{m \times (l-1)}, u]$, where
	$\Psi: \Xc \to \real^{l-1}$ and $1_\Xc: \Xc \to \real$ such that
	$1_\Xc (x) = 1$ for all $x \in \Xc$.
	% and $m$ is the dimension of the input vector.
	Then, the input-state separable model in
	Theorem~\ref{t:invariance-proximity-bounds-error-in-F}(b) turns into
	the following linear form with dimension $l$
	\begin{align*}
		\begin{bmatrix}
			\Psi(x^+)
			\\
			1_\Xc(x^+)
		\end{bmatrix}
		\approx
		\tilde{A}_{11}
		\begin{bmatrix}
			\Psi(x)
			\\
			1_\Xc(x)
		\end{bmatrix}
		+ \tilde{A}_{12} u. \eqoprocend
	\end{align*}
\end{lemma}
\smallskip

The proof of Lemma~\ref{l:linear-model} is a direct calculation based
on the closed-form in
Theorem~\ref{t:invariance-proximity-bounds-error-in-F}(b) and the fact
that $u 1_\Xc(x) = u$ for all $(x,u) \in \Xc \times
\Uc$. Lemma~\ref{l:linear-model} enables us to learn lifted linear
models by incorporating an additional (but simple) structure in
optimization~\eqref{eq:optimization} to learn lifted linear models.

\section{Conclusions}
We have presented the notion of Koopman Control Family (KCF), a
theoretical framework for modeling general nonlinear control
systems. We have shown that the KCF can fully characterize the
behavior of a control system on a (potentially infinite-dimensional)
function space. To build finite-dimensional models, we have introduced
a generalized notion of subspace invariance, leading to a universal
finite-dimensional form which we refer to as \emph{input-state
	separable}. Remarkably, the commonly-used lifted linear, bilinear,
and switched linear models are all special cases of the input-state
separable form. We have provided a complete theoretical analysis
accompanied by discussions on usage in data-driven
applications. Future work will build on the results of the paper to
include theoretical strategies for control design as well as methods
to determine reachable and control invariant sets. We also aim to
explore additional structures that the KCF might enjoy for special
classes of nonlinear systems such as control-affine and monotone
systems.

{
\small
\bibliographystyle{plainnat}
\bibliography{alias,JC,Main,Main-add}
}

\medskip

\setlength{\intextsep}{0pt}%
\setlength{\columnsep}{5pt}%
\begin{wrapfigure}{l}{0.25\linewidth}
  \centering
  % Figure removed
\end{wrapfigure}
\textbf{Masih Haseli} received the B.Sc. and M.Sc. degrees in
electrical engineering from the Amirkabir University of Technology
(Tehran Polytechnic), Tehran, Iran, in 2013 and 2015, respectively. He
also received the Ph.D. degree in Engineering Sciences (Mechanical
Engineering) from the University of California San Diego, CA, USA, in
2022. He is currently a postdoctoral researcher with the Department of
Mechanical and Aerospace Engineering, University of California, San
Diego, CA, USA. His research interests include system identification,
nonlinear systems, network systems, data-driven modeling and control,
and distributed and parallel computing. Dr. Haseli is the recipient of
the Bronze Medal of the 2014 Iran National Mathematics Competition and
the Best Student Paper Award of the 2021 American Control Conference.

\smallskip
\begin{wrapfigure}{l}{0.25\linewidth}
  \centering
  % Figure removed
\end{wrapfigure}
\textbf{Jorge Cort\'es} received the Licenciatura degree in
mathematics from Universidad de Zaragoza, Spain, in 1997, and the
Ph.D. degree in engineering mathematics from Universidad Carlos III de
Madrid, Spain, in 2001. He held postdoctoral positions with the
University of Twente, The Netherlands, and the University of Illinois
at Urbana-Champaign, USA. He was an Assistant Professor with the
Department of Applied Mathematics and Statistics, UC Santa Cruz,
USA. He is a Professor in the Department of Mechanical and Aerospace
Engineering, UC San Diego, USA. He is a Fellow of IEEE, SIAM, and
IFAC. His research interests include distributed control and
optimization, network science, nonsmooth analysis, reasoning and
decision making under uncertainty, network neuroscience, and
multi-agent coordination in robotic, power, and transportation
networks.




\end{document}
