\def\year{2024}\relax
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
% \usepackage[square,comma]{natbib} 
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS

\usepackage{amsmath,epsfig}
% \usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{array}
\usepackage{color}
\usepackage{bbding}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[capitalize]{cleveref}
\usepackage{pifont}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{float} 
%\usepackage{subfigure}

\newcommand{\xmark}{\ding{55}}%
\newcommand{\upmark}{\textcolor[RGB]{215,35,35}{$\uparrow$}}
\definecolor{sota}{RGB}{0, 0, 255}
\definecolor{commentcolor}{RGB}{6, 152, 124}
\newcommand{\PyComment}[1]{\footnotesize\ttfamily\textcolor{commentcolor}{\# #1}}
\newcommand{\PyCode}[1]{\small\ttfamily\textcolor{black}{#1}}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}
  \setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt plus 0.3ex}
}

\pagestyle{empty}


% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}


\begin{document}

\title{CLIP Brings Better Features to Visual Aesthetics Learners}
\author{Liwu Xu\footnotemark[2], Jinjin Xu\footnotemark[2], Yuzhe Yang, Yijie Huang, Yanchun Xie, Yaqian Li\thanks{Corresponding author.}\\
OPPO Research Institute\\
{\tt\small \{xuliwu, jinxu95, yangyuzhe, huangyijie, xieyanchun, liyaqian\}@oppo.com}
}

\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[2]{Equal contribution.}


\begin{abstract}
%Recent image aesthetics assessment (IAA) algorithms rely on high resolution or multi-patch input and the model parameters are overweight, which is not computationally-friendly to mobile devices. In addition, we see little discussion on IAA algorithm generalization ability, while it is vital and affects user experience in application scenario. In this work, we first-time discuss challenges of design lightweight IAA algorithm and propose a mobile device-oriented IAA scheme named \textbf{S}tronger \textbf{T}eacher-based \textbf{S}emi-supervised \textbf{K}nowledge \textbf{D}istillation (STSKD).
%A growing discrepancy exists in the field of image aesthetics assessment (IAA) between large-scale models with high input resolutions and models that are deployed on edge devices

%Most existing image aesthetics assessment (IAA) algorithms rely on large modeling capacities and high input resolutions to improve representation ability, which limits their applications on edge devices.

% pretrain method -> data hungury iaa -> clip distilled + semi-supervised

% There exists an increasing discrepancy in the field of image aesthetics assessment (IAA) between large-scale models with powerful representation capabilities and lightweight models that need to be deployed on edge devices. Moreover, although generalization ability is crucial to the user experience of computational aesthetics, the trap of overfitting is one that most IAA algorithms cannot escape. In this work, we design an IAA protocol that generalizes well and infers fast with the aid of a CLIP distilled stronger teacher and massive unlabeled data. Specifically, we first utilize feature distillation to train a well-learned \textbf{S}tronger \textbf{T}eacher by CLIP, then improve the pseudo-label quality via multi-patch aggregation under a test-time-augmentation manner. We then leverage the \textbf{S}emi-supervised \textbf{K}nowledge \textbf{D}istillation to facilitate the learning efficiency and generalization ability of the lightweight student model. Experimental results indicate that the proposed method achieves state-of-the-art performance on benchmark datasets and it can also improve model generalization ability, according to the cross-dataset evaluation. 
% Code and data are available at.
% The success of pre-training approaches on a variety of downstream tasks has revitalized the field of computer vision. Image aesthetics assessment (IAA) is one of the ideal application scenarios for such methods due to the difficult and expensive labeling procedure. In responding to this, an unified and flexible \textbf{C}LIP-based \textbf{S}emi-supervised \textbf{K}nowledge \textbf{D}istillation protocol is proposed for IAA in this work, namely CSKD. Specifically, we first leverage a multi-source unlabeled dataset to align the features between the given visual encoder and the CLIP image encoder in an unsupervised manner, aiming to enhance the generalization ability of the given encoder. Then, the encoder is not limited by size and, once well-trained, and can serve as a better learner for semi-supervised IAA tasks due to the alleviation of feature collapse problem. Extensive experiments indicate the superiority of our method, which achieves state-of-the-art performance on multiple widely used IAA benchmarks. (20230728 previous version)
The success of pre-training approaches on a variety of downstream tasks has revitalized the field of computer vision. Image aesthetics assessment (IAA) is one of the ideal application scenarios for such methods due to subjective and expensive labeling procedure. In this work, an unified and flexible two-phase \textbf{C}LIP-based \textbf{S}emi-supervised \textbf{K}nowledge \textbf{D}istillation paradigm is proposed, namely \textbf{\textit{CSKD}}. Specifically, we first integrate and leverage a multi-source unlabeled dataset to align rich features between a given visual encoder and an off-the-shelf CLIP image encoder via feature alignment loss. Notably, the given visual encoder is not limited by size or structure and, once well-trained, it can seamlessly serve as a better visual aesthetic learner for both student and teacher. In the second phase, the unlabeled data is also utilized in semi-supervised IAA learning to further boost student model performance when applied in latency-sensitive production scenarios. By analyzing the attention distance and entropy before and after feature alignment, we notice an alleviation of feature collapse issue, which in turn showcase the necessity of feature alignment instead of training directly based on CLIP image encoder. Extensive experiments indicate the superiority of CSKD, which achieves state-of-the-art performance on multiple widely used IAA benchmarks. 
%STSKD consists of two parts, including 1) stronger teacher (ST) and 2) semi-supervised knowledge distillation (SKD). ST aims at improving pseudo label quality via multi-patch aggregation under test-time-augmentation manner (TTA-MPA) and large teacher capacity. Then, SKD helps to distill knowledge from large teacher to lightweight student model with both labeled and unlabeled data to improve knowledge distillation efficiency and generalization ability. 
\end{abstract}
%
% \begin{keywords}
% image aesthetics assessment, feature alignment, semi-supervised knowledge distillation
% \end{keywords}
%
\section{Introduction}
\label{sec:intro}

%% --- give a brief introduction about IAA and cut in the necessity of generalization ability
% Image aesthetics assessment (IAA) aims at evaluating aesthetics perception of an image automatically and it has been broadly applied on various areas, such as image enhancement \cite{talebi2018learned}, personalized photo album management \cite{Ren_2017_ICCV} and etc. It is noteworthy that with the explosively increased amount and content diversity of photos, a novel challenge has also been brought into IAA task, generalization ability, which has also raised continuous attention from both industry and academia \cite{valenzise2022advances}. In addition, mobile device-oriented lightweight algorithm design is also critical since one of IAA technique's application is album management and recommendation.\par


% Meanwhile, the emergent abilities of large language models (LLMs) \cite{} makes multi-modal  traditional computer vision tasks \cite{}

The advent and success of unsupervised and multi-modal pre-training, such as GPTs \cite{radford2018improving, radford2019language,brown2020language} and CLIP \cite{radford2021learning}, have bought deep learning into a whole new era. Nevertheless, in the field of computer vision, the benefits of scaling up the model and data volumes are not as great as in natural language processing tasks, which may be caused by the low information density of visual inputs. Hence, leveraging cross-modal information becomes more and more popular for visual pre-training. 

Image aesthetics assessment (IAA), as a traditional computer vision task, aims at evaluating the aesthetic perception of an image automatically and has been broadly applied in various areas, such as image enhancement \cite{talebi2018learned}, personalized photo album management \cite{Yang_2022_CVPR} and etc. However, IAA task suffers from the poor model generalization ability due to the complex, non-uniform and expansive annotating process \cite{valenzise2022advances}. In the early stage, researchers dedicate themselves to developing hand-crafted features \cite{ke2006design}, while the representation ability of such features is not sufficient to capture complicated human perception. Fortunately, the emerging DL-based IAA algorithms show great potential in solving this problem \cite{talebi2018nima, ke2021musiq}. For example, Talebi et al. propose a simple-yet-effective scheme called NIMA that far exceeds the performance of hand-crafted methods \cite{talebi2018nima}. Recently, various deep learning (DL) based methods have been proven to be effective, such as multi-patch aggregation \cite{9777255}, composition-aware IAA \cite{she2021hierarchical} and multi-scale feature fusion \cite{ke2021musiq}. Furthermore, the transfer of knowledge between semantic features and aesthetics features has garnered significant attention among researchers, resulting in state-of-the-art outcomes \cite{he2022rethinking, hou2022distilling}.

% problems exist
% However, previous methods are not designed for mobile devices due to high computational cost and we see little discussion on lightweight IAA algorithm design. Therefore, In addition, cross dataset validation has been utilized for measuring algorithm generalization ability unsatisfying. It indicates that the generalization ability of current methods still remains to be improved and may suffer from over-fitting issues.

% For on-device algorithm design, power consumption is the main restriction and it relies on but not limited to image resolution, model capacity and etc.

% While data fidelity refers to a task-specific requirement that commonly used data augmentation, such as color jitter, gaussian blur, cannot be used in IAA task since it changes aesthetic related features and the data is not valid anymore. Under this setting, conflicts exist in IAA task. 

% First, awaiting processed image should be resized into a small size. Processing image with large resolution or multiple patches leads to relatively large inference time and power consumption. Second, model should be a lightweight one. Deploying model with large capacity on mobile device is not applicable, despite the fact that it surpasses lightweight model performance with large margin. One natural proposal is that we can enhance student model performance by knowledge distillation. However, we notice that the student model shows limited improvements, which motivates us to rethink ways of improving lightweight algorithm performance. Last but not least, algorithm generalization ability. Supervised learning-based IAA results are reliable when test samples are similar with samples in training set. When it meets data in various authentic scenarios (e.g., user album), model performance will become uncontrollable, since it meets lots of images that are unseen or dissimilar to training samples. \par

The high time \& space complexity of most DL-based methods make them hard to be deployed on mobile devices, and the lightweight design of such methods usually leads to an unacceptable performance drop. This is because the reduced image size and degenerated model capability amplify the error of the algorithms. One natural proposal is to enhance the performance of the on-device model by knowledge distillation, but a curious phenomenon undermines this hypothesis: without massive data volume, the lightweight student IAA model shows minor improvements even with a large, well-trained teacher model in a supervised manner. 

% Figure environment removed

In response to this, a novel \textbf{C}LIP-based two-phase \textbf{S}emi-supervised \textbf{K}nowledge \textbf{D}istillation method is proposed in this work, namely CSKD, which not only improves the generalization ability but also enhances the knowledge distillation efficiency of IAA algorithms. As illustrated in Fig.\ref{fig:pipeline}, we firstly utilize a large amount of unlabeled data (~3 million) to align the features of the visual encoders and CLIP image encoder/backbone with no constraints on the model structure. Then, the well-trained encoder (e.g., Swin-Base) is combined with a task-specific MLP head and fine-tuned in a supervised manner, which leads to a strong teacher for pseudo label generation. Finally, we train an efficient and generalizable student (e.g., MobileNetV2) via semi-supervised knowledge distillation. Contributions can be summarized in three folds:

% \begin{itemize}[itemsep=1.5pt,topsep=0pt,parsep=0pt]
\begin{itemize}
    % \item We first-time analyze the trade-off between IAA algorithm performance and on-device design restrictions. Moreover, we demonstrate the necessity and effectiveness of adopting semi-supervised learning into IAA for improving algorithm generalization ability.
    % \item This is the first study to examine how the size and performance of DL-based IAA models trade off. Additionally, we show how adding semi-supervised learning to IAA improves the algorithm's generalization capacity.
    \item An unified and generalizable unsupervised \textbf{C}LIP-based \textbf{F}eature \textbf{A}lignment (CFA) method is proposed in this paper, and proved to be a good solution for visual encoder pre-training.
    % \item A novel scheme named STSKD is presented and proven to be a versatile and effective tool for IAA tasks. The large-scale labeled and unlabeled data with high-quality pseudo labels generated by the CLIP-guided stronger teacher, and a semi-supervised knowledge distillation strategy make this possible.
    \item A \textbf{S}emi-supervised \textbf{K}nowledge \textbf{D}istillation (SKD) protocol is presented and proven to be a versatile and effective tool for IAA tasks. Both teacher and student distilled from CLIP show better performance when learning from large-scale unlabeled data.
    \item The proposed pipeline is evaluated with extensive experiments on commonly used IAA benchmarks. With our protocol, the aforementioned NIMA algorithm can surpass state-of-the-art results.

    %the NIMA scheme can surpass SOTA and achieve competitive results with basic and lightweight models.
\end{itemize}



\section{Related Work}
%% --- related works on deep iaa
% \subsection{Image Aesthetics Assessment}
% Recent deep learning-based IAA algorithms have shown great potential in aesthetics assessment. Lu et al. \cite{lu2015deep} claim that learning aesthetics representation with single-patch crop may lead to ambiguity, so that they propose a multi-patch aggregation network by cropping input image into multiple patches and aggregate patch features before prediction to reduce learning ambiguity. Mai et al. \cite{mai2016composition} propose a composition-preserving method that generates aesthetics score with adaptive spatial pooling. Later, studies shows that IAA can be regarded as a label distribution learning task by minimizing the distance between prediction and annotated score distribution \cite{talebi2018nima}. Hosu et al. \cite{hosu2019effective} point out that with multi-level spatially pooled features and original image as input, the learning performance can be improved significantly. However, most algorithms are not applicable on mobile device due to high computation cost in inference stage.

% Moreover, studies also discover to learn IAA with other vision tasks, such as personality traits prediction \cite{li2020personality}, distill knowledge from object detection \cite{hou2020object} and etc. Studies have also opened up IAA from representation learning perspective. Sheng et al. \cite{sheng2020revisiting} revisit the value of self-supervised learning in IAA task and propose a quality-aware pretext task for learn aesthetics representation. These approaches proves that utilizing data or knowledge from other relevant tasks are effective, while value of unlabeled data in IAA under semi-supervised learning manner has not yet been discussed. \par


%% --- related works on semi supervised learning
\subsection{Pre-training}
Unsupervised, weak-supervised and self-supervised pre-training techniques have gained a lot of attention in the field of computer vision recently \cite{erhan2010does}. Among the many efforts, CLIP \cite{radford2021learning} represents a crucial milestone due to its pioneering use of large-scale web-crawled data for image-text contrastive learning, yielding better feature representation ability and impressive zero-shot performance. And hence, the pre-trained CLIP visual encoder has been widely adopted as the feature extractor for various downstream tasks, such as MaskCLIP \cite{zhou2022extract} and Stable Diffusion \cite{rombach2022high}. 

Although the application progress of CLIP in the field of aesthetics has been slow, there have been some insightful works. For example, Wang et al. design a series of general prompts like \textit{\{good/bad photos\}} to assist the assessment of the images \cite{wang2022exploring}. In this paper, we further leverage the representation ability of CLIP to improve the performance of IAA models via knowledge distillation.

\subsection{Knowledge \& Feature Distillation}

Knowledge distillation (KD) \cite{hinton2015distilling} is an unified paradigm that enhances the performance of student models through the utilization of both ground truth labels and pseudo labels (hard or soft) generated by proficient teacher(s). KD offers a range of comprehensive benefits, including the reduction of model size and the improvement of model generalization \cite{9340578}. Nevertheless, we see limited improvements in the student model when normal KD is performed using labeled data only in IAA tasks (refer to Table \ref{tab:kd_skd}), yet Chen et al. claim that conducting knowledge distillation with unlabeled data can further improve learning efficiency \cite{chen2020big}, which inspires us to develop a semi-supervised knowledge distillation manner to increase the efficiency of KD.

Feature distillation (FD) is a subclass of KD, where knowledge is transferred by learning feature representations instead of outputs \cite{wang2021knowledge}. One crucial distinction between FD and KD lies on the flexibility, where the teacher and student models can originate from different domains, possess varying output dimensions, and exhibit other dissimilarities. Recent works showcase that learnt representations (e.g. CLIP) can be distilled to a untrained model via feature distillation \cite{wei2022contrastive, fang2023eva}. Inspired by this, we also adopted an off-the-shelf CLIP image encoder as teacher to transfer rich knowledge to our given visual encoder as initialized parameter (followed by a MLP projector). 

Notably, there are two major differences in our scheme. 1) Distillation target. We choose to use the CLS token, so that there is no model structure dependency and spatial alignment issues. 2) Initialization of distilled model. We start with ImageNet pretrained parameters instead of train from scratch for faster convergence and better performance.

% Hinton et al. point out that that knowledge can be compressed into single model \cite{hinton2015distilling}.

% Pseudo labeling is a simple yet effective protocol in semi-supervised learning \cite{lee2013pseudo}, where the ``mean teacher" with exponential moving average (EMA) strategy \cite{tarvainen2017mean}, the ``noisy student" strategy \cite{xie2020self}, and the fixmatch-based consistency strategy \cite{sohn2020fixmatch} have shown to be efficient 
% in image classification, semantic segmentation and many other fields.

% However, the above-mentioned approaches cannot be utilized in IAA tasks directly, since strong augmentations will significantly influence the subjective aesthetic perception of a certain image and the confidence levels of model predictions are difficult to measure directly. Thus, this task-specific restriction bans a large group of augmentation-based and confidence-based semi-supervised approaches, such as fixmatch.


\section{CSKD: CLIP-based Semi-supervised Knowledge Distillation}
As shown in Fig.\ref{fig:pipeline}, the proposed workflow consists of two phases, CLIP-based feature alignment (CFA) and semi-supervised knowledge distillation (SKD). In terms of phase I, a multi-source (e.g., ImageNet \& OpenImages detection subset) unlabeled dataset is introduced to train well generalized backbones (for teacher and student, respectively) for the next stage. In phase II, a teacher model (CFA teacher backbone and a MLP head) is fine-tuned using labeled task-specific data, then both labeled and unlabeled data are used to train the student model (CFA student backbone and a MLP head) in a semi-supervised manner. In the following content, we will introduce components of CSKD separately.

%We know that two critical factors influence semi-supervised IAA, 1) pseudo-label correctness and 2) efficiency of knowledge distillation. the designed scheme improves those from two perspectives, TTA-MPA and utilize teacher model with large capacity.
\subsection{Phase I: CLIP-based Feature Alignment (CFA)}
\label{sec:cfa}
In this section, we use the off-the-shell CLIP image encoder to adjust the feature distribution of task-specific backbones because, as was already mentioned, the feature collapse problem of CLIP has been alleviated by the plenty of training picture-text pairings. In this way, only unlabeled data is needed because closer feature representations rather than similar logit outputs are what are crucial. The pipeline of CFA phase is illustrated on the left side of Fig. \ref{fig:pipeline}. 

Specifically, we first create a multi-source unlabeled dataset of around 3M images from ImageNet \cite{deng2009ImageNet} (1.2M) and OpenImages \cite{kuznetsova2020open} detection subset (1.9M). Then, given a pre-trained backbone (for example, Swin-Base-In21k from timm \cite{rw2019timm}), we can further align the representation maps between CLIP image encoder and the given backbone (followed by a MLP projector) via a cosine similarity based alignment loss,
\begin{equation}
    \mathcal{L}_{align}(\bm{x}_1, \bm{x}_2) = 1 - \frac{\bm{x}_1 \cdot \bm{x}_2}{max(||\bm{x}_1||_2 \cdot ||\bm{x}_2||_2, \epsilon)}, 
\end{equation}
where $\bm{x}_1$ and $\bm{x}_2$ are the CLS token of CLIP and the projected feature of the given backbone, respectively. And $\epsilon$ is a constant that is usually set to $1e-8$ to avoid zero dividing. 

Notably, we train three backbones (e.g., Swin-Base \& MV2) in this way for the following phase, referred to as CFA teacher and CFA student, respectively. Discussions about the feature representations before and after alignment are given in Section \ref{sec:attention_ana}.


% \subsection{Reliable pseudo labeling with Stronger Teacher}
% % \subsubsection{Enhanced pseudo labels with TTA-MPA}
% The core point of the ST stage is the multi-patch aggregation (MPA) strategy, which has been wildly used in image style, aesthetic and quality estimation \cite{lu2015deep}. However, previous works utilize MPA at both the training and inference stages to avoid learning ambiguity, which brings high time and power consumption. To avoid this, we use MPA under test-time-augmentation (TTA) only to generate high-quality pseudo labels. Concretely, we first resize the original image into $H$x$W$ (e.g., 256x256) and then use ten-crop augmentation to generate a patch serial, in which each image is cropped into $h$x$w$ (e.g., 224x224) together with randomly applied horizontal flip. Later, each image patch is attached with a pseudo-label and aggregated by calculating the average distribution. Finally, the averaged pseudo-label is used for later SKD. The entire workflow of TTA-MPA is given in Fig. \ref{fig2}. 
% % Figure environment removed

% \subsubsection{Large teacher capacity}
% We know that the capacity of the teacher model plays a critical role when generating pseudo labels. For example, the model with a large capacity (e.g., Swin Transformer) can outperform mid- or small-scale one (e.g., ResNet50 or MobileNet-v2) with a large margin. 
% % Stronger teachers, 
% Larger teacher model, however, do not always result in better student models when just supervised learning is used, as it might be easy to fall into local optimization for particular tasks in the absence of enough data.

\subsection{Phase II: Semi-supervised Knowledge Distillation}
\label{sec:skd}
% Model compression is essential for on-device IAA algorithm design. 
It is typically to fine-tune models with task-specific labeled datasets for downstream tasks. We initially fine-tune the teacher (formed by a CFA teacher backbone and a MLP head) using AVA \cite{murray2012ava}, AADB \cite{kong2016photo} or PARA \cite{yang2022personalized} benchmarks. Then, the well-trained teacher is used to supervise the student (formed by a CFA student backbone and a MLP head). The pipeline of phase II can be found on the right hand of Fig. \ref{fig:pipeline}. However, in supervised KD settings, a larger teacher does not necessarily produce a better student when there is no enough data. Inspired by a recent study \cite{chen2020big}, we innovatively optimize the student model on both labeled and unlabeled data by minimizing the squared earth-mover-distance (EMD) \cite{talebi2018nima} between predictions and human-annotated labels or pseudo labels (called semi-supervised knowledge distillation, SKD), which is:
\begin{equation}
    \mathcal{L}_{EMD}(\bm{p}, \bm{q}) = (\frac{1}{d}\sum_{i=1}^{d}|(CDF_{\bm{p}}(i)-CDF_{\bm{q}}(i)|^r)^{1/r}, 
\end{equation}
in which $r$ is a hyper-parameter that is set to 2, $\bm{p} \in R^d$ and $\bm{q} \in R^d$ represent prediction and target vectors, and CDF refers to the cumulative distribution function, respectively. The objective function of the student model in the SKD stage is:
\begin{equation}
    \mathcal{L}_{stu} = \mathcal{L}_{s} + \beta * \mathcal{L}_{kd},
\end{equation}
where $\beta$ is a fixed balancing factor, and $L_{s}$ refers to the loss of supervision:
\begin{equation}
    \mathcal{L}_{s} = \frac{1}{B_s}\sum_{j=1}^{B_s}\mathcal{L}_{EMD}(\bm{p}^{l}, \bm{q}^{l}),
\end{equation}
where $B_{s}$ is the size of labeled data per batch, and $L_{kd}$ is the pseudo label based knowledge distillation loss:
\begin{equation}
    \mathcal{L}_{kd} = \frac{1}{B_s +\mu B_s}\sum_{k=1}^{B_s +\mu B_s}\mathcal{L}_{EMD}(\bm{p}^{u}, \bm{\hat{q}}^{u}),
\end{equation}
in which $\mu$ indicates the proportion of 
% data without/with pseudo labels. 
unlabeled data, and $\bm{\hat{q}}^{u}$ is the pseudo target vector.
Thus, the total batch size $B$ equals to $B_s + \mu * B_s$.

\section{Experiments}
\subsection{Datasets and Evaluation Protocol}
% In this work, we adopt three publicly available datasets for training and evaluation. Two IAA benchmark datasets (i.e., AVA, AADB) are utilized as labeled data. ImageNet dataset is selected as the source of unlabeled data. Below, we give introductions on datasets and train-test split condition.
% \paragraph{\textbf{AVA}}
% \subsubsection{\textbf{AVA}}
% AVA dataset \cite{murray2012ava} contains more than 250,000 images and each image is rated by 78 to 594 individual subjects. The aesthetic scores range from 1 to 10, indicating low to high aesthetic level. Aesthetics score distributions of images are used as ground truth of aesthetic perception. Note that since the aesthetic score in AVA ranges from 1 to 10, we normally use score 5 as threshold and transfer the mean score into classification labels, i.e., low and high aesthetic. In this work, we randomly select 237,623 images into train set and the rest 17,887 images are used as test set for performance evaluation.

% \subsubsection{\textbf{AADB}}
% The AADB dataset \cite{kong2016photo} collected 10,000 images annotated by 190 users. Except for the overall aesthetic score, each image is also attached with 11 aesthetic attributes annotation by at least 5 users. The attributes describe photo aesthetic from 11 perspectives. The ranges of aesthetic scores are [1, 5]. Higher value indicates high aesthetic perception, we randomly select 8,345 images from AADB as training set and 1,000 images as test set.

% \subsubsection{\textbf{ImageNet}}
% ImageNet \cite{deng2009ImageNet} is a large-scale visual dataset built upon the backbone of the WordNet structure. ImageNet contains over 14 million images are hand-annotated and at least 1 million images are annotated with bounding box. Later, ImageNet was used in ILSVRC, which is a large-scale vision recognition competition and with 1000 non-overlapping classes and images from ImageNet dataset are selected. In this work, we use the ImageNet dataset in ILSVRC 2012 to provide about 1.2 million unlabeled data to boost the generalizability of semi-supervised IAA algorithms.




%%%%%%%%%%%%%%%%加入两种设定的修改 by liwu

% AVA dataset  contains more than 250,000 images and each image is rated by 78 to 594 individual subjects. The aesthetic scores range from 1 to 10, aesthetics score distributions are used as ground truth of aesthetic perception. We randomly select 237,623 images into train set and the rest 17,887 images are used as test set. The AADB dataset  collected 10,000 images annotated by 190 users. The ranges of aesthetic scores are [1, 5]. We randomly select 8,345 images from AADB as training set and 1,000 images as test set. Two typical semi-surprised evaluation protocol for validating our method on IAA task:

Following previous work, three trustworthy benchmark IAA datasets, namely AVA \cite{murray2012ava}, AADB \cite{kong2016photo} and PARA \cite{yang2022personalized}, are adopted to evaluate the performance of the proposed scheme. The number of samples labeled with mean opinion score (MOS) are 242,538, 10,000 and 31,220 of AVA, AADB and PARA, respectively, where 237,623, 8345 and 28220 samples are randomly selected for training.
% And semi-supervised evaluation settings used in this work are summarized in Table \ref{tab:data_setting}.

% Moreover, two typical semi-supervised evaluation settings used in this work are as follows:

% \textbf{Partially Labeled}: Randomly select $1\%$, $5\%$, and $10\%$ of the samples (with labels) from the AVA dataset at a time, and remove the labels from the rest.

% \begin{table}[t]
% \caption{Data settings for SKD.}

% \label{tab:data_setting}
% \centering
% \footnotesize
% \begin{tabular}{lc|cc}
% \toprule
% &   & \multicolumn{2}{c}{Unlabeled} \\ \cline{3-4}
%                                               &       & ImageNet    & OpenImages \\ 
% \midrule
% \multicolumn{1}{c|}{\multirow{3}{*}{Labeled}} & AVA \cite{murray2012ava}   & \checkmark  & \checkmark \\
% \multicolumn{1}{c|}{}                         & AADB \cite{kong2016photo}  & \checkmark  & \checkmark \\
% \multicolumn{1}{c|}{}                         & PARA \cite{yang2022personalized}  & \checkmark  & \checkmark \\

% \bottomrule
% \end{tabular}
% \end{table}


% Notably, experiments conducted in the latter setup are the main emphasis of this paper, as it is more challenging and closer to real-world applications.


% require re-writing
\subsection{Implementation Details}
% All experiments are conducted on two NVIDIA Tesla V100 GPUs with 32Gb memory and require a pytorch 1.6.0 in Python 3.7.0 environment. When training and evaluating teacher and student model, we first resize the image into 256x256 and randomly crop the input image into a fixed size 224x224, followed by random horizontal flip with a probability of 0.5. When testing model performance, we resize input image into 224x224 directly to avoid randomness. As for hyper-parameter setting, the initialized learning rate is set to $1\times 10^{-4}$. For model with mobilenet-v2 and resnet50 as backbone, learning rate will decay at the epoch 6 by a factor of 0.1. While for swin tiny and swin base, learning rate will decay at epoch 2 with a factor of 0.1. Adam is selected as the optimizer and the entire training is iterated for 15 epochs in total. $\mu$ and $\beta$ in SKD stage are both set to 7. All utilized backbones are initialized with ImageNet pre-trained parameters.

All experiments are conducted with 4 NVIDIA Tesla V100 GPUs and PyTorch framework. Pre-processing starts with scaling the images to a size of $256\times256$. Subsequently, a random crop of size $224\times224$ is applied, and finally, a random horizontal flip is performed with a probability of 0.5. The initial learning rate for all backbones is fixed to $1\times 10^{-4}$, and will decay by a factor of 0.1 at epoch 5 for MV2 and ViT-Tiny, at epoch 2 for Swin-Base, 16 epochs in total. Adam is selected as the optimizer, and $\mu$ and $\beta$ in the SKD stage are both set to 15. Note that before CFA phase, backbones are initialized with ImageNet pre-trained weights (MV2 and ViT-Tiny) or ImageNet-21k (Swin-Base) from timm \cite{rw2019timm}. Finally, the default image encoder used of CLIP is ViT-L-14.


% \begin{table}[htb]
% \caption{Results on AVA dataset under \textbf{Partially Labeled} setting (student: MV2, teacher: Swin-Base).}
% \label{tab:part}
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|c|c|cccc} 
% \toprule
% Method & Labeled & Unlabeled & MSE & PLCC & SRCC & ACC \\
% \midrule
% % \multirow{5}{*}{\shortstack{teacher: Swin-Base\\student: ResNet50}}
% \multirow{3}{*}{\shortstack{Supervised (NIMA)}} &$1\%$ &$99\%$ &0.407  &0.507 &0.493  &0.747    \\
%   & $5\%$ &$95\%$  &0.369 &0.583   &0.567  &0.768  \\
%   & $10\%$ &$90\%$  &0.354 &0.608  &0.592  &0.775  \\
% \midrule
% \multirow{3}{*}{Ours (STSKD)}
%  & $1\%$ &$99\%$  &0.364 &\textbf{0.584}  & \textbf{0.570} &0.759  \\
% & $5\%$ &$95\%$ &0.321 &\textbf{0.653}  &\textbf{0.640}  &0.784  \\
% & $10\%$ &$90\%$ &0.302 &\textbf{0.668}  &\textbf{0.656}  &0.791  \\

% \bottomrule
% \end{tabular}
% }
% \end{table}


% \begin{table*}[ht]
% \centering
% \caption{Results on the AVA dataset. Note that NIMA* represents the reproduced version, and "full" means full resolution image, up to 800 × 800 pixels. Results in blue font color indicate the best performance under this metric.}
% \label{tab:ava}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{l|cc|ccc|ccc}
% \toprule
% Learning scheme &Methods & Year & \# Backbone & \# Params.& \# Input size  & MSE $\downarrow$ & SRCC $\uparrow$  & PLCC $\uparrow$ \\
% \midrule
% \multirow{13}{*}{\shortstack{Supervised}}
% &MNA-CNN \cite{mai2016composition} & 2016 & VGG + Places205-GoogLeNet &  / &   224      & / & /     & / \\
% &Kong et al. \cite{kong2016photo}  & 2016 & AlexNet&57 M & 227                             & /     & 0.558     & / \\
% &AMP \cite{murray2017deep}         & 2017 &  ResNet101&43 M & full                         & 0.279 &    0.709       & / \\
% %&A-Lamp \cite{ma2017lamp}          & 2017 & VGG16&134 M & 224 [50 crops]                   &   &          & / \\
% &NIMA \cite{talebi2018nima}        & 2018  &VGG16&  134 M& 224                             & /     & 0.592 &0.610\\
% &NIMA \cite{talebi2018nima}        & 2018  & Inception-v2&10 M & 224                       & /     & 0.612& 0.636\\
% &MPADA \cite{sheng2018attention}   & 2018  &ResNet18&11 M  &224 [$\geq$ 32 crops]          & /& / & / \\
% &Zeng et al \cite{zeng2019unified} & 2019  & ResNet101&43 M & 384                          &0.275 & 0.719& 0.720\\
% &Hosu et al. \cite{hosu2019effective} & 2019 & Inception-v3& 22 M& (full*0.875) [20 crops] & / & {0.756} & 0.757 \\
% &AFDC + SPP \cite{chen2020adaptive}   & 2020 & ResNet50& 26 M &332                          & 0.273& 0.648& /\\
% &AFDC + SPP \cite{chen2020adaptive}   & 2020 & ResNet50&26 M & (224, 256, 288, 320)          &0.271&0.649&0.671\\
% &MUSIQ-single \cite{ke2021musiq}    & 2021 & MUSIQ& 27 M&full                              &0.247&0.719&0.731\\
% &MUSIQ \cite{ke2021musiq}           & 2021 & MUSIQ& 27 M& (full, 384, 224)                 &0.242&0.726&0.738\\
% &TANet \cite{he2022rethinking}      & 2022 & ResNet18 + MV2 & 14 M& 224                    &/&0.758&0.765\\
% &Hou et al. \cite{hou2022distilling}      & 2022 &  [2* ResNeXt101 + ResNetv2] $\rightarrow$ ResNeXt101 &  89M&  300                   &/& 0.770&0.770\\
% \midrule
% % \multirow{4}{*}{\shortstack{Supervised}} & \multirow{3}{*}{NIMA} & MV2$\dagger$ &3 M &224  & 0.790 &0.305 & 0.660  & 0.675\\
% % & & ResNet50&26 M &224  & 0.796 & 0.295 & 0.674   & 0.688\\
% % & & ResNet50 & 26M & 332 & 0.800 & 0.281 & 0.696   & 0.710\\
% % % & &  EfficientNet-b2 &224 &0.803  &0.285 &0.690 &0.702 \\
% % & &  Swin-Tiny&29 M &224 & 0.809 & 0.256  & 0.714& 0.726\\

% \multirow{2}{*}{\shortstack{Supervised}} & \multirow{2}{*}{\shortstack{NIMA* \\ (ImageNet pre-trained)}} 
% & \multirow{2}{*}{2018} & MV2       &3 M &224   &0.314 & 0.661  & 0.669\\
% & &                     & Swin-Base &88 M &224  & 0.241 & 0.754   & 0.762\\

% \midrule
% \multirow{2}{*}{\shortstack{Supervised}} & \multirow{2}{*}{Ours (CFA)} 
% & \multirow{2}{*}{-} & MV2       &3 M &224             & 0.293 & 0.679  & 0.688\\
% & &                  & Swin-Base &88 M &224            & 0.226 & 0.767   & 0.776\\

% \midrule
% \multirow{2}{*}{\shortstack{Semi-Supervised}} & \multirow{2}{*}{Ours (CFA + SKD)} 
% & \multirow{2}{*}{-} & Swin-Base $\rightarrow$ MV2 &3 M &224 &0.272 & 0.706  & 0.717\\
% & &                  & Swin-Base $\rightarrow$ Swin-Base &88 M &224   & \textcolor{sota} {\textbf{0.224}}  & \textcolor{sota}{\textbf{0.770}}   & \textcolor{sota}{\textbf{0.779}}\\


% % \midrule
% % \multirow{5}{*}{\shortstack{Semi-supervised}} & \multirow{5}{*}{Ours (STSKD)} &
% % MV2$\dagger$ &3 M &224 &0.804  &0.274&0.690 &0.705 \\
% % & & ResNet50 & 26 M&224  &0.811 &0.258 &0.711 &0.723\\
% % & & ResNet50 & 26M & 332 &0.813 &0.247 & 0.724 &0.735\\
% % & & Swin-Tiny & 29 M& 224 & \textbf{0.818} & \textbf{0.237} & \textbf{0.739} & \textbf{0.749}\\
% % & & Swin-Tiny & 29 M& 224 [10 crops] & \textbf{0.823} & \textcolor{sota}{\textbf{0.228}} & \textbf{0.748} & \textcolor{sota}{\textbf{0.758}} \\

% \bottomrule
% \end{tabular}
% }
% \label{tab:overall}
% \end{table*}


\begin{table*}[ht]
\centering
\caption{Results on the AVA dataset. Note that NIMA* represents our replicated supervised version using ImageNet (MV2, ViT-Tiny) and ImageNet-21k (Swin-Base) pre-trained backbones, and "full" means full resolution image, up to 800 × 800 pixels. Results in blue font color indicate the SOTA performance under this metric.}
\label{tab:ava}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cc|cc|ccc}
\toprule
Learning scheme &Methods & Year & \# Backbone & \# Input size  & MSE $\downarrow$ & SRCC $\uparrow$  & PLCC $\uparrow$ \\
\midrule
\multirow{13}{*}{\shortstack{Supervised}}
&MNA-CNN \cite{mai2016composition} & 2016 & VGG + Places205-GoogLeNet &   224      & / & /     & / \\
&Kong et al. \cite{kong2016photo}  & 2016 & AlexNet & 227                             & /     & 0.558     & / \\
&AMP \cite{murray2017deep}         & 2017 &  ResNet101 & full                         & 0.279 &    0.709       & / \\
%&A-Lamp \cite{ma2017lamp}          & 2017 & VGG16&134 M & 224 [50 crops]                   &   &          & / \\
&\shortstack{NIMA \cite{talebi2018nima}}       & \shortstack{2018}  &VGG16& 224                             & /     & 0.592 &0.610\\
& \shortstack{NIMA \cite{talebi2018nima}}        &\shortstack{2018}   & Inception-v2 & 224                       & /     & 0.612& 0.636\\
&MPADA \cite{sheng2018attention}   & 2018  &ResNet18  &224 [$\geq$ 32 crops]          & /& / & / \\
&Zeng et al \cite{zeng2019unified} & 2019  & ResNet101 & 384                          &0.275 & 0.719& 0.720\\
&Hosu et al. \cite{hosu2019effective} & 2019 & Inception-v3& (full*0.875) [20 crops] & / & {0.756} & 0.757 \\
&AFDC + SPP \cite{chen2020adaptive}   & 2020 & ResNet50 &332                          & 0.273& 0.648& /\\
&AFDC + SPP \cite{chen2020adaptive}   & 2020 & ResNet50 & (224, 256, 288, 320)          &0.271&0.649&0.671\\
&MUSIQ-single \cite{ke2021musiq}    & 2021 & MUSIQ&full                              &0.247&0.719&0.731\\
&MUSIQ \cite{ke2021musiq}           & 2021 & MUSIQ& (full, 384, 224)                 &0.242&0.726&0.738\\
&TANet \cite{he2022rethinking}      & 2022 & ResNet18 + MV2 & 224                    &/&0.758&0.765\\
&Celona et al. \cite{celona2022composition}      & 2022 & EfficientNet-B4 & 224                    &/&0.732&0.733\\
&Hou et al. \cite{hou2022distilling}      & 2022 &  [2* ResNeXt101 + ResNetv2] $\rightarrow$ ResNeXt101 &  300                   &/& \textcolor{sota}{\textbf{0.770}} &0.770\\
&\shortstack{TAVAR \cite{li2023theme}}      & 2023 &2 * ResNet-50  + Swin-Base  & 224                    &/&0.725&0.736\\
\midrule
% \multirow{4}{*}{\shortstack{Supervised}} & \multirow{3}{*}{NIMA} & MV2$\dagger$ &3 M &224  & 0.790 &0.305 & 0.660  & 0.675\\
% & & ResNet50&26 M &224  & 0.796 & 0.295 & 0.674   & 0.688\\
% & & ResNet50 & 26M & 332 & 0.800 & 0.281 & 0.696   & 0.710\\
% % & &  EfficientNet-b2 &224 &0.803  &0.285 &0.690 &0.702 \\
% & &  Swin-Tiny&29 M &224 & 0.809 & 0.256  & 0.714& 0.726\\

\multirow{3}{*}{\shortstack{Supervised}} & \multirow{3}{*}{\shortstack{NIMA*}} 
& \multirow{3}{*}{2018} & MV2        &224   &0.314 & 0.661  & 0.669\\
& &  & ViT-Tiny        &224   &0.303 & 0.659  & 0.669\\
& &                     & Swin-Base  &224  & 0.241 & 0.754   & 0.762\\

\midrule
\multirow{3}{*}{\shortstack{Supervised}} & \multirow{3}{*}{CFA (ours)} 
& \multirow{3}{*}{-} & MV2        &224             & 0.293 & 0.679  & 0.688\\
& &                  & ViT-Tiny   &224             & 0.277  &0.690    & 0.701\\

& &                  & Swin-Base  &224             & 0.226 & 0.767   & 0.776\\

\midrule
\multirow{3}{*}{\shortstack{Semi-Supervised}} & \multirow{3}{*}{\shortstack{CSKD (ours) \\ (CFA + SKD)}} 
& \multirow{3}{*}{-} & Swin-Base $\rightarrow$ MV2  &224 &0.272 & 0.706  & 0.717\\
& &                  & Swin-Base $\rightarrow$ ViT-Tiny  &224   &  0.255 & 0.723  &0.734 \\

& &                  & Swin-Base $\rightarrow$ Swin-Base  &224   & \textcolor{sota} {\textbf{0.224}}  & \textcolor{sota}{\textbf{0.770}}   & \textcolor{sota}{\textbf{0.779}}\\


% \midrule
% \multirow{5}{*}{\shortstack{Semi-supervised}} & \multirow{5}{*}{Ours (STSKD)} &
% MV2$\dagger$ &3 M &224 &0.804  &0.274&0.690 &0.705 \\
% & & ResNet50 & 26 M&224  &0.811 &0.258 &0.711 &0.723\\
% & & ResNet50 & 26M & 332 &0.813 &0.247 & 0.724 &0.735\\
% & & Swin-Tiny & 29 M& 224 & \textbf{0.818} & \textbf{0.237} & \textbf{0.739} & \textbf{0.749}\\
% & & Swin-Tiny & 29 M& 224 [10 crops] & \textbf{0.823} & \textcolor{sota}{\textbf{0.228}} & \textbf{0.748} & \textcolor{sota}{\textbf{0.758}} \\

\bottomrule
\end{tabular}
}
\label{tab:overall}
\end{table*}




\subsection{Evaluation Metrics}
Following previous work, mean squared error (MSE), Spearman rank order cross-correlation coefficient (SRCC) and Pearson linear cross-correlation coefficient (PLCC) are used as the performance indicators. In general, MSE reflects the numeric distance between prediction and ground truth. PLCC and SRCC help to mirror the linear and ranking consistency between model estimation and human annotation. Both PLCC and SRCC range from -1 to 1 and higher value indicates better results. 

\section{Experimental Results}

\subsection{Main Results on AVA Dataset}

The proposed method is compared with several SOTA methods and the main results are summarized in Table \ref{tab:overall}. Notably, we include the size of the input image in the table for a fair comparison. It can be observed that the proposed methods CFA and SKD improve the performance with both large (e.g., Swin-Base) and lightweight student (e.g., MV2). We use three different groups of teacher student pairs, Swin-Base $\rightarrow$ MV2, Swin-Base $\rightarrow$ ViT-Tiny and Swin-Base $\rightarrow$ Swin-Base for semi-supervised KD. For supervised settings, NIMA* can be competitive with previous SOTA method \cite{hou2022distilling}. Moreover, there is a big improvement on SRCC and PLCC under the same model structure with CFA backbones, where the only difference is the pre-trained weights: in CFA, backbones are initialized with ImageNet pre-trained weights and then distilled based on CLIP, while in NIMA* backbones are initialized with ImageNet pre-trained weights only.

Then, the utilization of semi-supervised KD further enhance the performance of CFA backbones. The proposed CSKD method achieves SOTA results on MSE, SRCC and PLCC metrics. Especially, the performance gain of ViT-Tiny is quite significant, which may be due to the sufficient model capacity but insufficient representation training of the non-CFA backbones. The experimental results serve as implicit evidence supporting hypothesis in Section \ref{sec:skd}.


\subsection{AADB Evaluation}
In this section, the proposed two-phase approach is compared with several methods on the AADB dataset, the results are listed in Table \ref{tab:aadb_res}. With CFA and SKD, even MV2 can significantly outperform previous SOTA TAVAR \cite{li2023theme} that utilizing a theme branch to enhance the IAA ability. The reason for this might be attributed to the semantic information introduced through CLIP distillation.


\begin{table}[t]
\caption{Results on the AADB Dataset.}
\label{tab:aadb_res}
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|c|cc}
 \toprule
Methods&\#Backbone & SRCC & PLCC \\   
\midrule
Celona et al. \cite{celona2022composition}& EfficentNet-B4&0.757&0.762  \\
\midrule
Zeng et al \cite{zeng2019unified}&ResNet-50&0.726& / \\
\midrule
 \multirow{2}{*}{\shortstack{TAVAR \cite{li2023theme}}}  &   2 * ResNet-50  + MV3           & 0.733     &  /             \\
&   2 * ResNet-50  + Swin-Base     & 0.763     &  /             \\
\midrule
\multirow{3}{*}{\shortstack{NIMA*}} &MV2&0.728&0.736  \\
 &ViT-Tiny&0.703&0.712  \\
 &Swin-Base&0.768&0.774  \\
 \midrule
\multirow{3}{*}{\shortstack{CSKD (ours)}}  & Swin-Base $\rightarrow$ MV2                 & 0.770     &  0.780 \\

    & Swin-Base $\rightarrow$ ViT-Tiny                 &0.779      & 0.786  \\

    & Swin-Base $\rightarrow$ Swin-Base           & \textcolor{sota}{\textbf{0.794}}     & \textcolor{sota}{\textbf{0.798}}   \\
 
\bottomrule
\end{tabular}
}
\end{table}


\subsection{PARA Evaluation}
Table \ref{tab:para} shows the main results on a recently published IAA benchmark, PARA. Our approach exhibits approximately 0.1 to 0.4 improvements in terms of SRCC/PLCC compared with the previous SOTA method TAVAR \cite{li2023theme}. With CSKD, the obtained lightweight MV2 can achieve similar results with the strategy of combining three larger models in the TAVAR method.


\begin{table}[t]
\caption{Results on the PARA Dataset.}
\label{tab:para}
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|c|ccc}
 \toprule
Methods&\#Backbone & SRCC & PLCC \\   
\midrule

\multirow{2}{*}{\shortstack{TAVAR \cite{li2023theme}}}  &   2 * ResNet-50  + Mobilenet-v3  
   & 0.861 & 0.902                   \\
 &   2 * ResNet-50  + Swin-Base   &0.911  &0.940 \\
\midrule
\multirow{3}{*}{\shortstack{NIMA*}} &MV2&0.878&0.917  \\
   &ViT-Tiny&0.871&0.912  \\
   &Swin-Base&0.918&0.945  \\
\midrule
% Yang et al. \cite{yang2022personalized} &   MV2
%    & 0.871 & 0.912                   \\
\multirow{3}{*}{\shortstack{CSKD (ours)}}  & Swin-Base $\rightarrow$ MV2   &  0.908 &  0.937 \\
     & Swin-Base $\rightarrow$ ViT-Tiny   &  0.913 &  0.942 \\
     & Swin-Base $\rightarrow$ Swin-Base   & \textcolor{sota}{\textbf{0.926}} &  \textcolor{sota}{\textbf{0.951}}       \\
 
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Interval Error Rate}
The distributions of most IAA datasets are Gaussian-like imbalanced or long-tailed, leading to local unreliability of global indicators such as SRCC. In responding to this, we propose an interval-based evaluation metric to precisely identify the local performance improvement. Specifically, we split the test set into different score intervals and calculate the interval error rate ($IER$) independently, 
\begin{equation}
    IER_k= \frac{1}{N_k}\sum_{i=1}^{N_k}\mathbb I(|(s^{pred}_i-s^{true}_i| > t), k = 0, 1,..., K-1.
\end{equation}
where $k$ and $K$ are the index of a certain interval and the total number of intervals, respectively. $N_k$ is the total number of samples in interval $k$. $s_i^{pred}$ and $s_i^{true}$ are the predicted and real scores of sample $i$. And $t$ is a fixed threshold and is set to 0.5 in this work, referring to the error tolerance. The result is illustrated in Fig. \ref{fig:error}.

Compared with supervised baseline, using CFA backbone can effectively reduce the error rate in the intermediate score region. The reason lies in the fact that most of the samples in IAA datasets are located in the intermediate score range, and the feature collapse problem of the CFA backbones has been alleviated, thus making the samples in this score range more discriminative. Then, SKD can further enhance the performance of well-trained CFA backbones via a large volume of unlabeled data.

% Figure environment removed




\section{Ablation Study}
In this part, ablations are conducted to show the contribution of three key factors: 1) multi-source unlabeled dataset, 2) CLIP-based feature alignment and 3) semi-supervised knowledge distillation. 


\subsection{Attention Analysis}
\label{sec:attention_ana}
Here, we follow the method in \cite{wang2022closer} to examining the layer-wise attention maps before and after CFA using ViT-Tiny backbone, results can be found in Fig. \ref{fig:attention_distance}. The decrease in the mean attention distance of the CFA backbone, coupled with an increase in its variance, indicates a diverse and evenly representation map. Similarly, the decreased entropy suggests a greater concentration of the model. And hence, these could be the reasons why the model performs better on downstream tasks.

% Figure environment removed


\subsection{Influences of CFA and Data Sources}
In order to elucidate the benefits of introducing CFA and the impact of CLIP image encoders on CFA, we first employ CLIP ViT-B-16 and CLIP ViT-L-14 as teacher models for feature distillation. Subsequently, we exhaustively explored various combinations of ImageNet and OpenImages to validate the influence of the data source. Notably, all experiments are conducted in a supervised manner with AVA dataset, and non-CFA setting (ImageNet pre-trained weights for initialization only) is selected as the baseline.

In terms of CFA, experiments w/o CLIP and w/ CLIP ViT-B-16 using ImageNet for unsupervised pre-training shows the difference, the improvement on SRCC/PLCC proves the efficiency of CFA. Besides, we can see that  increased CLIP size results in better performance enhancement according to experiments w/ CLIP ViT-B-16 and w/ CLIP ViT-L-14 on mixed dataset. As for the sources, experiments w/ CLIP ViT-B-16 shows that using a mixed-source dataset brings better performance.
% than using only ImageNet or OpenImages. 

\begin{table}[t]
\caption{Ablations on CFA and unlabeled data sources under supervised learning setting, AVA benchmark.}

\label{tab:cfa}
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{cc|cc|cc}
\toprule
\multirow{2}{*}{CLIP} & \multirow{2}{*}{Dataset for CFA} & \multicolumn{2}{c}{Swin-Base} & \multicolumn{2}{c}{MV2} \\ \cline{3-6}
                         &              & SRCC     & PLCC      & SRCC     & PLCC   \\
\midrule
\xmark &   \xmark                     & 0.754    & 0.762     & 0.661    & 0.669    \\
\midrule
 \multirow{3}{*}{ViT-B-16}& ImageNet    & 0.762    & 0.769     & 0.674    & 0.683    \\
 & OpenImages                           & 0.760    & 0.768     & 0.671    & 0.681    \\
 & ImageNet + OpenImages                & 0.764    & 0.772     & 0.677    & 0.686     \\
\midrule                             
 \multirow{3}{*}{ViT-L-14} & ImageNet     & /     &  /   & /     &  /     \\
                           & OpenImages   & /     &  /   & /     &  /    \\
 & ImageNet + OpenImages &   \textcolor{sota}{\textbf{0.767}}  &\textcolor{sota}{\textbf{0.776}}   & \textcolor{sota}{\textbf{0.679}}  &\textcolor{sota}{\textbf{0.688}}                          \\
\bottomrule
\end{tabular}
}
\end{table}


\subsection{Influence of CFA Backbones}

As mentioned in Section \ref{sec:cfa}, CFA teacher and student are obtained by unsupervised fine-tuning on ImageNet pretrained weights, aided by a multi-source dataset. To ablate the influence of CFA backbones under SKD setting, we conduct experiments in Table \ref{tab:skd}. Notably, symbol \xmark \;indicates that the backbone is initialized with ImageNet pre-trained weights without CFA, and $\checkmark$ denotes the backbone is trained in a CFA manner: first initialized with ImageNet pre-trained weights and then distilled from CLIP. In the case of using only CFA teacher or CFA student, we can observe minor improvements. The collaboration of CFA teacher and student further enhances the performance, which demonstrates the efficacy of our approach.

\begin{table}[t]
\caption{Results with different CFA backbones under semi-supervised learning setting (teacher: Swin-Base, student: MV2), AVA benchmark.}

\label{tab:skd}
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|cc|cc}
 \toprule
 & Teacher(CFA) & Student(CFA) & SRCC & PLCC \\
% \midrule
% Unlabeled train set & \multicolumn{4}{c}{ImageNet}         \\
% \midrule
% Test set & AVA    & AADB     & AVA      & AADB                 \\
\midrule
 % \multirow{2}{*}{Supervised}
%  &   \xmark     &  \xmark          & 0.661  & 0.669                         \\
%  &   \xmark     &  \checkmark      & 0.679  & 0.688                           \\
% \midrule

 \multirow{4}{*}{Semi-Supervised} 
& \xmark      &   \xmark    & 0.698   & 0.709                   \\
& \checkmark  &   \xmark    & 0.699   & 0.711                  \\
& \xmark      &   \checkmark       & 0.704  & 0.714                          \\
& \checkmark  &   \checkmark       & \textcolor{sota}{\textbf{0.706}}   & \textcolor{sota}{\textbf{0.717}}           \\
\bottomrule
\end{tabular}
}
\end{table}


\subsection{Ablation on KD and SKD}

Table \ref{tab:kd_skd} gives the ablation on the influence of SKD, where supervised KD with CFA teacher and student is set as the baseline. It is clearly that SKD does outperform supervised KD, and the concated dataset can bring sustained benefits. This can be reasoned from two flips: 1) CFA phase optimizes the representation ability of backbones and alleviates feature collapse problem. 2) SKD can transfer more and better knowledges to the optimized backbones.

\begin{table}[t]
\caption{Results of KD and SKD (Swin-Base $\rightarrow$ MV2), AVA benchmark.}

\label{tab:kd_skd}
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|c|c|cc}
 \toprule
                     & Labeled  & Unlabeled                 &   SRCC    & PLCC     \\   
\midrule
                KD   & AVA      & \xmark                    &  0.690        & 0.699      \\
\midrule
\multirow{3}{*}{SKD} & AVA      & ImageNet                  &  0.701        & 0.713      \\
                     & AVA      & OpenImages                &  0.703        & 0.714      \\
                     & AVA      & ImageNet + OpenImages     & \textcolor{sota}{\textbf{0.706}}   & \textcolor{sota}{\textbf{0.717}}      \\
\bottomrule
\end{tabular}
}
\end{table}


% \subsection{Analysis from Knowledge Distillation Perspective}

% To identify the influence of SKD in the IAA task, we also open STSKD from a knowledge distillation perspective. As we know, SKD learns knowledge from labeled and unlabeled instances both with pseudo labels, while KD shows no usage of pseudo-labeled data. We compare both KD and SKD methods with augmentation and the backbone of the teacher model. Related results are reported in Table \ref{tab:abkd}. We can conclude that learning with pseudo-labeled data can continuously improve knowledge distillation efficiency in IAA tasks and the knowledge of stronger teacher can be better transferred when there are sufficient unlabeled data.



\section{Conclusion}
In this work, an unified CLIP-based two-phase semi-supervised scheme, CSKD, is proposed for data-hungry image aesthetics assessment tasks. Firstly, we start with a multi-source unlabeled dataset, and successfully distill the the rich features from CLIP to enhance the representation ability of the widely-used visual backbones. Subsequently, we utilize semi-supervised knowledge distillation to improve the efficiency of knowledge transfer. Finally, we conduct multidimensional ablations on CFA and SKD. Experimental results indicate the superiority of the proposed method, which we believe is a method with great potential. 

% \section{Acknowledgments}
% This work is 

{\small
\bibliographystyle{ieee_fullname}
\bibliography{cvpr24}
}

\end{document}
