\begin{thebibliography}{10}

\bibitem{talebi2018learned}
Talebi et~al.,
\newblock ``Learned perceptual image enhancement,''
\newblock in {\em 2018 IEEE international conference on computational photography}. IEEE, 2018, pp. 1--13.

\bibitem{mai2016composition}
Mai et~al.,
\newblock ``Composition-preserving deep photo aesthetics assessment,''
\newblock in {\em CVPR}, 2016, pp. 497--506.

\bibitem{hosu2019effective}
Hosu et~al.,
\newblock ``Effective aesthetics prediction with multi-level spatially pooled features,''
\newblock in {\em CVPR}, 2019, pp. 9375--9383.

\bibitem{ke2021musiq}
Ke~et~al.,
\newblock ``Musiq: Multi-scale image quality transformer,''
\newblock in {\em CVPR}, 2021, pp. 5148--5157.

\bibitem{ke2006design}
Ke~et~al.,
\newblock ``The design of high-level features for photo quality assessment,''
\newblock in {\em CVPR}, 2006, pp. 419--426.

\bibitem{9777255}
Yang et~al.,
\newblock ``Metamp: Metalearning-based multipatch image aesthetics assessment,''
\newblock {\em IEEE Trans. Cybern.}, pp. 1--13, 2022.

\bibitem{she2021hierarchical}
She et~al.,
\newblock ``Hierarchical layout-aware graph convolutional network for unified aesthetics assessment,''
\newblock in {\em CVPR}, 2021, pp. 8475--8484.

\bibitem{hou2022distilling}
Hou et~al.,
\newblock ``Distilling knowledge from object classification to aesthetics assessment,''
\newblock {\em IEEE Trans. Circ. Syst. Video Tech.}, vol. 32, no. 11, pp. 7386--7402, 2022.

\bibitem{brown2020language}
Brown et~al.,
\newblock ``Language models are few-shot learners,''
\newblock {\em NeurIPS}, vol. 33, pp. 1877--1901, 2020.

\bibitem{yu2022coca}
Yu~et~al.,
\newblock ``Coca: Contrastive captioners are image-text foundation models,''
\newblock {\em arXiv preprint arXiv:2205.01917}, 2022.

\bibitem{radford2021learning}
Radford et~al.,
\newblock ``Learning transferable visual models from natural language supervision,''
\newblock in {\em ICML}. PMLR, 2021, pp. 8748--8763.

\bibitem{wang2023exploring}
Wang et~al.,
\newblock ``Exploring clip for assessing the look and feel of images,''
\newblock in {\em Proceedings of the AAAI Conference}, 2023, vol.~37, pp. 2555--2563.

\bibitem{ke2023vila}
Ke~et~al.,
\newblock ``{Vila}: Learning image aesthetics from user comments with vision-language pretraining,''
\newblock in {\em CVPR}, 2023, pp. 10041--10051.

\bibitem{zhang2023blind}
Zhang et~al.,
\newblock ``Blind image quality assessment via vision-language correspondence: A multitask learning perspective,''
\newblock in {\em CVPR}, 2023, pp. 14071--14081.

\bibitem{chen2020big}
Chen et~al.,
\newblock ``Big self-supervised models are strong semi-supervised learners,''
\newblock {\em NeurIPS}, vol. 33, pp. 22243--22255, 2020.

\bibitem{li2024graphadapter}
Li~et~al.,
\newblock ``Graphadapter: Tuning vision-language models with dual knowledge graph,''
\newblock {\em NeurIPS}, vol. 36, 2024.

\bibitem{wortsman2022robust}
Wortsman et~al,
\newblock ``Robust fine-tuning of zero-shot models,''
\newblock in {\em CVPR}, 2022, pp. 7959--7971.

\bibitem{yang2022personalized}
Yang et~al.,
\newblock ``Personalized image aesthetics assessment with rich attributes,''
\newblock in {\em CVPR}, 2022, pp. 19861--19869.

\bibitem{sandler2018mobilenetv2}
Sandler et~al.,
\newblock ``Mobilenetv2: Inverted residuals and linear bottlenecks,''
\newblock in {\em CVPR}, 2018, pp. 4510--4520.

\bibitem{deng2009imagenet}
Deng et~al.,
\newblock ``Imagenet: A large-scale hierarchical image database,''
\newblock in {\em CVPR}. Ieee, 2009, pp. 248--255.

\bibitem{zhou2022extract}
Zhou et~al.,
\newblock ``Extract free dense labels from clip,''
\newblock in {\em ECCV}. Springer, 2022, pp. 696--712.

\bibitem{rombach2022high}
Rombach et~al.,
\newblock ``High-resolution image synthesis with latent diffusion models,''
\newblock in {\em CVPR}, 2022, pp. 10684--10695.

\bibitem{fang2023eva}
Fang et~al.,
\newblock ``Eva: Exploring the limits of masked visual representation learning at scale,''
\newblock in {\em CVPR}, 2023, pp. 19358--19369.

\bibitem{hinton2015distilling}
Hinton et~al.,
\newblock ``Distilling the knowledge in a neural network,''
\newblock {\em arXiv preprint arXiv:1503.02531}, vol. 2, no. 7, 2015.

\bibitem{9340578}
L.~Wang and K.-J. Yoon,
\newblock ``Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks,''
\newblock {\em IEEE Trans. Pattern Anal. Mach. Intell.}, vol. 44, no. 6, pp. 3048--3068, 2022.

\bibitem{wang2021knowledge}
Wang et~al.,
\newblock ``Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks,''
\newblock {\em CVPR}, 2021.

\bibitem{yang2021survey}
X.~Yang, Z.~Song, I.~King, and Z.~Xu,
\newblock ``A survey on deep semi-supervised learning,''
\newblock {\em arXiv preprint arXiv:2103.00550}, 2021.

\bibitem{kuznetsova2020open}
Kuznetsova et~al.,
\newblock ``The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale,''
\newblock {\em International Journal of Computer Vision}, vol. 128, no. 7, pp. 1956--1981, 2020.

\bibitem{rw2019timm}
R.~Wightman,
\newblock ``Pytorch image models,'' \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem{murray2012ava}
Murray et~al.,
\newblock ``Ava: A large-scale database for aesthetic visual analysis,''
\newblock in {\em CVPR}. IEEE, 2012, pp. 2408--2415.

\bibitem{kong2016photo}
Kong et~al.,
\newblock ``Photo aesthetics ranking network with attributes and content adaptation,''
\newblock in {\em ECCV}. Springer, 2016, pp. 662--679.

\bibitem{talebi2018nima}
H.~Talebi and P.~Milanfar,
\newblock ``Nima: Neural image assessment,''
\newblock {\em IEEE Trans. Image Process.}, vol. 27, no. 8, pp. 3998--4011, 2018.

\bibitem{touvron2021training}
Touvron et~al.,
\newblock ``Training data-efficient image transformers \& distillation through attention,''
\newblock in {\em ICML}. PMLR, 2021, pp. 10347--10357.

\bibitem{murray2017deep}
N.~Murray and A.~Gordo,
\newblock ``A deep architecture for unified aesthetic prediction,''
\newblock {\em arXiv preprint arXiv:1708.04890}, 2017.

\bibitem{sheng2018attention}
Sheng et~al.,
\newblock ``Attention-based multi-patch aggregation for image aesthetic assessment,''
\newblock in {\em ACM Multimedia}, 2018, pp. 879--886.

\bibitem{zeng2019unified}
Zeng et~al.,
\newblock ``A unified probabilistic formulation of image aesthetic assessment,''
\newblock {\em IEEE Trans. Image Process}, vol. 29, pp. 1548--1561, 2019.

\bibitem{chen2020adaptive}
Chen et~al.,
\newblock ``Adaptive fractional dilated convolution network for image aesthetics assessment,''
\newblock in {\em CVPR}, 2020, pp. 14114--14123.

\bibitem{he2022rethinking}
He~et~al.,
\newblock ``Rethinking image aesthetics assessment: Models, datasets and benchmarks,''
\newblock in {\em IJCAI}, 2022.

\bibitem{celona2022composition}
Celona et~al.,
\newblock ``Composition and style attributes guided image aesthetic assessment,''
\newblock {\em IEEE Trans. Image Process}, vol. 31, pp. 5009--5024, 2022.

\bibitem{li2023theme}
Li~et~al.,
\newblock ``Theme-aware visual attribute reasoning for image aesthetics assessment,''
\newblock {\em IEEE Trans. Circ. Syst. Video Tech.}, 2023.

\end{thebibliography}
