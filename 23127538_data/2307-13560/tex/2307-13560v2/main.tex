% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{cuted}
\usepackage{hyperref}




% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.

\usepackage{microtype}
\newcommand{\irene}[1]{\textcolor{violet}{[irene: #1]}}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
% \title{\emoji{grinning-squinting-face}XDLMusion: Cross-lingual Diffusion Model for Machine Translation}

\title{XDLM: Cross-lingual Diffusion Language Model for Machine Translation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Linyao Chen$^1$, Aosong Feng$^2$, Boming Yang$^1$, Zihui Li$^1$ \\
$^1$University of Tokyo, $^2$Yale University\\
{chen-linyao217@g.ecc.u-tokyo.ac.jp}
}


% \author{Linyao Chen \\
%   The University of Tokyo / Tokyo, Japan \\
%   \texttt{chen-linyao217@g.ecc.u-tokyo.ac.jp} \\\And
%   Aosong Feng \\
%   Yale University / New Haven, USA \\ 
%   \texttt{aosong.feng@yale.edu} \\ \And
%   Boming Yang \\
%   The University of Tokyo / Tokyo, Japan \\
%   \texttt{boming@g.ecc.u-tokyo.ac.jp} \\ \And
%   Irene Li \\
%   The University of Tokyo / Tokyo, Japan \\
%   \texttt{bkjl6178@g.ecc.u-tokyo.ac.jp} \\
%   }

\begin{document}
\maketitle
\begin{abstract}
Recently, diffusion models have excelled in image generation tasks and have also been applied to neural language processing (NLP) for controllable text generation. However, the application of diffusion models in a cross-lingual setting is less unexplored. Additionally, while pretraining with diffusion models has been studied within a single language, the potential of cross-lingual pretraining remains understudied. To address these gaps, we propose XDLM, a novel Cross-lingual diffusion model for machine translation, consisting of pretraining and fine-tuning stages.
In the pretraining stage, we propose TLDM, a new training objective for mastering the mapping between different languages; in the fine-tuning stage, we build up the translation system based on the pretrained model. We evaluate the result on several machine translation benchmarks and outperformed both diffusion and Transformer baselines. Our code is available in \href{https://github.com/Amayama/XDLM}{https://github.com/Amayama/XDLM}.

% \irene{can remove the following: } in this paper, we study the usage of diffusion models in neural language processing and introduce a novel cross-lingual Diffusion model framework for multilingual text generation, dubbed XDLM. XDLM can generate the result generate the result by graduating denoising the random noise in a non-autoregressive manner. To pre-train XDLM on a large-scale language corpus, several elaborate cross-language pretrained tasks are designed to encourage model to reconstruct and translate from the from the cross-language input. We evaluate the result on several cross-lingual text generation tasks and obtain \st{state-of-the-art results} compared to existing diffusion models.
\end{abstract}

\input{1_introduction}
% \irene{Our motivation: }

% \irene{
% 1. we investigate diffusion models for machine translation with pertaining: though existing models showed that diffusion could be applied for MT (i.e., RDM), the power of \textbf{cross-lingual pretraining} is not well-studied. }

%\irene{
%2. we further show that pretraining can improve such NAR machine translation with the help of diffusion models, we find that (need to discuss). Notably, we are the first work to show that pretraining could improve NAR for machine translation with diffusion models.}



% \input{background}

%In contrast, our research focuses on aligning cross-lingual distributions when generating cross-lingual text and proposes a specific multi-lingual pretrain task to improve the alignment learning on a multi-lingual corpus. To the best of our knowledge, our model is the first to implement large-scale cross-language pre-training on the language model based on the diffusion model.
% \section*{Acknowledgements}

\input{methodology}

\input{experiment}


\input{ablation}

%\input{case_study}

\section{Conclusion and Future Work}
In this study, we propose an innovative architecture that integrates cross-lingual pretraining into diffusion-based text generation. This is achieved through a carefully designed pretraining task. We compare our model with some previous works under automated evaluation method. 
%The quality of the generated text is evaluated using both automated and human assessment methods. 
Looking forward, we plan to extend our model to include additional languages, with the aim of constructing a robust multilingual model capable of handling more extensive cross-lingual translation tasks.
% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

% \section{Example Appendix}
%\label{sec:appendix}

%This is an appendix.

\end{document}