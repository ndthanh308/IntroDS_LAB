\section{Ablation Study}
\label{sec:ablation}
%\subsection{Study on Different Components}
%We study the impacts of proposed components in completing model this section. The results are listed in table 2. Compare to different settings in the table, it shows that the pretrain model can make a great improvement to the model.
% \begin{table}[]
% \centering
% \begin{tabular}{l|l}
% % \hline
% \toprule
% Setting     & BLEU   \\
% \midrule
%  XDLM             & 40     \\
% \hspace{0.2cm} w/o TDLM     & 30     \\
% \hspace{0.2cm} w/o MDLM     & 20     \\
% \hspace{0.2cm} w/o pretrain & 10    \\
% \bottomrule
% \end{tabular}
% \label{table:performance}
% \caption{Ablation Study on Different Components. Results are conducted on the WMT14 En-de task.}
% \end{table}

%\textbf{Study on Setting of Pretraining Steps}
%In previous sections, we discovered that XDLM significantly benefits from the implementation of pre-training objectives. However, this process requires a substantial amount of time. In this section, we delve into the influence of pre-training steps on XDLM's performance, as presented in Table 4. We fine-tuned the pre-trained model using the WMT14 En-De task across 10 epochs and compared the results. The findings suggest that an adequate number of pre-training steps substantially enhance the model's capacity to process cross-lingual mapping. This, in turn, substantially bolsters the performance of the translation model.


% \begin{table}[h]
% \centering
% \label{tab:rouge_scores}
% \begin{tabular}{l|c}
% \toprule
% Model       & BLEU \\
% \midrule
% w/o pre-train & 37.3  \\
% GENIE(100w) & 39.4  \\
% GENIE(200w) & 40.4  \\
% GENIE(300w) & 40.6  \\
% GENIE(400w) & 40.9  \\
% GENIE(500w) & 41.2  \\
% \bottomrule
% \end{tabular}
% \label{table:performance}
% \caption{Ablation Study on Different Pretrain Steps}
% \end{table}
\textbf{Study on the Denoising Capacity at Intermediate Steps}
We also focus on the denoising capacity in each diffusion step. In the reverse process, XDLM apply $T$ diffusion steps to the Gaussian noise $Z_T$, generating correspond output $y$ after all intermediate steps. Figure \ref{fig:figure 2} shows the change of BLEU scores with the increasing of reverse steps. We can find for different decoding settings, our model can reach a stable result after 10 iterations, which shows out the effectiveness of our model.  

% Figure environment removed

\textbf{Discussion}
In this section, we concentrate on the factors that contribute to the comparatively lower performance of our model relative to other models. 
One may have noticed that our method is not able to perform against the original RDM method, we discuss a few reasons in this section. Firstly, prior research such as RDM leverages a substantial batch size coupled with an extensive number of training iterations, a strategy that has been shown to enhance performance. Due to our machine limitations, we failed to conduct the experiments with the same level. Secondly, in terms of our pretraining configuration, we employ a pretraining dataset with an expanded vocabulary size to construct the Byte Pair Encoding (BPE) codes. This approach, while comprehensive, inadvertently increases the complexity of the problem and introduces out-of-vocabulary words that the model must interpret. Such challenges are not typically encountered in previous works. This discrepancy in methodology could potentially account for the performance differential observed.