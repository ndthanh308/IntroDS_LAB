\section{Introduction}
\label{sec:introduction}

Diffusion-based generative models, or diffusion models \cite{ho2020denoising}, have recently demonstrated substantial potential for generating high-quality output in computer vision (CV). Furthermore, several recent studies have explored their application in natural language processing (NLP), including generation tasks such as machine translation, text summarization, and controllable text generation \cite{li2022diffusion, zheng2023reparameterized, gao2022difformer}. Notably, GENIE \cite{lin2022genie} proposes the use of pretraining on diffusion models, leveraging large English corpora and subsequent fine-tuning on downstream tasks. However, there is a lack of research investigating the cross-lingual application of diffusion models, particularly in the context of pretraining. 
% A few recent works also demonstrated that it has the potential for text generation in the natural language processing (NLP) domain \cite{li2022diffusion,zheng2023reparameterized,lin2022genie,gao2022difformer}, including machine translation, text summarization, and controllable text generation. Moreover, GENIE \cite{lin2022genie} propose to apply pretraining on top of diffusion models, in which large English corpora are used and then finetuned on downstream tasks. However, limited work studies the cross-lingual setting with diffusion, especially, with pretraining. 



There are two types of diffusion models: discrete and continuous. Some works focused on the discrete nature of text and have attempted to extend diffusion models to generate high-quality text. The discrete diffusion \cite{austin2021structured,hoogeboom2021argmax} model was initially proposed to generate text samples, solved by denoising and resetting the mask state for each token step by step. In the other hand, the continuous diffusion model \cite{li2022diffusion} was introduced later, which added additional embedding and rounding steps to transform discrete tokens into continuous latent representations, enabling gradient-based methods for controllable text generation. Then, GENIE model \cite{lin2022genie} involves integrating the diffusion model with a Transformer-based model, which resulted in a large-scale language pre-training model based on the diffusion framework.  Furthermore, the Difformer model \cite{gao2022difformer} has improved the existing diffusion methods by updating the loss function, adding a layer normalization and a noise factor, to establish a more stable diffusion process. \cite{zheng2023reparameterized} introduce a trick of reparameterization to the discrete diffusion, contributing to a simplified training process and a flexible sampling process. Inspired by GENIE, we propose to apply pretraining in a cross-lingual setting with continuous diffusion. 

% We propose the XDLM model, a cross-lingual diffusion language model for machine translation. It contains a pretraining stage and a fine-tuning stage. In the pretraining stage, we propose the MLDM (masked Diffusion Language Modeling) training objective.... , and then we finetune the model for downstream tasks. With pretraining, we enable the diffusion model to comprehend the mapping relationship between languages; and diffusion steps are sped up .... compared with training from scratch. \irene{say the advantage of XDLM.} 

In this paper, we examine the properties of a large-scale multilingual corpus and propose the implementation of cross-lingual pre-training denoising tasks to construct a framework for a cross-lingual diffusion model, termed as Cross-Lingual Diffusion Language Model (XDLM). XDLM specifically designs a cross-lingual pre-training task and corresponding objective for  multilingual data, enabling the diffusion model to comprehend the mapping relationships between various languages. To the best of our knowledge, this is the inaugural attempt to introduce the concept of cross-lingual pretraining to diffusion-based models. 

% We evaluate XDLM using several machine translation corpora. Experimental results illustrate that the model can establish mappings between tokens in different languages and serves as a robust baseline for multilingual tasks. Additionally, we apply human evaluation to the texts generated by XDLM, with the model demonstrating competitive performance.

The principal contributions of this work can be summarized as follows:
\begin{itemize}
\item We introduce XDLM, the first architecture to the best of our knowledge aiming to incorporate cross-lingual pretraining into diffusion-based text generation.%.%Our code is alsp released in https://anonymous.4open.science/ (we will upload the code to the anonymous git and keep the link)
%\irene{improve the machine translation performance.  <-this is the most important one; not sure what is high-quality text, and in what ways? Also say the concrete value, i.e., we improved xxx\% on xxx dataset on 3 languages compared with competitive baselines, and even achieves the same level with auto-regressive models. We also release the source code here: https://anonymous.4open.science/ (we will upload the code to the anonymous git and keep the link) }
\item We propose a pre-training task, Translation Diffusion Language Modeling (TDLM), along with a corresponding loss function. These enhancements augment the model's capacity to capture contextual correlations across various language domains. We also provide a discussion on potential issues. 
% \item We affirm the effectiveness of the generated context through automatic, thereby attesting to the model's substantial impact. Additionally, we offer extensive analyses of the model's behavior and properties. 
\end{itemize}


% First mention how pretraining is successful in NLP, i.e., BERT, XLM. -> and we noticed that GENIE tried this and achieved competitive results -> However, to the best of our knowledge, we did not see any work discussing applying pretraining for diffusion in a cross-lingual setting. Then use 2-3 sentences to summarize the XDLM in a very high-level way. For example. To study this, we examine the properties of a large-scale multilingual corpus and propose the implementation of cross-lingual pre-training denoising tasks to construct a framework for a cross-lingual diffusion model for text generation, termed as Cross-Lingual Diffusion (XDLM). XDLM specifically designs cross-lingual pre-training tasks and objectives for both monolingual and multilingual data, thereby enabling the diffusion model to comprehend the mapping relationships between various languages.  (make it shorter) 





% Diffusion-based generative models, commonly referred to as diffusion models, have demonstrated substantial potential for generating high-quality output across various domains, earning state-of-the-art status in multiple generative tasks in recent years. Distinct from traditional generative language models, diffusion models introduce a novel paradigm for generation. This paradigm trains and generates data through a multi-step denoising process, effectively mitigating the mode collapse problem and dependence on surrogate objectives inherent in existing generative models.

% Despite their impressive performance in generating continuous data such as visual and auditory information, diffusion models have seen limited application in text generation due to the discrete nature of textual data. Current explorations predominantly focus on two methodologies. Continuous diffusions convert word tokens into latent space embeddings and apply continuous diffusion to these tokens for text generation. Conversely, discrete diffusions involve sampling generated results from an entirely noisy sequence and denoising specific tokens based on an underlying stochastic process. The application of continuous diffusion necessitates expertly crafted rounding functions to generate sequences from diffused latent vectors and requires a sufficient number of samples, which makes the generation slow and lack competitive.






% \section{Related Work}
% \label{sec:background}

% \paragraph{Cross-lingual Generation}

% Cross-lingual Generation, typified by Neural Machine Translation (NMT), is a task where the source paragraph $X={X_1, X_2,…,X_T}$ is translated into its corresponding output $Y={y_1,y_2,…,y_t}$ in a different language. Autoregressive methods\cite{sutskever2014sequence,kalchbrenner2016neural,schuster2019cross} generate possible output sentences with a left-to-right causal structure. With the widespread use of Transformers\cite{vaswani2017attention}, several works\cite{wang2019learning,sukhbaatar2019adaptive} have adapted them into NMT for efficient and better pretraining. \cite{lample2019cross,conneau2019unsupervised} have followed this trend and designed new unsupervised learning methods for cross-lingual language modeling on large-scale cross-lingual corpus. Several researchers have also explored solutions beyond autoregressive models. \cite{gu2017non} presents a non-autoregressive encoder-decoder architecture with a Fertility prediction module.\cite{yang2020universal} proposes a novel but efficient unsupervised training method that can generate text comparable to an autoregressive model.

% \paragraph{Diffusion models}
% Diffusion models have shown remarkable success in continuous data domains, demonstrating great potential for generating high-quality images and audio\cite{sohl2015deep,song2020denoising}.  These models employ a forward process that gradually transforms Gaussian noise into data, as well as a backward process that generates a sentence from a complete noise sequence. \cite{song2020denoising} have achieved remarkable results in generating high-quality images that rival traditional generative models such as Generative Adversarial Networks (GANs) \cite{creswell2018generative}, while requiring less training time. Several studies \cite{kingma2021variational,song2020defnoising,nichol2021improved} have explored and enhanced diffusion models for both quality and efficiency.

% On the other hand, some researchers have focused on the discrete nature of text and have attempted to extend diffusion models to generate high-quality text. The discrete diffusion \cite{austin2021structured,hoogeboom2021argmax} model was initially proposed to generate text samples, solved by denoising and resetting the mask state for each token step by step. The continuous diffusion model\cite{li2022diffusion} was introduced later, which added additional embedding and rounding steps to transform discrete tokens into continuous latent representations, enabling gradient-based methods for controllable generation. Another approach\cite{lin2022genie} involves integrating the diffusion model with a Transformer-based model, which resulted in a large-scale language pre-training model based on the diffusion framework.  Furthermore, \cite{gao2022difformer} has improved the existing diffusion methods by updating the loss function, adding a layer normalization and a noise factor, to establish a more stable diffusion process.


