\section{Experiments}
\label{sec:experiments}


\subsection{Baselines and Datasets}
We conduct a large cross-lingual corpus and two standard benchmarks for cross-lingual translation, which are introduced as follows, and the information of each dataset is shown in Table \ref{table:stat}.
\begin{itemize}
\item Opus-ENDE\footnote{\url{https://opus.nlpl.eu/}}: This dataset comprises a large volume of English-German sentence pairs.
\item IWSLT14 DE-EN \cite{cettolo-etal-2014-report}: This benchmark is specifically employed for the German to English translation task.
\item WMT14 EN-DE \cite{bojar-etal-2014-findings}: This benchmark is designated for the English to German translation task.
\end{itemize}



\begin{table}[htbp]
\centering
\footnotesize
\begin{tabular*}{0.5\textwidth}{lcccc}
\toprule
 \textbf{Dataset} & \textbf{Usage} & \textbf{Train} & \textbf{Test} & \textbf{Valid}\\
\midrule
Opus-ENDE & pre-train & 9,323,066 & 5,000 & 5,000 \\
IWSLT14 & fine-tune & 160,240 & 6,750 & 7,283 \\
WMT14 & fine-tune & 4,496,988 & 3,003 & 3,000 \\
% XDLM -wrong result & Discrete Diffusion & 38.4 (230 epochs) & 29.07 (41 epochs) & 34.10 \\
\bottomrule
\end{tabular*}
\caption{Dataset statistics.}
\label{table:stat}
\end{table}



We use the origin datasets with the same split and do not apply distillation on the dataset. Besides, we follow the data processing introduced by fairseq \footnote{\url{https://github.com/facebookresearch/fairseq}} and use the joint vocabulary as the pre-train model.
We compare with three groups of baselines:
\begin{itemize}
    \item \textbf{Auto-regressive model}: Transformers \cite{vaswani2017attention}. Which generates sentences in an auto-regressive manner. We follow the setting and the results introduced by \cite{gao2022difformer}, beam search with a beam size of 5 is used during generation.
    \item \textbf{Continuous Diffusion}: SeqDiffuSeq \cite{yuan2022seqdiffuseq}, DiffuSeq \cite{gong2022diffuseq}, Difformer \cite{gao2022difformer}. Which generates the sentences from a continuous latent space. We evaluate Difformer in the origin dataset with knowledge distillation on WMT14 dataset.
    \item \textbf{Discrete Diffusion}: CMLM \cite{ghazvininejad2019mask}, RDM \cite{zheng2023reparameterized}. Which generates sentences by denoising for each token gradually. We implement the RDM based on the same data setting and batch setting as our models.
\end{itemize}

\begin{table*}[ht]
\centering
\begin{tabular}{cccc}
\toprule
\textbf{Model} & \textbf{Type} & \textbf{IWTLS14 (De-En)} & \textbf{WMT14 (En-De)}  \\
\midrule
Transformer & Seq2seq & 33.91 & \textbf{27.37} \\ \midrule
Diffuseq (b=10) & Continuous Diffusion & 28.78  & 15.37   \\
Seqdiffuseq (b=10) & Continuous Diffusion & 30.03 & 17.14 \\
Difformer (b=20) & Continuous Diffusion & 34.13  & 21.42 \\ \midrule
RDM & Discrete Diffusion & \textbf{34.49} & 22.30 \\
CMLM & Discrete Diffusion & 31.76 & 20.03 \\ \midrule
XLDM (ours) & Discrete Diffusion & 23.78 & 20.30 \\
% XDLM -wrong result & Discrete Diffusion & 38.4 (230 epochs) & 29.07 (41 epochs) & 34.10 \\
\bottomrule
\end{tabular}
\caption{Model Performance.}
\label{table:performance}
\end{table*}

\subsection{Experimental Settings}
\paragraph{Model Framework} We constructed our XDLM on an encoder-decoder architecture, with both the encoder and decoder comprising six Transformer layers. We set the hidden size of the model to 512 with eight attention heads.
\paragraph{Pretraining Stage Setting} During the pretraining stage, we formulate the pretraining task on the large-scale corpus mentioned above. We initialize pretraining with a weight decay rate of 0.0001 and a dropout rate of 0.2. We set the maximum number of tokens in each batch to 4k and provided 30k warm-up steps. Besides, the max length of both the source and target language is 256, aiming to make the input sentence a proper length.
\paragraph{Fine-tuning Stage Setting}
We apply fine-tuning on corresponding datasets, leveraging the robust foundation established from the pretraining datasets. The parameter setting for the fine-tuning process is primarily based on the pretraining stage, but with a smaller learning rate of 5e-5.

\subsection{Main result}
To ascertain the efficacy of pre-training on XDLM, we fine-tuned the model across several machine translation tasks, with comparative results in Table \ref{table:performance}. Our model demonstrates superior performance relative to certain continuous diffusion models, with the exception of Difformer. However, it exhibits comparable effectiveness to some discrete diffusion models, such as CMLM on the WMT14 dataset.

During the evaluation phase, we assess the BLEU score at both word and BPE (Byte Pair Encoding) levels, each requiring different tokenization scales. A comparison of the two tokenization methods is depicted in Table \ref{table:bleu}. Our findings indicate that our model performs more effectively when evaluated at the BPE level tokenization across two datasets, registering an improvement of approximately 5\%. This enhancement can be attributed to the fact that different tokenization levels help mitigate the complexity of the problems.

\begin{table}[ht]
\centering
\begin{tabular*}{0.35\textwidth}{lcc}
\toprule
 & \textbf{IWSLT14} & \textbf{WMT14} \\
\midrule
word level & 18.38 & 15.20 \\
BPE level & 23.78 & 20.30 \\
% XDLM -wrong result & Discrete Diffusion & 38.4 (230 epochs) & 29.07 (41 epochs) & 34.10 \\
\bottomrule
\end{tabular*}
\caption{The BLUE results under different tokenization levels}
\label{table:bleu}
\end{table}