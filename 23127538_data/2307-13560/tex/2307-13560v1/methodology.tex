\section{Cross-Lingual Diffusion Language Model}
\label{sec:XDLMusion}

% In this section, we present our proposed language modeling objectives designed specifically for diffusion and the diffusion model applied for cross-lingual translation. These objectives cater to both monolingual and multilingual data, and they are situated within the diffusion model framework for facilitating cross-lingual translation.

In this section, we present the Cross-lingual Diffusion Language Model (XDLM), which incorporates a pre-training phase on cross-lingual data, utilizing diffusion techniques for the purpose of non-autoregressive machine translation, and a fine-tuning phase generating corresponding text from one language to another language based on the pre-trained model.

% \subsection{Preliminary}
% \subsubsection{Cross-lingual translation}
% (\irene{combine 3.1.1 and 3.1.2 as NAR machine translation, and, there is no such term called \textit{Cross-lingual translation}, all translation is cross-lingual, it should be either \textit{machine translation} or \textit{cross-lingual language model}})

% Cross-lingual translation typically involves generating an output sequence $Y=\{y_1, y_2,…, y_{|Y|}\}$ from a given input sequence $X=\{x_1,x_2,…,x_{|X|}\}$, with each sequence being in a different language. Three common generative paradigms exist for cross-lingual translation: AutoRegressive (AR) generation, Non-AutoRegressive (NAR) generation, and semi-NAR generation. Ordinarily, diffusion models employ the NAR approach for generation tasks.

% \subsubsection{Non-AutoRegressive(NAR) generation}
% The NAR generation follows the conditional probality: 
% $$
% p_{\theta}(Y|X)=\prod_{i=1}^{|Y|} p_{\theta}(y_i|X)
% $$

% Unlike AutoRegressive (AR) generation, all tokens $y_i$$(0\leq i \leq |Y|)$ in the generated sequence Y are predicted concurrently. The generation solely depends on the input sequence X, without any dependency on preceding tokens. This attribute presents a challenge in determining the length of the generated sequence. To address this issue, the prediction of the output sequence is introduced as an auxiliary task \cite{gu2017non}.

\textbf{Non-AutoRegressive (NAR) Machine Translation}
In machine translation, given the input sequence from a source language $X=\{x_1,x_2,…,x_{|X|}\}$, the task is to generate the output sequence of the translation in the target language $Y=\{y_1, y_2,…, y_{|Y|}\}$. In this work, we focus on the Non-AutoRegressive (NAR) translation setting with the diffusion model. Typically, it has the following conditional probability:  
$$
p_{\theta}(Y|X)=\prod_{i=1}^{|Y|} p_{\theta}(y_i|X).
$$

Unlike AutoRegressive (AR) text generation, all tokens $y_i$$(0\leq i \leq |Y|)$ in the generated sequence $Y$ are predicted concurrently. The generation solely depends on the input sequence $X$, without any dependency on preceding tokens. This attribute presents a challenge in determining the length of the generated sequence. To address this issue, the length prediction of the output sequence is introduced as an auxiliary task \cite{gu2017non}. And the training loss is defined as a weighted sum between the translation loss and the length prediction loss.

\textbf{Diffusion Models}
The Denoising Diffusion Probabilistic Model (DDPM) \cite{ho2020denoising} is a parametrized Markov chain, and it is trained using variational inference to generate samples that match the original input data. 
% a diffusion process for generative tasks was introduced by \cite{ho2020denoising}, yielding impressive results.
The diffusion process comprises a noise-adding forward process and a noise-removing backward process, both of which can be viewed as discrete-time Markov processes. During the forward process, the model gradually introduces random noise with different scheduled variance $\beta_1,...,\beta_t$, with the aim of generating a standard Gaussian noise $x_t$ after $t$ turns. This can be formalized as follows:
$$
q(x_{t+1}|x_t)=\mathcal{N}(x_{t+1};\sqrt{1-\beta_{t+1}}x_t,\beta_{t}\mathbf{I}).
$$

The backward process, the reverse of the forward process, attempts to reconstruct the target sequence from the standard noise. Like the forward process, this procedure is also applied incrementally and can be formalized as follows:

$$
    p(x_{t-1}|x_t)=\mathcal{N}(x_{t-1};\mu_{\theta}^{t-1},\sigma_{\theta}^{t-1}),
$$
$$
    \mu_{\theta}^{t-1}=\frac{1}{\sqrt{\alpha_{t}}}(x_t-\frac{\beta_{t}}{\sqrt{1-\overline(\alpha_{t})}}z_{\theta}(x_{t},t)), 
$$
$$
    \sigma_{\theta}^{{t-1}^2}=\frac{1-\overline{\alpha_{t-1}}}{1-\overline{\alpha_{t}}}\dot \beta_{t},
$$

where $\alpha_t=1-\beta_t, \overline{\alpha_{t}}=\prod_{i=1}^t \alpha_{i}$ and $z_\theta$ comes from the prediction of model parameterized by $\theta$. 
In this work, we apply discrete diffusion for text generating and cross-lingual translation. Based on \citet{zheng2023reparameterized}, we follow the proposed discrete diffusion model with the following routing mechanism.

$
    x_{t-1}, v_{t-1} \sim q(x_{t-1},v_{t-1}|x_t,x_0) \\
    q(v_{t-1}|x_t,x_0)=q(v_{t-1})=Bernoulli(\lambda) \\
    q(x_{t-1}|v_{t-1},x_t,x_0)= \\
    v_{t-1}x_t+(1-v^{(1)}_{t-1})q_{noise}, \quad if \quad x_t = x_0 \\
    v_{t-1}x_0+(1-v_{t-1}^{(2)})q_{noise} (x_t), \quad if \quad x_t \neq x_0 \\
$


Which models the joint distribution over both $x$ and $v$. The sampling process here also takes the reparameterized method, which improves flexibility and expressiveness compared to the original process.

% Figure environment removed
\textbf{Translation Diffusion Language Modeling (TDLM)}
% Contrary to previous language modeling objectives for diffusion models, which primarily focus on monolingual data and neglect the potential to harness cross-lingual modeling capabilities from parallel datasets, we propose a pretraining process for parallel language pairs along with a corresponding modeling objective.
Unlike previous diffusion model objectives for language modeling that primarily concentrate on monolingual data, we target to exploit cross-lingual modeling capabilities from parallel datasets. Consequently, we propose a pretraining process named Translation Diffusion Language Modeling (TDLM), aiming at enhancing cross-lingual pretraining with diffusion models. As illustrated in Figure 1, we first concatenate both source and target sentences and generate the corresponding language and position embedding sequences, and then stack them as the input to a diffusion model. 
% we select both source and target sentences, generate their corresponding language and position embedding series, and concatenate them to form the input text stream. 
In a similar vein to \citet{lin2023text}, we random mask 15\% of the tokens to the input as \cite{lample2019cross} designed, tasking the model with predicting the noise and its surrounding text based on the cross-lingual context. This denoising setting assists the model in grasping the cross-lingual context.

