\begin{thebibliography}{16}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Austin et~al.(2021)Austin, Johnson, Ho, Tarlow, and van~den
  Berg}]{austin2021structured}
Jacob Austin, Daniel~D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van~den
  Berg. 2021.
\newblock Structured denoising diffusion models in discrete state-spaces.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:17981--17993.

\bibitem[{Bojar et~al.(2014)Bojar, Buck, Federmann, Haddow, Koehn, Leveling,
  Monz, Pecina, Post, Saint-Amand, Soricut, Specia, and
  Tamchyna}]{bojar-etal-2014-findings}
Ond{\v{r}}ej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp
  Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve
  Saint-Amand, Radu Soricut, Lucia Specia, and Ale{\v{s}} Tamchyna. 2014.
\newblock \href {https://doi.org/10.3115/v1/W14-3302} {Findings of the 2014
  workshop on statistical machine translation}.
\newblock In \emph{Proceedings of the Ninth Workshop on Statistical Machine
  Translation}, pages 12--58, Baltimore, Maryland, USA. Association for
  Computational Linguistics.

\bibitem[{Cettolo et~al.(2014)Cettolo, Niehues, St{\"u}ker, Bentivogli, and
  Federico}]{cettolo-etal-2014-report}
Mauro Cettolo, Jan Niehues, Sebastian St{\"u}ker, Luisa Bentivogli, and
  Marcello Federico. 2014.
\newblock \href {https://aclanthology.org/2014.iwslt-evaluation.1} {Report on
  the 11th {IWSLT} evaluation campaign}.
\newblock In \emph{Proceedings of the 11th International Workshop on Spoken
  Language Translation: Evaluation Campaign}, pages 2--17, Lake Tahoe,
  California.

\bibitem[{Gao et~al.(2022)Gao, Guo, Tan, Zhu, Zhang, Bian, and
  Xu}]{gao2022difformer}
Zhujin Gao, Junliang Guo, Xu~Tan, Yongxin Zhu, Fang Zhang, Jiang Bian, and
  Linli Xu. 2022.
\newblock Difformer: Empowering diffusion model on embedding space for text
  generation.
\newblock \emph{arXiv preprint arXiv:2212.09412}.

\bibitem[{Ghazvininejad et~al.(2019)Ghazvininejad, Levy, Liu, and
  Zettlemoyer}]{ghazvininejad2019mask}
Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. 2019.
\newblock Mask-predict: Parallel decoding of conditional masked language
  models.
\newblock \emph{arXiv preprint arXiv:1904.09324}.

\bibitem[{Gong et~al.(2022)Gong, Li, Feng, Wu, and Kong}]{gong2022diffuseq}
Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. 2022.
\newblock Diffuseq: Sequence to sequence text generation with diffusion models.
\newblock \emph{arXiv preprint arXiv:2210.08933}.

\bibitem[{Gu et~al.(2017)Gu, Bradbury, Xiong, Li, and Socher}]{gu2017non}
Jiatao Gu, James Bradbury, Caiming Xiong, Victor~OK Li, and Richard Socher.
  2017.
\newblock Non-autoregressive neural machine translation.
\newblock \emph{arXiv preprint arXiv:1711.02281}.

\bibitem[{Ho et~al.(2020)Ho, Jain, and Abbeel}]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:6840--6851.

\bibitem[{Hoogeboom et~al.(2021)Hoogeboom, Nielsen, Jaini, Forr{\'e}, and
  Welling}]{hoogeboom2021argmax}
Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr{\'e}, and Max
  Welling. 2021.
\newblock Argmax flows and multinomial diffusion: Learning categorical
  distributions.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:12454--12465.

\bibitem[{Lample and Conneau(2019)}]{lample2019cross}
Guillaume Lample and Alexis Conneau. 2019.
\newblock Cross-lingual language model pretraining.
\newblock \emph{arXiv preprint arXiv:1901.07291}.

\bibitem[{Li et~al.(2022)Li, Thickstun, Gulrajani, Liang, and
  Hashimoto}]{li2022diffusion}
Xiang Li, John Thickstun, Ishaan Gulrajani, Percy~S Liang, and Tatsunori~B
  Hashimoto. 2022.
\newblock Diffusion-lm improves controllable text generation.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:4328--4343.

\bibitem[{Lin et~al.(2022)Lin, Gong, Shen, Wu, Fan, Lin, Chen, and
  Duan}]{lin2022genie}
Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Weizhu
  Chen, and Nan Duan. 2022.
\newblock Genie: Large scale pre-training for text generation with diffusion
  model.
\newblock \emph{arXiv preprint arXiv:2212.11685}.

\bibitem[{Lin et~al.(2023)Lin, Gong, Shen, Wu, Fan, Lin, Duan, and
  Chen}]{lin2023text}
Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Nan Duan,
  and Weizhu Chen. 2023.
\newblock \href {http://arxiv.org/abs/2212.11685} {Text generation with
  diffusion language models: A pre-training approach with continuous paragraph
  denoise}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30.

\bibitem[{Yuan et~al.(2022)Yuan, Yuan, Tan, Huang, and
  Huang}]{yuan2022seqdiffuseq}
Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, and Songfang Huang. 2022.
\newblock Seqdiffuseq: Text diffusion with encoder-decoder transformers.
\newblock \emph{arXiv preprint arXiv:2212.10325}.

\bibitem[{Zheng et~al.(2023)Zheng, Yuan, Yu, and
  Kong}]{zheng2023reparameterized}
Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong. 2023.
\newblock A reparameterized discrete diffusion model for text generation.
\newblock \emph{arXiv preprint arXiv:2302.05737}.

\end{thebibliography}
