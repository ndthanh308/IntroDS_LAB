{
  "title": "XDLM: Cross-lingual Diffusion Language Model for Machine Translation",
  "authors": [
    "Linyao Chen",
    "Aosong Feng",
    "Boming Yang",
    "Zihui Li"
  ],
  "submission_date": "2023-07-25T15:08:34+00:00",
  "revised_dates": [
    "2023-08-01T00:28:21+00:00"
  ],
  "abstract": "Recently, diffusion models have excelled in image generation tasks and have also been applied to neural language processing (NLP) for controllable text generation. However, the application of diffusion models in a cross-lingual setting is less unexplored. Additionally, while pretraining with diffusion models has been studied within a single language, the potential of cross-lingual pretraining remains understudied. To address these gaps, we propose XDLM, a novel Cross-lingual diffusion model for machine translation, consisting of pretraining and fine-tuning stages. In the pretraining stage, we propose TLDM, a new training objective for mastering the mapping between different languages; in the fine-tuning stage, we build up the translation system based on the pretrained model. We evaluate the result on several machine translation benchmarks and outperformed both diffusion and Transformer baselines.",
  "categories": [
    "cs.CL"
  ],
  "primary_category": "cs.CL",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.13560",
  "pdf_url": null,
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 86160565,
  "size_after_bytes": 84488665
}