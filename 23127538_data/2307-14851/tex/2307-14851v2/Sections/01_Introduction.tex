\section{Introduction}\label{sec1}
 Seismic processing is an important step in the seismic survey value chain, dedicated to improving the quality of the seismic data. It involves a range of operations that include filters and corrections, such as seismic denoising, deconvoluton, interpolation, and velocity estimation. These techniques are critical for extracting useful information from recorded data that can be used to understand the geological structure and characteristics of the Earth's subsurface \cite{gan2022ewr, sun2023multichannel, yang2023fwigan}. 

 Commonly, seismic processing techniques can be classified into two broad categories: theory-driven and data-driven approaches, which are based on the fundamental principles that guide each technique and the underlying mathematical models that drive the analysis. Theory-driven approaches leverage the nature of the physical processes that govern the propagation of seismic waves in the subsurface. For example, the $f\raisebox{0mm}{-}k$ filtering method, which relies on the difference in frequency distribution between noise and signal, uses Fourier transform to convert seismic data into the frequency-wavenumber domain in which noise is potentially separated from signal \cite{askari2008ground, liu2019seismic}. Radon transform-based interpolation is used to estimate the missing data by fitting linear events in the seismic data \cite{trad2002accurate, trad2003latest, yu2007wavelet, wang2010seismic, shao2022seismic}. Seismic normal moveout corrections utilize the relationship between the travel time of seismic waves and the recording distance away from the source to correct the time delay of seismic reflection data \cite{de1988normal, shatilo2000constant}. Considerable attention has been devoted to the development of theory-driven approaches for seismic processing \cite{spitz1991seismic, wang2002seismic, herrmann2008curvelet, herrmann2008non, naghizadeh2009f, fomel2010seislet, liu2010oc, chen2014random, chen2015random, chen2014iterative, geng2020relative, zhang20223}, as they have the potential to provide optimal solutions based on mathematical and physical principles. 
 
 However, theory-driven approaches are usually developed based on some assumptions, which means that the accuracy of the processed data can be significantly impacted if the assumptions are incorrect. Moreover, the complexity of the processing involved in this approach can be a significant obstacle, requiring a high level of expertise and knowledge. Consequently, processing can be time-consuming and costly. Furthermore, the computational challenges associated with theory-driven seismic processing cannot be overlooked either. The calculations involved in some of the processing tasks, like velocity estimation, are frequently complicated and require significant computing power and resources. 

 On the other hand, data-driven seismic processing techniques have been gaining popularity in recent years due to their ability to process and interpret large amounts of seismic data quickly and efficiently. Unlike theory-driven approaches that rely on a priori assumptions about the underlying physical mechanism, data-driven methods typically leverage machine learning algorithms to identify patterns and extract meaningful information from the seismic data. One of the primary strengths of data-driven methods is their flexibility and adaptability to different types of seismic processing tasks (SPTs). Hence, it has been used to perform a wide range of SPTs, such as noise attenuation \cite{yu2019deep, dong2019desert, dong2022seismic, yuan2020ground, birnie2021potential, liu2022accelerating, saad2020deep, liu2022coherent, saad2023unsupervised}, interpolation \cite{jia2017can, wang2019deep, wang2020seismic, tang2023simultaneous, zhang2020can}, deconvolution \cite{alaudah2018learning, gao2021deep}, imaging \cite{zhang2021consistent, zhang2021least, zhang2022improving, yu2023enhancing, cheng2023seismic, cheng2023elastic}, inversion \cite{yang2019deep, du2022deep, li2022target, yang2023well}, and interpretation \cite{zheng2019applications, waldeland2018convolutional, wu2019faultseg3d, shi2019saltseg}, and are often not limited by specific assumptions of any given processing approach. Meanwhile, the increasing amounts of seismic data being acquired has made it increasingly necessary to find faster and more efficient ways of processing and interpreting such data. Data-driven approaches have emerged as a viable solution to this problem, as they can automate many of the routine and repetitive tasks involved in theory-driven seismic processing workflows, reducing processing time and improving the accuracy of results. 

Although machine learning-based data-driven methods have shown successful applications in many SPTs, most of the developments involved developing networks for specific tasks or training them on specific datasets. This leads to many drawbacks. Firstly, different neural network (NN) models need to be trained and optimized separately for each SPT, which can be time-consuming and computationally expensive. Secondly, they may not fully capture the complexity and interrelationships among different SPTs. As a result, their generalization ability to other tasks or datasets is often limited. Moreover, training NN for various SPTs may require a large amount of labeled data for each specific task, which can be challenging and costly to obtain. This limitation may restrict the applicability of these methods, particularly in cases where data are scarce or expensive to acquire \cite{birnie2021potential, liu2022coherent, saad2023unsupervised}, like field data. Furthermore, training networks for SPTs independently may lead to suboptimal performance and inefficiencies. By considering the interrelationships among different tasks, it may be possible to improve the overall processing efficiency and accuracy. Therefore, it is important to develop more integrated approaches that can effectively capture the complex interrelationships among different SPTs. Multi-task learning (usually involving two tasks) have demonstrated improvements in performance compared to the common case in which the networks are trained for the two tasks seperately \cite{birnie2022transfer}. 

Obviously, multi-task learning is a challenging. However, it is feasible as seismic data may share common feature information, such as their sinusoidal nature and geometric behavior. This feature was demonstrated by Harsuko and Alkhalifah \cite{harsuko2022storseismic}, as they utilized a neural network, borrowed from natural language processing, to perform many SPTs. Specifically, they pretrain an NN for seismic reconstruction in a self-supervised fashion to extract seismic features. The pretrained NN is then fine-tuned via supervised learning for various SPTs, including denoising, velocity estimation, first arrival picking, and normal moveout. This work involves reusing knowledge from the source domain to improve performance on multiple target domains, synonymous to transfer learning. The hope is that the pretrained model will provide a good starting point for learning the various tasks, reducing the training time required to achieve high performance. Nonetheless, the utilization of transfer learning comes with certain limitations and challenges that impede its full potential. Firstly, transfer learning assumes that the source and target domains are similar, and that the knowledge learned in the source domain is relevant to the target domain, but this is not always the case. If the target domain is significantly different from the source domain, the performance of the transfer learning model may suffer. Even when the source and target domains are similar, the tasks in seismic processing can be different. For example, a model trained for event detection may not perform well for seismic inversion, which requires estimating the subsurface properties. Secondly, transfer learning models may suffer from overfitting to the source domain, leading to poor performance on the target domain. Furthermore, conventional transfer learning requires a large amount of labeled data in the source domain to train the model, which may not be readily available due to the high cost and time required to collect and label seismic data. Finally, it is difficult to determine which features or representations are transferable between different tasks and domains in seismic processing due to the complex nature of the data. 

In contrast, meta-learning (MetaL) \cite{schmidhuber1987evolutionary, hospedales2021meta} could be a promising alternative in seismic processing \cite{mousavi2022deep}, as it focuses on learning how to learn and adapt to new tasks and domains efficiently. MetaL models can be highly flexible and robust, allowing them to quickly adapt to new tasks and generalize across domains. Also, it can leverage prior knowledge and experience to learn new tasks more efficiently, without requiring a large amount of labeled data. Moreover, MetaL models can be trained on multiple tasks simultaneously, making them more scalable than transfer learning models. This flexibility of MetaL models is particularly useful in seismic processing, where the subsurface properties can vary greatly across different geological settings, and the tasks may differ substantially. The robustness of MetaL models helps them to be less susceptible to overfitting and task-specific biases, which is critical in seismic processing due to the limited availability of labeled data. Furthermore, the efficiency of MetaL in leveraging prior knowledge and experience can be beneficial for reducing the time and cost required to develop effective seismic processing models. Besides, the scalability of MetaL models can facilitate the handling of the numerous tasks required in seismic processing, allowing for more efficient and effective processing of seismic data. However, limited attention has been devoted to the application of MetaL in the field of seismology. Yuan et al. \cite{yuan2020adaptive} addressed the challenge of adaptation for first arrival picking among different seismic datasets using MetaL algorithms and demonstrated more accurate picking results than transfer learning methods. Sun and Alkhalifah \cite{sun2020ML} employ the concept of MetaL to develop an optimization algorithm that accelerates the convergence of full waveform inversion. 

Hence, in this paper, inspired by the many features that MetaL posses, we develop a unified paradigm for various SPTs, referred to as Meta-Processing, which uses limited training data and provides a common network initialization for universal adaptation. We, specifically, use the UNet \cite{ronneberger2015u}, which is the most popular and classic deep convolutional NN in the field of seismic processing \cite{wu2019faultseg3d, shi2019saltseg, yang2019deep, liu2022coherent, yu2023enhancing}, as our basic network architecture. Within the framework of Meta-Processing, this network will undergo a two-stage training: meta-training and meta-testing. In meta-training, each seismic processing is regarded as an independent task, whose limited training data is separated as two sets: support and query data sets. Different from conventional training approaches that optimize an NN model for a specific data set, a bilevel gradient updating from the support set to query sets is utilized to train a meta-learner model using various SPTs, including seismic denoising, interpolation, ground roll attenuation, imaging enhancement, and velocity estimation. In meta-testing, the meta-learner is used to quickly adapt the model to SPTs by fine-tuning its parameters on a few training data. Following that, the fine-tuned model is evaluated on a test set of examples from the various SPTs. Hence, the key idea of our method is to train an initialized set of parameters for a designated model across various SPTs. By doing so, we aim to achieve maximal performance on the corresponding task's test set with only a small number of gradient updates, which are computed using a small amount of training data specific to each task. 

The rest of the paper is structured as follows. We begin with reviewing the fundamental workflow of neural network-based seismic processing. Then, we detail the idea of the Meta-Processing algorithm, also, illustrate the data set establishment, network architecture, and the loss functions used in this study. Subsequently, we present the results from implementing our method on both synthetic and field data. Finally, we conclude by summarizing our work. \\