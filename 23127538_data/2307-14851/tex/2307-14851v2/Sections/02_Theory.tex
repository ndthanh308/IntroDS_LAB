\section{Methodologies}\label{sec2}
In this section we will highlight the concept of NN-based processing then describe the components of the Meta-processing including the algorithm, the data set, the network architecture, and loss function. 

\subsection{Neural network-based seismic processing}
Seismic processing is a complex and data-intensive task that aims to extract valuable information about the subsurface structure of the Earth by analyzing the signals embedded in recorded seismic waves. Machine learning algorithms, particularly NN, have emerged as a promising tool for addressing the challenges posed by seismic processing. The basic idea behind NN-based seismic processing, which is shown in Figure \ref{fig1}, is to leverage the power of machine learning algorithms to learn the underlying mapping relationships between input seismic data $x$ and the ideal processing results $y$ as follows:
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
y=\mathrm{NN}(x;\boldsymbol{\theta}),
\end{equation}
where the mapping relationships are represented by a parameterized function ${f_{\boldsymbol{\theta}}}$ with the learned parameters $\boldsymbol{\theta}$ of the NN. 

To obtain the parameterized function ${f_{\boldsymbol{\theta}}}$ for a specific SPT, we usually need to train the NN from scratch, whose workflow is reviewed in Figure \ref{fig2}. In brief, a large dataset is prepared and preprocessed, for example by normalizing it to $[-1,1]$. Next, we design an appropriate network architecture appropriate for the specific task at hand. Afterwards, careful consideration must be taken to set the hyperparameters (e.g., learning rate), NN initialization, the selection of optimizer, and the definition of a loss function. Initialization is a crucial component that affects the network's convergence speed and final optimization results, and therefore, it is essential to provide an appropriate initialization for the network. In practice, however, many initialization techniques rely on random or default values. Once these steps have been completed, the network can be trained using gradient descent to update the network's parameters iteratively. 

While the NN can effectively process input seismic data to predict desired output variables of interest after being trained, its efficacy is limited to the specific dataset and the task it was trained on. Furthermore, even for the same task, performance can be severely impacted when applied the network on seismic data from other regions. This occurs because the NN is only able to capture a limited number of seismic features from the training dataset, resulting in restricted performance and generalizability. Therefore, re-optimizing the NN is necessary to achieve advanced performance, although this process can be time-consuming. 

In light of these challenges, we are compelled to inquire whether it is possible to present a multi-task processing network with exceptional generalization abilities, capable of achieving superior performance via minimal gradient updates. In other words, our objective transcends the confines of a solitary SPT and instead seeks a task-level mapping relationship by standing on the shoulders of various SPTs. We illustrate this motivation in Figure \ref{fig3}. As seen, we aim to train a task-level parameterized function ${G_{\boldsymbol{\theta}}}$ that captures the designated task mapping relationship. Such a function can serve as a robust initialization for various target SPTs, enabling them to converge to the corresponding optimal solution ${f_{{\boldsymbol{\theta}}_i}}$ using limited training dataset and a small number of gradient updates. In the subsequent section, we will illustrate the application of MetaL algorithms to realize this goal.


% Figure environment removed 

% Figure environment removed 

% Figure environment removed 

\subsection{Meta-Processing algorithm}
MetaL provides a different framework in machine learning by enhancing the learning algorithm itself through multiple learning episodes across a distribution of related tasks \cite{finn2017model}. Inspired by this concept, we propose a Meta-Processing algorithm (see Algorithm 1) to provide a good initial NN model for various SPTs. In the following, we briefly introduce the algorithm. 

First, we need to split the collected training data into a support data set and a query data set for each task. The support and query data sets are similar to training and test sets in the context of conventional supervised learning (SL), but there are some important differences. In conventional SL, the training data set is used to optimize the model for a specific objective, while the test data set is used to evaluate the performance of the trained model on new, unseen data. In contrast, here, the support data set is used to train the model on the task-specific objective during the inner loop optimization, while the query data set is used to evaluate the performance of the adapted model after the inner loop optimization, i.e., in the outer loop optimization, which includes all the tasks. 

Following that, we can start the meta-training stage of the Meta-Processing algorithm. Let's use a parameterized function ${G_{\boldsymbol{\theta}}}$ to represent the NN with learned parameters ${\boldsymbol{\theta}}$. First, we randomly initialize the NN parameters ${\boldsymbol{\theta}}$. Then, we sample an SPT  ${\mathcal{T}_i}$ (like denoising, interpolation, and image enhancement), which includes limited support and query data sets, for a given task set ${p \left( \mathcal{T} \right)}$. For each sample task, we first perform a few iterations of gradient descent, namely the inner loop, to optimize the Meta-Processing model parameters. That is, we evaluate the performance of the NN on the support data set from the sampled task, and use gradient descent over single or multiple iterations to obtain the updated network parameters ${\boldsymbol{\theta}}^{'}$ as follows:
\begin{equation}\label{eq2}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\boldsymbol{\theta}_i^{'} = {\boldsymbol{\theta}} - lr_{inner} \cdot \nabla_{\boldsymbol{\theta}} \mathcal{L}_{\mathcal{T}_i} \left( G_{\boldsymbol{\theta}} \right),
\end{equation}
where ${lr_{inner}}$ denotes the learning rate for inner iterations as a hyperparameter, and ${\mathcal{L}}$ is the loss function. It should be emphasized that the inner loop is performed for each task separately, using a copy of the Meta-Processing model that is initialized with the current parameters of the Meta-Processing model. In essence, we are only updating the parameters of a copy of the meta-model, and not the meta-model itself. Specifically, we calculate the gradients based on the loss, and employ these gradients to update the model parameters using Equation (\ref{eq2}). Here, we do not use backpropagation to update the meta-model. 

Subsequently, the updated network parameters ${{\boldsymbol{\theta}}_i^{'}}$ are assessed on the query data set to calculate the corresponding loss value $\mathcal{L}_{\mathcal{T}_i}( G_{{\boldsymbol{\theta}}_i^{'}})$, i.e., the outer loop. This marks the completion of the training process for one task. As we repeat this process for all other tasks, we accumulate the losses evaluated on the query data sets of all tasks. Here, the accumulated loss 
\begin{equation}\label{eq3}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
{\boldsymbol{\theta}} \leftarrow {\boldsymbol{\theta}} - lr_{meta} \cdot \nabla_{\boldsymbol{\theta}} \sum_{\mathcal{T}_{i} \sim p (\mathcal{T})}^{} \mathcal{L}_{\mathcal{T}_i} ( G_{{\boldsymbol{\theta}}_i^{'}}),
\end{equation}
is finally used to update the desired Meta-Processing model parameters ${\boldsymbol{\theta}}$, where ${lr_{meta}}$ denotes the meta (outer iterations) learning rate. The aforementioned steps constitute the complete process of one epoch of training in the meta-training stage. As seen, the key difference between meta-training and conventional SL is that in meta-training, the model is trained to learn a good initialization that can be quickly adapted to new tasks, whereas the idea of conventional SL is to ensure the trained model can provide the accurate predictions on new, unseen data. 

After completing meta-training, we will perform the meta-testing stage to fine-tune the meta-based initialization model on each task, which is also trained with new limited data, and evaluate the model's convergence speed for each task, as well as its prediction accuracy on the corresponding test sets. The meta-testing stage follows a similar procedure as in conventional SL, with the only difference being that we provide the NN with a better and more robust initialization that can adapt to various SPTs. 

\begin{algorithm}
\caption{Meta-Processing}\label{alg:Framwork}
\textbf{Input:} ${p(\mathcal{T})}$: Different seismic processing tasks with the corresponding support and query datasets. \\
\textbf{Input:} ${lr_{inner}, lr_{meta}}$: Learning rate for inner and outer loops, respectively. \\
\textbf{Input:} ${iter}$: The number of iterations in the support dataset for every task. \\
\textbf{--------------------------------------- Meta-training stage ------------------------------------} \\
\textbf{Output:} Meta-based initialization of the NN model 
\begin{algorithmic}
\State 1: Randomly initialize network parameters ${\boldsymbol{\theta}}$
\State 2: \textbf{while} all tasks ${p(\mathcal{T})}$ \textbf{do}
\State 3: \quad Sample batch of tasks ${\mathcal{T}_i \sim p ( \mathcal{T})}$
\State 4: \quad \textbf{for} every $\mathcal{T}_i$ \textbf{do}
\State 5: \quad \quad \textbf{for} ${i}$ \textbf{in} ${iter}$ \textbf{do}
\State 6: \quad \quad \quad Evaluate $\nabla_{\boldsymbol{\theta}} \mathcal{L}_{\mathcal{T}_i} \left( G_{\boldsymbol{\theta}} \right)$ with respect to the support dataset for the sample task $\mathcal{T}_i$
\State 7: \quad \quad \quad Compute adapted parameters with gradient descent: \\
\quad \quad \quad \quad \quad \quad \quad \quad \quad ${\boldsymbol{\theta}}_i^{'} = {\boldsymbol{\theta}} - lr_{inner} \cdot \nabla_{\boldsymbol{\theta}} \mathcal{L}_{\mathcal{T}_i} \left( G_{\boldsymbol{{\boldsymbol{\theta}}}} \right)$
\State 8: \quad \quad \textbf{end for}
\State 9: \quad \quad Evaluate $ \mathcal{L}_{\mathcal{T}_i}( G_{{\boldsymbol{\theta}}_i^{'}})$ with respect to the query dataset from the sample task $\mathcal{T}_i$
\State 10: \quad \textbf{end for}
\State 11: \quad Sum the loss of all tasks on the query dataset: $\mathcal{L}_{sum} = \sum_{\mathcal{T}_i \sim p \left( \mathcal{T} \right)} \mathcal{L}_{\mathcal{T}_i} ( G_{{\boldsymbol{\theta}}_i^{'}})$
\State 12: \quad Update the Meta-Processing ${\boldsymbol{\theta}} \leftarrow {\boldsymbol{\theta}} - lr_{meta} \cdot \nabla_{\boldsymbol{\theta}} \mathcal{L}_{sum}$
\State 13: \textbf{end while}
\State 14: \textbf{Return:} Meta-Processing parameters ${\boldsymbol{\theta}}$
\end{algorithmic}
\textbf{--------------------------------------- Meta-testing stage ------------------------------------}\\
\textbf{Output:} Task-specific NN model 
\begin{algorithmic}
\State 15: Fine-tune the Meta-Processing parameters ${\boldsymbol{\theta}}$ on each specific task
\State 16: Testing the updated model to obtain the seismic processing results
\end{algorithmic}
\end{algorithm}

\subsection{Data set establishment}
As previously stated, unlike conventional SL, our algorithm requires a support data set and a query data set to be provided during the meta-training stage, and a training data set and a test data set to be utilized during the meta-testing stage. Therefore, for each of the five tasks specified in this study, we generate 200 pairs of input-label data for both the support and query data sets. Likewise, for the meta-testing stage, we also collect 200 pairs of input-label data for each task. It is worth emphasizing that all the training data used in our experiments are synthetic, and the size of the dataset is deliberately limited. This is due to the significant challenge of acquiring labeled data in real-world scenarios.  We will evaluate the effectiveness of our algorithm on both synthetic and field data to assess its performance in practical settings. 

\subsection{Network architecture}
The UNet is a type of convolutional neural network architecture commonly applied in the field of seismology, and has demonstrated excellent performance in numerous SPTs. Here, we also adopt the UNet network architecture, as shown in Figure \ref{fig4}a. The UNet architecture consists of a contracting path and an expanding path. The contracting path is a series of encoders ($E_1$, $E_2$, $E_3$, $E_4$, and $E_5$) and pooling layers that extract high-level features from the input seismic data, while reducing its resolution. The expanding path is a series of decoders ($D_1$, $D_2$, $D_3$, and $D_4$) and upsampling layers that reconstruct the original resolution. Also, the architecture includes skip connections to connect corresponding layers in the contracting and expanding paths, which allow the network to reconstruct detailed structures that might be lost in the down-sampling process.  

To further improve UNet's performance, we utilize a modified residual network baseline (MRNB) to replace the conventional convolutional layers in the encoder and decoder. We present the structure of MRNB in Figure \ref{fig4}b. As we can see, MRNB consists of two residual blocks that combine Layer Normalization (LayerNorm), convolutional layers (1x1 and 3x3 conv), a simplified channel attention (SCA) module, and nonlinear activation functions LeakyReLU. In the first residual block, the input data are first normalized using Layer Normalization and then processed through a 1x1 convolutional layer to double the number of channels. Next, a 3x3 convolutional layer is applied to extract features from the input data, followed by a nonlinear activation function LeakyReLU, which introduces nonlinearity into the network. An SCA module is then applied to the output of the LeakyReLU layer to perform channel-wise feature recalibration. Finally, another 1x1 convolutional layer is applied to restore the number of channels to the original input. Each channel of the resulting output is multiplied by a corresponding coefficient, which is updated during the network's training process, and then added to the input data as the input to the second residual block. The second residual block is similar to the first, but without the 3x3 convolutional layer and SCA module. This is done to reduce the number of trainable network parameters while maintaining the depth in the network. Here, the encoders $E_1$, $E_2$, $E_3$, $E_4$, and $E_5$ include 2, 2, 4, 8, and 12 MRNBs, respectively, with the corresponding number of feature maps are 64, 128, 256, 512, and 1024, respectively. The decoders $D_1$, $D_2$, $D_3$, and $D_4$ all utilize 2 MRNBs corresponding to 64, 128, 256, and 512 feature maps, respectively. 

% Figure environment removed 

\subsection{Loss functions}
Loss functions are a crucial component of NN training as they provide a measure of how well the network is performing on a particular task. Selecting the appropriate loss function can help improve the model's accuracy, convergence speed, and generalization ability. Hence, for both the meta-training and meta-testing stages, we combine the mean square error (MSE) and multiscale structure similarity index measure (MS-SSIM) to optimized the NN training. In which, the MSE loss is a common metric for evaluating the regression models' performance and can be expressed as: 
\begin{equation}\label{eq4}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\begin{gathered}
\mathcal{L}_{\rm{MSE}}\left (L,O \right)=\frac{1}{N}\displaystyle \sum^{N}_{i=1}{\left|L_{i}-O_{i} \right|^2},
\end{gathered}
\end{equation}
where $L$ and $O$ represent the label and the output of the network, respectively, and $N$ is the total number of samples. 

MS-SSIM is a sophisticated metric that assesses the degree of structural similarity between the prediction and label by considering its multiscale nature from a visual perspective \cite{du2022deep, geng2022deep}. The calculation process of MS-SSIM involves multiple steps, beginning with the division of the input data into non-overlapping patches at various scales using a Gaussian filter. Subsequently, the local luminance $l(\cdot)$, contrast $c(\cdot)$, and structural information $s(\cdot)$ at different scales are computed using the following equation
\begin{equation}\label{eq5}
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\left\{\begin{array}{c}
l(L, O)=\frac{2 \mu_L \mu_O+C_1}{\mu_L^2+\mu_O^2+C_1} 
\\[8pt]
c(L, O)=\frac{2 \sigma_L \sigma_O+C_2}{\sigma_L^2+\sigma_O^2+C_2} 
\\[8pt]
\setlength{\belowdisplayskip}{5pt}
s(L, O)=\frac{\sigma_{L O}+C_3}{\sigma_L \sigma_O+C_3}
\end{array}\right.,
\end{equation}
where $\mu_L$ and $\mu_O$ denote the mean values of the local patch corresponding to the label and output, respectively, $\sigma_L$ and $\sigma_O$ are the variance of the local patch, and $\sigma_{L O}$ represents the covariance of local patch. $C_1$, $C_2$, and $C_3$ are small constants to stabilize the division, where $C_2=C_3$. The MS-SSIM loss is then calculated by comparing the local luminance, contrast, and structural information at different scales, employing a weighted average exponents $\alpha_M$, $\beta_j$, and $\gamma_j$ across scales as follows
\begin{equation}\label{eq6}
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\begin{split}
\mathcal{L}_{\mathrm{MS\mbox{-}SSIM}}\left (L,O \right) = 1 -
\left [l_{M} \left (L,O \right) \right]^{\alpha_{M}} \cdot \displaystyle \prod^{M}_{j=1} \left [c_{M} \left (L,O \right) \right]^{\beta_{j}} \left [s_{M} \left (L,O \right) \right]^{\gamma_{j}}.
\end{split}
\end{equation}
Here, referring to Wang et al. \cite{wang2003multiscale}, we set five scales, and the exponents are $\beta_1=\gamma_1=0.0448$, $\beta_2=\gamma_2=0.2856$, $\beta_3=\gamma_3=0.3001$, $\beta_4=\gamma_4=0.2363$, and $\alpha_5=\beta_5=\gamma_5=0.1333$. 

Utilizing MSE and MS-SSIM losses, the total loss function is defined as
\begin{equation}\label{eq7}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\begin{gathered}
\mathcal{L}=c \cdot (\epsilon_1 \cdot \mathcal{L}_{\rm{MSE}}+\epsilon_2 \cdot \mathcal{L}_{\mathrm{MS\mbox{-}SSIM}}).
\end{gathered}
\end{equation}
where the hyperparameters $\epsilon_1$ and $\epsilon_2$ are used to balance the two losses. Here, for simplification, both $\epsilon_1$ and $\epsilon_2$ are set to 1. $c$ represents a scaling factor that is employed to adjust the magnitude of the loss value. During the meta-training stage, large loss values can trigger instability issues in the training process, so we, in this paper, set $c=0.1$ as a measure to prevent optimization failures. However, in the meta-testing stage, the network initialization provided by the meta-training stage is already sufficiently robust, and hence $c$ is set to 1.  \\