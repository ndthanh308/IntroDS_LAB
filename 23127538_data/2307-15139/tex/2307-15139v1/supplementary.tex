\appendix\onecolumn
\renewcommand{\theequation}{\thesection.\arabic{equation}}
\setcounter{equation}{0}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\setcounter{figure}{0}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\setcounter{table}{0}
\newpage

\begin{center}
\textbf{\Large Online Clustered Codebook} \\[5pt]
\end{center}

% The supplementary materials are organized as follows:
% \begin{enumerate}
%     \item The source code, named \textcolor[RGB]{255,0,0}{code}, is provided to verify and demonstrate our \mname.
%     \item Experimental details.
%     \item Quantitative results for more metrics.
%     \item Qualitative results for data compression.
% \end{enumerate}

\section{Experiment Details}

For data compression, we first demonstrate our method on small datasets with the officially released VQ-VAE~\cite{van2017neural} implementation~\footnote{\href{https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py}{https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py}}\footnote{\href{https://github.com/deepmind/sonnet/blob/v1/sonnet/examples/vqvae_example.ipynb}{https://github.com/deepmind/sonnet/blob/v1/sonnet/examples/vqvae\_
example.ipynb}}, and we then verify the generality of our quantiser on large datasets using the officially released VQ-GAN~\cite{esser2021taming} architecture~\footnote{\href{https://github.com/CompVis/taming-transformers}{https://github.com/CompVis/taming-transformers}}.
For image generation application, we apply our \mname's quantiser on LSUN dataset using the officially released LDM~\cite{rombach2022high} code\footnote{\href{https://github.com/CompVis/latent-diffusion}{https://github.com/CompVis/latent-diffusion}}.

For the small datasets (MNIST, CIFAR10, and Fashion MNIST), we use the submitted \textcolor[RGB]{255,0,0}{code} to train the model.
The training hyperparameters match the original VQ-VAE settings, and we train all models with batch size 1,024 across 4$\times$ NVIDIA GeForce GTX TITAN X (12GB per GPU) with 500 epochs (2-3 hours).

For the high resolution datasets (FFHQ and ImageNet), we just replace the original quantiser in VQGAN with our \mname quantiser.
The training hyperparameters also follow the original settings, and we train all models with batch size 64 across 4$\times$ NVIDIA RTX A6000 (48GB per GPU) with 4 days on FFHQ and 8 days on ImageNet until converge.

For the generation (LSUN bedrooms and Churches), we use the lSUN-beds256 config file for default setting with two modifications: 1) we also replace the VQGAN's quantiser with our \mname quantiser; 2) we reduce the images' resolution for faster training with 8$\times$ downsampling. For stage \textbf{a)} codebook learning, two models are trained with batch size 32 across 2$\times$ NVIDIA RTX A4000 (48GB per GPU) with 5 days. Then, for stage \textbf{b)} latent diffusion model with $32\times32\times4$ resolution, we train the models with batch size 128 across 2$\times$ NVIDIA RTX A4000 (48GB per GPU) with 7 days. During the inference, we follow the default settings to sample the images with 200 steps.

\section{Quantitative Results}

\begin{table}[htb!]
    \centering
    \renewcommand{\arraystretch}{1.0}
    \setlength\tabcolsep{6pt}
    \begin{tabular}{@{}l c ccccc @{}}
    \toprule
    \textbf{Method} & \textbf{Dataset} & $\ell_1 \text{loss}\downarrow$ & SSIM $\uparrow$ & PSNR $\uparrow$ & LPIPS $\downarrow$ & rFID $\downarrow$ \\
    \midrule
    VQ-VAE~\cite{van2017neural} & \multirow{4}{*}{MNIST}& 0.0207 & 0.9777 & 26.48 & 0.0282 & 3.43 \\
    HVQ-VAE~\cite{williams2020hierarchical} & & 0.0202 & 0.9790 & 26.90 & 0.0270 & 3.17 \\
    SQ-VAE~\cite{takida2022sq} & & 0.0197 & 0.9819 & 27.49 & 0.0256 & 3.05 \\
    \cdashline{1-5}
    \textbf{\mname} & & \textbf{0.0180} &  \textbf{0.9833} & \textbf{27.87} &\textbf{0.0222} & \textbf{1.80} \\
    \midrule
    VQ-VAE~\cite{van2017neural} & \multirow{4}{*}{CIFAR10}& 0.0527 & 0.8595 & 23.32 & 0.2504 & 39.67 \\
    HVQ-VAE~\cite{williams2020hierarchical} & & 0.0533 &  0.8553 & 23.22 & 0.2553 & 41.08 \\
    SQ-VAE~\cite{takida2022sq} & & 0.0482 & 0.8779 & 24.07 & 0.2333 & 37.92 \\
    \cdashline{1-5}
    \textbf{\mname} & & \textbf{0.0448} & \textbf{0.8978} & \textbf{24.72} & \textbf{0.1883} & \textbf{24.73} \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Reconstruction results} on the validation sets of MNIST (10,000 images) and CIFAR10 (10,000 images). }
    \label{tab:app_rec_quant}
\end{table}

\Cref{tab:app_rec_quant} provides a comparison of our results to state-of-the-art quantisers under the same training settings, except for the different quantisers, on the small datasets.
This is an extension of \cref{tab:rec_quant} in the main paper.
All images are normalised to the range [0,1] for quantitative evaluation.
See the code for more details.
While the proposed \mname achieve relative small improvements on traditional pixel-level $\ell_1$ loss, peak signal-to-noise ration (PSNR), and patch-level structure similarity index (SSIM), it significantly improves the feature-level LPIPS and dataset-level rFID, suggesting that our \mname is more capable of reconstructing the content closer to the dataset distribution.

\begin{table}[htb!]
    \centering
    \renewcommand{\arraystretch}{1.0}
    \setlength\tabcolsep{6pt}
    \begin{tabular}{@{}l c ccccccc @{}}
    \toprule
         \textbf{Method} &  \textbf{Dataset} & $\mathcal{S}$ $\downarrow$ & $\mathcal{K}$ $\downarrow$ & Usage $\uparrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & rFID $\downarrow$ \\
    \midrule
    VQGAN~\cite{esser2021taming} & \multirow{6}{*}{FFHQ}& 16$^2$& 1024& $42\%$ & 22.24 & 0.6641 & 0.1175 & 4.42\\
    ViT-VQGAN~\cite{yu2022vectorquantized} & & 32$^2$& 8192& -- & -- & -- & -- & 3.13\\
    RQ-VAE~\cite{lee2022autoregressive} & & 16$^2\times$4 & 2048 & -- & 22.99 & 0.6700 & 0.1302 & 3.88 \\
    MoVQ~\cite{zhengmovq} & & 16$^2\times$4 & 1024 & $56\%$& 26.72 & 0.8212 & 0.0585 & 2.26 \\
    SeQ-GAN~\cite{gu2022rethinking} & & 16$^2$& 1024 & $100\%$& -- & -- & -- & 3.12 \\
    \cdashline{1-9}
    \textbf{\mname} (ours) & & 16$^2$& 1024 & $100\%$ & 26.82 & 0.8313 & 0.0608 & 2.80 \\
    \textbf{\mname} (ours) & & 16$^2\times$4 & 1024 & $100\%$ & \textbf{26.87} & \textbf{0.8398} & \textbf{0.0533} & \textbf{2.03}\\
    \midrule
    VQGAN~\cite{esser2021taming} & \multirow{6}{*}{ImageNet}& 16$^2$& 1024 & $44\%$ & 19.07 & 0.5183 & 0.2011 & 7.94 \\
    ViT-VQGAN~\cite{yu2022vectorquantized} & & 32$^2$& 8192 & $96\%$& -- & -- & -- & 1.28 \\
    RQ-VAE~\cite{lee2022autoregressive} & & 8$^2\times$16& 16384 & - & -- & -- & -- & 1.83\\
    MoVQ~\cite{zhengmovq} & & 16$^2\times$4& 1024 & $63\%$& 22.42 & 0.6731 & 0.1132 & \textbf{1.12} \\
    SeQ-GAN~\cite{gu2022rethinking} & & 16$^2$ & 1024 & $100\%$ & -- & -- & -- & 1.99\\
    \cdashline{1-9}
    \textbf{\mname} (ours) & & 16$^2$& 1024 & $100\%$ & 21.95 & 0.6612 & 0.1340 & 1.57 \\
    \textbf{\mname} (ours) & & 16$^2\times$4 & 1024 & $100\%$ & \textbf{23.37} & \textbf{0.7115} & \textbf{0.1099}& 1.20 \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Reconstruction results} on validation sets of ImageNet (50,000 images) and FFHQ (10,000 images). $\mathcal{S}$ denotes the latent size of encoded features, and $\mathcal{K}$ is the number of codevectors in the codebook. }
    \label{tab:app_rec_sota}
\end{table}

We further compare our \mname to the state-of-the-art methods in data compression in \cref{tab:app_rec_sota}.
This is an extension of \cref{tab:rec_sota} in the main paper.
Here, we add the pixel-level PSNR, patch-level SSIM and feature-level LPIPS.
For FFHQ dataset, our \mname model outperforms baseline variants of previous state-of-the-art models.
As for ImageNet dataset, while our 4$\times$ channels setting does not achieve the better rFID than the latest MoVQ model, the other instantiations (PSNR, SSIM and LPIPS) significantly outperform existing state-of-the-art models.

\begin{table}[htb!]
    \centering
    \renewcommand{\arraystretch}{1.0}
    \setlength\tabcolsep{3pt}
    \begin{tabular}{@{}ll cccc cccc cccc@{}}
    \toprule
         &  \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{MNIST} (28$\times$28)} && \multicolumn{3}{c}{\textbf{CFAIR10} (32$\times$32)} && \multicolumn{3}{c}{\textbf{Fashion MNIST} (28$\times$28)}\\
         \cline{3-5}\cline{7-9}\cline{11-13}
         &  & $\ell_1 \downarrow$ & PSNR $\uparrow$ & rFID $\downarrow$ && $\ell_1 \downarrow$ & PSNR $\uparrow$ & rFID $\downarrow$ && $\ell_1 \downarrow$ & PSNR $\uparrow$ & rFID $\downarrow$\\
    \midrule
    $\mathbb{A}$ & Baseline VQ-VAE~\cite{van2017neural}$_{\text{\scriptsize{NeurIPS'2017}}}$ & 0.0207 & 26.48 & 3.43 && 0.0527 & 23.32 & 39.67 && 0.0377 & 23.93 & 12.73 \\
    \cdashline{1-13}
    $\mathbb{B}$ & + Cosine distance & 0.0200 & 26.77 & 3.06 && 0.0509 & 23.66 & 35.14 && 0.0378 & 24.01 & 11.40 \\
    $\mathbb{C}$ & + Anchor initialization (offline) &  0.0192 & 27.24 & 2.78 && 0.0481 & 24.16 & 31.10 && 0.0373 & 24.04 & 11.92 \\
    $\mathbb{D}$ & + Anchor initialization (online) & 0.0186 & 27.58 & 2.23 && \textbf{0.0445} & \textbf{24.79} & 26.62  && 0.0349 & \textbf{24.69} & 9.27 \\
    $\mathbb{E}$ & + Contrastive loss &  \textbf{0.0180} & \textbf{27.87} & \textbf{1.80} && 0.0448 & 24.72 & \textbf{24.73} && \textbf{0.0344} & 24.66 & \textbf{8.85}\\
    \bottomrule
    \end{tabular}
    \vspace{-0.1cm}
    \caption{\textbf{Results on various settings.} We add pixel-level $\ell_1$ and PSNR metrics.}
    \vspace{-0.3cm}
    \label{tab:app_rule}
\end{table}

\begin{table}[htb!]
    \centering
    \renewcommand{\arraystretch}{1.15}
        \setlength\tabcolsep{3pt}
        \begin{tabular}{@{}l c ccccc c ccccc  @{}}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Dataset}}& \multicolumn{5}{c}{Offline} && \multicolumn{5}{c}{Online} \\
        \cline{3-7}\cline{9-13}
        & & $\ell_1 \text{loss}\downarrow$ & SSIM $\uparrow$ & PSNR $\uparrow$ & LPIPS $\downarrow$ & rFID $\downarrow$ && $\ell_1 \text{loss}\downarrow$ & SSIM $\uparrow$ & PSNR $\uparrow$ & LPIPS $\downarrow$ & rFID $\downarrow$ \\
        \midrule
        random & \multirow{4}{*}{MNIST} & 0.0195 & 0.9802 & 27.11 & 0.0262 & 3.20 && \textbf{0.0185} & \textbf{0.9823} & \textbf{27.58} & \textbf{0.0236} & 2.27\\
        unique && 0.0191 & 0.9811 & 27.25 & 0.0255 & 2.84 && 0.0186 & 0.9820 & 27.51 & 0.0237 & 2.24 \\
        probability && 0.0192 & 0.9810 & 27.24 & 0.0253 & 2.78 && 0.0186 & \textbf{0.9823} & \textbf{27.58} & \textbf{0.0236} & \textbf{2.23}\\
        closest && \textbf{0.0186} & \textbf{0.9823} & \textbf{27.59} & \textbf{0.0242} & \textbf{2.51} && 0.0187 & 0.9819 & 27.49 & 0.0244 & 2.59 \\
        \midrule
        random & \multirow{4}{*}{CIFAR10} & 0.0494 & 0.8755 & 23.91 & 0.2256 & 34.49 && 0.0440 & \textbf{0.9010} & 24.88 & 0.1881 & 26.04 \\
        unique && 0.0507 & 0.8705 & 23.15 & 0.2346 & 36.99 && \textbf{0.0439} & 0.9007 & \textbf{24.91} & \textbf{0.1877} & 26.03 \\
        probability && \textbf{0.0481} & \textbf{0.8829} & \textbf{24.16} & \textbf{0.2131}& \textbf{31.10} && 0.0445 & 0.8991 & 24.79 & 0.1898 & 26.62 \\
        closest && 0.0487 & 0.8804 & 24.06 & 0.2156 & 32.31 && 0.0444 & 0.8994 & 24.83 & 0.1900 & \textbf{25.99} \\
        % random & \multirow{4}{*}{MNIST} & 3.20 & 2.27 \\
        % unique &&  2.84 & 2.24 \\
        % probability &&  2.78 & \textbf{2.23} \\
        % closest &&  \textbf{2.51} & 2.59 \\
        % \midrule
        % unique &&  36.99 & 26.02 \\
        \bottomrule
        \end{tabular}
    \caption{\textbf{Anchor sampling methods.} The choice of anchor sampling method has a significant impact on
    offline (one-time) feature initialization, while the online clustered method is robust for various samplings.}
    \label{tab:app_rec_ab_featm}
\end{table}

\Cref{tab:app_rule,tab:app_rec_ab_featm} are the extension of \cref{tab:rule,tab:rec_ab_featm} in the main paper, respectively.
Even reported with the different metrics, The conclusions are still the same.
For instance, the offline version is significantly affected by different anchor sampling methods, but the online version is not sensitive to various anchor sampling methods.
The online version holds very close performance with these anchor sampling methods.

% \section{Qualitative Results}

% % Figure environment removed

% \Cref{fig:app_rec} is the extension of \cref{fig:rec}.
% Here, we provide the qualitative comparison of our \mname and the baseline VQGAN \cite{esser2021taming}.
% Compared to the baseline VQGAN model, our \mname reconstructs high quality images by providing much more details, such as the hand, eyes, eyeglasses, hat, eardrop, and the whole faces.
% These detail information is important for the downstream task.
% For instance, in the image editing task, the model is expected to generate plausibly reasonable content for the modified regions, while preserving the original content information as much as possible.
