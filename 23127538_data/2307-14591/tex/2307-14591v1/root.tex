%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{booktabs} 
\usepackage{diagbox}  
\usepackage{multirow}  
\usepackage{caption}
\usepackage{longtable}
\usepackage{pdflscape}
\usepackage{changepage} 
\usepackage{stfloats}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\title{\LARGE \bf
The Detection and Rectification for Identity-Switch Based on the Unfalsified Control
}


\author{Junchao Huang, Xiaoqi He, Sheng Zhao% <-this % stops a space
\thanks{Junchao Huang and Sheng Zhao is with Electrical Information and Electrical Engineering, Automation, Shanghai Jiao Tong University, Shanghai, China.}%
\thanks{XiaoQi He is with Ningbo Industrial Internet Institute, Ningbo, China.}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

The goal of multi-object tracking (MOT) is to continuously track and identify objects detected in videos. Currently, most methods for multi-object tracking model the motion information and combine it with appearance information to determine and track objects. In this paper, unfalsified control is employed to address the identity-switch problem in multi-object tracking. We establish sequences of appearance information variations for the trajectories during the tracking process and design a detection and rectification module specifically for identity-switch detection and recovery. We also propose a simple and effective strategy to address the issue of ambiguous matching of appearance information during the data association process. Experimental results on publicly available MOT datasets demonstrate that the tracker exhibits excellent effectiveness and robustness in handling tracking errors caused by occlusions and rapid movements.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Multi-object tracking (MOT) aims to track and identify the trajectories of multiple objects in a video scene. With the rapid development of object detection methods, the current mainstream and effective approaches still rely on detection-based tracking paradigms. Detection-based tracking methods transform the multi-object tracking problem into a data association problem, where the current frame detection boxes are associated with the detection boxes from the previous frame to establish object trajectories. 

However, errors in information acquisition, processing, and prediction during the tracking process can lead to matching errors in data association, resulting in ID-switch in multi-object tracking.Current research efforts have been focused on reducing the occurrence of ID-switch by addressing data sources and data association processes. 

In this work, we take a different perspective on the ID-switch problem. Instead of solely reducing the occurrence of ID-switch, we also focus on identifying whether ID-switch occur and attempt to correct it. We establish a multi-object tracking model based on unfalsified control, which enables the tracker to monitor the state of objects and includes the ID-switch detection module(IDSD).The tracker incorporates historical data that is often overlooked. We introduce the ID-switch rectification module(IDSR) based on historical information to attempt the recovery of objects that have experienced ID-switch, making our tracker the first in the MOT field to address ID-switch and attempt rectification.The Ambiguous match improvement module(AMI) in our tracker effectively reduces the problems caused by small differences in appearance information during the data association process. Moreover, the IDSD, IDSR, and AMI modules of our tracker can be easily integrated into other tracking approaches.We conduct experiments on MOT datasets and achieve promising results. We believe that our approach provides a new possibility for addressing the ID-switch problem in multi-object tracking. Finally, we discuss the limitations and applicability of our approach.

\section{RELATED WORKS}
Different approaches in data processing lead to variations in multi-object tracking (MOT) methods, which can be broadly categorized into the following types: motion-based tracking, appearance-based tracking, and other types of learning-based tracking.
\subsection{Motion-based tracking}

Motion information is primarily used in the data association process. Motion-based trackers typically employ methods such as Kalman filtering \cite{ref23} and particle filtering \cite{ref24} to predict the motion of objects and then match their predicted positions with the detection boxes in the next frame to establish data association. SORT \cite{ref17} utilizes Kalman filtering for box prediction and performs data association using IOU and the Hungarian algorithm \cite{ref25}. Bytetrack \cite{ref3} focuses on leveraging low-confidence detection boxes, while OCR-SORT \cite{ref5} emphasizes modeling motion based on observed results, improving tracking robustness in nonlinear motion scenarios. These motion models employ Bayesian estimation to predict the next state and establish tracking trajectories through data association.

\subsection{Appearance-based tracking}

With the advancements in the field of re-identification(ReID), effective methods for extracting appearance features have been proposed \cite{ref40,ref38,ref39,ref22,ref21}, leading to the integration of appearance information in multi-object tracking research. DeepSORT \cite{ref7} is one of the early trackers that incorporates appearance information by combining it with motion information in the data association process, improving the robustness of tracking systems under occlusion. ATOM \cite{ref31} replaces Kalman filtering with deep learning for box estimation, while BOT-SORT \cite{ref2} incorporates ReID features into high-resolution detection boxes. Finetrack \cite{ref9} introduces a feature pyramid network that learns semantic flows between feature maps of different resolutions to correct spatial misalignments, enabling more accurate learning of appearance features. Approaches such as FairMOT \cite{ref11,ref12,ref13} perform object detection and appearance feature extraction within a single network, allowing end-to-end training and reducing inference time. More recently, Deep OCR-SORT \cite{ref6} adaptively integrates appearance matching into existing high-performance motion-based methods using object appearance, further enhancing tracking performance.

\subsection{Other types of learning-based tracking}

Motiontrack \cite{ref16} employs trained interaction and patrol modules to handle complex motion in dense crowds and reidentify lost trajectories. CBIOU \cite{ref4} extends the detection and tracking matching space by adding a buffer, mitigating the impact of irregular motion. MAAtrack \cite{ref10} proposes a novel tracking association method that models fuzzy matching by searching for potential track detections with similar distances. In recent years, the successful application of transformer \cite{ref26} in the visual domain, particularly the work by \cite{ref27}, has sparked a wave of combining transformer with multi-object tracking. Approaches such as \cite{ref8,ref18,ref19} treat multi-object tracking as a sequence prediction problem, where each sequence corresponds to a trajectory of an object. MOTR \cite{ref8}, in particular, achieves end-to-end multi-object tracking by tracking queries and objects.

\section{Methods}

In this section, we present our proposed detection and rectify module specifically designed to address the ID-switch problem within the tracking-by-detection paradigm. By integrating this module into the well-known tracking-by-detection algorithm, Bytetrack \cite{ref3}, we introduce our tracker, Unfctrack, which focuses on detecting and attempting to rectify ID-switch.


% Figure environment removed

% Figure environment removed

\subsection{ID-switch detection (IDSD)} 
Consider the state of the tracked object trajectory $O_i$ at time t=i-1 as $x_{i-1}$, and the state of the object within the detection box $K_i$ measured at time t=i as ${\hat{x}}_i$. We define the cost error as:

\begin{equation}
\label{deqn_ex1}
e_{K_i}^{O_i}=||{\hat{x}}_i-x_{i-1}||
\end{equation}

In the data association process between the current frame's tracked object trajectories and detection boxes, data association is only considered when $e_{K_i}^{O_i}$ is less than a given threshold $\epsilon$. Due to the existence of errors, multiple detection boxes may match with the same object trajectory. We include these multiple detection boxes in the candidate set $K$ for trajectory matching.

Suppose at time t=i, due to occlusion, there are two detection boxes in the candidate set K that match with the trajectory, $K$=[$k_1$, $k_2$], and the correct tracked target in the current frame is $k_2$. However, we select $k_1$ with a smaller error for data association, leading to an ID-switch. When the occlusion ends, the appearance information extracted based on the detection box becomes more accurate. As the ID-switch occurs, the difference between the historical appearance information stored in the trajectory and the appearance information in the current frame will gradually increase, and with time, it will exceed a threshold. Thus, we can determine that the trajectory has experienced an ID-switch. Therefore, the establishment of our unfalsified control model is as follows:


\begin{itemize}
\item Measurement information $P_{data}$: Appearance information extracted from objects during the tracking process.

\item Candidate set $K$: Detection boxes used for data association with trajectories.

\item Performance metric $T_{spec}$: The degree of change in appearance information of trajectories in a period of time.
\end{itemize}

To obtain measurement information $P_{data}$, we employ the BoT (SBS) \cite{ref29} feature extraction method from the FastReID \cite{ref20} library. We save appearance features every 5 frames, resulting in a sequence of appearance information with a length of 30. This sequence allows us to observe changes in appearance information. Even in the presence of occlusion-induced variations, the appearance information remains highly similar to the pre-occlusion state, providing data support for ID-switch detection. For the saved queue of appearance features, we establish another queue for post-processing to store the similarity costs between the current appearance feature and the previous appearance feature sequence. We calculate the cosine distance to measure the similarity between different appearance information. Then, we compute the variance of the similarity costs, which serves as the performance metric $T_{spec}$ for judging ID-switch cases.


We select the appearance feature $f$ of the current frame and calculate the cosine cost $C$ between $f$ and the previous 2⁄3 appearance features $f_i$ in the trajectory queue. We establish a cosine cost queue with a length of 30, and the calculation formula for $C$ is as follows:
\begin{equation}
\label{deqn_ex1}
C\ =(\sum_{i=1}^{n}{(1-\frac{f*f_i}{||f||*||f_i||}}))/n
\end{equation}

Based on the values of C in the queue, we calculate $T_{spec}$, where $\overline{C}$ represents the average of $C_i$:

\begin{equation}
\label{deqn_ex1}
T_{spec}=\frac{\sum_{i}^{n}{(C_i-\overline{C})}^2}{n}
\end{equation}

When there are short-term variations in appearance features due to occlusion, the variance will remain at a low level. However, when an ID-switch occurs and the appearance information undergoes long-term changes, the performance metric $T_{spec}$ will continue to rise. Once it exceeds a threshold $T_\theta$, reaching the falsification criterion, we consider the trajectory to have undergone an ID-switch and remove the matched trajectory from the candidate set $K$.

\subsection{ID-switch rectification (IDSR)}

After falsifying the trajectories that have undergone an ID-switch using unfalsified control, we attempt to restore the true detection boxes corresponding to these trajectories. We establish a queue for each trajectory to store its appearance information. Therefore, we can select the appearance information extracted before the occurrence of the ID-switch, which represents the appearance information when the trajectory was not subjected to an ID-switch, as data support for ID correction.

To ensure the accuracy of the rectification process, we consider a trajectory to be associated with a detection box and update the trajectory only when the cosine cost is below a very small threshold, denoted as $C_\theta$. In other words, the two appearance feature vectors are nearly identical. Furthermore, if we cannot find a suitable detection box to match the trajectory undergoing an ID-switch, we assign a new ID to the trajectory. This prevents it from continuing to be tracked using the incorrect ID.


\subsection{Ambiguous match improvement (AMI)}

The fusion of motion information and appearance information is performed in a similar manner to \cite{ref7}, using a weighted fusion approach. The motion information, represented by $d_{iou}$, and the appearance information, represented by $d_{ReID}$, are fused to obtain the fused information $d_{dist}$, using the following fusion method:

\begin{equation}
\label{deqn_ex1}
d_{dist}=\alpha*d_{iou}+(1-\alpha)*d_{ReID}(0<\alpha<1)
\end{equation}

The appearance information $d_{ReID}$, is obtained by extracting features from the detection boxes. However, many detection boxes suffer from overlap issues, resulting in significant ambiguity in some $d_{ReID}$ values. This ambiguity can lead to severe fuzzy matching problems, potentially causing ID-switch and even discarding tracked trajectories for further processing in the next frame. To address this issue, we have designed a simple yet effective module called AMI to handle the problem of fuzzy matching. Specifically, we discard matches with confidence scores higher than a threshold $d_\theta$, for low-confidence matches. For each row and column, we calculate the weights for high-confidence matches and low-confidence matches, respectively, and discard the low-confidence matches with lower weights.

% Figure environment removed

% Figure environment removed




\section{Experiments}

\subsection{Experimental settings}

\textbf{Datasets.} The experiments are conducted on the MOT17 \cite{ref14} and MOT20 \cite{ref15} datasets under the "private detection" protocol. The MOT17 and MOT20 datasets are widely used benchmarks in the field of multi-object tracking. The main difference between the two datasets is that MOT17 contains data captured by both static and dynamic cameras, while MOT20 includes larger and more crowded scenes. Since both MOT17 and MOT20 do not provide a separate validation set, we follow the common practice \cite{ref41,ref3} of splitting the training set into halves for training and validation.

\textbf{Metrics.} Existing methods in the field have not considered the possibility of recovering IDs after an ID-switch occurs. Therefore, widely accepted metrics in multi-object tracking, such as MOTA, HOTA, and IDSW \cite{ref36}, are not suitable for evaluating the performance of our tracker. We observed in our experiments that if the ID of a detected object changes once, IDSW increases by 1. However, if we correct the IDs of objects that have undergone an ID-switch, IDSW increases by 1 instead of decreasing by 1. This results in an increase in IDSW and a decrease in MOTA when we judge and correct ID-switch situations. Therefore, metrics like MOTA and IDSW cannot effectively evaluate our tracker, and we emphasize the potential of addressing the ID-switch problem from different perspectives.

\textbf{Implementation details.} Our multi-object tracking method follows the detection-to-tracking paradigm. We use the publicly available YOLOX \cite{ref37} as the detector, and the detection results are then used as input for our tracker. For ReID feature extraction, we utilize the SBS-50 model from the open-source FastReID \cite{ref20}, which is pre-trained on MOT17 and MOT20 \cite{ref2}. Throughout the experiments, we set the default detection score threshold $\tau$ to 0.6 and remove a trajectory if it is lost for more than 30 frames. The threshold value for $T_\theta$ in IDSD is set to 0.01 by default, and the threshold value for the cosine cost $C_\theta$ in IDSR is set to 0.1 by default.


\subsection{Testing and experiments results}

In order to comprehensively demonstrate the tracker's ability in detecting and correcting ID-switches, the AMI module was not used during the experimentation with the IDSD and IDSR modules.

\textbf{ID-switch detection(IDSD).} In the MOT17-01 dataset, we set the threshold $T_\theta$ for $T_{spec}$ in IDSD to 0.01 to evaluate the tracker's ability to detect ID-switch situations. To exclude temporary variations in trajectory appearance features due to occlusion rather than ID-switch, we consider a trajectory to have undergone an ID-switch only when its performance metric $T_{spec}$ exceeds $T_\theta$ for more than 10 consecutive frames. In the experiment, at frame 40, a new object enters from the right side of the field of view, resulting in an ID-switch for trajectory 6. We observe that the performance metric $T_{spec}$ for trajectory 6 exceeds $T_\theta$ at frame 43, and the tracker determines the occurrence of an ID-switch for trajectory 6 at frame 53.

In the experiments conducted on other datasets in MOT17 and MOT20, our tracker demonstrates excellent ability in falsifying trajectories that have undergone ID-switch. It is worth noting that due to differences in camera angles and tracking environments across different datasets, there may be variations in setting the threshold $T_\theta$ for $T_{spec}$ in IDSD.

\textbf{ID-switch rectification (IDSR).} In the MOT17-01 dataset, after falsifying trajectory 6 for an ID-switch at frame 53 using unfalsified control, we retrieve the first appearance feature $f_1$ from the trajectory 6 queue. This feature $f_1$ was extracted before the ID-switch occurred. We match this feature with the appearance information of the current frame's detection box, calculate the cosine cost, and if the cosine cost is below the threshold $C_\theta$, we perform ID rectification. Simultaneously, we remove the erroneous trajectory 23 and assign a new ID, 24, to the newly entered object on the right side. We observe that after correctly recovering the ID, the performance metric $T_{spec}$ decreases below the threshold $T_\theta$ and stabilizes.

\textbf{Ambiguous match improvement (AMI).} We conducted ablation experiments on the AMI module using the training sets of MOT17 and MOT20. We evaluated the tracker using the official evaluation tool, Trackeval, from the MOT challenge. We found that applying the AMI algorithm to process the appearance information $d_{ReID}$, resulted in significant performance improvement in tracking.

\begin{table*}[htbp]
  \centering
    \begin{tabularx}{\linewidth}{X|XXXXXXX} 
      \toprule
      Tracker & MOTA↑ & IDF1↑ & HOTA↑ & FP↓ & FN↓ & IDs↓ & FPS↑ \\
      \midrule
      Tube-TK \cite{ref30} & 63    & 58.6  & 48    & 27060 & 177483 & 4137  & 3 \\
      GSDT \cite{ref1} & 66.2  & 68.7  & 55.5  & 43368 & 144261 & 3318  & 4.9 \\
      LMOT \cite{ref32} & 72    & 70.3  & 56.7  & 28113 & 126704 & 3071  & 28.6 \\
      MOTR \cite{ref8} & 73.4  & 68.6  & 57.8  & \multicolumn{1}{p{3.78em}}{/} & \multicolumn{1}{p{4.055em}}{/} & 2439  & \multicolumn{1}{p{3.665em}}{/} \\
      FairMOT \cite{ref11} & 73.7  & 72.3  & 59.3  & 27507 & 117477 & 3303  & 25.9 \\
      Transtrack \cite{ref19} & 75.2  & 63.5  & 54.1  & 50157 & 86442 & 3603  & 59.2 \\
      CrowdTrack\cite{ref33} & 75.6  & 73.6  & 60.3  & 25950 & 109101 & 2544  & 140.8 \\
      STC \cite{ref34} & 75.8  & 70.9  & 59.8  & 44952 & 87039 & 4533  & 9.5 \\
      FCG \cite{ref35} & 76.7  & 77.7  & 62.6  & 13284 & 116205 & 1737  & 4.9 \\
      OC-SORT \cite{ref5} & 78    & 77.5  & 63.2  & 15129 & 107055 & 1950  & 29 \\
      Finetrack \cite{ref9} & 80    & 79.5  & 64.3  & 21750 & 90096 & 1272  & 35.5 \\
      BoT-SORT \cite{ref2} & 80.5  & 80.2  & 65    & 22521 & 86037 & 1212  & 6.8 \\
      \hline
      Bytetrack \cite{ref3} & 80.3  & 77.3  & 63.1  & 25491 & 83721 & 2196  & 29.6 \\
      Unfctrack(ours) & 79.7  & 79    & 64    & 21960 & 90834 & 1662  & 8.3 \\
      \bottomrule
    \end{tabularx}%
    \captionsetup{font=small,labelsep=colon} 
    \caption{Performance metrics comparison of Unfctrack tracker and other trackers on the MOT17 dataset under the private detection protocol.}
    \label{tab:addlabel}%
\end{table*}


\begin{table}[htbp]
  \centering
  \begin{tabular}{c|rrrr}
    \toprule
    Tracker & \multicolumn{1}{c}{MOTA↑} & \multicolumn{1}{c}{FP↓} & \multicolumn{1}{c}{FN↓} & \multicolumn{1}{c}{IDSW↓} \\
    \midrule
    MOT17 & 87.492 & 3355  & 10162 & 529 \\
    MOT17(AMI) & 90.042 & 2282  & 8540  & 361 \\
    MOT20 & 89.456 & 12703 & 37689 & 1093 \\
    MOT20(AMI) & 92.762 & 6315  & 28430 & 594 \\
    \bottomrule
  \end{tabular}%
  \captionsetup{font=small,labelsep=colon} 
  \caption{Validation of the AMI module results on the train datasets of MOT17 and MOT20 using the official evaluation tool Trackeval from MOT-challenge.} 
  \label{tab:addlabel}%
\end{table}

The experiments primarily emphasize the results of ID-switch detection and rectification. Although the metrics from \cite{ref36} are not suitable for effectively evaluating our tracker, we still provide experimental results for the unfctrack tracker on the MOT17 dataset. 

\subsection{Limitations}

Unfctrack heavily relies on appearance feature information for the detection and recovery of ID-switch situations. However, excessive ReID feature extraction in dense scenes can be time-consuming, potentially compromising real-time performance. Additionally, the effectiveness of ReID feature extraction significantly affects the performance of IDSD and IDSR. Factors such as camera movement and significant environmental background variations may cause certain parameters of the unfctrack tracker to change. For example, the threshold value $T_\theta$ for the performance metric $T_{spec}$ in IDSD may differ across different environments. For instance, $T_\theta$ = 0.01 may be suitable for MOT17-01, while $T_\theta$ = 0.05 may be more appropriate for MOT17-03. Adjustments may be necessary for $T_\theta$ in different environments. Finally, although Unfctrack demonstrates strong capabilities in detecting ID-switch situations, the correction and recovery of IDs still present significant challenges, which will be a focus of future research efforts.



\section{CONCLUSIONS}

In this paper, we have proposed a multi-object tracking method that utilizes unfalsified control to identify and attempt to rectify ID-switch situations. By leveraging data-driven unfalsified control, our tracker can dynamically identify and rectify errors during the tracking process. To the best of our knowledge, this is the first tracker that focuses on detecting and attempting to correct ID-switch occurrences. The proposed method, which incorporates appearance information, can be easily integrated into other tracking frameworks. We hope that this work provides a new perspective for addressing ID-switch problems and contributes to the advancement of the field of multi-object tracking. 


\begin{thebibliography}{99}
\bibitem{ref1}
Wang Y, Kitani K, Weng X. Joint object detection and multi-object tracking with graph neural networks[C]//2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021: 13708-13715.
\bibitem{ref2}
Aharon N, Orfaig R, Bobrovsky B Z. BoT-SORT: Robust associations multi-pedestrian tracking[J]. arXiv preprint arXiv:2206.14651, 2022.
\bibitem{ref3}
Zhang Y, Sun P, Jiang Y, et al. Bytetrack: Multi-object tracking by associating every detection box[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022: 1-21.
\bibitem{ref4}
Yang F, Odashima S, Masui S, et al. Hard to track objects with irregular motions and similar appearances? make it easier by buffering the matching space[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023: 4799-4808.
\bibitem{ref5}
Cao J, Pang J, Weng X, et al. Observation-centric sort: Rethinking sort for robust multi-object tracking[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 9686-9696.
\bibitem{ref6}
Maggiolino G, Ahmad A, Cao J, et al. Deep oc-sort: Multi-pedestrian tracking by adaptive re-identification[J]. arXiv preprint arXiv:2302.11813, 2023.
\bibitem{ref7}
Wojke N, Bewley A, Paulus D. Simple online and realtime tracking with a deep association metric[C]//2017 IEEE international conference on image processing (ICIP). IEEE, 2017: 3645-3649.
\bibitem{ref8}
 Zeng F, Dong B, Zhang Y, et al. Motr: End-to-end multiple-object tracking with transformer[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022: 659-675.
\bibitem{ref9}
Ren H, Han S, Ding H, et al. Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 11289-11298.
\bibitem{ref10}
Stadler D, Beyerer J. Modelling ambiguous assignments for multi-person tracking in crowds[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2022: 133-142.
\bibitem{ref11}
Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu, “Fairmot: On the fairness of detection and re-identifification in multiple object tracking,” IJCV, vol. 129, no. 11, pp. 3069–3087, 2021. 
\bibitem{ref12}
Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell, and Fisher Yu, “Quasi-dense similarity learning for multiple object tracking,” in CVPR, 2021, pp. 164–173. 
\bibitem{ref13}
Jinkun Cao, Hao Wu, and Kris Kitani, “Track targets by dense spatio-temporal position encoding,” arXiv preprint arXiv:2210.09455, 2022.
\bibitem{ref14}
Milan A, Leal-Taixé L, Reid I, et al. MOT16: A benchmark for multi-object tracking[J]. arXiv preprint arXiv:1603.00831, 2016.
\bibitem{ref15}
 Dendorfer P, Rezatofighi H, Milan A, et al. Mot20: A benchmark for multi object tracking in crowded scenes[J]. arXiv preprint arXiv:2003.09003, 2020.
\bibitem{ref16}
Qin Z, Zhou S, Wang L, et al. MotionTrack: Learning Robust Short-term and Long-term Motions for Multi-Object Tracking[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 17939-17948.
\bibitem{ref17}
 Bewley A, Ge Z, Ott L, et al. Simple online and realtime tracking[C]//2016 IEEE international conference on image processing (ICIP). IEEE, 2016: 3464-3468.
\bibitem{ref18}
Meinhardt T, Kirillov A, Leal-Taixe L, et al. Trackformer: Multi-object tracking with transformers[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 8844-8854.
\bibitem{ref19}
Sun P, Cao J, Jiang Y, et al. Transtrack: Multiple object tracking with transformer[J]. arXiv preprint arXiv:2012.15460, 2020.
\bibitem{ref20}
He L, Liao X, Liu W, et al. Fastreid: a pytorch toolbox for real-world person re-identification[J]. arXiv preprint arXiv:2006.02631, 2020, 1(7): 6.
\bibitem{ref21}
Wang G, Gong S, Cheng J, et al. Faster person re-identification[C]//European conference on computer vision. Cham: Springer International Publishing, 2020: 275-292.
\bibitem{ref22}
Ahmed E, Jones M, Marks T K. An improved deep learning architecture for person re-identification[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3908-3916.
\bibitem{ref23}
 Kalman R E. Contributions to the theory of optimal control[J]. Bol. soc. mat. mexicana, 1960, 5(2): 102-119.
\bibitem{ref24}
Djuric P M, Kotecha J H, Zhang J, et al. Particle filtering[J]. IEEE signal processing magazine, 2003, 20(5): 19-38.
\bibitem{ref25}
Kuhn H W. The Hungarian method for the assignment problem[J]. Naval research logistics quarterly, 1955, 2(1‐2): 83-97.
\bibitem{ref26}
Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.
\bibitem{ref27}
Carion N, Massa F, Synnaeve G, et al. End-to-end object detection with transformers[C]//European conference on computer vision. Cham: Springer International Publishing, 2020: 213-229.
\bibitem{ref28}
Safonov M G, Tsao T C. The unfalsified control concept and learning[C]//Proceedings of 1994 33rd IEEE conference on decision and control. IEEE, 1994, 3: 2819-2824.
\bibitem{ref29}
Luo H, Gu Y, Liao X, et al. Bag of tricks and a strong baseline for deep person re-identification[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. 2019: 0-0.
\bibitem{ref30}
Pang B, Li Y, Zhang Y, et al. Tubetk: Adopting tubes to track multi-object in a one-step training model[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020: 6308-6318.
\bibitem{ref31}
Danelljan M, Bhat G, Khan F S, et al. Atom: Accurate tracking by overlap maximization[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 4660-4669.
\bibitem{ref32}
Mostafa R, Baraka H, Bayoumi A E M. LMOT: efficient light-weight detection and tracking in crowds[J]. IEEE Access, 2022, 10: 83085-83095.
\bibitem{ref33}
Stadler D, Beyerer J. On the performance of crowd-specific detectors in multi-pedestrian tracking[C]//2021 17th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS). IEEE, 2021: 1-12.
\bibitem{ref34}
Galor A, Orfaig R, Bobrovsky B Z. Strong-transcenter: improved multi-object tracking based on transformers with dense representations[J]. arXiv preprint arXiv:2210.13570, 2022.
\bibitem{ref35}
Girbau A, Marqués F, Satoh S. Multiple Object Tracking from appearance by hierarchically clustering tracklets[J]. arXiv preprint arXiv:2210.03355, 2022.
\bibitem{ref36}
Bernardin K, Stiefelhagen R. Evaluating multiple object tracking performance: the clear mot metrics[J]. EURASIP Journal on Image and Video Processing, 2008, 2008: 1-10.
\bibitem{ref37}
Ge Z, Liu S, Wang F, et al. Yolox: Exceeding yolo series in 2021[J]. arXiv preprint arXiv:2107.08430, 2021.
\bibitem{ref38}
Liu C, Gong S, Loy C C, et al. Person re-identification: What features are important?[C]//Computer Vision–ECCV 2012. Workshops and Demonstrations: Florence, Italy, October 7-13, 2012, Proceedings, Part I 12. Springer Berlin Heidelberg, 2012: 391-401.
\bibitem{ref39}
Yi D, Lei Z, Liao S, et al. Deep metric learning for person re-identification[C]//2014 22nd international conference on pattern recognition. IEEE, 2014: 34-39.
\bibitem{ref40}
Li W, Zhao R, Xiao T, et al. Deepreid: Deep filter pairing neural network for person re-identification[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2014: 152-159.
\bibitem{ref41}
Zhou X, Koltun V, Krähenbühl P. Tracking objects as points[C]//European conference on computer vision. Cham: Springer International Publishing, 2020: 474-490.

\end{thebibliography}

\end{document}
