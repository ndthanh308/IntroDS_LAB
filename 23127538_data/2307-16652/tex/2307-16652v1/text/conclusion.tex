\section{Conclusion}
This paper presents several sequential and shared-memory parallel algorithms for PaLD \cite{pald_pnas22}.
We prove that sequential variants are communication-optimal, up to constant factors.
We illustrate that branch avoidance is critical to attaining high performance; achieving a speedup of up to $29\times$ over naive sequential variants.
Based on our theoretical and empirical studies, we conclude that the triplet variant is the faster sequential algorithm for large matrices due to less computation.
However, we show that the pairwise algorithm is more amenable to parallelization due to regular data dependencies and load balance.
We observe strong scaling speedups  up to $19.4\times$ ($60.5\%$ efficiency), and weak scaling efficiencies of up to $65.6\%$ at $p = 32$ after incorporating NUMA-aware optimizations.
 With the performance achieved on the text analysis application, we show that PaLD can be scaled to nearly any dataset with a distance matrix that fits in the memory of a single server.


\paragraph{Acknowledgements.} We would like to thank Kenneth S. Berenhaut for helpful feedback on the presentation of PaLD and discussions on applying PaLD to semantic analysis of word embedding in \Cref{sec:application}. We would also like to thank Yixin Zhang for code contributions to preliminary versions of the pairwise algorithms. This work is supported by the National Science Foundation under Grant No. OAC-2106920 and the U.S. Department of Energy, Office of Science, Advanced Scientific Computing Research program under Award Number DE-SC-0023296.