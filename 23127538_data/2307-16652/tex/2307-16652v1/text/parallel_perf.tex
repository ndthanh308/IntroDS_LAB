\section{Shared-Memory Parallel Algorithms}
\label{sec:par-algs}
This section presents the OpenMP parallelization of the optimized sequential pairwise and triplet algorithms.
\Cref{fig:omp_pairwise} shows the OpenMP version of the blocked pairwise algorithm.
The blocked pairwise algorithm first computes $U_{\mathcal{X},\mathcal{Y}}$ with a pass over all $n$ points $z$.
The local focus $z$-loop can be parallelized across $p$ threads using the OpenMP \verb|parallel for| construct.
All threads must write to $U_{\mathcal{X},\mathcal{Y}}$ so a sum-reduction is required to resolve write conflicts.
The cohesion update pass requires the quantities $1/u_{xy}~\forall~(x,y)~\in~\mathcal{X}{\times}\mathcal{Y}$, which can be parallelized without write conflicts.
Cohesion updates are within each column of $C$ to entries of $C_{\mathcal{X},z}$ and $C_{\mathcal{Y},z}$.
The cohesion pass can be parallelized without write conflicts by splitting the $z$-loop across $p$ threads.
\Cref{fig:pairwise_writes} illustrates the write patterns for optimized OpenMP pairwise for $n = 16$, $b = 4$, and $p = 8$.
Updates to entries of $C$ requires corresponding entries from $D$, so $D$ can also be partitioned column-wise.
The pairwise algorithm is amenable to NUMA optimizations due to the regular data dependencies.

\Cref{fig:omp_triplet} shows the OpenMP version of the blocked triplet algorithm. The triplet approach requires reading all of $D$ for local focus and cohesion update passes.
Blocking is performed over triplets of points, $\mathcal{X},\mathcal{Y},\mathcal{Z}$, and updates to $U$ and $C$ become irregular.
We use the OpenMP tasking model \cite{openmp-spec} for parallelism.
Each triplet block, $\mathcal{X}\times\mathcal{Y}\times\mathcal{Z}$, is a new task that can be executed by any available thread.
Tasks in the local focus pass write to $3$ blocks of $U$. $C$ is not symmetric, so the cohesion update pass writes to $6$ blocks.
Write conflicts arise when multiple tasks need to update the same blocks of $U$ or $C$.
We resolve conflicts by annotating dependencies using the \verb|depend| clause with the \verb|inout| modifier.
\Cref{fig:triplet-tasks} shows the write conflicts for the local focus pass.
Each vertex represents one of the $\binom{n/b + 2}{3}$ tasks and is labeled by $\mathcal{X},\mathcal{Y},\mathcal{Z}$ block values, and edges represent conflicts.
The degree for each vertex varies based on the symmetry in the block.
This leads to irregular dependencies which we will show in \cref{sec:omp-perf} are not as amenable to NUMA optimizations.

% Figure environment removed

% Figure environment removed
% Figure environment removed
% Figure environment removed

\subsection{OpenMP Performance.}
\label{sec:omp-perf}
We use OpenMP version 4.5 and test the OpenMP algorithms on randomly generated dense distance matrices with $n \in \{2048, 4096, 8192\}$. We incorporate NUMA optimizations into the pairwise algorithm by controlling thread affinity via the \verb|OMP_PROC_BIND| and \verb|OMP_PLACES| environment variables. We map OpenMP threads to physical cores, by assigning OpenMP thread ids $0$ to $16$ to CPU $0$ and threads $17$ to $31$ to CPU $1$.
% Figure environment removed
A static loop schedule yields best performance due to the pairwise algorithm's regular dependencies.
Each thread reads columns of $D$ and $C$ from thread-local fast memory so updates to $C$ are spatially local.
Thread binding ensures that accesses are temporally local by assigning fixed column blocks of $D$/$C$ to threads.
OpenMP allocates memory pages using a first-touch policy by default.
If a single thread allocates $D$, then $D$ resides in the memory hierarchy of the thread's CPU.
$D$ is typically computed outside the scope of the OpenMP algorithms, so we also study the effects of partitioning $D$ across sockets (i.e. memory binding).

\Cref{fig:numa} shows the speedup achieved by introducing thread binding only and thread + memory binding into the OpenMP pairwise algorithm across three matrix sizes, $n \in \{2048, 4096, 8192\}$.
We use the OpenMP pairwise algorithm without NUMA-aware optimizations as our baseline and report speedups for $32$ OpenMP threads.
When we use thread binding only, we observe average speedups of $1.4\times, 1.5\times,$ and $1.13\times$ for $n = 2048, 4098,$ and $8192$, respectively.
Thread binding with memory binding yields average speedups of speedup of $1.7\times, 1.69\times,$ and $1.2\times$ over the baseline.
We did not perform TLB optimizations, therefore, we observe decreasing speedups for large matrix sizes.
We also found that NUMA optimizations are useful at smaller thread counts, $2 \leq p \leq 16$, by mapping half the threads to CPU $0$ and the other half to CPU $1$.
This mapping provides access to the fast memory hierarchies on both CPUs.
We observe speedups ranging from $1.05\times$ ($n = 4096, p = 2$) to $1.33\times$ ($n = 2048, p = 16$) when splitting threads (where $p \leq 16$) across sockets. %to a baseline of mapping all threads to one CPU.
We experimented with thread binding for the OpenMP triplet algorithm but not memory binding due to the irregular data dependencies.
However, we did not observe significant performance improvements over the baseline, so we omit these results from \cref{fig:numa}.
We obtain best OpenMP scaling when using the \verb|untied| clause, which allows suspended tasks to be resumed on any available thread.
Suspended tasks may cause additional reads from slow memory after restart.
Hence, we do not expect NUMA optimizations to be helpful. %for the OpenMP triplet algorithm without a deterministic task schedule.
% Figure environment removed %
We perform strong scaling experiments in \cref{fig:strong} of the OpenMP variants under the same settings as for \cref{fig:numa} and report self-relative efficiency achieved.
We report efficiencies with and without NUMA optimizations. %Since our experimental platform is a dual-socket system we report the effects of NUMA optimizations across the entire range of $p$ in order to utilize the cache hierarchies on both CPUs.
The pairwise algorithm without NUMA optimizations achieves efficiencies of $24.2\%, 33.5\%,$ and $50.6\%$ at $p = 32$ for $n = 2048, 4096$ and $8192$, respectively.
Including NUMA optimizations yields efficiencies of $42.9\%, 56.6\%,$ and $60.5\%$ for $p = 32$.
The triplet algorithm achieves efficiencies of $28.0\%, 29.2\%,$ and  $40.9\%$ without NUMA optimizations and $36.9\%, 34.9\%$, and $41.2\%$ with NUMA optimizations for $p = 32$.
The triplet algorithm is the faster sequential baseline, hence the OpenMP triplet efficiencies are lower than those reported for OpenMP pairwise. % We focus on self-relative scaling behavior in this section.
We also study weak scaling of the two algorithms with and without NUMA optimizations.
We fix $n^3/p$ over the range of $p$ tested.
We use the matrix sizes $n_1 \in \{2048, 4096, 8192\}$, where $n_1$ is the matrix size at $p = 1$.
\Cref{fig:weak} shows the results of the weak scaling experiments.
The pairwise algorithm without NUMA optimizations attains weak scaling efficiencies of $30.6\%, 48.2\%$, and $61.4\%$ for $n_1 = 2048, 4096,$ and $8192$, respectively at $32$ threads.
With NUMA optimizations, the efficiencies increase to $59.1\%,~ 63.6\%$, and $65.6\%$ for each of the matrix size settings at $p = 32$.
Triplet without NUMA optimizations achieves weak scaling efficiencies of $44.2\%, 49.1\%,$ and $50.1\%$ and $47.6\%, 49.1\%,$ and $50.1\%$ with NUMA optimizations at $p = 32$. %for $n^3/p = 2048, 4096,$ and $8192$, respectively at $32$ threads. With NUMA optimizations included, we observed efficiencies of $47.6\%, 49.2\%,$ and $50.1\%$ for the same settings.
% Figure environment removed