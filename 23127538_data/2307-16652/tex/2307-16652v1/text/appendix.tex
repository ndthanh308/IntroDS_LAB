\section{Percentage of Hardware Peak}\label{sec:pct-peak}
\subsection{Pairwise Algorithm.}
This section details the operations counting for the optimized sequential pairwise and triplet algorithms used for percentage of peak calculations in \Cref{sec:seqexp}.
Unless otherwise specified, all operands are assumed to be in $32$-bit floating point format.
The optimized sequential pairwise algorithm requires $2$ comparisons during local focus update to determine if a point, $z$, is in the neighborhood.
The local focus matrix is incremented based on these comparisons.
However, since $U$ is stored in integer format, we ignore the cost of integer increments during the local focus pass.
The cohesion update requires $3$ comparisons: $2$ comparisons to compute mask $r$ which determines if a point $z$ is in the local focus and $1$ comparison to compute mask $s$ which determines the column entry of $C$ to update.
Since results of floating point comparisons are stored in unsigned (integer) format, $r$ and $s$ must be cast to $32$-bit floats before FMAs.
This requires $2$ unsigned int to floating point cast operations.
Finally, $2$ FMAs (each FMA requires two instructions) can be used to update $c_{xz}$ and $c_{yz}$.
We explicitly compute both entries and accumulate with an explicit zero, as this avoids branching.
Based on these operations, the total number of operations for sequential pairwise can be computed as follows:
\begin{align*}
    F = \left(5 \gamma_{cmp} + 2\cdot 2\gamma_{fma} + 2\gamma_{cast}\right)\cdot n \binom{n}{2}
\end{align*}
On our Intel Xeon Gold 6226R CPU, floating point comparisons have a CPI of $1$ whereas FMAs and casting each have a CPI of $0.5$.
Since comparisons are twice as expensive, we normalize our operation count to be relative to FMA/cast.
After normalization, the total number of operations become:
\begin{align*}
    F = 16\gamma\cdot n \binom{n}{2} \approx 8 n^3~\text{ops}.
\end{align*}
Finally, percentage of peak can be calculated by:
\begin{align}\label{eq:pct-peak}
    \frac{1}{249.6}\cdot \frac{F}{10^9 \cdot t_n}
\end{align}
where $t_n$ is the runtime time (in seconds) obtained empirically from executing the optimized sequential pairwise algorithm on a matrix of size $n$ and $249.6$ Gflops/sec is the single precision, single core machine peak of our Intel CPU.
The setting $n = 2048$ and $t_n = 0.99422~\text{seconds}$ (averaged over $5$ trials) yields $27.7\%$ of peak, as reported in \Cref{sec:seqexp}.
\subsection{Triplet Algorithm.}
The optimized sequential triplet algorithm makes two passes: one to compute $U$ in its entirety and one to compute $C$.
The triplet algorithm, which ignores ties, requires $6$ comparisons across the two passes to uniquely determine the pair of points in a triplet with minimum pairwise distance.
The local focus pass and cohesion pass must compute these distances.
Once again, we ignore integer increments in the local focus pass.
The remaining instructions are $3$ casting and $6$ FMA operations to update entries of $C$.
\begin{align*}
        F = \left(12 \gamma_{cmp} + 2\cdot 6\gamma_{fma} + 3\gamma_{cast}\right)\cdot& \binom{n}{3} \approx 6.5 n^3.
\end{align*}
Setting $F = 6.5n^3, n = 8192$ and $t_n = 51.15952$ seconds in \eqref{eq:pct-peak} yields $28\%$ of peak, as reported in \Cref{sec:seqexp}.

\section{Runtime Breakdown}
 % Figure environment removed

\Cref{fig:rt} shows the running time breakdown of the pairwise and triplet algorithms, grouped by the algorithm.
We report the fraction of time taken to compute the local focus, cohesion update, and memory overhead (e.g. memcpy into explicit cache blocks).
The OpenMP pairwise algorithm requires a reduction during the local focus computation.
As $p$ increases, we see that the local focus computation becomes a barrier to scalability.
The OpenMP triplet algorithm however does not require explicit synchronization.
This results in better scalability for the local focus computation as $p$ increases.
The cohesion update, however, does not scale efficiently since it updates up to $6$ unique blocks of $C$.
Given the irregular task dependencies (see \cref{fig:triplet-tasks}), the OpenMP triplet algorithm was not amenable to NUMA optimizations.
% This
In addition, variance in task costs due to symmetries and a non-static task schedule were barriers to efficiently scaling the cohesion update pass.
Memory overhead is a negligible fraction of runtime for pairwise and triplet algorithms and does not increase with $p$.
At $32$ threads, the pairwise algorithm is faster than triplet algorithm for cohesion update.
The reverse is true for the local focus update.
This behavior may indicate that the two algorithms can be combined by utilizing the triplet approach for local focus update and the pairwise approach for cohesion update for additional speedup.

\section{Scaling on SNAP Datasets.}
\begin{table}[t]
    \centering
    \begin{tabular}{c|c|c|c}
        Dataset & $n$ & sequential & $p = 32$\\\hline\hline
        ca-GrQc & 5242 & 21.69 &\bf 1.390 (15.6$\times$)\\\hline
        ca-HepPh & 12008 & 259.9 & \bf 13.16 (19.7$\times$)\\\hline
        ca-CondMat & 23133 & 1913 &\bf 91.89 (20.8$\times$)\\\hline
    \end{tabular}
    \caption{Pairwise runtimes (in sec.) and maximum speedup over pairwise sequential on SNAP datasets.}
    \label{tab:snapscaling}
\end{table}
We perform scaling experiments on large datasets obtained from the SNAP data repository \cite{snapnets} to illustrate PaLD scalability on collaboration networks.
We obtain distance matrices by computing all-pairs shortest path distances.
\Cref{tab:snapscaling} reports the running times (in seconds) and speedup achieved at $p = 32$ for the pairwise algorithm.
We use the optimized sequential pairwise algorithm as our baseline.
We achieve speedups of $15.6\times$, $19.7\times$ and $20.8\times$ on the ca-GrQC, ca-HepPh, and ca-CondMat datasets \cite{lkf07-tkdd}, respectively.
For the largest dataset, ca-CondMat, we are able to reduce the running time of computing $C$ from $31$ minutes (optimized pairwise sequential) to $92$ seconds (OpenMP pairwise with $p = 32$).
