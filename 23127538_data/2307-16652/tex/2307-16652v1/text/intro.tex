\section{Introduction.}
Partitioned local depths (PaLD) is a method for revealing community structure in distance-based data \cite{pald_pnas22}.
Given pairwise distances (or dissimilarities) of a set of points, PaLD computes another pairwise measure called cohesion that measures closeness based on relative distances.
By relying on relative distance, PaLD is able to use a universal threshold to distinguish between strong and weak ties without defining neighborhoods by a single number of neighborhoods, neighborhood size, or absolute distance threshold.
In this way, PaLD can identify neighborhoods of varying size and density, making it useful for data where the relationships among points behave differently across the space.

The input to PaLD is a distance matrix, and the output is a cohesion matrix.
As detailed in \cref{sec:back}, computing cohesion requires determining the size of the local neighborhood of each pair of points and then computing contributions to cohesion values based on neighborhood sizes.
In each case, the fundamental operation is a comparison of the pairwise distances among triplets of points.
Given $n$ points, this yields an arithmetic complexity of $O(n^3)$.
The goal of this paper is to develop efficient sequential and shared-memory parallel algorithms for scaling PaLD to datasets of size up to $O(10^5)$, making it computationally feasible to analyze ones that fit in memory on a single server.
\Cref{sec:alg-design} presents the structure of the PaLD computation and our two main algorithmic approaches, which we call pairwise and triplet, respectively.
As an $O(n^3)$ computation, PaLD shares many similarities with dense matrix multiplication (GEMM), and our algorithmic design borrows from ideas of cache-efficient algorithms for GEMM \cite{bacd-97-ics,GG_toms08,SLLvdG19}.
For example, the basic computation is a comparison between distances of points $x,y,z$, which involves distance matrix entries $d_{xy}$, $d_{yz}$, and $d_{xz}$ and has an access pattern similar to the fused multiply-adds (FMA) within GEMM.
There are a few key differences between PaLD and GEMM.
First, because of symmetric distances, the order of the points is irrelevant, so rather than requiring consideration of all $n^3$ possible values of $x,y,z$, we need consider only $\binom{n}{3}\approx n^3/6$ unique triplets.
Second, while the memory access of distances is regular, the updates of the cohesion requires branching based on distance comparisons.
Finally, the computation requires two passes because cohesion updates depend on the sizes of local neighborhoods.
Each pass requires a varying mix of integer and floating point operations in addition to the branching.
The pairwise and triplet approaches navigate a tradeoff between exploiting symmetry and achieving regular data access and parallelization.

In \cref{sec:analysis} we prove a lower bound on the cache efficiency of any PaLD algorithm, and we show that both of our algorithms achieve optimal cache performance, up to constant factors.
By exploiting symmetry and applying cache blocking, we obtain data locality in cache and minimize the number of reads and writes of matrix values. %distance values and writes of cohesion matrix values.
\Cref{sec:seqexp} details our low-level optimizations of the two PaLD algorithms. We show that branch avoidance has the highest impact on sequential performance given the high cost of branch misprediction \cite{hp-comparch,j88-sigarch,jw89-asplos}.
Along with other optimizations including cache blocking and vectorization, we show performance improvements over naive sequential code of up to $29\times$.
In \cref{sec:par-algs} we design, optimize, and evaluate OpenMP parallel versions of the two PaLD algorithms.
We show that the pairwise algorithm enables regular data access patterns and loop-based parallelism that can largely avoid write conflicts.
The triplet algorithm exploits more symmetry to reduce arithmetic operations but requires task-based parallelism due to more complicated data access patterns and write conflicts.
We also apply Non-Uniform Memory Access (NUMA) optimizations when scaling across sockets. % that exploit the data locality.
We achieve strong scaling speedups up to $19.4\times$ for pairwise and $13.2\times$ for triplet over their optimized sequential versions on $32$ threads.
Finally, we describe a text analysis application in \cref{sec:application}, demonstrating the utility of PaLD on larger datasets than previously considered, and we show a parallel speedup of $16.7\times$ on a task with $n=2712$ using $32$ threads.
