\section{Sequential Algorithm Analysis.}
\label{sec:analysis}
We model performance using the model, $\gamma F + \beta W$, where $F$ and $W$ represent an algorithm's computation and bandwidth costs, respectively, and $\gamma$ (time per operation) and $\beta$ (time per word moved) represent hardware parameters.
We analyze communication cost assuming a two-level memory hierarchy, which contains fast memory (cache) of size $M$ words and slow memory (DRAM) with unbounded size.
We assume that computation can only be performed on operands residing in fast memory.
If operands are in slow memory, then they must first be read into fast memory.
We limit analysis in this section to a two-level memory hierarchy, but this memory model can be used to analyze communication for each adjacent pair of levels in a multi-level memory hierarchy.
\subsection{Communication Lower Bounds.}

We use the framework in \cite{BCD+_actanumerica14} to derive communication lower bounds.
The lower bound of \cite[Theorem 2.6]{BCD+_actanumerica14} applies to all three-nested-loops (3NL) computations as defined in that paper.
We reproduce the 3NL definition here using the same notation, with sets $S_a, S_b, S_c \subseteq [n] \times [n]$ where $[n] = \{1,2,\dots,n\}$ and mappings $\mathbf{a}:S_a \rightarrow \cal{M}$, $\mathbf{b}:S_b \rightarrow \cal{M}$, $\mathbf{c}:S_c \rightarrow \cal{M}$, where $\cal{M}$ is slow memory.
For each $(i,j)\in S_c$, we also have a set $S_{ij} \subseteq [n]$.

\begin{definition}[{\cite[Definition 2.4]{BCD+_actanumerica14}}]
A computation is considered to be three-nested-loops (3NL) if it includes computing, for all $(i,j)\in S_c$ with $S_{ij}$,
\begin{displaymath}
    \text{Mem}(\mathbf{c}(i,j)) = f_{ij}(\{g_{ijk}(\text{Mem}(\mathbf{a}(i,k)),\text{Mem}(\mathbf{b}(k,j))\}_{k\in S_{ij}}),
\end{displaymath}
where
(a) mappings $\mathbf{a}$, $\mathbf{b}$, $\mathbf{c}$ are all one-to-one into slow memory, and (b) functions $f_{ij}$ and $g_{ijk}$ depend nontrivially on their arguments.
\end{definition}


We first verify that the cohesion matrix computation defined by \cref{eq:fxz,eq:gxyz} is 3NL when the distance matrix is stored explicitly in memory.
To satisfy the first constraint, we define the mappings $\mathbf{a}$, $\mathbf{b}$, and $\mathbf{c}$ as all mapping onto the distance matrix (that is, each mapping is one-to-one but the three mappings are not disjoint).
Here $\mathbf{a}(x,y)$ maps to the distance matrix entry $d_{xy}$.
To satisfy the second constraint, we see that computing $g_{xyz}$ depends nontrivially on $\mathbf{a}(x,y)$ and $\mathbf{b}(y,z)$, as both values must be compared with $d_{xz}$ to evaluate the indicator functions, and computing $c_{xz}$ depends nontrivially on its arguments, as it computes the sum over all values.
As argued in \cref{sec:alg-design}, the number of 3NL operations is $\sum_{i,j} |S_{ij}| = O(n^3)$.
Then, by \cite[Theorem 2.6]{BCD+_actanumerica14}, the bandwidth cost lower bound for PaLD is $W=\Omega(n^3/\sqrt M)$.

\subsection{Cost Analysis.}\label{sec:seqcost}

The blocked algorithms are described in \cref{sec:alg-design} with memory reference patterns depicted in \cref{fig:pairwise_dependency,fig:triplet_dependency}.
The loop structures of the blocked algorithms are shown (with OpenMP parallelization) in \cref{fig:omp_pairwise,fig:omp_triplet}.
We focus on the sequential costs in this section and discuss parallelization in \cref{sec:par-algs}.
Since the algorithms require mixed comparison and arithmetic instructions, we explicitly define the hardware parameters $\gamma_{cmp}$ and $\gamma_{fma}$ to represent the time per instruction for floating-point comparisons and FMAs, respectively.
We ignore the cost of integer arithmetic.
\Cref{fig:omp_pairwise} shows the loop structure of the blocked pairwise algorithm where inner loop computations match \cref{alg:pairwise}.
We use $b$ to represent the block size for the pairwise algorithm.

% ================================================================
% Operation, Instruction, Latency, and CPIs for PaLD instructions.
% ================================================================
% UInt32 Multiply, vpmuludq, 5, 0.5
% UInt32 Addition, vpaddd, 1, 0.5
% float FMA, vfmadd132ps, 4, 0.5
% float Compare, vcmpps, 3, 1
% UInt32 to float,vcvtudq2ps, 4, 0.5
% ================================================================
\begin{theorem}\label{thm:pairwise}
The blocked pairwise algorithm has the leading order computation and communication costs:% as follows:
    \begin{align*}
        F &= (5\gamma_{cmp} + 1\gamma_{fma})\cdot n\binom{n}{2} \approx 3n^3~\text{flops.}\\
        W &= 4\sqrt{2}~\frac{n^3}{\sqrt{M}} \approx 5.7~\frac{n^3}{\sqrt{M}}~\text{words moved.}
    \end{align*}
\end{theorem}
\begin{proof}

    The blocked pairwise algorithm selects $\binom{n/b+1}{2}$ unique sets of points $\mathcal{X}, \mathcal{Y}$ with $|\mathcal{X}| = |\mathcal{Y}| = b$.
    A total of $nb^2$ iterations are required to determine if a third point, $z$, is in the local focus for each $(x,y) \in \mathcal{X} \times \mathcal{Y}$.
    The local focus update requires $2$ floating-point comparisons followed by $1$ integer accumulate into $u_{xy}$.
    The cohesion update requires $3$ floating-point comparisons and $1$ FMA, as the reciprocals of elements of $U_{\mathcal{X},\mathcal{Y}}$ can be pre-computed once.
    When $\mathcal{X} = \mathcal{Y}$, only $n\binom{b}{2}$ iterations are required to perform local focus and cohesion updates.
    There are $n/b$ such overlapping sets.
    Multiplying over the iterations, summing the work over the local focus and cohesion update loops, and multiplying by $\gamma_{cmp}$ and $\gamma_{fma}$ yields the computation cost.

    Each of the $\binom{n/b + 1}{2}$ possible combinations of $\mathcal{X} \times \mathcal{Y}$ points requires reading the $b\times b$ block $D_{\mathcal{X},\mathcal{Y}}$ from slow memory.
    In the first pass to compute the local focus sizes, for each third point, $z$, we read the two $b \times 1$ vectors $D_{\mathcal{X},z}$ and $D_{\mathcal{Y},z}$ from slow memory.
    The local focus block $U_{\mathcal{X}, \mathcal{Y}}$ is computed and remains resident in fast memory.
    Similarly, each iteration of the second pass cohesion update requires reading the $b \times 1$ vectors $D_{\mathcal{X},z}, D_{\mathcal{Y},z}, C_{\mathcal{X},z}$ and $C_{\mathcal{Y},z}$ from slow memory.
    After each iteration within the second pass, $C_{\mathcal{X},z}$ and $C_{\mathcal{Y},z}$ must be written to slow memory.
    We must maintain $2 b^2$ words of data in fast memory for $D_{\mathcal{X},\mathcal{Y}}$ and $U_{\mathcal{X},\mathcal{Y}}$, along with a constant number of length-$b$ vectors, so $b \leq \sqrt{{M}/2}$ to leading order.
    Multiplying and summing these reads and writes over all iterations yields the leading order communication cost $4 n^3/b$, and choosing $b \approx \sqrt{{M}/2}$ yields the result.
\end{proof}

%\paragraph{Triplet Algorithm.}
\Cref{fig:omp_triplet} shows the loop structure of the blocked triplet algorithm, and the inner loop computations match \cref{alg:triplet}.
The local focus sizes and cohesion matrix updates are computed in two separate passes, and two block sizes $\hat b$ and $\tilde b$ can be tuned independently.

\begin{theorem}\label{thm:triplet}
The blocked triplet algorithm has the leading order computation and communication costs:% as follows:
    \begin{align*}
        F &= (6\gamma_{cmp} + 2\gamma_{fma})\cdot \binom{n}{3} \approx 1.33n^3~\text{flops.}\\
        W &= \left(\sqrt{6} + 4\sqrt{3}\right)\frac{n^3}{\sqrt{M}}\approx 9.4 \frac{n^3}{\sqrt{M}}~\text{words moved.}
    \end{align*}
\end{theorem}
\begin{proof}
    The blocked local focus and cohesion matrix passes have the same loop structure, each selecting $\binom{n/b + 2}{3}$ triplets of sets $\mathcal{X}, \mathcal{Y},$ and $\mathcal{Z}$ each of size $b$ points, though the value of $b$ differs in the two passes.
    The triplet algorithm contains $3$ types of symmetry: $\mathcal{X} = \mathcal{Y} = \mathcal{Z}$, $\mathcal{X} \neq \mathcal{Y} = \mathcal{Z}$, and $\mathcal{X} = \mathcal{Y} \neq \mathcal{Z}$.
    While our implementation accounts for each type of symmetry, we ignore it in our leading order cost analysis.
    The local focus and cohesion update inner iterations each require $3$ distance comparisons to determine the pair of points with minimum distance.
    The cohesion update iteration additionally requires $2$ FMAs to update entries of the cohesion matrix.
    Multiplying operations by their respective $\gamma$ terms and summing work over the two passes proves the computation cost. %, which is independent of the block sizes.

    There are $\binom{n/\hat{b} + 2}{3}$ possible combinations of triplet blocks in the local focus pass. %$\mathcal{X} \times \mathcal{Y} \times \mathcal{Z}$ including overlaps. % in the local focus update.
    The local focus update must read 2 $\hat{b} \times \hat{b}$ blocks of $D$, read 2 $\hat{b}\times \hat{b}$ blocks of $U$, and write 2 $\hat{b}\times \hat{b}$ blocks of $U$ from/to slow memory.
    Note that the block $D_{\mathcal{X},\mathcal{Y}}$ can be read and the block $U_{\mathcal{X},\mathcal{Y}}$ read and written only $\binom{n/\hat{b} + 1}{2}$ times since they remain fixed while blocks $\mathcal{Z}$ vary in the innermost loop.
    The cohesion update requires reading 2 $\tilde{b} \times \tilde{b}$ blocks of $D$ and $U$, respectively, followed by reading and writing 4 $\tilde{b} \times \tilde{b}$ blocks of $C$.
    The blocks $D_{\mathcal{X},\mathcal{Y}}$ and $U_{\mathcal{X},\mathcal{Y}}$ are read from slow memory and the blocks $C_{\mathcal{X},\mathcal{Y}}$ and $C_{\mathcal{Y},\mathcal{X}}$ can be read and written $\binom{n/\tilde{b} + 1}{2}$ times.
    The total I/O cost is then $n^3/\hat b + 2 n^3/\tilde b$, assuming that all blocks can be stored in fast memory.
    This requires that $\hat b \leq \sqrt{{M}/6}$ and $\tilde b \leq \sqrt{{M}/12}$ to leading order.
    Choosing block sizes at their approximate maximum value yields the communication cost.
\end{proof}
The constants for the communication cost in \Cref{thm:triplet} can be improved by unblocking the innermost loop over $\mathcal{Z}$ for the local focus and cohesion update passes, which allows for a slightly larger block size.
We use this technique for the pairwise algorithm, and it is useful in practice for matrix multiplication as well \cite{SLLvdG19}.
However, incorporating this optimization did not allow for auto-vectorization during cohesion updates where some updates require a stride of $n$.
Blocking all three loops allowed for unit-stride for all cohesion updates. We provide more details in the following section.

We can conclude from \Cref{thm:pairwise,thm:triplet} that the pairwise variant requires more computation than the triplet variant, but it moves less data.
Both sequential variants attain the 3NL lower bound of $\Omega({n^3}/{\sqrt{M}})$ and are communication-optimal within a constant factor.
We will show in the next section how additional performance optimizations can yield large speedups.
The optimized sequential algorithms serve as the baselines from which we derive efficient shared-memory parallel algorithms.