\section{A Robust Algorithm for Stationary \& Non-stationary Environments}
\label{sec:bobw}

%We should probably use  the words stochastic and adversarial (not just nonstationary) 

In this section, we present and analyze a new robust linear bandits BAI algorithm called \textsf{P1-RAGE}, which performs comparable to \textsf{G-BAI} in non-stationary environments but much better than it in stationary environments. We will show that it attains moderately good error probability in both stationary and non-stationary environments simultaneously without knowing a priori which environment it will encounter. We first discuss some intuitions behind the algorithm design.

\textbf{Stationary environments.} The development of our algorithm \textsf{P1-RAGE} is largely inspired by the high-level idea of the robust algorithm \textsf{P1}, proposed in \citet{abbasi2018best}, and the allocation strategy of \textsf{RAGE}, proposed in \citet{fiez2019sequential}. In particular, as discussed in \citet{abbasi2018best}, to minimize the error probability in stationary environment, we need to control the variance of the optimal arm well enough. Therefore, at each time step, we pull the current estimated best arm with the highest probability, then subsequently the second best arm with second highest probability and so on, just as what \textsf{P1} does. In particular, \textsf{P1} allocates the estimated best arm with unnormalized ``probability one'', estimated second best arm with ``probability half'' and so on. We can notice that it actually matches the allocation strategy of the successive halving algorithm in \citet{karnin2013almost}, which is proved to be near-optimal for BAI in stationary multi-armed bandits. Therefore, we design our probability allocation based on the allocation strategy of \textsf{RAGE}, which is proven to be near-optimal for fixed-confidence BAI in stationary linear bandits \citep{fiez2019sequential}. In particular, with the estimated parameter $\htheta_t$, we first find the estimated best arms $\hat{x}^*_t=\argmax_{x\in\X}x^\top\htheta_t$. Then, we repeatedly (virtually) eliminate arms with estimated gaps larger than certain threshold and compute $\mc{XY}$-allocation of the (virtually) remaining arms. Then, we average over the allocation probabilities computed during each iteration.


% As discussed in \citet{soare2014best}, for linear bandits best-arm identification problem, a more appropriate allocation strategy is $\mc{XY}$-allocation. The reason is that to correctly identify the best arm, we will essentially estimate the gap $\Delta_x$ for all $x\in\X$ instead of just $x^\top\theta^*$. Therefore, it will be more important to reduce the variance along the directions of $x_{(1)}-x$ for all $x\in\X$ instead of simply the directions of all $x\in\X$.

\textbf{Non-stationary environments.} Finally, to address the potential non-stationarity in environments, we uniformly mix the allocation probability computed above with a G-optimal design. With such a mixture, the variance over all arms can be controlled well and thus the algorithm will be robust for both stationary and non-stationary environments. The details of \textsf{P1-RAGE}, together with the subroutine to compute the allocation probability, called \textsf{RAGE-Elimination}, is summarized in Algorithm \ref{algo:p1_rage}.
\begin{algorithm}[ht]
    \caption{P1-RAGE}
    \label{algo:p1_rage}
    \SetAlgoLined
    \KwIn{budget, $T\in\mathbb{N}$; arm set $\mc{X}\subset\R^d$; maximum number of iterations, $m$}
    Compute G-optimal design $\lambda^*$ based on equation \eqref{equ:g_design} and initialize $\lambda_1=\lambda^*$\\
    \For{$t=1, 2, \dots, T$}{
        Sample $x_t\sim\lambda_t$ and receive reward $r_t$\\
        Estimate $\widehat{\theta}_t\leftarrow\frac{1}{t}\sum_{s=1}^t\E_{x\sim\lambda_s}\Mp{xx^\top}^{-1}x_s r_s$\\
        Update $\lambda_{t+1}\leftarrow$\textsf{RAGE-Elimination}$(\htheta_t, m)$
    }
    \textbf{return} $\argmax_{x\in\X}x^\top\widehat{\theta}_T$\\
    \SetKwFunction{proc}{\textsf{RAGE-Elimination}}
    \SetKwProg{myproc}{Subroutine}{}{}
    \myproc{\proc{$\htheta_t, m$}}{\label{algo:rage_elimination}
        Find $\hat{x}^*_t\leftarrow\argmax_{x\in\X}x^\top\htheta_t$\\
        Initialize $\X_t^{(0)}\leftarrow\X$ and $i\leftarrow 0$\\
        \While{$|\X_{t}^{(i)}|> 1$ and $i\leq m$}{
            Compute $\bar{\lambda}^{(i)}_t\leftarrow\arginf_{\lambda\in\triangle_{\X}}\max_{x, x'\in\X_t^{(i)}}\Norm{x-x'}^2_{A(\lambda)^{-1}}$\\
            Update $\mc{X}_{t}^{(i+1)} \leftarrow \Bp{ x \in \mc{X}_t^{(i)}\left|\  \htheta_t^\top(\hat{x}^*_t-x) \leq 2^{-i}\right. }$\\
            $i\leftarrow i+1$\\
        }
        % Compute $\bar{\lambda}_t\leftarrow \frac{1}{i}\sum_{i'=0}^{i-1}\lambda_t^{(i')}$\\
        \textbf{return} $(\bar{\lambda}_t+\lambda^*)/2$, where $\bar{\lambda}_t= \frac{1}{i}\sum_{i'=0}^{i-1}\lambda_t^{(i')}$\\
    }
\end{algorithm}

% \begin{algorithm}[ht]
%     \caption{RAGE-Elimination}
%     \label{algo:rage_elimination}
%     \SetAlgoLined
%     \KwIn{estimated parameter $\htheta_t$; maximum number of iterations, $m$}
%     Find $\hat{x}^*_t\leftarrow\argmax_{x\in\X}x^\top\htheta_t$\\
%     Initialize $\X_t^{(0)}\leftarrow\X$ and $i\leftarrow 0$\\
%     \While{$|\X_{t}^{(i)}|> 1$ and $i\leq m$}{
%         Compute $\bar{\lambda}^{(i)}_t\leftarrow\arginf_{\lambda\in\triangle_{\X}}\max_{x, x'\in\X_t^{(i)}}\Norm{x-x'}^2_{A(\lambda)^{-1}}$\\
%         Update $\mc{X}_{t}^{(i+1)} \leftarrow \Bp{ x \in \mc{X}_t^{(i)}\left|\  \htheta_t^\top(\hat{x}^*_t-x) \leq 2^{-i}\right. }$\\
%         $i\leftarrow i+1$\\
%     }
%     % Compute $\bar{\lambda}_t\leftarrow \frac{1}{i}\sum_{i'=0}^{i-1}\lambda_t^{(i')}$\\
%     \textbf{return} $(\bar{\lambda}_t+\lambda^*)/2$, where $\bar{\lambda}_t= \frac{1}{i}\sum_{i'=0}^{i-1}\lambda_t^{(i')}$
% \end{algorithm}

We bound the error probability of \textsf{P1-RAGE} under both stationary and non-stationary environments in the following theorem and its proof is deferred to Appendix \ref{sec:bobw_proof}.
\begin{theorem}[Error Probability of \textsf{P1-RAGE}]
    \label{theo:bobw_upper_bound}
    Fix arm set $\X\subset\R^d$ with $\abs{\X}=K$ and budget $T$. For a stationary environment with unknown parameter $\theta$, if $m\geq i_0=\ceil{\log_2\Sp{1/\Delta_{(1)}}}+1$, then there exists absolute constant $c>0$ such that the error probability of \textsf{P1-RAGE} satisfies
    \fontsize{9.5}{9.5}
    \begin{align}
        \P_{\theta}\Sp{J_T\neq (1)}\leq & 2i_0 KT\exp\Sp{-\frac{cT}{H_{\textsf{P1-RAGE}}(\theta)}},\nonumber\\
        H_{\textsf{P1-RAGE}}(\theta)=& \frac{mi_0}{\Delta_{(1)}}\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_{(1)}}\frac{\Norm{x-x_{(1)}}^2_{A(\lambda)^{-1}}}{\Delta_x} + \frac{m\sqrt{d}}{\Delta_{(1)}}\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_{(1)}}\Norm{x-x_{(1)}}_{A(\lambda)^{-1}}.\label{equ:H_bobw}
    \end{align}
    \normalsize
    For a non-stationary environment with unknown parameter $\Bp{\theta_t}_{t=1}^{T}$, there exists absolute constant $c'>0$ such that the error probability of \textsf{P1-RAGE} satisfies
    $$\P_{\otheta_T}\Sp{J_T\neq (1)}\leq K\exp\Sp{-\frac{c'T\Delta_{(1)}^2}{d}}.$$
\end{theorem}

We can immediately see that in non-stationary environments, the error probability of \textsf{P1-RAGE} matches (up to a constant) with \textsf{G-BAI}, showing that \textsf{P1-RAGE} is indeed robust to non-stationarity. On the other hand, because of the $\frac{1}{\Delta_{(1)}}$ factor, we can see that in stationary environments, $H_{\textsf{P1-RAGE}}(\theta)\gtrsim \rho^*(\theta)$ (defined in equation \eqref{equ:rho_star}), which implies that \textsf{P1-RAGE} is suboptimal in stationary settings. However, this should be expected since even for multi-armed bandits, as proved in \citet{abbasi2018best}, it is impossible for an algorithm to be optimal in a stationary setting while being robust to non-stationarity, let alone linear bandits. 

Nevertheless, when applying Theorem \ref{theo:bobw_upper_bound} to multi-armed bandits ($\X=\Bp{\ve{e}_1, \dots, \ve{e}_K}$), as long as we choose $m\approx i_0$, we can show that (proved in Corollary \ref{coro:bobw_linear_to_mab})
$$H_{\textsf{P1-RAGE}}(\theta)=\widetilde{O}\Sp{\frac{1}{\Delta_{(1)}}\max_{k\in[K]}\frac{k}{\Delta_{(k)}}}=\widetilde{O}\Sp{H_{\mathrm{BOB}}(\theta)},$$
where $H_{\mathrm{BOB}}(\theta)$ is the best-of-both-worlds complexity proposed in \citet{abbasi2018best}. In particular, \citet{abbasi2018best} proves that $H_{\mathrm{BOB}}(\theta)$ is the best complexity that any algorithm can possibly achieve if it is constrained to be robust to non-stationarity.\footnote{A more detaild discussion of this lower bound is given in Appendix \ref{sec:lower_bound_diss}.} That is, again, our algorithm \textsf{P1-RAGE} retains the near-optimal complexity for stationary multi-armed bandits while being robust in non-stationary environments.

\begin{remark}
    Here, we do not elaborate the proof details of Theorem \ref{theo:bobw_upper_bound} mainly because we do not recognize them as widely applicable techniques. However, we do want to emphasize that this proof is by no means a simple extension of the analysis of the algorithm \textsf{P1} in \citet{abbasi2018best}. In particular, our proof uses a different set of virtual events based on the estimated gaps and contains an analysis of the subroutine \textsf{RAGE-Elimination} that \textsf{P1} does not have.
\end{remark}

% \begin{remark}[Theoretical limitations of \textsf{P1-RAGE}]
\textbf{Theoretical limitations of \textsf{P1-RAGE}.} Despite being near-optimal in multi-armed bandits, $H_{\textsf{P1-RAGE}}(\theta)$ still contains an extra term $\frac{m\sqrt{d}}{\Delta_{(1)}}\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_{(1)}}\Norm{x-x_{(1)}}_{A(\lambda)^{-1}}$ that we suspect to be unnecessary. This term appears because the Bernstein's inequality requires a bound of the estimator's magnitude, which can be removed if the concentration bound only scales with the estimator's variance. Although this is actually achieved by the Catoni's estimator \citep{wei2020taking}, it requires a concrete confidence level to be specified before estimation, which is not feasible here since our algorithm needs to be robust to non-stationarity. Finding an approach to circumvent this difficulty and remove this extra term or showing it is not removable can serve as a good future direction.

% Another limitation of \textsf{P1-RAGE} is its strong reliance on G-optimal design to confront the non-stationarity. That is, there is no mechanism allowing it to be adaptive to different levels of non-stationarity. Finding a good measure of non-stationarity for BAI problem and developing a corresponding adaptive algorithm can serve as another promising direction.
% \end{remark}

Finally, although \textsf{P1-RAGE} has favorable theoretical guarantees, it requires user-specified parameter $m\geq i_0=\ceil{\log(1/\Delta_{(1)})}+1$, which is practically not always available. Meanwhile, computing a full \textsf{RAGE-Elimination} subroutine at each time step can make the algorithm unacceptably slow.  Therefore, we also design a implementation-friendly modified algorithm, called \textsf{P1-Peace}, that are both parameter-free and computationally efficient. The key modifications are that its (virtual) elimination strategy is based on \textsf{Peace} in \citet{katz2020empirical} and it updates the sampling distribution $\lambda_t$ in a low frequency. The algorithm details are summarized in Algorithm \ref{algo:p1_peace} in Appendix \ref{sec:modified_algo}.