\section{Error Probability of Algorithm \ref{algo:gbai} in Non-stationary Environments}
\label{sec:g_design_proof}

\advupperbound*

\begin{proof}
    % Define $\otheta_T=\frac{1}{T}\sum_{t=1}^{T}\theta_t$ and $\Delta_k=\frac{1}{T}\sum_{t=1}^{T}\theta_t^\top(x_{(1)}-x_{k})$ for $k\neq (1)$. For $k=(1)$, we have $\Delta_{(1)}=\Delta_{(2)}$. 
    Based on the recommendation rule $x_{J_T}=\argmax_{x\in\X}x^\top\htheta_T$, we have
    \begin{align}
        \P\Sp{J_T\neq (1)}=&\P\Sp{\exists k \in [2:K] \text{ s.t. } x_{(k)}^\top\htheta_T\geq x_{(1)}^\top\htheta_T}\nonumber\\
        \leq & \P\Sp{\exists k\in[2:K]\text{ s.t. } x_{(k)}^\top\htheta_T-x_{(k)}^\top\otheta_T\geq\frac{\Delta_{(k)}}{2} \text{ or } x_{(1)}^\top\htheta_T-x_{(1)}^\top\otheta_T\leq -\frac{\Delta_{(1)}}{2}}\nonumber\\
        \leq & \P\Sp{x_{(1)}^\top\htheta_T-x_{(1)}^\top\otheta_T\leq -\frac{\Delta_{(1)}}{2}} + \sum_{k=2}^{K}\P\Sp{x_{(k)}^\top\htheta_T-x_{(k)}^\top\otheta_T\geq\frac{\Delta_{(k)}}{2}}.\label{equ:g_bernstein}
    \end{align}
    The above terms can be bounded by Bernstein's inequality. In particular, for the first term, we have
    $$\P\Sp{x_{(1)}^\top\htheta_T-x_{(1)}^\top\otheta_T\leq -\frac{\Delta_{(1)}}{2}}=\P\Sp{\sum_{t=1}^{T}x_{(1)}^\top\Sp{A(\lambda^*)^{-1}x_tr_t-\theta_t}\leq-\frac{T\Delta_{(1)}}{2}}.$$
    Since IPS estimator is unbiased, $x_{(1)}^\top\Sp{A(\lambda^*)^{-1}x_tr_t-\theta_t}$ is a zero-mean random variable. Based on our bounded reward assumption, we have
    $$\abs{x_{(1)}^\top\Sp{A(\lambda^*)^{-1}x_tr_t-\theta_t}}\leq \abs{x_{(1)}^\top A(\lambda^*)^{-1}x_t} + 2\leq \Norm{x_{(1)}}_{A(\lambda^*)^{-1}}\Norm{x_t}_{A(\lambda^*)^{-1}}+2\leq d+2\leq 3d,$$
    where we use the property of G-optimal design $\max_{x\in\X}\Norm{x}^2_{A(\lambda^*)^{-1}}\leq d$. We can similarly bound its variance by 
    \begin{align*}
        \E\Mp{\Sp{x_{(1)}^\top\Sp{A(\lambda^*)^{-1}x_tr_t-\theta_t}}^2}\leq & \E\Mp{\Sp{x_{(1)}^\top A(\lambda^*)^{-1}x_t}^2}\\
        = & x_{(1)}^\top A(\lambda^*)^{-1}\E\Mp{x_tx_t^\top}A(\lambda^*)^{-1}x_{(1)}\\
        = & x_{(1)}^\top A(\lambda^*)^{-1}A(\lambda^*)A(\lambda^*)^{-1}x_{(1)}\tag{Since $x_t\sim\lambda^*$ by algorithm}\\
        = & \Norm{x_{(1)}}^2_{A(\lambda^*)^{-1}} \leq  d
    \end{align*}
    Thus, by Bernstein's inequality, we have
    $$\P\Sp{x_{(1)}^\top\htheta_T-x_{(1)}^\top\otheta_T\leq -\frac{\Delta_{(1)}}{2}}\leq \exp\Sp{-\frac{T^2\Delta^2_{(1)}/8}{Td+Td\Delta_{(1)}/2}}\leq\exp\Sp{-\frac{T\Delta_{(1)}^2}{12d}},$$
    where the last inequality uses the assumption that $\Delta_{(1)}\leq 1$. By similarly applying Bernstein's inequality to other terms in \eqref{equ:g_bernstein}, we can then have
    \begin{align*}
        \P\Sp{J_T\neq x_{(1)}} \leq &\P\Sp{x_{(1)}^\top\htheta_T-x_{(1)}^\top\otheta_T\leq -\frac{\Delta_{(1)}}{2}} + \sum_{k=2}^{K}\P\Sp{x_{(k)}^\top\htheta_T-x_{(k)}^\top\otheta_T\geq\frac{\Delta_{(k)}}{2}}\\
        \leq & \sum_{k=1}^{K}\exp\Sp{-\frac{T\Delta_{(k)}^2}{12d}}\\
        \leq & K\exp\Sp{-\frac{T\Delta_{(1)}^2}{12d}}.
    \end{align*}
\end{proof}