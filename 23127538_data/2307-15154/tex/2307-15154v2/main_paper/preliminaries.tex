\section{PRELIMINARIES}
\label{sec:preliminary}
\textbf{Notation.} Let $[a:b]=\Bp{a, a+1, \dots, b}$ for $a, b\in\mathbb{N}$ with $b>a$ and $[a]=\Bp{1, \dots, a}$. For  a vector $x\in\R^d$ and symmetric positive semi-definite (PSD) matrix $A\in\mathbb{S}_+^d$, we use $\Norm{x}_A=\sqrt{x^\top Ax}$ to denote the Mahalanobis norm. For a finite set $\X\subset\R^d$ and distribution $\lambda\in\triangle_{\X}$ over $\X$, we use $A(\lambda)=\E_{x\sim\lambda}\Mp{xx^\top}$ to denote the covariance matrix under $\lambda$.

\subsection{Linear Bandits Problem Formulation}
\label{sec:linear_formulation}

\textbf{General stationary/non-stationary environments.} In this paper, we assume a standard stationary/non-stationary linear bandits model with fixed horizon $T$. %$T\in\mathbb{N}_+$. 
In particular, let $\X\subset\R^d$ be a finite arm set with $\abs{\X}=K$ such that $\mathrm{span}(\X)=\R^d$. At each time $t=1, \dots, T$, the learner will pick some arm $x_t\in\X$ and receive some noisy reward $r_t=x_t^\top\theta_t+\epsilon_t$, where $\epsilon_t\in[-1, 1]$ is some independent zero-mean noise. All parameters $\Bp{\theta_t}_{t=1}^{T}$ are chosen and fixed by the environment before the game starts.\footnote{Theoretically, this non-stationary setting has no essential difference with the adversarial setting. We choose this non-stationary setting mainly to keep our presentation concise.} The ultimate goal of the learner is to find the optimal arm $\argmax_{x\in\X}x^\top\otheta_T$, where $\otheta_T=\frac{1}{T}\sum_{t=1}^{T}\theta_t$ is the average parameter. This protocol is summarized in Figure \ref{fig:bandits_protocal}.
% Figure environment removed

For simplicity, we further assume that $\forall t\in[T]$, $\forall x\in\X$, $x^\top\theta_t\in[-1, 1]$ and the optimal arm $\argmax_{x\in\X}x^\top\otheta_T$ is unique. Meanwhile, similar to \citet{abbasi2018best}, we use the subscript $(k)$ to denote the index of $k$-th best arm in $\X$, which means to have $x_{(1)}^\top\otheta_T> x_{(2)}^\top\otheta_T\geq\dots\geq x_{(K)}^\top\otheta_T$. For each arm $k\in[K]$, we define its gap $\Delta_k$ as
$$\Delta_k=\begin{cases}
    (x_{(1)}-x_k)^\top\otheta_T &\text{if }k\neq (1),\\
    (x_{(1)}-x_{(2)})^\top\otheta_T & \text{if }k=(1).
\end{cases}$$
That is,we have $\Delta_{(1)}=\Delta_{(2)}\leq\Delta_{(3)}\leq\dots\leq\Delta_{(K)}$. As a slight abuse of notation, for unindexed arm $x\in\X$, we will use $\Delta_x$ to denote the gap of $x$. The performance of the learner is measured by its error probability $\P_{\otheta_T}\Sp{J_T\neq (1)}$, where $J_T$ is the index of the learner's recommendation and the probability measure is taken over the randomness inside the learner and the reward noise. Finally, we note that when the setting is stationary, we simply have $\theta_1=\dots=\theta_T=\theta^*$ and everything else is then defined accordingly.

\begin{remark}[Comparison to the adversarial setting]
    The traditional oblivious adversarial setting can be viewed as a special case of our non-stationary setting, in which we simply pick $\epsilon_t=0$ for all $t$ \citep{abbasi2018best}.
\end{remark}

% \paragraph{Stationary setting} In the stationary setting, we define $\theta^*=\theta_1=\dots=\theta_T$. Then, we immediately have $\otheta_T=\theta^*$ and the arms are ranked acoording to $x_{(1)}^\top\theta^*> x_{(2)}^\top\theta^*\geq\dots\geq x_{(K)}^\top\theta^*$. Correspondingly, the gaps are defined as $\Delta_{k}=(x_{(1)}-x_k)^\top\theta^*$ for $k\neq (1)$ and $\Delta_{(1)}=\Delta_{(2)}$.

% \textbf{Fixed-budget setting.} Our primary focus in this paper is fixed-budget BAI problem, in which an algorithm first collects a fixed number $T$ of samples and then gives the recommendation with error probability as small as possible. 
% On the other hand, in fixed-confidence setting, an algorithm will try to collect as few number of samples as possible but needs to guarantee that the recommendation is optimal with probability at least $1-\delta$ for some $\delta>0$. In this setting, the performance measure is the number of samples collected by the algorithm.

% \subsection{BAI for Multi-Armed Bandits}
% \label{sec:bai_multi_arm}

\subsection{BAI for Linear Bandits in Stationary Environments}
\label{sec:stationary}

In this section, we briefly review the well-studied best-arm identification problem for linear bandits in stationary settings. This problem's complexity, first proposed in \citet{soare2014best}, is defined as
\begin{equation}
    \label{equ:rho_star}
    \rho^*(\theta)=H_{\mathsf{LB}}(\theta)=\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_{(1)}}\frac{\Norm{x-x_{(1)}}^2_{A(\lambda)^{-1}}}{\Delta_x^2},
\end{equation}
where the optimal arm index $(1)$ and gaps $\Delta_k$ are defined based on the input parameter $\theta$. As discussed in \citet{soare2014best}, this complexity is approximately equal to the number of samples required (up to logarithmic terms) to find the best arm by running an oracle algorithm. Later in \citet{fiez2019sequential}, this complexity is proved to be the optimal sample complexity that a BAI algorithm can possibly achieve in a fixed-confidence setting. Recently, \citet{katz2020empirical} proposes algorithm \textsf{Peace} in fixed-budget setting that achieves error probability $\P_{\theta}\Sp{J_T\neq (1)}\leq\widetilde{O}\Sp{\exp\Sp{-\frac{T}{\rho^*(\theta)\log(d)}}}$.\footnote{Rigorously speaking, the error probability of \textsf{Peace} contains another complexity term called $\gamma^*(\theta)$, which is defined as the minimum of a Gaussian width term. However, as argued in \citet{katz2020empirical}, $\gamma^*(\theta)$ is roughly in a same order of $\rho^*(\theta)$. } 
% \maryam{was notation $J_T$ introduced earlier?}

% Unfortunately, despite $\rho^*(\theta)$ being well-known, current literature does not contain any formal lower bound statement saying that $\rho^*(\theta)$ is the best complexity that any algorithm can possible achieve in a stationary environment. Here, we fill this gap by providing the following theorem and its proof is deferred into Appendix \ref{sec:linear_sto_lower_bound}.
% \begin{theorem}
%     \label{theo:sto_lower_bound}
%     Given any arm set $\X\subset\R^d$ and $\theta\in\R^d$ with complexity $\rho^*(\theta)$, for arbitrary learner with fixed budget $T$, if the learner satisfies
%     $$\P_{\theta}\Sp{J_T\neq (1)}\leq \frac{1}{2}\exp\Sp{-\frac{T}{\rho^*(\theta)}},$$
%     then there exists a counterexample $\theta_{\mathrm{alt}}$ such that $\P_{\theta_{\mathrm{alt}}}\Sp{J_T\neq (1)}\geq\frac{1}{2}$.
% \end{theorem}
