\section{INTRODUCTION}
\label{sec:intro}

% intro paragraph on improtance of abtesting with small effect sizes
% Data-driven decision making and A/B testing, in particular, allows businesses to compare different strategies or designs directly on real-time customer data to gain valuable insights into customer behavior and preferences. 
% This data-centric approach to decision-making is transforming traditional industries, leading to more efficient processes, superior user experiences, and ultimately, increased profitability. 
% As the use of these methods has increased, low-hanging fruit is more rare and these technologies are increasingly being utilized to determine problems with smaller effect sizes, while also targeting smaller audiences. These two competing trends of smaller effect sizes and smaller sample sizes make it increasingly challenging to obtain  statistical significance and correct inferences. 
% This tension is leading to the increased use of \emph{adaptive} sampling schemes such as multiarmed bandits that aim to obtain the same statistical insights using fewer total observations.

Data-driven decision-making and A/B testing enable businesses to evaluate strategies using real-time customer data, offering insights into customer tendencies. 
% As the use of these methods has increased, these technologies are being utilized to determine problems with smaller effect sizes, while also targeting smaller audiences, leading to challenges in achieving statistical significance and correct inference with limited data.
As the use of these methods has increased, these technologies are being utilized to determine problems with smaller effect sizes, while also targeting smaller audiences. These two competing trends of smaller effect sizes and smaller sample sizes make it increasingly challenging to obtain  statistical significance and correct inference since the absolute number of observations is limited. 
% However, as these methods gain popularity, significant findings become scarce due to smaller effect and sample sizes, making it more difficult to achieve statistical significance. 
Consequently, there is a rising trend in using \emph{adaptive} sampling like multi-armed bandits to obtain the same statistical insights using fewer total observations.

% paragraph about how time-variation arises 
However, using adaptive experimentation schemes can come with many pitfalls. Most algorithms that are effective in practice  (e.g., Thompson Sampling) are developed with the assumption that the \emph{environment is stationary} and that rewards from treatments are stochastic. However in practice this is far from the case. Non-stationarity can be introduced from a variety of sources such as user populations that change from hour to hour, customer preferences which vary over the course of a year, changes in one part of a platform that lead to latency and higher bounceback, site-wide promotions and sales, interference from competitors, macroeconomic shifts, and many other disruptions. 
Many of these issues are often totally unobservable, %and as a result 
and therefore cannot be controlled, modeled, or accounted for by an experimenter. 
Under such an environment, it is also possible for the underlying performance of treatments to wildly change, and as a result, the treatment that is best performing on any given day may change. %As a result, it 
This makes the concept of ``the best-performing arm'' poorly defined.

Instead, in time-varying settings, the goal of an experimenter is to identify the ``counterfactual best treatment'' at the end of the experimentation period.
That is, the treatment that would have received the \emph{highest total reward had received all the samples}. 
However, in the absence of being able to predict or model time-variation, predicting precisely how a treatment would behave at every time point, at which time at most one treatment can be evaluated, is impossible.
Fortunately, randomization is a powerful tool to provide the next best thing: unbiased \emph{estimates} of how a treatment would behave as if it had been used at every time in the past. 
These methods are well-understood in the causal-inference and online learning literature and are commonly known as inverse-propensity score (IPS) estimators.
The idea is simple: consider a sequence of evaluations from $n$ treatments at each time $\{ x_t \}_{t=1}^T \subset \R^n$. Note that a procedure can only observe at most one treatment per time denoted as $I_t \in [n]$, which is drawn from a distribution $p_t$ over the $n$ treatments. Then $\widehat{X}_i = \frac{1}{T} \sum_{t=1}^T \frac{ \1\{ I_t = i\} }{p_{t,i}} x_{t,i}$ is an unbiased estimator of the cumulative gain $\frac{1}{T}\sum_{t=1}^T x_{t,i}$ by

\IfTwoColumnElse{
    \begin{equation}
        \label{equ:usual_ips}
        \begin{split}
            \E\left[ \frac{ \1\{ I_t = i\} }{p_{t,i}} x_{t,i} \right] =& \sum_{j=1}^n \P(I_t = j) \frac{ \1\{ j = i\} }{p_{t,i}} x_{t,i} \\
            =& \sum_{j=1}^n p_{t,j} \frac{  \1\{ j = i\}  }{p_{t,i}} x_{t,i} = x_{t,i},
        \end{split}
    \end{equation}
}{
    \begin{equation}
        \label{equ:usual_ips}
        \E\left[ \frac{ \1\{ I_t = i\} }{p_{t,i}} x_{t,i} \right] = \sum_{j=1}^n \P(I_t = j) \frac{ \1\{ j = i\} }{p_{t,i}} x_{t,i}= \sum_{j=1}^n p_{t,j} \frac{  \1\{ j = i\}  }{p_{t,i}} x_{t,i} = x_{t,i},
    \end{equation}
}

as long as $\min_{t,i} p_{t,i} > 0$.
Of course, there is no free lunch, and the variance of $\widehat{X}_i$ behaves like $\frac{1}{T^2} \sum_{t=1}^T 1/p_{t,i}$. 
Intuitively, to maximize efficiency of the samples we do take for inference, we should try to minimize the probabilities on poor performing treatments and prioritize mass for the high performing treatments. 
However, if the treatment performances vary over time, it can be challenging to determine how one might do this optimally.
Fortunately, \citet{abbasi2018best} proposes a novel solution to defining these probabilities in a dynamic way that achieves a ``Best of Both Worlds'' (BOBW) guarantee, which is an algorithm called \textsf{P1} that manages to achieve near-optimal rates regardless of whether the environment is stochastic or arbitrarily non-stationary (adversarial).
This seminal work is the gold standard for A/B testing in unpredictable non-stationary settings.

%The metric, $\frac{1}{T}\sum_{t=1}^T x_{t,i}$, often referred to as the ``cumulative gain of arm $i$'', is well defined any setting regardless of the degree of non-stationarity and is a reasonable objective for experimenters to aim to optimize.  (SAY THAT FOR REGRET WE HAVE A FULL STORY). However, very few works have considered the cumulative gain with a notable exception being the P1 algorithm of \citet{abbasi2018best}. In this work, they establish a ``Best of Both Worlds'' guarantee, that is an algorithm that manages to achieve near-optimal rates regardless of whether the environment is stochastic or arbitrarily non-stationary. 

% \zhihan{TODO: revise the motivation here.} 
If the number of treatments is small (<10 in practice), BOBW provides a robust solution for practitioners. 
However, there are many situations that practitioners are interested in for which the number of treatments is very large and intractable for traditional A/B testing. For example, multivariate testing \citet{hill2017efficient} aims to identify not just a single best item, but a set or bundle of items, such as the best 6 pieces of content to highlight on a home screen. 
Given $n$ possibilities, this results in $\binom{n}{6}$ total distinct treatments for the A / B test!
Given this combinatorial explosion, practitioners have made structural parametric assumptions, such as the expected value of a set of items behaves like
\begin{align*}
    \theta^{(0)} + \sum_{i=1}^n \theta_i^{(1)} \alpha_i  + \sum_{i=2}^n \sum_{j < i} \theta_{i,j}^{(2)} \alpha_i \alpha_j,
\end{align*}
where $\alpha \in \{ 0, 1\}^n$ with $\sum_i \alpha_i =6$ indicates whether an item was included in the set or not. 
%\maryam{when is this function submodular? Answer: when $\theta_{i,j}$ is negative for $i\neq j$. In that case k-subset selection via greedy has good guarantees. But not needed to discuss here.}
Note that these sums can be succinctly written as $\langle x, \theta \rangle$ for $\theta = (\theta^{(0)},\theta^{(1)},\theta^{(2)})^\top \in \R^{1+n+\binom{n}{2}}$ and an appropriate $x \in \{0,1\}^{1+n+\binom{n}{2}}$. 
This can reduce the overall number of unknowns, and dimension, to just $O( n^2)$ versus $O(n^6)$.
But now the vectors $x \in \mc{X}$, each associated with a particular bundle, are overlapping and can share information.
A similar situation arises if we have features or covariates that describe each possible treatment. 
For example, a particular song comes with lots of metadata including artist, genre, beats per minute, etc. which can encode the useful properties about the song. 

In these kinds of scenario---whether it be multivariate testing or items with feature descriptions---we would like to perform adaptive experimentation in the presence of time-variation. 
Recall that without covariates, we have solutions like \textsf{P1} that are near-optimal for time-variation.
And without time-variation, there are many methods that take covariates into account and are known to be near-optimal.
This work aims to develop an algorithmic framework for handling covariates with time variation.

The remainder of the paper is organized as follows. We discuss the related work in Section \ref{sec:related_work} and presents detailed problem formulations in Section \ref{sec:preliminary}. In Section \ref{sec:nonstationary}, we propose a simple algorithm for general non-stationary environments and then in Section \ref{sec:bobw}, we propose a robust algorithm that can simultaneously tackle stationary and non-stationary environments. Experiment results are presented in Section \ref{sec:experiments} and our conclusions in Section \ref{sec:conclusion}.
  
% But in the quest to expand the scope and power of adaptive experimentation with very large numbers of treatments, where A/B testing is impossible, covaraites are introduced. 

% However in many experimentation settings, we may have additional covariates for each treatment that can be used to drive experimentation. 
% Existing Pure exploration for linear bandit algorithms in the stochastic setting give instance optimal exploration stratgies for discovering the best treatment in the stochastic setting. Yet to date, there do not exist algorithms that incorporate covariates effectively, and is robust to arbitrary nonstationarity which can find the counterfactual best arm. This work aims to close this gap by providing the first algorithm to do so.

%There has been an extensive study of the 

% paragraph on adaptive methods of address time-variation

