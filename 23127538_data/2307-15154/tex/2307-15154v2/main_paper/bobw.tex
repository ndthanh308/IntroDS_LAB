\section{A ROBUST ALGORITHM FOR STATIONARY/NON-STATIONARY ENVIRONMENTS}
\label{sec:bobw}

%We should probably use  the words stochastic and adversarial (not just nonstationary) 

In this section, we present and analyze a new robust linear bandits BAI algorithm called \textsf{P1-RAGE}, which performs comparable to \textsf{G-BAI} in non-stationary environments but much better than it in stationary environments. We will show that it attains good error probability in both stationary and non-stationary environments simultaneously, without knowing a priori which environment it will encounter. We first discuss some intuitions behind the algorithm design.

\textbf{Stationary environments.} The development of our algorithm \textsf{P1-RAGE} is largely inspired by the high-level idea of the robust algorithm \textsf{P1}, proposed in \citet{abbasi2018best}, and the allocation strategy of \textsf{RAGE}, proposed in \citet{fiez2019sequential}. In particular, as discussed in \citet{abbasi2018best}, in multi-armed bandits, to minimize the error probability in stationary environment, we need to control the estimation variance of the optimal arm well enough. Therefore, at each time step, algorithm \textsf{P1} pulls the current estimated best arm with the highest probability (unnormalized ``probability one''), then subsequently the second best arm with second highest probability (unnormalized ``probability half'') and so on. 
% In particular, \textsf{P1} allocates the estimated best arm with unnormalized ``probability one'', estimated second best arm with ``probability half'' and so on. 
We can notice that it actually matches the allocation strategy of the successive halving algorithm in \citet{karnin2013almost}, which is proved to be near-optimal for BAI in stationary multi-armed bandits. Therefore, we design our probability allocation based on the allocation strategy of \textsf{RAGE}, which is proven to be near-optimal for fixed-confidence BAI in stationary linear bandits \citep{fiez2019sequential}. In particular, with the estimated parameter $\htheta_t$, we first find the estimated best arms $\hat{x}^*_t=\argmax_{x\in\X}x^\top\htheta_t$. Then, we use a subroutine to repeatedly and virtually eliminate arms with estimated gaps larger than certain threshold and compute $\mc{XY}$-allocation of the (virtually) remaining arms.\footnote{The elimination is virtual because no samples are collected during the elimination subroutine.} Then, we average over the allocation probabilities computed during each iteration.


% As discussed in \citet{soare2014best}, for linear bandits best-arm identification problem, a more appropriate allocation strategy is $\mc{XY}$-allocation. The reason is that to correctly identify the best arm, we will essentially estimate the gap $\Delta_x$ for all $x\in\X$ instead of just $x^\top\theta^*$. Therefore, it will be more important to reduce the variance along the directions of $x_{(1)}-x$ for all $x\in\X$ instead of simply the directions of all $x\in\X$.

\textbf{Non-stationary environments.} Finally, to address the potential non-stationarity in environments, we uniformly mix the allocation probability computed above with a G-optimal design. With such a mixture, the variance over all arms can be controlled well and thus the algorithm will be robust for both stationary and non-stationary environments. The details of \textsf{P1-RAGE} are summarized in Algorithm \ref{algo:p1_rage} and the subroutine to compute the allocation probability, called \textsf{RAGE-Elimination}, is summarized in Algorithm \ref{algo:rage_elimination}.
% \begin{algorithm}[ht]
%     \caption{P1-RAGE}
%     \label{algo:p1_rage}
%     \SetAlgoLined
%     \KwIn{budget, $T\in\mathbb{N}$; arm set $\mc{X}\subset\R^d$; maximum number of virtual phases, $m$}
%     Compute G-optimal design $\lambda^*$ based on Eq. \eqref{equ:g_design} and initialize $\lambda_1=\lambda^*$\\
%     \For{$t=1, 2, \dots, T$}{
%         Sample $x_t\sim\lambda_t$ and receive reward $r_t$\\
%         Estimate $\widehat{\theta}_t\leftarrow\frac{1}{t}\sum_{s=1}^t\E_{x\sim\lambda_s}\Mp{xx^\top}^{-1}x_s r_s$\\
%         Update $\lambda_{t+1}\leftarrow$\textsf{RAGE-Elimination}$(\htheta_t, m)$
%     }
%     \textbf{return} $\argmax_{x\in\X}x^\top\widehat{\theta}_T$\\
%     \SetKwFunction{proc}{\textsf{RAGE-Elimination}}
%     \SetKwProg{myproc}{Subroutine}{}{}
%     \myproc{\proc{$\htheta_t, m$}}{\label{algo:rage_elimination}
%         Find $\hat{x}^*_t\leftarrow\argmax_{x\in\X}x^\top\htheta_t$\\
%         Initialize $\X_t^{(0)}\leftarrow\X$ and $i\leftarrow 0$\\
%         \While{$|\X_{t}^{(i)}|> 1$ and $i\leq m$}{
%             Compute $\lambda^{(i)}_t\leftarrow\arginf_{\lambda\in\triangle_{\X}}\max_{x, x'\in\X_t^{(i)}}\Norm{x-x'}^2_{A(\lambda)^{-1}}$\\
%             Update $\mc{X}_{t}^{(i+1)} \leftarrow \Bp{ x \in \mc{X}_t^{(i)}\left|\  \htheta_t^\top(\hat{x}^*_t-x) \leq 2^{-i}\right. }$\\
%             $i\leftarrow i+1$\\
%         }
%         % Compute $\bar{\lambda}_t\leftarrow \frac{1}{i}\sum_{i'=0}^{i-1}\lambda_t^{(i')}$\\
%         \textbf{return} $(\bar{\lambda}_t+\lambda^*)/2$, where $\bar{\lambda}_t= \frac{1}{i}\sum_{i'=0}^{i-1}\lambda_t^{(i')}$\\
%     }
% \end{algorithm}


\begin{algorithm}[ht]
    \caption{P1-RAGE}
    \label{algo:p1_rage}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} budget, $T\in\mathbb{N}$; arm set $\mc{X}\subset\R^d$; maximum number of virtual phases, $m$
        \STATE Compute G-optimal design $\lambda^*$ based on Eq. \eqref{equ:g_design} and initialize $\lambda_1=\lambda^*$
        \FOR{$t=1, 2, \dots, T$}
            \STATE Sample $x_t\sim\lambda_t$ and receive reward $r_t$
            \STATE Estimate $\widehat{\theta}_t\leftarrow\frac{1}{t}\sum_{s=1}^t\E_{x\sim\lambda_s}\Mp{xx^\top}^{-1}x_s r_s$
            \STATE Update $\lambda_{t+1}\leftarrow$\textsf{RAGE-Elimination}$(\htheta_t, m)$
            \IfTwoColumnElse{
                 \\ 
            }{}
            \hfill // \COMMENT{Call Algorithm \ref{algo:rage_elimination}}
           
        \ENDFOR
        \RETURN $\argmax_{x\in\X}x^\top\widehat{\theta}_T$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
    \caption{RAGE-Elimination}
    \label{algo:rage_elimination}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} arm set $\mc{X}\subset\R^d$; current estimate $\htheta_t$; maximum number of virtual phases, $m$
        \STATE Find $\hat{x}^*_t\leftarrow\argmax_{x\in\X}x^\top\htheta_t$
        \STATE Initialize $\X_t^{(0)}\leftarrow\X$ and $i\leftarrow 0$
        \WHILE{$|\X_{t}^{(i)}|> 1$ and $i\leq m$}
            \STATE $\lambda^{(i)}_t\leftarrow\arginf_{\lambda\in\triangle_{\X}}\max_{x, x'\in\X_t^{(i)}}\Norm{x-x'}^2_{A(\lambda)^{-1}}$
            \STATE $\mc{X}_{t}^{(i+1)} \leftarrow \Bp{ x \in \mc{X}_t^{(i)}\left|\  \htheta_t^\top(\hat{x}^*_t-x) \leq 2^{-i}\right. }$
            \STATE $i\leftarrow i+1$
        \ENDWHILE
        \RETURN $(\bar{\lambda}_t+\lambda^*)/2$, where $\bar{\lambda}_t= \frac{1}{i}\sum_{i'=0}^{i-1}\lambda_t^{(i')}$
    \end{algorithmic}
\end{algorithm}


We bound the error probability of \textsf{P1-RAGE} under both stationary and non-stationary settings in the following theorem and its proof is deferred to Appendix \ref{sec:bobw_proof}.
\begin{theorem}[Error Probability of \textsf{P1-RAGE}]
    \label{theo:bobw_upper_bound}
    Fix arm set $\X\subset\R^d$ with $\abs{\X}=K$ and budget $T$. For a stationary environment with unknown parameter $\theta$, if $m\geq i_0=\ceil{\log_2\Sp{1/\Delta_{(1)}}}+1$, then there exists absolute constant $c>0$ such that the error probability of \textsf{P1-RAGE} satisfies
    \IfTwoColumnElse{
        \fontsize{9.5}{9.5}
        $$\P_{\theta}\Sp{J_T\neq (1)}\leq 2i_0 KT\exp\Sp{-\frac{cT}{H_{\textsf{P1-RAGE}}(\theta)}},$$
        \begin{equation}
            \label{equ:H_bobw}
            \begin{split}
                &H_{\textsf{P1-RAGE}}(\theta)= \frac{mi_0}{\Delta_{(1)}}\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_{(1)}}\frac{\Norm{x-x_{(1)}}^2_{A(\lambda)^{-1}}}{\Delta_x} \\
                &\quad + \frac{m\sqrt{d}}{\Delta_{(1)}}\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_{(1)}}\Norm{x-x_{(1)}}_{A(\lambda)^{-1}}.
            \end{split}
        \end{equation}
        \normalsize
    }{
        \begin{align}
            \P_{\theta}\Sp{J_T\neq (1)}\leq & 2i_0 KT\exp\Sp{-\frac{cT}{H_{\textsf{P1-RAGE}}(\theta)}},\nonumber\\
            \text{where } H_{\textsf{P1-RAGE}}(\theta)=& \frac{mi_0}{\Delta_{(1)}}\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_{(1)}}\frac{\Norm{x-x_{(1)}}^2_{A(\lambda)^{-1}}}{\Delta_x} + \frac{m\sqrt{d}}{\Delta_{(1)}}\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_{(1)}}\Norm{x-x_{(1)}}_{A(\lambda)^{-1}}.\label{equ:H_bobw}
        \end{align}
    }
    
    For a non-stationary environment with unknown parameter $\Bp{\theta_t}_{t=1}^{T}$, there exists absolute constant $c'>0$ such that the error probability of \textsf{P1-RAGE} satisfies
    $$\P_{\otheta_T}\Sp{J_T\neq (1)}\leq K\exp\Sp{-\frac{c'T\Delta_{(1)}^2}{d}}.$$
\end{theorem}

We can immediately see that in non-stationary environments, the error probability of \textsf{P1-RAGE} matches (up to a constant) with \textsf{G-BAI}, showing that \textsf{P1-RAGE} is minimax optimal for linear bandits under non-stationarity. On the other hand, because of the $\frac{1}{\Delta_{(1)}}$ factor, we can see that in stationary environments, $H_{\textsf{P1-RAGE}}(\theta)\gtrsim H_{\textsf{LB}}(\theta)$ (defined in Eq. \eqref{equ:rho_star}), which implies that \textsf{P1-RAGE} is suboptimal in stationary settings. However, this should be expected since even for multi-armed bandits, as proved in \citet{abbasi2018best}, it is impossible for an algorithm to achieve $H_{\textsf{LB}}(\theta)$ while being robust to non-stationarity, let alone linear bandits. 

Nevertheless, when applying Theorem \ref{theo:bobw_upper_bound} to multi-armed bandits ($\X=\Bp{\ve{e}_1, \dots, \ve{e}_K}$), as long as we choose $m\approx i_0$, we can show that (Corollary \ref{coro:bobw_linear_to_mab} in Appendix \ref{sec:bobw_proof})
$$H_{\textsf{P1-RAGE}}(\theta)=\widetilde{O}\Sp{\frac{1}{\Delta_{(1)}}\max_{k\in[K]}\frac{k}{\Delta_{(k)}}}=\widetilde{O}\Sp{H_{\mathrm{BOB}}(\theta)},$$
where $H_{\mathrm{BOB}}(\theta)$ is the best-of-both-worlds complexity proposed in \citet{abbasi2018best}. In particular, \citet{abbasi2018best} proves that $H_{\mathrm{BOB}}(\theta)$ is the best complexity that any algorithm can possibly achieve if it is constrained to be robust to non-stationarity. 
% \footnote{A more detaild discussion of this lower bound is given in Appendix \ref{sec:lower_bound_diss}.} 
That is, again, our algorithm \textsf{P1-RAGE} retains the near-optimal complexity for stationary multi-armed bandits if it is constrained to be robust in non-stationary environments.

\begin{remark}
    Here, we do not elaborate the proof details of Theorem \ref{theo:bobw_upper_bound} mainly because we do not recognize them as widely applicable techniques. However, we do want to emphasize that this proof is by no means a simple extension of the analysis of the algorithm \textsf{P1} in \citet{abbasi2018best}. In particular, our proof uses a different set of virtual events based on the estimated gaps. Meanwhile, the analysis of subroutine \textsf{RAGE-Elimination} is intricately tailored to the unique characteristics of being a virtual elimination strategy, which is not presented in neither \textsf{RAGE} nor \textsf{P1} \citep{abbasi2018best, fiez2019sequential}.
\end{remark}

% \begin{remark}[Theoretical limitations of \textsf{P1-RAGE}]
\textbf{Theoretical limitations of \textsf{P1-RAGE}.} Despite being near-optimal in multi-armed bandits, $H_{\textsf{P1-RAGE}}(\theta)$ includes an extra low-order term $\frac{m\sqrt{d}}{\Delta_{(1)}}\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_{(1)}}\Norm{x-x_{(1)}}_{A(\lambda)^{-1}}$. This term appears because the Bernstein's inequality requires a bound of the estimator's magnitude, which can be removed if the concentration bound only scales with the estimator's variance. Although this can often be accomplished by using Catoni's robust mean estimator \citep{wei2020taking}, it requires a concrete confidence level to be specified before estimation, which is not feasible in our fixed budget setting. Finding an approach to circumvent this difficulty and remove this extra term, or alternatively, demonstrate that it is necessary, is an open question. 


\begin{remark}
    % The question of whether the extra term is removable naturally relates to the lower bound of this problem. Although proving a lower bound can indeed provide a more complete picture, we would like to emphasize that this is a too ambitious goal which reasonably lies beyond the scope of this paper. In particular, for fixed-budget best-arm identification problem in linear bandits, an instance-dependent lower bound remains open even in the pure stationary setting.\footnote{\citet{yang2022minimax} proves a minimax lower bound instead of an instance-dependent lower bound.} Therefore, it would require a major breakthrough to achieve a lower bound for our problem setting immediately since it requires constructing both stationary and non-stationary counterexamples, which is strictly harder than the pure stationary setting.
    The question of whether the extra term is removable naturally relates the instance-dependent lower bound of this problem. However, proving an instance-dependent lower bound for our setting requires constructing both stationary and non-stationary counterexamples. This task is thereby more challenging compared to proving an instance-dependent lower bound for the fixed-budget best-arm identification problem in linear bandits within a purely stationary setting, an open question that persists (see \citet{yang2022minimax} for a minimax lower bound).  We thus leave establishing such instance-dependent lower bounds for future work.
\end{remark}
% \romain{
% \begin{remark}
%     The question of whether the extra term is removable naturally relates the instance-dependent lower bound of this problem. Proving an instance-dependent lower bound for our bobw problem setting requires constructing both stationary and non-stationary counterexamples. This is thus strictly harder than proving an instance-dependent lower bound for fixed-budget best-arm identification in linear bandits which remains an open problem in the pure stationary setting.\footnote{\citet{yang2022minimax} proves a minimax lower bound instead of an instance-dependent lower bound.} We thus leave establishing such instance-dependent lower bounds for future work.
%     % Therefore, it would require a major breakthrough to 
% \end{remark}}

\textbf{Parameter choice of \textsf{P1-RAGE}.} Although \textsf{P1-RAGE} requires a user-specified parameter $m\geq \ceil{\log(1/\Delta_{(1)})}+1$ to bound the total number of virtual phases, it is not difficult to choose a reasonable value for this parameter in a practical implementation. On the one hand, since its dependence on $\Delta_{(1)}^{-1}$ is only logarithmic, taking some moderate value such as $m=25$ should safely satisfy $m\geq i_0$ for most practical scenarios; on the other hand, in most real-world applications, a sub-optimal arm should always be acceptable as long as its gap is small enough. Indeed, if we take $\epsilon$ to be the largest acceptable sub-optimality gap and take $m\geq \ceil{\log(1/\epsilon)}+1$, then \textsf{P1-RAGE} will output arm $x_{J_T}$ that satisfies $\Delta_{J_T}\leq \max\Bp{\epsilon, \Delta_{(1)}}$ with high probability in pure stationary environments (Corollary \ref{coro:epsilon_bai} in Appendix \ref{sec:bobw_proof}). That is, the output arm will either be an optimal arm if $\epsilon\leq\Delta_{(1)}$ or an arm with an acceptable suboptimality gap $\epsilon$ otherwise.
% even in some rare case where $\Delta_{(1)}$ is unreasonably small, a more common practical demand is to identify an arm $x_{J_T}$ such that $\Delta_{J_T}\leq \epsilon$ for some $\epsilon>\Delta_{(1)}$. That is, in this scenario, we can simply take $m\geq i_0(\epsilon)=\ceil{\log(1/\epsilon)}+1$ and \textsf{P1-RAGE} will guarantee that the output arm $x_{J_T}$ satisfies $\Delta_{J_T}\leq \epsilon$ (Corollary \ref{coro:epsilon_bai}). More details of the implementation can be found in Appendix \ref{sec:experiment_details}.
% Meanwhile, computing a full \textsf{RAGE-Elimination} subroutine at each time step can make the algorithm unacceptably slow. Therefore, to be computationally efficient, we update the sampling distributio $\lambda_t$ in a low frequency. More details of the implementation can be found in


% Another limitation of \textsf{P1-RAGE} is its strong reliance on G-optimal design to confront the non-stationarity. That is, there is no mechanism allowing it to be adaptive to different levels of non-stationarity. Finding a good measure of non-stationarity for BAI problem and developing a corresponding adaptive algorithm can serve as another promising direction.
% \end{remark}

% Finally, although \textsf{P1-RAGE} has favorable theoretical guarantees, it requires user-specified parameter $m\geq i_0=\ceil{\log(1/\Delta_{(1)})}+1$, which is practically not always available. \romain{In practice, the logarithmic dependence enables to set a fixed number (e.g. $m=25$).} Meanwhile, computing a full \textsf{RAGE-Elimination} subroutine at each time step can make the algorithm unacceptably slow.  Therefore, we also design a implementation-friendly modified algorithm, called \textsf{P1-Peace}, that are both parameter-free and computationally efficient. The key modifications are that its (virtual) elimination strategy is based on \textsf{Peace} in \citet{katz2020empirical} and it updates the sampling distribution $\lambda_t$ in a low frequency. The algorithm details are summarized in Algorithm \ref{algo:p1_peace} in Appendix \ref{sec:modified_algo}. 
% \zhihan{TODO: add more details of \textsf{P1-Peace} here.}