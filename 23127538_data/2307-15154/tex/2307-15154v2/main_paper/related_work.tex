\section{RELATED WORK}
\label{sec:related_work}

% \paragraph{Stationary Linear Bandits} \todol{Remove MAB - cite some of the linear bandit works}
The problem of identifying the best arm in linear bandits is a well-established and extensively researched problem. 
% It has been addressed in classical studies in \citet{bubeck2009pure}, and near-optimal algorithms have been developed in \citet{kaufmann2016complexity,jamieson2014lil}. Very recently, \citet{barrier2023best} studies this problem in non-parametric multi-armed bandits and proposes upper and lower bounds that depend on generic distribution models.
% In recent times, there has been an increasing interest in studying the sample complexity of identifying the best arm in linear bandit problems 
\citep{soare2014best, karnin2016verification, xu2018fully, fiez2019sequential, katz2020empirical, degenne2020gamification, jedra2020optimal, wagenmaker2023instance}. Notably, \citet{katz2020empirical, azizi2021fixed, yang2021towards} focus on the fixed-budget setting and are closely related to our paper. One notable limitation of these algorithms is their reliance on (unrealistic) stationary settings, which leads to their critical failure when applied in non-stationary scenarios. This motivated increasing interest in studying models for non-stationarity in bandits problems and algorithms agnostic to non-stationary settings, which we review next.
% \romain{Thompson sampling?}
% \cite{zhu2022near, zimmert2019optimal, tao2018best}

\textbf{Models for non-stationarity in bandits. } 
% \todol{Need to say that unlike all these other works, we dont need a model for non-stationarity!}
A reasonable approach in bandit problems with distribution shifts is to provide tight models for unknown variations in the reward distribution. Most literature in this setting focuses on minimizing the dynamic regret, which compares the reward obtained against the reward of the best arm in each round $t$. \cite{garivier2011upper} demonstrates that existing methods such as \citet{auer2002nonstochastic} could achieve a dynamic regret of $\widetilde{O}(\sqrt{LT})$ when $L$, the number of distribution shifts, is known. Then, \citet{auer2019adaptively} makes a significant advancement by introducing an adaptive approach with the same dynamic regret but without the knowledge of $L$. More recently, \cite{chen2019new, wei2021non} establish analogous results in the contextual bandits settings. 
Measures of non-stationarity other than $L$ are also considered. In particular, \citet{chen2019new} measures the non-stationarity by total variation and \citet{suk2022tracking} proposes the novel notion of %significant 
severe shifts. 
Note importantly that while this extensive body of work focuses on building tight models of non-stationarity and developing regret minimization algorithms tuned to them, our work is agnostic to such models. 
% \maryam{reading the paragraphs added below, it seems we use the word `model' to mean any assumption at all, including a bound on total variation, or even knowing the total variation is bounded but not knowing the value, maybe we can explicitly say this. Another question that may come up is the difference between being agnostic and assuming adversarial.}

% On the other hand, the best-arm identification (BAI) problem in non-stationary bandits is much less studied. In particular, \citet{jamieson2016non} studies non-stochastic multi-armed bandits but only considers deterministic rewards with well-defined asymptotic limits; \citet{allesiardo2017non} studies non-stationary multi-armed bandits with fixed best arm in a fixed-confidence setting; \citet{abbasi2018best} proposes an algorithm for multi-armed bandits that is robust for both stationary and non-stationary environments. To the best of our knowledge, there is currently no literature studying BAI for linear bandits with non-stationarity, nor literature studying BAI with any concrete measure of non-stationarity. 
% \cite{mukherjee2019distribution, besson2022efficient}. 
% We focus on best arm identification under non-stationary bandits, where the (pure exploration) task is to find the arm with the highest cumulative gain \cite{azizi2021fixed} \romain{papers?}

% Further, $L$ is a very worst-case measure of non-stationarity. \cite{chen2019new, suk2022tracking, abbasi2022new} propose alternative notions of non-stationarity. \cite{chen2019new} specifically consider total variation and \cite{suk2022tracking} utilize the notion of significant shifts.

%\paragraph{Agnostic non-stationary bandits (Best of both worlds)}
\textbf{Agnostic non-stationary bandits (Best of both worlds).}
\citet{bubeck2012best,seldin2014one,seldin2017improved,auer2016algorithm,abbasi2018best, lee2021achieving} focus on the ``best of both worlds'' (BOBW) problem: design a bandit algorithm that agnostically achieves optimal performance in both stationary and non-stationary scenarios, even without prior knowledge of the environment.
While most BOBW work focus on regret minimization goals, \citet{abbasi2018best} focuses on BOBW for best-arm identification. In this work, as in \citet{abbasi2018best}, we focus on the agnostic setting.

\textbf{A/B testing.} As discussed in the introduction, our work is closely related to non-stationary A/B testing. In settings with non-stationarity and adaptive sample allocations, non-stationarity can lead to Simpson's paradox if the sample means are used to estimate arm means \citet{kohavi2011unexpected}. It is common in large-scale industrial platforms to assume that means vary smoothly \cite{wu2022non}, or that the differences between them are constant; i.e., all arms are subject to the same random exogeneous shock \cite{stats-accelerator}. The recent work \citet{qin2022adaptivity} models time-variation as arising from confounding due to a context distribution and aims to find the arm with the best reward on average under this context distribution. Their goal is similar to ours, but, unlike them, we do not assume a context distribution.

% Being agnostic to models of non-stationarity offers several compelling advantages. Firstly, it allows for a more flexible and adaptable approach. By not relying on specific models, BOBW algorithms are not bound by the assumptions and limitations inherent in those models. This flexibility enables to tackle a wider range of scenarios and data, including complex and dynamic environments where even well-thought and tight models of non-stationarity may fall short. Secondly, being agnostic to models promotes robustness and generalization. Non-stationarity is a pervasive characteristic in most real-world domains, and different models may have varying degrees of accuracy in capturing its dynamics. By remaining agnostic, a BOBW perspective is better positioned to handle diverse non-stationary settings, avoiding the risk of critical failures that can arise when relying on specific models of non-stationarity that may not adequately capture the true underlying processes.
%\romain{still need to add russo's paper}

%\textbf{Time variation in Regret Settings - but it doesn't apply.} 

% \romain{TODO}
% % \romain{discuss russo's paper}
% \romain{add exp3}
% \kevin{A nice A/B testing primer \cite{johari2015always}}
