\section{ERROR PROBABILITY OF ALGORITHM \ref{algo:p1_rage}}
\label{sec:bobw_proof}
\subsection{Stationary Environments}
We first prove an error probability of Algorithm \ref{algo:p1_rage} in stationary environments that contains unspecified parameters from the virtual phases. Without loss of generality, assume that the arms $x_1, \ldots, x_K$ are ordered such that $\theta^\top x_1 > \theta^\top x_2 \geq \dots \geq \theta^\top x_K$ and $\Delta_1=\Delta_2\leq\Delta_3\leq\dots\leq\Delta_K$.

Throughout this section, we will the following definitions: $i_0 = \ceil{\log_2(1/\Delta_{1})} + 1$, $\mc{A}_i=\Bp{x\in\X\mid \Delta_x\leq 2\cdot 2^{-i}}$, $\bar{i}(k)=\max\Bp{i\in[i_0 - 1]\mid \Delta_k\leq 2^{-i}}$ and  
    $$f(\mc{A}_i)=\min_{\lambda\in\triangle_{\X}}\max_{x, x'\in\mc{A}_i}\Norm{x-x'}^2_{A(\lambda)^{-1}}.$$

\begin{theorem}
    \label{theo:bobw_error_prob_raw}
    Let $\mc{D}=\Bp{\ve{a}\in[0, 1]^{i_0+1}\mid 0=a_0<a_1\leq a_2 \leq \ldots \leq a_{i_0}=1}$. Then, if $m\geq i_0$, The error probability of Algorithm \ref{algo:p1_rage} in a stationary environment with parameter $\theta$ is bounded as 
    \begin{align}
        \P_{\theta}\Sp{J_T\neq 1} \leq& 2i_0 KT \exp\Sp{-\frac{T}{\overline{H}_{\textsf{P1-RAGE}}(\theta)}},\nonumber\\
        \overline{H}_{\textsf{P1-RAGE}}(\theta)=&\min_{\ve{a}\in\mc{D}} \max_{k\in[K]}\frac{48m\sum_{i'=1}^{\bar{i}(k)}(a_{i'}-a_{i'-1})f(\mc{A}_{i'-2}) + 8(m\sqrt{df(\mc{X})}+1)a_{\bar{i}(k)}\Delta_k}{3a_{\bar{i}(k)}^2\Delta_k^2}.\label{equ:H_p1rage}
    \end{align}
\end{theorem}

\begin{proof}
With $0=n_0<n_1\leq n_2 \leq\ldots \leq n_{i_0}=T$.\footnote{We do not specify the values of $n_1, \dots, n_{i_0-1}$ for now.} we define the event $\xi_i$ with $i \geq 1$ as follows: after $n_i$ samples all the arms with true gap smaller than $2\cdot2^{-i}$ are estimated with precision $2^{-i}/2$, which is
\begin{align*}
    \xi_i = \{\forall t \geq n_i, \forall k\in[K] \text{ s.t. } \Delta_k \leq 2 \cdot 2^{-i} \implies | \Delta_k - \widehat{\Delta}^{(t)}_k| < 2^{-i}/2\} ,
\end{align*}
where $\widehat{\Delta}^{(t)}_k = (x_1-x_k)^\top\widehat{\theta}^{(t)}$ for $k>1$ and $\widehat{\Delta}^{(t)}_1 = (x_1-x_2)^\top\widehat{\theta}^{(t)}$.
% Note that $\xi_0$ holds for $n_0=0$ with probability $1$ since we assume $\Delta_K\leq 1$. 
We first show how these events $\Bp{\xi_i}_{i=1}^{i_0}$ relate the correctness of Algorithm \ref{algo:p1_rage}.

\textbf{Correctness.} If $\bigcap_{i=1}^{i_0} \xi_i$ holds then the algorithm successfully identifies the best arm. Indeed, if we assume it does not, then there must exist non-optimal arm $k_0$ such that $\widehat{\Delta}^{(T)}_{k_0} < 0$. As $\bigcap_{i=1}^{i_0} \xi_i$ holds, for some $i'\leq i_0$, it holds that $2^{-i'} < \Delta_{k_0} \leq 2\cdot2^{-i'}$ and then $ | \Delta_{k_0} - \widehat{\Delta}^{(T)}_{k_0}| < 2^{-i'}/2$. Therefore, we have $2^{-i'} < \Delta_{k_0} \leq \Delta_{k_0} - \widehat{\Delta}^{(T)}_{k_0} \leq | \Delta_{k_0} - \widehat{\Delta}^{(T)}_{k_0}| \leq 2^{-i'}/2$, which is a contradiction.

Thus, the error probability is upper bounded by $\P\left(\bigcup_{i=1}^{i_0} \xi_i^c \right)$, which gives us

\begin{align*}
    \P\Sp{J_T\neq 1} \leq& \P\Sp{\bigcup_{i=1}^{i_0} \xi_i^c} = \P\Sp{\bigcup_{i=1}^{i_0}\Sp{\xi_i^c\setminus\bigcup_{j=1}^{i-1}\xi_j^c}} \leq  \sum_{i=1}^{i_0}\P\Sp{\xi_i^c\setminus \bigcup_{j=1}^{i-1}\xi_j^c} \\
    =& \sum_{i=1}^{i_0}\P\Sp{\xi_i^c\cap\Sp{\bigcup_{j=1}^{i-1}\xi_j^c}^c} = \sum_{i=1}^{i_0}\P\Sp{\xi_i^c\cap\Sp{\bigcap_{j=1}^{i-1}\xi_j}} \\
    \leq& \sum_{i=1}^{i_0}\P\Sp{ \xi_i^c\left| \bigcap_{j=1}^{i-1} \xi_j\right.}.
\end{align*}

\textbf{Bernstein's inequality.} Now, we just need to find an upper bound of $\P\left(\xi_i^c \left|\bigcap_{j=1}^{i-1} \xi_{j}\right.\right)$. Assume $\exists t \geq n_i, \exists k\in[K] \text{ s.t. } \Delta_k \leq 2 \cdot 2^{-i}$.\footnote{Otherwise, $\xi_i$ is vacuously true and $\P\Sp{\xi_i^c}=0$.} Then, we have
\begin{align}
    &\P(| \Delta_k - \widehat{\Delta}^{(t)}_k| \geq 2^{-i}/2)\nonumber\\
    =& \P( |(\theta - \widehat{\theta}_t)^\top (x_1-x_k)| \geq 2^{-i}/2 ) \label{equ:epsilon_val}\\
    =&\P\Sp{\abs{\sum_{s=1}^{t}\left(\theta-A(\lambda_s)^{-1}x_sr_s\right)^\top (x_1-x_k)}\geq 2^{-i}t/2}\nonumber\\
    \overset{\text{(a)}}{\leq}& 2\exp\Sp{-\frac{2^{-2i}t^2/8}{2\sum_{s=1}^{t}\Norm{x_1-x_k}^2_{A(\bar{\lambda}_s)^{-1}}+\Sp{\sqrt{d}\max_{s\in[1:t]}\Norm{x_1-x_k}_{A(\bar{\lambda}_s)^{-1}}+1}t2^{-i}/3}}\tag{By Bernstein's inequality for martingale differences \cite{freedman1975tail}}\\
    \leq& 2\exp\Sp{-\frac{2^{-2i}t^2/8}{\text{term I}}},\nonumber
\end{align}
\begin{align*}
    \text{where term I}=&2\sum_{i'=1}^{i}\sum_{s=n_{i'-1}+1}^{n_{i'}}\Norm{x_1-x_k}^2_{A(\bar{\lambda}_s)^{-1}}+2\sum_{s=n_i+1}^{t}\Norm{x_1-x_k}^2_{A(\bar{\lambda}_s)^{-1}}\\
    &\quad +\left(\sqrt{d}\max_{s\in[1:t]}\Norm{x_1-x_k}_{A(\bar{\lambda}_s)^{-1}}+1\right) \cdot\frac{t2^{-i}}{3}.
\end{align*}
Here, to use Bernstein's inequality for martingale differences in the inequality (a) above, we need to bound the variance and magnitude of $\left(\theta-A(\lambda_s)^{-1}x_sr_s\right)^\top (x_1-x_k)$ condition on $\lambda_s$.\footnote{Since IPS estimator is unbiased and $\lambda_s$ is determined by the history prior to time $s$, we have $\E\Mp{\left(\theta-A(\lambda_s)^{-1}x_sr_s\right)^\top (x_1-x_k)\mid \mc{H}_{s-1}}=0$, which implies that it is a martingale difference sequence.} In particular, we have
\begin{align*}
    \abs{\left(\theta-A(\lambda_s)^{-1}x_sr_s\right)^\top (x_1-x_k)} \leq& \abs{(x_1-x_k)^\top A(\lambda_s)^{-1}x_s} + \Delta_k\\
    \leq& \Norm{x_1-x_k}_{A(\lambda_s)^{-1}}\Norm{x_s}_{A(\lambda_s)^{-1}} + 2\\
    \leq& 2\sqrt{d}\Norm{x_1-x_k}_{A(\bar{\lambda}_s)^{-1}}+2.\tag{Since $\lambda_s=(\bar{\lambda}_s+\lambda^*)/2$ and $\lambda\mapsto\Norm{x_1-x_k}^2_{A(\lambda)^{-1}}$ is convex in $\lambda$}
\end{align*}
\begin{align*}
    &\E\Mp{\Sp{\left(\theta-A(\lambda_s)^{-1}x_sr_s\right)^\top (x_1-x_k)}^2\mid\lambda_s}\\
    \leq & \E\Mp{\Sp{(x_1-x_k)^\top A(\lambda_s)^{-1}x_s}^2\mid \lambda_s}\\
    =&(x_1-x_k)^\top A(\lambda_s)^{-1}\E\Mp{x_sx_s^\top\mid\lambda_s}A(\lambda_s)^{-1}(x_1-x_k)\\
    =&\Norm{x_1-x_k}^2_{A(\lambda_s)^{-1}}\\
    \leq& 2\Norm{x_1-x_k}^2_{A(\bar{\lambda}_s)^{-1}}.\tag{Since $\lambda_s=(\bar{\lambda}_s+\lambda^*)/2$}
\end{align*}

\textbf{Single-term error probability.} Now, we need to use the property of the subroutine \textsf{RAGE-Elimination} (Line \ref{algo:rage_elimination} of Algorithm \ref{algo:p1_rage}) that generates $\lambda_s$. That is, by Lemma~\ref{lmm:subroutine_guarantees}, since $x_k\in\mc{A}_i\subseteq\mc{A}_{i'}$ for $i'\leq i$ and $m\geq i_0$, for $s\in[n_{i'-1}+1, n_{i'}]$, we have $\Norm{x_1 - x_k}_{A(\bar{\lambda}_s)^{-1}}^2 \leq m\inf_{\lambda\in\triangle_{\X}} \max_{x, x'\in\mc{A}_{i'-2}}\Norm{x - x'}^2_{A(\lambda)^{-1}}\overset{\mathrm{def}}{=}m f(\mc{A}_{i'-2})$. Thus, we have
\begin{align*}
    &\P(| \Delta_k - \widehat{\Delta}^{(t)}_k| \geq 2^{-i}/2)\\
    \leq& 2\exp\Sp{-\frac{2^{-2i}t^2/8}{2m\sum_{i'=1}^{i}(n_{i'}-n_{i'-1})f(\mc{A}_{i'-2})+2m(t-n_i)f(\mc{A}_{i-1})+(m\sqrt{df(\mc{X})}+1)t2^{-i}/3}}\\
    \leq & 2\exp\Sp{-\frac{2^{-2i}n_i^2/8}{2m\sum_{i'=1}^{i}(n_{i'}-n_{i'-1})f(\mc{A}_{i'-2})+(m\sqrt{df(\mc{X})}+1)n_i2^{-i}/3}},
\end{align*}
where the last inequality above holds because of $t\geq n_i$ and a simple fact that $t\mapsto\frac{t^2}{at+b}$ is an increasing function when $t\geq 0$ if $a>0$ and $b>0$. 

\textbf{Final error probability.} Then, with the union bound over all $ t \geq n_i$ and $ k\in[K]$, it holds for any $0<n_1 \leq n_2 \ldots \leq n_i \leq T$ that
\begin{align*}
    &\P\Sp{\xi_i^c \left|\bigcap_{j=1}^{i-1} \xi_{j}\right.}
    \leq 2KT \exp\Sp{-\frac{2^{-2i}n_i^2/8}{2m\sum_{i'=1}^{i}(n_{i'}-n_{i'-1})f(\mc{A}_{i'-2})+(m\sqrt{d f(\mc{X})}+1)n_i2^{-i}/3}}\\
    &\qquad\leq 2KT\max_{k\in[K]}\exp\Sp{-\frac{3n_{\bar{i}(k)}^2\Delta_k^2}{48m\sum_{i'=1}^{\bar{i}(k)}(n_{i'}-n_{i'-1})f(\mc{A}_{i'-2})+8(m\sqrt{d f(\mc{X})}+1)n_{\bar{i}(k)}\Delta_k}},
\end{align*}
where $\bar{i}(k)=\max\Bp{i\in[i_0 - 1]\mid \Delta_k\leq 2^{-i}}$. Here, the last inequality use the same simple fact that $t\mapsto\frac{t^2}{at+b}$ is an increasing function when $t\geq 0$ if $a>0$ and $b>0$.

With values of $0=n_0 < n_1 \leq n_2 \leq \dots\leq n_{i_0}=T$, we can define $a_i=\frac{n_i}{T}$, which implies $0=a_0<a_1\leq a_2\leq\dots\leq a_{i_0}=1$. Since the choice of values $\ve{a}\in\mc{D}$ is arbitrary, the final error probability can be bounded as 
\begin{align*}
    &\P\Sp{J_T\neq 1} \leq \sum_{i=1}^{i_0}\P\Sp{ \xi_j^c\left| \bigcap_{j=1}^{i-1} \xi_j\right.}\\
    &\leq 2i_0 KT \min_{\ve{a}\in\mc{D}} \max_{k\in[K]}\exp\Sp{-\frac{3Ta_{i(k)}^2\Delta_k^2}{48m\sum_{i'=1}^{\bar{i}(k)}(a_{i'}-a_{i'-1})f(\mc{A}_{i'-2})+8(m\sqrt{d f(\mc{X})}+1)a_{i(k)}\Delta_k}},
\end{align*}
which completes the proof
\end{proof}

% \textbf{Guarantees of \texttt{subroutine}}\\
% Define $\mc{A}_i=\Bp{j: \Delta_j\leq 2\cdot 2^{-i}}$.


\subsubsection{Properties of \textsf{RAGE-Elimination}}
In this section, we prove some properties of the \textsf{RAGE-Elimination} algorithm that will be useful for proving Theorem \ref{theo:bobw_error_prob_raw}.

\begin{lemma}
\label{lem:X_in_A}
Assume $t \geq n_i$. Then, under $\bigcap_{j=1}^{i-1} \xi_{j}$, when running \textsf{RAGE-Elimination} (line \ref{algo:rage_elimination} in Algorithm \ref{algo:p1_rage}), it holds that 
$$\mc{X}_{t}^{(i+1)}\subseteq \Bp{x\in\X\mid \widehat{\Delta}^{(t)}_x \leq 2^{-i}} \subseteq \mc{A}_i.$$
\end{lemma}
\begin{proof}
To show $\mc{X}_{t}^{(i+1)}\subseteq \Bp{x\in\X\mid \widehat{\Delta}^{(t)}_x \leq 2^{-i}}$, let $x_{\widehat{(1)}_t}=\argmax_{x\in\mc{X}}\inner{\htheta_t, x}$. Then, for some arm $x$, if we have $\inner{\widehat{\theta}^{(t)}, x_{\hatone_t}-x}\leq 2^{-i}$, it holds that
$$\inner{\widehat{\theta}^{(t)}, x_1-x}=\underbrace{\inner{\widehat{\theta}^{(t)}, x_1-x_{\hatone_t}}}_{\leq 0}+\underbrace{\inner{\widehat{\theta}^{(t)}, x_{\hatone_t}-x}}_{\leq 2^{-i}}\leq 2^{-i},$$
which implies $x\in\{x\in\X\mid \widehat{\Delta}^{(t)}_x \leq 2^{-i}\}$.

To show $\Bp{x\in\X\mid \widehat{\Delta}^{(t)}_x \leq 2^{-i}} \subseteq \mc{A}_i$, let $\widehat{\Delta}^{(t)}_x \leq 2^{-i}$ for some $x$ and assume for the sake of a contradiction that $\Delta_x > 2 \cdot 2^{-i}$. As $\Delta_x > 2 \cdot 2^{-i}$, there must exist $\tilde{i} \leq i-1$ such that
$2^{-\tilde{i}} < \Delta_x \leq 2\cdot2^{-\tilde{i}}$. Then $| \Delta_x - \widehat{\Delta}^{(t)}_x| < 2^{-\tilde{i}}/2$ since event $\xi_{\tilde{i}}$ holds. Meanwhile, we have $\widehat{\Delta}^{(t)}_x \leq 2^{-i} \leq 2^{-\tilde{i}}/2$ since $\tilde{i} \leq i-1$. Now, this leads to the contradiction
$$2^{-\tilde{i}}/2 = 2^{-\tilde{i}} - 2^{-\tilde{i}}/2 \leq \Delta_x - \widehat{\Delta}^{(t)}_x \leq  | \Delta_x - \widehat{\Delta}^{(t)}_j| < 2^{-\tilde{i}}/2.$$
Thus, under $\bigcap_{j=1}^{i-1} \xi_{j}$, we have
$$\Bp{x\in\X\mid \widehat{\Delta}^{(t)}_x \leq 2^{-i}} \subseteq \Bp{x\in\X\mid \Delta_x \leq 2 \cdot 2^{-i}}=\mc{A}_i.$$

\end{proof}


\begin{lemma}
\label{lem:A_in_X}
Assume $t\geq n_i$. Then, under $\bigcap_{j=1}^{i-1} \xi_{j}$, when running \textsf{RAGE-Elimination}, if $x\in\mc{A}_{i}$, then $x\in\mc{X}_{t}^{(i-1)}$.
\end{lemma}
\begin{proof}
If $x\in\mc{A}_i$, then $\inner{\theta, x_1-x}\leq 2\cdot 2^{-i}$. Again, let $x_{\hatone_t}=\argmax_{x\in\mc{X}}\inner{\htheta_t, x}$ and we have
\begin{align*}
    \inner{\htheta_t, \hat{x}_1^{(t)}-x}=&\inner{\htheta_t, x_{\hatone_t}-x_1}+\inner{\htheta_t, x_1-x}\\
    =&\inner{\htheta_t, x_{\hatone_t}-x_1}+\inner{\htheta_t-\theta, x_1-x}+\underbrace{\inner{\theta, x_1-x}}_{\leq 2\cdot 2^{-i}}\\
    \leq & \inner{\htheta_t, x_{\hatone_t}-x_1} + |\hdeltat_x-\Delta_x| + 2\cdot 2^{-i}\\
    \leq & \inner{\htheta_t, x_{\hatone_t}-x_1} + 2^{-i} + 2\cdot 2^{-i}\tag{Since $\xi_{i-1}$ holds}\\
    = & -\hdeltat_{x_{\hatone_t}}+ 2^{-i} + 2\cdot 2^{-i}\\
    \leq & 2^{-i} + 2^{-i} + 2\cdot 2^{-i}\\
    =& 4\cdot 2^{-i}.
\end{align*}
The last inequality above holds because under $\bigcap_{j=1}^{i-1} \xi_{j}$, by Lemma \ref{lem:X_in_A}, we have $x_{\hatone_t}\in\mc{A}_i$, meaning that $|\hdeltat_{x_{\hatone_t}}-\Delta_{x_{\hatone_t}}|< 2^{-i}\implies\hdeltat_{x_{\hatone_t}}>\Delta_{x_{\hatone_t}} - 2^{-i}>-2^{-i}$.
\end{proof}

\begin{lemma}\label{lmm:subroutine_guarantees}
Assume $t\geq n_i$ and $\bigcap_{j=1}^{i-1} \xi_{j}$ holds. When running \textsf{RAGE-Elimination}, If $x_k\in\mc{A}_{i}$, then
$$\Norm{x_1-x_k}^2_{A(\bar{\lambda}_t)^{-1}}\leq m \min_{\lambda\in\triangle_{\mc{X}}}\max_{x, x'\in\mc{A}_{i-2}}\Norm{x-x'}^2_{A(\lambda)^{-1}}.$$
\end{lemma}
\begin{proof}
By Lemma \ref{lem:A_in_X}, we have $x_1, x_k\in\mc{A}_i\implies x_1, x_k\in\mc{X}_{t}^{(i-1)}$, which means that $\abs{\X_{t}^{(i-1)}}\geq 2$ and $\bar{\lambda}_t=\frac{1}{i_t}\sum_{i'=1}^{i_t}\lambda_{t}^{(i')}$ for some $i_t$ satisfying $i-1\leq i_t\leq m$. Thus, We have
\begin{align*}
    \Norm{x_1-x_k}^2_{A(\bar{\lambda}_t)^{-1}}\leq & m\Norm{x_1-x_k}^2_{A\left(\lambda_{t}^{(i-1)}\right)^{-1}}\\
    \leq& m \max_{x, x'\in\mc{X}_{t}^{(i-1)}}\Norm{x-x'}^2_{A\left(\lambda_{t}^{(i-1)}\right)^{-1}}\tag{Since $x_1, x_k\in\mc{X}_{i-1}^{(t)}$}\\
    \overset{\text{(i)}}{\leq}& m \min_{\lambda\in\triangle_{\mc{X}}}\max_{x, x'\in\mc{A}_{i-2}}\Norm{x-x'}^2_{A(\lambda)^{-1}}.
\end{align*}
Here, the above inequality (i) holds because by Lemma \ref{lem:X_in_A}, we have $\mc{X}_{t}^{(i-1)}\subseteq\mc{A}_{i-2}$ and by algorithm construction, we have $\lambda_{t}^{(i-1)}\in\argmin_{\lambda\in\triangle_{\mc{X}}}\max_{x, x'\in\mc{X}_{t}^{(i-1)}}\Norm{x-x'}^2_{A(\lambda)^{-1}}$.
\end{proof}

\subsubsection{Simplified Stationary Complexity and its Relation to Multi-armed Bandits}

In this section, we simplify the complexity of Algorithm \ref{algo:p1_rage} obtained in Theorem \ref{theo:bobw_error_prob_raw} by appropriately choosing values $\ve{a}\in\mc{D}$. In particular, we have the following theorem.
\begin{theorem}
    For $\overline{H}_{\textsf{P1-RAGE}}(\theta)$ defined in equation \eqref{equ:H_p1rage}, we have
    \fontsize{9.5}{9.5}
    $$\overline{H}_{\textsf{P1-RAGE}}(\theta)\leq \frac{1024mi_0}{\Delta_1}\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_1}\frac{\Norm{x-x_1}^2_{A(\lambda)^{-1}}}{\Delta_x} + \frac{16m\sqrt{d}}{3\Delta_1}\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_1}\Norm{x-x_1}_{A(\lambda)^{-1}}+\frac{1}{3\Delta_1}.$$
    \normalsize
\end{theorem}
\begin{proof}
    For $i\in\Bp{1, \dots, i_0-1}$, we take $a_i=\frac{\Delta_1}{\Delta_{\bar{k}(i)}}$, where $\bar{k}(i)=\min\Bp{k\in[K]\mid \Delta_k\geq\frac{2^{-i}}{2}}$. Then, since $\bar{i}(k)=\max\Bp{i\in[i_0-1]\mid \Delta_k\leq 2^{-i}}$, for any $k\in[K]$, we have $\frac{2^{-\bar{i}(k)}}{2}\leq\Delta_{\bar{k}(\bar{i}(k))}\leq\Delta_k$, which further implies 
    $$a_{\bar{i}(k)}\Delta_k=\frac{\Delta_1}{\Delta_{\bar{k}(\bar{i}(k))}}\cdot\Delta_k\geq\Delta_1.$$
    Then, for $\overline{H}_{\textsf{P1-RAGE}}(\theta)$ (defined in equation \eqref{equ:H_p1rage}), we have
    \begin{align*}
        &\overline{H}_{\textsf{P1-RAGE}}(\theta)\leq \max_{k\in[K]}\Bp{\frac{16m\sum_{i'=1}^{\bar{i}(k)}(a_{i'}-a_{i'-1})f(\mc{A}_{i'-2}) }{a_{\bar{i}(k)}^2\Delta_k^2} + \frac{8(m\sqrt{df(\mc{X})}+1)}{3a_{\bar{i}(k)}\Delta_k}}\\
        &\quad \leq  \frac{16m}{\Delta_1}\max_{k\in[K]}\Bp{\frac{f(\mc{A}_{-1})}{\Delta_{\bar{k}(1)}}+\sum_{i'=2}^{\bar{i}(k)}\Sp{\frac{1}{\Delta_{\bar{k}(i')}}-\frac{1}{\Delta_{\bar{k}(i'-1)}}}f(\mc{A}_{i'-2})} + \frac{8(m\sqrt{df(\mc{X})}+1)}{3\Delta_1}.\tag{Since $a_0=0$ by definition}
    \end{align*}
    For the second term, using the definition of $f(\X)$, we simply have
    \begin{align}
        \frac{8(m\sqrt{df(\mc{X})}+1)}{3\Delta_1}=&\frac{8m\sqrt{d}}{3\Delta_1}\inf_{\lambda\in\triangle_{\X}}\max_{x, x'\in\X}\Norm{x-x_1+x_1-x'}_{A(\lambda)^{-1}}+\frac{1}{3\Delta_1}\nonumber\\
        \leq & \frac{16m\sqrt{d}}{3\Delta_1}\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_1}\Norm{x-x_1}_{A(\lambda)^{-1}}+\frac{1}{3\Delta_1}.\label{equ:expand_norm_square}
    \end{align}
    For the first term, by fixing arm index $k\in[K]$ and defining $j\in\argmax_{\ell\in[\bar{i}(k)]}\frac{f(\mc{A}_{\ell-2})}{\Delta_{\bar{k}(\ell)}}$, we have
    \begin{align*}
        &\frac{f(\mc{A}_{-1})}{\Delta_{\bar{k}(1)}}+\sum_{i'=2}^{\bar{i}(k)}\Sp{\frac{1}{\Delta_{\bar{k}(i')}}-\frac{1}{\Delta_{\bar{k}(i'-1)}}}f(\mc{A}_{i'-2})\\
        =&\frac{f(\mc{A}_{\bar{i}(k)-2})}{\Delta_{\bar{k}(\bar{i}(k))}}+\sum_{i'=1}^{\bar{i}(k)-1}\frac{f(\mc{A}_{i'-2})-f(\mc{A}_{i'-1})}{\Delta_{\bar{k}(i')}}\\
        \overset{\text{(a)}}{\leq} & \frac{f(\mc{A}_{j-2})}{\Delta_{\bar{k}(j)}}\Sp{1+\sum_{i'=1}^{\bar{i}(k)-1}\frac{f(\mc{A}_{i'-2})-f(\mc{A}_{i'-1})}{f(\mc{A}_{i'-2})}}\\
        \leq & \bar{i}(k)\frac{f(\mc{A}_{j-2})}{\Delta_{\bar{k}(j)}} \tag{Since $f(\mc{A}_{i'-2})\geq f(\mc{A}_{i'-1})$}\\
        \leq & i_0\max_{\ell\in[\bar{i}(k)]}\frac{f(\mc{A}_{\ell-2})}{\Delta_{\bar{k}(\ell)}} \tag{Since $\bar{i}(k)\leq i_0$ for any $k\in[K]$}\\
        = & i_0 \max_{\ell\in[\bar{i}(k)]} \inf_{\lambda\in\triangle_{\X}}\max_{x, x'\in\mc{A}_{\ell-2}}\frac{\Norm{x-x'}^2_{A(\lambda)^{-1}}}{\Delta_{\bar{k}(\ell)}}\\
        \leq & i_0\inf_{\lambda\in\triangle_{\X}} \max_{\ell\in[\bar{i}(k)]}\max_{x, x'\in\mc{A}_{\ell-2}}\frac{\Norm{x-x'}^2_{A(\lambda)^{-1}}}{\Delta_{\bar{k}(\ell)}} \tag{By the weak duality inequality}\\
        \leq &64i_0\inf_{\lambda\in\triangle_{\X}} \max_{\ell\in[\bar{i}(k)]}\max_{x\in\mc{A}_{\ell-2}, x\neq x_1}\frac{\Norm{x-x_1}^2_{A(\lambda)^{-1}}}{16\Delta_{\bar{k}(\ell)}} \tag{By reasoning similar to equation \eqref{equ:expand_norm_square}}\\
        \overset{\text{(b)}}{\leq} & 64i_0\inf_{\lambda\in\triangle_{\X}} \max_{\ell\in[\bar{i}(k)]}\max_{x\in\mc{A}_{\ell-2}, x\neq x_1}\frac{\Norm{x-x_1}^2_{A(\lambda)^{-1}}}{\Delta_x}\\
        \leq & 64i_0\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_1}\frac{\Norm{x-x_1}^2_{A(\lambda)^{-1}}}{\Delta_x}.
    \end{align*}
    Here, the inequality (a) above holds because $f(\mc{A}_{i'-2})\geq f(\mc{A}_{i'-1})$ and by definition of $j$, we have $\frac{f(\mc{A}_{\ell-2})}{\Delta_{\bar{k}(\ell)}}\leq\frac{f(\mc{A}_{j-2})}{\Delta_{\bar{k}(j)}}$. The inequality (b) above holds because by definitions of $\bar{k}(\ell)=\min\Bp{k\in[K]\mid \Delta_k\geq\frac{2^{-i}}{2}}$ and $\mc{A}_{\ell-2}=\Bp{x\in\X\mid \Delta_x\leq 2\cdot 2^{-(\ell-2)}}$, we have $16\Delta_{\bar{k}(\ell)}\geq\Delta_x$ for any $x\in\mc{A}_{\ell-2}.$

    Therefore, by plugging the bound of both terms back, we have
    \fontsize{9}{9}
    $$\overline{H}_{\textsf{P1-RAGE}}(\theta)\leq \frac{1024mi_0}{\Delta_1}\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_1}\frac{\Norm{x-x_1}^2_{A(\lambda)^{-1}}}{\Delta_x} + \frac{16m\sqrt{d}}{3\Delta_1}\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_1}\Norm{x-x_1}_{A(\lambda)^{-1}}+\frac{1}{3\Delta_1}.$$
    \normalsize
\end{proof}

In the following corollary, we show that the above simplified complexity is in a same order (up to logarithmic factors) of $H_{\mathrm{BOB}}$ proposed in \citet{abbasi2018best}.
\begin{corollary}
    \label{coro:bobw_linear_to_mab}
    In multi-armed bandits, meaning $d=K$ and $\X=\Bp{\ve{e}_1, \dots, \ve{e}_K}$, for $H_{\textsf{P1-RAGE}}(\theta)$ (defined in equation \eqref{equ:H_bobw}), if $m=i_0$, we then have
    $$H_{\textsf{P1-RAGE}}(\theta)\leq \frac{2i_0\Sp{i_0\log(2K)+1}}{\Delta_{(1)}}\max_{k\in[K]}\frac{k}{\Delta_{(k)}}=2i_0\Sp{i_0\log(2K)+1}H_{\mathrm{BOB}}(\theta).$$
\end{corollary}
\begin{proof}
    When in multi-armed bandits, for the first term in $H_{\textsf{P1-RAGE}}(\theta)$, we have
    $$\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_{(1)}}\frac{\Norm{x-x_{(1)}}^2_{A(\lambda)^{-1}}}{\Delta_x}\leq 2\sum_{k=1}^{K}\frac{1}{\Delta_k}\leq 2\log(2K)\max_{k\in[K]}\frac{k}{\Delta_{(k)}},$$
    where the first inequality above comes from \citet{soare2014best} and the second inequality comes from \citet{audibert2010best}. For the second term in $H_{\textsf{P1-RAGE}}(\theta)$, we have
    $$\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_{(1)}}\Norm{x-x_{(1)}}_{A(\lambda)^-1{}}=\inf_{\lambda\in\triangle_{\X}}\max_{k\neq (1)}\sqrt{\frac{1}{\lambda_{(1)}}+\frac{1}{\lambda_{k}}}=\sqrt{2K}, $$
    which then gives us $\frac{\sqrt{K}\cdot\sqrt{2K}}{\Delta_{(1)}}\leq\frac{2K}{\Delta_{(1)}\Delta_{(K)}}\leq\frac{2}{\Delta_{(1)}}\max_{k\in[K]}\frac{k}{\Delta_{(k)}}$. 

    Finally, by plugging these inequalities back into $H_{\textsf{P1-RAGE}}(\theta)$ (defined in equation \eqref{equ:H_bobw}), we have
    \begin{align*}
        H_{\textsf{P1-RAGE}}(\theta)=& \frac{mi_0}{\Delta_{(1)}}\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_{(1)}}\frac{\Norm{x-x_{(1)}}^2_{A(\lambda)^{-1}}}{\Delta_x} + \frac{m\sqrt{d}}{\Delta_{(1)}}\inf_{\lambda\in\triangle_{\X}}\max_{x\neq x_{(1)}}\Norm{x-x_{(1)}}_{A(\lambda)^{-1}}\\
        \leq & \frac{2i_0^2\log(2K)}{\Delta_{(1)}}\max_{k\in[K]}\frac{k}{\Delta_{(k)}} + \frac{2i_0}{\Delta_{(1)}}\max_{k\in[K]}\frac{k}{\Delta_{(k)}}\\
        = & \frac{2i_0\Sp{i_0\log(2K)+1}}{\Delta_{(1)}}\max_{k\in[K]}\frac{k}{\Delta_{(k)}}.
    \end{align*}
\end{proof}

\subsubsection{Approximate BAI of Algorithm \ref{algo:p1_rage}}
\begin{corollary}
    \label{coro:epsilon_bai}
    Fix arm set $\X\subset\R^d$ with $\abs{\X}=K$ and budget $T$. For a stationary environment with unknown parameter $\theta$, if $m\geq i_0(\epsilon)=\ceil{\log_2\Sp{1/\epsilon}}+1$ for some $\epsilon\geq \Delta_{1}$, then there exists absolute constant $c>0$ such that the error probability of \textsf{P1-RAGE} satisfies
    $$\P_{\theta}\Sp{J_T\notin\mc{A}(\epsilon) }\leq 2i_0(\epsilon) KT\exp\Sp{-\frac{cT}{H_{\textsf{P1-RAGE}}(\theta, \epsilon)}},$$
    where $\mc{A}(\epsilon)=\Bp{x\in\X\mid \Delta_x\leq\epsilon}$ and $H_{\textsf{P1-RAGE}}(\theta, \epsilon)$ is defined as replacing $i_0$ by $i_0(\epsilon)$ in $H_{\textsf{P1-RAGE}}(\theta)$ (defined in Eq. \eqref{equ:H_p1rage}).
\end{corollary}
\begin{proof}
    The proof is the same as Theorem \ref{theo:bobw_upper_bound} through simply replacing $i_0$ by $i_0(\epsilon)$.
\end{proof}

\subsection{Non-stationary Environments}
In this section, we prove the error probability of Algorithm \ref{algo:p1_rage} in general non-stationary environments.
\begin{theorem}
    Fix time horizon $T$, arm set $\X\subset\R^d$ with $\abs{\X}=K$ and arbitrary unknown parameters $\Bp{\theta_t}_{t=1}^T$. If we run Algorithm \ref{algo:p1_rage} in this non-stationary environment and obtain $x_{J_T}$, then it holds that
    $$\P_{\otheta_T}\Sp{J_T\neq (1)}\leq K\exp\Sp{-\frac{3T\Delta_{(1)}^2}{64d}}.$$
\end{theorem}
\begin{proof}
    The proof will basically resemble the one for Theorem \ref{theo:adv_upper_bound}. In particular, by the same reasoning to obtain equation \ref{equ:g_bernstein}, we have
    $$\P\Sp{J_T\neq (1)}\leq \P\Sp{x_{(1)}^\top\htheta_T-x_{(1)}^\top\otheta_T\leq -\frac{\Delta_{(1)}}{2}} + \sum_{k=2}^{K}\P\Sp{x_{(k)}^\top\htheta_T-x_{(k)}^\top\otheta_T\geq\frac{\Delta_{(k)}}{2}},$$
    $$\text{where }\P\Sp{x_{(1)}^\top\htheta_T-x_{(1)}^\top\otheta_T\leq -\frac{\Delta_{(1)}}{2}}=\P\Sp{\sum_{t=1}^{T}x_{(1)}^\top\Sp{A(\lambda_t)^{-1}x_tr_t-\theta_t}\leq-\frac{T\Delta_{(1)}}{2}}.$$
    Since $\lambda_t=\frac{\bar{\lambda}_t+\lambda^*}{2}$ and $\lambda\mapsto\Norm{x}^2_{A(\lambda)^{-1}}$ is convex in $\lambda$, to use the Berstein's inequality for martingale differences \citep{freedman1975tail}, we have
    $$\abs{x_{(1)}^\top\Sp{A(\lambda_t)^{-1}x_tr_t-\theta_t}}\leq 2\Norm{x_{(1)}}_{A(\lambda^*)^{-1}}\Norm{x_t}_{A(\lambda^*)^{-1}}+2\leq 2d+2\leq 4d,$$
    $$\E\Mp{\Sp{x_{(1)}^\top\Sp{A(\lambda_t)^{-1}x_tr_t-\theta_t}}^2\mid \lambda_t}=\Norm{x_{(1)}}^2_{A(\lambda_t)^{-1}} \leq  2\Norm{x_{(1)}}^2_{A(\lambda^*)^{-1}}\leq 2d.$$
    Therefore, we have
    $$\P\Sp{x_{(1)}^\top\htheta_T-x_{(1)}^\top\otheta_T\leq -\frac{\Delta_{(1)}}{2}}\leq\exp\Sp{-\frac{T\Delta_{(1)}^2/8}{2d+2d\Delta_{(1)}/3}}\leq \exp\Sp{-\frac{3T\Delta_{(1)}^2}{64d}}.$$
    By applying the same inequality to other terms, we have
    $$\P\Sp{J_T\neq (1)}\leq K\exp\Sp{-\frac{3T\Delta_{(1)}^2}{64d}}.$$
\end{proof}