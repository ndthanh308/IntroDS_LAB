\begin{table*}[!htb]
	\footnotesize
	\begin{center}
\resizebox{0.99\textwidth}{!}{
		\begin{tabular}{ |l|l|l|l|l|l|l|} 
			
			\hline
			\rowcolor{gray}
			\textbf{Year} &
			\textbf{Work} &
			\textbf{Coherence Exploited} &
			%\textbf{Reprojection} &
			%\textbf{Temporal Direction} &
			\textbf{Method Used} &
			\textbf{ML-based} &
			\textbf{Upsampling}\\ [0.5ex] 
			\hline\hline
			2007 & Nehab et al.~\cite{nehab2007accelerating} & Spatial and temporal  & Interpolation & \textcolor{red}{$\times$} & \\
			\hline
			2010 & Andreev et al.~\cite{andreev2010real} & Temporal  & Interpolation & \textcolor{red}{$\times$} & x to 60 fps \\
			\hline
			2010 & Didyk et al.~\cite{didyk2010perceptually} & Temporal  & Interpolation & \textcolor{red}{$\times$} & 40 fps to 120 fps \\
			\hline
			2010 & Herzog et al.~\cite{herzog2010spatio} & Spatial and temporal  & Interpolation & \textcolor{red}{$\times$} & \\
			\hline
			2011 & Yang et al.~\cite{yang2011image} & Temporal & Interpolation & \textcolor{red}{$\times$}  & \\
			\hline
			2012 & Bowles et al.~\cite{bowles2012iterative} & Temporal  & Interpolation & \textcolor{red}{$\times$} & \\
			\hline
			2018 & SAS~\cite{mueller2018shading} & Temporal  & Interpolation & \textcolor{red}{$\times$} & from (7.5,15,30, 60) fps to 120 fps \\
			\hline
			%2021 & Dutta et al.~\cite{dutta2021efficient} & Temporal  & Interpolation & \textcolor{green}{\checkmark} & 60 fps to 120 fps (for videos)\\
			%\hline
			2021 & ExtraNet~\cite{extranet} & Temporal  & Extrapolation & \textcolor{green}{\checkmark} & upto 2$\times$ (30 fps to 60 fps) \\
			\hline
			%2022 & Somraj et al.~\cite{somraj2022temporal} & Temporal & Interpolation & \textcolor{red}{$\times$}  & 30 fps to 150 fps\\
			%\hline
			2022 &  DLSS 3~\cite{DLSS3} & Spatial and temporal   & Interpolation & \textcolor{green}{\checkmark}  & upto 4$\times$\\
			\hline
			\textbf{2023} & \textbf{Our work} & \textbf{Temporal}  & \textbf{Extrapolation} & \textbf{\textcolor{green}{\checkmark}} & \textbf{upto 3$\times$} \\
			\hline
			
		
		\end{tabular}
}
	\end{center}
	\caption{A comparison of related work}
	\label{table:relwork}
\vskip -6mm
\end{table*}

\section{Related Work}
\label{sec:RelatedWork}

Over the past few years, a variety of solutions have been developed that exploit the spatial and temporal coherence
present in graphics applications to increase the frame rate of graphics applications and synchronize the GPU
refresh rate with the display refresh rate for a seamless user experience. Recent works primarily focus on \circled{1}
predicting new frames using interpolation~\cite{yang2011image,nehab2007accelerating,herzog2010spatio, andreev2010real,
DLSS2} and \circled{2} generating new frames using extrapolation~\cite{extranet} to increase the frame rate. We present
a brief comparison of related work in Table \ref{table:relwork}. The high processing cost per frame is the primary cause
of the low frame rate~\cite{nehab2007accelerating}, and previous works aim to decrease this cost. According to Herzog et
al.~\cite{herzog2010spatio}, the visual appearance, illumination parameters, etc., are nearly identical between any two
consecutive frames and sometimes within a single frame -- they are
 known as temporal and spatial coherence, respectively. These effects can be
exploited to reduce the overall processing cost per frame. 
They mention that although approaches based on reducing the resolution of a frame, predicting the next few frames, and then
performing
spatial supersampling are very efficient, such approaches
can also undersample or blur sharp image features such as edges quite frequently. On the
other hand, pure temporal supersampling is markedly better. This work is based on exclusive temporal
supersampling. 

\subsection{Interpolation}
Previous approaches \cite{nehab2007accelerating} that use the temporal supersampling to fill in a frame between a pair
of rendered frames use the interpolation process that is guided by the scene flow: the 3D velocities of visible surface
points between two frames. For each pixel in an intermediate frame, the motion vector indicates where to pull pixel
information from the original frames. Early approaches had a fundamental drawback, which was that
whenever the scene
contained regions that were visible in the current frame but were not in the previous one, the results
were sub-par. Although Bowles et
al. \cite{bowles2012iterative} proposed an efficient way to fix this using an iterative method called fixed point
iteration (FPI), this  did not provide satisfactory results. To handle this case, various works
~\cite{yang2011image,didyk2010perceptually, DLSS3, mueller2018shading} propose  a bidirectional reprojection method that
temporally upsamples rendered content by reusing data from both the backward and forward temporal directions. Didyk et
al.~\cite{didyk2010perceptually} use motion flow to warp the previously shaded result into an in-between frame that is
then locally blurred to hide artifacts caused by morphing failures. Finally, they compensate for the lost
high-frequencies due to this blur by adding additional high frequencies wherever necessary. They perform an upsampling
from 40 Hz to 120 Hz. Similarly, NVIDIA's DLSS 3~\cite{DLSS3} use the optical flow computed from both the backward and
temporal directions to interpolate the frame. DLSS3 has two major components: an optical-flow generator and a frame
generator apart from the supersampling network. They use the in-built accelerator in their latest GPU architecture Ada
for the optical flow generation. The frame generator uses an AI-accelerated network that takes the computed optical flow
to generate an entirely new frame. As shown in Figure~\ref{inter_vs_extra}, this approach increases the frame rate but
also leads to an increased input latency that can easily be perceived by users. Since our approach is not based on 
optical flow fields, it does not require future frames to predict a new frame. 

Nehab et al.~\cite{nehab2007accelerating} use a reverse reprojection-based caching technique to store the information
that can be reused in the next frame, thereby avoiding the recomputation of the entire frame.  
Andreev et al.~\cite{andreev2010real} propose an approach to maintain a
consistent rate of 60 fps by dividing a frame into two parts: slow-moving and
fast-moving, and rendering each one at a different rate (slower parts at a lower rate and faster parts at a higher
rate). This approach works because some tests have shown that the temporal coherence of slowly moving parts is greater
than that of other parts.  Such approaches increase
the time required for an application to construct a frame while maintaining a
constant frame rate. 

\subsection{Extrapolation}

This is a very sparse area of research. The only prominent work that we are aware of is ExtraNet~\cite{extranet}.
This was discussed in detail in Section~\ref{sec:extra_lat}. 

















