\section{Characterization}
\label{sec:Characterization}
In this section, we first show the workloads used for experiments and the platform configuration for running
experiments. As mentioned in Section~\ref{subsubsec:inter_vs_extra}, interpolation adds latency to the system in
addition to its inherent operating latency, hence  extrapolation is a preferable choice for temporal supersampling in
real-time rendering systems. However, even extrapolation has some latency. So, in this section, we show the latency of
the various steps involved in the extrapolation process used in ExtraNet. This is to find the reasons for its
unacceptably large latency and possible solutions.

\subsection{Dataset}
Similar to prior work~\cite{extranet}, we use five different applications from the Unreal Engine
marketplace~\cite{Marketplace} with different artistic backgrounds and different levels of complexities~\ref{fig:bench}.
Together they cover a range of different shading effects and transforms. Each application has scenes with 
dynamically moving objects and different inter-frame variations. The experiments are run
on an NVIDIA RTX series GPU. The detailed configuration is given in Table~\ref{tab:config}. 
%FIXME: Mention exactly which NVIDIA GPU model

\begin{table}[]
\footnotesize
\begin{center}
	
\resizebox{0.99\columnwidth}{!}{
  \begin{tabular}{|l|l|l|l|l|}
    \hline
    %\rowcolor{gray}
   {\textbf{Abbr.}} &{ \textbf{Name}} &  {\textbf{Resolution}} & \textbf{API} & \textbf{Platform}    \\
    
    \hline
  \textit{LB} & Lab ~\cite{lab} & 480p & DX12 & UE   \\
    \hline
    
    
  \textit{TR} &  Tropical ~\cite{tropical} & 480p & DX12 & UE   \\
    \hline
    
  \textit{VL} &  Village ~\cite{village} & 480p & DX12 & UE   \\
    \hline
  \textit{TN} & Town ~\cite{town} & 480p & DX12 & UE  \\
   
 \textit{TN2}  &  & 720p &  &    \\
    
 \textit{TN3}  &  & 1080p &  &    \\
     \hline
  \textit{SL} &  Slum ~\cite{slum} & 480p & DX12 &UE   \\
    
\textit{SL2}  &  & 720p & DX12 &    \\
    
 \textit{SL3}  &  & 1080p & DX12 &    \\
    \hline
    \multicolumn{5}{|l|}{UE: Unreal Engine, DX: DirectX} \\
    \hline
  \end{tabular}
  }
 \end{center}
 \caption{Graphics benchmarks \label{fig:bench}}
\vspace{-3mm}
\end{table} 




\begin{table}[]
\footnotesize
\begin{center}
%\resizebox{0.99\columnwidth}{!}{
\begin{tabular}{| l l l|} 

\hline
\rowcolor{gray}
\textbf{Parameter} &
\multicolumn{2}{l|}{\textbf{Type/Value}} \\ 
\hline\hline
%\rowcolor{gray}\multicolumn{3}{c}{Desktop Configuration} \\ \hline
CPU & \multicolumn{2}{l|}{Intel\textregistered Xeon\textregistered Gold 6226R @ 2.90GHz }\\
GPU &  \multicolumn{2}{l|}{NVIDIA RTX\texttrademark A4000 }\\ 
GPU memory &  \multicolumn{2}{l|}{16 GB}\\ 
\hline
\end{tabular}
%}
\end{center}
%\caption{Details of the baseline system}
\caption{Platform Configuration \label{tab:config}}
\vspace{-6mm}
\end{table}





\subsection{Extrapolation Latency}
\label{sec:extra_lat}
To the best of our knowledge, 
there is only one prominent state-of-the-art work that uses extrapolation for temporal supersampling in real-time namely 
\textit{ExtraNet}\cite{extranet}. 
We consider ExtraNet as the baseline for our work. ExtraNet is a DNN-based approach to extrapolate
frames. The authors of this work divide the extrapolation task into two stages. First, they simply warp the past frame
and then feed the warped frame to the proposed neural network to synthesize the final frame. According to them, the
warped frame created using only past frames may have some visible artifacts such as improper shadows and ghosting
effects if there are dynamic objects or there is a movement in the camera. To remove these artifacts, they first mark
invalid pixels in the warped frame and then use extrapolation to correct those pixels with the help of the neural
network. The neural network takes the last three frames into account to capture more information about the scene. 
To
mark invalid pixels as {\em holes}, they use a few intermediate buffers that are created during the rendering process, also
known as geometry buffers or G-buffers. The input to the neural network is the warped frame based on the last three rendered
frames, the corresponding images marked with holes and G-buffers. 

Hence, the steps involved in the extrapolation process are G-buffer generation, image warping, hole marking and
DNN-based inference for extrapolation. We measure the latency of each step separately for each application at different
resolutions. The results are shown in Table~\ref{tab:runtime}. These results are collected for 1000 frames per benchmark. We
make the following observations from the table:

\circled{1} The latency of all the steps except G-buffer generation is almost constant across applications for a given
resolution because it depends upon the size of the input frames. Also, the latency increases with an increase in the
resolution or image size. The latency of G-buffer generation varies across applications because it depends upon the
scene complexity.\\ 

\circled{2} For all applications, the most time-consuming step is the inference part (latency: 3.5
ms to 13.8 ms), which puts a limit on the number of frames that can be extrapolated before the actual rendered frame.
Hence, we propose to perform the inference or extrapolation only when it is necessary. We, instead, replace it with
warping,
which is faster (max latency: 4.6 ms) at the cost of accuracy.


\begin{table}[!h]
\small
\begin{center}
	
\resizebox{0.99\columnwidth}{!}{
  \begin{tabular}{|l|l|l|l|l|}
    \hline
   \multirow{2}{*}{\textbf{App.}} &  {\textbf{G-buffer}} & {\textbf{Warping}} & \textbf{Hole} & {\textbf{Network}}  \\ 
  & {\textbf{generation}} &  & \textbf{marking}& {\textbf{inference}}  \\ 
    
     
    \hline
    LB & 0.17 & 0.95 & 1.94 & 3.67   \\
    \hline
    TR & 0.36 &  0.89 & 1.89 & 3.78   \\
    \hline
    VL & 0.48 &  0.83 & 1.81 & 3.45   \\
    \hline
   
    TN & 0.34& 0.96 & 1.89  &  3.61  \\
    
  TN2  & 1.01 & 1.58 & 2.49 &  7.04   \\
    
  TN3  & 1.02 & 2.89 & 4.57 & 13.54   \\
    \hline
     SL & 0.24 & 0.95 & 1.93 & 3.55   \\
    
   SL2  & 1.24 & 1.67 & 2.59 &  7.09   \\
   
  SL3  & 2.1 & 2.91 & 4.63 & 13.78   \\
    \hline
  \end{tabular}
  }
 \end{center}
 \caption{Runtime (ms) breakdown of the \textit{ExtraNet} model}
\label{tab:runtime}
\vspace{-6mm}
\end{table}


\subsection{Holes in Warped Frames}
\label{holes}
In the previous section, we discussed that ExtraNet finds invalid pixels or holes in the warped frame and then uses a
neural network to fill those holes. To see whether we can skip this hole-filling process and display the warped image
itself on the display or if this hole-filling is indeed necessary, we plot the number of holes present in warped frames
across benchmarks. We use 1000 frames for each benchmark to plot the results. The results are shown in
Figure~\ref{hole_plot}. 

We make the following observations from the figure:\\
\circled{1} The number of invalid pixels in the warped frame depends on the scenes getting rendered. There may also be
frames with no holes.\\
\circled{2} For example, in the case of LB, almost 90\% of full frames have less than 10\%
invalid pixels, whereas, for TR, more than 60\% of  total frames have more than 20\% invalid pixels.\\
\circled{3} This clearly shows that warping may provide better quality for some frames or many frames depending on the
type of application. Warping is clearly a much faster process.  
This insight motivates us to propose a method to
choose between warping and extrapolation based on the state of the current scene.  


% Figure environment removed


\subsection{State Representation}
\label{staterep}
As mentioned in Section~\ref{holes}, there may or may not be holes in the warped frame. If there are holes in the frame,
it means that there are dynamic objects or the camera is moving~\cite{extranet}. According to Scherzer et al.
~\cite{scherzer2011survey}, the next frame may be predicted given the previously rendered frame and motion vectors only
using warping if there are no dynamic objects in the scene. Therefore, we propose a method, a decision predictor, that
chooses between the two alternatives: warping and extrapolation. Since the performance of these two methods depends on the
scene's type or current state, we must discover a way to determine the scene's state before designing the predictor,
i.e., whether there is any movement (beyond a threshold) 
in the objects or camera leading to holes in the warped frame. Once we know the
state, we can choose between extrapolation and warping using the state information. Unlike ExtraNet, we do not require
the precise location of the holes in this particular scenario. We wish to determine whether the current state may result
in invalid pixels in the warped frame. Since we intend to produce frames for high-frequency displays, we need to find
this information quickly. Therefore, we propose a few features that capture motion information and represent the
system's current state. We use a few auxiliary buffers used in the rendering process for this purpose.  


First, to capture dynamic objects, we use a motion vector buffer. A motion vector stores the motion information --
direction and magnitude -- of small blocks (areas of $16\times 16$ pixels) in the frame. Since the motion information in
the block containing the dynamic object would differ from the background or static objects, we can use this for defining
the state. We choose the variance in the motion as our feature. According to Guo et al.~\cite{extranet}, three more
buffers capture the dynamic movement information. Those are \textit{custom stencil}, \textit{world position}, and
\textit{world normal} (refer to Figure~\ref{buffers}). As clearly shown in the figure, the custom stencil buffer
directly captures dynamic objects. We use the clustering algorithm on the stencil buffer to find the number of dynamic
objects present in the frame. Next, we have the world normal and world position buffers. The values change from the last
frame to the current frame for these two buffers. We use a metric known as the Earth Movers Distance (EMD)
~\cite{rubner2000earth} to capture the change in values for these buffers. The final list of features to define the
current state is thus shown in Table~\ref{features}.

% Figure environment removed


\begin{table}[!h]
\footnotesize
\begin{center}
\resizebox{0.99\columnwidth}{!}{
\begin{tabular}{|l|p{34mm}|l|} 

\hline
%\rowcolor{gray}
\textbf{Feature} &
\textbf{Description} &
\textbf{Source buffer} 
\\ 
\hline\hline

\textit{Var} & Variance in motion vector & Motion vector\\ 
\hline
\textit{$EMD_{W_N}$} & EMD between buffers corresponding to $F_t$ and $F_{t-1}$ & World normal\\
\hline
\textit{$EMD_{W_P}$} & EMD between buffers corresponding to $F_t$ and $F_{t-1}$ & World position\\
\hline
\textit{$N_D$} & Number of dynamic objects & Custom stencil\\
\hline
\end{tabular}
}
\end{center}
\caption{List of features}
\label{features}
\vspace{-6mm}
\end{table}

\subsection{Effect of the Identified Features}
\label{feature}
As mentioned in Section~\ref{staterep}, we use the identified features as inputs to the proposed predictor. We plot the
correlation between these variables and warping to demonstrate how they could help with the prediction. The results are
displayed in Figure~\ref{fea_var}. These results are for 1000 frames per benchmark.
Figure~\ref{fea_var} shows the variation in the quality of the warped frame. The major insights from the results are as
follows:\\ \circled{1} The pattern for all the features is the same i.e., there is a decrease in the PSNR with increase
in the
features' values.\\ \circled{2} We use this relation to design our model for the prediction.

% Figure environment removed

