In this paper, we propose \method{}, a quasi-Newton method that simultaneously mitigates computation and convergence instability barriers in second-order methods, as well as scalability issues in distributed systems. By introducing momentum and damping into the Hessian update, \method{} obviates the need for highly costly estimation on the Hessian. Approximation along diagonal blocks further reduces memory and compute costs in distributed systems. Empirical analyses on CV models, such as ResNet-50 and Vision Transformer show that \method{} achieves faster convergence, and reaches similar accuracy compared to SGD.

\section*{Acknowledgements}
This material is based upon work supported by Defense Advanced Research Projects Agency (DARPA) under Contract FASTNICS HR001120C0088. The views, opinions, and/or findings expressed are those of the author(s) and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.