We conduct various experiments on computer vision (CV) problems involving datasets such as CIFAR-10, CIFAR-100 and ImageNet. We choose SGD and Adam as the main baselines since it is widely used in these tasks and maintains the best training performance. 
We also compare with another quasi-Newton method, KFAC, in large-scale model training. 
We separately tune hyperparameters for each optimizer to ensure it achieves the best validation accuracy.

We use a single GPU server with 8 Nvidia Quadro RTX 5000 GPUs to simulate a distributed system, where each GPU is used as a worker to perform forward and backward passes, and model updates. Furthermore, each worker is also assigned with one Hessian block to compute the Hessian inverse and gradient conditioning. The current implementation is based on PyTorch. We set lower and upper thresholds of damping $\sigma_L, \sigma_H$ to be $0.01, 1.5$ in all experiments to smooth the Hessian approximation.

\subsection{Experiments on CIFAR-10/CIFAR-100}
We first evaluate \method{} on two small-scale problems: CIFAR-10 and CIFAR-100, and demonstrate the convergence advantage of \method{} compared to SGD and Adam. 
The models used are ResNet-18 and DeiT-Tiny \cite{DeiT}, where DeiT-Tiny is an efficient image transformer with 12 layers, 3 attention heads, and hidden and MLP dimension of 192.
% The ViT model is based on Vision Transformer model \citep{ViT}, with 6 layers, 8 attention heads, a patch size of 16, and both hidden and MLP dimension of 512 for a total of about 10M parameters.

For ResNet18, we divide it into 4 blocks such that each block consists of 2 \emph{resblocks} (\cite{2016_CVPR_ResNet}). The linear layer for classification is packed into the last block.  For DeiT-Tiny, due to the small model size, we choose to approximate the whole Hessian. 

Hyperparameters are tuned to achieve the best validation accuracy. Details are provided in Appendix \ref{appx:hparam:cifar}.

Figure \ref{fig:cifar} shows training loss. We observe that \method{} achieves a much faster convergence rate compared to SGD and ADAM. 
Table \ref{tab:acc:cifar} lists the validation accuracy on CIFAR-10 and CIFAR-100. 
We note that \method{} also achieves similar accuracy as SGD. On the other hand, as in Figure  \ref{fig:cifar}, although ADAM has a convergence rate close to \method{}, the validation accuracy are much lower in all experiments compared to \method{}.
Therefore, we obverse \method{} not only deliver faster convergence, but also achieves good generalization performance.
% Figure environment removed

\begin{table}[!htb]
\caption{\footnotesize Validation accuracy of ResNet-18 and Deit-Tiny on CIFAR-10/100 using SGD, ADAM, and \method{}. }
\label{tab:acc:cifar}
\centering
\small
\begin{tabular}{ccc|ccc|ccc}
\toprule
\multicolumn{3}{c|}{ResNet-18/CIFAR-10} & \multicolumn{3}{c|}{ResNet-18/CIFAR-100} & \multicolumn{3}{c}{DeiT-Tiny/CIFAR-100}\\
\midrule
 SGD & ADAM & \method{} & SGD & ADAM & \method{} & SGD & ADAM & \method{} \\
 $94.1\pm 0.1$ & $92.7\pm 0.1$ & $93.9\pm 0.1$ & $75\pm 0.15$ & $72.2\pm 0.16$ & $74.4 \pm 0.1$ & $80.5\pm 0.2$ & $75.3\pm 0.3$ & $79.9 \pm 0.2$ \\
\bottomrule
\end{tabular}
\vspace{-.4cm}
\end{table}

\subsection{Experiments on ImageNet}
ImageNet has been the gold standard for evaluating the performance of optimizers. It consists of $\sim$1.2M training and $\sim$50K test images, categorized into 1000 classes. 
We follow the standard data pre-processing procedure, where each image is first resized to $256\times 256$, and randomly cropped to $224\times 224$ and flipped horizontally. Each image is then normalized using pre-computed mean and variance.
%\subsubsection{ResNet-50}

\textbf{ResNet-50} --
When approximating the Hessian, we divide ResNet-50 into 8 blocks such that each block consists of 2 \emph{resblocks}. Similar to ResNet-18 in CIFAR-10, the linear layer is packed into the last block.
Figure~\ref{fig:resnet_imagenet} shows iteration-wise convergence on ResNet-50 using SGD, Adam, KFAC and \method{}. 
Detailed hyperparameter settings are provided in Appendix \ref{appx:hparam:imagenet}. Compared to Adam and SGD, \method{} enjoys much faster per-iteration convergence. Such fast convergence is also reflected in the validation dataset (Figure \ref{fig:resnet_imagenet_val}). Furthermore, it also generalizes well on the validation set, and finally reaches comparable validation accuracy to SGD.
% Figure environment removed

\begin{table}[!htb]
\caption{\footnotesize Validation accuracy of ResNet-50 on ImageNet using SGD, ADAM, KFAC, and \method{}. }
\label{tab:runtime}
\centering
\small
\begin{tabular}{cc|cc|cc|cc}
\toprule
\multicolumn{2}{c|}{SGD} & \multicolumn{2}{c|}{ADAM} & \multicolumn{2}{c|}{\method{}} & \multicolumn{2}{c}{KFAC}\\
\midrule
 Acc & Time/epoch & Acc & Time/epoch & Acc & Time/epoch & Acc & Time/epoch  \\
 $74.9\pm 0.11$ & 7.8min & $73.95\pm 0.1$ & 7.8min & $74.6\pm 0.13$ & 7.9min & $74.5\pm 0.1$ & 17min \\
\bottomrule
\end{tabular}
\end{table}

The benefit of \method{} is even more striking in terms of wall-clock time. 
As listed in Figure \ref{fig:resnet_imagenet_time} and Table \ref{tab:runtime}, due to light compute costs, the per-epoch runtime of \method{} is almost the same as SGD and Adam. 
On the other hand, for KFAC, while it delivers fast per-iteration convergence compared to SGD and ADAM, the wall-clock performance is significantly diminished by its additional compute costs. The per-epoch runtime is $>2\times$ more than \method{}.

\iffalse
\begin{table}[!htb]
\caption{Wall-clock time for each optimizer to reach the optimal accuracy. \method{} needs the least time to reach the optimal accuracy. }
\label{tab:runtime}
\centering
\begin{tabular}{c|ccc|cc}
\toprule
 & \multicolumn{3}{c|}{ResNet50} & \multicolumn{2}{c}{ViT}\\
\midrule
 Opt. & SGD & KFAC & \method{} & SGD & \method{} \\
 Time (h)& 31.2 & 50.5 & 22.7 & 12.5 & 10.4 \\
\bottomrule
\end{tabular}
\vspace{-.4cm}
\end{table}
\fi

\subsection{Ablation Study: The Effects of Momentum and Damping}\label{subsec:ablation}
In this section, we give more insight into the effects of momentum and damping used in \method{}. To this end, we ablate two critical components in \method{}: momentum and damping in the Hessian approximation, and then use the ablated version to train ResNet-18 on CIFAR-10. We focus on CIFAR-10 since we observed more convergence instability on this dataset compared to others. 

Figure~\ref{fig:ablation} shows convergence using the ablated \method{} with only momentum (black), with only damping (purple), and with no momentum or damping (red). Due to stochastic noise, the ablated version of \method{} without momentum/damping (vanilla L-BFGS) diverges easily in the early stages. 
With momentum (black), the whole optimization is significantly stabilized. However, it still fails to converge when there is a radical change in the loss landscape (for example, when learning rate decays). 
With damping (purple), the Hessian approximation is effectively restrained, especially when such sudden changes in the loss landscape happen. 
It is interesting to observe that while damping prevents divergence, the whole training is still largely affected by stochastic noise. Notable fluctuation in the loss is commonly observed during training. As a comparison, the complete \method{} (blue) effectively addresses these issues achieving much more stable convergence.
% Figure environment removed