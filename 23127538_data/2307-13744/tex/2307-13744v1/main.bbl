\begin{thebibliography}{24}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Al-Baali \& Grandinetti(2017)Al-Baali and
  Grandinetti]{improvedDamping}
Mehiddin Al-Baali and Lucio Grandinetti.
\newblock Improved damped quasi--newton methods for unconstrained optimization.
\newblock \emph{Pacific Journal of Optimization (To appear)}, 2017.

\bibitem[Al-Baali et~al.(2014)Al-Baali, Grandinetti, and Pisacane]{dampedBFGS}
Mehiddin Al-Baali, Lucio Grandinetti, and Ornella Pisacane.
\newblock Damped techniques for the limited memory bfgs method for large-scale
  optimization.
\newblock \emph{Journal of Optimization Theory and Applications}, 161\penalty0
  (2):\penalty0 688--699, 2014.

\bibitem[Ba et~al.(2017)Ba, Grosse, and Martens]{2016_ICLR_distKFAC}
Jimmy Ba, Roger Grosse, and James Martens.
\newblock Distributed second-order optimization using kronecker-factored
  approximations.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Byrd \& Nocedal(1989)Byrd and Nocedal]{BFGStool}
Richard~H Byrd and Jorge Nocedal.
\newblock A tool for the analysis of quasi-newton methods with application to
  unconstrained minimization.
\newblock \emph{SIAM Journal on Numerical Analysis}, 26\penalty0 (3):\penalty0
  727--739, 1989.

\bibitem[Chang et~al.(2019)Chang, Sun, and Zhang]{sLBFGS}
Daqing Chang, Shiliang Sun, and Changshui Zhang.
\newblock An accelerated linearly convergent stochastic l-bfgs algorithm.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  30\penalty0 (11):\penalty0 3338--3346, 2019.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{2011_JMLR_AdaGrad}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of machine learning research}, 12\penalty0 (7), 2011.

\bibitem[Fletcher(2013)]{BFGS}
Roger Fletcher.
\newblock \emph{Practical methods of optimization}.
\newblock John Wiley \& Sons, 2013.

\bibitem[Gao \& Goldfarb(2019)Gao and Goldfarb]{2019_OMS_Superlinear}
Wenbo Gao and Donald Goldfarb.
\newblock Quasi-newton methods: superlinear convergence without line searches
  for self-concordant functions.
\newblock \emph{Optimization Methods and Software}, 34\penalty0 (1):\penalty0
  194--217, 2019.

\bibitem[Goldfarb et~al.(2020)Goldfarb, Ren, and Bahamou]{2020_NIPS_KFAC-LBFGS}
Donald Goldfarb, Yi~Ren, and Achraf Bahamou.
\newblock Practical quasi-newton methods for training deep neural networks.
\newblock \emph{arXiv preprint arXiv:2006.08877}, 2020.

\bibitem[Gower et~al.(2016)Gower, Goldfarb, and
  Richt{\'a}rik]{2016_ICML_BlockBFGS}
Robert Gower, Donald Goldfarb, and Peter Richt{\'a}rik.
\newblock Stochastic block bfgs: Squeezing more curvature out of data.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1869--1878. PMLR, 2016.

\bibitem[Gupta et~al.(2018)Gupta, Koren, and Singer]{Shampoo}
Vineet Gupta, Tomer Koren, and Yoram Singer.
\newblock Shampoo: Preconditioned stochastic tensor optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1842--1850. PMLR, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{2016_CVPR_ResNet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{2014_arXiv_Adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lucchi et~al.(2015)Lucchi, McWilliams, and Hofmann]{VITE}
Aurelien Lucchi, Brian McWilliams, and Thomas Hofmann.
\newblock A variance reduced stochastic newton method.
\newblock \emph{arXiv preprint arXiv:1503.08316}, 2015.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{2015_ICML_KFAC}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pp.\
  2408--2417. PMLR, 2015.

\bibitem[Mokhtari \& Ribeiro(2015)Mokhtari and Ribeiro]{2015_JMLR_LBFGS}
Aryan Mokhtari and Alejandro Ribeiro.
\newblock Global convergence of online limited memory bfgs.
\newblock \emph{The Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 3151--3181, 2015.

\bibitem[Moritz et~al.(2016)Moritz, Nishihara, and Jordan]{2016_PMLR_LBFGS}
Philipp Moritz, Robert Nishihara, and Michael Jordan.
\newblock A linearly-convergent stochastic l-bfgs algorithm.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  249--258.
  PMLR, 2016.

\bibitem[Nesterov(2003)]{convexOpt}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Nocedal(1980)]{1980_LBFGS}
Jorge Nocedal.
\newblock Updating quasi-newton matrices with limited storage.
\newblock \emph{Mathematics of computation}, 35\penalty0 (151):\penalty0
  773--782, 1980.

\bibitem[Pauloski et~al.(2020)Pauloski, Zhang, Huang, Xu, and
  Foster]{2020_SC_KFAC}
J.~Gregory Pauloski, Zhao Zhang, Lei Huang, Weijia Xu, and Ian~T. Foster.
\newblock Convolutional {N}eural {N}etwork {T}raining with {D}istributed
  {K}-{FAC}.
\newblock In \emph{Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis}, SC '20. IEEE Press,
  2020.
\newblock ISBN 9781728199986.
\newblock \doi{10.5555/3433701.3433826}.

\bibitem[Polyak(1963)]{PL}
Boris~Teodorovich Polyak.
\newblock Gradient methods for minimizing functionals.
\newblock \emph{Zhurnal vychislitel'noi matematiki i matematicheskoi fiziki},
  3\penalty0 (4):\penalty0 643--653, 1963.

\bibitem[Rodomanov \& Nesterov(2021)Rodomanov and
  Nesterov]{2021_SIAM_Superlinear}
Anton Rodomanov and Yurii Nesterov.
\newblock Greedy quasi-newton methods with explicit superlinear convergence.
\newblock \emph{SIAM Journal on Optimization}, 31\penalty0 (1):\penalty0
  785--811, 2021.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou]{DeiT}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In \emph{International conference on machine learning}, pp.\
  10347--10357. PMLR, 2021.

\bibitem[Zeiler(2012)]{AdaDelta}
Matthew~D Zeiler.
\newblock Adadelta: an adaptive learning rate method.
\newblock \emph{arXiv preprint arXiv:1212.5701}, 2012.

\end{thebibliography}
