In this section, we first prove that \method{} achieves a linear convergence rate under non-convex settings with proper assumptions.
Then, in the second part of this section, we delve into the compute and memory costs in \method{}, and show its benefits in wall-clock convergence compared to other baseline methods: stochastic L-BFGS, and KFAC.

\subsection{Convergence Analysis}
We assume the risk function $\Loss$ satisfies the following conditions:
\begin{assumption}\label{as:diff}
$\Loss(\btheta)$ is twice continuously differentiable.
\end{assumption}

\begin{assumption}\label{as:smooth}
$\ell_i(\btheta)$ is $\Lambda$-smooth for $1\leq i \leq N$, $\Lambda > 0$: 
$\forall \btheta_1, \btheta_2$, $\norm{\nabla\ell_i(\btheta_2)-\nabla\ell_i(\btheta_1)}\leq \Lambda\norm{\btheta_2-\btheta_1}$.
\end{assumption}

\begin{assumption}\label{as:pl}
$\Loss(\btheta)$ is $\lambda$-PL: it satisfies Polyak-Lojasiewicz (PL) condition for a constant $\lambda > 0$: $\norm{\grad{L}(\btheta)}^2 \geq \lambda\Loss(\btheta)$.
\end{assumption}

The smooth condition in AS \ref{as:smooth} is commonly used in analyzing convergence in practical optimization. 
In addition, noting that compared to typical strong convexity assumptions, the PL condition in AS \ref{as:pl} applies to a more general setting \citep{PL}.
Strong convexity implies the PL condition, but not vice versa. In AS \ref{as:pl}, we relax the constraint and only require the gradient variance to be lower bounded. 

With the assumptions, we present the convergence theorem as follows. Proofs are deferred to Appendix \ref{appx:proof}.
\begin{theorem}\label{theorem:convergence}
Assume AS \ref{as:diff}-\ref{as:pl} hold at each iteration $t$ of \method{} with mini-batch input $\tX_t$ where each sample is randomly sampled from  $\mathcal{X}$ with replacement, then the expectation of $\Loss(\btheta_{t})$ satisfies
\begin{center}
    $E_{\tX_{t}}[\Loss(\btheta_{t})] \leq \alpha_{t-1} E_{\tX_{t-1}}[\Loss(\btheta_{t-1})]$,
\end{center}
where $\alpha_{t-1} = 1-\eta_{t-1}\lambda\xi + \eta_{t-1}^2\Lambda^2\Xi^2$. $\xi$ and $\Xi$ denotes the lower and upper bound of the $\HI$.% and the expectations are with respect to the minibatches $\mathcal{S}_t$ and $\mathcal{S}_{t+1}$. %With $0<\eta_t < \frac{1}{2\lambda\xi}$, SLIM-QN converges to minimum $\mathcal{L}_{\otheta}$ in a linear rate.
\end{theorem}

By choosing $\eta_{t-1}$ such that $\alpha_{t-1} < 1$, \method{} converges at a linear rate. 
The convergence rate matches the best rate in stochastic QN optimizations. It is worth mentioning that no convergence rate beyond linear is observed in stochastic L-BFGS optimizations.
Theorem~\ref{theorem:convergence} is not aimed to push the theoretical convergence limit. Instead, it investigates the effects of momentum in the Hessian and block-diagonal approximation.
In addition, as mentioned earlier, Theorem~\ref{theorem:convergence} also applies to convex settings, as strong convexity implies $\norm{\grad{L}(\btheta)}^2$ is lower bounded by $\Loss(\btheta)$ for an appropriate $\lambda > 0$ and hence AS \ref{as:pl} holds.


\subsection{Complexity Analysis}
In this section, we analyse the compute and memory cost of SGD, KFAC\citep{2016_ICLR_distKFAC}, stochastic L-BFGS (we call it sL-BFGS)\citep{sLBFGS} and \method{}.
As the main motivation, reducing the complexities of QN methods is crucial for their deployment in real large-scale neural network optimization.

Given a model with parameter $\btheta \in \mathbb{R}^d$, we use $\Cfb$ and $\Mfb$ to represent the compute and memory cost of a forward/backward (Fwd/Bwd) pass with a batch size of $b=1$. Furthermore,  $\Copt$ denotes the compute cost of model updates (Opt) which consists of gradient reduction, computing and apply the update $\Delta\btheta$.

Table~\ref{tab::complexity} summarizes the total compute and memory cost of SGD, KFAC, sL-BFGS and \method{} in a general distributed system with $p$ workers. 
Compared to SGD, during the forward and backward passes, \method{} needs to additionally compute $\Mp{}$, $\Mg{}$, for which the complexity increases linearly with model size ($d$). 
The main extra compute \method{} introduces is the Hessian-vector product, in which we need to iterate over $\left \{ \bm{s}_i \right \}_{i=1}^M$ and $\left \{ \bm{y}_i \right \}_{i=1}^M$, as shown in  Alg~\ref{appx:hessianvec}. The complexity increases linearly with the number of history vectors and model size ($2Md$). However, it only adds average cost of $O(\frac{2Md}{p})$ on each worker.
Compared to $O(b\Cfb)$ complexity in forward and backward passes, such costs are relatively marginal.
% However, these added computations are marginal compared to computations in forward and backward pass ($b\Cfb$).

As a comparison, KFAC adds significant additional computations through 1) possible multiple backward passes to update factors ($\gamma b\Cfb$ with $\gamma\geq 1$), 2) matrix inversion ($\sum(l_i^3+(\frac{d_i}{l_i})^3)$) for every $T$ iterations, and 3) Matrix-vector products ($2\sum(l_i+\frac{d_i}{l_i})d_i$). 
% If the Fisher matrix is updated more frequently (that is for small $L$), then the amortized cost for matrix inversion is even more striking. 
On the other hand, sL-BFGS also resorts to computation-intensive operations including full-batch gradients and a separate large batch to estimate the Hessian, which respectively adds amortized costs of $O(b\Cfb)$ and $\frac{1}{T}b_H\Cfb$. With data parallelism, it requires each worker to locally perform gradient conditioning, which adds another cost of $O(2Md)$ in total. 
% As a comparison, it is almost cost-free to update the approximate Hessian in SLIM-QN.

As for memory usage, \method{} mainly needs $O(2Md)$ to store history vectors. The amortized costs of each worker are $O(\frac{2Md}{p})$.
In practice, $M$ is set to be $10\sim 20$, which ensures that memory usage is manageable in \method{}.
sL-BFGS needs $O(Md)$ storage for $\sv{i},\yv{i}$ in total, and amortized cost of $O(\frac{1}{T}b_H\Mfb)$ for additional backward passes.
While KFAC needs $O(2\sum (l_i^2+(\frac{d_i}{l_i})^2))$ to store sub-matrices and their inverse, where the actual memory footprint hinges on model architectures. 
\begin{table*}[!htb]
\vspace{-2mm}
\caption{\footnotesize Computations and Memory in SGD, KFAC, sL-BFGS and \method{}}
\label{tab::complexity}
\centering
\small
\begin{tabular}{@{}ccccc@{}}
\toprule
 & SGD & KFAC & sL-BFGS & \method \\ \midrule
 \multicolumn{5}{c}{Per-Node Computation} \\ \midrule
 \multicolumn{1}{c|}{Fwd\&Bwd}& $O(b\Cfb)$ &$O(d+\gamma b\Cfb+\frac{1}{T}\sum(l_i^3+(\frac{d_i}{l_i})^3))$ & $O(d+2b\Cfb+\frac{1}{T}b_H\Cfb)$ & $O(\frac{d}{p}+b\Cfb)$ \\
 \multicolumn{1}{c|}{Opt}& $O(d)$ & $O(d+2\sum(l_i+\frac{d_i}{l_i})d_i)$ & $O(d+2Md)$ & $O(d+\frac{2Md}{p})$ \\ \midrule
 \multicolumn{5}{c}{Per-Node Memory} \\ \midrule
 \multicolumn{1}{c|}{Fwd\&Bwd}& \multicolumn{1}{c}{$O(b\Mfb)$} & \multicolumn{1}{c}{$O(d+b\Mfb)$} & $O(d + b\Mfb + \frac{1}{T}b_H\Mfb)$ & $O(\frac{d}{p}+b\Mfb)$\\
 \multicolumn{1}{c|}{Opt}& \multicolumn{1}{c}{$O(d)$} & \multicolumn{1}{c}{$O(d+2\sum(l_i^2+(\frac{d_i}{l_i})^2))$} & $O(d+2Md)$ & $O(d+\frac{2Md}{p})$\\ \bottomrule
\end{tabular}
\small
\begin{itemize}
    \item $b$: per-node batch size. $b_H$: batch size for the Hessian approx. $p$: \#workers. $T$: Hessian update period.
    \item  $d_i$: \#params in  $i$-th layer. $l_i$: \#input neurons in $i$-th layer. $M$: max \#history vectors.
\end{itemize}
\vspace{-0.4cm}
\end{table*}

Table \ref{tab:resnet_costs} lists detailed amortized costs of different optimizers on ResNet-50/ImageNet in a distributed system. Compared to KFAC and sL-BFGS, \method{} significantly reduces compute costs in approximating the Hessian and gradient conditioning. Due to the efficient distributed design, memory consumption on each worker is also reduced compared to sL-BFGS.
\begin{table}[!htb]
\vspace{-2mm}
\caption{\footnotesize Computation (MACs) and memory costs of different optimizers on ResNet-50/ImageNet ($b=64$, $T=20$; $M=10$; $b_H=1024$, $\gamma=1$, $p=8$). ``B" denotes billion, and ``M" denotes million.}
\label{tab:resnet_costs}
\centering
\begin{tabular}{c|cccc}
\toprule
 & SGD & KFAC & sL-BFGS & \method{}  \\
\midrule
Fwd/Bwd & \multicolumn{4}{c}{769B} \\
\midrule
$\HI$ Compute & -  & 570B & 1414B & 52M\\
Opt & 26M & 156B & 4B & 546M \\
$\HI$ Memory & - & 308M & 4B & 520M \\
\bottomrule
\end{tabular}
\vspace{-.4cm}
\end{table}