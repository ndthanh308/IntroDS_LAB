In supervised learning, a typical task is to minimize a empirical risk function,
\begin{equation}\label{eq:loss}
    \underset{\btheta}{\min}\text{ }\mathcal{L}(\btheta; \mathcal{X}) := \frac{1}{N}\sum_{i=1}^N\ell(\btheta; x_i, y_i),
\end{equation}
where $\btheta \in \mathbb{R}^d$ denote the parameters to be optimized, and $\mathcal{X}$ represents training samples $\left \{ x_i, y_i \right \}_{i=1}^N$. 

At present, stochastic gradient descent method (SGD) and its variants, such as Adam \citep{2014_arXiv_Adam} are the preferred methods to optimize parameters $\btheta$ due to their simplicity, especially for large-scale machine learning problems.
Nevertheless, second-order or quasi-Newton (QN) methods have also been extensively investigated due to their superior convergence over gradient descent (GD) in strongly convex optimization settings \citep{2019_OMS_Superlinear,2021_SIAM_Superlinear}.

However, the Achilles heel of QN methods that is impeding their wide adoption for large-scale machine learning problems is their substantial compute and memory costs.
Specifically, these barriers stem from attaining second-order information, including computing and storing the Hessian, performing matrix inversion, etc. 
For large-scale neural networks, these operations pose daunting challenges to QN's implementations and significantly affect running time.
Moreover, second-order methods are inherently hard to parallelize as the Hessian inversion usually involves many sequential steps, making it difficult to leverage large-scale distributed systems to partition computations and memory across multiple nodes.
As a result, even though superior convergence performance is observed in strongly convex settings, it still remains unclear how to convey such benefits to large-scale model training in distributed settings.

\iffalse
To mitigate the compute and memory barriers, approximation methods have been developed to either estimate second-order information via Fisher information matrix in KFAC (expected Hessian matrix under negative log-likelihood loss) \cite{2015_ICML_KFAC,2016_ICLR_distKFAC,2020_SC_KFAC} or to directly approximate the Hessian inverse as in L-BFGS \cite{BFGS,2015_JMLR_LBFGS,2016_PMLR_LBFGS,2016_ICML_BlockBFGS,2018_SIAM_BFGS}. 
While KFAC and L-BFGS, to some extent, alleviate the compute and memory costs, they still experience performance issues (runtime, convergence stability and scalability) when used to train large-scale neural networks.
For instance, KFAC's promise of faster per-iteration convergence compared to SGD is often neutralized by the extra compute cost of estimating Fisher information in each mini-batch iteration and then performing costly matrix inversions \cite{2020_SC_KFAC}. 
Similarly, stochastic L-BFGS variants \cite{2015_JMLR_LBFGS,2016_PMLR_LBFGS,2016_ICML_BlockBFGS} have yet to be proven efficient in training large-scale models.
Furthermore, different from the vanilla L-BFGS \cite{1980_LBFGS}, these variants also involve expensive Hessian approximation. In particular, in addition to normal gradient computations, these methods usually use a separate large batch of inputs to compute necessary statistics for the Hessian approximation. Such additional computations usually offset the benefit of faster convergence.
Finally, as mentioned before, another barrier of second order methods is that they are not amenable to efficient parallelization in distributed settings. This is primarily due to the difficulty of parallelizing Hessian inversion such as in KFAC and the memory cost of storing a full Hessian or its approximation at each node for more accurate inversion such as in L-BFGS.
\fi
Due to the prohibitive challenge above, approximation methods are getting increasing attention that attain second-order information by formulating the Hessian inverse in different ways.
These methods, to some extent, open the door for second-order methods to large-scale machine learning. 
Among them, two lines have proved very promising.
The first one arises from the Fisher information matrix $I$ (expectation of the Hessian under a negative log-likelihood loss).  
Methods such as KFAC \citep{2015_ICML_KFAC,2016_ICLR_distKFAC,2020_SC_KFAC} first approximate $I$ and then simplify matrix inversion by decomposing $I$ into small submatrices.
However, the considerable overhead for obtaining the empirical Fisher information and its inverse greatly neutralizes its faster per-iteration convergence promises.
Another line of the approximation methods directly approximates the Hessian inverse via BFGS update.
With pairs of history gradient and parameter changes, BFGS directly approaches the Hessian inverse with no additional costs on computing and storing the Hessian matrix.
However, BFGS or its variant L-BFGS \citep{1980_LBFGS} has not proved efficient in large-scale stochastic optimization, where convergence instability is commonly observed. 
Additional operations introduced in recent works stabilize the training but with substantial costs \citep{2015_JMLR_LBFGS,2016_PMLR_LBFGS,2016_ICML_BlockBFGS}.
For instance, \cite{2016_PMLR_LBFGS} uses a separate large batch of inputs to compute the consistent gradient and parameter changes, which inevitably increase the wall-clock time.  
Moreover, moving to distributed systems, these methods are not amenable to efficient parallelization due to either direct matrix inverse (e.g., KFAC) or the iterative procedure in BFGS-like methods.



%In addition, as models become large and require distributed implementation, memory/communication requirements for storing/computing the Hessian and its inverse emerge as the main bottleneck during optimization. Therefore scalability is another emerging barrier for QN methods in distributed systems.
% More recently, authors in \citep{goldfarb2020practical} propose to lessen the computation burden of matrix inversion in KFAC via BFGS updates with promising results on simple architectures. However the efficacy of this approach is yet to be demonstrated on practical DNNs and large-scale datasets.

To simultaneously mitigate the challenges of compute and memory costs, and scalability in distributed systems, we propose \method{}, a stable distributed BFGS variant that preserves the convergence promises of second-order methods, with modest compute and memory costs compared to other QN methods.
\method{} addresses the aforementioned barriers as follows. 
First, \method{} introduces an almost cost-free momentum scheme into the BFGS update rule to stabilize the optimization. 
By using the momentum of the \emph{history statistics} (parameter and gradient changes) to estimate the Hessian inverse, \method{} smooths out the approximation with no needs of costly variance reduction methods (e.g., a separate large batch size to estimate the Hessian). Hence, both stability and compute efficiency are achieved.
For efficient parallelization and low memory footprint in distributed systems, \method{} presents a more generic block-wise Hessian approximation with each block consisting of one or multiple layers.
During optimization, each node only computes and stores statistics for one block rather than the full Hessian.  
As a result, \method{} can perform the Hessian inverse and gradient conditioning in a distributed way with marginal communication and synchronization overhead.

Our theoretical analysis shows \method{} effectively suppresses noise in the Hessian approximation and achieves stable convergence.
Empirical evaluations show that, on benchmark datasets, CIFAR-10 and ImageNet, and models such as ResNet and Vision Transformer, \method{} achieves a faster per-iteration convergence compared to SGD and Adam. 
Furthermore, due to the lightweight momentum-based Hessian approximation, \method{} needs a much shorter wall-clock time to achieve the target accuracy compared to SGD, Adam and other QN methods such as KFAC.

In summary, our main contributions are as follows:

1. We develop \method{}, a distributed stochastic QN method for large-scale models, that achieves fast and stable convergence with low computational complexity.

2. We provide theoretical analyses that show \method{} significantly mitigates adverse effects of stochastic noise on the Hessian approximation and achieves stable convergence for stochastic optimization problems.

3. We provide complexity analyses that demonstrate \method{} incurs much less complexity than other QN methods, leading to reductions in overall wall-clock training time.

4. Finally, we carry out comprehensive evaluations on various models and datasets that show \method{} empirically delivers faster per-iteration and wall-clock convergence compared to SGD, Adam, and other second-order optimizers. 
% For instance, to reach near optimal accuracy when training ResNet-50 on ImageNet, \method{} is $1.5\times$ than SGD ($1.37\times$ faster in wall clock time). Similar observations also hold for other models.
% For instance, when training ResNet-50 on ImageNet, \method{} is $2\times$ faster compared to SGD in terms of iterations ($1.8\times$ faster in wall clock time) in the early stages, while achieving comparable final test accuracy to SGD. 
% Furthermore, when training Vision Transformer models, \method{} also achieves faster convergence and higher accuracy.
