While SGD is widely used in many machine learning tasks, other forms of optimization have also been investigated extensively in the past years. Among these attempts, designing optimizers with preconditioned gradients is one of the most promising areas.

Adaptive methods such as Adam, AdaGrad, AdaDelta \citep{2014_arXiv_Adam, 2011_JMLR_AdaGrad, AdaDelta} construct a diagonal matrix by incorporating knowledge from the past gradients. Such a diagonal matrix adaptively adjusts the learning rate for each parameter.
For instance, AdaGrad uses a large learning rate for those irrelevant features (small gradients), and small learning for those relevant ones (large gradients).  Adams further uses the first and second moment of gradients to adjust the learning rate. 

Besides diagonal preconditioning matrices, constructing block diagonal matrices or even full matrices has received increasing attention in recent years. Methods such as Shampoo \citep{Shampoo} approximate the full matrix version of AdaGrad as a block-diagonal matrix to incorporate more curvature information during optimization. Similarly, a well-known KFAC method \citep{2015_ICML_KFAC} approximates the Fisher information matrix as a block-diagonal matrix. L-BFGS methods \cite{2016_PMLR_LBFGS} on the other hand directly construct the full Hessian matrix as a preconditioner during optimization. These preconditioners have been empirically proved to achieve fast convergence compared to SGD, as well as adaptive methods. Authors in \cite{2020_NIPS_KFAC-LBFGS} further adopt L-BFGS to efficiently compute matrix inversion in KFAC. 

Variance reduction in the preconditioning matrix is also crucial to ensure stable optimization. The algorithm in \cite{2016_PMLR_LBFGS} adopts a separate large batch of data to estimate current curvature. On the other hand, VITE \cite{VITE} chooses to use a pivot parameter together with full-batch gradients to reduce variance in the Hessian approximation.