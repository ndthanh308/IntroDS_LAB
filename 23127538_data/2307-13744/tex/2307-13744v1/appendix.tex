The appendix is arranged as follows:
In Sec \ref{appx:hessianvec}, we include the Hessian-vector product used in \method{}. In Sec \ref{appx:proof} and \ref{appx:prooftheorem2}, we provide proofs of lemmas and theorems in the paper. In Sec \ref{appx:hparam}, we list hyperparameters used in the experiments.

\subsection{Hessian-Vector Product in L-BFGS}
\label{appx:hessianvec}
\begin{algorithm}[H]
\caption{Hessian-Vector in L-BFGS}
\label{alg:HessianVecProd}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE $\bm{g}_t$, $\left \{ \bm{y}_i \right \}_{i=1}^M, \left \{ \bm{s}_i \right \}_{i=1}^M$
\ENSURE $\bm{g}_t$
\FOR{$i = 0,\cdots,M-1$}
    \STATE $\rho_i = \bm{s}_i^T\cdot \bm{y}_i$
\ENDFOR
\FOR{$i = 0, \cdots, M-1$}
    \STATE $\alpha_i = \frac{\bm{s}_{M-i-1}^T\bm{g}_t}{\rho_{M-i-1}}$
    \STATE $\bm{g}_t = \bm{g}_t - \alpha_i\cdot\bm{y}_{M-i-1}$
\ENDFOR
\STATE $\bm{g}_t = \HI_0\cdot \bm{g}_t$ {\color{blue}\COMMENT{$\HI_0=$ $\frac{\bm{s}_{M-1}^T\bm{y}_{M-1}}{\bm{y}_{M-1}^T\bm{y}_{M-1}}\cdot I$}}
\FOR{$i = 0, \cdots, M-1$}
    \STATE $\beta_i = \frac{\bm{y}_i^T\bm{g}_t}{\rho_i}$
    \STATE $\bm{g}_t = \bm{g}_t + (\alpha_{M-i-1}-\beta_i)\cdot \bm{s}_i$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Proof of Lemmas and Theorems}\label{appx:proof}
\subsubsection{Proof of Lemma~\ref{lemma:noise}}
\label{subsec:prooflemma:noise}
\begin{proof}
Let $\Mn{t}$ denotes the stochastic noise in $\Mg{t}$, then it can be written as:

\begin{center}
    $\Mn{t} = \beta\Mn{t-1} + (1-\beta)\bm{n}_t= \beta^t\bm{n}_0+ (1-\beta)\sum_{i=1}^t\beta^{(t-i)}\bm{n}_i$
\end{center}

Since $\bm{n}_i$ for $i=0,\cdots,t$ are independent, therefore

\begin{center}
    $E\left [ \norm{\Mn{t}}^2 \right ]=\beta^{2t}E\left [ \norm{\bm{n}_0}^2 \right ] + \sum_{i=1}^t (1-\beta)^2\beta^{2(t-i)}E\left [ \norm{\bm{n}_i}^2 \right ]$
\end{center}

Since $E\left [ \norm{\bm{n}_i}^2 \right ]$ is bounded by $\epsilon^2$, therefore 

\begin{center}
    $E\left [ \norm{\Mn{t}}^2 \right ] \leq \beta^{2t}\epsilon^2 + \sum_{i=1}^t(1-\beta)^2\beta^{2(t-i)}\epsilon^2=\epsilon^2(\beta^{2t}+\frac{(1-\beta)^2}{1-\beta^2}(1-\beta^{2t}))$
\end{center}

It is obvious that $\lim_{t\rightarrow\infty}E\left [ \norm{\Mn{t}}^2 \right ]\leq \frac{1-\beta}{1+\beta}\epsilon^2$. 
% Therefore, $E\left [ \norm{\Mn{t}} \right] \leq \sqrt{E\left [ \norm{\Mn{t}}^2 \right]}\leq \sqrt{\frac{1-\beta}{1+\beta}}\epsilon$.

Furthermore, noise variance in $\yv{k}$ is bounded by $\lim_{t\rightarrow\infty}E\left [ \norm{\Mn{t}-\Mn{t-L}}^2 \right ] \leq \frac{4(1-\beta)}{1+\beta}\epsilon^2$.
\end{proof}

\subsubsection{Proof of Lemma~\ref{lemma:equivalence}}
\label{subsec:prooflemma:equiv}
\begin{proof}
First, consider the case with $T=1$, then

\begin{center}
    $\sv{0} = \Mp{1} - \Mp{0} = (1-\beta)(\btheta_1 - \btheta_0)$ \\
    $\yv{0} = \Mg{1} - \Mg{0} = (1-\beta)(\bm{g}_1 - \bm{g}_0)$
\end{center}

Since $\bm{g}_1-\bm{g}_0 = B(\btheta_1 - \btheta_0)$ in L-BFGS update, then it is also valid that $\yv{0} = B \sv{0}$.

Assume for $(k-1)$th approximation, $\yv{k} = \Mg{k}-\Mg{k-1} = B \sv{k} = B(\Mp{k} - \Mp{k-1})$.

Expand $\Mp{k}$, we can easily get $\yv{k}=\Mg{k}-\Mg{k-1}=(1-\beta)B(\btheta_k-\Mp{k-1})$.

Then, for $k$th approximation, we have

\begin{center}
    $B\sv{k+1} = B(\Mp{k+1} - \Mp{k}) = B(1-\beta)(\btheta_{k+1}-\Mp{k})$ \\
\end{center}

Further expand $\Mp{k}$, we get 
\begin{center}
    $B\sv{k+1} = (1-\beta)B(\btheta_{k+1}-\btheta_k+\beta(\btheta_k-\Mp{k-1}))$
\end{center}

Similarly, since $\bm{g}_{k+1}-\bm{g}_k = B(\btheta_{k+1} - \btheta_k)$, we have

\begin{center}
    $B\sv{k+1} = (1-\beta)(\bm{g}_{k+1}-\bm{g}_k) + \beta(\Mg{k}-\Mg{k-1}) = \yv{k}$.
\end{center}

Consider the case with $T>1$, $\yv{k}$ can be written as

\begin{center}
    $\yv{k} = \sum_{i=0}^{T-1}\Mg{g(k+1)T-i} - \Mg{g(k+1)T-i-1}$
\end{center}

According to the result with $T=1$, $\yv{k}$ can be further written as

\begin{center}
    $\yv{k} = \sum_{i=0}^{T-1}B(\Mp{g(k+1)T-i} - \Mp{g(k+1)T-i-1}) = B \sum_{i=0}^{T-1}(\Mp{g(k+1)T-i} - \Mp{g(k+1)T-i-1}) = B\sv{k}$. 
\end{center}

Therefore, it is equivalent to use the momentum based scheme to estimate the Hessian without losing accuracy compared to naive L-BFGS.

\end{proof}

\subsubsection{Proof of Theorem~\ref{theorem:simpleconvex}}
\label{subsec:prooftheorem:simpleconvex}
\begin{proof}
First, we show that $\frac{\left \langle \bm{s}_k, \bm{y}_k \right \rangle}{\left \langle \bm{s}_k, \bm{s}_k \right \rangle}$ is well bounded, so that most iterates using BFGS goes toward desirable direction according to \cite{BFGStool}.

Given $\left \langle \bm{s}_k, \bm{y}_k \right \rangle \geq \alpha\cdot\epsilon \left \| \bm{s}_k\right \|$, we have $\frac{\left \langle \bm{s}_k, \bm{y}_k \right \rangle}{\left \langle \bm{s}_k, \bm{s}_k \right \rangle} \geq \frac{\alpha\epsilon}{\left \| \sv{k} \right \|}$.

Let $\vvv{k}$ denotes gradient changes with no noise added, then $\yv{k} = \vvv{k} + \bm{n}_k - \bm{n}_{k-1}$. Since noise variance in $\yv{k}$ is bounded by $2\epsilon^2$, we have $E\left [ \norm{\bm{n}_k}^2 \right ] \leq \epsilon^2$.

Then,
\begin{center}
    $E\left [ \frac{\left \langle \bm{s}_k, \bm{y}_k \right \rangle}{\left \langle \bm{s}_k, \bm{s}_k \right \rangle} \right ] = E \left [ \frac{\left \langle \bm{s}_k, \bm{v}_k \right \rangle}{\left \langle \bm{s}_k, \bm{s}_k \right \rangle} - \frac{\left \langle \bm{s}_k, \bm{n}_k-\bm{n}_{k-1} \right \rangle}{\left \langle \sv{k}, \sv{k} \right \rangle}\right ]\geq \frac{\left \langle \bm{s}_k, \bm{v}_k \right \rangle}{\left \langle \bm{s}_k, \bm{s}_k \right \rangle}-\frac{2\epsilon}{\norm{\sv{k}}} \geq \lambda - \frac{2\epsilon}{\norm{\sv{k}}}$
\end{center}
According to the two inequalities above, we have $E\left [ \frac{\left \langle \bm{s}_k, \bm{y}_k \right \rangle}{\left \langle \bm{s}_k, \bm{s}_k \right \rangle} \right ] \geq \frac{\alpha}{\alpha+2}\lambda$.

As for the upper bound, first have $E\left [\inprod{\sv{k}}{ \yv{k}} \right ] = E\left [ \inprod{\sv{k}}{\vv{k}+\bm{n}_k-\bm{n}_{k-1}} \right ] \leq \inprod{\sv{k}}{\vv{k}} + 2\epsilon\norm{\sv{k}}$.

Because $H\preceq \Lambda I$, then $\inprod{\sv{k}}{\vv{k}}\leq \Lambda\norm{\sv{k}}^2$. Combined with $\inprod{\sv{k}}{\yv{k}}\geq \alpha\epsilon\norm{\sv{k}}$, we have 
\begin{equation}\label{eq:boundskyk}
    \norm{\sv{k}}\geq\frac{\alpha-2}{\Lambda}\epsilon,\quad
    \inprod{\sv{k}}{\yv{k}}\leq \norm{\sv{k}}(\Lambda\norm{\sv{k}}+2\epsilon)
\end{equation}.

With $\lambda I \preceq H \preceq \Lambda I$, we have
\begin{center}
    $\inprod{\sv{k}}{\vv{k}}\geq \frac{\lambda\Lambda}{\lambda+\Lambda}\norm{\sv{k}}^2+\frac{1}{\lambda+\Lambda}\norm{\vv{k}}^2$
\end{center}

Rearrange this inequality, we have
\begin{center}
    $\norm{\vv{k}-\frac{\lambda+\Lambda}{2}\sv{k}}\leq \frac{\Lambda-\lambda}{2}\norm{\sv{k}}$
\end{center}

Due to $\yv{k}=\vv{k}+\bm{n}_k - \bm{n}_{k-1}$, we can convert the equation above to
\begin{center}
    $E\left [\norm{\yv{k}-\frac{\lambda+\Lambda}{2}\sv{k}}^2 \right ]\leq (\frac{\Lambda-\lambda}{2}\norm{\sv{k}}+2\epsilon)^2$.
\end{center}

Expand the equation, we have
\begin{center}
    $E\left [\norm{\yv{k}}^2-(\Lambda+\lambda)\inprod{\sv{k}}{\yv{k}}+(\frac{\Lambda+\lambda}{2})^2\norm{\sv{k}}^2 \right ]\leq (\frac{\Lambda-\lambda}{2})^2\norm{\sv{k}}^2+2(\Lambda-\lambda)\norm{\sv{k}}\epsilon+4\epsilon^2$
\end{center}

Divide by $\inprod{\sv{k}}{\sv{k}}$ on both sides, we have
\begin{center}
    $E\left [\frac{\inprod{\yv{k}}{\yv{k}}}{\inprod{\sv{k}}{\sv{k}}} \right ]\leq E\left [\frac{(\Lambda+\lambda)\inprod{\sv{k}}{\yv{k}}}{\inprod{\sv{k}}{\sv{k}}} \right ] + \frac{(2\epsilon+\Lambda\norm{\sv{k}})(2\epsilon-\lambda\norm{\sv{k}})}{\inprod{\sv{k}}{\sv{k}}}$
\end{center}

According to Eq (\ref{eq:boundskyk}), we simplify the expectation as
\begin{align*}
    E\left [\frac{\inprod{\yv{k}}{\yv{k}}}{\inprod{\sv{k}}{\sv{k}}} \right ] &\leq \frac{(\Lambda+\lambda)(\Lambda\norm{\sv{k}}+2\epsilon)}{\norm{\sv{k}}}+\frac{(2\epsilon+\Lambda\norm{\sv{k}})(2\epsilon-\lambda\norm{\sv{k}})}{\inprod{\sv{k}}{\sv{k}}} \\
    &= \Lambda(\Lambda+\lambda) + \frac{2(\Lambda+\lambda)\epsilon}{\norm{\sv{k}}}+\frac{4\epsilon^2}{\norm{\sv{k}}^2} + \frac{2\Lambda\epsilon}{\norm{\sv{k}}}-\frac{2\lambda\epsilon}{\norm{\sv{k}}}-\Lambda\lambda \\
    & = \Lambda^2 + \frac{4\Lambda\epsilon}{\norm{\sv{k}}}+\frac{4\epsilon^2}{\norm{\sv{k}}^2} \\
    &\leq \Lambda^2+\frac{4\Lambda^2}{\alpha-2}+\frac{4\Lambda^2}{(\alpha-2)^2} = (\frac{\alpha}{\alpha-2}\Lambda)^2
\end{align*}

With the well-bounded Hessian approximation, we show that there is an lower bound for the optimization with noise involved. 

With $\inprod{\sv{k}}{\yv{k}}\geq \alpha\epsilon\norm{\sv{k}}$ and $E\left [ \norm{\bm{n}_t}^2 \right ] \leq \epsilon^2$, we get $\inprod{\sv{k}}{\yv{k}}=\inprod{\sv{k}}{\vv{k}+\bm{n}_{t+1}-\bm{n}_t}\leq \inprod{\sv{k}}{\vv{k}}+2\epsilon\norm{\sv{k}}$.

Then $\inprod{\sv{k}}{\vv{k}}\geq (\alpha-2)\epsilon\norm{\sv{k}}$.

With the upper bound of $H$, we have $(\alpha-2)\epsilon\norm{\sv{k}}\leq \Lambda\norm{\sv{k}}^2$.

With the lower bound of $H$, we have $\Loss(\btheta_t) - \Loss(\btheta_{t-1})\geq g_{t+1}(-\sv{k})+\frac{\lambda}{2}\norm{\sv{k}}^2$.

We focus on near-optimal region, where $\Loss({\btheta_{t+1}})=0$ and $g_{t+1} = \bm{0}$, then we get $\Loss(\btheta_t)\geq \frac{\lambda}{2}\norm{\sv{k}}^2 \geq \frac{(\alpha-2)^2\lambda}{2\Lambda^2}\epsilon^2$.

Therefore, the achievable loss by L-BFGS with noise is lower bounded by $\frac{(\alpha-2)^2\lambda}{2\Lambda^2}\epsilon^2$. 
\end{proof}

\subsubsection{Proof of Hessian Damping in Eq (\ref{eq::ydamping})}
\label{subsec:prooflemma:damping}
\begin{proof}
According to Eq (\ref{eq::ydamping}), $\sv{i}^T\yyv{i}=\sv{i}^T(\tau\yv{i} + (1-\tau)\sv{i})=(\mu\tau+1-\tau)\sv{i}^T\sv{i}$, where $\mu=\frac{\sv{i}^T\yv{i}}{\sv{i}^T\sv{i}}$.

For $\mu\leq\sigma_L$, two cases need to be considered: % 1) $\tau=\tau_0$; 2) $\tau=\frac{1-\sigma_L}{1-\mu}$.

If $\tau=\tau_0$, then $\frac{1-\sigma_L}{1-\mu} \geq \tau_0$,
and $\mu\tau+1-\tau \geq \sigma_L$.

If $\tau=\frac{1-\sigma_L}{1-\mu}$, then $\mu\tau+1-\tau=\sigma_L$

Therefore, when $\mu\leq\sigma_L$, $\sv{i}^T\yyv{i}\geq\sigma_L\sv{i}^T\sv{i}$\\

For $\sigma_L < \mu < \sigma_H$:

We can write $\mu\tau+1-\tau=\mu\tau_0+1-\tau_0$.
It is easy to show that 
\begin{center}
    $\mu\tau_0+1-\tau_0-\sigma_L \geq (1-\sigma_L)(1-\tau_0) > 0$
    
    $\mu\tau_0+1-\tau_0-\sigma_H \leq (1-\sigma_H)(1-\tau_o) < 0$
\end{center}

Therefore, when $\sigma_L < \mu < \sigma_H$, $\sigma_L\sv{i}^T\sv{i} < \sv{i}^T\yyv{i} < \sigma_H\sv{i}^T\sv{i}$.\\

For $\mu\geq\sigma_H$, similarly two cases might arise: % 1) $\tau=\tau_0$; 2) $\tau=\frac{\sigma_H-1}{\mu-1}$.

If $\tau=\tau_0$, then $\frac{\sigma_H-1}{\mu-1} \geq \tau_0$,
and $\mu\tau+1-\tau \leq \sigma_H$.

If $\tau=\frac{\sigma_H-1}{\mu-1}$, then $\mu\tau+1-\tau=\sigma_H$.

Therefore, when $\mu\geq\sigma_H$, $\sv{i}^T\yyv{i}\leq\sigma_H\sv{i}^T\sv{i}$\\

In summary, $\sigma_L \leq \frac{\bm{s}_i^T\cdot\hat{\bm{y}}_i}{\bm{s}_i^T\cdot\bm{s}_i} \leq \sigma_H$.
\end{proof}

\subsubsection{Proof of Lemma \ref{lemma:boundsubHessian}}
\label{subsec:prooflemma:boundsubHessian}
\begin{proof}
For any arbitrary vector $\bm{x} \neq \bm{0}$, $\bm{x}^T\cdot\HI_k\cdot\bm{x}$ can be written as 

$(\bm{x}^{1^T}\cdot\HI^1_k,\cdots, \bm{x}^{p^T}\cdot\HI^p_k)\cdot (\bm{x}^{1^T},\cdots, \bm{x}^{p^T})^T=\sum_{i=1}^p \bm{x}^{i^T}\cdot\HI^i_k\cdot\bm{x}^i$,

where $\bm{x}^i$ is a sub-vector corresponding to block $i$.

Let $\xi \equiv \min\xi^i$ and $\Xi \equiv \max\Xi^i$, therefore, $\xi \leq \bm{x}^T\cdot\HI_k\cdot\bm{x} \leq \Xi$. 
\end{proof}

\subsection{Proof of Theorem~\ref{theorem:convergence}}
\label{appx:prooftheorem2}

To prove Theorem~\ref{theorem:convergence}, we need several lemmas to bound the Hessian approximation and the gradient variance of the objective function.

\subsubsection{Some Lemmas for Theorem~\ref{theorem:convergence}}

\begin{lemma}\label{lemma:HIbound}
Given damping scheme in Eq (\ref{eq::ydamping}), at the $k$-th Hessian update, $\HI_k$ during the optimization is bounded by $\xi I \preceq \HI_k \preceq \Xi I$, where $\Xi = (M+1)\frac{1}{\sigma_L}$, and $\xi = \frac{1}{\sigma_H}$. % $\sigma_L$ and $\sigma_H$ is the lower and upper bound for $\frac{\bm{s}_i^T\cdot\hat{\bm{y}}_i}{\bm{s}_i^T\cdot\bm{s}_i}$ in Lemma~\ref{lemma:damping}.
\end{lemma}
\begin{proof}
\textbf{Lower bound}: 
$\HI$ is initialized as $\HI_0 = \frac{\sv{0}^T\yyv{0}}{\yyv{0}^T\yyv{0}}\cdot I$. 
According to the damping scheme \ref{subsec:prooflemma:damping}, there exists $H_0(\btheta)\preceq \sigma_H I$ such that $\yyv{0} = H_0\cdot\sv{0}$.

Therefore, $\frac{\sv{0}^T\yyv{0}}{\yyv{0}^T\yyv{0}}\cdot I = \frac{\sv{0}^TH_0\sv{0}}{\sv{0}^TH_0\cdot H_0\sv{0}}\cdot I=\frac{(\sv{0}^TH_0^{1/2})(H_0^{1/2}\sv{0})}{(\sv{0}^TH_0^{1/2})\cdot H_0\cdot(H_0^{1/2}\sv{0})}\cdot I \succeq \frac{1}{\sigma_H} I$.
% Because $H_0(\btheta)\preceq \sigma_H I$, we have $\frac{\sv{0}^T\yyv{0}}{\yyv{0}^T\yyv{0}}\cdot I \succeq \frac{1}{\sigma_H} I$

Then for $k\geq 1$, assuming $\HI_{k-1} \succeq \frac{1}{\sigma_H}I$ hold, based on Eq (\ref{eq::BFGSupdate}), $\HI_{k} = (I - \rho_k\yyv{k}\sv{k}^T)^T\HI_{k-1}(I-\rho_k\yyv{k}\sv{k}^T)+\frac{\sv{k}\sv{k}^T}{\sv{k}^T\yyv{k}}$.

Because $(I - \rho_k\yyv{k}\sv{k}^T)^T\HI_{k-1}(I-\rho_k\yyv{k}\sv{k}^T)$ is positive definite, we can bound $\HI_k$ as: $\HI_k \succeq \frac{\sv{k}\sv{k}^T}{\sv{k}^T\yyv{k}} = \frac{\sv{k}\sv{k}^T}{\sv{k}^T\cdot H_k\cdot\sv{k}} \succeq \frac{1}{\sigma_H} I$

Therefore, lower bound of $\HI_k$, $\xi = \frac{1}{\sigma_H}$.

\textbf{Upper bound}:
Since $H_0(\btheta) \succeq \sigma_L$, we can get
$\frac{\sv{0}^T\yyv{0}}{\yyv{0}^T\yyv{0}}\cdot I = \frac{(\sv{0}^TH_0^{1/2})(H_0^{1/2}\sv{0})}{(\sv{0}^TH_0^{1/2})\cdot H_0\cdot(H_0^{1/2}\sv{0})}\cdot I \preceq \frac{1}{\sigma_L}$.

Similarly, for $k\geq 1$, we assume $\HI_{k-1} \preceq k\frac{1}{\sigma_L}$ hold. 

For the first part in Eq (\ref{eq::BFGSupdate}), let $Q = (I - \rho_k\yyv{k}\sv{k}^T)$. Then for $\forall$ $\bm{x} \neq \bm{0}$, $\bm{x}^T\cdot Q^T \HI_{k-1} Q \cdot \bm{x} = (Q\bm{x})^T\cdot\HI_{k-1}\cdot (Q\bm{x}) \leq \frac{k}{\sigma_L}\bm{x}^T\cdot(Q^TQ)\cdot\bm{x}$.

Let $P = \frac{\sv{k}\yyv{k}^T\yyv{k}\sv{k}^T}{\sv{k}^T\yyv{k}\sv{k}^T\yyv{k}} - \frac{\yyv{k}\bm{s_k^T}+\sv{k}\yyv{k}^T}{\sv{k}^T\yyv{k}}$, then $Q^TQ = I + P$.

Because $P$ is rank-1 matrix with eigenvalue $-1$ and eigenvector $\yyv{k}$,
we have $\bm{x}^T\cdot Q^T \HI_{k-1} Q \cdot \bm{x} \leq \frac{k}{\sigma_L}\bm{x}^T\cdot(I+P)\cdot\bm{x} \leq \frac{k}{\sigma_L}\bm{x}^T\bm{x}$

For the second part in Eq (\ref{eq::BFGSupdate}), we can directly get $\frac{\sv{k}\sv{k}^T}{\bm{s}^T\bm{y}_k} = \frac{\sv{k}\sv{k}^T}{\sv{k}^T\cdot H_k \cdot \sv{k}} \preceq \frac{1}{\sigma_L}I$.

Therefore, $\bm{x}^T\cdot\HI_k\cdot\bm{x} \leq \frac{k}{\sigma_L}\bm{x}^T\bm{x} + \frac{1}{\sigma_L}\bm{x}^T\bm{x} = \frac{k+1}{\sigma_L} \bm{x}^T\bm{x}$

In \method{}, $k$ is at most $M$, which is the length of history vector, therefore $\Xi = (M+1)\frac{1}{\sigma_L}$.

In summary, $\frac{1}{\sigma_H} I \preceq \HI_k \preceq (M+1)\frac{1}{\sigma_L} I$
\end{proof}

\begin{lemma}\label{lemma:gradvariance}
Assume AS~\ref{as:smooth} holds, then loss function $\mathcal{L}(\btheta)$ is at least $\Lambda$-smooth. At iteration $t$ with mini-batch input $\mathcal{S}_t$, where each sample is randomly sampled from  $\mathcal{X}$ with replacement, gradient $\nabla\mathcal{L}(\btheta_t; \mathcal{S}_t)$ satisfies
\begin{center}
    $E_{\mathcal{S}_t}[\norm{\grad{L}(\btheta_t;\mathcal{S}_t)}^2] \leq 2\Lambda\cdot\mathcal{L}(\btheta_t)$.
\end{center}
\end{lemma}
\begin{proof}
\textbf{Smoothness of} $\mathcal{L}(\btheta)$.

Given AS \ref{as:smooth} hold, we have
$\norm{\grad{\ell}_i(\btheta_1)-\grad{\ell}_i(\btheta_2)} \leq \Lambda \norm{\btheta_1 - \btheta_2}$.

For $\Loss$, we have
$\grad{\Loss}(\btheta_1) - \grad{\Loss}(\btheta_2) = \frac{1}{N}\sum_{i=1}^N \grad{\ell}_i(\btheta_1)-\grad{\ell}_i(\btheta_2)$.

Then,
\begin{align*}
    \norm{\grad{\Loss}(\btheta_1) - \grad{\Loss}(\btheta_2)} &= \frac{1}{N}\norm{\sum_{i=1}^N \grad{\ell}_i(\btheta_1)-\grad{\ell}_i(\btheta_2)}\\
    &\overset{\text{TI}}{\leq} \frac{1}{N}\sum_{i=1}^N \norm{\grad{\ell}_i(\btheta_1)-\grad{\ell}_i(\btheta_2)}\\
    &\leq \frac{1}{N}\sum_{i=1}^N \Lambda\norm{\btheta_1 - \btheta_2}\\
    &= \Lambda\norm{\btheta_1 - \btheta_2}
\end{align*}
``TI" indicates triangle inequality.
Therefore, $\Loss$ is at least $\Lambda$-smooth.


\textbf{Gradient variance bound}.

$E_{\mathcal{S}_t}[\norm{\grad{L}(\btheta_t;\mathcal{S}_t)}^2] = E_{\mathcal{S}_t}[\left \langle {\frac{1}{b}\nabla\sum_{i=1}^b \ell_i(\btheta_t)}, {\frac{1}{b}\nabla\sum_{i=1}^b \ell_i(\btheta_t)} \right \rangle]$\\

Expand summation and regroup,

$E_{\mathcal{S}_t}[\norm{\grad{L}(\btheta_t;\mathcal{S}_t)}^2] = E_{\mathcal{S}_t}[\frac{1}{b^2}\sum_{i=1}^b\norm{\nabla\ell_i(\btheta_t)}^2 + \frac{1}{b^2}\sum_{i=1}^b\sum_{j=1,\neq i}^b \left \langle \nabla\ell_i(\btheta_t), \nabla\ell_j(\btheta_t) \right \rangle]$\\

Take expectation on each sample,

$E_{\mathcal{S}_t}[\norm{\grad{L}(\btheta_t;\mathcal{S}_t)}^2] = \frac{1}{b^2}\sum_{i=1}^b E_{\bm{x}_i}[\norm{\nabla\ell_i(\btheta_t)}^2] + \frac{1}{b^2}\sum_{i=1}^b\sum_{j=1,\neq i}^b E_{\bm{x}_i, \bm{x}_j}[\left \langle \nabla\ell_i(\btheta_t), \nabla\ell_j(\btheta_t) \right \rangle]$\\

Because $\bm{x}_i$ and $\bm{x}_j$ are independent, the second part can be simplified as,

$E_{\mathcal{S}_t}[\norm{\grad{L}(\btheta_t;\mathcal{S}_t)}^2] = \frac{1}{b^2}\sum_{i=1}^b E_{\bm{x}_i}[\norm{\nabla\ell_i(\btheta_t)}^2] + \frac{1}{b^2}\sum_{i=1}^b\sum_{j=1,\neq i}^b \norm{\grad{\mathcal{L}}(\btheta_t)}^2$\\

With further simplification, we get

$E_{\mathcal{S}_t}[\norm{\grad{L}(\btheta_t;\mathcal{S}_t)}^2] = \frac{1}{b^2}\sum_{i=1}^b E_{\bm{x}_i}[\norm{\nabla\ell_i(\btheta_t)}^2] + \frac{b-1}{b}\norm{\grad{\mathcal{L}}(\btheta_t)}^2$\\

Given AS \ref{as:smooth}, we have
$\norm{\nabla\ell_i(\btheta_t)}^2 \leq 2\Lambda\cdot\ell_i(\btheta_t)$ and 
$\norm{\grad{\mathcal{L}}(\btheta_t)}^2 \leq 2\Lambda\cdot\mathcal{L}(\btheta_t)$\\

Therefore, 
\begin{align*}
    E_{\mathcal{S}_t}[\norm{\grad{L}(\btheta_t;\mathcal{S}_t)}^2] &\leq \frac{1}{b^2}\sum_{i=1}^b E_{\bm{x}_i}[2\Lambda\ell_i(\btheta_t)] + \frac{b-1}{b} 2\Lambda\mathcal{L}(\btheta_t)\\
    &= \frac{1}{b}2\Lambda\mathcal{L}(\btheta_t) + \frac{b-1}{b}2\Lambda\mathcal{L}(\btheta_t) \\
    &= 2\Lambda\cdot\mathcal{L}(\btheta_t)
\end{align*}
\end{proof}

\subsubsection{Proof of Theorem~\ref{theorem:convergence}}
\begin{proof}
Given AS~\ref{as:smooth} and Lemma~\ref{lemma:gradvariance}, $\Loss(\btheta_{t})$ can be bounded by $\Loss(\btheta_{t})\leq \Loss(\btheta_{t-1}) + \nabla\Loss(\btheta_{t-1})^T(\btheta_{t} - \btheta_{t-1}) + \frac{\Lambda}{2}\left \| \btheta_{t}-\btheta_{t-1} \right \|^2$ for $\forall \btheta_{t-1}, \btheta_{t}$

In \method{}, $\btheta_{t} = \btheta_{t-1} - \eta_{t-1}\HI_k\nabla\mathcal{L}(\btheta_{t-1};\mathcal{S}_{t-1})$.
Therefore, we can upper bound $\Loss(\btheta_{t})$ as:
\begin{align*}
    \Loss(\btheta_{t}) \leq &\Loss(\btheta_{t-1})-\eta_{t-1}\cdot\nabla\Loss(\btheta_{t-1})^T\HI_k\nabla\Loss(\btheta_{t-1};\mathcal{S}_{t-1})\\
    &+n_{t-1}^2\frac{\Lambda}{2}\left \| \HI_k\nabla\Loss(\btheta_{t-1};\mathcal{S}_{t-1}) \right \|^2\\
    \leq &\Loss(\btheta_{t-1})-\eta_{t-1}\cdot\nabla\Loss(\btheta_{t-1})^T\HI_k\nabla\Loss(\btheta_{t-1};\mathcal{S}_{t-1})\\
    &+n_{t-1}^2\frac{\Lambda\Xi^2}{2}\left \| \nabla\Loss(\btheta_{t-1};\mathcal{S}_{t-1}) \right \|^2
\end{align*}

Since $\HI_k$ is independent with $\mathcal{S}_{t-1}$, we take expectation w.r.t $\mathcal{S}_{t-1}$ and $\mathcal{S}_{t}$,
\begin{align*}
    E_{\mathcal{S}_{t}}[\Loss(\btheta_{t})|\mathcal{S}_{t-1}] \leq 
    & \Loss(\btheta_{t-1}) - \eta_{t-1}\cdot\nabla\Loss(\btheta_{t-1})^T\HI_kE_{\mathcal{S}_{t-1}}[\nabla\Loss(\btheta_{t-1};\mathcal{S}_{t-1})]\\
    & + n_{t-1}^2\frac{\Lambda\Xi^2}{2}E_{\mathcal{S}_{t-1}}[\left \| \nabla\Loss(\btheta_{t-1};\mathcal{S}_{t-1}) \right \|^2]\\
    = & \Loss(\btheta_{t-1}) - \eta_{t-1}\cdot\nabla\Loss(\btheta_{t-1})^T\HI_k\nabla\Loss(\btheta_{t-1})\\
    & + n_{t-1}^2\frac{\Lambda\Xi^2}{2}E_{\mathcal{S}_{t-1}}[\left \| \nabla\Loss(\btheta_{t-1};\mathcal{S}_{t-1}) \right \|^2]\\
\end{align*}

According AS~\ref{as:pl}, we have
\begin{center}
    $L(\btheta_{t-1}) \leq \frac{1}{\lambda}\norm{\grad{\Loss}(\btheta_{t-1})}^2$
\end{center}

According to Lemma~\ref{lemma:gradvariance}, we have
\begin{center}
    $E_{\mathcal{S}_{t-1}}[\norm{\grad{\Loss}(\btheta_{t-1}; \mathcal{S}_{t-1})}^2] \leq 2\Lambda\Loss(\btheta_{t-1})$
\end{center}

Therefore, $E_{\mathcal{S}_{t}}[\Loss(\btheta_{t})|\mathcal{S}_{t-1}] \leq
    \mathcal{L}(\btheta_{t-1}) -\eta_{t-1}\lambda\xi\Loss(\btheta_{t-1})+\eta_{t-1}^2\Lambda^2\Xi^2\Loss(\btheta_{t-1})$.
    
After simply regrouping, we can get
\begin{center}
    $E_{\mathcal{S}_{t}}[\Loss(\btheta_{t})|\mathcal{S}_{t-1}] \leq
    (1-\eta_{t-1}\lambda\xi + \eta_{t-1}^2\Lambda^2\Xi^2)[\Loss(\btheta_{t-1}]$
\end{center}

Apply total expectation rule w.r.t $\mathcal{S}_t$, we have
\begin{center}
    $E_{\mathcal{S}_{t}}[\Loss(\btheta_{t})] \leq (1-\eta_{t-1}\lambda\xi + \eta_{t-1}^2\Lambda^2\Xi^2 ) E_{\mathcal{S}_{t-1}}[\Loss(\btheta_{t-1})]$
\end{center}
\end{proof}


\subsection{Hyperparameter Settings}\label{appx:hparam}
\subsubsection{CIFAR-10/CIFAR-100}\label{appx:hparam:cifar}
For ResNet-18 and DeiT, detailed settings are shown in Table \ref{tab:hyperparam:resnet18cifar}. The batch size is set to be $256$ for all optimizers. For the learning rate, we use a cosine annealing scheduling strategy with a minimum learning rate of 0.0001. Maximum epoch is $150$ for ResNet-18 and $50$ for DeiT.

\begin{table}[htb]
\centering
\caption{\footnotesize Hyperparameters for SGD, Adam, \method{} on CIFAR-10/CIFAR-100}
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\toprule
 Optimizer & $b$ & lr & momentum & $wd$ & $\tau_0$ & $\beta$ & $T$ & $M$\\
 \midrule
 SGD & 256 & 0.1 & 0.9 & 1e-4 & - & - & - &-\\
 Adam & 256 & 0.01 & 0.9/0.999 & 1e-2 & - & - & - &-\\
 \method{} & 256 & 0.1 & 0.9 & 2e-4 & 0.99 & 0.999 & 50 & 10\\
\bottomrule
\end{tabular}
\label{tab:hyperparam:resnet18cifar}\\
\footnotesize{$\beta$: momentum for the Hessian; $\tau_0$: initial damping; $T$: frequency for updating the Hessian; $M$: length of history vector ($\sv{}, \yv{}$)}
\end{table}

\subsubsection{ImageNet}\label{appx:hparam:imagenet}
For ResNet50, detailed settings are shown in Table \ref{tab:hyperparam:resnet50}. The batch size is set to be $512$ for all optimizers. For the learning rate, we used a cosine annealing scheduling strategy with a minimum learning rate of 0.0001. Maximum epoch is $100$.

\begin{table}[htb]
\centering
\caption{\footnotesize Hyperparameters for SGD, Adam, KFAC and \method{} of ResNet50 on ImageNet}
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\toprule
 Optimizer & $b$ & lr & momentum & $wd$ & $\tau_0$ & $\beta$ & $T$ & $M$\\
 \midrule
 SGD & 512 & 0.1 & 0.9 & 1e-4 & - & - & - &-\\
 Adam & 512 & 0.07 & 0.9/0.999 & 3e-2 & - & - & - &-\\
 KFAC & 512 & 0.06 & 0.9 & 3e-2 & - & - & 50 &-\\
 \method{} & 256 & 0.1 & 0.9 & 2e-4 & 0.99 & 0.999 & 50 & 10\\
\bottomrule
\end{tabular}
\label{tab:hyperparam:resnet50}\\
\footnotesize{$\beta$: momentum for the Hessian; $\tau_0$: initial damping; $T$: frequency for updating the Hessian; $M$: length of history vector ($\sv{}, \yv{}$)}
\end{table}

\subsection{Ablation Study: Impact of Granularity of Block-wise Approximation}

% Figure environment removed

We study the impact of granularity of block-wise approximation in this section. We train ResNet-18 on CIFAR-100 using mL-BFGS with a different number of Hessian blocks: 1, 2, 4 blocks. mL-BFGS with one block approximates the whole Hessian matrix, while mL-BFGS with two blocks approximates two diagonal Hessian blocks with each one consisting of 4 \emph{resblocks} in ResNet-18. At last, mL-BFGS with four blocks is the default setting in our CIFAR-100 experiment, where each block consists of 2 \emph{resblocks}. As shown in Figure \ref{fig:ablation:blocks}, as we increase the size of the blocks, mL-BFGS converges faster and achieve better generalization performance. This observation aligns with the argument that mL-BFGS can estimate more accurate curvature information by approximating a large Hessian block, improving training performance.

\subsection{Justification of PL-condition in Real Neural Networks}
% Figure environment removed

Figure \ref{fig:ablation:pl} shows gradient norm and loss when training ResNet-18 on CIFAR-100. We can observe that with an appropriate parameter $\lambda$, the PL-condition can be easily satisfied.

\subsection{Other Second-order Methods on CIFAR-10/100}

% Figure environment removed

Figure \ref{fig:cifar:more} show training loss of using mL-BFGS, KFAC, Adam, and SGD on CIFAR-10/100. Compared to the second-order method KFAC, mL-BFGS converges faster as it can estimate more accurate curvature information by approximating large Hessian blocks. 
On the validation dataset, mL-BFGS achieves higher accuracy than KFAC (KFAC on CIFAR-10: $93.1\pm0.1$, CIFAR-100: $73.2\pm0.13$).