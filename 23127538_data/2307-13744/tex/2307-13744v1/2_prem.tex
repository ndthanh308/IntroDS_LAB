% In this paper we consider a typical empirical loss minimization problem of the form $\underset{\theta}{\min}\text{ }\mathcal{L}(\btheta, \mathcal{X}) := \frac{1}{N}\sum_{i=1}^N\ell(\btheta, \bm{x}_i)$,
%\begin{equation*}\label{eq::Loss}
%    \underset{\theta}{\min}\text{ }\mathcal{L}(\btheta, \mathcal{X}) := \frac{1}{N}\sum_{i=1}^N\ell(\btheta, \bm{x}_i),
%\end{equation*}
% where $\btheta$ denotes the parameters of the model to be optimized, and $\mathcal{X}=\left \{ \bm{x}_i \right \}_{i=1}^N$ are the training data where $\bm{x}_i$ consists of both features and labels. 
The risk function $\mathcal{L}(\btheta, \mathcal{X})$ in Eq (\ref{eq:loss}) is usually optimized through a form of gradient descent as:
\begin{equation}\label{eq:ParamUpdateQN}
    \btheta_{t+1} = \btheta_{t} - \eta_{t}\cdot \HI\cdot\bm{g}_t,
\end{equation}
where $\eta_{t}$ denotes step size (learning rate) at iteration $t$ and $\HI$ is a gradient pre-conditioner.
% that uses the local gradients $\bm{g}_t=\nabla_{\btheta} \mathcal{L}(\btheta_t)$ directly to update the model parameters via iterates of the form  
%\begin{equation}\label{eq::ParamUpdateGD}
%    \btheta_{t+1} = \btheta_{t} - \eta_{t}\bm{g}_t,
%\end{equation}
%where $\eta_{t}$ denotes the step size (learning rate) at iteration $t$. However, such GD updates are typically slow especially for ill-conditioned problems \citep{convexOpt}. To speed up the convergence, often second-order methods are used. In particular, Quasi-Newton (QN) methods find an approximate Hessian inverse $\HI$ to pre-condition the gradient vector and apply the following update to minimize the loss:
In stochastic training, gradients are evaluated on a mini-batch input $\tX_t \subseteq \mathcal{X}$, namely $\bm{g}_t=\nabla_{\btheta} \mathcal{L}(\btheta_t, \tX_t)$. 

If $\HI$ is an identity matrix, the update above is reduced to SGD, whereas if $\HI$ is a diagonal matrix, it becomes an adaptive training algorithm such as Adagrad \citep{2011_JMLR_AdaGrad} or Adam \citep{2014_arXiv_Adam}. 
To further improve convergence performance, esp. in an ill-conditioned problem \citep{convexOpt}, it is desired to incorporate more second-order information into $\HI$ as done in quasi-Newton (QN) methods.

A prime challenge in QN methods is the evaluation of $\Hm$ and in particular its inverse. 
A well-known Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm \citep{BFGS} addresses the challenge by formulating the Hessian inverse as a minimization problem:
\begin{equation}\label{eq:BFGSformulation}
\begin{split}
    & \min_{\HI} \quad\left \| \HI - \HI_{k-1}\right \|^2, \\
    & \text{s.t.} \quad\HI\cdot \bm{y}_k = \bm{s}_k,\quad \HI \text{ is symmetric},
\end{split}
\end{equation}
where $\bm{s}_k = \btheta_k - \btheta_{k-1}$ denotes the parameter changes, and $\bm{y}_k = \bm{g}_k-\bm{g}_{k-1} $ the gradient changes in two consecutive updates \footnote{$k$ rather than $t$ is used in the equation as parameter/gradient used might be different from the one in  Eq (\ref{eq:ParamUpdateQN})}.
By imposing the \emph{secant} condition during minimization, BFGS gradually attains the curvature information close to the real Hessian. \cite{2021_SIAM_Superlinear} establishes that BFGS converges to the real Hessian by a greedy strategy of choosing $(\vs_k, \vy_k)$.

Knowing $\HI_{k-1}$, the current $\HI$ is obtained via:
\begin{equation}\label{eq::BFGSupdate}
    \HI_k = (I - \rho_k\bm{y}_k\bm{s}_k^T)^T\HI_{k-1}(I-\rho_k\bm{y}_k\bm{s}_k^T)+\rho_k\bm{s}_k\bm{s}_k^T,
\end{equation}
where $\rho_k = \frac{1}{\bm{y}_k^T \bm{s}_k}$. 
Hence the Hessian inverse $\HI$ is constructed in an iterative manner with no need to compute the Hessian matrix.
% \red{Given such an update rule, methods such as Greedy BFGS \citep{2021_SIAM_Superlinear} show $\HI$ can converge to the real Hessian at a linear rate.}
% By carefully choosing $\bm{s}_k$, $\HI_k$ will converge to the real Hessian inverse at a linear rate for any strongly convex function \citep{2021_SIAM_Superlinear}. 

%In real-world problems, $\btheta$ usually consists of millions of parameters. As a result, it is infeasible to store the whole $\HI_k$ matrix with $O(\Dim{\btheta}^2)$ memory cost. 
To simplify computation in the Hessian-vector product, $\HI$ in BFGS is stored in the form of a sequence of history vectors $\left \{ \bm{y}_i \right \}$ and $\left \{ \bm{s}_i \right \}$. The matrix-vector product $\HI_k\cdot\bm{g}_t$ is replaced by a sequence of fast vector-vector products as shown in Algorithm~\ref{alg:HessianVecProd} (See Appendix \ref{appx:hessianvec}).
Furthermore, a limited-memory BFGS, L-BFGS \citep{1980_LBFGS} is usually adopted that only uses several latest history vectors when approximating the Hessian inverse.
% Furthermore, the number of history vectors ($\bm{y}_i$ and $\bm{s}_i$) that need to be stored for the Hessian approximation constantly increases in each iteration as the optimization continues, and will quickly exhaust our finite available hardware resources.
%To limit memory utilization and compute cost, a limited-memory version of BFGS, L-BFGS \cite{1980_LBFGS} is proposed that only uses the latest $M$ history vectors when approximating the Hessian inverse. The complete algorithm is shown in Algorithm~\ref{alg::HessianVecProd}.