\method{} consists of two crucial techniques to improve convergence stability and scalability of using L-BFGS in large-scale models.
First, \method{} introduces momentum into the Hessian approximation. The momentum-based design effectively reduces the adverse effects of stochastic noise while without resorting to costly noise-reduction techniques as in current solutions \citep{2016_PMLR_LBFGS}. 
Second, in distributed training, \method{} allows a block-wise approximation along arbitrary diagonal blocks. \method{} can flexibly assign each computing node a block that comprises multiple layers to distribute the workload.
We describe the method in detail below.

\subsection{Momentum-based Hessian: Reduce Effects of Stochastic Noise}
While momentum is widely used in first-order methods, it is rarely explored in the second-order domain.
Surprisingly, we find that momentum is also a crucial component for a stable Hessian approximation. 
Furthermore, compared to other noise-reduction methods, the momentum-based design is almost cost-free. 

In this paper, we apply momentum to past parameters and gradients as
\begin{equation}\label{eq:momentum}
\begin{split}
    \btheta: \Mp{t} &= \beta\cdot\Mp{t-1} + (1-\beta)\btheta_t, \\
    \bm{g}: \Mg{t} &= \beta\cdot\Mg{t-1} + (1-\beta)\bm{g}_t,
\end{split}
\end{equation}
where $\btheta_t,\vg_t$ denotes parameters and gradients at $t$-th iteration, $\beta$ is the momentum coefficient.

Following the BFGS update rule in Eq~(\ref{eq:BFGSformulation}) and assuming that $\HI$ is updated for every $T$ mini-batch iterations, $\bm{s}_k$ and $\bm{g}_k$ are obtained as
\begin{equation}\label{eq:skyk}
    \bm{s}_k = \Mp{(k+1)T} - \Mp{kT}, \quad \bm{y}_k = \Mg{(k+1)T} - \Mg{kT}.
\end{equation}
%During training practical DL models, it is acceptable to update $\HI$ after a sufficient number of mini-batch iterations. Therefore, momentumized parameters $\Mp{t}$ and gradients $\Mg{t}$ is capable of reducing stochastic noise and accumulate useful history statistics.

This simple but effective technique works surprisingly well when gradients are noisy. To intuitively show improvements of using momentum over the vanilla L-BFGS, we visualize the stochastic optimization of a simple quadratic loss function, $\mathcal{L} = \frac{1}{2}\left \| \btheta \right \|^2$. 
We simulate gradients in stochastic settings at iteration $t$ as $\bm{g}_t=\btheta_t + \bm{n}_t$, where $\bm{n}_t$ denotes the \emph{stochastic noise}. 
We model the noise as i.i.d. Gaussian, namely $\bm{n}_t \sim \mathcal{N}(0, \sigma^2)$. 
Figure~\ref{fig::naiveopt} shows optimization trajectories for SGD, vanilla L-BFGS, L-BFGS with momentum and the exact 2nd-order method given $\sigma=0.2$. 
With the same initial point, we observe that L-BFGS with momentum is as fast as the exact 2nd-order method, and much faster than SGD. On the other hand, vanilla L-BFGS obviously suffers convergence issues due to noisy $\bm{y}_k$.

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    % Figure removed
    \caption{\footnotesize Optimization using SGD, vanilla L-BFGS, L-BFGS with momentum ($\beta=0.9$) and exact 2nd-order method. Vanilla L-BFGS fails to find the desirable optimization path. However, momentum reduces noise in gradients and significantly stabilizes the optimization.}
    \label{fig::naiveopt}
    \vspace{-0.2cm}
\end{wrapfigure}

\textbf{Analysis - } We use a quadratic function as a showcase to analyze important theoretical properties of the momentum-based design: 
\begin{equation*}
    \Loss(\btheta) = \Loss(\btheta_0) + \bm{g}(\btheta_0)(\btheta - \btheta_0) + \frac{1}{2}(\btheta-\btheta_0)^{*}B(\btheta-\btheta_0).
\end{equation*}
The following lemmas and theorem establish theoretical improvement of using momentum over vanilla L-BFGS.
Specifically, Lemma \ref{lemma:noise} shows that momentum in Eq (\ref{eq:momentum}) significantly reduces stochastic noise in gradient changes $\yv{k}$. 
Lemma \ref{lemma:equivalence} further provides a theoretical guarantee that using momentum can still obtain the same Hessian approximation as the vanilla L-BFGS algorithm in the noise-free case. 

Finally, Theorem \ref{theorem:simpleconvex} shows that in stochastic settings, the achievable loss by L-BFGS is lower bounded by a value depending on noise variance in $\vy_k$. Combined with Lemma \ref{lemma:noise} and \ref{lemma:equivalence}, it is obvious that the momentum-based approximation achieves a much lower bound compared to the vanilla version. Such a conclusion is aligned with our empirical observations in Figure \ref{fig::naiveopt}.

\begin{lemma}\label{lemma:noise}
Given $\mathcal{L}(\btheta)$, and assuming $\bm{g}_t$ in iteration $t$ contains Gaussian noise $\bm{n}_t$ with zero mean and $E[\left \| \bm{n}_t \right \|^2]\leq \epsilon^2$, then the noise variance in $\Mg{t}$ in Eq (\ref{eq:momentum}) is reduced to $\frac{1-\beta}{1+\beta}\epsilon^2$ as $t\rightarrow\infty$. Furthermore, the noise variance in $\yv{k}$ is reduced to $\frac{4(1-\beta)}{1+\beta}\epsilon^2$
\end{lemma}

\begin{lemma}\label{lemma:equivalence}
Considering $\mathcal{L}(\btheta)$ above, with no noise involved, Eq (\ref{eq:skyk}) obtains the same Hessian approximation as the vanilla L-BFGS.
\end{lemma}

\begin{theorem}\label{theorem:simpleconvex}
Considering $\mathcal{L}(\btheta)$ with $\lambda I \preceq B \preceq \Lambda I$ where $\Lambda \geq \lambda > 0$, and noise variance in $\yv{k}$ is bounded by $2\epsilon^2$. Assuming during the Hessian estimation, we always choose $\bm{s}_k, \bm{y}_k$ such that $\left \langle \bm{s}_k, \bm{y}_k \right \rangle \geq \alpha\cdot\epsilon \left \| \bm{s}_k\right \|$ with $\alpha > 2$, then $\mathcal{L}(\btheta_t) \geq \frac{(\alpha-2)^2\lambda}{2\Lambda^2}\epsilon^2$ at any iteration $t$.
\end{theorem}
\begin{remark}
$\left \langle \bm{s}_k, \bm{y}_k \right \rangle \geq \alpha\cdot\epsilon \left \| \bm{s}_k\right \|$ with $\alpha > 2$ ensures positive-definiteness in the Hessian approximation.
\end{remark}

\subsection{Block-wise Approximation: Improve Memory Efficiency in Distributed Training}
With the L-BFGS update rule, we need to store a sufficient number of history vectors $\vs_k, \vy_k$ to obtain a good Hessian approximation.  
Given model parameters $\btheta \in \mathbb{R}^d$ and supposing $M$ history vectors are needed, it required $O(2Md)$ memory space. 
While it is much smaller than storing the full Hessian ($O(d^2)$) with $M \ll d$, it can still be a critical bottleneck when training large models. 

Furthermore, when it comes to distributed training, simply adopting data parallelism (DP) unfortunately does not help relieve memory pressure on each node, as parameters and gradients are duplicated on each node in DP. 
A naive implementation of the L-BFGS update needs to maintain a copy of $\left \{ \vs_k, \vy_k \right \}_{k=1}^M$ on each node. As a result, more nodes do not help reduce per-node memory costs in the Hessian approximation. 
A more efficient design is needed to \emph{ reduce per-node memory costs when more nodes are available.}

To improve memory efficiency, we propose a new block-diagonal Hessian approximation. Unlike diagonal approximation methods such as KFAC \citep{2016_ICLR_distKFAC}, our block-wise approximation method allows each diagonal Hessian block to capture curvature information across multiple layers. 
Such a design enjoys a more flexible configuration depending on the size of each layer’s parameters and each node’s capacity. 
For instance, as shown in Figure \ref{fig:dist}, if node 1 and 2 have similar memory capacities, and the sizes of layers 1-2 and 3-4 are also close, we can approximate the Hessian blocks with layers 1-2 and 3-4 in node 1 and 2, respectively. Otherwise, we can group layers in a different way and ensure node 1 and 2 share similar memory costs. 

During training, for the Hessian approximation, the $i$-th node only collects parameters and gradients corresponding to its block and computes and stores vectors $\left \{ \vs_k^i, \vy_k^i \right \}$ based on Eq (\ref{eq:skyk}). 
When applying gradient conditioning, each node performs the Hessian-vector product on the corresponding layers, followed by updating parameters. And then, all nodes invoke a \emph{All-Gather} operation to send/collect updated parameters to/from other nodes 

% Figure environment removed

\textbf{Analysis - }
Given $p$ nodes and assuming a model’s parameters can be evenly distributed among these nodes, then each node only needs $O(\frac{2M}{p}d)$ memory to store the history vectors. If more nodes are available, per-node memory costs can be further reduced. 
On the other hand, it is easy to show that the Hessian inverse comprising block-diagonal matrices is positive-definite if each of these blocks is positive-definite, therefore ensuring stable convergence with positive-definite diagonal Hessian blocks.
\begin{lemma}\label{lemma:boundsubHessian}
In $k$-th Hessian update, if block $i (i=1,\cdots,p)$, ${\HI_k}^i$ is bounded by $\xi^i I \preceq {\HI_k}^i \preceq \Xi^i I$, then $\min\xi^i \preceq \HI_k \preceq \max\Xi^i$.
\end{lemma}

\subsection{Put All Together}
With the momentum-based design and block-wise approximation, we present the whole algorithm, \method{}, as shown in Alg \ref{alg:method}.
For each parameter block $i$ at iteration $t$, we compute momentum of parameters and gradients, and store them in $\Mp{t}$ and $\Mg{t}$ (line 6).
For every $T$ iterations, we compute $\vs_k^i, \vy_i$, and update the Hessian approximation (line 13-18). 
It is worth noting that in \emph{UpdateHessian}, the Hessian inverse is not explicitly computed. Instead, we only push $\vs_k^i, \vy_k^i$ to the history vector buffer. 
If the buffers are full, we will pop the oldest vectors. 
In the first $2T$ iterations, we use SGD to conduct a warmup training as the Hessian inverse is not available yet (line 8). 
After the initial warmup training, we will pre-condition gradients $\vg_t$ before applying updates to $\btheta$ (line 10-11).
At the end of each iteration, a \emph{All-Gather} is called to update parameters on all nodes (line 20).

\input{mlbfgs}

\textbf{Hessian damping - }
For non-convex QN optimization, damping is a common technique that ensures the positive definiteness of the Hessian \citep{dampedBFGS,improvedDamping, 2015_ICML_KFAC}. 
Furthermore, even with momentum, stochastic training can still cause undesirable fluctuation in the Hessian approximation. 
To preserve the positive of the Hessian, we adopt an adaptive damping scheme as

\begin{equation}\label{eq::ydamping}
    \hat{\vy}_i = \tau\cdot\vy_i + (1-\tau)\cdot\vs_i,
\end{equation}
with $\tau$ obtained as
\begin{equation*}\label{eq::dampfactor}
    \tau=\left\{\begin{matrix}
            \min(\frac{1-\sigma_L}{1-\mu}, \tau_0) &\mu\leq\sigma_L<1 \\ 
            \min(\frac{\sigma_H-1}{\mu-1}, \tau_0) &\mu\geq\sigma_H>1\\ 
            \tau_0 & \text{otherwise}
        \end{matrix}\right.
\end{equation*}
where $\mu=\frac{\bm{s}_i^T\cdot\bm{y}_i}{\bm{s}_i^T\cdot\bm{s}_i}$, $\sigma_L$ and $\sigma_H$ are the lower and upper thresholds for restraining eigenvalues in $\HI$ and $0<\tau_0<1$ is a constant coefficient. 

It is easy to show that $\frac{\vs_i^T\cdot\hat{\vy}_i}{\vs_i^T\cdot\vs_i}$ is bounded between $[\sigma_L, \sigma_H]$, so that the positive definiteness of the Hessian approximation is preserved during training (See \ref{subsec:prooflemma:damping} for the proof). 