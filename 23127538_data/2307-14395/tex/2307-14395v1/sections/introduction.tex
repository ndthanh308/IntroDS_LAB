\section{Introduction} \label{sec:intro}
Simulating complex spatio-temporal dynamics governed by partial differential equations (PDEs) has been a core of physical science and engineering.
Once the PDE that describes the dynamical system is explicitly established, classical numerical solvers such as Finite Difference Methods (FDMs) \cite{LeVeque2007FDM,Thomas2013NPDE-FDM} and Finite Volume Methods (FVMs) \cite{LeVeque2002FVM} which have been well-developed over the past few decades are usually applied for simulation via suitable discretizations on the grids. These methods are widely used because of their reliable theoretical analysis on convergence and stability.

Nevertheless, lots of dynamical systems appearing in reality such as turbulence modeling and weather forecasting are partially known. Turbulent flows are characterized by multiscale spatio-temporal chaos, and there is no strict separation between low-frequency coherent dynamics and turbulent fluctuations. To avoid the unaffordable computational costs, governing equations for filtered or averaged variables are established to study the statistical features, but the influence of the unresolved scales cannot be determined, which still leads to the closure problem \cite{Pope2000Turbulent}. Things become even more difficult in the field of modern numerical weather prediction (NWP) due to limit of computational resources and lack of obervation data. Apart from a dynamical core describing the atmosphere in terms of momentum, mass and enthalphy \cite{Jacobson2005,Ulrich2022,IFSdocDynamics}, a well-designed NWP model needs physical parameterizations for the subgrid-scale processes such as turbulent transport near the surface, cumulus convection, microphysics and radiation \cite{IFSdocPhysPara}. Unfortunately, these subgrid-scale processes are so complicated that there exist a large number of physical features unable to be modeled via explicit formulas.

In the field of deep learning, these kinds of spatio-temporal dynamical systems are often simulated by neural networks.
Some studies make use of either convolutional neural networks (CNNs) \cite{guo2016convolutional, YinhaoZhu2018BayesianDC, YaserAfshar2019PredictionOA, YuehawKhoo2021SolvingPP, PranshuPant2021DeepLF} or graph neural networks (GNNs) \cite{FrancisOgoke2021GraphCN, ZijieLi2022GraphNN, li2022graph} to learn the spatial relationships in a PDE based on the form of mesh grids, and then employ various schemes to evolve over time.
Meanwhile, the neural operator \cite{LuLu2021LearningNO,li2020fourier,guibas2021efficient, tran2021factorized,ShuhaoCao2021ChooseAT} directly learns the solution mapping between two infinite-dimensional function spaces. This success in various PDE prototypes has received significant attention from scientists studying atmospheric modeling \cite{Dueben2018Challenges}, and many network architectures have been adapted for use in atmospheric science for weather prediction \cite{Schultz2021CanDLbeat}. For example, FourCastNet \cite{pathak2022fourcastnet}, Pangu-Weather \cite{bi2022pangu}, and GraphCast \cite{lam2022graphcast} have demonstrated great potential in weather forecasting scenarios. Despite the vast potential of these pure data-driven black-box models, their high accuracy is dependent on excessive training costs and a large amount of training data. Additionally, these models do not take into account physics knowledge such as conservation laws and dynamic equations, raising concerns about interpretability.

Recently, more and more attention has been focused on embedding partial prior knowledge into network architectures. \cite{FilipedeAvilaBelbutePeres2020CombiningDP} establishes a hybrid neural network combining traditional graph convolutional networks with an embedded differentiable fluid dynamics simulator inside the network. Inspired by the idea of Large Eddy Simulation (LES) for turbulence, TF-Net \cite{Wang2020TowardsPIDLforTurbulence} tries to decompose the turbulent flow with respect to different levels of energy spectrum via spatial and temporal filters, while JAX-CFD \cite{Dmitrii2021MLaccCFD} contains learnable interpolation operators for advected and advecting velocity components and a pressure projection to enforce the zero-divergence condition. Such methods are problem-specific and thus difficult to be applied to other tasks. Another more universal kind of approaches try to directly resemble the associated differential operators. For instance, to address inverse problems, PDE-FIND \cite{Samuel2017PDE-FIND} takes the advantage of polynomial interpolation rather than directly ultilizes fixed finite-difference approximations. Alternatively, PDE-Net \cite{long2018pde,long2019pde} followed by DSN \cite{So2021DSN} substitutes the spatial differential operators with moment-constrained convolutions. Each convolutional operator is guaranteed to approximate a fixed differentiation with learnable parameters, which shows stability and high accuracy on long-term prediction tasks.

\paragraph{Our contributions.} In this paper, we propose a hybrid  network architecture ``PDE-Net++'' for partially known spatio-temporal dynamical systems. Specifically, PDE-Net++ incorporates possibly trainable difference operators for all the related derivatives appearing in the known part and a purely data-driven neural network for the unknown part. Motivated by the delicate numerical schemes as well as the dynamic convolution \cite{De2016DynamicFilter,Su2019Pixel,Han2021DynamicFilterSurvey}, we replace the original universal moment-constrained kernel in PDE-Net with a kernel for each grid point that encodes local features, which lead to the newly proposed TFDL and TDDL modules. PDE-Net++ is capable of performing long-period fast and stable simulation with high accuracy once trained with a few number of observation data. The main contributions are summarized as follows:
\begin{itemize}
    \item A new hybrid architecture named PDE-Net++ is proposed, which effectively combines difference operators and black-box models, essentially realizing the explicit encoding of the partially prior knowledge of the underlying PDEs.% into the neural network.
    \item Within the PDE-Net++ architecture, two new modules named the TFDL and the TDDL are first given in this paper based on the moment-constrained convolution, which generally show better performances in comparison of the existing well-designed difference schemes and moment-constrained convolutions.
    \item Extensive numerical experiments have been performed to demonstrate the effectiveness of PDE-Net++, which indicate that combining physical priors with neural networks have significantly higher prediction accuracy than the pure black-box models for both interpolation and extrapolation.% Furthermore, experiments also show that the trainable dynamic difference layer usually outperforms the other difference operators.
\end{itemize}
