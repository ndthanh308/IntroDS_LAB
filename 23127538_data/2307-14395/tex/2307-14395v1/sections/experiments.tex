\section{Numerical experiments}\label{sec:experiments}
We select three different types of PDEs, namely, the viscous Burgers' equation, the FitzHugh-Nagumo reaction-diffusion equation as well as the Navier-Stokes equation to test our proposed PDE-Net++ architecture. Comprehensive comparisons between the performances of PDE-Net++ and recent well-developed deep-learning models are illustrated and discussed.
Additional experimental settings and some extended experimental results are included in the Appendix \ref{sec:supplementary-for-experiments}.

\subsection{Training settings}
\paragraph{Data generation}
For each PDE setting, a high-resolution numerical solver with full knowledge of the operator $\mL$ in Eq. (\ref{eq:def_a}) is employed to generate the ground truth with a small time step $\delta t$ on a finer $256\times256$ grid, which is then downsampled to a coarser $64\times64$ grid with a much larger time step $\Delta t$ constituting both the training datasets and the testing datasets. The number of trajectories and time steps are denoted by $N$, $N'$ and $M$, $M'$ for the training and the testing datasets, respectively. The initial conditions of the trajectories in the datasets are sampled with the same distributions, but the simulation time in the testing dataset is longer than those in the training dataset in order to analysis the extrapolation performances of the models. Additionally, the training dataset is mixed with noise since the observation data are usually noisy in the real world. For notational convenience, the ground truth, the noisy data, and the predictions are denoted by $\bm U$, $\tilde{\bm U}$, and $\hat{\bm U}$, respectively, where
\[\tilde{\bm U}=\bm U+0.001\hat\sigma\varepsilon.\]
$\hat\sigma$ represents the standard deviation of $\bm U$ for the time step, and $\varepsilon$ is a field of white noise sampled from the standard Gaussian distribution.
\paragraph{Configurations of PDE-Net++}
The difference operators in our PDE-Net++ architecture are implemented in four different ways to approximate the associated derivatives. (I) \textbf{FDM}, the numerical difference schemes same as what we use for data generation (without any learnable parameters); (II) \textbf{Moment}, the convolutional difference layers with moment-constrained kernels proposed in PDE-Net \cite{long2018pde}; (III) \textbf{TFDL} and (IV) \textbf{TDDL} first proposed in this paper. As for the backbones $\mathcal{F}_{\textrm{NN}}$ for the PDE-Net++ models, we choose (i) \textbf{U-Net} \cite{OlafRonneberger2015UNetCN} coming from the field of computer vision; (ii) \textbf{ConvResNet} used in \cite{liu2022predicting}; (iii) \textbf{FNO} \cite{li2020fourier} together with its variant (iv) \textbf{F-FNO} \cite{tran2021factorized}, and (v) \textbf{Galerkin Tansformer} \cite{ShuhaoCao2021ChooseAT} widely used in operator learning tasks.

\paragraph{Black-Box models}
In contrast, we substitute the dynamical term $\mL(\bm x,\bm U_j)$ in Eq. (\refeq{eq:forward-euler-scheme}) as a whole with a neural network implemented by a certain type of backbone mentioned above directly, which is exactly the same idea as those in \cite{TobiasPfaff2020LearningMS} and thus considered as our baselines.

\paragraph{Metrics}
The measurements of the model performances mainly depend on the \textit{average relative} $L_2$ \textit{error} (abbreviated as $L_2$ \textit{error}), which is calculated with the testing dataset $\left\{\tilde{\bm U}_j^{(i)}\right\}_{j=0,i=1}^{M',N'}$ as
\begin{equation}
    \begin{aligned}
        L_2\ \textit{error} = \frac{1}{N'M'}\sum_{i=1}^{N'}\sum_{j=1}^{M'}R\left(\hat{\bm U}_{j}^{(i)},\bm U_{j}^{(i)}\right).
    \end{aligned}
\end{equation}
Here, the function $R(\cdot,\cdot)$ measuring the relative $L_2$ distance is defined as
\[R(\bm X,\bm Y):=\frac{\|\bm X-\bm Y\|_2}{\|\bm Y\|_2}.\]
Meanwhile, since some models may face the instability problem, we use the \textit{Success Rate} (abbreviated as \textit{SR})
\begin{equation}
    \begin{aligned}
        \textit{SR} = \frac{N'_{\textrm{unstable}}}{N'}\times100\%
    \end{aligned}
\end{equation}
as another measurement for prediction stability, where $N'_\textrm{unstable}$ is the number of failed simulations (with $L_2$ \textit{error}s exceeding $1.0$) during inference.


\input{sections/tab_all_l2error.tex}

% Figure environment removed

\subsection{Burgers' equation}\label{sec:burgers}
First of all, we study the 2-D viscous Burgers' equation, which is one of the most fundamental non-linear PDEs describing spatio-temporal dynamics. The spatial solution domain is fixed as $\Omega = [0, 2\pi]^2$ with a periodic boundary condition, and we solve for $\bm U=(u(x,y,t),v(x,y,t))^\trans$
\begin{equation}\label{eq:burgers}
    \begin{aligned}
        \frac{\partial\bm U}{\partial t} & = -\bm U \cdot \nabla\bm U + \nu \Delta\bm U +\bm f(x,y,\bm U), \\
        \bm U|_{t=0}                     & = \bm U_0(x,y)
    \end{aligned}
\end{equation}
where the viscosity $\nu = 0.05$. Suppose that both the convection and the diffusion terms are known, but no further information about the forcing term $\bm f$ is provided. The forcing term is set as
\[\bm f(x,y,\bm U) = (\sin(v) \cos(5x+5y), \sin(u) \cos(5x-5y))^\trans\]
when generating the training and testing datasets. The time step $\Delta t = 16\delta t = 0.01$, and the data sizes are set as $N=1000$, $N'=100$ and $M=10$, $M'=100$.

Table \ref{tb:different_method} shows that the PDE-Net++ architecture outperforms the black-box methods under the same backbones, which indicates that the explicit encoding of the known terms in the PDE helps improve the prediction accuracy significantly. Meanwhile, as for the implementations of the difference operators of PDE-Net++, we find that the non-trainable difference operators (FDM) reduce the prediction errors to some extent, but they are faced with the instability issue. We need to emphasize that the trainable difference layers, especially the proposed TFDL and TDDL modules are capable of resolving the instability brought by the difference schemes.
In addition, as illustrated in Figure \ref{fig:burgers_l2_error_with_timestep}, PDE-Net++ produces stable predictions for the $M'=100$ time steps with relatively high accuracy despite the fact that it is only trained with the first $M=10$ steps.

The learned dynamics are shown in Figure \ref{fig:burgers_result}, including the approximated known part $\hat{\bm\Phi}$ produced by the difference operators and the outputs of the FNO backbone designed to account for the unknown part $\bm f$. The predictions of $\bm U$ are nearly indistinguishable from the reference, but it can be seen from the recovered unknown part $\bm f$ that the non-trainable difference operator (FDM) is unable to express the derivatives with high accuracy where the known part $\bm\Phi$ changes dramatically. The poor performances of the non-trainable difference operator (FDM) should result from the failure of the CFL conditions as our time step $\Delta t$ is quite large. Meanwhile, the shift from the RK method used for data generation to the simple Euler forward integration may lead to instability. On the contrary, the trainable difference operator (TDDL) does not suffer from such issue, which implies the flexibility of the trainable difference operators help search for a better collection of coefficients for the specific time step combined with the integration method.

\input{sections/fig_burgers_snapshots}
\input{sections/fig_fn_ablation}

\subsection{FitzHugh-Nagumo equation}\label{sec:fn}
For the second case, we consider the 2-D FitzHugh-Nagumo (FN) model as a typical example of the reaction-diffusion dynamics. The FN equation is defined as
\begin{equation}\label{eq:fn_rd}
    \begin{aligned}
        \frac{\partial\bm U}{\partial t} & = \gamma \Delta\bm U +\bm R(\bm U), \\
        \bm U|_{t=0}                     & =\bm U_0(x,y)
    \end{aligned}
\end{equation}
with $\bm U=(u(x,y,t),v(x,y,t))^\trans$, where the diffusion coefficient $\gamma=1.0$. Here, the reaction term
\[\bm R(\bm U)=(u-u^3-v+\alpha,\beta(u-v))^\trans,\,\alpha=0.01,\,\beta=0.25\]
is assumed as the unknown part when we establish the PDE-Net++ models. The domain $\Omega$ is set as $[0,6.4]^2$ with the periodic boundary conditions.

To generate our datasets, the data sizes remain the same as those of Burgers' equation, but we modify the time steps to $\Delta t=200\delta t=0.002$ for the purpose of enabling the simulated trajectories to capture more details of the dynamics.

The comparison of different methods is shown in Table \ref{tb:different_method}. Note that the TFDL module is excluded in that it is specially defined for the first-order derivatives. The $L_2$ \textit{error}s still show the superiority of our PDE-Net++ architectures over the black-box models, and the PDE-Net++ architectures equipped with trainable difference operators (Moment, TDDL) generally predict more accurately than those equipped with non-trainable ones (FDM). No failed cases are found in these experiments, and our PDE-Net++ architecture with the proposed TDDL module and the FNO backbone achieves the minimum prediction error.


Figure \ref{fig:fn_ablation_train_size} compares the performances of the black-box method and the PDE-Net architectures with FDM or TDDL as the difference operators when the sizes of training dataset vary. It can be concluded that the PDE-Net++ architecture is able to learn the hidden dynamics $\mL_{\textrm{unknown}}$ from merely 100 training trajectories, while the black-box model is not well trained unless the training size is increased to 1000.

\subsection{Navier-Stokes equation}\label{sec:ns}
Finally, we test our models with the 2-D Navier-Stokes (NS) equation as a much harder case. The equation writes in vorticity form
\begin{equation}\label{eq:ns}
    \begin{aligned}
        \frac{\partial w}{\partial t} & = -\bm U \cdot \nabla w + \nu \Delta w + f(x,y), \\
        \nabla \cdot \bm U            & = 0,                                             \\
        w|_{t=0}                      & = w_0(x,y).
    \end{aligned}
\end{equation}
with the periodic boundary condition for the spatial solution domain $\Omega=[0,1]^2$. $\bm U=(u(x,y,t),v(x,y,t))^\trans$ is the velocity field, and $w=\nabla\times\bm U=\frac{\partial v}{\partial x}-\frac{\partial u}{\partial y}$ is the vorticity we need to solve. The viscosity coefficient $\nu$ is set as 0.001. For a smaller viscosity coefficient when the dynamics becomes more chaotic, we explore the performances of the models in the Appendix \ref{appendix:ns}.

The initial vorticity $w_0(x,y)$ is randomly sampled from $\mathcal{N}(0, 25 (-\Delta + 25 I)^{-3})$ as a Gaussian random field \cite{YangLiu2019AdvancesIG} and then z-score normalized. The forcing function $f(x,y)$ is generated in the same way, but we have to stress that all the trajectories starting from different initial conditions in the datasets share the same $f(x,y)$. For each time step, PDE-Net++ recovers the velocity field $\bm U=(u,v)=\left(-\frac{\partial\psi}{\partial y},\frac{\partial\psi}{\partial x}\right)$ after obtaining the stream function $\psi=\Delta^{-1}w$ via (discrete) Fourier transform on $w$. Then the gradient $\nabla w$ and the diffusion $\Delta w$ are calculated the same way as before. The time step $\Delta t=500\delta t=0.025$, and the data sizes are $N=1000$, $N'=100$ and $M=50$, $M'=200$.

As depicted in Table \ref{tb:different_method}, with the same backbone, PDE-Net++ attains a higher accuracy compared with the black-box models. Meanwhile, we find that PDE-Net++ with non-trainable difference operators (FDM) can hardly keep stable for the NS equation case, which may attribute to the fact that the dynamic system is much more complicated than the previous two cases. Although the TFDL and the TDDL modules predict almost as accurately as the moment-constrained convolution (Moment), the success rate reveals the consistent stability of our proposed modules.










