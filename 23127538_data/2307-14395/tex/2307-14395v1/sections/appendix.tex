\onecolumn
\section{Mathematical backgrounds}
Some of the following conclusions can be found in \cite{long2018pde,So2021DSN}. We restate and summarize the simplified versions for readers' convenience.
\begin{Prop}\label{prop:moment-constraints}
    For any smooth function $h$ and a compactly supported kernel $K$,
    \[(K\circledast h)(x,y)=\frac{\partial^{ p+ q}}{\partial x^{ p}\partial y^{ q}}h(x,y)+\mathcal{O}\left(\left((\Delta x)^2+(\Delta y)^2\right)^{(r+1)/2}\right)\]
    if and only if Eq. (\ref{eq:moment-constraints}) holds.
\end{Prop}
\begin{proof}
    Starting from Eq. (\ref{eq:conv-difflayer-concept}), as $\Delta x$ and $\Delta y$ approach zero, we have by truncating the Taylor series that
    \begin{align*}
        (K\circledast h)(x,y) & =\sum_{s,t\in\mZ}K(s,t)h(x+s\Delta x,y+t\Delta y)                                                                                                                                                                         \\
                              & =\sum_{s,t\in\mZ}\left[K(s,t)\sum_{l=0}^r\frac{(s(\Delta x)\partial_x+t(\Delta y)\partial_y)^l}{l!}h(x,y)+\mathcal{O}\left(\left((s\Delta x)^2+(t\Delta y)^2\right)^{(r+1)/2}\right)\right]                               \\
                              & =\sum_{s,t\in\mZ}K(s,t)\sum_{l=0}^r\sum_{u+v=l}\frac{(s\Delta x)^u}{u!}\frac{(t\Delta y)^v}{v!}\frac{\partial^lh}{\partial x^u\partial y^v}(x,y)+\mathcal{O}\left(\left((\Delta x)^2+(\Delta y)^2\right)^{(r+1)/2}\right) \\
                              & =\sum_{u+v\le r}M(K)(u,v)\frac{\partial^lh}{\partial x^u\partial y^v}(x,y)+\mathcal{O}\left(\left((\Delta x)^2+(\Delta y)^2\right)^{(r+1)/2}\right),
    \end{align*}
    and that the proposition immediately follows.
\end{proof}
Note that we take additional spatial mesh steps $\Delta x$ and $\Delta y$ together with the truncation order $(r+1)$ into consideration compared with the conclusion given in PDE-Net. From now on, we fix the sizes of both the kernel and the moment matrix as $(2L+1)\times(2L+1)$.
\begin{Prop}\label{prop:Q-invertible}
    There exists a unique moment matrix $M(K)$ corresponding with any kernel $K$ and vice versa. In other words, the map $K\mapsto M(K)$ is invertible.
\end{Prop}
\begin{proof}
    By identifying the kernel $K$ with a $(2L+1)\times(2L+1)$ matrix with indices from $-L$ to $L$ inclusively, Eq. (\ref{eq:moment-constraints}) implies
    \[M(K)(u,v)=\sum_{s=-L}^L\sum_{t=-L}^L\frac{s^u(\Delta x)^u}{u!}K(s,t)\frac{t^v(\Delta y)^v}{v!}.\]
    Hence $M(K)=Q_xKQ_y^\trans $ for
    \[Q_x(u,s)=\frac{s^u(\Delta x)^u}{u!},\,Q_y(u,s)=\frac{s^u(\Delta y)^u}{u!},\]
    where $u=0,\cdots, 2L$ and $s=-L,\cdots,0,\cdots,L$. $0^0$ is defined as $1$ as convention.
    \begin{align*}
        Q_x=
        \begin{pmatrix}
            0!                     \\
             & 1!                  \\
             &    & \ddots         \\
             &    &        & (2L)!
        \end{pmatrix}^{-1}
        \begin{pmatrix}
            1                 & \cdots & 1      & \cdots & 1                \\
            (-L\Delta x)^1    & \cdots & 0^1    & \cdots & (L\Delta x)^1    \\
            \vdots            &        & \vdots &        & \vdots           \\
            (-L\Delta x)^{2L} & \cdots & 0^{2L} & \cdots & (L\Delta x)^{2L}
        \end{pmatrix}
    \end{align*}
    %\[Q_x=\diag(0!,\cdots,(2L)!)^{-1}\left((s\Delta x)^u\right)_{u=0,s=-L}^{2L,L}\]
    is a multiplication of an invertible diagonal matrix and a non-singular Vandermonde matrix and thus invertible. Analogously, $Q_y$ is invertible as well, and it follows that the linear transformation $K\mapsto M(K)$ is invertible.
\end{proof}
\begin{Prop}\label{prop:kernels-in-hyperplane}
    For fixed $(p,q)$ and $r$ such that $p+q+r\le 2L$, the set $\mathcal{S}_{(p,q)}^r$ consisting of all the kernels satisfying Eq. (\ref{eq:moment-constraints}) forms a hyperplane of dimension $(2L+1)^2-(p+q+r+1)(p+q+r+2)/2$.
\end{Prop}
\begin{proof}
    Let $E_{ij}$ be the matrix with the only non-zero element $1$ at the position $(i,j)$. By the definition of $\mathcal{S}_{(p,q)}^r$,
    \begin{align*}
        K\in \mathcal{S}_{(p,q)}^r & \Leftrightarrow \exists c_{uv}\in\mR,\,M(K)=E_{pq}+\sum_{u+v>p+q+r}c_{uv}E_{uv}                                           \\
                                   & \Leftrightarrow \exists c_{uv}\in\mR,\,Q_xKQ_y^\trans =E_{pq}+\sum_{u+v>p+q+r}c_{uv}E_{uv}                                \\
                                   & \Leftrightarrow \exists c_{uv}\in\mR,\,K=Q_x^{-1}E_{pq}Q_y^{-\trans} +\sum_{u+v>p+q+r}c_{uv}Q_x^{-1}E_{uv}Q_y^{-\trans} .
    \end{align*}
    Here we abbreviate $\left(Q_y^\trans \right)^{-1}$ as $Q_y^{-\trans} $. Define
    \[K_{uv}^0=Q_x^{-1}E_{uv}Q_y^{-\trans} =Q_x^{-1}[:,i]Q_y^{-1}[:,j]^\trans \]for any $u,v=0,\cdots, 2L$, where $Q_x^{-1}[:,i]$ stands for the $i$-th column of $Q_x^{-1}$, then
    \[\mathcal{S}_{(p,q)}^r=K_{pq}^0+\Span\left(K_{uv}^0\right)_{u+v>p+q+r},\]
    where the notation $\Span(\cdot)$ represents the subspace spanned by the elements inside.
    \begin{equation}\label{eq:channel-num}
        \begin{aligned}
            \sum_{u+v>p+q+r}1 & =(2L+1)^2-\sum_{u+v\le p+q+r}1=(2L+1)^2-\frac12(p+q+r+1)(p+q+r+2)
        \end{aligned}
    \end{equation}
\end{proof}
\begin{Rem}
    We have to emphasize that all the kernels $\left\{K_{uv}^0\right\}_{u,v=0}^{2L}$ mentioned in the proof of Prop. \ref{prop:kernels-in-hyperplane} are constants and independent of the order $r$. Consequently, we prepare and save $\left\{K_{uv}^0\right\}_{u,v=0}^{2L}$ according to Prop. \ref{prop:Q-invertible} in advance before the training stage, which can be shared among convolutional layers corresponding to different derivatives afterward.
\end{Rem}

\begin{Prop}[Flipped kernels]\label{prop:flipped-kernels}
    For $(p,q)=(1,0)$ and any $r$, the kernel $K$ satisfies Eq. (\refeq{eq:moment-constraints}) if and only if
    $K'(s,t)=-K(-s,t)$ satisfies Eq. (\refeq{eq:moment-constraints}).
\end{Prop}
\begin{proof}
    Consider the moment matrices for $K$ and $K'$.
    \begin{align*}
        M(K')(u,v) & =\sum_{s=-L}^L\sum_{t=-L}^L\frac{(s\Delta x)^u(t\Delta y)^v}{u!v!}K'(s,t)                               \\
                   & =-\sum_{s=-L}^L\sum_{t=-L}^L\frac{(s\Delta x)^u(t\Delta y)^v}{u!v!}K(-s,t)                              \\
                   & =(-1)^{u+1}\sum_{s=-L}^L\sum_{t=-L}^L\frac{(-s\Delta x)^u(t\Delta y)^v}{u!v!}K(-s,t)                    \\
                   & =(-1)^{u+1}\sum_{s=-L}^L\sum_{t=-L}^L\frac{(s\Delta x)^u(t\Delta y)^v}{u!v!}K(s,t)=(-1)^{u+1}M(K)(u,v).
    \end{align*}
    Hence $M(K)(u,v)=0$ if and only if $M(K)(u,v)=0$, and $M(K)(1,0)=1$ if and only if $M(K')(1,0)=1$.
\end{proof}

\section{Some examples for numerical schemes encoding local features}\label{sec:schemes-with-local-features}
In this part, we list some types of classical numerical methods accounting for local features. For simplicity, the numerical methods are illustrated in 1D cases since similar derivations can be applied to high-dimensional cases with little difficulty. We choose the forward Euler integration with a fixed time step $\Delta t$, and the spatial grid points are assumed to be distributed uniformly with $\Delta x$ as the interval length. The superscripts and the subscripts indicate the indices along the temporal and spatial dimensions, respectively.
\subsection{Upwind schemes}
Consider the following 1D advection equation
\[\frac{\partial u}{\partial t}+c\frac{\partial u}{\partial x}=0,\,u:[0,1]\to\mR.\]It follows that the forward update rule reads
\[U_j^{m+1}=U_j^m-(c\Delta t)D(U^m)_j,\]
where $U_j^m$ represents the numerical solutions, and $D(U^m)_j$ is an approximation of $\partial_x U^m$ at the spatial index $j$. Intuitively one may choose the central difference $D(U^m)_j=(U^m_{j+1}-U^m_{j-1})/(2\Delta x)$ for a higher order of approximation, but such choice will suffer from unconditional instability by the von Neumann stability analysis \cite{Thomas2013NPDE-FDM}. Upwind schemes adopt a lower-order discretization to improve the stability, which writes
\begin{equation}\label{eq:1st-order-upwind}
    U_j^{m+1}=(1-|\mu|)U_j^m+\frac{\mu+|\mu|}2U_{j-1}^m-\frac{\mu-|\mu|}2U_{j+1}^m
\end{equation}
for a first order discretization and
\begin{equation}\label{eq:2nd-order-upwind}
    U_j^{m+1}=\frac{2-3|\mu|}2U_j^m+(\mu+|\mu|)U_{j-1}^m-(\mu-|\mu|)U_{j+1}^m-\frac{\mu+|\mu|}4U_{j-2}^m+\frac{\mu-|\mu|}4U_{j+2}^m
\end{equation}
for a second-order discretization, where $\mu=c\Delta t/\Delta x$. Other finite difference methods including the Lax-Friedrichs scheme, the Lax-Wendroff scheme, and the Beam-Warming scheme with various dispersion and dissipation \cite{Thomas2013NPDE-FDM} can be written as such updates, where the coefficient of each grid point on the stencil is a function of $\mu$ as well.
\subsection{Flux limiters}
FVMs model $U_j$ as the average of $u$ at the $j$-th cell, and for 1D hyperbolic PDEs with a scalar conservation law
\[\frac{\partial u}{\partial t}+\frac{\partial}{\partial x}f(u)=0,\,u:[0,1]\to\mR,\]
the discrete schemes usually appear as
\[U_j^{m+1}=U_j^m-\frac{\Delta t}{\Delta x}\left[F_{j+1/2}^m-F_{j-1/2}^m\right],\]
where $F_{j\pm1/2}^m$ approximates the flux passing through the right/left edge of the $j$-th cell. By setting $f(u)=cu$ with a constant $c$, the first-order upwind scheme (\ref{eq:1st-order-upwind}) corresponds to the numerical flux
\[F_{j+1/2}^m=\frac{\mu+|\mu|}2U_j^m+\frac{\mu-|\mu|}2U_{j+1}^m.\]
In general, the upwind scheme is not competitive when compared with higher-order schemes such as the Lax-Wendroff scheme for smooth solutions. However, these higher-order schemes tend to generate oscillations near discontinuities due to their dispersive nature \cite{LeVeque2002FVM}. Based on such observations, flux limiters are often used to switch between low-resolution and high-resolution discretizations. Formally, the numerical flux is defined as
\[F_{j+1/2}^m=\phi_{j+1/2}^mF_{H,j+1/2}^m+\left(1-\phi_{j+1/2}^m\right)F_{L,j+1/2}^m,\]
where $F_{H,j+1/2}^m$ and $F_{L,j+1/2}^m$ stand for a high-order flux and a low-order flux, respectively. The flux limiter $\phi_{j+1/2}^m$ depending on the local features of the solutions determines whether the scheme reduces to the lower-order method or not. the total variation diminishing (TVD) property together with the Courant-Friedrichs-Lewy (CFL) condition plays a significant role in constructing a large number of various flux limiters \cite{Roe1986minmod,VanLeer1974,VanLeer1979}.
\subsection{ENO/WENO reconstruction}
Given cell average
\[\bar u_j=\frac1{\Delta x}\int_{I_j}u(\xi)\md\xi\]
of $u$ over the $j$-th interval $I_j$ for all $j$, ENO and WENO schemes \cite{Liu1994WENO,Shu1998ENOandWENO} essentially look for a polynomial approximation $p_j$ of $u$ within $I_j$ by taking polynomial interpolation on the primitive function of $u$. It follows that the values $u_{j\pm1/2}$ are approximated by the evaluations of $p_j$ on the cell edges. Such feasible polynomials are not unique even when the size of stencils is fixed. For instance,
\begin{align*}
    u_{j+1/2} & =-\frac16\bar u_{j-1}+\frac56\bar u_j+\frac13\bar u_{j+1}+\mathcal{O}\left((\Delta x)^3\right),\textrm{ and} \\
    u_{j+1/2} & =\frac13\bar u_j+\frac56\bar u_{j+1}-\frac16\bar u_{j+2}+\mathcal{O}\left((\Delta x)^3\right).
\end{align*}
ENO and WENO schemes select the most suitable polynomials by certain measurements of smoothness in order to reduce the total variation, and one of the major differences is that WENO admits a weighted combination of polynomials coming from different stencils. Combined with certain monotone fluxes, the reconstruction will give good approximations of the fluxes on the cell edges.

\section{Supplementary materials for the experiments}\label{sec:supplementary-for-experiments}
This section formulates the training loss for the experiments and then summarizes extended experimental results for the three PDEs described in Sec. \ref{sec:experiments}.

\subsection{Training loss}
We train both our PDE-Net++ models and the black-box models with a single time step, and then roll out for hundreds of steps in testing. Formally, the updating rules for the observation $\tilde{\bm U}_j^{(i)}$ of the $i$-th trajectory at time step $j$ read
\begin{equation}\label{eq:pdenet_plus_pred}
    \hat{\bm U}_{j+1}^{(i)}=\tilde{\bm U}_j^{(i)}+\hat{\bm\Phi}\left(\bm x, \tilde{\bm U}_j^{(i)}\right)\Delta t+\mathcal{F}_{\mathrm{NN}}\left(\bm x, \tilde{\bm U}_j^{(i)}\right)\Delta t
\end{equation}
for the PDE-Net++ models and
\begin{equation}\label{eq:black_box_pred}
    \begin{aligned}
        \hat{\bm U}_{j+1}^{(i)} & =\tilde{\bm U}_j^{(i)}+\mathcal{F}_{\mathrm{NN}}\left(\bm x, \tilde{\bm U}_j^{(i)}\right)\Delta t
    \end{aligned}
\end{equation}
for the black-box models.

During the training stage, the learnable components are tuned to reduce the distance between the model predictions and the actual observations of the next step. Suppose that we have the training dataset $\left\{\tilde{\bm U}_j^{(i)}\right\}_{j=0,i=1}^{M,N}$, the training loss $L$ is defined as
\[L=L_{\textrm{pred}}+\lambda L_{\textrm{reg}},\]
where the prediction loss $L_{\textrm{pred}}$ is measured as
\[L_{\textrm{pred}}=\frac{1}{NM}\sum_{i=1}^N\sum_{j=1}^MR\left(\hat{\bm U}_j^{(i)},\tilde{\bm U}_{j}^{(i)}\right).\]
The $\lambda L_{\textrm{reg}}$ term is set to zero for all the black-box models and the PDE-Net++ architectures with the difference operators implemented as ``FDM'' described above. Otherwise, $L_{\textrm{reg}}$ penalizes the $L_1$ norm for the lower-right free parameters in the moment matrices with the hyper-parameter $\lambda=0.001$.

\subsection{Extended experimental results}\label{sec:extended_result}

\subsubsection{Trainable parameters}
\input{sections/tab_all_trainable_params.tex}
Compared with the black-box models, PDE-Net++ may have additional trainable parameters lying in the trainable difference operators (if exist). Table \ref{tb:num_params} lists the numbers of trainable parameters for all the experiments mentioned in Sec. \ref{sec:experiments}. With the exception of the F-FNO backbone, the increments resulting from the trainable difference operators (Moment, TFDL, and TDDL) are negligible with the same backbones.

\subsubsection{Burgers' equation}
We also investigate the effect of the number of training samples (training size) on different methods for Burgers' equation. Figure \ref{fig:burgers_ablation_train_size} shows that as the training size increases, PDE-Net++ with the TDDL module consistently outperforms both the other models.

% Figure environment removed

\subsubsection{FitzHugh-Nagumo equation}
For the FN equation, Figure \ref{fig:fn_result} exhibits snapshots of the reference and predicted solutions of PDE-Net++ (TDDL, FNO) at different time steps during testing. It turns out that our method is able to evolve the diffusion appropriately and that the differences with the reference are indistinguishable.

% Figure environment removed

\subsubsection{Navier-Stokes equation}\label{appendix:ns}
The viscosity coefficient $\nu$ has been set to 0.001 in the previous experiments for the NS equation. Here, we also attempt to set $\nu=0.0001$ as a more challenging scenario. The time steps and the number of roll-out steps are changed to $\Delta t=125\delta t=0.00625$ and $M=M'=800$, respectively. The comparison of different methods is displayed in Table \ref{tb:ns_different_method_0.0001}, which indicates that PDE-Net++ (TDDL, FNO) achieved the best performance under such circumstances. In addition, Figure \ref{fig:ns_result} presents a comparison between the predictions of PDE-Net++ (TDDL, FNO) and the ground truth. Four samples are randomly selected from all the testing trajectories of size $N'=100$.

\input{sections/tab_NS_l2error_0.0001.tex}

% Figure environment removed
