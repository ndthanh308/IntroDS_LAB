\section{Methodology}
We focus on solving spatio-temporal PDEs in the general form of
\begin{subequations}\label{eq:def}
    \begin{align}
         & \frac{\partial\bm U}{\partial t} = \mL(\bm x, \bm U), \quad (\bm x,t) \in \Omega \times [0,T], \label{eq:def_a} \\
         & \mathcal{I}(\bm x, \bm U) = 0, \quad \bm x \in \Omega, \label{eq:def_b}                                         \\
         & \mathcal{B}(\bm x, t, \bm U) = 0, \quad (\bm x,t) \in \partial \Omega \times [0,T] \label{eq:def_c}
    \end{align}
\end{subequations}
for $n$-dimensional state variables $\bm U=\bm U(\bm x,t):\Omega\times[0,T]\to\mR^n$. Here, $\mL(\cdot)$ stands for the differential operator describing the underlying dynamics with partial knowledge, which is furthermore decomposed into two parts, namely the known part $\mL_{\textrm{known}}$ and the unknown part $\mL_{\textrm{unknown}}$.
\[\mL(\bm x, \bm U)=\mL_{\text{known}}(\bm x, \bm U)+\mL_{\text{unknown}}(\bm x, \bm U).\]
The operators $\mathcal{I}(\cdot)$ and $\mathcal{B}(\cdot)$ correspond to the initial and the boundary conditions, respectively. Assume that we have an explicit formula
\begin{equation}\label{eq:Lknown-decomp}
    \mL_{\textrm{known}}(\bm x,\bm U)=\bm\Phi(\bm x,\bm U,\nabla\bm U,\nabla^2\bm U,\cdots)
\end{equation}
for the known part, but no prior knowledge is provided for the unknown one. Our task is to develop a surrogate model that recovers the hidden dynamical behavior based on noisy observations of $\bm U$. Once such model is established, it is capable of generating fast simulation starting from new initial conditions with high accuracy.

\subsection{The PDE-Net++ architecture}
We employ the forward Euler integration scheme
\begin{equation}\label{eq:forward-euler-scheme}
    \resizebox{.9\hsize}{!}{$%
            \begin{aligned}
                \bm U_{j+1} & \approx\bm U_j+\mL(\bm x,\bm U_j)\Delta t                                                          \\
                            & =\bm U_j+\mL_{\textrm{known}}(\bm x,\bm U_j)\Delta t+\mL_{\textrm{unknown}}(\bm x,\bm U_j)\Delta t
            \end{aligned}
        $%
    }%
\end{equation}
to discretize Eq. \eqref{eq:def_a}\noeqref{eq:def_b}\noeqref{eq:def_c} along the temporal dimension, where the subscript $j$ stands for the time index with a time interval $\Delta t$. The general idea of PDE-Net++ is to approximate the known part with certain finite difference operators, and to approximate the unknown part with a neural network $\mathcal{F}_{\text{NN}}(\bm x, \bm U_j)$ named as the ``\textit{backbone}''. The updating formula of PDE-Net++ according to Eq. \eqref{eq:forward-euler-scheme} is
\begin{equation}\label{eq:my_forward_euler}
    \begin{aligned}
        \hat{\bm U}_{j+1} & =\bm U_j+\hat{\bm\Phi}(\bm x,\bm U_j)\Delta t+\mathcal{F}_{\mathrm{NN}}(\bm x, \bm U_j)\Delta t,
    \end{aligned}
\end{equation}
where
\[\hat{\bm\Phi}(\bm x,\bm U_j)=\bm\Phi(\bm x,\bm U_j, D(\bm U_j), D^2(\bm U_j),\cdots),\]
and the symbol $\hat{\bm U}$ stands for the model prediction. The derivative terms such as $\nabla\bm U$ and $\nabla^2\bm U$ in $\bm\Phi$ are replaced by the corresponding difference operators $D(\bm U)$ and $D^2(\bm U)$ respectively, which can be implemented via existing stable finite difference schemes or the trainable difference layers introduced later. Besides, the neural network $\mathcal{F}_{\mathrm{NN}}$ is designed to account for the truncation error introduced by the former difference operators \cite{Um2020SolverintheLoop} as well as the remaining unknown part $\mL_{\textrm{unknown}}(\bm x, \bm U_j)$.% Given a trained PDE-Net++ model together with initial state variables $\bm U_0$, any future state $\bm U_p$ is predicted by iterating over Eq. \eqref{eq:my_forward_euler} for $p$ times.

% Figure environment removed

Fig.\ref{fig:method_pdenet_plus}(a) shows the overall architecture of PDE-Net++ with the difference operators realized by the TDDL module as an example. The input $\bm U_j$ records the state variables (possibly predictions from the previous data) at time step $j$, which is then fed separately to the TDDL module and the backbone. The TDDL module produces approximations of derivatives of $\bm U_j$ and saves them in a finite difference bank. Afterwards, the results of finite differences are collected to compute $\hat{\bm\Phi}(\bm x,\bm U_j)$ as an approximation of $\mL_{\textrm{known}}(\bm x,\bm U_j)$. Meanwhile, a backbone network processing on $\bm U_j$ produces $\mathcal{F}_{\textrm{NN}}(\bm x,\bm U_j)$ to approximate the $\mL_{\textrm{unknown}}$ term. Finally, the respective results of the two paths multiplied by $\Delta t$ give rise to the predictions for the next time step via the ``skip-connection'' introduced in ResNet \cite{he2016deep}. Regarding the choices of the backbone, we test some classic models including U-Net \cite{OlafRonneberger2015UNetCN}, FNO \cite{li2020fourier} together with its variant F-FNO \cite{tran2021factorized}, ConvResNet \cite{he2016deep}, and Galerkin Transformer \cite{ShuhaoCao2021ChooseAT}.

\subsection{Trainable difference layers}\label{sec:difflayers}
In order to introduce our physics prior, the ideas borrowed from FDMs are used to resemble the derivatives in $\mL_{\textrm{known}}$. For any 1D scalar function $f:\mR\to\mR$ and a set of $N_P$ different grid points $\{x_l\}_{l=1}^{N_P}$, the (possibly high-order) derivative at some point $\bar x$ can be approximated via a linear combination
\[\left.\frac{\md^kf}{\md x^k}\right|_{x=\bar x}\approx\sum_{l=1}^{N_P}\alpha_l^{(k)}f(x_l)\]
if the coefficients $\alpha_l^{(k)}$ satisfy a non-homogeneous linear system depending on the order $k$ and the relative distances $(x_l-\bar x)$ \cite{fornberg1988generation}. Similar deductions can be applied to high-dimensional cases. Note that such linear systems have solution sets that can be easily parameterized, which allows us to introduce tunable parameters when implementing the difference operators.
\begin{Def}
    A parameterized map $D_\theta:\mZ^2\times\mR_+^2\to\mZ^2$ is defined as a \textit{difference layer} corresponding to the differential operator $\frac{\partial^{p+q}}{\partial x^p\partial y^q}$ with a parameter space $\Theta$ if for any smooth function $h$ and $\theta\in\Theta$,
    \[D_\theta(G(h);\Delta x,\Delta y)(k,l)\to G\left(\frac{\partial^{p+q}h}{\partial x^p\partial y^q}\right)(k,l)\]
    for each $(k,l)\in\mZ^2$ as $\Delta x,\Delta y\to0$, where the regular grid sampler $G$ is defined as
    \[G(h)(k,l):=h(k\Delta x,l\Delta y).\]
\end{Def}
\begin{Rem}
    Indeed, the schemes coming from FDMs or FVMs can be viewed as difference layers with trivial parameter spaces (containing only one point). Our convention for difference layers excludes such cases to distinguish trainable difference layers from non-trainable ones.
\end{Rem}

Inspired by the systematic connection between spatially differential operators and convolutional kernels established by \cite{Cai2012ImageRestoration, dong2017ImageRestoration} within the image restoration setting, PDE-Net \cite{long2018pde,long2019pde} essentially gives a convolutional version of the difference layer defined above, which can be considered as a simple special case. % which means the parameterized map $D$ in the definition above can also be written as a convolution for fixed $\Delta x$, $\Delta y$ and $\theta\in\Theta$. 
In fact, PDE-Net gives the following proposition. Note that from now on, we omit the dependencies on $\Delta x$ and $\Delta y$ for simplicity.

\begin{Prop}
    It can be shown by proper truncation (Prop. \ref{prop:moment-constraints}) on the Taylor's expansion that the convolution
    \begin{equation}\label{eq:conv-difflayer-concept}
        \begin{aligned}
            D(G(h))(k,l) & =(K\circledast G(h))(k,l)            \\
                         & =\sum_{s,t\in\mZ}K(s,t)G(h)(k+s,l+t)
        \end{aligned}
    \end{equation}
    with a locally-supported convolutional kernel $K$ is a difference layer corresponding to the differential operator $\frac{\partial^{p+q}}{\partial x^p\partial y^q}$ with a convergence order $(r+1)$ if and only if the moment matrix of $K$
    \begin{equation}
        M(K)(u,v):=\sum_{s,t\in\mZ}K(s,t)\frac{s^ut^v}{u!v!}(\Delta x)^u(\Delta y)^v
    \end{equation}
    satisfies the moment constraint
    \begin{equation}\label{eq:moment-constraints}
        \begin{aligned}
            M(K)(u,v) = \delta_{u,p}\delta_{v,q}\,\textrm{ for all }\, u+v\le p+q+r
        \end{aligned}
    \end{equation}
    with the Kronecker delta $\delta$.
\end{Prop}

Furthermore, if both the kernel size and the moment matrix size are fixed as $(2L+1)\times(2L+1)$, then there is a one-to-one correspondence between the moment matrix and the kernel. In other words, for fixed $(p,q)$ and $r$, any convolutional difference layer can be uniquely determined and parameterized by the free elements $\{M(K)(u,v)\}_{u+v>p+q+r}$ in the moment matrix of its convolutional kernel. Consequently, all convolutional difference layers are of the form
\begin{equation}\label{eq:ConvDiffLayer}
    D_\theta(G(h))(k,l)=\sum_{s,t\in\mZ}\bar K_\theta(s,t)G(h)(k+s,l+t),
\end{equation}
where $\bar K_\theta$ stands for the convolutional kernels parameterized by the free elements of its moment matrix.
\subsection{Trainable flipping difference layers} \label{sec:TFDL}
Nevertheless, most classical numerical schemes do not fix the coefficients across the whole solution domain, which usually rely on the local features of the numerical solution. For instance, upwind schemes originated in \cite{Courant1952Upwind} switch among different sets of coefficients according to the signs of the advection velocities. FVMs usually adapt the flux limiter \cite{Roe1986minmod,VanLeer1974,VanLeer1979} to exchange some accuracy for stability. Besides, ENO and WENO methods \cite{Liu1994WENO,Shu1998ENOandWENO} seek for a (weighted) average of interpolation polynomials based on a heuristic smoothness measure for better reconstructions. One may refer to Appendix \ref{sec:schemes-with-local-features} for more details. These methods all result in different coefficients for the stencils at different grid points. Therefore, it is unwise to expect a universal finite difference operator that can achieve a good balance between accuracy and stability for each grid point simultaneously.

We focus on those difference layers where $D_\theta(G(h))(k,l)$ is defined as
\begin{equation}\label{eq:FlexDiffLayers}
    \sum_{s,t\in\mZ}K_\theta(s,t;\bm U,k,l)G(h)(k+s,l+t),
\end{equation}
where $K_\theta$ may depend on not only $h$ itself but also all other features associated with the differential equations. Such a formula allows $K_\theta$ to choose different coefficients according to the locations and the local features of $h$.

To mimic the upwind behavior, we propose the \textit{Trainable Flipping Difference Layer} (TFDL) for the first-order derivatives to switch between two sets of parameters for $K_\theta$ according to the signs of the coefficients before the derivatives. For instance, suppose that the known part $\mL_{\textrm{known}}$ contains the term ``$v\frac{\partial u}{\partial x}$'', the TFDL for the derivative $\frac{\partial u}{\partial x}$ is defined as
\begin{equation}\label{eq:FlippingKernels}
    \resizebox{.88\hsize}{!}{$%
            K_\theta(s,t;\bm U,k,l)=
            \begin{cases}
                \bar K_\theta(s,t),   & G(v)(k,l)>0, \\
                -\bar K_\theta(-s,t), & G(v)(k,l)<0.
            \end{cases}
        $%
    }%
\end{equation}
As for the derivative $\frac{\partial}{\partial y}$, the rule follows in a similar way, but the sign of $t$ rather than that of $s$ is flipped. It can be checked with little difficulty (Prop. \ref{prop:flipped-kernels}) that the TFDL (\refeq{eq:FlexDiffLayers}, \refeq{eq:FlippingKernels}) shares the same corresponding derivative and the same convergence order with the convolutional difference layer (\refeq{eq:ConvDiffLayer}).

\subsection{Trainable dynamic difference layers}\label{sec:TDDL}
Sometimes the dynamics is too complicated to be resolved by the convolutional difference layer or the TFDL. Motivated by the dynamic convolution \cite{De2016DynamicFilter,Su2019Pixel,Han2021DynamicFilterSurvey} as well as the data-driven discretizations \cite{Yohai2019Learningdata-drivenDiscretizations} in 1D cases, we employ the \textit{Trainable Dynamic Difference Layer} (TDDL) to generate localized $K_\theta$. In the TDDL, the parameter $\theta$ varies among grid points to capture local features, which is realized via
\begin{equation}
    K_\theta(s,t;\bm U,k,l)=\bar K_{\mathcal{H}(G(\bm U);k,l)}(s,t),
\end{equation}
where $\mathcal{H}$ is a hypernetwork \cite{DavidHaHypernetwork}. In this paper, we choose CNNs containing 3 convolutional layers with ReLU for the implementation of $\mathcal{H}$. A schematic diagram for the module is displayed in Fig.\ref{fig:method_pdenet_plus}(b). Suppose that the TDDL module is fed with a 2D field $\bm U_j=(V_1,\cdots,V_c)\in\mR^{c\times h\times w}$ with $c$ channels which we need to take derivatives on. We take the $(p,q)$ derivative of the first feature $V_1$ with a truncation order $(r+1)$ as an example. First, a hypernetwork $\mathcal{H}_{pq}$ receiving $\bm U_j$ gives
\[\bm W_{pq}=\mathcal{H}_{pq}(\bm U_j)\in\mR^{m\times h\times w},\]
where the number of output channels $m$ equals the dimension of parameter space $\Theta$. The output features $\bm W_{pq}(\cdot,k,l)\in\mR^m$ for each grid point $(k,l)$ are then reshaped into a local moment matrix $M_{pq}(\cdot,\cdot;k,l)\in\mR^{(2L+1)\times(2L+1)}$ with the upper-left constants initialized before the training stage. Afterwards, the local moment matrix for each grid point is converted into the corresponding local kernel $K_{pq}(\cdot,\cdot;k,l)\in\mR^{(2L+1)\times(2L+1)}$, followed by a point-wise convolutional operation with the feature $V_1$
\[D_\theta(G(V_1))(k,l)=\sum_{s,t=-L}^LK_{pq}(s,t;k,l)V_1(k+s,l+t)\]
for each grid point $(k,l)$. Other combinations of the derivatives and the features can be calculated in this way simultaneously, and all the results are then gathered and sent to the finite difference bank.
