\documentclass[journal,draftclsnofoot,onecolumn,12pt,twoside]{IEEEtran}
%\documentclass[journal]{IEEEtran}
%\pagestyle{headings}
%\renewcommand\baselinestretch{1.0}
%\documentclass{article}
\usepackage{soul}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[justification=centering]{caption}
\usepackage{color, xcolor}
\usepackage{diagbox}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{cite}
\usepackage{hyperref} %ÒýÓÃÌø×ª
\usepackage{blkarray}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pstricks}
\usepackage{rotating}
\usepackage{booktabs} % table bold
%%%%%%%%%%%%%%%%%%%%%%%%%%%ÉŸ³ý·ûºÅºêÃüÁî
\usepackage{ulem}
\usepackage{blkarray} %add indices for array
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\allowdisplaybreaks[4]

\theoremstyle{}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\maximize}{maximize}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{construction}{Construction}
\newtheorem{partition}{Partition}
\newtheorem{proposition}{Proposition}
\newtheorem{property}{Property}



\newcommand{\tabcaption}{\def\@captype{table}\caption}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}


\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1{\bullet}$}}}}}
\newenvironment{breakablealgorithm}
  { %\begin{breakablealgorithm}
   \begin{center}
     \refstepcounter{algorithm}% New algorithm
     \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
     \renewcommand{\caption}[2][\relax]{% Make a new \caption
       {\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
       \ifx\relax##1\relax % #1 is \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
       \else % #1 is not \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
       \fi
       \kern2pt\hrule\kern2pt
     }
  }{ %\end{breakablealgorithm}
     \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
   \end{center}
  }
\makeatother

\normalsize
\normalem

\ifCLASSINFOpdf

\else

\fi

\hyphenation{op-tical net-works semi-conduc-tor}
\begin{document}
\title{   Cascaded  Code Distributed Computing  With Low Complexity and Improved Flexibility    
\author{\IEEEauthorblockN{Mingming Zhang, Youlong Wu, Minquan Cheng, and Dianhua Wu}
	\thanks{M. Zhang and M. Cheng are with Guangxi Key Lab of Multi-source Information Mining $\&$ Security, Guangxi Normal University,
		Guilin 541004, China  (e-mail: ztw\_07@foxmail.com, chengqinshi@hotmail.com).}
	\thanks{Y. Wu is with the School of Information Science and Technology, ShanghaiTech University, Shanghai 201210, China  (e-mail:  wuyl1@shanghaitech.edu.cn).}
	\thanks{D. Wu is with the School of Mathematics and Statistics,  Guangxi Normal University,   Guilin 541006, China (e-mail: dhwu@gxnu.edu.cn). }
}
}
\maketitle

\begin{abstract}
Coded distributed computing, proposed by Li \emph{et al.}, offers significant potential for reducing the communication load in MapReduce computing systems. 
In the setting of the \emph{cascaded} coded distributed computing that consisting of  $K$ nodes, $N$ input files, and $Q$ output functions, the objective is to compute each output function through $s\geq 1$ nodes with a computation load $r\geq 1$, enabling the application of coding techniques during the Shuffle phase to achieve minimum communication load.
However, for most existing coded distributed computing schemes, a major limitation lies in their demand for splitting the original data into an exponentially growing number of input files in terms of $N/\binom{K}{r}
\in\mathbb{N}$ and requiring an exponentially large number of output functions $Q/\binom{K}{s}
\in\mathbb{N}$, which imposes stringent requirements for implementation and results in significant coding complexity when $K$ is large.
In this paper, we focus on the cascaded case of $K/s\in\mathbb{N} $, deliberately designing the strategy of input files store and output functions assignment based on a grouping method, such that a low-complexity two-round Shuffle phase is available.
The main advantages of our proposed scheme contains: 1) the communication load is quilt close to or surprisingly better than the optimal state-of-the-art scheme proposed by Li \emph{et al.}; 2) our scheme requires significantly less number of input files and output functions; 3) all the operations are implemented over the minimum binary field $\mathbb{F}_2$.
%Furthermore, when $s=1$, we have the optimality of our proposed scheme since the result is the same as the optimal scheme proposed by Li \emph{et al.}.
\end{abstract}

\begin{IEEEkeywords}
Cascaded coded distributed computing, MapReduce, computation-communication tradeoff
\end{IEEEkeywords}



\section{Introduction}
Distributed computing systems have been extensively employed for the execution of large-scale computing tasks due to their ability to significantly enhance execution speed.
This is achieved by allowing computation jobs to run in parallel while leveraging distributed computing and storage resources. However, in the context of massive data exchanged among computing nodes, distributed computing systems face a severe communication bottleneck arising from limited resources and high traffic load. For instance, in applications such as TeraSort \cite{TeraSort} and SelfJoin \cite{SelfJoin} running on an Amazon EC2 cluster, the data exchange process costs $65\%\sim 70\%$ of the overall job execution time \cite{CZMJS}.

In the pioneering paper \cite{LMYA}, Li \emph{et al.} extended the coding technique to distributed computing in order to  reduce the communication load by increasing the computations of input data. In the considered MapReduce setting, $K$ distributed computing nodes are utilized to compute $Q$ output functions from $N$ input files by three phases: Map, Shuffle, and Reduce.
During the Map phase, each input file is uniquely assigned by a distinct $r$-subset of nodes, then locally computed (mapped) to $Q$ equal size intermediate values (IVs) in each of the assigned nodes.
During the Shuffle phase, each node generates multicast messages by using its locally computed IVs, then broadcasts them to other nodes such that all the nodes can reconstruct their needed IVs to compute the assigned output functions in the subsequent Reduce phase.
During the Reduce phase, each output function will be computed (reduced) by a distinct $s$-subset of nodes.
When $s\geq 1$, the setting is called  \textit{cascaded coded distributed computing}, which matches the multiple rounds distributed computing jobs where the output results of the previous round serve as the input of the next round. In \cite{LMYA}, the authors demonstrated that their cascaded scheme achieves optimal communication load when  $N/\binom{K}{r} \in\mathbb{N}$\footnote{The condition $N/\binom{K}{r} \in\mathbb{N}$  can be relaxed if $N$ is sufficiently large.} and  $Q/\binom{K}{s} \in\mathbb{N}$.  %each output function is computed by a different of $s$-subset of nodes
Following the aforementioned coded distributed computing system, various problems have been studied in \cite{CDC1,YWYT,JC,horii2020improved,KR,compresscoded,WJC,chen2020coded,woolsey2021new,xu2021new,wang2021batch,reisizadeh2019coded,kim2019optimal,WWCJ,ELMA,YYW,Wang22,CDCsurvey,BE,BWCE}.
In this paper, we focus on the cascaded coded distributed computing problem.

Note that the above-mentioned coded distributed computing schemes need exponentially large quantities of both input files and output functions, denoted by $\binom{K}{r}$ and $\binom{K}{s}$, respectively.
As a consequence, practical implementations encounter unexpected performance degradation, particularly when $K$ assumes relatively large values.  To reduce the required numbers of both input files and output functions, the authors in \cite{JQ} generate cascaded coded distributed computing schemes by using the placement delivery array (PDA) under the binary field $\mathbb{F}_2$.
Since PDA was originally proposed to reduce the subpacketization of the coded caching problem in \cite{YCTC} and does not characterize the node requests when multiple nodes request the same content, these schemes fail to effectively utilize the common IVs required by multiple nodes, thus leading to the redundant communication load.
In \cite{WCJ}, the authors proposed the scheme based on hypercube structure under the constraint of $r=s$, which achieves less number of input files and output functions, and a high-performance communication load that proved to be asymptotically optimal.
However, both the number of input files and output functions are still exponentially in the form of $(\frac{K}{r})^{r-1}$.
In \cite{JWZ}, the authors employed a symmetric balanced incomplete block design (SBIBD) to present the strategy of  input files store and output functions assignment,  ultimately generating an asymptotically cascaded scheme with $K=N=Q$, while under the constraint of $r=s$ or $r+s=K$.
In \cite{CWL}, when $r=s$, the authors proposed low-complexity and asymptotically optimal cascaded schemes under the minimum binary field $\mathbb{F}_2$, by using combinatorial structures $t$-design ($t\geq2$) and group divisible $t$-design, then further extended the constructions to the case $r\neq s$.
In \cite{CLCEL}, the authors proposed two asymptotically optimal cascaded schemes with $K=N=Q$ via combinatorial designs. One of them uses symmetric design (SD) for the case $r+s=K$, while achieving a lower communication load than \cite{JWZ}; the other one uses $1$-design from almost different (AD) sets. 


It is worth noting that the aforementioned schemes that reduce the numbers of input files and output functions strongly rely on combinatorial structures, which impose stringent requirements on system parameters, i.e., their schemes are only feasible for specific $K$, $r$, and $s$ satisfying  $r=s$ or $r+s=K$.
In this paper, we propose a cascaded coded distributed computing scheme with a more flexible choice of computation load $r$ by carefully designing the assignments of input files and output functions based on a grouping method,  developing a low-complexity two-round Shuffle phase. Our contributions are summarized as follows.

\begin{itemize}
\item   The communication load of the proposed scheme is quite close to or surprisingly better than that of the optimal work \cite{LMYA}, especially when the computation load $r$ is not large.
\item   In our scheme, the required number of input files increases with ${K/s\choose (r-1)/s+1}(r+s-1)$, and the required number of output functions only linearly increases with $K$ (which is $K/s$), significantly relaxing the requirements for exponentially large numbers of input files and output functions in \cite{LMYA}. Besides, our scheme allows $r\neq s$ and $r+s\neq K$, improving the feasibility of the state-of-art schemes in \cite{WCJ,JWZ,CWL, CLCEL}.
\item    The proposed scheme operates on the minimum binary field $\mathbb{F}_2$, significantly reducing the  coding complexity.  
%\item  We have the optimality under the constraint of  $s=1$, since the result of our proposed scheme is the same as \cite{LMYA}.
\end{itemize}

This paper is organized as follows. In Section~\ref{sec-system model}, the system model and problem formulation are introduced. In Section~\ref{sec-main result}, the main results and some numerical analysis are proposed. Then, we present an illustrated example of the proposed achievable scheme in Section~\ref{sec-example}, and provide the detailed constructions of the proposed scheme in Section~\ref{sec-proof}. Finally, Section~\ref{sec-conclusion} concludes this paper.


\subsection*{Notations}
In this paper,  we use the following notations unless otherwise stated.
\begin{itemize}
\item Bold capital letter, bold lower case letter and curlicue font are used to denote array, vector and set respectively.
%The $r^{\text{th}}$ element of a set represents the  $r^{\text{th}}$ smallest element in this set.
$|\cdot|$ is used to represent the cardinality of a set or the length of a vector;
	
\item For any positive integers $a$, $b$, $t$ with $a<b$ and $t\leq b $,   non-negative set $\mathcal{V},\mathcal{J}$,
	\begin{itemize}
		\item   $[a]:=\{1,2,\ldots,a\}$, $[a:b] :=\{a,a+1,\ldots,b\}$, $[a:b):=\{a,a+1,\ldots,b-1\}$ and ${[b]\choose t}:=\{\mathcal{V}\ :\   \mathcal{V}\subseteq [b], |\mathcal{V}|=t\}$, i.e., ${[b]\choose t}$ is the collection of all $t$-sized subsets of $[b]$. Let $a|b$ denote  that $b/a\in\mathbb{N}$. 

		\item Assuming that all the sets are in increasing order.
		$\mathcal{V}[j]$ represents the $j^{\text{th}}$ smallest element of $\mathcal{V}$, where $j\in[|\mathcal{V}|]$, and $\mathcal{V}[\mathcal{J}]=\{\mathcal{V}_j:j\in\mathcal{J}\}$.
	\end{itemize}


\end{itemize}

\section{System Model}
\label{sec-system model}
In the coded distribute computing model \cite{LMYA}, the task is to compute $Q$ output functions from $N$ input files  by using $K$ computing nodes, where $Q$, $N$, and $K$ are positive integers.
The $N$ input files are denoted by $w_1,\ldots,w_N$;
the $Q$ output functions are denoted by $\phi_1,\ldots,\phi_Q$.
For each $q\in[Q]$, we define $\mathcal{A}_q\subseteq[K]$ as the set of nodes that are assigned to compute the output function $\phi_q$, which maps all the $N$ input files into a stream
\begin{align}
	u_q&\triangleq\phi_q(w_{1},w_{2},\ldots,w_{N})\\
	&\triangleq h_q(g_{q,1}(w_{1}),g_{q,2}(w_{2}),\ldots,g_{q,N}(w_{N})) \in \mathbb{F}_{2^E},
\end{align}
for some integer $E$. For any $q\in [Q]$ and $n\in [N]$,  $g_{q,n}(\cdot)$ is  called Map function and $h_q(\cdot)$ is called Reduce function. The parameter $v_{q,n}\triangleq g_{q,n}(w_{n})$ where $q\in [Q]$ and $n\in [N]$ is called intermediate value (IV). We assume that each IV has $T$ bits, for some positive $T$. In order to support multiple-round computing where the reduced results of the previous round are the inputs of the next round Map operation, each Reduce function is assumed to be computed by $s\in[K]$ nodes. The cascaded coded distributed computing problem consists of the following three phases.

\textit{Map Phase}:
For each $n\in[N]$, we define $\mathcal{D}_n\subseteq[K]$ as the set of nodes that store  the input file $w_n$.
Thus for each node $k$ where $k\in[K]$, the set of stored input files is denoted by
\begin{align}
\mathcal{M}_k=\{w_n:n\in[N],k\in\mathcal{D}_n\}. \label{eq-model Mk}
\end{align}
Each node $k$ computes the IVs $\mathcal{I}_k=\{v_{q,n}=g_{q,n}(w_{n})\ |\ q\in [Q],w_n\in \mathcal{M}_k\}$ locally by using the stored files in \eqref{eq-model Mk} and Map functions $\{g_{q,n}(\cdot)\}$.


\textit{Shuffle Phase}:
According to the output functions arranged, for each node $k$ where $k\in[K]$, the set of assigned output functions is denoted by
\begin{align}
\mathcal{W}_k=\{\phi_q:q\in[Q],k\in\mathcal{A}_q\}. \label{eq-model Wk}
\end{align}
Each node transmits multicast message(s) derived from locally computed IVs to the other nodes, such that after the Shuffle phase, each node can recover all necessary IVs of the assigned output functions.
We denote the multicast message(s) transmitted by node $k\in[K]$ as $X_k\in\mathbb{F}_{2^{l_k}}$. 


\textit{Reduce Phase}:
During the Reduce phase, each node $k\in[K]$ reconstructs all the needed IVs for each $q\in\mathcal{W}_k$ by using the messages transmitted in the Shuffle phase and the IVs computed locally in $\mathcal{I}_k$.

The \textit{computation load} is defined as the normalized total number of files stored by all the $K$ nodes, which is formulated by
\begin{align}
	r:=\frac{\sum_{k\in[K]}|\mathcal{M}_k|}{N}\label{eq-def of r}.
\end{align}


The \textit{communication load} is defined as the normalized total number of bits transmitted by all the $K$ nodes during the Shuffle phase, which is formulated by
\begin{align}
L:=\frac{\sum_{k\in[K]}|X_k|}{QNT}=\frac{\sum_{k\in[K]}l_k}{QNT}\label{eq-def of L}.
\end{align}

When $N/{K\choose r}\in\mathbb{N}$ and $Q/{K\choose s}\in\mathbb{N}$ (or $N\rightarrow\infty,Q\rightarrow\infty$), Li \textit{et al.} proposed the optimal computation-communication tradeoff in \cite{LMYA}, which is illustrated as follows.
\begin{remark}[\cite{LMYA}]
For any positive integers $K$, $r$ and $s$, there exists a cascaded coded distributed computing scheme achieving the optimal communication load
\begin{align}\label{eq-converse}
	L_{\text{Li \textit{et al.}}}(r,s)=\sum\limits_{l=\max\{r+1,s\}}^{\min\{r+s,K\}}\left(\frac{{K-r\choose K-l}{r\choose l-s}}{{K\choose s}}\cdot \frac{l-r}{l-1}\right),
\end{align}
where $r$ is the computation load,  $s$ is the number of nodes that compute each reduce function.
\end{remark}

We aim to design a low-complexity and communication-efficient scheme that reduce the communication load $L$ for given $r$ and $s$, with small quantities of $N$ and $Q$.

\section{Main results}
\label{sec-main result}

By deliberately designing the input file and output function assignments, we propose the cascaded coded distributed computing scheme as follows. The proof could be found in Section~\ref{sec-proof}.
\begin{theorem}[Achievable Scheme]
\label{th-proposed scheme}
In the cascaded coded distributed computing framework with $s>1$ and $s|K$, the following  communication load is achievable,
\begin{align}
L(r,s)=\frac{s(K-r+1)}{K(r+s-1)}, \ \ \ \forall r\in\{s+1,2s+1,\ldots,(K/s-1)s+1\},
\label{eq-Load}
\end{align}
and $L(K,s)=0$.
The proposed scheme requires $N/ \big({K/s\choose (r-1)/s+1}(r+s-1)\big)\in\mathbb{N}$ input files and $Q/(K/s)\in \mathbb{N}$ output functions.
\end{theorem}



\subsection{Performance Analysis}
\label{subsec-performance analysis}

% Figure environment removed

% Figure environment removed

We present some numerical comparisons of the proposed scheme in Theorem~\ref{th-proposed scheme} and the well-known optimal scheme in \cite{LMYA}.
It can be seen that compared with the results in \cite{LMYA}, not only the communication load of our proposed scheme is quite close to or even better than that of \cite{LMYA}, but also significantly reduces the number of input files.
Furthermore, the proposed scheme is implemented over the binary field, $\mathbb{F}_2$.
More precisely, we first focus on the cascaded coded computing problem with $K=30$ and $s=3$, as illustrated in Fig.~\ref{fig-K30s3}.
In Fig.~\ref{subfig-K30s3L}, the communication loads of our proposed scheme are quite close to those of \cite{LMYA} under different computation load $r$; in Fig.~\ref{subfig-K30s3N}, our proposed scheme has a significant advantage in the number of input files compare to the result in \cite{LMYA}.
Then, we focus on the cascaded coded computing problem with $K=30$ and $s=5$, as illustrated in Fig.~\ref{fig-K30s5}. Surprisingly, with the increase of $s$, in  Fig.~\ref{subfig-K30s5L}, some of the communication loads achieved by our proposed scheme are better than those of \cite{LMYA}, especially when
$r$ is not large; in Fig.~\ref{subfig-K30s3N}, the proposed scheme still has a significant advantage in the number of input files compare to the result in \cite{LMYA}.



\section{An Illustrative Example of the Proposed Coded Distributed Scheme}
\label{sec-example}
We consider the system with $K=6$ nodes that compute $Q=3$ output functions from $N=12$ input files, the computation load $r=3$, and each Reduce function is computed by $s=2$ nodes.  The main idea is that we divide the $K=6$ nodes  into $s=2$ groups,
\begin{align}
	\mathcal{G}_1=\{1,2,3\}, \ \ \ \ \ \ \mathcal{G}_2=\{4,5,6\}, \label{eq-G1G2}
\end{align}
Then as Table \ref{tab-exam all} shows the stored input files, assigned output functions, and the delivered multicasting messages of the $6$ computing nodes, we deliberately design the input files store and output functions assignment by using the relationship between each group of nodes, such that only two-round transmissions are required during the Shuffle phase. 
\begin{table}
\center
\caption{Files, functions, IVs, and multicasts of the $(K,r,s)=(6,3,2)$ cascaded coded distributed computing scheme}\label{tab-exam all}
\renewcommand\arraystretch{1}
\setlength{\tabcolsep}{0.6mm}{
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
node & files & computed IVs & function & requested IVs & round I  & round II  \\ \hline
1 & $w_1$, $w_2$, $w_3$ & $v_{1,1}$, $v_{2,1}$, $v_{3,1}$, $v_{1,2}$, $v_{2,2}$, $v_{3,2}$, $v_{1,3}$, $v_{2,3}$, $v_{3,3}$ & $\phi_1$ & $v_{1,5}$, $v_{1,6}$, $v_{1,8}$ & $v_{1,2}^{(1)}\oplus v_{2,1}^{(1)}$ & $v_{2,4}\oplus v_{3,2}$ \\
 & $w_4$, $w_7$, $w_9$ &  $v_{1,4}$, $v_{2,4}$, $v_{3,4}$, $v_{1,7}$, $v_{2,7}$, $v_{3,7}$, $v_{1,9}$, $v_{2,9}$, $v_{3,9}$ & & $v_{1,10}$, $v_{1,11}$, $v_{1,12}$ & $v_{1,4}^{(1)}\oplus v_{3,3}^{(1)}$ &  \\ \hline
2 & $w_1$, $w_2$, $w_5$ & $v_{1,1}$, $v_{2,1}$, $v_{3,1}$, $v_{1,2}$, $v_{2,2}$, $v_{3,2}$, $v_{1,5}$, $v_{2,5}$, $v_{3,5}$ & $\phi_2$ & $v_{2,3}$, $v_{2,4}$, $v_{2,7}$ & $v_{1,2}^{(2)}\oplus v_{2,1}^{(2)}$ & $v_{1,6}\oplus v_{3,1}$ \\
& $w_6$, $w_8$, $w_{11}$ &  $v_{1,6}$, $v_{2,6}$, $v_{3,6}$, $v_{1,8}$, $v_{2,8}$, $v_{3,8}$, $v_{1,11}$, $v_{2,11}$, $v_{3,11}$ & & $v_{2,9}$, $v_{2,10}$, $v_{2,12}$ & $v_{2,6}^{(1)}\oplus v_{3,5}^{(1)}$ &  \\ \hline
3 & $w_3$, $w_4$, $w_5$ & $v_{1,3}$, $v_{2,3}$, $v_{3,3}$, $v_{1,4}$, $v_{2,4}$, $v_{3,4}$, $v_{1,5}$, $v_{2,5}$, $v_{3,5}$ & $\phi_3$ & $v_{3,1}$, $v_{3,2}$, $v_{3,7}$ & $v_{1,4}^{(2)}\oplus v_{3,3}^{(2)}$ & $v_{1,5}\oplus v_{2,3}$ \\
& $w_6$, $w_{10}$, $w_{12}$ &  $v_{1,6}$, $v_{2,6}$, $v_{3,6}$, $v_{1,10}$, $v_{2,10}$, $v_{3,10}$, $v_{1,12}$, $v_{2,12}$, $v_{3,12}$ & & $v_{3,8}$, $v_{3,9}$, $v_{3,11}$ & $v_{2,6}^{(2)}\oplus v_{3,5}^{(2)}$ &  \\ \hline
4 & $w_1$, $w_3$, $w_7$ & $v_{1,1}$, $v_{2,1}$, $v_{3,1}$, $v_{1,3}$, $v_{2,3}$, $v_{3,3}$, $v_{1,7}$, $v_{2,7}$, $v_{3,7}$ & $\phi_1$ & $v_{1,2}$, $v_{1,4}$, $v_{1,5}$ & $v_{1,8}^{(1)}\oplus v_{2,7}^{(1)}$ & $v_{2,10}\oplus v_{3,8}$ \\
& $w_8$, $w_9$, $w_{10}$ &  $v_{1,8}$, $v_{2,8}$, $v_{3,8}$, $v_{1,9}$, $v_{2,9}$, $v_{3,9}$, $v_{1,10}$, $v_{2,10}$, $v_{3,10}$ & & $v_{1,6}$, $v_{1,11}$, $v_{1,12}$ & $v_{1,10}^{(1)}\oplus v_{3,9}^{(1)}$ &  \\ \hline
5 & $w_2$, $w_5$, $w_7$ & $v_{1,2}$, $v_{2,2}$, $v_{3,2}$, $v_{1,5}$, $v_{2,5}$, $v_{3,5}$, $v_{1,7}$, $v_{2,7}$, $v_{3,7}$ & $\phi_2$ & $v_{2,1}$, $v_{2,3}$, $v_{2,4}$ & $v_{1,8}^{(2)}\oplus v_{2,7}^{(2)}$ & $v_{1,12}\oplus v_{3,7}$ \\
& $w_8$, $w_{11}$, $w_{12}$ &  $v_{1,8}$, $v_{2,8}$, $v_{3,8}$, $v_{1,11}$, $v_{2,11}$, $v_{3,11}$, $v_{1,12}$, $v_{2,12}$, $v_{3,12}$ & & $v_{2,6}$, $v_{2,9}$, $v_{2,10}$ & $v_{2,12}^{(1)}\oplus v_{3,11}^{(1)}$ &  \\ \hline
6 & $w_4$, $w_6$, $w_9$ & $v_{1,4}$, $v_{2,4}$, $v_{3,4}$, $v_{1,6}$, $v_{2,6}$, $v_{3,6}$, $v_{1,9}$, $v_{2,9}$, $v_{3,9}$ & $\phi_3$ & $v_{3,1}$, $v_{3,2}$, $v_{3,3}$ & $v_{1,10}^{(2)}\oplus v_{3,9}^{(2)}$ & $v_{1,11}\oplus v_{2,9}$ \\
& $w_{10}$, $w_{11}$, $w_{12}$ &  $v_{1,10}$, $v_{2,10}$, $v_{3,10}$, $v_{1,11}$, $v_{2,11}$, $v_{3,11}$, $v_{1,12}$, $v_{2,12}$, $v_{3,12}$ & & $v_{3,5}$, $v_{3,7}$, $v_{3,8}$ & $v_{2,12}^{(2)}\oplus v_{3,11}^{(2)}$ &  \\ \hline
\end{tabular}}
\end{table}
Now let us introduce 1) input files placement; 2) output functions assignment; and 3) the transmission strategy in the two-round Shuffle phase in detail.

%\textit{1) Computing Nodes Grouping}: The $K=6$ nodes are evenly divided into $s=2$ groups,
%\begin{align}
%\mathcal{G}_1=\{1,2,3\}, \ \ \ \ \ \ \mathcal{G}_2=\{4,5,6\}. \label{eq-G1G2}
%\end{align}

\textit{1) Input Files Store}: Recall that $\mathcal{D}_n$, $n\in[N]$ presents the set of nodes storing the input file $w_n$. For each $n\in[12]$, we let $|\mathcal{D}_n|=r=3$, i.e., each file is stored by $3$ nodes.
To determine which nodes a file is stored by, we first select $1$ group of nodes, and choose $(r-1)/s+1=2$ nodes of it; then for the nodes in another group, we can choose any $(r-1)/s=1$ node with the same position as the previously selected nodes.
Hence, there are $N=s{K/Q\choose (r-1)/s+1}{(r-1)/s+1\choose (r-1)/s}=12$ different input files.
For instance, $w_1$ is stored by nodes ${1,2,4}$, where nodes ${1,2}$ are the first and second elements belonging to  $\mathcal{G}_1$, thus we can choose node ${4}$ which is the first element  belonging to $\mathcal{G}_2$, then we have $\mathcal{D}_1=\{1,2,4\}$.
Similarly, all the input files are stored as follows,
\begin{align}
&\mathcal{D}_1=\{1\!,\!2\!,\!4\}, & \mathcal{D}_2=\{1\!,\!2\!,\!5\}, \ \ & \mathcal{D}_3=\{1\!,\!3\!,\!4\},   & \mathcal{D}_4=\{1\!,\!3\!,\!5\}, \ \  &\mathcal{D}_5=\{2\!,\!3\!,\!5\},   &\mathcal{D}_6=\{2\!,\!3\!,\!6\},\nonumber\\
&\mathcal{D}_7=\{1\!,\!4\!,\!5\},   & \mathcal{D}_8=\{2\!,\!4\!,\!5\}, \ \ & \mathcal{D}_9=\{1\!,\!4\!,\!6\},   & \mathcal{D}_{10}=\{3\!,\!4\!,\!6\}, \ \  &\mathcal{D}_{11}=\{2\!,\!5\!,\!6\},   &\mathcal{D}_{12}=\{3\!,\!5\!,\!6\}. \label{eq-A1-A12}
\end{align}
By \eqref{eq-model Mk}, we obtain the stored input files for each node as shown in the second column of Table~\ref{tab-exam all}.

\textit{2) Output Functions Assignment}:
Recall that $\mathcal{A}_q$, $q\in[Q]$ represents the set of nodes  that are assigned with the output function $\phi_q$. Since $s=2$, i.e., each function is assigned to $2$ nodes, we have $|\mathcal{A}_q|=2$ where $q\in[3]$.
For an output function, we assign it to $s$ nodes, which are distributed in $s$ groups with the same position. Hence, there are $Q=K/s=3$ different output functions.
For instance, $\phi_1$ is assigned to nodes $1,4$, both of which are the first element of $\mathcal{G}_1$ and $\mathcal{G}_2$. Thus we have $\mathcal{A}_1=\{1,4\}$.
Similarly, all the output functions are assigned as follows,
\begin{align}
\mathcal{A}_1=\{1,4\}, \ \ \ \ \ \ \mathcal{A}_2=\{2,5\}, \ \ \ \ \ \ \mathcal{A}_3=\{3,6\}. \label{eq-B1-b3}
\end{align}
By \eqref{eq-model Wk}, we obtain the assigned output functions for each node as shown in the fourth column of Table~\ref{tab-exam all}.

\textit{3) Two-rounds Shuffle Phase}:
In the Map phase, each node can compute all the IVs from its stored files, as shown in the third column of Table~\ref{tab-exam all}.
For instance, node 1 can compute the IVs $\{v_{q,n} : q\in[3],w_n\in\mathcal{M}_1\}$.
Thus there are totally $N\times Q=12\times3$ different IVs $\{v_{q,n}:q\in[3],n\in[12]\}$ computed by all the $6$ nodes.
We classify these IVs into 3 categories based on the number of nodes requesting them during the Shuffle phase.
\begin{itemize}
\item IVs requested by $0$ node: The following IVs are requested by $0$ node,
\begin{align}
v_{1,1}, \ v_{1,3}, \ v_{1,7}, \ v_{1,9}, \
v_{2,2}, \ v_{2,5}, \ v_{2,8}, \ v_{2,11}, \
v_{3,4}, \ v_{3,6}, \ v_{3,10}, \ v_{3,12}, \label{eq-IVs 0 node}
\end{align}
which means these IVs do not need to be transmitted during the Shuffle phase.
For instance, $v_{1,1}$ is needed by nodes $1$ and $4$ since they are assigned function $\phi_1$, while these nodes store the file $w_1$ such that they can compute $v_{1,1}$ in the Map phase. Thus we say $v_{1,1}$ is requested by $0$ node.
%Similarly, we have all the IVs requested by $0$ nodes shown as follows,

\item IVs requested by $s-1=1$ node: The following IVs are requested by $1$ node,
\begin{align}
v_{1,2}, \ v_{1,4}, \ v_{1,8}, \ v_{1,10}, \
v_{2,1}, \ v_{2,6}, \ v_{2,7}, \ v_{2,12}, \
v_{3,3}, \ v_{3,5}, \ v_{3,9}, \ v_{3,11}. \label{eq-IVs 1 node}
\end{align}
For instance, $v_{1,2}$ is needed by nodes $1$ and $4$, and node $1$ can compute $v_{1,2}$ since it stores the file $w_2$, thus $v_{1,2}$ is only requested by $1$ node (node $4$).

\item IVs requested by $s=2$ nodes: The following IVs are requested by $2$ nodes,
\begin{align}
v_{1,5}, \ v_{1,6}, \ v_{1,11}, \ v_{1,12}, \
v_{2,3}, \ v_{2,4}, \ v_{2,9}, \ v_{2,10}, \
v_{3,1}, \ v_{3,2}, \ v_{3,7}, \ v_{3,8}. \label{eq-IVs 2 nodes}
\end{align}
For instance, $v_{1,5}$ is requested by $2$ nodes (nodes $1$ and $4$), since neither node store the file $w_5$ to compute $v_{1,5}$.
\end{itemize}

There are two rounds of transmission in the Shuffle phase, where the multicast messages by each node are shown in the last columns of Table~\ref{tab-exam all}, respectively.
In round $1$, we consider the IVs in \eqref{eq-IVs 1 node} which are requested by $1$ node. Each IV is split into $(r-1)/s+1=2$ disjoint equally sized packets.
For instance, $v_{1,2}$ is split into $v_{1,2}^{(1)}$ and $v_{1,2}^{2}$, which are transmitted by nodes $1$ and $2$, respectively.
Each node transmits $2$ bit-wise XOR of two available packets, where each XOR is multicasted to the other $2$ nodes. Furthermore, each node recovers a requested IV by $2$ XOR from $2$ different nodes.
For instance, node $1$ multicasts $v_{1,2}^{(1)}\bigoplus v_{2,1}^{(1)}$ to nodes $4$ and $5$, where $v_{1,2}^{(1)}$ is requested by node $4$ and available at node $5$, and the opposite is true for $v_{2,1}^{(1)}$, thus they can decode $v_{1,2}^{(1)}$ and $v_{2,1}^{(1)}$, respectively. Besides, nodes $4$ and $5$ can receive the XOR $v_{1,2}^{(2)}\bigoplus v_{2,1}^{(2)}$ from node $2$, such that they can decode $v_{1,2}^{(2)}$ and $v_{2,1}^{(2)}$, respectively.
Hence, nodes $4$ and $5$ recover $v_{1,2}$ and $v_{2,1}$, respectively.

In round $2$, we consider the IVs in \eqref{eq-IVs 2 nodes} which are requested by $2$ nodes.
Each node multicasts an XOR of two locally computed IVs to the other $4$ nodes.
For instance, node $1$ multicasts $v_{2,4}\bigoplus v_{3,2}$ to nodes $2$, $3$, $5$ and $6$. Since nodes $2$, $5$ (the second element of $\mathcal{G}_1$ and $\mathcal{G}_2$) know $v_{3,2}$ locally, thus they can decode the requested $v_{2,4}$. Similarly, nodes $3$ and $6$ (the third element of $\mathcal{G}_1$ and $\mathcal{G}_2$)  can decode the requested $v_{3,2}$.

%It is noteworthy that in the proposed scheme, all the operations are implemented over the binary field, $\mathbb{F}$.
%the transmission is one-shot (i.e., in a one-shot transmission, each node can recover its requested IVs from the locally computed IVs and the transmitted messages by at most one other node, immediately.)
In the two-round transmission in the Shuffle phase, there are $12$ messages of length $T/2$ bits ($T$ is the size of a single IV), and $6$  messages of length $T$ bits. By \eqref{eq-def of L}, the communication load is,
\begin{align}
L=\frac{1}{QNT}\left(12\cdot\frac{T}{2}+6T\right)=\frac{12}{36}=0.333,
\end{align}
which coincides with \eqref{eq-Load}.






\section{Proof of Theorem~\ref{th-proposed scheme}}
\label{sec-proof}
In this section, we formally prove the result of Theorem~\ref{th-proposed scheme} by presenting the cascaded coded distributed computing scheme. In order to keep the notations simple, we let $\Lambda=K/s$, $t=(r-1)/s$.
	
	Firstly, we divide the $K$ nodes into $s$ groups with equal length. For $i\in[s]$, the $i$th group of nodes is denoted by,
	\begin{align}
		\mathcal{G}_i=\left\{(i-1)\Lambda+1, (i-1)\Lambda+2,\ldots, i\Lambda\right\},\label{eq-group}
	\end{align}
	where $\Lambda=K/s$.
	Thus $|\mathcal{G}_i|=\Lambda$, i.e., each group contains $\Lambda$ nodes.
	Note that, when $K=6$, $r=3$, $s=2$, the grouping denoted by \eqref{eq-group} is illustrated in \eqref{eq-G1G2}.
	Based on the grouping method, the scheme is described by the following three parts: 1) input files placement; 2) output functions assignment; and 3) two-round  Shuffle phase.  



%\subsection{Computing Nodes Grouping}
%\label{subsec-nodes group}
%We divide the $K$ nodes into $s$ groups with equal length. For $i\in[s]$, the $i$th group of nodes is denoted by,
%\begin{align}
%\mathcal{G}_i=\left\{(i-1)\Lambda+1, (i-1)\Lambda+2,\ldots, i\Lambda\right\},\label{eq-group}
%\end{align}
%where $\Lambda=K/s$.
%Thus $|\mathcal{G}_i|=\Lambda$, i.e., each group contains $\Lambda$ nodes.
%
%Note that, when $K=6$, $r=3$, $s=2$, the grouping denoted by \eqref{eq-group} is illustrated in \eqref{eq-G1G2}.



\subsection{Input Files Store}
\label{subsec-input store}

For simplicity of expression, for each $n\in[N]$, the subscript $n$ of input files $w_n$ is denoted by a tuple $(i,\mathcal{C},\mathcal{T})\in\mathcal{N}$, and formulated by
\begin{align}
\mathcal{N}=\left\{(i,\mathcal{C},\mathcal{T}) \ : \ \ i\in[s], \ \mathcal{C}\in{[\Lambda]\choose t+1}, \ \mathcal{T}\in{\mathcal{C}\choose t}\right\}, \label{eq-n index}
\end{align}
where $\Lambda=K/s$ and $t=(r-1)/s$.
Furthermore,
\begin{align}
	N=|\mathcal{N}|=s{\Lambda\choose t+1}{t+1\choose t}=s(t+1){\Lambda\choose t+1}=(r+s-1){K/s\choose (r-1)/s+1}, \label{eq-N}
\end{align}
which gives the number of input files.


Then, the file $w_{(i,\mathcal{C},\mathcal{T})}$ is stored by nodes in
\begin{align}
\mathcal{D}_{(i,\mathcal{C},\mathcal{T})}=\left\{\mathcal{G}_i[\mathcal{C}]\bigcup_{j\in[s]\setminus\{i\}}\mathcal{G}_j[\mathcal{T}] : i\in[s],  \mathcal{C}\in{[\Lambda]\choose t+1},  \mathcal{T}\in{\mathcal{C}\choose t}\right\}. \label{eq-file store}
\end{align}
Thus each file is stored by $|\mathcal{D}_{(i,\mathcal{C},\mathcal{T})}|=(t+1)+(s-1)t=r$ computing nodes.


Then, for each node $k\in[K]$, it stores the input files denoted by
\begin{align}
	\mathcal{M}_k=\{w_{(i,\mathcal{C},\mathcal{T})} : k\in\mathcal{D}_{(i,\mathcal{C},\mathcal{T})}\}.
\end{align}
We can check that the computation load constraint is satisfied, since
\begin{align}
	\frac{\sum_{k\in[K]}|\mathcal{M}_k|}{N}=\frac{K\cdot\left({\Lambda-1\choose t}(t+1)+(s-1){\Lambda-1\choose t}{t\choose t-1}\right)}{s(t+1){\Lambda\choose t+1}}=\frac{s\Lambda\cdot{\Lambda-1\choose t}(st+1)}{s(t+1){\Lambda\choose t+1}}=st+1=r.
\end{align}

\begin{example}
When $K=6$, $r=3$, $s=2$, by replacing the tuples $(i,\mathcal{C},\mathcal{T})$ denoted by \eqref{eq-n index} into integers $n\in[N]=[12]$ according to the one-to-one mapping $\varphi$ in Table~\ref{tab-mapping}, we have the results as illustrated in \eqref{eq-A1-A12}. Thus, the stored input files for each node $k\in[6]$ coincide with the second column of Table~\ref{tab-exam all}.
\begin{table}
\center
\caption{The mapping $\varphi$ }\label{tab-mapping}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$(i,\mathcal{C},\mathcal{T})$ & $(1,\{1,2\},\{1\})$ & $(1,\{1,2\},\{2\})$ & $(1,\{1,3\},\{1\})$ & $(1,\{1,3\},\{3\})$ & $(1,\{2,3\},\{2\})$ & $(1,\{2,3\},\{3\})$ \\ \hline
$n$ & $1$ & $2$ & $3$ & $4$ & $5$ & $6$  \\ \hline
$(i,\mathcal{C},\mathcal{T})$ & $(2,\{1,2\},\{1\})$ & $(2,\{1,2\},\{2\})$ & $(2,\{1,3\},\{1\})$ & $(2,\{1,3\},\{3\})$ & $(2,\{2,3\},\{2\})$ & $(2,\{2,3\},\{3\})$ \\ \hline
$n$ & $7$ & $8$ & $9$ & $10$ & $11$ & $12$  \\ \hline
\end{tabular}
\end{table}


For instance, when $n=1$, i.e., $(i,\mathcal{C},\mathcal{T})=(1,\{1,2\},\{1\})$, by \eqref{eq-file store} we have
\begin{align}
\mathcal{D}_{(1,\{1,2\},\{1\})}=\{\mathcal{G}_1[\{1,2\}]\cup\mathcal{G}_2[\{1\}]\}=\{1,2,4\},
\end{align}
since the first and second elements of $\mathcal{G}_1$ are $1,2$ and the first element of $\mathcal{G}_2$ is $4$;
when $n=12$, i.e., $(i,\mathcal{C},\mathcal{T})=(2,\{2,3\},\{3\})$, by \eqref{eq-file store} we have
\begin{align}
\mathcal{D}_{(2,\{2,3\},\{3\})}=\{\mathcal{G}_2[\{2,3\}]\cup\mathcal{G}_1[\{3\}]\}=\{3,5,6\},
\end{align}
since the second and third elements of $\mathcal{G}_2$ are $5,6$ and the third element of $\mathcal{G}_1$ is $3$.
\end{example}




\subsection{Output Functions Assignment}
\label{subsec-output assign}
For $q\in[Q]$, we define the output function assignment as follows,
\begin{align}
\mathcal{A}_q=\{\mathcal{G}_i[q] : i\in[s]\}. \label{eq-out assign}
\end{align}
Thus each function is assigned to $|\mathcal{A}_q|=s$ computing nodes.
Furthermore, there are totally
\begin{align}
Q=\Lambda=K/s \label{eq-Q}
\end{align}
different integer $q$, which gives the number of output functions.

Then, for each node $k\in[K]$, it is assigned the output functions denoted by
\begin{align}
\mathcal{W}_k=\{\phi_q:k\in\mathcal{A}_q\}.
\end{align}

Note that, when $K=6$, $r=3$, $s=2$, the output function assignment denoted by $\eqref{eq-out assign}$ is illustrated in \eqref{eq-B1-b3}.
Thus, the assigned output functions for each node $k\in[6]$ coincide with the fourth column of Table~\ref{tab-exam all}.



\subsection{Two-rounds Shuffle Phase and Reduce Phase}
\label{subsec-shuffle}
There are totally $N\times Q$ different IVs, $\{v_{q,(i,\mathcal{C},\mathcal{T})}: q\in[Q],(i,\mathcal{C},\mathcal{T})\in\mathcal{N}\}$, where $v_{q,(i,\mathcal{C},\mathcal{T})}$ is locally mapped by the nodes in $\mathcal{D}_{(i,\mathcal{C},\mathcal{T})}$ and requested by the nodes in $\mathcal{A}_q$ during the Shuffle phase.
For these IVs, based on the number of nodes requesting them during the Shuffle phase, they are classified into 3 categories: \textit{Type I} requested by $0$ node; \textit{Type II} requested by $s-1$ node; and \textit{Type III} requested by $s$ node.
Thus there are two rounds of transmission during the Shuffle phase, to deal with  Type II and Type III IVs, respectively.

\begin{itemize}
\item Type I IVs: The IVs requested by $0$ node, which are formulated by
\begin{align}
\mathcal{V}_{\text{I}}&=\left\{v_{q,(i,\mathcal{C},\mathcal{T})}:\mathcal{A}_q\subseteq\mathcal{D}_{(i,\mathcal{C},\mathcal{T})}\right\} \nonumber \\
&=\left\{v_{q,(i,\mathcal{C},\mathcal{T})}:\forall (i,\mathcal{C,\mathcal{T}})\in\mathcal{N}, q\in\mathcal{T}\right\}\label{eq-IVs I}.
\end{align}	
Thus the number of IVs belonging to Type I is
\begin{align}
F_0=|\mathcal{V}_{\text{I}}|=Nt. \label{eq-F0}
\end{align}

\begin{remark}
\label{rem-s=1 F0'}
When $s=1$, we have $\Lambda=K/s=K$ and $t=(r-1)/s=r-1$. Then, \eqref{eq-n index} is reduced to $\mathcal{N}'=\left\{(1,\mathcal{C},\mathcal{T}):\mathcal{C}\in{[K]\choose r},\mathcal{T}\in{\mathcal{C}\choose r-1}\right\}$, thus $N'=|\mathcal{N}'|=r{K\choose r}$.
Furthermore, for each $(1,\mathcal{C},\mathcal{T})\in\mathcal{N}$, $\mathcal{D}_{(1,\mathcal{C},\mathcal{T})}\in\mathcal{C}$; for each $q'\in[Q']=[K]$, $\mathcal{A}_q'=q'$. Hence, there are $F_0'=N'r$ distinct IVs belonged to Type I, since  $\mathcal{V}_{\text{I}}'=\{v_{q',(1,\mathcal{C},\mathcal{T})}:\forall{(1,\mathcal{C},\mathcal{T})\in\mathcal{N}'},q'\in\mathcal{C}\}$.
\end{remark}

	
\item Type II IVs: The IVs requested by $s-1$ nodes, which are formulated by
\begin{align}
\mathcal{V}_{\text{II}}&=\left\{v_{q,(i,\mathcal{C},\mathcal{T})}:\mathcal{A}_q\cap\mathcal{D}_{(i,\mathcal{C},\mathcal{T})}=1\right\} \nonumber \\
&=\{v_{q,(i,\mathcal{C},\mathcal{T})} : \forall (i,\mathcal{C},\mathcal{T})\in\mathcal{N},q=\mathcal{C}\setminus\mathcal{T}\} \label{eq-IVs II}.
\end{align}
Thus the number of IVs belonging to Type II is
\begin{align}
F_1=|\mathcal{V}_{\text{II}}|=N. \label{eq-F1}
\end{align}

\begin{remark}
\label{rem-s=1 F1'}
When $s=1$, i.e., $s-1=0$, there is $F_1'=0$ IV  belonging to Type II. Thus there is only one round (the round 2) transmission during the Shuffle phase.
\end{remark}

	
\item Type III IVs: The set of IVs requested by $s$ nodes, which is formulated by
\begin{align}
\mathcal{V}_{\text{III}}&=\left\{v_{q,(i,\mathcal{C},\mathcal{T})}:\mathcal{A}_q\cap\mathcal{D}_{(i,\mathcal{C},\mathcal{T})}=\emptyset\right\} \nonumber \\
&=\{v_{q,(i,\mathcal{C},\mathcal{T})}: \forall (i,\mathcal{C},\mathcal{T})\in\mathcal{N},q\in[\Lambda]\setminus\mathcal{C}\}\label{eq-IVs III}.
\end{align}
Thus the number of IVs belonging to Type III is
\begin{align}
F_2=|\mathcal{V}_{\text{III}}|=N(\Lambda-(t+1)). \label{eq-F2}
\end{align}
\begin{remark}
When $K=r+s-1$, i.e., $\Lambda=t+1$, we have $F_2=0$, thus there is only one round (the round 1) transmission during the Shuffle phase.
\end{remark}	
\begin{remark}
\label{rem-s=1 F2'}
When $s=1$, there are $F_2'=N'(K-r)$ distinct IVs belonging to Type III, since $\mathcal{V}_{\text{III}}'=\{v_{q',(1,\mathcal{C},\mathcal{T})}:\forall(1,\mathcal{C},\mathcal{T})\in\mathcal{N}',q'\in[K]\setminus\mathcal{C})\}$.
\end{remark}
\end{itemize}
We can check that, $F_0+F_1+F_2=Nt+N+N(\Lambda-(t+1))=N\times Q$.
Furthermore, when $s=1$, from Remarks \ref{rem-s=1 F0'}, \ref{rem-s=1 F1'} and \ref{rem-s=1 F2'}, we can check that $F_0'+F_1'+F_2'=N'r+0+N'(K-r)=N'K=N'\times Q'$.

In round 1, we focus on the IVs requested by $s-1$ nodes which are belonging to Type II.
For the nodes $\mathcal{K}=\mathcal{G}_i[\mathcal{C}]$, where $i\in[s],\mathcal{C}\in{[\Lambda]\choose t+1}$, each of which can multicast the following message,
\begin{align}
X_{\mathcal{K}}=\left(\bigoplus_{l\in\mathcal{L}} v_{l,(i,\mathcal{C},\mathcal{T})} \ : \ \mathcal{L}=\mathcal{C},\mathcal{T}=\mathcal{L}\setminus\{l\}\right), \label{eq-X1-sum}
\end{align}
to the nodes in  $\{\mathcal{A}_l\setminus \mathcal{K}:l\in\mathcal{L}\}$ over the broadcast channel. Thus the number of nodes served by one message is
\begin{align}
g_1=|\mathcal{A}_l\setminus \mathcal{K}|\cdot|\mathcal{L}|=(s-1)(t+1). \label{eq-g1}
\end{align}
To be average, each IV included in \eqref{eq-X1-sum} is divided into $|\mathcal{K}|=t+1$ disjoint equal-size packets. For each node $k=\mathcal{K}[j],j\in[t+1]$, it multicast the following message,
\begin{align}
X_k=X_{\mathcal{K}[j]}=\left(\bigoplus_{l\in\mathcal{L}} v_{l,(i,\mathcal{C},\mathcal{T})}^{(j)} \ : \ \mathcal{L}=\mathcal{C},\mathcal{T}=\mathcal{L}\setminus\{l\}\right), \label{eq-X1}
\end{align}
to the nodes belonged to $\{\mathcal{A}_l\setminus \mathcal{K}:l\in\mathcal{L}\}$.
Furthermore, there are totally $S_1$ messages transmitted in round 1, where
\begin{align}
S_1=(t+1)s{\Lambda\choose t+1}. \label{eq-S1}
\end{align}
We can check that in round 1, there are totally multicast
\begin{align}
	\frac{g_1\cdot S_1}{(t+1)(s-1)}=\frac{(s-1)(t+1)\cdot(t+1)s{\Lambda\choose t+1}}{(t+1)(s-1)}=s(t+1){\Lambda\choose t+1}=N
\end{align}
distinct IVs, which matches $F_1$ in \eqref{eq-F1}, thus the decodability of round 1 is guaranteed.






In round 2, we focus on the IVs requested by $s$ nodes which are belonged to Type III.  For each node $k=\mathcal{G}_i[j], i\in[s], j\in[\Lambda]$, it multicast the following message,
\begin{align}
X_k=X_{\mathcal{G}_i[j]}=\left(\bigoplus_{l\in\mathcal{L}}v_{l,(i,\mathcal{C},\mathcal{T})} \ : \ \mathcal{L}\subseteq[\Lambda]\setminus\{j\},|\mathcal{L}|=t+1,\mathcal{C}=\{j\}\cup\mathcal{L}\setminus\{l\},\mathcal{T}=\mathcal{L}\setminus\{l\}\right), \label{eq-X2}
\end{align}
to the nodes belonging to $\{\mathcal{A}_l:l\in\mathcal{L}\}$ over the broadcast channel. Thus the number of nodes served by one message is
\begin{align}
g_2=|\mathcal{A}_l|\cdot|\mathcal{L}|=s(t+1)=r+s-1. \label{eq-g2}
\end{align}
Furthermore, there are totally $S_2$ messages transmitted in round 2, where
\begin{align}
	S_2=K{\Lambda-1\choose t+1}. \label{eq-S2}
\end{align}
We can check that in round 2, there are totally multicast
\begin{align}
\frac{g_2\cdot S_2}{s}&=\frac{s(t+1)\cdot K{\Lambda-1\choose t+1}}{s}=(t+1)\cdot s\Lambda\frac{\Lambda-(t+1)}{\Lambda}{\Lambda\choose t+1} \nonumber \\
&=s(t+1){\Lambda\choose t+1}(\Lambda-(t+1))=N(\Lambda-(t+1))
\end{align}
distinct IVs, which matches $F_2$ in \eqref{eq-F2}, thus the decodability of round 2 is guaranteed.

\subsection{Communication Load}
From \eqref{eq-N}, \eqref{eq-Q}, \eqref{eq-S1}, and \eqref{eq-S2}, the communication load guaranteed by the scheme described above is given by
\begin{align}
L(r,s)&=\frac{S_1\cdot T/(t+1)+S_2\cdot T}{QNT} \\
&=\frac{s{\Lambda\choose t+1}+K{\Lambda-1\choose t+1}}{\Lambda s(t+1){\Lambda\choose t+1}} =\frac{s{\Lambda\choose t+1}+\frac{K(\Lambda-(t+1))}{\Lambda}{\Lambda\choose  t+1}}{\Lambda s(t+1){\Lambda\choose t+1}} \\
&=\frac{\Lambda-t}{\Lambda(t+1)}=\frac{K/s-(r-1)/s}{K/s((r-1)/s+1)} \\
&=\frac{s(K-r+1)}{K(r+s-1)}, \label{eq-L}
\end{align}
which coincides with Theorem~\ref{th-proposed scheme}.
\begin{remark}
When $s=1$, the communication load guaranteed by the scheme described above is given by
\begin{align}
L(r)=\frac{S_2T}{Q'N'T}=\frac{K{K-1\choose r}}{Kr{K\choose r}}=\frac{1}{r}\cdot(1-\frac{r}{K}),
\end{align}
which is the same as the result \cite[Theorem 1]{LMYA}.
\end{remark}

\section{Conclusion and On-Going Works}
\label{sec-conclusion}
In this paper, we proposed a novel cascaded coded distributed computing scheme by carefully designing the input files store and output functions assignment strategy.
Compared with the optimal scheme \cite{LMYA}, the communication load of our proposed scheme can outperform or quite close to that of \cite{LMYA}, the number of requested input files and output functions are significantly less, and all the operations of the proposed scheme are implemented over the binary field.
Furthermore, when $s=1$, the result of our proposed scheme is the same as that of \cite{LMYA}, which means the optimality is guaranteed.
On-going works include the derivation of the converse bounds for the cascaded coded distributed computing problem, optimality of the proposed scheme, and the construction for more general case $s\nmid K$.

\bibliographystyle{IEEEtran}
\bibliography{ref}

\end{document}





