\section{Evaluation Metrics}

Before understanding the following evaluation indicators, we must know some of the meanings of the following expressions. For multi label classification tasks, we assume that there is a set of $D$ that is a multi label dataset consisting of $n$ samples $(x_i, Y_i)$, $1\leq i\leq n$. If $c$ is a multi label classifier, then $Z_i = c(x_i)$ is the predicted output of the corresponding $x_i$.

\subsection{label-based}
\textbf{Precision:} Accuracy is the ratio of the predicted correct labels to the total number of actual labels. The average of all instances, known as the accuracy rate. In a multi label classification task, the accuracy rate is actually the average accuracy rate of each sample, and its calculation formula is as follows:
$$
    Precision = \frac{1}{N}\sum_{i=1}^N\frac{|Y_i\cap Z_i|}{|Z_i|}
$$
\\
\textbf{Recall:} Recall rate refers to the ratio of correctly predicted labels to the total number of predicted labels, which is averaged across all instances. For each sample, the recall rate is to predict the proportion of the correct number of labels in the overall correct number of labels.
$$
    Recall = \frac{1}{N}\sum_{i=1}^N\frac{|Y_i\cap Z_i|}{|Y_i|}
$$
\\
\textbf{F1-score: }This evaluation index is actually the harmonic average of Precision and Recall, and its calculation formula is:
$$
  F1-score = \frac{1}{N}\sum_{i=1}^N\frac{2|Y_i\cap Z_i|}{|Y_i|+|Z_i|}=\frac{2\times Precision\times Recall}{Precision+Recall}  
$$
\subsection{example-based}
\textbf{Accuracy:}Accuracy refers to the accuracy of each instance, defined as the ratio of the predicted correct tags to the total number of tags for that instance (predicted and actual), which can measure how close the measures we have taken are to the true value. In the multi label classification problem, the accuracy rate actually calculates the average accuracy rate of all samples. For example, if the true label of a sample is $[0,1,1,0]$, and the prediction label is $[0,1,0,1]$, then the accuracy of the sample is $\frac{1}{1+1+1}=\frac{1}{3}$, so the global accuracy calculation formula is as follows:
$$
    Accuracy = \frac{1}{N}\sum_{i=1}^N\frac{|Y_i\cap Z_i|}{|Y_i\cup Z_i|}
$$
\\
\textbf{Hamming Loss: }This evaluation indicator is the most intuitive and easily understood one, which directly counts the number of incorrectly classified labels. Hamming Loss measures the proportion of incorrectly predicted tags in the total number of tags in all samples, and its specific calculation formula is as follows ($Y_j^{(i)}$represents the $j^{th}$ label of the $i^{th}$ sample):
$$
    Hamming\; Loss = \frac{1}{kN}\sum_{i=1}^N\sum_{j=0}^q I(Y_j^{(i)}\neq Z_j^{(i)} )
$$
\\
\textbf{Subset Accuracy:}
$$
    Subset \;Accuracy = \frac{1}{N}\sum_{i=1}^N I[Y_i=Z_i]
$$
\subsection{ranking-based}
\textbf{P@k} Full name is Precision@k. The calculation formula is similar to Precision:
$$
    P@k = \frac{1}{N}\sum_{i=1}^N \frac{|Y_i\cap Z_i|}{\min(k,|Y_i|)}
$$
\\
\textbf{AUC} The full name of AUC is Area under the curve. AUC is defined as the area enclosed by the coordinate axis under the ROC curve. It represents the ability of the model to correctly distinguish between positive and negative samples, with a value range of 0.5 to 1. The higher the AUC value, the better the performance of the model. Specifically, when AUC=0.5, the classification performance of the model is equivalent to random guessing; When AUC=1, the classification performance of the model is perfect. The calculation formula is:
    $$AUC = \frac{\sum_{i=1}^{M-1} (x_{i+1}-x_{i})\cdot(y_i+y_{i+1})}{2}$$

Where $M$ represents the total number of positive and negative samples, $x_i$represents FPR (false positive rate), $y_i$represents TPR (true case rate), which can be adjusted to obtain different FPRs and TPRs by adjusting the threshold value of the classification model. Each point on the ROC curve represents a different combination of FPR and TPR, and the AUC value is the size of the area under the ROC curve.\\
\\
\textbf{Mean Reciprocal Rank}  
Mean Reciprocal Rank (MRR) is an indicator used to measure the performance of ranking tasks (such as search, recommendation, etc.). It is the average reciprocal ranking of all queries. In information retrieval, MRR is often used to evaluate the quality of ranking results when a system answers a single query.

The MRR calculation method is to calculate the ranking of all documents returned by the system for each query. If a document matches the query, the reciprocal of its ranking is used as the score for the query, and the final score for all queries is averaged. The range of MRR values is 0 to 1. The higher the MRR value, the better the ranking of results the system can provide when answering queries.

The formula for MRR is as follows:

$$MRR = \frac{1}{|Q|}\sum_{i=1}^{|Q|}\frac{1}{rank_i}$$

Where $Q$ represents the query set, $|Q|$represents the total number of queries, and $rank_i$ indicates the ranking of the first correct document in the $i^{th}$ query. If the query does not have correct documents, then $rank_i$ Gets the total number of returned results from the query. The MRR value is the average of the reciprocal rankings of all queries.
