\section{Methodology}
\subsection{tree}
In this method, labels are organized into a tree structure, and predict the labels of each sample by recursively processing the relationship between child nodes and parent nodes. The tree models make the structure very clear, but most of them are early models. Thus the accuracy may not very high. \\
\\
\textbf{AttentionXML} 
 AttentionXML was published by Ronghui You in 2019\cite{2018arXiv181101727Y}, which used attention model and a PLT to improve performance.This model mainly wants to solve the defects of the BOW method and XML-CNN, such as the lack of flexibility and the capture of label information.The model does two things:(1)build a shallow and wide PLT.Each leaf on the PLT represents a label  and the PLT divides the label continuously recursively into two smaller clusters.Complete initialization by the sum of the bow features of the normalized text and then compress this tree.This model does not use deep label trees, because large trees or large clusters create problems that tail labels are not distinguishable, and there may be propagating and cumulative errors.(2) Build a five-layer attention model.The PLT at each level trains deep model to calculate context vectors for each label,in order to capture the most relevant labels.\\
 
This model has one problem, that is, the label awareness is not strong enough. The model does not consider the level correlation between labels, so may be it can be further modeled on the label information.\\
\\
\textbf{FastXML} FastXML\cite{10.1145/2623330.2623651} is also a model based tree. Its characteristic is fast. FastXML draws on multi-label random forest (MLRF) algorithm and sublinear ranking label (LPSR) partition algorithm, but it is faster than the most advanced MLRF and LPSR. The root node contains the whole label set. Whether the label should be assigned to the left child node or the right child node depends on a algorithm named node partitioning formulation. Like this, recurse until you reach the leaf node with only a few labels. The purpose of training is to fit a reasonable tree structure. When predicting, a data point can be passed along the tree until it reaches the leaf node. Then the classifier can be applied to these leaf nodes. This algorithm is local and cannot measure the whole situation. Optimizing global measurement may be expensive, because global measurement in the tree requires joint learning of all nodes.\\
\\
\textbf{HyperIM}  Hyperbolic representation capability\cite{DBLP:journals/corr/abs-1905-10802} is also a way to indicate the label hierachy. The primary reason is that the tree-like structure of hyperbolic space is well-suited for modeling symbolic data with hierarchical structures, which are often complex. Thus HyperIM is proposed which embeds both document words and labels jointly in the hyperbolic space to preserve their latent structures. HyperIM uses the text sequence of a document to measure the similarities between words and labels. To do this, it calculates the geodesic distance between words and labels that are jointly embedded in the Poincaré ball. After obtaining the word-label similarity scores, HyperIM aggregates them to generate label-aware document representations. These representations are then used to predict the probability of each label. The innovation of this method is that it can connect words in documents and labels, and use the most relevant part of documents to build the representations of the article, so as to improve the accuracy of classification. \\
\\
\textbf{Using CSSA with KDE} In this issue\cite{10.5555/3104482.3104485}, the authors creatively combined KDE with the CSSA algorithm.The problem was converted to a single-label calculation problem with kernel dependency estimation (KDE)and generalized CSSA is used to find the optimal subgraph in DAG to construct multiple labels with consistent hierarchical structure.KDE algorithm is to project labels into each dimension, learn them separately, get multiple predictions, and then map them into the original label space. This is the problem transformation method, which is more flexible than the adaptive method.The author improved the CSSA algorithm: (1) for the tree: the key is to merge the nodes. If the parent node is selected,then select it as well; if not, combine the parent node with the parent node.(2) For DAG: the CSSAG algorithm is proposed: when the label is inconsistent with the DAG, and the supernode is merged with the unassigned minimum parent node. That is, earlier supernodes are used first. However, for DAG of OR property, replication of subnodes is required before merging.
The advantage of this algorithm is that the labels can be fully learned using the whole dataset, and that the labels' insufficient learning will not happen.\\
\\\textbf{Hierarchically Regularized Deep Graph-CNN}  HR-DGCNN\cite{Peng2018LargeScaleHT} is a model proposed for discontinuous semantics and long-distance semantics. The model is mainly divided into three parts, input, convolution layer and output. The paper on the input layer has done a very interesting treatment. The input of the text data in the paper is to convert the text into a graph by using the co-occurrence of words. Then the nodes in the graph are sorted and divided into different sub-graphs (using the degree of nodes and the number of times words appear in the document to determine the sorting order) and the sub-graphs are regularized. Finally, word embedding is performed using the trained word2vec model. This results in a good word embedding input. The paper does not differ from the basic CNN model in the processing of the convolutional layer and the output layer, so we will not discuss it. But in the fully connected layer, the paper has done recursive regularization in the fully connected layer of the last layer. The so-called recursive regularization makes labels that are close in the hierarchy have similar parameters. At the same time, the paper also mentioned a method of recursive hierarchical segmentation, mainly to solve the large-scale hierarchical multi-label classification problem.\\ 
\\
\textbf{Bonsai Algorithm} Bonsai\cite{2019arXiv190408249K} is an algorithm proposed to solve extreme multi-label classification problems. The main innovation of this method is to build a shallow tree structure. For data input, the paper summarizes the previous three methods of generating label representations, namely input space representation, output space representation, and input-output space joint representation. After the tag representation is generated, the K-Means clustering algorithm is used to partition the resulting tag representation, and a shallow tree structure of related tags is generated through this clustering method. The paper divides all the labels into K sets through the clustering method, then establishes the root node, and links the root node with the separated classes (at this time, the K clusters will become the child nodes of the root node). Then perform the same clustering algorithm for each child node, and then continue to build new nodes until the nodes of the subtree are greater than the specified maximum depth value or the number of associated labels in the subtree is not greater than K. In the paper, in order to avoid propagation errors in deep tree cascades, a larger K value (k greater than or equal to 100) is set. In the prediction stage of the model, the paper adopts the one-vs-rest method for prediction. By using the shallow tree structure obtained earlier, set a linear classifier for each node. For nodes that are not leaf nodes, according to the predicted output of each non-leaf node in the current layer, judge whether to traverse the child nodes of these nodes according to the output results (to avoid unnecessary operations). For leaf nodes, only the node itself needs to be classified.\\
\\
\textbf{summary} For the tree-based methods, their advantages are that they can use the hierarchical structure between labels more accurately and the prediction results of the model are easy to explain. The disadvantage is that there is a high requirement for tag level knowledge that needs to be acquired in advance and accurate hierarchy information is required. Besides, because it uses a relatively simple tree structure model, it is difficult to deal with the complex dependency relationship between labels, which needs to be represented by a more powerful model.

\subsection{hierarchical step by step prediction}
hierarchical step by step prediction means labels at each level are predicted independently, and the prediction of each label depends on the results of the label predictions of the previous level.\\
\\
\textbf{Capsule network}  Capsule network \cite{aly-etal-2019-hierarchical} has a good performance in the field of natural language processing, which can mainly solve the problem of information loss during pooling. Capsule network can capture the categories’ underlying structures by associating each label with one capsule. An output of a capsule is a vector, not a single scalar value used in traditional neural network, so that it can encode more information of each category. The working principle of the capsule network is that the first layers prepossess the input data. After the data prepossessing, the results will be sent to the primary capsule layer, and a vector with potential information will be output. For each classification capsule, it weights the output of all primary capsules and predict the label probability by using squashing function. The connection between layers is based on routing algorithm, which can determine the contribution of each primary capsule’s output to a classification capsule, reflecting the hierarchy of labels. The benefit of using capsule network is that articles can be encodes with more messages, but not all of this information is useful. Valid information is always hard to get. Besides, due to the use of routing algorithm, there is also a problem of slow calculation. \\
\\
\textbf{Hierarchical Transfer Learning}  HTrans\cite{banerjee-etal-2019-hierarchical}, a Hierarchical Transfer Learning approach, is proposed, which uses a Gated Recurrent Unit (GRU)-based deep learning architecture coupled with attention. Attention is applied on top of the GRU. This model combines a max-pooled and mean-pooled representation of all the GRU hidden along with attention to feed into output layer. Binary classifiers are used for prediction, which provide more flexibility than training a single multi-label classifier, and can solve the problem of imbalance distribution of label data. In HTrans, binary classifiers at lower levels in the hierarchy are initialized using parameters of the parent classifier and fine-tuned on the child category classification task. Learning rate plays a more important role in this scene, so how to find a smart way to adjusting hyper-parameter need further try and research.\\
\\
\textbf{Hierarchical Fine-Tuning} Hierarchical Fine-Tuning(HFT)\cite{shimura-etal-2018-hft} was proposed in a paper published by Kazuya Shimura in 2018. HFT is mainly to solve the poor low-level label classification results caused by the non-hierarchical planar model when performing hierarchical classification. Therefore, this paper uses CNN as the basic model (a typical non-hierarchical planar model). In the process of training the model, the parameters of the word embedding layer and the convolutional layer are mainly transferred from top to bottom, that is, the parameters of the upper layer structure are used to learn the labels of the lower layer. At the same time, in order to avoid adverse effects caused by excessive correlation gaps between distant levels, parameter fine-tuning is only performed between adjacent levels.\\
\\
\textbf{Loosely Coupled Graph Convolutional Network} Loosely coupled graph convolutional neural network (LCGCN)\cite{xu-etal-2021-hierarchical} is a GCN model, mainly to solve the over-smoothing problem caused by too many nodes or edges of the GCN graph model. LCGCN mainly consists of two parts of the graph model, one part is the core graph, which is used to extract word embeddings and category embeddings; the other part is the document-word graph, which is mainly used to capture the relationship between documents and words. At the same time, in order to make full use of the correlation between categories, this paper also proposes a new method to capture the relevant information between different levels and different categories from horizontal correlation and vertical correlation respectively. For horizontal correlation, the article defines a correlation matrix, which can be obtained through the softmax function of the singular matrix embedded in the h-th layer category obtained from the core map, and then coupled with the label to obtain the $h$ layer Relationships between horizontal categories. The paper's vertical processing is very ingenious, fully considering the information structure of the upper layer and cleverly integrating the relationship between categories into the document representation. This method mainly uses the category embedding of the current layer obtained in the core graph, and dot-multiplies it with the prediction result of the current layer, and we can obtain a new vector representation of the category. Finally, the obtained new category representation is coupled with the current document embedding to obtain a new document representation, which will be used as the document input for the next layer. The results obtained by the above two correlation methods are applied in the final loss function for gradient optimization and parameter update.\\
\\
\textbf{Coherent Hierarchical Multi-Label Classification Networks} C-HMCNN\cite{2020arXiv201010151G} model was raised by Eleonora Giunchiglia,which used networks to generates predictions consistent with the hierarchical constraints. Many neural network models produce incoherent predictions and require a  additional post-processing step to be coherent with hierarchical constraints,while C-HMCNN can achieve coherent output without additional steps.C-HMCNN uses the loss function to make the hierarchical structure of the lower layer to predict the upper layer.This model has two parts:(1)There are two bottom modules of outputs that ensure  no hierarchical conflicts occur.(2) A maximum constraint module (MCM), which has a layer of input and implements hierarchical restrictions. In MCM, the maximum constraint loss function is trained so that hierarchical constraints can be exploited.The author used 20 datasets to prove this model can improve the performance.\\
\\
\textbf{LA-HCN} LA-HCN\cite{2020arXiv200910938Z}, whose full name is Label-based Attention for Hierarchical Mutlti-label Text Classification Neural Network, also extracts important information from the text hierarchically for labels from different hierarchical levels to. The label-based attention is used to bridge the labels and document content. Unlike the features extracted from typical attention may be diluted, it can extract meaningful information corresponding to different labels to learn learning disjoint features for each hierarchical level. Level information is transferred between layers. Besides, both local and global classifiers are applied to this model to get the final result, which can prevent the fact that with only local classifiers,the wrong classification of a local classifier will spread to the next level, thus affecting the final global classification. 
\\
\\
\textbf{HMCN-(F/R)} The HMCN-(F/R)\cite{wehrmann2018hierarchical} model is a combination of the HMCN-F and HMCN-R models. The HMCN-F model is a new model constructed to coordinate local information and global information. There are mainly two ways of information transmission in the model. The first is the main flow, where information passes from the input layer through the fully connected layer to the global output layer. The output of each layer is multiplied by the output of the previous layer and the input data of the model, after a linear change, and finally through a nonlinear activation function to obtain the output of the corresponding level. The second information transmission of the HMCN-F model is the local flow, which uses the output of the main information flow to perform two consecutive transformations to obtain the local information corresponding to the hierarchical structure. Finally, the outputs obtained from the two information streams are integrated to obtain the final prediction result of the HMCN-F model. The point of HMCN-F is to make full use of local information and global information. But as the label hierarchy increases, the HMCN-F model parameters also increase. Therefore, when the hierarchy is deep, it will lead to a sharp increase in the number of model parameters. To solve this problem, HMCN-R is proposed. HMCN-R still retains the idea of extracting local information from the main information flow, but HMCN-R refers to the structure of LSTM in structure, which will greatly reduce the consumption of parameters, thus fundamentally solving the problem of HMCN-R. F-parameters increase dramatically. HMCN-(F/R) output includes hierarchical output and global output, and the corresponding loss function is also composed of local loss and global loss. Since the consistency of hierarchical classification cannot be guaranteed by minimizing local loss and global loss, Therefore, the article proposes a hierarchical penalty loss to reduce this risk, expressed as $\mathcal{L}_{H_i} = \lambda \mathop{max}\{0,Y_{in}-Y_{ip}\}^2 $ where $Y_{in}$ and $Y_{ip}$ represent the predicted scores of child nodes and parent nodes .So the final loss function can be expressed as 
$$
\mathop{min}\limits_{W}\{\mathcal{L}_L + \mathcal{L}_G + \mathcal{L}_H\} 
$$\\
\\
\textbf{Summary} The popularity of this kind of model can be observed from the abundance of pertinent documents, as it is a highly structured and high-performing approach. The advantages of this kind of model include its ability to capitalize on label correlations for enhanced model accuracy, as well as its capacity for parameter sharing across multiple tasks to optimize computing resources. However, this kind of model also presents certain disadvantages, such as its challenging training process and complex structure.
\subsection{label plus text as input, simultaneous prediction}
The hierarchical structure of the label is entered into the model structure as known information, so the hierarchical structure information of the label can be used in the prediction of the label. The modeling of label hierarchy mostly uses graph model.\\
\\
\textbf{Metadata and Label Heterogeneous Graphs}  Many methods simply use metadata semantic information or established parent-son label structure, ignoring the heterogeneous graph structure. Heterogeneous graphs\cite{ye-etal-2021-beyond} are used by  Chenchen Ye to model labels' hierarchy and their statistical dependencies. Besides,the author also consider the metadata in the documents,which still be modeled by heterogeneous graph. This paper presents two heterogeneous maps: metadata heterogeneous map and label heterogeneous map and through these two heterogeneous graphs, the topological relation of the metadata and the hierarchy and dependence of the labels can be obtained.The author built a three-layer model to analyze, with multiple transformer layer, and used MHA and FFN methods to pass the parameters layer by layer.And use the heterogeneous map conversion (HGT) to analyze the above two heterogeneous maps.HGT contains Heterogeneous
Mutual Attention(ATT), , Heterogeneous Message Passing (MSG)and Target-Specific Aggregation(AGG) to obtain the vector of metadata and labels.Then by activate the function and the linear transformation layer, get the document text vector, and finally output the result through the prediction function.\\
\\
\textbf{Hierarchy-Aware Global Model}  In this paper\cite{zhou-etal-2020-hierarchy}, the hierarchy is represented as a directed graph, which is read into the encoder,modeling labels' dependencies with structural encoders.The authors propose the HiAGM global aware model, containing the label structure encoder, the text encoder.There are two submodels:(1) HiAGM-LA: The label  features can be fused with each other, and the text and label are independently expressed and predicted by attention mechanism.(2) HiAGM-TP: Text features can be directly input into the structural encoder to participate in the feature calculation.The article also proposes two types of structure encoder for labels:(1) Bidirectional Tree-LSTM: The LSTM structure is used to encode the tree  data structure, and the two-way tree corresponds to the two directions between the parent nodes and children nodes.(2): Hierarchy-GCN: GCN calculation the adjacent nodes of one node. This model better captures the hierarchical information and more fully utilizes the interaction of text features and label features.\\
The main problems in this paper are the possible lack of a loss function to calculate the differences between the children nodes, and the lack of some commonly used baselines for the model performance comparison.\\
\\
\textbf{Summary} This kind of model inputs the hierarchical information of labels into the network as known knowledge, and some documents also use the meta data of documents as input. The advantages of this method is that it can better handle the complex dependency between tags. Besides, the model design structure is not complex, which only needs to model and represent the hierarchical structure. However, the classifier cannot make more targeted use of the hierarchical structure between labels. What's more, the prediction results of the model are difficult to explain.
\subsection{ensemble (parallel)}

Ensemble is also a great idea. Labels at each level are predicted separately. This is done in parallel. After output, predicted labels interact with each other with hierarchical structure information to get final results, like voting in prediction time. However, this method is mostly used in the industrial field and is suitable for large engineering projects, so the number of models is not large.\\
\subsubsection{one-vs-one} One-vs-one refers to having a classifier between each two categories. This looks great, but as the number of levels of labels increases, the number of classifiers increases dramatically. When there are n categories here, then the number of the classifiers will grow to $C_{n}^{2}$. Thus this method is uncommon.
\subsubsection{one-vs-all} One-vs-all means every label has a special classifier. For each classifier, each classifier treats its corresponding category as a positive example and treats the remaining categories as negative examples. In this way, the number of classifiers is greatly reduced. The paper\cite{10.1007/978-3-031-17189-5_14} uses this method. The Author proposed two modules: a shared encoding module and a task-specific module. First, the language model SciBERT was pre-trained. The vector obtained by encoding the text feed into the task-specific module, where the sub-task of classifying each tag is performed. And then a shared TextCNN layer was introduced to learn the dependency information between labels at each level. Finally, the hierarchical feature information is fused by linear transformation and the prediction result of each level is getted.

