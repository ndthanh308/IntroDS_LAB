\section{Challenges}
Even though many methods have been proposed, there are still many challenges for hierarchical multi-label text classification, including but not limited to the label sparsity and imbalance, low resources labeling data, low accuracy in deep levels of labels, and the extreme multi-label problem. 

\textbf{Label sparsity and imbalance} Label sparsity and imbalance refers to the long-tail distribution of the labels, by which only a small set of labels have many training examples and a large number of the labels are low frequent ones. The model can easily over-fit the high-frequency labels and under-fit the low frequency ones. Often times, when a label appears only a few times, the classifier may ignore it and the general performance is not affected. The model will be more reliable to the data sets with relatively balanced distribution of labels. Some models try to alleviate the imbalance of labels at this level by utilizing predicted labels from the previous level or leveraging global information, such as HMCN-F \cite{inproceedings}, HTF-CNN \cite{shimura-etal-2018-hft}. However, due to the use of a large number of parameters and the lack of overall information, these methods are prone to exposing bias issues. HiAGM \cite{zhou-etal-2020-hierarchy}, improved with a bidirectional computing structure encoder, enhances its ability to handle sparse data. HiLAP \cite{Mao_2019} adopts reinforcement learning to to learn a Label Assignment Policy. ARS2 \cite{song-etal-2022-adaptive} is a framework which proposed two adaptive sampling strategies to address data imbalance issues. Meta-LMTC \cite{wang-etal-2021-meta-lmtc} is used to solve scenes with few and zero shots. Using a meta-learning approach with fine-tuning of parameters to quickly make labels with fewer instances adaptable to the task. And the balancing loss functions for multi-label text classification \cite{huang-etal-2021-balancing} is also introduced to deal with long-tailed distributions.
%\textbf{Weak correlation among the labels}Label weak correlation means the weak correlation between labels at different levels or within the same level, that is, the dependency between labels is not obvious or difficult to capture. This weak correlation is a hidden information but important for the classification accuracy.

\textbf{Low resource labeled data} The lack of labeled data another challenge. Zero-shot learning is a common setting, which refers that some labels do not have corresponding training data, only descriptions of the labels.  A normal method is to convert it into a nearest neighbor search problem. Currently, one model \cite{chalkidis-etal-2020-empirical} based on LWAN by using the label hierarchy is aimed to improve zero-shot learning. An end-to-end structural contrasting representation learning approach \cite{zhang-etal-2022-structural-contrastive} is proposed by raising a novel randomized text segmentation method. However, the enormous label space and complex association between labels and text lead to extensive calculations and low accuracy. 

\textbf{Lower accuracy in deeper levels} In most hierarchical multi-label classification tasks, deeper level labels involve more specific concepts or categories, and these categories may appear less frequently in the data set, which makes it difficult for the model to extract deep features. Besides, the incorrect prediction at one level may affect the results of its child labels. The error prediction can be propagated to the deeper levels, which causes low accuracy in deep level label prediction. 
%\subsection{Computational performance}
%Because a large number of complex network models are used, such as convolution neural network, recurrent neural network, graph structure and tree structure, there are a lot of parameters to learn, which causes much time and resources on training.Thus a more important thing is to simplify model. 

\textbf{Extreme multiple label problem}
The extreme multi-label text classification is the problem of labeling an instance with a small subset of relevant labels chosen from an extremely large pool of possible labels. Due to the lack of enough training data, clustering and ranking algorithms are often leveraged before the classification step, such as Divide and Conquer \cite{barros-etal-2022-divide} and Seq2Set \cite{cao-zhang-2022-otseq2set}.  

%\textcolor{blue}{A recent method named Divide and Conquer\cite{barros-etal-2022-divide} using Matcher module to allocate the probability of documents belonging to each cluster which created by using hierarchical information. Then the Ranker module assign labels only using the documents in the cluster. }\textcolor{blue}{Seq2Set\cite{cao-zhang-2022-otseq2set} uses the bipartite matching and the optimal transport distance ,through an attention mechanism to generate label sets autoregressively, solving the problem that fully connected layers can not generate variable-length label sets.}