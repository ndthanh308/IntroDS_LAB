% 所有段落都将取消首行缩进
\setlength{\parindent}{0pt} %
% 设置每个段落之间的间距
\setlength{\parskip}{1em}

\section{Evaluation Metrics for Hierarchical Multi-label Classification.}
For a hierarchical multi label classification task, three types of metrics are often used: \textbf{label-based}, \textbf{example-based} and \textbf{ranking based}. Precision, recall, and F1-score are often used as label-based evaluation metrics. Example-based evaluation metrics includes \textit{accuracy} , \textit{hamming loss} , \textit{subset accuracy} and \textit{information contrast model} \cite{amigo-delgado-2022-evaluating}. Ranking-based evaluation metrics includes \textit{Precision@k} and \textit{Mean Reciprocal Rank}. \textit{Precision@k} and \textit{mean reciprocal rank} are two widely used evaluation metrics for assessing the performance of machine learning classifiers and ranking tasks, respectively. 
%we assume that there is a data set of $D$ which consists of $n$ samples $(x_i, Y_i)$, $1\leq i\leq N$. $c$ is a multi label classifier, and $Z_i = c(x_i)$ is the prediction of the corresponding $x_i$.
%\textcolor{blue}
%{
%the evaluation metrics are gradually transitioning from flat metrics to hierarchical evaluation metrics. In the future, hierarchical evaluation metrics will be more deeply combined with common flat metrics.
%}
%\textbf{precision} denotes the proportion of accurately predicted labels to the overall count of true labels, computed on a per-instance basis and then averaged across all instances. In contrast, \textbf{recall} is the ratio of correctly predicted labels to the total count of predicted labels, also computed on a per-instance basis and then averaged across all instances. Finally, \textbf{F1-score} is an evaluation metric that represents the harmonic mean of precision and recall, providing a comprehensive measure of model performance.
% 原来的介绍
% \textbf{Precision} Precision is the ratio of the predicted correct labels to the total number of actual labels, averaged across all instances.

%The average of all instances, known as the accuracy rate. In a multi label classification task, the accuracy rate is actually the average accuracy rate of each sample, and its calculation formula is as follows:
% 原来的recall 和 F1-Score
% \textbf{Recall} Recall rate refers to the ratio of correctly predicted labels to the total number of predicted labels, which is averaged across all instances. 
% \textbf{F1-score} This evaluation index is actually the harmonic average of Precision and Recall.
% and its calculation formula is:
 %Specifically, \textbf{accuracy} measures the precision of each instance, determined by the ratio of accurately predicted labels to the total number of labels for that instance, reflecting the degree of alignment between the taken measures and the actual value. Meanwhile, \textbf{hamming loss} gauges the proportion of mispredicted labels among all labels in the entire data set. \textbf{Subset accuracy}, on the other hand, determines the correctness of a sample based on whether its predicted label set matches the actual label set, and is calculated as the proportion of all correctly predicted samples to the total sample size. The more recent \textbf{information contrast model}\cite{amigo-delgado-2022-evaluating} is based on similarity axioms, comparing the specificity of category sets to determine the similarity of the labels.
%In the multi label classification problem, the accuracy rate actually calculates the average accuracy rate of all samples. For example, if the true label of a sample is $[0,1,1,0]$, and the prediction label is $[0,1,0,1]$, then the accuracy of the sample is $\frac{1}{1+1+1}=\frac{1}{3}$, so the global accuracy calculation formula is as follows:
%原先的
%  \textbf{Accuracy} Accuracy refers to the accuracy of each instance, defined as the ratio of the predicted correct labels to the total number of labels for that instance (predicted and actual), which can measure how close the measures we have taken are to the true value. 

% \textbf{Hamming Loss} Hamming Loss measures the proportion of incorrectly predicted labels in the total number of labels in all samples.
% %  and its specific calculation formula is as follows ($Y_j^{(i)}$represents the $j^{th}$ label of the $i^{th}$ sample):

% \textbf{Subset Accuracy} For each sample, if this set of labels is completely consistent with the actual labels, then the sample is considered to have predicted correctly. Subset Accuracy is the proportion of all correctly predicted samples to the total sample size, and is a relatively simple evaluation metric.

% \textcolor{blue}{newly-add}

% \textbf{Information Contrast Model
% \cite{amigo-delgado-2022-evaluating}} This method is based on similarity axioms, comparing the specificity of category sets to determine the similarity of them.IC(A) represents the information of the feature set A.
%\textbf{Precision@k} is a commonly employed metric for evaluating the efficacy of machine learning classifiers. It gauges the ratio of accurate positive predictions among the top k anticipated instances, where k is a user-defined parameter. The Precision@k value falls between 0 and 1, with the value 1 signifying that all positive predictions among the top k predictions are correct.In contrast, \textbf{mean reciprocal rank} is is the average reciprocal ranking of all queries. In information retrieval, MRR is often used to evaluate the quality of ranking results when a system answers a single query.



%\textbf{AUC} The full name of AUC is Area under the curve. AUC is defined as the area enclosed by the coordinate axis under the ROC curve. It represents the ability of the model to correctly distinguish between positive and negative samples, with a value range of 0.5 to 1. The higher the AUC value, the better the performance of the model. Specifically, when AUC=0.5, the classification performance of the model is equivalent to random guessing; When AUC=1, the classification performance of the model is perfect. The calculation formula is:
%    $$AUC = \frac{\sum_{i=1}^{M-1} (x_{i+1}-x_{i})\cdot(y_i+y_{i+1})}{2}$$
%Where $M$ represents the total number of positive and negative samples, $x_i$represents FPR (false positive rate), $y_i$represents TPR (true case rate), which can be adjusted to obtain different FPRs and TPRs by adjusting the threshold value of the classification model. Each point on the ROC curve represents a different combination of FPR and TPR, and the AUC value is the size of the area under the ROC curve.\\
% \textbf{P@k}  is a commonly employed metric for evaluating the efficacy of machine learning classifiers. It gauges the ratio of accurate positive predictions among the top k anticipated instances, where k is a user-defined parameter. The Precision@k value falls between 0 and 1, with the value 1 signifying that all positive predictions among the top k predictions are correct. 

% \textbf{Mean Reciprocal Rank}  
% Mean Reciprocal Rank (MRR) is an indicator used to measure the performance of ranking tasks (such as search, recommendation, etc.). It is the average reciprocal ranking of all queries. In information retrieval, MRR is often used to evaluate the quality of ranking results when a system answers a single query.


%The MRR calculation method is to calculate the ranking of all documents returned by the system for each query. If a document matches the query, the reciprocal of its ranking is used as the score for the query, and the final score for all queries is averaged. The range of MRR values is 0 to 1. The higher the MRR value, the better the ranking of results the system can provide when answering queries.
% The formula for MRR is as follows:

