\section{Machine Learning Strategies}
% Who write this seciton, very good. 
Many machine learning strategies were adopted for hierarchical multi-label classification. These learning strategies usually consider multiple aspects, such as data, label and training. We list the following machine learning strategies. 
\paragraph{\textbf{Self-training}} Self training is a semi supervised learning method in which a small amount of labeled data is used for initial training, and then prediction results are used to generate more training data. In hierarchical multi label classification tasks, self training can generate more training data by using the prediction results of the current model, and then use these data for the next round of training.
%\subsection{Semi-supervised} Semi-supervised learning is a method of using unmarked data to enhance supervised learning. In hierarchical multi label classification tasks, semi supervised learning can use unlabeled data to learn hierarchical relationships between labels, and then use these relationships in supervised learning.\\

\paragraph{\textbf{Weakly-supervised learning}} In reality, a real situation is that there is no big amount of human labeled documents to train classifiers, so approaches based on only class surface names as supervision signals are explored. An example is TaxoClass \cite{shen-etal-2021-taxoclass}.  First, a pre-trained textual entailment model is used to  calculate the document-class similarity. After that, the calculated similarity is used to determine the core class of the article. Then training data extracted from document core classes is used to train a text classifier. The classifier includes a document encoder based on pre-trained BERT, a class encoder capturing class taxonomy structure, and a text matching network computing the probability of a document being tagged with each class. The class Encoder is a graph neural network to capture the hierarchical information of label. This method can train a large number of unlabeled documents, but because of the unlabeled data, the determination of the core class of the document may not be so accurate, which may affect the final accuracy. ASR2 \cite{song-etal-2022-adaptive} is proposed as a general, model-agnostic weakly supervised leading framework, which is dedicated to alleviate the data imbalance issue. It calculates a probabilistic margin score based on the output of the current model to measure and rank the cleanliness of each data point. Then, the ranked data is sampled based on both class-wise and rule-aware ranking. Another example \cite{Meng_2018} is the use of constructed pseudo-documents to address label sparsity by improving the dataset in three different ways. Assuming that words and documents are located in the same semantic space, a spherical distribution is established to sample keywords that generate pseudo-documents, thereby improving the generalization of seed information.

\paragraph{\textbf{Data augmentation}}
Data augmentation is a commonly used training strategy that can increase the diversity of training data and help models better generalize to new data. In hierarchical multi label classification tasks, data augmentation can be used to generate more high-quality training data, and can help models learn hierarchical relationships between labels by generating data related to hierarchical relationships. Data enhancement can be implemented in the following ways: Synonym Replacement, Random Insertion, Random Deletion, Random Swap, and Sentence Rearrangement. The above methods can be randomly applied to training data with a certain probability, thereby expanding the data set and increasing the generalization ability and robustness of the model. At the same time, it is also important to note that excessive data enhancement may lead to over fitting of the model, so it is necessary to make choices based on specific tasks and data characteristics.

\paragraph{\textbf{Label enhancement}}
Label enhancement is a method of enhancing original label information through the use of external resources (such as knowledge maps, dictionaries, corpora, etc.). In hierarchical multi label classification tasks, label enhancement can help models more accurately understand and depict the relationship and semantic information between different labels, thereby improving the classification accuracy and generalization ability of the model. However, it should be noted that excessive label enhancement may introduce noise and incorrect labels, thereby reducing the performance and generalization ability of the model. Therefore, this method needs to be used appropriately. MHG \cite{ye-etal-2021-beyond}
modeles metadata and its topological relationships to construct a metadata heterogeneous graph,to capture label dependencies. HVHMC \cite{xu-etal-2021-hierarchical} integrates hierarchical information through graph convolutional network learning as a way to capture the horizontal and vertical dependencies between labels.
\paragraph{\textbf{Post-processing}}
In hierarchical multi-label text classification, post processing refers to a strategy of adjusting the relationship between labels when the model has already predicted multiple labels. It is usually used to solve situations where there is inconsistency or conflict between the labels predicted by the model. Common post processing strategies include label filtering (remove predicted untrustworthy or irrelevant labels by setting thresholds or other rules), label combination (combine the predicted labels into higher-level labels to reflect their relationship), label conflict (post-processing predicted labels to resolve conflicts or inconsistencies between them). The post-processing steps may vary according to the hierarchy of the label scheme. 