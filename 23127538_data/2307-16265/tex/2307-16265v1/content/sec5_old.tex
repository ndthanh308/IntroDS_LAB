\section{Learning Strategies}
\subsection{self-training} Self training is a semi supervised learning method in which a small amount of labeled data is used for initial training, and then prediction results are used to generate more training data. In hierarchical multi label classification tasks, self training can generate more training data by using the prediction results of the current model, and then use these data for the next round of training.
\subsection{semi-supervised} Semi-supervised learning is a method of using unmarked data to enhance supervised learning. In hierarchical multi label classification tasks, semi supervised learning can use unlabeled data to learn hierarchical relationships between labels, and then use these relationships in supervised learning.\\
\\
\textbf{weakly-supervised hierarchical multi-label text classification} In reality, a real situation is that there is no large number of human tagged documents to train classifiers, so approaches based on only class surface names as supervision signals are explored. Thus TaxoClass\cite{shen-etal-2021-taxoclass} is proposed. First, a pre-trained textual entailment model is used to  calculate the document-class similarity. After that, the calculated similarity is used to determine the core class of the article. Then training data extracted from document core classes is used to train a text classifier. The classifier includes a document encoder based on pre-trained BERT, a class encoder capturing class taxonomy structure, and a text matching network computing the probability of a document being tagged with each class. The class Encoder is a graph neural network to capture the hierarchical information of label. This method can train a large number of unlabeled documents, but because of the unlabeled data, the determination of the core class of the document may not be so accurate, which may affect the final accuracy. \\
\\
\subsection{data augmentation}
Data augmentation is a commonly used training strategy that can increase the diversity of training data and help models better generalize to new data. In hierarchical multi label classification tasks, data augmentation can be used to generate more training data, and can help models learn hierarchical relationships between labels by generating data related to hierarchical relationships.Data enhancement can be implemented in the following ways: Synonym Replacement, Random Insertion, Random Deletion, Random Swap, and Sentence Rearrangement. The above methods can be randomly applied to training data with a certain probability, thereby expanding the dataset and increasing the generalization ability and robustness of the model. At the same time, it is also important to note that excessive data enhancement may lead to over fitting of the model, so it is necessary to make choices based on specific tasks and data characteristics.

\subsection{label enhancement}
Label enhancement is a method of enhancing original label information through the use of external resources (such as knowledge maps, dictionaries, corpora, etc.). In hierarchical multi label classification tasks, label enhancement can help models more accurately understand and depict the relationship and semantic information between different labels, thereby improving the classification accuracy and generalization ability of the model.However, it should be noted that excessive label enhancement may introduce noise and incorrect labels, thereby reducing the performance and generalization ability of the model. Therefore, this method needs to be used appropriately. 
 

\subsection{post-processing}