\section{Main Approaches for hierarchical multi-label classification}
In this section, we consider four main approaches for hierarchical multi-label text classification: tree-based approach, embedding based approach, graph based approach and the ensemble approach.        
\subsection{Tree-based approach} This method is based on probabilistic label tree, which was originally developed for extreme multi-label classification, where a probabilistic label tree (PLT) is used to partition labels, where each leaf in PLT corresponds to an original label and each internal node corresponds to a pseudo-label (meta-label). Then by maximizing a lower bound approximation of the log likelihood, each linear binary classifier for a tree node can be trained independently with only a small number of relevant samples. Parabel \cite{prabhu2018parabel} is a traditional label tree-based method using bag-of-words (BOW) features. FASTTEXT \cite{joulin2016bag} and EXTREMETEXT\cite{wydmuch2018no} extended Parabel by using dense features.  AttentionXML \cite{you2019attentionxml}, which used attention model and a PLT to further improve the classification performance. FastXML \cite{prabhu2014fastxml} draws on thee multi-label random forest (MLRF) algorithm and sublinear ranking label (LPSR) partition algorithm. Bonsai \cite{2019arXiv190408249K} proposed to build a shallow and diverse label tree structure, in which all the labels are segmented into K sets through the clustering, but it needs high space complexity because of using linear classifiers.
HPT \cite{wang-etal-2022-hpt} make efforts to transform hierarchy text classfication(HTC) into a hierarchy-aware multi-label mask language model(MLM) problem that focuses on bridging two gaps between HTC and MLM.
%In order to make good use of label hierarchy information, HPT not only constructed a template based on the depth of label hierarchy as input, but also utilized a K-layer Graph Attention Network(GAT) model to adopt relationships between labels.}

For the tree-based methods, their advantages are that they can use the hierarchical structure between labels more accurately and the prediction results of the model are easy to explain. The disadvantage is that there is a high requirement for label level knowledge that needs to be acquired in advance and accurate hierarchy information is required. Besides, because it uses a relatively simple tree structure model, it is difficult to deal with the complex dependency relationship between labels, which needs to be represented by a more powerful model.
\subsection{Embedding based approach}
Embedding based methods arise with deep learning models, most of these methods, use an encoder to represent the input text and learn a mapping function for input text and the labels. HyperIM \cite{DBLP:journals/corr/abs-1905-10802} embeds both document words and labels jointly in the hyperbolic space to preserve their latent structures. The innovation of this method is that it can connect words in documents and labels, and use the most relevant part of documents to build the representations of the article, so as to improve the accuracy of classification. HTrans \cite{banerjee-etal-2019-hierarchical}, a hierarchical transfer learning approach uses a Gated Recurrent Unit based  architecture coupled with attention mechanism. In Hierarchical Fine-Tuning (HFT) \cite{shimura-etal-2018-hft} the parameters of the word embedding layer and the convolution layer are mainly transferred from top to bottom, that is, the parameters of the upper layer structure are used to learn the labels of the lower layer. At the same time, in order to avoid adverse effects caused by excessive correlation gaps between distant levels, parameter fine-tuning is only performed between adjacent levels. C-HMCNN \cite{2020arXiv201010151G} uses a constraint loss function to make the hierarchical structure of the lower layer to predict the upper layer. LA-HCN \cite{2020arXiv200910938Z} uses the label-based attention to bridge the labels and document content. Unlike the features extracted from typical attention may be diluted, it can extract meaningful information corresponding to different labels to learn learning disjoint features for each hierarchy. The HMCN \cite{wehrmann2018hierarchical} simultaneously optimizes local and global loss functions for discovering local hierarchical class-relationships and global information from the entire class hierarchy while penalizing hierarchical violations.

The embedding based approach often needs a large amount of training examples, it relies on the attention mechanism and regularization from the label structure. A weakness is that there is no explainability for wrongly predicted labels.  
\subsection{Graph based approach}
Graph based approaches regard the whole document and hierarchical labels as a single graph, where the node type may include token, label, sentence, topic or even some meta information. A graph neural network is then applied on the graph and the hierarchical multi-label classification task can be converted into a node classification task. For example, heterogeneous graphs are used in \cite{ye-etal-2021-beyond}  to model the labels' hierarchy, their statistical dependencies and meta data.  Loosely coupled graph convolutional neural network (LCGCN) \cite{xu-etal-2021-hierarchical} mainly solves the over-smoothing problem caused by too many nodes or edges of the GCN graph model. There are two parts in it, one is the core graph, which is used to extract word embeddings and label embeddings, the other one is the document-word graph, which is mainly used to capture the relationship between documents and words. In another hierarchy-aware global model \cite{zhou-etal-2020-hierarchy}, the label hierarchy is represented as a directed graph with two sub models: HiAGM-LA (the label  features can be fused with each other, and the text and label are independently expressed and predicted by attention mechanism) and HiAGM-TP (text features can be directly input into the structural encoder to participate in the feature calculation). This model captures the hierarchical information and fully utilizes the interaction of text features and label features.  

The graph based approach inputs the hierarchical information of labels into the network as known knowledge, and some documents also use the meta data of documents as input. The advantage of this method is that it can better handle the complex dependency between tags. Besides, the model design structure is not complex, which only needs to model and represent the hierarchical structure. However, the classifier cannot make more targeted use of the hierarchical structure between labels. Similar with the embedding approach, the prediction results of the model are difficult to explain.
\subsection{Ensemble approach}
Ensemble is widely used in statistical machine learning. In terms of hierarchical multi-label classification, several models can be trained for each of the levels of the labels, which can be conducted in parallel. In the prediction time, a voting mechanism can be conducted, followed by further cross checking among the predicted labels. However, this method is mostly used in the industrial field and is suitable for large engineering projects.

Among different ensemble approaches, one-vs-one \cite{daengduang2017applying} and one-vs-all \cite{10.1007/978-3-031-17189-5_14} are the two typical ways. One-vs-one refers to learning a classifier between each two categories. It works well when the total amount of labels is small, but as the number of levels of labels increases, the number of classifiers also increases dramatically. When there are n categories here, then the number of the classifiers will grow to $C_{n}^{2}$. In contrast, one-vs-all means that a single classifier is trained for each label, and the classifier treats its corresponding category as positive examples and treats the remaining categories as negative examples. In the shared task 5 track 1 of NLPCC2022 multi-label classification for scientific literature \cite{liu2022overview}, the task requires participants to build multi-label text classification models for scientific abstracts from the chemistry domain. Among all the submitted system, the best system \cite{wang2022bit} was obtained by an ensemble model with more than five different deep learning models. Another boosting ensemble method is derived from Devil's advocate \cite{jo-etal-2021-devils-advocate}. This method requires at least three models to be integrated, and a new loss which uses the idea of generated confrontation to make the target model finally converge is designed.  
%The paper\cite{10.1007/978-3-031-17189-5_14} uses this method. The Author proposed two modules: a shared encoding module and a task-specific module. First, the language model SciBERT was pre-trained. The vector obtained by encoding the text feed into the task-specific module, where the sub-task of classifying each tag is performed. And then a shared TextCNN layer was introduced to learn the dependency information between labels at each level. Finally, the hierarchical feature information is fused by linear transformation and the prediction result of each level is getted.