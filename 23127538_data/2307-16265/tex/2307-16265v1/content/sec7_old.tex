\section{Challenges}


\subsection{label sparsity}

Label sparsity, which pays more attention to the distribution of labels
situation, refers to that many labels in the label set are not assigned to any data sample, that is, there is a large gap between the label set and the sample set. Due to the scarcity of samples in some categories, the model is hard to capture the characteristics of these categories, which affects the final classification results. A challenge about label sparsity is that how to make better use of label information so that the model can learn the characteristics of all data samples. 

\subsection{label imbalance}

label imbalance refers to the situation that the number of training samples of different categories in the classification task varies greatly. Generally speaking, the model is easy to over-fit the categories with a large number, and under-fit the categories with a small number, so unbalanced training samples will lead the model to be more inclined to categories with more samples. Thus when a label appears only a few times, the classifier may choose to ignore it so that this category will not appear in the predicted results. The model will be more friendly to data with relatively balanced categories. So an urgent challenge is to consider how to balance the number of samples in different categories, so that the model can better handle the data of various categories. 

\subsection{label weak correlation}

In hierarchical multi-label classification task, label weak correlation means the weak correlation between labels at different levels or within the same level, that is, the dependency between labels is not obvious or difficult to capture. This will affect the classification results and performance of the model, thus what need to do is take corresponding measures to improve the correlation between labels. 

\subsection{low resource labeled data}

The lack of labeled data is still a big challenge. Although there are some methods, such as data enhancement, migration learning, unsupervised learning and semi-supervised learning, to reduce the use of labeled data, enough tag data is still needed to ensure the performance of the model and the accuracy of result is not particularly ideal. 

\subsection{Deep level prediction difficulty(low accuracy in deep level prediction)}

In hierarchical multi-label classification tasks, deep-level labels usually involve more specific concepts or categories, and these categories may appear less frequently in the data set, which makes it difficult for the model to extract deep features, resulting in greater difficulty in predicting deep-level labels and a corresponding decline in accuracy. Besides, in the model of hierarchical multi-label classification, incorrect classification of a classifier at one level may affect the results of the whole classification. Error classification result information at one level will be propagated to the next level, which causes low accuracy in deep level prediction. 

\subsection{Computational performance}

Because a large number of complex network models are used, such as convolution neural network, recurrent neural network, graph structure and tree structure, there are a lot of parameters to learn, which causes much time and resources on training.Thus a more important thing is to simplify model. 

\subsection{extreme multiple labels}
The Extreme Multi-Label Text Classification (XMTC) is the problem of selecting labels for one text from a collection of tens of thousands of labels. The difficulty of this task lies in the construction of the classifier, data sparsity caused by the long-tail distribution, difficulty in learning dependence patterns among labels, etc. The key of solving the problem is how to construct a classifier to solve the problem rather than building a classifier for each label group. We cannot directly group hierarchical labels because of the long- tail nature of XMC labels that lacks many positive samples of tags, nor can we crop tag sets, which makes tag dependency more difficult to capture.The common methods that XMTC handles are one VS all (OVA), embedding-based, and partitioning methods. Through these methods, a large number of XMC labels are divided into label groups and converted into multiple groups of simple label classification problems. The current common methods are: coding labels as a vector (Chalkidis et al., 2019) and labeling them individually (Gibaja, 2015). However, the effect is mediocre, because the former ignores the tag dependence and semantic information, while the latter can only be annotated partially.