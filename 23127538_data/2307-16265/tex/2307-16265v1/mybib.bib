@inproceedings{huang-etal-2021-balancing,
    title = "Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution",
    author = {Huang, Yi  and
      Giledereli, Buse  and
      K{\"o}ksal, Abdullatif  and
      {\"O}zg{\"u}r, Arzucan  and
      Ozkirimli, Elif},
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.643",
    doi = "10.18653/v1/2021.emnlp-main.643",
    pages = "8153--8161",
    abstract = "Multi-label text classification is a challenging task because it requires capturing label dependencies. It becomes even more challenging when class distribution is long-tailed. Resampling and re-weighting are common approaches used for addressing the class imbalance problem, however, they are not effective when there is label dependency besides class imbalance because they result in oversampling of common labels. Here, we introduce the application of balancing loss functions for multi-label text classification. We perform experiments on a general domain dataset with 90 labels (Reuters-21578) and a domain-specific dataset from PubMed with 18211 labels. We find that a distribution-balanced loss function, which inherently addresses both the class imbalance and label linkage problems, outperforms commonly used loss functions. Distribution balancing methods have been successfully used in the image recognition field. Here, we show their effectiveness in natural language processing. Source code is available at https://github.com/blessu/BalancedLossNLP.",
}

@inproceedings{Mao_2019,
	doi = {10.18653/v1/d19-1042},
  
	url = {https://doi.org/10.18653%2Fv1%2Fd19-1042},
  
	year = 2019,
	publisher = {Association for Computational Linguistics},
  
	author = {Yuning Mao and Jingjing Tian and Jiawei Han and Xiang Ren},
  
	title = {Hierarchical Text Classification with Reinforced Label Assignment},
  
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})}
}
@inproceedings{zhou-etal-2020-hierarchy,
    title = "Hierarchy-Aware Global Model for Hierarchical Text Classification",
    author = "Zhou, Jie  and
      Ma, Chunping  and
      Long, Dingkun  and
      Xu, Guangwei  and
      Ding, Ning  and
      Zhang, Haoyu  and
      Xie, Pengjun  and
      Liu, Gongshen",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.104",
    doi = "10.18653/v1/2020.acl-main.104",
    pages = "1106--1117",
    abstract = "Hierarchical text classification is an essential yet challenging subtask of multi-label text classification with a taxonomic hierarchy. Existing methods have difficulties in modeling the hierarchical label structure in a global view. Furthermore, they cannot make full use of the mutual interactions between the text feature space and the label space. In this paper, we formulate the hierarchy as a directed graph and introduce hierarchy-aware structure encoders for modeling label dependencies. Based on the hierarchy encoder, we propose a novel end-to-end hierarchy-aware global model (HiAGM) with two variants. A multi-label attention variant (HiAGM-LA) learns hierarchy-aware label embeddings through the hierarchy encoder and conducts inductive fusion of label-aware text features. A text feature propagation model (HiAGM-TP) is proposed as the deductive variant that directly feeds text features into hierarchy encoders. Compared with previous works, both HiAGM-LA and HiAGM-TP achieve significant and consistent improvements on three benchmark datasets.",
}
@inproceedings{shimura-etal-2018-hft,
    title = "{HFT}-{CNN}: Learning Hierarchical Category Structure for Multi-label Short Text Categorization",
    author = "Shimura, Kazuya  and
      Li, Jiyi  and
      Fukumoto, Fumiyo",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1093",
    doi = "10.18653/v1/D18-1093",
    pages = "811--816",
    abstract = "We focus on the multi-label categorization task for short texts and explore the use of a hierarchical structure (HS) of categories. In contrast to the existing work using non-hierarchical flat model, the method leverages the hierarchical relations between the pre-defined categories to tackle the data sparsity problem. The lower the HS level, the less the categorization performance. Because the number of training data per category in a lower level is much smaller than that in an upper level. We propose an approach which can effectively utilize the data in the upper levels to contribute the categorization in the lower levels by applying the Convolutional Neural Network (CNN) with a fine-tuning technique. The results using two benchmark datasets show that proposed method, Hierarchical Fine-Tuning based CNN (HFT-CNN) is competitive with the state-of-the-art CNN based methods.",
}
@inproceedings{inproceedings,
author = {Wehrmann, Jônatas and Cerri, Ricardo and Barros, Rodrigo},
year = {2019},
month = {09},
pages = {},
title = {Hierarchical Multi-Label Classification Networks}
}
@inproceedings{wehrmann2018hierarchical,
  title={Hierarchical multi-label classification networks},
  author={Wehrmann, Jonatas and Cerri, Ricardo and Barros, Rodrigo},
  booktitle={International conference on machine learning},
  pages={5075--5084},
  year={2018},
  organization={PMLR}
}

@inproceedings{daengduang2017applying,
  title={Applying One-Versus-One SVMs to classify multi-label data with large labels using spark},
  author={Daengduang, Suthipong and Vateekul, Peerapon},
  booktitle={2017 9th International Conference on Knowledge and Smart Technology (KST)},
  pages={72--77},
  year={2017},
  organization={IEEE}
}


@article{DBLP:journals/corr/abs-1905-10802,
  author    = {Boli Chen and
               Xin Huang and
               Lin Xiao and
               Zixin Cai and
               Liping Jing},
  title     = {Hyperbolic Interaction Model For Hierarchical Multi-Label Classification},
  journal   = {CoRR},
  volume    = {abs/1905.10802},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.10802},
  eprinttype = {arXiv},
  eprint    = {1905.10802},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-10802.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{greenwade93,
    author  = "George D. Greenwade",
    title   = "The {C}omprehensive {T}ex {A}rchive {N}etwork ({CTAN})",
    year    = "1993",
    journal = "TUGBoat",
    volume  = "14",
    number  = "3",
    pages   = "342--351"
}
@inproceedings{aly-etal-2019-hierarchical,
    title = "Hierarchical Multi-label Classification of Text with Capsule Networks",
    author = "Aly, Rami  and
      Remus, Steffen  and
      Biemann, Chris",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-2045",
    doi = "10.18653/v1/P19-2045",
    pages = "323--330",
    abstract = "Capsule networks have been shown to demonstrate good performance on structured data in the area of visual inference. In this paper we apply and compare simple shallow capsule networks for hierarchical multi-label text classification and show that they can perform superior to other neural networks, such as CNNs and LSTMs, and non-neural network architectures such as SVMs. For our experiments, we use the established Web of Science (WOS) dataset and introduce a new real-world scenario dataset, the BlurbGenreCollection (BGC). Our results confirm the hypothesis that capsule networks are especially advantageous for rare events and structurally diverse categories, which we attribute to their ability to combine latent encoded information.",
}


@inproceedings{prabhu2018parabel,
  title={Parabel: Partitioned label trees for extreme classification with application to dynamic search advertising},
  author={Prabhu, Yashoteja and Kag, Anil and Harsola, Shrutendra and Agrawal, Rahul and Varma, Manik},
  booktitle={Proceedings of the 2018 World Wide Web Conference},
  pages={993--1002},
  year={2018}
}



@inproceedings{banerjee-etal-2019-hierarchical,
    title = "Hierarchical Transfer Learning for Multi-label Text Classification",
    author = "Banerjee, Siddhartha  and
      Akkaya, Cem  and
      Perez-Sorrosal, Francisco  and
      Tsioutsiouliklis, Kostas",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1633",
    doi = "10.18653/v1/P19-1633",
    pages = "6295--6300",
    abstract = "Multi-Label Hierarchical Text Classification (MLHTC) is the task of categorizing documents into one or more topics organized in an hierarchical taxonomy. MLHTC can be formulated by combining multiple binary classification problems with an independent classifier for each category. We propose a novel transfer learning based strategy, HTrans, where binary classifiers at lower levels in the hierarchy are initialized using parameters of the parent classifier and fine-tuned on the child category classification task. In HTrans, we use a Gated Recurrent Unit (GRU)-based deep learning architecture coupled with attention. Compared to binary classifiers trained from scratch, our HTrans approach results in significant improvements of 1{\%} on micro-F1 and 3{\%} on macro-F1 on the RCV1 dataset. Our experiments also show that binary classifiers trained from scratch are significantly better than single multi-label models.",
}

@inproceedings{xu-etal-2021-hierarchical,
    title = "Hierarchical Multi-label Text Classification with Horizontal and Vertical Category Correlations",
    author = "Xu, Linli  and
      Teng, Sijie  and
      Zhao, Ruoyu  and
      Guo, Junliang  and
      Xiao, Chi  and
      Jiang, Deqiang  and
      Ren, Bo",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.190",
    doi = "10.18653/v1/2021.emnlp-main.190",
    pages = "2459--2468",
    abstract = "Hierarchical multi-label text classification (HMTC) deals with the challenging task where an instance can be assigned to multiple hierarchically structured categories at the same time. The majority of prior studies either focus on reducing the HMTC task into a flat multi-label problem ignoring the vertical category correlations or exploiting the dependencies across different hierarchical levels without considering the horizontal correlations among categories at the same level, which inevitably leads to fundamental information loss. In this paper, we propose a novel HMTC framework that considers both vertical and horizontal category correlations. Specifically, we first design a loosely coupled graph convolutional neural network as the representation extractor to obtain representations for words, documents, and, more importantly, level-wise representations for categories, which are not considered in previous works. Then, the learned category representations are adopted to capture the vertical dependencies among levels of category hierarchy and model the horizontal correlations. Finally, based on the document embeddings and category embeddings, we design a hybrid algorithm to predict the categories of the entire hierarchical structure. Extensive experiments conducted on real-world HMTC datasets validate the effectiveness of the proposed framework with significant improvements over the baselines.",
}

@inproceedings{liu2022overview,
  title={Overview of NLPCC2022 Shared Task 5 Track 1: Multi-label Classification for Scientific Literature},
  author={Liu, Ming and Zhang, He and Tian, Yangjie and Zong, Tianrui and Cai, Borui and Xu, Ruohua and Li, Yunfeng},
  booktitle={Natural Language Processing and Chinese Computing: 11th CCF International Conference, NLPCC 2022, Guilin, China, September 24--25, 2022, Proceedings, Part II},
  pages={320--327},
  year={2022},
  organization={Springer}
}

@ARTICLE{2020arXiv201010151G,
       author = {{Giunchiglia}, Eleonora and {Lukasiewicz}, Thomas},
        title = "{Coherent Hierarchical Multi-Label Classification Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2020,
        month = oct,
          eid = {arXiv:2010.10151},
        pages = {arXiv:2010.10151},
          doi = {10.48550/arXiv.2010.10151},
archivePrefix = {arXiv},
       eprint = {2010.10151},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv201010151G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2020arXiv200910938Z,
       author = {{Zhang}, Xinyi and {Xu}, Jiahao and {Soh}, Charlie and {Chen}, Lihui},
        title = "{LA-HCN: Label-based Attention for Hierarchical Multi-label TextClassification Neural Network}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = 2020,
        month = sep,
          eid = {arXiv:2009.10938},
        pages = {arXiv:2009.10938},
          doi = {10.48550/arXiv.2009.10938},
archivePrefix = {arXiv},
       eprint = {2009.10938},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200910938Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{wang2022bit,
  title={BIT-WOW at NLPCC-2022 Task5 Track1: Hierarchical Multi-label Classification via Label-Aware Graph Convolutional Network},
  author={Wang, Bo and Lu, Yi-Fan and Wei, Xiaochi and Liu, Xiao and Shi, Ge and Yuan, Changsen and huang, Heyan and Feng, Chong and Mao, Xianling},
  booktitle={Natural Language Processing and Chinese Computing: 11th CCF International Conference, NLPCC 2022, Guilin, China, September 24--25, 2022, Proceedings, Part II},
  pages={192--203},
  year={2022},
  organization={Springer}
}

@inproceedings{ye-etal-2021-beyond,
    title = "Beyond Text: Incorporating Metadata and Label Structure for Multi-Label Document Classification using Heterogeneous Graphs",
    author = "Ye, Chenchen  and
      Zhang, Linhai  and
      He, Yulan  and
      Zhou, Deyu  and
      Wu, Jie",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.253",
    doi = "10.18653/v1/2021.emnlp-main.253",
    pages = "3162--3171",
    abstract = "Multi-label document classification, associating one document instance with a set of relevant labels, is attracting more and more research attention. Existing methods explore the incorporation of information beyond text, such as document metadata or label structure. These approaches however either simply utilize the semantic information of metadata or employ the predefined parent-child label hierarchy, ignoring the heterogeneous graphical structures of metadata and labels, which we believe are crucial for accurate multi-label document classification. Therefore, in this paper, we propose a novel neural network based approach for multi-label document classification, in which two heterogeneous graphs are constructed and learned using heterogeneous graph transformers. One is metadata heterogeneous graph, which models various types of metadata and their topological relations. The other is label heterogeneous graph, which is constructed based on both the labels{'} hierarchy and their statistical dependencies. Experimental results on two benchmark datasets show the proposed approach outperforms several state-of-the-art baselines.",
}

@inproceedings{shen-etal-2021-taxoclass,
    title = "{T}axo{C}lass: Hierarchical Multi-Label Text Classification Using Only Class Names",
    author = "Shen, Jiaming  and
      Qiu, Wenda  and
      Meng, Yu  and
      Shang, Jingbo  and
      Ren, Xiang  and
      Han, Jiawei",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.335",
    doi = "10.18653/v1/2021.naacl-main.335",
    pages = "4239--4249",
    abstract = "Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a taxonomic class hierarchy. Most existing HMTC methods train classifiers using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the document as its {``}core classes{''}, and then check core classes{'} ancestor classes to ensure the coverage. To mimic human experts, we propose a novel HMTC framework, named TaxoClass. Specifically, TaxoClass (1) calculates document-class similarities using a textual entailment model, (2) identifies a document{'}s core classes and utilizes confident core classes to train a taxonomy-enhanced classifier, and (3) generalizes the classifier via multi-label self-training. Our experiments on two challenging datasets show TaxoClass can achieve around 0.71 Example-F1 using only class names, outperforming the best previous method by 25{\%}.",
}



@inproceedings{10.5555/3104482.3104485,
author = {Bi, Wei and Kwok, James T.},
title = {Multi-Label Classification on Tree- and DAG-Structured Hierarchies},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Many real-world applications involve multi-label classification, in which the labels are organized in the form of a tree or directed acyclic graph (DAG). However, current research efforts typically ignore the label dependencies or can only exploit the dependencies in tree-structured hierarchies. In this paper, we present a novel hierarchical multi-label classification algorithm which can be used on both tree- and DAG-structured hierarchies. The key idea is to formulate the search for the optimal consistent multi-label as the finding of the best subgraph in a tree/DAG. Using a simple greedy strategy, the proposed algorithm is computationally efficient, easy to implement, does not suffer from the problem of insufficient/skewed training data in classifier training, and can be readily used on large hierarchies. Theoretical results guarantee the optimality of the obtained solution. Experiments are performed on a large number of functional genomics data sets. The proposed method consistently outperforms the state-of-the-art method on both tree- and DAG-structured hierarchies.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {17–24},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}
@ARTICLE{2019arXiv190408249K,
       author = {{Khandagale}, Sujay and {Xiao}, Han and {Babbar}, Rohit},
        title = "{Bonsai -- Diverse and Shallow Trees for Extreme Multi-label Classification}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2019,
        month = apr,
          eid = {arXiv:1904.08249},
        pages = {arXiv:1904.08249},
          doi = {10.48550/arXiv.1904.08249},
archivePrefix = {arXiv},
       eprint = {1904.08249},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190408249K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{kim-2014-convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}
@ARTICLE{2021arXiv210801921A,
       author = {{Alshubaily}, Ibrahim},
        title = "{TextCNN with Attention for Text Classification}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2021,
        month = aug,
          eid = {arXiv:2108.01921},
        pages = {arXiv:2108.01921},
          doi = {10.48550/arXiv.2108.01921},
archivePrefix = {arXiv},
       eprint = {2108.01921},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210801921A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{yang-etal-2016-hierarchical,
    title = "Hierarchical Attention Networks for Document Classification",
    author = "Yang, Zichao  and
      Yang, Diyi  and
      Dyer, Chris  and
      He, Xiaodong  and
      Smola, Alex  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1174",
    doi = "10.18653/v1/N16-1174",
    pages = "1480--1489",
}
@ARTICLE{2018arXiv181004805D,
       author = {{Devlin}, Jacob and {Chang}, Ming-Wei and {Lee}, Kenton and {Toutanova}, Kristina},
        title = "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2018,
        month = oct,
          eid = {arXiv:1810.04805},
        pages = {arXiv:1810.04805},
          doi = {10.48550/arXiv.1810.04805},
archivePrefix = {arXiv},
       eprint = {1810.04805},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv181004805D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@InProceedings{10.1007/978-3-031-17189-5_14,
author="Zhao, Xiuhao
and Li, Zhao
and Zhang, Xianming
and Wang, Jibin
and Chen, Tong
and Ju, Zhengyu
and Wang, Canjun
and Zhang, Chao
and Zhan, Yiming",
editor="Lu, Wei
and Huang, Shujian
and Hong, Yu
and Zhou, Xiabing",
title="An Interactive Fusion Model for Hierarchical Multi-label Text Classification",
booktitle="Natural Language Processing and Chinese Computing",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="168--178",
abstract="Scientific research literature usually has multi-level labels, and there are often dependencies between multi-level labels. It is crucial for the model to learn and integrate the information between multi-level labels for the hierarchical multi-label text classification (HMTC) of scientific research literature texts. Therefore, for the HMTC task in the scientific research literature, we use the pre-trained language model SciBERT trained on scientific texts. And we introduce a shared TextCNN layer in our multi-task learning architecture to learn the dependency information between labels at each level. Then the hierarchical feature information is fused and propagated from top to bottom according to the task level. We conduct ablation experiments on the dependency information interaction module and the hierarchical information fusion propagation module. Experimental results on the NLPCC2022 SharedTask5 Track1 dataset demonstrate the effectiveness of our model, and we rank 4th place in the task.",
isbn="978-3-031-17189-5"
}


@article{10.1145/3439726,
author = {Minaee, Shervin and Kalchbrenner, Nal and Cambria, Erik and Nikzad, Narjes and Chenaghlu, Meysam and Gao, Jianfeng},
title = {Deep Learning--Based Text Classification: A Comprehensive Review},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3439726},
doi = {10.1145/3439726},
abstract = {Deep learning--based models have surpassed classical machine learning--based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this article, we provide a comprehensive review of more than 150 deep learning--based models for text classification developed in recent years, and we discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and we discuss future research directions.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {62},
numpages = {40},
keywords = {deep learning, topic classification, Text classification, sentiment analysis, news categorization, question answering, natural language inference}
}
@INPROCEEDINGS{8260658,
  author={Kowsari, Kamran and Brown, Donald E. and Heidarysafa, Mojtaba and Jafari Meimandi, Kiana and Gerber, Matthew S. and Barnes, Laura E.},
  booktitle={2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
  title={HDLTex: Hierarchical Deep Learning for Text Classification}, 
  year={2017},
  volume={},
  number={},
  pages={364-371},
  doi={10.1109/ICMLA.2017.0-134}}




@inproceedings{NEURIPS2021_3bbca1d2,
 author = {Zhang, Jiong and Chang, Wei-Cheng and Yu, Hsiang-Fu and Dhillon, Inderjit},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {7267--7280},
 publisher = {Curran Associates, Inc.},
 title = {Fast Multi-Resolution Transformer Fine-tuning for Extreme Multi-label Text Classification},
 volume = {34},
 year = {2021}
}

@article{joulin2016bag,
  title={Bag of tricks for efficient text classification},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.01759},
  year={2016}
}

@article{wydmuch2018no,
  title={A no-regret generalization of hierarchical softmax to extreme multi-label classification},
  author={Wydmuch, Marek and Jasinska, Kalina and Kuznetsov, Mikhail and Busa-Fekete, R{\'o}bert and Dembczynski, Krzysztof},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{prabhu2014fastxml,
  title={Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning},
  author={Prabhu, Yashoteja and Varma, Manik},
  booktitle={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={263--272},
  year={2014}
}

@article{you2019attentionxml,
  title={Attentionxml: Label tree-based attention-aware deep model for high-performance extreme multi-label text classification},
  author={You, Ronghui and Zhang, Zihan and Wang, Ziye and Dai, Suyang and Mamitsuka, Hiroshi and Zhu, Shanfeng},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{Clement2019OnTU,
  title={On the Use of ArXiv as a Dataset},
  author={Colin B. Clement and Matthew Bierbaum and Kevin P. O’Keeffe and Alexander A. Alemi},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.00075}
}

@article{10.5555/1005332.1005345,
author = {Lewis, David D. and Yang, Yiming and Rose, Tony G. and Li, Fan},
title = {RCV1: A New Benchmark Collection for Text Categorization Research},
year = {2004},
issue_date = {12/1/2004},
publisher = {JMLR.org},
volume = {5},
issn = {1532-4435},
abstract = {Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {361–397},
numpages = {37}
}

@article{2015DBpedia,
  title={DBpedia - A Large-scale, Multilingual Knowledge Base Extracted from Wikipedia},
  author={ Lehmann, J.  and  Isele, R.  and  Jakob, M.  and  Jentzsch, A.  and  Kontokostas, D.  and  Mendes, P. N.  and  Hellmann, S.  and  Morsey, M.  and  Kleef, P. V.  and  Bizer, Christian },
  journal={Semantic Web},
  volume={6},
  number={2},
  year={2015},
}


@misc{suzgun2022harvard,
      title={The Harvard USPTO Patent Dataset: A Large-Scale, Well-Structured, and Multi-Purpose Corpus of Patent Applications}, 
      author={Mirac Suzgun and Luke Melas-Kyriazi and Suproteem K. Sarkar and Scott Duke Kominers and Stuart M. Shieber},
      year={2022},
      eprint={2207.04043},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{sadat-caragea-2022-hierarchical,
    title = "Hierarchical Multi-Label Classification of Scientific Documents",
    author = "Sadat, Mobashir  and
      Caragea, Cornelia",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.610",
    pages = "8923--8937",
    abstract = "Automatic topic classification has been studied extensively to assist managing and indexing scientific documents in a digital collection. With the large number of topics being available in recent years, it has become necessary to arrange them in a hierarchy. Therefore, the automatic classification systems need to be able to classify the documents hierarchically. In addition, each paper is often assigned to more than one relevant topic. For example, a paper can be assigned to several topics in a hierarchy tree. In this paper, we introduce a new dataset for hierarchical multi-label text classification (HMLTC) of scientific papers called SciHTC, which contains 186,160 papers and 1,234 categories from the ACM CCS tree. We establish strong baselines for HMLTC and propose a multi-task learning approach for topic classification with keyword labeling as an auxiliary task. Our best model achieves a Macro-F1 score of 34.57{\%} which shows that this dataset provides significant research opportunities on hierarchical scientific topic classification. We make our dataset and code for all experiments publicly available.",
}
@inproceedings{song-etal-2022-adaptive,
    title = "Adaptive Ranking-based Sample Selection for Weakly Supervised Class-imbalanced Text Classification",
    author = "Song, Linxin  and
      Zhang, Jieyu  and
      Yang, Tianxiang  and
      Goto, Masayuki",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.119",
    pages = "1641--1655",
    abstract = "To obtain a large amount of training labels inexpensively, researchers have recently adopted the weak supervision (WS) paradigm, which leverages labeling rules to synthesize training labels rather than using individual annotations to achieve competitive results for natural language processing (NLP) tasks. However, data imbalance is often overlooked in applying the WS paradigm, despite being a common issue in a variety of NLP tasks. To address this challenge, we propose Adaptive Ranking-based Sample Selection (ARS2), a model-agnostic framework to alleviate the data imbalance issue in the WS paradigm. Specifically, it calculates a probabilistic margin score based on the output of the current model to measure and rank the cleanliness of each data point. Then, the ranked data are sampled based on both class-wise and rule-aware ranking. In particular, the two sample strategies corresponds to our motivations: (1) to train the model with balanced data batches to reduce the data imbalance issue and (2) to exploit the expertise of each labeling rule for collecting clean samples. Experiments on four text classification datasets with four different imbalance ratios show that ARS2 outperformed the state-of-the-art imbalanced learning and WS methods, leading to a 2{\%}-57.8{\%} improvement on their F1-score.",
}
@inproceedings{chalkidis-etal-2020-empirical,
    title = "An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels",
    author = "Chalkidis, Ilias  and
      Fergadiotis, Manos  and
      Kotitsas, Sotiris  and
      Malakasiotis, Prodromos  and
      Aletras, Nikolaos  and
      Androutsopoulos, Ion",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.607",
    doi = "10.18653/v1/2020.emnlp-main.607",
    pages = "7503--7515",
    abstract = "Large-scale Multi-label Text Classification (LMTC) has a wide range of Natural Language Processing (NLP) applications and presents interesting challenges. First, not all labels are well represented in the training set, due to the very large label set and the skewed label distributions of datasets. Also, label hierarchies and differences in human labelling guidelines may affect graph-aware annotation proximity. Finally, the label hierarchies are periodically updated, requiring LMTC models capable of zero-shot generalization. Current state-of-the-art LMTC models employ Label-Wise Attention Networks (LWANs), which (1) typically treat LMTC as flat multi-label classification; (2) may use the label hierarchy to improve zero-shot learning, although this practice is vastly understudied; and (3) have not been combined with pre-trained Transformers (e.g. BERT), which have led to state-of-the-art results in several NLP benchmarks. Here, for the first time, we empirically evaluate a battery of LMTC methods from vanilla LWANs to hierarchical classification approaches and transfer learning, on frequent, few, and zero-shot learning on three datasets from different domains. We show that hierarchical methods based on Probabilistic Label Trees (PLTs) outperform LWANs. Furthermore, we show that Transformer-based approaches outperform the state-of-the-art in two of the datasets, and we propose a new state-of-the-art method which combines BERT with LWAN. Finally, we propose new models that leverage the label hierarchy to improve few and zero-shot learning, considering on each dataset a graph-aware annotation proximity measure that we introduce.",
}

@inproceedings{zhang-etal-2022-structural-contrastive,
    title = "Structural Contrastive Representation Learning for Zero-shot Multi-label Text Classification",
    author = "Zhang, Tianyi  and
      Xu, Zhaozhuo  and
      Medini, Tharun  and
      Shrivastava, Anshumali",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.362",
    pages = "4937--4947",
    abstract = "Zero-shot multi-label text classification (ZMTC) is a fundamental task in natural language processing with applications in the cold start problem of recommendation systems. Ideally, one would learn an expressive representation of both input text and label features so that ZMTC is transformed into a nearest neighbor search problem. However, the existing representation learning approaches for ZMTC struggle with accuracy as well as poor training efficiency. Firstly, the input text is structural, consisting of both short title sentences and long content paragraphs. It is challenging to model the correlation between short label descriptions and long structural input documents. Secondly, the enormous label space in ZMTC forces the existing approaches to perform multi-stage learning with label engineering. As a result, the training overhead is significant. In this paper, we address both problems by introducing an end-to-end structural contrastive representation learning approach. We propose a randomized text segmentation (RTS) technique to generate high-quality contrastive pairs. This RTS technique allows us to model title-content correlation. Additionally, we simplify the multi-stage ZMTC learning strategy by avoiding label engineering. Extensive experiments demonstrate that our approach leads to up to 2.33{\%} improvement in precision@1 and 5.94x speedup in training time on publicly available datasets. Our code is available publicly.",
}
@inproceedings{barros-etal-2022-divide,
    title = "Divide and Conquer: An Extreme Multi-Label Classification Approach for Coding Diseases and Procedures in {S}panish",
    author = "Barros, Jose  and
      Rojas, Matias  and
      Dunstan, Jocelyn  and
      Abeliuk, Andres",
    booktitle = "Proceedings of the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.louhi-1.16",
    pages = "138--147",
    abstract = "Clinical coding is the task of transforming medical documents into structured codes following a standard ontology. Since these terminologies are composed of hundreds of codes, this problem can be considered an Extreme Multi-label Classification task. This paper proposes a novel neural network-based architecture for clinical coding. First, we take full advantage of the hierarchical nature of ontologies to create clusters based on semantic relations. Then, we use a Matcher module to assign the probability of documents belonging to each cluster. Finally, the Ranker calculates the probability of each code considering only the documents in the cluster. This division allows a fine-grained differentiation within the cluster, which cannot be addressed using a single classifier. In addition, since most of the previous work has focused on solving this task in English, we conducted our experiments on three clinical coding corpora in Spanish. The experimental results demonstrate the effectiveness of our model, achieving state-of-the-art results on two of the three datasets. Specifically, we outperformed previous models on two subtasks of the CodiEsp shared task: CodiEsp-D (diseases) and CodiEsp-P (procedures). Automatic coding can profoundly impact healthcare by structuring critical information written in free text in electronic health records.",
}
@inproceedings{alturayeif-etal-2022-mawqif,
    title = "Mawqif: A Multi-label {A}rabic Dataset for Target-specific Stance Detection",
    author = "Alturayeif, Nora Saleh  and
      Luqman, Hamzah Abdullah  and
      Ahmed, Moataz Aly Kamaleldin",
    booktitle = "Proceedings of the The Seventh Arabic Natural Language Processing Workshop (WANLP)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wanlp-1.16",
    pages = "174--184",
    abstract = "Social media platforms are becoming inherent parts of people{'}s daily life to express opinions and stances toward topics of varying polarities. Stance detection determines the viewpoint expressed in a text toward a target. While communication on social media (e.g., Twitter) takes place in more than 40 languages, the majority of stance detection research has been focused on English. Although some efforts have recently been made to develop stance detection datasets in other languages, no similar efforts seem to have considered the Arabic language. In this paper, we present Mawqif, the first Arabic dataset for target-specific stance detection, composed of 4,121 tweets annotated with stance, sentiment, and sarcasm polarities. Mawqif, as a multi-label dataset, can provide more opportunities for studying the interaction between different opinion dimensions and evaluating a multi-task model. We provide a detailed description of the dataset, present an analysis of the produced annotation, and evaluate four BERT-based models on it. Our best model achieves a macro-F1 of 78.89{\%}, which shows that there is ample room for improvement on this challenging task. We publicly release our dataset, the annotation guidelines, and the code of the experiments.",
}
@inproceedings{cao-zhang-2022-otseq2set,
    title = "{OTS}eq2{S}et: An Optimal Transport Enhanced Sequence-to-Set Model for Extreme Multi-label Text Classification",
    author = "Cao, Jie  and
      Zhang, Yin",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.377",
    pages = "5588--5597",
    abstract = "Extreme multi-label text classification (XMTC) is the task of finding the most relevant subset labels from an extremely large-scale label collection. Recently, some deep learning models have achieved state-of-the-art results in XMTC tasks. These models commonly predict scores for all labels by a fully connected layer as the last layer of the model. However, such models can{'}t predict a relatively complete and variable-length label subset for each document, because they select positive labels relevant to the document by a fixed threshold or take top k labels in descending order of scores. A less popular type of deep learning models called sequence-to-sequence (Seq2Seq) focus on predicting variable-length positive labels in sequence style. However, the labels in XMTC tasks are essentially an unordered set rather than an ordered sequence, the default order of labels restrains Seq2Seq models in training. To address this limitation in Seq2Seq, we propose an autoregressive sequence-to-set model for XMTC tasks named OTSeq2Set. Our model generates predictions in student-forcing scheme and is trained by a loss function based on bipartite matching which enables permutation-invariance. Meanwhile, we use the optimal transport distance as a measurement to force the model to focus on the closest labels in semantic label space. Experiments show that OTSeq2Set outperforms other competitive baselines on 4 benchmark datasets. Especially, on the Wikipedia dataset with 31k labels, it outperforms the state-of-the-art Seq2Seq method by 16.34{\%} in micro-F1 score. The code is available at https://github.com/caojie54/OTSeq2Set.",
}

@inproceedings{wang-etal-2022-hpt,
    title = "{HPT}: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification",
    author = "Wang, Zihan  and
      Wang, Peiyi  and
      Liu, Tianyu  and
      Lin, Binghuai  and
      Cao, Yunbo  and
      Sui, Zhifang  and
      Wang, Houfeng",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.246",
    pages = "3740--3751",
    abstract = "Hierarchical text classification (HTC) is a challenging subtask of multi-label classification due to its complex label hierarchy.Recently, the pretrained language models (PLM)have been widely adopted in HTC through a fine-tuning paradigm. However, in this paradigm, there exists a huge gap between the classification tasks with sophisticated label hierarchy and the masked language model (MLM) pretraining tasks of PLMs and thus the potential of PLMs cannot be fully tapped.To bridge the gap, in this paper, we propose HPT, a Hierarchy-aware Prompt Tuning method to handle HTC from a multi-label MLM perspective.Specifically, we construct a dynamic virtual template and label words that take the form of soft prompts to fuse the label hierarchy knowledge and introduce a zero-bounded multi-label cross-entropy loss to harmonize the objectives of HTC and MLM.Extensive experiments show HPT achieves state-of-the-art performances on 3 popular HTC datasets and is adept at handling the imbalance and low resource situations. Our code is available at https://github.com/wzh9969/HPT.",
}

@inproceedings{Meng_2018,
	doi = {10.1145/3269206.3271737},
  
	url = {https://doi.org/10.1145%2F3269206.3271737},
  
	year = 2018,
	month = {oct},
  
	publisher = {{ACM}
},
  
	author = {Yu Meng and Jiaming Shen and Chao Zhang and Jiawei Han},
  
	title = {Weakly-Supervised Neural Text Classification},
  
	booktitle = {Proceedings of the 27th {ACM} International Conference on Information and Knowledge Management}
}


@inproceedings{amigo-delgado-2022-evaluating,
    title = "Evaluating Extreme Hierarchical Multi-label Classification",
    author = "Amigo, Enrique  and
      Delgado, Agust{\'\i}n",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.399",
    doi = "10.18653/v1/2022.acl-long.399",
    pages = "5809--5819",
    abstract = "Several natural language processing (NLP) tasks are defined as a classification problem in its most complex form: Multi-label Hierarchical Extreme classification, in which items may be associated with multiple classes from a set of thousands of possible classes organized in a hierarchy and with a highly unbalanced distribution both in terms of class frequency and the number of labels per item. We analyze the state of the art of evaluation metrics based on a set of formal properties and we define an information theoretic based metric inspired by the Information Contrast Model (ICM). Experiments on synthetic data and a case study on real data show the suitability of the ICM for such scenarios.",
}

@inproceedings{jo-etal-2021-devils-advocate,
    title = "Devil{'}s Advocate: Novel Boosting Ensemble Method from Psychological Findings for Text Classification",
    author = "Jo, Hwiyeol  and
      Lim, Jaeseo  and
      Zhang, Byoung-Tak",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.187",
    doi = "10.18653/v1/2021.findings-emnlp.187",
    pages = "2168--2174",
    abstract = "We present a new form of ensemble method{--}Devil{'}s Advocate, which uses a deliberately dissenting model to force other submodels within the ensemble to better collaborate. Our method consists of two different training settings: one follows the conventional training process (Norm), and the other is trained by artificially generated labels (DevAdv). After training the models, Norm models are fine-tuned through an additional loss function, which uses the DevAdv model as a constraint. In making a final decision, the proposed ensemble model sums the scores of Norm models and then subtracts the score of the DevAdv model. The DevAdv model improves the overall performance of the other models within the ensemble. In addition to our ensemble framework being based on psychological background, it also shows comparable or improved performance on 5 text classification tasks when compared to conventional ensemble methods.",
}
@inproceedings{wang-etal-2021-meta-lmtc,
    title = "Meta-{LMTC}: Meta-Learning for Large-Scale Multi-Label Text Classification",
    author = "Wang, Ran  and
      Su, Xi{'}ao  and
      Long, Siyu  and
      Dai, Xinyu  and
      Huang, Shujian  and
      Chen, Jiajun",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.679",
    doi = "10.18653/v1/2021.emnlp-main.679",
    pages = "8633--8646",
    abstract = "Large-scale multi-label text classification (LMTC) tasks often face long-tailed label distributions, where many labels have few or even no training instances. Although current methods can exploit prior knowledge to handle these few/zero-shot labels, they neglect the meta-knowledge contained in the dataset that can guide models to learn with few samples. In this paper, for the first time, this problem is addressed from a meta-learning perspective. However, the simple extension of meta-learning approaches to multi-label classification is sub-optimal for LMTC tasks due to long-tailed label distribution and coexisting of few- and zero-shot scenarios. We propose a meta-learning approach named META-LMTC. Specifically, it constructs more faithful and more diverse tasks according to well-designed sampling strategies and directly incorporates the objective of adapting to new low-resource tasks into the meta-learning phase. Extensive experiments show that META-LMTC achieves state-of-the-art performance against strong baselines and can still enhance powerful BERTlike models.",
}