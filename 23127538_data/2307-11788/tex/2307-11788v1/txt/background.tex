A central component in state-of-the-art NLP approaches is choosing how to model text mathematically using \emph{distributional semantics}, i.e., mapping each word onto a vector, which generally leads to similar words being described by vectors with a high similarity~\cite{KHATTAK2019100057}. Historically, such \emph{word embeddings} where mainly done in a static manner, meaning that words were mapped onto vectors based on their possible meanings according to, e.g., dictionaries~\cite{10.5555/555733}. As NLP evolved, more context aware word embeddings were used, which allows solving potential problems raised from \emph{homonyms}~\cite{kimura1992association}. Homonyms are words that have different meanings based on the context they are used in, e.g., the word ``rock'' can reference a stone or a genre of music. Today's state-of-the-art approaches in regards to qualitative performance avoid such problems using \emph{dynamic} embeddings, which is heavily driven by the availability of massive amounts of data and immense computing resources~\cite{openai2023gpt4}.

Expanding on the mathematical model for text, we now examine methods to process it computationally. As text is a form of temporal data with variable length, recurrent neural networks (RNNs) have been extensively used to infer information from the text~\cite{tarwani2017survey}. However, ordinary RNNs have difficulties accounting for a  grammatical semantics that extend beyond locally available information in a sentence~\cite{tang2018self}, e.g., as in ``The view from this ugly skyscraper was incredibly beautiful'', where the semantically related words ``view'' and ``beautiful'' are separated by almost the whole sentence. Long term dependencies are successfully addressed in a special type of RNNs, i.e., LSTMs, by using the propagation of a cell state in addition to the hidden state over time~\cite{10.1162/neco.1997.9.8.1735}. Expanding on this, the concept of transformers was introduced, which focus on self-attention while shifting away from the recurrent network structure to allow for parallelization in the otherwise necessarily iterative training procedure~\cite{Vashwani2017}.

In combination with substantial compute and large amounts of data, current high performance NLP approaches have shown the ability to learn grammatical semantics without any explicit knowledge about grammar. A completely different approach from RNNs, LSTMs and Transformers is DisCoCat, which natively integrates grammatical information given in \emph{syntax trees}\footnote{In the context of grammar, a syntax tree describes the parts of speech such as nouns, verbs, adjectives, etc., and puts them in structural context~\cite{lambek2008word}.} using \emph{category theory} to unify distributional semantics with the \emph{principle of compositionality}\footnote{This principle states that the meaning of an expression is fully described by the meaning and composition of its parts~\cite{coecke2010mathematical}.}. In essence, DisCoCat exploits a mathematical similarity between grammar, expressed through syntax trees, and quantum computing~\cite{Meichanetzidis2020}.

%In essence, DisCoCat maps syntax trees onto a tensor product of operations to be executed on a tensor product of the vectors describing the words to yield the meaning of a sentence in form of the resulting output vector.

Expanding on this brief overview of NLP, we now show how quantum computing techniques can be productively applied to enhance LSTMs and DisCoCat based NLP. 

\subsection{Using QLSTMs for QNLP}\label{subsec:QLSTMs}
(Q)LSTMs can be viewed as an extension of conventional (Q)RNNs with two core differences: (1) the introduction of an additional cell state $c_t$ for long term memory storage besides the hidden state $h_t$ and (2), the replacement of the single neural network with a specific composition of neural networks (i.e., a (Q)LSTM \emph{cell}) as shown in Fig.~\ref{fig:qlstm}.~\cite{10.1162/neco.1997.9.8.1735, 9747369}
\begin {figure}[htbp]
\centering
\resizebox{\columnwidth}{!}{\input{gfx/QLSTM.tikz}}
%play around with xscale, yscale and font size as necessary. font=\tiny to go smaller, font=\footnotesize to go bigger.
\caption{Structure of a (Q)LSTM in which the internal processing pipeline is indicated by arrows. Inputs are the cell state $c_{t-1}$ of the previous time step, the corresponding hidden state $h_{t-1}$ and the current state of the time series data $x_t$ to be processed. $c_t$ and $h_t$ denote the computed cell- and hidden states passed to the next cell. The output of the cell is $y_t$.}
\label{fig:qlstm}
\end{figure}

In this figure, $\sigma$ denotes a sigmoid activation function, analogue notation is used for $\tanh$. The operations $\bigodot$ and $\bigoplus$ denote element-wise multiplication and addition of vectors. $x_t$ denotes the data input at point $t$ and $y_t$ the output at the same point in time, $h_t$ denotes the hidden state similar to the hidden state in conventional RNNs and $c_t$ denotes the cell state of the LSTM. \cite{Goodfellow2016}

The LSTM cell is divided into three neural networks acting as gates:
\begin{enumerate}
    \item The \emph{forget gate}: The first (Q)NN determines the amount of information to be forgotten about the state $c_{t-1}$. The lower the value of $\sigma\left(\text{(Q)NN}_1\left(h_{t-1}, x_t\right)\right)$, the more the resulting entries of $c_{t-1}$ approach zero.
    \item The \emph{input gate}: Involving the second and third QNN, the update gate is split into deciding how much information is to be updated ((Q)NN${}_2$) and which information is to be updated  ((Q)NN${}_3$).
    \item The \emph{output gate}: Determines the information to be output based on all states $x_t$, $h_{t-1}$ and the processed $c_{t-1}$.
\end{enumerate}
In the presented structure, additional postprocessing of the generated state from the output gate is conducted individually to allow for yielding different outputs $h_t$ and $y_t$ (in some implementations, $y_t=h_t$ is used). Choosing suitable dimensionalities for $h$ and $c$, as well as architectures for all (Q)NNs, (Q)LSMTs can be used to process time series completely analog to standard (Q)RNNs~\cite{dhake2023algorithms}.

\subsection{Using DisCoCat for QNLP}\label{subsec:DisCoCat}
Following the original DisCoCat approach proposed by Coecke et al.~\cite{coecke2010mathematical, EPTCS221.8}, there are three steps to generate a mathematical model of a sentence that can be processed using quantum computers:
\begin{enumerate}
    \item \label{itm:step1} Parse the sentence into a pregroup expression.
    \item \label{itm:step2} Construct the corresponding DisCoCat diagram (incl. possible reductions).
    \item \label{itm:step3} Translate the DisCoCat diagram into a quantum circuit according to possibly parameterized word embeddings.
\end{enumerate}
A pregroup $\left(A, 1, \cdot, -^l, -^r,\leq\right)$ can be understood as a special non-commutative group, i.e., essentially a group with differentiating between left and right inverses and an order relation. By assigning a type (i.e., a pregroup element) to each word in a sentence and concatenating these with suitable left and right inverses depending on their grammatical function in the sentence, we can express the sentence in form of a pregroup expression. Considering the sentence ``Alice loves Bob'' and following the established pregroup grammar for the English language~\cite{EPTCS221.8}, all nouns are identified with the pregroup element $n$ and the verb ``loves'' is identified as $n^r \cdot s \cdot n^l$, meaning that it expects a noun from the left and right and yields a complete sentence $s$ when concatenated with these:
\begin{align}
    \text{``Alice loves Bob''} \quad \mapsto\quad  & n \cdot \left( n^r \cdot s \cdot n^l \right) \cdot n \label{eq:map-to-pregroup}\\
    =  &\left( n \cdot  n^r \right) \cdot s \cdot \left( n^l  \cdot n \right) = 1 \cdot s \cdot 1 = s \nonumber
\end{align}
Expanding on this model of grammar, we now introduce the distributional semantics aspect of DisCoCat to construct the DisCoCat diagram (step~\ref{itm:step2}). This is done by mapping each atomic type $a\in A$ onto a vector space, i.e., $n$ to $N$ and $s$ to $S$, and their concatenation to tensor product spaces ($ n^r \cdot s \cdot n^l$ to $N\otimes S\otimes N$). Therefore, a pregroup expression is then represented as a concatenation of functions, e.g., ``loves'' becomes a bilinear map $N\times N \rightarrow S$ while adjectives (of type $n\cdot n^l$) are represented by a linear map $N\rightarrow N$. Using the diagrammatic calculus of compact closed categories, such computations can be represented in so-called DisCoCat diagrams, as shown in Fig.~\ref{fig:discocat}. Following the established choices of maps and using complex vector spaces, these diagrams can be reshaped according to mathematical properties as, e.g., the isomorphism between row and column vectors through taking the conjugate transpose. A possible reduction of the example in Fig.~\ref{fig:discocat} is depicted in Fig.~\ref{fig:discocat-reduced}.

% Figure environment removed

To finally conduct step~\ref{itm:step3}, i.e., mapping the resulting DisCoCat diagram onto a quantum circuit, $\bigtriangledown$ boxes are substituted by quantum states $\ket{\psi}$ based on the chosen word embedding, while $\bigtriangleup$ boxes correspond to their adjoints $\bra{\psi}$. Reading the DisCoCat diagrams from top to bottom and defining straight wires $\mid$ as identity operations, this leads to the quantum circuit shown in Fig.~\ref{fig:circuit}. The different structures shown in equation~\eqref{eq:map-to-pregroup}, the diagrammatic representation in Fig.~\ref{fig:bigone} and the quantum circuit in Fig.~\ref{fig:circuit} are equivalent in a categorical sense.



Note that this QNLP approach requires classical postselection, i.e., to extract the meaning of the sentence $\ket{s}$, wires 1 and 3 have to be measured in the $\ket{0}^{\otimes \left| N\right|}$ state. In practice, this can contribute to a potentially immense increase in overall runtime, especially when scaling up the number of qubits per DisCoCat wire, i.e., in this case, the dimensionality of $N$.
