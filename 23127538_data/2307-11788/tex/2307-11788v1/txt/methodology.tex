

In the following, we describe our process of data generation, the choice for the classical baseline facilitating meaningful evaluation of the results, as well as the implementation of the QLSTM and the DisCoCat approach.

\subsection{Dataset generation}
Aiming to use the same datasets for the QLSTM and DisCoCat approaches, the sentences must inherit a grammatical structure that is comprehensible to the syntax parser employed in DisCoCat. To the authors' awareness, no real world datasets inheriting this property while also being small enough for current quantum circuit simulators are available to date. Motivated by the recent success of large language models like ChatGPT~\cite{OpenAI2022}, we propose to employ such means to create suitable synthetic datasets. To generate simple sentences containing sentiments on finance topics, we interacted with ChatGPT in the following way:

\noindent\begin{newchat}
Generate sentences with a maximum length of five words discussing financial topics or stocks in a positive, neutral or negative way. At the end of each sentence, mention its respective label with negative being 0, neutral being 1 and positive being 2. \hfill $[$\textit{Query}$]$
Inflation fears rattle markets (Negative - 0) \\Interest rates stay steady (Neutral - 1)\\$[$\textit{ChatGPT}$]$ \hfill Apple reports record profits (Positive - 2)
\end{newchat}

To generate more complex statements, we altered the input to: ``Generate detailed sentences discussing financial topics or stocks in a positive, neutral or negative way. [...]''. An example of a positive reply to this was ``The rise of online banking has made it easier and more convenient for customers
to manage their finances''.

An overview of the generated datasets is displayed in Tab.~\ref{tab:DataDistribution}.

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c|c|c|c}
        & \multicolumn{3}{c|}{Class} & & \\ \hline
        Complexity & $-$ & $\circ$ & $+$ & $\diameter$ word count & vocabulary size \\ \Xhline{2\arrayrulewidth}
         Low & 34\% & 18\% & 48\% & 4.9 & 913 \\ \hline
         Moderate & 37\% & 17\% & 46\% & 18.4 & 1608
    \end{tabular}
    \caption{Data class distribution of both generated datasets encompassing roughly 1000 sentences each.}
    \label{tab:DataDistribution}
\end{table}


\begin {figure}[t]
\centering
\begin{quantikz}%[font=\scriptsize, column sep=9pt, row sep={20pt,between origins}]
\lstick{$\ket{0}^{\otimes\nu}$} & \gate[3]{U_{\text{loves}}} & \gate{U_{\text{Bob}}^{\dagger}} & \meter{} & \rstick{$\ket{0}^{\otimes \nu}$}\qw \\
\lstick{$\ket{0}^{\otimes \lceil\log_2\left(\left| S\right|\right)\rceil}$} & \qw & \qw & \qw & \rstick{$\ket{s}$}\qw   \\
\lstick{$\ket{0}^{\otimes\nu}$} & \qw & \gate{U_{\text{Alice}}^{\dagger}} & \meter{} & \rstick{$\ket{0}^{\otimes\nu}$}\qw 
\end{quantikz}
\caption{A quantum circuit for ``Alice loves Bob'' when using an arbitrary word embedding $U_{\text{word}}$, where $\nu \coloneqq  \lceil\log_2\left(\left| N\right|\right)\rceil$.} %the least amount of required qubits for each state connected to $k$ wires of the DisCoCat diagram (i.e., $k$) and 
\label{fig:circuit}
\end{figure}

\subsection{Classical Baseline LSTM}
To allow for a meaningful comparison of results, we employ a LSTM as the classical baseline. Following Sec.~\ref{subsec:QLSTMs}, a hyperparameter search the low (moderate) complexity datasets lead to the use of the ReLU activation function, a trainable word embedding layer with size 5 (10), one unidirectional LSTM layer, one fully connected hidden layer of size 8 (16), followed by a dropout layer with rate 0 (0.1).

\subsection{QLSTM Implementation}
Analogously, QLSTM implementation also follows the general architecture described in Sec.~\ref{subsec:QLSTMs} but differs from the classic baseline by exchanging the LSTM with a QLSTM cell. As a result of a basic hyperparameter tuning for the low (moderate) complexity datasets, we choose: A three qubit one hot encoding for $y_t$, four qubits and one ansatz layer in each QNN, whereas the ansatz corresponds to that proposed in the original QLSTM paper\footnote{The ansatz consists of a data input layer, a variational layer and a measurement layer. The data input layer starts with Hadamard gates on every qubit $q_i$ (where $i\in\left\{1,...,n\right\}$), then proceeds with $Ry$ gates rotating for $\arctan\left(x_i\right)$ and finishes analogously with $R_z$ rotating for $\arctan\left(x^2_i\right)$, where $x_i$ denotes the scalar data inputs. The variational layer starts with cyclic CNOTs where every qubit $q_i$ iteratively acts as the control for the $q_{i+k \mod n}$-th qubit, where $k\in\left\{1,2\right\}$.} for the low (moderate) complexity datasets~\cite{9747369}. The shape of $h$ and $c$ inside the (Q)LSTM cell are determined by concatenation $v_t$ of the previous hidden state $h_{t-1}$ and current input vector $x_t$.

\subsection{DisCoCat Implementation}
For implementing the DisCoCat approach, we follow the procedure stated in Sec.~\ref{subsec:DisCoCat}. In doing so, the first key component is a suitable pregroup parser. As no stable pregroup parsers exists at the time of conducting this research, we use the common, closest suited substitution: A CCG parser~\cite{yeung2021ccg}. In our implementation we used the BobCatParser from the Lambeq~\cite{kartsaklis2021lambeq} python library for this task. For translating the DisCoCat diagrams into quantum circuits, we follow~\cite{10.1613/jair.1.14329} by choosing these ansÃ¤tze: Single qubit words are embedded with the standard Euler parameterization (i.e., $R_x(\theta_3)R_z(\theta_2)R_x(\theta_1)\ket{0}$) and words spanning over $d>1$ qubits are embedded with $d$ many IQP layers~\cite{havlivcek2019supervised}, which initially apply a Hadamard gate to each qubit and then apply parameterized, controlled $Rz$ gates in a chain-like pattern iteratively for all neighboring wires.

As the only available software implementation for DisCoCat\footnote{For details, see \href{https://github.com/ichrist97/qnlp\_finance}{https://github.com/ichrist97/qnlp\_finance}.} merely supported CPU-based circuit simulation (opposed to the available GPU-support for the (Q)LSTM implementation) lead to major wallclock time issues, we restricted the DisCoCat approach to binary classification by dropping the neutral data class data. While this generally makes the results less comparable, its impact is arguably negligible in the encountered case where QLSTM outperforms DisCoCat. The fundamental benefit of this is, that $\ket{s}$ can be represented with a single qubit, as it only needs to carry binary information hence requiring the least possible computational effort.

