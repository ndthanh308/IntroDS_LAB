@INPROCEEDINGS{8455771,
  author={Ao, Shen},
  booktitle={2018 International Conference on Audio, Language and Image Processing (ICALIP)}, 
  title={Sentiment Analysis Based on Financial Tweets and Market Information}, 
  year={2018},
  volume={},
  number={},
  pages={321-326},
  doi={10.1109/ICALIP.2018.8455771}}

@Article{info14040242,
AUTHOR = {Patwardhan, Narendra and Marrone, Stefano and Sansone, Carlo},
TITLE = {Transformers in the Real World: A Survey on NLP Applications},
JOURNAL = {Information},
VOLUME = {14},
YEAR = {2023},
NUMBER = {4},
ARTICLE-NUMBER = {242},
URL = {https://www.mdpi.com/2078-2489/14/4/242},
ISSN = {2078-2489},
ABSTRACT = {The field of Natural Language Processing (NLP) has undergone a significant transformation with the introduction of Transformers. From the first introduction of this technology in 2017, the use of transformers has become widespread and has had a profound impact on the field of NLP. In this survey, we review the open-access and real-world applications of transformers in NLP, specifically focusing on those where text is the primary modality. Our goal is to provide a comprehensive overview of the current state-of-the-art in the use of transformers in NLP, highlight their strengths and limitations, and identify future directions for research. In this way, we aim to provide valuable insights for both researchers and practitioners in the field of NLP. In addition, we provide a detailed analysis of the various challenges faced in the implementation of transformers in real-world applications, including computational efficiency, interpretability, and ethical considerations. Moreover, we highlight the impact of transformers on the NLP community, including their influence on research and the development of new NLP models.},
DOI = {10.3390/info14040242}
}

@article{Nath2022,
abstract = {Natural language processing (NLP) is a subfield of machine intelligence focused  on the interaction of human language with computer systems. NLP has recently been discussed in the mainstream media and the literature with the advent of Generative Pre-trained Transformer 3 (GPT-3), a language model capable of producing human-like text. The release of GPT-3 has also sparked renewed interest on the applicability of NLP to contemporary healthcare problems. This article provides an overview of NLP models, with a focus on GPT-3, as well as discussion of applications specific to ophthalmology. We also outline the limitations of GPT-3 and the challenges with its integration into routine ophthalmic care.},
author = {Nath, Siddharth and Marie, Abdullah and Ellershaw, Simon and Korot, Edward and Keane, Pearse A},
doi = {10.1136/bjophthalmol-2022-321141},
issn = {1468-2079 (Electronic)},
journal = {Br. J. Ophthalmol.},
keywords = {Humans,Natural Language Processing,Ophthalmology},
language = {eng},
month = {jul},
number = {7},
pages = {889--892},
pmid = {35523534},
title = {{New meaning for NLP: the trials and tribulations of natural language processing  with GPT-3 in ophthalmology.}},
volume = {106},
year = {2022}
}

@inproceedings{10.1145/3458817.3476209,
author = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
title = {Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476209},
doi = {10.1145/3458817.3476209},
abstract = {Large language models have led to state-of-the-art accuracies across several tasks. However, training these models efficiently is challenging because: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to scaling issues at thousands of GPUs. In this paper, we show how tensor, pipeline, and data parallelism can be composed to scale to thousands of GPUs. We propose a novel interleaved pipelining schedule that can improve throughput by 10+\% with memory footprint comparable to existing approaches. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs (per-GPU throughput of 52\% of theoretical peak).},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {58},
numpages = {15},
location = {St. Louis, Missouri},
series = {SC '21}
}

@article{10.1093/mind/LIX.236.433,
    author = {TURING, A. M.},
    title = "{I.—COMPUTING MACHINERY AND INTELLIGENCE}",
    journal = {Mind},
    volume = {LIX},
    number = {236},
    pages = {433-460},
    year = {1950},
    month = {10},
    issn = {0026-4423},
    doi = {10.1093/mind/LIX.236.433},
    url = {https://doi.org/10.1093/mind/LIX.236.433},
    eprint = {https://academic.oup.com/mind/article-pdf/LIX/236/433/30123314/lix-236-433.pdf},
}


@Article{app12115651,
AUTHOR = {Guarasci, Raffaele and De Pietro, Giuseppe and Esposito, Massimo},
TITLE = {Quantum Natural Language Processing: Challenges and Opportunities},
JOURNAL = {Applied Sciences},
VOLUME = {12},
YEAR = {2022},
NUMBER = {11},
ARTICLE-NUMBER = {5651},
URL = {https://www.mdpi.com/2076-3417/12/11/5651},
ISSN = {2076-3417},
ABSTRACT = {The meeting between Natural Language Processing (NLP) and Quantum Computing has been very successful in recent years, leading to the development of several approaches of the so-called Quantum Natural Language Processing (QNLP). This is a hybrid field in which the potential of quantum mechanics is exploited and applied to critical aspects of language processing, involving different NLP tasks. Approaches developed so far span from those that demonstrate the quantum advantage only at the theoretical level to the ones implementing algorithms on quantum hardware. This paper aims to list the approaches developed so far, categorizing them by type, i.e., theoretical work and those implemented on classical or quantum hardware; by task, i.e., general purpose such as syntax-semantic representation or specific NLP tasks, like sentiment analysis or question answering; and by the resource used in the evaluation phase, i.e., whether a benchmark dataset or a custom one has been used. The advantages offered by QNLP are discussed, both in terms of performance and methodology, and some considerations about the possible usage QNLP approaches in the place of state-of-the-art deep learning-based ones are given.},
DOI = {10.3390/app12115651}
}



@article{10.1613/jair.1.14329,
author = {Lorenz, Robin and Pearson, Anna and Meichanetzidis, Konstantinos and Kartsaklis, Dimitri and Coecke, Bob},
title = {QNLP in Practice: Running Compositional Models of Meaning on a Quantum Computer},
year = {2023},
issue_date = {May 2023},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {76},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.14329},
doi = {10.1613/jair.1.14329},
abstract = {Quantum Natural Language Processing (QNLP) deals with the design and implementation of NLP models intended to be run on quantum hardware. In this paper, we present results on the first NLP experiments conducted on Noisy Intermediate-Scale Quantum (NISQ) computers for datasets of size greater than 100 sentences. Exploiting the formal similarity of the compositional model of meaning by Coecke, Sadrzadeh, and Clark (2010) with quantum theory, we create representations for sentences that have a natural mapping to quantum circuits. We use these representations to implement and successfully train NLP models that solve simple sentence classification tasks on quantum hardware. We conduct quantum simulations that compare the syntax-sensitive model of Coecke et al. with two baselines that use less or no syntax; specifically, we implement the quantum analogues of a “bag-of-words” model, where syntax is not taken into account at all, and of a word-sequence model, where only word order is respected. We demonstrate that all models converge smoothly both in simulations and when run on quantum hardware, and that the results are the expected ones based on the nature of the tasks and the datasets used. Another important goal of this paper is to describe in a way accessible to AI and NLP researchers the main principles, process and challenges of experiments on quantum hardware. Our aim in doing this is to take the first small steps in this unexplored research territory and pave the way for practical Quantum Natural Language Processing.},
journal = {J. Artif. Int. Res.},
month = {apr},
numpages = {38}
}

@INPROCEEDINGS{9747369,
  author={Chen, Samuel Yen-Chi and Yoo, Shinjae and Fang, Yao-Lung L.},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Quantum Long Short-Term Memory}, 
  year={2022},
  volume={},
  number={},
  pages={8622-8626},
  doi={10.1109/ICASSP43922.2022.9747369}}

@article{10.1162/neco.1997.9.8.1735,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@Inproceedings{EPTCS221.8,
  author    = {Zeng, William and Coecke, Bob},
  year      = {2016},
  title     = {Quantum Algorithms for Compositional Natural Language Processing},
  editor    = {Kartsaklis, Dimitrios and Lewis, Martha and Rimell, Laura},
  booktitle = {{\rm Proceedings of the 2016 Workshop on}
               Semantic Spaces at the Intersection of NLP, Physics and Cognitive Science,
               {\rm Glasgow, Scotland, 11th June 2016}},
  series    = {Electronic Proceedings in Theoretical Computer Science},
  volume    = {221},
  publisher = {Open Publishing Association},
  pages     = {67-75},
  doi       = {10.4204/EPTCS.221.8},
}

@article{coecke2010mathematical,
  title={Mathematical foundations for distributed compositional model of meaning. Lambek festschrift},
  author={Coecke, Bob and Sadrzadeh, Mehrnoosh and Clark, Stephen},
  journal={Linguistic Analysis},
  volume={36},
  pages={345--384},
  year={2010}
}

@article{Stamatopoulos2022towardsquantum,
  doi = {10.22331/q-2022-07-20-770},
  url = {https://doi.org/10.22331/q-2022-07-20-770},
  title = {Towards {Q}uantum {A}dvantage in {F}inancial {M}arket {R}isk using {Q}uantum {G}radient {A}lgorithms},
  author = {Stamatopoulos, Nikitas and Mazzola, Guglielmo and Woerner, Stefan and Zeng, William J.},
  journal = {{Quantum}},
  issn = {2521-327X},
  publisher = {{Verein zur F{\"{o}}rderung des Open Access Publizierens in den Quantenwissenschaften}},
  volume = {6},
  pages = {770},
  month = jul,
  year = {2022}
}

@article{KHATTAK2019100057,
title = {A survey of word embeddings for clinical text},
journal = {Journal of Biomedical Informatics},
volume = {100},
pages = {100057},
year = {2019},
note = {Articles initially published in Journal of Biomedical Informatics: X 1-4, 2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.yjbinx.2019.100057},
url = {https://www.sciencedirect.com/science/article/pii/S2590177X19300563},
author = {Faiza Khan Khattak and Serena Jeblee and Chloé Pou-Prom and Mohamed Abdalla and Christopher Meaney and Frank Rudzicz},
keywords = {Word embeddings, Clinical data, Natural language processing},
abstract = {Representing words as numerical vectors based on the contexts in which they appear has become the de facto method of analyzing text with machine learning. In this paper, we provide a guide for training these representations on clinical text data, using a survey of relevant research. Specifically, we discuss different types of word representations, clinical text corpora, available pre-trained clinical word vector embeddings, intrinsic and extrinsic evaluation, applications, and limitations of these approaches. This work can be used as a blueprint for clinicians and healthcare workers who may want to incorporate clinical text features in their own models and applications.}
}

@book{10.5555/555733,
author = {Jurafsky, Daniel and Martin, James H.},
title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
year = {2000},
isbn = {0130950696},
publisher = {Prentice Hall PTR},
address = {USA},
edition = {1st},
abstract = {From the Publisher:This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora. Methodology boxes are included in each chapter. Each chapter is built around one or more worked examples to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language processing.}
}

@inproceedings{kimura1992association,
  title={Association-based natural language processing with neural networks},
  author={Kimura, Kazuhiro and Uzuoka, Takashi and Amano, Shin-ya},
  booktitle={30th Annual Meeting of the Association for Computational Linguistics},
  pages={224--231},
  year={1992}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Martinez.2022,
 author = {Martinez, Victor and Leroy-Meline, Guilhaume},
 title = {{A multiclass Q-NLP sentiment analysis experiment using DisCoCat}},
 url = {https://arxiv.org/abs/2209.03152},
 year = {2022}
}

@inproceedings{DiSipio.2022,
 author = {{Di Sipio}, Riccardo and Huang, Jia-Hong and Chen, Samuel Yen-Chi and Mangini, Stefano and Worring, Marcel},
 booktitle = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 title = {{The Dawn of Quantum Natural Language Processing}},
 year = {2022}
}

@online{OpenAI2022,
 author = {OpenAI},
 title = {{Introducing ChatGPT}},
 url = {https://openai.com/blog/chatgpt},
 urldate = {2023-03-07},
 year = {2022}
}

@article{kartsaklis2021lambeq,
 author = {Kartsaklis, Dimitri and Fan, Ian and Yeung, Richie and Pearson, Anna and Lorenz, Robin and Toumi, Alexis and de Felice, Giovanni and Meichanetzidis, Konstantinos and Clark, Stephen and Coecke, Bob},
 title = {{lambeq: {A}n {E}fficient {H}igh-{L}evel {P}ython {L}ibrary for {Q}uantum {NLP}}},
 year = {2021}
}

@article{Meichanetzidis2020,
 author = {Meichanetzidis, Konstantinos and Toumi, Alexis and de Felice, Giovanni and Coecke, Bob},
 title = {{Grammar-Aware Question-Answering on Quantum Computers}},
 year = {2020}
}

@book{Goodfellow2016,
 author = {Goodfellow, Ian and Bengio, Joshua and Courville, Aaron},
 publisher = {MIT Press},
 title = {{Deep Learning}},
 year = {2016}
}

@inproceedings{Vashwani2017,
 author = {Vashwani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems 30 (NIPS 2017)},
 publisher = {Curran Associates, Inc.},
 title = {{Attention Is All You Need}},
 year = {2017}
}


@article{tarwani2017survey,
  title={Survey on recurrent neural network in natural language processing},
  author={Tarwani, Kanchan M and Edem, Swathi},
  journal={Int. J. Eng. Trends Technol},
  volume={48},
  number={6},
  pages={301--304},
  year={2017}
}


@inproceedings{tang2018self,
  title={Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures},
  author={Tang, Gongbo and M{\"u}ller, Mathias and Rios, Annette and Sennrich, Rico},
  booktitle={2018 Conference on Empirical Methods in Natural Language Processing},
  year={2018},
  organization={Association for Computational Linguistics}
}

@inproceedings{bai-etal-2021-syntax,
    title = "Syntax-{BERT}: Improving Pre-trained Transformers with Syntax Trees",
    author = "Bai, Jiangang  and
      Wang, Yujing  and
      Chen, Yiren  and
      Yang, Yaming  and
      Bai, Jing  and
      Yu, Jing  and
      Tong, Yunhai",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.262",
    doi = "10.18653/v1/2021.eacl-main.262",
    pages = "3011--3020",
    abstract = "Pre-trained language models like BERT achieve superior performances in various NLP tasks without explicit consideration of syntactic information. Meanwhile, syntactic information has been proved to be crucial for the success of NLP applications. However, how to incorporate the syntax trees effectively and efficiently into pre-trained Transformers is still unsettled. In this paper, we address this problem by proposing a novel framework named Syntax-BERT. This framework works in a plug-and-play mode and is applicable to an arbitrary pre-trained checkpoint based on Transformer architecture. Experiments on various datasets of natural language understanding verify the effectiveness of syntax trees and achieve consistent improvement over multiple pre-trained models, including BERT, RoBERTa, and T5.",
}

@article{caro2022generalization,
  title={Generalization in quantum machine learning from few training data},
  author={Caro, Matthias C and Huang, Hsin-Yuan and Cerezo, Marco and Sharma, Kunal and Sornborger, Andrew and Cincio, Lukasz and Coles, Patrick J},
  journal={Nature communications},
  volume={13},
  number={1},
  pages={4919},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@book{lambek2008word,
  title={From Word to Sentence: A Computational Algebraic Approach to Grammar},
  author={Lambek, J.},
  isbn={9788876991172},
  series={Open access publications},
  url={https://books.google.de/books?id=ZHgRaRaadJ4C},
  year={2008},
  publisher={Polimetrica}
}


@inproceedings{yeung2021ccg,
  title={A CCG-Based Version of the DisCoCat Framework},
  author={Yeung, Richie and Kartsaklis, Dimitri},
  booktitle={Proceedings of the 2021 Workshop on Semantic Spaces at the Intersection of NLP, Physics, and Cognitive Science (SemSpace)},
  pages={20--31},
  year={2021}
}


@article{skolik2021layerwise,
  title={Layerwise learning for quantum neural networks},
  author={Skolik, Andrea and McClean, Jarrod R and Mohseni, Masoud and van der Smagt, Patrick and Leib, Martin},
  journal={Quantum Machine Intelligence},
  volume={3},
  pages={1--11},
  year={2021},
  publisher={Springer}
}

@article{dhake2023algorithms,
  title={Algorithms for Hyperparameter Tuning of LSTMs for Time Series Forecasting},
  author={Dhake, Harshal and Kashyap, Yashwant and Kosmopoulos, Panagiotis},
  journal={Remote Sensing},
  volume={15},
  number={8},
  pages={2076},
  year={2023},
  publisher={MDPI}
}

@article{havlivcek2019supervised,
  title={Supervised learning with quantum-enhanced feature spaces},
  author={Havl{\'\i}{\v{c}}ek, Vojt{\v{e}}ch and C{\'o}rcoles, Antonio D and Temme, Kristan and Harrow, Aram W and Kandala, Abhinav and Chow, Jerry M and Gambetta, Jay M},
  journal={Nature},
  volume={567},
  number={7747},
  pages={209--212},
  year={2019},
  publisher={Nature Publishing Group UK London}
}
