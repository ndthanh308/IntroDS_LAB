{
  "title": "Boon: A Neural Search Engine for Cross-Modal Information Retrieval",
  "authors": [
    "Yan Gong",
    "Georgina Cosma"
  ],
  "submission_date": "2023-07-26T15:08:02+00:00",
  "revised_dates": [
    "2023-11-02T00:25:55+00:00"
  ],
  "abstract": "Visual-Semantic Embedding (VSE) networks can help search engines better understand the meaning behind visual content and associate it with relevant textual information, leading to more accurate search results. VSE networks can be used in cross-modal search engines to embed image and textual descriptions in a shared space, enabling image-to-text and text-to-image retrieval tasks. However, the full potential of VSE networks for search engines has yet to be fully explored. This paper presents Boon, a novel cross-modal search engine that combines two state-of-the-art networks: the GPT-3.5-turbo large language model, and the VSE network VITR (VIsion Transformers with Relation-focused learning) to enhance the engine's capabilities in extracting and reasoning with regional relationships in images. VITR employs encoders from CLIP that were trained with 400 million image-description pairs and it was fine-turned on the RefCOCOg dataset. Boon's neural-based components serve as its main functionalities: 1) a 'cross-modal search engine' that enables end-users to perform image-to-text and text-to-image retrieval. 2) a 'multi-lingual conversational AI' component that enables the end-user to converse about one or more images selected by the end-user. Such a feature makes the search engine accessible to a wide audience, including those with visual impairments. 3) Boon is multi-lingual and can take queries and handle conversations about images in multiple languages. Boon was implemented using the Django and PyTorch frameworks. The interface and capabilities of the Boon search engine are demonstrated using the RefCOCOg dataset, and the engine's ability to search for multimedia through the web is facilitated by Google's API.",
  "categories": [
    "cs.MM"
  ],
  "primary_category": "cs.MM",
  "doi": "10.1145/3606040.3617440",
  "journal_ref": "MMIR '23: Proceedings of the 1st International Workshop on Deep Multimodal Learning for Information Retrieval (2023)",
  "arxiv_id": "2307.14240",
  "pdf_url": null,
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 19836774,
  "size_after_bytes": 333039
}