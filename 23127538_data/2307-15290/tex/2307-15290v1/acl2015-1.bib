@article{kudo2018sentencepiece,
  title={Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing},
  author={Kudo, Taku and Richardson, John},
  journal={arXiv preprint arXiv:1808.06226},
  year={2018}
}

@misc{gpt3.5,
  author = {OpenAI},
  title = {Chatgpt: Optimizing language models for dialogue},
  howpublished = {Blog post},
  year = {2023},
  url = {https://openai.com/blog/chatgpt},
}

@misc{gpt4,
  author = {OpenAI},
  title = {Gpt-4 technical report},
  howpublished = {Blog post},
  year = {2023},
  url = {https://arxiv.org/abs/2303.08774},
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@misc{Dolly,
  author = {Databricks},
  title = {Hello Dolly: Democratizing the magic of ChatGPT with open models},
  howpublished = {Blog post},
  month = {March},
  year = {2023},
  url = {https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html},
  urldate = {2023-03-24}
}

@misc{Dolly2.0,
  author = {Databricks},
  title = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
  howpublished = {Blog post},
  month = {April},
  year = {2023},
  url = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
  urldate = {2023-04-12}
}

@article{ji2023exploring2,
  title={Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases},
  author={Ji, Yunjie and Deng, Yong and Gong, Yan and Peng, Yiping and Niu, Qiang and Zhang, Lei and Ma, Baochang and Li, Xiangang},
  journal={arXiv preprint arXiv:2303.14742},
  year={2023}
}

@article{ji2023exploring1,
  title={Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences},
  author={Ji, Yunjie and Gong, Yan and Peng, Yiping and Ni, Chao and Sun, Peiyan and Pan, Dongyu and Ma, Baochang and Li, Xiangang},
  journal={arXiv preprint arXiv:2303.07610},
  year={2023}
}

@misc{ShareGPT,
  author = {ShareGPT},
  title = {Share your wildest ChatGPT conversations with one click},
  howpublished = {Blog post},
  year = {2023},
  url = {https://sharegpt.com/}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{peng2023instruction,
  title={Instruction Tuning with GPT-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality},
    url = {https://vicuna.lmsys.org},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@misc{koala_blogpost_2023,
  author = {Xinyang Geng and Arnav Gudibande and Hao Liu and Eric Wallace and Pieter Abbeel and Sergey Levine and Dawn Song},
  title = {Koala: A Dialogue Model for Academic Research},
  howpublished = {Blog post},
  month = {April},
  year = {2023},
  url = {https://bair.berkeley.edu/blog/2023/04/03/koala/},
  urldate = {2023-04-03}
}

@article{xu2023baize,
  title={Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data},
  author={Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  journal={arXiv preprint arXiv:2304.01196},
  year={2023}
}

@misc{chinese-llama-alpaca,
  author = {Yiming Cui and Ziqing Yang},
  title = {Chinese LLaMA and Alpaca LLMs},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/ymcui/Chinese-LLaMA-Alpaca}},
}

@article{xu2022zeroprompt,
  title={ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization},
  author={Xu, Hanwei and Chen, Yujun and Du, Yulun and Shao, Nan and Wang, Yanggang and Li, Haiyu and Yang, Zhilin},
  journal={arXiv preprint arXiv:2201.06910},
  year={2022}
}

@article{xie2022unifiedskg,
  title={Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models},
  author={Xie, Tianbao and Wu, Chen Henry and Shi, Peng and Zhong, Ruiqi and Scholak, Torsten and Yasunaga, Michihiro and Wu, Chien-Sheng and Zhong, Ming and Yin, Pengcheng and Wang, Sida I and others},
  journal={arXiv preprint arXiv:2201.05966},
  year={2022}
}

@inproceedings{wang2022super,
  title={Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Naik, Atharva and Ashok, Arjun and Dhanasekaran, Arut Selvan and Arunkumar, Anjana and Stap, David and others},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={5085--5109},
  year={2022}
}

@article{khashabi2020unifiedqa,
  title={Unifiedqa: Crossing format boundaries with a single qa system},
  author={Khashabi, Daniel and Min, Sewon and Khot, Tushar and Sabharwal, Ashish and Tafjord, Oyvind and Clark, Peter and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2005.00700},
  year={2020}
}

@article{min2021metaicl,
  title={Metaicl: Learning to learn in context},
  author={Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2110.15943},
  year={2021}
}

@article{ye2021crossfit,
  title={Crossfit: A few-shot learning challenge for cross-task generalization in nlp},
  author={Ye, Qinyuan and Lin, Bill Yuchen and Ren, Xiang},
  journal={arXiv preprint arXiv:2104.08835},
  year={2021}
}

@article{liu2019multi,
  title={Multi-task deep neural networks for natural language understanding},
  author={Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:1901.11504},
  year={2019}
}

@article{zhong2021adapting,
  title={Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections},
  author={Zhong, Ruiqi and Lee, Kristy and Zhang, Zheng and Klein, Dan},
  journal={arXiv preprint arXiv:2104.04670},
  year={2021}
}

@article{mishra2021cross,
  title={Cross-task generalization via natural language crowdsourcing instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2104.08773},
  year={2021}
}

@inproceedings{
  zeng2023glm-130b,
  title={{GLM}-130B: An Open Bilingual Pre-trained Model},
  author={Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and Zhiyuan Liu and Peng Zhang and Yuxiao Dong and Jie Tang},
  booktitle={The Eleventh International Conference on Learning Representations (ICLR)},
  year={2023},
  url={https://openreview.net/forum?id=-Aw0rrrPUF}
}

@software{openchatkit,
  title = {{OpenChatKit: An Open Toolkit and Base Model for Dialogue-style Applications}},
  author = {Together Computer},
  url = {https://github.com/togethercomputer/OpenChatKit},
  month = {3},
  year = {2023},
  version = {0.15},
}

@article{wang2022self2,
  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{suzgun2022challenging,
  title={Challenging BIG-Bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}

@article{bang2023multitask,
  title={A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity},
  author={Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others},
  journal={arXiv preprint arXiv:2302.04023},
  year={2023}
}

@article{ye2023comprehensive,
  title={A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models},
  author={Ye, Junjie and Chen, Xuanting and Xu, Nuo and Zu, Can and Shao, Zekai and Liu, Shichun and Cui, Yuhan and Zhou, Zeyang and Gong, Chao and Shen, Yang and others},
  journal={arXiv preprint arXiv:2303.10420},
  year={2023}
}

@misc{ConstitutionalAIHarmlessness2022,
  title = {Constitutional AI: Harmlessness from AI Feedback},
  shorttitle = {Constitutional AI},
  author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and others},
  year = {2022},
  month = dec,
  number = {arXiv:2212.08073},
  eprint = {arXiv:2212.08073},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{FineTuningLanguageModels2020,
  title = {Fine-Tuning Language Models from Human Preferences},
  author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and others},
  year = {2020},
  month = jan,
  number = {arXiv:1909.08593},
  eprint = {arXiv:1909.08593},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{LearningSummarizeHuman2022,
  title = {Learning to Summarize from Human Feedback},
  author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and others},
  year = {2022},
  month = feb,
  number = {arXiv:2009.01325},
  eprint = {arXiv:2009.01325},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{PretrainingLanguageModels2023,
  title = {Pretraining Language Models with Human Preferences},
  author = {Korbak, Tomasz and Shi, Kejian and Chen, Angelica and others},
  year = {2023},
  month = feb,
  number = {arXiv:2302.08582},
  eprint = {arXiv:2302.08582},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@article{RedTeamingLanguage,
  title = {Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned},
  author = {Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and others},
  year = {2022},
  langid = {english}
}

@article{glaese2022improving,
  title={Improving alignment of dialogue agents via targeted human judgements},
  author={Glaese, Amelia and McAleese, Nat and Trebacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},
  journal={arXiv preprint arXiv:2209.14375},
  year={2022}
}
@article{dong2019unified,
  title={Unified language model pre-training for natural language understanding and generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}
@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}
@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}
@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}
@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{yuan2021bartscore,
  title={Bartscore: Evaluating generated text as text generation},
  author={Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={27263--27277},
  year={2021}
}
@article{pillutla2022mauve,
  title={MAUVE Scores for Generative Models: Theory and Practice},
  author={Pillutla, Krishna and Liu, Lang and Thickstun, John and others},
  journal={arXiv preprint arXiv:2212.14578},
  year={2022}
}
@article{sellam2020bleurt,
  title={BLEURT: Learning robust metrics for text generation},
  author={Sellam, Thibault and Das, Dipanjan and Parikh, Ankur P},
  journal={arXiv preprint arXiv:2004.04696},
  year={2020}
}
@article{rei2020comet,
  title={COMET: A neural framework for MT evaluation},
  author={Rei, Ricardo and Stewart, Craig and Farinha, Ana C and Lavie, Alon},
  journal={arXiv preprint arXiv:2009.09025},
  year={2020}
}
@article{zhao2019moverscore,
  title={MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance},
  author={Zhao, Wei and Peyrard, Maxime and Liu, Fei and others},
  journal={arXiv preprint arXiv:1909.02622},
  year={2019}
}
@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and others},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}
@article{tangdoes,
  title={Does Synthetic Data Generation of LLMs Help Clinical Text Mining?},
  author={Tang, Ruixiang and Han, Xiaotian and Jiang, Xiaoqian and Hu, Xia},
  year={2022}
}
@misc{BLOOM176BParameterOpenAccess2022,
  title = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  shorttitle = {BLOOM},
  author = {Workshop, BigScience and Scao, Teven Le and Fan, Angela and others},
  year = {2022},
  month = dec,
  number = {arXiv:2211.05100},
  eprint = {arXiv:2211.05100},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{ChatAugLeveragingChatGPT2023,
  title = {ChatAug: Leveraging ChatGPT for Text Data Augmentation},
  shorttitle = {ChatAug},
  author = {Dai, Haixing and Liu, Zhengliang and Liao, Wenxiong and others},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13007},
  eprint = {arXiv:2302.13007},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{ChatGPTBeginningEnd2023,
  title = {ChatGPT: Beginning of an End of Manual Annotation? Use Case of Automatic Genre Identification},
  shorttitle = {ChatGPT},
  author = {Kuzman, Taja and Ljube{\v s}i{\'c}, Nikola and Mozeti{\v c}, Igor},
  year = {2023},
  month = mar,
  number = {arXiv:2303.03953},
  eprint = {arXiv:2303.03953},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{ChatGPTGeneralPurposeNatural2023,
  title = {Is ChatGPT a General-Purpose Natural Language Processing Task Solver?},
  author = {Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and others},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06476},
  eprint = {arXiv:2302.06476},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{ChatGPTGoodNLG2023,
  title = {Is ChatGPT a Good NLG Evaluator? A Preliminary Study},
  shorttitle = {Is ChatGPT a Good NLG Evaluator?},
  author = {Wang, Jiaan and Liang, Yunlong and Meng, Fandong and others},
  year = {2023},
  month = mar,
  number = {arXiv:2303.04048},
  eprint = {arXiv:2303.04048},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{CodeGenOpenLarge2023,
  title = {CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
  shorttitle = {CodeGen},
  author = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and others},
  year = {2023},
  month = feb,
  number = {arXiv:2203.13474},
  eprint = {arXiv:2203.13474},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{CrosslingualGeneralizationMultitask2022,
  title = {Crosslingual Generalization through Multitask Finetuning},
  author = {Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and others},
  year = {2022},
  month = nov,
  number = {arXiv:2211.01786},
  eprint = {arXiv:2211.01786},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{EvaluatingLargeLanguage2021,
  title = {Evaluating Large Language Models Trained on Code},
  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and others},
  year = {2021},
  month = jul,
  number = {arXiv:2107.03374},
  eprint = {arXiv:2107.03374},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@article{FinetunedLanguageModels2021,
  title = {Finetuned Language Models Are Zero-Shot Learners},
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and others},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.01652 [cs]},
  eprint = {2109.01652},
  primaryclass = {cs},
  archiveprefix = {arxiv}
}

@misc{FlanCollectionDesigning2023,
  title = {The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
  shorttitle = {The Flan Collection},
  author = {Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and others},
  year = {2023},
  month = feb,
  number = {arXiv:2301.13688},
  eprint = {arXiv:2301.13688},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{GeneralLanguageAssistant2021,
  title = {A General Language Assistant as a Laboratory for Alignment},
  author = {Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  year = {2021},
  month = dec,
  number = {arXiv:2112.00861},
  eprint = {arXiv:2112.00861},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{GPTNeoX20BOpenSourceAutoregressive2022,
  title = {GPT-NeoX-20B: An Open-Source Autoregressive Language Model},
  shorttitle = {GPT-NeoX-20B},
  author = {Black, Sid and Biderman, Stella and Hallahan, Eric and others},
  year = {2022},
  month = apr,
  number = {arXiv:2204.06745},
  eprint = {arXiv:2204.06745},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{HolisticEvaluationLanguage2022,
  title = {Holistic Evaluation of Language Models},
  author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and others},
  year = {2022},
  month = nov,
  number = {arXiv:2211.09110},
  eprint = {arXiv:2211.09110},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{LanguageModelsAre2020,
  title = {Language Models Are Few-Shot Learners},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and others},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {arXiv:2005.14165},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{LargeLanguageModels2023,
  title = {Large Language Models Are State-of-the-Art Evaluators of Translation Quality},
  author = {Kocmi, Tom and Federmann, Christian},
  year = {2023},
  month = feb,
  number = {arXiv:2302.14520},
  eprint = {arXiv:2302.14520},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@article{MultitaskPromptedTraining2021,
  title = {Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author = {Sanh, Victor and Webson, Albert and Raffel, Colin and others},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.08207 [cs]},
  eprint = {2110.08207},
  primaryclass = {cs},
  archiveprefix = {arxiv}
}

@misc{OPTOpenPretrained2022,
  title = {OPT: Open Pre-Trained Transformer Language Models},
  shorttitle = {OPT},
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and others},
  year = {2022},
  month = jun,
  number = {arXiv:2205.01068},
  eprint = {arXiv:2205.01068},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{PaLMScalingLanguage2022,
  title = {PaLM: Scaling Language Modeling with Pathways},
  shorttitle = {PaLM},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and others},
  year = {2022},
  month = oct,
  number = {arXiv:2204.02311},
  eprint = {arXiv:2204.02311},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{ScalingInstructionFinetunedLanguage2022,
  title = {Scaling Instruction-Finetuned Language Models},
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and others},
  year = {2022},
  month = oct,
  number = {arXiv:2210.11416},
  eprint = {arXiv:2210.11416},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@article{ScalingLanguageModels,
  title = {Scaling Language Models: Methods, Analysis \& Insights from Training Gopher},
  author = {Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and others},
  year = {2022}
}

@misc{TrainingComputeOptimalLarge2022,
  title = {Training Compute-Optimal Large Language Models},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and others},
  year = {2022},
  month = mar,
  number = {arXiv:2203.15556},
  eprint = {arXiv:2203.15556},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{TrainingHelpfulHarmless2022,
  title = {Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
  author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and others},
  year = {2022},
  month = apr,
  number = {arXiv:2204.05862},
  eprint = {arXiv:2204.05862},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{TrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and others},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02155},
  eprint = {arXiv:2203.02155},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{WebGPTBrowserassistedQuestionanswering2022,
  title = {WebGPT: Browser-Assisted Question-Answering with Human Feedback},
  shorttitle = {WebGPT},
  author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and others},
  year = {2022},
  month = jun,
  number = {arXiv:2112.09332},
  eprint = {arXiv:2112.09332},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.09332},
  archiveprefix = {arxiv}
}




@misc{AdapterFirst2019,
  title = {Parameter-Efficient Transfer Learning for NLP},
  author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and others},
  year = {2019},
  month = jun,
  number = {arXiv:1902.00751},
  eprint = {arXiv:1902.00751},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{PrefixTuning2021,
  title = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author = {Li, Xiang Lisa and Liang, Percy},
  year = {2021},
  month = jan,
  number = {arXiv:2101.00190},
  eprint = {arXiv:2101.00190},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{pTuning,
  title = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  year = {2021},
  month = apr,
  number = {arXiv:2104.08691},
  eprint = {arXiv:2104.08691},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@inproceedings{adapater2,
  title={Exploring versatile generative language model via parameter-efficient transfer learning},
  author={Lin, Zhaojiang and Madotto, Andrea and Fung, Pascale},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP},
  year={2020}
}

@misc{LoRA,
  title = {LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS},
  author = {Hu, Edward and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2022},
  month = jun,
  number = {arXiv:2106.09685},
  eprint = {arXiv:2106.09685},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{ptuning2,
  title = {Gpt understands, too},
  author = {Liu, Xiao and Zheng, Yanan and Du,Zhengxiao and Ding,Ming and others},
  year = {2021},
  number = {arXiv:2103.10385},
  eprint = {arXiv:2103.10385},
  publisher = {arXiv},
  archiveprefix = {arxiv},
}

@misc{Chatgpt,
  title = {Chatgpt: Optimizing language models for dialogue},
  author = {OpenAI},
  year = {2023a}
}


@misc{Gpt-4,
  title = {Gpt-4 technical report},
  author = {OpenAI},
  year = {2023b}
}


@misc{Liupeftoverview,
  title = {Parameter-efficient fine-tuning of large-scale pre-trained language models},
  author = {Ding, Ning and Qin, Yujia and Yang, Guang and others},
  month = mar,
  publisher = {Nature Machine Intelligence},
  year = {2023}
}


@misc{peft-github,
  title = {State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods},
  author = {huggingface},
  url = {https://github.com/huggingface/peft},
  year = {2023}
}

@misc{Aghajanyan,
  title = {Intrinsic Dimensionality Explains the
Effectiveness of Language Model Fine-Tuning},
  author = {Aghajanyan, Armen and Zettlemoyer, Luke and Gupta, Sonal},
  year = {2020},
  month = dec,
  number = {arXiv:2012.13255},
  eprint = {arXiv:2012.13255},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{Baize,
  title = {Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data},
  author = {Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  year = {2023},
  month = apr,
  number = {arXiv:2304.01196},
  eprint = {arXiv:2304.01196},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{alpaca-lora,
  title = {Alpaca-LoRA},
  author = {Stanford},
  url = {https://github.com/tloen/alpaca-lora},
  year = {2023}
}

@misc{belle-data,
  title = {Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases},
  author = {Ji, Yunjie and Deng, Yong and Gong, Yan and Peng, Yiping and Niu, Qiang and Zhang, Lei and Ma, Baochang and Li, Xiangang},
  year = {2023b},
  month = mar,
  number = {arXiv:2303.14742},
  eprint = {arXiv:2303.14742},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}

@misc{belle-eval,
  title = {Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences},
  author = {Ji, Yunjie and Gong, Yan and Peng, Yiping and Ni, Chao and Sun, Peiyan and Pan, Dongyu and Ma, Baochang and Li,Xiangang},
  year = {2023a},
  month = mar,
  number = {arXiv:2303.07610},
  eprint = {arXiv:2303.07610},
  publisher = {arXiv},
  archiveprefix = {arxiv}
}