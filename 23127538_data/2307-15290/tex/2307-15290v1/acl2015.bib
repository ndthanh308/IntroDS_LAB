@misc{gpt3.5,
  author = {OpenAI},
  title = {Chatgpt: Optimizing language models for dialogue},
  howpublished = {Blog post},
  year = {2023},
  url = {https://openai.com/blog/chatgpt},
}

@misc{gpt4,
  author = {OpenAI},
  title = {Gpt-4 technical report},
  howpublished = {Blog post},
  year = {2023},
  url = {https://arxiv.org/abs/2303.08774},
}
@misc{BELLE,
  author = {Ji, Yunjie and Deng, Yong and Gong, Yan and Peng, Yiping and Niu, Qiang and Ma, Baochang and Li, Xiangang},
  title = {BELLE: Be Everyone's Large Language model Engine },
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/LianjiaTech/BELLE}},
}
@article{wu2023bloomberggpt,
  title={Bloomberggpt: A large language model for finance},
  author={Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
  journal={arXiv preprint arXiv:2303.17564},
  year={2023}
}

@article{cui2023chatlaw,
  title={ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases},
  author={Cui, Jiaxi and Li, Zongjian and Yan, Yang and Chen, Bohua and Yuan, Li},
  journal={arXiv preprint arXiv:2306.16092},
  year={2023}
}

@article{huang2023lawyer,
  title={Lawyer LLaMA Technical Report},
  author={Huang, Quzhe and Tao, Mingxu and An, Zhenwei and Zhang, Chen and Jiang, Cong and Chen, Zhibin and Wu, Zirui and Feng, Yansong},
  journal={arXiv preprint arXiv:2305.15062},
  year={2023}
}


@article{DAPTpaper,
  title={Don't stop pretraining: Adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.10964},
  year={2020}
}


@misc{wang2023clinicalgpt,
      title={ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation}, 
      author={Guangyu Wang and Guoxing Yang and Zongxin Du and Longjun Fan and Xiaohu Li},
      year={2023},
      eprint={2306.09968},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2023huatuo,
      title={HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge},
      author={Haochun Wang and Chi Liu and Nuwa Xi and Zewen Qiang and Sendong Zhao and Bing Qin and Ting Liu},
      year={2023},
      eprint={2304.06975},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}

@article{yuan2021wudaocorpora,
  title={Wudaocorpora: A super large-scale chinese corpora for pre-training language models},
  author={Yuan, Sha and Zhao, Hanyu and Du, Zhengxiao and Ding, Ming and Liu, Xiao and Cen, Yukuo and Zou, Xu and Yang, Zhilin and Tang, Jie},
  journal={AI Open},
  volume={2},
  pages={65--68},
  year={2021},
  publisher={Elsevier}
}
@ONLINE{wikidump,
    author = "Wikimedia Foundation",
    title  = "Wikimedia Downloads",
    url    = "https://dumps.wikimedia.org"
}

@article{huang2023c,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Lei, Jiayi and others},
  journal={arXiv preprint arXiv:2305.08322},
  year={2023}
}
@article{li2023cmmlu,
  title={CMMLU: Measuring massive multitask language understanding in Chinese},
  author={Li, Haonan and Zhang, Yixuan and Koto, Fajri and Yang, Yifei and Zhao, Hai and Gong, Yeyun and Duan, Nan and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2306.09212},
  year={2023}
}

@article{peng2023instruction,
  title={Instruction Tuning with GPT-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@misc{ling2023domain,
      title={Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey}, 
      author={Chen Ling and Xujiang Zhao and Jiaying Lu and Chengyuan Deng and Can Zheng and Junxiang Wang and Tanmoy Chowdhury and Yun Li and Hejie Cui and Xuchao Zhang and Tianjiao Zhao and Amit Panalkar and Wei Cheng and Haoyu Wang and Yanchi Liu and Zhengzhang Chen and Haifeng Chen and Chris White and Quanquan Gu and Jian Pei and Carl Yang and Liang Zhao},
      year={2023},
      eprint={2305.18703},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ouyang2022training,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhao2023survey,
      title={A Survey of Large Language Models}, 
      author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
      year={2023},
      eprint={2303.18223},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Baichuan13Bgithub,
      title={Baichuan-13B}, 
      year={2023},
      author={Baichuan Intelligent Technology},
      publisher = {GitHub},
      journal = {GitHub repository},
      howpublished = {\url{https://github.com/baichuan-inc/Baichuan-13B}},
}

@misc{yang2023fingpt,
      title={FinGPT: Open-Source Financial Large Language Models}, 
      author={Hongyang Yang and Xiao-Yang Liu and Christina Dan Wang},
      year={2023},
      eprint={2306.06031},
      archivePrefix={arXiv},
      primaryClass={q-fin.ST}
}

@misc{Ext5,
      title={Ext5: Towards extreme multi-task scaling for transfer learning}, 
      author={Aribandi V, Tay Y, Schuster T, Rao J, Zheng HS, Mehta SV, Zhuang H, Tran VQ, Bahri D, Ni J, Gupta J},
      year={2021},
      eprint={2111.10952},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{glm130b,
      title={GLM-130B: An Open Bilingual Pre-trained Model}, 
      author={Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and Peng Zhang and Yuxiao Dong and Jie Tang},
      year={2022},
      eprint={2210.02414},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{galactica,
      title={Galactica: A Large Language Model for Science}, 
      author={Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and Anthony Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},
      year={2022},
      eprint={2211.09085},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{ram2023incontext,
      title={In-Context Retrieval-Augmented Language Models}, 
      author={Ori Ram and Yoav Levine and Itay Dalmedigos and Dor Muhlgay and Amnon Shashua and Kevin Leyton-Brown and Yoav Shoham},
      year={2023},
      eprint={2302.00083},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}