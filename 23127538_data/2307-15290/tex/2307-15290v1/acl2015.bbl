\begin{thebibliography}{}

\bibitem[\protect\citename{Aribandi~V}2021]{Ext5}
Schuster T Rao J Zheng HS Mehta SV Zhuang H Tran VQ Bahri D Ni J Gupta~J
  Aribandi~V, Tay~Y.
\newblock 2021.
\newblock Ext5: Towards extreme multi-task scaling for transfer learning.

\bibitem[\protect\citename{Cui \bgroup et al.\egroup }2023]{cui2023chatlaw}
Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li~Yuan.
\newblock 2023.
\newblock Chatlaw: Open-source legal large language model with integrated
  external knowledge bases.
\newblock {\em arXiv preprint arXiv:2306.16092}.

\bibitem[\protect\citename{Gururangan \bgroup et al.\egroup }2020]{DAPTpaper}
Suchin Gururangan, Ana Marasovi{\'c}, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy,
  Doug Downey, and Noah~A Smith.
\newblock 2020.
\newblock Don't stop pretraining: Adapt language models to domains and tasks.
\newblock {\em arXiv preprint arXiv:2004.10964}.

\bibitem[\protect\citename{Huang \bgroup et al.\egroup }2023a]{huang2023lawyer}
Quzhe Huang, Mingxu Tao, Zhenwei An, Chen Zhang, Cong Jiang, Zhibin Chen, Zirui
  Wu, and Yansong Feng.
\newblock 2023a.
\newblock Lawyer llama technical report.
\newblock {\em arXiv preprint arXiv:2305.15062}.

\bibitem[\protect\citename{Huang \bgroup et al.\egroup }2023b]{huang2023c}
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su,
  Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et~al.
\newblock 2023b.
\newblock C-eval: A multi-level multi-discipline chinese evaluation suite for
  foundation models.
\newblock {\em arXiv preprint arXiv:2305.08322}.

\bibitem[\protect\citename{Ji \bgroup et al.\egroup }2023]{BELLE}
Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Baochang Ma, and
  Xiangang Li.
\newblock 2023.
\newblock Belle: Be everyone's large language model engine.
\newblock \url{https://github.com/LianjiaTech/BELLE}.

\bibitem[\protect\citename{Lee \bgroup et al.\egroup }2020]{lee2020biobert}
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan~Ho So,
  and Jaewoo Kang.
\newblock 2020.
\newblock Biobert: a pre-trained biomedical language representation model for
  biomedical text mining.
\newblock {\em Bioinformatics}, 36(4):1234--1240.

\bibitem[\protect\citename{Li \bgroup et al.\egroup }2023]{li2023cmmlu}
Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan
  Duan, and Timothy Baldwin.
\newblock 2023.
\newblock Cmmlu: Measuring massive multitask language understanding in chinese.
\newblock {\em arXiv preprint arXiv:2306.09212}.

\bibitem[\protect\citename{Ling \bgroup et al.\egroup }2023]{ling2023domain}
Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang,
  Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, Tianjiao Zhao, Amit
  Panalkar, Wei Cheng, Haoyu Wang, Yanchi Liu, Zhengzhang Chen, Haifeng Chen,
  Chris White, Quanquan Gu, Jian Pei, Carl Yang, and Liang Zhao.
\newblock 2023.
\newblock Domain specialization as the key to make large language models
  disruptive: A comprehensive survey.

\bibitem[\protect\citename{OpenAI}2023a]{gpt3.5}
OpenAI.
\newblock 2023a.
\newblock Chatgpt: Optimizing language models for dialogue.
\newblock Blog post.

\bibitem[\protect\citename{OpenAI}2023b]{gpt4}
OpenAI.
\newblock 2023b.
\newblock Gpt-4 technical report.
\newblock Blog post.

\bibitem[\protect\citename{Ouyang \bgroup et al.\egroup
  }2022]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.
\newblock 2022.
\newblock Training language models to follow instructions with human feedback.

\bibitem[\protect\citename{Peng \bgroup et al.\egroup
  }2023]{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock 2023.
\newblock Instruction tuning with gpt-4.
\newblock {\em arXiv preprint arXiv:2304.03277}.

\bibitem[\protect\citename{Ram \bgroup et al.\egroup }2023]{ram2023incontext}
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin
  Leyton-Brown, and Yoav Shoham.
\newblock 2023.
\newblock In-context retrieval-augmented language models.

\bibitem[\protect\citename{Taylor \bgroup et al.\egroup }2022]{galactica}
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony
  Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.
\newblock 2022.
\newblock Galactica: A large language model for science.

\bibitem[\protect\citename{Technology}2023]{Baichuan13Bgithub}
Baichuan~Intelligent Technology.
\newblock 2023.
\newblock Baichuan-13b.
\newblock \url{https://github.com/baichuan-inc/Baichuan-13B}.

\bibitem[\protect\citename{Wang \bgroup et al.\egroup
  }2023a]{wang2023clinicalgpt}
Guangyu Wang, Guoxing Yang, Zongxin Du, Longjun Fan, and Xiaohu Li.
\newblock 2023a.
\newblock Clinicalgpt: Large language models finetuned with diverse medical
  data and comprehensive evaluation.

\bibitem[\protect\citename{Wang \bgroup et al.\egroup }2023b]{wang2023huatuo}
Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting
  Liu.
\newblock 2023b.
\newblock Huatuo: Tuning llama model with chinese medical knowledge.

\bibitem[\protect\citename{Wu \bgroup et al.\egroup }2023]{wu2023bloomberggpt}
Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian
  Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.
\newblock 2023.
\newblock Bloomberggpt: A large language model for finance.
\newblock {\em arXiv preprint arXiv:2303.17564}.

\bibitem[\protect\citename{Yang \bgroup et al.\egroup }2023]{yang2023fingpt}
Hongyang Yang, Xiao-Yang Liu, and Christina~Dan Wang.
\newblock 2023.
\newblock Fingpt: Open-source financial large language models.

\bibitem[\protect\citename{Yuan \bgroup et al.\egroup
  }2021]{yuan2021wudaocorpora}
Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu~Zou,
  Zhilin Yang, and Jie Tang.
\newblock 2021.
\newblock Wudaocorpora: A super large-scale chinese corpora for pre-training
  language models.
\newblock {\em AI Open}, 2:65--68.

\bibitem[\protect\citename{Zeng \bgroup et al.\egroup }2022]{glm130b}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
  Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng~Lam Tam, Zixuan Ma, Yufei Xue,
  Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang.
\newblock 2022.
\newblock Glm-130b: An open bilingual pre-trained model.

\bibitem[\protect\citename{Zhao \bgroup et al.\egroup }2023]{zhao2023survey}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
  Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,
  Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,
  Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.
\newblock 2023.
\newblock A survey of large language models.

\end{thebibliography}
