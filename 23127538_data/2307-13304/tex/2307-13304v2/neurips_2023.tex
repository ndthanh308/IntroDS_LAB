\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023
\PassOptionsToPackage{numbers,compress}{natbib}

% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}

\usepackage{amsmath,amssymb,amsthm}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
% \usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{algorithm}
\usepackage{algpseudocode}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}


% User defined packages
\usepackage{todonotes} %comments [disable] to hide todos
\usepackage{soul} %strikethrough text
%\usepackage[bibliography=common]{apxproof} %proofs automatically in appendix
\usepackage[bibliography=separate]{apxproof} %proofs automatically in appendix
%\newcommand{\customapxsubsectiontitle}[2]{\begin{toappendix}\subsection*{Proofs for Subsection~[#1]~([#2])}\end{toappendix}}
\renewcommand{\appendixbibliographystyle}{plainnat}
\newtheoremrep{theorem}{Theorem}
\newtheoremrep{corollary}{Corollary}[theorem]
\newtheoremrep{assumption}{Assumption}
\newtheoremrep{lemma}[theorem]{Lemma}
\newtheoremrep{definition}{Definition}
\usepackage{wrapfig}
\usepackage{multirow}
\newcommand\Tstrut{\rule{0pt}{2.6ex}}         % = `top' strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut
\usepackage{listings} % code 
\usepackage{adjustbox}

% User defined commands
\newcommand{\round}{\mathcal{Q}}
\newcommand{\alglin}{\mathcal{A}}
\newcommand{\Lworst}{\mathcal{L}_{\operatorname{worst}}}
\newcommand{\Lavg}{\mathcal{L}_{\operatorname{avg}}}
\newcommand{\eig}[1]{\operatorname{eig}(#1)}
\newcommand{\diag}[1]{\operatorname{diag}(#1)}
\newcommand{\ldl}{\mathsf{LDLQ}}
\newcommand{\optq}{\mathsf{OPTQ}}
\newcommand{\near}{\mathsf{Near}}
\newcommand{\stoch}{\mathsf{Stoch}}
\newcommand{\quip}{\mathsf{QuIP}}
\newcommand{\optquip}{\mathsf{OPTQuIP}}

\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\Abs}[1]{\left|#1\right|}
\newcommand{\Exv}[2][]{\mathbf{E}_{#1}\left[#2\right]}
\newcommand{\Var}[1]{\operatorname{Var}\left(#1\right)}
\newcommand{\trace}[1]{\operatorname{tr}\left(#1\right)}
\newcommand{\R}{\mathbb{R}}


\title{QuIP: 2-Bit Quantization of \\ Large Language Models With Guarantees}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  Jerry Chee \\
  % Department of Computer Science \\
  Cornell University \\
  \texttt{jerrychee@cs.cornell.edu} \\
  \And
  Yaohui Cai \\
  % Department of  Electrical and\\  Computer Engineering \\
  Cornell University \\
  \texttt{yc2632@cornell.edu} \\
  \AND
  Volodymyr Kuleshov \\
  % Department of Computer Science \\
  Cornell University \\
  \texttt{kuleshov@cornell.edu} \\
  \And
  Christopher    De Sa \\
  % Department of Computer Science \\
  Cornell University \\
  \texttt{cdesa@cs.cornell.edu} \\
  % David S.~Hippocampus\thanks{Use footnote for providing further information
  %   about author (webpage, alternative address)---\emph{not} for acknowledging
  %   funding agencies.} \\
  % Department of Computer Science\\
  % Cranberry-Lemon University\\
  % Pittsburgh, PA 15213 \\
  % \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}



\begin{document}


\maketitle

\begin{abstract}
This work studies post-training parameter quantization in large language models (LLMs). 
We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from {\em incoherent} weight and Hessian matrices, i.e., 
from the weights being even in magnitude
and the directions in which it is important to round them accurately being unaligned with the coordinate axes.
QuIP consists of two steps: 
(1)~an adaptive rounding procedure minimizing a quadratic proxy objective;
(2)~efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices.
%
We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ.
%
Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight.
Our code can be found at \url{https://github.com/Cornell-RelaxML/QuIP}.
\end{abstract}








%% Supplement (Put before proofs) ===========================================================
\begin{toappendix}
%\setcounter{section}{3} % for constitent numbering with main paper submission
\input{sections/supplement}
%\setcounter{section}{0} % for constitent numbering with main paper submission
\end{toappendix}


\section{Introduction}

Large language models (LLMs) have enabled advances in text generation, few-shot learning, reasoning, protein sequence modeling, and other tasks~\citep{brown2020LLMfewshot,workshop2023bloom,meta2022opt}.
The massive size of these models---often reaching into hundreds of billions of parameters---requires sophisticated deployment methods and motivates research into efficient inference algorithms.

This work studies the post-training quantization of LLM parameters as a way to improve their runtime efficiency~\citep{dettmers2022int8,frantar2023gptq,park2023lutgemm,xiao2023smooth,yao2022zero,yuan2023rptq}.
Our key insight is that quantization can be most effective when weight and proxy Hessian matrices are {\em incoherent}---that the weights themselves are even in magnitude,  
and the directions in which it is important to have good rounding accuracy are not too large in any one coordinate.
Intuitively, incoherence can be thought of as a principled form of outlier reduction,
which makes it easier to adaptively round the weights to a finite set of compressed values.
We use this intuition to develop theoretically sound two-bit quantization algorithms that scale to LLM-sized models.

Specifically, we introduce quantization with incoherence processing (QuIP),
a new method motivated by the above insight. QuIP consists of two steps: 
(1)~an adaptive rounding~\cite{nagel2020up} procedure, which minimizes a quadratic proxy objective $\ell(\hat W) = \operatorname{tr}((\hat W - W) H (\hat W - W)^T)$ of the error between the original weights $W$ and the quantized weights $\hat W$ using an estimate of the Hessian $H$;
(2)~efficient pre- and post- processing that ensures that the weight and Hessian matrices are incoherent by multiplying them by a Kronecker product of random orthogonal matrices.
We denote ``incoherence processing'' as both the pre- and post- processing steps of our procedure.
Incoherence processing can be viewed as a form of outlier suppression across the weights and the activation space.

We complement our method with a theoretical analysis---the first for a quantization algorithm that scales to LLM-sized models---which analyzes the role of incoherence and shows that our quantization procedure is optimal within a general class of rounding methods. Interestingly, we find that QuIP without incoherence processing yields a more efficient implementation of an earlier algorithm, OPTQ~\cite{frantar2023gptq}; our paper thus also provides the first theoretical analysis for that method.

Empirically, we find that incoherence processing greatly improves the quantization of large models, especially at higher compression rates, and yields the first LLM quantization method that produces viable results using only two bits per weight. For large LLM sizes (>2B parameters), we observe small gaps between 2-bit and 4-bit compression that further decrease with model size, hinting at the feasibility of accurate 2-bit inference in LLMs.

\textbf{Contributions.}\;
In summary, this paper makes the following contributions: (1) we propose QuIP, a quantization method based on the insight that model parameters should ideally be incoherent; (2) we provide a theoretical analysis for a broad class of adaptive rounding methods that encompass QuIP and OPTQ; (3) we demonstrate that QuIP makes two-bit LLM compression viable for the first time.


\section{Related Work}
\input{sections/related}

\section{Quantization With Incoherence Processing: Adaptive Rounding Step }
\label{secQuIPquant}
\input{sections/quant}

\section{Quantization With Incoherence Processing: Incoherence Processing Step }
\input{sections/process}

\section{Extensions and Further Analyses}
\input{sections/further}

\section{Experiments}\label{secExp}
\input{sections/experiments}

\section{Conclusion}
This paper introduced quantization with incoherence processing (QuIP), an algorithm consisting of (1) an optimal adaptive rounding procedure which minimizes a quadratic proxy of the weight error, and (2) efficient pre- and post-processing to ensure the incoherence of the weight and Hessian matrices by multiplying them by a Kronecker product of random orthogonal matrices.
We showed that QuIP quantization is optimal in a general class of adaptive rounding methods with linear feedback; this theoretical analysis is the first for any quantization algorithm that scales to LLM-sized models. 

Empirically, QuIP achieves the first viable two-bit quantization results for LLMs, especially at large model sizes,
hinting at the feasibility of accurate 2-bit inference in LLMs.

\section*{Acknowledgements and Disclosure of Funding}
This work was partially funded by the National Science Foundation under awards DGE-1922551, CAREER awards 2046760 and 2145577, by the National Institute of Health under award MIRA R35GM151243, and a gift from CISCO.

\newpage

\bibliography{references}
\bibliographystyle{plainnat}

\end{document}