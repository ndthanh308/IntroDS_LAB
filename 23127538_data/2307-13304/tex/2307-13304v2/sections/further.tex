\newcommand{\titleOptqEquiv}{OPTQ is a Special Case of LDLQ}
\subsection{\titleOptqEquiv}\label{secOPTQequiv}
\begin{toappendix}
\subsection*{Subsection~\ref{secOPTQequiv} (\titleOptqEquiv)}
\end{toappendix}
We prove a novel theoretical insight: QuIP without incoherence processing (i.e.,~LDLQ) is equivalent to a more efficient version of the OPTQ algorithm. % in Algorithm~\ref{algQuIPquant}.
That is, OPTQ falls under our class of adaptive rounding procedures with linear feedback, and is within-class optimal. 

\begin{theoremrep}
OTPQ~\citep{frantar2023gptq} falls within the class of adaptive rounding procedures with linear feedback as described by Eq.~\eqref{eqnVectorQuant}, and is equivalent to LDLQ in Section~\ref{secQuIPquant}.
\end{theoremrep}
\begin{proof}
OPTQ works in the following way.
After OPTQ has quantized the first $t-1$ components of the row vector $w$, it minimizes the proxy loss over the remaining $n-t+1$ elements, keeping the first $t-1$ elements fixed.
It then quantizes the $t$th element using nearest rounding to the grid and clamping.
It then proceeds to the next column.
If we let $\Delta = \hat w - w$, this proxy loss that it minimizes can be written in block form as
$$
\ell = \Delta_{1:(t-1)} H_{1:(t-1),1:(t-1)} \Delta_{1:(t-1)}^T + 2 \Delta_{1:(t-1)} H_{1:(t-1),t:n} + \Delta_{t:n} H_{t:n,t:n} \Delta_{t:n}^T
$$
and its minimum over $\Delta_{t:n}$ will occur when
$$
0 = \Delta_{1:(t-1)} H_{1:(t-1),t:n} + \Delta_{t:n} H_{t:n,t:n},
$$
i.e.
$$
\Delta_{t:n} = -\Delta_{1:(t-1)} H_{1:(t-1),t:n} \left( H_{t:n,t:n} \right)^{-1}.
$$
Now, suppose that $H = \tilde U D \tilde U^T$ is the LDL decomposition of $H$, where $\tilde U$ is unit upper triangular and $D$ is diagonal. Since $\tilde U$ is upper triangular,
$$
H_{t:n,t:n} = \tilde U_{t:n,t:n} D_{t:n,t:n} \tilde U_{t:n,t:n}^T.
$$
Similarly,
$$
H_{1:(t-1),t:n} = \tilde U_{1:(t-1),t:n} D_{t:n,t:n} \tilde U_{t:n,t:n}^T.
$$
This means that
$$
\Delta_{t:n} = -\Delta_{1:(t-1)} \tilde U_{1:(t-1),t:n} \left( \tilde U_{t:n,t:n} \right)^{-1}.
$$
Now, the only part of the value of $\Delta_{t:n}$ which matters is the first entry, since this is the one that's going to be used to make the next quantization decision. But since $\tilde U_{t:n,t:n}$ is unit upper triangular and so is its inverse, $\left( \tilde U_{t:n,t:n} \right)^{-1} e_t = e_t$, and so
$$
\Delta_t = \Delta_{t:n} e_1 = -\Delta_{1:(t-1)} \tilde U_{1:(t-1),t:n} e_t = -\Delta_{1:(t-1)} \tilde U_{1:(t-1),t} = -\Delta (\tilde U - I) e_t.
$$
Finally, we quantize the $t$-th weight as
\[
    \hat w_t
    =
    \round( w_t - (\hat W - W) (\tilde U - I) e_t ).
\]
This update is equivalent to our adaptive rounding with linear feedback procedure in Eq.~\eqref{eqnVectorQuant}, with $U$ assigned from the LDL decomposition of $H$.
\end{proof}

\textbf{Remarks.}
To the best of our knowledge, this equivalence yields the first theoretical analysis of OPTQ.
Even though the two methods are equivalent, LDLQ is more efficient.
OPTQ's implementation requires a matrix inversion of $H$, and two Cholesky decompositions.
Our implementation of LDLQ performs no matrix inversion, and only one Cholesky decomposition.

\textbf{Empirical Verification.}
The quantized outputs of the OPTQ implementation~\cite{frantar2023gptq} are shown to be exactly identical to the outputs of our LDLQ implementation.
Synthetic random data was used, with $W \sim \operatorname{Unif}[0,1]^{1000 \times 1000}$.
Full details can be found in Supplement~\ref{suppAddResults}.

% \newpage
\newcommand{\titleFiniteGrid}{A Bound for Rounding to a Finite Grid}
\subsection{\titleFiniteGrid}\label{secFiniteGrid}
\begin{toappendix}
\subsection*{Subsection~\ref{secFiniteGrid} (\titleFiniteGrid)}
\end{toappendix}

\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{-4em}
  \begin{center}
  % Figure removed
  \end{center}
\vspace{-1em}
  \caption{LDLQ underperforms.}
  \label{figLDLQbad}
\vspace{-1em}
\end{wrapfigure}


In Section~\ref{secQuIPquant}, we saw that LDLQ (equivalently, OPTQ) is optimal for minimizing the adaptive rounding objective. However, this analysis assumed rounding to the integers. In practice, we do not want to round $W$ just to the integers, but instead to scale it, shift it, and round it a finite subset corresponding to a $b$-bit integer. To do this, the ``real'' LDLQ algorithm uses a clamp operation to restrict the range of quantized values.  Is LDLQ still optimal when this small change is made? It turns out that the answer is \emph{no}, as the following concrete example illustrates.


\textbf{Finite Grid Counterexample.}
Figure~\ref{figLDLQbad} illustrates the behavior of LDLQ and other rounding methods---when restricted via clamping to a finite 4-bit grid $[0,15]$---on a particular example where $H$ is a (cleverly chosen) small perturbation of $(I_n + \mathbf{1}_{n \times n} - e_n e_n^T)/n$, and $W$ has $m = 16$ and is a small perturbation of $\mathbf{1}_{m \times n} / 2$. Details of the setup appear in Supplement~\ref{suppAddResults}. The figure shows that clamped LDLQ with nearest rounding is asymptotically worse, and the clamping to the finite grid is what causes it to be worse in this case.
 
Note that in our experiments in practice, OPTQ has been shown to soundly beat nearest rounding.
This clamping issue does not seem to arise in practice; however, since it is \emph{possible} we do need to take it into account to prove useful end-to-end bounds.

\textbf{A Procedure With a Bound.} In order to address the above issues in theory, here we describe a method that acts to restrict the value of $| \hat W_{ij} - W_{ij} |$, so that the rounded weights will remain inside the grid if $W$ is sufficiently far inside. We do this via the optimization problem with hyperparameter $c$
\begin{align}
    \mbox{minimize: } & \trace{H R^T R} \nonumber\\
    \mbox{over: } & R \text{ unit upper triangular} \label{eqnADMMobj}\\
    \mbox{subject to: } & e_i^T R^T R e_i \le 1 + c, \; \forall i \in \{1,\ldots,n\}.\nonumber
\end{align}

\begin{toappendix}
Algorithm~\ref{alg:qcvx} presents a quantization procedure which theoretically address OPTQ's clamping issue, by incorporating a restriction of $|\hat W_{ij} - W_{ij}|$ into objective~\eqref{eqnADMMobj}.
Note that for simplicity, here we present the explicit case where only two factors are used in each Kronecker product of orthogonal matrices; however, the proof should generalize to any number of factors.
\begin{algorithm}
\caption{``Fixed'' Rounding via a Convex Program}\label{alg:qcvx}
\begin{algorithmic}
\Require $W \in \R^{m \times n}$, $H \in \R^{n \times n}$, $c > 0$, $\rho > 0$
\Require factorization $m = p_1 p_2$, $n = p_3 p_4$
\State \textbf{draw } $U_1 \in \mathbb{R}^{p_1 \times p_1}$ uniformly from the set of orthogonal matrices using seed $\mathsf{seed}(U_1)$
\State \textbf{draw } $U_2 \in \mathbb{R}^{p_2 \times p_2}$ uniformly from the set of orthogonal matrices using seed $\mathsf{seed}(U_2)$
\State \textbf{draw } $U_3 \in \mathbb{R}^{p_3 \times p_3}$ uniformly from the set of orthogonal matrices using seed $\mathsf{seed}(U_3)$
\State \textbf{draw } $U_4 \in \mathbb{R}^{p_4 \times p_4}$ uniformly from the set of orthogonal matrices using seed $\mathsf{seed}(U_4)$
\State $W \gets (U_1 \otimes U_2) W (U_3 \otimes U_4)$
\State $H \gets (U_3^T \otimes U_4^T) H (U_3 \otimes U_4)$
\State $W \gets \frac{2^b - 1}{2} \left( \frac{W}{\rho} + 1 \right)$ elementwise
\State $W \gets \operatorname{clamp}(W, \min=0, \max=2^{b}-1))$ elementwise
\State use ADMM or some other solver to solve
\begin{align*}
    \mbox{minimize: } & \trace{H L^T L} \\
    \mbox{over: } & L \text{ unit upper triangular} \\
    \mbox{subject to: } & e_i^T L^T L e_i \le 1 + c, \; \forall i \in \{1,\ldots,n\}.
\end{align*}
\State note that when $c = \infty$, $L^{-1}$ is the factor from the LDL decomposition of $H$
\State $\grave{U} \gets L^{-1} - I$
\State \textbf{for} $k \in \{1, \dots, n\}$ \textbf{do} 
$\hat W_k \gets \operatorname{clamp}(\round( W_k + (W - \hat W) \grave{U}_k ), 0, 2^b-1)$ \Comment{round with LF}
\State $\hat W \gets \rho \left( \frac{2 \hat W}{2^b - 1} - 1 \right)$
\State $\hat W \gets (U_1^T \otimes U_2^T) \hat W (U_3^T \otimes U_4^T)$
\State \Return $\hat W$ encoded as a tuple of the integer rounded values, the scale factor $\rho$, and the seeds
\end{algorithmic}
\end{algorithm}


\begin{lemma}
Suppose that for positive definite $\mu$-incoherent matrix $H \in \mathbb{R}^{n \times n}$ and scalar $c > 0$, $L$ is the solution to the optimization problem
\begin{align*}
    \mbox{minimize: } & \trace{H L^T L} \\
    \mbox{over: } & L \text{ unit upper triangular} \\
    \mbox{subject to: } & e_i^T L^T L e_i \le 1 + c, \; \forall i \in \{1,\ldots,n\}.
\end{align*}
Then the solution satisfies
\[
	\trace{H L^T L} = \frac{\mu^2}{n \cdot \min(1,c)} \trace{H^{1/2}}^2.
\]
\end{lemma}
\begin{proof}
Let $\eta \in \mathbb{R}^{1 \times n}$ be a random standard Gaussian variable as a row vector, let $A$ be a matrix, and consider the recurrence relation over $x_t \in \mathbb{R}^{1 \times n}$ given by $x_0 = 0$ and
\[
	x_t = x_{t-1} - x_{t-1} A e_i e_i^T + \eta e_i e_i^T 
\]
We first note that since $x_t$ is supported only on $\{1,\ldots,t\}$, if $M$ denotes the strictly upper triangular mask, this update step is equivalent to
\[
	x_t = x_{t-1} - x_{t-1} (A \odot M) e_i e_i^T + \eta e_i e_i^T.
\]
From here, it's fairly easy to see by induction that
\[
	x_n = -x_n (A \odot M) + \eta,
\]
and so
\[
	x_n (I + A \odot M) = \eta,
\]
or
\[
	x_n = \eta (I + A \odot M)^{-1}.
\]
Now, since $I + A \odot M$ is a unit upper triangular matrix, its inverse is also a unit upper triangular matrix. If we let $L = (I + A \odot M)^{-1}$, then $L$ is a unit upper triangular matrix and
\[
	\Exv{x_n^T x_n} = L^T L.
\]
We are going to choose $A$ such that $L$ is a feasible solution to our optimization problem and has the desired objective.
Next, let $\Sigma_t = \Exv{x_t^T x_t}$, and observe that
\[
	\Sigma_t = \left( I - A e_i e_i^T \right)^T \Sigma_{t-1} \left( I - A e_i e_i^T \right) + e_i e_i^T.
\]
Let $\alpha > 0$ be some constant to be set later, and set $A = \alpha H^{1/2}$. Suppose by way of induction that for some constant $\beta > 0$ to be set later, $\Sigma_t \preceq \beta H^{-1/2}$.
The base case clearly holds since $\Sigma_0 = 0$. For the inductive step,
\begin{align*}
	\Sigma_t 
	&\preceq 
	\beta \left( I - \alpha H^{1/2} e_i e_i^T \right)^T H^{-1/2} \left( I - \alpha H^{1/2} e_i e_i^T \right) + e_i e_i^T
	\\&= 
	\beta H^{-1/2} - 2 \alpha \beta e_i e_i^T + \alpha^2 \beta e_i e_i^T H^{1/2} e_i e_i^T + e_i e_i^T.
\end{align*}
This inductive step will hold if, letting $h = \max_i e_i^T H^{1/2} e_i$,
\[
	2 \alpha \beta \ge 1 + \alpha^2 \beta h
\]
On the other hand,
\begin{align*}
	e_i^T L^T L e_i 
	&= \Exv{(x_n e_i)^2} 
	\\&=  \Exv{\left( -x_{i-1} A e_i + \eta e_i \right)^2}
	\\&=  \Exv{\left( -x_{i-1} A e_i \right)^2} + 1
	\\&= e_i^T A^T \Sigma_{i-1} A e_i + 1
	\\&= \alpha^2 e_i^T H^{1/2} \Sigma_{i-1} H^{1/2} e_i + 1
	\\&\le \alpha^2 \beta e_i^T H^{1/2} H^{-1/2} H^{1/2} e_i + 1
	\\&\le \alpha^2 \beta e_i^T H^{1/2} e_i + 1.
\end{align*}
So the constraint of our optimization problem will be satisfied if
\[
	\alpha^2 \beta h \le c.
\]
To satisfy these constraints, set $\beta = \max(h, h/c)$ and $\alpha = \beta^{-1}$. Then
\[
	2 \max(h, h/c)^{-1} \cdot \max(h, h/c) \ge 1 + \max(h, h/c)^{-2} \cdot \max(h, h/c) \cdot h,
\]
and
\[
	\max(h, h/c)^{-2} \cdot \max(h, h/c) \cdot h \le c.
\]
Also, the objective will be bounded by
\[
	\trace{H L^T L} = \trace{H \Sigma_n} \le \beta \trace{H^{1/2}} = \max(1, c^{-1}) \cdot h \cdot \trace{H^{1/2}}.
\]
Now, applying incoherence to bound $h$, where $H = U \Lambda U^T$ is the eigendecomposition of $H$,
\[
	e_i^T H^{1/2} e_i 
	= 
	\sum_{j=1}^n \lambda_j^{1/2} (e_i^T U e_j)^2
	\le
	\sum_{j=1}^n \lambda_j^{1/2} \frac{\mu^2}{n}
	=
	\frac{\mu^2}{n} \trace{H^{1/2}}.
\]
So this yields a whole bound of
\[
	\trace{H L^T L} = \frac{\mu^2}{n \cdot \min(1,c)} \trace{H^{1/2}}^2.
\]
This is what we wanted to show.
\end{proof}

\begin{lemma}
Suppose that we quantize the row vector $w \in \mathbb{R}^{1 \times n}$ using $L$ the solution to the optimization problem
\begin{align*}
    \mbox{minimize: } & \trace{H L^T L} \\
    \mbox{over: } & L \text{ unit upper triangular} \\
    \mbox{subject to: } & e_i^T L^T L e_i \le 1 + c, \; \forall i \in \{1,\ldots,n\}
\end{align*}
and
\[
	\hat w = \mathcal{Q}_{\operatorname{stoch}}\left( w - (\hat w - w) (L^{-1} - I) \right),
\]
where $\mathcal{Q}_{\operatorname{stoch}}$ denotes elementwise unbiased stochastic rounding. Then for any $u \in \mathbb{R}^n$ and any $\delta > 0$
\[
	\mathbf{P}\left( \Abs{ (\hat w - w) u } \ge \norm{L u} \sqrt{\frac{1}{2} \log\left( \frac{2}{\delta} \right) } \right) \le \delta.
\]
In particular,
\[
	\mathbf{P}\left( \Abs{ (\hat w - w) (L^{-1} - I) e_i } \ge \sqrt{\frac{c}{2} \log\left( \frac{2}{\delta} \right) } \right) \le \delta.
\]
\end{lemma}
\begin{proof}
Let $\eta$ be the error of stochastic rounding, and observe that each entry is, conditioned on earlier steps, zero mean and supported on two values that differ by $1$.
Also observe that
\[
	\hat w = \left( w - (\hat w - w) (L^{-1} - I) \right) + \eta,
\]
and so
\[
	\hat w - w = \eta L
\]
and
\[
	\Exv{\exp\left( (\hat w - w) u \right)} = \Exv{\exp\left( \eta L u \right)}.
\]
From a repeated application of Hoeffding's lemma, we get
\[
	\Exv{\exp\left( (\hat w - w) u \right)} \le \exp\left( \frac{1}{8} \norm{ L u }^2 \right).
\]
Setting $u \mapsto \gamma u$ for $\gamma > 0$,
\[
	\Exv{\exp\left( \gamma (\hat w - w) u \right)} \le \exp\left( \frac{\gamma^2}{8} \norm{ L u }^2 \right).
\]
And by Markov's inequality,
\[
	\mathbf{P}\left( \exp\left( \gamma (\hat w - w) u \right) \ge \exp(\gamma R) \right) \le \exp(-\gamma R) \exp\left( \frac{\gamma^2}{8} \norm{ L u }^2 \right),
\]
i.e.
\[
	\mathbf{P}\left( (\hat w - w) u \ge R \right) \le \exp\left( -\gamma R + \frac{\gamma^2}{8} \norm{ L u }^2 \right).
\]
Minimizing the right side over $\gamma$ yields $\gamma = 4 R \norm{L u}^{-2}$ and
\[
	\mathbf{P}\left( (\hat w - w) u \ge R \right) \le \exp\left( -2 R^2 \norm{L u}^{-2} \right).
\]
By a union bound,
\[
	\mathbf{P}\left( \Abs{ (\hat w - w) u } \ge R \right) \le 2 \exp\left( -2 R^2 \norm{L u}^{-2} \right).
\]
Now setting the right side equal to $\delta$,
\[
	\mathbf{P}\left( \Abs{ (\hat w - w) u } \ge \norm{L u} \sqrt{\frac{1}{2} \log\left( \frac{2}{\delta} \right) } \right) \le \delta.
\]
This is what we wanted to show. The second statement follows from the fact that
\[
	 \norm{L (L^{-1} - I) e_i}^2
	 =
	 \norm{e_i - L e_i}^2
	 =
	e_i^T e_i - e_i^T L e_i - e_i^T L^T e_i + e_i^T L^T L e_i
	\le
	1 - 1 - 1 + (1 + c)
	=
	c.
\]
\end{proof}

\begin{lemma}
Suppose that we quantize the row vector $w \in \mathbb{R}^{1 \times n}$ using $L$ the solution to the optimization problem
\begin{align*}
    \mbox{minimize: } & \trace{H L^T L} \\
    \mbox{over: } & L \text{ unit upper triangular} \\
    \mbox{subject to: } & e_i^T L^T L e_i \le 1 + c, \; \forall i \in \{1,\ldots,n\}
\end{align*}
and
\[
	\hat w = \mathcal{Q}_{\operatorname{stoch}}\left( w - (\hat w - w) (L^{-1} - I) \right),
\]
where $\mathcal{Q}_{\operatorname{stoch}}$ denotes elementwise unbiased stochastic rounding.
Suppose that for some integer $b$, $1 \le w_{ij} \le 2^{b} - 2$.
Then if we set
\[
	c = 2 \left( \log\left( \frac{4 m n}{\delta} \right) \right)^{-1},
\]
then with probability at least $1 - \delta$, $0 \le \hat w_{ij} \le 2^{b} - 1$ and
\[
	\trace{ (\hat w - w) H (\hat w - w)^T } \le \frac{\mu^2 m}{4n} \trace{H^{1/2}}^2 \left( \log\left( \frac{4 mn}{\delta}\right)^2 \right).
\]
\end{lemma}
\begin{proof}
First, from the previous lemmas, if $U e_i$ is the $i$th eigenvector of $H$, with eigenvalue $\lambda_i$ since
\[
	\mathbf{P}\left( \lambda_i ( e_j^T (\hat w - w) U e_i )^2 \ge \lambda_i \norm{L U e_i}^2 \cdot \frac{1}{2} \log\left( \frac{2}{\delta} \right) \right) \le \delta.
\]
By the union bound,
\[
	\mathbf{P}\left( \exists i,j, \; \lambda_i ( e_j^T (\hat w - w) U e_i )^2 \ge \lambda_i \norm{L U e_i}^2 \cdot \frac{1}{2} \log\left( \frac{2 mn}{\delta} \right) \right) \le \delta.
\]
And so
\[
	\mathbf{P}\left( \sum_{i,j} \lambda_i ( e_j^T (\hat w - w) U e_i )^2 \ge \sum_{i,j} \lambda_i \norm{L U e_i}^2 \cdot \frac{1}{2} \log\left( \frac{2 mn}{\delta} \right) \right) \le \delta,
\]
which simplifies to
\[
	\mathbf{P}\left( \trace{ (\hat w - w) H (\hat w - w)^T } \ge m \trace{H L^T L} \cdot \frac{1}{2} \log\left( \frac{2 mn}{\delta} \right) \right) \le \delta.
\]
Now applying the other lemma,
\[
	\mathbf{P}\left( \trace{ (\hat w - w) H (\hat w - w)^T } \ge \frac{\mu^2 m}{2n \cdot \min(1,c)} \trace{H^{1/2}}^2 \log\left( \frac{2 mn}{\delta} \right) \right) \le \delta.
\]
And substituting $\delta \mapsto \delta/2$,
\[
	\mathbf{P}\left( \trace{ (\hat w - w) H (\hat w - w)^T } \ge \frac{\mu^2 m}{2n \cdot \min(1,c)} \trace{H^{1/2}}^2 \log\left( \frac{4 mn}{\delta} \right) \right) \le \frac{\delta}{2}.
\]
On the other hand, again by a union bound from the previous lemma,
\[
	\mathbf{P}\left( \exists i,j, \; \Abs{ e_j^T (\hat w - w) (L^{-1} - I) e_i } \ge \sqrt{\frac{c}{2} \log\left( \frac{4 m n}{\delta} \right) } \right) \le \frac{\delta}{2}.
\]
Setting
\[
	c = 2 \left( \log\left( \frac{4 m n}{\delta} \right) \right)^{-1}
\]
yields
\[
	\mathbf{P}\left( \exists i,j, \; \Abs{ e_j^T (\hat w - w) (L^{-1} - I) e_i } \ge 1 \right) \le \frac{\delta}{2}.
\]
And so by another union bound, the probability that
\[
	\trace{ (\hat w - w) H (\hat w - w)^T } \le \frac{\mu^2 m}{4n} \trace{H^{1/2}}^2 \left( \log\left( \frac{4 mn}{\delta}\right) \right)^2
\]
and
\[
	\max_{i,j} \; \Abs{ e_j^T (\hat w - w) (L^{-1} - I) e_i } \le 1
\]
is no less than $1 - \delta$. It's clear that if this second inequality holds, the value we pass in to the stochastic quantizer will be in range, and thus so will the output. This proves what we want.
\end{proof}

\begin{theorem}
Suppose that we are given an input matrix $w$ with bounded maximum entry magnitude $\norm{w}_{\infty}$ and we want to quantize it using $b$ bits. Suppose that we first re-scale the entries of $w$ by mapping
\[
	w_{ij} \mapsto \frac{2^b - 3}{2} \left( \frac{w_{ij}}{\norm{w}_{\infty}} + 1 \right) + 1;
\]
this guarantees that $1 \le w_{ij} \le 2^b - 2$. Then, suppose we quantize using the procedure described in the previous lemma. Finally, we undo the scaling. Then then with probability at least $1 - \delta$, all the quantized weights will be in range (no overflow or need for clipping) and
\[
	\trace{ (\hat w - w) H (\hat w - w)^T } \le \frac{\mu^2 m}{n (2^b - 3)^2} \trace{H^{1/2}}^2 \norm{w}_{\infty}^2 \left( \log\left( \frac{4 mn}{\delta}\right)^2 \right).
\]
\end{theorem}
\begin{proof}
This is a straightforward consequence of the previous lemma.
\end{proof}

\begin{theorem}
Suppose that we are given an input matrix $w$ with bounded $\norm{w}_{F}$ and we want to quantize it using $b$ bits. Suppose that we first multiply by two-factor orthogonal matrices, and then we re-scale the entries of $w$ by mapping
\[
	w_{ij} \mapsto \frac{2^b - 3}{2} \left( \frac{w_{ij}}{\norm{w}_F \sqrt{ \frac{A^2}{mn} \log\left( \frac{2C m n}{\delta} \right)^2 }} + 1 \right) + 1;
\]
this guarantees that $1 \le w_{ij} \le 2^b - 2$. Then, suppose we quantize using the procedure described in the previous lemma. Finally, we undo the scaling and multiplication. Then then with probability at least $1 - \delta$, all the quantized weights will be in range (no overflow or need for clipping) and
\begin{align*}
	\trace{ (\hat w - w) H (\hat w - w)^T } &\le \frac{A^4}{n^2 (2^b - 3)^2} \trace{H^{1/2}}^2 \norm{w}_{F}^2 \left( \log\left( \frac{12 C m n^2}{\delta}\right) \right)^6
	\\&=
	\tilde{\mathcal{O}}\left( \frac{1}{n^2 4^b} \trace{H^{1/2}}^2 \norm{w}_{F}^2 \right).
\end{align*}
\end{theorem}
\begin{proof}
It is a straightforward consequence of Lemma~\ref{lemFastInco}, that unioning over the three bounds on the infinity norm of $w$, the incoherence of $H$, and the stochastic rounding, with probability at least $1 - 3 \delta$,
\begin{align*}
\trace{ (\hat w - w) H (\hat w - w)^T } &\le \frac{m}{n (2^b - 3)^2} \trace{H^{1/2}}^2 \norm{w}_{F}^2 \left( \log\left( \frac{4 mn}{\delta}\right)^2 \right) \\&\hspace{4em}\cdot A^2 \log\left(\frac{2 C n^2}{\delta}\right)^2 \cdot  \frac{A^2}{mn} \log\left( \frac{2 C n}{\delta} \right)^2.
\end{align*}
Substituting $\delta \mapsto \delta/3$, 
\begin{align*}
	\trace{ (\hat w - w) H (\hat w - w)^T } &\le \frac{1}{n (2^b - 3)^2} \trace{H^{1/2}}^2 \norm{w}_{F}^2 \left( \log\left( \frac{12 mn}{\delta}\right)^2 \right) \\&\hspace{4em}\cdot A^2 \log\left(\frac{6 C n^2}{\delta}\right)^2 \cdot  \frac{A^2}{n} \log\left( \frac{6 C n}{\delta} \right)^2.
\end{align*}
And this right side is clearly less than
\[
	\trace{ (\hat w - w) H (\hat w - w)^T } \le \frac{A^4}{n^2 (2^b - 3)^2} \trace{H^{1/2}}^2 \norm{w}_{F}^2 \left( \log\left( \frac{12 C m n^2}{\delta}\right) \right)^6.
\]
This is what we wanted to show.
\end{proof}


\end{toappendix}

Our ``fixed'' algorithm solves this convex problem (e.g. with ADMM), then runs QuIP using stochastic rounding and $U = R^{-1} - I$ in place of the LDL decomposition. Observe that for sufficiently large $c$, this is exactly equivalent to base QuIP, since the solution of that optimization problem is given by the LDL decomposition when the constraint is dropped. Doing this (the full algorithm is given in the supplemental) yields the following theorem.

\begin{theoremrep}
Suppose that we run Algorithm~\ref{alg:qcvx} (Supplement) to quantize a matrix $W \in \mathbb{R}^{m \times n}$ by solving the objective~\eqref{eqnADMMobj}. Then there exists an assignment of the algorithm's hyperparameters $c$ and $\rho$ such that with probability at least $1 - \delta$, all the quantized weights will be in range (no overflow or need for clipping) and
\[
	\trace{ (\hat W - W) H (\hat W - W)^T }
	=
	\tilde{\mathcal{O}}\left( \frac{1}{n^2 4^b} \trace{H^{1/2}}^2 \norm{W}_{F}^2 \right).
\]
\end{theoremrep}
\begin{proof}
This follows directly from the previous theorem, which says explicitly what the hyperparameter assignments should be.
\end{proof}

In practice, because clamping rarely causes issues, and because of the significant additional compute needed to solve this program, we always just use QuIP as described in the previous sections, which is equivalent to setting $c$ large and using nearest rounding.


