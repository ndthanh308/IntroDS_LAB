@inproceedings{frantar2023gptq,
  title={OPTQ: Accurate Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{nagel2020up,
  title={Up or down? adaptive rounding for post-training quantization},
  author={Nagel, Markus and Amjad, Rana Ali and Van Baalen, Mart and Louizos, Christos and Blankevoort, Tijmen},
  booktitle={International Conference on Machine Learning},
  pages={7197--7206},
  year={2020},
  organization={PMLR}
}

@article{xiao2023smooth,
  title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  journal={arXiv preprint arXiv:2211.10438},
  year={2023}
}

@inproceedings{yao2022zero,
  title={ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers},
  author={Yao, Zhewei and Aminabadi, Reza Yazdani and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{wei2022outlier,
  title={Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models},
  author={Wei, Xiuying and Zhang, Yunchen and Zhang, Xiangguo and Gong, Ruihao and Zhang, Shanghang and Zhang, Qi and Yu, Fengwei and Liu, Xianglong},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{dettmers2022int8,
  title={LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}

@article{park2023lutgemm,
  title={LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models},
  author={Park, Gunho and Park, Baeseong and Kim, Minsub and Lee, Sungjae and Kim, Jeonghoon and Kwon, Beomseok and Kwon, Se Jung and Kim, Byeongwook and Lee, Youngjoo and Lee, Dongsoo},
  journal={arXiv preprint arXiv:2206.09557},
  year={2023}
}

@article{yuan2023rptq,
  title={RPTQ: Reorder-based Post-training Quantization for Large Language Models},
  author={Yuan, Zhihang and Niu, Lin and Liu, Jiawei and Liu, Wenyu and Wang, Xinggang and Shang, Luzhang and Sun, Guangyu and Wu, Qiang and Wu, Jiaxiang and Wu, Bingzhe},
  journal={arXiv preprint arXiv:2304.01089},
  year={2023}
}

@inproceedings{lybrand2021greedy,
  title={A Greedy Algorithm for Quantizing Neural Networks},
  author={Lybrand, Eric and Saab, Rayan},
  booktitle={Journal of Machine Learning Research},
  year={2021}
}

@inproceedings{frantar2022obc,
  title={Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning},
  author={Frantar, Elias and Sing, Sidak Pal and Alistarh, Dan},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{yao2021hawq3,
  title={HAWQ-V3: Dyadic Neural Network Quantization},
  author={Yao, Zhewei and Dong, Zhen and Zheng, Zhangcheng and Gholami, Amir and Yu, Jiali and Tan, Eric and Wang, Leyuan and Huang, Qijing and Wang, Yida and Mahoney, Michael W. and Keutzer, Kurt},
  booktitle={International Conference on Machine Learning},
  year={2021},
  organization={PMLR}
}

@inproceedings{dong2020hawq2,
  title={HAWQ-V2: Hessian Aware trace-Weighted Quantization of Neural Networks},
  author={Dong, Zhen and Yao, Zhewei and Arfeen, Daiyaan and Gholami, Amir and Mahoney, Michael W. and Keutzer, Kurt},
  booktitle={Conference on Neural Information Processing Systems},
  year={2020}
}

@inproceedings{dong2019hawq1,
  title={HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision},
  author={Dong, Zhen and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W. and Keutzer, Kurt},
  booktitle={International Conference on Computer Vision},
  year={2019}
}

@inproceedings{wang2020bit,
  title={Towards Accurate Post-training Network Quantization via Bit-Split and Stitching},
  author={Wang, Peisong and Chen, Qiang and He, Xiangyu and Cheng, Jian},
  booktitle={International Conference on Machine Learning},
  year={2020},
  organization={PMLR}
}

@inproceedings{li2021brecq,
  title={BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction},
  author={Li, Yuhang and Gong, Ruihao and Tan, Xu and Yang, Yang and Hu, Peng and Zhang, Qi and Yu, Fengwei and Wang, Wei and Gu, Shi},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{nagel2019dfq,
  title={Data-Free Quantization Through Weight Equalization and Bias Correction},
  author={Nagel, Markus and van Baalen, Mart and Blankevoort, Tijmen and Welling, Max},
  booktitle={International Conference on Computer Vision},
  year={2019}
}

@inproceedings{hubara2021adaq,
  title={Accurate Post Training Quantization With Small Calibration Sets},
  author={Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  year={2021},
  organization={PMLR}
}

@inproceedings{jeon2022biq,
  title={Mr.BiQ: Post-Training Non-Uniform Quantization based on Minimizing the Reconstruction Error},
  author={Jeon, Yongkweon and Lee, Chungman and Cho, Eulrang and Ro, Yeonju},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2022},
}

@inproceedings{liu2023noisy,
  title={NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers},
  author={Liu, Yijian and Yang, Huanrui and Dong, Zhen and Keutzer, Kurt and Du, Li and Zhang, Shanghang},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  year={2023},
}

@inproceedings{liu2021ptqvit,
  title={Post-Training Quantization for Vision Transformer},
  author={Liu, Zhenhua and Wang, Yunhe and Han, Kai and Zhang, Wei and Ma, Siwei and Gao, Wen},
  booktitle={Conference on Neural Information Processing Systems},
  year={2021}
}

@inproceedings{li2022qvit,
  title={Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer},
  author={Li, Yanjing and Xu, Sheng and Zhang, Baochang and Cao, Xianbin and Gao, Peng and Guo, Guodong},
  booktitle={Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{yuan2022ptqvit,
  title={PTQ4ViT: Post-Training Quantization for Vision Transformers with Twin Uniform Quantization},
  author={Yuan, Zhihang and Xue, Chenhao and Chen, Yiqi and Wu, Qiang and Sun, Guangyu},
  booktitle={European Conference on Computer Vision},
  year={2022}
}

@inproceedings{desa2015matrix,
  title={Global Convergence of Stochastic Gradient Descent for Some Non-convex Matrix Problems},
  author={De Sa, Christopher and Olukotun, Kunle and R\'e, Christopher},
  booktitle={International Conference on Machine Learning},
  year={2015},
  organization={PMLR}
}

@inproceedings{jain2013complete,
  title={Low-rank matrix completion using alternating minimization},
  author={Prateek, Jain and Praneeth, Netrapalli and Sujay, Sanghavi},
  booktitle={Proceedings of the Forty-fifth Annual ACM STOC},
  year={2013}
}

@misc{meta2022opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{paszke2019pytorch,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      booktitle={Conference on Neural Information Processing Systems},
      year={2019},
}


@misc{workshop2023bloom,
      title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}, 
      author={BigScience Workshop and : and Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ilić and Daniel Hesslow and Roman Castagné and Alexandra Sasha Luccioni and François Yvon et. al.
      },
      year={2023},
      eprint={2211.05100},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{raffel2020c4,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@article{merity2016wiki,
  title={Pointer Sentinel Mixture Models}, 
  author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@inproceedings{marcus1994ptb,
  title={},
  author={De Sa, Christopher and Olukotun, Kunle and R\'e, Christopher},
  booktitle={International Conference on Machine Learning},
  year={2015},
  organization={PMLR}
}

@inproceedings{marcus1994penn,
    title = "The {P}enn {T}reebank: Annotating Predicate Argument Structure",
    author = "Marcus, Mitchell  and
      Kim, Grace  and
      Marcinkiewicz, Mary Ann  and
      MacIntyre, Robert  and
      Bies, Ann  and
      Ferguson, Mark  and
      Katz, Karen  and
      Schasberger, Britta",
    booktitle = "{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",
    year = "1994",
    url = "https://aclanthology.org/H94-1020",
}

@inproceedings{paperno2016lambada,
    title = "The {LAMBADA} dataset: Word prediction requiring a broad discourse context",
    author = "Paperno, Denis  and
      Kruszewski, Germ{\'a}n  and
      Lazaridou, Angeliki  and
      Pham, Ngoc Quan  and
      Bernardi, Raffaella  and
      Pezzelle, Sandro  and
      Baroni, Marco  and
      Boleda, Gemma  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1144",
    doi = "10.18653/v1/P16-1144",
    pages = "1525--1534",
}

@inproceedings{boratko2018arc,
    title = "A Systematic Classification of Knowledge, Reasoning, and Context within the {ARC} Dataset",
    author = "Boratko, Michael  and
      Padigela, Harshit  and
      Mikkilineni, Divyendra  and
      Yuvraj, Pritish  and
      Das, Rajarshi  and
      McCallum, Andrew  and
      Chang, Maria  and
      Fokoue-Nkoutche, Achille  and
      Kapanipathi, Pavan  and
      Mattei, Nicholas  and
      Musa, Ryan  and
      Talamadupula, Kartik  and
      Witbrock, Michael",
    booktitle = "Proceedings of the Workshop on Machine Reading for Question Answering",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-2607",
    doi = "10.18653/v1/W18-2607",
    pages = "60--70",
    abstract = "The recent work of Clark et al. (2018) introduces the AI2 Reasoning Challenge (ARC) and the associated ARC dataset that partitions open domain, complex science questions into easy and challenge sets. That paper includes an analysis of 100 questions with respect to the types of knowledge and reasoning required to answer them; however, it does not include clear definitions of these types, nor does it offer information about the quality of the labels. We propose a comprehensive set of definitions of knowledge and reasoning types necessary for answering the questions in the ARC dataset. Using ten annotators and a sophisticated annotation interface, we analyze the distribution of labels across the challenge set and statistics related to them. Additionally, we demonstrate that although naive information retrieval methods return sentences that are irrelevant to answering the query, sufficient supporting text is often present in the (ARC) corpus. Evaluating with human-selected relevant sentences improves the performance of a neural machine comprehension model by 42 points.",
}

@inproceedings{tat2003piqa,
      title={PiQA: An algebra for querying protein data sets}, 
      author={Tata, Sandeep and Patel, Jignesh M},
      booktitle={International Conference on Scientific and Statistical Database Management},
      year={2003},
}

@inproceedings{2016storycloze,
    title = "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories",
    author = "Mostafazadeh, Nasrin  and
      Chambers, Nathanael  and
      He, Xiaodong  and
      Parikh, Devi  and
      Batra, Dhruv  and
      Vanderwende, Lucy  and
      Kohli, Pushmeet  and
      Allen, James",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1098",
    doi = "10.18653/v1/N16-1098",
    pages = "839--849",
}

@inproceedings{brown2020LLMfewshot,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and et. al.},
 booktitle = {Conference on Neural Information Processing Systems},
 title = {Language Models are Few-Shot Learners},
 year = {2020}
}

@misc{lalley2018prob,
  author        = {Lalley, Steve},
  title         = {Lecture notes on Measure-Theoretic Probability 2},
  year          = {2018},
  howpublished  = {\url{http://galton.uchicago.edu/~lalley/Courses/383/Concentration.pdf}}
}

@misc{meta2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}