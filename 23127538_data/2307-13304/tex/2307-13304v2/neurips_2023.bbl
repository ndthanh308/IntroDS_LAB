\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Boratko et~al.(2018)Boratko, Padigela, Mikkilineni, Yuvraj, Das, McCallum, Chang, Fokoue-Nkoutche, Kapanipathi, Mattei, Musa, Talamadupula, and Witbrock]{boratko2018arc}
Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi, Nicholas Mattei, Ryan Musa, Kartik Talamadupula, and Michael Witbrock.
\newblock A systematic classification of knowledge, reasoning, and context within the {ARC} dataset.
\newblock In \emph{Proceedings of the Workshop on Machine Reading for Question Answering}, pages 60--70, Melbourne, Australia, July 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W18-2607}.
\newblock URL \url{https://aclanthology.org/W18-2607}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, and et. al.]{brown2020LLMfewshot}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and et. al.
\newblock Language models are few-shot learners.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2020.

\bibitem[De~Sa et~al.(2015)De~Sa, Olukotun, and R\'e]{desa2015matrix}
Christopher De~Sa, Kunle Olukotun, and Christopher R\'e.
\newblock Global convergence of stochastic gradient descent for some non-convex matrix problems.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2015.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and Zettlemoyer]{dettmers2022int8}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Llm.int8(): 8-bit matrix multiplication for transformers at scale.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2022.

\bibitem[Dong et~al.(2019)Dong, Yao, Gholami, Mahoney, and Keutzer]{dong2019hawq1}
Zhen Dong, Zhewei Yao, Amir Gholami, Michael~W. Mahoney, and Kurt Keutzer.
\newblock Hawq: Hessian aware quantization of neural networks with mixed-precision.
\newblock In \emph{International Conference on Computer Vision}, 2019.

\bibitem[Dong et~al.(2020)Dong, Yao, Arfeen, Gholami, Mahoney, and Keutzer]{dong2020hawq2}
Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael~W. Mahoney, and Kurt Keutzer.
\newblock Hawq-v2: Hessian aware trace-weighted quantization of neural networks.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2020.

\bibitem[Frantar et~al.(2022)Frantar, Sing, and Alistarh]{frantar2022obc}
Elias Frantar, Sidak~Pal Sing, and Dan Alistarh.
\newblock Optimal brain compression: A framework for accurate post-training quantization and pruning.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2022.

\bibitem[Frantar et~al.(2023)Frantar, Ashkboos, Hoefler, and Alistarh]{frantar2023gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Optq: Accurate quantization for generative pre-trained transformers.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Hubara et~al.(2021)Hubara, Nahshan, Hanani, Banner, and Soudry]{hubara2021adaq}
Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry.
\newblock Accurate post training quantization with small calibration sets.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2021.

\bibitem[Jeon et~al.(2022)Jeon, Lee, Cho, and Ro]{jeon2022biq}
Yongkweon Jeon, Chungman Lee, Eulrang Cho, and Yeonju Ro.
\newblock Mr.biq: Post-training non-uniform quantization based on minimizing the reconstruction error.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition}, 2022.

\bibitem[Li et~al.(2022)Li, Xu, Zhang, Cao, Gao, and Guo]{li2022qvit}
Yanjing Li, Sheng Xu, Baochang Zhang, Xianbin Cao, Peng Gao, and Guodong Guo.
\newblock Q-vit: Accurate and fully quantized low-bit vision transformer.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2022.

\bibitem[Li et~al.(2021)Li, Gong, Tan, Yang, Hu, Zhang, Yu, Wang, and Gu]{li2021brecq}
Yuhang Li, Ruihao Gong, Xu~Tan, Yang Yang, Peng Hu, Qi~Zhang, Fengwei Yu, Wei Wang, and Shi Gu.
\newblock Brecq: Pushing the limit of post-training quantization by block reconstruction.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Liu et~al.(2023)Liu, Yang, Dong, Keutzer, Du, and Zhang]{liu2023noisy}
Yijian Liu, Huanrui Yang, Zhen Dong, Kurt Keutzer, Li~Du, and Shanghang Zhang.
\newblock Noisyquant: Noisy bias-enhanced post-training activation quantization for vision transformers.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition}, 2023.

\bibitem[Liu et~al.(2021)Liu, Wang, Han, Zhang, Ma, and Gao]{liu2021ptqvit}
Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao.
\newblock Post-training quantization for vision transformer.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2021.

\bibitem[Lybrand and Saab(2021)]{lybrand2021greedy}
Eric Lybrand and Rayan Saab.
\newblock A greedy algorithm for quantizing neural networks.
\newblock In \emph{Journal of Machine Learning Research}, 2021.

\bibitem[Marcus et~al.(1994)Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, and Schasberger]{marcus1994penn}
Mitchell Marcus, Grace Kim, Mary~Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger.
\newblock The {P}enn {T}reebank: Annotating predicate argument structure.
\newblock In \emph{{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994}, 1994.
\newblock URL \url{https://aclanthology.org/H94-1020}.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher]{merity2016wiki}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[Mostafazadeh et~al.(2016)Mostafazadeh, Chambers, He, Parikh, Batra, Vanderwende, Kohli, and Allen]{2016storycloze}
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen.
\newblock A corpus and cloze evaluation for deeper understanding of commonsense stories.
\newblock In \emph{Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 839--849, San Diego, California, June 2016. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N16-1098}.
\newblock URL \url{https://aclanthology.org/N16-1098}.

\bibitem[Nagel et~al.(2019)Nagel, van Baalen, Blankevoort, and Welling]{nagel2019dfq}
Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling.
\newblock Data-free quantization through weight equalization and bias correction.
\newblock In \emph{International Conference on Computer Vision}, 2019.

\bibitem[Nagel et~al.(2020)Nagel, Amjad, Van~Baalen, Louizos, and Blankevoort]{nagel2020up}
Markus Nagel, Rana~Ali Amjad, Mart Van~Baalen, Christos Louizos, and Tijmen Blankevoort.
\newblock Up or down? adaptive rounding for post-training quantization.
\newblock In \emph{International Conference on Machine Learning}, pages 7197--7206. PMLR, 2020.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, Boleda, and Fern{\'a}ndez]{paperno2016lambada}
Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Ngoc~Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern{\'a}ndez.
\newblock The {LAMBADA} dataset: Word prediction requiring a broad discourse context.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1525--1534, Berlin, Germany, August 2016. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P16-1144}.
\newblock URL \url{https://aclanthology.org/P16-1144}.

\bibitem[Park et~al.(2023)Park, Park, Kim, Lee, Kim, Kwon, Kwon, Kim, Lee, and Lee]{park2023lutgemm}
Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok Kwon, Se~Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee.
\newblock Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models.
\newblock \emph{arXiv preprint arXiv:2206.09557}, 2023.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Köpf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2019.

\bibitem[Prateek et~al.(2013)Prateek, Praneeth, and Sujay]{jain2013complete}
Jain Prateek, Netrapalli Praneeth, and Sanghavi Sujay.
\newblock Low-rank matrix completion using alternating minimization.
\newblock In \emph{Proceedings of the Forty-fifth Annual ACM STOC}, 2013.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020c4}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0 (140):\penalty0 1--67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Tata and Patel(2003)]{tat2003piqa}
Sandeep Tata and Jignesh~M Patel.
\newblock Piqa: An algebra for querying protein data sets.
\newblock In \emph{International Conference on Scientific and Statistical Database Management}, 2003.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{meta2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
  Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023.

\bibitem[Wang et~al.(2020)Wang, Chen, He, and Cheng]{wang2020bit}
Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng.
\newblock Towards accurate post-training network quantization via bit-split and stitching.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2020.

\bibitem[Wei et~al.(2022)Wei, Zhang, Zhang, Gong, Zhang, Zhang, Yu, and Liu]{wei2022outlier}
Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi~Zhang, Fengwei Yu, and Xianglong Liu.
\newblock Outlier suppression: Pushing the limit of low-bit transformer language models.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2022.

\bibitem[Workshop et~al.(2023)Workshop, :, Scao, Fan, Akiki, Pavlick, Ilić, Hesslow, Castagné, Luccioni, and et. al.]{workshop2023bloom}
BigScience Workshop, :, Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra~Sasha Luccioni, and François~Yvon et. al.
\newblock Bloom: A 176b-parameter open-access multilingual language model, 2023.

\bibitem[Xiao et~al.(2023)Xiao, Lin, Seznec, Wu, Demouth, and Han]{xiao2023smooth}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for large language models.
\newblock \emph{arXiv preprint arXiv:2211.10438}, 2023.

\bibitem[Yao et~al.(2021)Yao, Dong, Zheng, Gholami, Yu, Tan, Wang, Huang, Wang, Mahoney, and Keutzer]{yao2021hawq3}
Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael~W. Mahoney, and Kurt Keutzer.
\newblock Hawq-v3: Dyadic neural network quantization.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2021.

\bibitem[Yao et~al.(2022)Yao, Aminabadi, Zhang, Wu, Li, and He]{yao2022zero}
Zhewei Yao, Reza~Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He.
\newblock Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2022.

\bibitem[Yuan et~al.(2023)Yuan, Niu, Liu, Liu, Wang, Shang, Sun, Wu, Wu, and Wu]{yuan2023rptq}
Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Luzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu.
\newblock Rptq: Reorder-based post-training quantization for large language models.
\newblock \emph{arXiv preprint arXiv:2304.01089}, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang, and Zettlemoyer]{meta2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.
\newblock Opt: Open pre-trained transformer language models, 2022.

\end{thebibliography}
