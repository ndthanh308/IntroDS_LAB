\begin{table}[h!]
\centering
\small
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{l|c|ccc|ccc|ccc|ccc}
% \textbf{OPT-30b} 
& \multicolumn{13}{c}{\underline{\textbf{Incoherence Processing --- OPT-2.7b}}}\Bstrut \\
& 
\multicolumn{1}{c}{\underline{Full}}\Tstrut\Bstrut &
\multicolumn{3}{c}{\underline{QuIP}} &
\multicolumn{3}{c}{\underline{QuIP-RG}} &
\multicolumn{3}{c}{\underline{Greedy+IncP}} &
\multicolumn{3}{c}{\underline{Near+IncP}} \\
& W16\Tstrut & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 \\
\hline
Wiki$\downarrow$\Tstrut  &  12.47 &  12.39 &  17.44 &  2,998 &  12.58 &  15.07 &  1,676 &  12.68 &  12.96 &  155.6 &  12.79 &  13.79 &  28.98 \\
PTB$\downarrow$          &  17.97 &  18.42 &  20.79 &  63.59 &  18.43 &  20.49 &  42.05 &  18.34 &  20.03 &  46.28 &  18.43 &  19.51 &  39.23 \\
C4$\downarrow$           &  14.34 &  14.55 &  15.63 &  38.07 &  14.65 &  15.97 &  27.89 &  14.64 &  15.22 &  26.84 &  14.67 &  15.52 &  27.34 \\
ArcE$\uparrow$           &  54.34 &  53.28 &  52.99 &  46.93 &  52.02 &  52.36 &  46.93 &  52.90 &  51.73 &  43.14 &  52.61 &  50.93 &  44.11 \\
LAMB$\uparrow$           &  64.82 &  66.04 &  64.99 &  36.06 &  64.64 &  63.46 &  43.39 &  64.68 &  62.95 &  45.53 &  65.40 &  61.05 &  35.65 \\
PiQA$\uparrow$           &  74.76 &  74.54 &  73.94 &  68.06 &  73.88 &  73.45 &  68.28 &  74.54 &  73.83 &  68.28 &  73.61 &  73.56 &  67.85 \\
SC$\uparrow$\Bstrut      &  71.74 &  71.80 &  70.21 &  66.14 &  71.55 &  70.15 &  64.67 &  70.85 &  71.10 &  65.82 &  71.16 &  70.02 &  63.27 \\
\hline
& \multicolumn{13}{c}{\underline{\textbf{Baseline Processing --- OPT-2.7b}}}\Tstrut\Tstrut\Bstrut \\
& 
\multicolumn{1}{c}{\underline{Full}}\Tstrut\Bstrut &
\multicolumn{3}{c}{\underline{OPTQ}} &
\multicolumn{3}{c}{\underline{LDLQ-RG}} &
\multicolumn{3}{c}{\underline{Greedy}} &
\multicolumn{3}{c}{\underline{Near}} \\
& W16\Tstrut & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 & W4 & W3 & W2 \\
\hline
Wiki$\downarrow$\Tstrut  &  12.47 &  12.93 &  17.09 &  8,949 &  12.77 &  16.47 &  7,718 &  12.95 &  18.92 &  9,665 &  16.69 &  15,685 &  10,641 \\
PTB$\downarrow$          &  17.97 &  19.10 &  25.36 &  8,281 &  19.05 &  23.94 &  7,389 &  19.06 &  28.75 &  8,254 &  32.22 &  14,532 &  10,516 \\
C4$\downarrow$           &  14.34 &  14.99 &  18.14 &  4,388 &  14.85 &  17.37 &  2,113 &  15.01 &  20.87 &  5,139 &  18.75 &  11,257 &   9,356 \\
ArcE$\uparrow$           &  54.34 &  52.57 &  50.04 &  26.94 &  52.02 &  48.95 &  25.76 &  52.02 &  43.39 &  25.46 &  52.74 &   26.56 &   27.19 \\
LAMB$\uparrow$           &  64.82 &  62.00 &  51.43 &  00.00 &  64.04 &  53.25 &  00.00 &  63.50 &  40.75 &  00.00 &  59.15 &   00.00 &   00.00 \\
PiQA$\uparrow$           &  74.76 &  73.88 &  70.73 &  48.42 &  74.54 &  69.91 &  49.95 &  73.61 &  66.05 &  50.65 &  73.83 &   51.41 &   50.22 \\
SC$\uparrow$\Bstrut      &  71.74 &  70.91 &  68.56 &  48.50 &  71.42 &  67.79 &  47.17 &  70.66 &  60.53 &  48.44 &  70.59 &   47.42 &   47.55 \\
\hline
\end{tabular}
\end{adjustbox}
\caption{
Quantizing \textbf{OPT-2.7b} with all combinations of quantization and pre-post processing methods, evaluating on language generation and zeroshot tasks.
Our incoherence processing enables a step function change in quantization at 2 bits, across all rounding methods.
}
\label{tabSuppAllOPT2.7b}
\end{table}
