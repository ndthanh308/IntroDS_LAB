
\textbf{Adaptive rounding.}
\citet{nagel2020up} are the first to motivate the ``adaptive rounding'' proxy objective~(Eq.~\eqref{eqnAdaptiveEll}) in a principled way.
There are many quantization methods which quantize by optimizing this proxy objective~\citep{dong2019hawq1,dong2020hawq2,hubara2021adaq,li2021brecq,liu2021ptqvit,nagel2020up,yao2021hawq3}.
Many require further retraining which can be expensive, and are not evaluated on the current largest open LLMs (OPT~\citep{meta2022opt}, BLOOM~\citep{workshop2023bloom}).
\citet{lybrand2021greedy} propose a greedy per-neuron quantization procedure that is similar to ours, except they do not consider arbitrary linear functions of the error correction.
Their work bounds the proxy objective, albeit on the first layer only.

\textbf{Post training quantization in large models.}
There is a growing body of work on PTQ in LLMs such as OPT and BLOOM.
The size of these models make it difficult to apply previously developed methods.
The majority of these methods make quantization easier by somehow reducing the range of weights or activations, but still use nearest rounding.
SmoothQuant~\citep{xiao2023smooth} rescales between activations and weights to remove outliers from the activations and make quantization overall easier.
ZeroQuant~\citep{yao2022zero} proposes a per-layer knowledge distillation method.
LLM.int8()~\citep{dettmers2022int8} decompose matrix multiplications into a majority of 8 bit and a minority of 16 bit operations.
LUT-GEMM~\citep{park2023lutgemm} designs kernels to accelerate quantized matrix multiplications.
RPTQ~\citep{yuan2023rptq} reorders activations and quantizes them in groups, reducing the impact of range differences between channels.

\textbf{OPTQ (Formerly known as GPTQ).}
OPTQ~\citep{frantar2023gptq} is based on OBQ~\citep{frantar2022obc}, and proposes a novel rounding method that can work on the largest OPT and BLOOM models.
The method works iteratively over the weight columns in a fixed order: (1)~quantize with nearest rounding and compute the error, (2)~update the remaining weights with a scaled error, and (3)~repeat.

\textbf{Other quantization methods.}
There are other quantization procedures which do not round based on the proxy objective of~\cite{nagel2020up}, or are not designed for the largest language models~\citep{jeon2022biq,li2022qvit,liu2023noisy,nagel2019dfq,wang2020bit,wei2022outlier}.
