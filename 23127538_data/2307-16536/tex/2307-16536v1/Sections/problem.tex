\section{Model}\label{sec:problem}
Let $ \l( N, \sspace, \ospace, \aspace, \mcl{P}_{tr}, \l( c, d\r), P_{1}, \mcl{U}, \alpha \r)$
denote a (cooperative) MA-C-POMDP with $N$ %cooperative/non-strategic 
agents, state space $\sspace$, joint-observation space $\ospace$, a joint-action space $\aspace$, transition-law $\mcl{P}_{tr}$, immediate-cost functions $c$ and $d$, (fixed) initial distribution $P_1$, space of decentralized policy-profiles $\mcl{U}$, and discount factor $\alpha\in\l(0, 1\r)$. The decision problem has the following attributes and notations.
\begin{enumerate}
\item \textbf{State Process}: 
The state-space $\sspace$ is some topological space with a Borel $\sigma$-algebra $\borel{\sspace}$. 
The state-process is denoted by $\l\{\Stt{t} \r\}_{t=1}^{\infty}$.
\item \textbf{Joint-Observation Process}: The joint-observation space $\ospace$ is a countable discrete space of the form $ \ospace = \prod_{n=0}^{N} \onspace{n}$, where $\onspace{0}$ denotes the common observation space of all agents and $\onspace{n}$ denotes the private observation space of agent $n \in [N]$. The joint-observation process is denoted by $\l\{ \Ot{t} \r\}_{t=1}^{\infty}$ where $\Ot{t} = \Otn{t}{0:N}$ and is such that at time $t$, agent $n\in [N]$ observes $\Otn{t}{0}$ and $\Otn{t}{n}$ only.

\item \textbf{Joint-Action Process}: The joint-action space $\aspace $ is a finite discrete space of the form $ \aspace  = \prod_{n=1}^{N} \anspace{n} $, where $\anspace{n}$ denotes the action space of agent $n\in [N]$. The joint-action process is denoted by $\l\{ \At{t} \r\}_{t=1}^{\infty}$ where $\At{t} = \Atn{t}{1:N}$ and $\Atn{t}{n}$ denotes the action of agent $n$ at time $t$.\footnote{The results in this work also hold if for every $( \hstn{h}{t}{0}, \hstn{h}{t}{n}) \in \hstnspace{t}{0}\times\hstnspace{t}{n}$, agent $n$ is allowed to take action from a separate finite discrete space $\anspace{n}(\hstn{h}{t}{0}, \hstn{h}{t}{n})$.} Since all $\anspace{n}$'s and $\aspace$ are finite, they are all compact metric spaces.\footnote{Hence, also complete and separable.}

\item \textbf{Transition-law}: 
At time $t \in \mbb{N}$, given the current state $\Stt{t}$ and current joint-action $\At{t}$, the next state $\Stt{t+1}$ and the next joint-observation $\Ot{t+1}$ are determined in a time-homogeneous manner independent of all previous states, all previous and current joint-observations, and all previous joint-actions. The transition-law is given by 
\begin{align*}
\mcl{P}_{tr} \defeq \l\{ P_{saBo}: s \in \sspace, a\in\aspace, B \in  \borel{\sspace}, o \in \ospace \r\},\numberthis\label{eq:transitionlaw}
\end{align*}
where for all $t \in \mbb{N}$, 
\begin{align}
\begin{split}\label{eq:psabo}
% P_{saBo} &\defeq \\
&\pr\l( \Stt{t+1} \in B , \Ot{t+1} = o | \Stt{1:t-1}  = \stt{1:t-1}, \r.\\
&\hspace{20pt} \l. \Ot{1:t} = \ot{1:t}, \At{1:t-1} = \at{1:t-1}, 
\Stt{t} = s, \At{t} = a \r)\\
&\hspace{10pt}
= \pr\l( \Stt{t+1} \in B, \Ot{t+1} = o | \Stt{t} = s, \At{t} = a \r)\\
&\hspace{10pt} 
\defeq P_{saBo}.
\end{split}
\end{align}

\item \textbf{Immediate-costs}: 
The immediate cost $c : \sspace \times \aspace \mapsto \mbb{R}$ is a real-valued function whose expected discounted aggregate (to be defined later) we would like to minimize. On the other hand, the immediate cost $d : \sspace \times \aspace \mapsto \mbb{R}^K$ is $\mbb{R}^K$-valued function whose expected discounted aggregate we would like to keep within a specified threshold. For these reasons, we call $c$ and $d$ as the immediate objective and constraint costs respectively. We make use of the following assumption on immediate costs.

%-----------------------------------------------------------%
%-------ASSUMPTION OF IMMEDIATE COSTS BOUNDED-------%
%-----------------------------------------------------------%
\begin{assumption}[Bounded Immediate-costs]\label{assmp:boundedcosts}
\begin{enumerate}
\item[]
\item[(a)] Immediate objective cost is bounded from below, i.e., there exists $\udl{c} \in \mbb{R}$ such that
\begin{align*}
\udl{c} \le c(\cdot, \cdot). \numberthis\label{eq:cboundedbelow}
\end{align*}
\item[(b)] Immediate objective cost is bounded from above, i.e., there exists $\ov{c} \in \mbb{R}$ such that
\begin{align*}
c(\cdot, \cdot)  \le \ov{c}. \numberthis\label{eq:cboundedabove}
\end{align*}
\item[(c)] Immediate constraints costs are bounded both from above and below, i.e., there exist $\udl{d}, \ov{d} \in \mbb{R}^K$ such that
\begin{align*}
\udl{d} \le d(\cdot, \cdot) \le \ov{d}. \numberthis\label{eq:dbounded}
\end{align*}
\end{enumerate}
Let $ \ulbar{c} = |\udl{c}| \vee |\ov{c}|$ and $ \ulbar{d} = \| \udl{d} \|_{\infty} \vee  \| \ov{d} \|_{\infty}$ so that under (a)-(c), we have 
\begin{align*}
|c(\cdot, \cdot)| \le \ulbar{c} < \infty \text{ and } \| d(\cdot, \cdot) \|_{\infty} \le \ulbar{d} < \infty.
\end{align*}
\end{assumption}

\item \textbf{Initial Distribution}: $P_1$ is a (fixed) probability measure for the initial state and initial joint-observation, i.e., $P_1 \in \m{\sspace\times \ospace}$ and
\begin{align}
\begin{split}\label{eq:initialdistribution}
P_1\l( B, o \r) &\defeq \pr\l( \Stt{1} \in B, \Ot{1} = o \r).
\end{split}
\end{align}    

\item \textbf{Space of Policy-Profiles}: At time $t\in \mbb{N}$, the \emph{common history} of all agents is defined as all the common observations received thus far, i.e.,  $\Hstn{t}{0} \defeq \l( \Otn{1:t}{0}  \r)$. Similarly, the \emph{private history} of agent $n\in [N]$ at time $t$ is defined as all observations received and all the actions taken by the agent thus far (except for those that are part of the common information), i.e., 
\begin{align}
\begin{split}\label{eq:Hstn}
\Hstn{1}{n} &\defeq \Otn{1}{n} \setminus \Otn{1}{0}, \text{ and}\\
\Hstn{t}{n} &\defeq \l( \Hstn{t-1}{n}, (\Atn{t-1}{n}, \Otn{t}{n})\setminus \Otn{t}{0} \r) \ \forall t \in [2,\infty]_{\mbb{Z}}.
\end{split}
\end{align}
Finally, the \emph{joint-history} at time $t$ is defined as the tuple of the common history and all the private histories at time $t$, i.e., $\Hst{t}  \defeq \Hstn{t}{0:n}$. 

With the above setup, we define a (decentralized) behavioral policy-profile $u $ as a tuple $\un{u}{1:N} \in \uspace \defeq \prod_{n=1}^{N} \uspace^{(n)} $ where $\un{u}{n}$ denotes some behavioral policy used by agent $n$, i.e., $\un{u}{n}$ itself is a tuple of the form $ \utn{u}{1:\infty}{n}$ where $\utn{u}{t}{n}$ maps $\hstnspace{t}{0} \times \hstnspace{t}{n}$ to $\m{\anspace{n}}$, and where agent $n$ uses the distribution $\utn{u}{t}{n} ( \Hstn{t}{0}, \Hstn{t}{n} )$ to choose its action $\Atn{t}{n}$. We pause to emphasize that at any time $t$, each agent randomizes over its action-set independently of all other agents, that is, without any \textit{common randomness}. Thus, given a joint-history $\hst{h}{t} \in \hstspace{t}$ at time $t$, the probability that joint-action $\at{t} \in \aspace$ is taken is given by
\begin{align*}
\ut{u}{t}\l(\at{t}|\hst{h}{t}\r) = \ut{u}{t}\l(\hst{h}{t}\r)\l(\at{t} \r) 
&\defeq \prod_{n=1}^{N} \utn{u}{t}{n}\l( \hstn{h}{t}{0}, \hstn{h}{t}{n}  \r) \l( \atn{t}{n} \r)\\
&=\prod_{n=1}^{N} \utn{u}{t}{n}\l( \atn{t}{n} \big| \hstn{h}{t}{0}, \hstn{h}{t}{n}  \r).\numberthis\label{eq:uah}
\end{align*}
\begin{rem}	
With Assumption \ref{assmp:boundedcosts}(a) and \ref{assmp:boundedcosts}(c), the conditional expectations $\mbb{E}_{P_1} \l[ \cCost \mid \Hst{t} = \hst{h}{t}, \At{t} = \at{t} \r] $ and $\mbb{E}_{P_1} \l[ \dCost \mid \Hst{t} = \hst{h}{t}, \At{t} = \at{t} \r] $ exist, are unique, and are bounded from below. Furthermore, the latter are element-wise finite. 	
\end{rem}
% \textbf{Remark}: We assume that all agents have perfect-recall which in our setting means that at time $t$ each agent remembers $\utn{u}{1:t-1}{n}$.


\item \textbf{Optimization Problem}: Let $\prup{u}{P_1}$ be the probability measure corresponding to policy-profile $u\in\uspace$ and initial-distribution $P_1$ and let $\E{u}{P_1}$ denote the corresponding expectation operator.\footnote{The existence and uniqueness of $\prup{u}{P_1}$ can be ensured by an adaptation of the Ionesca-Tulcea theorem \cite{tulcea49}.} We define \textit{infinite-horizon expected total discounted costs} $C:\uspace\ra \mbb{R} \cup \{ \infty \}$ and $D:\uspace \ra \mbb{R}^K$ as %follows.
\begin{align*}
\fullccosts{u} = \fullccost{u} &\defeq  \E{u}{P_1} \l[ \sum_{t=1}^{\infty} \alpha^{t-1} \cCost \r],
\numberthis\label{eq:C}\\
\text{and}\ 
\fulldcosts{u} = \fulldcost{u} &\defeq  \E{u}{P_1}\l[ \sum_{t=1}^{\infty} \alpha^{t-1} \dCost \r].\numberthis\label{eq:D}
\end{align*}
\begin{rem}\label{rem:real_valued_aggregatecosts}
Assumption \ref{assmp:boundedcosts}(a) (objective cost bounded from below) and \ref{assmp:boundedcosts}(c) (constraint costs bounded) ensures that $\fullccosts{u}\in \mbb{R} \cup \{ \infty \}$, and $\fulldcosts{u} \in \mbb{R}^K$ with (absolute) element-wise bound $\ulbar{d}/(1-\alpha)$.
\end{rem}
The decision process proceeds as follows: \textit{i}) At time $t\in\mbb{N}$, the current state $\Stt{t}$ and observations $\Ot{t}$ are generated (according to $\mcl{P}_{tr}$ and/or $P_1$); \textit{ii}) Each agent $n\in[N]$ chooses an action $\an{n} \in \anspace{n}$ based on $\Hstn{t}{0}, \Hstn{t}{n}$; \textit{iii}) the immediate-costs $c\l( \Stt{t}, \At{t} \r), d\l(\Stt{t}, \At{t}\r)$ are incurred\footnote{In the planning context, the immediate-costs are known by all agents. In the learning context, we assume that the immediate-costs are observed by all agents. This assumption is innocuous for CTDE based learning algorithms where all quantities are known by a central entity, which we will refer to as the \emph{supervisor} in this work. See Section \ref{sec:marl}.}; \textit{iv}) The system moves to the next state and observations according to the transition-law $\mcl{P}_{tr}$.
% Figure environment removed
Under these rules, the goal of the agents is to work cooperatively to solve the following constrained optimization problem.
\begin{equation}\tag{\textit{MA-C-POMDP}}\label{eq:macpomdp}
\begin{split}
&\text{minimize } \fullccosts{u}\\
&\hspace{10pt} \text{subject to } u \in \uspace \text{ and } \fulldcosts{u} \le \constraintv.
\end{split}
\end{equation}
Here, $\constraintv$ is a fixed $K$-dimensional real-valued vector. We refer to the solution of \eqref{eq:macpomdp} as its \emph{optimal value} and denote it by $\optcosts = \optcost$. In particular, if the set of feasible policy-profiles is empty, we set $\optcosts$ to $\infty$ and with slight abuse of terminology will consider any policy-profile in $\uspace$ to be optimal.

The following assumption about feasibility of \eqref{eq:macpomdp} will be used in the paper.
\begin{assumption}[Slater's Condition]\label{assmp:slatercondition}
There exists a policy-profile $\ov{u} \in \uspace$ and $\zeta > 0$ for which
\begin{align*}
\fulldcosts{\ov{u}} \le \constraintv - \zeta1.\numberthis\label{eq:slatercondition} 
\end{align*}
\end{assumption}

\end{enumerate}