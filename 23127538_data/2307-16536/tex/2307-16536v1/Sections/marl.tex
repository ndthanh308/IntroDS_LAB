\section{Primal Dual Type Reinforcement Learning Using Approximate Information States}\label{sec:marl}
% Figure environment removed
% \textcolor{red}{Suppress for conference version?} 
In this section, we use the notions of ASPS and ASCS to develop an algorithmic framework for reinforcement learning in (cooperative) MA-C-POMDPs. This framework will involve recurrent and feed-forward neural-networks as function-approximators, and will be based on \emph{centralized training, distributed execution (CTDE)} wherein the training will be performed in a three time-scale stochastic approximation setup\footnote{See \cite{borkar08_sabook} for details.} as follows: first, the (time-invariant) approximate-state generators will be learnt on fast time-scale (using loss-functions motivated by ASPS-2.1/2.2/3 and ASCS-2.1/2.2/3), then an ASPS-ASCS based coordination policy will be learnt on medium time-scale (using off-the-shelf policy-gradient algorithms), and finally the optimal Lagrange-multipliers vector will be learnt on the slowest time-scale (using projected gradient-ascent).

\subsection{Function-Approximators}
We use the following types of %neural-network based 
function-approximators (see Figure \ref{fig:marl1}):
\begin{enumerate}
    \item \emph{Coordinator's ASCS Network}, modelled by a recurrent neural network (RNN), has inputs $\Otn{t}{0}, \Lamdaht{t-1}$, internal state $\Zhtn{t-1}{0} $, and output $\Zhtn{t}{0}$. It is denoted by $\rho^{(0)}$.
    \item \emph{Agent-$n$'s ASPS Network}, modelled by an RNN with inputs $\Otn{t}{0}, ( \Otn{t}{n}, \Atn{t-1}{n} ) \setminus \Otn{t}{0} $, internal state $\Zhtn{t-1}{n} $, and output $\Zhtn{t}{n}$. It is denoted by $\rho^{(n)}$. 
    \item \emph{Coordinator's Prescription Network}, modelled by a feed-forward neural network (FNN) with input $\Zhtn{t}{0} $ and output $\Lamdahtn{t}{1:N}$. It is denoted by $\varphin{0}$.
    \item \emph{Agent-$n$'s Prescription-Applier Network}, modelled by an FNN (with softmax as its last layer), has inputs $\Zhtn{t}{n}, \Lamdahtn{t}{n}$, and outputs a distribution on $\anspace{n}$.% (agent-$n$'s action-set).
    \item \emph{Coordinator's Prediction Network}, modelled by an FNN, has inputs $ \Zhtn{t}{0}, \Lamdahtn{t}{1:N}$, and outputs $\utn{\wh{C}}{t}{0}, \utn{\wh{D}}{t}{0}$ and $\un{\wh{\pr}}{0}\l( \cdot \r) \in \m{\onspace{0}}$. It is denoted by $\psi^{(0)}$. Here, $\utn{\wh{C}}{t}{0}$ and $\utn{\wh{D}}{t}{0}$ respectively serve as estimates of the conditional expectation of $\cCost$ and $\dCost$ given $\Zhtn{t}{0}, \Lamdahtn{t}{1:N}$ (see ASCS-2.1/2.2) and $\un{\wh{\pr}}{0}\l( \cdot \r)$ serves as an estimate of the conditional distribution of $\Otn{t+1}{0}$ given $\Zhtn{t}{0}, \Lamdahtn{t}{1:N}$ (see ASCS-3).
    \item \emph{Supervisor's Prediction Network}, modelled by an FNN, has inputs $ \Zhtn{t}{0:N}, \Atn{t}{1:N}$, and outputs $\utn{\wh{C}}{t}{S}, \utn{\wh{D}}{t}{S}$ and $\un{\wh{\pr}}{S}\l( \cdot \r) \in \m{\onspace{0:N}}$. It is denoted by $\psi^{(S)}$. Here, $\utn{\wh{C}}{t}{S}$ and $\utn{\wh{D}}{t}{S}$ respectively serve as estimates of the conditional expectation of $\cCost$ and $\dCost$ given $\wtHstn{t}{0}, \Zhtn{t}{1:N}, \Atn{t}{1:N}$ (see ASPS-2.1/2.2), and $\un{\wh{\pr}}{S}\l( \cdot \r)$ serves as an estimate of the conditional distribution of $\Ot{t+1}$ given $\wtHstn{t}{0}, \Zhtn{t}{1:N}, \Atn{t}{1:N}$ (see ASCS-3).
    
    Here, \emph{supervisor} is a omniscient entity that can access the union of the information of all agents and is needed because the approximation criteria in ASPS-2.1/2.2/3 require the knowledge of the tuples $\Zhtn{t}{0:N}$ and $\Atn{t}{1:N}$.
\end{enumerate}

\subsection{Centralized Training Distributed Execution}
The architectural setup of the aforementioned function-approximators is shown pictorially in \figurename \ref{fig:marl1}. Here, the networks $\rho \defeq \rhon{0:N}$ and $ \varphi \defeq \varphin{0:N}$ collectively define a parametrized ASPS-ASCS coordination policy which we denote by $\ut{\wh{v}}{\rho, \varphi}$. The state networks $\rho$ collectively generate ASPS and ASCS $\Zhtn{t}{0:N}$. The coordinator's prescription network $\varphin{0}$ takes the generated ASCS $\Zhtn{t}{0}$ as input and outputs a \emph{pseudo-prescription} tuple $\Lamdaht{t}$.\footnote{We use the term \emph{pseudo-prescription} because the original interpretation of a prescription is lost.} Then, each agent-$n$'s prescription-applier network $\varphin{n}$ takes $\Zhtn{t}{n}$ and %the pseudo-prescription 
$\Lamdahtn{t}{n}$ as input and outputs a distribution on $\anspace{n}$. This distribution is used by the agent to draw its action $\Atn{t}{n}$. 
Finally, the environment generates the next observations in response to the joint-action $\At{t}$. 

\input{Sections/pseudocode.tex}
In the above framework, the prediction-networks $\psi^{(0)}$ and $\psi^{(S)}$ are used (during the training phase) to generate estimates of the conditional expectations of immediate-costs and conditional distributions of observations. These estimates are compared with ground-truth realizations to drive the learning of the state-generators $\rho$. As concerns the learning of the coordination policy, the basic idea (synonymous to single-agent learning settings) is to get sample-paths based estimates of the policy-gradient, 
\begin{align*}
\nabla_{\varphi} L_{\infty} \l(\ut{\wh{v}}{\rho, \varphi},\lambda  \r) = \nabla_{\varphi} \E{\ut{\wh{v}}{\rho, \varphi}}{P_1} \l[ \sum_{t=1}^{\infty} \alpha^{t-1} \lCost \r].\numberthis\label{eq:policy_gradient}
\end{align*}
For learning the Lagrange-multipliers vector, sample-bath estimates of the below gradient are used.
\begin{align*}
\nabla_{\lambda} L_{\infty}\l(\ut{\wh{v}}{\rho, \varphi},\lambda \r) = \nabla_{\lambda}  \E{\ut{\wh{v}}{\rho, \varphi}}{P_1} \l[ \sum_{t=1}^{\infty} \alpha^{t-1} \lCost \r].\numberthis\label{eq:lagrangian_gradient}
\end{align*}
Once the training is complete, the execution phase is distributed -- the prediction networks $\psi^{(0)}$ and $\psi^{(S)}$ are no longer needed and (as mentioned earlier) the coordinator's networks can be instantiated by all agents. In the remainder of this section, we give a concrete instance of this framework that is based on multi-agent extension of the (single-agent) REINFORCE algorithm. Extending other policy-gradient algorithms such as actor-critic methods is %also possible and is 
left as an exercise.

\subsection{Three Time-scale Stochastic Approximation - Example Instantiation Based on REINFORCE \cite{sutton98} Algorithm}
For simplicity, we assume that the observation-sets $\onspace{n}$'s are finite. Also, with slight abuse of notation, we will denote the weights of a neural-network by the same letter that is used to denote it. Let $\l\{ \delta_{1,i} \r\}_{i=1}^{\infty}, \l\{ \delta_{2,i} \r\}_{i=1}^{\infty}, \l\{ \delta_{3,i} \r\}_{i=1}^{\infty} $ be three sequences of step-sizes that satisfy the standard three time-scale stochastic approximation conditions \cite{borkar08_sabook}, namely,
\begin{align}
\begin{split}\label{eq:stepsizes}
    \sum_{i=1}^{\infty} \delta_{1, i} = \sum_{i=1}^{\infty} \delta_{2, i} = \sum_{i=1}^{\infty} \delta_{3, i} = \infty,\\
    \sum_{i=1}^{\infty} \delta_{1, i}^2 + \sum_{i=1}^{\infty} \delta_{2, i}^2 + \sum_{i=1}^{\infty} \delta_{3, i}^2 < \infty,\\
    \frac{\delta_{3, i} }{ \delta_{2, i}} , \frac{\delta_{2, i} }{ \delta_{1, i}} \xrightarrow{i\ra\infty} 0.
    \end{split}
\end{align}
In our setup, $\delta_{1,i}$ will be used to learn the approximate-state generator networks, $\delta_{2,i}$ will be used to update the parameters of the ASPS-ASCS based coordination policy, and $\delta_{3,i}$ will be used to update the Lagrange-multipliers vector.

\begin{rem}
In practical implementations, the above conditions are rarely satisfied.
\end{rem}

\subsubsection{Learning the Approximate-State Generator Networks}
To drive the learning of the approximate-state generators, a few definitions are in order.
\begin{enumerate}
    \item To minimize the epsilons in the definitions of ASCS and ASPS, we define the loss-function,
    \begin{align*}
        l_2 &: \mbb{R}\times \mbb{R} \ra \mbb{R}_{\ge 0}\\
        l_2\l(\cdot, \star \r) &= \operatorname{smoothL1}\l( \cdot - \star \r)\\
        &\defeq \begin{cases}
        \frac{1}{2} \l( \cdot - \star \r)^2, &|\cdot-\star|<1,\\
        |\cdot - \star| - \frac{1}{2}, &\text{otherwise}.
        \end{cases}\numberthis\label{eq:l2}
    \end{align*}
    \item To minimize $\delta_c$ in ASCS-3, we define the negative log-likelihood loss-function,
    \begin{align*}
        l_{c,3} : \onspace{0} \times \m{\onspace{0}} &\ra \mbb{R}_{\ge 0} %\cup \{ \infty \}
        ,\\
        l_{c,3} \l( \on{0}, \wh{\pr}^{(0)}\l( \cdot \r) \r) &\defeq - \log \l(  \wh{\pr}^{(0)}\l( \on{0} \r) + \eta \r).\numberthis\label{eq:lc3}
    \end{align*}
    \item And similarly, to minimize $\delta_p$ in ASPS-3, we define
    \begin{align*}
        l_{p,3} : \ospace \times \m{\ospace} &\ra \mbb{R}_{\ge 0} %\cup \{ \infty \}
        ,\\
        l_{p,3}\l( o, \wh{\pr}^{(S)}\l(\cdot\r) \r) &\defeq - \log \l( \wh{\pr}^{(S)}\l(o\r) + \eta \r).\numberthis\label{eq:lp3}
    \end{align*}
\end{enumerate}
In \eqref{eq:lc3} and \eqref{eq:lp3}, $\eta$ is a sufficiently-small hyper-parameter to avoid indefinite gradients.

Consider a set of $B$ finite-horizon trajectories of length $T$ generated (independently) using coordination policy $\ut{\wh{v}}{\rho, \varphi} $. We denote this set by $\l\{ \tau_j \r\}_{j=1}^{B}$, where $\tau_j$ is given by
\begin{align*}
\tau_j &\defeq \l\{ o_{j,t}, a_{j,t}, c_{j,t}, d_{j,t}, \zhtn{j,t}{0:N}, \lamdaht{j,t}, \r. \\
&\hspace{20pt} \l. \wh{c}_{j,t}^{(0)}, \wh{d}_{j,t}^{(0)}, \wh{c}_{j,t}^{(S)}, \wh{d}_{j,t}^{(S)}, \wh{\pr}^{(0)}_{j,t}\l( \cdot \r), \wh{\pr}^{(S)}_{j,t}\l( \cdot \r) \r\}_{t=1}^{T}.\numberthis\label{eq:tauj}
\end{align*}
Here, $\star_{j,t}$ denotes the realization of the corresponding variable at time $t$ of the $j^{th}$ trajectory and $T$ denotes the common length of all the $B$ trajectories.\footnote{For the case of infinite-horizon total expected discounted costs, $T$ should preferably be on the order of $\frac{1}{1-\alpha}$.} For learning of ASCS-generator $\rho^{(0)}$ (coupled with the learning of the prediction network $\psin{0}$), we can use the loss-function: 
\begin{align*}
    &l_{\rhon{0}, \psin{0}} \l( c_{j, t}, d_{j, t}, o_{j, t+1}^{(0)}, \wh{c}_{j,t}^{(0)}, \wh{d}_{j,t}^{(0)}, \wh{\pr}_{j,t}^{(0)}\l(\cdot \r) \r) \\
    &\defeq \beta_0 l_2\l( \ut{c}{j,t}, \ut{\wh{c}}{j,t} \r) + \sum_{k=1}^{K} \beta_k l_2 \l( \ut{\l(d_k\r)}{j,t}, \ut{\l(\wh{d}_k\r)}{j,t} \r) \\
    &\hspace{10pt} + \beta_{K+1}l_{c,3} \l( \otn{j,t}{0}, \wh{\pr}_{j,t}^{(0)}\l( \cdot \r) \r).\numberthis\label{eq:l_rho0_psi0}
\end{align*}
Similarly, for learning of the ASPS-generator (coupled with the learning of $\rhon{0}$ and $\psin{S}$), we can use the loss-function:
\begin{align*}
    l_{\rho, \psin{S}} &\defeq \beta'_0 l_2\l( \ut{c}{j,t}, \ut{\wh{c}}{j,t} \r) + \sum_{k=1}^{K} \beta'_k l_2 \l( \ut{\l(d_k\r)}{j,t}, \ut{\l(\wh{d}_k\r)}{j,t} \r) \\
    &\hspace{30pt} + \beta'_{K+1}l_{p,3} \l( \ot{j,t}, \wh{\pr}_{j,t}^{(S)}\l( \cdot \r) \r).\numberthis\label{eq:l_rho_psiS}
\end{align*}
In \eqref{eq:l_rho0_psi0} and \eqref{eq:l_rho0_psi0}, the weights $\beta_k, \beta'_k$ are hyper-parameters that satisfy $\beta_k, \beta'_k > 0, \text{ and } \sum_{k=0}^{K+1} \beta_k = \sum_{k=0}^{K+1}\beta'_k = 1$. The above loss-functions lead to the following empirical risk functions whose gradients can be used to update the networks $\rho, \psin{0}$, and $\psin{S}$.
\begin{align*}
    &R_{ \un{\rho}{0}, \un{\psi}{0} } \l( \l\{ \tau_j \r\}_{j=1}^{B} \r)
    \defeq  \frac{1}{BT} \sum_{j=1}^{B} \sum_{t=1}^{T} l_{\un{\rho}{0},\un{\psi}{0} } 
    \l( \cdot \r),\numberthis\label{eq:R_rho0_psi0}\\
    &R_{ \rho, \un{\psi}{S} } 
    \l( \l\{ \tau_j \r\}_{j=1}^{B} \r)
    \defeq  \frac{1}{BT} \sum_{j=1}^{B} \sum_{t=1}^{T} l_{\un{\rho}{0},\un{\psi}{0} } 
    \l( \cdot \r).\numberthis\label{eq:R_rho_psiS}
\end{align*}
These risk functions can be used to update the state-generator and prediction networks $\rho, \psi^{(0)}, \psi^{(S)}$ (See Lines \ref{line:alg3:R_rho0_psi0} and \ref{line:alg3:R_rho_psiS} in Algorithm \ref{alg:reinforce_macpomdp}).

\subsubsection{Learning Prescription and Prescription-Applier Networks}
Based on the collected trajectories, we define cost-to-go terms,
\begin{align*}
    g_{j,t} \defeq \sum_{t'=t}^{T} \alpha^{t'-t} \l( c_{j,t} + \dotp{\lambda}{d_{j,t}-\constraintv} \r).\numberthis\label{eq:gjt}
\end{align*}
Then, the sample-paths based estimate of $\nabla_{\varphi} L_{\infty} \l(\ut{\wh{v}}{\rho,\varphi},\lambda \r)$, in flavor of the REINFORCE algorithm \cite{sutton98}, is given by
\begin{align*}
&\wh{\nabla_{\varphi} L_{\infty}}\l(\ut{\wh{v}}{\rho,\varphi},\lambda \r) \\
&\hspace{5pt} \defeq \nabla_{\varphi} \frac{1}{B} \sum_{j=1}^{B} \sum_{t=1}^{T}  g_{j, t} \l[ \sum_{n=1}^{N} \log \l( \varphi^{(n)}\l( \atn{j,t}{n}, \zhtn{j,t}{n}, \r. \r. \r.\\
&\hspace{100pt} \l. \l. \l. \proj{n}{ \un{\varphi}{0}\l( \zhtn{j,t}{0} \r) } 
\r)   \r) \r],\numberthis\label{eq:pg_estimate_reinforce}
\end{align*}
which can be used to update the prescription and prescription-applier networks $\varphi$. (See Lines \ref{line:alg3:gjt} and \ref{line:alg3:pg_estimate} in Algorithm \ref{alg:reinforce_macpomdp}).

\subsubsection{Learning the Lagrange-Multiplier}
The sample-paths based estimate of $\nabla_{\lambda} L_{\infty} \l(\ut{\wh{v}}{\rho,\varphi},\lambda \r) $ is simply given by
\begin{align*}
    % \lambda \la \lambda + 
    \wh{\nabla_{\lambda} L_{\infty}} \l( \ut{\wh{v}}{\rho,\varphi} ,\lambda\r) \defeq \l[ 
    \l( \frac{1}{B} \sum_{j=1}^{B} \sum_{t=1}^{T} \alpha^{t-1} d_{j,t}\r) - \constraintv
    \r]^{+},\numberthis\label{eq:lg_estimate}
\end{align*}
which is used to project the updated $\lambda$ onto $\mcl{Y}$ (see Line \ref{line:alg3:lg_estimate}) in Algorithm \ref{alg:reinforce_macpomdp}).