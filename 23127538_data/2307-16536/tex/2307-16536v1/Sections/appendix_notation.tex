\section{List of Symbols}\label{sec:appendix:notation}
\begin{itemize}
\setlength\itemsep{2pt}
\item MDP: Markov Decision Process.
\item POMDP: Partially Observable Markov Decision Process.
\item SA-MDP: Single-Agent MDP.
\item SA-POMDP: Single-Agent POMDP.
\item SA-C-MDP: Single-Agent Constrained MDP.
\item SA-C-POMDP: Single-Agent Constrained POMDP.
\item MA-POMDP: Multi-Agent POMDP.
\item MA-C-POMDP: Multi-Agent Constrained POMDP.
\item MARL: Multi-Agent Reinforcement Learning.
\item CTDE: Centralized Training Distributed Execution.
\item ASPS: Approximate Sufficient Private State.
\item ASCS: Approximate Sufficient Common State.
\item RNN: Recurrent Neural Network.
\item FNN: Feed-forward Neural Network.
\item $N$: Number of agents.
\item $\sspace$: State space.
\item $\onspace{0}$: Space of common observations of all agents.
\item $\onspace{n}$: Space of private observations of agent-$n$.
\item $\ospace$: Joint-observation space, given by $\ospace = \prod_{n=0}^{N} \onspace{n}$.
\item $\anspace{n}$: Space of actions of agent $n$.
\item $\aspace$: Joint-action space, given by $\aspace = \prod_{n=1}^{N} \anspace{n}$.
\item $M_1(\cdot)$: Set of all probability measures on topological space $\cdot$ endowed with the topology of weak convergence. 
\item $\mcl{P}_{tr}$: Transition-law. See \eqref{eq:transitionlaw}.
\item $P_{saBo}$: Probability that the next state is in the measurable set $B$ and the next joint-observation is $o$ given action $a$ is taken at current state $s$. See \eqref{eq:psabo}.
\item $c, d$: Immediate-costs: $c$ is the immediate objective cost and $d$ is the immediate constraint cost.
\item $\udl{c}, \ov{c}, \udl{d}, \ov{d}$: Upper and lower bounds on immediate costs. See Assumption \ref{assmp:boundedcosts}.
\item $\alpha$: Discount factor.
\item $P_1$: Initial distribution on the initial state and joint-observation. See \eqref{eq:initialdistribution}.
\item $\uspacen{n}$: Space of policies of agent $n$.
\item $\un{u}{n}$: Used to denote a policy of agent $n$. (in $\uspacen{n}$).
\item $\uspace$: Space of (decentralized) policy-profiles, $\prod_{n=1}^{N} \uspacen{n}$.
\item $u$: Used to denote a policy-profile (in $\uspace$). %See \eqref{eq:uah}.
\item $\uspacemix$: Space of (decentralized) mixtures of policy-profiles in $\uspace$, i.e., $\prod_{n=1}^{N} \m{\uspacen{n}}$.
\item $\mu$: Used to denote a typical element of $\uspacemix$,  given by $\mymathop{\times}_{n=1}^{N} \mun{n}$.
\item $\prup{u}{P_1}, \E{u}{P_1}$: Probability measure and expectation operator corresponding to policy-profile $u\in \uspace$ and initial-distribution $P_1$.
\item $C$: Infinite-horizon expected total discounted objective cost. See \eqref{eq:C}.
\item $D$: Infinite-horizon expected total constraint cost. See \eqref{eq:D}.
\item $\Hstn{t}{0}$: Common history of all agents at time $t$.
\item $\Hstn{t}{n}$: Private history of agent $n$ at time $t$.
\item $\Hst{t}$: Joint history at time $t$, given by $\Hstn{t}{0:N}$.
\item $\hstnspace{t}{n}, \hstspace{t}, \hsspace$: See \eqref{eq:hthnandh}.
\item $\optcosts$: Optimal solution of \eqref{eq:macpomdp}. See \eqref{eq:optccost:infsup}.
\item $\zeta$: Slack of feasible policy-profile $\ov{u}$ in Assumption \ref{assmp:slatercondition}. See \eqref{eq:slatercondition}.
\item $\mcl{Y}$: Space of non-negative Lagrange-multipliers, $\l\{ \lambda \in \mbb{R}^K: \lambda\ge 0\r\}$.
\item $L$: Lagrangian function for \eqref{eq:macpomdp}. See \eqref{eq:lagrangian}.
\item $\wh{L}$: Extended Lagrangian function for \eqref{eq:macpomdp}. See \eqref{eq:lagrangianmix}.
\item $\xuspace$: $\prod_{n=1}^{N}\xtspace{\uspacen{n}}$, also see \eqref{eq:xuspace}.
\item $\pruphsts{u}{t}{\hst{h}{t}, \at{t}}$: 
%Shorthand for 
$\prup{u}{P_1}\l( \Hst{t} = \hst{h}{t}, \At{t} = \at{t} \r)$.
\item $\zuphsts{u}{t}{\hst{h}{t}, \at{t}}$: 
%Shorthand for 
$\E{u}{P_1}\l[ \cCost \mid \Hst{t} = \hst{h}{t}, \At{t} = \at{t} \r] $.
\item $^i u$: $i^{th}$ policy-profile in the sequence $\l\{^i u\r\}_{i=1}^{\infty}$.
\item $\Gt{t} = \Gtn{t}{1:N}$: Coordinator's prescriptions at time $t$.
% \item $\gt{t} = \gtn{t}{1:N}$: Realization of $\Gt{t}$
\item $\gtspace{t}$: Set of all possible prescriptions at time $t$.
\item $ \xtspace{\gtspace{t}}$: See \eqref{eq:xgtspace}.
\item $\wtHstn{t}{0}$: Prescription-observation history of coordinator.
\item $\un{l}{\lambda}$: Immediate-cost in unconstrained version of \eqref{eq:macpomdp} parametrized by Lagrange-multiplier $\lambda\in\mcl{Y}$. See \eqref{eq:l_lamda}.
\item $L_T$: See \eqref{eq:L_T}.
\item $\Qtl{t,T}{\lambda}$: See \eqref{eq:Q_tT_lamda}. 
\item $\Vtl{t,T}{\lambda}$: Optimal cost-to-go from time $t\in[T]$ onward in a finite-horizon $T$. See \eqref{eq:V_tT_lamda}.
\item $\utn{v}{1:T}{\lambda, \star}$: See \eqref{eq:v_tT_lamda_star}.
\item $\Vtl{t}{\lambda}$: See \eqref{eq:V_t_lamda}.
\item $\Qtl{t}{\lambda}$: See \eqref{eq:Q_t_lamda}.
\item $ \un{\udl{l}}{\lambda}$: Lower bound on $\un{l}{\lambda}$. See \eqref{eq:lower_and_upper_lcost}.
\item $\un{\ov{l}}{\lambda} $: Upper bound on $\un{l}{\lambda}$. See \eqref{eq:lower_and_upper_lcost}.
\item $ \Pi_t$: Conditional distribution of $\Stt{t}, \Hstn{t}{1:N}$ given $\wtHstn{t}{0}$, $\pr_{P_1} \l( \Stt{t}, \Hstn{t}{1:N} \mid \wtHstn{t}{0} \r)$.
\item $\Zhtn{t}{1:N}$: ASPS at time $t$. See Definitions \ref{dfn:asps_generator_finite_horizon} and \ref{dfn:asps_generator_infinite_horizon}. 
\item $\Lamdaht{t}$: ASPS-based prescription at time $t$.
\item $\Zhtn{t}{0}$: ASCS at time $t$. See Definitions \ref{dfn:ascs_generator_finite_horizon} and \ref{dfn:ascs_generator_infinite_horizon}.
\item $\varthetahtn{t}{1:N}$: $t$-th component of (finite-horizon) ASPS-generator. See Definition \ref{dfn:asps_generator_finite_horizon}.
\item $\eps_{p,1}, \eps_{p,2}, \delta_{p}$: Attributes of ASPS-generator.
\item $\phihtn{t}{1:N}$: $t$-th component of evolution functions of (finite-horizon) ASPS-generator. See ASPS-1.
\item $\kappa \l(\cdot, \star \r)$: Total variation distance between probability measures $\cdot$ and $\star$. 
% \item $\zhtnspaces{t}{n}$:
\item $\lamdahtnspace{t}{n}$: Set of all possible ASPS-ASCS based prescriptions for agent $n$ at time $t$
\item $\lamdahtspace{t}$. Set of all possible ASPS-ASCS based prescriptions at time $t$, $\prod_{n=1}^{N}\lamdahtnspace{t}{n} $.
\item $\xtspace{\lamdahtspace{t}}$: See \eqref{eq:xlamdahtspace}.
\item $\varthetahtn{t}{0}$: $t$-th component of (finite-horizon) ASCS-generator.
\item $\eps_{c,1}, \eps_{c,2}, \delta_{c}$: Attributes of ASCS-generator.
\item $\phihtn{t}{0}$: $t$-th component of evolution functions of (finite-horizon) ASCS-generator. See ASCS-1.
\item $\whVtl{t,T}{\lambda}$ See \eqref{eq:whV_tT_lamda}.
\item $\whQtl{t,T}{\lambda}$: See \eqref{eq:whQ_tT_lamda}. 
\item $\utn{\wh{v}}{1:T}{\lambda,\star}$: See \eqref{eq:whv_tT_lamda_star}.
\item $M_c\l( \cdot; \alpha, T \r)$: See \eqref{eq:M_c}.
\item $M_p\l( \cdot; \alpha, T\r)$: See \eqref{eq:M_p}.
\item $N\l( \alpha, T\r)$: See \eqref{eq:N_alpha_T}.
% \item $\zhnspace{1:N} $:
\item $\varthetahn{1:N}$: Infinite-horizon ASPS-generator. See Definition \ref{dfn:asps_generator_infinite_horizon}.
% \item $\zhnspace{0}$:
\item $\varthetahn{0}$: Infinite-horizon ASCS-generator. See Definition \ref{dfn:ascs_generator_infinite_horizon}.
\item $\lamdahspace$: Set of all possible ASPS-ASCS based prescriptions when infinite-horizon ASPS and ASCS generators are used.
\item $\xtspace{\lamdahspace}$: See \eqref{eq:xlamdahspace}.
\item $\wh{B}$: See \eqref{eq:whB_whV}.
\item $\whVl{\lambda}, \whQl{\lambda}$: See \eqref{eq:whVl} and \eqref{eq:whQl}.
\item $\rhon{0} $: RNN to serve as (infinite-horizon) ASCS-generator.
\item $\rhon{1:N}$: RNNs to collectively serve as (infinite-horizon) ASPS-generator.
\item $\varphin{0}$: Coordinator's prescription network.
\item $\varphin{1:N}$: Prescription-applier networks of all agents.
\item $\psin{0}$: Coordinator's prediction network.
\item $\psin{S}$: Supervisor's prediction network.
\item $\nabla_{\varphi}L_{\infty}\l( \ut{\wh{v}}{\rho, \varphi} \r)$: Policy-gradient. See \eqref{eq:policy_gradient}.
\item $\delta_{1,i}, \delta_{2,i}$, $\delta_{3,i}$: Sequences of time-steps that satisfy three time-scale stochastic approximation conditions. See \eqref{eq:stepsizes}.
\item $l_2, l_{c,3}, l_{p,3}$: See \eqref{eq:l2}, \eqref{eq:lc3}, and \eqref{eq:lp3}.
\item $\eta$: Used as a placeholder in \eqref{eq:lc3} and \eqref{eq:lp3}.
\item $B$: Batch-size of trajectories.
\item $\tau_j$: $j^{th}$ trajectory. See \eqref{eq:tauj}.
\item $l_{\rhon{0}, \psin{0}}$: See \eqref{eq:l_rho0_psi0}.
\item $l_{\rho, \psin{S}}$: See \eqref{eq:l_rho_psiS}.
\item $R_{\rhon{0}, \psin{0}}$: See \eqref{eq:R_rho0_psi0}.
\item $R_{\rho, \psin{S}}$: See \eqref{eq:R_rho_psiS}.
\item $g_{j,t}$: Cost-to-go at time $t$ in $j^{th}$ trajectory. See \eqref{eq:gjt}.
\item $\wh{\nabla_{\varphi}L_{\infty} }\l( \ut{\wh{v}}{\rho, \varphi} \r)$: REINFORCE-estimate of policy-gradient $\nabla_{\varphi} L_{\infty} \l( \ut{\wh{v}}{\rho,\varphi} \r)$. See \eqref{eq:pg_estimate_reinforce}.

\end{itemize}