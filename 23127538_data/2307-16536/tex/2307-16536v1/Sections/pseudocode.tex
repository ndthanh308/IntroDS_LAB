\begin{algorithm}[t]
\DontPrintSemicolon

\KwInput
{%\\
% \nonl \hspace{9pt} 
Initial weights of all neural-networks 
$\l( \utn{\rho}{0}{0:N}, \utn{\varphi}{0}{0:N}, \utn{\psi}{0}{0}, \utn{\psi}{0}{S} \r) $.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Parameter{
Batch-size ($B$), 
simulated-episodes' horizon ($T$), 
% ignored-tail's length ($W$), 
Learning schedules $\{ \delta_{1,i}, \delta_{2,i}, \delta_{3,i} \}_{i=1}^{\infty}$ satisfying \eqref{eq:stepsizes}, $maxiters$.
}
\KwOutput{%\\
%\nonl \hspace{9pt}
Learned weights of all neural-networks.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
$i \la 0$.\\
\Repeat{convergence or $i > maxiters$.}{
Collect a set of $B$ trajectories namely $\l\{ \tau_j \r\}_{j=1}^{B}$, each of horizon $T$, by running the ASPS-ASCS based coordination policy $\ut{\wh{v}}{\rho_i,\varphi_i}$. (See \eqref{eq:tauj}).\label{line:alg3:trajectories}%\\

% ----- Coordinator's Backprop start ----- %
\tcc{Coordinator's Backprop}
Compute $R_{ \un{\rho}{0}, \un{\psi}{0} } \l( \l\{ \tau_j \r\}_{j=1}^{B} \r)$ according to \eqref{eq:R_rho0_psi0}.\label{line:alg3:R_rho0_psi0}

\nonl \begin{align*}
\begin{bmatrix}
\un{\rho_{i+1}}{0}\\
\un{\psi_{i+1}}{0}
\end{bmatrix} \la 
\begin{bmatrix}
\un{\rho_{i}}{0}\\
\un{\psi_{i}}{0}
\end{bmatrix}
- \delta_{1,i} \nabla_{\un{\rho}{0}, \un{\psi}{0} } R_{\un{\rho}{0}, \un{\psi}{0}} \big|_{\un{\rho_i}{0}, \un{\psi_i}{0}} . 
\end{align*}
% ----- Coordinator's Backprop end ----- %

% ----- Supervisor's Backprop start ----- %
\tcc{Supervisor's Backprop}
\nl Compute $R_{ \rho, \un{\psi}{S} } \l( \l\{ \tau_j \r\}_{j=1}^{B} \r)$ according to \eqref{eq:R_rho_psiS}.\label{line:alg3:R_rho_psiS}
% \begin{align*}
% L_{ \un{\rho}{0:N}, \un{\psi}{S} }  \la  \frac{1}{B} \sum\limits_{j=1}^{B} \sum\limits_{t=1}^{T} l_{\un{\rho}{0},\un{\psi}{C} } 
% \l( c_{j, t}, d_{j, t}, o_{j, t+1}^{(0:N)}, \wh{c}_{j,t}^{(S)}, \wh{d}_{j,t}^{(S)}, \un{\nu_{j, t}}{S}  \r).
% \end{align*}
\nonl \begin{align*}
&\begin{bmatrix}
\un{\rho_{i+1}}{0}\\
\un{\rho_{i+1}}{1:N}\\
\un{\psi_{i+1}}{S}
\end{bmatrix} \la 
\begin{bmatrix}
\un{\rho_{i+1}}{0}\\
\un{\rho_{i}}{1:N}\\
\un{\psi_{i}}{S}
\end{bmatrix}
\\
&\hspace{60pt} - \delta_{1, i} \nabla_{\rho, \un{\psi}{S} } R_{\rho, \un{\psi}{S}} \big|_{\un{\rho_{i+1}}{0}, \un{\rho_i}{1:N}, \un{\psi_i}{S} } . 
\end{align*}
% ----- Supervisor's Backprop end ----- %

%------policy network updates start-------%
\tcc{Gradient-descent in the primal}
\nl Compute cost-to-go terms $\{ \{ g_{j,t} \}_{t=1}^{T} \}_{j=1}^{B}$ given by \eqref{eq:gjt}.\label{line:alg3:gjt}

Compute $\wh{\nabla_{\varphi}L_{\infty}}\l(\ut{\wh{v}}{\rho,\varphi} ,\lambda  \r) \big|_{\varphi_i}$ according to \eqref{eq:pg_estimate_reinforce}.\label{line:alg3:pg_estimate}

\nonl \begin{align*}
    \begin{bmatrix}
    \un{\varphi_{i+1}}{0}\\
    \un{\varphi_{i+1}}{1:N}
    \end{bmatrix}
    &\la
    \begin{bmatrix}
    \un{\varphi_{i}}{0}\\
    \un{\varphi_{i}}{1:N}
    \end{bmatrix}
    - \delta_{2, i}
    \wh{\nabla_{\varphi}L_{\infty}}\l(\ut{\wh{v}}{\rho,\varphi} ,\lambda  \r) \big|_{\varphi_i}.
\end{align*}
%------policy network updates end-------%

%------Lagrange Multipliers' Update start-------%
\tcc{Gradient-ascent in the dual}
\nl Compute $\wh{\nabla_{\lambda}L_{\infty}}\l(\ut{\wh{v}}{\rho,\varphi} ,\lambda  \r) \big|_{\lambda_i}$ according to \eqref{eq:lg_estimate}.\label{line:alg3:lg_estimate}

\nonl \begin{align*}
    \lambda_{i+1} \la \lambda_{i} + \delta_{3, i} \wh{\nabla_{\lambda}L_{\infty}}\l(\ut{\wh{v}}{\rho,\varphi} ,\lambda  \r) \big|_{\lambda_i}.
\end{align*}
%------Lagrange Multipliers' Update end-------%
$i \la i+1$.
}
\caption{Pseudo-code for History-Embedding Based Reinforcement Learning in constrained Multi-Agent POMDPs.}\label{alg:reinforce_macpomdp}
\end{algorithm}

