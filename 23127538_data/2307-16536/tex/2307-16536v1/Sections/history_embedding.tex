\section{Planning Using Common-Information Approach and Approximate-Information States}\label{sec:history_embedding}
Theorem \ref{thm:strongduality} provides firm theoretical support for primal-dual type planning and learning algorithms for a given MA-C-POMDP. Indeed, given the optimal Lagrange-multipliers vector $\lambda^\star$, MA-C-POMDP simply reduces to a MA-POMDP\footnote{Except the fact that randomization amongst equally valuable actions cannot be ignored, in general.}, so essentially all MA-POMPD algorithms apply for gradient descent in the primal space $\uspace$. However, one must find %the correct Lagrangian multipliers 
$\lambda^\star$. In light of the first inequality in \eqref{eq:saddlepointconditions}, we can do this by a projected gradient ascent in the dual space $\mcl{Y}$ -- on a slower time-scale so that it sees the minimization over the primal space $\uspace$ as having essentially equilibrated. In this section, we will assume that Assumption \ref{assmp:boundedcosts} ((a)-(c)) holds.

In this section, we shall first review the common-information approach \cite{nayyar13,nayyar14} that transforms a given MA-POMDP into an equivalent SA-POMDP. We will then use insights from the resulting SA-POMDP in order to derive a compression-framework for approximately solving the original MA-C-POMDP. This framework will be an extension of \cite{hsu22} the details of which will be left as an exercise for the reader. Nevertheless, an important goal will be achieved via this exercise: the approximation criteria of the compression-framework will be independent of the Lagrange-multipliers vector $\lambda$. This property will be essential in the learning context where we would like the learning of the compression-mapping to be independent of $\lambda$ (since $\lambda$ needs to be learned as well). Note that if we, instead, directly followed the approach of \cite{hsu22}, then for each value of the Lagrange-multipliers vector $\lambda$, we would have to find a new compression-mapping, and then adapt it as $\lambda$ is changed.

To achieve optimality gaps for the said compression-framework, we will first consider \eqref{eq:macpomdp} over a finite-horizon $T<\infty$, and then (with the aid of Assumption \ref{assmp:boundedcosts}) let $T$ go to infinity. Before, we proceed further, we present a simple lemma that uses Assumption \ref{assmp:slatercondition} to get an upper-bound on $\lambda^\star$ -- the existence and search of which can therefore be restricted to a compact cube in $\mbb{R}^K$. (This shall enable us to get $\lambda$-independent optimality-gaps for the compression-framework: see Remark \ref{rem:universalbound_optimalitygap}).  

\begin{lem}\label{lem:upperbound_lambda}
Under Assumptions \ref{assmp:boundedcosts}(a) and \ref{assmp:slatercondition}, the optimal Lagrange-multipliers vector $\lambda^\star$ is upper-bounded as follows:
\begin{align*}
\| \lambda^{\star} \|_{\infty} \le \| \lambda^{\star} \|_1 \le \frac{1}{\zeta} \l(  \fullccosts{\ov{u}} - \frac{\udl{c}}{1-\alpha} \r).
\end{align*}
\end{lem}

\begin{proof}
We have the following sequence of inequalities:
\begin{align*}
\sum_{t=1}^{\infty} \alpha^{t-1} \udl{c} &\labelrel{\le}{eqr:lamda:assmp:boundedcosts}  \optcosts = \lags{u^\star}{\lambda^\star} \\
&\hspace{-0pt} \labelrel{\le}{eqr:lamda:saddlepoint} \lags{\ov{u}}{\lambda^\star}\\
&\hspace{-0pt} = \fullccosts{\ov{u}} + \dotp{\lambda^\star}{\fulldcosts{\ov{u}}-\constraintv}\\
&\hspace{-0pt} \labelrel{\le}{eqr:lamda:assmp:slater} \fullccosts{\ov{u}} + \dotp{\lambda^\star}{-\zeta1}\\
&\hspace{-0pt} \labelrel{=}{eqr:lamda:positive} \fullccosts{\ov{u}} - \zeta \|\lambda^\star\|_1 \\
&\hspace{-0pt}\le \fullccosts{\ov{u}} - \zeta \|\lambda^\star\|_{\infty}.
\end{align*}
Here, \eqref{eqr:lamda:assmp:boundedcosts} uses Assumption \ref{assmp:boundedcosts}; \eqref{eqr:lamda:saddlepoint} uses the second inequality in \eqref{eq:saddlepointconditions}; \eqref{eqr:lamda:assmp:slater} uses Assumption \ref{assmp:slatercondition}; and \eqref{eqr:lamda:positive} uses the fact that $\lambda^{\star}$ is non-negative. %\textcolor{red}{Move proof to Appendix.}
\end{proof}


\subsection{Common-Information Approach}
The common-information approach \cite{nayyar13,nayyar14} shows that a (cooperative) MA-POMDP can be converted into an equivalent SA-POMDP---called the \textit{coordinated-system}. In this system, the single agent, called the \emph{coordinator}, is a virtual entity that has access to the common observations $\Otn{t}{0}$, and does not get to see the agents' private observations and actions $( \Otn{t}{1:N}, \Atn{t}{1:N} ) \setminus \Otn{t}{0}$. Therefore, from the perspective of the coordinator, the unknown state is the environment's state combined with the private histories of all agents, i.e., $( \Stt{t}, \Hstn{t}{1:N} )$. 

At time $t \in \mbb{N}$, via a \emph{coordination policy} (to be defined later), the coordinator uses the common-history $\Hstn{t}{0}$ to \udl{deterministically} chooses an action $ \Gt{t} = \Gtn{t}{1:N} $\footnote{As $\Gt{t}$ depends deterministically on the common-history, all agents can replicate it with consensus.}, where $\Gtn{t}{n}$ maps $ \hstnspace{t}{n}$ to $ \m{\anspace{n}} $ and is designed to be an enforcing \textit{prescription} for agent-$n$---agent-$n$ applies $\Gtn{t}{n}$ to its private history $\Hstn{t}{n}$ and then draws its action $\Atn{t}{n} \sim \Gtn{t}{n} ( \Hstn{t}{n})$. We use $\gtspace{t}$ to denote the set of all possible prescriptions at time $t$, i.e., $\gtspace{t} = \prod_{n=1}^{N} \gtnspace{t}{n}$ and $\gtnspace{t}{n} \defeq \{ \gtn{t}{n} : \hstnspace{t}{n} \ra \m{\anspace{n}} \}$. We note that $\gtspace{t}$ is in one-to-one correspondence with the set
\begin{align*}
    \xtspace{\gtspace{t}} \defeq \prod_{n=1}^{N} \prod_{ \hstn{h}{t}{n} \in \hstnspace{t}{n} } \m{\anspace{n}; \hstn{h}{t}{n} }.\numberthis\label{eq:xgtspace}
\end{align*}
which is a compact (and convex) space by Tychonoff's theorem (see Proposition \ref{prop:tychonoff}), and is also metrizable (see Proposition \ref{prop:metrizability}). Henceforth, we will assume $\gtspace{t}$ to have the same topology as $\xtspace{\gtspace{t}}$.
\begin{rem}
To help achieve equivalence between the coordinated system and MA-C-POMDP (which uses decentralized policy-profiles only), we have restricted the coordinator to choose prescriptions in a deterministic manner---no randomization over the elements of 
% the spaces of prescriptions 
$\xtspace{\gtspace{t}}$'s. 
\end{rem}
Let $\wtHstn{t}{0}$ denote the prescription-observation history of the coordinator, i.e.,
\begin{align}
\begin{split}\label{eq:aoh:coordinator}
\wtHstn{1}{0} &\defeq \Otn{1}{0} \text{ and } \\
\wtHstn{t}{0} &\defeq \l( \wtHstn{t-1}{0}, \Gt{t-1}, \Otn{t}{0} \r) \text{ for all } t\in[2,\infty]_{\mbb{Z}}.
\end{split}
\end{align}
Then we can define a coordination policy $v$ as a tuple $\ut{v}{1:\infty} \in \vvspace$ where $\ut{v}{t}$ maps $\wthstnspace{t}{0}$ to $\gtspace{t}$ and where agent $n$ draws its action $\Atn{t}{n}$ according to the distribution $\Gtn{t}{n}( \Hstn{t}{n} ) = ( \ut{v}{t}( \wtHstn{t}{0} ) )^{(n)}( \Hstn{t}{n} )$. 
%With this setup, of deterministic coordination policy (with stochastic prescriptions), 
Note that in this setup, each $\wtHstn{t}{0}$ is some deterministic function of $\Hstn{t}{0}$. Therefore, we can replace $\ut{v}{t} ( \wtHstn{t}{0} )$ by $\ut{\wt{v}}{t} ( \Hstn{t}{0} )$. One then establishes equivalence between the coordinated system and MA-C-POMDP by setting 
\begin{align*}
\utn{u}{t}{n} \l( \hstn{h}{t}{0}, \hstn{h}{t}{n} \r) 
&= \l( \ut{v}{t} \l(  \hstn{\wt{h}}{t}{0} \r) \r)^{(n)} \l( \hstn{h}{t}{n} \r) \\
&= \l( \wt{v}_t  \l(  \hstn{h}{t}{0} \r) \r)^{(n)} \l( \hstn{h}{t}{n} \r).
\end{align*}
In light of the above equivalence and the strong duality result of Theorem \ref{thm:strongduality}, from now onward, we will restrict our focus to deriving optimal and approximately optimal coordination policies for coordinated-systems whose immediate-costs are parametrized by 
%the Lagrange-multipliers vector 
$\lambda\in\mcl{Y}$, namely $\un{l}{\lambda}: \sspace \times \aspace \ra \mbb{R}$, where
\begin{align*}
\un{l}{\lambda}\l(s,a\r) &\defeq c\l(s,a\r) + \dotp{\lambda}{d\l(s,a\r) -\constraintv}.\numberthis\label{eq:l_lamda}
\end{align*}
Also, in line with our aforementioned plan, we parametrize the aggregate discounted costs by horizons $T \in \mbb{N} \cup \{ \infty \}$, namely $L_T: \vvspace \times \mcl{Y} \ra \mbb{R}$, where
\begin{align*}
    % L_T &: \vvspace \times \mcl{Y} \ra \mbb{R},\\ 
    L_T \l( v, \lambda \r) = L_T^{(P_1, \alpha)} \l( v, \lambda \r) &\defeq \E{v}{P_1} \l[ \sum_{t=1}^{T} \alpha^{t-1} \lCost \r].\numberthis\label{eq:L_T}
\end{align*}
With the above setup in place, for a (finite) horizon $T \in \mbb{N}$ and a fixed Lagrange-multiplier $\lambda\in\mcl{Y}$, one has the following dynamic program (Algorithm \ref{alg:cidecomposition}) to find the optimal coordination policy for the objective $L_T$ (see \cite{nayyar13,nayyar14}). We note that the usage of $\min$ and $\argmin$ in \eqref{eq:V_tT_lamda} and \eqref{eq:v_tT_lamda_star} is justified due to compactness of $\gtspace{t}$, and the choice of prescription in \eqref{eq:v_tT_lamda_star} is arbitrary. 

%---------------------------------------------------------%
%-----ALGORITHM FOR FULL COMMON AND PRIVATE HISTORIES-----%
%---------------------------------------------------------%
\begin{algorithm}[H]
\DontPrintSemicolon
\KwInput{Time-horizon $T \in \mbb{N}$, Discount-factor $\alpha\in(0,1]$, MA-C-POMDP Model (see Section \ref{sec:problem}).}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Parameter{Lagrange-multipliers vector $\lambda \in \mcl{Y}$.}
\KwOutput{$\utn{v}{1:T}{\lambda,\star}$ determined by $\l\{ \Qtl{t,T}{\lambda} : \wthstnspace{t}{0} \times \gtspace{t} \ra \mbb{R} \r\}_{t=1}^{T}$.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\nonl $V_{T+1,T}^{(\lambda)} \equiv 0$.

\nonl \For{$t = T, T-1, \dots, 1$}{
\nonl \begin{align*}
% \nonl
% $    
&\Qtl{t,T}{\lambda} \l( \hstn{\wt{h}}{t}{0}, \gt{t} \r) 
= \mbb{E} \l[ \lCost \r. \\
&\hspace{10pt} \l. 
+ \alpha \Vtl{t+1,T}{\lambda} \l( \wtHstn{t+1}{0} \r)  \mid \wtHstn{t}{0} = \hstn{\wt{h}}{t}{0}, \Gt{t} = \gt{t} \r].\numberthis\label{eq:Q_tT_lamda}\\
&\Vtl{t,T}{\lambda} \l( \hstn{\wt{h}}{t}{0} \r) = \min_{\gt{t} \in \gtspace{t} } \Qtl{t,T}{\lambda} \l( \hstn{\wt{h}}{t}{0}, \gt{t} \r).\numberthis\label{eq:V_tT_lamda}\\
&\utn{v}{t}{\lambda, \star} \l( \hstn{\wt{h}}{t}{0} \r) \in \argmin_{\gt{t} \in \gtspace{t}} \Qtl{t,T}{\lambda} \l( \hstn{\wt{h}}{t}{0}, \gt{t} \r).\numberthis\label{eq:v_tT_lamda_star}
% $
\end{align*}
}
\caption{Dynamic Programming with Full Common and Private Histories in Finite-Horizon setting.}\label{alg:cidecomposition}
\end{algorithm}
%---------------------------------------------------------%
%-----ALGORITHM FOR FULL COMMON AND PRIVATE HISTORIES-----%
%---------------------------------------------------------%
\begin{rem}\label{rem:necessaryonly}
    Akin to SA-C-MDP and SA-C-POMDPs, even when $\lambda^\star$ is known, finding an optimal policy for the unconstrained objective $L_T\l( \cdot, \lambda^*\r)$ does not necessarily imply solving the (finite-horizon version of the) original constrained optimization problem---because the coordination policy $\utn{v}{1:T}{\lambda, \star} $ obtained from \eqref{eq:v_tT_lamda_star} may not satisfy the constraints. However, from Theorem \ref{thm:strongduality}, we are guaranteed that an optimal coordination policy exists and it is also true that any such policy must choose a prescription from the set in the right-hand-side of \eqref{eq:v_tT_lamda_star}. It is hard to characterize how the optimal policy randomizes between these prescriptions, thus our choice of arbitrary selection in \eqref{eq:v_tT_lamda_star}. Having said that, this issue shall remain somewhat innocuous in the learning context where $\lambda$ will be continuously updated based on constraint violations. A similar remark would apply to the (approximate) dynamic program in Algorithm \ref{alg:dp_approximatestate}.
\end{rem}


From dynamic programming theory, it is known that $\Vtl{t,T}{\lambda} : \wthstnspace{t}{0} \ra \mbb{R}$ (see \eqref{eq:V_tT_lamda}) satisfies 
\begin{align*}
\Vtl{t,T}{\lambda} \l( \hstn{\wt{h}}{t}{0} \r) = \inf_{v\in\vvspace} \E{v}{P_1} \l[ \sum_{\tau=t}^{T} \alpha^{\tau - t} \un{l}{\lambda}\l(\Stt{\tau}, \At{\tau}\r) \Big| \wtHstn{t}{0} = \hstn{\wt{h}}{t}{0} \r].
\end{align*}
Therefore, the (finite-horizon) coordination policy $\utn{v}{1:T}{\lambda, \star}$ (see \eqref{eq:v_tT_lamda_star}) minimizes the (finite-horizon) objective $L_T$, i.e.,
\begin{align*}
    L_T\l( \utn{v}{1:T}{\lambda, \star}, \lambda \r) &= \inf_{v\in\vvspace} L_T\l( v, \lambda \r).
\end{align*}
\begin{rem}
The first argument of $L_T$, by definition (see \eqref{eq:L_T}), should be an element of $\vvspace$. However, since the specification of the policy for times $T+1$ and onward does not matter, the above equation is consistent (with slight abuse of notation). 
\end{rem}

Using Assumption \ref{assmp:boundedcosts}, we can now compare $\Vtl{t,T}{\lambda}$ (the optimal performance on a finite-horizon $T\ge t$) with the optimal performance on the infinite-horizon. Let us define value-functions $\{ \Vtl{t}{\lambda} : \wthstnspace{t}{0} \ra \mbb{R} \}_{t=1}^{\infty}$,
\begin{align*}
&\Vtl{t}{\lambda} \l( \hstn{\wt{h}}{t}{0} \r) \\
&\defeq \inf_{v\in \vvspace} 
\E{v}{P_1} \l[ \sum_{\tau=t}^{\infty} \alpha^{\tau-t} \un{l}{\lambda}\l(\Stt{\tau}, \At{\tau}\r) \Big| \wtHstn{t}{0} = \hstn{\wt{h}}{t}{0} \r],\numberthis\label{eq:V_t_lamda}
\end{align*}
and the corresponding prescription-value-functions $\{ \Qtl{t}{\lambda} : \wthstnspace{t}{0} \times \gtspace{t} \ra \mbb{R} \}_{t=1}^{\infty}$,
\begin{align*}
&\Qtl{t}{\lambda} \l( \hstn{\wt{h}}{t}{0}, \gt{t} \r) \defeq \mbb{E}_{P_1} \l[ \un{l}{\lambda}\l(\Stt{t}, \At{t}\r) \r. \\
&\hspace{10pt} \l. + \alpha \Vtl{t+1}{\lambda} \l( \wtHstn{t+1}{0} \r) \Big| \wtHstn{t}{0} = \hstn{\wt{h}}{t}{0}, \Gt{t} = \gt{t} \r].\numberthis\label{eq:Q_t_lamda}
\end{align*}
Then, the following bound on $\Vtl{t}{\lambda}$ with respect to $\Vtl{t,T}{\lambda}$ ($T\ge t$) holds.
\begin{prop}\label{prop:VtT_vs_Vt}
    Fix $\lambda\in\mcl{Y}$, $\alpha\in(0,1)$, and $t \in \mbb{N}$. Suppose Assumption \ref{assmp:boundedcosts} holds and consider a (finite) horizon $T\in [t, \cdot]$. Then, for any $\hstn{\wt{h}}{t}{0} \in \wthstnspace{t}{0}$, the following relation holds between $\Vtl{t,T}{\lambda} \l( \hstn{\wt{h}}{t}{0} \r) $ and $\Vtl{t}{\lambda} $.
    \begin{align*}
        &\Vtl{t,T}{\lambda} \l( \hstn{\wt{h}}{t}{0} \r) + \frac{\alpha^{T-t+1}}{1-\alpha} \un{\udl{l}}{\lambda} \le \Vtl{t}{\lambda} \l( \hstn{\wt{h}}{t}{0} \r) \\
        &\hspace{20pt} \le \Vtl{t,T}{\lambda} \l( \hstn{\wt{h}}{t}{0} \r) + \frac{\alpha^{T-t+1}}{1-\alpha} \un{\ov{l}}{\lambda},
    \end{align*}
    where
    \begin{align*}\numberthis\label{eq:lower_and_upper_lcost}
        \un{\udl{l}}{\lambda} \defeq \udl{c} + \dotp{\lambda}{\udl{d} - \constraintv } \text{ and }
        \un{\ov{l}}{\lambda} \defeq \ov{c} + \dotp{\lambda}{\ov{d} - \constraintv }
    \end{align*}
\end{prop}
\begin{proof}
    The proof can be established by backward induction. For brevity, it is omitted.
\end{proof}
With Proposition \ref{prop:VtT_vs_Vt}, we have a bound on the gap between $\Vtl{t}{\lambda}$ and $\Vtl{t,T}{\lambda}$ ($T\ge t$), that decays exponentially with $T$ (due to discounting). Therefore, for the time being, let us focus on the finite-horizon setup for which an optimal coordination policy can be found by Algorithm \ref{alg:cidecomposition}. There are, however, two key issues with Algorithm \ref{alg:cidecomposition}. Firstly, the domain of $\wtHstn{t}{0}$ grows exponentially in time and while one may compress $\wtHstn{t}{0}$ to a belief-state $\Pi_t \defeq \pr_{P_1} ( \Stt{t}, \Hstn{t}{1:N} \mid \wtHstn{t}{0} )  $ (without loss of optimality), 
% which is the conditional distribution of $\l( \Stt{t}, \Hstn{t}{1:N} \r) $ given the coordinator's prescription-observation history $\wtHstn{t}{0}$
the update of $\Pi_t$ requires knowledge of $\mcl{P}_{tr}$ (the transition-law) and $P_1$ (joint distribution of initial state and initial joint-observation) which are not available in the learning context. Secondly, and more importantly, the domain of the unobserved state $\Stt{t}, \Hstn{t}{1:N}$ grows exponentially with time and the number of agents---which leads to doubly-exponential growth of the coordinator's prescriptions. Due to these issues, Algorithm \ref{alg:cidecomposition} is generally not implementable, and thus remains conceptual. 

One means to address the above challenges is to compress the increasing common and private histories in such a way that when policies are restricted to take actions based on the compressed images of these histories, the performance is still approximately good. This is the goal in \cite{hsu22} which proposes one such compression-framework, and then characterizes its optimality-gap (as a function of the compression attributes). In the remainder of the section, we will extend the notions of \cite{hsu22} to the constrained setting, and as mentioned earlier, we will do so in a manner that the learning of the compression mapping shall remain independent of the Lagrange-multipliers vector $\lambda$. Also, it will be helpful to keep in mind that our ultimate goal will be to get a bound on the gap between $\Vtl{t,T}{\lambda}$ and $\whVtl{t,T}{\lambda}$ where $\whVtl{t,T}{\lambda}$ will be the optimal cost-to-go when coordination policies are restricted to use a prespecified compression framework.

\subsection{Approximate State Representations (Finite-Horizon)}
The framework in \cite{hsu22} uses a three-step process. It first compresses private histories from $\Hstn{t}{1:N} $ to an \textit{approximate sufficient private state (ASPS)}, which we will denote by $\Zhtn{t}{1:N} %\in \zhtnspace{t}{n} 
$. Compressing private histories to ASPS then changes the set of coordinator's prescriptions to a reduced set, the elements of which we will denote by $\lamdaht{t}$. Finally, the coordinator's prescription-observation history (now with the set of reduced prescriptions) is compressed from $\wtHstn{t}{0}$ to an \textit{approximate sufficient common state (ASCS)}, which we will denote by $\Zhtn{t}{0}$. With these three steps in mind, we now give formal definitions of ASPS and ASCS. (Note that ASCS relies on having an ASPS beforehand).

%---------------------------------------------------------%
%-----ASPS DEFINITION-----%
%---------------------------------------------------------%
\begin{dfn}[Finite-Horizon ASPS Generator]\label{dfn:asps_generator_finite_horizon}
Let $T\in\mbb{N}$ and let $\zhtnspace{1:T}{1:N}$ be a pre-specified collection of topological spaces.\footnote{%A complete normed vector space. 
We can assume each $\zhtnspace{t}{n}$ is a Euclidean space of some fixed dimension that can vary with $t$ and $n$, but a time-invariant domain is best in practice.} A collection $ \varthetahtn{1:T}{1:N} $ of compression-functions where $\varthetahtn{t}{n}$ maps $\hstnspace{t}{0} \times \hstnspace{t}{n}$ to $\zhtnspace{t}{n}$ is called a $T$-horizon $\l(\eps_{p,1}, \eps_{p,2}, \delta_p \r)$-ASPS generator if the process $\{ \Zhtn{t}{1:N} \}_{t=1}^{T}$, with $\Zhtn{t}{n} = \varthetahtn{t}{n}( \Hstn{t}{0}, \Hstn{t}{n} ) $ almost-surely, satisfies the following properties.
\begin{itemize}%[leftmargin=*]
\item[] \textbf{(ASPS1)} It evolves in a recursive manner: for each $n \in [N] $,
\begin{align*}
\Zhtn{t}{n}=\phihtn{t}{n} \l( \Zhtn{t-1}{n}, %\Gt{t-1}, 
\Otn{t}{0}, \l( \Otn{t}{n}, \Atn{t-1}{n}\r)\setminus \Otn{t}{0} \r).
\end{align*}

\item[] \textbf{(ASPS2.1)} It suffices for approximate prediction of objective-cost: for all $\l(\hstn{\wt{h}}{t}{0}, \hstn{h}{t}{1:N}, \at{t}\r) \in \wthstnspace{t}{0} \times \prod_{n=1}^{N} \hstnspace{t}{n} \times \aspace$,
\begin{align*}
&\l| \mbb{E}_{P_1} \l[  c\l( \Stt{t}, \At{t} \r) \Big| \hstn{\wt{h}}{t}{0}, \hstn{h}{t}{1:N}, \at{t} \r] - \r. \\
&\hspace{40pt} \l. \mbb{E}_{P_1} \l[ c\l( \Stt{t}, \At{t} \r) \Big| \hstn{\wt{h}}{t}{0}, \zhtn{t}{1:N}, \at{t} \r]  \r| \le \frac{\eps_{p,1}}{4}.
\end{align*}

\item[] \textbf{(ASPS2.2)} It suffices for approximate prediction of constraint cost: for all $\l(\hstn{\wt{h}}{t}{0}, \hstn{h}{t}{1:N}, \at{t}\r) \in \wthstnspace{t}{0} \times \prod_{n=1}^{N} \hstnspace{t}{n} \times \aspace$,
\begin{align*}
&\l\| \mbb{E}_{P_1} \l[  d\l( \Stt{t}, \At{t} \r) \Big| \hstn{\wt{h}}{t}{0}, \hstn{h}{t}{1:N}, \at{t} \r] - \r. \\
&\hspace{40pt} \l. \mbb{E}_{P_1} \l[ d\l( \Stt{t}, \At{t} \r) \Big| \hstn{\wt{h}}{t}{0}, \zhtn{t}{1:N}, \at{t} \r]  \r\|_{\infty} \le \frac{\eps_{p,2}}{4}.
\end{align*}

\item[] \textbf{(ASPS3)} It suffices for approximate prediction of observations: for all $\l(\hstn{\wt{h}}{t}{0}, \hstn{h}{t}{1:N}, \at{t}\r) \in \wthstnspace{t}{0} \times \prod_{n=1}^{N} \hstnspace{t}{n} \times \aspace$,
\begin{align*}
&\kappa \l( 
\mbb{P}_{P_1} \l( \Otn{t+1}{0:N} \mid \hstn{\wt{h}}{t}{0}, \hstn{h}{t}{1:N}, \at{t} \r), \r. \\ 
&\hspace{30pt} \l. \mbb{P}_{P_1} \l( \Otn{t}{0:N} \mid \hstn{\wt{h}}{t}{0}, \zhtn{t}{1:N}, \at{t} \r) 
\r)  \leq \frac{\delta_p}{8},
\end{align*}
where $\kappa\l(\cdot, \star \r)$ is the total variation distance between the two probability measures.
\end{itemize}
\end{dfn}

%---------------------------------------------------------%
%-----ASPS DEFINITION-----%
%---------------------------------------------------------%
Let us denote the range of $\varthetahtn{t}{n}$ by $\zhtnspaces{t}{n}$, i.e., $\zhtnspaces{t}{n} \defeq \varthetahtn{t}{n} \l( \hstnspace{t}{0} \times \hstnspace{t}{n} \r)$. Then the above definition induces a compression in the coordinator's prescriptions from $\Gt{t}$ to $\Lamdaht{t} = \Lamdahtn{t}{1:N}$ where $\Lamdahtn{t}{n}$ maps $\zhtnspaces{t}{n}$ (instead of $\hstnspace{t}{n}$) to $\m{\anspace{n}}$. We use $\lamdahtspace{t}$ to denote the set of all possible reduced prescriptions at time $t$, i.e., $\lamdahtspace{t} = \lamdahtnspace{t}{1:N}$ where $ \lamdahtnspace{t}{n} \defeq \{ \lamdahtn{t}{n} : \zhtnspaces{t}{n} \mapsto \m{\anspace{n}} \}$. We note that $\lamdahtspace{t}$ is in one-to-one correspondence with the set
\begin{align*}
    \xtspace{\lamdahtspace{t}} \defeq \prod_{n=1}^{N} \prod_{ \zhtn{t}{n} \in \zhtnspaces{t}{n} } \m{\anspace{n}; \zhtn{t}{n}}.\numberthis\label{eq:xlamdahtspace}
\end{align*}
which is a compact (and convex) space by Tychonoff's theorem (see Proposition \ref{prop:tychonoff}), and is also metrizable. From hereon, we will assume $\lamdahtspace{t}$ to have the same topology as $\xtspace{\lamdahtspace{t}}$.

Having detailed the compression of \textit{i}) private histories to ASPS, and \textit{ii}) private-history based prescriptions to ASPS-based prescriptions, we now proceed to formally characterizing the compression of the common history to ASCS.

%---------------------------------------------------------%
%-----ASCS DEFINITION-----%
%---------------------------------------------------------%
\begin{dfn}[Finite-Horizon ASCS Generator]\label{dfn:ascs_generator_finite_horizon}
Let $T\in\mbb{N}$ and let $\zhtnspace{1:T}{0}$ be a collection of topological spaces.\footnote{%A complete normed vector space. 
For our purposes, we can assume each $\zhtnspace{t}{0}$ is a Euclidean space of some fixed dimension that can vary with $t$.} For a given $T$-horizon $\l(\eps_{p,1}, \eps_{p,2}, \delta_p \r)$-ASPS generator, a collection $ \varthetahtn{1:T}{0} $ of compression-functions where $\varthetahtn{t}{0}$ maps $\wthstnspace{t}{0}$ to $\zhtnspace{t}{0}$ is called the corresponding $T$-horizon $\l(\eps_{c,1}, \eps_{c,2}, \delta_c \r)$-ASCS generator if the process $\{ \Zhtn{t}{0} \}_{t=1}^{T}$, with $\Zhtn{t}{0} = \varthetahtn{t}{0}( \wtHstn{t}{0} ) $ almost-surely, satisfies the following properties.
\begin{enumerate}
\item \textbf{(ASCS1)} It evolves in a recursive manner:
\begin{align*}
\Zhtn{t}{0}=\phihtn{t}{0} \l(\Zhtn{t-1}{0}, \Lamdaht{t-1}, \Otn{t}{0} \r).
\end{align*}

\item \textbf{(ASCS2.1)} It suffices for approximate prediction of objective-cost: for all $\l(\hstn{\wt{h}}{t}{0}, \lamdaht{t} \r) \in \wthstnspace{t}{0} \times \lamdahtspace{t}$,
\begin{align*}
&\l| \mbb{E}_{P_1} \l[  c\l( \Stt{t}, \At{t} \r) \Big| \hstn{\wt{h}}{t}{0}, \lamdaht{t} \r] - \r. \\
&\hspace{40pt} \l. \mbb{E}_{P_1} \l[ c\l( \Stt{t}, \At{t} \r) \Big| \zhtn{t}{0}, \lamdaht{t} \r]  \r| \le \frac{\eps_{c,1}}{4}.
\end{align*}

\item \textbf{(ASCS2.2)} It suffices for approximate prediction of constraint-cost: for all $\l(\hstn{\wt{h}}{t}{0}, \lamdaht{t} \r) \in \wthstnspace{t}{0} \times \lamdahtspace{t}$,
\begin{align*}
&\l\| \mbb{E}_{P_1} \l[  d\l( \Stt{t}, \At{t} \r) \Big| \hstn{\wt{h}}{t}{0}, \lamdaht{t} \r] - \r. \\
&\hspace{40pt} \l. \mbb{E}_{P_1} \l[ d\l( \Stt{t}, \At{t} \r) \Big| \zhtn{t}{0}, \lamdaht{t} \r]  \r\|_{\infty} \le \frac{\eps_{c,2}}{4}.
\end{align*}

\item \textbf{(ASCS3)} It suffices for approximate prediction of common-observations: for all $\l(\hstn{\wt{h}}{t}{0}, \lamdaht{t} \r) \in \wthstnspace{t}{0} \times \lamdahtspace{t}$,
\begin{align*}
&\kappa \l( 
\mbb{P}_{P_1} \l( \Otn{t+1}{0} \mid \hstn{\wt{h}}{t}{0}, \lamdaht{t} \r), \mbb{P}_{P_1} \l( \Otn{t}{0} \mid \zhtn{t}{0}, \lamdaht{t} \r) 
\r)  \leq \frac{\delta_c}{8}.
\end{align*}
\end{enumerate}
\end{dfn}
\begin{rem}
ASPS-1 and ASCS-1 are important for designing implementable algorithms; the recursive nature of $\Zhtn{t}{n}$'s obviates the requirement of storing the full histories.
\end{rem}

%---------------------------------------------------------%
%-----ASCS DEFINITION-----%
%---------------------------------------------------------%
Let us denote the range of $\varthetahtn{t}{0}$ by $\zhtnspaces{t}{0}$, i.e., $\zhtnspaces{t}{0} \defeq \varthetahtn{t}{0} \l( \wthstnspace{t}{0} \r)$. Then combining ASCS with ASPS (and ASPS-based prescriptions) leads to a reduction in the policy search space of the coordinator from $\vvspace$ to $\wh{\vvspace}( \varthetahtn{1:T}{0:N} )$. A typical element $\wh{v}$ of $\wh{\vvspace}$ is a tuple $\wh{v}_{1:\infty}$, where for all $t \in [T]$, $\wh{v}_t$ maps $\zhtnspaces{t}{0}$ to $\lamdahtspace{t}$, such that to take action $\Atn{t}{n}$, agent $n$ uses the distribution $$\Lamdahtn{t}{n}\l(\Zhtn{t}{n} \r) = \l[ \wh{v}_t \l(\Zhtn{t}{0}\r) \r]^{(n)} \l( \Zhtn{t}{n} \r), $$ 
which is almost-surely the same as 
$$ \l[ \wh{v}_t \l( \varthetahtn{t}{0}\l(\wtHstn{t}{0}\r) \r) \r]^{(n)} \l( \varthetahtn{t}{n} \l( \Hstn{t}{0}, \Hstn{t}{n} \r)  \r).$$

Given $T$-horizon ASPS and ASCS generators $\varthetahtn{1:T}{0:N}$, we can find an optimal-policy in $\wh{\vvspace}$ (thus approximately optimal in $\vvspace$) using the (approximate) dynamic program in Algorithm \ref{alg:dp_approximatestate}.

%---------------------------------------------------------%
%--ALGORITHM FOR COMPRESSED COMMON AND PRIVATE HISTORIES--%
%---------------------------------------------------------%
\begin{algorithm}[H]
\DontPrintSemicolon
\KwInput{Time-horizon $T \in \mbb{N}$, Discount-factor $\alpha\in(0,1]$, MA-C-POMDP Model (see Section \ref{sec:problem}).}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Parameter{Lagrange-multipliers vector $\lambda \in \mcl{Y}$.}
\KwOutput{$\utn{\wh{v}}{1:T}{\lambda,\star}$ determined by $\l\{ \whQtl{t,T}{\lambda} : \zhtnspaces{t}{0} \times \lamdahtspace{t} \ra \mbb{R} \r\}_{t=1}^{T}$.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\nonl $\wh{V}_{T+1,T}^{(\lambda)} \equiv 0$.

\nonl \For{$t = T, T-1, \dots, 1$}{
\nonl \begin{align*}
% \nonl
% $
&\whQtl{t,T}{\lambda}\l(\zhtn{t}{0}, \lamdaht{t}\r) = 
\mbb{E} \l[
\lCost \r. \\
&\hspace{10pt} \l. + \alpha \whVtl{t+1,T}{\lambda}  \l( \Zhtn{t+1}{0} \r)  \mid \Zhtn{t}{0} = \zhtn{t}{0}, \Lamdaht{t} = \lamdaht{t} \r].\numberthis\label{eq:whQ_tT_lamda}\\
&\whVtl{t,T}{\lambda}\l( \zhtn{t}{0} \r) = \min_{\lamdaht{t} \in \lamdahtspace{t} } \whQtl{t,T}{\lambda}\l( \zhtn{t}{0}, \lamdaht{t} \r)\numberthis\label{eq:whV_tT_lamda}\\
&\utn{\wh{v}}{t}{\lambda,\star} \l(\zhtn{t}{0} \r) = \argmin_{\lamdaht{t} \in \lamdahtspace{t}} \whQtl{t,T}{\lambda}\l( \zhtn{t}{0}, \lamdaht{t} \r).\numberthis\label{eq:whv_tT_lamda_star}
% .$
\end{align*}
}
\caption{%(Approximate) 
Dynamic Programming with Compressed Common and Private Histories in Finite-Horizon setting.}\label{alg:dp_approximatestate}
\end{algorithm}
%---------------------------------------------------------%
%--ALGORITHM FOR COMPRESSED COMMON AND PRIVATE HISTORIES--%
%---------------------------------------------------------%
Like before, the usage of $\min$ and $\argmin$ in Algorithm \ref{alg:dp_approximatestate} is justified due to compactness of $\lamdahtspace{t}$, and the choice of the reduced prescription in \eqref{eq:whv_tT_lamda_star} is arbitrary. Since ``$\Zhtn{t}{0} = \varthetahtn{t}{0} \l( \wtHstn{t}{0}  \r)$'' and ``$\Zhtn{t}{n} = \varthetahtn{t}{n} \l( \Hstn{t}{0}, \Hstn{t}{n} \r) \forall\  n\in[N]$'' hold almost-surely, from standard results in dynamic programming theory, it follows that $\whVtl{t,T}{\lambda} : \wthstnspace{t}{0} \ra \mbb{R} $ (see \eqref{eq:whV_tT_lamda}) satisfies 
\begin{align*}
&\whVtl{t,T}{\lambda} \l( \varthetahtn{t}{0} \l( \hstn{\wt{h}}{t}{0} \r) \r) \\
&\hspace{10pt} = \inf_{v\in\wh{\vvspace}} \E{\wh{v}}{P_1} \l[ \sum_{\tau=t}^{T} \alpha^{\tau - t} \lCost \Big| \wtHstn{t}{0} = \hstn{\wt{h}}{t}{0} \r].
\end{align*}
Therefore, the (finite-horizon) coordination policy $\utn{\wh{v}}{1:T}{\lambda, \star}$ (see \eqref{eq:whv_tT_lamda_star}) minimizes the (finite-horizon) objective \eqref{eq:L_T} amongst all coordination policies that are restricted to draw actions based on ASCS, ASPS, and ASPS-based prescriptions over the horizon $[T]$, i.e.,
\begin{align*}
    L_T\l( \utn{\wh{v}}{1:T}{\lambda, \star}, \lambda \r) &= \inf_{\wh{v}\in\wh{\vvspace}( \varthetahtn{1:T}{0:N} )} L_T\l( v, \lambda \r).
\end{align*}
A natural question is to estimate the finite-horizon optimality gap of the \emph{ASPS-ASCS coordination policy} obtained from Algorithm \ref{alg:dp_approximatestate} from the one obtained via Algorithm \ref{alg:cidecomposition}.


%---------------------------------------------------------%
%--OPTIMALITY GAP--%
%---------------------------------------------------------%
\begin{prop}[Optimality Gap for Finite-Horizon]\label{prop:optimalitygap_T}
Fix $ \lambda \in \mcl{Y}$, $\alpha\in(0,1]$, $T \in \mbb{N}$,  
and $T$-horizon ASPS and ASCS generators $\varthetahtn{1:T}{0:N}$ (see Definitions \ref{dfn:asps_generator_finite_horizon} and \ref{dfn:ascs_generator_finite_horizon}). 
Suppose that Assumption \ref{assmp:boundedcosts} holds. Then, for any $t\in[T]$ and any $\hstn{\wt{h}}{t}{0} \in \wthstnspace{t}{0}$ with $\gt{t}^\star \in \argmin_{\gt{t} \in \gtspace{t} } \Qtl{t,T}{\lambda} \l( \hstn{\wt{h}}{t}{0}, \gt{t} \r)$, there exists $\lamdaht{t} \in \lamdahtspace{t}$ such that
\begin{align*}
&\whQtl{t,T}{\lambda} \l( \varthetahtn{t}{0}\l( \hstn{\wt{h}}{t}{0} \r), \lamdaht{t} \r) - \Qtl{t,T}{\lambda}\l( \hstn{\wt{h}}{t}{0}, \gt{t}^\star \r) \\
&\hspace{80pt} \le M_c\l(t; \alpha, T \r) + M_p \l(t; \alpha, T \r),\\
&\whVtl{t,T}{\lambda} \l( \varthetahtn{t}{0}\l( \hstn{\wt{h}}{t}{0} \r) \r) - \Vtl{t,T}{\lambda}\l( \hstn{\wt{h}}{t}{0} \r) \\
&\hspace{80pt} \le M_c\l(t; \alpha, T \r) + M_p\l(t; \alpha, T \r),
\end{align*}
where,
\begin{align*}
%----------
%---------- common part
%----------
&M_c \l(t; \alpha, T \r) = M_c^{(\eps_{c,1}, \eps_{c,2}, \delta_c, \lambda, \ulbar{c}, \ulbar{d}, 
\constraintv)} \l( t; \alpha, T \r) \\
&\hspace{10pt} \defeq \l( \eps_{c,1} + \|\lambda\|_1 \eps_{c,2} \r) \\ 
&\hspace{20pt} + \alpha \l( \sum_{\tau = t+1}^{T} \alpha^{T-\tau} \r) \l[ \l( \eps_{c,1} + \eps_{c,2} \| \lambda \|_1 + N\l( \alpha, T\r) \delta_c \r) \r],\numberthis\label{eq:M_c}\\
%----------
%------------- private part
%----------
&M_p \l(t; \alpha, T \r) = M_p^{(\eps_{p,1}, \eps_{p,2}, \delta_p, \lambda, \ulbar{c}, \ulbar{d}, 
\constraintv)} \l(t; \alpha, T \r) \\
&\hspace{10pt} \defeq \l( \eps_{p,1} + \|\lambda\|_1 \eps_{p,2} \r)\l( \sum_{\tau=t}^{T} \alpha^{T-\tau} \r) \\
&\hspace{20pt} + \alpha \l( \sum_{i=0}^{T-t-1} \sum_{j=0}^{T-t-1-i} \alpha^{i+j} \r) \l[ \l( \eps_{p,1} + \|\lambda\|_1 \eps_{p,2} \r. \r. \\
&\hspace{150pt} \l. \l. + N \l( \alpha, T \r)\delta_p \r)  \r],\numberthis\label{eq:M_p}\\
%------------ definition of N(.)
&N\l( \alpha, T \r) = N^{(\lambda,\ulbar{c},\ulbar{d},\constraintv)} \l( \alpha, T \r) \\
&\hspace{0pt} \defeq  \sum_{\tau=1}^{T} \alpha^{\tau-1} \l[ \ulbar{c} + \|\lambda\|_1 \l( \ulbar{d} + \frac{1}{2} \l( \max_k \constraintv_k - \min_k \constraintv_k \r) \r)  \r].\numberthis\label{eq:N_alpha_T}
\end{align*}
\end{prop}
\begin{proof}
The proof follows the same methodology as in \cite{hsu22}[Theorem 7]. For brevity, it is omitted. It is helpful to note that the main result of \cite{hsu22} (namely Theorem 7) follows as a special case of this proposition (take $\alpha=1$ and $\lambda = 0$).
\end{proof}
%---------------------------------------------------------%
%--OPTIMALITY GAP--%
%---------------------------------------------------------%
Propositions \ref{prop:VtT_vs_Vt} and \ref{prop:optimalitygap_T} yield the following corollary.
\begin{cor}\label{cor:optimalitygap_T}
    Fix $\lambda\in\mcl{Y}$, $\alpha\in(0,1)$, $T \in \mbb{N}$, and $T$-horizon ASPS and ASCS generators $\varthetahtn{1:T}{0:N}$ (see Definitions \ref{dfn:asps_generator_finite_horizon} and \ref{dfn:ascs_generator_finite_horizon}). Suppose that Assumption \ref{assmp:boundedcosts} holds. Then, for any $t\in[T]$ and any $\hstn{\wt{h}}{t}{0} \in \wthstnspace{t}{0}$, the following relation holds between $\whVtl{t,T}{\lambda} \l( \varthetahtn{t}{0}\l(\hstn{\wt{h}}{t}{0}\r) \r) $ and $\Vtl{t}{\lambda} \l( \hstn{\wt{h}}{t}{0} \r) $.
    \begin{align*}
        &\whVtl{t,T}{\lambda} \l( \varthetahtn{t}{0}\l(\hstn{\wt{h}}{t}{0}\r) \r) + \frac{\alpha^{T-t+1}}{1-\alpha} \un{\udl{l}}{\lambda} \\
        &\hspace{30pt} - M_c\l(t;\alpha, T \r) - M_p\l(t;\alpha, T \r) \le \Vtl{t}{\lambda} \l( \hstn{\wt{h}}{t}{0} \r) \\
        &\hspace{90pt} \le \whVtl{t,T}{\lambda} \l( \varthetahtn{t}{0}\l(\hstn{\wt{h}}{t}{0}\r) \r) + \frac{\alpha^{T-t+1}}{1-\alpha} \un{\ov{l}}{\lambda}.
    \end{align*}
\end{cor}
\begin{rem}\label{rem:universalbound_optimalitygap}
Assumption \ref{assmp:slatercondition} can be combined with Corollary \ref{cor:optimalitygap_T} to get $\lambda$-independent bounds on the optimality gap. 
\end{rem}

\subsection{Approximate State Representations (Infinite-Horizon)}
We will now extend the notions from the finite-horizon setting to the infinite-horizon case. In doing so, synonymous to the standard MDP literature, our goal is to identify an ASPS-ASCS based fixed-point iteration scheme that can approximate the optimal value function $\Vtl{t}{\lambda}$ and return a policy that depends, in a time-homogeneous way, on the approximate-state processes $\{ \Zhtn{t}{0:N}\}_{t=1}^{\infty}$.

\begin{dfn}[Infinite-Horizon ASPS-Generator]\label{dfn:asps_generator_infinite_horizon}
Let $\zhnspace{1:N}$ be a collection of topological spaces. A collection $ \varthetahn{1:N} $ of compression-functions where $\varthetahn{n}$ maps $\cup_{t=1}^{\infty} \hstnspace{t}{0} \times \hstnspace{t}{n}$ to $\zhnspace{n}$ is called an infinite-horizon $\l(\eps_{p,1}, \eps_{p,2}, \delta_p \r)$-ASPS generator if the process $\{ \Zhtn{t}{1:N} \}_{t=1}^{\infty}$, with $\Zhtn{t}{n} = \varthetahn{n}( \Hstn{t}{0}, \Hstn{t}{n} ) $ almost-surely, satisfies ASPS-1, ASPS-2.1, ASPS-2.2, and ASPS-3 along with the addition that the evolution functions do not depend on time, i.e., $\phihtn{t}{n} = \phihtn{t'}{n}$.
\end{dfn}

\begin{dfn}[Infinite-Horizon ASCS-Generator]\label{dfn:ascs_generator_infinite_horizon}
Let $\zhnspace{0}$ be a topological space. A compression-function $ \varthetahn{0} $ where $\varthetahn{0}$ maps $\cup_{t=1}^{\infty}\wthstnspace{t}{0}$ to $\zhnspace{0}$ is called an infinite-horizon $\l(\eps_{c,1}, \eps_{c,2}, \delta_c \r)$-ASCS generator if the process $\{ \Zhtn{t}{0} \}_{t=1}^{\infty}$, with $\Zhtn{t}{0} = \varthetahn{0}( \wtHstn{t}{0} ) $ almost-surely, satisfies ASCS-1, ASCS-2.1, ASCS-2.2, and ASCS-3, along with the addition that 
\begin{enumerate}
    \item the evolution functions do not depend on time, i.e., $\phihtn{t}{0} = \phihtn{t'}{0}$; and
    \item the conditional expectations $ \mbb{E}_{P_1} \l[ c\l( \Stt{t}, \At{t} \r) \Big| \zhtn{t}{0}, \lamdaht{t} \r] $ and $ \mbb{E}_{P_1} \l[ d\l( \Stt{t}, \At{t} \r) \Big| \zhtn{t}{0}, \lamdaht{t} \r] $ do not depend on time.
\end{enumerate}

\end{dfn}
Let us denote the range of $\varthetahn{n}$ by $\zhnspaces{n}$, i.e., $\zhnspaces{0} \defeq \varthetahn{0} \l( \cup_{t=1}^{\infty} \wthstnspace{t}{0} \r)$ and $\zhnspaces{n} \defeq \varthetahn{n} \l( \cup_{t=1}^{\infty} \hstnspace{t}{0} \times \hstnspace{t}{n} \r)$ for all $n \in [N]$. Then the above definition leads to a time-invariant ASPS-based prescription space 
$\lamdahspace = \lamdahnspace{1:N}$ where $ \lamdahnspace{n} \defeq \{ \lamdahn{n} : \zhnspaces{n} \mapsto \m{\anspace{n}} \}$. We note that $\lamdahspace$ is in one-to-one correspondence with the set
\begin{align*}
    \xtspace{\lamdahspace} \defeq \prod_{n=1}^{N} \prod_{ \zhn{n} \in \zhnspaces{n} } \m{\anspace{n}; \zhn{n}}.\numberthis\label{eq:xlamdahspace}
\end{align*}
which is a compact (and convex) space by Tychonoff's theorem (see Proposition \ref{prop:tychonoff}), and is also metrizable (see Proposition \ref{prop:metrizability}). From hereon, we will assume $\lamdahspace$ to have the same topology as $\xtspace{\lamdahspace}$.

Definitions \ref{dfn:asps_generator_infinite_horizon} and \ref{dfn:ascs_generator_infinite_horizon} 
naturally lead to an approximate Bellman-type operator $\wh{B} : \{ \zhnspaces{0} \ra \mbb{R} \} \ra \{\zhnspaces{0} \ra \mbb{R}\} $ defined as follows: for a uniformly bounded function $\whVl{\lambda} : \zhnspaces{0} \ra \mbb{R}$,
\begin{align*}
    \l[ \wh{B} \whVl{\lambda} \r] \l( \zhn{0} \r) &\defeq \min_{\lamdah \in \lamdahspace} \mbb{E}_{P_1} \l[ \lCost \r. \\
    &\hspace{-30pt} \l. + \alpha \whVl{\lambda}\l( \Zhtn{t+1}{0} \r) \Big| \Zhtn{t}{0} = \zhtn{t}{0}, \Lamdaht{t} = \lamdah \r].\numberthis\label{eq:whB_whV}
\end{align*}
Note that with the definitions of infinite-horizon ASPS and ASCS (and the time-homogeneous prescriptions), the expectation on the right-hand-side in \eqref{eq:whB_whV} does not depend on time, and due to discounting ($\alpha\in(0,1)$), the operator $\wh{B}$ is a contraction under the supremum norm. Therefore, under Assumption \ref{assmp:boundedcosts}, the fixed point equation $\whVl{\lambda} = \wh{B} \whVl{\lambda}$ has a unique bounded solution according to the Banach fixed-point theorem, (the vector space of uniformly bounded functions with the supremum norm is a complete metric space, see Proposition \ref{prop:banach}).

\begin{thm}[Optimality Gap for Infinite-Horizon]\label{thm:optimalitygap}
Fix $\alpha\in(0,1)$ and $\lambda\in\mcl{Y}$, and infinite-horizon ASPS and ASCS generators $\varthetahn{0:N}$ (see Definitions \ref{dfn:asps_generator_infinite_horizon} and \ref{dfn:ascs_generator_infinite_horizon}). 
Suppose that Assumption \ref{assmp:boundedcosts} holds. Consider the fixed point equation \eqref{eq:whB_whV} which we rewrite as follows:
\begin{align*}
     \whVl{\lambda} \l( \zhn{0} \r) &\defeq \min_{\lamdah \in \lamdahspace} \whQl{\lambda} \l( \zhn{0}, \lamdah \r),\numberthis \label{eq:whVl} \\ 
    \whQl{\lambda} \l( \zhn{0}, \lamdah \r) &\defeq \mbb{E}_{P_1} \l[ \lCost \r. \\
    &\hspace{-30pt} \l. + \alpha \whVl{\lambda}\l( \Zhtn{t+1}{0} \r) \Big| \Zhtn{t}{0} = \zhtn{t}{0}, \Lamdaht{t} = \lamdah \r].\numberthis\label{eq:whQl}
\end{align*}
Let $ \whVl{\lambda,\star}$ denote the fixed-point of \eqref{eq:whB_whV} and $\whQl{\lambda, \star}$ denote the corresponding prescription-value function. Then, for any $t\in\mbb{N}$ and any $\hstn{\wt{h}}{t}{0} \in \wthstnspace{t}{0}$ with $\gt{t}^\star \in \argmin_{\gt{t} \in \gtspace{t} } \Qtl{t}{\lambda} \l( \hstn{\wt{h}}{t}{0}, \gt{t} \r)$, there exists $\lamdah \in \lamdahspace$ such that
\begin{align*}
&\whQl{\lambda,\star}\l( \varthetahn{0} \l( \hstn{\wt{h}}{t}{0} \r), \lamdah \r) - M_c \l( \alpha, \infty \r) - M_p\l( \alpha, \infty \r) \\
&\hspace{40pt} \le \Qtl{t}{\lambda}\l( \hstn{\wt{h}}{t}{0}, \gt{t}^\star \r) \le \whQl{\lambda,\star}\l( \varthetahn{0} \l( \hstn{\wt{h}}{t}{0} \r), \lamdah \r),\\
&\whVl{\lambda}\l( \varthetahn{0} \l( \hstn{\wt{h}}{t}{0} \r) \r) - M_c\l( \alpha, \infty \r) - M_p\l( \alpha, \infty \r) \\
&\hspace{40pt} \le \Vtl{t}{\lambda}\l( \hstn{\wh{h}}{t}{0} \r) \le \whVl{\lambda}\l( \varthetahn{0} \l( \hstn{\wt{h}}{t}{0} \r) \r),
\end{align*}
where $\Vtl{t}{\lambda}$ and $\Qtl{t}{\lambda}$ were defined in \eqref{eq:V_t_lamda} and \eqref{eq:Q_t_lamda}, respectively.
\end{thm}

\begin{proof}
Consider the following sequence of value-functions: $^{0}\whVl{\lambda} \equiv 0$ and for $i\in \mbb{N}$, $^{i+1}\whVl{\lambda} = \wh{B} \l(^{i}\whVl{\lambda}\r)$. Let $T \in [t,\cdot]_{\mbb{Z}}$; by construction of $\l\{^{i}\whVl{\lambda}\r\}_{i=0}^{\infty}$, it follows that $^{T-t+1}\whVl{\lambda} = \whVtl{t,T}{\lambda} $. Therefore, from Corollary 5.1, 
\begin{align*}
    &^{T-t+1}\whVl{\lambda} \l( \varthetahtn{t}{0}\l(\hstn{\wt{h}}{t}{0}\r) \r) + \frac{\alpha^{T-t+1}}{1-\alpha} \un{\udl{l}}{\lambda} \\
    &\hspace{20pt} - M_c\l( \alpha, T \r) - M_p\l( \alpha, T \r) \le \Vtl{t}{\lambda} \l( \hstn{\wt{h}}{t}{0} \r)\\
    &\hspace{40pt} \le ^{T-t+1}\whVl{\lambda} \l( \varthetahtn{t}{0}\l(\hstn{\wt{h}}{t}{0}\r) \r) + \frac{\alpha^{T-t+1}}{1-\alpha} \un{\ov{l}}{\lambda}.
    \end{align*}
Taking the limit $T\ra\infty$, we get
\begin{align*}
    &\whVl{\lambda,\star} \l( \varthetahtn{t}{0}\l(\hstn{\wt{h}}{t}{0}\r) \r) - M_c\l(\alpha, \infty\r) - M_p\l(\alpha, \infty\r)  \\
    &\hspace{20pt} \le \Vtl{t}{\lambda} \l( \hstn{\wt{h}}{t}{0} \r) \le \whVl{\lambda, \star} \l( \varthetahtn{t}{0}\l(\hstn{\wt{h}}{t}{0}\r) \r).
\end{align*}
By choosing $\lamdah \in \lamdahspace$ to be the minimizing prescription in the corresponding prescription-value function $\whQl{\lambda,\star}$, we get the inequality in \eqref{eq:whQl}.
\end{proof}

Theorem \ref{thm:optimalitygap} gives us a time-homogeneous ASPS-ASCS based coordination policy that is approximately optimal for the $\lambda$-parametrized infinite-horizon objective $L_{\infty}\l( \cdot, \lambda \r)$. Specifically, let $\ut{\wh{v}} = \ut{\wh{v}}{1:\infty} $ be such that for all $t\in\mbb{N}$, $\ut{\wh{v}}{t}\l( \varthetahn{0} \l( \hstn{\wt{h}}{t}{0} \r) \r) \in \argmin_{\lamdah \in \lamdahspace} \whQl{\lambda,\star} \l( \varthetahn{0} \l( \hstn{\wt{h}}{t}{0} \r) \r)$. Then,
\begin{align*}
L_\infty \l( \wh{v}, \lambda \r) - \inf_{v\in \vvspace} L_\infty \l( v, \lambda \r) \le M_c\l( \alpha, \infty \r) + M_p\l(\alpha, \infty \r).
\end{align*}