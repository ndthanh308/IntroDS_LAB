{
  "title": "Cooperative Multi-Agent Constrained POMDPs: Strong Duality and Primal-Dual Reinforcement Learning with Approximate Information States",
  "authors": [
    "Nouman Khan",
    "Vijay Subramanian"
  ],
  "submission_date": "2023-07-31T10:02:30+00:00",
  "revised_dates": [],
  "abstract": "We study the problem of decentralized constrained POMDPs in a team-setting where the multiple non-strategic agents have asymmetric information. Strong duality is established for the setting of infinite-horizon expected total discounted costs when the observations lie in a countable space, the actions are chosen from a finite space, and the immediate cost functions are bounded. Following this, connections with the common-information and approximate information-state approaches are established. The approximate information-states are characterized independent of the Lagrange-multipliers vector so that adaptations of the multiplier (during learning) will not necessitate new representations. Finally, a primal-dual multi-agent reinforcement learning (MARL) framework based on centralized training distributed execution (CTDE) and three time-scale stochastic approximation is developed with the aid of recurrent and feedforward neural-networks as function-approximators.",
  "categories": [
    "math.OC",
    "eess.SY"
  ],
  "primary_category": "math.OC",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.16536",
  "pdf_url": "https://arxiv.org/pdf/2307.16536v1",
  "comment": "arXiv admin note: substantial text overlap with arXiv:2303.14932",
  "num_versions": null,
  "size_before_bytes": 3476637,
  "size_after_bytes": 2943089
}