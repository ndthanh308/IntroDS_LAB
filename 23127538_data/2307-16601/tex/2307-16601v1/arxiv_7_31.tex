\documentclass[10pt,twocolumn,letterpaper]{article}


\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{amstext}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{color}
\usepackage{newfloat}
\usepackage{listings}
\usepackage{bibentry}

\usepackage[table]{xcolor}
\usepackage{utfsym}


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false,citecolor=cyan]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{3358} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi


\begin{document}


%%%%%%%%% TITLE
\title{Sampling to Distill: Knowledge Transfer from Open-World Data}


\author{Yuzheng Wang$^{1}$ $\quad$  Zhaoyu Chen$^{1}$ $\quad$  Jie Zhang$^{2}$ $\quad$  Dingkang Yang$^{1}$ $\quad$  Zuhao Ge$^{1}$\\   Yang Liu$^{1}$ $\quad$  Siao Liu$^{1}$ $\quad$   Yunquan Sun$^{1}$ $\quad$  Wenqiang Zhang$^{1}$ $\quad$   Lizhe Qi$^{1}$ \\
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
$^{1}$Academy for Engineering and Technology, Fudan University $\quad$ 
$^{2}$\;\!ETH Zurich \\
}





\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%\renewcommand{\thefootnote}{\fnsymbol{footnote}} 
%\footnotetext[2]{Corresponding authors.} 


%%%%%%%%% ABSTRACT
\begin{abstract}
Data-Free Knowledge Distillation (DFKD) is a novel task that aims to train high-performance student models using only the teacher network without original training data.
Despite encouraging results, existing DFKD methods rely heavily on generation modules with high computational costs. 
Meanwhile, they ignore the fact that the generated and original data exist domain shifts due to the lack of supervision information. 
Moreover, knowledge is transferred through each example, ignoring the implicit relationship among multiple examples.
To this end, we propose a novel Open-world Data Sampling Distillation (ODSD) method without a redundant generation process.
First, we try to sample open-world data close to the original data's distribution by an adaptive sampling module.
Then, we introduce a low-noise representation to alleviate the domain shifts and build a structured relationship of multiple data examples to exploit data knowledge.
Extensive experiments on CIFAR-10, CIFAR-100, NYUv2, and ImageNet show that our ODSD method achieves state-of-the-art performance. 
Especially, we improve 1.50\%-9.59\% accuracy on the ImageNet dataset compared with the existing results.
% In specific settings, it allows students to surpass the teachers' performance for the first time in DFKD, demonstrating its effectiveness.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Deep learning has made refreshing progress in various computer vision fields \cite{dosovitskiy2020image,yang2023context,liu2023generalized,yang2022emotion,chen2022towards,liu2022efficient,wang2023adversarial,chen2022shape,chen2023query}.
Despite success, large-scale models \cite{devlin2018bert,karras2019style,liu2021swin,liu2022collaborative,yang2023target,chen2023content,liu2022appearance} and unavailable privacy data \cite{burton2015data,sun2017revisiting,yang2023aide,wang2023model} often impede the application of advanced technology on mobile devices.
Therefore, model compression and data-free technology have become the key to breaking the bottleneck.
To this end, Lopes \etal \cite{lopes2017data} propose Data-Free Knowledge Distillation (DFKD).
In this process, knowledge is transferred from the cumbersome model to a small model that is more suitable for deployment without using the original training dataset.
As a result, this widely applicable technology has gained much attention.

%\cite{hinton2015distilling} proposes a lightweight framework named knowledge distillation.
%Knowledge is transferred from the cumbersome model to a small model that is more suitable for deployment.
%Despite its good performance, the training process depends on the original data.
%Such issues may hinder the further development of this technology.
%Therefore, \cite{lopes2017data} proposes data-free knowledge distillation (DFKD) to tackle the problem.

%, which refers to the situation where only the pre-trained cumbersome teacher model is available. However, the original training data is not available. 


% Figure environment removed


To replace unavailable private data and effectively train small models, most existing data-free knowledge distillation methods relied on alternately training of the generator and the student, called the generation-based method.
%Specifically, these methods adversarially updates the generator and student network. 
Despite not using the original training data, these generation-based methods have many issues. 
First, their trained generators are abandoned after the students' training \cite{chen2019,fang2019,micaelli2019zero,hao2021data,do2022momentum,zhang2022dense}.
The training of generators brings additional computational costs, especially for large datasets.
For instance, a thousand generators are trained for the ImageNet dataset \cite{deng2009imagenet}, which introduces more computational waste \cite{luo2020large,fang2022up}.
Then, large domain shifts exist between the generated data and the original data. 
The substitute data are composed of random noise transformation without supervision information.
Hence, the substitute domain usually does not match the unavailable original data domain and includes extensive label noise predicted by the teacher \cite{ye2020data}. 


%(3) These methods can be summarized as a distillation method of the student to mimic the outputs of a particular data example represented by the teacher.
%However, exploiting knowledge in substitute data is essential to complement missing supervision information (\cite{yim2017gift,park2019relational}).
%As a result, previous methods ignore this issue, which leads to the lack of effective knowledge expression in the distillation process.


% To overcome the disadvantages of the generation-based method, (\cite{chen2021learning}) proposes a method of training the student network via open-world sample unlabeled data (sampling-based method) without additional generation calculation costs.
Rather than relying on generation-based methods, Chen \etal \cite{chen2021learning} propose a sampling-based method for training the student network via open-world unlabeled data without the generation calculations.
% Figure \ref{fig1} shows the difference between the two methods.
Compared with generation-based methods, sampling-based methods can avoid the training cost of generators.
The comparison of the two methods is shown in Figure~\ref{fig1}.
Meanwhile, they try to reduce label noise by updating the learnable noise matrix, but the noise matrix's computational costs are expensive.
More importantly, their sampling method only relies on strict confidence ranking and does not consider the data domain similarity problem, so the domain shift problem is still severe.
In addition, the existing generation-based and sampling-based methods can be summarized as the distillation methods of the student to mimic the outputs of a particular data example represented by the teacher \cite{yim2017gift,park2019relational,wang2023explicit}.
Therefore, these methods do not adequately utilize the implicit relationship among multiple data examples, which leads to the lack of effective knowledge expression in the distillation process.



Based on the above observations, we construct a novel sampling-based method to avoid unnecessary computational costs. 
The difference is that we hope to mitigate the domain shifts issue better and exploit the relationship among multiple samples.
To cope with the domain shifts issue between the open-world and source data, we propose a comprehensive solution to it from two aspects.
Firstly, we preferentially try to sample data with similar distribution to the original data domain to reduce the shifts.
Secondly, low-noise knowledge representation learning is introduced to suppress the interference of label noise.
To explore the data knowledge adequately, we set up a structured representation of unlabeled data to enable the student to learn the implicit knowledge among multiple data examples.
As a result, the student can learn from carefully sampled unlabeled data instead of absolutely relying on the teacher.
At the same time, to explore an effective distillation process, we introduce a contrastive structured relationship between the teacher and student.
The student can make better progress through the structured prediction of the teacher network.


In this paper, we consider a solution of DFKD that does not require additional generation costs.
On the one hand, we hope to find a solution to data domain shifts from both data source and distillation methods.
On the other hand, we try to explore an effective structured knowledge representation method to deal with the issues of lack of supervision information and the training difficulties in DFKD scenes.
Therefore, we propose an Open-world Data Sampling Distillation (ODSD) method, which includes Adaptive Prototype Sampling (APS) and Denoising Contrastive Relational Distillation (DCRD) modules.
Specifically, the primary contributions and experiments are summarized as follows: 

\begin{itemize}
    \item We propose an Open-world Data Sampling Distillation (ODSD) method. The method does not require additional training of one or more generation modules, thus avoiding unnecessary computational costs.

    \item Considering the domain shifts between the open-world and source data, we introduce an Adaptive Prototype Sampling (APS) mechanism to obtain data closer to the original data distribution.
    
    \item We propose a Denoising Contrastive Relational Distillation (DCRD) module, which utilizes a low-noise representation to suppress label noise and builds contrast structured relationships to exploit knowledge from data and the teacher adequately.
    
    \item Experiments show that the proposed ODSD method improves the current state-of-the-art (SOTA) in various benchmarks. In particular, our method improves 1.50\%-9.59\% accuracy on the ImageNet dataset.

    
\end{itemize}


\section{Related Work}

\subsection{Data-Free Knowledge Distillation}
Data-free knowledge distillation is proposed to deal with the problem of a lightweight model when the original data are unavailable. 
Therefore, substitute data are indispensable to help transfer knowledge from the cumbersome teacher to the flexible student.
According to the source of these data, existing methods are divided into generation-based and sampling-based methods.

%\subsubsection{Generation-based Methods.}
\noindent\textbf{Generation-based Methods.}
The generation-based methods depend on the generation module to synthesize the substitute data.
Lopes \etal \cite{lopes2017data} propose the first generation-based DFKD method, which uses the data means to fit the training data. 
Due to the weak generation ability, it can only be used on a simple dataset such as the MNIST dataset.
The following methods combine the Generative Adversarial Networks (GANs) to generate more authentic and reliable data.
Chen \etal \cite{chen2019} firstly put the idea into practice and define an information entropy loss to increase the diversity of data.
However, this method relies on a long training time and a large batch size.
Fang \etal \cite{fang2019} suggest forcing the generator to synthesize images that do not match between the two networks to enhance the training effect.
Hao \etal \cite{hao2021data} suggest using multiple pre-trained teachers to help the student, which leads to additional computational costs.
Do \etal \cite{do2022momentum} propose a momentum adversarial distillation method to help the student recall past knowledge and prevent the student from adapting too quickly to new generator updates.
The same domain typically shares some reusable patterns, so Fang \etal \cite{fang2022up} introduce the sharing of local features of the generated graph, which speeds up the generation process.
Since the generation quality is still not guaranteed, some methods spend extra computational costs on gradient inversion to synthesize more realistic data \cite{yin2020dreaming, fang2021contrastive}.
In addition, Choi \etal \cite{choi2020data} combine DFKD with other compression technologies and achieve encouraging performance.
However, generation-based DFKD methods generate a large number of additional calculation costs in generation modules, while these modules will be discarded after students' training \cite{chen2021learning}.


% Figure environment removed



%\subsubsection{Sampling-based Methods.}
\noindent\textbf{Sampling-based Methods.}
To train the student more exclusively, Chen \etal \cite{chen2021learning} propose to sample unlabeled data to replace the unavailable data without the generation module.
Firstly, they use a strict confidence ranking to sample unlabeled data.
Then, they propose a simple distillation method with a learnable adaptive matrix.
Despite no additional training costs and promoting encouraging results, their method ignores the intra-class relationships of multiple unlabeled data.
Simultaneously, the simple strict confidence causes more data to be sampled for simple classes, leading to imbalanced data classes.
In addition, their proposed distillation method is relatively simple and lacks structured relationship expression, which limits the student's performance.



\subsection{Contrastive Learning}
Contrastive learning makes the model's training efficient by learning the data differences \cite{ye2019unsupervised}.
The unsupervised training usually requires to store negative data by a memory bank \cite{wu2018unsupervised}, large dictionaries \cite{he2020momentum}, or a large batch size \cite{chen2020simclr}. 
Even it requires a lot of computation, additional normalization \cite{grill2020bootstrap}, and network update operations \cite{caron2021emerging}. 
The high storage and computing costs seriously reduce knowledge distillation efficiency. 
But at the same time, this idea of mining knowledge in unlabeled data may be helpful for the student's learning.
Due to such technical conflicts, there are few methods to perfectly combine knowledge distillation and contrastive learning in the past.
As a rare attempt, Tian \etal \cite{tian2019contrastive} propose a contrastive data-based distillation method by an update a large memory bank. 
But for data-free knowledge distillation, the data quality cannot be guaranteed, and data domain shifts are intractable, which makes the above process challenging to carry out.


In this work, we attempt to explore additional knowledge from both data and the teacher.
Therefore, we further stimulate students' learning ability by using the internal relationship of unlabeled data and constructing a structured contrastive relationship.
To our knowledge, this is the first combination of data-free knowledge distillation and contrastive learning at a low cost, which achieves an unexpected effect.


\section{Methodology}

\subsection{Overview}

Considering the existing issues, our pipeline includes two stages: 1) unlabeled data sampling and 2) distillation training, as shown in Figure \ref{fig2}.
For the first stage, we sample unlabeled data by an adaptive sampling mechanism to obtain data closer to the original distribution. 
For the second stage, the student learns the knowledge representation after denoising through a spatial mapping denoise module.
Further, we mine more profound knowledge of the unlabeled data and build the structured relational distillation to help the student gain better performance.
The complete algorithm is shown in \textbf{Supplementary Sec.3}.


\subsection{Adaptive Prototype Sampling}

\iffalse
The class and scale of the unavailable source dataset and the unlabeled dataset are different in many cases, so there is a severe issue of data domain shifts. 
To this end, we aim to find unlabeled data that are closer to the distribution of source domain data.
%An adaptive prototype selection (APS) method is proposed. 
%We measure the value of unlabeled samples by defining the sample and class scores.
Therefore, we propose an Adaptive Prototype Sampling (APS) method that measures the value of unlabeled data by defining the data and class scores.

Specifically, firstly the pre-trained teacher model predicts the unlabeled dataset $P=[p_{1},p_{2},...,p_{s}]\in \mathbb{R}^{s\times C}$. 
$s$ is the number of data for the unlabeled dataset, and $C$ is the number of classes in the teacher's prediction.
The high confidence prediction for unlabeled data indicates that the data is more likely to be an acquaintance and contains more confident knowledge. 
To measure the confidence score of the data, the prediction is converted into probability of the unified scale as: ${p_{i}}'\!=\!softmax(p_{i}), \; \tilde{p}_{i}\!=\!\mathop{\arg\max}({p_{i}}')$. $\mathop{\arg\max}(p')$ denotes the confidence probability corresponding to the predicted result class. 
The confidence score can be calculated as:
$sc_{i} \!= \! \frac{\tilde{p}_{i}}{\left |\max\left \{\tilde{p} \right \}  \right | }$, where $\tilde{p}=[\tilde{p}_{1},\tilde{p}_{2},...,\tilde{p}_{s}]$, $\max$ denotes maximum value in the set.

Secondly, we classify the data according to the teacher's prediction and cluster the predicted results of each class to explore the intra-class relationships through prototype learning.
We refer to a group of $C\!K$ data sampling prototypes, i.e., $\left \{ \mu_{c,k}\in \mathbb{R}^{C} \right \} _{c,k=1}^{C,K} $, which are based solely on the subcenter of target classes. 
$C$ is the total number of classes, and $K$ is the number of prototypes defined for each class.
%c \in \left \{ 1,...,C \right \}
Further, each class $c $ is represented by $K$ prototypes $\left \{ \mu_{c,k} \right \} _{k=1}^{K} $, and prototype $\mu_{c,k}$ denotes the center of $k$-th sub-cluster of class $c$.
The predicted vector of each data is classified as $\rho _{i,c} = {p_{i}\in c}$.
Then, the intra-class outliers of each data can be calculated as $\tilde{o}_{i}= \cos(\rho _{i,c}, \mu_{c,k}) _{k=1}^{K} $, where $\cos$ denotes the cosine similarity.
Therefore, the outlier score can be calculated as:
$so_{i} \! =\! \frac{\tilde{o}_{i}}{\left |\max\left \{\tilde{o} \right \}  \right | }$.
%\vspace{-0.2cm}

Finally, we calculate the class density to better meet the balance of the data.
The class density $ D_{c} $ is calculated by the intra-class outliers mean $u_{c}$, the number of data predicted of this class $n_{c}$, and the total number of classes $C$ as: $D_{c} \! = \! \frac{\sqrt{u_{c}} }{\log_{e}{(n_{c}+C)}}$, where $u_{c} = \frac{1}{n_{c} } \sum \, _{p_{i} \in c}\tilde{o}_{i}$.
The class density value can be converted to the data density value as $d_{i} \!= \! D_{c}({p_{i} \in c})$.
Then, the class density is converted to the density score of each data $sd_{i}$, and $sd_{i} \!=\! \frac{d_{i}}{\left |\max\left \{ d \right \} \right |}$.


In summary, the sampled data should avoid data with low confidence that are easy to cause label noise and data with significant outliers that are not conducive to learning. 
In addition, the sampled data should meet the class balance to prevent unbalanced learning for the student.
%These samples are closer to the distribution of the original data domain and can help the student learn better.
So we define the data score $ S_{1} $ to measure two formers and the class score $ S_{2} $ to measure the latter.
$ S_{1} \! = \! sc_{i}- so_{i}, \;$
$ S_{2} \! = \! sd_{i}$.
Then the total score for each data $S_{total} $ is calculated as: $S_{total} = S_{1}+S_{2}$.
According to the total score, the data closer to the distribution of the original data domain are sampled, which can help the student learn better.
\fi
%efficient samples can be selected to restrain the influence of data domain offset and help the student learn better.





The class and scale of the unavailable source dataset and the unlabeled dataset are different in many cases, so there is a severe issue of data domain shifts, which will be discussed in Figure~\ref{fig3}.
To this end, we aim to find unlabeled data closer to the distribution of source domain data.
Therefore, we propose an Adaptive Prototype Sampling (APS) method that considers the teacher's familiarity, the intra-class outliers, and the class balance of the unlabeled data.
Based on these, we design three score indicators to evaluate the effectiveness of the unlabeled data for student training: the data confidence score, the data outlier score, and the class density score.


\textbf{(a) Data confidence score.} To sample data with similar distribution to original training data, we try to keep consistent prediction logits of the teacher.
Firstly the teacher provides the prediction logits for the unlabeled dataset as $P=[p_{1},p_{2},...,p_{s}]\in \mathbb{R}^{s\times C}$, where $p_{i}$ is the prediction for a single sample satisfying $\mathbb{R}^{1\times C}$.
$i$ is the $i$-th data, $s$ is the number of data for the unlabeled dataset, and $C$ is the number of classes in the teacher's prediction.
Then the prediction is converted into the probability of the unified scale as: ${p_{i}}'\!=\!softmax(p_{i}), \; \tilde{p}_{i}\!=\!\mathop{\arg\max}({p_{i}}')$, and $\mathop{\arg\max}(p')$ denotes the confidence probability corresponding to the predicted result class. 
Therefore, $\tilde{p}=[\tilde{p}_{1},\tilde{p}_{2},...,\tilde{p}_{s}]$ represents the confidence of each data in the whole dataset.
We choose the largest one $\max\left \{\tilde{p} \right \}$ for normalization.
The confidence score can be calculated as: $sc_{i} \!= \! \frac{\tilde{p}_{i}}{\left |\max\left \{\tilde{p} \right \}  \right | }$.


\textbf{(b) Data outlier score.} The label space of the two data domains is different, so there are edge data, which may affect the student's learning and need to be excluded.
%Due to different labels of the two data domains, lots of unlabeled data are classified as wrong classes because there is no more suitable target class.
For example, we try to exclude data like tigers from the real class of cats, as shown in the orange part of Stage 1 in Figure~\ref{fig2}.
Firstly we separate the data according to the classes predicted by the teacher.
Each class is clustered to explore the intra-class relationships through prototype learning.
Then we refer to a group of $C\!K$ data sampling prototypes, \ie, $\left \{ \mu_{c,k}\in \mathbb{R}^{1 \times C} \right \} _{c,k=1}^{C, K} $, which are based solely on the subcenter of target classes. 
$c$ is the $c$-th class, and $K$ is a hyperparameter that represents the number of prototypes defined for each class.
After clustering \cite{johnson2019billion}, the prediction results of the $c$-th class can be expressed as $K$ prototypes as $\left \{ \mu_{c,k} \right \} _{k=1}^{K}$, which reflect the intra-class relationship of the data predicted as class $c$.
To calculate the outlier of data in class $c$, the predictions of these data are expressed as $\rho _{i,c} = p_{i}$, which couples the predictions and class information.
According to the predictions and the prototype centers of the class $c$, the intra-class outliers of $i$-th data can be calculated as $\tilde{o}_{i}=\textstyle \sum_{k=1}^{K} \cos(\rho _{i,c}, \mu_{c,k}) $, where $\cos$ denotes the cosine similarity.
Similar to the above, we select the maximum value for normalization.
As a result, the outlier score can be calculated as: $so_{i} \! =\! \frac{\tilde{o}_{i}}{\left |\max\left \{\tilde{o} \right \}  \right | }$.
%\vspace{-0.2cm}

\textbf{(c) Class density score.} To better help the student learn various classes, we calculate the class density to better meet the sampled data's balance.
As shown in Stage 1 of Figure~\ref{fig2}, we increase the sampling range for classes with sparse data (the blue part) while we reduce the sampling range for classes with redundant data (the orange part).
Based on this, we firstly separate the above intra-class outliers $\tilde{o}_{i}$ of all data by their classes.
The outliers mean value of each class can be calculated as: $u_{c} = \frac{1}{n_{c} } \sum \, _{p_{i} \in c}\tilde{o}_{i}$, where $n_{c}$ is the number of the data predicted as $c$-th class.
Therefore, the Dcluster parameter $D_{c}$ can be calculated as: $D_{c} \! = \! \frac{\sqrt{u_{c}} }{\log_{e}{(n_{c}+C)}}$.
Each data's density value equals the density value of its class as: $d_{i} \!= \! D_{c}({p_{i} \in c})$.
After selecting the maximum value for normalization, the density score of each data can be calculated as: $sd_{i} \!=\! \frac{d_{i}}{\left |\max\left \{ d \right \} \right |}$.


Finally, we define the total score with $S_{total}$, which is calculated as: $S_{total} \! = \! sc_{i} \!- \! so_{i}  \!+ \! sd_{i}$.
According to the total score, the data closer to the distribution of the original data domain are sampled, which can help the student learn better.
The quantitative analysis is shown in Table \ref{tab6}.

%the sampled data should avoid data with low confidence that are easy to cause label noise and data with significant outliers that are not conducive to learning. 
%In addition, the sampled data should meet the class balance to prevent unbalanced learning for the student.




%These samples are closer to the distribution of the original data domain and can help the student learn better.
%So we define the data score $ S_{1} $ to measure two formers and the class score $ S_{2} $ to measure the latter.
%$ S_{1} \! = \! sc_{i}- so_{i}, \;$
%$ S_{2} \! = \! sd_{i}$.
%Then the total score for each data $S_{total} $ is calculated as: $S_{total} = S_{1}+S_{2}$.
%According to the total score, the data closer to the distribution of the original data domain are sampled, which can help the student learn better.








\subsection{Denoising Contrastive Relational Distillation}

After obtaining the high score data, the distillation process can be carried out.
We denote $f_{T}$ and $ f_{S}$ as the output of the teacher and student networks and denote $x$ as the sampled data. 
According to \cite{hinton2015distilling}, the knowledge distillation loss is calculated as:
%The above process can be expressed as:
$$ \mathcal{L}_{K\!D}=\sum_{x \in \mathcal{X}} D_{K\!L}(f_{T}(x)/\tau, f_{S}(x)/\tau), \eqno{(1)} $$where $D_{K\!L}$ is the Kullback-Leibler divergence, and $\tau$ is the distillation temperature. 
Although $\mathcal{L}_{K\!D}$ allows the student to imitate the teacher's output, only its use leads to poor learning results. 
The main challenge is the distribution differences between the substitute and original data domains, leading to label noise interference.
Simultaneously, the ground-truth labels are unavailable, so correct information supervision is missing. 
Therefore, we propose a Denoising Contrastive Relational Distillation (DCRD) module, which includes a spatial mapping denoise component and a contrastive relationship representation component to help the student get better performance.

%Therefore, the main challenges are to deeply mine the knowledge of unlabeled data and low-computational noise suppression.


%\subsubsection{Manifold Noisy Distillation}
\subsubsection{Spatial Mapping Denoise}
%manifold representation learning
The data distribution in the unlabeled data is different from the unavailable source data, which indicates the label noise is inevitable.
Low dimensional information contains purer knowledge, which is subject to less noise interference \cite{ALGAN2021106771}.
Here, we use a low dimensional spatial mapping denoise component to help the student learn low-noise knowledge representation.
$Z_{t}, Z_{s}$ are the low dimensional representation of teacher and student prediction. 
In order to obtain a distance invariant spatial projection transformation $\Phi$, the autocorrelation matrix $d_{ij}^{2} $ is defined as: $d_{ij}^{2} =\left \| \overrightarrow{f_{T}}(x_{i}) -\overrightarrow{f_{T}}(x_{j})  \right \| =\left \|  \overrightarrow{z_{i}}-\overrightarrow{z_{j} }\right \| = b_{ii}+b_{jj}-2b_{ij}$, where $b_{ij}= \overrightarrow{z_{i}} \cdot \overrightarrow{z_{j}} $. 
We sum $d_{ij}^{2}$ in a mini-batch as:
%\vspace{-0.1cm}
$$ \sum_{i}^{N}\sum_{j}^{N} d_{ij}^{2}= 2N\cdot tr(Z_{t}Z_{t}^{T} ), \eqno{(2)}$$where $N$ denotes the batch size, and $tr(\cdot)$ denotes the trace of a matrix. Then $Z_{t}$ can be calculated as $Z_{t}=V_{t}\Lambda _{t}^{1/2}$, where $V_{t}$ is the eigenvalue after eigendecomposition, and $\Lambda _{t}$ is the eigenmatrix. 
Similarly, we can get the student predictions of low dimensional representation as $Z_{s}$. 
Then, we set up a distillation loss to correct the impact of label noise by the spatial mapping of the two networks.
The spatial mapping denoise distillation loss is calculated as:
$$ \mathcal{L}_{n} =\ell_{h}(\Phi (f_{T}\cdot  f_{T}\!^{T}), \Phi (f_{S}\cdot  f_{S}\!^{T}) )  =\ell_{h}(Z_{t}, Z_{s}), \eqno{(3)}$$where $ \ell_{h} (\cdot , \cdot ) $ denotes the Huber loss. 
We can match the teacher-student relationship in a low dimensional space to learn a low-noise knowledge representation by $\mathcal{L}_{n}$.

%\subsubsection{Data Expansion Distillation}
\subsubsection{Contrastive Relational Distillation}
%In the past, the feature-based distillation methods and logit distillation methods often require careful and structured knowledge construction. 
%Generally speaking, it is difficult to build efficient structured knowledge in these locations to help students well.
%In this work, 

%In data-free knowledge distillation, the alternate samples are unlabeled leading to 
The missing supervision information limits the student's performance.
It is indispensable to adequately mine the knowledge in unlabeled data to compensate for lack of information.
%Therefore, we set the mutual relations of data examples distillation method.
To avoid single imitation of a particular data example, we build two kinds of structured relationship to mine knowledge from data and the teacher.


%In order to overcome the difficulty of constructing effective structured knowledge in the past, 

%First, the selected samples use RandAugust{} to expand knowledge. 
%In the previous selection process, we selected samples that are more efficient.
%In the last part, we propose an efficient samples selection method.
%Usually, these samples are enough to master some relatively simple categories, while other difficult categories may not be well learned due to the deviation between the training domain and the testing domain.

%First, we use RandAugment \cite{cubuk2020randaugment} to expand data knowledge representation. 
%Unlike the previous data-free distillation methods, we hope the student network can learn knowledge directly from data rather than relying entirely on the teacher.
Firstly, the student can adequately explore the structured relation among data by learning the instance invariant.
%Then, the instance invariant is learned to enable the student to explore the structural relation between samples deeply.
$x_{i}, x_{j}$ are the different data in a mini-batch. 
We calculate the prediction difference between data as:
$$ \ell_{s}^{x_{i}x_{j}}=\frac{\cos (f_{S}(x_{i}), f_{S}(x_{j}))  / \tau_{1} }{ {\textstyle \sum_{k=1,k\ne i}^{2N}} \cos (f_{S}(x_{i}), f_{S}(x_{k}))/ \tau_{1} }, \eqno{(4)} $$where $\tau_{1}$ denotes contrastive temperature.
Next, we can calculate the consistency instance discrimination loss as:
%\vspace{-0.1pt}
$$ \mathcal{L}_{c1}=-\frac{1}{N}\sum_{j=1}^{N}  \log_{}{\ell_{s}^{x_{j}x_{\bar{j}}}},  \eqno{(5)}$$where $x_{\bar{j}}$ denotes the data augmentation transform of data $x_{j}$. 
The student can find knowledge directly from the multiple unlabeled data through data consistency learning.
This unsupervised method is especially effective when the teacher makes wrong results.

Secondly, we construct a structured contrastive relationship between the teacher and student, which promotes consistent learning between the teacher and student.
The structured knowledge learning process is calculated as: 
$$ \ell_{ts}^{{x}'_{i}}=\frac{\cos (f_{T}({x}'_{i}), f_{S}({x}'_{i}))  / \tau_{2} }{ {\textstyle \sum_{k=1,k\ne i}^{4N}} \cos (f_{T}({x}'_{i}), f_{S}({x}'_{k}))/ \tau_{2} }, \eqno{(6)} $$where $ {x}' = {x \cup \bar{x}}$. 
Then, we can calculate the teacher-student consistency loss as:
%\vspace{-0.1pt}
$$ \mathcal{L}_{c2}=-\frac{1}{2N}\sum_{j=1}^{2N}  \log_{}{\ell_{ts}^{{x}'_{j}}}.  \eqno{(7)}$$The student can obtain better learning performance through the mixed structured and consistent relationship learning between the two networks. Then, the contrastive relational distillation loss is $\mathcal{L}_{c}\!=\! \mathcal{L}_{c1}+\mathcal{L}_{c2}  $.
Finally, we can get the total denoising contrastive relational distillation loss as:
%\vspace{-0.05cm}
$$
\mathcal{L}_{total}=\mathcal{L}_{K\!D}+\lambda_{1}\! \cdot \!\mathcal{L}_{n}+\lambda_{2}\! \cdot \! \mathcal{L}_{c}, \label{14}  \eqno{(8)}
$$where $\lambda_{1}$, $\lambda_{2}$ are the trade-off parameters for training losses.



\section{Experiments}
\iffalse
In this section, we first clarify the experimental settings. 
Secondly, we use diagnostic experiments to verify the effectiveness of each module.
Further, we compare our proposed DCRD algorithm with state-of-the-art data-free knowledge distillation methods to prove the brilliance of our method.
Finally, we show the partial visualization results.
\fi





\subsection{Experimental Settings}
%\subsubsection{Datasets and Models.} 
\textbf{Datasets and Models.}
We evaluate the proposed ODSD method for the classification and semantic segmentation tasks.
For classification, we evaluate it on widely used datasets: $32 \times 32$ CIFAR-10, CIFAR-100 \cite{krizhevsky2009learning}, and $224 \times 224$ ImageNet \cite{deng2009imagenet}.
In addition, we use the pre-trained models from CMI \cite{fang2021contrastive} and unify the teacher models among all baseline methods. 
The number of sampled data is 150k or 600k for CIFAR, and 600k for ImageNet following DFND \cite{chen2021learning}.
More detailed classification settings are shown in \textbf{Supplementary Sec.1}.
For semantic segmentation, we evaluate the proposed method on $128 \times 128$ NYUv2 dataset \cite{silberman2012indoor}.
200k data are sampled.
More detailed segmentation settings are shown in \textbf{Supplementary Sec.4}. 
Besides, the corresponding open-world datasets are shown in Table~\ref{tab1}, which is the same as DFND \cite{chen2021learning} for a fair comparison. 


\begin{table}[b]
\vspace{-0.2cm}
\centering
%\begin{center}
\tabcolsep=0.3cm
\caption{Illustration of original data and their substitute datasets.}
\vspace{0.1cm}
\begin{tabular}{@{}c|ccc@{}}
\toprule[1pt]
Original data            & CIFAR    & ImageNet & NYUv2    \\
Unlabeled data & ImageNet & Flickr1M & ImageNet \\ \bottomrule[1pt]
\end{tabular}
%\end{center}
\label{tab1}
\end{table}


%\subsubsection{Baselines.} 
\textbf{Baselines.}
We compare two kinds of data-free knowledge distillation methods.
One is to have to spend extra computing costs to obtain generation data by generation module, including: DeepInv \cite{yin2020dreaming}, CMI \cite{fang2021contrastive}, DAFL \cite{chen2019}, ZSKT \cite{micaelli2019zero}, DFED \cite{hao2021data}, DFQ \cite{choi2020data}, Fast \cite{fang2022up}, MAD \cite{do2022momentum}, DFD \cite{luo2020large}, and DFAD \cite{fang2019}. 
Another is to use unlabeled data as the substitute data from easily accessible open source datasets based on sampling, \ie, DFND \cite{chen2021learning}.


\iffalse
\begin{table*}[t]
\centering
%\vspace{-0.2cm}
\renewcommand\arraystretch{1.2}
\setlength{\tabcolsep}{3.4mm}
\scalebox{0.95}{
\begin{tabular}{clcrrrrr}
\toprule[1pt]
\multirow{2}{*}{\textbf{Dataset}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Method}}} & \multirow{2}{*}{\textbf{Type}} & \textbf{ResNet-34} &\textbf{VGG-11}    & \textbf{WRN40-2} & \textbf{WRN40-2} & \textbf{WRN40-2} \\
                         & \multicolumn{1}{c}{}                        &                       & \textbf{ResNet-18} & \textbf{ResNet-18} & \textbf{WRN16-1} & \textbf{WRN40-1} & \textbf{WRN16-2} \\ \hline
\multirow{13}{*}{\rotatebox{90}{\textbf{CIFAR-10}}}  & Teacher & \multirow{3}{*}{-}                                & 95.70     &     92.25      &     94.87    &   94.87      &     94.87  \\
                           & Student &                                 & 95.20 & 95.20 & 91.12 & 93.94 & 93.95 \\
                           & KD      &                                 & 95.58 & 94.96 & 92.23 & 94.45 & 94.52 \\ \cline{2-8} 
                           & DeepInv$_{2k}$ & \multirow{6}{*}{Generation} & 93.26 & 90.36 & 83.04 & 86.85 & 89.72 \\
                           & CMI$_{500}$     &                                  & 94.84 & 91.13 & 90.01 & 92.78 & 92.52 \\
                           & DAFL    &                                  & 92.22 & 81.10 & 65.71 & 81.33 & 81.55 \\
                           & ZSKT    &                                  & 93.32 & 89.46 & 83.74 & 86.07 & 89.66 \\
                           & DFQ     &                                  & 94.61 & 90.84 & 86.14 & 91.69 & 92.01 \\
                           & Fast$_{10}$    &                                  & 94.05 & 90.53 & 89.29 & 92.51 & 92.45 \\ \cline{2-8} 
                           & DFND$_{150k}$    & \multirow{4}{*}{Sampling}  & 94.18 & 91.77 & 87.95 & 92.56 & 92.02 \\
                           & DFND$_{600k} $   &                                  & \underline{95.36} & 91.86 & \underline{90.26} & \underline{93.33} & \underline{93.11} \\
                           & \textbf{ODSD}$_{150k}$    &                                  & 95.05 & \underline{92.02} & 89.14 & 92.94 & 92.34 \\
                           & \textbf{ODSD}$_{600k}$    &                                  & \textbf{95.70} & \textbf{92.55} & \textbf{91.53} & \textbf{94.31} & \textbf{94.02} \\ \midrule[1pt]
\multirow{13}{*}{\rotatebox{90}{\textbf{CIFAR-100}}} & Teacher & \multirow{3}{*}{-}                                & 78.05 & 71.32 & 75.83 & 75.83 & 75.83 \\
                           & Student &                                 & 77.10 & 77.10 & 65.31 & 72.19 & 73.56 \\
                           & KD      &                                 & 77.87 & 75.07 & 64.06 & 68.58 & 70.79 \\ \cline{2-8} 
                           & DeepInv$_{2k}$ & \multirow{6}{*}{Generation} & 61.32 & 54.13 & 53.77 & 61.33 & 61.34 \\
                           & CMI$_{500}$     &                                  & 77.04 & 70.56 & 57.91 & 68.88 & 68.75 \\
                           & DAFL    &                                  & 74.47 & 54.16 & 20.88 & 42.83 & 43.70 \\
                           & ZSKT    &                                  & 67.74 & 54.31 & 36.66 & 53.60 & 54.59 \\
                           & DFQ     &                                  & 77.01 & 66.21 & 51.27 & 54.43 & 64.79 \\
                           & Fast$_{10}$    &                                  & 74.34 & 67.44 & 54.02 & 63.91 & 65.12 \\ \cline{2-8} 
                           & DFND$_{150k}$    & \multirow{4}{*}{Sampling}  & 74.20 & 69.31 & 58.55 & 68.54 & 69.26 \\
                           & DFND$_{600k}$    &                                  & 74.42 & 68.97 & 59.02 & 69.39 & 69.85 \\
                           & \textbf{ODSD}$_{150k}$    &                                  & \underline{77.90} & \underline{72.24} & \underline{60.55} & \underline{71.66} & \underline{72.42} \\
                           & \textbf{ODSD}$_{600k}$    &                                  & \textbf{78.45} & \textbf{72.71} & \textbf{60.57} & \textbf{72.71} & \textbf{73.20} \\
                           
\bottomrule[1pt]
\end{tabular}
}
\caption{Student accuracy (\%) on CIFAR datasets. \textbf{Bold} and \underline{underline} denote the best and the second best results, respectively.}
\label{tab2}
%\vspace{-0.3cm}
\end{table*}
\fi

\begin{table*}[t]
\centering
%\begin{center}
%\vspace{-0.2cm}
\caption{Student accuracy (\%) on CIFAR datasets. \textbf{Bold} and \underline{underline} numbers denote the best and the second best results, respectively.}
\vspace{0.1cm}
\renewcommand\arraystretch{1.17}
\setlength{\tabcolsep}{2.7mm}
\scalebox{0.965}{
\begin{tabular}{clcccccc}
\toprule[1pt]
\multirow{2}{*}{\textbf{Dataset}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Method}}} & \multirow{2}{*}{\textbf{Type}} & \textbf{ResNet-34} &\textbf{VGG-11}    & \textbf{WRN40-2} & \textbf{WRN40-2} & \textbf{WRN40-2} \\
                            & \multicolumn{1}{c}{}                        &                       & \textbf{ResNet-18} & \textbf{ResNet-18} & \textbf{WRN16-1} & \textbf{WRN40-1} & \textbf{WRN16-2} \\ \midrule
\multirow{15}{*}{\rotatebox{90}{\textbf{CIFAR-10}}}  & Teacher & \multirow{3}{*}{-}          & 95.70     & 92.25     & 94.87   & 94.87   & 94.87   \\
                            & Student &                             & 95.20     & 95.20     & 91.12   & 93.94   & 93.95   \\
                            & KD      &                             & 95.58     & 94.96     & 92.23   & 94.45   & 94.52   \\ \cmidrule(l){2-8} 
                            & DeepInv$_{CV\!P\!\;\;\!\!\!R\;\!20}$ \cite{yin2020dreaming} & \multirow{8}{*}{Generation} & 93.26     & 90.36     & 83.04   & 86.85   & 89.72   \\
                            & CMI$_{I\!JC\!AI\;\!21}$  \cite{fang2021contrastive}    &                             & 94.84     & 91.13     & 90.01   & 92.78   & 92.52   \\
                            & DAFL$_{ICCV19}$ \cite{chen2019}   &                             & 92.22     & 81.10     & 65.71   & 81.33   & 81.55   \\
                            & ZSKT$_{N\!\;\;\!\!\!I\!\;\;\!\!\!PS\;\!19}$ \cite{micaelli2019zero}   &                             & 93.32     & 89.46     & 83.74   & 86.07   & 89.66   \\
                            & DFED$_{AC\!M\!M\!M21}$ \cite{hao2021data}   &                             & \multicolumn{1}{c}{-}         & \multicolumn{1}{c}{-}         & 87.37   & 92.68   & 92.41   \\
                            & DFQ$_{CV\!P\!RW20}$ \cite{choi2020data}    &                             & 94.61     & 90.84     & 86.14   & 91.69   & 92.01   \\
                            & Fast$_{AAAI\;\!22}$ \cite{fang2022up}   &                             & 94.05     & 90.53     & 89.29   & 92.51   & 92.45   \\
                            & MAD$_{N\!\;\;\!\!\!I\!\;\;\!\!\!PS\;\!22}$ \cite{do2022momentum}    &                             & 94.90     & \multicolumn{1}{c}{-}         & \multicolumn{1}{c}{-}       & \multicolumn{1}{c}{-}        & 92.64   \\ \cmidrule(l){2-8} 
                            & DFND$\_150k_{CV\!P\!\;\;\!\!\!R\;\!21}$ \cite{chen2021learning}   & \multirow{4}{*}{Sampling}   & 94.18     & 91.77     & 87.95   & 92.56   & 92.02   \\
                            & DFND$\_600k_{CV\!P\!\;\;\!\!\!R\;\!21}$ \cite{chen2021learning}   &                             & \underline{95.36}     & 91.86     & \underline{90.26}   & \underline{93.33}   & \underline{93.11}   \\
                            & ODSD$\_150k$    &                             & 95.05     & \underline{92.02}     & 89.14   & 92.94   & 92.34   \\
                            & ODSD$\_600k$     &                             & \textbf{95.70}     & \textbf{92.55}     & \textbf{91.53}   & \textbf{94.31}   & \textbf{94.02}   \\ \midrule[1pt]
\multirow{15}{*}{\rotatebox{90}{\textbf{CIFAR-100}}} & Teacher & \multirow{3}{*}{-}          & 78.05     & 71.32     & 75.83   & 75.83   & 75.83   \\
                            & Student &                             & 77.10     & 77.10     & 65.31   & 72.19   & 73.56   \\
                            & KD      &                             & 77.87     & 75.07     & 64.06   & 68.58   & 70.79   \\ \cmidrule(l){2-8} 
                            & DeepInv$_{CV\!P\!\;\;\!\!\!R\;\!20}$ \cite{yin2020dreaming} & \multirow{8}{*}{Generation} & 61.32     & 54.13     & 53.77   & 61.33   & 61.34   \\
                            & CMI$_{I\!JC\!AI\;\!21}$ \cite{fang2021contrastive}    &                             & 77.04     & 70.56     & 57.91   & 68.88   & 68.75   \\
                            & DAFL$_{ICCV19}$ \cite{chen2019}    &                             & 74.47     & 54.16     & 20.88   & 42.83   & 43.70   \\
                            & ZSKT$_{N\!\;\;\!\!\!I\!\;\;\!\!\!PS\;\!19}$ \cite{micaelli2019zero}    &                             & 67.74     & 54.31     & 36.66   & 53.60   & 54.59   \\
                            & DFED$_{AC\!M\!M\!M21}$ \cite{hao2021data}   &                             & \multicolumn{1}{c}{-}          & \multicolumn{1}{c}{-}          & 41.06   & 60.96   & 60.79   \\
                            & DFQ$_{CV\!P\!RW20}$ \cite{choi2020data}    &                             & 77.01     & 66.21     & 51.27   & 54.43   & 64.79   \\
                            & Fast$_{AAAI\;\!22}$ \cite{fang2022up}   &                             & 74.34     & 67.44     & 54.02   & 63.91   & 65.12   \\
                            & MAD$_{N\!\;\;\!\!\!I\!\;\;\!\!\!PS\;\!22}$ \cite{do2022momentum}    &                             & 77.31     & \multicolumn{1}{c}{-}       & \multicolumn{1}{c}{-}       & \multicolumn{1}{c}{-}       & 64.05   \\ \cmidrule(l){2-8} 
                            & DFND$\_150k_{CV\!P\!\;\;\!\!\!R\;\!21}$  \cite{chen2021learning}   & \multirow{4}{*}{Sampling}   & 74.20     & 69.31     & 58.55   & 68.54   & 69.26   \\
                            & DFND$\_600k_{CV\!P\!\;\;\!\!\!R\;\!21}$  \cite{chen2021learning}   &                             & 74.42     & 68.97     & 59.02   & 69.39   & 69.85   \\
                            & ODSD$\_150k$    &                             & \underline{77.90}     & \underline{72.24}     & \underline{60.55}   & \underline{71.66}   & \underline{72.42}   \\
                            & ODSD$\_600k$    &                             & \textbf{78.45}     & \textbf{72.71}     & \textbf{60.57}   & \textbf{72.71}   & \textbf{73.20}   \\ \bottomrule[1pt]
\end{tabular}
}
%\end{center}
\label{tab2}
\vspace{-0.1cm}
\end{table*}


\begin{table}[t]
\centering
\caption{Student accuracy (\%) on ImageNet dataset.}
%\begin{center}
\vspace{0.1cm}
\tabcolsep=0.06cm
\scalebox{0.87}{
\begin{tabular}{@{}lcrrr@{}}
\toprule[1pt]
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Type}}       & \textbf{ResNet-50}             & \textbf{ResNet-50} & \textbf{ResNet-50}             \\
                        &                             & \textbf{ResNet-18}             & \textbf{ResNet-50} & \textbf{MobileNetv2}           \\ \midrule
Teacher                 & \multirow{3}{*}{-}                           & 75.59                & 75.59    & 75.59                \\
Student                 &                            & 68.93                & 75.59    & 63.97                \\ 
KD                      &   & 68.10               & 74.76    & 61.67                \\ \midrule
DFD \cite{luo2020large}                     & \multirow{3}{*}{Generation}                           & \underline{54.66}                & \underline{69.75}    & \underline{43.15}                \\
$^\ast$DeepInv$_{2k}$ \cite{yin2020dreaming}                &                             & \multicolumn{1}{c}{\; \; \; -} & 68.00    & \multicolumn{1}{c}{\; \; \; \; \; \; -} \\
Fast$_{50}$ \cite{fang2022up}                   &                             & 53.45                & 68.61    & 43.02                \\ \midrule
DFND \cite{chen2021learning}                   & \multirow{2}{*}{Sampling}  & 42.82                & 59.03    & 16.03                \\
\textbf{ODSD}                    &                             & \textbf{58.24}                & \textbf{71.25}    & \textbf{52.74}                \\ \bottomrule[1pt]
\end{tabular}
}
%\end{center}
\label{tab3}
 \vspace{-0.3cm}
\end{table}

\iffalse
\begin{table}[h]
\centering
\tabcolsep=0.06cm
\scalebox{0.95}{
\begin{tabular}{@{}c|cccccc@{}}
\toprule[1pt]
Method & DAFL        & ZSKT        & DeepInv      & CMI          & DFQ         & Ours \\ \midrule
FLOPs  & $\sim$59.6G & $\sim$29.8G & $\sim$947.2G & $\sim$499.2G & $\sim$59.7G & -    \\
params & $\sim$1.1M  & $\sim$1.1M  & -            & $\sim$1.1M   & $\sim$5.8M  & -    \\ \bottomrule[1pt]
\end{tabular}
}
\caption{Additional FLOPs and params in DFKD methods.}
\label{tab4}
\end{table}
\fi




\iffalse
\subsubsection{Implementation Details.} The proposed model is implemented in PyTorch (\cite{paszke2019pytorch}) and trained with eight Nvidia Tesla V100 GPUs. 
The experiment of CIFAR datasets has five baseline settings, including the teacher and student using the same backbone or different backbones.
For selection based methods, we set two groups of different sample selection numbers (150k or 600k) to meet the needs of lightweight model training and high precision, respectively.
The settings of ImageNet dataset are similar, but there are only three backbone settings and sample selection numbers is 600k only.
For the above two datasets, we set $\tau$ as 4 to be the same with other distillation methods, and set $\tau_{1}$ and $\tau_{2}$ as 0.5 to be the same as (\cite{chen2020simclr}).
Finally, we choose $\lambda_{1}$ as 10 and $\lambda_{2}$ as 0.5, use the SGD optimizer with the momentum as 0.9, weight decay as $5\times10^{-4}$, the batch size $N$ as 64, and the learning rate initially equal to 0.025.
\fi


\subsection{Performance Comparison}
To evaluate the effectiveness of our ODSD, we comprehensively compare it with current SOTA DFKD methods regarding the student's performance, the effectiveness of the sampling method and training costs. 
In addition, some other methods only apply to small datasets and models (\eg, MNIST and AlexNet) for some reason.
The test baselines mentioned in the article may be difficult for these methods, causing serious performance decline and non-competitive experimental results.
To compare these methods fairly, we conduct experiments on the MNIST dataset and maintain the same experimental settings.
The experimental results are shown in \textbf{Supplementary Sec.2}.



%\subsubsection{CIFAR-10 \& CIFAR-100.}
\textbf{Experiments on CIFAR-10 and CIFAR-100.}
We first verify the proposed method on the CIFAR-10 and CIFAR-100 \cite{krizhevsky2009learning}.
We collate the performance of two kinds of SOTA methods based on data generation and data sampling.
The baseline ``\textit{Teacher}" and ``\textit{Student}" means to use the corresponding backbones of the teacher or student for direct training with the original training data, and ``\textit{KD}" represents distilling the student network with the original training data.
Generation-based methods include training additional generators and calculating model gradient inversion.
Sampling-based methods use the unlabeled ImageNet dataset. 
We reproduce the DFND using the unified teacher models, and the result is slightly higher than the original paper.


As shown in Table~\ref{tab2}, our ODSD has achieved the best results on each baseline.
Under most baseline settings, ODSD brings gains of 1\% or even higher than the SOTA methods, even though students' accuracy is very close to their teachers.
In particular, the students of our ODSD outperform the teachers on some baselines.
As far as we know, it is the first DFKD method to achieve such performance. 
The main reasons for its breakthrough in analyzing the algorithm's performance come from three aspects.
First, our data sampling method comprehensively analyzes the intra-class relationships in the unlabeled data, excluding the difficult edge data and significant distribution differences data. 
At the same time, the number of data in each class is relatively more balanced, which is conducive to all kinds of balanced learning compared with other sampling methods.
Second, our knowledge distillation method considers the representation of low-dimensional and low-noise information and expands the representation of knowledge through data augmentation.
The structured relationship distillation method helps the student effectively learn knowledge from both multiple data and its teacher.
Finally, the knowledge of our ODSD does not entirely come from the teacher but also the consistency and differentiated representation learning of unlabeled data, which is helpful when the teacher makes mistakes.
The previous methods ignore the in-depth mining of data knowledge, which affect students' performance.

%\subsubsection{ImageNet.}
\textbf{Experiments on ImageNet.}
We conduct experiments on a large-scale ImageNet dataset to further verify the effectiveness.
Due to the larger image size, it is challenging to synthesize training data for most generation-based methods effectively.
Most of them failed.
A small number of successful methods have to train 1,000 generators (one generator for one class), resulting in a large amount of additional computational costs.
We set up three baselines to compare the performance of our method with the SOTA methods.
Table \ref{tab3} reports the experimental results. 
Our ODSD still achieves several percentage points increase compared with other SOTA methods, especially in the cross-backbones situation (9.59\%).
Due to the lack of structured knowledge representation, the DNFD algorithm performs poorly on the large-scale dataset.
Comparing the performance of DFND and ODSD, our structured knowledge framework improves the overall understanding ability of the student. 
%Moreover, our method can better face the problem of the teacher and student with different backbones.
\renewcommand{\thefootnote}{}
\footnotetext{$\ast$ For fair comparisons, we select the original version of DeepInv without the mixup data augmentation, which is the same as other methods.}

\begin{table}[h]
\centering
\caption{Total FLOPs and params in DFKD methods.}
\vspace{0.1cm}
%\begin{center}
\tabcolsep=0.09cm
\scalebox{0.87}{
\begin{tabular}{@{}l|ccccccc@{}}
\toprule
Method & DeepInv & CMI   & DAFL   & ZSKT   & DFQ    & DFND   & \textbf{ODSD}   \\ \midrule
FLOPs  & 4.36G    & 4.56G  & 0.67G & 0.67G & 0.79G & 0.56G & 0.56G \\
params & 11.7M   & 12.8M & 12.8M  & 12.8M  & 17.5M  & 11.7M  & 11.7M  \\ \bottomrule
\end{tabular}
}
%\end{center}
\label{tab4}
% \vspace{-0.3cm}
\end{table}


%\subsubsection{Training Costs.}
\textbf{Comparison of training costs.}
In order to verify that the generation-based methods add extra costs that we mentioned in the introduction section, we further calculate the total floating point operations (FLOPs) and parameters (params) required by various DFKD algorithms, as shown in Table \ref{tab4}.
Because without additional generation modules, our method only needs training costs and params of the student network. 
Other methods list the required calculation cost and params of both the generation module and the student. 
These generation modules will be discarded after student training, which causes a waste of computing power.


\begin{table}[t]
\centering
\caption{APS compared with the SOTA sampling method.}
\vspace{0.1cm}
%\begin{center}
%\vspace{-0.2cm}
\scalebox{0.9}{
\begin{tabular}{@{}cccc@{}}
\toprule
\multirow{2}{*}{Sampling methods} & \multicolumn{3}{c}{Method} \\
                                   & KD       & DFND     & \textbf{ODSD}    \\ \midrule
Random                             & 76.85    & 73.15    & 76.43   \\
DFND                               & 76.67    & 73.68    & 77.40   \\
\textbf{APS}                         & \textbf{77.27}    & \textbf{73.89}    & \textbf{77.90}   \\ \bottomrule
\end{tabular}
}
%\end{center}
\label{tab5}
% \vspace{-0.3cm}
\end{table}

\begin{table}[t]
\centering
\caption{Segmentation results on NYUv2 dataset.}
\vspace{0.1cm}
%\begin{center}
\tabcolsep=0.07cm
\scalebox{0.9}{
\begin{tabular}{@{}c|cc|ccc|cc@{}}
\toprule[1pt]
Algorithm & Teacher    & Student   & DAFL      & DFAD     & Fast     & DFND  & \textbf{ODSD}                    \\
mIoU      & 0.517      & 0.375     & 0.105     & 0.364    & 0.366    & \underline{0.378} & \textbf{0.397} \\ \bottomrule[1pt]
\end{tabular}
}
%\end{center}
\label{tab7}
\vspace{-0.3cm}
\end{table}

%\subsubsection{Data Sampling Efficiency.}
\textbf{Comparison of data sampling efficiency.}
To verify the effectiveness of the sampling mechanism, we compare the performance of our APS method compared with the current SOTA unlabeled data sampling method DFND \cite{chen2021learning}.
Three data sampling methods (random sampling, DFND sampling, and our proposed APS) are set on three different distillation algorithms, including: KD \cite{hinton2015distilling}, DFND \cite{chen2021learning}, and our proposed ODSD.
Table \ref{tab5} reports the results.
For KD, we use the sampled data instead of the original generated data with $\mathcal{L}_{K\!D}$ distillation loss.
From the result, this setting is competitive, even better than the distillation loss of DFND.
For DFND, we reproduce it with open-source codes and keep the original training strategy unchanged.
We find the performance of the DFND sampling method is unstable, which causes it to be lower than random sometimes.
For ODSD, we use the distillation loss in Equation (8).
Our proposed sampling method achieves the best performance in all three benchmarks and significantly improves performance.
By comprehensively considering the data confidence, the data outliers, and the class density, our ODSD can more fully mine intra-class relationships of the unlabeled data.
As a result, the sampled data are more helpful for subsequent student learning.
%This way, the sampled data are more instrumental for subsequent student training.

%\subsubsection{Segmentation Results.}
\textbf{Experiments about semantic segmentation.}
We also conduct experiments on segmentation tasks. 
Mean Intersection over Union (mIoU) is set as the evaluation metric. 
Table~\ref{tab7} shows segmentation results on the NYUv2 dataset. 
Our ODSD also achieves the best performance.
The visualization results and more detailed analysis are shown in \textbf{Supplementary Sec.4}.



\subsection{Diagnostic Experiment}
To verify the effectiveness of our method, we conduct
diagnostic studies on the CIFAR-100 dataset. 
We use ResNet-34 as the teacher's backbone and ResNet-18 as the student's backbone. 
150k data are sampled, and the student trains 200 epochs.
The optimal values obtained by diagnostic experiments are also the default setting of 4.2 comparison experiments.
In addition to what is shown in this section, more diagnostic experiments are shown in \textbf{Supplementary Sec.2}.


%\subsubsection{Training Objective.}
\textbf{Distillation training objective.}
We first investigate our overall training objective (cf. Equation (8)). 
Two different data sampling numbers are set in this experiment.
As shown in the experiments (1-4) of Table \ref{tab6}, the model with $\mathcal{L}_{K\!D}$ alone achieves accuracy scores of $74.39\%$ and $77.27\%$ on 50k and 150k data sampling settings. 
Adding $\mathcal{L}_{n}$ or $\mathcal{L}_{c} $ individually brings gains (\ie, $\textbf{0.32\%, 0.31\%/ 0.43\%, 0.44\%}$), indicating the effectiveness of our proposed distillation method.
By combining all the training objectives, our method achieves better performance with $\textbf{75.26\%}$ and $\textbf{77.90\%}$.
Therefore, the proposed training objectives are effective and can help students gain good performance.


\iffalse
\begin{table}[h]
\caption{A set of diagnostic studies of proposed method.}
%\vspace{-0.2cm}
\begin{minipage}[h]{0.25\textwidth}
\scalebox{0.9}{
\begin{tabular}{@{}ccc@{}}
\toprule[1pt]
\multirow{2}{*}{Setting} & \multicolumn{2}{l}{Accuracy (\%)} \\
                         & 50k             & 150k            \\ \midrule
ours                     & \textbf{75.26}           & \textbf{77.90}           \\
w/o $\mathcal{L}_{n} $                 & 74.71           & 77.58           \\
w/o $\mathcal{L}_{c} $                 & 74.82           & 77.71           \\
w/o $\mathcal{L}_{n}$, w/o $\mathcal{L}_{c} $                     & 74.39           & 77.27           \\ \bottomrule[1pt]
\end{tabular}
}

%\vspace{-0.1cm}
\subcaption{Training objective $\mathcal{L}$.}
\label{tab:firsttable}
\end{minipage}
\hfill
\begin{minipage}[h]{0.195\textwidth}
\scalebox{0.9}{
\begin{tabular}{@{}cc@{}}
\toprule[1pt]
Setting & Accuracy (\%) \\ \midrule
ours    & \textbf{77.90}         \\
w/o $sc_{i}$   & 77.04         \\
w/o $so_{i}$   & 76.67         \\
w/o $sd_{i}$   & 76.59         \\ \bottomrule[1pt]
\end{tabular}
}
\centering
%\vspace{-0.1cm}
\subcaption{Data selection scores $S$.}
\label{tab:secondtable}
\end{minipage}
%~\\
%\vspace{0.2cm}
\centering
\begin{minipage}[h]{0.45\textwidth}
\centering
\scalebox{0.9}{
\tabcolsep=0.1cm
\begin{tabular}{@{}ccccccc@{}}
\toprule[0.7pt]
Prototype $K$ & 1     & 5     & 10    & 20    & 30    & 50    \\
Accuracy (\%)      & 77.25 & \textbf{77.90} & 77.47 & 77.64 & 77.62 & 77.54 \\ \bottomrule[0.7pt]
\end{tabular}
}
\centering
\subcaption{Prototype number per class $K$.}
\label{tab:thirdtable}
\end{minipage}
\label{tab1}
%\vspace{-0.5cm}
\end{table}
\fi


%\tabcolsep=0.04cm

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
%\tabcolsep=0.04cm

%\subsubsection{Data Sampling Scores.}
\textbf{Data sampling scores.}
To obtain more efficient data, we define three scores for our sampling method in section 3.2.
To verify their effectiveness, we further carry out ablation experiments.
When using the complete three efficient evaluation criteria, the model can achieve the best performance with
$\textbf{75.26\%}$ and $\textbf{77.90\%}$ accuracy shown in experiments (5-8) of Table \ref{tab6}.
When the confidence score $sc_{i}$ is abandoned, the familiarity of the teacher network with the sampled data decreases, reducing the amount of adequate information contained in the data.
Without the outlier score $so_{i}$, the lack of modelling of the intra-class relationship of the data to be sampled leads to increased data distribution difference between the substitute data domain and the original data domain.
Further, the class density score $sd_{i}$ can measure the number of data in each class and maintain the balance of the sampled data.
In summary, all three score indicators can help students perform better.


\subsection{Visualization}

To verify the distribution difference between sampled data and the original data of each sampling-based method, we use t-SNE \cite{van2008visualizing} to visualize the feature distribution.
The pre-trained ResNet-34 network on the CIFAR-100 dataset and ResNet-50 network on the ImageNet dataset is used as the teacher network. 
For both datasets, we reserve 100 classes of validation data.
We compare random sampling, the DFND sampling method, and our Adaptive Prototype Sampling (APS) method.
Figure \ref{fig3} shows the data distribution differentiation results.
Our clustering results are closer to the extracted features of the original data.
Reducing the distribution difference between sampled and original data helps reduce data label noise, which is the key for the student to perform well.


\iffalse
\subsubsection{Prototype Number Per Class $K$.}
The size of $K$ reflects the complexity of intra-class modeling. 
When $K \! = \! 1$, the expression of the intra-class relationship is relatively simple, but sometimes it can not accurately exclude data with abnormal predictions.
When $K$ is particularly large, the average number of data at each prototype decreases, which leads to the problem of the prototype itself shifting.
Experiments (9-14) in Table~\ref{tab6} report our approach's performance concerning the number of prototypes per class.
Student performance does not always rise with the increase in the number of prototypes.
Comprehensively considering student performance and calculation cost, we choose $K=5$ and achieve the best performance.
\fi

\begin{table}[t]
\centering
\caption{A set of diagnostic studies of proposed method.}
\vspace{0.1cm}
%\begin{center}
\scalebox{0.95}{
\tabcolsep=0.1cm
\begin{tabular}{@{}cccccccc@{}}
\toprule
\multicolumn{4}{c|}{Training objective $\mathcal{L}$}                                                         & \multicolumn{4}{c}{Data sampling scores $S$}        \\ \midrule
\multicolumn{1}{c|}{\multirow{2}{*}{ID}} &
  \multirow{2}{*}{Setting} &
  \multicolumn{2}{c|}{Accuracy (\%)} &
  \multicolumn{1}{c|}{\multirow{2}{*}{ID}} &
  \multirow{2}{*}{Setting} &
  \multicolumn{2}{c}{Accuracy (\%)} \\
\multicolumn{1}{c|}{}          &                           & 50k   & \multicolumn{1}{c|}{150k}  & \multicolumn{1}{c|}{}    &       & 50k   & 150k  \\ \midrule
\multicolumn{1}{c|}{(1)}       & ours                      & \textbf{75.26} & \multicolumn{1}{c|}{\textbf{77.90}} & \multicolumn{1}{c|}{(5)} & ours  &  \textbf{75.26}     & \textbf{77.90} \\
\multicolumn{1}{c|}{(2)}       & w/o  $\mathcal{L}_{n}$ & 74.82 & \multicolumn{1}{c|}{77.71} & \multicolumn{1}{c|}{(6)} & w/o $sc_{i}$   &   73.96    & 77.04 \\
\multicolumn{1}{c|}{(3)}       & w/o $\mathcal{L}_{c} $    & 74.71 & \multicolumn{1}{c|}{77.58} & \multicolumn{1}{c|}{(7)} & w/o $so_{i}$   &   68.07    & 76.67 \\
\multicolumn{1}{c|}{(4)}       & w/o $\mathcal{L}_{n},\mathcal{L}_{c} $       & 74.39 & \multicolumn{1}{c|}{77.27} & \multicolumn{1}{c|}{(8)} & w/o $sd_{i}$   &   70.24    & 76.59 \\ \bottomrule
\end{tabular}
}
%\end{center}
\label{tab6}
\end{table}


% Figure environment removed



%clearer than the DFND diagram classification, and is very close to the prediction of the teacher network.
%Thanks to the efficient sample selection method and effective knowledge distillation method, our method has obtained clearer results.




\section{Conclusion}

\iffalse
In this paper, we propose an Open-world Data Sampling Distillation (ODSD) method without unnecessary generation costs.
First, we model the intra-class relationship of unlabeled data to obtain data closer to the original data distribution. 
Then, we use low-noise knowledge representation to suppress the label noise of the substitute data. 
Further, structured contrastive knowledge is constructed to adequately exploit the knowledge in data and the teacher.
Extensive experiments have illustrated the effectiveness of our proposed data sampling method and distillation method.
As a result, the proposed method achieves a significant improvement and state-of-the-art performance on various benchmarks.
Especially, it improve 1.50\%-9.59\% accuracy on the ImageNet dataset compared with existing methods.
\fi

Since the original training data may not be available due to privacy concerns, most existing data-free knowledge distillation methods rely heavily on additional generation modules.
However, these generation modules bring additional computational costs. 
Meanwhile, existing DFKD methods disregard the domain shifts issue between the substitute and original data, and only consider the teacher's knowledge ignoring the data knowledge.
In this paper, we propose an Open-world Data Sampling Distillation (ODSD) method without unnecessary generation costs.
We sample unlabeled data with similar distribution to original data and introduce low-noise knowledge representation learning to cope with domain shifts.
To explore the data knowledge adequately, we design a structured knowledge representation.
Comprehensive experiments have illustrated the effectiveness of our proposed data sampling and distillation method.
As a result, the proposed method achieves significant improvement and state-of-the-art performance on various benchmarks.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{arxiv_7_31}
}


\end{document}
