\documentclass[lettersize,journal]{IEEEtran}

\usepackage{url}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{amssymb}
\usepackage{xspace}
\usepackage{amstext}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algorithmicx}

\usepackage{booktabs}
\usepackage{multirow}

\usepackage{bm}
\usepackage{newfloat}
\usepackage{listings}
\usepackage{bibentry}
\usepackage{algpseudocode}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage{amsfonts}

%\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}
%\hypersetup{colorlinks, linkcolor={dark-blue},citecolor={dark-blue}, urlcolor={medium-blue}}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}


\usepackage{balance}
\usepackage{multicol}

\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}


\newcommand\etal{\textit{et al}. }
\newcommand\eg{\textit{e}.\textit{g}.}
\newcommand\ie{\textit{i}.\textit{e}.}



\begin{document}
% \title{Learning Causality-inspired Representation Consistency for Video Anomaly Detection}
\title{Sampling to Distill: Knowledge Transfer from Open-World Data}


\author{
    \IEEEauthorblockN{
        Yuzheng Wang\textsuperscript{1},
        Zhaoyu Chen\textsuperscript{1},
        Jie Zhang\textsuperscript{2},
        Dingkang Yang\textsuperscript{1},
        Zuhao Ge\textsuperscript{1},
        Yang Liu\textsuperscript{1},
        Siao Liu\textsuperscript{1}, \\
        Yunquan Sun\textsuperscript{1,3},
        Wenqiang Zhang\textsuperscript{1,3$^\ast$}\thanks{$^\ast$Corresponding authors.},
        Lizhe Qi\textsuperscript{1,3$^\ast$}
    }

    \IEEEauthorblockA{
        \textsuperscript{1}Academy for Engineering \& Technology, Fudan University, Shanghai, China \quad
        \textsuperscript{2}ETH Zurich\\
        \textsuperscript{3}Engineering Research Center of AI \& Robotics, Ministry of Education, Academy for Engineering \& Technology, Fudan University, Shanghai, China \\
        % yzwang20@fudan.edu.cn
    }
}

% \author{Yang Liu\orcidA{}, Kun Yang, Jielin Li, Chujing Yu, Jing Liu\orcidB{},~\IEEEmembership{Member,~IEEE}, Peng Sun\orcidF{},~\IEEEmembership{Senior Member,~IEEE}, 	
% Azzedine Boukerche\orcidE{},~\IEEEmembership{Fellow,~IEEE}, Liang Song\orcidG{},~\IEEEmembership{Senior Member,~IEEE}
% \thanks{This work is funded in part by the China Mobile Research Fund of the Chinese Ministry of Education under Grant KEH2310029 to Prof.~Liang Song, in part by the National Natural Science Foundation of China under Grant 62250410368 to Prof. Peng Sun, and in part by the Specific Research Fund of the Innovation Platform for Academicians of Hainan Province under Grant YSPTZX202314. The work is also supported by the Shanghai Key Research Laboratory of NSAI and the Joint Laboratory on Networked AI Edge Computing, Fudan University-Changan. \textit{(Corresponding author: Liang Song.)}}
% \thanks{Yang Liu, Kun Yang, Jing Liu, and Liang Song are with the Academy for Engineering \& Technology, Fudan University, Shanghai 200433, China (e-mails: yang\_liu20@fudan.edu.cn; songl@fudan.edu.cn).}
% \thanks{Jielin Li is with the  Department of Electrical and Computer Engineering, University of Toronto, Ontario M5S 1A1, Canada (email: jielin.li@mail.utoronto.ca).}
% \thanks{Chujing Yu is with the School of Information Science and Technology, Fudan University, Shanghai 200433, China (email: yucj20@fudan.edu.cn).}
% \thanks{Peng Sun is with the Division of Natural and Applied Sciences, Duke Kunshan University, Kunshan 215316, Jiangsu, China (email: peng.sun568@duke.edu).}
% \thanks{Azzedine Boukerche is with the School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa K1N 6N5, Canada (email: aboukerc@uOttawa.ca).}
% }

% \markboth{IEEE Transactions on Circuits and Systems for Video Technology,~Vol.~xx, No.~xx, xxxx~xxxx}%
% {How to Use the IEEEtran \LaTeX \ Templates}

\maketitle


\begin{abstract}
Data-Free Knowledge Distillation (DFKD) is a novel task that aims to train high-performance student models using only the pre-trained teacher network without original training data.
Most of the existing DFKD methods rely heavily on additional generation modules to synthesize the substitution data resulting in high computational costs and ignoring the massive amounts of easily accessible, low-cost, unlabeled open-world data.
Meanwhile, existing methods ignore the domain shift issue between the substitution data and the original data, resulting in knowledge from teachers not always trustworthy and structured knowledge from data becoming a crucial supplement.
To tackle the issue, we propose a novel Open-world Data Sampling Distillation (ODSD) method for the DFKD task without the redundant generation process.
First, we try to sample open-world data close to the original data's distribution by an adaptive sampling module and introduce a low-noise representation to alleviate the domain shift issue. 
Then, we build structured relationships of multiple data examples to exploit data knowledge through the student model itself and the teacher's structured representation.
Extensive experiments on CIFAR-10, CIFAR-100, NYUv2, and ImageNet show that our ODSD method achieves state-of-the-art performance with lower FLOPs and parameters.
Especially, we improve 1.50\%-9.59\% accuracy on the ImageNet dataset and avoid training the separate generator for each class.

% In specific settings, it allows students to surpass the teachers' performance for the first time in DFKD, demonstrating its effectiveness.
\end{abstract}
%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%


%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\begin{IEEEkeywords}
Data-Free Knowledge Distillation, Open-World Unlabeled Data, Contrastive Learning, Relational Distillation
\end{IEEEkeywords}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   % Figure removed
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\section{Introduction}
Deep learning has made refreshing progress in computer vision and multimedia fields \cite{dosovitskiy2020image,redmon2018yolov3,radford2021learning,laine2016temporal,park2019relational,zhang2019your,he2022masked,yang2024towards3,liu2023amp,wang2024self,yang2023context,yang2023aide,liu2023stochastic,yang2024towards2}.
Despite the great success, large-scale models \cite{devlin2018bert,karras2019style,liu2021swin,ramesh2022hierarchical,tarvainen2017mean,chen2022towards,yang2023target,wang2023adversarial,yang2024how2comm,liu2023improving,yang2024towards,liu2023learning} and unavailable privacy data \cite{burton2015data,sun2017revisiting,wang2023explicit,wang2024out,wang2024confounded} often impede the application of advanced technology on mobile devices.
Therefore, model compression and data-free technology have become the key to breaking the bottleneck.
To this end, Lopes \etal \cite{lopes2017data} propose Data-Free Knowledge Distillation (DFKD).
In this process, knowledge is transferred from the cumbersome model to a small model that is more suitable for deployment without using the original training dataset.
As a result, this widely applicable technology has gained much attention.

%\cite{hinton2015distilling} proposes a lightweight framework named knowledge distillation.
%Knowledge is transferred from the cumbersome model to a small model that is more suitable for deployment.
%Despite its good performance, the training process depends on the original data.
%Such issues may hinder the further development of this technology.
%Therefore, \cite{lopes2017data} proposes data-free knowledge distillation (DFKD) to tackle the problem.

%, which refers to the situation where only the pre-trained cumbersome teacher model is available. However, the original training data is not available. 


% Figure environment removed


To replace unavailable private data and effectively train small models, most existing data-free knowledge distillation methods rely on alternately training of the generator and the student, called the generation-based method.
%Specifically, these methods adversarially updates the generator and student network. 
Despite not using the original training data, these generation-based methods have many issues. 
First, their trained generators are abandoned after the students' training \cite{chen2019,fang2019,micaelli2019zero,hao2021data,do2022momentum,zhang2022dense}.
The training of generators brings additional computational costs, especially for large datasets.
For instance, a thousand generators are trained for the ImageNet dataset \cite{deng2009imagenet}, which introduces more computational waste \cite{luo2020large,fang2022up}.
Then, a serious domain shift issue exists between the generated substitution data and the original training data. 
Because the substitute data are composed of random noise transformation without supervision information and are highly susceptible to teacher preferences \cite{wang2024confounded}.
As a result, the efficiency and effectiveness of the generation-based methods are constrained, affecting student performance \cite{do2022momentum,binici2022preventing,patel2023learning}. 

% the substitute data domain usually does not match the unavailable original data domain and includes extensive label noise predicted by the teacher \cite{do2022momentum,binici2022preventing,patel2023learning}. 



%(3) These methods can be summarized as a distillation method of the student to mimic the outputs of a particular data example represented by the teacher.
%However, exploiting knowledge in substitute data is essential to complement missing supervision information (\cite{yim2017gift,park2019relational}).
%As a result, previous methods ignore this issue, which leads to the lack of effective knowledge expression in the distillation process.


% To overcome the disadvantages of the generation-based method, (\cite{chen2021learning}) proposes a method of training the student network via open-world sample unlabeled data (sampling-based method) without additional generation calculation costs.
Rather than relying on additional generation modules, Chen \etal \cite{chen2021learning} propose a sampling-based method to train the student network via unlabeled data without the generation calculations.
% Figure \ref{fig1} shows the difference between the two methods.
Compared with generation-based methods, sampling-based methods can avoid the training cost of generators, thus improving algorithm efficiency.
The comparison of the two methods is shown in Figure~\ref{fig1}.
Meanwhile, they try to reduce label noise by updating the learnable noise matrix, but the noise matrix's computational costs are expensive.
More restrictedly, their sampling method relies on the strict confidence ranking and does not consider the data domain similarity issue (We discuss the distribution similarity between sampled data and original data in detail in Section 4.4).
In addition, the existing generation-based and sampling-based methods can be summarized as simple imitation learning, \ie, the student mimics the output of a particular data example represented by the teacher \cite{yim2017gift,park2019relational,wang2023explicit}.
Therefore, these methods do not adequately utilize the implicit relationship among multiple data examples, which leads to the lack of effective knowledge expression in the distillation process.



Based on the above observations, we construct a sampling-based method to sample helpful data from easily accessible, low-cost, unlabeled open-world data, avoiding the unnecessary computational costs of generation modules.
In addition, we propose two aspects of customized optimization.
(\textbf{\rmnum{1}}) To cope with the domain shift issue between the open-world and original data, we preferentially try to sample data with a similar distribution to the original data domain to reduce the shifts and design a low-noise knowledge representation learning module to suppress the interference of label noise from the teacher model.
(\textbf{\rmnum{2}}) To explore the data knowledge adequately, we set up a structured representation of unlabeled data to enable the student to learn the implicit knowledge among multiple data examples.
As a result, the student can learn from carefully sampled unlabeled data instead of relying on the teacher.
At the same time, to explore an effective distillation process, we introduce a contrastive structured relationship between the teacher and student.
The student can make better progress through the structured prediction of the teacher network.


In this paper, we consider a solution to the DFKD task that does not require additional generation costs.
On the one hand, we look forward to the solution to data domain shifts from both data source and distillation methods.
On the other hand, we try to explore an effectively structured knowledge representation method to deal with the missing supervision information and the training difficulties in the DFKD scenes.
Therefore, we propose an Open-world Data Sampling Distillation (ODSD) method, which includes Adaptive Prototype Sampling (APS) and Denoising Contrastive Relational Distillation (DCRD) modules.
Specifically, the primary contributions and experiments are summarized as follows: 

\begin{itemize}
    \item We propose a sampling-based method with the unlabeled open-world data.
    The method does not require additional training of one or more generator models, thus avoiding unnecessary computational costs and model parameters.
    \item During the sampling process, considering the domain shifts between the unlabeled data and the original data, we propose an Adaptive Prototype Sampling (APS) module to obtain data closer to the original data distribution.
    \item During the distillation process, we propose a Denoising Contrastive Relational Distillation (DCRD) module to suppress label noise and exploit knowledge from data and the teacher more adequately by building the structured relationships among multiple samples.
    \item The proposed method achieves state-of-the-art performance with lower FLOPs, improves the effectiveness of the sampling process, and alleviates the distribution shift between the unlabeled data and the original data.
\end{itemize}


\section{Related Work}

\subsection{Data-Free Knowledge Distillation}
Data-free knowledge distillation aims to train lightweight models when the original data are unavailable. 
Therefore, the substitute data are indispensable to help transfer knowledge from the cumbersome teacher to the flexible student.
According to the source of these data, existing methods are divided into generation-based and sampling-based methods.

%\subsubsection{Generation-based Methods.}
\noindent\textbf{Generation-based Methods.}
The generation-based methods depend on the generation module to synthesize the substitute data.
Lopes \etal \cite{lopes2017data} propose the first generation-based DFKD method, which uses the data means to fit the training data. 
Due to the weak generation ability, it can only be used on a simple dataset such as the MNIST dataset.
The following methods combine the Generative Adversarial Networks (GANs) to generate more authentic and reliable data.
Chen \etal \cite{chen2019} firstly put the idea into practice and define an information entropy loss to increase the diversity of data.
However, this method relies on a long training time and a large batch size.
Fang \etal \cite{fang2019} suggest forcing the generator to synthesize images that do not match between the two networks to enhance the training effect.
Hao \etal \cite{hao2021data} suggest using multiple pre-trained teachers to help the student, which leads to additional computational costs.
Do \etal \cite{do2022momentum} propose a momentum adversarial distillation method to help the student recall past knowledge and prevent the student from adapting too quickly to new generator updates.
The same domain typically shares some reusable patterns, so Fang \etal \cite{fang2022up} introduce the sharing of local features of the generated graph, which speeds up the generation process.
Since the generation quality is still not guaranteed, some methods spend extra computational costs on gradient inversion to synthesize more realistic data \cite{yin2020dreaming, fang2021contrastive}.
In addition, Choi \etal \cite{choi2020data} combine DFKD with other compression technologies and achieve encouraging performance.
However, generation-based DFKD methods generate a large number of additional calculation costs in generation modules, while these modules will be discarded after students' training \cite{chen2021learning}.


% Figure environment removed



%\subsubsection{Sampling-based Methods.}
\noindent\textbf{Sampling-based Methods.}
To train the student more exclusively, Chen \etal \cite{chen2021learning} propose to sample unlabeled data to replace the unavailable data without the generation module.
Firstly, they use a strict confidence ranking to sample unlabeled data.
Then, they propose a simple distillation method with a learnable adaptive matrix.
Despite no additional generating costs and promoting encouraging results, their method ignores the intra-class relationships of multiple unlabeled data.
Simultaneously, the simple strict confidence causes more data to be sampled for simple classes, leading to imbalanced data classes.
In addition, their proposed distillation method is relatively simple and lacks structured relationship expression, which limits the student's performance.



\subsection{Contrastive Learning}
Contrastive learning makes the model's training efficient by learning the data differences \cite{ye2019unsupervised}.
The unsupervised training pipeline usually requires storing negative data by a memory bank \cite{wu2018unsupervised}, large dictionaries \cite{he2020momentum}, or a large batch size \cite{chen2020simclr}. 
Even it requires a lot of computation costs, \eg, additional normalization \cite{grill2020bootstrap}, and network update operations \cite{caron2021emerging}. 
The high storage and computing costs seriously reduce knowledge distillation efficiency. 
But at the same time, this idea of mining knowledge in unlabeled data may be helpful for the student's learning.
Due to such technical conflicts, there are few methods to combine knowledge distillation and contrastive learning in the past perfectly.
As a rare attempt, Tian \etal \cite{tian2019contrastive} propose a contrastive data-based distillation method by updating a large memory bank. 
However, data quality cannot be guaranteed for data-free knowledge distillation, and data domain shifts are intractable, making the above process challenging.


In this work, we attempt to explore additional knowledge from both the data and the teacher.
Therefore, we further stimulate students' learning ability by using the internal relationship of unlabeled data and constructing a structured contrastive relationship.
To our best knowledge, this is the first combination of data-free knowledge distillation and contrastive learning at a low cost during the distillation process, which achieves an unexpected effect.



\section{Methodology}

\subsection{Overview}

Our pipeline includes two stages: 
(\textbf{\rmnum{1}}) unlabeled data sampling and (\textbf{\rmnum{2}}) distillation training, as shown in Figure \ref{fig2}.
For the sampling stage, we sample unlabeled data by an adaptive sampling mechanism to obtain data closer to the original distribution. 
For the distillation stage, the student learns the knowledge representation after denoising through a spatial mapping denoise module.
Further, we mine more profound knowledge of the unlabeled data and build the structured relational distillation to help the student gain better performance.
The complete algorithm is shown in Algorithm~\ref{alg1}.


\subsection{Adaptive Prototype Sampling}

The unlabeled data and the original data are distributed differently in many cases.
To obtain the substitution data with a more similar distribution to the original data from the specific unlabeled dataset, we propose an Adaptive Prototype Sampling (APS) module, which considers the teacher's familiarity with the data, excludes misclassified offset noisy data, and focuses on the class balance of the sampled data.
Based on these, we design three score indicators to evaluate the effectiveness of the unlabeled data for student training corresponding to the above three aspects, including the data confidence score, the data outlier score, and the class density score.


\noindent \textbf{(a) Data Confidence Score.} 
The teacher provides the prediction logits $P=[p_{1},\dots,p_{n}]\in \mathbb{R}^{n\times C}$ on the unlabeled dataset $\left \{\bm{x}_0,\dots, \bm{x}_n  \right \} $, where $p_{i}$ denotes the prediction for the $i$-th sample satisfying $p_{i} \in \mathbb{R}^{1\times C}$.
$n$ denotes the number of data, and $C$ denotes the number of classes.
Then the prediction is converted into the probability of the unified scale as ${\tilde{p}_{i}} =  \sigma(p_{i})$, where $\sigma$ denotes the softmax layer and $\tilde{p}_{i}$ denotes the confidence probability corresponding to the predicted result class. 
Therefore, $\tilde{p}=[\tilde{p}_{1},\dots,\tilde{p}_{n}]$ represents the confidence of each data in the unlabeled dataset.
We choose the largest $\max\!\left \{\tilde{p} \right \}$ for normalization.
The confidence score of $i$-th sample $\bm{x}_i$ can be calculated as: $sc_{i} =  \frac{\tilde{p}_{i}}{\left |\max\left \{\tilde{p} \right \}  \right | }$.



\begin{algorithm}[t]
  \caption{The proposed ODSD algorithm.}
  \label{alg1}
  \begin{algorithmic}[1]
    \Require
      A frozen teacher network $f_{T}$, an unlabeled open-world dataset $X_{U}$, and the target number of sampled data $M$.
    \State \textbf{Module 1:} \textbf{Adaptive pototype sampling}
    
    \For {unlabeled data $\bm{x}_i$ in $X_{U}$}  
        \State Classify teacher predictions $p_i$ as $\rho _{i,c}= {p_{i}\in c}$;
        \State Calculate confidence probability: ${\tilde{p}_{i}} =\sigma(p_{i})$
        \State Cluster the prediction vector as the prototypes $\mu_{c,k}$.
    \EndFor
    \For {Prototypes $\mu_{c,k}$ in class $c$}
        \State Obtain prototype similarity: $\tilde{o}_{i}= \cos(\rho _{i,c}, \mu_{c,k}) _{k=1}^{K} $;
        \State Calculate intra-class outliers mean: $u_{c} = \frac{1}{n_{c} } \sum \, _{p_{i} \in c}\tilde{o}_{i}$;
        \State Calculate the density score $D_{c}  =  \frac{\sqrt{u_{c}} }{\log_{e}{(n_{c}+C)}}$.
    \EndFor
    %\State Convert data density $d_{i} \!= \! D_{c}({p_{i} \in c)}$;
    \State Calculate sampling score:
    $S= \frac{\tilde{p}_{i}}{\left |\max\left \{\tilde{p} \right \}  \right | }-\frac{\tilde{o}_{i}}{\left |\max\left \{\tilde{o} \right \}  \right | } +\frac{D_{c}}{\left |\max\left \{ D \right \} \right |} $

    \State Sample $\text{top-} M$ data with the highest score as $X_{A}$.
    
    
    \State \textbf{Module 2: } \textbf{Denoising contrastive relational distillation}.
    %\State Initialize the student network $f_S$.
    \For {$i$ in number of epochs}
    
    \For {training data $\bm{x}$ in $X_{A}$}  

        \State Calculate $\mathcal{L}_{total} $ as Eq.8 and update the student $f_S$.
    \EndFor
    
    \EndFor
    \Ensure The trained student $f_S$ and a reusable sampling list $L$ of the teacher $f_{T}$ on dataset $X_{U}$.
    \end{algorithmic}
\end{algorithm}



\noindent \textbf{(b) Data Outlier Score.} 
The data distribution of the substitution data and the original data is different.
Therefore, the confusing edge data should be excluded, \ie, the data with different distributions but also predicted as the same target class.
%Due to different labels of the two data domains, lots of unlabeled data are classified as wrong classes because there is no more suitable target class.
For example, a tiger is predicted as the class of cat, as shown in the orange part of Stage 1 in Figure~\ref{fig2}.
We first separate the teacher predictions according to the predicted classes as $\rho _{i,c}= {p_{i}\in c}$.
For each class, $\rho _{i,c}$ is clustered \cite{johnson2019billion} to explore the intra-class relationships through $k$ layering as $\mu_{c,k}$.
Then the prediction features for the whole unlabeled dataset can be expressed as a group of $CK$ prototypes as $\left \{ \mu_{c,k}\in \mathbb{R}^{1 \times C} \right \} _{c,k=1}^{C, K} $, where
$c$ denotes the $c$-th class, and $K$ denotes the hyperparameter representing the number of prototypes for each class.
The prototype centers of the $c$-th class can be expressed as $\left \{ \mu_{c,k} \right \} _{k=1}^{K}$.
The outlier of each unlabeled data $x_i$ can be calculated with the sum of the prototype centers of its class as $\tilde{o}_{i}=\textstyle \sum_{k=1}^{K} \cos(\rho _{i,c}, \mu_{c,k}) $, where $\cos$ denotes the cosine similarity metric.
Similar to the confidence score, we select the maximum value $\max\!\left \{\tilde{o} \right \} $ for normalization.
As a result, the outlier score can be calculated as: $so_{i}  = \frac{\tilde{o}_{i}}{\left |\max\left \{\tilde{o} \right \}  \right | }$.


\noindent \textbf{(c) Class Density Score.} 
To help the student learn various classes effectively, we calculate the class density for the class balance of the sampled data.
As shown in Stage 1 of Figure~\ref{fig2}, we increase the sampling range for classes with sparse data (the blue part) while we reduce the sampling range for classes with redundant data (the orange part).
Based on this, we first separate the above intra-class outliers $\tilde{o}_{i}$ of all data by their predicted classes.
The outliers mean value of each class can be calculated as $u_{c} = \frac{1}{n_{c} } \sum \, _{p_{i} \in c}\tilde{o}_{i}$, where $n_{c}$ is the number of the data predicted as $c$-th class.
Therefore, the Dcluster parameter $D_{c}$ can be calculated as: $D_{c} =  \frac{\sqrt{u_{c}} }{\log_{e}{(n_{c}+C)}}$, which reflects the data density predicted to be $c$-th class.
The introduction of a constant $C$ (the number of classes) helps the numerical stability when the available unlabeled data is small while having little effect on the results when the amount of data is sufficient (under normal conditions).
%Each data's density value equals the density value of its class as: $d_{i} \!= \! D_{c}({p_{i} \in c})$.
%After selecting the maximum value for normalization, the density score of each data can be calculated as: $sd_{i} \!=\! \frac{d_{i}}{\left |\max\left \{ d \right \} \right |}$.
After selecting the maximum value $\max\!\left \{ D \right \}$ for normalization, the density score of each data can be calculated according to the predicted class as $sd_{i} = \frac{D_{c}}{\left |\max\left \{ D \right \} \right |}, \text{when} \;\; {\arg\max(p_{i}) = c} $.



Finally, we calculate the total score as $S_{total} \! = \! sc_{i} \!- \! so_{i}  \!+ \! sd_{i}$.
Based on this, the data closer to the distribution of the original data domain are sampled, which can help the student learn better.
The quantitative analysis is shown in Table \ref{tab6}.

%the sampled data should avoid data with low confidence that are easy to cause label noise and data with significant outliers that are not conducive to learning. 
%In addition, the sampled data should meet the class balance to prevent unbalanced learning for the student.




%These samples are closer to the distribution of the original data domain and can help the student learn better.
%So we define the data score $ S_{1} $ to measure two formers and the class score $ S_{2} $ to measure the latter.
%$ S_{1} \! = \! sc_{i}- so_{i}, \;$
%$ S_{2} \! = \! sd_{i}$.
%Then the total score for each data $S_{total} $ is calculated as: $S_{total} = S_{1}+S_{2}$.
%According to the total score, the data closer to the distribution of the original data domain are sampled, which can help the student learn better.








\subsection{Denoising Contrastive Relational Distillation}

After obtaining the high score data, the distillation process can be carried out.
We denote $f_{T}$ and $ f_{S}$ as the teacher and student networks and denote $\bm{x}$ as the data in sampled set $X_{A}$. 
According to the definition \cite{hinton2015distilling}, the knowledge distillation loss is calculated as:
%The above process can be expressed as:
$$ \mathcal{L}_{KD}=\sum_{\bm{x} \in X_{A}} D_{KL}(f_{T}(\bm{x})/\tau_{kd}, f_{S}(\bm{x})/\tau_{kd}), \eqno{(1)} $$where $D_{KL}$ is the Kullback-Leibler divergence, and $\tau_{kd}$ is the distillation temperature. 
$\mathcal{L}_{KD}$ allows the student to imitate the teacher's output. 
However, the main challenge is the distribution differences between the substitute and original data domains, leading to label noise interference.
Simultaneously, the ground-truth labels are unavailable, so correct information supervision is missing. 
Therefore, we propose a Denoising Contrastive Relational Distillation (DCRD) module, which includes a spatial mapping denoise component and a contrastive relationship representation component to help the student get better performance and mitigate label noise.

%Therefore, the main challenges are to deeply mine the knowledge of unlabeled data and low-computational noise suppression.


%\subsubsection{Manifold Noisy Distillation}
\noindent \textbf{Spatial Mapping Denoise.}
%manifold representation learning
The distribution in the unlabeled data differs from the unavailable original data, which indicates the inevitable label noise.
Inspired by manifold learning \cite{seung2000manifold}, low dimensional information representation contains purer knowledge with less noise interference \cite{ALGAN2021106771}.
Here, we utilize a low-dimensional spatial mapping denoise component to help the student learn low-noise knowledge representation.
Based on this, we perform eigendecomposition $\Phi$ on the teacherâ€™s prediction and its transposed product matrix \cite{hornmatrix}.
According to the distance invariance, the autocorrelation matrix $d_{ij}^{2} $ in a mini-batch as:
$$ \sum_{i}^{N}\sum_{j}^{N} d_{ij}^{2}= 2N\cdot tr(Z_{t}Z_{t}^{T} ), \eqno{(2)}$$where $N$ denotes the batch size, and $tr(\cdot)$ denotes the trace of a matrix. 
$Z_{t}$ is the low-dimensional spatial vector representation from the teacher calculated as $\Phi (f_{T}(\bm{x})\cdot  f_{T}\!^{T}(\bm{x})) = Z_{t}=V_{t}\Lambda _{t}^{1/2}$, where $V_{t}$ is the eigenvalue, and $\Lambda _{t}$ is the eigenvector. 
Similarly, we can get the student predictions of low-dimensional representation as $Z_{s}$. 
Then, we set up a distillation loss to correct the impact of label noise by the spatial mapping of the two networks.
The spatial mapping denoise distillation loss is calculated as:
$$ \mathcal{L}_{n} =\ell_{h}(\Phi (f_{T}(\bm{x})\cdot  f_{T}\!^{T}(\bm{x})), \Phi (f_{S}(\bm{x})\cdot  f_{S}\!^{T}(\bm{x})) )  =\ell_{h}(Z_{t}, Z_{s}), \eqno{(3)}$$where $ \ell_{h} (\cdot , \cdot ) $ denotes the Huber loss. 
%We can match the teacher-student relationship in a low dimensional space to learn a low-noise knowledge representation by $\mathcal{L}_{n}$.





%\subsubsection{Data Expansion Distillation}
\noindent \textbf{Contrastive Relational Distillation.}
%In the past, the feature-based distillation methods and logit distillation methods often require careful and structured knowledge construction. 
%Generally speaking, it is difficult to build efficient structured knowledge in these locations to help students well.
%In this work, 
%In data-free knowledge distillation, the alternate samples are unlabeled leading to 
The missing supervision information limits the student's performance.
It is indispensable to adequately mine the knowledge in unlabeled data to compensate for the lack of information.
%Therefore, we set the mutual relations of data examples distillation method.
To avoid a single imitation of a particular data example, we build two kinds of structured relationships to mine knowledge from the data and the teacher.


%In order to overcome the difficulty of constructing effective structured knowledge in the past, 

%First, the selected samples use RandAugust{} to expand knowledge. 
%In the previous selection process, we selected samples that are more efficient.
%In the last part, we propose an efficient samples selection method.
%Usually, these samples are enough to master some relatively simple categories, while other difficult categories may not be well learned due to the deviation between the training domain and the testing domain.

%First, we use RandAugment \cite{cubuk2020randaugment} to expand data knowledge representation. 
%Unlike the previous data-free distillation methods, we hope the student network can learn knowledge directly from data rather than relying entirely on the teacher.
Firstly, the student can adequately explore the structured relation among multiple unlabeled data by learning the instance invariant.
%Then, the instance invariant is learned to enable the student to explore the structural relation between samples deeply.
$\bm{x}_{i}, \bm{x}_{j}$ are the different data in a mini-batch. 
We calculate the prediction difference between data as follows:
$$ \ell_{s}^{\bm{x}_{i}\bm{x}_{j}}=\frac{\cos (f_{S}(\bm{x}_{i}), f_{S}(\bm{x}_{j}))  / \tau }{ {\textstyle \sum_{k=1,k\ne i}^{2N}} \cos (f_{S}(\bm{x}_{i}), f_{S}(\bm{x}_{k}))/ \tau }, \eqno{(4)} $$where $\tau_{1}$ denotes contrastive temperature.
Next, we can calculate the consistency instance discrimination loss as:
$$ \mathcal{L}_{c1}=-\frac{1}{N}\sum_{j=1}^{N}  \log_{}{\ell_{s}^{\bm{x}_{j}\bar{\bm{x}}_{j}}},  \eqno{(5)}$$where $\bar{\bm{x}}_{j}$ denotes the strong data augmentation of $\bm{x}_{j}$. 
%The student can find knowledge directly from the multiple unlabeled data through data consistency learning.
This unsupervised method is especially effective when the teacher makes wrong results.





Secondly, we construct a structured contrastive relationship between the teacher and student, which promotes consistent learning between the teacher and student.
The structured knowledge transfer process is calculated as: 
$$ \ell_{ts}^{{\bm{x}}'_{i}}=\frac{\cos (f_{T}({\bm{x}}'_{i}), f_{S}({\bm{x}}'_{i}))  / \tau }{ {\textstyle \sum_{k=1,k\ne i}^{4N}} \cos (f_{T}({\bm{x}}'_{i}), f_{S}({\bm{x}}'_{k}))/ \tau }, \eqno{(6)} $$where $ {\bm{x}}' = {\bm{x} \cup \bar{\bm{x}}}$ denotes the set of the sampled data before and after strong data augmentation. 
And $ {\bm{x}}'$ contains $2N$ samples for each batch.
We calculate the teacher-student consistency loss as:
$$ \mathcal{L}_{c2}=-\frac{1}{2N}\sum_{j=1}^{2N}  \log_{}{\ell_{ts}^{{\bm{x}}'_{j}}}.  \eqno{(7)}$$The student can obtain better learning performance through the mixed structured and consistent relationship learning between the two networks. Then, the contrastive relational distillation loss is $\mathcal{L}_{c}\!=\! \mathcal{L}_{c1}+\mathcal{L}_{c2}  $.
Finally, we can get the total denoising contrastive relational distillation loss as:
$$
\mathcal{L}_{total}=\mathcal{L}_{K\!D}+\lambda_{1}\! \cdot \!\mathcal{L}_{n}+\lambda_{2}\! \cdot \! \mathcal{L}_{c}, \label{14}  \eqno{(8)}
$$where $\lambda_{1}$, $\lambda_{2}$ are the trade-off parameters for training losses.





\section{Experiments}



\subsection{Experimental Settings}
%\subsubsection{Datasets and Models.} 

\textbf{Datasets.}
We evaluate the proposed ODSD method for the classification and semantic segmentation tasks.
For classification, we evaluate it on widely used datasets: $32 \times 32$ CIFAR-10, CIFAR-100 \cite{krizhevsky2009learning}, and $224 \times 224$ ImageNet \cite{deng2009imagenet}.
For semantic segmentation, we evaluate the proposed method on $128 \times 128$ NYUv2 dataset \cite{silberman2012indoor}.
Besides, the corresponding open-world datasets are shown in Table~\ref{tab1}, which is the same as DFND \cite{chen2021learning} for a fair comparison. 




\noindent \textbf{Implementation Details.}
The proposed model is implemented in PyTorch \cite{paszke2019pytorch} and trained with RTX 3090 GPUs. 
%The experiment of CIFAR has five baseline settings, including the teacher and student using the same backbone or different backbones.
For the CIFAR-10 and CIFAR-100 datasets, we conduct five sets of backbone combinations, set two groups of different numbers of sampled samples (150k or 600k), and train the students for 200 epochs.
For the ImageNet dataset, we conduct three sets of backbone combinations and train the students for 200 epochs.
The number of sampled samples is 600k.
For the NYUv2 dataset, the DeeplabV3 \cite{chen2017rethinking} is used as the model architecture followed previous work.
The teacher uses ResNet-50 \cite{he2016deep} as the backbone, and the student uses mobilenetv2 \cite{sandler2018mobilenetv2}.
We sample 200k unlabeled samples and train the student for 20 epochs.
For the above datasets, we set $\tau_{kd}$ as 4 to be the same as other distillation methods and set $\tau$ as 0.5 to be the same as \cite{chen2020simclr}.
Besides, we set $\lambda_{1}$ as 10 and $\lambda_{2}$ as 0.5, use the SGD optimizer with momentum as 0.9, weight decay as $5\times10^{-4}$, the batch size $N$ as 64, and cosine annealing learning rate with an initial value of 0.025.



\begin{table}[t]
\centering
\tabcolsep=0.3cm
\caption{Illustration of original private data and their corresponding substitute open-world datasets.}
\setlength{\tabcolsep}{4.4mm}
\scalebox{0.95}{
\begin{tabular}{@{}c|ccc@{}}
\toprule[1pt]
\textbf{Original data}            & CIFAR    & ImageNet & NYUv2    \\
\textbf{Unlabeled data} & ImageNet & Flickr1M & ImageNet \\ \bottomrule[1pt]
\end{tabular}
}
\label{tab1}
\end{table}


\begin{table*}[t]
\centering
\caption{Student accuracy (\%) on CIFAR datasets. \textbf{Bold} and \underline{underline} numbers denote the best and the second best results.}
\renewcommand\arraystretch{1.}
\setlength{\tabcolsep}{4mm}
\scalebox{0.9}{
\begin{tabular}{clcccccc}
\toprule[1pt]
\multirow{2}{*}{\textbf{Dataset}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Method}}} & \multirow{2}{*}{\textbf{Type}} & \textbf{ResNet-34} &\textbf{VGG-11}    & \textbf{WRN40-2} & \textbf{WRN40-2} & \textbf{WRN40-2} \\
                            & \multicolumn{1}{c}{}                        &                       & \textbf{ResNet-18} & \textbf{ResNet-18} & \textbf{WRN16-1} & \textbf{WRN40-1} & \textbf{WRN16-2} \\ \midrule
\multirow{18}{*}{\rotatebox{90}{\textbf{CIFAR-10}}}  & Teacher & \multirow{3}{*}{-}          & 95.70     & 92.25     & 94.87   & 94.87   & 94.87   \\
                            & Student &                             & 95.20     & 95.20     & 91.12   & 93.94   & 93.95   \\
                            & KD      &                             & 95.58     & 94.96     & 92.23   & 94.45   & 94.52   \\ \cmidrule(l){2-8} 
                            & DeepInv  \cite{yin2020dreaming}  & \multirow{11}{*}{Generation} & 93.26     & 90.36     & 83.04   & 86.85   & 89.72   \\
                            & CMI \cite{fang2021contrastive}   &                             & 94.84     & 91.13     & 90.01   & 92.78   & 92.52   \\
                            & DAFL  \cite{chen2019}   &                             & 92.22     & 81.10     & 65.71   & 81.33   & 81.55   \\
                            & ZSKT  \cite{micaelli2019zero}   &                             & 93.32     & 89.46     & 83.74   & 86.07   & 89.66   \\
                            & DFED \cite{hao2021data}   &                             & \multicolumn{1}{c}{-}         & \multicolumn{1}{c}{-}         & 87.37   & 92.68   & 92.41   \\
                            & DFQ  \cite{choi2020data}     &                             & 94.61     & 90.84     & 86.14   & 91.69   & 92.01   \\
                            & Fast \cite{fang2022up}    &                             & 94.05     & 90.53     & 89.29   & 92.51   & 92.45   \\
                            & MAD \cite{do2022momentum}  &                             & 94.90     & \multicolumn{1}{c}{-}         & \multicolumn{1}{c}{-}       & \multicolumn{1}{c}{-}        & 92.64 \\   
                             & KAKR\_MB \cite{patel2023learning} & & 93.73 & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} \\
                             & KAKR\_GR \cite{patel2023learning} & & 94.02 & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} \\
                             & SpaceshipNet \cite{yu2023data} & & \underline{95.39} & \underline{92.27} & \underline{90.38}  & \underline{93.56} & \underline{93.25}
                            \\ \cmidrule(l){2-8} 
                            & DFND$\_150k$  \cite{chen2021learning}   & \multirow{4}{*}{Sampling}   & 94.18     & 91.77     & 87.95   & 92.56   & 92.02   \\
                            & DFND$\_600k$  \cite{chen2021learning}   &                             & 95.36    & 91.86     & 90.26   & 93.33   & 93.11   \\
                            & ODSD$\_150k$    &                             & 95.05     & 92.02    & 89.14   & 92.94   & 92.34   \\
                            & ODSD$\_600k$     &                             & \textbf{95.70}     & \textbf{92.55}     & \textbf{91.53}   & \textbf{94.31}   & \textbf{94.02}   \\ \midrule[1pt]
\multirow{18}{*}{\rotatebox{90}{\textbf{CIFAR-100}}} & Teacher & \multirow{3}{*}{-}          & 78.05     & 71.32     & 75.83   & 75.83   & 75.83   \\
                            & Student &                             & 77.10     & 77.10     & 65.31   & 72.19   & 73.56   \\
                            & KD      &                             & 77.87     & 75.07     & 64.06   & 68.58   & 70.79   \\ \cmidrule(l){2-8} 
                            & DeepInv \cite{yin2020dreaming}  & \multirow{11}{*}{Generation} & 61.32     & 54.13     & 53.77   & 61.33   & 61.34   \\
                            & CMI \cite{fang2021contrastive}     &                             & 77.04     & 70.56     & 57.91   & 68.88   & 68.75   \\
                            & DAFL \cite{chen2019}     &                             & 74.47     & 54.16     & 20.88   & 42.83   & 43.70   \\
                            & ZSKT  \cite{micaelli2019zero}    &                             & 67.74     & 54.31     & 36.66   & 53.60   & 54.59   \\
                            & DFED \cite{hao2021data}    &                             & \multicolumn{1}{c}{-}          & \multicolumn{1}{c}{-}          & 41.06   & 60.96   & 60.79   \\
                            & DFQ  \cite{choi2020data}     &                             & 77.01     & 66.21     & 51.27   & 54.43   & 64.79   \\
                            & Fast \cite{fang2022up}   &                             & 74.34     & 67.44     & 54.02   & 63.91   & 65.12   \\
                            & MAD \cite{do2022momentum}   &                             & 77.31     & \multicolumn{1}{c}{-}       & \multicolumn{1}{c}{-}       & \multicolumn{1}{c}{-}       & 64.05 \\  & KAKR\_MB \cite{patel2023learning} & & 77.11 & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} \\
                             & KAKR\_GR \cite{patel2023learning} & & 77.21 & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} \\
                             & SpaceshipNet \cite{yu2023data} & & 77.41 & 71.41 & 58.06 & 68.78 & 69.95
                            \\ \cmidrule(l){2-8} 
                            & DFND$\_150k$  \cite{chen2021learning}     & \multirow{4}{*}{Sampling}   & 74.20     & 69.31     & 58.55   & 68.54   & 69.26   \\
                            & DFND$\_600k$  \cite{chen2021learning}     &                             & 74.42     & 68.97     & 59.02   & 69.39   & 69.85   \\
                            & ODSD$\_150k$    &                             & \underline{77.90}     & \underline{72.24}     & \underline{60.55}   & \underline{71.66}   & \underline{72.42}   \\
                            & ODSD$\_600k$    &                             & \textbf{78.45}     & \textbf{72.71}     & \textbf{60.57}   & \textbf{72.71}   & \textbf{73.20}   \\ \bottomrule[1pt]
\end{tabular}
}
\label{tab2}
\end{table*}




%\subsubsection{Baselines.} 
\noindent \textbf{Baselines.}
We compare two kinds of DFKD methods.
One is to spend extra computing costs to obtain generation data by generation module, including DeepInv \cite{yin2020dreaming}, CMI \cite{fang2021contrastive}, DAFL \cite{chen2019}, ZSKT \cite{micaelli2019zero}, DFED \cite{hao2021data}, DFQ \cite{choi2020data}, Fast \cite{fang2022up}, MAD \cite{do2022momentum}, DFD \cite{luo2020large}, KAKR \cite{patel2023learning}, SpaceshipNet \cite{yu2023data}, and DFAD \cite{fang2019}. 
Another is to use unlabeled data from easily accessible open source datasets based on sampling, \ie, DFND \cite{chen2021learning}.







\subsection{Performance Comparison}
To evaluate the effectiveness of our ODSD, we comprehensively compare it with current SOTA DFKD methods regarding the student's performance, the effectiveness of the sampling method, and training costs. 
% In addition, some other methods only apply to small datasets and models (\eg, MNIST and AlexNet) for some reason.
% The test baselines mentioned in the article may be difficult for these methods, causing serious performance decline and non-competitive experimental results.
% To compare these methods fairly, we conduct experiments on the MNIST dataset and maintain the same experimental settings.



%\subsubsection{CIFAR-10 \& CIFAR-100.}
\noindent \textbf{Experiments on CIFAR-10 and CIFAR-100.}
We first verify the proposed method on the CIFAR-10 and CIFAR-100 \cite{krizhevsky2009learning}.
%We collate the performance of two kinds of SOTA methods based on data generation and data sampling.
The baseline ``\textit{Teacher}" and ``\textit{Student}" means to use the corresponding backbones of the teacher or student for direct training with the original training data, and ``\textit{KD}" represents distilling the student network with the original training data.
Generation-based methods include training additional generators and calculating model gradient inversion.
Sampling-based methods use the unlabeled ImageNet dataset. 
We reproduce the DFND using the unified teacher models, and the result is slightly higher than the original paper.


As shown in Table~\ref{tab2}, our ODSD has achieved the best results on each baseline.
Under most baseline settings, ODSD brings gains of 1\% or even higher than the SOTA methods, even though students' accuracy is very close to their teachers.
In particular, the students of our ODSD outperform the teachers on some baselines.
As far as we know, it is the first DFKD method to achieve such performance. 
The main reasons for its breakthrough in analyzing the algorithm's performance come from three aspects.
First, our data sampling method comprehensively analyzes the intra-class relationships in the unlabeled data, excluding the difficult edge data and significant distribution differences data. 
At the same time, the number of data in each class is relatively more balanced, which is conducive to all kinds of balanced learning compared with other sampling methods.
Second, our knowledge distillation method considers the representation of low-dimensional and low-noise information and expands the representation of knowledge through data augmentation.
The structured relationship distillation method helps the student effectively learn knowledge from both multiple data and its teacher.
Finally, the knowledge of our ODSD does not entirely come from the teacher but also the consistency and differentiated representation learning of unlabeled data, which is helpful when the teacher makes mistakes.
The previous methods ignore the in-depth mining of data knowledge, decreasing students' performance.



\begin{table}[t]
\centering
\caption{Student accuracy (\%) on ImageNet dataset.}
\renewcommand\arraystretch{1.}
\setlength{\tabcolsep}{0.06cm}
\scalebox{0.9}{
\begin{tabular}{@{}lcrrr@{}}
\toprule[1pt]
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Type}}       & \textbf{ResNet-50}             & \textbf{ResNet-50} & \textbf{ResNet-50}             \\
                        &                             & \textbf{ResNet-18}             & \textbf{ResNet-50} & \textbf{MobileNetv2}           \\ \midrule
Teacher                 & \multirow{3}{*}{-}                           & 75.59                & 75.59    & 75.59                \\
Student                 &                            & 68.93                & 75.59    & 63.97                \\ 
KD                      &   & 68.10               & 74.76    & 61.67                \\ \midrule
DFD \cite{luo2020large}                     & \multirow{3}{*}{Generation}                           & \underline{54.66}                & \underline{69.75}    & \underline{43.15}                \\
DeepInv$_{2k}$ \cite{yin2020dreaming}                &                             & \multicolumn{1}{c}{\; \; \; -} & 68.00    & \multicolumn{1}{c}{\; \; \; \; \; \; -} \\
Fast$_{50}$ \cite{fang2022up}                   &                             & 53.45                & 68.61    & 43.02                \\ \midrule
DFND \cite{chen2021learning}                   & \multirow{2}{*}{Sampling}  & 42.82                & 59.03    & 16.03                \\
\textbf{ODSD}                    &                             & \textbf{58.24}                & \textbf{71.25}    & \textbf{52.74}                \\ \bottomrule[1pt]
\end{tabular}
}
\label{tab3}
\end{table}


\begin{table}[t]
\centering
\caption{Total FLOPs and params in DFKD methods.}
\tabcolsep=0.13cm
\scalebox{0.9}{
\begin{tabular}{@{}l|ccccccc@{}}
\toprule
Method & DeepInv & CMI   & DAFL   & ZSKT   & DFQ    & DFND   & \textbf{ODSD}   \\ \midrule
FLOPs  & 4.36G    & 4.56G  & 0.67G & 0.67G & 0.79G & 0.56G & 0.56G \\
params & 11.7M   & 12.8M & 12.8M  & 12.8M  & 17.5M  & 11.7M  & 11.7M  \\ \bottomrule
\end{tabular}
}
\label{tab4}
\end{table}





%\subsubsection{ImageNet.}
\noindent \textbf{Experiments on ImageNet.}
We conduct experiments on a large-scale ImageNet dataset to further verify the effectiveness.
Due to the larger image size, it is challenging to effectively synthesize training data for most generation-based methods.
Most of them failed.
A small number of methods train 1,000 generators (one generator for one class), resulting in a large amount of additional computational costs.
In this case, our sampling method reduces the computational costs more significantly.
We set up three baselines to compare the performance of our method with the SOTA methods.
Table \ref{tab3} reports the experimental results. 
Our ODSD still achieves several percentage points increase compared with other SOTA methods, especially in the cross-backbones situation (9.59\%).
Due to the lack of structured knowledge representation, the DNFD algorithm performs poorly on the large-scale dataset.
Comparing DFND and ODSD, our structured framework improves the overall understanding ability of the student. 
%Moreover, our method can better face the problem of the teacher and student with different backbones.




%\subsubsection{Training Costs.}
\noindent \textbf{Comparison of Training Costs.}
To verify that the generation-based methods add extra costs that we mentioned in the introduction section, we further calculate the total floating point operations (FLOPs) and parameters (params) required by various DFKD algorithms, as shown in Table \ref{tab4}.
Our method only needs training costs and params of the student network without additional generation modules.
Our sampling process introduces 256.78 seconds for sample selection ($K=5$) on the CIFAR100 with a single RTX 3090 GPU (The teacher uses the \textit{ResNet-34}) while the fastest generation-based method ZSKT also takes 1.54 hours to synthesize data.
These generation modules will be discarded after student training, which causes a waste of computing power. 









\begin{table}[t]
\centering
\caption{APS compared with the SOTA sampling method.}
\tabcolsep=0.25cm
\scalebox{0.9}{
\begin{tabular}{@{}cccc@{}}
\toprule
\multirow{2}{*}{Sampling methods} & \multicolumn{3}{c}{Method} \\
                                   & KD       & DFND     & \textbf{ODSD}    \\ \midrule
Random                             & 76.85    & 73.15    & 76.43   \\
DFND                               & 76.67    & 73.68    & 77.40   \\
\textbf{APS}                         & \textbf{77.27}    & \textbf{73.89}    & \textbf{77.90}   \\ \bottomrule
\end{tabular}
}
%\end{center}
\label{tab5}
\end{table}




%\subsubsection{Data Sampling Efficiency.}
\noindent \textbf{Comparison of Data Sampling Efficiency.}
To verify the sampling mechanism's effectiveness, we compare our APS method with the current SOTA unlabeled data sampling method DFND \cite{chen2021learning}.
Three data sampling methods (random sampling, DFND sampling, and our proposed APS) are set on three different distillation algorithms, including: KD \cite{hinton2015distilling}, DFND \cite{chen2021learning}, and our ODSD method.
Table \ref{tab5} reports the results.
For KD, we use the sampled data instead of the original generated data with $\mathcal{L}_{K\!D}$ distillation loss.
From the result, this setting is competitive, even better than the distillation loss of DFND.
For DFND, we reproduce it with open-source codes and keep the original training strategy unchanged.
We find the performance of the DFND sampling method is unstable, which causes it to be lower than random sometimes.
For ODSD, we use the distillation loss in Equation (8).
Our proposed sampling method achieves the best performance in all three benchmarks and significantly improves performance.
By comprehensively considering the data confidence, the data outliers, and the class density, our ODSD can more fully mine intra-class relationships of the unlabeled data.
As a result, the sampled data are more helpful for subsequent student learning.
%This way, the sampled data are more instrumental for subsequent student training.






%\subsubsection{Segmentation Results.}
\noindent \textbf{Experiments about Semantic Segmentation.}
In addition to image classification tasks, our algorithm can also effectively solve the problem of DFKD in image semantic segmentation on the NYUv2 dataset.
Mean Intersection over Union (mIoU) is set as the segmentation evaluation metric.
No generation module is defined for our method, and other settings are the same as DFAD \cite{fang2019}.
Table~\ref{tab7} shows segmentation results on the NYUv2 dataset. 
Our ODSD also achieves the best performance.
Besides, we visualize the segmentation results of different networks to get more convincing results as shown in Figure~\ref{fig3_sup}.
``\textit{Input}" and ``\textit{Ground Truth}" represent the input test data and their corresponding real labels. 
Most data-free distillation algorithms hide the code of the segmentation part, so it is not easy to make a visual comparison. 
Here, we choose DFAD as the baseline algorithm of visualization.
Our proposed ODSD algorithm achieves better segmentation results than DFAD, especially for object contour segmentation.
The slight noise around the contour is effectively suppressed. 
Further, through in-depth mining the knowledge from the data and teacher, our student have gained better understanding ability.



\begin{table}[t]
\centering
\caption{Segmentation results on NYUv2 dataset.}
\tabcolsep=0.11cm
\scalebox{0.9}{
\begin{tabular}{@{}c|cc|ccc|cc@{}}
\toprule[1pt]
Algorithm & Teacher    & Student   & DAFL      & DFAD     & Fast     & DFND  & \textbf{ODSD}                    \\
mIoU      & 0.517      & 0.375     & 0.105     & 0.364    & 0.366    & \underline{0.378} & \textbf{0.397} \\ \bottomrule[1pt]
\end{tabular}
}
\label{tab7}
\end{table}



\begin{table}[t]
\centering
\caption{Diagnostic studies of the proposed method.}
\tabcolsep=0.18cm
\scalebox{0.9}{
\begin{tabular}{@{}cccccccc@{}}
\toprule
\multicolumn{4}{c|}{Training objective $\mathcal{L}$}                                                         & \multicolumn{4}{c}{Data sampling scores $S$}        \\ \midrule
\multicolumn{1}{c|}{\multirow{2}{*}{ID}} &
  \multirow{2}{*}{Setting} &
  \multicolumn{2}{c|}{Accuracy (\%)} &
  \multicolumn{1}{c|}{\multirow{2}{*}{ID}} &
  \multirow{2}{*}{Setting} &
  \multicolumn{2}{c}{Accuracy (\%)} \\
\multicolumn{1}{c|}{}          &                           & 50k   & \multicolumn{1}{c|}{150k}  & \multicolumn{1}{c|}{}    &       & 50k   & 150k  \\ \midrule
\multicolumn{1}{c|}{(1)}       & ours                      & \textbf{75.26} & \multicolumn{1}{c|}{\textbf{77.90}} & \multicolumn{1}{c|}{(5)} & ours  &  \textbf{75.26}     & \textbf{77.90} \\
\multicolumn{1}{c|}{(2)}       & w/o  $\mathcal{L}_{n}$ & 74.82 & \multicolumn{1}{c|}{77.71} & \multicolumn{1}{c|}{(6)} & w/o $sc_{i}$   &   73.96    & 77.04 \\
\multicolumn{1}{c|}{(3)}       & w/o $\mathcal{L}_{c} $    & 74.71 & \multicolumn{1}{c|}{77.58} & \multicolumn{1}{c|}{(7)} & w/o $so_{i}$   &   68.07    & 76.67 \\
\multicolumn{1}{c|}{(4)}       & w/o $\mathcal{L}_{n},\mathcal{L}_{c} $       & 74.39 & \multicolumn{1}{c|}{77.27} & \multicolumn{1}{c|}{(8)} & w/o $sd_{i}$   &   70.24    & 76.59 \\ \bottomrule
\end{tabular}
}
\label{tab6}
\end{table}





% Figure environment removed



\noindent \subsection{Diagnostic Experiment}
We conduct the diagnostic studies on the CIFAR-100 dataset. 
We use ResNet-34 as the teacher's backbone and ResNet-18 as the student's backbone. 
50k and 150k data are sampled.
Other settings are the same as the Table~\ref{tab2}.


\noindent \textbf{Distillation Training Objective.}
We first investigate our overall training objective (cf. Equation (8)). 
Two different data sampling numbers are set in this experiment.
As shown in the experiments (1-4) of Table \ref{tab6}, the model with $\mathcal{L}_{KD}$ alone achieves accuracy scores of $74.39\%$ and $77.27\%$ on 50k and 150k data sampling settings. 
Adding $\mathcal{L}_{n}$ or $\mathcal{L}_{c} $ individually brings gains (\ie, $\textbf{0.32\%, 0.31\%/ 0.43\%, 0.44\%}$), indicating the effectiveness of our proposed distillation method.
Our method performs better with $\textbf{75.26\%}$ and $\textbf{77.90\%}$.
With the above results, the proposed training objectives are effective and can help the student gain better performance.




% Figure environment removed


%\subsubsection{Data Sampling Scores.}
\noindent \textbf{Data Sampling Scores.}
To verify the effectiveness of the three sampling scores in section 3.2, we further conduct ablation experiments.
Using all scores, the model can achieve the best performance with $\textbf{75.26\%}$ and $\textbf{77.90\%}$ accuracy shown in experiments (5-8) of Table \ref{tab6}.
When the confidence score $sc_{i}$ is abandoned, the familiarity of the teacher network with the sampled data decreases, reducing the amount of adequate information contained in the data.
Without the outlier score $so_{i}$, the lack of modeling of the intra-class relationship of the data to be sampled leads to increased data distribution difference between the substitute data domain and the original data domain.
Further, the class density score $sd_{i}$ can measure the number of data in each class and maintain the balance of the sampled data.
In summary, all three score indicators can help students perform better.





\subsection{Visualization}

To verify the distribution similarity between the sampled data and the original data of our APS sampling method and the DFND sampling method, we use t-SNE \cite{van2008visualizing} to visualize the data feature distribution.
Teacher uses ResNet-34 as the backbone on the CIFAR-100 and ResNet-50 as the backbone on the ImageNet. 
For both datasets, we reserve 100 classes from validation data.
In addition, we also visualize the distribution of data obtained by random sampling as a baseline reference.
Figure \ref{fig3} shows the data distribution results of different sampling methods.
Our clustering results are closer to the extracted features of the original data.
For the more complex ImageNet, this advantage is further amplified.
Reducing the distribution difference between sampled and original data helps reduce data label noise, which is the key for the student to perform well.










%clearer than the DFND diagram classification, and is very close to the prediction of the teacher network.
%Thanks to the efficient sample selection method and effective knowledge distillation method, our method has obtained clearer results.




\section{Conclusion}

\iffalse
In this paper, we propose an Open-world Data Sampling Distillation (ODSD) method without unnecessary generation costs.
First, we model the intra-class relationship of unlabeled data to obtain data closer to the original data distribution. 
Then, we use low-noise knowledge representation to suppress the label noise of the substitute data. 
Further, structured contrastive knowledge is constructed to adequately exploit the knowledge in data and the teacher.
Extensive experiments have illustrated the effectiveness of our proposed data sampling and distillation methods.
As a result, the proposed method achieves a significant improvement and state-of-the-art performance on various benchmarks.
Especially, it improve 1.50\%-9.59\% accuracy on the ImageNet dataset compared with existing methods.
\fi


Most existing data-free knowledge distillation methods rely heavily on additional generation modules, bringing additional computational costs. 
Meanwhile, these methods disregard the domain shifts issue between the substitute and original data and only consider the teacher's knowledge, ignoring the data knowledge.
This paper proposes an Open-world Data Sampling Distillation method.
We sample unlabeled data with a similar distribution to the original data and introduce low-noise knowledge representation learning to cope with domain shifts.
To explore the data knowledge adequately, we design a structured knowledge representation.
Comprehensive experiments illustrate the effectiveness of the proposed method, which achieves significant improvement and state-of-the-art performance on various benchmarks.


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}


\section*{Acknowledgements}
This work is supported by the Shanghai Engineering Research Center of AI \& Robotics, Fudan University, China, the Engineering Research Center of AI \& Robotics, Ministry of Education, China, and the Green Ecological Smart Technology School-Enterprise Joint Research Center.



%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.

\bibliographystyle{IEEEtran}
\bibliography{arxiv.bib}

\end{document}


