{
  "title": "Sampling to Distill: Knowledge Transfer from Open-World Data",
  "authors": [
    "Yuzheng Wang",
    "Zhaoyu Chen",
    "Jie Zhang",
    "Dingkang Yang",
    "Zuhao Ge",
    "Yang Liu",
    "Siao Liu",
    "Yunquan Sun",
    "Wenqiang Zhang",
    "Lizhe Qi"
  ],
  "submission_date": "2023-07-31T12:05:55+00:00",
  "revised_dates": [
    "2024-07-21T14:32:53+00:00"
  ],
  "abstract": "Data-Free Knowledge Distillation (DFKD) is a novel task that aims to train high-performance student models using only the pre-trained teacher network without original training data. Most of the existing DFKD methods rely heavily on additional generation modules to synthesize the substitution data resulting in high computational costs and ignoring the massive amounts of easily accessible, low-cost, unlabeled open-world data. Meanwhile, existing methods ignore the domain shift issue between the substitution data and the original data, resulting in knowledge from teachers not always trustworthy and structured knowledge from data becoming a crucial supplement. To tackle the issue, we propose a novel Open-world Data Sampling Distillation (ODSD) method for the DFKD task without the redundant generation process. First, we try to sample open-world data close to the original data's distribution by an adaptive sampling module and introduce a low-noise representation to alleviate the domain shift issue. Then, we build structured relationships of multiple data examples to exploit data knowledge through the student model itself and the teacher's structured representation. Extensive experiments on CIFAR-10, CIFAR-100, NYUv2, and ImageNet show that our ODSD method achieves state-of-the-art performance with lower FLOPs and parameters. Especially, we improve 1.50\\%-9.59\\% accuracy on the ImageNet dataset and avoid training the separate generator for each class.",
  "categories": [
    "cs.CV"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.16601",
  "pdf_url": "https://arxiv.org/pdf/2307.16601v2",
  "comment": "Accepted by ACMMM2024",
  "num_versions": null,
  "size_before_bytes": 8264085,
  "size_after_bytes": 476384
}