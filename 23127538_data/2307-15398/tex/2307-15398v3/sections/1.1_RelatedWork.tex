%
\label{sec:RelatedWork}

In formalizing the ISO problem, our work revives the role of position bias within a screening process involving human screeners.
% Click models
Works on search engine click models were the first to formalize \cite{CraswellZTR08_ExperimentsClickPositionBias} and test \cite{DBLP:journals/jcmc/PanHJLGG07, DBLP:conf/clef/GrotovCMSXR15, DBLP:conf/www/RichardsonDR07} how users search over an ISO.
These works 
% from the early 2000s 
modeled the different clicking procedures observed in users and
% , using among others eye-tracking technology, 
provided experimental evidence for the existence of position bias.
% The name choice of the two search procedures in Section~\ref{sec:ProblemFormulation.Algorithms} is a reference to this line of work. 
Hence, it is not surprising that today product owners are willing to pay premiums to search engines and similar platforms for the first spots in the search results of a user \cite{10.1093/qje/qjr028}.
Different from these works, we consider a user that chooses (i.e., ``clicks'') more than one item. Further, we formalize such users under a utility-maximizing framework with fairness constraints, which relates to past set selections works \cite{stoyanovich2018online}.

% Overall fairness
Within algorithmic fairness, focus has been mainly on learning an algorithm that provides a fair ISO \cite{Zehlike2023_FairRanking_P1, Zehlike2023_FairRanking_P2, DBLP:journals/vldb/PitouraSK22}.
% Within this literature, position bias is viewed as a technical bias, meaning it is due to platform design or inherent to the human subject.
Works on probability-based \cite{DBLP:conf/ssdbm/YangS17, DBLP:conf/cikm/ZehlikeB0HMB17} and exposure-based \cite{DBLP:journals/cacm/Baeza-Yates18, DBLP:journals/sigir/JoachimsGPHG17} fairness tackle position bias by (re-)arranging the ISO so that is fair according to, respectively, some model for user attention and spot allocations. 
These works, however, avoid formalizing the user for which the fair ISO is meant, e.g., assuming a complete search of the (re-)arranged ISO by the user.
Based on the click models and our own experience with G, we argue that it is important to model the human user to fully grasp the ISO problem.
The fair set selection literature, including the fair ranking, is vast; we position our work within this literature in Section~\ref{sec:Add_RelatedWork} after we have formulated the ISO problem.

% Evidence of position bias
We highlight two recent works that provide evidence for position bias and the implicit role of the ISO.
\citet{DBLP:conf/chi/EchterhoffYM22} collaborate with a college to study anchoring bias \cite{Kahneman2011Thinking} in admissions;
anchoring bias \cite{tversky_judgment_1974} occurs when a candidate's evaluation is conditioned by the quality of the previously evaluated candidates.
This paper finds that the same candidate is better off if it is preceded by worst candidates than better candidates, which violates individual fairness \cite{DBLP:conf/innovations/DworkHPRZ12}, and
% It occurs because the human screener anchors its expectations on a lower reference point despite each candidate being independent from each other. 
proposes an algorithm for capturing and balancing anchoring bias.
% Moreover, 
\citet{PeiEAAMO2023} work with a college to study how the platforms used by professors for evaluating homework affects the students' grades. 
This paper runs a set of experiments varying the order in which the homeworks are presented by the platform. 
The experiments shows that the default sorting order, which is in alphabetical order, unfairly rewards students with the same work quality depending on their last names, which violates individual fairness \cite{DBLP:conf/innovations/DworkHPRZ12}. 

Both papers provide evidence of the ISO problem (w.r.t.~fairness) and, similar to our work, highlight the role of the human using the ISO.
However, we differ from these works
% and the few other works based on real world collaborations (e.g.,~\citet{SukumarMH18_PeacanPie}) 
in that we provide a simulations framework supported by our problem formulation flexible enough to capture multiple screening scenarios. 
Gathering experimental data or having access to the real process are costly and limited to stakeholders studying questions of fairness, which positions simulations as useful tools for answering these questions \cite{DBLP:conf/fat/IonescuHJ21, DBLP:journals/corr/abs-2006-09663, DBLP:conf/fat/BountouridisHMM19, Bokanyi2020_Understanding, Schelling1971_Dynamic}.
% Our work, e.g., can be useful to simulate different ISO problems to inform the human screener and diminish the role of the position bias. 
We come back to this point in Section~\ref{sec:Discussion}.

%
% EOS
%
