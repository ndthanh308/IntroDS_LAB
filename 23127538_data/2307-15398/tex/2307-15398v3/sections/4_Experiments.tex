%
\label{sec:Experiments}

We now explore the implications of the best-$k$ and good-$k$ problems through Monte Carlo experiments for both the algorithmic $h_a$ and human-like $h_h$ screeners.
Moreover, this section shows how our framework can explore different screening scenarios involving the initial screening order (ISO).
The algorithms~\ref{algo:Examination}--\ref{algo:HumanCascade} and simulation procedures are developed in R \cite{Rlang}.\footnote{See the GitHub repository for the code; \url{https://github.com/cc-jalvarez/initial-screening-order-problem/tree/main}.}
%
% The source code is provided in an anonymous repository: \url{https://anonymous.4open.science/r/initial-screening-order-problem-D078}.

\subsection{Experimental Setup}
\label{sec:Experiments.Setup}

% We consider one possible setup; naturally, these inputs can be changed to account for other screening settings.

\subsubsection{Generating the sample.}
We assume a sample consisting of $n$ triplets $\{ (\theta({c_i}), s(\mathbf{X}_{c_i}), W_{c_i}) \}_{i=1}^n$ 
% for $i = 1, \ldots, n$, 
drawn from a probability distribution with domain 
$\mathcal{G}_n \times \mathbb{R}^n \times \{0, 1\}^n$ where $\mathcal{G}_n$ is the set of all permutations of $\{1, \ldots, n\}$. 
Each sample represents a specific candidate pool $\candidatesset$ sorted according to an ISO $\theta$.
 
Regarding $s(\mathbf{X}_{c_i})$, we consider three possible distributions of candidate scores. 
All are based on the truncated normal distribution family $tN(\mu, \sigma)$ \cite{Botev2017} with values bounded in the interval $[0, 1]$.
Here, we wish to model scenarios in which very good candidates, as in those with top scores, occur with different probabilities.
All three scenarios are shown in Figure~\ref{fig:1} (left).
% %
% \begin{itemize}
    % \item 
    \textit{A symmetric distribution of scores} (in red) defined by $tN(0.5, 0.02)$ with mean/median of $0.5$ implies that top candidates occur with a very low probability. 
    % Hence, setting a large minimum basic requirement $\psi$ is highly selective of top candidates.
    %
    % \item 
    \textit{An asymmetric distribution of scores} (in blue) defined by $tN(0.8, 0.05)$ implies that top candidates occur with a higher probability and median value ($\approx 0.75$) compared to the previous scenario. 
    % Hence, setting a large $\psi$ is less selective of top candidates.
    %
    % \item 
    \textit{An increasing distribution of scores} (in green) defined by $tN(1, 0.05)$ implies that top candidates occur with an even higher probability and median value ($\approx 0.85$). 
    % In this scenario, there are many good candidates, and thus $\psi$ is not selective.
% \end{itemize}
% %

The score scenarios have implications, in particular, for the good-k problem where we must set the minimum score $\psi$ and the screener is not required to explore all of $\candidatesset$ under $\theta$ (recall Remark~\ref{remark:ISOandPS}).
Setting a large $\psi$ makes the screening process highly selective in the first setting, less selective in the second setting, and not selective at all in the third setting.
These settings represent a range of candidate pools with different candidate quality on average.
% For instance, in the ideal setting of having to choose from mostly top candidates (the third setting), the screener is not worried of choosing a high $\psi$.

Regarding $\theta$, we consider two possible variants for the ISO.
%
First: \textit{the ISO is randomly and independently generated from the candidate scores.} 
This setting models the case in which $\theta$ brings no information regarding candidate quality, and the screener prefers the alphabetical order or performs a random shuffle of candidates. 
We denote such a scenario as $\theta \ci s$.
%
Second: \textit{The ISO is randomly generated generated with a given Spearman's rank correlation $\rho$ with the candidate scores.}
Formally, $\rho$ is the Spearman's rank correlation of the pairs $\{ (\theta(c_i), s(\mathbf{X}_{c_i})) \}_{i=1}^k$. 
% for $i=1, \ldots, n$. 
It assesses how well $\theta$ monotonically relates to the scores of candidates.\footnote{To generate correlated initial ordering and scores, we rely on copulas -- see e.g., \cite[Section 3.4]{EMBRECHTS2003329}.}
We denote such scenario as $\theta \not\!\perp\!\!\!\perp s$.
In particular, for $\rho=-1$, it means that $\theta$ ranks candidates by descending scores similar to giving a ranked list of candidates to the screener.
%
With these two variants, we can consider a $\theta$ chosen by or provided to the screener, respectively.
% %
% \begin{itemize}
%     \item  $\theta$ is randomly generated independently from the scores. This setting models the case where the screener prefers the alphabetical order, or performs a random shuffle of candidates. We denote such a scenario as $\theta \ci s$.
% %
%     \item $\theta$ is randomly generated with a given Spearman's rank correlation $\rho$ with the scores.\footnote{Formally, $\rho$ is the Spearman's rank correlation of the pairs $(\theta(c_i), s(\mathbf{X}_{c_i})$ for $i=1, \ldots, n$. It assesses how well the initial order $\theta$ monotonically relates to the scores of candidates. To generate correlated initial ordering and scores, we rely on copulas -- see e.g., \cite[Section 3.4]{EMBRECHTS2003329}.} 
%     In particular, for $\rho=-1$, it means that $\theta$ ranks candidates by descending scores. %Correlation close to $-1$ models scenarios where the initial order is the result of some pre-evaluation of the candidates, including automatic ranking by ML models or manual ordering based on job-required skills. 
% \end{itemize}
% %

Regarding $W$, we initially consider 
% one variant for the protected attribute where 
the sample of candidates drawn from $Ber(\mathit{pr})$ such that $\mathit{pr}=0.2$ is the fraction of protected group candidates in the population.
The sample is independently drawn from both the scores and the ISO, according to the assumptions \textit{A1} and \textit{A2} from Section~\ref{sec:PositionBias}.
Here, we are determining the diversity of $\candidatesset$.
Later on, we increase $\mathit{pr}$ to study a more diverse $\candidatesset$ and its effect on the screener reaching the representational quota $q$.
% By setting a low $\mathit{pr}$, we picture a $\candidatesset$ where the protected individuals are under-presented, forcing the screener to search it longer to meet the representational quota $q$.
%For the former, it means that scores are not unfairly assigned by the screener. 
%For the the latter, it means that the initial order provided by the screener or by a ML ranking model is fair. Clearly, these two cases are also interesting to experiment with, but this is out of the scope of the paper.

Beyond the triplet $\big( \theta({c_i}), s(\mathbf{X}_{c_i}), W_{c_i} \big)$, 
% Finally, 
for the fatigued scores of $h_h$, we fix $\lambda=1$, hence $\Phi(t) = t$, and define:
% %
% \begin{itemize}
    % \item 
    $\epsilon_1 \sim \mathcal{N}(0, \, (0.005 \cdot (t-1))^2)$, hence with constant expectation and with standard deviation of $0.005 \cdot (t-1)$; and
    %
    % \item 
    $\epsilon_2 \sim \mathcal{N}(-0.005 \cdot (t-1), \, (0.001 \cdot (t-1))^2)$, hence with a decreasing expectation and with a smaller standard deviation than $\epsilon_1$.
% \end{itemize}
% %
Recall Section~\ref{sec:HumanScreener.BiasedScores} for details.

\subsubsection{Evaluation metrics.}
% For concreteness, 
We consider the solution $S^k_{\besttext}$ of the best-$k$ problem (\ref{eq:fair_objective_all_screener}) for $h_a$ (Algorithm~\ref{algo:Examination}) as the baseline solution. 
We compare it with the analysis of the solutions for the good-$k$ problem (\ref{eq:fair_objective_U_psi}) under $h_a$ (Algorithm~\ref{algo:Cascade}) and of the solutions for the best-$k$ and good-$k$ problems under $h_h$ (respectively, Algorithms \ref{algo:HumanExamination} and \ref{algo:HumanCascade}).
We introduce two comparison metrics that capture how close is the compared solution to the baseline solution.
   
The \textbf{\textit{ratio to baseline (RtB)}} is defined as the ratio of $U^k_{\addtext}$ between the compared solution and the baseline solution. 
For the solution $S_{\goodtext}^k$ under $h_a$, e.g., it is $U^k_{\addtext}(S_{\goodtext}^k) / U^k_{\addtext}(S^k_{\besttext})$. 
The closer the ratio is to $1$, the better the compared solution approximates the best-$k$ solution under $h_a$ in terms of $U^k_{\addtext}$ utility.
Here, when calculating the utility of $h_h$, we use the truthful scores, not the fatigued scores, to be able to compare w.r.t. the baseline.
%
The \textbf{\textit{Jaccard similarity (JdS)}} is defined as the proportion of candidates in both the compared and baseline solutions over those in at least one of the two. 
For the solution $S_{\goodtext}^k$ under $h_a$, e.g., it is $|S_{\goodtext}^k \cap S^k_{\besttext}|\, / \, |S_{\goodtext}^k \cup S^k_{\besttext}|$. 
Such a metric quantifies the share of candidates between the two solutions.
Essentially, the RtB metric captures whether the compared solution achieves the same utility as the baseline solution as measured by $U^k_{\addtext}$, while the JdS metric captures the overlap in candidates between the compared solution and the baseline solution.

\subsubsection{Simulations.}
For each setting of the parameters ($n$, $k$, $q$, $\rho$, $\psi$ or others), we run 10,000 times the experiments by randomly generating $n$ triplets at each run. 
The runs for which a solution of the problem does not exist are discarded. This
mainly occurs in the good-$k$ problem when there are not enough $k$ candidates with scores greater or equal than $\psi$.
% \footnote{This mainly occurs in the best-$k$ problem when there are not enough candidates with scores greater or equal than $\psi$.}
The plots report the mean output based on the evaluation metrics over all the runs.

\subsection{Experiments without Fatigue}
\label{sec:Experiments.Metrics.outFatigue}

%
% Figure environment removed
%
% %
% % Figure environment removed
% %\vspace{-3ex}
% %

We start by exploring the settings without fatigue, meaning we consider $h_a$, which allows for clarifying the relation between the best-$k$ (Algorithm~\ref{algo:Examination}) and good-$k$ (Algorithm~\ref{algo:Cascade}) solutions.
Given these two problem formulations, here we are mainly interested if their solutions differ in practice due to the ISO, especially since the best-$k$ requires a full search of $\candidatesset$ while the good-$k$ allows for a partial search of $\candidatesset$.

% First, w
We study the impact of the score distributions on the metrics at the variation of $\psi$.
We consider the case of $n=120$, $k=6$, $q=0.5$, and $\theta \ci s$. 
Note that these parameters are shared by both best-$k$ and good-$k$ problems. 
Instead, $\psi$ is specific to the good-$k$, which is why we focus on it.
We find that, as $\psi$ increases and screening becomes more selective, the good-$k$ approximates the best-$k$ when there is a low probability of having good candidates in $\candidatesset$.
%
Figure~\ref{fig:1}, under the RtB (center) and the JdS (right) metrics, illustrates this point for the three score distribution scenarios.
In particular, the symmetric distribution (in red) allows the good-$k$ to better approximate the best-$k$ for medium-to-high $\psi$ values.
This result is expected. 
Having few good candidates forces $h_a$ to explore more of $\candidatesset$ under $\theta$, especially as $\psi$ increases and the $k$ first good-enough candidates essentially become the $k$ top candidates.

The opposite holds for the other two distributions (in green and blue), which are more resilient to $\psi$ as each represents a higher concentration of good candidates.
It follows that having many good candidates makes it difficult for $h_a$ to select the $k$ top candidates under a partial search.
As the RtB and JdS metrics show in Figure~\ref{fig:1} (center, right), $h_a$ still achieves significant utilities under the other two distributions but is unlikely to derive the same selected set of candidates under a partial search w.r.t. a full search of $\candidatesset$.
These results are also expected, but worth emphasizing. 
Having many good candidates means that $h_a$ can still partially search $\candidatesset$ despite having a highly selective $\psi$: i.e., $h_a$ finds $k$ good-enough candidates with high-enough scores as the $k$ top candidates but not the same ones.

Figure~\ref{fig:1} illustrates how the two problems materialize differently when implemented due to the ISO $\theta$.
Clearly, as noted back in Remark~\ref{remark:ISOandPS}, where the $k$ top candidates appear in $\theta$ can determine if they are selected or not by $h_a$ under a partial search.
The position bias in the ISO becomes more prevalent under many good candidates as, e.g., even the $\candidatesset$'s best candidate may never be selected by $h_a$ under a partial search if it lies at the bottom of $\theta$.

We also study what occurs when $\theta \not\!\perp\!\!\!\perp s$. 
We present these results in Appendix~\ref{Appendix:Experiments.Metrics.outFatigue} for  $n=120$, $k=6$, and $q=0.5$.
Here, we briefly discuss these results as they further illustrate the role of $\theta$.
Recall that $\rho$ represents the correlation between $\theta$ and the scores, with a negative $\rho$ implying a descending order.
We find that the good-$k$ solution approximates quite well the best-$k$ solution already for $\rho=-0.5$; for $\rho=-1$, the two solutions are the same.
These results are expected as $\theta$ essentially represents the best-$k$ solution or an approximation of it depending on $\rho$'s strength. 
Under $\rho=-1$, e.g., the $\theta$ searched by $h_a$ is already sorted by the candidate scores and, in turn, the $k$ first good-enough candidates are also the $k$ best candidates in the candidate pool. 
See Figure~\ref{fig:2} (center, right) for details.

In Appendix~\ref{Appendix:Experiments.Metrics.outFatigue} we also study the impact of the number $n$ candidates in $\candidatesset$ and the number of $k$ candidates to be selected.
We find that under $\theta \ci s$, the ratio $k/n$ is positively correlated with the ability of best-$k$ to approximate good-$k$. Intuitively and unsurprisingly, it means that the more candidates we can select from $\candidatesset$, the better the chance to include top ones under a partial search.
Clearly, the influence of $\theta$ diminishes as $k/n$ increases.
See Figure~\ref{fig:2} (left) for details.
Similarly, we study the role of changing the representational quota $q$. Here, results are expected given our setup and underlying assumptions (\textit{A1} and \textit{A2} from Section~\ref{sec:PositionBias}), finding that under $\theta \ci s$, $q$ does not affect the relative strengths of best-$k$ and good-$k$ solutions.
See Figure~\ref{fig:3} (all) for details.

%%%
%%% Moved to Appendix
% Second, we consider the impact of the number $n$ candidates in $\candidatesset$ and the number of $k$ candidates to be selected. 
% We focus only on the symmetric distribution and the ratio to baseline, but the results are similar for the other two distributions and the Jaccard similarity. 
% Figure~\ref{fig:2} (left) compares the case $n=120, k=6$ considered earlier to two other scenarios. 
% The first scenario increases $k=20$ but leaves the ratio of selected $k/n = 0.05$ the same by also increasing $n=400$. 
% The second scenario, instead, leaves $k=6$ the same, but it increases $k/n = 0.2$ by decreasing $n=30$. 
% The plot shows that changes in the ratio $k/n$ affect the metric, in particular a larger ratio ($n=30$, $k=6$) leads good-$k$ to better approximate best-$k$ for a same $\psi$. The more candidates we can select from the pool, the better the chance to include top ones. 
% \textit{In summary, under $\theta \ci s$, the ratio $k/n$ is positively correlated with the ability of best-$k$ to approximate good-$k$}.

% Third, we now consider the impact of the correlation $\rho$ between the initial order $\theta$ and the scores. 
% Recall that $\rho = -1$ means that the candidates are ordered by descending scores. Under such a condition, the good-$k$ and best-$k$ procedures return the same solution. 
% This result is apparent in Figure~\ref{fig:2} (center, right) where we report the ratio to baseline for the symmetric (left) and the increasing (right) score distributions. 
% The plots show that even a moderate correlation of $\rho = -0.5$ leads the good-$k$ solution to approximate the best-$k$ one quite well. 
% For the increasing distribution (right plot), the ratio to baseline is around 95\%. \textit{In summary, initial orders that negatively correlate to the score greatly reduce the difference in utility between the good-$k$ and best-$k$ solutions}.

% %
% % Figure environment removed
% %\vspace{-3ex}
% %

% Let us now consider the quota parameter $q$, thus far set to $q=0.5$ over a population with a fraction of protected candidates set to $\mathit{pr}=0.2$. 
% Since we assumed that $W$ is independent from both scores and the ISO, the fraction of protected group in the solutions of best-$k$ and good-$k$ is, on average, $\mathit{min}\{q, \mathit{pr}\}$. 
% Figure~\ref{fig:3} (left) shows this result in the solution for good-$k$. 
% A less trivial question is whether $q$ is also not affecting the evaluation metrics: e.g., whether the quota $q$ changes the ratio to baseline? 
% Figure~\ref{fig:3} (center, right) show that this is not the case in two experimental settings. 
% Again, this result is theoretically implied by the independence of $W$ with scores and initial order. 
% In summary, under $\theta \ci s$, the quota parameter in the best-$k$ and good-$k$ problem does not affect the relative strengths of their solutions.
%%%
%%%

\subsection{Experiments with Fatigue}
\label{sec:Experiments.Metrics.withFatigue}

% We now consider the settings with fatigue, meaning we consider $h_h$.
We now focus on $h_h$.
First, we consider whether fatigue impacts utility w.r.t.~the baseline solution, namely the solution of best-$k$ without fatigue (Algorithm~\ref{algo:Examination}). 
We compare to such a baseline both the best-$k$ with fatigue (Algorithm.~\ref{algo:HumanExamination}) and good-$k$ with fatigue (Algorithm.~\ref{algo:HumanCascade}).
%See Appendix~\ref{Appendix.HumanAlgorithms} for implementation details on both of these algorithms.
%In the latter case, we readily extend the metric of ratio to baseline as the ratio of the utility for the solution of best-$k$ with fatigue over the one of best-$k$ without fatigue. 

%
% Figure environment removed
%
%
% Figure environment removed
% \vspace{-3ex}
%

Figure~\ref{fig:4} (left) shows the RtB metric for the three score distributions for the good-$k$ 
% (i.e., $h_h$ can perform a partial search) 
solution with fatigue based on $\epsilon_1$ for the fatigued scores. 
Based on Figure~\ref{fig:1} (center), for the asymmetric (in blue) and increasing (in green) distributions, 
which are the scenarios with many good candidates in $\candidatesset$,
there is not much difference w.r.t.~the case without fatigue.
For the symmetric distribution (in red), instead, there is a considerable decrease for high $\psi$ values. 
This can be attributed to the low number of top scores, for which the perturbation due the $\epsilon_1$'s has a large effect. 
For the other two distributions, instead, there are sufficiently many top scores, for which perturbation does not dramatically change the score distribution for top scores.
Intuitively, since the RtB metric captures achieving the utility of the baseline model, under a partial search $h_h$ is able to reach high utility solutions when $\candidatesset$ has many good candidates because $h_h$ can avoid evaluating all candidates.
This is unlikely to be the case when few good candidates are in $\candidatesset$. 
As $\psi$ increases and $h_h$ becomes more selective, it also becomes more tired under $\theta$ as it needs to evaluate more and more candidates to achieve $k$.

Figure~\ref{fig:4} (center) is analogous to (left), but considers the fatigued scores based on $\epsilon_2$. 
The effect for the symmetric distribution (in red) is not present in such a case, due to the lower standard deviation of $\epsilon_2$. 
The bias of $\epsilon_2$ does not impact too much, apart from high values of $\psi$ where it causes the problem not to have a solution as fatigued scores are smaller than scores of an already low number of top candidates. 
In summary, under $\theta \ci s$, variance appears more relevant than bias in the case of low probability of top scores.
This result illustrates the importance of how we define fatigue. It can also inform how the a human screener should behave in practice to diminish the role of position bias within $\theta$. Given these results, e.g., we would be interested in exploring under what settings would the human screener experience $\epsilon_1$ over $\epsilon_2$.

Figure~\ref{fig:4} (right) shows the RtB for the best-$k$ solution with fatigue at the variation of the quota $q$. There is a considerable and constant loss in utility under fatigue, which is more consistent for the symmetric distribution (in red). 
Interestingly enough, the RtB is lower than in the case of the good-$k$ with fatigue (see left) for $\psi \geq 0.5$. 
This result means that, for the symmetric distribution, the good-$k$ solution with fatigue has better utility than the best-$k$ solution with fatigue. 
This noteworthy result can inform the screening practice.

Finally, we consider the impact of the correlation $\rho$ on the ISO $\theta$ for the good-$k$ solution with fatigue.  
Figure~\ref{fig:5} (left) considers the symmetric distribution (in red). 
For the lower half of $\psi$'s, lines are similar to the analogous case without fatigue shown in Figure~\ref{fig:2} (center). 
For the higher half of $\psi$'s, instead, there is a decrease in the metric. 
This is, again, due to the low probability of top scores, for which the effects of the variability of $\epsilon_1$ is not counter-balanced by correlation of the scores and $\theta$.
Such an effect does not appear for $\epsilon_2$ nor for $\epsilon_1$ with the increasing distribution. 
In fact, the plots in Figure~\ref{fig:5} (center) and (right) closely resemble those in Figure~\ref{fig:2} (center) and (right) respectively. 
This is an interesting result on its own. 
It means that, in the presence of correlation, fatigue does not have an impact on utility of the good-$k$ solution if there are sufficiently many top scores or a sufficiently small variability of the fatigue.

Moreover, we believe this last result points at the importance in practice of providing a $\theta$ to the human screener that has some information about candidate quality. 
Intuitively, under a partial search procedure and the threat of position bias materializing through $\theta$, we would like to decrease $h_h$'s fatigue by minimizing its need to search more of $\candidatesset$.
A way to do is to already provide to $h_h$ a sorted $\theta$. 
Note that this point excludes the difficulty behind deriving such a sorted $\theta$ in the first place, which is the main goal of the fair set selection literature. 
This point, however, hints at an interesting line of future work focused on human screeners and their interactions with an algorithmic aid.

%
% EOS
%

% \subsection{Evaluation Metrics}
% \label{sec:Experiments.Metrics}

% The solution $S^k_{\besttext}$ of the fair best-$k$ problem (\ref{eq:fair_objective_all_screener}) for the algorithmic screener represents a baseline to compare with in the analysis of the solutions of the fair good-$k$ problem (\ref{eq:fair_objective_U_psi}) for the algorithmic screener, and of the solution of both fair best-$k$ and fair good-$k$ for the human-like screener. 
% We introduce two comparison metrics.
% %
% %\begin{itemize}
%     %\item 
    
%     The \textit{ratio to baseline} is defined as the ratio of $U^k_{\addtext}$-utility between the compared solution and the baseline solution. E.g.,~for the solution $S_{\goodtext}^k$ of the algorithmic screener, it is $U^k_{\addtext}(S_{\goodtext}^k) / U^k_{\addtext}(S^k_{\besttext})$. 
%     The closer the ratio is to $1$, the better the compared solution approximates the best-$k$ solution of the algorithmic screener in terms of $U^k_{\addtext}$ utility. 
    
%     %\item 
    
%     The \textit{Jaccard similarity} is defined as the proportion of candidates in both the compared and baseline solutions over those in at least one of the two. E.g., for the solution $S_{\goodtext}^k$ of the algorithmic screener, it is $|S_{\goodtext}^k \cap S^k_{\besttext}|/|S_{\goodtext}^k \cup S^k_{\besttext}|$. 
%     Such a metric quantifies the share of candidates between the two solutions. 
    %The closer the metric is to $1$, the more equivalent is the set of selected candidates between the problems.
%\end{itemize}
%
% SR
%We will experiment with the values of these two metrics at the variation of $\psi$ and other parameter settings.
