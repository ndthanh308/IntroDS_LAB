%
\label{sec:HumanScreener}

We want to model a screener $h$ prone to error when evaluating the candidate pool $\mathcal{C}$ given the ISO $\theta$, allowing to study the consequences of an ISO 
% chosen by or provided to 
on a human screener solving for the best-$k$ \eqref{eq:fair_objective_all_screener} or good-$k$ \eqref{eq:fair_objective_U_psi} problems. 
% under, respectively, Algorithm~\ref{algo:Examination} and Algorithm~\ref{algo:Cascade}.

We distinguish two kinds of screeners $h$: the \textit{algorithmic} screener, denoted by $h_a \in \mathcal{H}_a$, if it can consistently evaluate $\mathcal{C}$; 
% The algorithmic screener is the implied screener in the fair ranking literature. 
the \textit{human-like} screener, denoted by $h_h \in \mathcal{H}_h$, if it is conditioned by a fatigue component that hinders the consistency of its evaluation as it explores $\mathcal{C}$.
% SR
%As we will show here, it is under this human-like screener that the distinction between best-$k$ and good-$k$ becomes relevant.
% \ja{Both kinds of screeners can search $\candidatesset$ under the best-$k$ or good-$k$ problem.}
% Time
% let us 
We introduce a \textit{time component} to study these screeners.
Let $t$ denote the discrete unit of time that represents how long $h$ takes to evaluate a candidate $c \in \mathcal{C}$. 
We assume that $t$ is constant (recall G5 Section~\ref{sec:Generali}), implying time itself cannot be optimized by $h$.
We track time along $\theta$, 
% to evaluate $\candidatesset$, 
meaning $h$ evaluates the first candidate that appears in $\theta$ at time $t=1$, and so on. 
Thus, time $t$ ranges from $0$ 
% (before start screening) 
to $n$ 
% (after screening all candidates) 
at maximum.
%SR
%Hence, time $t$ follows the index $i$ in $\theta$ that identifies a candidate $c \in \mathcal{C}$.

\subsection{Fatigue and Fatigued Scores}
\label{sec:HumanScreener.BiasedScores}

% Fatigue
Formally, we introduce a \textit{fatigue component} $\phi(t)$ specific to the human-like screener $h_h$ as a function of time $t$ and model the \textit{accumulated fatigue} $\Phi \colon \{0, \ldots, n\} \to \mathbb{R}$, with $\Phi(0) = 0$. 
The discrete derivative of $\Phi$, that is, $\phi(i) = \Phi(i) - \Phi(i-1)$, defined for $t \geq 1$, is the effort of $h_h$ to examine the $t$-th candidate. 
%SR
% $c=\theta^{-1}(i)$.
%
%%% Move to fairness section
% What $\Phi$ means in practice is that a \ja{human-like} screener $h_h$ will evaluate the identical candidates $c_1$ and $c_2$ differently at times $t_1$ and $t_2$, as long as $\Phi(t_1) \neq \Phi(t_2)$.
%%%
%
Clearly,
% We recognize that
% fatigue can accumulate with different functional forms, 
% meaning 
how we define $\Phi$ conditions the effect of fatigue on our analysis of $h_h$.
% Here, w
We make the simplest modeling choice to define $\phi$ by assuming that \textit{fatigue accumulates linearly over time}, or $\phi(t) = \lambda$ so that $\Phi(t) = \lambda \cdot t$.
This is one possible formulation based on our interpretation that $h_h$ becomes tired at a constant pace.
We leave the study of other $\Phi$ formulations for future work.

How does fatigue materialize for $h_h$ when searching $\candidatesset$ under $\theta$?
We model the effect of fatigue when $h_h$ evaluates $c$ through the \textit{fatigued score} $s_{h_h}(\mathbf{X}_c) + \epsilon$, where $\epsilon$ is a random variable depending on the fatigue $\Phi$ which quantifies the deviation from the \textit{truthful score} $s_h(\mathbf{X}_c)$.
We model $\epsilon$ according to \emph{two modeling choices}.
%
The first choice defines $\epsilon_1$ as a centered Gaussian variable, and the fatigue affects only its variance. 
Formally, at a given $t$, 
$\epsilon_1 \sim \mathcal{N}(0, \, v(\Phi(t-1)))$, where $v \colon \mathbb{R} \to \mathbb{R}$ defines the variance of $\epsilon_1$ as an increasing function of the accumulated fatigue.
%
The second choice assumes that a negative bias can affect each applicant's evaluation.
% In this case, 
The more fatigue, the more the screener $h_h$ tends to underscore the candidates.
Hence, we model $\epsilon_2$ as an uncentered Gaussian, whose mean is a decreasing function of the fatigue.
Formally, $\epsilon_2 \sim \mathcal{N}(\mu(\Phi(t-1), \, v(\Phi(t-1))$, where $\mu \colon \mathbb{R} \to \mathbb{R}$ is a decreasing function rather than a constant.

%
\begin{remark}
\label{remark:HumanAndIF}
    What $\Phi$ means in practice is that $h_h$ evaluates the identical candidates $c_1$ and $c_2$ differently under the ISO $\theta$ at times $t_1$ and $t_2$, as long as $\Phi(t_1) \neq \Phi(t_2)$ and regardless whether $h_h$ is solving for the best-$k$ or good-$k$ problem.
\end{remark}
%

We assume that $h_h$ is unaware of its fatigue as it goes over $\candidatesset$, representing an unconscious decision-making bias due, e.g., to mental heuristics \cite{tversky_judgment_1974}.
%
Such a setting allows capturing what occurs in real life,
% especially with repetitive tasks \cite{Kahneman2021Noise}, 
where $h_a$ is consistent in its evaluation of $\candidatesset$ whereas $h_h$ loses its consistency over time \cite{DBLP:conf/chi/EchterhoffYM22, PeiEAAMO2023}.
% Hence, we are interested not by how $h_h$ minimizes its fatigue, which would require adding the fatigue to the problem definition, but by what is the impact of $h_h$'s fatigue on its screening process? The answer depends on the set selection problem formulation as discussed in Section~\ref{sec:ProblemFormulation} and the initial \ja{screening} order $\theta$.
%
% Regarding the search procedures, 
Algorithms~\ref{algo:Examination} and \ref{algo:Cascade} represent $h_a$ as there is no notion of fatigue nor biased scores.
%SR
%Hence, we refer to such setting as the \textit{baseline model}.
To represent 
the human-like screener, we need to track the accumulated fatigue $\Phi$ over the time and draw $\epsilon$ accordingly to compute the fatigued scores of $h_h$. 
%$s_{h_h}(\mathbf{X}_c) + \epsilon$. 
The main change is, thus, to line 2 in Algorithm~\ref{algo:Examination} and line 7 in Algorithm~\ref{algo:Cascade}, where the score computed for candidate $c$ is biased by $\epsilon$ sampled at that time. 
We present the human-like versions of these search procedures in Appendix~\ref{Appendix.HumanAlgorithms} as Algorithms~\ref{algo:HumanExamination} and \ref{algo:HumanCascade}.

\subsection{Position Bias Implications}
\label{sec:PositionBias}


Importantly, no two candidates can occupy the same position in the ISO. With this in mind, we now analyse the fairness and optimality implications of the position bias implicit to the ISO.
For concreteness, let us consider the next two assumptions.
\textit{\textbf{A1}: We assume that $\theta$ is independent of the protected attribute $W$}, meaning the way in which candidates are sorted in $\theta$ contains no information about membership to either category in $W$.
\textit{\textbf{A2}: We assume that the individual scoring functions $s$ for either screener is able to evaluate any candidate $c$ fairly and truthfully}, meaning $s(\mathbf{X}_c)$ captures no information about $W_c$  and all information about the suitability of $c$. 
Under \textit{A1} and \textit{A2} we can control for other biases, such as measurement error in $s$, and focus on the bias coming from the ISO.

Let us start with the fairness implications in the best-$k$ and good-$k$ problems. 
Here, it is important to distinguish between the group-level fairness constraining $h$ given $\theta$ and the potential individual fairness violations under $\theta$, in terms of $h$ failing to evaluate similar candidates similarly \cite{DBLP:conf/innovations/DworkHPRZ12}.
Regarding group-level fairness, both $h_a$ and $h_h$ are fair in solving for $S_{\besttext}^k$ and $S_{\goodtext}^k$. 
Indeed, both fair $h_a$ and $h_h$ must meet the condition $f\big(S^k\big) \geq q$.
This point is clear for $h_a$ (i.e., Algorithms \ref{algo:Examination} and \ref{algo:Cascade}) as there is no fatigue involved.
The same holds for $h_h$ (i.e., Algorithms \ref{algo:HumanExamination} and \ref{algo:HumanCascade}) 
because the error on the score does not affect the evaluation of the representational quota $q$.
Formally, the expected error $\mathbb{E}[\epsilon \mid W_c = 1, \theta] = \mathbb{E}[\epsilon \mid W_c = 0, \theta]$, regardless of $\epsilon$ being $\epsilon_1$ or $\epsilon_2$.
% Intuitively, $h_h$ here simply needs to satisfy $q$. 
The fatigue and, thus, the fatigued scores are, on average, shared across protected and non-protected candidates.


The distinction between $h_a$ and $h_h$ becomes important under individual-level fairness because $h_a$ ensures it, while $h_h$ violates individual fairness for a specific candidate $c$. 
% This point follows from Remark~\ref{remark:HumanAndIF} as
Indeed, 
% even if the protected attribute does not directly affect the score evaluation with error, 
the candidate's position in $\theta$ influences the amount of error made by $h_h$ when evaluating that candidate. 
Therefore, similar candidates will not be evaluated similarly due to the unequal accumulation of fatigue experienced by $h_h$, because of their different positions in $\theta$.
% For instance, given two similar candidate $c_i = \theta^{-1}(i), c_j = \theta^{-1}(j)$, with $i < j$, their evaluation could be significantly different in the amount of error depending on the accumulated fatigue $\Phi(i) < \Phi(j)$. 
% In the case of $\epsilon_1$, $i$ has the advantage of being evaluated by a rested screener.
%Moreover, 
% In the case of
For $\epsilon_2$, e.g., even if $\mathbf{X}_{c_j} = \mathbf{X}_{c_i}$, but $j>i$, the score of $j$ is less, on average, than the one of $i$, so that $i$ has an unfair premium from $\theta$.
%Therefore the position bias of the human-like screener appears in both modeling of the error considered.

Let us now consider the optimality implications of the best-$k$ and good-$k$ problems. 
Here, we need to keep in mind that each problem, due to its utility model, has different optimal solutions, with best-$k$ requiring the top-$k$ candidates in $\candidatesset$ and good-$k$ the first good-enough $k$ candidates in $\candidatesset$.
It follows that $h_a$ reaches the optimal solution as the absence of fatigue enable $h_a$ to consistently judge suitable candidates, thus to obtain the optimal selected set. 
The opposite holds for $h_h$ due to the inconsistent scoring of candidates ascribed by the accumulated fatigue.

We can summarize the impact of the position bias in the ISO, if the $q$ defines the fairness constraint.
We find that \textit{$h_a$ reaches the optimal fair solution of both fair best-$k$ and good-$k$ problems}.
Moreover, \textit{$h_a$ guarantees individual fairness in both settings}.
Instead, we find that \textit{$h_h$ reaches the fair but, potentially, sub-optimal solutions for both fair best-$k$ and good-$k$ problems}.
Moreover, \textit{$h_h$, potentially, does not guarantee individual fairness in both settings}.
We purposely include ``potentially'' for $h_h$ because this analysis depends on how we model fatigue and whether it impacts the selected set. 

\subsection{The Initial Screening Order Problem}
\label{sec:TheISO}

The ISO problem amounts to a combination of factors that are likely to occur in real-world screening scenarios.
As our analysis shows, the ISO has no effect under an algorithmic screener $h_a$. 
This is a trivial result, especially given the ideal assumptions \textit{A1} and \textit{A2}. 
Under these conditions, automating the screening process is clearly the preferred option.
% Further, arguably, if automation is preferred then it seems reasonable to train $h_a$ to solve for the best-$k$ problem.
In practice, however, even under \textit{A1} and \textit{A2}, it is likely for the screening process to require the decision of a human-like algorithmic screener $h_h$, as it was the case for G (Section~\ref{sec:Generali}) and the other similar case studies (Section~\ref{sec:RelatedWork}).

Significantly under the $h_h$, the position of a candidate within the chosen or provided ISO $\theta$ matters. 
It has impact for determining whether a candidate, depending on the search strategy, is evaluated at all (Remark~\ref{remark:ISOandPS}) and, if so, is evaluated fairly (Remark~\ref{remark:HumanAndIF}).
Note that these results only worsen when relaxing the assumptions \textit{A1} and \textit{A2}.
The best position within $\theta$ to be as a candidate is the first spot, and less from there.
Being first implies meeting a non-fatigued $h_h$. 
To an extent, this result of position bias is also trivial. 
What is difficult to answer is \emph{how exactly the fatigue of the $h_h$ evolves over $\theta$}. 
The answer to this question passes through 
%This question can be answered with
our two problem formulations and their respective algorithmic implementations, as illustrated with the simulations in the next section.

%
% EOS
%

% \subsection{The Fairness of the Algorithmic and Human-like Screeners}
% \label{sec:HumanScreener.Analysis}

% \ja{
% Regarding group-level fairness, assuming \textit{A1} and \textit{A2}, both $h_a$ and $h_h$ are fair in solving for $S_{\besttext}^k$ and $S_{\goodtext}^k$.
% Regarding group-level fairness, as emphasized in Remark~\ref{remark:ISOandPS}, regardless of whether $h$ is of a $h_a$ or $h_h$ kind, the ISO $\theta$ exerts an influence on the $S^k$ under the partial search prescribed by the good-$k$ problem. Such influence, though, is on the composition of the selected set in terms if suitability of the candidates and not in terms of their representativeness. 
% This is because both fair $h_a$ and $h_h$ must meet the condition $f\big(S^k\big) \geq q$.
% This point is clear for $h_a$ (i.e., Algorithms \ref{algo:Examination} and \ref{algo:Cascade}) as there is no fatigue involved and assuming \textit{A1} and \textit{A2} controls for other common sources of bias.
% This point holds also for $h_h$ (i.e., Algorithms \ref{algo:HumanExamination} and \ref{algo:HumanCascade})
% }
% because, specifically by \textit{A1},
% We now focus on $h_h$ and the solutions of the best-$k$ and good-$k$ problems computed by Algorithms~\ref{algo:HumanExamination} and \ref{algo:HumanCascade}.
% Note that, under \textit{A1}, 
% the error on the score does not affect the evaluation of the representational quota $q$.
% \textit{The $h_h$ guarantees, in both best-$k$ or good-$k$ problems, a fair group level solution despite the fatigue.}
% Formally, the expected error $\mathbb{E}[\epsilon \mid W_c = 1, \theta] = \mathbb{E}[\epsilon \mid W_c = 0, \theta]$, whether $\epsilon$ is $\epsilon_1$ or $\epsilon_2$.
% Intuitively, $h_h$ here simply needs to satisfy $q$. 
% The fatigue and, thus, the fatigued scores are, on average, shared across protected and non-protected candidates.
% Further, here, we could potentially relax assumptions \textit{A1} and \textit{A2} as long as we only care for meeting the representational quota $q$. 

% \ja{
% The distinction between screeners becomes important under individual-level fairness. Clearly, lacking of a fatigue component and assuming \textit{A1} and \textit{A2}, $h_a$ is individually fair under the best-$k$ and good-$k$ problems. 
% This is not the case for $h_h$, which violates individual fairness. Essentially,
% }
% what $\Phi$ means in practice is that $h_h$ will evaluate the identical candidates $c_1$ and $c_2$ differently at times $t_1$ and $t_2$, as long as $\Phi(t_1) \neq \Phi(t_2)$.
% Indeed, even if the protected attribute does not directly affect the score evaluation with error, the position of the candidate in $\theta$ influences the amount of error made by $h_h$ when evaluating that candidate. 
% Therefore, similar candidates, because of their position in $\theta$ (i.e.,~no two candidate can occupy the same position), will not be evaluated similarly due to the accumulation of fatigue experienced by $h_h$.
% For instance,
% given two similar candidate $c_i = \theta^{-1}(i), c_j = \theta^{-1}(j)$, with $i < j$, their evaluation could be significantly different in the amount of error depending on the accumulated fatigue $\Phi(i) < \Phi(j)$. 
%In the case of $\epsilon_1$, $i$ has the advantage of being evaluated by a rested screener.
%Moreover, 
% E.g.,~in the case of $\epsilon_2$, even if $\mathbf{X}_{c_j} = \mathbf{X}_{c_i}$, the score of $j$ is less, on average, than the one of $i$, so that $i$ has an unfair premium from $\theta$.
%Therefore the position bias of the human-like screener appears in both modeling of the error considered.

% Position Bias and the Implication of the Initial Order
% We discuss the differences between an algorithmic $h_a$ or human-like $h_h$ screener from the fairness perspective.
% Under the assumptions below, we can focus on $\theta$ and how, through position bias, it leads to individual fairness violations.
% Let us consider two assumptions.
%
% \textit{A1}: We assume that the initial order $\theta$ is independent of the protected attribute $W$, meaning the way in which candidates are sorted in $\theta$ contains no information about membership to either category in $W$.
% \textit{A2}: We also assume that the individual scoring functions $s$, for either screener, is able to evaluate candidate $c$ fairly and truthfully, meaning $s(\mathbf{X}_c)$ captures no information about $W_c$ (i.e., scores and the protected attribute are independent) and captures all information about the suitability of $c$ for the position. 
%
% %
% \begin{itemize}
%     \item \textit{Assumption A1: We assume that the initial order $\theta$ is independent of the protected attribute $W$}, meaning the way in which candidates are sorted in $\theta$ contains no information about membership to either category in $W$.
%     %
%     \item \textit{Assumption A2: We also assume that the individual scoring functions $s$, for either screener, is able to evaluate candidate $c$ fairly and truthfully}, meaning $s(\mathbf{X}_c)$ captures no information about $W_c$ (i.e., scores and the protected attribute are independent) and captures all information about the suitability of $c$ for the position. 
% \end{itemize}
% %

% We first focus on $h_a$ and the solutions of the best-$k$ and good-$k$ problems computed by Algorithms \ref{algo:Examination} and \ref{algo:Cascade}.
% SR
%We observe that imposing the condition of representational quota $f(S^k) \geq q$ changes the solution of the unconstrained problem, so that the achieved fair utility value is upper bounded by the maximum utility.
%
% \textit{The $h_a$ reaches the optimal fair solution of both fair best-$k$ or good-$k$ problems.}
% %
% \textit{Moreover, $h_a$ guarantees individual fairness in both scenarios.}
%
%
% Under assumptions \textit{A1} and \textit{A2}, this result for $h_a$ is intuitive for both Algorithms \ref{algo:Examination} and \ref{algo:Cascade}. 
% Regarding group fairness, $h_a$ fulfills it by satisfying the representational quota $q$.
% Regarding individual fairness, it is fulfilled as $h_a$ is able to consistently evaluate each candidate, treating similar candidates similarly. 
% Therefore, $\theta$'s role here on fairness is trivial. 
% These results hold under \textit{A2},
% % \footnote{For instance, if $\mathbf{X}$ is a proxy for $W$, then $h_a$ can be consistently biased under $s$.} 
% which we know it is likely not to be true in reality \cite{Zehlike2023_FairRanking_P1, Zehlike2023_FairRanking_P2} but we assume it here as our focus is on the human-like screener $h_h$ and its interaction with $\theta$.

% However, \textit{individual fairness is not guaranteed under $h_h$.}
% This result is intuitive.
% Indeed, even if the protected attribute does not directly affect the score evaluation with error, the position of the candidate in $\theta$ influences the amount of error made by $h_h$ when evaluating that candidate. 
% Therefore, similar candidates, because of their position in $\theta$ (i.e.,~no two candidate can occupy the same position), will not be evaluated similarly due to the accumulation of fatigue experienced by $h_h$.
% For instance,
% given two similar candidate $c_i = \theta^{-1}(i), c_j = \theta^{-1}(j)$, with $i < j$, their evaluation could be significantly different in the amount of error depending on the accumulated fatigue $\Phi(i) < \Phi(j)$. 
% %In the case of $\epsilon_1$, $i$ has the advantage of being evaluated by a rested screener.
% %Moreover, 
% E.g.,~in the case of $\epsilon_2$, even if $\mathbf{X}_{c_j} = \mathbf{X}_{c_i}$, the score of $j$ is less, on average, than the one of $i$, so that $i$ has an unfair premium from $\theta$.
% %Therefore the position bias of the human-like screener appears in both modeling of the error considered.

% \subsection{Position Bias and the Initial Screening Order}
% \label{sec:ProblemFormulation.PositionBias}

% \textit{The $h_a$ reaches the optimal fair solution of both fair best-$k$ or good-$k$ problems.}
% %
% \textit{Moreover, $h_a$ guarantees individual fairness in both scenarios.}

% The implications of different modelings of the error $\epsilon$, or of other fairness requirements, need further discussion that we do not cover in this work.
% Based on the previous section, though, our analysis already shows the overall risk of the position bias inherent to the initial order $\theta$ under a human-like screener.


% In the case of the algorithmic screener $h_a$, we find trivial results, especially under the assumptions \textit{A1} and \textit{A2}.
% The results for $h_a$ are trivial because of the nature of algorithms and their inherent consistency when it comes to decision making, which is, after all, a strong selling point for proponents of ADM systems (e.g., \cite{Miller2018Bias, Kahneman2016Noise}). 
% Similarly, this is why position bias is treated as a technical bias in the fair ranking literature (recall, Section~\ref{sec:RelatedWork}) and focus is given on relaxing assumption \textit{A1} to learn a fair scoring function.

% As our analysis shows, by considering the human-like screener $h_h$, however, that position bias and the role of $\theta$ acquire a significant meaning for individual fairness. 
% We emphasize that the results in the previous subsection hold under assumptions \textit{A1} and \textit{A2}. If we relaxed either assumption, then the individual fairness violations should be even more concerning. 
% Moving forward, we note that position bias as manifested in $\theta$ is only a function of where the candidates lie in $\theta$ when $h_h$ starts the candidate screening. 
% We emphasize that
% \textit{this fact is true whether $\theta$ is chosen by $h_h$ or provided to by, e.g., a (fair) ranking algorithm.}
