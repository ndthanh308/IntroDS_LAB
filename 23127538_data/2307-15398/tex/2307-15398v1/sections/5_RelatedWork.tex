%
\label{sec:RelatedWork}

\paragraph{Estimating a scoring function}
The scoring function $f$ in Section~\ref{sec:ProblemFormulation} con be approximated using past screening data to obtain $\hat{f}$. In terms of the initial screening order, $\hat{f}$ represents the algorithmic screener. Two main approaches exist for obtaining $\hat{f}$: score-based ranking (SBR) \cite{Zehlike2023_FairRanking_P1} and learning-to-rank (LTR) \cite{Zehlike2023_FairRanking_P2}, where in the first a function is given to calculate the scores while in the second the function is learnt. The initial screening problem, we argue, requires elements of both approaches. 
What is clear to us from the interaction with the company G (Section~\ref{sec:Generali}), is that the human-like screener acts within the SBR framework (as underlined by $U^{*}$ in Section~\ref{sec:ProblemFormulation}). If it were (humanly) possible, such screener would go over all candidates in the candidate pool $\mathcal{C}$. The issue for obtaining $\hat{f}$ is not only that $f$ remains a mental process, but so do the scores $Y$: the \textit{selected}-$k$ is recorded, but never their individual scores.
In that regard, the LTR framework is appealing. Based on past candidate screenings and using LTR methods like RankNet~\cite{Burges2010ranknet} and ListNet~\cite{Cao2007learning}, we could learn $\hat{f}$ based on the characteristics of the candidates within and outside past \textit{selected}-$k$ sets. These methods are often used for providing suggestions to the user. The issue for obtaining $\hat{f}$, though, now becomes the lack of order within \textit{selected}-$k$. There is a difference in training a ranking algorithm that recommends the best item for the user over one that recommends $k$ items all interchangeable for the user (as underlined by $\underline{U}^{k}$ in Section~\ref{sec:ProblemFormulation}). 

We argue that the initial screening order problem, opens new formulations for both SBR and LTR (the fatigued scored \eqref{eq:FatiguedScores} in Section~\ref{sec:ISO:FatiguedScreener}, e.g., hints at this line of work). Estimating $\hat{f}$ to aid HR officers is an ultimate goal, but we need $\hat{f}$ to capture the role of fatigue, the influence of the initial order (and the fact that HR officers can alter it), and the objective of not always selecting the best possible candidates. With the initial screening order problem, we have highlighted a complex problem that, to the best of our knowledge, has not been tackled explicitly by the ranking literature.

\paragraph{Modeling position bias}
Position bias states that there is a premium for being, usually, at the top of a platform like a list or a website. Human are predisposed to favor those items, leading to biased decisions \cite{DBLP:journals/cacm/Baeza-Yates18}. Position bias falls under technical bias in the fair ranking literature \cite{DBLP:journals/vldb/PitouraSK22,Zehlike2023_FairRanking_P1,Zehlike2023_FairRanking_P2}. 
This behaviour was formalized (e.g., \cite{CraswellZTR08_ExperimentsClickPositionBias}) and tested (e.g., \cite{DBLP:journals/jcmc/PanHJLGG07, DBLP:conf/clef/GrotovCMSXR15, DBLP:conf/www/RichardsonDR07}) early on by the click model literature. The name choice of the two search procedures in Section~\ref{sec:ISO:SearchAlgorithms} is a reference to this line of work. Position bias, probably given its link to click models and current recommender system problems, is addressed within the LTR framework \cite{Zehlike2023_FairRanking_P2}. 

Fairness-wise, two lines of work---probability-based fairness (PBF) and exposure-based fairness (EBF)---tackle position bias. In PBF \cite{DBLP:conf/ssdbm/YangS17, DBLP:conf/cikm/ZehlikeB0HMB17}, we constraint the ranking algorithm to (re-)arrange the candidates under some fair distribution like a tossing a fair coin. In EBF \cite{DBLP:journals/cacm/Baeza-Yates18, DBLP:journals/sigir/JoachimsGPHG17}, we instead model attention, which decreases geometrically or logarithmically as the user faces a longer list of items, and constraint the ranking algorithm to (re-)arrange the candidates such that candidates receive similar exposure (e.g., \cite{DBLP:conf/kdd/SinghJ18}). The initial screening order, with its focus on fatigue, relates to the EBF works. We interpret fatigue as inversely proportional to attention: the more tired the screener, the less attention it gives to a candidate. Such link allows to use EBF to tackle our problem.

The initial screening problem, however, challenges current works on position bias. It is important to separate the bias itself from its effect on deriving a fair ranking. Position bias seems to be inherent to humans. EBF handles this by pre-emptily accounting for it and returning rankings that, e.g., are fair in exposure. In EBF problems we are returning rankings, meaning the user is expecting to go over a meaningful list of ordered items (like a Google search result or a recommendation of candidates to hire). This is not, however, the case for the initial screening order problem where the HR officer plans to make its own assessment of the list of candidates and, thus, attaches no meaning to it. That is way we make the distinction between the candidate pool $\mathcal{C}$ and the initial order $\theta$. EBF methods could be useful for providing a $\theta$ that accounts for fatigue, but it should do so without providing it in the form of recommendations. It is unclear from our experience with company G (Section~\ref{sec:Generali}) whether obtaining $\hat{f}$ is possible under such complex process; hence, it is unclear to us how the solutions proposed under EBF or even PBF can address the position bias when the goal is to return a fair initial order (Def.~\ref{def:FairIO}) instead of a fair ranking of candidates. 

We highlight recent work by \citet{DBLP:conf/chi/EchterhoffYM22} that focuses in capturing and balancing anchoring bias \cite{Kahneman2011Thinking} in sequential decision-making. Such bias occurs, e.g., when a given candidate is evaluated following, say, two very bad versus two very good previous candidates: that same candidate has a higher chance of a positive decision in the former scenario. This is because the screener anchors its expectations on a lower reference point despite each candidate being independent from each other. These researchers worked closely with a university to understand and model their admission process and proposed an algorithmic procedure to balance the anchoring bias. In that sense, our work also prioritizes the role of the user in the problem formulation and, like \cite{DBLP:conf/chi/EchterhoffYM22}, links future work between LTR, EBF, and theories on human decision making (see \cite[Ch10]{DBLP:books/daglib/0033056} and, e.g., \cite{DBLP:conf/chi/CarabanKGC19, DBLP:journals/isr/AdomaviciusBCZ13}).

\paragraph{Understanding candidate screening.}
With the exception of \cite{DBLP:conf/chi/EchterhoffYM22} and \cite{SukumarMH18_PeacanPie}, no other paper offers detailed insights on the process of candidate screening. Both of these papers focused on college admissions, with \cite{DBLP:conf/chi/EchterhoffYM22} proposing an algorithmic procedure for anchoring bias while \cite{SukumarMH18_PeacanPie} the use of visual tools to support the decision maker. We join these paper in stressing the complexity of candidate screening and share their view in using technology to aid the human decision maker. To the best of our knowledge, we offer the first formalization of hiring in candidate screening. 

%
% ESO
%

% \paragraph{Wed. 08/03.} In going over once again Meike's fair ranking survey, I came across the \textit{click model} literature. These ones were hidden under \textit{technical bias} and refer to practices that encourage bias. In particular, it refers to \textit{position bias}. Humans, for example, tend to read from top-to-bottom and, thus, associate items positioned at the top of a list as more important. Unsurprisingly, the ranking technologies reflect this bias: Google searches will put on top the most relevant search to your query; football leagues will order the classification board in terms of the leader, and so it goes. 

% The click model literature is interesting as it precedes fairness and the overall ML boom but still identifies the issue of bias. Based on user experiments (mostly done using eye-tracking technology), this literature assumes the position bias and tries to explain it via clicking models to address it. From ``In Google We Trust'', e.g., researchers found that humans will click on the top ranked item despite being the one with less relevance to the search. Both ``A Comparative Study of Click Models for Web Search'' and ``An Experimental Comparison of Click Position-Bias Models'', to name a few, propose/study several click models to see which one explains human behaviour better. The consensus seems to be: one, there is such a thing a position bias, two, no click model seems to outperform the other even when using several performance measures.

% Now, it is important to distinguish the setting from click models from the one studied in Generali. In the former, the user assumes that the search engine (or overall ranker) provides a ranking that is meaningful: i.e., the positions reflect the relevance of the item. In the latter, this is not the case: the HR platform offered many, non-meaningful ways to order the list of candidates but the HR screener was well aware that the first candidate on the list was not necessarily the better suited or more relevant one. The issue, of course, is whether this position bias can have unconscious effects. In that sense, the notion of fatigue, $\omega(t)$, is interesting as we just say the screener gets tired. The position bias arises because nobody wants to be reading a list of candidates all day... It is conceptually appealing to frame it as such. It also aligns with the notion of the \textit{decaying attention curve} used in click models where it is known that attention is scarce and it decays as the user goes over the ranked items. Hence, the position bias seems equivalent across these framings of the problem and, to an extent, unavoidable.

% Formally, say for candidate list $\mathcal{C}$ with $|\mathcal{C}|=n$ candidates, let $\mathcal{R}$ represent the set of all possible rankings of $\mathcal{C}$. In other words, it denotes all possible permutations. Clearly, $\mathcal{R}$ includes (assuming no tied items/candidates) the optimal initial ordering $r_{io}^*$, meaning the \textit{meaningful ordering for the screener} such that $r_{io}^*[1]$ represents the best suited candidates for the job or, in general, the most relevant item. $\mathcal{R}$ includes a given $r_{io}$ \textit{initial order ranking} that is not meaningful to the screener (though, it is possible for $r_{io} = r_{io}^*$ despite the screener being unable to know that: i.e., it does not matter to the problem setting. It is also very unlikely, with probability: $1/n!$.

% Therefore, in terms of related work, we need to \textit{(i)} consider the potential unconsciousness/unconscious ways the position bias materializes despite knowing that $r_{io}$ is not meaningful and \textit{(ii)} whether the human search algorithms (or models of behavior) relate to existing models like the clicking ones?

% Regarding \textit{(ii)}, I liked the models presented in ``An Experimental Comparison of Click Position-Bias Models''. It presents four models, though the mixed model would only apply if we allowed the screener to read the same $r_{io}$ more than once and also to be inconsistent in his/her search strategy (future work maybe?). We consider the following and link them to the IO problem:
% %
% \begin{itemize}
%     \item Baseline Model: there is no position bias or, equivalently, the screener never gets tired: $\forall t \omega(t)=0$. 
%     \item Examination Model: similar to the ExhaustiveSearch.
%     \item Cascade Model: similar to the LazySearch.
% \end{itemize}
% %
% where we must consider that these models are based on ``clicking'' once and stopping while we consider the case where we ``click'' $k$ times to reach \textit{select-k}. For instance, in the \textit{examination model}, a user looks over the ranking almost exhaustively and then decides where to click. Also, we must consider that these models are based on \textit{the user assuming that the initial order of the ranking is meaningful}. We, thus, should refer to them and, if we choose to use the same terminology, expand them according to our setting.

% Regarding \textit{(i)}, it seems it is a question on all possible $r_{io}$ rankings and, overall, a question \textit{sequential decision-making}: how can bias manifest itself when looking at many items sequentially with or without a meaningful order? In that sense, we need consider, one, these biases and, two, if there are studies on ExhaustiveSearch and LazySearch strategies.

% There are some interesting works in the FindHR folder, though I cannot find ones that relate directly to the setting studies here. We have a could on HR hiring and how AI could help/worsen fairness/bias. We also have one (``Sifting and Sorting'') that examines the hiring practices in a bank (though in 1997) and how personal contacts influence the sequential decisions. None of these seem to consider fatigue as a parameter (the one on HR tools to reduce bias do so indirectly). In that sense, the literature on judgement heuristics / nudging is closer to us in the sense that ``we known there is a bias due to mental processes and want th platform or ADM to help reduce this brisk of bias''. Still, in this line of work, I do not seem to find explicit formalizations on search strategies (as, e.g., the click model literature does for the WWW users). Maybe we have a found a bridge here?

% Similarly, we must be honest about what is implicit in our translation of the human screener: some degree of optimal decision making on the basis of rational thinking. At a minimum, we assume an agent that wants to reach a goal and minimize the task duration to some degree: time is limited; otherwise, the screener could spend as much as needed to build the \textit{select-k} set without getting tired.

% Along the lines of decision-making models, some of interest: \textit{the effort accuracy framework}, and \textit{the preference construction framework}. In the former, the agent knows what it wants and will try to find it while keeping in mind the accuracy-effort trade-off, meaning that the better suited candidate is not the optimal one if it lies at the bottom of the list... In the latter, instead, the agent does not know what it wants explicitly and forms its preferences as it starts searching, meaning the way the list is presented influences the final decisions. It seems that both frameworks can affect our screener in the Lazy and Exhaustive searches, meaning that it will depend on what we want to assume. Under the first framework, we are in a more algorithmic setting: try to minimize the risk that the desired candidates require a disproportionally high search effort. While under the second framework, we in a more cognitive setting: make sure that the platform or list nudges/helps the agent to make the right decision buy building the best preferences. For the Generali setting and ranking problem formulation, the effort accuracy framework comes more natural. The preference construction framework seems more natural for human-computer interaction works. Still, both frameworks are Related Work, no? Consider the Chapter 10 from the FindHR folder.

% Under preference construction, relevant to us are: \textit{primacy/recency effects}; \textit{priming}; \textit{defaults}... though I feel like, one, this is not much relative to the 1970s Judgment Heuristics, and, two, the nudging literature seems more robust than this. Also, the clicking models indirectly address some of these issues... its main default is that we assume one click, which may not translate to all decision-making scenarios. That said: we could frame the two searches in terms of \textit{personalities}. In particular, \textit{the maximizer} for the LazySearch and \textit{the satisficer} for the ExhaustiveSearch. Though, these are more psychology definitions... I have a preference for defining the agents in terms of fatigue or even some sort of \textit{memory parameter} to capture the effects of primacy/recency or priming or default. 
