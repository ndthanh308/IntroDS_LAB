%
\label{sec:HumanScreener}

In this section, we want to model a screener $h$ prone to error when evaluating the candidate pool $\mathcal{C}$ given the initial order~$\theta$.
We do so to capture real world screening problems, such as the one in Section~\ref{sec:Generali}, involving human decision-makers and their reliance on digital technologies.    
Let us distinguish \textit{two kinds of screeners}.
We define a screener $h$ as \textit{algorithmic}, denoted by $h_a \in \mathcal{H}_a$, if it can consistently evaluate $\mathcal{C}$. 
The algorithmic screener is the implied screener in the fair ranking literature.
We also define a screener $h$ as \textit{human-like}, denoted by $h_h \in \mathcal{H}_h$, if it is conditioned by a fatigue component that hinders the consistency of its evaluation as it explores $\mathcal{C}$.
% SR
%As we will show here, it is under this human-like screener that the distinction between best-$k$ and good-$k$ becomes relevant.

% Time
Let us introduce a \textit{time component} to study these two screeners.
Let $t$ denote the discrete unit of time. 
It will be used to represent how long $h$ takes to evaluate a candidate $c \in \mathcal{C}$. We assume such a time to be constant, according to fact G5 in Section~\ref{sec:Generali}.
% \footnote{This assumption is based on fact G5 in Section~\ref{sec:Generali}.}  
This assumption implies that time itself cannot be optimized by the screener $h$.
We track time along the $\theta$ chosen by $h$ to evaluate $\candidatesset$, meaning that, at time $t=1$, $h$ evaluates the first candidate that appear in $\theta$ and so on. Thus time $t$ can range from $0$ (before start screening) to $n$ (after screening all candidates) at maximum.
%SR
%Hence, time $t$ follows the index $i$ in $\theta$ that identifies a candidate $c \in \mathcal{C}$.
% Fatigue
Time allows to capture what occurs in real life, especially with performing repetitive tasks, where the algorithmic screener $h_a$ is consistent in its evaluation of candidates while the human-like screener $h_h$ loses its consistency over time (see, e.g., \cite{Kahneman2021Noise}).
Formally, we introduce a \textit{fatigue component} $\phi(t)$ specific to $h_h$ as a function depending on time $t$ and 
model the \textit{accumulated fatigue} $\Phi \colon \{0, \ldots, n\} \to \mathbb{R}$, with $\Phi(0) = 0$. 
The discrete derivative of $\Phi$, that is, $\phi(i) = \Phi(i) - \Phi(i-1)$, defined for $t \geq 1$, is the effort of $h_h$ to examine the $t$-th candidate. 
%SR
% $c=\theta^{-1}(i)$.
%
What $\Phi$ means in practice is that a screener will evaluate the identical candidates $c_1$ and $c_2$ differently at times $t_1$ and $t_2$, as long as $\Phi(t_1) \neq \Phi(t_2)$.
We recognize that
fatigue can accumulate with different functional forms, 
meaning how we define $\Phi$ conditions the effects of fatigue on our analysis of $h_h$.
Here, we make the simplest modeling choice to define $\phi$ by assuming that fatigue accumulates linearly over time, or $\phi(t) = \lambda$ so that $\Phi(t) = \lambda \cdot t$.
This is one possible formulation based on our interpretation that $h_h$ will become tired at a constant pace over time.
We leave the study of other $\Phi$ formulations for future work.

\subsection{Fatigued Scores}
\label{sec:HumanScreener.BiasedScores}

We model the effect of fatigue in the evaluation of a candidate $c$ by $h_h$ through the \textit{fatigued score} $s_{h_h}(\mathbf{X}_c) + \epsilon$, where $\epsilon$ is a random variable depending on the fatigue $\Phi$ which quantifies the deviation from the \textit{truthful (or unbiased) score} $s_h(\mathbf{X}_c)$.
We can model $\epsilon$ in multiple ways. 
Here, we consider two ways.
The \textit{first modeling} is to see the error $\epsilon$ as a centered Gaussian variable, and the fatigue affecting only its variance. 
Formally, at given time $t$, 
$\epsilon$ is defined as $\epsilon_1 \sim \mathcal{N}(0, \, v(\Phi(t-1)))$, where $v \colon \mathbb{R} \to \mathbb{R}$ defines the variance of $\epsilon_1$ as an increasing function of the accumulated fatigue. 
The \textit{second modeling} is to assume that there is a negative bias in the evaluation of applicants.
Under this condition, the more fatigue, the more the screener $h_h$ tends to underscore the candidates.
Hence, we model $\epsilon$ as uncentered Gaussian, whose mean is a decreasing function of the fatigue.
Formally, $\epsilon$ is defined as $\epsilon_2 \sim \mathcal{N}(\mu(\Phi(t-1), \, v(\Phi(t-1))$, where $\mu \colon \mathbb{R} \to \mathbb{R}$ is a decreasing function rather than a constant.

We assume that $h_h$ is unaware of its fatigue as it goes over $\candidatesset$, representing an unconscious decision-making bias due, e.g.,~mental heuristics (see, e.g.,~\cite{tversky_judgment_1974}).
Hence, we are interested not by how $h_h$ minimizes its fatigue, which would require adding the fatigue to the problem definition, but by what is the impact of $h_h$'s fatigue on its screening process?
The answer depends on the objective of the set selection problem as discussed in Section~\ref{sec:ProblemFormulation.Objectives} and the initial order $\theta$. % chosen by the human-like screener $h_h$.

In terms of the search procedures proposed in Section~\ref{sec:ProblemFormulation.Algorithms}, Algorithms~\ref{algo:Examination} and \ref{algo:Cascade} represent the algorithmic screener $h_a$ as there is no notion of fatigue nor biased scores due to it. 
%SR
%Hence, we refer to such setting as the \textit{baseline model}.
To represent the human-like screener $h_h$, we track the accumulated fatigue $\Phi$ over the time and use it to draw $\epsilon$ so that $h_h$ computes the fatigued scores. %$s_{h_h}(\mathbf{X}_c) + \epsilon$. 
The main change is to line 2 in Algorithm~\ref{algo:Examination} and line 7 in Algorithm~\ref{algo:Cascade} where the score(s) computed for $c$ is biased by the $\epsilon$ at that point of time. 
We present the human-like versions of these two search procedures as Algorithms~\ref{algo:HumanExamination} and \ref{algo:HumanCascade} in Appendix~\ref{Appendix.HumanAlgorithms}.

\subsection{The Fairness of the Algorithmic and Human-like Screener}
\label{sec:HumanScreener.Analysis}

% Position Bias and the Implication of the Initial Order
We discuss the differences between an algorithmic $h_a$ or human-like $h_h$ screener from the fairness perspective.
Under the assumptions below, we can focus on $\theta$ and how, through position bias, it leads to individual fairness violations.
% Let us consider two assumptions.
%
% \textit{A1}: We assume that the initial order $\theta$ is independent of the protected attribute $W$, meaning the way in which candidates are sorted in $\theta$ contains no information about membership to either category in $W$.
% \textit{A2}: We also assume that the individual scoring functions $s$, for either screener, is able to evaluate candidate $c$ fairly and truthfully, meaning $s(\mathbf{X}_c)$ captures no information about $W_c$ (i.e., scores and the protected attribute are independent) and captures all information about the suitability of $c$ for the position. 
%
%
\begin{itemize}
    \item \textit{Assumption A1: We assume that the initial order $\theta$ is independent of the protected attribute $W$}, meaning the way in which candidates are sorted in $\theta$ contains no information about membership to either category in $W$.
    %
    \item \textit{Assumption A2: We also assume that the individual scoring functions $s$, for either screener, is able to evaluate candidate $c$ fairly and truthfully}, meaning $s(\mathbf{X}_c)$ captures no information about $W_c$ (i.e., scores and the protected attribute are independent) and captures all information about the suitability of $c$ for the position. 
\end{itemize}
%

We first focus on $h_a$ and the solutions of the best-$k$ and good-$k$ problems computed by Algorithms \ref{algo:Examination} and \ref{algo:Cascade}.
% SR
%We observe that imposing the condition of representational quota $f(S^k) \geq q$ changes the solution of the unconstrained problem, so that the achieved fair utility value is upper bounded by the maximum utility.
%
\textit{The $h_a$ reaches the optimal fair solution of both fair best-$k$ or good-$k$ problems.}
%
\textit{Moreover, $h_a$ guarantees individual fairness in both scenarios.}
%
%
Under assumptions \textit{A1} and \textit{A2}, this result for $h_a$ is intuitive for both Algorithms \ref{algo:Examination} and \ref{algo:Cascade}. 
Regarding group fairness, $h_a$ fulfills it by satisfying the representational quota $q$.
Regarding individual fairness, it is fulfilled as $h_a$ is able to consistently evaluate each candidate, treating similar candidates similarly. 
Therefore, $\theta$'s role here on fairness is trivial. 
These results hold under \textit{A2},
% \footnote{For instance, if $\mathbf{X}$ is a proxy for $W$, then $h_a$ can be consistently biased under $s$.} 
which we know it is likely not to be true in reality \cite{Zehlike2023_FairRanking_P1, Zehlike2023_FairRanking_P2} but we assume it here as our focus is on the human-like screener $h_h$ and its interaction with $\theta$.

% % Group fairness is fulfilled by satisfying the representational quota. 
% In particular, we highlight the assumption of the independent choice of the initial order $\theta$ of the protected attribute $W$.
% Individual fairness is guaranteed because two conditions are met: (i) the protected attribute does not directly affect the score evaluation;
% (ii) if the initial order $\theta$ is chosen independently from the protected attribute, then the computed score of similar individuals is the same. (recall \cite{DBLP:conf/innovations/DworkHPRZ12})

We now focus on $h_h$ and the solutions of the best-$k$ and good-$k$ problems computed by Algorithms~\ref{algo:HumanExamination} and \ref{algo:HumanCascade}.
Note that, under \textit{A1}, the error on the score does not affect the evaluation of the representational quota $q$ at a group level.
\textit{The $h_h$ guarantees, in both best-$k$ or good-$k$ problems, a fair group level solution despite the fatigue.}
Formally, the expected error $\mathbb{E}[\epsilon \mid W_c = 1, \theta] = \mathbb{E}[\epsilon \mid W_c = 0, \theta]$, whether $\epsilon$ is $\epsilon_1$ or $\epsilon_2$.
Intuitively, $h_h$ here simply needs to satisfy $q$. 
The fatigue and, thus, the fatigued scores are, on average, shared across protected and non-protected candidates.

However, \textit{individual fairness is not guaranteed under $h_h$.}
This result is intuitive.
Indeed, even if the protected attribute does not directly affect the score evaluation with error, the position of the candidate in $\theta$ influences the amount of error made by $h_h$ when evaluating that candidate. 
Therefore, similar candidates, because of their position in $\theta$ (i.e.,~no two candidate can occupy the same position), will not be evaluated similarly due to the accumulation of fatigue experienced by $h_h$.
For instance,
given two similar candidate $c_i = \theta^{-1}(i), c_j = \theta^{-1}(j)$, with $i < j$, their evaluation could be significantly different in the amount of error depending on the accumulated fatigue $\Phi(i) < \Phi(j)$. 
%In the case of $\epsilon_1$, $i$ has the advantage of being evaluated by a rested screener.
%Moreover, 
E.g.,~in the case of $\epsilon_2$, even if $\mathbf{X}_{c_j} = \mathbf{X}_{c_i}$, the score of $j$ is less, on average, than the one of $i$, so that $i$ has an unfair premium from $\theta$.
%Therefore the position bias of the human-like screener appears in both modeling of the error considered.

\subsection{Position Bias and the Initial Screening Order}
\label{sec:ProblemFormulation.PositionBias}

The implications of different modelings of the error $\epsilon$, or of other fairness requirements, need further discussion that we do not cover in this work.
Based on the previous section, though, our analysis already shows the overall risk of the position bias inherent to the initial order $\theta$ under a human-like screener.

In the case of the algorithmic screener $h_a$, we find trivial results, especially under the assumptions \textit{A1} and \textit{A2}.
The results for $h_a$ are trivial because of the nature of algorithms and their inherent consistency when it comes to decision making, which is, after all, a strong selling point for proponents of ADM systems (e.g., \cite{Miller2018Bias, Kahneman2016Noise}). 
Similarly, this is why position bias is treated as a technical bias in the fair ranking literature (recall, Section~\ref{sec:RelatedWork}) and focus is given on relaxing assumption \textit{A1} to learn a fair scoring function.

As our analysis shows, by considering the human-like screener $h_h$, however, that position bias and the role of $\theta$ acquire a significant meaning for individual fairness. 
We emphasize that the results in the previous subsection hold under assumptions \textit{A1} and \textit{A2}. If we relaxed either assumption, then the individual fairness violations should be even more concerning. 
Moving forward, we note that position bias as manifested in $\theta$ is only a function of where the candidates lie in $\theta$ when $h_h$ starts the candidate screening. 
We emphasize that
\textit{this fact is true whether $\theta$ is chosen by $h_h$ or provided to by, e.g., a (fair) ranking algorithm.}

%
% EOS
%
