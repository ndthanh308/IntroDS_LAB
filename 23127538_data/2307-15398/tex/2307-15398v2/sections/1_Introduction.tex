%
\label{sec:Introduction}

Candidate screening is a complex, often human-dependent process. 
It consists of a decision-maker, which we refer to as the \textit{screener}, who is tasked with choosing suitable candidates from a pool of applicants for a position.
Common candidate screening processes include the evaluation of, e.g., resumes for a job interview \cite{Pisanelli2022_YourCV} or application packages for university admission \cite{SukumarMH18_PeacanPie, DBLP:conf/chi/EchterhoffYM22}.
The screener usually evaluates the candidate pool using limited information and under strict time constraints.
Because of this, as with other repetitive tasks performed by humans \cite{tversky_judgment_1974, Miller2018Bias, Kahneman2011Thinking, Kahneman2021Noise}, candidate screening is prone to biased decision-making \cite{Pisanelli2022_YourCV, DBLP:conf/chi/EchterhoffYM22}.
A booming industry around automating the candidate screening process using Machine Learning (ML) has emerged in recent years \cite{DBLP:journals/air/WillKL23}.
Automated decision-making (ADM) systems for candidate screening, though, have yet to deliver on their promises of providing a reliable, auditable, and fair pipeline \cite{DBLP:conf/fat/WilsonG0MBSTP21,Wehner20,Sonderling22,Raghavan2020AlgortihmicHiring,DBLP:journals/datamine/RheaMDSSSKS22,jintelligence9030046,DBLP:journals/patterns/SloaneMC22, Fabris2023_DBLP:journals/corr/abs-2309-13933}.
% Based on our experience, which we discuss in Section~\ref{sec:Generali}, we believe that their failure is in large part due to how complicated and sensitive screening processes are in the real world. 
The future of automated candidate screening is an open question, especially as ADM systems for this task are increasingly required by stakeholders, such as regulators and users, to consider how humans interact with their output in the real world \cite{Carricco2018EUHumanCentred, DBLP:conf/aaai/Ruggieri0PST23}. 

In this paper, we investigate the role of the \textit{initial screening order} (ISO) in the \textit{set selection problem} within candidate screening. % the \textit{initial screening order} (ISO) problem.
The ISO refers to the order in which a screener chooses to sort the pool of candidates before screening them.
%SR
%As we discuss extensively in this paper, 
Depending on how we describe the screener, the ISO has an impact in terms of optimality (choosing the best candidates) and fairness (treating similar candidates similarly) on the final selected set of candidates.
% The ISO determines the screening position of all candidates, which is important to consider under a human screener as it can lead to \textit{position bias} \cite{DBLP:journals/cacm/Baeza-Yates18, PeiEAAMO2023, DBLP:conf/chi/EchterhoffYM22} within the candidate screening process. 
% As the name suggests, position bias refers to the penalty or premium each candidate experiences due to where it lies on the ISO \cite{DBLP:journals/cacm/Baeza-Yates18}.
This problem is motivated by our collaboration with company G, which we discuss further in Section~\ref{sec:Generali}, for studying its hiring process.
At G the Human Resources (HR) officers choose how to sort the applications stored in a database before evaluating them.
HR officers, e.g., use applicants' last names, among other sorting options.
For the company, the ISO is a known screening parameter, though the product of the screeners' heuristics or preferences rather than of a policy.
We suggest that ISO is a significant parameter as it determines the screening position of all candidates, which under a human screener could lead to \textit{position bias} \cite{DBLP:journals/cacm/Baeza-Yates18, PeiEAAMO2023, DBLP:conf/chi/EchterhoffYM22} within the screening task. 
As the name suggests, 
position bias refers to the penalty or premium each candidate experiences due to where it lies on the ISO \cite{DBLP:journals/cacm/Baeza-Yates18}.

Given an ISO, in Section~\ref{sec:ProblemFormulation} we first describe the screener in terms of two optimal and group-fair objectives: choosing \textit{best-$k$} versus \textit{good-$k$} candidates while meeting a representational quota $q$ of protected candidates.
Under best-$k$, the screener selects the $k$ best candidates while meeting $q$, 
which requires it to evaluate the complete candidate pool.
This is the standard set selection problem formulation.
Under good-$k$, instead, the screener selects the first good-enough $k$ candidates according to a minimum basic requirement that candidates must satisfy and while meeting $q$, 
which allows it to partially evaluate the candidate pool.
To the best of our knowledge, this paper provides the first formalization for the good-$k$ setting.
We devise algorithmic solutions to both problem formulations.
We then describe in Section~\ref{sec:HumanScreener} the screener as an \textit{algorithmic screener}, when it is consistent over time, or as a \textit{human-like screener}, when it is inconsistent over time due to fatigue. We evaluate the effects of the ISO on each kind of screener and screener's objectives.

Our theoretical analysis and simulated experiments in Sections~\ref{sec:HumanScreener} and \ref{sec:Experiments} confirm the presence of position bias induced by the ISO when considering the human-like screener for both best-$k$ and good-$k$ settings.
We find that, as the human-like screener becomes tired over time, it violates individual fairness \cite{DBLP:conf/innovations/DworkHPRZ12} by not evaluating similar individuals similarly while still meeting the group-level fairness in the form of meeting $q$.
Intuitively, as fatigue accumulates, the human-like screener is unable to consistently evaluate the candidate pool.
Candidates are, thus, dependent on the chosen ISO, with the first position being the most desirable one.

We argue for the importance of the ISO in human-in-the-loop screening processes.
The ISO is largely taken for granted by the set selection literature \cite{DBLP:conf/eaamo/BueningSBGD22, Zehlike2023_FairRanking_P1, Zehlike2023_FairRanking_P2}, with some recent exceptions \cite{DBLP:conf/chi/EchterhoffYM22, PeiEAAMO2023} and the earlier works on click models \cite{CraswellZTR08_ExperimentsClickPositionBias, DBLP:journals/jcmc/PanHJLGG07, DBLP:conf/clef/GrotovCMSXR15, DBLP:conf/www/RichardsonDR07},
given that the literature often conceives the screener as algorithmic.
Relative to a human, an algorithm cannot get tired, though it can still be consistently biased.
The setting changes once we consider a human screener.
The case of G is one simple example that does not involve ADM, but it does involve a database that conditions how the human screener sorts candidates. 
Even in the ideal case of a fair and optimal ADM providing a ranking of candidates, though, we would still face the influence of the ISO, only that the ISO now is provided to the human screener by the ADM.
We conclude with a discussion on future work in Section~\ref{sec:Discussion}.

Our main contributions are threefold.
\textit{First,} 
we provide insights from a real-world candidate screening problem, summarized as stylized facts, that are important to the fair set selection literature.
\textit{Second,} we formalize the role of the ISO in two possible objectives of the screener: the best-$k$ and in the good-$k$ problems. 
%% With the good-$k$ we also introduce a screener that can search partially the candidate pool.
%In particular, we point out that for good-$k$ the optimality can be reached by a partial search of the candidate pool
\textit{Third,} we formulate a human-like screener and compare it to its algorithmic counterpart both theoretically and with Monte Carlo experiments.  
The latter two contributions emphasize the role of position bias in candidate screening.

\subsection{Related Work}
\label{sec:RelatedWork}

We position our work within the relevant literature.
% Modeling position bias
We argue that our work, under the two set selection problem formulations and the human-like screener, revives the role of position bias. 
We define position bias as the increasing premium an item gets for being placed at the top (or beginning) of a search query result.
Human are predisposed to favor those items, as we tend to read from top to bottom \cite{DBLP:journals/cacm/Baeza-Yates18, 10.1093/qje/qjr028}.
Position bias was formalized \cite{CraswellZTR08_ExperimentsClickPositionBias} and tested \cite{DBLP:journals/jcmc/PanHJLGG07, DBLP:conf/clef/GrotovCMSXR15, DBLP:conf/www/RichardsonDR07} early on by search engine click models. 
These works studied how users responded to the ordering of their online search results and recommendations, providing evidence for position bias.
These works also find, similar to what we observed at G, that users varied in their search procedures.
The name choice of the two search procedures in Section~\ref{sec:ProblemFormulation.Algorithms} is a reference to this line of work. 
Different to click models, we consider a user that must choose (or ``click'') more than one item. We also formalize such users under a utility-maximizing framework.

% Links to fair ranking
The ISO problem, as it is already the case with position bias, would fall under technical bias within the fair ranking literature \cite{DBLP:journals/vldb/PitouraSK22,Zehlike2023_FairRanking_P1,Zehlike2023_FairRanking_P2}. 
The term alludes to sources of bias that are due to, e.g., platform design or inherent to the human subject. 
Two lines of work---probability-based fairness (PBF) and exposure-based fairness (EBF)---are suited for tackling position bias. 
In PBF \cite{DBLP:conf/ssdbm/YangS17, DBLP:conf/cikm/ZehlikeB0HMB17}, we constraint the ranking algorithm to (re-)arrange the candidates under some fair distribution like, e.g., a tossing a fair coin. 
In EBF \cite{DBLP:journals/cacm/Baeza-Yates18, DBLP:journals/sigir/JoachimsGPHG17}, we instead model attention, which decreases geometrically or logarithmically as the user faces a list of items, and constraint the ranking algorithm to (re-)arrange the candidates such that candidates receive similar exposure \cite{DBLP:conf/kdd/SinghJ18}.
Both PBF and EBF implicitly prepare for some kind of human-like screener, though, they rely on the assumption that attention is distributed by the user of the ranking. 
What happens when attention, meaning the amount of time a user spends on each item, is fixed? This was case for G and it might be the case in other candidate screening setting given that time spent on evaluating a candidate is an easy parameter to fix among human screeners. 
Further, both PBF and EBF implicitly assume that the user will explore the full ranking. Hence, the solutions proposed might not hold under partial searches of the ranking.
By considering fixed attention and partial search, we challenge and extend the PBF and EBF problem formulations. 
% We believe our work is useful for extending and improving both lines of work with the goal to design an algorithm that provides a ranking that acts as fair initial screening order to a more complex human user.

% Evidence of position bias
We highlight two recent works that provide evidence for position bias and the implicit role of the ISO \cite{DBLP:conf/chi/EchterhoffYM22, PeiEAAMO2023}.
\citet{DBLP:conf/chi/EchterhoffYM22} collaborate with a college to study anchoring bias \cite{Kahneman2011Thinking} in college admissions.
Anchoring bias \cite{tversky_judgment_1974} occurs when a candidate's evaluation is conditioned by the quality of the previously evaluated candidates.
The paper finds that the same candidate is better off if it is preceded by worst candidates than better candidates, which violates individual fairness \cite{DBLP:conf/innovations/DworkHPRZ12}. 
It occurs because the human screener anchors its expectations on a lower reference point despite each candidate being independent from each other. 
The paper proposes an algorithm for capturing and balancing anchoring bias.
\citet{PeiEAAMO2023} work with a college to study how the platforms used by professors for evaluating homework affects the students' grades. 
The paper runs a set of experiments, varying the order in which the homeworks is presented by the platform, to show that the default sorting order, which is in alphabetical order, unfairly rewards students with the same work quality depending on their last names, which violates individual fairness \cite{DBLP:conf/innovations/DworkHPRZ12}. 
This paper, in particular, provides clear evidence of the ISO problem.
% Similar to us, both works show the importance of being at the top of the initial screening order.
Overall, our work is among few other works that study the problem of candidate screening based on a real world collaboration with a focus on ADM systems and enabling platforms for screening
(e.g., \cite{DBLP:conf/chi/EchterhoffYM22, SukumarMH18_PeacanPie, PeiEAAMO2023}). These woks show the importance of considering the human user.
% We believe that future similar works should consider theories on human decision making (see \cite[Ch10]{DBLP:books/daglib/0033056} and \cite{DBLP:conf/chi/CarabanKGC19, DBLP:journals/isr/AdomaviciusBCZ13}).
We add to these works, by looking specifically at hiring and extend them by formalizing the notion of the ISO.

%
% EOS
%
