%
\label{sec:HumanScreener}

To study the human interaction 
% under these two problems 
with the \textit{initial screening order} (ISO), we distinguish two kinds of screeners $h$ based on the proneness to error when evaluating the candidate pool: 
$h$ is an \textit{algorithmic screener}, denoted by $h_a \in \mathcal{H}_a$, if it can consistently evaluate $\mathcal{C}$; 
% The algorithmic screener is the implied screener in the fair ranking literature. 
whereas $h$ is a \textit{human-like screener}, denoted by $h_h \in \mathcal{H}_h$, if its fatigue hinders the consistency of its evaluation of $\mathcal{C}$.

\subsection{Fatigue and Fatigued Scores}
\label{sec:HumanScreener.BiasedScores}

% Time
We first introduce a \textit{time component} to study these two screeners.
Let $t$ denote the discrete unit of time that represents how long $h$ takes to evaluate a candidate $c \in \mathcal{C}$. 
We assume that $t$ is constant (recall G5 Section~\ref{sec:Generali}), implying that time itself cannot be optimized by $h$.
We track time along $\theta$, meaning $h$ evaluates the first candidate that appears in $\theta$ at time $t=1$, and so on. 
Time $t$, thus, ranges from $0$ to $n$ at maximum.
% Fatigue
We then introduce a \textit{fatigue component} $\phi(t)$ specific to $h_h$ as a function of $t$ and model the \textit{accumulated fatigue} $\Phi \colon \{0, \ldots, n\} \to \mathbb{R}$, with $\Phi(0) = 0$. 
The discrete derivative of $\Phi$, that is, $\phi(i) = \Phi(i) - \Phi(i-1)$, defined for $t \geq 1$, is the effort of $h_h$ to examine the $t$-th candidate. 
%SR
% $c=\theta^{-1}(i)$.
How we define $\Phi$ conditions the effect of fatigue on our analysis of $h_h$.
We make the simplest modeling choice for $\phi$ by assuming that \textit{fatigue accumulates linearly over time}, or $\phi(t) = \lambda$ so that $\Phi(t) = \lambda \cdot t$,
% This $\Phi$ is one possible formulation based on our interpretation that 
meaning $h_h$ becomes tired over time at a constant pace.
% We leave the study of other $\Phi$ formulations for future work.

How does fatigue materialize for $h_h$?
We model the effect of fatigue on $h_h$ through the \textit{fatigued score}:
%
\begin{equation}
\label{eq:BiasedScoresForHh}
    s_{h_h}(\mathbf{X}_c) + \epsilon
\end{equation}
%
% $s_{h_h}(\mathbf{X}_c) + \epsilon$, 
where $\epsilon$ is a random variable dependent on $\Phi$ that quantifies the deviation from the \textit{truthful score} $s_h(\mathbf{X}_c)$.
We model $\epsilon$ using \emph{two modeling choices} at a given $t$.
% %
% \begin{itemize}
%     \item 
    \textit{\textbf{First modeling choice}:}
    $\epsilon_1$ is a centered Gaussian, and the fatigue affects only its variance. 
    Formally, $\epsilon_1 \sim \mathcal{N}(0, \, v(\Phi(t-1)))$, where $v \colon \mathbb{R} \to \mathbb{R}$ defines the variance of $\epsilon_1$ as an increasing function of $\Phi$.
    % %
    % \item 
    \textit{\textbf{Second modeling choice}:}
    $\epsilon_2$ as an uncentered Gaussian, whose mean is a decreasing function of the fatigue.
    % The second choice assumes that a negative bias can affect each applicant's evaluation.
    % Hence, we model $\epsilon_2$ as an uncentered Gaussian, whose mean is a decreasing function of the fatigue.
    Formally, $\epsilon_2 \sim \mathcal{N}(\mu(\Phi(t-1), \, v(\Phi(t-1))$, where $\mu \colon \mathbb{R} \to \mathbb{R}$ is a decreasing function rather than a constant of $\Phi$.
    % The more fatigue, the more the screener $h_h$ tends to underscore the candidates.
% \end{itemize}
% %

Intuitively, under $\epsilon_1$, $h_h$ tends to overscore or underscore candidates over time, introducing both negative and positive bias (i.e., fatigue as ``less attention'' when evaluating more candidates) over time; under $\epsilon_2$, instead, $h_h$ tends to underscore the candidates (i.e., fatigue as ``less effort'' when evaluating more candidates) over time, introducing always a negative bias. 
With both $\epsilon_1$ and $\epsilon_2$, we capture two realistic biased settings driven by the ISO $\theta$.
We assume that $h_h$ is unaware of its fatigue, representing an unconscious bias due to, e.g., performing a repetitive tasks over time \cite{Kahneman2011Thinking, Kahneman2021Noise}.

%
\begin{remark}
\label{remark:HumanAndIF}
    $\Phi$ implies that $h_h$ evaluates identical candidates $c_1$ and $c_2$ differently under $\theta$ at $t_1$ and $t_2$, as long as $\Phi(t_1) \neq \Phi(t_2)$ and regardless of whether $h_h$ is solving for the best-$k$ or good-$k$ problem.
\end{remark}
%

Algorithms~\ref{algo:Examination} and \ref{algo:Cascade} represent $h_a$ as there is no notion of biased scores.
To represent the human-like screener $h_h$, we must track $\Phi$ over time and draw $\epsilon_1$ (or $\epsilon_2$) to compute the fatigued scores \eqref{eq:BiasedScoresForHh} of $h_h$ at time $t$. 
The only changes are to line 2 in Algorithm~\ref{algo:Examination} and line 7 in Algorithm~\ref{algo:Cascade}, where the score computed for candidate $c$ is biased by $\epsilon_1$ (or $\epsilon_2$). 
We present the human-like versions of the search procedures in Appendix~\ref{Appendix.HumanAlgorithms} as Algorithms~\ref{algo:HumanExamination} and \ref{algo:HumanCascade};
%
though these two can be observed using Figure~\ref{fig:TheAlgos}.

\subsection{Position Bias Implications}
\label{sec:PositionBias}

No two candidates occupy the same position in the ISO $\theta$. With this fact in mind, we now analyse the fairness and optimality implications of the position bias implicit to $\theta$.
For concreteness, we make \textit{two assumptions}.
%
\textit{\textbf{A1}: We assume that $\theta$ is independent of the protected attribute $W$}, meaning that how candidates appear in $\theta$ contains no information about $W$.
%
\textit{\textbf{A2}: We assume that the individual scoring function $s$ is able to evaluate any candidate $c$ fairly and truthfully}, meaning $s(\mathbf{X}_c)$ captures no information about $W_c$ and only information about the suitability of $c$.
%
Under \textit{A1} and \textit{A2}, we can control for other biases, such as measurement error in $s$, and focus on the position bias coming from $\theta$.

We start with the fairness implications for both best-$k$ and good-$k$ problems. 
Given $\theta$, it is important to distinguish between the group fairness constraining $h$ (i.e., the quota $q$) and the individual fairness violation when $h$ fails to evaluate similar candidates similarly \cite{DBLP:conf/innovations/DworkHPRZ12}.
Regarding group-level fairness, both $h_a$ and $h_h$ are fair in solving for \eqref{eq:fair_objective_all_screener} and \eqref{eq:AlternativeUtility} by satisfying $f\big(S^k\big) \geq q$.
This point is clear for $h_a$ in Algorithms \ref{algo:Examination} and \ref{algo:Cascade} as there is no fatigue involved.
The same holds for $h_h$ in Algorithms \ref{algo:HumanExamination} and \ref{algo:HumanCascade} 
because the error on the score does not affect the evaluation of $q$.
Here, we have that the expected error $\mathbb{E}[\epsilon \mid W_c = 1, \theta] = \mathbb{E}[\epsilon \mid W_c = 0, \theta]$, regardless of $\epsilon_1$ or $\epsilon_2$ for $h_h$. 
The fatigue and, thus, the fatigued scores are, on average, shared across protected and non-protected candidates.

Distinguishing between $h_a$ and $h_h$ becomes important under individual-level fairness because $h_a$ ensures it, while $h_h$ violates individual fairness.
A candidate's position in $\theta$ influences the amount of error made by $h_h$ when evaluating that candidate. 
Similar candidates will not be evaluated similarly due to the unequal accumulation of fatigue experienced by $h_h$ when searching $\theta$.
% For instance, given two similar candidate $c_i = \theta^{-1}(i), c_j = \theta^{-1}(j)$, with $i < j$, their evaluation could be significantly different in the amount of error depending on the accumulated fatigue $\Phi(i) < \Phi(j)$. 
% In the case of $\epsilon_1$, $i$ has the advantage of being evaluated by a rested screener.
For $\epsilon_2$, e.g., even if $\mathbf{X}_{c_j} = \mathbf{X}_{c_i}$ but $j>i$, the score of $j$ is less, on average, than the one of $i$, and $i$ has an unfair premium over $j$ from $\theta$.

We now consider the optimality implications for both best-$k$ and good-$k$ problems. 
Recall that each problem, due to its own utility model, has different optimal solutions.
It follows that $h_a$ reaches the optimal solution in both problems as the absence of fatigue enables $h_a$ to consistently judge suitable candidates.
The opposite holds for $h_h$ due to the inconsistent scoring of candidates ascribed by the accumulated fatigue.
The biased scores not only violate individual fairness, but also lead $h_h$ to misjudge candidates, eventually choosing the wrong ones when searching $\theta$.

To summarize, \textit{$h_a$ reaches the optimal and fair solution for both best-$k$ and good-$k$ problems}.
Moreover, \textit{$h_a$ guarantees individual fairness in both problems}.
Instead, \textit{$h_h$ reaches the fair but sub-optimal solutions for both best-$k$ and good-$k$ problems}.
Moreover, \textit{$h_h$ does not guarantee individual fairness in both settings}. 
Significantly, under the $h_h$, the position of a candidate in $\theta$ matters. 
It impacts whether a candidate, depending on the search strategy, is evaluated or not (Remark~\ref{remark:ISOandPS}) and, if so, is evaluated fairly or not (Remark~\ref{remark:HumanAndIF}).
These results only worsen when relaxing \textit{A1} and \textit{A2}.
We explore these insights further by considering multiple hiring settings for $h_a$ and $h_h$ in the next section.

%
% Figure environment removed
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Moved to the Appendix
% %
% % Figure environment removed
% %\vspace{-3ex}
% %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
% EOS
%
