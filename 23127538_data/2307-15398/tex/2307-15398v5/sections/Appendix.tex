%
\label{Appendix}

%
% Figure environment removed
%

%
% Figure environment removed
%

\section{Collaborating with G}
\label{Appendix.MoreOnG}

Candidate screening at G represented both a time-consuming, repetitive task prone to human error and a sensitive, high-risk task requiring human oversight.
Therefore, the option of full automation was not possible. 
The focus was, thus, on understanding and modeling the search of the HR officer via the hiring platform.
Importantly, under the realistic risk of position bias affecting candidate evaluations via the hiring platform, we wanted to study the influence of the \textit{initial screening order} (ISO) on the set of suitable candidates chosen by the HR officer.

\paragraph{Overall experience.}
The collaboration lasted for four months.
Due to the COVID-19 pandemic, it was a hybrid collaboration.
During this time, we mostly interviewed the HR officers to understand their tasks, constraints, and methodologies, often shadowing them during screening sessions.
We were also granted access to Taleo by Oracle, the platform used by HR for managing the hiring pipeline.
This allowed us to experience for ourselves the patterns we observed among the HR officers. These patterns resulted in the five stylized facts in Section~\ref{sec:Generali}. 

We interacted, in particular, with five HR officers specialized in screening applications for technical roles within G, such as the roles of data scientist and front-end developer.
These HR officers had to process considerable amounts of information within a time constraint. Based on what we observed, most candidate pools (except for very senior profiles like, e.g., director of data science) consisted of hundred of applications. 
These HR officers were involved in screening multiple candidate pools with similar deadlines within the same week.
It became apparent to us, specially for candidate screening, how time-consuming and human-dependent was the hiring process at G. 

We discussed our observations, often on a bi-weekly basis, to members of both the HR and Advanced Analytics (AA) teams.
We emphasize that we simply collaborated with these teams as equals, sharing the goal of understanding G's hiring process and whether it was suitable or not for (fair) automation. 
We specifically use the wording “stylized facts” in Section~\ref{sec:Generali} to emphasize that we draw inspiration from the collaboration with G in formulating the ISO problem rather than ``hard facts'' about G derived from an observational study. In fact, the first draft of this paper occurred about one year after our collaboration with G had concluded.

\paragraph{Deliverables.}
We concluded the collaboration with a report to AA that formalized G's candidate screening process as a ranking problem, evaluated the potential fairness implications, and assessed the risk and benefits of automation.
The report was discussed with a wider audience within G in a series of presentations hosted by the AA and HR teams. 
The report focused mainly on candidate screening.
It is worth mentioning also that throughout the collaboration, we followed G's strict ethical guidelines at all times.
No sensitive data (or data at all) from G was used for this work.
This paper is not a deliverable specific to G.

\paragraph{The hiring pipeline.}
Hiring at G consists mainly of three phases. With respect to the stylized facts, these are concerned with the candidate screening phase or phase two.
These three phases are the following:
%
\begin{itemize}
    \item In \textit{phase one}, the HR builds a candidate pool for the job opening. Candidates submit their CVs, complete a multiple-choice questioner, and write a motivation letter. Sensitive information, such as gender, ethnicity, and age, is also provided or it can be inferred. The candidate pool is stored in a database platform.
    %
    \item In \textit{phase two}, the HR officer reduces the candidate pool into a smaller pool of suitable candidates. The HR officer determines candidate suitability based on each candidate's profile using a set of minimum basic requirements.
    %
    \item In \textit{phase three}, the chosen candidates are interviewed by HR and the team offering the job. 
    It is common for the hiring team to also prepare a use case for the candidates.
    The best candidates receive an offer. If no candidates are hired, HR resorts to the runner-up candidates from phase two and repeat phase three.
\end{itemize}
%
Note that the above represent G's hiring pipeline during the collaboration. We do not know if this hiring pipeline is still the case today, though it is not important for the purpose of this paper.

\section{Supplementary Material}
\label{Appendix.SupplementaryMaterial}

\subsection{Additional Discussion on the Utility Model in Section~\ref{sec:good-k}}
\label{Appendix.NaiveUtilityGoodk}

First, we present Example~\ref{ex:diff_fractions_prot} below. 
It shows that for the utility model \eqref{eq:AlternativeUtility} for the good-$k$ problem, in the general case, i.e.,~$f(S^k) \geq q$, there can be two solutions, but with different fractions of the protected group.

%
\begin{example}
\label{ex:diff_fractions_prot}
    % For example, c
    Consider $n=3, k=2, q=0.5$. Assume three eligible candidates and $\theta(1) = c_1, \theta(2) = c_2, \theta(3) = c_3$ with $W_1 = 0$ and $W_2 = W_3 = 1$. Both $S' = \{c_1, c_2\}$ and $S'' = \{c_2, c_3\}$ are solutions of (\ref{eq:fair_objective_U_psi}) with $U^k_{\psi} \big(S', \theta\big) = U^k_{\psi} \big(S'', \theta\big) = 2$. However, $f(S') = 0.5$ and $f(S'') = 1$. Intuitively, $S'$ is obtained by strictly iterating over $\theta$.
\end{example}
%

\noindent
We now motivate the utility model \eqref{eq:AlternativeUtility} used in the good-$k$ problem.
As a simple alternative utility function in the good-$k$ setting consider the following utility model:
%
\begin{equation}
\label{eq:AlternativeUtility2}
    \hat{U}^k_{\psi}\big( S^k, \theta \big) = \left\{
    \begin{array}{ll}
        n - \max_{c \in S^k} \theta^{-1}(c) & \text{if} \  \forall c \in S^k \  s(\mathbf{X}_c) \geq \psi   \\
        0 & \text{otherwise.}
    \end{array} \right.
\end{equation}
%
where we use $\hat{U}_\psi^k$ (i.e., the $U$ hat) to differentiate from the utility model $U_\psi^k$ considered in \eqref{eq:AlternativeUtility}.

Intuitively, in the above utility definition the screener wants to find as quickly as possible a set of $k$ eligible candidates.
Therefore, if $S^k$ contains only eligible candidates, the utility of $h$ selecting $S^k$ under $\theta$ is expressed by the number of candidates past the last one who was screened, i.e.,~the ``saved effort" of the screener $h$.
Despite the simplicity of \eqref{eq:AlternativeUtility2}, the above utility model \eqref{eq:AlternativeUtility2} is not suitable to properly account for the intended good-$k$ problem.
To observe this last point, consider Example~\ref{ex:AltU2}.

%
\begin{example}
\label{ex:AltU2}    
    Let $n=3, k=2, q=0.5$. Assume three eligible candidates and $\theta(1) = c_1, \theta(2) = c_2, \theta(3) = c_3$ with $W_1 = W_2 = 0$ and $W_3 = 1$. It turns out that both $S' = \{c_1, c_3\}$ and $S'' = \{c_2, c_3\}$ maximize the utility \eqref{eq:AlternativeUtility2} and satisfy the fairness constraint $q$.
\end{example}
%

Following up on Example~\ref{ex:AltU2}, why should have been $c_2$ considered, and then returned in $S''$, if $c_1$ already meets the minimum basic requirement? 
A reason for doing that is a variant of our good-$k$ problem in which the screener $h$ keeps evaluating non-protected candidates in $\mathcal{C}$, even if their quota is reached but the one of protected candidates is not yet reached, for the purpose of keeping the best ones found so far. 
We do not consider such a variant in this paper.
For this reason, we introduce the penalty function \eqref{eq:Penalty} in \eqref{eq:AlternativeUtility} presented in Section~\ref{sec:good-k}.

\subsection{The Two Search Procedures under a Human-Like Screener}
\label{Appendix.HumanAlgorithms}

We update the \textit{ExaminationSearch} and \textit{CascadeSearch} and their corresponding Algorithm \ref{algo:Examination} and Algorithm \ref{algo:Cascade} from Section~\ref{sec:ProblemFormulation.Algorithms} under the human-like screener $h_h$ from Section~\ref{sec:HumanScreener}. 
% In particular, 
We incorporate the \textit{fatigued scores} formulation from Section~\ref{sec:HumanScreener.BiasedScores} into both algorithms, resulting in a human-like \textit{HuamnExaminationSearch} (Algorithm~\ref{algo:HumanExamination}) and a human-like \textit{HumanCascadeSearch} (Algorithm~\ref{algo:HumanCascade}).
In comparison to the algorithmic screener $h_a$, the main difference here is that both algorithms compute the fatigued score for candidate $c \in \mathcal{C}$:
%
\begin{equation}
    Y_c = s(\mathbf{X}_c) + \epsilon
\end{equation}
%
where $\epsilon$ is a random variable depending on the accumulated fatigue $\Phi$ of $h_h$.
In both algorithms \ref{algo:HumanExamination} and \ref{algo:HumanCascade}, by requiring the fatigue component $\Phi$, we also require a specific modeling choice for $\epsilon$ which is a probabilistic function of the accumulated fatigue $\Phi$. As discussed in Section~\ref{sec:HumanScreener.BiasedScores}, $\epsilon$ can be modeled as either $\epsilon_1$ or $\epsilon_2$.

We stress once again that other formulations for $\epsilon$ are possible.
These formulations are compatible with Algorithms~\ref{algo:HumanExamination} and \ref{algo:HumanCascade} as long as $\epsilon$ is treated as a random variable that is drawn each time $h_h$ evaluates candidate $c$.
These formulations can be implemented as with $\epsilon_1$ and $\epsilon_2$, meaning by providing the corresponding probability distribution for the intended accumulated fatigue $\Phi$.

\section{Additional Experiments}
\label{Appendix.MoreExperiments}

In this section, we present additional figures and corresponding discussions relating to the experimental analysis of the algorithmic screener $h_a$ from Section~\ref{sec:Experiments.Metrics.outFatigue}.

% First, we consider the impact of the correlation $\rho$ between $\theta$ and the scores. 
% Recall that $\rho = -1$ means that the candidates are ordered by descending scores. 
% Under such a condition, the good-$k$ and best-$k$ procedures return the same solution. 
% This result is apparent in Figure~\ref{fig:2} (center, right) where we report the ratio to baseline for the symmetric (left) and the increasing (right) score distributions. 
% The plots show that even a moderate correlation of $\rho = -0.5$ leads the good-$k$ solution to approximate the best-$k$ one quite well. 
% For the increasing distribution (right), the ratio to baseline is around 95\%. 
% In summary, initial orders that negatively correlate to the score greatly reduce the difference in utility between the good-$k$ and best-$k$ solutions.

% Second, we consider the impact of the number $n$ candidates in $\candidatesset$ and the number of $k$ candidates to be selected. 
% We focus only on the symmetric distribution (in red) and the RtB metric, but the results are similar for the other two distributions (in blue and green) and the JdS metric. 
% Figure~\ref{fig:2} (left) compares the case $n=120, k=6$ considered earlier to two other scenarios in terms of $n$ and $k$. 
% The first scenario increases $k=20$, but leaves the ratio of selected $k/n = 0.05$ the same by also increasing $n=400$. 
% The second scenario, instead, leaves $k=6$ the same, but it increases $k/n = 0.2$ by decreasing $n=30$. 
% The plot shows that changes in the ratio $k/n$ affect the metric, in particular a larger ratio ($n=30$, $k=6$) leads good-$k$ to better approximate best-$k$ for the same $\psi$. 
% In other words, the more candidates we can select from the pool, the better the chance to include top ones. 
% In summary, under $\theta \ci s$, the ratio $k/n$ is positively correlated with the ability of best-$k$ to approximate good-$k$.

% %
% % Figure environment removed
% %

Let us consider the quota parameter $q$, thus far set to $q=0.5$ over a population with a fraction of protected candidates set to $\mathit{pr}=0.2$. 
Since we assumed that $W$ is independent from both scores and the ISO, the fraction of protected group in the solutions of best-$k$ and good-$k$ is, on average, $\mathit{min}\{q, \mathit{pr}\}$. 
Figure~\ref{fig:3} (left) shows this result in the solution for good-$k$. 
A less trivial question is whether $q$ is also not affecting the evaluation metrics: e.g., whether the quota $q$ changes the ratio to baseline? 
Figure~\ref{fig:3} (center, right) show that this is not the case in two experimental settings. 
Again, this result is theoretically implied by the independence of $W$ with scores and initial order. 
In summary, under $\theta \ci s$, the quota $q$ in the best-$k$ and good-$k$ problem does not affect the relative strengths of their solutions.

%
% EOS
%
