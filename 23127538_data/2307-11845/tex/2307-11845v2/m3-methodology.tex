\section{Methodology}
\label{sec:methodology}
The methodology section clarifies the technical foundations of the study. First, we discuss the general principles of multimodal models before addressing specific applications in banking. The second half of the section focuses on multimodal document analytics and concludes with an in-depth exploration of the LayoutXLM model.

Unimodal models such as the Transformer BERT \citep{devlinBert} or the GPT-3 series \citep{brown2020language}, have made significant strides in text analysis. However, their focus on a single data type can miss relevant information in industries with various data sources, such as banking \citep{zhu2015}.


% \begin{table}[htbp]
%   \centering
%   \caption{Methodological Overview of Multimodal Models in Banking}
%   \label{tab:literature-methodology1}
%   \setlength\tabcolsep{5pt}
%   \renewcommand{\arraystretch}{1.4}
%   \scriptsize
%     \begin{tabularx}{\textwidth}{p{85px}Xc}
%     \toprule
%     \textbf{Study} & \textbf{Main Topic} & \textbf{NLP} \\
%     \midrule
%     \addlinespace[1ex]
%     \multicolumn{3}{l}{\textbf{Multimodal Models in Banking}} \\
%     \addlinespace[1ex]
%     \citet{lee2019} & Improved accuracy in stock market prediction using multimodal deep learning and compare fusion levels  &  \\
%     \citet{dang2018} & Stock2Vec, a deep learning approach for short-term stock trends prediction based on two-stream GRU network   & X \\
%     \citet{li2021} & LSTM model for stock prediction using text and fundamental data   & X \\
%     \citet{stevenson2021} & Combined model for small business default prediction using textual and traditional features   & X \\
%     \citet{tavakoli2023} & Credit rating prediction in banking using textual and numerical data in a CNN-based multimodal model  & X  \\
%     \addlinespace[1ex]
%     \bottomrule
%     \end{tabularx}%
% \end{table}

To bridge this gap, there is a growing body of research on multimodal models that integrate different types of data to understand complex data. Early, intermediate and late fusion describe the integration point within the model and have an impact on overall performance \citep{boulahia2021, zhu2015}. Multimodal models tend to deliver superior accuracy over unimodal predecessors \citep{poria2017}. 


%A chronological and thematic overview of the studies discussed in this context, from general multimodal models to their specific applications in banking, is provided in Table \ref{tab:literature-methodology1}. These studies help to establish our understanding of multimodal approaches and their efficacy.

Multimodal approaches already demonstrate promising results in banking. For example, \citet{lee2019} find that merging different sources of stock market data, using early or intermediate fusion, improves the accuracy of stock market predictions, while \citet{dang2018} also include textual data in their scope to effectively predict short-term stock trends. 
\citet{wang2023} implement multimodal financial statement fraud detection with textual and financial data. Similarly, \citet{stevenson2021} and \citet{tavakoli2023} use multimodal models for credit risk prediction, demonstrating their practical application in supporting decisions within banking.


\subsection{Multimodal Document Analytics}
\label{sec:multimodal-document-analytics}
Table \ref{tab:literature-methodology2} shows how the evolution of document analytics has been marked by the integration of multimodal data, extending the capabilities of traditional unimodal natural language understanding (NLU) models. 


\begin{table}[htbp]
  \centering
  \caption{Methodological Overview of Document Analytics Frameworks}
  \label{tab:literature-methodology2}
  \setlength\tabcolsep{33pt}
  \renewcommand{\arraystretch}{1.2}
  \scriptsize
    \begin{tabularx}{\textwidth}{lccc}
    \toprule
    \textbf{Study} & \textbf{Presented Model} & \textbf{Multimodal} & \textbf{Cross-lingual} \\
    \midrule
    \addlinespace[1ex]
    \citet{devlinBert} & mBERT & & X \\
    \citet{conneauXLM} & XLM& & X \\
    \citet{conneauXLMRoberta} & XLM-RoBERTa & & X \\
    \citet{brown2020language} & GPT-3 & & X \\
    \citet{xuLayoutLMv1} & LayoutLM & X & \\
    \citet{xuLayoutlmv2} & LayoutLMv2 & X & \\
    \citet{liSelfDoc} & SelfDoc & X & \\
    \citet{chiInfoXLM} & InfoXLM & X & X \\
    \citet{xu2021layoutxlm} & LayoutXLM & X & X \\
    \citet{liMarkupLM} & MarkupLM & X & \\
    \citet{chenXDoc} & XDoc & X & \\
    \citet{huang2022layoutlmv3} & LayoutLMv3& X & \\
    \citet{liDIT} & DiT & X & (X)\\
    \bottomrule
    \end{tabularx}
\end{table}


Despite their proficiency in handling textual data, early NLU models such as mBERT \citep{devlinBert}, XLM \citep{conneauXLM}, XLM-RoBERTa \citep{conneauXLMRoberta}, and GPT-3 \citep{brown2020language} are bound by their unimodal structure, making them insufficient for handling multimodal data. The introduction of LayoutLM \citep{xuLayoutLMv1}, which jointly analyzes text and layout, marks a shift towards multimodal document analysis. Building on this development, LayoutLMv2 \citep{xuLayoutlmv2} offers a refined approach and significantly enhances the visual-rich document understanding capabilities (VrDU).
In parallel, SelfDoc \citep{liSelfDoc} employs a self-supervised pre-training framework, while MarkupLM \citep{liMarkupLM} presents a pre-trained model to analyze text and markup information from websites. However, the scope of research on multimodal models is mostly limited to English, with some exceptions such as the DiT model \citep{liDIT}, which is also trained on Chinese documents.

To address a more global applicability, the cross-lingual pre-training model, InfoXLM \citep{chiInfoXLM}, is introduced to fill this gap. This advancement becomes instrumental in the development of LayoutXLM \citep{xu2021layoutxlm}, a multimodal cross-lingual document analytics foundation model that enables VrDU in 53 different languages. Despite the notable progress in the development of GPT-4,  in particular its multimodal capabilities that focus on image understanding and image content querying, it still lacks the ability to understand the spatial and layout information of word tokens \citep{openai2023gpt4}. Therefore, it is not included in this study. 

\subsection{LayoutXLM}
LayoutXLM uniquely combines the benefits of multimodal learning and cross-lingual understanding.
Its approach positions it as a leading model in this field, making it a cornerstone of our study \citep{xu2021layoutxlm}.


\subsubsection{Architecture}
LayoutXLM adopts the transformer-based architecture of LayoutLMv2 \citep{xuLayoutlmv2}, which takes inputs from three different modalities: text, layout, and images. These inputs are encoded using text embedding, layout embedding, and visual embedding layers. 

Embeddings are essential to deep learning models as they transform data into structured representations that can express complex syntactic and semantic information. They provide a solution to the high dimensionality and sparsity of raw data by transforming the data into low-dimensional dense vectors, which facilitates efficient computation and storage \citep{mikolov2013efficient}.



% Figure environment removed

Figure \ref{Fig:LayoutXLM} depicts the overall architecture of LayoutXLM \citep{xuLayoutlmv2, xu2021layoutxlm} in a token classification setup. In this section, we examine the individual components.


\paragraph{Text Embedding}
Essentially, each token's text embedding is derived from its own identity (the token itself), its position in the sequence, and its source segment.
To obtain the text embedding, the document's text is first tokenized into a sequence. This sequence begins with a [CLS] token, ends with a [SEP] token for each text segment, and is padded with [PAD] tokens to reach the maximum sequence length (L). Now, the $i$-th ($0 \leq i \le L$) text embedding is the sum of the following three components:

\begin{enumerate}
  \item The token embedding $TokEmb(w_i)$, which represents the token itself.
  \item The 1D positional embedding $PosEmb1D(i)$, which signifies the position of the token within the sequence.
  \item The segment embedding $SegEmb(s_i)$, which indicates the segment of the text from which the token originates.
\end{enumerate}

Thus, the $i$-th text embedding is represented as:
\[
t_i = \textrm{TokEmb}(w_i) + \textrm{PosEmb1D}(i) + \textrm{SegEmb}(s_i)
\]


\paragraph{Visual Embedding}

In the visual embedding step, each page image $I$ is resized to $224 \times 224$ and fed into a Convolutional Neural Network (CNN) to generate a fixed-length visual token embedding sequence. These embeddings are then unified in dimensionality using a linear projection $\textrm{Proj}(\textrm{VisTokEmb}(I)_i)$, ensuring consistency across different inputs. 
Since the CNN does not capture positional information, a shared 1D positional embedding $\textrm{PosEmb1D}(i)$, similar to the one used in the text embedding, is added to each visual token to provide positional information.  All visual tokens are grouped into the visual segment $C$. The final $i$-th visual embedding is expressed as:

\[
v_i = \textrm{Proj(VisTokEmb}(I)_i) + \textrm{PosEmb1D}(i) + \textrm{SegEmb}(C)
\]

where $0 \leq i < WH$. Dimensions $W$ and $H$ represent the size of the output feature map after processing the image through CNN, giving us a total of $WH$ visual tokens. This representation combines the visual features, positional information, and segment assignment for each visual token.


\paragraph{Layout Embedding}
The layout embedding encodes the spatial layout information derived from the OCR results, including data about the bounding box (width, height, and corner coordinates) of each token. These coordinates are normalized and discretized to integers in the range [0, 1000]. 

For the $i$-th text or visual token (with $0 \leq i < WH + L$), denoted by its normalized bounding box $box_i = (x_{min}, x_{max}, y_{min}, y_{max}, width, height)$, the layout embedding layer combines these six bounding box features to construct a 2D positional (layout) embedding $l_i$, with
\[
l_i = \textrm{Concat}(\textrm{PosEmb2D}_x(x_{min}, x_{max}, width), \\
\textrm{PosEmb2D}_y(y_{min}, y_{max}, height))
\]

The layout embedding hence encapsulates the spatial arrangement or layout information of the document. Special tokens such as [CLS], [SEP], and [PAD] are associated with an empty bounding box $(0, 0, 0, 0, 0, 0)$.


\paragraph{Multimodal Encoder with Spatial-Aware Self-Attention Mechanism}

The three embeddings - text, visual, and layout - are then combined using an early fusion approach, where the information from all the modalities is merged right from the beginning \citep{zhu2015, boulahia2021}. This combined multimodal input is fed into a transformer-based encoder, allowing the model to learn a joint representation where all modalities can influence each other during the encoding process.
Visual embeddings ${v_0, ..., v_{WH-1}}$ and text embeddings ${t_0, ..., t_{L-1}}$ are concatenated into a unified sequence $X = {v_0, ..., v_{WH-1}, t_0, ..., t_{L-1}}$. Spatial information is integrated into this sequence by adding layout embeddings to each corresponding element, yielding the $i$-th (with $0 \leq i < WH + L$) first-layer input $x^{(0)}_i = X_i + l_i$.

%\[x^{(0)}_i = X_i + l_i\ \textrm{, with}\]
%\[X = {v_0, ..., v_{WH-1}, t_0, ..., t_{L-1}\]

The core of the LayoutXLM encoder follows the transformer architecture, consisting of a stack of multi-head self-attention layers followed by a feed-forward network. However, unlike the original self-attention mechanism, LayoutXLM uses a spatially-aware self-attention mechanism to incorporate relative position information and efficiently model local invariance in the document layout \citep{xuLayoutlmv2}. Its unified approach of combining text, visual and layout information allows LayoutXLM to capture the rich information present in document images and facilitates powerful downstream applications such as document understanding and information extraction \citep{xu2021layoutxlm}.


\subsubsection{Pre-Training and Fine-Tuning}
The pre-training objectives of LayoutXLM are adapted from the LayoutLMv2 framework, which has shown efficacy in VrDU.
LayoutXLM undergoes pre-training with three tasks, Multilingual Masked Visual-Language Modeling, Text-Image Alignment, and Text-Image Matching, which together strengthen the model's understanding of language, visuals, and spatial relationships in documents \citep{xuLayoutlmv2}.

The pre-training tasks enable LayoutXLM to learn an understanding of documents at a semantic and visual level, as well as layout and spatial information. The extensive pre-training process allows LayoutXLM to use transfer learning, where the pre-trained model can be effectively tuned for specific tasks even with smaller labeled datasets. This adaptability makes LayoutXLM a versatile foundation model for document analytics that can be used in a wide range of downstream tasks. Its value is particularly pronounced in the banking sector, where document analysis is a key success factor. Integrating LayoutXLM into the workflow of a bank can enhance its processes and drive efficiency and decision-making.


