\section{Methodology}
\label{sec:methodology}
The methodology section focuses on the technical fundamentals of this study. Following the literature review, which concentrated on application-oriented studies of NLP and multimodal document analytics in banking, we shift our attention to the model architecture and theoretical foundations. We first discuss the general principles of multimodal models before addressing specific applications in banking. The second half of the section focuses on multimodal document analytics and concludes with an in-depth exploration of the LayoutXLM model.


Unimodal models like the Transformer BERT \cite{devlinBert}, have made significant strides in text analysis. However, their focus on one single data type can miss relevant information in industries with diverse data sources like banking \cite{zhu2015}.


% \begin{table}[htbp]
%   \centering
%   \caption{Methodological Overview of Multimodal Models in Banking}
%   \label{tab:literature-methodology1}
%   \setlength\tabcolsep{5pt}
%   \renewcommand{\arraystretch}{1.4}
%   \scriptsize
%     \begin{tabularx}{\textwidth}{p{85px}Xc}
%     \toprule
%     \textbf{Study} & \textbf{Main Topic} & \textbf{NLP} \\
%     \midrule
%     \addlinespace[1ex]
%     \multicolumn{3}{l}{\textbf{Multimodal Models in Banking}} \\
%     \addlinespace[1ex]
%     \citeay{lee2019} & Improved accuracy in stock market prediction using multimodal deep learning and compare fusion levels  &  \\
%     \citeay{dang2018} & Stock2Vec, a deep learning approach for short-term stock trends prediction based on two-stream GRU network   & X \\
%     \citeay{li2021} & LSTM model for stock prediction using text and fundamental data   & X \\
%     \citeay{stevenson2021} & Combined model for small business default prediction using textual and traditional features   & X \\
%     \citeay{tavakoli2023} & Credit rating prediction in banking using textual and numerical data in a CNN-based multimodal model  & X  \\
%     \addlinespace[1ex]
%     \bottomrule
%     \end{tabularx}%
% \end{table}

To bridge this gap, there is a growing body of research on multimodal models that integrate different data types to understand complex data. Early, intermediate, and late fusion, describe the integration point within the model and have an impact on the overall performance \cite{boulahia2021, zhu2015}. Multimodal models tend to deliver superior accuracy over unimodal predecessors \cite{poria2017}. 


%A chronological and thematic overview of the studies discussed in this context, from general multimodal models to their specific applications in banking, is provided in Table \ref{tab:literature-methodology1}. These studies help to establish our understanding of multimodal approaches and their efficacy.

Multimodal approaches already demonstrate promising results in banking. For instance, \citeay{lee2019} find that merging different stock market data sources, using early or intermediate fusion, improves the accuracy of stock market predictions, while \citeay{dang2018} also include textual data in their scope to effectively predict short-term stock trends. 
\citeay{wang2023} implement multimodal financial statement fraud detection with textual and financial data. Similarly, \citeay{stevenson2021} and \citeay{tavakoli2023} use multimodal models for credit risk prediction, demonstrating their practical application in supporting decisions within banking.


\subsection{Multimodal Document Analytics}
\label{sec:multimodal-document-analytics}
The evolution of document analytics has been significantly marked by the integration of multimodal data, extending the capabilities of traditional unimodal natural language understanding (NLU) models. The evolution of document analytics frameworks is depicted in Table \ref{tab:literature-methodology2}. 


\begin{table}[htbp]
  \centering
  \caption{Methodological Overview of Document Analytics Frameworks}
  \label{tab:literature-methodology2}
  \setlength\tabcolsep{14pt}
  \renewcommand{\arraystretch}{1.4}
  \small
    \begin{tabularx}{0.8\textwidth}{lccc}
    \toprule
    \textbf{Study} & \textbf{Presented Model} & \textbf{Multimodal} & \textbf{Cross-lingual} \\
    \midrule
    \addlinespace[1ex]
    \citeay{devlinBert} & mBERT & & X \\
    \citeay{conneauXLM} & XLM& & X \\
    \citeay{conneauXLMRoberta} & XLM-RoBERTa & & X \\
    \citeay{xuLayoutLMv1} & LayoutLM & X & \\
    \citeauthor{xuLayoutlmv2} (2021a) & LayoutLMv2 & X & \\
    \citeay{liSelfDoc} & SelfDoc & X & \\
    \citeay{chiInfoXLM} & InfoXLM & X & X \\
    \citeauthor{xu2021layoutxlm} (2021b) & LayoutXLM & X & X \\
    \citeay{liMarkupLM} & MarkupLM & X & \\
    \citeay{chenXDoc} & XDoc & X & \\
    \citeay{huang2022layoutlmv3} & LayoutLMv3& X & \\
    \citeay{liDIT} & DiT & X & (X)\\
    \bottomrule
    \end{tabularx}
\end{table}


Despite their proficiency in handling textual data, early NLU models such as mBERT \cite{devlinBert}, XLM \cite{conneauXLM}, and XLM-RoBERTa \cite{conneauXLMRoberta} are bound by their unimodal structure, making them insufficient for handling multimodal data. The introduction of LayoutLM \cite{xuLayoutLMv1}, which jointly analyzes text and layout, marks a shift towards multimodal document analysis. Building on this development, LayoutLMv2 \cite{xuLayoutlmv2} offers a refined approach and significantly boosts the capabilities of visually-rich document understanding (VrDU).
In parallel, SelfDoc \cite{liSelfDoc} employs a self-supervised pre-training framework, while MarkupLM \cite{liMarkupLM} presents a pre-trained model to analyze text and markup information from websites. However, the scope of research on multimodal models is mostly limited to English, with some exceptions like the DiT model \cite{liDIT}, which is also trained on Chinese documents.

To address a more global applicability, the cross-lingual pre-training model, InfoXLM \cite{chiInfoXLM}, is introduced to fill this gap. This advancement becomes instrumental in the development of LayoutXLM \cite{xu2021layoutxlm}, a multimodal cross-lingual document analytics foundation model that enables VrDU across 53 different languages. 

\subsection{LayoutXLM}
LayoutXLM uniquely combines the benefits of multimodal learning and cross-lingual understanding.
Its approach positions it as a leading model in this field, making it a cornerstone in our study \cite{xu2021layoutxlm}.


\subsubsection{Architecture}
LayoutXLM adopts the transformer-based architecture of LayoutLMv2 \cite{xuLayoutlmv2}, which takes inputs from three different modalities - text, layout, and images. These inputs are encoded using text embedding, layout embedding, and visual embedding layers. 

Embeddings are essential to deep learning models as they transform data into structured representations that can express complex syntactic and semantic information. They provide a solution to high dimensionality and sparsity of raw data by transforming it into low-dimensional dense vectors, which facilitates efficient computation and storage \cite{mikolov2013efficient, yihung2023}.



% Figure environment removed

Figure \ref{Fig:LayoutXLM} depicts the overall architecture of LayoutXLM \cite{xuLayoutlmv2, xu2021layoutxlm} in a token classification setup. The individual components are scrutinized in this section.


\paragraph{Text Embedding}
Essentially, each token's text embedding is derived from its own identity (the token itself), its position in the sequence, and its source segment.
To obtain the text embedding, the document's text is first tokenized into a sequence. This sequence begins with a [CLS] token, ends with a [SEP] token for each text segment, and is padded with [PAD] tokens to reach the maximum sequence length (L). Now, the $i$-th ($0 \leq i \le L$) text embedding is the sum of the following three components:

\begin{enumerate}
  \item The token embedding $TokEmb(w_i)$, which represents the token itself.
  \item The 1D positional embedding $PosEmb1D(i)$, which signifies the position of the token within the sequence.
  \item The segment embedding $SegEmb(s_i)$, which indicates the segment of the text the token originates from.
\end{enumerate}

Thus, the $i$-th text embedding is represented as:
\[
t_i = \textrm{TokEmb}(w_i) + \textrm{PosEmb1D}(i) + \textrm{SegEmb}(s_i)
\]


\paragraph{Visual Embedding}

In the visual embedding step, each page image $I$ is resized to $224 \times 224$ and fed into a Convolutional Neural Network (CNN) to generate a fixed-length visual token embedding sequence. These embeddings are then unified in dimensionality using a linear projection $\textrm{Proj}(\textrm{VisTokEmb}(I)_i)$, ensuring consistency across different inputs. 
Since the CNN doesn't capture positional information, a shared 1D positional embedding $\textrm{PosEmb1D}(i)$, similar to the one used in the text embedding, is added to each visual token to provide positional information.  All visual tokens are grouped into the visual segment $C$. The final $i$-th visual embedding is expressed as:

\[
v_i = \textrm{Proj(VisTokEmb}(I)_i) + \textrm{PosEmb1D}(i) + \textrm{SegEmb}(C)
\]

where $0 \leq i < WH$. The dimensions $W$ and $H$ represent the size of the output feature map after processing the image through the CNN, giving us a total of $WH$ visual tokens. This representation combines the visual features, positional information, and segment assignment for each visual token.


\paragraph{Layout Embedding}
The layout embedding encodes the spatial layout information derived from the OCR results, which includes data about the bounding box (width, height, and corner coordinates) of each token. These coordinates are normalized and discretized to integers in the range [0, 1000]. 

For the $i$-th text or visual token (with $0 \leq i < WH + L$), denoted by its normalized bounding box $box_i = (x_{min}, x_{max}, y_{min}, y_{max}, width, height)$, the layout embedding layer combines these six bounding box features to construct a 2D positional (layout) embedding $l_i$, with
\[
l_i = \textrm{Concat}(\textrm{PosEmb2D}_x(x_{min}, x_{max}, width), \\
\textrm{PosEmb2D}_y(y_{min}, y_{max}, height))
\]

The layout embedding hence encapsulates the spatial arrangement or layout information of the document. Special tokens such as [CLS], [SEP], and [PAD] are associated with an empty bounding box $(0, 0, 0, 0, 0, 0)$.


\paragraph{Multimodal Encoder with Spatial-Aware Self-Attention Mechanism}

The three embeddings - text, visual, and layout - are then combined using an early fusion approach, where the information from all modalities is merged right from the start \cite{zhu2015, boulahia2021}. This combined multimodal input is fed into a transformer-based encoder, allowing the model to learn a joint representation where all modalities can influence each other during the encoding process.
The visual embeddings ${v_0, ..., v_{WH-1}}$ and text embeddings ${t_0, ..., t_{L-1}}$ are concatenated into a unified sequence denoted by $X = {v_0, ..., v_{WH-1}, t_0, ..., t_{L-1}}$. Spatial information is integrated into this sequence by adding the layout embeddings to each corresponding element, yielding the $i$-th (with $0 \leq i < WH + L$) first-layer input $x^{(0)}_i = X_i + l_i$.

%\[x^{(0)}_i = X_i + l_i\ \textrm{, with}\]
%\[X = {v_0, ..., v_{WH-1}, t_0, ..., t_{L-1}\]

The core of the LayoutXLM encoder follows the transformer architecture, consisting of a stack of multi-head self-attention layers followed by a feed-forward network. However, unlike the original self-attention mechanism, LayoutXLM uses a spatially-aware self-attention mechanism to incorporate explicit relative position information to efficiently model local invariance in the document layout \cite{xuLayoutlmv2}.

Through this unified approach of combining text, visual, and layout information, LayoutXLM efficiently captures the rich information present in document images, enabling powerful downstream applications such as document understanding and information extraction \cite{xu2021layoutxlm}.


\subsubsection{Pre-Training and Fine-Tuning}
The pre-training objectives of LayoutXLM are adapted from the LayoutLMv2 framework, which has demonstrated efficacy in VrDU.
LayoutXLM undergoes pretraining with three tasks, Multilingual Masked Visual-Language Modeling, Text-Image Alignment, and Text-Image Matching, which together strengthen the model's understanding of language, visuals, and spatial relationships in documents \cite{xuLayoutlmv2, xu2021layoutxlm}.

The pre-training tasks enable LayoutXLM to learn an understanding of documents at a semantic and visual level, as well as layout and spatial information. The extensive pre-training process allows LayoutXLM to use transfer learning, where the pre-trained model can be effectively tuned for specific tasks even with smaller labeled datasets. This adaptability makes LayoutXLM a versatile foundation model for document analytics that can be employed in a wide range of downstream tasks. Its value is particularly pronounced in the banking sector, where document analysis is a key success factor. Integrating LayoutXLM into the workflow of a bank can enhance its processes and drive efficiency and decision-making.


