\section{Experimental Design}
\label{sec:experimental}

In this section, we explain our experimental framework. We start with the motivation for our chosen use case, which involves the extraction of information from German company register extracts. These documents present textual data within a tabular structure, often incorporating historical information indicated by red text or underlining. Hence, the chosen document type might benefit from a model that interprets not only textual data but also visual cues. The use of German data introduces an added level of complexity, as it requires a model with foreign language capabilities, reflecting the common needs in global banking processes.


One particular challenge associated with Optical Character Recognition (OCR) on tabular data is its row-by-row reading pattern. Although the semantic token structure might be column-based in tabular data, tokens are read sequentially across the rows, which could potentially misalign the semantic interpretation when processing scans because the reading direction crosses the table columns.

The aim is to classify each word token into one of the following classes: company name, legal form, headquarters, capital number, capital currency, director (geschaeftsfuehrer), authorized officer (prokurist), limited partner (kommanditist), and shareholder (gesellschafter). Tokens that do not fit into any of these categories are referred to as ``other", resulting in a total of 10 classes. A noteworthy aspect of this token classification task is the significant class imbalance, with about 95\% of tokens falling into the ``other" category. This presents a further challenge for the model's learning and performance optimization.

% Figure environment removed

To illustrate the typical structure of German company register extracts, an example is shown in Figure \ref{Fig:cr-labels}. The red underlinings mark historic information and the blue boxes indicate the token labels (only tokens that do not belong to the ``other" class are visualized). 

We proceed to describe our data collection and processing, detail the model selection, and outline the evaluation and benchmarking approaches.

\paragraph{Data Collection and Processing}
We collect public German company register extracts of over 500 companies across different industries, which generates a total data set of 2441 PDF pages. Additionally, we use company metadata XML files to derive the labeling information in a structured format. We then iterate over the OCR document tokens and perform a metadata-based token tagging process. This matching process is refined by analyzing the color of the text or its underlining and removing any entity labels from historical information (indicated by red highlighting). Finally, we exclude empty pages and retain only those pages containing at least three token labels with relevant company entity information, resulting in a dataset of 1503 pages.


\paragraph{Model Fine-Tuning and Benchmarking}

The pre-trained LayoutXLM token classification model implementation from Huggingface is chosen for this task.
Additional training epochs fine-tune this model for the specific task of information extraction from company register extracts.

The evaluation relies on a hold-out validation set that comprises 30\% of the data, or approximately 480 pages. This set is used in all ablation studies and comparisons, ensuring consistent and reliable comparisons. The performance of LayoutXLM is benchmarked against the BERT multilingual model, with the F1 score employed as the performance metric. 
