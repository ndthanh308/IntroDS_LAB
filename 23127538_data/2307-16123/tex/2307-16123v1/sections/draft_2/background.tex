\section{Background}
\label{sec:background}

%\hoda{A general comment: There are a lot of subsections in each section, and the paper seems too crowded. Wherever possible, please use (paragraph) instead of (subsection), or merge two subsections together if it is applicable.}

We develop our attacks on an Intel-based CPU-GPU system since Intel iGPUs are the most common GPUs with the largest market share~\cite{gpu-market-share}; given that every Intel-based CPU in desktop PCs, workstations, or laptops is equipped with an iGPU. In this section, we introduce Intel's integrated graphics architecture and provide the necessary background for our attacks.

\subsection{Intel Integrated Graphics Architecture}

% Figure environment removed

% Figure environment removed

In some Intel processors, multiple CPU cores are tightly coupled with GPU on the same chip. Such components in addition to other components such as MC, Last level cache (LLC) slices, and system agent are connected via ring interconnect as we show in Figure \ref{fig:intel_soc}.

%Intel processor graphics or integrated GPU (iGPU) provides graphics, compute, media, and display capabilities for many of Intel’s processor SoC products. The parallel architecture of iGPU offers high-performance computation for a wide range of graphics and general-purpose computing workloads. The tight integration of CPU cores with iGPU, and the shared memory subsystem (including LLC) provides efficient data transfers across CPU and iGPU.

Intel iGPU consists of a number of slices. Figure~\ref{fig:igpu-arch} demonstrates the internal architecture of a slice in iGPU. Each slice includes a number of subslices and each subslice contains a number of Execution Units (EUs), each capable of executing seven hardware threads simultaneously. %This modular design enables a variety of products in each generation. 
%Different generations of iGPU have a variant number of slices, sub-slices, and EUs~\cite{gen7.5, gen9, gen11}. 
Gen9 and Gen9.5 iGPUs contain one slice with three subslices, each with eight EUs (a total of 24 EUs in the slice)~\cite{gen7.5, gen9, gen11}. From the memory hierarchy aspect, each subslice includes read-only L1 and L2 sampler caches that are used only for the graphics stack. EU also includes a data port which is a load and store unit. 
All subslices share an L3 cache which is found to be non-inclusive \cite{leaky_buddies, macsim_ispass}. %L3 cache is used for both graphics and compute stack of iGPU. 
%A part of the L3 cache is dedicated to shared local memory (SLM) which is shared across all EUs in each subslice and is private to a group (called work-group) of threads (called work-items) that is assigned to that subslice.
Intel iGPU shares the LLC with CPU cores as well.

\begin{wrapfigure}[9]{r}{0.26\textwidth}
\vspace{-0.5cm}
    % Figure removed
    \caption{Memory Coalescing.%\hoda{can remove this figure to save space}
    }
    \vspace{-2mm}
    \label{fig:mem_coal}
\end{wrapfigure}%

Memory coalescing is one of the approaches used to reduce the memory bandwidth consumption of SIMD%(Single-Instruction Multiple-Data)
-based workloads. With memory coalescing, memory accesses from threads within the same warp or wavefront (i.e. threads executing the same instruction) which are targeting the same cache line are merged as a single memory request. With such an approach, in addition to improving the performance, it hides memory accesses of different threads within a wavefront due to merging. Figure \ref{fig:mem_coal} explains the process of memory coalescing.

\subsection{Execution Model and Kernel Life Cycle}
We use OpenCL (a cross-platform API) to program iGPU kernels~\cite{opencl}. We next explain about OpenCL execution model and iGPU kernel life cycle.

%\subsubsection{OpenCL Execution Model}
%Massively parallel GPUs accelerate both graphics and general-purpose computations.
%In the computational stack, the programming languages are OpenCL (a cross-platform API)~\cite{opencl}. %and CUDA (for Nvidia-based GPUs)~\cite{cuda}.
GPUs operate in SIMD mode, in which a group of work-items (threads) executes the same instruction on multiple data in parallel. Each GPU application consists of some GPU kernels. Each kernel launches a group of threads called work-group in OpenCL terminology. Work-groups are assigned to subslices in a round-robin fashion. Each work-group is broken into sub-work groups (8, 16 or 32 work-items), called wavefront.%The local thread dispatcher in each subslice selects the ready wavefronts and issues an instruction for all 16 or 32 threads of that wavefront to EUs in a given clock cycle.


%\subsubsection{iGPU Kernel Life Cycle}
The process of executing an iGPU kernel starts from a CPU process. Using OpenCL API, a CPU process enqueues a command in the command queue (CQ) to execute the targeted iGPU kernel. Additionally, buffers to be copied to kernel memory address space are also enqueued in CQ accordingly. At the end of kernel execution, buffers required by the CPU process (i.e. execution result of the iGPU kernel) are enqueued in CQ.

\begin{comment}
\subsubsection{Memory Coalescing}
\hoda{It's better to move it to the previous subsection or the next one, somehow unrelated to programming model, but related to architecture. Since in previous section we sill did not explain wavefronts,... maybe the next one is better. So probably having a title like "memory management" and a subsection for coalescing, another for cache??}
Memory coalescing is one of the approaches used to reduce the memory bandwidth consumption of SIMD-based workloads. With memory coalescing, memory accesses from threads within the same wavefront (or warp) which are targeting the same cache line are merged as a single memory request. With such an approach, in addition to improving the performance, it hides memory accesses of different threads within a wavefront due to merging. Figure \ref{fig:mem_coal} explains the process of memory coalescing.
\end{comment}

\subsection{LLC Write-backs and Write Buffer Management Policies}
%Memory traffic can be either due to memory read or write requests. 
There are multiple approaches that govern the process of writing cache lines in non-blocking caches to main memory due to writes. All dirty cache lines of LLC are added to a write buffer and are eventually written to memory. %Common approaches used to update memory due to dirty cache lines are write-through or write-back. In the write-through scenario, once dirty cache lines are pushed to the write buffer, they are sent to the main memory without waiting for other events like a full buffer or the existence of read requests. 
In the write-back scenario, dirty cache lines are grouped and written to memory based on specific events. In this case, a write-back buffer is required which holds dirty cache lines evicted from LLC to be added to MC write buffer.

There are different proposed policies to manage write buffers in memory controller. Such management policies are like \textit{drain-when-full} which empty the write buffer when it is full and stall serving memory reads upon this~\cite{lee_dram, staged_reads}. Another management policy gives priority to reads, such that when the read buffer has memory requests, it stalls serving memory write requests until memory reads are served.




%\subsection{DRAM and Memory Controller Architecture}


\begin{comment}
\subsection{SIMD-based Integrated Accelerators}
%TODO: remove this sub section to the next section, and add programming model sub section (workitems, groups, threads)
Processor chips nowadays include not just CPU cores but also integrated accelerators. Such accelerators could be GPU-based(Graphics Processing Unit), NPU-based (Neural Processing Uni), FPGA-based accelerators, and others \cite{amd_apu1, arm_npu1, intel_fpga1}. Intel processor chips integrate accelerators called Intel Integrated Graphics, AMD refer to its processor chips with integrated GPU as APUs (Accelerated processing Unit).

With the proliferation of graphics and deep learning based applications, processor chips are not architecturally and micro-architecturally adequate for executing them. Considering large data sizes and parallel characteristics of such applications, it is performance efficient to execute them on specialized accelerators like SIMD-based (Single Instruction Multiple Data) accelerators. With the large number of cores, compute units or execution units in integrated accelerators, performance of these applications are more efficient than executing them on CPU cores \cite{Aamodt2018}. 



\subsection{Intel SoC Architecture}

Figure~\ref{fig:intel_soc} shows the overall architecture of an Intel-based SoC. Mulitple CPU cores and an iGPU are tightly integrated on the same die and connected to the ring interconnect, a bi-directional ring that has a 32-byte wide data bus. CPU cores and iGPU also share the Last-Level Cache (LLC) and the system agent that bundles the DRAM memory controller, display controller, and other off-chip I/O controllers such as PCI Express. All off-chip system memory transactions to/from CPU cores and to/from iGPU are facilitated by ring interconnect, through the system agent, and the unified DRAM memory controller~\cite{gen9}.


% Figure environment removed



\subsection{Intel Integrated Graphics Architecture}

Intel processor graphics or integrated GPU provides graphics, compute, media, and display capabilities for many of Intel’s processor SoC products. The parallel architecture of iGPU offers high-performance computation for a wide range of graphics and general-purpose computing workloads. The tight integration of CPU cores with iGPU and as a result, the shared memory subsystem (including LLC) provides efficient data transfers across CPU and iGPU.

Figure~\ref{fig:igpu-arch} demonstrates the internal architecture of an Intel iGPU which consists of a number of slices, each slice includes a number of subslices. Each subslice contains a number of Execution Units (EUs), each capable of executing seven hardware threads simultaneously. This modular design enables a variety of products in each generation. Different generations of iGPU have a variant number of slices, sub-slices, and EUs~\cite{gen7.5, gen9, gen11}. For example, a Gen9 iGPU contains 1 slice of 3 subslices, each with 8 EUs (a total of 24 EUs in the slice). 

From the memory hierarchy aspect, each subslice includes read-only L1 and L2 sampler caches that are used only for the graphics stack. EU also includes a data port which is a load and store unit. All subslices share an L3 cache which is found to be non-inclusive \cite{leaky_buddies, macsim_ispass}. L3 cache is used for both graphics and compute stack of iGPU. A part of the L3 cache is dedicated to shared local memory (SLM) which is shared across all EUs in each subslice and is private to a group (called work-group) of threads (called work-items) that is assigned to that subslice. Intel iGPU shares the LLC with CPU cores.


%, the ring interconnect in Intel based processors is shared between CPU cores and integrated graphics as well \cite{leaky_buddies, macsim_ispass, lotr}.

%Current Intel iGPUs allow the execution of only one compute process (kernel) at a time, in addition to graphics. Multiple compute kernel execution is not supported in the current Intel iGPUs. 


% Figure environment removed


\end{comment}
%Some of Intel processors include integrated accelerators referred as Intel Integrated Graphics \cite{gen7.5, gen9, gen11}. Such accelerator is used to execute parallel applications and graphics as well. Figure \ref{fig:insert_figure0} explains the high level architecture of Intel integrated graphics gen9.5. Current Intel integrated graphics allow execution of one kernel at a time in addition to graphics. Multiple kernels cannot be executed on current Intel integrated graphics 

%Intel integrated graphics include a set of slices, and each slice contains a set of sub-slices. Sub-slices include a set of execution units (EU). EUs are capable of executing seven hardware threads simultaneously. For example, Gen9.5 include one slice with three sub-slices, this mean that this accelerator is capable of executing 168 threads simultaneously with consideration to micro-architectural support.

%The cache architecture of Intel integrated graphics is depicted in Figure \ref{fig:insert_figure0}. 



% \subsection{OpenCL Programming Model}

% Massively parallel GPUs accelerate both graphics and general-purpose computations. In the computational stack, the programming languages are OpenCL (a cross-platform API)~\cite{opencl} and CUDA (for Nvidia-based GPUs)~\cite{cuda}.

% GPUs operate in Single-Instruction Multiple-Data (SIMD) mode, in which a group of threads executes the same instruction on multiple data in each clock cycle (all in parallel). Each GPU application consists of some GPU kernels. Each kernel launches a group of threads called work-group in OpenCL terminology (note that thread is also called work-item in OpenCL terminology). Work-groups are assigned to subslices in a round-robin fashion. Each work-group is broken into SIMD-width of threads (16 or 32 threads, called wavefront). The local thread dispatcher in each subslice selects the ready wavefronts and issues an instruction for all 16 or 32 threads of that wavefront to EUs in a given clock cycle.

\begin{comment}

\subsection {Memory Controller}

In heterogeneous SoCs, CPUs and accelerators like GPUs share the main memory and memory controllers. In this paper, we build contention-based covert and side channel attacks on the shared memory controller across CPU cores and iGPU in Intel-based SoCs. Figure~\ref{fig:MC-arch} shows the architecture of a DRAM controller. There is a transaction queue that buffers all memory requests from CPU cores and iGPU and routes them to different DRAM banks for reads or writes. For each memory bank, a request buffer stores all read/write requests of that bank and these requests are scheduled using bank schedulers in the memory controller and are sent to DRAM banks. The memory controller is responsible for scheduling memory requests. These scheduling algorithms influence the performance of applications. One of the common memory scheduling algorithms is First Ready - First Come First Serve (FR-FCFS) \cite{mem_sys_book, rixner_isca}.

%\hoda{add more details about scheduling , ... in MC from reference books, papers, ...}




All structures within the memory controller are shared between processors on the chip. %Our attack exploits such sharing to leak information via covert and side channel attacks across processors (CPU and iGPU in our target system). 
Our attack is the first work to exploit contention on the memory controller caused by iGPU in a heterogeneous SoC to leak secret information via covert and side channels.





%\hoda{We repeat it in next subsection, we can remove it}
%The importance of our work stems from the proliferation of heterogeneous SoCs in which accelerators share the memory controller with CPU cores. Due to parallel data processing by accelerators like iGPU, contention on the memory controller is inevitable. This is the first work to explore exploiting contention caused by iGPU to leak secret information via covert and side channels and by targeting the shared memory controller. Exiting work \cite{cornell_defense_hpca} which aimed to cause contention on memory controller utilized CPU cores. As we demonstrate in Section~\ref{sec:motivation}, the contention level which can be caused by iGPU is much higher than the contention caused by CPU cores on a shared memory controller. Consequently, we build attacks with low error and high bit rates. 

%\hoda{not sure to introduce challenges here. we should mention in introduction and the corresponding sections. we can revise it after all sections are finalized.}
%There is a number of challenges when it comes to exploiting iGPU to perform covert and side channel contention-based attacks using memory controller. Such challenges include synchronization between CPU core and iGPU, number of parallel memory accesses to cause contention which is notable by the iGPU and sharing of other resources such as ring interconnect and LLC between CPU core and iGPU. Because our attack is mainly exploiting memory writes to cause contention, it is not known when memory writes will be written back to main memory. This introduces a challenge when associating kernel execution behavior with contention on memory controller due to memory writes.

%\hoda{add more details here why it is important and unique compared with prior  work}
% Figure environment removed


%\ghadeer{Add information related to iGPU kernel life time.}









%Most of integrated accelerators share the same memory controller and main memory \cite{gen7.5, gen9, gen11, arm_npu1, amd_apu1}. In fact, our attack exploits such sharing to leak information via covert and side channel attacks. In most cases, the memory controller include a queue referred as transaction queue used to buffer memory requests from CPU cores to deliver it later to main memory for reads and writes \cite{mem_sys_book}. There is alot of work proposing defenses against contention attacks targeting this shared transaction queue, but there is not any work showing this contention-based attacks targeting transaction queue within memory controller \cite{cornell_defense_hpca, utah_defense_micro, mit_defense_asplos, princeton_defense_hpca, utah_defense_hasp}.

%Transaction queue could be shared with integrated accelerators to serve memory requests from integrated GPU \cite{sms}. Such sharing can affect the performance of applications running on CPU cores or the kernel running on the integrated GPU. Performance degradation would depend on multiple factors such as memory scheduling policy, transaction queue size, cache sizes, number of memory read and write requests, number of threads, etc...

%\hoda{Add a figure showing the internal architecture of MC. Bank buffer here is not introduced.}Bank buffers are one of the shared resources that could be affected by integrated GPU memory traffic. Previous work shows how bank contention can be used to leak secret information but this work is targeting workloads running on CPU only \cite{drama_usenix}. Considering the large number of memory requests from the integrated GPU during a specific time slot, such traffic can tremendously degrade the performance of CPU applications if banks are shared between CPU and integrated GPU applications.
\end{comment}
\begin{comment}
\subsection {Contention-Based Microarchitectural Attacks}

Microarchitectural covert and side channel attacks have been widely studied in CPU-based systems, using several techniques and on different microarchitectural structures including different levels of cache~\cite{percival-05, mehmet-2016, liu-15, gruss-DIMVA-2016, Yarom-2014, fan_hpca2018}, random number generators~\cite{evtyushkin-16-random, crosstalk}, and branch predictors~\cite{branchscope, evtyushkin-16-branch}. In such scenarios, the spy process actively brings the shared microarchitectural resource (e.g. cache or branch predictor) into a known state, lets the trojan process (in a covert channel) or victim (in a side channel) execute, and then checks the state of the shared resource to decode the transferred secret (covert channel) or learn secrets about the victim’s execution (side channel).

Contention-based attacks have been also developed on a variety of resources including on-chip~\cite{Paccagnella-2021, Wan-2021} or off-chip~\cite{Tan-2021} interconnects in multi-core CPUs. In such a scenario, contention arises when the two components share bandwidth or capacity-limited microarchitectural structures such as buses, ports, or buffers/queues. When the two processes running on the two components access the same structure concurrently, a measurable contention can be achieved (by observing slowdowns). In these attacks, the spy passively monitors the latency to access the shared resource and uses variations in this latency to decode the transferred secret (covert channel) or infer secrets about the victim’s execution (side channel).

In a heterogeneous SoC with a shared memory subsystem, contention-based channels can be built across the CPU and accelerators. Accelerators can create a higher level of contention at a higher rate than a CPU core, mainly due to the SIMD architecture and parallel accesses to the memory hierarchy which generates a high volume of memory traffic during a short time slot.

In the memory hierarchy, several resources can serve as a leakage structure for contention-based covert and side channel attacks, this includes buses or interconnects and DRAM controller request queues (the transaction queue and bank queues). A recent work~\cite{leaky_buddies} implements a contention-based covert channel on ring interconnect in Intel-based CPU-GPU systems.  In this paper, we target the memory controller, specifically the shared request queues within the memory controller to develop contention-based channels between the CPU and iGPU. 


 %there is no prior work that characterizes the contention in the memory controller and shows a practical attack targeting the transaction queue and bank queues within the memory controller.



%\hoda{update this paragraph based on ghadeer's response in related work}
%Several works~\cite{cornell_defense_hpca, utah_defense_micro, mit_defense_asplos, princeton_defense_hpca, utah_defense_hasp} proposed defense mechanisms against memory contention-based attacks in multi-core CPUs. 
%Most of these works target multi-core CPUs. 



Although several prior works point out the memory contention-based attacks in multi-core CPUs~\cite{utah_defense_micro, utah_defense_hasp, cornell_defense_hpca, duplicon_micro2018, mit_defense_asplos, princeton_defense_hpca} and propose defense mechanisms against such attacks, none of these works implement a practical attack, specifically on memory controller resources such as request queues. To the best of our knowledge, our paper is the first work to characterize the contention on the memory controller resources (for both memory reads and writes) and develops a practical contention-based attack on the memory controller, by exploiting parallel memory writes. We build this attack across the processors (CPU and accelerator), showing that the contention that arises from the massively parallel memory accesses of a SIMD accelerator significantly impacts both the performance and security of modern heterogeneous SoCs.
\end{comment}


\begin{comment}
There is a lot of work in the field proposing attacks and defense targeting contention on shared resources between victims and attackers. Shared resources which were shown to be prone to contention are caches, memory controller queues, banks, ring interconnect, shared buffers and queues with in CPU core in case of multi-threaded applications \cite{paper with shared resoruces contention}. When it comes to memory related contention-based attacks, memory controllers and DRAM buffers can be resources of contention used to leak secret information \cite{drama_usenix, cite other papers.}.

Accelerators could cause higher degree of contention than CPU applications and this is mainly due to parallel access to data. Previous work shows how integrated GPUs can be exploited for side and covert channel attacks and showed how contention at the ring interconnect can leak secret information \cite{leaky_buddies}. Also, existing research shows that contention attacks on discrete accelerators are possible and can be powerful \cite{gpu_survey}.

In systems with integrated accelerators, multiple resources are prone to contention. Such resources are caches, bus interconnect, memory controller queues and buffers and main memory queues. With parallel processing of data, contention on these resources would be occur faster due to high volume of traffic during a time slot. 

Contention on memory related resources such as memory controller or main memory resources can happen due to read and writes memory traffic. Memory reads and dependent on application execution behavior, meaning when application needs its data it reads from main memory. Consequently, it is straightforward to know what is the application is executing based on read memory traffic.

On the other hand, memory writes are challenging from the perspective of memory controller or main memory resources. When an application is updated targeted data, it is not known when the data will be written back to main memory. It may be challenging to relate memory write traffic with application execution behavior. 

Our work is the first work to show memory controller contention-based attacks practically and with exploiting writes to do so not reads. Exploiting memory writes does not require us to ensure that targeted data is flushed before accessing them. More details about exploiting such scenario in the following sections.

\end{comment}
%\subsection{Role of Memory Reads and Writes in Contention Level}
%talk about ways to exploit memory writes vs memory reads, advantages and disadvantages
