\section{Reverse Engineering: Source of CPU Process's Read Latency Overhead}
\label{sec:cont_src}

CPU cores and Intel iGPU share microarchitectural components other than MC, such as Ring interconnect, LLC, and DRAM resources. Consequently, it is possible for such resources to be the reason for the slowdown observed by the process running on the CPU core during iGPU kernel memory writes. In this section, we investigate the source of slowdown due to iGPU kernel memory writes and confirm that it is due to the management policy of the write buffer in the shared MC. 

In all of our experiments in this section, the CPU process is reading from a buffer of size 128KB except for the experiment for LLC hits (in Figure~\ref{fig:llc_cont_hits}) where the buffer size is 512KB (double the size of L2 cache). We choose a 128KB buffer size to simplify the process of reverse engineering by reducing the set of read addresses from the CPU process (2048 accesses). While a 512KB buffer is used to ensure that most of the accesses from the CPU process are LLC hits.
\subsection{Ring Interconnect and Last Level Cache (LLC)}%
In our targeted system, LLC and ring interconnect are shared between CPU and iGPU. These two components could be the cause of the slowdown the CPU process is observing. We show that neither LLC nor ring interconnect is the source of contention observed in Figure~\ref{fig:llc_cont}.

We explored the slowdown due to iGPU kernel memory write traffic when CPU process accesses are LLC hits vs. LLC misses. Figure~\ref{fig:llc_cont_hits} shows the normalized latency of CPU process during iGPU kernel writes when buffer accesses are LLC hits. Note that in Figure~\ref{fig:llc_cont_hits} the baseline latency is CPU process average latency when iGPU kernel is not issuing any memory write requests. %The baseline average latency in this case is equal to LLC hit latency. 
Figure~\ref{fig:llc_cont_misses} shows the normalized latency of the CPU process during iGPU kernel writes when buffer accesses are LLC misses. The baseline for Figure~\ref{fig:llc_cont_misses} is CPU process average latency when the iGPU kernel is not issuing memory write requests. %The baseline average latency in this case is equal to LLC miss latency.

It can be noted from Figure~\ref{fig:llc_cont}, that the CPU process is suffering a higher level of slowdown in case of LLC misses ($\times5$ baseline LLC miss latency) compared to LLC hits ($\times2$ baseline LLC hit latency). This proves that neither LLC nor ring-interconnect is the source of slowdown. The higher latency starts to appear when the number of iGPU kernel memory requests is $2^{19}$ write requests, and iGPU kernel buffer size is 32MB.

\begin{tcolorbox}[arc=5mm, outer arc=1mm, width=\linewidth,halign=flush center, left=1mm, right=1mm]

\textit{\textbf{LLC and Ring Interconnect are not the cause of CPU process's slowdown.} During iGPU kernel execution, the normalized latency of the co-running CPU process when its accesses are LLC hits is much lower than the normalized latency when its accesses are LLC misses. 
}
\end{tcolorbox}

%\hoda{The second sentence is somehow confusing. They may think that it is the access time of the cache (hit vs. miss), especially with the numbers. The above paragraph clearly shows that access times are normalized to miss latency or hit latency so the slowdown is compared to these baselines. But here it is not clear, and if the reviewers just read these frames it is misleading. I would say something like this. If it makes sense to you, let's edit it: "When the accesses from the co-running CPU are LLC hits, the slowdown (access time/hit time) is much lower than the slowdown in case of LLC misses (access time/miss time)"} 
%\ghadeer{the case is NOT (access time / hit time), the case is (hit time (during iGPU memory write requests) / hit time (when iGPU is not issuing memory write requests. Also it is NOT (access time/miss time) it is (miss time during iGPU kernel memory requests / miss time when there is no iGPU kernel memory requests. the purpose of the red line in the figure is to indicate that during iGPU kernel execution almost 100\% of CPU buffer access are misses or hits. This is to answer the question, how did you know that the CPU process during iGPU kernel execution is hit or miss in LLC and by what percentage? when i say normalized to LLC hit latency or normalized to LLC miss latency. I'm indicating the baseline which is the CPU process latency when there is no iGPU memory write requests which is equal to LLC hit latency in the first case and LLC miss latency in the second case.  I updated the y axis and caption in addition to information in text box let me know if it still not clear}
\subsubsection{Writeback Buffer of LLC}
Writeback buffer of LLC stores dirty cache lines to be up- dated in main memory. Due to the high rate of memory write requests by the iGPU kernel, writeback buffer of LLC will rapidly get filled. We showed in Figure~\ref{fig:llc_cont_hits} that the latency of the co-running CPU process when buffer accesses are LLC hits is $\times2$ baseline LLC hit latency. This is about $\times12$-$\times15$ smaller than the latency when CPU process buffer accesses are LLC misses. This indicates that LLC does not get blocked from serving CPU process read requests even though the number of write memory requests is the same in Figure~\ref{fig:llc_cont_hits} and Figure~\ref{fig:llc_cont_misses}. Also, the LLC writeback buffer is not on the critical path of process's read requests whether they are hit or miss in LLC as long as they are not dependent on write requests.


\begin{tcolorbox}[arc=5mm, outer arc=1mm, width=\linewidth, halign=flush center, left=1mm, right=1mm]
\textit{\textbf{Writeback buffer of LLC is not the source of CPU process's slowdown}. This is because the Writeback buffer is not on the critical path of memory read requests which are not dependent on writes.}
\end{tcolorbox}



\begin{table}[t]
\centering
    \begin{tabular}{||m {0.33\columnwidth}|m{0.56\columnwidth}||}
        \hline
        Memory Controller & Dual channel \\
        \hline
        DRAM & DDR4 MR[ABC]4U320GJJM16G @ 2600MT/s\\        
        \hline
        Memory Capacity & 32GB (2-16GB DIMMs) \\
        \hline
        Ranks & Single Rank \\
        \hline
        Number of Bank groups and Banks & 4 bank groups, 4 banks/bank group \\
        \hline
        Channel Addressing & $b_8\oplus b_9\oplus b_{12}\oplus b_{13}\oplus b_{15} \oplus b_{16}$\\
        \hline
        Bank Group Addressing & BG0: $b_7\oplus b_{14}$ \newline BG1: $b_{15}\oplus b_{18}$\\
        \hline
        Bank Addressing & BA0: $b_{16}\oplus b_{19}$ \newline BA1: $b_{17}\oplus b_{20}$ \\
        \hline
    \end{tabular}
\caption{Targeted DRAM details and reversed engineered channel, bank group and bank addressing.}
\label{tab:dram_re}
\vspace{-2mm}
\end{table}


\subsection{Memory Controller and DRAM Resources}


% Figure environment removed%
%\vspace{-6mm}
%% Figure environment removed



CPU cores and iGPU also share MC and main memory (DRAM) resources. We investigate whether shared resources such as channels, bank groups, or banks contribute to the high latency level observed by the co-running CPU process during iGPU kernel memory write requests.

We reversed engineered bits in the physical address which indicate the rank, channel, bank group, and bank to access based on DRAMA paper ~\cite{drama_usenix}. Table~\ref{tab:dram_re} shows reverse engineering results of the channel, bank group, and bank addressing used to determine which DRAM resource is used based on the physical address. The table also shows our targeted DRAM details.

To study the impact of the channel, bank group, or bank contention on the observed high latency, we launched two experiments (A) and (B). In both experiments, the CPU process and iGPU kernel are doing the same number of memory reads and writes; memory reads in the case of the CPU process and memory writes in the case of the iGPU kernel. The difference is that we allocated a larger buffer in experiment (B) which is accessed at a stride of one cache line.% We will show later that dirty cache line eviction plays a role in increasing CPU process latency.

Figure~\ref{fig:dram_cont_a} and Figure~\ref{fig:dram_cont_b} depict CPU process read latency distribution and channel, bank group, and bank contention cases distribution. A contention case happens when CPU process read address uses the same channel, same bank group or same bank as iGPU kernel memory write addresses based on physical addresses. We infer if a channel, bank group, or bank contention case had happened based on reverse-engineered addressing using physical address bits in Table \ref{tab:dram_re}.

In these experiments, the total number of read requests by the CPU process is 2048 requests and the total number of write requests for the iGPU kernel is $2^{18}$ memory requests. In Figures~\ref{fig:channel_a}, ~\ref{fig:channel_b}, ~\ref{fig:bankgroup_a}, ~\ref{fig:bankgroup_b}, ~\ref{fig:bank_a} and ~\ref{fig:bank_b}, the x-axis indicates the number of contention cases observed by each CPU process read access due to iGPU kernel memory writes. The y-axis indicates the count of these contention cases. Note that the total of contention cases count (y-axis) is 2048 which is equal to CPU process memory accesses.


%\hoda{Before going to explain figures, first can you please elaborate on channel/bankgroup/bank contention cases (X-axis) more in the text? Explain what do you mean by .. contention cases? I expect this part to be very hard to follow for reviewers.}
%\ghadeer{addressed}.



Figure~\ref{fig:channel_a} and Figure~\ref{fig:channel_b} show the distribution of channel contention cases observed by the CPU process due to iGPU kernel memory writes. All of the CPU process memory reads observed the same number of channel contention cases equal to half of iGPU kernel write requests in both experiments ($2^{17}$). This is because based on our reverse engineering results, we found that about half of the iGPU kernel buffer was allocated on the first channel and the other half to the second channel. The same scenario is for the CPU process buffer. We conclude that channel contention is not the reason behind the high latency observed by the CPU process since channel contention level is the same in experiments (A) and (B).

Figure~\ref{fig:bankgroup_a} and Figure~\ref{fig:bankgroup_b} depict the distribution of bank group contention cases observed by the CPU process due to iGPU kernel memory write requests. In Figure~\ref{fig:bankgroup_a}, about 1000 CPU process memory accesses observed 32200 contention cases, while the rest observed 33300 contention cases. The case for Figure~\ref{fig:bankgroup_b} is close; about 950 memory accesses suffered 32550 contention cases the rest suffered 33000 contention cases. Total bank group contention cases in both experiments are close and do not explain the large difference in latency distribution between experiments (A) and (B).

Furthermore, we investigate the difference in bank contention between experiments (A) and (B) as we show in Figure~\ref{fig:bank_a} and Figure~\ref{fig:bank_b}. In Figure~\ref{fig:bank_a}, about 50\% of CPU process accesses resulted in 8200 bank contention cases or lower. Most CPU process accesses suffered contention cases between 7800 and 8910 cases. While for the second experiment in Figure~\ref{fig:bank_b}, 50\% of CPU process accesses resulted in about 8000 bank contention cases or lower. Also, most CPU process accesses in this experiment suffered contention cases between 8000 and 8370 cases. The range of contention cases in experiment (B) is smaller than in experiment (A). From these results, we can conclude that bank contention is not the reason behind the huge latency difference between experiments (A) and (B) shown in Figure~\ref{fig:latency_a} and Figure~\ref{fig:latency_b}. 

\begin{tcolorbox}[arc=5mm, outer arc=1mm, width=\linewidth, halign=flush center, left=1mm, right=1mm]
\textit{Channel, Bank group, and Bank contention are not the cause of the high latency of co-running CPU process during iGPU kernel memory writes.}
\end{tcolorbox}


\subsection{Write and Read Buffers in Memory Controller}
Other resources which could increase the latency of memory reads performed by the CPU process are read and write buffers in the MC. The high latency observed by the co-running CPU process happens only in the case of iGPU kernel memory writes not memory reads. Also, we noticed that the slowdown experienced by the CPU process happens once during iGPU kernel kernel buffer accesses when sending single bit '1' and not periodically as we show in Figure~\ref{fig:latency_per_access}. The CPU process continue to suffer high memory access latency during iGPU kernel execution.%

Considering these circumstances, it is possible for the management policy of the write buffer in the MC to be the cause of the CPU process's higher read latency. A common management policy used with write buffer in MC is \textit{drain\_when\_full}. In this management policy, when the write buffer gets full, memory read requests are stalled until the write buffer is drained. The purpose of such a management policy is to avoid DRAM latency due to \textit{write after read} and \textit{read after write}. In fact, there is a lot of research addressing this performance issue which occurs when stalling memory read requests to serve write requests in multi-core CPU environment~\cite{staged_reads, lee_dram}.%

Due to continuous parallel write requests from iGPU, the probability of getting the write buffer full is high. Also, when the write buffer is being drained, there will be a number of standing write requests waiting to be added to the write buffer to be served. This explains the high latency observed by the CPU process when reading from the main memory.
%\vspace{-2.5mm}
\begin{tcolorbox}[arc=5mm, outer arc=1mm, width=\linewidth, halign=flush center, left=1mm, right=1mm]
\textit{Due to the write buffer management policy (\textit{drain\_when\_full}), memory read requests in the read buffer need to be stalled to serve memory write requests.}
\end{tcolorbox}

