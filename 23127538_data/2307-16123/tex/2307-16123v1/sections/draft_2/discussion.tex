\section{Discussion}
\label{sec:discuss}
\noindent\textbf{Characterizing Bit and Error Rates.} Our covert channel attack variants depend on memory writes. From Figure~\ref{fig:latency_per_access}, we can notice the high latency of spy reads due to iGPU kernel writes. Such high latency contributes to lowering the bit rate depending on the frequency of iGPU kernel writes and the number of affected spy read requests. It can also be the reason behind the low error rate because the difference between bit '0' and bit '1' latency is quite high.

\noindent\textbf{Attack Generalization.} We discuss the possibility of generalizing our covert channel attack to other integrated accelerators. Our attack mainly depends on frequently filling the write buffer in the MC to stall spy (CPU process) memory read requests. If the system is adopting management policies for write buffers such as \textit{drain\_when\_full} or if it delays memory read requests upon frequent filling of memory write buffer, then our attack is applicable to such systems. 
Not all integrated accelerators share the LLC with CPU cores like Intel iGPUs. Current AMD APUs do not share LLC with iGPU~\cite{cezanne}. In such a system, because the size of caches in iGPU is much smaller than LLC, we expect better attack performance. This is because it is possible to frequently fill the write buffer using a smaller buffer and random evictions. Moreover, if deterministic evictions are chosen, this will speed up the process of finding an eviction set.


\noindent\textbf{Degree of iGPU Parallelism.}
From Figure~\ref{fig:glob_loc_threads} we noticed that increasing the local work-group size lowered the error rate but increased the spy execution time. Also, an increasing number of local work-groups beyond two groups increases the error rate but lowers the spy execution time. In our targeted system, the maximum size of a local workgroup is 256 work-items. From such results, we observe that increasing the number of work-groups does not necessarily improve the attack performance. This is due to the performance overhead associated with creating and scheduling these work-groups. Furthermore, a larger number of local work-groups means a lower number of writes per thread, if all global threads are used. This will cause the time required for scheduling to dominate, and thus reducing the frequency of writes which will cause the error rate to increase. Consequently, there has to be a consideration of the number of available resources such as the number of EU (Compute Units) and created threads.


%\hoda{The following paragraph is a minor issue to discuss, we can remove it to save space, if needed.}
% \noindent\textbf{Parallel Memory Writes using Multi-Core CPUs.}
% We argue that it is possible to exploit multiple CPU cores instead of iGPU to issue memory write requests. Such an approach involves a lot of challenges and does not result in better attack performance. %From Figure~\ref{fig:glob_loc_threads} we noticed that with 128 local threads, the error rate was close to 0\%. Because the CPU clock frequency of CPU cores is almost triple the clock frequency of iGPU as we show in Table \ref{tab:exp_setup}, ~43 cores can achieve similar memory write traffic frequency if CPU cores are used. 
% In this case, the attacker has to utilize large number of cores at the same time to issue memory write requests. This would make the attack detectable and it would require the attacker to synchronize between these cores such that memory write requests are issued at a close time. Another issue is that the possibility of context switching with system processes is very high due to using a lot of cores. This would increase the probability of high error rates.


