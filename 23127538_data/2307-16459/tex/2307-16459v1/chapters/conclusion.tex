\section{Conclusion}
\label{sec:conclusion}
In this paper, we propose a novel distillation strategy, L3DMC on mixed-curvature space to preserve the complex geometric structure  of medical data while training a DNN model on a sequence of tasks. L3DMC aims to optimize the lifelong learning model by minimizing the distance between new embedding and old subspace generated using current and old models respectively on higher dimensional RKHS. Extensive experiments show that L3DMC outperforms state-of-the-art L3 methods on standard medical image datasets for disease classification. In future, we would like to explore the effectiveness of our proposed distillation strategy on long-task and memory-free L3 setting.