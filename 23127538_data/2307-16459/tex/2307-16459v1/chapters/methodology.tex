\section{Proposed Method}
\label{sec:methodology}
\vspace{-2.0ex}
In our approach, we emphasize on modeling complex
latent structure of medical data by combining embedding representation of zero-curvature Euclidean and negative-curvature hyperbolic space. To attain richer representational power of RKHS~\cite{hofmann2008kernel}, we embed the low-dimensional fixed-curvature embedding onto higher-dimensional RKHS using the kernel method. %

\noindent
\textbf{Definition 1.} \textit{\big(Positive Definite Kernel\big)} A function $f: \mathcal{X}\times\mathcal{X} \rightarrow \mathbb{R}$ is \underline{p}ositive \underline{d}efinite (pd) if and only if \textbf{1.} $k(\vec{x_i}, \vec{x_j}) = k(\vec{x_j}, \vec{x_i})$ for any $\vec{x_i}, \vec{x_j} \in \mathcal{X}$, and \textbf{2.} for any given $n \in \mathbb{N}$, we have $\sum_i^n\sum_j^n c_ic_j k(\vec{x_j}, \vec{x_i}) \geq 0$ for any $\vec{x}_1, \vec{x}_2, \cdots, \vec{x}_n \in \mathcal{X}$ and  $c_1, c_2, \cdots, c_n \in \mathbb{R}$. Equivalently, the Gram matrix $K_{ij} = k(\vec{x}_i,\vec{x}_j) > 0$ for any set of $n$ samples $\vec{x}_1, \vec{x}_2, \cdots , \vec{x}_n \in \mathcal{X}$ should be Symmetric and Positive Definite (SPD). 

\noindent
Popular kernel functions (\eg, the Gaussian RBF) operate on flat-curvature Euclidean spaces. 
In $\mathbb{R}^n$, the Gaussian RBF kernel method is defined as
\begin{align}
\label{eq:rbf_e}
    k^{\mathfrak{e}}(\vec{z_i}, \vec{z_j}) \coloneqq \mathrm{exp}\big( -\lambda \| \vec{z_i} - \vec{z_j} \|^2 \big); \lambda > 0.
\end{align}
However, using the geodesic distance in a hyperbolic space along with an RBF function similar to \cref{eq:rbf_e} (\ie, replacing 
$\| \vec{z_i} - \vec{z_j} \|^2$ with the geodesic distance) does not lead to a valid positive definite kernel. 
Theoretically, a valid RBF kernel is impossible to obtain for hyperbolic space using geodesic distance~\cite{feragen2015geodesic,feragen2016open}. Therefore,  we use the tangent plane of hyperbolic space and employ $\text{log}_0^c$
\begin{align}
    \mathrm{log}_0^c(\vec{z}) = \frac{\vec{z}}{\sqrt{c}\|\vec{z}\|}\mathrm{tanh}^{-1}\big( \sqrt{c}\|\vec{z}\| \big)\;,
\end{align}
to embed hyperbolic data to RKHS via the following valid pd kernel (see~\cite{fang2021kernel} for the proof of positive definiteness):
\begin{align}
    k^{\mathfrak{h}}(\vec{z_i}, \vec{z_j}) = \mathrm{exp}\big( -\lambda\| \mathrm{log}_0^c(\vec{z_i}) - \mathrm{log}_0^c(\vec{z_j}) \|^2 \big).
\end{align}
\noindent
Now, in L3 setting, we have two models $h^t$ and $h^{t-1}$ at our hand at time $t$. 
We aim to improve $h^t$ while ensuring the past knowledge incorporated in $h^{t-1}$ is kept within $h^t$.
Assume $\vec{Z}_{t}$ and $\vec{Z}_{t-1}$ are the extracted feature vectors for input $\vec{X}$ using current and old feature extractor, $h_{feat}^t$ and $h_{feat}^{t-1}$, respectively. Unlike other existing distillation methods, we employ an independent 2-layer MLP for each fixed-curvature space to project extracted features to a new lower-dimensional embedding space on which we perform further operations. This has two benefits, \textbf{(i)} it relaxes the strong constraint directly applied on $\vec{Z}_{t}$ and $\vec{Z}_{t-1}$ and  \textbf{(ii)} reduce the computation cost of performing kernel method. Since we are interested in modeling embedding structure in zero-curvature Euclidean and negative-curvature hyperbolic spaces, we have two MLP as projection modules attached to feature extractors, namely $g_{e}$ and $g_{h}$. 



\paragraph{\textbf{Our idea.}} Our main idea is that, for a rich and overparameterized representation, the data manifold is low-dimensional.
Our algorithm makes use of RKHS, which can be intuitively thought of as a neural network with infinite width. Hence, we assume that the data manifold for the model at time $t-1$ is well-approximated by a low-dimensional hyperplane (our data manifold assumption). Let 
$\vec{Z}_{t-1}^{\mathfrak{e}} = \{\vec{z}^{\mathfrak{e}}_{t-1,1}, \vec{z}^{\mathfrak{e}}_{t-1,2}, \cdots, \vec{z}^{\mathfrak{e}}_{t-1,m}\}$
be the output of the Euclidean projection module for $m$ samples at time $t$ (\ie, current model). 
Consider $\vec{z}^{\mathfrak{e}}_{t}$,  a sample at time $t$  from the Euclidean projection head. We propose to minimize the following distance
\begin{align}
    \label{eqn:kernel_dist_subspace1}
    \delta^{\mathfrak{e}}(\vec{z}^{\mathfrak{e}}_{t}, \vec{Z}_{t-1}^{\mathfrak{e}}) & \coloneqq 
    \Big\| \phi(\vec{z}^{\mathfrak{e}}_{t}) - \text{span}\{ \phi(\vec{z}^{\mathfrak{e}}_{t-1,i}) \}_{i=1}^m \Big \|^2 
    \hspace{-2mm} \\ \notag & = 
    \min_{\alpha \in \mathbb{R}^m} \Big\| \phi(\vec{z}^{\mathfrak{e}}_{t}) - \sum_{i=1}^m \alpha_i \phi(\vec{z}^{\mathfrak{e}}_{t-1,i})\Big \|^2\;.
\end{align}

In~\cref{eqn:kernel_dist_subspace1}, $\phi$ is the implicit mapping to the RKHS defined by the Gaussian RBF kernel, \ie $k^{\mathfrak{e}}$. The benefit of formulation~\cref{eqn:kernel_dist_subspace1} is that it has a closed-form solution as
\begin{align}
    \label{eqn:kernel_dist_subspace2}
    \delta^{\mathfrak{e}}(\vec{z}^{\mathfrak{e}}_{t}, \vec{Z}_{t-1}^{\mathfrak{e}}) = 
    k(\vec{z}^{\mathfrak{e}}_{t},\vec{z}^{\mathfrak{e}}_{t}) - k_{\vec{z}\vec{Z}}^\top{K^{-1}_{\vec{Z}\vec{Z}}}k_{\vec{z}\vec{Z}}\;.
\end{align}
In~\cref{eqn:kernel_dist_subspace2},  $K_{\vec{Z}\vec{Z}} \in \mathbb{R}^{m \times m}$ is the Gram matrix of 
$\vec{Z}_{t-1}^{\mathfrak{e}}$, and   $k_{\vec{z}\vec{Z}}$ is an m-dimensional vector storing the kernel values between
$\vec{z}^{\mathfrak{e}}_{t}$ and elements of $\vec{z}^{\mathfrak{e}}_{t-1}$. We provide the proof of equivalency between 
\cref{eqn:kernel_dist_subspace1} and \cref{eqn:kernel_dist_subspace2} in the supplementary material due to the lack of space. 
Note that we could use the same form for the hyperbolic projection module $g_{h}$ to distill between the model at time $t$ and $t-1$, albeit this time, we employ the hyperbolic kernel $k^{\mathfrak{h}}$. Putting everything together, 
\begin{align}
    \label{eqn:total_kernel_distill_loss}
    \ell_{\text{KD}}(\vec{Z}_t) \coloneqq  \mathbb{E}_{\vec{z}_t} 
    \delta^{\mathfrak{e}}(\vec{z}^{\mathfrak{e}}_{t}, \vec{Z}_{t-1}^{\mathfrak{e}}) + 
    \beta \mathbb{E}_{\vec{z}_t}\delta^{\mathfrak{h}}(\vec{z}^{\mathfrak{h}}_{t}, \vec{Z}_{t-1}^{\mathfrak{h}}) \;. 
\end{align}
Here, $\beta$ is a hyper-parameter that controls the weight of distillation between the Euclidean and hyperbolic spaces.
We can employ~\cref{eqn:total_kernel_distill_loss} at the batch level. Note that in our formulation, computing the inverse of an $m\times m$
matrix is required, which has  a complexity of $\mathcal{O}(m^3)$. However, this needs to be done once per batch and manifold 
(\ie, Euclidean plus hyperbolic). $\ell_{\text{KD}}$ is differentiable with respect to $\vec{Z}_t$, which enables us to update the model at time $t$. We train our lifelong learning model by combining distillation loss, $\ell_{\text{KD}}$, together with standard cross entropy loss.
Please refer to the overall steps of training lifelong learning model using our proposed distillation strategy via mixed-curvature space in Algorithm~\ref{alg:CL3DMC}.





\vspace{-2.0ex}
\subsection{Classifier and Exemplar Selection}
We employ herding based exemplar selection method that selects examples that are closest to the class prototype, following iCARL~\cite{rebuffi2017icarl}. At inference time, we use exemplars from memory to compute class template and the nearest template class computed using Euclidean distance is used as the prediction of our L3 model. Assume $\vec{\mu}_{c}$ is the class template computed by averaging the extracted features from memory exemplars belonging to class $c$. Then, the prediction $\hat{y}$ for a given input sample $\vec{X}$ is determined as $\hat{y}=\underset{c=1,\ldots,t}{\argmin} \|h_{feat}^t(\vec{X}) - \mu_c\|_2$.

\vspace{-1.0ex}