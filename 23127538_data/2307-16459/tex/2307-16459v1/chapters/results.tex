\section{Results and Discussion}
\label{sec:results}

In our comparison, we consider two regularization-based methods (\ie, EWC~\cite{kirkpatrick2017overcoming}, and LwF~\cite{li2017learning}) and 5 memory-based methods (\eg, EEIL~\cite{castro2018end}, ER~\cite{riemer2018learning}, Bic~\cite{wu2019large}, LUCIR~\cite{hou2019learning} and iCARL~\cite{rebuffi2017icarl}). We employ the publicly available code\footnote{https://github.com/mmderakhshani/LifeLonger} of~\cite{derakhshani2022lifelonger} in our experiments to produce results for all baseline methods on BloodMNIST, PathMNIST, and OrganaMNIST datasets and report the quantitative results in Tab.~\ref{tab:benchmark}. The results suggest that the performance of all methods improves with the increase in buffer size (\eg, from 200 to 1000). We observe that our proposed distillation approach outperforms other baseline methods across the settings. The results suggest that the regularization-based methods, \eg, EWC and LwF perform poorly in task-agnostic settings across the datasets as those methods are designed for task-aware class-incremental learning. Our proposed method outperforms experience replay, ER method by a significant margin in both evaluation metrics (\ie, average accuracy and average forgetting) across datasets. For instance, our method shows around 30\%, 30\%, and 20\% improvement in accuracy compared to ER while the second best method, iCARL, performs about 4\%, 2\%, and 8\% worse than our method on BloodMNIST, PathMNIST, and OrganaMNIST respectively with 200 exemplars. Similarly, with 1000 exemplars, our proposed method shows consistent performances and outperforms iCARL by 4\%, 6\%, and 2\% accordingly on BloodMNIST, PathMNIST, and OrganaMNIST datasets. We also observe that catastrophic forgetting decreases with the increase of exemplars. Our method shows about 2\% less forgetting phenomenon across the datasets with 1000 exemplars compared to the second best method iCARL.

\begin{table*}[tp!]
\vspace{-.25em}
\centering
\caption{\label{tab:benchmark} Experimental results on BloodMNIST, PathMNIST, OrganaMNIST and TissueMNIST datasets for 4-tasks Class-Incremental setting  with varying buffer size. Our proposed method outperforms other baseline methods across the settings.
}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{ccccccc}
\hline
\multirow{1}{*}{Method}                           & \multicolumn{2}{c}{BloodMNIST} 
& \multicolumn{2}{c}{PathMNIST} 
& \multicolumn{2}{c}{OrganaMNIST} 
\\ \cline{2-7}
&     Accuracy~$\uparrow$ & Forgetting~$\downarrow$ & Accuracy~$\uparrow$      & Forgetting~$\downarrow$ 
&     Accuracy~$\uparrow$ & Forgetting~$\downarrow$ \\ \hline
Upper Bound   &  97.98  &  -    &   93.52   &   -   &  95.22   &   -   \\
Lower Bound   &  46.59  &  68.26    &   32.29   &   77.54   &  41.21   &   54.20   \\ \hline
EWC~\cite{kirkpatrick2017overcoming}   &  47.60  &  66.22    &  33.34   &  76.39   &  37.88   &  67.62   \\
LwF~\cite{li2017learning}   &  43.68  &  66.30    &  35.36   &  67.37   &  41.36   &  51.47   \\ 
\hline
\multicolumn{7}{c}{Memory Size: 200} \\  \cline{2-7}
EEIL~\cite{castro2018end}  & 42.17   &  71.25    &  28.42   &  79.39   &   41.03  &  62.47   \\ 
ER~\cite{riemer2018learning}    & 42.94   &  71.38    &  33.74   &  80.6    &   52.50  &  52.72   \\ 
LUCIR~\cite{hou2019learning} &  20.76  &  53.80    &  40.00   &  54.72   &  41.70   &  33.06   \\ 
BiC~\cite{wu2019large}   & 53.32   &  31.06    &  48.74   &  30.82   &  58.68   & 29.66    \\ 
iCARL~\cite{rebuffi2017icarl} &  67.70  &  \textbf{14.52}    &  58.46   &  \textbf{-0.70}   &  63.02   &  \textbf{7.75}    \\ 
\rowcolor{LightCyan}
Ours   &  \textbf{71.98}   &  14.62   &  \textbf{60.60}   &  21.18   &  \textbf{71.01}   &  13.88  \\ 
\hline
\multicolumn{7}{c}{Memory Size: 1000} \\  \cline{2-7}
EEIL~\cite{castro2018end}  &  64.40  &   40.92   &   34.18  &  75.42   &  66.24   &  34.60   \\ 
ER~\cite{riemer2018learning}    &  65.94  &   33.68   &  44.18   &  66.24   &  67.90   &  31.72   \\ 
LUCIR~\cite{hou2019learning} &  20.92  &   28.42   &  53.84   &  30.92   &   54.22  &  23.64   \\ 
BiC~\cite{wu2019large}   &  70.04  &  17.98    &  -   &   -  &  73.46   &  15.98   \\ 
iCARL~\cite{rebuffi2017icarl} &  73.10  &  13.18    &  61.72   &   14.14  &  74.54   &  10.50   \\ 
\rowcolor{LightCyan}
Ours  &  \textbf{77.26}   &  \textbf{10.9}   &  \textbf{67.52}   &  \textbf{12.5}   &  \textbf{76.46}   &  \textbf{9.08}   \\ 
\hline

\end{tabular}
\end{adjustbox}
\end{table*}

\begin{table}[t!]
\vspace{-1.75ex}    
    \centering
    \caption{\label{tab:cross_domain} Average accuracy on the cross-domain incremental learning scenario~\cite{derakhshani2022lifelonger} with 200 exemplars. CL3DMC outperforms baseline methods by a significant margin in both task-aware and task-agnostic settings. Best values are in bold.}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
Scenario & LwF & EWC & ER & EEIL & BiC & iCARL & LUCIR & L3DMC(Ours) \\    \hline
    Task-Agnostic (Accuracy~$\uparrow$)  &  29.45  & 18.44  &  34.54 &  34.54  & 26.79  &  48.87  & 19.05  &  \textbf{52.19}   \\ \hline
    Task-Aware  (Accuracy~$\uparrow$)     &  31.07  &  29.26 &  37.69 &  33.19 &  33.19  &  49.47 &  27.48 &  \textbf{52.83}   \\ 
    \hline
    
    \end{tabular}
\vspace{-5.0ex}    
\end{table}
Tab.~\ref{tab:cross_domain} presents the experimental results (\eg, average accuracy) on relatively complex cross-domain incremental learning setting where datasets (BloodMNIST, PathMNIST, OrganaMNIST, and TissueMNIST) with varying modalities from different institutions are presented at each novel task. Results show an unmatched gap between regularization-based methods (\eg, Lwf and EWC) and our proposed distillation method. CL3DMC outperforms ER method by around 16\% on both task-aware and task-agnostic settings. Similarly, CL3DMC performs around 3\% better than the second best method, iCARL. 

\vspace{-3.0ex}    
