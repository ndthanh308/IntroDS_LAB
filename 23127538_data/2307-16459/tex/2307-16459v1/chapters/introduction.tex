\section{Introduction}
\label{sec:intro}
\vspace{-2.5mm}
Lifelong learning~\cite{parisi2019continual,ring1997child} is the process of sequential learning  from a series of non-stationary data distributions through acquiring novel concepts while preserving already-learned knowledge. However, \underline{D}eep \underline{N}eural \underline{N}etworks (DNNs) exhibit a significant drop in performance on previously seen tasks when trained in continual learning settings. This phenomenon is often called catastrophic forgetting~\cite{mccloskey1989catastrophic,nguyen2019toward,robins1995catastrophic}. Furthermore, the unavailability of sufficient training data in medical imaging poses an additional challenge in tackling catastrophic forgetting of a DNN model.


In a lifelong learning scenario, maintaining a robust embedding space and preserving geometrical structure is crucial to mitigate performance degradation and catastrophic forgetting of old tasks\cite{knights2022incloud}. However, the absence of samples from prior tasks has been identified as one of the chief reasons for catastrophic forgetting.  Therefore, to address the problem, a small memory buffer has been used in the literature to store a subset of samples from already seen tasks and replayed together with new samples~\cite{rebuffi2017icarl,hou2019learning}. Nevertheless,  imbalances in data (\eg, between current tasks and the classes stored in the memory) make the model biased towards the current task~\cite{hou2019learning}.
\begin{wrapfigure}{t}{6.75cm}
\scriptsize
\vspace{-5mm}
\hspace{-5mm}
\centering
    % Figure removed
    \caption{ Geometrical interpretation of the proposed distillation strategy, L3DMC via mixed-curvature space that optimizes the model by combining the distillation loss from Euclidean with zero-curvature (left) and hyperbolic with negative-curvature (right) space. L3DMC preserves the complex geometrical structure by minimizing the distance between new data representation and subspace induced by old representation in RKHS.}
    \label{fig:l3dmc}
    \vspace{-7mm}
\end{wrapfigure}
Knowledge distillation~\cite{hinton2015distilling,parisi2019continual,douillard2020podnet,buzzega2020dark} has been widely used in the literature to preserve the previous knowledge while training on a novel data distribution. This approach applies constraints on updating the weight of the current model by mimicking the prediction of the old model. To maintain the old embedding structure intact in the new model,  feature distillation strategies have been introduced in the literature~\cite{hou2019learning,douillard2020podnet}. For instance, LUCIR~\cite{hou2019learning} emphasizes on maximizing the similarity between the orientation of old and new embedding by minimizing the cosine distance between old and new embedding. %
While effective, feature distillation applies strong constraints directly on the lower-dimensional embedding extracted from the old and new models, reducing the plasticity of the model. This is not ideal for adopting novel concepts while preserving old knowledge. 
Lower-dimensional embedding spaces, typically used for distillation, may not preserve all the latent information in the input data~\cite{jayasumana2015kernel}. As a result, they may not be ideal for distillation in lifelong learning scenarios. Furthermore, DNNs often operate on zero-curvature (\ie, Euclidean) spaces, which may not be suitable for modeling and distilling complex geometrical structures
in non-stationary biomedical image distributions with various modalities and discrepancies in imaging protocols and medical equipment. On the contrary, hyperbolic spaces have been successfully used to model hierarchical structure in input data for different vision tasks~\cite{khrulkov2020hyperbolic,ganea2018hyperbolic}. 



In this paper,  we propose to perform distillation in a Reproducing Kernel Hilbert Space (RKHS), constructed from the embedding space of multiple fixed-curvature spacess. This approach is inspired by the ability of kernel methods to yield rich representations in 
higher-dimensional RKHS~\cite{fang2021kernel,jayasumana2015kernel}. Specifically, we employ a \underline{R}adial \underline{B}asis \underline{F}unction (RBF) kernel on a mixed-curvature space that combines embeddings from hyperbolic (negative curvature), and Euclidean (zero curvature), using a decomposable Riemannian distance function as illustrated in Fig.~\ref{fig:l3dmc}. This mixed-curvature space is robust and can maintain a higher quality geometrical formation. This makes the space more suitable for knowledge distillation and tackling catastrophic forgetting in lifelong learning scenarios for medical image classification. Finally, to ensure a similar geometric structure between the old and new models in L3, we propose minimizing the distance between the new embedding and the subspace constructed using the old embedding in RKHS.
Overall, our contributions in this paper are as follows:
\begin{itemize}
    \item To the best of our knowledge, this is the first attempt to study mixed-curvature space for the continual medical image classification task.
    \item We propose a novel knowledge distillation strategy to maintain a similar geometric structure for continual learning by minimizing the distance between new embedding and subspace constructed using old embedding in RKHS.
    \item Quantitative analysis shows that our proposed distillation strategy is capable of preserving complex geometrical structure in embedding space resulting in significantly less degradation of the performance of continual learning  and superior performance compared to state-of-the-art baseline methods on BloodMNIST, PathMNIST, and OrganaMNIST datasets.
\end{itemize}

\vspace{-2.0ex}