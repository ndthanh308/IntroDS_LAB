\section{Related Work}
\label{sec:relatedwork}


\begin{wrapfigure}{R}{0.55\textwidth}
\vspace{-16ex}
\begin{minipage}{0.55\textwidth}
\begin{algorithm}[H] 
\caption{Lifelong Learning using Distillation via Mixed-Curvature Space}
\label{alg:CL3DMC}
\begin{algorithmic}[1]
\Require{$\text{Dataset}~\mathcal{D}^0,\mathcal{D}^1,...,\mathcal{D}^T, \text{ and Memory}~\mathcal{M}$}
\Ensure{The new model at time $t$ with parameters $\vec{\Theta}^t$}
\State {Randomly Initialize $\vec{\Theta}^0$;~$h^0_{\vec{\Theta}} = h^0_{\text{feat}} \circ h^0_{\text{cls}}$}
\State {Train $\vec{\Theta}^0$ on $\mathcal{D}^0$ using $\ell_{CE}$}
    \For {$t$  in  $\{1, 2, ... , T\}$}
    \State {$\text{Initialize }\vec{\Theta}^t~\text{ with }\vec{\Theta}^{t-1}$}
    \For {iteration $1$  to  max\_iter}

        \State {Sample a mini batch $(\mathcal{X}_B, \mathcal{Y}_B)$ from $({\mathcal{D}^t} \cup {\mathcal{M}})$}
                
        \State $\vec{Z}^t \leftarrow h_{\text{feat}}^t(\mathcal{X}_B)$
        \State $\vec{Z}^{t-1} \leftarrow h_{\text{feat}}^{t-1}(\mathcal{X}_B)$
        
        \State {$ \mathcal{\tilde{Y}}_B \leftarrow h_{\Theta^t}(\vec{Z}^t)$}
        
        \State {Compute $\ell_{\mathrm{CE}}$ between $\mathcal{Y}_B$ and  $\mathcal{\tilde{Y}}_B$}
        \State {Compute $\ell_{\text{KD}}$ between $\vec{Z}^{t-1}$ and $\vec{Z}^{t}$ %
        }

        \State {Update $\vec{\Theta}^t$ by minimizing the combined loss of cross-entropy $\ell_{\mathrm{CE}}$ and $\ell_{\mathrm{KD}}$ as in Eq.~\eqref{eqn:total_kernel_distill_loss}
        }
    \EndFor
    \State {Evaluate Model on test dataset}
    \State {Update Memory $\mathcal{M}$ with exemplars from $\mathcal{D}^t$}
    \EndFor
\end{algorithmic}
\end{algorithm}
\end{minipage}
\vspace{-3ex}
\end{wrapfigure}

In this section, we describe Mixed-curvature space and L3 methods to tackle catastrophic forgetting.

\noindent
\textbf{Constant-Curvature and Mixed-Curvature Space.} Constant-curvature spaces have been successfully used in the literature to realize the intrinsic geometrical orientation of data for various downstream tasks in machine learning. Flat-curvature Euclidean space is suitable to model grid data~\cite{wu2020comprehensive} while positive and negative-curvature space is better suited for capturing cyclical~\cite{bachmann2020constant} and hierarchical~\cite{liu2019hyperbolic} structure respectively. Hyperbolic representation has been used across domains ranging from image classification~\cite{mathieu2019continuous} and natural language processing~\cite{nickel2017poincare,nickel2018learning} to graphs~\cite{chami2019hyperbolic}. However, a constant-curvature space is limited in modeling the geometrical structure of data embedding as it is designed with a focus on particular structures~\cite{gu2019learning}.

\noindent
\textbf{Kernel Methods.} A Kernel is a function that measures the similarity between two input samples. The intuition behind the kernel method is to embed the low-dimensional input data into a higher, possibly infinite, dimensional RKHS space. Because of the ability to realize rich representation in RKHS, kernel methods have been studied extensively in machine learning~\cite{hofmann2008kernel}. 

\noindent
\textbf{L3 using Regularization with Distillation.}
Regularization-based approaches impose constraints on updating weights of L3 model to maintain the performance on old tasks. LwF mimics the prediction of the old model into the current model but struggles to maintain consistent performance in the absence of a task identifier. Rebuff \etal in~\cite{rebuffi2017icarl} store a subset of exemplars using a herding-based sampling strategy and apply knowledge distillation on output space like LwF~\cite{li2017learning}. Distillation strategy on feature spaces has also been studied in the literature of L3. Hou \etal in~\cite{hou2019learning} proposes a less-forgetting constraint that controls the update of weight by minimizing the cosine angle between old and new embedding representation. 

