

\noindent
This document presents additional details that were omitted from
the main paper, due to the space constraints.

\paragraph{\textbf{Poincar\`e Ball.}} Exponential mapping function, $\text{exp}_{\vec{x}}^c$ offers a way to embed Euclidean feature point to Hyperbolic space (\eg, Poincar\`e Ball) and is defined as follows:
\begin{align}
\label{eq:exp_map}
    \exp_{\vec{x}}^c(\vec{v}) = \vec{x}~\oplus_c~\Big( \mathrm{tanh}\big(\sqrt{c}\frac{\lambda_{\vec{x}}^c\|\vec{v}\|}{2}\big)\frac{\vec{v}}{\sqrt{c}\|\vec{v}\|} \Big)
\end{align}


\paragraph{\textbf{Proof of the Proposed Distillation Loss.}}
In the main paper, we state that our distillation approach aims to minimizes the RKHS kernel interpolation error for preserving the geometric structure from prior model into current model. Assuming 
$\vec{Z}_{t-1}^{\mathfrak{e}} \in \mathbb{R}^{d \times m} = \{\vec{z}^{\mathfrak{e}}_{t-1,1}, \vec{z}^{\mathfrak{e}}_{t-1,2}, \cdots, \vec{z}^{\mathfrak{e}}_{t-1,m}\}$
be the output of the Euclidean projection module for $m$ samples using the old model and 
 $\vec{z}^{\mathfrak{e}}_{t} \in \mathbb{R}^{d \times 1}$ be a sample at time $t$ from the Euclidean projection head using current model, we proposed to minimize the following distance
\begin{equation}
\label{eqn:dist_loss}
  \delta^{\mathfrak{e}}(\vec{z}^{\mathfrak{e}}_{t}, \vec{Z}_{t-1}^{\mathfrak{e}}) = \hspace{-1mm}
    \min_{\alpha \in \mathbb{R}^m} \Big\| \phi(\vec{z}^{\mathfrak{e}}_{t}) - \sum_{i=1}^m \alpha_i \phi(\vec{z}^{\mathfrak{e}}_{t-1,i})\Big \|^2 = k(\vec{z}^{\mathfrak{e}}_{t},\vec{z}^{\mathfrak{e}}_{t}) - k_{\vec{z}\vec{Z}}^\top{K^{-1}_{\vec{Z}\vec{Z}}}k_{\vec{z}\vec{Z}}
\end{equation}
Bellow we provide the proof of Eq.~\eqref{eqn:dist_loss}. \\
\noindent
Since, $\phi$ in Eq.~\eqref{eqn:dist_loss} refers to the implicit mapping function to the RKHS defined by the Gaussian RBF kernel, \ie $k^{\mathfrak{e}}$, the distance can be represented as follows
\begin{align}
    \label{eqn:kernel_dist_subspace_}
    \Big\| \phi(\vec{z}^{\mathfrak{e}}_{t}) - \sum_{i=1}^m \alpha_i \phi(\vec{z}^{\mathfrak{e}}_{t-1,i})\Big \|^2\ = k(\vec{z}^{\mathfrak{e}}_{t},\vec{z}^{\mathfrak{e}}_{t}) - 2\cdot \alpha^\top k_{\vec{z}\vec{Z}} + \alpha^\top K_{\vec{Z}\vec{Z}}\alpha\;.
\end{align}
Performing derivative w.r.t. $\alpha$ and setting to zero results into $\alpha = K_{\vec{Z}\vec{Z}}^{-1}k_{\vec{z}\vec{Z}}$. Therefore, we can rewrite Eq.~\eqref{eqn:kernel_dist_subspace_} as follows
\begin{align}
    \label{eqn:kernel_dist_subspace_1}
    & \min_{\alpha \in \mathbb{R}^m} \Big\| \phi(\vec{z}^{\mathfrak{e}}_{t}) - \sum_{i=1}^m \alpha_i \phi(\vec{z}^{\mathfrak{e}}_{t-1,i})\Big \|^2\ \notag \\ 
    & = k(\vec{z}^{\mathfrak{e}}_{t},\vec{z}^{\mathfrak{e}}_{t}) - 2\cdot \{K_{\vec{Z}\vec{Z}}^{-1}k_{\vec{z}\vec{Z}}\}^\top K_{\vec{Z}\vec{z}} + \{K_{\vec{Z}\vec{Z}}^{-1}k_{\vec{z}\vec{Z}}\}^\top K_{\vec{Z}\vec{Z}}\{K_{\vec{Z}\vec{Z}}^{-1}k_{\vec{z}\vec{Z}}\} \notag \\
    & = k(\vec{z}^{\mathfrak{e}}_{t},\vec{z}^{\mathfrak{e}}_{t}) - 2\cdot k_{\vec{z}\vec{Z}}^\top \{K_{\vec{Z}\vec{Z}}^\top\}^{-1} k_{\vec{z}\vec{Z}} + k_{\vec{z}\vec{Z}}^\top \{K_{\vec{Z}\vec{Z}}^\top\}^{-1} K_{\vec{Z}\vec{Z}}K_{\vec{Z}\vec{Z}}^{-1}k_{\vec{z}\vec{Z}}
    \;.
\end{align}
Since $\{\vec{K_{\vec{Z}\vec{Z}}}^\top\}^{-1}=\{\vec{K_{\vec{Z}\vec{Z}}}^{-1}\}^\top$ and $\vec{K_{\vec{Z}\vec{Z}}}^\top = \vec{K_{\vec{Z}\vec{Z}}}$, $\{\vec{K_{\vec{Z}\vec{Z}}}^\top\}^{-1} \vec{K_{\vec{Z}\vec{Z}}} = \mathbb{I}$. Hence, Eq.~\eqref{eqn:kernel_dist_subspace_1} can be represented as follows



\begin{align}
    \label{eqn:kernel_dist_subspace_2}
    \min_{\alpha \in \mathbb{R}^m} \Big\| \phi(\vec{z}^{\mathfrak{e}}_{t}) - \sum_{i=1}^m \alpha_i \phi(\vec{z}^{\mathfrak{e}}_{t-1,i})\Big \|^2\ & = k(\vec{z}^{\mathfrak{e}}_{t},\vec{z}^{\mathfrak{e}}_{t}) - 2\cdot k_{\vec{z}\vec{Z}}^\top K_{\vec{Z}\vec{Z}}^{-1} k_{\vec{z}\vec{Z}} + k_{\vec{z}\vec{Z}}^\top K_{\vec{Z}\vec{Z}}^{-1}k_{\vec{z}\vec{Z}} \notag \\
    & = k(\vec{z}^{\mathfrak{e}}_{t},\vec{z}^{\mathfrak{e}}_{t}) - k_{\vec{z}\vec{Z}}^\top K_{\vec{Z}\vec{Z}}^{-1} k_{\vec{z}\vec{Z}}
    \;.
\end{align}

\paragraph{\textbf{Gradient of the Proposed Distillation Loss.}} In this section, we provide a closed form solution to computed gradient of our proposed knowledge distillation loss via mixed-curvature space.\\
\noindent
Gradient computation of our proposed distillation loss (Eq.~\eqref{eqn:dist_loss}), $\delta^{\mathfrak{e}}(\vec{z}^{\mathfrak{e}}_{t}, \vec{Z}_{t-1}^{\mathfrak{e}})$ with respect to $\vec{z}^{\mathfrak{e}}_{t}$ requires to compute $\frac{\partial \delta^{\mathfrak{e}}(\vec{z}^{\mathfrak{e}}_{t}, \vec{Z}_{t-1}^{\mathfrak{e}})}{\partial k_{\vec{z}\vec{Z}}}$. Using chain rule, we have
\begin{align}
\label{eqn:grad_e}
       \frac{\partial \delta^{\mathfrak{e}}(\vec{z}^{\mathfrak{e}}_{t}, \vec{Z}_{t-1}^{\mathfrak{e}})}{\partial \vec{z}^{\mathfrak{e}}_{t}} = 
       \frac{\partial \delta^{\mathfrak{e}}(\vec{z}^{\mathfrak{e}}_{t}, \vec{Z}_{t-1}^{\mathfrak{e}})}{\partial k_{\vec{z}\vec{Z}}}
       \frac{\partial k_{\vec{z}\vec{Z}}}{\partial \vec{z}^{\mathfrak{e}}_{t}}\;.
\end{align}
Now, 
\begin{align}
\label{eqn:grad_e_1}
       \frac{\partial \delta^{\mathfrak{e}}(\vec{z}^{\mathfrak{e}}_{t}, \vec{Z}_{t-1}^{\mathfrak{e}})}{\partial k_{\vec{z}\vec{Z}}} = 
       -2{K^{-1}_{\vec{Z}\vec{Z}}}k_{\vec{z}\vec{Z}}\;.
\end{align}
Considering an RBF kernel $k^{\mathfrak{e}}(\vec{z_{t}}, \vec{z_{t-1}}) \coloneqq \mathrm{exp}\big( -\lambda \| \vec{z_{t}} - \vec{z_{t-1}} \|^2 \big); \lambda > 0.$, $\big( \vec{z}^{\mathfrak{e}}_{t} - \vec{Z}_{t-1}^{\mathfrak{e}} \big)$ is as follows
\begin{align}
\label{eqn:grad_e_2}
    \frac{\partial k_{\vec{z}\vec{Z}}}{\partial \vec{z}^{\mathfrak{e}}_{t}} = -2\lambda k_{\vec{z}\vec{Z}} \big( \vec{z}^{\mathfrak{e}}_{t} - \vec{Z}_{t-1}^{\mathfrak{e}} \big)\;.
\end{align}
Combining all together, given an RBF function as kernel, the derivative of $\delta^{\mathfrak{e}}(\vec{z}^{\mathfrak{e}}_{t}, \vec{Z}_{t-1}^{\mathfrak{e}})$ with respect to $z_t^{\mathfrak{e}}$ for zero-curvature Euclidean space is as follows
\begin{align}
   \frac{\partial \delta^{\mathfrak{e}}(\vec{z}^{\mathfrak{e}}_{t}, \vec{Z}_{t-1}^{\mathfrak{e}})}{\partial \vec{z}^{\mathfrak{e}}_{t}} = 4\lambda d_{zZ}{K^{-1}_{\vec{Z}\vec{Z}}} k_{\vec{z}\vec{Z}}^2;~d_{zZ}=\big( \vec{z}^{\mathfrak{e}}_{t} - \vec{Z}_{t-1}^{\mathfrak{e}} \big)
\end{align}
For $d$-dimensional projected embedding space and $m$ embedding representation from old model, we have $d_{zZ} \in \mathbb{R}^{d \times m}$, $K^{-1}_{\vec{Z}\vec{Z}} \in \mathbb{R}^{m \times m}$ and $K_{\vec{z}\vec{Z}} \in \mathbb{R}^{m \times 1}$.






