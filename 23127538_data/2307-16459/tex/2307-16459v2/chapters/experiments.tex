\section{Experimental Details}
\label{sec:experiments}

\noindent
\textbf{Datasets.} In our experiments, we use four datasets (\eg, BloodMNIST~\cite{acevedo2020dataset}, PathMNIST~\cite{kather2019predicting}, OrganaMNIST~\cite{bilic2023liver}) and TissueMNIST~\cite{bilic2023liver} from MedMNIST collection~\cite{yang2021medmnist} for the multi-class disease classification. BloodMNIST, PathMNIST, OrganaMNIST and TissueMNIST have 8, 9, 11, and 8 distinct classes, respectively that are split into 4 tasks with non-overlapping classes between tasks following~\cite{derakhshani2022lifelonger}. For cross-domain continual learning experiments, we present 4 datasets sequentially to the model. %

\noindent
\textbf{Implementation Details.} We employ ResNet18~\cite{he2016deep} as the backbone for feature extraction and a set of task-specific fully connected layers as the classifier to train all the baseline methods across datasets. To ensure fairness in comparisons, we run each experiment with the same set of hyperparameters as used in~\cite{derakhshani2022lifelonger} for five times with a fixed set of distinct seed values, ${1, 2, 3, 4, 5}$ and report the average value. Each model is optimized using Stochastic Gradient Decent (SGD) with a batch of 32 images for 200 epochs, having early stopping options in case of overfitting. Furthermore, we use gradient clipping by enforcing the maximum gradient value to 10 to tackle the gradient exploding problem.

\noindent
\textbf{Evaluation Metrics.} We rely on average accuracy and average forgetting to quantitatively examine the performances of lifelong learning methods as used in previous approaches~\cite{rebuffi2017icarl,chaudhry2018riemannian}. 
Average accuracy is computed by averaging the accuracy of all the previously observed and current tasks after learning a current task $t$ and defined as:
$\text{Acc}_{t} = \frac{1}{t}\sum _{i=1}^{t} \text{Acc}_{t, i},$
where $\text{Acc}_{t, i}$ is the accuracy of task $i$ after learning task $t$.
We measure the forgetting of the previous task at the end of learning the current task $t$ using:
$F_{t} = \frac{1}{t-1}\sum _{i=1}^{t-1} \max_{j \in \{1...t-1\}} \text{Acc}_{j,i} - \text{Acc}_{t, i},$
where at task $t$, forgetting on task $i$ is defined as the maximum difference value previously achieved accuracy and current accuracy on task $i$.

\vspace{-2ex}