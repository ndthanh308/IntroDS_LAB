
\section{Learned features of noisy data}
% Figure environment removed

In Fig. \ref{feature_distribution}(a) and (b), we visualize the learned features of clean and noisy datasets.
It can be seen that clean features have a clear cluster structure, where samples in the same class are close.
However, noisy features are more twisted, as in Fig. \ref{feature_distribution}(b), where some samples locate faraway from their cluster center. 
The softmax-based linear classifier on top of the penultimate layer features would be unable to separate such twisted features and, subsequently, the cross-entropy loss is unable to identify label noise \cite{DBLP:conf/cvpr/IscenVAS22,DBLP:conf/nips/KimKCCY21}.
We suggest that this is one of the reasons the small-loss criterion fails.

From Fig. \ref{feature_distribution} (b), we can also notice that most samples, even though not forming cluster structure, still have same-class neighbors.  
Therefore, we attempt to utilize the topological relationships between samples for label confidence estimation.
How possible a sample/node is mislabeled can be determined by all data points' distribution in the feature space.
The graph structure is commonly used to model such relationships, on which all nodes can propagate their labels to their neighbors.
Also considering that the propagation could be an iterative process, where corrected nodes can affect their neighbors again until reaching convergence.
We thus solve the label confidence estimation as the classic graph Laplacian minimization problem \cite{newman2004detecting,DBLP:conf/icml/ZhuGL03,DBLP:journals/corr/abs-2106-04527}, i.e., making the intrinsic connection structure sufficiently smooth.
After this process, the more the label changes, the more possible the original labels are wrong. 


\section{Details of RandAugment}
