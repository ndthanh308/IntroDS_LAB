% !TEX root = main.tex
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\approach} \label{sec:approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\approach takes as input a set of gameplay videos related to a specific video game and returns a hierarchy of segments of gameplay videos organized on three levels: (i) context (\eg level or game area), (ii) issue type (\eg bug or glitch), and (iii) specific issue (\eg game crashes when talking to a specific non-player character).

\figref{fig:workflow} shows an overview of the \approach workflow. We describe below in more detail the main steps of \approach.

% Figure environment removed


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Video Segmentation}
\label{sec:videosegmentation}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%
The first step of \approach consists in partitioning the video into meaningful segments that can be later analyzed as standalone shorter videos. %In other words, \approach aims at finding a set of ``cut points'' in the video.
In the computer vision literature, a similar problem is referred to as ``shot transitions detection'' \cite{souek2020transnet}. The aim is to detect sudden changes in the video content. An example of approaches defined to solve such a problem is the one introduced in \citep{tang2018fast}.
Video-related information, however, might not be sufficient to find cuts in gameplay contents. \REV{In the context of video segmentation, relying only on scene changes to identify meaningful segments may not be sufficient. Scene changes may be due to various minor factors, \eg rapid zoom into the viewfinder of a weapon and then back to the general framing of the scene. Such situations do not provide significant information for identifying potential issues. Furthermore, in some contexts, scene changes may not be evident, leading to the creation of very large segments that are difficult to analyse. Let us consider, for example, the gameplay video available at \url{https://www.youtube.com/watch?v=_kQIJ2Omy9w}: From 14:10 to 15:56 there is no shot transition, even though various separate events and actions occur.} Moreover, for example, if the game crashes and a shot transition detection approach is used to cut the video, the second in which the crash happens would probably be selected for segmentation. The streamer, however, might need a few seconds to react to such an event by commenting what happened providing useful information for the game developers. Thus, by using shot transitions as cut points, the spoken content related to the issue might be erroneously put in the subsequent segment. To solve this problem, we decided to mainly rely on the spoken content to decide the cut points in the video: The core idea is to get the points in which each subtitle entry (\ie units of text shown on the screen) begins and ends, slightly shifted by \REV{$t$} seconds (where \REV{$t$} is a parameter of the approach) to take into account the reaction time of the streamer, and thus consider the video in-between as a segment. As for the shifting operation, given a subtitle entry that starts at second $s$ and ends at second $s+d$ (where $d$ is the duration of the subtitle entry), our approach will extract the video segment between $\max(s-t, 0)$ and $\min(s+d+t, \mathit{video\ length})$. For example, consider the case where we set \REV{$t = 5$} and we detect a subtitle entry that starts at 13:45 (mm:ss) and lasts 3 seconds. Our approach will cut the video between 13:40 and 13:52. We report in \secref{sec:design} how we tune the \REV{$t$} parameter.

As a result, our segmentation approach will implicitly discard some parts of the input video (\ie the ones in which the streamer is not speaking) and it might put some parts of the video in many segments when \REV{$t > 0$} (\eg for contiguous subtitle entries). Also, it is worth noting that using this strategy might result in a very high number of extracted segments for each video since subtitle entries generally include only parts of a sentence: In subtitles, a given sentence is broken into several entries to allow the watcher to comfortably read each of them. To preliminarily exclude segments that most likely do not contain any piece of useful information and, thus, to reduce the effort for the next step, we use a keyword-matching approach. If at least a relevant keyword is found in the subtitle entry related to a given segment, we consider the segment, while we exclude it otherwise. 

To define the list of keywords, we relied on (i) the 12,122 change notes of video games used by \citet{truelove2021we} to define the taxonomy of the most frequently encountered problems in video games and (ii) the 996 titles and descriptions of the gamplay videos in the dataset defined by \citet{lin2019identifying}. One of the authors manually extracted, from each instance, a first set of keywords (also composed by more than a word) which were related to issues in video games (\eg ``glitch'' or ``bug''). As a result of this process, 161 basic keywords were identified the file containing the selected keywords is reported in the replication pacakage \citep{replicationpackage}. 
From such keywords, we automatically generated new semantically equivalent keywords to have a broader dictionary.
To do that, we first tokenized the keywords and automatically tagged the Part-of-Speech (PoS) by using the spaCy Python package \citep{spacy}. Then, for each token with its PoS tag, we used both WordNet \citep{miller1995wordnet} and SEWordSim \citep{tian2014sewordsim} to generate both general-purpose and domain-specific synonyms of each word.
At this point, for each keyword composed by the sequence of words $\langle w_1, \dots, w_n \rangle$, we combined all the synonyms of each word and generated the new set of candidate keywords by using the Cartesian product: $\{ \mathit{syn}(w_1) \times \dots \times \mathit{syn}(w_n) \}$. For example, given the initial keyword ``lag'', we generated the candidate alternative keywords ``stuttering'', ``FPS drop''. From the initial 161 identified keywords, we obtained a total of 207 candidate keywords.
Then, two of the authors independently validated the new keywords to discard the ones that were not related to issues in video games. In case of disagreement, they discussed to reach consensus. In the end, we added 96 new keywords, while 111 were discarded. \REV{In our analysis, we assessed the inter-rater reliability between the annotators involved in identifying keywords by calculating Cohen's Kappa coefficient. The obtained results indicate an agreement level of $k = 0.74$. The coefficient value of 0.74 indicates a good level of agreement between the annotators in terms of identifying the keywords. }For example, the keyword ``crash'' generated from ``break up'' was discarded. Thus, our final list of keywords is composed of 257 keywords which can be mapped to our replication package \citep{replicationpackage}.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsubsection{Segment Filtering}
\subsection{Segment Categorization} 
\label{sec:videocategorization}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this second step, \approach aims at categorizing segments based on their content. \approach considers five labels: One for \textit{non-informative} segments (\ie the ones not reporting issues), and four for \textit{informative} segments (\ie the ones reported in \tabref{tab:taxonomy}). Non-informative segments are discarded and not considered in the next steps.

Previous work successfully used machine-learning to solve similar classification problems in the context of mobile app reviews \citep{chen2014ar, scalabrino2017listening}. Such approaches mainly rely on textual features. In our context, we can extract information that could also help to correctly classify segments from video analysis. For example, segments without video might be more likely to be \textit{non-informative}, even if a reader comment is present. Therefore, we include in \approach also video-based features. 
More specifically, we extract five sets of features: Three of them only based on the subtitles (\ie what the streamer says), one of them based on the video (\ie what happens in the game), and one of them including the best set of textual features and the set of video-based features.

\textbf{Textual Features.}
As for the textual features, we consider Bag of Words (BoW) \citep{zhang2010understanding}, doc2vec (d2v) \citep{karvelis2018topic} and word2vec (w2v) \citep{rong2014word2vec}. BoW consists in detecting the dictionary of the training set and using each word of the dictionary as a feature. The value of each feature for a given instance corresponds to the number of times the related word appears in an instance. The number of features directly depends on the training set. In our case, given the training set described in \secref{sec:datasets}, we extracted 2,253 features.
The d2v model \citep{karvelis2018topic} allows to automatically extract a vector of features for an entire instance (document). Such a model allows to automatically represent a document (sequence of words) as a vector. Specifically, we represent each subtitle string \REV{for each identified segment} as a vector composed of 40 features \REV{since this is the default number of features extracted by such a model \citep{karvelis2018topic}}.
Finally, the w2v model \citep{rong2014word2vec} allows to represent a single word as a set of features. Thus, differently from doc2vec, it does not directly work at document-level. To define the features based on w2v, given all the words in a given instance, we extract the vectors through the w2v model and we compute the average of each feature. In this case, we represent each word as a vector of 300 features, \REV{again, because the w2v model extracts by default such a number of features \citep{rong2014word2vec}}.

\textbf{Video-based Features.}
With video-based features, instead, we mainly wanted to represent to what extent the video contains unexpected frames that could possibly be related to issues. To this aim, given each pair of subsequent frames $f_i$ and $f_{i+1}$: (i) we compute their structural similarity through SSIM \citep{ssim}, \ie $s_i = \mathit{SSIM}(f_i, f_{i+1})$; (ii) we extract their HSV histograms using the HISTCMP CORREL function of OpenCV \cite{opencv}, thus obtaining $h(f_i)$ and $h(f_{i+1})$; (iii) we then compute their Pearson correlation coefficient $\mathit{hsv}_i = \mathit{cor}(h(f_i), h(f_{i+1}))$. 
We use SSIM instead of other image similarity measures because it has been shown that such a metric best captures the similarity of images as perceived by humans \citep{ssim}. Since such a metric ignores colors but considers, by default, a black-and-white version of the image, we also use HSV histograms to detect differences in the colors.
Finally, given the vectors of values $\mathit{hsv}$ and $\mathit{s}$ for all the frames between 0 and $n$ (number of frames in the video), we aggregate their values and define 12 video based features by computing the mean, median, minimum, maximum, first quartile, and third quartile of both of them. Such features allow us to inform the model about the distribution of such vectors. For example, let us imagine that the game crashed: the frame $f_i$ before the crash is very similar to the previous ones, while the next frame, $f_{i+1}$ is different from $f_i$. As a result, both $\mathit{hsv}_i$ and $s_i$ will be very high. Two of our features (\ie the max of both the vectors) will reflect this information.

Given a training set of labeled video segments, we extract the features and train a ML classifier. Given an input (unknown) video segment, we extract the same features used to train the model, given the resulting vector as input to the trained ML model, and obtain the predicted label. We describe in \secref{sec:design} how we built the training set and how we select the best ML algorithm for this task among Random Forest \citep{ho1995random}, Logistic Regression, SMO \citep{hearst1998support}, Multilayer Perceptron \citep{ramchoun2016multilayer} and IBk\citep{choudhury2015comparative}.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Context-based Segment Grouping} 
\label{sec:contextgrouping}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%
After having collected and categorized segments that contain anomalies (\ie the ones classified as \textit{informative}, \ie as \textit{logic}, \textit{presentation}, \textit{performance}, or \textit{balance}), we group them according to their \textit{context}.
With ``context'' we refer to the part of the game (\eg a specific game level or area) in which the anomaly occurred. This may be helpful to provide the videos to the team in charge of the development of that specific part of the game.
Such a step is important for two reasons: (i) Developers analyzing hundreds of videos related to a specific game may experience information overload and this, in turn, would reduce the effectiveness of the video segments filtering step; (ii) Knowing the context in which more anomalies occur allows the developer to identify where attention needs to be focused to improve the gaming experience.

To achieve this goal, we rely on video information: The assumption is that videos with similar frames regard, most likely, the same context.
First, we extract the key frames from each segment by using the Video-kf Python package \citep{videokf}. Then, we define a summary frame of the whole segment by computing a pixel-by-pixel average of the previously identified key frames. Such a frame will roughly represent the content of the segment and, ideally, it can allow to visually represent the game area.
We use a clustering algorithm to group summary frames (and, thus, the associated segments). More specifically, given a distance function between two images (summary frames, in our case), we define a distance matrix which contains the distances between each couple of summary frames and use it to cluster them.

We test two similarity metrics (which are also used for computing the video-based features in the previous step): Structural similarity (SSIM) \cite{ssim}, computed on each pair of summary frames, and the correlation between the HSV histograms extracted from each pair of summary frames. Note that both of them are \textit{similarity} metrics, while clustering algorithms require to indicate the \textit{distances} between instances. Since both of them are bounded in the range $[0, 1]$, we simply transform them in distance metrics by computing $1 - s$ (where $s$ is the value of the similarity metric).

Since the number of scenes is not necessarily known \emph{a priori}, we use a non-parametric clustering technique. We describe in \secref{sec:design} how we select the best clustering algorithm between the two we tested, \ie DBSCAN \citep{ester1996density} and OPTICS \citep{ankerst1999optics}, and the best distance metric between SSIM and HSV histogram correlation.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Issue-based Segment Clustering}
\label{sec:issuegrouping}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%
A set of video segments of the same kind (\eg bugs) and reported in the same context might still be hard to manually analyze for developers. For example, if 100 segments report bugs for a given level, developers need to manually analyze all of them. It might be the case, however, that most of them report the same specific bug (\eg a game object disappears). To reduce the effort required to analyze such information, we cluster segments reporting the same specific issue. This would allow developers to analyze a single segment for each cluster to have an overview of the problems affecting the specific area of the game.

To achieve this goal, we represent the instances (\ie video segments) by using both textual and image-based features and, as in the previous step, we use non-parametric clustering to create homogeneous groups.
Textual features can help grasping the broad context (\eg objects disappearing or anomalous dialogues). Image-based features can help finding visually similar problems (\eg in the case of glitches). To this aim, we represent each instance (video segment) using the set of features from the categorization step that allows to obtain the best results for that task (as we report in \secref{sec:results}). Differently from the previous step, indeed, we do not pre-compute the distance matrix. This allows us to test this task not only with DBSCAN \citep{ester1996density} and OPTICS \citep{ankerst1999optics}, but also with Mean Shift \citep{fukunaga1975estimation}, which, differently from the previously-mentioned algorithms, does not allow to directly use a distance matrix. Also in this case, we describe in \secref{sec:design} how we select the best clustering algorithm among them.
