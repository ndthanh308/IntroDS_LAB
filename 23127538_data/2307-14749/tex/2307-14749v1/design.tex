\section{Empirical Study Design}
\label{sec:design}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The goal of our study is to evaluate the effectiveness of the four steps of \approach, \ie (i) extraction of meaningful video segments from gameplay videos (ii) accuracy in categorizing extracted video segments, (iii) capability of clustering video segments about the same gameplay area, and (iv) ability to correctly cluster segments reporting the same specific issue. The context of the study consists of a total of 275 gameplay videos.

Our study is steered by the following research questions (RQs).\begin{resultbox}
 \textbf{\RQ{1}}: \textit{How meaningful are the gameplay video segments extracted by \approach?}
\end{resultbox}
The first RQ aims at evaluating the quality of the segments extracted by \approach from gameplay videos in terms of their \textit{interpretability} and \textit{atomicity}. It aims at evaluating the ``video segmentation'' step described in \secref{sec:videosegmentation}.

\begin{resultbox}
 \textbf{\RQ{2}}: \textit{To what extent is \approach able to categorize gameplay video segments?}
\end{resultbox}
With this second RQ we want to understand which features and which classification algorithm allow to train the best model for categorizing gameplay video segments both in two classes (\textit{informative} and \textit{non-informative}, like previous work \cite{lin2019identifying}) and five classes (\textit{logic}, \textit{presentation}, \textit{performance}, \textit{balance}, and \textit{non-informative}). We also want to understand to what extent the best models for the two categorization problems would allow to achieve useful results in practice. \RQ{2} evaluates the ``segment categorization'' step described in \secref{sec:videocategorization}.

\begin{resultbox}
 \textbf{\RQ{3}}: \textit{What is the effectiveness of \approach in grouping gameplay video segments by context?}
\end{resultbox}
In the third RQ, we aim to understand what the best clustering algorithm is for grouping segments based on the game context, and how effective such an algorithm is in absolute terms. This RQ evaluates the clustering step described in \secref{sec:contextgrouping}.

\begin{resultbox}
 \textbf{\RQ{4}}: \textit{What is the effectiveness of \approach in clustering gameplay video segments based on the specific issue?}
\end{resultbox}
Similarly to \RQ{3}, \RQ{4} aims at understanding which features and clustering algorithm allow to achieve the best results for clustering segments based on the specific issue, and how effective such an algorithm is in absolute terms. This RQ evaluates the clustering step described in \secref{sec:issuegrouping}.

%\begin{resultbox}
 %\textbf{\RQ{5}}: \textit{To what extent is the information provided by \approach useful to practitioners?}
%end{resultbox}
%With this last RQ, we want to evaluate \approach as a whole. Specifically, we want to understand the perceived usefulness of the information provided by \approach, and which pieces of information are more relevant than others according to practitioners. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Context Selection}
\label{sec:datasets}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To the best of our knowledge, there are no large-scale, publicly available databases of gameplay videos that provide meaningful information on the classification of problems in video games through subtitle analysis. To answer our RQs and validate the defined approach, we rely on gameplay videos from YouTube. While other platforms, even more video game-oriented, could be used (\eg Twitch), YouTube provides APIs for searching videos of interest and it also allows to download videos including subtitles, which are required by \approach. While subtitles can be automatically generated when the video lacks them, the results could be noisy and, in this phase, we evaluate \approach assuming high-quality input data. \REV{In our study, we collect three datasets, and the criteria used to search for gameplay videos of interest depend on the dataset at hand (explicited in the subsections below).}

The first dataset is composed by video segments, and we use it used for training the supervised model used in step 2 of \approach (\ie segment categorization). We also use this dataset to select the best model for answering \RQ{2}. The second one is composed by complete videos, and we use it for evaluating the single components of \approach and answer \RQ{1-4}. The third one is a smaller dataset used to evaluate the parameters to be used in the different feature extraction and machine-learning techniques. We publicly release all datasets in our replication package \cite{replicationpackage}.

\subsubsection{Training Data}
Our goal is to build a training set of labeled segments containing at least 1,000 instances and covering all the issue types \approach is able to identify.
To select videos possibly useful to build our training set, we used the YouTube Search APIs\footnote{\url{https://developers.google.com/youtube/v3}}. Specifically, we ran a query using the same keywords used by \citet{lin2019identifying}, \ie ``bug'', ``hack'', ``glitch'', ``hacker'', ``cheat'', and ``cheater''. For each keyword, we retrieved a list of videos matching it. We also added a filter to exclude videos without subtitles or with subtitles in languages different from English since \approach relies on NLP-based features computed on them.
Some YouTube videos have manually-defined subtitles, while others have automatically generated ones. We include both of them. Indeed, while it is possible that the second category contains errors, this risk also exists in manually generated ones. Also, the quality of the subtitles generated by YouTube is generally quite high for the English language. As a result, we obtained 3,540 videos. Since some videos were present in more than a list (\ie they matched different keywords), we removed duplicates and obtained 3,196 videos. \REV{We report in \tabref{tab:trainingvideoselection}, for each keyword, the number of videos retrieved and filtered, along with the number of extracted segments. Note that the number of segments might be lower than the number of filtered videos because a video might not contain valid keywords in the subtitles even though it contains them in other metadata, such as the title.}

\begin{table}
 \caption{Number of videos retrieved for each keyword.}
 \label{tab:trainingvideoselection}
 \centering
 \begin{tabular}{lrrr}
  \toprule
  \textbf{Keyword}  & \textbf{\#Videos Retrieved} & \textbf{\#Filtered Videos} & \textbf{\#Segments}\\
  \midrule
  \textit{bug}      &  594  & 514 & 691     \\
  \textit{glitch}   &  509  & 487 & 282     \\
  \textit{hack}     &  514  & 155 & 64     \\
  \textit{hacker}   &  502  & 115 & 66     \\
  \textit{cheat}    &  528  & 145 & 112     \\
  \textit{cheater}  &  549  & 118 & 40     \\
  \bottomrule
 \end{tabular}

\end{table}

Our premise is that several gameplay videos report issues. However, issue-reporting videos represent a minority of the entire gameplay videos population (thus the relevance of our research). Therefore, to support the construction of the dataset containing training data for the categorization step, we relied on the approach defined by \citet{lin2019identifying} and consider only videos identified as issue-reporting. 
Specifically, we re-implemented their approach (since it is not publicly available) and, for each video retrieved as previously described, we ran the approach and discarded the videos classified as non-issue-reporting. As a result, we kept 1,534 videos. We shuffled such videos and manually analyzed them one by one to extract and label segments. 
One of the authors manually split each video into meaningful segments, and two of the authors manually labeled each segment as \textbf{logic}, \textbf{presentation}, \textbf{balance}, \textbf{performance}, or \textbf{non-informative} (when the segment does not report any issue). \REV{Specifically, in order to manually split the video into segments, one of the authors carefully watched each gameplay video, covering its entire duration. During this process, the author noted down the specific starting and ending times (in seconds) for each segment that they identified within the video. The identification of significant segments was guided by a specific criterion based on the classification outlined in Table 1, which can be found in \secref{sec:related:taxonomy}.
With the phrase ``meaningful segments'' we mean video segments that can be analyzed independently as shorter videos and contain enough information that can help achieve the objectives of GELID. To determine whether a segment is ``meaningful,'' as we report later, we use the principles of \textit{interpretability} (to what extent humans can get information from the segment) and \textit{atomicity} (to what extent the segment contain only the information related to a single issue).} At this stage, we discarded segments reporting more than an issue at a time. 
Given the large quantity of videos available compared to the target number of segments we had in mind, we decided to make sure that the training set was diverse in terms of video games considered. Thus, if we noticed that a video game was already taken into account in several videos previously analyzed, we avoided to analyze more videos of it. 
In total, we manually analyzed 170 gameplay videos, totaling about 17 hours of gameplay. As a result, we identified and labeled 1,255 video segments.

Specifically, we obtained 693 non-informative video segments ($\sim$55.2\%), 305 video segments reporting presentation-related problems ($\sim$24.3\%), 169 video segments reporting logic problems ($\sim$13.5\%), 47 video segments with balance problems ($\sim$3.7\%), and 41 video segments highlighting performance problems ($\sim$3.3\%). Given the nature of the problem at hand, as we expected, the dataset is imbalanced, with a great majority of segments being non-informative and a very small percentage of them reporting balance- and performance-related issues.

\subsubsection{Components Validation Data (Test Set)}
To select videos on which we validate the single components of \approach, we focused on a small set of video games. We did this because the third and fourth steps of \approach are reasonable only when segments from the same video game are considered. To select the video games to use, we rely on the information available on Steam, one of the largest video game marketplaces \citep{toy2018large}. \REV{Based on information obtained from Steam} we select three video games that are both popular (\eg for which many gameplay videos exist) and that had several reported issues (\eg for which \approach gives the best advantage). More specifically, we select video games with many downloads and low review scores. To do this, we first retrieved the list of the top 100 most downloaded games on Steam, as reported in \tabref{tab:gamessteam}. Then, we excluded the games with \textit{very positive} or better reviews (\ie we kept the ones with ``mostly positive'' reviews or lower). 
We preliminarily analyzed a random sample of 10 gameplay videos for each video game after this filter using the YouTube search feature. If we found no gameplay videos reporting issues, we discarded the video game. Then, for all the remaining video games, we used the YouTube Search APIs to search for ``\textit{video-game-name} gameplay video''. We applied filters to select only videos with English subtitles (either manually added or automatically generated) and with medium (4-20min) and long (+20min) duration, with the aim of excluding non-informative videos representing game trailers or identifying a compilation of issues (which, instead, were useful to build the training set). Finally, we selected the three video games with the highest number of gameplay videos retrieved, \ie \textit{Conan Exiles} \citep{conan}, \textit{DayZ} \citep{dayz} and \textit{New World} \citep{newworld}. In total, we obtained 80 gameplay videos, totaling about 45 hours of gameplay.

Since manually splitting the entire videos would have been very demanding, we decided to partially rely on the first step of \approach. More specifically, we identified in the subtitles the keywords selected for the segmentation step. Then, one of the authors manually segmented the video near those points to select a first set of possibly relevant segments, and two of the authors independently manually categorized and clustered them both based on the context and on the specific issue (only for informative videos). The two annotators discussed conflicts to reach consensus.
In total, we identified 604 video segments, distributed as depicted in \tabref{tab:testsetdistribution}. It is worth noting that we were able to identify only a few balance-related segments (4 in total, with DayZ having none of them).

\begin{table}
 \newcommand{\Lo}{\faIcon[regular]{bug}}
 \newcommand{\PR}{\faIcon[regular]{ghost}}
 \newcommand{\Pf}{\faIcon[regular]{clock}}
 \newcommand{\Bl}{\faIcon[regular]{balance-scale-right}}
 \newcommand{\NI}{\faIcon[regular]{trash}}

 \caption{Distribution of issue types (logic \Lo, presentation \PR, performance \Pf, balance \Bl, and non-informative \NI) for each video game considered in the test set.}
 \label{tab:testsetdistribution}
 
 \centering 
 \begin{tabular}{l | r r r r r | r}
 \toprule
 \textbf{Video Game} & \Lo   & \PR   & \Pf    & \Bl    & \NI   & \textbf{Total} \\
 \midrule
 Conan Exiles        & 37    & 109   & 10     & 1      & 157   & 314 \\
 DayZ                & 7     & 67    & 16     & 0      & 90    & 180 \\
 New World           & 2     & 44    & 6      & 3      & 55    & 110 \\
 \midrule
 Total               & 46    & 220   & 32     & 4      & 302   & 604 \\
 \bottomrule
 \end{tabular}

\end{table}



\begin{table*}[t]
 \newcommand{\OverwhelminglyPositive}{\faIcon[regular]{check}\faIcon[regular]{check}\faIcon[regular]{check}}
 \newcommand{\VeryPositive}{\faIcon[regular]{check}\faIcon[regular]{check}}
 \newcommand{\MostlyPositive}{\faIcon[regular]{check}}
 \newcommand{\Mixed}{$\mathbf{\thicksim}$}
 \newcommand{\MostlyNegative}{\faIcon[regular]{times}}
 \centering
 \caption{Top 100 most popular games on Steam and related summary review scores (``Overwhelmingly Positive'' \OverwhelminglyPositive, ``Very Positive'' \VeryPositive, ``Mostly Positive'' \MostlyPositive, ``Mixed'' \Mixed, and ``Mostly Negative'' \MostlyNegative)}
 \label{tab:gamessteam}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{llll}
    \toprule
    \textbf{Video game}               & \textbf{Review}         & \textbf{Video game}                         & \textbf{Review}          \\
    \midrule
    CS:GO                             & \VeryPositive           & BeamNG drive                                & \OverwhelminglyPositive  \\
    Pubg                              & \Mixed                  & Counter strike                              & \OverwhelminglyPositive  \\
    Dota 2                            & \VeryPositive           & RimWorld                                    & \OverwhelminglyPositive  \\
    GTA V                             & \VeryPositive           & World of Tanks Blitz                        & \VeryPositive            \\
    Tom Clancy's Rainbow Six® Siege   & \VeryPositive           & The Elder Scrolls V: Skyrim Special Edition & \VeryPositive            \\
    Team fortress 2                   & \VeryPositive           & NARAKA Bladepoint                           & \MostlyPositive          \\
    Terraria                          & \OverwhelminglyPositive & Hunt: Showdown                              & \VeryPositive            \\
    Garry's Mod                       & \OverwhelminglyPositive & Civilization V                              & \OverwhelminglyPositive  \\
    Rust                              & \VeryPositive           & Project Zomboid                             & \VeryPositive            \\
    Apex                              & \VeryPositive           & Factorio                                    & \OverwhelminglyPositive  \\
    Wallpaper Engine                  & \OverwhelminglyPositive & Smite                                       & \MostlyPositive          \\
    The Witcher® 3: Wild Hunt         & \OverwhelminglyPositive & The elder scrolls online                    & \VeryPositive            \\
    Warframe                          & \VeryPositive           & theHunter: Call of the Wild™                & \VeryPositive            \\
    Destiny 2                         & \VeryPositive           & Age of Empires II: Definitive Edition       & \VeryPositive            \\
    Cyberpunk 2077                    & \MostlyPositive         & Satisfactory                                & \OverwhelminglyPositive  \\
    Dead by Daylight                  & \VeryPositive           & Stellaris                                   & \VeryPositive            \\
    ARK                               & \VeryPositive           & Fifa 22                                     & \VeryPositive            \\
    Elden ring                        & \VeryPositive           & Forza Horizon 5                             & \VeryPositive            \\
    Stardew Valley                    & \OverwhelminglyPositive & Squad                                       & \VeryPositive            \\
    Euro track simulator 2            & \OverwhelminglyPositive & The sims 4                                  & \VeryPositive            \\
    Rocket League                     & \VeryPositive           & Europa Universalis IV                       & \VeryPositive            \\
    Phasmophobia                      & \OverwhelminglyPositive & Scum                                        & \MostlyPositive          \\
    Payday 2                          & \VeryPositive           & Stumble Guys                                & \VeryPositive            \\
    The forest                        & \OverwhelminglyPositive & Assetto Corsa                               & \VeryPositive            \\
    War Thunder                       & \MostlyPositive         & Conan Exiles                                & \MostlyPositive          \\
    Valheim                           & \OverwhelminglyPositive & FINAL FANTASY XIV ONLINE                    & \VeryPositive            \\
    Brawlhalla                        & \VeryPositive           & Crusader Kings III                          & \VeryPositive            \\
    Red dead redemption 2             & \VeryPositive           & Yugioh Master Duel                          & \MostlyPositive          \\
    DayZ                              & \MostlyPositive         & Left for dead                               & \OverwhelminglyPositive  \\
    Don't Starve together             & \OverwhelminglyPositive & eFootball 2023                              & \MostlyNegative          \\
    Sea of thieves                    & \VeryPositive           & Black desert                                & \MostlyPositive          \\
    New World                         & \Mixed                  & Soundpad                                    & \OverwhelminglyPositive  \\
    Geometry Dash                     & \VeryPositive           & Total War: Warhammer 3                      & \MostlyPositive          \\
    Bloons TD 6                       & \OverwhelminglyPositive & Fallout 76                                  & \MostlyPositive          \\
    The binding of Isaac: Rebirth     & \OverwhelminglyPositive & Warhammer 40,000: Darktide                  & \Mixed                   \\
    Path of exile                     & \VeryPositive           & Moster Hunter Rise                          & \VeryPositive            \\
    Hades                             & \OverwhelminglyPositive & Coockie clicker                             & \OverwhelminglyPositive  \\
    Fallout 4                         & \VeryPositive           & EA SPORTS™ FIFA 23                          & \Mixed                   \\
    VR Chat                           & \MostlyPositive         & Farming Simulator 22                        & \VeryPositive            \\
    Lost Ark                          & \MostlyPositive         & Victoria 3                                  & \Mixed                   \\
    Civilization VI                   & \VeryPositive           & Goose Goose Duck                            & \VeryPositive            \\
    7 days to die                     & \VeryPositive           & Undecember                                  & \Mixed                   \\
    Mount  Blade II: Bannerlord       & \VeryPositive           & Mir4                                        & \Mixed                   \\
    Vampire Survivors                 & \OverwhelminglyPositive & Footbal Manager 2022                        & \VeryPositive            \\
    Cities: Skylines                  & \VeryPositive           & Dwarf fortress                              & \OverwhelminglyPositive  \\
    TmodLoader                        & \OverwhelminglyPositive & Nba 2K23                                    & \Mixed                   \\
    Arma 3                            & \VeryPositive           & Project: Playtime                           & \Mixed                   \\
    Deep rock Galactic                & \OverwhelminglyPositive & Divinity: Original Sin 2 - Definitive Edition                            &   \OverwhelminglyPositive                       \\
    Hearth of Iron IV                 & \VeryPositive           & Paragon the Overprime                       & \Mixed                   \\
    Call of Duty®: Modern Warfare® II & \Mixed                  & Football Manager 2023                       & \VeryPositive            \\
    \bottomrule
\end{tabular}
}
    \vspace{-0.3cm}
\end{table*}

% \subsection{Tuning Data}%%?????????????
% We defined a third dataset, aiming at using mostly for tuning the hyperparameters of the classifier we use in step 2 of \approach. To do this, we use the exact same procedure previously described for building the test, but with a fourth game, \ie GTA V.  As a result, we identified 100 video segments based on the analysis of 25 gameplay videos, totaling about 2 hours of gameplay. Each video segment was categorized according to the information it contained (36 presentation, 25 logic, 5 balance, 3 performance, 31 non-informative).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Procedure}
\label{sec:execution}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
We summarize in \figref{fig:plan} our plan for answering the four research questions, and we provide the details below.

% Figure environment removed


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Research Method for \RQ{1}: \REV{Meaningfulness of Extracted Segments}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \noindent
% \begin{resultbox}
% \REV{\textbf{\RQ{1}}. \textit{How meaningful are the gameplay video segments extracted by \approach?}}
% \end{resultbox}
To answer \RQ{1}, we evaluate the technique we defined with different values of $t$ (streamer reaction times). Specifically, we instantiate our approach with $t$ in the set $\{0, 5, 10\}$ seconds. 
We ran the first step of \textit{Video Segmentation} on selected gameplay videos for each video game in the test set, collecting a total of 101 video segments. \REV{Note that the number of extracted segments is lower than the number of videos because some videos might not contain any keyword we use in the \textit{Video Segmentation} step to retrieve candidate relevant segments (see \secref{sec:videosegmentation}). Consequently, if a video does not contain any of these keywords, no segment is extracted from it.}

We evaluated the segments detected by each variant of our approach in terms of their (i) \textit{interpretability} (\ie it is possible to watch the segment and acquire all the information needed to understand what has been experienced by the streamer) (ii) the \textit{atomicity} (\ie it is not possible to further split the segments). Such aspects are complementary: It would be possible to maximize the \textit{interpretability} by creating few segments (\eg just one for the whole video); this, however, would result in lower \textit{atomicity} since the segments could be further divided into parts. \REV{While we would have ideally wanted to capture the ``quality'' of segments as a whole, it is quite hard to define a precise metric for such a complex aspect. Thus, we preferred to use two specific and easy-to-evaluate aspects instead. Concerning the relationship between such aspects and quality as a whole, we can say that, given two segments A and B, if interpretability(A) $>$ interpretability(B) and atomicity(A) $>$ atomicity(B), then quality(A) $>$ quality(B). On the other hand, if we have conflicting situations (\eg interpretability(A) $<$ interpretability(B) and atomicity(A) $>$ atomicity(B)), we can not say whether the quality of A is greater or lower than the quality of B.} 

Two of the authors watched the segments generated by each variant, for a total of 303 evaluations, and manually annotated each segment in terms of its \textit{interpretability} and \textit{atomicity} on a 5-point Likert scale. As for the first metric, we evaluated to what extent we could fully understand what is happening based only on the segment itself. As for atomicity, instead, we assessed whether the segment can be further divided in additional standalone (fully interpretable) segments. The final score was computed as 5 minus the number of additional standalone segments that could be further extracted, or 1 if more than four standalone segments were found. Each of the 303 manually analysed slices was independently inspected.
We report the inter-rater reliability between the annotators by using the Cohen's kappa coefficient \citep{cohen1960coefficient, wan2015kappa}. Then, for each segment, we compute the mean \textit{interpretability} and \textit{atomicity}.
Finally, we compare the tested techniques in terms of such metrics using a Mann-Whitney U test \citep{mann1947test, macfarland2016mann}, and adjusting the $p$-values resulting for multiple comparisons using the Benjamini and Hochberg procedure \citep{benjamini1995controlling}. 
We also report the effect size, using the Cliff's delta \citep{cliff1993dominance}, to understand the magnitude of differences observed.

\subsection{Research Method for \RQ{2}: \REV{Segment Categorization Effectiveness}}
% \begin{resultbox}
%  \REV{\textbf{\RQ{2}}: \textit{To what extent is \approach able to categorize gameplay video segments?}}
% \end{resultbox}
To answer \RQ{2}, we use all the three datasets previously described. We aimed at evaluating not only the complete approach on a multi-class categorization problem (the four informative classes reported in \tabref{tab:taxonomy}, plus the \textit{non-informative} class), but also its version on a simplified version of the same problem, \ie a binary classifier (\textit{informative}, \textit{non-informative}) like the one defined by \citet{lin2019identifying}. It is worth noting, however, that we could not compare our results with the ones obtained with such an approach because it is designed to work only on entire videos, not on segments.

As a first step, we aimed at selecting (i) the best machine learning algorithm, (ii) the best set of features, and (iii) the best preprocessing pipeline for categorizing gameplay video segments in both scenarios. As candidate machine learning algorithms, we selected Random Forest \citep{ho1995random}, Logistic Regression, SMO \citep{hearst1998support}, Multilayer Perceptron \citep{ramchoun2016multilayer} and IBk\citep{choudhury2015comparative}. We used the implementations available in the Weka toolkit.\footnote{\url{http://www.cs.waikato.ac.nz/ml/weka/}} At this stage, we used the default hyperparameters available in Weka for each of them. As candidate set of features, as explained in \secref{sec:approach}, we considered three textual-based sets of features (Bag of Words, word2vec, and doc2vec), a video-based set of feature, and a mixed set of features (including both the best set of textual features and the video-based set of features).
As candidate preprocessing pipelines, we considered the use of SMOTE \citep{SMOTE}, which allows to generate synthetic instances for balancing the training set, and a two-step attribute selection approach: We first rank the features based on their respective information gain and we discard the ones with score 0; then, we run a wrapper attribute evaluator \citep{gnanambal2018classification} to select the best subset of features in terms of AUC achieved by a simple kNN model with $k=3$. More specifically, we considered four options: the use of SMOTE alone, the use of our two-step attribute selection alone, the use of both of them, and the use of none of them.
At this stage, we relied on the training set, and we performed a 10-fold cross validation for all the combinations of ML algorithms, feature sets, and preprocessing pipelines for both the problems (binary and multi-class). For each of them, we compute and report the achieved AUC (Area Under the ROC curve \citep{bradley1997use}) \citep{flach2016roc}. An AUC of 0.5 indicates a model having the same prediction accuracy of a random classifier. A perfect model (\ie zero false positives and zero false negatives) has instead AUC = 1.0. Thus, the closer the AUC to 1.0, the higher the model performances.
In the end, we select the combination that allows achieving the highest score both for the binary and the multi-class model.

Finally, as a third step, we ran the best models on the test set to understand to what extent the models would be useful in practice. In this case we report not the AUC, but also the \textit{precision}, \textit{recall}, and \textit{F-measure} scores. \textit{Precision} is computed as $\frac{\mathit{TP}}{\mathit{TP}+\mathit{FP}}$ and \textit{recall} is computed as $\frac{\mathit{TP}}{\mathit{TP}+\mathit{FN}}$, where \textit{TP}, \textit{FP}, and \textit{FN} indicate the number of \textit{true positives}, \textit{false positives}, and \textit{false negatives}, respectively. \textit{F-measure} is computed as the harmonic mean of \textit{precision} and \textit{recall}.

\subsection{Research Method for \RQ{3}: \REV{Contextual Clustering Effectiveness}}
% \begin{resultbox}
%  \REV{\textbf{\RQ{3}}: \textit{What is the effectiveness of \approach in grouping gameplay video segments by context?}}
% \end{resultbox}
To address \RQ{3}, we tested the two non-parametric clustering techniques described in \secref{sec:approach}, \ie DBSCAN \citep{ester1996density}, OPTICS \citep{ankerst1999optics} with two distance metric, \ie HSV and SSIM.

Both DBSCAN and OPTICS require to set an $\epsilon$ parameter, which indicates the minimum distance to be used to consider two instances belonging to the same cluster. However, determining the input parameter values can be very difficult. For both non-parametric clustering techniques, we decide the value of $\epsilon$ by using a well-known procedure \citep{ozkok2017new}. Specifically, we (i) calculate the distance between each point and its nearest neighbour, (ii) sort the distances in ascending order, (iii) compute, for each pair of consecutive distances, their difference $\Delta_i$ = $d_{i+1}$ - $d_i$), and (iv) set $\epsilon = \max(\Delta_i)$. We used this procedure independently for each clustering operation we run (\ie each combination of video game and similarity metric).

We compare the results of the algorithms with the ground-truth partition produced in the manual clustering of the test set to evaluate this step of \approach. To do this, we use the MoJo eFfectiveness Measure (\textit{MoJoFM}) \citep{wen2004effectiveness}, a normalized variant of the MoJo distance. \textit{MoJoFM} is computed using the following formula:
$$
MoJoFM(A,B)  = 100 - (\frac{mno(A,B)}{max(mno(\forall E_{A}, B))} \times 100)
$$
\noindent where $mno(A,B)$ is the minimum number of \emph{Move} or \emph{Join} operations one needs to perform in order to transform a partition $A$ into a different partition $B$, and $max(mno(\forall \; E_{A}, B)$ is the maximum possible distance of any partition $A$ from any partition $B$. \textit{MoJoFM} returns 0 if partition $A$ is the farthest partition away from $B$; it returns 100 if $A$ is equal to $B$. 

We report the MoJoFM obtained for each combination of game and metric considered.

\subsection{Research Method for \RQ{4}: \REV{Specific Issue-Based Clustering Effectiveness}}
% \begin{resultbox}
%  \REV{ \textbf{\RQ{4}}: \textit{What is the effectiveness of \approach in clustering gameplay video segments based on the specific issue?}}
% \end{resultbox}
To answer \RQ{4}, we tested the same clustering techniques considered in \RQ{3} (DBSCAN \citep{ester1996density} and OPTICS \citep{ankerst1999optics}) plus a third (\ie Mean Shift \citep{fukunaga1975estimation}) which we could not use in \RQ{3} because it can not use custom distance metrics. 
We start from the ground-truth clusters manually defined in the test set. For each of them, we run the issue-based clustering approach defined in \secref{sec:approach} on the instances belonging to them. We use the same procedure described in \RQ{3} to define the $\epsilon$ hyperparameters for DBSCAN and OPTICS for each clustering operation. This time, we do not report the $\epsilon$ values used for space reasons (given the higher number of clustering operations).
We report, like for \RQ{3}, the MoJoFM score achieved for each video game.

\subsection{Replication Package}
We publicly release in our replication package \citep{replicationpackage} the datasets used in each research question, the ARFF files used to train and test the machine learning techniques, the raw data of our manual analyses for each research question, and additional data that did not fit in our paper. We also publicly provide the implementation of each step of \approach.
