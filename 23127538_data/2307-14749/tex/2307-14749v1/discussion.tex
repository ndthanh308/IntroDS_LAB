\section{Discussion}
\label{sec:discussion}

% In summary, our experiments show that it is possible to automatically extract segments from gameplay videos, but we are still far from being able to automatically identify the issue they report and to cluster them based on the game context. Instead, we are able to classify the segments regarding the same specific issue, assuming that the two previous steps work perfectly. 
The main problems we encountered are in the automated categorization of issues in gameplay video segments and in the context-based segment clustering (steps 2 and 3 of \approach).  

First, it is worth noting that our results partially contrast the ones obtained by \citet{lin2019identifying}, who defined a classifier able to correctly distinguish \textit{informative} from \textit{non-informative} gameplay videos. Segment-level categorization is a much harder problem than video-level categorization. This is confirmed by the fact that even simplifying our five-class categorization problem in binary categorization problem (similarly to the one addressed by \citet{lin2019identifying}, but on segments), we still obtain negative results (58\% F-Measure, with 0.61 AUC). We have some hypothesis on why this is the case. First, videos have metadata (such as tags, descriptions, and so on) that segments lack. \citet{lin2019identifying} used such metadata, but we could not use them in our context. If a video is specifically aimed at reporting issues (\ie it contains a compilation of game errors), it is very likely that the authors explicitly mention this in the description. Gameplay video subtitles, instead, are much more noisy. 

We observed that, often, the subtitle sentences are incomplete and ambiguous (\eg ``logics, bro. Well, I talk all'' used for a \textit{logic} problem, ``they are lower than that'' used for a \textit{presentation} problem, and ``less well-known logic that's arguably one'' used for a \textit{performance} problem). To some extent, this happens because the comment corresponding to the portion of the video in which the issue appears might not be in sync with the issue itself: Gamers might talk about the issues even several minutes after it appears. It is worth noting that this problem is not related to the automated segmentation, because in evaluating step 2 with \RQ{2} we used manually-defined segments. The problem is in the lack of (logical) sync between what streamers say and when what they say happens on screen.
Future work could consider a larger context for extracting the features (\eg the surrounding $n$ seconds, with even large values of $n$) instead of only considering the subtitles related to the specific segment. \REV{The idea based on the possibility of using a larger context stems from the assumption that expanding the context of observation allows for a broader view of what is happening in the specific gameplay video, thus in the game, and allows more features to be extracted.}

\begin{resultbox}
 \textbf{Lesson Learned 1.} Considering a larger context for extracting textual features might allow obtaining better results.
 
 \noindent \REV{\textbf{Future Research Idea 1.}  To overcome this limitation, future research could aim to consider a larger portion of video both before and after the given identified segment.}
\end{resultbox}

\REV{Using keywords to detect possibly useful segments of the gemeplay videos might be detrimental. Indeed, there may be segments without streamer comments, that would be completely ignored. These are blind spots for GELID. To address this limitation, it may be necessary to develop new and specialized approaches to detect specific problems, such as glitches or stuttering events.}

Related to this, another problem we noticed by analyzing some examples is that streamers sometimes comment on their gaming experience in an irregular manner, often even through simple exclamations (\eg ``the glitch myself?'' for \textit{performance}, ``BAM!'' for \textit{logic}, ``and there!'' for \textit{presentation}).
Catching those issues is probably infeasible by only relying on textual information. Similarly, we can observe a performance problem found in a gameplay video of New World:\footnote{\url{https://youtu.be/1duizy5DSOg?t=1540}} The game temporarily freezes while the player is running, but they say \textit{``here can see one right now okay stop doing that let's start running they're nasty big aren't they''}, referring to what is happening in the game. Automatically categorizing this kind of issue is, again, extremely challenging, and a more specific approach would be needed.
\REV{Another limitation of GELID is related to the fact that it only relies on gameplay videos in English. Future work is needed to generalize it to other languages. In CLAP \citep{scalabrino2017listening}, an attempt has been made to deal with this issue. The authors tried to translate the input textual information (in the context of GELID, subtitles) from foreign languages into English and then use the normal approach (which works on English) to deal with them. However, this solution proved to be unsuccessful. In this paper, we use word2vec: It would be possible to test the effectiveness of word2vec models trained on other languages. Based on the negative results obtained for English, which is quite widespread, we believe that the implementation of such an approach cannot be successful at present.}
\begin{resultbox}
 \textbf{Lesson Learned 2.} Sometimes, textual features are not useful at all since the streamers use generic exclamations to report issues. 
 
 \noindent \REV{\textbf{Future Research Idea 2.} Future research could aim at taking into account the slang used by streamers and to define a vocabulary of the terms most commonly used to describe different kinds of issues or to define specialized approaches to detect issues mostly based on the videos rather than on the captioned spoken content.}
\end{resultbox}

When looking at the multi-class categorization, the problem is even more evident in terms of general effectiveness of the model. We report in \tabref{tab:multiclass_confusion_matrix} the confusion matrix for the multi-class categorization model. While the model correctly identifies 81 \textit{presentation} issues, it correctly detects only 2 \textit{logic}-related issues and, again, no \textit{performance}- and \textit{balance}-related issue. More interestingly, the model often categorizes \textit{presentation}-related issues as \textit{logic} issues, while the opposite happens relatively less frequently. In general, instead, the model tends to confuse the specific categories of instances as \textit{presentation}-related, probably because it is the most frequent informative type of issue.

We analyzed some misclassified instances, aiming at getting some insights on why the model tends to confuse some \textit{presentation} issues for \textit{logic} issues and why it is not able to correctly identify \textit{performance} and \textit{balance} instances.
We found an interesting example in DayZ. The streamer says \textit{``my doesn't seem to be archived it back back is so annoying''},\footnote{\url{https://youtu.be/eDQIdqDC-sc?t=239}} but the model probably confuses the indication of an ``annoying'' circumstance for something related to a functional issue (\textit{logic}), while, in this case, it was referred to a \textit{presentation} issue.

\begin{resultbox}
 \textbf{Lesson Learned 3.} Given the strong class unbalance, categorization does not work well for detecting \textit{performance} and \textit{balance} problems. Approaches specifically designed for finding such categories of issues might be needed.
 
 \noindent \REV{\textbf{Future Research Idea 3.} To increase the number of \textit{balance} and \textit{performance} instances, it could be useful to look for and specifically take into account video games that are or have been notorious for such problems.} 
\end{resultbox}

\begin{table*}[h]
\newcommand{\Lo}{\faIcon[regular]{bug}}
\newcommand{\PR}{\faIcon[regular]{ghost}}
\newcommand{\Pf}{\faIcon[regular]{clock}}
\newcommand{\Bl}{\faIcon[regular]{balance-scale-right}}
\newcommand{\NI}{\faIcon[regular]{trash}}
\centering
\caption{\RQ{2}: Confusion Matrix for multi-class categorization on all the instances (Conan Exiles, DayZ, and New World). The columns indicate the categories assigned by the classifier, while the rows indicate actual ones.}
\label{tab:multiclass_confusion_matrix}
%\resizebox{\linewidth}{!}{%
\begin{tabular}{l|rrrrr}
         & \NI             & \PR          & \Lo        & \Pf         & \Bl         \\ 
\midrule                                                   
\NI      & \textbf{218}    & 75           & 5          & 0           & 0           \\
\PR      & 126             & \textbf{81}  & 8          & 0           & 0           \\
\Lo      & 27              & 16           & \textbf{2} & 0           & 0           \\
\Pf      & 18              & 13           & 0          & \textbf{0}  & 0           \\
\Bl      & 3               & 1            & 0          & 0           & \textbf{0}  \\
\bottomrule
\end{tabular}
%}
\end{table*}


Another possible reason behind the failure in categorization could be related to the procedure used to define the training set: To collect an adequate number of instances, we considered videos that explicitly report issues (\ie that contain keywords such as ``bug'' in their title or description). It is possible that these videos are intrinsically different from the long gameplay videos we used for testing the models. To check if this is the case, we trained/tested two classifiers (both for binary and multi-class categorization) based on the best configurations found in \RQ{2} by using 10-fold cross validation on the test set alone, both globally and by considering the instances of single games. We report the results in \tabref{tab:testastraining}. We observed a clear increase in the effectiveness of both the models, with the binary classification model achieving $\sim$82\% accuracy on two games. While more data would be necessary, the results of this analysis suggest that videos explicitly reporting issues are too different from long gameplay videos (that we aim to target) in which issues sometimes appear. Thus, it would be more appropriate to build the training set using the same procedure used to build the test set, even if this require a much bigger effort (it would not be possible, for example, to use the approach by \citet{lin2019identifying} as a filter). Also, using a training set composed of only game-specific instances might allow to achieve better results (even if we observed this only for two games out of three). \REV{In detail, again, a training set defined on a specific game allows for more precise information in relation to the game area/level. For example, open world games have very similar game areas, so a large amount of data would allow a more precise distinction to be made between the different game areas in which users find themselves.}


\begin{resultbox}
 \textbf{Lesson Learned 4.} A training set built on long gameplay videos not specifically aimed at reporting issues might help achieving better results. Also, game-specific training might help increasing the model accuracy.
  
 \noindent \REV{\textbf{Future Research Idea 4.} Future research should verify what is the impact of the type of video, \ie long and generic gameplay videos or short and focused gameplay videos reporting issues, on the performance of the four steps of GELID.}
\end{resultbox}

\begin{table}
 \caption{Accuracy and AUC achieved by training/testing the best models for binary and multi-class categorization on the test set alone using 10-fold cross validation.}
 \label{tab:testastraining}
 \centering
 \begin{tabular}{l|rr|rr}
\toprule
\multirow{2}{*}{\textbf{Game}}     & \multicolumn{2}{c|}{\textbf{Binary}} & \multicolumn{2}{c}{\textbf{Multi-class}} \\
                                   & Accuracy    & AUC        & Accuracy & AUC  \\
\midrule
{Conan Exiles}                     & 81.7\%      & 0.89       & 71.7\%   & 0.73 \\
{DayZ}                             & 64.7\%      & 0.75       & 59.0\%   & 0.56 \\
{New World}                        & 81.7\%      & 0.89       & 67.9\%   & 0.72 \\
\midrule                                                                                                                                                                                                
{Combined}                         & 72.7\%      & 0.79       & 59.9\%   & 0.63 \\
\bottomrule
\end{tabular}
\end{table}

As for the context-based segment categorization (step 3 of \approach), as we previously mentioned while analyzing the results, the poor performance can be due to the fact that some games have visually similar, but logically different game areas/levels. Some video games might suffer from this issue more than others. In our case, we observed that our approach  (specifically, the variant based on HSV histogram correlation, which achieves the best results) works reasonably well on New World, but remarkably bad on Conan Exiles. For the video games on which our approach does not work well, a more sophisticated (and game-specific) approach might be used, which should be specialized on the game at hand so that, for example, it is able to distinguish the specific game areas by recognizing specific game elements.

\begin{resultbox}
 \textbf{Lesson Learned 5.} A game-specific approach for recognizing the game area/level might be needed for some video games.
 
 \noindent \REV{\textbf{Future Research Idea 5.} Researchers should test the impact of introducing game- or game-genre-specific features on the effectiveness of the context-based clustering.}
\end{resultbox}

