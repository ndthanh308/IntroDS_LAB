
\vspace{-2mm}
\section{Related Works} \vspace{-2mm}
\label{sec:related_works}

\mypara{Scaling visuo-linguo-motor data.}

An increasing area of interest in robot learning has been in vision-and-language-conditioned motor (``visuo-linguo-motor'') policies \cite{lynch2020language,stepputtis2020language,jang22a,shridhar2021cliport,shridhar2022peract,lynch2022interactive,mees2022matters,brohan2022rt}, and one of the most important underlying questions is how to scale data collection.  %
Perhaps the most prevalent paradigm is to use humans to annotate both actions and language.
Using humans to provide action labels, i.e. teleoperation, has been prodigiously used for real-world skill learning~\cite{lynch2020language,stepputtis2020language,jang22a,shridhar2021cliport,shridhar2022peract,lynch2022interactive,mees2022matters,brohan2022rt}.
Within this category, humans can either provide episodic task-specific expert demonstrations \cite{jang22a,shridhar2021cliport,shridhar2022peract,brohan2022rt}, or non-task-specific (``play'') data~\cite{lynch2020language,stepputtis2020language,mees2022calvin,lynch2022interactive}.
A primary limitation, however, is that data scalability is human-limited.

Other prior works have proposed strategies to enable more-autonomously-scalable data.  To scale language annotation, prior works study using visual-language models~\cite{xiao2022robotic}, or procedurally post-hoc provided in simulation~\cite{mees2022calvin}.  To scale the action labels, methods study
how to use {\em{autonomous sub-optimal policies}} that are still viable for effective downstream learning, from random policies~\cite{fu2020d4rl} to more costly options like learned policies using behavior cloning or reinforcement learning (RL)~\cite{nair2022learning}.
As another option for scalability not depending on robots, human ego-vision data~\cite{goyal2017something,grauman2022ego4d,damen2018scaling} has been shown to be relevant to robot learning~\cite{chen2021learning,nair2022r3m}, but lacks ``action'' (motor) labels, and requires cross-embodiment transfer.
While these approaches reduce human efforts, they are still limited in optimality, generality, and have not been demonstrated to provide all of visuo-linguo-motor data generation all in one scalable framework without human annotation involvement. %

Another option for the autonomous data collection policy is to use a model-based policy, e.g. from task and motion planning (TAMP)~\cite{garrett2021integrated}.  Our method is in this category, but we improve it with an LLM - this greatly aids in the flexibility and task generality by leveraging LLM's common-sense knowledge.  In contrast to recent works exploring the use of LLMs as robotic task planners (\ie as the \emph{final} policy)~\cite{huang2022language,ahn2022can,huanginner,codeaspolicies2022,driess2023palm,lin2023text2motion}, we use the LLM-based planner as a suboptimal \emph{data-collection} policy. We then distill this data from a privileged-information policy into an observable-information \cite{agarwal2022legged,seita2019deep,miki2022learning} policy in a way that allows us to learn a better policy than the LLM-based planner.   %

\mypara{Policy Representations and Multi-task Policy Distillation.} One primary question in visuo-motor learning~\cite{levine2016end} has been how to represent the policy for effective learning, i.e. to enable high precision, multi-modal robot behavior~\cite{florence2021implicit,hausman2017multi,shafiullah2022behavior,chi2023diffusionpolicy,zhao2023learning}.  Another related question has been how to best train multi-task policies \cite{yu2020meta,kalashnikov2021mt}, including those conditioned on language~\cite{lynch2020language,jang22a,shridhar2021cliport,shridhar2022peract,lynch2022interactive,brohan2022rt}.  Our work presents the novel formulation of bringing diffusion-based \cite{sohl2015deep,ho2020denoising} policies~\cite{chi2023diffusionpolicy} into the language-conditioned~\cite{saharia2022photorealistic,rombach2022high} visuomotor domain.   Additionally, prior works in multi-task language-conditioning typically focus on cloning policies from experts, meanwhile we study distilling offline data from a suboptimal policy, typically studied under offline RL~\cite{levine2020offline}.







