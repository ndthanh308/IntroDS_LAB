\vspace{-2mm} \subsection{Limitations} \vspace{-2mm}


By using priviledged simulation state information, the LLM can infer success conditions which uses ground truth contact, joint information, and object poses.
This means our implementation of the data generation phase is limited to simulation environments, and our policy requires sim2real transfer.
Further, while our approach's dataset contains text labels and success labels for all subtasks, we have only evaluated its effectiveness in learning the root task.
Learning from all subtasks and growing a robot's set of learned, reusable sub-skills over time to enable compositional generalization is left for future work.
Lastly, extending the set of 6 DOF Exploration Primitives (\eg sampling-based model-predictive control planners) will further expand the flexibility of our system, and is an interesting future direction.


\vspace{-2mm}
\section{Conclusion}
\vspace{-2mm}

We proposed ``Scaling Up and Distilling Down'', a framework that combines the strengths of LLMs, sampling-based planners, and policy learning into a single system that automatically generates, labels, and distills diverse robot exploration experience into a multi-task visuo-linguo-motor policy.
The distilled policy inherits long-horizon behaviour, rich low-level manipulation skills, and robustness from its data collection policy while improving upon performance beyond its training distribution.
We believe that this integrated approach is a step towards putting robotics on the same scaling trend as that of LLM development while not compromising on the rich low-level control.
