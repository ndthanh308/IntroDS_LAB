\vspace{-2mm}
\section{Evaluation} \vspace{-2mm}
\label{sec:experiment}

Our experiments try to validate two questions:
1) Can our data generation approach efficiently perform task-directed exploration?
2) Can our policy learning approach effectively distill a multi-modal, multi-task dataset into a generalizable and robust visuo-linguo-motor policy?


\input{text_v2/benchmark_v1.tex}




\vspace{-1mm}
\textbf{Metrics.}
We report the average success rate over 200 episodes for each task.
If a domain has multiple tasks then we report the average performance of all tasks' average in the domain.
We report the quantitative results in Table~\ref{tab:sim}, an task completion efficiency plot in Fig.~\ref{fig:result:efficiency}, and qualitative results in Fig.~\ref{fig:result:retry_from_failure}.

\vspace{-1mm}
\textbf{Data Generation Baselines.}
Code-as-Policy~\cite{codeaspolicies2022} is a state-of-the-art approach for using an LLM directly as a robot policy by making state (\eg query present objects) and action primitive API calls to a robot.
Given an LLM-inferred code string, they execute the snippet in an open-loop fashion.
Crucially, in their table top manipulation setting, they assume access to planar action primitives.
Thus, we introduce the following baselines, which build on top of Code-as-Policy and each other as follows:
\vspace{-2mm}
\begin{itemize} [leftmargin=3mm]
      \item \textbf{LLM-as-Policy (2D)}: Similar to code-as-policy (2D pick-and-place) with ground truth segmentation.
      \item \textbf{(+) 6 DOF robot utils}: Builds on top of the previous baseline by adding access to 6 DOF robot utilities for grasping, placement, motion planning, and articulated manipulation.
      \item \textbf{(+) Verify \& Retry}:
            Adding to the previous baselines, this baseline uses the LLM's predicted success condition to label trajectories and retry failed ones.
            Since the robot utilities involve pseudo-random samplers (\eg RRT, grasp sampling), retrying the task means running these samplers again using the pseudo-random state and environment state from where failed trajectory left it.
            Since we use this approach as our data generation policy, it also serves as an ablation of our approach.
            \vspace{-2mm}
\end{itemize}

\textbf{Policy Distillation Baselines.}
\vspace{-2mm}
\begin{itemize} [leftmargin=3mm]
      \item \textbf{Distill No Retry} trains on successful trajectories generated from the data generation approach without Verify \& Retry.
            This means it does not observe any recovery behavior.
\end{itemize}


\vspace{-2mm}
\subsection{Data Collection Policy Evaluation}
\vspace{-2mm}



% Figure environment removed


\textbf{6DoF exploration is critical.}
First, we verify different approach's ability to perform and explore in 6DoF, which is crucial for general manipulation.
When 6DoF exploration is introduced, we observe a drop in the average success rate for simple tasks that could be accomplished with planar actions (Balance, Transport, Tab. \ref{tab:sim}).
However, this ability is critical for successfully exploring complex tasks.
In particular, we observed that 6DoF actions are important for grasping diverse objects with complex geometry (Transport, Tab. \ref{tab:sim}), and manipulating articulated objects (Drawer, Mailbox, Tab. \ref{tab:sim}).

\input{tables/sim}

Moreover, 6DoF exploration also helps in \textbf{diversifying} the data collection strategy, which provides the \textbf{possibility to improve upon} in the later distilling stage.
Catapult tries to grasp the button and place it on the catapult.
For example in the catapult domain, LLM-as-Policy (2D) is only able to solve one of three possible goals (the closest bin) using a deterministic strategy.
However, it provides no useful data for learning the other two goals, making it a poor data-collection policy.
In contrast, incorporating 6 DOF robot utilities achieves lower but non-zero average success rates in all bins ($16.3\%$, $3.3\%$, and $2.2\%$, full table in appendix), which provide much better exploration data for downstream policy distillation.


\textbf{Verify \& Retry always helps.}
Through the verify \& retry step, the LLM can retry a task after each failed attempt until its inferred success condition passes.
This simple addition improves performance across all domains, achieving
$2\times$, $3\times$, $8\times$, and $13\times$ in transport, catapult, balance, and drawer domains respectively.
In fact, without this crucial step, we observe $0.0\%$ success rate in the mailbox domain, underscoring the challenging long-horizon 6 DOF manipulation requirements of the domain, the difficulty of flawlessly executing a long sequence of 6 DOF actions, and the importance of recovery after a failure.

\vspace{-2mm}
\subsection{Distilled Policy Evaluation}
\vspace{-2mm}


\begin{wrapfigure}{r}{0.45\textwidth}
      \vspace{-14mm}
      \centering
      % Figure removed
      \vspace{-5mm}
      \caption{ \footnotesize \textbf{Distilled Robustness}.
            \textbf{\textcolor{plt_green}{Our policy}} inherits robust recovery from failure behavior from \textbf{\textcolor{plt_purple}{its data collection policy}}, while improving upon success rate.
      }
      \label{fig:result:efficiency}
      \vspace{-6mm}
\end{wrapfigure}

\textbf{Robustness In, Robustness Out.}
By filtering trajectories with LLM's inferred success condition, distilled policies inherit the robustness of their data collection policies while improving upon success rates ($+23.4\%$ and $+33.2\%$ for no-retry and ours, Tab. \ref{tab:sim}).
Since our distilled policy learned from a robust data collection policy, it also recovers from failures (\eg failed grasps or placements) and continuously retries a task until it succeeds.
Meanwhile, since the no-retry distilled policy learned from a data collection policy which did not retry upon failure, it is sensitive and brittle, leading to $-34.8\%$ lower average success rate across all domains compared to ours (Tab. \ref{tab:sim}).

\textbf{High Performance From Diverse Retry Attempts.}
Plotting how long policies take to solve the balance task (Fig.~\ref{fig:result:efficiency}), we observed that our policy and its data collection policy continuously tries a diverse set of grasps and placements after each failed attempt until it succeeds.
This results in higher success rates as the policy is given more time, and is reflected in their monotonically increasing success rates.
In contrast, baselines plateau after their first grasp/platement attempts.
This highlights the synergy of two design decisions.
First, the verify \& retry step (\S~\ref{sec:method:datagen:verifying}) is crucial for demonstrating retrying behavior, but is by itself \emph{insufficient} if each retrying action is the identical as the previous one.
Instead, opting for a diffusion policy (\S~\ref{sec:method:distill}) for learning from and generating high-entropy, diverse retry attempts (Fig~\ref{fig:result:retry_from_failure}) is also essential for high performance.



\textbf{Sim2Real Transfer.}
We evaluated a distilled policy which was trained on domain randomized (lighting, texture, camera field of view and position) synthetic data in a real world transport domain with five novel objects (Fig.~\ref{fig:result:retry_from_failure}e).
Averaging across ten episodes per object, our policy achieved 76\% success rate, demonstrating the effectiveness of our approach in sim2real transfer.
We provide more details on real world evaluation in the appendix.