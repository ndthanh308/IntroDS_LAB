\section{Policy Rollout Visualizations}
\label{sec:supp:policy_rollout_vis}

Our policy's 6DoF manipulation behavior is best visualized through videos.
Please visit our \href{https://www.cs.columbia.edu/~huy/scalingup/}{project website} to view the videos.

\section{LLM Prompts}
\label{sec:supp:prompts}

Below, we include all prompts used in our approach.
We use the same LLM pipeline and prompts in all domains and tasks.
We first outline the rationale behind our design of the LLM pipeline (\S~\ref{sec:supp:prompts:design}).
Next, we describe in detail the LLM modules and how they are used in the data generation stage (\S~\ref{sec:supp:prompts:pipeline}), summarize the general prompt structure (\S~\ref{sec:supp:prompts:structure}), and outline the API supplied to the LLM for success condition inference (\S~\ref{sec:supp:prompts:state_api}).
Finally, we show some examples of LLM completions (\S~\ref{sec:supp:prompts:completions}).

In all of our experiments, we use GPT3 (text-davinci-003) with temperature 0.0.

\subsection{LLM Pipeline Design}
\label{sec:supp:prompts:design}


Our LLM pipeline is factorized into multiple LLM modules, allowing each module's prompt to speciallize in a small reasoning skill (\eg one set of prompts for deciding whether a task involves a single or multiple objects).
We found that this not only improves the LLM's performance, but also makes designing and maintaining prompts easy.
For instance, during development, if the LLM outputs an unexpected task tree, the error could be traced back to a single module, and only that module's prompt needs to be updated.
Another convenient feature of this approach is that it also saves on token usage.
Since each module's task is small (\eg answer only ``one'' or ``multiple''), the amount of completion tokens is significantly smaller than a monolithic prompt.
Further, when a module's prompt is updated, only that module's outputs needs to be updated, allowing cost-effective approaches to cache-ing LLM's completions.

\subsection{LLM Pipeline}
\label{sec:supp:prompts:pipeline}


The recursive LLM-based planner starts with an ambiguous task description handler (Listing~\ref{list:supp:prompts:ambiguous}), which transforms ambiguous task descriptions such as ``move the block onto the catapult then shoot the block into the furthest bin'' into more specific task descriptions like ``move the block onto the catapult then shoot the block into the furthest bin by pressing the catapult's button''.
While this handler's task can occasionally overlap with the LLM planner's task, we found that it was more effective to keep them separate.

Next, given a un-ambiguous task description, the LLM planner first decides whether the planning step is necessary by checking whether the task involves touching only a single object or requiring further decomposition (Listing~\ref{list:supp:prompts:single_multiple}).
If the task involves multiple objects, it proceeds with planning (explained in the next paragraph).
If the task involves only one object part, an LLM identifies which object part name it should interact with (Listing~\ref{list:supp:prompts:obj_part}).
If the object part name is a single-link rigid object, the LLM is asked for which object it should move (the pick object part) and where (the place object part) using the prompt in Listing~\ref{list:supp:prompts:pick_place}.

In the planning step, the LLM planner outputs a list of subtasks (Listing~\ref{list:supp:prompts:planning}).
Given the recursive nature of this planning module, parent tasks also need to keep track of and propagate the current state of the environment to child tasks.
For instance, the ``open the fridge'' subtask should be followed with ``with the fridge door opened, move the eggs from the fridge ...'', such that the recursive call for moving the eggs knows it does not need to open the fridge door again.

After it has inferred the full task tree, the LLM also infers a success condition for every task in the task tree (Listing~\ref{list:supp:prompts:success}) in the form of a code-snippet.
Similar to ~\cite{codeaspolicies2022}, we inform the LLM which state API utilities are available for its usage by including import statements at the top of the file and demonstrating how they are used in the examples.

\subsection{Prompt Structure}
\label{sec:supp:prompts:structure}
All prompts start with instructions to explain to the LLM what the task is (\eg ``given an input task description, the goal is to output a list of subtasks ..''), followed by a few ``shots'' of examples, separated by a ``\#'' symbol (in text-based prompts) or a multi-line comment (in code-based prompts).
Each shot starts with a structured text encoding of the scene's object's and their parts' names in the form of a bullet list.
In the planning, success condition inference, single-or-multiple , pick-and-place, and ambiguous task description LLM tasks,
we found that it was helpful to encourage the LLM to output its reasoning (either with an explicit ``reasoning:'' field or through in-line code comments).
In contrast, we found the object part identifier task to be more effective without this explicit reasoning field.

\subsection{APIs for Success Condition Code Generation}
\label{sec:supp:prompts:state_api}

All functions take as the first argument the simulation state, which contains information on object and part names, kinematic structure, contact, all degrees of freedom, and collision meshes.

\mypara{Contact.}
This function takes as input two object (part) names, and returns whether they (or any of their parts) are in contact.

\mypara{Activation.}
A pair of functions, \texttt{check\_activated} and \texttt{check\_deactivated}, take as input an object part name and checks whether the revolute/prismatic joint connecting the object part to its parent link are near their maximum or minimum values, respectively.
This is useful for checking whether a lid is opened/closed or a button is pressed/released.

\mypara{Spatial Relations.}
We provide two spatial relations, \texttt{check\_on\_top\_of} and \texttt{check\_inside}, which takes two object (part) names and returns whether the first object (part) is on top of the second object (part) or inside the second object (part), respectively.
An object is on top of another if they are in contact and the contact normal's dot product with the up direction is greater than 0.99.
An object is inside a container if that the intersection of that object's axis-aligned bounding boxes with the container's axis-aligned bounding boxes is at least 75\% of the object's axis-aligned bounding box's volume.
This axis-aligned bounding box information can be parsed from the collision checker of most physics simulators.


\lstinputlisting[label={list:supp:prompts:ambiguous},language={},caption=Ambiguous task description handler's prompts]{text_v2/prompts/v4_ambiguous_handler.txt}

\lstinputlisting[label={list:supp:prompts:single_multiple},language={},caption=One-or-Multiple module's prompts]{text_v2/prompts/v7_one_or_multiple.txt}
\lstinputlisting[label={list:supp:prompts:obj_part}, language={},caption=Object part identifier's prompts]{text_v2/prompts/v1_object_part.txt}

\lstinputlisting[label={list:supp:prompts:pick_place},language={},caption=Pick \& place handler's prompts]{text_v2/prompts/v1_pick_place.txt}

\lstinputlisting[label={list:supp:prompts:planning},language={},caption=Planning module's prompts]{text_v2/prompts/v4_subtasks.txt}


\lstinputlisting[label={list:supp:prompts:success},language=Python,caption=Success Condition Inference module's prompts]{text_v2/prompts/high_level_api_v6.py}

\subsection{Example Completions}
\label{sec:supp:prompts:completions}

Most modules are designed such that their answers are multi-choice (\eg single or multiple).
We show examples of more interesting open-ended completions for the ambiguous task description handler, planning, and success condition inference modules.
The LLM's outputs are highlighted in blue.

\subsubsection{Ambiguous Task Description Handler}
\label{sec:supp:prompts:completions:ambiguous}

\noindent\fbox{\parbox{0.97\linewidth}{\scriptsize{\texttt{{%
\query{task: move the block onto the catapult arm, then press the button to shoot the block into the closest box}\\
{\color{prompt-gray}scene:\\
- catapult\\
\hspace*{4mm} + button\\
\hspace*{4mm} + catapult arm\\
- yellow block\\
- closest box\\
- furthest box\\
- middle box\\
- table\\
reasoning:} \hlcode{which block to move onto the catapult arm is ambiguous. we can specify exactly}\\
\hlcode{which block to move onto the catapult arm.}\\
\hlcode{answer: move the yellow block onto the catapult arm, then press the button to shoot the block}
\hlcode{into the closest box.}
}}}}}\\

\noindent\fbox{\parbox{0.97\linewidth}{\scriptsize{\texttt{{%
\query{task: send the amazon package for return}\\
{\color{prompt-gray} scene:\\
- mailbox\\
\hspace*{4mm}+ mailbox lid\\
\hspace*{8mm}+ mailbox lid handle\\
\hspace*{4mm}+ mailbox flag\\
- amazon package\\
- table\\
reasoning:} \hlcode{which actions to perform and in which order is ambiguous. we can specify exactly}\\
\hlcode{ which actions to take.}\\
\hlcode{answer: open the mailbox lid, place the amazon package inside the mailbox, raise the mailbox}\\
\hlcode{flag, and then close the mailbox lid.}
}}}}}\\

\subsubsection{Planning}
\label{sec:supp:prompts:completions:planning}

\noindent\fbox{\parbox{0.97\linewidth}{\scriptsize{\texttt{{%
\query{task: move the yellow block onto the catapult arm, then press the button to shoot the block into the closest box}\\
{\color{prompt-gray} scene:\\
- catapult\\
\hspace*{4mm}+ button\\
\hspace*{4mm}+ catapult arm\\
- yellow block\\
- closest box\\
- furthest box\\
- middle box\\
- table\\
reasoning:}
\hlcode{the catapult has a button (activation state) which starts off de-activated. it }\\\hlcode{needs to be pressed to shoot the block. after the task is done, the button should be reset to}\\\hlcode{its de-activated state.
}\\\hlcode{answer:}\\\hlcode{
   - 1. move the yellow block onto the catapult arm}\\\hlcode{
   - 2. with the yellow block on the catapult arm, press the button to shoot the block into the}\\\hlcode{ closest box}\\\hlcode{
   - 3. with the yellow block in the closest box, reset the button to its de-activated state}
}}}}}\\



\noindent\fbox{\parbox{0.97\linewidth}{\scriptsize{\texttt{{%
\query{task: open the mailbox lid, place the amazon package inside the mailbox, raise the mailbox flag, and then close the mailbox lid}\\
{\color{prompt-gray} scene:\\
- mailbox\\
\hspace*{4mm}+ mailbox lid\\
\hspace*{8mm}+ mailbox lid handle\\
\hspace*{4mm}+ mailbox flag\\
- amazon package\\
- table\\
reasoning:}
\hlcode{the mailbox lid has an activation state (closed/de-activated). it needs to be}\\\hlcode{opened before the package can be placed inside. after the task is done, the lid needs to be}\\\hlcode{closed (reset).}\\\hlcode{answer:}\\\hlcode{- 1. open the mailbox lid
}\\\hlcode{- 2. with the mailbox lid opened, move the amazon package from the table into the mailbox                                                                                                                                                     }\\\hlcode{- 3. with the amazon package in the mailbox, raise the mailbox flag                                                                                                                                                                           }\\\hlcode{- 4. with the amazon package in the mailbox and the mailbox flag raised, close the mailbox lid}
}}}}}\\

\subsubsection{Success Condition Inference}
\label{sec:supp:prompts:completions:success}

\noindent\fbox{\parbox{0.97\linewidth}{\scriptsize{\texttt{{%
\query{\# robot task: move the yellow block onto the catapult arm}\\
{\color{prompt-gray}\# scene:\\
\# - catapult\\
\#\hspace*{4mm} + button\\
\#\hspace*{4mm} + catapult arm\\
\# - yellow block\\
\# - closest box\\
\# - furthest box\\
\# - middle box\\
\# - table\\
def} \hlcode{{\color{code-function}yellow_block_on_catapult_arm}(init_state: EnvState, final_state: EnvState):}\\
\hlcode{{\color{code-syntax}return} {\color{code-function}check\_on\_top\_of}(final_state, {\color{code-constant}"yellow block"}, {\color{code-constant}"catapult arm"})}
}}}}}\\

\noindent\fbox{\parbox{0.97\linewidth}{\scriptsize{\texttt{{%
\query{\# robot task: open the mailbox lid}\\
{\color{prompt-gray}\# scene:\\
\# - mailbox\\
\# \hspace*{4mm}+ mailbox lid\\
\# \hspace*{8mm}+ mailbox lid handle\\
\# \hspace*{4mm}+ mailbox flag\\
\# - amazon package\\
\# - table\\
def} \hlcode{{\color{code-function}mailbox_lid_opened}(init_state: EnvState, final_state: EnvState):}\\
\hlcode{{\color{code-syntax}return} {\color{code-function}check_activated}(final_state,  {\color{code-constant}"mailbox lid"})}
}}}}}\\

\section{Training \& Data Details.}

\subsection{Data Generation}

Our data-collection policy uses the 6DoF Exploration Primitives with the Verify \& Retry step.
For each domain, we run data generation until we get at least 500 successful trajectories per task.
Although this can be costly when tasks are long horizon with low success rates (the mailbox domain took 2 days on 256 CPU cores Intel Xeon Gold 6230R CPU @ 2.10GHz), data generation happens only once.

\subsection{Network Architecture \& Hyperparameters}

We use the same network architecture and hyperparameters for all domains.
Our task descriptions are encoded using CLIP B/32's text encoder~\cite{radford2021learning}, and projected into a 512-dimensional vector.
For each of the two camera view, we learn a separate Resnet18-based~\cite{robomimic2021} vision encoder, whose features are flattened, concatenated, and projected into a 512-dimensional vector.
The Resnet18 architecture is pre-processed by replacing BatchNorm with GroupNorm and replacing the final average pool layer with a spatial softmax pooling~\cite{robomimic2021,chi2023diffusionpolicy}.
We use an image resolution of $160\times 240$ for each view, processed with a 90\% random crop to $144\times 216$.
Finally, the proprioception is concatenated with the vision and text encoder as the condition into the diffusion policy.
We use the convolution network-based diffusion policy architecture~\cite{chi2023diffusionpolicy}.
The final network has 108 million parameters.
All networks are optimized end-to-end with the AdamW optimizer, with 5e-5 learning rate and 1e-6 weight decay, and a cosine learning rate scheduler.
For evaluation, we use an exponential moving average of all networks with a decay rate of 0.75.

\subsection{Training}

We train a separate multi-task policy for each domain using the same hyperparameters and network architecture.
For domains with only a single task, this amounts to a single-task policy.
All networks are trained for 2 days on a single NVIDIA A6000, and the best checkpoint's performance is reported.
We found that performance typically saturates around 1 day into training.

\section{Utilities Implementation}

For motion planning, we implemented rapidly-exploring random trees (RRT~\cite{lavalle1998rapidly}) with grasped-object-aware collision checking, allowing the robot to motion plan with dynamic grasping constraints.
The geometry-based grasp and placement sampler is implemented using point clouds created from depth maps, camera matrices, and segmentation maps from the simulator.
While our grasp sampler uses only geometry, kinematics, and contact information, including other grasp quality metrics (\eg stability analysis) can improve its performance.
In the placement sampler, we sample candidate place positions at points whose estimated contact normal is aligned against the gravity direction.
The revolute and prismatic joint motion primitives are implemented by checking the grasp pose relative to the joint (\eg mailbox lid handle grasp relative to the mailbox lid hinge), then performing a circular motion around the joint axis or a linear motion along the joint axis, respectively.


% Figure environment removed


\section{Benchmark}
\label{sec:domains_supp}

Our benchmark is built on top of the Mujoco~\cite{todorov2012mujoco} simulator, using assets from the Google Scanned Objects dataset~\cite{downs2022scannedobjects,zakka2022scannedobjectsmujoco}.
We use a table-top manipulation set-up, with a WSG50 gripper and Toyota Research Institute Finray fingers mounted on a UR5e, with a policy control rate of 4Hz.
The workspace has two cameras, one front view, which observes the entire workspace and robot, and a wrist-mounted camera, which is used to help with fine-grained manipulation~\cite{robomimic2021}.
We end episodes when any object is dropped to the floor.
Below, we clarify how we design the tasks for each domain.



\subsection{Mailbox}
\label{sec:domains_supp:mailbox}

To be considered successful, the mailbox needs to be closed with the package inside the mailbox, with the mailbox flag raised within 200 seconds (800 control cycles).
During data generation and testing, the package's planar position is uniformly random in a planar bound of dimensions [10cm, 10cm].
At evaluation, the policy has to generalize to unseen package positions.
The amazon has is a rigid object with 6DoF.
The mailbox is a fixed rigid object, with one degree of freedom for each of its revolute joints, one for the mailbox lid, and one for the mailbox flag.


\subsection{Transport}
\label{sec:domains_supp:bin_transport}

To be considered successful, the toy needs to be inside the left bin within 100 seconds.
At the beginning of each episode, a random toy 3D asset is sampled.
During data generation and testing, the toy's position is uniformly random inside the right bin, and orientation uniformly random along all three euler axes.
On top of novel randomized poses, the policy also has to generalize to unseen object instances with novel geometry.
We use 22 toys for data generation, and 8 for testing (Fig.~\ref{fig:bin_transport_panel}).
The toy is a rigid object with 6DoF, while the bins are fixed rigid objects with no DoF.
The bin asset names corresponds with their spatial location (\eg the left bin is called ``left bin'' when the scene is presented to the LLM).

\subsection{Drawer}
\label{sec:domains_supp:drawer}

This is a multi-task domain with 12 tasks, where each task involves moving one of the four objects (vitamin bottle, pencil case, crayon box, horse) into one of the three drawers (top, middle, bottom).
The task description follows the template ``move the $\langle$\textit{object}$\rangle$ into the $\langle$ \textit{drawer}$\rangle$''.
To be considered successful, the specified object needs to be inside the specified drawer within 120 seconds.
During data generation and testing, each of the four object's position is uniformly random within a planar bound of dimensions [10cm,10cm], centered around 4 evenly spaced locations along the table.
At test time, the policy has to generalize to unseen object positions in the same distribution as its data generation.

All four objects are rigid objects with 6DoF.
The drawer is a fixed articulated object with 3 DoF, one for each of the drawers.

\subsection{Catapult}
\label{sec:domains_supp:catapult}

This is a multi-task domain with 3 tasks, one for each of the three bins.
The task description follows the template ``move the block onto the catapult arm, then press the button to shoot the block into the $\langle$\textit{bin}$\rangle$'' where $\langle$\textit{bin}$\rangle$ is either closest, middle, or furthest bin.
The bin asset names corresponds with their spatial location (\eg the furthest bin is called ``furthest bin'' when the scene is presented to the LLM).

In order to be considered successful, the block needs to be inside the specified bin within 60 seconds.
This is a short amount of time, which prevents policies from retrying after failure.
The block is a rigid object with 6DoF.
The bins are fixed rigid objects with no degrees of freedom.
The catapult has two degrees of freedom, one revolute joint for the catapult arm, and one prismatic joint for the button.
This task is designed to study tool-use, and does not have any pose randomization.
Thus, different seeds affect only the policy's pseudo random samplers or the diffusion process.

We implement the catapult with a special callback function which checks whether the button sliding joint is near its max value.
If it is, then the constraint that holds the catapult arm down is disabled, releasing the spring loaded catapult arm hinge joint.



\input{tables/sim_drawer_full.tex}


\subsection{Bus Balance}
\label{sec:domains_supp:balance}

In order to be considered successful, the bus needs to be fully balanced on top of the block within 100 seconds.
On top of testing for intuitive physics, this high precision requirement of this task was also used to test the policy's precision and ability to recover from failure, which is why we allow a generous time budget.
The task description is ``balance the bus on the block''.

The bus is a rigid object with 6DoF, dropped from a fixed location above the table with uniformly random orientation.
This means when the bus drops, it lands in different positions and orientations.
The block is fixed with no degrees of freedom.

\input{tables/sim_full.tex}

\section{Full Results}
We include the full results for all tasks in the drawer domain in Table~\ref{tab:sim_full_drawer}, and all other domains in Table~\ref{tab:sim_full}.
We omit data generation baseline numbers on the train set in the transport domain, since they are non-learning approaches.
All approaches are evaluated on 200 different seeds, which controls pose randomization, which asset is sampled, the pseudo-random robotic utility samplers, and the pseudo-random diffusion process.
We make one exception in the catapult domain, where due to the low success rates of getting the block into the middle and far bin, we run evaluation until there are 500 successful trajectories per task, then report the average success rate.
Since the time limit for the catapult is short, the data-collection policy will not have enough time to retry, leading to identical numbers with the baseline data-collection policy without verify \& retry.

In the drawer domain, we observe that the task is more difficult for:
\begin{enumerate}
   \item \textbf{Larger objects:}
         The most challenging objects are the vitamin bottle and the horse toy, both of which are too large to fit the drawer if they are in an upright orientation.
         This means to be effective at this task, the robot should perform sideway grasps on these objects, such that downstream placement is easier.
         In contrast, the small crayon box is has the highest success rates amongst the data-collection policies.
   \item \textbf{Top drawer:}
         We observe interacting with this drawer often brings the robot close to its kinematic reach range.
         This means slight imprecision in the policy's predicted actions or small shifts in the grasped object (which is unaccounted for during motion planning) in execution could lead to failure.
         For instance, while moving the objects inside the top drawer, the grasped object could collide with the drawer, causing the grasped object to drop or the drawer to close.
   \item \textbf{Planar Action Primitives:}
         A top-down grasp on the drawer handle will typically be in collision with the drawer's body.
         Thus, in LLM-as-Policy (2D)'s first action to open the drawer, its call to the motion planner will fail due to an invalid goal configuration.
\end{enumerate}

% Figure environment removed


\begin{wrapfigure}{l}{0.48\textwidth}
   \centering
   \vspace{-5mm}
   % Figure removed
   \caption{ \footnotesize \textbf{Real World Objects}.
   }
   \label{fig:real_world:objs}
   \vspace{-2mm}
\end{wrapfigure}


\section{Real World Evaluation}

We train a separate policy for real-world transfer on domain randomized scenes (Fig.~\ref{fig:domain_rand}).
We evaluate our policy on a real UR5e robot with a WSG50 gripper and Toyota Research Institute Finray fingers, matching our simulation set-up.
We use five unseen objects (Fig.~\ref{fig:real_world:objs}), ranging in shape, size, and visual appearance.
Each object is evaluated on 10 episodes, with the object placed at a random pose on the right bin.
We observe 70\%, 80\%, 60\%, 80\%, and 90\% for the pear, monster, rubiks cube, fetch controller, and mustard bottle respectively, giving a mean success rate of 76\%.










