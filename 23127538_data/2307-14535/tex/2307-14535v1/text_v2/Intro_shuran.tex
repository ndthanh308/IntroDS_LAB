% Figure environment removed

\vspace{-2mm}
\section{Introduction}
\vspace{-2mm}
How can we scalably acquire robust, reusable, real-world manipulation skills? This question has been the driving force behind extensive research in robot learning.
Attempts in the field have focused on two primary aspects:
First, how to \textbf{scale up the data collection} for a diverse range of manipulation skills, which involves efforts such as improving the hardware~\cite{song2020grasping,zhao2023learning} and software~\cite{todorov2012mujoco,zhu2020robosuite} which support demonstration collection, utilization of non-robotics datasets~\cite{nair2022r3m,grauman2022ego4d}, or trial-and-error explorations~\cite{fu2020d4rl}.
The second aspect of this question concerns \textbf{effective learning} from the offline data~\cite{levine2020offline}, which delves into
dataset filtering~\cite{florence2021implicit,chen2021decision} and
exploring effective observation~\cite{zhu2022viola}, action~\cite{wu2020spatial,shridhar2022peract,shridhar2021cliport,zeng2021transporter}, and policy~\cite{florence2021implicit,chi2023diffusionpolicy} formulations that can robustly model the training data and generalize to novel scenarios.


This paper proposes a new framework that provides a comprehensive solution for both aspects by leveraging language guidance, while using no expert demonstrations or reward specification/engineering.
We contribute two key components with our framework:

\begin{itemize}[leftmargin=3mm]
   \vspace{-2mm}
   \item  \textbf{Language-Guided Data Generation (Scaling Up): }
         Our data-collection policy is a large language model (LLM) which has access to a suite of 6DoF exploration primitives (\ie sampling-based robot planners and utilities).
         Given an input task description, this policy first \textbf{simplifies} the task by recursively decomposing it into subtasks, resulting in a hierarchical plan (\ie task tree).
         Next, this plan is \textbf{grounded} into a sequence of 6DoF exploration primitives, which generates diverse robot trajectories for the task.
         Finally, the data collection policy \textbf{verifies} the trajectories' success with an inferred success function and \textbf{retries} the task until it succeeds.
         This verify \& retry step not only improves the data-collection policy's success, but also adds robot experience on how to recover from failure, an important feature for downstream policy distillation.
         This data generation approach is scalable, enabling significantly more efficient autonomous task-directed exploration than unguided alternatives (\ie reinforcement learning) while not being limited by the lack of low-level understanding of the LLM-only solution.
   \item  \textbf{Language-Conditioned Visuomotor Policy (Distilling Down):}
         We distill these robot experiences into a visuo-linguo-motor policy that infers control sequences from visual observations and a natural language task description.
         To enable effective learning of high entropy, diverse robot trajectories, we extend the diffusion policy formulation~\cite{chi2023diffusionpolicy} to handle language-based conditioning for multi-task learning.
         This allows the learned policy to be reused and recomposed through language-based planners.
         We found that our distilled policy successfully learned the robust retrying behavior from its data collection policy, while improving upon its absolute success rate across five domains by $36.3\%$.
         Further, we demonstrate that our policy directly transfers to the real-world without fine-tuning using domain randomization.
         \vspace{-2mm}
\end{itemize}

Our framework combines these two components to get the best of both worlds -- leverage LLM's common-sense reasoning abilities for efficient exploration while learning robust and re-usable 6DoF skills for real-world deployment.
In summary, the key contribution of this paper is a new framework for \textbf{visuo-linguo-motor policy learning} that is enabled by three novel components:
\begin{itemize}[leftmargin=3mm]
   \vspace{-2mm}
   \item A new language-guided data collection framework that combines language-based task planner with 6DoF robot utilities (\eg motion planning, grasp sampling).
   \item New formulation of diffusion-based policy that effectively learns multi-task language-conditioned closed-loop control policies.
   \item In addition to our algorithmic contributions, we also contribute a new multi-task benchmark that includes 18 tasks across five domains, requiring long-horizon ($\approx 800$ control cycles), common sense, tool-use, and intuitive physics understanding -- capabilities lacking in existing manipulation benchmarks.
         \vspace{-2mm}
\end{itemize}