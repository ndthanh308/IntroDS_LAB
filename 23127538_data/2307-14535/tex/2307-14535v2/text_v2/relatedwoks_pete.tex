
\vspace{-3mm}
\section{Related Works} \vspace{-3mm}
\label{sec:related_works}

\mypara{Scaling visuo-linguo-motor data.}
In learning vision-and-language-conditioned motor policies for real-world deployment~\cite{lynch2020language,stepputtis2020language,jang22a,shridhar2021cliport,shridhar2022peract,lynch2022interactive,mees2022matters,brohan2022rt}, one of the most important questions is how to scale up ``robot-complete data'' -- data that has robot sensory inputs (\eg vision), action labels (\eg target end-effector \& gripper commands), and task labels (\eg language description, success).
The most prevalent paradigm is to use humans to annotate both actions (\eg teleoperation) and language~\cite{lynch2020language,stepputtis2020language,jang22a,shridhar2021cliport,shridhar2022peract,lynch2022interactive,mees2022matters,brohan2022rt}.
When providing action labels, humans can either provide task-specific~\cite{jang22a,shridhar2021cliport,shridhar2022peract,brohan2022rt}, or task-agnostic (``play'') data~\cite{lynch2020language,stepputtis2020language,mees2022calvin,lynch2022interactive}.
A primary limitation, however, is that data scalability is human-limited.

Other prior works have proposed strategies to enable more-autonomously-scalable data.
To scale language annotation, prior works study using visual-language models~\cite{xiao2022robotic,zhang2023sprint}, or procedurally post-hoc provided in simulation~\cite{mees2022calvin}.
To scale action labels, methods study how to use {\em{autonomous sub-optimal policies}} from random~\cite{fu2020d4rl} to learned~\cite{nair2022learning} policies.
Human egocentric videos~\cite{goyal2017something,grauman2022ego4d,damen2018scaling} has also been shown to be relevant to robot learning~\cite{chen2021learning,nair2022r3m}, but \emph{is not robot-complete} (lacks action labels), and requires cross-embodiment transfer.
Towards unsupervised exploration, prior works have also investigated evolving environments~\cite{wang2019paired,jiang2021replay} and embodiments~\cite{mouret2015illuminating}, automatic task generation~\cite{fang2022active}, leveraging language guidance~\cite{du2023guiding,mirchandani2021ella} and world-model error~\cite{mendonca2023alan}, but have not been demonstrated to scale to 6 DoF robotic skill learning.
While these approaches reduce human efforts, they are still limited in optimality, generality, and/or completeness of robot data labels.

Another option for the autonomous data collection policy is to use a model-based policy, \eg task and motion planning (TAMP)~\cite{garrett2021integrated}.
Our approach extends such methods in terms of flexibility and task generality by leveraging LLM's common-sense knowledge.
However, in contrast to recent works which use LLMs as the \emph{final} policy~\cite{huang2022language,ahn2022can,huanginner,codeaspolicies2022,driess2023palm,lin2023text2motion,singh2023progprompt}, we use the LLM-based planner as a suboptimal \emph{data-collection} policy.
We then distill only successful trajectories into an observable-information~\cite{agarwal2022legged,seita2019deep,miki2022learning} policy, allowing the distilled policy to improve upon its LLM data collection policy's performance.


\mypara{Policy Representations and Multi-task Policy Distillation.} One primary question in visuo-motor learning~\cite{levine2016end} has been how to represent the policy for effective learning, i.e. to enable high precision, multi-modal robot behavior~\cite{florence2021implicit,hausman2017multi,shafiullah2022behavior,chi2023diffusionpolicy,zhao2023learning}.  Another related question has been how to best train multi-task policies \cite{yu2020meta,kalashnikov2021mt}, including those conditioned on language~\cite{lynch2020language,jang22a,shridhar2021cliport,shridhar2022peract,lynch2022interactive,brohan2022rt}.  Our work presents the novel formulation of bringing diffusion-based \cite{sohl2015deep,ho2020denoising} policies~\cite{chi2023diffusionpolicy} into the language-conditioned~\cite{saharia2022photorealistic,rombach2022high} visuomotor domain.   Additionally, prior works in multi-task language-conditioning typically focus on cloning policies from experts, meanwhile we study distilling data from a success-filtered suboptimal policy.  Success-filtering \cite{florence2021implicit,chen2021decision} can be viewed as the simplest form of offline RL \cite{levine2020offline}.
