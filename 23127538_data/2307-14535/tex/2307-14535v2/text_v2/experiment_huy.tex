
\vspace{-2mm}
\section{Evaluation}

\vspace{-2mm}
\label{sec:experiment}

\input{tables/benchmark}

Our experiments try to validate two questions:
1) Can our data generation approach efficiently perform task-directed exploration?
2) Can our policy learning approach effectively distill a multi-modal, multi-task dataset into a generalizable and robust visuo-linguo-motor policy?

\input{text_v2/benchmark_v1.tex}






\vspace{-1mm}
\textbf{Metrics.}
We report the success rates (\%) averaged over 200 episodes in Table~\ref{tab:sim}, a task completion efficiency plot in Fig.~\ref{fig:result:efficiency}, and qualitative results in Fig.~\ref{fig:result:retry_from_failure}.
If a domain has multiple tasks then we report the average performance of all tasks.
We also compare different LLMs in Table~\ref{tab:llm} (10 samples per task) and investigate the sources of error in our system for the mailbox domain in Table~\ref{tab:failure_analysis} (200 trials per execution).

\vspace{-1mm}
\textbf{Data Generation Baselines.}
Code-as-Policy~\cite{codeaspolicies2022} is a state-of-the-art approach for using an LLM directly as a robot policy by making state (\eg query present objects) and action primitive API calls to a robot.
Given an LLM-inferred code string, they execute the snippet in an open-loop fashion.
Crucially, in their table top manipulation setting, they assume access to planar action primitives.
Thus, we introduce the following baselines, which build on top of Code-as-Policy and each other as follows:
\vspace{-2mm}
\begin{itemize} [leftmargin=3mm]
    \item \textbf{LLM-as-Policy (2D)}: Similar to code-as-policy using planar pick-and-place, but we use ground truth object segmentation instead of their off-the-shelf object detectors~\cite{kamath2021mdetr,gu2021open}.
    \item \textbf{(+) 6 DOF robot utils}: Builds on top of the previous baseline by adding access to 6 DOF robot utilities for grasping, placement, motion planning, and articulated manipulation.
    \item \textbf{(+) Verify \& Retry}:
          Adding to the previous baselines, this baseline uses the LLM's predicted success condition to label trajectories and retry failed ones.
          Since the robot utilities involve pseudo-random samplers (\eg RRT, grasp sampling), retrying the task means running these samplers again using the pseudo-random state and environment state from where failed trajectory left it.
          Since we use this approach as our data generation policy, it also serves as an ablation of our approach.
          \vspace{-2mm}
\end{itemize}


% Figure environment removed


\textbf{Policy Distillation Ablations.}
We compare against BC-Z~\cite{jang22a}'s single-task policies which does not use FiLM conditioning (used in their bin emptying and door opening tasks).
To understand the effects of our policy learning design decisions in the single-task regime, we fix training time and dataset size (2 days using at least 500 successful trajectories), and provide the following ablations:
\vspace{-2mm}
\begin{itemize} [leftmargin=3mm]
    \item \textbf{Action Generation}:
          Instead of using diffusion processes conditioned on the policy input embedding to decode actions, it is typical use multi-layer perceptrons.
          Following \citeauthor{jang22a}~\cite{jang22a}, we use one \textbf{MLP} with two hidden layers and ReLU activations for end effector position, one for the orientation, and another for gripper command.
          This standard policy architecture is deterministic, and is trained with mean-squared error loss for pose and binary cross entropy loss for gripper command.
    \item \textbf{Action Space}:
          Besides our absolute end effector pose action space, \textbf{Delta-Action} and velocity control spaces is another popular action space choice~\cite{jang22a,robomimic2021,zhang2018deep,florence2019self,mandlekar2020iris}.
          We also ablate BC-Z's execution action horizon (Exec) while keeping their original prediction horizon (Pred).
    \item \textbf{Observation Encoder}: All approaches encode images using a ResNet18~\cite{he2016deep} architecture. Although the original architecture was designed with an average pooling layer, its typical for robotic policies to use a spatial softmax pooling~\cite{levine2016end} layer instead.
    \item \textbf{Data usage}: \textbf{No-Retry} trains on successful trajectories generated from the data generation approach without Verify \& Retry, so it does not observe any recovery behavior.
\end{itemize}


\vspace{-2mm}
\subsection{Data Collection Policy Evaluation}
\vspace{-2mm}
\input{tables/sim}




\textbf{6DoF exploration is critical.}
First, we verify different approach's ability to perform and explore in 6DoF, which is crucial for general manipulation.
When 6DoF exploration is introduced, we first observe a drop in the average success rate for simple tasks that could be accomplished with planar actions (Balance, Transport, Tab. \ref{tab:sim}). However, this ability is critical for exploring complex tasks, providing data to improve upon in the later distilling stage.
In particular, we observed that 6DoF actions are important for grasping diverse objects with complex geometry (Transport, Tab. \ref{tab:sim}), and manipulating articulated objects (Drawer, Mailbox, Tab. \ref{tab:sim}).

\input{tables/failure_analysis}

Moreover, 6DoF exploration also helps in \textbf{diversifying} the data collection strategy, which provides the \textbf{possibility to improve upon} in the later distilling stage.
For example in the catapult domain, LLM-as-Policy (2D) is only able to solve one of three possible goals (the closest bin) using a deterministic strategy.
However, it provides no useful data for learning the other two goals, making it a poor data-collection policy.
In contrast, incorporating 6 DOF robot utilities achieves lower but non-zero average success rates in all bins ($16.3\%$, $3.3\%$, and $2.2\%$, full table in appendix), which provide much better exploration data for distillation.



\textbf{Verify \& Retry always helps.}
In the verify \& retry step, the LLM retries all tasks until they are successful.
This simple addition improves performance in all domains, with
$2\times$, $3\times$, $8\times$, and $13\times$ in transport, catapult, balance, and drawer domains.
Without this crucial step, we observe $0.0\%$ success rate in the mailbox domain, underscoring the difficulty of flawlessly executing long sequences of 6 DOF actions, and the importance of recovery after failure.
\input{tables/llm}
\textbf{Language Model Scaling.}
In addition to the final task success, we provide more detailed analysis of planning and success condition inference accuracy in Tab.~\ref{tab:llm}.
We evaluate on the proprietary GPT3~\cite{brown2020language} (175B text-davinci-003) and the open LLAMA2~\cite{touvron2023llama} (7B and 13B).
We found that Llama models struggles in complex planning domains because they do not follow instructions provided in the prompts.
For instance, in the drawer domain, both models fail to account for drawer opening and closing.
However, we observe an upwards trend with respect to Llama model size, with the 13B model outperforming the 7B model by $+20.0\%$ and $+38.3\%$ in planning and success verification accuracy respectively.

\vspace{-2mm}
\subsection{Distilled Policy Evaluation}
\vspace{-2mm}

\textbf{Robustness In, Robustness Out.}
By filtering trajectories with LLM's inferred success condition, distilled policies inherit the robustness of their data collection policies while improving upon success rates ($+23.4\%$ and $+33.2\%$ for no-retry and ours, Tab. \ref{tab:sim}).
Since our distilled policy learned from a robust data collection policy, it also recovers from failures (\eg failed grasps or placements) and continuously retries a task until it succeeds.
Meanwhile, since the no-retry distilled policy learned from a data collection policy which did not retry upon failure, it is sensitive and brittle, leading to $-34.8\%$ lower average success rate across all domains compared to ours (Tab. \ref{tab:sim}).




\textbf{High Performance From Diverse Retry Attempts.}
Plotting how long policies take to solve the balance task (Fig.~\ref{fig:result:efficiency}), we observed that our policy and its data collection policy continuously tries a diverse set of grasps and placements after each failed attempt until it succeeds.
This results in higher success rates as the policy is given more time, and is reflected in their monotonically increasing success rates.
\begin{wrapfigure}{r}{0.45\textwidth}
    \vspace{-5mm}
    \centering
    % Figure removed
    \vspace{-6mm}
    \caption{\footnotesize \textbf{Distilled Robustness}.
        \textbf{\textcolor{plt_green}{Our policy}} inherits robust recovery from failure behavior from \textbf{\textcolor{plt_purple}{its data collection policy}}, while improving upon success rate.
    }
    \label{fig:result:efficiency}
    \vspace{-7mm}
\end{wrapfigure}
In contrast, baselines plateau after their first grasp/platement attempts.
This highlights the synergy of two design decisions.
First, the verify \& retry step (\S~\ref{sec:method:datagen:verifying}) is crucial for demonstrating retrying behavior, but is by itself \emph{insufficient} if each retrying action is the identical as the previous one.
Instead, opting for a diffusion policy (\S~\ref{sec:method:distill}) for learning from and generating high-entropy, diverse retry attempts (Fig~\ref{fig:result:retry_from_failure}) is also essential for high performance.



\textbf{Policy Learning Baselines.}
We investigate policy learning design decisions on the single-task balance domain, and remove language conditioning.
While BC-Z found spatial softmax hurt their performance and opted for a mean pool, we observed using spatial softmax improved performance by +$5.0$\%.
Further, we found that switching from delta to absolute action spaces improved success rates $+6.5\%$ and $+9.5\%$ when using the MLP action decoder and our diffusion action decoder, respectively, confirming \citeauthor{chi2023diffusionpolicy}~\cite{chi2023diffusionpolicy}'s findings.
Lastly, we find that using our pseudo-random diffusion-based action encoder consistently outperforms a deterministic MLP action mappings, regardless of other design decisions.




\input{tables/policy}
\textbf{Sim2Real Transfer.}
We evaluated a policy trained on domain randomized synthetic data in a real world transport task with five novel objects (Fig.~\ref{fig:result:retry_from_failure}e).
Averaging across ten episodes per object, our policy achieved 76\% success rate, demonstrating the effectiveness of our approach in Sim2Real transfer.
