\section{General Open Issues}
\label{sec:open_issues}

Teaching argumentation through the use of NLP systems holds significant promise for enhancing educational experiences among students. However, the further research in this area still present various open issues. In this section, we explore the overall difficulties in designing and evaluating computational models for argumentation and discuss some methods for mitigating them.

\paragraph{Evaluating different systems}

The evaluation of NLP systems often heavily relies on human assessment, which is very insightful. However, this reliance on human evaluation makes it hard to reproduce the evaluation and compare different systems. To the best of our knowledge, no research have been dedicated to comprehensive comparative studies of different end-to-end systems.

While some systems, such as \citet{wambsganss-etal-2021-arguetutor}, exhibit promising performance, the lack of direct comparisons with other similar systems hampers the comprehension of their relative advantages and limitations. As researchers and educators, it becomes overwhelming to discern which system best fits specific pedagogical objectives.

One possible reason for this issue resides in the restricted access of various tools. Indeed, many systems may not be readily accessible as open-source resources, limiting researchers to test them.
Additionally, the lack of common guidelines with standardized metrics for evaluating NLP systems designed for teaching argumentation exacerbates the difficulty in evaluating different systems in a systematic manner. Metrics such as coherence, are essential aspects of an argumentation system's performance, but the absence of a uniform evaluation framework makes it harder to quantitatively compare different tools.

Therefore we encourage researchers to find a common guideline to evaluate their systems and to give access to their research.

% Figure environment removed

\paragraph{Domain Adaptation}
Towards effectively explaining output in a way which can improve critical thinking skills of end-users, future systems must be capable of understanding the topic of discussion in such a way that argumentation errors (e.g., fallacies, etc.) can be identified. In a pedagogical setting, teachers have the ability to choose new topics of discussion annually; hence, systems must also be capable of adapting to various domains. Recent works have focused on the task of domain adaptation for tasks such as short answer scoring~\cite{funayama-etal-2023-aied}, which focus on training models for several tasks in order to learn common properties useful for evaluating unseen topics. For computational argumentation, we must also adopt such strategies to ensure the most reliable feedback is given to the end-user. 

\vspace{-5pt}
\paragraph{Need to simplify the collaboration process}
As mentioned a priori, NLP researchers and pedagogical researchers generally conduct their research independently, thus creating a gap. We argue that researchers from both fields must come together to ensure appropriate, sufficient explanations are provided to learners. However, presently it is difficult to find the appropriate learners for evaluation. Ideally, a system for linking various educational schools and providers with artificial intelligence researchers could significantly help assist with ensuring systems can be properly evaluated.

\paragraph{Ethics}
Ideally, personalizing a constructive feedback system for each user, specific to their background and current understanding of the world, would benefit the user significantly. However, due to ethical issues and complex implementation, focusing on certain groups for personalizing feedback should be focused on. For example, in the context of education, the way of explanation for models can be categorized by age group (i.e., elementary vs. high school students). When integrating LLMs into the classroom, we must be careful to confine to appropriate standards.