\section{Related Work}

%\subsection{Explainable AI}

%\old{Explainable AI (XAI) is a research area to make AI models easily understandable for humans~\cite{adadi-berrada-2018}. Based on Clinciu and Hastie~\cite{clinciu-hastie-2019-survey}, XAI will help both expert and non-expert users ''to have a deeper understanding and the appropriate level of trust [in AI systems], which will hopefully lead to increased adoption of this technology.''}

%\old{To the best of our knowledge, XAI has been relatively little studied but has great potential in argumentation. Ideally, computational models should evaluate the quality of argumentation while providing efficient comments or feedback, i.e., explaining the results. However, bridging the gap between argumentation and XAI has remained unexplored.}

%\subsection{Explainable Computational Argumentation}

Several surveys have been done in the field of argumentation \cite{ke_2019,beigman-klebanov-madnani-2020-automated,lawrence_chris_2020,xinyu_2022} and explainability \cite{danilevsky_2020,islam_2021,hartmann_2022}. In this section, we focus on the recent surveys related to explainability in argumentation.

First, ~\citet{vassiliades_2021} highlight the potential of argumentation in explainable AI systems. They provide an exhaustive overview of argumentation systems by grouping them by domain, such as law, medicine, and semantic web. For each domain, papers are compared by tasks (e.g, argument classification). decision-making  justification, explanation through dialogue, and argument classification. Despite the extensiveness of this survey, some topics important for improving explanations in argumentative systems received little attention. For example, frameworks that include arguments with commonsense knowledge and diverse attack relations between them
have rarely been discussed, even though, they can enhance the model's explainability~\cite{saha-etal-2021-explagraphs}.

~\citet{cyras_2021} focus on the different frameworks, types, and forms of explanations. They distinguish intrinsic approaches (models using argumentative methods) from post-hoc approaches (non-argumentative models that provide complete or partial explanations). They discuss multiple forms of argumentation, such as dialogue, extensions, and sub-graphs. Their final roadmap is an essential part of their work and covers the need to focus more on properties and computational aspects of argumentation-based explanations. Whereas they focus on argumentation used to explain, our work discusses how computational argumentation needs more explanation.

Moreover, our work distinguishes itself from the two previous surveys~\cite{vassiliades_2021, cyras_2021} by focusing on evaluating and improving users' critical thinking skills.