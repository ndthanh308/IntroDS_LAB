\section{Conclusion}

Our survey delves into the domain of argumentation in education and the utilization of computational models to enhance critical thinking skills among students. 
In this survey, we explored several works providing explanations in argumentation, following various dimensions of feedback: \textit{Richness} (\S\ref{sec:richness-what}, \S\ref{sec:richness-why}), \textit{Visualization} (\S\ref{sec:visualization}), \textit{Interactivity} (\S\ref{sec:interactivity}), and \textit{Personalization} (\S\ref{sec:personalization}).

We identified different potential areas for improvement to enhance the overall quality of educational systems to teach argumentation. We summarized these challenges with the following points: (1) generate accurate, constructive feedback for a real-life input, (2) tailor the output based on the user's background, (3) evaluate and compare more deeply end-to-end systems, (4) collaborate with pedagogical teams and actual students, and finally (5) take into consideration ethical issues.

For instance, in challenge (2), the use of conversational agents becomes increasingly frequent. Nevertheless, such systems still leave room for improvement; in particular, only a few research focuses on using their ability to tailor the discussion based on the user to improve critical thinking skills.

We hope our survey contributes to expanding the argumentation community's horizons with a comprehensive understanding of current perspectives in NLP systems for teaching argumentation.

In our future work, we decided to focus further on real-life user-friendly end-to-end systems (challenges (1) and (3)). We plan to prototype a system to measure the effects of different feedback on critical thinking skills and determine how different interfaces can impact the learning process by evaluating this system in actual classrooms (cf., Figure~\ref{fig:interface} in the Appendix~\ref{sec:appendix}).