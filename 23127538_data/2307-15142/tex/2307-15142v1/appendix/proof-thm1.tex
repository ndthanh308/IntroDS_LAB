\subsection{Proof of \Cref{thm:general}}\label{sec:proof-thm1}
We now turn to the proofs of \Cref{thm:general}(i)-(iv). In each of these parts, we consider a set of recommendations with $a_t$ items of type $t$ for each $t\in [m].$ Then observe that the expected total value of the $k$ highest value recommended items is equal to
\begin{equation}
    \sum_{t=1}^m p_t h(a_t),
\end{equation}
for
\begin{equation}
    h:\ZZ \rightarrow \RR, \quad h:a\mapsto \EE\left[\topp_k\{X_1,\cdots,X_a\simiid \DD\}\right],
\end{equation}
where $\topp_k$ evaluates the sum of the $k$ highest values in a set. Intuitively, conditional on a user preferring type $t$, the top $k$ items are just the top $k$ items recommended of type $t$. The sum of their values, conditioned on the user preferring type $t$, is simply the sum of the $k$ highest values among $a$ random draws from $\DD$. Clearly, $h$ here is monotonically increasing.

Then, with \Cref{lem:fennel} in hand, parts (i)-(iv) reduces to showing the following:
\begin{enumerate}
    \item[(i)] If $\DD$ is a finite discrete distribution, there exist constants $A,B>0$ and $\sigma>0$ such that  \begin{equation}\lim_{a\rightarrow \infty} \frac{\log(A - h(a))}{Ba^\sigma} = 1.\end{equation}
    \item[(ii)] If $\DD$ has support bounded from above by $M$ with pdf $f_\DD$ satisfying
    \begin{equation}
    \lim_{x\rightarrow M} \frac{f_\DD(x)}{(M-x)^{\beta-1}} = c
    \end{equation}
    for some $\beta, c>0$, then there exist constants $A,B>0$ such that
        \begin{equation} \lim_{a\rightarrow \infty} \frac{A-h(a)}{Ba^{-\frac{1}{\beta}}} = 1.
        \end{equation}

    \item[(iii)] If $\DD = \Exp(\lambda)$ for $\lambda > 0,$ then $h$ is strictly concave and there exists a constant $B>0$ such that
        \begin{equation}
            \lim_{a\rightarrow \infty} \frac{h(a)}{B\log a} = 1.
        \end{equation}
    \item[(iv)] If $\DD = \Pareto(\alpha)$ for $\alpha>1$, then $h$ is strictly concave and there exists a constant $B>0$ such that
        \begin{equation}
            \lim_{a\rightarrow \infty} \frac{h(a)}{Ba^{\frac{1}{\alpha}}} = 1.
        \end{equation}
\end{enumerate}

The following identity, mentioned in \Cref{sec:proof-sketch}, will be useful for parts (ii)-(iv).
\begin{proposition}\label{prop:mu}
For $X_i^{(t)}\simiid \DD$,
    \begin{equation}
    h(a) = \sum_{i=1}^{\min\{k, a\}} \mu_\DD(a-i+1,a).
    \end{equation}
\end{proposition}
Recall that $\mu_\DD(i,a)$ is the expected value of the $i$-th order statistic of $a$ random variables drawn i.i.d. from $\DD$.

\begin{proof}
    Let $Y_{k,n}$ be the $k$-th order statistic of $n$ random variables distributed i.i.d. from $\DD$. Then
\begin{equation}\label{eq:pond}
\topp_{k}\{X_1^{(t)},\cdots,X_{a}^{(t)}\}
= \sum_{i=1}^{\min\{k,a\}} Y_{a-i+1, a}.
\end{equation}
So, as desired,
\begin{equation}
    \EE\left[\topp_{k}\{X_1^{(t)},\cdots,X_{a}^{(t)}\}\right] = \sum_{i=1}^{\min\{k,a\}} \EE\left[Y_{a-i+1,a}\right] = \sum_{i=1}^{\min\{k, a\}} \mu_\DD(a-i+1,a),
\end{equation}
where the first equality follows from \eqref{eq:pond} and the linearity of expectation.
\end{proof}

\paragraph{Proof of \Cref{thm:general}(i).}
Suppose $\DD$ has support $\{x_1,\cdots,x_r\}$ with $x_1>\cdots>x_r$ such that for $X\sim \DD$, $\Pr[X=x_1] = q.$ Now consider a set of recommendations with $a_t$ items of type $t$ for each $t\in [m].$ Then consider $X_1,\cdots,X_a\simiid \DD$. Let $E$ be the event that at least $k$ of $X_1,\cdots,X_a$ equal $x_1$. Then,
\begin{align}
    h(a) &\ge \EE[\topp_k\{X_1,\cdots,X_a\}|E] \cdot \Pr[E]\\
    &= x_1k\cdot \left(1 - \sum_{j=0}^{k-1}\binom{a}{j}(1-q)^{a-j}q^j\right)\\
    &\ge x_1k(1 - a^k(1-q)^{a-k+1})
\end{align}
for all $a>2.$ Now let $E'$ be the event that at least one of $X_1,\cdots, X_a$ equals $x_1$. Then,
\begin{align}
    h(a) &= \EE[\topp_k\{X_1,\cdots,X_a\}|E'] \cdot \Pr[E'] + \EE[\topp_k\{X_1,\cdots,X_a\}|\overline{E'}] \cdot (1-\Pr[E'])\\
    &\le x_1k(1 - (1-q)^{a}) + x_2k(1-q)^{a}\\
    &= x_1k(1 - (1-\frac{x_2}{x_1})(1-q)^{a}).
\end{align}
Now note that for $A = x_1k,$ we have that
\begin{align}
    x_1k(1 - \frac{x_2}{x_1})(1-q)^{a} &\le A - h(a) \le x_1k a^k (1-q)^{a-k+1}\\
    \log(x_1k(1 - \frac{x_2}{x_1})(1-q)^{a}) &\le \log(A - h(a)) \le \log(x_1k a^k (1-q)^{a-k+1})\\
    \log(x_1k) + \log(1 - \frac{x_2}{x_1}) + a\log(1-q) &\le \log(A - h(a)) \le \log(x_1k) + k\log(a) + (a-k+1)\log(1-q).
\end{align}
It follows that for $B = \log(1-q),$
\begin{equation}
    \lim_{a\rightarrow \infty} \frac{\log(A - h(a))}{Ba} = 1,
\end{equation}
as desired. The result follows from \Cref{lem:fennel}(i).

\paragraph{Proof of \Cref{thm:general}(ii).}

First recall from \Cref{prop:mu} that
\begin{equation}
    h(a) = \sum_{i=1}^{\min\{k, a\}} \mu_\DD(a-i+1,a).
\end{equation}
We will show that \begin{equation}
\lim_{a\rightarrow \infty} \frac{Mk - h(a)}{Ba^{-\frac{1}{\beta}}} = 1
\end{equation}
for a constant $B>0.$ \Cref{thm:general}(ii) then follows immediately by applying \Cref{lem:fennel}(ii) with $\sigma = -\frac{1}{\beta}$.

Consider a probability distribution $\DD'$ with pdf $g_X(x)=f_X(M-x)$ and cdf $G_X(x).$ Then
\begin{equation}
\mu_\DD(a-i+1,a) = M - \mu_{\DD'}(i,a),
\end{equation}
which implies that
\begin{equation}
    Mk - \sum_{i=1}^{k} \mu_\DD(a-i+1,a) = \sum_{i=1}^{k} \mu_{\DD'}(i,a)
\end{equation}
Since
\begin{equation}
    \mu_{\DD'}(i,a) = \sum_{j=0}^{i-1} \int_0^\infty \binom{a}{j}G_X(x)^j (1-G_X(x))^{a-j}\,dx,
\end{equation}
it remains to show that for all fixed $j$,
\begin{equation}\label{eq:orangepeel}
    \lim_{a\rightarrow \infty}\frac{\int_0^\infty \binom{a}{j}G_X(x)^j (1-G_X(x))^{a-j}\,dx}{a^{-\frac{1}{\beta}}} = B
\end{equation}
for some constant $B$ (that can vary depending on $j$). Verifying \eqref{eq:orangepeel} comprises the bulk of the technical work of the proof, and we isolate it in the following lemma.

\begin{lemma}
For $\beta>0$,
\begin{equation}
\int_0^\infty \binom{a}{j} G_X(x)^j (1 - G_X(x))^{a-j}\,dx \propto a^{-\frac{1}{\beta}}.
\end{equation}
\end{lemma}

\begin{proof}
We have that
\begin{equation}
    \lim_{x\rightarrow 0^+} \frac{g_X(x)}{cx^{\beta-1}} = \lim_{x\rightarrow M^{-}} \frac{f_X(x)}{c(M-x)^{\beta-1}} = 1
\end{equation}
for a positive constant $c$. So for all $\epsilon>0$ there exists $\delta>0$ such that
\begin{equation}
(1 - \epsilon)cx^{\beta-1} \le g_X(x) \le (1 + \epsilon)cx^{\beta-1}
\end{equation}
for all $x<\delta.$
Now note that $g_X(x) \le (1 + \epsilon)cx^{\beta-1}$ implies that
\begin{equation}
    G_X(x) = \int_0^x g_X(u)\,du \le (1+\epsilon)\int_0^x cu^{\beta-1}\,du = (1+\epsilon)\frac{c}{\beta}x^\beta.
\end{equation}
Likewise, $g_X(x) \ge (1 - \epsilon)cx^{\beta-1}$ implies that
\begin{equation}
    G_X(x) = \int_0^x g_X(u)\,du \ge (1-\epsilon)\int_0^x cu^{\beta-1}\,du = (1-\epsilon)\frac{c}{\beta}x^\beta.
\end{equation}
Now write
\begin{align}
&a^{\frac{1}{\beta}}\int_0^\infty \binom{a}{j} G_X(x)^j (1 - G_X(x))^{a-j}\,dx\\ &=a^{\frac{1}{\beta}}\int_0^\delta \binom{a}{j} G_X(x)^j (1 - G_X(x))^{a-j}\,dx + a^{\frac{1}{\beta}}\int_\delta^\infty \binom{a}{j} G_X(x)^j (1 - G_X(x))^{a-j}\,dx.
\end{align}
We will analyze these two integral separately. It will turn out that the second integral vanishes as $a$ grows.
\end{proof}
\paragraph{The first integral.} We have that
\begin{align}
&a^{\frac{1}{\beta}}\int_0^\delta \binom{a}{j} G_X(x)^j (1 - G_X(x))^{a-j}\,dx\\
&\le a^{\frac{1}{\beta}}\int_0^\delta \binom{a}{j} (1+\epsilon)^j\left(\frac{c}{\beta}\right)^j x^{\beta j} (1 - (1-\epsilon)\frac{c}{\beta}x^\beta)^{a-j}\,dx\\
&= \int_0^{\delta a^{\frac{1}{\beta}}} \binom{a}{j} (1+\epsilon)^j\left(\frac{c}{\beta}\right)^j \left(\frac{x}{a^{\frac{1}{\beta}}}\right)^{\beta j} \left(1 - (1-\epsilon)\frac{c}{\beta}\left(\frac{x}{a^{\frac{1}{\beta}}}\right)^\beta\right)^{a-j}\,dx\\
&= \int_0^{\delta a^{\frac{1}{\beta}}} \binom{a}{j} (1+\epsilon)^j\left(\frac{c}{\beta}\right)^j \frac{x^\beta j}{a^j} \left(1 - (1-\epsilon)\frac{c}{\beta}\frac{x}{a}\right)^{a-j}\,dx.
\end{align}
Then
\begin{equation}
    \int_0^{\delta a^{\frac{1}{\beta}}} \binom{a}{j} (1+\epsilon)^j\left(\frac{c}{\beta}\right)^j \frac{x^\beta j}{a^j} \left(1 - (1-\epsilon)\frac{c}{\beta}\frac{x}{a}\right)^{a-j}\,dx
    = \int_0^\infty \phi_{a}(x)\,dx,
\end{equation}
where
\begin{equation}
    \phi_{a}(x) := \begin{cases}
    \binom{a}{j} (1+\epsilon)^j\left(\frac{c}{\beta}\right)^j \frac{x^\beta j}{a^j} \left(1 - (1-\epsilon)\frac{c}{\beta}\frac{x}{a}\right)^{a-j}\,dx &\quad \text{for }0\le x\le \delta a^{\frac{1}{\beta}}\\
    0&\quad \text{for }x>\delta a^{\frac{1}{\beta}}.
    \end{cases}
\end{equation}
We have that
\begin{equation}
\lim_{a\rightarrow \infty} \phi_{a}(x) = \frac{1}{j!}(1+\epsilon)^j\left(\frac{c}{\beta}\right)^jx^{\beta j}e^{-(1-\epsilon)\frac{c}{\beta}x^{\beta}}
\end{equation}
and
\begin{equation}
    \phi_{a}(x) \le \frac{1}{j!}(1+\epsilon)^j\left(\frac{c}{\beta}\right)^jx^{\beta j}e^{-(1-\epsilon)\frac{c}{\beta}x^{\beta}} (1 - (1-\epsilon)\frac{c}{\beta}\epsilon^\beta))^{-j} = C(j,\epsilon)x^{\beta j}e^{-(1-\epsilon)\frac{c}{\beta}x^{\beta}}
\end{equation}
for a constant $C(j,\epsilon)$ independent of $a.$ Now note that $\int_0^\infty x^{\beta j}e^{-(1-\epsilon)\frac{c}{\beta}x^{\beta}} < \infty.$ It follows from the dominated convergence theorem that
\begin{equation}
    \lim_{a\rightarrow \infty} \int_0^\infty \phi_{a}(x)\,dx
    = \int_0^\infty \lim_{a\rightarrow \infty} \phi_{a}(x)\,dx
    = \int_0^\infty \frac{1}{j!}(1+\epsilon)^j\left(\frac{c}{\beta}\right)^jx^{\beta j}e^{-(1-\epsilon)\frac{c}{\beta}x^{\beta}}\,dx < \infty.
\end{equation}
Therefore, for $a$ sufficiently large,
\begin{equation}
    \int_0^\delta \binom{a}{j} G_X(x)^j (1 - G_X(x))^{a-j}\,dx \le a^{-\frac{1}{\beta}}(1+\epsilon)\int_0^\infty \frac{1}{j!}(1+\epsilon)^j\left(\frac{c}{\beta}\right)^jx^{\beta j}e^{-(1-\epsilon)\frac{c}{\beta}x^{\beta}}\,dx.
\end{equation}
Analogously, we can show that for $a$ sufficiently large,
\begin{equation}
    \int_0^\delta \binom{a}{j} G_X(x)^j (1 - G_X(x))^{a-j}\,dx \ge a^{-\frac{1}{\beta}}(1-\epsilon)\int_0^\infty \frac{1}{j!}(1-\epsilon)^j\left(\frac{c}{\beta}\right)^jx^{\beta j}e^{-(1+\epsilon)\frac{c}{\beta}x^{\beta}}\,dx.
\end{equation}
Now observe that
\begin{align}
&\lim_{\epsilon\rightarrow 0^+} (1+\epsilon)\int_0^\infty \frac{1}{j!}(1+\epsilon)^j\left(\frac{c}{\beta}\right)^jx^{\beta j}e^{-(1-\epsilon)\frac{c}{\beta}x^{\beta}}\,dx\\
&= \int_0^\infty \frac{1}{j!}\left(\frac{c}{\beta}\right)^j x^{\beta j}e^{-\frac{c}{\beta}x^\beta}\,dx\\
&= \lim_{\epsilon\rightarrow 0^+} (1-\epsilon)\int_0^\infty \frac{1}{j!}(1-\epsilon)^j\left(\frac{c}{\beta}\right)^jx^{\beta j}e^{-(1+\epsilon)\frac{c}{\beta}x^{\beta}}\,dx,
\end{align}
where we once again apply the dominated convergence theorem. It follows that
\begin{equation}\label{eq:onion}
    \lim_{a\rightarrow \infty} \frac{\int_0^\delta \binom{a}{j} G_X(x)^j (1 - G_X(x))^{a-j}\,dx}{a^{-\frac{1}{\beta}}} = \int_0^\infty \frac{1}{j!}\left(\frac{c}{\beta}\right)^j x^{\beta j}e^{-\frac{c}{\beta}x^\beta}\,dx.
\end{equation}

\paragraph{The second integral.}
We now analyze
\begin{equation}
    a^{\frac{1}{\beta}}\int_\delta^\infty \binom{a}{j} G_X(x)^j (1 - G_X(x))^{a-j}\,dx.
\end{equation}
Observe that
\begin{align}
    a^{\frac{1}{\beta}}\int_\delta^\infty \binom{a}{j} G_X(x)^j (1 - G_X(x))^{a-j}\,dx
    &< a^{\frac{1}{\beta}} \binom{a}{j} \int_\delta^\infty (1 - G_X(x))^{a-j}\,dx\\
    &< a^{\frac{1}{\beta}} \binom{a}{j} (1 - G_X(\delta))^{a-j} \int_\delta^\infty 1 - G_X(x)\,dx\\
    &< a^{\frac{1}{\beta}} \binom{a}{j} (1 - G_X(\delta))^{a-j} \EE[X].
\end{align}
Thus,
\begin{equation}\label{eq:carrot}
    \lim_{a\rightarrow \infty} \frac{\int_\delta^\infty \binom{a}{j} G_X(x)^j (1 - G_X(x))^{a-j}\,dx}{a^{\frac{1}{\beta}}} = 0.
\end{equation}

Combining \eqref{eq:onion} and \eqref{eq:carrot} gives us that
\begin{equation}
\int_0^\infty \binom{a}{j} G_X(x)^j (1 - G_X(x))^{a-j}\,dx \propto a^{-\frac{1}{\beta}},
\end{equation}
as desired.

\paragraph{Proof of \Cref{thm:general}(iii).} Recall again that
\begin{equation}
    h(a) := \sum_{i=1}^{\min\{k, a\}} \mu_\DD(a-i+1,a).
\end{equation}
We show that $h$ is strictly concave and
\begin{equation}\label{eq:mouse-2}
\lim_{a\rightarrow \infty} h(a) - B\log a - C = 0
\end{equation}
for constants $B,C>0$. Both of these facts follow directly from the lemma below. \Cref{thm:general}(iii) then follows immediately by applying \Cref{lem:fennel}(iii).

\begin{lemma}\label{lem:mouse}
For $\DD$ an exponential distribution with rate parameter $\lambda$, so that $f_X(x) = \lambda e^{-\lambda x}$ for $\lambda > 0,$
\begin{equation}\label{eq:papaya}
    \lim_{a\rightarrow \infty} \mu_\DD(a-i,a) - \log a - B(j) = 0
\end{equation}
for a constant $B(j)>0.$ Moreover, $\mu_\DD(a-i,a)$ is strictly concave.
\end{lemma}

\begin{proof}
For an exponential distribution with rate parameter $\lambda,$ it is well known that
\begin{equation}\label{eq:bridge}
    \mu_\DD(a-i,a) = \sum_{j=i+1}^{a} \frac{1}{\lambda n}.
\end{equation}
It is clear, then, that $\mu_\DD(a-i,a)$ is strictly concave. \eqref{eq:bridge}  is equal to
\begin{equation}
    \frac{1}{\lambda}\left(\log n + \gamma + \epsilon(a) - \sum_{j=1}^i \frac{1}{j}\right),
\end{equation}
where $\gamma$ is the Euler-Mascheroni constant and $\lim_{a\rightarrow \infty}\epsilon(a) = 0,$ from which \eqref{eq:papaya} follows.
\end{proof}

\paragraph{Proof of \Cref{thm:general}(iv).}

Recall again that
\begin{equation}
    h(a) := \sum_{i=1}^{\min\{k, a\}} \mu_\DD(a-i+1,a).
\end{equation} Then it suffices to show that $h$ is strictly concave and
\begin{equation}
\lim_{a\rightarrow \infty} \frac{h(a)}{Ba^{\frac{1}{\alpha}}} = 1
\end{equation}
for a constant $B>0.$ Both of these facts follow directly from the lemma below. \Cref{thm:general}(iv) then follows immediately by applying \Cref{lem:fennel}(iv).

\begin{lemma}
For $\DD$ a Pareto distribution with pdf $f_X(x) = x^{-\alpha-1}$ for $\alpha > 1,$
\begin{equation}
    \lim_{a_t\rightarrow \infty} \frac{\mu_\DD(a-i,a)}{a^\frac{1}{\alpha}} = C
\end{equation}
for a constant $C>0.$ Moreover, $\mu_\DD(a-i,a)$ is strictly concave.
\end{lemma}

\begin{proof}
The result follows directly from Lemmas D.10 and D.11 in \cite{kleinberg2018selection}, where it is shown (in our notation) that
\begin{align}
    \lim_{a\rightarrow \infty}\frac{\mu_\DD(a,a)}{a^{\frac{1}{\alpha}}} = \Gamma\left(\frac{\alpha-1}{\alpha}\right)
\end{align}
and
\begin{equation}
    \mu_\DD(a-i,a) = \prod_{j=1}^i \left(1 - \frac{1}{j\alpha}\right)\mu_\DD(a,a).
\end{equation}
\end{proof}

Thus,
\begin{equation}
    \lim_{a\rightarrow \infty} \frac{\sum_{i=1}^{k} \mu_\DD(a-i+1,a)}{B \log a} = 1
\end{equation}
for a constant $B$. Also, note that $\mu_\DD(a-i,a)$ is a constant multiple of $\mu_\DD(a,a)$, and that $\mu_\DD(a,a)$ is strictly concave, since the mean of the largest order statistic of a distribution is strictly concave in sample size. Thus, $\mu_\DD(a-i,a)$ is strictly concave.

