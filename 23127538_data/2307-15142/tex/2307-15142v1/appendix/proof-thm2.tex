\subsection{Proof of \Cref{thm:ber-decay}}\label{sec:proof-thm2}

Suppose that for each fixed $i$, $X_i^{(t)}$ are i.i.d. Bernoulli random variables with success probability $q_i$ such that $q_i= c(i+d)^{-\beta}$ for some $\beta,c,d\ge 0.$ We begin by proving parts (i),(ii), and (iii), wherein the user only utilizes the highest value recommended item. In the Bernoulli setting, this amounts to determining the probability that \textit{at least one} movie is satisfactory, i.e.,
\begin{equation}
    \sum_{t=1}^m p_t h(a_t),
\end{equation}
where
\begin{align}\label{eq:grass}
    h(a) = 1 - \prod_{i=1}^{a} (1 - c(i+d)^{-\beta}).
\end{align}
It suffices now to show the desired asymptotic properties for $h$ depending on $\beta$, and applying \Cref{lem:fennel}.

We note the following fact, which will be helpful in our analysis:
\begin{equation}\label{eq:exp-bound}
    1-x>e^{-x-x^2}\quad \text{for $0< x < \frac{1}{2}$}.
\end{equation}

\subsubsection{Proof of \Cref{thm:ber-decay}(i)}
We now consider the case $0\le \beta < 1$. We analyze
\begin{equation}
    1 - h(a) = \prod_{i=1}^{a} (1 - c(i+d)^{-\beta}) = \prod_{i=d+1}^{a+d} (1 - ci^{-\beta}),
\end{equation}
and bound it from above and below. Let $i'$ be the smallest $i'$ such that $c(i')^{-\beta}<\frac{1}{2}$. Bounding from above,
\begin{align}
\prod_{i=i'+1}^{a+d} (1 - ci^{-\beta})
&< \prod_{i=i'+1}^{a+d} e^{-ci^{-\beta}}\\
&= \exp \left[\sum_{i=i'}^{a+d} -ci^{-\beta}\right]\\
&< \exp \left[\int_{i'}^{a+d+1} -cx^{-\beta}\,dx\right]\\
&= \exp \left[ -\left[ \frac{cx^{1-\beta}}{1-\beta} \right]_{i'}^{a+d+1}\right]\\
&= \exp \left[-\frac{c}{1-\beta} \left((a+d+1)^{1-\beta}-(i')^{1-\beta}\right)\right]
\end{align}
Now, bounding from below,
\begin{align}
\prod_{i=i'}^{a+d} (1 - ci^{-\beta})
&> \prod_{i=i'}^{a+d} e^{-ci^{-\beta}-c^2i^{-2\beta}}\\
&= \exp \left[\sum_{i=i'}^{a+d} -ci^{-\beta}-c^2i^{-2\beta}\right]\\
&> \exp \left[\int_{i'-1}^{a+d} -cx^{-\beta}-c^2x^{-2\beta}\,dx\right]\\
&= \exp \left[ -\left[ \frac{cx^{1-\beta}}{1-\beta} + \frac{c^2x^{2-\beta}}{2-\beta} \right]_{i'-1}^{a+d}\right]\\
&= \exp \left[-\frac{c}{1-\beta} \left((a+d)^{1-\beta}-(i'-1)^{1-\beta}\right) -\frac{c^2}{1-2\beta} \left((a+d)^{2-\beta}-(i'-1)^{2-\beta}\right)\right],
\end{align}
where the first inequality follows from \eqref{eq:exp-bound}. Then observing that
\begin{equation}
    1 - h(a) = \prod_{i=d+1}^{i'-1} (1 - ci^{-\beta})\cdot \prod_{i=i'}^{a+d} (1 - ci^{-\beta}),
\end{equation}
where the first product is constant in $a$, it follows that
\begin{equation}
    \lim_{a \rightarrow \infty} \frac{\log (1-h(a))}{-\frac{c}{1-\beta}a^{1-\beta}} = 1,
\end{equation}
as desired. The result follows from \Cref{lem:fennel}(i).

\subsubsection{Proof of \Cref{thm:ber-decay}(ii)}
We turn to the case $\beta=1$. We have that
\begin{align}
    1-h(a) = \prod_{i=d+1}^{a+d} \left(1 - \frac{c}{i}\right) &= \frac{d+1-c}{d}\cdot \frac{d+2-c}{d+1}\cdot \ldots \cdot \frac{a+d-c}{a+d},
\end{align}
which telescopes to
\begin{equation}
    \prod_{i=1}^c \frac{d+i-c}{a+d-i+1}.
\end{equation}
Now note that
\begin{equation}
    \lim_{a\rightarrow \infty} \frac{\prod_{i=1}^c \frac{d+i-c}{a+d-i+1}}{a^{-c}} = \prod_{i=1}^c (d+i-c)
\end{equation}
is a finite constant. Therefore,
\begin{equation}
    \lim_{a\rightarrow \infty} \frac{1 - h(a)}{Ba^{-c}} = 1
\end{equation}
for constant $B$.


\subsubsection{Proof of \Cref{thm:ber-decay}(iii)}
We now consider the case $\beta > 1$. In this case, we will again bound \eqref{eq:grass} from above and below. First note that
\begin{equation}
    1 - h(a) = \prod_{i=d+1}^{\infty} (1 - ci^{-\beta}) = S
\end{equation}
for a finite constant $S$.

We have that
\begin{align}
\prod_{i=a+d+1}^{\infty} (1 - ci^{-\beta})
&< \prod_{i=a+d+1}^{\infty} e^{-ci^{-\beta}}\\
&= \exp \left[\sum_{i=a+d+1}^{\infty} -ci^{-\beta}\right]\\
&< \exp \left[\int_{a+d+1}^{\infty} -cx^{-\beta}\,dx\right]\\
&= \exp \left[ -\left[ \frac{cx^{1-\beta}}{1-\beta} \right]_{a+d+1}^{\infty}\right]\\
&= \exp \left[-\frac{c}{1-\beta} (a+d+1)^{1-\beta}\right]
\end{align}
Therefore,
\begin{equation}
    \prod_{i=1}^{a+d} (1 - ci^{-\beta}) = \frac{S}{\prod_{i=a+d+1}^{\infty}} (1 - ci^{-\beta}) > S / \exp \left[-\frac{c}{1-\beta} (a+d+1)^{1-\beta}\right].
\end{equation}

Also,
\begin{align}
\prod_{i=a+d+1}^{\infty} (1 - ci^{-\beta})
&> \prod_{i=a+d+1}^{\infty} e^{-ci^{-\beta}-c^2i^{-2\beta}}\\
&= \exp \left[\sum_{i=a+d+1}^{\infty} -ci^{-\beta}-c^2i^{-2\beta}\right]\\
&< \exp \left[\int_{a+d}^{\infty} -cx^{-\beta}-c^2x^{-2\beta}\,dx\right]\\
&= \exp \left[ -\left[ \frac{cx^{1-\beta}}{1-\beta} + \frac{c^2x^{2-\beta}}{2-\beta} \right]_{a+d}^{\infty}\right]\\
&= \exp \left[-\frac{c}{1-\beta} (a+d)^{1-\beta} -\frac{c^2}{1-2\beta} (a+d)^{2-\beta}\right],
\end{align}
where the first inequality holds for $a$ sufficiently large due to \eqref{eq:exp-bound}.

Therefore,
\begin{equation}
    \prod_{i=1}^{a+d} (1 - ci^{-\beta}) = \frac{S}{\prod_{i=a+d+1}^{\infty}} (1 - ci^{-\beta}) < S / \exp \left[-\frac{c}{1-\beta} (a+d-1)^{1-\beta} -\frac{c^2}{1-2\beta} (a+d)^{2-\beta}\right].
\end{equation}

Now observe that
\begin{equation}
    \lim_{a\rightarrow \infty} -\frac{c}{1-\beta} (a+d+1)^{1-\beta} = 0
\end{equation}
and
\begin{equation}
    \lim_{a\rightarrow \infty} -\frac{c}{1-\beta} (a+d-1)^{1-\beta} -\frac{c^2}{1-2\beta} (a+d)^{2-\beta} = 0.
\end{equation}

Therefore,
\begin{equation}
    \lim_{a\rightarrow \infty} \frac{\prod_{i=1}^{a+d} (1 - ci^{-\beta})}{\frac{S}{1 - \frac{c}{1-\beta} (a+d+1)^{1-\beta}}} = 1.
\end{equation}
Also note that
\begin{equation}
    \frac{S}{1 - \frac{c}{1-\beta} (a+d+1)^{1-\beta}}
    = S - \frac{S \frac{c}{1-\beta}}{(a+d+1)^{\beta-1} + \frac{c}{1-\beta}}
\end{equation}

Also,
\begin{equation}
    \lim_{a\rightarrow \infty} \frac{\prod_{i=1}^{a+d} (1 - ci^{-\beta})}{ \frac{S}{1 - \frac{c}{1-\beta} (a+d-1)^{1-\beta} -\frac{c^2}{1-2\beta} (a+d)^{2-\beta}}} = 1
\end{equation}
and
\begin{equation}
    \lim_{a\rightarrow \infty} \frac{1 - \frac{c}{1-\beta} (a+d)^{1-\beta} -\frac{c^2}{1-2\beta} (a+d)^{2-\beta}}{1 - \frac{c}{1-\beta} (a+d)^{1-\beta}} = 1.
\end{equation}

It follows that
\begin{equation}
    \lim_{a\rightarrow \infty} \frac{1 - S - h(a)}{-\frac{Sc}{1-\beta}a^{1-\beta}} = 1
\end{equation}
as desired. The result follows from \Cref{lem:fennel}(ii).

\subsubsection{Proof of \Cref{thm:ber-decay}(iv)}
We now prove part (iv), where we consider the expected total value of all recommended items, which is equal to
\begin{equation}
    \sum_{t=1}^{m} p_t h(a_t),
\end{equation}
where
\begin{equation}
    h(a) = \sum_{i=1}^{a} c(i+d)^{-\beta}.
\end{equation}
Again, we consider three cases, $0< \beta < 1, \beta=1, \beta > 1.$

\paragraph{Case 1: $0 < \beta < 1$.} For $0 <\beta < 1,$ observe that
\begin{align}
    \sum_{i=1}^{a} c(i+d)^{-\beta} < \int_{d}^{a+d} cx^{-\beta}\,dx &= \left[\frac{c}{1-\beta}x^{1-\beta}\right]_{d}^{a+d}\\
    &= \frac{c}{1-\beta}(a+d)^{1-\beta} - \frac{c}{1-\beta}d^{1-\beta}
\end{align}
and
\begin{align}
    \sum_{i=1}^{a} c(i+d)^{-\beta} > \int_{d+1}^{a+d+1} cx^{-\beta}\,dx &= \left[\frac{c}{1-\beta}x^{1-\beta}\right]_{d+1}^{a+d+1}\\
    &= \frac{c}{1-\beta}(a+d+1)^{1-\beta} - \frac{c}{1-\beta}(d+1)^{1-\beta}.
\end{align} 
It follows that
\begin{align}
\lim_{a\rightarrow \infty}\frac{h(a)}{a^{1-\beta}} = \frac{c}{1-\beta},
\end{align} 
and the result in this case follows by applying \Cref{lem:fennel}(iv).

\paragraph{Case 2: $\beta = 1$.} Now for $\beta=1,$ we have that
\begin{align}
    \sum_{i=1}^{a} c(i+d)^{-\beta} = c\sum_{i=d+1}^{a+d} \frac{1}{i}.
\end{align}
\begin{align}
\lim_{a\rightarrow \infty}h(a) - c\log a + c\gamma - c\sum_{i=1}^d \frac{1}{i} = 0,
\end{align} 
where $\gamma$ is the Euler-Mascheroni constant The result in this case follows by applying \Cref{lem:fennel}(iii).

\paragraph{Case 3: $\beta > 1$.} Finally, for $\beta>1,$ we have that 
\begin{equation}
     \sum_{i=1}^{\infty} c(i+d)^{-\beta} = S
\end{equation}
for some finite $S$. Then note that
\begin{equation}
    \sum_{i=1}^{a} c(i+d)^{-\beta} = S - \sum_{i=a+1}^{\infty} c(i+d)^{-\beta}.
\end{equation}
Then we have
\begin{align}
    \sum_{i=a+1}^{\infty} c(i+d)^{-\beta} < \int_{a}^{\infty} cx^{-\beta}\,dx &= \left[\frac{c}{1-\beta}x^{1-\beta}\right]_{a}^{\infty}
    = - \frac{c}{1-\beta}a^{1-\beta}
\end{align}
and
\begin{align}
    \sum_{i=a+1}^{\infty} c(i+d)^{-\beta} > \int_{a+1}^{\infty} cx^{-\beta}\,dx &= \left[\frac{c}{1-\beta}x^{1-\beta}\right]_{a+1}^{\infty}
    = - \frac{c}{1-\beta}(a+1)^{1-\beta}
\end{align}
It follows that
\begin{equation}
    \lim_{a\rightarrow \infty}\frac{h(a)}{S - \frac{c}{\beta - 1}a^{1-\beta}} = 1.
\end{equation}
The result in this case follows again by applying \Cref{lem:fennel}(ii).

\subsection{A rounding lemma}\label{sec:proof-rounding-lemma}
The following lemma is useful for showing that---for the class of problems we consider here---optimal integer solutions are well-approximated by optimal real solutions.

\begin{lemma}
\label{lem:roundingloss}
Let $g_1,g_2,\cdots,g_m: [0,\infty)^m \rightarrow \RR$ be strictly convex functions over the non-negative reals. Then define
\begin{equation}
    g(x_1,\cdots,x_m):=\sum_{t=1}^m g_t(x_t).
\end{equation}
Then, under the constraint that $\sum_{t=1}^m x_t = n$, if $(x_1^*,\cdots,x_m^*)$ is the maximum of $g$ over the non-negative reals and $(a_1^*,\cdots,a_m^*)$ is the maximum of $g$ over the non-negative integers, then
\begin{equation}
\lfloor x_t^* \rfloor - m < a_t^* < \lfloor x_t^* \rfloor + m
\end{equation}
for all $t$.
\end{lemma}

\begin{proof}
The key idea is to show that there cannot be $i,j$ such that $a_i^*\ge \lceil x_i \rceil + 1$ and $a_j^*\le \lceil x_j \rceil - 1.$ If such a pair does exist, we show that
\begin{equation}
g(\cdots,a_i^*-1,\cdots,a_j^*+1,\cdots) \ge g(\cdots,a_i^*,\cdots,a_j^*,\cdots),
\end{equation}
contradicting the optimality of $a_1^*,\cdots,a_m^*.$ It suffices to show that
\begin{equation}
g_i(a_i^*-1) + g_j(a_j^*+1)\ge g_i(a_i^*) + g_j(a_j^*),
\end{equation}
or equivalently,
\begin{equation}
g_j(a_j^*+1) - g_j(a_j^*)\ge g_i(a_i^*) - g_i(a_i^*-1).
\end{equation}
This holds, as
\begin{align}
   g_j(a_j^*+1) - g_j(a_j^*) \ge \frac{\partial g_j}{\partial x_j} = \frac{\partial g_i}{\partial x_i} \ge g_i(a_i^*) - g_i(a_i^*-1).
\end{align}
\end{proof}


\subsection{Proof of \Cref{prop:uniform}}\label{sec:proof-of-uniform}
 For ease of exposition, we will begin by proving the result for $k=1$ and then handle the more general case.
\paragraph{First case ($k=1$).} Let us first consider the case $k=1.$ We would like to find an ordered tuple of non-negative integers $(a_1,\cdots,a_m)$ that maximizes
\begin{equation}
\sum_{t=1}^m p_t\mu_\DD(a_t,a_t)=\sum_{t=1}^m p_t\left(1 - \frac{1}{a_t+1}\right)
\end{equation}
subject to the constraint $\sum_{t=1}^m a_t = n.$
Our strategy will be to solve the relaxed optimization problem over non-negative reals, and then show that the optimal integer solution is ``close to'' the optimal real solution. Consider the function $g:[0,\infty)^m \rightarrow \RR$, where
\begin{equation}
g(x_1,\cdots,x_m) = \sum_{t=1}^m p_t\left(1 - \frac{1}{x_t+1}\right).
\end{equation}
Subject to the constraint $\sum_{t=1}^m x_t = n,$ $g(x_1,\cdots,x_m)$ is maximized exactly when 
\begin{equation}
\frac{\partial g}{\partial x_1} = \frac{\partial g}{\partial x_2} = \cdots = \frac{\partial g}{\partial x_m}.
\end{equation}
This is clear after noting that $g$ is convex on its domain and applying Lagrange multipliers. We have that
\begin{equation}
\frac{\partial g}{\partial x_t} = p_t\left(\frac{1}{x_t+1}\right)^2.
\end{equation}
So if $(x_1^*,\cdots,x_m^*)$ is a maximum, then $x_t^*+1 \propto \sqrt{p_t},$ meaning that
\begin{equation}
x_t^* = \left(\frac{\sqrt{p_t}}{\sum_{i=1}^m \sqrt{p_i}}\right)n-1.
\end{equation}
We now apply \Cref{lem:roundingloss}: if $(a_1^*,\cdots,a_m^*)$ is the optimal integer solution, then $|a_t^*-x_t^*|\le m$. Thus,
\begin{equation}
    \left|a_t^* - \frac{\sqrt{p_t}}{\sum_{i=1}^m \sqrt{p_i}}n\right|\le m+1
\end{equation}
for all $n$.

\paragraph{General case $k$.}  More generally, we would like to maximize
\begin{equation}
\sum_{t=1}^m p_t \sum_{i=1}^{\min\{a_t,k(n)\}} \mu_\DD(a_t-i+1, a_t).
\end{equation}
subject to the constraint $\sum_{t=1}^m a_t = n,$ where we let $k=k(n)$ be a function of $n$.
We first analyze each case of the inner summation:
\begin{equation}
\sum_{i=1}^{\min\{a_t,k(n)\}} \mu_\DD(a_t,a_t-i+1) =
\begin{cases}
\frac{a_t}{2} &\quad a_t\le k(n)\\
\sum_{i=1}^{k(n)} 1 - \frac{i}{a_t+1} = k(n) - \frac{k(n)^2 + k(n)}{2(a_t+1)}& \quad a_t> k(n)
\end{cases}.
\end{equation}
The top case $a_t \leq k(n)$ follows from all $a_t$ items of that type contributing to the objective, each with a mean value of $\frac12$.

Then define
\begin{equation}
g:[0,\infty)^m \rightarrow \RR,\quad
(x_1,\cdots,x_m)\mapsto \sum_{t=1}^m p_th(x_t)
\end{equation}
where
\begin{equation}
    h(x):=\begin{cases}
\frac{x}{2} &\quad x\le k(n)\\
\sum_{i=1}^{k(n)} 1 - \frac{i}{x+1} = k(n) - \frac{k(n)^2 + k(n)}{2(x+1)}& \quad x> k(n)
\end{cases}.
\end{equation}

So we have
\begin{equation}
\frac{\partial g}{\partial x_t} = 
\begin{cases}
\frac{p_tx}{2} &\quad x_t\le k(n)\\
p_t(k(n)^2 + k(n))\left(\frac{1}{x_t + 1}\right)^2 &\quad x_t> k(n)
\end{cases}.
\end{equation}
We first consider possible solutions where $x_t > k(n)$ for all $t\in[m].$ In this case,
\begin{equation}
\frac{\partial g}{\partial x_t} = p_t(k(n)^2 + k(n))\left(\frac{1}{x_t + 1}\right)^2
\end{equation}
is equal for all $t$ at the maximum, as before. Remarkably, the new $k(n)^2 + k(n)$ term drops out. So if $(x_1^*,\cdots,x_m^*)$ is the optimal real solution, again,
$x_t^*+1 \propto \sqrt{p_t},$ and the result follows as in the first case. Here, $x_t^* > k(n)$ holds whenever $k\le \frac{\sqrt{p_m}}{\sum_{i=1}^m \sqrt{p_i}}n - m - 1.$



\subsection{Proof of \Cref{thm:ber-varying}}\label{sec:proof-thm3}
\begin{proof}
We would like to find $a_1,\cdots,a_m$ that maximizes
\begin{equation}
\sum_{t=1}^m p_t \left(1 - (1 - q_t)^{a_t}\right) = 1 - \sum_{t=1}^m p_t(1-q_t)^{a_t}.
\end{equation}
subject to the constraint $\sum_{t=1}^m p_t = n.$ This is equivalent to minimizing
\begin{equation}
\sum_{t=1}^m p_t(1-q_t)^{a_t}.
\end{equation}
Now define a function
\begin{equation}
    g:[0,\infty)^m\rightarrow \RR,\quad (x_1,\cdots,x_m)\mapsto \sum_{t=1}^m p_t(1-q_t)^{x_t}.
\end{equation}
Subject to the constraint $\sum_{t=1}^m x_t = n,$ $g(x_1,\cdots,x_m)$ is maximized exactly when 
\begin{equation}
\frac{\partial g}{\partial x_1} = \frac{\partial g}{\partial x_2} = \cdots = \frac{\partial g}{\partial x_m}.
\end{equation}
We have
\begin{equation}
    \frac{\partial g}{\partial x_t} = -p_t(1-q_t)^{x_t}\log(1-q_t).
\end{equation}
Solving $\partial g/\partial x_i = \partial g/\partial x_j$ gives
\begin{align}
    p_i(1-q_i)^{x_i}\log(1-q_i) &= p_j(1-q_j)^{x_j}\log(1-q_j)\\
    \implies \log p_i + x_i\log(1-q_i) + \log\log(1-q_i) &= \log p_j + x_j\log(1-q_j) + \log\log(1-q_j)
\end{align}
It follows that $a_t\propto \frac{1}{\log(1-q_t)}$ for all $t$, where we have once again applied \Cref{lem:roundingloss}.
\end{proof}