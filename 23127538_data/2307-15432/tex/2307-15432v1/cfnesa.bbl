% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{talreja2021deep}
V.~Talreja, M.~C. Valenti, and N.~M. Nasrabadi, ``Deep hashing for secure
  multimodal biometrics,'' \emph{IEEE Transactions on Information Forensics and
  Security}, vol.~16, pp. 1306--1321, 2021.

\bibitem{zhang2022towards}
X.~Zhang, X.~Zheng, B.~Liu, X.~Wang, W.~Mao, D.~D. Zeng, and F.-Y. Wang,
  ``Towards human-machine recognition alignment: An adversarilly robust
  multimodal retrieval hashing framework,'' \emph{IEEE Transactions on
  Computational Social Systems}, pp. 1--13, 2022.

\bibitem{chen2023uamdnet}
G.~Chen, J.~Lin, and H.~Qin, ``{UAMD}-{N}et: A unified adaptive multimodal
  neural network for dense depth completion,'' \emph{IEEE Transactions on
  Circuits and Systems for Video Technology}, pp. 1--1, 2023.

\bibitem{he2023multimodal}
L.~He, Z.~Wang, L.~Wang, and F.~Li, ``Multimodal mutual attention-based
  sentiment analysis framework adapted to complicated contexts,'' \emph{IEEE
  Transactions on Circuits and Systems for Video Technology}, pp. 1--1, 2023.

\bibitem{jiao2020real}
M.~R.~L. Wenxiang~Jiao and I.~King, ``Real-time emotion recognition via
  attention gated hierarchical memory network,'' in \emph{The Thirty-Fourth
  {AAAI} Conference on Artificial Intelligence, {AAAI} 2020}, 2020, pp.
  8002--8009.

\bibitem{shen2021dialogxl}
W.~Shen, J.~Chen, X.~Quan, and Z.~Xie, ``{D}ialog{XL}: All-in-one xlnet for
  multi-party conversation emotion recognition,'' in \emph{Proceedings of the
  AAAI Conference on Artificial Intelligence}, 2021, pp. 13\,789--13\,797.

\bibitem{nie2022igcn}
W.~Nie, R.~Chang, M.~Ren, Y.~Su, and A.~Liu, ``{I}-{GCN}: Incremental graph
  convolution network for conversation emotion detection,'' \emph{IEEE
  Transactions on Multimedia}, vol.~24, pp. 4471--4481, 2022.

\bibitem{zhao2022cauain}
W.~Zhao, Y.~Zhao, and X.~Lu, ``{C}au{AIN}: Causal aware interaction network for
  emotion recognition in conversations,'' in \emph{Proceedings of the
  Thirty-First International Joint Conference on Artificial Intelligence,
  {IJCAI-22}}, L.~D. Raedt, Ed.\hskip 1em plus 0.5em minus 0.4em\relax
  International Joint Conferences on Artificial Intelligence Organization, 7
  2022, pp. 4524--4530, main Track.

\bibitem{li2022contrast}
S.~Li, H.~Yan, and X.~Qiu, ``Contrast and generation make bart a good dialogue
  emotion recognizer,'' in \emph{Proceedings of the AAAI Conference on
  Artificial Intelligence}, 2022, pp. 11\,002--11\,010.

\bibitem{fan2022isnet}
W.~Fan, X.~Xu, B.~Cai, and X.~Xing, ``{ISN}et: Individual standardization
  network for speech emotion recognition,'' \emph{IEEE/ACM Transactions on
  Audio, Speech, and Language Processing}, vol.~30, pp. 1803--1814, 2022.

\bibitem{latif2022multitask}
S.~Latif, R.~Rana, S.~Khalifa, R.~Jurdak, and B.~W. Schuller, ``Multitask
  learning from augmented auxiliary data for improving speech emotion
  recognition,'' \emph{IEEE Transactions on Affective Computing}, pp. 1--13,
  2022.

\bibitem{lei2022bat}
J.~Lei, X.~Zhu, and Y.~Wang, ``{BAT}: Block and token self-attention for speech
  emotion recognition,'' \emph{Neural Networks}, vol. 156, pp. 67--80, 2022.

\bibitem{zhou2022multiclassifier}
Y.~Zhou, X.~Liang, Y.~Gu, Y.~Yin, and L.~Yao, ``Multi-classifier interactive
  learning for ambiguous speech emotion recognition,'' \emph{IEEE/ACM
  Transactions on Audio, Speech, and Language Processing}, vol.~30, pp.
  695--705, 2022.

\bibitem{mai2022hybrid}
S.~Mai, Y.~Zeng, S.~Zheng, and H.~Hu, ``Hybrid contrastive learning of
  tri-modal representation for multimodal sentiment analysis,'' \emph{IEEE
  Transactions on Affective Computing}, pp. 1--1, 2022.

\bibitem{yu2021learning}
W.~Yu, H.~Xu, Z.~Yuan, and J.~Wu, ``Learning modality-specific representations
  with self-supervised multi-task learning for multimodal sentiment analysis,''
  \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  vol.~35, no.~12, pp. 10\,790--10\,797, May 2021.

\bibitem{poria2017context}
S.~Poria, E.~Cambria, D.~Hazarika, N.~Majumder, A.~Zadeh, and L.-P. Morency,
  ``Context-dependent sentiment analysis in user-generated videos,'' in
  \emph{Proceedings of the 55th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}.\hskip 1em plus 0.5em
  minus 0.4em\relax Association for Computational Linguistics, 2017, pp.
  873--883.

\bibitem{hazarika2018conversational}
D.~Hazarika, S.~Poria, A.~Zadeh, E.~Cambria, L.-P. Morency, and R.~Zimmermann,
  ``Conversational memory network for emotion recognition in dyadic dialogue
  videos,'' in \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, NIH Public Access.\hskip 1em plus
  0.5em minus 0.4em\relax Association for Computational Linguistics, 2018, pp.
  2122--2132.

\bibitem{hazarika2018icon}
D.~Hazarika, S.~Poria, R.~Mihalcea, E.~Cambria, and R.~Zimmermann, ``{ICON}:
  Interactive conversational memory network for multimodal emotion detection,''
  in \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural
  Language Processing}.\hskip 1em plus 0.5em minus 0.4em\relax Brussels,
  Belgium: Association for Computational Linguistics, Oct.-Nov. 2018, pp.
  2594--2604.

\bibitem{majumder2019dialoguernn}
N.~Majumder, S.~Poria, D.~Hazarika, R.~Mihalcea, A.~Gelbukh, and E.~Cambria,
  ``{DialogueRNN}: An attentive {RNN} for emotion detection in conversations,''
  in \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  vol.~33, no.~01.\hskip 1em plus 0.5em minus 0.4em\relax Association for the
  Advancement of Artificial Intelligence ({AAAI}), jul 2019, pp. 6818--6825.

\bibitem{hazarika2020misa}
D.~Hazarika, R.~Zimmermann, and S.~Poria, ``{MISA}: Modality-invariant and
  -specific representations for multimodal sentiment analysis,'' in
  \emph{Proceedings of the 28th {ACM} International Conference on
  Multimedia}.\hskip 1em plus 0.5em minus 0.4em\relax {ACM}, oct 2020, pp.
  1122--1131.

\bibitem{hu2021mmgcn}
J.~Hu, Y.~Liu, J.~Zhao, and Q.~Jin, ``{MMGCN}: Multimodal fusion via deep graph
  convolution network for emotion recognition in conversation,'' in
  \emph{Proceedings of the Annual Meeting of the Association for Computational
  Linguistics and the International Joint Conference on Natural Language
  Processing}.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2021, Conference Proceedings, pp. 5666--5675.

\bibitem{chen2021learning}
F.~Chen, Z.~Sun, D.~Ouyang, X.~Liu, and J.~Shao, ``Learning what and when to
  drop: Adaptive multimodal and contextual dynamics for emotion recognition in
  conversation,'' in \emph{Proceedings of the 29th ACM International Conference
  on Multimedia}, ser. MM '21.\hskip 1em plus 0.5em minus 0.4em\relax New York,
  NY, USA: Association for Computing Machinery, 2021, pp. 1064--1073.

\bibitem{hu2022mmdfn}
D.~Hu, X.~Hou, L.~Wei, L.~Jiang, and Y.~Mo, ``{MM}-{DFN}: Multimodal dynamic
  fusion network for emotion recognition in conversations,'' in \emph{ICASSP
  2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}, 2022, pp. 7037--7041.

\bibitem{chen2022modeling}
F.~Chen, J.~Shao, A.~Zhu, D.~Ouyang, X.~Liu, and H.~T. Shen, ``Modeling
  hierarchical uncertainty for multimodal emotion recognition in
  conversation,'' \emph{IEEE Transactions on Cybernetics}, pp. 1--12, 2022.

\bibitem{mao2021dialoguetrm}
Y.~Mao, G.~Liu, X.~Wang, W.~Gao, and X.~Li, ``{D}ialogue{TRM}: Exploring
  multi-modal emotional dynamics in a conversation,'' in \emph{Findings of the
  Association for Computational Linguistics: EMNLP 2021}.\hskip 1em plus 0.5em
  minus 0.4em\relax Punta Cana, Dominican Republic: Association for
  Computational Linguistics, Nov. 2021, pp. 2694--2704.

\bibitem{yuan2023rbagcn}
L.~Yuan, G.~Huang, F.~Li, X.~Yuan, C.-M. Pun, and G.~Zhong, ``{RBA-GCN}:
  Relational bilevel aggregation graph convolutional network for emotion
  recognition,'' \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, vol.~31, pp. 2325--2337, 2023.

\bibitem{yang2019xlnet}
Z.~Yang, Z.~Dai, Y.~Yang, J.~Carbonell, R.~Salakhutdinov, and Q.~V. Le,
  ``{XLN}et: Generalized autoregressive pretraining for language
  understanding,'' in \emph{Proceedings of the 33rd International Conference on
  Neural Information Processing Systems}.\hskip 1em plus 0.5em minus
  0.4em\relax Red Hook, NY, USA: Curran Associates Inc., 2019, pp. 1--11.

\bibitem{lewis2020bart}
M.~Lewis, Y.~Liu, N.~Goyal, M.~Ghazvininejad, A.~Mohamed, O.~Levy, V.~Stoyanov,
  and L.~Zettlemoyer, ``{BART}: Denoising sequence-to-sequence pre-training for
  natural language generation, translation, and comprehension,'' in
  \emph{Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}.\hskip 1em plus 0.5em minus 0.4em\relax Online:
  Association for Computational Linguistics, Jul. 2020, pp. 7871--7880.

\bibitem{xu2022mdan}
L.~Xu, Z.~Wang, B.~Wu, and S.~Lui, ``{MDAN}: Multi-level dependent attention
  network for visual emotion analysis,'' in \emph{2022 IEEE/CVF Conference on
  Computer Vision and Pattern Recognition (CVPR)}, 2022, pp. 9469--9478.

\bibitem{zhu2017dependency}
X.~Zhu, L.~Li, W.~Zhang, T.~Rao, M.~Xu, Q.~Huang, and D.~Xu, ``{D}ependency
  {E}xploitation: A unified cnn-rnn approach for visual emotion recognition,''
  in \emph{Proceedings of the Twenty-Sixth International Joint Conference on
  Artificial Intelligence, {IJCAI-17}}, 2017, pp. 3595--3601.

\bibitem{she2020wscnet}
D.~She, J.~Yang, M.-M. Cheng, Y.-K. Lai, P.~L. Rosin, and L.~Wang, ``{WSCN}et:
  Weakly supervised coupled networks for visual sentiment classification and
  detection,'' \emph{IEEE Transactions on Multimedia}, vol.~22, no.~5, pp.
  1358--1371, 2020.

\bibitem{joshi2022cogmen}
A.~Joshi, A.~Bhat, A.~Jain, A.~Singh, and A.~Modi, ``{COGMEN}: {CO}ntextualized
  {GNN} based multimodal emotion recognitio{N},'' in \emph{Proceedings of the
  2022 Conference of the North American Chapter of the Association for
  Computational Linguistics: Human Language Technologies}.\hskip 1em plus 0.5em
  minus 0.4em\relax Seattle, United States: Association for Computational
  Linguistics, Jul. 2022, pp. 4148--4164.

\bibitem{hu2022unimse}
G.~Hu, T.-E. Lin, Y.~Zhao, G.~Lu, Y.~Wu, and Y.~Li, ``{U}ni{MSE}: Towards
  unified multimodal sentiment analysis and emotion recognition,'' in
  \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural
  Language Processing}.\hskip 1em plus 0.5em minus 0.4em\relax Abu Dhabi,
  United Arab Emirates: Association for Computational Linguistics, Dec. 2022,
  pp. 7837--7851.

\bibitem{raffel2020exploring}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu, ``Exploring the limits of transfer learning with a
  unified text-to-text transformer,'' \emph{The Journal of Machine Learning
  Research}, vol.~21, no.~1, jan 2020.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in
  \emph{Proceedings of the 31st International Conference on Neural Information
  Processing Systems}, ser. NIPS'17.\hskip 1em plus 0.5em minus 0.4em\relax Red
  Hook, NY, USA: Curran Associates Inc., 2017, pp. 6000--6010.

\bibitem{gao2021simcse}
T.~Gao, X.~Yao, and D.~Chen, ``{S}im{CSE}: Simple contrastive learning of
  sentence embeddings,'' in \emph{Proceedings of the 2021 Conference on
  Empirical Methods in Natural Language Processing}.\hskip 1em plus 0.5em minus
  0.4em\relax Online and Punta Cana, Dominican Republic: Association for
  Computational Linguistics, Nov. 2021, pp. 6894--6910.

\bibitem{kendall2018multi}
R.~Cipolla, Y.~Gal, and A.~Kendall, ``Multi-task learning using uncertainty to
  weigh losses for scene geometry and semantics,'' in \emph{Proceedings of 2018
  {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition}.\hskip
  1em plus 0.5em minus 0.4em\relax {IEEE}, jun 2018, pp. 7482--7491.

\bibitem{busso2008iemocap}
C.~Busso, M.~Bulut, C.-C. Lee, A.~Kazemzadeh, E.~Mower, S.~Kim, J.~N. Chang,
  S.~Lee, and S.~S. Narayanan, ``{IEMOCAP}: interactive emotional dyadic motion
  capture database,'' \emph{Language Resources and Evaluation}, vol.~42, no.~4,
  pp. 335--359, nov 2008.

\bibitem{poria2018meld}
S.~Poria, D.~Hazarika, N.~Majumder, G.~Naik, E.~Cambria, and R.~Mihalcea,
  ``{MELD}: A multimodal multi-party dataset for emotion recognition in
  conversations,'' in \emph{Proceedings of the 57th Annual Meeting of the
  Association for Computational Linguistics}.\hskip 1em plus 0.5em minus
  0.4em\relax Florence, Italy: Association for Computational Linguistics, Jul.
  2019, pp. 527--536.

\bibitem{huang2017densely}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger, ``Densely connected
  convolutional networks,'' in \emph{2017 IEEE Conference on Computer Vision
  and Pattern Recognition (CVPR)}, 2017, pp. 2261--2269.

\bibitem{barsoum2016training}
E.~Barsoum, C.~Zhang, C.~C. Ferrer, and Z.~Zhang, ``Training deep networks for
  facial expression recognition with crowd-sourced label distribution,'' in
  \emph{Proceedings of the 18th ACM International Conference on Multimodal
  Interaction}, ser. ICMI '16.\hskip 1em plus 0.5em minus 0.4em\relax New York,
  NY, USA: Association for Computing Machinery, 2016, pp. 279--283.

\bibitem{schuller2011recognising}
B.~Schuller, A.~Batliner, S.~Steidl, and D.~Seppi, ``Recognising realistic
  emotions and affect in speech: State of the art and lessons learnt from the
  first challenge,'' \emph{Speech Commun.}, vol.~53, no. 9--10, pp. 1062--1087,
  nov 2011.

\bibitem{ghosal2020cosmic}
D.~Ghosal, N.~Majumder, A.~Gelbukh, R.~Mihalcea, and S.~Poria, ``{COSMIC}:
  Commonsense knowledge for emotion identification in conversations,'' in
  \emph{Findings of the Association for Computational Linguistics: EMNLP
  2020}.\hskip 1em plus 0.5em minus 0.4em\relax Association for Computational
  Linguistics, 2020, pp. 2470--2481.

\bibitem{liu2020roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
  L.~Zettlemoyer, and V.~Stoyanov, ``Ro{BERT}a: A robustly optimized {BERT}
  pretraining approach,'' pp. 1--15, 2020.

\bibitem{loshchilov2018decoupled}
I.~Loshchilov and F.~Hutter, ``Decoupled weight decay regularization,'' in
  \emph{International Conference on Learning Representations}, 2019, pp. 1--8.

\end{thebibliography}
