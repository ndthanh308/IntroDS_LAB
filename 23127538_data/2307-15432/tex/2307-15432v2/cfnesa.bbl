% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{jiao2020real}
M.~R.~L. Wenxiang~Jiao and I.~King, ``Real-time emotion recognition via
  attention gated hierarchical memory network,'' in \emph{The Thirty-Fourth
  {AAAI} Conference on Artificial Intelligence}, 2020, pp. 8002--8009.

\bibitem{shen2021dialogxl}
W.~Shen, J.~Chen, X.~Quan, and Z.~Xie, ``{D}ialog{XL}: All-in-one xlnet for
  multi-party conversation emotion recognition,'' in \emph{Proceedings of the
  AAAI Conference on Artificial Intelligence}, 2021, pp. 13\,789--13\,797.

\bibitem{nie2022igcn}
W.~Nie, R.~Chang, M.~Ren, Y.~Su, and A.~Liu, ``{I}-{GCN}: Incremental graph
  convolution network for conversation emotion detection,'' \emph{IEEE
  Transactions on Multimedia}, vol.~24, pp. 4471--4481, 2022.

\bibitem{zhao2022cauain}
W.~Zhao, Y.~Zhao, and X.~Lu, ``{C}au{AIN}: Causal aware interaction network for
  emotion recognition in conversations,'' in \emph{Proceedings of the
  Thirty-First International Joint Conference on Artificial Intelligence},
  L.~D. Raedt, Ed., 2022, pp. 4524--4530, main Track.

\bibitem{li2022contrast}
S.~Li, H.~Yan, and X.~Qiu, ``Contrast and generation make bart a good dialogue
  emotion recognizer,'' in \emph{Proceedings of the AAAI Conference on
  Artificial Intelligence}, 2022, pp. 11\,002--11\,010.

\bibitem{fan2022isnet}
W.~Fan, X.~Xu, B.~Cai, and X.~Xing, ``{ISN}et: Individual standardization
  network for speech emotion recognition,'' \emph{IEEE/ACM Transactions on
  Audio, Speech, and Language Processing}, vol.~30, pp. 1803--1814, 2022.

\bibitem{latif2022multitask}
S.~Latif, R.~Rana, S.~Khalifa, R.~Jurdak, and B.~W. Schuller, ``Multitask
  learning from augmented auxiliary data for improving speech emotion
  recognition,'' \emph{IEEE Transactions on Affective Computing}, pp. 1--13,
  2022.

\bibitem{lei2022bat}
J.~Lei, X.~Zhu, and Y.~Wang, ``{BAT}: Block and token self-attention for speech
  emotion recognition,'' \emph{Neural Networks}, vol. 156, pp. 67--80, 2022.

\bibitem{zhou2022multiclassifier}
Y.~Zhou, X.~Liang, Y.~Gu, Y.~Yin, and L.~Yao, ``Multi-classifier interactive
  learning for ambiguous speech emotion recognition,'' \emph{IEEE/ACM
  Transactions on Audio, Speech, and Language Processing}, vol.~30, pp.
  695--705, 2022.

\bibitem{he2023multimodal}
L.~He, Z.~Wang, L.~Wang, and F.~Li, ``Multimodal mutual attention-based
  sentiment analysis framework adapted to complicated contexts,'' \emph{IEEE
  Transactions on Circuits and Systems for Video Technology}, pp. 1--1, 2023.

\bibitem{mai2022hybrid}
S.~Mai, Y.~Zeng, S.~Zheng, and H.~Hu, ``Hybrid contrastive learning of
  tri-modal representation for multimodal sentiment analysis,'' \emph{IEEE
  Transactions on Affective Computing}, pp. 1--1, 2022.

\bibitem{yu2021learning}
W.~Yu, H.~Xu, Z.~Yuan, and J.~Wu, ``Learning modality-specific representations
  with self-supervised multi-task learning for multimodal sentiment analysis,''
  in \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  vol.~35, no.~12, 2021, pp. 10\,790--10\,797.

\bibitem{poria2017context}
S.~Poria, E.~Cambria, D.~Hazarika, N.~Majumder, A.~Zadeh, and L.-P. Morency,
  ``Context-dependent sentiment analysis in user-generated videos,'' in
  \emph{Proceedings of the 55th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, 2017, pp. 873--883.

\bibitem{hazarika2018conversational}
D.~Hazarika, S.~Poria, A.~Zadeh, E.~Cambria, L.-P. Morency, and R.~Zimmermann,
  ``Conversational memory network for emotion recognition in dyadic dialogue
  videos,'' in \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}.\hskip 1em plus 0.5em minus 0.4em\relax
  NIH Public Access, 2018, pp. 2122--2132.

\bibitem{hazarika2018icon}
D.~Hazarika, S.~Poria, R.~Mihalcea, E.~Cambria, and R.~Zimmermann, ``{ICON}:
  Interactive conversational memory network for multimodal emotion detection,''
  in \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural
  Language Processing}, Brussels, Belgium, 2018, pp. 2594--2604.

\bibitem{majumder2019dialoguernn}
N.~Majumder, S.~Poria, D.~Hazarika, R.~Mihalcea, A.~Gelbukh, and E.~Cambria,
  ``{DialogueRNN}: An attentive {RNN} for emotion detection in conversations,''
  in \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  vol.~33, no.~01, 2019, pp. 6818--6825.

\bibitem{hazarika2020misa}
D.~Hazarika, R.~Zimmermann, and S.~Poria, ``{MISA}: Modality-invariant and
  -specific representations for multimodal sentiment analysis,'' in
  \emph{Proceedings of the 28th {ACM} International Conference on Multimedia},
  2020, pp. 1122--1131.

\bibitem{hu2021mmgcn}
J.~Hu, Y.~Liu, J.~Zhao, and Q.~Jin, ``{MMGCN}: Multimodal fusion via deep graph
  convolution network for emotion recognition in conversation,'' in
  \emph{Proceedings of the Annual Meeting of the Association for Computational
  Linguistics and the International Joint Conference on Natural Language
  Processing}, 2021, Conference Proceedings, pp. 5666--5675.

\bibitem{chen2021learning}
F.~Chen, Z.~Sun, D.~Ouyang, X.~Liu, and J.~Shao, ``Learning what and when to
  drop: Adaptive multimodal and contextual dynamics for emotion recognition in
  conversation,'' in \emph{Proceedings of the 29th ACM International Conference
  on Multimedia}, New York, NY, USA, 2021, pp. 1064--1073.

\bibitem{hu2022mmdfn}
D.~Hu, X.~Hou, L.~Wei, L.~Jiang, and Y.~Mo, ``{MM}-{DFN}: Multimodal dynamic
  fusion network for emotion recognition in conversations,'' in
  \emph{Proceedings of ICASSP 2022 - 2022 IEEE International Conference on
  Acoustics, Speech and Signal Processing}, 2022, pp. 7037--7041.

\bibitem{chen2022modeling}
F.~Chen, J.~Shao, A.~Zhu, D.~Ouyang, X.~Liu, and H.~T. Shen, ``Modeling
  hierarchical uncertainty for multimodal emotion recognition in
  conversation,'' \emph{IEEE Transactions on Cybernetics}, pp. 1--12, 2022.

\bibitem{mao2021dialoguetrm}
Y.~Mao, G.~Liu, X.~Wang, W.~Gao, and X.~Li, ``{D}ialogue{TRM}: Exploring
  multi-modal emotional dynamics in a conversation,'' in \emph{Findings of the
  Association for Computational Linguistics: EMNLP 2021}, Punta Cana, Dominican
  Republic, 2021, pp. 2694--2704.

\bibitem{ghosal2020cosmic}
D.~Ghosal, N.~Majumder, A.~Gelbukh, R.~Mihalcea, and S.~Poria, ``{COSMIC}:
  Commonsense knowledge for emotion identification in conversations,'' in
  \emph{Findings of the Association for Computational Linguistics: EMNLP
  2020}.\hskip 1em plus 0.5em minus 0.4em\relax Association for Computational
  Linguistics, 2020, pp. 2470--2481.

\bibitem{shen2021directed}
W.~Shen, S.~Wu, Y.~Yang, and X.~Quan, ``Directed acyclic graph network for
  conversational emotion recognition,'' in \emph{Proceedings of the 59th Annual
  Meeting of the Association for Computational Linguistics and the 11th
  International Joint Conference on Natural Language Processing}, 2021, pp.
  1551--1560.

\bibitem{yang2019xlnet}
Z.~Yang, Z.~Dai, Y.~Yang, J.~Carbonell, R.~Salakhutdinov, and Q.~V. Le,
  ``{XLN}et: Generalized autoregressive pretraining for language
  understanding,'' in \emph{Proceedings of the 33rd International Conference on
  Neural Information Processing Systems}, Red Hook, NY, USA, 2019, pp. 1--11.

\bibitem{lewis2020bart}
M.~Lewis, Y.~Liu, N.~Goyal, M.~Ghazvininejad, A.~Mohamed, O.~Levy, V.~Stoyanov,
  and L.~Zettlemoyer, ``{BART}: Denoising sequence-to-sequence pre-training for
  natural language generation, translation, and comprehension,'' in
  \emph{Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, Online, 2020, pp. 7871--7880.

\bibitem{huang2015aninvestigation}
Z.~Huang, ``An investigation of emotion changes from speech,'' in
  \emph{Proceedings of 2015 International Conference on Affective Computing and
  Intelligent Interaction (ACII)}, 2015, pp. 733--736.

\bibitem{huang2016detecting}
Z.~Huang and J.~Epps, ``Detecting the instant of emotion change from speech
  using a martingale framework,'' in \emph{Proceedings of 2016 IEEE
  International Conference on Acoustics, Speech and Signal Processing}, 2016,
  pp. 5195--5199.

\bibitem{xu2022mdan}
L.~Xu, Z.~Wang, B.~Wu, and S.~Lui, ``{MDAN}: Multi-level dependent attention
  network for visual emotion analysis,'' in \emph{Proceedings of 2022 IEEE/CVF
  Conference on Computer Vision and Pattern Recognition}, 2022, pp. 9469--9478.

\bibitem{zhu2017dependency}
X.~Zhu, L.~Li, W.~Zhang, T.~Rao, M.~Xu, Q.~Huang, and D.~Xu, ``{D}ependency
  {E}xploitation: A unified cnn-rnn approach for visual emotion recognition,''
  in \emph{Proceedings of the Twenty-Sixth International Joint Conference on
  Artificial Intelligence}, 2017, pp. 3595--3601.

\bibitem{she2020wscnet}
D.~She, J.~Yang, M.-M. Cheng, Y.-K. Lai, P.~L. Rosin, and L.~Wang, ``{WSCN}et:
  Weakly supervised coupled networks for visual sentiment classification and
  detection,'' \emph{IEEE Transactions on Multimedia}, vol.~22, no.~5, pp.
  1358--1371, 2020.

\bibitem{joshi2022cogmen}
A.~Joshi, A.~Bhat, A.~Jain, A.~Singh, and A.~Modi, ``{COGMEN}: {CO}ntextualized
  {GNN} based multimodal emotion recognitio{N},'' in \emph{Proceedings of the
  2022 Conference of the North American Chapter of the Association for
  Computational Linguistics: Human Language Technologies}, Seattle, United
  States, 2022, pp. 4148--4164.

\bibitem{hu2022unimse}
G.~Hu, T.-E. Lin, Y.~Zhao, G.~Lu, Y.~Wu, and Y.~Li, ``{U}ni{MSE}: Towards
  unified multimodal sentiment analysis and emotion recognition,'' in
  \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural
  Language Processing}, Abu Dhabi, United Arab Emirates, 2022, pp. 7837--7851.

\bibitem{raffel2020exploring}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu, ``Exploring the limits of transfer learning with a
  unified text-to-text transformer,'' \emph{The Journal of Machine Learning
  Research}, vol.~21, no.~1, 2020.

\bibitem{bansal-etal-2022-shapes}
K.~Bansal, H.~Agarwal, A.~Joshi, and A.~Modi, ``Shapes of emotions: Multimodal
  emotion recognition in conversations via emotion shifts,'' in
  \emph{Proceedings of the First Workshop on Performance and Interpretability
  Evaluations of Multimodal, Multipurpose, Massive-Scale Models}, Virtual,
  2022, pp. 44--56.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in
  \emph{Proceedings of the 31st International Conference on Neural Information
  Processing Systems}, Red Hook, NY, USA, 2017, pp. 6000--6010.

\bibitem{tsai-etal-2019-multimodal}
Y.-H.~H. Tsai, S.~Bai, P.~P. Liang, J.~Z. Kolter, L.-P. Morency, and
  R.~Salakhutdinov, ``Multimodal transformer for unaligned multimodal language
  sequences,'' in \emph{Proceedings of the 57th Annual Meeting of the
  Association for Computational Linguistics}, A.~Korhonen, D.~Traum, and
  L.~M{\`a}rquez, Eds., Florence, Italy, 2019, pp. 6558--6569.

\bibitem{Goncalves2022AuxFormer}
L.~Goncalves and C.~Busso, ``Auxformer: Robust approach to audiovisual emotion
  recognition,'' in \emph{ICASSP 2022 - 2022 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, 2022, pp. 7357--7361.

\bibitem{Wagner2023Dawn}
J.~Wagner, A.~Triantafyllopoulos, H.~Wierstorf, M.~Schmitt, F.~Burkhardt,
  F.~Eyben, and B.~W. Schuller, ``Dawn of the transformer era in speech emotion
  recognition: Closing the valence gap,'' \emph{IEEE Transactions on Pattern
  Analysis and Machine Intelligence}, vol.~45, no.~9, pp. 10\,745--10\,759,
  2023.

\bibitem{dosovitskiy2021an}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,
  and N.~Houlsby, ``An image is worth 16x16 words: Transformers for image
  recognition at scale,'' in \emph{Proceedings of the Ninth International
  Conference on Learning Representations}, 2021.

\bibitem{Li2023BLIP2}
J.~Li, D.~Li, S.~Savarese, and S.~Hoi, ``{BLIP-2}: bootstrapping language-image
  pre-training with frozen image encoders and large language models,'' in
  \emph{Proceedings of the 40th International Conference on Machine Learning},
  ser. ICML'23.\hskip 1em plus 0.5em minus 0.4em\relax JMLR.org, 2023.

\bibitem{liu2023visual}
H.~Liu, C.~Li, Q.~Wu, and Y.~J. Lee, ``Visual instruction tuning,'' in
  \emph{Proceedings of the Thirty-seventh Conference on Neural Information
  Processing Systems}, 2023.

\bibitem{gao2021simcse}
T.~Gao, X.~Yao, and D.~Chen, ``{S}im{CSE}: Simple contrastive learning of
  sentence embeddings,'' in \emph{Proceedings of the 2021 Conference on
  Empirical Methods in Natural Language Processing}.\hskip 1em plus 0.5em minus
  0.4em\relax Online and Punta Cana, Dominican Republic: Association for
  Computational Linguistics, 2021, pp. 6894--6910.

\bibitem{kendall2018multi}
R.~Cipolla, Y.~Gal, and A.~Kendall, ``Multi-task learning using uncertainty to
  weigh losses for scene geometry and semantics,'' in \emph{Proceedings of 2018
  {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition}.\hskip
  1em plus 0.5em minus 0.4em\relax {IEEE}, 2018, pp. 7482--7491.

\bibitem{poria2018meld}
S.~Poria, D.~Hazarika, N.~Majumder, G.~Naik, E.~Cambria, and R.~Mihalcea,
  ``{MELD}: A multimodal multi-party dataset for emotion recognition in
  conversations,'' in \emph{Proceedings of the 57th Annual Meeting of the
  Association for Computational Linguistics}, Florence, Italy, 2019, pp.
  527--536.

\bibitem{busso2008iemocap}
C.~Busso, M.~Bulut, C.-C. Lee, A.~Kazemzadeh, E.~Mower, S.~Kim, J.~N. Chang,
  S.~Lee, and S.~S. Narayanan, ``{IEMOCAP}: interactive emotional dyadic motion
  capture database,'' \emph{Language Resources and Evaluation}, vol.~42, no.~4,
  pp. 335--359, 2008.

\bibitem{huang2017densely}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger, ``Densely connected
  convolutional networks,'' in \emph{2017 IEEE Conference on Computer Vision
  and Pattern Recognition}, 2017, pp. 2261--2269.

\bibitem{barsoum2016training}
E.~Barsoum, C.~Zhang, C.~C. Ferrer, and Z.~Zhang, ``Training deep networks for
  facial expression recognition with crowd-sourced label distribution,'' in
  \emph{Proceedings of the 18th ACM International Conference on Multimodal
  Interaction}, New York, NY, USA, 2016, pp. 279--283.

\bibitem{schuller2011recognising}
B.~Schuller, A.~Batliner, S.~Steidl, and D.~Seppi, ``Recognising realistic
  emotions and affect in speech: State of the art and lessons learnt from the
  first challenge,'' \emph{Speech Commun.}, vol.~53, no. 9--10, pp. 1062--1087,
  2011.

\bibitem{liu2019roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
  L.~Zettlemoyer, and V.~Stoyanov, ``Ro{BERT}a: A robustly optimized {BERT}
  pretraining approach,'' \emph{arXiv preprint arXiv:1907.11692}, pp. 1--15,
  2019.

\bibitem{loshchilov2018decoupled}
I.~Loshchilov and F.~Hutter, ``Decoupled weight decay regularization,'' in
  \emph{Proceedings of the Seventh International Conference on Learning
  Representations}, 2019, pp. 1--8.

\end{thebibliography}
