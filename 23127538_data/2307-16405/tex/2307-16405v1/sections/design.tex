\section{Design}
The design philosophy of causal-learn is centered around building an open-source, modular, easily extensible and embeddable Python platform for learning causality from data and making use of causality for various purposes. Due to the different goals, assumptions, and techniques between causal learning and traditional learning tasks, newcomers to the field often find it hard to get a clear picture of the developments in modern causality research. Thus, we briefly introduce the algorithms and functionalities in causal-learn with a special focus on their use cases and suitable application scenarios.

\subsection{Search methods}
Causal-learn covers representative causal discovery methods across all major categories with official implementation of most algorithms. We briefly introduce the methods as follows. It is worth noting that we are actively updating the library to incorporate latest algorithms.
\begin{itemize}
    \item \textbf{Constraint-based causal discovery methods.} Current algorithms under that category are PC \citep{spirtes2000causation}, FCI \citep{spirtes1995causal}, and CD-NOD \citep{huang2020causal}. PC is a classical and widely-used algorithm with consistency guarantee under independent and identically distributed (i.i.d.) sampling assuming no latent confounders, the faithfulness assumption, and the causal Markov condition, which has been extensively applied in many fields. By continuously applying (conditional) independence tests on subsets of variables of increasing size in a smart way, its search procedure returns a Markov Equivalence Class (MEC), of which the graphical object consists of a mixture of directed and undirected edges, known as a Completed Partially Directed Acyclic Graph (CPDAG). PC is highly adaptable to various use cases, facilitated by the selection of an appropriate independence test; it can handle data with different assumptions, such as Fisher-Z test \citep{fisher1921014} for linear Gaussian data, Chi/G-squared test \citep{tsamardinos2006max} for discrete data, and Kernel-based Conditional Independence (KCI) test \citep{zhang2011kernel} for the nonparametric case. Moreover, causal-learn provides an extension, Missing-Value PC (MV-PC) \citep{tu2019causal}, to address issues of missing data. Furthermore, we have implemented FCI for causal structures that include hidden confounders (it indicates the possible existence of hidden confounders whenever the possibility cannot be excluded, but it cannot help determine possible relations among them), and causal discovery from nonstationary/heterogeneous data (CD-NOD). These constraint-based methods offer wide applicability as they can accommodate various types of data distributions and causal relations, provided that appropriate conditional independence testing methods are utilized. However, genenerally speaking, they may not be able to determine the complete causal graph uniquely and, accordingly, there usually exist some undirected edges in the returned CPDAGs.

    \item \textbf{Score-based causal discovery methods.} Different from the search style of constraint-bed methods, score-based methods find the causal structure by optimizing a properly defined score function. Greedy Equivalence Search (GES) \citep{chickering2002optimal} is a well-known two-phase procedure that directly searches over the space of equivalence classes. Similarly, exact search (e.g., A* \citep{yuan2013learning}, Dynamic Programming \citep{silander2006simple}), and permutation-based search (e.g., GRaSP \citep{lam2022greedy}) apply different search strategies to return a set of the sparsest Directed Acyclic Graphs (DAGs) that contains the true model under assumptions strictly weaker than faithfulness. These score-based methods are versatile, able to accommodate a wide array of data and causal relations by choosing suitable score functions, such as BIC \citep{schwarz1978estimating} for linear Guassian data, BDeu \citep{buntine1991theory} for discrete data, and Generalized Score \citep{huang2018generalized} for the nonparametric case. The choice of score function can be conveniently adjusted as a hyperparameter.

    \item \looseness=-1 \textbf{Causal discovery methods based on constrained functional causal models.} While constraint-based and score-based methods offer flexibility through the selection of an appropriate independence test or score function, they are limited to returning equivalence classes, yielding non-unique solutions where the causal direction between certain variable pairs remains indeterminate. In contrast, assuming specific Functional Causal Models (FCMs)--that is, functions in a particular functional class to specify how the effect is generated from its direct causes and noise--allows for the full determination of the causal structure, albeit at the cost of certain trade-offs. Causal-learn incorporates algorithms based on several FCM variants, capable of producing unique causal directions. Examples include the linear non-Gaussian acyclic model (LiNGAM) \citep{shimizu2006linear} and its variant, i.e., DirectLiNGAM \citep{shimizu2011directlingam}, which have been extensively applied for non-Gaussian noises with linear relations. VAR-LiNGAM \citep{hyvarinen2010estimation}, which combines LiNGAM with vector autoregressive models (VAR), to estimate both time-delayed and instantaneous causal relations from time series. RCD \citep{maeda2020rcd}, an extension of LiNGAM, allows for hidden confounders, while CAM-UV \citep{maeda2021causal} further extends this to the nonlinear additive noise case. In addition, the additive noise model (ANM) \citep{hoyer2008nonlinear} has been proven to be identifiable in the presence of nonlinearity and additive noises. Furthermore, we have also incorporated the post-nonlinear (PNL) causal model \citep{zhang2009identifiability}, a highly general form (with LiNGAM and ANM as special cases) that has been demonstrated to be identifiable in the generic case, barring five specific situations described in \citep{zhang2009identifiability}.

    \item \textbf{Causal representation learning: Finding causally related hidden variables.} Latent variables play an instrumental role in a multitude of real-world scenarios, often acting as hidden confounders that influence observed variables. Unfortunately, most existing methods may fail to produce convincing results in cases with latent variables (confounders). In causal-learn, we implement the Generalized Independent Noise (GIN) condition \citep{xie2020generalized} for estimating linear non-Gaussian latent variable causal model, which allows causal relationships between latent variables and multiple latent variables behind any two observed variables. This promises to improve the detection and understanding of the complex, often hidden, causal structures that govern real-world phenomena.

\end{itemize}

Besides, causal-learn also has Granger causality \citep{granger1969investigating, granger1980testing} implemented for statistical but not causal\footnote{As mentioned by Granger, Granger causality is not necessarily true causality. In fact, If one assumes 1) that there is no latent confounding process, 2) that the data are sampled at the right causal frequency, and 3) that there are no instantaneous causal influences, then Granger causality defined by Granger \citep{granger1980testing} can be seen as causal relations that can be discovered from stochastic processes with constraints-based methods such as PC. Of course, if those assumptions are violated, one may still apply Granger causal analysis, but the estimated relations may not be true causal influences.} time series analysis. Through the collective efforts of various teams and the contributions of the open-source community, causal-learn is always under active development to incorporate the most recent advancements in causal discovery and make them available to both practitioners and researchers.

\subsection{(Conditional) independence tests}

In addition to its comprehensive search methods, causal-learn also provides a variety of (conditional) independence tests as independent modules. Besides being an essential parts of several search methods, these tests can also be independently utilized and seamlessly integrated into existing statistical analysis pipelines. Currently,the library features a diverse array of such tests including Fisher-z test \citep{fisher1921014}, Missing-value Fisher-z test, Chi-Square test, Kernel-based conditional independence (KCI) test and independence test \citep{zhang2011kernel}, and G-Square test \citep{tsamardinos2006max}, each with distinct capabilities and benefits. The Fisher-z test is ideally suited for linear-Gaussian data, while the Missing-value Fisher-z test addresses the challenges of missing values by implementing a testwise-deletion approach. For categorical variables, the Chi-Square and G-Square tests are most effective. For users interested in a nonparametric test or the case with mixed categorical and continuous data types, the KCI test is an option. Overall, the range of tests offered by causal-learn underscores its versatility in handling diverse data types.

\subsection{Score functions}
\looseness=-1
Moreover, a diverse range of score functions is available in \textit{causal-learn}. These score functions quantify the goodness of fit of a model to the data, a crucial measure in score-based causal discovery methods, and can also be utilized independently for model selection in a broader range. Among these, the Bayesian Information Criterion (BIC) score \citep{schwarz1978estimating} is used extensively, offering a balance between model complexity and fit to the data. Another important score function is the Bayesian Dirichlet equivalent uniform (BDeu) score \citep{buntine1991theory}. This score function, especially beneficial for discrete data, incorporates a uniform prior over the set of Bayesian networks. Additionally, the Generalized Score \citep{huang2018generalized} is also available in causal-learn, which offers the flexibility to accommodate more complex scenarios and is beneficial for nonparametric cases where the true data-generating process does not align with the assumptions of BIC (linear Gaussian) or BDeu (discrete).


\subsection{Utilities}

Causal-learn further offers a suite of utilities designed to streamline the assembly of causal analysis pipelines. The package features a comprehensive range of graph operations encompassing transformations among various graphical objects integral to causal discovery. These include Directed Acyclic Graphs (DAGs), Completed Partially Directed Acyclic Graphs (CPDAGs), Partially Directed Acyclic Graphs (PDAGs), and Partially Ancestral Graphs (PAGs). Additionally, to enhance the convenience of experimental processes, \textit{causal-learn} features a set of commonly used evaluation metrics to appraise the quality of the causal graphs discovered. These metrics include precision and recall for arrow directions or adjacency matrices, along with the Structural Hamming Distance \citep{acid2003searching}.

\subsection{Demos, documentation, and benchmark datasets}

The \textit{causal-learn} package also contains extensive usage examples of all search methods, (conditional) independence tests, score functions, and utilities at 
\\ \centerline{ \url{https://github.com/py-why/causal-learn/tree/main/tests}.} 
\\
Furthermore, detailed documentation is available at \\
\centerline{\url{https://causal-learn.readthedocs.io/en/latest/}.} \\
It is worth noting that it also includes a collection of well-tested benchmark datasets--since ground-truth causal relations are often unknown for real data, evaluation of causal discovery methods has been notoriously known to be hard, and we hope the availability of such benchmark datasets can help alleviate this issue and inspire the collection of more real-world datasets with (at least partially) known causal relations. 


