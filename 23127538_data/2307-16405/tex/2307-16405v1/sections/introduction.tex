\section{Introduction}
A traditional way to uncover causal relationships is to resort to interventions or randomized experiments, which are often impractical due to their cost or logistical limitations. Hence, the importance of causal discovery, i.e., the process of revealing causal information through the analysis of purely observational data, has become increasingly apparent across diverse disciplines, including genomics, ecology, neuroscience, and epidemiology, among others \citep{glymour2019review}. For instance, in genomics, causal discovery has been instrumental in understanding the relationships between certain genes and diseases. Researchers might not have the resources to manipulate gene expressions, but they can analyze observational data, which are usually widely available, such as genomic databases, to uncover potential causal relationships. This can lead to breakthroughs in disease treatment and prevention strategies without the cost of traditional experimentation.

\looseness=-1
Current strategies for causal discovery can be broadly classified into constraint-based, score-based, functional causal models-based, and methods that recover latent variables. Constraint-based and score-based methods have been employed for causal discovery since the 1990s, using conditional independence relationships in data to uncover information about the underlying causal structure. Algorithms such as Peter-Clark (PC) \citep{spirtes2000causation} and Fast Causal Inference (FCI) \citep{spirtes1995causal} are popular, with PC assuming causal sufficiency and FCI handling latent confounders. In cases without latent confounders, score-based algorithms like the Greedy Equivalence Search (GES) \citep{chickering2002optimal} aim to find the causal structure by optimizing a score function. These methods provide asymptotically correct results, accommodating various data distributions and functional relations but do not necessarily provide complete causal information as they usually output Markov equivalence classes of causal structures (graphs within the same Markov equivalence class have the same conditional independence relations among the variables). 

\looseness=-1
On the other hand, algorithms based on Functional Causal Models (FCMs) have exhibited the ability to distinguish between different Directed Acyclic Graphs (DAGs) within the same equivalence class, thanks to additional assumptions on the data distribution beyond conditional independence relations. An FCM represents the effect variable as a function of the direct causes and a noise term; it renders causal direction identifiable due to the independence condition between the noise and cause: one can show that under appropriate assumptions on the functional model class and distributions of the involved variables, the estimated noise cannot be independent from the hypothetical cause in the reverse direction \citep{shimizu2006linear,hoyer2008nonlinear,zhang2009identifiability}. More recently, the Generalized Independent Noise condition (GIN) \citep{xie2020generalized} has demonstrated its potential in learning hidden causal variables and their relations in the linear, non-Gaussian case.


To equip both practitioners and researchers with computational tools, several packages have been developed for or can be adapted for causal discovery. The Java library TETRAD \citep{glymour1986causal, scheines1998tetrad, ramsey2018tetrad} contains a variety of well-tested causal discovery algorithms and has been continuously developed and maintained for over 40 years; R packages pcalg \citep{pcalg} and bnlearn \citep{bnlearn} also include some classical constraint-based and score-based methods such as PC and GES. However, these tools are based on Java or R, which may not align with the recent trend favoring Python in certain communities, particularly within machine learning. While there are Python wrappers available for these packages (e.g., py-tetrad \citep{pytetrad}/py-causal \citep{pycausal} for TETRAD, and Causal Discovery Toolbox \citep{kalainathan2020causal} for pcalg and bnlearn), they still rely on Java or R. This dependency can complicate deployment and does not cater directly to Python users seeking to develop their own methods based on an existing codebase. Thus, there is a pronounced need for a Python package that covers representative causal discovery algorithms across all primary categories. Such a tool would significantly benefit a diverse range of users by providing access to both classical methods and the latest advancements in causal discovery.

\looseness=-1
In this paper, we describe \textit{causal-learn}, an open-source python library for causal discovery. The library incorporates an extensive range of causal discovery algorithms, providing accessible APIs and thorough documentation to cater to a diversity of practical requirements and data assumptions. Moreover, it provides independent modules for specific functionalities, such as (conditional) independence tests, score functions, graph operations, and evaluation metrics, thereby facilitating custom needs and fostering the development of user-defined methods. An essential attribute of causal-learn is its full implementation in Python, eliminating dependencies on any other programming languages. As such, users are not required to have expertise in Java or R, enhancing the ease of integration within the enormous and growing Python ecosystem and promoting seamless utilization for a range of computational and scripting tasks. With causal-learn, modification and extensions based on the existing implementation of causal discovery methods also become plausible for developers and researchers who may not be familiar with Java or R, which could significantly accelerate the progress in related fields by lowering the threshold of the integration of causality into various pipelines.
