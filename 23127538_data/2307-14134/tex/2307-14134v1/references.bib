@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{abadji2022towards,
  title={Towards a cleaner document-oriented multilingual crawled corpus},
  author={Abadji, Julien and Suarez, Pedro Ortiz and Romary, Laurent and Sagot, Beno{\^\i}t},
  journal={arXiv preprint arXiv:2201.06642},
  year={2022}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@Online {musabgwikipediatr,
  title = {musabg/wikipedia-tr · Datasets at Hugging Face},
  date = {2023-07-24},
  year = {2023},
  file = {:./references/datasets-musabg-wikipedia-tr.html:html},
  url = {https://huggingface.co/datasets/musabg/wikipedia-tr},
  urldate = {2023-07-24}
}

@software{stefan_schweter_2020_3770924,
  author       = {Stefan Schweter},
  title        = {BERTurk - BERT models for Turkish},
  month        = apr,
  year         = 2020,
  publisher    = {Zenodo},
  version      = {1.0.0},
  doi          = {10.5281/zenodo.3770924},
  url          = {https://doi.org/10.5281/zenodo.3770924}
}

@Online {distilberturk,
  title = {dbmdz/distilbert-base-turkish-cased · Hugging Face},
  date = {2023-07-24},
  year = {2023},
  file = {:./references/dbmdz-distilbert-base-turkish-cased.html:html},
  url = {https://huggingface.co/dbmdz/distilbert-base-turkish-cased},
  urldate = {2023-07-24}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{jiao2019tinybert,
  title={Tinybert: Distilling bert for natural language understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal={arXiv preprint arXiv:1909.10351},
  year={2019}
}

@article{tsai2019small,
  title={Small and practical BERT models for sequence labeling},
  author={Tsai, Henry and Riesa, Jason and Johnson, Melvin and Arivazhagan, Naveen and Li, Xin and Archer, Amelia},
  journal={arXiv preprint arXiv:1909.00100},
  year={2019}
}

@inproceedings{ulvcar2020finest,
  title={FinEst BERT and CroSloEngual BERT: less is more in multilingual models},
  author={Ul{\v{c}}ar, Matej and Robnik-{\v{S}}ikonja, Marko},
  booktitle={Text, Speech, and Dialogue: 23rd International Conference, TSD 2020, Brno, Czech Republic, September 8--11, 2020, Proceedings 23},
  pages={104--111},
  year={2020},
  organization={Springer}
}

@article{ulvcar2019high,
  title={High quality ELMo embeddings for seven less-resourced languages},
  author={Ul{\v{c}}ar, Matej and Robnik-{\v{S}}ikonja, Marko},
  journal={arXiv preprint arXiv:1911.10049},
  year={2019}
}

@article{bilen2021lstm,
  title={LSTM network based sentiment analysis for customer reviews},
  author={B{\.I}LEN, Burhan and HORASAN, Fahrettin},
  journal={Politeknik Dergisi},
  volume={25},
  number={3},
  pages={959--966},
  year={2021},
  publisher={Gazi University}
}

@inproceedings{acikalin2020turkish,
  title={Turkish sentiment analysis using bert},
  author={Acikalin, Utku Umur and Bardak, Benan and Kutlu, Mucahid},
  booktitle={2020 28th Signal Processing and Communications Applications Conference (SIU)},
  pages={1--4},
  year={2020},
  organization={IEEE}
}

@inproceedings{ozberk2021offensive,
  title={Offensive language detection in turkish tweets with bert models},
  author={{\"O}zberk, Anil and {\c{C}}i{\c{c}}ekli, {\.I}lyas},
  booktitle={2021 6th International Conference on Computer Science and Engineering (UBMK)},
  pages={517--521},
  year={2021},
  organization={IEEE}
}

@inproceedings{ccelikten2021turkish,
  title={Turkish medical text classification using bert},
  author={{\c{C}}el{\i}kten, Azer and Bulut, Hasan},
  booktitle={2021 29th signal processing and communications applications conference (SIU)},
  pages={1--4},
  year={2021},
  organization={IEEE}
}

@inproceedings{ccarik2022twitter,
  title={A Twitter Corpus for named entity recognition in Turkish},
  author={{\c{C}}ar{\i}k, Buse and Yeniterzi, Reyyan},
  booktitle={Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  pages={4546--4551},
  year={2022}
}

@inproceedings{reimers-2019-sentence-bert,
  title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
  author = "Reimers, Nils and Gurevych, Iryna",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
  month = "11",
  year = "2019",
  publisher = "Association for Computational Linguistics",
  url = "https://arxiv.org/abs/1908.10084",
}

@Online {TurkishNewsDatasetKaggle,
  title = {Turkish News Dataset | Kaggle},
  date = {2023-07-24},
  year = {2023},
  file = {:./references/datasets-mehmetcalikus-turkish-news-data.html:html},
  url = {https://www.kaggle.com/datasets/mehmetcalikus/turkish-news-data},
  urldate = {2023-07-24}
}

@article{ccelik2023unified,
  title={Unified benchmark for zero-shot Turkish text classification},
  author={{\c{C}}elik, Emrecan and Dalyan, Tu{\u{g}}ba},
  journal={Information Processing \& Management},
  volume={60},
  number={3},
  pages={103298},
  year={2023},
  publisher={Elsevier}
}