\begin{thebibliography}{10}

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{abadji2022towards}
Julien Abadji, Pedro~Ortiz Suarez, Laurent Romary, and Beno{\^\i}t Sagot.
\newblock Towards a cleaner document-oriented multilingual crawled corpus.
\newblock {\em arXiv preprint arXiv:2201.06642}, 2022.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em The Journal of Machine Learning Research}, 21(1):5485--5551,
  2020.

\bibitem{musabgwikipediatr}
musabg/wikipedia-tr · datasets at hugging face, 2023.

\bibitem{stefan_schweter_2020_3770924}
Stefan Schweter.
\newblock Berturk - bert models for turkish, April 2020.

\bibitem{distilberturk}
dbmdz/distilbert-base-turkish-cased · hugging face, 2023.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{jiao2019tinybert}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
  Wang, and Qun Liu.
\newblock Tinybert: Distilling bert for natural language understanding.
\newblock {\em arXiv preprint arXiv:1909.10351}, 2019.

\bibitem{tsai2019small}
Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin Li, and Amelia
  Archer.
\newblock Small and practical bert models for sequence labeling.
\newblock {\em arXiv preprint arXiv:1909.00100}, 2019.

\bibitem{ulvcar2019high}
Matej Ul{\v{c}}ar and Marko Robnik-{\v{S}}ikonja.
\newblock High quality elmo embeddings for seven less-resourced languages.
\newblock {\em arXiv preprint arXiv:1911.10049}, 2019.

\bibitem{ulvcar2020finest}
Matej Ul{\v{c}}ar and Marko Robnik-{\v{S}}ikonja.
\newblock Finest bert and crosloengual bert: less is more in multilingual
  models.
\newblock In {\em Text, Speech, and Dialogue: 23rd International Conference,
  TSD 2020, Brno, Czech Republic, September 8--11, 2020, Proceedings 23}, pages
  104--111. Springer, 2020.

\bibitem{acikalin2020turkish}
Utku~Umur Acikalin, Benan Bardak, and Mucahid Kutlu.
\newblock Turkish sentiment analysis using bert.
\newblock In {\em 2020 28th Signal Processing and Communications Applications
  Conference (SIU)}, pages 1--4. IEEE, 2020.

\bibitem{ozberk2021offensive}
Anil {\"O}zberk and {\.I}lyas {\c{C}}i{\c{c}}ekli.
\newblock Offensive language detection in turkish tweets with bert models.
\newblock In {\em 2021 6th International Conference on Computer Science and
  Engineering (UBMK)}, pages 517--521. IEEE, 2021.

\bibitem{ccelikten2021turkish}
Azer {\c{C}}el{\i}kten and Hasan Bulut.
\newblock Turkish medical text classification using bert.
\newblock In {\em 2021 29th signal processing and communications applications
  conference (SIU)}, pages 1--4. IEEE, 2021.

\bibitem{ccarik2022twitter}
Buse {\c{C}}ar{\i}k and Reyyan Yeniterzi.
\newblock A twitter corpus for named entity recognition in turkish.
\newblock In {\em Proceedings of the Thirteenth Language Resources and
  Evaluation Conference}, pages 4546--4551, 2022.

\bibitem{ccelik2023unified}
Emrecan {\c{C}}elik and Tu{\u{g}}ba Dalyan.
\newblock Unified benchmark for zero-shot turkish text classification.
\newblock {\em Information Processing \& Management}, 60(3):103298, 2023.

\bibitem{reimers-2019-sentence-bert}
Nils Reimers and Iryna Gurevych.
\newblock Sentence-bert: Sentence embeddings using siamese bert-networks.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing}. Association for Computational Linguistics, 11
  2019.

\bibitem{TurkishNewsDatasetKaggle}
Turkish news dataset | kaggle, 2023.

\end{thebibliography}
