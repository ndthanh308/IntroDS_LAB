We consider a noisy nonlinear repetitive process of the form
\begin{equation}
	y = f(u) + w,
 \label{eq:repetitive-process}
\end{equation}
where $u\in\Reals^n$ is the input,
$y\in\Reals^m$ is the output,
$f:\Reals^n\rightarrow\Reals^m$ is the system response (input/output map), and
$w\in\Reals^m$ is a non-repeating disturbance, assumed to be zero-mean. %
We focus on the response of a dynamical system over a finite interval where $u$ and $y$ define input/output trajectories of the underlying nonlinear system.
Therefore, we have $u=\left(u(1), u(2), \ldots,u(N)\right)$ for an input trajectory of $N$ time steps, and similarly for $y$.
For example, the input $u$ could be a trajectory of actuator commands or a reference trajectory tracked by a low-level feedback controller, and the output $y$ is the actual trajectory traced by the system. In Section \ref{sec:case-study} we show a case study with this configuration.
The input and output must satisfy the constraints $u \in \mathcal{U} \subset \Reals^n$, $y \in \mathcal{Y} \subset \Reals^m$. 
For example, in a motion tracking problem, the sets $\mathcal{U}$ and $\mathcal{Y}$ may encode limits on actuation velocity, and acceleration. 

Our control objective is to choose the input $u$ such that the output $y$ tracks a target trajectory $\Target$ as closely as possible. The ILC approach designs a learning policy $\pi = (x, \mathcal{T}, q)$ of the form 
\begin{subequations}
	\begin{align}
		x_{k+1} &=\mathcal{T}(x_k,y_k),\\
		u_k   &= q(x_k) .
	\end{align}
\end{subequations}
where $x$ is the internal state of the policy,
$\mathcal{T}$ is the update function,
and $q$ is an output function that recovers the control input from $x$.
The goal is to design $x$, $\mathcal{T}$, and $q$ such that the (iteration domain) closed-loop system
\begin{subequations}
\label{eq:ideal_policy}
	\begin{align}
		u_k &= q(x_k), \\
		y_k &= f(u_k) + w_k, \\
		x_{k+1} &= \mathcal{T}(x_k, y_k),
	\end{align}
\end{subequations}
converges to some $y_k$ close to $\Target$, with  $u \in \mathcal{U}$ and $y \in \mathcal{Y}$. Due to the dynamics and constraints $y_k$ will, in general, not reach $\Target$ exactly.
Subscript $k$ indicates the iteration index of the ILC throughout the rest of the chapter.

% Figure environment removed

Here we propose to design the policy $(x, \mathcal{T}, q)$ using SQP. We assume access to a model that is used to derive gradient and hessian information.
In subsequent sections we provide the details of the individual components in Fig.~\ref{fig:scheme}.
We initialize with a feasible input and take an SQP step after each experiment to evaluate the ILC policy~\eqref{eq:ideal_policy} using the approximate model of the system, measurements, and past inputs.
The objective of the approach is to minimize the output tracking error with respect to a target trajectory, illustrated in Fig.~\ref{fig:cartoon-simple}.



