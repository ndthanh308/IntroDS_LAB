\subsection{Numerical Results}

% Figure environment removed


% Figure environment removed

% Figure environment removed
% Figure environment removed


% Figure environment removed

% Figure environment removed

\if11
% Figure environment removed
\fi

We present results using LM and NL1 for evaluating the approximations in \eqref{eq:approximations} and discuss the effect of 
the initial input $u_0$ and the decay rate of the form $\eta_k = \eta_0 k^{-c}$ on ILC performance.

\subsubsection{Effect of the model}
The results presented in Fig. \ref{fig:error-vs-iteration-init=xi} and Fig. \ref{fig:error-vs-iteration} show that for the iterations taken using gradient information from both the LM and the NL1, the error converges to a value of the same order of magnitude.
However, we see LM converge to a solution with lower deviation and at a faster rate compared to NL1.
This result is at first surprising given that the prediction error of the LM is one order of magnitude higher than NL1. We note however that NL1 is built with LeakyReLu activation functions, and thus its gradient is piecewise constant and discontinuous. Despite its higher prediction error, the structure of the LM is found to provide more accurate gradient information.
Different shapes and tunings of the cost function show mostly similar trends of the ILC loop using the linear and nonlinear models (data not shown).
We have observed that the system is only mildly nonlinear, and since the ILC step relies on measurements that do not depend on the models used, the fidelity of the approximations~\eqref{eq:approximations} is apparently not of critical importance. 
We hypothesize that a system with more pronounced nonlinearities would experience faster convergence with ILC steps relying on the nonlinear model for the approximations.
In Fig. \ref{fig:deviation-linearmodel} and Fig. \ref{fig:deviation-nonlinearmodel}, we show the output deviation as a function of time before and after $20$ steps of the proposed method. The deviations are computed between the output and the target trajectory depicted in Fig. \ref{fig:xy-detail-linearmodel}, where the output trajectories are plotted in $x-y$ coordinates.
Empirically, in previous studies, we found that the best rms error that can be obtained in this setup is close to the steady state values to which the ILC converges.
\subsubsection{Effect of initialization}
In Fig. \ref{fig:error-vs-iteration}, we show the results for two different initial conditions.
The results illustrate that since the underlying problem is nonlinear it is possible to be in a local minimum and not achieve a better solution, depending on the initial conditions.



\subsubsection{Effect of step size}


Next, we study the effect of step size on convergence behavior. We take $\eta_k = \eta_0 k^{-c}$ and compare the convergence.
We show the error trajectories for $c = \{0.2, 0.5, 0.9\}$ (Fig. \ref{fig:normalised-vs-iteration}).
In Fig. \ref{fig:rms-vs-c} we plot the error after $10$ and $50$ iterations for different values of $c$.
We observe that for $c$ between $0.3$ and $0.6$, we obtain fast decreases in the error without compromising the value at a steady state.


\if 01
\eb{
\begin{itemize}
    \item Comparison between different models starting from the same initial condition. 
    \item Compare to linear ILC and use the linear model for the Hessian etc. to show what we gain in simulation results
    \item Use the best-performing model and maybe do a plot on the effect of the initial condition to show the sensitivity to the initial condition, which should be part of the limitations of the method. Could make sense to compare against the linear ILC maybe? 
    \item (A side note that we could add only if we have time/space) If we have the 500ms NN as both model and system, we can show that the method (hopefully) almost converges to the optimization of the 500ms NN directly. This would be an additional motivation to use this framework for the optimization of such NN models instead of doing expensive optimization directly. 
\end{itemize}
}
\fi

