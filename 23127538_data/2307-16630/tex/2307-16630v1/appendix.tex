% \section*{Appendix}
% https://www.derivative-calculator.net/#
\appendices

% \BW{Do we need to restate the Definition 2 and Proposition 1?} \xy{I have deleted Definition 2. Proposition 1 needs to be revised.}

% \section{Certification for Combination of Input Space} \label{appendix:staircase}
% \noindent\textbf{Definition 2} (Staircase Mechanism). (restated~\cite{geng2015staircase}) Given $\gamma \in[0,1]$ and sensitivity $\epsilon>0$, we define the Staircase distribution with the probability density function $f_\gamma^\epsilon(\cdot)$ as
% \begin{equation}
%     f_\gamma^\epsilon(x\ |\ \mu,\Delta)=\left\{
%     \begin{aligned} 
%     &\exp({-k\epsilon})a(\gamma), \\  
%     &\qquad \|x-\mu\|_1\in[k\Delta,(k+\gamma)\Delta] \\ 
%     &\exp({-(k+1)\epsilon})a(\gamma), \\  
%     &\qquad \|x-\mu\|_1\in[(k+\gamma)\Delta,(k+1)\Delta] 
%     \end{aligned} \right.
% \label{eq:f_gamma_ori}
% \nonumber
% \end{equation}
% %
% where the normalization factor $a(\gamma)$ is utilized to ensure that $\int_{\BR} f_\gamma^\epsilon(x)\rd x=1$. Specifically, we define $b\triangleq \exp({-\epsilon})$, and $c_k \triangleq \sum_{i=0}^{\infty} i^k b^i, \forall k\in \BZ$. 
% Then the closed-form expression for $a(\gamma)$ is
% $$a(\gamma)\triangleq \dfrac{d!}{2^d\gamma^d \sum_{k=1}^d [\tbinom{d}{k}c_{d-k}(b+(1-b)\gamma^k)]}$$




\section{Proofs}

\vspace{-3mm}
\subsection{Proof for Theorem~\ref{thm:wss_1}}  \label{proof:wss_1}
\vspace{-3mm}
% We first state below the Neyman-Pearson Lemma~\cite{neyman1933ix}.
% \begin{lemma}[Neyman-Pearson] %  3 in \cite{cohen2019certified} 
% \label{lem:np}
% Let $W$ and $V$ be random variables in $\BR^{n}$ with densities $\mu_V$ and $\mu_W$. Let $h: \BR^{n} \to \{0,1\}$ be a random or deterministic function. Then:
% \begin{enumerate}[1)]
%     \item If $Q\!=\!\{z\in\BR^{n}\!:\! \frac{\mu_V}{\mu_W} \leq t\}$ for some $t>0$ and $\BP(h(X)=1) \geq \BP(X\in S)$ then $\BP(h(Y)=1) \geq \BP(Y\in S)$.
%     \item  If $Q\!=\!\{z\in\BR^{n}\!:\! \frac{\mu_V}{\mu_W} \geq t\}$ for some $t>0$ and $\BP(h(X)=1) \leq \BP(X\in S)$ then $\BP(h(Y)=1) \leq \BP(Y\in S)$.
% \end{enumerate}
% \end{lemma}



We first introduce the special case of Neyman-Pearson Lemma under isotropic Staircase Mechanisms.

\begin{lemma}[Neyman-Pearson for Staircase Mechanism under Different Means]
\label{lem:np_staircase}
Let $W\sim \CS_\gamma^\epsilon(\tau, \Delta)$ and $V\sim \CS_\gamma^\epsilon(\tau+\delta, \Delta)$ be two random synonym indexes. Let $h: \BR^{n} \to \{0,1\}$ be any deterministic or random function. Then:
%
\begin{enumerate}[label=\arabic*)]
    \item If $Q=\{z\in\BR^{n}: \|z\|_1-\|z-\delta\|_1 \leq \beta\}$ for some $\beta$ and $\BP(h(W)=1) \geq \BP(W\in Q)$ then $\BP(h(V)=1) \geq \BP(V\in Q)$
    \item If $Q=\{z\in\BR^{n}: \|z\|_1-\|z-\delta\|_1 \geq \beta\}$ for some $\beta$ and $\BP(h(W)=1) \leq \BP(W\in Q)$ then $\BP(h(V)=1) \leq \BP(V\in Q)$
\end{enumerate}
\end{lemma}

\begin{proof}
In order to prove
\begin{align}
\small
\begin{aligned}
    &\{z: \|z\|_1-\|z-\delta\|_1 \leq \beta\} \Longleftrightarrow \{z: \frac{\mu_V}{\mu_W} \leq t\} \textrm{\ and\ } \\
    &\{z: \|z\|_1-\|z-\delta\|_1 \geq \beta\} \Longleftrightarrow \{z: \frac{\mu_V}{\mu_W} \geq t\},
\end{aligned}
\end{align}
%
we need to show that for any $\beta$, there is some $t>0$, and for any $t>0$, there is also some $\beta$. 

When $W$ and $V$ are under the isotropic Staircase probability distributions, the likelihood ratio turns out to be
\begin{small}
\begin{align}
% \begin{aligned}
    \label{eq:stair_ratio_1} \frac{\mu_V}{\mu_W}=&\ \frac{\exp({-l_\Delta(z_0\ |\  \tau+\delta)\epsilon})a(\gamma)}{\exp({-l_\Delta(z_0\ |\  \tau)\epsilon})a(\gamma)} \\
    \nonumber =&\ \exp([\lfloor\frac{\|z_0-\tau\|_1}{\Delta}+(1-\gamma)\rfloor \\
    \label{eq:stair_ratio_2} &\ \qquad\qquad  -\lfloor\frac{\|z_0-(\tau+\delta)\|_1}{\Delta}+(1-\gamma)\rfloor]\epsilon) \\
    \label{eq:stair_ratio_3} =&\ \exp(\frac{\epsilon}{\Delta}[\|z\|_1-\|z-\delta\|_1]),
% \end{aligned}
\end{align}
\end{small}
%
where $z=z_0-\tau$. We assume that the perturbation ($\delta$) and the noise ($z$) are discrete, which means $\delta = k_1\cdot\Delta, k_1\in\BZ$ and $z=k_2\cdot\Delta, k_2\in \BZ$. Then we can derive Eq.(\ref{eq:stair_ratio_3}). %\BW{Which one?}
Therefore, given any $\beta$, we can choose $t=\exp(\frac{\epsilon \beta}{\Delta})$, and derive that $\frac{\mu_V}{\mu_W}\leq t$. Similarly, given any $t>0$, we can choose $\beta=\frac{\Delta}{\epsilon} \log t$, and derive $\|z\|_1-\|z-\delta\|_1\leq \beta$. Note that we clip case where $\mu_W= 0$.
\end{proof}


% \noindent\textbf{Theorem~\ref{thm:wss_1}.} (restated) \emph{Let $\phi_S: \CW \times \BR^n \to \CW$ be the embedding substituting transformation based on Staircase noise $\varepsilon \sim \CS_\gamma^\epsilon(w, \Delta)$ and let $g_S$ be the smoothed classifier from any deterministic or random function $h$, as in Eq.(\ref{eq:smoothg1}). Suppose $y_A, y_B\in \CY$ and $\underline{p_A}, \overline{p_B} \in [0,1]$ satisfy:}
% %
% \begin{equation}
% \begin{aligned}
%     \BP(h(u\cdot \phi_S(w,\varepsilon))=y_A) \geq \underline{p_A} &\geq \overline{p_B} \geq\\
%     \max_{y_B\neq y_A}& \BP(h(u\cdot \phi_S(w,\varepsilon))=y_B) 
% \nonumber
% \end{aligned}
% \end{equation}
% \emph{Then $g_S(u\cdot \phi_S(w, \delta_{S}\cdot \Delta))=y_A$ for all $\|\delta_{S}\|_1 \leq \rad_S$, where }
% %
% \begin{equation} \label{eq:rad_s}
% \rad_S = \max \{\frac{1}{2\epsilon}\log({\underline{p_A}}/{\overline{p_B}}), -\frac{1}{\epsilon}\log(1-\underline{p_A}+\overline{p_B})\} %
% \end{equation}

% \BW{We first show three Claims below, where are used to prove Theorem 1. Put claims here?}  
Next, we prove the Theorem \ref{thm:wss_1}.

\begin{proof}
%\BW{Define the random variable $W$ and $V$. Same as those in Lemma 2?} \xy{I have revised Lemma and Theorem in this section. ($w\to \tau$, synonym indexes)}
Denote $\delta_{{S}}=\{a_1, \cdots, a_n\}$. 
Let $\tau$ be the synonym indexes of the input $w$
%, i.e., an all-zero vector of length $n$, and $\delta_{{S}}=\{a_1, \cdots, a_n\}$. Let 
and $W\sim \CS_\gamma^\epsilon(\tau, \Delta)$ and $V\sim \CS_\gamma^\epsilon(\tau+\delta_{{S}}, \Delta)$ be random synonym indexes, as defined by Lemma~\ref{lem:np_staircase}.
The assumption is
\begin{small}
\begin{equation}
    \BP(h(W) = y_A) \geq \underline{p_A} \geq \overline{p_B} \geq \BP(h(W) = y_B)
\nonumber
\end{equation}
\end{small}
 %\BP(X \in A) > \BP(X \in B)

By the definition of $g$, we need to show that
\begin{small}
\begin{equation}
    \BP(h(V) = y_A) \geq \BP(h(V) = y_B)
\nonumber
\end{equation} %\BP(Y \in A) > \BP(Y \in B)
\end{small}

Denote $T(z) =\|z\|_1-\|z-\delta\|_1$ and use Triangle Inequality we can derive a bound for $T(x)$: 
\begin{small}
\begin{equation}
    -\|\delta\|_1 \leq T(z) \leq \|\delta\|_1
\end{equation}
\end{small}

We pick $\beta_1, \beta_2$ such that there exist the following $A, B$
\begin{equation}
\small
\begin{aligned}
    A:&=\{z: T(z)= \|z\|_1-\|z-\delta\|_1 \leq \beta_1\} \\
    B:&=\{z: T(z)= \|z\|_1-\|z-\delta\|_1 \geq \beta_2\}
\nonumber
\end{aligned}
\end{equation}
that satisfy conditions $\BP(W \in A) = \underline{p_A}$ and $\BP(W \in B) = \overline{p_B}$.
%
According to the assumption, we have
\begin{equation}
\small
\begin{aligned}
\BP(W \in A) = \underline{p_A} \leq p_A = \BP(h(W) = y_A) \\
\BP(W \in B) = \overline{p_B} \geq p_B = \BP(h(W) = y_B)
\nonumber
\end{aligned}
\end{equation}

Thus, by applying Lemma~\ref{lem:np_staircase}, we have
\begin{equation}
\small
\begin{aligned}
\BP(h(V) \!=\! y_A) \geq \BP(V \!\in\! A) \textrm{\ and\ }
\BP(h(V) \!=\! y_B) \leq \BP(V \!\in\! B).
\nonumber
\end{aligned}
\end{equation}


Based on our {\bf Claims} shown later, we have 
% Then we compute the following: \BW{Based on the PDF of staircase mechanism?}
\begin{small}
\begin{align}
\label{eq:YinA1}    \BP(V \in A) &\geq \exp(-\frac{\|\delta\|_1}{\Delta}\epsilon) \underline{p_A} \quad \textrm{and} \\
%
\label{eq:YinA2}    \BP(V \in A) &\geq 1-\exp(\frac{\|\delta\|_1}{\Delta}\epsilon)(1-\underline{p_A}) \\
%
\label{eq:YinB} \BP(V \in B) &\leq \exp(\frac{\|\delta\|_1}{\Delta}\epsilon)\overline{p_B}
\end{align}
\end{small}

In order to obtain $\BP(V \in A) > \BP(V \in B)$, from Eq.(\ref{eq:YinA1}) and Eq.(\ref{eq:YinB}), we need 
$\rad_S=\frac{\|\delta\|_1}{\Delta} \leq \frac{1}{2\epsilon} \log(\underline{p_A}/\overline{p_B}).$
Similarly, from Eq.(\ref{eq:YinA2}) and Eq.(\ref{eq:YinB}), we need 
$\rad_S=\frac{\|\delta\|_1}{\Delta} \leq -\frac{1}{\epsilon}\log(1-\underline{p_A}+\overline{p_B}).$
%
\end{proof}

\begin{claim}
\label{claim1}
\begin{small}
$\BP(V \in A) \geq \exp(-\frac{\|\delta\|_1}{\Delta}\epsilon) \underline{p_A}$
\end{small}
%
\begin{proof}
Recall that $\int_A \exp(-\frac{\|z\|_1}{\Delta}\epsilon)a(\gamma) \rd z=\underline{p_A}$.
\begin{small}
\begin{equation}
\begin{aligned}
    \BP(V \in A)
    =& %\int_A \exp({-\frac{\|z-\delta\|_1}{\Delta}\epsilon})a(\gamma) \rd z \\
    \int_A [\exp({-\frac{\|z\|_1}{\Delta}\epsilon}) \exp({\frac{T(z)}{\Delta}\epsilon})]a(\gamma) \rd z \\
    \geq& \exp(-\frac{\|\delta\|_1}{\Delta}\epsilon) \int_A \exp(-\frac{\|z\|_1}{\Delta}\epsilon)a(\gamma) \rd z \\
    =& \exp(-\frac{\|\delta\|_1}{\Delta} \epsilon) \underline{p_A}
\nonumber
\end{aligned}
\end{equation}
\end{small}
%
\vspace{-3mm}
\end{proof}
\end{claim}


\begin{claim}
\label{claim2}
\begin{small}
    $\BP(V \in A) \geq 1-\exp(\frac{\|\delta\|_1}{\Delta}\epsilon)(1-\underline{p_A})$
\end{small}
%
\begin{proof}
\begin{small}
\begin{equation}
\begin{aligned}
    \BP(V \in A)
    %=& \int_A \exp({-\frac{\|z-\delta\|_1}{\Delta}\epsilon})a(\gamma) \rd z \\
%    =& \int_A [\exp({-\frac{\|z\|_1}{\Delta}\epsilon}) \exp({\frac{T(z)}{\Delta}\epsilon})]a(\gamma) \rd z \\
    =& 1-\int_{\CW\backslash A} [\exp({-\frac{\|z\|_1}{\Delta}\epsilon}) \exp({\frac{T(z)}{\Delta}\epsilon})]a(\gamma) \rd z \\
    \geq& 1-\exp(\frac{\|\delta\|_1}{\Delta}\epsilon) \int_{\CW\backslash A} \exp(-\frac{\|z\|_1}{\Delta}\epsilon)a(\gamma) \rd z \\
    =& 1-\exp(\frac{\|\delta\|_1}{\Delta}\epsilon)(1-\underline{p_A})
\nonumber
\end{aligned}
\end{equation}
\end{small}
\vspace{-3mm}
\end{proof}
\end{claim}


\begin{claim}
\label{claim3}
\begin{small}
    $\BP(V \in B) \leq \exp(\frac{\|\delta\|_1}{\Delta}\epsilon )\overline{p_B}$
\end{small}
%
\begin{proof}
Recall that $\int_B \exp(-\frac{\|z\|_1}{\Delta}\epsilon)a(\gamma) \rd z = \overline{p_B}$.
\begin{small}
\begin{equation}
\begin{aligned}
    \BP(V \in B)
    %=& \int_B \exp({-\frac{\|z-\delta\|_1}{\Delta}\epsilon})a(\gamma) \rd z \\
    =& \int_B [\exp({-\frac{\|z\|_1}{\Delta}\epsilon}) \exp({\frac{T(z)}{\Delta}\epsilon})]a(\gamma) \rd z \\
    \leq \exp(\frac{\|\delta\|_1}{\Delta}\epsilon) &\int_B \exp(-\frac{\|z\|_1}{\Delta}\epsilon)a(\gamma) \rd z 
    =\exp(\frac{\|\delta\|_1}{\Delta} \epsilon) \overline{p_B}
\nonumber
\end{aligned}
\end{equation}
\end{small}
\vspace{-3mm}
\end{proof}
\end{claim}

% ===================================
% According to the formula for the summation of a geometric progression, we have 
% $$ F(z)=\int_{-d\Delta}^{z} f(u) \rd u=\left\{
% \begin{aligned}
% &\frac{1}{2} \frac{1-\exp(({\frac{z}{\Delta}+d)\epsilon})}{1-\exp({d\epsilon})}, \ & z\leq 0 \\
% &1-\frac{1}{2} \frac{1-\exp(({-\frac{z}{\Delta}+d)\epsilon})}{1-\exp({d\epsilon})}, \ & z>0
% \end{aligned}
% \right.
% $$ 
% where $z=z_0-w=k_2\Delta,\ k_2\in \BZ$.
% ===================================


\vspace{-3mm}
\subsection{Proof for Theorem~\ref{thm:wcr_1}} \label{proof:wcr_1}
\vspace{-3mm}
% \BW{This is the Lipschitz definition. No need to claim a proposition and no need to prove it. Making the definition as concise as possible} \xy{I searched the Lipschitz definition anew and quoted Lemma3.}

We first invoke the lemma 
% invoke Lemma 2.6 in~\cite{shalev2012online} to 
that relates the Lipschitz constant $L$ and the norm of subgradients of $g$. 
% https://homes.cs.washington.edu/~marcotcr/blog/lipschitz/
% Theorem 1 in https://www.tandfonline.com/doi/pdf/10.1080/13928619.2006.9637758

\begin{lemma}[\cite{shalev2012online}] \label{lem:l_lip_norm}
Given a norm $\|\cdot\|$ and consider a differentiable function $g: \BR^n\to \BR$.  
If $\textrm{sup}_x\|\nabla g(x)\|_* \leq L, \forall x\in \BR^n$, where $\|\cdot\|_*$ is the dual norm of $\|\cdot\|$, then $g$ is $L$-Lipschitz over $\BR^n$ with respect to $\|\cdot\|$, that is $|g(w) - g(v)| \leq L\|w - v\|$.
\end{lemma}

% \begin{proposition}
% Consider a differentiable function $g: \BR^n\to \BR$. If $\textrm{sup}_w\|\nabla g(w)\|_* \leq L$ where $\|\cdot\|_*$ has a dual norm $\|z\|=\max_w z^\top x \text{ s.t.\ } \|w\|_*\leq 1$,
% then $g$ is L-Lipschitz under norm $\|\cdot\|_*$, that is $|g(w) - g(v)| \leq L\|w - v\|$.
% \end{proposition}

% \begin{proof}
% Consider some $w,v\in \BR^n$ and a parameterization in $t$ as $\gamma(t)=(1-t)w+tv, \forall t\in[0,1]$. Note that $\gamma(0)=w$ and $\gamma(1)=v$. By the fundamental Theorem of calculus, we have:
% %
% \begin{equation}
% \begin{aligned}
%     |g(v)-g(w)|=&|g(\gamma(1))-g(\gamma(0))| 
%         =\bigg\vert\int_0^1 \frac{\rd g(\gamma(t))}{\rd t} \rd t \bigg\vert \\
%     =&\bigg\vert\int_0^1 \nabla g^\top \nabla \gamma \rd t \bigg\vert
%         \leq \int_0^1 \big\vert\nabla g^\top \nabla \gamma \big\vert \rd t \\
%     \leq& \int_0^1 \|\nabla g(x)\|_* \|\nabla \gamma(t)\|\rd t \leq L\|v-w\|
% \nonumber
% \end{aligned}
% \end{equation}
% %
% \end{proof}



Following Lemma~\ref{lem:l_lip_norm}, we show that the smoothed classifier $g(u\cdot w)$ is $L$-Lipschitz in $u$ as it satisfies $\textrm{sup}_u\|\nabla g(u\cdot w)\|_\infty \leq L$, where each $u_i$ in $u$ is a variable that follows uniform distribution and $w$ is a constant matrix. 

\begin{proposition} \label{porp:g_l_lip}
Given a uniform distribution noise $\rho\sim \FU[-\lambda, \lambda]$, the smoothed classifier $g(u,w)=\BP[h(\theta_I(u,\rho) \cdot w)]$ is $1/2\lambda$-Lipschitz in $u$ under $\|\cdot\|$ norm. %as defined in (\ref{eq:smoothg2})
\end{proposition}

\begin{proof}
It suffices to show that $\|\nabla_u g(u\cdot w)\|_\infty\leq 1/2\lambda$ to complete the proof. Here, $\theta_I(u,\rho)=u+\rho$ denotes applying uniform noise $\rho$ on $u$, i.e., randomly shuffling each row vector $u_i$ in $u$. Without loss of generality, we analyze $\partial g/\partial u_1$. Since $w$ is a fixed embedding matrix and does not affect the proof, we ignore $w$ in the following proof.
Let $u=[u_1, \hat{u}]$, where $\hat{u}=[u_2, \cdots, u_n]$, and $\rho=[\rho_1, \hat{\rho}]$, then: %\in \BR^{n\times (n-1)}
%\BW{$\hat{u} \in \mathbb{R}^{(n-1)\times d}$ or $\in \mathbb{R}^{n\times (n-1)} $} \xy{$\hat{u} \in(n-1)\times n$, because $u_i\in \BR^{1\times n}$ is a row vector}
%
% \BW{
% %I am kinda confused about the proof here. $u$ is a one-hot vector, while $\rho$ is a continuous noise. Then 
% What does $u+\rho$ mean? Or just do not care?} \xy{Thank you, I have explain $\theta(u,\rho)$ above.}
\begin{equation}
\small
\begin{aligned}
    \frac{\partial g}{\partial u_1}
    = & \frac{1}{(2\lambda)^n} \frac{\partial}{\partial u_1} \int_\Lambda \int_{-\lambda}^\lambda h( u_1+\rho_1, \hat{u}+\hat{\rho})  \textrm{d}\rho_1 \textrm{d}^{(n-1)}\hat{\rho} \\
    = & \frac{1}{(2\lambda)^n} \int_\Lambda\frac{\partial}{\partial u_1} \int_{u_1-\lambda}^{u_1+ \lambda} h(q, \hat{u}+\hat{\rho}) \textrm{d}q \textrm{d}^{(n-1)}\hat{\rho} \\
    = & \frac{1}{(2\lambda)^n} \int_\Lambda\! \Big( h(u_1+\lambda, \hat{u}+ \hat{\rho}) -h(u_1-\lambda, \hat{u}+\hat{\rho})\Big) \textrm{d}^{(n-1)}\hat{\rho}
\nonumber
\end{aligned}
\end{equation}
%
% \begin{equation}
% \begin{aligned}
%     & \frac{\partial g}{\partial u_1} \\
%     = & \frac{1}{(2\lambda)^n} \frac{\partial}{\partial u_1} \int_\Lambda \int_{-\lambda}^\lambda h((u_1+\rho_1, \hat{u}+\hat{\rho})\cdot w) \textrm{d}\rho_1 \textrm{d}^{(d-1)}\hat{\rho} \\
%     = & \frac{1}{(2\lambda)^n} \int_\Lambda\frac{\partial}{\partial u_1} \int_{-\lambda u_1}^{\lambda u_1} \frac{1}{\rho_1} h((q,\hat{u}+\hat{\rho})\cdot w) \textrm{d}q \textrm{d}^{(d-1)}\hat{\rho} \\
%     = & \frac{1}{(2\lambda)^n} \int_\Lambda \big\vert h((u_1+\lambda, \hat{u}+\hat{\rho})\cdot w) \\
%     &\qquad \qquad \qquad -h((u_1-\lambda, \hat{\rho}+ \hat{u})\cdot w)\big\vert \textrm{d}^{(d-1)}\hat{\rho}
% \nonumber
% \end{aligned}
% \end{equation}
%
where $\Lambda={[-\lambda,\lambda]^{n-1}}$. The second step follows the change of variable $q= u_1+\rho_1 \in [u_1-\lambda, u_1+\lambda]$. 
The last step follows the Leibniz rule. Let $H=\int h(q) \textrm{d}q$, then we have  $\frac{\partial H}{\partial u_1}=\frac{\partial H}{\partial q} \cdot \frac{\partial q}{\partial u_1}=h(q)$. Thus,
%
% \BW{I know this part tries to use the proof in [30], but please carefully double-check it.} \xy{I have checked it.}
%
\begin{equation}
\small
\begin{aligned}
    \big\vert \frac{\partial g}{\partial u_1} \big\vert 
    \leq & \frac{1}{(2\lambda)^n} \int_\Lambda \Big\vert h( u_1+\lambda, \hat{u}+\hat{\rho}) -h(u_1-\lambda, \hat{u}+\hat{\rho})\Big\vert \textrm{d}^{(n-1)}\hat{\rho} \\
    \leq & \frac{1}{(2\lambda)^n}\cdot (2\lambda)^{n-1}=\frac{1}{2\lambda}
\nonumber
\end{aligned}
\end{equation}
%
% \begin{equation}
% \begin{aligned}
%     \bigg\vert \frac{\partial g}{\partial u_1} \bigg\vert 
%     & \leq \frac{1}{(2\lambda)^n} \int_\Lambda \big\vert h( (u_1+\lambda, \hat{u}+\hat{\rho})\cdot w) \\
%     & \qquad \qquad \quad -h((u_1-\lambda, \hat{u}+\hat{\rho})\cdot w)\big\vert \textrm{d}^{(d-1)}\hat{\rho} \\
%     & \leq \frac{1}{(2\lambda)^n}\cdot (2\lambda)^{n-1}=\frac{1}{2\lambda}
% \nonumber
% \end{aligned}
% \end{equation}

Similarly, $\lvert \partial g/\partial u_i \rvert\leq 1/{2\lambda}$ for $\forall i\in \{2,\cdots, n\}$.
%This results in having 
Hence $\| \nabla_u g(u \cdot w) \|_\infty = \max_i |\partial g/\partial u_i| \leq 1/{2\lambda}$. 
\end{proof}



% \BW{Is Theorem 6 from Salman et al. NIPS 2019?} \xy{It is from ~\cite{alfarra2022deformrs}.}

% \BW{Is $h:\BR^n\to \BR$ or $h:\BR^n\to \BR^C$? I suggest using a different notation other than $h$ since it indicates the classification model.} \xy{Thank you, I have changed $h^i$ to $f^i$.}

Next, we show that the Lipschitz function is certifiable. %invoke Theorem~2 in~\cite{alfarra2022deformrs} to 

\begin{theorem} \label{thm:l-lipschitz}
Given a classifier $h$, let the function $f^i:\BR^n\to \BR$, defined as $f^i(x)=\BP(h(x)=i)$, be L-Lipschitz continuous under the norm $\|\cdot\|, \forall i\in \mathcal{Y}=\{1, \cdots, C\}$. If $y_A=\argmax_i f^i(x)$, then, we have $\argmax_i f^i(x+\delta)=y_A$ for all $\delta$ satisfying:
\begin{small}
\begin{equation}
\|\delta\|\leq \frac{1}{2L}(f^{y_A}(x)-\max_i f^{i\neq y_A}(x)).
\label{eq:l_lip_func}
\end{equation}
\end{small}
%
\end{theorem}

\begin{proof}
Take $y_B=\argmax_i h^{i\neq y_A}(x)$. Hence:
\begin{equation}
\small
\begin{aligned}
    & \big|f^{y_A}\!(x \!+\! \delta) \!-\! f^{y_A}\!(x)\big| \!\leq\! L \|\delta\| \Longrightarrow f^{y_A}\!(x \!+\! \delta) \!\geq\! f^{y_A}\!(x) \!-\! L \|\delta\| \\
    & \big|f^{y_B}\!(x \!+\! \delta) \!-\! f^{y_B}\!(x)\big| \!\leq\! L \|\delta\| \Longrightarrow f^{y_B}\!(x \!+\! \delta) \!\leq\! f^{y_B}\!(x) \!-\! L \|\delta\|
\nonumber
\end{aligned}
\end{equation}
%
By subtracting the inequalities and re-arranging terms, we have that as long as $f^{y_A}(x) -L\|\delta\| > f^{y_B}(x) -L\|\delta\|$, i.e., the bound in Eq.(\ref{eq:l_lip_func}), then $f^{y_A}(x+\delta) > f^{y_B}(x+\delta)$.
% , completing the proof.
\end{proof}


We now prove our Theorem~\ref{thm:wcr_1} based on the above Proposition~\ref{porp:g_l_lip} and Theorem~\ref{thm:l-lipschitz}.

% \BW{Proving Theorem 2 separately.} \xy{I have re-prove Theorem 2 as follows, but it's not very informative. Should I restate Theorem 2 here?} \BW{I suggest restating it} \xy{Thank you, Prof. Wang. I have restated and proven it.}

% \noindent\textbf{Theorem~\ref{thm:wcr_1}.} (restated)
% \emph{Let $\theta_R\!: \CU\times \BZ^n \!\to \CU$ be a permutation based on a uniform noise $\rho\!\sim\!\FU[-\lambda, \lambda]$ 
% and $g_R$ be the smoothed classifier from a base classifier $h$. Suppose $g_R$ assigns a class $y_A$ for the input $u\cdot w$, and $\underline{p_A}, \overline{p_B}\in(0,1)$. If }
% \begin{equation}
% \begin{aligned}
% \BP(h(\theta_R(u, \rho)\cdot w)=y_A) \geq \underline{p_A} &\geq \overline{p_B} \geq \\
% \max_{y_B\neq y_A}& \BP(h(\theta_R(u, \rho)\cdot w)=y_B)),
% \nonumber
% \end{aligned}
% \end{equation}
% %
% \emph{then $g_R(\theta_R(u, \delta_{R})\cdot w)=y_A$ for all permutation perturbations satisfying $\|\delta_{R}\|_1 \leq \rad_R$, where}
% \begin{equation} 
%     \rad_R= \lambda(\underline{p_A}-\overline{p_B})
% \label{eq:rad_r}
% \end{equation}


\begin{proof}
According to Proposition~\ref{porp:g_l_lip}, the uniform-based smoothed classifier $g_R$ is $1/2\lambda$-Lipschitz in $u$ under $\|\cdot\|$ norm. Combining Theorem~\ref{thm:l-lipschitz} and substituting $L=1/2\lambda$ in Eq.(\ref{eq:l_lip_func}), we have $\argmax_i \BP(g_R(\theta_R(u,\delta_R)\cdot w)=i)=y_A$ for all $\delta_R$ satisfying $\|\delta_R\|_1 \leq \& \lambda( \BP(g_R(\theta_R(u,\rho)\cdot w)={y_A}) \\
    \& \qquad -\max_{i,i\neq y_A} \BP(g_R(\theta_R(u,\rho)\cdot w)={i}))$, which holds in the case of $\|\delta_R\|_1 \leq \lambda(\underline{p_A}-\overline{p_B})$.
\end{proof}


\vspace{-3mm}
\subsection{Proof for Theorem~\ref{thm:combination}}  \label{proof:combination}
\vspace{-3mm}
% \noindent\textbf{Theorem~\ref{thm:combination}.} (restated)
% \emph{If a smoothed classifier $g$ is certified robust to permutation perturbation $\delta_u$ as defined in Eq.(\ref{eq:u_delta_1}), and to embedding permutation $\delta_w$ as defined in Eq.(\ref{eq:w_delta_1}), then $g$ can provide certified robustness to the combination of perturbations $\delta_u$ and $\delta_w$ as defined in Eq.(\ref{eq:uw_delta_1}).} assuming $\theta(u, \rho)$ is uniformly distributed in the permutation space.
% % when $u'=\theta(u, \rho)$ is uniformly distributed across the whole permutation space.
% %
% \begin{align}
%     \label{eq:u_delta_1} 
%     & \forall \delta_u, \delta_w, 
%     g(\theta(u\!+\!\delta_u,\rho)\cdot\! \phi(w, \varepsilon))\! =\! g(\theta(u,\rho)\!\cdot\! \phi(w, \varepsilon)) \\
%     \label{eq:w_delta_1} 
%     & \qquad \, \, \, \& \, 
%     g(\theta(u,\rho)\!\cdot\! \phi(w\!+\!\delta_w, \varepsilon))\! =\! g(\theta(u,\rho)\!\cdot\! \phi(w, \varepsilon)) \\
%     \label{eq:uw_delta_1} 
%     & \implies 
%     g(\theta(u\!+\!\delta_u,\rho)\!\cdot\! \phi(w\!+\!\delta_w, \varepsilon))\! =\! g(\theta(u,\rho)\!\cdot\! \phi(w, \varepsilon))
% \end{align}


\begin{proof}

% Let $\xi=u+\delta_u$, from Eq.(\ref{eq:u_delta}), we have
% \begin{equation}
%     \label{eq:covert_1} g(\theta(\xi,\rho)\cdot \phi(w,\varepsilon)) = g(\theta(u,\rho)\cdot \phi(w, \varepsilon))
% \end{equation}

% Based on $\rho\sim\FU[-\lambda, \lambda]$ and $\rho\sim \FU[-\lambda, \lambda]$, we have
% \begin{equation}
%     \label{eq:covert_2} g(\theta(\xi,\rho)\cdot \phi(w,\varepsilon)) = g(\theta(\xi,\rho)\cdot \phi(w,\varepsilon))
% \end{equation}

Because $u$ is uniformly distributed over the whole permutation space, $\theta(u,\rho)$ also is constant at arbitrary $u$, i.e., $\theta(u,\rho)=\theta(\xi,\rho)$, where $\xi$ is an arbitrary permutation. Therefore, considering Eq. \ref{eq:w_delta}, we have:
%
% \begin{equation}
%     \label{eq:covert_3} g(\theta(\xi,\rho)\cdot \phi(w+\delta_w, \varepsilon)) = g(\theta(u,\rho)\cdot \phi(w, \varepsilon))
% \end{equation}
%
% Let $\xi=u+\delta_u$, then we have
\begin{small}
\begin{align*}
    \nonumber g(\theta(u,\rho)\cdot \phi(w, \varepsilon)) =&\ g(\theta(u,\rho)\cdot \phi(w+\delta_w, \varepsilon)) \\
    \label{eq:convert_4} =&\ g(\theta(\xi,\rho)\cdot \phi(w+\delta_w, \varepsilon)) \\
    =&\ g(\theta(u+\delta_u,\rho)\cdot \phi(w+\delta_w, \varepsilon)) 
\end{align*}
\end{small}
which is 
% Eq.(\ref{eq:convert_4}) is 
exactly the Eq.(\ref{eq:uw_delta}). 
%
\end{proof}


\vspace{-3mm}
\subsection{Proof for Theorem~\ref{thm:wd_1}} \label{proof:wd_1}
\vspace{-3mm}

% \BW{Overall, I am not clear on how to link the two perturbations on the permutation and embedding together and derive the certified robustness.} 

% \BW{Also, Theorem 3 is said to be a universal theorem and be proved by directly applying the RS in Cohen et al. I am  unclear about that.}
% \xy{I have provided Theorem~\ref{thm:combination} for the combination of two perturbations.}

% \BW{Can shorten the proof somehow?} \xy{I have shortened the proof. Which step do you think can be further shorten?}

% We use ``1'' at different positions to indicate different words in the text. We use ``0'' to represent the deleted word, i.e., the word is converted to a \texttt{<pad>} token. 
We associate a binary variable to indicate the state (i.e., existed or deleted) of each word embedding. Initially, we have an all-ones state vector $x_0=\{1,1, \cdots, 1\}$ for all word emebeddings, indicating the existence of all words.     
%The embedding state of a text with $n$ embeddings is $x_0=\{a_1, \cdots, a_n\}=\{1, \cdots, 1\}$. 
The new embedding state of randomly deleting $\delta$ embeddings is $x_\delta=\{1, \cdots, 1, 0, \cdots, 0\}$, where the last $\delta$ states are set to be 0. % as follows
% \[
%     x_0 = \overbrace{1, \cdots, 1, 1, \cdots, 1}^{n}, \quad
%     x_\delta = \overbrace{1, \cdots, 1, \underbrace{0, \cdots, 0}_{\delta}}^{n}. 
% \]
% Our Bernoulli-based embedding deletion mechanism assumes that the probability of an embedding being deleted follows the Bernoulli distribution, i.e., $\BP(a_i=1 \to a_i=0)=p$ and $\BP(a_i=1 \to a_i=1)=1-p$. The deleted embeddings cannot be restored, i.e., $\BP(a_i=0\to a_i=1)=0$. 
By applying Bernoulli-based embedding deletion mechanism on $x_0$ and $x_\delta$, we obtain $x_{z_0}$ and $x_{z_\delta}$, respectively, as follows
% \[
%     x_{z_0} \!=\! \overbrace{1, \cdots\!, 1, \underbrace{0, \cdots, 0}_{z_0\in[0, n]}}^{n}, \
%     x_{z_\delta} \!=\! \overbrace{1, \cdots\!, 1, \underbrace{0, \cdots, 0}_{z_\delta\in[0, n-\delta]}, \underbrace{0, \cdots, 0}_{\delta}}^{n}.
% \]
\begin{small}
 \begin{equation}
\label{eq:x_z_delta}
    x_{z_0} \!=\! \underbrace{1,\cdots\!,1}_{n-z_0}, \underbrace{0,\cdots\!, 0}_{z_0\in[0, n]}; \
    x_{z_\delta} \!=\! \underbrace{1, \cdots\!, 1}_{n-\delta-z_\delta}, \! \underbrace{0, \cdots\!, 0}_{z_\delta\in[0, n-\delta]}, \! \underbrace{0,\cdots\!, 0}_{\delta}.
\end{equation}   
\end{small}

where the maximum of $z_0$ and $z_\delta$ is $n$ and $n-\delta$. \emph{For the sake of brevity, we place all ``0'' at the end of the text. Actually, ``0'' can occur anywhere in texts $x_\delta, x_{z_0}$, and $x_{z_\delta}$.} 

% \emph{Then, we assume $z\in[0,n]$ for $x_{0}$ and $z\in[\delta, n]$ for $x_{\delta}$.} Here $z$ represents the noise level that can be added to $x_0$ (or $x_\delta$), i.e., $x_0$ (or $x_\delta$) can transform $z\in[0, n]$ (or $z\in[\delta,n]$) words into \texttt{<pad>}s and generate $x_z$. The $x_z$ can be represented as

% Let noise be added on $x_0$ satisfies $\FB(n,p)$ and added on $x_\delta$ satisfies $\FB(n-\delta, p)$, where $n$ and $n-\delta$ limits the minimum value of noise. 
Next, we state the special case of Neyman-Pearson Lemma under isotropic Bernoulli Distributions.

\begin{lemma}[Neyman-Pearson for Bernoulli under Different Number of $1$]
\label{lem:np_bernoulli}
Let $W\sim \FB(n,p)$ and $V\sim \FB(n-\delta,p)$ be two random variables in $\{0,1\}^{n}$, denoting that at most $n$ and $n-\delta$ embedding states $b_i=1$ can be transformed into $b_i=0$ with probability $p$, respectively. Let $h: \{0,1\}^{n} \to \{0,1\}$ be any deterministic or random function. Then:
%
\begin{enumerate}[label=\arabic*)]
    \item If $Q=\{z\in\BZ: \tbinom{z}{\delta} \leq \beta\}$ for some $\beta$ and $\BP(h(W)=1) \geq \BP(W\in Q)$ then $\BP(h(V)=1) \geq \BP(V\in Q)$
    \item If $Q=\{z\in\BZ: \tbinom{z}{\delta} \geq \beta\}$ for some $\beta$ and $\BP(h(W)=1) \leq \BP(W\in Q)$ then $\BP(h(V)=1) \leq \BP(V\in Q)$
\end{enumerate}
\end{lemma}


\begin{proof} 
In order to prove:
\begin{small}
\begin{align}
\nonumber &\{z:\tbinom{z}{\delta} \leq \beta\} \Longleftrightarrow \{z:\frac{\mu_V}{\mu_W}\leq t\} \textrm{\ and\ } \\
\nonumber &\{z:\tbinom{z}{\delta} \geq \beta\} \Longleftrightarrow \{z:\frac{\mu_V}{\mu_W}\geq t\},
\end{align}    
\end{small}
%
we need to show that for any $\beta$, there is some $t$, and for any $t$ there is also some $\beta$. 
When $W$ and $V$ are under isotropic Bernoulli distributions, the likelihood ratio turns out to be
\begin{small}
\begin{equation}
\begin{aligned}
    \frac{\mu_V(x_z)}{\mu_W(x_z)} %= \frac{\BP(V=x_z)}{\BP(W=x_z)}
    =&\ \frac{\frac{\tbinom{n-\delta}{z-\delta}\cdot p^{z-\delta}\cdot (1-p)^{n-z}}{1-\sum_{z=0}^{\delta-1}\tbinom{n-\delta}{z-\delta}\cdot p^{z-\delta}\cdot (1-p)^{n-z}}}{\tbinom{n}{z}\cdot p^z\cdot (1-p)^{n-z}} \\
    =&\ \frac{\tbinom{n-\delta}{z-\delta}}{\tbinom{n}{z}}\cdot \frac{1}{p^\delta\cdot \Gamma(\delta, p)} = \frac{\tbinom{z}{\delta}}{\tbinom{n}{\delta} \cdot p^\delta\cdot \Gamma(\delta, p)}
    % = \frac{n(n-1)\cdots(n-\delta+1)}{z(z-1)\cdots(z-\delta+1)}\cdot p^\delta
\nonumber
\end{aligned}
\end{equation}
\end{small}%
where $\delta$ is the perturbation added on $x_0$, $z$ denotes the number of ``1'' transformed into ``0'', and $\Gamma(\delta, p)=1-\sum_{z=0}^{\delta-1}\tbinom{n-\delta}{z-\delta}\cdot p^{z-\delta}\cdot (1-p)^{n-z}\in(0,1)$ is a $z$-independent function. Here we divide the numerator by $\Gamma(\delta, p)$ because $z$ in $\mu_V(x_{z})$ requires $z \in [\delta, n]$, which means the number of ``0'' in $x_{z_\delta}$ in Eq.(\ref{eq:x_z_delta}) is $\geq \delta$.
Then for any $t$, we have $\beta=t/[\tbinom{n}{\delta} \cdot p^\delta \cdot \Gamma(\delta, p)]$. And for any $\beta$, we have $t=\tbinom{n}{\delta} \cdot p^\delta\cdot\Gamma(\delta, p) \cdot \beta$.
% As assumed above, $z$ of $x_0$ satisfies $z\in[0, n]$ and $z$ of $x_\delta$ satisfies $z\in[\delta, n]$. 
\end{proof}


% \noindent\textbf{Theorem~\ref{thm:wd_1}.} (restated) \emph{Let $\phi_D:\CW\times \{0,1\}^n \to \CW$ be the embedding deletion transformation based on Bernoulli noise $\varepsilon \sim \FB(n,p)$ and $\theta_D$ be the perturbation as same as $\theta_R$ based on a uniform noise $\rho\sim \FU[-n,n]$. Let $g_D$ be the smoothed classifier from a base classifier $h$, as in (\ref{eq:smoothg}) and suppose $y_A,y_B\in \CY$ and $\underline{p_A}, \overline{p_B} \in [0,1]$ satisfy:}
% %
% \begin{equation}
% \begin{aligned}
% \BP(h(\theta_D(u,\rho)\cdot\phi_D(w,\varepsilon)))& =y_A) \geq \underline{p_A} \geq \overline{p_B} \geq \\
% \max_{y_B\neq y_A}& \BP(h(\theta_D(u,\rho)\cdot\phi_D(w,\varepsilon))=y_B))
% \nonumber
% \end{aligned}
% \end{equation}
% %
% \textit{Then $g_D(\theta_D(u,\delta_{\theta_D})\cdot \phi_D(w,\delta_{\phi_D}))=y_A$ for all $\|\delta_{\theta_D}\|_1 < \rad_R$ as in (\ref{eq:rad_r}) and $\|\delta_{\phi_D}\|_0 < \rad_D$, where} %$\rad_D$ is the solution to the following optimization problem:
% %
% \begin{equation}
% \begin{aligned}
%     & \rad_D= \argmax \delta, \\
%     & \textrm{\ s.t.\ } \tbinom{z_{\max}}{\delta} \leq \underline{p_A}/\overline{p_B}, \\
%     & \qquad z_{\max} =\argmax z, \textrm{\ s.t.\ } \tbinom{n}{z} p^z (1-p)^{(n-z)} \leq \overline{p_B}.
% \nonumber
% \end{aligned}
% \end{equation}
% \begin{equation}
% \begin{aligned}
%     &\rad_D=\arg\max \delta \textrm{\ s.t.\ } \tbinom{z}{\delta} \leq \underline{p_A}/\overline{p_B}, \\ 
%     % &\textrm{s.t.}\ \tbinom{n}{\delta} \leq \underline{p_A}/\overline{p_B}
% \end{aligned}
% \end{equation}
% and $z=\argmax z$ s.t. $\tbinom{n}{z} p^z (1-p)^{(n-z)} \leq \overline{p_B}$.

Next, we prove Theorem \ref{thm:wd_1}.

\begin{proof} 
The assumption is 
\begin{small}
 $$
\BP(h(W) = y_A) \geq \underline{p_A} \geq \overline{p_B} \geq \BP(h(W) = y_B)
$$ %\BP(X \in A) > \BP(X \in B)   
\end{small}

By the definition of $g$, we need to show that
\begin{small}
\begin{equation}
    \BP(h(V) = y_A) \geq \BP(h(V) = y_B)
\nonumber
\end{equation} %\BP(Y \in A) > \BP(Y \in B)    
\end{small}


Denote $C(z)\!=\!\tbinom{z}{\delta}$ and according to \cite{UpperBou50:online}, we can derive: %a bound for $C(z)$:
\begin{small}
\begin{equation}
    1 \leq (\frac{z}{\delta})^\delta \leq C(z)=\tbinom{z}{\delta} \leq (\frac{ez}{\delta})^\delta %,\textrm{\ and\ } 1 \leq C(z)\leq \tbinom{n}{\delta}
\end{equation}    
\end{small}


We pick $\beta_1, \beta_2$ such that there exist the following $A,B$
\begin{equation}
\small
\begin{aligned}
    A:&=\{z: C(z)=\tbinom{z}{\delta} \leq \beta_1\} \\
    B:&=\{z: C(z)=\tbinom{z}{\delta} \geq \beta_2\}
\nonumber
\end{aligned}
\end{equation}
that satisfy conditions $\BP(h(W) = y_A)=\underline{p_A}$ and $\BP(h(W) = y_B)=\overline{p_B}$.
According to the assumption, we have 
\begin{equation}
\small
\begin{aligned}
\BP(W \in A) = \underline{p_A} \leq p_A = \BP(h(W) = y_A) \\
\BP(W \in B) = \overline{p_B} \geq p_B = \BP(h(W) = y_B) 
\nonumber
\end{aligned}
\end{equation}

Thus, by applying Lemma~\ref{lem:np_staircase}, we have
\begin{equation}
\small
\begin{aligned}
\BP(h(V) \!=\! y_A) \geq \BP(V \!\in\! A) \textrm{\ and\ }
\BP(h(V) \!=\! y_B) \leq \BP(V \!\in\! B)
\nonumber
\end{aligned}
\end{equation}


Based on our {\bf Claims} shown later, we have
\begin{small}
\begin{align}
\label{eq:VinA} \BP(V \in A) &\geq \frac{1}{\tbinom{n}{\delta}\cdot p^\delta \cdot \Gamma(\delta, p)} \cdot \underline{p_A} \\
%
\label{eq:VinB} \BP(V \in B) &\leq \frac{1}{\tbinom{n}{\delta}\cdot p^\delta \cdot \Gamma(\delta, p) } \cdot \tbinom{z_{\max}}{\delta}\cdot \overline{p_B}
\end{align}    
\end{small}

where $z_{\max}=\argmax z$ s.t. $\tbinom{n}{z} p^z (1-p)^{(n-z)} \leq \overline{p_B}$.

In order to obtain $\BP(V \in A) > \BP(V \in B)$, from Eq.(\ref{eq:VinA}) and Eq.(\ref{eq:VinB}), we need $\rad_D\!=\!\argmax\delta$
\begin{small}
\begin{equation}
    \textrm{\ s.t.\ } \underline{p_A} \geq \tbinom{z_{\max}}{\delta} \cdot \overline{p_B} \Longleftrightarrow \tbinom{z_{\max}}{\delta} \leq \underline{p_A}/\overline{p_B}
\nonumber
\end{equation}    
\end{small}
where $z_{\max}=\argmax z$ s.t. $\tbinom{n}{z} p^z (1-p)^{(n-z)} \leq \overline{p_B}$.
%
% \tbinom{n}{\delta}^\delta \leq \tbinom{n}{\delta} \leq \tbinom{en}{\delta}^\delta
% $\int_A \tbinom{z}{n}\cdot p^{z} \rd z = \int_A \frac{n!}{z!(n-z)!}\cdot p^{z} \rd z =\underline{p_A}$
\end{proof} 


\begin{claim}
\begin{small}
$\BP(V \in A) \geq \frac{1}{\tbinom{n}{\delta}\cdot p^\delta \cdot \Gamma(\delta, p)} \cdot \underline{p_A}$    
\end{small}
%
\begin{proof}
Recall that $\sum_A \tbinom{n}{z} p^z (1-p)^{(n-z)}=\underline{p_A}$.
\begin{equation}
\small
\begin{aligned}
    \BP(V \in A)
    = & \sum_A \frac{1}{\Gamma(\delta, p)}\tbinom{n-\delta}{z-\delta}\cdot p^{z-\delta} (1-p)^{n-z} \\
%    = & \frac{1}{\tbinom{n}{\delta} \cdot p^\delta \cdot \Gamma(\delta, p)} \sum_A C(z) \cdot \tbinom{n}{z} p^z (1-p)^{(n-z)} \\
    \geq & \frac{1}{\tbinom{n}{\delta}\cdot p^\delta \cdot \Gamma(\delta, p) } \sum_A \tbinom{n}{z} p^z (1-p)^{(n-z)} \\
    = & \frac{1}{\tbinom{n}{\delta}\cdot p^\delta \cdot \Gamma(\delta, p)} \cdot \underline{p_A} 
\nonumber
\end{aligned}
\end{equation}
\vspace{-3mm}
\end{proof}
\end{claim}
%\BW{How does the second-to-last equation connects with $p_A$ in the last inequality?} \xy{I have rewritten the proof.} 
% \vspace{-1mm}


\begin{claim}
\begin{small}
 $\BP(V \in B) \leq \frac{1}{\tbinom{n}{\delta}\cdot p^\delta \cdot \Gamma(\delta, p) } \cdot \tbinom{z_{\max}}{\delta}\cdot \overline{p_B}$,\\ where $z_{\max}=\argmax z$ s.t. $\tbinom{n}{z} p^z (1-p)^{(n-z)} \leq \overline{p_B}$.   
\end{small}
%
\begin{proof}
Recall that $\sum_B \tbinom{n}{z} p^z (1-p)^{(n-z)}=\overline{p_B}$.
\begin{equation}
\small
\begin{aligned}
    \BP(V \in B)
    = & \sum_B \frac{1}{\Gamma(\delta, p)}\tbinom{n-\delta}{z-\delta} p^{z-\delta} (1-p)^{n-z} \\
  %  = & \frac{1}{\tbinom{n}{\delta} \cdot p^\delta \cdot \Gamma(\delta, p)} \sum_B C(z)\cdot \tbinom{n}{z} p^z (1-p)^{(n-z)} \\
    % \leq &\ \frac{1}{\tbinom{n}{\delta}\cdot p^\delta \cdot \Gamma(\delta, p)\cdot } \sum_B \tbinom{z}{z/2} \tbinom{n}{z} p^z (1-p)^{(n-z)} \\
    = & \frac{1}{\tbinom{n}{\delta}\cdot p^\delta \cdot \Gamma(\delta, p) } \sum_B \tbinom{z}{\delta}\cdot \tbinom{n}{z} p^z (1-p)^{(n-z)} \\
    \leq & \frac{1}{\tbinom{n}{\delta}\cdot p^\delta \cdot \Gamma(\delta, p) } \cdot \tbinom{z_{\max}}{\delta}\cdot \overline{p_B}
\nonumber
\end{aligned}
\end{equation}
%
where $z_{\max}=\argmax z$ s.t. $\tbinom{n}{z} p^z (1-p)^{(n-z)} \leq \overline{p_B}$.
\end{proof}
\end{claim}
% \vspace{-1mm}
%\BW{Similarly, how does the second-to-last equation connects with $p_B$ in the last inequality?} \xy{I have rewritten the proof.}


% \begin{claim}
% Given $z$, $(\frac{ez}{\delta})^\delta \leq e^z$
% \begin{proof}
% \begin{equation}
% \begin{aligned}
% \textrm{Given\ } z, \textrm{\ when\ } \delta\leq z, \frac{\partial(\frac{ez}{\delta})^\delta}{\partial \delta} = \ln\left(\dfrac{z}{\delta}\right)\cdot\left(\dfrac{z}{\delta}\right)^\delta\,\textrm{e}^\delta \geq 0
% \nonumber
% \end{aligned}
% \end{equation}
% Then $(\frac{ez}{\delta})^\delta_{\max}=e^z$


% \begin{equation}
% \begin{aligned}
% \textrm{Given\ } \delta, \frac{\partial(\frac{e z}{\delta})^\delta}{\partial z} = \dfrac{d e^d\cdot\left(\frac{z}{d}\right)^d}{z} \geq 0, \textrm{\ then\ } (\frac{ez}{\delta})^\delta_{\max}=e^z.
% \nonumber
% \end{aligned}
% \end{equation}


% \end{proof}
% \end{claim}


%===============================================================================

% \vspace{-3mm}
\section{Certified Inference Algorithm} \label{appendix:alg}
% Prediction and Certification Algorithm
% \vspace{-5mm}

% Figure environment removed


% \begin{algorithm}[h]
% \small
% \caption{Prediction algorithm} \label{alg:predict}
% \begin{algorithmic}[1]
% \Require Test sample $x$, $h$, $T$, $L_{emb}$, $\theta_T(\cdot, \rho)$, $\phi_T(\cdot, \varepsilon)$, $N$
% % \Ensure $y = x^N$
% \State $u\cdot w\gets L_{emb}(x)$
% \State $\texttt{counts} \gets \textsc{SampleUnderNoise}(h, \theta(u, \rho), \phi(w, \varepsilon), N)$
% \State $y_A, y_B \gets$ top two indices in \texttt{counts}
% \State $n_A, n_B \gets \texttt{counts}[y_A], \texttt{counts}[y_B]$
% % \If{$\textsc{BinomPValue}(n_A, n_A+n_B, 0.5)\leq \alpha$}
% %     \State \Return prediction $y_A$
% % \Else
% %     \State \Return ABSTAIN
% % \EndIf
% \State \textbf{if} $\textsc{BinomPValue}(n_A, n_A+n_B, 0.5)\leq \alpha$ \textbf{return} $y_A$
% \State \textbf{else} \textbf{return} ABSTAIN
% \end{algorithmic}
% \end{algorithm}

% \vspace{-1mm}
\begin{algorithm}[H]
\small
\caption{Certified inference algorithm} \label{alg:certify}
\begin{algorithmic}[1]
\Require Test sample $x$, $h$, $T$, $L_{emb}$, $\theta_T(\cdot, \rho)$, $\phi_T(\cdot, \varepsilon)$, $N$, $N_0$
\State $u\cdot w\gets L_{emb}(x)$
\State $\texttt{counts0} \gets \textsc{SampleUnderNoise}(h, \theta(u, \rho), \phi(w, \varepsilon), N_0)$
\State $y_A \gets$ top index in \texttt{counts0}
\State $\texttt{counts} \gets \textsc{SampleUnderNoise}(h, \theta(u, \rho), \phi(w, \varepsilon), N)$
\State $\underline{p_A} \gets \textsc{LowerConfBound}(\texttt{counts}[p_A], N, 1-\alpha)$
% \If{$\underline{p_A} > \frac{1}{2}$}
%     \State \Return prediction $y_A$ and radius $\rad_T$
% \Else
%     \State \Return ABSTAIN
% \EndIf
\State \textbf{if} $\underline{p_A} > \frac{1}{2}$ \textbf{return} prediction $y_A$ and radius $\rad_T$
\State \textbf{else} \textbf{return} ABSTAIN
\end{algorithmic}
\end{algorithm}

% \vspace{-5mm}




\section{Additional Experimental Results}
% \vspace{-3mm}

Table~\ref{tab:clean_acc} shows that Text-CRS can train robust models with little sacrifice in performance. Table~\ref{tab:attack_acc} shows that all attacks result in a significant reduction in model accuracy.

\vspace{0.02in}
\noindent\textbf{Ablation Study for Enhanced Training Toolkit.}
We compare the accuracy of Text-CRS against word insertions, with and without the enhanced training toolkit. For LSTM, both OGN and ESR are added, improving the average clean accuracy from 79.2\% to 84.1\%, see Figure~\ref{fig:noise3_enhance_lstm_acc} (left). Figure~\ref{fig:noise3_enhance_lstm_acc} (right) shows a significant increase in certified accuracy against the SynonymInsert attack, particularly in Amazon.%, which shows a 14.2\% improvement on $\sigma=0.1$. 
We also evaluate the certified accuracy under different radii, see Figure~\ref{fig:noise3_enhance_lstm_radius}.
% (refer to Figure~\ref{fig:noise3_enhance_lstm_other_radius} for $\sigma=0.2$ and $0.3$). 
The results show that the enhanced training toolkit lead to higher certified accuracy at the same radius. Regarding BERT, acceptable accuracy is achieved for low and medium noise levels. %due to its pre-training. 
However, the accuracy drops to 50\% for high noise levels, indicating a failure to identify the gradient update direction. Thus, we employ PLM to guide the training, increasing the certified accuracy to 84\%, see the last row, column 8 of Table~\ref{tab:certi_acc_benchmark}. 
% \hl{briefly discussing Figure 13 and 14 here (within 18 pages)}



% \vspace{-3mm}
% Figure environment removed


\vspace{-3mm}
% Figure environment removed


\vspace{-2mm}
\begin{table}[!h]
\centering
\setlength\tabcolsep{4pt}
\scriptsize
\caption{Clean model accuracy under vanilla training (\emph{Clean vanilla}) and under robust training (\emph{Clean Acc.}).}
\vspace{-2mm}
\begin{tabular}{ccccccc}
\toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Dataset\\ (Model) \end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}} \emph{Clean}\\ \emph{vanilla} \end{tabular}} & \multirow{2}{*}{Noise} & \multicolumn{4}{c}{\emph{Clean Acc.}} \\
\cmidrule{4-7}
 &  &  & Substitution & Reordering & Insertion & Deletion \\
 \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}AG\\      (LSTM)\end{tabular}} &  & Low & \textbf{90.12\%} & \textbf{91.67\%} & 90.38\% & \textbf{91.53\%} \\
 & 91.79\% & Med. & 89.59\% & 91.21\% & \textbf{86.66\%} & 91.01\% \\
 &  & High & 88.82\% & 91.40\% & 84.83\% & 90.38\% \\
  \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}AG\\      (BERT)\end{tabular}} &  & Low & \textbf{93.24\%} & \textbf{93.62\%} & \textbf{93.43\%} & 93.70\% \\
 & 93.68\% & Med. & 92.75\% & 93.55\% & 93.05\% & \textbf{93.71\%} \\
 &  & High & 92.63\% & 93.43\% & 91.78\% & 93.51\% \\
  \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Amazon\\      (LSTM)\end{tabular}} &  & Low & \textbf{87.86\%} & \textbf{88.59\%} & \textbf{88.51\%} & \textbf{88.71\%} \\
 & 89.82\% & Med. & 87.29\% & 88.44\% & 83.61\% & 88.62\% \\
 &  & High & 86.23\% & 88.36\% & 79.53\% & 88.10\% \\
  \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Amazon\\      (BERT)\end{tabular}} &  & Low & \textbf{93.91\%} & 94.11\% & \textbf{94.64\%} & \textbf{94.73\%} \\
 & 94.35\% & Med. & 93.07\% & 94.27\% & 94.43\% & 94.49\% \\
 &  & High & 91.62\% & \textbf{94.30\%} & 92.89\% & 93.84\% \\
  \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}IMDB\\      (LSTM)\end{tabular}} &  & Low & \textbf{83.39\%} & \textbf{86.85\%} & \textbf{84.86\%} & \textbf{86.33\%} \\
 & 86.17\% & Med. & 82.58\% & 86.08\% & 80.45\% & \textbf{86.32\%} \\
 &  & High & 81.07\% & 86.11\% & 77.96\% & 84.76\% \\
  \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}IMDB\\      (BERT)\end{tabular}} &  & Low & \textbf{91.52\%} & \textbf{92.08\%} & \textbf{91.88\%} & \textbf{92.46\%} \\
 & 91.52\% & Med. & 90.42\% & 91.92\% & 91.68\% & 92.17\% \\
 &  & High & 88.68\% & 91.99\% & 87.49\% & 90.55\% \\
 \bottomrule
\end{tabular}
\vspace{-2mm}
\label{tab:clean_acc}
\end{table}


\vspace{-3mm}
\begin{table}[!h]
\centering
\setlength\tabcolsep{2pt}
\scriptsize
\caption{\emph{Attack accuracy} of different real-world attacks.}
\vspace{-2mm}
\begin{tabular}{cccccc}
\toprule
\begin{tabular}[c]{@{}c@{}}Dataset\\ (Model)\end{tabular} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Text-\\ Fooler\cite{jin2020bert} \end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Word\\      Reorder\cite{moradi2021evaluating}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} Synonym\\  Insert\cite{morris2020textattack}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}BAE-\\      Insert\cite{garg2020bae}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Input\\      Reduction\cite{feng2018pathologies}\end{tabular}} \\
\midrule
AG (LSTM) & 2.46\% & 76.38\% & 70.55\% & - & 40.85\% \\
AG (BERT) & 6.67\% & 49.36\% & 75.07\% & 28.53\% & 56.59\% \\
Amazon(LSTM) & 0.09\% & 54.16\% & 61.25\% & - & 30.85\% \\
Amazon(BERT) & 15.38\% & 5.29\% & 66.39\% & 9.30\% & 41.68\% \\
IMDB (LSTM) & 0.00\% & 40.20\% & 58.12\% & - & 30.65\% \\
IMDB (BERT) & 21.81\% & 7.54\% & 57.03\% & 18.66\% & 36.03\% \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\label{tab:attack_acc}
\end{table}





%\begin{table}[]
%\centering
%\setlength\tabcolsep{3pt}
%\caption{Summary statistics for the datasets}
%\begin{tabular}{|c|ccccc|}
%\hline
%Dataset & Topic & Classes & Train & Test & Avg Len \\ \hline
%\begin{tabular}[c]{@{}c@{}}AG’s News\end{tabular} & News headline & 4 & 120K & 7.6K & 43 \\
%\begin{tabular}[c]{@{}c@{}}Amazon\end{tabular} & Product reviews & 2 & 50K & 50K & 81 \\
%\begin{tabular}[c]{@{}c@{}}IMDB\end{tabular} & Movie reviews & 2 & 25K & 25K & 215 \\ \hline
%\end{tabular}
%\label{tab:dataset}
%\end{table}





% \subsection{Attack Performances} \label{appendix:attacks}

% //TextFooler generates adversarial examples for text by utilizing word embedding distance and part-of-speech matching to first identify the most critical words in terms of the model’s output and subsequently greedily replaces them with synonyms that fit both semantically and grammatically until a misclassification occurs. 
% //The BERT language model was utilized in two studies to create textual adversarial examples: Garg and Ramakrishnan and Li et al., both of which proposed generating adversarial examples through text perturbations that are based on the BERT masked language model as part of the original text is masked and alternative text pieces are generated to replace these masks.
% //InputReduction: Feng et al. introduced a process called “input reduction”, which can expose issues regarding overconfidence and oversensitivity in natural language processing models. Under input reduction, non-important words are interactively removed from the input text while the model’s prediction for that input remains unchanged. The authors demonstrated that input texts could have their words removed to the degree that makes no sense to humans without impacting the model’s output.



%\section{Discussions} \label{sec:disscuss}



%Text-CRS can provide certification not only for word-level adversarial attacks but also for character-level and sentence-level adversarial attacks that employ the operations of substitution, reordering, insertion, or deletion. Character-level operations involve substituting words with the \texttt{<unknown>} token or other words, which have a limited range~\cite{li2018textbugger}. Our synonym substitution method can be applied to character-level attacks by replacing the synonym set with a word set that manipulates words' characters. Sentence-level operations include shuffling the entire sentence in a text~\cite{lee2020slm}, inserting new sentences while preserving the original ones~\cite{ribeiro2020beyond}, or completely paraphrasing sentences~\cite{iyyer2018adversarial}. For the first attack, our word reordering method is applicable by shuffling the sentences rather than the words. For the second attack, it is equivalent to inserting multiple words into the original text, and our word insertion method can provide a certified defense. For the last attack, the word insertion method can be theoretically applicable. However, due to the large $\ell_2$ distance between two completely different sentences, it may result in lower certified accuracy.


% \hl{discuss the extension to defense against additional word-level perturbations (besides the four)}


%\textbf{Limitation} The radius of word insertion ($\ell_2$) under LSTM is not large enough. \xy{Should we discuss this limitation here?} 




% % Figure environment removed

% % Figure environment removed



% % Figure environment removed
