\vspace{-0.05in}
\section{Preliminaries} %and \BW{Problem Formulation}}
\label{sec:prelim}
\vspace{-0.1in}

%\BW{Should we separate Preliminaries and Related Work? This part looks like a mixture of Related work and Preliminaries.} \xy{I have merged Preliminaries and Related Work.} \YH{I think we need to separate them. This section is too long. We can introduce preliminaries here, and related work in the second last section (before the conclusion)}

%\BW{Fix $x$ as word sequence, text, or words?} \xy{done, text}

\subsection{Text Classification} \label{sec:textclass}
\vspace{-0.1in}
% intent detection, document classification
% https://levity.ai/blog/9-text-classification-examples
% Text classification 
% % is a machine learning technique that 
% aims to assign a predefined label to a given text
% % . It is one of the fundamental tasks of NLP 
% and has a wide range of NLP applications, such as sentiment analysis, topic classification, and content filtering~\cite{9TextCla71:online}. Text classification tasks can handle short sentences (such as tweets, chatbot queries, and news headlines) and large documents (such as product reviews, news articles, and government reports). The text classification involves three key components: data processing, embedding layer, and classification model.
% Using sentiment analysis on movie reviews as an example, a text classification model takes a movie review as input, analyzes its content, and automatically categorizes its sentiment as positive or negative.
% from the input space $\CX$ , in the label space $\CY$
% Specifically, 
The objective of text classification is to map texts to labels. A text consisting of $m$ words is denoted as $x=\{x_1,...,x_m\}$, where $x_i$ is the $i$th word, aka., a token. 
Text classification involves three key components: data processing, embedding layer, and classification model. Given a text $x$ labeled $y$, data processing first pads it to a fixed length $n$. When $m<n$, the processing inserts $n-m$ \texttt{<pad>} tokens to the end of the text, and when $m>n$, the processing drops the $m-n$ tokens. 
For brevity, we will denote a text $x$ with a fixed length of $n$ after data processing, i.e., $x=\{x_1,...,x_n\}$. 
Then, the embedding layer converts each token $x_i$ 
% in the text 
into a high-dimensional embedding vector $w_i$. After deriving the embedding matrix $w=\{w_1, \cdots, w_n\}$, the text classification model $h$ learns the relationship between the embedding matrix $w$ and the label $y$. For a tuple $(w, y)$, the model uses a loss function $\ell$ to derive the loss $\ell(h(w), y)$ and updates the model $h$, e.g., using stochastic gradient descent. 

In text classification tasks, the embedding layer is essential for representing words in a text and often has a large number of parameters. In practice, the embedding layer  is usually a pre-trained word embedding, such as Glove~\cite{pennington2014glove} or 
% an embedding layer in 
a pre-trained language model, such as BERT~\cite{devlin-etal-2019-bert}. The parameters in the embedding layer are frozen and 
% during the learning process and are not updated. On the contrary, 
only parameters in the classification model are updated during training. The classification model typically uses a deep neural network of different architectures, such as recurrent neural network (RNN)~\cite{schuster1997bidirectional}, convolutional neural network (CNN)~\cite{kim-2014-convolutional}, and Transformer model~\cite{vaswani2017attention}. 

% Then the classification model $h$ learns to map the embedding space $\CW$ to label space $\CY$.

\vspace{-0.05in}
\subsection{Adversarial Examples for Text Classification} \label{sec:adv_attacks}
\vspace{-0.1in}

% what is and how to generate adversarial examples
Adversarial examples are well-crafted inputs that lead to the misclassification of machine learning models via slightly perturbing the clean data. In text classification, an adversarial text fools the text classification model, and is also semantically similar to the clean text for human imperceptibility. 
To generate adversarial texts, there are three main approaches based on different perturbation strategies. (1) \textit{Character-level adversarial attacks}~\cite{li2018textbugger, ebrahimi2017hotflip, gao2018black} substitute, swap, insert, or delete certain characters in a word and generate carefully crafted typos, such as changing \textit{``terrible''} to \textit{``terrib1e''}. However, these typos can be detected and corrected by spell checker easily~\cite{pruthi2019combating}. 
(2) \textit{Word-level adversarial attacks}~\cite{jin2020bert, moradi2021evaluating, morris2020textattack, garg2020bae, feng2018pathologies} alter the words within a text through four primary operations: substituting words with their synonyms, reordering words, inserting descriptive words, and deleting unimportant words. This attack is widely exploited because only a few words of perturbations can lead to a high attack success rate~\cite{wang2adversarial}.
% Word-level perturbations do not alter the texts' semantic meanings since only a few words are modified, but they can completely change the output of the classification model. 
(3) \textit{Sentence-level adversarial attacks}~\cite{iyyer2018adversarial, ribeiro2020beyond} adopt another perturbation strategy that paraphrases the whole sentence or inserts irrelevant sentences into the text. This approach affects predictions by disrupting the syntactic and logical structure of sentences rather than specific words. However, this attack makes it difficult to preserve the original semantics due to rephrasing or inserting irrelevant sentences~\cite{yang2022robust}.

%\subsubsection{Word-level Adversarial Attacks} \label{sec:word_ae}
%To tackle the challenges of adversarial attacks,
As discussed above, the word-level adversarial attacks have widespread and severe impacts. Thus, in this paper, we focus on the certified defenses against \emph{word-level adversarial attacks}. We distill and consolidate such attacks into four fundamental adversarial operations: substitution, reordering, insertion, and deletion (see Figure \ref{fig:Text-CRS}). We offer provable robustness guarantees against these operations. The proposed theorems and defense methods can be extended to the other two types of adversarial attacks on text classification with the similar operations. %(as discussed in Section \ref{sec:disscuss}).

\vspace{0.03in}

\noindent\textbf{Synonym Substitution:} %~\cite{alzantot2018generating,ren2019generating,jin2020bert, zang2020word,tan2020s,dongtowards}. 
this operation generates adversarial texts by replacing certain words in the text with their synonyms, thereby preserving the text's semantic meanings. 
% Ren et al.~\cite{ren2019generating} propose a greedy PWWS algorithm to determine the replacement order of words in a sentence and the selection of synonyms. 
For instance, to minimize the word substitution rate, \textit{TextFooler}~\cite{jin2020bert} picks the word crucial to the prediction, i.e., when this word is removed, the prediction undergoes a significant deviation; then, it selects synonyms with high cosine similarity to the original embedding vector for substitution. 
% Tan et al.~\cite{tan2020s} proposed Morpheus, which generates plausible and semantically similar adversarial texts by replacing the words with their inflected form.


\vspace{0.03in}

\noindent\textbf{Word Reordering:} %~\cite{moradi2021evaluating,nie2019analyzing,yan2021consert,lee2020slm}. 
this operation selects and randomly reorders several consecutive words in the text while keeping the words themselves unchanged. For instance, 
\textit{WordReorder}~\cite{moradi2021evaluating} investigates the sensitivity of NLP systems to such adversarial operation and shows that reordering leads to an average decrease in the accuracy of 18.3\% and 15.4\% for the LSTM-based model and BERT on five datasets. 
% In Transformer models, the \texttt{"position\_ids"} parameter is utilized to determine the position of a token within the text\cite{devlin-etal-2019-bert}. However, this parameter is ineffective in defending against word reordering because the words' positions are altered directly in the input text. 


\vspace{0.03in}

\noindent\textbf{Word Insertion:} %~\cite{morris2020textattack,lietal2021contextualized,garg2020bae, behjati2019universal}. 
this operation generates adversarial text by inserting new words into the clean text. For instance, the NLP adversarial example generation library TextAttack~\cite{morris2020textattack} includes a basic insertion strategy \textit{SynonymInsert} that inserts synonyms of words already in the text to maintain semantic similarity between adversarial and clean texts. 
% Table~\ref{tab:certi_acc} displays that this operation can result in an average 26.48\% reduction in accuracy. 
\textit{BAE-Insert}~\cite{garg2020bae} uses masked language models (e.g., BERT) to predict newly inserted \texttt{<mask>} tokens in the text. The predicted words are then used to replace the \texttt{<mask>} tokens for the adversarial text. Compared to SynonymInsert, BAE-Insert improves syntactic and semantic consistency. %~\cite{garg2020bae}

\vspace{0.03in}

\noindent\textbf{Word Deletion:} %~\cite{feng2018pathologies,moradi2021evaluating,xie2022word}. 
this operation generates adversarial texts by removing several words from clean text. 
\textit{InputReduction}~\cite{feng2018pathologies} iteratively removes the least significant words from the clean text, and demonstrates that specific keywords play a critical role in the prediction of language models. Table~\ref{tab:attack_acc} shows that it can lead to an average of $51.78\%$ accuracy reduction. 
% As a result, modifying only a limited number of keywords can significantly impact the final prediction results while preserving semantic similarity. 

\vspace{-0.05in}
\subsection{Threat Model} \label{sec:threatmodel}
\vspace{-0.1in}
%\BW{A general comment on describing the attacks: should we add references for each attack only while leaving the attack details in the prelim section?} \xy{I have removed the details to the preliminary}\YH{let us merge 3.1 and 2.2 (adversarial examples and threat model) and name section 2 as preliminaries. Also, move 3.3 to Sec 4. Sections 2, 3, 4 are somewhat repetitive.} \xy{done}

We consider a threat model similar to that of other randomized smoothing and certified methods~\cite{ye2020safer, zhao2022certified, li2021tss}, which guarantees robustness as long as perturbations remain within the certified radius. They provide effective defense against both white-box and black-box attacks, irrespective of the specific attack types and adopted methods.
Specifically, we assume that the adversary can launch the evasion attack on a given text classification model by intercepting the input and perturbing the input with a wide variety of \emph{word-level adversarial attacks}~\cite{jin2020bert,moradi2021evaluating,morris2020textattack,garg2020bae}. 
% , assuming that the adversary has full access to the model (e.g., model architecture, model parameters). 
Given a text classification model $h$ and a text $x$ with label $y$, the goal of the adversary is to craft an adversarial text $x'$ from $x$ to alter its prediction, i.e., $h(x') \neq h(x)=y$. 
% To obtain $x'$, the adversary can employ various operations on $x$. 
While generating the adversarial texts, the adversary can choose any single aforementioned word-level adversarial operation or a combination thereof, as all textual adversarial attacks can be unified as the transformation of the embedding vector. These four types of operations encompass almost all possible modifications to texts in adversarial attacks~\cite{moradi2021evaluating} and their 
% (to be performed by the adversary) 
formal definitions are provided as below: 
% replacing words in $x$ with their synonyms, randomly reordering the words in $x$, inserting words, and removing random from $x$. These four operations encompass common word-level adversarial attacks.
% Next, we comprehensively explain how to produce an adversarial sequence $x'$ from a specified sequence $x$ using the four operations.
% and show that embedding-level transformations are equivalent to word-level operations. 

\vspace{0.03in}

\noindent\textbf{Synonym Substitution:} we replace certain words in the text $x$ with synonyms of the original word. Specifically, we convert each word $x_i$ to $x_i'$, where $x_i'$ may be a synonym of $x_i$ or $x_i$ itself. The operation can be represented as:
%
$$x=\{{x_1}, \cdots, {x_n}\}\to x'=\{x_1', \cdots, x_n'\},$$
where $x$ and $x'$ are of the same length. 


\vspace{0.03in}

\noindent\textbf{Word Reordering:} to reorder certain words in the text $x$, we move the word at position $i$ to position $r_i$, where $r_i$ may be equal to $i$. The operation can be expressed as:
% 
$$x=\{{x_1}, \cdots, {x_n}\} \to x'=\{{x_{r_1}}, \cdots, {x_{r_n}}\},$$
where $x$ and $x'$ are of the same length. 

\vspace{0.03in}

\noindent\textbf{Word Insertion:} 
we insert a word $x_\mathrm{In}^1$ into the $j_1$th position, a word $x_\mathrm{In}^2$ into the $j_2$th position, $\cdots$, and a word $x_\mathrm{In}^{m'}$ into the $j_{m'}$th position to $x$, where $m'$ is the total number of inserted words. The operation can be expressed as:
%
\begin{equation}
\begin{aligned}
x= &\{x_1,\cdots, x_{j_1-1}, x_{j_1}, \cdots,  x_{j_2-1}, x_{j_2}, \cdots, x_n\} \to \\
x'= &\{x_1,\cdots, x_{j_1-1}, x_\mathrm{In}^1, x_{j_1}, \cdots, x_{j_2-1}, x_\mathrm{In}^2, x_{j_2}, \cdots, x_n\},
\nonumber
\end{aligned}
\end{equation}
%
where $x'$ includes $m'$ more words than $x$.

\vspace{0.03in}

\noindent\textbf{Word Deletion:} 
we delete $m'$ words at position $j_1, \cdots, j_{m'}$ from $x$. The operation can be represented as:
%
\begin{equation}
\begin{aligned}
x=&\{x_1,\cdots, x_{j_1-1}, x_{j_1}, x_{j_1+1}, \cdots, x_{j_2-1}, x_{j_2}, \cdots, x_n\} \to \\
x'=&\{x_1,\cdots, x_{j_1-1}, x_{j_1+1}, \cdots, x_{j_2-1}, x_{j_2+1}, \cdots, x_n\},
\nonumber
\end{aligned}
\end{equation}
%
where $x'$ includes $m'$ words less than $x$.

%\BW{The above two attacks are described in a general way, but here described using a single word? It is for ease of description, but seems not consistent. Same for word deletion attacks}\xy{done}

\vspace{-0.05in}
\subsection{Randomized Smoothing for Certified Defense}
\vspace{-0.1in}

Randomized smoothing~\cite{lecuyer2019certified,cohen2019certified} is a widely adopted certified defense method that offers state-of-the-art provable robustness guarantees for classifiers against adversarial examples. % This approach involves training a smoothed version of the classifier and certifying its robustness during prediction. Specifically, during  training, a random noise sampled from certain distribution is injected to the input data. The classifier is then trained on the noisy input, resulting in a more stable decision boundary that is less susceptible to perturbations. During certification, the smoothed classifier is theoretically guaranteed to maintain stable prediction results within a certain range of perturbations. The perturbation radius for a given input is determined by considering the random noise level and the probability of the smoothed classifier assigning the input to the most probable class ($y_A$) as well as the second most probable class ($y_B$). 
% adding random noise sampled from a suitable probability distribution
It has two key advantages: applicable to any classifier and scalable to large models. 
Given a testing example ${x}$ with label $y$ from a label set $\mathcal{Y}$, 
randomized smoothing has three steps: 1) 
% given an arbitrary 
define a (\emph{base}) classifier $h$; 2) build a \emph{smoothed classifier} $g$ based on $h$, ${x}$, and a noise distribution;  
% via adding random noise to the testing example, 
and 3) derive certified robustness for the  smoothed classifier $g$. 
Under our context, the base classifier $h$ can be any trained text classifier and $x$ is a testing text.
Let  $\epsilon$ be a random noise drawn from an \emph{application-dependent} {noise distribution}. Then the smoothed classifier $g$ is defined as $g({x}) = \argmax_{l\in \mathcal{Y}}\text{Pr}(h({x}+\epsilon)=l)$.
Let $p_A, p_B\in[0,1]$ be the probability of the most ($y_A$) and the second most probable class ($y_B$) outputted by $\text{Pr}(h({x}+\epsilon))$, respectively, i.e., $p_A= \max_{l} \text{Pr}(h({x}+\epsilon)=l)$ and $p_B=\max_{l \neq y_A} \text{Pr}(h({x}+\epsilon)=l)$. 
Then $g$ provably predicts the same label $y_A$ for ${x}$ once the adversarial perturbation $\delta$ is bounded, i.e., $ g({x}+\delta) = y_A, \forall ||\delta||_p \leq R$, where  $||\cdot ||_p$ is an $\ell_p$ norm and $R$ is called \emph{certified radius} that depends on $p_A, p_B$. 
For example, when the noise distribution is an isotropic Gaussian distribution with mean 0 and standard deviation $\sigma$, \cite{cohen2019certified} adopted the Neyman-Person Lemma~\cite{neyman1933ix} and derived a \emph{tight} robustness guarantee against $l_2$ perturbation, i.e., 
$g({x}+\delta) = y_A, \forall ||\delta||_2 \leq R=\frac{\sigma}{2}(\Phi^{-1}({p_A})-\Phi^{-1}({p_B}))$, 
where $\Phi^{-1}$ is the inverse of the standard Gaussian {CDF}. 
This property implies that the smoothed classifier $g$ maintains constant predictions if the norm of the perturbation $\delta$ is smaller than the certified radius $R$.

% Formally, given a classification model $h_0: \CX\to \CY$, where $\CX$ is the input text space and $\CY$ is the set of possible labels, the randomized classifier $g_0(x)$ with Gaussian smoothing is defined as
% $ 
% g_0(x) = \argmax_{y\in \CY} \BP(h_0(x+\varepsilon) = y) 
% $,
% where $\varepsilon$ is the random noise sampled from Gaussian distribution $\CN(0, \sigma^2I)$ with noise level $\sigma$. 
% To obtain the robustness of the smoothed classifier, let $p_A, p_B\in[0,1]$ be the probability of the most ($y_A$) and the second most probable class ($y_B$), respectively.
% %  and it satisfies
% % \[
% % \BP(h_0(x+\varepsilon)=y_A)\geq \underline{p_A} \geq \overline{p_B} \geq 
% % \max_{y_B\neq y_A}\BP(h_0(x+\varepsilon)=y_B)
% % \] 
% The classifier can then guarantee that the predictions remain constant (i.e., $g_0(x+\delta)=y_A$) for all perturbations $\|\delta\|_2$ within a certain radius. Specifically, the radius is equal to $\frac{\sigma}{2}(\Phi^{-1}({p_A})-\Phi^{-1}({p_B}))$, where $\sigma$ is the standard deviation of the Gaussian noise, and $\Phi^{-1}$ is the inverse of the standard Gaussian cumulative distribution function (CDF). This property implies that the smoothed classifier maintains constant prediction if the perturbation is smaller than the certified radius.
% 

% Multiple variations of randomized smoothing exist that involve sampling random noise from diverse distributions. They can be implemented to defend against different adversarial attacks, including image semantic transformations~\cite{li2021tss,alfarra2022deformrs,hao2022gsmooth}, 3D point cloud modification~\cite{liu2021pointguard,perez20223deformrs}, and discrete graph perturbations~\cite{bojchevski2020efficient,wang2021certified}. However, the direct application of continuous smoothing methods to word-level operations is not feasible due to the discrete nature of words. And discrete smoothing methods for graph data are also not applicable due to the significantly larger word space compared to the (binary) graph space. Furthermore, previous research on word-level operations solely offers robustness guarantees in the $\ell_0$ norm concerning synonym substitution operations\cite{ye2020safer,wang2021certified}, while disregarding the other three types of operations. Therefore, it is imperative to develop a general randomized smoothing framework for fundamental word-level operations.

% different smoothing strategy by sampling from different distributions. For instance, smoothing with Gaussian, Uniform, and Laplace distributions can certify model robustness against image semantic transformations; smoothing with Gaussian and Uniform distributions can certify against 3D point cloud modification; smoothing with Gaussian distribution can certify against image backdoor attacks; smoothing with Binomial distribution can certify Graph Neural Networks against discrete structure perturbation. However, 

% \cite{yang2020randomized}

% \cite{weber2022rab,xie2021crfl} 
