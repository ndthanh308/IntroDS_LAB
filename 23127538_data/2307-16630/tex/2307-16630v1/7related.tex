\vspace{-0.05in}
\section{Related Work} \label{sec:related}
\vspace{-0.1in}

\noindent\textbf{Word-level Adversarial Attacks.} 
These attacks aim to mislead the model by modifying the words in four adversarial operations: synonym substitution~\cite{alzantot2018generating,ren2019generating,jin2020bert, zang2020word,tan2020s,dongtowards}, word reordering~\cite{moradi2021evaluating,nie2019analyzing,yan2021consert,lee2020slm}, word insertion~\cite{morris2020textattack,li2021contextualized,garg2020bae, behjati2019universal}, and word deletion~\cite{feng2018pathologies,moradi2021evaluating,xie2022word}. 
For instance, Ren et al.~\cite{ren2019generating} propose a greedy PWWS algorithm to determine the replacement order of words in a sentence and the selection of synonyms. 
% Jin et al.~\cite{jin2020bert} proposed TextFooler, which picks the word crucial to the prediction, i.e., when this word is removed, the prediction undergoes a significant change; then, selects synonyms with high cosine similarity to the original embedding vector for substitution. 
Tan et al.~\cite{tan2020s} proposed Morpheus, which generates plausible and semantically similar adversarial texts by replacing the words with their inflected form. Moradi and Samwald~\cite{moradi2021evaluating} investigated the sensitivity of NLP systems to such operation. % and showed that such operation led to an average 18.3\% decrease in the accuracy of LSTM-based models.
Morris et al.~\cite{morris2020textattack} proposed TextAttack, including a basic strategy of inserting synonyms of words already in the text.
% to maintain semantic similarity between the adversarial and clean text. 
Li et al.~\cite{li2021contextualized} proposed CLARE, which, similar to BAE, also employs masked language models to predict newly inserted \texttt{<mask>} tokens in the text and replace them.
% . The predicted words are then used to replace the \texttt{<mask>} tokens to generate the adversarial text. 
These adversarial texts have improved syntactic and semantic consistency compared to directly inserting synonym words~\cite{garg2020bae}. 
Feng et al.~\cite{feng2018pathologies} proposed input reduction, which involves the iterative removal of the least significant words from the clean text. 
% This method was employed to demonstrate that specific keywords play a critical role in the prediction of the language models.


\vspace{0.03in}

% \textbf{Synonym Substitution}~\cite{alzantot2018generating,ren2019generating,jin2020bert, zang2020word,tan2020s,dongtowards}. 
% This operation generates adversarial texts by replacing certain words in the text with their synonyms, thereby preserving the text's semantic meanings. 
% To minimize the word substitution rate, Ren et al.~\cite{ren2019generating} propose a greedy PWWS algorithm to determine the replacement order of words in a sentence and the selection of synonyms. Jin et al.~\cite{jin2020bert} proposed TextFooler, which picks the word crucial to the prediction, i.e., when this word is removed, the prediction undergoes a significant change; then, selects synonyms with high cosine similarity to the original embedding vector for substitution. Tan et al.~\cite{tan2020s} proposed Morpheus, which generates plausible and semantically similar adversarial texts by replacing the words with their inflected form.


% \textbf{Word Reordering}~\cite{moradi2021evaluating,nie2019analyzing,yan2021consert,lee2020slm}. 
% This operation selects and randomly reorders several consecutive words in the text while keeping the words themselves unchanged. 
% Moradi and Samwald~\cite{moradi2021evaluating} investigated the sensitivity of NLP systems to such operation and showed that such operation led to an average 18.3\% decrease in the accuracy of LSTM-based models. 
% In Transformer models, the \texttt{"position\_ids"} parameter is utilized to determine the position of a token within the text\cite{devlin-etal-2019-bert}. However, this parameter is ineffective in defending against word reordering because the words' positions are altered directly in the input text. 

% SOSwap~\cite{nie2019analyzing}, 
% Token Shuffling~\cite{yan2021consert}, 
% Sequence Shuffling~\cite{lee2020slm}


% \textbf{Word Insertion}~\cite{morris2020textattack,lietal2021contextualized,garg2020bae, behjati2019universal}. 
% This operation generates adversarial text by inserting new words into the clean text. Morris et al.~\cite{morris2020textattack} proposed an adversarial attacks framework, TextAttack, which includes a basic strategy of inserting synonyms of words already in the text to maintain semantic similarity between the adversarial and clean text. 
% Garg and Ramakrishnan~\cite{garg2020bae} and Li et al.~\cite{lietal2021contextualized} proposed BAE and CLARE, which both use masked language models (such as BERT) to predict newly inserted \texttt{<mask>} tokens in the text. The predicted words are then used to replace the \texttt{<mask>} tokens to generate the adversarial text. These adversarial texts have improved syntactic and semantic consistency compared to directly inserting synonym words~\cite{garg2020bae}.


% \textbf{Word Deletion}~\cite{feng2018pathologies,moradi2021evaluating,xie2022word}. 
% This operation generates adversarial texts by removing several words from the clean text. 
% Feng et al.~\cite{feng2018pathologies} proposed input reduction, which involves the iterative removal of the least significant words from the clean text. This method was employed to demonstrate that specific keywords play a critical role in the prediction of the language models. Table~\ref{tab:certi_acc} displays that this operation can result in an average 51.78\% reduction in accuracy.


\noindent\textbf{Certified Defenses against Word-level Adversarial Operations.}
These methods rely on interval bound propagation (IBP)~\cite{jia2019certified, huang2019achieving, ko2019popqorn}, zonotope abstraction~\cite{du2021cert, bonaert2021fast} or randomized smoothing~\cite{ye2020safer,zeng2021certified}. IBP~\cite{jia2019certified} and zonotope abstraction~\cite{du2021cert} are both linear relaxation methods, which calculate the lower and upper bound of the model output and then minimize the worst-case loss for certification. Ko et al.~\cite{ko2019popqorn} introduced POPQORN, which uses linear functions to bound the nonlinear activation function in RNN. On more complicated Transformer models, Bonaert et al.~\cite{bonaert2021fast} proposed DeepT to certify against synonym substitution operations based on multi-norm zonotope abstract interpretation. However, IBP and zonotope abstraction methods are not scalable, and few can tightly certify large-scale pre-trained models, such as BERT. 
To address this challenge, Ye et al.~\cite{ye2020safer} proposed SAFER, which leverages randomized smoothing to provide $\ell_0$ certified robustness against synonym substitutions. Zhao et al.~\cite{zhao2022certified} proposed CISS, which combines the IBP encoder and randomized smoothing to guarantee $\ell_2$ robustness against word substitution attacks. However, since CCIS maps the input into a semantic space, only the certified radius in the semantic space is available, not the certified radius in the practical word space. % CCIS improves the certified accuracy compared to SAFER. 
Zeng et al.~\cite{zeng2021certified} proposed RanMASK which provides $\ell_0$ certification against random word substitution. 
However, the exact $\ell_0$ radius of RanMASK is impractical to compute, requiring search traversal. Additionally, such methods cannot certify against universal adversarial operations. 

% Although CISS evaluated the empirical accuracy against other adversarial attacks, however, this method can not provide guarantees against other operations universally. 

% IBP: This technique can be used to verify the robustness of a neural network against adversarial attacks by ensuring that the output remains within a certain range for all possible inputs. 
% IBP only works for certifying neural networks with continuous inputs

% WordDP~\cite{wangw2021certified}
% manifold-based defenses: empirical

\vspace{0.03in}

\noindent\textbf{Randomized Smoothing against Semantic Attacks.}
Semantic attacks manipulate inputs through semantic transformations, such as image rotation and blurring, to mislead the models. To mitigate them, some randomized smoothing methods have been proposed by sampling random noise from diverse distributions. For instance, Li et al. proposed TSS~\cite{li2021tss} to use Gaussian, uniform, and Laplace distributions to certify against general semantic transformations. DeformRS~\cite{alfarra2022deformrs} and GSmooth~\cite{hao2022gsmooth} certify image semantic transformations like translation, scaling, and steering. Liu et al.~\cite{liu2021pointguard} proposed PointGuard to certify against point modification, addition, and deletion via uniform distribution. Perez et al.~\cite{perez20223deformrs} proposed 3DeformRS, for probabilistic certification of point cloud DNNs against point semantic transformations. Finally, Bojchevski et al.~\cite{bojchevski2020efficient} and Wang et al.~\cite{wang2021certified} used the Binomial distribution to certify the graph neural networks against discrete structure perturbations. 
% However, previous methods cannot be directly applied to the NLP domain. First, NLP involves numerous words in a larger, heterogeneous, and discrete space, which differs from image or graph spaces. Second, word reordering and insertion are new semantic transformations not typically encountered in computer vision.


% However, the direct application of continuous smoothing methods to word-level operations is not feasible due to the discrete nature of words. And discrete smoothing methods for graph data~\cite{bojchevski2020efficient,wang2021certified} are also not applicable due to the significantly larger word space compared to the (binary) graph space. 
% Furthermore, previous research on word-level operations solely offers robustness guarantees in the $\ell_0$ norm concerning synonym substitution operations\cite{ye2020safer,wang2021certified}, while disregarding the other three types of operations. Therefore, it is imperative to develop a general randomized smoothing framework for fundamental word-level operations.

