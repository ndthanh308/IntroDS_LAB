\vspace{-0.1in}
\section{Permutation and Embedding Transformation based Randomized Smoothing} \label{sec:theorem}
\vspace{-0.1in}

%\BW{Should we first show a universal robustness guarantee against the 4 attacks (as this is the most practical and most powerful). Then we relax to show a customized robustness guarantee for each attack?} 
%\xy{
%\ding{172} Synonym substitution is a most common attack which has other baseline methods. 
%\ding{173} Word reordering, insertion, and deletion all use uniform-based permutations. The last two are combination operations. My original logic was to illustrate operations from simple to complex. Then explain why word insertion can provide a universal guarantee. 
%\ding{174} If we describe the word insertion first, there is little context in word reordering.  

%But I also agree with Prof. Wang's suggestion. }
 
% \BW{I also suggest to clearly define "what perturbations we are going to certify" at the beginning.} \xy{I define the perturbation at the beginning of each subsection}


In this section, we design the randomized smoothing for Text-CRS. %by first defining the base transformation-smoothing classifier and then 
% And subsequently 
%optimizing it against various word-level attacks. 
We construct a new transformation-smoothing classifier $g$ from an arbitrary base classifier $h$ by performing random permutation and random embedding transformation. Specifically, the transformation-smoothing classifier $g$ predicts the top-1 class returned by $h$ when the permutation and embedding transformation perturb the input embedding $u\cdot w$. Such a smoothed classifier can be defined as below.

\begin{definition}[$(\rho, \varepsilon)$-Smoothed Classifier]
\label{def:smoothg}
Let $\theta: \CU\times \CR \to \CU$ be a permutation, $\phi: \CW\times \CT \to \CW$ be an embedding transformation, and let $h:\CU\cdot\CW\to \CY$ be an arbitrary base classifier. Taking random variables $\rho\sim \BP_\rho$ from $\CR$ and $\varepsilon\sim \BP_\varepsilon$ from $\CT$, respectively, we define the $(\rho, \varepsilon)$-smoothed classifier $g:\CU\cdot\CW\to \CY$ as
\begin{equation}
\small
    g(u\cdot w)=\argmax_{y\in \CY}\BP(h(\theta(u, \rho)\cdot \phi(w, \varepsilon)))
    % q(y|u\cdot w; \rho, \varepsilon)\coloneqq \BE(p(|\theta(u, \rho)\cdot \phi(w, \varepsilon)))
\label{eq:smoothg}
\end{equation}
%
Given a constant permutation matrix, only the embedding transformation is performed. Thus, we have

\begin{equation}
\small
    g(u\cdot w)=\argmax_{y\in \CY}\BP(h(u\cdot \phi(w, \varepsilon)))
    % q(y|u\cdot w; \rho, \varepsilon)\coloneqq \BE(p(|\theta(u, \rho)\cdot \phi(w, \varepsilon)))
\label{eq:smoothg1}
\end{equation}
%
Similarly, given a constant embedding matrix, only the permutation is performed. Thus, we have
\begin{equation}
\small
    g(u\cdot w)=\argmax_{y\in \CY}\BP(h(\theta(u, \rho)\cdot w))
    % q(y|u\cdot w; \rho, \varepsilon)\coloneqq \BE(p(|\theta(u, \rho)\cdot \phi(w, \varepsilon)))
\label{eq:smoothg2}
\end{equation}
\end{definition}

To certify the classifiers against various word-level attacks, adopting an appropriate permutation $\theta$ and embedding transformation $\phi$ is necessary. For instance, to certify robustness against synonym substitution, using the same (substitution) transformation in the smoothed classifier is reasonable. Nevertheless, this strategy may not yield the desired certification for other types of operations. %necessitating an alternative approach. 
Next, we illustrate the transformations and certification theorems corresponding to the four word-level operations.

\vspace{-0.1in}
\subsection{Certified Robustness to Synonym Substitution} 
\vspace{-0.1in}

Synonym substitution only transforms the embedding matrix without changing the permutation matrix. 
% $u$ 
%unchanged. 
Previous works~\cite{ye2020safer, zeng2021certified} assume a uniform distribution over a set of synonymous substitutions, i.e., the probability of replacing a word with any synonym is the same. \emph{However, this assumption is unrealistic since the similarity between each synonym and the word to be substituted would be different.} For instance, when substituting the word \emph{good}, \emph{excellent} and \emph{admirable} are both synonyms, but the cosine similarity between the embedding vector of (\emph{good, excellent}) is higher than that of (\emph{good, admirable})~\cite{pennington2014glove}. Hence, the likelihood of selecting \emph{excellent} as a substitution should be higher than choosing \emph{admirable}. 
To this end, 
% introduce 
% to model the relationship between a word and its synonyms 
we design a smoothing method based on the Staircase randomization~\cite{geng2015staircase} . 
% This method provides certification for the synonym substitution perturbation, denoted as $\delta_{S}$, which indicates the distance between the words and their synonym indexes (including the words themselves),  
% % by modeling the distance of synonym indexes, 
% i.e., 
% \begin{equation}
%     \delta_{S}= \{a_1, \cdots, a_n\}. %,\quad \|\delta_{S}\|_1=\sum_{i=1}^{n} a_i
% \end{equation}

% Each embedding vector $w_i$ has an independent set of synonym embeddings, $\Omega_{w_i}$, where all the embedding vectors for the synonyms are sorted based on their cosine similarity with $w_i$ in descending order. For a set of synonyms of size $s$, $\Omega_{w_i}$ can be represented as $\{w_i^0, w_i^1, \cdots, w_i^s\}$, where $w_i^0$ refers to $w_i$ itself and $w_i^{a_i}$ represents the $a_i$th vector. A larger value for $a_i$ indicates a lower cosine similarity between $w_i^{a_i}$ and $w_i$, resulting in a larger transformation noise.



% Accordingly, we propose a novel embedding transformation $\phi_S$ based on cosine similarity among embedding vectors. 
% Specifically, we define $\phi_S$ as below: %is denoted as 
% \begin{equation}
% \begin{aligned}
%     \phi_S(\{w_1,\cdots, w_n\},t)=\{w_1^{a_1}, \cdots, w_n^{a_n}\} %=\{w_1', \cdots, w_n'\}
% \nonumber
% \end{aligned}
% \end{equation}

% Each embedding vector $w_i$ has an independent synonym embedding set $\Omega_{w_i}$, where all synonyms' embedding vectors are sorted in descending order based on their cosine similarity with $w_i$. For a set of synonyms of size $d$, $\Omega_{w_i}=\{w_i^0, w_i^1, \cdots, w_i^d\}$, $w_i^0$ is $w_i$ itself, and $w_i^{a_i}$ is the $a_i$th vector. A larger $a_i$ indicates a lower cosine similarity between $w_i^{a_i}$ and $w_i$ and a larger transformation noise.

% , and $w_i'$ is an embedding vector in $\Omega_{w_i}$. 
% \begin{equation}
%     w_i+a_i \coloneqq w_i^{a_1}
% % \label{eq:s_alpha}
% \end{equation}


% \BW{No need to define $\phi_S$ again. Just need to explain that we design a novel staircase mechanism that captures this property. And use an example to illustrate how?} \xy{I have revised this subsection.}


% The mechanism involves partitioning the range of possible output values into intervals (steps) and adding noise to the output proportional to the width of the step where the output falls.
\vspace{-0.1in}
\subsubsection{Staircase Randomization-based Synonym Substitution}
% \subsubsection{Staircase  Mechanism}
%\BW{Motivation of Staircase Mechanism is unclear to me, though I know it is a better mechanism than Laplace.} \xy{I've added some reasons in the underlined area. Do you think they are convincing?}
The Staircase randomization mechanism originally uses a staircase-shape distribution to 
replace the standard Laplace distribution for improving the accuracy of differential privacy~\cite{geng2015staircase}. It consists of partitioning an additive noise into intervals (or steps) and adding the noise to the original input, with a probability proportional to the width of the step where the noise falls. The one-dimensional staircase-shaped probability density function (PDF) is defined as follows:

\begin{definition}[Staircase PDF~\cite{geng2015staircase}]
\label{def:staircase}
% a geometric mixture of uniform random variables, symmetric 
Given constants $\gamma, \epsilon \in[0,1]$, we define the PDF $f_\gamma^\epsilon(\cdot)$ at location $\mu$ with $\Delta>0$ as
%
\small
\begin{align}
    \label{eq:f_gamma} f_\gamma^\epsilon(x\ |\ \mu, \Delta)&=\exp({-l_\Delta(x\ |\ \mu)\epsilon})a(\gamma) \\
    \label{eq:l_delta} l_\Delta(x\ |\ \mu)&=\lfloor \frac{\|x-\mu\|_1}{\Delta}+(1-\gamma) \rfloor
\end{align}
%
\normalsize
where the normalization factor $a(\gamma)$ ensures $\int_{\BR} f_\gamma^\epsilon(x)\rd x=1$ and $\lfloor \cdot \rfloor$ is the floor function. 
%\BW{$\epislon$ is i} 
Note that we unify and simplify the segmentation function in the original definition. 
% We denote the Staircase noise $\CS_\gamma^\epsilon(x)$ as the distribution which PDF is $f_\gamma^\epsilon(\cdot)$. 
% The original definition of the Staircase mechanism and detailed expression for $a(\gamma)$ can be found in Appendix~\ref{appendix:staircase}.
\end{definition} 

In Text-CRS, we use the Staircase PDF to model the relationship between a word and its synonyms.  
% fit the distribution between a word and its various synonyms. 
Specifically, given a target word, we first compute the cosine similarity between the embedding of itself and its synonyms. 
%and rank the synonyms based on their similarities. 
Then, we define the noise intervals and the substitution probability, where the number of intervals equals the number of synonyms, and the synonyms are symmetrically positioned on the intervals based on their similarities. 
%cosine similarity to the original embedding.
Table~\ref{tab:synonym} shows an example target word ``\emph{good}'' and it has two synonyms \emph{excellent} and \emph{admirable} (the total number of synonyms is $s = 5/\epsilon=5$). %and we ensure that $\epsilon\in [0,1]$ in the evaluation). 
%As shown in Table~\ref{tab:synonym} (when $s=3$), 
Thus, the synonym with the highest cosine similarity, i.e., \textit{good} itself, is placed on the $[-\Delta, \Delta)$ interval, while the synonym with the lowest cosine similarity, i.e., \textit{admirable}, is placed on the $[-5\Delta, -4\Delta)$ and $[4\Delta, 5\Delta)$ intervals. 
%,  noise interval and  
% as shown in Figure~\ref{fig:synonymcorpus}, 
Figure~\ref{fig:synonymcorpus} shows that \textit{good} is replaced by \textit{excellent} with  probability $\exp(-\epsilon)a(\gamma)$, while by \textit{admirable} with probability $\exp(-4\epsilon)a(\gamma)$-- closer relationship between \textit{good} and \textit{excellent} is captured.

\vspace{-0.05in}
\begin{table}[!h]
  \centering
  \setlength\tabcolsep{3pt}
  \caption{Staircase-based synonym substitutions for \textit{good}}
  \vspace{-0.05in}
    \begin{tabular}{llll}
    \toprule
    \begin{tabular}[c]{@{}l@{}}Synonym\\word\end{tabular} & \begin{tabular}[c]{@{}l@{}}Cosine\\similarity\end{tabular}  & \begin{tabular}[c]{@{}l@{}}Noise interval\\(or step)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Substitution\\probability\end{tabular} \\
    \midrule
    \textit{admirable}  &  $0.223$ & $[-5\Delta,-4\Delta)$ & $\exp({-4\epsilon})a(\gamma), \exp({-5\epsilon})a(\gamma)$  \\
    \textit{excellent}  &  $0.788$ & $[-2\Delta,-\Delta)$ & $\exp({-\epsilon})a(\gamma), \exp({-2\epsilon})a(\gamma)$ \\
    \textit{good} &  $1.000$ & $[-\Delta,\Delta)$ & $a(\gamma), \exp({-\epsilon})a(\gamma)$ \\
    \textit{excellent}  &  $0.788$ & $[\Delta,2\Delta)$ & $\exp({-\epsilon})a(\gamma), \exp({-2\epsilon})a(\gamma)$ \\
    \textit{admirable}  &  $0.223$  & $[4\Delta, 5\Delta)$ & $\exp({-4\epsilon})a(\gamma), \exp({-5\epsilon})a(\gamma)$\\
    \bottomrule
    \end{tabular}
    \vspace{-0.1in}
    %
  \label{tab:synonym}%
\end{table}%


% Figure environment removed





\vspace{-0.1in}
%\subsubsection{Staircase-based Embedding Substitution} 
\subsubsection{Certification for Synonym Substitution}
The embedding transformation $\phi_S$ is defined by substituting each word $x_i$'s embedding $w_i$ with its $a_i$th synonym's $w_i^{a_i}$. 
% $w_i$ with the $a_i$th synonym. 
$a_i$ decides the substitution of $x_i$, e.g., a closer synonym $w_i^{a_i}$ to $x_i$ has a smaller $a_i$. 
% the magnitude of the transformation $\phi_S$. 
We assume $a_i$ follows a Staircase PDF, in which the probability of each synonym being selected is defined as Table~\ref{tab:synonym}. 
%We set the size of the thesaurus as $s\propto 1/\epsilon$. \BW{$s$ is not used here?}
Then, we provide the below robustness certification for the synonym substitution perturbation $\delta_{S}$.
% , denoted as $\delta_{S}$, which indicates the distance between all the words in a text $x$ and their synonym indexes,  %(including the words themselves),  
% by modeling the distance of synonym indexes, 
% i.e., $ \delta_{S}= \{a_1, \cdots, a_n\}$.
% \begin{equation}
%     \delta_{S}= \{a_1, \cdots, a_n\}. %,\quad \|\delta_{S}\|_1=\sum_{i=1}^{n} a_i
% \end{equation}

% The synonyms are positioned symmetrically on the steps based on their cosine similarity to the original embedding. As shown in Table~\ref{tab:synonym} (when $s=3$), the synonym with the highest cosine similarity, i.e., \textit{good} itself, is placed on the $[-\Delta, \Delta)$ interval, while the synonym with the lowest cosine similarity, i.e., \textit{admirable}, is placed on the $[-3\Delta, -2\Delta)$ and $[2\Delta, 3\Delta)$ intervals. \BW{Define the meaning of $\delta_{S}$ in advance?} \xy{have defined in advance.}



% $h: \BR^d\to \CY$ be any deterministic or random function and let 
\begin{theorem} \label{thm:wss_1}
Let $\phi_S: \CW \times \BR^n \to \CW$ be the embedding substituting transformation based on a Staircase distribution $\varepsilon \sim \CS_\gamma^\epsilon(w, \Delta)$ with PDF $f_\gamma^\epsilon(\cdot)$, and let $g_S$ be the smoothed classifier from any deterministic or random function $h$, as in (\ref{eq:smoothg1}). Suppose $y_A, y_B\in \CY$ and $\underline{p_A}, \overline{p_B} \in [0,1]$ satisfy:
%
\begin{equation}
\small
\begin{aligned}
    \BP(h(u\cdot \phi_S(w,\varepsilon))=y_A) \geq \underline{p_A} &\geq \overline{p_B} \geq\\
    \max_{y_B\neq y_A}& \BP(h(u\cdot \phi_S(w,\varepsilon))=y_B) 
\nonumber
\end{aligned}
\end{equation}
then $g_S(u\cdot \phi_S(w, \delta_{S}\cdot \Delta))=y_A$ for all $\|\delta_{S}\|_1 \leq \rad_S$, where 
% $\rad_S$ is called the certified radius that satisfies:
%
\begin{equation} \label{eq:rad_s}
\small
\rad_S = \max \Big \{\frac{1}{2\epsilon}\log({\underline{p_A}}/{\overline{p_B}}), -\frac{1}{\epsilon}\log(1-\underline{p_A}+\overline{p_B}) \Big \}. %
\end{equation}
\end{theorem}

\begin{proof}
Proven in Appendix~\ref{proof:wss_1}.
\end{proof}
% \BW{From the proof, certified radius is irrelevant to $\Delta$?} \xy{Yes, as shown in Figure 3, the perturbation added on $w$ is $\delta_{S} = \{a_1, \cdots, a_n\}\cdot \Delta$. I set the interval size ($\Delta=1$) in the evaluation.}

% \BW{Do you study the impact of the interval size $\Delta$? If not we can set $\Delta=1$ by default?}

% \BW{Here the perturbation $\delta$ is imposed on the ``synonym" indexes, but the proof is imposed on $w$? The perturbation definition is unclear to me.} \xy{I will revise the proof}

% The perturbation of the embedding substituting transformation, characterized by $\delta_{S}$, denotes the distance of synonym substitution between the original $w=\{w_1, \cdots, w_n\}$ and the transformed $w'=\{w_1^{a_1}, \cdots, w_n^{a_n}\}$, i.e., 
% \begin{equation}
%     \delta_{S}= \{a_1, \cdots, a_n\} %,\quad \|\delta_{S}\|_1=\sum_{i=1}^{n} a_i
% \end{equation}
Theorem~\ref{thm:wss_1} states that 
%if the $\ell_1$ norm of $\delta_{S}$ is sufficiently small, then $g_S$ exhibits a constant prediction. This means that 
\emph{any} synonym substitution would not succeed as long as the $\ell_1$ norm of $\delta_{S}$ is smaller than $\rad_S$. 
% in (\ref{eq:rad_s}). 
We can observe that the certified radius $\rad_S$ is large under the following conditions: 1) the noise level $\epsilon$ is low, indicating a larger synonym size; 2) the probability of the top class $y_A$ is high, and those of other classes are low. 

% The higher probability of the top class $y_A$ and the lower probability of the other classes are also general conditions for improving the certified radius of Theorem~\ref{thm:wcr_1}, Theorem~\ref{thm:wi_1} and Theorem~\ref{thm:wd_1}.



% \vspace{5pt}
% % \begin{theorem}[binary case]
% % \label{thm:wss_2}
% \noindent\textbf{Theorem 1 (binary case).} 
% \textit{Let $\phi_S: \CW\times\BR\to \CW$ be the embedding transformation based on Staircase noise $\varepsilon \sim f_\gamma(w)$ and let $g_S$ be defined as the smoothed classifier as in (\ref{eq:smoothg1}). Suppose $y_A\in \CY$ and $\underline{p_A} \in (\frac{1}{2},1]$ satisfy: }
% %
% $$
%     \BP(h(u\cdot \phi_S(w, \varepsilon))=y_A) \geq \underline{p_A}
% $$
% \textit{Then $g_S(u\cdot \phi_S(w, \delta_{S}))=y_A$ for all $\|\delta_{S}\|_1 \leq \rad_S\cdot\Delta$ where }
% $$\rad_S=-\frac{1}{\epsilon}\log (2(1-\underline{p_A}))$$
% % \end{theorem}
% \begin{proof}
% We provide a different proof from the one in Theorem~\ref{thm:wss_1}, see Section~\ref{proof:wss_2} in Appendix.
% \end{proof}

% \BW{Binary case is very straightforward from the multi-class case, i.e., by setting $\overline{p_B} = 1 - \underline{p_A}$? The proof looks like very complicated.}
% \xy{Because I first proved the binary case in a complicated way, but only afterward I realized that there is a simple way to prove Theorem 1. Maybe I can delete the proof of the binary case.} \BW{I agree. Setting $\overline{p_B} = 1 - \underline{p_A}$ directly reaches the binary case.} \xy{thanks, I delete the binary case directly}
% \textbf{Certified Word Corpus}

\vspace{-0.1in}
\subsection{Certified Robustness to Word Reordering}
\vspace{-0.1in}
% Word reordering only changes the position of permutation vector $u_i$ while keeping the embedding matrix $w$ unchanged, which is denoted as
% $$\theta_R(\{u_1, \cdots, u_n\}, z)=\{u_{r_1}, \cdots, u_{r_n}\}$$

\subsubsection{Uniform-based Permutation}
%Fixing the relative importance of each position to the prediction can be challenging due to heterogeneous model structure and datasets. 
%\BW{This claim is inaccurate, as a defender definitely can know the model details?} \xy{I have revised the reason.} 
%To address this, 
We assume that each position of the word is equally important to the prediction, and add a uniform distribution to the permutation matrix ($u$) to model permutation. Specifically, we simulate the uniform distribution by grouping the row vectors of $u$, then randomly reordering vector positions within the groups. For example, given a permutation matrix with $n$ row vectors $\{u_i\}$, we divide all $u_i$ uniformly and randomly into $n/\lambda$ groups with length $\lambda$ each. The row vector $u_i$ can be reordered randomly within the group. In this way, the noise added to each position is $1/\lambda$ (uniform). Also, $\lambda=n$ means randomly shuffling all row vectors of the entire permutation matrix. 
%\BW{I am confused about this. If different positions have different levels of importance, why using uniform distribution?} 
%\xy{Maybe there is something wrong with my expression. The different level here is for evaluation that can use multiple levels of noise (Figure 6).}
The proposed uniform smoothing method provides certification for the permutation perturbation $\delta_{R}$.
% , which is the distance between the positions of $1$ in $u_i$ before and after the permutation and can be expressed as 
% \begin{equation}
%     \delta_{R}=\{r_1-1, \cdots, r_n-n\}.
% \label{eq:perturb_r}
% \end{equation}

\begin{theorem}
\label{thm:wcr_1}
Let $\theta_R: \CU\times \BZ^n \to \CU$ be a permutation based on a uniform distribution $\rho\sim \FU[-\lambda, \lambda]$ 
%\BW{This means uniformly sampling from a set of integers -n to n? $\BR$ should be $\BZ$, the integer space?}\xy{Thank you. I have corrected this error.} 
and $g_R$ be the smoothed classifier from a base classifier $h$, as in (\ref{eq:smoothg2}). Suppose $g_R$ assigns a class $y_A$ to the input $u\!\cdot\! w$, and $\underline{p_A}, \overline{p_B}\in(0,1)$. If 
\small
\begin{align}
\BP(h(\theta_R(u, \rho)\cdot w)=y_A) \geq \underline{p_A} &\geq \overline{p_B} \geq \nonumber\\
\max_{y_B\neq y_A}& \BP(h(\theta_R(u, \rho)\cdot w)=y_B))\nonumber
\end{align}
\normalsize
%
then $g_R(\theta_R(u, \delta_{R})\cdot w)=y_A$ for all permutation perturbations satisfies $\|\delta_{R}\|_1 \leq \rad_R$, where
%\BW{No perturbation $\delta_{R}$ on $g_R$.} \xy{added}
\begin{equation} 
\small
    \rad_R= \lambda(\underline{p_A}-\overline{p_B})
\label{eq:rad_r}
\end{equation}
\end{theorem}

\begin{proof}
Proven in Appendix \ref{proof:wcr_1}.
\end{proof}


Theorem~\ref{thm:wcr_1} states that \emph{any} permutation would not succeed as long as $\delta_{R}< \rad_R$ in Eq.(\ref{eq:rad_r}). We can observe that the certified radius $\rad_R$ is larger when $\lambda$ is higher, which requires more shuffling, or/and $\underline{p_A}$ is larger. The maximum certified radius is $\lambda$, which is the size of a reordering group. Note that both $\underline{p_A}$ and $\lambda$ depend on the noise magnitude. 
%\BW{any parameter relevant to the noise level?}

%\BW{Since $\lambda=n$ shows the largest certified radius, why not just setting $\lambda=n$ directly, meaning shuffling all rows? This also does not involve a new hyperparameters?} \xy{If we directly set $\lambda=n$, can't we change the noise level? I set $\lambda=n, n/2, n/4$ in the evaluation.} \BW{I see. So $\underline{p_A}$ depends on the noise level?} \xy{Yes, $\underline{p_A}$ and $\lambda$ depend on the noise level.}

\vspace{-0.1in}
\subsection{Certified Robustness to  Word Insertion}
\vspace{-0.1in}
% Word insertion can be considered as the combination of permutation $\theta_I$ and embedding transformation $\phi_I$. Inserting a word at $j_i$th ($i\in[m']$) position can be expressed as 
% \begin{equation}
% \begin{aligned}
% \theta_I(\{&u_1, \cdots, u_n\}, z)= \\
% &\{u_1, \cdots, u_{j_1-1}, u_{j_1+1}, \cdots, u_{n}, u_{j_1}, \cdots , u_{j_m'}\} \\
% \phi_I(\{&w_1, \cdots, w_n\}, t)= \\
% &\{w_1, \cdots, w_{j_1-1}, w_{j_1}, \cdots, w_{n-m'}, w_{in}^{1*}, \cdots, w_{in}^{m'*}\} 
% \nonumber
% \end{aligned}
% \end{equation}


Word insertion employs a combination of permutation $\theta_I$ and embedding transformation $\phi_I$. Recall that the only transformation performed on the permutation matrix is to shuffle the position of $u_i$. Hence, we utilize the uniform-based permutation with the noise level set as the number of words ($n$), i.e., $\theta_I(u, \rho)= \theta_R(u, \rho)$, where $\rho \sim \FU[-n, n]$. 
% and the permutation perturbation follows the definition in Eq.(\ref{eq:perturb_r}) ($\delta_{R}\coloneqq \delta_{R}$). 
On the other hand, embedding insertion involves replacing $w_{n-m'+i}$ with an unrestricted inserted embedding $w_{in}^{j}$, which sets it apart from synonym substitution. To address the challenge of unrestricted embedding insertion, we propose a Gaussian-based smoothing method for the certification.
% , i.e., the embedding distance between the original text and the inserted text as follows 
% \begin{equation}
%     \delta_{I}=\{ w_{in}^{1}-w_{n-m'+1}, \cdots, w_{in}^{m'}-w_{n} \}.
% \end{equation}


% The definition of $\delta_{R}$ is the same as $\delta_{R}$ in (\ref{eq:perturb_r}). And we can obtain $\|\delta_{R}\|_1 = 2\times(\min(m, n)-j)$, $\|\delta_{R}\|_1 = \sum_{i=1}^{m'-1} j_{m'}-j_i$ according to the following analysis. 
% In reality, when $m\leq n$, there is no $w_{\mathtt{<pad>}}$ at the end of $w$, then $u_j$ moves to the $n$th position; when $n>m$, there are $n-m$ $w_{\mathtt{<pad>}}$ at the end of $w$, $u_j$ just need to move to the $(m+1)$th position.


% This method can provide provable robustness as long as both $\delta_{R}$ and $\delta_{I}$ satisfy the certified condition, and we provide 

\vspace{-0.1in}
\subsubsection{Gaussian-based Embedding Insertion} 
We consider the embedding matrix as a whole, the length of which is $n\times d$, where $d$ is the dimension of each embedding vector. We add Gaussian noise to the embedding matrix directly, similar to adding independent identically distributed Gaussian noise to each pixel of an image. We invoke Theorem 1 in \cite{cohen2019certified} as

\begin{theorem} %[Cohen'19\cite{cohen2019certified}]
\label{thm:wi_1}
Let $\phi_I:\CW\times \BR^{n\times d} \to \CW$ be the embedding insertion transformation based on Gaussian noise $\varepsilon \sim \CN(0,\sigma^2 I)$ and $\theta_I$ be the perturbation as same as $\theta_R$ based on a uniform distribution $\rho\sim \FU[-n, n]$. Let $g_I$ be the smoothed classifier from a base classifier $h$ as in (\ref{eq:smoothg}), and suppose $y_A,y_B\in \CY$ and $\underline{p_A}, \overline{p_B} \in [0,1]$ satisfy:
%
\begin{equation}
\small
\begin{aligned}
\BP(h(\theta_I(u,\rho)\cdot\phi_I(w,\varepsilon)))=&\ y_A) \geq \underline{p_A} \geq \overline{p_B} \geq \\
\max_{y_B\neq y_A}& \BP(h(\theta_I(u,\rho)\cdot\phi_I(w,\varepsilon))=y_B))
\nonumber
\end{aligned}
\end{equation}
%
then $g_I(\theta_I(u,\delta_{R})\cdot \phi_I(w,\delta_{I}))=y_A$ for all $\|\delta_{R}\|_1 < \rad_R$ as in Eq.(\ref{eq:rad_r}) and $\|\delta_{I}\|_2 < \rad_I$ where 
\begin{equation}\small
    \rad_I =\frac{\sigma}{2}(\Phi^{-1}(\underline{p_A})-\Phi^{-1}(\overline{p_B}))
\label{eq:rad_i}
\end{equation}
\end{theorem}

% \begin{proof}
% The proof is the same as that in~\cite{cohen2019certified}.
% \end{proof}


Theorem~\ref{thm:wi_1} states that $g_I$ can defend against word insertion transformation as long as the conditions about $\delta_{R}$ and $\delta_{I}$ are satisfied simultaneously. We observe that the certified radius $\rad_I$ is large when the noise level $\sigma$ is high. 


\vspace{-0.1in}
\subsubsection{Combination of Two Perturbations} 
The certified radii of Eq.(\ref{eq:rad_r}) and Eq.(\ref{eq:rad_i}) ensure the robustness of the smoothed classifier against permutation $\theta_I$ or embedding insertion $\phi_I$, respectively. However, the word insertion is a combination of $\theta_I$ and $\phi_I$. Next, we propose Theorem~\ref{thm:combination} to provide certified robustness for the combination of them. 

%\BW{I do not quite understand the proof for Theorem 4...}
%\xy{I have made some revisions to Theorem 4 and its proof. Do you think my writing is clearer now?}

\begin{theorem} \label{thm:combination}
If a smoothed classifier $g$ is certified robust to permutation perturbation $\delta_u$ as defined in Eq.(\ref{eq:u_delta}), and to embedding permutation $\delta_w$ as defined in Eq.(\ref{eq:w_delta}), then 
%we can conclude that 
$g$ can provide certified robustness to the combination of perturbations $\delta_u$ and $\delta_w$ as defined in Eq.(\ref{eq:uw_delta}) assuming $\theta(u, \rho)$ is uniformly distributed in the permutation space.
%Formally, 

\vspace{-0.15in}

\small
\begin{align}
    \label{eq:u_delta} 
    & \forall \delta_u, \delta_w, 
    g(\theta(u\!+\!\delta_u,\rho)\cdot\! \phi(w, \varepsilon))\! =\! g(\theta(u,\rho)\!\cdot\! \phi(w, \varepsilon)) \\
    \label{eq:w_delta} 
    & \qquad \, \, \, \& \, 
    g(\theta(u,\rho)\!\cdot\! \phi(w\!+\!\delta_w, \varepsilon))\! =\! g(\theta(u,\rho)\!\cdot\! \phi(w, \varepsilon)) \\
    \label{eq:uw_delta} 
    & \implies 
    g(\theta(u\!+\!\delta_u,\rho)\!\cdot\! \phi(w\!+\!\delta_w, \varepsilon))\! =\! g(\theta(u,\rho)\!\cdot\! \phi(w, \varepsilon))
\end{align}
\normalsize

\end{theorem}

\begin{proof}
    Proven in Appendix~\ref{proof:combination}.
\end{proof}


% \noindent \textbf{Remark:} Permutation noise $\rho$ and embedding noise $\varepsilon$ are applied to $u$ and $w$ during training, and then certified radii $\rad_R$ and $\rad_I$ are calculated in two spaces using Eq.(\ref{eq:rad_r}) and Eq.(\ref{eq:rad_i}). According to 

Thus, when the certified radii $\rad_R$ and $\rad_I$ are derived w.r.t. $\delta_u$ and $\delta_w$ in two spaces using Eq.(\ref{eq:rad_r}) and Eq.(\ref{eq:rad_i}), respectively, then $\theta(u, \delta_u)\cdot \phi(w, \delta_w)$ is within the certified region of $g$ as long as both $\delta_u<\rad_R$ and $\delta_w<\rad_I$ hold. 

\vspace{-0.1in}
\subsection{Certified Robustness to Word Deletion}
\vspace{-0.1in}
% Word insertion also combines permutation $\theta_D$ and embedding transformation $\phi_D$. Deleting a word at the $j_i$th ($i\in[m']$) position can be expressed as
% \begin{equation}
% \begin{aligned}
% \theta_D(u,z)=&\{u_1, \cdots, u_{j_1-1}, {u_{n-m'+i}}, u_{j_1}, \cdots, u_{n-m'}\} \\ %\{u_1, \cdots, u_n\}
% \phi_D(w,t)=&\{w_1, \cdots, w_{j_1-1}, {w_{\texttt{pad}}^1}, w_{j_1+1}, \cdots, w_n\} %\{w_1, \cdots, w_n\}
% \nonumber
% \end{aligned}
% \end{equation}

Similarly, a completely uniform shuffling of the orders is performed on permutation, denoted as $\theta_D(u, \rho) = \theta_R(u, \rho)$, where $\rho$ is drawn from $\FU[-n, n]$. Recall that embedding deletion is modeled as a change in the embedding state from $b_i=1$ to $b_i=0$. To certify against the $\ell_0$-norm of embedding deletion perturbation (as the number of deleted words), we propose a Bernoulli-based smoothing method.

% \subsubsection{Bernoulli Distribution} 
\vspace{-0.1in}
\subsubsection{Bernoulli-based Embedding Deletion} 
The transition of the embedding state ($a_i$) can be considered to follow the Bernoulli distribution $\FB(n,p)$, where $n$ is the total number of words in the text. Each word embedding can be transformed to $w_{\texttt{<pad>}}$ with probability $p$ and maintained with $1-p$, i.e, $\BP(b_i=1\to b_i=0)=p$ and $\BP(b_i=1\to b_i=1)=1-p$. 
\emph{The $w_{\texttt{<pad>}}$ cannot be transformed into a word embedding since we cannot recover the deleted words when they are removed from the text.} It can be denoted as $\BP(b_i=0\to b_i=1)=0$.



\begin{theorem} %[Cohen'19\cite{cohen2019certified}]
\label{thm:wd_1}
Let $\phi_D:\CW\times \{0,1\}^n \to \CW$ be the embedding deletion transformation based on Bernoulli distribution $\varepsilon \sim \FB(n,p)$ and $\theta_D$ be the perturbation same as $\theta_R$ based on a uniform distribution $\rho\sim \FU[-n,n]$. Let $g_D$ be the smoothed classifier from a base classifier $h$, as in (\ref{eq:smoothg}) and suppose $y_A,y_B\in \CY$ and $\underline{p_A}, \overline{p_B} \in [0,1]$ satisfy:
%
\begin{equation}\small
\begin{aligned}
\BP(h(\theta_D(u,\rho)\cdot\phi_D(w,\varepsilon)))& =y_A) \geq \underline{p_A} \geq \overline{p_B} \geq \\
\max_{y_B\neq y_A}& \BP(h(\theta_D(u,\rho)\cdot\phi_D(w,\varepsilon))=y_B))
\nonumber
\end{aligned}
\end{equation}
%
then $g_D(\theta_D(u,\delta_{R})\cdot \phi_D(w,\delta_{D}))=y_A$ for all $\|\delta_{R}\|_1 < \rad_R$ as in Eq.(\ref{eq:rad_r}) and $\|\delta_{D}\|_0 < \rad_D$, where %$\rad_D$ is the solution to the following optimization problem:
%
\begin{equation}
\small
\begin{aligned}
    & \rad_D= \argmax \delta, \\
    & \textrm{\ s.t.\ } \tbinom{z_{\max}}{\delta} \leq \underline{p_A}/\overline{p_B}, \\
    & \qquad z_{\max} \!=\! \argmax z, \textrm{\ s.t.\ } \tbinom{n}{z} p^z (1-p)^{(n-z)} \!\leq\! \overline{p_B}.
\end{aligned}
\end{equation}
\end{theorem}

\begin{proof}
Proven in Appendix~\ref{proof:wd_1}.
\end{proof}


Theorem~\ref{thm:wd_1} states that $g_D$ can defend against any word deletion transformation as long as the conditions about $\delta_{R}$ and $\delta_{D}$ are met. We observe that the certified radius $\rad_D$ is large when the total number of words $n$ is small.  % and $p_A$ are large 

% The same as Word Substitution Perturbation, Each word has the probability of $\exp({−k\epsilon})a(\gamma)$ to be deleted and the probability of $\exp({−(k+1)\epsilon}) a(\gamma)$ to remain unchanged. 


\vspace{-0.1in}
\subsection{Universality of Certification for Insertion}
\vspace{-0.1in}

All four adversarial operations are essentially transformations of the embedding vector. Hence, our certification for the word insertion,  a combination of uniform-based permutation ($\theta_I$) and Gaussian-based embedding transformation ($\phi_I$), is applicable to all the four operations. The synonym substitution operation only employs the embedding transformation ($\theta_I$), with the embedding perturbation being the sum of the embedding distances of the replaced synonyms. Word reordering is a simple version of word insertion, using only permutation ($\theta_I$). Word deletion, on the other hand, uses both permutation ($\theta_I$) and embedding transformation ($\theta_I$), with its embedding perturbation being the sum of the embedding distances of the deleted words.
