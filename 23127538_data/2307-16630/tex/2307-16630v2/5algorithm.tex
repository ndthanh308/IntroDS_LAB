\vspace{-0.05in}
\section{Practical Algorithms}
\vspace{-0.1in}

\subsection{Training}
\vspace{-0.1in}
Given the word operation $T\in\{S, R, I, D\}$, we aim to generate a certified model against the corresponding word-level attack. As described in Algorithm~\ref{alg:train}, we first generate a set of embedding matrices $w$ with the pre-trained embedding layer $L_{emb}$. The permutation matrix $u$ is an identity matrix with the same length as $w$ (line~1). Then, we perform permutation and embedding transformations on $u$ and $w$ to generate the dataset $\CD_T$. Finally, we update the model with the training dataset $\CD_T$ and obtain the model $h$.

%\BW{Inject noises in each epoch not just once?}\xy{I only inject noise once during training. I was wondering If there might be any errors in the sentences above?} 
%\BW{Okay, I see. No errors. I thought injecting noise per epoch, as this is what I did before.}

\begin{algorithm}[!h]\small
\caption{Training algorithm} \label{alg:train}
\begin{algorithmic}[1]
\Require Training dataset $\CD=\{(x,y)_i\}$, operation $T$, pre-trained embedding layer $L_{emb}$, permutation $\theta_T$ with noise $\rho$, embedding transformation $\phi_T$ with noise $\varepsilon$
% \Ensure $y = x^N$
\State $u\cdot w\gets L_{emb}(x)$ \Comment{$u$ is $w$'s permutation matrix}
\State $\CD_T=\{(\theta_T(u,\rho)\cdot \phi_T(w,\varepsilon), y)_i\}$
\State $h \gets$ Train the classification model with $\CD_T$
\State \Return Classification model $h$
\end{algorithmic}
\end{algorithm}

\vspace{-0.15in}

\subsubsection{Enhanced Training Toolkit for Word Insertions} \label{sec:enhance}

High-level Gaussian noise leads to distortion in the embedding space and results in barely model convergence. To address this issue, we develop a toolkit with three methods for the three steps in the training (see Figure~\ref{fig:toolkit}). The toolkit is mainly used to improve the certified accuracy against word insertions, and it is also applicable to other operations. 

% Figure environment removed
% The training toolkit can enhance the accuracy of a model (i.e., classifier) by either replacing or supplementing a part of the original training process.

\vspace{0.03in}
\noindent\ding{172} \textbf{Optimized Gaussian Noise (OGN).} Inspired by the Anisotropic-RS~\cite{hong2022certified}, an appropriate mean value of Gaussian noise can improve the certified accuracy. We analyze the embedding vectors of all words and observe that each element in the embedding vectors approximates a Gaussian distribution with a nonzero mean, as illustrated in Figure~\ref{fig:emb_values}. Consequently, we can enhance the certified accuracy by modifying the Gaussian noise of each dimension $\CN(0, \sigma I^2) \to \CN(\mu_i, \sigma I^2)$, where $\mu_i, \ i\in[1,d]$ denotes the average of the original embedding space of each dimension. 
% \BW{Unclear about $\mu$. Is $\mu$ a vector of a scalar? For context, each dimension $i$ has its mean $\mu_i$} \xy{I counted the entire thesaurus and set up $300$ $\mu_i, i\in[300]$. I did not set specific $\mu_i$ for each context.}

% ~\cite{wang2021natural}

\vspace{0.03in}
\noindent\ding{173} \textbf{Embedding Space Reconstruction (ESR).} To mitigate the disturbance of the embedding space, we introduce an encoder-decoder architecture to reconstruct the clean embedding space. The encoder-decoder can also be viewed as sanitizing the additive noise. This method can effectively improve accuracy in small-dimension embedding space, such as with the $300$-dimension GloVe embedding and LSTM classifier, resulting in a $10\%$ increase in average accuracy. 
% For large embedding space, a guiding loss should be added to guide the embedding space reconstruction. We refer to the Information Bottleneck-based strategy~\cite{tian2021farewell} and introduce an additional loss of the clean embedding vector to guide the training, i.e., $\textrm{loss}= \ell(h(w+\varepsilon), y)+\alpha\cdot \ell(h(w),y)$, where $\alpha$ is the balanced weight. 

% However, for large pre-trained models, such as BERT, we do not have much data to pre-train the added encoder-decoder structure, so this method is not applicable.

\vspace{0.03in}
\noindent\ding{174} \textbf{Pre-trained Large Model (PLM).} Fine-tuning from a pre-trained model is a typical training approach for large models. When applying high-level Gaussian noise to a large model, we can fine-tune it on a pre-trained large model trained with small Gaussian noise (e.g., $\sigma=0.1$). For instance, when adding Gaussian noise of $\sigma = 1.5$ to the IMDB dataset and using BERT as the classifier, this approach can substantially enhance model accuracy from $50\%$ to $84\%$.


\vspace{-0.1in}
\subsection{Certified Inference}  % Prediction and Certification
\vspace{-0.1in}

The certified inference algorithm is identical to the classical randomized smoothing in \cite{cohen2019certified}. We first obtain the embedding $u\cdot w$ of the test sample $x$ by the pre-trained embedding layer $L_{emb}$. Then, we utilize $\theta_T(u, \rho)\cdot \phi_T(w, \varepsilon)$ to draw $N$ samples by the $T$ transformation. Finally, we certify robustness on $N$ samples and output the robust prediction.
Details are presented in Algorithm~\ref{alg:certify} in Appendix~\ref{appendix:alg}.
%  prediction and Algorithms~\ref{alg:predict}
