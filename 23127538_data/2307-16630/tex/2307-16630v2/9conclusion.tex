\vspace{-0.09in}
\section{Conclusion}
\vspace{-0.1in}

In this paper, we present a generalized framework Text-CRS for certifying model robustness against word-level adversarial attacks. Specifically, we propose four randomized smoothing methods that utilize appropriate noise to align with four fundamental adversarial operations, including one that is applicable to all operations. In addition, we propose an enhanced training toolkit to further improve the certified accuracy. We conduct extensive evaluations of our methods, considering both adversarial operations and real-world adversarial attacks on diverse datasets and models. The results demonstrate that our method outperforms SOTA methods in their settings (substitution) and establishes new benchmarks of certified accuracy for the other three operations. 


\vspace{-0.09in}
\section*{Acknowledgments}
\vspace{-0.1in}

This work is partially supported by the National Key R\&D Program of China (2020AAA0107700), the National Natural Science Foundation of China (62172359), the Hangzhou Leading Innovation and Entrepreneurship Team (TD2020003), the National Science Foundation grants (CNS-2308730, CNS-2302689, CNS-2319277, CMMI-2326341, CNS-2241713, and ECCS-2216926), and the Cisco Research Award. The authors would like to thank the anonymous reviewers for their constructive comments.


% This work is supported by National Key R&D Program of China (Grant No. 2020AAA0107700), National Natural Science Foundation of China (No. 62172359, 62032021, 61972348, 62102354, 62227805, 62072398, 61772236), Funding for Postdoctoral Scientific Research Projects in Zhejiang Province (ZJ2021139), Fundamental Research Funds for the Central Universities (No. 2021FZZX001-27), Research Institute of Cyberspace Governance in Zhejiang University, National Key Laboratory of Science and Technology on Information System Security (6142111210301), and State Key Laboratory of Mathematical Engineering and Advanced Computing.