\vspace{-0.05in}
\section{The Text-CRS Framework}
\vspace{-0.1in}

This section introduces Text-CRS, a novel certification framework that offers provable robustness against adversarial word-level operations. We first outline the new challenges in designing such certified robustness. Next, we 
%provide an overview of the frequently used notations and 
formally define the permutation and embedding transformations that correspond to each adversarial operation. We then define the perturbation that Text-CRS can certify for each permutation and embedding transformation. Finally, we conclude with a summary of our framework and its defense goals.


\vspace{-0.1in}
\subsection{New Challenges in Certified Defense Design} 
\vspace{-0.1in}

Previous studies on certified defenses in text classification, including SAFER~\cite{ye2020safer} and CISS~\cite{zhao2022certified}, only provide robustness guarantees \emph{against substitution operations}. 
% under an unrealistic assumption
Certified defenses against other word-level operations, such as word reordering, insertion, and deletion, are 
% remain largely 
unexplored. 
We list below the weaknesses of existing defenses as well as several technical challenges: 
% due to the following main challenges. %should be addressed.

% \begin{enumerate}[(C1)]
\begin{itemize}[leftmargin=*]

\setlength\itemsep{0.3em}

     % \item 
     % % The assumption of a uniform substitution distribution among synonyms is impractical, as different synonyms exhibit varying substitution probabilities. However, prior studies on d a uniform distribution among synonyms, resulting in low certified accuracy. 
      
     % Existing certified defenses against synonym substitutions assume that all synonyms have the same probability to substitute a target word. However, this assumption is unrealistic, as certain synonyms are more frequent/similar than others, and hence should have larger probabilities to do so. Such an assumption makes the existing works yield relatively low certified radius/accuracy. 
     % % A follow-up research question is how can we model the different p 

    \item \textbf{Measuring the perturbation and certified radius}. Words are unstructured strings and there is no numerical relationship among discrete words. This makes it challenging to measure the $\ell_1$ and $\ell_2$ distance between words, as well as the perturbation distance between the original and adversarial text (while deriving the certified radius).
    
    \item \textbf{Customized noise distribution for randomized smoothing against different word-level attacks}.
           The assumption of a uniform substitution distribution among synonyms is unrealistic, as different synonyms exhibit varying substitution probabilities. However, previous works on the certified robustness against word substitution almost assume a uniform distribution within the set of synonyms. Such an assumption makes these works yield relatively low certified accuracy (see Section~\ref{sec:results}). Hence, we need to 
           construct customized noise distributions best suited for the certification against the synonyms substitution attack as well as the other three word-level attacks. 
                      
           
         \item \textbf{Inaccurate representation of distance}. The absolute distance between operation sequences is typically high for word reordering, insertion, and deletion operations. Although studies, such as TSS~\cite{li2021tss} and DeformRS~\cite{alfarra2022deformrs}, have investigated the pixel coordinate transformation in the image domain, the word reordering transformation has not been studied in the NLP domain. Moreover, word insertion and deletion are unique transformations specific to NLP which are not applicable in the image domain. 
         
      % \item Although both synonym substitution and word insertion perturb a text, the space to insert words (e.g., 65,713 in Glove) is often much larger than to substitute words (usually at most 1,000). Hence, providing robustness guarantees for arbitrary word insertion is more challenging. 

      %     \item Effective word-level attacks could cause a wide range of perturbations to a text. For instance, a word reordering operation changes two words' positions, and a word insertion operation introduces a new word in any position. To the best of our knowledge, no certified defense method provides certifiable robustness bounds against these different word-level perturbations. 
     
    % \item Words are unstructured strings with no numerical relationship among discrete words. It is challenging to measure the $\ell_1$ and $\ell_2$ distance between two words and the perturbation distance between the original and adversarial texts. 
    % \BW{Is this really a challenge?} \xy{Thanks, we can delete it.}
   


    % providing certified guarantees for word insertion is more challenging. This is because the number of inserted words (e.g., 65,713 in Glove) can be much larger than that of synonyms (usually at most 1,000). It makes defending against the arbitrary insertion of new words more challenging than defending against the substitution of synonyms.
   
    % \item The absolute distances between adversarial and clean texts are typically high due to different word-level operations, which may greatly exceed the classic certification robustness bound. Previous image studies, such as TSS~\cite{li2021tss} and DeformRS~\cite{alfarra2022deformrs}, have investigated pixel coordinate transformations; however, word coordinate transformations have not been studied yet. Moreover, word insertion and deletion are NLP-specific transformations that cannot be applied with the methods in the image domain. 
    
    % The length of each sequence $x$ in the input space $\CX$ is different. Therefore, it is difficult to define the noise distribution each word is subjected to when doing a transformation (e.g., word reordering) on the whole sentence. In contrast, the data dimensions are all fixed in the embedding space so that the noise intensity can be easily defined.
    
% \end{enumerate}
\end{itemize}

To address these challenges, we employ the 
% numerical data structure, i.e., 
numerical word embedding matrix\cite{pennington2014glove, devlin-etal-2019-bert} as inputs to our model instead of the word sequence. We can then use embedding matrices to measure the $\ell_1$ and $\ell_2$ distance between word sequences. We also introduce a permutation space to solve the problem of high absolute distance. 
Moreover, we design a customized noise distribution for randomized smoothing w.r.t. each attack and derive the corresponding certified radius, shown in Theorems in Section~\ref{sec:theorem}. 
% also 
% caused by modifying word permutations. 
% Furthermore, in Section~\ref{sec:theorem}, we propose four randomized smoothing theorems that provide certified robustness against four word-level operations.


%\BW{Is it possible the universally certify the robustness against all the four attacks?} \xy{Yes. Theorem 3 can certify the robustness against all attacks, and we have evaluated the performance in Table~5. The performance of Theorem 3 is well but worse than the other theorems' performances.}


\vspace{-0.1in}
\subsection{Permutation and Embedding Transformation} \label{sec:notation}
\vspace{-0.1in}

\begin{table}[]
% \setlength\tabcolsep{3pt}
\centering
\caption{Frequently Used Notations}
\vspace{-0.05in}
% \resizebox{\linewidth}{!}{
% \begin{threeparttable}  
\begin{tabular}{@{}ll@{}}
\toprule
Term      & Description               \\
\midrule
$\CX$ & Input text space \\
$\CW\subseteq \BR^{n\times d}$ & Embedding space \\
$\CU\subseteq \BR^{n\times n}$ & Permutation space \\
$\CU\cdot \CW \subseteq \BR^{n\times d} $ & Embedding space after applying permutation \\ 
$\CY$ & Label space \\ 
$\theta(u, r): \CU \times \CR$ & Permutation with parameter $r$ on $u$ \\ %: \CU\times \CR \to \CU
$\phi(w, t): \CW \times \CT$ & Embedding transformation with parameter $t$ on $w$ \\ %: \CW\times \CT \to \CW
$L_{emb}:\CX\to \CW$ & The pre-trained embedding layer \\
$h:\CU\cdot \CW\to \CY$ & Classification model (i.e., base classifier) \\
%$m$ & Length of the $x$, varying among samples  \\
$n$ & Constant maximum length of input sequence \\
$d$ & Dimension of each embedding vector \\
$\delta$ & Perturbations of permutation or embedding space \\
% $S$ & Set of parameters for permutation or embedding \\
\bottomrule
\end{tabular}
% \begin{tablenotes}
%     \item{(E)-Effectiveness; (S)-Stability; (R)-Robustness}
% \end{tablenotes}
% \end{threeparttable}
% }
\label{tab:notation}
\vspace{-4mm}
\end{table}


\subsubsection{Notations}
We denote the space of input text as $\CX $, the space of corresponding embedding as $\CW\subseteq \BR^{n\times d}$ (where $n$ is the constant maximum length of each input sequence and $d$ is the dimension of each embedding vector), and the space of output as $\CY=\{1, \cdots C\}$ where $C$ is the number of classes. We denote the space of embedding permutations as $\CU \subseteq \BR^{n\times n}$. For instance, given a word sequence $x$, its embedding matrix is $w=\{w_1, \cdots , w_n\}$, its permutation matrix is $u=\{u_1, \cdots, u_n\}$, and the input to the classification model will be $u\cdot w$. The position of $w_i$ is denoted by $u_i=[0, \cdots, 0, 1, 0, \cdots, 0]$, a standard basis vector represented as a row vector of length $n$ with a value of $1$ in the $i$th position and a value of $0$ in all other positions.
% \[
%     x_{z_0} = 0, \cdots, 0 \underbrace{1}_{n-z_0}, 0, \cdots, 0
% \]
%
% \begin{equation}
% u\cdot w=
% \begin{bmatrix}
%  u_1\\  u_2\\ \vdots \\ u_n
% \end{bmatrix}
% \cdot 
% \begin{bmatrix}
%  w_1\\  w_2\\ \vdots \\ w_n
% \end{bmatrix}
% =
% \begin{bmatrix}
%   1&  0&  \cdots& 0\\
%   0&  1&  \cdots& 0\\
%   \vdots & \vdots & \ddots& \vdots\\
%   0&  0& \cdots& 1
% \end{bmatrix} 
% \cdot 
% \begin{bmatrix}
%  w_1\\  w_2\\ \vdots \\ w_n
% \end{bmatrix}
% = 
% \begin{bmatrix}
%  w_1\\  w_2\\ \vdots \\ w_n
% \end{bmatrix}
% \nonumber
% \end{equation}
%

We model the \emph{permutation transformation} as a deterministic function $\theta: \CU \times \CR \to \CU$, where the permutation matrix $u\in \CU$ is permuted by a $\CR$-valued parameter $r$. Vector $u_i$ is replaced with $u_{r}$ by applying $r$, and then the word embedding ($w_i$) is moved from position $i$ to $r$. 
% The only transformation for the permutation matrix $u$ is to swap the positions of $u_i$, which is equivalent to reordering the words in the sequence. 
Moreover, we model the \emph{embedding transformation} as a deterministic function $\phi: \CW\times \CT \to \CW$, where the original embedding $w\in \CW$ is transformed by a $\CT$-valued  parameter $t$. Based on such transformation, we can define all the operations in Section~\ref{sec:threatmodel}. For example, $\theta_I(u, r)\cdot \phi_I(w, t)$ represents the word insertion on the original input $u\cdot w$ with a permutation parameter $r$ and an embedding parameter $t$. Here, we denote $\cdot$ as applying the permutation $u$ to the embedding $w$, and $\times$ as the operations of the parameters applied to the permutation or the embedding matrices.
% We define different types of permutation and embedding transformations corresponding to the four word-level operations (i.e., synonym substitution, reordering, insertion, and deletion) and explain them in Section ~\ref{sec:emb_transform}. 

For simplicity of notations, we denote the classification model as $h: \CU\cdot \CW \to \CY$. Then, we adopt the pre-trained embedding layer ($L_{emb}$) for the text classification task, freeze its parameters, and only update parameters in the classification model. Essentially, the input space of the model ($\CU\cdot \CW \subseteq \BR^{n\times d}$) is the same as $\CW$, and the training process of $h$ is identical to that of a classical text classification model.
Table~\ref{tab:notation} shows our frequently used notations.
% \BW{Here we use $\theta_I(u, r)$, but later $\theta_I(u)$. Make them consistent?} \xy{done}

%\subsubsection{Word-level Operations} \label{sec:emb_transform}

%\YH{first introducing notations for permutation and embedding transformation, and then map them to the word-level operations. I moved the framework to the last part of this section}
%\xy{If we move Figure 2 and Section 3.3.3 to the end, is this less helpful for illustration? Maybe Figure 2 and Section 3.3.3 can just be deleted?}

\vspace{-0.1in}
\subsubsection{Permutation and Embedding Transformation} \label{sec:transform_detail}
%The four word-level operations mentioned in Section ~\ref{sec:threatmodel} can all be converted to the corresponding 
Given the above permutation and embedding transformations, synonym substitution and word reordering are single transformations while word insertion and deletion are composite transformations. Our transformations of the input tuple $(u=\{u_1, \cdots, u_n\}, w=\{w_1, \cdots, w_n\})$ are further represented as follows (no change to the sizes of $u$ and $w$).

%\noindent \BW{In Threat model, text length is $m$, but here it is $n$?} \xy{Because during training, the embedding layer pads each text to a fixed length $n$. The original text length $m$ is not fixed.}


\vspace{0.03in}

% \BW{I also have a general question here. Should we indeed need two parameters to represent each transformation $\theta(u,z)$ and $\phi(w,t)$? I checked the proof and looks like it may be unnecessary, as using $\theta(u)$ and $\phi(w)$ is fine? OR should we instantiate each $z$ and $t$ for each operation, as otherwise the two parameters are isolated. For instance, for Synonym Substitution, $z=\textrm{null}$ and $t=\{a^1, a^2, \cdots\}$. Also I suggest $z$ and $t$ correspond to the notations $a_i$ and $r_i$. How about changing $t$ to $a$ and $z$ to $r$, but not knowing whether are used in other places?}. 
% \xy{

% \noindent\ding{172} I think we need to preserve $r, t$ to control the noise level. Because in Definition 1 and Theorem 1-4 in Section  4, we need the noise $\rho, \varepsilon$ added on $u,w$ to follow some distribution. I mentioned $r,t$ here so I can replace $r,t$ with $\rho, \varepsilon$. 
% If I use $\theta(u)$ and $\phi(w)$, can I add $\rho$ and $\varepsilon$ directly to Definition 1 and Theorem?

% \noindent\ding{173} I have instantiate each $r$ and $t$.

% \noindent\ding{174} I have changed $z$ to $r$, and preserves $t$, because $t$ represents three transformations ($a, w, j$) in (substitution, insertion and deletion).
% }

% \BW{We can have a discussion if needed.}


\noindent\textbf{Synonym Substitution}
replaces the original word's embedding vector with the synonym's embedding vector. 

\vspace{-0.05in}

\begin{equation}
\small
\begin{aligned}
    (\theta_S(u,\textrm{null}), \phi_S(w,& \{a_1, \cdots, a_n\})) \\
    \qquad = (u, {w'})=& (u, \{w_1^{a_1}, \cdots, w_n^{a_n}\})
\nonumber
\end{aligned}
\end{equation}
%

\vspace{-0.05in}


where $w_j^{a_j}$ ($a_j$ are nonnegative integers) is the embedding vector of the $a_j$th synonym of the original embedding $w_j$, and $a_j=0$ indicates % the embedding vector 
%
$w_j$ itself: $w_i^0=w_i$. The permutation $\theta_S(u,\textrm{null})$ does not modify any entries of the permutation matrix $u$.


\vspace{0.03in}

\noindent\textbf{Word Reordering} does not modify the embedding vector but modifies the permutation matrix.

\vspace{-0.05in}

\begin{equation}
\small
\begin{aligned}
    (\theta_R(u, \{r_1, \cdots, r_n\}),&\ \phi_R(w,\textrm{null})) \\
     \qquad = (u', {w}) =& (\{{u_{r_1}}, \cdots, {u_{r_n}}\}, w)
\nonumber
\end{aligned}
\end{equation}
%

\vspace{-0.05in}

where $\{r_1, \cdots, r_n\}$ is the reordered list of $\{1, \cdots, n\}$. The transformation $\phi_R(w,\textrm{null})$ does not modify the elements of the embedding matrix $w$.


\vspace{0.03in}

\noindent\textbf{Word Insertion} first inserts $m'$ embedding vectors of the specified words at the specified positions ($j_1, \cdots, j_{m'}$). Then, it removes the last $m'$ embedding vectors to maintain the constant length $n$ of the text. (see Figure~\ref{fig:insert_delete}) %to obtain $(u'_0,w'_0)$. 

\vspace{-0.15in}

\begin{equation}
\small
\begin{aligned}
    &(\theta_I(u, \{r_1, \cdots, r_n\})), \phi_I(w,\{w_\mathrm{In}^{1}, \cdots, w_\mathrm{In}^{m'}\})) \\
    =&(u'_0, w'_0)
    =\!(\{u_1, \cdots, u_{j_1-1}, u_{j_1}, u_{j_1+1}, \cdots, u_{n}\}, \\
    &\qquad \quad \{w_1, \cdots, w_{j_1-1}, w_\mathrm{In}^{1}, w_{j_1}, \cdots, w_{n-m'}\}) \\
    =&(u', w')
    =\!(\{u_1, \cdots, u_{j_1-1}, u_{j_1+1}, \cdots, u_{n}, u_{j_1}, \cdots u_{j_{m'}}\}, \\
    &\qquad \quad \{w_1, \cdots, w_{j_1-1}, w_{j_1}, \cdots, w_{n-m'}, w_\mathrm{In}^{1}, \cdots, w_\mathrm{In}^{m'}\})
\nonumber
\end{aligned}
\end{equation}

%\vspace{-0.05in}

where $w_\mathrm{In}^{i}$ is the embedding vector of the inserted word at position $j_i$, $i\in [1,m']$. To minimize the distance between $w$ and $w'_0$, $w_\mathrm{In}^{i}$ and its corresponding position vector $u_{j_i}$ are shifted to the end of the sequence to obtain $(u', w')$.
% The $\ell_0$ distance between $u$ and $u'$ is $2(n-j)$. The $\ell_2$ distance between $w$ and $w'$ is $\delta_\mathrm{In}= \|w_\mathrm{In}-w_n\|_2$.


\vspace{-2mm}
% Figure environment removed


\noindent\textbf{Word Deletion}
first replaces $m'$ embedding vectors with all-zero % embedding 
vectors (of \texttt{<pad>}) at position $j_1, \cdots, j_{m'}$. Then it moves the positions (i.e., permutation vector) of these all-zero vectors to the end of the sequence. (see Figure~\ref{fig:insert_delete})  

\vspace{-0.1in}

\begin{equation}
\small
\begin{aligned}
    &(\theta_D(u,\{r_1, \cdots, r_n\})), \phi_D(w,\{j_1, \cdots, j_{m'}\})) \\
    % =&(u'_0=\{u_1, \cdots, u_{j_1-1}, u_{j_1}, \cdots, u_{n-m'}, {u_{n-m'+1}}, \cdots, {u_{n}}\}\\
    % &{w'_0}=\{w_1, \cdots, w_{j_1-1}, w_{j_1+1}, \cdots, w_n, {w_{\texttt{<pad>}}^{1}}, \cdots, {w_{\texttt{<pad>}}^{m'}}\}) \\
    =&(u', w')=(\{u_1, \cdots, u_{j_1-1}, {u_{n-m'+1}}, u_{j_1}, \cdots, u_{n-m'}\}, \\
    &\qquad \qquad \quad \{w_1, \cdots, w_{j_1-1}, {w_{\texttt{<pad>}}^{1}}, w_{j_1+1}, \cdots, w_n\})
\nonumber
\end{aligned}
\end{equation}

\vspace{-0.05in}

where $w_\texttt{<pad>}^i$ is the embedding vector of \texttt{<pad>} (i.e., the all-zero vector), and it replaces the original embedding vector $w_{j_i}$, $i\in [1, m']$. The position vector of $w_\texttt{<pad>}^i$ is $u_{n-m'+i}$, such that $u'\cdot w'$ corresponds to the embedding matrix generated after deleting $m'$ words at position $j_1, \cdots, j_{m'}$ in the text $x$.
% The $\ell_0$ distance between $u$ and $u'$ is $2(n-j)$. The $\ell_0$ distance between $w$ and $w'$ is the number of deletion words.
% the delete can only defend the delete attacks, which replace the word with a \texttt{<pad>}

% \BW{Is it appropriate to use a single word insertion to illustrate the transformation? Same for word deletion.} \xy{done, use $m'$ words}


\vspace{-0.1in}
\subsection{Framework Overview}
\vspace{-0.1in}

% \BW{By checking the proofs and notations, I strongly suggest clearly defining the perturbation for each operation and what we want to certify here, as otherwise it is easily to get lost. For instance, to describe the perturbation for Synonym Substitution, we can say $w \oplus \delta_S = [w_1 \oplus a_1, w_2 \oplus a_2, \cdots]$, where  $w_j \oplus a_j$ means we replace the word embedding $w_j$ with any synonym $w_j^{a_j}$. 
% Then in the next Section, we do not need to redefine them.} \xy{I have defined the four perturbations here and deleted the definition in Section  4.}

\subsubsection{Perturbations of Adversarial Operations} \label{sec:perturbation}
Since different operations involve different permutations and embedding transformations, we first define their perturbations and will certify against these perturbations in Section ~\ref{sec:theorem}.

\vspace{0.03in}

\noindent\textbf{Synonym Substitution} involves only the embedding substitution, which can be represented as $w \oplus \delta_S = \{w_1 \oplus a_1, \cdots, w_n \oplus a_n\}$. Here, $w_j \oplus a_j$ denotes the replacement of the word embedding $w_j$ with any of its synonyms $w_j^{a_j}$, while the original embedding can be $w_j=w_j \oplus 0$. Thus, the perturbation is defined as $\delta_S = \{a_1, \cdots, a_n\}$.  
% $a_j\in [1,s]$ where $s$ is the size of the thesaurus. 

\vspace{0.03in}


\noindent\textbf{Word Reordering} involves only the permutation of embeddings, which can be represented as $u \oplus \delta_R = \{u_1 \oplus r_1, \cdots, u_n \oplus r_n\}$, where $u_j \oplus r_j = u_{r_j}$ indicates that the embedding originally at position $j$ is reordered to position $r_j$. The reordering perturbation is $\delta_R=\{r_1-1, \cdots, r_n-n\}$.

\vspace{0.03in}


\noindent\textbf{Word Insertion} includes permutation and embedding insertion, with the permutation perturbation, $\delta_R$, being identical to word reordering. Embedding insertion preserves the first $n-m'$ embeddings while replacing only the last $m'$ embeddings with new ones. The perturbation of insertion is defined as $\delta_I=\{w_\mathrm{In}^1-w_{n-m'+1}, \cdots, w_\mathrm{In}^{m'}-w_n\}$.

\vspace{0.03in}

\noindent\textbf{Word Deletion} involves permutation and embedding deletion, where permutation perturbation is equivalent to $\delta_R$. We model the embedding deletion that converts any selected embedding to $w_{\texttt{<pad>}}$ as an embedding state transition from $b=1$ to $b=0$. The deletion perturbation is therefore defined as $\delta_D=\{1-b_1, \cdots, 1-b_n\}$, where all $b_j, j\in[1, n]$ are equal to 1, except for $b_{j_1}=0, \cdots, b_{j_{m'}}=0$, which represents the deleted embeddings at positions $j_1, \cdots, j_{m'}$.


%\YH{defense goal can be merged with Framework}

\vspace{-0.1in}
\subsubsection{Framework and Defense Goals}
Figure~\ref{fig:system} summarizes our Text-CRS framework. The input space is partitioned into a permutation space $\CU$ and an embedding space $\CW$, by representing each operation as a combination of permutation and embedding transformation (see Section~\ref{sec:transform_detail}). We analyze the characteristics of each operation and select an appropriate smoothing distribution to ensure certified robustness for each of them (see Section~\ref{sec:theorem}).

% \vspace{-3mm}

% Figure environment removed

Since each word-level operation is equivalent to a combination of the permutation and embedding transformation, any adversary that perturbs the text input is indeed adjusting the parameter tuple $(r, t)$ of permutation and embedding transformation. Our goal is to guarantee the robustness of the model under the attack of a particular set of parameter tuple $(r, t)$. 
Specifically, we aim to find a set of permutation parameters $S_{adv}^r \subseteq \CR$ and a set of embedding parameters $S_{adv}^t \subseteq \CT$, such that the prediction results of the model $h$ remain consistent under any $(r, t) \in S_{adv}^r \times S_{adv}^t$, i.e., 
\begin{equation}
\small
    h(u\cdot w)=h(\theta(u, r)\cdot \phi(w, t)), \forall{r} \in S_{adv}^r, \forall{t} \in S_{adv}^t
\end{equation}

% $h: \BR^{n\times d} \to \CY$ be any deterministic or random function. 
% From $h$, we aim to construct a provably robust classifier $g: \BR^{n\times d} \to \CY$. 

