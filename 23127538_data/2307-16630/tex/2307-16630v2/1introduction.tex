\vspace{-0.1in}
\section{Introduction}
\vspace{-0.1in}
% With recent advances in natural language processing (NLP), large language models (e.g., ChatGPT and chatbot) are increasingly common use. As an essential and fundamental part of language models, the text classification task has a wide range of applications, such as content moderation, sentiment analysis, fraud detection, and spam filtering. However, text classification models are vulnerable to adversarial attacks, deliberately manipulating the model output by modifying the input text. \cite{RealWorl22:online}

With recent advances in natural language processing (NLP), large language models (e.g., ChatGPT \cite{chatgpt:online} and Chatbots \cite{zhang2020dialogpt,shuster2020dialogue,roller2021recipes}) have become increasingly popular and widely deployed in practice. Wherein, text classification plays an important role in language models, and it has a wide range of applications, including content moderation, sentiment analysis, fraud detection, and spam filtering~\cite{9TextCla71:online,AIDocume25:online}. Nevertheless, text classification models are vulnerable to word-level adversarial attacks, which imperceptibly manipulate the words in input text to alter the output~\cite{ren2019generating,maheshwary2021generating,jin2020bert,garg2020bae,lee2022query,li2021contextualized,feng2018pathologies}. These attacks can be exploited maliciously to spread misinformation, promote hate speech, and circumvent content moderation~\cite{wu2019misinformation}.

% \BW{To mitigate such attacks on text classification models, a series of empirical defenses have been proposed~\cite{pruthi2019combating,zhufreelb,jones2020robust,zhou2021defense,dongtowards,bao2021defending,huang2022word,yang2022robust,yoo2022detection,moon2022gradmask,mosca2022suspicious,azizi2021t,liu2022piccolo,zhang2022improving,qi2021onion,minh2022textual,DBLP:conf/iclr/MiyatoDG17}, where the state-of-the-art is based on adversarial training~\cite{DBLP:conf/iclr/MiyatoDG17} or data augmentation~\cite{jones2020robust,zhou2021defense}. 
% % For instance, 
% % Miyato et al.~\cite{DBLP:conf/iclr/MiyatoDG17} proposed to apply  adversarial training to the text domain. 
% % Zhou et al.~\cite{zhou2021defense} augments training data by creating virtual texts that mix the embedding of the original word with its synonyms' embedding via Dirichlet sampling. 
% However, these methods are shown to be broken by adaptive adversaries~\cite{jin2020bert}. 
% % For instance, \cite{} show that the defense 
% To end the cat-and-mouse game, in the past three years, researchers start focusing on certified defenses~\cite{jia2019certified,huang2019achieving,ye2020safer,zeng2021certified,wang2021certified}, i.e., defenses with provable robustness guarantees against the worst-case attacks. Among different certified defense...}

To defend against such attacks, numerous techniques have been proposed to improve the robustness of language models, especially for text classification models. For instance, adversarial training~\cite{goodfellow2014explaining,madrytowards,dongtowards} retains the model using both clean and adversarial examples to enhance the model performance; feature detection~\cite{yoo2022detection,mozes2021frequency} checks and discards detected adversarial inputs to mitigate the attack; and input transformation~\cite{wang2021natural, yang2022robust} processes the input text to eliminate possible perturbations. However, these empirical defenses are only effective against specific adversarial attacks and can be broken by adaptive attacks~\cite{jin2020bert}.
% (without formal guarantees). New adversarial examples in unseen attacks or adaptive attacks may explicitly break the defense.
% Thus, the defender becomes caught in a cycle with the adversary.
% robustness and generalization of ML models can be improved by crafting high-quality adversaries and including them in the training data.

One promising way to win the arms race against unseen or adaptive attacks %escape the attack-defense cycle 
is to provide provable robustness guarantees for the model. This line of work aims to develop certifiably robust algorithms that ensure the model's predictions are stable over a certain range of adversarial perturbations. Among different certified defense methods, randomized smoothing~\cite{cohen2019certified,zhang2020black,li2022sok} does not impose any restrictions on the model architecture and achieves acceptable accuracy for large-scale datasets. This method injects random noise, sampled from a smoothing distribution, into the input data during training to smoothen the classifier. The smoothed classifier will make a consistent prediction for a perturbed test instance (with noise) as the original class label. %under the smoothing distribution and ultimately outputs the most probable results. 
% Although randomized smoothing methods have been successfully applied to image inputs, applying them to text inputs remains fairly challenging.
Despite the successful application of randomized smoothing in protecting vision models, applying these methods to safeguard language models remains fairly challenging.
% We summarize four challenges for certifying the text classification model:
% While existing works on adversarial examples have succeeded in the image domain, it is still challenging to deal with text data due to its discrete nature. 

\begin{table*}[]
\centering
\setlength\tabcolsep{3pt}
% \scriptsize
\caption{Comparison of certified defense methods for NLP robustness against textual adversarial attacks.}
\vspace{-0.07in}
\resizebox{\linewidth}{!}{
\begin{threeparttable}
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Model architecture} & \multicolumn{4}{c}{Adversarial operations (smoothing distribution / $\ell_p$ perturbation)} & Certified & Uni- & Accuracy \\
%\cline{3-6}
 &  & Substitution & Reordering & Insertion & Deletion & radius / $\rad$ & versality & (large-scale data) \\
\midrule
IBP-trained~\cite{jia2019certified} & LSTM/Att. layer & \ding{51}& \ding{51}& \ding{51}& \ding{51} & \ding{55} & \ding{55} & Low \\
POPQORN~\cite{ko2019popqorn} & RNN/LSTM/GRU & \ding{51}& \ding{51}& \ding{51}& \ding{51} & \ding{55} & \ding{55} & Low \\
Cert-RNN~\cite{du2021cert} & RNN/LSTM & \ding{51}& \ding{51}& \ding{51}& \ding{51} & \ding{55} & \ding{55} & Low \\
DeepT~\cite{bonaert2021fast} & Transformer & \ding{51}& \ding{51}& \ding{51}& \ding{51} & \ding{55} & \ding{55} & Low \\
SAFER~\cite{ye2020safer} & Unrestricted & \ding{51} (Uniform / $\ell_0$) & \ding{55} & \ding{55} & \ding{55} & \ding{51} Practical & \ding{55} & High \\
% WordDP~\cite{wangw2021certified} & Unrestricted & \ding{51} (Uniform) & \ding{55} & \ding{55} & \ding{55} & \ding{51} (1) & \ding{55} & High \\
RanMASK~\cite{zeng2021certified} & Unrestricted & \ding{51} (Uniform / $\ell_0$) & \ding{55} & \ding{55} & \ding{55} & \ding{51} & \ding{55} & High\\
CISS~\cite{zhao2022certified} & Unrestricted & \ding{51} (Gaussian / $\ell_2$) & \ding{55}& \ding{55}& \ding{55} & \ding{51} & \ding{55} & High \\
\midrule
Text-CRS (Ours) & Unrestricted & \ding{51} (Staircase / $\ell_1$) & \ding{51} (Uniform / $\ell_1$) & \ding{51} (Gaussian / $\ell_2$) & \ding{51} (Bernoulli / $\ell_0$) & \ding{51} Practical & \ding{51} & $>$  SOTA \\
\bottomrule
\end{tabular}
\begin{tablenotes}
    \item {1. The model architectures applicable to the first four methods have size restrictions, i.e., the number and size of layers cannot be too large.}
    \item {2. "Practical" means that the certified radius can correspond to the $\rad$-word level of perturbation. (We propose four practical certified radii.)}
    % \item {3. We introduce four certified radii under different smoothing distributions.}
\end{tablenotes}
\end{threeparttable}
}
\label{tab:intro}
\vspace{-0.23in}
\end{table*}


% And because they do not consider the numerical relationships between words
First, due to the discrete nature of the text data, numerical $\ell_1$ or $\ell_2$-norms cannot be directly used to measure the distance between texts. Without considering word embeddings,
% (e.g., may need encoding of words before distance calculation). 
previously certified defenses in the NLP domain, such as SAFER~\cite{ye2020safer} and WordDP~\cite{wang2021certified2}, are limited to certifying robustness against $\ell_0$ perturbations generated by synonym substitution attacks. Also, their assumption of uniformly distributed synonyms is impractical, leading to relatively low certified accuracy. Second, text classification models are vulnerable to a range of word-level operations that result in various perturbations. For instance, the word insertion operation introduces random words in the lexicon, while the word reordering operation causes positional permutation. These diverse perturbations can deceive text classification models successfully~\cite{garg2020bae,jin2020bert,lee2022query,li2021contextualized,feng2018pathologies}. To our best knowledge, there exist 
no certified defense methods 
% have been investigated 
against these word-level perturbations. 
% can provide certifiable robustness bounds for text classification models against these word-level perturbations. 
Third, significant absolute differences between adversarial and clean texts may exist due to word-level operations, while conventional  
% Conversely, 
randomized smoothing can only ensure the model's robustness against perturbations within a small radius. 
% Thus, the substantial difference may exceed the bounds of classical smoothing methods. 
Prior works~\cite{fischer2020certified,li2021tss,alfarra2022deformrs,hao2022gsmooth, liu2021pointguard} for images address this challenge by providing robustness guarantees against semantic transformations (e.g., rotation, scaling, shearing). However, they cannot be directly applied to the NLP domain because texts and words have a more heterogeneous discrete domain, and word insertion and deletion are new semantic transformations not involved in the image domain. %Firstly, unlike image pixels, numerous words have a greater heterogeneous discrete domain. Secondly, word insertion and deletion are new semantic transformations not involved in the image domain. 


% Figure environment removed

%\BW{We need to mention somewhere that our Text-CRS can address the three challenges in the existing works.}
In this paper, we present Text-CRS, the first generalized certified robustness framework against common word-level textual adversarial attacks (i.e., synonym substitution, reordering, insertion, and deletion) via randomized smoothing (see Figure\,\ref{fig:Text-CRS}). Text-CRS 
%encompasses most of the fundamental operations that occur in textual adversarial attacks (i.e., synonym substitutions, reorderings, insertions, and deletions), and 
\emph{certifies the robustness in the word embedding space} without imposing restrictions on model architectures, and demonstrates \emph{high universality} and \emph{superior accuracy} compared to state-of-the-art (SOTA) methods (see Table\,\ref{tab:intro}). 
% instead of the discrete text space. 
%Particularly, we consider incorporating word embeddings instead of d
%\BW{To our best knowledge, we take the first cut to?} 
%we consider numerical relationships between words rather than discrete relationships by incorporating the word embedding vectors into the certification. 
% Furthermore, by partitioning the operations' input space into permutation and embedding spaces, 
Specifically, we first model word-level adversarial attacks as combinations of permutations and embedding transformations. 
%\BW{Here we briefly describe the permutation and embedding transformations for two attacks, e.g., Synonym substitution and word insertion?}
%\BW
{For instance, synonym substitution attacks transform the original words' embeddings with the synonyms' embeddings, while word reordering attacks transform the word orderings with certain word permutations.} 
Then, the {word-level adversarial attacks} can be guaranteed to be certified robust (``\emph{practical}'') if their corresponding permutation and embedding transformation are certified. 

%\BW
To this end, we develop customized theorems (see Section~\ref{sec:theorem}) to derive the certified robustness against each attack.
% Furthermore, to improve the certified accuracy/radius,  % theoretically,
In each theorem, we use an 
appropriate 
% novel theorems that apply well-designed 
noise distribution for our smoothing distributions in order to fit different word-level attacks. 
% transformations in word-level adversarial operations. 
% Specifically, 
% in Text-CRS, 
For instance, unlike existing works~\cite{ye2020safer,huang2022word} that use the uniform distribution, we propose to use the Staircase-shape distribution~\cite{geng2015staircase} to simulate the synonym substitutions, which ensures that semantically similar synonyms are more likely to be substituted. %fixing the relative importance of each position in a sentence to the prediction is impossible due to differences in model architectures and datasets; thus, 
% \BW{address the uniform sampling issue..}
Moreover, we use a uniform distribution to simulate the word reordering. %as it is difficult to determine the relative importance of the words.
For the word insertion with a wide range of inserted words, we inject Gaussian noise into the embeddings. For the word deletion, the embedding vector of each word is either kept or deleted (i.e., set to be zero). %(simulate deleted words). 
%the deleted words are set to be all-zero vectors. 
Hence, we use the Bernoulli distribution to simulate the status of each word. 
%randomize the word deletions with a certain probability. 
Our certified radii for the four attacks are then derived based on the corresponding noise distributions. 
% For each of the four adversarial operations, we theoretically derive the certified radius based on the corresponding randomization. 


%In summary, Text-CRS provides certified robustness for the four fundamental operations without imposing restrictions on model architectures and demonstrates superior accuracy compared to SOTA methods, as outlined in Table~\ref{tab:intro}.  %\hl{briefly discuss Table 1} \xy{(done). The IBP-based method and the RS-based method have no restrictions on the operation and model architecture, respectively. So should I define universality as having no restrictions on both operations and models? I only compared Text-CRS with SAFER and CISS in the evaluation.} \YH{we need to be careful here about over-claiming. It seems good if considering both of these as universal....and then we can add universal to the title, and revise "general" to "universal"}
% Our framework provides provable robustness for text classification models against four operations. 



To further improve the certified accuracy/radius, 
% Finally, 
we also propose a training toolkit incorporating three optimization techniques designed for training. For instance, instead of using isotropic Gaussian noises that can lead to distortion in the word embedding space, we propose to use an anisotropic Gaussian noise and optimize it to enlarge the certified radius.  
%for text classification models, including three simple yet effective training methods to improve the certified accuracy and robustness bound. 

%Notably, these word-level adversarial operations can essentially be treated as word insertions. As a result, our word insertion theorem also applies to the other three word-level adversarial operations, while our refined theorems are more effective for the other three operations. Furthermore, we provide theoretical robustness bounds of models trained with Text-CRS to allow the model owners (defenders) to assess the model's robustness against different operations. 



%We conduct extensive experiments to evaluate our framework Text-CRS on two types of models (i.e., LSTM~\cite{LSTM} and BERT~\cite{devlin-etal-2019-bert}) and three datasets with different tasks. We offer certified accuracy under different certified radii for word-level adversarial operations and certified accuracy against five SOTA adversarial attacks (i.e., TextFooler~\cite{jin2020bert}, WordReorder\cite{moradi2021evaluating}, SynonymInsert\cite{morris2020textattack}, BAE-Insert\cite{garg2020bae}, InputReduction\cite{feng2018pathologies}). Besides, we evaluate and analyze the universality of the word insertion theorem. Our framework outperforms SOTA methods in synonym substitution and provides the first certified robustness results for other word-level adversarial operations. 
% where the adversarial perturbation may exceed the radius threshold.


Thus, our major contributions are summarized below: 

\begin{itemize}[leftmargin=*]
\setlength\itemsep{0.3em}
    \item To our best knowledge, we propose the first generalized framework Text-CRS to certify the robustness for text classification models against four fundamental word-level adversarial operations, covering most word-level textual adversarial attacks. Also, the certification against word insertion can be \emph{universally} applied to other operations. %\hl{mention universality? and enhance?}

    \item We provide novel robustness theorems based on Staircase, Uniform, Gaussian, and Bernoulli smoothing distributions against different operations. We also theoretically derive certified robustness bounds for each operation. 

    % \item We propose an enhanced training toolkit, including three simple yet effective methods to improve certified accuracy and robustness bounds.

    \item To study the deceptive potential of adversarial texts, we apply ChatGPT to assess whether adversarial texts can be crafted to be semantically similar to clean texts.

    \item We conduct extensive experiments to evaluate Text-CRS, including our enhanced training toolkit, on three real datasets (AG’s News, Amazon, and IMDB) with two NLP models (LSTM and BERT). The results show that Text-CRS effectively handles five representative adversarial attacks and achieves an average certified accuracy of $81.7\%$, which is a $64\%$ improvement over SOTA methods. Text-CRS also significantly outperforms SOTA on the substitution operation. Besides, it provides new benchmarks for certified robustness against the other three operations. 
    % (AG’s News~\cite{zhang2015character}, Amazon~\cite{mcauley2013hidden}, and IMDB~\cite{maas2011learning}) with two NLP models (LSTM~\cite{LSTM} and BERT~\cite{devlin-etal-2019-bert})
    % On the certification against synonym substitution, Text-CRS significantly outperforms the SOTA methods. 
\end{itemize}

