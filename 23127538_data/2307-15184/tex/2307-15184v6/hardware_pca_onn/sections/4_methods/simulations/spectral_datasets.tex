\subsubsection{Spectral Datasets and Performance Evaluation}


% \rnote{How well this model can work w/o noise}

% \subsubsection{Performance Evaluation}


\Xhide{\Xpolish{The performances of the models were evaluated through their average classification rates. To ensure a thorough assessment, the model was tested five times independently, each time with a randomly split dataset for 70\% training and 30\% validation. During each assessment, the model generated noisy photon counts, which were then used to train the classifier until the validation rate reached a plateau or declined. The best validation rate achieved during the training process was chosen as the representative performance upper bound for each assessment. The overall performance of the model was determined by averaging the results of all five assessments.}
{The model performances were assessed based on their average classification rates, which were calculated after five independent tests. In each test, the dataset was randomly split into 70\% for training and 30\% for validation. The model generated noisy photon counts during each assessment, and the training continued until the validation rate reached a plateau or declined. The highest validation rate obtained during the training process was considered as the representative upper bound for each assessment. The overall model performance was determined by averaging the results from all five assessments. This comprehensive evaluation ensured the reliability and accuracy of the model performance.}}

\Xpolish{\Xpolish{In this experiment, w}{W}e utilized the Indian Pines dataset \cite{PURR1947}, which is acquired through the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) on June 12, 1992, covering the Purdue University Agronomy farm northwest of West Lafayette and its surrounding area. Comprising $145 \times 145$ pixels and 224 spectral reflectance bands in the wavelength range of 0.4 to 2.5 $\mu m$, this dataset provides a comprehensive view of the study area. Fig. \ref{fig:pines_dataset} illustrates the classes within the Indian Pines dataset, presenting the counts associated with each class. The figure further features a representation of one channel (band 12) for all pixels, along with the ground truth of semantic labeling.}{
We utilize the Indian Pines dataset \cite{PURR1947}, acquired through AVIRIS on June 12, 1992, covering Purdue University's Agronomy farm and surrounding areas. With $145 \times 145$ pixels and 224 spectral bands (0.4 to 2.5 $\mu m$) per pixel, this dataset offers a comprehensive view {of this area}. Fig. \ref{fig:pines_dataset} illustrates 16 classes in Indian Pines, along with their counts and semantic labeling ground truth. It features a representation of the band-12 for all pixels. Fig. \ref{fig:AeroRIT_Histogram} shows the relative intensity of the selected classes from the spectrum data, providing a concise overview of their differences.
}

% Figure environment removed

% % Figure environment removed

\Xpolish{In this experiment, spectral data were zero-padded from 224 to 256 to align with the Hadamard mask and \YLnote{\Xpolish{normalized}{re-scaled}} to the range $[0,1]$. We utilized the scanner-classifier module from Fig. \ref{fig:model_config}, adapting the ONN classification layers to the new dataset sizes. Each classification ran for 2000 epochs, with a learning rate of $5 \times 10^{-3}$ and a batch size of 5000.}{
\Xpolish{In the experiment, spectral data \YLnote{extracted as arrays from each pixel}, zero-padded from 224 to 256 for Hadamard mask alignment and rescaled to $[0,1]$, undergoes processing using the modified scanner-classifier module from Fig. \ref{fig:model_config}}{In the experiment, spectral data vectors, extracted as arrays from each pixel, are zero-padded from 224 to 256 in length for Hadamard mask alignment and rescaled to $[0,1]$. It undergoes processing using the modified scanner-classifier module illustrated in Fig. \ref{fig:model_config}}. The classifier is streamlined to a single fully-connected layer with 1478 neurons, utilizing the default architecture provided by \texttt{scikit-learn} during the pre-project testing phase which gives an 86\% classification rate without any coding or noise. The ONN classification layers are adjusted to accommodate input data subsets, each containing 400 entries per class. Classes with less than 400 entries are omitted to maintain unbiased input. 
}

\Xpolish{The model performances were assessed based on their average classification rates, which were calculated after five independent tests. In each test, the dataset is randomly split into 90\% for training and 10\% for validation. The model generated noisy photon counts during each assessment, and the training continues until the validation rate reached a plateau or declined. The highest validation rate obtained during the training process is considered as the representative upper bound for each assessment. The overall model performance is determined by averaging the results from all five assessments. This comprehensive evaluation ensures the reliability and accuracy of the model performance.}{
Model performance is evaluated using average classification rates from five independent tests. Each test involves a random 90\%-10\% training-validation split of the dataset. Training continues until the validation rate plateaued or declined, with the peak validation rate representing the upper bound for each test. The final performance metric is the average of all five peak validation rates, ensuring a reliable and accurate assessment of the model.
}