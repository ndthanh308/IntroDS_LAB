% \YLnote{1. What are sensing matrix 2. their types 3. limitations}
\Xhide{As a representative technique in the CI, the mathematical underpinning of single-pixel camera is encapsulated by Eq. \ref{eqn:NoiselessMeasurement}. The diverse selections of the sensing matrix $\B{M}$ lead to distinct coding strategies, as elaborated upon subsequently. }

\YLnote{\Xpolish{\AVnote{ChatGPT problem}In the realm of the Computational Imaging (CI), the foundational equation for single pixel imaging is encapsulated in Eq. \ref{eqn:NoiselessMeasurement}}{Eq. \ref{eqn:NoiselessMeasurement} is the foundational equation for single pixel imaging}}. \Xpolish{The diverse array of choices for the sensing matrix $\B{M}$ engenders two basic coding strategies, as expounded in Fig \ref{fig:SinglePixelImaging}. However, the process of discerning an optimal $\B{M}$, whether drawn from random matrices or Hadamard matrices, hinges on multifaceted considerations. In the present study, we advocate the notion that the selection of an ideal $\B{M}$ should be contingent upon the requisite prior information pertinent to the tasks at hand. Herein, we delineate various coding types along with their corresponding optimal applications for reference and guidance.}{Based on it, we distinguish different types of $\B{M}$ below for reference and guidance.}

\iffalse
% Figure environment removed
\fi
% As a typical CI technique, the mathematical foundation of single pixel imaging follows the equation \ref{eqn:NoiselessMeasurement}. However, the different choices of the sensing matrix $\B{M}$ give rise to different coding types (strategies) introduced later. However, how to select a good $\B{M}$ from random matrices to Hadamard matrices for the Basis scan depends on many factors. In this project, we proposed that the optimal $\B{M}$ should be selected based on the required prior information of tasks we want to complete. The coding types and the most suitable task of each type are listed below for reference.

\subsubsection{Coding Types}

\begin{itemize}
    \item \textbf{Raster scan} entails a pixel-wise scanning approach where photon counts for all pixels are sequentially measured \cite{gibson2020single}. Mathematically, the sensing matrix $\B{M} = \B{I}$, an identity matrix.      
    
    \item \textbf{Impulse imaging} entails an ideal camera that simultaneously captures \Xpolish{complete hyper-spectral data}{all elements in the measurements, $\B{y}$}. A prominent instance is a pixel array. When the field of view $\B{x}$ consists of $N$ pixels, the sensing matrix assumes the form $\B{M} = N\B{I}$ where $\B{I}$ is an identity matrix. Each pixel receives $N$ times the exposure in comparison to the raster scan scenario. This technique sets a benchmark akin to a gold standard, serving as an upper limit for the CI performance and an indicator of the noise level for comparison.
    
    \Xhide{\item \textbf{Non-Selective Codes} \Xpolish{are codes that are not adjusted to the specific sparsity of the dataset. Any general code not generated using the statistics of a representative training set is included in this group. This includes many codes used in practice today, such as Hadmard patterns and Rademacher Random codes. We also include truncated Hadamard patterns that are popular in compressed sensing. A truncated Hadamard code uses a random subset of the full Hadamard basis.
    }{encompass coding schemes that lack adaptation to the inherent sparsity of a given dataset. Codes falling within this category are not crafted based on the statistical attributes of a representative training set. This classification encompasses a range of widely used codes, such as Hadamard patterns and Rademacher Random codes\Xhide{, which do not leverage the specific characteristics of the dataset}. This grouping also extends to truncated Hadamard patterns\Xpolish{, a popular choice in compressed sensing, which employ random subsets of complete Hadamard basis.}{ which employ low frequency components of complete Hadamard basis.}
    }
    }%change to a newer and simpler one
    \item \YLnote{\textbf{Non-Selective Codes} refer to coding schemes that uncorrelated with the inherent \Xpolish{sparsity}{compressibility} or statistical attributes of a specific dataset. Examples include widely used codes like Hadamard patterns and Rademacher Random codes. This category also includes truncated Hadamard patterns that use low-frequency components of the complete Hadamard basis.}

    
    \item \textbf{Selective Codes}
    \Xpolish{In this work we show that for single photon cameras, codes need to be specifically selected to measure sparse representations of the data. To achieve this we employ PCA codes and ONN generated codes. The principal component analysis (PCA) is a classical method for multispectral data correspondence analysis and classification \cite{rodarmel2002PCA, carr1999PCA, jensen1996PCA, gonzales1987PCA, schowengerdt2006PCA}. In this project, the most significant components are computed from the training data and implemented as the hardware masks. Measured photon counts $\B{y}$ are then be feed to a classifier for model optimization. The optical neural networks (ONN) method is like a fully-connected neural networks model but its first layer is used to simulate the sensing matrix $\B{M}$. By properly applying the physical constraints on the first layer, it is possible for this model to find optimal sensing matrix for a given task.}
    {\Xpolish{.In this study, we elucidate the necessity for the tailored selection of codes in}{encompass coding schemes selectively \YLnote{\Xpolish{incorporating the context of single pixel cameras}{learn the significant features from training data}}} to effectively capture sparse data representations. \Xpolish{To address this, we harness both PCA codes and ONN-generated codes. Principal Component Analysis (PCA) serves as a classical technique for analyzing and classifying multispectral data correspondences \cite{rodarmel2002PCA, carr1999PCA, jensen1996PCA, gonzales1987PCA, schowengerdt2006PCA}. Specifically, we compute the most significant components from training data and implement them as hardware masks within this project. The recorded photon counts, $\B{y}$, are subsequently fed into a classifier to optimize the model. The Optical Neural Networks (ONN) approach mirrors a fully-connected neural network model, yet with its first hidden layer simulating the sensing matrix $\B{M}$. By adeptly applying physical constraints to this foundational layer, the model becomes capable of determining the optimal sensing matrix for a designated task.}{When under AGN, FSI finds data specific codes through Principal Component Analysis (PCA).}
    }
    
\end{itemize}

\subsubsection{Vision Tasks}
% \paragraph{Null Prior Reconstruction/No Compression}
% \paragraph{Imaging: Weak Prior/Some compression}
% \paragraph{Classification: Strong Prior/Strong compression}
\iffalse
\begin{itemize}
    \item Classic imaging: null-prior/no compression. In this task, the rank of $\B{M}$ is required to be $N$, the number of pixels in the FOV. Therefore, there is no compression when conduct measurements. This case is universal and doesn't require any prior knowledge of the FOV. It can be found in some scenarios such as raster scan. When there is no noise, we can also use full rank Hadamard to reconstruct the FOV. The coding types commonly seen in this kind of tasks are raster scan and non-selective codes.
    \item Image Processing and the Computational Imaging: weak-prior/some compression. This idea comes from some prior knowledge such as signals are sparse in some linear space and noise is concentrated in high frequency domain. One example is the JPEG format of pictures. If we only measure the FOV with low frequency components of the Hadamard matrices, we are working on this task. This type can be found in scenarios such as compressed sensing and image compression. The coding type commonly seen in this kind of tasks is non-selective codes.
    \item Computer vision: strong-prior/much compression. Usually we switch to downstream computer vision tasks such as classification after reconstruct the FOV. However, it is possible to directly capture the principal features by $\B{M}$ if some of its components are similar to the FOV. In this kind of tasks, only a few measurements are needed if we have a strong prior from training data. The coding type involved in this kind of tasks is selective codes.
\end{itemize}
\fi

\begin{itemize}
    \item Signal Reconstruction (Null-prior/No Compression):
    In tasks involving classic imaging, a null-prior is assumed, and there is no compression during measurements. The rank of the measurement matrix $\B{M}$ is required to be equal to $N$, the number of pixels in the \Xpolish{FOV}{$\B{x}$}\Xpolish{. This scenario, such as in raster scan applications, is universal and does not necessitate prior knowledge of the FOV. Full-rank Hadamard matrices can be employed for reconstruction in noise-free situations. Common coding types for this task include raster scan and non-selective codes.}{ and no prior or bias should be introduced through $\B{M}$.}
    \Xhide{
    \item Image Reconstruction and Computational Imaging (Weak-prior/Some Compression):
    In this kind of tasks, a weak-prior assumption is made\Xhide{, allowing for some compression during measurements}. This idea is based on prior knowledge, such as signals being sparse in some linear space, with noise concentrated in the high-frequency domain. \Xpolish{An example is the JPEG format. If only the low-frequency components of Hadamard matrices are measured for reconstructing the FOV, this task is engaged. Non-selective codes are typically used for this purpose.}{One example in this category is compressed sensing.}    
    }%hide and update to newer one below    
    \YLnote{\item Computational Imaging (Weak-prior/Some Compression): In this kind of tasks, a weak-prior assumption is made. This idea is based on prior knowledge, such as signals being sparse in some linear space, with noise concentrated in the high-frequency domain. One example in this category is compressed sensing.
    }\AVnote{\checkednote{We are not doing any imge reconstruction any more Jan 18, 2024 10:40 AM}}
    \item Classification (Strong-prior/Much Compression):
    \Xpolish{Tasks related to computer vision}{Many computer vision tasks} involve a strong prior, enabling substantial compression during measurements. \YLnote{\Xpolish{After reconstructing the \Xpolish{FOV}{$\B{x}$}, the process often transitions to downstream computer vision tasks like classification. Alternatively, direct}{Directly}} capture of principal features by $\B{M}$ is possible if some of its components resemble the \Xpolish{FOV}{$\B{x}$}. In these tasks, a few measurements are often sufficient with a strong prior derived from training data. \Xpolish{Selective codes are commonly employed in this context.}{Our focus is to optimize codes in this scenario.}
\end{itemize}