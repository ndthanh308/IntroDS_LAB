\checkednote{\subsection{Definition of a measurement}}
In compressed sensing and coding is typically defined as a measurement of an analog flux through some type of coding projection or in our example a mask. As has been shown by multiple works, this analog model of light does not account for the poisson noise inherent in any real measurements and leads to counter intuitive behavior of coding approaches.

If we instead think of our system as measuring photons through different codes, the code behavior makes intuitive sense. A photon measured through a mask with many open pixels carries less information about the scene than one captured through a raster mask because our measurement is ambiguous regarding which pixel in the mask was the origin of the photon. In a raster mask every photon can be uniquely assigned to one pixel. In essence, more photons do not equal more information.

The implication of this well documented problem become ever more important in the age of low noise and photon counting cameras where Poisson noise dominates all measurements. It is wide reaching since the projection process we study here in a specific coding experiment is part of the design of any camera. In other words: Any camera or vision system has to project data from a high dimensional scene space down into a lower dimensional sensor space where it encounters Poisson noise and then uses those noisy measurements to make inferences about the scene. 

\Xpolish{This paper has shown that the challenges for computational imaging under Poisson noise. Algorithms based on the AGN noise assumption are problematic on the modern sensors. However, if the task requires no reconstruction but direct feature extraction, the Selective Sensing using the Optical Neural Networks model can find the optimal coding methods. We have shown the feasibility of the Selective Sensing via simulations and experiments, and it demonstrated a promising classification performance on the MNIST handwritten number dataset. Also, it is robust in the application scenarios where their noise level is hard to estimate. Furthermore, the Selective Sensing provides an motivation for proposed optical ANNs or ANNs with optical layers which allow us to optimize the coding schemes wherever the Poisson noise happens. }
{Our paper highlights the challenges of computational imaging under Poisson noise and its impact on algorithms based on the AGN noise assumption. We find that for compressible measurements, and especially tasks that involve direct feature extraction instead of signal reconstruction, a Selective Sensing approach using task optimized codes provides a viable coding solution. Through simulations and experiments, we demonstrate the feasibility of Selective Sensing and its promising classification performance on the MNIST handwritten number dataset. It is also robust in application scenarios with difficult-to-estimate noise levels. Our ONN method represents a method that can generate these selective measurements. Furthermore, Selective Sensing motivates the development of optical ANNs or ANNs with optical layers to globally optimize imaging systems.}

\Xpolish{On the other hand, there are some limitations in our project. First, we employed the AGN model and reparameterization trick for the model training, which is only an approximation to the noise at the sensor. Second, our test set in the experiments only contains 10 numbers, which may not be representative enough. There is also an inconsistency as the model was trained by simulated data but tested with experimental data. Last but not the least, the Photon Distribution Factor rescales the masks $\B{M}$, but its value changes during the training and it is not evolved in the back-propagation. In general, the optimization of the ONN model still has some defects and we still need to improve the results by using better optimization methods and more experimental data.}
{Despite the promising results of our project, there are some limitations that must be acknowledged. First, we used a Gaussian noise model with reparameterization to train our model, which is only an approximation of the actual quantization noise at the sensor. Second, our test set consisted of only 10 numbers, which may not provide a comprehensive evaluation of the model's performance. Additionally, we noted an inconsistency in that the model was trained using simulated data but tested with experimental data. Lastly, the Photon Distribution Factor rescales the masks $\B{M}$, but its value changes during training and is not evolved during back-propagation. These limitations highlight the need for further improvements in the optimization of the ONN model, such as using more advanced optimization methods and larger sets of experimental data. Our work highlights the importance of the integration of imaging hardware and signal processing. In single photon accurate imaging systems, comprehensibility and sparsity of the data can be exploited to far greater effect during the measurement, as opposed to post processing.}

