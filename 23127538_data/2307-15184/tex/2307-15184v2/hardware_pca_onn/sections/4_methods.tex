\subsection{Model Configuration \ynote{\checkednote{this is just the ONN. We probably also want to say something about PCA}}} 
\Xpolish{Although coding strategies are different, they essentially share the same model structure shown in figure \ref{fig:model_config} for classification tasks. In this study, we built a classification pipeline consisting of a scanner module and a classifier in PyTorch. This model consists of a \textit{scanner} module and a \textit{classifier} module.\Xhide{The scanner is simulating the measurement process described in equation \ref{eqn:NoisyMeasurement} and the classifier is a neural networks model.} The scanner simulates the operations to count the photons, following the techniques\Xhide{ \bnote{"tricks" -> "techniques"}} mentioned in section \ref{ssec:ModelFormulation}. We\Xhide{ also \bnote{"Also" could mean we have two different models.}} employed a two-hidden layer artificial neural network (ANN) classifier with 40 and 128 nodes as the main model for predicting which number the object is.
%by the corresponding  number of re-centered, normalized, and noisy photon counts. 
% The input data for the classifier was obtained by subtracting the average of the training data, represented by $\B{\bar{x}}_T$, from the re-centered normalized noisy photon counts, denoted by $\tilde{\B{y}}$. 
To prevent overfitting and improve the generalizability of the model, dropout  was applied after each hidden layer, followed by a ReLU activation layer.  }
{All coding strategies for classification tasks share the same fundamental model structure, as illustrated in figure \ref{fig:model_config}. To classify handwritten numbers using optical devices with limited photon budget, we built a PyTorch-based pipeline consisting of a \textit{scanner} module and a \textit{classifier} module. The scanner module simulates the photon counting process using the techniques described in section \ref{ssec:ModelFormulation}. For the classification task, we utilized a two-hidden layer artificial neural network (ANN) with 40 and 128 nodes, serving as the primary model. To prevent overfitting and enhance generalizability, we applied dropout after each hidden layer, followed by a rectified linear unit (ReLU) activation layer.}

\Xpolish{When training this model, the scanner computes the photon counts with noise, and feeds it to the classifier. The noise is sampled from a predefined noise distribution, and is different in different epochs. After the loss is computed, parameters in the classifier are optimized by gradient descent. The ONN is different from the others as the back-propagation is extended into the scanner so that the masks are optimized together with the classifier.}
{During the training of the model, the scanner computes the photon counts with noise and then transmits this data to the classifier. The noise is sampled from a pre-defined noise distribution and varies across epochs. Once the loss has been computed, gradient descent is used to optimize the parameters of the classifier. \textbf{What sets the ONN apart from other models is that back-propagation is extended into the scanner, allowing for the simultaneous optimization of the masks and the classifier {under different noise models}}. This approach offers distinct advantages compared to traditional methods, which typically optimize the classifier independently of the scanner {or optimize the scanner without the appropriate noise model}.}

% Figure environment removed

ONN can also be used for reconstruction. \YLnote{To address this, we propose a simple approach of optimizing the ONN masks for a reconstruction task under different noise models. To achieve this, we developed a new ONN model with $m = N$ masks. The scanner takes raw image vectors $\B{x}$ and outputs $N$-element measured photon count vectors under a chosen noise model, $\noisy{\B{y}}$. The classifier is a single-layer network that takes $\noisy{\B{y}}$ as input and outputs $\noisy{\B{x}}$, also an $N$-element vector. The objective is to minimize the MSE between the input images and the output vectors, $\|\noisy{\B{x}} - \B{x}\|_2^2$. In noiseless cases, the classifier is equivalent to $\B{M}^{-1}$, where $\B{M}$ is the masks used in the scanner.}


\subsubsection*{ONN Optimization under Poisson Noise} 

\Xpolish{Since the Poisson noise generating function has no gradient, it is challenging to optimize the scanner. The reparameterization trick is a useful method for the AGN instead of the Poisson noise. Our method is using a normal distribution whose variance equals its expectation to approximate the Poisson distribution in training, and therefore, the reprameterization trick can be employed.}
{Optimizing the scanner can be challenging as the Poisson noise generating function typically lacks a gradient. While reparameterization is a useful method for addressing AGN, it is not as effective for addressing Poisson noise. To address this limitation, we employ a method mentioned by Cossairt et al. that utilizes a normal distribution with a variance equal to its expectation to approximate the Poisson distribution during training \cite{cossairt2012does}.\ynote{\checkednote{we need more details on this and how it is implemented in the network. Probably a figure of the entire network structure and the equation for the gradient (i think we also have references to cite here)}} This approximation enables us to employ PyTorch builtin reparameterization method, i.e. \texttt{rsample}, and optimize the scanner more effectively under Poisson noise.}

% \YLnote{
% A seperate section? And mention that Ollie did that as well.  \\
% Similar to corssairt et al, we use a normal dist to approximate poisson
% }

% \YLnote{
% Transform $\B{y} \sim \mathcal{P}(\B{Mx})$ to $\B{y} \sim \mathcal{N}(\B{Mx}, (\B{Mx})^\frac{1}{2})$ to use the reparamaterization trick. The variance will also play a role in computing the gradient.
% } 

To approximate the Poisson noise by the AGN, we can re-write the measurement with noise $\noisy{\B{y}} = \B{Mx} + \B{J}$ where $\B{J} =  (\B{Mx})^{\frac{1}{2}} \odot \B{\epsilon}, \B{\epsilon} \sim \mathcal{N}(0, \B{I})$. During the training under Poisson noise, $\B{J}$ also plays a role in gradient computation, which is the most significant difference compared with training under the AGN. 


\subsection{Sensing Matrices by Priors}
\Xpolish{In the last section, we mentioned the masks $\B{M}$ can be generated by different coding strategies. 
In order to effectively determine the appropriate coding strategies for a given scenario, it is necessary to evaluate the various strategies based on their priors. Image priors, which can vary in type, can enhance the robustness of models in the presence of noise and improve overall image quality \cite{cossairt2012does, levin2007coded}. It should be noted that all of the strategies discussed in this paper possess three distinct prior levels. }
{In the preceding section, we discussed the generation of masks $\B{M}$ using different coding strategies. Evaluating the efficacy of these strategies for a given scenario requires a systematic assessment of their respective priors. Image priors come in varying types and can enhance model robustness in noisy environments and improve overall image quality \cite{cossairt2012does, levin2007coded}. It is important to note that all the strategies outlined in this paper possess three distinct prior levels.}
% \bnote{suggest bolding the priors}
\begin{itemize}
    \item \textbf{Null prior} (data is not compressible or needs to be reconstructed without priors). \Xpolish{Coding strategies in this category require no information about the data. They usually generate full rank matrices as $\B{M}$. In this project, we chose Raster Scan (RS) and Hadamard Basis (HB) for performance evaluation. $\B{M}$ for RS is an identity matrix while it is a Hadamard matrix for HB. On top of these two stratgies, we also include Impulse Imaging (II) which is the optimal measurement that can only be achieved with a sensor array. In our toy example this would be achieved by a camera sensor that captures the image so no coding is necessary. Mathematically, the sensing matrix $\B{M}$ of II is also an identity matrix, but each mask in II has $N$ times longer exposure time compared with RS.}
    {Coding strategies in this category require no information about the data. They usually generate full rank matrices as $\B{M}$. In this project, we chose Raster Scan (RS), Binary Random Basis (BR), and Hadamard Basis (HB) for performance evaluation. $\B{M}$ for RS is an identity matrix while it is a Hadamard matrix for HB. BR employs random values for each pixel in the mask, i.e  $\B{M} \in \{0,1\}^{N\times N}$. On top of these strategies, we also include Impulse Imaging (II) which works as a golden standard in this project. II is an ideal case assuming it can capture the whole data spectrum simultaneously. Mathematically, the sensing matrix $\B{M}$ of II is also an identity matrix, but each mask in II has $N$ times longer exposure time compared with RS.}
    \item \textbf{Reconstruction prior} (data is moderately compressible). \Xpolish{Coding strategies in this category are based on the fact that noise has greater influence on the high-frequency components in any images. We used Low-Frequency-Hadamard Basis which only keeps the low frequency vectors in a Hadamard matrix to study this prior. In this paper, it is labeled as Truncated Hadamard (TH).}
    {One common strategy in coding for images is to take advantage of the fact that noise tends to have a greater impact on high-frequency components. To investigate this phenomenon, we utilized a Low-Frequency-Hadamard Basis approach, in which only the low-frequency vectors of a Hadamard matrix are retained. This approach is referred to as Truncated Hadamard (TH) in this paper.}
    \item \textbf{Task-Specific prior or Selective Sensing prior} (data is extremely compressible). \Xpolish{Coding strategies requires much information from the data and tasks, and their corresponding sensing matrices $\B{M}$ vary from task to task. One strategy in this category is called Hardware PCA (Principal Components Analysis). It computes the most principal components in the training data, and constructs the masks with these components. Another interesting strategy is Selective Sensing (SS) whose sensing matrix $\B{M}$ is not pre-defined nor fixed, but is optimized within a Neural Networks model during the training. This optimization method is called Optical Neural Networks (ONN) in this project.}
    {Coding strategies are an essential component of signal processing that involve gathering task-specific information from data. In order to design these strategies, sensing matrices $\B{M}$ are developed, which can vary depending on the task at hand. One such approach is the Hardware Principal Components Analysis (PCA) technique, which identifies the most principal components in the training data and uses them to construct masks. Another strategy\Xhide{ is the Selective Sensing (SS), which} uses a sensing matrix $\B{M}$ that is not pre-defined or fixed, but rather is optimized using a Neural Networks model during training. In this project, this optimization method is referred to as Optical Neural Networks (ONN).}
\end{itemize}

\Xpolish{In this study, we examined the impact of various priors on a single-pixel imaging system in terms of\Xhide{ both \textbf{reconstruction}} and classification. Firstly, we explained our the methods for imeplementing the constraints in the single-pixel imaging model. Then, we simulated the effect of different prior types on classification performance using the MNIST dataset. Subsequently, we tested the feasibility of these strategies by experiments. \Xhide{Specifically, we first conducted a theoretical analysis to compare the reconstruction capabilities of the HB and RS in the presence of noise. Subsequently, we simulated the effect of different prior types on classification performance using the MNIST dataset. \bnote{is our theoretical analysis novel?} \rnote{I thought it was novel before read the hadamard book}}}
{In this study, we examine the impact of various priors on a single-pixel imaging system in terms of classification. Firstly, we explain our methods for implementing constraints in the single-pixel imaging model. Then, we simulate the effect of different prior types on classification performance using the MNIST dataset. Finally, we test the feasibility of these strategies through experiments.}



\Xhide{\bnote{The first and second sentences here are somewhat confusing due to the "Proposed model" language. We have not proposed a model yet, so unclear if it is our model or someone else's.} \rnote{This model is proposed by Rebecca, but I believe there is some error on the negative entries} In equation \ref{eqn:NoiselessMeasurement}, the parameter regarding the number of photons is an integral part of the mathematical representation of $\B{x}$. To further examine the mask basis under varied light levels, we propose a revised photon-counting model that decouples $\B{M}$ from the number of photons. In the following sections, we detail the development of this new model and provide insights into its practical implementation to avoid violating these constraints.} 

\subsection{Constraints Implementation}

\Xpolish{This section introduces our methods for the optical implementations for the model. To construct a valid model, the Flux-preserving and Positivity-preserving constraints has been satisfied by the following techniques.}
{In this section, we outline our methods for implementing the model optically. In order to ensure that our model is valid, we have implemented techniques that satisfy both the Flux-preserving and Positivity-preserving constraints. By using these techniques, we can ensure that the model accurately represents the underlying physical processes and produces results that are both reliable and interpretable.}

\subsubsection{Photon Distribution Factor} \label{sssection:PDF}
\Xhide{\bnote{A: This section shouldn't be here}}

\Xpolish{The optimization of $\B{M}$ can be challenging, as it is subject to constraint \ref{constraint:photonNumber}.\Xhide{ Specifically, $\B{M}$ must belong to a subset $S$ of $\IR^{m \times N}$, which can vary depending on the specific case \bnote{<- not sure why this sentence is here}. To address this issue \bnote{the issue of belonging to a subset?}, } Here, we propose using a scale factor as a solution. This allows us to select any values for $\B{M}$ without being limited by the constraint \ref{constraint:photonNumber}\Xhide{\bnote{It would be nice to name the constraints}}. The Photon Distribution Factor, $\lambda$, is thus introduced to ensure compliance with constraint \ref{constraint:photonNumber}. As a scale factor, it is calculated as \Xequa{\lambda = \frac{\flux \totexposure}{N \sum_{k=1}^m v_k}},\Xhide{ \bnote{this should be in an equation line}} where $\flux$ represents the number of photons per second projected onto the DMD, $\totexposure$ is the total exposure time for all masks, $N$ represents the number of pixels, and $v_k$ represents the greatest value of the $k_\text{th}$ mask. The maximum value $v_k$ is selected as its corresponding pixel is open for longer than any other one in the $k_\text{th}$ mask, and the exposure time of this mask depends solely on $v_k$. Furthermore, the total exposure time $\totexposure$ is proportional to $\sum_{k=1}^m v_k$ and the total number of photons from the field of view is $\totexposure \sum_{i=1}^N \frac{\flux}{N} x_i$.  }
{Optimizing $\B{M}$ subject to constraint \ref{constraint:photonNumber} can be challenging. To overcome this limitation, we propose the use of a scale factor, known as the Photon Distribution Factor $\lambda$, which allows us to select any values for $\B{M}$ without violating constraint \ref{constraint:photonNumber}. The Photon Distribution Factor is calculated as \Xequa{\lambda = \frac{\flux \totexposure}{N \sum_{k=1}^m v_k}}, where $\flux$ is the number of photons per second projected onto the DMD, $\totexposure$ is the total exposure time for all masks, $N$ is the number of pixels, and $v_k$ is the greatest value of the $k_\text{th}$ mask. The maximum value $v_k$ is chosen as its corresponding pixel is open for longer than any other pixel in the $k_\text{th}$ mask, and the exposure time of this mask depends solely on $v_k$. Moreover, the total exposure time $\totexposure$ is proportional to $\sum_{k=1}^m v_k$, while the total number of photons from the field of view is $\totexposure \sum_{i=1}^N \frac{\flux}{N} x_i$.}

\Xhide{\rnote{Maybe in appendix} \bnote{ Could just state the first and last lines of the math and move the proof to the appendix but still state this ensures constraint one is true}
% Suppose we use an arbitrary matrix $\{\B{a}_1, \dots, \B{a}_m \} \in \IR^{N \times m}$ for measurement, the total number of photons detected is
When an arbitrary measurement matrix, $\{\B{a}_1, \dots, \B{a}_m \} \in \IR^{N \times m}$, is used, the total number of detected photons can be expressed as $\sum_{i=1}^m \lambda \B{a}_i^\top \B{x}$ and the following inequality always holds:
\begin{equation}
\begin{aligned}
    \sum_{i=1}^m \lambda \B{a}_i^\top \B{x} &\le \sum_{i=1}^m \lambda \sum_{j=1}^N v_i x_j\\
    &= \sum_{i=1}^m \sum_{j=1}^N \frac{\flux \totexposure}{N \sum_{k=1}^m v_k} v_i x_j \\
    &= \frac{\flux \totexposure}{N} \sum_{j=1}^N x_j  \frac{\sum_{i=1}^m v_i}{\sum_{k=1}^m v_k} \\
    &= \totexposure \sum_{j=1}^N \frac{\flux}{N} x_j
\end{aligned}.
\end{equation}
% Therefore, by the Photon Distribution Factor, the constraint \ref{constraint:photonNumber} is never violated.
}

\Xpolish{By utilizing the Photon Distribution Factor, constraint \ref{constraint:photonNumber} is guaranteed to be never violated. Then we can re-formulate model in Equation \ref{eqn:NoiselessMeasurement} as the following equation
 \begin{equation} \label{eqn:NoisyMeasurement}
    \begin{aligned}
        \tilde{\B{y}} &=  \frac{1}{\lambda}f_r(\lambda \B{ M x})
    \end{aligned}
\end{equation}
, where $f_r$ generates random numbers with expectation $\lambda \B{ M x}$ to simulate the noise and gives rise to the measurement with noise \bnote{Be consistent with language throughout the text. "noisy-version-measurement" is probably not the best name} $\tilde{\B{y}}$.}
{By utilizing the Photon Distribution Factor, we can guarantee that constraint \ref{constraint:photonNumber} is never violated. With this in mind, we can re-formulate the model in Equation \ref{eqn:NoiselessMeasurement} into the following equation,
\Xequa{\tilde{\B{y}} &=  \frac{1}{\lambda}f_r(\lambda \B{ M x})}.
Here, $f_r$ generates random numbers with an expected value of $\lambda \B{ M x}$ to simulate noise and produce a measurement with noise, denoted as $\tilde{\B{y}}$.}

\subsubsection{Dual-Rail Technique for Negative Entries}\label{sssection:DualBranch}
\Xpolish{\Xhide{\ynote{this could be part of a methods section later on or go to the supplement. Shouldn't be here.}\bnote{Consistent language "Rail" or "Branch" or "Path"} }
The dual-rail technique is introduced for constraint \ref{constraint:negativeEntry} \cite{neifeld2003dual}. In it\Xhide{ \bnote{ might be better to call it a "technique"}}, one measurement needs two complementary branches where one measures with all positive entries, $\B{M}^+$, and the other measures with all negative entries, $\B{M}^-$ \cite{neifeld2003dual}. }
{To satisfy constraint \ref{constraint:negativeEntry}, the dual-rail technique \cite{neifeld2003dual} employs two complementary measurement branches: one measures with all positive entries, denoted as $\B{M}^+$, while the other measures with all negative entries, denoted as $\B{M}^-$. By doing so, the dual-rail technique effectively converts the constraint on negative entries to a constraint on the difference between the measurements made by the two branches.}

% Figure environment removed


\Xpolish{\bnote{Many of these equations should be on their own line for readability and to reference them later. We have 11 pages so we can do this easily.}
To create the two branches, the mask $\B{M}$ should be split into $\B{M}^+ = \max(\B{M}, 0)$ and $\B{M}^- = \max(-\B{M}, 0)$. The superscripts of the parameters indicate the branches they belong to. Intuitively, we have $\lambda \tilde{\B{y}} = \lambda \tilde{\B{y}}^+ - \lambda \tilde{\B{y}}^-$ where $\tilde{\B{y}}^+$ and $\tilde{\B{y}}^-$ are photon counts with noise measured by $\B{M}^+$ and $\B{M}^-$ respectively. The two-rail trick also changes the expression of the  Photon Distribution Factor. In the case that each branch has a PMT, 
\Xequa{\lambda = \frac{\flux \totexposure}{N \sum_{k=1}^m |v_k|}}
where $|\cdot|$ takes the absolute value. In the case of using only one PMT, it is worth noting that the Photon Distribution Factor changes to 
\Xequa{\lambda = \frac{\flux \totexposure}{N (\sum_{k=1}^m v_k^+ + \sum_{k=1}^m v_k^-)}}
as one PMT measures with $\B{M}_k^+$ and $\B{M}_k^-$ in turn to accomplish measuring $\B{M}_{k}$. \iffalse as there are approximately two times of measurements. \fi \rnote{figure}All the parameters in this updated Photon Distribution Factor are of the same meaning as in section \ref{sssection:PDF}.}{To implement the two branches, we split the mask $\B{M}$ into two components: $\B{M}^+ = \max(\B{M}, 0)$ and $\B{M}^- = \max(-\B{M}, 0)$, with the superscripts indicating the respective branches. This enables us to express $\lambda \tilde{\B{y}}$ as the difference between the photon counts with noise measured by $\B{M}^+$ and $\B{M}^-$, denoted $\lambda \tilde{\B{y}}^+$ and $\lambda \tilde{\B{y}}^-$, respectively. Notably, the two-rail approach also modifies the expression for the Photon Distribution Factor. In the case that each branch has a PMT, 
\Xequa{\lambda = \frac{\flux \totexposure}{N \sum_{k=1}^m |v_k|}}, 
where $|\cdot|$ takes the absolute value. In the case of using only one PMT, it is worth noting that the Photon Distribution Factor changes to 
\Xequa{\lambda = \frac{\flux \totexposure}{N (\sum_{k=1}^m v_k^+ + \sum_{k=1}^m v_k^-)}},
since one PMT measures with both $\B{M}_k^+$ and $\B{M}_k^-$ sequentially to obtain the measurement result for $\B{M}_{k}$. All the parameters in this updated Photon Distribution Factor are of the same meaning as in section \ref{sssection:PDF}, and $v_k^+$ and $v_k^-$ stand for the maximum values of $\B{M}^+$ and $\B{M}^-$ respectively. 
} 

\subsection{Simulated Data Generation}

\subsubsection{MNIST Data Pre-processing}

\Xpolish{The MNIST dataset consists of handwritten digits from 0 to 9, with a default size of 28 by 28 pixels \cite{lecun1998MNIST}. In order to align the data with the Hadamard matrices, we padded the images with black pixels at the edges to resize them to 32 by 32 pixels \cite{lecun1998MNIST}. We then transformed the range of all pixel values from $[0,1]$ to $[0.3, 1]$. This operation enhanced the persuasiveness of the dataset for various noise models, as the added black pixels do not generate Poisson noise, whose variance is proportional to the expected photon counts. \bnote{did we ever vary the dark level (0.3)/ have results showing its effect?}}
{The MNIST dataset, which comprises handwritten digits ranging from 0 to 9, has a default size of 28 by 28 pixels \cite{lecun1998MNIST}. To align the data with Hadamard matrices, we added black pixels to the edges of the images and resized them to 32 by 32 pixels \cite{lecun1998MNIST}. Subsequently, we rescaled the pixel values from the original range of $[0,1]$ to $[0.3, 1]$. This rescaling was aimed at improving the dataset's suitability for different noise models, as the initial black backgrounds do not introduce Poisson noise, which is typically proportional to the expected photon counts.}

\subsection{Spectral Datasets}





% \rnote{How well this model can work w/o noise}

\subsubsection{Performance Evaluation}


\Xpolish{The performances of the models were evaluated through their average classification rates. To ensure a thorough assessment, the model was tested five times independently, each time with a randomly split dataset for 70\% training and 30\% validation. During each assessment, the model generated noisy photon counts, which were then used to train the classifier until the validation rate reached a plateau or declined. The best validation rate achieved during the training process was chosen as the representative performance upper bound for each assessment. The overall performance of the model was determined by averaging the results of all five assessments.}
{The model performances were assessed based on their average classification rates, which were calculated after five independent tests. In each test, the dataset was randomly split into 70\% for training and 30\% for validation. The model generated noisy photon counts during each assessment, and the training continued until the validation rate reached a plateau or declined. The highest validation rate obtained during the training process was considered as the representative upper bound for each assessment. The overall model performance was determined by averaging the results from all five assessments. This comprehensive evaluation ensured the reliability and accuracy of the model performance.}

\subsection{Experiments}

\Xpolish{Besides the simulations, an experiment is also conducted to enhance our conclusion. The DMD we use is the DLP 7000 purchased from Texas Instruments and the PMT is the PMA Series from PicoQuant. This experiment consisted of a hardware-end data capture and a software-end classifier training. \Xhide{The steps are introduced in the following sections.}}
{To further support our conclusions, we conducted an experiment in addition to simulations. The experiment utilized a DMD (digital micromirror device), specifically the DLP 7000 model from Texas Instruments, and a PMT (photomultiplier tube) from PicoQuant, specifically the PMA Series. The experimental setup involved both hardware-end data acquisition and software-end classifier training.}

% \subsubsection{Data preparation}

\Xpolish{Before data acquisition, we transformed and re-calibrated the MNIST images into what the PMT exactly "observed", and trained the classifier with this new data. This step is important if we don't use the hardware-end data for training.}
{To ensure that the classifier was accurately trained using the PMT data, we performed a transformation and re-calibration of the MNIST images to match what the PMT would actually "observe" during data acquisition. This step is crucial when training the classifier without the use of hardware-end data. By implementing this transformation and re-calibration process, we were able to accurately simulate the imaging conditions that the PMT would encounter during actual data acquisition, allowing for more reliable and robust classification results.}

\Xpolish{First, we conducted a Raster scan on a sample from MNIST data and each mask in this scan was exposed for 1 second. The light level had been adjusted and the brightest pixel in this experiment generated about 1000 photon records per second. The captured data vector was then reshaped into a 32 by 32 image, which can be treated as a linearly transformed version of the raw image.}
{First, we performed a Raster scan of a sample from the MNIST dataset, with each mask exposed for 1 second. The light intensity was adjusted to ensure that the brightest pixel generated approximately 1000 photon counts per second. The data captured during this experiment was then reshaped into a 32 by 32 image, which can be regarded as a linearly transformed version of the original raw image.}

% Figure environment removed

\Xpolish{Then, we estimated the linear transformation between the digital data and exprimental data, and then transformed all digital data in the same way so that the new data are similar to the sensor's observation. This transformed data was then input into the code modules used for simulation to train the software-end classifier. For the selective sensing, the masks were also trained in this step.}
{Then, we estimated the linear transformation between the digital and experimental data to ensure that the digital data closely matched the sensor's observations. After applying this transformation to all digital data, we used it to train the software-end classifier through simulation. Additionally, in this step, the masks used for Selective Sensing were also trained.} \YLnote{The masks with decimals use the fraction of a super pixel \cite{spall22hybrid_training} The ONN training is called \textit{in silico} method (Wright et al) \cite{spall22hybrid_training, wright2021backprop}}

\Xpolish{Finally, we uploaded the masks as the pattern sequences onto the DMD to acquire the data $\noisy{\B{y}}$. The testset consisted of ten images and each handwritten number appeared exactly once. The exposure time for each mask was 1 second. Since all photon records preserved the arrival time, we could change the noise level by choosing random intervals during the whole exposure time and counting the photons within it. }
{After generating the mask pattern sequences, we uploaded them onto the DMD to acquire the data, denoted as $\noisy{\B{y}}$. Our test set consisted of ten handwritten images, each of which appeared exactly once during the experiment. To control the noise level in the data, we varied 
% the number of photons recorded within 
lengths of random intervals throughout the 1-second exposure time for each mask. By preserving the arrival times of all photons, we were able to systematically adjust the level of noise in the acquired data.}


% \bnote{The ONN should be introduced here first, then show the results in reconstruction}


