% \IEEEPARstart{I}{n} this project, we analyzed the influence of different priors on a single-pixel imaging system regarding both reconstruction and classification and propose the inductive bias for single-pixel imaging. The first step was a theoretical investigation to compare the reconstruction performances of HB and RS in both noise models. Then, we investigated the roles of different prior types in classification on the MNIST dataset by simulations.

\IEEEPARstart{I}{n} this study, we examined the impact of various priors on a single-pixel imaging system in terms of both reconstruction and classification. Specifically, we first conducted a theoretical analysis to compare the reconstruction capabilities of the HB and RS in the presence of noise. Subsequently, we simulated the effect of different prior types on classification performance using the MNIST dataset.

% if have experiment, list here

% \subsection{Physical Constraints and Solutions}

% There are two main physical constraints for the implementation of equation \ref{eqn:NoiselessMeasurement} optically \cite{neifeld2003dual}. Firstly, negative values of the masks $\B{M}$ cannot be implemented directly \cite{neifeld2003dual}. Secondly, the single-pixel imaging model concerns the re-distribution of available photons among masks \cite{neifeld2003dual}. When optimizing $\B{M}$, no photons should be created by improper entries.

% \subsubsection{Dual-Branch Trick for Negative Entries}

% Although we can obtain a negative number of photons from the difference between the two measurements, the Poisson noise cannot be directly applied to these values. Otherwise, it violates the nature of Poisson distributions and the fact that Poisson noise appears in the sensors. On the contrary, noise should be considered in both measurements.

% Supposing we split the masks $\B{M}$ into the positive part $\B{M}^+$ and the negative part $\B{M}^-$, the final measurement $\lambda \tilde{\B{y}} = \lambda  \B{M}^+\B{x} - \lambda (1 - \B{c})\odot  \tilde{\B{y}}^-$. The positive and negative measurement obeys two independent distributions and the coefficient $\B{c}$ is not necessarily to be $\frac{1}{2}$. The determination of $\B{c}$ will be discussed in the next section.

% In the AWGN model, the measurement $\lambda \tilde{\B{y}} \sim \mathcal{N}(\lambda \B{Mx}, 2\sigma \B{I})$ with the dual-branch trick. But in the Poisson noise model, the measurement $\lambda \tilde{\B{y}} \sim \text{Skellam}(\lambda \B{Mx}, \diag\{\lambda \B{M}^{|\cdot|}\B{x}\})$. The superscript $|\cdot|$ represents taking the elementwise absolute values of the matrix.


% \subsubsection{Photon Distribution on Masks}








% In the derivation, we decided to use a Gaussian noise $\mathcal{N}(x,x)$ to approximate $\mathcal{P}(x)$.

\subsection{Simulations}

% To interpret the roles of the different priors in reconstruction and classification, we conducted a bunch of simulations on the MNIST dataset. The model for all the simulations consists of a homemade scanner module mimicking the single-pixel imaging and an Artificial Neural Networks (ANN) classifier. Notably, the ONN's scanner and classifier are optimized together. 

% A noise layer is implemented at the end of the scanner to generate noise photon counts $\tilde{\B{y}}$. The random number functions, in principle, do not have gradients. Studies on the \textit{score function estimator method} suggest that their gradients can be restored by \textit{Monte Carlo gradient estimation} \cite{shakir2020scorefunction}.

To understand the impact of different priors on reconstruction and classification, we conducted a series of simulations using the MNIST dataset. Our model included a custom scanner module that simulated single-pixel imaging and an Artificial Neural Network (ANN) classifier. The scanner and classifier were optimized together using the ONN approach.

To simulate noise, we included a noise layer at the end of the scanner that generated photon count estimates $\tilde{\B{y}}$. While random number functions typically do not have gradients, we utilized the score function estimator method and Monte Carlo gradient estimation to restore these gradients, as demonstrated in previous research by Shakir et al. \cite{shakir2020scorefunction}.


\subsubsection{MNIST Data Pre-processing}

% The MNIST dataset includes handwritten numbers from 0 to 9 with default size of 28 by 28. To reconcile the data to the Hadamard matrices, we padded black pixels at the edges of each image to resize it into 32 by 32. Afterward, we adjusted the range of all the pixel values from $[0,1]$ to $[0.3, 1]$. Since the black pixels never invoke Poisson noise of which the variance is proportional to the expected photon counts, this operation renders the dataset more persuasive for different noise models.

The MNIST dataset consists of handwritten digits from 0 to 9, with a default size of 28 by 28 pixels. In order to align the data with the Hadamard matrices, we padded the images with black pixels at the edges to resize them to 32 by 32 pixels. We also transformed the range of all pixel values from $[0,1]$ to $[0.3, 1]$. This operation enhanced the persuasiveness of the dataset for various noise models, as the added black pixels do not generate Poisson noise, whose variance is proportional to the expected photon counts.

% training details, how to evaluate the classification
\subsubsection{Scanner}

% The scanner module is modified from the \textit{Linear} module in PyTorch. In addition to the input and mask matrix, it also requires the total number of available photons and noise models as the arguments. The tricks mentioned in section \ref{sec:3_background} are implemented inside. Photon Distribution Factor $\lambda$ is updated whenever the mask is changed. The last sub-module takes the noiseless photon counts and returns its noisy version. The scanner can also reconstruct the field of view when the measurement is finished. 

In order to improve the performance of our scanner module, we have modified the \textit{Linear} module from PyTorch. The modified module takes in three primary arguments: the input and mask matrix, as well as the total number of available photons. Additionally, it requires the implementation of certain tricks, as described in Section \ref{sec:3_background}, in order to optimize its performance.

One key aspect of our modified module is the incorporation of the Photon Distribution Factor $\lambda$, which is dynamically updated whenever the mask is altered. This ensures that the scanner can accurately simulate the photon distribution throughout the measurement process.

Finally, the modified module also includes a sub-module that takes in the noiseless photon counts and returns a noisy version, replicating the effects of real-world measurement noise. Upon completion of the measurement, the scanner is also able to reconstruct the field of view with high accuracy.

\subsubsection{Classifier}

% The classifier is an ANN model including two hidden layers with 40 and 128 nodes. It takes the re-centered normalized noisy photon counts as the input and predicts its corresponding number. The re-centered normalized noisy photon counts is calculated by $\tilde{\B{y}} - \B{M} \B{\bar{x}}_T$ where $\B{\bar{x}}_T$ is the average of the training data. After each hidden layer, there is a dropout layer followed by a ReLU activation layer.
In this study, we employed a two-hidden layer artificial neural network (ANN) classifier with 40 and 128 nodes as the main model for predicting the corresponding number of re-centered, normalized, and noisy photon counts. The input data for the classifier was obtained by subtracting the average of the training data, represented by $\B{\bar{x}}_T$, from the re-centered normalized noisy photon counts, denoted by $\tilde{\B{y}}$. To prevent overfitting and improve the generalizability of the model, dropout layers were inserted after each hidden layer, followed by a ReLU activation layer.

% run a comparison show that number of parameters doesn't change a lot? use reconstructed and compare with the direct ones

\subsubsection{Performance Evaluation}

% The models' performances were evaluated by their average classification rates. For each strategy, the model was assessed 5 times independently. After randomly splitting the dataset into training and validation sets in each assessment, the scanner of each model engendered the noisy photon counts. The classifier was trained on the corresponding re-centered normalized photon counts until the validation rate grew insignificantly or dropped. The best validation rate throughout all epochs was chosen as the representative performance for this assessment. Eventually, the model's overall performance referred to the average of all 5 representatives.

The performances of the models were evaluated through their average classification rates. To ensure a thorough assessment, the model was tested five times independently, each time with a randomly split dataset for training and validation. During each assessment, the scanner generated noisy photon counts, which were then used to train the classifier until the validation rate reached a plateau or declined. The best validation rate achieved during the training process was chosen as the representative performance for each assessment. The overall performance of the model was determined by averaging the results of all five assessments.

\subsubsection{Reparameterization Trick for Poisson Noise}

% include a figure
% Unlike the static strategies, the ONN can optimize the masks by the back-propagation during the training. This method has several advantages.
% \begin{enumerate}
%     \item The model can find the most optimal scanner for the classifier.
%     \item The model can be more robust to the noise since the noise in each epoch is different. 
% \end{enumerate}
% % include the gradient method
% As the main problem of this strategy, it is crucial to determine a proper gradient for the stochastic process. In our observation, the score function estimator method was worse than the method in which the gradient of the noise layer was always 1. Though no proof shows the latter is the optimal, it can be considered a lower bound for the ONN's performance.

One key advantage of the ONN is its ability to optimize the masks through back-propagation during training. This dynamic approach allows the model to identify the optimal scanner for the classifier and to become more resilient to noise, as the noise in each epoch is distinct. However, determining a suitable gradient for the stochastic process remains a challenge. In our experiments, we found that using the score function estimator method resulted in inferior performance compared to using a constant gradient of 1 for the noise layer. While we cannot definitively prove that this latter approach is optimal, it serves as a lower bound for the ONN's performance.

% It is observed that the score function estimator method is unstable.

\subsection{Experiments}