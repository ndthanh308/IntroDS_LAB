% The very first letter of the paper is a 2 line initial drop letter
% followed by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.

\iffalse
\IEEEPARstart{T}{his} demo file is intended to serve as a ``starter
file'' for ICCP 2020 submissions produced under
\LaTeX~\cite{kopka-latex} using IEEEtran.cls version 1.8b and later.
\fi

% Coding is a fundamental technique used in computational imaging (CI) for image quality improvement \cite{cossairt2012does}. On top of increasing the light throughput, coding reduces the dimensionality of a measurement space where the reconstruction accuracy is a usual metric for the CI's performance. For applications such as single pixel imaging, spectroscopy, structured illumination imaging, time of flight imaging, and polarization imaging, coding enables a low dimensional sensor to sample in a \textbf{complete} measurement space.
% %an $n$-dimension measurement space.
% Due to its range of applications, coding schemes have been investigated thoroughly. Most implementations, however, are using very low noise scenarios or additive Gaussian noise models. The rationale is that additive Gaussian noise is a good model for the readout noise introduced by the imperfect measurement of cameras and sensors. This AGN is easy to model analyticlly and computationally since it is a signal independent linear additive noise. As optical imaging technologies improve, readout noise is decreasing and Poisson noise to to the quantized nature of light is dominating measured imaging data. While readout noise is a technical problem introduced by an imperfect measurement, quantization noise is a fundamental physical property of light. In other words, since light is actually composed of photons that hit the sensor at discrete times, the light flux property we are trying to measure and compute on exists only as an ensemble average over a Poisson distribution and is not in that sense a "real" quantity. In state of the art and future sensors measuring visible light, it is therefore important to consider measurement and processing techniques under Poisson noise. It has been shown in multiple foundational publications, that coding behaves fundamentally different under Poisson noise than it does under the AGN. This problem becomes particularly pronounced in emerging single-photon sensors that only suffer from Poisson noise. Therefore, the previous conclusion would imply that existing techniques of coding/computational imaging are not applicable  single-photon or "few-photon" sensors (i.e. sensors where the readout noise is on the order of just a few photons, such as most state for the are sCMOS and machine vision cameras). 

% In this paper we analyze the performance of coding techniques using simple toy applications, compare the behavior of popular approaches for AGN and Poisson noise and develop coding strategies that work with Poisson noise. Similar to \cite{cossairt2012does} we find that coding has negligible performance gains when the task is \textit{image reconstruction}. However, when performance gain is measured for downstream tasks, coding with codes that are adapted to the downstream tasks can provide non-trivial performance gains and in some cases recover the performance of an optimal measurement performed with a high dimensional sensor.

% Coding is a widely-used technique in computational imaging (CI) that aims to improve image quality and reduce the dimensionality of the measurement space \cite{cossairt2012does}. It has been shown to be effective in a variety of applications, including single pixel imaging, spectroscopy, structured illumination imaging, time of flight imaging, and polarization imaging. These techniques rely on coding to allow low-dimensional sensors to sample a complete measurement space. While coding schemes have been widely studied, most research has focused on low noise scenarios or additive Gaussian noise models. This is due to the fact that additive Gaussian noise, which is introduced by imperfections in cameras and sensors, is easy to model both analytically and computationally as it is a signal-independent linear noise.



\IEEEPARstart{C}{oding} or multiplexing is a widely used technique in computational imaging (CI) for improving image quality \cite{cossairt2012does}. In addition to increasing light throughput, coding can reduce the dimensionality of the measurement space, which is a common metric for the performance if a CI method. Coding allows low-dimensional sensors to sample a complete measurement space in applications such as single pixel imaging, spectroscopy, structured illumination imaging, time of flight imaging, and polarization imaging.

While coding schemes have been extensively researched, the mathematical foundation is based on low noise scenarios or Additive Gaussian Noise (AGN) models. This is because the AGN is a linear noise model that allows for rigorous mathematical treatment. It is a good model for the readout and background noise introduced by imperfect measurements in cameras and sensors, and is easy to analyze and compute analytically as it is signal-independent and linearly additive.

However, as optical imaging technologies improve, readout noise is decreasing and Poisson noise, due to the quantized nature of light, is becoming more dominant in measured imaging data. Poisson noise is a fundamental physical property of light and arises from the discrete nature of light. An optical signal is composed of discrete photons that can not be arbitrarily divided across codes. Therefore, in state-of-the-art and future sensors measuring visible light, it is important to consider measurement and processing techniques under Poisson noise.

Previous research has shown that coding behaves substantially different under Poisson noise compared to the AGN \cite{duarte2008CS}, which is particularly significant in emerging single-photon sensors that only experience Poisson noise. This means that existing coding and computational imaging techniques may not be applicable to single-photon or "few-photon" sensitive sensors (such as most state-of-the-art CMOS and machine vision cameras) where readout noise is on the order of just a few photons.

In this paper, we analyze the performance of coding techniques using toy applications in single-pixel-imaging, compare the behavior of popular approaches for both additive Gaussian and Poisson noise, and develop coding strategies that work under Poisson noise. Similar to the conclusion of Cossairt et al., we find that coding provides little benefit in unbiased prior free image reconstruction tasks, but can provide significant performance gains \cite{cossairt2012does} when codes are optimized specifically for downstream vision tasks.


\iffalse
\textbf{New paper story:}
\begin{enumerate}
    \item Coding is one of the fundamental techniques used in computational imaging (CI) for image quality improvement \cite{cossairt2012does}. On top of increasing the light throughput, coding reduces the dimensionality of a measurement space where the reconstruction accuracy is a usual metric for the CI's performance.\bnote{Insert definition of computational imaging, probably similar to the one of \cite{cossairt2012does}}.
    \item 
    Coding is important in any application where an n dimensional measurement space is sampled with a sensor or dimensionality $<$ n. It is relevant in single pixel imaging, spectroscopy, structured illumination imaging, time of flight imaging, polarization imaging, compultational imaging, and compressed sensing.
    \item Due to its range of applications coding schemes have been explored thoroughly. Most implementations, however, are using very low noise scenarios or additive Gaussian noise models. The rationale is that additive Gaussian noise is a good model for the readout noise introduced by the imperfect measurement of cameras and sensors. This AGN is easy to model analyticlly and computationally since it is a signal independent linear additive noise. 
    \item As optical imaging technologies improve, readout noise is decreasing and Poisson noise to to the quantized nature of light is dominating measured imaging data. While readout noise is a technical problem introduced by an imperfect measurement, quantization noise is a fundamental physical property of light. In other words, since light is actually composed of photons that hit the sensor at discrete times, the light flux property we are trying to measure and compute on exists only as an ensemble average over a Poisson distribution and is not in that sense a "real" quantity. \item In state of the art and future sensors measuring visible light, it is therefore important to consider measurement and processing techniques under Poisson noise.
    \item It has been shown in multiple foundational publications, that coding behaves fundamentally different under Poisson noise than it does under AGN.
    % \item For example, recent work explored the regimes where coding/computational imaging can lead to improved performance \cite{cossairt2012does}. The conclusion of their analysis was that coding/computational imaging can improve performance in scenarios where the dominant noise source is due to Gaussian noise often associated to read-out noise. However, when the dominant noise source is Poisson noise, due to the random arrival of photons, the performance gains are negligible. In other words: In a Poisson noise limited measurement, the best "code" is the trivial "pinhole" code, where each sample is scanned on it's own. 
    % \item Beccas paper.
    \item This problem becomes particularly pronounced in emerging single-photon sensors that only suffer from Poisson noise. Therefore, the previous conclusion would imply that existing techniques of coding/computational imaging are not applicable  single-photon or "few-photon" sensors (i.e. sensors where the readout noise is on the order of just a few photons, such as most state for the are sCMOS and machine vision cameras). 
    \item In this paper we analyze the performance of coding techniques using simple toy applications, compare the behavior of popular approaches for AGN and Poisson noise and develop coding strategies that work with Poisson noise. 
    \item Similar to \cite{cossairt2012does} we find that coding has negligible performance gains when the task is \textit{image reconstruction}. However, when performance gain is measured for downstream tasks, coding with codes that are adapted to the downstream tasks can provide non-trivial performance gains and in some cases recover the performance of an optimal measurement performed with a high dimensional sensor.
\end{enumerate}
\fi

\section{Outline (for planning only)}
Definition of coding, sensor dimensionality, measurement dimensionality

definition of signal reconstruction, imaging, and pattern recognition

\bnote{Single-pixel imaging.} 

\bnote{Compressive sensing} Compressive sensing has enabled the design of single-pixel imaging systems that are able to reconstruct images from very few measurements. 
These systems are useful in applications where it is expensive to have pixel arrays that sample the full image in parallel. 
For example in low-light applications, PMTs are commonly used, and it is expensive to build arrays of PMTs.

Additionally, compressive sensing can also be applied to other imaging modalities such as hyper-spectral imaging,  fluorescence imaging etc... 

\bnote{Signal reconstruction under Poisson noise.} The most efficient compressive sensing systems have been implemented for signal reconstruction under Gaussaian noise. 
Unfortunately, compressive sensing in the presence of Poisson noise does not benefit from the same low-sample complexity \cite{raginsky2011performance}.
This is due to XXX

\bnote{Signal reconstruction amplifies noise so we should skip it.} 
Signal reconstruction, however, is often an intermediate step. 
The final goal is often to execute a high-level task such as classification or segmentation.
To this end, current computer vision systems extract features from the reconstructed signal and use these features to accomplish the task at hand.
In this paper, we find that in a single-pixel imaging system under Poisson noise, it is sub-optimal to reconstruct from compressive measurements and then extract features.
This is because the signal reconstruction step amplifies noise.

\bnote{Task Specific Masks vs General Masks}
Should we adapt our masks based on our objective (pattern recognition task, image reconstruction domain, different types of signals)? The conventional wisdom for AGN is that that doesn't help much. 

Now we need to do analysis:
define toy problems, discuss performance for simple adaptive masks
conclusion: with Poisson noise signal reconstruction makes no sense. But pattern recognition works with an adapted mask set.

How do we find the masks:
PCA, ONN

\bnote{Optical neural nets to the rescue.} 
Inspired by recent trends in optical neural networks, we propose a single-pixel vision system for low-light applications. 
By capturing image features directly we avoid amplifying noise

\bnote{How to design these masks.} 

Discussion:
end to end computational imaging with adaptive asks works under Poisson noise.

This provides an additional motivation for proposed optical ANNs or ANNs with optical layers. Such an architecture allows me to choose where in the design I want to add the Poisson noise.

Possibly: Quantum mechanics interpretation.