
\textbf{Coding under Poisson noise}. In coded imaging, \Xhide{coding allows the capture of a two-dimensional image with a single-pixel sensor \cite{harwit1979hadamard}. }Hadamard matrices are considered the optimal coding scheme for multiplexing \cite{harwit1979hadamard, cossairt2012does, wuttig2005optimal, mitra2014can} in systems with only AGN, though this conclusion doesn't hold for Poisson noise \cite{mitra2014can}. \Xpolish{However, when Poisson noise is predominant, we should not use any coding \cite{harwit1979hadamard, cossairt2012does}. When the noise is Poisson distributed, coding with sparse priors such as compresses sensing are not recommended \cite{willet2009CSPoisson, willett2011poisson, vanden2019various}.}
{In many recent projects, optimization efforts focus on algorithmic enhancements for reconstruction, often maintaining the use of random coding strategies \cite{goyal2016performance, gibson2020single, liutkus2014imaging, candes2008introduction}\YLnote{\checkednote{need more here}}, though this type of matrices are not recommended under Poisson noise\cite{willet2009CSPoisson,  willett2011poisson, vanden2019various}.\Xhide{ While some approaches, such as Feature Specific Imaging (FSI), optimize sensing matrices based on learnable priors \cite{neifeld2003dual, neifeld2003FSI, neifeld2014optimizing}, these methods explicitly do not consider Poisson noise. Thus the challenge of optimizing masks under Poisson noise persists \cite{neifeld2014optimizing}.} \YLnote{Wuttig, Ratner, et al. \cite{wuttig2005optimal, ratner2007optimal} introduced Poisson noise-aware optimization techniques for multiplexing matrices, outperforming Hadamard in the presence of moderate signal-dependent noise. However, their analysis neglects data priors, leading to suboptimal matrices for greater Poisson noise \cite{mitra2014can}.} Additionally, attempts at matrix optimization focusing on minimal mutual coherence \cite{mordechay2014matrixRIP} lack a foundation in the Poisson noise model, as highlighted in this study.}



% \textbf{Task Specific Imaging}


\textbf{Feature Specific Imaging}. Feature-specific imaging (FSI) is a type of imaging system \Xpolish{that directly measures linear features of the object irradiance distribution, instead of forming a conventional image and then extracting features from it}{where sensing matrix $\B{M}$ incorporates linear features of expected data} \cite{neifeld2003FSI, neifeld2003dual}. This approach can provide higher feature fidelity and lower detector count than conventional imaging, especially for applications that require relatively few features \cite{neifeld2003FSI, neifeld2003dual, neifeld2014optimizing}. This technique can be viewed as a variant of compressed sensing, wherein the sensing matrix is determined based on prior information \cite{neifeld2014optimizing}. Nevertheless, \Xpolish{its performance and implications under Poisson noise conditions remain relatively unexplored in the existing literature, warranting further discussion and investigation}{\YLnote{current state-of-the-art} FSI is explicitly designed for AGN rather than Poisson noise\cite{neifeld2014optimizing}}. \YLnote{FSI can be regarded as a technique that leverages data driven priors, and Mitra et al. have asserted that employing data driven priors can enhance image recovery under Poisson noise \cite{mitra2014can}. Therefore, in our work, we underscore the promising prospects of extending feature-specific concepts to a broad spectrum of vision tasks under Poisson noise.}


\textbf{End-to-end optimization}. This method \Xpolish{refers to training hardware and software networks for image processing pipelines}{jointly optimizes camera design and digital signal processing to achieve optimal overall performance} \cite{diamond2021dirty, zhang2021deep, jacome2023middle}. \YLnote{In many previous projects, this idea was usually implemented \Xpolish{without considering Poisson noise}{without considering Poisson noise in cost functions} \cite{hinojosa2021learning, dun2020learned, metzler2020deep, chang2019deep, onzon2021neural, spall22hybrid_training, jacome2023middle, gibson2020single} or without optimizing masks \Xpolish{under Poisson noise during training}{with gradient incorporating Poisson distribution} \cite{tseng2021differentiable, diamond2021dirty, rego2022deep, duarte2008CS, nature2022ONN, gibson2020single, xu2020compressed}. Rego et. al froze the sensing matrix as a pinhole without optimizing it \cite{rego2022deep}}.
Wang et al. \cite{nature2022ONN} successfully implemented a neural network model for handwritten number classification on an optical device with limited photon budget, demonstrating the potential for AI-assisted optimization of coding schemes\Xhide{ in CI}. 
\YLnote{However, the Poisson noise was considered only in model testing where the most robust model was picked among a set of \Xpolish{hyper-parameter combinations}{different initialization seeds} \cite{nature2022ONN}. %supplementary note 13
Xu et al. \cite{xu2020compressed} examined the robustness of the proposed neural networks model to Poisson noise; however, the training process does not explicitly incorporate the gradients affected by Poisson noise.} Our contribution is developing a noise-\Xpolish{included}{aware} training approach within a neural network model to find the globally optimal masks under Poisson noise.

% \textcolor{blue}{Add section on feature specific imaging}



\iffalse

\YLnote{For the feature specific imaging, here are some paper to cite}

\YLnote{yizhou notes start}
\textbf{Mark A. Neifeld and Premchandra Shankar, "Feature-specific imaging," Appl. Opt. 42, 3379-3389 (2003)} Feature-specific imaging is a type of imaging system that directly measures linear features of the object irradiance distribution, instead of forming a conventional image and then extracting features from it. This approach can provide higher feature fidelity and lower detector count than conventional imaging, especially for applications that require relatively few features.  Poisson noise when the mean and variance of the photon count are equal. The paper shows that feature-specific imaging does not provide any advantage over conventional imaging in the presence of shot noise, because the shot-noise-limited SNR is inherent in the irradiance collected by the instrument and does not depend on the energy incident on any particular detector. Therefore, we can infer that feature-specific imaging would not work well with Poisson noise either, unless the mean and variance of the photon count are very different. \YLnote{However, that is where we come up with hardware PCA. We can then further reduce the text by citing Neifeld for the PCA part. }

\textbf{Abhijit Mahalanobis and Mark Neifeld, "Optimizing measurements for feature-specific compressive sensing," Appl. Opt. 53, 6108-6118 (2014)} "Although our analysis assumes that the noise is independent of the signal, this is not the case when Poisson noise is present. While a formal treatment of the effects of signal-dependent noise is beyond the scope of the paper, we compare the behavior of optimized masks to the normalized PCA in signal-dependent noise." It also has imcomplete future work "Other types of noise models will also be considered, such as signal-dependent shot noise." It seems it hasn't been studied after this paper. We could say that our contribution is designing a method to extend feature specific imaging in poisson noise model.

\YLnote{Interesting conclusions listed below}  

"FSI is a form of CS in which the measurement kernels are not random, but are based on prior knowledge of the information we are interested in sensing. Working in this FSI framework, we have developed a methodology for designing a set of masks that satisfy the photon constraint and are optimum for making measurements that minimize the reconstruction MSE in the presence of noise. "

"Of course, the primary reason for CS is to make fewer measurements than the number of pixels in the reconstructed image, and to collect the information more efficiently than a conventional image. One might intuitively expect that utilizing more features (or measurements) in the reconstruction process will yield a smaller reconstruction error. We demonstrated, however, that the photon constraint limits the number of masks that can be used at a particular SNR to reduce the reconstruction MSE. In noisy conditions, the MSE initially decreases as the number of measurements is increased, but then increases when measurements that contain more noise than signal information are included." \YLnote{It aligns with Rebecca's work.}

\textbf{W. Van den Broek, B. W. Reed, A. Béché, A. Velazco, J. Verbeeck and C. T. Koch, "Various Compressed Sensing Setups Evaluated Against Shannon Sampling Under Constraint of Constant Illumination," in IEEE Transactions on Computational Imaging, vol. 5, no. 3, pp. 502-514, Sept. 2019, doi: 10.1109/TCI.2019.2894950.}  show that for the investigated sensing matrices and in the absence of read-out noise, i.e. with only Poisson noise present, compressed sensing does not raise the amount of Fisher information in the recordings above that of a Shannon sampled signal.





\YLnote{yizhou notes end}

\ynote{andreas notes start}
Other refrerences:
M. Mordechay and Y. Y. Schechner, "Matrix optimization for poisson compressed sensing," 2014 IEEE Global Conference on Signal and Information Processing (GlobalSIP), Atlanta, GA, USA, 2014, pp. 684-688, doi: 10.1109/GlobalSIP.2014.7032205.
“Considering that the measurements are noisy, it is important to
avoid energy loss, since measurements with low energy have low
signal-to-noise ratio (SNR).”
We prove this assumption wrong in this paper ... \YLnote{ONN reconstruction}

Photon-noise: is a single-pixel camera better than point scanning? A signal-to-noise ratio analysis for Hadamard and Cosine positive modulation, Camille Scotté, Frédéric Galland1 and Hervé Rigneault
Published 7 June 2023 • © 2023 The Author(s). Published by IOP Publishing Ltd
Journal of Physics: Photonics, Volume 5, Number 3 Citation Camille Scotté et al 2023 J. Phys. Photonics 5 035003 DOI 10.1088/2515-7647/acc70b
Looking at SNR for different codes. Mostly confirms prior results. Finds that snr is improved for brightest pixels, but degraded for others. \YLnote{Only $x_i \ge k \bar{x} = \frac{k\sum_{j=1}^N x_j}{N}$ can get improved. $k$ is usually $2, 4, 16$, which means only bright pixels get reconstruction gain. E.g. cosmophotography. For normal images, that usually means the general performance degrades!}

D. Shin, J. H. Shapiro and V. K. Goyal, "Performance Analysis of Low-Flux Least-Squares Single-Pixel Imaging," in IEEE Signal Processing Letters, vol. 23, no. 12, pp. 1756-1760, Dec. 2016, doi: 10.1109/LSP.2016.2617329.
Finds bounds for noise. Probably agrees with paper above.
This is very interesting. AT very low rates the non negativity constraint can improve our estimate for multiplexing so it may become better than poisson. The paper doesn’t prove that it is better. Just says the proof for the general case is not valid and shows an experiment where an improvement is shown. Could we use the same constraint in convex optimization? What does this say about other constrained or regularized optimizations?
\ynote{andreas notes end}


\iffalse
better arrangement
coding under Poisson: mention hadamard book, ollie and becca's paper show that coding under Poiss is not ideal
mask optimization: people optimize the masks under wrong noise model, we proposed a better way for Poiss 

\fi


\YLnote{In summary, we can have a list of literature by their category
\begin{enumerate}
    \item Open more pixels doesn't help under Poisson noise (no compression): \cite{harwit1979hadamard, neifeld2003FSI, cossairt2012does, scotte2022photon_noise}
    \item Regularization doesn't help (usually comes with compression):
    \cite{willet2009CSPoisson, willett2011poisson, vanden2019various}. Their suggestion is to use fewer measurements as more measurements give rise to a greater reconstruction error upper bound. BTW, ONN are can have many measurements with low rank, which doesn't contradict to this conclusion.
    \item Feature specific imaging is designed for gaussian noise especially. It is a good way for feature preserving and reconstruction. No follow-up about the poisson. That is why we start this project.
    \item When doing a classification task, we give some higher compression by labelling. E.g, gender recognition. We only need one mask to tell the gender in the most ideal case. \textbf{It will be good to find some paper directly saying this.} As the data can be highly compressible, we can avoid dense measurements, and thus have lower error bound. Reconstruction is just a linear operation, so it doesn't matter a lot if we use Rebecca's conclusion about the upper bound, even though it is for reconstruction. 
        \begin{itemize}
        \item Think about reconstructing an image about the night sky. After using TV \textbf{re-binning}, we have two colors, black for background, and white for stars. It can be treated as a classification task as well! That is why rebecca proposed some algorithms with TV stuff: to minimize the possible amount of measurements.
    \end{itemize}
    Neifeld uses features learned from training data to do reocnstruction, that is how our idea of directly classification comes from. If we want to apply the ONN for reconstruction, it may be useful to read "RecDNN: deep neural network for image reconstruction from limited view projection data"
\end{enumerate}
}
\fi


\AVnote{\checkednote{Regularization, CS, might help but noise bounds are very unfavorable (cite becca). \\ In most of the papers, people optimize the algorithm. In only very few people try to optimize the masks. No paper where masks are optimize with poisson noise. \\ In this paper we are looking at how to optimize the measurement under poisson noise. \\ We find that optimizing the measurement can have profound improvements to the performance of single pixel vision approaching the perfonance of ideal mutipixel cameras in particular for Poisson noise dominated signals like visible light. }}


