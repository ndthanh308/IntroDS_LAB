\subsection{Model Configuration \ynote{\checkednote{this is just the ONN. We probably also want to say something about PCA}}} 
\Xpolish{Although coding strategies are different, they essentially share the same model structure shown in figure \ref{fig:model_config} for classification tasks. In this study, we built a classification pipeline consisting of a scanner module and a classifier in PyTorch. This model consists of a \textit{scanner} module and a \textit{classifier} module.\Xhide{The scanner is simulating the measurement process described in equation \ref{eqn:NoisyMeasurement} and the classifier is a neural networks model.} The scanner simulates the operations to count the photons, following the techniques\Xhide{ \bnote{"tricks" -> "techniques"}} mentioned in section \ref{ssec:ModelFormulation}. We\Xhide{ also \bnote{"Also" could mean we have two different models.}} employed a two-hidden layer artificial neural network (ANN) classifier with 40 and 128 nodes as the main model for predicting which number the object is.
%by the corresponding  number of re-centered, normalized, and noisy photon counts. 
% The input data for the classifier was obtained by subtracting the average of the training data, represented by $\B{\bar{x}}_T$, from the re-centered normalized noisy photon counts, denoted by $\tilde{\B{y}}$. 
To prevent overfitting and improve the generalizability of the model, dropout  was applied after each hidden layer, followed by a ReLU activation layer.  }
{All coding strategies in this project for classification tasks share the same fundamental model structure, as illustrated in Fig. \ref{fig:model_config}\Xhide{ and \ref{fig:ONN_architecture}}. \Xpolish{In this project, we extend the applicability of our model to three distinct tasks: image reconstruction\AVnote{This idea, which focuses on image reconstruction, is based on prior knowledge, such as signals being sparse in some linear space, with noise concentrated in the high-frequency domain. One example in this category is compressed sensing. Apr 10, 2024 12:38 PM}, handwritten number classification with the MNIST dataset, and classification of hyperspectral data.}{
\YLnote{In this project, we extend the applicability of our model to handwritten number classification with the MNIST dataset and classification of hyperspectral data.}
} To classify handwritten numbers using optical devices with limited photon budget, we built a PyTorch-based pipeline consisting of a \textit{scanner} module \YLnote{including $\B{M}$ and a noise generator} to simulate the acquisition of $\noisy{\B{y}}$ described in Eq. \ref{eqn:gaussian_measurement} and Eq. \ref{eqn:poisson_measurement}, and a \textit{classifier} module classifying the measured objects by $\noisy{\B{y}}$.\Xhide{ The scanner module simulates the photon counting process using the techniques described in section \ref{ssec:selectionofsensingmatrix}.} For the classification task, we utilized a two-hidden layer artificial neural network (ANN) with 40 and 128 nodes, serving as the primary model. To prevent overfitting and enhance generalizability, we applied dropout after each hidden layer, followed by a rectified linear unit (ReLU) activation layer.}
Once operational, this model can serve various purposes, such as object classification and signal recovery. For instance, it can process images of handwritten numbers, providing the likelihood of each image corresponding to a specific number by cross-entropy loss. 
Furthermore, the classifier can function as a decoder to recover the input image by MSE loss, which is adopted in section \ref{ssec:model_validation}.

\Xpolish{When training this model, the scanner computes the photon counts with noise, and feeds it to the classifier. The noise is sampled from a predefined noise distribution, and is different in different epochs. After the loss is computed, parameters in the classifier are optimized by gradient descent. The ONN is different from the others as the back-propagation is extended into the scanner so that the masks are optimized together with the classifier.}
{During the training of the model, the scanner computes the photon counts with noise and then transmits this data to the classifier. The noise is sampled from a pre-defined noise distribution and varies across epochs. Once the loss has been computed, gradient descent is used to optimize the parameters of the classifier. \YLnote{In addition, we propose an end-to-end optimization model called Optical Neural Networks (ONN).} \textbf{What sets the ONN apart from other \Xpolish{models}{coding schemes} is the learnable $\B{M}$ which is not predefined or fixed. In other words, back-propagation is extended into the scanner, allowing for the simultaneous optimization of the masks and the classifier {under different noise models}}. This approach offers distinct advantages compared to traditional methods, which typically optimize the classifier independently of the scanner {or optimize the scanner without the appropriate noise model}.}

% Figure environment removed



\YLnote{\checkednote{Another figure showing network}}

\Xpolish{ONN can also be used for reconstruction. \YLnote{To address this, we propose a simple approach of optimizing the ONN masks for a reconstruction task under different noise models. To achieve this, we developed a new ONN model with $m = N$ masks. The scanner takes raw image vectors $\B{x}$ and outputs $N$-element measured photon count vectors under a chosen noise model, $\noisy{\B{y}}$. The classifier is a single-layer network that takes $\noisy{\B{y}}$ as input and outputs $\noisy{\B{x}}$, also an $N$-element vector. The objective is to minimize the MSE between the input images and the output vectors, $\|\noisy{\B{x}} - \B{x}\|_2^2$. In noiseless cases, the classifier is equivalent to $\B{M}^{-1}$, where $\B{M}$ is the masks used in the scanner.}}{}


\subsubsection*{ONN Optimization under Poisson Noise} 

\Xpolish{Since the Poisson noise generating function has no gradient, it is challenging to optimize the scanner. The reparameterization trick is a useful method for the AGN instead of the Poisson noise. Our method is using a normal distribution whose variance equals its expectation to approximate the Poisson distribution in training, and therefore, the reprameterization trick can be employed.}
{Optimizing the scanner can be challenging as the Poisson noise generating function typically lacks a gradient. While reparameterization is a useful method for addressing AGN, it is not as effective for addressing Poisson noise. To address this limitation, we employ a method mentioned by Cossairt et al. that utilizes a normal distribution with a variance equal to its expectation to approximate the Poisson distribution during training \cite{cossairt2012does}.\ynote{\checkednote{we need more details on this and how it is implemented in the network. Probably a figure of the entire network structure and the equation for the gradient (i think we also have references to cite here)}} This approximation enables us to employ PyTorch builtin reparameterization method, i.e. \texttt{rsample}, and optimize the scanner more effectively under Poisson noise.}

% \YLnote{
% A seperate section? And mention that Ollie did that as well.  \\
% Similar to corssairt et al, we use a normal dist to approximate poisson
% }

% \YLnote{
% Transform $\B{y} \sim \mathcal{P}(\B{Mx})$ to $\B{y} \sim \mathcal{N}(\B{Mx}, (\B{Mx})^\frac{1}{2})$ to use the reparamaterization trick. The variance will also play a role in computing the gradient.
% } 

To approximate the Poisson noise by AGN, we can re-write the measurement with noise $\noisy{\B{y}} = \B{Mx} + \B{J}$ where $\B{J} =  (\B{Mx})^{\circ \frac{1}{2}} \odot \B{\epsilon}, \B{\epsilon} \sim \mathcal{N}(0, \B{I})$ \cite{shin2013low}. During the training under Poisson noise, $\B{J}$ also plays a role in gradient computation, which is the most significant difference compared with training under AGN. 

\textcolor{blue}{\checkednote{I think we can remove sections 4.2 and 4.3. They mostly repeat waht we already said and add too much detail for a paper.}}