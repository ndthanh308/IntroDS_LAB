\section{INTRODUCTION}
Imitation learning aims to train an intelligent agent to mimic expert demonstrations for a particular task. Various imitation learning instantiations, such as behavior cloning and inverse reinforcement learning, have been widely applied to fields including robotics \cite{Calinon2007IncrementalLO, Krishnan2018SWIRLAS}, autonomous driving \cite{Wang2021InverseRL, Kuefler2017ImitatingDB}, and optimal navigation \cite{hussein2018deep, shou2020optimal}. Imitation learning enables agents to learn from high-quality samples instead of exploring from scratch, leading to significantly higher learning efficiency when compared with reinforcement learning methods \cite{bojarski2016end}. This is especially important in safety-critical settings where reinforcement learning are difficult to execute \cite{yin2021imitation, pfrommer2022safe}. Even when the flexibility of reinforcement learning is desired, imitation learning can be used to accelerate the learning process \cite{Hester2017DeepQF}.

Despite its broad applicability, imitation learning exhibits an issue known as \textit{causal confusion} \cite{de2019causal}: the learned policy misattributes features which are primarily \emph{correlated} with expert actions as reflecting a \emph{causal} relationship \cite{kaddour2022causal}. This can manifest itself both through the observed features which are spuriously correlated with the expert actions (``nuisance variables'') as well as confounders which are available to the expert but not the imitator (``unobserved confounders''). We restrict ourselves to the former, although for completeness we include approaches addressing the latter in our work.

Consider an illustrative example of causal confusion adapted from \cite{de2019causal}. The task at hand is learning to drive a car from expert demonstrations. A behavior cloning agent is provided video observations from the driver's perspective, including a brake light on the dashboard. Although the learned braking policy is excellent on the supervised dataset, deployment performance is poor: the agent has effectively learned to brake when the brake light is on, instead of attending to other pedestrians or vehicles. In this case, the brake light is a ``nuisance variable,'' and we can dramatically improve the performance of the policy by covering the brake light and reducing information for the model.

Existing approaches for completely masking such nuisance variables generally require either a queryable expert or access to the expert reward function. The seminal work of \cite{de2019causal} introduced a $\beta$-Variational Auto Encoder ($\beta$-VAE) decomposition the observation space along with a joint policy parameterized by hypothetical causal structures. The space of causal structures can then be searched with two distinct algorithms, one leveraging expert queries and the other based on policy evaluations and reward feedback. The existence of nuisance variables was also noted \cite{ortega2021shaking} as part of a broader issue with sequential models that can be addressed with Dagger-style expert queries \cite{ross2011reduction}. The work of \cite{park2021object} partially addresses the nuisance variable problem by regularizing the learned policies to attend to multiple objects in the scene. While this approach does not require policy executions, it only weakens the learner's attention to a nuisance variable and does not eliminate it completely.

The complementary problem of unobserved confounders considers the setting where experts observe confounding variables that are inaccessible to the learner. In the car driving example, this might include a human driver listening to honking that is not detected with visual sensors. One exciting theoretical line of research in this area \cite{zhang2020causal, kumor2021sequential} presents causal-model derived conditions for imitability and an algorithm for imitating the expert policy when possible. However, these works make the strong assumption that the causal graph is provided to the imitation learning agent. Other efforts to apply causal inference techniques to the unobserved confounder problem either require strong assumptions, such as the knowledge of the expert reward \cite{etesami2020causal} and purely additive temporally correlated noise \cite{swamy2022causal}, or only evaluate simple multi-armed bandit problems \cite{vuorio2022deconfounded}.

This work focuses on the problem of observed nuisance variables. Our approach, presented in Section~\ref{sec: method} leverages initial state interventions to identify and completely mask causally confusing features without relying on expert queries or policy interventions. We provide \emph{conservativeness} guarantees for our method in Section~\ref{sec: theory} and present illustrative experiments in Section~\ref{sec: experiments}.
