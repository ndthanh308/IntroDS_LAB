\section{RELATED WORKS}
\textbf{Imitation learning.}
Imitation learning is learning optimal control policies from the demonstrations from the expert policy executor, such as human expert \cite{schneider_analytic_2018}. Due to its advantage of removing the necessity for extensive interaction with the environment and delicate reward design, imitation learning have been widely used in reinforcement learning \cite{rashidinejad2021bridging, zhu2018reinforcement, sun2018truncated,DBLP:conf/cdc/CosnerYA22,DBLP:conf/cdc/StrongLB22,DBLP:conf/cdc/ZhangHLLZ21,DBLP:conf/cdc/GiammarinoP21,DBLP:conf/cdc/ChenPS021}. One of the major issues in imitation learning is the distributinoal shift problem where the learned policy shows unjustifiable behaviour due to encountering unseen stats during its execution. Existing works show that the distributional shift problem in imitation learning comes from causal misidentification or spurious causal relationship between the data and the policy \cite{de2019causal,DBLP:journals/corr/abs-2106-03443}. 

\textbf{Causal Inference.}
Causal Inference is the well-known problem that deduces the relationships between causes and effects among the variables \cite{DBLP:conf/cdc/Li021,DBLP:conf/cdc/XieKB020,DBLP:conf/cdc/PauliGBA21,DBLP:conf/cdc/ParkP22}. Most existing work take one of the approaches called ``causal discover'',
to find its relationship from recorded observations under some constraints \cite{https://doi.org/10.48550/arxiv.1605.08179,LouizosEtAl_arxiv17,Maathuis2010PredictingCE}.

Regarding with causal inference, one challenge in the imitation learning is confounding, which is also known as $\textit{causal delusion}$ \cite{de2019causal}. Namely, standard imitation learning does not assume the existence of latent variables that may affect the expert behavior but not captured by observations. Ortega et al.\cite{pedro2021CI} propose a interventional distribution in a causal structure model and claim that an imitator needs to refer the distribution during the policy learning. Tien et al. \cite{Tien2022} show a systematic study of causal confusion that learns the preference of reward functions which is provided by the human expert. Vuorio et al.\cite{Yuorio2022} propose learning a latent-conditional policy, where the environment dynamics is influnced by the latent variable. Pim de Haan et al. \cite{DBLP:journals/corr/abs-1905-11979} proposed causal graph parameterized policy learning that maps from multiple causal graphs to policies.
