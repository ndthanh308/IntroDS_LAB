\section{CONCLUSION}
This work introduces a novel method to address the causal confusion problem in imitation learning. The proposed method leverages the typical imitation learning ability to intervene in the initial system state. Unlike previous works, our method masks causally confusing observations without relying on online expert queries, knowledge of the expert reward function, or specification of the causal graph. Our theoretical results establish that our masking algorithm is \emph{conservative}, with excess conservatism strictly reduced by interventions on the initial state. We illustrate the effectiveness of our method with experiments on \cp and \re.
