\section{Experiments} \label{app: experiments}

We include here essential environment, architecture, and hyperparameter details.


\subsection{Environments} \label{app: environments}

We consider two environments: \cp and \re. Both systems are rendered to $64 \times 64$ RGB images, pictured in Figure~\ref{fig: environments}.

% Figure environment removed

\subsubsection{\cp}
The \cp environment \cite{barto1983neuronlike} describes a nonlinear dynamic system consisting of four states: the cart position $x$, the cart velocity $\dot{x}$, the pole angle $\theta$, and the pole angular velocity $\dot{\theta}$. The state vector at time $t$ is $S_t \coloneqq (\xt, \dxt, \tht, \dtht)$. The agent action is a continuous horizontal force acting on the cart, bounded symmetrically in the range $[-25, 25]$. The length of the pole is $1$ meter, with the masses of the cart and the pole set to $1$ and $0.1$ kilograms, respectively. We specify the gravitational acceleration constant as $g = 9.8 \textrm{m/s}^2$. The system is then discretized with $\Delta t = 0.05$ for $100$ time steps using the forward Euler method, with the standard CartPole dynamics equations adapted from OpenAI Gym \cite{OpenAIGym}.
We minimize cumulative stepwise quadratic form loss to the upright target state $\Starget = (0, 0, 0, 0)$, with an additional quadratic control cost.
%The initial state is sampled via $x_0 \sim \Unif(-2.5, 2.5)$, $\dot{x}_0 \sim \Unif(-5, 5)$, $\theta_0 \sim \Unif(-\frac{\pi}{4}, \frac{\pi}{4})$, $\dot{\theta}_0 \sim \Unif(-\frac{\pi}{4}, \frac{\pi}{4})$.
%The target state is upright state $\Starget = (0, 0, 0, 0)$, with a standard stepwise quadratic loss on deviation from this state and control cost. The initial state is sampled via $x_0 \sim \Unif(-2.5, 2.5)$, $\dot{x}_0 \sim \Unif(-5, 5)$, $\theta_0 \sim \Unif(-\frac{\pi}{4}, \frac{\pi}{4})$, $\dot{\theta}_0 \sim \Unif(-\frac{\pi}{4}, \frac{\pi}{4})$.
% The expert CFTOC policy additionally enforces that $\xt \in [-5, 5]$, $\dxt \in [-10, 10]$, $\tht \in [-\frac{\pi}{2}, -\frac{\pi}{2}]$, and $\dtht \in [-\frac{\pi}{2}, -\frac{\pi}{2}]$ for all $t$.

To each frame, we add a $15 \times 15$ square nuisance feature at the top-left corner of each image. The color of the square interpolates linearly between green and red, depending on the action (cart force) from the previous time step. At the initial time step $t = 1$ there is no previous action, and thus we use a random number drawn from $\Unif(-25, 25)$ to generate the square.

We generate $5,000$ random-policy trajectories for training the $\beta$-VAE and $1,000$ expert trajectories for imitation learning. Random-policy trajectories are terminated when the states become out-of-bound, and are thus generally significantly shorter than the expert trajectories.

\subsubsection{\re}
\re is also implemented based on the classic OpenAI Gym environment \cite{OpenAIGym}. The system contains six states: target position $x^*, y^*$; joint one angle and velocity $\theta_1, \dot \theta_1$; and joint two angle and velocity $\theta_2, \dot \theta_2$. The target positions $x^*, y^*$ is fixed over the course of one trajectory to a random point in the reachable area. Both links have mass $1$ kilogram and length $0.5$ meters. Agents specify torques at both joints, bounded in the range $[-2, 2]$. The objective penalizes squared distance of the end effector from $(x^*, y^*)$---visualized as a black dot---at each time step, along with a quadratic control cost. We simulate with a time step $\Delta t=0.05$ seconds for $200$ time steps.

For \re, we demonstrate that our method can eliminate a visually different type of confounding than the colored square in the \cp experiment. The considered confounder is a red dot in the upper-left corner of the image that moves translationally according to the agent action in the previous time step. Specifically, the horizontal position is linearly interpolated according to the first joint torque, and the vertical position is linearly interpolated according to the second joint torque. Similarly to \cp, we choose a random previous action for the first time step.

We generate $2,000$ random-policy trajectories for training the $\beta$-VAE and $2,000$ expert trajectories for imitation learning.


\subsection{VAE training}

We train a standard $\beta$-VAE \cite{burgess2018understanding} as implemented by \cite{Subramanian2020}. We choose a latent space dimension of $3$ for \cp and $12$ for \re, although we note that larger choices for the latent space dimension yield similar results. We train for $150$ epochs at a learning rate of $0.0005$ on \re and $100$ epochs at a learning rate of $0.005$ on \cp. Both use an exponential learning rate scheduler with decay factor $0.95$.  Our batch size is $256$ for \re and $64$ for \cp. Finally, we choose the disentaglement factor $\beta=100$ for \cp and $\beta=1000$ for \re.


\subsection{Behavior cloning training}

For \re, we use a standard pre-activation ResNet-18 \cite{he2016deep}; for the simpler \cp environment, a simple ConvNet with $3 \times 3$ convolutions and channel sizes $[32, 64, 128, 256, 512]$ suffices (this architecture resembles the VAE encoder). Both architectures are trained with the Adam optimizer \cite{kingma2014adam} at an initial learning rate of $0.001$ and exponential learning rate decay with factor $0.96$. We use a batch size of $256$ and evaluate the performance of the agent with $25$ validation rollouts every $10$ epochs. As a single image of the environment cannot convey higher-order state information such as velocity, we input the previous two images into our policies---i.e., $L=2$ in \eqref{eqn: policy}. Thus, we make the necessary architectural change to the underlying models of setting the number of input channels to $6$. Since the first time step does not have an associated previous image, we use a blank image as a surrogate.
