% \vspace{0.05in}
\section{Data Collection}
\label{sec:design}



%\subsection{Overview}

%\noindent \textbf{Overview.} 
Our dataset consists of three sections: \textit{a) base dataset, b) pilot dataset}, and \textit{c) augmented dataset}. Figure~\ref{fig:system} illustrates the composition and construction procedure of~\db{}. 
We first form the base dataset by collecting the commits associated with CVE records indexed by MITRE. Yet, less than 50\% of CVE records have published their security commits. %, which only provides partial semantics for further security commit mining.
To introduce more code semantics, we further build the pilot dataset by filtering the wild GitHub commits that have pre-defined security keywords in their commit messages. 
However, not all commits contain well-maintained commit messages that precisely describe the rationales of changed code.
Thus, we consider directly mining the most critical part of commits, i.e., the source code changes. 
% While well-maintained commit messages that precisely describe the rationales of changed code are critically lacking in practice, we consider directly mining the source code changes, the most necessary part of a commit.
To the end, we propose an intermediate commit representation (i.e., \cpg{}) and design a dependency-aggregation graph neural network (i.e., \gnn{}) to capture the inherent sequential and structural semantics of code changes.
%and expand the current dataset with an augmented dataset that includes the commit pinpointed by \gnn{}. 
Trained with the base and pilot datasets, \gnn{} is able to further build the augmented dataset by pinpointing the silent security commits from the wild.

% Figure environment removed

% In this section, we elaborate on the design of~\TN{}. 
% It collects an extensive security commits dataset from version control repositories for Python projects, represents code control/data dependency and syntax using Code Property Graphs, and learns the nodes embedding with CodeBERT~\cite{feng2020codebert}, the edge embedding with multiple attributes. 
% We then build a commit graph classifier expediently leveraging a graph-based deep neural network. 
% Figure~\ref{} illustrates the overall workflow of PySP with phase (1) for Commit Dataset Collection, phase (2) for Graph Generation for Commits and phase (3) for Learning from Commit Graphs.
\subsection{Base Dataset Collection}
We build the base dataset % from the CVE records indexed with MITRE~\cite{cve}.
according to the CVE records~\cite{cve}.
The first step is to retrieve the vulnerabilities that have already been indexed with CVE IDs. Then, we parse the vulnerability reports and crawl the corresponding commits via the provided reference hyperlinks. It comes to our attention that the collected commits may contain some noise, e.g., changelog, test case, refactoring, and renaming. %After excluding the changed file other than Python, we remove the commits that only contain changes of comments, refactoring, and renaming. In this way, we obtained 730 commits to form the base dataset.
After excluding these unrelated documents, we obtain 729 security commits to form the base dataset; meanwhile, we collect the excluded commits as the non-security subset in the base dataset and expand it by manually identifying the commits that add new features or perform refactoring, linting, and version updates.

\subsection{Pilot Dataset Collection}
\label{db:pilot}

%We could only find 46\% fixes of the indexed CVE records, which implies that there are still many vulnerabilities that have not been fixed or the fix commit is still in the wild. Also, the limited patches only provide partial semantics, which will impede the security commits understanding from capturing a wide variety of code change features.

After examining all the indexed CVE records (as of 01/27/2023), only 46\% of them contain the corresponding security fixes. Therefore, the limited samples in the base dataset may not provide adequate syntactic and semantic information.
That means, only with the base dataset, we are unable to train a robust model for capturing a wide variety of security commits in the real world.
% simply using the base dataset may be unable 
% with complete understanding of security commits for capturing a wide variety of code change features in the real world.
Given the fact that a majority number of security patches are silently committed without reporting to the MITRE~\cite{wang2019detecting,zhou2021spi}, we propose to enrich the security commits with the pilot dataset collected from GitHub, i.e., the most common OSS hosting platform. 

The pilot dataset is constructed by keyword filtering with humans in the loop. %We check the existence of security keywords in the wild commit's subject. 
%We look for security-related keywords in the subject of the GitHub commit. 
The list of security-related keywords is built automatically by analyzing the CVE descriptions, CWE types (if exist), and commit messages.
We determine the final keywords by calculating the word frequency and evaluating the correlation between the keywords and security commits. 
To obtain the security subset of the pilot dataset, we locate and manually verify the security commit candidates that contain the pre-defined security keywords in the commit messages; the excluded commits are collected as the non-security subset.
% After locating the commits with security-related keywords in their commit messages, we manually verify these security commit candidates to finalize the security subset of the pilot dataset; the excluded commits have been grouped to form the non-security part.

% Next, we cloned popular repositories for Python projects and retrieved all commit history from it till Jan 01, 2023, and filter out the candidate security commits by checking the existence of any keyword in the security-related keywords. 

\noindent{\bf Security Keyword Extraction. }%Modeling.} 
%Table~\ref{tab:keywords} includes the essential part of keywords list. 
For each security commit collected from the CVE records, we generate its security impact summary by combining the commit message, the CWE information (if exists), and the CVE report.
% we combine its commit message, its CWE information (if exists), and its description in its CVE report as the summary of the security impact. 
After generating the summary, we conduct 1-gram, 2-gram, and 3-gram tokenization. 
Then, we consider the frequency of each token and the correlation of each token with security and non-security commits.
% By considering the frequency of each token as well as the correlation of the tokens with security commits compared to non-security commits, 
We set the frequency threshold and derive the list of security-related keywords, as shown in Table~\ref{tab:keywords}. 
%formed the selection of the list of keywords. 
%\noindent{\bf Keyword Filtering.} We check the existence of security keywords in the wild commit's subject, and select the matched commits as candidates, which will be verified manually later.
Then, we determine the security commit candidates by checking if the wild GitHub commits contain any security keywords in their commit messages.
These candidates will be manually verified.


% Then, we check the existence of security keywords in the commit messages of wild GitHub commits and select ones containing at least one keyword as candidates, which will be manually verified later.

\begin{table}[t]
\begin{center}
\caption{Security-related keywords for commit filtering.}
\label{tab:keywords}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c}
\toprule
{\bf \#Tokens} & \textbf{Keywords} \\ 
\midrule
{1-gram} & {\begin{tabular}[c]{@{}c@{}}
attack, bypass, CVE, DoS, exploit, injection, \\ leakage, malicious, overflow, smuggling, \\ spoofing, unauthorized, underflow, vulnerability
% vulnerability, malicious, exploit, \\ attack, cve, unauthorized, bypass, injection
\end{tabular}} \\ 
\midrule
{2-gram} & {access control, open redirect, race condition} \\ 
\midrule
{3-gram} & {denial of service, out of bound,  dot dot slash} \\ 
\bottomrule
\end{tabular}
}
\end{center}
\vspace{-0.2in}
\end{table}



\noindent{\bf Manual Verification.} 
We hire three security experts to manually verify the security commit candidates. 
To guarantee the data quality and minimize false positives, the experts are required to follow our two-step labeling procedure strictly. 
First, each expert tags the commits independently with the labels: security, non-security, or unsure. 
Then, they gather together to discuss each disagreement and reach a consensus on each uncertain candidate. 
Only the security commits with 100\% agreement will be included in the pilot dataset. In total, it takes 48 man-hours to finish the labeling work.



The proposed keyword filtering mechanism reduces the workload and time for manual verification; meanwhile, the human-in-the-loop ensures the quality of the pilot dataset. More details on labeling efficiency are shown in Section~\ref{results:efficiency}.

\subsection{Augmented Dataset Construction}

The pilot dataset overlooks the commits that lack security keywords in the commit messages, while these commits may provide additional variants in syntax and semantics. 
Therefore, we propose to further augment our dataset. 
While the pilot dataset is collected based on commit messages, we build the augmented dataset by only analyzing the source code changes.
%while the state-of-the-art code representation models (e.g., PatchRNN~\cite{wang2021patchrnn}) neglect the inherent structure semantics. Thus, 
Different from existing works that simply regard source code as sequential data~\cite{wang2021patchrnn,zhou2021spi}, we present a commit graph representation named CommitCPG and a graph learning-based model \gnn{} to capture the inherent structural information.% \gnn{} is mainly composed of two parts: graph representation for commits and corresponding graph understanding.

%\subsubsection{Graph Representation for Commits}
\vspace{0.05in}
\noindent \textit{1) CommitCPG: Graph Representation for Commits}
\vspace{0.02in}

% Figure environment removed

To preserve the inherent structure of source code and the modified content between two versions, we propose a graph-based commit representation called \cpg{}, which offers essential syntactic and semantic information for comprehensive commit understanding. 
Code property graph (CPG)~\cite{y2014cpg} is a program representation that contains abstract syntax trees (AST), control-flow graphs (CFG), and program dependence graphs (PDG), providing a more comprehensive view for code static analysis, compared with traditional sequential structure adopted by NLP-based works~\cite{guo2020graphcodebert}.
% With various structural information (i.e., control flow/dependency, intra-procedural data dependency, and program syntax), CPG 
%With this goal in mind, 
In Figure \ref{fig:commitCPG}, we first preprocess the raw commits by excluding the irrelevant functions. Inspired by \cite{wang2022graphspd}, we then merge the CPGs \cite{y2014cpg} constructed from the previous and current versions by aligning the unchanged statements. Next, we adopt a code slicing method to retain the crucial context-related code snippets, which are not changed directly by commits but can assist us to understand the reason and the effects of  code changes.%and apply the intermediate representation to assist us to understand the security-related code change semantics.

\noindent{\bf Commit Preprocessing.} 
To generate the CPG for each code version, we need to retrieve the source code of the previous version and the current version, respectively.
% A CPG reveals the property of a program under a specific version, while a commit is a set of code changes between two versions. Thus, we need to retrieve the source code of the previous version and the current version so as to process them respectively. 
% \XD{Not Pre/Cur-Commit, should be version, commit refers to changes between two versions.} 
To reduce the overhead of CPG generation, we only focus on the modified files instead of the whole project. 
Then, we extract the functions with code revisions as well as the modified global statements. 
To achieve this goal, Joern parser \cite{y2014cpg} is applied to detect all relevant functions and their corresponding scopes. 
% (i.e., the range of line numbers between function start and end point). Among them, 
We only retain the functions whose scope overlaps with the modified lines in the commits.
% , i.e., functions modified by the commit. 
For example, in List~\ref{lst:security commit}, we will only keep the content of function \texttt{\small \_load\_yamlconfig()}.

\noindent{\bf CPG Generation for Previous and Current Versions.} 
With the extracted source code of two versions, we employ Joern~\cite{y2014cpg} to generate the CPGs for both versions. 
% Taking the source code of the previous and current versions as well as the modified function as input, we employ Joern~\cite{y2014cpg} to generate the CPGs for both versions. 
A CPG can be described as $(V, E)$, where $V$ is the node set and $E$ is the edge set.
$V$ is comprised of multiple 5-tuples $(id, func\_name, file\_name, version, code)$, which contain the information of each node.  
% For a CPG described as $(V, E)$, the set of nodes, $V$, is comprised of 5-tuples $(id, function\ name, file\ name, version, code)$, where $id$ is a unique identifier for the node, $function\ name$ and $file\ name$ records the function and file. 
The node version, represented by $version \in \{previous, current\}$, reflects if the code line belongs to the previous version or current version.
% and the $code$ represents a statement (usually a line of source code). 
The directed edge set $E$ is made up of 4-tuples $(id_1, id_2, type, version)$, where $id_1$ and $id_2$ denote the start and end node IDs, respectively.
The edge type, represented by $type \in \{AST, CDG, DDG\}$, specifies if the edge belongs to the AST or control/data dependency graphs. 
The edge $version$ is consistent with the node's version.

\noindent{\bf CPG Merging and CommitCPG Slicing.}
We first generate a unified commit graph by fusing the CPGs of two versions according to each node pair.
% Then, we conduct a bi-directional program slicing to generate the \cpg{}, which preserves the most related security semantics.
% We fuse the CPGs for each pair of previous and current versions to create a unified graph structure %, which we refer to as MergedCPG, 
% and further conduct two-direction program slicing %on MergedCPG 
% to generate \cpg{}, which preserves most related security semantics.
% Function names are used to match the functions in the previous version with their corresponding ones in the current version. 
Then, we update the representation of the merged CPG by two sets: $(V', E')$. 
The node set $V '$ is comprised of 5-tuples, which are denoted as $(id, func\_name, file\_name, version, code)$. 
$id$ is updated so that each node has a unique identifier and the node version is changed to $version \in \{current, previous, unchanged\}$, representing if the code in this node belongs to the current, previous, or both commits. 
% The $function\ name$, $file\ name$, and $code$ remain consistent with $V$. 
The directed edge set $E'$ is made up of 4-tuples $(id_1, id_2, type, version)$, where $id_1$, $id_2$, and $type$ stay the same as $E$. The edge $version$ will be updated as $unchanged$ if both connected nodes are unchanged nodes.

%\noindent{\bf Slicing the \cpg{}.} 
To reduce the noise introduced by irrelevant nodes and emphasize the semantics of code changes, we generate the \cpg{} by a bi-directional program slicing \cite{weiser1984program}, i.e., forward and backward slicing. 
Backward slicing is to reason the code changes, while forward slicing is to locate the statements affected by the commit. 
For example, in List~\ref{lst:security commit}, if we set the deleted statement (Line 10) as a backward slicing criterion, the slicing results include Lines 5, 7, and 8; if we set the added statement (Line 11) as a forward slicing criterion, the slicing results contain Line 14.
% that are influenced by the code revision. 
After we conduct code slicing over control/data dependency, we can obtain \cpg{} by only retaining all the nodes of modified and sliced statements (i.e., Line 7, 8, 10, 11, and 14) along with the traced edges.

%when we set the deleted statement (Line 6) as a backward slicing criterion, there are no previous statements affected by it, which reveals Line 6 is the vulnerability location; when we set the added statement (Line 7) as a forward slicing criterion, the slicing results contain Line 8 that has been influenced by the modification. Once we conduct backward and forward slicing using control and data dependency, we keep all the nodes of the modified and sliced statements, along with the traced edges, denoted as \cpg{}.

\vspace{0.05in}
\noindent \textit{2) \gnn{}: Graph Learning for Commits}
\vspace{0.02in}

% Figure environment removed

Figure~\ref{fig:model} illustrates the workflow of \gnn{}, our proposed graph-based network to identify security commits in Python. \gnn{} contains two steps: (i) node embedding with CodeBERT and edge embedding with dependency-aggregation mechanism, (ii) graph convolution with multi-head attention.
% , and (iii) \cpg{} classification. 

\noindent{\bf \cpg{} Embedding.} To feed the \cpg{} to our \gnn{}, we encode the node and edge attributes into numeric vectors. 
Each node represents a code statement; hence, the node embedding should capture the semantics within the statement.
Thus, we first utilize CodeBERT~\cite{feng2020codebert} to generate token embeddings and grasp the sequential-based semantics.
Then, we obtain the node embedding by aggregating the token embeddings.
% We formulate the embedding for each node in \cpg{} using , assisting our graph-based neural network to grasp the security-related semantics.
% Recall that each node represents a code statement and each edge represents the dependency between two nodes.
% Node embedding helps us capture the semantic information within the code statement, and 
In addition, each edge presents the dependency between two nodes; thus edge embedding preserves crucial structural and attribute information.
We generate edge embeddings with 5-dimensional one-hot vectors. 
The first two dimensions are used to embed the structural information and present which code version the edge belongs to.
The last three dimensions are used to embed the attribute information, which indicates if the edge presents control dependency, data dependency, or syntax relationship.


% To extract the dependencies between two nodes, which also reveal the inherent structure of code, we take both syntactic-level structures (i.e., AST) and semantic-level structures (i.e., CDG and DDG) into consideration.

% We merge the syntactic elements from the same statement as one node, thus, the edges in AST will also be merged into the statement level and the attribute from each edge will be concatenated. 

% The semantic-level structure of code encodes the dependency of “where-the-value-comes-from” between variables and the dependency of "what-the-next-statement-to-be-executed" among each line of code. 

% The semantic-level edge attributes have been appended to the syntactic-level edge attributes. 

% Therefore, edge embedding has four dimensions. The first dimension is used to indicate the version information, the second dimension demonstrates the AST attributes and the last two bits show if there are any control dependency and data dependency, respectively.




\noindent{\bf Graph Convolution with Multi-Head Attention.} 
After embedding the \cpg{} with a sequential model, we adopt a graph convolutional network with multi-head attention to learn the structural representation of commits. 
% To learn commit representation from code sequences and code structures, we adopt a graph convolution network with multi-head attention. 
To avoid over-smoothing, the number of convolutional layers is limited to 3. 
We feed the embedded \cpg{} into 3 multi-attributed graph convolutional layers. 
The node embeddings of \cpg{} are updated with the neighborhood information from different subgraphs. 
Then, the graph embeddings, i.e., a unified vector representation transformed from all the nodes, edges, and features, can be obtained through graph pooling and vector concatenation.
% Graph embedding is a unified representation that transforms all the nodes, edges, and features into a single vector. 
The graph embeddings learned by the \gnn{} are finally fed into a multi-layer perceptron to determine the likelihood that a commit fixes a security vulnerability, i.e., whether the given commit is a security commit.



To demonstrate the generalization ability of \gnn{}, we utilize the trained \gnn{} to discover more security commits in the wild. 
We directly send the \cpg{} of wild commits into \gnn{}. For the commits labeled as security commits, we adopt a similar process (as described in~\ref{db:pilot}) to manually verify if they are real security commits. The excluded commits composite the non-security subset of the augmented dataset.

%the commits that have been labeled as security commits. The labor process keeps consistent with the manual verification described in~\ref{db:pilot}.