\section{Implementation}
\label{sec:implementation}

% We demonstrate the implementation of constructing each dataset as follows. 

\subsection{Base Dataset Construction}

The base dataset consists of the commits linked with CVE records, which have been indexed by MITRE~\cite{cve}. MITRE provides hyperlinks of vulnerability fixes for 46\% of CVE entries. We focus on the hyperlinks from GitHub, where each commit is identified with a unique hash value and the hyperlink is in the form: \emph{https://github.com/\{owner\}/\{repo\}/commit/\{hash\}}. {We build the base dataset by downloading the vulnerability fixing commits and removing the commits that are not written in Python or only focus on security-unrelated modifications (e.g., renaming and refactoring).}

\subsection{Pilot Dataset Construction}
%We use topic modeling methods to extract essential security-related tokens from the commit messages. We adopt Latent Dirichlet Allocation (LDA)~\cite{blei2003latent} to collect the security-related tokens in the commit message. 
We adopt Latent Dirichlet Allocation (LDA)~\cite{blei2003latent}, a topic modeling method, to extract the essential security-related tokens from the commit messages.
Then, a keyword filtering algorithm is applied to exclude non-security commits. %Given a commit with the security key token set, we assign the security-related commit candidate to it if it has at least one keyword in the token set. 
We download popular open-source repositories in Python and retrieve their commit histories till Jan 27, 2023. For each commit, if it contains at least one proposed keyword, we regard it as a security commit candidate. Later, we manually check these security commit candidates and finalize the dataset.

To facilitate the verification process, we use {PyQt}~\cite{pyqt} to develop a graphical user interface (GUI) that %loads the commit candidates sequentially, records the verification results, and groups the security commits after verification.
visualizes the code changes of each individual commit and stores the verified security commits according to verification results.

\subsection{Augmented Dataset Construction}

% To build the augmented dataset, we implement each part of \gnn{}:  \cpg{} Generator, \cpg{} Learner, and Commit Classifier respectively

%\noindent{\bf \cpg{} Generator.} 
To build \gnn{} for further dataset augmentation, we first generate the corresponding \cpg{} for each commit in the base and pilot datasets. Specifically,
%While we cannot access the code property graph directly from the code change itself, we need to retrieve the source code of the previous commit and the current commit and merge them at the unchanged lines. Thus, 
we adopt {Joern}~\cite{y2014cpg} to generate CPGs for the code versions before and after applying the commit, respectively. Then, we parse the generated graph files and merge the graphs to build \cpg{}. To achieve the program slicing, we develop a Python script to analyze the control/data dependency and AST information and output a sliced CommitCPG ready to be embedded.

%\noindent{\bf \cpg{} Learner.} 
To prepare an embedded graph for \gnn{}, we embed the nodes and edges respectively. We fine-tune CodeBERT~\cite{feng2020codebert} to generate the node embedding dedicated to Python. For the edge embeddings, we apply the one-hot encoding to represent the attributes on each edge. We build the \gnn{} on the deep learning library PyTorch 1.6, which is optimized for tensor computing. We develop and optimize our graph model based on the PyTorch-geometric 1.6 library, which supports deep learning on graphs and other structured data.
%\noindent{\bf Commit Classifier.} 
Finally, a multiple-layer perceptron (MLP) is used as a binary predictor, which converts the graph embeddings into predicted labels. 
We train \cpg{} with the base and pilot datasets. 
Then, we feed the wild unlabeled commits into the trained \gnn{} and apply manual verification to generate our augmented dataset.
