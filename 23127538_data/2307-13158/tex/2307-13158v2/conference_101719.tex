% \newcommand{\CLASSINPUTtoptextmargin}{0.7in}
% \newcommand{\CLASSINPUTbottomtextmargin}{0.95in}
\documentclass[10pt,conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
%\bibliographystyle{IEEEtran}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
% \usepackage{xcolor}
\usepackage[english]{babel}
% Useful packages
\usepackage{amsmath}
\usepackage{graphicx,epstopdf}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[english]{babel}
% \usepackage[utf8x]{inputenc}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
% \usepackage[margin=2.2cm]{geometry}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{latexsym}
\usepackage{eurosym}
\usepackage{units}
\usepackage{mathtools}
%\usepackage[numbers]{natbib}
\usepackage{lscape} 
% \usepackage{algpseudocode}
%\usepackage{algorithm}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{optidef}
% \usepackage[dvipsnames]{xcolor}
\usepackage{multirow}
\usepackage{booktabs}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\IEEEoverridecommandlockouts\IEEEpubid{\makebox[\columnwidth]{ 979-8-3503-1090-0/23/\$31.00~\copyright~2023 IEEE \hfill} \hspace{\columnsep}\makebox[\columnwidth]{ }}


\begin{document}

\title{
 % Multi-UAV Traffic Flow Optimization with HO-aware Cell Association: Deep Reinforcement Learning with Action Branching

Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching
%UAV 3D Aerial Traffic Flow Optimization with HO-aware Network Selection 
%This research was supported by a Discovery Grant funded by the Natural Sciences and Engineering Research Council of Canada. 
\thanks{The authors emails are \{zjyan, hinat\}@yorku.ca; \{wael.jaafar, bassant.selim\}@etsmtl.ca.}
}

\author{%
  \IEEEauthorblockN{%
    Zijiang~Yan\IEEEauthorrefmark{1}, 
    Wael~Jaafar\IEEEauthorrefmark{2},
    Bassant~Selim\IEEEauthorrefmark{2} and
    Hina~Tabassum\IEEEauthorrefmark{1}%
  }%
  \IEEEauthorblockA{\IEEEauthorrefmark{1} York University, ON, Canada, \IEEEauthorrefmark{2} École de Technologie Supérieure (ÉTS), University of Quebec, QC, Canada}%
  %\IEEEauthorblockA{\IEEEauthorrefmark{3} Dept. of Systems Engineering, École de Technologie Supérieure (ÉTS), Canada}
  % \thanks{This research was supported by a Discovery Grant funded by the Natural Sciences and Engineering Research Council of Canada (NSERC). }
  % \IEEEauthorblockA{\IEEEauthorrefmark{3} Affiliation 3}%
}


\maketitle
% \begingroup\renewcommand\thefootnote{\textsection}
% \footnotetext{Equal contribution}
% \endgroup
\raggedbottom
\begin{abstract}
%
This paper develops a deep reinforcement learning solution to simultaneously optimize the multi-UAV cell-association decisions and their moving speed decisions on a given 3D aerial highway.  The objective is to improve both the transportation and communication performances, e.g., collisions, connectivity, and HOs. We cast this problem as a Markov decision process (MDP) where the UAVs' states are defined based on their speed and communication data rates. We have a 2D transportation-communication action space with decisions like UAV acceleration/deceleration, lane changes, and UAV-base station (BS) assignments for a given UAV’s state. To deal with the multi-dimensional action space, we propose a neural architecture having a shared decision module with multiple network branches, one for each action dimension. A linear increase of the number of network outputs with the number of degrees of freedom can be achieved by allowing a level of independence for each individual action dimension. To illustrate the approach, we develop Branching Dueling Q-Network (BDQ) and Branching Dueling Double Deep Q-Network (Dueling DDQN). Simulation results demonstrate the efficacy of the proposed approach, i.e., 18.32\% improvement compared to the existing benchmarks.
\end{abstract}

\begin{IEEEkeywords}
Unmanned aerial vehicles,  HOs,   Deep Reinforcement Learning, speed, cell-association.
\end{IEEEkeywords}
% In this paper, a novel unmanned aerial vehicle (UAV) space-time-frequency (S-T-F) non-stationary channel model with time-space consistency for sixth generation (6G) massive multiple-input multiple-output (MIMO) millimeter wave (mmWave) wireless communication systems is proposed. In the proposed model, the line-of-sight (LoS) transmission and non-LoS (NLoS) transmission through ground reflection, single-clusters, and twin-clusters are modeled. Meanwhile, the three-dimensional (3D) continuously arbitrary trajectory and the self-rotation of UAV are imitated. To capture the time-space consistency and S-T-F non-stationarity simultaneously, a new UAV-related non-stationary modeling algorithm that integrates the visibility region (VR), frequency-dependent path gain, and survival probability is developed for the first time. In this algorithm, the UAV-related parameters are considered, including UAV’s height, 3D moving velocity, and self-rotation angles. In the proposed model, the calculation of channel impulse response (CIR) is developed, which considers the cluster density index influenced by communication scenarios, frequency, UAV’s height, and the distance between transceivers. Some important channel statistical properties, such as S-T-F correlation function (STF-CF), Doppler power spectral density (DPSD), and stationary interval, are derived. Finally, simulation results match well with ray-tracing-based results, which verifies the utility of the proposed mode

%Abstract—Vehicle-to-Infrastructure (V2I) communication is becoming critical for the enhanced reliability of autonomous vehicles (AVs). However, the uncertainties in the road-traffic and AVs’ wireless connections can severely impair timely decisionmaking. It is thus critical to simultaneously optimize the AVs’ network selection and driving policies in order to minimize road collisions while maximizing the communication data rates. In this paper, we develop a reinforcement learning (RL) framework to characterize efficient network selection and autonomous driving policies in a multi-band vehicular network (VNet) operating on conventional sub-6GHz spectrum and Terahertz (THz) frequencies. The proposed framework is designed to (i) maximize the traffic flow and minimize collisions by controlling the vehicle’s motion dynamics (i.e., speed and acceleration) from autonomous driving perspective, and (ii) maximize the data rates and minimize HOs by jointly controlling the vehicle’s motion dynamics and network selection from communication perspective. We cast this problem as a Markov Decision Process (MDP) and develop a deep Q-learning based solution to optimize the actions such as acceleration, deceleration, lane-changes, and AV-base station assignments for a given AV’s state. The AV’s state is defined based on the velocities and communication channel states of AVs. Numerical results demonstrate interesting insights related to the inter-dependency of vehicle’s motion dynamics, HOs, and the communication data rate. The proposed policies enable AVs to adopt safe driving behaviors with improved connectivity.https://ieeexplore.ieee.org/abstract/document/9905970

% In the proposed model, the calculation of channel HO aware mobility throughput is developed, which considers the cluster density index influenced by communication scenarios, frequency, UAV’s height, and the distance between transceivers. Meanwhile, the three-dimensional (3D) aerial highway continuously arbitrary trajectory and the longitude/lattitude motion control of UAV are imitated.




\section{Introduction}

Unmanned aerial vehicles (UAVs) are gaining popularity  across a broad range of applications due to their mobility, flexible deployment, gradually decreasing production costs, and line-of-sight (LOS) channels. 
\cite{yu2022deep}. 
A UAV can either require cellular connectivity for its own use (UAV-UEs) or provide cellular coverage as a base station (BS). Nevertheless, controlling UAVs that operate beyond visual line of sight (BVLoS) requires reliable command and control which is crucial for mission safety and security.  



Existing research primarily focuses on optimizing cellular link availability and quality of service (QoS) using reinforcement learning (RL) algorithms with no considerations to multi-UAV aerial traffic flow and motion dynamics of UAVs.  In \cite{cherif2022cellular}, the authors proposed a RL algorithm that considers disconnectivity, HOs, and energy consumption for trajectory planning and cell association in cargo UAVs. However, the algorithm's actions only consider the direction of motion with no speed and lane considerations. In \cite{ chen2020efficient}, the authors present strategies based on deep learning to predict HOs in mmWave communications and optimize HO rates and radio link quality for known UAV trajectories. However, these works have not considered the motion dynamics factors, such as acceleration, deceleration, and lane changes on the aerial highway. Furthermore, the existing works in \cite{cherif2021disconnectivity} mostly considered $Q$-learning and its variants which can lead to sub-optimal policies and slower convergence. In terms of transportation, achieving high performance for multi-UAV traffic flow and collision avoidance is crucial. On the communication side, UAVs require: \textbf{(i)} high data rates and \textbf{(ii)} minimal HO losses. Increasing speed can increase traffic flow but results in frequent HOs, which can negatively impact the communications between UAVs and base stations (BSs). Very recently, this trade-off has been investigated in the context of autonomous vehicles \cite{10077729,10001396}.

 % Several aforementioned works have explored the practicality and feasibility of using cellular-connected UAVs for BVLoS operations \cite{cherif2021disconnectivity}. Position control involves determining the location and orientation of each UAV and controlling its movement to achieve the desired trajectory while avoiding collisions with other UAVs. 

% UAVs collision avoidance (transportation side) and communication side
% Unmanned aerial vehicles (UAVs) have a lot of room for improvement in traffic science and communications. 
% On the transportation side, It is high priority to pursue high transportation performance for multi-agent UAVs traffic flow, which includes \textbf{(i)} avoid collision and \textbf{(ii)} maintaining high speed movement. Due to the cost for UAV is high, reduce the collisions on the trip planning is necessary.  On the communication side, UAVs rely on  ultra-reliable low latency communications (URLLC), which consists \textbf{(i)} high transmission data rate and \textbf{(ii)} low HO loss. A single-minded increase in speed leads to potential frequent HOs and impact the UAV-BSs connection performance.


% Using terrestrial networks for cargo-UAV cellular connectivity poses challenges because these networks were originally designed to serve ground users and may not provide reliable and seamless connectivity for cargo-UAVs, especially at high altitudes. This is because cargo-UAVs can only be served through the sidelobes of base station (BS) antennas, which are down-tilted. The cargo-UAV's mobility also leads to frequent switching from one serving BS to another, resulting in an additional amount of information exchange that further degrades the transmission link. While maintaining collision-free transportation efficiency, HO events should be minimized to ensure command and control (C&C) stability. Due to these factors, occasional cellular disconnectivity may occur for cargo-UAVs operating in BVLoS. The cargo-UAV's ability to tolerate end-to-end disconnectivity, defined as the ratio of time spent disconnected from all cellular BSs to total mission time, depends on the criticality of the mission and the level of autonomy. Although economically attractive, providing reliable cellular connectivity to cargo-UAVs through terrestrial networks may prove difficult.

% In order to achieve high transportation efficiency with minimum collision among UAVs, position control plays a crucial role. Position control involves determining the location and orientation of each UAV and controlling its movement to achieve the desired trajectory while avoiding collisions with other UAVs. This requires accurate and reliable localization systems and control algorithms that can handle the dynamics of the UAVs and their interaction with the environment. Several approaches have been proposed for position control, including GPS-based systems, visual-based systems, and hybrid systems that combine multiple sensors and techniques. Additionally, advanced control algorithms such as model predictive control (MPC) and optimal control can be used to optimize the UAVs' trajectories and minimize the risk of collisions. Overall, effective position control is essential for ensuring safe and efficient transportation of cargo and other payloads by UAVs.



% %As of 2022, the worldwide market for drones is valued at \$43 billion and is projected to grow at a Compound Annual Growth Rate (CAGR) of 21.5\% to reach \$43.35 billion by 2025.
% % intelligent decision-making algorithms to enable real-time collision avoidance and path re-planning
% % \subsection{Related Work}
% Most of previous research work focus on optimizing cellular link availability and Quality of service (QoS) (i.e. reduce latency) in terrestrial networks based on reinforcement learning (RL) algorithm. For instance, Authors optimize both mmWave and Terahertz (THz) downlink HO-aware performance and traffic flow with minimum collision based on deep $Q$ leaning for autonoumous vehicles (AVs) in \cite{10001396}. Hence, they are not consider UAVs motion control. Authors proposed an RL-based algorithm that takes into account disconnectivity, HO, and energy consumption for trajectory planning and cell association in cargo-UAVs in \cite{cherif2022cellular}.  However, Authors action is only consider the direction motion, which is not precise motion change.  Authors in in \cite{alkhateeb2018machine} and \cite{chen2020efficient} presented the strategy based on deep learning for predicting HO in mmWave communications and to optimize HO rate and radio link quality for a known UAV trajectory.  
% % However, these researches only consider terrestrial networks  performance regarding to cellular link reliability,  HO, QoS, energy performance. 
% Thus, none of these works has investigated the transportation factor including acceleration, deceleration and change lane on the aerial highway. HO cost on the data rate still not consider. Furthermore, majority of reinforcement researches based on $Q$-learning and its variants , which leads to drawing suboptimal policies and slower convergence to the optimal solution. 
% % DQNs have limited capacity to deal with continuous state and action spaces, which is not appropriate to 3D aerial highway scenarios. 



% \subsection{Contribution}
 % In our research, we also optimize UAV 3D aerial traffic flow and reduce the collision rate among travelling.
 % To our best knowledge, the existing research have overlooked the interdependency of the UAV motion dynamics (velocity, acceleration), HOs,lane changes and wireless data rates. 
 Nevertheless, previous studies have not considered speed optimization of multiple UAVs on an aerial highway in conjunction with cell association, while considering collision avoidance, lane changes, and HO-aware wireless data rates. 
% In this paper, we develop a RL framework to jointly optimize the AV motion dynamics and network selection while maximizing a novel   reward function that captures the impact of collisions, traffic flow, lane-changes, and HO-aware communication data rate. 


In this paper, we develop a deep RL (DRL) solution with action branching architecture to jointly optimize cell-association and multi-UAV flying  policies on a 3D aerial highway such that \textbf{(i)}  aerial traffic flow can be maximized  with the collision avoidance, and \textbf{(ii)} HO-aware data rates can be maximized. Specifically, we first cast this problem as Markov decision process (MDP) where a UAV state is modeled based on its speed and data rates. Moreover, to deal with the 2D communication-transportation action space, we develop a DRL solution with an action branching architecture in which a shared module coordinates among multiple network branches. In our case, the module performs 2D decision-making related to UAV acceleration/deceleration, lane changes, and UAV-BS assignments. 
To illustrate the approach, we proposed Branching deep Q (BDQ) network and Branching double deep Q-network (BDDQN)-based UAV agents.
The proposed BDQN offers improved exploration-exploitation trade-off, and enhances robustness and stability compared to conventional DQN.
% $\bullet$ The interaction of UAV and environment is modeled as a Markov decision process (MDP) characterized by a two dimensional (2D) discrete state-action space for {transportation} moving actions and {communication} terrestrial network selection actions. Also, we consider a novel reward function that maximizes  data rate and  traffic flow, ensures traffic load balancing across the network, and penalizes HOs, disconnectivities and collisions.
% $\bullet$ We proposed Markov decision process (MDP) to model the interaction between UAV and its environment, which consists of both transportation and communication aspects. The MDP has a discrete state-action space in 2D for transportation and terrestrial network selection actions. Additionally, we introduced a new reward function that aims to maximize data rate and traffic flow, while ensuring traffic load balancing across the network, and also penalizes HOs, disconnectivities, and collisions.
 % To improve the training efficiency of Deep $Q$-learning, we adopt BDQ to select two actions (transportation action and communication action) on each training step to optimize joint Q-table in the same time  \cite{tavakoli2018action}. Moreover, in BDDQN, we adopt two separate networks, one for selecting actions and one for evaluating $Q$-values, to reduce overestimation of $Q$-values on the BDQN training to improve performance compared with $Q$-learning.
% The proposed solution enables robust connectivity with improved 3D aerial highway traffic flow and safety at all times. RL does not require prior network information or access to complete knowledge of the system. Therefore, the RL-based approach is a promising tool to solve the aforementioned dynamic RRM problem. 
% The BDQ technique used in this approach does not need any previous knowledge about the network or full system information, making it a promising tool to optimize joint transportation and communication performance.

% introduce 

% related work
% contribution
 

\section{System Model}

As illustrated in Figs. \ref{fig:BS_distributions_projection}, 
%-\ref{fig:BS_distributions_vertical}, 
we assume a 3D area where $N_U$ UAVs in a set $\mathcal{U}=\{ u_1, \ldots, u_{N_U}\}$ are flying along the defined 3D highway lanes, while being connected to terrestrial BSs. The latter are uniformly distributed on the targeted area and constitute a set $\mathcal{B}=\{b_1,\ldots, b_{N_R} \}$. To simulate the UAVs' movements on a given aerial highway, we consider the continuous intelligent driver model that models acceleration as in \cite{Treiber2013}. 
UAVs cannot fly above $h_{\max}=300$ m \cite{3gpp777}, and each UAV is identified by its location $\textbf{q}_{k}(t)=\left(x_k(t),y_k(t),h_k \right)$ at any time slot $t$, $\forall k \in \mathcal{U}$. Similarly, the BSs are defined by their locations $\textbf{q}_i=\left(x_i, y_i, h_i \right)$, $\forall i \in \mathcal{B}$. For the sake of simplicity, we assume that $h_i=0$ m, $\forall i \in \mathcal{B}$. The distance between BS $i$ and UAV $k$ is defined as $q_{ik}(t)=\sqrt{(x_k(t)-x_i)^2+(y_k(t)-y_i)^2+h_k^2}$ and the projected distance on the 2D plane (X,Y) is $d_{ik}(t)=\sqrt{(x_k(t)-x_i)^2+(y_k(t)-y_i)^2}$.  
%According to \cite{3gpp.36.331}, which denoted by a UAV $u_k$ is allowed to fly at most 300m above the ground level. Assume a set of UAVs which denoted by $U$ contains $N_U$ UAV. UAVs  are followed by a highway to move with same direction and different lanes. $u_k \subset U$ and $k \in \{1, \dots, N_U \}$  Each UAV has a 3D coordinate with $(x_u,y_u,h_u)$, which are the longitudinal position and lateral position and height respectively.

%We suppose UAVs served by terrestial cellular network. We consider a downlink network, whcih is $N_R$ RF BSs (BSs) serve  $N_U$ UAV users.  the BS $i$  locates at $(x_\mathrm{RF}^{i},y_\mathrm{RF}^{i},z_\mathrm{RF}^{i})$. $i \in \{1, \dots, N_R \}$ In this paper, we ignore the height of BSs. Each UAV user can associate to only one RF BS at a time.  The  distance  and the projection distance for association between UAV-BS at time step $t$ is defined by $r_{ik}(t) = \sqrt{(h_k(t))^2 + (x_{k}(t)-x_i)^2 + (y_{k}(t)-y_i)^2}$ and   $d_{ik}(t) = \sqrt{ (x_{k}(t)-x_i)^2 + (y_{k}(t)-y_i)^2}$  respectively.

% Figure environment removed

%% Figure environment removed

\subsection{G2A Channel Model}
%To alleviate the congestion at the terrestrial BS, a UAV-BS is utilized to serve as many users as possible. 
%It is assumed that each UAV requires a specific quality of service (QoS) among $K$ QoS levels in terms of 
%, which is one of $K$ options and is defined in terms of the 
%signal-to-noise ratio (SIR). Each BS has the capacity to serve a limited number of UAVs. %We denote $U$ as the set of users. 
%We consider a downlink network between  $N_R$ RF BSs (BSs) and  $N_U$ users. 

According to 3GPP \cite{3gpp777}, the ground-to-air (G2A) channel model is characterized by the BS's antenna gain, and the experienced path loss and line-of-sight (LoS) probability. 
%An illustration of the system model is presented in Figs. \ref{fig:BS_distributions_projection}-\ref{fig:BS_distributions_vertical}.

%The other signal link and interference links are visualized in projection view and vertical 


\subsubsection{BS's antenna gain}
In cellular-connected aerial networks, UAVs rely on the radiating sidelobes to connect to terrestrial BSs. Hence, it is important to accurately model the 3D radiation pattern of BSs for cellular-connected UAVs. We opt here for the 3GPP antenna pattern model \cite{3gpp777} that mimics realistic antenna radiation patterns. Specifically, each BS is divided into three sectors, each equipped with cross-polarized antennas to create a uniform linear array (ULA). Each antenna element provides a gain up to $G_{\max}=8$ dBi through the direction of the main lobe \cite{3gpp777}. The antenna element pattern provides different gains on sidelobes depending on the azimuth and elevation angles of the associated UAV \cite{cherif2022cellular}. The latter are given by 
% $ G_{\mathrm{az}}(\phi_i) =\min \left\{ 12 \left( \frac{\phi_i}{\phi_\mathrm{3dB}} \right),G_m  \right\}$ and $G_{\mathrm{el}}(\theta_i) =\min \left\{ 12 \left( \frac{\theta_i}{\theta_\mathrm{3dB}} \right),\mathrm{SLA}  \right\}$ .
\begin{equation}
    G_{\mathrm{az}}(\phi_{ik}(t)) =\min \left\{ 12 \left( \frac{\phi_{ik}(t)}{\phi_\mathrm{3dB}} \right),\mathrm{G_m}  \right\},
\end{equation}
and
\begin{equation}
    G_{\mathrm{el}}(\theta_{ik}(t)) =\min \left\{ 12 \left( \frac{\theta_{ik}(t)}{\theta_\mathrm{3dB}} \right), \mathrm{SLA}  \right\},
\end{equation}
where $\phi_{ik}(t) = \arctan \left( \frac{h_k}{d_{ik}(t)}\right)$ and $\theta_{ik}(t) = \arctan \left(\frac{y_k(t)-y_i}{x_k(t)-x_i}\right)$ are the azimuth and elevation angles between BS $i$ and UAV $k$. $\phi_\mathrm{3dB}=\theta_\mathrm{3dB}=\frac{65\pi}{180}$ at 3dB bandwidths. In addition,  $\mathrm{G_m} $ and $\mathrm{SLA}$ are the antenna nulls thresholds, which are fixed at 30 dB in our study.   The antenna element gain is defined by \cite{cherif2022cellular}
\begin{flalign}
    G(\theta_{ik}(t), \phi_{ik}(t)) &= G_{\mathrm{max}} \\ 
    &- \min \{- (G_{\mathrm{az}}(\phi_{ik}(t)) + G_{\mathrm{el}}(\theta_{ik}(t)) ) , G_m \} \nonumber.
\end{flalign}
Assuming that BS $i$ has $N$ antennas inter-separated by half of the wavelength distance \cite{cherif2022cellular}, the array factor, denoted AF, of the ULA of BS $i$ towards UAV $k$ is expressed by
\begin{equation}
    \mathrm{AF}(\theta_{ik}(t)) = \frac{\sin(\frac{N\pi}{2} (\sin \theta_{ik}(t) - \sin \theta_i^d))}{\sqrt{N} \sin(\frac{\pi}{2} (\sin\theta_{ik}(t) - \sin\theta_i^d))},
\end{equation}
where $\theta_i^d$ is the down-tilt of BS $i$'s ULA. Finally, the array radiation pattern from BS $i$ towards UAV $k$ is written as
\begin{equation}
G_{ik}(t) = G(\theta_{ik}(t),\phi_{ik}(t)) + \mathrm{AF}(\theta_{ik}(t)), \; \forall i \in \mathcal{B}, \forall k \in \mathcal{U}.
\end{equation}
% % right now, we are not considering beam pattern 
% Assuming directional-antenna equipped BSs to serve UAV users. Antenna gain is defined by \cite{cherif2020optimal}
% \begin{equation}
%     G= \begin{cases}
% G_\mathrm{3dB},\quad & -\frac{\theta_B}{2} \leq \psi \leq \frac{\theta_B}{2}   \\
% g_s,\quad & \mathrm{otherwise}
% \end{cases} 
% \end{equation}
% where $\frac{\theta_B}{2}, \psi, G_\mathrm{3dB},g_s$ are BS antenna's half beamwidth in radians, aector angle in radians, main lobe gain and side-lobe antenna gain respectively. 
% Our research aims at provide cellular connectivity for the maximum number of UAV users possible. To achieve this, BS should be positioned above the UAV users and its antenna should be tilted down. This will ensure that the cellular network can support the operations of the UAV users. It is important that the altitude of the UAVs are higher than 80m. In addition, we assume that the effective isotropic radiated power (EIRP) of the BS, which is denoted as $P_T$, defined by
% \begin{equation}
%     P_T = P_i(\theta_B) + G_{\mathrm{3dB}}
% \end{equation}

\subsubsection{LoS probability}
The likelihood of UAV $k$ having a LoS with BS $i$ primarily relies on the altitude of the UAV and the surrounding  environment. 
Assuming that $h_k \in [22.5,100]$ m, the probability of LoS is given by \cite{cherif2022cellular} is 
\begin{equation}
\small
    P_{\mathrm{LoS}}(q_{ik}(t))= 
\begin{cases}
1,\quad &d_{ik}(t) \leq d_1 \\
\frac{d_1}{d_{ik}(t)} + e^{-\frac{d_{ik}(t)}{p_1}}\left(1- \frac{d_1}{d_{ik}(t)} \right),\quad &\text{otherwise,}
\end{cases} 
\end{equation}
where $d_1 = \max\{460\log_{10}(h_k)-700,18\}$ and $p_1 = 4300\log_{10}(h_k)-3800$. If $h_k \in [100,300]$ m, $P_{\mathrm{LoS}}(q_{ik}(t)) = 1$. Thus, the probability of Non-LoS (NLoS) is written as $P_{\mathrm{NLoS}}(q_{ik}(t)) = 1 - P_{\mathrm{LoS}}(q_{ik}(t))$.

%  The path loss is generated by LoS and NLoS links \cite{alzenad20173} in dB computed by 

% \begin{equation}
%  L_{\mathrm{LoS}} = 20 \log \left( \frac{4 \pi f_c d_{ik}}{c} \right)+\eta_{\mathrm{LoS}}
% \end{equation} 
% \begin{equation}
%        L_{\mathrm{NLoS}} = 20 \log \left( \frac{4 \pi f_c d_{ik}}{c} \right) + \eta_{\mathrm{NLoS}}
% \end{equation}
% where $\eta_{\mathrm{LoS}}$ and $\eta_{\mathrm{NLoS}}$ are average additional losses for LoS and NLoS. 

% The probability of LoS association between UAV user $u_k$ and BS $i$ at an elevation angle $\theta_{ik} = \arctan ( \frac{h}{r_{ik}})$ is defined by 
% \begin{equation}
%     P_{\mathrm{LoS}} = \frac{1}{1+a \exp{-b(\theta_{ik} - a)}}
% \end{equation}
% where a and b is relied on UAVs surrounding environments. We compute $P_{\mathrm{NLoS}} = 1 - P_{\mathrm{LoS}} $ and $\theta_{ik}$ is on radian in this research.

\subsubsection{Path loss}
For the sake of simplicity, we consider the mean path loss  since we focus here on the long-term operation of cellular-connected UAVs rather than the short term \cite{alzenad20173}. The probabilistic mean path loss between BS $i$ and UAV $k$ at time slot $t$ can be expressed by 
% \begin{equation}
\begin{flalign}
    L_{ik}(t) &= L_i^{\mathrm{LoS}}P_{\mathrm{LoS}}(r_{ik}(t))  \\ &+L_i^{\mathrm{NLoS}}P_{\mathrm{NLoS}}(r_{ik}(t)), \forall i \in \mathcal{B},\nonumber
\end{flalign}
% \end{equation}
where  $L_i^{\mathrm{LoS}}$ and $L_i^{\mathrm{NLoS}}$ are the path loss related to LoS and NLoS communication links, respectively, as defined in \cite[Tables B-1 and B-2]{3gpp777}.
% \begin{equation}
%     L(h_k(t),r_{ik}(t)) = L_{\mathrm{LoS}}(t)P_{\mathrm{LoS}}(t)+L_{\mathrm{NLoS}}(t)P_{\mathrm{NLoS}}(t)
% \end{equation}
% We can rewrite this as 

% \begin{multline}
%     L(h_k(t),r_{ik}(t)) =\frac{A}{1+a \exp{-b(\theta_{ik}(t) - a)}} + \\ 20\log \left(\frac{r_{ik}(t)}{\cos({\theta_{ik}(t)})}\right)+ B
% \end{multline}
% Where $A$ and $B$ are constant depend on environment. $A = \eta_{\mathrm{LoS}} -\eta_{\mathrm{NLoS}} $ and  $B =  20 \log \left( \frac{4 \pi f_c }{c} \right)+\eta_{\mathrm{NLoS}}$

\subsection{Received Power and Achievable Data Rate Analysis}
Assuming that UAV $k$ has an omni-directional antenna, and using the G2A channel model, the average power received from BS $i$ can be expressed by
% \begin{equation}
%     P_k^i = P_T - L(h_k(t),r_{ik}(t)) - P_n
% \end{equation}
\begin{equation}
    P_{ik}(t) = P_T + G_{ik}(t) - L_{ik}(t) - P_n , \forall i \in \mathcal{B}, \forall k \in \mathcal{U},
\end{equation}
where $P_T$ is the transmit power of any BS $i$ and $P_n$ is the noise power (in dBm).
The quality of the link between UAV $k$ and BS $i$ is determined by the strength of the received signal from the latter, evaluated with $P_{ik}(t)$. However, since the aerial highways can be served by several terrestrial BSs with the same frequency, mainly due to the strong LoS between BSs and UAVs, then significant interference can be generated. Consequently, the quality of a communication link is rather evaluated using the signal-to-interference-ratio (SIR)\footnote{In practice, the quality of the link should be evaluated using the signal-to-interference-plus-noise-ratio (SINR). However, due to the significant interference generated in the considered system model, we ignore the noise's effect.}. The latter is written by
%A UAV users $u_k$ have cleared LoS conditions with several base stations sharing same frequency resource at time step $t$. Hence, we assume the communication performance rely on the the level of SIR, defined by
\begin{equation}
    \text{SIR}_{ik}(t) = \frac{P_{ik}(t)}{\sum_{\substack{j=1,\\ j \neq i}}^{N_R}P_{jk}(t)}, \forall i = 1 , \dots ,  N_R, \forall k=1, \dots,N_U.
\end{equation}
Assuming that all BSs use the same bandwidth $W_R$, then the achievable data rate between BS $i$ and UAV $k$ is given by 
%Each BS has the available bandwidth given by $W_R$.  All UAVs are equipped with a single antenna. Each UAV measures the channel quality as well as signal and interference levels from each BS.  Subsequently, the data rate of each UAV to BS link  can be computed as
 \begin{equation}
     R_{ik}(t) = W_R\log_2(1+\text{SIR}_{ik}(t)), \forall i \in \mathcal{B}, \forall k \in \mathcal{U}.
 \end{equation}


\subsection{Handovers}
Since a flying UAV $k$ can evaluate $P_{ik}(t)$ from neighbouring BSs, i.e., $i \in \mathcal{B}_{k}(t)$ where $\mathcal{B}_{k}(t)$ is the set of the closest $n_{\rm rf}$ BSs to UAV $k$ that can serve UAV $k$, i.e., BS $i \in \mathcal{B}_{k}(t)$ if $q_{ik}(t)\leq d_{th}$ and SIR$_{ik}(t)\geq \gamma_{th}$, where $d_{th}$ is the maximal communication distance between any BS and UAV $k$ and $\gamma_{th}$ is the UAV's reception sensitivity. Subsequently, a UAV can trigger a HO event whenever required, e.g., when SIR$_{i_0k}(t)< \gamma_{th}$, where BS $i_0$ is the one that UAV $k$ is currently associated with.    
To reflect HO events, let $c_k(t)$ be the index of the BS to which UAV $k$ is associated at time slot $t$ and $\eta_k$ be the HO binary variable, such that $\eta_k(t,t+1)=1$ if $c_{k}(t)\neq c_k(t+1)$ and $\eta_k(t,t+1)=0$ otherwise.  
Frequent HOs can severely impact the received SIR due to HO overhead and risk of HO ping-pong effect \cite{chen2020efficient}.  




%From \cite{10001396} and \cite{cherif2021disconnectivity}, to simulate individual surrounding UAVs movement on the 3D-highway, we consider the intelligent driver model, where acceleration is modeled as follows \cite{treiber2013traffic}.

%All UAVs are equipped with a single antenna. Each UAV measures the channel quality as well as signal and interference levels from each BS.  Subsequently, the transmission data rate of each AV to BS link  can be computed as $T_{ik}$,  Each BS  has a maximum limit of $Q_R$ on the number of UAVs that can be supported. Consequently, each BS can calculate the number of possible UAV user $u_k$ associations at each time instance denoted by $n_c$. As UAVs fly along the corridor, they switch from (connecting to) one BS to another, which is called a ``HO\footnote{ We define HO is the situation where the AV connection moves from one BS to another BS.}". 



%-------------------------
%\textcolor{red}{Moreover, we assume that each BS can be associated with a maximum number of UAVs $Q_R$, and that BS $i$ continuously holds the number of associated UAVs, denoted $n_i(t)$, $\forall i \in \mathcal{B}$.} 

\section{Problem Formulation as MDP and Proposed DRL with Action Branching}
Given the described system model, we aim to collaborative optimize the autonomous motion of multiple UAVs travelling along a 3D highway, such that both the transportation and communication performances, e.g., collisions, connectivity, and HOs, are improved. First, we specify the state-action space and rewards of our system. Then, we present the proposed collaborative RL-based solutions to control the UAVs. These solutions are based on the BDQN and BDDQN algorithms.    


%In this section, we specify the state-action space, and rewards of the considered problem. Then, we present the BDQN and BDDQN algorithm.

%Each RBS and TBS has the available bandwidth given by $W_R$ and $W_T$, respectively. where $W_j$ is the transmission bandwidth of the BS $j$. Each UAV maintains a list of top three BSs in terms of the achievable SIR and then informs those BSs. Vertical HO refers to the situation where the AV connection moves from one specific type of BS to a different type of BS, such as moving from RBS to a TBS. We discourage HOs by introducing a HO penalty ($\mu$) which is higher for TBS and lower for RBS because THz transmission  is limited to short  distances.

\subsection{Observation and State Space}
The observation space $\mathcal{O}$ 
%which typically takes the form of a matrix, 
provides RL agents with the necessary information to take actions that result in rewards. For our system, our observation space is composed of transportation and communication observations.  The transportation observation space is known as kinematics and is included in the \textit{highway-env}  environment \cite{highway-env}. The kinematics observation consists of a $V \times F$ array that describes a list of nearby UAVs $V < N_U$ based on $F$ specific features, namely ($\textbf{q}_k(t), \textbf{v}_k(t), n_\mathrm{rf}$), where $\textbf{v}_k(t)=[v_k^x(t),v_k^y(t)]$ is the directional speed of UAV $k$ on (X,Y) plane at time $t$, $v_k^x(t)$ represents the longitudinal speed, and $v_k^y(t)$ represents the latitudinal speed. $n_\mathrm{rf}$ is number of feasible BSs in radius of 1000m of target UAV. The feature values may be normalized within a predetermined range, with the normalization relative to the UAV that is about to take an action. 
%$n_\mathrm{rf}$ are the number of BSs  in the radius of 1000m of the current position of UAV user.   
% A normalized transportation observation space example $o=(u_k, \textbf{q}_k(t), \textbf{v}_k(t), n_\mathrm{rf})$ is presented in Table \ref{table:states}. 
Moreover, each UAV $k$ observes communications-related features such as the received SIR levels from BSs in $\mathcal{B}_k(t)$.  
%If fewer UAVs are observed than the number of rows in the matrix ($V$), the remaining rows are filled with zeros as placeholders. 
The presence of several UAVs allows to simulate different traffic flow scenarios as in \cite{10077729}.
Indeed, as the density of UAVs in the highway increases, higher competition is expected to connect to the best terrestrial BSs among them. This holds true assuming that BS $i$ cannot be associated to more than $Q_i$ UAVs at the same time, $\forall i \in \mathcal{B}$. Thus, each BS $i$ has to continuously keep track of the number of associated UAVs to it all the time, denoted $n_i(t)$, $\forall i \in \mathcal{B}$.
% u_1, \ldots, u_V,
Consequently, a state $s_t$ for an RL agent at UAV $k$ is constituted from several observations as $s_t=( \textbf{q}_1(t), \ldots, \textbf{q}_V(t), \textbf{v}_1(t), \ldots, \textbf{v}_V(t), n_\mathrm{rf}, \text{SIR}_{1k}(t),$ $\ldots,\text{SIR}_{n_{\mathrm{rf}}k},Q_1, \ldots, Q_{n_{\mathrm{rf}}},n_1(t),\ldots,Q_{n_{\mathrm{rf}}}(t))$.  

%We also introduce the surrounding UAVs in the system to simulate different traffic flow scenarios in 3D aeriation highway \cite{10077729}. The more surrounding UAVs, the higher competition resource sharing among UAVs.
% Although other observation spaces could be employed, kinematics \cite{highway-env} was selected because it is built-in and provides the most realistic representation for real traffic scenarios.The current ego-UAV is always described in the first row of the observation space.

% \begin{table}[]
% \centering
% \caption{Example of MDP observation for one UAV}
% \label{table:states} 
% \begin{tabular}{ccccccc}
% \textbf{Nearby UAV} & \textbf{$x_k(t)$} & \textbf{$y_k(t)$} & \textbf{$v_k^x(t)$} & \textbf{$v_k^y(t)$} & \textbf{$n_{RF}(t)$}  \\ \midrule
% % ego UAV      & 6            & 5            & 20             & 0              & 0                                \\ \midrule
% $u_1$        & -8           & 5            & 15             & 0.5            & 1                                 \\ \midrule
% $u_2$        & 9            & 15           & 22             & -0.5           & 3                              \\ \midrule
% \vdots         & \vdots          & \vdots          & \vdots            & \vdots            & \vdots                            \\ \midrule
% $u_V$        & 13           & 10           & 19             & 0              & 2                            \\ \bottomrule
% \end{tabular}
% \end{table}

\subsection{Action Space}
\label{sec:2daction}
At each time step $t$, UAV $u_k$ selects action $a_t=(a_t^{\rm tran}, a_t^{tele}) \in \mathcal{A}_{\rm tran} \times \mathcal{A}_{\rm tele}$, where $a_t^{\rm tran}$ is the moving transportation action, i.e., trajectory action and $a_t^{tele}$ is the communication-related action, i.e., association with a terrestrial BS.
%$a_t^\mathrm{tele}$ from the action space $\mathcal{A}$. Specifically, the action space consists of the trajectory action space $\mathcal{A}_\mathrm{tran}$ and communication action space $\mathcal{A}_\mathrm{tele}$, i.e., $\mathcal{A} = \{  \mathcal{A}_\mathrm{tran},  \mathcal{A}_\mathrm{tele}\}$. 
%For each time step, the UAV user $u_k$ must select both  $a_t^\mathrm{tran}$ and   $a_t^\mathrm{tele}$. 
$\mathcal{A}_{\rm tran}=\{a_{\rm tran}^1,\ldots,a_{\rm tran}^5\}$, where $a_{\rm tran}^1$ is the change lane to the left lane action, $a_{\rm tran}^2$ is maintaining the same lane, $a_{\rm tran}^3$ is the change lane to the right one, $a_{\rm tran}^4$ is accelerating within the same lane, and $a_{\rm tran}^5$ is decelerating within the same lane. Similarly, the communication action space for $u_k$ at time $t$ can be given by $\mathcal{A}_{k,\rm tele}(t)=\left\{ c_k^1(t), \ldots, c_k^{n}(t) \right\}$, where $c_k^i(t)$ is the $i^{th}$ potential BS to be associated with.
Based on the quota of each BS $Q_i$, UAV computes a \textit{weighted rate metric}, denoted WR, that encourages traffic load balancing between BSs and discourages unnecessary HOs. It is expressed by
\begin{equation}
    \text{WR}_{ik}(t) = \frac{R_{ik}(t)}{\min \left(Q_i, n_i(t) \right)} (1 - \mu), \forall i=1,\ldots,n,
\end{equation}
   where $\mu$ denotes the HO penalty, written as
    \begin{equation}
    {
    \mu = 
    \begin{dcases}
    % 0.5 ,& \text{if switch to a TBS} \\
    0.1, & \text{if HO is triggered,} \\
    0, & \text{otherwise.} 
    \end{dcases}
    }   
    \end{equation}
This criterion will be taken into account to further reduce the set of potential BSs. Specifically, the 
final BSs' set should be composed with $n \leq n_{\rm rf}$ candidates, which belong to $\mathcal{B}_k(t)$, satisfy   
$n_i(t)<Q_i$, and obtained the best WSIR values. 
  
%We have 5 actions selected from the aeration moving side and, for each aeration moving  related action, we have three communication related actions.For each time step $t$, Thus, the total number of two different dimensional actions are 15, which include :
%\subsubsection{Aeriation moving action}
%There are 5 actions for UAV user $u_k$ select as follows. \textbf{(1)} Change lane to left lane. \textbf{(2)} IDLE, maintain the current motion. \textbf{(3)} Change lane to right lane. \textbf{(4)} Accelerating on the current lane. \textbf{(5)} Decelerating on the current lane. For accelerating and decelerating, we applied the intelligent driver model and MOBIL lane change model \cite{treiber2013traffic} to dynamically change the acceleration/deceleration to maintain safety and pursue higher transportation efficiency. 
%\subsubsection{C2A Network Selection Action} are listed as follows.
%\textbf{(1)} \textit{HO-aware Network Selection:} Each AV prepares a sorted list of three BSs offering best SIR and associates to those who can fulfill SIR requirement of the AV $R_{\mathrm{th}}$. 
%Then, UAV $k$ collects the traffic load information from these three BSs (i.e.,  the number of UAVs $u_k$ associated with each BS $n_s$). 
    % \item \textbf{Keep Previous Selection:} $[0,0,0]$  Keep the previous base station connection . HO does not occur.


%In the particular case of $\mu=0$, the actions no longer take into account the HO effect.  

%\textcolor{red}{Each UAV then  connects to the BS with  maximum ${T}_{{ik}}$, if $Q_j \geq n_s$. Otherwise, the UAV  recursively selects the other vacant best-performing BS in terms of $P_{{ik}}$.}


% \textcolor{red}{THIS HAS TO BE MOVED SOMEWHERE IN EXPERIMENT RESULTS, WHERE YOU WILL DEFINE THE BENCHMARK APPROACHES. \textbf{(2)}~\textit{Network Selection with No HO Consideration:}
The UAV computes $P_{{ik}}$ by substituting $\mu=0$ and chooses to connect to BS with the maximum  ${T}_{{ik}}$, if $Q_j \geq n_i(t)$, Otherwise, the UAV recursively selects the next vacant best-performing BS. 
%in order by the performance of ${T}_{{ik}}$. 
%In this case, we still consider competitive resource sharing in terms of $n_s$ and $Q_R$ constraints.
%\textbf{(3)} \textit{Network Selection based on Maximum  data rate:}
%The AV computes $P_{ik} = T_{ik} $ and chooses to connect to a BS with the maximum  data rate.
% \footnote{if $Q_j \geq n_s$, UAV chooses to connect to a BS with the maximum HO-aware data rate.  Otherwise, the best performance BS are not available to connect with UAV, we recursively select the other vacant best-performing BS in order by performance of ${T}_{{ik}}$.}
% }



\subsection{Reward Function Design}
The definition of the associated reward function is directly related to the optimization of both UAV transportation and communication performances. 
%The reward function is critical for accelerating the convergence of the model. 
%When the UAV is receiving a higher HO-aware data rate, while guaranteeing safe travelling (i.e. no collision with the other UAVs), it receives a positive reward. By taking any other actions, which may lead to an increase of the HOs, collision or traffic violation, the UAV user receives a penalty. 

\subsubsection{UAV Transportation Reward}
We define the UAV transportation reward as follows \cite{highway-env}:
\begin{equation}
    r^{\mathrm{tran}}_{k} (t)=  \omega_1 \left( \frac{||\mathbf{v}_k(t)|| -v_\mathrm{min}}{v_\mathrm{max}-v_\mathrm{min}} \right)- \omega_2 \cdot \delta, \forall k \in \mathcal{U},
\end{equation}
where $v_{\min}$ and $v_{\max}$ are the minimum and maximum speed limits, and $\delta$ is the collision indicator. $\omega_1$ and $\omega_2=1-\omega_1$ are the weights that adjust the value of the UAV transportation reward with its collision penalty.
%In order to ensure that the rewards fall within a certain range and to make sure that the optimal policy is not affected by rewards' scaling or shifting, we normalize them conventionally. This normalization of rewards has been observed to have practical benefits in BDQN. 
It is important to note that negative rewards are not allowed since they might encourage the agent to prioritize ending an episode early, by causing a collision, instead of taking the risk of receiving a negative return if no satisfactory trajectory is available.

\subsubsection{UAV Communication Reward}
% For communication reward, we introduce  $P_\mathrm{min} $ and $P_\mathrm{max}$ to define the upper and lower threshold to normalize the HO aware throughput SIR to commute reward. We apply linear map to normalize this throughput.
% \begin{equation}
% T_q= \begin{cases}
% 0,\quad &P_q\leq P_\mathrm{min} \\
%  \frac{P_q -P_\mathrm{min}}{P_\mathrm{max}-P_\mathrm{min}},\quad &P_\mathrm{min} < P_q < P_\mathrm{max} \\
% 1,\quad &P_q \geq P_\mathrm{max} 
% \end{cases} 
% \end{equation}

% y= \begin{cases}
% -x,\quad &x\leq 0 \\
% x,\quad &x>0
% \end{cases} 

% the linear map
We define the communication reward as follows:
%apply linear map to normalize the HO aware throughput $T_{ik}$.  Defining the reward from communication side as follows:
\begin{equation}
    r^{\mathrm{tele}}_{k}(t)= \omega_3 R_{i_0k}(t) \left(1- \text{min}(1,\xi_k (t))\right),% + w_6 j +w_5 l
\end{equation}
where  $R_{i_0k}(t)$ is the achievable data rate when associated with BS $i_0$, and  $\xi_k(t)$ is the HO probability, computed by dividing the number of HOs accounted until the current time $t$ by the time duration of previous time slots in the episode. 
%we can determine the discounted expected cumulative reward for UAV $k$ as 
%\begin{equation}
%    R_k = \mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^t\left(r^{\mathrm{tran}}_{k}(t) + r^{\mathrm{tele}}_{k}(t)\right)\right], \forall k \in \mathcal{U},
%\end{equation}
%and 
% Subsequently, 
% the overall discounted expected cumulative reward among all UAVs can be given as 
% \begin{equation}
% \label{eqn:total_reard}
% R(t)=\mathbb{E}\left[\sum_{k=1}^{N_U} \sum_{t=0}^{\infty}\gamma^t\left(r^{\mathrm{tran}}_{k}(t) + r^{\mathrm{tele}}_{k}(t)\right)\right],
% \end{equation} 
% where $\gamma \in [0,1]$ is the discount factor. 
%To obtain  a reasonable performance, we consider intermediate reward  and find the policy $\mathcal{J}$ to maximize the discounted expected cumulative rewards among all UAV users in the set from the among travelling  , i.e.,
%\begin{equation}
%        R = \sum_{k=1}^{N_U} {R_k}, %\mathbb{E}\left[\sum_{k=1}^{N_U}\sum_{t=0}^{\infty}\gamma^t(r^{\mathrm{tran}}_{k}(t) + r^{\mathrm{tele}}_{k}(t))\right],
%\end{equation}

%The state transitions and rewards are a function of the  UAV  environment and actions taken by the UAV user $u_k$. At time step $t$, the transition from state $s_t$ to $s_{t+1}$ can be defined as the conditional transition probability given by $p(s_{t+1},r_t|s_t,a_t^\mathrm{tele},a_t^\mathrm{tran})$. Note that the UAVs do not have any prior information of the transition probabilities. 

% \subsection{Deep Q-Learning Algorithm}

% \begin{algorithm}
% \SetAlgoLined
% \KwResult{Action function $Q_\theta$ and Policy $\mathcal{J}$ }
% \KwData{$Q$-network, Experience replay memory $D$, mini batch-size $m$}
% \textbf{Initialization:} $\mathcal{D} \gets \bold{0}$,  Q-network weights $\theta \gets \bold{0}$, Target network $\theta^* \gets \theta$, $Q(s,a)$, AVs, TBSs, RBSs 
% \While{$\mathrm{episode} < \mathrm{episode \ limit \ or \ run \ time}< \mathrm{time \ limit}$}{
%   $t \gets 0$,
%   $s_t \gets \mathrm{horizon \ limit} $\\
% \While{$t \leq \mathrm{horizon \ limit} $}{AV selects $a_t$ by $\epsilon$-greedy search as  \textbf{Algorithm-1}.\\  

% Derive $ a^{\mathrm{tran} }_{t}$ and $a^{\mathrm{tele} }_{t}$  from $a_t$\\
% Apply $a^{\mathrm{tran} }_{t}$ and $a^{\mathrm{tele} }_{t}$ to AV;\\
% Compute reward $r_t$ and update $s'$;\\
% Store $(s_t,a_t,s_{t+1},r_t)$ to $\mathcal{D}$;\\
% \textbf{Experience Replay:} sample transitions mini-batch in $\mathcal{D}$ $(s_k,a_k,r_k,s'_k)$ where  $k \in m$;\\
% \textbf{Set target-$Q$ function:} $ \hat{y}_k = r_k + \gamma\max_{a'}\hat{Q}(s'_k,a'_k;\theta_k) $;\\
% \textbf{Set real $Q$-function:} $ y_k = {Q}(s_t,a_t;\theta)$;\\
% Compute loss \\$ \mathcal{L}(\theta) =  \frac{1}{m} \sum_{k \in m} (y_k - \hat{y}_k)^2$;\\
% Perform gradient descent step by minimizing loss $\mathcal{L}$;
% $\theta \gets \theta - a_{t} \cdot \mathcal{L}(\theta) \cdot \triangledown_{\theta}{y_k}$;\\
% Update deep Q network weights $\theta \gets \theta^*$
% $Q_d(s,a_d) = V(s) + (A_d(s,a_d) - \max_{a'_d \in A_d}A_d(s,a'_d))$
%   }
%    Policy $\mathcal{J}$ updated in terms of Q
%  }
%   \caption{Double DQN Algorithm to Optimize Network Selection and Autonomous Driving Policies}
% \end{algorithm}


% Figure environment removed

% % Figure environment removed

%   \begin{table}[t]
%    \caption{Simulation Parameters}
%  \label{tab:parameters}
%   \centering
%   \begin{tabular}{|p{4cm}|p{2cm}|}
%   \hline
%  Parameter & Value\\
%   \hline
%   % Epsilon $\epsilon$ & 0.1\\
%   % Epsilon decay rate & 20\\
%   % discount factor $\gamma$ & 0.2\\
%   % learning rate $\alpha$   & 0.005 \\
%   % episodes & 10000 \\
%   % Desire velocity ($m/s$) & 30,40,50 \\
% %   minimum velocity ($m/s$) & 15 \\
% %   maximum velocity ($m/s$) & 45 \\
%   number of UAVs  & 5\\
%   RF BS amount  & 5,10,15,20 \\
%   Carrier Frequency for RF BSs & 2.1 GHz \\
%   Maxiumum users support for single BS & 3 \\
%   BS Transmission Power $P_T$ & 40 dBm \\
%   $\eta_{\mathrm{LoS}}$ & 1 \\
%   $\eta_{\mathrm{NLoS}}$ & 20 \\
%   $P_{\mathrm{min}}$ & -100 \\
%   $P_{\mathrm{max}}$ & -80 \\
%   % $(a , b)$ in Eq.5,7 & (-1.5,3.5) \\
%   % Bandwidth & \\
%   Number of lanes & 4 \\
%     \hline
%   Training FNN Layers and Neurons  & (256,256) \\
%   Training learning rate $\alpha$ & $5\mathrm{e}{-4}$ \\
%   Training discount factor $\gamma$ & $0.8$ \\
%   Training batch size  & 32 \\
  
  
%   \hline
%  \end{tabular}
% \label{tab:params}
%  \end{table}

\subsection{Proposed Branching Dueling Q-Network-based Methods}
The use of discrete-action algorithms has contributed to many recent successes in deep reinforcement learning. However, implementing these algorithms in high-dimensional action tasks is challenging due to the exponential increase in the size of action space. In our study, for each time step $t$, we need to apply both communication action and transportation action on $N_U$ RL agents. 
To cope with such complex action design, authors of \cite{tavakoli2018action} introduced a novel RL agent based on branching dueling Q-network (BDQ) and illustrate the performance of branching deep Q-network (BDQN) or dueling double deep Q-network (BDDQN). BDQ features a decision module shared among multiple network branches, each corresponding to an action dimension, e.g., the transportation and communication action dimensions in our work. This approach allows for independent handling of each individual action dimension, resulting in a linear increase in the number of network outputs with the degrees of freedom. It also demonstrates the importance of the shared decision module in coordinating the distributed action branches. In this work, we take advantage of this method by deploying BDQ agents at the UAVs, and each of them makes actions branching for $\mathcal{A}_{\rm tran}$ and $\mathcal{A}_{\rm tele}$. 

\begin{algorithm}[ht!]
\caption{Proposed BDQN/BDDQN for Multi-UAV Speed Control and BS Association}
\SetAlgoLined
\KwResult{Action function $Q_\theta$ and Policy $\mathcal{J}$ }
\KwData{$Q$-network, Experience replay memory $\mathcal{D}$, mini batch-size $m$}
\textbf{Initialization:} $\mathcal{D} \gets \mathbf{0}$,  Q-network weights $\theta \gets \mathbf{0}$, Target network $\theta^* \gets \theta$, $Q(s,a)$, UAVs, BSs \\
\While{$\mathrm{episode} < \mathrm{episode \ limit \ or \ run \ time}< \mathrm{time \ limit}$}{
  $t \gets 0$,
  $s_t \gets \mathrm{horizon \ limit} $\\
\While{$t \leq \mathrm{horizon \ limit} $}{Each UAV $k$ selects $a_t$ by $\epsilon$-greedy search as  $a_t$:
% \textbf{Algorithm-1}.\\  
\begin{align*}
\begin{cases} 
\text{Select $a_t$ from } \mathcal{A} & \text{prob. } \epsilon \\
\text{Select } a_t=\max_{a \in \mathcal{A}}{Q_{\theta}(s_t,a_t)} & \text{prob. } (1 - \epsilon)
\end{cases}
\end{align*}
%$a_t$ select action from $\mathcal{A}$ with probability of $\epsilon$ or Select $a_t$ frpm $\max_{a \in \mathcal{A}}{Q_{\theta}(s_t,a_t)}$ with probability of $ 1-\epsilon$.\\ 
Extract $a^{\mathrm{tran} }_{t}$ and $a^{\mathrm{tele} }_{t}$  from $a_t$ and apply them to UAV $k$;\\
% Compute reward $r_t=r_k^{\rm tran}(t)$ and $r_k^{\rm tele}(t)$ update $s_{t+1}$, Compute instant Reward $R_t$ in Eq. \ref{eqn:total_reard}; \\
\textbf{State-value Estimator:} Apply Eq. \ref{eqn:common_state_value_estimator} to compute $Q_d(s,a_d)$ \\
Store $(s_t,a_t,s_{t+1},r^{\mathrm{tran}}(t),r^{\mathrm{tele}}(t))$ to $\mathcal{D}$;\\
\textbf{Experience Replay:} sample transitions mini-batch in $\mathcal{D}$ $(s_k,a_k,r_k,s'_k)$ where  $k \in m$;\\
\textbf{Set target-$Q$ function:} Set $\hat{y}_k$ by Eq. \ref{eqn:target_function} % $ \hat{y}_k = r_k + \gamma\max_{a'}\hat{Q}(s'_k,a'_k;\theta_k) $;\\
% \begin{equation*}
% \hat{y}_k = 
% \begin{dcases}
%      r_t + \gamma\max_{a'}\hat{Q}(s'_k,a'_k;\theta_k),\;\text{(BDQN)} \\
%     r_t + \gamma Q(s_{k+1},\max_{a'}\hat{Q}(s'_k,a'_k;\theta_k);\theta_k),\\
%     \text{(BDDQN)}\\ 
%     % 0, & \text{Keep Previous BS} 
% \end{dcases}
% \end{equation*}
% \begin{equation*}
% \begin{dcases}
%     r^{\mathrm{tele}}_{k}(t) +  r^{\mathrm{tran}}_{k}(t) +  \frac{\gamma}{2} \sum_d Q_d^-\operatorname*{argmax}_{a_d' \in \mathrm{A}_d} (Q_d(s_k,a_d(t))) \\
%     \text{For BDQN} \\
%     r^{\mathrm{tele}}_{k}(t) +  r^{\mathrm{tran}}_{k}(t) +  \frac{\gamma}{2} \sum_d Q_d^-(s_k',\operatorname*{argmax}_{a_d' \in \mathrm{A}_d} (Q_d(s_k,a_d(t))) \\
%     \text{For BDDQN} \\
% \end{dcases}
% \end{equation*}
% \begin{equation*}
%     r^{\mathrm{tele}}_{k}(t) +  r^{\mathrm{tran}}_{k}(t) +  \frac{\gamma}{2} \sum_d Q_d^-\operatorname*{argmax}_{a_d' \in \mathrm{A}_d} (Q_d(s_k,a_d(t))) 
%     r^{\mathrm{tele}}_{k}(t) +  r^{\mathrm{tran}}_{k}(t) +  \frac{\gamma}{2} \sum_d Q_d^-(s_k',\operatorname*{argmax}_{a_d' \in \mathrm{A}_d} (Q_d(s_k,a_d(t)))
% \end{equation*}
                            % $\hat{y}_k = 
                            % \begin{dcases}
                            %      r_k + \gamma\max_{a'}\hat{Q}(s'_k,a'_k;\theta_k) ,& \text{BDQN} \\
                            %     r_k + \gamma Q(s_{k+1},\max_{a'}\hat{Q}(s'_k,a'_k;\theta_k);\theta_k) , & \text{BDDQN} \\
                            %     % 0, & \text{Keep Previous BS} 
                            % \end{dcases}
                            %  $

\textbf{Set real $Q$-function:} $ y_k = {Q}(s_t,a_t;\theta)$;\\
Compute loss: $\mathcal{L}(\theta) =$ \\
$  \mathbb{E}_{(s_t,a_t,s_{t+1},R_t) \sim \mathcal{D}} \left[ \frac{1}{m} \sum_{k \in m} (y_k - \hat{y}_k)^2 \right]$;
\\
Perform gradient descent step by minimizing loss $\mathcal{L}$;
$\theta \gets \theta - a_{t} \cdot \mathcal{L}(\theta) \cdot \triangledown_{\theta}{y_k}$;\\

Update the deep-$Q$-network weights $\theta \gets \theta^*$
% $Q_d(s,a_d) = V(s) + (A_d(s,a_d) - \max_{a'_d \in A_d}A_d(s,a'_d))$
% $Q_d(s,a_d) = (A_d(s,a_d) - \max_{a'_d \in A_d}A_d(s,a'_d))$
  }
   Policy $\mathcal{J}$ updated in terms of $Q$
 }
\end{algorithm}


%Action branching reinforcement learning \cite{tavakoli2018action} is introduced to our study. 

%Instead,  Authors in \cite{tavakoli2018action} introduces a new agent Branching Dueling Q-Network (BDQ) based on the Deep Q-Network (DQN), Dueling Double Deep Q-Network (BDDQN), which uses the proposed branching architecture. This features a decision module shared among multiple network branches, each corresponding to an action dimension. This approach allows for independent handling of each individual action dimension, resulting in a linear increase in the number of network outputs with the degrees of freedom. This architecture also demonstrates the importance of the shared decision module in coordinating the distributed action branches. Our study take this advantage and implement UAVs moving action and group BSs selection as two action branches via a special aggregation layer.


According to \ref{sec:2daction}, we have two action dimensions and a total of $5 \times n$ sub-actions for each UAV at each time step. For an action dimension $d \in \{1,2\}$, each individual branch Q-value on state $s \in S $ and sub-action $a_d \in \mathcal{A}_d$ ($\mathcal{A}_1=\mathcal{A}_{\rm tran}$ and $\mathcal{A}_2=\mathcal{A}_{\rm tele}$) is defined by 
\begin{equation}
\label{eqn:common_state_value_estimator}
 Q_d(s,a_d) = V(s) + (A_d(s,a_d) - \max_{a'_d \in A_d}A_d(s,a'_d)), \forall d \in \{1,2\}. 
\end{equation}
Each sub-action affects the aggregating layer of $Q_d$ regard to dimension $d$. Based on the double DQN algorithm, we update the state-value estimator and loss function as in  \cite{tavakoli2018action}. 
% $V(s)$ is the shared state value among two branches
We also adopt  the common state-value estimator based on dueling architecture. Dueling architecture reduces similar action redundancies and learning is shared by two branches visualized in Fig. \ref{fig:bdq_agent}. The target $Q$ function  $\hat{y}_k$ in BDDQN defined by \footnote{For the sake of simplicity, we define $\hat{y}_k = r^{\mathrm{tele}}_{k}(t) +  r^{\mathrm{tran}}_{k}(t) +  \frac{\gamma}{2} \sum_d Q_d^-\operatorname*{argmax}_{a_d' \in \mathrm{A}_d} (Q_d(s_k,a_d(t)))$ for BDQN. BDDQN depends on both Q-network and the target network while BDQN only rely on $Q$-network. $Q$-network, target network are designed for agent action selection and agent action evaluation respectively. }
\begin{equation}
\label{eqn:target_function}
   \hat{y}_k = r^{\mathrm{tele}}_{k}(t) +  r^{\mathrm{tran}}_{k}(t) +  \frac{\gamma}{2} \sum_d Q_d^-(s_k',\operatorname*{argmax}_{a_d' \in \mathrm{A}_d} (Q_d(s_k,a_d(t))) 
\end{equation}
where $Q_d^-$ is the branch $d$ of the target network $Q^-$.
% $r^{\mathrm{tele}}_{k}(t) +  r^{\mathrm{tran}}_{k}(t) +  \frac{\gamma}{2} \sum_d Q_d^-\operatorname*{argmax}_{a_d' \in \mathrm{A}_d} (Q_d(s_k,a_d(t))) $ for BDQN. $r^{\mathrm{tele}}_{k}(t) +  r^{\mathrm{tran}}_{k}(t) +  \frac{\gamma}{2} \sum_d Q_d^-(s_k',\operatorname*{argmax}_{a_d' \in \mathrm{A}_d} (Q_d(s_k,a_d(t)))$ for BDDQN. 
The operation of the proposed BDQN/BDDQN-based approaches are summarized within Algorithm 1. DQN aims at compute weight sum $Q$ values for each aggregate actions tupple. This approach is eager to contribute unbalanced trade off between transportation reward and communication reward. However, The benefit for BDQN is finding optimized $Q_d$ in terms of $d \in \mathrm{A}_d$, which draws 2 optimal policies regarding to communication and transportation perspectives.

% , Sigmoid activation function ($f_r(x) = \frac{1}{1+e^{-x}}$) for hidden layers,  and linear activation function for the output layer. 

 % Figure environment removed

% Figure environment removed

 % Figure environment removed

\section{Numerical Results and Discussions}
% In this section, we show the performance of the proposed algorithms (BDQN and BDDQN ) and highlight the complex dynamics between the wireless connectivity, HO rates, Group UAVs traffic flow, and UAV's speed. 
In this section, we present the results of the suggested algorithms (BDQN and BDDQN) and emphasize the intricate relationships among wireless connectivity, handover rates, traffic flow of group UAVs, and the speed of UAVs.
% Unless stated otherwise, the parameter settings for expierment and other terrestrial and learning parameters are shown in Table \ref{tab:params}.
Unless explicitly mentioned, we employ the subsequent simulation parameters. BSs operating on 2.1 GHz and maximum support $Q_R =5$ UAV users. We define $\eta_{\mathrm{LoS}}$ and $\eta_{\mathrm{NLoS}}$ as 1 and 20, respectively. There are 5 aerial highway lanes, where  $N_U=5$ UAVs fly at speeds between $v_{\min}=5$ m/s and $v_{\max}=20$ m/s. BS's transmission power $P_T$ is 40 dBm. The BDQN training learning rate $\alpha$, the discount factor $\gamma$, and the batch size are $5\times{10}^{-4},0.8$ and $32$, respectively.



The BDQ agent is represented in Fig. \ref{fig:bdq_agent}. To improve the performance of training and reduce the training complexity, we deploy a fully-connected feed-forward neural network (FNN) $N(s)$ with weights $\{\theta\}$ to approximate the $Q$-value for a given action and state \cite{10001396}. FNN takes the state as an input and outputs shared-observation in Fig. \ref{fig:bdq_agent}.
Since Q-values are real, the FNN performs a multivariate linear regression task. We apply ReLU activation function, i.e., $f_r(x) = \mathrm{max}(0,x)$, as the first layer. There are $2$ FNN hidden layers and $256$ neurons on each layer. There are single layers with $128$ neurons on each Branching dueling network on the branching stage. Linear activation function is at the output layer. 




Fig. \ref{fig:episodes-rewards} states the training for UAV transportation rewards, communication rewards, and HO rate in terms of algorithms and target speed of UAVs, $v$. Fig. \ref{fig:episodes-rewards}(a) states transportation rewards reduce with higher UAV target speed. This is because higher speed contributes to higher collision occurrences. According to Fig. \ref{fig:episodes-rewards}(b), the communication rewards of BDQN and BDDQN are closer to each other for a given target speed while showing that a target speed of $10$ m/s is preferable over the $20$ m/s one. In any case, the gaps in communication rewards are small suggesting that the proposed algorithms tend to maximize the total communication reward regardless of the speed's impact. Finally, Fig. \ref{fig:episodes-rewards}(c) illustrates the HO rate convergence for BDDQN for different $v$. Clearly, after about 1000 episodes, BDDQN converges to HO values below 1\% since UAVs tend to avoid the HO penalty.

Fig. \ref{fig:velocities-rewards} depicts the average communication and transportation rewards and the HO rate for targeted speed $v=10$ m/s. It compares the performance of BDQN and BDDQN to those of the DDQN and the Shortest Distance Based-BS selection (SDB) benchmarks. From Fig. \ref{fig:velocities-rewards}, BDDQN and BDQN perform better than the benchmarks for all performance metrics. 
%Despite the total communication rewards is approximate the same for BDQN and BDDQN in Fig. \ref{fig:episodes-rewards}, the counterparts' average communication rewards differ since the BDDQN travelling timesteps are greater than BDQN. 
For instance, BDDQN outperforms SDN by $16.7\%$, $23.4\%$, and $10.9\%$, in terms of average transportation reward, average communication reward, and average HO rate, respectively.

%Both average communication and transportation rewards reduce with increasing targeted velocities. On average, transportation rewards, communication rewards, and HO rates improve by $16.7\%$ , $23.4\%$, $10.9\%$ compared between BDDQN and SDN benchmark.

Fig. \ref{fig:nrbss-rewards} illustrates the average communication reward, average transportation reward, and average HO rate for different numbers of BSs. The comparison is realized for BDQN, BDDQN, and SDN. As the number of available BSs along the 3D highway increases, the transportation reward decreases for BDQN and BDDQN, while it is slightly the same for SDB, as shown in Fig. \ref{fig:nrbss-rewards}(a). Indeed, the availability of more BSs encourages BDQN and BDDQN to increase further the communication reward and the expense of transportation reward, while SDN keeps the same strategy of connecting to the closest available BS all the time, as remarked in Fig. \ref{fig:nrbss-rewards}(b). 
%Less BSs distribution gain advantage on average transportation rewards since the preference SDN BS is fulfilled in priority. Each agent will focus more on the moving perspective to improve the transportation reward since there is less communication rewards difference for them to select BS to connect. 
For the proposed algorithms, an optimal number of BSs is identified. Specifically, with 15 BSs, the communication reward achieves its best values. However, increasing further the number of BSs is counter-productive as it causes more HO-related penalties.
%Increasing the number of BSs from 5 to 15 improve the communication rewards. However, there is a slight reduction for communication reward from futher increasing number of BSs from 15 to 20 leads to more HOs for UAV agents selecting and switching BS. 
Finally, 
Fig. \ref{fig:nrbss-rewards}(c) presents the HO rate for BDDQN when only 5 BSs or 15 BSs are available along the 3D highway. Surprisingly, when the number of BSs $N_R=5$ BSs, the HO rate is higher than that for $N_R=15$ BSs. Indeed, with a small BSs' number, the algorithm focuses on improving its transportation reward, e.g., reaching its target speed, thus causing the system to trigger HO more often.

%indicate the hanover rate reduced among the training. 
% In contrast to the above results



% We defined a score for each episode of the Q-learning training process. The score represents the reward of the ego vehicle travelled in each episode. There are three types rewards. a) Transportation rewards measured by the travelling distances. b) communication rewards measured by the mobility-aware SIR of the ego vehicle.
% c) Full rewards, which defined by the sum of Transportation rewards and communication rewards. 
% For the convergence purpose, we defined the following equation, $
%  \text{Full rewards} = 0.5 \cdot \text{Transportation  rewards} +0.5 \cdot \text{communication rewards} $



 


% % Figure environment removed

% % Figure environment removed

% % Figure environment removed






\section{Conclusion}
In this work, we proposed BDQN and BDDQN algorithms to jointly optimize the network selection and autonomous moving actions such as changing the speed of the UAVs and switching the lanes to maximize both HO-aware data rate and 3D aerial highway traffic flow.
In the future, we will focus more on UAVs' precise positioning in intelligent transportation systems.
\bibliographystyle{IEEEtran}
\bibliography{main.bib}
\end{document}
