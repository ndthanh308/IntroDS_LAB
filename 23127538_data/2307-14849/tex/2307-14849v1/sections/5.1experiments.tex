\section{Experimental Evaluation} \label{sec:results}
We next showcase the application of density-based counterfactuals in the context of brain networks, highlighting the high interpretability of such counterfactuals.

\subsection{Brain Networks}
Brain networks can be constructed using non-invasive techniques such as Functional Magnetic Resonance Imaging (fMRI) in resting-state patients.
By measuring blood flow, fMRI exploits the link between neural activity and blood flow and oxygenation, to associate a time series of activation scores at voxel level.
The voxels' signals are parcellated into Regions of Interest (ROIs) (\textit{nodes of the graph}) using specific templates, such as the Automated Anatomical Labeling (AAL)~\cite{tzourio2002automated} or the 200~\cite{craddock2012whole} parcellation scheme.
Then, interactions between ROIs (\textit{edges of the graph}) are identified by looking at the correlation between the corresponding time series.
Finally, relevant interactions are selected by applying a threshold (edge pruning), to obtain the brain's functional connectome.
ROIs can be further aggregated into areas associated with the lobes of the brain. As discussed before, this aggregation can be exploited to express interpretable density-based counterfactual explanations.

We consider seven publicly available brain network datasets.

\spara{AUT} is a dataset gathered within the Autism Brain Image Data Exchange (ABIDE) \cite{craddock2013neuro} project. This dataset includes brain network data from 49 patients with Autism Spectrum Disorder (ASD, condition group) and 52 Typically Developed (TD, control group) patients, all under the age of 9 years old.

\spara{BIP} dataset about lithium response in type I bipolar disorder patients~\cite{sani2018association}.

\spara{ADHD, ADHDM} come from the Multimodal Treatment of Attention Deficit Hyperactivity Disorder project\footnote{\url{http://fcon\_1000.projects.nitrc.org/indi/ACPI/html/acpi\_mta\_1.html}}, which investigated the impact of cannabis use on adults with or without a childhood diagnosis of ADHD. In the ADHD dataset, subjects are labeled as either ``ADHD'' or ``TD'', while in the ADHDM dataset, they are labeled as ``Marijuana use'' or ``Marijuana not used''.
  
\spara{OHSU, PEK, KKI}\footnote{\url{https://github.com/GRAND-Lab/graph_datasets}} are datasets constructed for three brain classification tasks: Attention Deficit Hyperactivity Disorder classification (OHSU), Hyperactive Impulsive classification (PEK), and gender classification (KKI)~\cite{pan2016task}.

\smallskip

Data is preprocessed following the literature for converting time series to correlation matrices \footnote{See \url{http://preprocessed-connectomes-project.org/abide/dparsf.html} for AUT and BIP, and \url{https://ccraddock.github.io/cluster_roi/atlases.html} for ADHD and ADHDM. For OHSU, PEK, and KKI, the data was already preprocessed.}.
To generate the graph dataset, correlation matrices are transformed into adjacency matrices by setting edges when the correlation between the two nodes is higher than a fixed threshold.
The threshold is selected based on the distribution of the correlation matrix values, using the 90th percentile for ADHD and AUT, and 80th for BIP. All the preprocessed graph datasets are available in our repository\footnote{\url{https://github.com/carlo-abrate/Counterfactual-Explanations-for-Graph-Classification-Through-the-Lenses-of-Density.git}}.

\Cref{tab-data} reports, for each dataset, the number of networks, the percentage of networks in class 1 (since we deal with binary classification, we report values for one class only), the total number of vertices and edges in the networks, and the accuracy and F1 score of the binary classifier trained on the dataset (see below).



\begin{table}[t]
    \centering
    \caption{Num. of graphs $|\mathcal{G}|$, percentage of graphs in class $1$, num. of nodes $|V|$, average num. of edges per graph $\mathrm{avg}|E|$, and accuracy $\textsf{ACC}$ and $\textsf{F1}$ score of the binary classifier, for each dataset.}
	\label{tab-data}
	\begin{tabular}{ccccccc}
	\toprule
	\textbf{Dataset} & \textbf{$|\mathcal{G}|$} & \textbf{$|Y_{=1}|$}  & \textbf{$|V|$} & \textbf{$\mathrm{avg}|E|$} & \textsf{ACC} & \textsf{F1} \\
	\midrule
	%AUT-CS & CS + KNN & 101 & $48 \%$ & 116 & 665 & 0.80 & 0.80 \\
	AUT & 101 & $48 \%$ & 116 & 665 & 0.92 & 0.90 \\
		%BIP & SF + KNN & 118 & $46 \%$ & 116 & 1334 & 0.64 & 0.52 \\
        BIP & 118 & $46 \%$ & 116 & 667 & 0.66 & 0.54 \\
	ADHD & 123 & $32 \%$ & 116 & 667 & 0.80 & 0.59 \\
	ADHDM & 123 & $50 \%$ & 116 & 667 & 0.93 & 0.93 \\
	OSHU & 79 & $56 \%$ & 190 & 199 & 0.68 & 0.72 \\
	PEK & 85 & $42 \%$ & 190 & 77 & 0.71 & 0.58 \\
	KKI & 83 & $55 \%$ & 190 & 48 & 0.66 & 0.68 \\
	\bottomrule
    \end{tabular}
\vspace{3mm}
\end{table}

\mpara{Classifier.}
The proposed framework is model-agnostic, making it suitable for explaining any kind of binary classifier.
In our experiments, we consider a binary classifier designed for graph classification that exploits the \emph{Spectral Features} (SF)~\cite{sfclassifier} of the graph to determine class memberships.
Let $A \in \{0,1\}^{|V|\times |V|}$ be the adjacency matrix of the graph, $D$ be the diagonal matrix of node degrees, and $L = I - D^{-1/2} A D^{-1/2}$ be the normalized Laplacian of $A$.
The SF of the graph is a vector consisting of the $k$ smallest positive eigenvalues of $L$, sorted in ascending order.
We trained a KNN classifier with various parameter settings and selected the optimal configuration based on the accuracy (\textsf{ACC}) and \textsf{F1} score using 5-fold cross-validation. The values of \textsf{ACC} and \textsf{F1} of the configurations selected are reported in the last two columns of \Cref{tab-data}.

\subsection{Metrics}\label{sec:metrics}
Various metrics have been proposed to evaluate the quality of counterfactual explanations \cite{guidotti2022counterfactual}. The selection of which measures to prioritize over others depends on factors such as the data type, the black-box models considered, and the vocabulary used to formulate the counterfactual statements.
We consider three measures specifically proposed to evaluate graph counterfactuals~\cite{prado2022survey}.

\spara{Flip rate:}
measures the percentage of graphs in the dataset for which the algorithm was able to find a counterfactual explanation~\cite{mothilal2020explaining,prado2022survey}.

\spara{Edit distance:} 
measures how \emph{different} is a graph $G$ from its counterfactual $G'$, and, in our case, is defined as the ratio between the symmetric difference of the edge sets of $G$ and $G'$ (\Cref{eq:xor}) and $|E \cup E'|$:
\[
d_{\%}(G,G') = \frac{\mathsf{d}(G,G')}{|E \cup E'|}\,.
\]
\spara{Calls:}
run-time complexity of a counterfactual search method measured in terms of the number of calls to the black-box model ($C$).

\subsection{Baselines}
We compare the performance of \tri, \cli, and \rcli against three baseline methods. The first baseline, \edg~\cite{countg}, utilizes an edge-based language to generate counterfactual explanations. The second baseline, \dataset, is an instance-level counterfactual search method proposed in \cite{guidotti2022counterfactual}. This method searches for the closest graph in the dataset that is classified by the black-box model in the opposite class and returns it as a counterfactual explanation for the input graph. 

Following~\cite{countg} we also equip the \rcli and \dataset methods with a \emph{backward search} phase which tries to refine the counterfactual found by 
modifying the edges in the symmetric difference between the edge set of the input graph and that of the counterfactual graph, with the aim of reducing the distance between the two graphs. The resulting methods are named \rclibw and \datasetbw respectively.

All the methods are implemented in Python and the code is made publicly available\footnote{\url{https://github.com/carlo-abrate/Counterfactual-Explanations-for-Graph-Classification-Through-the-Lenses-of-Density}} together with the datasets used in our analysis, and a supplemental material document containing further experimental results.