\section{Introduction}\label{sec:intro}
Many applications, such as time series forecasting, text classification, and speech recognition, have benefited greatly from the effectiveness of AI systems. However, as these systems become more complex, users may find it challenging to understand their output.
This opacity can lead to uninformed decisions, complicate the audit process, and impede trust in AI adoption.
One solution is to use only interpretable models (white-box), such as linear and logistic regression, decision trees, and Lasso and Ridge regressions \cite{rudin2019stop}. These models are easier for humans to comprehend as they are transparent and their inner workings can be interpreted.
Alternatively, post-hoc explanation methods can be employed to make black-box models more interpretable \cite{biran2017explanation,guidotti2018survey}.
Explanations of black-box models can help to build trust in AI systems by enabling users to understand the decision-making process and assess the reliability of the system.
This is particularly important in applications that impact people's lives, such as healthcare, finance, and criminal justice~\cite{bhatore2020machine,kononenko2001machine}.
%However, one drawback of these models is that they are often complex and difficult to interpret. This means that users, developers, and in general stakeholder may struggle to understand why a particular decision was made or suggested. Given that these AI tools provide significant support in highstake decision making processes supporting humans, providing understandable explanations for the outputs made by machine learning models has become a critical task for their application in sensitive field such as finance, credit scoring, and healthcare.

%check for a better definition
\textit{Counterfactual explanations}~\cite{moraffah2020causal} are a method of explaining predictions for individual examples. 
These explanations take the form of "If X had been different, Y would not have occurred", making them concise and easy to understand.
A counterfactual explanation consists in a \textit{counterfactual example}, which is a modified version of the instance that leads to a different outcome of the black-box model.
Counterfactual explanations have been increasingly used in recourse generation for loan applications~\cite{karimi2020survey}, among others. 
An example of counterfactual explanation for a person $P$ with a credit score of $400$ who applied for a loan and was rejected is as follows: 
\begin{displayquote}
\textit{If the credit score of $P$ had been 700, $P$ would have qualified for the loan.}
\end{displayquote}
% Therefore, the counterfactual explanation is made selecting a counterfactual instance and then the difference between the original and the counterfactual instance is highlighted in the counterfactual statement.

% advantages of counterfactual as explanations
Define and generate an optimal counterfactual is not a straightforward task, because counterfactual examples must be searched in a huge feature space, where some states may not be feasible. As an example, the following counterfactual example for a woman $P$ whose college application got rejected is not feasible:
\begin{displayquote}
\textit{If $P$ were a man, she would have been admitted to college.}
\end{displayquote}

Due to these challenges, there are multiple approaches to retrieve counterfactuals, such as with optimization strategies, heuristics search, and decision trees~\cite{guidotti2022counterfactual}. 
% ---------

Various metrics have been proposed to evaluate a counterfactual instance. 
\textbf{Fidelity} tells whether the prediction for the counterfactual instance is different from the prediction for the original example; 
\textbf{Similarity} measures how similar is the counterfactual instance to the original example;
\textbf{Sparsity} measures the proportion of features changed to generate the counterfactual instance; 
\textbf{Feasibility} indicates whether the counterfactual instance is feasible or not;
\textbf{Efficiency} determines how fast the search for the counterfactual instance is;
and \textbf{Diversity} measures the level of diversity of a set of counterfactual explanations generated for the same example. By providing a set of diverse explanations, the intricate details of the black-box model can be brought to light~\cite{mothilal2020explaining}.

In this paper, we emphasize the significance of \textbf{semantic} in generating diverse counterfactual explanations for \textit{graph data}, and show how it can improve the comprehensibility of the explanations while also distinguishing them from adversarial attacks~\cite{browne2020semantics}.
% The term semantic is used ambiguously in Machine Learning and Deep Learning \cite{bengio2009learning,lecun2015deep}.
% We refer to semantics as the human-understandable logic that is used to connect sign and signified \cite{browne2020semantics}. 
In the field of counterfactual-based explainable AI, semantic is defined as the unit of change between the original instance and its counterfactual counterpart.
An instance of how semantic is critical in counterfactual search can be seen in the domain of  Computer Vision, where semantics helps to discern a counterfactual explanation from an adversarial attack~\cite{akula2020cocox}. 
Let assume that the black-box model is an image classifier and that the input instance is a horse miss-classified as a zebra.
with a low-level semantic such as pixel color, the counterfactual explanation would be useless, unless as an adversarial attack. 
With a human-understandable concept as semantic, such as stripes, the counterfactual would be easily interpreted by a human.
% Check!!!!
%Furthermore, we show that the semantic of \textit{Counterfactual Statements} can be encoded in the heuristic search method and measured by the distance between the original and the counterfactual graphs.
\subsection{Semantic-based Counterfactuals for Brain Networks}
% Figure environment removed

We consider binary classifiers $f:\mathcal{G} \rightarrow \{0,1\}$ for graphs $G$. 
% Given a classifier $f:\mathcal{G} \rightarrow \{0,1\}$, a \textbf{counterfactual graph} $G'$ for a $G \in \mathcal{G}$ is a graph $G' \in \mathcal{G}$ that is classified in the opposite class, i.e., $f(G')=1-f(G)$, with the minimum change between the two graphs, and possibly satisfies some other criteria~\cite{Dandl_2020}.
Previous works~\cite{countg} generate counterfactual graphs by applying the finest possible change to the input graph, i.e., edge addition/deletion.
Then, they evaluate the goodness of the counterfactual graphs found in terms of the symmetric difference between the two edge sets. 
In some applications \cite{lanciano2023survey}, however, an edge-based semantic may lead to counterfactual graphs less relevant than those obtained by simultaneously changing multiple edges in specific parts of the graph.
We present a novel approach for constructing \textbf{density-based counterfactual graphs} that leverages a new semantic based on density. 
In detail, this approach identifies the most informative regions of the graphs and manipulates their density.
%We hypothesize that this method will enhance the accuracy and interpretability of the counterfactual graphs by focusing on the aspects of the data that are most likely to impact the outcomes of interest.
An instance of the practical utilization of density-based counterfactual graph search is evident in the classification of brain networks.
Brain networks can be modeled as undirected graphs, with nodes denoting Regions of Interest (ROIs), and edges indicating correlations of activation.
%Figure \ref{fig:first_fig}, for instance, shows the brain parcellation given by the AAL atlas~\cite{aal}: different node colors indicate different relevant areas of the brain.
%In this scenario, explaining the outcome with density-based counterfactual graph can be more descriptive than using changes at edge level, because it uses a semantic of explanation that is consistent with the language used to describe the organization of the brain.

% \begin{comment}
% % Figure environment removed

% % Figure environment removed
% \end{comment}
RoIs are usually grouped into distinct partitions (\emph{brain parcellation}), according to several properties such as structural and functional markers.
Figure~\ref{fig:first_fig} shows a brain taken from the Autism Brain Image Data Exchange (ABIDE) dataset~\cite{craddock2013neuro}. Nodes in different brain parcellations are denoted with different colors.
% The brain in figure \ref{fig:first_fig} is part of a brain dataset about patients with Autism Spectrum Disorder (ASD) and Typically Developed (TD) patients.
In this context, explaining the outcome of the classifier through the use of density-based counterfactual graph explanations can be more comprehensive than changes made at the edge level. This is due to the fact that the semantic employed in such explanations is more consistent with the terminology used to describe the organization of the brain.
As an example, given the brain in Figure~\ref{fig:first_fig}, our density-based counterfactual method finds a counterfactual graph by removing the dense region colored in red and adding the dense region colored in blue.
Such result can be formulated in the form of a counterfactual statement as follows:
\begin{displayquote}
\textit{Patient X is classified as Typically Developed. If X's brain had less co-activation between the \textit{Temporal Lobe} and the \textit{Parietal Lobe}, and more co-activation between the \textit{Frontal Lobe} and the \textit{Cerebellum}; then X would have been classified as Autism Spectrum Disorder.}
\end{displayquote}
The change in density in the counterfactual graph for $X$ is shown in the matrix in Figure~\ref{fig:first_fig} (right).
This matrix illustrates the number of edges added/removed within and between different regions in the brain parcellation.

In the next sections, we formalize the \textit{density-based counterfactual} problem, discuss the intuition behind such formalization, propose a clique-based framework for searching counterfactual graphs, and present some experimental results.
\mynote[from=Giulia]{Add description of the methods and main results.}