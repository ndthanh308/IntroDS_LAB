%%%%%%%%%%%
% Check the algorithms
% variation of the method in modularity: backward, triangles, ...
% add other benchmark method
%%%%%%%%
\section{Density-based Counterfactual Search} \label{sec:dcs}
We next introduce our general \emph{Density-based Counterfactual Search} framework (\dcs), which builds instance-level counterfactual explanations by iteratively searching for sparse regions to densify and for dense regions to sparsify.
Pseudocode of {\dcs} is provided in \Cref{alg:db_counter}.

\begin{algorithm}[thb]
\caption{\dcs}
\small
\label{alg:db_counter}
    \begin{algorithmic}[1]
    \Require Binary Graph Classifier $f$; Graph $G$
    \Ensure Counterfactual $G'$
    \State $G' \gets G$
    \While{$f(G) = f(G')$}
        \State \textsc{Densify} a sparse region in $G'$
        \State \textsc{Sparsify} a dense region in $G'$
    \EndWhile
    \State \Return $G'$
    \end{algorithmic}
\end{algorithm}

The algorithm iteratively changes the input graph $G$ until the modified graph $G'$ is classified in the opposite class of $G$.
At each iteration, it adds a dense structure to a sparse region in $G'$ and removes a dense structure from a dense region in $G'$.
Since two different regions of the graph undergo changes at each iteration, $G'$ gradually diverges from the original graph $G$ as the number of iterations increases.
In generating counterfactual graphs, a commonly desired objective is to produce graphs that closely resemble the input graphs. This is because such counterfactual graphs are more likely to provide a concise and interpretable explanation.
For this reason, the algorithm returns the counterfactual graph found in the smallest number of iterations.

\Cref{alg:db_counter} can accommodate any definition of a dense substructure.
In the rest of this section, we introduce two alternative approaches for defining the operations of densification and sparsification.
The first approach, \tri, is based on triadic closure; while the second approach, \cli, is based on maximal cliques.
%In the context of brain networks, \cite{countg} proposed a counterfactual generation method based on the most fundamental unit of change, i.e., the edge.
%In this work, we adapted this method to ensure comparability within the density-based search framework.
%For a comprehensive analysis, please refer to \Cref{sec:local} and \Cref{sec:global}.

% \subsection{Edge-based Search }
% As proposed in a previous work \cite{countg}, the edge-level semantic of change can be adapted as smallest unit of change for graph.
% We refer to \cite{countg} for more details and consideration.

\subsection{Triangle-based Counterfactual Search}\label{sec:tri}
The \emph{Triangle-based Counterfactual Search} ({\tri}) is illustrated in \Cref{alg:TSearch}.
In addition to the original graph $G$ and the classifier $f$, {\tri} takes as input a sorted list of candidate edges to remove $E_-$ (to destroy triangles) and a sorted list of candidate edges to add $E_+$ (to create triangles) in the counterfactual graph. These lists are prepared using \Cref{alg:TScore} (discussed below).
\tri iterates over the two lists until the graph becomes a counterfactual graph for $G$.
At each iteration $i$, it selects the next best edge to add ($e_+$) and to remove ($e_-$) from the current graph $G_i$.
If all the possible wedges have been closed, or if all the possible triangles have been opened (i.e. there are no more edges available in either $E_{-}$ or $E_{+}$), but $G_i$ is still classified in the same class as $G$, the algorithm returns $\emptyset$, indicating that a counterfactual could not be found.

\begin{algorithm}[thb]
    \small
    \caption{\tri}
    \label{alg:TSearch}
    \begin{algorithmic}[1]
    \Require Binary Graph Classifier $f$; Graph $G$
    \Require Sorted lists of candidate edges to remove $E_{-}$ and to add $E_{+}$
    \Ensure Counterfactual $G'$ if found; $\emptyset$ otherwise
    \State $G_0 \gets G$; $i \gets 1$
    \While{$f(G_{i-1}) = f(G)$ \textbf{and} $i \leq \min\left(|E_{-}|, |E_{+}|\right)$}
        \State $e_{-} \gets \Call{nextBest}{E_{-}}$; $e_{+} \gets \Call{nextBest}{E_{+}}$
        \State $E_i \gets E_{i-1} \setminus \{e_{-}\} \cup \{e_{+}\}$; $i \gets i + 1$
    \EndWhile
    \If{$f(G_i) \neq f(G)$} \Return $G_i$
    \Else{ \Return $\emptyset$ }
    \EndIf
    \end{algorithmic}
\end{algorithm}

Given a graph $G$, \Cref{alg:TScore} first computes a score for each feasible edge $(u,v) \in V\times V$, and then partitions the edges into two lists: a list of existing edges that could be removed ($E_{-}$) and a list of non-existing edges that could be added ($E_{+}$).
Finally, the algorithm sorts both lists based on the number of triangles in $G$ that contain the vertices of each edge.
In particular, $E_{-}$ is sorted in ascending order, while $E_{+}$ is sorted in descending order.
This sorting strategy ensures that {\tri} adds triangles in sparse regions of $G$, and removes triangles from the dense regions.

\begin{algorithm}[thb]
    \small
    \caption{\textsc{Triangle Score}}
    \label{alg:TScore}
    \begin{algorithmic}[1]
    \Require Graph $G$
    \Ensure Sorted lists of candidate edges to remove $E_{-}$ and to add $E_{+}$
    \State $\mathsf{T}(v) \gets$ number of triangles including $v$, $\forall v \in V$
    \State $E_{-} \gets E_{+} \gets \emptyset$
    \For{$(v,u) \in V\times V$}
    %\inote{Not clear how you use $s_v$ and $s_u$}
        \State $s_v \gets \mathrm{T}(v)$; $s_u \gets \mathrm{T}(u)$
	\If{$(v,u) \in E$}
		$E_{-} \gets E_{-} \cup \{(s_v+s_u,(v,u))\}$
        \Else
            { $E_{+} \gets E_{+} \cup \{(s_v+s_u,(v,u))\}$ }
%\inote{Not clear how do you ensure that these edges close triangles}
	\EndIf
    \EndFor
    \State $Sort_s (E_{-})$ in ascending order of score;
    \State $Sort_s (E_{+})$ in descending order of score;
    \State \Return $(E_{-}, E_{+})$
    \end{algorithmic}
\end{algorithm}

\subsection{Clique-based Counterfactual Search}\label{sec:cli}
The \emph{Clique-based Counterfactual Search} ({\cli}), illustrated in \Cref{alg:DS}, follows a structure similar to \Cref{alg:db_counter} but employs several heuristics to speed up the search for a counterfactual graph $G'$ for $G$.
The algorithm receives in input two additional parameters: the maximum number of iterations $\textsf{max}_I$, and the list of nodes in $G$ ranked according to a metric that gives more importance to nodes that belong to dense regions in $G$.
At each iteration $i$, the algorithm adds a clique to a sparse region of the current graph $G_i$ around the next worst node in the ranking $\bar{n}_h$ and removes a maximal clique from a dense region in $G_i$ around the next best node in the ranking $\bar{n}_l$.
The algorithm terminates when either $G_i$ is classified in the opposite class of $G$ or the maximum number of iterations $\textsf{max}_I$ is reached.
The densification of a sparse region is carried out by \Cref{alg:HS} (\textsc{DensifyCLI}) and the sparsification by \Cref{alg:LS} (\textsc{SparsifyCLI}).
In the following, a clique is represented by its set of vertices, as its set of edges is the set of all the possible edges between such nodes.

\begin{algorithm}[thb]
    \small
    \caption{\cli}
    \label{alg:DS}
    \begin{algorithmic}[1]
    \Require Binary Graph Classifier $f$; Graph $G$
    \Require Max Num Iteration $\textsf{max}_I$; Sorted list of vertices $\bar{V}$
    \Ensure Counterfactual $G'$ if found; $\emptyset$ otherwise
    \State $D[v] \gets 0$, $\forall v \in V$
    \State $G_0 \gets G$; $\mathcal{L} \gets \emptyset$; $i \gets 1$
    \While{$f(G_{i-1}) = f(G)$ \textbf{and} $i \leq \textsf{max}_I$}
        \State $\bar{n}_l,\bar{n}_h \gets \textsc{nextBest}(\bar{V}), \textsc{nextWorst}(\bar{V})$
	\State $G_i \gets \Call{SparsifyCLI}{G, G_{i-1}, \bar{n}_l, D, \mathcal{L}}$
	\State $d_h\gets 0$; $s \gets d_l \gets \textsf{d}(G_{i-1}, G_{i})$\label{line:xor}
	\While{$f(G_i) = f(G)$ \textbf{and} $d_l > d_h$}
            \State $s \gets s - d_h$
		\State $G^h \gets \Call{DensifyCLI}{G_{i},\bar{n}_h,D,s}$
		\State $d_h \gets \textsf{d}(G_{i}, G^h)$; $G_{i} \gets G^h$\label{line:xor2}
	\EndWhile
	\State $i \gets i + 1$		
    \EndWhile
    \If{$f(G_i) \neq f(G)$} \Return $G_i$
    \Else{ \Return $\emptyset$ }
    \EndIf
    \end{algorithmic}
\end{algorithm}

Procedure~\textsc{SparsifyCLI} identifies a maximal clique in the input graph $G$ surrounding a given node $n$, and removes all the edges in that clique in the candidate counterfactual graph $G'$.
The algorithm operates by identifying the largest clique in $G$ including $n$ that has the lowest overlap with the cliques removed in prior iterations.
By choosing the largest, lowest-overlap clique, the algorithm sparsifies one of the densest regions in $G'$.
Note that the cliques considered by the algorithm are found in the original graph $G$, and some of their edges may have already been removed from $G'$ in previous iterations of \textsc{SparsifyCLI}.
After the desired clique $C$ has been identified, the algorithm removes all its edges from $G'$ and stores $C$ in the set of cliques removed $\mathcal{L}$. Finally, the counts associated with each node in the clique are incremented by 1.

\begin{algorithm}[thb]
    \small
    \caption{\textsc{SparsifyCLI}}
    \label{alg:LS}
    \begin{algorithmic}[1]
    \Require Graph $G$; Candidate Counterfactual $G'$
    \Require Node $n$; Dictionary $D$; Set of Cliques $\mathcal{L}$
    \Ensure $G'$ with a clique removed
    \State $\mathcal{C} \gets$ cliques in $G$ including $n$
    \If{$\mathcal{L} = \emptyset$}
	\Return largest clique in  $\mathcal{C}$
    \EndIf
    \State $O \gets \emptyset$
    \For{$C \in \mathcal{C}$}
	\State $o \gets \max_{L \in \mathcal{L}}{(|C \cap L|)}$; $O \gets O \cup \{(o, C)\}$
    \EndFor
    \State $C \gets $ clique in $O$ with lowest value $o$
    \State $\mathcal{L} \gets \mathcal{L} \cup \{C\}$
    \State $D[v] \gets D[v] + 1$ for each $v$ in $C$\label{line:dict}
    \State \Return $G' \setminus \{$ edges in $C \,\}$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[thb]
    \caption{\textsc{DensifyCLI}}
    \label{alg:HS}
    \begin{algorithmic}[1]
    \Require Graph $G$; Node $n$; Dictionary $D$; Size $s$
    \Ensure $G$ with a clique added
    \State $\Gamma_G(n) \gets$ 2-hop neighborhood of $n$ in $G$ sorted according to $D$ (ascending)\label{line:2hop}
    \State $W \gets V \setminus \Gamma_G(n)$ sorted according to $D$ (ascending)\label{line:others}
    \State $C \gets \Call{concat}{\Gamma_G(n), W}$; $V_s \gets $ first $s$ nodes in $C$
    \State $D[v] \gets D[v] - 1$ for each $v \in V_s$
    \State \Return $G \cup \{$ edges between the nodes in $V_s \,\}$
    \end{algorithmic}
\end{algorithm}

Procedure~\textsc{DensifyCLI} is iteratively called until the dense region added to the candidate counterfactual graph has at least as many edges as the dense region removed by Algorithm~\ref{alg:LS}~\footnote{In our experiments we constrained the max deviation between the number of edges added and removed in terms of the max number of nodes $b$ that a clique added can have with respect to the number of nodes in the clique removed, and set $b = 10$.}.
This ensures that the size of the counterfactual is similar to that of the original graph.
At each iteration, the algorithm identifies a sparse region of size $s$ around the given node $n$ and adds all the possible edges between the $s$ nodes.
The size of the region $s$ is determined by subtracting the number of edges added in the previous iterations from the number of edges removed by Algorithm~\ref{alg:LS}.
To avoid densifying a region that has just been sparsified, the algorithm selects a sparse region involving nodes that are not present in many cliques added in previous iterations.
To achieve this, the algorithm uses a dictionary $D$, which keeps track of the number of times each node has been part of a clique added to the candidate counterfactual.
Given a node $n$, \textsc{DensifyCLI} sorts both the 2-hop neighborhood $\Gamma_G(n)$ of $n$ and the rest of the vertices $W$ according to their counts in $D$ (Algorithm~\ref{alg:HS} lines~\ref{line:2hop}-\ref{line:others}).
Then, it adds to $G$ the clique $C$ consisting of the first $s$ nodes in the concatenation between $\Gamma_G(n)$ and $W$, and updates $D$ by decreasing the counts associated with the nodes in $C$.



\spara{Further customizability of the framework.}
In \Cref{alg:DS}, we made specific design choices that, however, can be customized and adapted to meet the needs of the application at hand.

Firstly, \Cref{alg:DS} receives as input the list $\bar{V}$  of nodes in $G$ ranked according to a metric that prioritizes nodes in dense regions for \Cref{alg:LS} and nodes in sparse regions for \Cref{alg:HS}. In our implementation of \cli\ used in the experiments in Section \ref{sec:results} we sort the nodes in $\bar{V}$ based on the number of triangles in which each node participates.
However, alternative measures could be used to rank the nodes, such as the node clustering coefficient or other features at node level.
The key is to select a ranking that allows for traversing in one direction to be a good heuristic for sparsifying regions while traversing in the other direction is a good heuristic for densifying regions.

When domain-specific information is available, it is important to customize the algorithm to take such information into consideration. In the case of brain networks, for instance, 
nodes can be partitioned into well-defined and distinct regions (i.e. the brain lobes).
In \Cref{sec:results}, we explore a variation of \cli that selects the regions to sparsify/densify based on the brain lobes, which we refer to as \rcli.
The {\rcli} algorithm uses a two-level ranking strategy that first ranks the brain lobes according to the density of the subgraph induced by their nodes (regions having higher density ranked higher) and then, within each region, ranks the nodes according to the number of triangles in which they participate.
This two-level ranking allows \rcli to conduct the edge changes within a lower number of brain regions and thus generate more interpretable explanations.

%Secondly, in \Cref{alg:HS} we identify a sparse region around a node $n$ in the 2-hop neighborhood of $n$, with the aim of increasing the density of a region that is at least partially connected. However, in some scenarios, it may be preferable to connect nodes that are currently disconnected.

Secondly, for the sake of \emph{feasibility} of the counterfactual, i.e., keeping its density similar to the original graph, we alternate between 
\Cref{alg:LS} and \Cref{alg:HS}, meaning that we call them the same number of times.
However, while this strategy is effective in most cases, it may not be the optimal approach.
%This approach allows the algorithm to move dense structures from one region of the counterfactual to another.
%However, if the original graph is very sparse, a preferable strategy would be to call \Cref{alg:HS} more often, whereas if the input is dense, \Cref{alg:LS} may be called more frequently or using more edge in the change.
An alternative approach might let the density of the original graph govern the calls to \Cref{alg:LS} and \Cref{alg:HS}, so that when the graph is very sparse, \Cref{alg:HS} is called more often than \Cref{alg:LS}, and the other way around.