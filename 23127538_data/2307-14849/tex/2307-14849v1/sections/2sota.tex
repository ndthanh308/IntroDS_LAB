% !TEX root = ../main.tex
\section{Related Work}\label{sec:sota}
%Explainable Artificial Intelligence (XAI) is receiving growing attention in the last few years. Multiple methods have been proposed, and many taxonomies have been defined. Since interpretable models are explained-by-design, the focus of XAI is on black-box classifiers that are not transparent by design. Therefore, black-box models need post-hoc explanation methods for multiple reasons: fairness and ethics, scientific-advance, and regulatory and audit of models.
Post-hoc explanation methods have become essential for understanding the behavior of black-box machine learning models. One such method is counterfactual explanations~\cite{Wachter2017}, which produces example-based explanations by means  of a counterfactual instance for each instance being classified.
More specifically, counterfactual explanations need to exhibit two key characteristics: they must be \emph{similar} to the original instance while being classified in the opposite class of the original instance.
Numerous methods have been proposed to generate counterfactual explanations that possess these critical characteristics~\cite{guidotti2022counterfactual}.

\spara{Explanations for Graph Classifiers.}
There has been a growing interest in addressing the challenge of explaining graph classifiers, resulting in a surge of the number of proposed methods, providing either local or global explanations.
A recent survey \cite{DBLP:journals/corr/abs-2012-15445} categorizes the main (local or) instance-level techniques into four main classes.
\textit{Gradient/feature-based methods}, such as SA and Guided BP ~\cite{baldassarre2019explainability}, and CAM and Grad-CAM~\cite{pope2019explainability}, aim to evaluate the relevance of each feature in the classification task.
\textit{Perturbation-based methods}, such as GNNExplainer~\cite{DBLP:journals/corr/abs-1903-03894}, PGExplainer~\cite{luo2020parameterized}, ZORRO~\cite{funke2020hard}, GraphMask~\cite{schlichtkrull2020interpreting}, RC-Explainer~\cite{wang2020causal}, SubgraphX~\cite{yuan2021explainability}, measure the impact of the perturbation of the input features on the output of the classifier, to detect the most important features.
Among them, GraphShap~\cite{perotti2022graphshap} generates graph-level explanations by ranking a set of input motifs according to their Shapley values.

\textit{Decomposition methods} for graph neural networks, such as LRP~\cite{baldassarre2019explainability}, Excitation BP~\cite{pope2019explainability} and GNN-LRP~\cite{schnake2020higher}, generate feature importance scores by back-propagating decomposed prediction scores to the input layer of the network.
\textit{Surrogate methods}, such as GraphLime~\cite{huang2022graphlime}, RelEx~\cite{zhang2021relex}, and PGM-Explainer~\cite{vu2020pgm}, fit an interpretable model in the neighborhood of the input graph.

Only a few works provide (global or) model-level explanations.
Among them, XGNN~\cite{yuan2020xgnn} is based on graph generation.

\spara{Counterfactual Explanations for Graph Classifiers.}
DBS and OBS~\cite{countg} propose heuristics to locally perturb a generic input graph.
Specifically, they consider two types of modifications: edge addition and edge removal.
The counterfactual explanations are found using a bidirectional search approach that first identifies a feasible counterfactual graph, and then modifies the candidate graph to make it more similar to the input graph.
% The method applied to a set of graph can also provide insights on the model and class level logic of the black-box classifier.
On the other hand, targeted approaches have been proposed for molecular graphs~\cite{DBLP:journals/corr/abs-2102-03322,wellawatte2022model}, which are graphs where nodes represent atoms and edges are bonds.

CF-GNNExplainer~\cite{DBLP:journals/corr/abs-2102-03322} is a counterfactual version of GNNExplainer~\cite{DBLP:journals/corr/abs-1903-03894} that returns relevant subgraphs as explanations. This method removes edges using a matrix sparsification technique that  minimizes the number of edges changed.
MMACE~\cite{wellawatte2022model} generates counterfactuals for molecular graphs by exploring the chemical space vis the Superfast Traversal, Optimization, Novelty, Exploration and Discovery (STONED) method. In addition, the method uses DBSCAN to generate multiple counterfactuals.

%This paper proposes a novel method, called Density-based Counterfactual Search, for discovering relevant counterfactual graphs. This method is capable of generating counterfactual statements that use density as the basic building block of the vocabulary of the explanation. This creates a coherent language for many graph structures, including brain networks, and provides a powerful tool for generating meaningful and interpretable explanations.
% Moreover, as in \cite{countg}, we provide a human readable statement useful as simple and specific explanation of a single classification output.

% \mpara{Counterfactual Explanations Vs Adversarial Attack}
% Given a black-box model $f(\cdot) \in \mathcal{F}$ and a data point $x \in \mathcal{X}$, both Counterfactual Explanation (CE) methods and Adversarial Explanation (AE) methods aim to generate a modified version $x' \in \mathcal{X}$ of $x$ that is classified by $f$ in the opposite class, i.e., $f(x') = 1 - f(x)$.
% % \footnote{To simplify the discussion we consider a simple binary classification problem, the multiclass case can be extended without important considerations.}.
% \cite{browne2020semantics} highlights an apparent paradox: \textit{what is the difference between CEs and AEs}?
% % Therefore, there is no clear consensus on the difference and boundaries between CEs and AEs \cite{pawelczyk2022exploring,freiesleben2022intriguing,browne2020semantics}.
% The first difference is in their purpose. AEs are perturbations to the input that change the output of the model.
% In contrast, CEs are modified versions of the input that aim to explain and interpret the black-box model.
% The second distinguishing factor between AEs and CEs lies in the \textit{distance} between the original and perturbed instances. While AEs strive to go unnoticed by the classifier, CEs are modifications that aim to be sparse~\cite{freiesleben2022intriguing}.
% % In fact, the optimization function can be generally defined in the same way \cite{freiesleben2022intriguing}:
% % $$
% % \argmin_{x' \in \mathcal{X}} d_{\mathcal{X}}(x,x') + \lambda d_{\mathcal{F}}(f(x'),f(x))
% % $$
% % Where $x \in \mathcal{X}$ is the input data point, $f(\cdot)$ is the black-box function, $x' \in \mathcal{X}$ is the adversarial or counterfactual instance, the first term $d_{\mathcal{X}}(x,x')$ measures the difference between the original and perturbed instance, meanwhile, $d_{\mathcal{F}}(f(x'),f(x))$ quantifies the difference in the predicted output.
% % In \cite{Wachter2017}, the difference between CEs and AEs is in the definition of the distance $d_{\mathcal{X}}(\cdot,\cdot)$ between the input data point. Therefore, they define different measure of distance to both minimize the change of the feature values and the number of perturbed features. However, this point cannot be considered as sufficient for explanation, for instance in computer vision the change in a single pixel can be used to flip the classifier enough \cite{su2019one}.
% %grath2018interpretable
% %However, the latter simplification based on the distance is not totally accepted, "do not fundamentally change the nature of the method" and so do not justify the explanatory divide \cite{browne2020semantics}.
% %The common denominator is that the change in the input should be small.
% The third difference~\cite{Wachter2017,browne2020semantics} relates to the "possible world" hypothesis: AEs may be data points that lie outside the data distribution (infeasible "world"), while CEs aim to be feasible data points similar to the original instances.
% %to write better
% The last distinguishing factor is interpretability: CEs aim to be concise, minimal, and human-understandable, while AEs are unconstrained. 

This work proposes a more general framework for counterfactual graph generation that goes beyond existing approaches such as DBS, OBS~\cite{countg} and CF-GNNExplainer~\cite{DBLP:journals/corr/abs-2102-03322}.
While previous works primarily focused on modifying the structure of the original graph by adding or removing one edge at a time, our framework provides more fine-grained control over the graph modifications, as it operates on the dense and sparse regions of the graph.
This opens up possibilities for generating counterfactuals for various scenarios.