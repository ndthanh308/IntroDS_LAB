\section{Introduction}\label{sec:intro}
Graphs provide a flexible, expressive, and powerful data representation paradigm to model complex systems made of entities and relationships between them, such as users in social networks, regions in the brain, and proteins in an organism.
A widely studied task on graph-structured data is \emph{graph classification}, which involves assigning labels or categories to graphs based on their structural properties or node features.
Graph classification has benefited greatly from the many recent technical advances, especially thanks to \emph{graph neural networks} (GNN). However, as AI techniques become more complex, it becomes challenging to understand their output~\cite{DBLP:journals/corr/abs-2012-15445}.
This opacity can lead to uninformed decisions, complicate the audit process, and ultimately limit the trust in AI techniques, and thus their adoption.
In these regards, post-hoc explanation methods have emerged as an approach to make black-box models more interpretable \cite{biran2017explanation,guidotti2018survey}.
Explanations of black-box models can help to build trust in AI systems by enabling users to understand the decision-making process and assess the reliability of the system.
While this is particularly important in applications that impact people's lives, such as healthcare, finance, and justice~\cite{bhatore2020machine,kononenko2001machine}, explaining AI models is also
of uttermost importance in biological domains in which, more than the mere classification accuracy, it is important for the scientist to understand which modules play a role in a specific pathology or biological condition. For instance, in brain networks analysis, the neuroscientist needs to understand which are the
regions of the brain whose interactions discriminate between
disordered and healthy individuals~\cite{ha2015characteristics,weston2019four}.

\textit{Counterfactual explanations}~\cite{Wachter2017,moraffah2020causal} are a method
for providing post-hoc explanations of individual instance classification.
These explanations consist in a counterfactual example, which is a modified version of the instance that leads to a different classification.
They take the form of a counterfactual statement, such as \emph{``If X had been different, Y would not have occurred"}, which is typically concise and easy to understand~\cite{karimi2020survey}.
Defining and generating optimal counterfactuals is a challenging task, especially when working with graph data. This is due to the large space of interdependent features
and the complex interconnections between the nodes, which can make determining which features to modify difficult and time-consuming. Additionally, some states within the feature space may be too complex for humans to fully comprehend or may be difficult to explain using the same semantic associated with the type of data under consideration.

\spara{Graph classification for brain networks.}
In this work, without loss of generality, we adopt as the main application example, the binary classification of brain networks~\cite{wang2017brainclassification,du2018brainclassificationsurvey,meng2018brainclassification,yan2019groupinn,misman2019brainclassification,cslanciano}.
Brain networks can be modeled as undirected graphs, with nodes denoting \emph{regions of interest} (ROIs), and edges indicating correlations of activation.
In brain networks classification we are given two groups of individuals, e.g.,
a condition group  and
a control group, where each individual is represented by a graph $G_i=(V,E_i)$,
defined over the same set $V$ of nodes (corresponding to the ROIs).
The set of edges $E_i$ represents the connections, either structural or functional, between the ROIs of the observed $G_i$.
The goal is to learn a binary classifier $f: \mathcal{G} \rightarrow \{0, 1\}$
which, given an unseen brain network $G_n=(V,E_n)$,
predicts to which of the two groups it belongs.

Besides brain networks, this specific type of graph classification task, i.e., graph classification \emph{with node identity awareness} \cite{Barabsi2004,gutierrez2019embedding,Koutrouli_Karatzas_2020,you2021identity}, occurs whenever the identity of the node is an important information which identifies the same entity across all the input graphs. This is, for instance, the case
in \emph{``omics''} domains, such as in gene co-expression networks
\cite{bth379,giad010}, protein-protein interaction networks \cite{wsbm121,gulfidan2020pan}, or gene regulatory networks \cite{kim2018diffgrn,singh2018differential}.




\spara{Density-based graph counterfactuals.}
Intuitively, given a specific graph $G$ and a binary classifier $f$, a \emph{counterfactual graph}~\cite{countg} is a graph $G'$ such that $f(G') = 1 - f(G)$, while being as close as possible to $G$.
Previous work has focused on generating counterfactual explanations for graphs by changing the most elementary unit of a graph, i.e., an existing edge that might be removed, or a non-existing edge that might be added~\cite{countg}. However, as other researchers have observed \cite{perotti2022graphshap}, an explanation language based on the most fundamental unit of a graph structure, might be too fine-grained for producing interesting explanations. Aiming at a higher-order language for producing counterfactual graphs, we turn our attention to some of the main characterizing features of real-world complex networks. In fact, social and technological networks, as well as biological networks (such as brain networks, metabolic and regulatory networks), are all characterized by some common structural features, such as: \emph{network transitivity}, which is the property that two nodes that are both neighbors of the same third node have a high probability of also being connected (a.k.a. \emph{triadic closure}), the existence of \emph{repeated local motifs} and, more importantly, the organization into \emph{communities} or dense modules \cite{milo2002network,pnas.122653799,pnas.0601602103}.
Indeed, the extraction
of dense substructures in networks, such as \emph{maximal cliques}, \emph{quasi-cliques}, $k$-\emph{plex}, $k$-\emph{club}, etc., has received
substantial attention in the algorithmic literature (see~\cite{aggarwal,gionis2015dense,WU2015693,chang2018cohesive,farago2019survey,fang2022cohesive} for surveys). Finding groups of nodes that are densely connected inside and sparsely connected with the outside, is a key concept that has been approached under several different names, including \emph{graph clustering}, \emph{graph partitioning}, \emph{spectral clustering}, and \emph{community detection}~\cite{schaeffer2007graph,fortunato2010community,malliaros2013clustering,nascimento2011spectral,bulucc2016recent}.

Following this observation, in this work, we propose to produce counterfactual graphs based on the alteration of dense substructures. For our purposes, we define a general
\emph{density-based counterfactual search} framework to generate instance-level counterfactual explanations for graph classifiers, which can be instantiated with different notions of dense substructures. This framework identifies the most informative regions of the graphs and manipulates them by adding or removing dense structures until a counterfactual is found. We then instantiate the general framework to specific special cases. In Section \ref{sec:tri}, we present a method (\tri) that, inspired by network transitivity, searches for counterfactual graphs by opening or closing triangles.
Then in Section \ref{sec:cli}, we move to a counterfactual search driven by maximal cliques (\cli).

Our framework can be instantiated with any notion of a dense structure, or region of interest: for instance, in the context of brain networks, ROIs are usually grouped into distinct partitions (\emph{brain parcellation}), according to several properties such as structural and functional markers. Counterfactual graphs generated using the language of density w.r.t. these coarser-grain and well-established taxonomies, might produce explanations that are more consistent with the terminology used to describe the organization of the brain, and thus more comprehensible for the neuroscientists. In fact, deviations in the functional connections among the brain
regions from the normal pattern of connectivity are typically associated
with functional impairments: as a consequence, the notions of \emph{hyper-connectivity} or  \emph{hypo-connectivity} within and between specific regions, are heavily adopted by neuroscientists as fingerprints of specific disorders. Subgraphs that are dense in one class and sparse in the other, have also been proven effective in discriminating between a condition group and
a control group \cite{cslanciano}.

% Figure environment removed


Figure~\ref{fig:first_fig} showcases an example of our proposal over a brain network from the Autism Brain Image Data Exchange (ABIDE) dataset~\cite{craddock2013neuro} (more details in Section \ref{sec:results}). Nodes in different brain areas are denoted with different colors. In particular, the figure shows three different counterfactual graphs for the same brain network (patient 9): the leftmost one is generated using \cite{countg} (edge-based), the central one is produced using \tri, and the right-most one is created by \cli.
In each counterfactual graph, red edges identify the regions sparsified (\emph{removed edges}), while blue edges indicate the regions densified (\emph{added edges}).
The counterfactual statement corresponding to the counterfactual graph produced by \cli can be expressed in English as follows:
\begin{displayquote}
\textit{Patient X is classified as Autism Spectrum Disorder. If X's brain had less activation in the \textsc{\color{red} Frontal Lobe} and more co-activation between the \textsc{\color{blue} Posterior Fossa}, \textsc{\color{blue} Insula Cingulate Gyri}, and the \textsc{\color{blue} Temporal Lobe} then X would have been classified as Typically Developed.}
\end{displayquote}




\spara{Summary of contributions and roadmap.}
The contributions of this paper can be summarised as follows:
\begin{itemize}
    \item We propose to use the language of dense substructure to guide the search for counterfactual graphs and thus to produce more comprehensible post-hoc counterfactual explanations for graph classifiers.
    \item We define a general and flexible framework, dubbed {\dcs}, that can be instantiated to find counterfactual graphs leveraging different notions of a dense substructure of interest. Our framework is highly modular, providing users with a great deal of flexibility in defining the various parameters involved in the search process. Users can specify how to densify and sparsify (i.e., dense structures of interest), how to rank the nodes to identify the regions to modify, which black-box classifier to use, and whether the search should be refined via perturbation as post-processing.
    \item We showcase in detail two instantiations of {\dcs}: a triangle-based counterfactual search (\tri) and a clique-based counterfactual search (\cli). Both approaches can be further customized, and we present a variation, \rcli, which identifies relevant regions to modify by leveraging the brain's parcellation. This variation can further improve the search process and enhance the interpretability of the explanations.
    %enabling the users to tailor them to their specific needs.
    \item We evaluate {\dcs} in seven brain networks datasets and compare it with two baseline methods, demonstrating the efficiency of the proposed method and the high interpretability of the explanations it generates.
\end{itemize}

After an overview of the related work (\Cref{sec:sota}), we introduce the density-based counterfactual search problem (\Cref{sec:problem}).
\Cref{sec:dcs} presents our framework to generate counterfactual graphs adhering to the proposed density-oriented language, while \Cref{sec:tri} and \Cref{sec:cli} describe two implementations with customizable parameters.
Finally, \Cref{sec:results} shows our experimental evaluation of the framework and \Cref{sec:conclusions} discusses advantages and limitations.

%The problem is usually defined to maximize some metric related to accuracy of the model, sometimes generating opaque model that are not interpretable-by-design.
%In other words, the risk is generate a classifier that perform well in terms of accuracy but do not provide insights on the classification, so we refer to this kind of classifier as black-box model.

%We focus on a specific case of graph classification, where a set of graph $\mathcal{G}$ is given and each graph is defined on the same set of nodes $V$, that can be uniquely identified, named node identity awareness.
