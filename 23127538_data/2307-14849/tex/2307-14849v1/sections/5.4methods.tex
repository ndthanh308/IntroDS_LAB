
% Figure environment removed

\subsection{Quantitative Comparison}\label{sec:global}
We next present a comparison of the various counterfactual generation methods, using the metrics outlined in \Cref{sec:metrics}.


\Cref{fig:distance_methods} reports the distribution of $d_{\%}$ and $C$ values (the latter in logarithmic scale) at the class level for each method across three datasets (AUT, BIP, and ADHDM). Results for the remaining datasets can be found in the supplementary material shared in our repository.
Computation complexity, which is measured as the number of calls to the black-box classifier, varies across the different methods tested, with \dataset and \datasetbw being the most time-consuming due to the need to compare the input network with each graph classified in the opposite class. We note that \datasetbw requires slightly more calls to the oracle $C$ because it also performs a backward search.
\cli tends to find solutions more quickly than the other methods due to its tendency to make larger changes in the regions of the network, whereas \edg and \tri may require more iterations (and thus calls to the oracle) to achieve the same change.

We next examine the proximity of the counterfactual graphs to the corresponding input networks. As we observed in the previous section, the explanations generated by \dataset differ significantly from the input networks, as it searches for counterfactuals among the graphs in the dataset (which can vary considerably from each other) rather than perturbing the network itself.
The application of the backward search on top of \dataset results in counterfactuals that are much closer to the original networks compared to other methods. 
Interestingly, applying a backward search after \rcli does not significantly alter the resulting counterfactuals, suggesting that these solutions are more robust than those generated by \dataset.
Finally, since both \tri and \edg change a few edges at each iteration, the corresponding distributions of symmetric differences are comparable.

Methods such as \cli and \rcli operate on the maximal cliques in the network, which causes them to change a larger number of edges at each iteration, resulting in counterfactuals that are more distant than those obtained by \edg and \tri.
It is important to stress that \cli and \rcli, by design, are expected to induce larger changes when producing a counterfactual as they use a coarser-grain vocabulary in the explanation (dense regions), w.r.t. the fine-grain approaches of \edg and \tri. As motivated in \Cref{sec:intro}, we aim to have explanations at the level of regions (in which the changes are concentrated), because these are more interpretable for the domain expert than a simple list of flipped edges.

%Despite the increased distance, these methods produce more interpretable counterfactual statements due to the changes being concentrated in the same regions of the brain.




We finally report the flip rate per class (class 0/class 1) for each dataset (columns) and each method (rows) in \Cref{tab:fr}. 
We remind that \rcli was tested only in the datasets where the brains' parcellations were available (i.e., all but OHSU, PEK, and KKI).
By definition, the flip rate of \dataset is always 100\%, as it picks the closest counterfactual among the graphs in the database.
The other methods, instead, did not achieve a perfect score, as they were run for a fixed number of iterations.
In particular, \tri is run for at most $\min\left(|E_-|, |E_+|\right)$ iterations, \cli and \rcli for at most $\max_I =200$ iterations, and \edg for st most 2000 iterations. 
We observe that \tri converges to a counterfactual more frequently than the other methods, even in the unbalanced ADHD dataset. However, it struggles in the three sparsest networks (OHSU, PEK, KKI), likely because triadic closure is less observable in these graphs, while \edg adds and removes edges more indiscriminately, which allows it to eventually find a counterfactual even in these cases. 
Finally, \cli strikes a balance between \edg and \tri, as it acts on maximal cliques and can thus remove and add cliques even in sparser graphs (where cliques are just edges).




\begin{table}[t!]
    \centering
    \caption{Flip rate (class 0/ class 1), for each method and each dataset. Flip rate for \datasetbw and \rclibw are not reported as they are the same as \dataset and \rcli.}
    \vspace{-2mm}
    \label{tab:fr}
    \begin{tabular}{cccccccc}
    \toprule
    %\multicolumn{7}{c}{Data} \\
    %\cmidrule(lr){2-8}
    \textbf{Method} &
    \textbf{AUT} & 		
    \textbf{BIP} & 		
    \textbf{ADHD} & 		
    \textbf{ADHDM} & 		
    \textbf{OHSU} & 		
    \textbf{PEK} &
    \textbf{KKI} \\
    \midrule
    EDG & 100/85  & 70/100  & 75/100  & 74/100  & 100/87  & 100/91  & 90/100 \\
    TRI & 100/100  & 100/100  & 100/100  & 98/100  & 62/58  & 89/96  & 90/71 \\
    CLI & 91/100  & 100/100  & 100/100  & 56/100  & 53/87  & 100/91  & 90/100 \\
    RCLI & 96/93  & 94/100  & 95/98  & 61/100  & -  & - & -\\
    DATA & 100/100  & 100/100  & 100/100  & 100/100  & 100/100  & 100/100  & 100/100 \\
    %DATA+BW & 100/100  & 100/100  & 100/100  & 100/100  & 100/100  & 100/100  & 100/100 \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
\end{table}