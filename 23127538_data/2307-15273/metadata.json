{
  "title": "Recovering high-quality FODs from a reduced number of diffusion-weighted images using a model-driven deep learning architecture",
  "authors": [
    "J Bartlett",
    "C E Davey",
    "L A Johnston",
    "J Duan"
  ],
  "submission_date": "2023-07-28T02:47:34+00:00",
  "revised_dates": [],
  "abstract": "Fibre orientation distribution (FOD) reconstruction using deep learning has the potential to produce accurate FODs from a reduced number of diffusion-weighted images (DWIs), decreasing total imaging time. Diffusion acquisition invariant representations of the DWI signals are typically used as input to these methods to ensure that they can be applied flexibly to data with different b-vectors and b-values; however, this means the network cannot condition its output directly on the DWI signal. In this work, we propose a spherical deconvolution network, a model-driven deep learning FOD reconstruction architecture, that ensures intermediate and output FODs produced by the network are consistent with the input DWI signals. Furthermore, we implement a fixel classification penalty within our loss function, encouraging the network to produce FODs that can subsequently be segmented into the correct number of fixels and improve downstream fixel-based analysis. Our results show that the model-based deep learning architecture achieves competitive performance compared to a state-of-the-art FOD super-resolution network, FOD-Net. Moreover, we show that the fixel classification penalty can be tuned to offer improved performance with respect to metrics that rely on accurately segmented of FODs. Our code is publicly available at https://github.com/Jbartlett6/SDNet .",
  "categories": [
    "cs.CV",
    "cs.LG",
    "eess.IV"
  ],
  "primary_category": "cs.CV",
  "doi": "10.1002/mrm.30187",
  "journal_ref": "Magn Reson Med.2024;92:2193-2206",
  "arxiv_id": "2307.15273",
  "pdf_url": "https://arxiv.org/pdf/2307.15273v1",
  "comment": "10 pages, 7 figures, This work has been submitted to the IEEE for possible publication",
  "num_versions": null,
  "size_before_bytes": 4245809,
  "size_after_bytes": 773743
}