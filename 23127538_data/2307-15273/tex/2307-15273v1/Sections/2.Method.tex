\subsection{Network Architecture}
%Explaining the motivation for spherical deconvolution and how it is formulated. (explanation)
Constrained spherical deconvolution is used to fit FODs to DWI signal by optimising the following objective function:
\begin{equation}    
\label{eq:SDGen}
\mathop {\min\limits_{\bf{c}}} { {\frac{1}{2m}\| {{\cal A} {\cal Q}{\bf{c}} - {\bf{b}}} \|_2^2}  + {\cal R}\left( {\bf{c}} \right)}
\end{equation}

%SDNet architecture figure:
% Figure environment removed


%Specifying what each componenet of the above formula represents and what ehir sizes are.(explanation)
\noindent where ${\bf{c}} \in \mathbb{R}^{n}$ are the SH coefficients of the FOD, ${\bf{b}}\in\mathbb{R}^{m}$ are the DWI signals, and ${\cal AQ} \in\mathbb{R}^{m \times n}$ spherically convolves the FOD with the response functions of the tissue types being modelled. To facilitate a data-driven regularisation term, optimised for FOD reconstruction, we consider an arbitrary regularisation term, ${\cal R}(\cdot)$, in place of the ubiquitous non-negativity constraint. In the following we outline how the variable splitting methods used in \citet{jia2021learning, duan2019vs} can be adapted to solve \eqref{eq:SDGen}.

First, we introduce an auxiliary splitting variable ${\bf{w}} \in \mathbb{R}^{n}$, converting \eqref{eq:SDGen} into the following equivalent form:
\begin{equation}
\label{eq:SDdecouplecon}
\mathop {\min\limits_{{\bf{c,w}}}} { {\frac{1}{2m}\| {{\cal A} {\cal Q}{\bf{c}} - {\bf{b}}} \|_2^2}  + {\cal R}\left( {\bf{w}} \right)} \; s.t. \; {\bf{c=w}}
\end{equation}

Using the penalty function method, we add these constraints back into the model and minimise the joint objective:
\begin{equation}
\label{eq:SDdecouple}
 \mathop { \min\limits_{\bf{c,w}}} { {\frac{1}{2m}\| {{\cal A} {\cal Q}{\bf{c}} - {\bf{b}}} \|_2^2}  + {\cal R}\left( {\bf{w}} \right)} + \frac{\lambda}{2}\|{\bf{c-w}}\|^{2}_{2}
\end{equation}

Eq.~(\ref{eq:SDdecouple}) can  be solved for ${\bf{c}}$ and ${\bf{w}}$ using an alternating optimisation scheme:
\begin{equation}
\label{SDalt}
\left\{ \begin{array}{l}{{\bf{c}}^{k + 1}} = \mathop {\arg \min\limits_{\bf{c}}} \frac{1}{2m} \| {{\cal A} {\cal Q}{\bf{c}} - {\bf{b}}} \|_2^2 + \frac{\lambda }{2} \| {\bf{c-w}}^{k} \|_2^2\\{\bf{w}}^{k + 1} = \mathop {\arg \min\limits_{{\bf{w}}}}  {\frac{\lambda}{2}\| {\bf{c}}^{k+1}-{\bf{w}} \|_2^2}  + {\cal R}\left( {\bf{c}}^{k+1} \right) \\\end{array} \right..
\end{equation}

% Explanation as to how solve these two equations:
The first convex optimisation can be solved using matrix inversion. The second equation is a denoising problem with arbitrary regularisation, the optimal form of which is unknown. In order to learn the regularisation to improve FOD reconstruction performance, the iterative process can be unrolled and the denoising step solved using a neural network, ${\cal{NN}}(\cdot)$:
\begin{equation}
\label{eq:SDunroll}
\left\{ \begin{array}{l}{{\bf{c}}^{k + 1}} = \left(\frac{1}{m} {\cal{Q}^{T}}{\cal{A}^{T}}{\cal{A}}{\cal{Q}} + \lambda {\cal{I}}\right)^{-1}\left(\frac{1}{m}{\cal{Q}^{T}}{\cal{A}^{T}}{\bf{b}} + \lambda {\bf{w}}^{k}\right)\\
{\bf{w}}^{k + 1} = {\cal{NN}}\left({\bf{c}}^{k+1}\right)\end{array} \right..
\end{equation}


 %Network Architecture  (explanation/repeatability). 
The network architecture (Fig.~\ref{fig:SDNet}) takes nine voxels in each spatial dimension for 30 different diffusion gradients, resulting in a $9 \times 9 \times 9 \times 30$ volume of DWI signals as input, and passes them through alternating DWI consistency and deep regularisation blocks. The network outputs a vector ${\hat{\bf{c}}} \in \mathbb{R}^{n}$, a high-fidelity prediction of the FOD from the central voxel of the $9\times9\times9$ input patch. 

 \subsubsection{DWI Consistency}
%DWI consistency:
Each DWI consistency block solves the matrix inversion in (\ref{eq:SDunroll}) independently for each voxel, maintaining spatial resolution. The initial DWI consistency block optimises only for the first three even orders of spherical harmonic coefficients $(l_{max}=4)$ to ensure robustness to aggressive DWI undersampling. 

\subsubsection{Deep Regularisation}
%Deep Regularisation:
Each deep regularisation block is applied to a concatenation of the previous two DWI consistency blocks, meaning the block is conditioned on  both earlier representations. Validation tests showed these connections improve network performance (data not shown). The initial $3 \times 3 \times 3$ convolution kernels are applied with one layer of zero padding in each dimension, as to maintain spatial resolution, and are followed by 3D batch normalisation layers and parametric rectified linear unit (PReLU) activation functions. The number of channels is increased in this manner until it has reached 448 (Fig.~\ref{fig:SDNet}). No padding is applied in the final $3 \times 3 \times 3$ convolution kernel followed by a PReLU function, reducing the resolution in each spatial dimension by two. Finally, a $1 \times 1 \times 1$ convolution kernel is then applied to the 512-channel feature maps to obtain a 94-channel input to a gated linear unit (GLU) activation function,which is the output of the block. Residual connections, referencing the output of the previous DWI consistency block, are used to improve gradient flow through the network. The deep regularisation block reduces each spatial dimension of its input by two.   





\subsection{Loss Functions}
%Classification Loss function Motivation. 
In addition to the customary MSE loss, a fixel classification penalty is proposed to give greater control over the angular separation of the reconstructed FODs. The mechanics of this method can be considered similar to the microstructure sensitive loss proposed for DWI signal reconstruction in \citep{chen2023deep}.
To overcome the inherent non-differentiable nature of the fast marching level set FOD segmentation algorithm \citep{smith2013sift}, a fixel classification network is applied to predict the number of fixels each voxel contains. The output is passed into a cross-entropy component of the loss function. Since we are concerned with the white matter components of the FODs, the loss function and performance metrics are not functions of the grey matter and cerebrospinal fluid components of the FOD. For notational simplicity, from this point onwards ${\bf{c}}$ refers only to the white matter component of the FOD.  The overall loss function is as follows: 
\begin{equation}
{\cal{L}}(\hat{{\bf{c}}}) =\frac{1}{N_{batch}} \sum^{N_{batch}}_{i = 1} \left(\|\hat{{\bf{c}}}_{i} - {\bf{c}}_{i}\|^{2}_{2} + \kappa {\cal{E}}(\hat{{\bf{f}}}(\hat{{\bf{c}}}_{i}),{\bf{f}}_{i})\right)
\label{eq:Loss}
\end{equation}
where $N_{batch}$ is the number of data points in the mini-batch, $\hat{{\bf{c}}}_{i},\;{\bf{c}}_{i}\in\mathbb{R}^{n-2}$ are the reconstructed and fully sampled white matter FODs, ${\cal{E}}(\cdot,\cdot)$ is the cross-entropy, $\hat{{\bf{f}}}(\hat{{\bf{c}}}_{i}), \;{\bf{f}}_{i} \in \mathbb{R}^{5}$ are the predicted logits and the one-hot encoding of the number of fixels respectively and $\kappa$ is a hyperparameter to balance the two components of the loss function.

\begin{table}[h]
\caption{Count and percentage values of fixels in white matter voxels of an individual from the HCP dataset before and after thresholding at 4 fixels. Before thresholding there is a severe class imbalance.}
\label{tab:fixdist}
    \centering
    \begin{tabular}{?c?c?c?c?}
    \specialrule{1.25pt}{0pt}{0pt}
         \makecell{Number \\ of Fixels} & Count & \makecell{Percentage \\ before thresholding} & \makecell{Percentage \\ after thresholding} \\ \specialrule{1.25pt}{0pt}{0pt}
         1 & 310994 & 49\% & 49\% \\
         2 & 200673 & 32\% & 32\% \\
         3 & 76672 & 12\% & 12\% \\
         4 & 24095 & 4\% & 6.7\%\\
         5 & 10975 & 2\% & - \\
         6 & 4979 & 0.8\% & - \\
         7 & 1800 & 0.3\% & - \\
         \specialrule{1.25pt}{0pt}{0pt}
    \end{tabular}
\vspace{-0pt} % \vspace{-10pt} (JB) for arxiv
\end{table}


%One example of downstream analysis of FODs is fixel based analysis, which relies on segmenting the FOD into individual fibres. We propose by using the ground truth fixel maps as supervision signal, we can learn FOD reconstructions which predict the correct number of fibres. 

%Explaining why we thresholded fixel value
 When training the fixel classification network, the number of fixels in each voxel were thresholded to four (Tab.~\ref{tab:fixdist}), reducing the inclusion of spurious peaks and class imbalance. A simple, fully-connected architecture was used, with layers containing $\{$45, 1000, 800, 600, 400, 200, 100, 5$\}$ neurons. Between each layer there are ReLU activation and 1D batch normalisation functions, other than between the penultimate and final layer where the batch normalisation is omitted. A softmax activation function, followed by cross-entropy loss, were then applied to the output of the network. The classification network was trained using the same training set as SDNet. Fully sampled FODs were used as the input, and the ground truth targets were calculated using the fast level set marching algorithm \citep{smith2013sift}. 
 
 % Figure environment removed

\subsection{Implementation Details}
%Training the SDNEt 
To demonstrate the impact of the fixel classification penalty, experiments were carried out with $\kappa = 0$ and $ \kappa = 1.6 \times 10^{-4}$. The ADAM optimiser \citep{kingma2014adam}, with learning rate warm-up, was used for parameter optimisation, with an initial learning rate of $10^{-6}$, increasing to $10^{-4}$ after $10,000$ iterations. To minimise hyperparameter tuning, $\lambda$ was optimised simultaneously with the network weights. From validation experiments (data not included), we found that the most effective way to utilise the classification loss to train SDNet was to initially train the model with $\kappa=0$ and then to increase $\kappa$ to its final value after this initial training stage. To do so we trained SDNet with only MSE loss until convergence, then trained the network until convergence with $ \kappa = 1.6 \times 10^{-4}$. 

