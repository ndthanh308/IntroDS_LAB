\documentclass[journal,twoside,web]{IEEEtran}
\makeatletter
\let\NAT@parse\undefined
\makeatother
% \documentclass[journal,twoside,web]{ieeecolor}
\usepackage{tmi}
\usepackage[numbers]{natbib}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{layouts}
\usepackage{algorithmic}
\usepackage{graphicx, caption}
\usepackage{bm}
\usepackage{textcomp}
\usepackage{diagbox}
\usepackage{makecell}
\usepackage{hyperref}
% \usepackage[english]{babel}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{booktabs}
\usepackage{array}
\newcolumntype{?}{!{\vrule width 1.25pt}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
% (JB changed for arxiv) 
% \markboth{\journalname, VOL. XX, NO. XX, XXXX 2020}
% (JB changed for arxiv)
% {Author \MakeLowercase{\textit{et al.}}: Preparation of Papers for IEEE TRANSACTIONS ON MEDICAL IMAGING}

\begin{document}
\title{Recovering high-quality FODs from a reduced number of diffusion-weighted images using a model-driven deep learning architecture}
\author{Joseph Bartlett, Catherine E. Davey, Leigh A. Johnston, \IEEEmembership{Senior Member, IEEE}, and Jinming Duan
% \thanks{This paragraph of the first footnote will contain the date on which
% you submitted your paper for review. It will also contain support information,
% including sponsor and financial support acknowledgment. For example, 
% ``This work was supported in part by the U.S. Department of Commerce under Grant BS123456.'' }
\thanks{J. Bartlett, and J. Duan are with the School of Computer Science, the University of Birmingham, Birmingham, UK.}
\thanks{J. Duan is with the Alan Turing Institute, London, UK.}
\thanks{J. Bartlett, C. E. Davey and L. A. Johnston are with the Department of Biomedical Engineering, the Melbourne Brain Centre Imaging Unit and the Graeme Clark Institute, the University of Melbourne, Melbourne, Australia.}
\thanks{The corresponding author is J. Duan (j.duan@bham.ac.uk).}}
\maketitle

\begin{abstract}
Fibre orientation distribution (FOD) reconstruction using deep learning has the potential to produce accurate FODs from a reduced number of diffusion-weighted images (DWIs), decreasing total imaging time. Diffusion acquisition invariant representations of the DWI signals are typically used as input to these methods to ensure that they can be applied flexibly to data with different b-vectors and b-values; however, this means the network cannot condition its output directly on the DWI signal. In this work, we propose a spherical deconvolution network, a model-driven deep learning FOD reconstruction architecture, that ensures intermediate and output FODs produced by the network are consistent with the input DWI signals. Furthermore, we implement a fixel classification penalty within our loss function, encouraging the network to produce FODs that can subsequently be segmented into the correct number of fixels and improve downstream fixel-based analysis. Our results show that the model-based deep learning architecture achieves competitive performance compared to a state-of-the-art FOD super-resolution network, FOD-Net. Moreover, we show that the fixel classification penalty can be tuned to offer improved performance with respect to metrics that rely on accurately segmented of FODs. Our code is publicly available at \href{https://github.com/Jbartlett6/SDNet}{https://github.com/Jbartlett6/SDNet}.
\end{abstract}

\begin{IEEEkeywords}
Diffusion MRI, model-based deep learning, FOD reconstruction
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}
\input{Sections/1.Introduction.tex}

\section{Method}
\label{sec:method}
\input{Sections/2.Method.tex}

\section{Experiments}
\label{sec:experiments}
\input{Sections/3.Experiments.tex}

\section{Results}
\label{sec:results}
\input{Sections/4.Results.tex}

\section{Discussion}
\label{sec:discussion}
\input{Sections/5.Discussion.tex}

\section{Conclusion}
\label{sec:conclusion}
\input{Sections/6.Conclusion}

\bibliographystyle{IEEEtranN}
% \bibliographystyle{IEEEtranN}
\small\bibliography{bibliography}


% \newpage
% \input{Sections/7.Supplementary}


\end{document}
