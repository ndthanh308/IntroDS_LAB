{
  "title": "Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization",
  "authors": [
    "Hongchang Gao"
  ],
  "submission_date": "2023-07-25T11:51:20+00:00",
  "revised_dates": [
    "2023-08-02T00:17:17+00:00"
  ],
  "abstract": "The stochastic compositional minimax problem has attracted a surge of attention in recent years since it covers many emerging machine learning models. Meanwhile, due to the emergence of distributed data, optimizing this kind of problem under the decentralized setting becomes badly needed. However, the compositional structure in the loss function brings unique challenges to designing efficient decentralized optimization algorithms. In particular, our study shows that the standard gossip communication strategy cannot achieve linear speedup for decentralized compositional minimax problems due to the large consensus error about the inner-level function. To address this issue, we developed a novel decentralized stochastic compositional gradient descent ascent with momentum algorithm to reduce the consensus error in the inner-level function. As such, our theoretical results demonstrate that it is able to achieve linear speedup with respect to the number of workers. We believe this novel algorithmic design could benefit the development of decentralized compositional optimization. Finally, we applied our methods to the imbalanced classification problem. The extensive experimental results provide evidence for the effectiveness of our algorithm.",
  "categories": [
    "cs.LG",
    "math.OC"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.13430",
  "pdf_url": null,
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 302622,
  "size_after_bytes": 128845
}