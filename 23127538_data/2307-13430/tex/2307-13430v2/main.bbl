\begin{thebibliography}{10}

\bibitem{chen2020solving}
T.~Chen, Y.~Sun, and W.~Yin.
\newblock Solving stochastic compositional optimization is nearly as easy as
  solving stochastic optimization.
\newblock {\em arXiv preprint arXiv:2008.10847}, 2020.

\bibitem{coates2011analysis}
A.~Coates, A.~Ng, and H.~Lee.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock In {\em Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pages 215--223. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem{fang2018spider}
C.~Fang, C.~J. Li, Z.~Lin, and T.~Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  689--699, 2018.

\bibitem{finn2017model}
C.~Finn, P.~Abbeel, and S.~Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1126--1135. PMLR, 2017.

\bibitem{gao2021fast}
H.~Gao and H.~Huang.
\newblock Fast training method for stochastic compositional optimization
  problems.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{gao2022convergence}
H.~Gao, J.~Li, and H.~Huang.
\newblock On the convergence of local stochastic compositional gradient descent
  with momentum.
\newblock In {\em International Conference on Machine Learning}, pages
  7017--7035. PMLR, 2022.

\bibitem{gao2021convergence}
H.~Gao, X.~Wang, L.~Luo, and X.~Shi.
\newblock On the convergence of stochastic compositional gradient descent
  ascent method.
\newblock In {\em Thirtieth International Joint Conference on Artificial
  Intelligence (IJCAI)}, 2021.

\bibitem{ghadimi2020single}
S.~Ghadimi, A.~Ruszczynski, and M.~Wang.
\newblock A single timescale stochastic approximation method for nested
  stochastic optimization.
\newblock {\em SIAM Journal on Optimization}, 30(1):960--979, 2020.

\bibitem{koloskova2021improved}
A.~Koloskova, T.~Lin, and S.~U. Stich.
\newblock An improved analysis of gradient tracking for decentralized machine
  learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{koloskova2019decentralized}
A.~Koloskova, T.~Lin, S.~U. Stich, and M.~Jaggi.
\newblock Decentralized deep learning with arbitrary communication compression.
\newblock {\em arXiv preprint arXiv:1907.09356}, 2019.

\bibitem{lian2017can}
X.~Lian, C.~Zhang, H.~Zhang, C.-J. Hsieh, W.~Zhang, and J.~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock {\em arXiv preprint arXiv:1705.09056}, 2017.

\bibitem{lin2021quasi}
T.~Lin, S.~P. Karimireddy, S.~U. Stich, and M.~Jaggi.
\newblock Quasi-global momentum: Accelerating decentralized deep learning on
  heterogeneous data.
\newblock {\em arXiv preprint arXiv:2102.04761}, 2021.

\bibitem{lu2019gnsd}
S.~Lu, X.~Zhang, H.~Sun, and M.~Hong.
\newblock Gnsd: a gradient-tracking based nonconvex stochastic algorithm for
  decentralized optimization.
\newblock In {\em 2019 IEEE Data Science Workshop, DSW 2019}, pages 315--321.
  Institute of Electrical and Electronics Engineers Inc., 2019.

\bibitem{pu2020distributed}
S.~Pu and A.~Nedi{\'c}.
\newblock Distributed stochastic gradient tracking methods.
\newblock {\em Mathematical Programming}, pages 1--49, 2020.

\bibitem{qi2021stochastic}
Q.~Qi, Y.~Luo, Z.~Xu, S.~Ji, and T.~Yang.
\newblock Stochastic optimization of areas under precision-recall curves with
  provable convergence.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{sun2020improving}
H.~Sun, S.~Lu, and M.~Hong.
\newblock Improving the sample and communication complexity for decentralized
  non-convex optimization: Joint gradient estimation and tracking.
\newblock In {\em International Conference on Machine Learning}, pages
  9217--9228. PMLR, 2020.

\bibitem{tang2019deepsqueeze}
H.~Tang, X.~Lian, S.~Qiu, L.~Yuan, C.~Zhang, T.~Zhang, and J.~Liu.
\newblock Deepsqueeze: Decentralization meets error-compensated compression.
\newblock {\em arXiv preprint arXiv:1907.07346}, 2019.

\bibitem{tarzanagh2022fednest}
D.~A. Tarzanagh, M.~Li, C.~Thrampoulidis, and S.~Oymak.
\newblock Fednest: Federated bilevel, minimax, and compositional optimization.
\newblock {\em arXiv preprint arXiv:2205.02215}, 2022.

\bibitem{tsaknakis2020decentralized}
I.~Tsaknakis, M.~Hong, and S.~Liu.
\newblock Decentralized min-max optimization: Formulations, algorithms and
  applications in network poisoning attack.
\newblock In {\em ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 5755--5759. IEEE, 2020.

\bibitem{wang2021memory}
B.~Wang, Z.~Yuan, Y.~Ying, and T.~Yang.
\newblock Memory-based optimization methods for model-agnostic meta-learning.
\newblock {\em arXiv preprint arXiv:2106.04911}, 2021.

\bibitem{wang2017stochastic}
M.~Wang, E.~X. Fang, and H.~Liu.
\newblock Stochastic compositional gradient descent: algorithms for minimizing
  compositions of expected-value functions.
\newblock {\em Mathematical Programming}, 161(1-2):419--449, 2017.

\bibitem{xian2021faster}
W.~Xian, F.~Huang, Y.~Zhang, and H.~Huang.
\newblock A faster decentralized algorithm for nonconvex minimax problems.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{xiao2017/online}
H.~Xiao, K.~Rasul, and R.~Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem{yang2022decentralized}
S.~Yang, X.~Zhang, and M.~Wang.
\newblock Decentralized gossip-based stochastic bilevel optimization over
  communication networks.
\newblock {\em arXiv preprint arXiv:2206.10870}, 2022.

\bibitem{ying2016stochastic}
Y.~Ying, L.~Wen, and S.~Lyu.
\newblock Stochastic online auc maximization.
\newblock {\em Advances in neural information processing systems}, 29:451--459,
  2016.

\bibitem{yuan2020stochastic}
H.~Yuan and W.~Hu.
\newblock Stochastic recursive momentum method for non-convex compositional
  optimization.
\newblock {\em arXiv preprint arXiv:2006.01688}, 2020.

\bibitem{yuan2019stochastic}
H.~Yuan, X.~Lian, and J.~Liu.
\newblock Stochastic recursive variance reduction for efficient smooth
  non-convex compositional optimization.
\newblock {\em arXiv preprint arXiv:1912.13515}, 2019.

\bibitem{yuan2021decentlam}
K.~Yuan, Y.~Chen, X.~Huang, Y.~Zhang, P.~Pan, Y.~Xu, and W.~Yin.
\newblock Decentlam: Decentralized momentum sgd for large-batch deep training.
\newblock {\em arXiv preprint arXiv:2104.11981}, 2021.

\bibitem{yuan2021compositional}
Z.~Yuan, Z.~Guo, N.~Chawla, and T.~Yang.
\newblock Compositional training for end-to-end deep auc maximization.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{zhang2019composite}
J.~Zhang and L.~Xiao.
\newblock A composite randomized incremental gradient method.
\newblock In {\em International Conference on Machine Learning}, pages
  7454--7462, 2019.

\bibitem{zhang2019stochastic}
J.~Zhang and L.~Xiao.
\newblock A stochastic composite gradient method with incremental variance
  reduction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9078--9088, 2019.

\bibitem{zhang2021gt}
X.~Zhang, J.~Liu, Z.~Zhu, and E.~S. Bentley.
\newblock Gt-storm: Taming sample, communication, and memory complexities in
  decentralized non-convex learning.
\newblock {\em arXiv preprint arXiv:2105.01231}, 2021.

\bibitem{zhang2023federated}
X.~Zhang, Y.~Zhang, T.~Yang, R.~Souvenir, and H.~Gao.
\newblock Federated compositional deep auc maximization.
\newblock {\em arXiv preprint arXiv:2304.10101}, 2023.

\bibitem{zhao2022distributed}
S.~Zhao and Y.~Liu.
\newblock Distributed stochastic compositional optimization problems over
  directed networks.
\newblock {\em arXiv preprint arXiv:2203.11074}, 2022.

\end{thebibliography}
