\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}

% Include other packages here, before hyperref.
\usepackage{array}
\usepackage{multirow}
\usepackage{setspace}
\usepackage[table,xcdraw]{xcolor}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{authblk}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand\extrafootertext[1]{%
    \bgroup
    \renewcommand\thefootnote{\fnsymbol{footnote}}%
    \renewcommand\thempfootnote{\fnsymbol{mpfootnote}}%
    \footnotetext[0]{#1}%
    \egroup
}
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{min}}\;}
\newcommand\CoAuthorMark{\footnotemark[\arabic{footnote}]}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{12618} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{SelFormaly: Towards Task-Agnostic Unified Anomaly Detection}

\author{Yujin Lee\thanks{These authors contributed equally to this work.}\qquad Harin Lim\protect\CoAuthorMark\qquad Hyunsoo Yoon\\
Department of Industrial Engineering, Yonsei University\\
Seoul, South Korea\\
{\tt\small \{yjlee9040,harini1029,hs.yoon\}@yonsei.ac.kr}}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both


\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
    The core idea of visual anomaly detection is to learn the normality from normal images, but previous works have been developed specifically for certain tasks, leading to fragmentation among various tasks: defect detection, semantic anomaly detection, multi-class anomaly detection, and anomaly clustering. This one-task-one-model approach is resource-intensive and incurs high maintenance costs as the number of tasks increases. This paper presents SelFormaly, a universal and powerful anomaly detection framework. We emphasize the necessity of our off-the-shelf approach by pointing out a suboptimal issue with fluctuating performance in previous online encoder-based methods. In addition, we question the effectiveness of using ConvNets as previously employed in the literature and confirm that self-supervised ViTs are suitable for unified anomaly detection. We introduce back-patch masking and discover the new role of top $k$-ratio feature matching to achieve unified and powerful anomaly detection. Back-patch masking eliminates irrelevant regions that possibly hinder target-centric detection with representations of the scene layout. The top $k$-ratio feature matching unifies various anomaly levels and tasks. Finally, SelFormaly achieves state-of-the-art results across various datasets for all the aforementioned tasks.

    % To prove the necessity of our off-the-shelf approach, we first show the performance fluctuation phenomenon emerging on the methods relying on a proxy task. Further, we confirm that self-supervised ViTs are robust foundation encoders with consideration across self-supervised and supervised with two popular architectures, ConvNets and ViTs.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

% Figure environment removed

   Anomaly detection, a long-standing field in computer vision, aims to identify anomalous data that deviate from normality. With the exponential growth of image data, there is an increasing demand for visual anomaly detection systems that apply to various industries. Consequently, various tasks have emerged within the field of anomaly detection. These tasks primarily encompass defect detection and semantic anomaly detection, which share a common idea but differ in the level of anomalies. In defect detection, as Fig.~\ref{fig:defect_dataset_sample}, methods seek to identify anomalies that are locally deviant from the normal appearance while being semantically identical. On the other hand, semantic anomaly detection targets anomalies that are semantically different and do not belong to any known normal class. Moreover, recent advancements have introduced multi-class anomaly detection, where a single framework processes multiple classes or datasets for anomaly detection~\cite{you2022a}. In addition to the primary task of differentiating anomalies from normal data, there has been a growing effort within the field to comprehensively understand and classify various types or categories of anomalies~\cite{sohn2023anomaly}.
   
% Figure environment removed

    Meanwhile, in the recent AI community, a unified system has been extensively developed in various domains ~\cite{raffel2020exploring, brown2020language, gupta2022towards, wang2022ofa, lu2022unified} due to its ability to avoid task-specific design and training, reducing memory consumption and maintenance costs. However, previous anomaly detection approaches remain fragmented across different tasks, with limited exploration of unification. This fragmentation is primarily driven by the prevailing emphasis on achieving high accuracy within specific tasks, which is considered more significant than in other domains. Nevertheless, since anomaly detection tasks share a common goal of learning normality, adhering to the conventional one-task-one-model scheme, which requires task-specific training, could be more efficient and resource-intensive. Therefore, the anomaly detection domain requires a unified and highly effective system. In this work, we propose \textit{SelFormaly}, a powerful and universal anomaly detection system.
    
    Recent studies have demonstrated the effectiveness of pre-trained encoder-based methods in anomaly detection, while reconstruction or self-supervised learning approaches have been prevalent in the literature. However, these approaches require proxy tasks that may not align with the target task (i.e., anomaly detection) and require prior knowledge of anomalies in the datasets. In contrast, pre-trained encoders lack proxy tasks, making them a promising foundation for unified anomaly detection. In Section 3.1, we empirically demonstrate the superiority of pre-trained encoder methods as the foundation of a unified system compared to other approaches. Furthermore, in Section 3.2, through comparative studies across different encoder architectures and levels of supervision, we discover that self-supervised representations from Vision Transformers (ViTs) hold great potential for achieving unified anomaly detection.
    
    We leverage the assistance of Back-Patch Masking (BPM) and top k-ratio feature matching to construct an effective and unified anomaly detection system. BPM employs fixed self-attention to guide the system in non-target rejection. Since the self-attention is derived from a fixed pre-trained encoder, our BPM operates in a task- and dataset-agnostic manner. Additionally, BPM enables our system to handle false-positive and false-negative cases, enhancing overall effectiveness. Top k-ratio feature matching, another crucial component, contributes to its generality and effectiveness. The level of representation in pre-trained encoder approaches varies across tasks. For example, defect detection utilizes mid-level representation for effective defect detection~\cite{roth2022towards}, while semantic anomaly detection relies on global representation~\cite{sun2022out}. To achieve unification, we adopt mid-level features and employ them in a multiple-instance learning fashion. Specifically, we extract the top-$k$\% anomalous patch features and utilize them across various anomaly detection tasks. Furthermore, we demonstrate that this top $k$-ratio feature matching enables anomaly clustering, marking a significant step toward developing a general-purpose anomaly detection system.

    Through extensive experiments, we empirically demonstrate that \textit{SelFormaly} achieves high accuracy across various tasks. We evaluate its performance in (1) defect detection on MVTecAD, BTAD, MTD, and CPD, (2) semantic anomaly detection on ImageNet-30 and Species-60, (3) multi-class anomaly detection, (4) anomaly clustering, and (5) low-shot anomaly detection. Remarkably, we show state-of-the-art results (e.g., AUROC of 99.64 and 97.1 on MVTecAD and ImageNet-30, respectively) on a wide range of datasets with varying levels of anomalies. In addition, we conduct ablation studies on BPM and top $k$-ratio feature matching, confirming their contributions to both generality and effectiveness of the anomaly detection system.

         
% \begin{enumerate}
%     \item We first propose a unified anomaly detection framework that is highly effective in various tasks and datasets, reducing the inefficiency of the previous one-task-one-model scheme.
%     \item We introduce major components to boost our method. 1) Patch-level nearest neighbor retrieval with the superiority of self-supervised pretrained ViTs 2) Back-patch masking with a smoothed attention map 3) Top $k$-ratio anomaly scoring strategy for handling multi-task.
% \end{enumerate}
%------------------------------------------------------------------------
\section{Related Works}

%-------------------------------------------------------------------------
Over the past decade, deep learning approaches have gained prominence in addressing various anomaly detection tasks. Reconstruction-based approaches~\cite{sakurada2014anomaly, cheng2020adgan, schlegl2017unsupervised, schlegl2019f, akcay2019ganomaly} assume that anomalies cannot be accurately reconstructed, while self-supervised learning-based approaches ~\cite{hendrycks2019using,li2021cutpaste,sohn2021learning,tack2020csi, zavrtanik2021reconstruction,yi2020patch} rely on augmentations of normal images. These methods have gained popularity in the literature. Recent distance-based methods, which utilize off-the-shelf representations from an encoder pre-trained on large-scale image datasets~\cite{deng2009imagenet} and feed them into a normal feature memory bank, have shown superior detection performance. However, despite their effectiveness, previous approaches in the literature have been developed in a fragmented manner with a lack of attempts for unification. While a recent reconstruction-based method ~\cite{you2022a} unifies models across classes into a single architecture, it does not address grouping anomalies or detecting anomalies in high-resolution natural images. In contrast, our method achieves unified detection at various levels and enables clustering anomalies arising in subtle regions. Furthermore, our method handles low normal-shot anomaly detection, making it more practical for real-world unified anomaly detection systems.

In pre-trained encoder-based methods, it is critical to generate effective feature maps for anomaly detection. This issue encompasses two factors: encoder architecture and feature aggregation. While supervised ConvNets are widely used in pre-trained encoder-based methods, we explore alternative representation choices for anomaly detection. A recent study~\cite{reiss2022anomaly} has highlighted the superiority of self-supervised representations in anomaly detection, which aligns with our work. However, our research specifically focuses on assessing the scalability of self-supervised representations for a general-purpose anomaly detection system. As for feature aggregation, previous pre-trained encoder methods often use a feature map from the last layer without aggregation~\cite{bergman2020deep} or employ rescaling and subsequent concatenation~\cite{cohen2020sub} for aggregation. However, more sophisticated feature aggregation techniques are required for defect detection, particularly when dealing with subtle anomalies. Recent approaches~\cite{roth2022towards} have introduced more complex feature aggregation methods that leverage local neighborhoods with a fixed window size and apply adaptive pooling in the channel dimension. While these techniques significantly improve defect detection, they are complex and sensitive to hyperparameters such as neighborhood size and pooling dimension, limiting scalability across various tasks or datasets. In contrast, \textit{SelFormaly} simplifies feature aggregation with minimal hyperparameters, leading to robustness across datasets and tasks.

Patch-level anomaly learning has emerged as a prominent approach in anomaly detection, considering images as collections of patch features. This methodology allows for anomaly detection at the image level by calculating anomaly scores for individual patches and aggregating them. Anomaly scores can be obtained through explicit training of features~\cite{bergmann2020uninformed, yi2020patch, pang2021explainable, wang2021glancing} or by utilizing nearest neighbor search with a memory bank of normal features~\cite{cohen2020sub, defard2021padim, roth2022towards}. Patch-level anomaly learning has demonstrated considerable effectiveness, particularly in defect detection~\cite{bergmann2020uninformed, yi2020patch, cohen2020sub, pang2021explainable, wang2021glancing, defard2021padim, ding2022catching, roth2022towards}, which deals with subtle anomalies, as opposed to semantic anomaly detection that relies on image-level representations for higher-level anomalies~\cite{bergman2020deep, sun2022out}. We bridge the gap and detect anomalies in a task-agnostic manner by extending patch-level anomaly learning with back patch masking and top $k$-ratio feature matching.
%------------------------------------------------------------------------
\section{Method}

%------------------------------------------------------------------------
\subsection{Rethinking off-the-shelf representations for unified anomaly detection}\label{sec:representation_analysis}

Online anomaly detection methods, such as reconstruction or self-supervised learning methods, often require explicit learning of normality using proxy tasks. However, the discrepancy between proxy tasks and target tasks renders online methods less suitable for general-purpose anomaly detection. Fig.~\ref{fig:periodic_fluctuation} presents this disagreement by showcasing the training curve of proxy tasks from ~\cite{zavrtanik2021draem,li2021cutpaste} alongside periodic evaluation results. While the training loss shows a decreasing trend, the AUROC does not consistently follow the same pattern. Moreover, the optimal points vary across different object categories. Since ground truth labels are unavailable for anomaly detection when transferring to other datasets and tasks, periodic evaluation for monitoring performance become infeasible. As a result, the reliability of online methods is compromised, making it impractical to build a unified system based on these approaches.

% Figure environment removed

We advocate for a pre-trained encoder-based approach that relies on rich representations learned from large-scale datasets, eliminating the need for proxy task learning on the specific dataset. The choice of representation is pivotal in these methods. While supervised ConvNets have traditionally been used for anomaly detection, they may not be optimal for constructing a unified system applicable to diverse datasets due to potential constraints from label information from the pre-training dataset. Moreover, conventional ConvNets with fixed kernel sizes lack flexibility and necessitate additional aggregation steps, increasing the involvement of hyperparameters. This hinders the development of a task-agnostic anomaly detection system. To validate these arguments, we conduct anomaly detection experiments considering supervision levels and architectural aspects, utilizing heterogeneous datasets (i.e., MVTecAD) from the pre-training datasets (i.e., ImageNet) for evaluation. This assessment focuses solely on measuring representational power, excluding factors such as aggregation.

Fig.~\ref{fig:compare_sup_selfsup} illustrates the results, demonstrating that Supervised ResNet, without specific aggregation, fails to achieve high performance based solely on representation. In contrast, self-supervised ViT demonstrates strong performance in both image-level and pixel-level AUROC, with minimal variation between the two. While supervised ConvNets have shown good performance in defect detection, particularly when combined with effective aggregation~\cite{roth2022towards}, they may not necessarily be the optimal choice in terms of sole representation. In fact, self-supervised ViT, which has achieved impressive results in image classification, exhibits effectiveness across different industrial image benchmarks, further highlighting its potential as a scalable representation for task-agnostic anomaly detection.

    
    % Figure environment removed

% % Figure environment removed

%------------------------------------------------------------------------
\subsection{SelFormaly}

 %------------------------------------------------------------------------
  % \subsubsection{Patch-level nearest neighbor retrieval}
  %   {\hskip1em} We conjecture that an image can be represented by a bag of patch features and aim to detect a range of anomalies using different numbers of patches, which is beneficial for capturing local and global appearances. Following our analysis in Section~\ref{sec:representation_analysis}, we utilize patch embeddings $\mathrm{z}_l \in \mathbb{R}^{(N+1)\times D}$ from each layer $l$ in a set of layers $L$ from the self-supervised pretrained ViT, excluding $\texttt{[CLS]}$ token for patch embeddings. The patch representations are simply summarized as an average of the output from each layer, as $\mathrm{Z} = \frac{1}{|L|}\sum_{l \in L}\mathrm{z}_l \in \mathbb{R}^{N \times D}$. For normal training data, we construct a memory bank $\mathcal{M}$ of the summarized patch features $\mathrm{Z}$, downsizing $\mathcal{M}$ with a coreset subsampling~\cite{Sener2017AGA} to reduce computational costs at inference, following previous works~\cite{roth2022towards, lee2022cfa}. When novel test data comes in, we retrieve the nearest neighbor of each patch and compute patch-level distances. Consequently, we obtain a set of nearest distance $\mathcal{D}_N=(d_1, d_2, ..., d_N)$ for all $N$ aggregated patch embeddings $\mathrm{Z}=[\mathrm{Z}^{(1)}; ...;\mathrm{Z}^{(N)}]$ within the image. We use patch-level distances $\mathcal{D}_N$ for anomaly localization with rescaling to the size of an image as in~\cite{roth2022towards, defard2021padim}, but for anomaly detection and clustering, take our novel top $k$-ratio feature matching based on $\mathcal{D}_N$.


% We set $d_i$ as anomaly score $s_i$. Consequently, we obtain a set of nearest distance $\mathbb D_N=(d_1, d_2, ..., d_N)$ for $N$ patches within the image.

% \begin{equation}
% \mathcal{S}_N = \{s_i | 1 \le i \le N \}
% \end{equation}


% % Figure environment removed    


%------------------------------------------------------------------------
\subsubsection{Back Patch Masking}\label{sec:bpm}

   In anomaly detection, it is crucial to prioritize the target and appropriately disregard non-target areas. This is especially important in pre-trained encoder approaches like ours, where explicit learning of normality from normal images is not performed, making the model more susceptible to distractions from spurious signals in the background. Previous research has explored training attention alongside the detection model to mitigate the model's susceptibility to background distractions~\cite{kimura2020adversarial, venkataramanan2020attention, wu2021learning}. However, in the case of \textit{SelFormaly}, where normality is not explicitly learned, it is impractical to learn instance-aware attention separately. Furthermore, the learned attention map is not guaranteed to effectively generalize across diverse datasets and tasks due to the inherent disparities between proxy tasks and the target task. These limitations make traditional attention-based approaches unsuitable for constructing a unified anomaly detection system.

   % Figure environment removed
   
   To overcome these challenges, alternative target-aware detection approaches need to be explored. Drawing inspiration from the inherent property of self-supervised Vision Transformers (ViT), which includes information about object boundaries in its self-attention map (Fig. ~\ref{fig:backpatch_masking}), we leverage this characteristic to develop a target-aware detection framework. We generate a pseudo mask $\mathrm{M}$ from the attention map of self-supervised ViTs to segment the foreground, including the region of interest (RoI), and apply it to a set of nearest distances $\mathcal{D}_N$. Specifically, we calculate the self-attention of the $\texttt{[CLS]}$ token in the last layer of $L$ and average it across heads, resulting in an attention map $\mathrm{S}_A \in \mathbb{R}^{\frac{H}{p}\times\frac{W}{p}}$, where $H$ and $W$ denote the height and width of an input image, respectively. 
   
   However, the attention map tends to be rather coarse, attending to non-RoI areas, and may fail to effectively filter out background noise, as depicted in Fig.~\ref{fig:backpatch_masking}. To address this concern, we introduce attention smoothing to reduce the risk of false positive cases. For attention smoothing, we utilize a smoothing kernel $g(n) = n^{-2}J_n$ with a kernel size of $n$. Subsequently, we apply $g(n)$ to $\mathrm{S}_A^{(i,j)}$ at position $(i,j)$ according to the following equation:
   
   \begin{equation}
   \mathrm{S}_A^{(i,j)} = \sum{a}\sum_{b}{\mathrm{S}_A^{(i+a,j+b)}g(n)}, \quad a,b \in (-n, n)
   \label{eq:attention_smoothing}
   \end{equation}
   
   The resulting smoothed attention map serves as the aforementioned pseudo mask $\mathrm{M} \in \mathbb{R}^{\frac{H}{p} \times \frac{W}{p}}$ (Fig.~\ref{fig:backpatch_masking}). While target-aware detection guidance through attention proves valuable in preventing false positives, it may introduce vulnerability to false negatives when no instances are present. Hence, instead of employing soft masking as commonly found in the literature, we binarize $\mathrm{M}$ using a threshold to remove irrelevant regions. Finally, we apply this to $\mathcal{D}_N$ as $\mathcal{D}_{mask, N} = \mathrm{M} \otimes \mathcal{D}_N \in \mathbb{R}^N$. Our Back Patch Masking (BPM) method provides effective target-aware guidance by ensuring that regions below a certain threshold are not mistakenly rejected while attending to target areas. This allows us to achieve unified and robust anomaly detection.

% \begin{equation}
% \mathbb{D}_{mask, N} = \mathrm{M} \otimes \mathbb{D}_N,
% \label{eq:masking}
% \end{equation}
% where $\mathbb{D}_{mask, N} \in \mathbb{R}^N$.

%------------------------------------------------------------------------
\subsubsection{Top $k$-ratio feature matching}\label{sec:anomaly_scoring}
    We discover that task unification can be achieved by adopting the patch-level anomaly learning approach and incorporating multiple instance learning (MIL) into anomaly scoring. In patch-level anomaly learning, it is a common practice to aggregate patch-level score maps to compute an image-level score map. Two aggregation approaches are commonly used: the max operation, which determines the anomaly based on the patch with the highest anomaly score \cite{roth2022towards}, and top-k MIL aggregations \cite{ding2022catching}. The former has the drawback of reduced robustness as it relies on a single element to make judgments for the entire image. In contrast, the latter, which utilizes top-$k$ MIL, addresses this vulnerability.
    
    Our approach shares similarities with the top-k MIL approach. In a previous study \cite{ding2022catching}, a supervised setting is employed, where the features of the top-k anomalous image patches are updated based on their corresponding labels (anomalous or not). In contrast, we solely rely on the anomaly score without undergoing any training process. Additionally, the focus of \cite{ding2022catching} is specifically on defect detection, whereas our approach extends the top-k MIL approach to integrate the handling of various levels of anomaly detection, from local to semantic.
    
    To compute the image-level anomaly score, we reorder $\mathcal{D}_{N}$ in ascending order (i.e., order by distance from normal patches) and set $\mathcal{S}_{(i)} = \mathcal{D}_{(N-i+1)}$. We consider only the top $k$-\% of reverse-ordered patch distances $\mathcal{S}_{(i)}$ for image-level anomaly score as $\mathcal{S}_{(K)} = \{s{(1)}, s_{(2)}, ..., s_{(K)}\}$. Finally, an image-level anomaly score $s^*$ is as follows:
    \begin{equation}
    s^{*} = \frac{1}{K}\sum_{i=1}^{N}{\mathbbm{1}(i \le K)}\mathcal{S}_{mask,(i)},
    \label{eq:score}
    \end{equation}
    
    where $K$ = $\lceil{(k/100)*N}\rceil$ and $\mathcal{S}_{mask}$ denotes masked scores. By conducting anomaly scoring as an ensemble of multiple instances, we can achieve unified anomaly detection in a task-agnostic manner across various tasks and datasets. Furthermore, our experimental validation has confirmed that this top-$k$ ratio feature ensemble can be extended beyond binary detection to perform grouping of slight anomalies (Sec. \ref{sec:topk_validation}).

%------------------------------------------------------------------------
\section{Experiments}\label{sec:experiments}
    % Our experiments aim to prove that \textit{SelFormaly} is effectively extended to various tasks compared to each state-of-the-art method for a specific task. Additionally, we conduct ablation studies to demonstrate the significance of our findings and the effectiveness of our major components.

%------------------------------------------------------------------------

% \begin{table*}[ht!]
% \begin{center}
% \caption{\textbf{Defect detection on MVTecAD.} We present the image-level AUROC and pixel-level AUROC. The values in parentheses denote pixel-level AUROC. We do not report the pixel-level AUROC for MetaFormer as it was not reported in the original paper.}
% \resizebox{1.0\textwidth}{!}{
% \begin{tabular}{@{}cc|cccccc|>{\centering\arraybackslash}p{6em} @{}}
% \toprule[1.5pt]
% \multicolumn{2}{c|}{Category}
%  & PSVDD~\cite{yi2020patch} & CutPaste~\cite{li2021cutpaste} & PaDiM~\cite{defard2021padim} & Metaformer~\cite{wu2021learning} & DRAEM~\cite{zavrtanik2021draem} & PatchCore~\cite{roth2022towards} & Ours \\
% \midrule\midrule
% \multicolumn{1}{c|}{\multirow{5}{*}{Textures}}
% & carpet & 92.9 (92.6) & 93.9 (98.3) & 99.8 (99.0) & 94.0 & 97.0 (95.5) & 98.7 (99.0) & \textbf{100} (99.4) \\
% \multicolumn{1}{c|}{}
% & grid & 94.6 (96.2) & \textbf{100} (97.5) & 96.7 (97.1) & \underline{85.9} & 99.9 (99.7) & 98.2 (98.7) & 99.3 (99.4) \\
% \multicolumn{1}{c|}{}
% & leather & 90.9 (97.4) & \textbf{100} (99.5) & \textbf{100} (99.0) & 99.2 & \textbf{100} (98.6) & \textbf{100} (99.3) & \textbf{100} (99.6) \\
% \multicolumn{1}{c|}{}
% & tile & 97.8 (91.4) & 94.6 (90.5) & 98.1 (94.1) & 99.0 & \textbf{99.6} (99.2) & 98.7 (95.6) & 99.5 (97.6) \\
% \multicolumn{1}{c|}{}
% & wood & 96.5 (90.8) & 99.1 (95.5) & 99.2 (94.1) & 99.2 & 99.1 (96.4) & 99.2 (95.0)& \textbf{100} (97.6) \\
% \midrule
% \multicolumn{1}{c|}{\multirow{10}{*}{Objects}}
% & bottle & 98.6 (98.1) & 98.2 (97.6) & 99.9 (98.2) & 99.1 & 99.2 (99.1) & \textbf{100} (98.6) & \textbf{100} (98.9) \\
% \multicolumn{1}{c|}{}
% & cable & 90.3 (96.8) & \underline{81.2} (90.0) & 92.7 (96.7) & 97.1 & \underline{91.8} (94.7)& 99.5 (98.4) & \textbf{99.6} (98) \\
% \multicolumn{1}{c|}{}
% & capsule & 76.7 (95.8) & 98.2 (97.4) & 91.3 (98.6) & 87.5 & 98.5 (94.3) & 98.1 (98.8) & \textbf{99.4} (99.0) \\
% \multicolumn{1}{c|}{}
% & hazelnut & 92.0 (97.5) & 98.3 (97.3) & 92.0 (98.1) & 99.4 & \textbf{100} (99.7) & \textbf{100} (98.7) & \textbf{100} (99.2) \\
% \multicolumn{1}{c|}{}
% & metalnut & 94.0 (98.0) & 99.9 (93.1) & 98.7 (97.3) & 96.2 & 98.7 (99.5) & \textbf{100} (98.4) & \textbf{100} (98.0) \\
% \multicolumn{1}{c|}{}
% & pill & 86.1 (95.1) & 94.9 (95.7) & 93.3 (95.7) & 90.1 & 98.9 (97.6) & \underline{96.6} (97.4) & \textbf{99.3} (97.5) \\
% \multicolumn{1}{c|}{}
% & screw & \underline{81.3} (95.7) & 88.7 (96.7) & \underline{85.8} (98.4) & 97.5 & 93.9 (97.6) & 98.1 (99.4) & \textbf{\underline{98.7}} (99.6) \\
% \multicolumn{1}{c|}{}
% & toothbrush & \textbf{100} (98.1) & 99.4 (98.1) & 96.1 (98.8) & \textbf{100} & \textbf{100} (98.1) & \textbf{100} (98.7) & \textbf{100} (98.9) \\
% \multicolumn{1}{c|}{}
% & transistor & 91.5 (97.0) & 96.1 (93.0) & 97.4 (97.6) & 94.4 & 93.1 (90.9) & \textbf{100} (96.3) & 99.8 (96.7) \\
% \multicolumn{1}{c|}{}
% & zipper & 97.9 (95.1) & 99.9 (99.3) & 90.3 (98.4) & 98.6 & \textbf{100} (98.8) & 99.4 (98.8) & 99.0 (98.6) \\
% \midrule\midrule
% \multicolumn{1}{c|}{\multirow{2}{*}{AUROC}}
% & Mean & 92.07 (95.71) & 95.16 (95.97) & 95.42 (97.41) & 95.81 & 97.98 (97.31) & 99.07 (98.07) & \textbf{99.64 (98.52)} \\
% \multicolumn{1}{c|}{}
% & Std. & 6.30 (2.29) & 5.02 (2.90) & 4.15 (1.57) & 4.41 & 2.67 (2.43) & 0.98 (1.32) & \textbf{0.41 (0.87)} \\
% \bottomrule[1.5pt]
% \end{tabular}
% \label{tab:mvtec_res}
% }
% \end{center}
% \end{table*}


\begin{table*}[ht!]
\begin{center}
\caption{\textbf{Defect detection on MVTecAD.} We present the image-level AUROC and pixel-level AUROC. The values in parentheses denote pixel-level AUROC. Pixel-level AUROC for MetaFormer is not reported as it is unavailable in the original paper.}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{@{}cc|cccccc|>{\centering\arraybackslash}p{6em} @{}}
\toprule[1.5pt]
\multicolumn{2}{c|}{Category}
 & PSVDD~\cite{yi2020patch} & CutPaste~\cite{li2021cutpaste} & PaDiM~\cite{defard2021padim} & Metaformer~\cite{wu2021learning} & DRAEM~\cite{zavrtanik2021draem} & PatchCore~\cite{roth2022towards} & Ours \\
\midrule\midrule
\multicolumn{1}{c|}{\multirow{2}{*}{AUROC}}
& Mean & 92.07 (95.71) & 95.16 (95.97) & 95.42 (97.41) & 95.81 & 97.98 (97.31) & 99.07 (98.07) & \textbf{99.64 (98.52)} \\
\multicolumn{1}{c|}{}
& Std. & 6.30 (2.29) & 5.02 (2.90) & 4.15 (1.57) & 4.41 & 2.67 (2.43) & 0.98 (1.32) & \textbf{0.41 (0.87)} \\
\bottomrule[1.5pt]
\end{tabular}
\label{tab:mvtec_res}
}
\end{center}
\end{table*}


%------------------------------------------------------------------------
\subsection{Experimental Details}
% \noindent{\bf Implementation Details.} We use the self-supervised pretrained ViT, DINO-ViTB8, as the default patch-level feature extractor of \textit{SelFormaly}. We aggregate the patch embeddings of all layers in the ViT encoder to compact patch-level representations using simple average operation. To smooth the attention map, we utilize 2D average pooling with $p=7$ of pooling size. The threshold for back-patch masking is used $0.1$. We set $k$ to $1\%$ for defect detection, $10$ for semantic anomaly detection, and $0.5$ for anomaly clustering as similar aspects emerge on different datasets in the same task. For fair comparisons, we set the ratio of coreset for subsampling to $1\%$ for defect detection, $10\%$ for semantic anomaly detection, and $0.1\%$ for multi-class anomaly detection. Note that we use the same strategy except $k$-ratio in all tasks, without parameter updates or data augmentation requiring prior knowledge.

    \noindent{\bf Datasets.} Our defect detection experiments primarily utilize the widely-used benchmark MVTecAD~\cite{bergmann2019mvtec}, which contains 15 sub-datasets with different categories, including 10 objects and 5 textures. In addition, we conduct experiments on Magnetic Tile Defects (MTD)~\cite{huang2020surface}, BeanTech Anomaly Detection (BTAD)~\cite{mishra2021vt}, and Casting manufacturing Product Defect (CPD) to demonstrate the scalability of \textit{SelFormaly} to more specialized tasks. For semantic anomaly detection, we perform experiments on high-resolution images from ImageNet-30 following the evaluation protocol~\cite{hendrycks2019using} and Species-60, the sub-datasets of Species~\cite{hendrycks2022scaling}. Species is a large-scale and high-resolution dataset of which labels are non-overlapped with ImageNet, comprising over 700,000 images of a thousand subcategories. We randomly select 60 subcategories with more than 500 images and prepare the train-test split following the split provided in~\cite{hendrycks2022scaling}.

    \noindent{\bf Implementation details.} We extract patch embeddings from layers $L = {3,...,10}$ in DINO~\cite{caron2021emerging} using the ViT-B/8 architecture for our experiments. To smooth the attention map in our back-patch masking, we utilize the attention scores from the last layer and apply a smoothing kernel with a size of $n=7$. We fix the threshold to 0.1 for binary masking in all tasks, as we have empirically confirmed its consistent effectiveness across various datasets and tasks. Unless specified otherwise, we set the top-$k$ ratio to 5\% for all tasks. Since all tasks share the same settings, our system operates in a task-agnostic manner. Additionally, we do not fine-tune all methods on the data, as unsupervised settings assume the unavailability of abnormal data during training.
    
    \noindent{\bf Evaluation metrics.} We report the threshold-independent metric AUROC to evaluate image-level and pixel-level anomaly detection performance. For anomaly clustering, we provide NMI (Normalized Mutual Information) and ARI (Adjusted Random Index), two popular metrics for clustering quality analysis when ground-truth cluster assignments are available for the test set. Furthermore, to address label imbalance, we report the F1 score after matching ground truth and cluster prediction using the Hungarian method~\cite{kuhn1955hungarian}.


%------------------------------------------------------------------------
\subsection{Main Results}\label{sec:main_results}

    \noindent{\bf Defect detection.} In Table \ref{tab:mvtec_res}, we present a comprehensive evaluation result for anomaly detection and localization across various approaches, including unsupervised~\cite{akcay2019ganomaly,wu2021learning,zavrtanik2021draem}, self-supervised~\cite{yi2020patch,li2021cutpaste}, pretrained~\cite{defard2021padim,roth2022towards}, and adaptation~\cite{reiss2021panda}, on the MVTecAD. The results demonstrate that \textit{SelFormaly} consistently outperforms the baselines, achieving a 61\% reduction in error compared to the previous state-of-the-art method~\cite{roth2022towards}. In particular, \textit{SelFormaly} exhibits only 27 incorrect detections out of 1725 images at the optimal F1 threshold. Additionally, our method shows significantly less standard deviation across classes than baselines, indicating our applicability to various sub-datasets with distinct categories (Objects, Textures). Furthermore, Table~\ref{tab:defect_other_res} presents the results on other benchmarks (MTD, BTAD, CPD), demonstrating that our method outperforms the state-of-the-art methods in all datasets. This confirms that, compared to pretrained encoder-based methods, the methods with proxy tasks exhibit larger performance fluctuations with changes in datasets. In contrast, \textit{SelFormaly} consistently presents outstanding performance without fine-tuning, demonstrating its robustness in various applications.

    \begin{table}
    \caption{\textbf{Defect detection on other benchmarks.} Our baselines are state-of-the-art framework from unsupervised~\cite{zavrtanik2021draem}, self-supervised~\cite{li2021cutpaste}, pretrained~\cite{roth2022towards} methods. We produced the baseline results for other benchmarks in the identical settings on MVTecAD using their released code. However, for CutPaste, we produced the results ourselves since the official code is unavailable.}
    \begin{center}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c c c|c} \toprule[1.5pt]
    Method & DRAEM~\cite{zavrtanik2021draem} & CutPaste~\cite{li2021cutpaste} & PatchCore~\cite{yi2020patch} & Ours \\
    \midrule\midrule
    MTD & 70.3 & 93.8 & 97.6 & \textbf{98.8}\\
    BTAD & 85.3 & 93.1 & 92.9 & \textbf{95.1}\\
    CPD & 92.8 & 97.4 & 96.8 & \textbf{99.9}\\
    \bottomrule[1.5pt]
    \end{tabular}}
    \end{center}
    \label{tab:defect_other_res}
    \end{table}

    \noindent{\bf Semantic anomaly detection.} In Table~\ref{tab:semantic_res}, we observe that our method outperforms baselines by a significant margin. While there may be some concerns about the fairness of comparing with ImageNet-30 since \textit{SelFormaly} is based on the ImageNet-pretrained encoder with self-supervision, we address this issue by conducting experiments on Species-60. This dataset does not semantic overlap with ImageNet. As seen in Table~\ref{tab:semantic_res}, our method outperforms its competitors by a large gap (10.67, 11.62, 18.15 for~\cite{roth2022towards, tack2020csi, hendrycks2019using}, respectively). This confirms the effectiveness of our approach in detecting semantic anomalies and slight anomalies, such as defects, regardless of the overlap issue. Meanwhile, both~\cite{hendrycks2019using} and~\cite{tack2020csi}, which are self-supervised approaches trained from scratch on the given dataset, exhibit a more significant difference of about 10 in AUROC compared to ours. This result suggests that the training setups of baselines are tailored to the ImageNet-30 and suboptimal for Species-60. Moreover, PatchCore, another pretrained encoder-based method, performs poorly on semantic anomaly detection due to an architectural design tailored to defect detection. Notably, the top $k$-ratio feature matching improves the AUROC of PatchCore (from 82.43 to 87.40), although it still lags behind ours by 5\%. This highlights the role of the top $k$ ratio feature matching in improving scalability to various anomaly levels.

    \begin{table}
    \caption{\textbf{Semantic anomaly detection.} AUROC of PatchCore for ImageNet-30 is not reported, as PatchCore uses a supervised pretrained encoder on ImageNet with label information. We produced the results of our competitors for Species-60 using the same setting as for ImageNet-30.}
    \begin{center}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c c c|c} \toprule[1.5pt]
    Dataset & SS-OOD~\cite{hendrycks2019using} &  CSI~\cite{tack2020csi} & PatchCore~\cite{roth2022towards} & Ours \\
    \midrule\midrule
    ImageNet-30 & 85.7 & 91.6 & - &\textbf{97.1} \\
    Species-60 & 74.8 & 81.4 & 82.4/87.4$^*$ &\textbf{92.9}\\
    \bottomrule[1.5pt]
    \end{tabular}}
    \end{center}
    \label{tab:semantic_res}
    \end{table}

    \begin{table*}[ht!]
    \begin{center}
    \caption{\textbf{Results for Multi-class Anomaly Detection and Localization on the MVTecAD Dataset.}}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c c c c c c c c|c} \toprule[1.5pt]
     & US~\cite{bergmann2020uninformed} & PSVDD~\cite{yi2020patch} & FCDD~\cite{liznerski2021explainable} & CutPaste~\cite{li2021cutpaste} & PaDiM~\cite{defard2021padim} & MKD~\cite{salehi2021multiresolution} & DRAEM~\cite{zavrtanik2021draem} & UniAD~\cite{you2022a} & Ours \\
    \midrule\midrule
    Image & 74.5(87.7) & 76.8(92.1) & - & 77.5(96.1) & 84.2(95.5) & 81.9(87.8) & 88.1(98.0) & 96.5(96.6) & \textbf{99.59(99.64)} \\
    Pixel & 81.8(93.9) & 85.6(95.7) & 63.3(92) & - & 89.5(97.4) & 84.9(90.7) & 87.2(97.3) & 96.8(96.6) & \textbf{98.39(98.52)} \\
    \bottomrule[1.5pt]
    \end{tabular}
    \label{tab:multiclass_res}
    }
    \end{center}
    \end{table*}

    \noindent{\bf Multi-class anomaly detection.} Our method is easily scalable to multi-class anomaly detection by incorporating representations of multi-class data in a normal memory bank. As shown in Table~\ref{tab:multiclass_res}, our approach indicates almost no performance drop in AUROC. The method proposed for the multi-class detection task~\cite{you2022a} also shows a minimal performance drop compared to the other methods, except ours. However, this method shows weak performance in a separate case (i.e., one-class defect detection). On the contrary, \textit{SelFormaly} achieves the new state-of-the-art performance in one- and multi-class settings. Likewise, our method shows the same result for anomaly localization, achieving state-of-the-art results with a lower performance drop of 0.13.

    \noindent{\bf Anomaly clustering.} \textit{SelFormaly} effectively clusters the types of anomalies with top $k$\% patch embeddings based on reordered features, capturing more than just binary information of normality. We easily apply our averaged features on K-means clustering~\cite{macqueen1965some}. We compare our method with~\cite{sohn2023anomaly}, the only method that has studied anomaly clustering task, as well as the deep clustering methods, IIC~\cite{ji2019invariant}, GATCluster~\cite{niu2020gatcluster}, and SCAN~\cite{van2020scan} as baselines. Although our approach is not primarily designed for clustering, it outperforms baselines, including \cite{sohn2023anomaly} (Table \ref{tab:clustering}). In F1 scores, we improve the strongest baseline by 10.48\% and achieve the highest F1 score. In particular, merely with K-means clustering, we outperform the state-of-the-art method~\cite{sohn2023anomaly}, while their K-means result (in parenthesis) is not as powerful as their best score. Additionally, we provide the qualitative results for the effect of the top $k$-ratio on anomaly clustering in the supplementary material.

%------------------------------------------------------------------------
\begin{table}
\caption{\textbf{Clustering results on MVTecAD.}}
\resizebox{\linewidth}{!}{

\begin{tabular}{c||cccc|c}
\toprule[1.5pt]
    & IIC~\cite{ji2019invariant} & GATCluster~\cite{niu2020gatcluster} & SCAN~\cite{van2020scan}   & AC~\cite{sohn2023anomaly} & \textbf{Ours}   \\ \midrule
NMI & 0.093 & 0.136     & 0.210 & 0.608 (0.500)            & \textbf{0.613} \\
ARI & 0.020 & 0.053     & 0.103 & 0.489 (0.390)             & \textbf{0.505} \\
F1  & 0.285 & 0.264     & 0.335 & 0.641 (0.601)             & \textbf{0.707} \\ \bottomrule[1.5pt]
\end{tabular}}
\label{tab:clustering}
% \centering{\small{AC: Anomaly Clustering}}
\end{table}

%------------------------------------------------------------------------
\subsection{Low-shot anomaly detection}

    To assess the performance of our unified system in scenarios with limited normal data, we conducted low-shot anomaly detection experiments. Even in the few normal shot scenario, \textit{SelFormaly} outperforms other pretrained methods, as depicted in Fig.~\ref{fig:lowshot}. Despite previous pretrained encoder-based methods performing well in the low-shot setting, our approach achieves an AUROC of 90.3 in the one-shot setting, improving the previous state-of-the-art by around 8\%. Moreover, even when using only one-fourth of the normal training data, \textit{SelFormaly} still matches previous state-of-the-art performance, 99.03 and 99.07 (Fig.~\ref{fig:lowshot}). This demonstrates the cost-effectiveness of our method for building datasets and its effectiveness in cases with a narrow boundary for normal data, where only a limited number of sample images are available.

    % Figure environment removed
%------------------------------------------------------------------------
% \begin{table}
% \caption{\textbf{Effect of design choices for \textit{SelFormaly}.} We report Image-level AUROC on MVTecAD for defect detection (Defect-AD) and Species-60 for semantic anomaly detection (Sem-AD).} 
% \begin{center}
% \resizebox{0.85\linewidth}{!}{
% \begin{tabular}{@{}cccc|cc@{}}
% \toprule[1.0pt]
% P.Agg & SP & Top-k & BPM & Defect-AD & Sem-AD \\ \midrule\midrule
% \xmark    &\checkmark      &\xmark       &\xmark     &98.08       &87.13       \\
% \checkmark    &\checkmark      &\xmark       &\xmark     &98.31       &86.74      \\
% \checkmark    &\checkmark      &\xmark       &\checkmark     &98.86       &87.30       \\
% \checkmark   &\xmark    &\checkmark       &\checkmark     &99.11       &92.03       \\ 
% \xmark    &\checkmark      &\checkmark       &\checkmark     &99.57       &93.12       \\ 
% \midrule

% \checkmark    &\checkmark      &\checkmark       &\checkmark     &\textbf{99.64}       &\textbf{92.93}       \\ 
% \bottomrule[1.0pt]
% \end{tabular}}

% \small{SP:Small Patch in the encoder, P.Agg.: Patch-wise Aggregation \\ 
% Top-$k$:Top $k$-ratio, BPM: Back-Patch Masking \\}
% \end{center}
% \label{tab:ablations}
% \end{table}
\subsection{Ablation Study}
    We conduct ablation studies to verify the critical role of BPM and Top $k$-ratio feature matching in developing a unified anomaly detection system. The results are reported on the MVTecAD for defect detection, multi-class anomaly detection, and anomaly clustering, while for semantic anomaly detection, results are reported on Species-60.

    \subsubsection{Top $k$-ratio feature matching}\label{sec:topk_validation}
    
    In Fig.~\ref{fig:topk_scores}, we investigate the effect of $k$-ratio on various tasks by evaluating changes in image-level AUROC over varying $k$ from 0.1 to 20. The results confirm that the top $k$-ratio scores enable the detection of anomalies of various levels by referring to the multiple high-scored regions. When $k$ is set to 0.1, representing the case without top $k$-ratio feature matching applied, a substantial performance gap exists between semantic anomaly detection and other tasks, indicating a lack of task generality. However, when top $k$-ratio feature matching is applied ($k$ is larger than 0.1), individual task performances improve, and the performance gap between tasks diminishes. Regardless of the different shape of the AUROC curve for $k$ due to varying anomaly levels for each task, all tasks achieve outstanding performance at around 5\%, highlighting the effectiveness of top $k$-ratio feature matching for task-agnostic anomaly detection.

    % Figure environment removed
    
    We further compare three cases—All, Max, and Top $k$-ratio—regarding the selection of features in anomaly clustering, and the results demonstrate the superiority of top $k$-ratio feature matching. Table ~\ref{tab:topk_clustering} illustrates that top $k$-ratio outperforms other cases significantly, improving ARI by 84\% and 33.2\% over All and Max, respectively. Anomaly clustering for defect types requires capturing fine-grained details in semantically similar but locally distinct images, making global representations unsuitable. The Max case, relying on a single patch with the highest anomaly score, is less effective due to its vulnerability to false positives. In contrast, the top $k$-ratio approach utilizes patch-level representations, capturing fine-grained details while being robust against false positives, resulting in improved clustering performance. 
    
    \begin{table}
    \begin{center}
    \caption{\textbf{Effect of top $k$-ratio feature matching in anomaly clustering on MVTecAD.} For each case, we run clustering algorithms on the average pooled feature of all patch-level representations, the feature with the highest anomaly score, and the average pooled feature of patch features with top $k$\% of anomaly scores.}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{@{}c|ccc|ccc|ccc@{}}
    \toprule[1.0pt]
        & \multicolumn{3}{c|}{All}   & \multicolumn{3}{c|}{Max}   & \multicolumn{3}{c}{Top $k$-ratio ($k$=1)} \\ \cmidrule(l){2-10}
        & Object & Texture & Mean   & Object & Texture & Mean   & Object   & Texture   & Mean     \\ \midrule
    \multicolumn{1}{c||}{NMI} & 0.330  & 0.515   & 0.3916 & 0.447  & 0.668   & 0.5204 & \textbf{0.548}    & \textbf{0.743}     & \textbf{0.6127}   \\
    \multicolumn{1}{c||}{ARI} & 0.210  & 0.406   & 0.2757 & 0.294  & 0.548   & 0.3790 & \textbf{0.419}    & \textbf{0.675}     & \textbf{0.5048}   \\
    \multicolumn{1}{c||}{F1}  & 0.495  & 0.581   & 0.5236 & 0.551  & 0.712   & 0.6042 & \textbf{0.663}    & \textbf{0.795}     & \textbf{0.7072}   \\ \bottomrule[1.0pt]
    \end{tabular}}
    \label{tab:topk_clustering}
    \end{center}
    \end{table}
    

    \subsubsection{Back-patch masking}

    \noindent{\bf Smoothing kernel size $n$.} Table ~\ref{tab:bpm_smoothing} demonstrates the impact of smoothing kernel size $n$ on back-patch masking. When using a raw attention map with $n$ set to 1, inferior results are obtained, especially in terms of anomaly localization and semantic anomaly detection. This finding aligns with the previous discussion, where we noted that a raw attention map is highly susceptible to anomalous-like normal regions. However, increasing the value of $n$ to a larger value significantly improves AUROC across all of the tasks considered in our study. In particular, as the smoothing kernel size reaches 5, the performance improvement rate begins to slow down, and little to no improvement is observed for kernel sizes greater than or equal to our default setting, 7.

     \begin{table}[ht!]
    \begin{center}
    \caption{\textbf{Results on varying smoothing kernel size $n$.} We evaluate the performance of our method on multiple tasks with varying $n$ from 1 to 13 by 2.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}cccccccc@{}}
    \toprule[1.0pt]
        $n$  & 1      & 3      & 5      & 7      & 9      & 11     & 13     \\ \midrule
    \multicolumn{8}{l}{\textit{\textbf{Defect Detection}}}               \\
    \rowcolor[HTML]{FFD5D5} 
    Image & 99.41  & 99.5   & 99.63  & 99.64  & 99.64  & 99.64  & 99.65  \\
    \rowcolor[HTML]{FFD5D5} 
    Pixel & 94.33  & 97.49  & 98.31  & 98.53  & 98.55  & 98.55  & 98.56  \\ \midrule
    \multicolumn{8}{l}{\textit{\textbf{Semantic Anomaly Detection}}}     \\
    \rowcolor[HTML]{DEF5F8} 
    Image & 82.52  & 90.6   & 92.47  & 92.93  & 92.98  & 92.95  & 92.9   \\ \midrule
    \multicolumn{8}{l}{\textit{\textbf{Multiclass Anomaly Detection}}}   \\
    \rowcolor[HTML]{FFF2CC} 
    Image & 99.06  & 99.33  & 99.41  & 99.41  & 99.4   & 99.41  & 99.43  \\
    \rowcolor[HTML]{FFF2CC} 
    Pixel & 93.01  & 96.95  & 97.95  & 98.24  & 98.28  & 98.26  & 98.25  \\ \midrule
    \multicolumn{8}{l}{\textit{\textbf{Anomaly Clustering}}}             \\
    \rowcolor[HTML]{D9D2E9} 
    NMI   & 0.6008 & 0.6108 & 0.6142 & 0.6127 & 0.6055 & 0.6048 & 0.6056 \\
    \rowcolor[HTML]{D9D2E9} 
    ARI   & 0.4859 & 0.4979 & 0.5036 & 0.5048 & 0.496  & 0.4978 & 0.4983 \\
    \rowcolor[HTML]{D9D2E9} 
    F1    & 0.6866 & 0.6996 & 0.7022 & 0.7072 & 0.7023 & 0.7011 & 0.7029 \\ \bottomrule[1.0pt]
    \end{tabular}
    }
    \label{tab:bpm_smoothing}
    \end{center}
    \end{table}

    \noindent{\bf Binary masking threshold $\lambda$.} To examine the impact of binary masking with a threshold on back-patch masking, we compare it with soft masking, where the original representations are multiplied by the floating-point values of the raw attention scores ranging from 0 to 1. Table~\ref{tab:bpm_thres} shows that soft masking is significantly less effective than binary masking in most cases, even under the lowest-performing threshold value of $\lambda=0.3$. This highlights the critical role played by binary masking in back-patch masking. Moreover, we investigate the effect of varying threshold value $\lambda$ on back-patch masking performance. Our results demonstrate that a threshold value of $\lambda=0.1$ performs best across most of the tasks considered, with a slight decrease in performance observed as $\lambda$ increases beyond this value.

    \begin{table}[ht!]
    \begin{center}
    \caption{\textbf{Effect of binary masking and its threshold $\lambda$.} We show the efficacy of binary masking compared to soft masking. To further investigate the effect of binary masking threshold $\lambda$, we present the results for our tasks with varying $\lambda$ from 0.1 to 0.3.}
    \scriptsize{
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}cc|ccc|c@{}}
    \toprule[1.0pt]
                                                                          &       & \multicolumn{3}{c|}{Binary}  &                        \\ \cmidrule(lr){3-5}
                                                                          &       & 0.1    & 0.2    & 0.3     & \multirow{-2}{*}{Soft} \\ \midrule
    \rowcolor[HTML]{FFD5D5} 
    \cellcolor[HTML]{FFD5D5}                                              & Image & 99.64   & 99.57   & 99.38   & 97.71                  \\
    \rowcolor[HTML]{FFD5D5} 
    \multirow{-2}{*}{\cellcolor[HTML]{FFD5D5}\textit{Defect-AD}}             & Pixel & 98.53   & 97.2    & 94.58   & 94.89                  \\ \midrule
    \rowcolor[HTML]{DEF5F8} 
    \textit{Sem-AD}                                                       & Image & 92.93   & 93.06   & 92.77   & 81.69                  \\ \midrule
    \rowcolor[HTML]{FFF2CC} 
    \cellcolor[HTML]{FFF2CC}                                              & Image & 99.41   & 99.34   & 99.07   & 95.11                  \\
    \rowcolor[HTML]{FFF2CC} 
    \multirow{-2}{*}{\cellcolor[HTML]{FFF2CC}\textit{MC-AD}}      & Pixel & 98.24   & 96.67   & 94.01   & 92.43                  \\ \midrule
    \rowcolor[HTML]{D9D2E9} 
    \cellcolor[HTML]{D9D2E9}                                              & NMI   & 0.6127  & 0.6079  & 0.5975  & 0.5725                 \\
    \rowcolor[HTML]{D9D2E9} 
    \cellcolor[HTML]{D9D2E9}                                              & ARI   & 0.5048  & 0.4869  & 0.4906  & 0.4629                 \\
    \rowcolor[HTML]{D9D2E9} 
    \multirow{-3}{*}{\cellcolor[HTML]{D9D2E9}\textit{Clustering}} & F1    & 0.7072  & 0.685   & 0.6904  & 0.6692                 \\ \bottomrule[1.0pt]
    \end{tabular}}}
    \label{tab:bpm_thres}
    \end{center}
    \end{table}

    
    % \noindent{\bf Patch-wise aggregation} Compared to the case without aggregation, our approach promotes performance for defect detection and competitive results for semantic anomaly detection. Considering the aggregation of features over layers reduces the feature dimensions for computing nearest neighbor search, patch-wise layer aggregation loses almost no information encoded in each layer while reducing computational costs. We further demonstrate that our approach achieves better results than other layer aggregation choices in the supplementary material. In addition, we find experimentally the feature extractor achieves higher performance with the smaller patch size in the ViT, providing a closer look with a single patch and, accordingly, fine-grained features for anomaly detection in Table~\ref{tab:ablations}.


% \begin{table}
% \begin{center}
%     \caption{\textbf{AUROC on varying $n$ for our tasks.} We report both an image and pixel-level AUROC. We tested the effect of smoothing kernel size $n$ by varying $n$ from 1 to 15.}
%     \resizebox{1.0\linewidth}{!}{
%     \begin{tabular}{@{}c||cccccccc@{}}
%     \toprule[1.0pt]
%           & 1     & 3    & 5     & 7     & 9     & 11    & 13    & 15    \\ \midrule
%     Image & 99.41 & 99.5 & 99.63 & 99.63 & 99.63 & 99.64 & 99.65 & 99.63 \\
%     Pixel & 94.33 & 97.5 & 98.32 & 98.52 & 98.55 & 98.55 & 98.55 & 98.54 \\ \bottomrule[1.0pt]
%     \end{tabular}}
%     \label{tab:bpm_mvtec}

%     \resizebox{1.0\linewidth}{!}{
%     \begin{tabular}{@{}c||cccccccc@{}}
%     \toprule[1.0pt]
%     &1     & 3    & 5     & 7     & 9     & 11    & 13   & 15 \\ \midrule
%     Image & 82.52 & 90.6 & 92.47 & 92.93 & 92.98 & 92.95 & 92.9 & 92.84   \\ \bottomrule[1.0pt]
%     \end{tabular}}
%     \label{tab:bpm_species}

%     \resizebox{1.0\linewidth}{!}{
%     \begin{tabular}{@{}c||cccccccc@{}}
%     \toprule[1.0pt]
%           & 1     & 3     & 5     & 7     & 9     & 11    & 13    & 15    \\ \midrule
%     Image & 99.06 & 99.33 & 99.41 & 99.41 & 99.4  & 99.41 & 99.43 & 99.42 \\
%     Pixel & 93.01 & 96.95 & 97.95 & 98.24 & 98.28 & 98.26 & 98.25 & 98.25 \\ \bottomrule[1.0pt]
%     \end{tabular}
%     }
%     \label{tab:bpm_multiclass}

%     \resizebox{1.0\linewidth}{!}{
%     \begin{tabular}{@{}c||cccccccc@{}}
%     \toprule[1.0pt]
%         & 1      & 3      & 5      & 7      & 9      & 11     & 13     & 15     \\ \midrule
%     NMI & 0.6008 & 0.6108 & 0.6142 & 0.6127 & 0.6055 & 0.6048 & 0.6056 & 0.5973 \\
%     ARI & 0.4859 & 0.4979 & 0.5036 & 0.5048 & 0.496  & 0.4978 & 0.4983 & 0.4862 \\
%     F1  & 0.6866 & 0.6996 & 0.7022 & 0.7072 & 0.7023 & 0.7011 & 0.7029 & 0.6927 \\ \bottomrule[1.0pt]
%     \end{tabular}
%     }
%     \label{tab:bpm:clustering}
% \end{center}
% \end{table}
%% smoothed attention better than raw attention.

% \begin{table}[ht!]
% \begin{center}
%     \caption{\textbf{AUROC on varying $n$ for MVTecAD.} We report both an image and pixel-level AUROC. We tested the effect of smoothing kernel size $n$ by varying $n$ from 1 to 15.}
%     \resizebox{1.0\linewidth}{!}{
%     \begin{tabular}{@{}cccccccc@{}}
%     \toprule[1.0pt]
%     1     & 3    & 5     & 7     & 9     & 11    & 13   & 15 \\ \midrule
%     82.52 & 90.6 & 92.47 & 92.93 & 92.98 & 92.95 & 92.9 & 92.84   \\ \bottomrule[1.0pt]
% \end{tabular}}
    
% \end{center}
% \end{table}




%------------------------------------------------------------------------

\section{Conclusion}
Previous anomaly detection approaches have primarily focused on handling a single task with a single architecture, which is resource-intensive and incurs high maintenance costs. Thus, we propose a powerful unified anomaly detection framework that handles various anomaly detection tasks with a single architecture. Online methods are unsuitable for universal anomaly detection due to the gap between the proxy and target tasks. Thus, we propose a unified anomaly detection framework based on a self-supervised representations after exploration on suitable off-the-shelf representations for unified anomaly detection. By incorporating techniques such as BPM and top-$k$ ratio feature matching, the framework enables task unification in a task-agnostic manner. Experimental results demonstrate that the proposed framework achieves outstanding performance across various tasks.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}