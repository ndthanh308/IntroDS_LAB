%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
% \documentclass[sigconf,anonymous,review,authordraft]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

% %% Rights management information.  This information is sent to you
% %% when you complete the rights form.  These commands have SAMPLE
% %% values in them; it is your responsibility as an author to replace
% %% the commands and values with those provided to you when you
% %% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}


% %% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
% %%
% %%  Uncomment \acmBooktitle if the title of the proceedings is different
% %%  from ``Proceedings of ...''!
% %%
% %%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
% %%  June 03--05, 2018, Woodstock, NY}
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}


\copyrightyear{2023}
\acmYear{2023}
\setcopyright{acmlicensed}
\acmConference[MM '23] {Proceedings of the 31st ACM International Conference on Multimedia}{October 29--November 3, 2023}{Ottawa, ON, Canada.}
\acmBooktitle{Proceedings of the 31st ACM International Conference on Multimedia (MM '23), October 29--November 3, 2023, Ottawa, ON, Canada}
\acmPrice{15.00}
\acmDOI{10.1145/3581783.3611752}
\acmISBN{979-8-4007-0108-5/23/10}

% 1 Authors, replace the red X's with your assigned DOI string during the rightsreview eform process.
% 2 Your DOI link will become active when the proceedings appears in the DL.
% 3 Retain the DOI string between the curly braces for uploading your presentation video.

\acmSubmissionID{266}

\usepackage{graphicx}
\usepackage{amsmath}
% \usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bbding}
\usepackage{subcaption} % for 'subtable' env
\usepackage{multirow} % for cmd 'multirow', 
\usepackage{balance}

\usepackage[ruled,vlined]{algorithm2e}
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Uncertainty-Guided Spatial Pruning Architecture for Efficient Frame Interpolation}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


\author{Ri Cheng}
\orcid{0000-0002-5866-6847}
\affiliation{%
  \institution{School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Shanghai Collaborative Innovation Center of Intelligent Visual Computing, Fudan University}
  \city{Shanghai}
  \country{China}
}
\email{rcheng22@m.fudan.edu.cn}


\author{Xuhao Jiang}
\orcid{0000-0002-4646-5052}
\affiliation{%
  \institution{School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Shanghai Collaborative Innovation Center of Intelligent Visual Computing, Fudan University}
  \city{Shanghai}
  \country{China}
}
\email{20110240011@fudan.edu.cn}

\author{Ruian He}
\orcid{0000-0001-9598-3043}
\affiliation{%
  \institution{School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Shanghai Collaborative Innovation Center of Intelligent Visual Computing, Fudan University}
  \city{Shanghai}
  \country{China}
}
\email{rahe16@fudan.edu.cn}

\author{Shili Zhou}
\orcid{0000-0001-7283-2314}
\affiliation{%
  \institution{School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Shanghai Collaborative Innovation Center of Intelligent Visual Computing, Fudan University}
  \city{Shanghai}
  \country{China}
}
\email{slzhou19@fudan.edu.cn}


\author{Weimin Tan}
\orcid{0000-0001-7677-4772}
\authornote{Corresponding Author. This work is supported by NSFC (Grant No.: U2001209) and Natural Science Foundation of Shanghai (21ZR1406600).}
\affiliation{%
  \institution{School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Shanghai Collaborative Innovation Center of Intelligent Visual Computing, Fudan University}
  \city{Shanghai}
  \country{China}
}
\email{wmtan@fudan.edu.cn}


\author{Bo Yan}
\orcid{0000-0003-0256-9682}
% \authornote{Corresponding Author. This work is supported by NSFC (Grant No.: U2001209, 61902076) and Natural Science Foundation of Shanghai (21ZR1406600).}
\authornotemark[1]
\affiliation{%
  \institution{School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Shanghai Collaborative Innovation Center of Intelligent Visual Computing, Fudan University}
  \city{Shanghai}
  \country{China}
}
\email{byan@fudan.edu.cn}



%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Ri Cheng et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
% The video frame interpolation (VFI) model applies the convolution operation to all locations. Therefore, it has redundant computation in the easy regions containing static and simple movement. Recent dynamic spatial pruning architectures try to skip redundant computation but cannot properly identify easy regions in VFI tasks without supervision. In this paper, we develop an Uncertainty-Guided Spatial Pruning (UGSP) architecture to skip redundant computation for efficient frame interpolation dynamically. Specifically, pixels with low uncertainty indicate the easy regions and can be skipped from the calculation. Therefore, we utilize uncertainty-generated mask labels to guide our UGSP in properly locating the easy region. Furthermore, we propose a self-contrast training strategy that utilizes an auxiliary non-pruning branch to increase the performance of our UGSP. Extensive experiments show that our UGSP can save up to 33.3\% FLOPs and 48.7\% inference time compared to baseline while maintaining state-of-the-art performance on multiple benchmarks.
The video frame interpolation (VFI) model applies the convolution operation to all locations, leading to redundant computations in regions with easy motion. 
We can use dynamic spatial pruning method to skip redundant computation, but this method cannot properly identify easy regions in VFI tasks without supervision. In this paper, we develop an Uncertainty-Guided Spatial Pruning (UGSP) architecture to skip redundant computation for efficient frame interpolation dynamically. Specifically, pixels with low uncertainty indicate easy regions, where the calculation can be reduced without bringing undesirable visual results. Therefore, we utilize uncertainty-generated mask labels to guide our UGSP in properly locating the easy region. Furthermore, we propose a self-contrast training strategy that leverages an auxiliary non-pruning branch to improve the performance of our UGSP. Extensive experiments show that UGSP maintains performance but reduces FLOPs by 34\%/52\%/30\% compared to baseline without pruning on Vimeo90K/UCF101/MiddleBury datasets. In addition, our method achieves state-of-the-art performance with lower FLOPs on multiple benchmarks.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010224.10010245.10010254</concept_id>
       <concept_desc>Computing methodologies~Reconstruction</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Reconstruction}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Video Frame Interpolation, Dynamic Network, Spatial Pruning, Uncertainty}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\section{Introduction}
\label{sec:intro}

    % Figure environment removed


% task
% motivate
% the applications performing on
Video frame interpolation (VFI) attempts to interpolate intermediate frames in the middle of two input consecutive frames.
Applications such as slow-motion generation \cite{Jiang_2018_CVPR}, video compression \cite{Wu_2018_ECCV}, and novel view synthesis \cite{10.1007/978-3-319-46493-0_18}, utilize frame interpolation frameworks extensively.
In recent years, well-designed deep learning-based model shows excellent progress in VFI task due to its powerful feature representation ability.
When performing such deep VFI models on the resource-limited edge devices, it encounters a computational resource constraint issue.
% particularly for large resolution input frames.
Consequently, efficient VFI is a realistic requirement for these devices.

% efficiency, adv, diadv
% method
Real-world videos contain large and non-linear
motion that requires a large number of convolution layers to expand the receptive field, resulting in significant computational demand.
To address this problem, several efforts have been made by utilizing coarse-to-fine structure \cite{huang2022rife, Park_2021_ICCV,Lee_2020_CVPR} or predicting the intermediate flow and frame in one step \cite{Kong_2022_CVPR}.
% To address this problem, the VFI model  utilizes a coarse-to-fine manner to generate intermediate frames at a fine scale based on the flow or intermediate frame features estimated at a coarse scale.
% To further enhance the accuracy and efficiency, several efforts have been made by predicting the intermediate flow and feature in one step \cite{Kong_2022_CVPR} or using the distillation knowledge method \cite{huang2022rife}.
However, these networks still involve redundant computation since they apply the convolution operation to all locations equally without distinguishing between challenging and easy regions.
Based on the observation in Section~\ref{sec:ob} that for easy regions with static or straightforward motion,  convolutions in coarse scales are sufficient to achieve satisfactory outcomes.
What is more, these easy regions occupy a large portion of the input frames.
Therefore, by reducing the heavy computation in these regions, model inference speed can be significantly increased. As shown in Figure~\ref{001_intro}(a), by reducing the redundant calculations in static regions (colored blue), we can save 68\% FLOPs. In addition, our pruning model has a higher PSNR since the baseline model cannot estimate the trajectory of the ball accurately, indicating that our model can prioritize the reconstructed quality of challenging areas by using the pruning mask.

% because it focuses on large-scale motion estimation using the estimated pruning mask during training.

% and that the image construction loss and training strategy are ineffective for the dynamic VFI network.
% by the absolute difference as shown in the upper right of Figure~\ref{001_intro} (b).
In this paper, we propose an Uncertainty-Guided Spatial Pruning (UGSP) architecture to dynamically skip redundant computations for efficient frame interpolation.
During inference, UGSP predicts pruning masks to localize redundant computation regions and skip them using sparse convolution \cite{Wang_2021_CVPR}.
However, we discover that directly predicting such a mask without supervision is inaccurate and unstable.
Aleatoric uncertainty \cite{NIPS2017_2650d608,NEURIPS2021_88a19961} measures the prediction difficulty of each pixel in an instance, so it can be used to guide the estimation of easy regions that require less computation resources.
Therefore, we propose using uncertainty to enhance the performance of our VFI pruning model. 

% As show in in the upper right of Figure~\ref{001_intro} (b), moving edge regions include more errors than static regions. Since aleatoric uncertainty measures the prediction difficulty of an instance, it can be utilized to estimate these challenging regions.

Specifically, the UGSP training procedure consists of two phases.
In the first phase, the model is trained to predict the mean and variance of each pixel in the intermediate target frame.
We observe that pixels with large uncertainty (variation) denote complex and large movements, so they demand more computational resources, as shown at the middle of Figure~\ref{001_intro}(b). As observed in Section~\ref{sec:ob}, we find that large uncertainty regions are crucial to the final visual quality, since the improvement in visual quality will increase significantly when sufficient computational resources are allocated to these regions.
In this case, we propose an uncertainty-guided mask prediction approach
to guide the prediction of pruning mask using the uncertainty-generated mask label. An example of the mask label is shown at the bottom of Figure~\ref{001_intro}(b).

In the second phase, we supervise the estimation of the pruning mask using the mask label.
In addition, we design a self-contrast training strategy for enhancing the performance of VFI model estimating ability.
Specifically, an auxiliary non-pruning branch in UGSP generates the features of intermediate frame to guide training through our self-contrast loss. 
In summary, our main contributions are summarized as follows:

% utilizes the pruning mask in the finest scale as the pruning mask for the whole model training.
% This is reasonable given that the feature pyramid is highly structured in the VFI model, and it makes the model further focus on the challenge regions to generate visually pleasing results in the case of resource limitation. In summary, our main contributions are summarized as follows:

\begin{itemize}
\item We propose the Uncertainty-Guided Spatial Pruning (UGSP) architecture for dynamically accelerating VFI by reducing redundant computation. To our knowledge, we are the first to incorporate uncertainty into spatial pruning networks with promising results.

% using uncertainty-generated pruning mask labels
\item 
We observe low uncertainty occurs in easy movement regions where redundant computations exists, so we propose using uncertainty-generated mask labels to guide our framework in properly locating easy regions.
In addition, we propose a self-contrast training strategy that guides UGSP training using the interpolated frame features generated by the auxiliary non-pruning branch.

\item 
% Our UGSP can reduce FLOPs by 33\% while maintaining comparable PSNR performance to backbone in Vimeo90K dataset \cite{vimeo}.
Our UGSP can reduce FLOPs by 34\%/52\%/30\% while maintaining PSNR performance to the baseline without pruning on Vimeo90K\cite{vimeo}/UCF101\cite{ucf101}/MiddleBury\cite{Baker2011} datasets.
The experimental results show that UGSP achieves the best performance with lower FLOPs compared to state-of-the-arts.
% The results of our benchmarks show that our proposed model not only maintains VFI accuracy but also benefits from at least 34\% FLOPs and 25\% CPU inference times reduction compared to state-of-the-arts.

\end{itemize}


\section{Related Work}
\label{sec:related_work}

% \noindent
% \textbf{Video Frame Interpolation.}
% VFI aims to estimate the motion between two input frames and then interpolate one or more intermediate frames. Recent efforts on VFI primarily utilize kernel-based and flow-based approaches.
% Kernel-based approaches are suggested to convolve across local patches [34, 35] but suffer from limited spatial range and are computationally expensive. 

% The flow-based VFI methods explicitly predict the optical flow to perform forward or backward warping for generating intermediate frames or features. 
% The above-discussed approaches need too much redundant computation and inference delay to be viable for resource-limited devices.

% In recent years, several works have proposed computationally efficient methods for addressing the issue of limited computing resources.
% IFRNet builds an efficient encoder-decoder-based network that requires no extra synthesis or refinement modules.
% M2M performs motion estimation only once to interpolate an arbitrary number of in-between frames using forward warping from several bidirectional flows.
% RIFE decreases the size of the model by utilizing the distillation knowledge technique.
% CDFI and MADA perform model pruning to remove redundant computations at a coarse-grained level. CDFI uses a static pruning method to reduce the same amount of computation at all locations, and MADA allocates computational resources for each sub-image.

% Then, utilizing optical flow estimates [2,3,44] and deformable convolutions to handle large motions has significantly improved model performance.
% The flow-based VFI methods have also achieved remarkable progress in recent years. 
% Hallucination-based methods directly hallucinate pixels, and most works [13, 46] utilize the deformable convolution to the feature of middle frames. However, they often produce blurry results when fast-moving objects are present.
% For generating visual pleasing results, some works introduce texture or refine network to further refine the generated target frame.

\noindent
\textbf{Efficient Video Frame Interpolation.}
VFI aims to estimate the motion between two input frames and then interpolate one or more intermediate frames. Recent efforts on VFI primarily utilizes kernel-based \cite{Niklaus_2017_CVPR,Niklaus_2017_ICCV,Cheng_Chen_2020,Lee_2020_CVPR,Reda_2018_ECCV,9501506,Peleg_2019_CVPR,Bao_2019_CVPR,8840983}, flow-based \cite{Niklaus_2018_CVPR,Niklaus_2020_CVPR,Park_2021_ICCV,Jiang_2018_CVPR,NEURIPS2019_d045c59a,10.1007/978-3-030-58583-9_7,Kim_Oh_Kim_2020}, and transformer-based \cite{Lu_2022_CVPR,Shi_2022_CVPR} approaches.
However, these approaches need too much redundant computation and inference delay to be viable for resource-limited devices.
In recent years, several works have proposed computationally efficient methods for addressing the issue of limited computing resources.
IFRNet \cite{Kong_2022_CVPR} builds an efficient encoder-decoder based network that requires no extra synthesis or refinement modules.
% M2M \cite{Hu_2022_CVPR} performs motion estimation only once to interpolate an arbitrary number of in-between frames using forward warping from several bidirectional flows.
RIFE \cite{huang2022rife} decreases the inference time by directly estimating the intermediate flows with much better speed.
% CDFI \cite{Ding_2021_CVPR} and MADA \cite{Choi_2021_ICCV} perform model pruning to remove redundant computations at a coarse-grained level. 
CDFI \cite{Ding_2021_CVPR} uses a static pruning method to reduce the same amount of computation at all locations, and MADA \cite{Choi_2021_ICCV} determines computational resources for each sub-image.

% Figure environment removed

\noindent
\textbf{Dynamic inference.}
Dynamic inference techniques \cite{9560049} can adapt the network structures during inference based on the input and consequently have advantageous properties such as efficiency, representation power, and interpretability.
Inference path selection \cite{Kong_2021_CVPR,Ding_2021_CVPR,Liu_2022_CVPR}, early stopping strategies \cite{10.1007/978-3-030-58517-4_17,pmlr-v70-bolukbasi17a,huang2018multi}, and adaptive skipping of redundant computation via sparse convolution \cite{Wang_2021_CVPR,Yang_2022_CVPR,10.1007/978-3-030-58452-8_31,Habibian_2021_CVPR,Parger_2022_CVPR} or explicitly skipping convolutions \cite{Wu_2018_CVPR,Mullapudi_2018_CVPR,2022arXiv220304845C} are related methods.
ClassSR \cite{Kong_2021_CVPR}, for instance, uses a classification network to identify the model channel number for each sub-image, while MADA \cite{Choi_2021_ICCV} identifies the model layer number and input scale for each sub-image.
However, due to the low resolution of the sub-image, the receptive field of convolution is restricted, and computing resources are roughly decreased at the patch level.
SMSR \cite{Wang_2021_CVPR} and QueryDet \cite{Yang_2022_CVPR} allocate computation resources for the important location using predicted masks and sparse convolution for the super-resolution and object detection tasks, respectively. However, their effectiveness is not demonstrated for VFI, and estimated masks are inaccurate as they are not supervised.

% ClassSR, SD, Quantization

% SMSR, QueryDet

% early stop

\noindent
\textbf{Uncertainty in Deep Learning.}
There are two primary types of uncertainty in deep learning models \cite{NIPS2017_2650d608}. Aleatoric uncertainty captures the noise inherent in observation data, and epistemic uncertainty accounts for the uncertainty of the model about its predictions. Uncertainty is widely used to increase the performance of deep-learning tasks such as face recognition \cite{Chang_2020_CVPR}, image classification \cite{NIPS2017_2650d608}, image segmentation \cite{7803544}, image denoising \cite{10096540}, video object segmentation \cite{Xu_Wang_Li_Lu_2022}, and super-resolution \cite{NEURIPS2021_88a19961,tmp}.
In the image super-resolution task, pixels with large certainty, such as texture and edge pixels, will be prioritized based on their importance to visual quality using uncertainty-driven loss \cite{NEURIPS2021_88a19961}.
However, uncertainty is rarely incorporated into dynamic networks, even though it can provide sparse priors. For example, low-uncertainty regions require lower computational demands, so convolutions can be skipped.

\section{Proposed Method}
\label{sec:proposed_framework}

% % Figure environment removed

% % Figure environment removed




\subsection{Observation}
\label{sec:ob}
We first illustrate our statistical observations regarding the VFI results of the Middlebury \cite{Baker2011} and Vimeo90K \cite{vimeo} training datasets.
These observations reveal the inherent sparsity of the VFI task and motivate us to design more effective VFI frameworks.
As illustrated in Figure~\ref{002_ob}(a), we use four observation models Model(1)$\sim$Model(4) that are created by eliminating 0$\sim$3 convolution blocks from the IFRnet \cite{Kong_2022_CVPR}. The convolution block accounts for the majority of the computational burden at each scale. Consequently, Model(1) has the least computing resources for each pixel location, followed by Model(2), Model(3), and Model(4).

We first calculate the pixel-level reconstruction errors between each model output and the ground truth, then rank the pixel errors from small to large, divide them evenly into five intervals based on the ranking, and aggregate them.
In Figure~\ref{002_ob}(b), we discover that although Model(4) has three more convolution blocks than Model(1), it only achieves 17 and 10 error reductions in the 0$\sim$40 interval compared to Model(1) for the Middlebury and Vimeo90K datasets.
However, Model(4) achieves a decrease error of 346 and 251 in the 80$\sim$100 interval for these two datasets.
Inspired by this observation, we should allocate most computing resources to pixel locations in the 80$\sim$100 interval, since the VFI for these locations can be significantly improved with more computational resources.

Figure~\ref{002_ob}(d-e) illustrates the difference between Model(4) and ground truth, Model(1) and ground truth.
It can be observed that both Model(1) and Model(4) can predict well for the static or simple movement regions, indicating redundant computations in these regions and intrinsic spatial sparse for VFI.
However, both models have trouble predicting large or complex movement regions, which are crucial for visual pleasure in the VFI task.
% In order to design an efficient VFI framework, it is essential to locate these regions precisely.
From a Bayesian perspective \cite{NIPS2017_2650d608,NEURIPS2021_88a19961}, the targeted pixels in the reconstructed challenging region have large uncertainty (variance) as shown in the Figure~\ref{002_ob}(f).
Based on the aforementioned observation and discussion, we propose using uncertainty-generated mask label to guide our VFI framework allocating more computational resources to these challenging region.


% Figure environment removed
\subsection{Overview of the Proposed Framework}
\label{sec:overview}

As illustrated in Figure~\ref{003_method_model}, our UGSP consists of two training phases.
In the first phase, we train an uncertainty estimation network (UEN). As shown in the upper part of Figure~\ref{003_method_model}(a), UEN can predict the uncertainty (variance) field $(U_0, U_1, U_2)$ for the unknown intermediate frame.
The uncertainty map is then used to generate pruning masks $(P_0^u, P_1^u, P_2^u$) as a guide that supervises the second phase's spatial pruning mask $(P_1, P_2, P_3$) estimation.
In the second phase, spatial pruning masks at higher resolution scales are estimated from lower resolution scales, as shown in the bottom part of Figure~\ref{003_method_model}(a). Then, we use the pruning masks to skip redundant computations using sparse convolution in our VFI network.

In this section, we detail the structure of UEN and VFI networks.
The backbone network structure in UEN is identical to that of the VFI network, but UEN removes the branch for estimating the pruning mask and adds a branch for uncertainty estimation. We provide its structural details in the Appendix~\ref{appendix:ugsp}.
Our VFI network is designed in a coarse-to-fine manner similarly to most VFI models \cite{Kong_2022_CVPR,huang2022rife, Park_2021_ICCV,Lee_2020_CVPR}.
% which simultaneously predicts intermediate features $\hat{\phi}_t^k$ and flow $F_{t \rightarrow 0,1}^k$.

% Figure environment removed

Specifically, as displayed in Figure~\ref{003_method_model}(a), it first downsamples two input frames $I_0$ and $I_1$ four times using a block of two 3×3 convolutions with strides 2 and 1 to obtain four level features $\phi_{0,1}^k, k \in\{0,1,2,3\}$.
Then, we predict the flow field $F_{t \rightarrow 0,1}^k$ and intermediate feature $\hat{\phi}_t^k$ for each scale level.
At the largest three scales, we use Sparse Block $(f_s^0, f_s^1, f_s^2)$ to skip redundant computations based on the spatial pruning mask $(P_1, P_2, P_3)$ which is estimated by Sparse Block $(f_s^1, f_s^2)$ and Conv Block $f_c^3$, respectively.
The details of Sparse Block $(f_s^0, f_s^1)$ and Conv Block $f_c^3$ is described in the Appendix~\ref{appendix:network}.
We provide the overview of the Sparse Block $f_s^2$ during training in Figure~\ref{003_method_model}(b).
We can see that we first concatenate the feature $\hat{\phi}_{t}^3$ from the Conv Block and the aligned feature produced by back warping using flow field $F_{t \rightarrow 0,1}^3$ and feature $\phi_{0,1}^2$.
Then we input it into five consecutive $3\times3$ convolutions and one $1\times1$ convolution to refine the feature. $P_3$ is used to skip the redundant computations in the $3\times3$ convolutions.
Finally, we generate flow field $F_{t \rightarrow 0,1}^2$, feature $\phi_{0,1}^2$ by using one bilinear up-sampling and generate pruning mask $P_2$ by using one $3\times3$ convolutions, one bilinear up-sampling and a Gumbel softmax.
$(\hat{\phi}_{t}^2, F_{t \rightarrow 0,1}^2, P_2)$ are used as input for next Sparse Block $f_s^1$.

% Figure environment removed

During training, as shown in Figure~\ref{003_method_model}(b), we achieve sparse convolution by using dotting product after each $3\times3$ convolutions.
The dotting product enables the backpropagation of gradients at all locations.
During inference, as shown in Figure~\ref{004_method_sparse}, we only allocate convolution operation on the important areas which we can index from the estimated pruning mask $P$. Specifically, we gather the select location according to the pruning mask, then we apply the $3\times3$ and $1\times1$ convolutions on them. After the $1\times1$ convolution, we scatter the feature back.
% and the overall procedures are summarized as follow:
% \begin{equation}
% \begin{aligned}
% {F_{t \rightarrow 0,1}^3, \hat{\phi}_t^3, P_3 } &=f_c^3 (\phi_{0,1}^3), \\
% F_{t \rightarrow 0,1}^k, \hat{\phi}_t^k, P_k &=f_s^k (F_{t \rightarrow 0,1}^{k+1}, \hat{\phi}_t^{k+1}, P_{k+1}, \hat{\phi}_{0,1}^k ), k \in\{1,2\}, \\
% % F_{t \rightarrow 0,1}^1, \hat{\phi}_t^1, P_1 &=f_s^1\left(F_{t \rightarrow 0,1}^2, \hat{\phi}_t^2, P_2, \hat{\phi}_{0,1}^1\right) \\
% F_{t \rightarrow 0,1}^0, M, R &=f_s^0 (F_{t \rightarrow 0,1}^1, \hat{\phi}_t^1, P_1, \hat{\phi}_{0,1}^0 ), \\
% {I}_t &= M \odot {I}_{0 \rightarrow t}+(1-M) \odot {I}_{1 \rightarrow t} + R,
% \end{aligned}
% \label{equ:overview}
% \end{equation}

% \noindent
% where $f_s^k$ and $f_c^3$ represent the sparse block for level $k$ and the convolution block for level 3. $\hat{\phi}_{0,1}^k$ and ${I}_{0,1 \rightarrow t}$ are the aligned features and frames obtained by inputting frame features $\phi_{0,1}^k$ or frames $I_{0,1}$ and flow fields $F_{t \rightarrow 0,1}^{k+1}$ for back warping. $\odot$ denotes element-wise multiplication.  
% $P_k$ is the estimated pruning mask that is the input of sparse block $f_s^{k-1}$ for skipping redundant computing operations. 
% In Figure~\ref{004_method_sparse}, it details the structure of sparse block $f_s^1$ and $f_s^2$ where $P^{k+1}$ is used to skip the redundant computations in the $3\times3$ and $1\times1$ convolutions.
% The structure of convolution block $f_c^3$ is identical to $f_s^1$ and $f_s^2$ except for using $P^{k+1}$. The sparse block $f_s^0$ consists of five $3\times3$ convolutions, one $1\times1$ convolution, and one deconvolution to output flow $F_{t \rightarrow 0,1}^0$, mask $M$, and residual $R$.
To control the pruning degree of UGSP, we utilize a sparse loss $\mathcal{L}_{s}$ on $P_k$:
\begin{equation}
 \begin{aligned}
% \mathcal{L}_{s}= \left\| \frac{1}{\sum_{k=1}^{4} (H_k \times W_k)} (\sum_{k=1}^{3}\sum_{h=1}^{H_k}\sum_{w=1}^{W_k} P_{k,h,w} + {H_4} \times {W_4}) - S_t  \right\|_{1},
\mathcal{L}_{s}= \left\| \frac{1}{\sum_{k=1}^{3} (H_k \times W_k)} (\sum_{k=1}^{3}\sum_{h=1}^{H_k}\sum_{w=1}^{W_k} P_{k,h,w}) - S_t  \right\|_{1},
\label{equ:s}
\end{aligned}
\end{equation}
where $H_k$ and $W_k$ is the height and width of the $P_k$, and $S_t$ is the target sparsity to control the FLOPs of our UGSP framework.
% $H_4$ and $W_4$ is the height and width of the feature $\phi_{0,1}^3$ that is unchanged during training, and $S_t$ is the target sparsity to control the FLOPs of our UGSP framework.

Similar to other VFI methods \cite{Niklaus_2018_CVPR,Niklaus_2020_CVPR,huang2022rife}, we use the image reconstruction loss to measure the difference between an interpolated frame $I_t$ and its corresponding ground truth $I_{gt}$:
\begin{equation}
 \begin{aligned}
\mathcal{L}_{rec}=  \left\| I_t - I_t^{gt}  \right\|_{1}.
\label{equ:u}
\end{aligned}
\end{equation}
Here, we use $\mathcal{L}_{1}$ between two laplacian pyramid representations of the output of the VFI network $I_t$ and the ground truth $I_t^{gt}$.
% The backbone of the UEN is identical to VFI network described above, except the absence of pruning mask prediction branch and the addition of uncertainty and VFI blocks for estimating the uncertainty field and the intermediate frame for levels $0,1$ and $2$.
% Uncertainty blocks ($f_u^0$, $f_u^1$ and $f_u^2$) first use one nearest up-sampling operation to maintain same resolution as the input frame. Then, they respectively estimate uncertainty using 2, 3, and 4 blocks of one $3\times3$ convolution and one ELU activation layer. VFI blocks ($f_v^1$ and $f_v^2$) respectively contain 1 and 2 blocks of one $3\times3$ convolution, one PRelu activation layer, and one deconvolution to estimate the middle frame. Therefore, the resolution of $U_k$ and $I_t^k$ is the same as the input frame $I_{0,1}$.


\subsection{Uncertainty-Guided Mask Prediction}
\label{sec:ugm}

In the previous study, large uncertainty can be used to identify semantically
and visually challenging pixels, such as object boundaries for semantic segmentation tasks \cite{NIPS2017_2650d608} or texture and edge pixels in super-resolution tasks \cite{NEURIPS2021_88a19961}.
In our VFI task, complex and large movement areas display large uncertainty, such as the ball movement areas in Figure~\ref{001_intro}(b).
Therefore, we propose estimating the pixel-by-pixel uncertainty field of the intermediate frame and using the uncertainty to guide the prediction of the pruning.
% Specifically, we skip the computation in the low uncertainty areas that contain static or simple movement, and allocate more computational resource for areas with high uncertainty.
% As shown in Figure~\ref{003_method_model}, we train a UEN to generate uncertainty (variance) filed  by 
Following previous works \cite{NIPS2017_2650d608,NEURIPS2021_88a19961}, we utilize the sparse uncertainty loss $\mathcal{L}_{s u}$ to estimate the uncertainty field as follows:
\begin{equation}
 \begin{aligned}
% \mathcal{L}_{S U}=\frac{1}{N} \sum_{i=1}^{N} \left( \exp \left(-\boldsymbol{s}_{i}\right)\left\|\boldsymbol{x}_{i}-f\left(\boldsymbol{y}_{i}\right)\right\|_{1}+2 \boldsymbol{s}_{i} \right)
\mathcal{L}_{s u}= \exp \left(-{U}\right)\left\|{I}_{t}-f\left({I}_{0},{I}_{1}\right)\right\|_{1}+2 {U},
\label{equ:su}
\end{aligned}
\end{equation}
where $f\left({I}_{0},{I}_{1}\right)$ and $U$ denote the learned mean and uncertainty (variance) of the intermediate frame, respectively. 



% We can observe that the variance is negatively and positively correlated in the first and second parts of $\mathcal{L}_{s u}$, respectively, which reduces the performance of estimating the intermediate frame (mean).
As shown in Figure~\ref{003_method_model}, we first train a UEN to estimate the uncertainty field and then explicitly guide the spatial pruning mask estimation using the uncertainty in our VFI network.
Specifically, we use $\mathcal{L}_{s u}$ to estimate the variance $(U_0, U_1, U_2)$ for each scale, and the resolution of $U_k$ is the same as the input frame $I_{0,1}$. We provide the structural details of UEN in the Appendix~\ref{appendix:network}.
Then we generate the pruning mask label based on the uncertainty for 0,1 and 2 levels of the VFI network, which can be expressed as:
\begin{equation}
\begin{aligned}
T_k&=\operatorname{sort}\left(U_{k-1}\right)\left[\alpha_k\% \times H \times W \right], \\
P_{k, i, j}^u&= \begin{cases}1, & U_{k-1, i, j} > T_k \\
0, & \text { otherwise, }\end{cases}
\end{aligned}
\label{equ:label}
\end{equation}
where $k \in (1,2,3)$.  In Equation~(\ref{equ:label}), we first sort the value in $U_k$ from small to large and then assign threshold $T_k$ to the $\alpha_k\%$ smallest value of $U_{k-1}$. $H$ and $W$ are the height and width of $I_{0,1}$. Then we generate the pruning mask label $P_k^u$ with the ($i,j$) location set to 1 if $U_{k-1,i,j}$ is larger than the threshold $T_k$ and 0 otherwise. 
Value 0 in $P_k$ indicates the location where convolution will be skipped.
Finally, an uncertainty-guide mask loss $\mathcal{L}_{u g m}$ is imposed on the prediction of pruning mask $P_k$ in the VFI network to properly skip redundant computation as follow:
\begin{equation}
 \begin{aligned}
\mathcal{L}_{u g m}=  \left\| \left({P}_{k}^{u}\right)\downarrow_{2^{k+1}}-{P}_{k+1} \right\|_{1},
\label{equ:ugm}
\end{aligned}
\end{equation}
where ${P}_{k}^{u}$ and ${P}_{k+1} (k=0,1,2)$ stand for the guided pruning mask from the UEN and the estimated pruning mask from VFI network. $\downarrow_{2^{k+1}}$ denotes that we down-sample the ${P}_{k}^{u}$ by a factor of $2^{k+1}$ to keep the resolution consistently.

% \subsection{Uncertainty Image Reconstruction Loss}
% \label{sec:u}
% Large uncertainty areas account for the large and complex movement areas, which are crucial for the visual quality of the predicted intermediate frame. We prioritize pixels with large uncertainty in the reconstruction loss as follows:
% \begin{equation}
%  \begin{aligned}
% \mathcal{L}_{u}=  U_0 \left\| I_t - I_t^{gt}  \right\|_{1},
% \label{equ:u}
% \end{aligned}
% \end{equation}
% where $U_0$ is the uncertainty map generated by the UEN network in the first phase. Here, we  use $\mathcal{L}_{1}$ between two laplacian pyramid representations of the output of the VFI network $I_t$ and the ground truth $I_t^{gt}$ \cite{Niklaus_2018_CVPR,Niklaus_2020_CVPR,huang2022rife}.


\begin{table*}[t]
    \centering
    % \resizebox{\textwidth}{!}
    % \resizebox{.9\textwidth}{!}
    \resizebox{\linewidth}{!}
{
    \begin{tabular}{lcccllcllcllc}
    \toprule
    \multirow{2}{*}{ Model} & \multirow{2}{*}{Pruning}  & \multirow{2}{*}{$\mathcal{L}_{ugm}$} & \multirow{2}{*}{$\mathcal{L}_{sc}$} & \multicolumn{3}{c}{Vimeo90K} & \multicolumn{3}{c}{UCF101} & \multicolumn{3}{c}{MiddleBury} \\
    \cmidrule(r){5-7} \cmidrule(r){8-10} \cmidrule(r){11-13} & & & & Time(s) & FLOPs(G) & PSNR & Time(s) & FLOPs(G) & PSNR & Time(s) & FLOPs(G) & PSNR \\
    \hline
    Baseline & - & - & - & 1.16 & 31.7 & 35.74 & 0.70 & 18.1 & 35.30 & 2.99 & 85.0 & 37.47 \\
    UGSP-P & \Checkmark & - & - & 0.44(-62\%) & 15.7(-50\%) & 35.45 & 0.26(-63\%) & 9.0(-50\%) & 35.29 & 1.12(-63\%) & 38.3(-55\%) & 36.48  \\
    UGSP-C & \Checkmark & - & \Checkmark & 0.47(-59\%) & 16.3(-49\%) & 35.61 & 0.25(-64\%) & 8.6(-52\%) & 35.28 & 1.10(-63\%) & 39.8(-53\%) & 36.90  \\
    % UGSP-M & \Checkmark &  \Checkmark & - & - & 0.46s & -59.3\% & 124.65G & -51.1\% & 35.59 & 0.9771 \\
    UGSP-M & \Checkmark &  \Checkmark & - & 0.45(-61\%) & 15.5(-51\%) & \textbf{\textcolor{red}{35.63}} & 0.26(-63\%) & 8.7(-52\%) & 35.30 & 1.14(-62\%) & 40.7(-52\%) & 36.83 \\
    % UGSP-U & \Checkmark &  \Checkmark & \Checkmark & - & 0.45s & -60.2\% & 125.22G & -50.9\% & 35.61 & 0.9772  \\
    % UGSP-U & \Checkmark &  \Checkmark & \Checkmark & - & s & \% & 124.96G & -51.0\% & 35.62 &  0.9775 \\
    % UGSP-C & \Checkmark &  \Checkmark & - & \Checkmark  & 0.47s & -58.4\% & 126.21G & -50.5\% & 35.62 & 0.9771 \\
    UGSP & \Checkmark &  \Checkmark & \Checkmark  & 0.45(-61\%) & 15.6(-51\%) & 35.62 & 0.26(-63\%) & 8.6(-52\%) & \textbf{\textcolor{red}{35.31}} & 1.10(-63\%) & 39.3(-54\%) & \textbf{\textcolor{red}{36.96}} \\
    \hline
    Baseline & - & - & - & 1.16 & 31.7 & 35.74 & 0.70 & 18.1 & 35.30 & 2.99 & 85.0 & 37.47 \\
    % UGSP & \Checkmark & \Checkmark & \Checkmark & \Checkmark & 0.46s & -59.3\% & 126.09G & -50.6\% & 35.63 & 0.9772 \\
    % UGSP & \Checkmark & \Checkmark & \Checkmark & \Checkmark & s & \% & 126.02G & -50.6\% & 35.65 & 0.9775 \\
    % UGSP-large & \Checkmark & \Checkmark & \Checkmark & (-\%) & 20.9(-\%) & 35.71 & 11.9 & 35.28 & 55.5 & 37.33  \\
    % \hline
    % UGSP-large & \Checkmark & \Checkmark & \Checkmark & 0.59(-49\%) & 21.0(-34\%) & 35.72 & 0.36(-49\%) & 12.4(-31\%) & 35.31 & 1.58(-47\%) & 59.2(-30\%) & 37.46  \\
    % UGSP-65 & \Checkmark & \Checkmark & \Checkmark & (-\%) & 24.7(-22\%) & 35.73 & &  14.0 & 35.32 & & 66.2 & 37.27  \\
    % UGSP-45 & \Checkmark & \Checkmark & \Checkmark & (-\%) & 18.4(-42\%) & 35.67 & & 10.3 & 35.29 & & 48.7 & 37.11  \\
    % UGSP-25 & \Checkmark & \Checkmark & \Checkmark & (-\%) & 12.8(-42\%) & 35.52 & & 6.8 & 35.29 & & 32.6 & 36.60  \\
    % UGSP-25-progressive & \Checkmark & \Checkmark & \Checkmark & (-\%) & 12.8(-42\%) & 35.52 & 6.6 & 35.28 & 32.4 & 36.72  \\
    % 20.96 35.72 20.9(-\%) & 35.71
    UGSP-large & \Checkmark & \Checkmark & \Checkmark & \textbf{\textcolor{red}{0.59(-49\%)}} & \textbf{\textcolor{red}{21.0(-34\%)}} & 35.72 & \textbf{\textcolor{red}{0.36(-49\%)}} & \textbf{\textcolor{red}{12.4(-31\%)}} & 35.31 & \textbf{\textcolor{red}{1.58(-47\%)}} & \textbf{\textcolor{red}{59.2(-30\%)}} & 37.46  \\
    \hline
    \end{tabular}
}
    \caption{ Quantitative comparison of the ablation study. We increase $S_t$ in sparse loss $L_s$ to train and achieve UGSP-large.  
    Time and FLOPs denote inference time and FLOPs percentage reduction compared to `Baseline'. While maintaining the performance, UGSP-large reduces 34\%/30\% FLOPs and 49\%/47\% Time on the Vimeo90K/MiddleBury datasets and UGSP reduces 52\% FLOPs and 63\% Time on the UCF101 dataset.
    }
    \label{tab:ablation-loss}
\end{table*}

\subsection{Self-Contrast Training Strategy}
\label{sec:sc}
 % auxiliary non-pruning branch
To further enhance the estimation capacity of the model, we use the auxiliary non-pruning branch, which does not utilize pruning mask to skip computation and output the intermediate frame $I_t^{sc}$ and feature $\hat{\phi}_{t}^{sc,k}$. The auxiliary non-pruning branch shares the same weight with UGSP in each level. Then we propose a self-contrast loss $\mathcal{L}_{s c}$, which can be expressed as:
\begin{equation}
 \begin{aligned}
\mathcal{L}_{sc}= \mathcal{L}_{rec} \left(I_t^{sc}, I_t^{gt}  \right) + \sum_{i=1}^{3} \mathcal{L}_{cen} \left(\hat{\phi}_{t}^{sc,k}, \hat{\phi}_{t}^{k}  \right) ,
\label{equ:sc}
\end{aligned}
\end{equation}
where $I_t^{sc}$ and $\hat{\phi}_{t}^{sc,k}$ are the intermediate frame and features outputted from the auxiliary non-pruning branch. $\mathcal{L}_{cen}$ is the census loss \cite{Meister_Hur_Roth_2018}, which computes the soft Hamming distance for the intermediate features.
$\mathcal{L}_{s c}$ has two benefits for UGSP training. 
First, as UGSP skips the redundant computation areas by multiplying the features with pruning mask during training, the backward gradient propagation in the skipped areas is close to zero. To address this problem, the first part of $\mathcal{L}_{sc}$ applies $\mathcal{L}_{rec}$ to $I_t^{sc}$ and $I_t^{gt}$, producing a non-zero gradient in these regions. Second, analogous to the idea of knowledge distillation \cite{44873}, the intermediate feature $\hat{\phi}_{t}^{sc,k}$ generated from the model without pruning (teacher), can be regarded as a soft label that transfers knowledge to facilitate the performance of UGSP (student) by comparing the difference with $\hat{\phi}_{t}^{k}$.



\subsection{Overall Loss Function}
\label{sec:loss}
As described in Section~\ref{sec:ugm}, the loss function is sparse uncertainty loss $\mathcal{L}_{s u}$ (Equation~(\ref{equ:su})) for level 0, 1, and 2 in the first phase.
For the second phase, we summarize the loss described above as follows:
\begin{equation}
        \mathcal{L}_{{overall }}=\mathcal{L}_{{r e c }} +\lambda_{s}\mathcal{L}_{s} +\lambda_{u g m} \mathcal{L}_{u g m}+\lambda_{{s c}} \mathcal{L}_{{s c}}.
\label{equ:overall}
\end{equation}
$\lambda_{s}$, $\lambda_{u g m}$, and $\lambda_{{s c}}$ are the weight for $\mathcal{L}_{s}$, $\mathcal{L}_{u g m}$, and $\mathcal{L}_{{s c}}$.

\subsection{Relationships with Related Work}
\label{sec:rrw}
Many state-of-the-art efficient VFI methods, such as RIFE \cite{huang2022rife} and IFRnet \cite{Kong_2022_CVPR}, aim to design novel losses or apply the knowledge distillation strategy. By contrast, we propose another new strategy to achieve efficient VFI by designing a spatial pruning-friendly architecture and specific spatial pruning loss functions.
What is more, our architecture UGSP is compatible with previous methods.
Therefore, we also implement the flow distillation method and geometry consistency loss of IFRnet into our UGSP as UGSP-\textit{distill} and implement the priveleged distillation and refinement network of RIFE into UGSP as UGSP-\textit{refine}. We provide the implementation details in the Appendix~\ref{appendix:ugsp}.
The compared experiments in Section~\ref{sec:compare} shows that UGSP-\textit{distill} and UGSP-\textit{refine} can further improve the performance.


\section{Experiments}

This section begins by introducing benchmarks and evaluation metrics. Then, we analyze our UGSD framework to demonstrate the proposed design. Finally, we quantitatively and qualitatively compare UGSP with state-of-the-arts on the benchmark, and show the high compatibility of our framework. We introduce the implementation detail in the Appendix~\ref{appendix:implementation}.

% Our UGSP implements sparse convolution the same as SMSR.


\subsection{Benchmarks and Evaluation Metrics}
Our UGSP is trained on the Vimeo90K training dataset and evaluated on the Vimeo90K testing datasets, UCF 101, and Middlebury Other datasets.

\noindent
\textbf{Vimeo90k \cite{vimeo}:} A widely used dataset that has 51312 and 3782 triplets of size 256×448 for training and testing.

\noindent
\textbf{UCF101 \cite{ucf101}:} The DVF-selected \cite{Liu_2017_ICCV} test set having 379 triplets with a resolution of 256×256, is used to evaluate our algorithm.

\noindent
\textbf{Middlebury \cite{Baker2011}:} It is a widely dataset used for optical flow and VFI tasks. Other set is used for testing, and its image resolution is around 640×480.

We measure the peak signal-to-noise ratio (PSNR) for quantitative evaluation.
Each method traverses the three testing datasets on an Intel I5-10400K CPU to measure the CPU inference speed.
Additionally, we calculate floating point operations (FLOPs) to determine the computational complexity for each datasets.


\subsection{Method Analysis}
\label{sec:abaltion_study}


We conducted several experiments on the Vimeo90K, UCF101, and MiddleBury testing datasets to verify the effectiveness of the proposed approaches. 
% We measure the CPU runtimes, FLOPs, PSNR, and SSIM evaluation metrics for each variant model.
Only $\mathcal{L}_{1}$ distance image reconstruction loss $\mathcal{L}_{r e c}$ (Equation~\ref{equ:u}) is used to train the `Baseline' network, and the result is shown in Table~\ref{tab:ablation-loss}. 
We adjust sparsity loss $\mathcal{L}_{s}$ to ensure that FLOPs is nearly equal across all variant models.



\noindent
\textbf{Ablation of Uncertainty-Generated Mask Label $(\mathcal{L}_{u g m})$.}
We first conduct experiments to demonstrate the effectiveness of the uncertainty-generated pruning mask.
We prune the baseline model using sparsity loss $\mathcal{L}_{s}$ but remove our designed loss ($\mathcal{L}_{ugm}$ and $\mathcal{L}_{sc}$), and name it UGSP-P.
Then we develop UGSP-M, which uses the uncertainty-generated mask label from UEN to guide the estimation of pruning mask in the VFI network through $\mathcal{L}_{ugm}$.
Table~\ref{tab:ablation-loss} shows that UGSP-M has superior PSNR than UGSP-P by 0.18dB and 0.35dB on the Vimeo90K and MiddleBury datasets, respectively.

% Figure environment removed

% Figure environment removed

Additionally, we conducted a qualitative ablation study, as shown in Figure~\ref{005_ablation_ugm_sc}(a). Figure~\ref{005_ablation_ugm_sc}(a) presents the estimated pruning masks ($P_1, P_2$) and uncertainty-generated mask labels ($P_0^u, P_1^u$).
We observe that the white areas requiring convolution are more concentrated in UGSP-M and `Mask Label' than UGSP-P. This is advantageous to large and complex motion estimation because more contextual information of motion is included. In addition, the scatter and small white regions in UGSP-P increase the number of unnecessary calculations since the receptive field is too limited to capture the motion.
In short, using the uncertainty-generated mask label, the performance of our model can significantly enhance.

\begin{table*}[tb]
    \centering
    \resizebox{.8\linewidth}{!}
{
    \begin{tabular}{lccccccccc}
    \toprule
   \multirow{2}{*}{ Model} &  \multicolumn{3}{c}{Vimeo90K} & \multicolumn{3}{c}{UCF101} &  \multicolumn{3}{c}{Middlebury} \\
     \cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10} & Time(s) & FLOPs(G) & PSNR & Time(s) & FLOPs(G) & PSNR & Time(s) &  FLOPs(G) & PSNR \\
    \hline
    SepConv \cite{Niklaus_2017_ICCV} & - & 44.00 & 33.79 & - & 25.14 & 34.78 & - & 117.87 & 35.85 \\
    CAIN \cite{Choi_Kim_Han_Xu_Lee_2020} & 2.80 & 171.98 & 34.65 & 1.38 & 85.99 & 34.98 & 6.72 & 429.95 & 35.08 \\
    % Superslomo & 19.8 & & & 34.64 & 0.9740 & 35.15 & 0.9680\\
    AdaCof \cite{Lee_2020_CVPR} & - & 43.73 & 34.38 & - & 24.99 & 35.20 & - & 117.13 & 35.74  \\
    EDSC \cite{9501506} & - & 31.55 & 34.84 & - & 18.03 & 35.13 & - & 84.52 & 36.80  \\
    BMBC \cite{10.1007/978-3-030-58568-6_7} & 20.51 & 305.33 & 35.01 & 12.35 & 174.47 & 35.15 & 47.88 & 817.84 & 36.79 \\
    CDFI \cite{Ding_2021_CVPR} & - & 100.26 & 35.17 & - & 57.29 & 35.21 & - & 268.56 & 37.14\\
    IFRnet \cite{Kong_2022_CVPR} & 0.62 & \textcolor{blue}{\underline{15.19}} & 35.59 & 0.38 & 8.68 & 35.28 & 1.56 & 40.68 &  37.21  \\
    RIFE \cite{huang2022rife} & 0.66 & 20.45 & \textcolor{blue}{\underline{35.62}} & 0.39 & 11.68 & 35.28  & 1.70 & 54.77 & \textcolor{blue}{\underline{37.29}}  \\
    \hline
    % UGSP &  15.70 & 35.63 & 0.9772 & 7.57 & 35.28 & 0.9689 & 38.76  & 36.89 & 0.9829 \\
    % UGSP &  15.60 & 35.62 & 0.9773 & 8.54 & 35.30 & 0.9689 & 41.44  & 37.11 & 0.9833 \\
    % Baseline & 31.74 & 35.74  & 18.14 & 35.30  & 85.02 & 37.47  \\
    UGSP & \textcolor{blue}{\underline{0.46}} & 15.60 & \textcolor{blue}{\underline{35.62}} & \textcolor{blue}{\underline{0.26}} & \textcolor{blue}{\underline{8.54}} & \textcolor{blue}{\underline{35.31}} & \textcolor{blue}{\underline{1.10}} & \textbf{\textcolor{red}{39.29}}  & 36.96 \\
    % UGSP-P & 15.70 & 35.45  & 9.04 & 35.29  & 38.27 & 36.48  \\
    % UGSP-M &  15.52 & 35.63  & 8.68 & 35.30  & 40.68  & 36.83  \\
    % UGSP-RIFE & 8.9 & 0.58 & 0.0136 & 149.78 & 35.67 & 0.9778 & \textbf{\textcolor{red}{35.28}} & 0.9689 & \textbf{\textcolor{red}{37.29}} & 0.9837  \\
    % UGSP-RIFE & 20.22 & 35.67 & 0.9777 & 10.83 & \textbf{35.29} & 0.9690 & 51.63 & \textbf{37.29} & 0.9845  \\
    % UGSP+IFRnet & 6.8 & 0.47/0.0090 & 129.97 & 35.75 & 0.9779 & 35.27 & 0.9688 & 36.98 & 0.9827 \\
    % UGSP-IFRnet &  & \textbf{35.71} & 0.9776 & & 35.27 & 0.9688 & & 36.92 & 0.9824 \\
    % UGSP-IFRnet & 122.98 & \textbf{35.70} & 0.9776 & 35.28 & 0.9689 & 37.03 & 0.9827 \\
    % 0.0119/0.44  & & 35.28 & 0.9689& 37.03 & 0.9827\\
    UGSP-\textit{distill} & \textbf{\textcolor{red}{0.43}} & \textbf{\textcolor{red}{14.72}} & \textbf{\textcolor{red}{35.65}} & \textbf{\textcolor{red}{0.23}} & \textbf{\textcolor{red}{8.00}} & 35.27 & \textbf{\textcolor{red}{1.04}} & \textcolor{blue}{\underline{40.48}} & 36.98  \\
    UGSP-\textit{refine} & 0.59 & 19.73 & \textcolor{blue}{\underline{35.62}} & 0.35 & 11.25 & \textbf{\textcolor{red}{35.33}} & 1.52 & 53.12 & \textbf{\textcolor{red}{37.31}}  \\
    % UGSP-IFRnet & \textbf{14.41} & \textbf{35.64}  & \textbf{8.00} & 35.27  & \textbf{35.94} & 36.78  \\
    % UGSP-IFRnet & \textbf{14.72} & \textbf{35.65}  & \textbf{8.00} & 35.27  & \textbf{35.94} & 36.78  \\

    % UGSP: 
    % MB: /home/user3/EfficientVFI/ECCV2022-RIFE-33-sparse-l1-wstep-contrast-feat-finetune/train_log/flownet_44.pkl
    % UCF: 284 
    % Vimeo90K: 284

    % UGSP-refine: 
    % MB: 299
    % UCF:  284 
    % Vimeo90K: 284

    % UGSP-distill: 
    % MB: 299
    % UCF: train_log_32 299
    % Vimeo90K: train_log_35 299
    
    % \hline
    % \hline
    % ABME & 18.1 & - & 0.0977 & 1707.37 & 36.18 & 0.9805 & 35.38 & 0.9698 & 37.68 & 0.9856 \\
    % % RIFE large & & & & 36.19 & 0.9810 & 35.41 & 0.9700 \\
    % IFRnet large & 19.7 & 2.69 &  & 817.17 & 36.20 & 0.9808 & 35.42 & 0.9698 & 38.08 & 0.9872 \\
    % \hline
    \hline
    \end{tabular}
}
    \caption{ 
 Quantitative comparison.
 For each item, best results are \textbf{\textcolor{red}{bold}}, and the second best is \textcolor{blue}{\underline{underlined}}. The symbol '-' indicates that the method is not testable on the CPU due to the absence of its implementation in the CPU-oriented version.
}
    \label{tab:compare}
\end{table*}

\noindent
\textbf{Ablation of $\mathcal{L}_{sc}$.}
As shown in Table~\ref{tab:ablation-loss}, using our $\mathcal{L}_{sc}$, our UGSP-C performs better than UGSP-P, and our UGSP can increase PSNR by 0.13dB compared to UGSP-M on the MiddleBury dataset. UGSP achieves a reduction of 52\% in FLOPs and 63\% in inference time while maintaining performance on the UCF101 dataset. Additionally, UGSP-large reduces FLOPs by 34\%/30\% and inference time by 49\%/47\%, while keeping  a comparable PSNR performance to the 'Baseline' model (35.72 vs. 35.74 for Vimeo90K, and 37.46 vs. 37.47 for MiddleBury).
Our proposed self-contrast training strategy ($\mathcal{L}_{sc}$) guides UGSP training using the output of non-pruning branch.
As shown in Figure~\ref{005_ablation_ugm_sc}(b), the intermediate feature of UGSP reveals high activation in the motion areas, which is vital to a pleasing visual experience.



%  \begin{table}
%     \centering
%     \resizebox{\linewidth}{!}
% {
%     \begin{tabular}{lcccccc}
%     % \topru.le
%     \hline
%     Model &  Params. & Runtime  & FLOPs  & PSNR & SSIM \\
%     \hline
%     UGSP & 6.8M  & 0.46s  & 126.09G & 35.63 & 0.9772\\
%     % \hline
%     % IFRnet & 2.8M  & 0.63s  & 122.04G & 35.59 & 0.9786\\
%     % UGSP-IFRnet & 6.8M & 0.47s & 129.97G  & 35.75 & 0.9779 \\
%     UGSP-IFRnet & 6.8M & 0.46s & 122.87G  & \textbf{35.71} & 0.9776 \\
%     % \hline
%     % RIFE & 10.1M & 0.65s  & 164.31G & 35.62 & 0.9780 \\
%     UGSP-RIFE & 8.9M & 0.58s & 159.60G & \textbf{35.67} & 0.9777\\
%     \hline
%     \hline
%     \end{tabular}
    
% }
%     \caption{Analysis of the UGSP compatibility.}
%     \label{tab:extend}
% \end{table}





\noindent
\textbf{Controllable Computational Complexity.}
We can easily control the FLOPs for UGSP by setting different target sparsity hyperparameter $S_t$ in the $\mathcal{L}_{s}$ (Equation~\ref{equ:s}) to train the model.
As presented in Figure~\ref{006_ablation_backbone}, we change $S_t$ to achieve UGSP with different FLOPs and compare this to the `Channel Reduction'. `Channel Reduction' indicates that we scale feature channels in `Baseline' to reduce FLOPs.
We find that UGSP has superior PSNR performance than `Channel Reduction', since UGSP allocates more computation resources to challenging areas, which can significantly improve results as discussed in Section~\ref{sec:ob}.

 We provide a qualitative understanding of the effects of estimated pruing mask in Figure~\ref{006_ablation_rebuttal}. It shows that when the computational complexity around the hands (red regions) increases, it predicts hand motion more accurately. In addition, it can be observed that the allocation of computational resources is concentrated in the regions associated with motion, which is crucial to obtain pleasing visual results.


% (a) compared to baseline, UGDP only decreases the performance of 0.01dB (35.73 vs. 35.74) but reduce the FLOPs by 33\%, and (b)
% by allocation more computation resource to challenging areas as discussed in Section~\ref{sec:ob}, UGSP has superior PSNR performance than `Channel Reduction'.
% Furthermore, as shown in Table~\ref{tab:ablation-extent}, UGSP obtains leading results on the PSNR by 0.01dB and 0.02dB to RIFE and IFRnet, but only contains 23.3\% FLOPs reduction for RIFE and 25.8\% times reduction for IFRnet.




% \begin{table}[t]
%     \centering
%     % \resizebox{\textwidth}{!}
%     % \resizebox{.9\textwidth}{!}
%     \resizebox{.85\linewidth}{!}
% {
%     \begin{tabular}{lccccc}
%     \hline
%     Model & Time & FLOPs & PSNR & SSIM \\
%     \hline
%     CAIN \cite{Choi_Kim_Han_Xu_Lee_2020} & 2.80s & 171.98G  & 34.65 & 0.9730 \\
%     BMBC \cite{10.1007/978-3-030-58568-6_7} &  20.51s & 305.33G  & 35.01 & 0.9760 \\
%     \hline
%     UGSP & 0.46s & 15.60G & 35.62 & 0.9772 \\
%     IFRnet \cite{Kong_2022_CVPR} & 0.62s & 15.19G  & 35.59 & 0.9786 \\
%     UGSP-IFRnet & 0.43s & 14.72G  & 35.65 & 0.9776 \\
%     RIFE \cite{huang2022rife} & 0.66s & 20.45G  & 35.62 & 0.9780 \\
%     UGSP-RIFE & 0.59s & 19.73G  & 35.62 & 0.9777 \\
%     \hline
%     \end{tabular}
% }
%     \caption{ Analysis of the UGSP compatibility and comparison of inference times on the Vimeo90K dataset. }
%     \label{tab:ablation-extent}
% \end{table}



% Figure environment removed



\subsection{Comparison with the State-of-the-Arts}
\label{sec:compare}
Our UGSP, UGSP-\textit{distill}, and UGSP-\textit{refine} are compared to 8 state-of-the-art methods, including SepConv \cite{Niklaus_2017_ICCV}, CAIN \cite{Choi_Kim_Han_Xu_Lee_2020}, AdaCof \cite{Lee_2020_CVPR}, EDSC \cite{9501506}, BMBC \cite{10.1007/978-3-030-58568-6_7}, CDFI \cite{Ding_2021_CVPR}, RIFE \cite{huang2022rife}, and IFRnet \cite{Kong_2022_CVPR}.

\noindent
\textbf{Compatibility of UGSP.}
Other VFI methods can be applied to UGSP as we discussed in Section~\ref{sec:rrw}.
We implement the flow distillation and geometry consistency loss of IFRnet \cite{Kong_2022_CVPR} into UGSP as UGSP-\textit{distill}, and we add refinement network and priveleged distillation strategy of RIFE \cite{huang2022rife} into UGSP as UGSP-\textit{refine}. As shown in Table~\ref{tab:compare}, UGSP-\textit{distill} performs superior to UGSP and IFRnet by 0.03dB and 0.06dB PSNR but achieves 0.88G and 0.41G lower FLOPs on the Vimeo90K dataset.
Moreover, UGSP-\textit{distill} outperforms IFRnet in significantly reducing inference time.
The UGSP-\textit{refine} achieves the best PSNR on the UCF101 and MiddleBury compared to UGSP and RIFE with lower inference time.

% We use the numerical results from the IFRnet and RIFE paper, and we execute the released codes to get the missing results.
%  Due to the irregular and fragmented memory patterns, sparse convolution cannot make
% full use of the characteristics of general GPUs (e.g., memory coalescing) and relies on specialized designs to improve memory locality and cache hit rate for acceleration


% \cmidrule(r){12-15} IFRnet & 4.96  & 0.92  & 211.53 & 35.80/0.9794 & 35.29/0.9693  & 1.95 \\


% & CDFI & RIFE & IFRnet & UGSP & UGSP-RIFE & UGSP-IFRnet


\noindent
\textbf{Quantitative Results.}
Table~\ref{tab:compare} shows that our UGSP, UGSP-\textit{refine}, and UGSP-\textit{distill} outperform the state-of-the-art methods on most datasets.
For example, our UGSP-\textit{distill} achieves the best 35.65dB PSNR values for the Vimeo90K dataset with lower FLOPs.
In addition, despite other methods, such as RIFE, CDFI, and BMBC, having larger FLOPs than UGSP-\textit{refine}, our UGSP-\textit{refine} performs favorably against them on all benchmarks.
Furthermore, without combing our UGSP with RIFE or IFRnet, UGSP has already achieved the highest performance on Vimeo90K and UCF101 datasets with minimal FLOPs.

\noindent
\textbf{Qualitative Results.}
The visual results shown in Figure~\ref{006_compare} reveal that our framework can interpolate large and complex regions.
For example, our UGSP produces a sharp yellow ball in the first examples in Figure~\ref{006_compare}, and UGSP-\textit{distill} maintains the structure of the ball in the last example.
In short, as our approaches guide the computation reducing only in the simple and static regions, we can maintain the high-quality in challenging movement regions, which is important for the pleasing visual experience.


\section{Conclusion}
In this paper, we propose the UGSP framework for video frame interpolation that aims to skip redundant computation using uncertainty as a guide.
We observe that low uncertainty regions appear in the easy regions, such as small-scale motion, which can be skipped to increase inference speed.
Therefore, a UEN in our UGSP framework first estimates uncertainty. Then VFI network uses uncertainty to guide the pruning mask estimation through proposed uncertainty-guided mask loss, which increases a significant PSNR value compared to when uncertainty is not used.
In addition, by including the proposed self-contrast training strategy, our UGSP can effectively reduce computational cost while maintaining comparable performance to baseline.
UGSP achieves 34\%/52\%/30\% FLOPs and 49\%/49\%/47\% inference time compared to baseline, but maintains the PSNR performance on Vimeo90K/UCF101/MiddleBury datasets.
What is more, other efficient VFI network can be integrated into our UGSP. 
Extensive experiments demonstrate that our network also achieve state-of-the-art performance with large FLOPs reduction and a speedup on CPU devices.



%%%%%%%%% REFERENCES
\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{samples/egbib}

\appendix

\section{Implementation Details}
\label{appendix:implementation}
Our UGSP consists of two training phases. In the first phase, we train the UEN network to predict the uncertainty, and in the second phase, we use the uncertainty to guide the training of the VFI network.
We choose the AdamW \cite{Loshchilov2019DecoupledWD} optimizer for 100 and 300 epochs to train the UEN and VFI networks.
UGSP is implemented with Pytorch and trained on four NVIDIA GeForce RTX 3090 GPUs with a batch size of 32 and a patch size of 224$\times$224. 
The learning rate decays from 1e-4 to 1e-5 through cosine annealing.
We set $\alpha_1\%$, $\alpha_2\%$, and $\alpha_3\%$ to 20\%, 40\%, and 80\%, respectively, to generate pruning mask label in Equation~\ref{equ:label} when the target sparsity $S_t$ is 35\% in the sparse loss (Equation~\ref{equ:s}).
The weights $\lambda_{s}$, $\lambda_{u g m}$, and $\lambda_{{s c}}$ in the overall loss (Equation~\ref{equ:overall}) are set to 0.01.
We used the sparse convolution implemented by Wang \emph{et al.} \cite{Wang_2021_CVPR}.

\section{Limitation}
Currently, the decreases in FLOPs do not bring faster GPU inference times, as shown in Table~\ref{tab:limitation}.
As discussed in Wang \emph{et al.} \cite{Wang_2021_CVPR}, because of the irregular and fragmented memory patterns, sparse convolution in UGSP cannot fully exploit GPUs without specific optimization.
Therefore, we intend to explore sparse convolution friendly optimizations in the future.


\begin{table}[h]
    % \footnotesize
    \small
    % \resizebox{\textwidth}{!}
    % \resizebox{.9\textwidth}{!}
%     \resizebox{\linewidth}{!}
% {
    \begin{tabular}{lccc}

    \hline
    %  & Adacof \cite{Lee_2020_CVPR} & EDSC \cite{9501506} & CDFI \cite{Ding_2021_CVPR} & RIFE \cite{huang2022rife} & IFRnet \cite{Kong_2022_CVPR}
    % & UGSP \\
    & Baseline & UGSP-large
    & UGSP \\
    \hline
    FLOPs & 31.7G & 21.0G & 15.6G\\
    CPU &  1.16s & 0.59s & 0.45s \\
    GPU &  9.4ms & 14.2ms & 14.0ms \\
    \hline
    \end{tabular}
% }
    \caption{ Comparison of FLOPs and GPU time on an NVIDIA GeForce RTX 3090 GPU on the Vimeo90K dataset. }
    \label{tab:limitation}
\end{table}



\section{Algorithm of UGSP Framework}
\label{appendix:algorithm}
We summarize our two phases of UGSP training in Algorithm~\ref{alg:ugsp}. 
The uncertainty estimation network (UEN) in the first phase is trained using the Vimeo90K training dataset. UEN can predict the uncertainty (variance) field $U_k$ for the unknown intermediate frame. The uncertainty map is then used to generate pruning masks $P_k^u$ that serve as a guide for the second phase's spatial pruning mask estimation using $\mathcal{L}_{ugm}$.
Therefore, we save the $P_k^u$ for each Vimeo90K training dataset sample at the end of first phase. 

In the second phase, we train the VFI network using the Vimeo90K training dataset and $P_k^u$.
$\mathcal{L}_s$ controls the degree of sparsity in our UGSP, and $\mathcal{L}_{sc}$ utilizes the feature of the auxiliary non-pruning branch to enhance the performance of our UGSP.
Finally, we only obtain the VFI nework $E_{vfi}$, and the UEN $E_{uen}$ is not required during the inference phase.

\begin{algorithm}[h]
	\caption{Two training phases of UGSP.}
	\label{alg:ugsp}
	\KwIn{Initialize network parameters $\theta_{uen}$ and $\theta_{vfi}$ of UEN $E_{uen}$ and VFI network $E_{vfi}$, respectively. Set $\lambda_{s}$=$\lambda_{ugm}$=$\lambda_{sc}$=0.01.}
	% \KwOut{Interpolated frame  $I_t$}  
	\BlankLine

        \# First phase:
        
	\While{\textnormal{$\theta_{uen}$ has not converged}}{
    	 Sample $\{I_0, I_1\}$ a batch from the Vimeo90K training dataset; \\
         $\{I_t^k, U_k\} \leftarrow E_{uen}(I_0, I_1), k\in(0,1,2)$; \\
         Calculate $\mathcal{L}_{s u}(I_t^k, U_k), k\in(0,1,2)$;
        $\theta_{esu} \leftarrow \theta_{esu}- \nabla_{\theta_{esu}} L_{s u}$;
	}
        \ForEach{$\{I_0, I_1\}$ in Vimeo90K training dataset}{
		$\{U_k\} \leftarrow E_{uen}(I_0, I_1), k\in(0,1,2)$; \\
            $\{P_k^u\} \leftarrow \{U_k\}, k\in(0,1,2)$ using Equation (4); \\
            Save $\{P_k^u\}, k\in(0,1,2)$;
		}
        


	\BlankLine
        \# Second phase:
        
	\While{\textnormal{$\theta_{vfi}$ has not converged}}{
    	 Sample $\{I_0, I_1, \{P_k^u\}, k\in(0,1,2) \}$ a batch from the Vimeo90K training dataset; \\
         $\{I_t, I_t^{sc}\} \leftarrow E_{vfi}(I_0, I_1, \{P_k^u\}, k\in(0,1,2))$; \\
         Calculate $\mathcal{L}_{r e c}(I_t, I_t^{gt})$, $\mathcal{L}_{sc}(I_t, I_t^{sc}, I_t^{gt})$; \\
         Calculate $\mathcal{L}_{s}(P_k), \mathcal{L}_{ugm}(P_k, P_k^u), k\in(0,1,2)$; \\
        $\theta_{vfi} \leftarrow \theta_{vfi}- \nabla_{\theta_{vfi}} \mathcal{L}_{rec}$;\\
        $\theta_{vfi} \leftarrow \theta_{vfi}- \nabla_{\theta_{vfi}} \lambda_{s}\mathcal{L}_{s}$;\\
        $\theta_{vfi} \leftarrow \theta_{vfi}- \nabla_{\theta_{vfi}} \lambda_{ugm}\mathcal{L}_{ugm}$;\\
        $\theta_{vfi} \leftarrow \theta_{vfi}- \nabla_{\theta_{vfi}} \lambda_{sc}\mathcal{L}_{sc}$;\\
	}
     % return UEN $E_{uen}$ and VFI network $E_{vfi}$.
    \BlankLine
    \KwOut{VFI network $E_{vfi}$.}  

\end{algorithm}

% Figure environment removed

% % Figure environment removed

% % Figure environment removed

% Figure environment removed

% Figure environment removed



\section{Network Architecture}
\label{appendix:network}
This section illustrates all VFI and UEN structure details, and the overview of our UGSP is shown in Figure~\ref{s_overview_encoder}(a).
In Figure~\ref{s_uen}, we display the structure details of Uncertainty blocks ($f_u^0$, $f_u^1$ and $f_u^2$) and VFI blocks ($f_v^1$ and $f_v^2$).
Uncertainty blocks ($f_u^0$, $f_u^1$ and $f_u^2$) first use one nearest up-sampling operation to maintain same resolution as the input frame. Then, they respectively estimate uncertainty ($U_0$, $U_1$, and $U_2$) using 2, 3, and 4 blocks of one $3\times3$ convolution and one ELU activation layer. VFI blocks ($f_v^1$ and $f_v^2$) respectively contain 1 and 2 blocks of one $3\times3$ convolution, one PReLu activation layer, and one deconvolution to estimate the middle frame ($I_t^1$ and $I_t^2$). Therefore, the resolution of $U_k$ and $I_t^k$ is the same as the input frame $I_{0,1}$.

Since the backbone network structure in UEN is identical to that in the VFI network, but removes a branch for estimating the pruning mask and adding a brach for estimationg uncertainty. Therefore, we will only describe the structure of the VFI network.
As shown in Figure~\ref{s_overview_encoder}(b), VFI first down-samples two input frames $I_0$ and $I_1$, four times using a block of two 3×3 convolutions with strides 2 and 1 to obtain four level features with 32, 48, 72, and 96 channels $\phi_{0,1}^k, k \in\{0,1,2,3\}$ in the encoder. 

In Figure~\ref{s_vfi}, we illustrate the process in each scale of VFI network. 
As shown in Figure~\ref{s_vfi}(a), we concatenate the output of six consecutive $3\times3$ convolutions and then use one $1\times1$ convolution to extract features.
$F_{t \rightarrow 0,1}^3$ and $\hat{\phi}_t^3$ are obtained via one bilinear up-sampling, and $P_3$ is obtained via one $3\times3$ convolution, one bilinear up-sampling, and a Gumbel softmax.
The Gumbel softmax trick is applied to obtain a softened spatial mask for $P^k, k\in{1,2,3}$:
\begin{equation}
P^k=\frac{\exp \left(\left(\hat{P}^{k}[1]+G^k[1]\right) / \tau\right)}{\sum_{i=1}^2 \exp \left(\left(\hat{P}^{k}[i]+G^k[i]\right) / \tau\right)},
\end{equation}
where ${P}^{k}$ is the output of the bilinear up-sampling. $G^k$ is a Gumbel noise tensor where elements all have a Gumbel(0, 1) distribution, and $\tau$ is a temperature parameter. In practice, we generate binary pruning masks by starting at a high temperature and annealing to a low one.
$P_{3}$ is used to skip the redundant computations in the $3\times3$ and $1\times1$ convolutions of scale 2. 
% $\hat{\phi}_{0,1}^2$ are the aligned features which are obtained by back warping with frame features $\phi_{0,1}^2$ and flow fields $F_{t \rightarrow 0,1}^{3}$ as input. 


As shown in Figure~\ref{s_vfi}(b-d), we achieve sparse convolution by multiplying each convolution operation's output feature with the pruning mask predicted from the previous scale during training.
We first concatenate the feature $\hat{\phi}_{t}^k$ and the aligned feature produced by back warping using flow field $F_{t \rightarrow 0,1}^k$ and feature $\phi_{0,1}^{k-1}$.
Then we input it into consecutive $3\times3$ convolutions and one $1\times1$ convolution to refine the feature.
% We first concatenate $ (F_{t \rightarrow 0,1}^{k+1}, \hat{\phi}_t^{k+1}, P_{k+1}, \hat{\phi}_{0,1}^k ), k \in\{0,1,2\}$ and then input it to six or five $3\times3$ convolutions and one $1\times1$ convolution in sequence to extract features.
In scales 1 and 2, $(F_{t \rightarrow 0,1}^{k-1}, \hat{\phi}_t^{k-1}, P_{k-1})$ are achieved in the same manner as in scale 3.
In scale 0, we utilize one deconvolution to achieve $F_{t \rightarrow 0,1}^0, M, R$, which are then blended into the estimated intermediate frame $I_t$.


\section{Details of UGSP-\textit{distill} and UGSP-\textit{refine}}
\label{appendix:ugsp}

This section describes the structure of UGSP-\textit{distill} and UGSP-\textit{refine}.

\noindent
\textbf{UGSP-\textit{distill}.} We implement the flow distillation and geometry consistency loss of IFRnet \cite{Kong_2022_CVPR} into UGSP as UGSP-\textit{distill}.
For the flow distillation loss, the pre-trained liteflownet \footnote{http://content.sniklaus.com/github/pytorch-liteflownet/network-default.pytorch} is used to predict the pseudo flow label $F_{t\rightarrow0}^p$,  $F_{t\rightarrow1}^p$.
Then,  we can obtain robustness masks
$P_l(l \in {0,1})$ by the following formulation:
\begin{equation}
P_l=\exp \left(-\beta\left|F_{t \rightarrow l}^{0}-F_{t \rightarrow l}^p\right|_{e p e}\right)
\end{equation}
where $\beta$ is set to 0.3 as IFRnet, and per-pixel end-point error is calculated between our estimated flow $F_{t \rightarrow l}^{0}$ in level 0 and the pseudo label $F_{t\rightarrow0}^p$.
Finally, we use the flow distillation loss, which can be expressed as:
\begin{equation}
\mathcal{L}_d=\sum_{k=1}^3 \sum_{l=0}^1 \left(\left(\left(F_{t \rightarrow l}^k\right) \uparrow_{2^k}-F_{t \rightarrow l}^{p}\right)^2 + 10^{-(10p-1)/3}\right)^{p/2},
\end{equation}
where $p$ denotes the value of any position in the mask $P_0$ and $P_1$. By task-oriented adjusting the Charbonnier loss $\rho(x)=\left(x^2+\epsilon^2\right)^\alpha$ using $p$, we can prevent the model from learning the pseudo label with noise.
$\uparrow_{2^k}$ denotes we bilinearly up-sample estimated flow to make the resolution consistent with the pseudo label.

    Geometry consistency loss can be expressed as:
    \begin{equation}
    \mathcal{L}_g=\sum_{k=1}^3 \mathcal{L}_{c e n}\left(\hat{\phi}_t^k, \phi_t^k\right),
    \end{equation}
where $\hat{\phi}_t^k$ denotes the output of the encoder when the ground truth intermediate frame is input.
$\mathcal{L}_{g}$ uses low-level structure information in $\hat{\phi}_t^k$ to regularize the reconstructed intermediate feature $\phi_t^k$.

\noindent
\textbf{UGSP-\textit{refine}.}
We incorporate RIFE's \cite{huang2022rife} refinement network and priveleged distillation strategy into our framework as UGSP-\textit{refine}.
Specifically, we use the same refinement network in RIFE, but the channel is scaled by 0.625.
RIFE uses a teacher model to estimate optical flow by inputting the intermediate features of input frames and the ground truth intermediate frame $I_t^{gt}$.
Therefore, we also implement a teacher network, as shown in Figure~\ref{s_rife}. It is similar to the scale 0 procedure in the VFI network in Figure~\ref{s_vfi}(d) but without a pruning mask $P_1$.
Another difference is that we do not use the residual $R$ since the input contains the ground truth feature $\phi_t^{1}$, which would make the flow estimation inefficient if the residual is used.
$\phi_t^{1}$ is achieved by inputting the ground truth $I_t^{gt}$ into the encoder as shown in Figure~\ref{s_overview_encoder}(b).
% In addition, we also modify the VFI network not to utilize residual $R$.
Then, the distillation loss is defined as:
    \begin{equation}
    \mathcal{L}_{dis}=\mathcal{L}_{r e c} \left(I_t^{Tea}, I_t^{gt}  \right) + \sum_{k=0}^2 \left\|(F_{t\rightarrow{0,1}}^k)\uparrow_{2^k}- F_{t\rightarrow{0,1}}^{Tea}\right\|.
    \end{equation}
The teacher model obtains $I_t^{Tea}$ and $F_{t\rightarrow{0,1}}^{Tea}$ using $\phi_t^{1}$ as additional input.
$\uparrow_{2^k}$ denotes we bilinearly up-sample estimated flow to make resolution consistent with the teacher flow $F_{t\rightarrow{0,1}}^{Tea}$.
Like RIFE, the teacher block will be discarded after the training phase, so there would be no additional cost for inference.
$\mathcal{L}_{dis}$ not only makes more stable training but also enhances our model's estimation ability.

% Figure environment removed

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}
{
    \begin{tabular}{lcccccc}
    \toprule
   \multirow{2}{*}{ Model} &  \multicolumn{2}{c}{Vimeo90K} & \multicolumn{2}{c}{UCF101} &  \multicolumn{2}{c}{Middlebury} \\
     \cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7}  & FLOPs(G) & PSNR  & FLOPs(G) & PSNR &  FLOPs(G) & PSNR \\
    \hline
    10\%,60\%,80\% & 15.7 & 35.60 & 8.9 & 35.30 & 39.4 & 36.94 \\
    20\%,40\%,80\% & 15.6 & 35.62 & 8.6 & 35.31 & 39.3 & 36.96 \\
    30\%,30\%,60\% & 15.6 & 35.62 & 8.5 & 35.29 & 39.4 & 36.95 \\
    \hline
    \end{tabular}
}
    \caption{ 
  Ablation study of uncertainty map threshold.
}
    \label{tab:threshold}
\end{table}

\section{Ablation of Uncertainty Map Threshold}
\label{appendix:thresholding}

As described in Section~\ref{sec:ugm}, we assign the threshold $T_k$
 to the $\alpha_k$ smallest value of $U_{k-1}$. We set the threshold based on the convolution’s FLOPs in the pruning scale, and the FLOPs ratio of convolution on the scale 0, 1, 2 is close to 1: 2: 4, so we set $\alpha_1$, $\alpha_2$, $\alpha_3$ to 1: 2: 4. For example, $\alpha_1$, $\alpha_2$, $\alpha_3$ are assigned to 20\%, 40\%, 80\% when the target sparsity is 35\%. Here, we do the sensitivity ablation study for two models. The $\alpha_1$, $\alpha_2$, $\alpha_3$ for the first model are 10\%, 60\%, 80\%, and the 
$\alpha_1$, $\alpha_2$, $\alpha_3$ for the second model are 30\%, 30\%, 60\%. The results are shown in Table~\ref{tab:threshold}. 
We can observe that (20\%, 40\%, 80\%) model is better than (10\%, 60\%, 80\%) model. This might be because larger $\alpha_1$ forces more areas to use all computing resources, leading to more challenging regions to obtain all computing resources. In addition, (20\%, 40\%, 80\%) model obtains similar results to (30\%, 30\%, 60\%) model. The percentage of challenge regions might be less than 30\%, so increasing the number of regions utilizing all computing resources cannot significantly improve performance. We believe there are better ways to choose $\alpha_k$, and we will study it in the future. 

% Figure environment removed

\section{More Visual Results}
\label{visual}
Figure~\ref{s_intro} illustrates the computational resource map for two examples, with the red, yellow, and blue regions requiring high, medium, and low computational costs, respectively.
It is observed that red regions nearly contain the region of large or complex movement, which is essential for a pleasing visual experience but challenging for VFI tasks.
Moreover, when we increase the computational resources, the PSNR increases, and complex and large motion is estimated more accurately .
Furthermore, our pruning model has a higher PSNR, and there are two reasons for this. First, using our estimated mask during training, our UGSP prioritizes the reconstructed quality of challenging areas.
Second, the estimation of simple and small motions in the baseline may harm large and complex motion estimation.


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
