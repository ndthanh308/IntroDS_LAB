\section{Introduction}

Contextual bandits \citep{auer2002using, langford2007epoch} represent a classical sequential decision-making problem where an agent aims to maximize cumulative reward based on context information. At each round $t$, the agent observes a context and must choose one of $K$ available actions based on both the current context and previous observations. Once the agent selects an action, she observes the associated reward, which is then used to refine future decision-making. %A special example is the linear contextual bandit, where the reward is a noisy observation of a linear function of the context for each action. ???
The contextual bandit problem is a typical example of reinforcement learning where balancing the exploration of new actions with the exploitation of previously acquired information is necessary to achieve optimal long-term rewards. It also has numerous real-world applications including personalized recommendation systems \citep{li2010contextual, bouneffouf2012contextual}, healthcare \citep{yom2017encouraging, liao2020personalized}, and online education \citep{liu2014trading, shaikh2019balancing}. 

Despite the extensive existing literature on contextual bandits, in many real-world applications, the agent never observes the context \emph{exactly}. One common reason is that the true context for decision-making can only be detected or learned approximately from observable auxiliary data. For instance, consider the Sense2Stop mobile health study, in which the context is whether the individual is currently physiologically stressed \citep{battalio2021sense2stop}. A complex predictor of current stress was constructed/validated based on multiple prior studies \citep{cohen1985measuring, sarker2016finding, sarker2017markers}.  This predictor was then tuned to each user in Sense2Stop prior to their quit-smoke attempt and then following the user's attempt to quit smoking, at each minute, the predictor input  high-dimensional sensor data on the user and output a continuous likelihood of stress for use by the decision-making algorithm.
%\sam{check to see where this cite belongs! \citep{coughlin2021toward}.}
Similarly, in online advertising, the context might be the measure of how likely a user is to buy an item, which cannot be observed. The second reason why the context is not observed exactly is because of measurement error. Noisy measurement of variables is ubiquitous in practice, which introduces an additional level of uncertainty that must be accounted for when making decisions. 

Motivated by the above, we consider the linear contextual bandit problem where at each round, the agent only has access to a noisy observation of the true context. Moreover, the agent has limited knowledge about the underlying distribution of this noisy observation, as in many practical applications (e.g. the above-mentioned ones). This is especially the case when this `observation' is the output of a complex machine learning algorithm. We only assume that the noisy observation is unbiased, its covariance is known or can be estimated, and we put {no} other essential restrictions to its distribution. This setting is intrinsically difficult for two main reasons: First, when estimating the reward model, the agent needs to take into account the misalignment between the noisy context observation and the reward which depends on the true context. Second, even if the reward model is known, the agent suffers from making bad decisions because of inaccurate context information at each round. 

\noindent{\textbf{Our contributions.}} We present the first online algorithm \texttt{MEB} (Measurement Error Bandit) with sublinear regret in this setting, with a specific discussion on the choice of appropriate benchmarks. \texttt{MEB} achieves $\cO(T^{2/3})$ regret compare to a standard benchmark and $\cO(T^{1/2})$ regret compare to a clipped benchmark with minimum exploration probability which is common in many applications \citep{yang2020targeting, yao2021power}. \texttt{MEB} is based on a novel approach to model estimation which is able to remove the systematic bias caused by the noisy context observation. The estimator is inspired by the measurement error literature in statistics \citep{carroll1995measurement, fuller2009measurement}: We extend this classical method with additional tools in the online decision-making setting due to the policy being dependent on the measurement error. 

%In the wider scope of latent contextual bandits, our main contribution lies in identifying a general and practical setting where a good policy can be formed even without having any emissions/observations of the latent context. \sam{I now realize that the prior sentence is not quite correct. We do have emissions--these are the high dimensional inputs to our predictor. The real difference is in the use of a learned predictor of a low dimensional unobserved true state. The predictor (weights in predictor are not updated during study)  is not learned during the implementation of the bandit.  This greatly reduces problems with low signal to noise challenges that slow down learning.   This is a subtle rationale and I don't think you should include this here.   I suggest to delete prior sentence. }

\subsection{Related work}

Our work complements several lines of literature in contextual bandits, as listed below.

\noindent\textbf{Latent contextual bandit.} In the latent contextual bandit literature \citep{zhou2016latent, sen2017contextual, hong2020latent, hong2020non, xu2021generalized, %hong2022thompson,
nelson2022linearizing, galozy2023information}, the reward is typically modeled as jointly depending on the latent state, the context, and the action. %through a parametric model
Several works \citep{zhou2016latent, hong2020latent, hong2020non, galozy2023information} assume no direct relations between the latent state and the context while setting a parametric reward model. \citep{hong2020latent} assumes the latent state $s\in\cS_l$ is unknown but constant over time and provides an algorithm with regret $\tilde\cO(\sqrt{T|\cS_l|})$ when the reward model is known. \citep{hong2020non, galozy2023information} assume that the latent state evolves through a Markov chain $\PP_{\phi^*}(\cdot|\cdot)$ and propose algorithms with regret $\cO(T^{2/3}\sqrt{|\cS_l|(1+pT)})$. Here $p = 1-\min_{s\in\cS}\PP_{\phi^*}(s|s)$.  \citep{zhou2016latent} considers a sequence of contextual bandits, each with one of $N$ latent states drawn from a distribution. \citep{xu2021generalized} sets a specific context as well as the latent feature for each action, and models the reward depending on them through a generalized linear model. Different from the aforementioned studies, we specify that the observed context is a noisy version of the latent context (which aligns with many applications), and then leverage this structure to design the online algorithm.

In another line of work, \citep{sen2017contextual, nelson2022linearizing} consider contextual bandit with latent confounder instead of latent state in the sense that the observed context influences the reward only through a latent confounder variable, which ranges within a small discrete set. In \citep{sen2017contextual}, the authors apply a matrix-factorization approach; While in \citep{nelson2022linearizing}, the authors assume additionally that the latent confounder evolves through a Markov process, and the observed context depends on it through a parametric model. Under some conditions, the problem can be transformed into a linear contextual bandit. Our setting is distinct from these works in that the latent context can span an infinite (or even continuous) space, which is not of small cardinality.

\noindent\textbf{Bandit with noisy context.} 
In bandit with noisy context \citep{yun2017contextual, kirschner2019stochastic, yang2020multi, lin2022distributed, lin2022stochastic}, the agent has access to a noisy version of the true context and/or some knowledge of the distribution of the noisy observation. \citep{yun2017contextual} considers the setting where for each time $t$ and action $a$, the true context $z_{a, t}$ is i.i.d. normal. The agent only observes each entry of $z_{a, t} + \epsilon_{a, t}$ with probability $p$, where the error $\epsilon_{a, t}$ is multivariate normal with mean 0. The authors develop algorithms with regret $\tilde\cO(d\sqrt{T}+d^2\sqrt{T/p^3})$ compared to the Bayesian oracle strategy. Other works such as \citep{kirschner2019stochastic, yang2020multi, lin2022stochastic} assume that the agent knows the full distribution of the context each time, and observes no context. By assuming a linear reward model, \citep{kirschner2019stochastic} transforms the problem into a linear contextual bandit, and obtains $\tilde\cO(d\sqrt{T})$ regret compared to the policy maximizing the expected reward over the context distribution. \citep{yang2020multi, lin2022stochastic} consider variants of the problem such as multiple linear feedbacks, multiple agents, and additional constraints. \citep{lin2022distributed} considers another variant where $M$ collaborating agents know the full distribution of context each time and observe the exact context after some delay. Compare to these works, we consider a related but more challenging setting where besides an unbiased noisy observation(prediction) for each context, the agent only knows the second-moment information about the distribution. This does not transform into a standard linear contextual bandit as \citep{kirschner2019stochastic}.%and significant changes in the algorithm are necessary.

\noindent\textbf{Bandit with inaccurate/corrupted context.} There are also a number of works considering the setting where the context is simply inaccurate (without randomness), or is corrupted and cannot be recovered. In \citep{yang2021bandit, yang2021robust}, at each round, only an inaccurate context $\hat\xb_t$ is available to the decision-maker, and the exact context $\xb_t$ is revealed after the action is taken. Under suitable conditions, \citep{yang2021bandit} proves that the regret of Thompson sampling is $\tilde\cO(d\sqrt{T} + L_f\sqrt{d}\sum_{t\in[T]}\|\hat\xb_t - \xb_t\|_2)$, where $L_f$ is the smoothness parameter of the reward. \citep{yang2021robust} considers the robustness objective of maximizing the worst-case reward or minimizing the worst-case regret. In \citep{bouneffouf2020online, galozy2020corrupted}, the authors consider the setting where each context $\xb_t$ is completely corrupted with some probability and the corruption cannot be recovered. In \citep{ding2022robust}, the context is attacked by an adversarial agent. Because these works focus more on adversarial settings for the context observations, the application of their regret bounds to our setting results in a linear regret. Given the applications we consider, we can exploit the stochastic nature of the noisy context observations in our algorithm to achieve improved performance.

\subsection{Notations}

Throughout this paper, we use $[n]$ to represent the set $\{1, 2, \ldots, n\}$ for any positive integer $n$. For two real numbers $a, b\in\RR$, let $a\wedge b$ denote the minimum of $a$ and $b$. Given a positive integer $d$, $\bI_d$ denotes the $d$-by-$d$ identity matrix, and $\mathbf{1}_d$ denotes the $d$-dimensional vector with 1 in each entry. For a vector $\bv\in\RR^d$, denote $\|\bv\|_2$ as its $\ell_2$ norm. For a matrix $\bM\in\RR^{m\times n}$, denote $\|\bM\|_2$ as its operator norm. The notation $\cO(X)$ refers to a quantity that is upper bounded by $X$ up to constant multiplicative factors, while $\tilde\cO(X)$ refers to a quantity that is upper bounded by $X$ up to poly-log factors.

%\noindent\textbf{Measurement error models.} ??? (including Bayesian measurement error models)


\section{Measurement error adjustment to bandit with noisy context}

\subsection{Problem setting}

We consider a linear contextual bandit with context space $\cX\subset\RR^d$ and binary action space $\cA = \{0, 1\}$\footnote{For simplicity, we consider the binary-action setting, which is common in healthcare \citep{trella2022reward}, economics \citep{athey2017efficient, kitagawa2018should} and many other applications. However, all the results presented in this paper can be extended to the setting with multiple actions. See Appendix \ref{appendix:generalization-to-K-actions} for details.}. Let $T$ be the time horizon. As discussed above, we consider the setting where at each time $t\in[T]$, the agent only observes a noisy version of the context $\tilde\xb_t$ instead of the true underlying context $\xb_t$. Thus at time $t$, the observation $o_t$ only contains $(\tilde \xb_t, a_t, r_t)$, where $a_t$ is the action and $r_t$ is the corresponding reward. We further assume that $\tilde \xb_t = \xb_t + \bepsilon_t$, where the error $\bepsilon_t$ is uniformly bounded and is independent of the history $\cH_{t-1} := \{o_\tau\}_{\tau\leq t-1}$, $\EE \bepsilon_t = 0$, $\Var \bepsilon_t = \bSigma_{e, t}$. Initially we assume that $(\bSigma_{e, t})_{t\geq 1}$ is known. In Section \ref{section:estimated-Sigma-e}, we consider the setting where we have estimators of $(\bSigma_{e, t})_{t\geq 1}$. We do not make further restrictions on the distribution of $(\bepsilon_t)_{t\geq 1}$. The reward $r_t = \langle\btheta_{a_t}^*, \xb_t\rangle + \eta_t$, where $\EE[\eta_t|\cH_{t-1}, \bepsilon_t, a_t] = 0$ and $(\btheta_a^*)_{a\in\cA}$ are the unknown parameters. % \sam{we are not assuming subgaussian tails on $\eta_t$?}
It's worth noting that besides the policy, all the randomness here comes from the reward noise $\eta_t$ and the measurement error $\bepsilon_t$. We treat $(\xb_t)_{t\geq 1}$ as fixed throughout but unknown to the algorithm (Unlike \cite{yun2017contextual}, we don't assume $\xb_t$ are i.i.d.). Our goal at each time $t$ is to design policy $\pi_t(\cdot|\cH_{t-1}, \tilde\xb_t)\in\Delta(\cA)$ given past history $\cH_{t-1}$ and current observed noisy context $\tilde\xb_t$, so that the agent can maximize the reward by taking action $a_t\sim \pi_t(\cdot|\cH_{t-1}, \tilde\xb_t)$.

If $\bSigma_{e, t}$ does not go to zero as $t$ grows, naive contextual bandit algorithms are clearly sub-optimal, as they do not take the errors $(\bepsilon_t)_{t\geq 1}$ into account. It is thus necessary to design an online algorithm that adjusts for these errors. To achieve this goal, we must first redefine the regret by comparing to a suitable benchmark policy in this setting. 

\subsection{Choice of the benchmark policy}\label{section:benchmark-policy}

We set the benchmark to perform `optimally' with the knowledge of infinite data. As the sequence $(\xb_t)_{t\geq 1}$ is never revealed, 
%in observable data,  
the  benchmark policy $\pi_t^\dagger(\cdot|\tilde\xb_t, \cH_{t-1})$ can only use observation of $\tilde\xb_t$, $\cH_{t-1}$ and knowledge of $(\btheta_a^*)_{a\in\{0, 1\}}$ to select actions at time $t$. 

As for reward maximization, notice that condition on the history $\cH_{t-1}$, maximizing expected reward with respect to policy $\pi$ is equivalent to minimizing the loss 
\begin{align}
\ell_t(\pi;\xb_t, \cH_{t-1})&=\EE_{\tilde\xb_t = \xb_t + \bepsilon_t}\EE_{a_t\sim \pi(\cdot|\tilde\xb_t, \cH_{t-1})}|\langle\btheta_0^*-\btheta_1^*, \xb_t\rangle|\cdot 1_{\{\langle\btheta_{a_t}^*-\btheta_{1-a_t}^*, \xb_t\rangle<0\}}\nonumber\\
&=|\langle\btheta_0^*-\btheta_1^*, \xb_t\rangle|\cdot\EE_{\tilde\xb_t = \xb_t + \bepsilon_t}\left[1_{\left\{f(\tilde\xb_t)\cdot {\sign}(\langle\btheta_0^*-\btheta_{1}^*, \xb_t\rangle)<0\right\}}\right].\label{eq:oracle-problem}
\end{align}
Here $f(\tilde\xb_t)\in\{\pm 1\}$ is a (possibly random) mapping of $\tilde\xb_t$, potentially depending on $\cH_{t-1}$, to predict the sign of $\langle\btheta_0^*-\btheta_{1}^*, \xb_t\rangle$; $f=1$ and $-1$ correspond to the optimal action $a_t = 0$ and 1 respectively. This implies that there's a one-to-one correspondence between the randomized mapping $f$ and the policy $\pi$, and thus the loss minimization can be viewed as minimizing $\ell_t(\pi(f);\xb_t)$ with respect to $f$. Furthermore, upon close inspection, (\ref{eq:oracle-problem}) can be viewed as a task of estimating the sign of a linear transform of the unknown parameter $\xb_t$ given one single sample $\tilde\xb_t$ with mean $\xb_t$. 

As in most parameter estimation problems, there is no function that minimizes the loss for all parameter values. To illustrate in this context, fix some $\xb$ s.t. $\langle\btheta_0^* - \btheta_1^*, \xb\rangle\neq 0$ and consider the constant mapping $f_{\xb}(\tilde \xb_t) \equiv \sign(\langle\btheta_0^* - \btheta_1^*, \xb\rangle)$. It is easy to see that $\ell_t(\pi(f_{\xb}); \xb_t, \cH_{t-1}) = 0$ for all $\xb_t$ such that $\sign(\langle\btheta_0^* - \btheta_1^*, \xb_t\rangle) = \sign(\langle\btheta_0^* - \btheta_1^*, \xb\rangle)$. On the other hand, $\ell_t(\pi(f_{\xb}); \xb_t, \cH_{t-1}) = |\langle\btheta_0^*-\btheta_1^*, \xb_t\rangle|$ for all $\xb_t$ such that $\sign(\langle\btheta_0^* - \btheta_1^*, \xb_t\rangle) =- \sign(\langle\btheta_0^* - \btheta_1^*, \xb\rangle)$. Therefore, for any $\xb_t$, there is always a mapping $f$ that achieves zero loss at $\xb_t$. But these mappings typically behave badly at other values of $\xb_t$: In fact, it's impossible to find a mapping that achieves 0 loss at \emph{all} $\xb_t$. 

Despite the above fact, the following lemma shows the advantage of the natural mapping $f(\tilde\xb_t) = f_t^\dagger(\tilde\xb_t):= \sign(\langle\btheta_0^*-\btheta_1^*, \tilde\xb_t\rangle)$. The proof is in Appendix \ref{pf:lem:oracle}.

\begin{lemma}\label{lem:oracle}
Let $\cG$ be the set of all mappings (including deterministic and random ones) from $\RR$ to $\RR$. Then the followings hold:

(i) If $d=1$, then for any fixed distribution of $\bepsilon_t$ that is continuous and symmetric around zero, for any $\xb\in\RR$, $f_t^\dagger\in\argmin_{f\in\cG} \{\ell_t(\pi(f); \xb)+\ell_t(\pi(f); -\xb)\}$.

(ii) For general $d$, given any fixed distribution of $\bepsilon_t$ that is continuous and symmetric around zero, for any $\xb$, $f_t^\dagger\in\argmin_{f\in\cF} \{\ell_t(\pi(f); \xb)+\ell_t(\pi(f); -\xb)\}$. Here, $\cF:=\{f(\cdot):\exists g\in\cG\text{ s.t. }f(\tilde\xb_t)=g(\langle\btheta_0^*-\btheta_1^*, \tilde\xb_t\rangle)\}$.
\end{lemma}

That is, in the one-dimensional case, $f(\tilde\xb_t) $ always minimizes the average loss between $\xb_t = \xb$ and $\xb_t = -\xb$ for any $\xb$ under mild regularity conditions.   In the general case, $f(\tilde\xb_t) $  minimizes this average loss for any $\xb$ among all mappings of the form $f(\tilde\xb_t) = g(\langle\btheta_0^*-\btheta_1^*, \tilde\xb_t\rangle)$. Therefore, $f=f_t^\dagger$ is a reasonable choice for reward maximization. Actually, it corresponds to the action $a_t^\dagger:= \argmax_a\langle \btheta^*_{a}, \tilde\xb_t\rangle$. 
Thus we set the benchmark policy as 
\begin{equation}\label{eq:nonclipped-oracle-policy}
\pi_t^*(a)=
\begin{cases}
1,\quad &\text{if }a=a_t^\dagger=\argmax_a\langle \btheta^*_{a}, \tilde\xb_t\rangle,\\
0,\quad &\text{otherwise.}
\end{cases}
\end{equation}
For any policy $\pi = \{\pi_t\}_t$, we define the cumulative regret as 
\begin{equation}\label{eq:regret}
\text{Regret}(T; \pi^*) = \sum\nolimits_{t\in[T]} [\EE_{a\sim \pi_t^*}\langle \btheta^*_{a}, \tilde\xb_t\rangle - \EE_{a\sim \pi_t}\langle \btheta^*_{a}, \tilde\xb_t\rangle].
\end{equation}
Further, in many applications, it's desirable to design the policy under the constraint that each action is sampled with a minimum probability $p_0$. One reason for maintaining exploration is that we can update and re-optimize the policy for future users to allow for potential non-stationarity \citep{yang2020targeting}. Additionally, keeping the exploration is also important for after-study analysis \citep{yao2021power}, especially when the goal of the analysis is not pre-specified prior to  conducting data with the online algorithm. Considering these factors, we also set a clipped benchmark policy
\begin{equation}\label{eq:oracle-policy}
\pi_t^\dagger(a)=
\begin{cases}
1-p_0,\quad &\text{if }a=a_t^\dagger=\argmax_a\langle \btheta^*_{a}, \tilde\xb_t\rangle,\\
p_0,\quad &\text{otherwise,}
\end{cases}
\end{equation}
and define $\text{Regret}(T; \pi^\dagger)$ accordingly.

\subsection{Estimation using weighted measurement error adjustment}\label{section2.3}

In this section, we focus on learning the reward model parameters $(\btheta_a^*)_{a\in\cA}$ with data $\cH_t$ after a policy $(\pi_\tau(\cdot|\tilde\xb_\tau, \cH_{\tau-1}))_{\tau\in[t]}$ has been executed up to time $t\in[T]$. Learning a consistent model is important in many bandit algorithms for achieving low regret \citep{abbasi2011improved, agrawal2013thompson}. As we shall see in Section \ref{section:meb}, consistent estimation of $(\btheta_a^*)_{a\in\cA}$ also plays an essential role in controlling the regret of our proposed algorithm.

\noindent\textbf{A naive estimator.} If $(\bepsilon_\tau, \eta_\tau)_{\tau\in[t]}$ are i.i.d., there is only a single action $a=0$ and $\bSigma_{e, \tau}\equiv \bSigma_e$, the classic linear measurement error model provides a useful solution for estimating the model parameter $\btheta_0^*$ given batch data $(\tilde\xb_\tau, r_\tau)_{\tau\in[t]}$ \citep{fuller2009measurement, carroll1995measurement}.
In this single-action setting,
$
\hat\btheta_{0, me}^{(t)}:= \big(\frac{1}{t}\sum_{\tau\in[t]}\tilde\xb_\tau\tilde\xb_\tau^\top - \bSigma_e\big)^{-1}\big(\frac{1}{t}\sum_{\tau\in[t]}\tilde\xb_\tau r_\tau\big)
$
is a consistent estimator of $\btheta_0^*$. When there are multiple actions, a naive generalization of the above estimator might be 
\begin{equation}\label{eq:naive-estimator}
  \hat\btheta_{a, me}^{(t)}:= \bigg(\sum\nolimits_{{\tau}\in[t]}1_{\{a_{\tau} = a\}}(\tilde\xb_{\tau}\tilde\xb_{\tau}^\top - \bSigma_e)\bigg)^{-1}\bigg(\sum\nolimits_{{\tau}\in[t]}1_{\{a_{\tau} = a\}}\tilde\xb_{\tau}r_{\tau}\bigg).  
\end{equation}
Unfortunately, $\hat\btheta_{a, me}^{(t)}$ is generally not consistent in the multiple-action setting, even if the policy $(\pi_\tau)_{\tau\in[t]}$ is \emph{stationary and not adaptive to history}. This difference is essentially due to the interaction between the policy and the measurement error: In the classical measurement error (single-action) setting, $\EE(\tilde\xb_\tau\tilde\xb_{\tau}^\top) = \xb_\tau\xb_{\tau}^\top + \bSigma_e$, and $\frac{1}{t}\sum_{\tau\in[t]}\tilde\xb_\tau\tilde\xb_{\tau}^\top - \bSigma_e$ concentrates around its expectation $\frac{1}{t}\sum_{{\tau}\in[t]}\xb_{\tau}\xb_{\tau}^\top$. Likewise, $\frac{1}{t}\sum_{{\tau}\in[t]}\tilde\xb_{\tau}r_{\tau}$ concentrates around $(\frac{1}{t}\sum_{\tau\in[t]}\xb_{\tau}\xb_{\tau}^\top)\btheta_0^*$. Combining the above parts thus yields a consistent estimator of $\btheta_0^*$. In our setting with multiple actions, however, the agent picks the action $a_\tau$ \emph{based on} $\tilde\xb_\tau$, so only certain values of $\tilde\xb_\tau$ lead to $a_\tau = a$. Therefore, for those $\tau\in[t]$ when we pick $a_\tau = a$, it's more likely that $\tilde\xb_\tau$ falls in certain regions depending on the policy, and we shouldn't expect $\EE(\tilde\xb_{\tau}\tilde\xb_{\tau}^\top)=\xb_{\tau}\xb_{\tau}^\top + \bSigma_e$ anymore. In other words, the policy creates a complicated dependence between $\tilde\xb_{\tau}\tilde\xb_{\tau}^\top$ and $1_{\{a_\tau=0\}}$ for each $\tau$, which changes the limit of $\sum_{{\tau}\in[t]}1_{\{a_{\tau} = a\}}(\tilde\xb_{\tau}\tilde\xb_{\tau}^\top - \bSigma_e)$ (and similarly $\sum_{{\tau}\in[t]}1_{\{a_{\tau} = a\}}\tilde\xb_{\tau}r_{\tau}$). This leads to the inconsistency of the naive estimator (See Appendix \ref{appendix-failure-of-naive-estimator} for a concrete example). In Section \ref{section:simulations}, we provide examples to show that (\ref{eq:naive-estimator}) not only deviates from the true parameters, but also leads to bad decision-making.

\noindent\textbf{Our proposed estimator.}
%\sam{in the algorithm we include $T_0$ a pure exploration phase.  Do not include this if the proof does not require it.  If you include a pure exploration phase then the reader will think it is required for the regret bounds... and then you need an assumption on the duration of the exploration phase.}  
Inspired by the above observations, for $\pi_\tau(a |\tilde\xb_\tau, \cH_{\tau-1})$ positive, we construct the following estimator for $\btheta^*_a$ given $\cH_t$, which corrects (\ref{eq:naive-estimator}) using importance weights:
\begin{align}\label{eq:proposed-estimator}
\hat\btheta_a^{(t)}:= \bigg(\hat\bSigma_{\tilde\xb, a}^{(t)} - 
\frac{1}{t}\sum\nolimits_{\tau\in[t]}\pi^{nd}_\tau(a)\bSigma_{e, \tau}\bigg)^{-1}\cdot
\hat\bSigma_{\tilde\xb, r, a}^{(t)},
\end{align}
where $$
\hat\bSigma_{\tilde\xb, a}^{(t)}=\frac{1}{t}\sum_{\tau\in[t]}\frac{\pi_{\tau}^{nd}(a_\tau)}{\pi_\tau(a_\tau|\tilde\xb_\tau, \cH_{\tau-1})}1_{\{a_\tau = a\}}\tilde\xb_\tau\tilde\xb_{\tau}^\top,\quad
\hat\bSigma_{\tilde\xb, r, a}^{(t)}=\frac{1}{t}\sum_{\tau\in[t]}\frac{\pi_{\tau}^{nd}(a_\tau)}{\pi_\tau(a_\tau|\tilde\xb_\tau, \cH_{\tau-1})}1_{\{a_\tau = a\}}\tilde\xb_\tau r_\tau.
$$
Here, $(\pi_\tau^{nd}(\cdot))_{\tau\in[t]}$ is a pre-specified 
policy (i.e. not data-dependent) that can be chosen by the algorithm. We provide theoretical guarantees for $\hat\btheta_a^{(t)}$ under the following assumptions: 

\begin{assumption}[Boundedness]\label{ass:boundedness}
$\forall t\in[T]$, $\|\tilde\xb_t\|_2\leq 1$; There exists a positive constant $R_\theta$ such that $\forall a\in\{0, 1\}$, $\|\btheta_a^*\|_2\leq R_\theta$; There exists a positive constant $R$ such that $\forall t\in[T]$, $|r_t|\leq R$. 
\end{assumption}

\begin{assumption}[Minimum eigenvalue]\label{ass:min-signal}
%$\{\pi_\tau^{nd}(a)\}_{\tau, a}$ is chosen such that 
There exists a positive constant $\lambda_{0}$ such that $\forall t\geq \sqrt{T}$, $a\in\{0, 1\}$, $\frac{1}{t}\sum_{\tau\leq t}\pi_\tau^{nd}(a)\xb_\tau\xb_\tau^\top \succeq \lambda_{0}\bI_d$.
\end{assumption}

\begin{remark}\label{rmk:ass:min-signal}
Assumption \ref{ass:min-signal} is mild. Even restricted to the choice of $\pi_\tau^{nd}(a)\equiv 1/2$, under mild conditions, the assumption can be satisfied with deterministic $(\xb_\tau)_{\tau\geq 1}$ or stochastic $(\xb_\tau)_{\tau\geq 1}$ such as an i.i.d. sequence, a weakly dependent stationary time series (e.g. multivariate ARMA process \citep{fan2017elements}), or a sequence with periodicity/seasonality with high probability (See Appendix \ref{pf:thm:theta-estimation} for details).
\end{remark}
The theorem below gives a high-probability upper bound on $\|\hat\btheta_a^{(t)}-\btheta_a^*\|_2$ (proof in Appendix \ref{pf:thm:theta-estimation}).%  \sam{the theorem does not require sub-Gaussianity of noise?  Can the noise distribution have infinite variance?}

\begin{theorem}\label{thm:theta-estimation}
For any $t\in[T]$, denote $q_t\!:=\inf_{\tau\leq t, a\in\{0, 1\}}\pi_\tau(a|\tilde\xb_\tau, \cH_{\tau-1})$. Then under Assumptions \ref{ass:boundedness} and \ref{ass:min-signal}, there exist constants $C$ and $C_1$ such that as long as $q_t\geq \frac{C_1}{\lambda_{0}\wedge \lambda_{0}^2}\frac{d+\log t}{t}$, with probability at least $1\!-\!\frac{8}{t^2}$,
\begin{equation}\label{eq:thm:theta-estimation}
    \|\hat\btheta_a^{(t)}-\btheta_a^*\|_2\leq \frac{C(R+R_{\btheta})}{\lambda_{0}}\max\bigg\{\frac{d\!+\!\log t}{q_tt}, \sqrt{\frac{d\!+\!\log t}{q_tt}}\bigg\}, \quad \forall a\in\{0, 1\}.
\end{equation}
\end{theorem}
\begin{remark}
If $q_tt\gg d$, Theorem \ref{thm:theta-estimation} indicates that $\|\hat\btheta_a^{(t)}-\btheta_a^*\|_2\lesssim \sqrt{\frac{d}{q_t t}}$ ignoring logarithmic terms. This result intuitively aligns with the typical rate observed in statistical estimation, considering that the minimum `effective sample size' for an action is $q_t t$. 
%If we fix $t$ and consider all policies such that $q_t\geq p_0$ for some positive constant $p_0$, then (\ref{eq:thm:theta-estimation}) also matches the minimax lower bound up to logarithmic terms among all such policies (See the discussion in Appendix F). 
However, in general, Theorem \ref{thm:theta-estimation} allows $q_t$ to go to zero as fast as the rate $\frac{d+\log t}{t}$.
\end{remark}
Unlike the existing literature on off-policy learning in contextual bandits (e.g. \cite{wang2017optimal, zhan2021off, zhang2021statistical, bibaut2021post}), the role of the importance weights here is to correct the dependence of a policy on the observed noisy context with error. The proof idea can be generalized to a large class of off-policy method-of-moment estimators, which might be of independent interest (See Appendix \ref{pf:thm:theta-estimation}).

\subsection{\texttt{MEB}: Online bandit algorithm with measurement error adjustment}\label{section:meb}

\begin{algorithm}[t]
	\caption{\texttt{MEB} (Measurement Error Bandit)}	
	\label{alg1}
	\begin{algorithmic}[1]		
		\STATE \textbf{{Input}}: $(\bSigma_{e, t})_{t\in[T]}$: covariance sequence of $(\bepsilon_t)_{t\in[T]}$; $(p_0^{(t)})_{t\in[T]}$: minimum selection probability at each time $t$; $T_0$: length of pure exploration
        \FOR{time $t = 1, 2, \ldots, T$}
        \IF{$t\leq T_0$}
        \STATE Sample $a_t\sim \pi_t(\cdot|\tilde\xb_t, \cH_{t-1})$, where $\pi_t(a|\tilde\xb_t, \cH_{t-1}) = 1/2$ for all $a\in\{0, 1\}$
        \ELSE
        \STATE Obtain updated estimators $(\hat\btheta_{a}^{(t-1)})_{a\in\{0, 1\}}$ from (\ref{eq:proposed-estimator})
        \STATE $\tilde a_t \leftarrow \argmax_{a\in\{0, 1\}}\langle\hat\btheta_{a}^{(t-1)}, \tilde\xb_t\rangle$
        \STATE Sample $a_t\sim \pi_t(\cdot|\tilde\xb_t, \cH_{t-1})$, where $\pi_t(a|\tilde\xb_t, \cH_{t-1}):=
        \begin{cases}
        1-p_{0}^{(t)}, \quad\text{if }a = \tilde a_t\\
        p_{0}^{(t)}, \quad\text{otherwise}
        \end{cases}
        $
        \ENDIF
        \ENDFOR
	\end{algorithmic}
\end{algorithm}

In this section, we propose \texttt{MEB} (Measurement Error Bandit), an online bandit algorithm with measurement error adjustment based on the estimator (\ref{eq:proposed-estimator}). The algorithm is presented in Algorithm \ref{alg1} and is designed for the binary-action setting, although it can be generalized to the case with multiple actions (see Appendix \ref{appendix:generalization-to-K-actions}). At each time $t$, given the noisy context $\tilde\xb_t$, the algorithm computes the best action $\tilde a_t$ according to $(\hat\btheta_a^{(t-1)})_{a\in\{0, 1\}}$ calculated from (\ref{eq:proposed-estimator}). Then, it simply samples $\tilde a_t$ with probability $1-p_0^{(t)}$ and keeps an exploration probability of $p_0^{(t)}$ to sample the other action. Here, $p_0^{(t)}\in(0, \frac{1}{2})$. In many common scenarios, we can often set $p_0^{(t)}$ to be monotonically decreasing in $t$, in which case $q_t\!=\inf_{\tau\leq t, a\in\{0, 1\}}\pi_\tau(a|\tilde\xb_\tau, \cH_{\tau-1}) = p_0^{(t)}$ for all $t\in[T]$.
%\sam{above we used $q_t$ instead of $p_0^{(t)}$.  I suggest to keep notation consistent and then in next paragraph only consider $\{q_t\}_{t\ge 1}$ that are bounded below by $p_0$.  Would be much cleaner..}

Below, we focus on policies that always maintain an exploration probability of at least $p_0$ where $p_0>0$. As discussed in Section \ref{section:benchmark-policy}, these policies are used in many real-life applications in order to cope with potential future non-stationarity and to perform efficient after-study analysis. In this setting, it's natural to choose $p_0^{(t)}\equiv p_0$ %\sam{woudl be cleaner to assume that $\{q_t\}_{t\ge 1}$ are bounded below by $p_0$.  } 
in Algorithm \ref{alg1}, and we consider the regret compared to the clipped benchmark policy (\ref{eq:oracle-policy}). In Appendix \ref{appendix:usual-benchmark}, we show that with other hyperparameter choices, \texttt{MEB} achieves a regret upper bound of $\tilde \cO(T^{2/3})$ with respect to the horizon $T$ compared to the non-clipped benchmark (\ref{eq:nonclipped-oracle-policy}).

We first prove the following theorem, which states that the regret of \texttt{MEB} can be directly controlled by the estimation error. In fact, this theorem holds regardless of the form or quality of the estimation procedure (i.e. in line 6 of Algorithm \ref{alg1}). The proof is in Appendix \ref{pf:cor:alg1-with-theta-estimation}.

\begin{theorem}\label{thm-regret}
Given Assumption \ref{ass:boundedness}, the regret of Algorithm \ref{alg1} satisfies
$$
\text{Regret}(T; \pi^\dagger)\leq 2T_0R_{\theta}+2(1-2p_0)\sum\nolimits_{t\in(T_0, T]}\max\nolimits_{a\in\{0, 1\}}\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2.
$$
\end{theorem}
The following corollary provides the regret guarantee of \texttt{MEB} by combining Theorem \ref{thm:theta-estimation} and \ref{thm-regret} (Proof in Appendix \ref{pf:cor:alg1-with-theta-estimation}).

\begin{corollary}\label{cor:alg1-with-theta-estimation}
There exist constants $C, C'$ such that by choosing $T_0 = \lceil2\sqrt{T}\rceil$, for any $T$ satisfying $T\geq C'\max\{1, \frac{d^2+\log^2({T})}{p_0^2}, \frac{d^2+\log^2({T})}{\lambda_{0}^2(1\wedge \lambda_{0}^2)p_0^2}\}$, with probability at least $1\!-\!\frac{16}{\sqrt{T}}$, the regret of Algorithm \ref{alg1}: 
$$
\text{Regret}(T; \pi^\dagger)\leq C\cdot \left\{R_\theta\sqrt{T}+ \frac{(R\!+\!R_\theta)(1\!-\!2p_0)}{\lambda_{0}\sqrt{p_0}}\sqrt{T({d}\!+\!{\log T})}\right\}.
$$
\end{corollary}
Ignoring other factors, the regret upper bound is of order $\cO(\sqrt{T(d\!+\!\log T)})$ depending on the horizon $T$ and dimension $d$.

%\begin{remark}\label{remark:S}
In certain scenarios (e.g. when $d$ is large), it is desirable to save computational resources by updating the estimates of $(\btheta_a^*)_{a\in\{0, 1\}}$ less frequently in Algorithm \ref{alg1}. Fortunately, low regret guarantees can still be achieved: Suppose at each time $t$, the agent only updates the estimators according to (\ref{eq:proposed-estimator}) at selected time points $t\in\cS\subseteq[T]$ (in line 6); Otherwise, the agent simply makes decisions based on the most recently updated estimators. In Appendix \ref{pf:cor:alg1-with-theta-estimation}, we show that time points to perform the updates can be very infrequent, such as $(n^k)_{k\in\mathbb N^+}$ ($n\geq 2, n\in\mathbb N^+$), while still achieving the same rate of regret upper bound as in Corollary \ref{cor:alg1-with-theta-estimation}.
%\end{remark}

\subsection{Results given estimated error variance}\label{section:estimated-Sigma-e}

In practice, the agent might not have perfect knowledge about $\bSigma_{e, t}$, the variance of the error $\bepsilon_t$. In this section, we assume that at each time $t$, the agent has a (potentially adaptive) estimator $\hat\bSigma_{e, t}$ that is close to $\bSigma_{e, t}$. Then, it is natural to replace $\bSigma_{e, t}$ with $\hat\bSigma_{e, t}$ in (\ref{eq:proposed-estimator}) to obtain the estimator for $\btheta_a^*$ at time $t$: 
\begin{equation}
\tilde\btheta_a^{(t)}:= \big(\hat\bSigma_{\tilde\xb, a}^{(t)} - 
\frac{1}{t}\sum\nolimits_{\tau\in[t]}\pi^{nd}_\tau(a)\hat\bSigma_{e, \tau}\big)^{-1}\cdot
\hat\bSigma_{\tilde\xb, r, a}^{(t)}.
\end{equation} 
The theorem below shows the consistency guarantees for $\tilde\btheta_a^{(t)}$ (Proof in Appendix \ref{appendix:estimated-error-variance}). An essential quantity in the theorem is defined as $\bDelta_t(a):= \frac{1}{t}\sum_{\tau\in[t]}\pi_\tau^{nd}(a)(\hat\bSigma_{e, \tau} - \bSigma_{e, \tau})$ for $t\in[T]$, $a\in\{0, 1\}$.

\begin{theorem}\label{thm:theta-estimation-estimated-Sigma}
Recall that $q_t\!:=\inf_{\tau\leq t, a\in\{0, 1\}}\pi_\tau(a|\tilde\xb_\tau, \cH_{\tau-1})$. Then under Assumptions \ref{ass:boundedness} and \ref{ass:min-signal}, there exist constants $C$ and $C_1'$ such that as long as $q_t\geq \frac{C_1'}{\min\{\lambda_{0}, \lambda_{0}^2\}}\frac{d+\log t}{t}$ and $\max_{a\in\{0, 1\}}\|\bDelta_t(a)\|_2\leq \frac{\lambda_{0}}{4}$, with probability at least $1\!-\!\frac{8}{t^2}$,
\begin{equation}\label{eq:thm:theta-estimation-estimated-Sigma}
    \|\tilde\btheta_a^{(t)}-\btheta_a^*\|_2\leq \frac{C(R+R_{\btheta})}{\lambda_{0}}\max\bigg\{\frac{d\!+\!\log t}{q_tt}, \sqrt{\frac{d\!+\!\log t}{q_tt}}\bigg\} + \frac{2R_\theta}{\lambda_{0}}\|\bDelta_t(a)\|_2, \quad \forall a\in\{0, 1\}.
\end{equation}
\end{theorem}

By plugging in $\tilde\btheta_a^{(t)}$ instead of $\hat\btheta_a^{(t)}$ as estimators in Algorithm \ref{alg1} (line 6), we have the following regret bound combining Theorems \ref{thm-regret} and \ref{thm:theta-estimation-estimated-Sigma} (Proof in Appendix \ref{appendix:estimated-error-variance}). 
 
\begin{corollary}\label{cor:alg1-with-theta-estimation-estimated-Sigma}

Suppose that $\max_{t\in[\sqrt{T}, T]}\max_{a\in\{0, 1\}}\|\bDelta_t(a)\|_2\leq \frac{\lambda_{0}}{4}$. Then there exist constants $C$, $C''$ such that by choosing $T_0 = \lceil2\sqrt{T}\rceil$, for any $T$ satisfying $T\geq C''\max\{1, \frac{d^2+\log^2({T})}{p_0^2}, \frac{d^2+\log^2({T})}{\lambda_{0}^2(1\wedge \lambda_{0}^2)p_0^2}\}$, with probability at least $1-\frac{16}{\sqrt{T}}$, Algorithm \ref{alg1} with $\tilde\btheta_a^{(t)}$ as plug-in estimators achieves:
\begin{align}
\text{Regret}(T; \pi^\dagger)\leq &\thickspace C\cdot \left\{R_\theta\sqrt{T}+ \frac{(R\!+\!R_\theta)(1\!-\!2p_0)}{\lambda_{0}\sqrt{p_0}}\sqrt{T({d}\!+\!{\log T})}\right\}\nonumber\\
&+ \frac{4(1-2p_0)R_\theta}{\lambda_{0}}\sum\nolimits_{t\in[2\sqrt{T}-1, T)\cap \mathbb N}\max\nolimits_{a\in\{0, 1\}}\|\bDelta_{t}(a)\|_2.\label{eq:alg1-with-theta-estimation-estimated-Sigma}
\end{align}
\end{corollary}

Theorem \ref{thm:theta-estimation-estimated-Sigma} and Corollary \ref{cor:alg1-with-theta-estimation-estimated-Sigma} show that both $\tilde\btheta_a^{(t)}$ and Algorithm \ref{alg1} behave nicely if $\bDelta_t(a)$, the weighted average of the estimation errors $(\hat\bSigma_{e, \tau} - \bSigma_{e, \tau})_{\tau\in[t]}$ is small for $t\geq\sqrt{T}$. This is not a particularly high requirement: For example, suppose the agent gathers more data over time so that $\|\hat\bSigma_{e, t} - \bSigma_{e, t}\|_2\lesssim \frac{1}{\sqrt{t}}$, then it is easy to see that the last term in the regret bound (\ref{eq:alg1-with-theta-estimation-estimated-Sigma}) $\lesssim \frac{(1-p_0)R_\theta}{\lambda_{0}}\sqrt{T}$.

\section{Simulation results}\label{section:simulations}

In this section, we set $d = 5$, $T = 20000$. In the reward model, let $\btheta_0^* = (0, 1, -1, 1, -1)$, $\btheta_1^* = (1, 0, 0, 0, 0)$, and $\eta_t$ drawn i.i.d. from $\cN(0, 1)$. Let $(\xb_t)_{t\in[T]}$ be independently sampled from distribution $\cN(\bmu_x, \bI_d)$, where $\bmu_x = \mathbf{1}_d$. We further set $\bSigma_{e, t}\equiv \bSigma_e:=\frac{1}{4}\bI_d$, and consider independent $(\bepsilon_t)_{t\in[T]}$ with two distribution settings: (i) Normal distribution; (ii) Multivariate $t(3)$ distribution with covariance $\bSigma_e$ (which is a typical heavy-tailed distribution). For each setting, we independently generate bandit data for $n_{exp} = 500$ times, and compare among several candidate algorithms in terms of both estimation quality and cumulative regret with a moderate exploration probability $p_0 = 0.1$ and a small exploration probability $p_0 = 0.01$. The algorithms for comparison include: Thompson sampling given normal priors \citep{russo2018tutorial} (with the constraint of exploration probability at least $p_0$ at each time $t$), \texttt{MEB} (Algorithm \ref{alg1}), and \texttt{MEB-naive} (\texttt{MEB} plugged in with the naive measurement error estimator (\ref{eq:naive-estimator}) instead of (\ref{eq:proposed-estimator})). 

% Figure environment removed

Figure \ref{fig:simulations} shows the estimation error ($\|\hat\btheta_0^{(t)}-\btheta_0^*\|_2$)\footnote{Since the results of $\|\hat\btheta_1^{(t)}-\btheta_1^*\|_2$ is similar to $\|\hat\btheta_0^{(t)}-\btheta_0^*\|_2$, we omit them here. In fact, estimating $\btheta_0^*$ is more challenging in this setting since any algorithm with low regret should choose action 0 less often. For Thompson sampling, we take the posterior mean to be the estimators for model parameters.} as well as the cumulative regret of the algorithms with two choices of $p_0$ under different settings of $\bepsilon_t$ distributions. Across all scenarios, \texttt{MEB} is the only algorithm that produces accurate model estimates and achieves sublinear regret. Thompson sampling fails to make optimal decisions by disregarding the presence of the noise $(\bepsilon_t)_{t\geq 1}$ in the observed context. Notably, for \texttt{MEB-naive}, we see that the inconsistent model estimators (\ref{eq:naive-estimator}) also lead to poor decision-making, as the regret of \texttt{MEB-naive} blows up in all four settings. This is especially the case when the minimum exploration probability $p_0$ is small. Moreover, the superior performance of \texttt{MEB} is robust to the heavy-tailedness of $(\bepsilon_t)_{t\geq 1}$, as demonstrated by the right two columns in Figure \ref{fig:simulations}. We further conduct experiments with different levels of measurement errors and observe similar findings (More details in Appendix \ref{appendix-simulation+}).

Despite the fact that the naive estimator (\ref{eq:naive-estimator}) is inconsistent and can perform even worse than the Thompson sampling estimator which completely ignores the measurement errors, it's interesting to notice that, when  $p_0$ is small, in the earlier rounds of the online algorithm, the naive estimator exhibits a smaller estimation error compared to the other two algorithms. This phenomenon may arise because, in situations with limited data, the variance of the estimator (\ref{eq:proposed-estimator}) with importance weights outweighs the bias in the naive estimator. In practice, we can potentially mitigate this issue in two ways: (i) To use variance reduction techniques \citep{robins2007comment} to enhance the estimator (\ref{eq:proposed-estimator}); (ii) To fine-tune the sampling rate $(p_0^{(t)})_{t\geq 1}$ or switch between estimators during the algorithm for best performance. See Appendix \ref{appendix-simulation+} for more explanations.

\section{Discussion and conclusions}

We propose a new algorithm, \texttt{MEB}, which is the first algorithm with sublinear regret guarantees in contextual bandits where the context is observed with noise, and we have limited knowledge of the noise distribution. This setting is very common in practice, especially where only predictions for unobserved context are available.
%the true context for decision-making can only be detected or learned approximately from observable auxiliary data. 
\texttt{MEB} leverages the novel estimator (\ref{eq:proposed-estimator}), which extends the conventional measurement error adjustment techniques by considering the interplay between the policy and the measurement error. 

\noindent\textbf{Limitations and future directions.} Several questions remain for future investigation. First, is $\cO(T^{2/3})$ the optimal rate of regret compared to the standard benchmark policy (\ref{eq:oracle-policy}), as in some other bandits with semi-parametric reward model (e.g. \cite{xu2022towards})? Providing lower bounds on the regret helps us understand the limit of improvement in the online algorithm. Second, in our work, we assume that the agent has an unbiased prediction of the true context. It is important to understand how biased predictions affect the results, as in practice, machine learning algorithms may well generate biased predictions. Last but not least, all analyses in this paper are restricted to bandits, where future contexts are not affected by previous actions. It's interesting to see how we can extend our method to more complicated decision-making settings (e.g. Markov decision processes). 

\section{Societal impact}
This work represents the first steps toward using predictions of unobserved context in an online reinforcement learning algorithm in digital intervention in clinical trials.  This work is intended to lead to the broader impact of helping health scientists develop more effective digital interventions. The biggest potential negative impact of this work would be a poor, careless implementation in real life: For example, an implementation of this algorithm (or a generalization of this algorithm) that is unstable and thus leads to patient disengagement from care and a decrease in the patient's confidence in the ability of the health care system to help them.  

