\section{Simulation results}\label{section:simulations}

In this section, we complement our theoretical analyses with simulation results on a synthetic environment with artificial noise and reward models as well as a simulation environment based on the real dataset, HeartStep V1 \citep{10.1093/abm/kay067}.

\paragraph{Compared algorithms.} In both simulation environments, we compare the following algorithms: Thompson sampling (\texttt{TS}) with normal priors \citep{russo2018tutorial}, Linear Upper Confidence Bound (\texttt{UCB}) approach \citep{chu2011contextual}, \texttt{MEB} (Algorithm \ref{alg1}), and \texttt{MEB-naive} (\texttt{MEB} plugged in with the naive measurement error estimator (\ref{eq:naive-estimator}) instead of (\ref{eq:proposed-estimator})). See Appendix \ref{appendix-simulation+} for a detailed description of the algorithms.

\subsection{Synthetic environment}

We first test our algorithms on a synthetic environment. We consider a contextual bandit environment with $d = 5$, $T = 50000$. In the reward model, we set $\btheta_0^* = (5, 6, 4, 6, 4)$, $\btheta_1^* = (6, 5, 5, 5, 5)$, and $\eta_t$ drawn i.i.d. from $\mathcal{N}(0, \sigma_{\eta}^2)$. Let $(\xb_t)_{t\in[T]}$ be independently sampled from $\mathcal{N}(\bmu_{x}, \bI_d)$, where $\bmu_x = \one_d$. We further set $\bSigma_{e, t} \equiv \bSigma_e \coloneqq \bI_d / 4$ and consider independent $(\bepsilon_{t})_{t\in[T]}$ with Normal distribution%; (ii) Multivariate $t(3)$ distribution 
with covariance $\bSigma_{e}$. We independently generate bandit data for $n_{exp} = 100$ times, and compare among the candidate algorithms in terms of estimation quality and cumulative regret with a moderate exploration probability $p_0 = 0.2$. 

% Table \ref{tab:synthetic} (a) shows the average regret (cumulative regret divided by $T$) over 50000 steps under under $\sigma^2_{\eta} \in \{0.01, 0.1, 1.0\}$ and $\sigma_{\epsilon}^2 \in \{0.1, 1.0, 2.0\}$. \texttt{MEB} showed significantly smaller average regret compared to other baseline methods under all choices of $\sigma^2_{\eta}$ and $\sigma_{\epsilon}^2$. An L2 estimation error plot can be found in Appendix Figure \ref{fig:synthetic_error}, which also demonstrates that \texttt{MEB} has lower estimation error. In general, a higher $\sigma_{\epsilon}^2$ leads to larger context noise leading to a larger benefit of using \texttt{MEB}.

% % Figure environment removed

% \begin{table}[h]
%     \centering
%     \begin{tabular}{c|cccc}
%        Algorithm & \texttt{TS} & \texttt{UCB} & MEB & MEB naive \\
%        \hline
%        Normal    & 2.38   & 2.33    &  0.123   & 1.50 \\
%        $t(3)$    & 1.81   & 1.63    &  0.617   & 1.66
%     \end{tabular}
%     \caption{Simple regret at $T = 10000$ with $p = 0.2$. For each independent run, we calculate the simple regret with 10000 samples. All the numbers are reported with a $\times 10^{-3}$ omitted.}
%     \label{tab:simple_regret}
% \end{table}

\subsection{HeartStep V1 simulation environment}

We also construct a simulation environment with HeartSteps dataset. HeartSteps is a physical activity mobile health application, whose primary goal is to help the user prevent negative health outcomes and adopt and maintain healthy behaviors, for example, higher physical activity level. HeartSteps V1 is a 42-day mobile health trial \citep{dempsey2015randomised,klasnja2015microrandomized,liao2016sample}, where participants are provided a Fitbit tracker and a mobile phone application. One of the intervention components is a contextually-tailored physical activity suggestion that may be delivered at any of the five user-specified times during each day. The delivery times are roughly separated by 2.5 hours.

% [TODO: mention data sensible consent in the appendix]

\paragraph{Construction of the simulated environment.} We follow the simulation setups in \cite{liao2020personalized}.  The true context at the time $t$ is denoted by $\xb_t$ with three main components $\xb_t = (I_t, Z_t, B_t)$. Here, $I_t$ is an indicator variable of whether an intervention ($A_t = 1$) is feasible (e.g. $I_t$ is 0 when the participant is driving a car, a situation where the suggestion should not be sent). $Z_t$ contains some features at time $t$. $B_t$ is the true treatment burden, which is a function of the participant's treatment history\footnote{Note this violates contextual bandits assumption and leads to an MDP. We believe this is a good setup to test the robustness of our proposed approach.}. Specifically, $B_{t+1} = \lambda B_t + 1_{\{A_{t} = 1\}}$. We assume that $(I_t)_{t\in[T]}$ and $(Z_t)_{t\in[T]}$ are sampled i.i.d with the empirical distribution from the Heartstep V1 dataset, and $(B_t)_{t\in[T]}$ is given by the aforementioned transition model.

The reward model is 
$
    r_t(\xb, a; \btheta) = \xb^{\top} \balpha + a f(\xb)^{\top} \bbeta + \eta_t,
$
where $\xb$ is the full context, $f(\xb)$ is a subset of $\xb$ that is considered to have an impact on the treatment effects, and $\btheta = (\balpha^\top, \bbeta^\top)^\top \in \mathbb{R}^9$. Here $\eta_t$ is the Gaussian noise on the reward observation, whose variance $\sigma_{\eta}^2$ is chosen to be $0.1, 1.0$, and $5.0$ respectively \citep{liao2016sample}. For a detailed list of variables in the context, see Table \ref{tab:variables} in Appendix \ref{appendix-simulation+}.

The true parameters $(\btheta_a^*)_{a\in\{0,1 \}}$ is estimated from GEE (Generalized Estimating Equations) with rewards being the log-transformed step count collected 30 minutes after the decision time.

In light of the measurement error setting in this paper, we consider an observation noise on $B_t$ for the following reasons: 1) The burden $B_t$ can be understood as a prediction of the burden level of the participant, which is particularly crucial in mobile health studies; 2) Other variables are normally believed to have low or no observation noise. Thus, we assume that the agent only observes
$
\tilde{\xb}_t = (I_t, Z_t, \tilde{B}_t),
$
where $\tilde{B}_t = B_t + \epsilon_t$ and $\epsilon_t$ is drawn i.i.d. from normal distribution with mean zero and variance $\sigma_{\epsilon}^2$.

\subsection{Results} Table \ref{tab:synthetic} (a) and (b) shows the average regret (cumulative regret divided by $T$) in both the synthetic environment and the real-data environment based on HeartStep V1. We use the same set of $\sigma_{\epsilon}^2 \in \{0.1, 1.0, 2.0\}$, while different $\sigma^2_{\eta}$ reflect the change of absolute values in coefficients in two different environments ($\sigma_{\eta}^2 = 5.0$ is the level of reward noise in HeartStep V1). \texttt{MEB} shows significantly smaller average regret compared to other baseline methods under most combinations of $\sigma^2_{\eta}$ and $\sigma_{\epsilon}^2$. An estimation error plot can be found in Appendix \ref{appendix-simulation+},
%Figure \ref{fig:synthetic_error} and \ref{fig:real-data_error}
which also demonstrates that \texttt{MEB} has a lower estimation error.

\begin{table}[ht]
\centering
\caption{Average regret for both synthetic environment and real-data environment under different combinations of $\sigma_{\eta}^2$ and $\sigma_{\epsilon}^2$. The results are averages over 100 independent runs and the standard deviations are reported in the full table in Appendix \ref{appendix-simulation+}.}
\begin{subtable}[h]{0.5\textwidth}
\caption{Average regret in the synthetic environment over $50000$ steps with clipping probability $p = 0.2$. }
\begin{tabular}{ll|llll}
$\sigma_{\eta}^2$ & $\sigma_{\epsilon}^2$ & \texttt{TS} & \texttt{UCB} & \texttt{MEB} & \texttt{MEB-naive} \\
\hline
\hline
0.01 & 0.1 & 0.047 & 0.046 & \textbf{0.027} & 0.038 \\
0.1  & 0.1 & 0.047 & 0.047 & \textbf{0.026} & 0.039 \\
1.0  & 0.1 & 0.048 & 0.048 & \textbf{0.027} & 0.038 \\
% 0.01 & 0.5 & 0.301 & 0.310 & \textbf{0.099} & 0.221 \\
% 0.1  & 0.5 & 0.328 & 0.320 & \textbf{0.110} & 0.221 \\
% 1.0  & 0.5 & 0.299 & 0.301 & \textbf{0.119} & 0.224 \\
0.01 & 1.0 & 0.757 & 0.647 & \textbf{0.198} & 0.371 \\
0.1  & 1.0 & 0.769 & 0.721 & \textbf{0.205} & 0.392 \\
1.0  & 1.0 & 0.714 & 0.697 & \textbf{0.218} & 0.404 \\
0.01 & 2.0 & 1.492 & 1.504 & \textbf{0.358} & 0.616 \\
0.1  & 2.0 & 1.195 & 1.333 & \textbf{0.368} & 0.584 \\
1.0  & 2.0 & 1.299 & 1.476 & \textbf{0.416} & 0.625\\
\hline
\hline
\end{tabular}
\end{subtable}

\vspace{3mm}

\begin{subtable}[h]{0.5\textwidth}
\caption{Average regret in the real-data environment over $2500$ steps with clipping probability $p = 0.2$. }
\begin{tabular}{ll|llll}
$\sigma_{\eta}^2$ & $\sigma_{\epsilon}^2$ & \texttt{TS} & \texttt{UCB} & \texttt{MEB} & \texttt{MEB-naive} \\
\hline
\hline
0.05 & 0.1 & 0.027 & 0.027 & \textbf{0.022} & 0.024 \\
0.1  & 0.1 & 0.026 & 0.024 & \textbf{0.020} & \textbf{0.020} \\
5.0  & 0.1 & 1.030 & \textbf{0.743} & 0.831 & 1.173 \\
0.05 & 1.0 & 0.412 & 0.408 & 0.117 & \textbf{0.112} \\
0.1  & 1.0 & 0.309 & 0.316 & \textbf{0.085} & 0.087 \\
5.0  & 1.0 & 1.321 & \textbf{0.918} & 1.458 & 1.322 \\
0.05 & 2.0 & 0.660 & 0.634 & \textbf{0.144} & 0.148 \\
0.1  & 2.0 & 0.740 & 0.704 & \textbf{0.151} & 0.155 \\
5.0  & 2.0 & 1.585 & 2.415 & 1.577 & \textbf{1.436}
\end{tabular}
\end{subtable}
\label{tab:synthetic}
\end{table}

\section{Discussion and conclusions}

We propose a new algorithm, \texttt{MEB}, which is the first algorithm with sublinear regret guarantees in contextual bandits with noisy context, where we have limited knowledge of the noise distribution. This setting is common in practice, especially where only predictions for unobserved context are available.
%the true context for decision-making can only be detected or learned approximately from observable auxiliary data. 
\texttt{MEB} leverages the novel estimator (\ref{eq:proposed-estimator}), which extends the conventional measurement error adjustment techniques by considering the interplay between the policy and the measurement error. 

\noindent\textbf{Limitations and future directions.} Several questions remain for future investigation. First, is $\tilde\cO(T^{2/3})$ the optimal rate of regret compared to the standard benchmark policy (\ref{eq:oracle-policy}), as in some other bandits with semi-parametric reward model (e.g. \cite{xu2022towards})? Providing lower bounds on the regret helps us understand the limit of improvement in the online algorithm. Second, we assume that the agent has an unbiased prediction of the true context. It is important to understand how biased predictions affect the results. %, as in practice, machine learning algorithms may generate biased predictions. 
%Last but not least, all analyses in this paper are restricted to bandits, where future contexts are not affected by previous actions.
Last but not least, it's interesting to see our method can be extended to more complicated decision-making settings (e.g. Markov decision processes).