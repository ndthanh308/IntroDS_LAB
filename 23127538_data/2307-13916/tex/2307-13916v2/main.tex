\section{Introduction}

Contextual bandits \citep{auer2002using, langford2007epoch} represent a classical sequential decision-making problem where an agent aims to maximize cumulative reward based on context information. At each round $t$, the agent observes a context and must choose one of $K$ available actions based on both the current context and previous observations. Once the agent selects an action, she observes the associated reward, which is then used to refine future decision-making. 
Contextual bandits are typical examples of reinforcement learning problems where a balance between exploring new actions and exploiting previously acquired information is necessary to achieve optimal long-term rewards. It has numerous real-world applications including personalized recommendation systems \citep{li2010contextual, bouneffouf2012contextual}, healthcare \citep{yom2017encouraging, liao2020personalized}, and online education \citep{liu2014trading, shaikh2019balancing}. 

Despite the extensive existing literature on contextual bandits, in many real-world applications, the agent never observes the context \emph{exactly}. One common reason is that the true context for decision-making can only be detected or learned approximately from observable auxiliary data. For instance, consider the Sense2Stop mobile health study, in which the context is whether the individual is currently physiologically stressed \citep{battalio2021sense2stop}. A complex predictor of current stress was constructed/validated based on multiple prior studies \citep{cohen1985measuring, sarker2016finding, sarker2017markers}. This predictor was then tuned to each user in Sense2Stop prior to their quit-smoke attempt and then following the user's attempt to quit smoking, at each minute, the predictor inputs high-dimensional sensor data on the user and outputs a continuous likelihood of stress for use by the decision-making algorithm.
%\sam{check to see where this cite belongs! \citep{coughlin2021toward}.}
{In many such applications in health interventions, models using validated predictions as contexts are preferred to raw sensor data because of the high noise in these settings, and that the decision rules are interpretable so they can be critiqued by domain experts.}
%Similarly, in online advertising, the context might be the measure of how likely a user is to buy an item, which cannot be observed. 
The second reason why the context is not observed exactly is because of measurement error. {Contextual variables, such as user preferences in online advertising, content attributes in recommendation systems, and patient conditions in clinical trials, are prone to noisy measurement.} This introduces an additional level of uncertainty that must be accounted for when making decisions.

Motivated by the above, we consider the linear contextual bandit problem where at each round, the agent only has access to a noisy observation of the true context. Moreover, the agent has limited knowledge about the underlying distribution of this noisy observation, as in many practical applications (e.g. the above-mentioned ones). This is especially the case when this `observation' is the output of a complex machine learning algorithm. We only assume that the noisy observation is unbiased, its variance is known or can be estimated, and we put no other essential restrictions to its distribution. This setting is intrinsically difficult for two main reasons: First, when estimating the reward model, the agent needs to take into account the misalignment between the noisy context observation and the reward which depends on the true context. Second, even if the reward model is known, the agent may suffer from making bad decisions at each round because of the inaccurate context. 

\noindent{\textbf{Our contributions.}} We present the first online algorithm \texttt{MEB} (Measurement Error Bandit) with sublinear regret in this setting under mild conditions. \texttt{MEB} achieves $\tilde\cO(T^{2/3})$ regret compare to a standard benchmark and $\tilde\cO(T^{1/2})$ regret compare to a clipped benchmark with minimum exploration probability which is common in many applications \citep{yang2020targeting, yao2021power}. \texttt{MEB} is based on a novel approach to model estimation which removes the systematic bias caused by the noisy context observation. The estimator is inspired by the measurement error literature in statistics \citep{carroll1995measurement, fuller2009measurement}: We extend this classical method with additional tools in the online decision-making setting due to the policy being dependent on the measurement error. 

\subsection{Related work}

Our work complements several lines of literature in contextual bandits, as listed below.

\noindent\textbf{Latent contextual bandit.} In the latent contextual bandit literature \citep{zhou2016latent, sen2017contextual, hong2020latent, hong2020non, xu2021generalized, %hong2022thompson,
nelson2022linearizing, galozy2023information}, the reward is typically modeled as {jointly} depending on the latent state, the context, and the action. %through a parametric model
Several works \citep{zhou2016latent, hong2020latent, hong2020non, galozy2023information} assume no direct relation between the latent state and the context while setting a parametric reward model. For example, \cite{hong2020latent} assumes the latent state $s\in\cS_l$ is unknown but constant over time. %and provides an algorithm with regret $\tilde\cO(\sqrt{T|\cS_l|})$ when the reward model is known. 
\cite{hong2020non} assume that the latent state evolves through a Markov chain. % $\PP_{\phi^*}(\cdot|\cdot)$ and propose algorithms with regret $\cO(T^{2/3}\sqrt{|\cS_l|(1+pT)})$. Here $p = 1-\min_{s\in\cS}\PP_{\phi^*}(s|s)$. 
%\cite{zhou2016latent} considers a sequence of contextual bandits, each with one of $N$ latent states drawn from a distribution. 
\cite{xu2021generalized} sets a specific context as well as the latent feature for each action, and models the reward depending on them through a generalized linear model. Different from the aforementioned studies, we specify that the observed context is a noisy version of the latent context (which aligns with the applications we are addressing), and then leverage this structure to design the online algorithm.

In another line of work, \cite{sen2017contextual, nelson2022linearizing} consider contextual bandit with latent confounder, where the observed context influences the reward through a latent confounder variable, which ranges within a small discrete set. %In \cite{sen2017contextual}, the authors apply a matrix-factorization approach; While in \cite{nelson2022linearizing}, the authors assume additionally that the latent confounder evolves through a Markov process. 
%Under some conditions, the problem can be transformed into a linear contextual bandit. 
Our setting is distinct from these works in that the latent context can span an infinite (or even continuous) space.

\noindent\textbf{Bandit with noisy context.} 
In bandit with noisy context \citep{yun2017contextual, kirschner2019stochastic, yang2020multi, lin2022distributed, lin2022stochastic}, the agent has access to a noisy version of the true context and/or some knowledge of the distribution of the noisy observation. \cite{yun2017contextual} considers the setting where for each time $t$ and action $a$, the true context $z_{a, t}$ is i.i.d. normal. The agent only observes each entry of $z_{a, t} + \epsilon_{a, t}$ with probability $p$, where the error $\epsilon_{a, t}$ has a normal distribution with mean zero. 
%The authors develop algorithms with regret $\tilde\cO(d\sqrt{T}+d^2\sqrt{T/p^3})$ compared to the Bayesian oracle strategy. 
Other works, such as \cite{kirschner2019stochastic, yang2020multi, lin2022stochastic}, assume that the agent knows the exact distribution of the context each time, and observes no context. By assuming a linear reward model, \cite{kirschner2019stochastic} transforms the problem into a linear contextual bandit, and obtains $\tilde\cO(d\sqrt{T})$ regret compared to the policy maximizing the expected reward over the context distribution. \cite{yang2020multi, lin2022stochastic, lin2022distributed} consider variants of the problem such as multiple linear feedbacks, multiple agents, and delayed observation of the exact context. %\cite{lin2022distributed} considers another variant where $M$ collaborating agents know the full distribution of context each time and observe the exact context after some delay. 
Compared to these works, we consider a related but more challenging setting where besides an unbiased noisy observation(prediction) for each context, the agent only knows the second-moment information about the distribution. This does not transform into a standard linear contextual bandit as in \cite{kirschner2019stochastic}.%and significant changes in the algorithm are necessary.

\noindent\textbf{Bandit with inaccurate/corrupted context.} These works consider the setting where the context is simply inaccurate (without randomness), or is corrupted and cannot be recovered. In \cite{yang2021bandit, yang2021robust}, at each round, only an inaccurate context is available to the decision-maker, and the exact context is revealed after the action is taken. %\cite{yang2021robust} considers the robustness objective of maximizing the worst-case reward or minimizing the worst-case regret. 
In \cite{bouneffouf2020online, galozy2020corrupted}, each context $\xb_t$ is completely corrupted with some probability and the corruption cannot be recovered. In \cite{ding2022robust}, the context is attacked by an adversarial agent. Because these works focus more on adversarial settings for the context observations, the application of their regret bounds to our setting generally results in a linear regret. {For example, in \cite{yang2021bandit}, the regret of Thompson sampling is $\tilde\cO(d\sqrt{T} + \sqrt{d}\sum_{t\in[T]}\|\hat\xb_t - \xb_t\|_2)$, where $\|\hat\xb_t - \xb_t\|_2$ is the error of the inaccurate context. As is typical in our setting, $\|\hat\xb_t - \xb_t\|_2$ will be non-vanishing through time, so the second term is linear in $T$.} Given the applications we consider, we can exploit the stochastic nature of the noisy context observations in our algorithm to achieve improved performance.
\subsection{Notations}
Throughout this paper, we use $[n]$ to represent the set $\{1, 2, \ldots, n\}$ for $n\in\mathbb N^+$. For $a, b\in\RR$, let $a\wedge b$ denote the minimum of $a$ and $b$. Given $d\in\mathbb N^+$, $\bI_d$ denotes the $d$-by-$d$ identity matrix, and $\mathbf{1}_d$ denotes the $d$-dimensional vector with 1 in each entry. For a vector $\bv\in\RR^d$, denote $\|\bv\|_2$ as its $\ell_2$ norm. For a matrix $\bM\in\RR^{m\times n}$, denote $\|\bM\|_2$ as its operator norm. The notation $\cO(X)$ refers to a quantity that is upper bounded by $X$ up to constant multiplicative factors, while $\tilde\cO(X)$ refers to a quantity that is upper bounded by $X$ up to poly-log factors.

\section{Measurement error adjustment to bandit with noisy context}

\subsection{Problem setting}\label{sec::setting}

We consider a linear contextual bandit with context space $\cX\subset\RR^d$ and binary action space $\cA = \{0, 1\}$\footnote{For simplicity, we state our results under the binary-action setting, which is common in healthcare \citep{trella2022reward}, economics \citep{athey2017efficient, kitagawa2018should} and other applications. However, all the results presented in this paper can be extended to the setting with multiple actions. See Appendix \ref{appendix:generalization-to-K-actions}.}. Let $T$ be the time horizon. As discussed above, we consider the setting where at each time $t\in[T]$, the agent only observes a noisy version of the context $\tilde\xb_t$ instead of the true underlying context $\xb_t$. Thus, at time $t$, the observation $o_t$ only contains $(\tilde \xb_t, a_t, r_t)$, where $a_t$ is the action and $r_t$ is the corresponding reward. We further assume that $\tilde \xb_t = \xb_t + \bepsilon_t$, where the error $\bepsilon_t$ is independent of the history $\cH_{t-1} := \{o_\tau\}_{\tau\leq t-1}$, $\EE \bepsilon_t = 0$, $\Var(\bepsilon_t) = \bSigma_{e, t}$. {Here, `e' in the subscript means `error'}. Initially we assume that $(\bSigma_{e, t})_{t\geq 1}$ is known. In Section \ref{section:estimated-Sigma-e}, we consider the setting where only estimators of $(\bSigma_{e, t})_{t\geq 1}$ are available. 
%No further restrictions on the distribution of $(\bepsilon_t)_{t\geq 1}$ are made except boundedness. 
There is no restriction that the distribution of $(\bepsilon_t)_{t\geq 1}$ belongs to any known (parametric) family.
The reward $r_t = \langle\btheta_{a_t}^*, \xb_t\rangle + \eta_t$, where $\EE[\eta_t|\cH_{t-1}, \bepsilon_t, a_t] = 0$ and $(\btheta_a^*)_{a\in\cA}$ are the unknown parameters. % \sam{we are not assuming subgaussian tails on $\eta_t$?}
It's worth noting that besides the policy, all the randomness here comes from the reward noise $\eta_t$ and the context error $\bepsilon_t$. We treat $(\xb_t)_{t\geq 1}$ as fixed throughout but unknown to the algorithm (Unlike \cite{yun2017contextual}, we don't assume $\xb_t$ are i.i.d.). Our goal at each time $t$ is to design policy $\pi_t(\cdot|\cH_{t-1}, \tilde\xb_t)\in\Delta(\cA)$ given past history $\cH_{t-1}$ and current observed noisy context $\tilde\xb_t$, so that the agent can maximize the reward by taking action $a_t\sim \pi_t(\cdot|\cH_{t-1}, \tilde\xb_t)$.

If $\bSigma_{e, t}$ is non-vanishing, standard contextual bandit algorithms are generally sub-optimal. {To see this, notice that $r_t = \langle \btheta_{a_t}^*, \xb_t\rangle + \eta_t = \langle \btheta_{a_t}^*, \tilde\xb_t\rangle + (\eta_t - \langle \btheta_{a_t}^*, \bepsilon_t\rangle)$. This means the error in the reward $r_t$ after observing the noisy context is $\eta_t - \langle \btheta_{a_t}^*, \bepsilon_t\rangle$, where $\bepsilon_t$ and $\tilde\xb_t$ are  dependent. Thus, $\EE[r_t|\tilde\xb_t, a_t]\neq \langle \btheta_{a_t}^*, \tilde\xb_t\rangle$. This is in contrast to the standard linear bandit setting, where given the true context $\xb_t$, $\EE[r_t|\xb_t, a_t]= \langle \btheta_{a_t}^*, \xb_t\rangle$, which ensures the sublinear regret of classical bandit algorithms such as UCB and Thompson sampling.} Therefore, it is necessary to design an online algorithm that adjusts for the errors $(\bepsilon_t)_{t\geq 1}$. %To achieve this goal, we must first redefine the regret by comparing to a suitable benchmark policy in this setting. 
We assume that the context, parameters and the reward are bounded, as below.

\begin{assumption}[Boundedness]\label{ass:boundedness}
$\forall t\in[T]$, $\|\tilde\xb_t\|_2\leq 1$; There exists a positive constant $R_\theta$ such that $\forall a\in\{0, 1\}$, $\|\btheta_a^*\|_2\leq R_\theta$; There exists a positive constant $R$ such that $\forall t\in[T]$, $|r_t|\leq R$. 
\end{assumption}

%\subsection{The standard setting and the clipped policy setting}\label{section:benchmark-policy}

For any policy $\pi = (\pi_t)_t$, we define the (standard) cumulative regret as 
\begin{equation}\label{eq:regret}
\text{Regret}(T; \pi^*) = \sum\limits_{t\in[T]} [\EE_{a\sim \pi_t^*}\langle \btheta^*_{a}, \xb_t\rangle - \EE_{a\sim \pi_t}\langle \btheta^*_{a}, \xb_t\rangle], \text{where}
\end{equation}
\begin{equation}\label{eq:nonclipped-oracle-policy}
\pi_t^*(a)=
\begin{cases}
1,\quad &\text{if }a=a_t^*:=\argmax_a\langle \btheta^*_{a}, \xb_t\rangle,\\
0,\quad &\text{otherwise.}
\end{cases}
\end{equation}
We denote the standard benchmark policy $\pi^* = (\pi_t^*)_{t}$. This is summarized in the setting below.

\noindent\textbf{Setting 1.} (Standard setting) We aim to minimize $\text{Regret}(T; \pi^*)$ among the class of all policies.

In many applications, it's desirable to design the policy under the constraint that each action is sampled with a minimum probability $p_0$. One reason for maintaining exploration is that we can update and re-optimize the policy for future users to allow for potential non-stationarity \citep{yang2020targeting}. Additionally, keeping the exploration is also important for after-study analysis \citep{yao2021power}, especially when the goal of the analysis is not pre-specified prior to collecting data with the online algorithm. In these situations, it is desirable to consider only the policies that always maintain an exploration probability of $p_0>0$ for each arm, and compare the performance to the clipped benchmark policy $(\bar\pi_t^*)$:
\begin{equation}\label{eq:oracle-policy}
\bar \pi_t^*(a)=
\begin{cases}
1\!-\!p_0, &\text{if }a=a_t^*,\\%=\argmax_a\langle \btheta^*_{a}, \xb_t\rangle,\\
p_0, &\text{otherwise.}
\end{cases}
\end{equation}
This is summarized in the setting below.

\noindent\textbf{Setting 2.} (Clipped policy setting) We minimize 
\begin{equation*}
\text{Regret}(T; \bar\pi^*) = \sum\limits_{t\in[T]} [\EE_{a\sim \bar \pi_t^*}\langle \btheta^*_{a}, \xb_t\rangle - \EE_{a\sim \pi_t}\langle \btheta^*_{a}, \xb_t\rangle]
\end{equation*}
among the class of policies that explore any action with probability at least $p_0$.

In this work, we will provide policies with sublinear regret guarantees in both settings. 

\subsection{Estimation using weighted measurement error adjustment}\label{section::estimation}

In this section, we focus on learning the reward model parameters $(\btheta_a^*)_{a\in\cA}$ with data $\cH_t$ after a policy $(\pi_\tau(\cdot|\tilde\xb_\tau, \cH_{\tau-1}))_{\tau\in[t]}$ has been executed up to time $t\in[T]$. Learning a consistent model is important in many bandit algorithms for achieving low regret \citep{abbasi2011improved, agrawal2013thompson}. As we shall see in Section \ref{section:meb}, consistent estimation of $(\btheta_a^*)_{a\in\cA}$ plays an essential role in controlling the regret of our proposed algorithm.

\noindent\textbf{Inconsistency of the regularized least-squares (RLS) estimator.} UCB and Thompson sampling, the two classical bandit algorithms, both achieve sublinear regret based on the consistency of the estimator 
$
\hat\btheta_{a, RLS}^{(t)} = \big(\lambda I + \sum_{\tau\in[t]}1_{\{a_{\tau} = a\}}\xb_{\tau}\xb_{\tau}^\top\big)^{-1}\big(\sum_{\tau\in[t]}1_{\{a_{\tau} = a\}}\xb_{\tau}r_{\tau}\big)
$
under certain norms. When $\tilde\xb_\tau = \xb_\tau + \bepsilon_\tau$ is observed instead of $\xb_\tau$, the RLS estimator becomes 
$
\hat\btheta_{a, RLSCE}^{(t)} = \big(\lambda I + \sum_{\tau\in[t]}1_{\{a_{\tau} = a\}}\tilde\xb_{\tau}\tilde\xb_{\tau}^\top\big)^{-1}\big(\sum_{\tau\in[t]}1_{\{a_{\tau} = a\}}\tilde \xb_{\tau}r_{\tau}\big).
$
Here `RLSCE' means the RLS estimator with contextual error. However, when $\bSigma_{e, \tau} = \mathrm{Var}(\bepsilon_\tau)$ is non-vanishing, $\hat\btheta_{a, RLSCE}^{(t)}$ is generally no longer consistent, which may lead to bad decision-making (see Appendix \ref{appendix-failure-of-naive-estimator} for details). In the simple case where $(\xb_\tau, \bepsilon_\tau, \eta_\tau)_{\tau\in[t]}$ are i.i.d. and there is no action (i.e. set $a_\tau\equiv 0$), the inconsistency of $\hat\btheta_{a, RLSCE}^{(t)}$ is studied in the measurement error model literature in statistics \citep{fuller2009measurement, carroll1995measurement}, and is known as \emph{attenuation}.

\noindent\textbf{A naive measurement error adjustment.} A measurement error model is a type of regression model designed to accommodate inaccuracies in the measurement of regressors (i.e., instead of observing $\xb_t$, we observe $\xb_t+\bepsilon_t$ where $\bepsilon_t$ is a noise term with zero mean). As conventional regression techniques yield inconsistent estimators, measurement error models rectify this issue with adjustments to the estimator that consider these errors. In the current context when we want to estimate $\btheta_a^*$ from history $\mathcal H_t$, $(\tilde \xb_\tau)_{\tau\in[t]}$ can be viewed as regressors `measured with error', while $(r_\tau)_{\tau\in[t]}$ are dependent variables. If $(\bepsilon_\tau, \eta_\tau)_{\tau\in[t]}$ are i.i.d., $\bSigma_{e, \tau}\equiv \bSigma_e$, and there is no action (i.e. set $a_\tau\equiv0$), 
$
\hat\btheta_{0, me}^{(t)}:= \big(\frac{1}{t}\sum_{\tau\in[t]}\tilde\xb_\tau\tilde\xb_\tau^\top - \bSigma_e\big)^{-1}\big(\frac{1}{t}\sum_{\tau\in[t]}\tilde\xb_\tau r_\tau\big)
$
is a consistent estimator for $\btheta_0^*$. When multiple actions are present, a naive generalization of the above estimator, $\hat\btheta_{a, me}^{(t)}$, is 
\begin{align}
  \bigg(\sum\limits_{{\tau}\in[t]}1_{\{a_{\tau} = a\}}(\tilde\xb_{\tau}\tilde\xb_{\tau}^\top\! - \!\bSigma_e)\!\bigg)^{-1}\!\!\cdot \bigg(\!\sum\limits_{{\tau}\in[t]}1_{\{a_{\tau} = a\}}\tilde\xb_{\tau}r_{\tau}\bigg). \label{eq:naive-estimator} 
\end{align}
Unfortunately, $\hat\btheta_{a, me}^{(t)}$ is inconsistent in the multiple-action setting, even if the policy $(\pi_\tau)_{\tau\in[t]}$ is \emph{stationary and not adaptive to history}. This difference is essentially due to the interaction between the policy and the measurement error: In the classical measurement error (no action) setting, $\EE(\tilde\xb_\tau\tilde\xb_{\tau}^\top) = \xb_\tau\xb_{\tau}^\top + \bSigma_e$, and $\frac{1}{t}\sum_{\tau\in[t]}\tilde\xb_\tau\tilde\xb_{\tau}^\top - \bSigma_e$ concentrates around its expectation $\frac{1}{t}\sum_{{\tau}\in[t]}\xb_{\tau}\xb_{\tau}^\top$. Likewise, $\frac{1}{t}\sum_{{\tau}\in[t]}\tilde\xb_{\tau}r_{\tau}$ concentrates around $(\frac{1}{t}\sum_{\tau\in[t]}\xb_{\tau}\xb_{\tau}^\top)\btheta_0^*$. Combining the above parts thus yields a consistent estimator of $\btheta_0^*$. In our setting with multiple actions, however, the agent picks the action $a_\tau$ \emph{based on} $\tilde\xb_\tau$, so only certain values of $\tilde\xb_\tau$ lead to $a_\tau = a$. Therefore, for those $\tau\in[t]$ when we pick $a_\tau = a$, it's more likely that $\tilde\xb_\tau$ falls in certain regions depending on the policy, and we shouldn't expect $\EE(\tilde\xb_{\tau}\tilde\xb_{\tau}^\top)=\xb_{\tau}\xb_{\tau}^\top + \bSigma_e$ anymore. In other words, the policy creates a complicated dependence between $\tilde\xb_{\tau}\tilde\xb_{\tau}^\top$ and $1_{\{a_\tau=0\}}$ for each $\tau$, which changes the limit of $\frac1t\sum_{{\tau}\in[t]}1_{\{a_{\tau} = a\}}(\tilde\xb_{\tau}\tilde\xb_{\tau}^\top - \bSigma_e)$ (and similarly $\frac1t\sum_{{\tau}\in[t]}1_{\{a_{\tau} = a\}}\tilde\xb_{\tau}r_{\tau}$). This leads to the inconsistency of the naive estimator (See Appendix \ref{appendix-failure-of-naive-estimator} for a concrete example). In Section \ref{section:simulations}, we provide examples to show that (\ref{eq:naive-estimator}) not only deviates from the true parameters, but also leads to suboptimal decision-making.

\noindent\textbf{Our proposed estimator.}
%\sam{in the algorithm we include $T_0$ a pure exploration phase.  Do not include this if the proof does not require it.  If you include a pure exploration phase then the reader will think it is required for the regret bounds... and then you need an assumption on the duration of the exploration phase.}  
Inspired by the above observations, for $\pi_\tau(a |\tilde\xb_\tau, \cH_{\tau-1})$ positive, we construct the following estimator for $\btheta^*_a$ given $\cH_t$, which corrects (\ref{eq:naive-estimator}) using importance weights:
\begin{align}\label{eq:proposed-estimator}
\hat\btheta_a^{(t)}:= \bigg(\hat\bSigma_{\tilde\xb, a}^{(t)} - 
\frac{1}{t}\sum\nolimits_{\tau\in[t]}\pi^{nd}_\tau(a)\bSigma_{e, \tau}\bigg)^{-1}\!\!\cdot
\hat\bSigma_{\tilde\xb, r, a}^{(t)},
\end{align}
where 
\begin{align*}
\hat\bSigma_{\tilde\xb, a}^{(t)}=\frac{1}{t}\sum_{\tau\in[t]}\frac{\pi_{\tau}^{nd}(a_\tau)}{\pi_\tau(a_\tau|\tilde\xb_\tau, \cH_{\tau-1})}1_{\{a_\tau = a\}}\tilde\xb_\tau\tilde\xb_{\tau}^\top,\\
\hat\bSigma_{\tilde\xb, r, a}^{(t)}=\frac{1}{t}\sum_{\tau\in[t]}\frac{\pi_{\tau}^{nd}(a_\tau)}{\pi_\tau(a_\tau|\tilde\xb_\tau, \cH_{\tau-1})}1_{\{a_\tau = a\}}\tilde\xb_\tau r_\tau.
\end{align*}
Here, $(\pi_\tau^{nd}(\cdot))_{\tau\in[t]}$ is a pre-specified policy (doesn't depend on $(\tilde\xb_\tau)_\tau$ or $\cH_{\tau-1}$) that can be chosen by the algorithm. We only require the following:

\begin{assumption}[Minimum eigenvalue]\label{ass:min-signal}
%$\{\pi_\tau^{nd}(a)\}_{\tau, a}$ is chosen such that 
There exists a positive constant $\lambda_{0}$ such that $\forall t\geq \sqrt{T}$, $a\in\{0, 1\}$, $\frac{1}{t}\sum_{\tau\leq t}\pi_\tau^{nd}(a)\xb_\tau\xb_\tau^\top \succeq \lambda_{0}\bI_d$.
\end{assumption}

\begin{remark}\label{remark::ass-min-signal}
Assumption \ref{ass:min-signal} is mild. Even restricted to the choice of $\pi_\tau^{nd}(a)\equiv 1/2$, under mild conditions, the assumption can be satisfied with deterministic $(\xb_\tau)_{\tau\geq 1}$ or stochastic $(\xb_\tau)_{\tau\geq 1}$ such as an i.i.d. sequence, a weakly dependent stationary time series (e.g. multivariate ARMA process \citep{fan2017elements}), or a sequence with periodicity/seasonality with high probability (See Appendix \ref{pf:thm:theta-estimation} for details).
\end{remark}
The theorem below gives a high-probability upper bound on $\|\hat\btheta_a^{(t)}-\btheta_a^*\|_2$ (proof in Appendix \ref{pf:thm:theta-estimation}).%  \sam{the theorem does not require sub-Gaussianity of noise?  Can the noise distribution have infinite variance?}

\begin{theorem}\label{thm:theta-estimation}
For any $t\in[T]$, denote $q_t\!:=\inf_{\tau\leq t, a\in\{0, 1\}}\pi_\tau(a|\tilde\xb_\tau, \cH_{\tau-1})$. Then under Assumptions \ref{ass:boundedness} and \ref{ass:min-signal}, there exist absolute constants $C$, $C_1$, such that as long as $q_t\geq \frac{C_1}{\lambda_{0}\wedge \lambda_{0}^2}\frac{d+\log t}{t}$, with probability at least $1\!-\!\frac{8}{t^2}$, $\forall a\in\{0, 1\},$
\begin{equation}\label{eq:thm:theta-estimation}
    \|\hat\btheta_a^{(t)}-\btheta_a^*\|_2\leq \frac{C(R+R_{\btheta})}{\lambda_{0}}\max\bigg\{\frac{d\!+\!\log t}{q_tt}, \sqrt{\frac{d\!+\!\log t}{q_tt}}\bigg\}.
\end{equation}
\end{theorem}
\begin{remark}
If $q_tt\gg d$, Theorem \ref{thm:theta-estimation} indicates that $\|\hat\btheta_a^{(t)}-\btheta_a^*\|_2\lesssim \sqrt{\frac{d}{q_t t}}$ ignoring logarithmic terms. This result intuitively aligns with the typical rate observed in statistical estimation, considering that the minimum `effective sample size' for an action is $q_t t$. Note that Theorem \ref{thm:theta-estimation} allows $q_t$ to go to zero as fast as the rate $\frac{d+\log t}{t}$.
\end{remark}
Unlike the existing literature on off-policy learning in contextual bandits (e.g. \cite{wang2017optimal, zhan2021off, zhang2021statistical, bibaut2021post}), the role of the importance weights here is to correct the dependence of a policy on the observed noisy context with error. The proof idea can be generalized to a large class of off-policy method-of-moment estimators, which might be of independent interest (see Appendix \ref{pf:thm:theta-estimation}).

\subsection{\texttt{MEB}: Online bandit algorithm with measurement error adjustment}\label{section:meb}

\begin{algorithm}[t]
	\caption{\texttt{MEB} (Measurement Error Bandit)}	
	\label{alg1}
	\begin{algorithmic}[1]
		\STATE \textbf{{Input}}: $(\bSigma_{e, t})_{t\in[T]}$: variance sequence of $(\bepsilon_t)_{t\in[T]}$; $(p_0^{(t)})_{t\in[T]}$: minimum selection probability at time $t\in[T]$; $T_0$: warm-up stage length
        \FOR{time $t = 1, 2, \ldots, T$}
        \IF{$t\leq T_0$}
        \STATE Set 
       $\pi_t(a|\tilde\xb_t, \!\cH_{t-1}) \!\in\![p_0^{(t)}\!, 1\!-p_0^{(t)}]$, $a\in\{0, 1\}$
        \STATE Sample $a_t\sim \pi_t(\cdot|\tilde\xb_t,\!\cH_{t-1})$
        \ELSE
        \STATE Obtain $(\hat\btheta_{a}^{(t-1)})_{a\in\{0, 1\}}$ from (\ref{eq:proposed-estimator})
        \STATE $\tilde a_t \leftarrow \argmax_{a\in\{0, 1\}}\langle\hat\btheta_{a}^{(t-1)}, \tilde\xb_t\rangle$
        \STATE Set $\pi_t(a|\tilde\xb_t,\!\cH_{t-1}):=
        \begin{cases}
        1-p_{0}^{(t)}, \thickspace\text{if }a = \tilde a_t\\
        p_{0}^{(t)}, \thickspace\text{otherwise}
        \end{cases}
        $
        \STATE Sample $a_t\sim \pi_t(\cdot|\tilde\xb_t,\!\cH_{t-1})$
        \ENDIF
        \ENDFOR
	\end{algorithmic}
\end{algorithm}

We propose \texttt{MEB} (Measurement Error Bandit), an online bandit algorithm with measurement error adjustment based on the estimator (\ref{eq:proposed-estimator}). The algorithm is presented in Algorithm \ref{alg1} and is designed for the binary-action setting, although it can be generalized to the case with multiple actions (see Appendix \ref{appendix:generalization-to-K-actions}). {For $t\leq T_0$, the algorithm is in a warm-up stage and can pick any policy such that there is a minimum sampling probability $p_0^{(t)}$ for each action (Here $p_0^{(t)}\in(0, \frac{1}{2}]$).} {For instance, the algorithm can do pure exploration with $\pi_t(a|\tilde\xb_t, \cH_{t-1})\equiv \frac12$.} For $t>T_0$, given the noisy context $\tilde\xb_t$, the algorithm computes the best action $\tilde a_t$ according to $(\hat\btheta_a^{(t-1)})_{a\in\{0, 1\}}$ calculated from (\ref{eq:proposed-estimator}). Then, it samples $\tilde a_t$ with probability $1-p_0^{(t)}$ and keeps an exploration probability of $p_0^{(t)}$ to sample the other action. In practice, we can often set $p_0^{(t)}$ to be monotonically decreasing in $t$, in which case $q_t\!=\inf_{\tau\leq t, a\in\{0, 1\}}\pi_\tau(a|\tilde\xb_\tau, \cH_{\tau-1}) = p_0^{(t)}$ for all $t\in[T]$.
%\sam{above we used $q_t$ instead of $p_0^{(t)}$.  I suggest to keep notation consistent and then in next paragraph only consider $\{q_t\}_{t\ge 1}$ that are bounded below by $p_0$.  Would be much cleaner..}

Before presenting the regret analysis, we should first note that our problem is harder than a standard contextual bandit: $\xb_t$ is unknown, and only $\tilde\xb_t$ is observed. Thus, even if $(\btheta_a^*)_{a\in\{0, 1\}}$ is known, we may still perform suboptimally if $\tilde\xb_t$ is too far from $\xb_t$ so that it leads to a different optimal action. Example \ref{ex::unknown-xt-linear-regret} below shows that in general, we cannot avoid a linear regret.

\begin{example}\label{ex::unknown-xt-linear-regret}
Let $d = 1$, $(\btheta_1^*, \btheta_0^*) = (1, -1)$. $(\xb_t)_{t\in[T]}$ are drawn i.i.d. from $\{\pm0.2\}$ with equal probability. $\PP(\tilde\xb_t = 1|\xb_t=0.2) = 0.6$, $\PP(\tilde\xb_t = -1|\xb_t=0.2) = 0.4$; $\PP(\tilde\xb_t = 1|\xb_t=-0.2) = 0.4$, $\PP(\tilde\xb_t = -1|\xb_t=-0.2) = 0.6$. Intuitively, even if we know $(\btheta_a^*)_{a\in\{0, 1\}}$, there is still a constant probability at each time $t$ that we cannot make the right choice due to $\tilde \xb_t$ and $\xb_t$ having different signs, and $\xb_t$ is never known (details in Appendix \ref{pf:cor:alg1-with-theta-estimation}). This results in a $\Omega(T)$ regret.
\end{example}

Fortunately, in practice, we expect that the errors $(\bepsilon_t)_{t\in[T]}$ are relatively `small' in the sense that the optimal action (given $(\btheta_a^*)_{a\in\{0, 1\}}$) is not affected. Specifically, we assume the following:

\begin{assumption}\label{ass:small-error}
There exist a constant $\rho\in(0, 1)$ such that $\forall t\in[T]$, 
$|\langle\bm{\delta}_\theta, \bepsilon_t\rangle|\leq \rho|\langle\bm{\delta}_\theta, \xb_t\rangle|$ almost surely. Here $\bm{\delta}_\theta:= \btheta_1^* - \btheta_0^*$.
\end{assumption}

Assumption \ref{ass:small-error} ensures that the perturbation to the suboptimality gap between the two arms caused by $\bepsilon_t$ is controlled by the true suboptimality gap. In this way, given $(\btheta_a^*)_{a\in\{0, 1\}}$, the optimal action based on $\tilde\xb_t$ will not deviate too much from that based on $\xb_t$. As a special case, this assumption is satisfied with if $\forall t$, $|\langle\bm{\delta}_\theta, \xb_t\rangle|\geq {B_{e, t}}\|\bm{\delta}_\theta\|_2/{\rho}$. Here $B_{e, t}$ is an upper bound of $\|\bepsilon_t\|_2$. Assumption \ref{ass:small-error} can be further weakened to the inequalities holding with high probability (see Appendix \ref{pf:cor:alg1-with-theta-estimation}). Note that Assumption \ref{ass:small-error} only guarantees the optimal action is not affected by $(\bepsilon_t)_{t\in[T]}$ \emph{given} $(\btheta_a^*)_{a\in\{0, 1\}}$. To achieve sublinear regret, $(\btheta_a^*)_{a\in\{0, 1\}}$ still needs to be well-estimated. Thus, even with Assumption \ref{ass:small-error}, classical bandit algorithms such as UCB may still suffer from linear regret because of the inconsistent estimator $\hat\btheta_{a, RLSCE}^{(t)}$ (see Appendix \ref{appendix-failure-of-naive-estimator} for a concrete example).

We first prove the following theorem, which states that the regret of \texttt{MEB} can be directly controlled by the estimation error. In fact, this theorem holds regardless of the form or quality of the estimation procedure (i.e. in line 7 of Algorithm \ref{alg1}). The proof is in Appendix \ref{pf:cor:alg1-with-theta-estimation}.

\begin{theorem}\label{thm-regret}
Let Assumption \ref{ass:boundedness} and \ref{ass:small-error} hold.

(i) For the standard setting, Algorithm \ref{alg1} outputs a policy with $\text{Regret}(T; \pi^*)$ no more than 
$$
2T_0R_{\theta}+\frac{2}{1\!-\!\rho}\cdot\!\!\!\sum\limits_{t=T_0\!+1}^T\!\!\!\!\big(p_0^{(t)}R_\theta + \!\!\max\limits_{a\in\{0, 1\}}\!\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2\big).
$$
(ii) For the clipped policy setting, Algorithm \ref{alg1} with the choice of $p_0^{(t)} \equiv p_0$ outputs a policy with $\text{Regret}(T; \bar\pi^*)$ no more than 
$$
2T_0R_{\theta}+\frac{2(1\!-\!2p_0)}{1\!-\!\rho}
\cdot\!\!\!\sum\limits_{t=T_0\!+1}^T\max\limits_{a\in\{0, 1\}}\|\hat\btheta_a^{(t-1)}\!-\!\btheta_a^*\|_2.
$$
\end{theorem}
The following corollary provides regret guarantees of \texttt{MEB} by combining Theorem \ref{thm:theta-estimation} and \ref{thm-regret} (proof in Appendix \ref{pf:cor:alg1-with-theta-estimation}).

\begin{corollary}\label{cor:alg1-with-theta-estimation}
Let Assumption \ref{ass:boundedness} to \ref{ass:small-error} hold. There exist universal constants $C, C'$ such that:

(i) For the standard setting, $\forall T\geq C'\max\{1,$ $ (\frac{d+\log T}{1\wedge\lambda_0\wedge\lambda_0^2})^{\frac94}\}$, with probability at least $1\!-\!\frac{16}{\sqrt{T}}$, Algorithm \ref{alg1} with the choice of $T_0 = \lceil 2T^{\frac23}\rceil$, $p_0^{(t)} = \min\{\frac12, t^{-\frac13}\}$ outputs a policy with $\text{Regret}(T; \pi^*)$ no more than
$$
C T^{\frac23}\bigg\{\frac{R_\theta}{1\!-\!\rho} + \frac{R\!+\!R_{\theta}}{(1\!-\!\rho)\lambda_0}\sqrt{d\!+\!\log T}\bigg\}.
$$

(ii) For the clipped policy setting, $\forall T\geq C'\max\{1, \!\frac{d^2\!+\log^2({T})}{p_0^2}, \!\frac{d^2\!+\log^2({T})}{\lambda_{0}^2(1\wedge \lambda_{0}^2)p_0^2}\}$, with probability at least $1\!-\!\frac{16}{\sqrt{T}}$, Algorithm \ref{alg1} with the choice of $T_0 = \lceil2\sqrt{T}\rceil$ and  $p_0^{(t)}\equiv p_0$ outputs a policy with $\text{Regret}(T; \bar\pi^*)$ no more than 
$$
CT^{\frac12}\bigg\{R_\theta + \frac{(1\!-\!2p_0)(R\!+\!R_\theta)}{\sqrt{p_0}(1\!-\!\rho)\lambda_0}\sqrt{d\!+\!\log T}\bigg\}.
$$
\end{corollary}
Ignoring other factors, the regret upper bound is of order $\tilde\cO(\sqrt{d}T^{2/3})$ for the standard setting, and  $\tilde\cO(\sqrt{dT})$ for the clipped policy setting, depending on the horizon $T$ and dimension $d$.

%\begin{remark}\label{remark:S}
In certain scenarios (e.g. when $d$ is large), it is desirable to save computational resources by updating the estimates of $(\btheta_a^*)_{a\in\{0, 1\}}$ less frequently in Algorithm \ref{alg1}. Fortunately, low regret guarantees can still be achieved: Suppose at each time $t$, the agent only updates the estimators according to (\ref{eq:proposed-estimator}) at selected time points $t\in\cS\subseteq[T]$ (in line 7); Otherwise, the agent simply makes decisions based on the most recently updated estimators. In Appendix \ref{pf:cor:alg1-with-theta-estimation}, we show that time points to perform the updates can be very infrequent, such as $(n^k)_{k\in\mathbb N^+}$ ($n\geq 2, n\in\mathbb N^+$), while still achieving the same rate of regret upper bound as in Corollary \ref{cor:alg1-with-theta-estimation}.
%\end{remark}

\subsection{\texttt{MEB} given estimated error variance}\label{section:estimated-Sigma-e}

In practice, the agent might not have perfect knowledge about $\bSigma_{e, t}$, the variance of the error $\bepsilon_t$. In this section, we discuss the situation where at each time $t$, the agent does not know $\bSigma_{e, t}$, and only has a (potentially adaptive) estimator $\hat\bSigma_{e, t}$ for $\bSigma_{e, t}$. This estimator may be derived from auxiliary data or outside knowledge. In this case, in Algorithm \ref{alg1}, we need to replace the estimator (\ref{eq:proposed-estimator}) with the following estimator for decision-making (i.e. in line 7 of Algorithm \ref{alg1}): 
\begin{equation}\label{eq::proposed-estimator-estimated-Sigma}
\tilde\btheta_a^{(t)}:= \big(\hat\bSigma_{\tilde\xb, a}^{(t)} - 
\frac{1}{t}\sum\nolimits_{\tau\in[t]}\pi^{nd}_\tau(a)\hat\bSigma_{e, \tau}\big)^{-1}\cdot
\hat\bSigma_{\tilde\xb, r, a}^{(t)}.
\end{equation} 
In Appendix \ref{appendix:estimated-error-variance}, we show that with this modification, the additional regret of Algorithm \ref{alg1} is controlled by 
$$\sum\nolimits_{t=T_0+1}^T\max\nolimits_{a\in\{0, 1\}}\|\bDelta_t(a)\|_2$$ 
up to a constant depending on the assumptions. Here, for each $t$, $\bDelta_t(a):= \frac{1}{t}\sum_{\tau\in[t]}\pi_\tau^{nd}(a)(\hat\bSigma_{e, \tau} - \bSigma_{e, \tau})$ is the weighted average of the estimation errors $(\hat\bSigma_{e, \tau} - \bSigma_{e, \tau})_{\tau\in[t]}$. In practice, it is reasonable to assume that $\bDelta_t(a)$ is small so as not to significantly affect the overall regret: For example, suppose the agent gathers more auxiliary data over time so that $\|\hat\bSigma_{e, t} - \bSigma_{e, t}\|_2\lesssim \frac{1}{\sqrt{t}}$, then the additional regret term will be less than $\cO(\sqrt{T})$ up to a constant depending on the assumptions.




