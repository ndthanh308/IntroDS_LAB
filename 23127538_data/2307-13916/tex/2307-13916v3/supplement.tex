% \documentclass[twoside]{article}

% \usepackage{aistats2024}
% % If your paper is accepted, change the options for the package
% % aistats2022 as follows:
% %
% %\usepackage[accepted]{aistats2024}
% %
% % This option will print headings for the title of your paper and
% % headings for the authors names, plus a copyright note at the end of
% % the first column of the first page.

% % If you set papersize explicitly, activate the following three lines:
% %\special{papersize = 8.5in, 11in}
% %\setlength{\pdfpageheight}{11in}
% %\setlength{\pdfpagewidth}{8.5in}

% % If you use natbib package, activate the following three lines:
% \usepackage[round]{natbib}
% \renewcommand{\bibname}{References}
% \renewcommand{\bibsection}{\subsubsection*{\bibname}}

% % If you use BibTeX in apalike style, activate the following line:
% %\bibliographystyle{apalike}

% %%%%%%%%%%%%%%%%%%%% beginning of my own packages and defs

% \usepackage{algorithm}
% \usepackage{algorithmic}
% %\usepackage{algpseudocode} 
% \usepackage{mystyle}
% %\usepackage{refcheck}
% \newcommand{\revise}[1]{\textcolor{blue}{#1}}
% \def\r#1{\textcolor{red}{#1}}
% \def\b#1{\textcolor{blue}{#1}}
% \newcommand{\sam}[1]{
%   {\color{blue} [SAM: {#1}]}
%  }
% %%%%%%%%%%%%%%%%%%%% end of my own packages and defs


% \begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

% Supplementary material: To improve readability, you must use a single-column format for the supplementary material.
\onecolumn
%\aistatstitle{Instructions for Paper Submissions to AISTATS 2024: \\Supplementary Materials}

%\aistatstitle{Appendices: Supplementary materials for \\
%Online learning in bandits with predicted context}

\appendix

\section{Additional details for simulation studies}\label{appendix-simulation+}

\subsection{Compared algorithms}
In both simulation environments, we compare the following algorithms: Thompson sampling (\texttt{TS}, see details in Algorithm \ref{alg:TS}) given normal priors \citep{russo2018tutorial}, Linear Upper Confidence Bound (\texttt{UCB}, see details in Algorithm \ref{alg:UCB}) approach \citep{chu2011contextual}, \texttt{MEB} (Algorithm \ref{alg1}), and \texttt{MEB-naive} (\texttt{MEB} plugged in with the naive measurement error estimator (\ref{eq:naive-estimator}) instead of (\ref{eq:proposed-estimator})). To make a fair comparison between algorithms, we use the same regularization parameter $l = 1$ for all algorithms. The hyper-parameter $\rho$, $C$ is set to be $\sigma_{\eta}^2$ for all results for \texttt{TS} and \texttt{UCB}.

% \setcounter{algorithm}{1}
\begin{algorithm}[H]
	\caption{Linear Thompson Sampling (\texttt{TS})}	
	\label{alg:TS}
	\begin{algorithmic}[1]		
\STATE \textbf{{Input}}: $T$: total number of steps; $\rho$: variance of $(\eta_t)_{t\in[T]}$; $l$: prior variance; $p_0$: minimum selection probability; $\bmu_{0, a} = \mathbf{0}$ and $\bSigma_{0, a} = l \bI$
        \FOR{time $t = 1, 2, \ldots, T$}
        \STATE Observe $\tilde \bx_t = \bx_t + \bepsilon_t$
        \STATE Generate posterior sample $(\tilde\btheta_{t, a})_{a\in\cA}$ from $\mathcal{N}(\bmu_{t-1, a}, \bSigma_{t-1, a})$
        \STATE Set $\tilde a_t \leftarrow \argmax_{a\in\cA}\langle\tilde\btheta_{t, a}, \tilde\xb_t\rangle$
        \STATE Sample $a_t\sim \pi_t(\cdot|\tilde\xb_t, \cH_{t-1})$, where $\pi_t(a|\tilde\xb_t, \cH_{t-1}):=
        \begin{cases}
        1-(K-1)p_{0}, \quad\text{if }a = \tilde a_t\\
        p_{0}, \quad\text{otherwise}
        \end{cases}
        $
        \STATE Observe reward $r_t = \langle \btheta_{a_t}^\star, \bx_t \rangle + \eta_t$
        \STATE Set $\bV_{t, a} = l\bI + \sum_{t'= 1}^t \mathbf{1}_{\{a_{t'} = a\}} \tilde \bx_t\tilde \bx_t^{\top}$, $\bb_t = \sum_{t' = 1}^t \mathbf{1}_{\{a_{t'} = a\}} \tilde \bx_t r_t$
        \STATE Update $\bmu_{t, a} = \bV_{t, a}^{-1} \bb_{t, a}$ and $\bSigma_{t, a} = \rho \bV_{t, a}^{-1}$
        \ENDFOR
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption{Linear UCB (\texttt{UCB})}	
	\label{alg:UCB}
	\begin{algorithmic}[1]		
\STATE \textbf{{Input}}: $T$: total number of steps; $C > 0$; $l$: regularization; $p_0$: minimum selection probability; $\bV_{0, a} = l \bI$ and $\bb_{0, a} = \mathbf{0}$
        \FOR{time $t = 1, 2, \ldots, T$}
        \STATE Observe $\tilde \bx_t = \bx_t + \bepsilon_t$
        \STATE Estimate $\hat\btheta_{t, a} = \bV_{t-1, a}^{-1} \bb_{t-1, a}$
        \STATE Set $\hat\mu_a = \langle \hat \btheta_{t, a}, \tilde \bx_{t} \rangle + C\sqrt{\tilde\bx_{t}^{\top} \bV_{t, a}^{-1} \tilde \bx_t}$
        \STATE Set $\tilde a_t \leftarrow \argmax_{a\in\cA} \hat \mu_a$
        \STATE Sample $a_t\sim \pi_t(\cdot|\tilde\xb_t, \cH_{t-1})$, where $\pi_t(a|\tilde\xb_t, \cH_{t-1}):=
        \begin{cases}
        1-(K-1)p_{0}, \quad\text{if }a = \tilde a_t\\
        p_{0}, \quad\text{otherwise}
        \end{cases}
        $
        \STATE Observe reward $r_t = \langle \btheta_{a_t}^\star, \bx_t \rangle + \eta_t$
        \STATE Set $\bV_{t, a} = l\bI + \sum_{t'= 1}^t \mathbf{1}_{\{a_{t'} = a\}} \tilde \bx_t\tilde \bx_t^{\top}$, $\bb_t = \sum_{t' = 1}^t \mathbf{1}_{\{a_{t'} = a\}} \tilde \bx_t r_t$
        \ENDFOR
	\end{algorithmic}
\end{algorithm}



We further compare with robust linear UCB (Algorithm 1 in \cite{ding2022robust}) that is shown to achieve minimax rate for adversarial linear bandit.



\subsection{Additional details for HeartStep V1 study}
Table \ref{tab:variables} presents the list of variables to include in the reward model and in the feature construction for algorithms. Recall that our reward model is $r_t(\bx, a, \btheta) = \bx^{\top} \balpha + a f(\bx)^{\top} \bbeta + \eta_t$. All the variables are included in $\bx$ while only those considered to have an impact on treatment effect will be included in $f(\bx)$.
\begin{table}[ht]
    \centering
    \caption{List of variables in HeartSteps V1 study.}
    \begin{tabular}{l|lc}
        Variable & Type & Treatment? \\
        \hline
        Availability ($I_t$) & Discrete & No \\
        Prior 30-minute step count & Continuous & No \\
        Yesterday's step count & Continuous & No \\
        Prior 30-minute step count & Continuous & No \\
        Location & Discrete & Yes \\
        Current temperature & Continuous & No \\
        Step variation level & Discrete & Yes \\
        Burden variable ($B_t$) & Continuous & Yes
    \end{tabular}
    \label{tab:variables}
\end{table}

\subsection{Additional results on estimation error}

% Figure environment removed


% Figure environment removed

\subsection{Average regret with standard deviation}


\begin{table}[H]
\centering
\caption{Average regret for both synthetic environment and real-data environment under different combinations of $\sigma_{\eta}^2$ and $\sigma_{\epsilon}^2$. The numbers in parentheses are  the standard deviations calculated from 100 independent runs.}
\begin{subtable}[h]{1\textwidth}
\caption{Average regret in the synthetic environment over $50000$ steps with clipping probability $p = 0.2$. }
\centering
\begin{tabular}{ll|lllll}
$\sigma_{\eta}^2$ & $\sigma_{\epsilon}^2$ & \texttt{TS} & \texttt{UCB} & \texttt{MEB} & \texttt{MEB-naive} & \texttt{RobustUCB} \\
\hline
\hline
0.01 & 0.1 & 0.047 (0.0015) & 0.046 (0.0015) & 0.027 (0.0011) & 0.038 (0.0013) & 0.050 (0.0051) \\
0.1  & 0.1 & 0.047 (0.0015) & 0.047 (0.0015) & 0.026 (0.0011) & 0.039 (0.0013) & 0.049 (0.0048) \\
1.0  & 0.1 & 0.048 (0.0015) & 0.048 (0.0015) & 0.027 (0.0011) & 0.038 (0.0013) & 0.044 (0.0047)\\
0.01 & 1.0 & 0.757 (0.0164) & 0.647 (0.0145) & 0.198 (0.0079) & 0.371 (0.0107) & 0.652 (0.0050)\\
0.1  & 1.0 & 0.769 (0.0160) & 0.721 (0.0156) & 0.205 (0.0080) & 0.392 (0.0110) & 0.753 (0.0056)\\
1.0  & 1.0 & 0.714 (0.0155) & 0.697 (0.0150) & 0.218 (0.0083) & 0.404 (0.0112) & 0.589 (0.0047)\\
0.01 & 2.0 & 1.492 (0.0281) & 1.504 (0.0283) & 0.358 (0.0129) & 0.616 (0.0169) & 1.608 (0.0102)\\
0.1  & 2.0 & 1.195 (0.0244) & 1.333 (0.0260) & 0.368 (0.0131) & 0.584 (0.0164) & 1.064 (0.0079)\\
1.0  & 2.0 & 1.299 (0.0257) & 1.476 (0.0277) & 0.416 (0.0139) & 0.625 (0.0170) & 1.881 (0.0114)\\
\hline
\hline
\end{tabular}
\end{subtable}

\vspace{3mm}

\begin{subtable}[h]{1\textwidth}
\centering
\caption{Average regret in the real-data environment over $2500$ steps with clipping probability $p = 0.2$. }
\begin{tabular}{ll|lllll}
$\sigma_{\eta}^2$ & $\sigma_{\epsilon}^2$ & \texttt{TS} & \texttt{UCB} & \texttt{MEB} & \texttt{MEB-naive} & \texttt{RobustUCB} \\
\hline
\hline
0.05 & 0.1 & 0.027 (0.0067) & 0.027 (0.0070) & 0.022 (0.0057) & 0.024 (0.0058) & 0.025 (0.0079) \\
0.1  & 0.1 & 0.026 (0.0057) & 0.024 (0.0053) & 0.020 (0.0046) & 0.020 (0.0046) & 0.028 (0.0079) \\
5.0  & 0.1 & 1.030 (0.0287) & 0.743 (0.0262) & 0.831 (0.0267) & 1.173 (0.0343) & 1.400 (0.0447) \\
0.05 & 1.0 & 0.412 (0.0355) & 0.408 (0.0351) & 0.117 (0.0148) & 0.112 (0.0143) & 0.226 (0.0020) \\
0.1  & 1.0 & 0.309 (0.0293) & 0.316 (0.0299) & 0.085 (0.0112) & 0.087 (0.0116) & 0.206 (0.0125) \\
5.0  & 1.0 & 1.321 (0.0417) & 0.918 (0.0304) & 1.458 (0.0422) & 1.322 (0.0388) & 1.065 (0.0400)\\
0.05 & 2.0 & 0.660 (0.0343) & 0.634 (0.0322) & 0.144 (0.0129) & 0.148 (0.0133) & 0.304 (0.0241)\\
0.1  & 2.0 & 0.740 (0.0505) & 0.704 (0.0489) & 0.151 (0.0145) & 0.155 (0.0149) & 0.432 (0.0386)\\
5.0  & 2.0 & 1.585 (0.0454) & 2.415 (0.0816) & 1.577 (0.0508) & 1.436 (0.0462) & 1.345 (0.0423)
\end{tabular}
\end{subtable}
\label{tab:synthetic_full}
\end{table}

\section{Additional explanations on the regularized least-squares (RLS) estimator the naive estimator (\ref{eq:naive-estimator}) under noisy context}\label{appendix-failure-of-naive-estimator}

\subsection{Inconsistency of the RLS estimator}

\noindent\textbf{Measurement error model and attenuation.} As briefly mentioned in the main text, a measurement error model is a regression model designed to accommodate inaccuracies in the measurement of regressors. Suppose that there is no action (i.e. set $a_\tau\equiv 0$), and $(\xb_\tau, \bepsilon_\tau, \eta_\tau)_{\tau\in[t]}$ are i.i.d., then the measurement error model is a useful tool to learn $\btheta^*_0$ from $\cH_t$ collected as follows:
\begin{equation}\label{eq::appendixB-measurement-error-model}
\begin{cases}
r_\tau = \langle\btheta_0^*, \xb_\tau\rangle + \eta_\tau,\\
\tilde\xb_\tau = \xb_\tau + \bepsilon_\tau.
\end{cases}
\end{equation}
Here in the measurement error model's perspective, $(\tilde\xb_\tau)_{\tau\in[t]}$ are regressors `measured with error', and $(r_\tau)_{\tau\in[t]}$ are dependent variables. 

Regression attenuation, a phenomenon intrinsic to measurement error models, refers to the observation that when the predictors are subject to measurement errors, the Ordinary Least Squares (OLS) estimators of regression coefficients become biased (see, for instance, \cite{carroll1995measurement}). Specifically, in simple linear regression, the OLS estimator for the slope tends to be biased towards zero. Intuitively, this is because the measurement errors effectively `dilute' the true relationship between variables, making it appear weaker than it actually is. 

\noindent\textbf{Inconsistency of the RLS estimator.} Before presenting a concrete numerical example to show the inconsistency of the RLS estimator and that it leads to bad decision-making, below we first apply the theory of measurement error model to give a heuristic argument of why the RLS estimator is inconsistent even in the simplified situation where there is no action (i.e. set $a_\tau\equiv 0$), and $(\xb_\tau, \bepsilon_\tau, \eta_\tau)_{\tau\in[t]}$ are i.i.d..

From Section 3.3.2 in \cite{carroll1995measurement}, given data $(\tilde\xb_\tau, r_\tau)_{\tau\in[t]}$ from (\ref{eq::appendixB-measurement-error-model}) with multiple covariates (in the simplified case with no action as described above), the OLS estimator for $\btheta_0^*$, denoted as $\hat\btheta_0^{OLS, t}$, consistently estimates not $\btheta_0^*$ but
\begin{equation}\label{eq::AppendixB-thetatilde}
\tilde\btheta_0^* = (\bSigma_{\xb} + \bSigma_e)^{-1}\bSigma_{\xb}\btheta_0^*,
\end{equation}
where $\bSigma_\xb = \mathrm{Var}(\xb_\tau)$, $\bSigma_e = \mathrm{Var}(\bepsilon_\tau)$.
In addition, given fixed $\lambda$, the regularized least squares (RLS) estimator
$$
\hat\btheta_0^{RLS, t}(\lambda) = \bigg(\frac{\lambda}{t}I+\frac1t\sum_{\tau\in[t]}\tilde\xb_\tau\tilde\xb_\tau^\top\bigg)^{-1}\bigg(\frac1t\sum_{\tau\in[t]}\tilde\xb_\tau r_\tau\bigg) = \bigg(\frac{\lambda}{t}I+\frac1t\sum_{\tau\in[t]}\tilde\xb_\tau\tilde\xb_\tau^\top\bigg)^{-1}\bigg(\frac1t\sum_{\tau\in[t]}\tilde\xb_\tau\tilde\xb_\tau^\top\bigg)\hat\btheta_0^{OLS, t},
$$
where $\frac1t\sum_{\tau\in[t]}\tilde\xb_\tau\tilde\xb_\tau^\top\rightarrow \mathrm{Var}(\tilde\xb_\tau) = \mathrm{Var}(\xb_\tau) + \mathrm{Var}(\bepsilon_\tau)$, and $\lambda/t\rightarrow 0$ as $t\rightarrow \infty$. This means that as $t\rightarrow \infty$, $\hat\btheta_0^{RLS, t}(\lambda)$ and $\hat\btheta_0^{OLS, t}$ converges to the same limit, which is $\tilde\btheta_0^*$. Thus, for fixed $\lambda$, as $t$ grows, $\|\hat\btheta_0^{RLS, t}(\lambda) - \btheta_0^*\|_2$ converges to $\|\tilde\btheta_0^* - \btheta_0^*\|_2$ and does not converge to zero in general.

Finally, recall that in classical bandit algorithms such as UCB, the sublinear regret relies on the key property that with high probability, for all $t$, $\|\hat\btheta_0^{RLS, t}(\lambda) - \btheta_0^*\|_{V_t} \leq \beta$, where $V_t = \lambda I + \sum_{\tau\in[t]}\tilde\xb_\tau\tilde\xb_\tau^\top$, $\beta = \tilde\cO(\sqrt{d})$. Here for a vector $\bv\in \RR^d$ and positive definite matrix $\mathbf M\in\RR^{d\times d}$, $\|\bv\|_{\mathbf M}:= \sqrt{\bv^\top \mathbf M\bv}$. We argue that this requirement generally no longer holds in the setting with measurement error $(\bepsilon_\tau)_{\tau\geq 1}$. In fact, notice that since $\tilde\xb_\tau$ is i.i.d. in this simplified setting, and $V_t = \lambda I + \sum_{\tau\in[t]}\tilde\xb_\tau\tilde\xb_\tau^\top$, we expect that $\frac1t V_t$ concentrates around $\mathrm{Var}(\tilde\xb_t)$. As long as $\lambda_{\min}(\mathrm{Var}(\tilde\xb_t))>0$, with high probability, for all $t$, $\frac1t V_t\succ c$ for some constant $c$. If this holds, 
$$
\|\hat\btheta_0^{RLS, t}(\lambda) - \btheta_0^*\|_{V_t}\geq \sqrt{ct}\|\hat\btheta_0^{RLS, t}(\lambda) - \btheta_0^*\|_2,
$$
while the last term $\|\hat\btheta_0^{RLS, t}(\lambda) - \btheta_0^*\|_2$ converges to a nonzero limit $\|\tilde\btheta_0^* - \btheta_0^*\|_2$. This indicates that in general, $\|\hat\btheta_0^{RLS, t}(\lambda) - \btheta_0^*\|_{V_t}$ scales with a rate of at least $\sqrt{t}$, and will not be uniformly bounded by $\tilde\cO(\sqrt{d})$ for all $t$.

\noindent\textbf{An example.} The following is an example where given the errors $(\bepsilon_\tau)_{\tau\in[t]}$, the RLS estimator in the classical bandit algorithms inconsistently estimates the true reward model, and in addition, it leads to bad decision-making (linear regret) in the classic bandit algorithms.

\begin{example}\label{example::UCBTSfail}
Consider the standard setting described in Section \ref{sec::setting}. Let $T = 10000$, $d = 2$, $\btheta_1^* = (1, 0)^\top$, $\btheta_0^* = (-1, 0)^\top$. Let $\xb_t$ sampled i.i.d. from $\mathrm{Unif}(\cS)$, where $\cS = \{(1, 3)^\top,\thickspace (-3, 1)^\top, \thickspace(-1, -3)^\top,\thickspace (3, -1)^\top\}$. Condition on $\xb_t$, $\bepsilon_t$ is uniformly sampled from $((\rho_0 \xb_t^{[1]}, \rho_0 \xb_t^{[1]})^\top, (-\rho_0 \xb_t^{[1]}, -\rho_0 \xb_t^{[1]})^\top)$, independent from any other variable in the history. Here $\rho_0 = 0.9$, $\xb_t^{[1]}$ denotes the first entry in $\xb_t$. We also let $\eta_t$ be i.i.d. drawn from $N(0,\thickspace 0.01)$. 

We conduct 100 independent experiments. For each experiment, we generate data according to the above, and test the performance of UCB (Algorithm 1 in \cite{chu2011contextual}) and Thompson sampling with normal priors \citep{russo2018tutorial} using noisy context $\tilde\xb_t$ instead of the true context. We choose the regularization parameter $\lambda = 1$ in the RLS estimator. Additionally, in UCB \citep{chu2011contextual}, we choose the parameter $\alpha = 1$. Figure \ref{fig::appendixB-UCBTSfail} summarizes the estimation error of the RLS estimator and the cumulative regret of both algorithms with respect to time $t$, showing both the average and standard error across the random experiments. We see that the RLS estimator is unable to estimate the true reward model well. Moreover, it is clear that the regret of both UCB and Thompson sampling is linear in the time horizon. Intuitively, this is because the direction of $\tilde \btheta_0^*$ in (\ref{eq::AppendixB-thetatilde}) is twisted compared to $\btheta_0^*$, which not only leads to inconsistent estimators, but also the optimal action altered.

% Figure environment removed

Finally, we note that in this setup, Assumption \ref{ass:small-error} is satisfied. This demonstrates that even if the errors $(\bepsilon_t)_{t\geq 1}$ do not affect the optimal action given $(\btheta_a^*)_{a\in\{0, 1\}}$, the poor performance of the RLS estimator may still lead to linear regret in classical bandit algorithms.
\end{example}

\subsection{Inconsistency of the naive measurement error adjusted estimator (\ref{eq:naive-estimator})}
\begin{example}\label{ex:failure-of-naive-estimator}
Let $d = 1$, $\xb_{\tau}\equiv 1$ for all ${\tau}$, and $\bepsilon_{\tau}\sim \text{Unif}(-2, 2)$ sampled independently. For the reward model, let $\btheta_0^* = -1$, $\btheta_1^*= 1$, $\eta_{\tau}\sim \text{Unif}(-0.1, 0.1)$ sampled independently. So in order to maximize expected reward, we should choose action 1 if $\xb_{\tau}$ is positive and action 0 otherwise. Suppose the agent takes the following policy that is stationary and non-adaptive to history:  
$$
\pi_{\tau}(A)=
\begin{cases}
\frac{2}{3}1_{\{A = 1\}} + \frac{1}{3}1_{\{A = 0\}},\quad\text{if }\tilde\xb_{\tau}
%= \xb_{\tau}+\bepsilon_{\tau}
>\rho \\
\frac{1}{3}1_{\{A = 1\}} + \frac{2}{3}1_{\{A = 0\}},\quad\text{otherwise.}
\end{cases}
$$
Here, $\rho$ is a pre-specified constant. Figure \ref{fig:theta_estimation} (a) plots the mean and standard deviation of $\hat\btheta_{0, me}^{(t)}$ (as in (\ref{eq:naive-estimator})) given 100 independent experiments for each $t=1,\ldots, 10000$, where $\rho = -0.5, 0, 0.5$. Observe that as $t$ grows, $\hat\btheta_{0, me}^{(t)}$ 
converges to different limits for different policies. In general, 
the limit is not equal to $\btheta_0^* = -1$.

In contrast, Figure \ref{fig:theta_estimation} (b) shows the mean and standard deviation of $\hat\btheta_0^{(t)}$ (as in (\ref{eq:proposed-estimator})) given 100 independent experiments under the same setting with the same three policies as in Figure \ref{fig:theta_estimation} (a). Unlike the naive estimator (\ref{eq:naive-estimator}), the proposed estimator (\ref{eq:proposed-estimator}) quickly converges around the true value $-1$ for all three candidate policies.
\end{example}


% Figure environment removed

%\section{Regret guarantees compare to the standard benchmark policy (2.2)}\label{appendix:usual-benchmark}

\section{{Generalization to $K\geq 2$ actions}} \label{appendix:generalization-to-K-actions}

In this section, we assume that $\cA = \{1, 2, \ldots, K\}$ instead of $\{0, 1\}$. The standard and clipped benchmark become
\begin{equation}\label{eq:standard-benchmark-multiple-actions}
\pi_t^*(a) = 
\begin{cases}
1,\quad &\text{if }a = a_t^* = \argmax_a\langle\btheta_a^*, \xb_t\rangle\\
0,\quad &\text{otherwise, }
\end{cases}
\end{equation}
and 
\begin{equation}\label{eq:clipped-benchmark-multiple-actions}
\bar\pi_t^*(a) = 
\begin{cases}
1-(K-1)p_0,\quad &\text{if }a = a_t^*,\\
p_0,\quad &\text{otherwise. }
\end{cases}
\end{equation}

In the $K$-arm setting, we can still estimate $\btheta_a^*$ using (\ref{eq:proposed-estimator}) for each $a\in\cA$. Using the same proof ideas as Theorem \ref{thm:theta-estimation}, we get the following guarantees for estimation error (proof omitted):

\begin{theorem}\label{thm:theta-estimation-multiple-actions}
For any $t\in[T]$, let $q_t = \inf_{\tau\in[t], a\in\cA}\pi_\tau(a|\tilde\xb_\tau, \cH_{\tau-1})$. Then under Assumption \ref{ass:boundedness} and \ref{ass:min-signal}, there exist constants $C$ and $C_1$ such that as long as $q_t\geq C_1\max\left\{\frac{d(d+\log t)}{\lambda_0 t}, \frac{\xi(d+\log t)}{\lambda_0^2 t}\right\}$, with probability at least $1-\frac{4K}{t^2}$,
$$
\|\hat\btheta_a^{(t)}-\btheta_a^*\|_2\leq \frac{C(R+R_\theta)d}{\lambda_0}\max\left\{\frac{d+\log t}{q_tt}, \frac{\sqrt{\nu} + \sqrt{\xi}}{\sqrt{d}}\sqrt{\frac{d+\log t}{q_tt}}\right\}, \quad \forall a\in\cA.
$$
\end{theorem}

%\setcounter{algorithm}{1}
\begin{algorithm}[t]
	\caption{\texttt{MEB} with $K$ actions}	
	\label{alg1-multiple-actions}
	\begin{algorithmic}[1]		
\STATE \textbf{{Input}}: $(\bSigma_{e, t})_{t\in[T]}$: covariance sequence of $(\bepsilon_t)_{t\in[T]}$; $(p_0^{(t)})_{t\in[T]}$: minimum selection probability at each time $t$; $T_0$: length of pure exploration
\FOR{time $t = 1, 2, \ldots, T$}
        \IF{$t\leq T_0$}
\STATE Sample $a_t\sim \pi_t(\cdot|\tilde\xb_t, \cH_{t-1})$, where $\pi_t(a|\tilde\xb_t, \cH_{t-1}) \in[p_0^{(t)}, 1-(K-1)p_0^{(t)}]$ for all $a\in\cA$
\ELSE
\STATE Obtain updated estimators $(\hat\btheta_{a}^{(t-1)})_{a\in\cA}$ from (\ref{eq:proposed-estimator})
\STATE $\tilde a_t \leftarrow \argmax_{a\in\cA}\langle\hat\btheta_{a}^{(t-1)}, \tilde\xb_t\rangle$
\STATE Sample $a_t\sim \pi_t(\cdot|\tilde\xb_t, \cH_{t-1})$, where $\pi_t(a|\tilde\xb_t, \cH_{t-1}):=
\begin{cases}
1-(K-1)p_{0}^{(t)}, \quad\text{if }a = \tilde a_t\\
p_{0}^{(t)}, \quad\text{otherwise}
\end{cases}
$
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\texttt{MEB} with $K$ actions is shown in Algorithm \ref{alg1-multiple-actions}. As in Theorem \ref{thm-regret}, we can control the regret of Algorithm \ref{alg1-multiple-actions} by the estimation error of (\ref{eq:proposed-estimator}). Here, Assumption \ref{ass:small-error} needs to be generalized to the following to adapt to multiple actions:

\begin{assumption}\label{ass:small-error-K-actions}
There exists a constant $\rho\in(0, 1)$ such that $\forall t\in[T]$, $\forall a_1, a_2\in\mathcal A$, $|\langle\btheta_{a_1}^* - \btheta_{a_2}^*, \bepsilon_t\rangle|\leq \rho |\langle\btheta_{a_1}^* - \btheta_{a_2}^*, \xb_t\rangle|$ almost surely.
\end{assumption}

The theorem below is a generalization of Theorem \ref{thm-regret} to multiple actions (The proof is only slightly different from Theorem \ref{thm-regret}; We briefly discuss the difference in Appendix \ref{pf:thm:regret-multiple-actions}).

\begin{theorem}\label{thm:regret-multiple-actions}
Let Assumption \ref{ass:boundedness} and \ref{ass:small-error-K-actions} hold.
\begin{itemize}
\item[(i)] For the standard setting, Algorithm \ref{alg1-multiple-actions} outputs a policy with $\text{Regret}(T; \pi^*)$ no more than 
$$
2T_0R_{\theta}+\frac{2}{1\!-\!\rho}\cdot\!\!\!\sum\limits_{t=T_0\!+1}^T\!\!\!\!\big[(K-1)p_0^{(t)}R_\theta + \!\!\max\limits_{a\in\cA}\!\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2\big].
$$
\item[(ii)] For the clipped policy setting, Algorithm \ref{alg1-multiple-actions} with the choice of $p_0^{(t)} \equiv p_0$ outputs a policy with $\text{Regret}(T; \bar\pi^*)$ no more than 
$$
2T_0R_{\theta}+\frac{2(1\!-\!Kp_0)}{1\!-\!\rho}
\cdot\!\!\!\sum\limits_{t=T_0\!+1}^T\max\limits_{a\in\cA}\|\hat\btheta_a^{(t-1)}\!-\!\btheta_a^*\|_2.
$$
\end{itemize}
\end{theorem}

Combining Theorems \ref{thm:theta-estimation-multiple-actions} and \ref{thm:regret-multiple-actions}, we obtain the following corollary. %Its proof is the same as Corollary \ref{cor:alg1-with-theta-estimation}, and is thus omitted.

\begin{corollary}\label{cor:alg1-with-theta-estimation-multiple-actions}
Let Assumption \ref{ass:boundedness}, \ref{ass:min-signal} and \ref{ass:small-error-K-actions} hold. There exist universal constants $C, C'$ such that:

(i) For the standard setting, $\forall T\geq C'\max\{(1+1/\lambda_0^{\frac94})(d+\log T)^3, (\xi/\lambda_0)^{\frac94}(d+\log T)^{\frac43}\}$, with probability at least $1\!-\!\frac{8K}{\sqrt{T}}$, Algorithm \ref{alg1-multiple-actions} with the choice of $T_0 = \lceil 2dT^{\frac23}\rceil$, $p_0^{(t)} = \min\{\frac12, t^{-\frac13}\}$ outputs a policy with $\text{Regret}(T; \pi^*)$ no more than
$$
C dT^{\frac23}\bigg\{\frac{(K-1)R_\theta}{1\!-\!\rho} + \frac{(\sqrt{\nu}+\sqrt{\xi}+1)(R\!+\!R_{\theta})}{(1\!-\!\rho)\lambda_0}\sqrt{1+\frac{\log T}{d}}\bigg\}.
$$

(ii) For the clipped policy setting, $\forall T\geq C'\max\{(d+\log T)^2/(\lambda_0p_0)^2, \xi^2/\lambda_0^4(1+\log T/d)^2\}$, with probability at least $1\!-\!\frac{8K}{\sqrt{T}}$, Algorithm \ref{alg1-multiple-actions} with the choice of $T_0 = \lceil2d\sqrt{T}\rceil$ and  $p_0^{(t)}\equiv p_0$ outputs a policy with $\text{Regret}(T; \bar\pi^*)$ no more than 
$$
CdT^{\frac12}\bigg\{R_\theta + \frac{(\sqrt{\nu}+\sqrt{\xi}+1)(1\!-\!Kp_0)(R\!+\!R_\theta)}{\sqrt{p_0}(1\!-\!\rho)\lambda_0}\sqrt{1+\frac{\log T}{d}}\bigg\}.
$$
\end{corollary}


\section{{Analysis of the proposed estimator (\ref{eq:proposed-estimator})}}\label{pf:thm:theta-estimation}

\subsection{{Proof of Theorem \ref{thm:theta-estimation}}}

We fix some $t\in[T]$, and control $\|\hat\btheta_{a}^{(t)}-\btheta_a^*\|_2$ for $a\in\{0, 1\}$. Towards this goal, we combine analysis of the two random terms $\hat\bSigma_{\tilde\xb, a}^{(t)}$ and $\hat\bSigma_{\tilde\xb, r, a}^{(t)}$ in the lemma below.
\begin{lemma}\label{lem:hat-Sigma-x-r}
Under the same assumptions of Theorem 2.1, there exists an absolute constant $C$ such that with probability at least $1-4/t^2$, both of the followings hold:
\begin{align}
\bigg\|\hat\bSigma_{\tilde\xb, a}^{(t)}-\frac{1}{t}\sum_{\tau\in[t]}\pi_{\tau}^{nd}(a)(\xb_\tau\xb_{\tau}^\top+\bSigma_{e, \tau})\bigg\|_2\leq C\max\left\{\frac{d+\log t}{q_tt}, \frac{\sqrt{\xi}}{d}\cdot \sqrt{\frac{d+\log t}{q_tt}}\right\},\label{eq:hat-Sigma-x}\\
\bigg\|\hat\bSigma_{\tilde\xb, r, a}^{(t)}-\bigg(\frac{1}{t}\sum_{\tau\in[t]}\pi_{\tau}^{nd}(a)\xb_\tau \xb_{\tau}^\top\bigg)\btheta^*_{a}\bigg\|_2\leq CR\max\left\{\frac{d+\log t}{q_tt}, \sqrt{\frac{\nu}{d}}\cdot \sqrt{\frac{d+\log t}{q_tt}}\right\}.\label{eq:hat-Sigma-x-r}
\end{align}
\end{lemma}
Proof of Lemma \ref{lem:hat-Sigma-x-r} is in Section \ref{pf:lem:hat-Sigma-x-r}.

Denote $\bDelta_1 =\hat\bSigma_{\tilde\xb, a}^{(t)}-\frac{1}{t}\sum_{\tau\in[t]}\pi_{\tau}^{nd}(a)(\xb_\tau\xb_{\tau}^\top+\bSigma_{e, \tau})$, $\bDelta_2 = \hat\bSigma_{\tilde\xb, r, a}^{(t)}-\bigg(\frac{1}{t}\sum_{\tau\in[t]}\pi_{\tau}^{nd}(a)\xb_\tau \xb_{\tau}^\top\bigg)\btheta^*_{a}$. Then
\begin{align*}
\hat\btheta_a^{(t)}&= \bigg(\hat\bSigma_{\tilde\xb, a}^{(t)} - 
\frac{1}{t}\sum_{\tau\in[t]}\pi^{nd}_\tau(a)\bSigma_{e, \tau}\bigg)^{-1}\cdot
\hat\bSigma_{\tilde\xb, r, a}^{(t)}\\
& = \bigg(\frac{1}{t}\sum_{\tau\in[t]}\pi^{nd}_\tau(a)\xb_\tau\xb_{\tau}^\top+\bDelta_1\bigg)^{-1}\cdot \left[\bigg(\frac{1}{t}\sum_{\tau\in[t]}\pi_{\tau}^{nd}(a)\xb_\tau \xb_{\tau}^\top\bigg)\btheta^*_{a}+\bDelta_2\right]\\
&= \btheta_a^* - \bJ_1 + \bJ_2,
\end{align*}
where 
\begin{align*}
\bJ_1:=\left[\frac{1}{t}\sum_{\tau\in[t]}\pi_{\tau}^{nd}(a)\xb_\tau\xb_{\tau}^\top+\bDelta_1\right]^{-1}\cdot\bDelta_1\btheta_a^*,\quad
\bJ_2:=\left[\frac{1}{t}\sum_{\tau\in[t]}\pi_{\tau}^{nd}(a)\xb_\tau\xb_{\tau}^\top+\bDelta_1\right]^{-1}\cdot\bDelta_2.
\end{align*}
It's easy to verify that under the event where both (\ref{eq:hat-Sigma-x}) and (\ref{eq:hat-Sigma-x-r}) hold, whenever 
\begin{equation}\label{eq:condition-eigv-control}
C\max\left\{\frac{d+\log t}{q_tt}, \frac{\sqrt{\xi}}{d}\cdot \sqrt{\frac{d+\log t}{q_tt}}\right\}\leq \frac{\lambda_{0}}{2d},
\end{equation}
we have 
$$
\|\bJ_1\|_2\leq \frac{2CR_{\btheta}d}{\lambda_{0}}\max\left\{\frac{d+\log t}{q_tt}, \frac{\sqrt{\xi}}{d}\cdot\sqrt{\frac{d+\log t}{q_tt}}\right\}
$$
and 
$$
\|\bJ_2\|_2\leq \frac{2CRd}{\lambda_{0}}\max\left\{\frac{d+\log t}{q_tt}, \sqrt{\frac{\nu}{d}}\cdot \sqrt{\frac{d+\log t}{q_tt}}\right\}.
$$
(\ref{eq:condition-eigv-control}) can be ensured by $t\geq C_1\max\{\frac{d(d+\log t)}{\lambda_0q_t}, \frac{\xi(d+\log t)}{\lambda_0^2 q_t}\}$, where $C_1 = \max\{2C, 4C^2\}$. Given these guarantees, we have with probability at least $1-4/t^2$,
$$
\|\hat\btheta_a^{(t)}-\btheta_a^*\|_2\leq \|\bJ_1\|_2+\|\bJ_2\|_2\leq \frac{2C(R+R_\theta)d}{\lambda_0}\max\left\{\frac{d+\log t}{q_t t}, \frac{\sqrt{\nu}+\sqrt{\xi}}{\sqrt{d}}\cdot\sqrt{\frac{d+\log t}{q_t t}}\right\}.
$$
Thus we conclude the proof.

\subsection{Additional comments on Assumption \ref{ass:min-signal}}

Following Remark \ref{remark::ass-min-signal}, all the following examples of $\{\xb_\tau\}$ allow the existence of $\lambda_{0}$ with $\pi_\tau^{nd}(a)\equiv 1/2$ given any reasonably big $t$ with high probability:
\begin{itemize}
    \item $\{\sqrt{d}\xb_\tau\}$ is an i.i.d. sequence satisfying $d\EE\xb_\tau\xb_\tau^\top\succeq \lambda_{1}I_d$, $\lambda_1>0$;
    \item $\{\sqrt{d}\xb_\tau\}$ is a weakly-dependent stationary time series (a common example is the multivariate ARMA process under regularity conditions, see e.g. \citep{banna2016bernstein}). The stationary distribution $P$ satisfies $d\EE_{\xb\sim P}\xb\xb^\top\succeq \lambda_2I_d$, $\lambda_2>0$;
    \item $\{\sqrt{d}\xb_\tau\}$ is a periodic time series such that there exists $t_0\in \mathbb N^+$ which satisfies $\frac{d}{t_0}\sum_{\tau\in(kt_0+1, (k+1)t_0]}\xb_\tau\xb_\tau^\top\succeq \lambda_3I_d$ a.s. $\forall k\in\mathbb N$.
\end{itemize}

%\subsection{{Near optimality among policies with constant minimum sampling probability}}

%For fixed $p_0>0$, consider the data-independent policy $\{\pi_\tau\}_{\tau\geq 1}$ that always selects action 0 with probability $p_0$ and action 1 with probability $1-p_0$. Given this policy, $\cH_t = \{(\tilde\xb_\tau, A_\tau, Y_\tau)\}_{\tau\geq 1}$ consists of i.i.d. samples, where $A_\tau$ and $\tilde\xb_\tau$ are independent. Moreover, with high probability, the number of samples where $A_\tau = 0$ is $\cO(p_0t)$. Using results from classical statistics (e.g. \citep{van2000asymptotic}), among the reward model class $\{\btheta_0^*, \btheta_1^*\}\in \Theta$, the minimax lower bound $\ell_2$ error is of order $\Omega(\sqrt{\frac{d}{p_0t}})$. Combining Theorem 2.1, we see that the upper bound (2.7) is near-optimal among all policies with minimum sampling probability $p_0$ and $\{\btheta_0^*, \btheta_1^*\}\in \Theta$.

\subsection{{Generalization to off-policy method-of-moment estimation}}

(\ref{eq:proposed-estimator}) can be generalized to a class of method-of-moment estimators for off-policy learning. In this section, we delve into the general framework of off-policy method-of-moment estimation. This framework proves valuable in scenarios where a fully parametric model class for the reward is unavailable, yet there is a desire to estimate certain model parameters using offline batched bandit data.

For simplicity, we assume that $(X_t, Y_t(a):a\in\cA)_{t\in[T]}$ are drawn i.i.d. from an unknown distribution $\cP$. At each time $t\in[T]$, the action $A_t$ is drawn from a policy $\pi_t(\cdot|X_t, \cH_{t-1})$, and the agent observes only $o_t = (X_t, A_t, Y_t(A_t))$ together with the action selection probabilities $\pi_t$. Define the history up to time $t$ as $\cH_t = \{o_\tau\}_{\tau\leq t}$. For $a_0\in\cA$, we're interested in estimating $\btheta_{a_0}^*$, a $d$-dimensional parameter in $\cP^{(a_0)}$, which is the joint distribution of $\{X_t, Y_t(a_0)\}$. 

\begin{remark}
When the context is i.i.d., the problem of estimating $\{\btheta_a^*\}_{a\in\cA}$ in Section \ref{section::estimation} is a special case of this setup by taking $\tilde\xb_t$ as $X_t$ and $r_t$ as $Y_t$.
\end{remark}

The traditional method-of-moment estimator looks for functions $f_1, \ldots, f_d$ as well as a mapping $\bphi:\RR^d\rightarrow \RR^d$, such that 
$$
\btheta_{a_0}^* = \bphi\big(\EE_{(X, Y)\sim \cP^{(a_0)}}f_1(X, Y), \ldots, \EE_{(X, Y)\sim \cP^{(a_0)}}f_d(X, Y)\big).
$$
Then, if given i.i.d. samples $(U_t, V_t)_{t\in[n]}$ from $\cP^{(a_0)}$, the estimator takes the form 
$$
\hat\btheta_{a_0} = \bphi\bigg(\frac{1}{T}\sum_{t\in[T]}f_1(U_t, V_t), \ldots, \frac{1}{T}\sum_{t\in[T]}f_d(U_t, V_t)\bigg).
$$
In fact, the naive estimator (\ref{eq:naive-estimator}) is of this form. It is clear that we cannot use this estimator for offline batched data $\cH_T$: There are no i.i.d. samples from $\cP^{(a_0)}$ because of the policy $\{\pi_t\}_{t\in[T]}$. Instead, we propose the following estimator: 
$$
\hat\btheta_{a_0} = \bphi\bigg(\frac{1}{T}\sum_{t\in[T]}W_tf_1(X_t, Y_t), \ldots, \frac{1}{T}\sum_{t\in[T]}W_tf_d(X_t, Y_t)\bigg),
$$
where $W_t = 1_{\{A_t = a_0\}}\frac{\pi^{nd}(A_t)}{\pi_t(A_t|X_t, \cH_{t-1})}$ for a data-independent probability distribution $\pi^{nd}$ on $\cA$. Similar to the proof of Theorem \ref{thm:theta-estimation}, it's not difficult to see that $\hat\btheta_{a_0}$ is consistent under mild conditions. In fact, (\ref{eq:proposed-estimator}) is a special case of this estimator when $\pi_{\tau}^{nd}$ does not depend on $\tau$. A more detailed analysis is left for future work.

\section{{Analysis of \texttt{MEB}}}\label{pf:cor:alg1-with-theta-estimation}

\subsection{{Additional comments on Example \ref{ex::unknown-xt-linear-regret}}}

In example \ref{ex::unknown-xt-linear-regret}, the joint distribution of $(\xb_t, \tilde\xb_t)$ is: 
\begin{align*}
&\PP((\xb_t, \tilde\xb_t) = (0.2, 1)) = 0.3,\quad \PP((\xb_t, \tilde\xb_t) = (0.2, -1)) = 0.2,\\
&\PP((\xb_t, \tilde\xb_t) = (-0.2, 1)) = 0.2,\quad \PP((\xb_t, \tilde\xb_t) = (-0.2, -1)) = 0.3.
\end{align*}
The optimal action given $(\btheta_a^*)_{a\in\{0, 1\}}$ and $\xb_t$ is 
$$
a_t^*(\xb_t) = \argmax_a\langle\btheta_a^*, \xb_t\rangle = 
\begin{cases}
1,\quad& \text{if }\xb_t = 0.2,\\
0,\quad& \text{if }\xb_t = -0.2.
\end{cases}
$$
In the standard bandit setting, the benchmark policy is 
$$
\pi_t^*(a) = 
\begin{cases}
1,\quad& \text{if }a = a_t^*(\xb_t),\\
0,\quad& \text{otherwise}.
\end{cases}
$$
For any policy $\pi = (\pi_t)_{t\geq 1}$, the instantaneous regret at time $t$ is 
$$
\textrm{Regret}_t(\pi_t, \pi_t^*) = 0.4\EE_{a_t\sim \pi_t(\cdot|\tilde\xb_t, \cH_{t-1})}1_{\{a_t\neq a_t^*(\xb_t)\}}.
$$
Even if both $(\btheta_a^*)_{a\in\{0, 1\}}$ and the joint distribution of $(\xb_t, \tilde\xb_t)$ are known, $\pi_t$ can only depend on $\tilde\xb_t$ and history, and cannot be based on $\xb_t$. Thus, there is always a (constant) positive probability that the action $a_t$ sampled from $\pi_t$ does not match $a_t^*(\xb_t)$ (otherwise, $a_t$ sampled from $\pi_t(\cdot|\tilde\xb_t, \cH_{t-1})$ should be equal to $a_t^*(\xb_t)$ a.s.). Thus, the standard cumulative regret will be linear in the time horizon. %It's easy to see that the situation is similar in the clipped policy setting when the minimum selection probability $p_0$ is small.

%Similarly, in the clipped policy setting with minimum selection probability $p_0$, the benchmark policy is 
%$$
%\bar \pi_t^*(a) = 
%\begin{cases}
%1-p_0,\quad& \text{if }a = a_t^*(\xb_t),\\
%p_0,\quad& \text{otherwise}.
%\end{cases}
%$$
%For a policy $\pi = (\pi_t)_{t\geq 1}$, the instantaneous regret (compared to $\bar\pi^*$) at time $t$ is 
%$$
%\textrm{Regret}_t(\pi_t, \bar\pi_t^*) = 0.4\big[\PP_{a_t\sim \pi_t(\cdot|\tilde\xb_t, \cH_{t-1})}1_{\{a_t\neq a_t^*(\xb_t)\}} - p_0\big].
%$$
%When $p_0$ is small (e.g. $p_0<0.1$), it's also easy to see that even given $(\btheta_a^*)_{a\in\{0, 1\}}$ and the joint distribution of $(\xb_t, \tilde\xb_t)$, the above term cannot be zero, because $\pi_t(\cdot|\tilde\xb_t, \cH_{t-1})$ only depends on $\tilde\xb_t$ and history, and cannot depend on $\xb_t$. Therefore, a linear regret can not be avoided.

\subsection{Proof of Theorem \ref{thm-regret}}\label{pf:thm-regret}

We first prove the lemma below. Its proof is in Appendix \ref{pf::lem::at-star=at-dagger}.
\begin{lemma}\label{lem::at-star=at-dagger}
Under Assumption \ref{ass:small-error}, we have $a_t^* = a_t^\dagger:= \argmax_a\langle\btheta_a^*, \tilde\xb_t\rangle$. Consequently, $\pi_t^* = \pi_t^\dagger$, $\bar\pi_t^* = \bar\pi_t^\dagger$ (given a fixed minimum action selection probability $p_0$), where 
$$
\pi_t^\dagger(a) = 
\begin{cases}
1,\quad& \text{if }a = a_t^\dagger,\\
0,\quad& \text{otherwise},
\end{cases}
\quad
\bar\pi_t^\dagger(a) = 
\begin{cases}
1-p_0,\quad& \text{if }a = a_t^\dagger,\\
p_0,\quad& \text{otherwise}.
\end{cases}
$$
\end{lemma}

In the below, we define 
\begin{align*}
\widehat{\text{Regret}}_t(\pi, \pi^*) : = \EE_{a\sim \pi_t^*}\langle \btheta^*_{a}, \tilde\xb_t\rangle - \EE_{a\sim \pi_t}\langle \btheta^*_{a}, \tilde\xb_t\rangle, \\
\widehat{\text{Regret}}_t(\pi, \bar\pi^*) : = \EE_{a\sim \bar\pi_t^*}\langle \btheta^*_{a}, \tilde\xb_t\rangle - \EE_{a\sim \pi_t}\langle \btheta^*_{a}, \tilde\xb_t\rangle.
\end{align*}

\noindent\textbf{Standard setting.} In the standard setting, we give the lemma below (proof in Appendix \ref{pf::lem::standard-setting-regret-hat}).
\begin{lemma}\label{lem::standard-setting-regret-hat}
Under the assumptions of Theorem \ref{thm-regret}, at any time $t>T_0$,
$$
\widehat{\text{Regret}}_t(\pi, \pi^*)\leq 2p_0^{(t)}R_\theta + 2\max_{a\in\{0, 1\}}\|\hat\btheta_a^{(t-1)} - \btheta_a^*\|_2.
$$
\end{lemma}

Note that for any time $t>T_0$, the instantaneous regret at time $t$: $\text{Regret}_t(\pi, \pi^*) = (\pi_t^*(1)-\pi_t(1))\langle\btheta_1^* - \btheta_0^*, \xb_t\rangle$, and that $\widehat{\text{Regret}}_t(\pi, \pi^*) = (\pi_t^*(1)-\pi_t(1))\langle\btheta_1^* - \btheta_0^*, \tilde\xb_t\rangle$. Moreover,
\begin{align*}
|\langle\btheta_1^* - \btheta_0^*, \tilde\xb_t\rangle|
= &|\langle\btheta_1^* - \btheta_0^*, \xb_t\rangle + \langle\btheta_1^* - \btheta_0^*, \bepsilon_t\rangle|\\
\geq &|\langle\btheta_1^* - \btheta_0^*, \xb_t\rangle| - |\langle\btheta_1^* - \btheta_0^*, \bepsilon_t\rangle|\\
\geq & \frac1\rho |\langle\btheta_1^* - \btheta_0^*, \bepsilon_t\rangle| - |\langle\btheta_1^* - \btheta_0^*, \bepsilon_t\rangle|\\
= & \frac{1-\rho}{\rho} |\langle\btheta_1^* - \btheta_0^*, \bepsilon_t\rangle|.
\end{align*}
Here we used Assumption \ref{ass:small-error}. Thus we have
\begin{align*}
\text{Regret}_t(\pi, \pi^*)
& = \widehat{\text{Regret}}_t(\pi, \pi^*) - (\pi_t^*(1)-\pi_t(1))\langle\btheta_1^* - \btheta_0^*, \bepsilon_t\rangle\\
&\leq \widehat{\text{Regret}}_t(\pi, \pi^*) + |\pi_t^*(1)-\pi_t(1)|\cdot \frac{\rho}{1-\rho}|\langle\btheta_1^* - \btheta_0^*, \tilde\xb_t\rangle|\\
& = \frac{1}{1-\rho}\widehat{\text{Regret}}_t(\pi, \pi^*).
\end{align*}
Combining Lemma \ref{lem::standard-setting-regret-hat}, we obtain that for any $t>T_0$, 
$$
\text{Regret}_t(\pi, \pi^*)\leq \frac2{1-\rho}\big(p_0^{(t)}R_\theta + \max_{a\in\{0, 1\}}\|\hat\btheta_a^{(t-1)} - \btheta_a^*\|_2\big).
$$

Finally, when $t\leq T_0$, since $\|\xb_t\|\leq 1$, $\|\btheta_a^*\|\leq R_\theta$, the instantaneous regret $\text{Regret}_t(\pi, \pi^*)\leq 2R_\theta$. We conclude the proof by summing up all the instantaneous regret terms.

\noindent\textbf{Clipped policy setting.} In the clipped policy setting, we give the lemma below (proof in Appendix \ref{pf::lem::clipped-setting-regret-hat}).

\begin{lemma}\label{lem::clipped-setting-regret-hat}
Under the assumptions of Theorem \ref{thm-regret}, at any time $t>T_0$, 
$$
\widehat{\text{Regret}}_t(\pi, \bar\pi^*)
\leq 2(1-2p_0) \max_{a\in\{0, 1\}}\|\hat\btheta_a^{(t-1)} - \btheta_a^*\|_2.
$$
\end{lemma}

Note that the instantaneous regret at time $t$: $\text{Regret}_t(\pi, \bar\pi^*) = (\bar\pi_t^*(1)-\pi_t(1))\langle\btheta_1^* - \btheta_0^*, \xb_t\rangle$, and that $\widehat{\text{Regret}}_t(\pi, \bar\pi^*) = (\bar\pi_t^*(1)-\pi_t(1))\langle\btheta_1^* - \btheta_0^*, \tilde\xb_t\rangle$. Similar to the standard setting, for $t>T_0$, under Assumption \ref{ass:small-error}, we have 
$$
\text{Regret}_t(\pi, \bar\pi^*)\leq \frac{1}{1-\rho}\widehat{\text{Regret}}_t(\pi, \bar\pi^*)\leq \frac{2(1-2p_0)}{1-\rho}\max_{a\in\{0, 1\}}\|\hat\btheta_a^{(t-1)} - \btheta_a^*\|_2.
$$
We conclude the proof by summing up all the instantaneous regret terms, and noticing that for $t\leq T_0$, $\text{Regret}_t(\pi, \bar\pi^*)\leq 2R_\theta$.

\noindent\textbf{Results with a high-probability version of Assumption \ref{ass:small-error}.} As briefly mentioned in the main paper, Assumption \ref{ass:small-error} can be weakened to the inequalities holding with high probability. Now instead of Assumption \ref{ass:small-error}, we assume the following:
\begin{assumption}\label{ass:small-error-high-probability}
There exist constants $\rho\in(0, 1)$, $c_e\in[0, 1]$ such that $\sum_{t=1}^T\PP(A_t^c)\leq c_e$. Here $A_t$ denotes the event $\{|\langle\bm{\delta}_\theta, \bepsilon_t\rangle|\leq \rho|\langle\bm{\delta}_\theta, \xb_t\rangle|\}$, and $\bm{\delta}_\theta = \btheta_1^* - \btheta_0^*$.
\end{assumption}

It's easy to see that the result of Lemma \ref{lem::at-star=at-dagger} hold at time $t$ under the event $A_t$. Further, following the same arguments, we obtain that under Assumption \ref{ass:boundedness}, the results for either the standard setting or the clipped policy setting hold under the event $\cap_{t = T_0+1}^TA_t$. Therefore, with Assumption \ref{ass:boundedness} and \ref{ass:small-error-high-probability}, in either setting, the results in Theorem \ref{thm-regret} hold with probability at least $1-c_e$.

\subsection{Proof of Corollary \ref{cor:alg1-with-theta-estimation}}\label{Appendix:subsection:Proof-of-corollary-2.1}

\noindent\textbf{Standard setting.} First, notice that $q_t = \min_{\tau\leq t, a\in\{0, 1\}}\pi_\tau(a|\tilde\xb_t, \cH_{\tau-1}) = p_0^{(t)}$, since $p_0^{(t)}$ is monotonically decreasing in $t$. Theorem \ref{thm:theta-estimation} indicates that, as long as $\forall t> T_0$,
\begin{equation}\label{eq:standard-benchmark-condition1}
p_0^{(t)}\geq {C_1}\max\left\{\frac{d(d+\log t)}{\lambda_0 t}, \frac{\xi(d+\log t)}{\lambda_0^2t}\right\},
\end{equation}
then with probability at least $1-\frac{8}{t^2}$, $\forall a\in\{0, 1\}$,
\begin{align}
\|\hat\btheta_a^{(t)}-\btheta_a^*\|_2
&\leq \frac{C(R+\!R_\theta)d}{\lambda_0}\max\left\{\!\frac{d+\log t}{q_t t}, \frac{\sqrt{\nu} + \sqrt{\xi}}{\sqrt{d}}\sqrt{\frac{d+\log t}{q_t t}}\right\}\label{eq:standard-benchmark-estimation-error}
\end{align}

Plug it into Theorem \ref{thm-regret}, we have that with high probability, 
\begin{align*}
\text{Regret}(T;\pi^*)
&\leq 2R_\theta\cdot \lceil 2dT^{2/3}\rceil + \frac2{1-\rho} I_1,
\end{align*}
where
\begin{align*}
I_1& = \sum_{t=T_0+1}^T
\bigg(p_0^{(t)}R_\theta + \max_a\|\hat \btheta_a^{(t-1)} - \btheta^*_a\|_2\bigg)\\
&\leq \sum_{t=T_0+1}^T t^{-\frac13}R_\theta + \sum_{t=T_0}^{T-1}\frac{C(R+\!R_\theta)d}{\lambda_0}\max\left\{\!\frac{d+\log t}{t^{\frac23}}, \frac{\sqrt{\nu} + \sqrt{\xi}}{\sqrt{d}}\sqrt{\frac{d+\log t}{t^{\frac23}}}\right\}\\
&\leq 2R_\theta T^{\frac23} + \frac{C(R+\!R_\theta)d}{\lambda_0}\cdot \sum_{t=T_0}^{T-1} \frac{d+\log t}{t^{\frac23}} + \frac{C(R+\!R_\theta)d}{\lambda_0}\cdot\sum_{t=T_0}^{T-1}\frac{\sqrt{\nu} + \sqrt{\xi}}{\sqrt{d}}\sqrt{\frac{d+\log t}{t^{\frac23}}}\\
&\leq 2R_\theta T^{\frac23} + \frac{3C(R+\!R_\theta)}{\lambda_0}d(d+\log T)T^{\frac13} + \frac{3C(\sqrt{\nu} + \sqrt{\xi})(R+\!R_\theta)}{2\lambda_0}\sqrt{d(d+\log T)}T^{\frac23}\\
&\leq 2R_\theta T^{\frac23} +\frac{C'}{\lambda_0}(\sqrt{\nu} + \sqrt{\xi} + 1)(R+\!R_\theta)\sqrt{d(d+\log T)} T^{\frac23},
\end{align*}
for a universal constant $C'$, where the last inequality holds if in addition, $T\geq[d(d+\log T)]^{\frac32}$.

The proof is concluded by combining the above requirement for $T$ as well as \eqref{eq:standard-benchmark-condition1}.

\noindent\textbf{Clipped policy setting.} Similar to the standard setting, according to Theorem \ref{thm:theta-estimation}, as long as $\forall t> T_0$,
\begin{equation}\label{eq:clipped-benchmark-condition1}
p_0\geq {C_1}\max\left\{\frac{d(d+\log t)}{\lambda_0 t}, \frac{\xi(d+\log t)}{\lambda_0^2t}\right\},
\end{equation}
then with probability at least $1-\frac{8}{t^2}$, $\forall a\in\{0, 1\}$,
\begin{align}
\|\hat\btheta_a^{(t)}-\btheta_a^*\|_2
&\leq \frac{C(R+\!R_\theta)d}{\lambda_0}\max\left\{\!\frac{d+\log t}{p_0 t}, \frac{\sqrt{\nu} + \sqrt{\xi}}{\sqrt{d}}\sqrt{\frac{d+\log t}{p_0 t}}\right\}\label{eq:clipped-benchmark-estimation-error}
\end{align}

Plug it into Theorem \ref{thm-regret}, we have that with high probability, 
\begin{align*}
\text{Regret}(T;\bar \pi^*)
&\leq 2R_\theta\cdot \lceil 2dT^{1/2}\rceil + \frac{2(1-2p_0)}{1-\rho} I_2,
\end{align*}
where
\begin{align*}
I_2& = \sum_{t=T_0+1}^T \max_a\|\hat\btheta_a^{(t-1)}-\btheta^*_a\|_2\\
&\leq \sum_{t=T_0}^{T-1}\frac{C(R+\!R_\theta)d}{\lambda_0}\max\left\{\!\frac{d+\log t}{p_0 t}, \frac{\sqrt{\nu} + \sqrt{\xi}}{\sqrt{d}}\sqrt{\frac{d+\log t}{p_0 t}}\right\}\\
&\leq \frac{C(R+\!R_\theta)d}{\lambda_0}\sum_{t=T_0}^{T-1}\frac{d+\log t}{p_0 t} + \frac{C(R+\!R_\theta)d}{\lambda_0}\sum_{t=T_0}^{T-1}\frac{\sqrt{\nu} + \sqrt{\xi}}{\sqrt{d}}\sqrt{\frac{d+\log t}{p_0 t}}\\
&\leq \frac{2C(R+\!R_\theta)}{\lambda_0}d(d+\log T)\log T + \frac{2C(R+\!R_\theta)}{\lambda_0}\frac{\sqrt{\nu} + \sqrt{\xi}}{\sqrt{p_0}}\sqrt{d(d+\log T)}\sqrt{T}\\
&\leq \frac{2C(R+R_\theta)(\sqrt{\nu}+\sqrt{\xi}+1)}{\lambda_0\sqrt{p_0}}\sqrt{d(d+\log T)}\sqrt{T},
\end{align*}
where the last inequality holds if in addition, $T\geq d(d+\log T)\log^2 T$. 

The proof is concluded by plugging the above into the regret upper bound formula and combining the requirements for $T$.

\noindent\textbf{Results with a high-probability version of Assumption \ref{ass:small-error}.} Recall that Assumption \ref{ass:small-error-high-probability} is a weakened version of Assumption \ref{ass:small-error} with a high-probability statement. Given Assumptions \ref{ass:boundedness}, \ref{ass:min-signal}, and \ref{ass:small-error-high-probability} (instead of \ref{ass:small-error}), in the standard setting, the results of Theorem \ref{cor:alg1-with-theta-estimation} hold as long as (\ref{eq:standard-benchmark-estimation-error}) or (\ref{eq:clipped-benchmark-estimation-error}) for all $t>T_0$, $a\in\{0, 1\}$ and under the event $\cap_{t = T_0+1}^TA_t$. Thus, we deduce that the regret upper bound in (i) hold with probability at least $1-16/\sqrt{T} - c_e$. Similarly, given Assumptions \ref{ass:boundedness}, \ref{ass:min-signal}, and \ref{ass:small-error-high-probability}, in the clipped benchmark setting, the regret upper bound in (ii) hold with probability at least $1-16/\sqrt{T} - c_e$.

\subsection{\texttt{MEB} with infrequent model update}
\begin{algorithm}[t]
\caption{\texttt{MEB} with infrequent model update}	
\label{alg2}
\begin{algorithmic}		
\STATE \textbf{{Input}}: $(\bSigma_{e, t})_{t\in[T]}$: covariance sequence of $(\bepsilon_t)_t$; $(p_0^{(t)})_{t\in[T]}$: minimum selection probability at each time $t$; $\cS\subset [T]$: set of time points to update model estimates
\STATE $\hat\btheta_a \leftarrow 0$, $\forall a\in\{0, 1\}$  \thickspace \thickspace {\color{blue}$\%$\thinspace$\hat\btheta_a$ stores the most recent updated estimate of $\btheta_a^*$, only update if $t\in\cS$}
        \FOR{time $t = 1, 2, \ldots, T$}
        \IF{$\min_{s\in\cS}s\geq t$}
        \STATE Sample $a_t\sim \pi_t(\cdot|\tilde\xb_t, \cH_{t-1})$, where $\pi_t(a|\tilde\xb_t, \cH_{t-1}) \in[p_0^{(t)}, 1-p_0^{(t)}]$ for all $a\in\{0, 1\}$\\
        {\color{blue} $\%$ If the model has never been learned before, explore with equal probability}
        \STATE \textbf{continue}
        \ENDIF
        \STATE $\tilde a_t \leftarrow \argmax_{a\in\{0, 1\}}\langle\hat\btheta_{a}, \tilde\xb_t\rangle$
        \STATE Sample $a_t\sim \pi_t(\cdot|\tilde\xb_t, \cH_{t-1})$, where $\pi_t(a|\tilde\xb_t, \cH_{t-1}):=
        \begin{cases}
        1-p_{0}^{(t)}, \quad\text{if }a = \tilde a_t\\
        p_{0}^{(t)}, \quad\text{otherwise}
        \end{cases}
        $
        \IF{$t\in\cS$}
        \STATE $\hat\btheta_a\leftarrow \hat\btheta_a^{(t)}$ as in (\ref{eq:proposed-estimator})
        \ENDIF
        \ENDFOR
	\end{algorithmic}
\end{algorithm}

As mentioned at the end of Section \ref{section:meb}, in certain scenarios (e.g. when $d$ is large), we can save computational resources by updating the estimates of $(\btheta_a^*)_{a\in\{0, 1\}}$ less frequently. In Algorithm \ref{alg2}, we propose a variant of Algorithm \ref{alg1}. At each time $t$, given the noisy context $\tilde\xb_t$, the algorithm computes the best action $\tilde a_t$ according to the most recently updated estimators of $(\btheta_a^*)_{a\in\{0, 1\}}$. Then, it samples $\tilde a_t$ with probability $1-p_0^{(t)}$ and keeps an exploration probability of $p_0^{(t)}$ to sample the other action. In the meantime, the agent only has to update the estimate of $(\btheta_a^*)_{a\in\{0, 1\}}$ once in a while to save computation power: The algorithm specifies a subset $\cS\subset[T]$ and updates the estimators according to (\ref{eq:proposed-estimator}) only when $t\in\cS$. 

Under mild conditions, Algorithm \ref{alg2} achieves the same order of regret upper bound as Algorithm \ref{alg1}, as seen from Theorem \ref{thm-regret-S} and Corollary \ref{cor:alg1-with-theta-estimation-S} below. They are modified versions of Theorem \ref{thm-regret} and Corollary \ref{cor:alg1-with-theta-estimation}.

\begin{theorem}\label{thm-regret-S}
Let $s_{\min}:=\min_{s\in\cS}s$ be the first time Algorithm \ref{alg2} updates the model. Suppose Assumption \ref{ass:boundedness} and \ref{ass:small-error} hold. 

\begin{itemize}
\item[(i)] For the standard setting, for any $T_0\geq s_{\min}$, Algorithm \ref{alg2} outputs a policy such that 
$$
\text{Regret}(T; \pi^*)\leq 2T_0R_{\theta}+\frac{2}{1-\rho}\sum_{t\in(T_0, T]}\big(p_0^{(t)}R_\theta + \max_{a\in\{0, 1\}}\|\hat\btheta_a^{(s_t)}-\btheta_a^*\|_2\big).
$$
\item[(ii)] For the clipped policy setting, for any  $T_0\geq s_{\min}$, Algorithm \ref{alg2} with the choice of $p_0^{(t)}\equiv p_0$ outputs a policy such that
$$
\text{Regret}(T; \bar\pi^*)\leq 2T_0R_{\theta}+\frac{2(1-2p_0)}{1-\rho}\sum\nolimits_{t\in(T_0, T]}\max_{a\in\{0, 1\}}\|\hat\btheta_a^{(s_t)}-\btheta_a^*\|_2.
$$
\end{itemize}
Here for any $t\in[T]$, $s_t := \max\{s\in\cS: s<t\}$.
\end{theorem}

The proof of Theorem \ref{thm-regret-S} is very similar to that of Theorem \ref{thm-regret}, and is thus omitted.

\begin{corollary}\label{cor:alg1-with-theta-estimation-S}
Let Assumption \ref{ass:boundedness} to \ref{ass:small-error} hold. There exist constants $C, C'$ such that:
\begin{itemize}
    \item[(i)] In the standard setting, as long as the set of model update times $\cS$ satisfies: (a) $s_{\min}\leq dT^{2/3}$; (b) $\forall t\in(dT^{2/3}, T]$, $s_t = \max\{s\in\cS: s<t\} \geq \alpha t$ for some constant $\alpha\in(e^{-d}, 1)$, then $\forall T\geq C'/\alpha\cdot \max\left\{(1+1/\lambda_0^{\frac94})(d+\log T)^3, (\xi/\lambda_0)^{\frac94}(d+\log T)^{\frac43}\right\}$, with probability at least $1-\frac{16}{\sqrt{T}}$, Algorithm \ref{alg2} with the choice of $p_0^{(t)} = \min\{\frac12, t^{-1/3}\}$ achieves $\text{Regret}(T; \pi^*)\leq CdT^{\frac23}\left\{\frac{R_\theta}{\alpha}+\frac{R_\theta}{1-\rho}+\frac{(\sqrt{\nu}+\sqrt{\xi}+1)(R+R_\theta)}{\alpha^{1/3}\lambda_{0}(1-\rho)}\sqrt{1+\frac{\log T}{d}}\right\}$.
    \item[(ii)] In the clipped policy setting, as long as the set of model update times $\cS$ satisfies: (a) $s_{\min}\leq d\sqrt{T}$; (b) $\forall t\in(d\sqrt{T}, T]$, $s_t = \max\{s\in\cS: s<t\} \geq \alpha t$ for some constant $\alpha\in(e^{-d}, 1)$, then for any $T$ s.t. $T\geq C'/\alpha\cdot \max\{(d+\log T)^2/(\lambda_0p_0)^2, \xi^2/\lambda_0^4(1+\log T/d)^2\}$, with probability at least $1-\frac{16}{\sqrt{T}}$, Algorithm \ref{alg2} with the choice of $p_0^{(t)}\equiv p_0$ achieves: $\text{Regret}(T; \bar\pi^*)\leq Cd\sqrt{T}\left\{\frac{R_\theta}{\alpha}+\frac{(\sqrt{\nu}+\sqrt{\xi} + 1)(1-2p_0)(R+R_\theta)}{\lambda_{0}(1-\rho)\sqrt{\alpha p_0}}\sqrt{1+\frac{\log T}{d}}\right\}$.
\end{itemize}
\end{corollary}

The proof of Corollary \ref{cor:alg1-with-theta-estimation-S} can be directly obtained by combining Theorem \ref{thm:theta-estimation} and \ref{thm-regret-S} with $T_0 = dT^{2/3}/\alpha$ in the standard setting and $T_0 = dT^{1/2}/\alpha$ in the clipped benchmark setting. Thus, the proof is omitted here.

In Corollary \ref{cor:alg1-with-theta-estimation-S}, condition (a) and (b) essentially requires Algorithm \ref{alg2} not to start learning the model too late, and to keep updating the learned model at least at time points with a `geometric' growth rate. This covers a wide range of choices of $\cS$ in practice. Two typical examples of $\cS$ could be: (1) $\cS=\{t\in[T]: t=kt_0\text{ for some }k\in\mathbb N^+\}$ (the model is learned every $t_0$ time points routinely, where $t_0$ is a constant integer); (2) If $1/\alpha\in \mathbb N^+$, $\cS=\{t\in[T]: t=(1/\alpha)^k\text{ for some }k\in\mathbb N^+\}$ (the model only needs to be learned $\cO(\log T)$ times to save computation).

\section{{Analysis with estimated error variance}}\label{appendix:estimated-error-variance}

We consider the setting where at each time $t$, the agent does not have access to $\bSigma_{e, t}$, but has a (potentially adaptive) estimator $\hat\bSigma_{e, t}$. In this setting, we estimate the model using (\ref{eq::proposed-estimator-estimated-Sigma}) instead of (\ref{eq:proposed-estimator}) and plug into Algorithm \ref{alg1}. The following theorem controls the estimation error of $\tilde\btheta_a^{(t)}$. Note that compared to Theorem \ref{thm:theta-estimation}, the additional error caused by inaccuracty of $\hat\bSigma_{e, t}$ can be controlled by $\bDelta_t(a):= \frac{1}{t}\sum_{\tau\in[t]}\pi_\tau^{nd}(a)(\hat\bSigma_{e, \tau} - \bSigma_{e, \tau})$, the weighted average of the estimation errors $(\hat\bSigma_{e, \tau} - \bSigma_{e, \tau})_{\tau\in[t]}$.


\begin{theorem}\label{thm:theta-estimation-estimated-Sigma}
Recall that $q_t\!:=\inf_{\tau\leq t, a\in\{0, 1\}}\pi_\tau(a|\tilde\xb_\tau, \cH_{\tau-1})$. Then under Assumptions \ref{ass:boundedness} and \ref{ass:min-signal}, there exist constants $C$ and $C_1'$ such that as long as $q_t\geq {C_1'}\max\left\{\frac{d(d+\log t)}{\lambda_0t}, \frac{\xi(d+\log t)}{\lambda_0^2t}\right\}$ and $\max_{a\in\{0, 1\}}\|\bDelta_t(a)\|_2\leq \frac{\lambda_{0}}{4d}$, with probability at least $1\!-\!\frac{8}{t^2}$,
\begin{equation}\label{eq:thm:theta-estimation-estimated-Sigma}
    \|\tilde\btheta_a^{(t)}-\btheta_a^*\|_2\leq \frac{C(R+R_{\btheta})d}{\lambda_{0}}\max\bigg\{\frac{d\!+\!\log t}{q_tt}, \frac{\sqrt{\xi}+\sqrt{\nu}}{\sqrt{d}}\cdot \sqrt{\frac{d\!+\!\log t}{q_tt}}\bigg\} + \frac{2R_\theta d}{\lambda_{0}}\|\bDelta_t(a)\|_2, \quad \forall a\in\{0, 1\}.
\end{equation}
\end{theorem}
The proof of Theorem \ref{thm:theta-estimation-estimated-Sigma} is in Appendix \ref{pf::theta-estimation-estimated-Sigma}.

By combining Theorem \ref{thm:theta-estimation-estimated-Sigma} and \ref{thm-regret} (with (\ref{eq::proposed-estimator-estimated-Sigma}) instead of (\ref{eq:proposed-estimator})), we obtain the following regret bounds for Algorithm \ref{alg1} with (\ref{eq::proposed-estimator-estimated-Sigma}) as the plug-in estimator.

\begin{corollary}\label{cor:alg1-with-theta-estimation-estimated-Sigma}
Suppose Assumption \ref{ass:boundedness} to \ref{ass:small-error} hold. Then there exist universal constants $C, C''$ such that:
\begin{itemize}
    \item[(i)] In the standard setting, if $\max_{t\in[dT^{2/3}, T]}\max_{a\in\{0, 1\}}\|\bDelta_t(a)\|_2\leq \frac{\lambda_0}{4d}$, $T\geq C''\max\{(1+{1}/{\lambda_0^{\frac94}})(d+\log T)^3, (\xi/\lambda_0)^{\frac94}(d+\log T)^{\frac43}\}$, by choosing $T_0 = \lceil2dT^{\frac23}\rceil$, $p_0^{(t)} = \min\{\frac12, t^{-\frac13}\}$, and (\ref{eq::proposed-estimator-estimated-Sigma}) instead of (\ref{eq:proposed-estimator}) in Algorithm \ref{alg1}, with probability at least $1-\frac{16}{\sqrt{T}}$,
    $$
\text{Regret}(T;\pi^*)\leq CdT^{\frac23}\left\{\frac{R_\theta}{1-\rho} + \frac{(\!\sqrt{\nu}\! +\!\! \sqrt{\xi}\! +\!\! 1)(R+R_\theta)}{(1-\rho)\lambda_0} \sqrt{1+\frac{\log T}{d}}\right\} + \frac{4R_\theta d}{(1-\rho)\lambda_0}\sum\limits_{t=T_0-1}^T\max\limits_{a\in\{0, 1\}}\|\bDelta_{t}(a)\|_2.
$$
    
\item[(ii)] In the clipped policy setting, as long as $\max_{t\in[d\sqrt{T}, T]}\max_{a\in\{0, 1\}}\|\bDelta_t(a)\|_2\leq \frac{\lambda_0}{4d}$, $T\geq C''\max\{(d+\log T)^2/(\lambda_0p_0)^2, \xi^2/\lambda_0^4(1+\log T/d)^2\}$, by choosing $T_0 = \lceil2d\sqrt{T}\rceil$, $p_0^{(t)}\equiv p_0$, and (\ref{eq::proposed-estimator-estimated-Sigma}) instead of (\ref{eq:proposed-estimator}) in Algorithm \ref{alg1}, with probability at least $1-\frac{16}{\sqrt{T}}$, 
$$
\text{Regret}(T;\bar\pi^*)\leq CdT^{\frac12}\bigg\{\!\!R_\theta + \frac{(\!\sqrt{\nu}\! +\!\! \sqrt{\xi}\! +\!\! 1)(1\!-\!2p_0)(R\!+\!R_\theta)}{\sqrt{p_0}(1\!-\!\rho)\lambda_0}\sqrt{1\!+\!\frac{\log T}{d}}\bigg\}+\frac{4(1\!-\!2p_0)R_\theta d}{(1-\rho)\lambda_0}\sum\limits_{t = T_0-1}^T\max\limits_{a\in\{0, 1\}}\|\bDelta_{t}(a)\|_2.
$$
\end{itemize}
\end{corollary}

The proof is in Appendix \ref{pf::cor:alg1-with-theta-estimation-estimated-Sigma}.


\section{Additional proofs}

\subsection{Proof of Theorem \ref{thm:regret-multiple-actions}}\label{pf:thm:regret-multiple-actions}

The proof of Theorem \ref{thm:regret-multiple-actions} is very similar to Theorem \ref{thm-regret}, and we only need to note the difference in Lemma \ref{lem::standard-setting-regret-hat} and \ref{lem::clipped-setting-regret-hat} (for the standard setting and the clipped policy setting respectively), as stated below. Recall that 
\begin{align*}
\widehat{\text{Regret}}_t(\pi, \pi^*) : = \EE_{a\sim \pi_t^*}\langle \btheta^*_{a}, \tilde\xb_t\rangle - \EE_{a\sim \pi_t}\langle \btheta^*_{a}, \tilde\xb_t\rangle, \\
\widehat{\text{Regret}}_t(\pi, \bar\pi^*) : = \EE_{a\sim \bar\pi_t^*}\langle \btheta^*_{a}, \tilde\xb_t\rangle - \EE_{a\sim \pi_t}\langle \btheta^*_{a}, \tilde\xb_t\rangle.
\end{align*}

\noindent\textbf{Standard setting.} At any time $t>T_0$, we have
\begin{align}
\widehat{\text{Regret}}_t(\pi, \pi^*) &= \EE_{a\sim \pi_t^*}\langle \btheta^*_{a}, \tilde\xb_t\rangle - \EE_{a\sim \pi_t}\langle \btheta^*_{a}, \tilde\xb_t\rangle\nonumber\\
&=\langle\btheta^*_{a_t^*}, \tilde\xb_t\rangle-\big[(1-(K-1)p_0^{(t)})\langle\btheta^*_{\tilde a_t}, \tilde\xb_t\rangle+\sum_{a\neq \tilde a_t}p_0^{(t)}\langle\btheta^*_{a}, \tilde\xb_t\rangle\big]\nonumber\\
&= p_0^{(t)}\sum_{a\neq a_t^*}\langle \btheta^*_{a_t^*} - \btheta^*_{a}, \tilde\xb_t\rangle + 1_{\{a_t^*\neq \tilde a_t\}}(1-Kp_0^{(t)})\langle \btheta^*_{a_t^*} - \btheta^*_{\tilde a_t}, \tilde\xb_t\rangle\nonumber\\
&\leq 2(K-1)p_0^{(t)}R_\theta + 1_{\{a_t^*\neq \tilde a_t\}}\langle \btheta^*_{a_t^*} - \btheta^*_{\tilde a_t}, \tilde\xb_t\rangle.\label{eq:instant-regret-standard-benchmark-multiple-actions}
\end{align}
Here note that Lemma \ref{lem::at-star=at-dagger} still holds under Assumption \ref{ass:small-error-K-actions}, so $a_t^*=\argmax_a\langle \btheta^*_{a}, \xb_t\rangle=\argmax_a\langle \btheta^*_{a}, \tilde\xb_t\rangle$, $\tilde a_t = \argmax_{a}\langle\hat\btheta_{a}^{(t-1)}, \tilde\xb_t\rangle$.

Note that $a_t^*\neq \tilde a_t$ implies that 
\begin{align*}
\begin{cases}
\langle\btheta_{a_t^*}^*, \tilde\xb_t\rangle\geq \langle\btheta_{\tilde a_t}^*, \tilde\xb_t\rangle\\
\langle\hat\btheta_{a_t^*}^{(t-1)}, \tilde\xb_t\rangle\leq \langle\hat\btheta_{\tilde a_t}^{(t-1)}, \tilde\xb_t\rangle
\end{cases}
\end{align*}
which leads to 
\begin{align*}
\langle \btheta_{a_t^*}^*, \tilde\xb_t\rangle&\geq \langle\btheta_{\tilde a_t}^*, \tilde\xb_t\rangle\geq \langle\hat\btheta_{\tilde a_t}^{(t-1)}, \tilde\xb_t\rangle - \max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2\\
&\geq \langle\hat\btheta_{a_t^*}^{(t-1)}, \tilde\xb_t\rangle - \max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2\geq \langle\btheta_{a_t^*}^*, \tilde\xb_t\rangle - 2\max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2,
\end{align*}
and further implies $\left|\langle \btheta^*_{a_t^*} - \btheta^*_{\tilde a_t}, \tilde\xb_t\rangle\right|\leq 2\max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2$.
Plugging in the above to (\ref{eq:instant-regret-standard-benchmark-multiple-actions}) leads to 
\begin{align*}
\widehat{\text{Regret}}_t(\pi, \pi^*)&\leq 2(K-1)p_0^{(t)}R_\theta + 2\max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2.
\end{align*}

The rest of the proof can be done in the same way as the proof of Theorem \ref{thm-regret}.

\noindent\textbf{Clipped policy setting.} At any time $t>T_0$, we have 
\begin{align}
\widehat{\text{Regret}}_t(\pi, \bar\pi^*) &= \EE_{a\sim \pi_t^*}\langle \btheta^*_{a}, \tilde\xb_t\rangle - \EE_{a\sim \pi_t}\langle \btheta^*_{a}, \tilde\xb_t\rangle\nonumber\\
&\leq (1-Kp_0)1_{\{a_t^*\neq \tilde a_t\}}\left|\langle \btheta^*_{a_t^*} - \btheta^*_{\tilde a_t}, \tilde\xb_t\rangle\right|.\label{eq:instant-regret-multiple-actions}
\end{align}
Here recall that $a_t^*=\argmax_a\langle \btheta^*_{a}, \tilde\xb_t\rangle$, $\tilde a_t := \argmax_{a}\langle\hat\btheta_{a}^{(t-1)}, \tilde\xb_t\rangle$.

Note that $a_t^*\neq \tilde a_t$ implies that 
\begin{align*}
\begin{cases}
\langle\btheta_{a_t^*}^*, \tilde\xb_t\rangle\geq \langle\btheta_{\tilde a_t}^*, \tilde\xb_t\rangle\\
\langle\hat\btheta_{a_t^*}^{(t-1)}, \tilde\xb_t\rangle\leq \langle\hat\btheta_{\tilde a_t}^{(t-1)}, \tilde\xb_t\rangle
\end{cases}
\end{align*}
which leads to 
\begin{align*}
\langle \btheta_{a_t^*}^*, \tilde\xb_t\rangle&\geq \langle\btheta_{\tilde a_t}^*, \tilde\xb_t\rangle\geq \langle\hat\btheta_{\tilde a_t}^{(t-1)}, \tilde\xb_t\rangle - \max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2\\
&\geq \langle\hat\btheta_{a_t^*}^{(t-1)}, \tilde\xb_t\rangle - \max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2\geq \langle\btheta_{a_t^*}^*, \tilde\xb_t\rangle - 2\max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2,
\end{align*}
and further implies $\left|\langle \btheta^*_{a_t^*} - \btheta^*_{\tilde a_t}, \tilde\xb_t\rangle\right|\leq 2\max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2$.

Plugging in the above to (\ref{eq:instant-regret-multiple-actions}) leads to 
$$
\widehat{\text{Regret}}_t(\pi, \bar\pi^*)\leq 2(1-Kp_0)\max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2
$$
for $t>T_0$. The rest of the proof can be done in the same way as the proof of Theorem \ref{thm-regret}.


\subsection{Proof of Lemma \ref{lem:hat-Sigma-x-r}}\label{pf:lem:hat-Sigma-x-r}
We first analyze $\hat\bSigma_{\tilde\xb, a}^{(t)}$. Notice that $\hat\bSigma_{\tilde\xb, a}^{(t)}=\frac{1}{t}\sum_{\tau\in[t]}\bV_{\tau, a}$, where $\bV_{\tau, a}=\frac{\pi_{\tau}^{nd}(A_\tau)}{\pi_\tau(A_\tau|\tilde\xb_\tau, \cH_{\tau-1})}1_{\{A_\tau=a\}}\tilde\xb_\tau\tilde\xb_{\tau}^\top$.  For any fixed $\bu\in\mathbb S^{d-1}:= \{\bu'\in \mathbb R^d: \|\bu'\|_2=1\}$, $(v_{\bu, \tau, a}:=\bu^\tau[\bV_{\tau, a}-\EE[\bV_{\tau, a}|\cH_{\tau-1}]]\bu)_\tau$ is a martingale difference sequence. Moreover, we can verify that $|v_{\bu, \tau, a}|\leq \frac{2}{q_t}$ and
\begin{align*}
    \Var(v_{\bu, \tau, a}|\cH_{\tau-1})&
    \leq \EE[(\bu^\top \bV_{\tau, a}\bu)^2|\cH_{\tau-1}]=\EE\left[\left(\frac{\pi_{\tau}^{nd}(A_\tau)}{\pi_\tau(A_\tau|\tilde\xb_\tau, \cH_{\tau-1})}\right)^21_{\{A_\tau=a\}}(\bu^\top\tilde\xb_\tau\tilde\xb_\tau^\top\bu)^2\bigg|\cH_{\tau-1}\right]\\
    &=\EE_{\bepsilon_\tau, A_\tau\sim \pi_{\tau}^{nd}(\cdot)}\left[\left(\frac{\pi_{\tau}^{nd}(A_\tau)}{\pi_\tau(A_\tau|\tilde\xb_\tau, \cH_{\tau-1})}\right)1_{\{A_\tau=a\}}(\bu^\top\tilde\xb_\tau\tilde\xb_\tau^\top\bu)^2\bigg|\cH_{\tau-1}\right]\leq \frac{1}{q_t}\mathbb E(\bu^\top\tilde\xb_\tau)^4\leq \frac{\xi}{d^2q_t}.
\end{align*}

According to Freedman's Inequality \citep{freedman1975tail}, for any $\gamma_1, \gamma_2>0$, 
$$
\PP\left(\sum_{\tau\in[t]}v_{\bu, \tau, a}\geq \gamma_1, \sum_{\tau\in[t]}\Var(v_{\bu, \tau, a}|\cH_{\tau-1})\leq \gamma_2\right)\leq e^{-\frac{\gamma_1^2}{2(\frac{2}{q_t}\gamma_1+\gamma_2)}}.
$$
Set $\gamma_2 = \frac{\xi t}{d^2q_t}$, and we obtain 
$\PP\left(\sum_{\tau\in[t]}v_{\bu, \tau, a}\geq \gamma_1\right)\leq e^{-\frac{d^2q_t\gamma_1^2}{2(2d^2\gamma_1+\xi t)}}
$. Applying the same analysis to $(-v_{\bu, \tau, a})_\tau$ and combining the results gives $\PP\left(|\sum_{\tau\in[t]}v_{\bu, \tau, a}|\geq \gamma_1\right)\leq 2e^{-\frac{d^2q_t\gamma_1^2}{2(2d^2\gamma_1+\xi t)}}
$.

Denote $\bM_t= \frac{1}{t}\sum_{\tau\in[t]}(\bV_{\tau, a}-\EE[\bV_{\tau, a}|\cH_{\tau-1}])$, then the above means that $\forall \bu\in\mathbb S^{d-1}$, 
\begin{equation}\label{eq:v-concentration-singledirection} 
  \PP\left(|\bu^\top\bM_t\bu|\geq \frac{\gamma_1}{t}\right)\leq 2e^{-\frac{d^2q_t\gamma_1^2}{2(2d^2\gamma_1+\xi t)}}.
\end{equation}%
Let $\cN$ be a $\frac{1}{4}$-net of $\mathbb S^{d-1}$, $|\cN|\leq 9^d$. $\forall \bu\in\mathbb S^{d-1}$, find $\bu'\in\cN$ s.t. $\|\bu-\bu'\|_2\leq \frac{1}{4}$, and we have 
$$
|\bu^\top \bM_t\bu-\bu'^\top\bM_t\bu'|\leq |\bu^\top \bM_t(\bu-\bu')| + |\bu'^\top\bM_t(\bu-\bu')|\leq \frac12 \|\bM_t\|_2.
$$
This implies that 
$$
\|\bM_t\|_2 = \sup_{\bu\in\mathbb S^{d-1}}|\bu^\top\bM_t\bu|\leq \sup_{\bu'\in\mathcal \cN}|\bu'^\top\bM_t\bu'|+\frac12 \|\bM_t\|_2,
$$
and thus $\sup_{\bu\in\cN}|\bu^\top\bM_t\bu|\geq\frac12\|\bM_t\|_2$.
Combining the above and (\ref{eq:v-concentration-singledirection}), we obtain that for any $\gamma_1>0$,
\begin{align*}
\PP\left(\|\bM_t\|_2\geq \frac{2\gamma_1}{t}\right)\leq \PP\left(\sup_{\bu\in\cN}|\bu^\top\bM_t\bu|\geq \frac{\gamma_1}{t}\right)\leq 9^d\cdot \PP\left(|\bu^\top\bM_t\bu|\geq \frac{\gamma_1}{t}\right)=2\cdot 9^d\cdot e^{-\frac{d^2q_t\gamma_1^2}{2(2d^2\gamma_1+\xi t)}}.
\end{align*}
By choosing $\gamma_1 = 24\max\{\frac{d+\log t}{q_t}, \frac{\sqrt{\xi}}{d}\cdot \sqrt{\frac{t(d+\log t)}{q_t}}\}$, and noticing that \\$\bM_t = \hat\bSigma_{\tilde\xb, a}^{(t)}-\frac{1}{t}\sum_{\tau\in[t]}\EE[\bV_{\tau, a}|\cH_{\tau-1}]$, we have
\begin{equation}\label{eq:hat-Sigma-x-1}
\PP\left(\bigg\|\hat\bSigma_{\tilde\xb, a}^{(t)}-\frac{1}{t}\sum_{\tau\in[t]}\EE[\bV_{\tau, a}|\cH_{\tau-1}]\bigg\|_2\geq 48\max\left\{\frac{d+\log t}{q_t t}, \frac{\sqrt{\xi}}{d}\cdot \sqrt{\frac{d+\log t}{q_t t}}\right\}\right)\leq \frac{2}{t^2}.
\end{equation}
At the same time, we have 
\begin{align}
\EE[\bV_{\tau, a}|\cH_{\tau-1}]
&= \EE_{\bepsilon_\tau}\left[\EE_{A_\tau\sim\pi_\tau(\cdot|\tilde\xb_{\tau}, \cH_{\tau-1})}\bigg[\frac{\pi_{\tau}^{nd}(A_\tau)}{\pi_\tau(A_\tau|\tilde\xb_\tau, \cH_{\tau-1})}1_{\{A_\tau=a\}}\tilde\xb_\tau\tilde\xb_{\tau}^\top\big|\bepsilon_\tau, \cH_{\tau-1}\bigg]\bigg|\cH_{\tau-1}\right]\nonumber\\
& =\EE_{\bepsilon_\tau}\left[\EE_{A_\tau\sim\pi_{\tau}^{nd}(\cdot)}\big[1_{\{A_\tau=a\}}\tilde\xb_\tau\tilde\xb_{\tau}^\top\big|\bepsilon_\tau, \cH_{\tau-1}\big]\bigg|\cH_{\tau-1}\right]\nonumber\\
& =\EE_{\bepsilon_\tau}\left[\pi_{\tau}^{nd}(a)\cdot\tilde\xb_\tau\tilde\xb_{\tau}^\top\bigg|\cH_{\tau-1}\right]\nonumber\\
&=\pi_{\tau}^{nd}(a)(\xb_\tau\xb_{\tau}^\top+\bSigma_{e, \tau}).\label{eq:condition-e-V}
\end{align}
Here we've used the facts that (i) $\{\pi_{\tau}^{nd}\}_{\tau}$ is data-independent; (ii) $\EE[\bepsilon_\tau|\cH_{\tau-1}] = 0$, $\Var[\bepsilon_\tau|\cH_{\tau-1}] = \bSigma_{e, \tau}$.
Plug (\ref{eq:condition-e-V}) into (\ref{eq:hat-Sigma-x-1}), and we get
\begin{equation}\label{eq:hat-Sigma-x-2}
\PP\left(\bigg\|\hat\bSigma_{\tilde\xb, a}^{(t)}-\frac{1}{t}\sum_{\tau\in[t]}\pi_{\tau}^{nd}(a)(\xb_\tau\xb_{\tau}^\top+\bSigma_{e, \tau})\bigg\|_2\geq 48\max\left\{\frac{d+\log t}{q_t t}, \frac{\sqrt{\xi}}{d}\cdot \sqrt{\frac{d+\log t}{q_t t}}\right\}\right)\leq \frac{2}{t^2}.
\end{equation}


The analysis for $\hat\bSigma_{\tilde\xb, r, a}^{(t)}$ is similar. Write $\hat\bSigma_{\tilde\xb, r, a}^{(t)}=\frac{1}{t}\sum_{\tau\in[t]}\bZ_{\tau, a}$, $\bZ_{\tau, a}:=\frac{\pi_{\tau}^{nd}(A_\tau)}{\pi_\tau(A_\tau|\tilde\xb_\tau, \cH_{\tau-1})}1_{\{A_\tau = a\}}\tilde\xb_\tau r_\tau$. Then for any $\bu\in \mathcal S^{d-1}$, it's easy to verify that $|(\bZ_{\tau, a}-\EE[\bZ_{\tau, a}|\cH_{\tau-1}])^\top\bu|\leq \frac{2R}{q_t}$, and $\Var((\bZ_{\tau, a}-\EE[\bZ_{\tau, a}|\cH_{\tau-1}])^\top\bu|\cH_{\tau - 1})\leq \EE[(\bZ_{\tau, a}^\top\bu)^2|\cH_{\tau-1}]\leq\frac{\nu R^2}{q_td}$. Applying Freedman's Inequality leads to 
\begin{equation}\label{eq:z-concentration-singledirection}
\PP\left(|\bz_t^\top \bu|\geq \frac{\gamma_1}{t}\right)\leq 2e^{-\frac{dq_t\gamma_1^2}{4Rd\gamma_1+2R^2\nu t}},
\end{equation}
where $\bz_t:=\frac{1}{t}\sum_{\tau\in[t]}(\bZ_{\tau, a}-\EE[\bZ_{\tau, a}|\cH_{\tau-1}])$. 

Recall that $\cN$ is a $\frac{1}{4}$-net of $\mathbb S^{d-1}$, $|\cN|\leq 9^d$. $\forall \bu\in\mathbb S^{d-1}$, find $\bu'\in\mathbb S^{d-1}$ s.t. $\|\bu-\bu'\|\leq 1/4$, then $|\bz_t^\top\bu-\bz_t^\top \bu'|\leq \frac{1}{4}\|\bz_t\|_2$, and thus 
$$
\|\bz_t\|_2=\sup_{\bu\in\mathbb S^{d-1}}|\bz_t^\top\bu|\leq \sup_{\bu'\in\cN}|\bz_t^\top\bu'|+\frac14 \|\bz_t\|_2
$$
which implies that $\sup_{\bu\in\cN}|\bz_t^\top \bu|\geq \frac34 \|\bz_t\|_2$. Taking this and (\ref{eq:z-concentration-singledirection}) into account, we derive that
$$
\PP\left(\|\bz_t\|_2\geq \frac{4}{3}\frac{\gamma_1}{t}\right)\leq \PP\left(\sup_{\bu\in\cN}|\bz_t^\top\bu|\geq \frac{\gamma_1}{t}\right)\leq 9^d\cdot 2e^{-\frac{dq_t\gamma_1^2}{4Rd\gamma_1+2R^2\nu t}}
$$
By choosing $\gamma_1=24R\max\left\{\frac{d+\log t}{q_t}, \sqrt{\nu}\cdot \sqrt{\frac{(d+\log t)t}{dq_t}}\right\}$ and noticing that $\bz_t = \hat\bSigma_{\tilde\xb, r, a}^{(t)}-\frac{1}{t}\sum_{\tau\in[t]}\EE[\bZ_{\tau, a}|\cH_{\tau-1}]$, we obtain 
\begin{equation}\label{eq:hat-Sigma-x-r-1}
\PP\left(\bigg\|\hat\bSigma_{\tilde\xb, r, a}^{(t)}-\frac{1}{t}\sum_{\tau\in[t]}\EE[\bZ_{\tau, a}|\cH_{\tau-1}]\bigg\|_2\geq 32R\max\left\{\frac{d+\log t}{q_tt}, \sqrt{\frac{\nu}{d}}\cdot \sqrt{\frac{d+\log t}{q_tt}}\right\}\right)\leq \frac{2}{t^2}.
\end{equation}
Finally, because 
\begin{align*}
\EE[\bZ_{\tau, a}|\cH_{\tau-1}]
&=\EE_{\bepsilon_\tau}\left[\EE_{A_\tau\sim\pi_\tau(\cdot|\tilde\xb_\tau, \cH_{\tau-1}), \eta_\tau}\left[\frac{\pi_{\tau}^{nd}(A_\tau)}{\pi_{\tau}(A_\tau|\tilde\xb_\tau, \cH_{\tau-1})}1_{\{A_\tau=a\}}\tilde\xb_\tau r_\tau\big|\cH_{\tau-1}, \bepsilon_\tau\right]\bigg|\cH_{\tau-1}\right]\\
&=\EE_{\bepsilon_\tau}\left[\EE_{A_\tau\sim\pi_{\tau}^{nd}(\cdot), \eta_\tau}\left[1_{\{A_\tau=a\}}\tilde\xb_\tau r_\tau|\cH_{\tau-1}, \bepsilon_\tau\right]\bigg|\cH_{\tau-1}\right]\\
&=\EE_{\bepsilon_\tau}\left[\EE_{A_\tau\sim\pi_{\tau}^{nd}(\cdot), \eta_\tau}\left[1_{\{A_\tau=a\}}\tilde\xb_\tau (\xb_{\tau}^\top\btheta^*_{a}+\eta_\tau)|\cH_{\tau-1}, \bepsilon_\tau\right]\bigg|\cH_{\tau-1}\right]\\
&=\EE_{\bepsilon_\tau}\left[\pi_{\tau}^{nd}(a)\EE_{\eta_\tau}\left[\tilde\xb_\tau (\xb_{\tau}^\top\btheta^*_{a}+\eta_\tau)|\cH_{\tau-1}, \bepsilon_\tau\right]\bigg|\cH_{\tau-1}\right]\\
&=\pi_{\tau}^{nd}(a)\EE_{\bepsilon_\tau}\left[\tilde\xb_\tau \xb_{\tau}^\top\btheta^*_{a}\bigg|\cH_{\tau-1}\right]=\pi_{\tau}^{nd}(a)(\xb_\tau \xb_{\tau}^\top)\btheta^*_{a},
\end{align*}
Plug in (\ref{eq:hat-Sigma-x-r-1}), and we obtain 
\begin{equation}\label{eq:hat-Sigma-x-r-2}
\PP\left(\bigg\|\hat\bSigma_{\tilde\xb, r, a}^{(t)}-\bigg[\frac{1}{t}\sum_{\tau\in[t]}\pi_{\tau}^{nd}(a)\xb_\tau \xb_{\tau}^\top\bigg]\btheta^*_{a}\bigg\|_2\geq 32R\max\left\{\frac{d+\log t}{q_tt}, \sqrt{\frac{\nu}{d}}\cdot \sqrt{\frac{d+\log t}{q_tt}}\right\}\right)\leq \frac{2}{t^2}.
\end{equation}
Combining (\ref{eq:hat-Sigma-x-r-2}) and (\ref{eq:hat-Sigma-x-2}), we conclude the proof.

\subsection{Proof of Lemma \ref{lem::at-star=at-dagger}}\label{pf::lem::at-star=at-dagger}

We only need to prove
\begin{equation}\label{eq::at-star=at-dagger}
\sign(\langle \btheta_1^* - \btheta_0^*, \xb_t\rangle) = \sign(\langle \btheta_1^* - \btheta_0^*, \tilde\xb_t\rangle).
\end{equation}
If $\langle\btheta_1^* - \btheta_0^*, \xb_t \rangle = 0$, (\ref{eq::at-star=at-dagger}) is a direct consequence of Assumption \ref{ass:small-error}. If $\langle\btheta_1^* - \btheta_0^*, \xb_t \rangle \neq 0$, without loss of generality, suppose $\langle\btheta_1^* - \btheta_0^*, \xb_t \rangle > 0$. %Recall that $\bm{\delta}_\theta = \btheta_1^* - \btheta_0^*$. 
Then according to Assumption \ref{ass:small-error},
$$
\langle\btheta_1^* - \btheta_0^*, \tilde\xb_t \rangle = \langle\btheta_1^* - \btheta_0^*, \xb_t \rangle + \langle\btheta_1^* - \btheta_0^*, \bepsilon_t \rangle\geq(1-\rho)\langle\btheta_1^* - \btheta_0^*, \xb_t \rangle>0.
$$
Thus (\ref{eq::at-star=at-dagger}) is true.

\subsection{Proof of Lemma \ref{lem::standard-setting-regret-hat}}\label{pf::lem::standard-setting-regret-hat}

{
%\color{blue} (Proof of Lemma \ref{lem::standard-setting-regret-hat})
We have 
\begin{align}
\widehat{\text{Regret}}_t(\pi, \pi^*) &= \EE_{a\sim \pi_t^*}\langle \btheta^*_{a}, \tilde\xb_t\rangle - \EE_{a\sim \pi_t}\langle \btheta^*_{a}, \tilde\xb_t\rangle\nonumber\\
&=\langle\btheta^*_{a_t^*}, \tilde\xb_t\rangle-[(1-p_0^{(t)})\langle\btheta^*_{\tilde a_t}, \tilde\xb_t\rangle+p_0^{(t)}\langle\btheta^*_{1-\tilde a_t}, \tilde\xb_t\rangle]\nonumber\\
&= 1_{\{a_t^*= \tilde a_t\}}p_0^{(t)}\langle \btheta^*_{a_t^*} - \btheta^*_{1-a_t^*}, \tilde\xb_t\rangle + 1_{\{a_t^*\neq \tilde a_t\}}(1-p_0^{(t)})\langle \btheta^*_{a_t^*} - \btheta^*_{1-a_t^*}, \tilde\xb_t\rangle\nonumber\\
&\leq 2p_0^{(t)}R_\theta + 1_{\{a_t^*\neq \tilde a_t\}}\langle \btheta^*_{a_t^*} - \btheta^*_{1-a_t^*}, \tilde\xb_t\rangle.\label{eq:instant-regret-standard-benchmark}
\end{align}

Here recall that $a_t^*=\argmax_a\langle \btheta^*_{a}, \xb_t\rangle = \argmax_a\langle \btheta^*_{a}, \tilde\xb_t\rangle$, $\tilde a_t := \argmax_{a\in\{0, 1\}}\langle\hat\btheta_{a}^{(t-1)}, \tilde\xb_t\rangle$.

Note that $a_t^*\neq \tilde a_t$ implies that 
\begin{align*}
\begin{cases}
\langle\btheta_{a_t^*}^*, \tilde\xb_t\rangle\geq \langle\btheta_{1-a_t^*}^*, \tilde\xb_t\rangle\\
\langle\hat\btheta_{a_t^*}^{(t-1)}, \tilde\xb_t\rangle\leq \langle\hat\btheta_{1-a_t^*}^{(t-1)}, \tilde\xb_t\rangle
\end{cases}
\end{align*}
which leads to 
\begin{align*}
\langle \btheta_{a_t^*}^*, \tilde\xb_t\rangle&\geq \langle\btheta_{1-a_t^*}^*, \tilde\xb_t\rangle\geq \langle\hat\btheta_{1-a_t^*}^{(t-1)}, \tilde\xb_t\rangle - \max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2\\
&\geq \langle\hat\btheta_{a_t^*}^{(t-1)}, \tilde\xb_t\rangle - \max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2\geq \langle\btheta_{a_t^*}^*, \tilde\xb_t\rangle - 2\max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2,
\end{align*}
and further implies $\left|\langle \btheta^*_{a_t^*} - \btheta^*_{1-a_t^*}, \tilde\xb_t\rangle\right|\leq 2\max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2$.

Plugging in the above to (\ref{eq:instant-regret-standard-benchmark}) leads to 
$$
\widehat{\text{Regret}}_t(\pi, \pi^*)\leq 2p_0^{(t)}R_\theta + 2\max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2.
$$
}

\subsection{Proof of Lemma \ref{lem::clipped-setting-regret-hat}}\label{pf::lem::clipped-setting-regret-hat}

{
At any time $t>T_0$, we have 
\begin{align}
\widehat{\text{Regret}}_t(\pi, \bar\pi^*) &= \EE_{a\sim \bar\pi_t^*}\langle \btheta^*_{a}, \tilde\xb_t\rangle - \EE_{a\sim \pi_t}\langle \btheta^*_{a}, \tilde\xb_t\rangle\nonumber\\
&=[(1-p_0)\langle\btheta^*_{a_t^*}, \tilde\xb_t\rangle+p_0\langle\btheta^*_{1-a_t^*}, \tilde\xb_t\rangle]-[(1-p_0)\langle\btheta^*_{\tilde a_t}, \tilde\xb_t\rangle+p_0\langle\btheta^*_{1-\tilde a_t}, \tilde\xb_t\rangle]\nonumber\\
&\leq (1-2p_0)1_{\{a_t^*\neq \tilde a_t\}}\left|\langle \btheta^*_{a_t^*} - \btheta^*_{1-a_t^*}, \tilde\xb_t\rangle\right|.\label{eq:instant-regret}
\end{align}
Here recall that $a_t^*=\argmax_a\langle \btheta^*_{a}, \xb_t\rangle = \argmax_a\langle \btheta^*_{a}, \tilde\xb_t\rangle$, $\tilde a_t := \argmax_{a\in\{0, 1\}}\langle\hat\btheta_{a}^{(t-1)}, \tilde\xb_t\rangle$.

Note that $a_t^*\neq \tilde a_t$ implies that 
\begin{align*}
\begin{cases}
\langle\btheta_{a_t^*}^*, \tilde\xb_t\rangle\geq \langle\btheta_{1-a_t^*}^*, \tilde\xb_t\rangle\\
\langle\hat\btheta_{a_t^*}^{(t-1)}, \tilde\xb_t\rangle\leq \langle\hat\btheta_{1-a_t^*}^{(t-1)}, \tilde\xb_t\rangle
\end{cases}
\end{align*}
which leads to 
\begin{align*}
\langle \btheta_{a_t^*}^*, \tilde\xb_t\rangle&\geq \langle\btheta_{1-a_t^*}^*, \tilde\xb_t\rangle\geq \langle\hat\btheta_{1-a_t^*}^{(t-1)}, \tilde\xb_t\rangle - \max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2\\
&\geq \langle\hat\btheta_{a_t^*}^{(t-1)}, \tilde\xb_t\rangle - \max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2\geq \langle\btheta_{a_t^*}^*, \tilde\xb_t\rangle - 2\max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2,
\end{align*}
and further implies $\left|\langle \btheta^*_{a_t^*} - \btheta^*_{1-a_t^*}, \tilde\xb_t\rangle\right|\leq 2\max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2$.

Plugging in the above to (\ref{eq:instant-regret}) leads to 
$$
\widehat{\text{Regret}}_t(\pi, \bar\pi^*)\leq 2(1-2p_0)\max_a\|\hat\btheta_a^{(t-1)}-\btheta_a^*\|_2
$$
}
\subsection{Proof of Theorem \ref{thm:theta-estimation-estimated-Sigma}}\label{pf::theta-estimation-estimated-Sigma}

Fix $t\in[T]$ such that the conditions of Theorem 2.3 hold. Fix $a\in\{0, 1\}$. As in Appendix \ref{pf:thm:theta-estimation}, define $\bDelta_1 :=\hat\bSigma_{\tilde\xb, a}^{(t)}-\frac{1}{t}\sum_{\tau\in[t]}\pi_{\tau}^{nd}(a)(\xb_\tau\xb_{\tau}^\top+\bSigma_{e, \tau})$, $\bDelta_2 := \hat\bSigma_{\tilde\xb, r, a}^{(t)}-\bigg(\frac{1}{t}\sum_{\tau\in[t]}\pi_{\tau}^{nd}(a)\xb_\tau \xb_{\tau}^\top\bigg)\btheta^*_{a}$. We also let $\bDelta_3 := -\bDelta_t(a) = -\frac{1}{t}\sum_{\tau\in[t]}\pi_{\tau}^{nd}(a)(\hat\bSigma_{e, \tau} - \bSigma_{e, \tau})$. Recall Lemma \ref{lem:hat-Sigma-x-r}: with probability at least $1-4/t^2$, 
$$
\|\bDelta_1\|_2\leq C\max\left\{\frac{d+\log t}{q_tt}, \frac{\sqrt{\xi}}{d}\sqrt{\frac{d+\log t}{q_tt}}\right\},\quad \|\bDelta_2\|_2\leq CR\max\left\{\frac{d+\log t}{q_tt}, \sqrt{\frac{\nu}{d}}\sqrt{\frac{d+\log t}{q_tt}}\right\}.
$$
Meanwhile, 
\begin{align*}
\tilde\btheta_a^{(t)}&= \bigg(\hat\bSigma_{\tilde\xb, a}^{(t)} - 
\frac{1}{t}\sum_{\tau\in[t]}\pi^{nd}_\tau(a)\hat\bSigma_{e, \tau}\bigg)^{-1}\cdot
\hat\bSigma_{\tilde\xb, r, a}^{(t)}\\
& = \bigg(\frac{1}{t}\sum_{\tau\in[t]}\pi^{nd}_\tau(a)\xb_\tau\xb_{\tau}^\top+\bDelta_1+\bDelta_3\bigg)^{-1}\cdot \left[\bigg(\frac{1}{t}\sum_{\tau\in[t]}\pi_{\tau}^{nd}(a)\xb_\tau \xb_{\tau}^\top\bigg)\btheta^*_{a}+\bDelta_2\right]\\
&= \btheta_a^* - \bJ_1' + \bJ_2',
\end{align*}
where 
\begin{align*}
\bJ_1':=\bigg[\frac{1}{t}\sum_{\tau\in[t]}\pi_{\tau}^{nd}(a)\xb_\tau\xb_{\tau}^\top+\bDelta_1+\bDelta_3\bigg]^{-1}(\bDelta_1+\bDelta_3)\btheta_a^*,\thickspace
\bJ_2':=\bigg[\frac{1}{t}\sum_{\tau\in[t]}\pi_{\tau}^{nd}(a)\xb_\tau\xb_{\tau}^\top+\bDelta_1+\bDelta_3\bigg]^{-1}\bDelta_2.
\end{align*}
Under the events where both (\ref{eq:hat-Sigma-x}) and (\ref{eq:hat-Sigma-x-r}) hold, whenever 
\begin{equation}\label{eq:condition-eigv-control-estimated-Sigma}
C\max\left\{\frac{d+\log t}{q_tt}, \frac{\sqrt{\xi}}{d}\sqrt{\frac{d+\log t}{q_tt}}\right\}\leq \frac{\lambda_{0}}{4d}
\end{equation}
and $\|\bDelta_3\|_2\leq \frac{\lambda_{0}}{4d}$, we have 
$$
\|\bJ_1'\|_2\leq \frac{2dR_{\btheta}}{\lambda_{0}}\left(C\max\left\{\frac{d+\log t}{q_tt}, \frac{\sqrt{\xi}}{d}\sqrt{\frac{d+\log t}{q_tt}}\right\} + \|\bDelta_3\|_2\right)
$$
and 
$$
\|\bJ_2'\|_2\leq \frac{2CdR}{\lambda_{0}}\max\left\{\frac{d+\log t}{q_tt}, \sqrt{\frac{\nu}{d}}\sqrt{\frac{d+\log t}{q_tt}}\right\}.
$$
(\ref{eq:condition-eigv-control-estimated-Sigma}) can be ensured by $t\geq {C_1'}\max\left\{\frac{d(d+\log t)}{\lambda_0q_t}, \frac{\xi(d+\log t)}{\lambda_0^2q_t}\right\}$, where $C_1' = \max\{4C, 16C^2\}$. Given these guarantees, we have with probability at least $1-4/t^2$, 
$$
\|\tilde\btheta_a^{(t)}-\btheta_a^*\|\leq \|\bJ_1'\|_2+\|\bJ_2'\|_2\leq \frac{2C(R+R_{\btheta})d}{\lambda_{0}}\max\left\{\frac{d+\log t}{q_tt}, \frac{\sqrt{\xi} + \sqrt{\nu}}{\sqrt{d}}\sqrt{\frac{d+\log t}{q_tt}}\right\} + \frac{2R_\theta d}{\lambda_0}\|\bDelta_t(a)\|_2.
$$
Thus we conclude the proof.

\subsection{{Proof of Corollary \ref{cor:alg1-with-theta-estimation-estimated-Sigma}}}\label{pf::cor:alg1-with-theta-estimation-estimated-Sigma}

\noindent\textbf{Standard setting.} Notice that $p_0^{(t)}$ is monotonically decreasing in $t$. Theorem \ref{thm:theta-estimation-estimated-Sigma} indicates that, as long as $\forall t> T_0$,
\begin{equation}\label{eq:standard-benchmark-condition1-estimated-sigma}
p_0^{(t)}\geq {C'}\max\left\{\frac{d(d+\log t)}{\lambda_0 t}, \frac{\xi(d+\log t)}{\lambda_0^2t}\right\},
\end{equation}
then with probability at least $1-\frac{8}{t^2}$, $\forall a\in\{0, 1\}$,
\begin{align}
\|\hat\btheta_a^{(t)}-\btheta_a^*\|_2
&\leq \frac{C(R+\!R_\theta)d}{\lambda_0}\max\left\{\!\frac{d+\log t}{t^{\frac23}}, \frac{\sqrt{\nu} + \sqrt{\xi}}{\sqrt{d}}\sqrt{\frac{d+\log t}{t^{\frac23}}}\right\} + \frac{2R_\theta d}{\lambda_0}\|\bDelta_t(a)\|_2\label{eq:standard-benchmark-estimation-error-estimated-sigma}
\end{align}

Plug it into Theorem \ref{thm-regret}, we have that with high probability, 
\begin{align*}
\text{Regret}(T;\pi^*)
&\leq 2R_\theta\cdot \lceil 2dT^{2/3}\rceil + \frac2{1-\rho} I_1',
\end{align*}
where
\begin{align*}
I_1& = \sum_{t=T_0+1}^T
\bigg(p_0^{(t)}R_\theta + \max_a\|\tilde \btheta_a^{(t-1)} - \btheta^*_a\|_2\bigg)\\
&\leq \sum_{t=T_0+1}^T t^{-\frac13}R_\theta + \sum_{t=T_0}^{T-1}\left[\frac{C(R+\!R_\theta)d}{\lambda_0}\max\left\{\!\frac{d+\log t}{t^{\frac23}}, \frac{\sqrt{\nu} + \sqrt{\xi}}{\sqrt{d}}\sqrt{\frac{d+\log t}{t^{\frac23}}}\right\}+\max_a\frac{2 R_\theta d}{\lambda_0}\|\bDelta_t(a)\|_2\right]\\
&\leq 2R_\theta T^{\frac23} +\frac{C'}{\lambda_0}(\sqrt{\nu} + \sqrt{\xi} + 1)(R+\!R_\theta)\sqrt{d(d+\log T)} T^{\frac23} + \frac{2 R_\theta d}{\lambda_0}\sum_{t\geq T_0}\|\bDelta_t(a)\|_2,
\end{align*}
for a universal constant $C'$, where the last inequality holds if in addition, $T\geq[d(d+\log T)]^{\frac32}$.

The proof is concluded by combining the above requirement for $T$ as well as \eqref{eq:standard-benchmark-condition1-estimated-sigma}.

\noindent\textbf{Clipped policy setting.} 

Similar to the standard setting, according to Theorem \ref{thm:theta-estimation-estimated-Sigma}, as long as $\forall t> T_0$,
\begin{equation}\label{eq:clipped-benchmark-condition1-estimated-sigma}
p_0\geq {C_1}\max\left\{\frac{d(d+\log t)}{\lambda_0 t}, \frac{\xi(d+\log t)}{\lambda_0^2t}\right\},
\end{equation}
then with probability at least $1-\frac{8}{t^2}$, $\forall a\in\{0, 1\}$,
\begin{align}
\|\tilde\btheta_a^{(t)}-\btheta_a^*\|_2
&\leq \frac{C(R+\!R_\theta)d}{\lambda_0}\max\left\{\!\frac{d+\log t}{p_0 t}, \frac{\sqrt{\nu} + \sqrt{\xi}}{\sqrt{d}}\sqrt{\frac{d+\log t}{p_0 t}}\right\} + \frac{2R_\theta d}{\lambda_0}\|\bDelta_t(a)\|_2\label{eq:clipped-benchmark-estimation-error-estimated-sigma}
\end{align}

Plug it into Theorem \ref{thm-regret}, we have that with high probability, 
\begin{align*}
\text{Regret}(T;\bar \pi^*)
&\leq 2R_\theta\cdot \lceil 2dT^{1/2}\rceil + \frac{2(1-2p_0)}{1-\rho} I_2',
\end{align*}
where
\begin{align*}
I_2'& = \sum_{t=T_0+1}^T \max_a\|\tilde \btheta_a^{(t-1)}-\btheta^*_a\|_2\\
&\leq \sum_{t=T_0}^{T-1}\left[\frac{C(R+\!R_\theta)d}{\lambda_0}\max\left\{\!\frac{d+\log t}{p_0 t}, \frac{\sqrt{\nu} + \sqrt{\xi}}{\sqrt{d}}\sqrt{\frac{d+\log t}{p_0 t}}\right\} + \max_a\frac{2R_\theta d}{\lambda_0}\|\bDelta_t(a)\|_2\right]\\
&\leq \frac{2C(R+R_\theta)(\sqrt{\nu}+\sqrt{\xi}+1)}{\lambda_0\sqrt{p_0}}\sqrt{d(d+\log T)}\sqrt{T} + \frac{2R_\theta d}{\lambda_0}\sum_{t=T_0}^{T-1}\max_a\|\bDelta_t(a)\|_2,
\end{align*}
where the last inequality holds if in addition, $T\geq d(d+\log T)\log^2 T$. 

The proof is concluded by plugging the above into the regret upper bound formula and combining the requirements for $T$.

\vfill

% \bibliographystyle{plainnat}
%\bibliography{AISTATS24/rl_aistats}

% \end{document}
