{
  "title": "Online learning in bandits with predicted context",
  "authors": [
    "Yongyi Guo",
    "Ziping Xu",
    "Susan Murphy"
  ],
  "submission_date": "2023-07-26T02:33:54+00:00",
  "revised_dates": [
    "2023-11-02T00:04:01+00:00",
    "2024-03-19T01:26:59+00:00"
  ],
  "abstract": "We consider the contextual bandit problem where at each time, the agent only has access to a noisy version of the context and the error variance (or an estimator of this variance). This setting is motivated by a wide range of applications where the true context for decision-making is unobserved, and only a prediction of the context by a potentially complex machine learning algorithm is available. When the context error is non-vanishing, classical bandit algorithms fail to achieve sublinear regret. We propose the first online algorithm in this setting with sublinear regret guarantees under mild conditions. The key idea is to extend the measurement error model in classical statistics to the online decision-making setting, which is nontrivial due to the policy being dependent on the noisy context observations. We further demonstrate the benefits of the proposed approach in simulation environments based on synthetic and real digital intervention datasets.",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "primary_category": "stat.ML",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.13916",
  "pdf_url": null,
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 17071629,
  "size_after_bytes": 4681028
}