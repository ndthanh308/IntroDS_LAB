@misc{batchelor_2022, 
    title={Training CIFAR10 with sharpened cosine similarity}, url={https://github.com/oliver-batchelor/scs_cifar}, 
    journal={GitHub}, 
    publisher={GitHub}, 
    author={Batchelor, Oliver}, 
    year={2022}, 
    month={Feb}}

@misc{brohrer_2022, 
    title={Gallery for Sharpened Cosine Similarity}, url={https://github.com/brohrer/scs-gallery}, 
    journal={GitHub}, 
    publisher={GitHub}, 
    author={Rohrer, Brandon}, 
    year={2022}, 
    month={May}}

@misc{Idelbayev18a, 
    author = "Yerlan Idelbayev", 
    title = "Proper {ResNet} Implementation for {CIFAR10/CIFAR100} in {PyTorch}", 
    publisher = {GitHub},
    year = {2018},
    url={https://github.com/akamaster/pytorch\_resnet\_cifar10}}

@inproceedings{luo2018cosine, 
    title={Cosine normalization: Using cosine similarity instead of dot product in neural networks}, 
    author={Luo, Chunjie and Zhan, Jianfeng and Xue, Xiaohe and Wang, Lei and Ren, Rui and Yang, Qiang}, 
    booktitle={International Conference on Artificial Neural Networks}, pages={382--391}, 
    year={2018}, 
    organization={Springer}}
    
@misc{nestler_2022, 
    title={To reproduce SCS' incredible results on large-scale datasets, I implemented a TPU-compatible version in Pytorch.}, url={https://twitter.com/_clashluke/status/1497092150906941442}, journal={Twitter}, 
    publisher={Twitter}, 
    author={Nestler, Lucas}, 
    year={2022}, 
    month={Feb}} 

@inproceedings{optuna_2019, 
    title={Optuna: A Next-generation Hyperparameter Optimization Framework}, author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori}, 
    booktitle={Proceedings of the 25rd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining}, 
    year={2019} }

@misc{rohrer_2020, 
    title={The thing that has surprised me the most about convolution is that it's used in neural networks as a feature detector, but it's pretty bad at detecting features.}, url={https://twitter.com/_brohrer_/status/1232063619657093120?ref_src=twsrc\%5Etfw\%7Ctwcamp\%5Etweetembed\%7Ctwterm\%5E1232063619657093120\%7Ctwgr\%5E6c3a18acd4c451876f1acf93a5327d080a9cabc0\%7Ctwcon\%5Es1_&amp;ref_url=https\%3A\%2F\%2Frpisoni.dev\%2Fposts\%2Fcossim-convolution\%2F}, journal={Twitter}, 
    publisher={Twitter}, 
    author={Rohrer, Brandon}, 
    year={2020}, 
    month={Feb}}

@misc{rohrer_2022, 
    title={Want to see something cool? Sharpened Cosine Similarity (CosSim) is an alternative to convolution for building features in neural networks.it doesn't need normalization, dropout, or activation functions and it performs as well as convnets with 10x-100x more parameters}, url={https://twitter.com/_brohrer_/status/1487928061240946688?lang=en}, journal={Twitter}, 
    publisher={Twitter}, 
    author={Rohrer, Brandon}, 
    year={2022}, 
    month={Jan}}

@misc{rohrer_2022github, 
    title={Sharpened Cosine Similarity: An alternative to convolution in Neural Networks}, url={https://github.com/brohrer/sharpened-cosine-similarity}, journal={Github}, publisher={GitHub}, author={Rohrer, Brandon}, year={2022}, month={May}}

@article{singhal2001modern, 
    title={Modern information retrieval: A brief overview}, 
    author={Singhal, Amit and others}, 
    journal={IEEE Data Eng. Bull.}, 
    volume={24}, 
    number={4}, 
    pages={35--43}, 
    year={2001}}

@misc{smith_2017, 
    doi = {10.48550/ARXIV.1708.07120}, 
    url = {https://arxiv.org/abs/1708.07120}, 
    author = {Smith, Leslie N. and Topin, Nicholay}, 
    keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences}, 
    title = {Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates}, 
    publisher = {arXiv}, 
    year = {2017}, 
    copyright = {arXiv.org perpetual, non-exclusive license} }

@article{tan2013data, 
    title={Data mining cluster analysis: basic concepts and algorithms}, author={Tan, Pang-Ning and Steinbach, Michael and Kumar, Vipin}, journal={Introduction to data mining}, 
    volume={487}, 
    pages={533}, 
    year={2013}, 
    publisher={Pearson Education India}}

@misc{pisoni2022sharpenedcossim, 
    title={Sharpened Cosine Distance as an Alternative for Convolutions}, author={Raphael Pisoni}, 
    year={2022}, 
    note={\url{https://rpisoni.dev/posts/cossim-convolution/}}, }
    
@misc{wagner_2022, 
    title={Kaggle Notebooks}, url={https://github.com/DrJohnWagner/Kaggle-Notebooks}, journal={GitHub}, 
    publisher={GitHub}, 
    author={Wagner, John}, 
    year={2022}, 
    month={Feb}} 

@misc{walton_2022, 
    title={Sharpened Cosine Similarity For Compact Transformers}, url={https://github.com/stevenwalton/SCS-CCT}, 
    journal={GitHub}, 
    publisher={GitHub}, 
    author={Walton, Steven}, 
    year={2022}, 
    month={Apr}}
 
@misc{zimonitrome_2022, 
    title={Generative adversarial network using sharpened cosine similarity}, 
    url={https://github.com/zimonitrome/scs_gan}, 
    journal={GitHub}, 
    publisher={GitHub}, 
    author={Zimonitrome}, 
    year={2022}, 
    month={Feb}}
    
