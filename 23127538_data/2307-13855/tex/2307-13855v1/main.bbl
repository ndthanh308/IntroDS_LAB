% Generated by IEEEtranN.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{18}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtranN.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem[Rohrer(2022{\natexlab{a}})]{rohrer_2022github}
\BIBentryALTinterwordspacing
B.~Rohrer, ``Sharpened cosine similarity: An alternative to convolution in
  neural networks,'' May 2022. [Online]. Available:
  \url{https://github.com/brohrer/sharpened-cosine-similarity}
\BIBentrySTDinterwordspacing

\bibitem[Rohrer(2020)]{rohrer_2020}
\BIBentryALTinterwordspacing
------, ``The thing that has surprised me the most about convolution is that
  it's used in neural networks as a feature detector, but it's pretty bad at
  detecting features.'' Feb 2020. [Online]. Available:
  \url{https://twitter.com/_brohrer_/status/1232063619657093120?ref_src=twsrc\%5Etfw\%7Ctwcamp\%5Etweetembed\%7Ctwterm\%5E1232063619657093120\%7Ctwgr\%5E6c3a18acd4c451876f1acf93a5327d080a9cabc0\%7Ctwcon\%5Es1_&amp;ref_url=https\%3A\%2F\%2Frpisoni.dev\%2Fposts\%2Fcossim-convolution\%2F}
\BIBentrySTDinterwordspacing

\bibitem[Luo et~al.(2018)Luo, Zhan, Xue, Wang, Ren, and Yang]{luo2018cosine}
C.~Luo, J.~Zhan, X.~Xue, L.~Wang, R.~Ren, and Q.~Yang, ``Cosine normalization:
  Using cosine similarity instead of dot product in neural networks,'' in
  \emph{International Conference on Artificial Neural Networks}.\hskip 1em plus
  0.5em minus 0.4em\relax Springer, 2018, pp. 382--391.

\bibitem[Singhal et~al.(2001)]{singhal2001modern}
A.~Singhal \emph{et~al.}, ``Modern information retrieval: A brief overview,''
  \emph{IEEE Data Eng. Bull.}, vol.~24, no.~4, pp. 35--43, 2001.

\bibitem[Tan et~al.(2013)Tan, Steinbach, and Kumar]{tan2013data}
P.-N. Tan, M.~Steinbach, and V.~Kumar, ``Data mining cluster analysis: basic
  concepts and algorithms,'' \emph{Introduction to data mining}, vol. 487, p.
  533, 2013.

\bibitem[Pisoni(2022)]{pisoni2022sharpenedcossim}
R.~Pisoni, ``Sharpened cosine distance as an alternative for convolutions,''
  2022, \url{https://rpisoni.dev/posts/cossim-convolution/}.

\bibitem[Rohrer(2022{\natexlab{b}})]{brohrer_2022}
\BIBentryALTinterwordspacing
B.~Rohrer, ``Gallery for sharpened cosine similarity,'' May 2022. [Online].
  Available: \url{https://github.com/brohrer/scs-gallery}
\BIBentrySTDinterwordspacing

\bibitem[Nestler(2022)]{nestler_2022}
\BIBentryALTinterwordspacing
L.~Nestler, ``To reproduce scs' incredible results on large-scale datasets, i
  implemented a tpu-compatible version in pytorch.'' Feb 2022. [Online].
  Available: \url{https://twitter.com/_clashluke/status/1497092150906941442}
\BIBentrySTDinterwordspacing

\bibitem[Batchelor(2022)]{batchelor_2022}
\BIBentryALTinterwordspacing
O.~Batchelor, ``Training cifar10 with sharpened cosine similarity,'' Feb 2022.
  [Online]. Available: \url{https://github.com/oliver-batchelor/scs_cifar}
\BIBentrySTDinterwordspacing

\bibitem[Walton(2022)]{walton_2022}
\BIBentryALTinterwordspacing
S.~Walton, ``Sharpened cosine similarity for compact transformers,'' Apr 2022.
  [Online]. Available: \url{https://github.com/stevenwalton/SCS-CCT}
\BIBentrySTDinterwordspacing

\bibitem[Wagner(2022)]{wagner_2022}
\BIBentryALTinterwordspacing
J.~Wagner, ``Kaggle notebooks,'' Feb 2022. [Online]. Available:
  \url{https://github.com/DrJohnWagner/Kaggle-Notebooks}
\BIBentrySTDinterwordspacing

\bibitem[Zimonitrome(2022)]{zimonitrome_2022}
\BIBentryALTinterwordspacing
Zimonitrome, ``Generative adversarial network using sharpened cosine
  similarity,'' Feb 2022. [Online]. Available:
  \url{https://github.com/zimonitrome/scs_gan}
\BIBentrySTDinterwordspacing

\bibitem[Idelbayev(2018)]{Idelbayev18a}
\BIBentryALTinterwordspacing
Y.~Idelbayev, ``Proper {ResNet} implementation for {CIFAR10/CIFAR100} in
  {PyTorch},'' 2018. [Online]. Available:
  \url{https://github.com/akamaster/pytorch\_resnet\_cifar10}
\BIBentrySTDinterwordspacing

\bibitem[Smith and Topin(2017)]{smith_2017}
\BIBentryALTinterwordspacing
L.~N. Smith and N.~Topin, ``Super-convergence: Very fast training of neural
  networks using large learning rates,'' 2017. [Online]. Available:
  \url{https://arxiv.org/abs/1708.07120}
\BIBentrySTDinterwordspacing

\bibitem[Raff and Farris(2022)]{Raff2022a}
\BIBentryALTinterwordspacing
E.~Raff and A.~L. Farris, ``A {Siren} {Song} of {Open} {Source}
  {Reproducibility},'' in \emph{{ML} {Evaluation} {Standards} {Workshop} at
  {ICLR} 2022}, 2022. [Online]. Available:
  \url{https://arxiv.org/abs/2204.04372}
\BIBentrySTDinterwordspacing

\bibitem[Raff(2021)]{Raff2020c}
\BIBentryALTinterwordspacing
E.~Raff, ``Research {Reproducibility} as a {Survival} {Analysis},'' in
  \emph{The {Thirty}-{Fifth} {AAAI} {Conference} on {Artificial}
  {Intelligence}}, 2021, arXiv: 2012.09932. [Online]. Available:
  \url{http://arxiv.org/abs/2012.09932}
\BIBentrySTDinterwordspacing

\bibitem[Raff(2019)]{Raff2019_quantify_repro}
\BIBentryALTinterwordspacing
------, ``A {Step} {Toward} {Quantifying} {Independently} {Reproducible}
  {Machine} {Learning} {Research},'' in \emph{{NeurIPS}}, 2019, arXiv:
  1909.06674. [Online]. Available: \url{http://arxiv.org/abs/1909.06674}
\BIBentrySTDinterwordspacing

\bibitem[Akiba et~al.(2019)Akiba, Sano, Yanase, Ohta, and Koyama]{optuna_2019}
T.~Akiba, S.~Sano, T.~Yanase, T.~Ohta, and M.~Koyama, ``Optuna: A
  next-generation hyperparameter optimization framework,'' in \emph{Proceedings
  of the 25rd {ACM} {SIGKDD} International Conference on Knowledge Discovery
  and Data Mining}, 2019.

\end{thebibliography}
