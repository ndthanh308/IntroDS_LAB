{
  "title": "Pay Attention to What You Need",
  "authors": [
    "Yifei Gao",
    "Shaohong Chen",
    "Lei Wang",
    "Ruiting Dai",
    "Ziyun Zhang",
    "Kerui Ren",
    "Jiaji Wu",
    "Jun Cheng"
  ],
  "submission_date": "2023-07-25T09:34:42+00:00",
  "revised_dates": [
    "2023-07-28T00:18:20+00:00",
    "2025-02-25T01:35:12+00:00"
  ],
  "abstract": "Although large language models (LLMs) have achieved significant success in natural language processing, they still struggle with long-context comprehension. Traditional approaches to mitigating this issue typically rely on fine-tuning or retraining, which is both resource-intensive and challenging to deploy in lightweight industrial settings. In this paper, we investigate the potential to accomplish this without any additional resources. Through an in-depth study of the attention mechanism in LLMs, we propose a method called Scaled ReAttention (SRA) to strengthen LLMs' ability to interpret and retrieve information by strategically manipulating their attention scores during inference. Through extensive experiments, we demonstrate that integrating SRA significantly boosts LLMs' performance on a variety of downstream tasks, highlighting its practical potential for enhancing language understanding without incurring the overhead of traditional training.",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "primary_category": "cs.CL",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.13365",
  "pdf_url": null,
  "comment": "LLM for long context comprehension, attention mechanism",
  "num_versions": null,
  "size_before_bytes": 7173689,
  "size_after_bytes": 311499
}