\begin{thebibliography}{82}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebron, and Sanghai]{ainslie2023gqa}
Ainslie, J., Lee-Thorp, J., de~Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S.
\newblock {GQA}: Training generalized multi-query transformer models from multi-head checkpoints.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pp.\  4895--4901. Association for Computational Linguistics, December 2023.

\bibitem[Anthropic(2024)]{claude}
Anthropic.
\newblock {I}ntroducing the next generation of {C}laude.
\newblock \url{https://www.anthropic.com/news/claude-3-family}, 2024.
\newblock [Accessed 28-05-2024].

\bibitem[Author(2021)]{anonymous}
Author, N.~N.
\newblock Suppressed for anonymity, 2021.

\bibitem[Bai et~al.(2023{\natexlab{a}})Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, et~al.]{bai2023qwen}
Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et~al.
\newblock Qwen technical report.
\newblock \emph{arXiv preprint arXiv:2309.16609}, 2023{\natexlab{a}}.

\bibitem[Bai et~al.(2023{\natexlab{b}})Bai, Lv, Zhang, Lyu, Tang, Huang, Du, Liu, Zeng, Hou, Dong, Tang, and Li]{bai2023longbench}
Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and Li, J.
\newblock Longbench: A bilingual, multitask benchmark for long context understanding, 2023{\natexlab{b}}.

\bibitem[Bai et~al.(2024)Bai, Tu, Zhang, Peng, Wang, Lv, Cao, Xu, Hou, Dong, et~al.]{bai2024longbench}
Bai, Y., Tu, S., Zhang, J., Peng, H., Wang, X., Lv, X., Cao, S., Xu, J., Hou, L., Dong, Y., et~al.
\newblock Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.
\newblock \emph{arXiv preprint arXiv:2412.15204}, 2024.

\bibitem[Bondarenko et~al.(2023)Bondarenko, Nagel, and Blankevoort]{bondarenko2023quantizable}
Bondarenko, Y., Nagel, M., and Blankevoort, T.
\newblock Quantizable transformers: Removing outliers by helping attention heads do nothing.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 75067--75096, 2023.

\bibitem[Chen et~al.(2023)Chen, Wong, Chen, and Tian]{chen2023pi}
Chen, S., Wong, S., Chen, L., and Tian, Y.
\newblock Extending context window of large language models via positional interpolation.
\newblock \emph{arXiv preprint arXiv:2306.15595}, 2023.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing]{vicuna}
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J.~E., Stoica, I., and Xing, E.~P.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R\'{e}]{dao2022flashattention}
Dao, T., Fu, D., Ermon, S., Rudra, A., and R\'{e}, C.
\newblock Flashattention: Fast and memory-efficient exact attention with io-awareness.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~35, pp.\  16344--16359, 2022.

\bibitem[Dasigi et~al.(2021)Dasigi, Lo, Beltagy, Cohan, Smith, and Gardner]{dasigi2021dataset}
Dasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N.~A., and Gardner, M.
\newblock A dataset of information-seeking questions and answers anchored in research papers, 2021.

\bibitem[DeepSeek-AI et~al.(2024)DeepSeek-AI, Liu, Feng, Wang, Wang, Liu, Zhao, Dengr, Ruan, Dai, Guo, Yang, Chen, Ji, Li, Lin, Luo, Hao, Chen, Li, Zhang, Xu, Yang, Zhang, Ding, Xin, Gao, Li, Qu, Cai, Liang, Guo, Ni, Li, Chen, Yuan, Qiu, Song, Dong, Gao, Guan, Wang, Zhang, Xu, Xia, Zhao, Zhang, Li, Wang, Zhang, Zhang, Tang, Li, Tian, Huang, Wang, Zhang, Zhu, Chen, Du, Chen, Jin, Ge, Pan, Xu, Chen, Li, Lu, Zhou, Chen, Wu, Ye, Ma, Wang, Zhou, Yu, Zhou, Zheng, Wang, Pei, Yuan, Sun, Xiao, Zeng, An, Liu, Liang, Gao, Zhang, Li, Jin, Wang, Bi, Liu, Wang, Shen, Chen, Chen, Nie, Sun, Wang, Liu, Xie, Yu, Song, Zhou, Yang, Lu, Su, Wu, Li, Wei, Zhu, Xu, Huang, Li, Zhao, Sun, Li, Wang, Zheng, Zhang, Xiong, Zhao, He, Tang, Piao, Dong, Tan, Liu, Wang, Guo, Zhu, Wang, Zou, Zha, Ma, Yan, You, Liu, Ren, Ren, Sha, Fu, Huang, Zhang, Xie, Hao, Shao, Wen, Xu, Zhang, Li, Wang, Gu, Li, and Xie]{deepseekai2024deepseekv2}
DeepSeek-AI, Liu, A., Feng, B., Wang, B., Wang, B., Liu, B., Zhao, C., Dengr, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Xu, H., Yang, H., Zhang, H., Ding, H., Xin, H., Gao, H., Li, H., Qu, H., Cai, J.~L., Liang, J., Guo, J., Ni, J., Li, J., Chen, J., Yuan, J., Qiu, J., Song, J., Dong, K., Gao, K., Guan, K., Wang, L., Zhang, L., Xu, L., Xia, L., Zhao, L., Zhang, L., Li, M., Wang, M., Zhang, M., Zhang, M., Tang, M., Li, M., Tian, N., Huang, P., Wang, P., Zhang, P., Zhu, Q., Chen, Q., Du, Q., Chen, R.~J., Jin, R.~L., Ge, R., Pan, R., Xu, R., Chen, R., Li, S.~S., Lu, S., Zhou, S., Chen, S., Wu, S., Ye, S., Ma, S., Wang, S., Zhou, S., Yu, S., Zhou, S., Zheng, S., Wang, T., Pei, T., Yuan, T., Sun, T., Xiao, W.~L., Zeng, W., An, W., Liu, W., Liang, W., Gao, W., Zhang, W., Li, X.~Q., Jin, X., Wang, X., Bi, X., Liu, X., Wang, X., Shen, X., Chen, X., Chen, X., Nie, X., Sun, X., Wang, X., Liu, X., Xie, X., Yu, X., Song, X., Zhou, X., Yang,
  X., Lu, X., Su, X., Wu, Y., Li, Y.~K., Wei, Y.~X., Zhu, Y.~X., Xu, Y., Huang, Y., Li, Y., Zhao, Y., Sun, Y., Li, Y., Wang, Y., Zheng, Y., Zhang, Y., Xiong, Y., Zhao, Y., He, Y., Tang, Y., Piao, Y., Dong, Y., Tan, Y., Liu, Y., Wang, Y., Guo, Y., Zhu, Y., Wang, Y., Zou, Y., Zha, Y., Ma, Y., Yan, Y., You, Y., Liu, Y., Ren, Z.~Z., Ren, Z., Sha, Z., Fu, Z., Huang, Z., Zhang, Z., Xie, Z., Hao, Z., Shao, Z., Wen, Z., Xu, Z., Zhang, Z., Li, Z., Wang, Z., Gu, Z., Li, Z., and Xie, Z.
\newblock Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and Zettlemoyer]{dettmers2022llmint8}
Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.
\newblock Llm.int8(): 8-bit matrix multiplication for transformers at scale, 2022.

\bibitem[Duda et~al.(2000)Duda, Hart, and Stork]{DudaHart2nd}
Duda, R.~O., Hart, P.~E., and Stork, D.~G.
\newblock \emph{Pattern Classification}.
\newblock John Wiley and Sons, 2nd edition, 2000.

\bibitem[Emozilla(2023)]{emozillareddit}
Emozilla.
\newblock {Dynamically Scaled RoPE further increases performance of long context LLaMA with zero fine-tuning}, 2023.
\newblock URL \url{https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/}.

\bibitem[Frantar \& Alistarh(2024)Frantar and Alistarh]{4bitfrantar2024marlin}
Frantar, E. and Alistarh, D.
\newblock Marlin: a fast 4-bit inference kernel for medium batchsizes.
\newblock \url{https://github.com/IST-DASLab/marlin}, 2024.

\bibitem[Ge et~al.(2024)Ge, Zhang, Liu, Zhang, Han, and Gao]{ge2024model}
Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao, J.
\newblock Model tells you what to discard: Adaptive {KV} cache compression for {LLM}s.
\newblock In \emph{The Twelfth International Conference on Learning Representations (ICLR)}, 2024.

\bibitem[Gliwa et~al.(2019)Gliwa, Mochol, Biesek, and Wawer]{gliwa2019samsum}
Gliwa, B., Mochol, I., Biesek, M., and Wawer, A.
\newblock {SAMS}um corpus: A human-annotated dialogue dataset for abstractive summarization.
\newblock In \emph{Proceedings of the 2nd Workshop on New Frontiers in Summarization}, pp.\  70--79, Hong Kong, China, November 2019.

\bibitem[Guo et~al.(2023{\natexlab{a}})Guo, Tang, Hu, Leng, Zhang, Yang, Liu, Guo, and Zhu]{Guo_2023}
Guo, C., Tang, J., Hu, W., Leng, J., Zhang, C., Yang, F., Liu, Y., Guo, M., and Zhu, Y.
\newblock Olive: Accelerating large language models via hardware-friendly outlier-victim pair quantization.
\newblock In \emph{Proceedings of the 50th Annual International Symposium on Computer Architecture}, ISCA ’23. ACM, June 2023{\natexlab{a}}.
\newblock \doi{10.1145/3579371.3589038}.
\newblock URL \url{http://dx.doi.org/10.1145/3579371.3589038}.

\bibitem[Guo et~al.(2023{\natexlab{b}})Guo, Xu, Duan, Yin, and McAuley]{guo2023longcoder}
Guo, D., Xu, C., Duan, N., Yin, J., and McAuley, J.
\newblock Longcoder: A long-range pre-trained language model for code completion.
\newblock In \emph{International Conference on Machine Learning}, pp.\  12098--12107. PMLR, 2023{\natexlab{b}}.

\bibitem[Han et~al.(2024)Han, Jayaram, Karbasi, Mirrokni, Woodruff, and Zandieh]{han2023hyperattention}
Han, I., Jayaram, R., Karbasi, A., Mirrokni, V., Woodruff, D., and Zandieh, A.
\newblock Hyperattention: Long-context attention in near-linear time.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=Eh0Od2BJIM}.

\bibitem[Hoffmann et~al.(2024)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de~Las~Casas, and Hendricks]{hoffmann2022training}
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de~Las~Casas, D., and Hendricks.
\newblock Training compute-optimal large language models.
\newblock In \emph{Proceedings of the 36th International Conference on Neural Information Processing Systems}, 2024.

\bibitem[Hsieh et~al.(2024)Hsieh, Sun, Kriman, Acharya, Rekesh, Jia, and Ginsburg]{hsieh2024ruler}
Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., and Ginsburg, B.
\newblock {RULER}: What{\textquoteright}s the real context size of your long-context language models?
\newblock In \emph{First Conference on Language Modeling}, 2024.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Huang et~al.(2021)Huang, Cao, Parulian, Ji, and Wang]{huang-etal-2021-efficient}
Huang, L., Cao, S., Parulian, N., Ji, H., and Wang, L.
\newblock Efficient attentions for long document summarization.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  1419--1436, Online, June 2021. Association for Computational Linguistics.

\bibitem[Izacard et~al.(2023)Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, and Grave]{izacard2023atlas}
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E.
\newblock Atlas: Few-shot learning with retrieval augmented language models.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (251):\penalty0 1--43, 2023.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Jiang et~al.(2022)Jiang, Gao, Wang, Araki, Ding, Callan, and Neubig]{jiang2022retrieval}
Jiang, Z., Gao, L., Wang, Z., Araki, J., Ding, H., Callan, J., and Neubig, G.
\newblock Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pp.\  2336--2349, December 2022.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer]{joshi-etal-2017-triviaqa}
Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L.
\newblock {T}rivia{QA}: A large scale distantly supervised challenge dataset for reading comprehension.
\newblock In Barzilay, R. and Kan, M.-Y. (eds.), \emph{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1601--1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.

\bibitem[Karpukhin et~al.(2020)Karpukhin, Oguz, Min, Lewis, Wu, Edunov, Chen, and Yih]{karpukhin2020dense}
Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t.
\newblock Dense passage retrieval for open-domain question answering.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.\  6769--6781, November 2020.

\bibitem[Kazemnejad et~al.(2024)Kazemnejad, Padhi, Natesan~Ramamurthy, Das, and Reddy]{kazemnejad2024impact}
Kazemnejad, A., Padhi, I., Natesan~Ramamurthy, K., Das, P., and Reddy, S.
\newblock The impact of positional encoding on length generalization in transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Kearns(1989)]{kearns89}
Kearns, M.~J.
\newblock \emph{Computational Complexity of Machine Learning}.
\newblock PhD thesis, Department of Computer Science, Harvard University, 1989.

\bibitem[Ko{\v{c}}isk{\'y} et~al.(2018)Ko{\v{c}}isk{\'y}, Schwarz, Blunsom, Dyer, Hermann, Melis, and Grefenstette]{kocisky-etal-2018-narrativeqa}
Ko{\v{c}}isk{\'y}, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K.~M., Melis, G., and Grefenstette, E.
\newblock The {N}arrative{QA} reading comprehension challenge.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 6:\penalty0 317--328, 2018.
\newblock \doi{10.1162/tacl_a_00023}.
\newblock URL \url{https://aclanthology.org/Q18-1023}.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica]{kwon2023efficient}
Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.~H., Gonzalez, J.~E., Zhang, H., and Stoica, I.
\newblock Efficient memory management for large language model serving with pagedattention.
\newblock In \emph{Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles}, 2023.

\bibitem[Langley(2000)]{langley00}
Langley, P.
\newblock Crafting papers on machine learning.
\newblock In Langley, P. (ed.), \emph{Proceedings of the 17th International Conference on Machine Learning (ICML 2000)}, pp.\  1207--1216, Stanford, CA, 2000.

\bibitem[Li et~al.(2023)Li, Shao, Xie, Sheng, Zheng, Gonzalez, Stoica, Ma, and Zhang]{longchat2023}
Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J., Stoica, I., Ma, X., and Zhang, H.
\newblock How long can context length of open-source {LLM}s truly promise?
\newblock In \emph{NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following}, 2023.

\bibitem[Li \& Roth(2002)Li and Roth]{li2002learning}
Li, X. and Roth, D.
\newblock Learning question classifiers.
\newblock In \emph{COLING 2002: The 19th International Conference on Computational Linguistics}, 2002.

\bibitem[Lin et~al.(2023)Lin, Tang, Tang, Yang, Dang, Gan, and Han]{lin2023awq}
Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., Gan, C., and Han, S.
\newblock Awq: Activation-aware weight quantization for llm compression and acceleration, 2023.

\bibitem[Lin et~al.(2024)Lin, Tang, Tang, Yang, Chen, Wang, Xiao, Dang, Gan, and Han]{lin2024awq}
Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S.
\newblock Awq: Activation-aware weight quantization for llm compression and acceleration, 2024.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Liu, Pan, He, Haffari, and Zhuang]{liu2024minicache}
Liu, A., Liu, J., Pan, Z., He, Y., Haffari, G., and Zhuang, B.
\newblock Minicache: {KV} cache compression in depth dimension for large language models.
\newblock \emph{arXiv preprint arXiv:2405.14366}, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Yan, Zaharia, and Abbeel]{liu2024world}
Liu, H., Yan, W., Zaharia, M., and Abbeel, P.
\newblock World model on million-length video and language with blockwise ringattention, 2024{\natexlab{b}}.

\bibitem[Liu et~al.(2024{\natexlab{c}})Liu, Lin, Hewitt, Paranjape, Bevilacqua, Petroni, and Liang]{liu2024lost}
Liu, N.~F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P.
\newblock Lost in the middle: How language models use long contexts.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 12:\penalty0 157--173, 2024{\natexlab{c}}.

\bibitem[Liu et~al.(2024{\natexlab{d}})Liu, Yan, An, Qiu, and Lin]{liu2024scaling}
Liu, X., Yan, H., An, C., Qiu, X., and Lin, D.
\newblock Scaling laws of ro{PE}-based extrapolation.
\newblock In \emph{The Twelfth International Conference on Learning Representations (ICLR)}, 2024{\natexlab{d}}.
\newblock URL \url{https://openreview.net/forum?id=JO7k0SJ5V6}.

\bibitem[Liu et~al.(2023)Liu, Wang, Dao, Zhou, Yuan, Song, and Shrivastava]{liu2023deja}
Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., and Shrivastava.
\newblock Deja vu: Contextual sparsity for efficient llms at inference time.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  22137--22176. PMLR, 2023.

\bibitem[Michalski et~al.(1983)Michalski, Carbonell, and Mitchell]{MachineLearningI}
Michalski, R.~S., Carbonell, J.~G., and Mitchell, T.~M. (eds.).
\newblock \emph{Machine Learning: An Artificial Intelligence Approach, Vol. I}.
\newblock Tioga, Palo Alto, CA, 1983.

\bibitem[Mitchell(1980)]{mitchell80}
Mitchell, T.~M.
\newblock The need for biases in learning generalizations.
\newblock Technical report, Computer Science Department, Rutgers University, New Brunswick, MA, 1980.

\bibitem[Mohtashami \& Jaggi(2024)Mohtashami and Jaggi]{mohtashami2023landmark}
Mohtashami, A. and Jaggi, M.
\newblock Random-access infinite context length for transformers.
\newblock In \emph{Proceedings of the 37th International Conference on Neural Information Processing Systems}, 2024.

\bibitem[Narayan et~al.(2018)Narayan, Cohen, and Lapata]{narayan2018xsum}
Narayan, S., Cohen, S.~B., and Lapata, M.
\newblock Don`t give me the details, just the summary! {T}opic-aware convolutional neural networks for extreme summarization.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pp.\  1797--1807, October-November 2018.

\bibitem[Newell \& Rosenbloom(1981)Newell and Rosenbloom]{Newell81}
Newell, A. and Rosenbloom, P.~S.
\newblock Mechanisms of skill acquisition and the law of practice.
\newblock In Anderson, J.~R. (ed.), \emph{Cognitive Skills and Their Acquisition}, chapter~1, pp.\  1--51. Lawrence Erlbaum Associates, Inc., Hillsdale, NJ, 1981.

\bibitem[Ni et~al.(2022)Ni, Hernandez~Abrego, Constant, Ma, Hall, Cer, and Yang]{ni2021t5}
Ni, J., Hernandez~Abrego, G., Constant, N., Ma, J., Hall, K., Cer, D., and Yang, Y.
\newblock Sentence-{T}5: Scalable sentence encoders from pre-trained text-to-text models.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2022}, pp.\  1864--1874, Dublin, Ireland, May 2022.

\bibitem[NLPCC(2014)]{LSHT}
NLPCC.
\newblock {Task Definition for Large Scale Text Categorization at NLPCC 2014}, 2014.

\bibitem[NVIDIA(2023)]{ada6000}
NVIDIA.
\newblock Nvidia ada lovelace professional gpu architecture.
\newblock \url{https://images.nvidia.com/aem-dam/en-zz/Solutions/technologies/NVIDIA-ADA-GPU-PROVIZ-Architecture-Whitepaper_1.1.pdf}, 2023.
\newblock [Accessed 28-05-2024].

\bibitem[NVIDIA(2024)]{nvidia_nvbench}
NVIDIA.
\newblock Nvbench: Nvidia's benchmarking tool for gpus, 2024.
\newblock Available online: \url{https://github.com/NVIDIA/nvbench}.

\bibitem[Nye et~al.(2022)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber, Dohan, Lewkowycz, Bosma, Luan, Sutton, and Odena]{nye2021show}
Nye, M., Andreassen, A.~J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., and Odena, A.
\newblock Show your work: Scratchpads for intermediate computation with language models.
\newblock In \emph{Deep Learning for Code Workshop}, 2022.

\bibitem[OpenAI(2023)]{openaiannouncement}
OpenAI.
\newblock New models and developer products announced at devday.
\newblock \url{https://openai.com/blog/new-models-and-developer-products-announced-at-devday#OpenAI}, November 2023.
\newblock Accessed: 2024-01-31.

\bibitem[OpenAI(2024)]{gpt4}
OpenAI.
\newblock Introducing {GPT}-4o: our fastest and most affordable flagship model.
\newblock \url{https://platform.openai.com/docs/models}, 2024.
\newblock [Accessed 28-05-2024].

\bibitem[Oren et~al.(2024)Oren, Hassid, Yarden, Adi, and Schwartz]{oren2024transformers}
Oren, M., Hassid, M., Yarden, N., Adi, Y., and Schwartz, R.
\newblock Transformers are multi-state {RNN}s.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pp.\  18724--18741. Association for Computational Linguistics, November 2024.

\bibitem[Peng et~al.(2023)Peng, Quesnelle, Fan, and Shippole]{peng2023yarn}
Peng, B., Quesnelle, J., Fan, H., and Shippole, E.
\newblock Yarn: Efficient context window extension of large language models, 2023.

\bibitem[Press et~al.(2022)Press, Smith, and Lewis]{press2021alibi}
Press, O., Smith, N., and Lewis, M.
\newblock Train short, test long: Attention with linear biases enables input length extrapolation.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Rae et~al.(2019)Rae, Potapenko, Jayakumar, Hillier, and Lillicrap]{raecompressive2019}
Rae, J.~W., Potapenko, A., Jayakumar, S.~M., Hillier, C., and Lillicrap, T.~P.
\newblock Compressive transformers for long-range sequence modelling.
\newblock \emph{arXiv preprint}, 2019.
\newblock URL \url{https://arxiv.org/abs/1911.05507}.

\bibitem[Ribar et~al.(2025)Ribar, Chelombiev, Hudlass-Galley, Blake, Luschi, and Orr]{ribar2023sparq}
Ribar, L., Chelombiev, I., Hudlass-Galley, L., Blake, C., Luschi, C., and Orr, D.
\newblock Sparq attention: bandwidth-efficient llm inference.
\newblock In \emph{ICML'24: Proceedings of the 41st International Conference on Machine Learning}, 2025.

\bibitem[Samuel(1959)]{Samuel59}
Samuel, A.~L.
\newblock Some studies in machine learning using the game of checkers.
\newblock \emph{IBM Journal of Research and Development}, 3\penalty0 (3):\penalty0 211--229, 1959.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2023roformer}
Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Tang et~al.(2024)Tang, Zhao, Zhu, Xiao, Kasikci, and Han]{tang2024quest}
Tang, J., Zhao, Y., Zhu, K., Xiao, G., Kasikci, B., and Han, S.
\newblock {QUEST}: Query-aware sparsity for efficient long-context {LLM} inference.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Thakkar et~al.(2023)Thakkar, Ramani, Cecka, Shivam, Lu, Yan, Kosaian, Hoemmen, Wu, Kerr, Nicely, Merrill, Blasig, Qiao, Majcher, Springer, Hohnerbach, Wang, and Gupta]{Thakkar_CUTLASS_2023}
Thakkar, V., Ramani, P., Cecka, C., Shivam, A., Lu, H., Yan, E., Kosaian, J., Hoemmen, M., Wu, H., Kerr, A., Nicely, M., Merrill, D., Blasig, D., Qiao, F., Majcher, P., Springer, P., Hohnerbach, M., Wang, J., and Gupta, M.
\newblock {CUTLASS}, January 2023.
\newblock URL \url{https://github.com/NVIDIA/cutlass}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G.
\newblock Llama: Open and efficient foundation language models, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Proceedings of the 31st International Conference on Neural Information Processing Systems}, pp.\  6000–6010, 2017.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Wang, S., Li, B.~Z., Khabsa, M., Fang, H., and Ma, H.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Wang et~al.(2023)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{wang2022self}
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.~A., Khashabi, D., and Hajishirzi, H.
\newblock Self-instruct: Aligning language models with self-generated instructions.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  13484--13508, Toronto, Canada, July 2023.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.~V., Zhou, D., et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Wu et~al.(2023)Wu, Zhan, Tan, Hou, Liang, and Song]{wu-etal-2023-vcsum}
Wu, H., Zhan, M., Tan, H., Hou, Z., Liang, D., and Song, L.
\newblock {VCSUM}: A versatile {C}hinese meeting summarization dataset.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pp.\  6065--6079, Toronto, Canada, July 2023.

\bibitem[Xiao et~al.(2023)Xiao, Lin, Seznec, Wu, Demouth, and Han]{xiao2023smoothquant}
Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S.
\newblock Smoothquant: Accurate and efficient post-training quantization for large language models, 2023.

\bibitem[Xiao et~al.(2024)Xiao, Tian, Chen, Han, and Lewis]{xiao2023streamingllm}
Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M.
\newblock Efficient streaming language models with attention sinks.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2024.

\bibitem[Yang et~al.(2018)Yang, Qi, Zhang, Bengio, Cohen, Salakhutdinov, and Manning]{yang2018hotpotqa}
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W.~W., Salakhutdinov, R., and Manning, C.~D.
\newblock Hotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.

\bibitem[Ye et~al.(2024)Ye, Lai, Lu, Lin, Zheng, Chen, Chen, and Ceze]{flashinfer}
Ye, Z., Lai, R., Lu, R., Lin, C.-Y., Zheng, S., Chen, L., Chen, T., and Ceze, L.
\newblock Cascade inference: Memory bandwidth efficient shared prefix batch decoding.
\newblock \url{https://flashinfer.ai/2024/01/08/cascade-inference.html}, Jan 2024.
\newblock URL \url{https://flashinfer.ai/2024/01/08/cascade-inference.html}.
\newblock Accessed on 2024-02-01.

\bibitem[Zandieh et~al.(2023)Zandieh, Han, Daliri, and Karbasi]{zandieh2023kdeformer}
Zandieh, A., Han, I., Daliri, M., and Karbasi, A.
\newblock Kdeformer: Accelerating transformers via kernel density estimation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  40605--40623. PMLR, 2023.

\bibitem[Zhang et~al.(2021)Zhang, Lei, Zhang, Han, Li, Yang, Yang, and Gao]{zhang2021deep}
Zhang, J., Lei, Y.-K., Zhang, Z., Han, X., Li, M., Yang, L., Yang, Y.~I., and Gao, Y.~Q.
\newblock Deep reinforcement learning of transition states.
\newblock \emph{Physical Chemistry Chemical Physics}, 23\penalty0 (11):\penalty0 6888--6895, 2021.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Naruse, Li, and Wang]{topk2023}
Zhang, J., Naruse, A., Li, X., and Wang, Y.
\newblock Parallel top-k algorithms on gpu: A comprehensive study and new methods.
\newblock In \emph{Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, New York, NY, USA, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2024)Zhang, Chen, Hu, Xu, Chen, Hao, Han, Thai, Wang, Liu, et~al.]{zhang2024bench}
Zhang, X., Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M., Han, X., Thai, Z., Wang, S., Liu, Z., et~al.
\newblock Infinitebench: Extending long context evaluation beyond 100k tokens.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  15262--15277, 2024.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Sheng, Zhou, Chen, Zheng, Cai, Song, Tian, R\'{e}, Barrett, Wang, and Chen]{zhang2023h2o}
Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., R\'{e}, C., Barrett, C., Wang, Z.~A., and Chen, B.
\newblock H2o: Heavy-hitter oracle for efficient generative inference of large language models.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~36, pp.\  34661--34710, 2023{\natexlab{b}}.

\bibitem[Zhao et~al.(2024)Zhao, Lin, Zhu, Ye, Chen, Zheng, Ceze, Krishnamurthy, Chen, and Kasikci]{zhao2023atom}
Zhao, Y., Lin, C.-Y., Zhu, K., Ye, Z., Chen, L., Zheng, S., Ceze, L., Krishnamurthy, A., Chen, T., and Kasikci, B.
\newblock Atom: Low-bit quantization for efficient and accurate llm serving, 2024.

\bibitem[Zheng et~al.(2022)Zheng, Li, Zhang, Zhuang, Chen, Huang, Wang, Xu, Zhuo, Xing, Gonzalez, and Stoica]{280874}
Zheng, L., Li, Z., Zhang, H., Zhuang, Y., Chen, Z., Huang, Y., Wang, Y., Xu, Y., Zhuo, D., Xing, E.~P., Gonzalez, J.~E., and Stoica, I.
\newblock Alpa: Automating inter- and {Intra-Operator} parallelism for distributed deep learning.
\newblock In \emph{16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)}, pp.\  559--578, Carlsbad, CA, Jul. 2022.
\newblock ISBN 978-1-939133-28-1.
\newblock URL \url{https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin}.

\end{thebibliography}
