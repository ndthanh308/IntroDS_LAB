% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{openai2023gpt4}
OpenAI, ``Gpt-4 technical report,'' 2023.

\bibitem{hoffmann2022training}
J.~Hoffmann, S.~Borgeaud, A.~Mensch, E.~Buchatskaya, T.~Cai, E.~Rutherford,
  D.~de~Las~Casas, L.~A. Hendricks, J.~Welbl, A.~Clark, T.~Hennigan, E.~Noland,
  K.~Millican, G.~van~den Driessche, B.~Damoc, A.~Guy, S.~Osindero,
  K.~Simonyan, E.~Elsen, J.~W. Rae, O.~Vinyals, and L.~Sifre, ``Training
  compute-optimal large language models,'' 2022.

\bibitem{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix,
  B.~Rozière, N.~Goyal, E.~Hambro, F.~Azhar, A.~Rodriguez, A.~Joulin,
  E.~Grave, and G.~Lample, ``Llama: Open and efficient foundation language
  models,'' 2023.

\bibitem{zhu2023minigpt4}
D.~Zhu, J.~Chen, X.~Shen, X.~Li, and M.~Elhoseiny, ``Minigpt-4: Enhancing
  vision-language understanding with advanced large language models,'' 2023.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' 2017.

\bibitem{lan2020albert}
Z.~Lan, M.~Chen, S.~Goodman, K.~Gimpel, P.~Sharma, and R.~Soricut, ``Albert: A
  lite bert for self-supervised learning of language representations,'' 2020.

\bibitem{clark2020electra}
K.~Clark, M.-T. Luong, Q.~V. Le, and C.~D. Manning, ``Electra: Pre-training
  text encoders as discriminators rather than generators,'' 2020.

\bibitem{devlin2019bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' 2019.

\bibitem{shaw2018selfattention}
P.~Shaw, J.~Uszkoreit, and A.~Vaswani, ``Self-attention with relative position
  representations,'' 2018.

\bibitem{dai2019transformerxl}
Z.~Dai, Z.~Yang, Y.~Yang, J.~Carbonell, Q.~V. Le, and R.~Salakhutdinov,
  ``Transformer-xl: Attentive language models beyond a fixed-length context,''
  2019.

\bibitem{he2021deberta}
P.~He, X.~Liu, J.~Gao, and W.~Chen, ``Deberta: Decoding-enhanced bert with
  disentangled attention,'' 2021.

\bibitem{su2022roformer}
J.~Su, Y.~Lu, S.~Pan, A.~Murtadha, B.~Wen, and Y.~Liu, ``Roformer: Enhanced
  transformer with rotary position embedding,'' 2022.

\bibitem{wang2020linformer}
S.~Wang, B.~Z. Li, M.~Khabsa, H.~Fang, and H.~Ma, ``Linformer: Self-attention
  with linear complexity,'' 2020.

\bibitem{zhou2021informer}
H.~Zhou, S.~Zhang, J.~Peng, S.~Zhang, J.~Li, H.~Xiong, and W.~Zhang,
  ``Informer: Beyond efficient transformer for long sequence time-series
  forecasting,'' 2021.

\bibitem{dao2022flashattention}
T.~Dao, D.~Y. Fu, S.~Ermon, A.~Rudra, and C.~Ré, ``Flashattention: Fast and
  memory-efficient exact attention with io-awareness,'' 2022.

\bibitem{mohtashami2023landmark}
A.~Mohtashami and M.~Jaggi, ``Landmark attention: Random-access infinite
  context length for transformers,'' 2023.

\bibitem{tworkowski2023focused}
S.~Tworkowski, K.~Staniszewski, M.~Pacek, Y.~Wu, H.~Michalewski, and
  P.~Miłoś, ``Focused transformer: Contrastive training for context
  scaling,'' 2023.

\bibitem{xsum-emnlp}
S.~Narayan, S.~B. Cohen, and M.~Lapata, ``Don't give me the details, just the
  summary! {T}opic-aware convolutional neural networks for extreme
  summarization,'' in \emph{Proceedings of the 2018 Conference on Empirical
  Methods in Natural Language Processing}, Brussels, Belgium, 2018.

\bibitem{penedo2023refinedweb}
G.~Penedo, Q.~Malartic, D.~Hesslow, R.~Cojocaru, A.~Cappelli, H.~Alobeidli,
  B.~Pannier, E.~Almazrouei, and J.~Launay, ``The refinedweb dataset for falcon
  llm: Outperforming curated corpora with web data, and web data only,'' 2023.

\bibitem{zeng2022glm130b}
A.~Zeng, X.~Liu, Z.~Du, Z.~Wang, H.~Lai, M.~Ding, Z.~Yang, Y.~Xu, W.~Zheng,
  X.~Xia, W.~L. Tam, Z.~Ma, Y.~Xue, J.~Zhai, W.~Chen, P.~Zhang, Y.~Dong, and
  J.~Tang, ``Glm-130b: An open bilingual pre-trained model,'' 2022.

\bibitem{lin2023mpt}
K.~Lin, C.-C. Lin, L.~Liang, Z.~Liu, and L.~Wang, ``Mpt: Mesh pre-training with
  transformers for human pose and mesh reconstruction,'' 2023.

\end{thebibliography}
