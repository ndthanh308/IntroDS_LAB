% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage[dvipsnames]{xcolor}

% Remove the "review" option to generate the final version.
% \usepackage[review]{acl2023}
%\usepackage[acceptedWithA]{tacl2018v2}
\usepackage[acceptedWithA]{tacl2021v1}


% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{xfrac}

\usepackage[font=small]{caption}

\usepackage{tikz}
\usetikzlibrary{bayesnet}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{booktabs}

\usepackage{rycolab}
\usepackage{subspaceprobing}

\DeclareMathOperator*{\expect}{\mathbb{E}}



% If the title and author information does not fit in the area allocated, uncomment the following
%
\setlength\titlebox{3.25cm}
%
% and set <dim> to something 5cm or larger.

%\title{Functional Concept Erasure}
%\title{On the Geometric Encoding of Concepts in Language Models}
%\title{The Causal Geometry of Probing and Controlled Generation}
\title{A Geometric Notion of Causal Probing}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Cl{\'e}ment Guerner\quad Anej Svete\quad Tianyu Liu \quad Alexander Warstadt \quad Ryan Cotterell \\
\setlength{\fboxsep}{2.5pt}%
\setlength{\fboxrule}{2.5pt}%
\fcolorbox{white}{white}{
    $\{$\texttt{\href{mailto:cguerner@student.ethz.ch}{cguerner}}, \texttt{\href{mailto:anej.svete@inf.ethz.ch}{anej.svete}, }\texttt{\href{mailto:tianyu.liu@inf.ethz.ch}{tianyu.liu},
    }\texttt{\href{mailto:awarstadt@inf.ethz.ch}{awarstadt},
    }\texttt{\href{mailto:ryan.cotterell@inf.ethz.ch}{ryan.cotterell}}$\}$\texttt{@inf.ethz.ch}
} \\
    {%
\setlength{\fboxsep}{2.5pt}%
\setlength{\fboxrule}{2.5pt}%
\fcolorbox{white}{white}{
    % Figure removed
}
}}

%\usepackage{draftwatermark}
%\SetWatermarkText{Under Review}
%\SetWatermarkScale{0.3}  % Adjust the scale as needed (0.5 means 50% size)

\begin{document}
\maketitle

\begin{abstract}
%Linear concept erasure is a set of methods for removing information about concepts such as verbal number from a language model's representations via orthogonal projection onto subspaces of the representation space.
The linear subspace hypothesis \citep{bolukbasi2016} states that, in a language model's representation space, all information about a concept such as verbal number is encoded in a linear subspace.
Prior work has relied on auxiliary classification tasks to identify and evaluate candidate subspaces that might give support for this hypothesis.
% Existing work has quantified concept information via extrinsic measures, relying on an auxiliary classification task to measure the predictability of the concept before and after erasing the subspace from the model representations.
%These extrinsic measures have been used to claim that a language model relies on a particular subspace for prediction.
We instead give a set of intrinsic criteria which characterize an ideal linear concept subspace and enable us to identify the subspace using only the language model distribution.
% We instead propose a formal definition of \emph{intrinsic} information in a subspace of a language model's representation space, i.e., derived from the language model directly.
Our information-theoretic framework accounts for spuriously correlated features in the representation space \citep{kumar2022probing}.
As a byproduct of this analysis, we hypothesize a causal process for how a language model might leverage concepts during generation. 
Empirically, we find that LEACE \citep{belrose2023leace} returns a one-dimensional subspace containing roughly half of total concept information under our framework for verbal-number. 
Our causal intervention for controlled generation shows that, for at least one concept, the subspace returned by LEACE can be used to manipulate the concept value of the generated word with precision.\looseness=-1
%Importantly, our framework accounts for spurious correlations 
%We propose a counterfactual approach that avoids the failure mode of spurious correlations \citep{kumar2022probing} by treating components in the concept subspace and its orthogonal complement independently.
%We show that our counterfactual notion of information in a subspace is optimized by a \emph{causal} concept subspace.
%Furthermore, this intervention allows us to attempt concept controlled generation by manipulating the value of the concept component of a representation.
\end{abstract}


\section{Introduction} % \ryan{It needs to be in the TACL sty file.}
%Autoregressive large language models (LLMs) generate representations of words and their textual contexts to perform next word prediction.
%Given their ability to generate naturalistic text, it is reasonable to assume that the representations induced by language models encode, for example, some notion of world knowledge.
%We may suspect that a language model contains knowledge about the concept of a \conceptf{bird}\footnote{Throughout the text, we will use a distinguished typesetting to refer to concepts. For instance, the concept of a bird is written as \conceptf{bird}.} if it, when prompted, is able to generate a plausible story about birds flying, making nests, laying eggs, etc.
%Similarly, it seems reasonable to assume that a language model has some knowledge of linguistic concepts such as \conceptf{verbal-number} since almost all sentences generated by the model are syntactically correct, e.g., in generated text in English verbs agree with their subjects in number.
%This paper assumes that language models deal in underlying concepts. 
%We make this assumption to see whether it can help us better understand how LLMs make predictions, and how these predictions can be controlled. 
%This assumption has two implications, which we explore in this paper. 
%First, that the model has the ability to associate a textual context with a concept, and that this association impacts the causal generative process of the language model.
%Second, that there exists a geometric encoding of concept knowledge in a language model's representation space.\footnote{For now, we define a representation space simply as the $d$-dimensional vector space that a language model relies on to encode text. We propose a more formal definition in \cref{sec:lmsconcepts}.}
%Studying these requires nailing down an operational definition of a concept so mathematically precise as to be able to be tested, which is a non-trivial endeavor.\looseness=-1

%This paper focuses on linguistic concepts, specifically \conceptf{verbal-number}, which we use as a running example, and \conceptf{grammatical-gender}.

The reliance of language models (LMs) on concepts to make predictions---especially linguistic concepts such as \conceptf{verbal-number}\footnote{Throughout the text, we will use a distinguished typesetting to refer to concepts. For instance, the concept of a bird is written as \conceptf{bird}.}---is a well-studied phenomenon \citep{ravfogel-etal-2021-counterfactual,lasri-etal-2022-probing, amini-etal-2023-causal}. Earlier studies on this topic test whether an LM uses the concept of \conceptf{verbal-number} by giving it forced choice between a grammatical and an ungrammatical variant of a sentence \cite{linzen2016assessing, marvin-linzen-2018-targeted, goldberg2019assessing, lasri-etal-2022-probing}. Consider, for example, the sentences:
\ex. \a. \label{ex:gramm} \textit{The kids \textbf{walk} the dog.} \\ 
        {\small the kid.\textsc{pl} walk.\textsc{3pl.pres} the dog.\textsc{sg}}
     \b. \label{ex:ungramm} $^*$\textit{The kids \textbf{walks} the dog.} \\
        {\small the kid.\textsc{pl} walk.\textsc{3sg.pres} the dog.\textsc{sg}} \vspace{-2pt}

\citet{goldberg2019assessing} shows that LMs can achieve near perfect accuracy when forced to choose between two such variants. Such results strongly suggest that LMs make use of \conceptf{verbal-number} and other concepts to perform next-word prediction, but tell us little about how the representation spaces of these models encode such concepts.
% In this paper, we seek to go beyond extrinsic measurement of whether a language model is capable of choosing between \cref{ex:gramm} and \cref{ex:ungramm}.\clem{this sentence feels kinda weak now, think of something stronger}\looseness=-1
%Namely, we argue that models deal with concepts probabilistically\ryan{I would not say it's probabilistic. I would focus on geometric information idea. The stochastic $\rvC$ is a bit of a hack we had to make for reasons we've discussed. It's not a key contribution.}, via an unobserved concept-valued latent variable. 
%This allows us to reason about the probability of a concept, and in turn, to derive an information-theoretic notion of the mutual 
%information between concepts and representations.
%We do so by studying how a model's representation space. a geometric notion of how a concept is encoded in a model's representation space.

Our primary contribution is to construct a novel geometric notion of what it means for a neural LM's representation space\footnote{For now, we define a representation space simply as the $d$-dimensional vector space that a language model relies on to encode text. We propose a more formal definition in \cref{sec:lmsconcepts}.} to have information about a concept. 
Following \citet{bolukbasi2016} and \citet{ravfogel2022linear}, we argue that concepts are naturally operationalized by \emph{linear} subspaces. 
Linear subspaces lend themselves to tractable algorithms, and they have a simple geometric interpretation which makes it possible to erase a concept from a representation.
Existing work \citep{lasri-etal-2022-probing, ravfogel-etal-2023-linear} has relied on $\mathcal{V}$-information \cite{xu2020theory} to quantify the amount of information in the representation space of a language model, before and after concept erasure. 
This measure is \emph{extrinsic} to the language model, in the sense that it relies on a variational family $\mathcal{V}$ of auxiliary classifiers to measure concept information.
In contrast, we propose an \emph{intrinsic}, information-theoretic \citep{shannon1948mathematical} definition of information, by which we mean that information is quantified using distributions induced from the language model, i.e., without relying on an additional classifier.\looseness=-1

  
We show, via an example inspired by \citet{kumar2022probing}, that a naÃ¯ve approach to measuring intrinsic information in a subspace falls victim to spurious correlations. 
Specifically, while a ground truth, \emph{causal} concept subspace may exist in the representation space, correlated non-concept features can also contain information about the concept, complicating the task of estimating concept information in either subspace.
Our framework breaks the dependence between the concept subspace and its orthogonal complement, allowing us to \emph{correctly} compute information contained in either subspace while marginalizing out the other.
This approach is counterfactual in the sense that it creates representations that would not otherwise occur under the language model.
Crucially, it allows us to talk about the mutual information between linear subspaces and concepts.\looseness=-1

We derive four geometric properties within our counterfactual framework that characterize a precise geometric encoding of a concept.
First, \textbf{erasure} is the condition that the orthogonal complement of the concept subspace should contain \emph{no} information about the concept. 
Second, \textbf{encapsulation} states that projecting a representation onto our concept subspace should preserve \emph{all} the information about the concept.
Third, \textbf{stability} quantifies the requirement that projection onto the orthogonal complement of our concept subspace should preserve non-concept information.
Finally, \textbf{containment} ensures that the concept subspace does not contain additional information beyond the concept.\looseness=-1

Empirically, we study \conceptf{verbal-number} in English and \conceptf{grammatical-gender} in French. 
We find, for \conceptf{verbal-number}, that LEACE \citep{belrose2023leace} yields a one-dimensional concept subspace which, according to our novel counterfactual metrics, contains a large share of concept information while leaving non-concept information untouched. 
We then leverage our intrinsic measure of information to posit a causal graphical model by which a latent concept may govern LM text generation.
This model enables us to derive a causal controlled generation method by manipulating the concept component of a representation.
And, indeed, we find evidence that it is possible to use a one-dimensional subspace to control the generation behavior of the language model with respect to \conceptf{verbal-number}, but not for \conceptf{grammatical-gender}.\footnote{Code will be made available in camera ready version.} 


%The task of isolating such subspaces is known as linear concept erasure \cite{ravfogel-etal-2020-null, ravfogel2022linear, dev2020oscar}. 
%Methods for linear concept erasure have been applied to a variety of tasks, e.g., the mitigation gender and other demographic biases \citep{dev2019attenuating, dev2020oscar,ravfogel-etal-2020-null, ravfogel2022linear, shao-etal-2023-gold, shao2023erasure, kleindessner2023efficient} from textual representations, the generation of counterfactuals for the prediction of \conceptf{verbal-number} inside and outside of relative clauses \citep{ravfogel-etal-2021-counterfactual}, and to the study of the importance of linguistic concepts for LLMs \citep{elazar-etal-2021-amnesic, amini-etal-2023-causal}.\looseness=-1
%Since a isolating a linear subspaces inherently partitions the space in two, learning to erase a subspace naturally yields the subspace itself.
%We learn this partitioning, and use our information-theoretic framework to formally analyze this feature partition as an information partition.\looseness=-1

%In learning this partition, We learn such a partitioning, and leverage our information-theoretic framework to 
%We formalize this feature partitioning as an information partition. 

%Using these four 
%We rely on this structure to intervene concept, and measure the effect of this intervention on the model's generation process.
%Empirically, we show that our counterfactual information-theoretic framework fixes the misestimation issues associated with estimation of information in a subspace.
%We find that R-LACE \citep{ravfogel2022linear}, despite not optimizing our metrics, yields a remarkably precise partition of concept information.
%Finally, our control experiment shows that our proposed causal structure, under a clean partitioning of concept information, may be correct for some models.

%Importantly, after laying out these intuitive desiderata,\ryan{Don't call them desiderata. We are laying out properties that we want a subspace to have. Can we call it a ``causal subspace'' if it has all four?} we show that complete erasure by partitioning the representations space into two linear subspaces is in general not possible.\afra{this sounds like an important contribution, right? I would highlight that more and explain to what extent this is different from previous findings. also, it is good to know if this is an empirical result, or a theoretical finding.}\looseness=-1


%First, we propose a probabilistic framework for how LLMs deal with concepts, in the form of a latent variable model\clem{or causal structure? latent variable could explain 2 and 4, causal is more 4?}, that is far more comprehensive than the forced choice setup.
%Second, we use this framework to show that \emph{correctly} estimating information in a subspace requires a counterfactual approach. 
%We provide four definitions, under this counterfactual framework, that together characterize a clean partitioning of concept information in a model's representation space into concept and non-concept subspaces.
%Finally, we proposal a causal structure according to which a language model may leverage concepts to act on the concept subspace. 
%


%Our goal is to isolate a linear subspace that contains \emph{all} of the information about a concept.\ryan{I think we should say our goal is to give a clean definition.}
%In doing so, we derive four properties that such a linear a


%We provide a counterexample\ryan{I would present this as a contribution. I would just say we provide very clean definitions and mention we have a toy example that demonstrates the point.} inspired by \citet{kumar2022probing}, who showed this failure mode of adversarial linear erasure methods in the context of downstream main-task classification.\ryan{The counter example has nothing to do with main-task classification. It has to do with a common-cause effect. Rewrite.}
%The issue stems from the dependencies between the parts of the representations related to the concept and those related to other parts of the context.
%This motivates looking at concept erasure through a \emph{causal} perspective.
%Although imperfect, our information partition does return a one-dimensional subspace that contains a significant amount of concept information.
%More importantly, aside from the model's handling of the %concept, the removal of this subspace does not significantly alter the model's language generating behavior.
%Thanks to this positive result, we can argue on reasonable grounds for a causal intervention on our concept subspace, conditional on the value of the non-concept subspace.\afra{I had a hard time understanding this paragraph and in particular, understanding your causal probing approach. I guess what confuses me is that in what sense your perspective is a causal one? My proposal is to divide this into two paragraphs, one where you explain your contribution and how your causal probing is different and useful. Second, one where you explain the empirical positive results.}


\section{Concepts and Information} \label{sec:lmsconcepts} 

In this section, we build towards a definition of mutual information between representations and the concept of interest.
\looseness=-1
%We now define in information-theoretic terms what it means for a language model to encode a concept in its representation space.
%We start from the intuition that, in order to make such a claim, there should be high mutual information between the representations and the concept of interest.
%Importantly, this section gives the necessary tools to discuss the relationship between a \emph{subspace} of a language model's representation space that encode concepts in an information-theoretic sense, which we discuss in \cref{sec:functionalsubspace}.

\subsection{Language Modeling Basics} \label{sec:lms}

%We start with the basic definition of a language model.
A language model is a probability distribution $\plm$ over $\kleene{\alphabet}$, the Kleene closure over an alphabet $\alphabet$.
% We further define $\eosalphabet \defeq \alphabet \cup \{\eos\}$, with $\eos \notin \alphabet$ being a distinguished end-of-string symbol. 
% Importantly, $\eos \not \in \alphabet$.
%We refer to strings as $\str \in \kleene{\alphabet}$ and words as $\word \in \alphabet$. 
We parameterize $\plm$ in an autoregressive manner \citep{du-etal-2023-measure} as follows: 
\begin{equation}\label{eq:lm}
\plm(\str) = \plm(\eos \mid \str) \prod_{t=1}^{T} \plm(\wordt \mid \cxt)
\end{equation}
where $\wordt \in \alphabet$ refers to $t^{\text{th}}$ word\footnote{We refer to $\word \in \alphabet$ as words for simplicity, even though in the context of neural language modeling, these are often called subwords, tokens, or symbols.} in a string $\str \in \kleene{\alphabet}$, where $\cxt$ represents the first $(t-1)$ words of $\str$, and $\eos \notin \alphabet$ being a distinguished end-of-string symbol.
 

Many language models make use of contextual representations, i.e., they encode a textual context $\cxt$ as a real-valued column vector $\cxtenc \in \Rd$.
%, for example by a neural network.
%In modern NLP, the most common network for computing $\cxtenc$ is the Transformer \citep{vaswani2017attention}.
%The contextual representations $\cxtenc$ are integrated into the language model's parameterization, meaning $\plm(\wordt \mid \cxt)$ is often parameterized using a softmax as follows
%\begin{equation}\label{eq:softmax}
%\!\!\!\plm(\word \mid \cxtenc) \!\defeq\! \frac{\exp \left( \bv(\word) \cdot \cxtenc \right)}{\sum_{\word' \in \eosalphabet} \exp \left( \bv(\word') \cdot \cxtenc \right)}
%\end{equation}
%where $\bv(\word) \in \Rd$, a column vector, is a word-specific parameter vector.\footnote{We have omitted the bias term without loss of generality as we can always pad the contextual representations by a single dimension.}
Generally, $\cxtenc$ is deterministically computed from the context string $\cxt$\footnote{We relax this assumption later on, such that $\cxtenc$ can be stochastic given $\cxt$. One example of a language model with stochastic contextual embeddings is \citet{bowman-etal-2016-generating}.\looseness=-1}, such that the representation space of \cref{eq:lm} is defined as
\begin{equation}
\repspace \defeq \Big\{ \bh(\str) \mid \str \in \kleene{\alphabet} \Big\} \subset \Rd.\footnote{\textnormal{Despite consisting of real vectors, the cardinality of $\repspace$ is \emph{countably} infinite, because it contains exactly one element for every string in the countably infinte set $\kleene{\alphabet}$.
Thus, summing over $\repspace$\ is discrete and does not require integration.}}
\end{equation}
%where $\cxtenc$ is a deterministic function of $\cxt$.


\subsection{Language Models and Concepts} \label{sec:concepts}

We now discuss an exact sense in which a language model can be said to encode a concept. 
First, we define a concept based on the possible values it can take.
We formalize this with a \defn{concept set}, a finite, non-empty set $\concepts$ whose elements are those values.
%\footnote{
%In principle, the concept sets could be infinite if additional care is taken in the formal development.}
For example, we take the concept set for \conceptf{verbal-number} to include three values: \conceptval{sg} (e.g., \wordf{walks}), \conceptval{pl} (e.g., \wordf{walk}), and \conceptval{n/a} (e.g., \wordf{consternation}). 
For various reasons, including syncretism \citep{baerman2007syncretism}, some verbs in English can have ambiguous concept value depending on context.
For example, the \wordf{You} in the sentence \wordf{You walked to the store} can be \conceptval{sg} or \conceptval{pl}.
We find similar facts for other concepts in different languages. 
For instance, for \conceptf{grammatical-gender} in French, the adjective \wordf{marron} can be both \conceptval{fem} and \conceptval{msc}.\looseness=-1

To relate language models to concept sets, we introduce a deterministic probability distribution $\iota(\concept \mid \cxt, \word)$. 
$\iota$ tells us the probability that, in the sequential context $\cxt \in \kleene{\alphabet}$, word $\word \in \alphabet$ is annotated with the concept value $\concept \in \concepts$.
For now, we make the simplifying assumption that $\iota$ is deterministic, i.e., we have $\iota(\concept \mid \cxt, \word) \in \{0, 1\}$ for all $\concept \in \concepts$, $\word \in \alphabet$, and $\cxt \in \kleene{\alphabet}$.\footnote{To illustrate this formalism, consider the concept \conceptf{verbal-number} and \cref{ex:gramm,ex:ungramm}. The concept set for \conceptf{verbal-number} is $\concepts = \{\conceptvalfootnote{sg}, \conceptvalfootnote{pl}, \conceptvalfootnote{n/a}\}$, and $\iota$ maps as follows, e.g., $\iota(\conceptvalfootnote{sg} \mid \wordf{The kids}, \wordf{walk}) = 0$, $\iota(\conceptvalfootnote{pl} \mid \wordf{The kids}, \wordf{walk}) = 1$.
% \item $\iota(\conceptvalfootnote{n/a} \mid \wordf{The kids}, \wordf{walk}) = 0$
% \item $\iota(\conceptvalfootnote{sg} \mid \wordf{The kids}, \wordf{walks}) = 0$
% \item $\iota(\conceptvalfootnote{n/a} \mid \wordf{The kids}, \wordf{consternation}) = 1$
% \begin{tabular}{ lccc } 
%     \toprule
%     & \conceptvalfootnote{sg} & \conceptvalfootnote{pl} & \conceptvalfootnote{n/a} \\
%     \midrule
%     $\iota( . \mid \wordf{The kids}, \wordf{walk})$ & 0 & 1 & 0  \\ 
%     $\iota( . \mid \wordf{The kids}, \wordf{walks})$ & 0 & 0 & 1  \\ 
%     $\iota( . \mid \wordf{The kids}, \wordf{consternation})$ & 0 & 0 & 1  \\ 
%     \bottomrule
% \end{tabular}
}
%This deterministic assumption is an oversimplification, yet it proves useful in experiments. %---indeed, we just exhibited an example where a verb had ambiguous \conceptf{verbal-number} given full sentential context. 
%This deterministic oversimplication proves useful in the experimental portion of the paper, . 
%This deterministic formulation is useful for our experiments. 
We later relax this assumption in \cref{sec:causal} by proposing a stochastic operationalization of concepts.
% Next, we use $\iota$ to construct a joint distribution over concept values and representations.
\looseness=-1

%While this is an oversimplification, this assumption will prove useful in the experimental portion of our paper.

%Our ultimate goal is to talk about the information shared between a concept, with values $\concepts$, and representations $\repspace$, and discussing information in a mathematical manner requires probability.

%\begin{align} \label{eq:string-concept-joint}
%\pconcept&(\conceptseq , \str)\defeq  \\
%&p(\eos \mid \str) \prod_{t=1}^{T} \plm(\wordt \mid \cxt) \iota(\ct \mid \cxt, \wordt) \nonumber
%\end{align}
% \begin{equation} \label{eq:string-concept-joint}
% \pconcept (\conceptseq , \str)\defeq \plm(\str) \prod_{t=1}^{T} \iota(\ct \mid \cxt, \wordt) \end{equation}
%where $\conceptseq \in \concepts^T$.
%Where does $\pconcept(\conceptseq, \str)$ get us?


\subsection{Unigram Information}\label{sec:reps-information}

To construct a mutual information between the model's notion of a concept and its contextual representations, we require a joint distribution between a concept-valued random variable and a representation-valued random variable. In order for this estimate to be intrinsic, we obtain this distribution from the language model itself.\looseness=-1
%We now build our information-theoretic framework in this section, first by introducing a probability distribution over representations, and then by integrating concepts using $\iota$.

We begin in \cref{eq:inducedunigramwith} by defining the \defn{joint induced unigram} distribution of the language model over words and representations. 
In words, this distribution tells how frequently each word $\word \in \alphabet$ co-occurs with a representation $\bh \in \repspace$, on average, in a string $\str \sim \plm$.
%The induced unigram distribution is formally defined as 
%\begin{equation} \label{eq:inducedunigram}
%\punigram(\word) \defeq \sum_{\str \in \kleene{\alphabet}} \plm(\str) \frac{\sum_{t=1}^{T} \mathbbm{1}\{ \word = \wordt \}}{T} 
%\end{equation}
%We can further augment the induced unigram distribution to expose the language model's representations.
%This leads to a joint induced unigram over both words \emph{and} representations; we define this joint distribution as
\begin{align} \label{eq:inducedunigramwith}
    &\punigram(\word, \bh) \defeq \\ 
    &\sum_{\str \in \kleene{\alphabet}} \plm(\str) \frac{\sum_{t=1}^{T} \mathbbm{1}\Big\{ \word = \wordt \land
 \bh = \cxtenc \Big\}}{T} \nonumber
\end{align}%\clem{keeping for now, can be cut if necessary}
%The joint word--representation induced unigram distribution $\punigram$ tells us the frequency with which a word $\word \in \alphabet$ co-occurs with a representation $\bh \in \repspace$.
%We can use this joint word--representation distribution to define the following mutual information between words and representations
%\begin{equation}\label{eq:cxtmi}
%\!\!\!\MI(\rvX ; \rvH) =  \sum_{\word \in \alphabet} \sum_{\bh \in \repspace} \punigram(\word, \bh) \log\! \frac{\punigram(\word, \bh) }{\punigram(\word) \punigram(\bh)}
%\end{equation}
%where $\rvH$ is a $\repspace$-valued random variable. \Cref{eq:cxtmi} tells us how much information, on average, a representation $\bh \in \repspace$ tells us about the identity of a word $\word \in \alphabet$.\looseness=-1
%We have now established how to think probabilistically about representations jointly with \emph{words}. However, we are primarily interested in the relationships between \emph{concepts} and representations. 
Next, using $\iota$, we can define a concept--representation induced unigram distribution as
\begin{align} \label{eq:inducedconceptunigram}
    &\punigram(\concept, \bh) \defeq \\ 
    &\sum_{\str \in \kleene{\alphabet}} \plm (\str) \frac{\sum_{t=1}^{T}  \iota(\concept \mid \cxt, \wordt) \mathbbm{1}\Big\{\bh = \cxtenc \Big\}}{T} \nonumber
\end{align}\looseness=-1
%In other words, \cref{eq:inducedconceptunigram} can be computed from \cref{eq:inducedunigramwith} simply by summing $\punigram(\word, \bh)$ over all values of \word that are instances of \concept.\clem{@Alex idk if this is clearer than counting concept instances as opposed to word instances. id change back}
The only difference, relative to \cref{eq:inducedunigramwith}, is that we count instances of concept values mapped from context strings and words via our distribution $\iota$.\looseness=-1

We can now use \cref{eq:inducedconceptunigram} to compute our intrinsic measure of concept information in representations:\looseness=-1
\begin{equation} \label{eq:cxtconceptmi}
\!\!\MI(\rvC ; \rvH) =
\sum_{\concept \in \concepts}
\sum_{\bh \in \repspace} \punigram(\concept, \bh) \log \!\frac{\punigram(\concept, \bh)}{\punigram(\concept) \punigram(\bh)}
\end{equation}
%where the $\alphabet$-valued random variable $\rvX$ is replaced 
where $\rvC$ is a $\concepts$-valued random variable and $\rvH$ is a $\repspace$-valued random variable. 
\Cref{eq:cxtconceptmi} tells us how much information on average a representation $\bh \in \repspace$ tells us about the identity of a concept $\concept \in \concepts$. \looseness=-1%\cref{eq:cxtconceptmi} allows us to precisely define the \emph{information} between a concept and a representation in a rigorous way.

%By relating words in $\alphabet$ to concept values in $\concepts$ with $\iota$, we have reformulated the language model as a joint concept--string model. 
%From this joint model, we can derive an information-theoretic framework for evaluating how much information the model's representations contain about a concept. 
%Importantly, the language model is a first class citizen in the mutual information in \cref{eq:cxtconceptmi}: Aside from $\iota$, all of the components are derived from the language model, and it is computed in expectation over samples drawn from the model's joint concept--representation unigram distribution. 
%$\iota$ is the only ingredient that we have added, and it is non-parametric and rather uncontroversial.
%This means we are focusing on the model's notion of a concept, and has implications for how this quantity can be computed empirically. 
%We discuss this point in \cref{sec:otherdistributions}.\looseness=-1

Next, we define the following conditional mutual information: 
\begin{align}\label{eq:conditionalmi}
\MI(&\rvX ; \rvH \mid \rvC) = \\
&\sum_{\concept \in \concepts}
\sum_{\word \in \alphabet} \sum_{\bh \in \repspace} 
\punigram(\word, \bh, \concept) \log \frac{\punigram(\word, \bh \mid \concept) }{\punigram(\word\mid \concept) \punigram(\bh \mid \concept)} \nonumber
\end{align}
where $\punigram(\word, \bh, \concept)$ is trivially obtained by combining approaches used to derive \cref{eq:inducedunigramwith} and \cref{eq:inducedconceptunigram}.
This quantity measures, given a particular concept value $\concept \in \concepts$, how much additional information about a word $\word \in \alphabet$ is encoded in the model's representations. 
%Recall that with $\MI(\rvX; \rvH)$ defined in \cref{eq:cxtmi}, we quantified information about the next word contained in the language model's representation space.
%Conditioning this quantity on a particular $\rvC = \concept$ value reveals a lot about the identity of the next word. 
%For example, conditioning on $\rvC = \conceptval{sg}$ for \conceptf{verbal-number} tells us that both the textual context $\cxtenc$ encoded by $\rvH$ and the next word $\word$ encoded by $\rvX$ are \conceptval{sg}.
%For example, conditioning on $\rvC = \conceptval{sg}$ for \conceptf{verbal-number} tells us that the next word $\word$ encoded by $\rvX$ is \conceptval{sg}.%\footnote{We have defined the probability of a concept value $\concept$ via $\iota(\concept \mid \cxt, \word)$, i.e., as a function of textual context and a word.
%Conditioning on $\rvC = \conceptvalfootnote{\scriptsize sg}$ amounts to reverse engineering---we consider only samples $(\word, \cxtenc, \concept) \sim \punigram$ such that $\rvC = \conceptvalfootnote{\scriptsize sg}$ and $\iota(\conceptvalfootnote{\scriptsize sg} \mid \cxt, \word) = 1$.}
%\Cref{eq:conditionalmi} measures how much additional information, relative to the model's unigram distribution over \conceptval{sg} verbs $\punigram(\word \mid \conceptval{sg})$, a representation $\cxtenc$ provides about the identity of the \conceptval{sg} verb to follow $\cxt$.

%\begin{align} \label{eq:condcxtconceptmi}
%&\MI(\rvC ; \rvH \mid \rvbX) \\
%&\!=\!\!\sum_{\str \in \kleene{\alphabet}}
%\!\sum_{\concept \in \concepts}
%\sum_{\bh \in \repspace} \punigram(\concept, \bh \mid \str) \log \!\frac{\punigram(\concept, \bh \mid \str)}{\punigram(\concept \mid \str) \punigram(\bh \mid \str)} \nonumber
%\end{align}


%\subsection{Interfacing with Other Distributions} \label{sec:otherdistributions}

%For an autoregressive language model like the one defined in \cref{eq:lm}, our various mutual information measures defined in \cref{eq:cxtconceptmi,eq:conditionalmi} are taken in expectation with respect to the induced unigram $\punigram$.
%This means we can only estimate these quantities by generating samples from the language model using ancestral sampling.
Our information-theoretic framework can be generalized to handle different language-generating processes, i.e., different decoding algorithms for language models. 
Let $\ptilde$ be a distribution over $\kleene{\alphabet}$, which we assume we can easily draw samples from, e.g., a language model decoded with nucleus sampling \citep{holtzman2020nucleus}. 
We obtain the joint induced unigram distribution $\ptildeunigram(\concept, \bh)$ with respect to this distribution by replacing $\plm$ with $\ptilde$ in \cref{eq:inducedconceptunigram}.\looseness=-1
% Next, we specify how we expect concept information to be encoded in representations, in the form of a linear concept subspace.


\section{A Geometric Encoding of Concepts} \label{sec:functionalsubspace}

%In this section, we explore how concepts are geometrically encoded in a language model's representation space $\repspace$.
The \defn{linear subspace hypothesis} \cite{bolukbasi2016} makes a prediction about how the concept information we quantify in the previous section is represented geometrically in the LM's representation space. 
Specifically, it postulates that there exists a \emph{linear subspace} $\linspacec \subseteq \repspace$ that contains all of the information about a concept with values $\concepts$.%\ryan{This is not a linear subspace in the linear algebra sense. I noticed that when reading another paper that builds on this. We need to fix this before the resubmission.}
This hypothesis has been tested on various linguistic concepts, including \conceptf{verbal-number} \citep{ravfogel-etal-2021-counterfactual,lasri-etal-2022-probing, amini-etal-2023-causal} and \conceptf{grammatical-gender} \citep{amini-etal-2023-causal}.
We follow in this vein, and decompose the representation space $\repspace$ into a concept linear subspace and a non-concept, orthogonal subspace. 
We provide four definitions, using our information-theoretic framework, that characterize these subspaces in terms of the information that they contain.


\subsection{Concept Partition} \label{sec:concept-partition}

%We begin by reviewing a bit of linear algebra. 
%A linear subspace $\linspace \subseteq \Rd$ may be characterized by an orthogonal projection matrix $\bP \in \R^{d \times d}$ that maps any vector in $\Rd$ onto $\linspace$. 
%Such a projection matrix has the following properties: 
%\begin{enumerate*}[label=\roman*)]
%    \item symmetry, i.e., $\bP = \bP^{\top}$,
%    \item idempotence, i.e., $\bP^2 = \bP$,
%    \item orthogonality, i.e., $\bP \bh = \text{proj}_{\linspace}(\bh)$, where $\text{proj}_{\linspace}$ is the orthogonal projection operator.
%\end{enumerate*}
%We note that the rank $k$ of a projection matrix $\bPk \in \R^{d \times d}$ determines the dimensionality of the subspace that it characterizes. 
%Furthermore, for such a $\bPk$, $\eye_d - \bPk$, where $\eye_d$ the $d \times d$ identity matrix, is also an orthogonal projection matrix.
%It orthogonally projects onto the $(d-k)$-dimensional orthogonal complement of $\linspace$, denoted $\linspace^\bot$. In this sense, a projection matrix $\bP$ partitions a vector space into two subspaces $\linspace$ and $\linspacecomp$.\looseness=-1

Given a concept set $\concepts$, we define a partition of a language model's representation space $\repspace$ into a \defn{concept subspace} $\linspacec$ and its orthogonal complement, the \defn{non-concept subspace} $\linspaceccomp$. 
We refer to $\bPk \in \R^{d \times d}$ as the orthogonal projection matrix that projects onto $\linspaceccomp$, i.e., $\bPk \bh = \text{proj}_{\linspaceccomp}(\bh)$.
In turn, $\eyeminusPk$ projects onto our concept subspace $\linspacec$ with dimensionality $|\concepts|-1$, such that $(\eyeminusPk) \bh = \text{proj}_{\linspacec}(\bh)$.
% We note that the dimensionality of the $\linspacec$ is deterministically set to $1-|\concepts|$. 
% What makes $\linspacec$ a concept subspace, and $\linspaceccomp$ a non-concept subspace?
The partition of $\repspace$ into $\linspacec$ and $\linspaceccomp$ is an \emph{information} partition. \looseness=-1

We use \cref{eq:cxtconceptmi} to define the information about the concept encoded in both. 
Consider, for example, information in $\linspaceccomp$ about $\concepts$ on average over textual contexts %given a particular textual context
\begin{align} \label{eq:subspace-mi}
\MI(\rvC ;& \bPk \rvH) =
\\ &\sum_{\concept \in \concepts}
\sum_{\bh \in \repspace} 
\punigram(\concept, \bPk \bh) \log \!\frac{\punigram(\concept, \bPk \bh)}{\punigram(\concept) \punigram(\bPk \bh)} \nonumber
\end{align}
where the language model's representations are orthogonally projected onto $\linspaceccomp$ using $\bPk$. 
\cref{eq:subspace-mi} relates the \emph{geometric} notion of a linear subspace with the \emph{information-theoretic} notion of information.
Thus, if $\MI(\rvC ; \bPk \rvH)$ is low, we can say that $\bPk$ erases a lot of concept information in $\repspace$ by projecting onto the subspace $\linspaceccomp$.
%On the other, if $\MI(\rvC ; \bPk \rvH)$ is high, we can say it erases little information. 
We refer to $\repspacepar \defeq \{ \eyeminusPkh \mid \bh \in \repspace \}, \repspacebot \defeq \{ \bPk  \bh \mid \bh \in \repspace \}$ as random variables corresponding to contextual representations projected onto concept and non-concept subspaces, respectively.%\looseness=-1


\begin{table}[t!]
    \centering
    \begin{minipage}[b]{1.0\columnwidth}    
        \centering
        %% Figure removed
        % Figure removed
            \captionof{figure}{Example of erasure of a \conceptf{verbal-number} subspace, when predicting the next word given \wordf{The kids}. The representation space is two-dimensional with the $y$-axis representing the correct subspace encoding the concept $\conceptf{verbal-number}$, while the $x$-axis encodes the lemma. Word representations are denoted with $\mathbf{v}$ and contextual representation with $\bh$. On the left, we have the original representation space, and on the right, we have the space resulting from erasing information in our concept subspace, i.e., setting the $y$-coordinates of all vectors in the space to 0.\looseness=-1}\label{fig:geomexample}
    \end{minipage}
    \begin{minipage}[b]{1.0\columnwidth}
        \captionsetup{type=table}
        \begin{subtable}{\columnwidth}
            \small
            \centering
            \begin{tabular}{ ccccc } 
                 \toprule
                  & \wordf{walks} & \wordf{walk} & \wordf{goes} & \wordf{go} \\
                 \midrule
                 \conceptval{sg} & 0 & 0 & 0.7 & 0  \\ 
                 \conceptval{pl} & 0 & 0.3 & 0 & 0 \\ 
                 \bottomrule
            \end{tabular}
        \end{subtable} 
        \caption{Hypothetical joint unigram distribution $\punigram(\word, \concept)$ of \conceptf{verbal-number} and word. The lemma \wordf{walk} is only used as \conceptvalfootnote{pl} and \wordf{go} only as \conceptvalfootnote{sg}.} \label{tab:counterexample-joint}
        \vspace{-15pt}
    \end{minipage}
    %    \begin{subtable}{\columnwidth}
    %    \small \centering
    %    \begin{tabular}{ ccc } 
    %        \toprule
    %          & \wordf{walk} & \wordf{go} \\
    %        \midrule
    %         \conceptval{sg} & 0 & 1 \\ 
    %         \conceptval{pl} & 1 & 0 \\ 
    %         \bottomrule
    %    \end{tabular}
    %        \caption{Distribution of \conceptval{\footnotesize verbal-number} given the word.} \label{tab:counterexample-condi}
    %    \end{subtable} 
    %    \caption{
    %        Probability distribution of the example. \label{tab:counterexample}
    %    }
    %    \vspace{-15pt}
    %\end{minipage}
\end{table}

\subsection{The Perils of Correlation} \label{sec:example}


% \Cref{eq:subspace-mi} suggests an attractive objective for finding the concept subspace in the representation space: Finding the linear subspace of the representation space that minimizes the mutual information from \cref{eq:subspace-mi}.
\Cref{eq:subspace-mi} suggests an attractive property we might ask from $\bPk$: It should satisfy $\MI\left(\rvC; \bPk \rvH\right) = 0$, i.e., completely erase the information about the concept by projecting onto $\linspaceccomp$.
However, as we show next, this na{\"i}ve characterization is flawed.
We illustrate this point with a counterexample inspired by \citet{kumar2022probing}, as shown in \cref{fig:geomexample}.
%Together, these comprise all of the information stored in word representations $\bv(\word)$ and contextual representations $\cxtenc$.
Intuitively, such a transformation constitutes successful erasure.\footnote{One might refer to the y-axis as the \emph{causal} subspace, in the sense that manipulating the values of that subspace would result in changing precisely the concept encoded by the representation while leaving other aspects intact.}
To the extent that such a subspace exists in reality, finding the $\bPk$ that erases this subspace seems like the correct objective.

Now, consider the hypothetical joint word--concept unigram distribution $\punigram(\word, \concept)$ in \Cref{tab:counterexample-joint}. 
Under this distribution, a projection matrix $\bPk$ that erases the correct $y$-axis as shown in \cref{fig:geomexample} is \emph{not} the minimizer of \cref{eq:subspace-mi}. 
%That is  while being perpendicular to each other. 
Knowledge of the lemma alone reveals the \conceptf{verbal-number}, because $\rvHerase$ ($x$-axis) and $\rvHconcept$ ($y$-axis) are heavily correlated.
This means that $\MI(\rvC; \rvHerase) = 0.88 > 0$ in our toy example in \cref{fig:geomexample}. 
In order to have $\MI(\rvC; \rvHerase) = 0$, we would need to let $\bPk = \mathbf{0}$, thereby erasing all lemma information as well.
Thus, requiring $\bPk$ to satisfy $\MI(\rvC; \rvHerase) = 0$ does not characterize successful erasure because it requires removing all spuriously correlated features.\looseness=-1

\subsection{A Counterfactual Unigram Distribution} \label{sec:counterfactual}

The underlying problem with the example given in \cref{sec:example} is that $\rvHconcept$ and $\rvHerase$ have a common cause that introduces a spurious correlation---the $\kleene{\alphabet}$-valued context random variable $\rvbX$.
%The answer is actually relatively simple. 
This means $\MI(\rvHerase; \rvHconcept) > 0$, i.e., these variables are \emph{not} statistically independent. 
We resolve this issue by building a variant of our information-theoretic objective in \cref{eq:subspace-mi} that \emph{assumes} these two variables are statistically \emph{independent}, i.e., $\MI(\rvHerase; \rvHconcept) = 0$.
Under this assumption, $\rvHerase$ would contain no information about the concept, and identification of $\rvHconcept$ would be possible via mutual information.
%We build this assumption into our framework by breaking the statistical relationship between the variables. 
While this assumption likely never holds for a concept in practice, this does not matter here---we are crafting a metric under which the correct subspace will be optimal.\looseness=-1

We denote with $\bhpar \defeq (\eyeminusPk) \bh$ and $\bhbot \defeq \bPk \bh$ the projections onto the concept and non-concept subspace for $\bh \in \repspace$. 
Marginalizing with respect to the induced unigram distribution defined in \cref{sec:lmsconcepts}, we arrive at the following unigram distributions: 
\begin{equation}\label{eq:counterfactual-unigram-dist-bot}
    \punigram(\bhbot) \defeq \sum_{\bh \in \repspace}  \mathbbm{1}\{\bhbot = \bPk \bh\} \punigram(\bh) 
\end{equation}
\begin{equation}\label{eq:counterfactual-unigram-dist-par}     
    \punigram(\bhpar) \defeq \sum_{\bh \in \repspace} \mathbbm{1}\{\bhpar = (\eyeminusPk) \bh\} \punigram(\bh)     
\end{equation}
We now construct a variant of our induced unigram $\punigram(\word, \concept, \bh)$ that assumes independence between $\bhbot$ and $\bhpar$, i.e., $\qunigram(\bh) = \qunigram(\bhbot, \bhpar) \defeq \punigram(\bhbot)\,\punigram(\bhpar)$.
This \defn{counterfactual unigram distribution} $\qunigram$ assigns probability mass to $(\bhbot, \bhpar)$ pairs which, under $\punigram(\bh)$, would have zero probability.\looseness=-1
%The definition is given below\looseness=-1
\begin{align}\label{eq:counterfactualunigram}
  \qunigram(\word, &\concept,\bhpar, \bhbot) \defeq \sum_{\cxt \in \kleene{\alphabet}}\,\iota(\concept \mid \word, \cxt) \\
&\plm(\word \mid \bhpar, \bhbot)\,p(\cxt)\, \punigram(\bhpar)\, \punigram(\bhbot) \nonumber
\end{align}
%where $\plm(\word \mid \bhpar, \bhbot)$ is computed by summing the $\bhbot$ and $\bhpar$.
%Intuitively, this quantity measures the probability of the word--concept pair $(\word, \concept)$ jointly with a representation $\bhbot + \bhpar$ that may not exist in reality, thanks to the factorized unigram distributions of $\bhpar$ and $\bhbot$.
% \Cref{eq:counterfactualunigram} allows us to decouple $\bhpar$ and $\bhbot$ and, thus, compute the information contained in $\rvHerase$ and $\rvHconcept$ without worrying about their dependence. 
The choice of the name counterfactual, as well as the implications of this decoupling, will be made precise in \cref{sec:causal} when we introduce the causal interpretation of the word--concept model.\looseness=-1

We define the \defn{counterfactual mutual information} between the concept and the projection onto the non-concept subspace as\looseness=-1
\begin{align}  \label{eq:miqconceptcounterfactual}
    \MIq (& \rvC; \rvHerase ) \defeq \\
    &\sum_{\concept \in \concepts} \sum_{\bhbot \in \repspacebot} 
    \qunigram(\concept, \bhbot) \log \!\frac{\qunigram(\concept, \bhbot)}{\qunigram(\concept) \qunigram(\bhbot)} \nonumber
\end{align}
%This quantity measures information about the concept in $\rvHerase$, and is computed by marginalizing out $\bhpar$ from \cref{eq:counterfactualunigram}.
Importantly, \cref{eq:miqconceptcounterfactual} is minimized by the correct subspace in our example in \cref{sec:example}. 
Note that $\MIq (\rvC; \rvHconcept)$ can also be obtained by marginalizing out $\bhbot$ instead.
Finally, we define $\MIq (\rvX ; \rvHerase \mid \rvC)$ by using $\qunigram$ instead of $\punigram$ in \cref{eq:conditionalmi}.\looseness=-1
%Next, we leverage our counterfactual framework to establish several definitions that help us reason about the quality of our partition into concept and non-concept subspaces.\looseness=-1


\subsection{Erasure and Encapsulation} \label{sec:erasure}

We now give formal definitions of erasure and encapsulation based on \cref{eq:miqconceptcounterfactual}. 
These two notions, combined, determine the extent to which a projection matrix $\bPk$ has decomposed the representation space into concept and non-concept subspaces.

\begin{defin}[Counterfactual Erasure]\label{def:eraser}
Let $\rvHerase \defeq \bPk \rvH$ be an $\Rd$-valued random variable.
An orthogonal projection matrix $\bPk \in \R^{d \times d}$ is an $\varepsilon$-\defn{eraser} of $\concepts$ if $\MIq(\rvC; \rvHerase) < \varepsilon$.\looseness=-1
\end{defin}
As $\varepsilon \to 0$, the subspace $\linspaceccomp$ characterized by an $\varepsilon$-eraser $\bPk$ for concept set $\concepts$ with respect to $\repspace$ encodes very little information about the concept.
This means that the language model is no longer able to determine the concept value required by the textual context when generating the next word.
% In doing so, we have removed the distinction between values in the concept set $\concepts$, i.e., we can no longer easily predict the concept value of a word in a particular context.
%On the other hand, our concept subspace $\linspacec$, characterized by orthogonal projection matrix $\eyeminusPk$, should encode nearly \emph{all} information about $\concepts$.
We now show that given an $\varepsilon$-eraser $\bPk$, projecting onto its orthogonal complement with $\eyeminusPk$ preserves nearly all of the information.\looseness=-1

\begin{defin}[Counterfactual Encapsulation]\label{def:encapsulation}
Let $\rvHconcept \defeq \left(\eyeminusPk\right) \rvH$ be an $\Rd$-valued random variable.
An orthogonal projection matrix $\eyeminusPk \in \R^{d \times d}$ is an $\varepsilon$-\defn{encapsulator} 
of $\concepts$ if $\MIq(\rvC; \rvH) - \MIq(\rvC; \rvHconcept) < \varepsilon$.\looseness=-1
\end{defin}
The quantity $\MIq(\rvC; \rvH) - \MIq(\rvC; \rvHconcept)$ is always non-negative due to the data-processing inequality \citep[\S2.8]{coverthomas2006}.
Encapsulation operationalizes the idea that a subspace gives us all the information needed to correctly identify the concept value required by textual context.
Combining erasure and encapsulation, we show that the mutual information decomposes additively in the following sense. 
\begin{restatable}{proposition}{pleasantdecomposition} \label{prop:pleasantdecomposition}
Suppose $\bPk$ is a $\varepsilon$-eraser and $(\eyeminusPk)$ is a $\varepsilon$-encapsulator of $\concepts$ with respect to $\repspace$.
Then, as $\varepsilon \rightarrow 0$, the following holds
\begin{equation} \label{eq:pleasantdecomposition}
\MIq(\rvC; \rvH) = \MIq(\rvC; \rvHerase) + \MIq(\rvC; \rvHconcept)
\end{equation}
\end{restatable}
\begin{proof}
    See \cref{app:pleasantdecomposition}. % for a detailed derivation of \cref{eq:pleasantdecomposition}.
\end{proof}

\input{figs/causalgraph}

\subsection{Containment and Stability} \label{sec:containstab}

Erasure and encapsulation do not consider the information content of the representation aside from the concept.
With perfect erasure and encapsulation, the learned orthogonal projection matrix $\bPk$ could erase much of the non-concept related information from $\linspaceccomp$. 
Specifically, if $\concepts$ is encoded non-linearly \citep{ravfogelkernel2022}, then erasure via a linear orthogonal projection could require the removal of additional dimensions that also contain non-concept information.
Therefore, in the concept erasure literature, tests of successful erasure are paired with a verification that the representations are not otherwise damaged \citep{kumar2022probing,ravfogel-etal-2020-null,ravfogel2022linear,ravfogelkernel2022,elazar-etal-2021-amnesic}. 
We, too, need an information-theoretic notion of preservation of non-concept information in $\rvHerase$.
% Specifically, we would expect non-concept features\tianyu{loss of non-concept \textbf{information}. the term for the concept/non-concept component should better be fixed throughout the paper} to be erased if concepts are \emph{not} encoded linearly. 
% To account for that, we extend our formalization to define stability, a natural way to measure the precision with which $\bPk$ isolates \emph{exclusively} information relating to the concept in a language model's representation space. 
%We refer to this measure as stability. 
%Stability is the foil to erasure, in that it is about keeping the relative probability of words with the \emph{same} concept value the same.
%We elaborate on this notion in the subsequent paragraphs.\looseness=-1

Preserving information about non-concept aspects of $\cxt$ in $\rvHerase$ requires that $\rvHconcept$ \emph{only} capture information about the concept, i.e. that it should be the \emph{minimal} subspace that captures $\rvC$. 
Containment formalizes this notion by requiring that, conditioned on $\rvC$, $\rvHconcept$ contains little information about the next word $\rvX$.\looseness=-1
% To formally capture what it means to preserve the information about non-concept parts of the representation, we introduce containment.
%This is characterized by containment, which formalizes the requirement that $\rvHconcept$ only capture the information about the concept.
%We just stated that the only information (or stochastisity) in $\rvHconcept$ should come from the concepts, so by conditioning on the concept, we should remove all stochasticity from $\rvHconcept$.
\begin{defin}[Counterfactual Containment] \label{def:containment}
Let $\bPk$ be an eraser for concept set $\concepts$ with respect to $\repspace$.
Let $\rvHconcept \defeq \left(\eyeminusPk\right) \rvH$ be an $\Rd$-valued random variable.
Then, we say that $\bPk$ is $\varepsilon$-\defn{contained} with respect to $\repspace$ and $\concepts$ if $\MIq(\rvX; \rvHconcept \mid \rvC) < \varepsilon$.\looseness=-1
\end{defin}
Lastly, we define stability to measure how much non-concept information about the next word is \emph{preserved} in the non-concept subspace $\rvHerase$.
Ideally, this should be as close as possible to the information present in the entire representation space, ignoring the information about the concept.
% the precision with which $\bPk$ isolates \emph{exclusively} information relating to the concept in a language model's representation space. 
\begin{defin}[Counterfactual Stability] \label{def:stability}
Let $\bPk$ be an eraser for concept set $\concepts$ with respect to $\repspace$.
Let $\rvHerase \defeq \bPk \rvH$ be an $\Rd$-valued random variable.
Then, we say that $\bPk$ is an $\varepsilon$-\defn{stabilizer} with respect to $\repspace$ and $\concepts$ if $\MIq(\rvX; \rvH \mid \rvC) - \MIq(\rvX; \rvHerase \mid \rvC) < \varepsilon$.\looseness=-1
\end{defin}
The data processing inequality once again ensures that $\MIq(\rvX ; \rvH \mid \rvC) - \MIq(\rvX ; \rvHerase \mid \rvC) \geq 0$. 
Containment and stability together characterize the \emph{preservation} of information not related to concepts.\looseness=-1
%In analogy to the goal of erasure and encapsulation in \cref{sec:erasure}, containment can be viewed as the erasure of next-word-related information from $\rvHconcept$, and stability can be viewed as the encapsulation of next-word-related information in $\rvHerase$.\looseness=-1

%We have now defined four properties which, together, allow us to verify that we have properly identified the concept subspace and partitioned the representation space according to \cref{eq:pleasantdecomposition}. 
%Next, we argue that such a partitioning enables us to control a language model's generative process.\looseness=-1




%we first provide a brief introduction of the causal vocabulary we will require and then discuss how the causal perspective can be useful for controlling generation of text in our setting.
%Then,
\section{A Causal Graphical Model} \label{sec:causal}
%We now explore the question of how the geometric treatment of concept information discussed in \cref{sec:functionalsubspace} can be used to motivate a latent causal structure for controlled generation from a language model: 
We now propose a causal structure by which language models leverage concepts, in the form of a latent variable, in the generation process.
We relate this causal structure to the information partition definitions given in \cref{sec:functionalsubspace}.
This enables causal controlled generation via a do-intervention \cite{pearl2009causal} on the concept random variable $\rvC$.
We finish with a discussion of how our causal controlled generation approach improves upon existing approaches. 
\looseness=-1


\subsection{Concept as a Latent Variable}

We illustrate the tradidional autoregressive causal structure, based on the model definition put forth in \cref{sec:lms}, in \cref{fig:causal-graph-a}.
In it, the $\kleene{\alphabet}$-valued random variable $\rvbX$ represents the textual context that was previously sampled from the model, $\rvH$ is the deterministic contextual representation, and $\rvX$ the word which is sampled using $\rvH$.

To enable controlled generation with respect to the concept, we introduce a $\concepts$-valued latent variable $\rvC$ in the generation process, as shown in \cref{fig:causal-graph-b}.
We make two assumptions about $\rvC$.
First, we assume that the distribution of $\rvC$ is influenced by the textual context $\rvbX$, and, moreover, that $\rvC$ is not \emph{fully} determined by the context $\cxt$, i.e., $\rvC$ is stochastic.
This assumption is justified by the fact that the concept value of the next word may not be fully determined by the preceding context, as discussed in \cref{sec:lmsconcepts}.
%We saw in an example in \cref{sec:lmsconcepts} that the concept value of a word is not fully determined by the preceding context.
Second, we assume that the concept is determined \emph{before} the word is sampled.
This enables controlled generation, as the concept can directly influence the sampled word $\word$.
In doing so, we break away from $\iota$, which deterministically assigned a concept value to a word based on the preceding context. 

Our two assumptions on $\rvC$ have an important implication: $\rvbX$ is no longer the only source of stochasticity in $\rvH$, as in \cref{fig:causal-graph-a}.
Rather, we assume that both $\rvbX$ as well as $\rvC$ influence the representation $\rvH$, i.e., $\bh = \bh\left(\cxt, \concept\right)$.
%This means $\bh$ is now a function of both $\cxt$ and $\concept$, i.e., 
Although this construction is not the norm in neural language models, it is a minor departure from reality that greatly enables our model.
%This new contextual representation is then used to determine the distribution over the next word $\word$ according to \cref{eq:softmax}.
%This is not, strictly speaking, an accurate model of the construction of representations based on textual context, because $\rvH$ is \emph{deterministic} given $\rvbX$ in most neural language models, as previously discussed in this paper.
%Nevertheless, we make this assumption because we consider this to be a minor departure from reality that enables our model.\looseness=-1

\subsection{Causal Controlled Generation} \label{sec:causal-con-gen}

We now derive a formal relationship between erasure, encapsulation, stability, containment, and the assumed causal graph in \cref{fig:causal-graph-b}. 
First, inspecting \cref{fig:causal-graph-b}, we see that if we wish to intervene on $\rvC$ to influence $\rvX$, there is a single backdoor \backdoorcolor{path} from $\rvC$ to $\rvH$.
As shown in \cref{fig:causal-graph-c}, \emph{intervening} on $\rvC$ directly (denoted by $\causaldo(\rvC = \concept)$) removes the edge $\rvbX \rightarrow \rvC$, which lets us easily compute the distribution over the next word after intervention as follows
\begin{align} \label{eq:do-intervention}
p(&\word \mid \rvHerase = \bhbot, \causaldo(\rvC = \concept)) \\
&= \sum_{\boldg \in \repspace}p(\word \mid \rvH = \bhbot + (\eyeminusPk)\boldg)\,p(\boldg \mid \concept) \nonumber
\end{align}
where, as shown in \cref{fig:causal-graph-b}, we assume that $\bhbot$ is deterministic given the context $\cxt$. $\boldg$ is an $\Rd$-valued contextual representation that encodes a textual context $\cxt'$ with concept value $\concept$. With high probability, $\cxtenc$ and $\boldg(\cxt')$ will be different. This is the logical conclusion of our decision to treat $\bhbot$ and $\bhpar$ as statistically independent---we can intervene on the generation process by setting the value of the concept component independently.
\looseness=-1

We now make good on our decision to name the counterfactual unigram distribution from \cref{eq:counterfactualunigram} as such.
Assuming the model \cref{fig:causal-graph-b}, a do-intervention on $\rvC$---as depicted in \cref{fig:causal-graph-c}---implies erasure, encapsulation, stability, and containment.
We make this idea formal in the following theorem.
\begin{restatable}{theorem}{bigboy}\label{thm:graph}
Consider a joint distribution $p$ that factors as in \cref{fig:causal-graph-b} parameterized by orthogonal projection matrix $\bPk$.
Under the distribution\looseness=-1
\begin{align}
\pdo(&\word, \bhbot, \bhpar, \concept) = p(\word \mid \bhbot, \bhpar)\\
&p(\bhbot \mid \mathrm{do}\left(\rvC = \concept\right))\,p(\bhpar \mid \mathrm{do}\left(\rvC = \concept\right))\,p(\concept) \nonumber
\end{align}
we have that $\bPk$ is an $\varepsilon$-eraser, $\eyeminusPk$ is an $\varepsilon$-encapsulator, $\eyeminusPk$ is an $\varepsilon$-container and $\bPk$ is an $\varepsilon$-stabilizer for every $\varepsilon > 0$.\looseness=-1
\end{restatable}
\begin{proof}
See \cref{app:graph}.
\end{proof}
What \Cref{thm:graph} tells us is that the graph given in \cref{fig:causal-graph-b} is consistent with the technical elaboration in \cref{sec:functionalsubspace}. 
Specifically, it means that erasure, encapsulation, stability, and containment are all properties that we expect a causal distribution resulting from an intervention on a concept to have. 
The interventional distributions, hence, motivate our discussion on independent $p(\bhpar)$ and $p(\bhbot)$ in \cref{sec:counterfactual}.\looseness=-1

%\subsection{Constant Concept Subspace Hypothesis} \label{sec:constantsubspace}

%Using the vocabulary and the graphical model established so far, we can formalize another special case as the following hypothesis: The case where there is no arc $\rvbX \rightarrow \rvHconcept$ in \cref{fig:causal-graph-b}. 
% In terms of our causal graphical model formulation, we consider the case where there is no arc $\rvbX \rightarrow \rvHconcept$. 
% \begin{hypothesis}[Constant concept subspace hypothesis]\label{hyp:constantsubspace}
%\begin{hypothesis}\label{hyp:constantsubspace}
%    Let $\left(\eyeminusPk\right)$ be an $\varepsilon$-encapsulator of $\rvC$.
%    The \defn{constant concept subspace hypothesis} states that, for any $\cxt \in \kleene{\alphabet}$ and $\concept \in \concepts$, the projection of $\bh\left(\cxt, \concept\right)$ onto the $\rvHconcept$ depends only on $\concept$ as $\varepsilon \rightarrow 0$.
    % That is, the contextual representation $\bh(\cxt)$ of all $\cxt \in \kleene{\alphabet}$, as $\varepsilon \rightarrow 0$,
    % That is, the contextual representation $\bh(\cxt)$ of all $\cxt \in \kleene{\alphabet}$, as ,
%    That is $\left(\eyeminusPk\right)\bh(\cxt, \concept) = \bgc \in \Rd$ for all $\concept \in \concepts$ for some $\boldg\colon \concepts \to \Rd$ assigning concepts vector representations.
    % , i.e., all possible values of $\rvC$} .
%\end{hypothesis}
%In words, each concept value $\concept \in \concepts$ has a constant encoding $\bgc$ in the concept subspace represented by random variable $\rvHconcept$.
%This means that $\rvC$ \emph{uniquely} determines $\rvHconcept$, i.e., $\rvHconcept$ is a deterministic function of $\rvC$. 
%We suspect that the constant concept subspace hypothesis will \emph{not} hold completely in practice.\looseness=-1

\subsection{Non-causal Controlled Generation} \label{sec:noncausalgen}

Controlled generation involving the manipulation of concepts is not a new problem. 
We contextualize our approach relative to \citeposs{yang-klein-2021-fudge} method.
They perform controlled generation as follows. 
First, they train a classifier to predict a concept value $\concept \in \concepts$ from the contextual representation $\bh$ of a language model.
Then, they perform controlled generation by conditioning on a concept value $\rvC = \concept$ and applying Bayes' rule as follows:
\begin{align}
p(&\word \mid \cxt, \rvC = \concept) \\ 
&\propto  p(\rvC = \concept \mid (\eyeminusPk)\cxtenc)\,p(\word \mid \cxt) \nonumber
\end{align}
We illustrate the causal structure implied by this approach in \cref{fig:causal-graph-d}.
We use $\bPk$ to relate this approach to our subspace formulation,\footnote{Thus, we assume that the classifier is restricted to looking at $\rvHconcept$ to make its prediction.} but \citet{yang-klein-2021-fudge} do not make use of concept subspaces.

A do-intervention on $\rvC$ has no effect on $\rvX$ with this causal structure, because there is no causal path from $\rvC$ to $\rvX$ in \cref{fig:causal-graph-d}. %i.e.,\looseness=-1
%A do-intervention would remove the edge from $\rvH$ to $\rvC$, thereby having no effect on $\rvX$, i.e., we have\looseness=-1
%\begin{equation}
%\!p(\word \mid \bh(\cxt), \causaldo(\rvC = \concept)) = p(\word \mid \bh(\cxt)) 
%\end{equation}
This is why the authors \emph{condition} on $\rvC$ instead.
In this sense, \citeposs{yang-klein-2021-fudge} and similar methods are not causal and cannot easily be extended to be so.
As discussed in \cref{sec:causal-con-gen}, our approach \emph{is} causal, but such an analysis may come at the price of a number of restricting assumptions that are not fully met in practice. 
In the next section, we explain how we go about testing these assumptions with data.

\section{Experiments and Results} \label{sec:results}

In the remainder of the paper, we test our framework empirically. Specifically, we answer two questions. First, are we able to find a projection matrix $\bPk$ that meets our definitions in \cref{sec:functionalsubspace}? Second, can we use the resulting concept subspace to successfully control the model's generation behavior, as theorized in \cref{sec:causal}?

\subsection{Experimental Setup}\label{sec:experimentsetup}

\begin{table*}[ht!]
    \centering
    %% Figure removed
    \small  
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l C{0.15\textwidth} C{0.15\textwidth} C{0.15\textwidth} C{0.15\textwidth}}%{ ccccc }
         \toprule
         & \multicolumn{2}{c}{Number (\texttt{gpt2-large})} & \multicolumn{2}{c}{Gender (\texttt{gpt2-base-french})}  \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5}
         % & Gender & Gender & Number & Number \\
         % & gpt2-base-french & gpt2-base-french & gpt2-large & gpt2-large \\
         & Ancestral & Nucleus & Ancestral & Nucleus \\
         \midrule
         Total Info, $\MIq(\rvC; \rvH)$ & 0.27\pms{0.01} & 0.37\pms{0.01} & 0.42\pms{0.01} & 0.47\pms{0.02} \\ 
         Erasure, $\MIq(\rvC; \rvHerase)$ & 0.11\pms{0.01} & 0.19\pms{0.01} & 0.27\pms{0.01} & 0.30\pms{0.02} \\ 
         Subspace Info, $\MIq(\rvC; \rvHconcept)$ & 0.16\pms{0.01} & 0.19\pms{0.01} & 0.10\pms{0.00} & 0.11\pms{0.01} \\ 
         Encapsulation, $\MIq(\rvC; \rvH) - \MIq(\rvC; \rvHconcept)$ & 0.11\pms{0.02} & 0.18\pms{0.02} & 0.32\pms{0.01} & 0.36\pms{0.02} \\
         Reconstructed Info, $\MIq(\rvC; \rvHerase) + \MIq(\rvC; \rvHconcept)$ & 0.27\pms{0.02} & 0.39\pms{0.01} & 0.37\pms{0.01} & 0.41\pms{0.02}\\
         \midrule
         Erasure Ratio & 0.42\pms{0.04} & 0.53\pms{0.04} & 0.64\pms{0.03} & 0.64\pms{0.04} \\
         Subspace Info Ratio & 0.58\pms{0.06} & 0.52\pms{0.04} & 0.23\pms{0.02} & 0.23\pms{0.02} \\ 
         Encapsulation Ratio & 0.42\pms{0.06} & 0.48\pms{0.04} & 0.77\pms{0.02} & 0.77\pms{0.02} \\
         Reconstructed Info Ratio & 1.00\pms{0.10} & 1.05\pms{0.06} & 0.87\pms{0.04} & 0.87\pms{0.04}\\
         \midrule
         Baseline, $\MIq(\rvX; \rvH \mid \rvC)$ & 1.08\pms{0.12} & 1.34\pms{0.10} & 1.81\pms{0.02} & 2.06\pms{0.04} \\
         Containment, $\MIq(\rvX; \rvHconcept \mid \rvC)$ & 0.12\pms{0.09} & 0.40\pms{0.11} & 0.27\pms{0.12} & 0.36\pms{0.11} \\
         Stability, $\MIq(\rvX; \rvH \mid \rvC) - \MIq(\rvX; \rvHerase \mid \rvC)$ & 0.06\pms{0.10} & 0.08\pms{0.09} & 0.02\pms{0.11} & -0.02\pms{0.08}\\
         \bottomrule
    \end{tabular}%
    }
    \caption{Counterfactual Information-Theoretic Results. In separate columns, we report two sets of results for each concept, estimated over samples generated using ancestral and nucleus sampling. Each entry shows mean $\pm$ standard deviation over random restarts (see \cref{sec:experimentsetup}). The first set of rows shows concept-related information metrics, namely \textit{Erasure} and \textit{Encapsulation}, along with previously undefined metrics. \textit{Total Info} is the amount of concept information in the original representations, \textit{Subspace Info} is concept information in the concept subspace, and \textit{Reconstructed Info} tests whether the partition is lossy relative to total information. The second set of rows shows the ratio of each of these quantities relative to \textit{Total Info}. The third set of rows shows \textit{Containment} and \textit{Stability} metrics, i.e., information about the next word $\rvX$, conditioned on $\rvC$. We include \textit{Baseline} as a reference point.}
    \label{tab:resultsmain}
    % \vspace{-.5cm}
\end{table*}


\paragraph{Concepts and Models.} We perform our analysis on two concepts, \conceptf{verbal-number} in English with $\concepts = \{\conceptval{sg}, \conceptval{pl}, \conceptval{n/a}\}$ and \conceptf{grammatical-gender} in French with $\concepts = \{\conceptval{fem}, \conceptval{msc}, \conceptval{n/a}\}$. For each of these concepts, we study the representation spaces of an autoregressive language model, namely GPT2 \citep{radford2019language}.\footnote{We rely on the implementations in the transformers library \citep{wolf-etal-2020-transformers}, namely: \modelnamef{gpt2-large} for \conceptf{verbal-number} and \modelnamef{gpt2-base-french} for \conceptf{grammatical-gender}.} 

\paragraph{Data.} For \conceptf{verbal-number} in English, we use \citeposs{linzen2016assessing} number agreement dataset. 
This dataset consists of sentences from Wikipedia that contain a \conceptval{sg} or \conceptval{pl} verb with the \textbf{fact} (ground truth verb) and the \textbf{foil} (inflected form of the fact to have opposite concept value). 
For \conceptf{grammatical-gender} in French, we rely on three treebanks from Universal Dependencies \citep{nivre-etal-2020-universal}: French GSD \citep{guillaumegsd}, ParTUT \citep{Sanguinetti2015, sanguinettipisa2014, boscotub2014}, and Rhapsodie \citep{lacheretrhapsodie2014}. 
We replicate the preprocessing steps of \citet{linzen2016assessing} on each of these datasets, i.e., we filter sentences to those containing \conceptval{fem} or \conceptval{msc} nouns with an associated adjective, and we obtain the foil by inflecting the \conceptf{grammatical-gender} of this adjective.


\paragraph{Vocabulary Partition.} In \cref{sec:concepts}, we defined our context-dependent distribution $\iota$ as a means of relating language models and concepts. %This distribution is context-sensitive in order to handle words with ambiguous concept values out of context, for example, due to syncretism in English. 
In practice, we drop the context-dependent aspect and consider a single partition of $\alphabet$ as our definition of a concept. 
%This assumption is justified by the fact that, in our evaluation framework, words with ambiguous concept values should see their probabilities change in the same manner as words with concept value \conceptval{n/a}\clem{idk if i can say this anymore -- need to think}. 
We start constructing the partition in a model-agnostic manner: we use SpaCy \citep{spacy} to tag the French and English Wikipedia corpora \citep{wikidump}, respectively. For \conceptf{verbal-number}, we use the tagged English words to obtain lists of third person present \conceptval{sg} and \conceptval{pl} verbs, which we then align to obtain matching pairs, e.g., (\wordf{walks}, \wordf{walk}). The process is the same for \conceptf{grammatical-gender} in French, leading to gendered pairs of adjectives, e.g., (\wordf{franÃ§ais}, \wordf{franÃ§aise}). For each model, we then partition the vocabulary according to the appropriate list, with tokens not included in either list classified as \conceptval{n/a}. One limitation of our work is that we do not consider concept words that are tokenized into more than one subword in our analysis---for example, if \wordf{disambiguates} tokenizes to [\wordf{disambiguate}, "\wordf{\#s}"], then the pair (\wordf{disambiguates}, \wordf{disambiguate}) is assigned to \conceptval{n/a}.\looseness=-1

\paragraph{The concept value $\conceptval{n/a}$.} In practice, we exclude $\conceptval{n/a}$ from our concept set when computing our metrics. The reason for this is that in both text sampled from the model and natural text, the vast majority of words do not invoke the concept, meaning the concept marginals $\punigram(\concept)$ and $\ptildeunigram(\concept)$ assign very low probabilities to our values of interest. Resulting information metrics would therefore be dominated by $\conceptval{n/a}$, so we choose to exclude it. 
%\clem{should i say more about this amounting to a conditional mutual information?}



\paragraph{Finding the Concept Subspace.} We find $\bPk$ using LEACE \citep{belrose2023leace}, the state-of-the-art method for linear concept erasure. %\citet{belrose2023leace} perform an affine transformation to erase the concept, i.e., $\bhbot = \bPk \bh + \boldsymbol{b}$, where $\boldsymbol{b} \in \Rd$ is a bias vector. Our framework trivially extends to affine functions by setting $\bPk' \in \R^{d \times (d+1)}$ and $[\bh; 1] \in \R^{d+1}$.\clem{@Ryan please confrim, this seems sus cuz P is not a projection matrix anymore}
LEACE maximizes a cross-entropy loss on samples from $\ptildeunigram$ with respect to $\bPk$, which constitutes a lower bound on our \emph{correlational} $\MI(\rvC; \rvHerase)$. 
Results are reported for three $\bPk$ estimates obtained from randomized train, test splits for each concept, and three random restarts of the experiment for each $\bPk$.
%Since \citet{belrose2023leace} finds a closed form solution that depends only on the training data, we report results for three $\bPk$ estimates obtained from randomized train, test splits for each concept. 
%Since computing both our mutual information estimates in \cref{sec:functionalsubspace} and our do-intervention in \cref{eq:do-intervention} requires Monte Carlo estimates of countably infinite sums, we additionally report three random restarts of the experiments for each $\bPk$ from LEACE.


\subsection{Partitioning of Concept Information} \label{sec:partitionresults}

In this section, we test empirically whether LEACE \citep{belrose2023leace} yields a $\bPk$ that performs well according to our counterfactual information-theoretic framework defined in \cref{sec:functionalsubspace}.
Although LEACE is the state of the art for concept erasure, we can anticipate several reasons why it might perform poorly.
First, LEACE does not optimize counterfactual erasure, i.e., it does not distinguish between causal vs.~spuriously correlated components of $\rvH$ (see \cref{sec:example}). 
Second, as discussed in \cref{sec:containstab}, the concept may be non-linearly encoded, such that the removal of a $(|\concepts|-1)$-dimensional subspace would not be sufficient to significantly reduce information.
Third, the models under study are not the state of the art, meaning their representation spaces are not necessarily of high quality, and we are limited by available data.
%Both of these issues are particularly problematic for \conceptf{grammatical-gender} in French using \modelnamef{gpt2-base-french}.
\looseness=-1 

Results in \Cref{tab:resultsmain} for \conceptf{verbal-number} show that LEACE finds a one-dimensional concept subspace that partitions only about 50\% of concept information according to our counterfactual metrics (\textit{Erasure Ratio} and \textit{Subspace Info Ratio}). 
Total amount of information in $\rvHerase$ and $\rvHconcept$ is slightly greater than \textit{Total Info}, but no information is lost in the partitioning (\textit{Reconstructed Info Ratio}).
\textit{Stability} values near 0 show that this partitioning preserves non-concept information in $\linspaceccomp$.
\textit{Containment} is relatively high compared to the \textit{Baseline}, meaning that the concept subspace found by LEACE is not minimal. 
For \conceptf{grammatical-gender} however, LEACE does not find a good projection matrix. 
Approximately only 30\% of concept information is erased, and information is lost in the case of ancestral sampling (\textit{Reconstructed Info Ratio} less than 1).
\looseness=-1 

Looking back on the three failure modes outlined at the start of this section, these results show that LEACE can sometimes return an adequate partitioning according to our framework, but does not drive erasure to 0. 
Empirically, it is difficult to determine whether this is due to the LEACE objective falling victim to spurious correlations or to the concept encoding being non-linear.
If the former is true, one solution might be to learn a $\bPk$ that optimizes our counterfactual framework. 
However, this is computationally intractable due to nested sums over the infinite representations space $\repspace$, and we leave the development of a tractable approximation to future work.
We attribute the failure to learn a concept partition for \conceptf{grammatical gender} to limitations of the model itself.
Compared to English, the best available French \modelnamef{gpt2} model is trained on less data and has fewer parameters.
In our preliminary experiments, we noticed that smaller English \modelnamef{gpt2} models for \conceptf{verbal-number} were also notably worse than \modelnamef{gpt2-large}.
%, while counterfactual MI is not reduced to 0, the model does rely on this subspace heavily for generation. Â¯

%We note that performance 

% Figure environment removed

\subsection{Causal Controlled Generation} \label{sec:controlresults}

In \cref{sec:causal}, we argued for a causal structure for language generation that allows us to intervene on the concept-valued random variable $\rvC$. 
We now test this causal model empirically by computing the do-intervention in \cref{eq:do-intervention}.
We define success for the intervention using the forced-choice setup shown in \cref{ex:gramm,ex:ungramm}. 
For example, given a context with a \conceptval{sg} fact, we consider $\causaldo(\rvC = \conceptval{pl})$ successful if $p(\word \mid \rvHerase = \bhbot, \causaldo(\rvC = \conceptval{pl}))$ assigns higher probability to the \conceptf{pl} foil over \conceptf{sg} fact.
 

Results for this experiment are shown in \Cref{fig:resultscontrol}.
For context, we report the model's accuracy in the forced-choice setup before (\textit{Orig. Acc.}) and after (\textit{Erased Acc.}) erasure. 
%These results are split by the concept value of the fact, to see whether the model performs better for one concept value over the other.
We note the consistency between information-theoretic metrics in \Cref{tab:resultsmain} and post-erasure accuracy in \cref{fig:resultscontrol}---the erasure intervention successfully lowers the accuracy of the minority class \conceptval{pl} for \conceptf{verbal-number}, however the intervention fails to significantly reduce accuracy for \conceptf{grammatical-gender}.

With this context in mind, the do-intervention is remarkably successful for \conceptf{verbal-number}. 
In particular, $\causaldo(\rvC = \conceptval{pl})$ succeeds for roughly 90\% of textual contexts in getting the model to assign higher probability to the \conceptval{pl} form of the (fact, foil) pair. 
This result is notable because the low accuracy on predicting the \conceptval{pl} fact after erasure means that, without the do-intervention, erasure strongly biases the model against generating the \conceptval{pl} form. 
%Recall that, in \cref{eq:do-intervention}, our point of departure is the erased representation $\bhbot$, from which the model would almost never predict \conceptval{pl} over \conceptval{sg}. 
By acting solely in our concept subspace via \conceptval{pl} values of $\bhpar$, we are able to instead get the model to almost always predict \conceptval{pl}.


Results for \texttt{gpt2-base-french} are much worse---$\causaldo(\rvC = \conceptval{msc})$ actually reduces the accuracy relative to after erasure, while, with $\causaldo(\rvC = \conceptval{fem})$, we see no significant difference.
Viewed together with results in \cref{sec:partitionresults}, this confirms that our causal structure only holds given an adequate $\bPk$ under our counterfactual framework. 
Nonetheless, the success of the do-intervention on \conceptf{verbal-number} despite concept information not being perfectly isolated in $\rvHconcept$ suggests that identifying the causal concept direction is not a necessary requirement for causal concept-based controlled generation, so long as $\rvHconcept$ contains a significant share of concept information.


\section{Related Work} \label{sec:related-work}

% usage-based probing
In terms of our stated goal of developing a geometrically oriented causal probing framework, our work is most closely related to \citet{elazar-etal-2021-amnesic} and \citet{lasri-etal-2022-probing}. 
\citet{elazar-etal-2021-amnesic} pose the problem of identifying a subspace used by a model to perform a task via an erasure intervention, on the assumption that a reduction in word prediction accuracy after intervention certifies the usage of the subspace. 
%Amnesic probing \citep{elazar-etal-2021-amnesic} measures usage by a language model using word prediction accuracy after erasure, without specifying which set of words is considered at each step (e.g., entire vocabulary, specific pairs). 
%\citet{lasri-etal-2022-probing} extend this evaluation procedure with a carefully designed dataset, referred to as a behavioral task, in which each context has a fact--foil pair, such that usage can be measured via change in the relative probabilities of the fact and foil, and the model's ability to assign higher probability to the fact. 
%In short, 
\citet{lasri-etal-2022-probing} applied \citeposs{linzen2016assessing} forced choice approach to the problem of evaluating the impact of concept erasure.
Our work relies on a behavioral (forced choice) dataset for learning the linear projection matrix, but importantly, not for evaluating usage. 
Our definition of erasure, for example, formalizes the expectation that \emph{all} concept-related word pairs should be indistinguisable under the language model after erasure given any contextual representation. 

%should be unable to differentiate between \emph{all} concept-related word pairs for any given sentential context 
%on average across sentential contexts, i.e., we look beyond the choice between a pair of words by formalizing the expectation that erasure should result in an inability to distinguish between concept-related pairs generally.

%In terms of evaluating the ability of linear concept erasure methods to remove a concept subspace that is useful to the model, our work builds on both: we rely on behavioral datasets to learn our projection matrices, but we \looseness=-1 

%Amnesic probing establishes a baseline for loss of performance on the task by removing the same number of dimensions returned by INLP, but using a random projection matrix. The idea behind this baseline is that if the concept encoding found by INLP reduces performance more than the baseline, then it must have been relevant to the main task. 

% Evaluation of concept erasure intervention
%% the erasure bit
%Concept erasure interventions have been evaluated in a number of different ways, depending on the task. The most common task is simply removing information from neural representations, without then plugging these back into the language model. For this task, successful erasure is defined via the accurary of a linear diagnostic probe in recovering the concept from the modified representation. 

% the preservation bit
Previous work is also interested in measuring the degree to which erasure preserves non-concept related features. 
% The first approach \citep{ravfogel-etal-2020-null, ravfogel2022linear, ravfogelkernel2022} is to run SimLex-999 \citep{hill2015-simlex}, a word similarity test that checks that the model's understanding of words hasn't changed significantly after the intervention. 
%\citet{ravfogel-etal-2020-null, ravfogel2022linear, ravfogelkernel2022} evaluate whether the model's understanding of word similarity is affected by erasure using SimLex-999 \citep{hill2015-simlex}. 
%\citet{ravfogelkernel2022} also check that the ability of a main-task classifier on an orthogonal task is not seriously harmed after erasure. % (though we note that a new classifier is trained on the representations \emph{after} intervention). 
\citet{ravfogel-etal-2020-null, ravfogel2022linear, ravfogelkernel2022} perform various tests, e.g., evaluating whether the model's understanding of word similarity is affected by erasure using SimLex-999 \citep{hill2015-simlex}, which have little to do with language modeling. 
%Both of these tests have little to do with language modeling, which is our primary concern. % language model $\plm$, which is our primary concern. 
\citet{elazar-etal-2021-amnesic} assess damage to $\plm$ via two different tests. 
First, they attempt to recover task performance after concept erasure by finetuning the language model on gold annotations for the concept. 
%Results show that this procedure can actually increase task performance relative to the model before erasure. effectively yields another language model, they fine-tune a language model after erasure with gold annotations for the concept, assuming that if performance on the task recovers fully, then no other information was removed by the intervention. %In turn, they posit that an incomplete recovery suggests other information was removed. 
Fine-tuning results in the paper show an increase in task performance, which suggests that further training can improve the model overall, casting doubt on the validity of performance recovery as an evaluation criterion. 
Second, the authors also report the overall KL divergence in the LM's output distribution, over the entire vocabulary. This last approach was a source of inspiration for our work, which delves much deeper into this distributional distance idea via our stability and containment tests.\looseness=-1

%Lastly, with regards to the application of information theory to probing, and specifically the idea of mutual information between concept-valued random variables and model representations, we drew inspiration from \citet{pimentel-etal-2020-information} and \citet{voita-titov-2020-information}.\looseness=-1




%Probing classifiers have played an important role in these methods, serving to identify the concept subspace \citep{ravfogel-etal-2020-null}, as the adversary attempting to recover concept information \citep{ravfogel2022linear}, and as the arbiter of whether concept information has been successfully removed \citep{ravfogel-etal-2020-null, ravfogel2022linear}. 
%Recent work has called into question whether probing classifiers can reliably measure the amount of information encoded about a concept in a representation \citep{hewitt-liang-2019-designing}, and whether they are able to identify subspaces used by the model \cite{ravichander-etal-2021-probing}. Specifically in the context of concept erasure, \citet{kumar2022probing} claim that the tendency of probing classifiers to pick up on highly correlated non-concept features results in imprecise interventions that remove non-concept features, or fail to completely remove concept features. \clem{point of this paragraph is to make the point that success of these approaches is not a given/it's a hard problem. perhaps a good chunk of this is better suited to related work}



%Probing for linguistic properties

%Finally, there is an extensive list of works on probing for linguistic properties in language models that we drew inspiration from %\citep{}.   


\section{Conclusion}

In this paper, we set out to define an \emph{intrinsic} measure of information in a subspace of a language model's representation space. 
In light of the correlational failure mode of linear concept erasure methods \cite{kumar2022probing}, doing so requires a counterfactual approach:
By assuming statistical independence between the components of a representation in the concept subspace and its orthogonal complement, we are able to correctly measure information in a subspace by marginalizing out the remainder of the space.
To the extent that a causal concept subspace exists for a particular concept and model, erasure under this metric is optimized by that subspace.
In practice, we did not actually optimize this metric. 
%This is a complex task that we leave to future work---computing it involves a double nested sum over countably infinite vector spaces.
Our theoretical analysis, combined with the efficacy of linear erasure methods using a correlational objective, suggests a tantalizing prospect: That a counterfactual objective could identify a one-dimensional causal subspace containing \emph{all} information about the concept empirically.

%\section*{Acknowledgements}
%The last author, Ryan Cotterell, acknowledges support from
%the Swiss National Science Foundation (SNSF) as
%part of the ``The Forgotten Role of Inductive Bias
%in Interpretability'' project.
%Anej Svete is supported by the ETH AI Center Doctoral Fellowship.
%The author list acknowledges Vil{\'e}m Zouhar, Lucas Torroba Hennigen, Afra Amini, Naomi Saphra, Tiago Pimentel, Clara Meister, Shauli Ravfogel and Ethan Gotlieb Wilcox for earlier discussions and feedback on the manuscript at various stages.
%Any errors in the work are exclusively those of the authors. 

% Entries for the entire Anthology, followed by custom entries
\bibliography{custom}
\bibliographystyle{acl_natbib}

\clearpage
\onecolumn

\appendix

\section{Proof of \Cref{prop:pleasantdecomposition}} \label{app:pleasantdecomposition}
\pleasantdecomposition*
\begin{proof}
On the left-hand side,
\begin{subequations}
\begin{align}  \MIq(\rvC; \rvH)  + \varepsilon &\ge \MIq(\rvC; \rvHconcept) + \varepsilon 
 & \justify{data-processing inequality} \\
% =& \MI\left(\rvC; \left(\bPk \rvH, (\eyeminusPk)\rvH \right)\right) \\
&\ge \MIq(\rvC; \rvHconcept) + \MIq(\rvC; \rvHerase)  
 & \justify{$\bPk$ is an $\varepsilon$-eraser} \label{eq:pleasantdecomposition-lhs}
\end{align}
\end{subequations}
On the right-hand side,
\begin{subequations}
\begin{align}
\MIq(\rvC; \rvH) + \varepsilon &\le  \MIq(\rvC; \rvHconcept) +  2 \varepsilon&  \justify{$(\eyeminusPk)$ is an $\varepsilon$-encapsulator}\\
&\le \MIq(\rvC; \rvHconcept) + \MIq(\rvC; \rvHerase) + 2\varepsilon 
 & \justify{non-negativity of MI} \label{eq:pleasantdecomposition-rhs}
\end{align}
\end{subequations}
Combining \cref{eq:pleasantdecomposition-lhs} and \cref{eq:pleasantdecomposition-rhs}, we have
\begin{subequations} \label{eq:c-h-decomposition-sandwich}
\begin{align}
    \MIq(\rvC; \rvHconcept) + \MIq(\rvC; \rvHerase) &\le \MIq(\rvC; \rvH) + \varepsilon \\
    &\le \MIq(\rvC; \rvHconcept) + \MIq(\rvC; \rvHerase) + 2\varepsilon  
\end{align}
\end{subequations}
Taking $\varepsilon \to 0$ in \cref{eq:c-h-decomposition-sandwich}, we have \cref{eq:pleasantdecomposition}
\begin{subequations}
\begin{align}
     \MIq(\rvC; \rvH) = \MIq(\rvC; \rvHconcept) + \MIq(\rvC; \rvHerase) \nonumber
\end{align}
\end{subequations}
\end{proof}


%\section{Proof of \Cref{prop:decomposition-mi-x}} \label{app:decomposition-mi-x}
%\propdecomposition

%\begin{proof}
%On the left-hand side,
%\begin{subequations}
%\begin{align} 
% \MIq(\rvX; \rvH \mid \rvC)  + \varepsilon \ge& \MIq(\rvX;\rvHconcept \mid \rvC) + \varepsilon
% & \justify{data-processing inequality}  \\
%\ge& \MIq(\rvX; \rvHconcept \mid \rvC) + \MIq(\rvX; \rvHerase \mid \rvC)  
% & \justify{$\bPk$ is $\varepsilon$-contained} \label{eq:decomposition-x-lhs}
%\end{align}
%\end{subequations}
%On the right-hand side,
%\begin{subequations}
%\begin{align}
%\MIq(\rvX; \rvH \mid \rvC) + \varepsilon &\le \MIq(\rvX; \rvHconcept \mid \rvC) +  2 \varepsilon & \justify{$\bPk$ is an $\varepsilon$-encapsulator}\\
%&\le \MIq(\rvX; \rvHconcept \mid \rvC) + \MIq(\rvX; \rvHerase \mid \rvC) + 2\varepsilon  & \justify{non-negativity of MI} \label{eq:decomposition-x-rhs}
%\end{align}
%\end{subequations}
%Combining \cref{eq:decomposition-x-lhs} and \cref{eq:decomposition-x-rhs}, we have
%\begin{subequations} \label{eq:x-h-decomposition-sandwich}
%    \begin{align}
%    \MIq(\rvX; \rvHconcept \mid \rvC) + \MIq(\rvX; \rvHerase \mid \rvC)  &\le \MIq(\rvX; \rvH \mid \rvC) + \varepsilon \\
%    &\le \MIq(\rvX; \rvHconcept \mid \rvC) + \MIq(\rvX; \rvHerase \mid \rvC) + 2\varepsilon 
%\end{align}
%\end{subequations}
%Taking $\varepsilon \to 0$ in \cref{eq:x-h-decomposition-sandwich}, we have \cref{eq:pleasantdecomposition-x}
%\begin{subequations}
%\begin{align}
%     \MIq(\rvX; \rvH \mid \rvC) = \MIq(\rvX; \rvHconcept \mid \rvC) + \MIq(\rvX; \rvHerase \mid \rvC) \nonumber
%\end{align}
%\end{subequations}
%\end{proof}

\section{Proof of \Cref{thm:graph}}\label{app:graph}
\bigboy*


\begin{proof}
Given the factorization in \cref{fig:causal-graph-b}, we derive the following equation using the independence assumptions given in \cref{fig:causal-graph-b}:
\begin{subequations}
\begin{align}
\pdo(\word, \bhbot, \bhpar, \concept) 
&=p(\word \mid \bhbot, \bhpar)\,p(\bhbot \mid \mathrm{do}\left(\rvC = \concept\right))\,\pdo(\bhpar \mid \mathrm{do}\left(\rvC = \concept\right))\,p(\concept) &  \\
&= p(\word \mid \bhbot, \bhpar)
p(\bhbot)\,\pdo(\bhpar \mid \concept)\,p(\concept)  \label{eq:do-joint-derivation}
\end{align}
\end{subequations}
% & \justify{$\rvHerase \perp \rvC $} \nonumber


\paragraph{Erasure.}

Given \cref{eq:do-joint-derivation}, we have the following joint distribution
\begin{subequations}
\begin{align}
\pdo(\concept, \bhbot) &= \sum_{\bhpar \in \repspacepar} \sum_{\word \in \alphabet} \pdo(\word, \bhbot, \bhpar, \concept)   \\
&= \sum_{\bhpar \in \repspacepar} \sum_{\word \in \alphabet} p(\word \mid \bhbot, \bhpar)
p(\bhbot)\,\pdo(\bhpar \mid \concept)\,p(\concept) \\
&= \sum_{\bhpar \in \repspacepar} \underbrace{\left(\sum_{\word \in \alphabet} p(\word \mid \bhbot, \bhpar) \right)}_{=1}
p(\bhbot)\,\pdo(\bhpar \mid \concept)\,p(\concept)  \\
&= \underbrace{\left(\sum_{\bhpar \in \repspacepar} 
\pdo(\bhpar \mid \concept) \right)}_{=1}\,p(\bhbot)\,p(\concept) &  \\
&= p(\bhbot)\,p(\concept) & \label{eq:do-bot-c-independence}
\end{align}
\end{subequations}
The mutual information $\MI(\rvC ; \rvHerase)$ can be computed as follows 
\begin{subequations}
\begin{align}
    \MI(\rvC ; \rvHerase) &= \sum_{\concept \in \concepts}\sum_{\bhbot \in \repspacebot} \pdo(\concept, \bhbot) \log \frac{\pdo(\concept, \bhbot)}{p(\concept) p(\bhbot)} & \\
    &= \sum_{\concept \in \concepts}\sum_{\bhbot \in \repspacebot} \pdo(\concept, \bhbot) \log \frac{p(\bhbot)\,p(\concept)}{p(\concept) p(\bhbot)} & \justify{applying \cref{eq:do-bot-c-independence}}  \\
    &= 0 < \varepsilon \label{eq:graph-erasure-proof} &
\end{align}
\end{subequations}
for every $\varepsilon > 0$.



\paragraph{Encapsulation.}
The following equation holds given \cref{eq:do-joint-derivation}
\begin{subequations}
\begin{align}
     \MI(\rvC; \rvH) - \MI(\rvC; \rvHconcept) &= \MI(\rvC; \rvHconcept,\rvHerase ) - \MI(\rvC; \rvHconcept) &  \justify{$\rvH = \rvHerase, \rvHconcept$} \\
    &= \MI(\rvC; \rvHerase \mid \rvHconcept ) &  \\
    &= \MI(\rvC; \rvHerase ) & \justify{$\rvHerase, \rvHconcept$ are independent (\cref{sec:counterfactual})}  \\
    &= 0 < \varepsilon & \justify{applying \cref{eq:graph-erasure-proof}}   \\
\end{align}
\end{subequations}


\paragraph{Containment.}
The following joint distribution can be derived from \cref{eq:do-joint-derivation}
\begin{subequations}
\begin{align}
 \pdo(& \word, \bhpar, \concept = \concept)  
=  \sum_{\bhbot \in \repspacebot} \pdo(\word, \bhbot, \bhpar, \concept = \concept)  & \\
=&  \sum_{\bhbot \in \repspacebot} p(\word \mid \bhbot, \bhpar)
p(\bhbot)\,\pdo(\bhpar \mid \concept = \concept)\,p(\concept = \concept)  & \\
=&  \sum_{\bhbot \in \repspacebot} \frac{p(\word, \bhbot, \bhpar)}{p(\bhbot, \bhpar)}
p(\bhbot)\,\pdo(\bhpar \mid \concept = \concept)\,p(\concept = \concept)  & \\
=& \sum_{\bhbot \in \repspacebot} \frac{p(\word, \bhbot, \bhpar)}{\cancel{p(\bhbot)}\,p(\bhpar)} \cancel{p(\bhbot)} \,\pdo(\bhpar \mid \concept = \concept)\,p(\concept = \concept)  & \justify{$\rvHerase, \rvHconcept$ are independent (\cref{sec:counterfactual})}  \\
=& \underbrace{\sum_{\bhbot \in \repspacebot} p(\word,\bhbot \mid \bhpar)}_{=p(\word \mid \bhpar)} \,\pdo(\bhpar \mid \concept = \concept)\,p(\concept = \concept)  &  \\
=& \, p(\word \mid \bhpar) \,\pdo(\bhpar \mid \concept = \concept)\,p(\concept = \concept)  &  \\
=& \, p(\word \mid \bhpar, \concept=\concept) \,p(\bhpar \mid \concept=\concept)  &  \justify{$\rvHconcept$ is deterministic given $\rvC$ } \label{eq:do-par-x-independence}
\end{align}
\end{subequations}
The mutual information $\MI(\rvX; \rvHconcept \mid \rvC = \concept)$ can be computed as follows 
\begin{subequations}
\begin{align}
&    \MI(\rvX; \rvHconcept \mid \rvC = \concept) \\
    &= \sum_{\word \in \alphabet} \sum_{\bhpar \in \repspacepar} \pdo(\word, \bhpar,\concept=\concept) \log \frac{\pdo(\word, \bhpar,\concept=\concept)}{p(\word \mid \concept=\concept)  p(\bhpar|\concept=\concept)} &  \\
    &= \sum_{\word \in \alphabet} \sum_{\bhpar \in \repspacepar} \pdo(\word, \bhpar,\concept=\concept) \log \frac{p(\word \mid \bhpar, \concept=\concept) \,p(\bhpar \mid \concept=\concept)}{p(\word \mid \concept=\concept) \,p(\bhpar \mid \concept=\concept)} & \justify{applying \cref{eq:do-par-x-independence}}  \\
    &= \sum_{\word \in \alphabet} \sum_{\bhpar \in \repspacepar} \pdo(\word, \bhpar,\concept=\concept) \log \frac{p(\word \mid \bhpar, \concept=\concept) \,p(\bhpar \mid \concept=\concept)}{p(\word \mid \bhpar , \concept=\concept) \,p(\bhpar \mid \concept=\concept)} & \justify{$\rvHconcept$ is deterministic given $\rvC$}  \\
    &= 0 < \varepsilon  & \label{eq:graph-containment-proof}
\end{align}
\end{subequations}


\paragraph{Stability.}
The following equation holds given \cref{eq:do-joint-derivation}
\begin{subequations}
\begin{align}
    & \MI( \rvX; \rvH \mid \rvC = \concept) - \MI(\rvX; \rvHerase \mid \rvC = \concept) & \\ = 
    &\MI(\rvX; \rvHerase, \rvHconcept \mid \rvC = \concept) - \MI(\rvX; \rvHerase \mid \rvC = \concept) & \justify{$\rvH = (\rvHerase, \rvHconcept)$}  \\
    = &\MI(\rvX; \rvHconcept \mid \rvHerase, \rvC = \concept) & \justify{conditional mutual information}  \\
    = &\MI(\rvX; \rvHconcept \mid \rvC = \concept) & \justify{$\rvHerase, \rvHconcept$ are independent (\cref{sec:counterfactual})}  \\
    = &0 < \varepsilon & \justify{applying \cref{eq:graph-containment-proof}}  
\end{align}
\end{subequations}

\end{proof}

% \section{Proof of \Cref{thm:bigboy}} \label{app:bigboyproof}
% % \bigboy*
% \begin{proof}
% First, we show $\rvbX \ci \rvHconcept \mid \rvC$.
% To do so, we argue the conditional mutual information $\MI(\rvbX ; \rvHconcept \mid \rvC) = 0$.
% This proceeds by the following manipulation
% \begin{subequations}
% \begin{align}
% \MI(\rvbX ; \rvHconcept \mid \rvC) &= \MI(\rvH;  \rvHconcept\mid \rvC) &  \justify{bijection} \\
% &= \MI(\rvHerase, \rvHconcept ; \rvHconcept\mid \rvC) & \justify{expand $\rvH$} \\
% &= \ent(\rvHerase, \rvHconcept \mid \rvC) - 
% \ent(\rvHerase, \rvHconcept \mid \rvHconcept, \rvC) & \justify{definition of MI} \\
% &= \ent(\rvHerase, \rvHconcept \mid \rvC) - 
% \ent(\rvHerase \mid \rvHconcept, \rvC) \\
% &= \ent(\rvHconcept \mid \rvC) 
% + \ent(\rvHerase \mid \rvHconcept, \rvC) - 
% \ent(\rvHerase \mid \rvHconcept, \rvC) & \justify{chain rule} \\
% &= \ent(\rvHconcept \mid \rvC)  \\
% &= 0 & \justify{determinism \tianyuchange{I think this line is wrong. $\rvHconcept$ isn't deterministic on $\rvC$ anymore}}\label{eq:bigboy-final}
% \end{align}
% \end{subequations}
% Now we simply note that $\rvC \ci \rvHerase \mid \rvbX$ follows from $\varepsilon$-erasure, which states $\MI(\rvC ; \rvHerase \mid \rvbX) < \varepsilon$. 
% As $\varepsilon \rightarrow 0$, we have the result.
% %To do so, we argue the conditional mutual information $\MI(\rvC ; \rvHerase \mid \rvbX)  = 0$.
% %This proceeds by the following manipulation
% %\begin{subequations}
% %\begin{align}
% %\MI(\rvC ; \rvHerase \mid \rvbX) &= \MI(\rvC ; \rvHerase \mid \rvH) & \justify{bijection}\\
% %&= \MI(\rvC ; \rvHerase) - \MI(\rvC ; \rvH) + \MI(\rvC; \rvH \mid \rvHerase) & \justify{chain rule}\\
% %&= \MI(\rvC ; \rvHerase) - \MI(\rvC ; \rvH) + \ent(\rvC \mid \rvHerase) - \ent(\rvC \mid \rvH) & \justify{definition of MI}\\
% %&\leq \MI(\rvC ; \rvHerase) - \MI(\rvC ; \rvH) + \ent(\rvC) - \ent(\rvC \mid \rvH) & \justify{cond. reduces entropy}\\
% %&= \MI(\rvC ; \rvHerase) - \MI(\rvC ; \rvH) + \MI(\rvC ; \rvH)& \justify{definition of MI}\\
% %&= \MI(\rvC ; \rvHerase)  
% %\end{align}
% %\end{subequations}
% %Now, $\MI(\rvC ; \rvHerase) < \varepsilon$ by erasure and $\varepsilon \rightarrow 0$, we have $\MI(\rvC ; \rvHerase \mid \rvbX) \rightarrow 0$ as desired.
% %\Ryan{This needs to be its own Lemma}
% %To see why the last step follows from encapsulation, note that\ryan{We need another lemma to show why $\ent(\rvC) =  \MI(\rvC ; \rvH)$ follows from there being a bijection between $\kleene{\alphabet}$ and $\repspace$. This basically means that there is no information loss about the concept in the representations.}
% %\begin{equation}
% % \ent(\rvC) =  \MI(\rvC ; \rvH) =  \MI(\rvC ; \rvHconcept) = \ent(\rvC) - \ent(\rvC \mid \rvHconcept)
% %\end{equation}
% %Thus, $0 =  - \ent(\rvC \mid \rvHconcept)$
% %which implies $\ent(\rvC \mid \rvHconcept) = 0$.

% \end{proof}

\end{document}
