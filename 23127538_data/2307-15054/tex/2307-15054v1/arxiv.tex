\pdfoutput=1


\documentclass[11pt]{article}
\usepackage[dvipsnames]{xcolor}

\usepackage[acceptedWithA]{tacl2018v2}

\usepackage{times}
\usepackage{latexsym}
\usepackage{xfrac}

\usepackage[font=small]{caption}

\usepackage{tikz}
\usetikzlibrary{bayesnet}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{booktabs}

\usepackage{rycolab}
\usepackage{subspaceprobing}

\DeclareMathOperator*{\expect}{\mathbb{E}}



\setlength\titlebox{4.0cm}


\title{A Geometric Notion of Causal Probing}



\author{Cl{\'e}ment Guerner\quad Anej Svete\quad Tianyu Liu \quad Alexander Warstadt \quad Ryan Cotterell \\
\setlength{\fboxsep}{2.5pt}\setlength{\fboxrule}{2.5pt}\fcolorbox{white}{white}{
    $\{$\texttt{\href{mailto:cguerner@student.ethz.ch}{cguerner}}, \texttt{\href{mailto:anej.svete@inf.ethz.ch}{anej.svete}, }\texttt{\href{mailto:tianyu.liu@inf.ethz.ch}{tianyu.liu},
    }\texttt{\href{mailto:awarstadt@inf.ethz.ch}{awarstadt},
    }\texttt{\href{mailto:ryan.cotterell@inf.ethz.ch}{ryan.cotterell}}$\}$\texttt{@inf.ethz.ch}
} \\
    {\setlength{\fboxsep}{2.5pt}\setlength{\fboxrule}{2.5pt}\fcolorbox{white}{white}{
    % Figure removed
}
}}

\begin{document}
\maketitle

\begin{abstract}
Large language models rely on real-valued representations of text to make their predictions.
These representations contain information learned from the data that the model has trained on, including knowledge of linguistic properties and forms of demographic bias, e.g., based on gender.
A growing body of work has considered information about concepts such as these using orthogonal projections onto subspaces of the representation space.
We contribute to this body of work by proposing a formal definition of \emph{intrinsic} information in a subspace of a language model's representation space.
We propose a counterfactual approach that avoids the failure mode of spurious correlations \citep{kumar2022probing} by treating components in the subspace and its orthogonal complement independently.
We show that our counterfactual notion of information in a subspace is optimizing by an \emph{causal} concept subspace.
Furthermore, this intervention allows us to attempt concept controlled generation by manipulating the value of the conceptual component of a representation.
Empirically, we find that R-LACE \citep{ravfogel2022linear} returns a one-dimensional subspace containing roughly half of total concept information under our framework. 
Our causal controlled intervention shows that, for at least one model, the subspace returned by R-LACE can be used to manipulate the concept value of the generated word with precision.
\end{abstract}



\section{Introduction}







Autoregressive large language models (LLMs) generate representations of words and their textual contexts to perform next word prediction.
Given their ability to generate naturalistic text, it is reasonable to assume that the representations induced by language models encode some notion of world knowledge.
For instance, we may suspect that a language model contains knowledge about the concept of a \conceptf{bird}\footnote{Throughout the text, we will use a distinguished typesetting to refer to concepts. For instance, the concept of a bird is written as \conceptf{bird}.} if it, when prompted, is able to generate a plausible story about birds flying, making nests, laying eggs, etc.
Similarly, it seems reasonable to assume that a language model has some knowledge of linguistic concepts such as \conceptf{verbal-number} since almost all sentences generated by the model are syntactically correct, e.g., in generated text in English verbs agree with their subjects in number.
This paper assumes that language models deal in underlying concepts. 
We make this assumption to see whether it can help us better understand how LLMs make predictions, and how these predictions can be controlled. 
This assumption has two implications, which we explore in this paper. 
First, that the model has the ability to associate a textual context with a concept, and that this association impacts the causal generative process of the language model.
Second, that there exists a geometric encoding of concept knowledge in a language model's representation space.\footnote{For now, we define a representation space simply as the $d$-dimensional vector space that a language model relies on to encode text. We propose a more formal definition in \cref{sec:lmsconcepts}.}
Studying these requires nailing down an operational definition of a concept so mathematically precise as to be able to be tested, which is a non-trivial endeavor.\looseness=-1



The reliance of language models on concepts to make predictions, especially linguistic concepts such as \conceptf{verbal-number}, is a well-studied phenomenon \citep{ravfogel-etal-2021-counterfactual,lasri-etal-2022-probing, amini-etal-2023-causal}. Early methods examined this question using a  forced-choice experimental setup, by for example testing whether a language model has correctly learned the concept of \conceptf{verbal-number} by asking it to choose between a grammatical and an ungrammatical variant of a sentence \cite{linzen2016assessing, marvin-linzen-2018-targeted, goldberg2019assessing, lasri-etal-2022-probing}. Consider, for example, the sentences:
\ex. \a. \label{ex:gramm} \textit{The kids walk to the park.} \\ 
        {\small the.\textsc{PL} kids.\textsc{PL} walk.\textsc{VBP} to the.\textsc{SG} park.\textsc{SG}}
     \b. \label{ex:ungramm} $^*$\textit{The kids walk\textbf{s} to the park.} \\
        {\small the.\textsc{PL} kids.\textsc{PL} walks.\textsc{VBZ} to the.\textsc{SG} park.\textsc{SG}}

In the case of \conceptf{verbal-number}, \citet{goldberg2019assessing} shows that when the model is forced to choose between two such variants it has nearly perfect accuracy. In this paper, we seek to go beyond extrinsic measurement of whether a language model is capable of choosing between \cref{ex:gramm} and \cref{ex:ungramm}.\looseness=-1


Our primary contribution is to construct a geometric notion of what it means for a neural language model's representation space to have information about a concept. 
We build on the proposal, given by \citet{bolukbasi2016} and \citet{ravfogel2022linear}, who argue that concepts are naturally operationalized by \emph{linear} subspaces. 
Linear subspaces are a convenient operationalization because they lend themselves to tractable algorithms, and they are inherently equipped with a basic notion of interpretability, in that they represent a concept geometrically.
Existing work \citep{lasri-etal-2022-probing, ravfogel-etal-2023-linear} has relied on $\mathcal{V}$-information \cite{xu2020theory} to quantify the amount of information in the representation space of a language model, before and after concept erasure. 
This measure is \emph{extrinsic} to the language model, in the sense that it measures information by asking, for a particular variational family $\mathcal{V}$ of classifiers, how well the concept can be predicted by these classifiers.
In contrast, we propose an information-theoretic \citep{shannon1948mathematical} definition of information, which is \emph{intrinsic} to the language model, by which we mean that information is quantified using distributions induced from the language model, i.e., without relying on an additional classifier.\looseness=-1

We show, via a counterexample, that a naÃ¯ve approach to measuring intrinsic information falls victim to spurious correlations \citep{kumar-etal-2022-gradient}. 
We then explain how to construct a counterfactual distribution that avoids this pitfall. 
Specifically, we perform an intervention that results in the concept subspace and its orthogonal complement being independent, which allows us to compute information contained in either subspace while marginalizing out the other.
This approach is counterfactual in the sense that it creates representations that would not otherwise occur under the language model.
Crucially, it allows us to talk about the mutual information between linear subspaces and concepts.\looseness=-1

 Making use of our counterfactual framework, we derive four properties that characterize a precise geometric encoding of a concept.
First, \textbf{erasure} is the condition that the orthogonal complement of the isolated subspace should contain \emph{no} information about the concept. 
Second, the related property of \textbf{encapsulation} states that projecting a representation onto our concept subspace should preserve \emph{all} the information about the concept.
Third, \textbf{stability} quantifies the requirement that projection onto the orthogonal complement of our concept subspace should preserve non-concept information.
Finally, \textbf{containment} ensures that the concept subspace does not contain additional information beyond the concept.\looseness=-1

Empirically, we study \conceptf{verbal-number} in English and \conceptf{grammatical-gender} in French. We find, for both concepts, that R-LACE \citep{ravfogel2022linear} yields a one-dimensional concept subspace which, according to our novel counterfactual metrics, contains a large share of concept information while leaving non-concept information untouched.
We then leverage our intrinsic measure of information to posit a causal graphical model by which a latent concept may govern LLM text generation.
Our graphical model and our independence assumption enable us to derive a causal controlled generation method by manipulating the conceptual component of a representation.
And, indeed, we find evidence that it is possible to use a one-dimensional subspace to control the generation behavior of the language model with respect to \conceptf{verbal-number}.




















\section{Concepts and Information} \label{sec:lmsconcepts} 

We begin by formally defining a language model, and what it could mean for a language model to encode a concept in its representation space.
We take an information-theoretic approach.
Specifically, we claim that, if a concept is encoded in a language model's representation space, then we should see high mutual information between its representations and the concept of interest.
\looseness=-1

\subsection{Language Modeling Basics} \label{sec:lms}
A language model is a probability distribution $\plm$ over $\kleene{\alphabet}$, the Kleene closure over an alphabet\footnote{An alphabet is a finite, non-empty set.} $\alphabet$.
We further define $\eosalphabet \defeq \alphabet \cup \{\eos\}$, with $\eos \notin \alphabet$ being a distinguished end-of-string symbol. 
Without loss of generality, we parameterize $\plm$ in an autoregressive manner \citep{du-etal-2023-measure} as follows: 
\begin{equation}\label{eq:lm}
\plm(\str) = \plm(\eos \mid \str) \prod_{t=1}^{T} \plm(\wordt \mid \cxt)
\end{equation}
where $\wordt \in \alphabet$ refers to $t^{\text{th}}$ word\footnote{We refer to $\word \in \alphabet$ as words for simplicity, even though in the case of neural language models, these are often called subwords, tokens, or symbols.} in a string $\str \in \kleene{\alphabet}$, where $\cxt$ represents the first $(t-1)$ words of $\str$.
 

Many language models make use of contextual representations---specifically, they typically encode a textual context $\cxt$ as a real-valued column vector $\cxtenc \in \Rd$.
Generally, $\cxtenc$ is deterministically computed from the context string $\cxt$, for example by a neural network.
The contextual representations $\cxtenc \in \Rd$ are integrated into the language model's parameterization. 
And, in this case, $\plm(\wordt \mid \cxt)$ is often parameterized
using a softmax as follows
\begin{equation}\label{eq:softmax}
\!\!\!\plm(\word \mid \cxtenc) \!\defeq\! \frac{\exp \left( \bv(\word) \cdot \cxtenc \right)}{\sum_{\word' \in \eosalphabet} \exp \left( \bv(\word') \cdot \cxtenc \right)}
\end{equation}
where $\bv(\word) \in \Rd$, a column vector, is a word-specific parameter vector.\footnote{We have omitted the bias term without loss of generality as we can always pad the contextual representations by a single dimension.}
Under such a model, the representation space of \cref{eq:softmax} is defined as
\begin{equation}
\repspace \defeq \Big\{ \bh(\str) \mid \str \in \kleene{\alphabet} \Big\} \subset \Rd
\end{equation}
where $\cxtenc$ is a deterministic function of $\cxt$.\footnote{We relax this assumption later on, such that $\cxtenc$ can be stochastic given $\cxt$. One example of a language model with stochastic contextual embeddings is \citet{bowman-etal-2016-generating}.\looseness=-1}
We remark that the cardinality of $\repspace$ is \emph{countably} infinite despite its elements being real vectors.
This is due to the fact that there exists exactly one representation for every string in $\kleene{\alphabet}$, which is, itself, countably infinite.
Since $\repspace$ is countable, summing over it is discrete and does not require integration.

\subsection{Language Models and Concepts} \label{sec:concepts}

We now discuss an exact sense in which a language model can be said to encode a concept. 
First, we define a concept based on the possible values it can take.
We formalize this with a \defn{concept set}, a finite, non-empty set $\concepts$ whose elements are those values.\footnote{
In principle, the concept sets could be infinite if additional care is taken in the formal development.}
If we consider the concept set corresponding to \conceptf{verbal-number}, it is natural to include three values: \conceptval{sg} (e.g., \wordf{walks}), \conceptval{pl} (e.g., \wordf{walk}), and \conceptval{n/a} (e.g., \wordf{consternation}). 
For various reasons, including syncretism \citep{baerman2007syncretism}, some verbs in English can have ambiguous concept value depending on context.
For example, the \wordf{you} in the sentence ``\wordf{You walked to the store.}'' can be \conceptval{sg} or \conceptval{pl}.
We find similar facts for other concepts in different languages. 
For instance, for \conceptf{grammatical-gender} in French, the adjective \wordf{marron} can be both \conceptval{fem} and \conceptval{msc}.\looseness=-1

To relate language models to concept sets, we introduce a deterministic probability distribution $\iota(\concept \mid \cxt, \word)$. 
$\iota$ tells us the probability that, in the sequential context $\cxt \in \kleene{\alphabet}$, word $\word \in \alphabet$ is annotated with the concept value $\concept \in \concepts$.
We assume that $\iota$ is deterministic, i.e., we have $\iota(\concept \mid \cxt, \word) \in \{0, 1\}$ for all $\concept \in \concepts$, $\word \in \alphabet$, and $\cxt \in \kleene{\alphabet}$.
We acknowledge that this is an oversimplification---indeed, we just exhibited an example where a verb had ambiguous \conceptf{verbal-number} given full sentential context. 
Nevertheless, this assumption will prove useful in the experimental portion of our paper.
Moreover, we relax the assumption in \cref{sec:causal} by proposing a stochastic operationalization of concepts.\looseness=-1

To illustrate this formalism, consider the concept \conceptf{verbal-number} and \cref{ex:gramm,ex:ungramm}. The concept set for \conceptf{verbal-number} is $\concepts = \{\conceptval{sg}, \conceptval{pl}, \conceptval{n/a}\}$, and $\iota$ maps as follows:
\begin{itemize}
\item $\iota(\conceptval{sg} \mid \wordf{The kids}, \wordf{walk}) = 0$
\item $\iota(\conceptval{pl} \mid \wordf{The kids}, \wordf{walk}) = 1$
\item $\iota(\conceptval{n/a} \mid \wordf{The kids}, \wordf{walk}) = 0$
\item $\iota(\conceptval{sg} \mid \wordf{The kids}, \wordf{walks}) = 0$
\item $\iota(\conceptval{n/a} \mid \wordf{The kids}, \wordf{consternation}) = 1$
\end{itemize}
We now use $\iota$ to construct a joint distribution over strings and concept sequences as follows:
\begin{align} \label{eq:string-concept-joint}
\pconcept&(\conceptseq , \str)\defeq  \\
&p(\eos \mid \str) \prod_{t=1}^{T} \plm(\wordt \mid \cxt) \iota(\ct \mid \cxt, \wordt) \nonumber
\end{align}
where $\conceptseq \in \concepts^T$.
Where does $\pconcept(\conceptseq, \str)$ get us?
Our ultimate goal is to talk about the information shared between a concept, with values $\concepts$, and representations $\repspace$, and discussing information in a mathematical manner requires probability.\looseness=-1


\subsection{Unigram Information}\label{sec:reps-information}

To construct a mutual information between the model's notion of a concept and its contextual representations, we require a joint distribution between a concept-valued random variable and a representation-valued random variable. 
Importantly, in order for our information measure to be instrinsic, we must obtain this joint distribution using our language model.
We now build our information-theoretic framework in this section, first by introducing a probability distribution over representations, and then by integrating concepts using $\iota$.\looseness=-1

We begin by defining the \defn{induced unigram} distribution of the language model. 
In words, the induced unigram distribution is a marginal distribution of the language model that tells how frequently each word $\word \in \alphabet$ appears, on average, in a string $\str \sim \plm$.
The induced unigram distribution is formally defined as 
\begin{equation} \label{eq:inducedunigram}
\punigram(\word) \defeq \sum_{\str \in \kleene{\alphabet}} \plm(\str) \frac{\sum_{t=1}^{T} \mathbbm{1}\{ \word = \wordt \}}{T} 
\end{equation}
We can further augment the induced unigram distribution to expose the language model's representations.
This leads to a joint induced unigram over both words \emph{and} representations; we define this joint distribution as
\begin{align} \label{eq:inducedunigramwith}
    &\punigram(\word, \bh) \defeq \\ 
    &\sum_{\str \in \kleene{\alphabet}} \plm(\str) \frac{\sum_{t=1}^{T} \mathbbm{1}\Big\{ \word = \wordt \land
 \bh = \cxtenc \Big\}}{T} \nonumber
\end{align}
The joint word--representation induced unigram distribution $\punigram$ tells us the frequency with which a word $\word \in \alphabet$ co-occurs with a representation $\bh \in \repspace$.
We can use this joint word--representation distribution to define the following mutual information between words and representations
\begin{equation}\label{eq:cxtmi}
\!\!\!\MI(\rvX ; \rvH) =  \sum_{\word \in \alphabet} \sum_{\bh \in \repspace} \punigram(\word, \bh) \log\! \frac{\punigram(\word, \bh) }{\punigram(\word) \punigram(\bh)}
\end{equation}
where $\rvH$ is a $\repspace$-valued random variable. \Cref{eq:cxtmi} tells us how much information, on average, a representation $\bh \in \repspace$ tells us about the identity of a word $\word \in \alphabet$.\looseness=-1

We have now established how to think probabilistically about representations jointly with \emph{words}. However, we are primarily interested in the relationships between \emph{concepts} and representations. 
Using $\iota$, we can define a concept--representation induced unigram distribution as
\begin{align} \label{eq:inducedconceptunigram}
    &\punigram(\concept, \bh) \defeq \\ 
    &\sum_{\str \in \kleene{\alphabet}} \plm (\str) \frac{\sum_{t=1}^{T}  \iota(\concept \mid \cxt, \wordt) \mathbbm{1}\Big\{\bh = \cxtenc \Big\}}{T} \nonumber
\end{align}
where the only difference, relative to \cref{eq:inducedunigramwith}, is that we count instances of concept values mapped from context strings and words via our distribution $\iota$.
We can now use our induced concept--representation unigram to compute our intrinsic measure of concept information in representations:\looseness=-1
\begin{equation} \label{eq:cxtconceptmi}
\!\!\MI(\rvC ; \rvH) =
\sum_{\concept \in \concepts}
\sum_{\bh \in \repspace} \punigram(\concept, \bh) \log \!\frac{\punigram(\concept, \bh)}{\punigram(\concept) \punigram(\bh)}
\end{equation}
where the $\alphabet$-valued random variable $\rvX$ is replaced with a $\concepts$-valued random variable $\rvC$. \cref{eq:cxtconceptmi} allows us to precisely define the \emph{information} between a concept and a representation in a rigorous way.

By relating words in $\alphabet$ to concept values in $\concepts$ with $\iota$, we have reformulated the language model as a joint concept--string model. 
From this joint model, we can derive an information-theoretic framework for evaluating how much information the model's representations contain about a concept. 
Importantly, the language model is a first class citizen in the mutual information in \cref{eq:cxtconceptmi}: Aside from $\iota$, all of the components are derived from the language model, and it is computed in expectation over samples drawn from the model's joint concept--representation unigram distribution. 
$\iota$ is the only ingredient that we have added, and it is non-parametric and rather uncontroversial.
This means we are focusing on the model's notion of a concept, and has implications for how this quantity can be computed empirically. 
We discuss this point in \cref{sec:otherdistributions}.\looseness=-1

Lastly, we define the following conditional mutual information: 
\begin{align}\label{eq:conditionalmi}
\MI(&\rvX ; \rvH \mid \rvC) = \\
&\sum_{\concept \in \concepts}
\sum_{\word \in \alphabet} \sum_{\bh \in \repspace} 
\punigram(\word, \bh, \concept) \log \frac{\punigram(\word, \bh \mid \concept) }{\punigram(\word\mid \concept) \punigram(\bh \mid \concept)} \nonumber
\end{align}
where $\punigram(\word, \bh, \concept)$ is trivially obtained by combining approaches used to derive \cref{eq:inducedunigramwith} and \cref{eq:inducedconceptunigram}.
This quantity measures, given a particular concept value $\concept \in \concepts$, how much additional information about a word $\word \in \alphabet$ is encoded in the model's representations. 
Recall that with $\MI(\rvX; \rvH)$ defined in \cref{eq:cxtmi}, we quantified information about the next word contained in the language model's representation space.
Conditioning this quantity on a particular $\rvC = \concept$ value reveals a lot about the identity of the next word. 
For example, conditioning on $\rvC = \conceptval{sg}$ for \conceptf{verbal-number} tells us that the next word $\word$ encoded by $\rvX$ is \conceptval{sg}.\footnote{We have defined the probability of a concept value $\concept$ via $\iota(\concept \mid \cxt, \word)$, i.e., as a function of textual context and a word.
Conditioning on $\rvC = \conceptvalfootnote{\scriptsize sg}$ amounts to reverse engineering---we consider only samples $(\word, \cxtenc, \concept) \sim \punigram$ such that $\rvC = \conceptvalfootnote{\scriptsize sg}$ and $\iota(\conceptvalfootnote{\scriptsize sg} \mid \cxt, \word) = 1$.}
\Cref{eq:conditionalmi} measures how much additional information, relative to the model's unigram distribution over \conceptval{sg} verbs $\punigram(\word \mid \conceptval{sg})$, a representation $\cxtenc$ gives you about the identity of the \conceptval{sg} verb to follow $\cxt$.




\subsection{Interfacing with Other Distributions} \label{sec:otherdistributions}

For an autoregressive language model like the one defined in \cref{eq:lm}, our various mutual information measures defined in \cref{eq:cxtmi,eq:cxtconceptmi,eq:conditionalmi} are taken in expectation with respect to the induced unigram $\punigram$.
This means we can only estimate these quantities by generating samples from the language model.
However, in practice, it has been noted that autoregressive language models often generate strings that are judged to be unnatural by humans \citep{holtzman2020nucleus}. 
We found our samples generated using ancestral sampling to be of poor quality, meaning that the correct concept value was extremely unclear relative to, for example, sentences like \cref{ex:gramm}.
For that reason, it may be wise to consider computing the expectation with respect to a \emph{different} language-generating process, i.e., a different distribution over strings. 
Thus, we also generalize our information-theoretic framework to handle this case.

Let $\ptilde$ be a distribution over $\kleene{\alphabet}$, which we assume we can easily draw samples from.
In the general case, $\ptilde$ will be different than $\plm$.   
Define the following induced unigram distribution
\begin{align} \label{eq:tildeinducedconceptunigram}
    &\ptildeunigram(\concept, \bh) \defeq \\ 
    &\sum_{\str \in \kleene{\alphabet}} \ptilde (\str) \frac{\sum_{t=1}^{T}  \iota(\concept \mid \cxt, \wordt) \mathbbm{1}\Big\{\bh = \cxtenc \Big\}}{T} \nonumber
\end{align}
where the expectation is taken with respect to $\ptilde$, but the hidden states $\cxtenc$ are computed according to the autoregressive neural language model $\plm$.
Using $\ptildeunigram$, we can define a concept--representation mutual information $\MI_{\ptildeunigram}(\rvC ; \rvH)$ that relativizes the language model's $\plm$ with respect to $\ptilde$.


In the experimental section \cref{sec:experimentsetup}, we consider two language generating processes other than language model $\plm$: Natural language text from several corpora, and text generated from the language model using nucleus sampling \citep{holtzman2020nucleus}.
Having established our information-theoretic foundation, and explained how to compute it, we now specify how we expect concept information to be encoded in representations, in the form of a linear concept subspace.
\looseness=-1



\section{A Geometric Encoding of Concepts} \label{sec:functionalsubspace}

In this section, we explore how concepts are geometrically encoded in a language model's representation space $\repspace$.
The \defn{linear subspace hypothesis} \cite{bolukbasi2016} postulates that there exists a \emph{linear subspace} $\linspacec \subseteq \repspace$ that contains all of the information about a concept with values $\concepts$.
This hypothesis has been tested on various linguistic concepts, including \conceptf{verbal-number} \citep{ravfogel-etal-2021-counterfactual,lasri-etal-2022-probing, amini-etal-2023-causal} and \conceptf{grammatical-gender} \citep{amini-etal-2023-causal}.
Inspired by their work, we posit a possible characterization of the representation space in which every $\bh \in \repspace$ has a conceptual component and an orthogonal, non-conceptual component.
Under that assumption, we decompose the representation space $\repspace$ into these two subspaces--specifically, two linear subspaces orthogonal to each other.
Then, using our concept--representation mutual information defined in \cref{eq:cxtconceptmi}, we quantify the amount of concept information encoded in a subspace of a language model's representation space.







Such an information-theoretic partition of the representation space feels very natural, yet it is misleading.
We show, via a counterexample, that isolating information about a concept to a subspace can be impossible without erasing information about other aspects of the context as well. 
Instead, we propose an alternative framework of quantifying the erasure of information using \emph{counterfactual} distributions over the concept and non-concept partitions, which allow us to control for the common-cause confounding effects.\looseness=-1









\subsection{Concept Partition} \label{sec:concept-partition}

We begin by reviewing a bit of linear algebra. 
A linear subspace $\linspace \subseteq \Rd$ may be characterized by an orthogonal projection matrix $\bP \in \R^{d \times d}$ that maps any vector in $\Rd$ onto $\linspace$. 
Such a projection matrix has the following properties: 
\begin{enumerate*}[label=\roman*)]
    \item symmetry, i.e., $\bP = \bP^{\top}$,
    \item idempotence, i.e., $\bP^2 = \bP$,
    \item orthogonality, i.e., $\bP \bh = \text{proj}_{\linspace}(\bh)$, where $\text{proj}_{\linspace}$ is the orthogonal projection operator.
\end{enumerate*}
We note that the rank $k$ of a projection matrix $\bPk \in \R^{d \times d}$ determines the dimensionality of the subspace that it characterizes. 
Furthermore, for such a $\bPk$, $\eye_d - \bPk$, where $\eye_d$ the $d \times d$ identity matrix, is also an orthogonal projection matrix.
It orthogonally projects onto the $(d-k)$-dimensional orthogonal complement of $\linspace$, denoted $\linspace^\bot$. In this sense, a projection matrix $\bP$ partitions a vector space into two subspaces $\linspace$ and $\linspacecomp$.\looseness=-1





Now, given a concept set $\concepts$, we define a partition of a language model's representation space $\repspace$ into a \defn{concept subspace} $\linspacec$ and its orthogonal complement, the \defn{non-concept subspace} $\linspaceccomp$. 
We refer to $\bPk \in \R^{d \times d}$ as the rank-$k$ orthogonal projection matrix that projects onto $k$-dimensional $\linspaceccomp$, i.e., $\bPk \bh = \text{proj}_{\linspaceccomp}(\bh)$.
In turn, $\eyeminusPk$ projects onto our $(d-k)$-dimensional concept subspace $\linspacec$, such that $(\eyeminusPk) \bh = \text{proj}_{\linspacec}(\bh)$.
What makes $\linspacec$ a concept subspace, and $\linspaceccomp$ a non-concept subspace?
The partition of $\repspace$ into $\linspacec$ and $\linspaceccomp$ is an \emph{information} partition.\looseness=-1

We use \cref{eq:cxtconceptmi} to define the information about the concept encoded in both. 
Consider, for example, information in $\linspaceccomp$ about $\concepts$ on average over textual contexts \begin{align} \label{eq:subspace-mi}
\MI(\rvC ;& \bPk \rvH) =
\\ &\sum_{\concept \in \concepts}
\sum_{\bh \in \repspace} 
\punigram(\concept, \bPk \bh) \log \!\frac{\punigram(\concept, \bPk \bh)}{\punigram(\concept) \punigram(\bPk \bh)} \nonumber
\end{align}
where the language model's representations are orthogonally projected onto $\linspaceccomp$ using $\bPk$. 
\cref{eq:subspace-mi} relates the \emph{geometric} notion of a linear subspace with the \emph{information-theoretic} notion of information.
Thus, if $\MI(\rvC ; \bPk \rvH)$ is low, we can say that $\bPk$ erases a lot of concept information in $\repspace$ by projecting onto the subspace $\linspaceccomp$.
On the other, if $\MI(\rvC ; \bPk \rvH)$ is high, we can say it erases little information. 
In the remainder of this paper, for brevity, we refer to $\repspacepar \defeq \{ \eyeminusPkh \mid \bh \in \repspace \}, \repspacebot \defeq \{ \bPk \mid \bh \in \repspace \}$ as random variables corresponding to contextual representations projected onto concept and non-concept subspaces, respectively.


\begin{table}[ht!]
    \centering
    \begin{minipage}[b]{1.0\columnwidth}    
        \centering
        % Figure removed
            \captionof{figure}{An example erasure of a correct \conceptf{verbal-number} subspace, when predicting the next word given ``The kids''.\looseness=-1}\label{fig:geomexample}
        \bigskip 
    \end{minipage}
    \begin{minipage}[b]{1.0\columnwidth}
        \captionsetup{type=table}
        \begin{subtable}{\columnwidth}
            \small
            \centering
            \begin{tabular}{ ccccc } 
                 \toprule
                  & \wordf{walks} & \wordf{walk} & \wordf{goes} & \wordf{go} \\
                 \midrule
                 \conceptval{sg} & 0 & 0 & 0.7 & 0  \\ 
                 \conceptval{pl} & 0 & 0.3 & 0 & 0 \\ 
                 \bottomrule
                \end{tabular}
                \caption{Joint unigram distribution of \conceptf{verbal-number} and word. The word \wordf{walk} is only used as \conceptvalfootnote{pl} and \wordf{go} only as \conceptvalfootnote{sg}.
                } \label{tab:counterexample-joint}
            \bigskip
        \end{subtable} 
        
        \begin{subtable}{\columnwidth}
        \small \centering
        \begin{tabular}{ ccc } 
            \toprule
              & \wordf{walk} & \wordf{go} \\
            \midrule
             \conceptval{sg} & 0 & 1 \\ 
             \conceptval{pl} & 1 & 0 \\ 
             \bottomrule
        \end{tabular}
            \caption{Distribution of \conceptval{\footnotesize verbal-number} given the word.} \label{tab:counterexample-condi}
        \end{subtable} 
        \caption{
            Probability distribution of the example. \label{tab:counterexample}
        }
        \vspace{-15pt}
    \end{minipage}
\end{table}

\subsection{The Perils of Correlation} \label{sec:example}


\Cref{eq:subspace-mi} suggests an attractive property we might ask from $\bPk$: It should satisfy $\MI\left(\rvC; \bPk \rvH\right) = 0$, i.e., it should completely erase the information about the concept by projecting onto $\linspaceccomp$.
However, as we show next, this is a na{\"i}ve characterization.
We illustrate the notion of a concept subspace with an example inspired by \citet{kumar-etal-2022-gradient}, as shown in \cref{fig:geomexample}.
In the example, the representation space is two-dimensional with the $y$-axis representing the correct subspace encoding the concept $\conceptf{verbal-number}$, while the $x$-axis encodes the lemma.
Together, these comprise all of the information stored in word representations $\bv(\word)$ and contextual representations $\cxtenc$.
On the left, we have the original representation space, and on the right, we have the space resulting from erasing information in our concept subspace, i.e., setting the $y$-coordinates of all vectors in the space to 0.
Such a transformation constitutes successful erasure \emph{in the sense} that it works on the correct linear subspace.\footnote{We could also say that the erasure works with the \emph{causal} subspace, in the sense that manipulating the values of that subspace would result in changing the precisely the concept encoded by the representation while leaving other aspects about the it intact.}
To the extent that such a subspace exists in reality, finding the $\bPk$ that erases this subspace seems like the correct objective.

Now, consider the hypothetical joint word--concept unigram distribution $\punigram(\word, \concept)$ in \Cref{tab:counterexample-joint}. 
Under this distribution, a projection matrix $\bPk$ that erases the correct $y$-axis as shown in \cref{fig:geomexample} is \emph{not} the minimizer of \cref{eq:subspace-mi}. 
That is because $\rvHerase$ ($x$-axis) and $\rvHconcept$ ($y$-axis) are heavily correlated while being perpendicular to each other. 
This means that $\MI(\rvC; \rvHerase) = 0.88 > 0$ in our toy example in \cref{fig:geomexample}---unless $\bPk = \mathbf{0}$. 
Requiring $\bPk$ to satisfy $\MI(\rvC; \rvHerase) = 0$ is thus not a feasible characterization for our problem---in the example above, achieving that would require removing the entire representation space.
Altogether, this simple example exposes a crucial issue with linear concept erasure: While characterizing the concept subspace-defining projection matrix with $\MI(\rvC; \rvHerase) = 0$ might seem like a sensible idea, achieving this might result in the removal of correlated information and not identify the correct (causal) subspace, even if one exists \citep{kumar-etal-2022-gradient}.\looseness=-1















\subsection{A Counterfactual Unigram Distribution} \label{sec:counterfactual}

The underlying problem with the example given in \cref{sec:example} is that $\rvHconcept$ and $\rvHerase$ have common cause that introduces a spurious correlation---the $\kleene{\alphabet}$-valued context random variable $\rvbX$.
Although this variable is not included in the example, it explains why \Cref{tab:counterexample-joint} is plausible---the word \wordf{walk} only co-occurs with \conceptval{pl} context strings $\cxt$, and \wordf{goes} with \conceptval{sg} context strings.
So, how do we account for the interplay between the geometry of the space and our probabilistic framework to obtain an information-theoretic objective that helps us identify the \emph{correct} concept encoding?

The answer is actually relatively simple. 
Our problem is that $\MI(\rvHerase; \rvHconcept) > 0$, i.e., these variables are \emph{not} statistically independent. 
We resolve this issue by building a variant of our information-theoretic objective in \cref{eq:subspace-mi} that \emph{assumes} these two variables are statistically \emph{independent}, i.e., $\MI(\rvHerase; \rvHconcept) = 0$.
Notice that, under this assumption, $\rvHerase$ would contain no information about the concept, and identification of $\rvHconcept$ would be possible via mutual information.
We build this assumption into our framework by breaking the statistical relationship between the variables. 
Note that, although this assumption likely never holds for a concept in practice, this does not matter here---we are crafting an objective under which the correct subspace will be optimal.

We begin by considering how we went about computing $\MI(\rvC, \rvHerase)$.
We denote with $\bhpar \defeq (\eyeminusPk) \bh$ and $\bhbot \defeq \bPk \bh$ the projections onto the concept and non-concept subspace for $\bh \in \repspace$. 
We relied on our induced unigram $\punigram(\concept, \bh)$, and simply plugged in $\bhbot$. This was wrong---we instead have to compute:
\begin{equation} \label{eq:propercorrelationalcond}
    \punigram(\word \mid \bhbot) = \sum_{\bh'_{\parallel}} \plm(\word \mid \bhbot, \bhpar) \punigram(\bh'_{\parallel} \mid \bhbot)
\end{equation} 
Importantly, the dependence between $\bhbot$ and $\bhpar$ means that $\punigram(\bh'_{\parallel} \mid \bhbot)$ only ever assigns probability mass to the pair $(\bhbot, \bhpar)$, i.e., there is only ever one $\bhpar$ to go with each $\bhbot$ in practice. This results in
\begin{equation}
    \punigram(\word \mid \bhbot) = \plm(\word \mid \bhbot, \bhpar)
\end{equation}
under which $\MI(\rvC, \rvHerase) = \MI(\rvC, \rvH)$.
This is just another way of explaining why, even after erasing $\bhpar$, we recover information about the concept from $\bhbot$. 
Statistical independence is defined as $\punigram(\bhbot, \bhpar) = \punigram(\bhbot) \punigram(\bhpar)$, so we look at the marginal distributions of these variables more closely.\looseness=-1

Marginalizing with respect to the induced unigram distribution defined in \cref{sec:lmsconcepts}, we arrive at the following marginal unigram distributions: 
\begin{equation}\label{eq:counterfactual-unigram-dist-bot}
    \punigram(\bhbot) \defeq \sum_{\bh \in \repspace}  \mathbbm{1}\{\bhbot = \bPk \bh\} \punigram(\bh) 
   \end{equation}
 \begin{equation}\label{eq:counterfactual-unigram-dist-par}   \!\punigram(\bhpar) \defeq \sum_{\bh \in \repspace} \mathbbm{1}\{\bhpar = (\eyeminusPk) \bh\} \punigram(\bh)     
\end{equation}
We now construct a variant of our induced unigram $\punigram(\word, \concept, \bh)$ that assumes independence between $\bhbot$ and $\bhpar$, i.e., $\punigram(\bh) = \punigram(\bhbot, \bhpar) = \punigram(\bhbot)\,\punigram(\bhpar)$.
We refer to this definition as the \defn{counterfactual unigram distribution} $\qunigram$, because we are assigning probability mass to $(\bhbot, \bhpar)$ pairs which, under $\punigram(\bh)$, would have zero probability.
The definition is given below\looseness=-1
\begin{align}\label{eq:counterfactualunigram}
  \qunigram(\word, \concept,\bhpar, &\bhbot) \defeq \sum_{\cxt \in \kleene{\alphabet}}\,\iota(\concept \mid \word, \cxt) \\
&\plm(\word \mid \bhpar, \bhbot)\,p(\cxt)\, \punigram(\bhpar)\, \punigram(\bhbot) \nonumber
\end{align}
Intuitively, this quantity measures the probability of the word--concept pair $(\word, \concept)$ jointly with a representation $\bhbot + \bhpar$ that may not exist in reality, thanks to the factorized unigram distributions of $\bhpar$ and $\bhbot$.
The choice of the name counterfactual, as well as the implications of this decoupling, will be made precise in \cref{sec:causal} when we introduce the causal interpretation of the word--concept model.\looseness=-1

Our counterfactual unigram distribution in \cref{eq:counterfactualunigram} allows us to decouple $\bhpar$ and $\bhbot$ and, thus, separate the information contained in $\rvHerase$ and $\rvHconcept$ without worrying about their dependence. 
We define the \defn{counterfactual mutual information} between the concept and the projection on to the non-concept subspace as\looseness=-1
\begin{align}  \label{eq:miqconceptcounterfactual}
    \MIq (& \rvC; \rvHerase ) \defeq \\
    &\sum_{\concept \in \concepts} \sum_{\bhbot \in \repspacebot} 
    \qunigram(\concept, \bhbot) \log \!\frac{\qunigram(\concept, \bhbot)}{\qunigram(\concept) \qunigram(\bhbot)} \nonumber
\end{align}
This quantity measures information about the concept in $\rvHerase$, and is computed by marginalizing out $\bhpar$ from \cref{eq:counterfactualunigram}.
$\MIq (\rvC; \rvHconcept)$ can also be obtained by marginalizing out $\bhbot$ instead. 
Importantly, as we show later in this section, \cref{eq:miqconceptcounterfactual} is minimized by the correct subspace in our example.

The same logic also allows us to correctly measure information contained in $\rvHerase$ and $\rvHconcept$ about the identity of the next word as\looseness=-1
\begin{align} \label{eq:miqwordcounterfactual}
    &\MIq (\rvX ; \rvHerase \mid \rvC) \defeq \\
    &\sum_{\concept \in \concepts} \sum_{\word \in \alphabet} \sum_{\bhbot \in \repspacebot} 
\qunigram(\word, \bhbot, \concept) \log \!\frac{\qunigram(\word, \bhbot \mid \concept)}{\qunigram(\word \mid \concept) \qunigram(\bhbot \mid \concept)} \nonumber
\end{align}
Computing \cref{eq:miqconceptcounterfactual,eq:miqwordcounterfactual} in practice allows us to come full circle. The independence assumption means a conditional is computed as an average over components obtained from other $\bh$ representations as follows\looseness=-1
\begin{equation} \label{eq:propercounterfactualcond}
    \qunigram(\word \mid \bhbot) = \sum_{\bh \in \repspacepar} \punigram(\word \mid \bhbot, \bhpar) \punigram(\bhpar) 
\end{equation}
Comparing this to \cref{eq:propercorrelationalcond} shows the practical implication of the independence assumption---$\punigram(\bhpar \mid \bhbot)$ becomes $\punigram(\bhpar)$. 







\paragraph{Resolving the example from \cref{sec:example}.}
As mentioned in \cref{sec:example}, the suboptimality of the erasure of our correct subspace under our \emph{correlational} $\MI(\rvC; \rvHerase)$ was caused by the fact that $\rvHconcept$ and $\rvHerase$ had a common cause and, thus, were not independent. 
Factorizing the distributions of $\rvHconcept$ and $\rvHerase$ in $\qunigram$, however, fixes this issue: Assuming that $\bPk$ defines the true subspace $\repspacepar$, (which, in \cref{sec:example}, is the $y$-axis), the $\qunigram$-based mutual information $\MIq(\rvC; \rvHerase)$ \emph{will} be $0$, no matter the possible common causes between $\rvHconcept$ and $\rvHerase$.
$\MIq(\rvC; \rvHerase)$, therefore, provides a much more suitable characterization of the correct matrix $\bPk$ defining the concept subspace. Now, we leverage \cref{eq:miqconceptcounterfactual,eq:miqwordcounterfactual} to establish several definitions that help us reason about the quality of our partition into concept and non-concept subspaces.\looseness=-1

\subsection{Erasure and Encapsulation} \label{sec:erasure}

We now make use of \cref{eq:miqconceptcounterfactual} to give a formal definition of erasure and encapsulation. 
These two notions, combined, determine the extent to which a projection matrix $\bPk$ has decomposed the representation space into concept and non-concept subspaces.

\begin{defin}[Counterfactual Erasure]\label{def:eraser}
Let $\rvHerase \defeq \bPk \rvH$ be an $\Rd$-valued random variable.
An orthogonal projection matrix $\bPk \in \R^{d \times d}$ of rank $k$ is an $\varepsilon$-\defn{eraser} of $\concepts$ if $\MIq(\rvC; \rvHerase) < \varepsilon$.\looseness=-1
\end{defin}
As $\varepsilon \to 0$, the subspace $\linspaceccomp$ characterized by an $\varepsilon$-eraser $\bPk$ for concept set $\concepts$ with respect to $\repspace$ encodes very little information about the concept.
This means that $\bPk$ effectively removes the distinction between values in the concept set $\concepts$, i.e., the language model is no longer able to determine the concept value required by the textual context when generating the next word.
We now show that given an $\varepsilon$-eraser $\bPk$, projecting onto its orthogonal complement with $\eyeminusPk$ preserves nearly all of the information.\looseness=-1

\begin{defin}[Counterfactual Encapsulation]\label{def:encapsulation}
Let $\rvHconcept \defeq \left(\eyeminusPk\right) \rvH$ be an $\Rd$-valued random variable.
An orthogonal projection matrix $\eyeminusPk \in \R^{d \times d}$ of rank $(d-k)$ is an $\varepsilon$-\defn{encapsulator} 
of $\concepts$ if $\MIq(\rvC; \rvH) - \MIq(\rvC; \rvHconcept) < \varepsilon$.\looseness=-1
\end{defin}
We remark that the quantity $\MIq(\rvC; \rvH) - \MIq(\rvC; \rvHconcept)$ is always non-negative due to the data-processing inequality \citep[\S2.8]{coverthomas2006}.
Encapsulation operationalizes the idea that a subspace gives us all the information needed to correctly identify the concept value required by textual context.
Combining erasure and encapsulation, we show that the mutual information decomposes additively in the following sense. 
\begin{restatable}{proposition}{pleasantdecomposition} \label{prop:pleasantdecomposition}
Suppose $\bPk$ is a rank-$k$ $\varepsilon$-eraser and $(\eyeminusPk)$ is a rank-$(d-k)$ $\varepsilon$-encapsulator of $\concepts$ with respect to $\repspace$.
Then, as $\varepsilon \rightarrow 0$, the following decomposition holds
\begin{equation} \label{eq:pleasantdecomposition}
\!\!\MIq(\rvC; \rvH) = \MIq(\rvC; \rvHerase) + \MIq(\rvC; \rvHconcept)
\end{equation}
\end{restatable}
\begin{proof}
    See \cref{app:pleasantdecomposition}. \end{proof}


\begin{comment}

Now, we introduce several definitions that help us reason about the quality of our partition into concept and orthogonal subspaces.\looseness=-1

We now make use of \cref{eq:subspace-mi} to give a formal definition of erasure and our partition hypothesis. 
Erasure is meant to determine the extent to which projection matrix $\bPk$ has isolated concept information.
\looseness=-1

\begin{defin}[Encapsulation]\label{def:encapsulation}
Let $\rvHconcept \defeq \left(\eyeminusPk\right) \rvH$ be an $\Rd$-valued random variable conditioned on the input $\rvbX$.
An orthogonal projection matrix $\eyeminusPk \in \R^{d \times d}$ of rank $(d-k)$ is an $\varepsilon$-\defn{encapsulator} 
of $\concepts$ if $\MI(\rvC; \rvH \mid \rvbX) - \MI(\rvC; \rvHconcept \mid \rvbX) < \varepsilon$.\looseness=-1
\end{defin}
We remark that the quantity $\MI(\rvC; \rvH \mid \rvbX) - \MI(\rvC; \bPk \rvH \mid \rvbX)$ is always non-negative due to the data-processing inequality \citep[\S2.8]{coverthomas2006}.
Encapsulation operationalizes the idea that a subspace gives us all the information needed to predict a concept.
Combining erasure and encapsulation, we show that the mutual information decomposes additively in the following sense. 
\begin{restatable}{proposition} {pleasantdecomposition} \label{prop:pleasantdecomposition}
Suppose $\bPk$ is a rank-$k$ $\varepsilon$-eraser and $(\eyeminusPk)$ is a rank-$(d-k)$ $\varepsilon$-encapsulator of $\concepts$ with respect to $\repspace$.
Then, as $\varepsilon \rightarrow 0$, the following decomposition holds
\begin{align} \label{eq:pleasantdecomposition}
\!\!\MI(&\rvC; \rvH \mid \rvbX) \\
&= \MI(\rvC; \bPk \rvH \mid \rvbX) + \MI(\rvC; (\eyeminusPk) \rvH \mid \rvbX) \nonumber
\end{align}
\end{restatable}
\begin{proof}
    See \cref{app:pleasantdecomposition}. \end{proof}

\end{comment}


\subsection{Containment and Stability}













Erasure and encapsulation give us a precise definition of what it means to partition the representation space into a component that does not contain the information about the concept ($\rvHerase$) and one that does ($\rvHconcept$).
They do \emph{not}, however, consider the information content of the representation aside from the concept.
Observe that, with perfect erasure and encapsulation, the learned orthogonal projection matrix $\bPk$ could erase much of the non-concept related information from $\linspaceccomp$. 
Specifically, if $\concepts$ is encoded non-linearly \citep{ravfogelkernel2022}, then erasure via a linear orthogonal projection could require the removal of additional dimensions that would likely also contain non-concept information.
Therefore, in the concept erasure literature, tests of successful erasure are generally paired with a verification that the representations are not otherwise damaged \citep{kumar2022probing,ravfogel-etal-2020-null,ravfogel2022linear,ravfogelkernel2022,elazar-etal-2021-amnesic}. 
We, too, need an information-theoretic notion of preservation of non-concept information in $\rvHerase$.


Preserving information about non-concept aspects of $\cxt$ in $\rvHerase$ requires that $\rvHconcept$ \emph{only} capture information about the concept. 
In this sense, $\rvHconcept$ should be the \emph{minimal} subspace that captures $\rvC$. 
Containment formalizes this notion.
Containment requires that, conditioned on $\rvC$, $\rvHconcept$ contains little information about the next word $\rvX$.
We just stated that the only information (or stochastisity) in $\rvHconcept$ should come from the concepts, so by conditioning on the concept, we should remove all stochasticity from $\rvHconcept$.\looseness=-1
\begin{defin}[Counterfactual Containment] \label{def:containment}
Let $\bPk$ be an eraser for concept set $\concepts$ with respect to $\repspace$.
Let $\rvHconcept \defeq \left(\eyeminusPk\right) \rvH$ be an $\Rd$-valued random variable.
Then, we say that $\bPk$ is $\varepsilon$-\defn{contained} with respect to $\repspace$ if $\MIq(\rvX; \rvHconcept \mid \rvC) < \varepsilon$.\looseness=-1
\end{defin}
Lastly, we define stability, a natural way to measure how much non-concept information about the next word is \emph{preserved} in the non-concept subspace $\rvHerase$.
Ideally, this should be as close as possible to the information present in the entire representation space, ignoring the information about the concept.
\begin{defin}[Counterfactual Stability] \label{def:stability}
Let $\bPk$ be an eraser for concept set $\concepts$ with respect to $\repspace$.
Let $\rvHerase \defeq \bPk \rvH$ be an $\Rd$-valued random variable.
Then, we say that $\bPk$ is an $\varepsilon$-\defn{stabilizer} with respect to $\repspace$ if $\MIq(\rvX; \rvH \mid \rvC = \concept) - \MIq(\rvX; \rvHerase \mid \rvC) < \varepsilon$.\looseness=-1
\end{defin}
We remark that the data processing inequality once again ensures that $\MIq(\rvX ; \rvH \mid \rvC) - \MIq(\rvX ; \rvHerase \mid \rvC) \geq 0$. 
Containment and stability together characterize the \emph{preservation} of information not related to concepts.
In analogy to the goal of erasure and encapsulation in \cref{sec:erasure}, containment can be viewed as the erasure of next-word-related information from $\rvHconcept$, and stability can be viewed as the encapsulation of next-word-related information in $\rvHerase$.\looseness=-1

A decomposition of the mutual information of $\rvX$ with regard to $\rvHerase$ and $\rvHconcept$ can also be derived in a similar sense to \Cref{prop:pleasantdecomposition}.



\begin{restatable}{proposition}{propdecompositionx} 
    \label{prop:decomposition-mi-x}
    Suppose $\bPk$ is $\varepsilon$-contained and $\varepsilon$-stabilized.
    Then, as $\varepsilon \rightarrow 0$, the following decomposition holds
    \begin{align} \label{eq:pleasantdecomposition-x}
    &\!\!\MIq(\rvX; \rvH \mid \rvC) \\
    &= \MIq(\rvX; \rvHerase \mid \rvC) + \MIq(\rvX; \rvHconcept \mid \rvC) \nonumber
    \end{align}
\end{restatable}

\begin{proof}
    See \cref{app:decomposition-mi-x}.
\end{proof}




We have now defined four properties which, together, allow us to verify that we have properly identified the concept subspace and partitioned the representation space according to \cref{eq:pleasantdecomposition,eq:pleasantdecomposition-x}. 
Next, we argue that such a partitioning enables us to control a language model's generative process.\looseness=-1


\newcommand{\minnodesize}{7ex}
% Figure environment removed


\begin{comment}
    \begin{subfigure}{0.20\textwidth}
    \centering
    \begin{tikzpicture}[latent/.style={circle, draw, minimum size=\minnodesize}]
        \node[const]     (bp) {$\bPk$};
        \node[latent, right=of bp]   (concept) {$\rvC$};        
        \node[latent, below=of bp] (compsubspace) {$\rvHerase$};
        \node[latent, below=of concept]     (conceptsubspace) {$\rvHconcept$};
        \node[latent, below= of conceptsubspace] (x) {$\rvX$};
        \node[det, below= of compsubspace]   (repspace) {$\rvH$};
        
        \edge {conceptsubspace, compsubspace} {repspace} ; \edge {concept}  {conceptsubspace} ; \edge {repspace} {x} ; \edge {bp} {conceptsubspace,compsubspace}
    \end{tikzpicture}
    \caption{} \label{fig:causal-graph-b}
    \end{subfigure}
    
    \begin{subfigure}{0.20\textwidth}
    \centering
    \begin{tikzpicture}[latent/.style={circle, draw, minimum size=\minnodesize}]
        
        \node[const]     (bp) {$\bPk$};
        \node[latent, right=of bp]   (concept) {$\rvC$};        
        \node[latent, below=of bp] (compsubspace) {$\rvHerase$};
        \node[det, below=of concept]     (conceptsubspace) {$\rvHconcept$};
        \node[latent, below= of conceptsubspace] (x) {$\rvX$};
        \node[det, below= of compsubspace]   (repspace) {$\rvH$};
        
        \edge {conceptsubspace, compsubspace} {repspace} ; \edge {concept}  {conceptsubspace} ; \edge {repspace} {x} ; \edge {bp} {conceptsubspace,compsubspace}
        
    \end{tikzpicture}
    \caption{} 
    \label{fig:causal-graph-c}
    \end{subfigure}
\end{comment}
 
\section{A Causal Graphical Model} \label{sec:causal}
We now propose a causal structure by which language models leverage concepts, in the form of a latent variable, in the generation process.
We relate this causal structure to the information partition definitions given in \cref{sec:functionalsubspace}.
This enables causal controlled generation via a do-intervention \cite{pearl2009causal} on the concept random variable $\rvC$.
We finish with a discussion of how our causal controlled generation approach improves upon existing approaches. 
\looseness=-1





\subsection{Concept as a Latent Variable}

While autoregressive language models are also referred to as \emph{causal} informally,\footnote{The monicker causal is a recent development and ahistorical. 
It was brought about primarily by the rise in popularity of masked language models, e.g., \citet{devlin-etal-2019-bert}, which are not language models in the traditional sense in that they do \emph{not} define a distribution over strings. 
} 
in this work, we delve into what the actual causal structure behind an autoregressive language model may be.
We illustrate the tradidional autoregressive causal structure, based on the model definition put forth in \cref{sec:lms}, in \cref{fig:causal-graph-a}.
In it, the $\kleene{\alphabet}$-valued random variable $\rvbX$ represents the textual context that was previously sampled from the model, $\rvH$ is the deterministic contextual representation, and $\rvX$ the word which is sampled from $\rvH$ based on \cref{eq:softmax}.\footnote{Circles in \cref{fig:causal-graph} represent random variables and diamonds represent deterministic variables.\looseness=-1}





When we discussed the construction of a word--concept model in \cref{eq:string-concept-joint}, we were implicitly assuming a causal structure. 
This structure is identical to \cref{fig:causal-graph-a}, with the addition of the concept variable depending on $\rvbX$ and $\rvX$.
The conditional distribution $\iota(\concept \mid \cxt, \word)$ indicates that the concept $\concept$ is determined after the word $\word$ is sampled. 
According to such a causal structure, the concept is a byproduct of the generation process, in that it does not contribute to determining the word $\word$. 
For this reason, while this structure allows us to study the relationship between textual context, words and the associated concepts, it does not lend itself well to \emph{controlling} the generated sequence based on a selected value of a concept.\looseness=-1



To enable controlled generation with respect to the concept, we introduce a $\concepts$-valued latent variable $\rvC$ in the generation process, as shown in \cref{fig:causal-graph-b}.
We make two assumptions about $\rvC$.
First, we assume that the distribution of $\rvC$ is determined by the textual context $\rvbX$, and, moreover, that $\rvC$ is not \emph{fully} determined by the context $\cxt$, i.e., $\rvC$ is stochastic.
As we saw in an example in \cref{sec:lmsconcepts}, the concept value of a word is not fully determined by the preceding context.
Second, we assume that the concept is determined \emph{before} the word is sampled.
In this way, the concept can directly influence the sampled word $\word$ and is thus better suited for controlled generation.
In doing so, we break away from $\iota$, which deterministically assigned a concept value to a word based on the preceding context. 

Our two assumptions on $\rvC$ have an important implication: $\rvbX$ is no longer the only source of stochasticity in $\rvH$, as in \cref{fig:causal-graph-a}.
Rather, we assume that both $\rvbX$ as well as $\rvC$ influence the representation $\rvH$. 
This means $\bh$ is now a function of both $\cxt$ and $\concept$, i.e., $\bh = \bh\left(\cxt, \concept\right)$.
This new contextual representation is then used to determine the distribution over the next word $\word$ according to \cref{eq:softmax}.
This is not, strictly speaking, an accurate model of the construction of representations based on textual context, because $\rvH$ is \emph{deterministic} given $\rvbX$ in most neural language models, as previously discussed in this paper.
Nevertheless, we make this assumption because we consider this to be a minor departure from reality that enables our model.\looseness=-1

\subsection{Causal Controlled Generation} \label{sec:causal-con-gen}
We now derive a formal relationship between erasure, encapsulation, stability, containment, and the assumed causal graph in \cref{fig:causal-graph-b}. 
First, inspecting \cref{fig:causal-graph-b}, we see that if we wish to intervene on $\rvC$ to influence $\rvX$, there is a single backdoor \backdoorcolor{path} from $\rvC$ to $\rvH$.
However, as shown in \cref{fig:causal-graph-c}, \emph{intervening} on $\rvC$ directly (denoted by $\causaldo(\rvC = \concept)$) breaks this path by removing the edge $\rvbX \rightarrow \rvC$, which lets us easily compute the distribution over the next word after intervening on $\rvC$ as follows
\begin{align} \label{eq:do-intervention}
p(&\word \mid \rvHerase = \bhbot, \causaldo(\rvC = \concept)) \\
&= \sum_{\boldg \in \repspace}p(\word \mid \rvH = \bhbot + (\eyeminusPk)\boldg)\,p(\boldg \mid \concept) \nonumber
\end{align}
where, as shown in \cref{fig:causal-graph-b}, we assume that $\bhbot$ is deterministic given the context $\cxt$. $\boldg$ is an $\Rd$-valued contextual representation that encodes a textual context $\cxt'$ with concept value $\concept$. With high probability, $\cxtenc$ and $\boldg(\cxt')$ will be different. This is the logical conclusion of our decision to treat $\bhbot$ and $\bhpar$ as statistically independent---we can intervene on the generation process by setting the value of the conceptual component independently.
\looseness=-1

We now make good on our decision to name the counterfactual unigram distribution from \cref{eq:counterfactualunigram} as such.
Assuming the model \cref{fig:causal-graph-b}, a do-intervention on $\rvC$---as depicted in \cref{fig:causal-graph-c}---implies erasure, encapsulation, stability, and containment.
We make this idea formal in the following theorem.
\begin{restatable}{theorem}{bigboy}\label{thm:graph}
Consider a joint distribution $p$ that factors as in \cref{fig:causal-graph-b} parameterized by orthogonal projection matrix $\bPk$.
Under the distribution\looseness=-1
\begin{align}
\pdo(&\word, \bhbot, \bhpar, \concept) = p(\word \mid \bhbot, \bhpar)\\
&p(\bhbot \mid \mathrm{do}\left(\rvC = \concept\right))\,p(\bhpar \mid \mathrm{do}\left(\rvC = \concept\right))\,p(\concept) \nonumber
\end{align}
we have that $\bPk$ is an $\varepsilon$-eraser, $\eyeminusPk$ is an $\varepsilon$-encapsulator, $\eyeminusPk$ is an $\varepsilon$-container and $\bPk$ is an $\varepsilon$-stabilizer for every $\varepsilon > 0$.\looseness=-1
\end{restatable}
\begin{proof}
A proof is given in \cref{app:graph}.
\end{proof}
What \Cref{thm:graph} tells us is that the graph given in \cref{fig:causal-graph-b} is consistent with the technical elaboration in \cref{sec:functionalsubspace}. 
Specifically, it means that erasure, encapsulation, stability, and containment are all properties that we expect a causal distribution resulting from an intervention on a concept to have. 
The interventional distributions, hence, motivate our discussion on independent $p(\bhpar)$ and $p(\bhbot)$ in \cref{sec:counterfactual}).\looseness=-1

\subsection{Constant Concept Subspace Hypothesis} \label{sec:constantsubspace}

Using the vocabulary and the graphical model established so far, we can formalize another special case as the following hypothesis: The case where there is no arc $\rvbX \rightarrow \rvHconcept$ in \cref{fig:causal-graph-b}. 
\begin{hypothesis}\label{hyp:constantsubspace}
    Let $\left(\eyeminusPk\right)$ be an $\varepsilon$-encapsulator of $\rvC$.
    The \defn{constant concept subspace hypothesis} states that, for any $\cxt \in \kleene{\alphabet}$ and $\concept \in \concepts$, the projection of $\bh\left(\cxt, \concept\right)$ onto the $\rvHconcept$ depends only on $\concept$ as $\varepsilon \rightarrow 0$.
That is $\left(\eyeminusPk\right)\bh(\cxt, \concept) = \bgc \in \Rd$ for all $\concept \in \concepts$ for some $\boldg\colon \concepts \to \Rd$ assigning concepts vector representations.
\end{hypothesis}
In words, each concept value $\concept \in \concepts$ has a constant encoding $\bgc$ in the concept subspace represented by random variable $\rvHconcept$.
This means that $\rvC$ \emph{uniquely} determines $\rvHconcept$, i.e., $\rvHconcept$ is a deterministic function of $\rvC$. 
We suspect that the constant concept subspace hypothesis will \emph{not} hold completely in practice.



\subsection{Non-causal Controlled Generation} \label{sec:noncausalgen}



Controlled generation involving the manipulation of concepts is not a new problem. 
We contextualize our approach relative to \citeposs{yang-klein-2021-fudge} method.
They perform controlled generation as follows. 
First, they train a classifier to predict a concept value $\concept \in \concepts$ from the contextual representation $\bh$ of a language model.
Then, they perform controlled generation by conditioning on a concept value $\rvC = \concept$ and applying Bayes' rule as follows:
\begin{align}
p(&\word \mid \cxt, \rvC = \concept) \\ 
&\propto  p(\rvC = \concept \mid (\eyeminusPk)\cxtenc)\,p(\word \mid \cxt) \nonumber
\end{align}
We illustrate the causal structure implied by this approach in \cref{fig:causal-graph-d}.
Additionally, we include $\bPk$ to relate this approach to our subspace formulation,\footnote{Thus, we assume that the classifier is restricted to looking at $\rvHconcept$ to make its prediction.} but note that \citet{yang-klein-2021-fudge} do not make use of concept subspaces.
Importantly, a do-intervention on this causal structure is not possible, because there is no causal path from $\rvC$ to $\rvX$ in \cref{fig:causal-graph-d}.
A do-intervention would remove the edge from $\rvH$ to $\rvC$, thereby having no effect on $\rvX$, i.e., we have\looseness=-1
\begin{equation}
\!p(\word \mid \bh(\cxt), \causaldo(\rvC = \concept)) = p(\word \mid \bh(\cxt)) 
\end{equation}
This is why the authors \emph{condition} on $\rvC$ instead.
In this sense, \citeposs{yang-klein-2021-fudge} and similar methods are not causal and cannot easily be extended to be so.
As discussed in \cref{sec:causal-con-gen}, our approach \emph{is} causal, but such an analysis may come at the price of a number of restricting assumptions that are not fully met in practice. 
In the next section, we explain how we go about testing these assumptions with data.

\section{Experimental Setup} \label{sec:experimentsetup}

\paragraph{Concepts and Models.} We perform our analysis on two concepts, \conceptf{verbal-number} in English with $\concepts = \{\conceptval{sg}, \conceptval{pl}, \conceptval{n/a}\}$ and \conceptf{grammatical-gender} in French with $\concepts = \{\conceptval{fem}, \conceptval{msc}, \conceptval{n/a}\}$. For each of these concepts, we study the representation spaces of an autoregressive language model, namely GPT2 \citep{radford2019language}.\footnote{We rely on the implementations in the transformers library \citep{wolf-etal-2020-transformers}, namely: \modelnamef{gpt2-large} for \conceptf{verbal-number} and \modelnamef{gpt2-base-french} for \conceptf{grammatical-gender}.} 

\paragraph{Data.} For \conceptf{verbal-number} in English, we rely on \citeposs{linzen2016assessing} number agreement dataset. 
This dataset consists of sentences from Wikipedia with a \conceptval{sg} or \conceptval{pl} verb, constructed such that each sentence is provided with the fact (\conceptval{sg} verb in the case of a \conceptval{sg} sentence, and vice-versa) and the foil (inflected form of the fact to have opposite concept value). 
For \conceptf{grammatical-gender} in French, we rely on three treebanks from Universal Dependencies \citep{nivre-etal-2020-universal}: French GSD \citep{guillaumegsd}, ParTUT \citep{Sanguinetti2015, sanguinettipisa2014, boscotub2014}, and Rhapsodie \citep{lacheretrhapsodie2014}. We replicate the preprocessing steps of \citet{linzen2016assessing} on each of these datasets.


% Figure environment removed





\paragraph{Vocabulary Partition.} In \cref{sec:concepts}, we defined our concept mapping $\iota$ as a means of relating language models and concepts. This distribution is context-sensitive in order to handle words with ambiguous concept values out of context, for example, due to syncretism in English. In practice, we drop the context-dependent aspect and consider a single partition of $\eosalphabet$ as our definition of a concept. 
We start constructing the partition in a model-agnostic manner: we use Spacy \citep{spacy} to tag the French and English Wikipedia corpora \citep{wikidump}, respectively. For \conceptf{verbal-number}, we use the tagged English words to obtain lists of \conceptval{sg} and \conceptval{pl} verbs, which we then align to obtain matching pairs, e.g., (\wordf{walks}, \wordf{walk}). The process is the same for \conceptf{grammatical-gender} in French, leading to gendered pairs of adjectives, e.g., (\wordf{franÃ§ais}, \wordf{franÃ§aise}). For each model, we then partition the vocabulary according to the appropriate list, with tokens not included in either list classified as \conceptval{n/a}. One limitation of our work is that we don't consider concept words that are tokenized into more than one subword in our analysis---for example, if \wordf{disambiguates} tokenizes to [\wordf{disambiguate}, "\wordf{\#s}"], then the pair (\wordf{disambiguates}, \wordf{disambiguate}) is assigned to \conceptval{n/a}.\looseness=-1

\paragraph{The concept value $\conceptval{n/a}$.} In practice, we exclude $\conceptval{n/a}$ from our concept set when computing our metrics. The reason for this is that in both text sampled from the model and natural text, the vast majority of words don't invoke the concept, meaning the concept marginals $\punigram(\concept)$ and $\ptildeunigram(\concept)$ assign very low probabilities to our values of interest. Resulting information metrics would therefore be dominated by $\conceptval{n/a}$, so we choose to exclude it. 


\paragraph{Finding the Concept Subspace.} We find $\bPk$ for different values of rank parameter $k \in \{0, \ldots, d\}$ using the minimax game of R-LACE \citep{ravfogel2022linear}. Until recently \citep{belrose2023leace}, R-LACE was the state-of-the-art approach for finding the optimal rank $k$ subspace $\linspaceccomp$ characterized by $\bPk$, such that the concept is no longer linearly recoverable from the representations. R-LACE optimizes a cross-entropy loss on samples from $\ptildeunigram$, as follows
\begin{equation}
\min_{\bw} \max_{\bPk}  \sum_{\concept \in \concepts} \sum_{\bh \in \repspace} \ptildeunigram(\concept \mid \bh) \log p_{\bw} (\concept \mid \bPk \bh) \nonumber  
\end{equation}
up to an additive constant where we define\looseness=-1
\begin{equation}
p_{\bw} (\concept \mid \bPk \bh) \defeq \frac{\exp \left( \bw(\concept) \cdot \bPk \bh \right)}{\sum_{\concept' \in \concepts} \exp \left( \bw(\concept') \cdot \bPk \bh \right)}
\end{equation}
and $\bw \colon \concepts \to \Rd$ is a parameter matrix. 
By optimizing against linear predictability, R-LACE maximizes a lower bound on our\emph{correlational} $\MI(\rvC; \rvHerase)$ with respect to $\bPk$. 
We report \emph{counterfactual} information-theoretic results for the projection matrices learned by R-LACE, and leave the optimization of this metric for identification of the causal subspace to future work.
Results are reported over 3 random instantiations of R-LACE, i.e., over 3 learned $\bPk$ matrices for each $k$. Due to the instability of the adversarial game objective, extensive hyperparameter tuning was done for low $k$ values, especially $k=1$. Higher variance at higher $k$ values, for some models, is due to instability in training resulting, we suspect, from lack of hyperparameter tuning.


\section{Results} \label{sec:results}
We now discuss our empirical findings.\looseness=-1

\subsection{Partitioning of Concept Information} \label{sec:partitionresults}

The first key finding in \cref{fig:resultsmain} is that, across the board, R-LACE finds a $1$-dimensional concept subspace that performs well according to our \emph{counterfactual} metrics. 
In terms of erasure, encapsulation and reconstructed information, we see that the partitioning of concept information is both substantive and lossless. 
Roughly 50\% of total concept information is erased from the non-concept subspace, and this information is preserved in the concept subspace (encapsulation), such that the sum over our partition is approximately equal to total information contained in the original representations.
Low values of stability and containment metrics for $k=1$ show that this partitioning is made while preserving non-concept information in $\linspaceccomp$.
This result is notable because R-LACE was not trained to minimize counterfactual erasure.
As explained in \cref{sec:experimentsetup}, R-LACE optimizes an objective that is susceptible to correlations. 
Our results show that, while counterfactual MI is not reduced to 0, the model does rely on this subspace heavily for generation. 

Next, we note that as $k$ increases, our metrics change as we would expect. 
Erasure is driven to 0 as the representations are gradually destroyed, while concept information in the concept subspace increases with $k$. 
We note that for \texttt{gpt2-large}, information loss when partitioning concept information into concept and non-concept subspaces is near 0. This is not the case for \texttt{gpt2-base-french}---for this model, as $k$ increases, concept information is erased from $\rvHerase$ but that information is not recovered in $\rvHconcept$, resulting in a lossy partition. 
In terms of the preservation of non-concept information over $k$, stability increases roughly linearly in $k$, while containment stays relatively constant as $k$ increases initially. 
This means that information about the next word is erased from the non-concept subspace by removing more dimensions, but that information is destroyed in the process of breaking it off into the concept subspace.\looseness=-1 

% Figure environment removed

\subsection{Causal Controlled Generation} \label{sec:controlresults}

In \Cref{fig:resultscontrol}, we show accuracy results for the controlled generation experiment.
We consider only a $1$-dimensional concept subspace because of the results given in \cref{sec:partitionresults}.
We report values on samples from the test split of our curated datasets of natural text used to train R-LACE, averaging over $\bgc$ values from the validation set.
We compute accuracy in the forced-choice setup shown in \cref{ex:gramm,ex:ungramm}, i.e., assigning higher probability to the fact over foil. 
In order to properly evaluate the control experiment, we first contextualize our results with accuracy measures before and after concept erasure.\looseness=-1

\textit{Orig.\,\,Accuracy} refers to the accuracy of the model using original representations. 
With \textit{Orig. Top Concept}, we report the accuracy with which the model assigns highest rank among concept words to the fact.
Then, we show the accuracy of the model in predicting each class of the binary concept after erasure. We see, in the case of \texttt{gpt2-large}, R-LACE learns a $\bPk$ that erases \conceptval{pl}. 
Similarly, \conceptval{fem} is erased from \texttt{gpt2-base-french}. 
We note that erasure with $k=1$ is more successful, in terms of accuracy, for \texttt{gpt2-large} than \texttt{gpt2-base-french}. 
We attribute this to differences in the quality and the amount of training data.
Both correspond to minority classes in the training corpora.

With this context in mind, we find a mixed result for the control experiment. 
For \texttt{gpt2-large}, the do-intervention to $\rvC = \conceptval{pl}$ completely recovers the accuracy for this class. 
\textit{Erased Accuracy $\rvC=\conceptval{pl}$} shows a baseline of accuracy after erasure that is near 0, i.e., the model always assigns higher probability to the \conceptval{sg} form. 
The intervention drives this accuracy back up to above 0.9, the value prior to erasure. 
Note that we chose not to show a concept value breakdown for \textit{Orig. Accuracy} because models were equally good at predicting both classes, across both concepts.
Furthermore, the fact that top rank accuracy from original representations is preserved after erasure and intervention operations suggests that each of these operations has essentially no impact on non-concept information for $k = 1$. 
Results for \texttt{gpt2-base-french} are not as good---$\rvC = \conceptval{msc}$ actually reduces the accuracy relative to after erasure, while, with $\rvC = \conceptval{fem}$, we see no significant difference.
Results in \cref{sec:partitionresults} do not explain this difference---the information partition for \texttt{gpt2-base-french} was more lossy, but not to an extent that foreshadowed such a difference.

\begin{comment}
% Figure environment removed
\subsection{Comparison with Correlational Metrics} \label{sec:counterfactualcompare}

In this section, we compare our four counterfactual metrics (erasure, encapsulation, stability and containment) with their corresponding \emph{correlational} metrics.
\Cref{fig:resultscorr} shows correlational metric values for the same learned $\bPk$ matrices as \cref{fig:resultsmain}.
We argued in \cref{sec:example} that such correlational metrics were inadequate on the basis of a simple example. 
However, in practice, even for small $k$, we are able to find a correlation $\varepsilon$-eraser in practice for small $\varepsilon > 0$.
Yet, we note negative values in all of the mutual information estimates for erasure in \texttt{gpt2-large}, and for containment in all models.
This hints that the estimator is unstable because mutual information is provably non-negative so we should take any conclusions drawn from these results with a grain of salt.
Lastly, we note differences in the value of \textit{Reconstructed MI} for the same model and sampling technique---correlational metrics suggest information loss for \texttt{gpt2-large}, while counterfactual results suggest information is preserved well over $k$.
\end{comment}


\section{Related Work} \label{sec:related-work}

In terms of our stated goal of developing a geometrically oriented causal probing framework, our work is most closely related to \citet{elazar-etal-2021-amnesic} and \citet{lasri-etal-2022-probing}. 
\citet{elazar-etal-2021-amnesic} pose the problem of identifying a subspace used by a model to perform a task via an erasure intervention, on the assumption that failure to perform the task after intervention certifies the usage of the subspace. 
Amnesic probing \citep{elazar-etal-2021-amnesic} measures usage by a language model using word prediction accuracy after erasure, without specifying which set of words is considered at each step (e.g., entire vocabulary, specific pairs). 
\citet{lasri-etal-2022-probing} extend this work with a carefully designed dataset, referred to as a behavioral task, in which each context has a fact--foil pair, such that usage can be measured via change in the relative probabilities of the fact and foil, and the model's ability to assign higher probability to the fact. 
In short, \citet{lasri-etal-2022-probing} applied \citeposs{linzen2016assessing} forced choice approach to the problem of evaluating the impact of concept erasure.
In terms of evaluating the ability of linear concept erasure methods to remove a concept subspace that is useful to the model, our work builds on both: we rely on behavioral datasets to learn our projection matrices, but we look beyond the choice between a pair of words by formalizing the expectation that erasure should result in an inability to distinguish between concept-related pairs generally.  





With regards to the preservation of non-concept related features after the intervention, existing work on concept erasure has done this in two different ways. The most common approach \citep{ravfogel-etal-2020-null, ravfogel2022linear, ravfogelkernel2022} is to run SimLex-999 \citep{hill2015-simlex}, a word similarity test that checks that the model's understanding of words hasn't changed significantly after the intervention. 
\citet{ravfogelkernel2022} also check that the ability of a main-task classifier on an orthogonal task is not seriously harmed after erasure (though we note that a new classifier is trained on the representations \emph{after} intervention). Altogether, these tests have little to do with the model's output distribution over the next word, which is our primary concern. 
\citet{elazar-etal-2021-amnesic} fine-tune a language model after erasure with gold annotations for the concept, assuming that if performance on the task recovers fully, then no other information was removed by the intervention. In turn, they posit that an incomplete recovery suggests other information was removed. 
Fine-tuning results in the paper show an increase in task performance, which suggests that further training can improve the model overall, casting doubt on the validity of performance recovery as an evaluation criterion. Finally, \citet{elazar-etal-2021-amnesic} also report the overall KL divergence in the LLM's output distribution, over the entire vocabulary. This last approach was a source of inspiration for our work, which delves much deeper into this distributional distance idea via our stability and containment tests.

Lastly, with regards to the application of information theory to probing, and specifically the idea of mutual information between concept-valued random variables and model representations, we drew inspiration from \citet{pimentel-etal-2020-information} and \citet{voita-titov-2020-information}.\looseness=-1













\section{Conclusion}

In this paper, we set out to define an \emph{intrinsic} measure of information in a subspace of a language model's representation space. 
In light of the correlational failure mode of linear concept erasure methods \cite{kumar2022probing}, doing so requires a counterfactual approach. 
By treating the components of a representation in the concept subspace and its orthogonal complement independently, we are able to correctly measure information in a subspace by marginalizing out the remainder of the space.
To the extent that a causal concept subspace exists for a particular concept and model, erasure under this metric is optimized by that subspace.
In practice, we did not actually optimize this metric. 
This is a complex task that we leave to future work---computing it involves a double nested sum over countably infinite vector spaces.
Our theoretical analysis, combined with the efficacy of linear erasure methods using a correlational objective, suggests a tantalizing prospect: That a counterfactual objective could identify a one-dimensional causal subspace containing \emph{all} information about the concept empirically.

\section*{Acknowledgements}
The last author, Ryan Cotterell, acknowledges support from
the Swiss National Science Foundation (SNSF) as
part of the ``The Forgotten Role of Inductive Bias
in Interpretability'' project.
Anej Svete is supported by the ETH AI Center Doctoral Fellowship.
The author list acknowledges Lucas Torroba Hennigen, Afra Amini, Naomi Saphra, Tiago Pimentel, Clara Meister, Shauli Ravfogel and Ethan Gotlieb Wilcox for earlier discussions and feedback on the manuscript at various stages.
Any errors in the work are exclusively those of the authors. 

\bibliography{custom}
\bibliographystyle{acl_natbib}

\clearpage
\onecolumn

\appendix

\section{Proof of \Cref{prop:pleasantdecomposition}} \label{app:pleasantdecomposition}
\pleasantdecomposition*
\begin{proof}
On the left-hand side,
\begin{subequations}
\begin{align}  \MIq(\rvC; \rvH)  + \varepsilon &\ge \MIq(\rvC; \rvHconcept) + \varepsilon 
 & \justify{data-processing inequality} \\
&\ge \MIq(\rvC; \rvHconcept) + \MIq(\rvC; \rvHerase)  
 & \justify{$\bPk$ is an $\varepsilon$-eraser} \label{eq:pleasantdecomposition-lhs}
\end{align}
\end{subequations}
On the right-hand side,
\begin{subequations}
\begin{align}
\MIq(\rvC; \rvH) + \varepsilon &\le  \MIq(\rvC; \rvHconcept) +  2 \varepsilon&  \justify{$(\eyeminusPk)$ is an $\varepsilon$-encapsulator}\\
&\le \MIq(\rvC; \rvHconcept) + \MIq(\rvC; \rvHerase) + 2\varepsilon 
 & \justify{non-negativity of MI} \label{eq:pleasantdecomposition-rhs}
\end{align}
\end{subequations}
Combining \cref{eq:pleasantdecomposition-lhs} and \cref{eq:pleasantdecomposition-rhs}, we have
\begin{subequations} \label{eq:c-h-decomposition-sandwich}
\begin{align}
    \MIq(\rvC; \rvHconcept) + \MIq(\rvC; \rvHerase) &\le \MIq(\rvC; \rvH) + \varepsilon \\
    &\le \MIq(\rvC; \rvHconcept) + \MIq(\rvC; \rvHerase) + 2\varepsilon  
\end{align}
\end{subequations}
Taking $\varepsilon \to 0$ in \cref{eq:c-h-decomposition-sandwich}, we have \cref{eq:pleasantdecomposition}
\begin{subequations}
\begin{align}
     \MIq(\rvC; \rvH) = \MIq(\rvC; \rvHconcept) + \MIq(\rvC; \rvHerase) \nonumber
\end{align}
\end{subequations}
\end{proof}


\section{Proof of \Cref{prop:decomposition-mi-x}} \label{app:decomposition-mi-x}
\propdecompositionx*

\begin{proof}
On the left-hand side,
\begin{subequations}
\begin{align} 
 \MIq(\rvX; \rvH \mid \rvC)  + \varepsilon \ge& \MIq(\rvX;\rvHconcept \mid \rvC) + \varepsilon
 & \justify{data-processing inequality}  \\
\ge& \MIq(\rvX; \rvHconcept \mid \rvC) + \MIq(\rvX; \rvHerase \mid \rvC)  
 & \justify{$\bPk$ is $\varepsilon$-contained} \label{eq:decomposition-x-lhs}
\end{align}
\end{subequations}
On the right-hand side,
\begin{subequations}
\begin{align}
\MIq(\rvX; \rvH \mid \rvC) + \varepsilon &\le \MIq(\rvX; \rvHconcept \mid \rvC) +  2 \varepsilon & \justify{$\bPk$ is an $\varepsilon$-encapsulator}\\
&\le \MIq(\rvX; \rvHconcept \mid \rvC) + \MIq(\rvX; \rvHerase \mid \rvC) + 2\varepsilon  & \justify{non-negativity of MI} \label{eq:decomposition-x-rhs}
\end{align}
\end{subequations}
Combining \cref{eq:decomposition-x-lhs} and \cref{eq:decomposition-x-rhs}, we have
\begin{subequations} \label{eq:x-h-decomposition-sandwich}
    \begin{align}
    \MIq(\rvX; \rvHconcept \mid \rvC) + \MIq(\rvX; \rvHerase \mid \rvC)  &\le \MIq(\rvX; \rvH \mid \rvC) + \varepsilon \\
    &\le \MIq(\rvX; \rvHconcept \mid \rvC) + \MIq(\rvX; \rvHerase \mid \rvC) + 2\varepsilon 
\end{align}
\end{subequations}
Taking $\varepsilon \to 0$ in \cref{eq:x-h-decomposition-sandwich}, we have \cref{eq:pleasantdecomposition-x}
\begin{subequations}
\begin{align}
     \MIq(\rvX; \rvH \mid \rvC) = \MIq(\rvX; \rvHconcept \mid \rvC) + \MIq(\rvX; \rvHerase \mid \rvC) \nonumber
\end{align}
\end{subequations}
\end{proof}

\section{Proof of \Cref{thm:graph}}\label{app:graph}
\bigboy*


\begin{proof}
Given the factorization in \cref{fig:causal-graph-b}, we derive the following equation using the independence assumptions given in \cref{fig:causal-graph-b}:
\begin{subequations}
\begin{align}
\pdo(\word, \bhbot, \bhpar, \concept) 
&=p(\word \mid \bhbot, \bhpar)\,p(\bhbot \mid \mathrm{do}\left(\rvC = \concept\right))\,\pdo(\bhpar \mid \mathrm{do}\left(\rvC = \concept\right))\,p(\concept) &  \\
&= p(\word \mid \bhbot, \bhpar)
p(\bhbot)\,\pdo(\bhpar \mid \concept)\,p(\concept)  \label{eq:do-joint-derivation}
\end{align}
\end{subequations}


\paragraph{Erasure.}

Given \cref{eq:do-joint-derivation}, we have the following joint distribution
\begin{subequations}
\begin{align}
\pdo(\concept, \bhbot) &= \sum_{\bhpar \in \repspacepar} \sum_{\word \in \alphabet} \pdo(\word, \bhbot, \bhpar, \concept)   \\
&= \sum_{\bhpar \in \repspacepar} \sum_{\word \in \alphabet} p(\word \mid \bhbot, \bhpar)
p(\bhbot)\,\pdo(\bhpar \mid \concept)\,p(\concept) \\
&= \sum_{\bhpar \in \repspacepar} \underbrace{\left(\sum_{\word \in \alphabet} p(\word \mid \bhbot, \bhpar) \right)}_{=1}
p(\bhbot)\,\pdo(\bhpar \mid \concept)\,p(\concept)  \\
&= \underbrace{\left(\sum_{\bhpar \in \repspacepar} 
\pdo(\bhpar \mid \concept) \right)}_{=1}\,p(\bhbot)\,p(\concept) &  \\
&= p(\bhbot)\,p(\concept) & \label{eq:do-bot-c-independence}
\end{align}
\end{subequations}
The mutual information $\MI(\rvC ; \rvHerase)$ can be computed as follows 
\begin{subequations}
\begin{align}
    \MI(\rvC ; \rvHerase) &= \sum_{\concept \in \concepts}\sum_{\bhbot \in \repspacebot} \pdo(\concept, \bhbot) \log \frac{\pdo(\concept, \bhbot)}{p(\concept) p(\bhbot)} & \\
    &= \sum_{\concept \in \concepts}\sum_{\bhbot \in \repspacebot} \pdo(\concept, \bhbot) \log \frac{p(\bhbot)\,p(\concept)}{p(\concept) p(\bhbot)} & \justify{applying \cref{eq:do-bot-c-independence}}  \\
    &= 0 < \varepsilon \label{eq:graph-erasure-proof} &
\end{align}
\end{subequations}
for every $\varepsilon > 0$.



\paragraph{Encapsulation.}
The following equation holds given \cref{eq:do-joint-derivation}
\begin{subequations}
\begin{align}
     \MI(\rvC; \rvH) - \MI(\rvC; \rvHconcept) &= \MI(\rvC; \rvHconcept,\rvHerase ) - \MI(\rvC; \rvHconcept) &  \justify{$\rvH = \rvHerase, \rvHconcept$} \\
    &= \MI(\rvC; \rvHerase \mid \rvHconcept ) &  \\
    &= \MI(\rvC; \rvHerase ) & \justify{$\rvHerase, \rvHconcept$ are independent (\cref{sec:counterfactual})}  \\
    &= 0 < \varepsilon & \justify{applying \cref{eq:graph-erasure-proof}}   \\
\end{align}
\end{subequations}


\paragraph{Containment.}
The following joint distribution can be derived from \cref{eq:do-joint-derivation}
\begin{subequations}
\begin{align}
 \pdo(& \word, \bhpar, \concept = \concept)  
=  \sum_{\bhbot \in \repspacebot} \pdo(\word, \bhbot, \bhpar, \concept = \concept)  & \\
=&  \sum_{\bhbot \in \repspacebot} p(\word \mid \bhbot, \bhpar)
p(\bhbot)\,\pdo(\bhpar \mid \concept = \concept)\,p(\concept = \concept)  & \\
=&  \sum_{\bhbot \in \repspacebot} \frac{p(\word, \bhbot, \bhpar)}{p(\bhbot, \bhpar)}
p(\bhbot)\,\pdo(\bhpar \mid \concept = \concept)\,p(\concept = \concept)  & \\
=& \sum_{\bhbot \in \repspacebot} \frac{p(\word, \bhbot, \bhpar)}{\cancel{p(\bhbot)}\,p(\bhpar)} \cancel{p(\bhbot)} \,\pdo(\bhpar \mid \concept = \concept)\,p(\concept = \concept)  & \justify{$\rvHerase, \rvHconcept$ are independent (\cref{sec:counterfactual})}  \\
=& \underbrace{\sum_{\bhbot \in \repspacebot} p(\word,\bhbot \mid \bhpar)}_{=p(\word \mid \bhpar)} \,\pdo(\bhpar \mid \concept = \concept)\,p(\concept = \concept)  &  \\
=& \, p(\word \mid \bhpar) \,\pdo(\bhpar \mid \concept = \concept)\,p(\concept = \concept)  &  \\
=& \, p(\word \mid \bhpar, \concept=\concept) \,p(\bhpar \mid \concept=\concept)  &  \justify{$\rvHconcept$ is deterministic given $\rvC$ } \label{eq:do-par-x-independence}
\end{align}
\end{subequations}
The mutual information $\MI(\rvX; \rvHconcept \mid \rvC = \concept)$ can be computed as follows 
\begin{subequations}
\begin{align}
&    \MI(\rvX; \rvHconcept \mid \rvC = \concept) \\
    &= \sum_{\word \in \alphabet} \sum_{\bhpar \in \repspacepar} \pdo(\word, \bhpar,\concept=\concept) \log \frac{\pdo(\word, \bhpar,\concept=\concept)}{p(\word \mid \concept=\concept)  p(\bhpar|\concept=\concept)} &  \\
    &= \sum_{\word \in \alphabet} \sum_{\bhpar \in \repspacepar} \pdo(\word, \bhpar,\concept=\concept) \log \frac{p(\word \mid \bhpar, \concept=\concept) \,p(\bhpar \mid \concept=\concept)}{p(\word \mid \concept=\concept) \,p(\bhpar \mid \concept=\concept)} & \justify{applying \cref{eq:do-par-x-independence}}  \\
    &= \sum_{\word \in \alphabet} \sum_{\bhpar \in \repspacepar} \pdo(\word, \bhpar,\concept=\concept) \log \frac{p(\word \mid \bhpar, \concept=\concept) \,p(\bhpar \mid \concept=\concept)}{p(\word \mid \bhpar , \concept=\concept) \,p(\bhpar \mid \concept=\concept)} & \justify{$\rvHconcept$ is deterministic given $\rvC$}  \\
    &= 0 < \varepsilon  & \label{eq:graph-containment-proof}
\end{align}
\end{subequations}


\paragraph{Stability.}
The following equation holds given \cref{eq:do-joint-derivation}
\begin{subequations}
\begin{align}
    & \MI( \rvX; \rvH \mid \rvC = \concept) - \MI(\rvX; \rvHerase \mid \rvC = \concept) & \\ = 
    &\MI(\rvX; \rvHerase, \rvHconcept \mid \rvC = \concept) - \MI(\rvX; \rvHerase \mid \rvC = \concept) & \justify{$\rvH = (\rvHerase, \rvHconcept)$}  \\
    = &\MI(\rvX; \rvHconcept \mid \rvHerase, \rvC = \concept) & \justify{conditional mutual information}  \\
    = &\MI(\rvX; \rvHconcept \mid \rvC = \concept) & \justify{$\rvHerase, \rvHconcept$ are independent (\cref{sec:counterfactual})}  \\
    = &0 < \varepsilon & \justify{applying \cref{eq:graph-containment-proof}}  
\end{align}
\end{subequations}

\end{proof}

\end{document}
