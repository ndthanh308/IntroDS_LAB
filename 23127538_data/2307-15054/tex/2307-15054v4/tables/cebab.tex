\begin{table*}[ht!]
    \centering
    %% Figure removed
    \small  
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l l C{0.09\textwidth} C{0.09\textwidth} C{0.09\textwidth} C{0.09\textwidth} C{0.09\textwidth} C{0.09\textwidth} C{0.09\textwidth} C{0.09\textwidth} C{0.09\textwidth}}%{ ccccc }
         \toprule
         & & \multicolumn{3}{c}{Binary Overall Sentiment} & \multicolumn{3}{c}{3-way Overall Sentiment} & \multicolumn{3}{c}{5-way Overall Sentiment} \\ 
         \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
         % & Gender & Gender & Number & Number \\
         % & gpt2-base-french & gpt2-base-french & gpt2-large & gpt2-large \\
         Model & Method & ICaCE-cosine ($\downarrow$) & ICaCE-L2 ($\downarrow$) & ICaCE-normdiff($\downarrow$) & ICaCE-cosine ($\downarrow$) & ICaCE-L2 ($\downarrow$) & ICaCE-normdiff($\downarrow$) & ICaCE-cosine ($\downarrow$) & ICaCE-L2 ($\downarrow$) & ICaCE-normdiff($\downarrow$)\\
         %& $\MIq(\rvC; \rvH)$ & $1 - \frac{\MIq(\rvC; \rvHerase)}{\MIq(\rvC; \rvH)}$ & $\frac{\MIq(\rvC; \rvHconcept)}{\MIq(\rvC; \rvH)}$ & $\frac{\MIq(\rvC; \rvHconcept) + \MIq(\rvC; \rvHerase)}{\MIq(\rvC; \rvH)}$ & $\MIq(\rvX; \rvH \mid \rvC)$ & $1 - \frac{\MIq(\rvX; \rvHconcept | \rvC)}{\MIq(\rvX; \rvH | \rvC)}$ & $\frac{\MIq(\rvX; \rvHerase | \rvC)}{\MIq(\rvX; \rvH | \rvC)}$ \\
         \midrule
            \modelnamef{bert-base-uncased} & Best CEBaB & \textbf{0.64\pms{0.05}} & \textbf{0.31\pms{0.00}} & \textbf{0.30\pms{0.00}} & \textbf{0.54\pms{0.04}} & \textbf{0.56\pms{0.00}} & \textbf{0.48\pms{0.00}} & 0.63\pms{0.01} & \textbf{0.74\pms{0.02}} & \textbf{0.54\pms{0.02}} \\
            \modelnamef{bert-base-uncased} & INLP & 0.79\pms{0.00} & 0.52\pms{0.05} & 0.52\pms{0.05} & 0.70\pms{0.01} & 0.58\pms{0.02} & 0.55\pms{0.01} & 0.60\pms{0.02} & 0.80\pms{0.02} & 0.72\pms{0.03} \\
            \modelnamef{bert-base-uncased} & Do-INLP & 0.78\pms{0.01} & 0.55\pms{0.01} & 0.55\pms{0.01} & 0.68\pms{0.01} & 0.62\pms{0.04} & 0.53\pms{0.02} & \textbf{0.54\pms{0.01}} & 0.77\pms{0.02} & 0.61\pms{0.03} \\
            \modelnamef{bert-base-uncased} & LEACE & 0.77\pms{0.01} & 0.36\pms{0.01} & 0.35\pms{0.01} & 0.78\pms{0.03} & 0.57\pms{0.01} & 0.53\pms{0.01} & 0.77\pms{0.03} & 0.80\pms{0.02} & 0.71\pms{0.02} \\
            \modelnamef{bert-base-uncased} & Do-LEACE & 0.75\pms{0.01} & 0.37\pms{0.01} & 0.37\pms{0.01} & 0.69\pms{0.02} & 0.57\pms{0.01} & 0.49\pms{0.01} & 0.58\pms{0.02} & 0.75\pms{0.02} & 0.61\pms{0.03} \\
        \midrule
            \modelnamef{gpt2} & Best CEBaB & \textbf{0.58\pms{0.01}} & 0.29\pms{0.00} & 0.29\pms{0.00} & \textbf{0.50\pms{0.01}} & \textbf{0.52\pms{0.01}} & \textbf{0.42\pms{0.01}} & \textbf{0.59\pms{0.01}} & \textbf{0.60\pms{0.02}} & \textbf{0.40\pms{0.01}} \\
            \modelnamef{gpt2} & INLP & 1.00\pms{0.00} & 0.49\pms{0.04} & 0.44\pms{0.03} & 1.00\pms{0.00} & 0.65\pms{0.04} & 0.52\pms{0.01} & 1.00\pms{0.00} & 0.72\pms{0.02} & 0.58\pms{0.02} \\
            \modelnamef{gpt2} & Do-INLP & 0.98\pms{0.02} & 0.30\pms{0.01} & 0.30\pms{0.00} & 0.99\pms{0.02} & 0.53\pms{0.01} & 0.51\pms{0.01} & 0.99\pms{0.01} & 0.68\pms{0.02} & 0.66\pms{0.02} \\
            \modelnamef{gpt2} & LEACE & 1.00\pms{0.00} & 0.31\pms{0.01} & 0.30\pms{0.00} & 1.00\pms{0.00} & 0.53\pms{0.01} & 0.51\pms{0.01} & 1.00\pms{0.00} & 0.68\pms{0.02} & 0.66\pms{0.02} \\
            \modelnamef{gpt2} & Do-LEACE & 0.99\pms{0.06} & \textbf{0.28\pms{0.00}} & \textbf{0.28\pms{0.00}} & 0.99\pms{0.04} & \textbf{0.52\pms{0.01}} & 0.51\pms{0.01} & 0.99\pms{0.02} & 0.68\pms{0.02} & 0.67\pms{0.02} \\
        \midrule
            \modelnamef{roberta-base} & Best CEBaB & \textbf{0.70\pms{0.03}} & \textbf{0.29\pms{0.01}} & \textbf{0.29\pms{0.01}} & \textbf{0.62\pms{0.02}} & 0.55\pms{0.01} & 0.48\pms{0.00} & 0.64\pms{0.01} & \textbf{0.78\pms{0.01}} & \textbf{0.59\pms{0.01}} \\
            \modelnamef{roberta-base} & INLP & 0.80\pms{0.00} & 0.32\pms{0.02} & 0.32\pms{0.02} & 0.72\pms{0.01} & 0.55\pms{0.00} & 0.54\pms{0.01} & 0.59\pms{0.01} & 0.84\pms{0.01} & 0.81\pms{0.01} \\
            \modelnamef{roberta-base} & Do-INLP & 0.79\pms{0.00} & 0.30\pms{0.06} & 0.30\pms{0.06} & 0.70\pms{0.01} & \textbf{0.52\pms{0.01}} & 0.47\pms{0.02} & \textbf{0.56\pms{0.01}} & 0.80\pms{0.00} & 0.72\pms{0.01} \\
            \modelnamef{roberta-base} & LEACE & 0.82\pms{0.01} & 0.31\pms{0.01} & 0.31\pms{0.01} & 0.83\pms{0.01} & 0.54\pms{0.01} & 0.51\pms{0.01} & 0.83\pms{0.01} & 0.83\pms{0.01} & 0.80\pms{0.01} \\
            \modelnamef{roberta-base} & Do-LEACE & 0.77\pms{0.01} & 0.31\pms{0.01} & 0.31\pms{0.01} & 0.70\pms{0.01} & 0.53\pms{0.01} & \textbf{0.47\pms{0.01}} & 0.59\pms{0.01} & 0.81\pms{0.01} & 0.76\pms{0.01} \\
         \bottomrule
    \end{tabular}%
    }
    \caption{CEBaB benchmark results for our causal do-intervention. In three sets of columns, we report empirical Individual Causal Concept Effect (ICaCE) losses for 2-, 3- and 5-class overall sentiment prediction. Three types of losses are reported, lower is better for all three. \textit{ICaCE-cosine} indicates whether the estimated and observed effect have the same direction, without accounting for magnitude. \textit{ICaCE-L2} compares the Euclidean norm of the difference of the estimated and observed effect, and therefore reflects both magnitude and direction differences. \textit{ICaCE-normdiff} is the absolute difference between the Euclidean norms of the observed and estimated effects, thereby honing in on magnitude differences. Each entry shows mean $\pm$ standard deviation over random restarts. We report results for the best performing method from CEBaB \citep{abraham-etal-2022-cebab}, INLP \citep{ravfogel-etal-2020-null}, and LEACE \citep{belrose2023leace}. We apply our causal do-intervention from \cref{sec:causal-con-gen}, this time in a classification setting, using $\bPk$ returned by INLP and LEACE. See \cref{sec:cebabresults} for discussion of these results.}
    \label{tab:cebabresults}
 % \vspace{-15pt}
\end{table*}
