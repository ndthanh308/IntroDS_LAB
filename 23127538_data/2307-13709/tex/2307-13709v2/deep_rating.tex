\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref} 
\usepackage{url}
\usepackage{booktabs} 
\usepackage{amsfonts} 
\usepackage{nicefrac}
\usepackage{microtype} 
\usepackage{lipsum}	
\usepackage[dvipdfmx]{graphicx}
\usepackage{bm}
\usepackage[square,numbers]{natbib}
\usepackage{doi}

\usepackage{amssymb, amsmath}
\newtheorem{cond}{Condition}[section]

\title{Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items}

\author{Satoru Fujii \\
	Graduate School of Human and Environmental Studies\\
	Kyoto University\\
	\texttt{fujii.satoru.75c@st.kyoto-u.ac.jp} \\
}

\begin{document}
\maketitle

\begin{abstract}
Many properties in the real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior works have primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in the training data. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.
\end{abstract}


% keywords can be removed
\keywords{Neural Network \and Bradley-Terry Model \and Rating}

\section{Introduction}

There exists a multitude of properties that humans recognize. For some of them, we have well-defined metrics: grams for the weight, dollars for the price, or sometimes it's just numbers for the count. However, a considerable number of properties can't be simply measured like them, and some of them aren't even directly observable: We don't have metrics for "attractiveness", nor can we directly observe "strength" of a deck in card games. Our goal is to quantify and evaluate those properties of items from their vector representation or explanatory variables.

As for estimating competitiveness of certain players in competitive environment, many rating algorithms based on mathmatical models like Bradley-Terry Model\cite{bradley1952rank} has been reserched. These methods allow us to quantify strengh of competitors based on their match histories: winning against strong competitors makes their estimated rating higher, and vice versa. However, those methods can be used only on known competitors, thus can't predict properties of unseen items which aren't present in match histories.

Generalizing estimations for unseen data is what machine learning does. As recent advancements in deep learning enabled us to do a lot of things previously impossible, in this paper we introduce \textbf{Deep Bradley-Terry Rating (DBTR)}, a ML framework which seemlessly integrate traditional Bradley-Terry-model based rating with neural networks. This network learns quantification of any properties, based on paired comparison dataset that contains information about which item won the comparison. We also introduces a generalized variant which discounts unfairness of the comparison to estimate ratings in theoretical fair ground, thus allows us to use the method on asymmetric environment. For example, our framework can be used to obtain a estimator of:
\begin{itemize}
	\item Attractiveness of the name and preview texts/thumbnails, by learning what was chosen and clicked on internet search engine, e-commerce service or online video platform.
	\item Strength of a deck in card games, by learning match histories.
	\item Appeal to mouth of an image of food, by learning a result of questionnaire about which image looks more delicious.
\end{itemize}

Moreover, the output of DBTR estimator is not just any numbers. Good metrics have more meanings than just being high or low, which wouldn't be the case if we simply learn and estimate user ratings like 1-5 star scores. Since our method is based on Bradley-Terry model, the rating it produces can be used to caluculate winning probabilities against others.

We empirically shows that our methods can successfully obtain good estimators using controlled settings. We also shows the validity our network structure by comparing several possible variants in our experiment.

% (examples)
% (experimental results).

\section{Notation and Background}

\subsection{Rating}

\subsubsection{Bradley-Terry Model}

\textbf{Bradley-Terry score} $\pi_i$ is a scalar which represents the strength of player $i$. Let $W_{ij}$ be the winning probability of player $i$ against player $j$. Bradley-Terry model\cite{bradley1952rank} assumes:
\begin{equation}
W_{ij} = \frac{\pi_i}{\pi_i + \pi_j}
\end{equation}

Note that $(\pi_1, \dots ,\pi_N)$ is scale-free. In other words, $(\pi_1, \dots ,\pi_N)$ is the same as $(c\pi_1, \dots , c\pi_N)$ for any constant $c$ in terms of winning probabilities. We can simply assume $\sum_k \pi_{k} = N$ to uniquely determine $(\pi_1, \dots ,\pi_N)$.

Given this model, we can caluculate Maximum Likelihood Estimation (MLE) of $(\pi_1, \dots ,\pi_N)$ from a match history among player $1, \cdots , N$. Note that there are situations where calculating MLE is infeasible. For example, if player $i$ won at least once and never lost a single match, MLE of $\pi_i$ will diverge to infinity. More preceisely, the following condition should be met\cite{doi:10.1080/00029890.1957.11989117}:

\begin{cond} \label{condi}
In every possible partition of the players into two nonempty subsets, some player in the second set beats some player in the first set at least once.
\end{cond}

Numerical approaches such as MM algorithm\cite{hunter2004mm} are commonly used to approximately caluculate those values. Incremental methods like the ones we discuss in \ref{elosub} are also widely used, although their outputs aren't MLE scores.

\subsubsection{Generalizations of Bradley-Terry Model}
Bradley-Terry model can be generalized for settings where matches are held among more than 2 players by assuming the winning rate of player $i$ in a match among player $1, \dots , M$ to be $\pi_i / \sum_{k = 1}^M \pi_k$\cite{hunter2004mm}. This can be also appllied to a situation where the ranks of players are decided rather than just a single winner, by regarding the 2nd place to be the winner of the competition of remaining $M-1$ players.

For asymmetric environment where players competes in unfair settings, a variant of Bradley-Terry model\cite{hunter2004mm} also has been proposed. It assumes
\begin{equation} \label{home}
W_{ij} = \frac{\eta \pi_i}{\eta \pi_i + \pi_j}
\end{equation}
where $\eta > 0$ is the strength of home-field advantage of player $i$. 

\subsubsection{Elo Rating} \label{elosub}
Elo Rating\cite{elo1978rating} is a incremental approach to estimate strength. It defines a \textbf{rating} of player $i$ as
\begin{equation} \label{elo}
R_i := \alpha \log_{10} \pi_i + \beta
\end{equation}
usually with $\alpha = 400, \beta = 1500, \sum_k \pi_{k} = N$. Using logarithm for rating prevents the value from scaling too much. %scaling in a too steep curve?

In Elo Rating, every player starts with rating of $\beta$. When player $i$ wins $W$ times among $G$ matches against player $j$, rating of player $i$ will be refreshed as
\[
R'_i \gets R_i + K (W - G W_{ij}) = R_i + K (W - G \dfrac{1}{10^\frac{R_j - R_i}{\alpha} + 1})
\]
where $K$ is a learning rate.

Microsoft Research proposed TrueSkill\cite{herbrich2006trueskill}, which combined the probability graph model with Elo Rating and achieved increased accuracy and convergence speed.

\subsection{Neural Network}

\subsubsection{Feedforward Neural Network}

The most simple structure of Neural Network (NN) is feedforward neural network. Feedforward neural network consists of multiple layers of function which can be written as $\bm{x} \mapsto f(W\bm{x} + \bm{b})$,  where activation function $f$ is a non-linear function, and matrix $W$ and vector $\bm{b}$ is a parameter. Let $\theta$ be all parameters NN has. The difference between target label and output of NN on data $i$ is evaluated by loss function $L_i$, and NN learns better parameters by gradually moving those parameters in the direction of $\partial L / \partial \theta_i$,  where $L$ is an average of $L_i$ within mini-batch of training samples.

%softmax説明

\subsubsection{NN Modifications and Expansions}

In some network structures, we want to use shared weights for certain parameters. This idea is introduced in the context of natural language processing\cite{inan2016tying}.

Skip connection\cite{he2016deep} is a method which directly adds the output of some layer to another, which was introduced and proved to be successful on image recognition.

Using NN structure to trasform a data into a vector with lower dimensionality is a commonly accepted idea. Autoencoder\cite{hinton2006reducing} is arguably the first one of those methods, which learns dimensionality reduction by using the input itself as a learning target with a small hidden layer.

\section{Related Work}

There has been plenty of researchs around user rating prediction (such as 1-5 star ratings). Collaborative Filtering (CF)\cite{Jalili2018EvaluatingCF} is an actively researched field, which aims to predict user ratings based on the past ratings. Xi et al.\cite{9446537} proposed Deep Rating and Review Neural Network (DRRNN), which uses review text as an additional target for back-propagation to avoid the problem of using user rating as ground truth.

There also has been a small amount of researchs which combines neural network with Bradley-Terry model. 

Zhao et al.\cite{gorating} used neural network to estimate ratings of Go players present in match history, combined with estimated intermediate winning rate during a match and history decay of rating, and outperformed traditional rating algorithms including Elo and TrueSkill. 

Li, Ma and Hu\cite{li2021neural} used neural network to predict image beauty scores. They caluculated MLE of Bradley-Terry scores of images from paired comparison datasets in a traditional way, then used winning probabilities based on those scores as target label in the training of NN. Although our basic ideas have several similarities, since their work is focused on their specific image beauty task, there are important differences between our approaches and theirs, as our method donsn't require any outside pre-calculation of Bradley-Terry scores, only using the one-hot winner information as the target label. This not only simplifies the implementation, but also allows us to properly weight the importance of training data, which is especially important on a data-collecting environment which isn't statistically controlled. In those cases, Condition\ref{condi} might not be hold, which makes the caluculation impossible. Even if that's not the case, Bradley-Terry scores for a competitor with a small number of matches would be less reliable. We also generalized the architecture further for asymmetric environments, which we see more often in real world settings. Caluculating MLE score is infeasible on those environments without using some simplification like Formula (\ref{home}). 

\section{Descriptions of DBTR}

\subsection{Symmetric Setting}
In this section we describe DBTR. The goal of DBTR is estimate rating (represents the quantfication of some properties) of unseen data which isn't present in training dataset.
It uses a dataset where each entry contains explanatory variables of $M(\geq 2)$ items (not necessarily human competitors), and the results of comparison $\bm{y}$, like which athlete won the match, or which pair of thumbnail and text was clicked. $\bm{y}$ is a $M$-dimensional one-hot vector which represents what item won the comparison.

Let $\bm{x}_i$ as the vector of explanatory variables of competitor $i$. Our goal is to obtain a NN we call \textbf{rating estimator} $E: \bm{x}_i \mapsto R_i$, where scalar $R_i$ is the rating of item $i$. To obtain $R$, we connects the outputs of $M$ rating estimators with shared weight via softmax function $(R_1, \cdots R_M) \mapsto (c\pi_1, \cdots, c\pi_M)$ where
\[
\pi_i = e^{R_i}, \ c = \dfrac{1}{\sum_{k=1}^M \pi_i}
\]
then uses $\bm{y}$ as a target label with cross entropy loss. This structure is shown in Figure \ref{structure_symmetric}.

Since NN learns its parameters to minimize the difference between $(c\pi_1, \cdots, c\pi_M)$ and actual comparison result $\bm{y}$, $\pi_i$ should be the equivalent of Bradley-Terry scores $\pi_i$ after enough training, as we used the same notations for them. Since $R_i = \log \pi_i$, $R_i$ should be equivalent of Elo Rating values, where $\alpha = 1, \ \beta = \log 10$ with a certain scale of $\pi_i$ in Formula (\ref{elo}).

Note that DBTR is incapable of considering any rock-paper-scissors relations between items because we separated rating estimators for each items and each of their outputs is a scalar which only contains the most crucial information for estimating the result of comparison: rating itself. Since we uses weight sharing, rating estimators should caluculate ratings of each item in the same way, thus we can obtain a single rating estimator.

\subsection{Asymmetric Setting} \label{asym}
In many real world environments, comparisons aren't made on a fair ground. Take this as an example: on online shopping platform, we choose a product we buy. On search engine, we choose a web site we visit. In those cases, we usually prioritize the ones shown in the upper side, and we rarely keep scrolling down and going next pages until we hit the bottom, which results in unfair comparisons. However, we still want to use DBTR on those situations to estimate ratings on a hypothetical environment where those items are compaired on a fair condition.

To achieve this, we insert a NN which we call \textbf{advantage adjuster}, which takes $(c\pi_1, \cdots, c\pi_M)$ as an input, and aims to return the probability distibutions of each item winning the comparison, based on input and learned unfairness of environment. We also uses skip connection around it to prevent the situation where rating estimator deviates from original rating settings we discussed (for example: learns to output $(-5 \pi_1 + 7, ... , -3 \pi_M + 2)$) and still advantage adjuster adapt to it and properly predict the outcome. We believe using skip connection can solve this problem since $A$ would learn to do nothing if the environment turned out to be fair. This structure is shown in Figure \ref{structure_asymmetric}.

In a case where there considered to be factors which affects the unfairness of the environment outside of each $\bm{x}_i$, it is also possible to add a environment vector $\bm{e}$ as an additional input to advantage adjuster. For example, home-court advantage might be stronger on a sunny day due to more spectators. In this case, advantage adjuster can receive information about weather as an additional input.

% Figure environment removed

% Figure environment removed

\section{Experiments}

\subsection{Experiment on a Symmetric Setting} \label{sym_exp}
Firstly, to measure the performance of our method in a symmetric setting, we customized MNIST\cite{lecun1998gradient} dataset in a way that each entry has 2 images of handwritten letters and the result of comparison, which is simply determined by which number is higher (random for ties). %Note that this is rather harsh settings than determining the result according to the ratio of 2 numbers, which would be in favor of the assumption of Bradley-Terry model.

For $E$, we used a simple NN with 2 hidden layers consist of 512 nodes each. We used ReLU for activation function and Adam\cite{kingma2014adam} for optimizer. We transformed 60000 and 10000 entries of data into a training and test dataset with the same number of entries, as each image on original dataset was compared 2 times in our transformation: with the image above and below. The number of epochs were 5. We tried not to optimize network parameters as that is not the goal of this paper.

Figure\ref{exp_mnist_sym} is a scatter chart of the first 1000 entries in test data with the actual number of the image and the output of rating estimator $R_i$. This shows that our method successfully learned the quantfication "number" in a manner that aligns with our data settings, only from the results of comparison.

In a world where we know all actual numbers of the image, the ratio of $\pi_i : \pi_j$ would be $1 : \infty$ when the actual number of image $i$ lower by 1 than image $j$ since there's no chance $i$ would win against $j$. However in our setting, the fuzziness of letters prevents this from happening. For exapmle, the average value of $R_i$ on images of number 1 and 2 were 2.83 and 6.41 as shown in Table \ref{table}, which would make $p_i$ 16.86 and 610.37, 97.31\% winrate of 2 against 1. This would make sense considering the existence of a handwritten letter which really looks like 1 while it's actually 7 or 9 with extremely small curve on top.

\subsection{Experiment on an Asymmetric Setting}
Secondly, to measure the validity of our network structure in an asymmetric setting, we again customized MNIST dataset mostly in the same way, except that this time the result labels were determined by the comparison of (1.4 $\times$ the number of the left image + 0.1) with the number of the right image. We used a simple 2 nodes linear layer for $A$, and the rest of the settings were the same as \ref{sym_exp}.

We compared three network structures in this experiment: one without advantage adjuster, one with advantage adjuster but without skip connection, and the one with both, which we propose. They achieved 82.8\%, 93.7\% and 94.6\% accuracy on test dataset during the training, and the result of rating estimastions are shown in Figure \ref{exp_mnist_asym_noadj}, \ref{exp_mnist_asym_noskip}, \ref{exp_mnist_asym} respectively. Note that due to the scale-free nature of Bradley-Terry scores, we will focus on the shape of the curve rather than the scale of the output values.

As shown in Figure \ref{exp_mnist_asym_noadj}, without advantage adjuster, rating estimations were distorted by environmental unfairness. The structure without skip connection achieved almost the same high accuracy during the training, rating estimator failed to learn desired quantification as shown in Figure \ref{exp_mnist_asym_noskip}, we believe this is caused by the degree of freedom around $A$ as we discussed in \ref{asym}. Our proposing structure was able to denoise the unfairness of the environment and obtained similar curve of rating estimation to the one in a symmetric setting, as shown in Figure \ref{exp_mnist_asym}

% Figure environment removed

% Figure environment removed

\begin{table}[tbp]
\begin{center}
{\tabcolsep = 0.65cm
\begin{tabular}{l|llll}
\multicolumn{1}{c|}{}  & \multicolumn{1}{c}{Symmetric} & \multicolumn{1}{c}{Asym wo/ A} & \multicolumn{1}{c}{Asym with A wo/ $\oplus$} & \multicolumn{1}{c}{Asym with A and $\oplus$} \\ \hline
\multicolumn{1}{c|}{0} & 0.06 $\pm$ 0.39 & 0.24 $\pm$ 1.27    & 35.71 $\pm$ 7.73   & 0.13 $\pm$ 0.65 \\
\multicolumn{1}{c|}{1} & 2.83 $\pm$ 0.51 & 3.07 $\pm$ 0.61    & 23.46 $\pm$ 2.71   & 1.71 $\pm$ 0.38 \\
\multicolumn{1}{c|}{2} & 6.41 $\pm$ 2.18 & 7.25 $\pm$ 1.47    & 11.68 $\pm$ 2.78   & 5.31 $\pm$ 1.69 \\
\multicolumn{1}{c|}{3} & 8.20 $\pm$ 1.60 & 9.35 $\pm$ 0.80    & 5.7 $\pm$ 2.02     & 8.17 $\pm$ 1.31 \\
4                      & 10.72 $\pm$ 1.64 & 9.60 $\pm$ 1.17   & 4.61 $\pm$ 1.27    & 9.39 $\pm$ 1.30 \\
5                      & 12.40 $\pm$ 1.82 & 10.53 $\pm$ 0.62  & 2.48 $\pm$ 1.07    & 11.33 $\pm$ 1.51 \\
6                      & 13.73 $\pm$ 1.99 & 11.31 $\pm$ 1.37  & 1.9 $\pm$ 2.94     & 12.71 $\pm$ 1.84 \\
7                      & 15.69 $\pm$ 2.04 & 11.69 $\pm$ 0.55  & 0.26 $\pm$ 1.01    & 14.57 $\pm$ 1.40 \\
8                      & 17.71 $\pm$ 2.76 & 11.61 $\pm$ 0.75  & 0.39 $\pm$ 1.76    & 15.08 $\pm$ 2.31 \\
9                      & 21.85 $\pm$ 2.78 & 11.92 $\pm$ 0.54  & 0.08 $\pm$ 0.70    & 17.51 $\pm$ 2.05                                                   
\end{tabular}
}
\caption{Average and standard deviation of $E$'s output} \label{table}
\end{center}
\end{table}

% \subsection{Experiment on Pokemon Datasets}
% Thirdly, we shows an small example of actual usage of our framework, closer to what we envision. We used Weedle's Cave\cite{pokemon} dataset, which contains match results between pokemons, generated by a custom algorithm which omits some charactaristics of the game. As explanatory variables, we used 6 base stats and 18-dimensional 0-1 vector which represents whether a pokemon has a certain type or not.

\section{Conclusions}
In this paper, we proposed DBTR, a ML framework to quantify a property and estimate those values of unknown items by integrating Bradley-Terry model into neural network structures. Our method successfully learned and generalized the estimation of "number" in both symmetric and asymmetric experimental setting.

Our framework provides a new ground for data mining and poses an alternative of data gathering format, especially on the environment where comparison data is structurally easier to collect. There are tons of online platforms where we choose to click something, and we can just record those choises as training data. Same can be said for deck-building card games or the ones with similar mechanics like Pokemon, as it creates a lot of match results between decks or parties. Even when that's not the case, gathering a data of comparisons or rankings by questionnaire might be sometimes a better option than asking people to rate them on a scale of 1 to 5, since predicting those five-grade scores doesn't have any additional meanings other than it's high or low, while our ratings provides an strong insight about the outcome of comparisons between items. Besides, it's arguably easier to answer "which do you prefer" questions and it can discrern smaller differences with higher resolving power than 5-grade evaluation.

On asymmetric environment, advantage adjuster would help us to understand the strength of unfariness of the environment, since DBTR learns it simultaneously along with quantfication of the property. Using a small linear layer for $A$ like we did in our experiment should make it easily explainable.

Feature importance explanation methods like DeepSHAP\cite{lundberg2017unified} can be useful to understand what feature matters more for strength, when used to a DBTR estimator. Unlike doing so on a normal NN predictor of the winner, we believe it would prevent the feature relates to rock-paper-scissors mechanics from getting high importance. In other words, feature importance on a normal classifier would tell you what decides the winner of the match, while the one on a DBTR estimator would tell you what decides overall strength.

Since the paradigm we introduced is novel, there aren't many real world datasets that fits into our framework, which narrowed our options of experiments in this paper. We believe applying our method to more meaningfull dataset would be an important future work.

%accelaration

\bibliographystyle{unsrtnat}
\bibliography{biblo}

\end{document}