
%% bare_jrnl_comsoc.tex
%% V1.4\elseb
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Communications Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.   ***   ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal,comsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal,comsoc]{../sty/IEEEtran}


\usepackage[T1]{fontenc}% optional T1 font encoding
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{url}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{bbding}

\urlstyle{same}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\cmmnt}[1]{}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
% Do NOT use the amsbsy package under comsoc mode as that feature is
% already built into the Times Math font (newtxmath, mathtime, etc.).
% 
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


% Select a Times math font under comsoc mode or else one will automatically
% be selected for you at the document start. This is required as Communications
% Society journals use a Times, not Computer Modern, math font.
\usepackage[cmintegrals]{newtxmath}
% The freely available newtxmath package was written by Michael Sharpe and
% provides a feature rich Times math font. The cmintegrals option, which is
% the default under IEEEtran, is needed to get the correct style integral
% symbols used in Communications Society journals. Version 1.451, July 28,
% 2015 or later is recommended. Also, do *not* load the newtxtext.sty package
% as doing so would alter the main text font.
% http://www.ctan.org/pkg/newtx
%
% Alternatively, you can use the MathTime commercial fonts if you have them
% installed on your system:
%\usepackage{mtpro2}
%\usepackage{mt11p}
%\usepackage{mathtime}


%\usepackage{bm}
% The bm.sty package was written by David Carlisle and Frank Mittelbach.
% This package provides a \bm{} to produce bold math symbols.
% http://www.ctan.org/pkg/bm





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "% Figure environment removed


\subsection{Overview}
\label{sc:Overview}

The proposed framework is built on a multilingual text processing front-end, which supports both Chinese and English. The front end encodes Chinese text input into labels of phonemes, tones, word boundaries, and prosodic boundaries while encoding English text input into labels of phonemes. Note that the Chinese phoneme set is based on Pinyin, while English is based on CMU-Dict. Therefore, we merge the Chinese and English phoneme sets for unified textual inputs of the bilingual TTS system.

As shown in Figure~\ref{fig_1}, the backbone of METTS is based on DelightfulTTS~\cite{DBLP:journals/corr/abs-2110-12612}, which consists of a text encoder, a variance adaptor, and a mel-spectrogram decoder. Notably, the improved Conformer~\cite{DBLP:conf/interspeech/GulatiQCPZYHWZW20} structure better model local and global dependency of mel-spectrogram, which has led DelightfulTTS to win the Blizzard Challenge 2021.
In general, METTS updates DelightfulTTS with \textit{multi-scale emotion modeling} to achieve natural multilingual emotional TTS by both emotion reference (METTS-REF) and ID (METTS-ID) as the control signal. Specifically, the coarse-grained emotion embedding is provided by a GST layer or an emotion matcher, while the fine-grained emotion embedding is obtained from the CVAE module. 
Moreover, a perturb module is introduced to distort the speaker timbre of the reference signal for decoupling timbre from speech. 
The speaker embedding is obtained through a lookup table and is added to the output of the variance adaptor, which is then fed into the mel-spectrogram decoder to synthesize the final mel-spectrogram.


\subsection{Multi-scale emotion modeling}
\label{sc:Multi-scale emotion modeling}

In general, the emotional expressions of multi-lingual speech could be factorized to the shared prosody pattern and distinct fine details of prosody due to the different manners of pronunciation. METTS utilizes the Global Style Tokens (GST) and Conditional Variational Autoencoder (CVAE) modules to establish coarse-grained language-agnostic and fine-grained language-specific emotional representations. 

The GST module employs a reference encoder to encode mel-spectrogram into a hidden representation and utilizes multi-head attention to calculate the global emotional style tokens. Notably, L2 normalization is applied to the global emotional representation, eliminating magnitude-related information that may vary across languages and speakers. This normalization improves the generalization ability of the model, allowing for emotion embedding control based solely on angular information geometrically~\cite{DBLP:conf/smc/KimLLJL21}. By mapping the emotional representation of different languages to the same global tokens, the GST module forms the language-agnostic representation.

The CVAE module focuses on learning fine-grained emotion expression from the mel-spectrogram, with text and coarse-grained emotion conditions. It utilizes a conformer block and a GRU layer to extract frame-level emotion embeddings, which are then downsampled to the phoneme level based on duration. These phoneme-level embeddings are used to derive the mean and variance of the distribution of phoneme-level prosody. Additionally, taking inspiration from VITS~\cite{DBLP:conf/icml/KimKS21}, a fine-grained predictor is designed to predict the distribution of fine-grained emotion from text. To improve the expressiveness of the predicted distribution, a normalizing flow technique is employed, enabling an invertible transformation from a simple distribution to a more complex one. By learning fine-grained emotional representations that are consistent with the text, the CVAE module forms the language-specific representation.

To ensure that the extracted multi-scale representations are relevant to emotions, even for training data without emotion annotations, a semi-supervised strategy is employed, which includes an emotion classifier. Specifically, only the embeddings of annotated audio are used to supervise the emotion classifier for both coarse- and fine-grained representations. The audio without emotion annotations is not involved to optimize the emotion classifiers and is utilized to train the acoustic model by the extracted embeddings. Furthermore, the frame and phoneme emotion embeddings are processed through a GRU layer to extract a single vector, which is then used as input to the emotion classifier.

\subsection{Speaker disentanglement based on information perturbation}

In our multilingual emotional speech synthesis setup, where each speaker is mono-lingual and only some speakers in the training set have emotional speech data, the entanglement of speaker timbre with emotion and language poses a challenge, resulting in synthetic speech with low speaker similarity and unusual emotional expression and pronunciation.

To address this issue more comprehensively, we adopt a pre-processing step that utilizes a signal perturbation module to remove the speaker timbre information. Specifically, we apply a dynamic \textit{formant} shift to the mel-spectrogram of the reference speech. Speech formants are primarily determined by the size, shape, and position of the vocal tract, which are highly specific to each speaker and represent their vocal identity~\cite{DBLP:journals/taslp/YooLY15}. By performing a formant shift function, denoted as $fs$, on the original waveform $Wave$, we obtain a speaker-independent signal denoted as $\widetilde{Wave} = fs (Wave)$.

Subsequently, we extract the mel-spectrogram of the perturbed wave, denoted as $\widetilde{Mel}$, which serves as the input for emotion representations extraction. The perturbation module perturbs the timbre of the recordings at a random scale by each step during training, allowing the GST and CVAE modules to learn a speaker-independent representation and effectively disentangle the speaker's timbre from speech.

% Figure environment removed

\subsection{VQ based emotion matcher}

%During inference, the utilization of different coarse-grained language-agnostic emotion representations extracted from different references leads to variations in synthetic speech. 
During inference, different reference usually lead to different emotional expression of the synthetic speech. The selection of an appropriate reference is crucial for achieving natural speech and accurate emotional expression. Therefore, we propose an emotion matcher that automatically matches the most suitable reference embedding based on the textual content. Figure~\ref{fig_2} illustrates the architecture of the emotion matcher, which takes the text encoder output and the language ID as input and generates the optimal reference embedding, facilitating reference-free inference in METTS.

Directly modeling the complex relationship between bilingual textual representation and emotional representation is quite challenging, so we employ VQ to quantize the coarse-grained emotion representation, forming a reference pool. Subsequently, we predict the correct codebook from the reference pool for the current utterance. This transforms the intricate regression task into a simpler classifier task, simplifying the modeling process.


Specifically, in our approach, we begin by extracting the coarse-grained emotional representation and predicted emotion ID for all training audio samples using the GST layer and the emotion classifier. Subsequently, we apply VQ to quantize the coarse-grained emotional representation. To achieve this, we employ the k-means algorithm to obtain $N$ clusters for each emotion category, resulting in a total of $N \times M$ reference embeddings ($M$ is the number of emotion categories), which form the reference pool.

To select the appropriate reference from the pool, the emotion matcher utilizes a multilayer perceptron (MLP) to generate a text-emotion vector. This vector captures the contextual information related to emotion by taking the text encoder output and emotion ID as inputs. Using the text-emotion vector and the embedding-candidate pool, we calculate the correlation coefficient (CC) matrix between them. The calculation of CC is similar to the process of Scaled Dot-Product Attention~\cite{DBLP:conf/nips/VaswaniSPUJGKP17} and is defined as:

\begin{equation}
CC(V_{t},E_{c}) = \text{softmax}\left(\frac{V_{t}E_{c}^T}{\sqrt{d_{E_C}}}\right),
\end{equation}
where $V_t$ and $E_c$ represent the text-emotion vector and embedding candidates, respectively, and $d_{E_C}$ denotes the dimension of the embedding candidates. The softmax function is applied to normalize the correlation coefficients. The embedding with the highest correlation coefficient is selected as the coarse-grained emotion representation for the TTS system.

To ensure that the selected embedding corresponds to the input text, a matcher classifier is introduced to supervise the CC matrix. It ensures that the embedding with the highest correlation coefficient is the cluster center corresponding to the input text.

\subsection{Training and fine-tuning}
For flexible control of the generated emotional expressions, we use pre-training and fine-tuning procedures to conduct multilingual emotional speech synthesis from a reference signal and manual emotion ID, respectively.

The training objective of METTS-REF is
% \vspace{-10pt}
\begin{equation}
\begin{aligned}  
\mathcal{L}_{\mathrm{pretrain}} = & 0.05 * \mathcal{L}_{\mathrm{kl}} +  \mathcal{L}_{\mathrm{prosody}} + \\
& 0.1 * \mathcal{L}_{\mathrm{emo}} + \mathcal{L}_{\mathrm{ssim}}+  \mathcal{L}_{\mathrm{iter}},
\end{aligned}\label{eq:eq1}
\end{equation}
where $\mathcal{L}_{\mathrm{prosody}}$ is the L1 loss between the predicted pitch/energy/duration and the ground-truth pitch/energy/duration, $\mathcal{L}_{\mathrm{emo}}$ is the semi-supervised crossentropy loss of emotion classifier. $\mathcal{L}_{\mathrm{kl}}$ is the KL divergence to predict the phoneme-level emotion distribution from text encoder output, and $\mathcal{L}_{\mathrm{iter}}$ is the sum of mel-spectrogram L1 loss between the predicted and ground-truth mel-spectrogram in each Conformer block. Moreover, we use structural similarity $\mathcal{L}_{\mathrm{ssim}}$ ~\cite{DBLP:journals/tip/WangBSS04} to measure the similarity between predicted and ground-truth mel-spectrogram in the final Conformer block.

The purpose of fine-tuning is to support METTS-ID. 
During fine-tuning, we use the ground-truth clustering center as the coarse-grained emotion representation for the TTS model and jointly optimize the emotion matcher. To stabilize the joint training, we freeze the GST-layer and emotion classifier as a discriminator to distinguish the emotion category of the generated mel-spectrogram.

The fine-tuning objective is
% \vspace{-8pt}
\begin{equation}   
\mathcal{L}_{\mathrm{finetune}} = \mathcal{L}_{\mathrm{match}} + \mathcal{L}_{\mathrm{disc}} + \mathcal{L}_{\mathrm{base^{'}}},
\end{equation}
where $\mathcal{L}_{\mathrm{match}}$ is the cross entropy loss between the selected clustering centre and the actual clustering centre in the emotion matcher, $\mathcal{L}_{\mathrm{disc}}$ is the emotion classification loss of the predicted mel-spectrogram taking GST layer as an discriminator, and $\mathcal{L}_{\mathrm{base^{'}}}$ means removing $\mathcal{L}_{\mathrm{emo}}$ from the pre-trained model objectives. 

\begin{table*}[h]
\centering
\caption{Dataset for the multilingual emotional TTS.}
\setlength{\tabcolsep}{3.0mm}
\label{tab:data}
\begin{tabular}{c|c|ccccccc|c}
\toprule
\multirow{2}{*}{Corpus}  & \multirow{2}{*}{Language} & \multicolumn{7}{c|}{Emotion (sentences)}   & \multirow{2}{*}{ Usage} \\
 &     & Neutral & Happy  & Surprise &Sadness & Angry & Disgust & Fear &   \\ \midrule
CN1        & Chinese  &5k    &0.5k   &0.5k     &0.5k    &0.5k   &0.5k   &0.5k       &Training\&Evaluation               \\
CN1     & Chinese  &5k   &2k    &2k    &2k   &2k  &2k  &2k     &Training\&Evaluation   \\
EN1       & English  &10k   &-   &-     &-    &-   &-   &-        &Training\&Evaluation   \\ 
EN2      & English  &10k   &-    &-     &-    &-   &-   &-       &Training\&Evaluation   \\ \bottomrule
\end{tabular}
\end{table*}

\section{Experimental Setups}
\label{sc:experiments}

This section introduces the database configuration, training setups, compared methods, and evaluation methods.

\subsection{Dataset} 
\label{sc:database}

To assess the performance of METTS, we conduct a series of experiments on Chinese and English datasets, as shown in Table~\ref{tab:data}. These two languages have tremendous pronunciation differences, which poses a challenge for multilingual speech synthesis. The Chinese dataset includes audio clips from two female speakers, denoted as CN1 and CN2, expressing six types of emotions~(anger, fear, happiness, sadness, surprise, and neutral). The total number of audio clips was 22,205, approximately 21 hours of audio in sum. The English dataset includes audio clips from two female speakers, EN1 and EN2, for 19,676 audio clips, approximately 20 hours. There is no apparent emotional expression in English datasets. % Its reading style is neutral in general, while some recordings may inevitably have slight expressions according to the semantic context of the text.
All data are studio-quality recorded at 48KHz.


\subsection{Training setups}
\label{exset}

For all texts, the TTS front end encodes Chinese text input into phonemes, tones, word boundaries, and prosodic boundaries while decoding English text input into labels of phonemes. We down-sample all the audios into 24k Hz and set the frame and hop sizes to 1200 and 300, respectively, when extracting optional auxiliary acoustic features like pitch and mel-spectrogram. The auxiliary pitch and energy contour is extracted through WORLD~\cite{DBLP:journals/ieicet/MoriseYO16}, and the implementation of formant shifting is achieved by Praat, following the NANSY~\cite{DBLP:conf/nips/ChoiLKLHL21} model. The phoneme duration is obtained through an HMM-based force alignment model~\cite{Sjlander2003AnHS}.

METTS takes DelightfulTTS~\cite{DBLP:journals/corr/abs-2110-12612} as the backbone, which consists of an encoder and decoder, both containing six conformer blocks. The dimensions of the emotion embedding and speaker embedding are set to 384. The CVAE module uses the Flow setting from VITS~\cite{DBLP:conf/icml/KimKS21}, and the dimension of the fine-grained emotion embedding is set to 16. The multi-layer perceptron (MLP) consists of one conformer block layer, six two-dimensional convolution layers, and one gated recurrent unit (GRU) layer, which outputs a 384-dimensional vector. The number of clusters in the k-means algorithm $N$ is set to 64. All classifiers have the same structure that consists of 3 fully connected layers with the Relu activation function.

All models are trained up to 400k steps on two 2080Ti GPUs with a batch size of 12 and use a MelGAN~\cite{DBLP:conf/nips/KumarKBGTSBBC19} vocoder to convert the generated mel-spectrogram into waveforms.


\subsection{Comparison methods}

As this work is the first attempt, to the best of our knowledge, to synthesize foreign emotional speech through emotion transfer from reference speech or directly based on emotion ID, there are no existing methods directly comparable to our proposed approach. However, we compare our proposed METTS with the most relevant and recent methods in the field to provide a fair evaluation.
To ensure fairness in the comparison, we implement the following comparison models on the delightful TTS model backbone and maintain identical training setups.

\begin{itemize}
  \item
  \textbf{CET}~\cite{DBLP:journals/corr/abs-2110-04153} is a powerful Cross-speaker Emotion Transfer speech synthesis system, which defines several emotion tokens that are trained to be highly correlated with corresponding emotions by a semi-supervised training strategy. Speaker condition layer normalization is implemented to eliminate the down-gradation to the timbre similarity for cross-speaker emotion transfer. During inference, the model transfers emotion from a reference mel-spectrogram to the synthetic speech.
  \item
  \textbf{M3}~\cite{DBLP:conf/interspeech/ShangHZZ021} is a Multi-speaker, Multi-style, and Multi-lingual text-to-speech system, which utilizes a speaker conditional variational encoder and conducts adversarial speaker training by the gradient reversal layer. Moreover, the model uses a Mixture Density Network (MDN) for mapping text and the extracted style vectors for each speaker. In inference time, the model predicts emotion representation according to emotion ID and text to synthesize speech.
  \item
  \textbf{METTS-REF} is the proposed model that transfers emotion from a reference mel-spectrogram to synthesize speech. 
  \item
  \textbf{METTS-ID} is the proposed model that automatically matches the most suitable reference embedding according to the input text and emotion ID to synthesize speech.
\end{itemize}

\begin{table*}[htb]
\centering
\caption{Results of subjective evaluation with 95$\%$ confidence interval for Chinese speakers.}
\label{tab_1}
\begin{tabular}{@{}c|ccc|ccc@{}}
\toprule
                             & \multicolumn{3}{c|}{Chinese Text}                            & \multicolumn{3}{c}{English Text}                      \\ \midrule
Model                        & Naturalness & Speaker Similarity & Emotion Similarity & Naturalness & Speaker Similarity & Emotion Similarity \\ \midrule
METTS-REF    & \textbf{4.11±0.12} & \textbf{3.94±0.16} & \textbf{4.12±0.14} & 4.00±0.11 & \textbf{3.94±0.12} & \textbf{3.44±0.22} \\
METTS-ID     & 4.07±0.13          & 3.88±0.16          & 3.95±0.13          & \textbf{4.06±0.18}          & 3.77±0.20          & 3.24±0.22          \\
\textbf{CET}~\cite{DBLP:journals/corr/abs-2110-04153}      & 3.65±0.14          & 3.69±0.12          & 4.00±0.11          & 3.01±0.19          & 3.35±0.14          & 3.39±0.16          \\
M3~\cite{DBLP:conf/interspeech/ShangHZZ021}     & 2.69±0.19          & 3.35±0.15          & 3.21±0.18          & 2.49±0.23          & 3.46±0.16          & 2.97±0.16          \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[htb]
\centering
\caption{Results of subjective evaluation with 95$\%$ confidence interval for English speakers.}
\label{tab_2}
\begin{tabular}{@{}c|ccc|ccc@{}}
\toprule
                             & \multicolumn{3}{c|}{Chinese Text}                            & \multicolumn{3}{c}{English Text}                      \\ \midrule
Model                        & Naturalness & Speaker Similarity & Emotion Similarity & Naturalness & Speaker Similarity & Emotion Similarity \\ \midrule
METTS-REF    & 3.91±0.14          & \textbf{3.68±0.18} & 3.71±0.15          & 3.95±0.14          & \textbf{3.82±0.16} & \textbf{3.44±0.19} \\
METTS-ID     & \textbf{4.02±0.15} & 3.57±21            & \textbf{3.73±0.17} & \textbf{4.05±0.18} & 3.74±0.18          & 3.26±0.14          \\
\textbf{CET}~\cite{DBLP:journals/corr/abs-2110-04153}      & 3.08±0.16          & 2.88±0.17          & 3.41±0.16          & 2.89±0.12          & 3.33±0.15          & 3.21±0.20          \\
M3~\cite{DBLP:conf/interspeech/ShangHZZ021}     & 2.72±0.15          & 3.17±0.15          & 3.01±0.18          & 2.41±0.19          & 3.24±01.5          & 2.81±0.18          \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Evaluation metrics} 
\label{sc:evaluation}

% To evaluate the performance of the benchmark systems, we prepare a set of forty English texts and forty Chinese texts as test sets. We generate samples for each speaker and emotion category, resulting in a total of 1,920 samples (2 languages $\times$ 40 texts $\times$ 4 speakers $\times$ 6 emotions) for evaluation. As for those models that  transfer emotion from a reference mel-spectrogram to synthesize speech, we provide randomly selected mel-spectrograms from CN\_spk1 as reference. As for those models that synthesize speech based on emotion ID, we provide emotion ID. 

To evaluate the performance of the benchmark systems, we conduct a comprehensive set of evaluation methods. We prepare two test sets consisting of forty English texts and forty Chinese texts. For each speaker and emotion category, we generate samples, resulting in a total of 1,920 samples (2 languages $\times$ 40 texts $\times$ 4 speakers $\times$ 6 emotions) for evaluation. For models that transfer emotion from a reference mel-spectrogram, we provide randomly selected mel-spectrograms from CN\_spk1 as the reference. For models that synthesize speech based on emotion ID, we provide the corresponding emotion ID as input.

For subjective evaluation, we conducted two types of human perceptual rating experiments.  A total of twenty-two volunteers with basic English skills participate in these experiments. Mean Opinion Score (MOS)~\cite{shang2021incorporating} is used to evaluate the naturalness of the synthetic speech. Participants are asked to rate the speech on a scale ranging from 1 to 5, reflecting the influence of foreign accents and emotion on naturalness. The rating criteria are as follows: bad = 1, poor = 2, fair = 3, good = 4, great = 5, with 0.5-point increments. Similarity Mean Opinion Scores (SMOS)~\cite{Li2021ControllableCE} is adopted to subjectively evaluate the synthetic speech from two aspects: emotion similarity and speaker similarity. Participants are asked to rate the speech's similarity to a given emotional reference and the similarity to the reference of the target speaker. The rating scale and criteria are the same as those used in the MOS evaluation.

For objective evaluation, we measure speaker cosine similarity, character error rate (CER), and word error rate (WER) for the synthetic audio. To measure speaker cosine similarity, we train an ECAPA-TDNN~\cite{DBLP:conf/interspeech/DesplanquesTD20} model trained on 3,300 hours of Mandarin speech and 2,700 hours of English speech from 18,083 speakers to extract x-vectors. We extract the averaged x-vector of all utterances for each English speaker and six averaged x-vectors for each emotion category of the Chinese speaker. We then extract the x-vector of the synthetic audio and calculate the cosine distance. A higher cosine similarity indicates a more similar speaker timbre. To evaluate CER and WER, we use an open-source model provided by the WeNet community~\cite{DBLP:conf/interspeech/YaoWWZYYPCXL21}, which uses the U2++ conformer architecture and is trained on 10,000 hours of open-source Gigaspeech English data~\cite{DBLP:conf/interspeech/ChenCWDZWSPTZJK21} and 10,000 hours of open-source WeNet Mandarin data~\cite{DBLP:conf/icassp/ZhangLGSYXXBCZW22}, respectively. A higher CER or WER indicates less accurate pronunciation.

%\vspace{-0.2cm}
\section{Experimental results}
\label{sc:results}

This section evaluates the performance of each system to produce bilingual emotional speech for Chinese and English speakers. The comparison between METTS and other methods is presented and discussed.


\subsection{Subjective evaluation}

We initially conducts a subjective evaluation to assess the performance of the generated multilingual emotional speech in terms of speech naturalness, speaker similarity, and emotion similarity for both Chinese and English speakers. The evaluation results, as presented in Table~\ref{tab_1} and Table~\ref{tab_2}, demonstrate that the proposed METTS family consistently outperforms the baseline models across all evaluation metrics for both Chinese and English speakers. Notably, all models exhibit a performance degradation during cross-lingual emotional speech synthesis, indicating that the synthetic Chinese speech for English speakers generally has lower quality compared to that for Chinese speakers. Nevertheless, the proposed METTS family demonstrates relatively minor degradation in performance during cross-lingual emotional speech synthesis, suggesting its capability to generate natural and fluent foreign speech for a given target speaker.

%The performance of different models will be analyzed in detail.
Comparing the different models in the METTS family, METTS-REF achieves the highest speaker and emotion similarity scores, indicating its effectiveness in transferring emotions from reference to synthetic speech. On the other hand, METTS-ID achieves almost the highest naturalness score and comparable emotion similarity to METTS-REF. This result validates the efficacy of the emotion matcher module in accurately matching a suitable reference embedding to synthesize more natural speech. Furthermore, there are two exceptional cases worth mentioning. In Table~\ref{tab_1}, METTS-REF achieves the highest naturalness score in synthesizing Chinese emotional speech for Chinese speakers, indicating that intra-lingual emotion expressions of different speakers are similar. In Table~\ref{tab_2}, METTS-ID obtains the highest emotion similarity score in synthesizing Chinese emotional speech for English speakers, which suggests that the coarse-grained emotion embedding provided by the emotion matcher module is close to that of the emotion encoder for English speakers in this particular condition.


CET demonstrates similar emotion similarity to METTS-REF under specific test conditions, indicating its powerful ability in emotion transfer. However, CET is primarily designed for inter-language emotion transfer and relies on a single-scale emotion representation, which is hard to capture the diverse emotional expressions across different languages. As a result, the synthetic speech may exhibit a heavy accent. Therefore, CET receives lower scores in naturalness and speaker similarity evaluations. In contrast, our proposed METTS model incorporates multi-scale emotion modeling to capture both language-specific and language-agnostic emotional expressions, effectively avoiding the entanglement of accents with emotions.
Furthermore, M3 performs poorly across all evaluation metrics. M3 assumes a strong correlation between style coding and the speaker's attributes and content~\cite{DBLP:conf/interspeech/ShangHZZ021}, which leads to an entanglement between the speaker's timbre and emotion. Additionally, the domain adversarial training used in M3 for speaker timbre disentanglement is not stable~\cite{DBLP:conf/iclr/AcunaLZF22}. In contrast, our proposed model employs information perturbation to effectively remove the speaker's timbre, resulting in a more stable and practical approach.


\subsection{Objective evaluation}
\label{sc:emotrans}

To comprehensively evaluate the performance of our multilingual emotional TTS system, we conduct objective tests to measure speaker cosine similarity, character error rate (CER) for synthetic Chinese speech, and word error rate (WER) for synthetic English speech.

\begin{table}[htb!]
\centering
\caption{Results of objective evaluation for Chinese speakers.}
\label{tab_7}
\begin{tabular}{@{}c|cc|cc@{}}
\toprule
                             & \multicolumn{2}{c|}{Chinese Text} & \multicolumn{2}{c}{English Text} \\ \midrule
Model                        & Cosine Similarity                & CER          & Cosine Similarity             & WER            \\ \midrule
METTS-REF  & \textbf{0.813} & 0.48          & \textbf{0.753}  & 5.60                  \\
METTS-ID & 0.805          & 0.48          & 0.711  & \textbf{5.46} \\
\textbf{CET}~\cite{DBLP:journals/corr/abs-2110-04153}  & 0.726          & \textbf{0.35} & 0.638   & 12.65                 \\
M3~\cite{DBLP:conf/interspeech/ShangHZZ021}  & 0.754          & 11.02         & 0.673  & 55.32                 \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[htb!]
\centering
\caption{Results of objective evaluation for English speakers.}
\label{tab_8}
\begin{tabular}{@{}c|cc|cc@{}}
\toprule
                             & \multicolumn{2}{c|}{Chinese Text} & \multicolumn{2}{c}{English Text} \\ \midrule
Model                        & Cosine Similarity                & CER          & Cosine Similarity             & WER            \\ \midrule
METTS-REF   & \textbf{0.735}            & 1.38                           & 0.769 & 5.51                                              \\
METTS-ID    & 0.709            & 1.36                           & \textbf{0.786}  & \textbf{2.15}                           \\
\textbf{CET}~\cite{DBLP:journals/corr/abs-2110-04153}     & 0.659                     & \textbf{1.24}                  & 0.663                   & 8.05                           \\
M3~\cite{DBLP:conf/interspeech/ShangHZZ021}           & 0.671                     & 16.74                          & 0.704                       & 38.22                                      \\ \bottomrule
\end{tabular}
\end{table}

% As presented in Tables~\ref{tab_7} and Table~\ref{tab_8}, the objective test results confirm the findings of the subjective evaluation, indicating that inter-lingual speech synthesis outperforms cross-lingual speech synthesis. The METTS family achieves the highest speaker cosine similarity, providing evidence of the effectiveness of our approach in disentangling speaker timbre from emotion and language. Moreover, the METTS family obtains balanced and low CER and WER, indicating its stability in generating accurate and natural-sounding multilingual emotional speech.

The objective test results presented in Tables~\ref{tab_7} and Table~\ref{tab_8} confirm the observations from the subjective evaluation, highlighting the distinction between inter-lingual and cross-lingual speech synthesis. The METTS family achieves the highest speaker cosine similarity, demonstrating the effectiveness of our approach in disentangling speaker timbre from both emotion and language. Furthermore, the METTS family achieves lower CER and WER, indicating its stability in generating intelligent, natural-sounding multilingual emotional speech.

% CET gets the lowest CER, which indicates its ability in emotion transfer intra-lingual. However, the WER of CET is much higher as cross-lingual emotion transfer is significantly challenging, which aligns with the subjective evaluation results regarding naturalness. In addition, M3 fails to effectively address the accent-related challenges in multilingual emotional speech synthesis, leading to incorrect pronunciation and yielding the highest CER and WER scores. Moreover, the result of speaker cosine similarity suggests that information perturbation is better than SALN in CET and speaker adversarial training in M3 in speaker disentanglement of multilingual emotional speech synthesis.

It is worth noting that CET achieves the lowest Chinese CER, showcasing its ability in intra-lingual emotion transfer. However, the higher English WER of CET reflects the significant challenges of cross-lingual emotion transfer, which aligns with the subjective evaluation results concerning naturalness. Additionally, M3 fails to effectively address accent-related challenges in multilingual emotional speech synthesis, resulting in incorrect pronunciation and yielding the highest CER and WER. Furthermore, the results of speaker cosine similarity suggest that information perturbation for speaker timbre removal employed in our approach is more effective than the SALN method used in CET and the speaker adversarial training method in M3 in terms of speaker disentanglement in multilingual emotional speech synthesis.

\begin{table*}[htb]
\centering
\caption{Results of Ablation study with 95$\%$ confidence interval for Chinese speakers.}
\label{tab_9}
\begin{tabular}{@{}l|ccc|ccc@{}}
\toprule
                             & \multicolumn{3}{c|}{Chinese Text}                            & \multicolumn{3}{c}{English Text}                      \\ \midrule
Model                        & Naturalness & Speaker Similarity & Emotion Similarity & Naturalness & Speaker Similarity & Emotion Similarity \\ \midrule
METTS-REF     & \textbf{4.11±0.12} & \textbf{3.94±0.16} & 4.12±0.14 & \textbf{4.00±0.11}  & \textbf{3.94±0.12}  & 3.44±0.22 \\
\: - GST                 & 3.50±0.15 & 3.82±0.17 & 3.19±0.18  & 3.69±0.13 & 3.80±0.18  & 3.07±0.17 \\
\: - CVAE              & 3.95±0.12 & 3.81±0.16  & 3.88±0.12 & 3.43±0.14 & 3.82±0.13  & 3.39±0.17 \\
\: - Perturb & 4.02±0.12 & 3.87±0.15 &\textbf{4.19±0.14} & 3.45±0.17 & 3.55±0.16   & \textbf{3.58±0.21} \\ \bottomrule
\end{tabular}
\end{table*}

\begin{table*}[htb]
\centering
\caption{Results of Ablation study with 95$\%$ confidence interval for English speakers.}
\label{tab_10}
\begin{tabular}{@{}l|ccc|ccc@{}}
\toprule
                             & \multicolumn{3}{c|}{Chinese Text}                            & \multicolumn{3}{c}{English Text}                      \\ \midrule
Model                        & Naturalness & Speaker Similarity & Emotion Similarity & Naturalness & Speaker Similarity & Emotion Similarity \\ \midrule
METTS-REF     & \textbf{3.91±0.14} & \textbf{3.68±0.18} & 3.71±0.15 & \textbf{3.95±0.14}  & \textbf{3.82±0.16}  & 3.44±0.19 \\
\: - GST                & 3.75±0.19          & 3.52±0.21          & 3.24±0.21          & 3.73±0.19          & 3.64±0.21          & 3.17±0.18          \\
\: - CVAE             & 3.71±0.18          & 3.56±0.19          & 3.58±0.19          & 3.17±0.18          & 3.76±0.21          & 3.30±0.20          \\
\: - Perturb   & 3.85±0.21 & 3.19±0.27          & \textbf{3.82±0.19} & 3.50±0.22 & 3.26±0.29                  & \textbf{3.52±0.20} \\ \bottomrule
\end{tabular}
\end{table*}

\subsection{Visual analysis of emotional representation}

% Figure environment removed

We further visualize 
We further visualize the coarse-grained emotional representation via T-SNE~\cite{Maaten2008VisualizingDU}. Specifically, we preserve 100 utterances per emotion in the Chinese training speech data and 600 in English.

Figure~\ref{fig_3}(a) presents the T-SNE visualization of the emotion embeddings for Chinese utterances, demonstrating clear clusters. This observation validates the effectiveness of our semi-supervised emotion classifier. However, in Figure~\ref{fig_3}(a), we notice that certain emotion embeddings of English utterances are intermixed with those of Chinese utterances. We hypothesize that the language-agnostic nature of the coarse-grained emotion representation enables it to capture subtle emotional expressions in English utterances, resulting in their clustering alongside the Chinese utterances. This intermixed phenomenon of coarse-grained emotion representation signifies the METTS family's ability to transfer emotions across languages.

To further explore the extent to which the emotional representation encompasses the speaker's timbre attribute, we color the T-SNE visualization based on speaker ID. Figure~\ref{fig_3}(b) illustrates that the emotion embeddings are not well clustered according to the speaker, providing evidence of the speaker's independence in the coarse-grained emotion representation. This finding reinforces the effectiveness of our approach in disentangling speaker characteristics and isolating them from the emotional representation.


% \subsection{Case Study}
% \label{sc:emotrans}

\section{Component analysis}
\label{component}

In Section~\ref{sc:results}, we demonstrate the excellent performance of METTS in both intra- and cross-lingual scenarios of emotional speech synthesis.
In this section, we aim to evaluate the effectiveness of each component by examining their impact on naturalness, speaker similarity, and emotion similarity.
Additionally, we analyze the influence of different values of clusters on the performance of METTS-ID.

\subsection{Ablation study of METTS-REF}

\begin{table*}[]
\centering
\caption{Results of different values of $N$ on model's performance with 95$\%$ confidence interval for Chinese speakers.}
\label{tab_11}
\begin{tabular}{@{}c|cccc|cccc@{}}
\toprule
   & \multicolumn{4}{c|}{Chinese Text}                                             & \multicolumn{4}{c}{English Text}                                              \\ \midrule
$N$  & Accuracy       & Naturalness        & Speaker Similarity & Emotion Similarity & Accuracy       & Naturalness        & Speaker Similarity & Emotion Similarity \\ \midrule
32 & \textbf{0.916} & 3.91±0.13          & 3.60±0.15          & 3.51±0.19          & \textbf{0.920} & 3.81±0.13          & 3.60±0.15          & 2.95±0.21          \\
64 & 0.854          & \textbf{4.07±0.13} & \textbf{3.88±0.16} & \textbf{3.95±0.13} & 0.875          & \textbf{4.06±0.18} & \textbf{3.77±0.20} & \textbf{3.24±0.22} \\
96 & 0.656          & 4.00±0.12          & 3.76±0.14          & 3.68±0.16          & 0.664          & 3.86±0.14          & 3.63±0.17          & 2.98±0.20          \\ \bottomrule
\end{tabular}
\end{table*}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table*}[]
\centering
\caption{Results of different values of $N$ on model's performance with 95$\%$ confidence interval for English speakers.}
\label{tab_12}
\begin{tabular}{@{}c|cccc|cccc@{}}
\toprule
   & \multicolumn{4}{c|}{Chinese Text}                                             & \multicolumn{4}{c}{English Text}                                              \\ \midrule
$N$  & Accuracy       & Naturalness        & Speaker Similarity & Emotion Similarity & Accuracy       & Naturalness        & Speaker Similarity & Emotion Similarity \\ \midrule
32 & \textbf{0.916} & 3.80±0.17          & 3.62±0.17          & 3.57±0.19          & \textbf{0.920} & 3.76±0.18          & 3.65±0.18          & 3.21±0.17          \\
64 & 0.854          & \textbf{4.02±0.15} & 3.57±0.21          & \textbf{3.73±0.17} & 0.875          & \textbf{4.05±0.18} & \textbf{3.74±0.18} & \textbf{3.26±0.14} \\
96 & 0.656          & 3.90±0.20          & \textbf{3.72±0.15} & 3.64±0.17          & 0.664          & 3.74±0.16          & 3.52±0.18          & 3.10±0.22          \\ \bottomrule
\end{tabular}
\end{table*}
We conduct ablation studies where the GST module, CVAE module, and perturb module are removed individually. The corresponding results are presented in Table~\ref{tab_9} and Table~\ref{tab_10}, respectively.

The removal of the GST module significantly affects the control of global emotional expression in bilingual speech. Without GST, METTS fails to map emotional expressions of different languages to the same global token and provide global emotion conditions. As a result, there is a significant decrease in emotion similarity and noticeable declines in naturalness and speaker similarity. This highlights the crucial role of the coarse-grained language-agnostic emotion representation in our approach.

Furthermore, when the CVAE module is removed, there is a sharp decline in naturalness and a decrease in emotion similarity. This indicates that the fine-grained emotional representation learned by the CVAE module, which is consistent with the input text, not only enhances the emotional expression but also plays a vital role in addressing the foreign accent problem and improving the overall naturalness of the synthetic speech.

% Although the omission of information perturbation slightly increases emotion similarity in most test conditions, it significantly compromises naturalness and speaker similarity. This suggests a trade-off and substantial entanglement between speaker timbre, emotion, and language in multilingual emotional speech synthesis, as speaker timbre entangled with language leads to abnormal pronunciation and speaker timbre entangled emotion leads to high emotional expressiveness but low speaker similarity. The necessity of speaker disentanglement becomes apparent in order to achieve idiomatic pronunciation for each speaker.

Regarding the perturbation module, its omission slightly increased emotion similarity in most test conditions. However, it significantly compromised naturalness and speaker similarity. This trade-off suggests a substantial entanglement between speaker timbre, emotion, and language in multilingual emotional speech synthesis. Speaker timbre entangled with language may lead to abnormal pronunciation, while speaker timbre entangled with emotion may result in slightly high emotional expressiveness but low speaker similarity. Therefore, the necessity of speaker disentanglement becomes apparent to achieve idiomatic pronunciation and natural emotional expression for each speaker.


\subsection{Ablation study of METTS-ID}

Given the significance of the codebook size in Vector Quantization (VQ), we investigate the impact of different values of $N$ in the emotion matcher module on the performance of METTS-ID. Alongside evaluating naturalness, speaker similarity, and emotion similarity, we also examine the accuracy of the emotion matcher. For analysis, we retain 100 utterances per emotion in Chinese and 600 in English.

We first evaluate the accuracy of the emotion matcher by extracting the ground-truth cluster labels for each utterance and calculating the predicted accuracy. The results, presented in Table~\ref{tab_11} and Table~\ref{tab_12}, demonstrate that as the value of $N$ increases, there is an increase in the diversity of emotion embeddings. In contrast, the predicted accuracy of the emotion matcher gradually decreases. This indicates a complex trade-off between the diversity of emotion embeddings and the predicted accuracy of the emotion matcher. Notably, the predicted accuracy remains consistent across target speakers and languages, ensuring that METTS-ID can generate natural and emotionally expressive bilingual speech for each target speaker.
%Additionally, there is no significant difference between the Chinese and English text inputs, indicating the model's sound understanding of the contextual content in multilingual scripts.

Furthermore, as shown in Table~\ref{tab_11} and Table~\ref{tab_12}, the effect of $N$ on speaker similarity is negligible, while naturalness and emotion similarity achieve their highest scores when $N$ is set to 64. Therefore, considering the overall performance, we designate $N$ as 64 to strike a balance between predicted accuracy, naturalness, speaker similarity, and emotion similarity.


\section{Conclusion}
\label{sc:conclusion}

This paper proposes METTS for multilingual emotional speech synthesis, aiming at achieving natural and diverse bilingual emotional speech across speakers. First, we introduce multi-scale emotion modeling to learn emotional expressions from a language-agnostic emotion representation (coarse-grained) and a language-specific emotion representation (fine-grained), effectively addressing the foreign accent problem. Meanwhile, we leverage information perturbation to address the problem of speaker timbre coupling and obtain speaker-independent multi-scale emotion representation. Moreover, we design a VQ-based emotion matcher to construct an embedding-candidate pool and select appropriate references according to the input text and emotion category for better emotional diversity and the naturalness of synthetic speech. English-Chinese bilingual experiments show that METTS can synthesize expressive bilingual speech with natural emotion and native pronunciation for each mono-lingual speaker.

During our investigation of the multilingual emotional TTS system through cross-speaker cross-lingual emotion transfer, we have identified the need for further improvements in synthesizing English emotional speech for both Chinese and English speakers. This is mainly because the English training corpus is mainly neutral in our study. We believe that leveraging emotional English corpus to train METTS will effectively improve the expressiveness of English synthetic speech in multilingual emotional text-to-speech.

% In this study, we investigate the multilingual emotional TTS system through cross-speaker cross-lingual emotion transfer in the inter-gender situation (female to female in our experiments). As we know, cross-gender emotion transfer is even more challenging as the distinct characteristics in pitch and pronunciation fine-details in the spectrum. Our future research plans to focus on cross-gender emotion transfer in multilingual emotional TTS to address this limitation.

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.




%\textbf{Comparison of Audio2Keypoint with various target independent audio-driven reenactment methods}

% \textbf{dense optical flow} a simple dense optical flow method Farmeback \cite{farneback2003two} is adopted to calculate the dense flow of the generated videos.







% use section* for acknowledgment
% \section*{Acknowledgment}


% The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
% \ifCLASSOPTIONcaptionsoff
%  \newpage
% \fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
\bibliography{mybibfile.bib}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

\end{document}
