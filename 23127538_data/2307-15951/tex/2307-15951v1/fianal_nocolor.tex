
%% bare_jrnl_comsoc.tex
%% V1.4\elseb
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Communications Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.   ***   ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal,comsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal,comsoc]{../sty/IEEEtran}


\usepackage[T1]{fontenc}% optional T1 font encoding
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{url}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{bbding}

% \usepackage{cite}
\urlstyle{same}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\cmmnt}[1]{}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
% Do NOT use the amsbsy package under comsoc mode as that feature is
% already built into the Times Math font (newtxmath, mathtime, etc.).
% 
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


% Select a Times math font under comsoc mode or else one will automatically
% be selected for you at the document start. This is required as Communications
% Society journals use a Times, not Computer Modern, math font.
\usepackage[cmintegrals]{newtxmath}
% The freely available newtxmath package was written by Michael Sharpe and
% provides a feature rich Times math font. The cmintegrals option, which is
% the default under IEEEtran, is needed to get the correct style integral
% symbols used in Communications Society journals. Version 1.451, July 28,
% 2015 or later is recommended. Also, do *not* load the newtxtext.sty package
% as doing so would alter the main text font.
% http://www.ctan.org/pkg/newtx
%
% Alternatively, you can use the MathTime commercial fonts if you have them
% installed on your system:
%\usepackage{mtpro2}
%\usepackage{mt11p}
%\usepackage{mathtime}


%\usepackage{bm}
% The bm.sty package was written by David Carlisle and Frank Mittelbach.
% This package provides a \bm{} to produce bold math symbols.
% http://www.ctan.org/pkg/bm





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "% Figure environment removed


\section{Methodology}
\label{sc:method}

This section first gives a system overview of the proposed DiCLET-TTS and then introduces the design of each module in detail.

Figure~\ref{fig:frame1} illustrates the proposed DiCLET-TTS architecture for cross-lingual emotion transfer, a DPM-based TTS model consisting of three major components: a prior text encoder, an orthogonal projection based emotion disentangling module (OP-EDM), and a condition-enhanced DPM decoder. 
As discussed, the entangled speaker and linguistic representation can lead to a \textit{foreign accent} problem.
Thus, the prior text encoder is adopted to parameterize the forward diffusion's terminal distribution as a speaker-irrelevant but emotion-related linguistic prior, to mitigating the foreign accent while improving emotional expression.
The speaker identity is only modeled by a speaker look-up table with speaker ID in the DPM decoder to further disentangle the speaker from other factors.
Considering that the speaker disentanglement in emotion embedding could lead to \textit{weaker emotional expressiveness} in synthesized speech, our disentangled emotion embedding space is further constrained by an introduced orthogonal projection loss to ensure that the embedding maintains intense emotion discrimination after removing speaker-related information.
Finally, a condition-enhanced DPM decoder is adopted to restore the target Mel-spectrum from the speaker-irrelevant but emotion-related terminal distribution, guided by the speaker and emotion embeddings.

\vspace{-0.3cm}
\subsection{Prior text encoder}
\label{sc:text_encoder}

The prior text encoder consists of a text encoder, a length regulator, and an emotional adaptor, aiming to parameterize the terminal distribution of the forward diffusion process into a speaker-irrelevant but emotion-related linguistic prior. 
Specifically, to remove the speaker information from the linguistic representation $l_i$, the text encoder is encouraged to encode input phonemes in a speaker-irrelevant manner by introducing a speaker adversarial classifier.
Then, we encode the length-regulated $l_i$ through an emotional adaptor under the condition of emotion embedding (will be introduced in Section~\ref{sc:edm}) to obtain the speaker-irrelevant but emotion-related linguistic representation $\mu_{emo}$.
However, adversarial training could disturb linguistic encoding to some extent. 
Therefore, a content loss is introduced to mitigate this disturbance.
Details of the prior text encoder and the loss functions will be introduced.

\subsubsection{Text encoder}
The text encoder converts the phoneme sequence into the hidden linguistic representation $l_i \in \mathbb{R}^{C_i \times d}$, where $C_i$ denotes the length of the phoneme sequence, and $d$ denotes the dimension of representation.
The speaker adversarial classifier is to make the linguistic representation $l_i$ speaker-irrelevant through the softmax layer with gradient reversal (GR), and the loss function is defined as:

\begin{equation}
    {{\cal L}_{ladv}} =  - \sum\limits_{i = 1}^n {\log } P\left( {{s_i}\mid {l_i}} \right),
\end{equation}
where $n$ is the batch size, $P( {{s_i}\mid {l_i}})$ is possibility of $l_i$ belonging to the speaker $s_i$.
So that we can minimize the speaker classification loss to reversely optimize the text encoder on the speaker classification task.

The content loss guarantees the text encoder's stability to encode the input phoneme sequence when using the speaker adversarial classifier.
The corresponding loss function is defined as:
\begin{equation}
    {{\cal L}_{c}} =  - \sum\limits_{i = 1}^n \sum\limits_{j = 1}^{C_i} {\log } P\left( {{p_i^{j}}\mid {l_i^{j}}} \right).
\end{equation} 
where $p_i^{j}$ denotes the ground-truth label of the $j$-th phoneme in the $i$-th input sequence, and $l_i^{j}$ denotes the $j$-th hidden linguistic representation of the $i$-th input sequence.

\subsubsection{Length regulator}
The length regulator has the same architecture as that in FastSpeech~\cite{Ren2019FastSpeechFR}.
It takes emotion embedding as an extra input since the duration of the same sentence in different emotions should be different. 
The ${l_i}$ is length-regulated according to its real duration by the length regulator during training.
The duration predictor is trained by the mean square error (MSE) loss with the ground-truth duration. The duration loss is denoted as $\mathcal{L}_{dur}$. 

\subsubsection{Emotional adaptor}
The emotional adaptor aims to transform the length-regulated ${l_i}$ into a speaker-irrelevant but emotion-related linguistic representation $\mu_{emo}$ through multiple FFT blocks with Conditional LayerNorm~\cite{chen2020adaspeech}, which takes the emotion embedding as the condition. The $\mu_{emo}$ has the same dimension as the Mel-spectrum and is adopted to define the forward diffusion's terminal distribution ($\mathcal{N}(\mu_{emo}, I)$.
An MSE loss $\mathcal{L}_{mel}$ constrains the $\mu_{emo}$ from the target Mel-spectrum.
Regarding the prior text encoder, the total objective function is defined as:
\begin{equation}
{{\cal L}_{prior}} = 0.01*{{\cal L}_{ladv}} + {{\cal L}_{c}} + {{\cal L}_{dur}} + {{\cal L}_{mel}}. 
\end{equation}

\vspace{-0.4cm}
\subsection{Orthogonal projection based emotion disentanglement module}
\label{sc:edm}

The orthogonal projection based emotion disentanglement module (OP-EDM) is to learn an emotion encoder that extracts the speaker-irrelevant emotion embedding $e_i$ from the reference. Ideally, the embedding $e_i$ should be free of speaker-related information and discriminative in distinguishing different emotion categories. 
To this end, the emotion encoder in OP-EDM is trained with two loss functions: 1) an adversarial loss to make the obtained embedding $e_i$ speaker-irrelevant; 2) a classification loss to make the obtained embedding $e_i$ emotion-dependent.
Specifically, the emotion encoder in OP-EDM has a similar architecture as the Reference Encoder~\cite{Skerry2018Towards} to generate a 256-dimensional vector as the emotion embedding $e_i$.

The adversarial loss aims to make the emotion embedding $e_i$ speaker indistinguishable. 
A GRL is adopted between the emotion encoder and a speaker classifier.
Then, the emotion encoder is reversely optimized on the speaker classification by minimizing the following loss function:
\begin{equation}
    {{\cal L}_{sadv}} =  - \sum\limits_{i = 1}^n {\log } P\left( {{s_i}\mid {e_i}} \right),
\end{equation}
where $P( {{s_i}\mid {e_i}})$ is the possibility of the emotion embedding $e_i$ extracted from speech with the speaker label $s_i$.

The classification loss is implemented by an emotion classifier with the same structure as the above speaker classifier, to make the obtained $e_i$ emotion-dependent.
Note that the sentences of all non-emotional speakers are treated as a separate emotion category, denoted as \textit{neutral\_N}.
Thus, the softmax layer produces the probability of $8$ emotion types, i.e., $neutral$, $happy$, $surprise$, $angry$, $disgust$, $fear$, $sad$, and $neutral\_N$. The corresponding objective function is: 

\begin{equation}
    {{\cal L}_{emo}} =  - \sum\limits_{i = 1}^n {\log } P\left( {{t_i}\mid {e_i}} \right),
\end{equation}
where $P( {{t_i}\mid {e_i}})$ is possibility of emotion embedding $e_i$ belonging to the emotion label $t_i$.

\subsubsection{Explicit constraint for emotion embedding space}
As mentioned, the emotion information conveyed by the $e_i$ would be weakened after removing the speaker-related information, leading to weaker emotional expressiveness in synthesized speech. 
To address this issue, we resort to the Orthogonal Projection Loss~\cite{ranasinghe2021orthogonal} (OPL), a potent technique to construct discriminative embedding space without learnable parameters.
The objective of OPL is to enforce constraints to embedding space such that the embedding $e_i$ for different emotion classes $t_i$ is orthogonal to each other and the $e_i$ for the same class is similar, which can effectively disentangle the class-specific characteristics of different emotions, further improving the emotion discrimination of $e_i$. 
The objective function is defined as: 
\begin{equation}
\mathcal{L}_{opl}=(1-E_{same})+0.5 * |E_{different}|,
\end{equation}
where $|\cdot|$ is the absolute value operator. 
When minimizing this loss $\mathcal{L}_{opl}$, the first term $(1-E_{same})$ can ensure clustering of same class samples, while the second term $|E_{different}|$ can ensure the orthogonality of different class samples.
The $E_{same}$ and $E_{different}$ are defined as:

\begin{equation}
E_{same}=\sum_{\substack{t_{i}=t_{j}}}^n\left\langle\mathbf{e}_{i}, \mathbf{e}_{j}\right\rangle, \quad
E_{different}=\sum_{\substack{t_{i} \neq t_{k}}}^n\left\langle\mathbf{e}_{i}, \mathbf{e}_{k}\right\rangle,
\end{equation}
where $\langle\cdot, \cdot\rangle$ is the cosine similarity operator applied on two emotion embeddings. 
The total objective function of OP-EDM is defined as:
\begin{equation}
{{\cal L}_{op-edm}} = 0.2*{{\cal L}_{sadv}} + 0.8*{{\cal L}_{emo}} + {{\cal L}_{opl}}. 
\end{equation}

%\vspace{-0.2cm}
\subsection{Condition-enhanced DPM decoder}

A DPM with data-dependent prior can be seen as such: a forward diffusion converts the raw data into simple terminal distribution (usually standard Gaussian) by gradually adding Gaussian noise, then based on this terminal distribution, a reverse diffusion parameterized with a neural network learns to follow the trajectories of the reverse-time forward diffusion~\cite{song2020score,popov2021diffusion}.
If the forward and reverse diffusion processes have close trajectories, then the distribution of generated samples will be very close to that of the raw data.

\begin{table*}[h]
\centering
\caption{Dataset for the cross-lingual emotion transfer TTS.}
\setlength{\tabcolsep}{3.0mm}
\label{tab:data}
\begin{tabular}{c|c|c|ccccccc|c}
\toprule
\multirow{2}{*}{Corpus} & \multirow{2}{*}{Gender} & \multirow{2}{*}{Language} & \multicolumn{7}{c|}{Emotion (sentences)}   & \multirow{2}{*}{ Usage} \\
 &    &    & Neutral & Happy  & Surprise &Sadness & Angry & Disgust & Fear &   \\ \midrule
CN1     & Female   & Chinese  &5k   &-    &-     &-    &-   &-   &-       &Training\&Evaluation   \\
CN2     & Female   & Chinese  &5k    &-   &-     &-    &-   &-   &-       &Training               \\
CN-emo  & Female   & Chinese  &5k   &2k    &2k    &2k   &2k  &2k  &2k     &Training\&Evaluation   \\
EN1     & Female   & English  &9k   &-   &-     &-    &-   &-   &-        &Training\&Evaluation   \\ 
EN2     & Female   & English  &9k   &-    &-     &-    &-   &-   &-       &Training   \\ \bottomrule
\end{tabular}
\end{table*}

In DiCLET-TTS, the terminal distribution of forward diffusion has been parameterized by the prior text encoder as a simple linguistic-based prior distribution $\mathcal{N}(\mu_{emo}, I)$, which is emotion-related but speaker-irrelevant. 
We parameterize the reverse diffusion with a condition-enhanced DPM decoder to further improve the emotion expressiveness in speech delivery.
Specifically, the condition-enhanced DPM decoder's architecture is based on Unet and is the same as that in Grad-TTS~\cite{popov2021grad}, but the speaker and emotion embeddings are added to each ResBlock rather than just concatenated with the decoder's input. 
The speaker and emotion embeddings are produced by a speaker look-up table and the OP-EDM, respectively.

We mostly follow the formulation introduced in Grad-TTS, the forward and reverse diffusion processes of DiCLET-TTS as satisfies the following It$\widehat{o}$ stochastic differential equations (SDEs):

\begin{equation}
\begin{aligned}
d X_t=\frac{1}{2} \Sigma^{-1}\left(\mu_{emo}-X_t\right) \beta_t d t+\sqrt{\beta_t} d \overrightarrow{W}_t,
%\\&X_{t} \mid X_{0} \stackrel{d}{\rightarrow} \mathcal{N}(\mu_{emo}, I),
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\label{eq2}
d X_t=&\left(\frac{1}{2}\Sigma^{-1}\left(\mu_{emo}-X_t\right)-\nabla \log p_t\left(X_t \mid X_0\right)\right) \beta_t d t
\\&+\sqrt{\beta_t} d \overleftarrow{W}_t,
\end{aligned}
\end{equation}
where the $\overrightarrow{W}_t$ and $\overleftarrow{W}_t$ are forward and reverse-time Brownian motion. The $X_{0}$ and $X_{t}$ are raw and noise data, where $X_{t}\sim\mathcal{N}(\mu_{emo}, I)$, $t \in[0,1]$.
The $\beta_{t}$ is a noise schedule with the same definition in Grad-TTS. 
The $\log p_{t}\left(X_{t} \mid X_{0}\right)$ is the log probability density function which is predicted by a learnable score function $s_{\theta}\left(X_{t}, \mu_{emo}, t, E_{spk}, e_{i}\right)$ parameterized with the condition-enhanced DPM decoder $\theta$.
The $E_{spk}$ and $e_{i}$ are speaker and emotion embeddings, respectively.
The reverse diffusion (\ref{eq2}) is solved by a defined ordinary differential equation (ODE):

\begin{equation}
d X_{t}=\frac{1}{2}\left(\mu_{emo}-X_{t}-s_{\theta}\left(X_{t}, \mu_{emo}, t, E_{spk}, e_{i}\right)\right) \beta_{t} d t.
\end{equation}
This reverse diffusion process is trained by minimizing weighted L2 loss as follows:

\begin{equation}
\begin{aligned}
\mathcal{L}(\theta)_{diff}=&\underset{\theta}{\arg \min } \int_{0}^{1} \lambda_{t} \mathbb{E}_{X_{0}, X_{t}}\left\|s_{\theta}\left(X_{t}, \mu_{emo}, t, E_{spk}, e_{i}\right)
\right.\\ &\left.
-\nabla \log p_t\left(X_{t} \mid X_{0}\right)\right\|_{2}^{2} d t,
\end{aligned}
\end{equation}
where the $\lambda_{t} = 1-e^{-\int_{0}^{t} \beta_{s} ds}, 0<s<t$.
In brief, the reverse diffusion parameterized with the $s_{\theta}\left(X_{t}, \mu_{emo}, t, E_{spk}, e_{i}\right)$ is trained to approximate gradient of log-density of $X_{t}$ given $X_{0}$, $E_{spk}$, $e_{i}$ and $\mu_{emo}$. 
During the inference, we first predict a speaker-irrelevant but emotion-related $\mu_{emo}$ from input text with the emotion embedding $e_{i}$.
The $e_{i}$ is extracted by OP-EDM from the reference with desired emotion. 
Then the condition-enhanced DPM decoder gradually reconstructs the target Mel-spectrum using the score predicted from $s_{\theta}$ in adjustable iterations, with the conditions of $e_{i}$ and $E_{spk}$.


\vspace{-0.1cm}
\subsection{Final objective function}
All modules introduced in the previous sections are trained together. 
The final objective function of the proposed DiCLET-TTS is defined as:
\begin{equation}
    {{\cal L}_{total}} ={{\cal L}_{prior}} + {{\cal L}_{op-edm}} + {{\cal L}_{diff}},
\end{equation}
where ${\cal L}_{prior}$, ${\cal L}_{op-edm}$, and ${\cal L}_{diff}$ are loss functions of the prior text encoder, OP-EDM, and condition-enhanced DPM decoder. 

\vspace{-0.1cm}
\section{Experimental Setups}
\label{sc:experiments}

This section introduces the database configuration, evaluation methods, training setups, and compared methods.

\vspace{-0.2cm}
\subsection{Dataset} 
\label{sc:database}

As shown in Table~\ref{tab:data}, the dataset used in this paper comprises five female monolingual speakers, denoted as CN1, CN2, CN-emo, EN1, and EN2.
Note that CN1 and CN2 are publicly available Chinese corpus\footnote{The dataset is available at \url{http://www.data-baker.com/hc_znv_1.html}} and EN1 and EN2 are internal English corpora.
Only \textbf{CN-emo} is the emotional corpus employed as the \textbf{source speaker} during emotion transfer. 
All data are studio-quality recorded at 48KHz.

The test set consists of $1100$ sentences in total.
Specifically, we randomly select $700$ sentences from the \textit{CN-emo} corpus, and each emotion category (including \textit{neutral}) contains $100$ sentences. 
In addition, $400$ sentences are randomly selected from the four neutral speaker corpora, and every speaker contains $100$ sentences.

\vspace{-0.2cm}
\subsection{Model Configurations}
\label{sc:model_config}

The text encoder has the same architecture in DelightfulTTS~\cite{liu2021delightfultts}, which is composed of a pre-net (3 layers of convolutions followed by a fully-connected layer), 
6 Conformer blocks~\cite{gulati2020conformer} with multi-head self-attention, and the final linear projection layer to generate 448-dimensional linguistic representation. 
The speaker adversarial classifier in the text encoder consists of a GRU layer, a fully connected (FC) layer, and a softmax layer. Especially, a gradient reversal layer (GRL) is adopted between the GRU and the FC layer. 
The content loss is implemented by a phoneme classifier consisting of two FC layers and a softmax layer.
The emotional adaptor consists of 1 layer of 1D convolution, 2 FFT blocks, and a 1D convolution output layer, where each FFT block is followed by a Conditional LayerNorm~\cite{chen2020adaspeech}.
We employ the same structure for speaker and emotion classifiers in OP-EDM: an FC layer and a softmax layer. 
The difference is that a GRL layer is inserted before the speaker classifier.
%The Reference Encoder consisting of 6 2-D convolutional, a GRU layer, and a fully connected (FC) layer. Only the last GRU state of the GRU layer is adopted as the input of the FC layer.



\vspace{-0.3cm}
\subsection{Evaluation Methods} 
\label{sc:database}

Three types of human perceptual rating experiments are performed:
1) Mean Opinion Score (MOS)~\cite{shang2021incorporating} is used for subjective evaluation of the naturalness, which can reflect the influence of foreign accents and emotion on synthesized naturalness.
2) Differential Mean Opinion Scores (DMOS)~\cite{Li2021ControllableCE} is adopted to subjectively evaluate the synthesized speech from two aspects, emotion similarity (between the synthesized speech and compared emotional reference) and speaker similarity (between the synthesized speech and compared target speaker's reference).
3) AB preference test~\cite{an2022disentangling} (AB test) is adopted to compare samples synthesized by two models, where participants are asked to choose which speech sample sounds closer to the compared reference in terms of speaker or emotion. 
In both MOS and DMOS tests, the participants are asked to rate given speech a score ranging from $1$ to $5$ based on the specific purpose.
The rating criteria is: \textit{bad = 1}; \textit{poor = 2}; \textit{fair = 3}; \textit{good = 4}; \textit{great = 5}, in 0.5 point increments.

\begin{table*}[t]
\caption{Naturalness MOS results of DiCLET-TTS with M3, CSET, and Grad-TTS in transferring emotion to the intra- and cross-lingual target speakers, with confidence intervals of 95$\%$. The bold indicates the best performance of the four models in each emotion.}
\label{tab:naturalness}
\setlength{\tabcolsep}{2.8mm}
\centering
\begin{tabular}{l|c|cccc|cccc}
\toprule
\multirow{2}{*}{Emotion} & \multirow{2}{*}{Language} & \multicolumn{4}{c|}{Intra-lingual scenario (target Chinese speaker)}       & \multicolumn{4}{c}{Cross-lingual scenario (target English speaker)}         \\ \cmidrule{3-10}
                         &                           & \multicolumn{1}{c}{M3} & \multicolumn{1}{c}{CSET} & \multicolumn{1}{c}{Grad-TTS} & \multicolumn{1}{c|}{DiCLET-TTS} & \multicolumn{1}{c}{M3} & \multicolumn{1}{c}{CSET} & \multicolumn{1}{c}{Grad-TTS} & \multicolumn{1}{c}{DiCLET-TTS} \\ \midrule
\multirow{2}{*}{Neutral}   &  Chinese                     & 4.17$\pm$0.03 & 4.15$\pm$0.05 & 4.19$\pm$0.04 & \bf{4.23}$\pm$0.06 & 3.92$\pm$0.06 & 3.81$\pm$0.04 & 3.87$\pm$0.06      & \bf{3.98}$\pm$0.05  \\
                           &  English                     & 3.95$\pm$0.04 & 3.84$\pm$0.02 & 3.88$\pm$0.07 & \bf{3.99}$\pm$0.05 & 4.18$\pm$0.04 & 4.14$\pm$0.06 & \bf{4.24}$\pm$0.03 & 4.21$\pm$0.05       \\
\midrule
Fear      &  \multirow{6}{*}{Chinese}    & 4.05$\pm$0.05 & 3.92$\pm$0.08 & 4.03$\pm$0.04 & \bf{4.07}$\pm$0.08 & 3.82$\pm$0.05 & 3.51$\pm$0.07 & 3.68$\pm$0.04      & \bf{3.90}$\pm$0.06  \\
Disgust   &                              & 4.08$\pm$0.09 & 4.05$\pm$0.10 & 4.10$\pm$0.09 & \bf{4.12}$\pm$0.07 & 3.87$\pm$0.08 & 3.69$\pm$0.09 & 3.72$\pm$0.09      & \bf{3.93}$\pm$0.04  \\
Angry     &                              & 4.00$\pm$0.10 & 3.93$\pm$0.07 & 4.02$\pm$0.05 & \bf{4.03}$\pm$0.05 & 3.76$\pm$0.03 & 3.42$\pm$0.10 & 3.59$\pm$0.10      & \bf{3.82}$\pm$0.07  \\
Sadness   &                              & 4.03$\pm$0.09 & 3.99$\pm$0.04 & 4.04$\pm$0.09 & \bf{4.06}$\pm$0.08 & 3.81$\pm$0.06 & 3.57$\pm$0.08 & 3.69$\pm$0.07      & \bf{3.88}$\pm$0.07  \\
Happy     &                              & 4.01$\pm$0.04 & 3.98$\pm$0.05 & 4.00$\pm$0.07 & \bf{4.04}$\pm$0.03 & 3.75$\pm$0.09 & 3.46$\pm$0.06 & 3.61$\pm$0.08      & \bf{3.84}$\pm$0.08  \\
Surprise  &                              & 4.02$\pm$0.07 & 3.96$\pm$0.06 & 4.05$\pm$0.04 & \bf{4.09}$\pm$0.02 & 3.73$\pm$0.07 & 3.44$\pm$0.11 & 3.64$\pm$0.08      & \bf{3.83}$\pm$0.05  \\
\bottomrule
\end{tabular}
%\vspace{-0.2cm}
\end{table*}

During our experiments, we found that the results of different speakers in the same language were similar in human evaluation.
Consequently, to reduce the cost of human evaluation, we randomly selected one speaker from each of the two languages, i.e., CN1 and EN1, as our target speakers without loss of generality.
For MOS evaluation, $20$ Chinese and $20$ English sentences are randomly selected from the test set to synthesize speech foreign to the target speaker. 
For DMOS and AB tests, we randomly select $10$ Chinese sentences from the test set to synthesize speech with $6$ types of emotions for two target speakers, respectively, resulting in $120$ testing sentences. 
These synthesized emotional speech sentences also are evaluated for naturalness by MOS.
Twenty participants with basic English skills participated in these experiments, and the final score for each utterance was the average rating by all participants for this sample. 
The results are associated with 95$\%$ confidence intervals in all tests. 
Besides, speaker cosine similarity and embedding visualization are adopted to evaluate speaker similarity and emotion discrimination objectively.


\vspace{-0.2cm}
\subsection{Training setups}
\label{exset}
 
All the speech sentences are down-sampled to $16$ KHz and represented by 80-band Mel-spectrum with a frame length of 50ms, frameshift of 12.5ms, and hop size of $200$.
A grapheme-to-phoneme (G2P) module converts text sentences into phoneme sequences.
The phoneme duration is obtained by a pre-trained Montreal Forced Alignment (MFA) tool~\cite{Ren2019FastSpeechFR}.
We train all the models for 300K iterations with a batch size of 38 on 4 NVIDIA Tesla V100 GPUs.
During the inference, a well-trained Hifi-Gan~\cite{kong2020hifi} is adopted as the neural vocoder to reconstruct waveform from the predicted Mel-spectrum.

\vspace{-0.2cm}
\subsection{Compared methods}
As this work, to our knowledge, is the first time that attempts to synthesize foreign emotional speech based on emotion transfer by a DPM-based model, there is no existing method that can be compared directly. 
Therefore, we selected the most recent relevant methods to compare with our proposed DiCLET-TTS. 
For fairness, some modifications are made to make the compared methods suitable for cross-lingual emotion transfer.
The comparative model and the corresponding improvements are as follows: 
1) \textbf{M3}~\cite{shang2021incorporating} is a FastSpeech-based~\cite{Ren2019FastSpeechFR} multi-speaker multi-style multi-lingual speech synthesis method that introduced a fine-grained style encoder to relieve the foreign accent problem. 
To make M3 suitable for emotion transfer, the emotion ID and emotion classifier is introduced in the style predictor and style encoder, respectively. 
2) \textbf{CSET}~\cite{Li2021ControllableCE} is a reference-based cross-speaker emotion transfer method, which introduced an emotion disentangling module to Tacotron2. 
The text encoder and decoder are extended by the speaker adversarial training and language embedding~\cite{zhang2019learning}, respectively. 
3) \textbf{Grad-TTS}~\cite{popov2021grad} is also improved for cross-lingual emotion transfer.
We follow the original setting of Grad-TTS, where the decoder's input is concatenated with the speaker embedding and emotion embedding obtained from two look-up tables with the speaker ID and emotion ID as input, respectively.
The text encoder structure is the same as that in DiCLET-TTS and is trained by the speaker adversarial loss. 

%\vspace{-0.2cm}
\section{Experimental results}
\label{sc:results}

In this section, the results of emotions transferred to the intra- and cross-lingual target speakers are presented, i.e., the comparison of DiCLET-TTS with other methods in naturalness, speaker similarity, and emotion similarity.
The corresponding demos can be found on the project page\textsuperscript{\ref{ft:homepage}}, and we recommend readers listen to those demos.

\vspace{-0.2cm}
\subsection{Performance on naturalness}

Two MOS tests are conducted to evaluate the naturalness of Chinese emotional speech and cross-lingual neutral speech generated by DiCLET-TTS, M3, CSET, and Grad-TTS for intra- and cross-lingual target speakers.
The results are shown in Table~\ref{tab:naturalness}, and unsurprisingly, the highest MOS scores are obtained when synthesizing intra-lingual neutral speech for the target speaker in each method since the synthesized speech is unaffected by the foreign accent and emotion.
DiCLET-TTS achieves higher scores in cross-lingual neutral speeches, while there is no significant difference in scores between the compared methods. 
This may be due to the fact that the text encoder in these three compared methods is only constrained by speaker adversarial training, which could somewhat disturb the linguistic coding.
In DiCLET-TTS, this disturbance is mitigated by the content loss to stabilize the training and effectively improve the naturalness.

\begin{table*}[h]
 \caption{Speaker and emotion similarity DMOS comparison of DiCLET-TTS, M3, CSET, and Grad-TTS in transferring the emotions to intra- and cross-lingual target speakers, with a confidence interval of 95$\%$. The bold indicates the best performance of the four models in each emotion.
 }
 \label{tab:emotiontransfer}
\setlength{\tabcolsep}{3.5mm}
 \centering
\begin{tabular}{l|cccc|cccc}
\toprule
\multicolumn{1}{c|}{\multirow{3}{*}{Emotion}} & \multicolumn{8}{c}{Intra-lingual scenario (target Chinese speaker)}  \\ \cmidrule{2-9} 
       & \multicolumn{4}{c|}{Speaker similarity DMOS}                                                  & \multicolumn{4}{c}{Emotion similarity DMOS}                                                \\ \cmidrule{2-9} 
       & \multicolumn{1}{c}{M3} & \multicolumn{1}{c}{CSET} & \multicolumn{1}{c}{Grad-TTS} & \multicolumn{1}{c|}{DiCLET-TTS} & \multicolumn{1}{c}{M3} & \multicolumn{1}{c}{CSET} & \multicolumn{1}{c}{Grad-TTS} & \multicolumn{1}{c}{DiCLET-TTS} \\ \midrule
Fear             &\bf{4.04}$\pm$0.04 &3.91$\pm$0.02 &4.02$\pm$0.01 &4.01$\pm$0.05   &3.85$\pm$0.03 &3.71$\pm$0.06 &3.17$\pm$0.03 &\bf{4.04}$\pm$0.04  \\
Disgust          &4.05$\pm$0.02 &3.96$\pm$0.04 &4.06$\pm$0.07 &\bf{4.08}$\pm$0.04   &3.79$\pm$0.05 &3.60$\pm$0.03 &3.13$\pm$0.05 &\bf{3.90}$\pm$0.06    \\ 
Angry            &\bf{4.01}$\pm$0.06 &3.87$\pm$0.03 &3.97$\pm$0.05 &3.98$\pm$0.08   &3.81$\pm$0.06 &3.68$\pm$0.08 &3.19$\pm$0.11 &\bf{3.96}$\pm$0.09    \\
Sadness          &4.02$\pm$0.02 &3.77$\pm$0.05 &\bf{4.03}$\pm$0.02 &4.00$\pm$0.06   &3.90$\pm$0.04 &3.89$\pm$0.04 &3.28$\pm$0.08 &\bf{4.02}$\pm$0.03    \\
Happy            &\bf{3.97}$\pm$0.05 &3.79$\pm$0.04 &3.94$\pm$0.06 &\bf{3.96}$\pm$0.04   &3.92$\pm$0.07 &3.87$\pm$0.06 &3.30$\pm$0.04 &\bf{4.04}$\pm$0.08    \\
Surprise         &3.94$\pm$0.06 &3.84$\pm$0.07 &\bf{4.01}$\pm$0.04 &3.97$\pm$0.07   &3.87$\pm$0.03 &3.82$\pm$0.04 &3.25$\pm$0.07 &\bf{3.97}$\pm$0.06   \\ \midrule
\multicolumn{1}{c}{\multirow{2}{*}{}} &  \multicolumn{8}{c}{Cross-lingual scenario (target English speaker)}  \\ \midrule 
Fear             &\bf{3.91}$\pm$0.05 &3.70$\pm$0.04 &3.86$\pm$0.06 &3.89$\pm$0.02   &3.64$\pm$0.07 &3.55$\pm$0.05 &3.07$\pm$0.09 &\bf{3.86}$\pm$0.06  \\
Disgust          &\bf{3.94}$\pm$0.04 &3.74$\pm$0.07 &3.92$\pm$0.08 &3.90$\pm$0.09   &3.51$\pm$0.08 &3.39$\pm$0.04 &3.05$\pm$0.08 &\bf{3.81}$\pm$0.07   \\ 
Angry            &\bf{3.81}$\pm$0.05 &3.66$\pm$0.10 &3.78$\pm$0.04 &3.79$\pm$0.07   &3.62$\pm$0.11 &3.56$\pm$0.03 &3.14$\pm$0.04 &\bf{3.84}$\pm$0.09   \\
Sadness          &\bf{3.87}$\pm$0.09 &3.64$\pm$0.05 &3.76$\pm$0.03 &3.85$\pm$0.06   &3.57$\pm$0.09 &3.41$\pm$0.08 &3.19$\pm$0.07 &\bf{3.91}$\pm$0.03  \\
Happy            &3.72$\pm$0.04 &3.65$\pm$0.02 &3.75$\pm$0.07 &\bf{3.80}$\pm$0.08   &3.68$\pm$0.04 &3.64$\pm$0.06 &3.21$\pm$0.10 &\bf{3.93}$\pm$0.05   \\
Surprise         &3.68$\pm$0.06 &3.68$\pm$0.09 &3.71$\pm$0.03 &\bf{3.74}$\pm$0.07    &3.60$\pm$0.05 &3.55$\pm$0.02 &3.20$\pm$0.06 &\bf{3.79}$\pm$0.04   \\ \bottomrule %\midrule
\end{tabular}
%\vspace{-0.15cm}
\end{table*}

%For synthesized Chinese emotional speech, generally speaking, the naturalness of transferring the emotion to the intra-lingual target speaker is better than that of transferring emotion to the cross-lingual target speaker in all methods. 
For synthesized Chinese emotional speech, generally speaking, the naturalness of transferring emotion to the intra-lingual target speaker is better than transferring emotion to the cross-lingual target speaker among all methods.
This phenomenon is caused by the fact that emotion could make the tone change more violently and the foreign accent more obvious.
The score gap between the synthesized Chinese emotional speech for intra- and cross-lingual target speakers in DiCLET-TTS and M3 is smaller than that in Grad-TTS and CSET. 
This advantage mainly comes from DiCLET-TTS and M3 adopting prosodic-related linguistic representation, which can alleviate the foreign accent problem and improve the naturalness of cross-lingual emotion transfer.
Besides, DiCLET-TTS achieves the highest naturalness score in synthesized neutral and emotional speeches, indicating that the proposed method can disentangle speakers and languages while stabilizing the training, making the speakers speak foreign languages fluently and express various emotions in authentic Chinese.
%\textcolor{blue}{Besides, the model parameters of M3, CSET, Grad-TTS, and DiCLET-TTS are 35 M, 36.5 M, 43 M, and 47.5 M. Although the DiCLET-TTS has the largest parameters, compared to Grad-TTS, the added parameters in the extension module of DiCLET-TTS are insignificant, but a more informative prior is formulated, which eases the burden of learning the reverse diffusion and improved generation quality.}

\vspace{-0.2cm}
\subsection{Performance on emotion transfer}
\label{sc:emotrans}

Besides measuring the naturalness, the target speaker similarity and transferred emotion similarity are also evaluated.
Four DMOS tests are conducted to evaluate the speaker similarity and emotion similarity of generated Chinese emotional speech by DiCLET-TTS, M3, CSET, and Grad-TTS for intra- and cross-lingual target speakers.
The results are shown in Table~\ref{tab:emotiontransfer}, where the upper part is the speaker and emotion similarity results of transferring emotion from the source speaker to the intra-lingual target speaker, the lower part is the results of transferring emotion from the source speaker to the cross-lingual target speaker.

As seen in Table~\ref{tab:emotiontransfer}, regarding the speaker similarity of all emotion categories in each method, the scores of synthesized emotional speech of the intra-lingual target speaker are higher than that of the cross-lingual target speaker. 
This phenomenon could be partially caused by emotion and language affecting participants' perception since the compared reference during the DMOS test of the cross-lingual target speaker is neutral English audio rather than Chinese emotional audio.
A similar situation also occurs in emotion similarity DMOS.
These results indicate that compared with the cross-speaker emotion transfer task, which only recombines the two factors (speaker, emotion), it is more challenging to simultaneously recombine the three factors (speaker, language, and emotion), which are deeply entangled.

Specifically, regarding speaker similarity, the difference between DiCLET-TTS, M3, and Grad-TTS are not noticeable, while CSET performs the worst.
Although the emotion similarity of CSET is better than Grad-TTS, the poor scores in speaker similarity and cross-lingual naturalness (see Table~\ref{tab:naturalness}) indicate the weakness of CSET for the cross-lingual emotion transfer task.
Grad-TTS achieves reasonable speaker similarity in transferring the emotion to intra- and cross-lingual speakers but performs poorly in emotion similarity. 
It is mainly caused by Grad-TTS adopting a look-up table in emotion modeling, which produces average emotion expressiveness. 
DiCLET-TTS outperforms Grad-TTS in terms of emotion and speaker similarity, showing that such emotion transfer performance is derived not only from the diffusion model but also from the introduced OP-EDM and emotional adaptor.

\begin{table}[t]
 \caption{Speaker cosine similarity of synthesized speech with the cross-lingual target speaker and emotional source speaker, respectively.}
 \label{tab:cosine}
\setlength{\tabcolsep}{1.4mm}
 \centering
\begin{tabular}{c|c|c|c|c|c}
\toprule
\multicolumn{1}{c|}{Speaker} & \multicolumn{1}{c}{Target speaker} & \multicolumn{1}{c}{M3} & \multicolumn{1}{c}{CSET}  & \multicolumn{1}{c}{Grad-TTS} & \multicolumn{1}{c}{DiCLET-TTS} \\ \midrule
    Source speaker &\it{0.18}   &0.23  &0.29  &\bf{0.21}  &0.25 \\\midrule
    Target speaker &\it{0.80}   &\bf{0.75}  &0.65  &0.72  &0.73 \\
  \bottomrule
\end{tabular}
\vspace{-0.1cm}
\end{table}

DiCLET-TTS significantly outperforms all comparison methods in emotion similarity and obtains a comparable speaker similarity score with M3.
The slight speaker similarity gap between M3 and DiCLET-TTS could be caused by the stronger emotional expressiveness of DiCLET-TTS, which could affect participants on the rating of the timbre similarity.
Besides, M3 and DiCLET-TTS adopt speaker adversarial training to remove speaker-related information in emotion embedding. 
The emotion information conveyed by such disentangled emotion embedding tends to be weakened since the speaker and emotion are deeply entangled and both related to the prosody.
While in DiCLET-TTS, the emotion embedding space obtained by OP-EDM is further constrained to ensure that the emotion embedding retains high emotion discrimination after removing the speaker-related information, thus promoting the expressiveness of transferred emotions.
These results show that DiCLET-TTS can well balance maintaining the target speaker's identity and enriching the transferred emotion expressiveness in intra- and cross-lingual scenarios.

\begin{table*}[h]
 \caption{Speaker and emotion similarity DMOS comparison of DiCLET-TTS, ``w/o EA'' and ``w/o CE-D'' in transferring the emotion to the cross-lingual target speaker, with a confidence interval of 95$\%$, and the higher value means better performance, and the bold indicates the best performance out of four models in terms of each emotion.
 $\mu$ and $\mu_{emo}$ represent emotion-irrelevant and emotion-related linguistic representation, respectively.}
 \label{tab:abemospk}
\setlength{\tabcolsep}{3mm}
 \centering
\begin{tabular}{l|ccc|ccc}
\toprule 
\multicolumn{1}{c|}{\multirow{2}{*}{Emotion}} & \multicolumn{3}{c|}{Speaker similarity DMOS}   & \multicolumn{3}{c}{Emotion similarity DMOS}                                                \\ \cmidrule{2-7} 
\multicolumn{1}{c|}{}    & \multicolumn{1}{c}{``w/o EA'' ($\mu$)} & \multicolumn{1}{c}{``w/o CE-D''  ($\mu_{emo}$)} & \multicolumn{1}{c|}{DiCLET-TTS ($\mu_{emo}$)} 
                        & \multicolumn{1}{c}{``w/o EA'' ($\mu$)} & \multicolumn{1}{c}{``w/o CE-D''  ($\mu_{emo}$)} & \multicolumn{1}{c}{DiCLET-TTS ($\mu_{emo}$)} \\ \midrule
Fear             &\bf{3.91}$\pm$0.05  &3.83$\pm$0.12   &\bf{3.89}$\pm$0.02      &3.40$\pm$0.09  &3.53$\pm$0.05  &\bf{3.86}$\pm$0.06  \\
Disgust          &\bf{4.00}$\pm$0.04  &3.88$\pm$0.07   &\bf{3.96}$\pm$0.09      &3.41$\pm$0.08  &3.46$\pm$0.04  &\bf{3.81}$\pm$0.07   \\ 
Angry            &\bf{3.82}$\pm$0.03  &3.73$\pm$0.05   &3.79$\pm$0.07           &3.60$\pm$0.04  &3.66$\pm$0.03  &\bf{3.84}$\pm$0.09   \\
Sadness          &\bf{3.90}$\pm$0.06  &3.77$\pm$0.09   &\bf{3.83}$\pm$0.06      &3.43$\pm$0.07  &3.61$\pm$0.08  &\bf{3.91}$\pm$0.03  \\
Happy            &\bf{3.79}$\pm$0.06  &3.66$\pm$0.04   &3.72$\pm$0.08           &3.66$\pm$0.10  &3.72$\pm$0.06  &\bf{3.93}$\pm$0.05   \\
Surprise         &\bf{3.75}$\pm$0.08  &3.63$\pm$0.04   &3.68$\pm$0.07           &3.58$\pm$0.06  &3.69$\pm$0.02  &\bf{3.79}$\pm$0.04   \\ \bottomrule
\end{tabular}
%\vspace{-0.25cm}
\end{table*}

\vspace{-0.2cm}
\subsection{Speaker similarity with target speaker and source speaker in cross-lingual emotion transfer}

To objectively show the speaker leakage degree of each method, we calculate the speaker cosine similarity between synthesized speech and ground-truth neutral speech from the cross-lingual target speaker and emotional source speaker, respectively. 
Specifically, we adopt a pre-trained speaker verification model ECAPA-TDNN~\cite{TDNN} to extract the x-vectors of synthesized and ground truth speech. 
The speaker cosine similarity with the target speaker and the source speaker has measured on $80$ synthesized speech.

We first calculated the upper bound of cosine similarity within the target speaker's ground truth speech, and the lower bound between the target speaker and the source speaker. 
As shown in Table~\ref{tab:cosine}, the upper bound is \textbf{0.80}, and the lower bound is \textbf{0.18}.
Note that the synthesized speech from CSET has the highest similarity with the source speaker and the lowest similarity with the target speaker, consistent with the results shown in Section~\ref{sc:emotrans}.
The speech synthesized by DiCLET-TTS achieves a comparable cosine similarity score with M3, and as explained above, this gap may also be caused by the stronger emotion expressiveness of DiCLET-TTS. 

\vspace{-0.05cm}
\section{Component analysis}
\label{component}

In Section~\ref{sc:results}, DiCLET-TTS has shown good performance on emotion transfer in intra- and cross-lingual scenarios. 
In this section, the effectiveness of each proposed component, i.e., content loss, emotional adaptor, and condition-enhanced DPM decoder, is evaluated by transferring emotion to the cross-lingual target speaker.
The advantages of the proposed orthogonal projection based emotion disentanglement module (OP-EDM) are also analyzed. 

\vspace{-0.1cm}
\subsection{The effectiveness of content loss and emotional adaptor on naturalness}
\label{abneu}
In DiCLET-TTS, the content loss and emotional adaptor are the keys to improving the naturalness of synthesized cross-lingual speech.
Besides, with the guidance of emotion embedding extracted by OP-EDM, the emotional adaptor and condition-enhanced DPM decoder are further committed to enhancing emotion expressiveness.
Therefore, we first conduct an ablation study via the MOS test to verify the benefits of content loss and emotional adaptor in improving naturalness. 
We do not verify the benefits of the condition-enhanced DPM decoder since it contributes little to improving naturalness.
Specifically, two variants are evaluated: 
1) no content loss is adopted for the text encoder's output, which is constrained only by speaker adversarial training. We denote this variant as ``w/o CTL''. 
2) No emotional adaptor is adopted for the length regulator's output.
We denote this variant as ``w/o EA''.

Table~\ref{tab:abmos} shows the naturalness MOS results of DiCLET-TTS and its two variants. 
Comparing DiCLET-TTS and ``w/o CTL'', we can find the drop of naturalness when discarding content loss in ``w/o CTL'', indicating that introducing content loss in adversarial training can effectively improve the naturalness in synthesized speech.
We also find that the degradation is more prominent in some emotion categories, i.e., \textit{happy, surprise}, and \textit{angry}, since the intonation changes in these categories are more dramatic.
Besides, the naturalness significantly drops in ``w/o EA'', where the linguistic representation is emotion-irrelevant.
This result suggests that parameterizing the terminal distribution of the diffusion process into emotion-related linguistic prior by the emotional adaptor plays an essential role in promoting naturalness.

\begin{table}[htbp]
 \caption{Naturalness MOS results of DiCLET-TTS, ``w/o CTL'', and ``w/o EA'' in transferring emotion to the cross-lingual target speaker, with confidence intervals of 95$\%$. Neutral (Chinese) and neutral (English) represent synthesized neutral Chinese and English speech, respectively.}
 \label{tab:abmos}
\setlength{\tabcolsep}{3mm}
 \centering
\begin{tabular}{l|c|c|c}
\toprule
\multicolumn{1}{c|}{Method} & \multicolumn{1}{c}{``w/o CTL''} & \multicolumn{1}{c}{``w/o EA''} & \multicolumn{1}{c}{DiCLET-TTS} \\ \midrule
Neutral (Chinese)    &3.93$\pm$0.04    &3.86$\pm$0.05    &\bf{3.98}$\pm$0.05   \\
Neutral (English)   &4.15$\pm$0.07         &4.11$\pm$0.04    &\bf{4.21}$\pm$0.05   \\\midrule
Fear                  &3.84$\pm$0.09         &3.71$\pm$0.07    &\bf{3.90}$\pm$0.06   \\
Disgust               &3.85$\pm$0.08         &3.76$\pm$0.12    &\bf{3.93}$\pm$0.04   \\ 
Angry                 &3.71$\pm$0.05         &3.66$\pm$0.11    &\bf{3.82}$\pm$0.07   \\
Sadness               &3.83$\pm$0.10         &3.74$\pm$0.07    &\bf{3.88}$\pm$0.07   \\
Happy                 &3.74$\pm$0.09         &3.70$\pm$0.05    &\bf{3.84}$\pm$0.08    \\
Surprise              &3.72$\pm$0.07         &3.69$\pm$0.08    &\bf{3.83}$\pm$0.05    \\ \bottomrule %\midrule
\end{tabular}
\end{table}

\vspace{-0.2cm}
\subsection{The effectiveness of emotional adaptor and condition-enhanced DPM decoder on speaker and emotion similarity}
The effectiveness of the emotional adaptor in improving naturalness has been verified in Section~\ref{abneu}.
In this section, we further present the benefits of the emotional adaptor and condition-enhanced DPM decoder in terms of the speaker and emotion similarity by two DMOS tests.
Therefore, besides the variant ``w/o EA'', the variant ``w/o CE-D'' is also taken into the test, where the emotion embedding and speaker embedding are concatenated with the input of the decoder rather than being added to each ResBlock.

As shown in Table~\ref{tab:abemospk}, regarding the emotion similarity, the two variants in all categories have dropped compared with DiCLET-TTS, and the degradation of ``w/o EA'' is the most significant. 
The lower emotion similarity of ``w/o EA'' brings a weaker impact on the speaker identity of synthesized speech, resulting in a slightly better performance than DiCLET-TTS in speaker similarity.
Specifically, the emotion modeling of ``w/o EA'' is only completed in the condition-enhanced DPM decoder under the condition of the emotion embedding learned by OP-EDM. 
And the linguistic prior of ``w/o EA'' is emotion-irrelevant.
This result indicates that parameterizing the terminal distribution of the diffusion process as an emotion-related linguistic prior by the emotional adaptor can also effectively improve the expressiveness of transferred emotion. 
Besides, referring to the results in Table~\ref{tab:emotiontransfer}, the emotion similarity of ``w/o EA'' is superior to that of Grad-TTS in terms of all emotion categories, and ``w/o EA'' also has an improved performance than CSET in most cases (except \textit{disgust}).
These results also reflect the effectiveness of the introduced OP-EDM in learning speaker-irrelevant emotion embedding, which can result in a good performance in terms of speaker similarity and emotional expressiveness.

For ``w/o CE-D'', although it achieves better performance than ``w/o EA'' on emotion expressiveness,
this improvement is not always significant.  
Emotion expressiveness is still unsatisfactory for emotions (e.g., $disgust$ and $fear$) that rely on speaking speed and stress. 
Meanwhile, for emotions partially reflected in the changes of the source speaker's timbre (e.g., $happy$ and $surprise$), the target speaker similarity of ``w/o CE-D'' is dropped.
All these results show that with the guidance of speaker-irrelevant emotion embedding extracted from OP-EDM, the emotional adaptor and condition-enhanced DPM decoder can effectively improve the performance of cross-lingual emotion transfer while maintaining reasonable speaker similarity and speech naturalness.

\vspace{-0.2cm}
\subsection{Advantages of emotion embedding space with orthogonal projection}

This section analyzes the benefits of the proposed orthogonal projection based emotion disentanglement module (OP-EDM) by comparing it with the variant ``w/o OPL'', in which ``w/o OPL'' means the orthogonal projection loss in OP-EDM is removed.
Ideally, the emotion embedding learned by the emotion disentanglement module is expected to be irrelevant to the speaker identities but holds high emotion discrimination.
Therefore, the t-distributed stochastic neighbor embedding (t-SNE)~\cite{Laurens2008Visualizing} is adopted to demonstrate the capacity of emotion embedding learned from these two modules on distinguishing emotion categories or speaker identities.

% Figure environment removed
% Figure environment removed

\subsubsection{Emotion discrimination ability}

To display the distribution of emotion embeddings extracted by ``w/o OPL'' and OP-EDM, $80$ speeches of each emotion category from the \textit{CN-emo}'s test set are randomly selected, resulting in $560$ reference speeches in total and then embedded as emotion embeddings by these two modules, respectively.
The distributions of these embeddings are presented in Fig.~\ref{fig:emb_tsne}, where each point indicates an emotion embedding, and points with the same color are from the same emotion category. 
Smaller distances between the two points indicate that the embeddings are more similar.
As shown in Fig.~\ref{fig:emb_tsne} (a), the emotional embedding generated by ``w/o OPL'' only retains weak emotion discrimination, where emotions with similar characteristics tend to be confused: (1) \textit{happy}, \textit{surprise}, and \textit{angry} with a higher pitch and fast speech speed; (2) \textit{sad}, \textit{fear}, and \textit{disgust} with a deep voice and slower speech speed.
In contrast, in Fig.~\ref{fig:emb_tsne} (b), the emotion embeddings from the same emotion category are clustered together while different clusters are separated, demonstrating that benefits from the orthogonal projection loss, OP-EDM can obtain an embedding space with high emotion discrimination. 
We notice that although these embeddings are all from the same speaker, the neutral embeddings are far away from the others. This phenomenon could be due to the fact that emotions are mainly reflected in pitch, energy, and speech speed, and these attributes are relatively flat in neutral emotions.

\subsubsection{Speaker identity removal capability}
For speaker identity visualization, $80$ neutral speeches are randomly selected from each speaker's test set, resulting in $400$ speeches. 
The visualization results are shown in Fig.~\ref{fig:spk_tsne}, in which the same color colors the embeddings from the identical speaker.
As mentioned, the emotion embedding should contain no speaker-related information but only emotion information, which implies embeddings extracted from different speakers' speech are expected to be inseparable.
As shown in Fig.~\ref{fig:spk_tsne}(a), the embeddings from different speakers extracted by ``w/o OPL'' are indeed inseparable, while the cost is that these embeddings maintain little emotion information from the reference audio (see Fig.~\ref{fig:emb_tsne}(a)).
For the OP-EDM module (see Fig.~\ref{fig:spk_tsne}(b)), the embeddings from the four neutral speakers' corpus are clustered into one cluster. 
It is worth noting that the neutral speech from \textit{CN-emo} is treated as an independent emotion category, so the embeddings from \textit{CN-emo} are clustered into a separate cluster in Fig.~\ref{fig:spk_tsne}(b).
This distribution indicates that the proposed OPL-EDM can effectively remove the speaker-related information while greatly retaining the emotion-related information, resulting in speaker-irrelevant but emotion-discriminative embedding. 

% Figure environment removed

% Figure environment removed

\subsubsection{Preference test}
To further investigate the effectiveness of using OPL in learning emotion embedding for emotion transfer.
We conducted two AB tests between DiCLET-TTS and the variant ``w/o OPL'' regarding emotion and speaker similarity.
The results are shown in Fig.~\ref{fig:abemo} and Fig.~\ref{fig:abspk}, respectively.
As shown in Fig.~\ref{fig:abemo}, we can find that ``w/o OPL'' obtains lower preference in all emotion categories, showing lower emotion similarity is perceived.
In contrast, the listeners preferred DiCLET-TTS more when we inserted OPL into OP-EDM. 
As analyzed, the performance gain is essentially contributed by the OPL strategy in learning discriminative emotion embeddings.
Regarding speaker similarity, as shown in Fig.~\ref{fig:abspk}, there is no significant difference between ``w/o OPL'' and DiCLET-TTS, i.e., most listeners give \textit{No preference}. 
All the above evidence shows that OP-EDM introduced in this paper contributes to better emotion similarity without reducing speaker similarity.

\section{Conclusion}
\label{sc:conclusion}

This paper proposes a DPM-based cross-lingual emotion transfer model -- DiCLET-TTS.
We adopt prosodic information to alleviate the foreign accent problem, where a prior text encoder takes emotion embedding as a condition to parameterize the terminal distribution of the forward diffusion processes into a speaker-irrelevant but emotion-related linguistic prior.
To address the weaker emotional expressiveness problem caused by removing speaker information from emotion embedding, an orthogonal projection based emotion disentangling module (OP-EDM) is proposed to learn the speaker-irrelevant but high emotion-discriminative embedding.
The reverse diffusion process is parameterized by a condition-enhanced DPM decoder, where the modeling ability of the speaker and emotion is enhanced to further improve emotion expressiveness in synthetic speech.
Experimental results demonstrate that DiCLET-TTS performs well in intra- and cross-lingual emotion transfer while preserving the timbre of the target speaker and synthesized naturalness. 
The results also prove the advantages of OP-EDM in learning speaker-irrelevant but emotion-discriminative embedding.

In this study, only the same-gender speakers are involved in our experiments while cross-gender emotion transfer is considered as a difficult task~\cite{shang2021incorporating} itself and it can be more challenging in the cross-lingual scenario. We will further study this cross-gender task as a follow-up work.


% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.




%\textbf{Comparison of Audio2Keypoint with various target independent audio-driven reenactment methods}

% \textbf{dense optical flow} a simple dense optical flow method Farmeback \cite{farneback2003two} is adopted to calculate the dense flow of the generated videos.







% use section* for acknowledgment
% \section*{Acknowledgment}


% The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
% \ifCLASSOPTIONcaptionsoff
%  \newpage
% \fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
\bibliography{mybibfile.bib}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

\end{document}