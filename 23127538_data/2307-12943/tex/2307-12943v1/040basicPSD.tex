
\section{Sampling the PSD cone \label{sec:basic-psdSampling}}

With the framework in Section~\ref{sec:mixingDikin} in hand, we
can design and analyze the $\dw$ for sampling the constrained PSD
cone. The framework is formally for $\Rn$, while handling the PSD
cone necessitates reasoning with matrices in $\S^{n}$. We bridge
this gap in Section~\ref{subsec:formalism} to clarify any implicit
transitions between vectors in $\R^{d}$ and matrices in $\S^{n}$.
Then in Section~\ref{subsec:scBasicMetric}, we introduce a local
metric that is both strongly self-concordant and lower trace self-concordant
in the PSD cone; along with a bound on its $\onu$-symmetry, we obtain
the mixing time of the $\dw$ with this metric. We then give an efficient
implementation of each step of this $\dw$ in Section~\ref{subsec:oracleImplementation}.
Theorem~\ref{thm:basicPSD} restated below summarizes these results.

\thmBasicPSD*

\subsection{Formalism via matrix-vector transformations \label{subsec:formalism}}

In dealing with the $\dw$ for PSD sampling, we work in $\R^{d}$
and $\S^{n}$ simultaneously in the sequel (recall $d=n(n+1)/2$),
moving back and forth between them implicitly. We justify this identification
as follows.

\paragraph{Measure on $\protect\S^{n}$.}

We can define and work with the Lebesgue measure on $\S^{n}$ by identifying
it with the Lebesgue measure on $\R^{d}$, where each component in
the Lebesgue measure on $\S^{n}$ corresponds to each entry in the
upper triangular part. Hence, with the Lebesgue measure $dX$ on $\S^{n}$
it is straightforward to define a probability distribution on $\S^{n}$
whose probability density function with respect to $dX$ is proportional
to $e^{-f(X)}$ for a function $f:\S^{n}\to\R$. For instance, the
uniform distribution over a region corresponds to $f(X)$ being constant
in the region and infinity outside of the region, and an exponential
distribution to $f(X)=\inner{C,X}=\tr(C^{\top}X)$ for $C\in\S^{n}$.

\paragraph{Directional derivatives.}

A function $\phi:\S^{n}\to\R$ induces its counterpart $\psi:\R^{d}\to\R$
defined by $\psi(x)=\phi(X)$ for $x:=\svec(X)$. For symmetric matrices
$\{H_{i}\}_{i\leq k}$, the $k^{th}$-directional derivative of $\phi$
in directions $H_{1},\dots,H_{k}$ is 
\[
D^{k}\phi(X)[H_{1},\cdots,H_{k}]\defeq\frac{d^{k}}{dt_{k}\cdots dt_{1}}\phi\Par{X+\sum_{i=1}^{k}t_{i}H_{i}}\bigg\vert_{t_{1},\dots,t_{k}=0}.
\]
For $h_{i}:=\svec(H_{i})$, it follows that $\phi\Par{X+\sum_{i=1}^{k}t_{i}H_{i}}=\psi(x+\sum_{i=1}^{k}t_{i}h_{i})$
and thus
\[
D^{k}\phi(X)[H_{1},\cdots,H_{k}]=D^{k}\psi(x)[h_{1},\cdots,h_{k}].
\]
With this identification in hand, since the notion of (symmetric or
strong) self-concordance is formulated in terms of directional derivatives,
we can deal with both representations without having to specify one
of them.

\paragraph{Important operators.}

We introduce three linear operators that enable us to make smooth
transitions between $\S^{n}$ and $\R^{d}$.
\begin{defn}
[\cite{magnus1980elimination}] \label{def:linearOperators} Let
$E_{ij}=e_{i}e_{j}^{\top}\in\Rnn$ be the matrix with a single $1$
in the $(i,j)$ position and zeros elsewhere.
\begin{itemize}
\item $M:\R^{d}\to\R^{n^{2}}$ is the linear operator that maps $\svec(\cdot)$
to $\vec(\cdot)$ (i.e., $M\circ\svec=\vec$). It can be written as
$M=\sum_{i\geq j}\vec(T_{ij})u_{ij}^{\top}$, where $T_{ij}\in\R^{n\times n}$
has all zero entries except for $1$ at $(i,j)$ and $(j,i)$ positions
(i.e., $T_{ij}=E_{ij}+E_{ji}$ if $i\neq j$ and $E_{ij}$ if $i=j$),
and $u_{ij}=\svec(E_{ij})$.
\item $N:\R^{n^{2}}\to\R^{n^{2}}$ is the linear operator that maps $\vec(A)$
to $\vec\Par{\half(A+A^{\top})}$ for a matrix $A\in\R^{n\times n}$.
Note that $N$ is symmetric.
\item $L:\R^{d}\to\R^{n^{2}}$ is the linear operator that maps $\vec(A)$
to $\svec(A)$ for a matrix $A\in\R^{n\times n}$. It can be written
as $L=\sum_{i\geq j}u_{ij}\vec(E_{ij})^{\top}$. 
\end{itemize}
\end{defn}


\subsection{Self-concordant metric for the PSD cone \label{subsec:scBasicMetric}}

Our goal is to design an algorithm that samples a positive-definite
matrix from (\ref{eq:PSDcone}) uniformly at random (i.e., according
to the Lebesgue measure on $\S^{n}$ truncated over the PSD cone).
We observe that $-\log\det X$ is a self-concordant barrier for the
PSD constraint $X\succeq0$, and $-\sum_{i}\log\Par{b_{i}-\inner{A_{i},X}}$
(the standard log barrier) for linear constraints $\inner{A_{i},X}\leq b_{i}$
for all $i\in[m]$. With our general framework for analyzing the $\dw$
in mind, this observation suggests that the combination of the Hessians
of these self-concordant barriers could be a good candidate as the
local metric of the $\dw$. We show this is indeed the case with appropriate
scaling.

\subsubsection{Analysis of the log-determinant barrier}

We first study the metric defined by the Hessian of self-concordant
barrier $\phi(X):=-\log\det X$ (see Theorem 4.3.3 in \cite{nesterov2003introductory}
for self-concordance). In this case, its Hessian and inverse have
clean formulas. 
\begin{lem}
\label{lem:metricFormula} Let $\grad_{X}^{2}\phi(X)=\grad_{x}^{2}\Par{-\log\det\svec^{-1}(x)}\in\R^{d\times d}$
for $X\in\psd$. Its Hessian and inverse are
\begin{align*}
\hess\phi(X) & =M^{\top}(X^{-1}\otimes X^{-1})M=M^{\top}(X\otimes X)^{-1}M,\\
\Par{\hess\phi(X)}^{-1} & =M^{\dagger}(X\otimes X)M^{\dagger\top}=LN(X\otimes X)NL^{\top},
\end{align*}
where $M^{\dagger}=(M^{\top}M)^{-1}M^{\top}\in\R^{d\times n^{2}}$
is the Moore-Penrose inverse of $M\in\R^{n^{2}\times d}$.
\end{lem}

We defer the proof to Appendix~\ref{app:matrixCalculus}. We remark
that as an immediate corollary to this, the local norm of $h\in\R^{d}$
with metric $\hess\phi(X)$ becomes
\begin{align*}
\norm h_{X}^{2} & =h^{\top}M^{\top}(X^{-1}\otimes X^{-1})Mh\\
 & =\svec(H)^{\top}M^{\top}(X^{-1}\otimes X^{-1})M\svec(H)\\
 & \underset{\text{(i)}}{=}\tr\Par{HX^{-1}HX^{-1}}=:\norm H_{X}^{2},
\end{align*}
where (i) follows from $\vec=M\circ\svec$ (Definition~\ref{def:linearOperators})
and $\tr\Par{DB^{\top}A^{\top}C}=\vec(A)^{\top}(B\otimes C)\vec(D)$
(Lemma~\ref{lem:Kronecker}). 

Next, the convexity of the log-determinant of $\hess\phi(X)$ is immediate
from Lemma~\ref{lem:Kronecker}.
\begin{prop}
[Convexity of log-determinant of Hessian] $\log\det\hess\phi(X)$
is convex in $X$.
\end{prop}

\begin{proof}
Using Lemma~\ref{lem:metricFormula} and $\det\Par{M^{\top}(A\otimes A)M}=2^{n(n-1)/2}(\det A)^{n+1}$
(Lemma~\ref{lem:Kronecker}) in the first and second equality below,
\begin{align*}
\log\det\hess\phi(X) & =\log\det\Par{M^{\top}(X^{-1}\otimes X^{-1})M}\\
 & =\frac{n(n-1)}{2}\cdot\log2-(n+1)\log\det X.
\end{align*}
Since $-\log\det X$ is convex in $X$ (immediate from (\ref{eq:2ndDiffLogDet})),
the convexity of $\log\det\hess\phi(X)$ also follows.
\end{proof}
Observe from the proof that $\log\det\hess\phi(X)=\text{Const.}+(n+1)\phi(X)$.
Differentiating both sides in direction $H$,
\begin{align}
 & \tr\Par{\Par{\hess\phi(X)}^{-1}D^{3}\phi(X)[H]}=(n+1)D\phi(X)[H]\quad(\because(\ref{eq:gradLogDet}))\nonumber \\
\Longrightarrow & \tr\Par{\Par{\hess\phi(X)}^{-\half}D^{3}\phi(X)[H]\Par{\hess\phi(X)}^{-\half}}=-(n+1)\tr(X^{-1}H).\label{eq:difflogdet}
\end{align}
We now show strong self-concordance of $n\phi(X)$.
\begin{thm}
\label{thm:logdet-scaling}For $\psi_{X}:=\sup_{H\in\S^{n}}$$\norm{\Par{\hess\phi(X)}^{-\half}D^{3}\phi(X)[H]\Par{\hess\phi(X)}^{-\half}}_{F}/\norm H_{X}$,
we have $\sqrt{2(n+1)}\leq\psi_{X}\leq2\sqrt{n}$. 
\end{thm}

\begin{proof}
For $H\in\S^{n}$ and $t\in\R$, denote $X_{t}:=X+tH$ and $g_{t}:=M^{\top}(X_{t}\otimes X_{t})^{-1}M$.
Note that
\[
\norm{\Par{\hess\phi(X)}^{-\half}D^{3}\phi(X)[H]\Par{\hess\phi(X)}^{-\half}}_{F}^{2}=\tr\Par{g^{-1}\del_{t}g_{t}\vert_{t=0}g^{-1}\del_{t}g_{t}\vert_{t=0}}
\]
and
\begin{align}
\del_{t}g_{t}\vert_{t=0} & \underset{\text{(i)}}{=}\del_{t}\Par{M^{\top}(X_{t}\otimes X_{t})^{-1}M}\bigg|_{t=0}\nonumber \\
 & \underset{\text{(ii)}}{=}-M^{\top}(X\otimes X)^{-1}\del_{t}(X_{t}\otimes X_{t})\vert_{t=0}(X\otimes X)^{-1}M\nonumber \\
 & =-M^{\top}(X^{-1}\otimes X^{-1})\Par{H\otimes X+X\otimes H}(X^{-1}\otimes X^{-1})M\nonumber \\
 & \underset{\text{(iii)}}{=}-M^{\top}\Par{X^{-1}HX^{-1}\otimes X^{-1}+X^{-1}\otimes X^{-1}HX^{-1}}M,\label{eq:18-1}
\end{align}
where (i) is due to Lemma~\ref{lem:metricFormula}, (ii) is from
(\ref{eq:diffInverse}), and (iii) follows from $(A\otimes B)(C\otimes D)=(AC)\otimes(BD)$
(Lemma~\ref{lem:Kronecker}-3).

Recall that positive semidefinite matrices have unique positive semidefinite
square roots, so $(X\otimes X)^{\half}=X^{\half}\otimes X^{\half}$
(due to the fact that $(X^{\half}\otimes X^{\half})\cdot(X^{\half}\otimes X^{\half})=X\otimes X$).
Since $g_{t}=M^{\top}(X_{t}\otimes X_{t})^{-\half}(X_{t}\otimes X_{t})^{-\half}M$,
the corresponding orthogonal projection matrix is 
\[
P_{t}:=P\Par{(X_{t}\otimes X_{t})^{-\half}M}=(X_{t}\otimes X_{t})^{-\half}Mg_{t}^{-1}M^{\top}(X_{t}\otimes X_{t})^{-\half}.
\]
 By substituting $\del_{t}g_{t}\big|_{t=0}$ with (\ref{eq:18-1}),
\begin{align*}
 & \tr\Par{g^{-1}\del_{t}g_{t}\vert_{t=0}g^{-1}\del_{t}g_{t}\vert_{t=0}}\\
 & =\tr\bigg(g^{-1}M^{\top}\Par{X^{-1}HX^{-1}\otimes X^{-1}+X^{-1}\otimes X^{-1}HX^{-1}}M\\
 & \qquad\qquad\cdot g^{-1}M^{\top}\Par{X^{-1}HX^{-1}\otimes X^{-1}+X^{-1}\otimes X^{-1}HX^{-1}}\cblue M\bigg)\\
 & =\tr\bigg(\cblue Mg^{-1}M^{\top}\Par{X^{-1}HX^{-1}\otimes X^{-1}+X^{-1}\otimes X^{-1}HX^{-1}}M\\
 & \qquad\qquad\cdot g^{-1}M^{\top}\Par{X^{-1}HX^{-1}\otimes X^{-1}+X^{-1}\otimes X^{-1}HX^{-1}}\bigg)\\
 & =\tr\Par{\Par{\cred{Mg^{-1}M^{\top}}\Par{X^{-1}HX^{-1}\otimes X^{-1}+X^{-1}\otimes X^{-1}HX^{-1}}}^{2}}\\
 & =\tr\Par{\Par{\cred{(X\otimes X)^{\half}P(X\otimes X)^{\half}}\Par{X^{-1}HX^{-1}\otimes X^{-1}+X^{-1}\otimes X^{-1}HX^{-1}}}^{2}}\\
 & =\tr\bigg(\bigg(P\underbrace{(X\otimes X)^{\half}\Par{X^{-1}HX^{-1}\otimes X^{-1}+X^{-1}\otimes X^{-1}HX^{-1}}(X\otimes X)^{\half}}_{=:S}\bigg)^{2}\bigg)\\
 & =\tr\Par{PSPS}.
\end{align*}
By Lemma~\ref{lem:Kronecker}-3 once again in the second equality,
we can further manipulate $S$ as follows:
\begin{align*}
S & =(X^{\half}\otimes X^{\half})\Par{X^{-1}HX^{-1}\otimes X^{-1}+X^{-1}\otimes X^{-1}HX^{-1}}(X^{\half}\otimes X^{\half})\\
 & =\underbrace{X^{-\half}HX^{-\half}\otimes I}_{=:A}+\underbrace{I\otimes X^{-\half}HX^{-\half}}_{=:B}.
\end{align*}
By the Cauchy-Schwartz inequality along with $P^{\top}P=P^{2}=P$
and $P\preceq I$,
\begin{align*}
\tr\Par{PSPS} & \leq\tr\Par{\Par{PS}^{\top}PS}=\tr\Par{S^{\top}P^{\top}PS}=\tr\Par{S^{\top}PS}\\
 & \leq\tr\Par{S^{\top}S}=\norm S_{F}^{2}\\
 & \leq\Par{\norm A_{F}+\norm B_{F}}^{2}.
\end{align*}
Using Lemma~\ref{lem:Kronecker}-3, 
\begin{align*}
\norm A_{F}^{2} & =\tr\Par{\Par{X^{-\half}HX^{-\half}\otimes I}\cdot\Par{X^{-\half}HX^{-\half}\otimes I}}\\
 & =\tr(X^{-\half}HX^{-1}HX^{-\half}\otimes I)\\
 & =\tr\Par{X^{-\half}HX^{-1}HX^{-\half}}\tr(I)\\
 & =n\norm H_{X}^{2},
\end{align*}
and similarly $\norm B_{F}^{2}=n\norm H_{X}^{2}$. Therefore, 
\[
\norm{\Par{\hess\phi(X)}^{-\half}D^{3}\phi(X)[H]\Par{\hess\phi(X)}^{-\half}}_{F}\leq\sqrt{\tr(PSPS)}\leq2\sqrt{n}\norm H_{X},
\]
and $\psi_{X}\leq2\sqrt{n}$.

To see the optimality of $\sqrt{n}$, let us recall (\ref{eq:difflogdet}):
\[
\tr\Par{\Par{\hess\phi(X)}^{-\half}D^{3}\phi(X)[H]\Par{\hess\phi(X)}^{-\half}}=-(n+1)\tr(X^{-1}H).
\]
Taking supremum on both sides, 
\begin{align*}
\sup_{H:\norm H_{X}=1}\tr\Par{\Par{\hess\phi(X)}^{-\half}D^{3}\phi(X)[H]\Par{\hess\phi(X)}^{-\half}} & =\sup_{\substack{H\in\S^{n}:\\
\norm{X^{-\half}HX^{-\half}}_{F}=1
}
}-(n+1)\tr(X^{-\half}HX^{-\half})\\
 & =\sup_{S\in\S^{n}:\norm S_{F}=1}(n+1)\tr(S),
\end{align*}
and this objective achieves the maximum at $H=-\frac{1}{\sqrt{n}}X$,
with the supremum being $(n+1)\sqrt{n}$. On the other hand, due to
$\tr(A)\leq\sqrt{d}\norm A_{F}$ for $A\in\R^{d\times d}$,
\begin{align*}
\tr\bigg(\Par{\hess\phi(X)}^{-\half}D^{3}\phi(X)[H] & \Par{\hess\phi(X)}^{-\half}\bigg)\\
 & \leq\sqrt{\frac{n(n+1)}{2}}\cdot\norm{\Par{\hess\phi(X)}^{-\half}D^{3}\phi(X)[H]\Par{\hess\phi(X)}^{-\half}}_{F}\\
 & \leq\sqrt{\frac{n(n+1)}{2}}\cdot\psi_{X}\norm H_{X}
\end{align*}
and thus by taking supremum on both sides over a symmetric matrix
$H$ with $\norm H_{X}=1$, it follows that $(n+1)\sqrt{n}\leq\sqrt{\frac{n(n+1)}{2}}\psi_{X}$
and 
\[
\sqrt{2(n+1)}\leq\psi_{X}.\qedhere
\]
\end{proof}
This result informs us of the best possible scaling of $\phi(X)$
that leads to a strongly self-concordant barrier. Recall that if $g$
satisfies $\norm{g^{-\half}Dg[h]g^{-\half}}_{F}\leq2\alpha\norm h_{g}$
for $\alpha>0$, then $\alpha^{2}g$ is strongly self-concordant.
We remark that the scaling of $\sqrt{n}$ is obviously better than
the trivial scaling of $\sqrt{d}=\Theta(n)$.
\begin{cor}
[Strong self-concordance] \label{cor:strongSCofLOGDET} For $X\in\psd$,
a function $n\phi(X)=-n\log\det X$ is a strongly self-concordant
barrier for $\psd$. Moreover, the scaling factor of $n$ cannot be
further improved.
\end{cor}

\begin{lem}
[$\onu$-symmetry] \label{lem:logdet-symm}For $X\in K=\mathbb{S}_{+}^{n}$,
$n\phi(X)=-n\log\det X$ is $n^{2}$-symmetric.
\end{lem}

\begin{proof}
For $X\in K$, pick any $Y\in K\cap(2X-K)$, and define a symmetric
matrix $H:=Y-X$. Since $Y\in K$ and $2X-Y\in K$, we have $X+H\in K$
and $X-H\in K$. Thus,
\[
-I\preceq X^{-\half}HX^{-\half}\preceq I,
\]
and the magnitude of each eigenvalue $\{\lda_{i}\}_{i=1}^{n}$ of
$X^{-\half}HX^{-\half}$ is bounded by $1$. Hence,
\begin{align*}
\norm H_{X}^{2} & =\tr(X^{-1}HX^{-1}H)=\norm{X^{-\half}HX^{-\half}}_{F}^{2}\leq\sum_{i=1}^{n}\lda_{i}^{2}\leq n.
\end{align*}
As we scale $\phi(X)$ by $n$, the symmetry parameter becomes $\onu\leq n^{2}$
by Lemma~\ref{lem:symmScaling}.
\end{proof}

\subsubsection{Local metric for PSD cone}

To handle both constraints of $X\succeq0$ and $\inner{A_{i},X}\leq b_{i}$
in (\ref{eq:PSDcone}), we use the metric $g(X)$ induced by a barrier
$\phi(X):=-2\Par{n\log\det X+\sum_{i=1}^{m}\log\Par{b_{i}-\inner{A_{i},X}}}$.
To be precise, 
\begin{align*}
g & =2\Par{ng_{1}+g_{2}},\ \text{where}\\
g_{1}(X) & :=M^{\top}(X^{-1}\kro X^{-1})M,\\
g_{2}(X) & :=M^{\top}\left[\begin{array}{ccc}
\vec(A_{1}) & \cdots & \vec(A_{m})\end{array}\right]S_{X}^{-2}\left[\begin{array}{c}
\vec(A_{1})^{\top}\\
\vdots\\
\vec(A_{m})^{\top}
\end{array}\right]M\\
 & =M^{\top}A^{\top}S_{X}^{-2}AM=M^{\top}A_{X}^{\top}A_{X}M,
\end{align*}
where $S_{X}=\Diag(b_{i}-\inner{A_{i},X})\in\R^{m\times m}$, $A^{\top}=\left[\begin{array}{ccc}
\vec(A_{1}) & \cdots & \vec(A_{m})\end{array}\right]\in\R^{n^{2}\times m}$, and $A_{X}=S_{X}^{-1}A\in\R^{m\times n^{2}}$.

We now show that this matrix function $g$ satisfies the assumptions
in Theorem~\ref{thm:generalMixing}: 
\begin{lem}
\label{lem:metricBasic} The matrix function $g$ is $O(n^{2}+m)$-symmetric
and strongly and lower trace self-concordant.
\end{lem}

The discussion above tells us that $-n\log\det X$ is $n^{2}$-symmetric
and strongly self-concordant. On the other hand, we cannot directly
refer to results about the symmetric parameter and strong self-concordance
of $\psi(X):=-\sum_{i=1}^{m}\log\Par{b_{i}-\inner{A_{i},X}}$, since
$m$ could be less than $d$ in (\ref{eq:PSDcone}). It means that
$g_{2}=\hess\psi(x)$ could be singular, so it requires additional
technical work.
\begin{proof}
It suffices to focus on the setting where $\psi$ is the logarithmic
barrier for $P=\{x\in\R^{d}:Ax\leq b\}$ with $A\in\R^{m\times d}$
and $m<d$. We check the symmetry, strong self-concordance, and lower
trace self-concordance of $g$ in order.

\paragraph{Symmetry.}

Let $y\in P\cap(2x-P)$ and $h:=y-x$. We have $\norm{A_{x}h}_{\infty}\leq1$
by Lemma~\ref{lem:symmforPolytope}, so 
\[
h^{\top}A^{\top}\Diag(s_{x})^{-2}Ah=\sum_{i=1}^{m}\Par{\frac{(Ah)_{i}}{\Par{s_{x}}_{i}}}^{2}\leq m,
\]
and thus the symmetry parameter $\onu$ is still $m$ even when $\hess\psi$
is singular. By Lemma~\ref{lem:sumSymmetricSC}, $g$ is $O(n^{2}+m)$-symmetric.

\paragraph{Strong self-concordance.}

As the PSD cone in (\ref{eq:PSDcone}) is bounded, we can simply add
dummy constraints of the form $\ell\leq x\leq u$ on top of $Ax\leq b$
so that the new representation of (\ref{eq:PSDcone}) is still the
same with $K=\{X\succeq0\}\cap\{\inner{A_{i},X}\leq b_{i}\}$. Hence,
we may assume that $\psi_{t}(x):=\psi(x)-t\sum_{i=1}^{l}\log(u_{i}-x_{i})(x_{i}-\ell_{i})$
for $t>0$ and some $l$ has the non-singular Hessian. 

Since $-n\log\det X$ and logarithmic barriers are strongly self-concordant
(see Corollary~\ref{cor:strongSCofLOGDET} and Lemma~\ref{lem:paramsBarrier}-1),
we apply Lemma~\ref{lem:sumStrongSC} to $ng_{1}$ and $\hess\psi_{t}$,
showing that $\phi_{t}:=-2\Par{n\log\det X+\psi_{t}}$ is strongly
self-concordant. Since $\hess\phi_{0}=g\succeq2ng_{1}\succ0$ is invertible,
and both sides in the definition of strongly self-concordance are
continuous in $t$, by sending $t\to0$ we can argue that $g$ is
also strongly self-concordant. 

\paragraph{Lower trace self-concordance.}

By calling upon a special case of Lemma~\ref{lem:additiveCondition},
it suffices to show that each metric $g_{i}$ satisfies $D^{2}g_{i}[h,h]\succeq0$.
For $g_{1}(X)=-\hess\log\det X$, recall that $g_{1}(X)[H,H]=\tr\Par{X^{-1}HX^{-1}H}$,
and thus for any $V\in\S^{n}$
\begin{align*}
Dg_{1}(X)[H,H,V] & =-\tr\Par{X^{-1}VX^{-1}\cdot HX^{-1}H}-\tr\Par{X^{-1}H\cdot X^{-1}VX^{-1}\cdot H}\\
 & =-2\tr\Par{X^{-1}VX^{-1}HX^{-1}H},
\end{align*}
and by differentiating again
\begin{align}
 & D^{2}g_{1}(X)[H,H,V,V]\nonumber \\
 & =4\tr\Par{X^{-1}VX^{-1}VX^{-1}HX^{-1}H}+2\tr\Par{X^{-1}VX^{-1}HX^{-1}VX^{-1}H}\nonumber \\
 & =4\tr\Par{X^{-\half}HX^{-1}VX^{-1}VX^{-1}HX^{-\half}}+2\tr\Par{X^{-\half}VX^{-1}HX^{-\half}\cdot X^{-\half}VX^{-1}HX^{-\half}}\nonumber \\
 & \underset{\text{(i)}}{\geq}4\tr\Par{X^{-\half}HX^{-1}VX^{-1}VX^{-1}HX^{-\half}}-2\tr\Par{X^{-\half}HX^{-1}VX^{-\half}\cdot X^{-\half}VX^{-1}HX^{-\half}}\nonumber \\
 & =2\tr\Par{X^{-\half}HX^{-1}VX^{-1}VX^{-1}HX^{-\half}}\geq0,\label{eq:D4ph1}
\end{align}
where we used the Cauchy-Schwartz inequality in (i), and thus $D^{2}g_{1}(X)[H,H]\succeq0$.
For the Hessian of logarithmic barrier $g_{2}(X)$, direct computation
leads to $D^{2}g_{2}(X)[H,H]\succeq0$ (see Claim~\ref{claim:diffLogBarrier}).
\end{proof}
Putting all these together, it follows that the matrix function
\[
g(X)=-2\hess\Par{n\log\det X+\sum_{i=1}^{m}\log\Par{b_{i}-\inner{A_{i},X}}}
\]
is $O(n^{2}+m)$-symmetric, strongly self-concordant, and lower trace
self-concordant. Therefore, we conclude by Theorem~\ref{thm:generalMixing}
that the $\dw$ with $g(X)$ mixes in $\otilde{n^{2}(n^{2}+m)}$ steps,
which completes the first half of Theorem~\ref{thm:basicPSD}.

\subsection{Implementation of per-step \label{subsec:oracleImplementation}}

Now we design an oracle that implements each iteration of the $\dw$
(Algorithm~\ref{alg:DikinWalk}). This can be implemented as follows:
when the current point is $x$,
\begin{itemize}
\item Sample $z\sim B_{r}(0)$.
\item Compute $y=x+g(x)^{-\half}z$ and propose it.
\item Accept $y$ with probability $\min\Par{1,\sqrt{\frac{\det g(y)}{\det g(x)}}}$.
\end{itemize}
We provide two algorithms with the complexity of $O\Par{mn^{\omega}+m^{2}n^{2}}$
and $O\Par{n^{2\omega}+mn^{2(\omega-1)}}$. We can implement each
iteration in $O\Par{\min\Par{mn^{\omega}+m^{2}n^{2},n^{2\omega}+mn^{2(\omega-1)}}}$
time by using the former for small $m$ and the latter for large $m$.
This completes the second half of Theorem~\ref{thm:basicPSD}. 

\paragraph{Algorithm for small $m$.}

For simplicity here, we ignore the constant factors of $g$, denoting
$g=g_{1}+g_{2}$ for
\begin{align*}
g_{1}(X) & =M^{\top}(X\kro X)^{-1}M=:BB^{\top},\\
g_{2}(X) & =M^{\top}A^{\top}S_{X}^{-2}AM=:UU^{\top},
\end{align*}
where $B:=M^{\top}(X\kro X)^{-1/2}\in\R^{d\times n^{2}}$ and $U:=M^{\top}A^{\top}S_{X}^{-1}\in\R^{d\times m}$.
Letting $u_{i}$ be the $i^{th}$ column of $U$ for $i\in[m]$, we
note that $g_{2}=\sum_{i=1}^{m}u_{i}u_{i}^{\top}$.

We first implement a subroutine for computing $g(X)^{-1}v$ for a
given vector $v\in\R^{d}$ in $O(mn^{\omega}+m^{2}n^{2})$ time.

\begin{algorithm2e}[t]

\caption{Computation of $g(X)^{-1}v$}\label{alg:subroutine}

\SetAlgoLined

\textbf{Input:} $X\in\psd$, vector $v\in\R^{d}$, local metric $g$.

\textbf{Output:} $g(X)^{-1}v$

Prepare the column vectors $u_{i}$ of $U=M^{\top}A^{\top}S_{X}^{-1}$.

For $\bar{g}_{0}:=g_{1}(X)$, compute $\bar{g}_{0}^{-1}v$ and $\bar{g}_{0}^{-1}u_{i}$
for $i\in[m]$.

\For{$i=1,\cdots,m$}{

Compute $\bar{g}_{i}^{-1}v$ and $\bar{g}_{i}^{-1}u_{j}$ for $j\in[m]$,
according to 

\[
\bar{g}_{i}^{-1}w=\bar{g}_{i-1}^{-1}w-\frac{\bar{g}_{i-1}^{-1}u_{i}\cdot u_{i}^{\top}\bar{g}_{i-1}^{-1}w}{1+u_{i}^{\top}\bar{g}_{i-1}^{-1}u_{i}}.
\]

}

Return $\bar{g}_{m}^{-1}v$.

\end{algorithm2e}
\begin{prop}
\label{prop:oracle} Algorithm~\ref{alg:subroutine} computes $g(X)^{-1}v$
in $O(mn^{\omega}+m^{2}n^{2})$ time for a query vector $v\in\R^{d}$.
\end{prop}

\begin{proof}
Let $v\in\R^{d}$ be a given vector, and denote $\bar{g}_{0}:=g_{1}$
and $\bar{g}_{i}:=\bar{g}_{i-1}+u_{i}u_{i}^{\top}$ for $i\in[m]$.
We first prepare the column vectors $u_{i}$'s of $U=M^{\top}A^{\top}S_{X}^{-1}$
in $O(mn^{2})$ time and then initialize $\bar{g}_{0}^{-1}v$ and
$\bar{g}_{0}^{-1}u_{i}$ for $i\in[m]$ in $O(mn^{\omega})$ time.
For $u_{i}$'s, note that $S_{X}$ can be prepared in $O(mn^{2})$
time, and thus $A^{\top}S_{X}^{-1}$ takes $O(mn^{2})$ time due to
$A\in\R^{n^{2}\times m}$. Since each row of $M^{\top}\in\R^{d\times n^{2}}$
has at most two non-zero entries, we can obtain $u_{i}$'s in $O(mn^{2})$
time.

For $\bar{g}_{0}^{-1}v$ and $\bar{g}_{0}^{-1}u_{i}$, we recall from
Lemma~\ref{lem:metricFormula} that for a vector $z\in\R^{d}$ 
\begin{align*}
g_{1}^{-1}z & =M^{\dagger}(X\kro X)M^{\dagger\top}z=LN(X\kro X)NL^{\top}z.
\end{align*}
Since each row of $L^{\top}\in\R^{n^{2}\times d}$ has at most two
non-zero entries, $w:=L^{\top}z\in\R^{n^{2}}$ can be computed in
$O(n^{2})$ time. From the definition of $N$, it follows that $Nw=\vec\Par{\half(W+W^{\top})}$
for $W:=\vec^{-1}(w)\in\R^{n\times n}$, which also can be computed
in $O(n^{2})$ time. For $\overline{W}:=\half\Par{W+W^{\top}}$, it
follows that
\begin{align*}
(X\kro X)Nw & =(X\kro X)\vec\Par{\overline{W}}\underset{\text{Lemma \ref{lem:Kronecker}-1}}{=}\vec\Par{X\overline{W}X},
\end{align*}
which can be computed in $O(n^{\omega})$ time by the fast matrix
multiplication, and in a similar way we can compute $LN\vec\Par{X\overline{W}X}$
in $O(n^{2})$ time. Putting all these together, $\bar{g}_{0}^{-1}v$
can be computed in $O(n^{\omega})$ time, and repeating this for $u_{j}$'s
yields $\Brace{\bar{g}_{0}^{-1}v,\bar{g}_{0}^{-1}u_{1},\dots,\bar{g}_{0}^{-1}u_{m}}$
in $O(mn^{\omega})$ time.

Starting with these initializations, we recursively use the Sherman--Morrison
formula: for a given vector $z\in\R^{d}$
\begin{equation}
\bar{g}_{i}^{-1}z=\bar{g}_{i-1}^{-1}z-\frac{\bar{g}_{i-1}^{-1}u_{i}u_{i}^{\top}\bar{g}_{i-1}^{-1}z}{1+u_{i}^{\top}\bar{g}_{i-1}^{-1}u_{i}}.\label{eq:sherman-morrison}
\end{equation}
Using $\bar{g}_{i-1}^{-1}u_{j}$ and $\bar{g}_{i-1}^{-1}v$ from a
previous iteration, we can compute each of $\bar{g}_{i}^{-1}u_{j}$
and $\bar{g}_{i}^{-1}v$ in the current iteration in $O(n^{2})$ time,
and thus each round for update takes $O(mn^{2})$ time in total. Since
we iterate for $m$ rounds, Algorithm~\ref{alg:subroutine} outputs
$\bar{g}_{m}^{-1}v=g(X)^{-1}v$ in $O(mn^{\omega}+m^{2}n^{2})$ time.
\end{proof}
With this subroutine in hand, we proceed to an efficient implementation
of two tasks -- computation of (1) $g(x)^{-\half}z$ for a given
vector $z\in\R^{d}$ and (2) $\frac{\det g(y)}{\det g(x)}$.

\begin{algorithm2e}[t]

\caption{Per-step implementation of $\dw$}\label{alg:perStep-small-m}

\SetAlgoLined

\textbf{Input:} current point $X\in\psd$, local metric $g$

\tcp{Step 1: Uniform sampling from Dikin ellipsoid $\dcal_{g}^{r}(X)$}

Draw $w\sim\ncal\Par{0,I_{n^{2}+m}}$ and $v\gets g(X)^{-1}\left[\begin{array}{cc}
B & U\end{array}\right]w$ by Algorithm~\ref{alg:subroutine}.\

Draw $s\sim\text{Uniform(}[0,r])$ and $h\gets s^{1/d}v/\norm v_{g(X)}$.\

Propose $y\gets\svec(X)+h$.

\

\tcp{Step 2: Computation of acceptance probability}

Use Algorithm~\ref{alg:subroutine} to prepare $\Brace{\bar{g}_{i}^{-1}u_{1},\dots,\bar{g}_{i}^{-1}u_{m}}_{i=0}^{m}$
at $X$ and $Y:=\svec^{-1}(y)$.\

$\det\bar{g}_{0}(\cdot)\gets2^{n(n-1)/2}(\det(\cdot))^{-(n+1)}$ ($\because$
Lemma~\ref{lem:Kronecker}-7)

\For{$i=1,\cdots,m$}{

$\det(\bar{g}_{i+1})\gets\det\bar{g}_{i}\cdot\Par{1+u_{i+1}^{\top}\bar{g}_{i}^{-1}u_{i+1}}$.

}

Accept $Y$ with probability $\min\Par{1,\sqrt{\frac{\det\bar{g}_{m}(Y)}{\det\bar{g}_{m}(X)}}}$.

\end{algorithm2e}
\begin{lem}
\label{lem:perStep-small-m}Algorithm~\ref{alg:perStep-small-m}
implements each iteration of the $\dw$ with complexity of $O\Par{mn^{\omega}+m^{2}n^{2}}$.
\end{lem}

\begin{proof}
Here we provide details of Algorithm~\ref{alg:perStep-small-m} in
two stages -- (1) uniform sampling from a Dikin ellipsoid and (2)
computation of acceptance probability.

\paragraph{(1) Uniform sampling from Dikin ellipsoid:}

Instead of directly computing $g(X)^{-\half}v$ for $v\sim\text{Uniform}(B_{r}(0))$,
we illustrate how to draw $v\sim\ncal(0,g(X)^{-1})$ without full
computation of $g(X)^{-1}$ and use this to generate a uniform sample
from $\dcal_{g}^{r}(X)$ in $O(mn^{\omega}+m^{2}n^{2})$ time. 

Our approach is to compute $v:=g(X)^{-1}\left[\begin{array}{cc}
B & U\end{array}\right]w$ for $w\sim\ncal(0,I_{n^{2}+m})$, which follows the Gaussian distribution
with covariance
\begin{align*}
g(X)^{-1}\left[\begin{array}{cc}
B & U\end{array}\right]\Par{g(X)^{-1}\left[\begin{array}{cc}
B & U\end{array}\right]}^{\top} & =g(X)^{-1}(BB^{\top}+CC^{\top})g(X)^{-1}\\
 & =g(X)^{-1}g(X)g(X)^{-1}\\
 & =g(X)^{-1},
\end{align*}
since $v$ is a linear transformation of the Gaussian random variable
$w$.

Denoting $w=\left[\begin{array}{c}
w_{b}\\
w_{u}
\end{array}\right]$ for $w_{b}\sim\ncal(0,I_{n^{2}})$ and $w_{u}\sim\ncal(0,I_{m})$,
we can show that $\left[\begin{array}{cc}
B & U\end{array}\right]w$ can be computed in $O(n^{\omega}+mn^{2})$ time as follows:
\begin{align*}
\left[\begin{array}{cc}
B & U\end{array}\right]w & =Bw_{b}+Uw_{c}\\
 & =M^{\top}\underbrace{(X\kro X)^{-\half}w_{b}}_{\text{Use Lemma \ref{eq:sherman-morrison}}}+M^{\top}A^{\top}S_{X}^{-1}w_{c}\\
 & =M^{\top}\Par{\vec\Par{X^{-\half}\vec^{-1}(w_{b})X^{-\half}}+A^{\top}S_{X}^{-1}w_{c}},
\end{align*}
where $\vec\Par{X^{-\half}\vec^{-1}(w_{b})X^{-\half}}$ and $A^{\top}S_{X}^{-1}w_{u}$
can be computed in $O(n^{\omega})$ and $O(mn^{2})$ time, respectively.
Since each row of $M^{\top}\in\R^{d\times n^{2}}$ has at most two
non-zero entries, $\left[\begin{array}{cc}
B & U\end{array}\right]w$ can be computed in $O(n^{\omega}+mn^{2})$ time. Using Algorithm~\ref{alg:subroutine},
we obtain $v=g(X)^{-1}\left[\begin{array}{cc}
B & U\end{array}\right]w$ in $O(mn^{\omega}+m^{2}n^{2})$ time. 

Recall that a uniform sample from $B_{r}(0)\subset\R^{d}$ can be
drawn via $z=s^{1/d}\frac{\zeta}{\norm{\zeta}_{2}}$ for $\zeta\sim\ncal(0,I_{d})$
and $s\sim\text{Uniform(}[0,r])$. Then $g(X)^{-\half}z=s^{1/d}\frac{g(X)^{-\half}\zeta}{\norm{\zeta}_{2}}$
corresponds to a uniform sample from $\dcal_{g(X)}^{r}(0)$. Due to
$\frac{g(X)^{-\half}\zeta}{\norm{\zeta}_{2}}\sim\frac{v}{\norm v_{g(X)}}$
for $v\sim\ncal(0,g(X)^{-1})$, we can generate a uniform sample $s^{1/d}\frac{v}{\norm v_{g(X)}}$
from $\dcal_{g(X)}^{r}(0)$. Note that for $V=\svec^{-1}(v)$ we can
compute $\norm v_{g(X)}^{2}=\tr(VX^{-1}VX^{-1})+v^{\top}M^{\top}A_{X}^{\top}A_{X}Mv$
in $O\Par{n^{\omega}+mn^{2}}$ time. Therefore, we can conclude that
the proposal $y=\svec(X)+s^{1/d}\frac{v}{\norm v_{g(X)}}$ is obtained
in $O(mn^{\omega}+m^{2}n^{2})$ time.

\paragraph{(2) Computation of acceptance probability. }

We show that this step also takes $O(mn^{\omega}+m^{2}n^{2})$ time.
To compute $\det g(X)$, we use Algorithm~\ref{alg:subroutine} to
prepare $\Brace{\bar{g}_{i}^{-1}u_{1},\dots,\bar{g}_{i}^{-1}u_{m}}_{i=0}^{m}$
at $X$ and $Y=\svec^{-1}(y)$ in $O(mn^{\omega}+m^{2}n^{2})$ time.
Recall the matrix determinant lemma:
\[
\det\Par{A+uu^{\top}}=\det A\cdot\Par{1+u^{\top}A^{-1}u}.
\]
 Using the following recursive formula
\begin{align*}
\det\Par{\bar{g}_{i+1}} & =\det\Par{\bar{g}_{i}+u_{i+1}u_{i+1}^{\top}}=\det\bar{g}_{i}\cdot\Par{1+u_{i+1}^{\top}\bar{g}_{i}^{-1}u_{i+1}},
\end{align*}
we start with $\det\bar{g}_{0}=\det g_{1}=2^{n(n-1)/2}\Par{\det X}^{-(n+1)}$
(see Lemma~\ref{lem:Kronecker}-7), which can be computed in $O(n^{\omega})$
time, and compute $\det g(X)$ (and $\det g(Y)$ in the same way)
in $O(mn^{\omega}+m^{2}n^{2})$ time.
\end{proof}


\paragraph{Algorithm for large $m$.}

The algorithm right above has quadratic dependence on the number $m$
of constraints, which could become expensive for large $m$. In this
regime, we just fully compute the whole matrix function of size $\R^{d\times d}$,
which takes $O(n^{2\omega}+mn^{2(\omega-1)})$ time, and computing
its inverse, square-root, and determinant takes $O(n^{2\omega})$
time.


