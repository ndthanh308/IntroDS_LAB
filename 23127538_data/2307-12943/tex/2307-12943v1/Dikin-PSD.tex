%% LyX 2.3.6.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt,english]{article}
\usepackage{lmodern}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{color}
\usepackage{array}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{figure}{section}
\numberwithin{equation}{section}
\theoremstyle{definition}
\newtheorem*{problem*}{\protect\problemname}
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{definition}
\newtheorem{defn}[thm]{\protect\definitionname}
\theoremstyle{plain}
\newtheorem{lem}[thm]{\protect\lemmaname}
\theoremstyle{plain}
\newtheorem{prop}[thm]{\protect\propositionname}
\theoremstyle{plain}
\newtheorem{cor}[thm]{\protect\corollaryname}
\theoremstyle{remark}
\newtheorem*{acknowledgement*}{\protect\acknowledgementname}
\theoremstyle{remark}
\newtheorem{claim}[thm]{\protect\claimname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\theoremstyle{plain}
\newtheorem{thm2}{\protect\theoremname}
\theoremstyle{definition}
\newtheorem{defn2}[thm2]{\protect\definitionname}
\theoremstyle{plain}
\newtheorem*{thm2*}{\protect\theoremname}
\theoremstyle{plain}
\newtheorem{lem2}[thm2]{\protect\lemmaname}
\theoremstyle{plain}
\newtheorem{prop2}[thm2]{\protect\propositionname}
\theoremstyle{plain}
\newtheorem{cor2}[thm2]{\protect\corollaryname}


\usepackage{hyperref}
\hypersetup{
    colorlinks,
    allcolors=magenta
}
\usepackage[lined,boxed,ruled,norelsize,algo2e]{algorithm2e}
\@addtoreset{section}{part}
\usepackage{graphicx}
\usepackage{thmtools}
\usepackage{microtype}
\usepackage{mathrsfs}
\usepackage{ragged2e}
\usepackage{thm-restate}
\usepackage{caption}
\usepackage{subcaption}

\allowdisplaybreaks
\usepackage{natbib}
\bibliographystyle{plainnat}
\bibpunct{(}{)}{;}{a}{,}{,}
\declaretheorem[name=Theorem,sibling=thm2]{thmre}
\declaretheorem[name=Lemma,sibling=thm2]{lemre}
\declaretheorem[name=Proposition,sibling=thm2]{propre}
\declaretheorem[name=Corollary,sibling=thm2]{corre}

\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{positioning, arrows.meta, decorations.pathreplacing}

\makeatother

\usepackage{babel}
\providecommand{\acknowledgementname}{Acknowledgement}
\providecommand{\claimname}{Claim}
\providecommand{\corollaryname}{Corollary}
\providecommand{\definitionname}{Definition}
\providecommand{\lemmaname}{Lemma}
\providecommand{\problemname}{Problem}
\providecommand{\propositionname}{Proposition}
\providecommand{\theoremname}{Theorem}

\begin{document}
\global\long\def\defeq{\stackrel{\mathrm{{\scriptscriptstyle def}}}{=}}%

\global\long\def\norm#1{\left\Vert #1\right\Vert }%
\global\long\def\prob#1{\mathbf{P}\left(#1\right)}%
\global\long\def\Par#1{\left(#1\right)}%
\global\long\def\Brack#1{\left[#1\right]}%
\global\long\def\Abs#1{\left|#1\right|}%
\global\long\def\Brace#1{\left\{  #1\right\}  }%
\global\long\def\inner#1{\left\langle #1\right\rangle }%
\global\long\def\otilde#1{\widetilde{O}\Par{#1}}%
\global\long\def\wtilde{\widetilde{W}}%
\global\long\def\wt#1{\widetilde{#1}}%

\global\long\def\ginverse{g^{-1}}%
\global\long\def\ginv#1{g(#1)^{-1}}%
\global\long\def\normg#1{\left\Vert #1\right\Vert _{g}}%
\global\long\def\normginv#1{\left\Vert #1\right\Vert _{g^{-1}}}%

\global\long\def\cred#1{\textcolor{red}{#1}}%
\global\long\def\cblue#1{\textcolor{blue}{#1}}%
\global\long\def\cgreen#1{\textcolor{green}{#1}}%
\global\long\def\ccyan#1{\textcolor{cyan}{#1}}%

\global\long\def\R{\mathbb{R}}%
\global\long\def\Rn{\mathbb{R}^{n}}%
\global\long\def\Rnn{\mathbb{R}^{n\times n}}%
\global\long\def\E{\mathbb{E}}%
\global\long\def\P{\mathbb{P}}%
\global\long\def\S{\mathbb{S}}%
\global\long\def\N{\mathbb{N}}%

\global\long\def\acal{\mathcal{A}}%
\global\long\def\bcal{\mathcal{B}}%
\global\long\def\ccal{\mathcal{C}}%
\global\long\def\dcal{\mathcal{D}}%
\global\long\def\ecal{\mathcal{E}}%
\global\long\def\fcal{\mathcal{F}}%
\global\long\def\gcal{\mathcal{G}}%
\global\long\def\hcal{\mathcal{H}}%
\global\long\def\ical{\mathcal{I}}%
\global\long\def\tcal{\mathbb{\mathcal{T}}}%
\global\long\def\mcal{\mathbb{\mathcal{M}}}%
\global\long\def\pcal{\mathcal{P}}%
\global\long\def\ncal{\mathcal{N}}%
\global\long\def\kcal{\mathcal{K}}%

\global\long\def\veps{\varepsilon}%
\global\long\def\lda{\lambda}%
\global\long\def\vphi{\varphi}%

\global\long\def\half{\frac{1}{2}}%

\global\long\def\bw{\textsf{Ball walk}}%
\global\long\def\dw{\textup{\textsf{Dikin walk}}}%

\global\long\def\tr{\textsf{\textup{Tr}}}%
\global\long\def\diag{\textsf{\textup{diag}}}%
\global\long\def\cov{\mathrm{Cov}}%
\global\long\def\Var{\mathrm{Var}}%
\global\long\def\rank{\mathrm{rank}}%
\global\long\def\range{\mathrm{Range}}%
\global\long\def\nulls{\mathrm{Null}}%
\global\long\def\Diag{\textup{\textsf{Diag}}}%
\global\long\def\vec{\textup{\textsf{vec}}}%
\global\long\def\vol{\textup{\textsf{vol}}}%
\global\long\def\svec{\textup{\textsf{svec}}}%
\global\long\def\id{\mathrm{id}}%
\global\long\def\dtv{d_{\text{TV}}}%
\global\long\def\st{\mathrm{s.t.\ }}%
\global\long\def\nnz{\textup{\textsf{nnz}}}%

\global\long\def\ov{\overline{v}}%
\global\long\def\ox{\overline{x}}%
\global\long\def\og{\overline{g}}%
\global\long\def\onu{\overline{\nu}}%

\global\long\def\tx{\widetilde{x}}%
\global\long\def\tv{\widetilde{v}}%
\global\long\def\tp{\tilde{p}}%

\global\long\def\dv{\delta_{v}}%
\global\long\def\dx{\delta_{x}}%
\global\long\def\del{\mathcal{\partial}}%
\global\long\def\grad{\nabla}%
\global\long\def\hess{\nabla^{2}}%
\global\long\def\psd{\S_{+}^{n}}%

\global\long\def\kro{\otimes}%
\global\long\def\hada{\circ}%

\title{Efficiently Sampling the PSD Cone with the Metric Dikin Walk\author{Yunbum Kook\\ Georgia Tech\\  \texttt{yb.kook@gatech.edu} \and Santosh S. Vempala\\ Georgia Tech\\ \texttt{vempala@gatech.edu}}}
\maketitle
\begin{abstract}
Semi-definite programs represent a frontier of efficient computation.
While there has been much progress on semi-definite optimization,
with moderate-sized instances currently solvable in practice by the
interior-point method, the basic problem of \emph{sampling} semi-definite
solutions remains a formidable challenge. The direct application of
known polynomial-time algorithms for sampling general convex bodies
to semi-definite sampling leads to a prohibitively high running time.
In addition, known general methods require an expensive \emph{rounding
}phase as pre-processing. Here we analyze the Dikin walk, by first
adapting it to general metrics, then devising suitable metrics for
the PSD cone with affine constraints. The resulting mixing time and
per-step complexity are considerably smaller, and by an appropriate
choice of the metric, the dependence on the number of constraints
can be made polylogarithmic. We introduce a refined notion of self-concordant
matrix functions and give rules for combining different metrics. Along
the way, we further develop the theory of interior-point methods for
sampling.
\end{abstract}

\tableofcontents{}


\section{Introduction}

Semi-definite programs are ubiquitous in the theory of complexity
and algorithms, with applications ranging from optimization to compressed
sensing to error-correcting codes to sphere packing to extremal graph
theory to solving unique games, among many others. There has been
much progress on semi-definite optimization over the past half-century,
with moderate-sized instances currently solvable in practice by the
interior-point method. Here we consider the basic problem of \emph{sampling}
semi-definite solutions. This remains a formidable challenge in theory
and in practice, with many applications (to volume computation/integration,
Bayesian inference, systems biology, data privacy etc.). Formally,
we have the following problem.
\begin{problem*}
Given matrices $A_{i}$ of size $n\times n$ and $b_{i}\in\R$, for
$i\in[m]$, 
\begin{align}
\text{uniformly sample \ensuremath{X} subject to } & \inner{A_{i},X}\leq b_{i}\ \forall i\in[m]\text{ and }X\succeq0.\label{eq:PSDcone}
\end{align}
\end{problem*}
There are polynomial-time algorithms for sampling general convex bodies
in the general membership oracle model. The direct application of
these algorithms to semi-definite programs leads to a prohibitively
high running time. In addition, the known general methods require
an expensive \emph{rounding }phase as pre-processing. The $\bw$ takes
$n^{3}$ membership queries for a convex body in $\R^{n}$ for the
first sample, which implies $n^{6}$ membership tests for the PSD
sampling problem since the dimension is effectively $n^{2}$. Each
membership test consists of checking the linear constraints and the
semi-definiteness constraint, leading to an overall complexity of
$n^{6}(n^{w}+mn^{2})$ arithmetic operations (ignoring logarithmic
factors). Besides the high polynomial running time, a more critical
issue is space. The $\bw$ needs to compute and maintain an affine
transformation to achieve its convergence time. For a convex body
in $\R^{n}$, this would be an $n\times n$ matrix. For PSD sampling
it is effectively an $n^{2}\times n^{2}$ matrix, which makes it impractical
for $n$ larger than a few hundred. Although the $\bw$ is now practical
for sampling polytopes in very high dimension, PSD sampling is currently
out of reach even for moderate-sized instances.

How can we sample faster? A natural idea is to look for an affine-invariant
sampling algorithm that would not need to maintain an affine transformation.
The simplest of these is the $\dw$, first analyzed for polytopes
by \cite{kannan2012random}. The $\dw$ is a generalization of the
$\bw$. For each point of the target domain, we define an ellipsoid
and use it to pick a random next point (rather than a fixed ball at
any point, as in the $\bw$). The local ellipsoid is defined in an
affine-invariant manner, typically via the Hessian of a convex function
over the convex set. This approach was used by \cite{kannan2012random}
to show that the mixing time is bounded by $O(mn)$ for a polytope
in $\R^{n}$ defined by $m$ inequalities. Historically, the Dikin
ellipsoid was proposed by \cite{dikin1967iterative} as the main ingredient
of a method for convex optimization and was the first ``interior-point''
method. 
% Figure environment removed

A local metric $g$ is defined at each point $x$ in a set $K\subset\Rn$
by a positive definite matrix $g(x)$ and naturally induces the local
norm as $\norm v_{g(x)}:=\sqrt{v^{\top}g(x)v}$. Using this we can
define the Metric Dikin walk, which is the main object of study in
this paper.

\paragraph{Metric Dikin walk.}

A Dikin ellipsoid of radius $r$ at $x\in\Rn$ with a local metric
$g$, defined by
\[
\dcal_{g}^{r}(x)\defeq\Brace{y\in\Rn:\sqrt{(y-x)^{\top}g(x)(y-x)}=\norm{y-x}_{g(x)}\leq r},
\]
is a ball defined by the local metric. From this perspective, the
$\dw$ defined below is a natural generalization of the $\bw$ to
a local metric setting.

\medskip{}

\begin{algorithm2e}[H]

\caption{$\dw$}\label{alg:DikinWalk}

\SetAlgoLined

\textbf{Input:} Initial distribution $\pi_{0}$, step size $r$, local
metric $g$.

\textbf{Output:} $x_{T}$

Draw an initial point $x_{0}\sim\pi_{0}$ at random. 

\For{$t=1,\cdots,T$}{

Pick $y$ from $\dcal_{g}^{r}(x_{t-1})$ uniformly at random.

Set $x_{t}\gets y$ with probability $\min\Par{1,\frac{\vol\Par{\dcal_{g}^{r}(x_{t-1})}}{\vol\Par{\dcal_{g}^{r}(y)}}}$.
Otherwise, $x_{t}\gets x_{t-1}$.

}

\end{algorithm2e}

\medskip{}

This walk was studied in \cite{kannan2012random,laddha2020strong}
to sample polytopes using the standard logarithmic barrier and by
\cite{laddha2020strong} with a faster weighted version of the log
barrier. Their analysis relied on \emph{self-concordance }properties
of these metrics, which we briefly review next. A principal motivation
of the current paper is to go beyond polytopes, and in particular,
to see if the $\dw$ can be effectively used to sample the PSD cone. 

\paragraph{Self-concordance and $\protect\onu$-symmetry. }

While the $\dw$ can be defined in principle for any local metric,
in order to converge efficiently to a desired target distribution,
it is important for the local metric to change slowly, i.e., the derivative
of the metric should be appropriately bounded. This consideration
is also crucial in interior-point algorithms for optimization and
led to the theory of self-concordant functions and barriers, which
capture this property formally. When the local metric $g$ is \emph{self-concordant}
(defined below) on a convex body $K$, the $\dw$ always remains within
$K$ and converges to the uniform distribution over $K$ (due to reversibility
enforced by the rejection step in the last line). As we will see presently,
the analysis of mixing requires additional properties. In the following
discussion, $D^{i}g(x)[h_{1},\cdots,h_{i}]$ denotes the $i$'th directional
derivative of $g$ at $x$ along directions $h_{1},\cdots,h_{i}\in\Rn$.
% Figure environment removed

\begin{defn}
[Self-concordance] \label{def:sc}Let $K\subset\Rn$ be convex and
$g(x):K\to\R^{n\times n}$ a positive definite matrix function\emph{.}
\begin{itemize}
\item \emph{Self-concordance }(operator, \cite{nesterov1994interior}):\emph{
}$\norm{g(x)^{-\half}Dg(x)[h]g(x)^{-\half}}_{2}\leq2\norm h_{g(x)}$
for any $h\in\Rn$ and $x\in K$. 
\item \emph{Strong self-concordance }(Frobenius, \cite{laddha2020strong}):
$\norm{g(x)^{-\half}Dg(x)[h]g(x)^{-\half}}_{F}\leq2\norm h_{g(x)}$\footnote{In general, $\norm A_{2}\leq\norm A_{F}$ and $\norm A_{F}\leq\sqrt{n}\norm A_{2}$
for a matrix $A\in\Rnn$. Hence, if $g$ is self-concordant, then
$ng$ is strongly self-concordant.} for any $h\in\Rn$ and $x\in K$. 
\item \emph{Lower trace self-concordance}: $\tr\Par{g(x)^{-\half}D^{2}g(x)[h,h]g(x)^{-\half}}\geq-\norm h_{g(x)}^{2}$
for any $h\in\Rn$ and $x\in K$.
\item A function $\phi:K\to\R$ is self-concordant or strongly self-concordant
if the matrix function $\hess\phi(x)$ is self-concordant or strongly
self-concordant, respectively.
\item \emph{Self-concordance parameter}: The self-concordance parameter
$\nu$ of a self-concordant barrier function $\phi$ is defined by
$\nu=\max_{x\in K}\grad\phi(x)\Par{\hess\phi(x)}^{-1}\grad\phi(x)$.
\end{itemize}
\end{defn}

\noindent A convex function $\phi:K\rightarrow\R$ is said to be
a \emph{barrier} for a convex set $K$ if $\phi\rightarrow\infty$
as $x\rightarrow\partial K$. Note that strong self-concordance implies
self-concordance and that if $\norm{g^{-\half}Dg[h]g^{-\half}}_{F}\leq2\alpha\norm h_{g}$
for some $\alpha>0$, then $\alpha^{2}g$ is strongly self-concordant;
in other words, the specific constant of $2$ is convention, and any
constant could be used in its place. Next, we recall a symmetry parameter
of a self-concordant metric.
\begin{defn}
[$\onu$-symmetry] \label{def:symm}For a convex set $K\subset\Rn$,
a positive definite matrix function $g(x):K\to\R^{n\times n}$ is
said to be $\onu$-\emph{symmetric} if $\dcal_{g}^{1}(x)\subseteq K\cap(2x-K)\subseteq\dcal_{g}^{\sqrt{\onu}}(x)$
for any $x\in K$.
\end{defn}

We note that $K\cap(2x-K)$ is the locally symmetrized convex body
with respect to $x$. Hence, $\onu$-symmetry measures how accurately
a Dikin ellipsoid approximates the locally symmetrized body (see Figure~\ref{fig:sc-symm}).
One can show that $\onu=O(\nu^{2})$ for any metric induced by a self-concordant
barrier.

\subsection{Results}

We are now ready to state our main results, summarized in Table~\ref{tab:complexity}.

\begin{table}
\begin{centering}
\begin{tabular}{cccc}
\toprule 
Sampler & Mixing rate & Per-step complexity & Citation\tabularnewline
\midrule
\midrule 
\multicolumn{4}{c}{\textbf{Polytope}}\tabularnewline
\midrule 
{\footnotesize{}BW} & $n^{2}\,(+n^{3})$ & $mn$ & \textcolor{black}{\footnotesize{}\cite{kannan1997random,jia2021reducing}}\tabularnewline
\midrule 
\multirow{2}{*}{{\footnotesize{}DW}} & \multirow{2}{*}{$mn$} & $mn^{\omega-1}$ & \textcolor{black}{\footnotesize{}\cite{kannan2012random}}\tabularnewline
\cmidrule{3-4} \cmidrule{4-4} 
 &  & $n^{2}+\nnz(A)$ & \textcolor{black}{\footnotesize{}\cite{laddha2020strong}}\tabularnewline
\midrule 
{\footnotesize{}DW (Vaidya)} & $\sqrt{m}n^{3/2}$ & $mn^{\omega-1}$ & \textcolor{black}{\footnotesize{}\cite{chen2018fast}}\tabularnewline
\midrule 
{\footnotesize{}DW (Lewis)} & $n^{2}$ & $mn^{\omega-1}$ & \textcolor{black}{\footnotesize{}\cite{laddha2020strong}}\tabularnewline
\midrule 
\multicolumn{4}{c}{\textbf{PSD cone}}\tabularnewline
\midrule 
{\footnotesize{}BW} & $n^{4}\,(+n^{6})$ & $mn^{2}+n^{\omega}$ & (convex body bound)\tabularnewline
\midrule 
{\footnotesize{}DW} & \textcolor{red}{$(n^{2}+m)n^{2}$} & \textcolor{red}{\small{}$\min(mn^{\omega}+m^{2}n^{2},n^{2\omega}+mn^{2(\omega-1)})$} & \textcolor{red}{this paper}\tabularnewline
\midrule 
{\footnotesize{}DW (Vaidya)} & \textcolor{red}{$(n+\sqrt{m})n^{3}$} & \textcolor{red}{$mn^{2(\omega-1)}$} & \textcolor{red}{this paper}\tabularnewline
\midrule 
{\footnotesize{}DW (Lewis)} & \textcolor{red}{$n^{5}$} & \textcolor{red}{$mn^{2(\omega-1)}$} & \textcolor{red}{this paper}\tabularnewline
\bottomrule
\end{tabular}
\par\end{centering}
\caption{\label{tab:complexity}The complexity of uniformly sampling a polytope
in $\protect\Rn$ / PSD cone in $\protect\S^{n}$ with $m$ affine
constraints. The cost in the parenthesis is the number of iterations
needed for isotropic rounding (\cite{jia2021reducing}). The exponent
$\omega<2.373$ indicates the matrix multiplication constant.}
\end{table}

In Section~\ref{sec:mixingDikin}, we provide a general analysis
of the mixing time of the $\dw$ under milder assumptions than those
in \cite{laddha2020strong}. While the latter paper provides a general
guarantee for strongly self-concordant metrics, it relies on a nice
convexity property which is challenging to verify in the PSD setting.
To address this, we introduce a weaker condition. Then we discuss
how our weaker assumptions enable the analysis of the $\dw$ with
a combination of different metrics. This refined analysis turns out
to be crucial to improve over the bounds of the $\bw$. 

\begin{restatable}{thmre}{thmGeneralMixing} \label{thm:generalMixing} 

For a convex body $K\subset\Rn$, let $\pi_{0}$ be an initial distribution,
$\pi$ the uniform distribution over $K$, and $\Lambda=\sup_{S\subset K}\frac{\pi_{0}(S)}{\pi(S)}$
the warmness of the initial distribution $\pi_{0}$. Let $\pi_{T}$
be the distribution obtained after $T$ steps of the $\dw$ starting
from $x_{0}\sim\pi_{0}$ with a $\onu$-symmetric, lower trace self-concordant
and strongly self-concordant matrix function $g$. Then, for any $\veps>0$,
the total variation distance between $\pi_{T}$ and $\pi$ satisfies
$\dtv(\pi_{T},\pi)\leq\veps$ for $T=O\Par{n\onu\log\frac{\Lambda}{\veps}}$.

\end{restatable}

We replace the requirement in \cite{laddha2020strong} that the local
metric $g$ has convex log-determinant by lower trace self-concordance,
which is considerably weaker. The importance of this refinement will
become clear presently. 

In Section~\ref{sec:basic-psdSampling}, we introduce a metric that
satisfies all the assumptions in Theorem~\ref{thm:generalMixing}.
The resulting $\dw$ mixes in $\otilde{(n^{2}+m)n^{2}}$ iterations
for uniformly sampling the PSD cone with $m$ linear constraints.
We also provide an efficient per-step implementation. For a matrix
$X\in\Rnn$, let $\vec(X)\in\R^{n^{2}}$ denote the vector obtained
by stacking columns of $X$ vertically. Additionally, we define $A\in\R^{m\times n^{2}}$,
$S_{X}\in\R^{m\times m}$, and $A_{X}\in\R^{m\times n^{2}}$ by 
\[
A:=\left[\begin{array}{ccc}
\vec(A_{1}) & \cdots & \vec(A_{m})\end{array}\right]^{\top},\,S_{X}:=\Diag\Par{b_{i}-\inner{A_{i},X}},\,A_{X}:=S_{X}^{-1}A.
\]
The metric below comes from the Hessian of the following function:
\[
-2n\log\det X-2\sum_{i=1}^{m}\Par{b_{i}-\inner{A_{i},X}}.
\]
Here the first term, the log-determinant, serves as a barrier for
the PSD cone while the second term is the standard logarithmic barrier
for linear constraints.

% Figure environment removed
\begin{restatable}{thmre}{thmBasicPSD} \label{thm:basicPSD} 

Let $K$ be the constrained PSD cone in (\ref{eq:PSDcone}). Let $g$
be the local metric such that at each $X\in K$, for symmetric matrices
$H_{1},H_{2}$,
\[
g_{X}(H_{1},H_{2})=2n\tr\Par{X^{-1}H_{1}X^{-1}H_{2}}+2\vec(H_{1})^{\top}A_{X}^{\top}A_{X}\vec(H_{2}).
\]
Then the $\dw$ with the local metric $g$ mixes in $\otilde{(n^{2}+m)n^{2}}$
steps, where each step runs in $O(\min(mn^{\omega}+m^{2}n^{2},n^{2\omega}+mn^{2(\omega-1)}))$
time\footnote{Here $\omega<2.373$ is the current matrix multiplication complexity
exponent (\cite{le2014powers}).}.

\end{restatable}

When $m=O(n^{2})$, this improves substantially on existing bounds
(see Table~\ref{tab:complexity}). In particular, for an interesting
regime of $m=O(1)$, the $\dw$ is faster than the $\bw$ by the order
of $n^{2}$.

To improve the mixing time when the number of constraints $m$ is
large, in Section~\ref{sec:hybrid-psdSampling} we use a metric inspired
by a self-concordant barrier due to \cite{vaidya1996new}, which was
used to improve the complexity of the interior-point method for optimization.
The resulting Vaidya metric takes advantage of the \emph{leverage
scores} $\sigma(A_{X})$ of $A_{X}$, the diagonal entries of the
orthogonal projection matrix $P_{X}=A_{X}(A_{X}^{\top}A_{X})^{-1}A_{X}\in\R^{m\times m}$,
i.e., $\sigma(A_{X})_{i}:=\Par{P_{X}}_{i}$ for $i=1,\dots,m$. We
let $\Sigma_{X}$ be the diagonal matrix with $(\Sigma_{X})_{i}=\sigma(A_{X})_{i}$
for $i=1,\dots,m$. The resulting $\dw$ with this metric has $\sqrt{m}$-dependence. 

\begin{restatable}{thmre}{thmHybridPSD} \label{thm:hybridPSD}

Let $K$ be the constrained PSD cone in (\ref{eq:PSDcone}). Let $g$
be the local metric such that at each $X\in K$, for symmetric matrices
$H_{1},H_{2}$,
\[
g_{X}(H_{1},H_{2})=2n\tr\Par{X^{-1}H_{1}X^{-1}H_{2}}+44\sqrt{\frac{m}{n}}\vec(H_{1})^{\top}A_{X}^{\top}\Par{\Sigma_{X}+\frac{n}{m}I_{m}}A_{X}\vec(H_{2}).
\]
Then the $\dw$ with the local metric $g$ mixes in $\otilde{\Par{n+\sqrt{m}}n^{3}}$
steps, with each step running in $\otilde{mn^{2(\omega-1)}}$ amortized
time.

\end{restatable}

% Figure environment removed

See Figure~\ref{fig:betterBarrier}. Unlike the previous theorem,
this metric is not a Hessian of some barrier due to the term $A_{X}^{\top}\Sigma_{X}A_{X}$.
However, this term is close to the Hessian of the \emph{volumetric
barrier} of linear constraints (see Appendix~\ref{app:subsec:volBarrier}).
When $m<n(n+1)/2$, the leverage scores can be generalized as $\sigma(A_{X})=\diag\Par{A_{X}(A_{X}^{\top}A_{X})^{\dagger}A_{X}}$
by replacing the inverse with the Moore-Penrose inverse. If $A$ is
full-rank (i.e., linearly independent rows), then $A_{X}(A_{X}^{\top}A_{X})^{\dagger}A_{X}=A_{X}A_{X}^{\dagger}=I_{m}$,
and thus the second term in $g_{X}$ above becomes the Hessian of
the logarithmic barrier as in Theorem~\ref{thm:basicPSD}.

For very large $m$, this improved dependence is still polynomial
in $m$, and it is natural to ask if the dependence on $m$ can be
removed or made polylogarithmic. In Section~\ref{sec:LS-psdSampling},
we show that the \emph{Lewis weights} of $A_{X}$ enable the $\dw$
to achieve mixing without polynomial dependence on $m$ at the cost
of an additional factor of $n$. The $\ell_{p}$-Lewis weight of $A_{X}$
is the vector $w_{X}\in\R^{m}$ satisfying the implicit equation $w_{X}=\sigma\Par{\Diag(w_{X})^{\half-\frac{1}{p}}A_{X}}$.
Note that the leverage scores can be recovered as the $\ell_{2}$-Lewis
weight of $A_{X}$. By setting $p=O(\log m)$, the $\dw$ with a Lewis-weight
metric achieves poly-logarithmic dependence on $m$.

\begin{restatable}{thmre}{thmLSPSD} \label{thm:LSPSD}

Let $K$ be the constrained PSD cone in (\ref{eq:PSDcone}). Let $g$
be the local metric such that at each $X\in K$, for symmetric matrices
$H_{1},H_{2}$, 
\[
g_{X}(H_{1},H_{2})=2n\tr\Par{X^{-1}H_{1}X^{-1}H_{2}}+c_{1}\Par{\log m}^{c_{2}}\sqrt{n}\vec(H_{1})^{\top}A_{X}^{\top}W_{X}A_{X}\vec(H_{2}),
\]
where $W_{X}$ is the diagonalized $\ell_{p}$-Lewis weight of $A_{X}$
with $p=O(\log m)$, and $c_{1},c_{2}>0$ are universal constants.
Then the $\dw$ with local metric $g$ mixes in $\otilde{n^{5}}$
steps, with each step running in $\otilde{mn^{2(\omega-1)}}$ amortized
time.

\end{restatable}

We show that these theoretical guarantees also hold for approximate
Lewis weights. When $m<\frac{n(n+1)}{2}$ with full-rank $A$, this
metric is also equivalent to the metric in Theorem~\ref{thm:basicPSD}.
Taken together, our results show that the $\dw$ with appropriate
is significantly faster than the state-of-the-art (the $\bw$) for
every value of $m$, the number of affine constraints. 

\subsection{Background and related work}

Uniformly sampling convex bodies is a special case of \emph{logconcave
sampling}: sample from a distribution $\pi$ with density proportional
to $e^{-V(x)}$ for a convex function $V$ on $\Rn$. This problem
has spawned a long line of research in several communities, as it
captures various important distributions, including uniform distributions
over convex bodies and Gaussians.

A large body of recent work in machine learning and statistics makes
the assumption of $0\prec\alpha I\preceq\hess V(x)\preceq\beta I$
for $x\in\Rn$ (i.e., $\alpha$-strong convexity and $\beta$-smoothness
of the potential $V$), where the strong-convexity assumption is sometimes
relaxed to isoperimetry assumptions such as log-Sobolev inequalities
(LSI), Poincar inequality (PI), and Cheeger isoperimetry.  The guarantees
provided on the mixing time of samplers under this assumption have
polynomial dependence on the condition number defined as $\beta/\alpha$
(or $\alpha$ is replaced by the isoperimetric constant). These guarantees
do not apply to constrained sampling. For example, in uniform sampling,
the simplest constrained sampling problem, $V$ is set to be a constant
within the convex body and infinity outside the body, which leads
to discontinuity of $V$ and $\beta=\infty$. The sudden change of
$V$ around the boundary requires special consideration, such as small
step size, use of a Metropolis filter, projection, etc., making it
a more challenging problem.

\paragraph{Uniform sampling.}

Uniform sampling can be done by the $\bw$ (\cite{lovasz1993random,kannan1997random})
and \textsf{Hit-and-Run} (\cite{smith1984efficient}), which require
access to a function proportional to the density. When a convex body
$K\subset\Rn$ satisfies $B_{r}(x_{0})\subset K\subset B_{R}(x_{0})$
for some $x_{0}$, the $\bw$ mixes in $\otilde{n^{2}\Par{R/r}^{2}}$
steps from warm start (\cite{kannan1997random}) and $\textsf{Hit-and-Run}$
mixes in $\otilde{n^{2}\Par{R/r}^{2}}$ steps from any start\footnote{In our paper, \emph{warm start} means polynomial dependence on the
warmness parameter $M$, while \emph{any start} means poly-logarithmic
dependency on $M$. We assume any start unless specified otherwise.} (\cite{lovasz1999hit,lovasz2006hit}). \cite{lovasz2007geometry}
further extended these results to general logconcave distributions.
These algorithms need to use a ``step size'' of $O(1/\sqrt{n})$,
and their mixing is affected by the skewed geometry of the convex
body (i.e., when $R/r\gg1$). The latter can be addressed by first
\emph{rounding} the body, after which the $\bw$ and the $\textsf{Hit-and-Run}$
mix in $\otilde{n^{2}}$ steps from warm start, due to recent progress
on the KLS constant by \cite{chen2021almost,klartag2023logarithmic}
and stochastic localization by \cite{chen2022hit}. The fastest rounding
algorithm by \cite{jia2021reducing} requires $\otilde{n^{3}}$ queries
to a membership oracle, using the $\bw$.


\paragraph{Sampling with local geometry.}

The $\bw$ uses the same radius ball for every point in the convex
body. One might want to use a different radius depending on the distance
to the boundary. This by itself does not work as it simply makes the
current point converge to the boundary. However, replacing balls with
ellipsoids whose shape changes based on the proximity to the boundary
does work! Several sampling algorithms are motivated by the use of
local metrics: the $\dw$ (\cite{kannan2012random}), \textsf{Riemannian Hamiltonian Monte Carlo}
(RHMC), \textsf{Riemannian Langevin algorithm} (\cite{girolami2011riemann}),
etc.

Which local metrics would be suitable candidates? It turns out that
a suitable metric can be derived from self-concordant barriers, a
concept dating back to the development of the interior-point method
in convex-optimization literature (\cite{nesterov1994interior}).
It is well-known that any convex body admits an $n$-self-concordant
barrier such as universal barrier (\cite{nesterov1994interior,lee2021universal})
and entropic barrier (\cite{bubeck2014entropic,chewi2021entropic}),
but these are computationally expensive. Moreover, as noted in \cite{laddha2020strong},
the symmetry parameter of these general barriers is $\Omega(n^{2})$
for $n$-dimensional bodies (even for second-order cones), and so
the resulting complexity for the $\dw$ on (\ref{eq:PSDcone}) is
$\Omega(n^{2}\cdot n^{4})=\Omega(n^{6})$. Thus, there is a need to
find barriers that are more closely aligned with the structure of
sets we wish to sample. 

\paragraph{Polytope sampling.}

Samplers such as the $\bw$ and \textsf{Hit-and-Run} can be used
to sample polytopes, but they do not really use any special properties
of polytopes. In contrast, non-Euclidean samplers based on barriers
can leverage the properties of polytopes, making polytope-sampling
a fertile theoretical and practical ground where rich theory for linear
programming and sampling intersect. 

For polytopes with $m$ linear constraints in $n$-dimension ($m>n$),
the first theoretical result via self-concordant barriers dates back
to \cite{kannan2012random} who proposed the $\dw$ with the $m$-self-concordant
logarithmic barrier and established the mixing rate of $\otilde{mn}$
for uniform sampling. \cite{chen2018fast} revisited the idea of \cite{vaidya1996new}
using the $O(\sqrt{mn})$-self-concordant hybrid barrier, which is
a hybrid of the volumetric barrier and the log barrier and leads to
a faster interior-point method. They presented the $\dw$ with the
hybrid barrier giving an $\otilde{\sqrt{m}n^{3/2}}$-mixing guarantee.
Lastly, \cite{laddha2020strong} proposed the $\dw$ with a variant
of the $O^{*}(n)$-self-concordant LS barrier based on Lewis weights,
developed by \cite{lee2019solving}, and showed a mixing rate of $\otilde{n^{2}}$.

While a next point proposed by all these $\dw$ lies on a straight
line, the \textsf{Geodesic walk} and RHMC use curves (geodesics and
Hamiltonian-preserving curves respectively). \cite{lee2017geodesic}
and \cite{lee2018convergence} showed that for uniform sampling, the
\textsf{Geodesic walk} and RHMC with the log barrier mix in $\otilde{mn^{\frac{3}{4}}}$
and $\otilde{mn^{\frac{2}{3}}}$ steps respectively. \cite{kook2022condition}
extended theoretical analysis of RHMC to truncated exponential distributions
and showed that discretization of Hamilton's equations by practical
numerical integrators maintains a fast mixing rate. \cite{gatmiry2023sampling}
showed that just as the $\dw$ enjoys faster mixing via a barrier
with a better self-concordance parameter, RHMC with a hybrid barrier
consisting of the Lewis-weight and log barrier mixes in $\otilde{m^{\frac{1}{3}}n^{\frac{4}{3}}}$
steps. Their proof is based on developing suitable properties and
algorithmic bounds for Riemannian manifolds.

Extending these methods to PSD sampling, to potentially improve the
complexity of the problem significantly beyond the general bounds
that follow from convex-body sampling, has been an open research direction,
likely to be mathematically and algorithmically interesting in light
of the analogy with convex optimization. 


\subsection{Overview of ideas and contributions}

We refer readers to Section~\ref{subsec:prelim} for definitions
and notation.

\paragraph{Faster uniform sampling from PSD cone.}

Our work presents a polynomial-time sampling algorithm for general
PSD cones. Compared with the $\bw$, the known general method for
uniform sampling from convex bodies, the $\dw$ has two notable advantages:
\textbf{(1)} \emph{affine invariance} that eliminates the need for
preprocessing, and \textbf{(2)} \emph{polynomial-time mixing from
any starting point} within the PSD cone, due to poly-log dependence
on the warmness parameter. In contrast, both the $\bw$ and \textsf{Hit-and-Run}
require an expensive rounding phase. The $\bw$ also needs an $O(1)$-warm
start (i.e., $\Lambda=O(1)$) to achieve polynomial-time mixing. Our
improvements are attained by leveraging the geometric attributes of
the PSD cone, which admits an efficiently computable self-concordant
barrier --- the log-determinant barrier $\phi_{1}$ for semi-definiteness
together with the logarithmic barrier $\phi_{2}$ for linear constraints.

The sum of two metrics, $g=\hess\phi_{1}+\hess\phi_{2}$, serves as
a natural local metric for the $\dw$ to handle two types of constraints.
However, adapting the previous framework from \cite{laddha2020strong}
for the PSD setting presents challenges. It is unclear if $g$ is
strongly self-concordant, and verifying the convexity of $\log\det g$
is difficult. To address this, in Section~\ref{sec:prop-SC} we first
establish additive properties of strong self-concordance (Lemma~\ref{lem:sumStrongSC})
and provide simple sufficient conditions for lower trace self-concordance
of the sum of local metrics (Lemma~\ref{lem:additiveCondition}).
Second, we utilize matrix calculus and properties of the Kronecker
product to show in Section~\ref{subsec:scBasicMetric} that the log-determinant
barrier $\phi_{1}$ must be scaled by $n$ for strong self-concordance,
along with the optimality of  this scaling factor (Theorem~\ref{thm:logdet-scaling}).
Then we estimate the symmetry parameter of $\phi_{1}$ (Lemma~\ref{lem:logdet-symm})
and demonstrate its fourth-order convexity (i.e., $D^{4}\phi_{1}(X)[H,H]\succeq0$
for any $H\in\S^{n}$ via (\ref{eq:D4ph1})). In Lemma~\ref{lem:metricBasic},
we obtain strong and lower trace self-concordance and bound the symmetry
parameter of $g$. Applying our framework developed in Section~\ref{sec:mixingDikin}
to $g$, we obtain the mixing time $\otilde{n^{2}(n^{2}+m)}$ of the
$\dw$ as in Theorem~\ref{thm:basicPSD}.

\paragraph{Analysis of the $\protect\dw$ under trace self-concordance.}

\cite{laddha2020strong} analyzed the $\dw$ in $\Rn$ assuming the
following properties of the metric: strong self-concordance, $\onu$-symmetry,
and convexity of the log-determinant. In the case of the constrained
PSD cone, we intersect the PSD cone with linear constraints, and the
natural efficient metrics arise by adding metrics for the two different
types of constraints. While each one by itself could potentially satisfy
all the above (as verified for the log barrier in earlier work), it
does not follow that the sum of metrics satisfies the last condition.
Self-concordance is additive, and symmetry can be maintained for the
intersection, but verifying the last condition --- convexity of log
determinant --- proves challenging. We replace this restrictive condition
with \emph{lower trace self-concordance}, which is considerably weaker
and easier to check. In Section~\ref{sec:mixingDikin}, we demonstrate
that under this weaker condition, we can achieve the same mixing time
of $O\Par{n\onu\log\frac{\Lambda}{\veps}}$ as stated in Theorem~\ref{thm:generalMixing}.

Its overall proof of this theorem is analogous to \cite{laddha2020strong};
due to \cite{lovasz1993random}, it suffices to establish a lower
bound on the conductance of the $\dw$, which requires \textbf{(1)}
the one-step coupling of the $\dw$ (Lemma~\ref{lem:one-step}) and
\textbf{(2)} the isoperimetry for the Dikin distance (Lemma~\ref{lem:isoperimetry}).
For \textbf{(1)}, when bounding the overlap of the transition kernels
started at two close points, there are two sources of deviation ---
overlap of two Dikin ellipsoids and rejection probability. The former
leverages strong self-concordance and a geometric analysis of overlapping
Dikin ellipsoids. The latter comes down to establishing a constant
lower bound on $\vphi(z)-\vphi(x)$ with $\vphi(\cdot)=\log\det g(\cdot)$
for a proposal $z$ and current point $x$. After using Taylor's expansion
of $\vphi(z)$ at $z=x$, we bound the first-order term, $\tr\Par{g(x)^{-1}Dg(x)[z-x]}$,
via strong self-concordance and the concentration of measure. For
the second-order term, $\tr\Par{g^{-1}D^{2}g[h,h]}-\norm{g^{-\half}Dgg^{-\half}}_{F}^{2}$,
lower trace and strong self-concordance play crucial roles. For \textbf{(2)},
following \cite{laddha2020strong}, we recall the isoperimetric inequality
in terms of the cross-ratio distance $d_{K}$ over a convex body $K$,
which states $\vol(S_{3})\vol(K)\geq d_{K}(S_{1},S_{2})\vol(S_{1})\vol(S_{2})$
for any partition $\{S_{1},S_{2},S_{3}\}$ of $K$. We then use $d_{K}(x,y)\geq\norm{x-y}_{g(x)}/\sqrt{\onu}$
to prove that the isoperimetry of the uniform distribution over $K$
with the Dikin distance $\norm{\cdot}_{g(x)}$ is $\Omega\Par{1/\sqrt{\onu}}$.

\paragraph*{Efficient implementation of each step.}

In Section~\ref{subsec:oracleImplementation}, we propose an efficient
per-step implementation that avoids explicit computation of a local
metric and its inverse of size $\frac{n(n+1)}{2}\times\frac{n(n+1)}{2}$
(Algorithm~\ref{alg:perStep-small-m} and Lemma~\ref{lem:perStep-small-m}).
Each step of the $\dw$ involves \textbf{(1)} uniform sampling from
$Z\sim\dcal_{g}^{r}(X)$ and \textbf{(2)} computing the acceptance
probability, $\min\Par{1,\sqrt{\frac{\det g(Z)}{\det g(X)}}}$. To
achieve efficiency, we first design a subroutine for computing $g(X)^{-1}v$
for a given vector $v$ in $O\Par{mn^{\omega}+m^{2}n^{2}}$ time (Algorithm~\ref{alg:subroutine}
and Proposition~\ref{prop:oracle}). To this end, we find matrices
$B\in\R^{d\times n^{2}}$ and $U\in\R^{d\times m}$ such that $g_{1}=\hess\phi_{1}=BB^{\top}$
and $g_{2}=\hess\phi_{2}=UU^{\top}=\sum_{i=1}^{m}u_{i}u_{i}^{\top}$,
with  $u_{i}$ being the column vectors of $U$. Viewing $g=g_{1}+g_{2}$
as a series of rank-one updates to $g_{1}$ by $u_{i}u_{i}^{\top}$
for $i\in[m]$, we apply the Sherman-Morrison formula to recursively
update $\Par{g_{1}+\sum_{i=1}^{j}u_{i}u_{i}^{\top}}^{-1}v,\Par{g_{1}+\sum_{i=1}^{j}u_{i}u_{i}^{\top}}^{-1}u_{l}$
for $l\in[m]$, starting from $j=1$ to $m$. These updates leverage
the linear transformation defined in Section~\ref{subsec:formalism}
facilitating smooth transitions between $X\in\S^{n}$ and $\vec(X)\in\R^{n(n+1)/2}$.
This approach effectively reduces costly calculations on one side
to simpler ones on another, avoiding the materialization of $g$ and
$g^{-1}$.

With this core subroutine in place, we proceed to implement those
two main steps. For \textbf{(1)}, instead of computing $g(X)^{-\half}v$
with $v\sim\text{Uniform}(B_{r}(0))$, we (i) draw $v\sim\ncal(0,g(X)^{-1})$
without fully computing $g(X)^{-1}$ and (ii) use this to generate
a uniform sample from $\dcal_{g}^{r}(X)$ in $O\Par{mn^{\omega}+m^{2}n^{2}}$
time. For (i), we observe that $g(X)^{-1}\left[\begin{array}{cc}
B & U\end{array}\right]v$ with $v\sim\ncal\Par{0,I_{n^{2}+m}}$ follows $\ncal(0,g(X)^{-1})$.
Utilizing the linear transformation between $X\in\S^{n}$ and $\vec(X)\in\R^{n(n+1)/2}$,
we compute $\left[\begin{array}{cc}
B & U\end{array}\right]v$ efficiently and employ the subroutine to multiply $g(X)^{-1}$ on
the left side. Regarding (ii), we adapt the observation that a uniform
sample from $B_{r}(0)\subset\R^{d}$ can be obtained by $s^{1/d}\zeta/\norm{\zeta}_{2}$
with $s\sim\text{Uniform}([0,r])$ and $\zeta\sim\ncal(0,I_{d})$.
For \textbf{(2)}, we revisit the perspective of viewing $g$ as rank-one
updates to $g_{1}$ by $u_{i}u_{i}^{\top}$, using the matrix determinant
lemma.

\paragraph{Hybrid metrics.}

Despite the challenge of preserving properties like log-determinant
convexity when combining metrics, we demonstrate that the improvement
on $m$-dependency achieved in polytope sampling using better self-concordant
metrics (e.g., log-barriers to Vaidya metrics to Lewis-weight metrics)
can also be realized for PSD sampling, albeit with considerably more
technical challenges. The $\dw$ with the Vaidya metric and Lewis-weight
metric (along with $\hess\phi_{1}$) mixes in $\otilde{(n+\sqrt{m})n^{3}}$
and $\otilde{n^{5}}$ iterations respectively as stated in Theorem~\ref{thm:hybridPSD}
in Section~\ref{sec:hybrid-psdSampling} and Theorem~\ref{thm:LSPSD}
in Section~\ref{sec:LS-psdSampling}, respectively. Notably, by setting
$g_{1}$ to $0$, our framework recovers the mixing time of the \textsf{Vaidya walk}
for polytope sampling in \cite{chen2018fast}, providing a substantially
simpler analysis for this special case. Alongside, by using clean
matrix algebra we identify the appropriate scaling of self-concordant
metrics for linear constraints $Ax\leq b$ with $A\in\R^{m\times n}$
and $b\in\R^{m}$ that ensures strong self-concordance: $\sqrt{m}$
for approximate volumetric metrics, $\sqrt{m/n}$ for Vaidya metrics
(Lemma~\ref{lem:paramsBarrier}), and $(\log m)^{O(1)}$ for Lewis-weight
metrics (Lemma~\ref{lem:LSmetricStrongandSymmetry}). For lower trace
self-concordance, $\sqrt{m/n}$-scaling is sufficient for Vaidya metrics
(Lemma~\ref{lem:hybridLowerSCTrace}), while Lewis-weight metrics
require an additional scaling of $\sqrt{n}$ along with $(\log m)^{O(1)}$
(Lemma~\ref{lem:LSLowerSCTrace}).

In analyzing the Vaidya and Lewis-weight metrics, a main technical
challenge is to find the minimal scaling of these metrics for two
key properties: \textbf{(1)} strong self-concordance and symmetry,
and \textbf{(2)} lower trace self-concordance. For \textbf{(1)}, we
investigate a metric of the form $A_{x}^{\top}D_{x}A_{x}$ for a diagonal
matrix $D_{x}$, which is designed to respect affine constraints.
In Lemma~\ref{lem:helper4Diagonal}, we relate the notions of strong
self-concordance and $\onu$-symmetry to well-studied terms in the
field of optimization, namely $\max_{i}\frac{\sigma\Par{\sqrt{W_{x}}A_{x}}_{i}}{(D_{x})_{ii}}$
and $\norm{DD_{x}[h]}_{D_{x}^{-1}}^{2}$. Then we can refer to existing
bounds on these terms, estimating the smallest possible scaling required
for strong self-concordance and symmetry. For \textbf{(2)}, ensuring
trace self-concordance of $g$ with $g_{2}$ being the Vaidya or Lewis
metrics is challenging, as $D^{2}g_{2}[h,h]\succeq0$ is difficult
to directly verify due to complicated expressions for $D^{2}\Sigma_{x}[h,h]$
and $D^{2}W_{x}[h,h]$. To address this, we aim to show $\tr\Par{(g_{1}+g_{2})^{-1}D^{2}g_{2}[h,h]}\geq-\alpha\norm h_{g}^{2}$
for a small constant $\alpha>0$ and then refer to Lemma~\ref{lem:additiveCondition}
for trace self-concordance of $g$. In the case of the Vaidya metric,
we compute higher-order derivatives of leverage scores and other pertinent
matrices in Proposition~\ref{prop:calculusLeverage}, based on properties
of the Hadamard product, and use these results to derive lower trace
self-concordance of $g$. For the Lewis-weight metric, the situation
is more complicated due to numerous terms appearing in $D^{2}W_{x}[h,h]$.
In order to avoid dealing with each of the terms, we employ Calabi-type
estimates on $W_{x}$ and other relevant matrices. This strategy significantly
simplifies the computation but comes at the cost of an additional
scaling of $\sqrt{n}$, which as far as we can tell might be unavoidable.


\subsection{Preliminaries and notation \label{subsec:prelim}}

\paragraph{Basics.}

For $n\in\mathbb{N}$, let $[n]:=\{1,\cdots,n\}$. The $\widetilde{O}$
complexity notation suppresses poly-logarithmic factors and dependence
on error parameters. For $v\in\Rn$, the Euclidean norm (or $\ell_{2}$-norm)
is denoted by $\norm v_{2}\defeq\sqrt{\sum_{i\in[n]}v_{i}^{2}}$,
and the infinity norm is denoted by $\norm v_{\infty}\defeq\max_{i\in[n]}\Abs{v_{i}}$.
A Gaussian distribution with mean $\mu\in\Rn$ and covariance $\Sigma\in\Rnn$
is denoted by $\ncal(\mu,\Sigma)$.

\paragraph{Matrices.}

We use $\S^{n}$ to denote the set of symmetric matrices of size $n\times n$.
For $X\in\S^{n}$, we call it \emph{positive semidefinite} (PSD) (resp.
\emph{positive definite} (PD)) if $h^{\top}Xh\geq0$ ($>0)$ for any
$h\in\R^{n}$. We use $\psd$ to denote the set of positive definite
matrices of size $n\times n$. Note that their effective dimension
is $d:=n(n+1)/2$ due to symmetry. For a positive (semi) definite
matrix $X$, its \emph{square root} is denoted as $X^{\half}$, and
is the unique positive (semi) definite matrix satisfying $X^{\half}X^{\half}=X$.
For $A,B\in\S^{n}$, we use $A\preceq B$ ($A\prec B$) to indicate
that $B-A$ is PSD (PD). For a matrix $A\in\R^{n\times n}$, its \emph{trace}
is denoted by $\tr\Par A=\sum_{i=1}^{n}A_{ii}$. The \emph{operator
norm} and \emph{Frobenius norm} are denoted by $\norm A_{2}\defeq\sup_{x\in\Rn}\norm{Ax}_{2}/\norm x_{2}$
and $\norm A_{F}\defeq\Par{\sum_{i,j=1}^{n}A_{ij}^{2}}^{\half}=\sqrt{\tr\Par{A^{\top}A}}$,
respectively.

\paragraph{Basic operations.}

For $X\in\S^{n}$, its \emph{vectorization} $\vec{(}X)\in\R^{n^{2}}$
is obtained by stacking each column of $X$ vertically. Its symmetric
vectorization $\svec(X)\in\R^{d}$ is obtained by stacking the lower
triangular part in vertical direction. For a matrix $A\in\R^{n\times n}$
and vector $x\in\Rn$, we use $\diag(A)$ to denote the vector in
$\Rn$ with $(\diag(A))_{i}=A_{ii}$ for $i\in[n]$, $\Diag(A)$ to
denote the diagonal matrix with $(\Diag(A))_{ii}=A_{ii}$ for $i\in[n]$
and $\Diag(x)$ to denote the diagonal matrix in $\R^{n\times n}$
with $(\Diag(x))_{ii}=x_{i}$ for $i\in[n]$.

\paragraph{Matrix operations.}

For matrices $A,B\in\R^{n\times n}$, their inner product is defined
as the inner product of $\vec(A)$ and $\vec(B)$, denoted by $\inner{A,B}=\tr\Par{A^{\top}B}$.
Their \emph{Kronecker product} $A\kro B$ is the matrix of size $n^{2}\times n^{2}$
defined by 
\[
A\otimes B=\left[\begin{array}{ccc}
A_{11}B & \cdots & A_{1n}B\\
\vdots &  & \vdots\\
A_{n1}B & \cdots & A_{nn}B
\end{array}\right],
\]
where $A_{ij}B$ is the block matrix of size $n\times n$ obtained
by multiplying each entry of $B$ by the scalar $A_{ij}$. Their \emph{Hadamard
product} $A\circ B$ is the matrix of size $n\times n$ defined by
$(A\hada B)_{ij}=A_{ij}B_{ij}$ (i.e., obtained by element-wise multiplication).

\paragraph{Projection matrix, Leverage score and Lewis weights.}

For a full-rank matrix $A\in\R^{m\times n}$ with $m\geq n$, we recall
that $P(A):=A(A^{\top}A)^{-1}A^{\top}$ is the orthogonal projection
matrix onto the column space of $A$. The leverage scores of $A$
is denoted by $\sigma(A):=\diag(P(A))\in\R^{m}$. We let $\Sigma(A):=\Diag(\sigma(A))=\Diag(P(A))$
and $P^{(2)}(A):=P(A)\circ P(A)$. The $\ell_{p}$-Lewis weights of
$A$ is denoted by $w(A)$, the solution $w$ to the equation $w(A)=\diag\Par{W^{\half-\frac{1}{p}}A\Par{A^{\top}W^{1-\frac{2}{p}}A}^{-1}A^{\top}W^{\half-\frac{1}{p}}}\in\R^{m}$
for $W:=\Diag(w)$. When $m<n$ or $A$ is not full rank, both leverage
scores and Lewis weights can be generalized via the Moore-Penrose
inverse in place of the inverse in the definitions.

\paragraph{Derivatives.}

For a function $f:\R^{n}\to\R$, let $\grad f(x)\in\R^{n}$ denote
the gradient of $f$ at $x$ (i.e., $\Par{\grad f(x)}_{i}:=\frac{\del f}{\del x_{i}}(x)$)
and $\hess f(x)\in\R^{n\times n}$ denote the Hessian of $f$ at $x$
(i.e., $\Par{\hess f(x)}_{ij}:=\frac{\del^{2}f}{\del x_{i}\del x_{j}}(x)$).
For a matrix function $g:\R^{n}\to\R^{n\times n}$ in $x$, we use
$Dg$ and $D^{2}g$ to denote the third-order and fourth-order tensor
defined by $(Dg(x))_{ijk}=\frac{\del\Par{g(x)}_{ij}}{\del x_{k}}$
and $(D^{2}g(x))_{ijkl}=\frac{\del^{2}\Par{g(x)}_{ij}}{\del x_{k}\del x_{l}}$.
We use the following shorthand notation: $g_{x,h}':=Dg(x)[h]$ and
$g_{x,h}'':=D^{2}g(x)[h,h]$. We let $D^{i}g(x)[h_{1},\cdots,h_{i}]$
denote the $i^{\text{th}}$ directional derivative of $g$ at $x$
in directions $h_{1},\cdots,h_{i}\in\Rn$, i.e.,
\[
D^{i}g(x)[h_{1},\cdots,h_{i}]=\frac{d^{i}}{dt_{1}\cdots dt_{i}}g\Par{x+\sum_{j=1}^{i}t_{j}h_{j}}\bigg|_{t_{1},\cdots,t_{i}=0}.
\]


\paragraph*{Local norm.}

At each point $x$ in a set $K\subset\Rn$, a \emph{local metric}
$g$, denoted as $g_{x}$ or $g(x)$, is a positive-definite inner
product $g_{x}:\R^{n}\times\R^{n}\to\R$, which naturally induces
the local norm as $\norm v_{g(x)}:=\sqrt{g_{x}(v,v)}$. We use $\norm v_{x}$
to refer to $\norm v_{g(x)}$ when the context is clear. When an ambient
space has an orthogonal basis as in our setting (e.g., $\{e_{1},\dots,e_{n}\}$),
the local metric $g_{x}$ can be represented as a positive-definite
matrix of size $n\times n$. With this perspective, the inner product
can be written as $g_{x}(v,w)=v^{\top}g(x)w$. Going forward, we use
$g_{x}=g(x)$ to denote a local metric (or positive definite matrix
of size $\dim(x)\times\dim(x)$) at each point $x\in K$. The local
metric $g$ is assumed to be at least twice differentiable.

\paragraph{Markov chains.}

Many sampling algorithms are based on \emph{Markov chains}. A \emph{transition
kernel} $P:\Rn\times\bcal(\Rn)\to\R_{\geq0}$ (or \emph{one-step distribution})
for the Borel $\sigma$-algebra $\bcal(\Rn)$ quantifies the probability
of the Markov chains transitioning from one point to another measurable
set. The next-step distribution is defined by $P_{x}(A):=P(x,A)$,
which is the probability of a step from $x$ landing in the set $A$.
The transition kernel characterizes the Markov chain in the sense
that if a current distribution is $\mu$, then the distribution after
$n$ steps can be expressed as $\mu P^{(n)}$, where $\mu P^{(i)}:=\int_{\Rn}P(x,\cdot)\cdot(\mu P^{(i-1)})(dx)$
is defined recursively for $i=1,...,n$ with the convention $\mu P^{(0)}(x)=\mu(x)$.
We call $\pi$ a \emph{stationary distribution} of the Markov chain
if $\pi=\pi P$. If the stationary distribution further satisfies
$\int_{A}P(x,B)\pi(dx)=\int_{B}P(x,A)\pi(dx)$ for any two measurable
subsets $A,B$, then the Markov chain is said to be \emph{reversible}
with respect to $\pi$.

It is expected that the Markov chain approaches the stationary distribution.
We measure this with the \emph{total variation distance} (TV-distance):
for two distributions $\mu$ and $\pi$ on $\Rn$, the TV-distance
is defined as $\dtv(\mu,\pi)=\sup_{A\in\bcal(\Rn)}\Abs{\mu(A)-\pi(A)}=\half\int_{\Rn}\Abs{\frac{d\mu}{dx}-\frac{d\pi}{dx}}dx$,
where the last equality holds when the two distributions admit densities
with respect to the Lebesgue measure on $\Rn$. Moreover, the rate
of convergence can be quantified by the \emph{mixing time}: for an
error parameter $\veps\in(0,1)$ and an initial distribution $\pi_{0}$,
the mixing time is defined as the smallest $n\in\N$ such that $\dtv(\pi_{0}P^{(n)},\pi)\leq\veps$.
In this paper, we consider a \emph{lazy} Markov chain, which does
not move with probability $1/2$ at each step, in order to avoid a
uniqueness issue of a stationary distribution. Note that this change
worsens the mixing time by at most a factor of $2$. One of the standard
tools to control progress made by each iterate is the \emph{conductance}
$\Phi$ of the Markov chain with its stationary distribution $\pi$,
defined by $\Phi\defeq\inf_{\text{measurable }S}\frac{\int_{S}P(x,S^{c})\pi(dx)}{\min\Par{\pi(S),\pi(S^{c})}}$.
Another crucial factor affecting the convergence rate is geometry
of the stationary distribution $\pi$, as measured by \emph{Cheeger
isoperimetry} $\psi_{\pi}\defeq\inf_{S}\frac{\lim_{\delta\to0^{+}}\pi\Par{\{x:\,0<d(S,x)\leq\delta\}}/\delta}{\min(\pi(S),\pi(S^{c}))}$
where the infimum is over measurable subsets $S$ and $d(S,x)$ is
some distance between $x$ and the set $S$.

\input{020propSC.tex}\input{030mixing.tex}\input{040basicPSD.tex}\input{050vaidyaPSD.tex}\input{060LSPSD.tex}
\begin{acknowledgement*}
We thank Khashayar Gatmiry for helpful discussions. This work was
supported in part by NSF awards CCF-2007443 and CCF-2134105.
\end{acknowledgement*}
\bibliography{Dikin-PSD} 

\newpage{}

\input{099appendix.tex}
\end{document}
