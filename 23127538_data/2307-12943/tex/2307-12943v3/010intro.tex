
\section{Introduction}

As a motivating example, consider the following problem: how can we
efficiently sample an $n\times n$ matrix from a distribution with
the following density?
\begin{align*}
\text{sample } & X\sim\exp\Par{-\Par{\inner{A,X}+\norm{X-B}_{F}+\norm{X-C}_{F}^{2}-\log\det X}}\\
\text{s.t. } & X\succeq0,\,\inner{D_{i},X}\geq c_{i}\,\forall i\in[m]
\end{align*}
This rather complicated looking distribution recovers as special cases
the problems of sampling from the Max-Cut semi-definite programming
relaxation and the set of minimum (or bounded) volume ellipsoids that
contain a given set of points. The above density function is logconcave,
and so we can use the $\bw$ to sample the distribution with $O\Par{n^{8}\log n}$
membership/evaluation queries (\cite{lovasz2007geometry}). This ``general-purpose''
sampler already gives a poly-time mixing algorithm. However, each
term in the density function and constraints above is ``structured'',
and this naturally motivates the following question: can we leverage
structure inherent in the problem to get more efficient algorithms?

The interior-point method (IPM) is a powerful optimization framework
suitable for solving convex optimization problems when the problem
has structured objectives and constraints: for proper convex functions
$f_{i}$ and $h_{j}$
\begin{align*}
\min & \sum_{i}f_{i}(x)\ \text{s.t. }h_{j}(x)\leq0.
\end{align*}
Is there a sampling analogue of IPM that generates samples from the
density proportional to $e^{-\sum_{i}f_{i}}$ restricted to the convex
region defined by structured convex functions $h_{j}$? This is the
genreal problem we will address here and is stated formally below.
\begin{problem*}
Let $f_{i}$ be a proper convex function and $h_{j}$ a convex function
on $\Rn$ for $i\in[I]$ and $j\in[J]$. Then we are interested in
\begin{align}
\text{sample } & \frac{d\pi}{dx}\propto e^{-\sum_{i}f_{i}(x)}\label{eq:problem}\\
\text{s.t. } & x\in K:=\bigcap_{j\in[J]}\Brace{x\in\Rn:h_{j}(x)\leq0},\nonumber 
\end{align}
where we assume that $K$ has non-empty interior and $\pi$ has finite
second moment.
\end{problem*}
In this paper, we derive an IPM framework for structured logconcave
sampling. We use the $\dw$ as a sampler to implement the ``inner''
step of the IPM. We provide a mixing time bound for the $\dw$, going
beyond uniform distributions (Section~\ref{sec:mixing-Dikin}). This
generalization is necessary to be able to utilize the $\dw$ within
the IPM framework. In Section~\ref{sec:IPM-framework}, we present
the sampling IPM and derive its guarantees. Our framework is suited
for breaking down complicated sampling problems into smaller structured
problems. An important part of this paper is Section~\ref{sec:sc-theory-rules},
where we develop a ``calculus'' for combining multiple constraints
and objectives and deriving the resulting theoretical guarantees (analogous
to and inspired by the work of \cite{nesterov1994interior} for optimization).
To provide concrete understanding and instances, we apply the framework
to some well-known families of constraints in Section~\ref{sec:handbook-barrier}.

% Figure environment removed


\paragraph{Dikin walk.}

Given a local metric $g$ in $\R^{n}$, the Dikin ellipsoid of radius
$r$ at $x\in\Rn$ is defined as
\[
\dcal_{g}^{r}(x)\defeq\Brace{y\in\Rn:\sqrt{(y-x)^{\top}g(x)(y-x)}=\norm{y-x}_{g(x)}\leq r},
\]
i.e., it is a norm ball of radius $r$ defined by the local metric.
From this perspective, the $\dw$ defined below is a natural generalization
of the $\bw$ to a local metric setting. For convenience, we define
the notation $\ncal_{g}^{r}(x):=\ncal\Par{x,\frac{r^{2}}{n}g(x)^{-1}}$. 

\begin{algorithm2e}[H]

\caption{$\dw(g,\pi_{0},\pi,\veps)$}\label{alg:DikinWalk}

\SetAlgoLined

\textbf{Input:} Initial distribution $\pi_{0}$, target distribution
$\frac{d\pi}{dx}\propto e^{-f(x)}$, local metric $g$, step size
$r$, target accuracy $\veps$.

\textbf{Output:} $x_{T}$

Draw an initial point $x_{0}\sim\pi_{0}$ at random. 

\For{$t=1,\cdots,T$}{

Pick $z$ from $\ncal\Par{x_{t-1},\frac{r^{2}}{n}g(x_{t-1})^{-1}}$.

$x_{t}\gets z$ w.p. $\min\Par{1,\frac{p_{z}(x_{t-1})}{p_{x_{t-1}}(z)}\frac{\exp\Par{-f(z)}}{\exp\Par{-f(x_{t-1})}}}$,
where $p_{x}(z)=\frac{\sqrt{\det g(x)}}{(2\pi)^{n/2}}e^{-\frac{n}{2r^{2}}\norm{z-x}_{g(x)}^{2}}$.

Otherwise, $x_{t}\gets x_{t-1}$.

}

\end{algorithm2e}

\cite{kannan2012random} analyzed the $\dw$ to uniformly sample a
polytope. They make use of structural properties of polytopes (i.e.,
linear constraints) via the logarithmic barrier, resulting in advantages
over a general-purpose sampler like the $\bw$. In subsequent work,
\cite{narayanan2016randomized} extended the approach to analyze the
$\dw$ for sampling more general cones, including slices of the PSD
cone. 

\paragraph{Dikin metrics and self-concordance.}

The metric $g$ used to define the $\dw$ plays a crucial role in
its convergence. Our metrics will be defined by Hessians of convex
self-concordant barrier functions. We now collect definitions of these
functions; they will be important to state our general guarantees
for the mixing of the $\dw$. The concept we need is summarized by
the definition of a $(\nu,\onu)$\emph{-Dikin-amenable metric}.
% Figure environment removed

\begin{defn}
[Self-concordance] \label{def:sc} For convex $K\subset\Rn$, let
$\phi:\intk\to\R$ be a convex function, $g:\intk\to\S_{+}^{n}$ a
PSD matrix function, and $\ncal_{g}^{r}(x):=\ncal\Par{x,\frac{r^{2}}{n}g(x)^{-1}}$.
\begin{itemize}
\item \emph{Self-concordance} (SC): A $C^{3}$-function $\phi$ is called
a self-concordant barrier if $\Abs{D^{3}\phi(x)[h,h,h]}\leq2\norm h_{\hess\phi(x)}^{3}$
for any $x\in\intk$ and $h\in\Rn$, and $\lim_{x\to\del K}\phi(x)=\infty$.
The first condition is equivalent to $-2\norm h_{\hess\phi(x)}\hess\phi(x)\preceq D^{3}\phi(x)[h]\preceq2\norm h_{\hess\phi(x)}\hess\phi(x)$.
We call it a $\nu$-self-concordant barrier for $K$ if $\sup_{h\in\Rn}\Par{2\inner{\grad\phi(x),h}-\norm h_{\hess\phi(x)}^{2}}\leq\nu$
for any $x\in\intk$ in addition to self-concordance. A $C^{1}$-PSD
matrix function $g:\intk\to\S_{+}^{n}$ is called self-concordant
if $-2\norm h_{g(x)}g\preceq Dg(x)[h]\preceq2\norm h_{g(x)}g$ for
any $x\in\intk$ and $h\in\Rn$, and there exists a self-concordant
function $\phi:\intk\to\R$ such that $\hess\phi\asymp g$ on $\intk$.
We call it a $\nu$-self-concordant barrier for $K$ if its counterpart
$\phi$ is $\nu$-self-concordant.
\item \emph{Highly self-concordant function} (HSC): A $C^{4}$-function
$\phi$ is called highly self-concordant if $\Abs{D^{4}\phi(x)[h,h,h,h]}\leq6\norm h_{\hess\phi(x)}^{4}$
for any $x\in\intk$ and $h\in\Rn$, and $\lim_{x\to\del K}\phi(x)=\infty$.
\item \emph{Strong self-concordance} (SSC): A SC matrix function $g$ is
called strongly self-concordant if $g$ is PD on $\intk$ and $\norm{g(x)^{-\half}Dg(x)[h]g(x)^{-\half}}_{F}\leq2\norm h_{g(x)}$
for any $x\in\intk$ and $h\in\Rn$. We call a SC function $\phi$
strongly self-concordant if $\hess\phi(x)$ is strongly self-concordant.
\item \emph{Lower trace self-concordant matrix} (LTSC): A SC matrix function
$g$ is called lower trace self-concordant if $g$ is PD on $\intk$
and $\tr\Par{g(x)^{-1}D^{2}g(x)[h,h]}\geq-\norm h_{g(x)}^{2}$ for
any $x\in\intk$ and $h\in\Rn$. We call it strongly lower trace self-concordant
(SLTSC) if for any PSD matrix function $\bar{g}$ on $\intk$ it holds
that $\tr\Par{\Par{\bar{g}(x)+g(x)}^{-1}D^{2}g(x)[h,h]}\geq-\norm h_{g(x)}^{2}$
for any $x\in\intk$ and $h\in\Rn$.
\item \emph{Average self-concordance} (ASC): A matrix function $g$ is called
average self-concordant if for any $\veps>0$ there exists $r_{\veps}>0$
such that $\P_{z\sim\ncal_{g}^{r}(x)}\Par{\norm{z-x}_{g(z)}^{2}-\norm{z-x}_{g(x)}^{2}\leq2\veps\frac{r^{2}}{n}}\geq1-\veps$
for $r\leq r_{\veps}$. We call it strongly average self-concordant
(SASC) if for $\veps>0$ and any PSD matrix function $\bar{g}$ on
$\intk$ it holds that $\P_{z\sim\ncal_{g+\bar{g}}^{r}(x)}\Par{\norm{z-x}_{g(z)}^{2}-\norm{z-x}_{g(x)}^{2}\leq2\veps\frac{r^{2}}{n}}\geq1-\veps$
for $r\leq r_{\veps}$.
\end{itemize}
\end{defn}

Note that SC and HSC can be defined even when $\hess\phi$ or $g$
is singular (or degenerate). Next, we recall a symmetry parameter
of a self-concordant metric. We will later see that it has a natural
connection to the Cheeger isoperimetry.
\begin{defn}
[$\onu$-symmetry] \label{def:symm-param} For convex $K\subset\Rn$,
a PSD matrix function $g:\intk\to\psd$ is said to be $\onu$-\emph{symmetric}
if $\dcal_{g}^{1}(x)\subseteq K\cap(2x-K)\subseteq\dcal_{g}^{\sqrt{\onu}}(x)$
for any $x\in K$.
\end{defn}

We note that $K\cap(2x-K)$ is the locally symmetrized convex body
with respect to $x$. Hence, $\onu$-symmetry measures how accurately
a Dikin ellipsoid approximates the locally symmetrized body. One can
show that $\onu=O(\nu^{2})$ for any metric induced by a self-concordant
barrier.

Going forward, we call a PD matrix function $\onu$\emph{-Dikin-amenable}
if it is SSC, LTSC, ASC, and $\onu$-symmetric. We sometimes call
it $(\nu,\onu)$-Dikin-amenable to reveal its self-concordance parameter
$\nu$. For example, the Hessian of a logarithmic barrier is an $(m,m)$-Dikin-amenable
metric. We present more concrete examples after introducing Theorem~\ref{thm:IPM-sampling}.

\subsection{Results}

\paragraph{Dikin walk.}

We begin with our analysis of the $\dw$ for general settings (Section~\ref{sec:mixing-Dikin}),
going beyond uniform distributions.

\begin{restatable}{thmre}{thmDikin} \label{thm:Dikin} 

Let $K\subset\Rn$ be convex and $0\leq\alpha\leq\beta<\infty$.
\begin{itemize}
\item (Local metric) Assume that a $C^{1}$-matrix function $g:\intk\to\pd$
is $\onu$-Dikin-amenable. 
\item (Distribution) Let $\pi_{0}$ be an initial distribution and $\pi$
be a target distribution with $\frac{d\pi}{dx}\propto e^{-f(x)}\cdot\mathbf{1}_{K}(x)$,
where $f$ is $\alpha$-relatively strongly convex and $\beta$-smooth
in $g$. Let $\Lambda$ be the minimum of the warmness $\sup_{S\subset K}\frac{\pi_{0}(S)}{\pi(S)}$
and the $L^{2}$-distance $\norm{\pi_{0}/\pi}=\E_{\pi_{0}}\frac{d\pi_{0}}{d\pi}$.
Let $\pi_{T}$ be the distribution obtained after $T$ steps of the
$\dw$ (Algorithm~\ref{alg:DikinWalk}) started at $x_{0}\sim\pi_{0}$
with the local metric $g$.
\end{itemize}
Then for any $\veps>0$ the total variation distance between $\pi_{T}$
and $\pi$ satisfies $\dtv(\pi_{T},\pi)\leq\veps$ for $T\lesssim n\max(1,\beta)\min\Par{\onu,\frac{1}{\alpha}}\log\frac{\Lambda}{\veps}$.

\end{restatable}

This result serves as a unifying framework that recovers as special
cases previous works on the $\dw$ for uniform sampling (\cite{kannan2012random,narayanan2016randomized,chen2018fast,laddha2020strong}),
as seen later in Section~\ref{sec:examples}. Our analysis extends
beyond uniform sampling, considering the $\dw$ under a more general
setting where the potential $f$ satisfies $\alpha g\preceq\hess f\preceq\beta g$
on $\intk$. This setting is a generalization of $\alpha I\preceq\hess f\preceq\beta I$
under a local metric $\hess\phi\asymp g$. We also note that the $\dw$
is the first \emph{implementable} algorithm that provides a clean
mixing guarantee under this general setting. We refer readers to Section~\ref{subsec:related-work}
for related work.

We will use this as a key ingredient of the sampling IPM. 

\paragraph{Gaussian cooling with the Dikin walk ($\protect\gcdw$).}

\begin{restatable}{thmre}{thmDikinannealing} \label{thm:Dikin-annealing} 

For convex $K\subset\Rn$, suppose that $g:\intk\to\pd$ is $(\nu,\onu)$-Dikin-amenable
and $\phi$ is its function counterpart such that $\min_{K}\phi$
exists. $\gc$ with the $\dw$ (Algorithm~\ref{alg:IPM-sampling}
with Algorithm~\ref{alg:DikinWalk}) generates a sample from $e^{-f}\cdot1_{K}$
using $O\Par{n\max\Par{n\frac{\nu\beta+n}{\nu\alpha+n},\nu,\onu}\log\frac{n\nu}{\veps}}$
iterations of the $\dw$ with $g$, where a $C^{2}$-function $f:\intk\to\R$
satisfies $\alpha\hess\phi\preceq\hess f\preceq\beta\hess\phi$ on
$K$ for $0\leq\alpha\leq\beta<\infty$. In particular, when $f(x)=\alpha^{\top}x$
or $c\phi(x)$ for $\alpha\in\Rn$ and $c\in\R_{+}$, the algorithm
uses $\otilde{n\max(n,\nu,\onu)}$ iterations of the $\dw$.

\end{restatable}

The inner loop of $\gc$ runs the $\dw$. The basic GC algorithm was
introduced in \cite{cousins2018gaussian} for efficient sampling and
volume computation. It was later used for the proximal sampling under
the name Restricted Gaussian oracle (RGO) in \cite{lee2021structured}.
\cite{lee2018convergence} studied its extension to Hessian manifolds
for uniformly sampling polytopes. Our framework is more general, and
can handle more general distributions through a more sophisticated
annealing scheme.

We note that this framework of sampling IPM provides an efficient
algorithm for generating a warm start for constrained log-concave
distributions. If we were to apply Theorem~\ref{thm:Dikin} with
an initial distribution being a single point at some distance from
boundary, even for the simplest case of uniform sampling, then an
additional factor of $n$ would be incurred. On the other hand, given
that $\nu$ and $\onu$ are typically $n^{O(1)}$, our framework only
has a logarithmic (in dimension) factor overhead for generating a
warm start. An important reason why this works is the affine-invariance
of the $\dw$. Samplers like the $\bw$ have to apply isotropic transformation
to achieve a warm start efficiently, which requires a near-linear
number of samples and thus have at least a linear in dimension overhead.

\paragraph{}

\paragraph{Derivation of sampling IPM.}

We begin with a high-level summary of the ideas of how a sampling
analogue of IPM (Algorithm~\ref{alg:IPM-sampling}) can be naturally
derived through comparison with the optimization IPM (see Section~\ref{subsec:derivation-IPM-sampling}
for details). In solving the optimization problem, $\min_{x\in K}f(x)$
for convex $K\subset\Rn$ and convex function $f:K\to\R$, the interior-point
method (IPM) first replaces $f(x)$ by a new variable $t$ and appends
the epigraph $\{(x,t)\in\R^{n+1}:f(x)\leq t\}$ to the constraint
in addition to $x\in K$. Then the combination $\phi$ of self-concordant
barriers for $K$ and the epigraph comes into play to convert this
constrained problem to a unconstrained problem defined by $\min f_{\lda}(x,t):=t+\frac{1}{\lda}\phi(x,t)$
for a parameter $\lda>0$. Then a proper optimization step (e.g.,
the Newtonian gradient descent) that takes into account a local geometry
given by $\hess\phi$ moves a current point closer to an optimal point,
while the self-concordant barrier traps the next point inside of the
original constraint. Increasing $\lda$ a bit, IPM repeats this procedure
with the updated point used as a starting point. As $\lda$ becomes
sufficiently large (until $\lda\leq\nu/\veps$ for a target accuracy
$\veps>0$), the effect of $\frac{1}{\lda}\phi(x,t)$ vanishes in
the regularized problem, and this eventually brings us to a point
sufficiently closer to the minimum.

We recall the following conceptual match between convex optimization
and logconcave sampling: for convex $K\subset\Rn$ and convex function
$f:K\to\R$
\begin{align*}
\min f(x) & \quad\longleftrightarrow\quad\text{sample }\frac{d\pi}{dx}\propto e^{-f(x)}\\
\text{s.t. }x\in K & \qquad\qquad\quad\text{s.t. }x\in K
\end{align*}
Now let us translate IPM's machinery into the sampling setting. We
first introduce a new variable $t$ by replacing $f(x)$ and then
adding the epigraph constraint in addition to $x\in K$. Just as IPM
did for the optimization, we introduce a parameter $\sigma^{2}$ (corresponding
to $\lda$ above) and attempt to sample a `regularized' distribution
whose density is proportional to $\exp\Par{-f_{\sigma^{2}}(x,t)}=\exp\Par{-\Par{t+\frac{1}{\sigma^{2}}\phi(x,t)}}$.
This step should be executed by a proper sampling step (call it $\textsf{NE-Sampler}$)
that can reflect a local geometry given by $\hess\phi$, which is
the $\dw$ in our case. Then we increase $\sigma^{2}$ slightly, and
using the previous regularized distributions as a warm start we sample
a next regularized distribution $\exp\Par{-f_{\sigma^{2}+\veps}(x,t)}$.
This procedure is repeated until $\sigma^{2}$ becomes as large as
$\nu$. 

Looking at Algorithm~\ref{alg:IPM-sampling}, we can notice that
under proper modifications this sampling analogue of IPM equipped
with a $\textsf{NE-Sampler}$ corresponds to $\gcdw$ with $f(x)$
being a linear function (i.e., exponential distribution)! Specifically
in the inner loop, we use the $\dw$ to sample exponential distributions
subject to $x\in K$ and $\{(x,t)\in\R^{n+1}:f(x)\leq t\}$, where
the local metric therein consists of the Hessians of self-concordant
barriers for $K$ and the level set of $f$. Comparing with the $\bw$
for a general logconcave distribution (\cite{lovasz2007geometry}),
incorporating the geometry of a level set of $f$ (not just $\hess f$)
is a natural approach to sampling from $e^{-f}$.

The two IPMs are different in two aspects. First, the sampling version
requires different scales of annealing in Phase 2 until $\sigma^{2}$
reaches $\nu/n$, annealing not only $\phi$ but also the density
part $f$. In this phase, closeness of regularized distributions in
consecutive iterations follows from a property of logconcavity established
by \cite{kalai2006simulated,lovasz2006simulated}. Another difference
is that the sampling version runs the $\dw$ in the last iteration
after Phase 3. If one stopped after Phase 3 until $\sigma^{2}$ reaches
(say) $\nu/\text{poly}(\veps)$ just as in the optimization version,
the total number of iterations of the $\dw$ would be $O\Par{n\max(n,\nu)/\text{poly}(\veps)}$.
This guarantee can avoid the symmetry parameter, but this comes at
the cost of low-accuracy of the sampler (i.e., polynomial dependency
on $1/\veps$). Hence, to make it high-accuracy (i.e., poly-log dependency
on $1/\veps$), we finish up the algorithm with another execution
of the $\dw$, obtaining $O\Par{n\max(n,\nu,\onu)\log\frac{1}{\veps}}$-mixing
algorithm.

% Figure environment removed


\paragraph{Self-concordance theory for combining barriers.}

From the earlier discussion, the sampling IPM allows us to focus on
the following reduced problem: Let $t_{1},\dots,t_{I}\in\R$ and $y=(x,t_{1},\dots,t_{I})\in\R^{n}\times\R^{I}=\R^{n+I}$.
We denote $E_{i}:=\Brace{(x,t_{i})\in\R^{n+1}:f_{i}(x)\leq y_{n+i}}$
for $i\in[I]$ and $K_{j}:=\Brace{x\in\Rn:h_{j}(x)\leq0}$ for $j\in[J]$,
whose convexity follows from convexity of $f_{i}$ and $h_{j}$. Denoting
the embeddings of $E_{i}$ and $K_{j}$ onto $\R^{n+I}$ by $\bar{E}_{i}$
and $\bar{K}_{j}$, we can reduce the sampling problem of (\ref{eq:problem})
to
\begin{align}
\text{sample } & \frac{d\tilde{\pi}}{dy}(y)\propto e^{-(\underbrace{0,\dots,0}_{n\text{ times}},\underbrace{1,\dots,1}_{I\text{ times}})\,\cdot\,y}\label{eq:reduced-problem}\\
\text{s.t. } & y\in K':=\bigcap_{i=1}^{I}\bar{E}_{i}\cap\bigcap_{j=1}^{J}\bar{K}_{j},\nonumber 
\end{align}
where $K'$ is closed convex and has non-empty interior, and we are
given self-concordant barriers for each $E_{i}$ and $K_{j}$. As
the $x$-marginal of $\tilde{\pi}$ is $\pi$, we just project a drawn
sample from $\tilde{\pi}$ to the $x$-space. When $f_{i}(x)$ can
be written as $n$ separable terms (i.e., $f_{i}(x)=\sum_{l=1}^{n}f_{i,l}(x_{l})$),
it is more convenient to introduce $n$ many variables $t_{i,1},\dots t_{i,n}$
for $f_{i,1}(x_{1}),\dots,f_{i,n}(x_{n})$.

Now that we have a good understanding of the $\dw$ which can be used
within the IPM framework, in Section~\ref{sec:sc-theory-rules} we
study how to combine a self-concordant metric and its parameters (for
the $\dw$) on each epigraph $E_{i}$ and convex set $K_{j}$. We
show that as in the optimization IPM, the addition of all these barriers
is actually a good candidate of a barrier for $K'$, but under an
appropriate scaling. Unlike the optimization version, the sampling
version requires not only just self-concordance parameters but also
symmetry parameters, SSC, and LTSC for final mixing time guarantees.
Notably, SSC and LTSC assume invertibility of a local matrix function,
but the Hessian of a barrier for a lower-dimensional space is degenerate
with respect to the augmented variable $y\subset\R^{n+I}$. Thus,
we carefully deal with how to maintain or update each of the main
properties such as self-concordance parameters, symmetry, SSC, and
LTSC under addition and scaling, especially when going up from lower-dimensional
spaces to the extended space. We handle these technical issues in
Definition~\ref{def:sc-along-subspace}, elaborating on how to either
restrict or embed a matrix function $g$ onto a lower dimensional
space or into a higher dimensional space. 

Using these notions, we can formally state how to put together information
of a barrier for each constraint / epigraph. The readers can note
the analogy to Nesterov and Nemirovski's IPM theory for optimization.

\begin{restatable}{thmre}{thmIPMsampling} \label{thm:IPM-sampling} 

In the reduced problem of (\ref{eq:reduced-problem}), let us assume
the following:
\begin{itemize}
\item For $i\in[I]$, the epigraph $E_{i}$ admits a PSD matrix function
$g_{i}^{e}(x,t_{i})$ (or $g_{i}^{e}(x,t_{i,1},\dots,t_{i,n})$) that
is a $(\nu_{i},\bar{\nu}_{i})$-SC barrier, SSC along some subspace,
SLTSC, and SASC.
\item For $j\in[J]$, the constraint $K_{j}$ admits a PSD matrix function
$g_{j}^{c}(x)$ that is a $(\eta_{j},\bar{\eta}_{j})$-SC barrier,
SSC along some subspace, SLTSC, and SASC.
\end{itemize}
For appropriate projections $\pi_{i}^{e}$ and $\pi^{c}$, a matrix
function $g$ on $y\in\inter(K')$ defined by
\[
\inner{u,v}_{g(y)}:=\Par{I+J}\Par{\sum_{i=1}^{I}\inner{\pi_{i}^{e}u,\pi_{i}^{e}v}_{g_{i}^{e}(\pi_{i}^{e}(y))}+\sum_{j=1}^{J}\inner{\pi^{c}u,\pi^{c}v}_{g_{j}^{c}(\pi^{c}(y))}}
\]
is $\Par{(I+J)\Par{\sum_{i=1}^{I}\nu_{i}+\sum_{j=1}^{J}\eta_{j}},\,(I+J)\Par{\sum_{i=1}^{I}\onu_{i}+\sum_{j=1}^{J}\bar{\eta}_{j}}}$-Dikin-amenable
on $K'$.

\end{restatable}
\begin{example*}
Based on our theory (Theorem~\ref{thm:Dikin-annealing} and \ref{thm:IPM-sampling})
and the study of barriers for fundamental constraints and epigraphs
(see Table~\ref{tab:scaling-table}), we can propose a natural self-concordant
metrics under which the $\dw$ mixes in polynomial time and also faster
than the $\bw$. Explicit description of these metrics can be found
in Section~\ref{sec:examples}.
\begin{itemize}
\item In our motivating example at the beginning, let us introduce a variable
for each of $\norm{X-B}_{F}$ and $\norm{X-C}_{F}^{2}$. A proper
barrier for this reduced problem is
\[
4\Par{\phi_{\textsf{log}}+n^{2}\phi_{\textsf{Gaussian}}+n^{2}\phi_{\textsf{SOC}}+n^{2}\phi_{\textsf{PSD}}},
\]
and its Hessian is $O(1)\cdot(m+n^{3},m+n^{3})$-self-concordant,
SSC, LTSC, and ASC. In applying Theorem~\ref{thm:Dikin-annealing},
we can set $\alpha=0$ and $\beta=1$ (due to the $-\log\det X$ term
in the potential), so we need $\otilde{n^{2}(m+n^{3})}$ iterations
of the $\dw$ in total.
\item \textbf{Uniform sampling}: Let us first consider uniform sampling
over linear constraints given by $Ax\geq b$ for $A\in\R^{m\times n}$
and $b\in\R^{m}$. Recall that for uniform sampling the $\bw$ mixes
in $\otilde{n^{3}}$ iterations along with isotropic rounding. Using
the logarithmic barrier $\phi_{\textsf{\ensuremath{\log}}}$, one
can obtain an $(m,m)$-Dikin amenable metric, with which the $\dw$
mixes in $\otilde{mn}$. This recovers the mixing time of \cite{kannan2012random}
\emph{without} a warmness assumption. If we replace the log-barrier
by a $(\sqrt{mn},\sqrt{mn})$-Dikin-amenable Vaidya or $(n^{3/2},n^{3/2})$-Dikin-amenable
Lewis-weight metric, then we can recover the $\otilde{m^{\half}n^{\frac{3}{2}}}$
and $\otilde{n^{\frac{5}{2}}}$ mixing of the $\textsf{Vaidya walk}$
and $\textsf{Approximate John walk}$ in \cite{chen2018fast} without
warmness. For a second-order cone with linear constraints, we can
use the barrier of $2\Par{\phi_{\textsf{log}}+n\phi_{\text{\textsf{SOC}}}}$.
Its Hessian is an $\Par{m+n,m+n}$-Dikin-amenable metric, with which
the $\dw$ mixes in $\otilde{n(m+n)}$ iterations in total. Lastly,
for a PSD cone with linear constraints, we can use the barrier of
$2\Par{\phi_{\textsf{log}}+n^{2}\phi_{\text{PSD}}}$, which provides
an $(m+n^{3},m+n^{3})$-Dikin-amenable metric. The $\dw$ with this
metric mixes in $\otilde{n^{2}\Par{m+n^{3}}}$ iterations. For large
$m$, one can use $2\Par{ng_{\textsf{Lw}}+n^{2}\hess\phi_{\textsf{PSD}}}$
that is $(n^{3},n^{3})$-Dikin-amenable, with which the $\dw$ mixes
in $\otilde{n^{5}}$ iterations. Note that the $\bw$ mixes in $\otilde{n^{6}}$
iterations in this case.
\item \textbf{Uniform sampling over hyperbolic cones}: \cite{narayanan2016randomized}
went beyond linear constraints and analyzed the $\dw$ for uniform
sampling over a convex region given as the intersection of (1) linear
constraints, (2) a hyperbolic cone with a hyperbolic barrier, and
(3) a general convex set with a SC barrier. Using a metric defined
by
\[
\hess\Par{\phi_{\log}+n\phi_{h}+n^{2}\phi_{s}},
\]
where $\phi_{\log}$ is an $m$-SC log barrier for (1), $\phi_{h}$
is a $\nu_{h}$-SC hyperbolic barrier for (2), and $\phi_{s}$ is
a $\nu_{s}$-SC barrier $\phi_{s}$ for (3), this work shows that
the $\dw$ mixes in $O\Par{n\Par{m+n\nu_{h}+(n\nu_{s})^{2}}}$ steps
from a warm start. The term $n(n\nu_{s})^{2}$ induced by self-concordance
alone is typically the largest one in the provable guarantee. Interesting
results of this work arise when $K$ is the intersection of (1) and
(2). Since a hyperbolic barrier is HSC by Theorem~4.2 in \cite{guler1997hyperbolic},
and an additional $n$-scaling of a HSC barrier makes it SSC, SLTSC,
and SASC, $n\phi_{h}$ is SSC, SLTSC, and SASC. Also, as a $\nu_{h}$-SC
hyperbolic barrier $\phi_{h}$ is $O(\nu_{h})$-symmetric (implied
in Section~4 of \cite{guler1997hyperbolic}), it follows that $n\phi_{h}$
is $(n\nu_{h},n\nu_{h})$-Dikin-amenable. As presented shortly, $\phi_{\log}$
is $m$-symmetric, SSC, SLTSC, and SASC. Hence, $\phi_{\log}+n\phi_{h}$
induces an $\Par{m+n\nu_{h},m+n\nu_{h}}$-Dikin-amenable metric, and
the $\dw$ with this metric mixes in $O\Par{n(m+n\nu_{h})}$ iterations
from a warm start by Theorem~\ref{thm:Dikin}. Without a warm start,
\cite{narayanan2016randomized} showed that the $\dw$ started at
$x\in K$, where $s\geq\frac{|p|}{|q|}$ for any chord $\overline{pq}$
of $K$ passing through $x$, mixes in $O\Par{n(m+n\nu_{h})\Par{n\log\Par{(m+n\nu_{h})s}+\log\frac{1}{\veps}}}$
steps. On the other hand, $\gcdw$ requires only $O\Par{n\Par{m+n\nu_{h}}\log\frac{n\Par{m+n\nu_{h}}}{\veps}}$
iterations by Theorem~\ref{thm:Dikin-annealing}.
\item \textbf{Gaussian sampling}: For simplicity, we only consider the logarithmic
barriers to address linear constraints. We recall that the $\bw$
for general log-concave distributions mixes in $\otilde{n^{4}}$ iterations.
According to our reduction scheme, we first replace a quadratic potential
(coming from the Gaussian distribution) by a new variable, adding
its epigraph to a constraint. In a polytope, one can use $2(\phi_{\textsf{log}}+n\phi_{\textsf{Gaussian}})$,
whose Hessian serves an $(m+n,m+n)$-Dikin-amenable metric. Thus,
we need $\otilde{n(m+n)}$ iterations of the $\dw$. For a second-order
cone with linear constraints, the $\dw$ with the Hessian of $3\Par{\phi_{\textsf{log}}+n\phi_{\textsf{SOC}}+n\phi_{\textsf{Gaussian}}}$,
which is $(m+n,m+n)$-Dikin-amenable, mixes in $\otilde{n(m+n)}$
iterations. Lastly, in a PSD cone with linear constraints, the $\dw$
with the Hessian of $3\Par{\phi_{\textsf{log}}+n^{2}\phi_{\textsf{PSD}}+n^{2}\phi_{\textsf{Gaussian}}}$,
which is $(m+n^{3},m+n^{3})$-Dikin-amenable, mixes in $\otilde{n^{2}(m+n^{3})}$
iterations. Note that the $\bw$ is much slower --- it mixes in $\otilde{n^{8}}$
iterations in this setting.
\item \textbf{Entropy sampling}: In a polytope, one can use $2(\phi_{\textsf{log}}+n\phi_{\textsf{entropy}})$,
whose Hessian serves an $(m+n^{2},m+n^{2})$-Dikin-amenable metric.
Thus, we need $\otilde{n(m+n^{2})}$ iterations of the $\dw$. For
a second-order cone with linear constraints, the $\dw$ with the Hessian
of $3\Par{\phi_{\textsf{log}}+n\phi_{\textsf{SOC}}+n\phi_{\textsf{entropy}}}$,
which is $(m+n^{2},m+n^{2})$-Dikin-amenable, mixes in $\otilde{n(m+n^{2})}$
iterations. Lastly, in a PSD cone with linear constraints, the $\dw$
with the Hessian of $3\Par{\phi_{\textsf{log}}+n^{2}\phi_{\textsf{PSD}}+n^{2}\phi_{\textsf{entropy}}}$,
which is $(m+n^{4},m+n^{4})$-Dikin-amenable, mixes in $\otilde{n^{2}(m+n^{4})}$
iterations. The $\bw$ mixes in $\otilde{n^{8}}$ iterations in this
setting also.
\end{itemize}
\end{example*}

\paragraph{Metrics for well-known structured instances.}

\begin{table}
\begin{centering}
\begin{tabular}{ccccccccc}
\toprule 
\textbf{Constraints / Epigraphs} & \textbf{Barrier} & $\nu$ & $\onu$ & SSC & LTSC & SLTSC & ASC & SASC\tabularnewline
\midrule
\midrule 
\multirow{3}{*}{$Ax\geq b$} & $\phi_{\textsf{log}}$ & $m$ & $m$ &  &  &  &  & \tabularnewline
\cmidrule{2-9} \cmidrule{3-9} \cmidrule{4-9} \cmidrule{5-9} \cmidrule{6-9} \cmidrule{7-9} \cmidrule{8-9} \cmidrule{9-9} 
 & $g_{\text{\textsf{Vaidya}}}$ & $\sqrt{mn}$ & $\sqrt{mn}$ &  &  &  &  & \tabularnewline
\cmidrule{2-9} \cmidrule{3-9} \cmidrule{4-9} \cmidrule{5-9} \cmidrule{6-9} \cmidrule{7-9} \cmidrule{8-9} \cmidrule{9-9} 
 & $g_{\textsf{Lw}}$ & $n$ & $n$ &  & $\sqrt{n}$ & $\sqrt{n}$ & $\sqrt{n}$ & $\sqrt{n}$\tabularnewline
\midrule 
$\norm{x-\mu}_{\Sigma}^{2}\leq1$ & $\phi_{\textsf{\text{ellip}}}$ &  &  & $n$ &  &  & $n$ & $n$\tabularnewline
\midrule 
$\norm{x-\mu}_{\Sigma}^{2}\leq t$ & $\phi_{\text{\textsf{Gaussian}}}$ &  &  & $n$ &  &  & $n$ & $n$\tabularnewline
\midrule 
$\norm{x-\mu}_{\Sigma}\leq t$ & $\phi_{\text{\textsf{SOC}}}$ &  &  & $n$ & $n$ & $n$ & $n$ & $n$\tabularnewline
\midrule 
$X\succeq0$ & $\phi_{\text{\textsf{PSD}}}$ & $n$ & $n$ & $n$ &  &  & $n$ & $n^{2}$\tabularnewline
\midrule 
$-x_{i}\log x_{i}\leq t_{i}$ $\forall i\in[n]$ & $\phi_{\text{\textsf{entropy}}}$ & $n$ & $n$ &  &  &  & $n$ & $n$\tabularnewline
\midrule 
$\Abs{x_{i}}^{p}\leq t_{i}$ $\forall i\in[n]$ & $\phi_{\text{\textsf{power}}}$ & $n$ & $n$ &  &  &  & $n$ & $n$\tabularnewline
\bottomrule
\end{tabular}
\par\end{centering}
\caption{\label{tab:scaling-table} Self-concordance and symmetry parameters,
and required scaling factors for a family of barriers. In this table,
we assume $A\in\protect\R^{m\times n},x\in\protect\Rn,$ and $X\in\protect\S_{+}^{n}$.
Empty entries indicate $O(1)$-scalings.}
\end{table}

In Section~\ref{sec:handbook-barrier}, we examine required parameters
and properties of a barrier for a structured constraint and potential,
such as linear, quadratic, entropy, $\ell_{p}$-norm, and PSD cone. 

\subparagraph{(1) Linear constraints.}

We start with linear constraints given by $K:=\Brace{x\in\Rn:Ax\geq b}$
for $A\in\R^{m\times n}$ and $b\in\R^{m}$, where $A$ is assumed
to have no all-zero rows. For $x\in\intk$, we denote $S_{x}:=\Diag\Par{a_{i}^{\top}x-b_{i}}\in\R^{m\times m}$
and $A_{x}:=S_{x}^{-1}A\in\R^{m\times n}$, where $a_{i}$ is the
$i^{th}$-row of $A$ for $i\in[m]$. These linear constraints admit
efficiently computable self-concordant barriers: logarithmic barrier,
Vaidya metric, and Lewis-weight metric.

The simplest one is the logarithmic barrier defined by
\[
\phi_{\textsf{log}}(x):=-\sum_{i=1}^{m}\log\Par{a_{i}^{\top}x-b_{i}}.
\]

When the number of constraints $m$ is large, one can use a self-concordant
metric due to \cite{vaidya1996new}, which was used to improve the
complexity of the interior-point method for optimization. For a full-rank
matrix $A$, the resulting Vaidya metric takes advantage of the \emph{leverage
scores} $\sigma\Par{A_{x}}$ of $A_{x}$, the diagonal entries of
the orthogonal projection matrix $P_{x}=A_{x}\Par{A_{x}^{\top}A_{x}}^{-1}A_{x}\in\R^{m\times m}$,
i.e., $\sigma(A_{x})_{i}:=\Par{P_{x}}_{ii}>0$ for $i=1,\dots,m$.
We let $\Sigma_{x}$ be the diagonal matrix with $(\Sigma_{x})_{ii}=\sigma\Par{A_{x}}_{i}>0$
for $i=1,\dots,m$. Then the Vaidya metric is defined by
\[
g_{\text{\textsf{Vaidya}}}(x):=O(1)\sqrt{\frac{m}{n}}A_{x}^{\top}\Par{\Sigma_{x}+\frac{n}{m}I_{m}}A_{x},
\]
which satisfies $g_{\textsf{\text{Vaidya}}}\asymp\hess\Par{\sqrt{\frac{m}{n}}\Par{\phi_{\vol}+\frac{n}{m}\text{\ensuremath{\phi_{\textsf{\ensuremath{\log}}}}}}}$
for $\phi_{\vol}:=\half\log\det\hess\phi_{\textsf{log}}$.

Its self-concordance parameter is still polynomial in $m$, and it
is natural to ask if the dependence on $m$ can be removed or made
poly-logarithmic. This can be achieved by a Lewis-weight metric that
makes use of the \emph{Lewis weights} of $A_{x}$. The $\ell_{p}$-Lewis
weight of $A_{x}$ is the vector $w_{x}\in\R^{m}$ satisfying the
implicit equation $w_{x}=\sigma\Par{\Diag(w_{x})^{\half-\frac{1}{p}}A_{x}}$.
Note that the leverage scores can be recovered as the $\ell_{2}$-Lewis
weight of $A_{x}$. Then the Lewis-weight metric is defined by
\[
g_{\text{\textsf{Lw}}}(x):=O(\log^{O(1)}m)A_{x}^{\top}W_{x}A_{x},
\]
which serves as an $O\Par{\log^{O(1)}m}$-approximation of the Hessian
of $\phi_{\lw}(x):=\log\det\Par{A_{x}^{\top}W_{x}^{1-\frac{2}{p}}A_{x}}$.
By setting $p=O(\log m)$, the self-concordance parameter of this
barrier and metric can be made $O^{*}(n)$.

For the sampling purpose, we should look into other properties such
as symmetry, SSC, SLTSC, and SASC, going beyond just self-concordance
parameter. We note that the log-barrier and Vaidya metric fulfill
these properties without additional scaling, while the Lewis-weight
metric requires a $\sqrt{n}$-scaling for SLTSC and SASC. We summarize
these results below.
\begin{thm*}
[Linear constraints] Let $K:=\{x\in\Rn:Ax\geq b\}$ for $A\in\R^{m\times n}$
and $b\in\R^{m}$. Let $x\in\intk$ and $h\in\Rn$. Let $\Sigma_{x}$
be the diagonalized leverage scores and $W_{x}$ be the diagonalized
$\ell_{p}$-Lewis weights with $p=O(\log m)$.
\begin{itemize}
\item Log-barrier $\phi_{\textsf{log}}(x)=-\sum_{i=1}^{m}\log(a_{i}\cdot x-b_{i})$:
$g=\hess\phi_{\textsf{log}}$ satisfies $\nu,\onu\leq m$, SSC along
$\rowspace(A)$, and $D^{2}g(x)[h,h]\succeq0$ (so SLTSC), and SASC.
\item Vaidya metric $g_{\text{\textsf{Vaidya}}}(x)=\sqrt{\frac{m}{n}}A_{x}^{\top}\Par{\Sigma_{x}+\frac{n}{m}I_{m}}A_{x}$
and $\phi_{\text{\textsf{Vaidya}}}=\sqrt{\frac{m}{n}}\Par{\half\log\det\hess\phi_{\textsf{\ensuremath{\log}}}+\frac{n}{m}\phi_{\textsf{\ensuremath{\log}}}}$
(with $m\geq n)$: $g=44g_{\textsf{Vaidya}}$ $\nu,\onu=O(\sqrt{mn})$,
SSC, SLTSC, and SASC.
\item Lewis-weight metric $g_{\textsf{Lw}}(x)=O\Par{\log^{O(1)}m}A_{x}^{\top}W_{x}A_{x}$
and $\phi_{\textsf{Lw}}=\log\det A_{x}^{\top}W_{x}^{1-\frac{2}{O(\log m)}}A_{x}$:
$g=\sqrt{n}g_{\textsf{Lw}}$ satisfies $\nu,\onu=O(n^{3/2}\log^{O(1)}m)$,
SSC, SLTSC, and SASC.
\end{itemize}
\end{thm*}

\subparagraph{(2) Quadratic potentials and constraints.}

Now we move onto quadratic potential (i.e., Gaussian) and constraints
(i.e., ellipsoid and second-order cone). Each of a self-concordant
barrier introduced by \cite{nesterov1994interior} serves as an efficient
barrier for each constraint or epigraph of a potential. We can show
that all barriers are highly self-concordant, so the scaling of $n$
makes it satisfy SLTSC and SASC.
\begin{thm*}
[Quadratic] Let $K_{1}=\Brace{x\in\Rn:\half x^{\top}Qx+p^{\top}x+l\leq0}$
with $p\in\Rn$ and $0\neq Q\in\psd$. Let $K_{2}=\Brace{(x,t)\in\R^{n+1}:\half\norm{x-\mu}_{\Sigma}^{2}\leq t}$
and $K_{3}=\Brace{(x,t)\in\R^{n+1}:\norm{x-\mu}_{\Sigma}\leq t}$
with $\mu\in\Rn$ and $\Sigma\in\pd$. Let $x\in\inter(K_{i})$ and
$h\in\R^{n}$ or $\in\R^{n+1}$.
\begin{itemize}
\item Ellipsoid $\phi_{\textsf{\text{ellip}}}(x)=-\log\Par{-l-p^{\top}x-\half x^{\top}Qx}$
for $K_{1}$: $g=n\hess\phi_{\textsf{ellip}}$ satisfies $\nu,\onu=O(n)$,
SSC when $Q\in\pd$, $D^{2}g(x)[h,h]\succeq0$ (so SLTSC), and SASC.
\item Gaussian $\phi_{\text{\textsf{Gaussian}}}(x)=-\log\Par{t-\half\norm{x-\mu}_{\Sigma}^{2}}$
for $K_{2}$: $g=n\hess\phi_{\text{\textsf{Gaussian}}}$ satisfies
$\nu,\onu=O(n)$, SSC, and $D^{2}g(x,t)[h,h]\succeq0$ (so SLTSC),
and SASC.
\item Second-order cone $\phi_{\text{\textsf{SOC}}}(x)=-\log\Par{t^{2}-\norm{x-\mu}_{\Sigma}^{2}}$
for $K_{3}$: $g=n\hess\phi_{\text{\textsf{SOC}}}$ satisfies $\nu,\onu=O(n)$,
SSC, SLTSC, and SASC.
\end{itemize}
\end{thm*}

\subparagraph{(3) PSD cone.}

Another fundamental constraint is the PSD constraint represented by
$X\succeq0$. This convex region admits an $n$-self-concordant barrier
$\phi_{\textsf{\text{PSD}}}(X)=-\log\det X$. We show that it satisfies
SLTSC, while the $n$-scaling further guarantees SSC and ASC. In establishing
ASC, we find an interesting connection to the \emph{Gaussian orthogonal
ensemble} (GOE), one of the main objects studied in random matrix
theory. However, we cannot prove SASC, so we need the scaling of $n(n+1)/2$
for SASC (due to HSC of $\phi_{\textsf{\text{PSD}}}$).
\begin{thm*}
[PSD cone] Let $K=\psd$, $X\in\intk$, and $h\in\S^{n}$. For $\phi_{\textsf{\text{PSD}}}(X)=-\log\det X$,
$g:=n\hess\phi_{\textsf{PSD}}$ on $K$ satisfies $\nu,\onu=O(n^{2})$,
SSC, $D^{2}g(X)[H,H]\succeq0$ (so SLTSC), and ASC. $\frac{n(n+1)}{2}\hess\phi_{\textsf{\text{PSD}}}$
is SASC.
\end{thm*}

\subparagraph{(4) Entropy and $\ell_{p}$-norm.}

It is sometime more convenient to introduce $n$ many new variables
as seen in the following:
\begin{thm*}
[Entropy and $\ell_p$-norm] Let $K_{1}=\prod_{i=1}^{n}\Brace{(x_{i},t_{i})\in\R^{2}:x_{i}\geq0,\,t_{i}\geq x_{i}\log x_{i}}$
and $K_{2}=\prod_{i=1}^{n}\Brace{(x_{i},t_{i})\in\R^{2}:\Abs{x_{i}}^{p}\leq t_{i}}$.
\begin{itemize}
\item Entropy $\phi_{\textsf{entropy}}(x,t)=-\sum_{i=1}^{n}\Par{\log(t_{i}-x_{i}\log x_{i})+36\log x_{i}}$
for $K_{1}$: $g=n\hess\phi$ satisfies $\nu,\onu=O(n^{2})$, SSC,
SLTSC, and SASC.
\item $p^{th}$-power of $\ell_{p}$-norm $\phi_{\textsf{power}}(x,t)=-\sum_{i=1}^{n}\Par{\log(t_{i}^{2/p}-x_{i}^{2})+72\log t_{i}}$
for $K_{2}$: $g=n\hess\phi$: $\nu,\onu=O(n^{2})$, SSC, SLTSC, and
SASC.
\end{itemize}
\end{thm*}

\subsection{Background and related work\label{subsec:related-work}}

Uniformly sampling convex bodies is a special case of \emph{logconcave
sampling}: sample from a distribution $\pi$ with density proportional
to $e^{-V(x)}$ for a convex function $V$ on $\Rn$. This problem
has spawned a long line of research in several communities, as it
captures various important distributions, including uniform distributions
over convex bodies and Gaussians.

A large body of recent work in machine learning and statistics makes
the assumption of $0\prec\alpha I\preceq\hess V(x)\preceq\beta I$
for $x\in\Rn$ (i.e., $\alpha$-strong convexity and $\beta$-smoothness
of the potential $V$), where the strong-convexity assumption is sometimes
relaxed to isoperimetry assumptions such as log-Sobolev inequalities
(LSI), Poincar inequality (PI), and Cheeger isoperimetry.  The guarantees
provided on the mixing time of samplers under this assumption have
polynomial dependence on the condition number defined as $\beta/\alpha$
(or $\alpha$ is replaced by the isoperimetric constant). These guarantees
do not apply to constrained sampling. For example, in uniform sampling,
the simplest constrained sampling problem, $V$ is set to be a constant
within the convex body and infinity outside the body, which leads
to discontinuity of $V$ and $\beta=\infty$. The sudden change of
$V$ around the boundary requires special consideration, such as small
step size, use of a Metropolis filter, projection, etc., making it
a more challenging problem.

\paragraph{Uniform sampling.}

Uniform sampling can be done by the $\bw$ (\cite{lovasz1993random,kannan1997random})
and \textsf{Hit-and-Run} (\cite{smith1984efficient}), which require
access to a function proportional to the density. When a convex body
$K\subset\Rn$ satisfies $B_{r}(x_{0})\subset K\subset B_{R}(x_{0})$
for some $x_{0}$, the $\bw$ mixes in $\otilde{n^{2}\Par{R/r}^{2}}$
steps from warm start (\cite{kannan1997random}) and $\textsf{Hit-and-Run}$
mixes in $\otilde{n^{2}\Par{R/r}^{2}}$ steps from any start\footnote{In this section, \emph{warm start} means polynomial dependence on
the warmness parameter $M$, while \emph{any start} means poly-logarithmic
dependency on $M$. We assume any start unless specified otherwise.} (\cite{lovasz1999hit,lovasz2006hit}). \cite{lovasz2007geometry}
further extended these results to general logconcave distributions.
These algorithms need to use a ``step size'' of $\Omega(1/\sqrt{n})$,
and their mixing is affected by the skewed geometry of the convex
body (i.e., when $R/r\gg1$). The latter can be addressed by first
\emph{rounding} the body, after which the $\bw$ and the $\textsf{Hit-and-Run}$
mix in $\otilde{n^{2}}$ steps from a warm start, due to bounds on
the KLS constant by \cite{chen2021almost,klartag2023logarithmic}
and stochastic localization by \cite{chen2022hit}. The fastest rounding
algorithm by \cite{jia2021reducing} requires $\otilde{n^{3}}$ queries
to a membership oracle, and uses the $\bw$.


\paragraph{Sampling with local geometry.}

The $\bw$ uses the same radius ball for every point in the convex
body. One might want to use a different radius depending on the distance
to the boundary. This by itself does not work as it simply makes the
current point converge to the boundary. However, replacing balls with
ellipsoids whose shape changes based on the proximity to the boundary
does work. Several sampling algorithms are motivated by the use of
local metrics: the $\dw$ (\cite{kannan2012random}), \textsf{Riemannian Hamiltonian Monte Carlo}
(RHMC), \textsf{Riemannian Langevin algorithm} (\cite{girolami2011riemann}),
etc.

Which local metrics would be suitable candidates? It turns out that
a suitable metric can be derived from self-concordant barriers, a
concept dating back to the development of the interior-point method
in convex-optimization literature (\cite{nesterov1994interior}).
It is well-known that any convex body admits an $n$-self-concordant
barrier such as universal barrier (\cite{nesterov1994interior,lee2021universal})
and entropic barrier (\cite{bubeck2014entropic,chewi2021entropic}),
but these are computationally expensive. Moreover, as noted in \cite{laddha2020strong},
the symmetry parameter of these general barriers is $\Omega(n^{2})$
for $n$-dimensional bodies (even for second-order cones), and so
the resulting complexity for the $\dw$ on a PSD cone is $\Omega(n^{2}\cdot n^{4})=\Omega(n^{6})$.
Thus, there is a need to find barriers that are more closely aligned
with the structure of sets we wish to sample. 

\paragraph{Polytope sampling.}

Samplers such as the $\bw$ and \textsf{Hit-and-Run} can be used
to sample polytopes, but they do not really use any special properties
of polytopes. 

For polytopes with $m$ linear constraints in $n$-dimension ($m>n$),
the first theoretical result via self-concordant barriers dates back
to \cite{kannan2012random} which proposed the $\dw$ with the $m$-self-concordant
logarithmic barrier and established the mixing rate of $\otilde{mn}$
for uniform sampling. \cite{chen2018fast} revisited the idea of \cite{vaidya1996new}
using the $O(\sqrt{mn})$-self-concordant hybrid barrier, which is
a hybrid of the volumetric barrier and the log barrier and leads to
a faster interior-point method. They presented the $\dw$ with the
hybrid barrier giving an $\otilde{\sqrt{m}n^{3/2}}$-mixing guarantee.
Lastly, \cite{laddha2020strong} proposed the $\dw$ with a variant
of the $O^{*}(n)$-self-concordant LS barrier based on Lewis weights,
developed by \cite{lee2019solving}, and showed a mixing rate of $\otilde{n^{2}}$.

While the next point proposed by all these Markov chains is obtained
by a Euclidean straight line step, the \textsf{Geodesic walk} and
RHMC use curves (geodesics and Hamiltonian-preserving curves respectively).
\cite{lee2017geodesic} and \cite{lee2018convergence} showed that
for uniform sampling, the \textsf{Geodesic walk} and RHMC with the
log barrier mix in $\otilde{mn^{\frac{3}{4}}}$ and $\otilde{mn^{\frac{2}{3}}}$
steps respectively. \cite{kook2022condition} extended theoretical
analysis of RHMC to truncated exponential distributions and showed
that discretization of Hamilton's equations by practical numerical
integrators maintains a fast mixing rate. \cite{gatmiry2023sampling}
showed that just as the $\dw$ enjoys faster mixing via a barrier
with a better self-concordance parameter, RHMC with a hybrid barrier
consisting of the Lewis weights and log barrier mixes in $\otilde{m^{\frac{1}{3}}n^{\frac{4}{3}}}$
steps. Their proof is based on developing suitable properties and
algorithmic bounds for Riemannian manifolds.

\paragraph{Generalization of the approach.}

Extending these non-Euclidean methods to more general cones (e.g.,
the PSD cone) and to more general densities (e.g., Gaussian, entropy,
$\alpha$-relatively strong convex and $\beta$-relatively smooth)
to potentially improve the complexity of the problem significantly
beyond the bounds that follow from general convex body sampling, have
been open research directions and motivate our paper. 

\cite{narayanan2016randomized} explored the first direction, analyzing
the $\dw$ for uniform sampling over the intersection of linear constraints,
a hyperbolic cone with a hyperbolic barrier, and a general convex
set with a SC barrier. Our current understanding of the second direction
is rather limited. A line of work has focused on the analysis of first-order
non-Euclidean samplers, such as discretized \textsf{Mirror Langevin algorithm}
(MLA) or \textsf{Riemannian Langevin algorithm} (RLA) but under strong
assumptions. For example, \cite{li2022mirror} provided mixing-rate
guarantees of MLA under the \emph{modified self-concordance} of $\phi$
in the setting $\alpha\hess\phi\preceq\hess f\preceq\beta\hess\phi$.
However, the modified SC is not affine-invariant, so it does not correctly
capture affine-invariance of the algorithm. \cite{ahn2021efficient,gatmiry2022convergence}
avoid the modified self-concordance, analyzing MLA under an alternative
discretization scheme that requires an exact simulation of the Brownian
motion $\hess\phi(x_{t})^{-1/2}dW_{t}$ which is not known to be achievable
algorithmically. 

Our study of the $\dw$ for general cones and general densities provides
a rather complete picture of zeroth-order non-Euclidean samplers.
It also provides a general framework and improved bounds as well as
a ``handbook'' for structured sampling.

\subsection{Preliminaries and notation}

\paragraph{Basics.}

For $n\in\mathbb{N}$, let $[n]:=\{1,\cdots,n\}$. We use $f\lesssim g$
to denote $f\leq cg$ for some universal constant $c>0$ and $f\asymp g$
to indicate $f\lesssim g$ and $g\lesssim f$. The $\widetilde{O}$
complexity notation suppresses poly-logarithmic factors and dependence
on error parameters. For $v\in\Rn$, the Euclidean norm (or $\ell_{2}$-norm)
is denoted by $\norm v_{2}\defeq\sqrt{\sum_{i\in[n]}v_{i}^{2}}$,
and the infinity norm is denoted by $\norm v_{\infty}\defeq\max_{i\in[n]}\Abs{v_{i}}$.
A Gaussian distribution with mean $\mu\in\Rn$ and covariance $\Sigma\in\Rnn$
is denoted by $\ncal(\mu,\Sigma)$.

\paragraph{Matrices.}

We use $\S^{n}$ to denote the set of symmetric matrices of size $n\times n$.
For $X\in\S^{n}$, we call it \emph{positive semidefinite} (PSD) (resp.
\emph{positive definite} (PD)) if $h^{\top}Xh\geq0$ ($>0)$ for any
$h\in\R^{n}$. We use $\psd$ to denote the set of positive definite
matrices of size $n\times n$. Note that their effective dimension
is $d:=n(n+1)/2$ due to symmetry. For a positive (semi) definite
matrix $X$, its \emph{square root} is denoted as $X^{\half}$, and
is the unique positive (semi) definite matrix satisfying $X^{\half}X^{\half}=X$.
For $A,B\in\S^{n}$, we use $A\preceq B$ ($A\prec B$) to indicate
that $B-A$ is PSD (PD). For a matrix $A\in\R^{n\times n}$, its \emph{trace}
is denoted by $\tr\Par A=\sum_{i=1}^{n}A_{ii}$. The \emph{operator
norm} and \emph{Frobenius norm} are denoted by $\norm A_{2}\defeq\sup_{x\in\Rn}\norm{Ax}_{2}/\norm x_{2}$
and $\norm A_{F}\defeq\Par{\sum_{i,j=1}^{n}A_{ij}^{2}}^{\half}=\sqrt{\tr\Par{A^{\top}A}}$,
respectively.

\paragraph{Basic operations.}

For $X\in\S^{n}$, its \emph{vectorization} $\vec{(}X)\in\R^{n^{2}}$
is obtained by stacking each column of $X$ vertically. Its symmetric
vectorization $\svec(X)\in\R^{d}$ is obtained by stacking the lower
triangular part in vertical direction. For a matrix $A\in\R^{n\times n}$
and vector $x\in\Rn$, we use $\diag(A)$ to denote the vector in
$\Rn$ with $(\diag(A))_{i}=A_{ii}$ for $i\in[n]$, $\Diag(A)$ to
denote the diagonal matrix with $(\Diag(A))_{ii}=A_{ii}$ for $i\in[n]$
and $\Diag(x)$ to denote the diagonal matrix in $\R^{n\times n}$
with $(\Diag(x))_{ii}=x_{i}$ for $i\in[n]$.

\paragraph{Matrix operations.}

For matrices $A,B\in\R^{n\times n}$, their inner product is defined
as the inner product of $\vec{(}A)$ and $\vec{(}B)$, denoted by
$\inner{A,B}=\tr\Par{A^{\top}B}$. Their \emph{Kronecker product}
$A\kro B$ is the matrix of size $n^{2}\times n^{2}$ defined by 
\[
A\otimes B=\left[\begin{array}{ccc}
A_{11}B & \cdots & A_{1n}B\\
\vdots &  & \vdots\\
A_{n1}B & \cdots & A_{nn}B
\end{array}\right],
\]
where $A_{ij}B$ is the block matrix of size $n\times n$ obtained
by multiplying each entry of $B$ by the scalar $A_{ij}$. Their \emph{Hadamard
product} $A\circ B$ is the matrix of size $n\times n$ defined by
$(A\hada B)_{ij}=A_{ij}B_{ij}$ (i.e., obtained by element-wise multiplication).

\paragraph{Projection matrix, Leverage score and Lewis weights.}

For a full-rank matrix $A\in\R^{m\times n}$ with $m\geq n$, we recall
that $P(A):=A(A^{\top}A)^{-1}A^{\top}$ is the orthogonal projection
matrix onto the column space of $A$. The leverage scores of $A$
is denoted by $\sigma(A):=\diag(P(A))\in\R^{m}$. We let $\Sigma(A):=\Diag(\sigma(A))=\Diag(P(A))$
and $P^{(2)}(A):=P(A)\circ P(A)$. The $\ell_{p}$-Lewis weights of
$A$ is denoted by $w(A)$, the solution $w$ to the equation $w(A)=\diag\Par{W^{\half-\frac{1}{p}}A\Par{A^{\top}W^{1-\frac{2}{p}}A}^{-1}A^{\top}W^{\half-\frac{1}{p}}}\in\R^{m}$
for $W:=\Diag(w)$. When $m<n$ or $A$ is not full rank, both leverage
scores and Lewis weights can be generalized via the Moore-Penrose
inverse in place of the inverse in the definitions.

\paragraph{Derivatives.}

For a function $f:\R^{n}\to\R$, let $\grad f(x)\in\R^{n}$ denote
the gradient of $f$ at $x$ (i.e., $\Par{\grad f(x)}_{i}:=\frac{\del f}{\del x_{i}}(x)$)
and $\hess f(x)\in\R^{n\times n}$ denote the Hessian of $f$ at $x$
(i.e., $\Par{\hess f(x)}_{ij}:=\frac{\del^{2}f}{\del x_{i}\del x_{j}}(x)$).
For a matrix function $g:\R^{n}\to\R^{n\times n}$ in $x$, we use
$Dg$ and $D^{2}g$ to denote the third-order and fourth-order tensor
defined by $(Dg(x))_{ijk}=\frac{\del\Par{g(x)}_{ij}}{\del x_{k}}$
and $(D^{2}g(x))_{ijkl}=\frac{\del^{2}\Par{g(x)}_{ij}}{\del x_{k}\del x_{l}}$.
We use the following shorthand notation: $g_{x,h}':=Dg(x)[h]$ and
$g_{x,h}'':=D^{2}g(x)[h,h]$. We let $D^{i}g(x)[h_{1},\cdots,h_{i}]$
denote the $i^{\text{th}}$ directional derivative of $g$ at $x$
in directions $h_{1},\cdots,h_{i}\in\Rn$, i.e.,
\[
D^{i}g(x)[h_{1},\cdots,h_{i}]=\frac{d^{i}}{dt_{1}\cdots dt_{i}}g\Par{x+\sum_{j=1}^{i}t_{j}h_{j}}\bigg|_{t_{1},\cdots,t_{i}=0}.
\]


\paragraph*{Local norm.}

At each point $x$ in a set $K\subset\Rn$, a \emph{local metric}
$g$, denoted as $g_{x}$ or $g(x)$, is a positive-definite inner
product $g_{x}:\R^{n}\times\R^{n}\to\R$, which naturally induces
the local norm as $\norm v_{g(x)}:=\sqrt{g_{x}(v,v)}$. We use $\norm v_{x}$
to refer to $\norm v_{g(x)}$ when the context is clear. When an ambient
space has an orthogonal basis as in our setting (e.g., $\{e_{1},\dots,e_{n}\}$),
the local metric $g_{x}$ can be represented as a positive-definite
matrix of size $n\times n$. With this perspective, the inner product
can be written as $g_{x}(v,w)=v^{\top}g(x)w$. Going forward, we use
$g_{x}=g(x)$ to denote a local metric (or positive definite matrix
of size $\dim(x)\times\dim(x)$) at each point $x\in K$. The local
metric $g$ is assumed to be at least twice differentiable.

\paragraph{Markov chains.}

Many sampling algorithms are based on \emph{Markov chains}. A \emph{transition
kernel} $P:\Rn\times\bcal(\Rn)\to\R_{\geq0}$ (or \emph{one-step distribution})
for the Borel $\sigma$-algebra $\bcal(\Rn)$ quantifies the probability
of the Markov chains transitioning from one point to another measurable
set. The next-step distribution is defined by $P_{x}(A):=P(x,A)$,
which is the probability of a step from $x$ landing in the set $A$.
The transition kernel characterizes the Markov chain in the sense
that if a current distribution is $\mu$, then the distribution after
$n$ steps can be expressed as $\mu P^{(n)}$, where $\mu P^{(i)}:=\int_{\Rn}P(x,\cdot)\cdot(\mu P^{(i-1)})(dx)$
is defined recursively for $i=1,...,n$ with the convention $\mu P^{(0)}(x)=\mu(x)$.
We call $\pi$ a \emph{stationary distribution} of the Markov chain
if $\pi=\pi P$. If the stationary distribution further satisfies
$\int_{A}P(x,B)\pi(dx)=\int_{B}P(x,A)\pi(dx)$ for any two measurable
subsets $A,B$, then the Markov chain is said to be \emph{reversible}
with respect to $\pi$.

It is expected that the Markov chain approaches the stationary distribution.
We measure this with the \emph{total variation distance} (TV-distance):
for two distributions $\mu$ and $\pi$ on $\Rn$, the TV-distance
is defined as $\dtv(\mu,\pi)\defeq\sup_{A\in\bcal(\Rn)}\Abs{\mu(A)-\pi(A)}=\half\int_{\Rn}\Abs{\frac{d\mu}{dx}-\frac{d\pi}{dx}}dx$,
where the last equality holds when the two distributions admit densities
with respect to the Lebesgue measure on $\Rn$. We also recall other
probabilistic distances: when $\mu\ll\nu$,
\begin{align*}
\text{The chi-squared divergence\ } & \chi^{2}\Par{\mu\|\nu}\defeq\int\Par{\frac{d\mu}{d\nu}-1}^{2}d\nu,\\
L^{2}\text{-distance\ } & \norm{\mu/\nu}\defeq\int\frac{d\mu}{d\nu}d\mu=\chi^{2}\Par{\mu\|\nu}+1.
\end{align*}
Moreover, the rate of convergence can be quantified by the \emph{mixing
time}: for an error parameter $\veps\in(0,1)$ and an initial distribution
$\pi_{0}$, the mixing time is defined as the smallest $n\in\N$ such
that $\dtv(\pi_{0}P^{(n)},\pi)\leq\veps$. In this paper, we consider
a \emph{lazy} Markov chain, which does not move with probability $1/2$
at each step, in order to avoid a uniqueness issue of a stationary
distribution. Note that this change worsens the mixing time by at
most a factor of $2$. One of the standard tools to control progress
made by each iterate is the \emph{conductance} $\Phi$ of the Markov
chain with its stationary distribution $\pi$, defined by 
\[
\Phi\defeq\inf_{\text{measurable }S}\frac{\int_{S}P(x,S^{c})\pi(dx)}{\min\Par{\pi(S),\pi(S^{c})}}.
\]
Another crucial factor affecting the convergence rate is geometry
of the stationary distribution $\pi$, as measured by \emph{Cheeger
isoperimetry} 
\[
\psi_{\pi}\defeq\inf_{\text{measurable }S}\frac{\lim_{\delta\to0^{+}}\frac{1}{\delta}\pi\Par{\{x:\,0<d(S,x)\leq\delta\}}}{\min(\pi(S),\pi(S^{c}))},
\]
 where $d(S,x)$ is some distance between $x$ and the set $S$.
