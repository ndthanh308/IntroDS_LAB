
\appendix

\section{Algebraic identities}

Here we collect useful algebraic identities related to trace, vectorization,
Kronecker product and Hadamard product. 
\global\long\def\vec{\textup{\textsf{vec}}}%
 
\begin{lem}
[Kronecker product] \label{lem:Kronecker} For $A,B,C,D\in\R^{n\times n}$
and $M$ in Definition~\ref{def:linearOperators},
\begin{itemize}
\item $(A\otimes B)\vec(C)=\tr\Par{BCA^{\top}}$.
\item $\vec(A)^{\top}\Par{B\otimes C}\vec(D)=\tr\Par{DB^{\top}A^{\top}C}$.
\item $(A\otimes B)(C\otimes D)=AC\otimes BD$.
\item $(A\otimes B)^{-1}=A^{-1}\otimes B^{-1}$.
\item $(A\otimes B)^{\top}=A^{\top}\otimes B^{\top}$.
\item $\tr\Par{A\otimes B}=\tr(A)\tr(B)$.
\item $\det(M^{\top}(A\otimes A)M)=2^{n(n-1)/2}\Par{\det A}^{n+1}$.
\end{itemize}
\end{lem}

\begin{lem}
[Hadamard product] \label{lem:Hadamard} Let $A,B,C,D\in\R^{n\times n}$,
$x,y\in\Rn$, and $D_{1},D_{2}\in\Rnn$ be diagonal matrices.
\begin{itemize}
\item $(A\circ B)y=\diag(A\Diag(y)B^{\top})$.
\item $x^{\top}(A\circ B)y=\tr\Par{\Diag(x)A\Diag(y)B^{\top}}$.
\item $D_{1}(A\hada B)=(D_{1}A)\hada B=A\hada(D_{1}B)$.
\item $(A\hada B)D_{2}=(AD_{2})\hada B=A\hada(BD_{2})$.
\item $(A\otimes B)\circ(C\otimes D)=(A\circ C)\otimes(B\circ D)$.
\end{itemize}
\end{lem}


\section{Matrix calculus \label{app:matrixCalculus}}

Let $g(x):\Rn\to\R^{n\times n}$ be a matrix function. Its gradient
at $x$, denoted by $Dg(x)$, is the third-order tensor defined by
$(Dg(x))_{ijk}=\frac{\del g_{ij}(x)}{\del x_{k}}$. Unless specified
otherwise, the multiplication between higher-order tensors and a matrix
of size $n\times n$ is running over $i,j$-entries. For instance,
for a matrix $M\in\R^{n\times n}$ the product $Dg(x)M$ is the third-order
tensor defined by
\[
(Dg(x)M)_{\cdot,\cdot,k}=(Dg(x))_{\cdot,\cdot,k}M\text{ for each }k\in[n].
\]
In the same way, the trace of higher-order tensors is applied to a
matrix spanned by $i,j$-entries, i.e.,
\[
\Par{\tr\Par{Dg(x)}}_{k}=\tr\Par{\Par{Dg(x)}_{\cdot,\cdot,k}}.
\]

Now define $\vphi(x)=\log\det g(x):\Rn\to\R$. Its gradient is
\begin{equation}
\grad\vphi(x)=D\log\det g(x)=\tr\Par{g(x)^{-1}Dg(x)},\label{eq:gradLogDet}
\end{equation}
and this indicates that its directional derivative in $h\in\Rn$ is
$\grad\vphi(x)\cdot h=\tr\Par{g(x)^{-1}Dg(x)[h]}$. To compute the
Hessian of $\vphi$, we note that
\begin{equation}
D(g^{-1})(x)=-g(x)^{-1}Dg(x)g(x)^{-1}.\label{eq:diffInverse}
\end{equation}
Using this and the product rule, we have
\begin{align}
\hess\vphi(x) & =D\tr\Par{g(x)^{-1}Dg(x)}\nonumber \\
 & =-\tr\Par{g(x)^{-1}Dg(x)g(x)^{-1}Dg(x)}+\tr\Par{g(x)^{-1}D^{2}g(x)}\nonumber \\
 & =\tr\Par{g(x)^{-1}D^{2}g(x)}-\norm{g(x)^{-\half}Dg(x)g(x)^{-\half}}_{F}^{2},\label{eq:hessLogDet}
\end{align}
where $D^{2}g(x)$ is the fourth-order tensor defined by $(D^{2}g(x))_{ijkl}=\frac{\del g(x)_{ij}}{\del x_{k}\del x_{l}}$.

We now prove Proposition~\ref{prop:metricFormula}, providing formulas
of the Hessian and its inverse of $\phi(X)=-\log\det X$ for a matrix
$X\in\psd$.
\begin{proof}
[Proof of Proposition \ref{prop:metricFormula}] By setting $g(X)=X$
and $\phi(X)=-\vphi(X)$ above, (\ref{eq:hessLogDet}) implies that
for a symmetric matrix $H\in\S^{n}$
\begin{align}
\hess\phi(X)[H,H] & =\tr\Par{X^{-1}HX^{-1}H}\label{eq:2ndDiffLogDet}\\
 & =\vec{(}H)^{\top}\Par{X^{-1}\otimes X^{-1}}\vec{(}H)=\vec{(}H)^{\top}\Par{X\otimes X}^{-1}\vec{(}H)\nonumber 
\end{align}
where the last line follows from Lemma~\ref{lem:Kronecker}. When
representing $X$ and $H$ in $\R^{d}$ space with notations $x:=\svec(X)$
and $h:=\svec(H)$, the definition of $M$ (see Definition~\ref{def:linearOperators})
turns the formula above into
\[
\hess\phi(x)[h,h]=h^{\top}M^{\top}(X\otimes X)^{-1}Mh,
\]
and thus $g_{X}:=\nabla_{x}^{2}\phi(x)=\nabla_{X}^{2}\phi(X)$ is
equal to $M^{\top}(X\otimes X)^{-1}M$. The formula of the inverse,
$g_{X}^{-1}=M^{\dagger}(X\otimes X)M^{\dagger\top}$, is immediate
from \cite{magnus1980elimination}, and another part follows from
$M^{\dagger}=LN$ and $N^{\top}=N$ (Lemma 3.6 and Lemma 2.1 in \cite{magnus1980elimination}).
\end{proof}

