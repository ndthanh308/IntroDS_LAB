\global\long\def\vec{\textup{\textsf{vec}}}%
\global\long\def\svec{\textup{\textsf{svec}}}%


\section{Examples \label{sec:examples}}

For given constraints and epigraphs, combining metrics for them (according
to the self-concordance theory for sampling developed in Section~\ref{sec:sc-theory-rules})
and employing $\gcdw$ with the combined metric lead to a poly-time
mixing sampling algorithm. Compared to the state-of-the-art poly-time
mixing algorithm, the $\bw$, $\gcdw$ offers several advantages.
First, it does not require any preprocessing (e.g., rounding) due
to affine invariance. Also, it achieves faster mixing by leveraging
inherent geometric information in sampling problems.

The per-step complexity of $\dws$, however, is in general higher
than that of the $\bw$. The primary computational bottleneck lies
in computing the inverse of a local metric. Nevertheless, efficient
implementation of inverse maintenance can significantly reduce the
per-step complexity, improving the total complexity (the number of
iterations needed for mixing times the per-step complexity).

In this section, we illustrate how our framework recovers theoretical
guarantees of previous work on $\dws$ for uniform sampling and extends
beyond uniform sampling. In particular, we show that $\gcdw$ is a
poly-time mixing algorithm capable of sampling uniform, exponential,
or Gaussian distributions on second-order cones or truncated PSD cones.
Additionally, we illustrate an efficient per-step implementation that
yields a faster total complexity when compared to general-purpose
samplers such as the $\bw$.

\subsection{Polytope sampling}

Consider a set of linear constraints given by $K=\Brace{x\in\Rn:Ax\geq b}$
with $A\in\R^{m\times n}$ and $b\in\R^{m}$. 

\paragraph{Uniform sampling.}

\cite{kannan2012random} first studied the $\dw$ for uniformly sampling
a polytope, where a local metric is set to be the Hessian of the logarithmic
barrier, $g=\hess\phi_{\textsf{log}}=A_{x}^{\top}A_{x}.$ They showed
that the $\dw$ with the log-barrier mixes in $O\Par{mn\log\frac{M}{\veps}}$
iterations with a warmness parameter $M$. An immediate consequence
of our work is that $\gcdw$ achieves the mixing time of $\otilde{mn}$
\emph{without a warmness assumption}, as $\onu,\nu=m$ and $g$ is
SSC, LTSC, and ASC by Lemma~\ref{lem:log-barrier}.

\cite{chen2018fast} introduced the $\textsf{Vaidya walk}$ and the
$\textsf{Approximate John walk}$, which are essentially $\dws$ with
the Vaidya metric $\hess\phi_{\textsf{Vaidya}}$ and a version of
the Lewis-weight metric $\sqrt{n}\hess\phi_{\textsf{Lw}}$. Their
work showed that both walks achieves mixing times of $O\Par{\sqrt{m}n^{3/2}\log\frac{M}{\veps}}$
and $O\Par{n^{5/2}\log^{O(1)}m\log\frac{M}{\veps}}$, respectively.
Building upon our analysis of the Vaidya metric and Lewis-weight metric
in Lemma~\ref{lem:vaidya} and \ref{lem:Lewis-weight}, we find that
$\gcdw$ with these metrics achieves the same mixing but without any
warmness assumption.

We note that for the same task the $\bw$ without a warm start requires
$\otilde{n^{3}}$ membership queries due to \cite{kannan1997random,jia2021reducing}.
Given that a membership query involves $O(mn)$ arithmetic operations,
the total complexity of the $\bw$ is $\otilde{mn^{4}}$. In contrast,
the per-step of the $\dw$ with the log-barrier can be run in $O\Par{mn^{\omega-1}}$
operations through the fast matrix multiplication, so the total number
of arithmetic operations is $\otilde{m^{2}n^{\omega}}$. Thus, for
$m$ close to $n$ $\gcdw$ is provably faster than the $\bw$. When
an efficient inverse maintenance proposed in \cite{laddha2020strong}
is employed, the per-step complexity can be improved to $O(n^{2}+\nnz(A))=O(mn)$.
In such cases $\gcdw$ is faster in a broader range of $m$. In particular,
if $A$ is as sparse as $\nnz(A)=O(n^{2})$, then $\gcdw$ is always
faster than the $\bw$. Moreover, $\gcdw$ with the Lewis-weight metric
mixes in $\otilde{n^{2.5}}$ steps with the per-step complexity of
$\otilde{mn^{\omega-1}}$, so it is always faster than the $\bw$
for any $m$.

\paragraph{Exponential and Gaussian sampling.}

The current mixing bound of the $\bw$ for general log-concave sampling
is $\otilde{n^{4}}$ due to \cite{lovasz2007geometry}. On the other
hand, the $\dw$ employed with any metric above for exponential sampling
converges in the same iterations as the $\dw$ for uniform sampling.
Since only difference between two sapling is the additional term of
$\exp(-f(z))/\exp(-f(x))$ in the Metropolis filter, the fast implementation
techniques mentioned earlier can be applied to the context of exponential
sampling. As a result, for the exponential sampling each of the $\dws$
described above surpasses the $\bw$ by a larger margin.

For Gaussian sampling over a polytope, we first reduce it to the exponential
sampling as in (\ref{eq:reduced-problem}): for $y=(x,t)\in\R^{n+1}$
\begin{align*}
\text{sample } & \frac{d\tilde{\pi}}{dy}(y)\propto e^{-t}\\
\text{s.t. } & Ax\geq b,\ \half\norm{x-\mu}_{\Sigma}^{2}\leq t.
\end{align*}
According to our theory, it is natural to use the metric given by
\[
g(x,t)=2\Par{\left[\begin{array}{cc}
\hess_{x}\phi_{\textsf{log}}(x)\\
 & 0
\end{array}\right]+(n+1)\hess_{(x,t)}\phi_{\textsf{Gaussian}}(x,t)},
\]
which is $\Par{O(m+n),O(m+n)}$-Dikin-amenable due to Lemma~\ref{lem:Gaussian-potential}.
Thus, $\gcdw$ needs $\otilde{n(m+n)}$ iterations of the $\dw$.
We note that the log-barrier can be replaced by the Vaidya or Lewis-weight
metrics, and in such cases one can obtain provable guarantees on the
mixing time by computing $\nu$ and $\onu$, referring to Section~\ref{sec:handbook-barrier}
or Table~\ref{tab:scaling-table}.

\subsection{Second-order cone sampling}

We consider a region given by $\norm{x-\mu'}_{\Sigma'}\leq t$ and
$A\left[\begin{array}{c}
x\\
t
\end{array}\right]\leq b$ for $A\in\R^{m\times(n+1)},b\in\R^{m},$ and $\Sigma'\in\pd$.

\paragraph{Uniform and exponential sampling.}

In this case, our self-concordance theory suggests using
\begin{align*}
2\Par{\hess(\phi_{*}+(n+1)\phi_{\textsf{SOC}})}\quad & \text{for }*=\text{log, Vaidya},\\
\text{or }2\Par{\hess(\sqrt{n+1}\phi_{\textsf{Lw}}+(n+1)\phi_{\textsf{SOC}})}\quad
\end{align*}
to deal with the truncated SOC constraint. For the log-barrier case,
this yields an $\Par{O(m+n),O(m+n)}$-Dikin-amenable metric due to
Lemma~\ref{lem:soc}, with which $\gcdw$ requires $\otilde{n(m+n)}$
iterations of the $\dw$.

\paragraph{Gaussian sampling.}

Following the reduction as in the polytope sampling, we should use
\[
g(x,t,t')=3\Par{\left[\begin{array}{cc}
\hess_{(x,t)}\phi_{\textsf{log}}(x,t)\\
 & 0
\end{array}\right]+(n+1)\left[\begin{array}{cc}
\hess_{(x,t)}\phi_{\textsf{SOC}}(x,t)\\
 & 0
\end{array}\right]+(n+2)\hess_{(x,t,t')}\phi_{\textsf{Gaussian}}(x,t,t')},
\]
which is $\Par{O(m+n),O(m+n)}$-Dikin-amenable, and thus $\gcdw$
needs $\otilde{n(m+n)}$ iterations of the $\dw$.

\subsection{PSD cone sampling\label{subsec:PSD-cone-sampling}}

For a matrix $X\in\Rnn$, recall that $\vec(X)\in\R^{n^{2}}$ denotes
the vector obtained by stacking columns of $X$ vertically. Additionally,
we define $A\in\R^{m\times n^{2}}$, $S_{X}\in\R^{m\times m}$, and
$A_{X}\in\R^{m\times n^{2}}$ by 
\[
A:=\left[\begin{array}{ccc}
\vec\Par{A_{1}} & \cdots & \vec\Par{A_{m}}\end{array}\right]^{\top},\,S_{X}:=\Diag\Par{\inner{A_{i},X}-b_{i}},\,A_{X}:=S_{X}^{-1}A,
\]
where we assume $A$ has no all-zero rows and $(S_{X})_{ii}>0$ for
$i\in[m]$. 

\paragraph{Uniform and exponential sampling.}

The metric below comes from the Hessian of the following function:
\[
-2n^{2}\log\det X-2\sum_{i=1}^{m}\log\Par{\inner{A_{i},X}-b_{i}}.
\]
Here the first term, the log-determinant, serves as a barrier for
the PSD cone while the second term is the standard logarithmic barrier
for linear constraints. We note that the $-\log\det X$ is strictly
convex on $x\in\intk$ for $K$ the truncated PSD cone, so all metrics
$g$ introduced in our main results are positive definite. Thus, the
$\dw$ with those $g$ is well-defined.
\begin{prop}
\label{thm:basicPSD} Let $K$ be the truncated PSD cone and $g$
be the local metric such that at each $X\in\intk$, for symmetric
matrices $H_{1},H_{2}$,
\[
g_{X}(H_{1},H_{2})=2n^{2}\tr\Par{X^{-1}H_{1}X^{-1}H_{2}}+2\vec(H_{1})^{\top}A_{X}^{\top}A_{X}\vec(H_{2}).
\]
Then $\gcdw$ needs $\otilde{(n^{3}+m)n^{2}}$ steps of the $\dw$
with the local metric $g$, where each step runs in $O\Par{\min(mn^{\omega}+m^{2}n^{2},n^{2\omega}+mn^{2(\omega-1)})}$
time\footnote{Here $\omega<2.373$ is the current matrix multiplication complexity
exponent (\cite{le2014powers}).}.
\end{prop}

Since $g_{X}$ is $\Par{O(m+n^{3}),O(m+n^{3})}$-Dikin-amenable due
to Lemma~\ref{lem:psd}, $\gcdw$ requires $\otilde{(n^{3}+m)n^{2}}$
iterations of the $\dw$. As mentioned earlier, efficient maintenance
of the inverse of a metric function could lead to a faster per-step
complexity. As an example, we provide such an implementation of Proposition~\ref{thm:basicPSD}
in Section~\ref{subsec:oracle-implementation}. Putting these together,
for an interesting regime of $m=O(1)$ $\gcdw$ is faster than the
$\bw$ by a factor of $n$ in terms of the total complexity.

If we replace the log-barrier by the Vaidya metric, then the dependence
on $m$ is improved to $\sqrt{m}$ as in the polytope sampling:
\begin{prop}
\label{thm:hybridPSD} Let $K$ be the truncated PSD cone and $g$
be the local metric such that at each $X\in\intk$, for symmetric
matrices $H_{1},H_{2}$,
\[
g_{X}(H_{1},H_{2})=2n^{2}\tr\Par{X^{-1}H_{1}X^{-1}H_{2}}+44\sqrt{\frac{m}{n}}\vec(H_{1})^{\top}A_{X}^{\top}\Par{\Sigma_{X}+\frac{n}{m}I_{m}}A_{X}\vec(H_{2}).
\]
Then $\gcdw$ needs $\otilde{\Par{n^{2}+\sqrt{m}}n^{3}}$ steps of
the $\dw$ with the local metric $g$, with each step running in $\otilde{mn^{2(\omega-1)}}$
amortized time.
\end{prop}

\begin{proof}
We set
\begin{align*}
g(X) & =2\Par{n^{2}g_{1}(X)+g_{2}(X)},\ \text{where}\\
g_{1}(X) & =M^{\top}(X\kro X)^{-1}M,\\
g_{2}(X) & =22\sqrt{\frac{m}{n}}M^{\top}A_{X}^{\top}\Par{\Sigma_{X}+\frac{n}{m}I_{m}}A_{X}M.
\end{align*}
Since $n^{2}g_{1}$ and $g_{2}$ are SSC, $g$ is also SSC due to
Lemma~\ref{lem:ssc-sum} and $O(n^{3}+\sqrt{mn^{2}})$-symmetric\footnote{Since the dimension is $d$ in the PSD setting, we should replace
$n$ by $d=O(n^{2})$ when applying Lemma~\ref{lem:paramsBarrier}.} due to Lemma~\ref{lem:symmetry-addition}. As $n^{2}g_{1}$ and
$g_{2}$ is SLTSC and SASC, $g$ is LTSC and ASC. Putting these together,
it follows that $g$ is $\Par{O(n^{3}+\sqrt{mn^{2}}),O(n^{3}+\sqrt{mn^{2}})}$-Dikin-amenable.
Therefore, Theorem~\ref{thm:Dikin-annealing} implies that $\gcdw$
incurs $\otilde{n^{2}\Par{n^{3}+\sqrt{mn^{2}}}}=\otilde{n^{3}\Par{n^{2}+\sqrt{m}}}$
total iterations of the $\dw$ with $g$.

Now we bound the per-step complexity of the $\dw$ (Algorithm~\ref{alg:DikinWalk}).
Recall that it requires (1) the update of the leverage scores, (2)
computation of the matrix function induced by the local metric $g$,
(3) the inverse of the matrix function and (4) its determinant. By
Theorem~46 in \cite{lee2019solving} (with $p=2$ and $n\gets d$
therein), the initialization of the leverage scores at the beginning
takes $\otilde{mn^{2\omega}}$ and their updates takes $\otilde{mn^{2(\omega-1)}}$
time. Since (1) takes $\otilde{mn^{2(\omega-1)}}$, (2) takes $\otilde{n^{4}+mn^{2(\omega-1)}}$,
and (3) and (4) take $O\Par{n^{2\omega}}$, each iteration runs in
$\otilde{n^{2\omega}+mn^{2(\omega-1)}}$ time. Note that even though
the initialization of leverage scores takes $\otilde{mn^{2\omega}}$
time, the amortized per step time complexity (since the mixing rate
is $\otilde{n^{3}(n^{2}+\sqrt{m})}$) goes to $\otilde{n^{2\omega}+mn^{2(\omega-1)}}=\otilde{mn^{2(\omega-1)}}$
time.
\end{proof}
Lastly, the dependence on $m$ can be made poly-logarithmic by working
with the Lewis-weight metric. We remark that for uniform sampling
the total complexity of $\gcdw$ is less than that of the $\bw$ by
the order of $n^{5-2\omega}$.
\begin{prop}
\label{thm:LSPSD} Let $K$ be the truncated PSD cone and $g$ be
the local metric such that at each $X\in\intk$, for symmetric matrices
$H_{1},H_{2}$, 
\[
g_{X}(H_{1},H_{2})=2n^{2}\tr\Par{X^{-1}H_{1}X^{-1}H_{2}}+c_{1}\Par{\log m}^{c_{2}}n\vec(H_{1})^{\top}A_{X}^{\top}W_{X}A_{X}\vec(H_{2}),
\]
where $W_{X}$ is the diagonalized $\ell_{p}$-Lewis weight of $A_{X}$
with $p=O(\log m)$, and $c_{1},c_{2}>0$ are universal constants.
Then $\gcdw$ requires $\otilde{n^{5}}$ steps of the $\dw$, with
each step running in $\otilde{mn^{2(\omega-1)}}$ amortized time.
\end{prop}

\begin{proof}
We set
\begin{align*}
g(X) & =2\Par{n^{2}g_{1}(X)+g_{2}(X)},\ \text{where}\\
g_{1}(X) & =-\hess_{X}\log\det X=M^{\top}(X\kro X)^{-1}M,\\
g_{2}(X) & =c_{1}\Par{\log m}^{c_{2}}nM^{\top}A_{X}^{\top}W_{X}A_{X}M\ \text{for some constants \ensuremath{c_{1},c_{2}>0}.}
\end{align*}
Since $n^{2}g_{1}$ and $g_{2}$ are SSC, $g$ is also SSC due to
Lemma~\ref{lem:ssc-sum} and $O^{*}\Par{n^{3}}$-symmetric due to
Lemma~\ref{lem:symmetry-addition}. As $n^{2}g_{1}$ and $g_{2}$
is SLTSC and SASC, $g$ is LTSC and ASC. Putting these together, it
follows that $g$ is $\Par{O^{*}\Par{n^{3}},O^{*}\Par{n^{3}}}$-Dikin-amenable.
Therefore, Theorem~\ref{thm:Dikin-annealing} implies that $\gcdw$
requires $\otilde{n^{5}}$ iterations of the $\dw$ with $g$. Since
the initialization and update of the Lewis weight takes $\otilde{mn^{2\omega}}$
and $\otilde{mn^{2(\omega-1)}}$ time (Theorem~46 in \cite{lee2019solving}),
the same implementation with Theorem~\ref{thm:hybridPSD} also has
the time complexity of $\otilde{mn^{2(\omega-1)}}$.
\end{proof}

\paragraph{Gaussian sampling.}

Just as in polytope or second-order cone sampling, we introduce a
new variable $t$ by replacing a quadratic term in the potential.
This reduces the Gaussian sampling problem to an exponential sampling
problem. We then work with a local metric 
\[
g(X,t)=3\Par{n\left[\begin{array}{cc}
\hess_{X}\phi_{\textsf{Lw}}(X)\\
 & 0
\end{array}\right]+n^{2}\left[\begin{array}{cc}
\hess_{X}\phi_{\textsf{PSD}}(X)\\
 & 0
\end{array}\right]+n^{2}\hess_{(X,t)}\phi_{\textsf{Gaussian}}(X,t)},
\]
which is $\Par{O^{*}(n^{3}),O^{*}(n^{3})}$-Dikin-amenable. Thus,
$\gcdw$ needs $\otilde{n^{5}}$ iterations of the $\dw$ with the
local metric $g$, and the per-step complexity remains $\otilde{mn^{2(\omega-1)}}$
in amortized time.

\subsubsection{Per-step implementation \label{subsec:oracle-implementation}}

Now we design an oracle that implements each iteration of the $\dw$
(Algorithm~\ref{alg:DikinWalk}). This can be implemented as follows:
when the current point is $x$,
\begin{itemize}
\item Sample $z\sim\ncal\Par{0,\frac{r^{2}}{n}g(x)^{-1}}$.
\item Compute $y=x+g(x)^{-\half}z$ and propose it.
\item Accept $y$ with probability $\min\Par{1,\sqrt{\frac{\det g(y)}{\det g(x)}}\frac{\exp\Par{f(x)}}{\exp\Par{f(y)}}}$.
\end{itemize}
We provide two algorithms with the complexity of $O\Par{mn^{\omega}+m^{2}n^{2}}$
and $O\Par{n^{2\omega}+mn^{2(\omega-1)}}$. We can implement each
iteration in $O\Par{\min\Par{mn^{\omega}+m^{2}n^{2},n^{2\omega}+mn^{2(\omega-1)}}}$
time by using the former for small $m$ and the latter for large $m$.
This completes the second half of Theorem~\ref{thm:basicPSD}. 

\paragraph{Algorithm for small $m$.}

For simplicity here, we ignore the constant factors of $g$, denoting
$g=g_{1}+g_{2}$ for
\begin{align*}
g_{1}(X) & =M^{\top}(X\kro X)^{-1}M=:BB^{\top},\\
g_{2}(X) & =M^{\top}A^{\top}S_{X}^{-2}AM=:UU^{\top},
\end{align*}
where $B:=M^{\top}(X\kro X)^{-1/2}\in\R^{d\times n^{2}}$ and $U:=M^{\top}A^{\top}S_{X}^{-1}\in\R^{d\times m}$.
Letting $u_{i}$ be the $i^{th}$ column of $U$ for $i\in[m]$, we
note that $g_{2}=\sum_{i=1}^{m}u_{i}u_{i}^{\top}$.

We first implement a subroutine for computing $g(X)^{-1}v$ for a
given vector $v\in\R^{d}$ in $O(mn^{\omega}+m^{2}n^{2})$ time.

\begin{algorithm2e}[t]

\caption{Computation of $g(X)^{-1}v$}\label{alg:subroutine}

\SetAlgoLined

\textbf{Input:} $X\in\psd$, vector $v\in\R^{d}$, local metric $g$.

\textbf{Output:} $g(X)^{-1}v$

Prepare the column vectors $u_{i}$ of $U=M^{\top}A^{\top}S_{X}^{-1}$.

For $\bar{g}_{0}:=g_{1}(X)$, compute $\bar{g}_{0}^{-1}v$ and $\bar{g}_{0}^{-1}u_{i}$
for $i\in[m]$.

\For{$i=1,\cdots,m$}{

Compute $\bar{g}_{i}^{-1}v$ and $\bar{g}_{i}^{-1}u_{j}$ for $j\in[m]$,
according to 

\[
\bar{g}_{i}^{-1}w=\bar{g}_{i-1}^{-1}w-\frac{\bar{g}_{i-1}^{-1}u_{i}\cdot u_{i}^{\top}\bar{g}_{i-1}^{-1}w}{1+u_{i}^{\top}\bar{g}_{i-1}^{-1}u_{i}}.
\]

}

Return $\bar{g}_{m}^{-1}v$.

\end{algorithm2e}
\begin{prop}
\label{prop:oracle} Algorithm~\ref{alg:subroutine} computes $g(X)^{-1}v$
in $O(mn^{\omega}+m^{2}n^{2})$ time for a query vector $v\in\R^{d}$.
\end{prop}

\begin{proof}
Let $v\in\R^{d}$ be a given vector, and denote $\bar{g}_{0}:=g_{1}$
and $\bar{g}_{i}:=\bar{g}_{i-1}+u_{i}u_{i}^{\top}$ for $i\in[m]$.
We first prepare the column vectors $u_{i}$'s of $U=M^{\top}A^{\top}S_{X}^{-1}$
in $O(mn^{2})$ time and then initialize $\bar{g}_{0}^{-1}v$ and
$\bar{g}_{0}^{-1}u_{i}$ for $i\in[m]$ in $O(mn^{\omega})$ time.
For $u_{i}$'s, note that $S_{X}$ can be prepared in $O(mn^{2})$
time, and thus $A^{\top}S_{X}^{-1}$ takes $O(mn^{2})$ time due to
$A\in\R^{n^{2}\times m}$. Since each row of $M^{\top}\in\R^{d\times n^{2}}$
has at most two non-zero entries, we can obtain $u_{i}$'s in $O(mn^{2})$
time.

For $\bar{g}_{0}^{-1}v$ and $\bar{g}_{0}^{-1}u_{i}$, we recall from
Lemma~\ref{prop:metricFormula} that for a vector $z\in\R^{d}$ 
\begin{align*}
g_{1}^{-1}z & =M^{\dagger}(X\kro X)M^{\dagger\top}z=LN(X\kro X)NL^{\top}z.
\end{align*}
Since each row of $L^{\top}\in\R^{n^{2}\times d}$ has at most two
non-zero entries, $w:=L^{\top}z\in\R^{n^{2}}$ can be computed in
$O(n^{2})$ time. From the definition of $N$, it follows that $Nw=\vec\Par{\half(W+W^{\top}}$
for $W:=\vec^{-1}(w)\in\R^{n\times n}$, which also can be computed
in $O(n^{2})$ time. For $\overline{W}:=\half\Par{W+W^{\top}}$, it
follows that
\begin{align*}
(X\kro X)Nw & =(X\kro X)\vec\Par{\overline{W}}\underset{\text{Lemma \ref{lem:Kronecker}-1}}{=}\vec\Par{X\overline{W}X},
\end{align*}
which can be computed in $O(n^{\omega})$ time by the fast matrix
multiplication, and in a similar way we can compute $LN\vec\Par{X\overline{W}X}$
in $O(n^{2})$ time. Putting all these together, $\bar{g}_{0}^{-1}v$
can be computed in $O(n^{\omega})$ time, and repeating this for $u_{j}$'s
yields $\Brace{\bar{g}_{0}^{-1}v,\bar{g}_{0}^{-1}u_{1},\dots,\bar{g}_{0}^{-1}u_{m}}$
in $O(mn^{\omega})$ time.

Starting with these initializations, we recursively use the Sherman--Morrison
formula: for a given vector $z\in\R^{d}$
\begin{equation}
\bar{g}_{i}^{-1}z=\bar{g}_{i-1}^{-1}z-\frac{\bar{g}_{i-1}^{-1}u_{i}u_{i}^{\top}\bar{g}_{i-1}^{-1}z}{1+u_{i}^{\top}\bar{g}_{i-1}^{-1}u_{i}}.\label{eq:sherman-morrison}
\end{equation}
Using $\bar{g}_{i-1}^{-1}u_{j}$ and $\bar{g}_{i-1}^{-1}v$ from a
previous iteration, we can compute each of $\bar{g}_{i}^{-1}u_{j}$
and $\bar{g}_{i}^{-1}v$ in the current iteration in $O(n^{2})$ time,
and thus each round for update takes $O(mn^{2})$ time in total. Since
we iterate for $m$ rounds, Algorithm~\ref{alg:subroutine} outputs
$\bar{g}_{m}^{-1}v=g(X)^{-1}v$ in $O(mn^{\omega}+m^{2}n^{2})$ time.
\end{proof}
With this subroutine in hand, we proceed to an efficient implementation
of two tasks -- computation of (1) $g(x)^{-\half}z$ for a given
vector $z\in\R^{d}$ and (2) $\sqrt{\frac{\det g(y)}{\det g(x)}}\frac{\exp\Par{f(x)}}{\exp\Par{f(y)}}$.

\begin{algorithm2e}[t]

\caption{Implementation of the $\dw$}\label{alg:perStep-small-m}

\SetAlgoLined

\textbf{Input:} current point $X\in\psd$, local metric $g$

\tcp{Step 1: Sampling from $\ncal\Par{0,\frac{r^{2}}{n}g(X)^{-1}}$}

Draw $w\sim\ncal\Par{0,I_{n^{2}+m}}$ and $v\gets g(X)^{-1}\left[\begin{array}{cc}
B & U\end{array}\right]w$ by Algorithm~\ref{alg:subroutine}.\

Propose $y\gets\svec(X)+\frac{r}{\sqrt{n}}v$.

\

\tcp{Step 2: Computation of acceptance probability}

Use Algorithm~\ref{alg:subroutine} to prepare $\Brace{\bar{g}_{i}^{-1}u_{1},\dots,\bar{g}_{i}^{-1}u_{m}}_{i=0}^{m}$
at $X$ and $Y:=\svec^{-1}(y)$.\

$\det\bar{g}_{0}(\cdot)\gets2^{n(n-1)/2}(\det(\cdot))^{-(n+1)}$ ($\because$
Lemma~\ref{lem:Kronecker}-7)

\For{$i=1,\cdots,m$}{

$\det(\bar{g}_{i+1})\gets\det\bar{g}_{i}\cdot\Par{1+u_{i+1}^{\top}\bar{g}_{i}^{-1}u_{i+1}}$.

}

Accept $Y$ with probability $\min\Par{1,\sqrt{\frac{\det\bar{g}_{m}(Y)}{\det\bar{g}_{m}(X)}}\frac{\exp\Par{f(X)}}{\exp\Par{f(Y)}}}$.

\end{algorithm2e}
\begin{lem}
\label{lem:perStep-small-m}Algorithm~\ref{alg:perStep-small-m}
implements each iteration of the $\dw$ with complexity of $O\Par{mn^{\omega}+m^{2}n^{2}}$.
\end{lem}

\begin{proof}
Here we provide details of Algorithm~\ref{alg:perStep-small-m} in
two stages -- (1) sampling from $\ncal\Par{0,\frac{r^{2}}{n}g(x)^{-1}}$
and (2) computation of acceptance probability.

\paragraph{(1) Gaussian sampling:}

For simplicity, we ignore $r^{2}/n$ and illustrate how to draw $v\sim\ncal(0,g(X)^{-1})$
without full computation of $g(X)^{-1}$ in $O(mn^{\omega}+m^{2}n^{2})$
time.

Our approach is to compute $v:=g(X)^{-1}\left[\begin{array}{cc}
B & U\end{array}\right]w$ for $w\sim\ncal(0,I_{n^{2}+m})$, which follows the Gaussian distribution
with covariance
\begin{align*}
g(X)^{-1}\left[\begin{array}{cc}
B & U\end{array}\right]\Par{g(X)^{-1}\left[\begin{array}{cc}
B & U\end{array}\right]}^{\top} & =g(X)^{-1}(BB^{\top}+CC^{\top})g(X)^{-1}\\
 & =g(X)^{-1}g(X)g(X)^{-1}\\
 & =g(X)^{-1},
\end{align*}
since $v$ is a linear transformation of the Gaussian random variable
$w$.

Denoting $w=\left[\begin{array}{c}
w_{b}\\
w_{u}
\end{array}\right]$ for $w_{b}\sim\ncal(0,I_{n^{2}})$ and $w_{u}\sim\ncal(0,I_{m})$,
we can show that $\left[\begin{array}{cc}
B & U\end{array}\right]w$ can be computed in $O(n^{\omega}+mn^{2})$ time as follows:
\begin{align*}
\left[\begin{array}{cc}
B & U\end{array}\right]w & =Bw_{b}+Uw_{c}\\
 & =M^{\top}\underbrace{(X\kro X)^{-\half}w_{b}}_{\text{Use Lemma \ref{eq:sherman-morrison}}}+M^{\top}A^{\top}S_{X}^{-1}w_{c}\\
 & =M^{\top}\Par{\vec\Par{X^{-\half}\vec^{-1}(w_{b})X^{-\half}}+A^{\top}S_{X}^{-1}w_{c}},
\end{align*}
where $\vec\Par{X^{-\half}\vec^{-1}(w_{b})X^{-\half}}$ and $A^{\top}S_{X}^{-1}w_{u}$
can be computed in $O(n^{\omega})$ and $O(mn^{2})$ time, respectively.
Since each row of $M^{\top}\in\R^{d\times n^{2}}$ has at most two
non-zero entries, $\left[\begin{array}{cc}
B & U\end{array}\right]w$ can be computed in $O(n^{\omega}+mn^{2})$ time. Using Algorithm~\ref{alg:subroutine},
we obtain $v=g(X)^{-1}\left[\begin{array}{cc}
B & U\end{array}\right]w$ in $O(mn^{\omega}+m^{2}n^{2})$ time.

\paragraph{(2) Computation of acceptance probability. }

We show that this step also takes $O(mn^{\omega}+m^{2}n^{2})$ time.
To compute $\det g(X)$, we use Algorithm~\ref{alg:subroutine} to
prepare $\Brace{\bar{g}_{i}^{-1}u_{1},\dots,\bar{g}_{i}^{-1}u_{m}}_{i=0}^{m}$
at $X$ and $Y=\svec^{-1}(y)$ in $O(mn^{\omega}+m^{2}n^{2})$ time.
Recall the matrix determinant lemma:
\[
\det\Par{A+uu^{\top}}=\det A\cdot\Par{1+u^{\top}A^{-1}u}.
\]
 Using the following recursive formula
\begin{align*}
\det\Par{\bar{g}_{i+1}} & =\det\Par{\bar{g}_{i}+u_{i+1}u_{i+1}^{\top}}=\det\bar{g}_{i}\cdot\Par{1+u_{i+1}^{\top}\bar{g}_{i}^{-1}u_{i+1}},
\end{align*}
we start with $\det\bar{g}_{0}=\det g_{1}=2^{n(n-1)/2}\Par{\det X}^{-(n+1)}$
(see Lemma~\ref{lem:Kronecker}-7), which can be computed in $O(n^{\omega})$
time, and compute $\det g(X)$ (and $\det g(Y)$ in the same way)
in $O(mn^{\omega}+m^{2}n^{2})$ time.
\end{proof}


\paragraph{Algorithm for large $m$.}

The algorithm right above has quadratic dependence on the number $m$
of constraints, which could become expensive for large $m$. In this
regime, we just fully compute the whole matrix function of size $\R^{d\times d}$,
which takes $O(n^{2\omega}+mn^{2(\omega-1)})$ time, and computing
its inverse, square-root, and determinant takes $O(n^{2\omega})$
time. 

\subsubsection{Handling approximate Lewis weights \label{subsec:app-lewis-weight}}

In the implementation of the $\dw$ with the Lewis-weights metric,
we use an approximation algorithm presented in \cite{lee2019solving}
for computing and updating the Lewis weight, which ensures 
\[
(1-\delta)\wtilde_{X}\preceq W_{X}\preceq(1+\delta)\wt W_{X}
\]
for the approximate Lewis weights $\wtilde_{X}$ and a target accuracy
parameter $\delta$ (note that the initialization and update times
of the Lewis weight above hide poly-logarithmic dependence on $\log(1/\delta)$).
Strictly speaking, we should check that these approximate Lewis weights
do not affect the theoretical guarantees above.

To see this, let us define
\begin{align*}
\widetilde{g}(X) & =2\Par{ng_{1}(X)+\widetilde{g_{2}}(X)},\ \text{where}\\
g_{1}(X) & =-n^{2}\hess_{X}\log\det X=n^{2}M^{\top}(X\kro X)^{-1}M,\\
\wt g_{2}(X) & =c_{1}\Par{\log m}^{c_{2}}nM^{\top}A_{X}^{\top}\widetilde{W}_{X}A_{X}M\ \text{for some constants \ensuremath{c_{1},c_{2}>0}.}
\end{align*}
First of all, the $\dw$ with $\widetilde{g}$ still converges to
a target distribution, since the approximation algorithm in \cite{lee2019solving}
is deterministic and thus the condition of detailed balance still
holds under the acceptance probability of $\min\Par{1,\sqrt{\frac{\det\tilde{g}(Y)}{\det\tilde{g}(X)}}\frac{\exp\Par{f(X)}}{\exp\Par{f(Y)}}}$.
For $\widetilde{P}_{X}$ the one-step distribution of the $\dw$ started
at $X$ with $\widetilde{g}$, we can show one-step coupling similar
to Lemma~\ref{lem:one-step}, following the overall proof therein
and taking $\delta=1/\text{poly}(n)$ small enough.
\begin{lem}
[One-step coupling] For convex $K\subset\Rn$, let $g:\intk\to\pd$
be SSC, ASC, LTSC, and $\phi:\intk\to\R$ be its function counterpart.
Suppose that the potential $f$ of the target distribution $\pi$
is $\beta$-relatively smooth in $\phi$. Then there exist constants
$s_{1},s_{2}>0$ such that if $\norm{x-y}_{g(x)}\leq s_{1}\frac{r}{\sqrt{n}}$
with $r=s_{2}\min\Par{1,\frac{1}{\sqrt{\beta}}}$ for $x,y\in\intk$,
then $\dtv\Par{\widetilde{P}_{x},\widetilde{P}_{y}}\leq\frac{3}{4}+0.01$.
\end{lem}

\begin{proof}
We just reproduce the proof of Lemma~\ref{lem:one-step}. For $\frac{d\pi}{dx}\propto\exp\Par{-f(x)}\cdot\mathbf{1}_{K}(x)$,
we denote
\begin{align*}
g_{x}(z) & :=\frac{d\ncal\Par{x,\frac{r^{2}}{n}\tilde{g}(x)^{-1}}}{dx}(z),\\
R(x,z) & =\frac{g_{z}(x)}{g_{x}(z)}\frac{\exp\Par{-f(z)}}{\exp\Par{-f(x)}},\\
A(x,z) & =\min\Par{1,\mathbf{1}\Par{z\in K}\cdot R(x,z)}.
\end{align*}
Then the transition kernel of the $\dw$ started at $x$ can be written
as 
\begin{align*}
\widetilde{P}(x,dz) & =\underbrace{\Par{1-\int A(x,z')g_{x}(z')dz'}}_{=:r_{x}}\delta_{x}(dz)+A(x,z)g_{x}(z)dz,\text{ so}\\
\widetilde{P}(x,S) & =r_{x}\dx(S)+\int_{S}A(x,z)g_{x}(z)dz\ \text{for any measurable set }S.
\end{align*}
Thus, for $x,y\in\intk$
\begin{align*}
\dtv(P_{x},P_{y}) & =\underbrace{\half\Par{r_{x}+r_{y}}}_{\textsf{I}}+\underbrace{\half\int\Abs{A(x,z)g_{x}(z)-A(y,z)g_{y}(z)}dz}_{\textsf{II}}.
\end{align*}

We note that $(1-\delta)\wt g_{2}\preceq g_{2}\preceq(1+\delta)\wt g_{2}$
and thus 
\begin{equation}
(1-\delta)\wt g\preceq g\preceq(1+\delta)\wt g,\label{eq:closeness-approx}
\end{equation}
and this implies $(1-\delta)I\preceq\wt g^{-\half}g\wt g^{-\half}\preceq(1+\delta)I$.
Hence, $(1-\delta)^{n^{2}/2}\leq\sqrt{\frac{\det g}{\det\wt g}}\leq(1+\delta)^{n^{2}/2}$
and
\begin{align}
(1-\delta)^{n^{2}}\sqrt{\frac{\det\wt g(z)}{\det\wt g(x)}} & \leq\sqrt{\frac{\det g(z)}{\det g(x)}}\leq(1+\delta)^{n^{2}}\sqrt{\frac{\det\wt g(z)}{\det\wt g(x)}}.\label{eq:similar-ratio-approx}
\end{align}

With this in mind, recall that 
\[
r_{x}=1-\int A(x,z)g_{x}(z)dz=1-\int\min\bigg(1,\,\underbrace{\mathbf{1}\Par{z\in K}\frac{e^{-f(z)}}{e^{-f(x)}}}_{=:\textsf{A}}\underbrace{\frac{g_{z}(x)}{g_{x}(z)}}_{=:\textsf{B}}\bigg)g_{x}(z)dz.
\]
We can bound $\textsf{A}$ in a similar way by using (\ref{eq:closeness-approx}).
For $\textsf{B}$, 
\[
\log\text{\textsf{B}}=-\frac{n}{2r^{2}}\Par{\norm{z-x}_{z}^{2}-\norm{z-x}_{x}^{2}}+\half\Par{\log\det\widetilde{g}(z)-\log\det\widetilde{g}(x)}.
\]
As in Lemma~\ref{lem:one-step}, the second term can be bounded lower
by $\exp\Par{-3\veps}$ using (\ref{eq:similar-ratio-approx}). The
first term can be lower-bounded by invoking ASC of $g$. To see this,
ignoring the normalization constant of $g_{x}$
\begin{align*}
(*)= & \int\mathbf{1}\Par{\norm{z-x}_{\widetilde{g}(z)}^{2}-\norm{z-x}_{\widetilde{g}(x)}^{2}\leq2\veps\frac{r^{2}}{n}}\sqrt{\Abs{\widetilde{g}(x)}}\exp\Par{-\half\norm{z-x}_{\widetilde{g}(x)}^{2}}dz\\
= & \int\mathbf{1}\Par{\norm{z-x}_{\widetilde{g}(z)}^{2}-\norm{z-x}_{\widetilde{g}(x)}^{2}\leq2\veps\frac{r^{2}}{n}}\sqrt{\Abs{g(x)}}\exp\Par{-\half\norm{z-x}_{g(x)}^{2}}\\
 & \qquad\cdot\sqrt{\Abs{\frac{\widetilde{g}(x)}{g(x)}}}\exp\Par{-\half\Par{\norm{z-x}_{\widetilde{g}(x)}^{2}-\norm{z-x}_{g(x)}^{2}}}dz\\
\leq & \int\mathbf{1}\Par{\norm{z-x}_{\widetilde{g}(z)}^{2}-\norm{z-x}_{\widetilde{g}(x)}^{2}\leq2\veps\frac{r^{2}}{n}}\sqrt{\Abs{g(x)}}\exp\Par{-\half\norm{z-x}_{g(x)}^{2}}\\
 & \qquad\cdot(1+\delta)^{n^{2}/2}\exp\Par{\frac{\delta}{2}\norm{z-x}_{g(x)}^{2}}dz.
\end{align*}
Due to $\norm{z-x}_{g(x)}^{2}\lesssim r^{2}$ w.h.p., taking $\delta=\veps/n^{10}$
leads to 
\[
(*)\leq2\int\mathbf{1}\Par{\norm{z-x}_{\widetilde{g}(z)}^{2}-\norm{z-x}_{\widetilde{g}(x)}^{2}\leq2\veps\frac{r^{2}}{n}}\sqrt{\Abs{g(x)}}\exp\Par{-\half\norm{z-x}_{g(x)}^{2}}dz.
\]
Also, due to 
\begin{align*}
\norm{z-x}_{\widetilde{g}(z)}^{2}-\norm{z-x}_{\widetilde{g}(x)}^{2} & \geq(1-\delta)\norm{z-x}_{g(z)}^{2}-(1+\delta)\norm{z-x}_{g(x)}^{2}\\
 & =(1-\delta)\Par{\norm{z-x}_{g(z)}^{2}-\norm{z-x}_{g(x)}^{2}}-2\delta\norm{z-x}_{g(x)}^{2},
\end{align*}
we have
\begin{align*}
(*) & \leq2\int\mathbf{1}\Par{\norm{z-x}_{g(z)}^{2}-\norm{z-x}_{g(x)}^{2}\leq2\veps(1-\delta)^{-1}\frac{r^{2}}{n}+\veps\frac{r^{2}}{n}}\sqrt{\Abs{g(x)}}\exp\Par{-\half\norm{z-x}_{g(x)}^{2}}dz\leq6\veps
\end{align*}
by invoking ASC of $g$ in the last inequality. Putting these together,
$\textsf{I}\leq\half+O(1)\veps$. For $\textsf{II}$, we can follow
the proof of Lemma~\ref{lem:one-step} to show $\textsf{II}\leq\frac{1}{4}+O(1)\veps$,
and every technical issue can be resolved by repeating the same techniques
above.
\end{proof}

