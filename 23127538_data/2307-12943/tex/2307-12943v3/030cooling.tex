
\section{Gaussian cooling on manifolds revisited: IPM framework for sampling
\label{sec:IPM-framework}}

In this section, we derive a sampling analogue of the Interior-Point
Method through comparison with IPM in optimization, by extending \emph{Gaussian
cooling on manifolds} introduced in \cite{cousins2018gaussian,lee2018convergence}.
We demonstrate that combining the sampling IPM framework with the
$\dw$ efficiently generates a warm start, allowing us to sample from
a target distribution $\pi$ with $\frac{d\pi}{dx}\propto e^{-f(x)}\cdot\mathbf{1}_{K}(x)$
and finite second moment.

\subsection{Derivation of sampling IPM \label{subsec:derivation-IPM-sampling}}

We begin by revisiting our setup. Let $K\subset\Rn$ be a closed convex
set, $g:\intk\to\pd$ a $(\nu,\onu)$-self-concordant matrix function,
and $\phi:\intk\to\R$ its (strictly convex) self-concordant counterpart.
We may assume that $\min_{x}\phi(x)=0$ by considering $\phi-\min_{x}\phi(x)$
(here, $\arg\min\phi(x)$ can be efficiently found by the optimization
IPM). We assume that $f$ is $\alpha$-relatively strongly convex
and $\beta$-relatively smooth in $\phi$ for $0\leq\alpha\leq\beta<\infty$,
i.e., $0\preceq\alpha\hess\phi\preceq\hess f\preceq\beta\hess\phi$
on $\inter(K)$. Lastly, we assume that $d\pi/dx\propto e^{-f}\cdot\mathbf{1}_{K}$
is integrable (i.e., $\int_{K}e^{-f(x)}dx<\infty$). We define $\bar{f}(x):=\frac{\nu}{n}f(x)$
and $g_{\phi}(x):=\hess\phi(x)$.

\paragraph{Interior-point method for optimization.}

In solving structural convex optimization problems, we encounter $\min_{x\in K}f(x)$,
where $f:\Rn\to\R$ is a convex function, and $K\subset\Rn$ is a
closed convex set. Also, both $K$ and $\{(x,t):f(x)\leq t\}$ admit
efficiently computable self-concordant barriers denoted by $\phi_{1}$
and $\phi_{2}$, respectively. We can simplify the problem by equivalently
solving $\min_{x\in K,\,\{(x,t):f(x)\leq t\}}t$. Therefore, it suffices
to focus on $\min_{x\in K,\,\{(x,t):f(x)\leq t\}}c\cdot(x,t)$ for
a constant $c\in\R^{n+1}$. 

IPM then regularizes this linear objective function by adding $\frac{1}{\lda}\phi(x,t)=\frac{1}{\lda}\Par{\phi_{1}(x)+\phi_{2}(x,t)}$
for $\lda>0$. This regularization removes the hard constraints of
$K$ and $\{f(x)\leq t\}$, and the resulting formulation becomes
\[
\min_{y=(x,t)\in\R^{n+1}}f_{\lda}(y):=c^{\top}y+\frac{1}{\lda}\phi(y),
\]
where $\phi(y)$ approaches infinity as $y$ approaches the boundary
of the constraints $K\cap\Brace{f(x)\leq t}$. For each fixed $\lda>0$,
there exists a minimum $y_{\lda}$ of the convex function $f_{\lda}(y)$.
Intuitively, as $\lda\to\infty$ the regularization term $\frac{1}{\lda}\phi(y)$
vanishes, so $y_{\lda}$ converges to the optimal point of $\min_{y\in K\cap\{f(x)\le t\}}c^{\top}y$.
The path followed by $\{x_{\lda}\}_{\lda>0}$ is known as the \emph{central
path}, and IPM aims to approximatly follow this central path as $\lda$
increases. 

To be precise, suppose that for $\lda_{1}>0$, an approximation solution
$\bar{y}_{\lda_{1}}$ maintained by IPM is close enough to $y_{\lda_{1}}$.
Then IPM takes an optimization step (e.g., a Newton step), which takes
into account the local geometry induced by the Hessian of the barrier
$\phi$, to find an approximate solution $\bar{y}_{\lda_{2}}$ when
$\lda_{2}>\lda_{1}$. As long as $\bar{y}_{\lda_{1}}$ is sufficiently
close to $y_{\lda_{1}}$, this approximate solution $\bar{y}_{\lda_{1}}$
serves a good starting point for the non-Euclidean optimizer, which
takes $\bar{y}_{\lda_{1}}$ to $\bar{y}_{\lda_{2}}$. IPM alternates
between increasing $\lda$ and updating $\bar{y}_{\lda}$, until $\lda$
reaches $\nu/\veps$. This is described formally as Algorithm~\ref{alg:IPM}.

\begin{algorithm2e}[H]

\caption{Interior-Point Method} \label{alg:IPM}

\SetAlgoLined

\textbf{Input:} A $\nu$-self-concordant barrier $\phi$ for a constraint

\textbf{Output:} $y_{\lda}$

Denote $f_{\lda}(y):=c^{\top}y+\frac{1}{\lda}\phi(y)$.

\tcp{Phase 1: Starting feasible point}

Find $y_{0}=\arg\min\phi(y)$, set $\lda=\frac{1}{6}\norm c_{\hess\phi(y_{0})}^{-1}$,
and $\bar{y}_{\lda}\gets y_{0}$.

\tcp{Phase 2: Increasing $\lda$ until $\lda\leq\frac{\nu+1}{\veps}$}

\While{$\lda\leq\frac{\nu+1}{\veps}$}{

$\bar{y}_{\lda}\gets\bar{y}_{\lda}-\Par{\hess f_{\lda}(\bar{y}_{\lda})}^{-1}\grad f_{\lda}(\bar{y}_{\lda})$
\tcp{``Opt. step'' (e.g., the Newton step)}

$\lda\gets(1+r)\lda$ with $r=\frac{1}{9\sqrt{\nu}}$. \tcp{Increase $\lda$}

}

\end{algorithm2e}

The ideas behind IPM are justified by the following theoretical guarantee:
Algorithm~\ref{alg:IPM} returns $y$ in $O\Par{\sqrt{\nu}\log\Par{\frac{\nu}{\veps}\norm c_{\hess\phi(y_{0})^{-1}}}}$
iterations such that $c^{\top}y\leq c^{\top}y^{*}+\veps$ for $y^{*}=\arg\min_{y\in K\cap\{f(x)\le t\}}c^{\top}y$.
% Figure environment removed


\paragraph{Translation to sampling.}

Now let us adapt each step of IPM into the sampling context with the
conceptual analogy between convex optimization and logconcave sampling
in mind: For convex $K\subset\Rn$ and convex function $f:K\to\R$
\begin{align*}
\min f(x) & \quad\longleftrightarrow\quad\text{sample }e^{-f(x)}\\
\text{s.t. }x\in K & \qquad\qquad\quad\text{s.t. }x\in K
\end{align*}
Similar to the optimization IPM, we first replace $f(x)$ by a new
variable $t$ and the constraint $\{f(x)\leq t\}$ (which is convex
due to convexity of $f$), resulting in the following sampling problem:
sample $(x,t)$ from a distributions whose density is proportional
to $e^{-t}$ subject to $x\in K$ and $\{(x,t)\in\R^{n+1}:f(x)\leq t\}$.
We note that this is indeed an equivalent sampling problem, since
the $x$-marginal of this distribution follows $e^{-f(x)}\cdot\mathbf{1}_{K}(x)$:
\[
\int_{\{(x,t)\in\R^{n+1}:f(x)\leq t\}}e^{-t}\cdot\mathbf{1}_{K}(x)dt=\int_{f(x)}^{\infty}e^{-t}\cdot\mathbf{1}_{K}(x)dt=e^{-f(x)}\cdot\mathbf{1}_{K}(x).
\]
Suppose we have a barrier $\phi$ for $K\cap\{f(x)\leq t\}$. Thus,
this motivates our focus on sampling from distributions of the form
$e^{-c^{\top}y}$ subject to a convex region $K$ with a barrier $\phi$
for $K$, where $y:=(x,t)\in\R^{n+1}$ is the extended variable and
$c\in\R^{n+1}$ is a vector.

Regularizing the potential $c^{\top}y$ of the distribution by adding
$\frac{1}{\sigma^{2}}\phi(y)$ for some $\sigma^{2}>0$, we can ignore
the hard constraint $K$ and obtain the following formulation: for
$f_{\sigma^{2}}(y)=c^{\top}y+\frac{1}{\sigma^{2}}\phi(y)$
\[
\text{sample }y\sim\frac{d\mu_{\sigma^{2}}}{dy}\propto e^{-f_{\sigma^{2}}(y)}=\exp\Par{-\Par{c^{\top}y+\frac{1}{\sigma^{2}}\phi(y)}},
\]
where $\phi(y)$ goes to infinity as it approaches the boundary of
$K$. For each fixed $\sigma^{2}>0$, as $\sigma^{2}\to\infty$ the
regularization term $\frac{1}{\sigma^{2}}\phi$ vanishes, and we can
expect $\mu_{\sigma^{2}}\to\pi$ for $\frac{d\pi}{dy}\propto e^{-c^{\top}y}\cdot\mathbf{1}_{K}(y)$.
Comparing this with the optimization IPM, the path of measures $\{\mu_{\sigma^{2}}\}_{\sigma^{2}>0}$
can be viewed as the central path in the space of measures. In an
ideal scenario, a sampling IPM should closely follow this central
path while increasing $\sigma^{2}$ along the path. To this end, we
update the current distribution $\bar{\mu}_{\sigma^{2}}$, which is
already close to $\mu_{\sigma^{2}}$ on the central path. This update
should leverage a \emph{sampling step} that is aware of the local
geometry induced by $\hess\phi$, which may involve running a non-Euclidean
sampler such as the $\dw$. This update brings $\bar{\mu}_{\sigma^{2}}$
to a new distribution $\bar{\mu}_{\sigma^{2}+\delta}$ that should
be close to $\mu_{\sigma^{2}+\delta}$ for small $\delta>0$, while
$\bar{\mu}_{\sigma^{2}}$ serves a good starting point for this sampling
step to find $\bar{\mu}_{\sigma^{2}+\delta}$. This procedure is repeated
until $\sigma^{2}$ becomes large enough.

To use this sampling IPM, we further refine the framework via \emph{Gaussian
cooling on manifolds}.

\paragraph{Comparison with the Gaussian cooling on manifolds (GCM).}

Gaussian Cooling introduced in \cite{cousins2018gaussian} was extended
to manifolds (GCM) by \cite{lee2018convergence}. It was initially
proposed for volume computation but shares remarkable similarities
with our sampling IPM. In fact, GCM can be recovered as a special
case of the sampling IPM with $c=0$ (i.e., uniform sampling) and
the Riemannian Hamiltonian Monte Carlo employed for the non-Euclidean
sampling step.

Returning to the comparison with the optimization IPM, we note that
two algorithms use different rules for updating $\sigma^{2}$. While
the optimization IPM updates $\sigma^{2}\gets\Par{1+\frac{1}{\sqrt{\nu}}}\sigma^{2}$,
GCM utilizes two distinct annealing schemes: 
\[
\sigma^{2}\gets\begin{cases}
\Par{1+\frac{1}{\sqrt{n}}}\sigma^{2} & \text{if }\sigma^{2}\leq\frac{\nu}{n}\\
\Par{1+\frac{\sigma}{\sqrt{\nu}}}\sigma^{2} & \text{o.w.}
\end{cases}
\]
While the first type of update in the small regime of $\sigma^{2}$
is inspired by a property of logconcavity of regularized distributions,
the second type of update in the large regime of $\sigma^{2}$ is
justified by concentration of measure $\exp\Par{-s\phi}$ in a thin
shell for $s>0$. We note that the second type in fact accelerates
the annealing process.

% Figure environment removed

However, significant challenges remain for the sampling IPM. First,
we need to extend this annealing scheme to exponential distributions
(recall that GCM was proposed for uniform sampling). To be precise,
we must account for the linear term $c^{\top}y$ (in addition to the
$\phi$ term) when designing the annealing scheme. Unfortunately,
the previous update scheme (which is applied only to $\phi$ part)
and analysis do not go through for this purpose. 

To address this issue, we introduce a further generalization of the
GCM annealing scheme in the small regime of $\sigma^{2}$, enabling
us to leverage the property of logconcavity of regularized distributions.
In the large regime of $\sigma^{2}$, we maintain the same annealing
scheme but employ a different analytical approach, utilizing a functional
inequality that avoids the need to quantify the thin-shell phenomenon
of $\exp\Par{-\Par{c^{\top}y+s\phi(y)}}$. 

To discuss another remaining issue, we note that a non-Euclidean sampler
used in the sampling step must have a provable mixing-time guarantee
for a target distribution proportional to $\exp\Par{-\Par{c^{\top}y+s\phi(y)}}$.
We already provided this through Theorem~\ref{thm:Dikin} in Section~\ref{sec:mixing-Dikin}
for the $\dw$, since the target potential is $s$-relatively strongly
convex and $s$-relatively smooth in $\phi$!

\subsection{IPM algorithm for sampling}

Our algorithm consists of four phases, where each phase updates a
current distribution in a different way. For generality, we present
this annealing process for a general potential $f$ instead of linear
functions, where $\alpha\hess\phi\preceq\hess f\preceq\beta\hess\phi$.

\begin{algorithm2e}[H]

\caption{Interior-Point Method for sampling} \label{alg:IPM-sampling}

\SetAlgoLined

\textbf{Input:} Target accuracy $\veps$, local metric $g$, its counterpart
$\phi$, non-Euclidean sampler $\textsf{NE-Sampler}(g,\veps)$, target
distribution $\frac{d\pi}{dy}\propto\exp(-f(y))$.

\textbf{Output:} $x'$

For $\bar{f}(x):=\frac{\nu}{n}f(x)$, let $\mu_{\sigma^{2}}$ be a
probability distribution such that $\frac{d\mu_{\sigma^{2}}}{dx}\propto\exp\Par{-V_{\sigma^{2}}(x)}$,
where
\[
V_{\sigma^{2}}(y):=\begin{cases}
\frac{\bar{f}(y)+\phi(y)}{\sigma^{2}} & \text{if }\sigma^{2}\leq\frac{\nu}{n}\\
f(y)+\frac{1}{\sigma^{2}}\phi(y) & \text{o.w}.
\end{cases}
\]

\tcp{Phase 1: Initial distribution}

Find $x^{*}=\arg\min_{x\in K}(\bar{f}+\phi)$ and let $D:=\dcal_{g}^{3\sigma_{0}\sqrt{n}}(x^{*})$
for $\sigma_{0}^{2}:=10^{-5}/n^{3}$.\label{line:min}

Draw $x_{0}\sim\textsf{NE-Sampler}\Par{g,\frac{\veps}{\sqrt{n}}}$
with initial dist. $\ncal\Par{x^{*},\frac{\sigma_{0}^{2}}{1+\nu\beta/n}g(x^{*})^{-1}}\cdot\mathbf{1}_{D}$
and target dist. $\mu_{\sigma_{0}^{2}}$.

\tcp{Phase 2 \& 3: Annealing until $\sigma^{2}\leq\nu$}

\While{$\sigma^{2}\leq\nu$}{

Update $\sigma^{2}$ by 
\[
\sigma^{2}\gets\begin{cases}
\sigma^{2}\Par{1+\frac{1}{\sqrt{n}}} & \text{if }\sigma^{2}\leq\frac{\nu}{n}\text{ (Phase 2)}\\
\sigma^{2}\Par{1+\frac{\sigma}{\sqrt{\nu}}} & \text{if }\frac{\nu}{n}\leq\sigma^{2}\leq\nu\text{ (Phase 3)},
\end{cases}
\]

Draw $x_{i+1}\sim\textsf{NE-Sampler}\Par{g,\frac{\veps}{\sqrt{n}}}$
started at $x_{i}$ with target dist. $\mu_{\sigma^{2}}$, and increment
$i$.

}

\tcp{Phase 4: Sampling from $e^{-f}$} 

Draw $x'\sim\textsf{NE-Sampler}\Par{g,\frac{\veps}{\sqrt{n}}}$ started
at $x_{i}$ with target dist. $\pi$.

\end{algorithm2e}

\subsubsection{Well-definedness}

Going forward, we use the following notation: for $\bar{f}(x):=\frac{\nu}{n}f(x)$
\begin{align*}
F(\sigma^{2}) & :=\begin{cases}
\int_{K}\exp\Par{-\frac{\bar{f}(x)+\phi(x)}{\sigma^{2}}}dx & \text{if }\sigma^{2}\leq\frac{\nu}{n},\\
\int_{K}\exp\Par{-f(x)-\frac{\phi(x)}{\sigma^{2}}}dx & \text{if }\frac{\nu}{n}\leq\sigma^{2}\leq\nu.
\end{cases}
\end{align*}
We show that $y^{*}=\arg\min_{y\in K}(\bar{f}+\phi)$ exists in Line~\ref{line:min}
of Algorithm~\ref{alg:IPM-sampling} and that all distributions involved
in the algorithm are indeed integrable. We defer the proof to Section~\ref{proof:IPM-welldefined}.
%
\begin{prop}
\label{prop:annealing-welldefined} Each probability density involved
in the algorithm is integrable.
\end{prop}

 

\subsubsection{Closeness of distributions in sampling IPM}

In this section, we demonstrate that within each phase a probability
distribution $\mu_{\sigma_{i}^{2}}$ serves as a good warm start for
sampling the subsequent distribution $\mu_{\sigma_{i+1}^{2}}$. While
Algorithm~\ref{alg:IPM-sampling} uses as an initial distribution
$\bar{\mu}_{\sigma^{2}}$ that is approximately close to $\mu_{\sigma^{2}}$,
we resolve this discrepancy through a coupling argument referred to
as ``divine intervention''. We refer readers to Remark~\ref{rem:divine-intervention}
and to Proof of Lemma~4.2 in \cite{lovasz2006simulated} for fuller
details.

For the first two phases, closeness of consecutive distributions follow
purely from a property of log-concave distributions, which is independent
of local metrics.
\begin{lem}
[\cite{kalai2006simulated}, Lemma 3.2] \label{lem:adam-logconcave}
For a log-concave function $g$, the function $a\mapsto a^{n}\int g(x)^{a}dx$
is log-concave in $a$.
\end{lem}

In Phase 1, we leverage another fundamental property of log-concave
distributions. It allows us to establish that the Gaussian distribution
truncated over a small Dikin ellipsoid in Phase 1 provides an $O\Par{\Par{\frac{\nu\beta+n}{\nu\alpha+n}}^{n}}$-warm
start for $\mu_{\sigma_{0}^{2}}$. Thus, the $\dw$ which has a log-dependency
on the warmness parameter introduces an additional factor of $n$. 
\begin{lem}
[\cite{lovasz2007geometry}, Lemma 5.16] \label{lem:mostMass-logconcave}
Let $X$ be a random point drawn from a log-concave distribution with
a density function $f:\Rn\to\R$. If $\gamma\geq2$, then 
\[
\P\Par{f(X)\leq e^{-\gamma(n-1)}\max f}\leq\Par{e^{1-\gamma}\gamma}^{n-1}.
\]
\end{lem}

\begin{rem}
If we can show that the $\dw$ has a $\log\log$-dependency through
the \emph{blocking conductance} or \emph{Gaussian isoperimetry}, or
if we utilize a non-Euclidean sampler with a double-log dependency,
we can avoid the additional factor of $n$.
\end{rem}

We defer the proofs for closeness to Section~\ref{proof:IPM-closeness}.
\begin{lem}
[Phase 1] \label{lem:phase1} Let $x^{*}=\arg\min_{x\in K}\Par{\bar{f}+\phi}$.
For $\sigma^{2}=10^{-5}/n^{3}$ and $g=\hess\phi$, let $\mu$ be
the Gaussian distribution $\ncal\Par{x^{*},\frac{\sigma^{2}}{1+\nu\beta/n}g(x^{*})^{-1}}$
truncated over $\dcal_{g}^{3\sigma\sqrt{n}}(x^{*})$ and $\mu_{0}$
the initial distribution used in Phase 2 such that $\frac{d\mu_{0}}{dx}\propto\exp\Par{-\frac{\bar{f}(x)+\phi(x)}{\sigma^{2}}}\cdot\mathbf{1}_{K}(x)$.
Then $\norm{\mu/\mu_{0}}\lesssim\Par{\frac{\nu\beta+n}{\nu\alpha+n}}^{n}$.
\end{lem}

In the following lemmas, we show that within each phase of our algorithm
$\mu_{\sigma_{i}^{2}}$ serves as an $O(1)$-warm start for the following
distribution $\mu_{\sigma_{i+1}^{2}}$. In Phase 2, for $1/n^{3}\lesssim\sigma^{2}\leq\nu/n$
the multiplicative update of $(1+1/\sqrt{n})$ allows us to achieve
an $O(1)$-warm start.
\begin{lem}
[Phase 2] \label{lem:phase2} In Phase 2 (i.e., $\sigma_{i}^{2}\leq\nu/n$
with the update $\sigma_{i+1}^{2}=\sigma_{i}^{2}(1+1/\sqrt{n})$),
a previous distribution $\mu_{i}$ serves as an $O(1)$-warm start
for the next distribution $\mu_{i+1}$, i.e., $\norm{\mu_{i}/\mu_{i+1}}=O(1)$.
\end{lem}

In the large regime of $\nu/n\leq\sigma^{2}\leq\nu$ during Phase
3, we leverage the Brascamp-Lieb inequality to show that the accelerated
update of $(1+\sigma/\sqrt{\nu})$ ensures an $O(1)$-warm start.
Moreover, we employ the same technique along with a limiting argument
to show that in Phase 4 the final distribution of $\mu_{\nu}$ is
an $O(1)$-warm start for the target distribution $\pi$.
\begin{lem}
[Phase 3 and 4] \label{lem:phase34} In Phase 3 (i.e., $\nu/n\leq\sigma_{i}^{2}\leq\nu$
with the update $\sigma_{i+1}^{2}=\sigma_{i}^{2}(1+\sigma_{i}/\sqrt{\nu})$,
a previous distribution $\mu_{i}$ serves as an $O(1)$-warm start
for the next distribution $\mu_{i+1}$, i.e., $\norm{\mu_{i}/\mu_{i+1}}=O(1)$.
In Phase 4, the distribution $\mu$ with $\frac{d\mu}{dx}\propto\exp\Par{-\Par{f(x)+\frac{\phi(x)}{\nu}}}\cdot\mathbf{1}_{K}(x)$
is an $O(1)$-warm start for the target distribution $\pi$ with $\frac{d\pi}{dx}\propto\exp\Par{-f(x)}\cdot\mathbf{1}_{K}(x)$.
\end{lem}


\subsubsection{Proof of Theorem \ref{thm:Dikin-annealing}}

We now provide the proof of Theorem~\ref{thm:Dikin-annealing}, Algorithm~\ref{alg:IPM-sampling}
with the $\dw$ employed for the non-Euclidean sampler. 

\thmDikinannealing*
\begin{proof}
By Theorem~\ref{thm:Dikin}, if the potential $V$ of a target distribution
satisfies $\alpha\hess\phi\preceq\hess V\preceq\beta\hess\phi$, the
mixing time of the $\dw$ is $n(1+\beta)\min(\onu,1/\alpha)\log\frac{\Lambda}{\veps}$.
\begin{itemize}
\item Phase 1: When a target distribution is $\exp\Par{-\frac{\bar{f}+\phi}{\sigma^{2}}}$
with $\sigma^{2}=10^{-5}/n^{3}$ 
\begin{align*}
n^{2}\Par{1+\frac{\frac{\nu}{n}\beta+1}{\sigma^{2}}}\min\Par{\onu,\frac{\sigma^{2}}{\frac{\nu\alpha}{n}+1}}\log\Par{\frac{\nu\beta+n}{\nu\alpha+n}} & \leq n^{2}\frac{\nu\beta+n}{\nu\alpha+n}\log\Par{\frac{\nu\beta+n}{\nu\alpha+n}}\\
 & \leq n^{2}(\kappa+1)\log\kappa.
\end{align*}
\item Phase 2 ($1/n^{3}\lesssim\sigma^{2}\leq\nu/n$): note that we need
$O^{*}(\sqrt{n})$-many iterations to double $\sigma^{2}$. Hence,
in this phase the number of iterations of the $\dw$ with a target
$\exp\Par{-\frac{\bar{f}+\phi}{\sigma^{2}}}$ adds up to 
\begin{align*}
n\Par{1+\frac{\frac{\nu}{n}\beta+1}{\sigma^{2}}}\min\Par{\onu,\frac{\sigma^{2}}{\frac{\nu\alpha}{n}+1}}\cdot\sqrt{n} & \leq n^{1.5}\frac{\nu\beta+n}{\nu\alpha+n}+\sqrt{n}\nu\\
 & \leq n^{1.5}(\kappa+1)+\sqrt{n}\nu.
\end{align*}
\item Phase 3 ($\nu/n\leq\sigma^{2}\leq\nu$): we need $O^{*}\Par{1+\frac{\sqrt{\nu}}{\sigma}}$-many
iterations to double $\sigma^{2}$. Hence, in this phase the total
number of iterations of the $\dw$ with a target $\exp\Par{-\Par{f+\frac{\phi}{\sigma^{2}}}}$
is
\begin{align*}
n\Par{1+\beta+\frac{1}{\sigma^{2}}}\min\Par{\onu,\frac{1}{\alpha+\sigma^{-2}}}\cdot\Par{1+\frac{\sqrt{\nu}}{\sigma}} & \leq n(\kappa+\nu).
\end{align*}
\item Phase 4: the $\dw$ takes $O(n\onu)$ iterations.
\end{itemize}
Adding up all iterations in phases, we need $\otilde{n\max\Par{n(\kappa+1),\nu,\onu}}$
iterations of the $\dw$ in total.
\end{proof}

