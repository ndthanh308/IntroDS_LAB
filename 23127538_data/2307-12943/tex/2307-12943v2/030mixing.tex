
\section{Mixing of the $\protect\dw$ \label{sec:mixingDikin}}

We prove Theorem~\ref{thm:generalMixing}, carrying out the convergence
analysis of the $\dw$ under weaker assumptions on the local metric
compared to those in \cite{laddha2020strong}. Their framework requires
the convexity of $\log\det g(x)$, i.e., $\hess\log\det g(x)[h,h]\succeq0$
for any $h\in\Rn$. By direct computation using matrix calculus (see
(\ref{eq:hessLogDet})), this condition corresponds to
\[
\tr\Par{g(x)^{-\half}D^{2}g(x)[h,h]g(x)^{-\half}}\geq\norm{g(x)^{-\half}Dg(x)[h,h]g(x)^{-\half}}_{F}^{2}.
\]
The main purpose of this condition is to ensure that the acceptance
probability of each Dikin step (see the last line of Algorithm~\ref{thm:generalMixing})
is bounded below by a constant. We show that this condition can be
relaxed to what we refer to as \emph{lower trace self-concordance}
(see Definition~\ref{def:sc}):
\[
\tr\Par{g(x)^{-\half}D^{2}g(x)[h,h]g(x)^{-\half}}\geq-\norm h_{g(x)}^{2},
\]
which is generally much weaker than the convexity of $\log\det g(x)$.

\paragraph{Proof outline.}

We follow a standard conductance based argument (see e.g., \cite{lovasz1993random,vempala2005geometric}).
A lower bound on the conductance of a Markov chain provides an upper
bound on the mixing time of the Markov chain due to the following
result.
\begin{lem}
[\cite{lovasz1993random}] \label{lem:conductanceBound}Let $\pi_{T}$
be the distribution obtained after $T$ steps of a lazy reversible
Markov chain of conductance at least $\Phi$ with stationary distribution
$\pi$ and initial distribution $\pi_{0}$. For the warmness parameter
$\Lambda=\sup_{\text{\textup{measurable} }S}\frac{\pi_{0}(S)}{\pi(S)}$
of the initial distribution,
\[
\dtv\Par{\pi_{T},\pi}\leq\sqrt{\Lambda}\Par{1-\frac{\Phi^{2}}{2}}^{T}.
\]
\end{lem}

A lower bound on the conductance follows from two ingredients: \textbf{(i)}
one-step coupling and \textbf{(ii)} isoperimetry. The first refers
to showing that the one-step distributions of the $\dw$ from two
nearby points have TV distance bounded away from one. The second is
a purely geometry property about the expansion of the target distribution. 
\begin{prop}
[\cite{kook2022condition}, Adapted from Proposition 9] \label{prop:conductance}Let
$\pi$ be the stationary distribution of a lazy reversible Markov
chain on $\mcal$ with a transition kernel $P_{x}$. Assume the isoperimetry
$\psi_{\mcal}$ under distance $d(x,y)=\norm{x-y}_{g(x)}$ and the
following one-step coupling: if $\norm{x-y}_{g(x)}\leq\Delta<1$ for
$x,y\in\mcal$, then $\dtv\Par{P_{x},P_{y}}\leq0.9$. Then the conductance
$\Phi$ of the Markov chain is bounded lower by $\Omega\Par{\psi_{\mcal}\Delta}$.
\end{prop}

We begin with one-step coupling.
\begin{lem}
[One-step coupling of $\dw$] \label{lem:one-step} For a convex
body $K\subset\Rn$, let $g\in\R^{n\times n}$ be strongly and lower
trace self-concordant on $K$. For step size $r=\frac{1}{2^{12}}$,
if $\norm{x-y}_{g(x)}\leq\frac{r}{\sqrt{n}}$ for $x,y\in K$, then
$\dtv(P_{x},P_{y})\leq3/4$ for the transition kernel $P_{x}$ of
the $\dw$.
\end{lem}

When bounding the overlap of the transition kernels of two close-by
points, there are two sources of deviation --- rejection probability
and overlap of two Dikin ellipsoids. Analyzing the former involves
both strong self-concordance and lower trace self-concordance of the
metric along with the concentration of measure, while the latter only
uses strong self-concordance.
\begin{proof}
Let $q_{x}(z)$ be the probability density of the uniform distribution
$U_{g}^{r}(x)$ over $\dcal_{g}^{r}(x)$. Let $A(x,z)=\min\Par{1,\frac{q_{z}(x)}{q_{x}(z)}}=\min\Par{1,\sqrt{\frac{\det g(z)}{\det g(x)}}}$.
We can write the transition kernel at $x$ as
\[
P_{x}(dz)=\underbrace{\Par{1-\int A(x,z)q_{x}(z)dz}}_{=:r_{x}}\cdot\delta_{x}(dz)+A(x,z)q_{x}(z)dz,
\]
where $\delta_{x}$ is the point mass at $x$. Let $r_{x}=1-\int A(x,z)q_{x}(dz)$
be the rejection probability at $x$. By the triangle inequality,
\begin{align}
\dtv(P_{x},P_{y}) & \leq\dtv\Par{P_{x},U_{g}^{r}(x)}+\dtv\Par{U_{g}^{r}(x),U_{g}^{r}(y)}+\dtv\Par{U_{g}^{r}(y),P_{y}}\nonumber \\
 & =r_{x}+\dtv\Par{U_{g}^{r}(x),U_{g}^{r}(y)}+r_{y}\label{eq:TVfinal}
\end{align}
Going forward, we drop the superscript of the radius $r$ to remove
clutters. We first bound the rejection probabilities $r_{x},r_{y}$
(Part I) and then $\dtv\Par{U_{g}^{r}(x),U_{g}^{r}(y)}$ (Part II).

\paragraph{Part I: $r_{x}$ and $r_{y}$.}

Here we use strong and lower trace self-concordance. Suppose $z$
is drawn from $U_{g}(x)$ at random. Note that the rejection probability
when moving from $x$ to $z$ can be written as 
\[
1-\min\Par{1,\frac{1/\vol\Par{\dcal_{g}(z)}}{1/\vol\Par{\dcal_{g}(x)}}}=1-\min\Par{1,\sqrt{\frac{\det\Par{g(z)}}{\det\Par{g(x)}}}}.
\]
For $\varphi(z):=\log\det g(z)$, it suffices to show that $\vphi(z)-\varphi(x)$
can be made larger than $-\veps$ for any given $\veps>0$. By Taylor's
theorem, there exists $x^{*}\in[x,z]$ such that
\[
\vphi(z)=\vphi(x)+\underbrace{\inner{\grad\vphi(x),z-x}}_{=:\text{I}}+\half\underbrace{D^{2}\vphi(x^{*})\Brack{z-x,z-x}}_{=:\text{II}}.
\]
For I, note that $z'=g(x)^{\half}(z-x)$ follows the uniform distribution
over the ball of radius $r$ centered at $0$. Using a standard concentration
inequality in high-dimensional space (see Theorem~2.7 in \cite{blum2020foundations}\footnote{For $c\ge1$, $n\geq3$, and a fixed vector $v\in\Rn$,
\[
\P_{x\sim\mathrm{Unif}(B_{r}(0))}\Par{x\in\Rn:v^{\top}x\geq-\frac{c}{\sqrt{n-1}}r\norm v_{2}}\geq1-\frac{2}{c}e^{-c^{2}/2}.
\]
}), with probability at least $0.99$ the following holds:
\begin{align}
\inner{\grad\vphi(x),z-x} & =\inner{g(x)^{-\half}\grad\vphi(x),g(x)^{\half}(z-x)}\geq-\frac{4r}{\sqrt{n}}\norm{g(x)^{-\half}\grad\vphi(x)}_{2}.\label{eq:eq1}
\end{align}
We can further bound $\norm{g(x)^{-\half}\grad\vphi(x)}_{2}$ via
strong self-concordance of $g$ as follows:
\begin{align}
\norm{g(x)^{-\half}\grad\vphi(x)}_{2} & =\sup_{v:\norm v_{2}=1}\grad\vphi(x)^{\top}g(x)^{-\half}v\nonumber \\
 & \underset{\text{(i)}}{=}\sup_{v:\norm v_{2}=1}\tr\Par{g(x)^{-1}Dg(x)\Brack{g(x)^{-\half}v}}\nonumber \\
 & =\sup_{v:\norm v_{2}=1}\tr\Par{g(x)^{-\half}Dg(x)\Brack{g(x)^{-\half}v}g(x)^{-\half}}\nonumber \\
 & \underset{\text{(ii)}}{\leq}\sup_{v:\norm v_{2}=1}\sqrt{n}\norm{g(x)^{-\half}Dg(x)\Brack{g(x)^{-\half}v}g(x)^{-\half}}_{F}\nonumber \\
 & \underset{\text{(iii)}}{\leq}\sup_{v:\norm v_{2}=1}2\sqrt{n}\norm{g(x)^{-\half}v}_{g(x)}=2\sqrt{n},\label{eq:eq2}
\end{align}
where (i) follows from (\ref{eq:gradLogDet}), (ii) is due to $\tr(A)\leq\sqrt{n}\norm A_{F}$
for $A\in\Rnn$ in general, and (iii) is simply the definition of
strong self-concordance. Putting (\ref{eq:eq1}) and (\ref{eq:eq2})
together, we have
\[
\inner{\grad\vphi(x),z-x}\geq-\frac{4r}{\sqrt{n}}2\sqrt{n}=-8r
\]
and thus $\inner{\grad\vphi(x),z-x}\geq-1/512$ with probability at
least $0.99$ due to $r=\frac{1}{2^{12}}$. 

For II, with $h=z-x$ we have 
\begin{align*}
D^{2}\vphi(x^{*})[h,h] & \underset{\eqref{eq:hessLogDet}}{=}\tr\Par{g(x^{*})^{-1}D^{2}g(x^{*})[h,h]}-\norm{g(x^{*})^{-\half}Dg(x^{*})[h]g(x^{*})^{-\half}}_{F}^{2}\\
 & \underset{\text{(i)}}{\geq}-\norm h_{g(x^{*})}^{2}-\norm{g(x^{*})^{-\half}Dg(x^{*})[h]g(x^{*})^{-\half}}_{F}^{2}\\
 & \underset{\text{(ii)}}{\geq}-\norm h_{g(x^{*})}^{2}-4\norm h_{g(x^{*})}^{2}\\
 & \underset{\text{(iii)}}{\geq}-\frac{5}{\Par{1-\norm{x-x^{*}}_{g(x)}}^{2}}\norm h_{g(x)}^{2},
\end{align*}
where (i) follows from lower trace self-concordance, (ii) is strong
self-concordance, and (iii) follows from Lemma~\ref{lem:scCloseness}.
Hence, $D^{2}\vphi(x^{*})[z-x,z-x]\geq-6r^{2}\geq-1/512$. Putting
the bounds on I and II together, with probability $0.99$ we have
$\vphi(z)-\vphi(x)\geq-1/256$ and thus 
\begin{equation}
r_{x}\leq0.99\cdot(1-\exp(-1/256))+0.01\leq0.014,\label{eq:rxbound}
\end{equation}
and similarly $r_{y}\leq0.014$.

\paragraph{Part II: $\protect\dtv\protect\Par{U_{g}^{r}(x),U_{g}^{r}(y)}$.}

This part is analogous to \cite{laddha2020strong}, using strong self-concordance
of the metric. In this part, in order to make clear the local metric,
let $E(A,x):=\{y\in\Rn:\sqrt{(y-x)^{\top}A(y-x)}\leq r\}$ and $U_{E}^{r}(x)$
the uniform distribution over $E(A,x)$. By the triangle inequality
\[
\dtv\Par{U_{g}^{r}(x),U_{g}^{r}(y)}\leq\underbrace{\dtv\Par{U_{g(x)}^{r}(x),U_{g(x)}^{r}(y)}}_{\text{(I)}}+\underbrace{\dtv\Par{U_{g(x)}^{r}(y),U_{g(y)}^{r}(y)}}_{\text{(II)}}.
\]
(I) is invariant under the transformation $z\mapsto g(x)^{1/2}z$,
and thus (I) is equal to $\dtv(B_{r}(x),B_{r}(y))$. Since the condition
of $\norm{x-y}_{g(x)}\leq\frac{r}{\sqrt{n}}$ turns into $\norm{x-y}_{2}\leq\frac{r}{\sqrt{n}}$
under the transformation, Lemma~3.2 in \cite{kannan1997random} implies
that $\dtv(B_{r}(x),B_{r}(y))\leq\frac{e}{e+1}$.

For (II), WLOG assume $\vol\Par{E(g(y),y)}\geq\vol\Par{E(g(x),y)}$.
In the following equation, let us use $D(\cdot)$ to denote the Dikin
ellipsoid $E(g(\cdot),y)$ for simplicity (i.e., omit the center $y$):

\begin{align}
\text{II} & =\half\int_{\Rn}\Abs{\frac{\bm{1}_{D(y)}(z)}{\vol\Par{D(y)}}-\frac{\bm{1}_{D(x)}(z)}{\vol\Par{D(x)}}}dz\nonumber \\
 & =\half\Par{\frac{\vol\Par{D(y)\backslash D(x)}}{\vol\Par{D(y)}}+\frac{\vol\Par{D(x)\backslash D(y)}}{\vol\Par{D(x)}}+\frac{\vol\Par{D(y)}-\vol\Par{D(x)}}{\vol\Par{D(y)}\vol\Par{D(x)}}\cdot\vol\Par{D(x)\cap D(y)}}\nonumber \\
 & =\half\Par{1-\frac{\vol\Par{D(x)\cap D(y)}}{\vol\Par{D(y)}}+1-\frac{\vol\Par{D(x)\cap D(y)}}{\vol\Par{D(x)}}+\frac{\vol\Par{D(x)\cap D(y)}}{\vol\Par{D(x)}}-\frac{\vol\Par{D(x)\cap D(y)}}{\vol\Par{D(y)}}}\nonumber \\
 & =1-\frac{\vol\Par{D(x)\cap D(y)}}{\vol\Par{D(y)}}.\label{eq:eq3}
\end{align}
Since the ratio of volumes is invariant under affine transformation,
we may assume that $y=0$ and $g(y)=I$. Let $g(x)^{-1}=U^{\top}\Diag(\lda)U$
be a spectral decomposition of $g(x)^{-1}$, where $\{\lda_{i}\}_{i=1}^{n}$
is the set of eigenvalues of $g(x)^{-1}$ and $\lda:=(\lda_{i})\in\Rn$.
Consider a matrix $C\in\R^{n\times n}$ such that $C^{-1}=U^{\top}\Diag(\min(1,\lda))U$,
and by construction $D(C)\subset D(g(x))\cap D(I)$. Thus,
\begin{align}
\frac{\vol\Par{D(x)\cap D(y)}}{\vol\Par{D(y)}} & =\frac{\vol\Par{D(x)\cap I}}{\vol\Par I}\nonumber \\
 & \geq\sqrt{\prod_{i:\lda_{i}\leq1}\lda_{i}}=\sqrt{\prod_{i:\lda_{i}\leq1}(1-(1-\lda_{i}))}\nonumber \\
 & \geq\sqrt{\exp\Par{-2\sum_{i:\lda_{i}\leq1}(1-\lda_{i})}},\label{eq:eq4}
\end{align}
where the last inequality follows from $1-x\geq\exp(-2x)$ for $0\leq x\leq\half$
and the fact that $\Par{1-\norm{y-x}_{g(x)}}^{2}I\preceq g(x)^{-1}\preceq\Par{1-\norm{y-x}_{g(x)}}^{-2}I$
in Lemma~\ref{lem:scCloseness} guarantees $\lda_{i}\geq\half$.
We further note that 
\begin{align}
\sum_{\lda_{i}<1}(1-\lda_{i}) & \leq\sqrt{n}\sqrt{\sum_{i}(1-\lda_{i})^{2}}=\sqrt{n}\norm{I-g(x)^{-1}}_{F}\nonumber \\
 & \underset{\text{(i)}}{\leq}\sqrt{n}\norm{x-y}_{g(x)}\leq\frac{1}{256},\label{eq:eq5}
\end{align}
where (i) follows from Lemma~\ref{lem:strongSC-closeness}.

Combining (\ref{eq:eq3}), (\ref{eq:eq4}), and (\ref{eq:eq5}) together,
$(\text{II})\leq1-e^{-\frac{1}{512}}$, and along with the bound on
I we obtain 
\[
\dtv\Par{U_{g}^{r}(x),U_{g}^{r}(y)}\leq\frac{e}{e+1}+1-e^{-\frac{1}{512}}.
\]
Putting this and (\ref{eq:rxbound}) back to (\ref{eq:TVfinal}),
we conclude that
\begin{align*}
\dtv(P_{x},P_{y}) & \leq\half\Par{r_{x}+r_{y}}+\dtv\Par{U_{g}^{r}(x),U_{g}^{r}(y)}\\
 & \leq0.028+\frac{e}{e+1}+1-e^{-\frac{1}{512}}\\
 & \leq3/4.\qedhere
\end{align*}
\end{proof}
Next, the isoperimetry of the uniform distribution under the local
metric $\norm{x-y}_{g(x)}$ is guaranteed to be $\Omega\Par{1/\sqrt{\onu}}$,
a fact that is used in \cite{laddha2020strong}. We state it formally
below with a proof. 
\begin{lem}
\label{lem:isoperimetry} For a convex body $K$, the isoperimetry
$\psi$ of the uniform distribution over $K$ under distance $\norm{x-y}_{g(x)}$
is at least $\Omega\Par{1/\sqrt{\onu}}$.
\end{lem}

Recall that the \emph{cross-ratio distance} $d_{K}$ defined on a
convex body $K$: for $x,y\in K$, suppose that the chord passing
through $x,y$ has endpoints $p$ and $q$ in the boundary $\del K$
(so the order of points is $p,x,y,q$), then the cross-ratio distance
between $x$ and $y$ is defined by 
\[
d_{K}(x,y)\defeq\frac{\norm{x-y}_{2}\norm{p-q}_{2}}{\norm{p-x}_{2}\norm{y-q}_{2}}.
\]

\begin{proof}
By Theorem~1 in \cite{lovasz1999hit}, we have that for any partition
$\{S_{1},S_{2},S_{3}\}$ of $K$ 
\[
\textrm{\ensuremath{\vol}}(S_{3})\vol(K)\geq d_{K}(S_{1},S_{2})\vol(S_{1})\vol(S_{2}),
\]
where $d_{K}(S_{1},S_{2})=\inf_{x\in S_{1},y\in S_{2}}d_{K}(x,y)$.
By Lemma~2.3 in \cite{laddha2020strong} ($d_{K}(x,y)\geq\norm{x-y}_{x}/\sqrt{\onu}$
for any $x,y\in K$), we have $\textrm{\ensuremath{\vol}}(S_{3})\vol(K)\geq\inf_{x\in S_{1},y\in S_{2}}\norm{x-y}_{g(x)}\vol(S_{1})\vol(S_{2})/\sqrt{\onu}$
and 
\[
\frac{\vol(S_{3})/\inf_{x\in S_{1},y\in S_{2}}\norm{x-y}_{g(x)}}{\vol(S_{1})\vol(S_{2})/\vol(K)}\geq\frac{1}{\sqrt{\onu}},
\]
leading to $\psi=\Omega\Par{1/\sqrt{\onu}}$.
\end{proof}

Putting the one-step coupling and the isoperimetry together, we can
bound the mixing time of the $\dw$.

\thmGeneralMixing*
\begin{proof}
Proposition~\ref{prop:conductance} ensures that $\Phi=\Omega\Par{1/\sqrt{n\onu}}$
due to the one-step coupling in Lemma~\ref{lem:one-step} and the
isoperimetry in Lemma~\ref{lem:isoperimetry}. Therefore, using Lemma~\ref{lem:conductanceBound},
we can enforce $\dtv(\pi_{T},\pi)\leq\veps$ by solving $\sqrt{\Lambda}e^{-T\Phi^{2}/2}\leq\veps$
for $T$, which results in
\[
T=O\Par{n\onu\log\frac{\Lambda}{\veps}}=\otilde{n\onu}.\qedhere
\]
\end{proof}

