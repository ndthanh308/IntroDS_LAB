\global\long\def\vec{\textup{\textsf{vec}}}%
\global\long\def\svec{\textup{\textsf{svec}}}%


\section{Examples \label{sec:examples}}

For given constraints and epigraphs, combining metrics for them (according
to the self-concordance theory for sampling developed in \S\ref{sec:sc-theory-rules})
and employing $\gcdw$ with the combined metric lead to a poly-time
mixing sampling algorithm. Compared to the state-of-the-art poly-time
mixing algorithm, the $\bw$, $\gcdw$ offers several advantages.
First, it does not require any preprocessing (e.g., rounding) due
to affine invariance. Also, it achieves faster mixing by leveraging
inherent geometric information in sampling problems.

The per-step complexity of $\dws$, however, is in general higher
than that of the $\bw$. The primary computational bottleneck lies
in computing the inverse of a local metric. Nevertheless, efficient
implementation of inverse maintenance can significantly reduce the
per-step complexity, improving the total complexity ($\#\,$iterations
needed for mixing times the per-step complexity).

In this section, we illustrate how our framework recovers theoretical
guarantees of previous work on $\dws$ for uniform sampling and extends
beyond uniform sampling. In particular, we show that $\gcdw$ is a
poly-time mixing algorithm capable of sampling uniform, exponential,
or Gaussian distributions on second-order cones or truncated PSD cones.
Additionally, we illustrate an efficient per-step implementation that
yields a faster total complexity when compared to general-purpose
samplers such as the $\bw$.

\subsection{Polytope sampling}

Consider a set of linear constraints given by $K=\{x\in\Rd:Ax\geq b\}$
with $A\in\R^{m\times d}$ and $b\in\R^{m}$. 

\paragraph{Uniform sampling.}

\citet{kannan2012random} first studied the $\dw$ for uniformly sampling
a polytope, where a local metric is set to be the Hessian of the logarithmic
barrier, $g=\hess\phi_{\textsf{log}}=A_{(\cdot)}^{\T}A_{(\cdot)}$.
They showed that the $\dw$ with the log-barrier mixes in $\mc O\bpar{md\log\frac{M}{\veps}}$
iterations with a warmness parameter $M$. An immediate consequence
of our work is that $\gcdw$ achieves the mixing time of $\otilde{md}$
\emph{without a warmness assumption}, as $\onu,\nu=m$ and $g$ is
SSC, LTSC, and ASC by Lemma~\ref{lem:log-barrier}.

\citet{chen2018fast} introduced the $\textsf{Vaidya walk}$ and the
$\textsf{Approximate John walk}$, which are essentially $\dws$ with
the Vaidya metric $\hess\phi_{\textsf{Vaidya}}$ and a version of
the Lewis-weight metric $\sqrt{d}\,\hess\phi_{\textsf{Lw}}$. Their
work showed that both walks achieves mixing times of $\mc O\bpar{\sqrt{m}d^{3/2}\log\frac{M}{\veps}}$
and $\mc O\bpar{d^{5/2}\log^{\mc O(1)}m\,\log\frac{M}{\veps}}$, respectively.
Building upon our analysis of the Vaidya metric and Lewis-weight metric
in Lemma~\ref{lem:vaidya} and \ref{lem:Lewis-weight}, we find that
$\gcdw$ with these metrics achieves the same mixing but without any
warmness assumption.

We note that for the same task the $\bw$ without a warm start requires
$\otilde{d^{3}}$ membership queries due to \citet{kannan1997random,jia2021reducing}.
Given that a membership query involves $\mc O(md)$ arithmetic operations,
the total complexity of the $\bw$ is $\otilde{md^{4}}$. In contrast,
the per-step of the $\dw$ with the log-barrier can be run in $\mc O(md^{\omega-1})$
operations through the fast matrix multiplication, so the total number
of arithmetic operations is $\otilde{m^{2}d^{\omega}}$. Thus, for
$m$ close to $d$ $\gcdw$ is provably faster than the $\bw$. When
an efficient inverse maintenance proposed in \citet{laddha2020strong}
is employed, the per-step complexity can be improved to $\mc O(d^{2}+\nnz(A))=\mc O(md)$.
In such cases $\gcdw$ is faster in a broader range of $m$. In particular,
if $A$ is as sparse as $\nnz(A)=\mc O(d^{2})$, then $\gcdw$ is
always faster than the $\bw$. Moreover, $\gcdw$ with the Lewis-weight
metric mixes in $\otilde{d^{2.5}}$ steps with the per-step complexity
of $\otilde{md^{\omega-1}}$, so it is always faster than the $\bw$
for any $m$.

\paragraph{Exponential and Gaussian sampling.}

The current mixing bound of the $\bw$ for general log-concave sampling
is $\otilde{d^{4}}$ due to \citet{lovasz2007geometry}. On the other
hand, the $\dw$ employed with any metric above for exponential sampling
converges in the same iterations as the $\dw$ for uniform sampling.
Since only difference between two sapling is the additional term of
$\exp\bpar{-(f(z)-f(x))}$ in the Metropolis filter, the fast implementation
techniques mentioned earlier can be applied to the context of exponential
sampling. As a result, for the exponential sampling each of the $\dws$
described above surpasses the $\bw$ by a larger margin.

For Gaussian sampling over a polytope, we first reduce it to the exponential
sampling as in \eqref{eq:reduced-problem}: for $y=(x,t)\in\R^{d+1}$
\begin{align*}
\text{sample } & y\sim\tilde{\pi}\propto\exp(-t)\\
\text{s.t. } & Ax\geq b,\ \half\snorm{x-\mu}_{\Sigma}^{2}\leq t\,.
\end{align*}
According to our theory, it is natural to use the metric given by
\[
g(x,t)=2\left[\begin{array}{cc}
\hess_{x}\phi_{\text{log}}(x)\\
 & 0
\end{array}\right]+2(d+1)\,\hess_{(x,t)}\phi_{\text{Gauss}}(x,t)\,,
\]
which is $\bpar{\mc O(m+d),\mc O(m+d)}$-Dikin-amenable due to Lemma~\ref{lem:Gaussian-potential}.
Thus, $\gcdw$ needs $\otilde{d(m+d)}$ iterations of the $\dw$.
We note that the log-barrier can be replaced by the Vaidya or Lewis-weight
metrics, and in such cases one can obtain provable guarantees on the
mixing time by computing $\nu$ and $\onu$, referring to \S\ref{sec:handbook-barrier}
or Table~\ref{tab:scaling-table}.

\subsection{Second-order cone sampling}

We consider a region given by $\snorm{x-\mu}_{\Sigma}\leq t$ and
$A\left[\begin{array}{cc}
x & t\end{array}\right]^{\T}\leq b$ for $A\in\R^{m\times(d+1)},b\in\R^{m},$ $\mu\in\Rd$, and $\Sigma\in\pd$.

\paragraph{Uniform and exponential sampling.}

In this case, our self-concordance theory suggests using
\[
\hess(2\sqrt{d+1}\phi_{\text{Lw}}+2(d+1)\phi_{\text{SOC}})\quad\text{or}\quad\hess(2\phi_{*}+2(d+1)\,\phi_{\text{SOC}})\ \text{for }*=\text{log, Vaidya}\,,
\]
to deal with the truncated SOC constraint. For the log-barrier case,
this yields an $\bpar{\mc O(m+d),\mc O(m+d)}$-Dikin-amenable metric
due to Lemma~\ref{lem:soc}, with which $\gcdw$ requires $\otilde{d(m+d)}$
iterations of the $\dw$.

\paragraph{Gaussian sampling.}

Following the reduction as in the polytope sampling, we should use
\[
g(x,t,t')=3\left[\begin{array}{cc}
\hess_{(x,t)}\phi_{\textup{log}}(x,t)+(d+1)\,\hess_{(x,t)}\phi_{\textup{SOC}}(x,t)\\
 & 0
\end{array}\right]+3(d+2)\,\hess_{(x,t,t')}\phi_{\textup{Gauss}}(x,t,t'),
\]
which is $\bpar{\mc O(m+d),\mc O(m+d)}$-Dikin-amenable, and thus
$\gcdw$ needs $\otilde{d(m+d)}$ iterations of the $\dw$.

\subsection{PSD cone sampling\label{subsec:PSD-cone-sampling}}

For a matrix $X\in\Rdd$, recall that $\vec(X)\in\R^{d^{2}}$ denotes
the vector obtained by stacking columns of $X$ vertically. Additionally,
we define $A\in\R^{m\times d^{2}}$, $S_{X}\in\R^{m\times m}$, and
$A_{X}\in\R^{m\times d^{2}}$ by 
\[
A:=\left[\begin{array}{ccc}
\vec(A_{1}) & \cdots & \vec(A_{m})\end{array}\right]^{\T},\quad S_{X}:=\Diag(\inner{A_{i},X}-b_{i}),\quad A_{X}:=S_{X}^{-1}A\,,
\]
where we assume $A$ has no all-zero rows and $(S_{X})_{ii}>0$ for
$i\in[m]$. 

\paragraph{Uniform and exponential sampling.}

The metric below comes from the Hessian of 
\[
-2d^{2}\,\log\det X-2\sum_{i=1}^{m}\log(\inner{A_{i},X}-b_{i})\,.
\]
Here the first term, the log-determinant, serves as a barrier for
the PSD cone while the second term is the standard logarithmic barrier
for linear constraints. We note that the $-\log\det X$ is strictly
convex on $x\in\intk$ for $K$ the truncated PSD cone, so all metrics
$g$ introduced in our main results are positive definite. Thus, the
$\dw$ with those $g$ is well-defined.
\begin{prop}
\label{thm:basicPSD} Let $K$ be the truncated PSD cone and $g$
be the local metric such that at each $X\in\intk$, for symmetric
matrices $H_{1},H_{2}$,
\[
g_{X}(H_{1},H_{2})=2d^{2}\tr(X^{-1}H_{1}X^{-1}H_{2})+2\,\vec(H_{1})^{\T}A_{X}^{\T}A_{X}\vec(H_{2})\,.
\]
Then $\gcdw$ needs $\otilde{(d^{3}+m)d^{2}}$ steps of the $\dw$
with the local metric $g$, where each step runs in $\mc O\bpar{(md^{\omega}+m^{2}d^{2})\wedge(d^{2\omega}+md^{2(\omega-1)})}$
time\footnote{Here $\omega<2.373$ is the current matrix multiplication complexity
exponent (\citet{le2014powers}).}.
\end{prop}

Since $g_{X}$ is $\bpar{\mc O(m+d^{3}),\mc O(m+d^{3})}$-Dikin-amenable
by Lemma~\ref{lem:psd}, $\gcdw$ requires ${\Otilde(d^{2}\,(d^{3}+m))}$
iterations of the $\dw$. As mentioned earlier, efficient maintenance
of the inverse of a metric function could lead to a faster per-step
complexity. As an example, we provide such an implementation of Proposition~\ref{thm:basicPSD}
in \S\ref{subsec:oracle-implementation}. Putting these together,
for an interesting regime of $m=\mc O(1)$, $\gcdw$ is faster than
the $\bw$ by a factor of $d$ in terms of the total complexity.

If we replace the log-barrier by the Vaidya metric, then the dependence
on $m$ is improved to $\sqrt{m}$ as in the polytope sampling. See
\S\ref{proof:Algorithms-for-PSD} for the proofs of the two claims
below.
\begin{prop}
\label{thm:hybridPSD} Let $K$ be the truncated PSD cone and $g$
be the local metric such that at each $X\in\intk$, for symmetric
matrices $H_{1},H_{2}$,
\[
g_{X}(H_{1},H_{2})=2d^{2}\tr(X^{-1}H_{1}X^{-1}H_{2})+44\sqrt{\frac{m}{d}}\,\vec(H_{1})^{\T}A_{X}^{\T}\bpar{\Sigma_{X}+\frac{d}{m}I_{m}}A_{X}\vec(H_{2})\,.
\]
Then $\gcdw$ needs $\otilde{(d^{2}+\sqrt{m})d^{3}}$ steps of the
$\dw$ with the local metric $g$, with each step running in $\otilde{md^{2(\omega-1)}}$
amortized time.
\end{prop}

Lastly, the dependence on $m$ can be made poly-logarithmic by working
with the Lewis-weight metric. We remark that for uniform sampling
the total complexity of $\gcdw$ is less than that of the $\bw$ by
the order of $d^{5-2\omega}$.
\begin{prop}
\label{thm:LSPSD} Let $K$ be the truncated PSD cone and $g$ be
the local metric such that at each $X\in\intk$, for symmetric matrices
$H_{1},H_{2}$, 
\[
g_{X}(H_{1},H_{2})=2d^{2}\tr(X^{-1}H_{1}X^{-1}H_{2})+dc_{1}(\log m)^{c_{2}}\,\vec(H_{1})^{\T}A_{X}^{\T}W_{X}A_{X}\vec(H_{2})\,,
\]
where $W_{X}$ is the diagonalized $\ell_{p}$-Lewis weight of $A_{X}$
with $p=\mc O(\log m)$, and $c_{1},c_{2}>0$ are universal constants.
Then $\gcdw$ requires $\otilde{d^{5}}$ steps of the $\dw$, with
each step running in $\otilde{md^{2(\omega-1)}}$ amortized time.
\end{prop}


\paragraph{Gaussian sampling.}

Just as in polytope or second-order cone sampling, we introduce a
new variable $t$ by replacing a quadratic term in the potential.
This reduces the Gaussian sampling problem to an exponential sampling
problem. We then work with a local metric 
\[
g(X,t)=3\Par{d\left[\begin{array}{cc}
\hess_{X}\phi_{\textup{Lw}}(X)\\
 & 0
\end{array}\right]+d^{2}\left[\begin{array}{cc}
\hess_{X}\phi_{\textup{PSD}}(X)\\
 & 0
\end{array}\right]+d^{2}\hess_{(X,t)}\phi_{\textup{Gauss}}(X,t)}\,,
\]
which is $\bpar{\mc O^{*}(d^{3}),\mc O^{*}(d^{3})}$-Dikin-amenable.
Thus, $\gcdw$ needs $\otilde{d^{5}}$ iterations of the $\dw$ with
the local metric $g$, and the per-step complexity remains $\otilde{md^{2(\omega-1)}}$
in amortized time.

\subsubsection{Per-step implementation \label{subsec:oracle-implementation}}

Now we design an oracle that implements each iteration of the $\dw$
(Algorithm~\ref{alg:DikinWalk}). This can be implemented as follows:
when the current point is $x$,
\begin{itemize}
\item Sample $z\sim\ncal\bpar{0,\frac{r^{2}}{d}g(x)^{-1}}$.
\item Compute $y=x+g(x)^{-1/2}z$ and propose it.
\item Accept $y$ with probability $1\wedge\bpar{\sqrt{\frac{\det g(y)}{\det g(x)}}\,\frac{\exp f(x)}{\exp f(y)}}$.
\end{itemize}
We provide two algorithms with the complexity of $\mc O(md^{\omega}+m^{2}d^{2})$
and $\mc O(d^{2\omega}+md^{2(\omega-1)})$. We can implement each
iteration in $\mc O\bpar{(md^{\omega}+m^{2}d^{2})\wedge(d^{2\omega}+md^{2(\omega-1)})}$
time by using the former for small $m$ and the latter for large $m$.
This completes the second half of Theorem~\ref{thm:basicPSD}. 

\paragraph{Algorithm for small $m$.}

For simplicity here, we ignore the constant factors of $g=g_{1}+g_{2}$,
where 
\[
g_{1}(X)=M^{\T}(X\kro X)^{-1}M=:BB^{\T}\qquad\text{and}\qquad g_{2}(X)=M^{\T}A^{\T}S_{X}^{-2}AM=:UU^{\T}\,.
\]
where $B:=M^{\T}(X\kro X)^{-1/2}\in\R^{d_{s}\times d^{2}}$ and $U:=M^{\T}A^{\T}S_{X}^{-1}\in\R^{d_{s}\times m}$.
Letting $u_{i}$ be the $i$-th column of $U$ for $i\in[m]$, we
note that $g_{2}=\sum_{i=1}^{m}u_{i}u_{i}^{\T}$.

We start with a subroutine for computing $g(X)^{-1}v$ for given $v\in\R^{d_{s}}$
in $\mc O(md^{\omega}+m^{2}d^{2})$ time.

\begin{algorithm2e}[t]

\caption{Computation of $g(X)^{-1}v$}\label{alg:subroutine}

\SetAlgoLined

\textbf{Input:} $X\in\psd$, vector $v\in\R^{d_{s}}$, local metric
$g$.

\textbf{Output:} $g(X)^{-1}v$

Prepare the column vectors $u_{i}$ of $U=M^{\T}A^{\T}S_{X}^{-1}$.

For $\bar{g}_{0}:=g_{1}(X)$, compute $\bar{g}_{0}^{-1}v$ and $\bar{g}_{0}^{-1}u_{i}$
for $i\in[m]$.

\For{$i=1,\cdots,m$}{

Compute $\bar{g}_{i}^{-1}v$ and $\bar{g}_{i}^{-1}u_{j}$ for $j\in[m]$,
according to 

\[
\bar{g}_{i}^{-1}w=\bar{g}_{i-1}^{-1}w-\frac{\bar{g}_{i-1}^{-1}u_{i}\cdot u_{i}^{\T}\bar{g}_{i-1}^{-1}w}{1+u_{i}^{\T}\bar{g}_{i-1}^{-1}u_{i}}\,.
\]

}

Output $\bar{g}_{m}^{-1}v$.

\end{algorithm2e}
\begin{prop}
\label{prop:oracle} Algorithm~\ref{alg:subroutine} computes $g(X)^{-1}v$
in $\mc O(md^{\omega}+m^{2}d^{2})$ time for a query $v\in\R^{d_{s}}$.
\end{prop}

See \S\ref{proof:eff_implement} for the proof. With this subroutine
in hand, we proceed to an efficient implementation of two tasks --
computation of (1) $g(x)^{-\half}z$ for a given vector $z\in\R^{d_{s}}$
and (2) $\sqrt{\frac{\det g(y)}{\det g(x)}}\,\frac{\exp f(x)}{\exp f(y)}$.

\begin{algorithm2e}[t]

\caption{Implementation of the $\dw$}\label{alg:perStep-small-m}

\SetAlgoLined

\textbf{Input:} current point $X\in\psd$, local metric $g$

\tcp{Step 1: Sampling from $\ncal\bpar{0,\frac{r^{2}}{d}g(X)^{-1}}$}

Draw $w\sim\ncal(0,I_{d^{2}+m})$ and $v\gets g(X)^{-1}\left[\begin{array}{cc}
B & U\end{array}\right]w$ by Algorithm~\ref{alg:subroutine}.\

Propose $y\gets\svec(X)+\frac{r}{\sqrt{d}}v$.

\

\tcp{Step 2: Computation of acceptance probability}

Use Algorithm~\ref{alg:subroutine} to prepare $\{\bar{g}_{i}^{-1}u_{1},\dots,\bar{g}_{i}^{-1}u_{m}\}_{i=0}^{m}$
at $X$ and $Y:=\svec^{-1}(y)$.\

$\det\bar{g}_{0}(\cdot)\gets2^{d(d-1)/2}(\det(\cdot))^{-(d+1)}$ ($\because$
Lemma~\ref{lem:Kronecker}-7)

\For{$i=1,\cdots,m$}{

$\det(\bar{g}_{i+1})\gets\det\bar{g}_{i}\cdot(1+u_{i+1}^{\T}\bar{g}_{i}^{-1}u_{i+1})$.

}

Accept $Y$ with probability $1\wedge\bpar{\sqrt{\frac{\det\bar{g}_{m}(Y)}{\det\bar{g}_{m}(X)}}\,\frac{\exp f(X)}{\exp f(Y)}}$.

\end{algorithm2e}
\begin{lem}
\label{lem:perStep-small-m} Algorithm~\ref{alg:perStep-small-m}
implements the $\dw$ with per-step complexity of $\mc O(md^{\omega}+m^{2}d^{2})$.
\end{lem}


\paragraph{Algorithm for large $m$.}

The algorithm right above has quadratic dependence on the number $m$
of constraints, which could become expensive for large $m$. In this
regime, we just fully compute the whole matrix function of size $\R^{d_{s}\times d_{s}}$,
which takes $\mc O(d^{2\omega}+md^{2(\omega-1)})$ time, and computing
its inverse, square-root, and determinant takes $\mc O(d^{2\omega})$
time. 

\subsubsection{Handling approximate Lewis weights \label{subsec:app-lewis-weight}}

When implementing the $\dw$ with the Lewis-weights metric, we use
an approximation algorithm presented in \citet{lee2019solving} for
computing and updating the Lewis weight, which ensures 
\[
(1-\delta)\wtilde_{X}\preceq W_{X}\preceq(1+\delta)\wt W_{X}
\]
for the approximate Lewis weights $\wtilde_{X}$ and a target accuracy
parameter $\delta$ (note that the initialization and update times
of the Lewis weight above hide poly-logarithmic dependence on $\log(1/\delta)$).
Strictly speaking, we should check that these approximate Lewis weights
do not affect the theoretical guarantees above.

To see this, let us define $\widetilde{g}=2(dg_{1}+\widetilde{g}_{2})$,
where for some constants $c_{1},c_{2}>0$
\[
g_{1}(X)=d^{2}M^{\T}(X\kro X)^{-1}M\qquad\text{and}\qquad\widetilde{g}_{2}=dc_{1}\Par{\log m}^{c_{2}}M^{\T}A_{X}^{\T}\widetilde{W}_{X}A_{X}M\,.
\]
First of all, the $\dw$ with $\widetilde{g}$ still converges to
a target distribution, since the approximation algorithm in \citet{lee2019solving}
is deterministic and thus the condition of detailed balance still
holds under the acceptance probability of $1\wedge\bpar{\sqrt{\frac{\det\tilde{g}(Y)}{\det\tilde{g}(X)}}\,\frac{\exp f(X)}{\exp f(Y)}}$.
For $\widetilde{P}_{X}$ the one-step distribution of the $\dw$ started
at $X$ with $\widetilde{g}$, we can show one-step coupling similar
to Lemma~\ref{lem:one-step}, following the overall proof therein
and taking $\delta=1/\text{poly}(d)$ small enough. See \S\ref{proof:Handling-approximate-Lewis}
for the proof. 
\begin{lem}
[One-step coupling] \label{lem:onestep-app-Lewis} For convex $K\subset\Rd$,
let $g:\intk\to\pd$ be SSC, ASC, LTSC, and $\phi:\intk\to\R$ be
its function counterpart. Suppose that the potential $f$ of the target
distribution $\pi$ is $\beta$-relatively smooth in $\phi$. Then
there exist constants $s_{1},s_{2}>0$ such that if $\snorm{x-y}_{g(x)}\leq s_{1}r/\sqrt{d}$
with $r=s_{2}\,(1\wedge1/\sqrt{\beta})$ for $x,y\in\intk$, then
$\dtv(\widetilde{P}_{x},\widetilde{P}_{y})\leq\frac{3}{4}+0.01$.
\end{lem}


