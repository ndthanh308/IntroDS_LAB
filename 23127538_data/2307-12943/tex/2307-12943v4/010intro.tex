\addtocontents{toc}{\protect\setcounter{tocdepth}{2}} 

\newpage{}

\section{Introduction}

As a motivating example, consider the following problem: how can we
efficiently sample a $d\times d$ matrix from a distribution with
the following density?
\begin{align*}
\text{sample } & X\sim\exp\Bpar{-\bpar{\inner{A,X}+\snorm{X-B}_{F}+\snorm{X-C}_{F}^{2}-\log\det X}}\\
\text{s.t. } & X\succeq0,\,\inner{D_{i},X}\geq c_{i}\,,\quad\forall i\in[m]\,.
\end{align*}
This rather complicated looking distribution recovers as special cases
the problems of sampling from the Max-Cut semi-definite programming
relaxation and the set of minimum (or bounded) volume ellipsoids that
contain a given set of points. The above density is logconcave, so
we can use the $\bw$ (along with isotropic rounding) to sample the
distribution with $\mc O(d^{8}\log d)$ membership/evaluation queries
(\citet{lovasz2007geometry}). This ``general-purpose'' sampler
already gives a poly-time mixing algorithm. However, each term in
the density and constraints is ``structured'', which poses the following
natural question: can we leverage \emph{structure} inherent in the
problem to get more efficient algorithms?

The interior-point method (IPM) is a powerful optimization framework
suitable for solving convex optimization problems with structured
objectives and constraints: for proper convex functions $f_{i}$ and
$h_{j}$
\begin{align*}
\min & \sum_{i}f_{i}(x)\ \text{s.t. }h_{j}(x)\leq0\,.
\end{align*}
This leads us to our main question: is there a sampling analogue of
IPM that generates samples from the density proportional to $\exp(-\sum_{i}f_{i})$
restricted to the convex region defined by structured convex functions
$h_{j}$? This is the general problem we will address here and is
stated formally below.
\begin{problem*}
Let $f_{i}$ be a proper convex function and $h_{j}$ a convex function
on $\Rd$ for $i\in[I]$ and $j\in[J]$. Then the goal is:
\begin{align}
\text{sample } & x\sim\pi\propto\exp\Bpar{-\sum_{i}f_{i}}\tag{\ensuremath{\msf{strLC}}}\label{eq:problem}\\
\text{s.t. } & x\in K:=\bigcap_{j\in[J]}\{x\in\Rd:h_{j}(x)\leq0\}\,,\nonumber 
\end{align}
where we assume that $K$ has non-empty interior and $\pi$ has finite
second moment.
\end{problem*}
In this paper, we derive an IPM framework for structured logconcave
sampling. We use the $\dw$ as a sampler to implement the ``inner''
step of IPM. We provide a mixing time bound for the $\dw$, going
beyond uniform distributions (\S\ref{sec:mixing-Dikin}). This generalization
is necessary to be able to utilize the $\dw$ within the IPM framework.
In \S\ref{sec:IPM-framework}, we present the sampling IPM and derive
its guarantees. Our framework is suited for breaking down complicated
sampling problems into smaller structured problems. An important part
of this paper is \S\ref{sec:sc-theory-rules}, where we develop a
``calculus'' for combining multiple constraints and objectives,
and deriving the resulting theoretical guarantees (analogous to and
inspired by the work of \citet{nesterov1994interior} for optimization).
To provide concrete understanding and instances, we illustrate the
framework on some well-known families of constraints in \S\ref{sec:handbook-barrier},
in particular obtaining faster algorithms to sample uniform, exponential,
or Gaussian distributions on truncated PSD cones in \S\ref{sec:examples}.

% Figure environment removed


\subsection{Warm-up: Dikin walk and self-concordance}

We use the same symbol for a distribution and its density w.r.t. the
Lebesgue measure. We use $\psd$ (and $\pd$) to denote the set of
$d\times d$ positive semidefinite (and definite) matrices, respectively.
For two matrices $A,B$, we use $A\asymp B$ to indciate $A\precsim B$
and $B\precsim A$. A \emph{local metric} $g$ defines at each point
$x\in K\subset\Rd$ a positive-definite inner product $\inner{\cdot,\cdot}_{g(x)}:\Rd\times\Rd\to\R$,
which induces the local norm $\snorm v_{g(x)}:=\sqrt{\inner{v,v}_{g(x)}}$.
We use $\snorm v_{x}$ to refer to $\snorm v_{g(x)}$ when the context
is clear. We abuse notation and use $g(x)$ to denote the $d\times d$
positive-definite matrix represented with respect to the canonical
basis $\{e_{1},\dots,e_{d}\}$. For a function $f$ defined on $K\subset\Rd$,
we let $\Dd^{i}f(x)[h_{1},\dotsc,h_{i}]$ denote the $i$-th directional
derivative of $f$ at $x$ in directions $h_{1},\dotsc,h_{i}\in\Rd$,
i.e., 
\[
\Dd^{i}f(x)[h_{1},\dotsc,h_{i}]=\frac{\D^{i}}{\D t_{1}\cdots\D t_{i}}f\Bpar{x+\sum_{j=1}^{i}t_{j}h_{j}}\Big|_{t_{1},\dotsc,t_{i}=0}\,.
\]
We let $\mc N_{g}^{r}(x):=\mc N(x,\frac{r^{2}}{d}g(x)^{-1})$ be the
normal distribution with mean $x$ and covariance $\frac{r^{2}}{d}g(x)^{-1}$.
See \S\ref{subsec:prelim} for full preliminaries and other notation.

\paragraph{Dikin walk.}

Given a local metric $g$ in $\Rd$, the Dikin ellipsoid of radius
$r$ at $x\in\Rd$ is defined as
\[
\mc D_{g}^{r}(x)\defeq\Big\{ y\in\Rd:\sqrt{(y-x)^{\T}g(x)(y-x)}=\snorm{y-x}_{g(x)}\leq r\Big\}\,,
\]
i.e., it is a norm ball of radius $r$ defined by the local metric.
From this perspective, the $\dw$ defined below is a natural generalization
of the $\bw$ to a local metric setting.

\begin{algorithm2e}[H]

\caption{$\dw(\pi_{0},\pi,g,rT)$}\label{alg:DikinWalk}

\SetAlgoLined

\textbf{Input:} Initial distribution $\pi_{0}$, target distribution
$\pi\propto\exp(-f)\cdot\mathbf{1}_{K}$, local metric $g$, step
size $r$, $\#$ iterations $T$.

\textbf{Output:} $x_{T}$

Draw an initial point $x_{0}\sim\pi_{0}$ at random. 

\For{$t=0,\cdots,T-1$}{

Sample $z\sim\mc N\bpar{x_{t},\frac{r^{2}}{d}g(x_{t})^{-1}}$.

$x_{t+1}\gets z$ w.p. $A_{x_{t}}(z):=\min\Bpar{1,\frac{p_{z}(x_{t})}{p_{x_{t}}(z)}\,\frac{\pi(z)}{\pi(x_{t})}}$,
where $p_{x}=\mc N_{g}^{r}(x)$.

Otherwise, $x_{t+1}\gets x_{t}$.

}

\end{algorithm2e}


\paragraph{Dikin metrics and self-concordance.}

The metric $g$ used to define the $\dw$ plays a crucial role in
its convergence. Our metrics will be defined by Hessians of convex
self-concordant barrier functions. We now collect definitions of these
functions; they will be important to state our general guarantees
for the mixing of the $\dw$. The concept we need is summarized by
the definition of a $(\nu,\onu)$\emph{-Dikin-amenable metric}.
% Figure environment removed

\begin{defn}
[Self-concordance (brief version of Definition~\ref{def:sc})]
For convex $K\subset\Rd$, let $\phi:\intk\to\R$ be a smooth convex
function, $g(\cdot)\asymp\hess\phi(\cdot)$, and $\mc N_{g}^{r}(x):=\mc N\bpar{x,\frac{r^{2}}{d}g(x)^{-1}}$. 
\begin{itemize}
\item \emph{$\nu$-self-concordant barrier} (SC): (i) $|\Dd^{3}\phi(x)[h,h,h]|\leq2\snorm h_{\hess\phi(x)}^{3}$
for any $x\in\intk$ and $h\in\Rd$, (ii) $\lim_{x\to\de K}\phi(x)=\infty$,
and (iii) $\snorm{\nabla\phi(x)}_{[\hess\phi(x)]^{-1}}^{2}\leq\nu$
for any $x\in\intk$.
\item \emph{Highly SC} (HSC): $|\Dd^{4}\phi(x)[h,h,h,h]|\leq6\snorm h_{\hess\phi(x)}^{4}$
for any $x\in\intk$ and $h\in\Rd$, and $\lim_{x\to\de K}\phi(x)=\infty$.
\item \emph{Strong SC} (SSC): $\snorm{g(x)^{-\half}\Dd g(x)[h]\,g(x)^{-\half}}_{F}\leq2\snorm h_{g(x)}$
for any $x\in\intk$ and $h\in\Rd$. 
\item \emph{Strongly lower trace SC} (SLTSC): $\tr\bpar{\bpar{\bar{g}(x)+g(x)}^{-1}\Dd^{2}g(x)[h,h]}\geq-\snorm h_{g(x)}^{2}$
for any $\bar{g}:\intk\to\psd$, $x\in\intk$, and $h\in\Rd$. We
call it lower trace self-concordant (LTSC) if it is satisfied when
$\bar{g}=0$.
\item \emph{Strongly average SC} (SASC): For any $\veps>0$ and $\bar{g}:\intk\to\psd$,
there exists $r_{\veps}>0$ such that $\P_{z\sim\mc N_{g+\bar{g}}^{r}(x)}\bpar{\snorm{z-x}_{g(z)}^{2}-\snorm{z-x}_{g(x)}^{2}\leq\frac{2\veps r^{2}}{d}}\geq1-\veps$
for $r\leq r_{\veps}$. We call it average self-concordant (ASC) if
this is satisfied when $\bar{g}=0$. 
\end{itemize}
\end{defn}

SC imposes regularity on the eigenvalues of the directional derivative
$\Dd g[h]$ through its definition $-2\snorm h_{g}^{2}g\preceq\Dd g[h]\preceq2\snorm h_{g}^{2}g$
(or equivalently the largest magnitude of eigenvalues of $g^{-\nicefrac{1}{2}}\Dd g[h]\,g^{-\nicefrac{1}{2}}$),
and HSC does the same on the higher-order derivative $\Dd^{2}g[h,h]$.
SSC introduced by \citet{laddha2020strong} imposes \emph{stronger}
regularity on the eigenvalues of $\Dd g[h]$ by definition, as SSC
is stated in terms of the \emph{Frobenius norm} of $g^{-\half}\Dd g[h]\,g^{-\half}$.
LTSC relaxes `convexity of $\log\det g$' required by \citet{laddha2020strong}.
In particular, SSC and LTSC control the change of $\log\det g$, leading
to a refined analysis of the $\dw$. Lastly, ASC is pertinent to the
average of the squared local norm difference of $z-x$ computed at
$z$ and $x$, which controls the acceptance-probability of each iterate
of the $\dw$. 

These notions are sophisticated enough to carry out a tight mixing
analysis of the $\dw$, but also simple enough for us to develop a
``calculus'' for combining metrics for multiple constraints in \S\ref{sec:sc-theory-rules}.
Moreover, these conditions may look difficult to verify, but we show
that a proper scaling of (H)SC barriers immediately makes them satisfy
these properties.

Next, we recall a symmetry parameter of a self-concordant metric.
We will later see that it has a natural connection to the Cheeger
isoperimetry.
\begin{defn}
[$\onu$-symmetry] \label{def:symm-param} For convex $K\subset\Rd$,
a PSD matrix function $g:\intk\to\psd$ is said to be $\onu$-\emph{symmetric}
if $\mc D_{g}^{1}(x)\subseteq K\cap(2x-K)\subseteq\mc D_{g}^{\sqrt{\onu}}(x)$
for any $x\in K$.
\end{defn}

We note that $K\cap(2x-K)$ is the locally symmetrized convex body
with respect to $x$. Hence, $\onu$-symmetry measures how accurately
a Dikin ellipsoid approximates the locally symmetrized body. One can
show that $\onu=\mc O(\nu^{2})$ for any metric induced by a self-concordant
barrier.

Going forward, we call a PD matrix function $\onu$\emph{-Dikin-amenable}
if it is SSC, LTSC, ASC, and $\onu$-symmetric. We sometimes call
it $(\nu,\onu)$-Dikin-amenable to reveal its self-concordance parameter
$\nu$. For example, the Hessian of a logarithmic barrier is an $(m,m)$-Dikin-amenable
metric. We present more concrete examples after introducing Theorem~\ref{thm:IPM-sampling}.

\subsection{Results}

\subsubsection{Dikin walk ($\S$\ref{sec:mixing-Dikin})}

We begin with our analysis of the $\dw$ for general settings, going
beyond uniform distributions.

\begin{restatable}{thmre}{thmDikin} \label{thm:Dikin} Let $K\subset\Rd$
be convex and $0\leq\alpha\leq\beta<\infty$.
\begin{itemize}
\item (Local metric) Assume that a $C^{1}$-matrix function $g:\intk\to\pd$
is $\onu$-Dikin-amenable. 
\item (Distribution) Let $\pi_{0}$ and $\pi\propto e^{-f}\cdot\mathbf{1}_{K}$
be an initial and target distribution respectively, where $f$ is
$\alpha$-relatively strongly convex and $\beta$-smooth in $g$.
Let $\norm{\pi_{0}/\pi}=\E_{\pi_{0}}\big[\deriv{\pi_{0}}{\pi}\big]$
and $P$ be the transition kernel of the $\dw$ (Algorithm~\ref{alg:DikinWalk})
with the local metric $g$ and step size $r=\O(\min(1,\beta^{-1/2}))$.
\end{itemize}
Then for any $\veps>0$, it holds that $\dtv(\pi_{0}P^{(T)},\pi)\leq\veps$
for $T\gtrsim d\,\max(1,\beta)\,\min(\onu,\nicefrac{1}{\alpha})\,\log\frac{\norm{\pi_{0}/\pi}}{\veps}$.

\end{restatable}

This result serves as a unifying framework that recovers as special
cases previous works on the $\dw$ for uniform sampling (\citet{kannan2012random,narayanan2016randomized,chen2018fast,laddha2020strong}),
as seen later in \S\ref{sec:examples}. Our analysis extends beyond
uniform sampling, considering the $\dw$ under a more general setting
where the potential $f$ satisfies $\alpha g\preceq\hess f\preceq\beta g$
on $\intk$. This setting is a generalization of $\alpha I\preceq\hess f\preceq\beta I$
under a local metric $\hess\phi\asymp g$. We also note that the $\dw$
is the first \emph{implementable} algorithm that provides a clean
mixing guarantee under this general setting, which is a necessary
ingredient for theory of our sampling IPM. We refer readers to \S\ref{subsec:related-work}
for related work.

All previous analyses of the $\dw$ do not go through for general
distributions. The techniques either have gap (e.g., omit ASC) or
yield a wrong proof (e.g., for one-step coupling, the TV distance
bound from the triangle inequality is larger than $1$, so becomes
vacuous). Our analysis proceeds with the exact form of the TV distance,
additionally requiring the control of $\half\int|A_{x}(z)p_{x}(z)-A_{y}(z)p_{y}(z)|\,\D z$
for close points $x$ and $y$ (see Algorithm~\ref{alg:DikinWalk}).
As sketched in \S\ref{sec:mixing-Dikin}, this involved task simultaneously
quantifies closeness of acceptance probabilities $A_{x}(z)$ and $A_{y}(z)$
as well as that of the Gaussian densities $p_{x}(z)$ and $p_{y}(z)$.
This can be achieved through sophisticated conditioning on high-probability
events due to ASC, SSC, and symmetry of Gaussians.

\subsubsection{Sampling IPM: Gaussian cooling with the Dikin walk ($\protect\gcdw$)
($\S$\ref{sec:IPM-framework})}

We present $\gc$, essentially a sampling analogue of the optimization
IPM. The \emph{function counterpart} below refers to a self-concordant
barrier $\phi$ such that $\hess\phi\asymp g$ on $\intk$.

\begin{restatable}{thmre}{thmDikinannealing} \label{thm:Dikin-annealing}
For convex $K\subset\Rd$, suppose that $g:\intk\to\pd$ is $(\nu,\onu)$-Dikin-amenable
and $\phi$ is its function counterpart such that $\min_{K}\phi$
exists. $\gc$ with the $\dw$ (Algorithm~\ref{alg:IPM-sampling}
with the $\dw$ serving as a non-Euclidean sampler) generates a sample
that is $\veps$-close to $\exp(-f)\cdot\mathbf{1}_{K}$ in TV-distance
using $\mc O\bpar{d\,\max(d\frac{\nu\beta+d}{\nu\alpha+d},\nu,\onu)\log\frac{d\nu}{\veps}}$
iterations of the $\dw$ with $g$, where a $C^{2}$-function $f:\intk\to\R$
satisfies $\alpha\hess\phi\preceq\hess f\preceq\beta\hess\phi$ on
$K$ for $0\leq\alpha\leq\beta<\infty$. In particular, when $f(x)=\alpha^{\T}x$
or $c\phi(x)$ for $\alpha\in\Rd$ and $c\in\R_{+}$, the algorithm
uses $\widetilde{\mc O}(d\,\max(d,\nu,\onu))$ iterations of the $\dw$.

\end{restatable}

The inner loop of $\gc$ runs the $\dw$. The basic GC algorithm was
introduced in \cite{cousins2018gaussian} for efficient sampling and
volume computation. \cite{lee2018convergence} studied its extension
to Hessian manifolds for uniformly sampling polytopes. Our framework
is general in that it handles more general distributions through a
sophisticated annealing scheme.

This framework provides an efficient algorithm for generating a warm
start for constrained log-concave distributions. If we were to apply
Theorem~\ref{thm:Dikin} with initial distribution being a single
point at some distance from boundary, even for the simplest case of
uniform sampling, then an additional factor of $d$ would be incurred.
On the other hand, given that $\nu$ and $\onu$ are typically $\O(d)$,
our framework only has a logarithmic (in dimension) factor overhead
for generating a warm start. An important reason why this works is
the affine-invariance of the $\dw$. Samplers like the $\bw$ have
to apply isotropic transformation to achieve a warm start efficiently,
which requires a near-linear number of samples and thus have at least
a linear in dimension overhead.

\paragraph{Derivation of the algorithm.}

We describe this algorithm alongside its interpretation as a `sampling
analogue of the \emph{interior-point method}'. To this end, we revisit
ideas of IPM, derive its sampling version via a conceptual analogy
between optimization and sampling, and refine the derived sampling
IPM by highlighting the distinctions between the two methods. See
\S\ref{subsec:derivation-IPM-sampling} for details.

\subparagraph*{(1) Optimization IPM (Algorithm~\ref{alg:IPM}).}

In solving the optimization problem, $\min_{x\in K}f(x)$ for a real-valued
convex function $f$ on convex $K\subset\Rd$, IPM first replaces
$f$ by a new variable $t$ and appends the epigraph $\{(x,t)\in\R^{d+1}:f(x)\leq t\}$
to the constraint in addition to $x\in K$. Then summation of self-concordant
barriers for $K$ and the epigraph results in a $\nu$-self-concordant
barrier $\phi$ for the augmented constraints. This barrier $\phi$
allows one to convert the constrained problem to a unconstrained one,
$\min f_{\lda}(x,t):=t+\frac{1}{\lda}\phi(x,t)$ for a parameter $\lda>0$.
Then an optimization step (e.g., the Newtonian gradient descent) that
takes into account the local geometry given by $\hess\phi$ moves
a current point closer to an optimal point, with the barrier $\phi$
preventing escape from the constraints. Increasing $\lda\gets\lda(1+\frac{1}{\sqrt{\nu}})$,
IPM repeats this procedure with the updated point used as a starting
point. As $\lda$ increases (until $\lda\leq\nu/\veps$ for target
accuracy $\veps>0$), the effect of $\frac{1}{\lda}\,\phi(x,t)$ vanishes
in the regularized problem, which gradually brings us to a point sufficiently
closer to the minimum.

\subparagraph*{(2) Translation to sampling (Figure~\ref{fig:opt-samp-IPM}).}

We recall the following conceptual match between convex optimization
and logconcave sampling: for convex $K\subset\Rd$ and convex function
$f:K\to\R$
\begin{align*}
\min f(x) & \quad\longleftrightarrow\quad\text{sample }x\sim\pi\propto\exp(-f)\\
\text{s.t. }x\in K\,. & \qquad\qquad\quad\text{s.t. }x\in K\,.
\end{align*}
With the connection in mind, we can translate IPM's machinery into
the sampling context. As in IPM, we replace $f$ by a new variable
$t$, introduce the epigraph constraint, and attempt to sample a `regularized'
distribution $\mu_{\sigma^{2}}(x,t)\propto\exp\bigl(-f_{\sigma^{2}}(x,t)\bigr)=\exp\bpar{-\bpar{t+\frac{1}{\sigma^{2}}\,\phi(x,t)}}$,
where a parameter $\sigma^{2}$ corresponds to $\lda$ above. This
sampling step should be carried out with a sampler \emph{aware} of
the local geometry given by $\hess\phi$ (call it $\msf{NE\text{-}sampler}$,
which is the $\dw$ in our case). Then we increase $\sigma^{2}$ slightly,
and using the previous regularized distribution $\mu_{\sigma^{2}}$
as a warm start, we sample a next regularized distribution $\mu_{\sigma^{2}+\veps}$.
This iterative procedure continues until $\sigma^{2}$ reaches $\nu$.

\subparagraph*{(3) Refinements (Figure~\ref{fig:gc}).}

We now make this conceptual algorithm concrete in Algorithm~\ref{alg:IPM-sampling}.
The finalized sampling IPM\footnote{For the sake of exposition, we focus on just the exponential distribution
$e^{-t}$. Our algorithm can deal with more general potentials (relatively
convex and smooth).} consists of four phases --- Phase 1 for initialization, Phase 2
and 3 for increasing $\sigma^{2}$ with control, and Phase 4 for high-accuracy
sampling.

Phase 1 initializes the algorithm by a Gaussian truncated over a Dikin
ellipsoid of radius $\O(d^{-\Theta(1)})$. This Gaussian serves as
a good warm start for a regularized distribution with small $\sigma^{2}$.

The sampling IPM, in contrast to both the optimization IPM and the
basic GC algorithm, proceeds with a distinct annealing scheme. Phase
2 updates $\sigma^{2}\gets\sigma^{2}(1+\nicefrac{1}{d})$ until $\sigma^{2}$
reaches $\nu/d$, annealing not only $\phi$ but also the `modified'
potential $\nu t/d$. While $\frac{\nu}{d}\leq\sigma^{2}\leq\nu$,
Phase 3 updates $\sigma^{2}\gets\sigma^{2}(1+\nicefrac{\sigma}{\sqrt{\nu}})$
but only $\phi$ part with the potential $t$ now fixed. We note that
the basic GC anneals only regularization term throughout.

Lastly, the sampling IPM runs the $\dw$ once in Phase 4. If one stopped
after Phase 3 (when $\sigma^{2}$ reaches as the optimization version,
then the total iterates of the $\dw$ would be $\mc O(d\,(d\vee\nu)/\text{poly}(\veps))$.
This guarantee can avoid the symmetry parameter, but this comes at
the cost of low-accuracy of the sampler (i.e., dependence on $\text{poly}(\veps^{-1})$).
Hence, we finish up the algorithm with another execution of the $\dw$,
obtaining high-accuracy $\mc O(d\,(d\vee\nu\vee\onu)\,\log\frac{1}{\veps})$-mixing.

At the heart of the algorithm lies closeness of regularized distributions
in consecutive iterations. Closeness in the first two phases follows
from a property of logconcavity established by \citet{lovasz2006simulated},
while closeness in the last two phases is assured by the Brascamp-Lieb
inequality.

$\gcdw$ is exactly this refined algorithm with the $\dw$ used for
the $\msf{NE\text{-}sampler}$ (Algorithm~\ref{alg:IPM-sampling}).
Specifically in the inner loop, it runs the $\dw$ to sample regularized
exponential distributions of the form $\exp(-(c_{1}t+c_{2}\phi(x,t))$
subject to $x\in K$ and $\{(x,t)\in\R^{d+1}:f(x)\leq t\}$, where
the local metric therein consists of the Hessians of self-concordant
barriers for $K$ and the level set of $f$. Comparing with the $\bw$
for a general logconcave distribution \cite{lovasz2007geometry},
incorporating the geometry of a level set of $f$ (not $\hess f$)
is a natural approach to sampling from $e^{-f}$.

% Figure environment removed


\subsubsection{Self-concordance theory for combining barriers ($\S$\ref{sec:sc-theory-rules})}

From the earlier discussion, the sampling IPM allows us to focus on
the following reduced problem: Let $t_{1},\dots,t_{I}\in\R$ and $y=(x,t_{1},\dots,t_{I})\in\Rd\times\R^{I}=\R^{d+I}$.
We denote $E_{i}:=\{(x,t_{i})\in\R^{d+1}:f_{i}(x)\leq y_{n+i}\}$
for $i\in[I]$ and $K_{j}:=\{x\in\Rd:h_{j}(x)\leq0\}$ for $j\in[J]$,
whose convexity follows from convexity of $f_{i}$ and $h_{j}$. Denoting
the embeddings of $E_{i}$ and $K_{j}$ onto $\R^{d+I}$ by $\bar{E}_{i}$
and $\bar{K}_{j}$, we can reduce \eqref{eq:problem} to
\begin{align}
\text{sample } & y\sim\tilde{\pi}\propto\exp\bpar{-(\underbrace{0,\dotsc,0}_{d\text{ times}},\underbrace{1,\dotsc,1}_{I\text{ times}})^{\T}\,y}\tag{\ensuremath{\msf{redLC}}}\label{eq:reduced-problem}\\
\text{s.t. } & y\in K':=\bigcap_{i=1}^{I}\bar{E}_{i}\,\cap\,\bigcap_{j=1}^{J}\bar{K}_{j}\,,\nonumber 
\end{align}
where $K'$ is closed convex and has non-empty interior, and we are
given self-concordant barriers for each $E_{i}$ and $K_{j}$. As
the $x$-marginal of $\tilde{\pi}$ is $\pi$, we just project a drawn
sample from $\tilde{\pi}$ to the $x$-space. When $f_{i}(x)$ can
be written as $d$ separable terms (i.e., $f_{i}(x)=\sum_{l=1}^{d}f_{i,l}(x_{l})$),
it is more convenient to introduce $d$ many variables $t_{i,1},\dots t_{i,d}$
for $f_{i,1}(x_{1}),\dots,f_{i,d}(x_{d})$.

In \S\ref{sec:sc-theory-rules}, we study how to combine a self-concordant
metric and its parameters from each epigraph $E_{i}$ and convex set
$K_{j}$ (for the mixing estimation of $\dw$). As in the optimization
IPM, the addition of all barriers is actually a good candidate of
a barrier for $K'$, but under an appropriate scaling. However, the
sampling version requires not only just self-concordance parameters
but also symmetry parameters, SSC, and LTSC for final mixing time
guarantees. Notably, SSC and LTSC assume invertibility of a local
matrix function, but the Hessian of a barrier for a lower-dimensional
space is degenerate with respect to the augmented variable $y\subset\R^{d+I}$.
We address this technical issue by working with Definition~\ref{def:sc-along-subspace}
and several matrix lemmas to study how to maintain or update each
of the main properties such as symmetry, SSC, and LTSC under addition
and scaling.

Now we can state how to put together information of a barrier for
each constraint and epigraph. The readers can note the analogy to
Nesterov and Nemirovski's IPM theory for optimization.

\begin{restatable}{thmre}{thmIPMsampling} \label{thm:IPM-sampling}
In the reduced problem of \eqref{eq:reduced-problem}, let us assume
the following:
\begin{itemize}
\item For $i\in[I]$, the epigraph $E_{i}$ admits a PSD matrix function
$g_{i}^{e}(x,t_{i})$ (or $g_{i}^{e}(x,t_{i,1},\dots,t_{i,d})$) that
is a $(\nu_{i},\bar{\nu}_{i})$-SC barrier, SSC along some subspace,
SLTSC, and SASC.
\item For $j\in[J]$, the constraint $K_{j}$ admits a PSD matrix function
$g_{j}^{c}(x)$ that is a $(\eta_{j},\bar{\eta}_{j})$-SC barrier,
SSC along some subspace, SLTSC, and SASC.
\end{itemize}
For appropriate projections $\pi_{i}^{e}$ and $\pi^{c}$, a matrix
function $g$ on $y\in\inter(K')$ defined by
\[
\langle u,v\rangle_{g(y)}:=(I+J)\,\Bpar{\sum_{i=1}^{I}\langle\pi_{i}^{e}u,\pi_{i}^{e}v\rangle_{g_{i}^{e}(\pi_{i}^{e}(y))}+\sum_{j=1}^{J}\langle\pi^{c}u,\pi^{c}v\rangle_{g_{j}^{c}(\pi^{c}(y))}}\quad\text{for }u,v\in\Rd
\]
is $\bpar{(I+J)(\sum_{i=1}^{I}\nu_{i}+\sum_{j=1}^{J}\eta_{j}),\,(I+J)(\sum_{i=1}^{I}\onu_{i}+\sum_{j=1}^{J}\bar{\eta}_{j})}$-Dikin-amenable
on $K'$.

\end{restatable}

\subsubsection{Metrics for well-known structured instances ($\S$\ref{sec:handbook-barrier})}

\begin{table}
\begin{centering}
\begin{tabular}{ccccccccc}
\toprule 
\textbf{Constraints / Epigraphs} & \textbf{Barrier} & $\nu$ & $\onu$ & SSC & LTSC & SLTSC & ASC & SASC\tabularnewline
\midrule
\midrule 
\multirow{3}{*}{$Ax\geq b$} & $\phi_{\textup{log}}$ & $m$ & $m$ &  &  &  &  & \tabularnewline
\cmidrule{2-9} \cmidrule{3-9} \cmidrule{4-9} \cmidrule{5-9} \cmidrule{6-9} \cmidrule{7-9} \cmidrule{8-9} \cmidrule{9-9} 
 & $g_{\textup{Vaidya}}$ & $\sqrt{md}$ & $\sqrt{md}$ &  &  &  &  & \tabularnewline
\cmidrule{2-9} \cmidrule{3-9} \cmidrule{4-9} \cmidrule{5-9} \cmidrule{6-9} \cmidrule{7-9} \cmidrule{8-9} \cmidrule{9-9} 
 & $g_{\textup{Lw}}$ & $d$ & $d$ &  & $\sqrt{d}$ & $\sqrt{d}$ & $\sqrt{d}$ & $\sqrt{d}$\tabularnewline
\midrule 
$\norm{x-\mu}_{\Sigma}^{2}\leq1$ & $\phi_{\textup{ellip}}$ &  &  & $d$ &  &  & $d$ & $d$\tabularnewline
\midrule 
$\norm{x-\mu}_{\Sigma}^{2}\leq t$ & $\phi_{\textup{Gauss}}$ &  &  & $d$ &  &  & $d$ & $d$\tabularnewline
\midrule 
$\norm{x-\mu}_{\Sigma}\leq t$ & $\phi_{\textup{SOC}}$ &  &  & $d$ & $d$ & $d$ & $d$ & $d$\tabularnewline
\midrule 
$X\succeq0$ & $\phi_{\textup{PSD}}$ & $d$ & $d$ & $d$ &  &  & $d$ & $d^{2}$\tabularnewline
\midrule 
$-x_{i}\log x_{i}\leq t_{i}$ $\forall i\in[d]$ & $\phi_{\textup{ent}}$ & $d$ & $d$ &  &  &  & $d$ & $d$\tabularnewline
\midrule 
$\Abs{x_{i}}^{p}\leq t_{i}$ $\forall i\in[d]$ & $\phi_{\textup{power}}$ & $d$ & $d$ &  &  &  & $d$ & $d$\tabularnewline
\bottomrule
\end{tabular}
\par\end{centering}
\caption{\label{tab:scaling-table} Self-concordance and symmetry parameters,
and required scaling factors for a family of barriers. In this table,
we assume $A\in\protect\R^{m\times d},x\in\protect\Rd,$ and $X\in\protect\psd$.
Empty entries indicate $\protect\O(1)$-scalings.}
\end{table}

In \S\ref{sec:handbook-barrier}, we examine required parameters
and properties of a barrier for a structured constraint and potential,
such as linear, quadratic, entropy, $\ell_{p}$-norm, and PSD cone.
See Table~\ref{tab:scaling-table}.

\paragraph{(1) Linear constraints.}

We start with linear constraints given by $K:=\{x\in\Rd:Ax\geq b\}$
for $A\in\R^{m\times d}$ and $b\in\R^{m}$, where $A$ is assumed
to have no all-zero rows. For $x\in\intk$ and $i\in[m]$, let $a_{i}$
be the $i$-th row of $A$, and denote $S_{x}:=\Diag(a_{i}^{\T}x-b_{i})\in\R^{m\times m}$
and $A_{x}:=S_{x}^{-1}A\in\R^{m\times d}$.

These linear constraints admit efficiently computable self-concordant
barriers: logarithmic barrier, Vaidya metric, and Lewis-weight metric.
The logarithmic barrier is the simplest defined by 
\[
\phi_{\textup{log}}(x):=-\sum_{i=1}^{m}\log(a_{i}^{\T}x-b_{i})\,.
\]
When the number of constraints $m$ is large, one can use a self-concordant
metric due to \citet{vaidya1996new}. For a full-rank matrix $A$,
the resulting Vaidya metric takes advantage of the \emph{leverage
scores} $\sigma(A_{x})$ of $A_{x}$, the diagonal entries of the
orthogonal projection $P_{x}=A_{x}(A_{x}^{\T}A_{x})^{-1}A_{x}\in\R^{m\times m}$,
i.e., $[\sigma(A_{x})]_{i}:=(P_{x})_{ii}>0$ for $i\in[m]$. For $\Sigma_{x}=\Diag(\sigma(A_{x}))\in\R^{m\times m}$,
the Vaidya metric is defined by 
\[
g_{\textup{Vaidya}}(x):=\mc O(1)\sqrt{\frac{m}{d}}A_{x}^{\T}\bpar{\Sigma_{x}+\frac{d}{m}I_{m}}A_{x}\,,
\]
which satisfies $g_{\textup{Vaidya}}\asymp\hess\bpar{\sqrt{\frac{m}{d}}(\phi_{\vol}+\frac{d}{m}\phi_{\textup{log}})}$
for $\phi_{\vol}:=\half\log\det(\hess\phi_{\textup{log}})$.

Its self-concordance parameter is still polynomial in $m$, and it
is natural to ask if the dependence on $m$ can be removed or made
poly-logarithmic. This can be achieved by a Lewis-weight metric that
makes use of the \emph{Lewis weights} of $A_{x}$. The $\ell_{p}$-Lewis
weight of $A_{x}$ is the vector $w_{x}\in\R^{m}$ satisfying the
implicit equation $w_{x}=\sigma\bpar{\Diag(w_{x})^{1/2-1/p}A_{x}}$.
Note that the leverage scores can be recovered as the $\ell_{2}$-Lewis
weight of $A_{x}$. Then the Lewis-weight metric is defined by
\[
g_{\text{\textsf{Lw}}}(x):=\mc O(\log^{\mc O(1)}m)\,A_{x}^{\T}W_{x}A_{x}\,,
\]
which is an $\mc O\bpar{\log^{\mc O(1)}m}$-approximation of the Hessian
of $\phi_{\text{Lw}}(x):=\log\det(A_{x}^{\T}W_{x}^{1-2/p}A_{x})$.
With $p=\mc O(\log^{\Theta(1)}m)$, the self-concordance parameter
of this barrier and metric can be made $\mc O^{*}(d)$.

For the sampling purpose, we should look into other properties such
as symmetry, SSC, SLTSC, and SASC, going beyond just self-concordance
parameter. We note that the log-barrier and Vaidya metric fulfill
these properties without additional scaling, while the Lewis-weight
metric requires a $\sqrt{d}$-scaling for SLTSC and SASC. We summarize
these results below.
\begin{thm*}
[Linear constraints] We assume $m\geq d$ in the cases of the Vaidya
and Lewis-weight. Let $w_{x}$ be the $\ell_{p}$-Lewis weights with
$p=\O(\log^{\Theta(1)}m)$. 
\begin{itemize}
\item Log-barrier $\phi_{\textup{log}}$: $g=\hess\phi_{\textup{log}}$
satisfies $\nu,\onu\leq m$, SSC along $\rowspace(A)$, and $\Dd^{2}g(x)[h,h]\succeq0$
(so SLTSC), and SASC.
\item Vaidya metric $g_{\textup{Vaidya}}(x)=\sqrt{\frac{m}{d}}A_{x}^{\T}\bpar{\Sigma_{x}+\frac{d}{m}I_{m}}A_{x}$
and $\phi_{\textup{Vaidya}}=\sqrt{\frac{m}{d}}\bpar{\half\log\det(\hess\phi_{\textup{log}})+\frac{d}{m}\phi_{\textup{log}}}$
(with $m\geq d)$: $g=44g_{\textup{Vaidya}}$ satisfies $\nu,\onu=\mc O(\sqrt{md})$,
SSC, SLTSC, and SASC.
\item Lewis-weight metric $g_{\textup{Lw}}(x)=\mc O(\log^{\mc O(1)}m)\,A_{x}^{\T}W_{x}A_{x}$
and $\phi_{\textup{Lw}}=\log\det(A_{x}^{\T}W_{x}^{1-2/\mc O(\log m)}A_{x})$:
$g=\sqrt{d}g_{\textup{Lw}}$ satisfies $\nu,\onu=\mc O(d^{3/2}\log^{\mc O(1)}m)$,
SSC, SLTSC, and SASC.
\end{itemize}
\end{thm*}


\paragraph{(2) Quadratic potentials and constraints.}

Now we consider quadratic potential (i.e., Gaussian) and constraints
(i.e., ellipsoid and second-order cone). A self-concordant barrier
introduced by \citet{nesterov1994interior} serves as an efficient
barrier for each constraint or epigraph of a potential. We show that
all barriers are HSC, so the scaling of $d$ makes it satisfy SLTSC
and SASC.
\begin{thm}
[{[}Quadratic] Let $K_{1}=\{x\in\Rd:\half x^{\T}Qx+p^{\T}x+l\leq0\}$
with $p\in\Rd$ and $0\neq Q\in\psd$. Let $K_{2}=\{(x,t)\in\R^{d+1}:\half\norm{x-\mu}_{\Sigma}^{2}\leq t\}$
and $K_{3}=\{(x,t)\in\R^{d+1}:\norm{x-\mu}_{\Sigma}\leq t\}$ with
$\mu\in\Rd$ and $\Sigma\in\pd$. Let $x\in\inter(K_{i})$ and $h\in\R^{\dim(K_{i})}$. 
\begin{itemize}
\item Ellipsoid $\phi_{\textup{ellip}}(x)=-\log(-l-p^{\T}x-\half x^{\T}Qx)$
for $K_{1}$: $g=d\,\hess\phi_{\textup{ellip}}$ satisfies $\nu,\onu=\mc O(d)$,
SSC when $Q\in\pd$, $\Dd^{2}g(x)[h,h]\succeq0$ (so SLTSC), and SASC.
\item Gaussian $\phi_{\textup{Gauss}}(x,t)=-\log(t-\half\norm{x-\mu}_{\Sigma}^{2})$
for $K_{2}$: $g=d\,\hess\phi_{\textup{Gauss}}$ satisfies $\nu,\onu=\mc O(d)$,
SSC, and $\Dd^{2}g(x,t)[h,h]\succeq0$ (so SLTSC), and SASC.
\item Second-order cone $\phi_{\textup{SOC}}(x,t)=-\log(t^{2}-\norm{x-\mu}_{\Sigma}^{2})$
for $K_{3}$: $g=d\,\hess\phi_{\textup{SOC}}$ satisfies $\nu,\onu=\mc O(d)$,
SSC, SLTSC, and SASC.
\end{itemize}
\end{thm}


\paragraph{(3) PSD cone.}

Another fundamental constraint is the PSD cone. This convex region
admits a $d$-self-concordant barrier $\phi_{\textup{PSD}}(\cdot)=-\log\det(\cdot)$.
We show that it satisfies SLTSC, while the $d$-scaling further guarantees
SSC and ASC. In establishing ASC, we find an interesting connection
to the \emph{Gaussian orthogonal ensemble} (GOE), one of the main
objects studied in random matrix theory. However, we cannot prove
SASC, so we need the $\frac{d(d+1)}{2}$-scaling for SASC (due to
HSC of $\phi_{\textup{PSD}}$). 
\begin{thm}
[PSD cone] Let $K=\psd$, $X\in\intk$, and $H\in\mbb S^{d}$. Then,
$d\,\hess\phi_{\textup{PSD}}$ satisfies $\nu,\onu=\mc O(d^{2})$,
SSC, $\Dd^{2}g(X)[H,H]\succeq0$ (so SLTSC), and ASC. $\frac{d(d+1)}{2}\,\hess\phi_{\textup{PSD}}$
is SASC.
\end{thm}


\paragraph{(4) Entropy and $\ell_{p}$-norm.}

It is sometime more convenient to introduce $d$ many new variables
as seen in the following:
\begin{thm*}
[Entropy and $\ell_p$-norm] Let $K_{1}=\prod_{i=1}^{d}\{(x_{i},t_{i})\in\R^{2}:x_{i}\geq0,\,t_{i}\geq x_{i}\log x_{i}\}$
and $K_{2}=\prod_{i=1}^{d}\{(x_{i},t_{i})\in\R^{2}:\Abs{x_{i}}^{p}\leq t_{i}\}$.
\begin{itemize}
\item Entropy $\phi_{\textup{ent}}(x,t)=-\sum_{i=1}^{d}\bpar{\log(t_{i}-x_{i}\log x_{i})+36\log x_{i}}$
for $K_{1}$: $g=d\,\hess\phi_{\textup{ent}}$ satisfies $\nu,\onu=\mc O(d^{2})$,
SSC, SLTSC, and SASC.
\item The $p$-th power of $\ell_{p}$-norm $\phi_{\textup{power}}(x,t)=-\sum_{i=1}^{d}\bpar{\log(t_{i}^{2/p}-x_{i}^{2})+72\log t_{i}}$
for $K_{2}$: $g=d\,\hess\phi$ satisfies $\nu,\onu=\mc O(d^{2})$,
SSC, SLTSC, and SASC.
\end{itemize}
\end{thm*}

\subsubsection{Examples ($\S$\ref{sec:examples})}

Our theory (Theorem~\ref{thm:Dikin-annealing} and~\ref{thm:IPM-sampling})
with the study of barriers (Table~\ref{tab:scaling-table}) proposes
local metrics for structured instances. $\gcdw$ with them mixes in
poly-time faster than the $\bw$. For fair comparison, the complexity
of the $\bw$ refers to that of isotropic rounding\footnote{For general logconcave sampling, the $\bw$ needs isotropic rounding,
using $\Otilde(d^{4})$ queries, \textbf{after which} an $\O(1)$-warm
start and isotropy are provided, and then it mixes using additoinal
$\Otilde(d^{2})$ queries \citep{lovasz2007geometry}. Without rounding,
it is not necessarily poly-time mixing. %, so isotropic rounding is an inevitable procedure for fair comparison.
For uniform sampling \emph{only}, the complexity of obtaining isotropy
and an $\O(1)$-warm start was improved to $\Otilde(d^{3})$ by \citet{jia2021reducing}.} (see \S\ref{sec:examples}).

\paragraph{Motivating example.}

Let us introduce a variable for each of $\snorm{X-B}_{F}$ and $\snorm{X-C}_{F}^{2}$.
Then our theory suggests the following barrier: $4(\phi_{\textup{log}}+d^{2}\phi_{\textup{Gaussian}}+d^{2}\phi_{\textup{SOC}}+d^{2}\phi_{\textup{PSD}})$,
which is $\O(1)\,(m+d^{3},m+d^{3})$-self-concordant, SSC, LTSC, and
ASC. By Theorem~\ref{thm:Dikin-annealing} with $\alpha=0$ and $\beta=1$
(due to $\phi_{\textup{PSD}}$ in the potential), we need $\Otilde\bpar{d^{2}(m+d^{3})}$
iterations of the $\dw$ in total.

\paragraph{Uniform and exponential sampling.}

Let us first consider uniform sampling over linear constraints given
by $Ax\geq b$ for $A\in\R^{m\times d}$ and $b\in\R^{m}$. Recall
that for uniform sampling the $\bw$ mixes in $\Otilde(d^{3})$ iterations
(including isotropic rounding). On the other hand, $\Otilde(md)$
queries are enough for $\gcdw$ with the $(m,m)$-Dikin amenable metric
induced by $\phi_{\textup{log}}$. This recovers the mixing time of
\citet{kannan2012random} \emph{without} warmness. If we use the $(\sqrt{md},\sqrt{md})$-Dikin-amenable
Vaidya or $(d^{3/2},d^{3/2})$-Dikin-amenable Lewis-weight metric
instead, then $\gcdw$ with each metric recovers the $\Otilde(m^{1/2}d^{3/2})$
and $\Otilde(d^{5/2})$ mixing of the $\msf{Vaidya\ walk}$ and $\msf{Approximate\ John\ walk}$
\citep{chen2018fast} \emph{without} warmness. For a second-order
cone with linear constraints, we can use the Hessian of $2(\phi_{\textup{log}}+d\phi_{\textup{SOC}})$
that is $(m+d,m+d)$-Dikin-amenable, with which $\gcdw$ mixes in
$\Otilde(d\,(m+d))$ iterations in total. Lastly, for the PSD cone
with linear constraints, we can use the $(m+d^{3},m+d^{3})$-Dikin-amenable
$2\hess(\phi_{\textup{log}}+d^{2}\phi_{\textup{PSD}})$. $\gcdw$
with this needs $\Otilde(d^{2}(m+d^{3}))$ queries. For large $m$,
we use the $(d^{3},d^{3})$-Dikin-amenable $2(dg_{\textup{Lw}}+d^{2}\hess\phi_{\textup{PSD}})$,
with which $\gcdw$ mixes in $\Otilde(d^{5})$ iterations. In the
same setting, the $\bw$ needs $\Otilde(d^{6})$ queries.\\
For exponential sampling, $\gcdw$ requires the same number of iterations
of the $\dw$ for each case (i.e., polytope, second-order cone, PSD),
while the $\bw$ needs $\Otilde(d^{4})$ iterations for the polytope
and second-order cone, and $\Otilde(d^{8})$ iterations for the PSD
cone. Detailed statements on the mixing times and efficient per-step
implementation can be found in \S\ref{subsec:PSD-cone-sampling}.

\paragraph{Uniform sampling over hyperbolic cones.}

\citet{narayanan2016randomized} went beyond linear constraints and
analyzed the $\dw$ for uniform sampling over a convex region given
as the intersection of (1) linear constraints, (2) a hyperbolic cone
with a $\nu_{h}$-SC hyperbolic barrier $\phi_{h}$, and (3) a general
convex set with a $\nu_{s}$-SC barrier $\phi_{s}$. Using $\hess(\phi_{\textup{log}}+d\phi_{h}+d^{2}\phi_{s})$
as a local metric, this work shows that the $\dw$ mixes in $\O\bpar{d\bpar{m+d\nu_{h}+(d\nu_{s})^{2}}}$
steps from a warm start. The term $d(d\nu_{s})^{2}$ induced by self-concordance
alone is typically the largest one in the provable guarantee. Interesting
results of this work arise when $K$ is the intersection of (1) and
(2). Since a hyperbolic barrier is HSC \citep[Theorem 4.2]{guler1997hyperbolic},
the $d$-scaling of a HSC barrier makes it SSC, SLTSC, and SASC. Also,
as a $\nu_{h}$-SC hyperbolic barrier is $\O(\nu_{h})$-symmetric
(implied in \citet[\S4]{guler1997hyperbolic}), it follows that $d\phi_{h}$
is $(d\nu_{h},d\nu_{h})$-Dikin-amenable. Hence, $\phi_{\log}+d\phi_{h}$
induces an $(m+d\nu_{h},m+d\nu_{h})$-Dikin-amenable metric, and the
$\dw$ with this metric mixes in $\O(d\,(m+d\nu_{h}))$ iterations
from a warm start by Theorem~\ref{thm:Dikin}. Without warmness,
\citet{narayanan2016randomized} showed that the $\dw$ started at
$x\in K$, where $s\geq\nicefrac{|p|}{|q|}$ for any chord $\overline{pq}$
of $K$ passing through $x$, mixes in $\mc O\bpar{d(m+d\nu_{h})\bbrack{d\log\bpar{s(m+d\nu_{h})}+\log\frac{1}{\veps}}}$
steps. On the other hand, $\gcdw$ requires only $\O\bpar{d(m+d\nu_{h})\log\frac{d(m+d\nu_{h})}{\veps}}$
iterations.

\paragraph{Gaussian sampling.}

Going forward, we consider only logarithmic barriers for linear constraints.
The $\bw$ for general log-concave distributions mixes in $\Otilde(d^{4})$
iterations. As per our reduction, we first replace a quadratic potential
(coming from the Gaussian distribution) by a new variable, adding
its epigraph to a constraint. For a polytope, one can use the $(m+d,m+d)$-Dikin-amenable
$2\hess(\phi_{\textup{log}}+d\phi_{\textup{Gauss}})$, so $\gcdw$
needs $\Otilde(d\,(m+d))$ iterations of the $\dw$. For the second-order
cone with linear constraints, $\gcdw$ with the $(m+d,m+d)$-Dikin-amenable
metric $3\hess(\phi_{\textup{log}}+d\phi_{\textup{SOC}}+d\phi_{\textup{Gauss}})$
requires $\Otilde(d\,(m+d))$ iterations. For the PSD cone with linear
constraints, $\gcdw$ with the $(m+d^{3},m+d^{3})$-Dikin-amenable
metric $3\hess(\phi_{\textup{log}}+d^{2}\phi_{\textup{PSD}}+d^{2}\phi_{\textup{Gauss}})$
mixes in $\Otilde(d^{2}(m+d^{3}))$ iterations. The $\bw$ is much
slower, requiring $\Otilde(d^{8})$ iterations.

\paragraph{Entropy sampling. }

For a polytope, we use the $(m+d^{2},m+d^{2})$-Dikin-amenable $2\hess(\phi_{\textup{log}}+d\phi_{\textup{ent}})$
in $2d$-dimensional space. Thus, $\gcdw$ needs $\Otilde(d\,(m+d^{2}))$
iterations of the $\dw$. For the second-order cone with linear constraints,
$\gcdw$ with the $(m+d^{2},m+d^{2})$-Dikin-amenable $3\hess(\phi_{\textup{log}}+d\phi_{\textup{SOC}}+d\phi_{\textup{ent}})$,
requires in $\Otilde(d\,(m+d^{2}))$ iterations. Lastly, for the PSD
cone with linear constraints, $\gcdw$ with the $(m+d^{4},m+d^{4})$-Dikin-amenable
$3\hess(\phi_{\textup{log}}+d^{2}\phi_{\textup{PSD}}+d^{2}\phi_{\textup{ent}})$
mixes in $\Otilde(d^{2}(m+d^{4}))$ iterations. The $\bw$ mixes in
$\Otilde(d^{8})$ iterations in this setting.

\paragraph{Discussion.}

The inner loop of the sampling IPM samples from a distribution whose
potential is of the form $c^{\T}x+\alpha\phi(x)$. Thus, the study
of other non-Euclidean samplers for relatively convex and smooth potentials
will be interesting future work. Next, one question unanswered is
if the $d^{2}$-scaling of $\phi_{\textup{PSD}}$ can be improved,
which is mathematically interesting in its own right. The $d$-scaling
for ASC is shown through the random matrix theory, which is challenging
to extend to SASC (see Remark~\ref{rem:challenge-extension-SASC}). 

\subsection{Background and related work\label{subsec:related-work}}

Our problem \eqref{eq:problem} is a special case of \emph{logconcave
sampling}: sample from a distribution $\pi$ with density proportional
to $\exp(-V)$ for a convex function $V$ on $\Rd$. This problem
has spawned a long line of research in several communities, as it
captures various important distributions, including uniform distributions
over convex bodies and Gaussians.

A large body of recent work in machine learning and statistics makes
the assumption of $0\prec\alpha I\preceq\hess V\preceq\beta I$ on
$\Rd$ (i.e., $\alpha$-strong convexity and $\beta$-smoothness of
the potential $V$), where the strong-convexity assumption is sometimes
relaxed to isoperimetry assumptions such as log-Sobolev inequalities
(LSI), Poincar inequality (PI), and Cheeger isoperimetry. See \citet{chewi2023log}
for a survey on this topic. The guarantees provided on the mixing
time of samplers under this assumption have polynomial dependence
on the condition number defined as $\beta/\alpha$ (or $\alpha$ is
replaced by the isoperimetric constant). These guarantees do not apply
to constrained sampling. For example, in uniform sampling, the simplest
constrained sampling problem, $V$ is set to be a constant within
the convex body and infinity outside the body, which leads to discontinuity
of $V$ and $\beta=\infty$. The sudden change of $V$ around the
boundary requires special consideration, such as small step size,
use of a Metropolis filter, projection, etc., making it a more challenging
problem.

\paragraph{Uniform sampling.}

Uniform sampling can be accomplished through the $\bw$ (\citet{lovasz1993random,kannan1997random})
and $\msf{Hit\text{-}and\text{-}Run}$ (\citet{smith1984efficient}),
both of which only require access to a function proportional to the
density. When a convex body $K\subset\Rd$ satisfies $B_{r}(x_{0})\subset K\subset B_{R}(x_{0})$
for some $x_{0}$, the $\bw$ mixes in $\Otilde\bpar{d^{2}(R/r)^{2}}$
steps from warm start (\citet{kannan1997random}) and $\textsf{Hit-and-Run}$
mixes in $\Otilde\bpar{d^{2}(R/r)^{2}}$ steps from any start\footnote{In this section, \emph{warm start} means polynomial dependence on
the warmness parameter $M$, while \emph{any start} means poly-logarithmic
dependency on $M$. We assume any start unless specified otherwise.} (\citet{lovasz1999hit,lovasz2006hit}). \citet{lovasz2007geometry}
further extended these results to general logconcave distributions.
These algorithms need to use a ``step size'' of $\Omega(1/\sqrt{d})$,
and their mixing is affected by the skewed geometry of the convex
body (i.e., when $R/r\gg1$). The latter can be addressed by first
\emph{rounding} the body, after which the $\bw$ and the $\textsf{Hit-and-Run}$
mix in $\Otilde(d^{2})$ steps from a warm start, due to bounds on
the KLS constant by \citet{chen2021almost,klartag2023logarithmic}
and stochastic localization by \citet{chen2022hit}. The fastest rounding
algorithm by \citet{jia2021reducing} requires $\Otilde(d^{3})$ queries
to a membership oracle, and uses the $\bw$.


\paragraph{Sampling with local geometry.}

The $\bw$ uses the same radius ball for every point in the convex
body. One might want to use a different radius depending on the distance
to the boundary. This by itself does not work as it simply makes the
current point converge to the boundary. However, replacing balls with
ellipsoids whose shape changes based on the proximity to the boundary
does work. Several sampling algorithms are motivated by the use of
local metrics: the $\dw$ (\citet{kannan2012random}), $\msf{Riemannian\ Hamiltonian\ Monte\ Carlo}$
(RHMC), $\msf{Riemannian\ Langevin\ algorithm}$ (\citet{girolami2011riemann}),
etc.

Which local metrics would be suitable candidates? It turns out that
a suitable metric can be derived from self-concordant barriers, a
concept dating back to the development of the interior-point method
in convex-optimization literature (\citet{nesterov1994interior}).
It is well-known that any convex body admits an $d$-self-concordant
barrier such as universal barrier (\citet{nesterov1994interior,lee2021universal})
and entropic barrier (\citet{bubeck2014entropic,chewi2021entropic}),
but these are computationally expensive. Moreover, as noted in \citet{laddha2020strong},
the symmetry parameter of these general barriers is $\Omega(d^{2})$
for $d$-dimensional bodies (even for second-order cones), and so
the resulting complexity for the $\dw$ on a PSD cone is $\Omega(d^{2}\cdot d^{4})=\Omega(d^{6})$.
Thus, there is a need to find barriers that are more closely aligned
with the structure of sets we wish to sample. 

\paragraph{Polytope sampling.}

Samplers such as the $\bw$ and $\msf{Hit\text{-}and\text{-}Run}$
can be used to sample polytopes, but they do not really use any special
properties of polytopes. 

For polytopes with $m$ linear constraints in $d$-dimension ($m>d$),
the first theoretical result via self-concordant barriers dates back
to \citet{kannan2012random} which proposed the $\dw$ with the $m$-self-concordant
logarithmic barrier and established the mixing rate of $\Otilde(md)$
for uniform sampling. \citet{chen2018fast} revisited the idea of
\citet{vaidya1996new} using the $\mc O(\sqrt{md})$-self-concordant
hybrid barrier, which is a hybrid of the volumetric barrier and the
log barrier and leads to a faster interior-point method. They presented
the $\dw$ with the hybrid barrier giving an $\Otilde(\sqrt{m}d^{3/2})$-mixing
guarantee. Lastly, \citet{laddha2020strong} proposed the $\dw$ with
a variant of the $\mc O^{*}(d)$-self-concordant LS barrier based
on Lewis weights, developed by \citet{lee2019solving}, and showed
a mixing rate of $\otilde{d^{2}}$. 

While the next point proposed by all these Markov chains is obtained
by a Euclidean straight line step, the $\msf{Geodesic\ walk}$ and
RHMC use curves (geodesics and Hamiltonian-preserving curves respectively).
\citet{lee2017geodesic} and \citet{lee2018convergence} showed that
for uniform sampling, the $\msf{Geodesic\ walk}$ and RHMC with the
log barrier mix in $\otilde{md^{3/4}}$ and $\otilde{md^{2/3}}$ steps
respectively. \citet{kook2022condition} extended theoretical analysis
of RHMC to truncated exponential distributions and showed that discretization
of Hamilton's equations by practical numerical integrators maintains
a fast mixing rate. \citet{gatmiry2023sampling} showed that just
as the $\dw$ enjoys faster mixing via a barrier with a better self-concordance
parameter, RHMC with a hybrid barrier consisting of the Lewis weights
and log barrier mixes in $\otilde{m^{1/3}d^{4/3}}$ steps. Their proof
is based on developing suitable properties and algorithmic bounds
for Riemannian manifolds.

\paragraph{Generalization of the approach.}

Extending these non-Euclidean methods to general domains (e.g., $\psd$)
and to more general densities (e.g., Gaussian, relatively strong convex
and smooth) to potentially improve the complexity of the problem significantly
beyond the bounds that follow from general convex body sampling, have
been open research directions and motivate our paper. 

\citet{narayanan2016randomized} explored the first direction, analyzing
the $\dw$ for uniform sampling over the intersection of linear constraints,
a hyperbolic cone with a hyperbolic barrier, and a general convex
set with a SC barrier. Our current understanding of the second direction
is rather limited. A line of work has focused on the analysis of first-order
non-Euclidean samplers, such as discretized $\msf{Mirror\ Langevin\ algorithm}$
(MLA) or $\msf{Riemannian\ Langevin\ algorithm}$ (RLA) but under
strong assumptions. For example, \citet{li2022mirror} provided mixing-rate
guarantees of MLA under the \emph{modified self-concordance} of $\phi$
in the setting $\alpha\hess\phi\preceq\hess f\preceq\beta\hess\phi$.
However, the modified self-concordance is not affine-invariant, so
it does not correctly capture affine-invariance of the algorithm.
\citet{ahn2021efficient,gatmiry2022convergence} avoid the modified
self-concordance, analyzing MLA and RLA under an alternative discretization
scheme that requires an exact simulation of the Brownian motion $\hess\phi(X_{t})^{-1/2}\,\D W_{t}$
which is not known to be achievable algorithmically. \citet{gopi2023algorithmic}
proposed a non-Euclidean version of the proximal sampler based on
the log-Laplace transformation (LLT) and analyzed its mixing when
a potential is strongly convex and \emph{Lipschitz} (not smooth) relatively
in $\hess\phi$. However, the LLT has no closed form in general. Recently,
\citet{srinivasan2023fast} analyzed the Metropolis-adjusted MLA under
the relative Lipschitzness of the potential  (i.e., $\norm{\nabla f}_{[\hess\phi]^{-1}}<\infty$)
in addition to the relative convex and smoothness.

Our study of the $\dw$ for general cones and general densities provides
a rather complete picture of zeroth-order non-Euclidean samplers.
It also provides a general framework and improved bounds as well as
a ``handbook'' for structured sampling.

\subsection{Preliminaries and notation\label{subsec:prelim}}

\paragraph{Basics.}

For $n\in\mathbb{N}$, let $[n]:=\{1,\cdots,n\}$. We use $f\lesssim g$
to denote $f\leq cg$ for some universal constant $c>0$. The $\widetilde{\mc O}$
complexity notation suppresses poly-logarithmic factors and dependence
on error parameters. For $a,b\in\Rd$, we denote $a\wedge b:=\min(a,b)$
and $a\vee b:=\max(a,b)$. For $v\in\Rd$, the Euclidean norm (or
$\ell_{2}$-norm) is denoted by $\snorm v_{2}\defeq\sqrt{\sum_{i\in[d]}v_{i}^{2}}$,
and the infinity norm is denoted by $\snorm v_{\infty}\defeq\max_{i\in[d]}|v_{i}|$.
A Gaussian distribution with mean $\mu\in\Rd$ and covariance $\Sigma\in\Rdd$
is denoted by $\mc N(\mu,\Sigma)$.

\paragraph{Matrices.}

We use $\mbb S^{d}$ to denote the set of symmetric matrices of size
$d\times d$. For $X\in\mbb S^{d}$, we call it \emph{positive semidefinite}
(PSD) (resp. \emph{positive definite} (PD)) if $h^{\T}Xh\geq0$ ($>0)$
for any $h\in\Rd$. We use $\psd$ to denote the set of positive definite
matrices of size $d\times d$. Note that their effective dimension
is $d_{s}:=d(d+1)/2$ due to symmetry. For a positive (semi) definite
matrix $X$, its \emph{square root} is denoted as $X^{\half}$, and
is the unique positive (semi) definite matrix satisfying $X^{\half}X^{\half}=X$.
For $A,B\in\mbb S^{d}$, we use $A\preceq B$ ($A\prec B$) to indicate
that $B-A$ is PSD (PD). For a matrix $A\in\Rdd$, its \emph{trace}
is denoted by $\tr(A)=\sum_{i=1}^{d}A_{ii}$. The \emph{operator norm}
and \emph{Frobenius norm} are denoted by $\snorm A_{2}\defeq\sup_{x\in\Rd}\snorm{Ax}_{2}/\snorm x_{2}$
and $\snorm A_{F}\defeq\bpar{\sum_{i,j=1}^{d}A_{ij}^{2}}^{1/2}=\sqrt{\tr(A^{\T}A)}$,
respectively.

\paragraph{Basic operations.}

For $X\in\mbb S^{d}$, its \emph{vectorization} $\vec{(}X)\in\R^{d^{2}}$
is obtained by stacking each column of $X$ vertically. Its symmetric
vectorization $\svec(X)\in\R^{d_{s}}$ is obtained by stacking the
lower triangular part in vertical direction. For a matrix $A\in\Rdd$
and vector $x\in\Rd$, we use $\diag(A)$ to denote the vector in
$\Rd$ with $[\diag(A)]_{i}=A_{ii}$ for $i\in[d]$, $\Diag(A)$ to
denote the diagonal matrix with $[\Diag(A)]_{ii}=A_{ii}$ for $i\in[d]$
and $\Diag(x)$ to denote the diagonal matrix in $\Rdd$ with $[\Diag(x)]_{ii}=x_{i}$
for $i\in[d]$.

\paragraph{Matrix operations.}

For matrices $A,B\in\Rdd$, their inner product is defined as the
inner product of $\vec{(}A)$ and $\vec{(}B)$, denoted by $\langle A,B\rangle=\tr(A^{\T}B)$.
Their \emph{Hadamard product} $A\circ B$ is the matrix of size $d\times d$
defined by $(A\hada B)_{ij}=A_{ij}B_{ij}$ (i.e., obtained by element-wise
multiplication). For $A\in\R^{p\times q}$ and $B\in\R^{r\times s}$,
their \emph{Kronecker product} $A\kro B$ is the matrix of size $pr\times qs$
defined by 
\[
A\otimes B=\left[\begin{array}{ccc}
A_{11}B & \cdots & A_{1q}B\\
\vdots &  & \vdots\\
A_{p1}B & \cdots & A_{pq}B
\end{array}\right]\,,
\]
where $A_{ij}B$ is the matrix of size $r\times s$ obtained by multiplying
each entry of $B$ by the scalar $A_{ij}$. 

\paragraph{Projection matrix, Leverage score and Lewis weights.}

For a full-rank matrix $A\in\R^{m\times d}$ with $m\geq d$, we recall
that $P(A):=A(A^{\T}A)^{-1}A^{\T}$ is the orthogonal projection matrix
onto the column space of $A$. The leverage scores of $A$ is denoted
by $\sigma(A):=\diag\bpar{P(A)}\in\R^{m}$. We let $\Sigma(A):=\Diag\bpar{\sigma(A)}=\Diag\bpar{P(A)}$
and $P^{(2)}(A):=P(A)\circ P(A)$. The $\ell_{p}$-Lewis weights of
$A$ is denoted by $w(A)$, the solution $w$ to the equation $w(A)=\diag\bpar{W^{\nicefrac{1}{2}-\nicefrac{1}{p}}A(A^{\T}W^{1-\nicefrac{2}{p}}A)^{-1}A^{\T}W^{\nicefrac{1}{2}-\nicefrac{1}{p}}}\in\R^{m}$
for $W=\Diag(w)$. When $m<d$ or $A$ is not full rank, both leverage
scores and Lewis weights can be generalized via the Moore-Penrose
inverse in place of the inverse in the definitions.

\paragraph{Derivatives.}

For a function $f:\Rd\to\R$, let $\grad f(x)\in\Rd$ denote the gradient
of $f$ at $x$ (i.e., $[\nabla f(x)]_{i}=\pderiv f{x_{i}}(x)$) and
$\hess f(x)\in\Rdd$ denote the Hessian of $f$ at $x$ (i.e., $[\hess f(x)]_{ij}=\frac{\de^{2}f}{\de x_{i}\de x_{j}}(x)$).
For a matrix function $g:\Rd\to\Rdd$ in $x$, we use $\Dd g$ and
$\Dd^{2}g$ to denote the third-order and fourth-order tensor defined
by $[\Dd g(x)]_{ijk}=\frac{\de[g(x)]_{ij}}{\de x_{k}}$ and $[\Dd^{2}g(x)]_{ijkl}=\frac{\de^{2}[g(x)]_{ij}}{\de x_{k}\de x_{l}}$.
We use the following shorthand notation: $g_{x,h}':=\Dd g(x)[h]$
and $g_{x,h}'':=\Dd^{2}g(x)[h,h]$, where $\Dd^{i}g(x)[h_{1},\dotsc,h_{i}]=\Dd^{i}g(x)[h_{1}\otimes\cdots\otimes h_{i}]$
denote the $i$-th directional derivative of $g$ at $x$ in directions
$h_{1},\dotsc,h_{i}\in\Rd$, i.e.,
\[
\Dd^{i}g(x)[h_{1},\dotsc,h_{i}]=\frac{\D^{i}}{\D t_{1}\cdots\D t_{i}}g\Bpar{x+\sum_{j=1}^{i}t_{j}h_{j}}\bigg|_{t_{1},\dotsc,t_{i}=0}\,.
\]


\paragraph*{Local norm.}

At each point $x$ in a set $K\subset\Rd$, a \emph{local metric}
$g$, denoted as $g_{x}$ or $g(x)$, is a positive-definite inner
product $g_{x}:\Rd\times\Rd\to\R$, which induces the local norm as
$\snorm v_{g(x)}:=\sqrt{g_{x}(v,v)}$. We use $\snorm v_{x}$ to refer
to $\snorm v_{g(x)}$ when the context is clear. When an ambient space
has an orthonormal basis as in our setting (e.g., $\{e_{1},\dots,e_{d}\}$),
the local metric $g_{x}$ can be represented as a positive-definite
matrix of size $d\times d$. In this case, we abuse notation by using
$g(x)$ to indicate the $d\times d$ positive-definite matrix represented
with respect to such an orthonormal basis. Also, the inner product
can be written as $g_{x}(v,w)=v^{\T}g(x)w$. Going forward, we use
$g_{x}=g(x)$ to denote a local metric (or positive definite matrix
of size $\dim(x)\times\dim(x)$) at each point $x\in K$. The local
metric $g$ is assumed to be at least twice differentiable.

\paragraph{Markov chains.}

We use the same symbol for a distribution and its density with respect
to the Lebesgue measure. Many sampling algorithms are based on \emph{Markov
chains}. A \emph{transition kernel} $P:\Rd\times\mc B(\Rd)\to\R_{\geq0}$
(or \emph{one-step distribution}) for the Borel $\sigma$-algebra
$\mc B(\Rd)$ quantifies the probability of the Markov chains transitioning
from one point to another measurable set. The next-step distribution
is defined by $P_{x}(A):=P(x,A)$, which is the probability of a step
from $x$ landing in the set $A$. The transition kernel characterizes
the Markov chain in the sense that if a current distribution is $\mu$,
then the distribution after $n$ steps can be expressed as $\mu P^{(n)}$,
where $\mu P^{(i)}(x):=\int_{\Rd}P(\cdot,x)\,\mu P^{(i-1)}$ is defined
recursively for $i\in[n]$ with the convention $\mu P^{(0)}=\mu$.
We call $\pi$ a \emph{stationary distribution} of the Markov chain
if $\pi=\pi P$. If the stationary distribution further satisfies
$\int_{A}P(x,B)\,\pi(\D x)=\int_{B}P(x,A)\,\pi(\D x)$ for any two
measurable subsets $A,B$, then the Markov chain is said to be \emph{reversible}
with respect to $\pi$.

It is expected that the Markov chain approaches the stationary distribution.
We measure this with the \emph{total variation distance} (TV-distance):
for two distributions $\mu$ and $\pi$ on $\Rd$, the TV-distance
is defined as $\dtv(\mu,\pi)\defeq\sup_{A\in\mc B(\Rd)}|\mu(A)-\pi(A)|=\half\textint_{\Rd}\big|\frac{\D\mu}{\D x}-\frac{\D\pi}{\D x}\big|\,\D x$,
where the last equality holds when the two distributions admit densities
with respect to the Lebesgue measure on $\Rd$. We also recall other
probabilistic distances: when $\mu\ll\nu$,
\begin{align*}
\text{The chi-squared divergence\ } & \chi^{2}(\mu\mmid\nu)\defeq\int\bpar{\frac{\D\mu}{\D\nu}-1}\,\D\nu\,,\\
L^{2}\text{-distance\ } & \snorm{\mu/\nu}\defeq\int\frac{\D\mu}{\D\nu}\,\D\mu=\chi^{2}(\mu\mmid\nu)+1\,.
\end{align*}
Moreover, the rate of convergence can be quantified by the \emph{mixing
time}: for an error parameter $\veps\in(0,1)$ and an initial distribution
$\pi_{0}$, the mixing time is defined as the smallest $n\in\N$ such
that $\dtv(\pi_{0}P^{(n)},\pi)\leq\veps$. In this paper, we consider
a \emph{lazy} Markov chain, which does not move with probability $\texthalf$
at each step, in order to avoid a uniqueness issue of a stationary
distribution. Note that this change worsens the mixing time by at
most a factor of $2$. One of the standard tools to control progress
made by each iterate is the \emph{conductance} $\Phi$ of the Markov
chain with its stationary distribution $\pi$, defined by
\[
\Phi\defeq\inf_{\text{measurable }S}\frac{\int_{S}P(x,S^{c})\,\pi(\D x)}{\pi(S)\wedge\pi(S^{c})}\,.
\]
Another crucial factor affecting the convergence rate is geometry
of the stationary distribution $\pi$, as measured by \emph{Cheeger
isoperimetry} 
\[
\psi_{\pi}\defeq\inf_{\text{measurable }S}\frac{\lim_{\delta\to0^{+}}\frac{1}{\delta}\pi\bpar{\{x:\,0<d(S,x)\leq\delta\}}}{\pi(S)\wedge\pi(S^{c})}\,,
\]
 where $d(S,x)$ is some distance between $x$ and the set $S$.

\paragraph{Full definition of self-concordance.}
\begin{defn}
[Self-concordance] \label{def:sc} For convex $K\subset\Rd$, let
$\phi:\intk\to\R$ be a convex function, $g:\intk\to\psd$ a PSD matrix
function, and $\mc N_{g}^{r}(x):=\mc N\bpar{x,\frac{r^{2}}{d}g(x)^{-1}}$.
\begin{itemize}
\item \emph{Self-concordance} (SC): A $C^{3}$-function $\phi$ is called
a self-concordant barrier if $|\Dd^{3}\phi(x)[h,h,h]|\leq2\snorm h_{\hess\phi(x)}^{3}$
for any $x\in\intk$ and $h\in\Rd$, and $\lim_{x\to\de K}\phi(x)=\infty$.
The first condition is equivalent to $-2\snorm h_{\hess\phi(x)}\hess\phi(x)\preceq\Dd^{3}\phi(x)[h]\preceq2\snorm h_{\hess\phi(x)}\hess\phi(x)$.
We call it a $\nu$-self-concordant barrier for $K$ if $\sup_{h\in\Rd}(2\langle\grad\phi(x),h\rangle-\snorm h_{\hess\phi(x)}^{2})\leq\nu$
for any $x\in\intk$ in addition to self-concordance. A $C^{1}$-PSD
matrix function $g:\intk\to\psd$ is called self-concordant if $-2\snorm h_{g(x)}g\preceq\Dd g(x)[h]\preceq2\snorm h_{g(x)}g$
for any $x\in\intk$ and $h\in\Rd$, and there exists a self-concordant
function $\phi:\intk\to\R$ such that $\hess\phi\asymp g$ on $\intk$.
We call it a $\nu$-self-concordant barrier for $K$ if its counterpart
$\phi$ is $\nu$-self-concordant.
\item \emph{Highly self-concordant function} (HSC): A $C^{4}$-function
$\phi$ is called highly self-concordant if $|\Dd^{4}\phi(x)[h,h,h,h]|\leq6\snorm h_{\hess\phi(x)}^{4}$
for any $x\in\intk$ and $h\in\Rd$, and $\lim_{x\to\de K}\phi(x)=\infty$.
\item \emph{Strong self-concordance} (SSC): A SC matrix function $g$ is
called strongly self-concordant if $g$ is PD on $\intk$ and $\snorm{g(x)^{-\nicefrac{1}{2}}\Dd g(x)[h]\,g(x)^{-\nicefrac{1}{2}}}_{F}\leq2\snorm h_{g(x)}$
for any $x\in\intk$ and $h\in\Rd$. We call a SC function $\phi$
strongly self-concordant if $\hess\phi(x)$ is strongly self-concordant.
\item \emph{Lower trace self-concordant matrix} (LTSC): A SC matrix function
$g$ is called lower trace self-concordant if $g$ is PD on $\intk$
and $\tr\bpar{g(x)^{-1}\Dd^{2}g(x)[h,h]}\geq-\snorm h_{g(x)}^{2}$
for any $x\in\intk$ and $h\in\Rd$. We call it strongly lower trace
self-concordant (SLTSC) if for any PSD matrix function $\bar{g}$
on $\intk$ it holds that $\tr\bpar{\bpar{\bar{g}(x)+g(x)}^{-1}\Dd^{2}g(x)[h,h]}\geq-\snorm h_{g(x)}^{2}$
for any $x\in\intk$ and $h\in\Rd$.
\item \emph{Average self-concordance} (ASC): A matrix function $g$ is called
average self-concordant if for any $\veps>0$ there exists $r_{\veps}>0$
such that $\P_{z\sim\mc N_{g}^{r}(x)}\bpar{\snorm{z-x}_{g(z)}^{2}-\snorm{z-x}_{g(x)}^{2}\leq\frac{2\veps r^{2}}{d}}\geq1-\veps$
for $r\leq r_{\veps}$. We call it strongly average self-concordant
(SASC) if for $\veps>0$ and any PSD matrix function $\bar{g}$ on
$\intk$ it holds that $\P_{z\sim\mc N_{g+\bar{g}}^{r}(x)}\bpar{\snorm{z-x}_{g(z)}^{2}-\snorm{z-x}_{g(x)}^{2}\leq\frac{2\veps r^{2}}{d}}\geq1-\veps$
for $r\leq r_{\veps}$.
\end{itemize}
\end{defn}


