\section{Related Work}
\label{sec:related}

In this section, we review literature relevant to eye tracking calibration techniques and gaze typing methods which informed the development of our autocalibration approach. We discuss prior works with respect to calibration methods for eye trackers and adaptive gaze typing techniques. 




\subsection{Calibration Methods for Eye Trackers}
Calibration is a crucial aspect of eye tracking systems,
 
mapping gaze points to locations in the field of view, requiring accurate data from eye trackers. However, it can be time-consuming, intrusive, and needs to be repeated frequently to maintain accuracy, as eye-tracking performance can degrade over time due to factors such as user movement or changes in ambient lighting \citep{jacob2003eye}.
Various calibration methods have 
been proposed to improve the accuracy and user experience of eye tracking devices. Traditional calibration techniques \citep{harezlak2014towards} involve the user following a series of on-screen targets, such as dots or markers, while the eye tracker records the user's gaze data (Fig. \ref{fig:calibration}). These methods, while effective, can be time-consuming and uncomfortable for some users, particularly those with motor impairments \citep{jacob2003eye}. 

\citet{santini2017calibme} propose a technique for eye tracker calibration via dynamic fiducial markers placed on the screen, gathering a large array of calibration points
in a fast and unsupervised manner. While this approach can cover more areas of the screen compared to a 9-point calibration system and it can account for natural variances in the way different individuals' eyes track moving objects, it  relies on the availability of specific markers for explicit user calibration. In comparison, our approach leverages natural elements of the interface such as the text box displaying the typed text which enables implicit calibration (without the user being aware that it is taking place). \citet{sugano2015self} propose a self-calibrating approach for eye trackers based on a computational model of bottom-up visual saliency. This work assumes that the user's gaze fixations always lie inside a small cluster of salient regions in the egocentric view of the user. While this approach is implicitly adaptive and leverages natural junctures of the user's visual view on a screen or the 3D environment, it is data intensive for accurate autocalibration. In comparison, our approach relies on a natural elements of the user interface and has the potential to autocalibrate quickly even with one reading attempt.



\subsection{Adaptive Gaze Typing Techniques}
Adaptive techniques for gaze typing aim to improve the efficiency and user experience of gaze-based text entry by dynamically adjusting system parameters based on the user's performance and gaze behavior. 
To enable widespread adoption of gaze typing technologies for AR applications, \citet{schenkluhn2022look} emphasize the need to create gaze typing that is proactively adapting dwell time instead of retrospectively reacting to user fatigue. This would enable users to type short texts at their peak performance and economically utilizing cognitive resources for long texts. \citet{mott2017cascading} proposed a cascading dwell technique that automatically adjusts the dwell time for gaze-based input based on the user's performance. This approach has been shown to improve typing speeds and reduce errors in text entry tasks, highlighting the importance of dynamic adjustments in gaze-based input systems. \citet{chen2021adaptive} propose an adaptive gaze typing system using a computational model of the control of eye movements in gaze-based selection. They formulate the model as an optimal sequential planning problem bounded by the limits of the human visual and motor systems and use reinforcement learning to approximate optimal solutions for number of fixations and duration required to make a gaze-based selection. While these approaches present adaptive learning based solutions for dwell-time customization, we propose a technique which adaptively corrects for miscalibration of an eye tracker during gaze typing. 

\citet{gao2022x2t} present an adaptive learning approach which adaptively calibrates an RGB-based eye tracker used for display screens. However, they assume the user can provide click-based feedback in the learning system in the form of backspaces activated on a physical keyboard/device. In comparison, our method only relies on feedback from the gaze trajectory available on the visual keyboard and does not use any external hardware other than the eye tracker.

