\section{Introduction}

Eye tracking technology has emerged as a valuable tool in a variety of applications, including accessibility, augmented reality (AR), virtual reality (VR), and even gaming \citep{morimoto2005eye}. It has been particularly beneficial for individuals with motor impairments who rely on gaze-based input methods for communication and device control \citep{zhang2017smartphone, mott2017cascading}. Gaze typing, a common use case of eye tracking, enables users to input text by looking at keys of an on-screen keyboard, thereby offering a hands-free and non-vocal method of communication.% (Fig.  \ref{fig:gazetyping}). 
However, the accuracy and efficiency of gaze typing depend heavily on the calibration of the eye tracker. Calibration is a process that establishes a relationship between the user's gaze and the corresponding screen coordinates (Fig. \ref{fig:calibration}). It can be time-consuming, tedious, and in some cases, uncomfortable for the user \citep{zhang2017smartphone}.



% Figure environment removed





Moreover, users' head movements, eye fatigue, changes in lighting and other environmental conditions, and hardware inconsistencies lead to miscalibration over time, which can significantly impact the performance of gaze typing systems \citep{jacob2003eye}. This problem is further exacerbated for users with motor impairments who may have difficulty maintaining a stable head position or participating in traditional calibration procedures. Reportedly, ALS users recalibrate their eye trackers more than 15-20 times each day. Consequently, there is a pressing need for calibration methods that are not only accurate but also user-friendly and adaptive to changes in the user's condition and environment.

We introduce autocalibrated gaze tracking, a novel approach to calibrating eye trackers during gaze typing. Specifically, we leverage differences in gaze behavior during reading versus typing, demonstrated in Figure \ref{fig:insight}. When typing, users compensate and autocorrect for miscalibration in eye tracking software by purposefully glancing at adjacent keys to activate the key of interest. While such behavior adds additional cognitive load for users, it can enable them to type despite some miscalibration. Such behavior also confounds the detection of miscalibration or any struggle from the user's gaze behavior. However, when users read the typed text, they do not compensate for miscalibration, providing a signal for detecting the calibration offset. We leverage this insight to track a user's reading behavior and compare their gaze to the location of typed characters on the screen to estimate the miscalibration amount and direction. The proposed technique aims to improve the gaze typing experience by continuously adjusting the calibration in real-time based on the user's gaze behavior, thereby reducing the need for manual recalibration and offering a more natural and efficient interaction. Our approach can potentially benefit a wide range of users, including those with motor impairments who use gaze keyboards for everyday communication and others who use gaze typing systems for extended periods, such as gamers and virtual/augmented reality headset users.

The paper proceeds as follows. In Section \ref{sec:related}, we review related work on calibration for eye trackers, other adaptive gaze typing techniques, and machine learning applications for eye tracking. In Section \ref{sec:interface}, we describe the system setup and gaze typing interface used. In Section \ref{sec:algo}, we outline the implementation details of our autocalibrated gaze typing technique and in Section \ref{sec:user-study}, we present the design of our in-lab user study procedure and evaluation metrics. 
Our results from the in-lab user study with 15 able-bodied participants, reported in Section \ref{sec:results}, show that the proposed technique significantly reduces typing errors, improves typing speed, and enhances user satisfaction compared to a standard static calibration approach. 
We discuss the implications of these findings for the design of future gaze typing systems and other future work in Section \ref{sec:discussion} and provide a conclusion in Section \ref{sec:conclusion}. %\ref{sec:future}.
