\section{Discussion}
\label{sec:discussion}

Autocalibrating eye trackers, without introducing additional tasks for the end user, represents a step towards building more seamless user experiences. Gaze data offers vast potential as a form of input for computer interaction, effectively making technology more accessible and user-friendly. Coupled with methods for autocalibration, an array of seamless gaze-controlled interactions may become possible. 

While our technique introduces new possibilities for making interactions more seamless, it also has several limitations. Similar to several gaze-controlled systems, our technique relies heavily on users having clear, unobstructed vision and the capacity to make smooth eye movements across the visual keyboard, which may not always be the case, particularly for users with specific ocular or neurological conditions. Additionally, eye movements during reading and writing are influenced by numerous cognitive and physiological factors, such as attention, fatigue, and cognitive load. These factors can cause dynamic changes in eye movement behavior, potentially affecting the computation of the miscalibration offset and hence the calibration accuracy. 

Moreover, the application of this approach within the ALS community or other individuals who might rely on this technology on a daily basis necessitates a deeper understanding of the user group. While our results suggest that the proposed autocalibration approach significantly eases the calibration process and user experience for able-bodied participants, the comfort, ease-of-use, and adaptation to individual user needs for disabled populations are factors that must be evaluated and explored in future work.


We envision the potential of combining our reading-based autocalibration technique with offset computations from most likely next character predictions. Powerful language models can provide more flexibility and adaptability to user-specific typing and reading patterns through next character or next word predictions, thereby providing avenues to develop a more personalized gaze typing experience. Researching this approach could also lead to a better understanding of the cognitive processes involved in gaze typing and eye movement in general. 

Furthermore, as we move towards a future where human-computer interaction becomes increasingly multifaceted, incorporating a higher bandwidth of input signals like speech, gestures, and facial expressions becomes essential. The autocalibration approach, by enhancing the accuracy and efficiency of eye tracking, can contribute towards creating more holistic, powerful, and intuitive interfaces especially for real-world applications like augmented reality (AR) and virtual reality (VR). With increasing reliance on these technologies for various purposes – from gaming to professional training – the need for seamless and efficient interfaces is paramount. Our work can contribute towards this goal by offering more accurate eye-tracking, thereby improving the user's interaction with the AR/VR environment. %