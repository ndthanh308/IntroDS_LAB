
\section{Autocalibration Algorithm}
\label{sec:algo} 

Current eye-gaze systems enable users to pause the task being performed with their gaze and manually recalibrate if they think their calibration is off. We propose an approach that automatically recalibrates \textit{while the person is typing} without the need for manual recalibration, for a more seamless user experience.

Often when an eye tracker's miscalibration is detected by a user during the typing task, they can attempt to compensate for the offset in the detected gaze coordinates to type the characters of interest. If this is manageable, a user can tradeoff the time and effort necessary for manual recalibration with some cognitive load of compensation to accomplish the task at hand. However, when users look up at the text box to read the text they have typed, they do not need to compensate for miscalibration and only observe the typed characters. This difference provides a free calibration signal during reading, as it is not influenced by the miscalibration that may occur during typing. By monitoring the difference in users' eye movements during typing versus reading, our technique dynamically adjusts the calibration in real-time, compensating for miscalibration errors. 

\begin{algorithm}
\caption{Autocalibration for Eye Tracking during Gaze Typing}
\begin{algorithmic}[1]
\REQUIRE{Stream of gaze coordinates $G$ detected by an eye tracker, window size $w$ for running average, detected calibration error bound $b$, step size $\eta$ for calibration zone radius updates, initial threshold $\tau_\mathrm{init}$, threshold bounds $\tau_{\mathrm{min}}, \tau_{\mathrm{max}}$, typing speed threshold $s_\mathrm{fluent}$, y-coordinate for lower end of text box $y_\mathrm{text\_box\_bottom}$}
\STATE Initialize $\tau := \tau_\mathrm{init}$ 
\COMMENT{threshold pixel distance of gaze coordinates from last typed character}\\
\STATE Initialize $n_\mathrm{char} := 0$ \COMMENT{number of characters visible in text box}\\
\STATE Initialize $s := 0$ \COMMENT{typing speed in characters per minute}
\STATE Initialize $\epsilon_{x_0} := 0, \epsilon_{y_0} := 0$ \COMMENT{calibration error in $x$ and $y$ directions}\\
\FOR{raw gaze coordinates $(x_t, y_t) \in G: $}
\STATE Receive $n_\mathrm{char}, s$ from typing application
\IF{$y < y_\mathrm{text\_box\_bottom}$  \textbf{and} $n_\mathrm{char} > 0$}
\STATE{Receive ($x_c, y_c$) from typing application} \COMMENT{location of last typed character}
\STATE $d := \sqrt{(x_c-x_t)^2 + (y_c-y_t)^2}$\\
\STATE{Detect fixation}
\IF{fixation is True \textbf{and} $d < \tau$}
\STATE{$\epsilon_{x_t} := x_c-x_t, \quad \epsilon_{y_t} := y_c-y_t$}
\STATE{$\epsilon_{x_t} := \frac{1}{\min(w,t)} \sum_{j=\max(0,t-w+1)}^{t} \epsilon_{x_j}$, \quad $\epsilon_{y_t} := \frac{1}{\min(w,t)} \sum_{j=\max(0,t-w+1)}^{t} \epsilon_{y_j}$}\\
\STATE{$\epsilon_{x_t} := \min(b,\max(-b,\epsilon_{x_t})), \quad \epsilon_{y_t} := \min(b,\max(-b,\epsilon_{y_t}))$} \COMMENT{clip the estimated error}\\
\IF{key activation \textbf{and} $s \geq s_\mathrm{fluent}$}
\STATE{$\tau := \max(\tau-\eta, \tau_\mathrm{min})$} \COMMENT{narrow the calibration zone}\\
\ENDIF
\ENDIF
\ELSE
\STATE{$\epsilon_{x_t} := \epsilon_{x_{t-1}}, \quad \epsilon_{y_t} = \epsilon_{y_{t-1}}$}
\IF{key activation \textbf{and} $s < s_\mathrm{fluent}$}
\STATE{$\tau := \min(\tau+\eta, \tau_\mathrm{max})$} \COMMENT{expand the calibration zone}\\
\ENDIF
\ENDIF
\STATE{$\widehat x_t := x+\epsilon_{x_t}, \widehat y_t := y+\epsilon_{y_t}$}
\STATE \textbf{return} calibrated gaze coordinates $(\widehat x_t, \widehat y_t)$
\ENDFOR
\end{algorithmic}
\label{alg:main}
\end{algorithm}

The user's gaze is assumed to be directed towards the text box when it falls above the keyboard layout and within a certain threshold distance from the center of the text typed. This threshold distance $\tau$ is incrementally decreased for robustness as the system corrects the miscalibration and the user successfully types characters on the screen. It is incrementally increased again if the user struggles to type characters with repeated miscalibration. If the gaze is detected within this area (calibration zone), the system then identifies whether the user's gaze is in a state of fixation or saccade. Visual fixations maintain the focus of gaze on a single location. To detect fixations, we consider spatial and temporal criteria from the taxonomy of fixation identification algorithms described by \citet{salvucci2000identifying}. We use velocity-based criteria under spatial characteristics and duration based criteria under temporal characteristics to filter out fixations from saccades. We first filter out eye movements with very high speeds (a large distance traversed over a very short period of time is likely a saccade). If gaze continues to not be classified as a saccade for more than 100 ms, we consider it a fixation.

We assume that the user reads the last typed character when they look up to read and detect the offset in calibration accordingly. 
Our system computes a moving average of the calibration offsets, 
which enables the calibration to be updated continuously and smoothly in real-time. 
The system continuously updates the calibration error based on the user's gaze behavior while reading the typed text. The correction to the detected miscalibration is continuously applied when the user's gaze moves away from the text box (i.e. while typing on the visual keyboard). The autocalibrated gaze coordinates are displayed to the user with the updated location of the red gaze cursor on the screen (Fig. \ref{fig:activation}(a)) %Similar to the static calibration phase, typing speed and accuracy were recorded for each participant.
The approach is detailed in Algorithm \ref{alg:main}.
We instantiate this algorithm with calibration zone size parameters 
$\tau_\mathrm{init} = 150, \tau_{\mathrm{min}} = 50, \tau_{\mathrm{max}} = 200$, typing speed threshold $s_\mathrm{fluent} = 5$, window size $w=64$, calibration error bound $b=200$, and step size $\eta=25$.