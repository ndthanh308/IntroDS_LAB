\section{User Study}
\label{sec:user-study}

To evaluate the effectiveness of our autocalibrated gaze typing technique, we ran an in-lab user study with IRB approval. We compared our approach to a standard manual calibration approach, and used a Tobii PCEye eye tracker to collect gaze data. All manual calibrations were performed via the standard Tobii SDK 9-point calibration procedure (Fig. \ref{fig:calibration}). Participants typed a set of 5 phrases each for two gaze typing systems, but were not made aware of any differences between the two. Informed consent was obtained from each participant before the study commenced.

\subsection{Participants}

We recruited 15 participants (9 male, 6 female) aged 18-55 with no prior experience in gaze typing. All participants were sighted (i.e. they did not need glasses, or their vision was correctable with glasses) and did not report any eye conditions. Two participants wore corrective eye glasses during their session.

\subsection{System Setup}
\label{sec:interface}

% Figure environment removed


Our experimental setup consists of both hardware and software components to effectively evaluate the performance of the proposed autocalibration technique with real users. We utilized a Tobii PCEye eye tracker, with a sampling rate of 60 Hz, connected to a standard Windows laptop via USB. The eye tracker tracked gaze data directed towards a 24-inch screen that displayed an on-screen visual keyboard (Fig. \ref{fig:activation}).  
The keyboard was designed as part of an UWP application built on top of the Microsoft Gaze Interaction Library \citep{gil}. The application was written using the Tobii Pro SDK (designed to offer access to gaze data from Tobii eye trackers) in C\#. It facilitates the capture and processing of x, y coordinates from the eye tracker, providing necessary data for our autocalibration technique.
The software for our autocalibrated gaze typing technique was developed in Python, which received the 2D gaze coordinates in real-time from the UWP application via Google's remote procedure call (RPC) protocol. This allowed Python scripts to access gaze data, analyze it, and apply the necessary miscalibration corrections which could be displayed back on the visual keyboard application. 

% Figure environment removed

The customized gaze typing application is functionally similar to visual keyboards available as part of standard software such as Windows Eye Control. However, it serves as a test bed with more control to process and update the miscalibrated gaze coordinates in real-time. Figure \ref{fig:activation} illustrates the mechanism for typing a character on the visual keyboard through eye gaze. A user's detected gaze location is displayed as a red dot on the screen. The user can direct their gaze at a desired character for typing. If they fixate on a key for $50$ ms, a dwell timer of $400$ms is initiated by the system. The start of the timer is depicted by a green rectangle on the key of interest (Fig. \ref{fig:activation}(a)). If the user continues to look at the same key, the rectangle slowly changes size and decreases in area over time, eventually collapsing at the center of the key (Fig. \ref{fig:activation}(b)). Otherwise, the timer is aborted and the user must fixate and dwell again to type any character subsequently. The dwell timer is a mechanism to ensure that a user is intentionally willing to type that specific character. Once the timer is completed, the user receives visual feedback (the key turns red as shown in Fig. \ref{fig:activation}(c)) that the character is typed on the text box at the top of the screen. The eye tracker can be calibrated via a standard Tobii software available with the purchase of the tracker (Fig.\ref{fig:calibration}). It uses a procedure of following at a set of dots (in sequence) on the screen that, and then return to controlling an application of interest with their calibrated gaze.



\subsection{Procedure}
Participants were initially introduced to gaze typing and given a brief tutorial on how to use the on-screen keyboard. The experiment consisted of an initial calibration procedure, a practice round, and two experimental phases: a static calibration phase and an autocalibrated phase (all described below in detail). The order of the static calibration and autocalibrated gaze typing phases was counterbalanced across participants. Users were free to abort a typing session if they found the setup too cumbersome to continue typing.

\subsubsection{Initial Calibration and Practice Session}
All participants first performed a manual calibration. Next, participants completed a practice round in which they typed a simple phrase (`happy new year') using the gaze typing application to familiarize themselves with the process.

% Figure environment removed

\subsubsection{Static Calibration System (No Learning)}
\label{sec:static}
In the static calibration phase, participants first completed a manual calibration and were then asked to type 5 different phrases (1 per session) using the gaze typing system with the initial calibration settings. The phrases were randomly selected from a standard corpus to evaluate text entry systems \citep{mackenzie2002text}, representing a diverse range of words and character combinations. At the start of each of the 5 sessions, we miscalibrated the calibrated tracker by a fixed amount of 0 or +100, -100 pixels in the x-direction, or +100, -100 pixels in the y-direction (counterbalanced order between sessions). These induced miscalibrations affected the entire screen uniformly.  After completing the set of 5 sessions, users provided feedback about their experience with the system via the NASA TLX questionnaire \citep{hart1988development}. %We introduce this system to all users as `System B'.%Typing speed and accuracy were recorded for each participant.

\subsubsection{Autocalibration System (Learning Interface)}
\label{sec:learning}
In the autocalibrated gaze typing phase, participants first completed a manual calibration, and then used our proposed autocalibrated gaze typing technique. Participants were again asked to type 5 random predetermined phrases from the corpus by \citet{mackenzie2002text}, with similar induced miscalibration effects as the static calibration system. No two phrases were repeated across phases for the same participant or across sessions for different participants. After completing the set of 5 sessions, users provided feedback about their experience with the system via the NASA TLX questionnaire \citep{hart1988development}. %We introduce this system to all users as `System A'.

\subsubsection{Open feedback} 
\label{sec:interview}
After typing sentences in both the static and autocalibrated phases (counterbalanced), participants shared open feedback in written form, and were also asked which of the two systems they preferred and why.

\subsection{Metrics}
\label{sec:metrics}
To assess the effectiveness of our autocalibrated gaze typing technique, we compared efficiency in terms of typing speed (characters per minute), number of backspaces, and abort frequency (number of incomplete sessions during which a user decided to give up typing a phrase) as quantitative metrics. For qualitative feedback, we compare NASA TLX \citep{hart1988development} responses and user preferences between the static calibration and the autocalibrated gaze typing systems, and open feedback. A two-sided t-test was performed to determine the significance of any observed differences. 
