\section{Background and Related Work}
\label{sec:related}

We provide a brief overview of different eye movement categories identified in prior work (Sec~\ref{sec:eye_movement}), prior approaches for explicit and implicit calibration of eye trackers (Sec.~\ref{sec:rel_calibration}), and techniques beyond autocalibration that have been proposed in the past to make gaze typing interactions adaptive and seamless (Sec.~\ref{sec:rel_adaptive}).
Our work builds on this prior work, by learning from differences in eye gaze between input (e.g. typing) and output (e.g. reading) to improve seamless autocalibration.

\subsection{Eye Movement}
\label{sec:eye_movement}
Eye gaze movements can be characterized as: (a) fixations, (b) saccades, (c) smooth pursuits, and (d) vestibulo-ocular movements \citep{holmqvist2011eye}. Visual fixations maintain the focus of gaze on a single location. Fixation duration varies based on the task, but one fixation is typically 100-500 ms, although it can be as short as 30 ms. Saccades are rapid, ballistic, voluntary eye movements (usually between 20-200 ms) that abruptly change the point of fixation. Smooth pursuit movements are slower tracking movements of the eyes that keep a moving stimulus on the fovea. Such movements are voluntary in that the observer can choose to track a moving stimulus, but only highly trained people can make smooth pursuit movements without a target to follow. The gaze typing interface used in this work does not consist of any moving targets. Vestibulo-ocular movements stabilize the eyes relative to the external world to compensate for head movements. These reflex responses prevent visual images from slipping on the surface of the retina as head position changes. With respect to gaze typing on visual keyboards, we don't expect users to move their heads significantly. This is also valid for ALS users who progressively lose the ability to move their head with loss in muscle strength. Therefore, in our setup we expect users to primarily use fixations and saccades during gaze typing.



\subsection{Calibration Methods for Eye Trackers}
\label{sec:rel_calibration}
Calibration is a crucial aspect of eye tracking systems, 
mapping gaze points to locations in the field of view, requiring accurate data from eye trackers. However, it can be time-consuming, intrusive, and needs to be repeated frequently to maintain accuracy, as eye-tracking performance can degrade over time due to factors such as user movement or changes in ambient lighting \citep{jacob2003eye, cerrolaza2012error, wang2016deep, kasprowski2018comparison}.
Various calibration methods have 
been proposed to improve the accuracy and user experience of eye tracking devices. Traditional calibration techniques \citep{harezlak2014towards} involve the user following a series of on-screen targets, such as dots or markers (typically 9-point calibration), while the eye tracker records the user's gaze data (Fig. \ref{fig:calibration}). These methods, while effective, can be time-consuming and uncomfortable for some users, particularly those with motor impairments \citep{jacob2003eye, kasprowski2018comparison}. 


\subsubsection{Explicit Gaze Calibration}
Prior works have proposed to reduce the time and effort required by the traditional 9-point calibration process through novel calibration interfaces where users are asked to explicitly fixate on fewer \citep{hoshino2020gaze} or more natural scene targets \citep{ohno2004free, saxena2022towards, kohlbecher2008calibration, vspakov2018enabling} to estimate parameters for a calibration function mapping the position of the eyes in 3D space to gaze coordinates on a 2D visual screen. 
\citet{cerrolaza2012error} estimate the calibration offset due to the user's head movement in the depth direction for a 4x4 grid on a 2D visual display. While these methods alleviate the burden of traditional calibration on an end-user, they all require the user's explicit cooperation with a novel calibration interface, and additionally might need custom hardware \citep{ohno2004free, kohlbecher2008calibration, vspakov2018enabling} and calibration interfaces \citep{saxena2022towards}. In contrast, our approach corrects for calibration errors without explicit cooperation of the user as they naturally perform the task at hand, without context switching to a new interface or additional custom hardware setups.


\subsubsection{Implicit Gaze Calibration}
Several prior approaches have also proposed implicit forms of eye gaze calibration (with no explicit cooperation from a user) either using saliency maps to determine natural target fixations in visual scenes \citep{kasprowski2018comparison, wang2016deep, hiroe2018implicit, hiroe2023implicit} or leverage assumptions that eye movements follow moving targets (smooth pursuits) \citep{pfeuffer2013pursuit, abdrabou2019calibration, bhatti2021eyelogin, lutz2015smoovs} and mouse clicks \citep{kasprowski2016implicit, gao2022x2t}.
\citet{sugano2015self} propose a self-calibrating approach for eye trackers based on a computational model of bottom-up visual saliency. This work assumes that the visual scene will have a user's gaze fixations always lie inside a small cluster of salient regions in the egocentric view of the user. While this approach is implicitly adaptive and leverages natural junctures of the user's visual view on a screen or the 3D environment, it is data intensive for accurate autocalibration. Similarly, \citet{kasprowski2018comparison} propose a calibration technique for headset eye trackers that maximizes the likelihood of 2D gaze coordinates falling in regions of high saliency. In comparison, our approach relies on natural elements of the user interface as fixation targets, leverages the difference between gaze behaviors as an input versus an output modality, and is sample efficient (autocalibrating effectively with fixation on one target point).

\citet{gao2022x2t} present an adaptive learning approach which autocalibrates an RGB-based eye tracker for a custom designed circular keyboard (words to be typed next are displayed on the circle). However, they assume the user can provide click-based feedback to the learning system in the form of backspaces activated on a physical keyboard/device. In comparison, our method only relies on feedback from the gaze trajectory available on the visual keyboard and does not use any external hardware other than the eye tracker.

\citet{lutz2015smoovs} designed a custom gaze typing interface (clusters of characters placed in a circle which move outwards) for calibration-free text entry on public displays by learning correlations between eye movements and target movements. Similarly, \citet{bhatti2021eyelogin} and \citet{abdrabou2019calibration} use smooth pursuit eye movement correlations with visual key movements for calibration-free text entry systems.
While these approaches alleviate the need for explicit calibration prior to gaze typing, they rely on custom designed visual keyboards with moving targets for implicit calibration. Our proposed approach instead is flexible and can augment any existing keyboard layout and eye tracker, relies on gaze fixation during natural junctures of the typing task while leveraging  differences between gaze behaviors during perception versus control.


\subsection{Other Adaptive Gaze Typing Techniques}
\label{sec:rel_adaptive}
Adaptive techniques for gaze typing aim to improve the efficiency and user experience of gaze-based text entry by dynamically adjusting system parameters based on the user's performance and gaze behavior. 
To enable widespread adoption of gaze typing technologies for AR applications, \citet{schenkluhn2022look} emphasize the need to create gaze typing that proactively adapts dwell time instead of retrospectively reacting to user fatigue. This would enable users to type short texts at their peak performance and economically utilizing cognitive resources for long texts. \citet{mott2017cascading} proposed a cascading dwell technique that automatically adjusts the dwell time for gaze-based input based on the user's performance. This approach has been shown to improve typing speeds and reduce errors in text entry tasks, highlighting the importance of dynamic adjustments in gaze-based input systems. \citet{cui2023glancewriter} propose to alleviate the effort required to type with dwell-based gaze typing systems by probabilistically predicting next characters to type from natural glances at visual keys without any need to dwell or specify the starting and ending positions of a gaze path when typing a word. Our autocalibration approach can augment such dwell-free gaze typing systems where differences in gaze behaviors during reading and typing still persist.


\citet{chen2021adaptive} propose an adaptive gaze typing system using a computational model of the control of eye movements in gaze-based selection. They formulate the model as an optimal sequential planning problem bounded by the limits of the human visual and motor systems and use reinforcement learning to approximate optimal solutions for number of fixations and duration required to make a gaze-based selection. While these approaches present adaptive learning based solutions for dwell-time customization, we propose a technique which adaptively corrects for miscalibration of an eye tracker during gaze typing that can be combined with dwell-time customization to further enhance user experience in the future. 