\section{Conclusion}
\label{sec:conclusion}

Eye tracking technology has enabled computer interactions for many users, with the potential to unlock additional future applications and interactions. 
Gaze typing is a common application of eye tracking technology, and is particularly beneficial to those with physical disabilities who cannot use traditional keyboards. 
However, the accuracy and efficiency of gaze tracking (including gaze typing) can be significantly impacted by the calibration of the eye tracking system. Conventional calibration methods are time-consuming and require users to periodically perform manual calibrations, which can be inconvenient and disruptive to the user experience. Users also often compensate for miscalibration by intentionally ofsetting their gaze, confounding the automatic detection and correction of miscalibration. As a result, calibration problems remain a significant barrier to use.

To help address this problem, we provide the insight that gaze is used for both input and output, and differences in performance between these two modes can enable detection and correction of miscalibration. We demonstrate this approach through our \systemName{} prototype, which provides autocalibration for gaze typing to provide a more seamless and accurate user experience. Our approach dynamically compensates for miscalibration using the difference in users' gaze during typing (when the user may compensate for miscalibration) versus reading the text they have typed (when the user does not compensate). Results from our user study suggest that by leveraging the natural gaze behavior of users during reading, our autocalibration system provides a more efficient, less mentally demanding, and overall preferable experience compared to traditional manual calibration. Results from our semi-structured interview with stakeholders deepens understanding of users' difficulties, resources, and the potential utility and future improvements for such autocalibration systems. 
We hope that other researchers and developers find useful insights in our work, and in particular are able to similarly leverage the difference in gaze performance during input and output to improve user experiences. 