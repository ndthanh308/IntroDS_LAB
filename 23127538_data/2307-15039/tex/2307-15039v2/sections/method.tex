\section{E\lowercase{ye}O Prototype} %%% prevent header camelcasing
\label{sec:algo} 


Current eye-gaze systems enable users to stop what they are doing and manually recalibrate if they think their calibration is off. We present a prototype, \systemName{}, that automatically recalibrates \textit{while the person is typing} without the need for manual recalibration. 
Our approach is based on the differences in gaze behavior during input (perception) and output (control). When an eye tracker's miscalibration is detected by a user during the typing task, they may attempt to compensate for the offset in the detected gaze coordinates to type the characters of interest. 
However, when the user looks up at the text box, and fixates to read the text they have typed, they do not need to compensate for miscalibration since they are not attempting to control the system. In this way, reading provides a free calibration signal, as eye gaze is not being compensated for miscalibration (but may occur during typing). By monitoring the difference in users' eye movements during typing versus reading, our technique dynamically adjusts the calibration in real-time, compensating for miscalibration errors. 



\subsection{System Design}
\label{sec:interface}

\systemName{} is functionally similar to currently available visual keyboards, such as in Windows Eye Control \citep{wec}. However, it provides more control to process and update the miscalibrated gaze coordinates in real-time. Figure \ref{fig:activation} illustrates the mechanism for typing a character on the visual keyboard through eye gaze. A user's detected gaze location is displayed as a red dot on the screen. If they fixate on a key for $50$ ms, a dwell timer of $400$ms is initiated by the system. The start of the timer is depicted by a green rectangle on the selected key (Fig. \ref{fig:activation}(a)). During fixation on the key, the rectangle slowly decreases in area, eventually collapsing at the center of the key (Fig. \ref{fig:activation}(b)). When the timer finishes, the user receives visual feedback that the character is typed: the key turns red and the letter is added to the text box at the top (Fig.~\ref{fig:activation}(c)). 

% Figure environment removed



Our system utilized a infrared Tobii PCEye eye tracker \citep{tobii}, with a sampling rate of 60 Hz, connected to a standard Windows 11 laptop via USB (Fig.~\ref{fig:gazetyping}). The eye tracker can be calibrated via a standard Tobii software available with the purchase of the tracker (Fig.\ref{fig:calibration}). It requires a user to follow a set of dots (in sequence) on the screen to calibrate their gaze. Users receive feedback about how the calibration process went and if satisfied with the results, they can then return to interacting with any application of interest.


% Figure environment removed

We work with a customized gaze typing application. The tracked gaze is directed towards an on-screen visual keyboard (Fig. \ref{fig:activation}), displayed on a 24-inch laptop screen. 
The on-screen keyboard came as part of an UWP application which was customized on top of the Microsoft Gaze Interaction Library \citep{gil}. The application was written using the Tobii Pro SDK (designed to offer access to gaze data from Tobii eye trackers) in C\#. It facilitates the capture and processing of x, y coordinates from the eye tracker, providing necessary data for \systemName{} to leverage and update the gaze coordinates in real-time. 
The software for our autocalibrated gaze typing technique was developed in Python, which received the 2D gaze coordinates in real-time from the UWP application via Google's remote procedure call (RPC) protocol. This allowed Python scripts to access gaze data, analyze it, and apply the necessary miscalibration corrections to display back on the visual keyboard application. 


\subsection{Gaze Filtering}
In our work, we primarily distinguish between fixations and saccades (see Section \ref{sec:related}). We assume smooth pursuit movements are not present in our trials as the visual keyboard has no moving targets. To detect fixations, we consider spatial and temporal criteria from the taxonomy of fixation identification algorithms described by \citet{salvucci2000identifying}. We use velocity-based criteria from spatial characteristics and duration based criteria from temporal characteristics, to filter out fixations from saccades. 
\citet{salvucci2000identifying} proposed a novel taxonomy of fixation identification algorithm and evaluated existing algorithms in the context of this taxonomy. They identify two characteristics---spatial and temporal---to classify different algorithms for fixation identification. For spatial characteristics, three criteria distinguish primary types of algorithms: velocity-based, dispersion-based, and area-based. For temporal characteristics, they include two criteria: whether the algorithm uses duration information, and whether the algorithm is locally adaptive. The use of duration information is guided by the fact that fixations are rarely less than 100 ms and often in the range of 100-500 ms. In our work, we use velocity-based and area-based criteria under spatial characteristics and duration-based criteria under temporal characteristics to filter out fixations from saccades. We first filter out eye movements with very high speeds (a large distance traversed over a very short period of time is likely a saccade). If gaze
continues to not be classified as a saccade for more than 100 ms, then we consider it a fixation.

\subsection{Autocalibration Algorithm}

% Figure environment removed

In this section, we describe the low-level autocalibration algorithm implemented in EyeO. Figure~\ref{fig:alg} provides a simplified schematic of the procedure, while Algorithm~\ref{alg:main} presents it in full detail. The algorithm receives a stream of coordinates $(x_t, y_t)$ from the device driver, as well as several variables describing the state of the keyboard interface. It produces estimated gaze coordinates $(\widehat x_t, \widehat y_t)$ based on adaptively estimated calibration errors $(\epsilon_{x_t}, \epsilon_{y_t})$, which are refined whenever the user is inferred to be ingesting the system's output (i.e. reading from the text box).

The user's gaze is assumed to be directed towards the text box when it falls above the keyboard layout and within a certain threshold distance $\tau$ from the center of the last typed character. 
If the gaze is detected within this area (calibration zone), the system then identifies whether the user's gaze is in a state of fixation or saccade. Visual fixations maintain the focus of gaze on a single location. 
We first filter out eye movements with very high speeds (a large distance traversed over a very short period of time is likely a saccade), and then detect fixations within the zone of calibration. If gaze continues to not be classified as a saccade for more than 100 ms and lies within the calibration zone, we consider it a fixation for reading.

\input{sections/alg-box}

We assume that the user reads the last typed character when they look up to read and detect the offset in calibration accordingly. 
Our system computes a moving average of the calibration offsets, 
which enables the calibration to be updated continuously and smoothly in real-time. 
The system continuously updates the calibration error based on the user's gaze behavior while reading the typed text. The computed correction to the detected miscalibration is continuously applied when the user's gaze moves away from the text box (i.e. while typing on the visual keyboard). The autocalibrated gaze coordinates are displayed to the user with the updated location of the red gaze cursor on the screen (Fig. \ref{fig:activation}(a)). 
The approach is detailed in Algorithm \ref{alg:main}.
We instantiate this algorithm with calibration zone size parameter
$\tau = 150$,
window size $w=64$, and calibration error bound $b=200$. 