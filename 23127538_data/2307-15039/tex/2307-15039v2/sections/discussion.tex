\section{Discussion}
\label{sec:discussion}


As this work suggests, autocalibrating eye trackers without introducing additional tasks for the end user represents a step towards building more seamless user experiences. Gaze data offers vast potential as a form of input for computer interaction, effectively making technology more accessible and user-friendly. Coupled with methods for autocalibration, an array of seamless gaze-controlled interactions may become possible in the future.
To help inform such developments, below we discuss how our work informs gaze control more broadly, outline future work, and discuss our work's limitations.


\subsection{Limitations}
While our technique introduces new possibilities for making interactions more seamless, it also has several limitations. As in gaze tracking generally, our technique relies heavily on users having clear, unobstructed vision and the capacity to make smooth eye movements across the visual keyboard, which may not always be the case, particularly for users with specific ocular or neurological conditions. Additionally, eye movements during reading and writing are influenced by numerous cognitive and physiological factors, such as attention, fatigue, and cognitive load. These factors can cause dynamic changes in eye movement behavior, potentially affecting the computation of the miscalibration offset and hence the calibration accuracy. 

We also recognize that our user study did not include people with disabilities or others who regularly rely on gaze typing, though our semi-structured interview engaged more with gaze typing stakeholders. 
While our user study results suggest that the proposed autocalibration approach can significantly improve the typing efficiency and user experience for able-bodied participants, the comfort, ease-of-use, and adaptation to individual user needs for disabled populations are factors that must be evaluated and explored further in future work.

\subsection{Implications for Gaze Control}

The insight that gaze behavior differs when used for input (e.g. typing) and output (e.g. reading) can be leveraged for gaze interactions beyond gaze typing. 
Any interactive application where gaze functions as control at some points but is used for perception at other points can leverage this insight to improve performance and calibration. For example, an AR/VR meeting that enables participants to select from a menu through gaze control, and also provides captions in another part of the screen, autocalibration can safely occur while the user reads the captions to improve interaction with the menu and other parts of the application. 
Applications in VR/AR for gaze-based web browsing, gaming, and computer-control 
can especially benefit in terms of user experience and control latency with precise gaze prediction. 


We also note that the insight we leverage about differences in gaze behavior is closely related to the Midas Touch problem. The Midas Touch problem \citep{velichkovsky1997towards} states that gaze used for both perception and control can trigger unintended actions (such as activation of unintended keys while typing). Until now, this problem has served as a barrier to seamless gaze-controlled interactions. We note however, that when the differences in gaze behavior can be well isolated to different parts of an application -- i.e. if the system can accurately predict whether the user is engaging in control or perception -- the system can strategically leverage this difference. In our work, we leveraged this difference to selectively trigger calibration processes. This difference could also be leveraged to selectively trigger control events only when the user is attempting control, and prevent control events when the user is engaging in perception. 
It may also be possible to design interfaces where it is easier to predict if the user is engaging in control or perception. For example, designing visual targets to be used exclusively either for control or perception could boost prediction accuracy of user intent. 



\subsection{Future Work}


Future work includes exploring autocalibration for multimodal interfaces and more immersive environments. 
One user study participant shared their excitement for integrating gaze typing technology with other modalities (e.g.  speech or joystick input to smart TVs). They also recognized that the additional cognitive load may necessitate autocalibration and additional support for real-world deployment: \emph{``As someone who is open to alternatives means of typing due to physical restrictions, I like the potential of the technology but also acknowledge the potential strain for persons mentally and physically. It would be cool for persons typing on TVs though''}. Developing and studying how to best autocalibrate and support such multimodal interfaces makes interesting future work.


During the semi-structured interview, participants suggested many potential improvements to autocalibration. For example, their suggestion to improve transparency around when autocalibration occurs introduces rich HCI research opportunities. Future studies can explore what information should be made available to the user (e.g. binary signal to indicate autocalibration, detected errors indicating miscalibration, and/or the direction of correction), and the interface design for conveying this signal (e.g. auditory/ visual feedback). 

Additionally, designing a feature to enable user control for autocalibration introduces interesting research questions for future work -- What amount of detected miscalibration should trigger suggestions for autocalibration? Should these recommendations be personalized? Can the impact of user tolerance to miscalibrations/compensation be learned?
While we tested our autocalibration prototype with a single fixation point on the text box placed at the top of the screen, additional perception-based landmarks could be placed on different parts of the screen to help estimate a fine-grained miscalibration map. Prior approaches using saliency maps, correlation of smooth pursuits and moving targets, next word predictions etc. can further augment \systemName{} for enhanced autocalibration, and in turn more seamless gaze interactions.



Furthermore, as we move towards a future where human-computer interaction becomes increasingly multifaceted, incorporating a higher bandwidth of input signals like speech, gestures, and facial expressions becomes essential. The autocalibration approach, by enhancing the accuracy and efficiency of eye tracking, can contribute towards creating more holistic, powerful, and intuitive interfaces especially for real-world applications like augmented reality (AR) and virtual reality (VR). With increasing reliance on these technologies for various purposes – from gaming to professional training – the need for seamless and efficient interfaces is paramount. Our work can contribute towards this goal by offering more accurate eye-tracking, thereby improving the user's interaction with the AR/VR environment. %