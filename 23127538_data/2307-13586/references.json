{
  "2305-16589": {
    "title": "The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model",
    "authors": [
      "Laixi Shi",
      "Gen Li",
      "Yuting Wei",
      "Yuxin Chen",
      "M. Geist",
      "Yuejie Chi"
    ],
    "submission_date": "2023-05-26",
    "semantic_scholar_id": "2551852e940e8bbaf728049f3df3613380d4d48a"
  },
  "2305-15703": {
    "title": "The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning",
    "authors": [
      "Kaiwen Wang",
      "Kevin Zhou",
      "Runzhe Wu",
      "Nathan Kallus",
      "Wen Sun"
    ],
    "submission_date": "2023-05-25",
    "semantic_scholar_id": "8d7154a1714f4a076ff211b9a4e0b95429e7a9c1"
  },
  "2305-15546": {
    "title": "Regret-Optimal Model-Free Reinforcement Learning for Discounted MDPs with Short Burn-In Time",
    "authors": [
      "Xiang Ji",
      "Gen Li"
    ],
    "submission_date": "2023-05-24",
    "semantic_scholar_id": "9a09099b53349214fd3301f3e4dacf9a71716452"
  },
  "2304-07278": {
    "title": "Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning",
    "authors": [
      "Gen Li",
      "Yuling Yan",
      "Yuxin Chen",
      "Jianqing Fan"
    ],
    "submission_date": "2023-04-14",
    "semantic_scholar_id": "5071dda43602ad4dfbba90aced6122cfe2b16abf"
  },
  "2302-10371": {
    "title": "Variance-Dependent Regret Bounds for Linear Bandits and Reinforcement Learning: Adaptivity and Computational Efficiency",
    "authors": [
      "Heyang Zhao",
      "Jiafan He",
      "Dongruo Zhou",
      "Tong Zhang",
      "Quanquan Gu"
    ],
    "submission_date": "2023-02-21",
    "semantic_scholar_id": "5ae7ef3231f94618f5d7f1f63e659bf5de731a59"
  },
  "2301-13446": {
    "title": "Sharp Variance-Dependent Bounds in Reinforcement Learning: Best of Both Worlds in Stochastic and Deterministic Environments",
    "authors": [
      "Runlong Zhou",
      "Zihan Zhang",
      "S. Du"
    ],
    "submission_date": "2023-01-31",
    "semantic_scholar_id": "ac97c8e14033741627dad9f33f0212dbea9c4821"
  },
  "2208-10458": {
    "title": "Minimax-Optimal Multi-Agent RL in Markov Games With a Generative Model",
    "authors": [
      "Gen Li",
      "Yuejie Chi",
      "Yuting Wei",
      "Yuxin Chen"
    ],
    "submission_date": "2022-08-22",
    "semantic_scholar_id": "46a0c00d6ffdb919af244700f57831d64622c85c"
  },
  "2206-00177": {
    "title": "On Gap-dependent Bounds for Offline Reinforcement Learning",
    "authors": [
      "Xinqi Wang",
      "Qiwen Cui",
      "S. Du"
    ],
    "submission_date": "2022-06-01",
    "semantic_scholar_id": "7daebdbfc2b6a3d9f8143d2f735d4f8a4bcd9868"
  },
  "2204-05275": {
    "title": "Settling the Sample Complexity of Model-Based Offline Reinforcement Learning",
    "authors": [
      "Gen Li",
      "Laixi Shi",
      "Yuxin Chen",
      "Yuejie Chi",
      "Yuting Wei"
    ],
    "submission_date": "2022-04-11",
    "semantic_scholar_id": "ee0d43083dbbad55ff1da3c2f9213c364159afc7"
  },
  "2203-12922": {
    "title": "Horizon-Free Reinforcement Learning in Polynomial Time: the Power of Stationary Policies",
    "authors": [
      "Zihan Zhang",
      "Xiangyang Ji",
      "S. Du"
    ],
    "submission_date": "2022-03-24",
    "semantic_scholar_id": "5898955105e10baa17ad224263449e23f2a6c0ab"
  },
  "2203-07368": {
    "title": "The Efficacy of Pessimism in Asynchronous Q-Learning",
    "authors": [
      "Yuling Yan",
      "Gen Li",
      "Yuxin Chen",
      "Jianqing Fan"
    ],
    "submission_date": "2022-03-14",
    "semantic_scholar_id": "8a20cc7983e748615e912e59688defc9b7db4be7"
  },
  "2203-05804": {
    "title": "Near-optimal Offline Reinforcement Learning with Linear Representation: Leveraging Variance Information with Pessimism",
    "authors": [
      "Ming Yin",
      "Yaqi Duan",
      "Mengdi Wang",
      "Yu-Xiang Wang"
    ],
    "submission_date": "2022-03-11",
    "semantic_scholar_id": "dc94a18c69c71434590299fad9aeb2c932f45e15"
  },
  "2202-13890": {
    "title": "Pessimistic Q-Learning for Offline Reinforcement Learning: Towards Optimal Sample Complexity",
    "authors": [
      "Laixi Shi",
      "Gen Li",
      "Yuting Wei",
      "Yuxin Chen",
      "Yuejie Chi"
    ],
    "submission_date": "2022-02-28",
    "semantic_scholar_id": "e1ac9023f2991ebc840b99d6f0203201a3adfaf4"
  },
  "2112-03432": {
    "title": "First-Order Regret in Reinforcement Learning with Linear Function Approximation: A Robust Estimation Approach",
    "authors": [
      "Andrew Wagenmaker",
      "Yifang Chen",
      "Max Simchowitz",
      "S. Du",
      "Kevin G. Jamieson"
    ],
    "submission_date": "2021-12-07",
    "semantic_scholar_id": "991f8b0ca4aee42a048a5ab053a94aa4bdf3ad44"
  },
  "2111-00633": {
    "title": "Settling the Horizon-Dependence of Sample Complexity in Reinforcement Learning",
    "authors": [
      "Yuanzhi Li",
      "Ruosong Wang",
      "Lin F. Yang"
    ],
    "submission_date": "2021-11-01",
    "semantic_scholar_id": "8bb44ca40380804fb822752d44b2e4b2291f4016"
  },
  "2110-04645": {
    "title": "Breaking the Sample Complexity Barrier to Regret-Optimal Model-Free Reinforcement Learning",
    "authors": [
      "Gen Li",
      "Laixi Shi",
      "Yuxin Chen",
      "Yuantao Gu",
      "Yuejie Chi"
    ],
    "submission_date": "2021-10-09",
    "semantic_scholar_id": "1f4484086d210a2c44efe5eef0a2b42647822abf"
  },
  "2107-01264": {
    "title": "Beyond Value-Function Gaps: Improved Instance-Dependent Regret Bounds for Episodic Reinforcement Learning",
    "authors": [
      "Christoph Dann",
      "T. V. Marinov",
      "M. Mohri",
      "Julian Zimmert"
    ],
    "submission_date": "2021-07-02",
    "semantic_scholar_id": "1b2a5406a8db1c578f10a2be2cb519c843a368a9"
  },
  "2106-13013": {
    "title": "A Fully Problem-Dependent Regret Lower Bound for Finite-Horizon MDPs",
    "authors": [
      "Andrea Tirinzoni",
      "Matteo Pirotta",
      "A. Lazaric"
    ],
    "submission_date": "2021-06-24",
    "semantic_scholar_id": "44fdc993420c24e026b74dc64b0183346ec0cf17"
  },
  "2106-08377": {
    "title": "Implicit Finite-Horizon Approximation and Efficient Optimal Algorithms for Stochastic Shortest Path",
    "authors": [
      "Liyu Chen",
      "Mehdi Jafarnia-Jahromi",
      "R. Jain",
      "Haipeng Luo"
    ],
    "submission_date": "2021-06-15",
    "semantic_scholar_id": "2888e0b70d6d238b7faf0334a6d602607f26c710"
  },
  "2106-04895": {
    "title": "Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning",
    "authors": [
      "Tengyang Xie",
      "Nan Jiang",
      "Huan Wang",
      "Caiming Xiong",
      "Yu Bai"
    ],
    "submission_date": "2021-06-09",
    "semantic_scholar_id": "d769ca62d90adc7e7869849a421426bdc54a32fb"
  },
  "2104-11186": {
    "title": "Stochastic Shortest Path: Minimax, Parameter-Free and Towards Horizon-Free Regret",
    "authors": [
      "Jean Tarbouriech",
      "Runlong Zhou",
      "S. Du",
      "Matteo Pirotta",
      "M. Valko",
      "A. Lazaric"
    ],
    "submission_date": "2021-04-22",
    "semantic_scholar_id": "1821768a5a67048917e8328cc5ecbf97e21b86d6"
  },
  "2103-14077": {
    "title": "Nearly Horizon-Free Offline Reinforcement Learning",
    "authors": [
      "Tongzheng Ren",
      "Jialian Li",
      "Bo Dai",
      "S. Du",
      "S. Sanghavi"
    ],
    "submission_date": "2021-03-25",
    "semantic_scholar_id": "4e37e0228798364dc6bfd0e05ac3eaac0002ae6b"
  },
  "2103-12021": {
    "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism",
    "authors": [
      "Paria Rashidinejad",
      "Banghua Zhu",
      "Cong Ma",
      "Jiantao Jiao",
      "Stuart J. Russell"
    ],
    "submission_date": "2021-03-22",
    "semantic_scholar_id": "0bcc734246586966596b1aa22efac2565224ebee"
  },
  "2103-01312": {
    "title": "UCB Momentum Q-learning: Correcting the bias without forgetting",
    "authors": [
      "Pierre M√©nard",
      "O. D. Domingues",
      "Xuedong Shang",
      "Michal Valko"
    ],
    "submission_date": "2021-03-01",
    "semantic_scholar_id": "6056961e0e4b5fb8bdce7aa64a8499e5a38da7df"
  },
  "2102-09703": {
    "title": "Near-Optimal Randomized Exploration for Tabular Markov Decision Processes",
    "authors": [
      "Zhihan Xiong",
      "Ruoqi Shen",
      "Qiwen Cui",
      "Maryam Fazel",
      "S. Du"
    ],
    "submission_date": "2021-02-19",
    "semantic_scholar_id": "e2381b66f552eacfb5f8ec3bdf6bb98c5e1e50f7"
  },
  "2102-06548": {
    "title": "Is Q-Learning Minimax Optimal? A Tight Sample Complexity Analysis",
    "authors": [
      "Gen Li",
      "Ee",
      "Changxiao Cai",
      "Yuting Wei"
    ],
    "submission_date": "2021-02-12",
    "semantic_scholar_id": "a7d8c132589e4e90da4f417f81fe2068afed9091"
  },
  "2102-04692": {
    "title": "Fine-Grained Gap-Dependent Bounds for Tabular MDPs via Adaptive Multi-Step Bootstrap",
    "authors": [
      "Haike Xu",
      "Tengyu Ma",
      "S. Du"
    ],
    "submission_date": "2021-02-09",
    "semantic_scholar_id": "15401c99dbd977001c1e2076f0decd8362e7d57a"
  },
  "2012-15085": {
    "title": "Is Pessimism Provably Efficient for Offline RL?",
    "authors": [
      "Ying Jin",
      "Zhuoran Yang",
      "Zhaoran Wang"
    ],
    "submission_date": "2020-12-30",
    "semantic_scholar_id": "9f8a70e9188a913317e97ba1874c8f763fd13c04"
  },
  "2011-14267": {
    "title": "Minimax Sample Complexity for Turn-based Stochastic Game",
    "authors": [
      "Qiwen Cui",
      "Lin F. Yang"
    ],
    "submission_date": "2020-11-29",
    "semantic_scholar_id": "ad2d2561265c1f59d060d40102cea771b3c6b5fd"
  },
  "2010-03531": {
    "title": "Episodic Reinforcement Learning in Finite MDPs: Minimax Lower Bounds Revisited",
    "authors": [
      "O. D. Domingues",
      "Pierre M'enard",
      "E. Kaufmann",
      "Michal Valko"
    ],
    "submission_date": "2020-10-07",
    "semantic_scholar_id": "0b0c82e33d3328246b6adc3ef2b55be9b606a0cd"
  },
  "2009-13503": {
    "title": "Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal Algorithm Escaping the Curse of Horizon",
    "authors": [
      "Zihan Zhang",
      "Xiangyang Ji",
      "S. Du"
    ],
    "submission_date": "2020-09-28",
    "semantic_scholar_id": "93e5b29890d3fbd7f8732bafbcad310e0c78078e"
  },
  "2007-01891": {
    "title": "A Unifying View of Optimism in Episodic Reinforcement Learning",
    "authors": [
      "Gergely Neu",
      "Ciara Pike-Burke"
    ],
    "submission_date": "2020-07-03",
    "semantic_scholar_id": "fb68cb1191db2134422068a71bd148e1da963f97"
  },
  "2006-09118": {
    "title": "Q-learning with Logarithmic Regret",
    "authors": [
      "Kunhe Yang",
      "Lin F. Yang",
      "S. Du"
    ],
    "submission_date": "2020-06-16",
    "semantic_scholar_id": "8a0ec47a51ed130a83a599c5c81c570fe1689cf3"
  },
  "2006-08040": {
    "title": "Bias no more: high-probability data-dependent regret bounds for adversarial bandits and MDPs",
    "authors": [
      "Chung-Wei Lee",
      "Haipeng Luo",
      "Chen-Yu Wei",
      "Mengxiao Zhang"
    ],
    "submission_date": "2020-06-14",
    "semantic_scholar_id": "58fada2a762cf4a9a0799f88fa154c6260b2b801"
  },
  "2006-03864": {
    "title": "Model-Free Reinforcement Learning: from Clipped Pseudo-Regret to Sample Complexity",
    "authors": [
      "Zihan Zhang",
      "Yuanshuo Zhou",
      "Xiangyang Ji"
    ],
    "submission_date": "2020-06-06",
    "semantic_scholar_id": "0538928f1937389eea0d6ba2ad4be09854d74fda"
  },
  "2006-03041": {
    "title": "Sample Complexity of Asynchronous Q-Learning: Sharper Analysis and Variance Reduction",
    "authors": [
      "Gen Li",
      "Yuting Wei",
      "Yuejie Chi",
      "Yuantao Gu",
      "Yuxin Chen"
    ],
    "submission_date": "2020-06-04",
    "semantic_scholar_id": "3f01a03a284ad2d1745e6d0d378384bcab49bd3e"
  },
  "2005-12900": {
    "title": "Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model",
    "authors": [
      "Gen Li",
      "Yuting Wei",
      "Yuejie Chi",
      "Yuantao Gu",
      "Yuxin Chen"
    ],
    "submission_date": "2020-05-26",
    "semantic_scholar_id": "ad6770658096a738d0ca0abcd56b392c80017488"
  },
  "2005-01643": {
    "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems",
    "authors": [
      "S. Levine",
      "Aviral Kumar",
      "G. Tucker",
      "Justin Fu"
    ],
    "submission_date": "2020-05-04",
    "semantic_scholar_id": "5e7bc93622416f14e6948a500278bfbe58cd3890"
  },
  "2005-00527": {
    "title": "Is Long Horizon Reinforcement Learning More Difficult Than Short Horizon Reinforcement Learning?",
    "authors": [
      "Ruosong Wang",
      "S. Du",
      "Lin F. Yang",
      "S. Kakade"
    ],
    "submission_date": "2020-05-01",
    "semantic_scholar_id": "1359c5d2096eb08f9ba7b4f5ef2cab0330d80925"
  },
  "2004-10019": {
    "title": "Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition",
    "authors": [
      "Zihan Zhang",
      "Yuanshuo Zhou",
      "Xiangyang Ji"
    ],
    "submission_date": "2020-04-21",
    "semantic_scholar_id": "8046dbd4ebc3fda0fcc43d9110c0d2d940052980"
  },
  "2002-02794": {
    "title": "Reward-Free Exploration for Reinforcement Learning",
    "authors": [
      "Chi Jin",
      "A. Krishnamurthy",
      "Max Simchowitz",
      "Tiancheng Yu"
    ],
    "submission_date": "2020-02-07",
    "semantic_scholar_id": "90f9ef793c1e4000ee523e1f8cea501fb8806fa0"
  },
  "2002-00260": {
    "title": "Finite-Time Analysis of Asynchronous Stochastic Approximation and Q-Learning",
    "authors": [
      "Guannan Qu",
      "A. Wierman"
    ],
    "submission_date": "2020-02-01",
    "semantic_scholar_id": "5fe5376f88ff94e67ef7f8677a7cb70820bb8338"
  },
  "1912-05830": {
    "title": "Provably Efficient Exploration in Policy Optimization",
    "authors": [
      "Qi Cai",
      "Zhuoran Yang",
      "Chi Jin",
      "Zhaoran Wang"
    ],
    "submission_date": "2019-12-12",
    "semantic_scholar_id": "d0cf6bc0d44e2af3135d238a54c58dcd5c2d4e84"
  },
  "1906-04697": {
    "title": "Variance-reduced Q-learning is minimax optimal",
    "authors": [
      "M. Wainwright"
    ],
    "submission_date": "2019-06-11",
    "semantic_scholar_id": "9172c8e49be3bf204d3cecfad6ba1a7b4ddd2207"
  },
  "1906-03804": {
    "title": "Model-Based Reinforcement Learning with a Generative Model is Minimax Optimal",
    "authors": [
      "Alekh Agarwal",
      "S. Kakade",
      "Lin F. Yang"
    ],
    "submission_date": "2019-06-10",
    "semantic_scholar_id": "10f2b0f9744aec689b2fc75740bf8d9d9c1896e3"
  },
  "1906-02870": {
    "title": "Worst-Case Regret Bounds for Exploration via Randomized Value Functions",
    "authors": [
      "Daniel Russo"
    ],
    "submission_date": "2019-06-07",
    "semantic_scholar_id": "e5e440562aa293b43cd95a5b52dd705fbf88115b"
  },
  "1905-12849": {
    "title": "Provably Efficient Q-Learning with Low Switching Cost",
    "authors": [
      "Yu Bai",
      "Tengyang Xie",
      "Nan Jiang",
      "Yu-Xiang Wang"
    ],
    "submission_date": "2019-05-30",
    "semantic_scholar_id": "6101d16008e536740adb97588616af51fe392950"
  },
  "1905-06265": {
    "title": "Stochastic approximation with cone-contractive operators: Sharp ùìÅ‚àû-bounds for Q-learning",
    "authors": [
      "M. Wainwright"
    ],
    "submission_date": "2019-05-15",
    "semantic_scholar_id": "e460cccfa9c09bcbcd7913b179dbe78866334bec"
  },
  "1905-11527": {
    "title": "Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies",
    "authors": [
      "Yonathan Efroni",
      "Nadav Merlis",
      "M. Ghavamzadeh",
      "Shie Mannor"
    ],
    "submission_date": "2019-05-01",
    "semantic_scholar_id": "6246b3d27f6a9f211126cafe39d8c8a7f6ed06f4"
  },
  "1905-03814": {
    "title": "Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs",
    "authors": [
      "Max Simchowitz",
      "Kevin G. Jamieson"
    ],
    "submission_date": "2019-05-01",
    "semantic_scholar_id": "7f074f4e30b196df8a7f93bc58cbd88f3acc8641"
  },
  "1901-09311": {
    "title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP",
    "authors": [
      "Kefan Dong",
      "Yuanhao Wang",
      "Xiaoyu Chen",
      "Liwei Wang"
    ],
    "submission_date": "2019-01-27",
    "semantic_scholar_id": "225cc727daeca281f4f932a70765e0a32d849d6b"
  },
  "1901-00210": {
    "title": "Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds",
    "authors": [
      "A. Zanette",
      "E. Brunskill"
    ],
    "submission_date": "2019-01-01",
    "semantic_scholar_id": "f14ea2243fe74cbf1a4ea86cebed1fb9547c3a7c"
  },
  "1811-03056": {
    "title": "Policy Certificates: Towards Accountable Reinforcement Learning",
    "authors": [
      "Christoph Dann",
      "Lihong Li",
      "Wei Wei",
      "E. Brunskill"
    ],
    "submission_date": "2018-11-07",
    "semantic_scholar_id": "cd4ee7825ea974f0fbb69445d403c9dde54f6a04"
  },
  "1807-03765": {
    "title": "Is Q-learning Provably Efficient?",
    "authors": [
      "Chi Jin",
      "Zeyuan Allen-Zhu",
      "S√©bastien Bubeck",
      "Michael I. Jordan"
    ],
    "submission_date": "2018-07-10",
    "semantic_scholar_id": "03cc81e98942bdafd994af7a1d1e62a68ff8b682"
  },
  "1803-01626": {
    "title": "Variance-Aware Regret Bounds for Undiscounted Reinforcement Learning in MDPs",
    "authors": [
      "M. S. Talebi",
      "Odalric-Ambrym Maillard"
    ],
    "submission_date": "2018-03-05",
    "semantic_scholar_id": "4bcfc5a34a22363881b54a906a1dd5040cf0a22e"
  },
  "1802-04020": {
    "title": "Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning",
    "authors": [
      "Ronan Fruit",
      "Matteo Pirotta",
      "A. Lazaric",
      "R. Ortner"
    ],
    "submission_date": "2018-02-12",
    "semantic_scholar_id": "6d993a2111f47b5096fada7bed42cc4a523c294d"
  },
  "1802-03386": {
    "title": "Make the Minority Great Again: First-Order Regret Bound for Contextual Bandits",
    "authors": [
      "Zeyuan Allen-Zhu",
      "S√©bastien Bubeck",
      "Yuanzhi Li"
    ],
    "submission_date": "2018-02-09",
    "semantic_scholar_id": "6077662d29c768ce81353b197ac8c31453b8a297"
  },
  "1710-09988": {
    "title": "Variance reduced value iteration and faster algorithms for solving Markov decision processes",
    "authors": [
      "Aaron Sidford",
      "Mengdi Wang",
      "X. Wu",
      "Y. Ye"
    ],
    "submission_date": "2017-10-27",
    "semantic_scholar_id": "a50adbbed1f8097d0a0b1b0fc0a1c8b5fa938a7a"
  },
  "1703-05449": {
    "title": "Minimax Regret Bounds for Reinforcement Learning",
    "authors": [
      "M. G. Azar",
      "Ian Osband",
      "R. Munos"
    ],
    "submission_date": "2017-03-16",
    "semantic_scholar_id": "67d10cea808937089d6492acff14ad9ef156e3c5"
  },
  "1703-07710": {
    "title": "Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning",
    "authors": [
      "Christoph Dann",
      "Tor Lattimore",
      "E. Brunskill"
    ],
    "submission_date": "2017-03-01",
    "semantic_scholar_id": "5dcc07acb63cc909c5be701c1c88fef3718ba326"
  },
  "1510-08906": {
    "title": "Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning",
    "authors": [
      "Christoph Dann",
      "E. Brunskill"
    ],
    "submission_date": "2015-10-29",
    "semantic_scholar_id": "f4aa31b7ae2e03ee5a6b94f34cb3b6a554230aef"
  },
  "1306-0940": {
    "title": "(More) Efficient Reinforcement Learning via Posterior Sampling",
    "authors": [
      "Ian Osband",
      "Daniel Russo",
      "Benjamin Van Roy"
    ],
    "submission_date": "2013-06-04",
    "semantic_scholar_id": "789783016fb708abbc061790612ebe91273c05d3"
  },
  "1202-3890": {
    "title": "PAC Bounds for Discounted MDPs",
    "authors": [
      "Tor Lattimore",
      "Marcus Hutter"
    ],
    "submission_date": "2012-02-17",
    "semantic_scholar_id": "8e4d0530499fdbbc0581894371013da9fec8ed95"
  },
  "0907-3740": {
    "title": "Empirical Bernstein Bounds and Sample-Variance Penalization",
    "authors": [
      "Andreas Maurer",
      "M. Pontil"
    ],
    "submission_date": "2009-07-21",
    "semantic_scholar_id": "6e71a24fc0bba4a712b89dd9ff87a452230c2c4b"
  },
  "1205-2661": {
    "title": "REGAL: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating MDPs",
    "authors": [
      "P. Bartlett",
      "Ambuj Tewari"
    ],
    "submission_date": "2009-06-18",
    "semantic_scholar_id": "71886c33a34c6effe14f465ebe2806383e0d76a3"
  }
}