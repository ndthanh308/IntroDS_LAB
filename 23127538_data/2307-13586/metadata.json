{
  "title": "Settling the Sample Complexity of Online Reinforcement Learning",
  "authors": [
    "Zihan Zhang",
    "Yuxin Chen",
    "Jason D. Lee",
    "Simon S. Du"
  ],
  "submission_date": "2023-07-25T15:42:11+00:00",
  "revised_dates": [
    "2024-03-05T03:31:28+00:00",
    "2024-05-24T01:55:13+00:00",
    "2025-04-30T00:16:50+00:00"
  ],
  "abstract": "A central issue lying at the heart of online reinforcement learning (RL) is data efficiency. While a number of recent works achieved asymptotically minimal regret in online RL, the optimality of these results is only guaranteed in a ``large-sample'' regime, imposing enormous burn-in cost in order for their algorithms to operate optimally. How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory.\n  We settle this problem for the context of finite-horizon inhomogeneous Markov decision processes. Specifically, we prove that a modified version of Monotonic Value Propagation (MVP), a model-based algorithm proposed by \\cite{zhang2020reinforcement}, achieves a regret on the order of (modulo log factors) \\begin{equation*}\n  \\min\\big\\{ \\sqrt{SAH^3K}, \\,HK \\big\\}, \\end{equation*} where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, and $K$ is the total number of episodes. This regret matches the minimax lower bound for the entire range of sample size $K\\geq 1$, essentially eliminating any burn-in requirement. It also translates to a PAC sample complexity (i.e., the number of episodes needed to yield $\\varepsilon$-accuracy) of $\\frac{SAH^3}{\\varepsilon^2}$ up to log factor, which is minimax-optimal for the full $\\varepsilon$-range.\n  Further, we extend our theory to unveil the influences of problem-dependent quantities like the optimal value/cost and certain variances. The key technical innovation lies in the development of a new regret decomposition strategy and a novel analysis paradigm to decouple complicated statistical dependency -- a long-standing challenge facing the analysis of online RL in the sample-hungry regime.",
  "categories": [
    "cs.LG"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.13586",
  "pdf_url": null,
  "comment": "accepted to Journal of the ACM (also appeared in COLT 2024 as an extended abstract)",
  "num_versions": null,
  "size_before_bytes": 1967715,
  "size_after_bytes": 1986763
}