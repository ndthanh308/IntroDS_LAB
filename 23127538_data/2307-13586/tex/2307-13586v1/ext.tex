


With the refined error bound derived in Section~\ref{sec:key-lemmas-tec}, 
we can readily obtain more refined regret bounds for Algorithm~\ref{alg:main} to reflect the role of several problem-dependent quantities.  Most of the arguments in the analysis are similar to those in the previous work \citet{zhou2023sharp}. 
Detailed proofs are postponed to Appendix~\ref{sec:appfirst} and Appendix~\ref{app:var}.
% Our results show that 




%\subsection{Value-based and cost-based regret bounds}\label{sec:value-cost}



%We restate Theorem~\ref{thm:first} as below.
%
%
%\noindent\textbf{Theorem 2. (Restated)}\emph{
%For any $K \ge 1$, with probability $1-\delta$, the regret of Algorithm~\ref{alg:main} is 
%$$\widetilde{O}\left(\min\{\sqrt{SAH^2K v^*}+SAH^2,Kv^* \}\right)$$ where $v^*$ is the value of the optimal policy, i.e., $v^* =\max_{\pi} \mathbb{E}_{\pi,s_1\sim \mu}\left[\sum_{h=1}^H r_h\right]$.}

% The proof of Theorem~\ref{thm:first} is similar to that of Theorem~\ref{thm1}, except for that we provide refined analysis for some of the regret terms. See  Appendix~\ref{sec:appfirst} for more details.


%
\paragraph{Value-based regret bounds.} 
%
Thus far, we have not yet introduced the crucial quantity $v^{\star}$ in Theorem~\ref{thm:first}, 
which we define now. 
%
When the initial states are drawn from $\mu$, $v^{star}$ stands for the weighted optimal value: 
%
\begin{equation}
	v^{*} \coloneqq \mathbb{E}_{s\sim \mu}\big[ V_1^*(s) \big]. 
	\label{eq:defn-vstar-formal}
\end{equation}
%
Encouragingly, 
the value-dependent regret bound in Theorem~\ref{thm:first} is still minimax-optimal, 
as asserted by the following lower bound. 
%
\begin{theorem}\label{thm:lb1} 
Consider any $p\in [0,1]$ and $K\geq 1$. 
For any learning algorithm, there exists an MDP with $S$ states, $A$ actions and horizon $H$
	obeying $v^*\leq  Hp$ and 
	%
	\begin{equation}
		\mathbb{E}\big[\mathsf{Regret}(K)\big]  \gtrsim \min\big\{ \sqrt{SAH^3Kp},\, KHp \big\} .
	\end{equation}
	%
\end{theorem}
%
In fact, the construction of the hard instance (as required in Theorem~\ref{thm:lb1}) is quite simple. 
Design a new branch with $0$ reward and set the probability of reaching this branch to be $1-p$. 
Also, with probability $p$, we direct the learner to a hard instance with regret $\Omega(\min\{\sqrt{SAH^3Kp},KpH\})$ and optimal value $H$. This guarantees that the optimal value $v^* \leq Hp$ and that the expected regret is at least $\Omega(\min\{ \sqrt{SAH^3Kp},KHp   \}) \gtrsim \min\{ \sqrt{SAH^2Kv^*},Kv^*   \}$.  
See Appendix~\ref{app:lb} for more details.


%Recall that in Section~\ref{sec:pre}, we assume the reward satisfies Assumption~\ref{assum1}, which means $R_{h,s,a}\in \Delta ([0,H])$ and $\sum_{h=1}^H r_h \leq H$. 
%


%
\paragraph{Cost-based regret bounds.} 
%
Next, we turn to the cost-aware regret bound as in Corollary~\ref{thm:cost}. 
Note that all other results except for Corollary~\ref{thm:cost} are about rewards as opposed to cost. 
In order to facilitate discussion, let us first formally introduce the cost-based scenarios. 



Suppose that the reward distributions $\{R_{h,s,a}\}_{(s,a,h)}$ are replaced with the cost distributions $\{C_{h,s,a}\}_{(s,a,h)}$, 
where each distribution $C_{h,s,a}\in \Delta([0,H])$ has mean $c_h(s,a)$. 
In the $h$-th step of an episode, the learner pays an immediate cost $c_h\sim C_{h,s_h,a_h}$ instead of receiving an immediate reward $r_h$,  
and the objective of the learner is instead to minimize the total cost $\sum_{h=1}^H c_h$ (in an expected sense). 
The optimal cost quantity $c^*$ is then defined as 
%
\begin{equation}
	c^* \coloneqq \min_{\pi}\mathbb{E}_{\pi,s_1\sim \mu}\bigg[\sum_{h=1}^H c_h \bigg]. 
\label{eq:defn-cstar-formal}
\end{equation}
%
Similarly, we can re-define the $Q$-function and value function as follows:
%
\begin{align}
	 \mathtt{Q}_{h}^{\pi}(s,a) &\coloneqq \mathbb{E}_{\pi}\left[\sum_{h'=h}^H c_{h'} \,\Big|\, (s_h,a_h)=(s,a)\right] ,
	 && \forall (s,a,h)\in \mathcal{S}\times \mathcal{A}\times [H], \nonumber
	\\ 
	\mathtt{V}_h^{\pi}(s) &\coloneqq \mathbb{E}_{\pi}\left[\sum_{h'=h}^H c_{h'} \,\Big|\, s_h = s\right],
	&& \forall (s,h)\in \mathcal{S}\times \times [H], 
	\nonumber
\end{align}
%
where we use different fonts to differentiate them from the original Q-function and value function. 
The optimal cost function is then given by $\mathtt{Q}_h^*(s,a) = \min_{\pi}\mathtt{Q}_h^{\pi}(s,a)$ and $\mathtt{V}_h^*(s)=  \min_{\pi}\mathtt{V}_h^{\pi}(s)$. 
Given the definitions above, 
we overload the notation $\mathsf{Regret}(K)$ to denote the regret for the cost-based scenario as 
$$
	\mathsf{Regret}(K) \coloneqq \sum_{k=1}^K \Big( \mathtt{V}^{\pi^k}_{1}(s_1^k) -  \mathtt{V}_1^*(s_1^k) \Big).
$$
%
One can also simply regard the cost minimization problem as  reward maximization with negative rewards by choosing $r_h = -c_h$. 
This way allows us to apply Algorithm~\ref{alg:main} directly, except that \eqref{eq:updateq} is replaced by 
%
\begin{align}
Q_h(s,a) \,\leftarrow\, \max\left\{\min \left\{ \widehat{r}_h(s,a) + \widehat{P}_{s,a,h}V_{h+1}+b_h(s,a), \, 0 \right\} ,\,-H \right\}.
	\label{eq:updatecost}
\end{align}
%
%
%We restate Theorem~\ref{thm:cost} as below.
%
%\noindent \textbf{Theorem 3. (Restated)}
%\emph{
%For any $K \ge 1$, with probability $1-\delta$, the regret of Algorithm~\ref{alg:main} is} 
%$$\widetilde{O}\left(\min\{\sqrt{SAH^2K c^*}+SAH^2,K(H-c^*) \}\right).$$
%
%
Note that the proof of Corollary~\ref{thm:cost} closely resembles that of Theorem~\ref{thm:first}, 
which can be found in Appendix~\ref{app:cost}.
%
%, except that $\mathbb{E}[\sum_{k=1}^K\sum_{h=1}^H r_h^k] \leq Kv^*$, while $\mathbb{E}[\sum_{k=1}^K\sum_{h=1}^H c_h^k] \geq Kc^*$. Refer to Appendix~\ref{app:cost} for more details.



To confirm the tightness of  Corollary~\ref{thm:cost}, we develop the following matching lower bound, 
which basically employs the same hard instance as in the proof of Theorem~\ref{thm:lb1}. 
%
\begin{corollary}\label{corollary:costlb}
Consider any $p\in [0,\frac{1}{4}]$ and any $K\geq 1$.
For any algorithm, one can construct an MDP with $S$ states, $A$ actions and horizon $H$ 
 obeying $c^*= \Theta(Hp)$ and 
	%
	\[
		\mathbb{E}\big[\mathsf{Regret}(K)\big] \gtrsim \min\big\{ \sqrt{SAH^3Kp}+SAH^2, \,KH(1-p) \big\} .
	\]
	%
\end{corollary}





\paragraph{Variance-dependent regret bound.}
%\input{variance}

The final regret bound presented in Theorem~\ref{thm:var} depends on a sort of variance metrics. 
Towards this end, let us first make precise the variance metrics of interest:
%
\begin{itemize}
	\item[(i)] The first variance metric is defined as 
		%
		\begin{equation}
			\mathrm{var}_1 \coloneqq \max_{\pi}\mathbb{E}_{\pi}\Bigg[\sum_{h=1}^H \mathbb{V}\big(P_{s_h,a_h,h},V_{h+1}^*\big)+\sum_{h=1}^H \mathrm{Var}\big(R_h(s_h,a_h)\big) \Bigg],
			\label{eq:defn-var1}
		\end{equation}
		%
		where $\{(s_h,a_h)\}_{1\leq h\leq H}$ represents a sample trajectory under policy $\pi$. 
		This captures the maximal possible expected sum of variance with respect to the optimal value function $\{V_{h}^*\}_{h=1}^H$. 
		
	\item[(ii)] Another useful variance metric is defined as
		%
		\begin{equation}
			\mathrm{var}_2 \coloneqq \max_{\pi,s}\mathrm{Var}_{\pi}\bigg[\sum_{h=1}^H r_h \,\Big|\, s_1=s\bigg],
			\label{eq:defn-var2}
		\end{equation}
		%
		where $\{r_h\}_{1\leq h\leq H}$ denotes a sample sequence of immediate rewards under policy $\pi$. 
		This indicates the maximal possible variance of the accumulative reward. 
%
\end{itemize}
%
The interested reader is referred  to \citet{zhou2023sharp} for further discussion about these two metrics. 
Our final variance metric is then defined as
%
\begin{align}
	\mathrm{var} \coloneqq \min\big\{\mathrm{var}_1,\mathrm{var}_2 \big\} .
	\label{eq:defn-var-formal}
\end{align}
%


 %and restate Theorem~\ref{thm:var} as below.

%\noindent\textbf{Theorem 4. (Restated)}\emph{
% With probability $1-\delta$, the regret of Algorithm~\ref{alg:main} is at most $\tilde{O}(\min\{\sqrt{SAHK\mathrm{var}}+SAH^2,HK\})$.}


With the above metric $\mathrm{var}$ in mind, we can then revisit Theorem~\ref{thm:var}. 
When the transition model is fully deterministic, the regret bound in Theorem~\ref{thm:var} simplifies to $$\mathsf{Regret}(K)\leq \widetilde{O}\big(\min\big\{SAH^2,\,HK\big\}\big)$$ for any $K\geq 1$, which is roughly the cost of visiting each state-action pair. 
%
The full proof of Theorem~\ref{thm:var} is postponed to Appenndix~\ref{app:var}. 
%depend on tighter bounds for $T_2$, $T_4$, $T_5$ and $T_6$. Refer to Appenndix~\ref{app:var} for more details. 



To finish up, let us develop a matching lower bound to corroborate the tightness and optimality of Theorem~\ref{thm:var}. 
%
\begin{theorem}\label{thm:lb3}
Consider any $p\in [0,1]$ and any $K\geq 1$. For any algorithm, one can find an MDP with $S$ states, $A$ actions, and horizon $H$ satisfying $\max\{\frac{\mathrm{var}_1}{H^2},\frac{\mathrm{var}_2}{H^2}\}\leq p$ and 
	$$	
		\mathbb{E}\big[\mathsf{Regret}(K)\big]  \gtrsim \min\big\{\sqrt{SAH^3Kp}+SAH^2,\,KH\big\}.
	$$
\end{theorem}

The proof of Theorem~\ref{thm:lb3} resembles that of Theorem~\ref{thm:lb1}, except that we need to construct a hard instance when $K\leq SAH/p$. For this purpose, we construct a fully deterministic MDP  (i.e., all of its transitions are deterministic and all rewards are fixed), and show that the learner has to visit about half of the state-action-layer tuples in order to learn a near-optimal policy. 
The proof details are deferred to Appendix~\ref{app:lb}.






