

In this section, we introduce the basics of tabular online RL, as well as some basic assumptions to be imposed throughout. 
% 
 

\paragraph{Basics of finite-horizon MDPs.}
%
This paper concentrates on time-inhomogeneous (or nonstatioanry) finite-horizon MDPs. 
Throughout the paper, 
we employ $\mathcal{S}=\{1,\ldots,S\}$ to denote the state space, 
$\mathcal{A}=\{1,\ldots,A\}$ the action space, and $H$ the planning horizon. 
The notation $P=\big\{P_h: \mathcal{S}\times \mathcal{A} \rightarrow \Delta(\mathcal{S}) \big\}_{1\leq h\leq H}$ denotes the probability transition kernel of the MDP; 
for any current state $s$ at any step $h$, if action $a$ is taken, then the state at the next step $h+1$ of the environment is randomly drawn from $P_{s,a,h}\coloneqq P_h(\cdot \mymid s,a) \in \Delta(\mathcal{S})$.   
Also, the notation $R=\big\{R_{h,s,a}\in \Delta([0,H]) \big\}_{1\leq h\leq H, s\in \mathcal{S}, a\in \mathcal{A}}$ 
indicates the reward distribution; that is,  
while executing action $a$ in state $s$ at step $h$, 
the agent receives an immediate reward --- which is non-negative and possibly stochastic --- drawn from the distribution $R_{h,s,a}$ with mean $r_h(s,a)$.   
Additionally, a deterministic policy $\pi=\{\pi_h: \mathcal{S}\rightarrow \mathcal{A}  \}_{1\leq h\leq H}$ 
stands for an action selection rule, so that the action selected in state $s$ at step $h$ is given by $\pi_h(s)$. 
The readers can consult standard textbooks (e.g., \citet{bertsekas2019reinforcement}) for more extensive descriptions.  





In each episode, the learner starts from an initial state $s_1$ independently drawn 
from some fixed (but unknown) distribution $\mu\in \Delta(\mathcal{S})$. 
For each step $1\leq h\leq H$, 
the learner takes action $a_h$, gains an immediate reward $r_h  \sim R_{h,s_h,a_h}$, 
and the environment transits to the state $s_{h+1}$ at step $h+1$ according to $P_{s_h,a_h,h}$.  
All of our results in this paper operate under the following assumption on the total reward. 
%
\begin{assumption}\label{assum1}
	In any episode, it holds that $0\leq \sum_{h=1}^H r_h\leq H$.
\end{assumption}
%
\noindent 
As can be easily seen, Assumption~\ref{assum1} is less stringent than another common choice that assumes 
$r_h\in [0,1]$ for any $h$ in any episode. 
In particular, Assumption~\ref{assum1} allows for sparse and spiky rewards along an episode; 
more discussions can be found in \cite{jiang2018open,wang2020long}.







\paragraph{Value function and Q-function.} 
%
For any given policy $\pi$, one can define the value function $V^{\pi}=\{V_h^{\pi}: \mathcal{S}\rightarrow \mathbb{R}\}$ 
and the $Q$-function $Q^{\pi}=\{Q_h^{\pi}: \mathcal{S}\times \mathcal{A}\rightarrow \mathbb{R}\}$ such that
%
\begin{subequations}
\begin{align}
	V_h^{\pi}(s) &\coloneqq \mathbb{E}_{\pi}\left[\sum_{h'=h}^H r_{h'} \,\Big|\, s_h=s\right], \qquad &&\forall (s,h)\in \mathcal{S}\times [H], \\
	Q_h^{\pi}(s,a) &\coloneqq \mathbb{E}_{\pi}\left[\sum_{h'=h}^H r_{h'} \,\Big|\, (s_h,a_h)=(s,a)\right],
	\qquad &&\forall (s,a,h)\in \mathcal{S}\times \mathcal{A}\times [H],
\end{align}
\end{subequations}
%
where the expectation $\mathbb{E}_{\pi}[\cdot]$ is taken over the randomness of the MDP under policy $\pi$, 
that is, the trajectory chooses $a_{h'} = \pi_h(s_h)$ for all $h\leq  h' \leq H$ (resp.~$h<  h' \leq H$) 
in the definition of $V_h^{\pi}$ (resp.~$Q_h^{\pi}$). 
Accordingly, we can define the optimal value function and the optimal $Q$-function respectively as follows: 
%
\begin{subequations}
\begin{align}
	V_h^*(s) &\coloneqq \max_{\pi}V^{\pi}_h(s) , \qquad &&\forall (s,h)\in \mathcal{S}\times [H],\\
	Q_h^*(s,a) &\coloneqq \max_{\pi}Q^{\pi}_h(s,a) \qquad &&\forall (s,a,h)\in \mathcal{S}\times \mathcal{A}\times [H]. 
\end{align}
\end{subequations}
%
Two important properties are worth mentioning: (a) the optimal value and the optimal Q-function are linked by the Bellman equation:
%
\begin{align}
Q_h^*(s,a) = r_h(s,a)+(P_{s,a,h})^{\top}V_{h+1}^*,\qquad V_h^*(s) = \max_{a'}Q_h^*(s,a'), 
\qquad \forall (s,a,h)\in \mathcal{S}\times \mathcal{A}\times[H]; 
\end{align}
%
(b) there exists a deterministic policy, denoted by $\pi^{*}$, that achieves optimality for all state-action-step tuples simultaneously, that is, 
%
\[
	V_h^{\pi^*}(s)=V_h^*(s) \qquad \text{and} \qquad 
	Q_h^{\pi^*}(s,a)=Q_h^*(s,a),\qquad \forall (s,a,h)\in \mathcal{S}\times \mathcal{A}\times[H]. 
\]


\paragraph{Data collection process and performance metrics.} 
%
During the learning process, 
the learner is allowed to collect $K$ episodes of samples (using arbitrary policies it selects). 
More precisely, in the $k$-th episode, 
the learner is given an independently generated initial state $s_1^k \sim \mu$, 
and executes policy $\pi^k$ (chosen based on data collected in previous episodes) to obtain a sample trajectory 
$\big\{ (s_h^k,a_h^k,r_h^k) \big\}_{1\leq h\leq H}$, 
with $s_h^k$, $a_h^k$ and $r_h^k$ denoting the state, action and immediate reward at step $h$ of this episode. 



To evaluate the learning performance, 
a widely used metric is the (cumulative) regret over all $K$ episodes:   
%
\begin{align}
	\mathsf{Regret}(K) \coloneqq \sum_{k=1}^K \left( V^*_{1}(s_1^k) -V^{\pi^k}_1(s_1^k) \right),
	\label{eq:defn-regret}
\end{align}
%
and our goal is to design an online RL algorithm that minimizes $\mathsf{Regret}(K)$ regardless of the allowable sample size $K$. 
It is also well-known (see, e.g., \citet{jin2018q}) that 
a regret bound can often be readily translated into a PAC sample complexity result (which counts the number of episodes needed to find an $\varepsilon$-optimal policy $\widehat{\pi}$ in the sense that $\mathbb{E}_{s_1 \sim \mu}\left[V_1^*(s_1) - V^{\widehat{\pi}}_1(s_1)\right] \le \varepsilon$).  
For instance, the standard reduction argument in \citet{jin2018q} reveals that: if an algorithm achieves  $\mathsf{Regret}(K)\leq f(S,A,H) K^{1-\alpha}$ for some function $f$ and some parameter $\alpha \in (0,1)$, then by randomly selecting a policy from $\{\pi^k\}_{1\leq k\leq K}$ as $\widehat{\pi}$ one achieves  $\mathbb{E}_{s_1 \sim \mu}\left[V_1^*(s_1) - V_1^{\widehat{\pi}}(s_1)\right] \lesssim f(S,A,H) K^{-\alpha}$, 
thus resulting in a sample complexity bound of $\big(\frac{f(S,A,H)}{\varepsilon}\big)^{1/\alpha}$.









