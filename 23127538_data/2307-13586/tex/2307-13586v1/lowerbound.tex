


In this section we focus on the proof of the lower bounds

\subsection{Proof of Theorem~\ref{thm:lb1}}\label{app:lbf}



Fix $(S,A,H)$. 
We start with the following lemma.
\begin{lemma}\label{lemma:lb2}
For any $K'\geq 1$, for any algorithm, there exists an MDP with $S$ states, $A$ actions and horizon $H$, such that the regret in $K'$ episodes is at least 
\begin{align}
\mathrm{Regret}(K')=\Omega( f(K')):=\Omega\left(\min\left\{\sqrt{SAH^3K'},K'H\right\}\right).\nonumber
\end{align}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:lb2}]
The hard instance is based on the hard instance JAO-MDP \citep{jaksch2010near,jin2018q}. In Appendix.D \citep{jin2018q}, the authors show that when $K\geq C_0SAH$ for some constant $C_0$, the minimax regret lower bound is $\Omega(\sqrt{SAH^3K})$. Now we focus on the regime $K\leq C_0SAH$. Without loss of generality, we assume $S=A=2$, and the generalization to arbitrary $(S,A)$ is routine.  Recall the definition of JAO-MDP \citep{jaksch2010near}. Let the two state be $x$ and $y$, and the two actions be $a$ and $b$. The reward  is always $x$ at state $1$ and always $\frac{1}{2}$ at state $y$. The transition model is give by $P_{x,a} =P_{x,b}= [1-\delta,\delta]^{\top}$, $P_{y,a} = [1-\delta,\delta]^{\top}, P_{y,b}= [1-\delta -\epsilon,\delta+\epsilon]$. Here we choose $\delta = \frac{C_1}{H}$ and $\epsilon =\frac{1}{H}$. Then the mixture time of the MDP is roughly $O(H)$. By choosing $C_1$ large enough, we can ensure that the MDP is $C_3$-mixing after the first half horizons for some proper constant $C_3\in (0,\frac{1}{2})$.

It is then easy to show that action $b$ is the optimal action for state $y$. Moreover, each time action $a$ is chosen at state $y$, the learner needs to pay regret $\Omega(\epsilon H)=\Omega(1)$. On the other hand, to discriminate action $a$ from action $b$ at state $y$ with probability $1-\frac{1}{10}$, the learner needs at least $\Omega(\frac{\epsilon}{\delta^2}) = \Omega(H)$ rounds, saying $C_4H$ rounds for some proper constant $C_4>0$. As a result, in the case $K\leq C_4H$, the minimax regret is at least $\Omega(KH^2\epsilon)=\Omega(KH)$. When $C_4H \leq K \leq C_0SAH = 4C_0H$, the minimax regret is at least $\Omega(C_4H^2)=\Omega(KH)$. The proof is completed.

\end{proof}



%Fix the algorithm $\mathcal{G}$.
Let $\mathcal{M}$ be the hard instance  for $K' = \max\left\{\frac{1}{10}Kp ,1\right\}$. We consider an MDP $\mathcal{M}'$ as below. In the first layer, for any state $s$, with probability $p$, the leaner transits to a copy of $\mathcal{M}$, and with probability $1-p$, the learner transits to a dumb state with $0$ reward. Then we have $v^*\leq pH$.
Let $X=X_1+X_2+\ldots+X_k$, where $\{X_i\}_{i=1}^K$ are i.i.d. Bernoulli random variables with mean $p$. Let $g(X,K')$ denote the minimax regret on the hard instance $\mathcal{M}$ in $X$ episodes. Clearly $g(X,K')$ is non-decreasing in $X$. 
Then $\mathrm{Regret}(K) \geq  \mathbb{E}[g(X,K')]$. In the case $Kp\geq 10$, by Lemma~\ref{lemma:con}, with probability $1/2$, $X\geq \frac{1}{10}Kp = K'$. Then it holds that $\mathbb{E}[g(X,K')] \geq \frac{1}{2}\cdot g(K',K')=\frac{1}{2}f(K')= \frac{1}{2}\cdot \Omega\left(\min\left\{\sqrt{SAH^3K'},K'H\right\}\right) = \Omega(\sqrt{SAH^3Kp},KHp)$. In the case $Kp<10$, with probability $1-(1-p)^K \geq (1-e^{-Kp})\geq  \frac{Kp}{30}$, $X\geq 1$. Then $\mathbb{E}[g(X,K')]\geq \frac{Kp}{30}\cdot g(1,K')=\frac{Kp}{30}\cdot g(1,1) =\Omega(KHp)$. The proof is completed.




\subsection{Proof of Corollary~\ref{corollary:costlb}}
Without loss of generality, we  assume $S=A=2$. Note that $p\leq 1/4$. 
We consider a hard instance where the learner needs to identify the correct action for each layer. Let $\mathcal{S}=\{s_1,s_2\}$. For any action $a$ and $h$, we set $P_{s_2,a,h} = \textbf{1}_{s_2}$ and $r_h(s_2,a)=0$. For any action $a\neq a^*$ and $h$, we also set $P_{s_1,a,h} = \textbf{1}_{s_2}$ and $c_h(s_2,a)=1$. At last, we set $P_{s_1,a^*,h} = \textbf{1}_{s_1}$ and $r_h(s_1,a^*) = p$. Let the initial state be $s_1$.
It is then clear that $c^* =Hp$ by choosing $a^*$ for each layer. To identify the correct action $a^*$ for at least half of the $H$ layers, we need $\Omega(H)$ episodes, which implies that, there exists $C_5>0$ such that in the first $K\leq C_5H$ episodes, the cost of the learner is at least $\frac{H(1-p)}{2}$. Then the minimax regret is at least $\Omega(K(H-c^*))=\Omega(KH^2(1-p))$ for $K\leq C_5H$.

In the case $C_5H \leq K \leq  \frac{100H}{p}$, the minimax regret is at least $\Omega(H(H-c^*))= \Omega(H^2(1-p))$. 

For $K\geq \frac{100H}{p}$, we let $\mathcal{M}$ be the  hard instance with the same transition as that in Lemma~\ref{lemma:lb2}, and set the cost function as $\frac{1}{2}$ for state $x$ and $1$ for state $y$ with respect to $K' = Kp/10 \geq 10H$.  
Let $\mathcal{M}'$ be the MDP such that, in the first layer,  with probability $p$, the learner transits to a copy of $\mathcal{M}$, and with probability $1-p$, the learner transits to a dumb state with $0$ cost. Then $c^* =\Theta(Hp)$. 
Using Lemma~\ref{lemma:con}, with probability $\frac{1}{2}$, $ X\geq \frac{1}{3}Kp - \log(2)\geq \frac{1}{6}Kp$. Then $\mathrm{Regret}(K)$ is at least $\frac{1}{2}\cdot \Omega\left(\min\{\sqrt{H^3K'}, K'H\}\right)= \Omega\left(\sqrt{H^3Kp}\right)$. 

The proof is completed by combining the minimax regret lower bounds for the three regimes $K\in [1,C_5H], (C_5H,\frac{100H}{p}], (\frac{100H}{p},\infty]$.





%Mention the lower bound of $\Omega(KH)$ when $K\leq BSAH$ ($K\leq \frac{SAH^2}{v^*}, \frac{SAH^3}{\mathrm{var}_1}, \frac{SAH^3}{\mathrm{var}_2}$)





\subsection{Proof of Theorem~\ref{thm:lb3}}


For $K\geq SAH/p$, the lower bound in Theorem~\ref{thm:lb1} applies because the regret is at least $\Omega(\sqrt{SAH^3Kp})$ and the variance $\mathrm{var}$ is at most $pH^2$. 
On the other hand, for $1\leq K\leq SAH$, 
by Lemma~\ref{lemma:lb34}, the minimax regret is at least $\Omega(KH)$. For $SAH\leq K \leq SAH/p$, the regret is at least $\Omega(SAH^2)=\Omega(\min\{\sqrt{SAH^3Kp}+SAH^2,KH \})$.
The proof is completed.


\begin{lemma}\label{lemma:lb34}
Fix $1\leq K\leq SAH$. There exists  an MDP with $S$ states, $A$ actions, horizon $H$, and $\mathrm{var}_1 = \mathrm{var}_2 = 0$, such that the regret is at least $\Omega(KH)$.
\end{lemma}

\begin{proof}
We consider an MDP with deterministic transition. That is, for each $(s,a,h)$, there is some $s'$ such that $P_{s,a,h,s'}=1$ and $P_{s,a,h,s''}=0$ for any $s''\neq s'$. The reward function is also deterministic. In this case, it is easy to verify that $\mathrm{var}_1 = \mathrm{var}_2 = 0$.


We first assume $S=2$. For any action $a$ and horizon $h$, we set $P_{s_2,a,h} = \textbf{1}_{s_2}$ and $r_h(s_2,a)=0$. For any action $a\neq a^*$ and $h$, we also set $P_{s_1,a,h} = \textbf{1}_{s_2}$ and $r_h(s_2,a)=0$. At last, we set $P_{s_1,a^*,h} = \textbf{1}_{s_1}$ and $r_h(s_1,a^*) = 1$. In other words, there are a dumb state and a normal state in each horizon. The learner hopes to find the correct action to avoid the dumb state. Obviously, $V_1^*(s_1)=H$. To find a $\frac{H}{2}$-optimal policy, the learner needs to identify $a^*$ for the first $\frac{H}{2}$ horizons, which needs at least $\Omega(HA)$ rounds in expectation. As a result, the minimax regret is at least $\Omega(KH)$ for $K\leq cHA$ with some proper constant $c$. 



We name the hard instance above as a \emph{hard chain}.
For general $S$, we construct $d:=\frac{S}{2}$ hard chains. Let the two states in the $i$-th be $(s_1(i),s_2(i) )$. We set the initial distribution to be the uniform distribution over $\{s_1(i)\}_{i=1}^d$. Then $V_1^*(s_1(i))=H$ for any $1\leq i\leq d$.  Let $\mathrm{Regret}_i(K)$ be the expected regret due to the $i$-th hard chain. 
When $K\geq 100S$, by Lemma~\ref{lemma:con}, with probability $\frac{1}{2}$, $s_1(i)$ is visited for at least $\frac{K}{10S}\geq 10$ times. As a result, we have that $\mathrm{Regret}_i(K)\geq \frac{1}{2}\cdot \Omega\left(\frac{KH}{S}\right)$. Taking sum over $i$, we learn that the total regret is at least $\sum_{i=1}^d \mathrm{Regret}_i(K) = \Omega(KH)$. When $K<100S$, with probability $1-(1-\frac{1}{S})^K\geq 0.0001\frac{K}{S}$, $s_1(i)$ is visited for at least one time. Therefore, $\mathrm{Regret}_i(K)\geq \Omega(\frac{KH}{S})$. Taking sum over $i$, we obtain that $\mathrm{Regret}(K) = \sum_{i=1}^K \mathrm{Regret}_i(K) =\Omega(KH)$.





\end{proof}