

In this section, 
we formally describe our algorithm: a simple variation of the model-based algorithm called {\em Monotonic Value Propagation} proposed by \citet{zhang2020reinforcement}. 
%
We present the full procedure in Algorithm~\ref{alg:main}, and point out several key ingredients. 
%
\begin{itemize}
	\item {\em Optimistic updates using upper confidence bounds (UCB).} The algorithm implements the optimism principle in the face of uncertainty 
by adopting the frequently used UCB-based framework \citep{azar2017minimax,jin2018q}. 
More specifically, the learner maintains upper estimates for both the value and Q-function, 
by calculating the following optimistic Bellman equation backward (from $h=H,\ldots,1$): 
%
\begin{subequations}
\begin{align}
Q_{h}(s,a)\, & \leftarrow\,\min\big\{\widehat{r}_{h}(s,a)+\langle\widehat{P}_{s,a,h},V_{h+1}\rangle+b_{h}(s,a),H\big\}, 
	\label{eq:Qh-UCB-informal}\\
V_{h}(s)\, & \leftarrow\, \max\nolimits_{a}Q_{h}(s,a). 
\end{align}
\end{subequations}
%
Here, $Q_{h}$ (resp.~$V_h$) is the running estimate for the Q-function (resp.~value function), 
		$\widehat{r}_{h}(s,a) \in \mathbb{R}$ is an estimate of the mean reward at $(s,a,h)$, 
$\widehat{P}_{s,a,h}\in \mathbb{R}^S$ indicates an estimate of the transition probability vector from $(s,a,h)$, 
whereas $b_{h}(s,a)\geq 0$ is some suitably chosen bonus term that compensates for the uncertainty.   



	\item {\em Monotonic bonus functions.} Another crucial step in order to ensure near-optimal regret lies in careful designs of the data-driven bonus terms $\{b_h(s,a)\}$ in \eqref{eq:Qh-UCB-informal}. 
		Here, we adopt the monotonic bonus function for MVP originally proposed in \citet{zhang2020reinforcement}, 
		to be made precise in \eqref{eq:update1}. 
		Compared to the bonus function in $\mathtt{Euler}$~\citep{zanette2019tighter} and $\mathtt{UCBVI}$~\citep{azar2017minimax},  the monotonic bonus form has a cleaner structure that effectively avoid large lower order terms. In order to enable variance-aware regret, we also need to keep track of the empirical variance of the (stochastic) immediate rewards.


	\item {\em An epoch-based procedure and a doubling trick.} 
%
		A key step of our algorithm is to update the empirical transition kernel and empirical rewards in an epoch-based fashion. 
		More concretely, the whole learning process is divided into several consecutive epochs via a simple doubling rule.
		That is, once the number of visits to a $(s,a,h)$-tuple reaches a power of 2, we end the current epoch,  reconstruct the empirical transition kernel and rewards using data from this epoch (cf.~lines~\ref{line:r-hsa-update} and \ref{line:P-hsa-update} of Algorithm~\ref{alg:main}), compute the Q-function and value function using the newly updated transition kernel and rewards (cf.~\eqref{eq:updateq}), and then start a new epoch. In each epoch, the learned policy is induced by the optimistic Q-function estimate computed based on the empirical transition kernel of the {\em current} epoch. 




\end{itemize}




\begin{remark}[Doubling batch]
We note that a doubling update rule has also been used in the original MVP \citep{zhang2020reinforcement}. 
A major difference between our modified version and the original one is that: when the visitation count for some $(s,a,h)$ reaches $2^i$ for some integer $i$, we only use the second half of the samples (i.e., the $\{2^{i-1}+j\}_{j=1}^{2^{i-1}}$-th samples) to compute the empirical model, whereas the original MVP makes use of all the $2^i$ samples. This step is crucial for decoupling statistical dependence.

\end{remark}







\begin{algorithm}[h]
	\DontPrintSemicolon
\caption{Monotoinic Value Propagation (MVP)~\citep{zhang2020reinforcement}\label{alg:main}}
%\begin{algorithmic}[ht]
	\textbf{input:} state space $\mathcal{S}$, action space $\mathcal{A}$, horizon $H$, total number of episodes $K$, confidence parameter $\delta$, 
	%$\mathcal{L}=\{1,2,\ldots, 2^{\log_2K}\}$, 
	$c_1=\frac{460}{9}$, $c_2 = 2\sqrt{2}$ and $c_3=\frac{544}{9}$.  \\
%\textbf{Initialize:} . \\
	\textbf{initialization: } for all $(s,a,s',h)\in \mathcal{S}\times \mathcal{A}\times\mathcal{S}\times [H]$, set $\theta_h(s,a)\leftarrow 0$, $\kappa_h(s,a)\leftarrow 0$, $\overline{N}_h(s,a,s')\leftarrow 0$, $N_h(s,a,s')\leftarrow 0$, $n_h(s,a)\leftarrow 0$, $Q_h(s,a)\leftarrow H-h+1$, $V_h(s)\leftarrow H-h+1$. \\
	\For{$k=1,2,...$} {
		Set $\pi^k$ such that $\pi_h^k(s) = \arg\max_{a}Q_h(s,a)$ for all $s\in \mathcal{S}$ and $h\in [H]$. {\color{blue}\tcc{policy iterate.}}
		\For {$h=1,2,...,H$} {
			Observe $s_{h}^k$, 
			take action $ a_h^k= \arg\max_{a}Q_h(s_h^k,a)$, 
%	%	\STATE{ Receive reward $r_h^k$ (receive $c_h^k$ and set $r_h^k = -c_h^k$ for the cost case) and  observe $s_{h+1}^k$.}
			receive  $r_h^k$,  observe $s_{h+1}^k$. \label{line:choose_action} 
			{\color{blue}\tcc{sampling.}}
			$(s,a,s')\leftarrow (s_h^k,a_h^k,s_{h+1}^k)$. \\
			Update $\overline{N}_h(s,a) \leftarrow  \overline{N}_h( s,a )+1$, $N_h(s,a,s') \leftarrow   N_h(s,a,s')+1$, $\theta_h(s,a)\leftarrow \theta_h(s,a)+r_h^k$, $\kappa_h(s,a)\leftarrow \kappa_h(s,a)+(r_h^k)^2$. \\
		{\color{blue}\tcc{perform updates using data of this epoch.}}
		\If{$\overline{N}_h(s,a)\in \{1,2,\ldots, 2^{\log_2K}\}$ \label{line:rp_update_start} }   {
			$n_h(s,a)\leftarrow \sum_{\widetilde{s}}N_h(s,a,\widetilde{s})$. 
			{\color{blue}\tcp{number of visits to $(s,a,h)$ in this epoch.}}
			$\widehat{r}_h(s,a)\leftarrow \frac{\theta_h(s,a)}{n_h(s,a)}$. \label{line:r-hsa-update}
			{\color{blue}\tcp{empirical rewards of this epoch.}} 
			$\widehat{\sigma}_h(s,a)\leftarrow \frac{\kappa_h(s,a)}{n_h(s,a) }  $. 
			{\color{blue}\tcp{empirical squared rewards of this epoch.}}
			$\widehat{P}_{s,a,h}(\widetilde{s}) \leftarrow  \frac{N_h(s,a,\widetilde{s})}{n_h(s,a)}$ for all $\widetilde{s} \in \mathcal{S}$.  \label{line:P-hsa-update}
			{\color{blue}\tcp{empirical transition for this epoch.}}
			%
			Set TRIGGERED = TRUE, 
			and $\theta_h(s,a)\leftarrow 0$, $\kappa_h(s,a)\leftarrow 0$,  $N_h(s,a,\widetilde{s})\leftarrow 0$  for all $\widetilde{s}\in \mathcal{S}$. 
%		\ENDIF \label{line:rp_update_end}
		}
		}
		{\color{blue}\tcc{optimistic Q-estimation using empirical model of this epoch.}}
		\If {\textnormal{TRIGGERED= TRUE}} {
			Set TRIGGERED = FALSE, and $V_{H+1}(s)\leftarrow 0$ for all $s\in \mathcal{S}$. \\
			\For{$h=H,H-1,...,1$} {
				%
				\For{$(s,a)\in \mathcal{S}\times \mathcal{A}$} {

					%\vspace{-3ex}
					\begin{align} 
						\vspace{-3ex}
						b_h(s,a) &\leftarrow c_1 \sqrt{\frac{   \mathbb{ V}(\widehat{P}_{s,a,h} ,V_{h+1}) \log \frac{1}{\delta}  }{ \max\{n_h(s,a),1 \} }}+c_2 \sqrt{\frac{\big(\widehat{\sigma}_h(s,a)- (\widehat{r}_h(s,a))^2 \big)\log \frac{1}{\delta}}{\max\{n_h(s,a),1\}}} \qquad\qquad\qquad\qquad\qquad\qquad\nonumber\\
						&\qquad\qquad\qquad +c_3\frac{H\log \frac{1}{\delta}}{ \max\{n_h(s,a) ,1\}  },  \label{eq:update1}  \\
						Q_h(s,a) &\leftarrow \min\big\{    \widehat{r}_h(s,a)+\langle \widehat{P}_{s,a,h}, V_{h+1} \rangle +b_h(s,a)    ,H\big\},\,
						V_{h}(s) \leftarrow \max_{a}Q_h(s,a).
						\label{eq:updateq}
					\end{align}
					\vspace{-3ex}
				}
			}
			%
		}
	}
%\end{algorithmic}
\end{algorithm}






















\begin{comment}

\begin{algorithm}
	\DontPrintSemicolon
\caption{Monotoinic Value Propagation (MVP)~\citep{zhang2020reinforcement}\label{alg:main}}
\begin{algorithmic}[ht]
\STATE{\textbf{Input:} number of states $S$, number of actions $A$, horizon $H$, total number of episodes $K$, confidence parameter $\delta$;}
\STATE{\textbf{Initialize:} $\mathcal{L}=\{1,2,\ldots, 2^{\log_2(K)}\}$; $c_1=\frac{460}{9}, c_2 = 2\sqrt{2},  c_3=\frac{544}{9}$;}
\STATE{\textbf{Initialization: } $\theta_h(s,a)\leftarrow 0, \kappa_h(s,a)\leftarrow 0, \overline{N}_h(s,a,s')\leftarrow 0, N_h(s,a,s')\leftarrow 0, n_h(s,a)\leftarrow 0, Q_h(s,a)\leftarrow H-h+1, V_h(s)\leftarrow H-h+1, \forall (s,a,s',h)\in \mathcal{S}\times \mathcal{A}\times\mathcal{S}\times [H]$;}
\FOR {$k=1,2,...$}
		\FOR {$h=1,2,...,H$}
		\STATE{ Observe $s_{h}^k$;}
		\STATE{ Take action $ a_h^k= \arg\max_{a}Q_h(s_h^k,a)$;} \label{line:choose_action}
	%	\STATE{ Receive reward $r_h^k$ (receive $c_h^k$ and set $r_h^k = -c_h^k$ for the cost case) and  observe $s_{h+1}^k$.}
 \STATE{ Receive reward $r_h^k$  and  observe $s_{h+1}^k$.}
		\STATE{ Set $(s,a,s')\leftarrow (s_h^k,a_h^k,s_{h+1}^k)$;.}
		\STATE{ Set $\overline{N}_h(s,a) \leftarrow  \overline{N}_h( s,a )+1$, \ \,$N_h(s,a,s') \leftarrow   N_h(s,a,s')+1$, $\theta_h(s,a)\leftarrow \theta_h(s,a)+r_h^k$, $\kappa_h(s,a)\leftarrow \kappa_h(s,a)+(r_h^k)^2$.}
		\STATE{ \verb|\\| \emph{Update empirical rewards and transition probability}}
		\IF {$\overline{N}_h(s,a)\in \mathcal{L}$}  \label{line:rp_update_start}
  \STATE{Set $\widehat{r}_h(s,a)\leftarrow \frac{\theta_h(s,a)}{\sum_{\widetilde{s}'}N_h(s,a,\widetilde{s}')}$;}
  \STATE{Set $\widehat{\sigma}_h(s,a)\leftarrow \frac{\kappa_h(s,a)}{\sum_{\widetilde{s}'}N_h(s,a,\widetilde{s'}) } - (\widehat{r}_h(s,a))^2 $}
		\STATE Set $\widehat{P}_{s,a,h,\widetilde{s}} \leftarrow  \frac{N_h(s,a,\widetilde{s})}{\sum_{\widetilde{s}'}N_h(s,a,\widetilde{s}')}$ for all $\widetilde{s} \in \mathcal{S}$.
		%			\ENDFOR
  \STATE{}
		\STATE{ Set $n_h(s,a)\leftarrow \sum_{\widetilde{s}'}N_h(s,a,\widetilde{s}')$;}
		\STATE{ Set TRIGGERED = TRUE.}
  \STATE{$\theta_h(s,a)\leftarrow 0, \kappa_h(s,a)\leftarrow 0,  N_h(s,a,\widetilde{s})\leftarrow 0, \forall \widetilde{s}\in \mathcal{S}$;}
		\ENDIF \label{line:rp_update_end}
		\ENDFOR
		\STATE{ \verb|\\| \emph{Update $Q$-function}}
		\IF {TRIGGERED}
  \STATE{$V_{H+1}(s)\leftarrow 0,\forall s\in \mathcal{S}$;}
		\FOR{$h=H,H-1,...,1$}
		\FOR{$(s,a)\in \mathcal{S}\times \mathcal{A}$}
		%		  \vspace{-3ex}
		\STATE 	 {		%\vspace{-0.5cm} 	
			%					\vspace{-0.5cm}
			%			\simon{Maybe move to the line $N(s,a) \in \mathcal{L}$ so it's clear for the reader that we update the policy only in the set.}
			Set
			\begin{align} 
			~~~~~~~~~~~~~&b_h(s,a)\leftarrow c_1 \sqrt{\frac{   \mathbb{ V}(\widehat{P}_{s,a,h} ,V_{h+1}) \log(\frac{1}{\delta})  }{ \max\{n_h(s,a),1 \} }}+c_2 \sqrt{\frac{(\widehat{\sigma}_h(s,a)- (\widehat{r}_h(s,a))^2 )\log(\frac{1}{\delta})}{\max\{n_h(s,a),1\}}}+c_3\frac{H\log(\frac{1}{\delta})}{ \max\{n_h(s,a) ,1\}  },  \label{eq:update1}  \\
			%\\ 	\hspace{-20ex} 	&  \left\{\begin{array}{l}   Q_h(s,a)\leftarrow \min\{    \widehat{r}_h(s,a)+\widehat{P}_{s,a,h} V_{h+1} +b_h(s,a)    ,H\} \quad  \text{for reward case} \\ Q_h(s,a)\leftarrow \max\{\min \left\{ \widehat{r}_h(s,a) + \widehat{P}_{s,a,h}V_{h+1}+b_h(s,a), 0 \right\} ,-H\} \quad \text{for cost case}\label{eq:updateq}\end{array}\right.
   \hspace{-20ex} 	&    Q_h(s,a)\leftarrow \min\{    \widehat{r}_h(s,a)+\widehat{P}_{s,a,h} V_{h+1} +b_h(s,a)    ,H\}\label{eq:updateq}
			\\ & V_{h}(s) \leftarrow \max_{a}Q_h(s,a).\nonumber
			\end{align}
			\vspace{-3ex}
		}
		\ENDFOR
		\ENDFOR
		\STATE{ Set TRIGGERED = FALSE}
		\ENDIF
		\ENDFOR
\end{algorithmic}
\end{algorithm}

\end{comment}


