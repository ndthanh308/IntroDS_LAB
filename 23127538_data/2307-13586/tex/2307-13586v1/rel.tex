



\label{sec:rel}
Let us take a moment to discuss several related theoretical works on tabular RL; 
there has also been an active line of research that exploits low-dimensional function approximation to further reduce sample complexity, 
which is beyond the scope of this paper. 


Our discussion below focuses on two mainstream approaches that have received widespread adoption: the model-based approach and the model-free approach. 
In a nutshell, 
model-based algorithms decouple model estimation and policy learning, and often use the learned transition kernel to
 compute the value function and find a desired policy. 
In stark contrast, the model-free approach attempts to estimate the optimal value function and optimal policy directly without explicit estimation of the model. 
%While both approaches are capable of achieving asymptotic sample optimality, 
%thus far only the model-based approach has been shown to fully eliminate the burn-in cost, regardless of the sampling mechanism in use \citet{li2020breaking, li2022settling}.  
%On the other hand, model-free algorithms are often more advantageous in terms of memory complexity. 
In general, model-free algorithms only require $O(SAH)$ memory for the purpose of storing Q-functions and value functions, while the model-based counterpart might require $O(S^2AH)$ space in order to store the estimated transition kernel.  



\paragraph{Sample complexity for RL with a simulator and offline RL.} 
%
As an idealistic setting that separates the problem of exploration from that of estimation, 
RL with a simulator (or generative model) has been studied by numerous works, 
allowing the learner to query any state-action pairs and draw independent samples~\citep{kearns1998finite,kakade2003sample,azar2013minimax,agarwal2020model,wainwright2019variance,wainwright2019stochastic,sidford2018near,sidford2018variance,chen2020finite,li2020breaking,pananjady2020instance,li2023q,li2022minimax,even2003learning,shi2023curious,beck2012error,cui2021minimax}.
While both model-based and model-free approaches are capable of achieving asymptotic sample optimality \citep{sidford2018variance,wainwright2019variance,azar2013minimax,agarwal2020model}, 
all model-free algorithms that enjoy asymptotically optimal sample complexity suffer from dramatic burn-in cost. 
Thus far,  only the model-based approach has been shown to fully eliminate the burn-in cost  
for both discounted  infinite-horizon MDPs and inhomogeneous finite-horizon MDPs \citep{li2020breaking}.  
The optimal sample complexity for time-homogeneous finite-horizon MDPs remains open.  





\paragraph{Sample complexity for offline RL.} 
%
The emergent subfield of offline RL is concerned with learning based purely on a pre-collected dataset 
\citep{levine2020offline}. 
A frequently used mathematical model assumes that historical data are collected (often independently) using some behavior policy, 
and the key challenges (compared with RL with a simulator) come from distribution shift and incomplete data coverage. 
%
The sample complexity of offline RL has been the focus of 
a large strand of recent works, with asymptotically optimal sample complexity achieved by multiple algorithms~\citep{jin2021pessimism,xie2021policy,yin2022near,ren2021nearly,shi2022pessimistic,qu2020finite,yan2022efficacy,rashidinejad2021bridging,li2022settling,li2021sample,wang2022gap}.
Akin to the simulator setting, the fully optimal sample complexity (without burn-in cost) has only been achieved via the model-based approach when it comes to discounted infinite-horizon and inhomogeneous finite-horizon settings~\citep{li2022settling}. 
All model-free methods incur substantial burn-in cost. Time-homogeneous finite-horizon MDPs also remain unsettled. 


\paragraph{Sample complexity for online RL.} 
Obtaining optimal sample complexity (or regret bound) in online RL without incurring any burn-in cost 
has been one of the most fundamental open problems in RL theory. 
In fact, the past decades have witnessed a flurry of activity towards improving the sample efficiency of online RL,  
partial examples including \citet{kearns1998near,brafman2002r,kakade2003sample,strehl2006pac,strehl2008analysis,kolter2009near,bartlett2009regal,jaksch2010near,szita2010model,lattimore2012pac,osband2013more,dann2015sample,agralwal2017optimistic,dann2017unifying,jin2018q,efroni2019tight,fruit2018efficient,zanette2019tighter,cai2019provably,dong2019q,russo2019worst,pacchiano2020optimism,neu2020unifying,zhang2020almost,zhang2020reinforcement,tarbouriech2021stochastic,xiong2021randomized,menard2021ucb,wang2020long,li2021settling,li2021breaking,domingues2021episodic,zhang2022horizon,li2023minimax,ji2023regret}. 
% 
Unfortunately, no work has settled this problem completely: 
the state-of-the-art result for model-based algorithms still incurs a burn-in that scales at least quadratically in $S$ 
\citep{zhang2020reinforcement}, while the burn-in cost of the best model-free algorithms (particularly with the aid of variance reduction introduced in \citet{zhang2020almost}) still suffers from sub-optimal horizon dependency \citep{li2021breaking}.  








\subsection{Notation} 
%
Before proceeding, let us introduce a set of notation to be used throughout. 
%
For any set $\mathcal{X}$,  $\Delta(\mathcal{X})$ represents the set of probability distributions over the set $\mathcal{X}$. 
For any positive integer $N$, we denote $[N]=\{1,\ldots,N\}$.
For any two vectors $x,y$ with the same dimension, 
we use $xy$ to abbreviate $x^{\top}y$. 
For any integer $S>0$, any probability vector $p\in \Delta([S])$ and another vector $v=[v_i]_{1\leq i\leq S}$, 
we denote by $\mathbb{V}(p,v) \coloneqq p v^2 - (pv)^2$ the associated variance,  
%Let $\{V^*_h\}_{h=1}^H$ and $\{Q_h^*\}_{h=1}^H$ denote respectively the optimal value function and $Q$-function. 
where $v^2=[v_i^2]_{1\leq i\leq S}$ represents element-wise square of $v$. 
%Let $[N]$ denote the set $\{1,\ldots,N\}$. 
Let $\textbf{1}$ and $\textbf{0}$ indicate respectively the all-one vector and the all-zero vector. Let $\textbf{1}_{s}$ denote the vector with $1$ at the $s$-th coordinate and $0$ at other coordinates.
 We shall often use $\{\cdot\}_{(\lambda)}$ as shorthand for $\{\cdot\}_{\lambda \in \Lambda}$, where $\Lambda$ is the set of all proper choices of the index $\lambda$; 
for example, $\{\cdot \}_{(s,a,h,k)}$ denotes $\{\cdot \}_{(s,a,h,k)\in \mathcal{S}\times \mathcal{A}\times [H]\times [K]}$.  Without loss of generality, we assume throughout that  $K$ is a power of $2$ to streamline presentation.  



%
