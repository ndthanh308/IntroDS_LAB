\begin{abstract}
%
A central issue lying at the heart of online reinforcement learning (RL) is data efficiency.  
While a number of recent works achieved asymptotically minimal regret in online RL, 
the optimality of these results is only guaranteed in a ``large-sample'' regime, 
imposing enormous burn-in cost in order for their algorithms to operate optimally. 
How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory.  

We settle this problem for the context of finite-horizon inhomogeneous Markov decision processes. 
Specifically, we prove that a modified version of Monotonic Value Propagation (MVP), a model-based algorithm proposed by \cite{zhang2020reinforcement}, achieves a regret on the order of (modulo log factors)
%
\begin{equation*}
	\min\big\{  \sqrt{SAH^3K}, \,HK \big\},
\end{equation*}
% 
where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, and $K$ is the total number of episodes. 
This regret matches the minimax lower bound for the entire range of sample size $K\geq 1$, essentially eliminating any burn-in requirement. It also translates to a PAC sample complexity (i.e., the number of episodes needed to yield $\varepsilon$-accuracy) of $\frac{SAH^3}{\varepsilon^2}$ up to log factor, which is minimax-optimal for the full $\varepsilon$-range. 
 Further, we extend our theory to unveil the influences of problem-dependent quantities like the optimal value/cost and certain variances.  
The key technical innovation lies in the development of a new regret decomposition strategy and a novel analysis paradigm to decouple complicated statistical dependency --- a long-standing challenge facing the analysis of online RL in the sample-hungry regime. 

\end{abstract}



\noindent \textbf{Keywords:} online RL; minimax regret; burn-in cost; optimal sample complexity; model-based algorithms







\begin{comment}
%
\begin{abstract}
We obtain the optimal regret bound in time-inhomogeneous finite-horizon Markov Decision Processes (modulo logarithmic factors).
Specifically, we show that a modified version of the model-based algorithm, Monotonic Value Propagation (MVP) proposed by \cite{zhang2020reinforcement}, achieves the minimax regret bound $\tilde{\Theta}(\min\{  \sqrt{SAH^3K)},HK\})$.
% \footnote{The $\tilde{O}(\cdot)$ operator hides the logarithmic terms of $(S,A,H,K)$. }, 
Here $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, the total reward is bounded by $H$, and $K$ is the total number of episodes. 
As a direct corollary, we obtain a minimax PAC bound $\widetilde{\Theta}\left(\frac{SAH^3}{\varepsilon^2}\right)$ where $\epsilon \in (0,H]$ is the target accuracy.
Notably, these are the first minimax optimal bounds that
\emph{accommodate the entire range of sample sizes} whereas prior works at least require $K = \Omega\left(S^3AH\right)$ or $\Omega\left(SAH^5\right)$ for regret bounds, or $\epsilon = O\left(H/S\right)$ or $O\left(1/H^2\right)$ for PAC bounds.
% thereby settling the one major open problem in theoretical reinforcement learning.


Going beyond worst-case bounds, we obtain a first-order bound $\widetilde{\Theta}\left(\min\{\sqrt{v^* SAH^2K} + SAH^2 , Kv^*\}\right)$ where $v^*$ is the optimal value, a small-loss bound $\widetilde{\Theta}\left(\min\big\{\sqrt{SAH^2K c^*}+SAH^2,\, Kc^* \big\}\right)$ where $c*$ is the cost of optimal the policy in the, and a variance-dependent bound $\widetilde{\Theta}\left(\min\{\sqrt{\mathrm{var}SAHK} + SAH^2,HK\}\right)$ where $\mathrm{var}$ is the environment norm that characterizes the randomness of the Markov Decision Processes. We also provide lower bounds to show these upper bounds are optimal.
% Lastly, we provide lower bounds $\Omega\left(\sqrt{v^* SAH^2K} + SAH^2\right)$ and $\Omega\left(\sqrt{\mathrm{var}SAH^2K} + SAH^2\right)$ to show our first-order and variance-dependent bounds are optimal.

% also the optimal first-order regret bound and variance-dependent regret bound.\simon{TO DO: add details}

To achieve these bounds, we resolve one long-standing technical challenge in analyzing model-based reinforcement learning algorithms: the dependence between the estimated transition probabilities and the estimated value function of the next time step.
We introduce a new notion, \emph{profile}, to describe the sampling status of each state-action pair.
We further exploit the monotonicity of profiles during the learning procedure to provide a sharp bound of the size of possible profiles.
% Our algorithm is a modified version of Monotonic Value Propagation (MVP) by Zhang et al.~\cite{zhang2020reinforcement}, where we decouple the empirical transition model from the empirical value function by introducing the \emph{profile} of the whole learning process.  
This technique allows us to employ a new regret decomposition analysis that is more straightforward and qualitatively different from prior works.
We believe our new techniques will have broad applications in other reinforcement learning problems.
%
%    
\end{abstract}
%
\end{comment}

