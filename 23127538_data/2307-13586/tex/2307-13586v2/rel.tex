



\label{sec:rel}
Let us take a moment to discuss several related theoretical works on tabular RL. 
Note that there has also been an active line of research that exploits low-dimensional function approximation to further reduce sample complexity, 
which is beyond the scope of this paper. 


Our discussion below focuses on two mainstream approaches that have received widespread adoption: the model-based approach and the model-free approach. 
In a nutshell, 
model-based algorithms decouple model estimation and policy learning, and often use the learned transition kernel to
 compute the value function and find a desired policy. 
In stark contrast, the model-free approach attempts to estimate the optimal value function and optimal policy directly without explicit estimation of the model. 
In general, model-free algorithms only require $O(SAH)$ memory --- needed when storing the running estimates for Q-functions and value functions --- while the model-based counterpart might require $\Omega(S^2AH)$ space in order to store the estimated transition kernel.  



\paragraph{Sample complexity for RL with a simulator.} 
%
As an idealistic setting that separates the consideration of exploration from that of estimation, 
RL with a simulator (or generative model) has been studied by numerous works, 
allowing the learner to draw independent samples for any state-action pairs~\citep{kearns1998finite,pananjady2020instance,kakade2003sample,azar2013minimax,agarwal2020model,wainwright2019variance,wainwright2019stochastic,sidford2018near,sidford2018variance,chen2020finite,li2020breaking,li2023q,li2022minimax,even2003learning,shi2023curious,beck2012error,cui2021minimax}.
While both model-based and model-free approaches are capable of achieving asymptotic sample optimality \citep{sidford2018variance,wainwright2019variance,azar2013minimax,agarwal2020model}, 
all model-free algorithms that enjoy asymptotically optimal sample complexity suffer from dramatic burn-in cost. 
Thus far,  only the model-based approach has been shown to fully eliminate the burn-in cost  
for both discounted  infinite-horizon MDPs and inhomogeneous finite-horizon MDPs \citep{li2020breaking}.  
The full-range optimal sample complexity for time-homogeneous finite-horizon MDPs in the presence of a simulator remains open.  





\paragraph{Sample complexity for offline RL.} 
%
The emergent subfield of offline RL is concerned with learning based purely on a pre-collected dataset 
\citep{levine2020offline}. 
A frequently used mathematical model assumes that historical data are collected (often independently) using some behavior policy, 
and the key challenges (compared with RL with a simulator) come from distribution shift and incomplete data coverage. 
%
The sample complexity of offline RL has been the focus of 
a large strand of recent works, with asymptotically optimal sample complexity achieved by multiple algorithms~\citep{jin2021pessimism,xie2021policy,yin2022near,ren2021nearly,shi2022pessimistic,qu2020finite,yan2022efficacy,rashidinejad2021bridging,li2022settling,li2021sample,wang2022gap}.
Akin to the simulator setting, the fully optimal sample complexity (without burn-in cost) has only been achieved via the model-based approach when it comes to discounted infinite-horizon and inhomogeneous finite-horizon settings~\citep{li2022settling}. 
All asymptotically optimal model-free methods incur substantial burn-in cost. The case with time-homogeneous finite-horizon MDPs also remains unsettled. 


\paragraph{Sample complexity for online RL.} 
Obtaining optimal sample complexity (or regret bound) in online RL without incurring any burn-in cost 
has been one of the most fundamental open problems in RL theory. 
In fact, the past decades have witnessed a flurry of activity towards improving the sample efficiency of online RL,  
partial examples including \citet{kearns1998near,brafman2002r,kakade2003sample,strehl2006pac,strehl2008analysis,kolter2009near,bartlett2009regal,jaksch2010near,szita2010model,lattimore2012pac,osband2013more,dann2015sample,agralwal2017optimistic,dann2017unifying,jin2018q,efroni2019tight,fruit2018efficient,zanette2019tighter,cai2019provably,dong2019q,russo2019worst,pacchiano2020optimism,neu2020unifying,zhang2020almost,zhang2020reinforcement,tarbouriech2021stochastic,xiong2021randomized,menard2021ucb,wang2020long,li2021settling,li2021breaking,domingues2021episodic,zhang2022horizon,li2023minimax,ji2023regret}. 
% 
Unfortunately, no work has been able to conquer this problem completely: 
the state-of-the-art result for model-based algorithms still incurs a burn-in that scales at least quadratically in $S$ 
\citep{zhang2020reinforcement}, while the burn-in cost of the best model-free algorithms (particularly with the aid of variance reduction introduced in \citet{zhang2020almost}) still suffers from highly sub-optimal horizon dependency \citep{li2021breaking}.  










\subsection{Notation} 
%
Before proceeding, let us introduce a set of notation to be used throughout. 
Let $1$ and $0$ indicate respectively the all-one vector and the all-zero vector. Let $e_{s}$ denote the $s$-th standard basis vector (which has $1$ at the $s$-th coordinate and $0$ otherwise).  
For any set $\mathcal{X}$,  $\Delta(\mathcal{X})$ represents the set of probability distributions over the set $\mathcal{X}$. 
For any positive integer $N$, we denote $[N]=\{1,\ldots,N\}$.
For any two vectors $x,y$ with the same dimension, 
we use $xy$ to abbreviate $x^{\top}y$. 
For any integer $S>0$, any probability vector $p\in \Delta([S])$ and another vector $v=[v_i]_{1\leq i\leq S}$, 
we denote by 
%
\begin{equation}
	\mathbb{V}(p,v) \coloneqq \langle p, v^2 \rangle - (\langle p, v\rangle)^2 = \big\langle p,  \big(v-\langle p, v\rangle 1 \big) ^2  \big\rangle 
\end{equation}
%
 the associated variance,  
%Let $\{V^*_h\}_{h=1}^H$ and $\{Q_h^*\}_{h=1}^H$ denote respectively the optimal value function and $Q$-function. 
where $v^2=[v_i^2]_{1\leq i\leq S}$ represents element-wise square of $v$. 
For any two vectors $a=[a_i]_{1\leq i\leq n}$ and $b=[b_i]_{1\leq i\leq n}$, 
the notation $a\geq b$ (resp.~$a\leq b$) means $a_i\geq b_i$ (resp.~$a_i\leq b_i$) holds simultaneously for all $i$.  
Without loss of generality, we assume throughout that  $K$ is a power of $2$ to streamline presentation. 


%Let $[N]$ denote the set $\{1,\ldots,N\}$. 
%
% We shall often use $\{\cdot\}_{(\lambda)}$ as shorthand for $\{\cdot\}_{\lambda \in \Lambda}$, where $\Lambda$ is the set of all proper choices of the index $\lambda$; for example, $\{\cdot \}_{(s,a,h,k)}$ denotes $\{\cdot \}_{(s,a,h,k)\in \mathcal{S}\times \mathcal{A}\times [H]\times [K]}$.  
%
%
%We employ the notation $\iota$ to abbreviate $\iota\coloneqq \log\frac{SAHK}{\delta}$. 
%
