









In this section, 
we formally describe our algorithm: a simple variation of the model-based algorithm called {\em Monotonic Value Propagation} proposed by \citet{zhang2020reinforcement}. 
%
We present the full procedure in Algorithm~\ref{alg:main}, and point out several key ingredients. 
%
\begin{itemize}
	\item {\em Optimistic updates using upper confidence bounds (UCB).} The algorithm implements the optimism principle in the face of uncertainty 
		by adopting the frequently used UCB-based framework (see, e.g., $\mathtt{UCBVI}$ by \citet{azar2017minimax}). 
More specifically, 
the learner calculates the optimistic Bellman equation backward (from $h=H,\ldots,1$): 
		it first computes an empirical estimate $\widehat{P}=\{\widehat{P}_h\in \mathbb{R}^{SA\times S}\}_{1\leq h\leq H}$ of the transition probability kernel  
		as well as an empirical estimate $\widehat{r}= \{\widehat{r}_h \in \mathbb{R}^{SA}\}_{1\leq h\leq H}$ of the mean reward function, 
and then maintains upper estimates for the associated value function and Q-function using  
%
\begin{subequations}
\label{eq:Qh-Vh-UCB-informal}	
\begin{align}
Q_{h}(s,a)\, & \leftarrow\,\min\big\{\widehat{r}_{h}(s,a)+\langle\widehat{P}_{s,a,h},V_{h+1}\rangle+b_{h}(s,a),H\big\}, 
	\label{eq:Qh-UCB-informal}\\
V_{h}(s)\, & \leftarrow\, \max\nolimits_{a}Q_{h}(s,a) 
\end{align}
\end{subequations}
%
for all state-action pairs. 
Here, $Q_{h}$ (resp.~$V_h$) indicates the running estimate for the Q-function (resp.~value function), 
%$\widehat{r}_{h}(s,a) \in \mathbb{R}$ is an estimate of the mean reward at $(s,a,h)$, 
%$\widehat{P}_{s,a,h}\in \mathbb{R}^S$ indicates an estimate of the transition probability vector from $(s,a,h)$, 
whereas $b_{h}(s,a)\geq 0$ is some suitably chosen bonus term that compensates for the uncertainty.  
The above opportunistic Q-estimate in turn allows one to obtain a policy estimate (via a simple greedy rule), 
which will then be exeuted to collect new data. 
The fact that we first estimate the model (i.e., the transition kernel and mean rewards) makes it a model-based approach.
Noteworthily, the empirical model $(\widehat{P},\widehat{r})$ shall be updated multiple times as new samples continue to arrive, 
and hence the updating rule \eqref{eq:Qh-Vh-UCB-informal} will be invoked a couple of times as well.  
 


\end{itemize}





\begin{algorithm}[H]
	\DontPrintSemicolon
\caption{Monotoinic Value Propagation ($\mathtt{MVP}$)~\citep{zhang2020reinforcement}\label{alg:main}}
%\begin{algorithmic}[ht]
	\textbf{input:} state space $\mathcal{S}$, action space $\mathcal{A}$, horizon $H$, total number of episodes $K$, confidence parameter $\delta$, 
	%$\mathcal{L}=\{1,2,\ldots, 2^{\log_2K}\}$, 
	$c_1=\frac{460}{9}$, $c_2 = 2\sqrt{2}$, $c_3=\frac{544}{9}$. \\
	%, and $\iota\coloneqq \log\frac{SAHK}{\delta}$.  \\
%\textbf{Initialize:} . \\
	\textbf{initialization: } set $\delta' \leftarrow \frac{\delta}{200SAH^2K^2}$, and for all $(s,a,s',h)\in \mathcal{S}\times \mathcal{A}\times\mathcal{S}\times [H]$, set $\theta_h(s,a)\leftarrow 0$, $\kappa_h(s,a)\leftarrow 0$, $N^{\mathsf{all}}_h(s,a,s')\leftarrow 0$, $N_h(s,a,s')\leftarrow 0$, $N_h(s,a)\leftarrow 0$, $Q_h(s,a)\leftarrow H$, $V_h(s)\leftarrow H$. \\
%\textbf{initialization: } 
	\For{$k=1,2,\ldots,K$} {
		Set $\pi^k$ such that $\pi_h^k(s) = \arg\max_{a}Q_h(s,a)$ for all $s\in \mathcal{S}$ and $h\in [H]$. {\color{blue}\tcc{policy iterate.}}
		\For {$h=1,2,...,H$} {
			Observe $s_{h}^k$, 
			take action $ a_h^k= \arg\max_{a}Q_h(s_h^k,a)$, 
%	%	\STATE{ Receive reward $r_h^k$ (receive $c_h^k$ and set $r_h^k = -c_h^k$ for the cost case) and  observe $s_{h+1}^k$.}
			receive  $r_h^k$,  observe $s_{h+1}^k$. \label{line:choose_action} 
			{\color{blue}\tcc{sampling.}}
			$(s,a,s')\leftarrow (s_h^k,a_h^k,s_{h+1}^k)$. \\
			Update $N^{\mathsf{all}}_h(s,a) \leftarrow  N^{\mathsf{all}}_h( s,a )+1$, $N_h(s,a,s') \leftarrow   N_h(s,a,s')+1$, $\theta_h(s,a)\leftarrow \theta_h(s,a)+r_h^k$, $\kappa_h(s,a)\leftarrow \kappa_h(s,a)+(r_h^k)^2$. \\
		{\color{blue}\tcc{perform updates using data of this epoch.\label{line:a1}}}
		\If{$N^{\mathsf{all}}_h(s,a)\in \{1,2,\ldots, 2^{\log_2K}\}$ \label{line:rp_update_start} }   {\label{line:trigger-set}
			 $N_h(s,a)\leftarrow \sum_{\widetilde{s}}N_h(s,a,\widetilde{s})$.  
			{\color{blue}\tcp{number of visits to $(s,a,h)$ in this epoch.} \label{line:Nh-update}}
			$\widehat{r}_h(s,a)\leftarrow \frac{\theta_h(s,a)}{N_h(s,a)}$. \label{line:r-hsa-update}
			{\color{blue}\tcp{empirical rewards of this epoch.}} 
			$\widehat{\sigma}_h(s,a)\leftarrow \frac{\kappa_h(s,a)}{N_h(s,a) }  $. 
			{\color{blue}\tcp{empirical squared rewards of this epoch.}}
			$\widehat{P}_{s,a,h}(\widetilde{s}) \leftarrow  \frac{N_h(s,a,\widetilde{s})}{N_h(s,a)}$ for all $\widetilde{s} \in \mathcal{S}$.  \label{line:P-hsa-update}
			{\color{blue}\tcp{empirical transition for this epoch.}}
			%
			Set TRIGGERED = TRUE, 
			and $\theta_h(s,a)\leftarrow 0$, $\kappa_h(s,a)\leftarrow 0$,  $N_h(s,a,\widetilde{s})\leftarrow 0$  for all $\widetilde{s}\in \mathcal{S}$. 
%		\ENDIF \label{line:rp_update_end}
		}
		}
		{\color{blue}\tcc{optimistic Q-estimation using empirical model of this epoch.}}
		\If {\textnormal{TRIGGERED= TRUE}} {
			Set TRIGGERED = FALSE, and $V_{H+1}(s)\leftarrow 0$ for all $s\in \mathcal{S}$. \\
			\For{$h=H,H-1,...,1$} {
				%
				\For{$(s,a)\in \mathcal{S}\times \mathcal{A}$} {

					%\vspace{-3ex}
					\begin{align} 
						\vspace{-3ex}
						b_h(s,a) &\leftarrow c_1 \sqrt{\frac{   \mathbb{ V}(\widehat{P}_{s,a,h} ,V_{h+1}) \log \frac{1}{\delta'}  }{ \max\{N_h(s,a),1 \} }}+c_2 \sqrt{\frac{\big(\widehat{\sigma}_h(s,a)- (\widehat{r}_h(s,a))^2 \big)\log \frac{1}{\delta'}}{\max\{N_h(s,a),1\}}} \qquad\qquad\qquad\qquad\qquad\qquad\nonumber\\
						&\qquad\qquad\qquad +c_3\frac{H\log \frac{1}{\delta'}}{ \max\{N_h(s,a) ,1\}  },  \label{eq:update1}  \\
						Q_h(s,a) &\leftarrow \min\big\{    \widehat{r}_h(s,a)+\langle \widehat{P}_{s,a,h}, V_{h+1} \rangle +b_h(s,a)    ,H\big\},\,
						V_{h}(s) \leftarrow \max_{a}Q_h(s,a).
						\label{eq:updateq}
					\end{align}
					\vspace{-3ex}
				}
			}
			%
		}
	}
%\end{algorithmic}
\end{algorithm}





\begin{itemize}

	\item {\em An epoch-based procedure and a doubling trick.} 
%
		Compared to the original $\mathtt{UCBVI}$ \citep{azar2017minimax}, 
		one distinguishing feature of $\mathtt{MVP}$ is to update the empirical transition kernel and empirical rewards in an epoch-based fashion, 
		as motivated by a doubling update framework adopted in \citet{jaksch2010near}. 
		More concretely, the whole learning process is divided into consecutive epochs via a simple doubling rule; 
		namely, whenever there exits a $(s,a,h)$-tuple whose visitation count reaches a power of 2, we end the current epoch,  reconstruct the empirical model (cf.~lines~\ref{line:r-hsa-update} and \ref{line:P-hsa-update} of Algorithm~\ref{alg:main}), compute the Q-function and value function using the newly updated transition kernel and rewards (cf.~\eqref{eq:updateq}), and then start a new epoch with an updated sampling policy. 
		This stands in stark contrast with the original $\mathtt{UCBVI}$, which computes new estimates for the transition model, Q-function and value function in every episode. With this doubling rule in place, the estimated transition probability vector for each $(s,a,h)$-tuple will be updated by no more than $\log_2K$ times, 
		a feature that plays a pivotal role in significantly reducing some sort of covering number needed in our covering-based analysis (as we shall elaborate on shortly in Section~\ref{sec:tec}). In each epoch, the learned policy is induced by the optimistic Q-function estimate --- computed based on the empirical transition kernel of the {\em current} epoch --- which will then be employed to collect samples in {\em all} episodes of the next epoch. 
		More technical explanations of the doubling update rule will be provided in Section~\ref{sec:tec1}. 



		
	\item {\em Monotonic bonus functions.} Another crucial step in order to ensure near-optimal regret lies in careful designs of the data-driven bonus terms $\{b_h(s,a)\}$ in \eqref{eq:Qh-UCB-informal}. 
		Here, we adopt the monotonic Bernstein-style bonus function for $\mathtt{MVP}$ originally proposed in \citet{zhang2020reinforcement}, 
		to be made precise in \eqref{eq:update1}. 
		Compared to the bonus function in $\mathtt{Euler}$~\citep{zanette2019tighter} and $\mathtt{UCBVI}$~\citep{azar2017minimax},  the monotonic bonus form has a cleaner structure that effectively avoids large lower-order terms. Note that in order to enable variance-aware regret, we also need to keep track of the empirical variance of the (stochastic) immediate rewards. 

\end{itemize}








\begin{remark}
We note that a doubling update rule has also been used in the original $\mathtt{MVP}$ \citep{zhang2020reinforcement}. 
A subtle difference between our modified version and the original one lies in that: when the visitation count for some $(s,a,h)$ reaches $2^i$ for some integer $i\geq 1$, we only use the second half of the samples (i.e., the $\{2^{i-1}+l\}_{l=1}^{2^{i-1}}$-th samples) to compute the empirical model, whereas the original $\mathtt{MVP}$ makes use of all the $2^i$ samples. This modified step turns out to be helpful in our analysis, 
	while still preserving sample efficiency in an orderwise sense (since the latest batch always contains at least half of the samples). 
\end{remark}


















\begin{comment}

\begin{algorithm}
	\DontPrintSemicolon
\caption{Monotoinic Value Propagation ($\mathtt{MVP}$)~\citep{zhang2020reinforcement}\label{alg:main}}
\begin{algorithmic}[ht]
\STATE{\textbf{Input:} number of states $S$, number of actions $A$, horizon $H$, total number of episodes $K$, confidence parameter $\delta$;}
\STATE{\textbf{Initialize:} $\mathcal{L}=\{1,2,\ldots, 2^{\log_2(K)}\}$; $c_1=\frac{460}{9}, c_2 = 2\sqrt{2},  c_3=\frac{544}{9}$;}
\STATE{\textbf{Initialization: } $\theta_h(s,a)\leftarrow 0, \kappa_h(s,a)\leftarrow 0, N^{\mathsf{all}}_h(s,a,s')\leftarrow 0, N_h(s,a,s')\leftarrow 0, n_h(s,a)\leftarrow 0, Q_h(s,a)\leftarrow H-h+1, V_h(s)\leftarrow H-h+1, \forall (s,a,s',h)\in \mathcal{S}\times \mathcal{A}\times\mathcal{S}\times [H]$;}
\FOR {$k=1,2,...$}
		\FOR {$h=1,2,...,H$}
		\STATE{ Observe $s_{h}^k$;}
		\STATE{ Take action $ a_h^k= \arg\max_{a}Q_h(s_h^k,a)$;} \label{line:choose_action}
	%	\STATE{ Receive reward $r_h^k$ (receive $c_h^k$ and set $r_h^k = -c_h^k$ for the cost case) and  observe $s_{h+1}^k$.}
 \STATE{ Receive reward $r_h^k$  and  observe $s_{h+1}^k$.}
		\STATE{ Set $(s,a,s')\leftarrow (s_h^k,a_h^k,s_{h+1}^k)$;.}
		\STATE{ Set $N^{\mathsf{all}}_h(s,a) \leftarrow  N^{\mathsf{all}}_h( s,a )+1$, \ \,$N_h(s,a,s') \leftarrow   N_h(s,a,s')+1$, $\theta_h(s,a)\leftarrow \theta_h(s,a)+r_h^k$, $\kappa_h(s,a)\leftarrow \kappa_h(s,a)+(r_h^k)^2$.}
		\STATE{ \verb|\\| \emph{Update empirical rewards and transition probability}}
		\IF {$N^{\mathsf{all}}_h(s,a)\in \mathcal{L}$}  \label{line:rp_update_start}
  \STATE{Set $\widehat{r}_h(s,a)\leftarrow \frac{\theta_h(s,a)}{\sum_{\widetilde{s}'}N_h(s,a,\widetilde{s}')}$;}
  \STATE{Set $\widehat{\sigma}_h(s,a)\leftarrow \frac{\kappa_h(s,a)}{\sum_{\widetilde{s}'}N_h(s,a,\widetilde{s'}) } - (\widehat{r}_h(s,a))^2 $}
		\STATE Set $\widehat{P}_{s,a,h,\widetilde{s}} \leftarrow  \frac{N_h(s,a,\widetilde{s})}{\sum_{\widetilde{s}'}N_h(s,a,\widetilde{s}')}$ for all $\widetilde{s} \in \mathcal{S}$.
		%			\ENDFOR
  \STATE{}
		\STATE{ Set $n_h(s,a)\leftarrow \sum_{\widetilde{s}'}N_h(s,a,\widetilde{s}')$;}
		\STATE{ Set TRIGGERED = TRUE.}
  \STATE{$\theta_h(s,a)\leftarrow 0, \kappa_h(s,a)\leftarrow 0,  N_h(s,a,\widetilde{s})\leftarrow 0, \forall \widetilde{s}\in \mathcal{S}$;}
		\ENDIF \label{line:rp_update_end}
		\ENDFOR
		\STATE{ \verb|\\| \emph{Update $Q$-function}}
		\IF {TRIGGERED}
  \STATE{$V_{H+1}(s)\leftarrow 0,\forall s\in \mathcal{S}$;}
		\FOR{$h=H,H-1,...,1$}
		\FOR{$(s,a)\in \mathcal{S}\times \mathcal{A}$}
		%		  \vspace{-3ex}
		\STATE 	 {		%\vspace{-0.5cm} 	
			%					\vspace{-0.5cm}
			%			\simon{Maybe move to the line $N(s,a) \in \mathcal{L}$ so it's clear for the reader that we update the policy only in the set.}
			Set
			\begin{align} 
			~~~~~~~~~~~~~&b_h(s,a)\leftarrow c_1 \sqrt{\frac{   \mathbb{ V}(\widehat{P}_{s,a,h} ,V_{h+1}) \log(\frac{1}{\delta'})  }{ \max\{n_h(s,a),1 \} }}+c_2 \sqrt{\frac{(\widehat{\sigma}_h(s,a)- (\widehat{r}_h(s,a))^2 )\log(\frac{1}{\delta'})}{\max\{n_h(s,a),1\}}}+c_3\frac{H\log(\frac{1}{\delta'})}{ \max\{n_h(s,a) ,1\}  },  \label{eq:update1}  \\
			%\\ 	\hspace{-20ex} 	&  \left\{\begin{array}{l}   Q_h(s,a)\leftarrow \min\{    \widehat{r}_h(s,a)+\widehat{P}_{s,a,h} V_{h+1} +b_h(s,a)    ,H\} \quad  \text{for reward case} \\ Q_h(s,a)\leftarrow \max\{\min \left\{ \widehat{r}_h(s,a) + \widehat{P}_{s,a,h}V_{h+1}+b_h(s,a), 0 \right\} ,-H\} \quad \text{for cost case}\label{eq:updateq}\end{array}\right.
   \hspace{-20ex} 	&    Q_h(s,a)\leftarrow \min\{    \widehat{r}_h(s,a)+\widehat{P}_{s,a,h} V_{h+1} +b_h(s,a)    ,H\}\label{eq:updateq}
			\\ & V_{h}(s) \leftarrow \max_{a}Q_h(s,a).\nonumber
			\end{align}
			\vspace{-3ex}
		}
		\ENDFOR
		\ENDFOR
		\STATE{ Set TRIGGERED = FALSE}
		\ENDIF
		\ENDFOR
\end{algorithmic}
\end{algorithm}

\end{comment}


