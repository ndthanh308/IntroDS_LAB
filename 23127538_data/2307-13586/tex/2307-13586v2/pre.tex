

In this section, we introduce the basics of tabular online RL, as well as some basic assumptions to be imposed throughout. 
% 
 

\paragraph{Basics of finite-horizon MDPs.}
%
This paper concentrates on time-inhomogeneous (or nonstatioanry) finite-horizon MDPs. 
Throughout the paper, 
we employ $\mathcal{S}=\{1,\ldots,S\}$ to denote the state space, 
$\mathcal{A}=\{1,\ldots,A\}$ the action space, and $H$ the planning horizon. 
The notation $P=\big\{P_h: \mathcal{S}\times \mathcal{A} \rightarrow \Delta(\mathcal{S}) \big\}_{1\leq h\leq H}$ denotes the probability transition kernel of the MDP; 
for any current state $s$ at any step $h$, if action $a$ is taken, then the state at the next step $h+1$ of the environment is randomly drawn from $P_{s,a,h}\coloneqq P_h(\cdot \mymid s,a) \in \Delta(\mathcal{S})$.   
Also, the notation $R=\big\{R_{s,a,h}\in \Delta([0,H]) \big\}_{1\leq h\leq H, s\in \mathcal{S}, a\in \mathcal{A}}$ 
indicates the reward distribution; that is,  
while executing action $a$ in state $s$ at step $h$, 
the agent receives an immediate reward --- which is non-negative and possibly stochastic --- drawn from the distribution $R_{s,a,h}$. 
We shall also denote by $r=\big\{r_h(s,a) \big\}_{1\leq h\leq H, s\in \mathcal{S}, a\in \mathcal{A}}$ the mean reward function, 
so that $r_{h}(s,a)\coloneqq \mathbb{E}_{r'\sim R_{s,a,h}}[r'] \in [0,H]$ for any $(s,a,h)$-tuple.     
Additionally, a deterministic policy $\pi=\{\pi_h: \mathcal{S}\rightarrow \mathcal{A}  \}_{1\leq h\leq H}$ 
stands for an action selection rule, so that the action selected in state $s$ at step $h$ is given by $\pi_h(s)$. 
The readers can consult standard textbooks (e.g., \citet{bertsekas2019reinforcement}) for more extensive descriptions.  





In each episode, 
a trajectory $(s_1,a_1,r_1', s_2,\ldots,s_H, a_H, r_H')$ is rolled out as follows: 
the learner starts from an initial state $s_1$ independently drawn 
from some fixed (but unknown) distribution $\mu\in \Delta(\mathcal{S})$;  
for each step $1\leq h\leq H$, 
the learner takes action $a_h$, gains an immediate reward $r_h'  \sim R_{s_h,a_h,h}$, 
and the environment transits to the state $s_{h+1}$ at step $h+1$ according to $P_{s_h,a_h,h}$.  
All of our results in this paper operate under the following assumption on the total reward. 
%
\begin{assumption}\label{assum1}
	For any possible trajectory $(s_1,a_1,r_1', \ldots,s_H, a_H, r_H')$, one always has $0\leq \sum_{h=1}^H r_h'\leq H$.
\end{assumption}
%
\noindent 
As can be easily seen, Assumption~\ref{assum1} is less stringent than another common choice that assumes 
$r_h'\in [0,1]$ for any $h$ in any episode. 
In particular, Assumption~\ref{assum1} allows for sparse and spiky rewards along an episode; 
more discussions can be found in \citep{jiang2018open,wang2020long}.







\paragraph{Value function and Q-function.} 
%
For any given policy $\pi$, one can define the value function $V^{\pi}=\{V_h^{\pi}: \mathcal{S}\rightarrow \mathbb{R}\}$ 
and the $Q$-function $Q^{\pi}=\{Q_h^{\pi}: \mathcal{S}\times \mathcal{A}\rightarrow \mathbb{R}\}$ such that
%
\begin{subequations}
\begin{align}
	V_h^{\pi}(s) &\coloneqq \mathbb{E}_{\pi}\left[\sum_{j=h}^H r_{j}' \,\Big|\, s_h=s\right], \qquad &&\forall (s,h)\in \mathcal{S}\times [H], \\
	Q_h^{\pi}(s,a) &\coloneqq \mathbb{E}_{\pi}\left[\sum_{j=h}^H r_{j}' \,\Big|\, (s_h,a_h)=(s,a)\right],
	\qquad &&\forall (s,a,h)\in \mathcal{S}\times \mathcal{A}\times [H],
\end{align}
\end{subequations}
%
where the expectation $\mathbb{E}_{\pi}[\cdot]$ is taken over the randomness of an episode $\big\{(s_h,a_h,r_h')\big\}_{1\leq h\leq H}$ 
generated under policy $\pi$, 
that is, $a_{j} = \pi_{j}(s_{j})$ for every $h\leq  j \leq H$ (resp.~$h<  j \leq H$) is chosen 
in the definition of $V_h^{\pi}$ (resp.~$Q_h^{\pi}$). 
Accordingly, we define the optimal value function and the optimal $Q$-function respectively as: 
%
\begin{subequations}
\begin{align}
	V_h^{\star}(s) &\coloneqq \max_{\pi}V^{\pi}_h(s) , \qquad &&\forall (s,h)\in \mathcal{S}\times [H],\\
	Q_h^{\star}(s,a) &\coloneqq \max_{\pi}Q^{\pi}_h(s,a) \qquad &&\forall (s,a,h)\in \mathcal{S}\times \mathcal{A}\times [H]. 
\end{align}
\end{subequations}
%
Throughout this paper, we shall often abuse the notation by letting both $V_h^{\pi}$ and $V_h^{\star}$ (resp.~$Q_h^{\pi}$ and $Q_h^{\star}$) represent $S$-dimensional (resp.~$SA$-dimensional) vectors containing all elements of the corresponding value functions (resp.~Q-functions).  
Two important properties are worth mentioning: (a) the optimal value and the optimal Q-function are linked by the Bellman equation:
%
\begin{align}
Q_h^{\star}(s,a) = r_h(s,a)+\big\langle P_{h,s,a}, V_{h+1}^{\star} \big\rangle,\qquad V_h^{\star}(s) = \max_{a'}Q_h^{\star}(s,a'), 
\qquad \forall (s,a,h)\in \mathcal{S}\times \mathcal{A}\times[H]; 
\end{align}
%
(b) there exists a deterministic policy, denoted by $\pi^{\star}$, that achieves optimal value functions and Q-functions for all state-action-step tuples simultaneously, that is, 
%
\[
	V_h^{\pi^{\star}}(s)=V_h^{\star}(s) \qquad \text{and} \qquad 
	Q_h^{\pi^{\star}}(s,a)=Q_h^{\star}(s,a),\qquad \forall (s,a,h)\in \mathcal{S}\times \mathcal{A}\times[H]. 
\]


\paragraph{Data collection protocol and performance metrics.} 
%
During the learning process, 
the learner is allowed to collect $K$ episodes of samples (using arbitrary policies it selects). 
More precisely, in the $k$-th episode, 
the learner is given an independently generated initial state $s_1^k \sim \mu$, 
and executes policy $\pi^k$ (chosen based on data collected in previous episodes) to obtain a sample trajectory 
$\big\{ (s_h^k,a_h^k,r_h^k) \big\}_{1\leq h\leq H}$, 
with $s_h^k$, $a_h^k$ and $r_h^k$ denoting the state, action and immediate reward at step $h$ of this episode. 



To evaluate the learning performance, 
a widely used metric is the (cumulative) regret over all $K$ episodes:   
%
\begin{align}
	\mathsf{Regret}(K) \coloneqq \sum_{k=1}^K \left( V^{\star}_{1}(s_1^k) -V^{\pi^k}_1(s_1^k) \right),
	\label{eq:defn-regret}
\end{align}
%
and our goal is to design an online RL algorithm that minimizes $\mathsf{Regret}(K)$ regardless of the allowable sample size $K$. 
It is also well-known (see, e.g., \citet{jin2018q}) that 
a regret bound can often be readily translated into a PAC sample complexity result, 
the latter of which counts the number of episodes needed to find an $\varepsilon$-optimal policy $\widehat{\pi}$ in the sense that $\mathbb{E}_{s_1 \sim \mu}\big[V_1^{\star}(s_1) - V^{\widehat{\pi}}_1(s_1)\big] \le \varepsilon$.  
For instance, the reduction argument in \citet{jin2018q} reveals that: if an algorithm achieves  $\mathsf{Regret}(K)\leq f(S,A,H) K^{1-\alpha}$ for some function $f$ and some parameter $\alpha \in (0,1)$, then by randomly selecting a policy from $\{\pi^k\}_{1\leq k\leq K}$ as $\widehat{\pi}$ one achieves  $\mathbb{E}_{s_1 \sim \mu}\big[V_1^{\star}(s_1) - V_1^{\widehat{\pi}}(s_1)\big] \lesssim f(S,A,H) K^{-\alpha}$, 
thus resulting in a sample complexity bound of $\big(\frac{f(S,A,H)}{\varepsilon}\big)^{1/\alpha}$.









