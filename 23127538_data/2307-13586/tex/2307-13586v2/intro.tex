



In reinforcement learning (RL), 
an agent is often asked to learn optimal decisions (i.e., the ones that maximize cumulative reward) through real-time ``trial-and-error'' interactions with an unknown environment. 
This task is commonly dubbed as {\em online RL}, underscoring the critical role of adaptive online data collection and differentiating it from other RL settings that rely upon pre-collected data. A central challenge in achieving sample-efficient online RL boils down to how to optimally balance exploration and exploitation during data collection, 
namely, how to trade off the potential revenue of exploring unknown terrain/dynamics against the benefit of exploiting past experience. While decades-long effort has been invested towards unlocking the capability of online RL, 
how to {\em fully} characterize --- and more importantly, attain --- its fundamental performance limit remains largely unsettled. 







In this paper, we take an important step towards settling the sample complexity limit of online RL, 
focusing on tabular Markov Decision Processes (MDPs) with finite horizon and finite state-action space. 
% 
More concretely, imagine that one seeks to learn a near-optimal policy of a time-inhomogeneous MDP with $S$ states, $A$ actions, and horizon length $H$, and is allowed to execute the MDP of interest $K$ times to collect $K$ sample episodes each of length $H$. 
This canonical problem is among the most extensively studied in the RL literature, 
with formal theoretical pursuit dating back to more than 25 years ago (e.g., \citet{kearns1998near}). 
Numerous works have since been devoted to improving the sample efficiency and/or refining the analysis framework~\citep{brafman2002r,kakade2003sample,jaksch2010near,azar2017minimax, jin2018q,dann2017unifying,zanette2019tighter,bai2019provably,zhang2020almost,zhang2020reinforcement,menard2021ucb,li2021breaking,domingues2021episodic}. 
As we shall elucidate momentarily, however, information-theoretic optimality has only been achieved in the ``large-sample'' regime. 
When it comes to the most challenging sample-hungry regime, 
there remains a substantial gap between the state-of-the-art regret upper bound and the best-known minimax lower bound, 
which motivates the research of this paper.   






\subsection{Inadequacy of prior art: enormous burn-in cost} 

While past research has obtained asymptotically optimal (i.e., optimal when $K$ approaches infinity) regret bounds in the aforementioned setting, 
all of these results incur an enormous burn-in cost --- that is, the minimum sample size needed for an algorithm to operate sample-optimally --- which we explain in the sequel.  
For simplicity of presentation, we assume that each immediate reward lies within the normalized range $[0,1]$ when discussion the prior art.  


\paragraph{Minimax lower bound.}
%
To provide a theoretical benchmark,
we first make note of the best-known minimax regret lower bound developed by \citet{jin2018q,domingues2021episodic}:\footnote{Let $\mathcal{X}=\{S,A,H,K,\frac{1}{\delta}\}$, where $1-\delta$ is the target success rate (to be seen shortly). The standard notation $f(\mathcal{X})=O\big(g(\mathcal{X})\big)$ (or $f(\mathcal{X})\lesssim g(\mathcal{X})$) indicates the existence of some universal constant $c_1>0$ such that $f(\mathcal{X})\leq c_1 g(\mathcal{X})$; 
$f(\mathcal{X})=\Omega\big(g(\mathcal{X})\big)$ (or $f(\mathcal{X})\gtrsim g(\mathcal{X})$) means that there exists some universal constant $c_2>0$ such that $f(\mathcal{X})\geq c_2 g(\mathcal{X})$; and $f(\mathcal{X})=\Theta\big(g(\mathcal{X})\big)$ (or $f(\mathcal{X})\asymp g(\mathcal{X})$) means that $f(\mathcal{X})\lesssim g(\mathcal{X})$ and $f(\mathcal{X})\gtrsim g(\mathcal{X})$ hold simultaneously. 
%
Moreover, $\widetilde{O}\left(\cdot \right)$, $\widetilde{\Omega}\left(\cdot\right)$ and $\widetilde{\Theta}\left(\cdot\right)$ are defined analogously, except that all logarithmic dependency on the quantities of $\mathcal{X}$ are hidden. 
} 
%
\begin{align}
	\text{(minimax lower bound)} \qquad 
	\Omega\left(\min\big\{\sqrt{SAH^3K} ,\, HK \big\}\right), 
	\label{eq:minimax-lower-bound-1}
\end{align}
%
assuming that the immediate reward at each step falls within $[0,1]$ and imposing no restriction on $K$.  
%
Given that a regret of $O(HK)$ can be trivially achieved (as the sum of rewards in any $K$ episodes cannot exceed $HK$), 
we shall sometimes drop the $HK$ term and simply write
%
\begin{align}
	\text{(minimax lower bound)} \qquad 
	\Omega\big(\sqrt{SAH^3K} \big) \qquad \text{if }K \geq SAH. 
	\label{eq:minimax-lower-bound-2}
\end{align}
%
 



\paragraph{Prior upper bounds and burn-in cost.}
%
We now turn to the upper bounds developed in prior literature. For ease of presentation, we shall assume 
%
\begin{align}
	K \geq SAH  
\end{align}
%
in the rest of this subsection unless otherwise noted. Log factors are also ignored in the discussion below. 


The first paper that achieves asymptotically optimal regret is \citet{azar2017minimax}, 
which came up with a model-based algorithm called $\mathtt{UCBVI}$ that enjoys a regret bound $\widetilde{O}\big(\sqrt{SAH^3K} + H^3S^2A\big)$. 
A close inspection reveals that this regret matches the minimax lower bound \eqref{eq:minimax-lower-bound-2} if and only if 
%
\begin{align}
	\big(\text{burn-in cost of \citet{azar2017minimax}}\big) \qquad K \gtrsim S^3A H^3, 
\end{align}
%
due to the presence of the lower-order term $H^3S^2A$ in the regret bound. 
This burn-in cost is clearly undesirable, 
since the sample size available in many practical scenarios might be far below this requirement. 


In light of its fundamental importance in contemporary RL applications (which often have unprecedented dimensionality and relatively limited data collection capability), reducing the burn-in cost without compromising sample efficiency has emerged as a central problem in recent pursuit of RL theory~\citep{zanette2019tighter,dann2019policy,zhang2020reinforcement,zhou2023sharp,menard2021ucb,li2021breaking,li2021settling,li2022minimax,agarwal2020model,sidford2018variance}. The state-of-the-art regret upper bounds for finite-horizon inhomogeneous MDPs can be summarized below (depending on the size of $K$): 
%
\begin{subequations}
\begin{align}
	\text{\citep{menard2021ucb}}  \qquad & \widetilde{O}\big(\sqrt{SAH^3K}+SAH^4 \big),   \\
	\text{\citep{zhang2020reinforcement,zhou2023sharp}} \qquad & \widetilde{O}\big(\sqrt{SAH^3K} +S^2AH^2 \big), 
\end{align}
\end{subequations}
%
meaning that even the most advanced prior results fall short of sample optimality unless 
%
%
\begin{align}
	\big(\text{best burn-in cost in past works}\big) \qquad K \gtrsim \min \big\{ SAH^5, S^3AH \big\}.  
\end{align}
%
The interested reader is referred to Table~\ref{tab:comparisons} for more details about existing regret upper bounds and their associated sample complexities. 



In summary, no prior theory was able to achieve optimal sample complexity in the data-hungry regime 
$$ SAH\leq  K \lesssim \min \big\{ SAH^5, S^3AH \big\},$$ suffering from a significant barrier of either long horizon (as in the term $SAH^5$) or large state space (as in the term $S^3AH$). 
% 
In fact, the information-theoretic limit is yet to be determined within this regime (i.e., neither the achievability results nor the lower bounds had been shown to be tight), although it has been conjectured by \citet{menard2021ucb} that 
the lower bound \eqref{eq:minimax-lower-bound-1} reflects the correct scaling for any sample size $K$.\footnote{Note that the original conjecture in \citet{menard2021ucb} was $\widetilde{\Theta}\big(\sqrt{SAH^3K}+SAH^2\big)$. 
Combining it with the trivial upper bound $HK$ allows one to remove the term $SAH^2$ (with a little algebra).}




\newcommand{\topsepremove}{\aboverulesep = 0mm \belowrulesep = 0mm} \topsepremove

\begin{table}[t]
		\centering
  \resizebox{\textwidth}{!}{
			\begin{tabular}{ c|c|c|c}
				\toprule
				\textbf{Algorithm} & \textbf{Regret upper bound} & \makecell{\textbf{Range of $K$ that} $\vphantom{\frac{1^{7^{7}}}{1^{7}}}$ \\\textbf{attains optimal regret}}   & \makecell{\textbf{Sample complexity} \\ \textbf{(or PAC bound)}} \\ 
				\toprule
%\hline 
\rowcolor{gray!25} 
	$\mathtt{MVP}$ &  &  & \\
%
\rowcolor{gray!25}
	{\bf (this work, Theorem~\ref{thm1})} & \multirow{-2}{*}{\cellcolor{gray!25}$\min\big\{\sqrt{SAH^3K},HK\big\}$} & 
				\multirow{-2}{*}{\cellcolor{gray!25}$[1,\infty)$ }
				& \multirow{-2}{*}{\cellcolor{gray!25}$\frac{SAH^3}{\varepsilon^2}$ }\\
%
				\hline
			%	\hline{|=|=|=|}
				\makecell{$\mathtt{UCBVI}$\\\citep{azar2017minimax}} & $\min\big\{\sqrt{SAH^3K}+S^2AH^3,\,HK\big\}$  & $[S^3AH^3,\infty)$  & $\frac{SAH^3}{\varepsilon^2} + \frac{S^2AH^3}{\varepsilon}$ \\
				\hline 


				\makecell{
$\mathtt{ORLC}$\\\citep{dann2019policy} }& $\min\big\{\sqrt{SAH^3K}+S^2AH^4,\,HK\big\}$ & $[S^3AH^5,\infty)$ & $\frac{SAH^3}{\varepsilon^2}+\frac{S^2AH^4}{\varepsilon}$\\
  \hline
			
			\makecell{$\mathtt{EULER}$\\\citep{zanette2019tighter}} &    $\min\big\{\sqrt{SAH^3K}+ S^{3/2}AH^3(\sqrt{S}+\sqrt{H}),\,HK\big\}$ & $\big[S^{2}AH^3(\sqrt{S}+\sqrt{H}),\infty\big)$ & $\frac{SAH^3}{\varepsilon^2} + \frac{S^2AH^3(\sqrt{S}+\sqrt{H})}{\varepsilon}$\\
				% \hline 
   % $\mathtt{UCB-Q-Bernstein}$ \cite{jin2018q}  &  $\sqrt{SAH^4K}+H^{9/2}S^{3/2}A^{3/2}$ & Never\\
    \hline			%	\hline{|=|=|=|}
				\makecell{ $\mathtt{UCB}\text{-}\mathtt{Adv}$\\\citep{zhang2020almost}} & $\min\big\{\sqrt{SAH^3K}+S^2A^{3/2}H^{33/4}K^{1/4},\,HK\big\}$ & $[ S^6A^4H^{27},\infty)$ & $\frac{SAH^3}{\varepsilon^2} + \frac{S^{8/3}A^2H^{11}}{\varepsilon^{4/3}}$ 
  \\
  \hline
  \makecell{$\mathtt{MVP}$\\\citep{zhang2020reinforcement}} & $\min\big\{\sqrt{SAH^3K}+S^2AH^2,\,HK\big\}$  &  $[S^3AH,\infty)$ &  $\frac{SAH^3}{\varepsilon^2} + \frac{S^2AH^2}{\varepsilon}$\\
  \hline  
				\makecell{$\mathtt{UCBMQ}$\\\citep{menard2021ucb}} & $\min\big\{\sqrt{SAH^3K}+SAH^4,\,HK\big\}$ & $[SAH^5,\infty)$ & $\frac{SAH^3}{\varepsilon^2} + \frac{SAH^4}{\varepsilon}$\\
  \hline 
  \makecell{$\mathtt{Q}$-$\mathtt{Earlysettled}$-$\mathtt{Adv}$
  \\\citep{li2021breaking}}  & $\min\big\{\sqrt{SAH^3K}+SAH^6,\,HK\big\}$ & $[SAH^{9},\infty)$ &  $\frac{SAH^3}{\varepsilon^2} + \frac{SAH^{6}}{\varepsilon}$\\
  \toprule
	\makecell{Lower bound\\\citep{domingues2021episodic}} & $\min\big\{\sqrt{SAH^3K},HK\big\}$ & n/a & $\frac{SAH^3}{\varepsilon^2}$  \\
				\toprule
			\end{tabular}}
		\caption{
 %\simon{double check ORLC, UCBVI}
			Comparisons between our result and prior works that achieve asymptotically optimal regret for finite-horizon inhomogeneous MDPs (with all log factors omitted),   where $S$ (resp.~$A$) is the number of states (resp.~actions), $H$ is the planning horizon, and $K$ is the number of episodes. The third column reflects the burn-in cost, and the sample complexity (or PAC bound) refers to the number of episodes needed to yield $\varepsilon$ accuracy. 
The results provided here account for all $K\geq 1$ or all $\varepsilon \in (0,H]$. 
Our paper is the only one that gives regret (resp.~PAC) bound matching the minimax lower bound for the entire range of $K$ (resp.~$\varepsilon$).  
  % We note that every regret bound can be combined with the trivial bound $O(KH)$.
 % \simon{I remember UCBVI also requires $S\ge H$?}
% \simon{add the old MVP here as well.} \simon{I think we don't need to write $KH$ term since lower bound also requires $K = \Omega\left(SAH\right)$}.
 \label{tab:comparisons}
}
	\end{table}

% \subsection{Main Results}


	

\paragraph{Comparisons with other RL settings and key challenges.} 
%
In truth, the incentives to minimize the burn-in cost and improve data efficiency arise in multiple other settings beyond online RL.  
For instance, in an idealistic setting that assumes access to a simulator (or a generative model) --- a model that allows the learner to query arbitrary state-action pairs to draw samples --- a recent work \citet{li2020breaking} developed a perturbed model-based approach that is provably optimal without incurring any burn-in cost. Analogous results have been obtained in \citet{li2021settling} for offline RL --- a setting that requires policy learning to be performed based on historical data --- unveiling the full-range optimality of a pessimistic model-based algorithm. 


Unfortunately, the algorithmic and analysis frameworks developed in the above two works fail to accommodate the online counterpart. 
%
The main hurdle stems from the complicated statistical dependency intrinsic to episodic online RL; 
for instance, in online RL, the empirical transition probabilities and the running estimates of the value function are oftentimes statistically dependent in an intertwined manner (unless we waste data).  
How to decouple the intricate statistical dependency without compromising data efficiency constitutes the key innovation of this work. 
More precise, in-depth technical discussions will be provided in Section~\ref{sec:tec}.


%, while in contrast, there is no such dependency in the simulator setting (as it typically assumes samples are taken completely independently).































\subsection{A peek at our main contributions} 

We are now positioned to summarize the main findings of this paper. 
Focusing on time-inhomogeneous finite-horizon MDPs, 
our main contributions can be divided into two parts:  
 the first part fully settles the minimax-optimal regret and sample complexity of online RL, 
 whereas the second part further extends and augments our theory to make apparent the impacts of certain problem-dependent quantities. 
 %
 Throughout this subsection, the regret metric $\mathsf{Regret}(K)$ captures the cumulative sub-optimality gap (i.e., the gap between the performance of the policy iterates and that of the optimal policy) over all $K$ episodes, 
 to be formally defined in \eqref{eq:defn-regret}. 





\subsubsection{Settling the optimal sample complexity with no burn-in cost} 

Our first result {\em fully} determines the sample complexity limit of online RL in a minimax sense, 
allowing one to attain the optimal regret regardless of the number $K$ of episodes that can be collected. 
%
\begin{theorem}\label{thm1}  
For any $K \ge 1$ and any $0<\delta<1$, 
there exists an algorithm (see Algorithm~\ref{alg:main}) obeying
%
\begin{equation}
	\mathsf{Regret}(K) 
	\lesssim \min\bigg\{\sqrt{SAH^{3}K \log^5\frac{SAHK}{\delta}},HK\bigg\}
	%\leq \widetilde{O}\left(\min\big\{\sqrt{SAKH^3},\,HK\big\}\right)
	\label{eq:regret-Thm1}
\end{equation}
%
with probability at least $1-\delta$.
	%where the regret metric will be formally defined in \eqref{eq:defn-regret}.  
\end{theorem}
%
The optimality of our regret bound \eqref{eq:regret-Thm1} can be readily seen given that 
it matches the minimax lower bound \eqref{eq:minimax-lower-bound-1} (modulo some logarithmic factor). 
One can also easily translate the above regret bound into sample complexity or probably approximately correct (PAC) bounds: 
%invoking a standard reduction argument (cf.~Section~\ref{sec:pre}), we see that 
the proposed algorithm is able to return an $\varepsilon$-suboptimal policy with high probability using at most 
%
\begin{equation}
	\text{(sample complexity)}\qquad 
	\widetilde{O}\left(\frac{SAH^3}{\varepsilon^2}\right) \quad \text{episodes}
\end{equation}
%
(or equivalently, $\widetilde{O}\big(\frac{SAH^4}{\varepsilon^2}\big)$ sample transitions as each episode has length $H$).  
Remarkably, this result holds true for the entire $\varepsilon$ range (i.e., any $\varepsilon \in (0,H]$), essentially eliminating the need of any burn-in cost.  
It is noteworthy that even in the presence of an idealistic generative model, this order of sample size is un-improvable \citep{azar2013minimax,li2020breaking}. 




The algorithm proposed herein is a modified version of $\mathtt{MVP}$: {\em Monotonic Value Propagation}. 
Originally proposed by \citet{zhang2020reinforcement},  
the $\mathtt{MVP}$ method falls under the category of the model-based approach, 
a family of algorithms that construct explicit estimates of the probability transition kernel before value estimation and policy learning. 
Notably, a technical obstacle that obstructs the progress in understanding model-based algorithms  
arises from the exceedingly large model dimensionality: 
given that the dimension of the transition kernel scales proportionally with $S^2$, 
all existing analyses for model-based online RL fell short of effectiveness unless the sample size already far exceeds $S^2$ \citep{azar2017minimax,zhang2020reinforcement}.  
To overcome this undesirable source of burn-in cost, 
a crucial step is to empower the analysis framework in order to accommodate the highly sub-sampled regime (i.e., a regime where the sample size scales linearly with $S$), 
which we shall elaborate on in Section~\ref{sec:tec}. 
The full proof of Theorem~\ref{thm1} will be provided in Section~\ref{app:thmmain}.


% Our main contribution is precisely to break this technical barrier 

 



%The transition probability matrix scales $O\left(S^2\right)$, and this is the reason why all existing model-based algorithm has at least an $\widetilde{O}\left(S^2\right)$ term in the burn-cost. Our main technical contribution is to break this barrier using a novel decoupling technique.

%See Section~\ref{sec:tec} for more details. 




\subsubsection{Extension: optimal problem-dependent regret bounds}


In practice, RL algorithms often perform far more appealingly than what their worst-case performance guarantees would suggest. 
This motivates a recent line of works that 
%goes beyond worst-case regret to 
investigate optimal performance 
in a more problem-dependent fashion \citep{talebi2018variance,simchowitz2019non,zanette2019tighter,zhou2023sharp,fruit2018efficient,xu2021fine,yang2021q,jin2020reward,wagenmaker2022first,zhao2023variance,dann2021beyond,tirinzoni2021fully}. 
Encouragingly, the proposed algorithm automatically achieves optimality on a more refined problem-dependent level, 
without requiring prior knowledge of additional problem-specific knowledge. 
This results in a couple of extended theorems that take into account different problem-dependent quantities. 



The first extension below investigates how the optimal value influences the regret bound. 
%
\begin{theorem}[Optimal value-dependent regret] \label{thm:first}
For any $K \ge 1$, 
%and any $0<\delta<1$, 
Algorithm~\ref{alg:main} satisfies
%
\begin{align}
	%\mathsf{Regret}(K) \leq \widetilde{O}\left(\min\big\{\sqrt{SAH^2K v^{\star}}+SAH^2,\, Kv^{\star} \big\}\right)
	\mathbb{E}\big[\mathsf{Regret}(K)\big] \lesssim \min\big\{ \sqrt{SAH^{2}Kv^{\star}}, Kv^{\star}\big\} \log^{5}(SAHK), 
	\label{eq:optimal-regret-value}
\end{align}
%
%with probability at least $1-\delta$, 
where $v^{\star}$ is the value of the optimal policy averaged over the initial state distribution (to be formally defined in \eqref{eq:defn-vstar-formal}).
\end{theorem}


Moreover, there is also no shortage of applications where the use of a cost function is preferred over a value function \citep{agarwal2017open,allen2018make,lee2020bias,wang2023benefits}. For this purpose, we provide another variation based upon the optimal cost. 
%
\begin{theorem}[Optimal cost-dependent regret]\label{thm:cost}
%
For any $K \ge 1$ and any $0<\delta<1$, 
Algorithm~\ref{alg:main} achieves 
\begin{align}
	\mathsf{Regret}(K) \leq \widetilde{O}\left(\min\big\{\sqrt{SAH^2K c^{\star}}+SAH^2,\, K(H-c^{\star}) \big\}\right)
	\label{eq:optimal-regret-cost}
\end{align}
with probability exceeding $1-\delta$, where $c^{\star}$ denotes the cost of the optimal policy averaged over the initial state distribution (to be formally defined in \eqref{eq:defn-cstar-formal}).
\end{theorem}
%
\noindent 
It is worth noting that: despite the apparent similarity between the statements of Theorem~\ref{thm:first} and Theorem~\ref{thm:cost}, 
they do not imply each other, although their proofs are very similar to each other.
 

Finally, we establish another regret bound that reflects the effect of certain variance quantities of interest.  
%
\begin{theorem}[Optimal variance-dependent regret] \label{thm:var}
%
For any $K \ge 1$ and any $0<\delta<1$, 
Algorithm~\ref{alg:main} obeys
%
\begin{align}
	\mathsf{Regret}(K) \leq \widetilde{O}\left(\min\big\{\sqrt{SAHK\mathrm{var}}+SAH^2,\,KH\big\}\right) 
	\label{eq:optimal-regret-var}
\end{align}
%
with probability at least $1-\delta$, 
	where $\mathrm{var}$ is a certain variance-type metric (to be formally defined in \eqref{eq:defn-var-formal}).
\end{theorem}
%

Two remarks concerning the above extensions are in order: 
%
\begin{itemize}
	\item 
		In the worst-case scenarios, the quantities $v^{\star}$, $c^{\star}$ and $\mathrm{var}$ can all be as large as the order of $H$, 
		in which case Theorems~\ref{thm:first}-\ref{thm:var} readily recover Theorem~\ref{thm1}. 
%
In contrast, the advantages of Theorems~\ref{thm:first}-\ref{thm:var} 
		%and Corollary~\ref{thm:cost} 
		over Theorem~\ref{thm1} become more evident 
		in those favorable cases (e.g., the situation where $v^{\star} \ll H$ or $c^{\star} \ll H$, or the case when the environment is nearly deterministic (so that $\mathrm{var}\approx 0$)).

	\item 
Interestingly, the regret bounds in Theorems~\ref{thm:first}-\ref{thm:var} 
		%and Corollary~\ref{thm:cost} 
		all contain a lower-order term $SAH^2$, 
and one might naturally wonder whether this term is essential. 
To demonstrate the unavoidable nature of this term and hence the optimality of Theorems~\ref{thm:first}-\ref{thm:var},
		%and Corollary~\ref{thm:cost}, 
we will provide matching lower bounds, to be detailed in Section~\ref{sec:extensions}.
%
\end{itemize}
 %

 
% We note that Theorem~\ref{thm1} works on the regime $K\geq SAH$. For the left regime, i.e., $K\leq SAH$, the minimax regret bound is trivially $\Theta(KH)$.

%\simon{In the final version, I think we can state the minimax bound first, and as an extension talking about first-order regret.}

%\begin{remark}
%In the case $K\leq SAH$, it is easy to construct a counter example with regret $\Omega(KH)$. On the other hand, by Theorem~\ref{thm1}, we can obtain a regret bound of $\min\{KH , \sqrt{SAKH^3}\}$ with no lower order terms.
%\simon{no lower order term here.}
%\end{remark}

% \simon{we may emphasize MVP is much simpler than other algorithms like EULER or Azar's.}




%\paragraph{On improving the lower order terms}
%In the seminal work \cite{azar2017minimax}, to achieve asymptotic optimal regret bound, the authors




%More precisely, existing methods needs $\|V-V^{\mathrm{ref}}\|$

%In other word, to improve the sample efficiency, we need $\|V-V^{\mathrm{ref}}\|$, which implies we can not make the improvement before $V^{\mathrm{ref}}$ is learned

%to make $\|V-V^{\mathrm{ref}}\|_{\infty}$ small enough, 
 %to learn such a reference value function $V^{\mathrm{ref}}$, existing methods need to estimate the optimal value function $V^{\star}$ up to a non-trivial accuracy, 


%This leads to the conflict that: to make $\|V-V^{\mathrm{ref}}\|_{\infty}$ small











%The burn-in time is defined as $\min\{k: \mathrm{Regret}(k')\leq cHk',\forall k'\geq k'\}$ where $c$ is some proper constants. 
%Here we take $c=0.1$ for convenience. It is clear the best  possible minimax burn-in time is $SAH$, since we need to visit each state-action-horizon triple for one time. Currently the best burn-in time is $O(\min\{S^2AH,SAH^3\})$, where the first $O(S^2AH)$ bound is by some model-based algorithms like MVP, and the second bound is by model-free like algorithms.

%\begin{remark}
%In recent, first-order dependent regret bound has been proposed, where the leading term in the regret is  $\widetilde{O}(\sqrt{SAH^2 V^{\star}})$ where $V^{\star}$ is the optimal value function at the initial state. It is easy to observe that we need  $\Omega(1/V^{\star})$ episodes to learn an $V^{\star}$-optimal policy in the worst case. And in the case $V^{\star}<0.1$, the burn-in time is trivially $1$. 
%\end{remark}



