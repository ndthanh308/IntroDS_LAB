
 


Focusing on tabular online RL in time-inhomogeneous finite-horizon MDPs, 
this paper has established the minimax-optimal regret (resp.~sample complexity) --- up to log factors --- for the entire range of sample size $K\geq 1$ (resp.~target accuracy level $\varepsilon \in (0,H]$), thereby fully settling an open problem at the core of recent RL theory. The $\mathtt{MVP}$ algorithm studied herein is model-based in nature. Remarkably, the model-based approach remains the only family of algorithms that is capable of obtaining minimax optimality without burn-ins, regardless of the data collection mechanism in use (e.g., online RL, offline RL, and the simulator setting). We have further unlocked the optimality of this algorithm in a more refined manner, 
making apparent the effect of several problem-dependent quantities (e.g., optimal value/cost, variance statistics) upon the fundamental performance limits. 
The new analysis and algorithmic techniques put forward herein might shed important light on  
how to conquer other RL settings as well.  
  


Moving forward, there are multiple directions that anticipate further theoretical pursuit. 
To begin with, is it possible to develop a model-free algorithm --- which often exhibits more favorable memory complexity compared to the model-based counterpart --- that achieves full-range minimax optimality?  
As alluded to previously, existing paradigms that rely on reference-advantage decomposition (or variance reduction) seem to incur a high burn-in cost \citep{zhang2020almost,li2021breaking}, thus calling for new ideas to overcome this barrier. 
Additionally, multiple other tabular settings (e.g., time-homogeneous finite-horizon MDPs, discounted infinite-horizon MDPs) have also suffered from similar issues regarding the burn-in requirements \citep{zhang2020reinforcement,ji2023regret}. 
Take time-homogeneous finite-horizon MDPs for example: in order to achieve optimal sample efficiency, 
one needs to carefully deal with the statistical dependency incurred by aggregating data from across different time steps to estimate the same transition matrix (due to the homogeneous nature of $P$), which results in more intricate issues than the time-homogeneous counterpart.  
 We believe that resolving these two open problems will greatly enhance our theoretical understanding about online RL and beyond.






%One natural open problem is whether we can obtain optimal regret and sample complexity bounds for time-homogeneous settings. Here, one needs to deal with the dependency incurred by using the data from different time steps to estimate one transition matrix. 
%Another open problem is to develop model-free algorithms that attain the optimal regret. As we discussed in Section~\ref{sec:tec}, the existing approach based on the reference function will incur a high burn-in cost.




%\yxc{edit later} To our knowledge, even beyond the time-inhomogeneous finite-horizon MDPs, in all other tabular RL settings such as time-homogeneous finite-horizon MDPs, infinite-horizon discounted MDPs,  infinite-horizon average reward MDPs, and stochastic shortest path, existing asymptotically optimal algorithms also suffer some burn-in cost.


%\yxc{edit later} % Furthermore, the tabular RL algorithms and analyses often serve as the prototypes for solving more complex RL problems, and in some problems, the burn-in cost becomes the dominating term even when $K$ goes to infinity~\citep{domingues2020regret,sinclair2020adaptive}.



