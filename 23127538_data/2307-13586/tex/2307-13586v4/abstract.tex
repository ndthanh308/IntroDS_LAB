A central issue lying at the heart of online reinforcement learning (RL) is data efficiency. 
While a number of recent works achieved asymptotically minimal regret in online RL, 
the optimality of these results is only guaranteed in a ``large-sample'' regime, 
imposing enormous burn-in cost in order for their algorithms to operate optimally. 
How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory.  

We settle this problem for finite-horizon inhomogeneous Markov decision processes. 
Specifically, we prove that a modified version of $\mathtt{MVP}$ (Monotonic Value Propagation), an optimistic model-based algorithm proposed by \citet{zhang2020reinforcement}, achieves a regret on the order of (modulo log factors)
%
\begin{equation*}
	\min\big\{  \sqrt{SAH^3K}, \,HK \big\},
\end{equation*}
% 
where $S$ is the number of states, $A$ is the number of actions, $H$ is the horizon length, and $K$ is the total number of episodes. 
This regret matches the minimax lower bound for the entire range of sample size $K\geq 1$, essentially eliminating any burn-in requirement. It also translates to a PAC sample complexity (i.e., the number of episodes needed to yield $\varepsilon$-accuracy) of $\frac{SAH^3}{\varepsilon^2}$ up to log factor, which is minimax-optimal for the full $\varepsilon$-range. 
 Further, we extend our theory to unveil the influences of problem-dependent quantities like the optimal value/cost and certain variances.  
The key technical innovation lies in a novel analysis paradigm (based on a new concept called ``profiles'') to decouple complicated statistical dependency across the sample trajectories --- a long-standing challenge facing the analysis of online RL in the sample-starved regime. 




%\noindent \textbf{Keywords:} online RL; minimax regret; burn-in cost; optimal sample complexity; model-based algorithms






