\begin{thebibliography}{}

\bibitem[Agarwal et~al., 2020]{agarwal2020model}
Agarwal, A., Kakade, S., and Yang, L.~F. (2020).
\newblock Model-based reinforcement learning with a generative model is minimax
  optimal.
\newblock In {\em Conference on Learning Theory}, pages 67--83.

\bibitem[Agarwal et~al., 2017]{agarwal2017open}
Agarwal, A., Krishnamurthy, A., Langford, J., Luo, H., et~al. (2017).
\newblock Open problem: First-order regret bounds for contextual bandits.
\newblock In {\em Conference on Learning Theory}, pages 4--7.

\bibitem[Agrawal and Jia, 2017]{agralwal2017optimistic}
Agrawal, S. and Jia, R. (2017).
\newblock Optimistic posterior sampling for reinforcement learning: worst-case
  regret bounds.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems 30}, pages 1184--1194. Curran Associates, Inc.

\bibitem[Allen-Zhu et~al., 2018]{allen2018make}
Allen-Zhu, Z., Bubeck, S., and Li, Y. (2018).
\newblock Make the minority great again: First-order regret bound for
  contextual bandits.
\newblock In {\em International Conference on Machine Learning}, pages
  186--194.

\bibitem[Azar et~al., 2013]{azar2013minimax}
Azar, M.~G., Munos, R., and Kappen, H.~J. (2013).
\newblock Minimax {PAC} bounds on the sample complexity of reinforcement
  learning with a generative model.
\newblock {\em Machine learning}, 91(3):325--349.

\bibitem[Azar et~al., 2017]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R. (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, pages 263--272.

\bibitem[Bai et~al., 2019]{bai2019provably}
Bai, Y., Xie, T., Jiang, N., and Wang, Y.-X. (2019).
\newblock Provably efficient {Q}-learning with low switching cost.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8004--8013.

\bibitem[Bartlett and Tewari, 2009]{bartlett2009regal}
Bartlett, P.~L. and Tewari, A. (2009).
\newblock Regal: a regularization based algorithm for reinforcement learning in
  weakly communicating mdps.
\newblock In {\em Proceedings of the 25th Conference on Uncertainty in
  Artificial Intelligence (UAI 2009))}.

\bibitem[Beck and Srikant, 2012]{beck2012error}
Beck, C.~L. and Srikant, R. (2012).
\newblock Error bounds for constant step-size {Q}-learning.
\newblock {\em Systems \& control letters}, 61(12):1203--1208.

\bibitem[Bertsekas, 2019]{bertsekas2019reinforcement}
Bertsekas, D. (2019).
\newblock {\em Reinforcement learning and optimal control}.
\newblock Athena Scientific.

\bibitem[Brafman and Tennenholtz, 2003]{brafman2002r}
Brafman, R.~I. and Tennenholtz, M. (2003).
\newblock R-max - a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock {\em J. Mach. Learn. Res.}, 3(Oct):213--231.

\bibitem[Cai et~al., 2019]{cai2019provably}
Cai, Q., Yang, Z., Jin, C., and Wang, Z. (2019).
\newblock Provably efficient exploration in policy optimization.
\newblock {\em arXiv preprint arXiv:1912.05830}.

\bibitem[Chen et~al., 2021]{chen2021implicit}
Chen, L., Jafarnia-Jahromi, M., Jain, R., and Luo, H. (2021).
\newblock Implicit finite-horizon approximation and efficient optimal
  algorithms for stochastic shortest path.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Chen et~al., 2020]{chen2020finite}
Chen, Z., Maguluri, S.~T., Shakkottai, S., and Shanmugam, K. (2020).
\newblock Finite-sample analysis of contractive stochastic approximation using
  smooth convex envelopes.
\newblock {\em Advances in Neural Information Processing Systems},
  33:8223--8234.

\bibitem[Cui and Yang, 2021]{cui2021minimax}
Cui, Q. and Yang, L.~F. (2021).
\newblock Minimax sample complexity for turn-based stochastic game.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 1496--1504.

\bibitem[Dann and Brunskill, 2015]{dann2015sample}
Dann, C. and Brunskill, E. (2015).
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2818--2826.

\bibitem[Dann et~al., 2017]{dann2017unifying}
Dann, C., Lattimore, T., and Brunskill, E. (2017).
\newblock Unifying {PAC} and regret: Uniform {PAC} bounds for episodic
  reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 30.

\bibitem[Dann et~al., 2019]{dann2019policy}
Dann, C., Li, L., Wei, W., and Brunskill, E. (2019).
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, pages 1507--1516.

\bibitem[Dann et~al., 2021]{dann2021beyond}
Dann, C., Marinov, T.~V., Mohri, M., and Zimmert, J. (2021).
\newblock Beyond value-function gaps: Improved instance-dependent regret bounds
  for episodic reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34:1--12.

\bibitem[Domingues et~al., 2021]{domingues2021episodic}
Domingues, O.~D., M{\'e}nard, P., Kaufmann, E., and Valko, M. (2021).
\newblock Episodic reinforcement learning in finite mdps: Minimax lower bounds
  revisited.
\newblock In {\em Algorithmic Learning Theory}, pages 578--598.

\bibitem[Dong et~al., 2019]{dong2019q}
Dong, K., Wang, Y., Chen, X., and Wang, L. (2019).
\newblock Q-learning with {UCB} exploration is sample efficient for
  infinite-horizon {MDP}.
\newblock {\em arXiv preprint arXiv:1901.09311}.

\bibitem[Efroni et~al., 2019]{efroni2019tight}
Efroni, Y., Merlis, N., Ghavamzadeh, M., and Mannor, S. (2019).
\newblock Tight regret bounds for model-based reinforcement learning with
  greedy policies.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Even{-}Dar and Mansour, 2003]{even2003learning}
Even{-}Dar, E. and Mansour, Y. (2003).
\newblock Learning rates for {Q}-learning.
\newblock {\em Journal of Machine Learning Research}, 5(Dec):1--25.

\bibitem[Freedman, 1975]{freedman1975tail}
Freedman, D.~A. (1975).
\newblock On tail probabilities for martingales.
\newblock {\em the Annals of Probability}, 3(1):100--118.

\bibitem[Fruit et~al., 2018]{fruit2018efficient}
Fruit, R., Pirotta, M., Lazaric, A., and Ortner, R. (2018).
\newblock Efficient bias-span-constrained exploration-exploitation in
  reinforcement learning.
\newblock In {\em ICML 2018-The 35th International Conference on Machine
  Learning}, volume~80, pages 1578--1586.

\bibitem[Jaksch et~al., 2010]{jaksch2010near}
Jaksch, T., Ortner, R., and Auer, P. (2010).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 11(Apr):1563--1600.

\bibitem[Ji and Li, 2023]{ji2023regret}
Ji, X. and Li, G. (2023).
\newblock Regret-optimal model-free reinforcement learning for discounted
  {MDP}s with short burn-in time.
\newblock {\em Advances in neural information processing systems}.

\bibitem[Jiang and Agarwal, 2018]{jiang2018open}
Jiang, N. and Agarwal, A. (2018).
\newblock Open problem: The dependence of sample complexity lower bounds on
  planning horizon.
\newblock In {\em Conference On Learning Theory}, pages 3395--3398.

\bibitem[Jin et~al., 2018]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I. (2018).
\newblock Is {Q}-learning provably efficient?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4863--4873.

\bibitem[Jin et~al., 2020]{jin2020reward}
Jin, C., Krishnamurthy, A., Simchowitz, M., and Yu, T. (2020).
\newblock Reward-free exploration for reinforcement learning.
\newblock {\em International Conference on Machine Learning}.

\bibitem[Jin et~al., 2021]{jin2021pessimism}
Jin, Y., Yang, Z., and Wang, Z. (2021).
\newblock Is pessimism provably efficient for offline {RL}?
\newblock In {\em International Conference on Machine Learning}, pages
  5084--5096.

\bibitem[Kakade, 2003]{kakade2003sample}
Kakade, S.~M. (2003).
\newblock {\em On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University of London London, England.

\bibitem[Kearns and Singh, 1998a]{kearns1998finite}
Kearns, M. and Singh, S. (1998a).
\newblock Finite-sample convergence rates for {Q}-learning and indirect
  algorithms.
\newblock {\em Advances in neural information processing systems}, 11.

\bibitem[Kearns and Singh, 1998b]{kearns1998near}
Kearns, M.~J. and Singh, S.~P. (1998b).
\newblock Near-optimal reinforcement learning in polynominal time.
\newblock In {\em Proceedings of the Fifteenth International Conference on
  Machine Learning}, pages 260--268.

\bibitem[Kolter and Ng, 2009]{kolter2009near}
Kolter, J.~Z. and Ng, A.~Y. (2009).
\newblock Near-bayesian exploration in polynomial time.
\newblock In {\em Proceedings of the 26th annual international conference on
  machine learning}, pages 513--520.

\bibitem[Lattimore and Hutter, 2012]{lattimore2012pac}
Lattimore, T. and Hutter, M. (2012).
\newblock {PAC} bounds for discounted {MDPs}.
\newblock In {\em International Conference on Algorithmic Learning Theory},
  pages 320--334. Springer.

\bibitem[Lee et~al., 2020]{lee2020bias}
Lee, C.-W., Luo, H., Wei, C.-Y., and Zhang, M. (2020).
\newblock Bias no more: high-probability data-dependent regret bounds for
  adversarial bandits and mdps.
\newblock {\em Advances in neural information processing systems},
  33:15522--15533.

\bibitem[Levine et~al., 2020]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J. (2020).
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock {\em arXiv preprint arXiv:2005.01643}.

\bibitem[Li et~al., 2024a]{li2023q}
Li, G., Cai, C., Chen, Y., Wei, Y., and Chi, Y. (2024a).
\newblock Is {Q}-learning minimax optimal? a tight sample complexity analysis.
\newblock {\em Operations Research}, 72(1):222--236.

\bibitem[Li et~al., 2022]{li2022minimax}
Li, G., Chi, Y., Wei, Y., and Chen, Y. (2022).
\newblock Minimax-optimal multi-agent {RL} in {M}arkov games with a generative
  model.
\newblock {\em Advances in Neural Information Processing Systems},
  35:15353--15367.

\bibitem[Li et~al., 2023]{li2022settling}
Li, G., Shi, L., Chen, Y., Chi, Y., and Wei, Y. (2023).
\newblock Settling the sample complexity of model-based offline reinforcement
  learning.
\newblock {\em accepted to the Annals of Statistics}.

\bibitem[Li et~al., 2021a]{li2021breaking}
Li, G., Shi, L., Chen, Y., Gu, Y., and Chi, Y. (2021a).
\newblock Breaking the sample complexity barrier to regret-optimal model-free
  reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Li et~al., 2024b]{li2020breaking}
Li, G., Wei, Y., Chi, Y., and Chen, Y. (2024b).
\newblock Breaking the sample size barrier in model-based reinforcement
  learning with a generative model.
\newblock {\em Operations Research}, 72(1):203--221.

\bibitem[Li et~al., 2021b]{li2021sample}
Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y. (2021b).
\newblock Sample complexity of asynchronous {Q}-learning: Sharper analysis and
  variance reduction.
\newblock {\em IEEE Transactions on Information Theory}, 68(1):448--473.

\bibitem[Li et~al., 2024c]{li2023minimax}
Li, G., Yan, Y., Chen, Y., and Fan, J. (2024c).
\newblock Minimax-optimal reward-agnostic exploration in reinforcement
  learning.
\newblock {\em Conference on Learning Theory (COLT)}.

\bibitem[Li et~al., 2021c]{li2021settling}
Li, Y., Wang, R., and Yang, L.~F. (2021c).
\newblock Settling the horizon-dependence of sample complexity in reinforcement
  learning.
\newblock In {\em IEEE Symposium on Foundations of Computer Science}.

\bibitem[Maurer and Pontil, 2009]{maurer2009empirical}
Maurer, A. and Pontil, M. (2009).
\newblock Empirical {B}ernstein bounds and sample variance penalization.
\newblock In {\em Conference on Learning Theory}.

\bibitem[M{\'e}nard et~al., 2021]{menard2021ucb}
M{\'e}nard, P., Domingues, O.~D., Shang, X., and Valko, M. (2021).
\newblock {UCB} momentum {Q}-learning: Correcting the bias without forgetting.
\newblock In {\em International Conference on Machine Learning}, pages
  7609--7618.

\bibitem[Neu and Pike-Burke, 2020]{neu2020unifying}
Neu, G. and Pike-Burke, C. (2020).
\newblock A unifying view of optimism in episodic reinforcement learning.
\newblock {\em arXiv preprint arXiv:2007.01891}.

\bibitem[Osband et~al., 2013]{osband2013more}
Osband, I., Russo, D., and Van~Roy, B. (2013).
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3003--3011.

\bibitem[Pacchiano et~al., 2020]{pacchiano2020optimism}
Pacchiano, A., Ball, P., Parker-Holder, J., Choromanski, K., and Roberts, S.
  (2020).
\newblock On optimism in model-based reinforcement learning.
\newblock {\em arXiv preprint arXiv:2006.11911}.

\bibitem[Pananjady and Wainwright, 2020]{pananjady2020instance}
Pananjady, A. and Wainwright, M.~J. (2020).
\newblock Instance-dependent $\ell_{\infty}$-bounds for policy evaluation in
  tabular reinforcement learning.
\newblock {\em IEEE Transactions on Information Theory}, 67(1):566--585.

\bibitem[Qu and Wierman, 2020]{qu2020finite}
Qu, G. and Wierman, A. (2020).
\newblock Finite-time analysis of asynchronous stochastic approximation and
  {Q}-learning.
\newblock In {\em Proceedings of Thirty Third Conference on Learning Theory
  (COLT)}.

\bibitem[Rashidinejad et~al., 2021]{rashidinejad2021bridging}
Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. (2021).
\newblock Bridging offline reinforcement learning and imitation learning: A
  tale of pessimism.
\newblock {\em Advances in Neural Information Processing Systems},
  34:11702--11716.

\bibitem[Ren et~al., 2021]{ren2021nearly}
Ren, T., Li, J., Dai, B., Du, S.~S., and Sanghavi, S. (2021).
\newblock Nearly horizon-free offline reinforcement learning.
\newblock {\em Advances in neural information processing systems},
  34:15621--15634.

\bibitem[Russo, 2019]{russo2019worst}
Russo, D. (2019).
\newblock Worst-case regret bounds for exploration via randomized value
  functions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  14433--14443.

\bibitem[Shi et~al., 2022]{shi2022pessimistic}
Shi, L., Li, G., Wei, Y., Chen, Y., and Chi, Y. (2022).
\newblock Pessimistic {Q}-learning for offline reinforcement learning: Towards
  optimal sample complexity.
\newblock In {\em International Conference on Machine Learning}, pages
  19967--20025.

\bibitem[Shi et~al., 2023]{shi2023curious}
Shi, L., Li, G., Wei, Y., Chen, Y., Geist, M., and Chi, Y. (2023).
\newblock The curious price of distributional robustness in reinforcement
  learning with a generative model.
\newblock {\em Advances in Neural Information Processing Systems}.

\bibitem[Sidford et~al., 2018a]{sidford2018near}
Sidford, A., Wang, M., Wu, X., Yang, L., and Ye, Y. (2018a).
\newblock Near-optimal time and sample complexities for solving {Markov}
  decision processes with a generative model.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5186--5196.

\bibitem[Sidford et~al., 2018b]{sidford2018variance}
Sidford, A., Wang, M., Wu, X., and Ye, Y. (2018b).
\newblock Variance reduced value iteration and faster algorithms for solving
  {Markov} decision processes.
\newblock In {\em Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 770--787. Society for Industrial and Applied
  Mathematics.

\bibitem[Simchowitz and Jamieson, 2019]{simchowitz2019non}
Simchowitz, M. and Jamieson, K.~G. (2019).
\newblock Non-asymptotic gap-dependent regret bounds for tabular {MDPs}.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1153--1162.

\bibitem[Strehl et~al., 2006]{strehl2006pac}
Strehl, A.~L., Li, L., Wiewiora, E., Langford, J., and Littman, M.~L. (2006).
\newblock {PAC model-free reinforcement learning}.
\newblock In {\em Proceedings of the 23rd international conference on Machine
  learning}, pages 881--888. ACM.

\bibitem[Strehl and Littman, 2008]{strehl2008analysis}
Strehl, A.~L. and Littman, M.~L. (2008).
\newblock An analysis of model-based interval estimation for markov decision
  processes.
\newblock {\em Journal of Computer and System Sciences}, 74(8):1309--1331.

\bibitem[Szita and Szepesv{\'a}ri, 2010]{szita2010model}
Szita, I. and Szepesv{\'a}ri, C. (2010).
\newblock Model-based reinforcement learning with nearly tight exploration
  complexity bounds.
\newblock In {\em ICML}.

\bibitem[Talebi and Maillard, 2018]{talebi2018variance}
Talebi, M.~S. and Maillard, O.-A. (2018).
\newblock Variance-aware regret bounds for undiscounted reinforcement learning
  in mdps.
\newblock {\em arXiv preprint arXiv:1803.01626}.

\bibitem[Tarbouriech et~al., 2021]{tarbouriech2021stochastic}
Tarbouriech, J., Zhou, R., Du, S.~S., Pirotta, M., Valko, M., and Lazaric, A.
  (2021).
\newblock Stochastic shortest path: Minimax, parameter-free and towards
  horizon-free regret.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Tirinzoni et~al., 2021]{tirinzoni2021fully}
Tirinzoni, A., Pirotta, M., and Lazaric, A. (2021).
\newblock A fully problem-dependent regret lower bound for finite-horizon
  {MDPs}.
\newblock {\em arXiv preprint arXiv:2106.13013}.

\bibitem[Wagenmaker et~al., 2022]{wagenmaker2022first}
Wagenmaker, A.~J., Chen, Y., Simchowitz, M., Du, S., and Jamieson, K. (2022).
\newblock First-order regret in reinforcement learning with linear function
  approximation: A robust estimation approach.
\newblock In {\em International Conference on Machine Learning}, pages
  22384--22429.

\bibitem[Wainwright, 2019a]{wainwright2019stochastic}
Wainwright, M.~J. (2019a).
\newblock Stochastic approximation with cone-contractive operators: Sharp
  $\ell_{\infty} $-bounds for {Q}-learning.
\newblock {\em arXiv preprint arXiv:1905.06265}.

\bibitem[Wainwright, 2019b]{wainwright2019variance}
Wainwright, M.~J. (2019b).
\newblock Variance-reduced {Q}-learning is minimax optimal.
\newblock {\em arXiv preprint arXiv:1906.04697}.

\bibitem[Wang et~al., 2023]{wang2023benefits}
Wang, K., Zhou, K., Wu, R., Kallus, N., and Sun, W. (2023).
\newblock The benefits of being distributional: Small-loss bounds for
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2305.15703}.

\bibitem[Wang et~al., 2020]{wang2020long}
Wang, R., Du, S.~S., Yang, L.~F., and Kakade, S.~M. (2020).
\newblock Is long horizon reinforcement learning more difficult than short
  horizon reinforcement learning?
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Wang et~al., 2022]{wang2022gap}
Wang, X., Cui, Q., and Du, S.~S. (2022).
\newblock On gap-dependent bounds for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems},
  35:14865--14877.

\bibitem[Xie et~al., 2021]{xie2021policy}
Xie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y. (2021).
\newblock Policy finetuning: Bridging sample-efficient offline and online
  reinforcement learning.
\newblock {\em Advances in neural information processing systems},
  34:27395--27407.

\bibitem[Xiong et~al., 2022]{xiong2021randomized}
Xiong, Z., Shen, R., Cui, Q., Fazel, M., and Du, S.~S. (2022).
\newblock Near-optimal randomized exploration for tabular markov decision
  processes.
\newblock {\em Advances in Neural Information Processing Systems},
  35:6358--6371.

\bibitem[Xu et~al., 2021]{xu2021fine}
Xu, H., Ma, T., and Du, S. (2021).
\newblock Fine-grained gap-dependent bounds for tabular {MDPs} via adaptive
  multi-step bootstrap.
\newblock In {\em Conference on Learning Theory}, pages 4438--4472.

\bibitem[Yan et~al., 2023]{yan2022efficacy}
Yan, Y., Li, G., Chen, Y., and Fan, J. (2023).
\newblock The efficacy of pessimism in asynchronous {Q}-learning.
\newblock {\em IEEE Transactions on Information Theory}, 69(11):7185--7219.

\bibitem[Yang et~al., 2021]{yang2021q}
Yang, K., Yang, L., and Du, S. (2021).
\newblock {$Q$}-learning with logarithmic regret.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1576--1584.

\bibitem[Yin et~al., 2022]{yin2022near}
Yin, M., Duan, Y., Wang, M., and Wang, Y.-X. (2022).
\newblock Near-optimal offline reinforcement learning with linear
  representation: Leveraging variance information with pessimism.
\newblock {\em arXiv preprint arXiv:2203.05804}.

\bibitem[Zanette and Brunskill, 2019]{zanette2019tighter}
Zanette, A. and Brunskill, E. (2019).
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In {\em International Conference on Machine Learning}, pages
  7304--7312.

\bibitem[Zhang et~al., 2021a]{zhang2020reinforcement}
Zhang, Z., Ji, X., and Du, S. (2021a).
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock In {\em Conference on Learning Theory}, pages 4528--4531.

\bibitem[Zhang et~al., 2022]{zhang2022horizon}
Zhang, Z., Ji, X., and Du, S. (2022).
\newblock Horizon-free reinforcement learning in polynomial time: the power of
  stationary policies.
\newblock In {\em Conference on Learning Theory}, pages 3858--3904.

\bibitem[Zhang et~al., 2020]{zhang2020almost}
Zhang, Z., Zhou, Y., and Ji, X. (2020).
\newblock Almost optimal model-free reinforcement learning via
  reference-advantage decomposition.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Zhang et~al., 2021b]{zhang2020model}
Zhang, Z., Zhou, Y., and Ji, X. (2021b).
\newblock Model-free reinforcement learning: from clipped pseudo-regret to
  sample complexity.
\newblock In {\em International Conference on Machine Learning}, pages
  12653--12662.

\bibitem[Zhao et~al., 2023]{zhao2023variance}
Zhao, H., He, J., Zhou, D., Zhang, T., and Gu, Q. (2023).
\newblock Variance-dependent regret bounds for linear bandits and reinforcement
  learning: Adaptivity and computational efficiency.
\newblock {\em arXiv preprint arXiv:2302.10371}.

\bibitem[Zhou et~al., 2023]{zhou2023sharp}
Zhou, R., Zihan, Z., and Du, S.~S. (2023).
\newblock Sharp variance-dependent bounds in reinforcement learning: Best of
  both worlds in stochastic and deterministic environments.
\newblock In {\em International Conference on Machine Learning}, pages
  42878--42914.

\end{thebibliography}
