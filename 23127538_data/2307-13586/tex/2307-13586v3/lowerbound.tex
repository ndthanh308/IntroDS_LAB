

In this section, we establish the lower bounds advertised in this paper. 

\subsection{Proof of Theorem~\ref{thm:lb1}}\label{app:lbf}


Consider any given $(S,A,H)$.   We start by establishing the following lemma. 
%
\begin{lemma}\label{lemma:lb2}
Consider any $K'\geq 1$. 
For any algorithm, there exists an MDP instance with $S$ states, $A$ actions, and horizon $H$, such that the regret in $K'$ episodes is at least 
%
\begin{align}
\mathsf{Regret}(K')=\Omega\big( f(K')\big) = \Omega\left(\min\big\{\sqrt{SAH^3K'},K'H\big\}\right).\nonumber
\end{align}
%
\end{lemma}
%
\begin{proof}[Proof of Lemma~\ref{lemma:lb2}]
	Our construction of the hard instance is based on the hard instance JAO-MDP constructed in  \citet{jaksch2010near,jin2018q}. In  \citet[Appendix.D]{jin2018q}, the authors already showed that when $K'\geq C_0SAH$ for some constant $C_0>0$, the minimax regret lower bound is $\Omega(\sqrt{SAH^3K'})$. 
	Hence, it suffices for us to focus on the regime where $K'\leq C_0SAH$. Without loss of generality, we assume $S=A=2$, and the argument to generalize it to arbitrary $(S,A)$ is standard and henc omitted for brevity.  
	

	Recall the construction of JAO-MDP in \citet{jaksch2010near}. Let the two states be $x$ and $y$, and the two actions be $a$ and $b$. 
	The reward  is always equal to $x$ in state $1$ and $1/2$ in state $y$. The probability transition kernel is given by 
	$$
		P_{x,a} =P_{x,b}= [1-\delta,\delta], 
		~~~P_{y,a} = [1-\delta,\delta], ~~~
		P_{y,b}= [1-\delta -\epsilon,\delta+\epsilon],
	$$
	%
	where we choose $\delta = C_1 / H$ and $\epsilon =1/H$. Then the mixing time of the MDP is roughly $O(H)$. By choosing $C_1$ large enough, 
	we can ensure that the MDP is $C_3$-mixing after the first half of the horizons for some proper constant $C_3\in (0,1/2)$.

It is then easy to show that action $b$ is the optimal action for state $y$. 
	Moreover, whenever action $a$ is chosen in state $y$, the learner needs to pay regret $\Omega(\epsilon H)=\Omega(1)$. 
	In addition, to differentiate action $a$ from action $b$ in state $y$ with probability at least $1-\frac{1}{10}$, the learner needs at least $\Omega(\frac{\epsilon}{\delta^2}) = \Omega(H)$ rounds --- let us call it $C_4H$ rounds for some proper constant $C_4>0$. As a result, in the case where $K'\leq C_4H$, the minimax regret is at least $\Omega(K'H^2\epsilon)=\Omega(K'H)$. When $C_4H \leq K' \leq C_0SAH = 4C_0H$, the minimax regret is at least $\Omega(C_4H^2)=\Omega(K'H)$. This concludes the proof.
%
\end{proof}



%Fix the algorithm $\mathcal{G}$. 


With Lemma~\ref{lemma:lb2}, we are ready to prove Theorem~\ref{thm:lb1}. 
Let $\mathcal{M}$ be the hard instance  for $K' = \max\left\{\frac{1}{10}Kp ,1\right\}$ constructed in the proof of Lemma~\ref{lemma:lb2}. 
We construct an MDP $\mathcal{M}'$ as below. 
%
\begin{itemize}
	\item	In the first step, for any state $s$, with probability $p$, the leaner transitions to a copy of $\mathcal{M}$, and with probability $1-p$, the learner transitions to a dumb state with $0$ reward. 
\end{itemize}
%
It can be easily verified that $v^{\star}\leq pH$.
Let $X=X_1+X_2+\dots+X_k$, where $\{X_i\}_{i=1}^K$ are i.i.d.~Bernoulli random variables with mean $p$. Let $g(X,K')$ denote the minimax regret on the hard instance $\mathcal{M}$ in $X$ episodes. Given that $g(X,K')$ is non-decreasing in $X$,  
one sees that $$\mathsf{Regret}(K) \geq  \mathbb{E}\big[g(X,K') \big].$$ 
%
\begin{itemize}
	\item
In the case where $Kp\geq 10$,  Lemma~\ref{lemma:con} tells us that with probability at least $1/2$, $X\geq \frac{1}{10}Kp = K'$, and then it holds that 
		$$
			\mathbb{E}\big[g(X,K')\big] \geq \frac{1}{2} g(K',K')=\frac{1}{2}f(K')= \frac{1}{2} \Omega\left(\min\left\{\sqrt{SAH^3K'},K'H\right\}\right) = \Omega(\sqrt{SAH^3Kp},KHp).
		$$
	\item 
		In the case where $Kp<10$, with probability exceeding $1-(1-p)^K \geq (1-e^{-Kp})\geq  \frac{Kp}{30}$, one has $X\geq 1$. Then one has 
		$$
			\mathbb{E}\big[g(X,K')\big]\geq \frac{Kp}{30}\cdot g(1,K')=\frac{Kp}{30}\cdot g(1,1) =\Omega(KHp). 
		$$
%
\end{itemize}
%
The preceding bounds taken together complete the proof. 




\subsection{Proof of Theorem~\ref{corollary:costlb}}\label{app:lbc}

Without loss of generality, assume that $S=A=2$ (as in the proof of Theorem~\ref{thm:lb1}). Note that $p\leq 1/4$. 
We would like to construct a hard instance for which the learner needs to identify the correct action for each step. 
Let $\mathcal{S}=\{s_1,s_2\}$, and take the initial state to be $s_1$. The transition kernel and cost are chosen as follows. 
%
\begin{itemize}
	\item For any action $a$ and $h$,  set $P_{s_2,a,h} = e_{s_2}$ and $c_h(s_2,a)=0$. 
	\item For any action $a\neq a^{\star}$ and $h$,  set $P_{s_1,a,h} = e_{s_2}$ and $c_h(s_2,a)=1$. 
	\item Set $P_{s_1,a^{\star},h} = e_{s_1}$ and $c_h(s_1,a^{\star}) = p$. 
%
\end{itemize}
%
% Let the initial state be $s_1$.
It can be easily checked that $c^{\star} =Hp$ by choosing $a^{\star}$ for each step. To identify the correct action $a^{\star}$ for at least half of the $H$ steps, we need $\Omega(H)$ episodes, which implies that, there exists a constant $C_5>0$ such that in the first $K\leq C_5H$ episodes, the cost of the learner is at least $\frac{H(1-p)}{2}$. Then the minimax regret is at least 
$$
	\Omega\big(K(H-c^{\star}) \big)=\Omega\big(KH^2(1-p) \big)
$$ 
%
when $K\leq C_5H$. 
In the case where $C_5H \leq K \leq  \frac{100H}{p}$, the minimax regret is at least $$ \Omega\big(H(H-c^{\star})\big)= \Omega\big(H^2(1-p)\big).$$ 

For $K\geq \frac{100H}{p}$, we let $\mathcal{M}$ be the  hard instance with the same transition as that in the proof of Lemma~\ref{lemma:lb2}, and set the cost as $1'2$ for state $x$ and $1$ for state $y$ with respect to $K' = Kp/10 \geq 10H$.  
Let $\mathcal{M}'$ be the MDP such that: in the first step,  with probability $p$, the learner transitions to a copy of $\mathcal{M}$, and with probability $1-p$, the learner transitions to a dumb state with $0$ cost. Then we have $c^{\star} =\Theta(Hp)$. 
It follows from Lemma~\ref{lemma:con} that, with probability exceeding $1/2$, one has $ X\geq \frac{1}{3}Kp - \log 2 \geq \frac{1}{6}Kp$. Then one has 
%
$$\mathsf{Regret}(K)  \geq 
\frac{1}{2} \Omega\left(\min\big\{\sqrt{H^3K'}, K'H \big\}\right)= \Omega\big(\sqrt{H^3Kp}\big).$$ 
%
The proof is thus completed by combining the above minimax regret lower bounds for the three regimes $K\in [1,C_5H]$, $K\in(C_5H,\frac{100H}{p}]$ and $K\in(\frac{100H}{p},\infty]$.





%Mention the lower bound of $\Omega(KH)$ when $K\leq BSAH$ ($K\leq \frac{SAH^2}{v^{\star}}, \frac{SAH^3}{\mathrm{var}_1}, \frac{SAH^3}{\mathrm{var}_2}$)





\subsection{Proof of Theorem~\ref{thm:lb3}}


When $K\geq SAH/p$, the lower bound in Theorem~\ref{thm:lb1} readily applies because the regret is at least $\Omega(\sqrt{SAH^3Kp})$ and the variance $\mathrm{var}$ is at most $pH^2$. 
When $SAH\leq K \leq SAH/p$, the regret is at least $\Omega(SAH^2)=\Omega(\min\{\sqrt{SAH^3Kp}+SAH^2,KH \})$. 
As a result, it suffices to focus on the case where  $1\leq K\leq SAH$, 
Towards this end, we only need the following lemma, which suffices for us to complete the proof.  


% by Lemma~\ref{lemma:lb34}, the minimax regret is at least $\Omega(KH)$.


% For $SAH\leq K \leq SAH/p$, the regret is at least $\Omega(SAH^2)=\Omega(\min\{\sqrt{SAH^3Kp}+SAH^2,KH \})$. The proof is completed.


\begin{lemma}\label{lemma:lb34}
Consider any $1\leq K\leq SAH$. There exists  an MDP instance with $S$ states, $A$ actions, horizon $H$, and $\mathrm{var}_1 = \mathrm{var}_2 = 0$, such that the regret is at least $\Omega(KH)$.
\end{lemma}

\begin{proof}
Let us construct an MDP with deterministic transition; more precisely, for each $(s,a,h)$, there is some $s'$ such that $P_{s,a,h,s'}=1$ and $P_{s,a,h,s''}=0$ for any $s''\neq s'$. 
	The reward function is also chosen to be deterministic. In this case, it is easy to verify that $\mathrm{var}_1 = \mathrm{var}_2 = 0$. 


	We first assume $S=2$. For any action $a$ and horizon $h$, we set $P_{s_2,a,h} = e_{s_2}$ and $r_h(s_2,a)=0$. For any action $a\neq a^{\star}$ and $h$, we also set $P_{s_1,a,h} = e_{s_2}$ and $r_h(s_2,a)=0$. At last, we set $P_{s_1,a^{\star},h} = e_{s_1}$ and $r_h(s_1,a^{\star}) = 1$. In other words, there are a dumb state and a normal state in each step. The learner would naturally hope to find the correct action to avoid the dumb state. Obviously, $V_1^{\star}(s_1)=H$. To find an $\frac{H}{2}$-optimal policy, the learner needs to identify $a^{\star}$ for the first $\frac{H}{2}$ steps, requiring at least $\Omega(HA)$ rounds in expectation. As a result, the minimax regret is at least $\Omega(KH)$ when $K\leq cHA$ for some proper constant $c>0$.  


Let us refer to the hard instance above as a \emph{hard chain}.
	For general $S$, we can construct $d \coloneqq \frac{S}{2}$ hard chains. Let the two states in the $i$-th hard chain be $(s_1(i),s_2(i) )$. We set the initial distribution to be the uniform distribution over $\{s_1(i)\}_{i=1}^d$. Then $V_1^{\star}(s_1(i))=H$ holds for any $1\leq i\leq d$.  Let $\mathsf{Regret}_i(K)$ be the expected regret resulting from the $i$-th hard chain. 
When $K\geq 100S$,  Lemma~\ref{lemma:con} tells us that with probability at least $\frac{1}{2}$, $s_1(i)$ is visited for at least $\frac{K}{10S}\geq 10$ times. As a result, we have $$\mathsf{Regret}_i(K)\geq \frac{1}{2}\cdot \Omega\left(\frac{KH}{S}\right).$$  Summing over $i$, we see that the total regret is at least $\sum_{i=1}^d \mathsf{Regret}_i(K) = \Omega(KH)$. When $K<100S$, with probability at least $1-(1-\frac{1}{S})^K\geq 0.0001\frac{K}{S}$, we know that $s_1(i)$ is visited for at least one time. Therefore, it holds that $\mathsf{Regret}_i(K)\geq \Omega(\frac{KH}{S})$. 
	Summing over $i$, we obtain 
	%
	$$\mathsf{Regret}(K) = \sum_{i=1}^K \mathsf{Regret}_i(K) =\Omega(KH)$$
	as claimed. 
\end{proof}
