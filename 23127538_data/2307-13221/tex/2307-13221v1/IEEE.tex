
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



%\documentclass[journal,10pt,draftclsnofoot,onecolumn]{IEEEtran}
\documentclass[journal,10pt]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}


\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{subfigure}
\usepackage{overpic}
\usepackage{colortbl}
% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Multilevel Large Language Models for Everyone}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Yuanhao~Gong\\College of Electronics and Information Engineering, Shenzhen University, China.~~gong.ai@qq.com% <-this % stops a space
	\thanks{Manuscript received April 19, 2005; revised September 17, 2014.}
	}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Yuanhao: LLM on Blockchains}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Large language models have made significant progress in the past few years. However, they are either generic {\it or} field specific, splitting the community into different groups. In this paper, we unify these large language models into a larger map, where the generic {\it and} specific models are linked together and can improve each other, based on the user personal input and information from the internet. The idea of linking several large language models together is inspired by the functionality of human brain. The specific regions on the brain cortex are specific for certain low level functionality. And these regions can jointly work together to achieve more complex high level functionality. Such behavior on human brain cortex sheds the light to design the multilevel large language models that contain global level, field level and user level models. The user level models run on local machines to achieve efficient response and protect the user's privacy. Such multilevel models reduce some redundancy and perform better than the single level models. The proposed multilevel idea can be applied in various applications, such as natural language processing, computer vision tasks, professional assistant, business and healthcare.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
language model; neural network; multilevel; personal LLM
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\section{Introduction}
%%%%%%%%% BODY TEXT
Large language models (LLM) are a fascinating result of recent advancements in artificial intelligence technology. These models have the potential to transform communication and writing. With the latest technological advancements, large language models have become more sophisticated in imitating human language. They are trained on enormous amounts of text data, enabling them to produce coherent and relevant text on various topics. This technology has opened up a wide range of usage in fields such as business, education, and healthcare.

In recent years, several popular large language models have emerged, each with unique qualities designed to cater to different requirements and perform different functions. For instance, GPT-3 generates highly accurate and convincing text, making it suitable for applications such as generating conversational responses or writing articles~\cite{Brown2020}. On the other hand, T5 is versatile and can perform a wide range of natural language processing tasks, making it valuable in areas such as translation and question answering~\cite{Raffel2020}.

Apart from GPT-3 and T5, BERT is another popular large language model that has made significant contributions to the field of natural language processing~\cite{Devlin2019}. BERT excels at understanding the context of words and sentences, making it valuable for applications such as search engines and chatbots. By understanding the context of sentences, BERT can provide more accurate and relevant search results, and it helps chatbots to produce more natural and human-like responses.

Despite their differences, all of these models share a common goal: to advance the field of artificial intelligence and push the limits of what machines can do. With the continued development of large language models, we can expect to see even more exciting breakthroughs in artificial intelligence technology in the future.

% Figure environment removed

%Overall, while the trend towards larger and more complex neural networks presents challenges, there are also many opportunities for innovation and advancement in the field of artificial intelligence. The development of neural networks is an exciting field of research, and researchers are actively working to overcome the challenges posed by these models and unlock their full potential.
\subsection{Why Large Models?}
Large language models have transformed the field of natural language processing, allowing machines to understand and generate human-like language. These models learn from vast amounts of data and store and process more information due to their large number of parameters, enabling them to produce accurate, relevant, and contextually appropriate results. However, it is important to balance the number of parameters to prevent overfitting, which can reduce the model's overall performance. Additionally, training models with many parameters can be time-consuming, slowing down the training process.

Furthermore, large language models are versatile and can enhance various natural language processing tasks, such as language translation, sentiment analysis, and text summarization. By training on extensive data sets, these models can detect patterns and nuances in language that are difficult for humans to recognize. They are essential tools for applications such as virtual assistants, chatbots, and language understanding systems. Large language models can also generate human-like text, which has various applications in fields such as journalism, creative writing, and content generation. In conclusion, large language models are a powerful tool that can revolutionize how we interact with machines and generate human-like language, but it is important to balance the number of parameters and the model's performance for optimal results.
\subsection{Limitations}
Large language models have some limitations that need to be addressed. One such limitation is their potential to perpetuate biases present in their training data, which can lead to unfair and discriminatory outcomes. Additionally, these models have high computational and energy requirements, making them expensive to train and operate. Their tendency to generate unreliable or false information, particularly when dealing with complex or sensitive topics, is also problematic. Concerns have also been raised about the potential misuse of large language models for generating disinformation.

Another concern is the potential threat to privacy posed by large language models. Since these models require large amounts of data to train, they can potentially capture sensitive information. Therefore, it is important to develop privacy-preserving models that can ensure users' sensitive information is not compromised.

Furthermore, the environmental impact of running these models is also a concern. These models require significant amounts of energy to train and operate, leading to a significant carbon footprint. To mitigate this, researchers and developers are actively working to address these limitations and reduce the risks associated with large language models.

To address these limitations, researchers are developing more robust and diverse datasets for training. This can help reduce bias and enable models to produce more accurate results. Ethical guidelines are being implemented for model development and use to ensure that these models are used responsibly and do not cause harm. Finally, alternative approaches to language processing are being explored, such as using smaller models or leveraging other technologies like knowledge graphs to process language more efficiently.

%In conclusion, while large language models have some limitations, researchers and developers are working hard to address these issues and mitigate potential risks associated with these models.

\section{Multilevel Large Language Models}
The development of artificial intelligence has resulted in increasingly complex neural networks with more parameters. This has led to the creation of large language models such as GPT-3 and BERT, which have significantly impacted natural language processing.

However, a major challenge associated with these models is their high computational cost. Traditional large language models, like GPT-3 and BERT, require thousands of GPUs for training and deployment. This high cost is a significant obstacle for many researchers and organizations, limiting access to these powerful models.

Despite this challenge, advancements in hardware and software have made it possible to train and deploy these models on fewer GPUs. This has opened new opportunities for researchers and organizations to explore the capabilities of these models and leverage their potential in various applications.

Another problem with current LLM is that these models are static. In other words, they can not evolve to achieve higher performance with more and more user input. In fact, millions of users can help to improve the LLM in each specific field, such as poem, medical diagnosis and car design.

\subsection{Multilevel Models}
To tackle these issues, we propose a multilevel strategy. We propose to decompose the system into three levels: global level, field level and user level. Each level contains different LLM for different purpose. The full map is illustrated in Fig.~\ref{fig}. A simple comparison between these levels is shown in Table~\ref{table}.

At the global level, the LLM are trained using all available dataset. As a result, they are good at everything but not excellent at any specific fields. Letting users directly using these models is wasting the computation resource because the performance is just good, not excellent. However, such global LLM can help in improving the field LLM.

At the field level, the LLM are designed to be excellent in a specific field, such as poem, music, medical knowledge, and finance. These models correspond to the different professional career in human society such as lawyers and doctors. These models are excellent in a specific field in general, but not for any specific person.

At the user level, the LLM are trained to be an assistant with both personal information and professional advice from some specific field LLM. These models fill the gap between each individual user and the field LLM. Moreover, these models run on local machines to protect the user's privacy.  

\begin{table} 
	\centering \caption{Summary Multilevel LLM. }
	\begin{tabular}{c|c|c|c}
		\hline
		LLM & \# parameters & update period & inference time\\
		\hline
		global & large & long & long\\
		field & medium & medium & medium\\
		user & small & short & real-time\\
		\hline
	\end{tabular}
	\label{table}
\end{table}
\subsubsection{Global LLM}
The global LLM contain a very large parameters and are updated at low frequency. If nothing happens, they will automatically update themselves every week. However, if there are more information from the training data, they will update themselves with a shorter period.

Meanwhile, these global LLM are not designed to serve users. Instead, they are designed to update the field LLM that have specific domain knowledge and also smaller parameters. Therefore, users' response time is not affected by the long time training and inference processing from the global LLM.
\subsubsection{Field LLM}
The field LLM are designed to be professional in some field, such as art, music, object tracking, and education. They are automatically updated by the global LLM which receive new knowledge from the user input.

The field LLM can be considered as a buffer that exchanges information between the global LLM and the user LLM. Their parameter weights are updated by the global LLM. But they extract the information from the user LLM and feed the information into the global LLM. 

\subsubsection{User LLM}
At the user level, user LLM have much less parameters and also the inference time. Different from the global and field LLM, user LLM run on local machine and can contain each user's personal information such as movie and music preferences. Such user LLM can be considered as an excellent assistant who helps the user in planing, shopping, and working.

More importantly, these models are stored and encrypted on the user's local machine. Ideally, they will not betray the user. The user's privacy is protected.


\subsection{Dynamic Large Language Models}
Be aware that these models can evolve during users' usage. With more and more user input, the local LLM know more about the user's preference. Therefore, the local LLM can evolve to update themselves accordingly. They can also dynamically link themselves to several related field LLM because the user's input is also dynamically changing. Such dynamics affects the whole system, leading to dynamic LLM.  
\subsubsection{Learning during Using}
The multilevel large language models are designed to evolve during its usage. Thus, they are highly dynamic, especially with the user input. They are life-long learning systems, keeping evolving to understand the user from his/her history recording.
\subsubsection{Interaction between User and User LLM}
There is a direct interaction between users and users' LLM. The user input is dynamic and the user's preference might be also changing with time and experience. Such dynamics requires the user LLM quickly adapt to such change. 

To be adaptive, one way is to change the parameter or architecture in user LLM, based on the received users' input. The other way is to change the user LLM from the interaction with field LLM.
\subsubsection{Interaction between User LLM and Field LLM}
The user LLM and field LLM have a soft link. And such link can be changed dynamically when the user's preference is changed. For example, if the user's preference is changed from IT to finance, the user LLM will find such change and trigger a require to the field LLM. The field LLM will update the user LLM accordingly.

If the field LLM are updated, they will also update the user LLM to fresh the new knowledge into the user LLM. Such fresh frequency is medium to balance the model accuracy and the inference running time. 
\subsubsection{Interaction between Field LLM and Global LLM}
The field LLM can be updated by the user LLM and also the global LLM. Meanwhile, the global LLM can be updated by the field LLM that receive new information from the user LLM. The global LLM can be updated if there is new data set available.

Such interaction happens in a very low frequency because the global LLM are large and should be stable (not so adaptive to an individual user). In contrast, the user LLM are quickly adaptive to each user.
\section{Economical Ecosystem}
Our multilevel large language model (MLLM) is an innovative approach to training and deploying the LLM that offers significant benefits to both users and developers. By leveraging advanced computational techniques and sophisticated algorithms, the MLLM achieves unparalleled levels of accuracy and efficiency in natural language processing tasks. This translates to significant cost savings for developers, who can now train and deploy their models more quickly and easily than ever before. Meanwhile, users can enjoy a more seamless and intuitive experience when interacting with language-based applications and services, thanks to the MLLM's ability to understand and interpret natural language in a more nuanced and sophisticated way. With its cutting-edge technology and user-focused design, the MLLM is poised to revolutionize the field of natural language processing and usher in a new era of innovation and discovery.
\subsection{Users}
Users in the system have two roles. First, users are able to utilize the system's features, and as a result, they are required to pay a fee for the services rendered. The payment allows for continued access to the system and its benefits, including but not limited to increased productivity, streamlined processes, and enhanced data analysis capabilities. Additionally, regular payment ensures that the system can be maintained and further developed to meet the evolving needs of its users. Therefore, it is important to maintain a consistent payment schedule to ensure uninterrupted access to the system and its benefits.

Second, it is important to acknowledge the critical role that users play in the system. Users are valuable sources of input and integral to the system's success. Without their contributions, the system would lack quality and reliability. To encourage more high-quality contributors, incentivizing users to share their knowledge and expertise is an effective approach. One way to achieve this is by establishing a system of fair compensation for users, recognizing their efforts and contributions. This compensation can come in various forms, including monetary rewards, recognition, or exclusive access to features. By providing fair compensation, the system can attract a larger pool of motivated users who will contribute their best work, resulting in a more robust and reliable platform. Therefore, prioritizing the establishment of a fair compensation system for users is crucial, as it will ultimately benefit the system as a whole and contribute to its long-term success.
\subsection{Developers}
The developers can work on these three level LLM. They can develop the user LLM that can be more adaptive to each individual user. They can also develop field LLM, to make them more professional. They can work on global LLM to accelerate the model convergence and efficiency.

The developers also have two roles in this system. First, thanks to the extensive work on their LLM at these three levels, developers can earn money from their LLM. They are well-equipped to capitalize on their expertise and earn a lucrative income from their specialized model. With a deep understanding of the complexities of the field and the latest industry trends, they can confidently navigate the landscape and offer valuable services to a wide range of users. 

Second, they have to pay money for the (valuable) input from users. In order to obtain valuable input from their users, developers may pay a fee. This fee is necessary to ensure that the developer can continue to provide high-quality services and products that meet the needs and expectations of their customers. By compensating users for their input, developers can gain valuable insights that can help them improve existing products or develop new ones that better meet the needs of their target market. Additionally, this feedback can be used to identify areas for improvement in the developer's operations and customer service, which can lead to increased satisfaction and loyalty among existing customers and attract new ones.
\subsection{Dynamic Models}
The models in the system are not equal. Some are more important than others and thus have higher service fees. But this is also dynamic, which means that some important models might become important in the future. Therefore, all the price in the models are dynamic, according to their service quality and market demand. 

As a result, both users and developers have the potential to earn an income that is closely tied to the growth and progress of the system. As new technologies and innovations are introduced, users and developers have the opportunity to adapt and evolve LLM in order to stay competitive and continue to earn a sustainable income. This is different from the traditional economical mode for LLM, where the developers have to pay for the training and inference process first and then charge the users with their usage. 
\subsection{Blockchains}
From the hardware point of view, implementation of the proposed multilevel large language models is difficult. Each LLM requires a lot of GPUs to train. And there are many LLM in the multilevel large language models. The traditional way to train the system is unrealistic.

To tackle this issue, we propose to develop MLLM on blockchains, which have high computation performance, are decentralized and run parallel on all nodes. Interestingly, the decentralized mode in blockchains also fits the proposed local machine and server mode in the MLLM.

Each miner client in the blockchain can be considered as a user in the MLLM. Instead of only contributing the computation to the blockchain, the users also get services from the blockchain in the terms of computation and economy. Therefore the miner client and the blockchain charge each other according to their performance.

More importantly, the user's input is also considered as valuable resource that can improve the LLM at each level. Such economics is not considered in the traditional blockchains and traditional LLM. This novel idea will trigger the promotion of high quality input in the system and eventually benefit everyone in the ecosystem.

The system is still under developing and we will release the system in the future to accelerate the related research and practical applications. The economic mode behind MLLM can reduce the barrier for LLM development, making the developing easier and available for everyone. 

Nowadays, the mobile phones and chips become more and more powerful. They are likely the node in the blockchains. And both the users and developers should benefit from such devices and get higher quality services.
\section{Conclusion and Discussion}
In recent times, there has been a significant rise in the usage of large language models. These models are created to comprehend and analyze human language and are becoming increasingly popular in various industries such as natural language processing, sentiment analysis, machine translation, and speech recognition. With the increasing demand for these models, we can expect to see further developments in the field of artificial intelligence and natural language processing. These advancements will undoubtedly impact the way we interact with technology and communicate in the future.

Although the large language models have made significant progress in natural language processing, there are still some limitations that need to be addressed. One of the most important limitations is the computational power needed to train and run these models. In addition, while these models can generate coherent text, they often struggle with generating text that is both relevant and accurate. Another limitation is the lack of diversity in the training data used to create these models, which can lead to bias and inaccuracies in the generated text. Therefore, it is essential to continue exploring ways to improve these models, such as incorporating more diverse and representative training data and developing more powerful computational resources to train and run these models.

In this paper, we have introduced multilevel large language models that address some of the challenges posed by previous large language models. Specifically, our models incorporate innovative strategies for handling long-term dependencies, improving computational efficiency, and enhancing the accuracy of predictions. Overall, our work represents a significant step forward in the development of large language models and lays the groundwork for future research in this area.

The multilevel approach seeks to balance accuracy and running time. It is based on the concept that complex problems can be broken down into smaller, simpler sub-problems to achieve accurate results. By solving these sub-problems, the overall accuracy of the solution can be improved. However, creating too many sub-problems can increase the running time. The multilevel approach suggests that a balance can be struck by dividing the problem into an optimal number of sub-problems. This allows for the maintenance of accuracy while keeping the running time within acceptable limits. The multilevel approach has gained popularity in various fields, such as computer science, mathematics, and engineering. It has been successfully applied to problems such as image processing, signal analysis, and optimization. Overall, the multilevel approach is a promising strategy that can improve the accuracy of complex problem solutions while keeping the running time practical.
%Finally, blockchain can be used to support the decision-making process in AI systems. By creating a decentralized network of nodes that vote on decisions, we can ensure that decisions are made in a fair and transparent manner.

Our approach is a novel step forward in the development of next generation large language models. The multilevel methods also show a novel economic model for developing large language models. They can be adopted in various applications such as audio, vision and machine learning tasks \cite{chenouard:2014,gong2009symmetry,Lewis2019,zhao2023survey,Gong2012,Brown2020,gong2013a,Yu2019,Gong:2014a,Yin2019a,gong:phd,Yu2022a,gong:gdp,Guo2022,gong:cf,Zong2021,gong:Bernstein,Ezawa2023,Gong2017a,Tang2021a,Gong2018,Gong2018a,Yu2020,GONG2019329,Sancheti2022,Gong2019a,Tang2021,Gong2019,Yin2019b,Gong2022,Yin2020,Gong2020a,Jin2022,Gong2021,Tang2022,Gong2021a,Tang2022a,Tang2023,Gong2022,Tang2023a,Xu2023,Han2022,Scheurer2023,Zhang2023b}.
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,../IP}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{% Figure removed}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


