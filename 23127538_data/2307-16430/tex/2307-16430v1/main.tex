\documentclass{INTERSPEECH2023}

% 2023-01-06 modified by Simon King (Simon.King@ed.ac.uk)  

% **************************************
% *    DOUBLE-BLIND REVIEW SETTINGS    *
% **************************************
% Comment out \interspeechcameraready when submitting the 
% paper for review.
% If your paper is accepted, uncomment this to produce the
%  'camera ready' version to submit for publication.
\interspeechcameraready 


% **************************************
% *                                    *
% *      STOP !   DO NOT DELETE !      *
% *          READ THIS FIRST           *
% *                                    *
% * This template also includes        *
% * important INSTRUCTIONS that you    *
% * must follow when preparing your    *
% * paper. Read it BEFORE replacing    *
% * the content with your own work.    *
% **************************************

\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{graphicx}

\DeclareMathOperator*{\argmax}{arg\, max}
\DeclareMathOperator*{\std}{std}

\title{VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design}
\name{Jungil Kong, Jihoon Park, Beomjeong Kim,  Jeongmin Kim, Dohee Kong, Sangjin Kim}

\address{
  SK Telecom, South Korea}
\email{\texttt{\{jik876,batho2n,beomjeong.kim,jmkim94,dohee.kong,kimsangjin\}@sk.com}}

\begin{document}

\maketitle
 

\begin{abstract}
% 1000 characters. ASCII characters only. No citations.
Single-stage text-to-speech models have been actively studied recently, and their results have outperformed two-stage pipeline systems. Although the previous single-stage model has made great progress, there is room for improvement in terms of its intermittent unnaturalness, computational efficiency, and strong dependence on phoneme conversion. In this work, we introduce VITS2, a single-stage text-to-speech model that efficiently synthesizes a more natural speech by improving several aspects of the previous work. We propose improved structures and training mechanisms and present that the proposed methods are effective in improving naturalness, similarity of speech characteristics in a multi-speaker model, and efficiency of training and inference. Furthermore, we demonstrate that the strong dependence on phoneme conversion in previous works can be significantly reduced with our method, which allows a fully end-to-end single-stage approach.
\end{abstract}
\noindent\textbf{Index Terms}: Text to Speech, Speech Synthesis, VITS


\input{sections/introduction}
\input{sections/method}
\input{sections/experiments}
\input{sections/results}
\input{sections/conclusion}
\input{sections/acknowledgements}

% \ifinterspeechfinal
%      The INTERSPEECH 2023 organisers
% \else
%      The authors
% \fi
% would like to thank ISCA and the organising committees of past INTERSPEECH conferences for their help and for kindly providing the previous version of this template.

% As a final reminder, the 5th page is reserved exclusively for references. No other content must appear on the 5th page. Appendices, if any, must be within the first 4 pages. The references may start on an earlier page, if there is space.

\bibliographystyle{IEEEtran}
\bibliography{mybib}

\end{document}
