\section{Introduction}
\label{sec:Introduction}
Recent developments in deep neural network-based text-to-speech have seen significant advancements. Deep neural network-based text-to-speech is a method for generating corresponding raw waveforms from input texts; it has several interesting features that often make the text-to-speech task challenging. A quick review of the features reveals that the text-to-speech task involves converting text, which is a discontinuous feature, into continuous waveforms. The input and output have a time step difference of hundreds of times, and the alignment between them must be very precise to synthesize high-quality speech audio. Additionally, prosody and speaker characteristics not present in the input text should be expressed naturally and it is a one-to-many problem in which text input can be spoken in multiple ways. Another factor that makes synthesizing high-quality speech challenging is that humans focus on individual components when listening to an audio; therefore, even if a fraction of the hundreds of thousands of signals that constitute the entire audio are unnatural, humans can easily sense them. Efficiency is another factor that makes the task difficult. The synthesized audio has a substantial time resolution, which generally comprises more than 20,000 data per second, demanding highly efficient sampling methods.

Owing to the text-to-speech task features, the solution can also be sophisticated. Previous works have addressed these problems by dividing the process of generating waveforms from input texts into two cascaded stages. A popular method involves producing intermediate speech representations such as mel-spectrograms or linguistic features from the input texts in the first stage \cite{shen2018natural,li2019neural,ren2019fastspeech,kim2020glow,valle2020flowtron,popov2021grad,ren2021fastspeech} and then generating raw waveforms conditioned on those intermediate representations in the second stage \cite{oord2016wavenet,kalchbrenner2018efficient,prenger2019waveglow,kumar2019melgan,binkowski2019high,yamamoto2020parallel,kong2020hifi,chen2020wavegrad}.
Two-stage pipeline systems have the advantages of simplifying each model and facilitating training; however, they also have the following limitations.
1) Error propagation from the first stage to the second stage.
2) Rather than utilizing the learned representation inside the model, it is mediated through human-defined features such as mel-spectrogram or linguistic features.
3) Computation required to generate intermediate features.
Recently, to address these limitations, single-stage models that directly generate waveforms from input texts have been actively studied \cite{donahue2021endtoend, ren2021fastspeech, kim2021conditional, lim22_interspeech}.
The single-stage models not only outperformed the two-stage pipeline systems, but also showed an ability to generate high-quality speech nearly indistinguishable from humans.

Although the previous work \cite{kim2021conditional} has achieved great success with the single-stage approach, the model \cite{kim2021conditional} has the following problems: intermittent unnaturalness, low efficiency of the duration predictor, complex input format to alleviate the limitations of alignment and duration modeling (use of blank token), insufficient speaker similarity in the multi-speaker model, slow training, and strong dependence on the phoneme conversion.
In this work, we provide methods to address these problems.
We propose a stochastic duration predictor trained through adversarial learning, normalizing flows improved by utilizing the transformer block and a speaker-conditioned text encoder to model multiple speakers' characteristics better. We confirm that the proposed methods improve quality and efficiency. Furthermore, we show that the methods reduce the dependency on the phoneme conversion through the experiment using normalized texts as the input of the model. 
Thus, the methods move closer to a fully end-to-end single-stage approach.
