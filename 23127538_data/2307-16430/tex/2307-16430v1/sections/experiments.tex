\section{Experiments}
\label{sec:Experiments}
We conducted experiments on two different datasets. We used the LJ Speech dataset~\cite{ljspeech17} to confirm the improvement in naturalness and the VCTK dataset~\cite{veaux2017cstr} to verify whether our model could reproduce speaker characteristics better. 
The LJ Speech dataset consists of 13,100 short audio clips of a single speaker with a total length of approximately 24 hours. The audio format is 16-bit PCM with a sample rate of 22.05 kHz, and we used it without any manipulation. We randomly split the dataset into a training set (12,500 samples), validation set (100 samples), and test set (500 samples). The VCTK dataset consists of approximately 44,000 short audio clips uttered by 109 native English speakers with various accents. The total length of the audio clips is approximately 44 hours. The audio format is 16-bit PCM with a sample rate of 44.1 kHz. We reduced the sample rate to 22.05 kHz. We randomly split the dataset into a training set (43,470 samples), validation set (100 samples), and test set (500 samples).

We used 80 bands mel-scale spectrograms for calculating the reconstruction loss. In contrast with the previous work~\cite{kim2021conditional}, we used the same spectrograms as the input of the posterior encoder. The fast Fourier transform, window, and hop sizes were set to 1024, 1024, and 256, respectively.

We conducted experiments using both phoneme sequences and normalized texts as the input of the model. 
We converted text sequences into International Phonetic Alphabet sequences using open-source software~\cite{phonemizer20} and fed the text encoder with the sequences. Contrasting with the previous work~\cite{kim2021conditional}, we did not use the blank token. For the experiment with normalized texts, we normalized the input text with simple rules using open-source software~\cite{keithitotacotron} and fed the text encoder with it.

The networks were trained using the AdamW~\cite{loshchilov2018decoupled} optimizer with $\beta_1 = 0.8$, $\beta_2 = 0.99$, and weight decay $\lambda = 0.01$. The learning rate decay was scheduled by a $0.999^{1/8}$ factor in every epoch, with an initial learning rate of $2\times10^{-4}$. We fed the networks with 256 training instances per step.
Following the previous work~\cite{kim2021conditional}, the windowed generator training was applied. We used mixed precision training on four NVIDIA V100 GPUs. The networks to generate waveforms and the duration predictor were trained up to 800k and 30k steps, respectively.
