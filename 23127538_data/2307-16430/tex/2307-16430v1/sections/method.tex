\section{Method}
\label{sec:Method}
In this section, we describe improvements in four subsections: duration prediction, augmented variational autoencoder with normalizing flows, alignment search, and speaker-conditioned text encoder. We propose a method that uses adversarial learning to train the duration predictor to synthesize natural speech with high efficiency in both training and synthesis. Our model essentially learns alignments using the Monotonic Alignment Search (MAS) proposed in the previous work~\cite{kim2020glow, kim2021conditional}, and we further suggest a modification to improve the quality. In addition, we propose a method to improve naturalness by introducing the transformer block into the normalizing flows, which enables capturing long-term dependencies when transforming the distribution. Furthermore, we modify the speaker conditioning to improve the speaker similarity in a multi-speaker model.

% Figure environment removed

\subsection{Stochastic Duration Predictor with Time Step-wise Conditional Discriminator}
\label{ssec:mehtod_sdpadv}
The previous work~\cite{kim2021conditional} has shown that the flow-based stochastic duration predictor is more effective in improving the naturalness of synthesized speech than the deterministic approach. It showed great results; however, the flow-based method requires relatively more computations and some sophisticated techniques. We propose a stochastic duration predictor with adversarial learning to synthesize more natural speech with higher efficiency in both training and synthesis than the previous work~\cite{kim2021conditional}. The overview of the proposed duration predictor and discriminator is shown in Figure~\ref{fig:dp}. We apply adversarial learning to train the duration predictor with a conditional discriminator that is fed the same input as the generator to appropriately discriminate the predicted duration. 
We use the hidden representation of the text $h_{text}$ and Gaussian noise $z_{d}$ as the input of the generator $G$; and the $h_{text}$ and duration obtained using MAS in the logarithmic scale denoted as $d$ or predicted from the duration predictor denoted as $\hat{d}$, are used as the input of the discriminator $D$.
Discriminators of general generative adversarial networks are fed inputs of a fixed length, whereas the duration for each input token is predicted, and the length of the input sequence varies for each training instance. To properly discriminate the inputs of variable length, we propose a time step-wise discriminator that discriminates each of the predicted durations of all tokens.
We use two types of losses; the least-squares loss function~\cite{mao2017least} for adversarial learning and the mean squared error loss function:
\begin{flalign}
    L_{adv}(D) &= \mathbb{E}_{(d,z_{d},h_{text})}\Big[(D(d,h_{text})-1)^2 \notag \\& 
    \quad\quad\quad\quad \ +(D(G(z_{d},h_{text}),h_{text}))^2\Big], \\
    L_{adv}(G) &= \mathbb{E}_{(z_{d},h_{text})}\Big[(D(G(z_{d},h_{text}))-1)^2\Big], \\
    \vspace{1cm} 
    L_{mse} &= MSE(G(z_{d},h_{text}), d) 
\end{flalign}
Our proposed duration predictor and training mechanism allow for a learning duration in short steps, and the duration predictor is separately trained as the last training step, which reduces the overall computation time for training.


\subsection{Monotonic Alignment Search with Gaussian Noise}
\label{ssec:mehtod_maswn}
Following the previous work~\cite{kim2020glow, kim2021conditional}, we introduce MAS into our model to learn the alignment. 
The algorithm yields the alignment between text and audio that has the highest probability among all possible monotonic alignments, and the model is trained to maximize its probability. The method is efficient; however, after searching and optimizing a particular alignment, it is limited in exploration to search for other alignments that are more appropriate.
To mitigate this, we add a small Gaussian noise to the calculated probabilities. This gives the model extra opportunities to search for other alignments. We only add this noise at the beginning of training because MAS enables the model to learn the alignments quickly. Referring to a previous work~\cite{kim2020glow}, which described the algorithm in detail, $Q$ values have the maximum log-likelihood calculated for all possible positions in the forward operation. We add small Gaussian noise $\epsilon$ to the calculated $Q$ values in the operation.
\begin{align}
P_{i,j} & = {\log \mathcal{N}(z_j; \mu_{i}, \sigma_{i}}) &&\\
Q_{i,j} & = \max_{A}{\sum_{k=1}^{j}{\log \mathcal{N}(z_k; \mu_{A(k)}, \sigma_{A(k)}}}) \notag &&\\
&= \max(Q_{i-1, j-1}, Q_{i, j-1}) + P_{i,j}+\epsilon 
% \epsilon & = {n \times \std(P) \times s}
\end{align}
where $i$ and $j$ denote a specific position on the input sequence and posterior, respectively, $z$ represents transformed latent variables from the normalizing flows. $\epsilon$ is obtained as the product of noise sampled from the standard normal distribution, the standard deviation of $P$, and the noise scale starting at 0.01 and decreasing by $2\times10^{-6}$ for every step.

\subsection{Normalizing Flows using Transformer Block}
\label{ssec:mehtod_nftr}
The previous work~\cite{kim2021conditional} demonstrated the capability of the variational autoencoder augmented with normalizing flows to synthesize high-quality speech audio. The normalizing flows comprise convolution blocks, which are effective structures for capturing the patterns of adjacent data and enabling the model to synthesize high-quality speech. The ability to capture long-term dependencies can be crucial when transforming distribution because each part of the speech is related to other parts that are not adjacent. Although a convolution block captures adjacent patterns effectively, it has a disadvantage in capturing long-term dependencies owing to the limitations of its receptive field. Therefore, we add a small transformer block with the residual connection into the normalizing flows to enable the capturing of long-term dependencies, as shown in Figure~\ref{fig:nf}. Figure~\ref{fig:nf_attn} shows an actual attention score map and the receptive field of the convolution block. We can confirm that the transformer block collects information at various positions when transforming the distribution, which is impossible with the receptive field. 

\subsection{Speaker-Conditioned Text Encoder}
\label{ssec:mehtod_scte}
Because the multi-speaker model is to synthesize speech in multiple characteristics according to the speaker condition with one single model, expressing individual speech characteristics of each speaker is an important quality factor as well as naturalness. The previous work showed that the single-stage model can model multiple speakers with high quality. Considering some features, such as a speakerâ€™s particular pronunciation and intonation, significantly influences the expression of the speech characteristics of each speaker but are not contained in the input text, we design a text encoder conditioned with the speaker information to better mimic various speech characteristics of each speaker by learning the features while encoding the input text. We condition the speaker vector on the third transformer block of the text encoder, as shown in Figure~\ref{fig:scte}.

% Figure environment removed

