\section{Results}
\label{sec:Results}
\subsection{Evaluation of Naturalness}
\label{ssec:results_mos}
To confirm that the proposed model synthesizes natural speech, crowdsourced mean opinion score (MOS) tests were conducted. Raters rated their naturalness on a 5-point scale from 1 to 5 after listening to randomly selected audio samples from the test sets. Considering that the previous work~\cite{kim2021conditional} has already demonstrated similar quality to human recordings, we also conducted a comparative mean opinion score (CMOS) test, which is appropriate for evaluating high-quality samples by direct comparison. Raters rated their relative preference in terms of naturalness on a 7-point scale from 3 to -3 after listening to randomly selected audio samples from the test sets.\footnote{Demo: \href{https://vits-2.github.io/demo/}{https://vits-2.github.io/demo/}}
Raters were allowed to evaluate each audio sample once. All audio samples were normalized to avoid the effect of amplitude differences on the score. We used the official implementation and pre-trained weights of the previous work~\cite{kim2021conditional} as the comparison model. The evaluation results are presented in Table~\ref{tab:mos} and Table~\ref{tab:cmos}. The MOS difference between our method and the previous work~\cite{kim2021conditional} was 0.09, and the CMOS and confidence interval were 0.201 and $\pm$0.105, respectively. The results demonstrate that the our method significantly improves the quality of synthesized speech. Additionally, we evaluated CMOS with the method~\cite{lim22_interspeech} that showed good performance using different structures and training mechanisms. For evaluation, we generated samples using the official implementation and pre-trained weights. The CMOS and confidence intervals of the evaluation are 0.176 and $\pm$0.125, respectively, indicating that our method significantly outperforms the method.


\subsection{Ablation Studies}
\label{ssec:ablation_studies}
Ablation studies were also conducted to verify the validity of the proposed methods. To verify the validity of the stochastic duration predictor trained with adversarial learning, it was substituted with the deterministic duration predictor that had the same structure and was trained with L2 loss. The deterministic duration predictor was trained up to the same steps as the previous work~\cite{kim2021conditional}. To verify the efficacy of the noise scheduling used in the alignment search, the model was trained without the noise. We trained the model without the transformer block in the normalizing flows to verify its effectiveness. The evaluation results are presented in Table~\ref{tab:mos}. The MOS differences of the ablation studies on the deterministic duration predictor, alignment search without the noise, and normalizing flows without the transformer block are 0.14, 0.15, and 0.06, respectively. As we do not use the blank token and linear spectrogram, the computational efficiency would be improved, and removing some of the proposed methods shows lower performance compared with the previous work~\cite{kim2021conditional}. The results show that the proposed methods are effective in improving the quality.

\begin{table}[t]
  \caption{Comparison of MOS of the proposed model, the previous work, and the ablation studies on the LJ Speech dataset with 95\% confidence intervals. }
  \label{tab:mos}
  \vspace{-0.8\baselineskip}
  \begin{center}
  \vskip -0.1in
  \begin{tabular}[htbp]{lc}
    \toprule
    Model   & 
    MOS (CI) \\
    \midrule
    Ground Truth    & 4.43 ($\pm$0.06) \\
    \midrule
    VITS    & 4.38 ($\pm$0.06) \\
    \midrule
    VITS2    & \textbf{4.47} ($\pm$0.06) \\
    \,\, w/o \scalebox{.86}[1.0]{Adversarial Learning for Duration Predictor} & 4.33 ($\pm$0.07) \\
    \,\, w/o Alignment Noise & 4.32 ($\pm$0.07) \\
    \,\, w/o Transformer Block & 4.41 ($\pm$0.07) \\
    \bottomrule
  \end{tabular}
  \end{center}
  % \vskip -0.2in
  \vspace{-0.5\baselineskip}
\end{table}

\begin{table}[t]
    \caption{(a) Comparative MOS of the proposed model and the previous works on the LJ Speech dataset with 95\% confidence intervals. (b) Comparison of similarity MOS of the proposed model and the previous work on the VCTK dataset with 95\% confidence intervals .}
    \centering
    \hskip -0.1in
    \vspace{-0.3\baselineskip}
    \subfloat[Comparative MOS]{%
      \label{tab:cmos}
      \vspace{-1.2\baselineskip}
      % \begin{center}
      \begin{tabular}[htbp]{lcc}
        \toprule
        Model   & 
        CMOS   & 
        CI \\
        \midrule
        VITS & \textbf{0.201} & $\pm$0.105 \\
        JETS & \textbf{0.176} & $\pm$0.125 \\
        \bottomrule
      \end{tabular}
      % \end{center}
      }
  \quad
    \subfloat[Similarity MOS]{%
      \label{tab:smos}
      % \begin{center}
      \begin{tabular}[htbp]{lc}
        \toprule
        Model   & 
        MOS (CI) \\
        \midrule
        VITS    & 3.79 ($\pm$0.09) \\
        VITS2    & \textbf{3.99} ($\pm$0.08) \\
        \bottomrule
      \end{tabular}
      % \end{center}
      }
      \vskip -0.01in
  \hspace{0.3cm}
\end{table}


\subsection{Evaluation of Speaker Similarity}
\label{ssec:results_smos}
To confirm the improvement in speaker similarity in the multi-speaker model, similarity MOS tests similar to the previous work~\cite{jia2018transfer} were conducted through crowdsourcing. In the test, randomly sampled human recorded audio from the test set was presented as a reference, and raters scored the similarity between the reference and the corresponding synthesized audio on a five-point scale from 1 to 5. As in section 4.1, raters were allowed to evaluate each audio sample once, and the audio samples were normalized.
The evaluation results are presented in Table~\ref{tab:smos}. VITS2 was rated 0.2 MOS higher than the previous work~\cite{kim2021conditional}, which shows the effectiveness of our method in improving speaker similarity when modeling multiple speakers.

\subsection{Reduced dependency on the phoneme conversion}
\label{ssec:results_phoneme}
Previous works~\cite{kim2021conditional,tan2022naturalspeech} have shown good performance with single-stage approaches but continue to have a strong dependence on phoneme conversion. Because normalized text does not inform its actual pronunciation, it makes learning accurate pronunciations challenging. It is currently a crucial barrier to achieving a fully end-to-end single-stage speech synthesis. We present that our method significantly improves this problem through intelligibility tests. After transcribing 500 synthesized audio in the test set using Google's automatic speech recognition API, we calculated the character error rate (CER) with the ground truth text as the reference. We compared the results of the following four models with the ground truth: the proposed model using phoneme sequences, the proposed model using normalized texts, the previous work using phoneme sequences, and the previous work using normalized texts. Table~\ref{tab:intelligibility_test} presents the comparison,
which confirms that not only the proposed model outperforms the previous work, but also the performance of our model using normalized texts is comparable to that of the model using phoneme sequences. It demonstrates the possibility of a data-driven, fully end-to-end approach.


\begin{table}[t]
  \caption{Comparison of the intelligibility tests on the LJ Speech dataset.}
  \label{tab:intelligibility_test}
  \vspace{-0.5\baselineskip}
  \begin{center}
  \vskip -0.1in
  \begin{tabular}{lccc}
    \toprule
    Model\hspace{4.5cm}   & 
    CER \\
    \midrule
    Ground Truth &  4.91 \\
    \midrule
    VITS with Phoneme Sequences &  4.26 \\
    VITS with Normalized Texts &  5.07 \\
    \midrule
    VITS2 with Phoneme Sequences &  \textbf{3.92} \\
    VITS2 with Normalized Texts &  \textbf{4.01} \\
    \bottomrule
  \end{tabular}
  \vspace{-0.5\baselineskip}
  \end{center}
\end{table}


\subsection{Comparison of Synthesis and Training Speed}
\label{ssec:results_synspd}
We compared our model's synthesis and training speed with those of the previous work~\cite{kim2021conditional}. We measured the synchronized elapsed time over the entire process to generate raw waveforms from input sequences with 500 sentences randomly selected from the LJ Speech dataset. We used a single NVIDIA V100 GPU with a batch size of 1. We also measured and averaged the elapsed time for the training computation of each step for five epochs on four NVIDIA V100 GPUs. Table~\ref{tab:sspeed} shows the results. 
As the duration predictor is more efficient and can be trained separately and the input sequences are shorter than in the previous work, its training and synthesis speed are improved; the improvements are 20.5\% and 22.7\%, respectively.

\newcommand{\mlcell}[2][p{2cm}c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\begin{table}[t]
  \caption{Comparison of the synthesis and training speed. The columns '$n$~kHz' and 'Real-time' of 'Synthesis' column denote the model's ability to generate $n\times1000$ raw audio samples per second and the synthesis speed over real-time, respectively. The 'sec/step' value of 'Training' column denotes the average elapsed time for the computation of training per step.}
  \label{tab:sspeed}
  \vspace{-0.5\baselineskip}
  \begin{center}
  \vskip -0.1in
  \begin{tabular}{lccc}
    \toprule
    Model\hspace{0.8cm}   & 
    \multicolumn{2}{c}{\mlcell{Synthesis\\ \hspace{0.18em} kHz \quad\quad Real-time}}\hspace{0.5em} & 
    \mlcell{Training\\sec/step}\hspace{0.1em} \\
    \midrule
    VITS & 1,779 & $\times$80.68  & 1.227  \\
    VITS2 & \textbf{2,144} & \textbf{$\times$97.25}  & \textbf{0.951}  \\
    \bottomrule
  \end{tabular}
  \vspace{-0.5\baselineskip}
  \end{center}
  \vskip -0.15in
\end{table}
