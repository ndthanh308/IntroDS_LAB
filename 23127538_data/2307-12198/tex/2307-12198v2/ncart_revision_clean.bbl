\begin{thebibliography}{10}
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
\expandafter\ifx\csname href\endcsname\relax
  \def\href#1#2{#2} \def\path#1{#1}\fi

\bibitem{breiman2001random}
L.~Breiman, Random forests, Machine learning 45 (2001) 5--32.

\bibitem{chen2016xgboost}
T.~Chen, C.~Guestrin, Xgboost: A scalable tree boosting system, in: Proceedings
  of the 22nd acm sigkdd international conference on knowledge discovery and
  data mining, 2016, pp. 785--794.

\bibitem{ke2017lightgbm}
G.~Ke, Q.~Meng, T.~Finley, T.~Wang, W.~Chen, W.~Ma, Q.~Ye, T.-Y. Liu, Lightgbm:
  A highly efficient gradient boosting decision tree, Advances in neural
  information processing systems 30 (2017).

\bibitem{prokhorenkova2018catboost}
L.~Prokhorenkova, G.~Gusev, A.~Vorobev, A.~V. Dorogush, A.~Gulin, Catboost:
  unbiased boosting with categorical features, Advances in neural information
  processing systems 31 (2018).

\bibitem{lecun2015deep}
Y.~LeCun, Y.~Bengio, G.~Hinton, Deep learning, nature 521~(7553) (2015)
  436--444.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, J.~Sun, Deep residual learning for image recognition,
  in: Proceedings of the IEEE conference on computer vision and pattern
  recognition, 2016, pp. 770--778.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, I.~Polosukhin, Attention is all you need, Advances in neural
  information processing systems 30 (2017).

\bibitem{goodfellow2020generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, Y.~Bengio, Generative adversarial networks, Communications of
  the ACM 63~(11) (2020) 139--144.

\bibitem{borisov2022deep}
V.~Borisov, T.~Leemann, K.~Se{\ss}ler, J.~Haug, M.~Pawelczyk, G.~Kasneci, Deep
  neural networks and tabular data: A survey, IEEE Transactions on Neural
  Networks and Learning Systems (2022).

\bibitem{gorishniy2021revisiting}
Y.~Gorishniy, I.~Rubachev, V.~Khrulkov, A.~Babenko, Revisiting deep learning
  models for tabular data, Advances in Neural Information Processing Systems 34
  (2021) 18932--18943.

\bibitem{grinsztajn2022tree}
L.~Grinsztajn, E.~Oyallon, G.~Varoquaux, Why do tree-based models still
  outperform deep learning on typical tabular data?, in: Thirty-sixth
  Conference on Neural Information Processing Systems Datasets and Benchmarks
  Track, 2022.

\bibitem{yang2018deep}
Y.~Yang, I.~G. Morillo, T.~M. Hospedales, Deep neural decision trees, arXiv
  preprint arXiv:1806.06988 (2018).

\bibitem{popov2019neural}
S.~Popov, S.~Morozov, A.~Babenko, Neural oblivious decision ensembles for deep
  learning on tabular data, arXiv preprint arXiv:1909.06312 (2019).

\bibitem{katzir2021net}
L.~Katzir, G.~Elidan, R.~El-Yaniv, Net-dnf: Effective deep modeling of tabular
  data, in: International Conference on Learning Representations, 2021.

\bibitem{zantedeschi2021learning}
V.~Zantedeschi, M.~Kusner, V.~Niculae, Learning binary decision trees by argmin
  differentiation, in: International Conference on Machine Learning, PMLR,
  2021, pp. 12298--12309.

\bibitem{arik2021tabnet}
S.~{\"O}. Arik, T.~Pfister, Tabnet: Attentive interpretable tabular learning,
  in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol.~35,
  2021, pp. 6679--6687.

\bibitem{huang2020tabtransformer}
X.~Huang, A.~Khetan, M.~Cvitkovic, Z.~Karnin, Tabtransformer: Tabular data
  modeling using contextual embeddings, arXiv preprint arXiv:2012.06678 (2020).

\bibitem{somepalli2021saint}
G.~Somepalli, M.~Goldblum, A.~Schwarzschild, C.~B. Bruss, T.~Goldstein, Saint:
  Improved neural networks for tabular data via row attention and contrastive
  pre-training, arXiv preprint arXiv:2106.01342 (2021).

\bibitem{kossen2021self}
J.~Kossen, N.~Band, C.~Lyle, A.~N. Gomez, T.~Rainforth, Y.~Gal, Self-attention
  between datapoints: Going beyond individual input-output pairs in deep
  learning, Advances in Neural Information Processing Systems 34 (2021)
  28742--28756.

\bibitem{hollmann2022tabpfn}
N.~Hollmann, S.~M{\"u}ller, K.~Eggensperger, F.~Hutter, Tabpfn: A transformer
  that solves small tabular classification problems in a second, arXiv preprint
  arXiv:2207.01848 (2022).

\bibitem{chen2023excelformer}
J.~Chen, J.~Yan, D.~Z. Chen, J.~Wu, Excelformer: A neural network surpassing
  gbdts on tabular data, arXiv preprint arXiv:2301.02819 (2023).

\bibitem{yan2023t2g}
J.~Yan, J.~Chen, Y.~Wu, D.~Z. Chen, J.~Wu, T2g-former: Organizing tabular
  features into relation graphs promotes heterogeneous feature interaction, in:
  Proceedings of the AAAI Conference on Artificial Intelligence, Vol.~37, 2023,
  pp. 10720--10728.

\bibitem{zheng2023deep}
Q.~Zheng, Z.~Peng, Z.~Dang, L.~Zhu, Z.~Liu, Z.~Zhang, J.~Zhou, Deep tabular
  data modeling with dual-route structure-adaptive graph networks, IEEE
  Transactions on Knowledge and Data Engineering 01 (2023) 1--13.

\bibitem{shavitt2018regularization}
I.~Shavitt, E.~Segal, Regularization learning networks: deep learning for
  tabular datasets, Advances in Neural Information Processing Systems 31
  (2018).

\bibitem{cheng2016wide}
H.-T. Cheng, L.~Koc, J.~Harmsen, T.~Shaked, T.~Chandra, H.~Aradhye,
  G.~Anderson, G.~Corrado, W.~Chai, M.~Ispir, et~al., Wide \& deep learning for
  recommender systems, in: Proceedings of the 1st workshop on deep learning for
  recommender systems, 2016, pp. 7--10.

\bibitem{guo2017deepfm}
H.~Guo, R.~Tang, Y.~Ye, Z.~Li, X.~He, Deepfm: a factorization-machine based
  neural network for ctr prediction, arXiv preprint arXiv:1703.04247 (2017).

\bibitem{ke2019deepgbm}
G.~Ke, Z.~Xu, J.~Zhang, J.~Bian, T.-Y. Liu, Deepgbm: A deep learning framework
  distilled by gbdt for online prediction tasks, in: Proceedings of the 25th
  ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining,
  2019, pp. 384--394.

\bibitem{wang2021dcn}
R.~Wang, R.~Shivanna, D.~Cheng, S.~Jain, D.~Lin, L.~Hong, E.~Chi, Dcn v2:
  Improved deep \& cross network and practical lessons for web-scale learning
  to rank systems, in: Proceedings of the web conference 2021, 2021, pp.
  1785--1797.

\bibitem{sun2019supertml}
B.~Sun, L.~Yang, W.~Zhang, M.~Lin, P.~Dong, C.~Young, J.~Dong, Supertml:
  Two-dimensional word embedding for the precognition on structured tabular
  data, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops, 2019, pp. 0--0.

\bibitem{yin2020tabert}
P.~Yin, G.~Neubig, W.-t. Yih, S.~Riedel, Tabert: Pretraining for joint
  understanding of textual and tabular data, arXiv preprint arXiv:2005.08314
  (2020).

\bibitem{ke2018tabnn}
G.~Ke, J.~Zhang, Z.~Xu, J.~Bian, T.-Y. Liu, Tabnn: A universal neural network
  solution for tabular data (2018).

\bibitem{chen2022danets}
J.~Chen, K.~Liao, Y.~Wan, D.~Z. Chen, J.~Wu, Danets: Deep abstract networks for
  tabular data classification and regression, in: Proceedings of the AAAI
  Conference on Artificial Intelligence, Vol.~36, 2022, pp. 3930--3938.

\bibitem{chen2022tabcaps}
J.~Chen, K.~Liao, Y.~Fang, D.~Chen, J.~Wu, Tabcaps: A capsule neural network
  for tabular data classification with bow routing, in: The Eleventh
  International Conference on Learning Representations, 2022.

\bibitem{yoon2020vime}
J.~Yoon, Y.~Zhang, J.~Jordon, M.~van~der Schaar, Vime: Extending the success of
  self-and semi-supervised learning to tabular domain, Advances in Neural
  Information Processing Systems 33 (2020) 11033--11043.

\bibitem{erickson2020autogluon}
N.~Erickson, J.~Mueller, A.~Shirkov, H.~Zhang, P.~Larroy, M.~Li, A.~Smola,
  Autogluon-tabular: Robust and accurate automl for structured data, arXiv
  preprint arXiv:2003.06505 (2020).

\bibitem{suganthan2018non}
P.~N. Suganthan, On non-iterative learning algorithms with closed-form
  solution, Applied Soft Computing 70 (2018) 1078--1082.

\bibitem{pao1994learning}
Y.-H. Pao, G.-H. Park, D.~J. Sobajic, Learning and generalization
  characteristics of the random vector functional-link net, Neurocomputing
  6~(2) (1994) 163--180.

\bibitem{shi2021random}
Q.~Shi, R.~Katuwal, P.~N. Suganthan, M.~Tanveer, Random vector functional link
  neural network based ensemble deep learning, Pattern Recognition 117 (2021)
  107978.

\bibitem{shi2022weighting}
Q.~Shi, M.~Hu, P.~N. Suganthan, R.~Katuwal, Weighting and pruning based
  ensemble deep random vector functional link network for tabular data
  classification, Pattern Recognition 132 (2022) 108879.

\bibitem{rokach2005decision}
L.~Rokach, O.~Maimon, Decision trees, Data mining and knowledge discovery
  handbook (2005) 165--192.

\bibitem{martins2016softmax}
A.~Martins, R.~Astudillo, From softmax to sparsemax: A sparse model of
  attention and multi-label classification, in: International conference on
  machine learning, PMLR, 2016, pp. 1614--1623.

\bibitem{peters-etal-2019-sparse}
B.~Peters, V.~Niculae, A.~F.~T. Martins, Sparse sequence-to-sequence models,
  in: Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics, Association for Computational Linguistics, 2019,
  pp. 1504--1519.

\bibitem{veit2016residual}
A.~Veit, M.~J. Wilber, S.~Belongie, Residual networks behave like ensembles of
  relatively shallow networks, Advances in neural information processing
  systems 29 (2016).

\bibitem{kingma2014adam}
D.~P. Kingma, J.~Ba, Adam: A method for stochastic optimization, arXiv preprint
  arXiv:1412.6980 (2014).

\bibitem{zhang2020gbdt}
Z.~Zhang, C.~Jung, Gbdt-mo: gradient-boosted decision trees for multiple
  outputs, IEEE transactions on neural networks and learning systems 32~(7)
  (2020) 3156--3167.

\end{thebibliography}
