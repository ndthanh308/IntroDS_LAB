\documentclass[preprint,12pt]{elsarticle}
% \documentclass[preview]{elsarticle}
%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}


\usepackage{amssymb}
% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\usepackage{lineno}
% The lineno packages adds line numbers. Start line numbering with
%\usepackage{ulem}
\usepackage{bm}
% \usepackage[hidelinks]{hyperref}   
\usepackage[colorlinks=true]{hyperref}
\usepackage{wrapfig}
\usepackage{booktabs} 
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathptmx}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage{colortbl}
\usepackage{subfigure}
\usepackage{indentfirst}
\usepackage{ulem}
\usepackage{url}
\usepackage{verbatim}
\usepackage{adjustbox}
\usepackage{wrapfig}
\graphicspath{{./images/}}


\journal{arXiv}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{NCART: Neural Classification and Regression Tree for Tabular Data}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author[first]{Jiaqi Luo}
\ead{jiaqi.luo@dukekunshan.edu.cn}
\author[first]{Shixin Xu}
\ead{shixin.xu@dukekunshan.edu.cn}
\affiliation[first]{organization={Data Science Research Center, Duke Kunshan University},%Department and Organization
            addressline={No.8 Duke Ave}, 
            city={Kunshan},
            postcode={215000}, 
            state={Jiangsu Province},
            country={China}}

\begin{abstract}

Deep learning models have become popular in the analysis of tabular data, as they address the limitations of decision trees and enable valuable applications like semi-supervised learning, online learning, and transfer learning. However, these deep-learning approaches often encounter a trade-off. On one hand, they can be computationally expensive when dealing with large-scale or high-dimensional datasets. On the other hand, they may lack interpretability and may not be suitable for small-scale datasets. In this study, we propose a novel interpretable neural network called Neural Classification and Regression Tree (NCART) to overcome these challenges. NCART is a modified version of Residual Networks that replaces fully-connected layers with multiple differentiable oblivious decision trees. By integrating decision trees into the architecture, NCART maintains its interpretability while benefiting from the end-to-end capabilities of neural networks. The simplicity of the NCART architecture makes it well-suited for datasets of varying sizes and reduces computational costs compared to state-of-the-art deep learning models. Extensive numerical experiments demonstrate the superior performance of NCART compared to existing deep learning models, establishing it as a strong competitor to tree-based models.

\end{abstract}

% %%Graphical abstract
% \begin{graphicalabstract}
% %% Figure removed
% \end{graphicalabstract}

% %%Research highlights
% \begin{highlights}
% \item Research highlight 1
% \item Research highlight 2
% \end{highlights}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
Tabular data \sep Neural Networks \sep Classification and Regression Tree
\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{s:intro}
Tabular data is ubiquitous across various fields and industries, representing a wide range of valuable information, including financial records, customer information, and scientific measurements. Its pervasive presence underscores the importance of accurate prediction in tabular data analysis. Such predictions empower businesses, researchers, and analysts to fully leverage the potential of these datasets and drive meaningful advancements in their respective domains.

Decision tree models are widely recognized as an effective machine learning technique, particularly well-suited for analyzing tabular data. They offer several advantages, including interpretability, handling both numerical and categorical data, ease of implementation, and scalability.
Decision trees are the basis for more advanced ensemble methods such as random forests \cite{breiman2001random} and gradient boost decision trees (GBDT) \cite{friedman2001greedy,chen2016xgboost,ke2017lightgbm,prokhorenkova2018catboost}.
Nevertheless, they suffer from a lack of flexibility, discontinuity and the inability to pre-trained models, online learning, and transfer learning, which limits their use.


The capabilities of deep learning \cite{lecun2015deep} in extracting high-level features, its end-to-end training capability, and overall flexibility have led to remarkable achievements in various domains, including image analysis \cite{he2016deep}, natural language processing \cite{vaswani2017attention}, and generative modeling \cite{goodfellow2020generative}.
In the context of modeling tabular data, deep learning has emerged as a promising approach to address the limitations of decision trees. Researchers have been actively exploring innovative techniques \cite{borisov2022deep} with the aim of overcoming the drawbacks of traditional methods and expanding the range of applications suitable for deep learning.


Recent studies have highlighted that deep learning models may not always outperform other machine learning methods, such as gradient boosting, when applied to tabular data \cite{gorishniy2021revisiting, grinsztajn2022tree, shwartz2022tabular}. The relatively inferior performance of deep learning models in this context can be attributed to various factors.
One reason is the limited availability of labeled training data. Deep learning models typically require large amounts of labeled data to effectively learn meaningful representations and generalize well. When training data is limited, the performance of deep learning models may be compromised.
Additionally, deep learning models often require a considerable amount of time to converge to a satisfactory performance level. The training process may involve numerous iterations and adjustments to model parameters, which can be time-consuming.
Interpretability remains a significant challenge in deep learning due to the complexity of these models. With their numerous layers and millions of parameters, deep learning models often act as black boxes, making it difficult to understand the decision-making process behind their predictions. This lack of interpretability can be a significant drawback, especially in clinical or financial applications where limited data sets and interpretability are crucial for informed decision-making and interventions.
%because, in real-life scenarios, dataset, time, and resources are limited when training a model and optimizing its hyperparameters.
% Furthermore, it is crucial for deep learning models to provide interpretability, especially in fields like medicine and finance where understanding the reasoning behind predictions is essential.


To tackle these challenges, we propose a novel tree-induced deep learning model called Neural Classification and Regression Tree (NCART). Drawing inspiration from the success of differentiable decision trees \cite{yang2018deep, popov2019neural, katzir2021net, zantedeschi2021learning} and ResNet \cite{gorishniy2021revisiting} in handling tabular data, our model combines the strengths of decision trees and deep learning.
Instead of using fully-connected layers, our model replaces them with a sum of several differentiable decision trees. This allows our model to capture interpretability similar to traditional tree models while benefiting from the end-to-end training capabilities of deep learning. Furthermore, the simplicity of the ResNet architecture ensures that our model achieves desirable results in a limited time.
Our proposed model is designed to work effectively with datasets of varying sizes and demonstrates competitive performance compared to other state-of-the-art machine learning methods. By integrating the superiority of decision trees with the computational power of deep learning, our model not only achieves good performance but also provides insights into the underlying patterns and relationships within tabular data.
Through this innovative combination, our NCRAT model bridges the gap between interpretability and performance in the analysis of tabular data, making it a promising approach for a wide range of applications.


The main contributions of this paper are summarized as follows:
\begin{itemize}
\item Proposal of NCART: We introduce NCART, a deep learning model that integrates differentiable decision trees into a fully-connected ResNet.
The model is applicable to datasets of varying scales while maintaining lower computational costs compared to existing deep learning-based methods.

\item Interpretable Model: NCART exhibits interpretability, generating feature importance similar to those produced by traditional decision tree models. This information is valuable for identifying the most influential features in the decision-making process and understanding the underlying patterns in the data.

\item Superior Performance: Through extensive experimentation, NCART demonstrates superior performance compared to state-of-the-art deep learning models. Furthermore, it remains competitive with decision tree models, showcasing its efficacy in tackling complex tasks.
\end{itemize}






\section{Related Work}
\label{s:rel}
In this section, we provide an overview of the concepts from previous research that are pertinent to deep learning for tabular data. We divide the deep learning modes into three categories according to the network structure, namely tree-induced networks, transformer-based networks, and other specialized models.
% For more research, we refer interested readers to the survey \cite{borisov2022deep}.

\subsection{Tree-induced Neural Networks}
Given the effectiveness of tree-based models for tabular data, researchers have made efforts to incorporate differentiable trees that combine the advantages of tree ensembles with gradient-based optimization of neural networks. Several notable approaches in this direction are summarized as follows:
\begin{itemize}
    % \item Soft Decision Tree: The Soft Decision Tree \cite{frosst2017distilling} is an interpretable network distilled from a pre-trained model. It offers improved generalization compared to direct training on the data, although its performance may be slightly inferior to the teacher network.
    \item Deep Neural Decision Trees (DNDT): Proposed by Yang et al. \cite{yang2018deep}, DNDT is a neural network-based tree model that is straightforward to implement. It incorporates a soft binning function for split decisions, enhancing interpretability. However, its scalability is limited in high-dimensional feature spaces due to the Kronecker product computation for binned features.
    \item Neural Oblivious Decision Ensembles (NODE): Inspired by CatBoost, NODE \cite{popov2019neural} employs oblivious decision trees to construct an ensemble model. It is a fully differentiable model that can handle both numerical and categorical features without requiring preprocessing. NODE has demonstrated superior performance compared to GBDT models across multiple datasets, albeit with higher computational costs.
    \item Net-DNF: Net-DNF \cite{katzir2021net} leverages the representation of decision trees as Boolean formulas, specifically disjunctive normal forms (DNF). By utilizing this insight, Net-DNF emulates the characteristics of GBDT and achieves comparable results to XGBoost on certain classification tasks.
    \item Binary Tree Learning: The authors propose a novel method \cite{zantedeschi2021learning} that combines Argmin differentiation and a mixed-integer programming model to simultaneously learn discrete and continuous parameters of the tree, which allow the users to leverage the generalization capabilities of stochastic gradient optimization for decision splits and principled tree purning. 
\end{itemize}

These approaches aim to combine the interpretability of decision trees with the optimization capabilities of deep learning models, providing promising alternatives for tackling tabular data challenges. 





% Given the circumstance that tree-based models have proven to be solid for tabular data, many efforts have included differentiable trees, which combine the advantages of tree ensembles with gradient-based optimization of neural networks.
% The Soft Decision Tree \cite{frosst2017distilling} is an explainable network distilled from a trained model. This model is interpretable and generalizes better than one trained directly on the data, but it tends to perform a little worse than the teacher network.
% Yang et al. \cite{yang2018deep} proposed Deep Neural Decision Trees (DNDT), a neural network-based tree model that is easy to implement. The method utilizes a soft binning function for split decisions, which enhances interpretability. However, the model's scalability is limited when the feature dimension is high due to the output being the Kronecker product of the binned features.
% Neural Oblivious Decision Ensembles (NODE) \cite{popov2019neural} is inspired by CatBoost and employs oblivious decision trees to construct an ensemble model. This fully differentiable model can handle both numerical and categorical features without requiring any preprocessing. NODE has demonstrated superior performance compared to GBDT models across multiple datasets, albeit at the expense of high computational costs.
% Net-DNF \cite{katzir2021net} leverages the insight that decision trees can be represented as Boolean formulas, specifically disjunctive normal forms (DNF). By utilizing this knowledge, Net-DNF emulates the characteristics of GBDT and can achieve comparable results to XGBoost on some classification tasks. 
%% However, the approach does not address the handling of high-cardinality categorical data, as the utilized datasets primarily consist of numerical and sparse binary features.

 
\subsection{Neural Networks based on Transformer}
Inspired by the recent successes of transformer-based models in
% computer vision (CV) \cite{han2022survey} or 
natural language processing (NLP) \cite{vaswani2017attention}, researchers have proposed multiple approaches using attention mechanisms for heterogeneous tabular data.
TabNet \cite{arik2021tabnet} is one of the pioneering transformer-based models designed for tabular data. Its architecture consists of multiple subnetworks processed hierarchically in a sequential manner. Each subnetwork represents a decision step that performs soft instance-wise feature selection. The authors demonstrate that TabNet surpasses other variants on various large-scale tabular datasets.
TabTransformer \cite{huang2020tabtransformer} employs self-attention-based transformers to map categorical features to contextual embeddings. This approach enhances robustness to missing or noisy data and enables interpretability. Extensive experiments show that TabTransformer achieves performance comparable to tree-based ensemble techniques, demonstrating success even in the presence of missing or noisy data.
FT-Transformer \cite{gorishniy2021revisiting} is a straightforward adaptation of the Transformer architecture for tabular data. It considers the relationship between numerical and categorical features by feeding them together to transformer blocks, which TabTransformer has not considered.
% The experiments conducted illustrate that this model performs well on a wide range of tasks compared to other deep learning models, making it a more versatile architecture.
% Nevertheless, the method has a high computational cost.
Self-Attention and Intersample Attention Transformer (SAINT) \cite{somepalli2021saint} is a hybrid attention approach that combines self-attention with inter-sample attention over multiple rows. Additionally, the researchers leverage self-supervised contrastive pre-training to enhance performance for semi-supervised problems. In the original experiments, SAINT consistently outperforms other methods on supervised and semi-supervised tasks.
Non-Parametric Transformer (NPT) \cite{kossen2021self} utilizes the entire dataset as input. By employing attention between data points, the model can model and leverage relations between arbitrary samples for classifying test samples.
However, these transformer-based methods have been observed to incur significant computational costs, including running time and hardware.
Recently, researchers have proposed a new transformed-based model called TabPFN \cite{hollmann2022tabpfn} that can handle small-scale tabular data in less than a second with no hyperparameters tuning and can get competitive results with GBDT. However, its effectiveness is restricted to classification tasks involving numerical features. Furthermore, performing inference on larger-scale datasets poses a challenge on current consumer GPUs.



\subsection{Models with other specialized structures}
To explain the underlying relationships between features, Zheng et al. introduced Dual-Route Structure-Adaptive Graph Networks (DRSA-Net) \cite{zheng2023deep}. DRSA-Net dynamically learns a sparse graph structure between variables and characterizes interactions through dual-route message passing. Extensive experiments in various application scenarios demonstrate that DRSA-Net is competitive with classical algorithms and deep models.
Regularization Learning Networks (RLN) \cite{shavitt2018regularization} incorporate trainable regularization coefficients for individual weights in a neural network to reduce sensitivity. These coefficients promote sparsity in the network and can outperform deep neural networks, achieving results comparable to those of GBDT. However, the evaluation is based on a dataset primarily composed of numerical data and cannot handle categorical features.
Wide\&Deep \cite{cheng2016wide} and DeepFM \cite{guo2017deepfm} are designed for online prediction where the input space is tabular data. They both use embedding techniques to handle the categorical features, but the difference is that Wide\&Deep employs a linear model and a wide selection of hand-crafted logical expressions on features to improve the generalization capabilities while DeepFM applies a learned Factorization Machines to replace the hand-crafted features to improve the model performance.
The DeepGBM model \cite{ke2019deepgbm} is also for online prediction, it consists of two components: CatNN and GBDT2NN. The former is to handle the categorical features and has the same structure as DeepFM, the latter is to distill knowledge from a trained GBDT. By integrating the advantages of GBDT on tabular data and the flexibility of deep neural networks, DeepGBM can outperform other models on various online prediction problems.
The lack of a consistent structure in general tabular data, unlike image and language data, poses a limitation on the application of deep learning. In order to address this challenge, researchers have undertaken numerous endeavors to transform tabular data into images or text. By utilizing more efficient models in CV and NLP, these efforts aim to enhance the performance of deep learning in the domain of tabular data \cite{sun2019supertml, yin2020tabert, iida2021tabbie, hegselmann2023tabllm, wang2022transtab}.
Furthermore, Value Imputation and Mask Estimation (VIME) \cite{yoon2020vime} extends the success of self- and semi-supervised learning in CV and NLP to tabular data.
AutoGluon Tabular \cite{erickson2020autogluon} builds ensembles of basic neural networks together with other traditional ML techniques, with its key contribution being a strong stacking approach. 
Most deep neural networks are trained using back-propagation (BP), which may yield suboptimal performance or limited generalization \cite{suganthan2018non}. To overcome this drawback, researchers have utilized Random Vector Functional Link (RVFL) networks \cite{pao1994learning} to design models for tabular data \cite{shi2021random,shi2022weighting}. Experimental results demonstrate that the proposed networks are competitive with other state-of-the-art neural networks. However, the method lacks comparisons with GBDT, which is usually considered the preferred choice for tabular data.



\section{Neural Classification and Regression Tree}
\label{s:method}
In this section,  detailed descriptions of Neural Classification and Regression Tree (NCART) are presented. 
We first introduce the differentiable decision trees. Next, we present the architecture of NCART. Finally, we conclude this section with a discussion on the interpretability of NCART.

\subsection{Approximation of a Decision Tree}
The standard decision tree is a recursive subdivision of the feature space, but this process cannot be made differentiable, and of course, it cannot be expressed by a neural network.
Instead of using the traditional decision tree, we opt for the use of the oblivious decision tree \cite{rokach2005decision}. This allows for the incorporation of a "soften" decision function within the internal tree nodes, ensuring interpretability while also enabling the differentiable aspect of the model.
An oblivious decision tree (ODT)  is a type of binary decision tree where each level tests the same feature. However, an ODT does not consider the interaction between different features when making splits, which makes it less expressive compared to standard trees.
To address the issue, we combine multiple ODTs together to approximate the representation ability of a standard decision tree.

Figure \ref{f.approx} is used to illustrate how to approximate a decision tree with two ODTs. 
On the left is a decision tree that splits the feature space into 5 distinct subregions. We first refine the 5 subregions into 9 subdivisions and then  approximate the refined space by the sum of two individual spaces, each space has 4 subregions split along each feature and can be expressed as an oblivious tree. 
The leaf values of the oblivious trees can be obtained by solving the following objective:
\begin{align*}
   & \min |ODT_1+ODT_2-DT| \\
    = &\min_{a_i, b_j} |a_1+b_1-c_1|+|a_2+b_1-c_1|+|a_2+b_2-c_2|\\
    &+|a_3+b_1-c_3| + |a_3+b_3-c_3|+|a_4+b_1-c_4|\\
    &+|a_4+b_2-c_4|+|a_4+b_3-c_5| +|a_4+b_4-c_5|, 
\end{align*}
 where $ ODT_i$ and $DT$ are the vectors formed by the leaf values of ODT  and the decision tree, respectively. 
 By doing so, we can approximate the decision tree by two oblivious trees. 


It is important to highlight that ensemble tree models can be approximated using Oblivious Decision Trees (ODTs) as they essentially consist of models composed of multiple trees. However, to achieve similar performance, ODTs often require a larger number of trees compared to standard decision trees.
Furthermore, in an ODT, while nodes at the same level share the same test feature, the split values may vary. Since the results are based on an ensemble of ODTs, we can focus on a specific formulation where each level of ODTs has identical split values. This approach helps reduce the dimensionality of the function and simplifies the computation process.

% Figure environment removed



\subsection{NCART Architecture}
The architecture of an NCART block, which is illustrated in Figure \ref{f.block}, consists of 4 components: Data preprocessing, Feature Selection, Differentiable Oblivious Trees, and Ensemble.
% \begin{itemize}
%     \item \textit{}
%     \item \textit{Feature selection}
%     \item \textit{Differentiable Oblivious Tree}
%     \item \textit{Weighted Mean}
% \end{itemize}


% Figure environment removed



\paragraph{Data preprocessing}
We do not do any special processing on the input vector (including categorical features), the vector is directly sent to the first layer which is a simple \textit{Batch Normalization}.



\paragraph{Feature selection}
The feature selection is adopted to discover the most influential features and improve the model performance. Ideally, we want to learn a projection matrix $\mathbf{P}_{d,n}$ ($d<n$) that each row is a one-hot vector, 
\begin{equation}
\label{e.feat_sel}
\widetilde{\mathbf{x}} = \mathbf{P}\mathbf{x}, 
% s.t. \min t(\widetilde{\mathbf{x}}; \Theta)
\end{equation}
where $\mathbf{x} = (x_{1},\cdots, x_{n})^\intercal$, $\widetilde{\mathbf{x}} = (x_{k_1},\cdots, x_{k_d})^\intercal$, $\{k_1, k_2, \cdots, k_d\}$ is a subsequence of $\{1, \cdots, n\}$, $P_{ij}\in \{0,1\}$ and satisfies $\sum_{j=1}^{n} P_{ij} = 1, \forall{i}$. 

Instead of  learning  such a matrix  in a discrete integer space,   a sparse transformation on each row of a learnable matrix is used to learn a sparse projection
\begin{equation}
\label{e.sel_mat}
    \widetilde{x_i} = h(\mathbf{A}_i)\mathbf{x}.
\end{equation}
Here $\mathbf{A}_i=(A_{i1}, \cdots, A_{in})$ is the $i$th row of a learnable matrix $\mathbf{A}$, $h$ is a sparse function similar to the softmax, but able to output sparse probabilities, which ensures the sum of each row is equal to 1,  and is everywhere differentiable.   Here,  the sparse function is chosen as \textit{Sparsemax} \cite{martins2016softmax} or \textit{entmax} \cite{peters-etal-2019-sparse}, which is a hyperparameter in the network.

The learnable matrices in different \textit{feature selection} blocks share the same size but are optimized independently. Please note this layer is not mandatory, the usage depends on the user.




\paragraph{Differentiable oblivious tree}
The leaf value of an ODT with the same split value in one level is defined as
\begin{equation}
\label{eq.odt}
    c(\mathbf{x}) = g(H(x_1-s_1), \cdots, H(x_n-s_n)),
\end{equation}
where $\mathbf{x}=(x_1,\cdots,x_n) \in \mathbb{R}^n$ is the input feature vector, $\mathbf{s}=(s_1,\cdots,s_n) \in \mathbb{R}^n$ is the learned threshold,  $g: \mathbb{R}^n \to \mathbb{R}$ is a function mapping each split region to its corresponding leaf value and $$H(x) = \begin{cases} 0, & \text{if } x < 0 \\ 1, & \text{if } x \geq 0 \end{cases}$$  is the \textit{Heaviside step function} that indicates the decision routes.


To make the tree  differentiable,    $H(x)$ is replaced by \textit{Sigmoid function} $\sigma(x)=\frac{1}{1+e^{-x}}$, and   $g$  is changed to be a two-layer fully-connected network $f$ with $ReLU$ activation, which is  $f(\mathbf{x}; \Theta) = \mathbf{W_2}(ReLU(\mathbf{W_1}\mathbf{x}+\mathbf{b_1})))+\mathbf{b_2}$. After that, we obtain the output of the differentiable oblivious decision tree, 
\begin{equation}
\label{eq.ndt}
    t(\mathbf{x}) = f(\sigma(x_1-s_1), \sigma(x_2-s_2), \dots, \sigma(x_n-s_n); \Theta).
\end{equation}
The differentiability of both $H$ and $f$ allows the tree model $t$ to be differentiable as well, thereby enhancing its learning capability. 
%Since $\sigma(x)$ is a smooth approximation to $H(x)$, they serve the same function of indicating the direction (left or right child node). Furthermore, $f$ transforms the leaf from a mere singular value into a functional form, thereby augmenting the model's learning capabilities.




\paragraph{Ensemble}
% After that, if we want to do the dimensionality reduction, the feature selection layer will be applied. 
% The third layer is the core building in the network, it consists of $N$ independent differentiable trees, each tree gives an output of the corresponding leaf, and 
All the outputs are together sent to the last layer to take a weighted vote and generate the final output, which is:
\begin{equation}
\label{e.ncart}
\begin{aligned}
O(\mathbf{x}) & = \frac{1}{N}\sum_{i=1}^{N}w_i\cdot t_i(\mathbf{x}) \\
 & = \frac{1}{N}\sum_{i=1}^{N}w_i\cdot f(\sigma(x_1-s_1^i), \sigma(x_2-s_2^i), \dots, \sigma(x_n-s_n^i); \Theta_i), 
\end{aligned}
\end{equation}
where $w_i, i=1, \cdots, N$ are $N$ learnable parameters. $\Theta_i, i=1, \cdots, N$ are the parameters in their corresponding networks.




\paragraph{NCART Network Architecture}

In a GBDT algorithm, each new tree is to learn the residual errors or gradients of the loss function. ResNet has a similar idea, where each new block represents the residual mapping to be learned and it behaves like the ensemble of shallow networks \cite{veit2016residual}. 
Inspired by these methods, we can combine many NCART blocks to enhance the model performance. Fig.~\ref{f.resblock} shows the architecture of our ensemble model, we can see that it is just a variant of a simple feed-forward  ResNet where each fully-connected layer is replaced by an NCART layer. In our default architecture, each NCART block has the same number of differentiable trees, and the last NCART block (filled with red) does the dimensionality reduction operation.


% Figure environment removed

% \begin{wrapfigure}{r}{0.4\textwidth}
%   \centering
%   % Figure removed
%   \caption{Ensemble of NCART. An NCART block filled with green means all the features are employed to do the feature split while a block filled with red means it contains a feature selection layer.}
% \end{wrapfigure}

\subsection{Feature Importance}
We now discuss the calculation of the feature importance in NCART.
Given that our model comprises multiple tree structures, the method used to calculate feature importance is similar to that in traditional decision trees.


For a differentiable ODT, we can easily derive the decision rule for a feature from Eq.\eqref{eq.ndt}. For a given feature index $j$, if $\sigma(x_j-s_j) > 0.5$, it indicates that $x_j > s_j$ and the corresponding samples will be assigned to the right leaf. Conversely, if $\sigma(x_j-s_j) \le 0.5$, the samples will be assigned to the left leaf.
With this approach, we can directly apply the Gini coefficient, commonly used in traditional decision trees, to measure the impurity of feature $x_j$. The formulation for the Gini coefficient is as follows: 
\begin{equation}
\label{e.feat_import1}
    I_j = 1-(\frac{m_{j1}}{M})^2-(\frac{m_{j2}}{M})^2,
\end{equation}
where $M$ is the number of samples. $m_{j1}$ is the number of samples divided into the left leaf, and $m_{j2}$ is the number of samples that are divided into the right leaf. 


If the NCART block includes a feature selection layer, the Gini coefficient can be formulated as follows:
\begin{equation}
\label{e.feat_import2}
    I_j^{sel} = \sum_{k=1}^{d} (1-(\frac{m_{k1}}{M})^2-(\frac{m_{k2}}{M})^2) h(A)_{kj}.
\end{equation}
where $d$ is the dimensionality of feature selection. 


In an NCART with $L$ blocks (Fig.~\ref{f.resblock}), where the last block contains a feature selection layer, each differentiable tree is independent, and every feature in a differentiable tree only splits once. Thus, to calculate the Gini coefficient of a feature $x_i$ in an NCART, we can obtain it by summing up the results from all the individual trees:
\begin{equation}
\label{e.feat_import3}
    Importance_j = \sum_{j=1}^{(L-1)N} I_j + \sum_{j=1}^{N} I_j^{sel}
\end{equation}



% \paragraph{Decision rules} For a differentiable ODT, the decision rules can be easily obtained from Eq.\eqref{eq.ndt}. For each $i$, $\sigma(x_i-s_i) > 0.5$ means $x_i>s_i$ while $\sigma(x_i-s_i) \le 0.5$ means $x_i \le s_i$. Since all the differentiable trees are independent, the decision rules of a feature $x_i$ in an NCART block can be obtained by taking the intersection of all the inequalities.


% \paragraph{Feature importance}
% The feature importance of NCART is based on the number of times a feature is used to make a split. The importance score is determined by the frequency of feature usage and the values of the sparse transformation matrix of all the differential trees. Features with higher scores are considered more important. The following is the mathematical formulation of the importance of feature $x_j$: 
% \begin{equation}
% \label{e.importance}
% I(x_j) = N*(M-1) + \sum_{k=1}^{N}\sum_{i=1}^{d} h(\mathbf{A}_i^k)_j 
% \end{equation}
% where $h(\mathbf{A}_i^k)=h(A_{i1}^k, \cdots, A_{in}^k)$, $M$ is the number of NCART blocks.
% Since all the NCART blocks except the last one employ all the features to do the split, if there is no feature selection layer in the last block, then all the features have the same importance, i.e., $I(x_j) = N*M, \forall j $.






\section{Experiments}
\label{s:exps}
\subsection{Experiments Setup}
\paragraph{Datasets} 
We make the comparisons on 20 datasets, which are all from OpenML 
\footnote{\url{https://www.openml.org/}}. The datasets are used for both classification and regression tasks, with varying sample sizes and a mixture of categorical and numerical variables present in some of them. 
% According to the data size, we divide them into three categories: small-scale (\#Samples$\le$5000), medium-scale (5000$<$\#Samples$<$50000), and large-scale (\#Samples$\ge$50000).
For all datasets, we didn't do any special preprocessing, except apply the ordinal encoding to the categorical features and specify the corresponding indices for CatBoost and Transformer-based models. More details about the datasets are introduced in \ref{a.data}.



\paragraph{Benchmark models}
We compare the proposed models to three GBDT models and six widely-used deep learning baselines, namely XGBoost \cite{chen2016xgboost}, CatBoost \cite{prokhorenkova2018catboost}, LightGBM \cite{ke2017lightgbm}, NODE \cite{popov2019neural}, TabNet \cite{arik2021tabnet}, SAINT \cite{somepalli2021saint}, FT-Transformer \cite{gorishniy2021revisiting}, RLN \cite{shavitt2018regularization} and ResNet \cite{gorishniy2021revisiting}.




\paragraph{Evaluation metrics} For the binary classification tasks, we utilize \textbf{AUC} as the evaluation metric, while for multiclass classification problems, we chose \textbf{Accuracy} as the metric. For the regression task, \textbf{MSE} is applied to measure the performance of different methods. After the hyperparameter optimization, we conduct 5-fold cross-validation and obtain the performance.





\paragraph{Training procedure} 
We use the Optuna library \cite{akiba2019optuna} to search hyperparameters, conducting 10 trials for each model's hyperparameter optimization.
The hyperparameters adjusted are introduced in \ref{a:params}, while leaving others at their default values. 
We train all neural network models for 1000 epochs by Adam optimizer \cite{kingma2014adam} with an early stopping rounds of 10 and a learning rate of 0.001.
%and a batch size of 1024.
All the experiments are conducted on a workstation equipped with an Intel Core i9-10900X CPU, 128GB RAM, and one NVIDIA-3080 GPU.





\subsection{Performance}
\begin{table*}[!ht]
\renewcommand\arraystretch{1.3}
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{lcccccccccc}
    \toprule[2pt]
        Dataset & XGBoost & CatBoost & LightGBM & NODE & TabNet & SAINT & FTTrans & RLN & ResNet & NCART \\
    \midrule[1.5pt]
    \multicolumn{11}{c}{Binary Classification (AUC $\uparrow$)}\\
        diabetes &82.44  &83.53  & 82.63  &82.98  &54.06 & 62.30& 77.52 &63.59&82.66 &\textbf{83.72}\\

        credit-g &78.14  &78.45  &\textbf{79.13} &76.66  &57.34 &77.54& 56.84& 54.22 &74.67 & 76.86\\

        qsar-biodeg &93.43  &92.87 & 93.54 &92.48 &62.89  &93.01 & 91.05&  91.33 & 92.93 & \textbf{93.64}\\

        scene &99.14  &99.21 & \textbf{99.30} &98.93  &81.26 & $OOM$ & $OOM$ &\textbf{99.30}&  98.06 & 98.65\\

        ozone-level &91.58  &91.87 & 91.94 &89.97  &58.63 & 89.18& 69.98& 64.14& 89.35& \textbf{93.10}\\

        delta$\_$ailerons &97.91  &97.85 & \textbf{97.98} &97.38  &97.53 & 96.38& 90.65& 60.67& 97.18 & 97.56\\

        MagicTelescop &93.46  &\textbf{93.57} & 93.47 &92.05  &92.71 & 90.23& 91.21& 84.96& 90.80& 92.98\\

        jannis &87.34  &\textbf{87.52} &87.13 &84.65  &86.17 & 78.39 & 86.85& 80.38 & 85.58 & 86.74\\
        
        road-safety &\textbf{90.24}  &89.29 &90.11 &87.43  &88.54 & 86.62& 69.22& 54.78 & 79.53 & 88.23\\

        Higgs  &82.80  &82.79 & 82.46 &82.34 & 83.26 & 83.62& 82.82&  69.9 & \textbf{84.35} & 83.45\\

        
    \midrule[1pt]
    \multicolumn{11}{c}{Multiclass Classification (Acc. $\uparrow$)}\\

        autoUniv-au7 &\textbf{53.57}  &48.43 &52.14 &45.00  &34.71 & 40.86& 37.43& 35.0 & 41.71 & 48.29\\

        plants-margin &74.25  &\textbf{84.81} &75.38 &81.69  &1.00 & 11.19& 1.00&  0.94 & 78.31 & 77.31\\

        waveform &85.44  &86.04 & 84.88 &86.48  &83.40 & 86.16& 86.82&  \textbf{87.00} & 86.68 & 86.34\\

        gas-drift &\textbf{99.45} &99.44  & 99.42 &98.45  &97.65 & 99.37& 92.44&  65.14 & 98.86 & \textbf{99.45} \\
        
        EMNIST &83.53  &84.15 &83.14 &81.11  &80.02 & $OOM$& $OOM$ &70.22& \textbf{86.08} & 84.31\\

        
    \midrule[1pt]
    \multicolumn{11}{c}{Regression (MSE $\downarrow$)}\\
        
        analcatdata  &\textbf{0.0051}  &0.0052 & 0.0054 &0.0091 &0.3984 &0.2988 & 0.3034 &1.1591 &0.0126 & 0.0140\\

        kin8nm &0.0137 &0.0092 & 0.0111 &\textbf{0.0042}  &0.0168 & 0.0044& 0.0079&  0.0347& 0.0063 & 0.0080\\

        superconduct &86.19  &105.70 & \textbf{85.17} &1350.31 &141.46 & 199.16& 338.45& 315.75& 157.02 & 125.81\\
        
        house$\_$sales &\textbf{0.0281}  &0.0286 & \textbf{0.0281} &0.1327 &0.2257 & 0.1248&  0.1692& 7.4292&0.1295 & 0.0371\\

        year  &\textbf{74.76}  &76.74 & 76.60 &$TimeOut$  &82.03 & 94.12& 86.3701&  145.42& 77.09 & 75.06\\

    \midrule[1pt]
    Average Rank & 3.35 & \textbf{3.15} & 3.35 & 5.75 & 7.2 & 6.5 & 7.25 & 8.4 & 5 & 3.5 \\

    Best/Worst& \textbf{6/0} & 3/0& 5/0 & 1/2 & 0/4 & 0/3 &0/2 &2/10 & 2/0 &4/0\\
    \bottomrule[2pt]
    \end{tabular}
    \end{adjustbox}
    \caption{Mean results of 10 models on different datasets. The datasets are sorted by their size. The \textbf{bold} indicates the top result; $OOM$ represents there exists GPU overflow; $TimeOut$ means the running time exceeds the time limit.}
    \label{T.results}
\end{table*}



% Figure environment removed


The results of the comparison are summarized in Table.~\ref{T.results} and Fig.~\ref{f.rank_results}. For all methods, we report the mean performance of 5-fold cross-validation with optimal hyperparameters.



Based on the results, it is evident that tree-based models retain their advantages in this comparison. Among the models evaluated, CatBoost demonstrates the highest average performance. However, XGBoost and LightGBM achieve more optimal results, even though their average rank is slightly higher than that of CatBoost. This suggests that while CatBoost performs better overall across different scenarios, XGBoost and LightGBM excel in certain cases.
NODE can achieve competitive or even superior results on the majority of datasets. However, its performance falls short when confronted with high-dimensional regression datasets such as superconduct and year. The year dataset was also evaluated in NODE's research paper, but with larger hyperparameters compared to those used in this paper. This necessitates additional computational resources, making it impractical for general use.
TabNet has proven to be a competitive model in cases where the sampling number is not large, which matches the highlight in the original paper. However, its performance decreases considerably when the sample size is small. Therefore, its effectiveness can be limited based on the size of the dataset. 
Both SAINT and FT-Transformer share a common limitation: they struggle to effectively handle small-scale datasets and require additional GPU memory when confronted with high-dimensional features, such as scene and EMNIST datasets. 
Although RLN is suitable for all kinds of datasets and can obtain the two best results, it also produces some of the worst scores, indicating a higher variability in their results.
ResNet proves to be a highly effective baseline, particularly when dealing with large-scale datasets. As a variant of fully-connected ResNet, NCART surpasses the performance of  ResNet on a wide range of datasets and delivers competitive results comparable to those achieved by tree-based models.

Additionally, our experiments indicate that deep learning models can achieve comparable or competitive results in classification tasks across datasets of varying sizes. However, when it comes to regression tasks, tree-based methods consistently demonstrate superior performance. This discrepancy can be attributed to the discrete nature of tabular datasets and the non-smooth characteristics of the target function. On the contrary, deep learning models tend to bias towards excessively smooth solutions, which may hinder their performance in regression tasks \cite{grinsztajn2022tree}. But if the distribution behind the dataset tends to be smooth (kin8nm), then deep learning can get better results than tree models.









\subsection{Training time}

% Figure environment removed


We also analyze the average running time, which is given in Fig.~\ref{f.time_hist} and Table.~\ref{T.running_time}, of the 5-fold cross-validation of different models in comparison to their performance.
For each algorithm and for each 5-fold cross-validation, we run the algorithm for up to 50000s.
% The time results are shown in  Fig.~\ref{f.time_hist}. 
% and Fig.~\ref{f.time_results}, and the detailed running time is given in \ref{a.time}.




Compared to evaluation scores, tree models have bigger advantages in running time.
% We observe that as the data size increases, the computation time for all models also increases. 
% However, among all the models, tree models have the slowest growth rate.
When dealing with a small dataset, TabNet can compute faster than the tree model. Nevertheless, as the data size increases, its computational cost rises very quickly.
In terms of total running time, RLN stands out as the fastest model among all deep learning models and exhibits even greater superiority when dealing with large-scale datasets. This is due to its straightforward nature as a simple MLP augmented with regularization components.
ResNet and FT-Transformer also show outstanding results on small-scale classification datasets, but when confronted with large regression datasets, the computational cost also goes up sharply.
While NCART demonstrates slightly inferior performance compared to ResNet, it offers the advantage of relatively faster total computation time. Although it may not be optimal for every dataset, its overall calculation time is favorable.
On the other hand, NODE and SAINT are the two slowest of all models due to their complex network architectures.

It is indeed surprising that deep learning models exhibit better running times compared to tree-based methods on two classification datasets that contain more than 40 object classes, such as plants-margin and EMNIST. This is because current GBDT methods are designed for single output, if they want to handle multiple outputs, they need to construct multiple decision trees first and then concatenate the predictions of all trees to obtain multiple outputs. When the output dimension is high, this strategy is not efficient \cite{zhang2020gbdt}. 
Instead, neural networks are more flexible and they just need to adjust the number of neurons in the last layer to adapt to any output dimension. For networks with numerous parameters, such small modification has little impact on the running time. The computation time of deep learning is more affected by the network structure and dataset size. The structure determines the speed of convergence, while the amount of data influences the number of iterations needed in the training process. These two examples highlight the potential of deep learning models to efficiently handle complex classification tasks with large numbers of classes.


\begin{table*}[!ht]
\renewcommand\arraystretch{1.4}
% \setlength\tabcolsep{10pt}
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{lcccccccccc}
    \toprule[2pt]
        Dataset & XGBoost & CatBoost &LightGBM & NODE & TabNet & SAINT& FTTrans & RLN & ResNet &  NCART \\
    \midrule[1.5pt]
    \multicolumn{11}{c}{Binary Classification}\\

        diabetes &    \textbf{0.08} &      1.08 &     0.11  &     32.85 &     0.13 &    156.10 &           1.59 &   24.22 &     1.99 &     5.79\\

        credit-g  &     0.18 &      4.19 &      0.16 &     84.10 &   \textbf{0.07} &     73.99 &           1.18 &   16.08 &     2.44 &     7.18 \\

        qsar-biodeg & \textbf{0.13} &      3.21 &      0.18 &     20.84 &     0.14 &    199.24 &           4.75 &   46.89 &     1.39 &    13.34 \\

        scene & 0.44 &     3.48 &      \textbf{0.21} &    193.95 &    14.73 &  $OOM$ &       $OOM$ &   34.73 &     1.63 &    13.16\\

        ozone-level &   \textbf{0.11} &      1.91 &      0.24 &    211.53 &     2.15 &    298.27 &          18.89 &    4.98 &     3.23 &     8.26 \\

        delta$\_$ailerons  &    \textbf{0.06} &      2.95 &      0.17 &    346.49 &    50.27 &     89.17 &           3.66 &    5.11 &    10.97 &    15.22\\

        MagicTelescop  &    \textbf{0.29} &     10.61 &      0.38 &    369.31 &    31.19 &    180.62 &          20.34 &   59.22 &    20.43 &    19.19\\

        jannis &     1.31 &     18.75 &     \textbf{0.86} &    500.89 &   684.32 &    169.47 &         438.28 &   83.88 &    48.70 &    36.45 \\
        
        road-safety &    3.87 &      5.85 &     \textbf{3.82} &   1000.29 &  1306.03 &    443.30 &         181.94 &   25.47 &   192.86 &   132.75  \\

        Higgs  &    \textbf{6.63} &     9.63 &    10.73 &    954.56 &  8209.51 &   1162.72 &        2914.61 &  599.21 &  1015.46 &  1424.64\\

        
    \midrule[1pt]
    \multicolumn{11}{c}{Multiclass Classification}\\

        autoUniv-au7 &     0.20 &      1.01 &      0.18 &     30.11 &     \textbf{0.06} &    136.29 &           0.73 &   41.37 &     1.11 &     4.13\\

        plants-margin &    6.53 &     10.05 &   6.66 &    405.50 &     \textbf{0.69} &    103.81 &           4.13 &    4.60 &     3.94 &     4.85 \\

        waveform &     1.19 &      2.30 &      \textbf{0.45} &     28.52 &     9.57 &    129.35 &          22.78 &   46.18 &     4.34 &     9.48\\

        gas-drift &     \textbf{2.10} &      6.68 &     6.81 &   1145.76 &    41.46 &    455.32 &         110.79 &   33.51 &    14.81 &    35.15  \\
        
        EMNIST &   251.56 &    207.63 &    249.05 &   2649.74 &  1536.03 &  $OOM$ &  $OOM$ &   \textbf{98.74} &    99.62 &   288.04 \\

        
    \midrule[1pt]
    \multicolumn{11}{c}{Regression}\\
        
        analcatdata  &     0.31 &      1.10 &      \textbf{0.15} &     97.77 &     7.66 &     42.17 &           9.39 &   30.32 &     6.41 &    11.49 \\

        kin8nm &     1.69 &      4.13 &     \textbf{1.43} &    210.32 &   108.63 &    323.10 &          29.08 &   39.24 &    15.71 &    14.98 \\

        superconduct &     \textbf{2.46} &     13.37 &     16.24 &    858.54 &   132.95 &    569.11 &         279.68 &   81.50 &    28.59 &    44.02\\
        
        house$\_$sales &    0.81 &     5.07 &    \textbf{0.64} &   1744.48 &   132.61 &    387.82 &          70.98 &  169.44 &    22.45 &    36.68 \\

        year & 14.22 &     \textbf{8.25} &   12.92 &  $TimeOut$ &  3036.32 &   3916.41 &        1805.23 &  120.02 &  1594.27 &   690.45\\

    \midrule[1pt]
    Average Rank & \textbf{2.1} & 3.45 & 2.2 & 9.0 & 5.9 & 9.05 & 6.4 & 6.3 & 4.7 & 5.8 \\

    Best/Worst& \textbf{8/0} & 1/0& 7/0 & 0/9 & 3/3 & 0/8 & 0/2 & 1/0 & 0/0 & 0/0\\

    Total time & \textbf{294.17}   &    321.25 & 311.39   &  $\gg$10885.55    &  15304.52  &  $\gg$8836.26  &  $\gg$5918.03   &   1564.71   &   3090.35   &   2815.25 \\
    \bottomrule[2pt]
    \end{tabular}
    \end{adjustbox}
% \vspace{0.5em}
    \caption{Average running time(s) of a five cross-validation with the best hyperparameters. The datasets are sorted by their size. The \textbf{bold} indicates the top result; $OOM$ represents there exists GPU overflow; $TimeOut$ means the running time exceeds the time limit.}
    \label{T.running_time}
\end{table*}

% % Figure environment removed


\subsection{Interpretability}
In this subsection, we utilize four interpretable models, namely XGBoost, LightGBM, CatBoost, and NCART, to explore the feature importance in the medical dataset diabetes. Fig.~\ref{f.feature_importance} illustrates the importance scores obtained from these models, revealing variations in their assessment of feature importance.

These differences in feature importance primarily stem from the distinct structures and methodologies employed by each model. XGBoost, LightGBM, and CatBoost are ensemble methods based on Gradient Boosting Decision Trees, while the Neural Network operates on a different paradigm. The ensemble methods construct decision trees using diverse subsets of data and apply various randomization techniques, leading to discrepancies in their individual feature importance rankings. Additionally, the models' unique hyperparameters, such as learning rates, tree depths, and the number of trees, significantly influence the final importance scores for each feature. On the other hand, the Neural Network, with its layers, neurons, and activation functions, approaches feature importance from a different perspective.

Despite these differences in feature importance, all four models provide varying degrees of interpretability. This interpretability enables us to gain valuable insights into the factors driving their predictions, making them useful tools for understanding the relationships between features and target variables in the dataset.

% Figure environment removed



\subsection{Summary}
We end this section with some summaries according to the numerical experiments:
\begin{itemize}
    \item Tree models still have the advantages on tabular data, especially in running time. But when meeting classification tasks with large numbers of classes, they may have some drawbacks.
    \item Deep learning can achieve similar results in classification tasks to GBDT methods, but has little competitiveness when doing regression problems. 
    \item Network with differentiable trees (NCART, NODE) can obtain competitive results as tree-based methods, but the calculation time is much longer than that of tree methods when the dataset is large-scale.
    \item Simple fully-connected MLPs (RLN, ResNet) are fast and can obtain excellent results, but they are not stable and have high variability.
    \item Transformer-based models are more suitable for handling large-scale datasets, but they usually need more computing resources.
\end{itemize}







\section{Conclusions}
\label{s:conclu}

The performance of deep learning models in handling tabular data has been relatively restricted when compared to other machine learning techniques. Challenges such as limited data availability, high computational costs, and interpretability issues have posed significant obstacles to the widespread adoption of deep learning in real-life applications.

In this paper, we introduce NCART, a novel interpretable neural network that overcomes these challenges by incorporating differentiable decision trees within the ResNet architecture. By fusing decision trees with deep learning, our model combines the interpretability of decision trees with the powerful capabilities of deep neural networks.


NCART's simplicity in architecture makes it well-suited for datasets of varying sizes, while its efficiency leads to reduced computational expenses in comparison to state-of-the-art deep learning models. Our extensive numerical experiments reveal that NCART achieves competitive performance across datasets of different sizes, offering valuable insights into underlying patterns and relationships.


The introduction of NCART represents a significant step forward in addressing the limitations of deep learning in handling tabular data, opening up new possibilities for valuable applications in diverse scenarios. With its interpretability, efficiency, and performance, NCART paves the way for enhanced utilization of deep learning techniques in practical data analysis and decision-making tasks.








%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\appendix
\section{Datasets Description}
\label{a.data}
Table. \ref{T.description} are the datasets used in this paper, the column \#Target means the number of distinct values in the label. The selection of data follows the following principles: 1. Diversity of feature types; 2. Diversity of feature dimensions; 3. Diversity of sample numbers.


\begin{table*}[!ht]
\renewcommand\arraystretch{1.2}
\footnotesize
\setlength\tabcolsep{10pt}
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{lccccc}
    \toprule[2pt]
        Dataset & \#Samples & \#Num.Feat. &\#Cat.Feat. & \#Target & Data id   \\
    \midrule[1.5pt]
        \multicolumn{6}{c}{Binary Classification}\\
    \hline
        diabetes & 768  & 8 &0 & 2  & 37 \\
    \hline
        credit-g & 1000  & 20  &13 & 2  & 31 \\
    \hline
        qsar-biodeg & 1055   & 41 &0 & 2  & 1494 \\
    \hline
        scene & 2407  & 299 &5 & 2  & 312 \\
    \hline
        ozone-level & 2534  & 72 &0 & 2  & 1487 \\
    \hline
        delta$\_$ailerons & 7129  & 5 &0 & 2  & 803 \\
    \hline
        MagicTelescop & 13376  & 10 &0 & 2  & 44125 \\
    \hline
        jannis & 57580  & 54 &0 & 2  & 44131 \\
    \hline
        road-safety & 111762  & 32 &3 & 2  & 44161 \\
    \hline
        Higgs & 940160  & 24 &0 & 2  & 44129 \\
    \midrule[1.5pt]
        \multicolumn{6}{c}{Multiclass Classification}\\
    \hline
        autoUniv-au7 & 700  & 12 &4 & 3  & 1553 \\
    \hline
        plants-margin & 1600  & 64 &0 & 100  & 1491 \\
    \hline
        waveform & 5000  & 40 &0 & 3  & 60 \\
    \hline
        gas-drift & 13910  & 128 &0 & 6  & 1476 \\
    \hline
        EMNIST  & 131600  & 784 &0 & 47  & 41039 \\
    \midrule[1.5pt]
    \multicolumn{6}{c}{Regression}\\
    \hline
        analcatdata & 4052  & 7 &5 & 10  & 44055 \\
    \hline
        kin8nm & 8192  & 8 &0 & 8188  & 189 \\
    \hline
        superconduct & 21263  & 79 &0 & 3007  & 44148 \\
    \hline
        house$\_$sales & 21613  & 17 &2 & 4028  & 44066 \\
    \hline
        year & 515345  & 90 &0 & 89  & 44027 \\
    \bottomrule[2pt]
    \end{tabular}
    \end{adjustbox}
    \caption{Details of datasets. \#Target in regression task means the number of unique values.}
    \label{T.description}
\end{table*}







% \section{Calculation time}
% \label{a.time}

% Table. \ref{T.time_results} are the average running time of a five cross-validation with the best hyperparameters. 

% \begin{table*}[!ht]
% \renewcommand\arraystretch{1.4}
% \setlength\tabcolsep{10pt}
%     \centering
%     \begin{adjustbox}{width=\textwidth}
%     \begin{tabular}{lcccccccccc}
%     \toprule[2pt]
%         Dataset & XGBoost & CatBoost &LightGBM & NODE & TabNet & SAINT& FTTrans & RLN & ResNet &  NCART \\
%     \midrule[1.5pt]
%     \multicolumn{11}{c}{Binary Classification}\\

%         diabetes &    \textbf{0.04} &      1.88 &      0.63 &     32.85 &     0.13 &    156.10 &           1.59 &   24.22 &     1.99 &     5.79\\

%         credit-g  &     0.13 &      7.76 &      0.47 &     84.10 &   \textbf{0.07} &     73.99 &           1.18 &   16.08 &     2.44 &     7.18 \\

%         qsar-biodeg & 0.18 &      6.83 &      0.70 &     20.84 &     \textbf{0.14} &    199.24 &           4.75 &   46.89 &     1.39 &    13.34 \\

%         scene & \textbf{0.33} &      1.72 &      0.78 &    193.95 &    14.73 &  $OOM$ &       $OOM$ &   34.73 &     1.63 &    13.16\\

%         ozone-level &   \textbf{0.23} &      6.12 &      0.35 &    211.53 &     2.15 &    298.27 &          18.89 &    4.98 &     3.23 &     8.26 \\

%         delta$\_$ailerons  &    \textbf{0.10} &      6.60 &      0.90 &    346.49 &    50.27 &     89.17 &           3.66 &    5.11 &    10.97 &    15.22\\

%         MagicTelescop  &    \textbf{0.77} &     12.39 &      7.97 &    369.31 &    31.19 &    180.62 &          20.34 &   59.22 &    20.43 &    19.19\\

%         jannis &     \textbf{1.49} &     26.65 &     10.83 &    500.89 &   684.32 &    169.47 &         438.28 &   83.88 &    48.70 &    36.45 \\
        
%         road-safety &    \textbf{4.48} &      9.89 &     41.16 &   1000.29 &  1306.03 &    443.30 &         181.94 &   25.47 &   192.86 &   132.75  \\

%         Higgs  &    \textbf{8.03} &     13.08 &     50.50 &    954.56 &  8209.51 &   1162.72 &        2914.61 &  599.21 &  1015.46 &  1424.64\\

        
%     \midrule[1pt]
%     \multicolumn{11}{c}{Multiclass Classification}\\

%         autoUniv-au7 &     0.34 &      0.86 &      0.74 &     30.11 &     \textbf{0.06} &    136.29 &           0.73 &   41.37 &     1.11 &     4.13\\

%         plants-margin &    29.47 &     91.65 &    564.46 &    405.50 &     \textbf{0.69} &    103.81 &           4.13 &    4.60 &     3.94 &     4.85 \\

%         waveform &     \textbf{0.31} &      4.41 &      2.31 &     28.52 &     9.57 &    129.35 &          22.78 &   46.18 &     4.34 &     9.48\\

%         gas-drift &     \textbf{2.56} &      8.58 &     19.25 &   1145.76 &    41.46 &    455.32 &         110.79 &   33.51 &    14.81 &    35.15  \\
        
%         EMNIST &   431.77 &    253.56 &    971.29 &   2649.74 &  1536.03 &  $OOM$ &  $OOM$ &   \textbf{98.74} &    99.62 &   288.04 \\

        
%     \midrule[1pt]
%     \multicolumn{11}{c}{Regression}\\
        
%         analcatdata  &     \textbf{0.46} &      1.70 &      0.89 &     97.77 &     7.66 &     42.17 &           9.39 &   30.32 &     6.41 &    11.49 \\

%         kin8nm &     \textbf{1.92} &      9.42 &     15.95 &    210.32 &   108.63 &    323.10 &          29.08 &   39.24 &    15.71 &    14.98 \\

%         superconduct &     \textbf{2.78} &     26.18 &     12.93 &    858.54 &   132.95 &    569.11 &         279.68 &   81.50 &    28.59 &    44.02\\
        
%         house$\_$sales &    \textbf{1.07} &     10.04 &      9.60 &   1744.48 &   132.61 &    387.82 &          70.98 &  169.44 &    22.45 &    36.68 \\

%         year & 14.06 &     \textbf{11.82} &     38.69 &  $TimeOut$ &  3036.32 &   3916.41 &        1805.23 &  120.02 &  1594.27 &   690.45\\

%     \midrule[1pt]
%     Average Rank & \textbf{1.65} & 3.8 & 3.3 & 8.95 & 5.75 & 9.05 & 6.25 & 6.15 & 4.5 & 5.6 \\

%     Best/Worst& \textbf{14/0} & 1/0& 0/1 & 0/8 &4/3 & 0/6 & 0/0 & 1/0 & 0/0 & 0/0\\

%     Total time & 500.52   &    511.14 & 1750.40   &  $\gg$10885.55    &  15304.52  &  $\gg$8836.26  &  $\gg$5918.03   &   1564.71   &   3090.35   &   2815.25 \\
%     \bottomrule[2pt]
%     \end{tabular}
%     \end{adjustbox}
% % \vspace{0.5em}
%     \caption{Running time (s) of different models on different datasets.}
%     \label{T.time_results}
% \end{table*}















\section{Optimization of hyperparameters}
\label{a:params}
Table. \ref{T.hyperparam} lists the search range of hyperparameters, which refers to the survey \cite{borisov2022deep} and some adjustments are made on this basis to try to avoid GPU memory overflow and to lower the risk of overfitting on small-scale datasets. We implement the NCART model \footnote{The codes will be released to GitHub after acceptance.}  using PyTorch and employ the official open-source implementations for other models 
% \footnote{\url{https://github.com/kathrinse/TabSurvey}}.
\footnote{XGBoost: \url{https://xgboost.readthedocs.io/en/stable/}}
\footnote{CatBoost: \url{https://catboost.ai/}}
\footnote{LightGBM: \url{https://lightgbm.readthedocs.io/en/latest/}}
\footnote{NODE: \url{https://github.com/Qwicen/node}}
\footnote{TabNet: \url{https://github.com/dreamquark-ai/tabnet}}
\footnote{SAINT: \url{https://github.com/somepago/saint}}
\footnote{FT-Transformer \& ResNet: \url{https://github.com/Yura52/rtdl}}
\footnote{RLN: \url{https://github.com/irashavitt/regularization_learning_networks}}.

\begin{table*}[!ht]
% \renewcommand\arraystretch{1.2}
\footnotesize
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{ll|ll}
\toprule[2pt]
HyperParameters & Range & HyperParameters & Range \\
\midrule[1.5pt]
\multicolumn{4}{c}{XGBoost} \\
\hline
$num\_boost\_round$ & 1000  & $early\_stopping\_rounds$ & 10  \\
\hline
$max\_depth$ & LogUniformInt [2, 10] & $alpha$ & LogUniform [1e-8, 0.1]  \\
\hline
$lambda$ & LogUniform [0.5, 2] & $eta$ & LogUniform [0.05, 0.3]  \\
\midrule[1.5pt]

\multicolumn{4}{c}{CatBoost} \\
\hline
$iterations$ & 1000 & $od\_wait$ & 10\\
\hline
$max\_depth$ & LogUniformInt [2, 10] & $l2\_leaf\_reg$ & LogUniform [0.1, 2]\\
\hline
$learning\_rate$ & LogUniform [0.05, 0.3] &&\\
\midrule[1.5pt]

\multicolumn{4}{c}{LightGBM} \\
\hline
$iterations$ & 1000 & $early\_stopping\_round$ & 10\\
\hline
$num\_leaves$ & LogUniformInt [8, 64] & $lambda\_l_1$ & LogUniform [1e-8, 0.1]\\
\hline
$lambda\_l_2$ & LogUniform [1e-8, 0.1] & $learning\_rate$ & LogUniform [0.05, 0.3] \\
\midrule[1.5pt]

\multicolumn{4}{c}{NODE} \\
\hline
$num\_layers$& [2, 4, 8] & $total\_tree\_count$& [128, 256]  \\
\hline
$tree\_depth$& [4, 6, 8]  & $tree\_output\_dim$& [2, 3]  \\
\midrule[1.5pt]

\multicolumn{4}{c}{TabNet} \\
\hline
$n\_d$& LogUniform [8, 16] & $n\_steps$& UniformInt [1, 6]\\
\hline
$gamma$& Uniform [1, 2] & $n\_independent$& UniformInt [1, 5]\\
\hline
$n\_shared$& UniformInt [1, 5] & $momentum$& LogUniform [0.01, 0.4]\\
\hline
$mask\_type$& [sparsemax, entmax] && \\
\midrule[1.5pt]

\multicolumn{4}{c}{SAINT} \\
\hline
$dim$& [16, 32, 64] & $depth$& [1, 2, 3, 6]\\
\hline
$heads$& [2, 4, 8] & $dropout$& [0, 0.1, 0.2, 0.3, 0.4, 0.5]\\
\midrule[1.5pt]

\multicolumn{4}{c}{FT-Transformer} \\
\hline
$num\_blocks$& UniformInt [1, 6] & $num\_tokens$& [8, 16, 24, 32]\\
\hline
$dropout\_att$& [0, 0.1, 0.2, 0.3, 0.4, 0.5] & $dropout\_ffn$& [0, 0.1, 0.2, 0.3, 0.4, 0.5]\\
\hline
$dropout\_res$& [0, 0.1, 0.2, 0.3, 0.4, 0.5] & \\
\midrule[1.5pt]


\multicolumn{4}{c}{RLN} \\
\hline
$layers$& UniformInt [2, 6] & $theta$& UniformInt [-12, -8]\\
\hline
$norm$& [1, 2] & & \\
\midrule[1.5pt]

\multicolumn{4}{c}{ResNet} \\
\hline
$n\_blocks$& UniformInt [1, 10] & $d\_hidden$& [32, 64, 128, 256, 512]  \\
\hline
$dropout$& [0, 0.1, 0.2, 0.3, 0.4, 0.5] && \\
\midrule[1.5pt]

\multicolumn{4}{c}{NCART} \\
\hline
% $n\_trees (N)$& [8, 16, 32, 64] & $n\_selected (d)$& UniformInt [2, 10]  \\
% \hline
% $n\_layers$& [2, 4]  & $mask\_type (h)$& [sparsemax, entmax]  \\
$N$ in Eq. \eqref{e.ncart} & [8, 16, 32, 64] & $d$ in Eq. \eqref{e.feat_sel} : feature selection dim& UniformInt [2, 10]  \\
\hline
$L$& [2, 4] in Eq. \eqref{e.feat_import3} & $h$ in Eq. \eqref{e.sel_mat}& \ [sparsemax, entmax]  \\
\bottomrule[2pt]
\end{tabular}
\end{adjustbox}
\caption{Hyperparameters space.}
\label{T.hyperparam}
\end{table*}










%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
%%  \bibliographystyle{elsarticle-num} 
%%  \bibliography{<your bibdatabase>}

%% else use the following coding to input the bibitems directly in the
%% TeX file.


\bibliographystyle{elsarticle-num} 
\bibliography{references}

\end{document}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.


% Mathematical, we hope the following equations hold:
% \begin{equation} 
% \left\{
% \begin{aligned}
% c_1 &= a_1+b_1\\
% c_1 &= a_2+b_1\\
% c_2 &= a_2+b_2\\
% c_3 &= a_3\\
% c_3 &= b_3\\
% \end{aligned}
% \right.
% \end{equation}



\subsection{XGBoost}
The search spaces and distributions of hyperparameters for XGBoost:
\begin{itemize}
    \item $max\_depth$: LogUniformInt [2, 10]
    \item $num\_boost\_round$: 1000
    \item $early\_stopping\_rounds$: 10
    \item $alpha$: LogUniform [1e-8, 10]
    \item $lambda$: LogUniform [1e-8, 10]
    \item $eta$: LogUniform [0.01, 1]
\end{itemize}

\subsection{CatBoost}
The search spaces and distributions of hyperparameters for CatBoost:
\begin{itemize}
    \item $max\_depth$: LogUniformInt [2, 10]
    \item $iterations$: 1000
    \item $od\_wait$: 10
    \item $l2\_leaf\_reg$: LogUniform [1e-8, 10]
    \item $learning\_rate$: LogUniform [0.01, 1]
\end{itemize}

\subsection{NODE}
The search spaces and distributions of hyperparameters for NODE:
\begin{itemize}
    \item $num\_layers$: [2, 4, 8]
    \item $total\_tree\_count$: [16, 128]
    \item $tree\_depth$: LogUniformInt [2, 10]
    \item $tree\_output\_dim$: [2, 3]
\end{itemize}

\subsection{TabNet}
The search spaces and distributions of hyperparameters for TabNet:
\begin{itemize}
    \item $n\_d$: [4, 8, 16, 32, 64]
    \item $n\_steps$: UniformInt [1, 10]
    \item $gamma$: Uniform [1, 2]
    \item $n\_independent$: UniformInt [1, 5]
    \item $n\_shared$: UniformInt [1, 5]
    \item $momentum$: LogUniform [0.01, 0.4]
    \item $mask\_type$: [sparsemax, entmax]
\end{itemize}

\subsection{ResNet}
The search spaces and distributions of hyperparameters for ResNet:
\begin{itemize}
    \item $n\_blocks$: UniformInt [1, 10]
    \item $d\_hidden$: [32, 64, 128, 256, 512]
    \item $dropout$: [0, 0.1, 0.2, 0.3, 0.4, 0.5]
\end{itemize}

\subsection{NCART}
The search spaces and distributions of hyperparameters for NCART:
\begin{itemize}
    \item $n\_trees$: [8, 16, 32, 64]
    % \item $n\_cut$: UniformInt [1, 5]
    \item $n\_selected$: UniformInt [2, 10]
    \item $n\_layers$: UniformInt [2, 4]
    \item $mask\_type$: [sparsemax, entmax]
\end{itemize}





% \begin{table*}[!ht]
% \renewcommand\arraystretch{1.3}
%     \centering
%     \begin{adjustbox}{width=\textwidth}
%     \begin{tabular}{lcccccccccc}
%     \toprule[2pt]
%         Dataset & XGBoost & CatBoost & LightGBM & NODE & TabNet & SAINT & FTTrans & RLN & ResNet & NCART \\
%     \midrule[1.5pt]
%     \multicolumn{11}{c}{Binary Classification (AUC $\uparrow$)}\\
%         diabetes &82.23  &83.34  & 82.79  &82.98  &54.06 & 62.30& 77.52 &63.59&82.66 &\textbf{83.72}\\

%         credit-g &77.95  &78.14  &\textbf{79.04} &76.66  &57.34 &77.54& 56.84& 54.22 &74.67 & 76.86\\

%         qsar-biodeg &92.91  &92.38 & 93.28 &92.48 &62.89  &93.01 & 91.05&  91.33 & 92.93 & \textbf{93.64}\\

%         scene &99.23  &99.24 & 99.22 &98.93  &81.26 & $OOM$ & $OOM$ &\textbf{99.30}&  98.06 & 98.65\\

%         ozone-level &92.25  &91.52 & 91.51 &89.97  &58.63 & 89.18& 69.98& 64.14& 89.35& \textbf{93.10}\\

%         delta$\_$ailerons &\textbf{97.89}  &97.77 & 97.87 &97.38  &97.53 & 96.38& 90.65& 60.67& 97.18 & 97.56\\

%         MagicTelescop &93.43  &93.22 &\textbf{93.48} &92.05  &92.71 & 90.23& 91.21& 84.96& 90.80& 92.98\\

%         jannis &\textbf{87.32}  &87.21 &87.23 &84.65  &86.17 & 78.39 & 86.85& 80.38 & 85.58 & 86.74\\
        
%         road-safety &\textbf{90.10}  &89.02 &89.44 &87.43  &88.54 & 86.62& 69.22& 54.78 & 79.53 & 88.23\\

%         Higgs  &82.79  &82.43 & 82.45 &82.34 & 83.26 & 83.62& 82.82&  69.9 & \textbf{84.35} & 83.45\\

        
%     \midrule[1pt]
%     \multicolumn{11}{c}{Multiclass Classification (Acc. $\uparrow$)}\\

%         autoUniv-au7 &\textbf{51.74}  &47.51 &50.43 &45.00  &34.71 & 40.86& 37.43& 35.0 & 41.71 & 48.29\\

%         plants-margin &73.13  &\textbf{84.44} &76.75 &81.69  &1.00 & 11.19& 1.00&  0.94 & 78.31 & 77.31\\

%         waveform &85.14  &85.64 & 85.48 &86.48  &83.40 & 86.16& 86.82&  \textbf{87.00} & 86.68 & 86.34\\

%         gas-drift &99.42 &99.44  & \textbf{99.46} &98.45  &97.65 & 99.37& 92.44&  65.14 & 98.86 & 99.45 \\
        
%         EMNIST &83.36  &84.03 &83.2 &81.11  &80.02 & $OOM$& $OOM$ &70.22& \textbf{86.08} & 84.31\\

        
%     \midrule[1pt]
%     \multicolumn{11}{c}{Regression (MSE $\downarrow$)}\\
        
%         analcatdata  &0.0053  &\textbf{0.0051} & 0.0054 &0.0091 &0.3984 &0.2988 & 0.3034 &1.1591 &0.0126 & 0.0140\\

%         kin8nm &0.0136 &0.0087 & 0.0125 &\textbf{0.0042}  &0.0168 & 0.0044& 0.0079&  0.0347& 0.0063 & 0.0080\\

%         superconduct &87.40  &107.05 & \textbf{86.84} &1350.31 &141.46 & 199.16& 338.45& 315.75& 157.02 & 125.81\\
        
%         house$\_$sales &\textbf{0.0279}  &0.0284 & 0.0282 &0.1327 &0.2257 & 0.1248&  0.1692& 7.4292&0.1295 & 0.0371\\

%         year  &75.41  &78.11 & 76.64 &$TimeOut$  &82.03 & 94.12& 86.3701&  145.42& 77.09 & \textbf{75.06}\\

%     \midrule[1pt]
%     Average Rank & 3.5 & 3.55 & \textbf{3.35} & 5.85 & 7.35 & 6.7 & 7.5 & 8.55 & 5.1 & 3.6 \\

%     Best/Worst& \textbf{5/0} & 2/0& 4/0 & 1/2 & 0/4 & 0/3 &0/2 &2/11 & 2/0 &4/0\\
%     \bottomrule[2pt]
%     \end{tabular}
%     \end{adjustbox}
%     \caption{Mean results of 10 models on different datasets. The \textbf{bold} indicates the top result; $OOM$ represents there exists GPU overflow; $TimeOut$ means the running time exceeds the time limit.}
%     \label{T.results}
% \end{table*}



