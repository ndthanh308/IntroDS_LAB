{
  "title": "R-LPIPS: An Adversarially Robust Perceptual Similarity Metric",
  "authors": [
    "Sara Ghazanfari",
    "Siddharth Garg",
    "Prashanth Krishnamurthy",
    "Farshad Khorrami",
    "Alexandre Araujo"
  ],
  "submission_date": "2023-07-27T19:11:31+00:00",
  "revised_dates": [
    "2023-07-31T16:06:47+00:00"
  ],
  "abstract": "Similarity metrics have played a significant role in computer vision to capture the underlying semantics of images. In recent years, advanced similarity metrics, such as the Learned Perceptual Image Patch Similarity (LPIPS), have emerged. These metrics leverage deep features extracted from trained neural networks and have demonstrated a remarkable ability to closely align with human perception when evaluating relative image similarity. However, it is now well-known that neural networks are susceptible to adversarial examples, i.e., small perturbations invisible to humans crafted to deliberately mislead the model. Consequently, the LPIPS metric is also sensitive to such adversarial examples. This susceptibility introduces significant security concerns, especially considering the widespread adoption of LPIPS in large-scale applications. In this paper, we propose the Robust Learned Perceptual Image Patch Similarity (R-LPIPS) metric, a new metric that leverages adversarially trained deep features. Through a comprehensive set of experiments, we demonstrate the superiority of R-LPIPS compared to the classical LPIPS metric. The code is available at https://github.com/SaraGhazanfari/R-LPIPS.",
  "categories": [
    "cs.CV",
    "cs.LG",
    "eess.IV"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.15157",
  "pdf_url": "https://arxiv.org/pdf/2307.15157v2",
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 32247616,
  "size_after_bytes": 268644
}