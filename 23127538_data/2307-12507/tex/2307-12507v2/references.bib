
@inproceedings{cheng_seq2sick_2020,
	title = {{Seq2Sick}: {Evaluating} the robustness of sequence-to-sequence models with adversarial examples},
	volume = {34},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5767},
	doi = {10.1609/aaai.v34i04.5767},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Cheng, Minhao and Yi, Jinfeng and Chen, Pin-Yu and Zhang, Huan and Hsieh, Cho-Jui},
	month = apr,
	year = {2020},
	pages = {3601--3608},
}

@inproceedings{welbl_undersensitivity_2020,
	address = {Online},
	title = {Undersensitivity in {Neural} {Reading} {Comprehension}},
	url = {https://aclanthology.org/2020.findings-emnlp.103},
	doi = {10.18653/v1/2020.findings-emnlp.103},
	abstract = {Current reading comprehension methods generalise well to in-distribution test sets, yet perform poorly on adversarially selected data. Prior work on adversarial inputs typically studies model oversensitivity: semantically invariant text perturbations that cause a model's prediction to change. Here we focus on the complementary problem: excessive prediction undersensitivity, where input text is meaningfully changed but the model's prediction does not, even though it should. We formulate an adversarial attack which searches among semantic variations of the question for which a model erroneously predicts the same answer, and with even higher probability. We demonstrate that models trained on both SQuAD2.0 and NewsQA are vulnerable to this attack, and then investigate data augmentation and adversarial training as defences. Both substantially decrease adversarial vulnerability, which generalises to held-out data and held-out attack spaces. Addressing undersensitivity furthermore improves model robustness on the previously introduced ADDSENT and ADDONESENT datasets, and models generalise better when facing train / evaluation distribution mismatch: they are less prone to overly rely on shallow predictive cues present only in the training set, and outperform a conventional model by as much as 10.9\% F1.},
	urldate = {2023-08-10},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Welbl, Johannes and Minervini, Pasquale and Bartolo, Max and Stenetorp, Pontus and Riedel, Sebastian},
	month = nov,
	year = {2020},
	pages = {1152--1165},
}

@inproceedings{liang_deep_2018,
	address = {Stockholm, Sweden},
	series = {{IJCAI}'18},
	title = {Deep text classification can be fooled},
	isbn = {978-0-9992411-2-7},
	abstract = {In this paper, we present an effective method to craft text adversarial samples, revealing one important yet underestimated fact that DNN-based text classifiers are also prone to adversarial sample attack. Specifically, confronted with different adversarial scenarios, the text items that are important for classification are identified by computing the cost gradients of the input (white-box attack) or generating a series of occluded test samples (blackbox attack). Based on these items, we design three perturbation strategies, namely insertion, modification, and removal, to generate adversarial samples. The experiment results show that the adversarial samples generated by our method can successfully fool both state-of-the-art character-level and wordlevel DNN-based text classifiers. The adversarial samples can be perturbed to any desirable classes without compromising their utilities. At the same time, the introduced perturbation is difficult to be perceived.},
	urldate = {2023-08-10},
	booktitle = {Proceedings of the 27th {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Liang, Bin and Li, Hongcheng and Su, Miaoqiang and Bian, Pan and Li, Xirong and Shi, Wenchang},
	month = jul,
	year = {2018},
	pages = {4208--4215},
}

@inproceedings{meng_geometry-inspired_2020,
	address = {Barcelona, Spain (Online)},
	title = {A {Geometry}-{Inspired} {Attack} for {Generating} {Natural} {Language} {Adversarial} {Examples}},
	url = {https://aclanthology.org/2020.coling-main.585},
	doi = {10.18653/v1/2020.coling-main.585},
	abstract = {Generating adversarial examples for natural language is hard, as natural language consists of discrete symbols, and examples are often of variable lengths. In this paper, we propose a geometry-inspired attack for generating natural language adversarial examples. Our attack generates adversarial examples by iteratively approximating the decision boundary of Deep Neural Networks (DNNs). Experiments on two datasets with two different models show that our attack fools natural language models with high success rates, while only replacing a few words. Human evaluation shows that adversarial examples generated by our attack are hard for humans to recognize. Further experiments show that adversarial training can improve model robustness against our attack.},
	urldate = {2023-08-10},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Meng, Zhao and Wattenhofer, Roger},
	month = dec,
	year = {2020},
	pages = {6679--6689},
}

@inproceedings{sato_interpretable_2018,
	address = {Stockholm, Sweden},
	title = {Interpretable {Adversarial} {Perturbation} in {Input} {Embedding} {Space} for {Text}},
	isbn = {978-0-9992411-2-7},
	url = {https://www.ijcai.org/proceedings/2018/601},
	doi = {10.24963/ijcai.2018/601},
	abstract = {Following great success in the image processing ﬁeld, the idea of adversarial training has been applied to tasks in the natural language processing (NLP) ﬁeld. One promising approach directly applies adversarial training developed in the image processing ﬁeld to the input word embedding space instead of the discrete input space of texts. However, this approach abandons such interpretability as generating adversarial texts to signiﬁcantly improve the performance of NLP tasks. This paper restores interpretability to such methods by restricting the directions of perturbations toward the existing words in the input embedding space. As a result, we can straightforwardly reconstruct each input with perturbations to an actual text by considering the perturbations to be the replacement of words in the sentence while maintaining or even improving the task performance1.},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Sato, Motoki and Suzuki, Jun and Shindo, Hiroyuki and Matsumoto, Yuji},
	month = jul,
	year = {2018},
	pages = {4323--4330},
}

@incollection{pasi_generating_2018,
	address = {Cham},
	title = {Generating {Adversarial} {Text} {Samples}},
	volume = {10772},
	isbn = {978-3-319-76940-0 978-3-319-76941-7},
	url = {http://link.springer.com/10.1007/978-3-319-76941-7_71},
	abstract = {Adversarial samples are strategically modiﬁed samples, which are crafted with the purpose of fooling a trained classiﬁer. In this paper, we propose a new method of crafting adversarial text samples by modiﬁcation of the original samples. Modiﬁcations of the original text samples are done by deleting or replacing the important or salient words in the text or by introducing new words in the text sample. While crafting adversarial samples, one of the key constraint is to generate meaningful sentences which can at pass oﬀ as legitimate from the language (English) viewpoint. Experimental results on IMDB movie review dataset for sentiment analysis and Twitter dataset for gender detection show the eﬃcacy of our proposed method.},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Samanta, Suranjana and Mehta, Sameep},
	editor = {Pasi, Gabriella and Piwowarski, Benjamin and Azzopardi, Leif and Hanbury, Allan},
	year = {2018},
	doi = {10.1007/978-3-319-76941-7_71},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {744--749},
}

@inproceedings{ebrahimi_adversarial_2018,
	address = {Santa Fe, New Mexico, USA},
	title = {On {Adversarial} {Examples} for {Character}-{Level} {Neural} {Machine} {Translation}},
	url = {https://aclanthology.org/C18-1055},
	abstract = {Evaluating on adversarial examples has become a standard procedure to measure robustness of deep learning models. Due to the difficulty of creating white-box adversarial examples for discrete text input, most analyses of the robustness of NLP models have been done through black-box adversarial examples. We investigate adversarial examples for character-level neural machine translation (NMT), and contrast black-box adversaries with a novel white-box adversary, which employs differentiable string-edit operations to rank adversarial changes. We propose two novel types of attacks which aim to remove or change a word in a translation, rather than simply break the NMT. We demonstrate that white-box adversarial examples are significantly stronger than their black-box counterparts in different attack scenarios, which show more serious vulnerabilities than previously known. In addition, after performing adversarial training, which takes only 3 times longer than regular training, we can improve the model's robustness significantly.},
	urldate = {2023-08-10},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Ebrahimi, Javid and Lowd, Daniel and Dou, Dejing},
	month = aug,
	year = {2018},
	pages = {653--663},
}

@misc{li_survey_2023,
	title = {A {Survey} on {Out}-of-{Distribution} {Evaluation} of {Neural} {NLP} {Models}},
	url = {http://arxiv.org/abs/2306.15261},
	abstract = {Adversarial robustness, domain generalization and dataset biases are three active lines of research contributing to out-of-distribution (OOD) evaluation on neural NLP models. However, a comprehensive, integrated discussion of the three research lines is still lacking in the literature. In this survey, we 1) compare the three lines of research under a unifying definition; 2) summarize the data-generating processes and evaluation protocols for each line of research; and 3) emphasize the challenges and opportunities for future work.},
	urldate = {2023-08-10},
	publisher = {arXiv},
	author = {Li, Xinzhe and Liu, Ming and Gao, Shang and Buntine, Wray},
	month = jun,
	year = {2023},
	note = {arXiv:2306.15261 [cs]},
}

@article{han_text_2022,
	title = {Text {Adversarial} {Attacks} and {Defenses}: {Issues}, {Taxonomy}, and {Perspectives}},
	volume = {2022},
	issn = {1939-0122, 1939-0114},
	shorttitle = {Text {Adversarial} {Attacks} and {Defenses}},
	url = {https://www.hindawi.com/journals/scn/2022/6458488/},
	doi = {10.1155/2022/6458488},
	abstract = {Deep neural networks (DNNs) have been widely used in many fields due to their powerful representation learning capabilities. However, they are exposed to serious threats caused by the increasing security issues. Adversarial examples were early discovered in computer vision (CV) field when the models were fooled by perturbing the original inputs, and they also exist in natural language processing (NLP) community. However, unlike the image, the text is discrete and semantic in nature, making the generation of adversarial attacks even more difficult. In this work, we provide a comprehensive overview of adversarial attacks and defenses in the textual domain. First, we introduce the pipeline of NLP, including the vector representations of text, DNN-based victim models, and a formal definition of adversarial attacks, which makes our review self-contained. Second, we propose a novel taxonomy for the existing adversarial attacks and defenses, which is fine-grained and closely aligned with practical applications. Finally, we summarize and discuss the major existing issues and further research directions of text adversarial attacks and defenses.},
	language = {en},
	urldate = {2023-08-10},
	journal = {Security and Communication Networks},
	author = {Han, Xu and Zhang, Ying and Wang, Wei and Wang, Bin},
	editor = {Guo, Yanhui},
	month = apr,
	year = {2022},
	pages = {1--25},
}

@article{DBLP:journals/corr/abs-2206-00169,
	title = {Discovering the hidden vocabulary of {DALLE}-2},
	volume = {abs/2206.00169},
	url = {https://doi.org/10.48550/arXiv.2206.00169},
	doi = {10.48550/arXiv.2206.00169},
	journal = {CoRR},
	author = {Daras, Giannis and Dimakis, Alexandros G.},
	year = {2022},
	note = {arXiv: 2206.00169
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/corr/abs-2206-00169.bib
tex.timestamp: Mon, 13 Jun 2022 15:31:50 +0200},
}

@misc{ramesh_zero-shot_2021,
	title = {Zero-{Shot} {Text}-to-{Image} {Generation}},
	shorttitle = {Dall-{E}},
	url = {http://arxiv.org/abs/2102.12092},
	abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
	urldate = {2023-07-28},
	publisher = {arXiv},
	author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2102.12092 [cs]},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-07-28},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
}

@misc{luo_lexlip_2023,
	title = {{LexLIP}: {Lexicon}-{Bottlenecked} {Language}-{Image} {Pre}-{Training} for {Large}-{Scale} {Image}-{Text} {Retrieval}},
	shorttitle = {{LexLIP}},
	url = {http://arxiv.org/abs/2302.02908},
	abstract = {Image-text retrieval (ITR) is a task to retrieve the relevant images/texts, given the query from another modality. The conventional dense retrieval paradigm relies on encoding images and texts into dense representations using dual-stream encoders, however, it faces challenges with low retrieval speed in large-scale retrieval scenarios. In this work, we propose the lexicon-weighting paradigm, where sparse representations in vocabulary space are learned for images and texts to take advantage of the bag-of-words models and efficient inverted indexes, resulting in significantly reduced retrieval latency. A crucial gap arises from the continuous nature of image data, and the requirement for a sparse vocabulary space representation. To bridge this gap, we introduce a novel pre-training framework, Lexicon-Bottlenecked Language-Image Pre-Training (LexLIP), that learns importance-aware lexicon representations. This framework features lexicon-bottlenecked modules between the dual-stream encoders and weakened text decoders, allowing for constructing continuous bag-of-words bottlenecks to learn lexicon-importance distributions. Upon pre-training with same-scale data, our LexLIP achieves state-of-the-art performance on two benchmark ITR datasets, MSCOCO and Flickr30k. Furthermore, in large-scale retrieval scenarios, LexLIP outperforms CLIP with a 5.5 {\textasciitilde} 221.3X faster retrieval speed and 13.2 {\textasciitilde} 48.8X less index storage memory.},
	urldate = {2023-07-27},
	publisher = {arXiv},
	author = {luo, Ziyang and Zhao, Pu and Xu, Can and Geng, Xiubo and Shen, Tao and Tao, Chongyang and Ma, Jing and lin, Qingwen and Jiang, Daxin},
	month = feb,
	year = {2023},
	note = {arXiv:2302.02908 [cs]},
	keywords = {FromKC},
}

@inproceedings{tramer_fundamental_2020,
	title = {Fundamental {Tradeoffs} between {Invariance} and {Sensitivity} to {Adversarial} {Perturbations}},
	url = {https://proceedings.mlr.press/v119/tramer20a.html},
	abstract = {Adversarial examples are malicious inputs crafted to induce misclassification. Commonly studied {\textbackslash}emph\{sensitivity-based\} adversarial examples introduce semantically-small changes to an input that result in a different model prediction. This paper studies a complementary failure mode, {\textbackslash}emph\{invariance-based\} adversarial examples, that introduce minimal semantic changes that modify an input’s true label yet preserve the model’s prediction. We demonstrate fundamental tradeoffs between these two types of adversarial examples. We show that defenses against sensitivity-based attacks actively harm a model’s accuracy on invariance-based attacks, and that new approaches are needed to resist both attack types. In particular, we break state-of-the-art adversarially-trained and {\textbackslash}emph\{certifiably-robust\} models by generating small perturbations that the models are (provably) robust to, yet that change an input’s class according to human labelers. Finally, we formally show that the existence of excessively invariant classifiers arises from the presence of {\textbackslash}emph\{overly-robust\} predictive features in standard datasets.},
	language = {en},
	urldate = {2023-07-27},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tramer, Florian and Behrmann, Jens and Carlini, Nicholas and Papernot, Nicolas and Jacobsen, Joern-Henrik},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {9561--9571},
}

@inproceedings{ghorbani_interpretation_2019,
	title = {Interpretation of {Neural} {Networks} {Is} {Fragile}},
	volume = {33},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4252},
	doi = {10.1609/aaai.v33i01.33013681},
	abstract = {In order for machine learning to be trusted in many applications, it is critical to be able to reliably explain why the machine learning algorithm makes certain predictions. For this reason, a variety of methods have been developed recently to interpret neural network predictions by providing, for example, feature importance maps. For both scientiﬁc robustness and security reasons, it is important to know to what extent can the interpretations be altered by small systematic perturbations to the input data, which might be generated by adversaries or by measurement biases. In this paper, we demonstrate how to generate adversarial perturbations that produce perceptively indistinguishable inputs that are assigned the same predicted label, yet have very different interpretations. We systematically characterize the robustness of interpretations generated by several widely-used feature importance interpretation methods (feature importance maps, integrated gradients, and DeepLIFT) on ImageNet and CIFAR-10. In all cases, our experiments show that systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. inﬂuence functions) are similarly susceptible to adversarial attack. Our analysis of the geometry of the Hessian matrix gives insight on why robustness is a general challenge to current interpretation approaches.},
	language = {en},
	urldate = {2023-07-27},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Ghorbani, Amirata and Abid, Abubakar and Zou, James},
	month = jul,
	year = {2019},
	pages = {3681--3688},
}

@inproceedings{DBLP:conf/iclr/JacobsenBZB19,
	title = {Excessive invariance causes adversarial vulnerability},
	url = {https://openreview.net/forum?id=BkfbpsAcF7},
	booktitle = {7th international conference on learning representations, {ICLR} 2019, new orleans, {LA}, {USA}, may 6-9, 2019},
	publisher = {OpenReview.net},
	author = {Jacobsen, Jörn-Henrik and Behrmann, Jens and Zemel, Richard S. and Bethge, Matthias},
	year = {2019},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/iclr/JacobsenBZB19.bib
tex.timestamp: Thu, 25 Jul 2019 14:25:53 +0200},
}

@misc{pan_unifying_2023,
	title = {Unifying {Large} {Language} {Models} and {Knowledge} {Graphs}: {A} {Roadmap}},
	shorttitle = {Unifying {Large} {Language} {Models} and {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2306.08302},
	abstract = {Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.},
	urldate = {2023-07-26},
	publisher = {arXiv},
	author = {Pan, Shirui and Luo, Linhao and Wang, Yufei and Chen, Chen and Wang, Jiapu and Wu, Xindong},
	month = jun,
	year = {2023},
	note = {arXiv:2306.08302 [cs]},
}

@misc{ye_learning_2018,
	title = {Learning to discover and localize visual objects with open vocabulary},
	url = {http://arxiv.org/abs/1811.10080},
	abstract = {To alleviate the cost of obtaining accurate bounding boxes for training today's state-of-the-art object detection models, recent weakly supervised detection work has proposed techniques to learn from image-level labels. However, requiring discrete image-level labels is both restrictive and suboptimal. Real-world "supervision" usually consists of more unstructured text, such as captions. In this work we learn association maps between images and captions. We then use a novel objectness criterion to rank the resulting candidate boxes, such that high-ranking boxes have strong gradients along all edges. Thus, we can detect objects beyond a fixed object category vocabulary, if those objects are frequent and distinctive enough. We show that our objectness criterion improves the proposed bounding boxes in relation to prior weakly supervised detection methods. Further, we show encouraging results on object detection from image-level captions only.},
	urldate = {2023-07-25},
	publisher = {arXiv},
	author = {Ye, Keren and Zhang, Mingda and Li, Wei and Qin, Danfeng and Kovashka, Adriana and Berent, Jesse},
	month = nov,
	year = {2018},
	note = {arXiv:1811.10080 [cs]},
	keywords = {Classification, Detection, Image, Machine Learning, Open-vocabulary},
}

@misc{fang_uatvr_2023,
	title = {{UATVR}: {Uncertainty}-{Adaptive} {Text}-{Video} {Retrieval}},
	shorttitle = {{UATVR}},
	url = {http://arxiv.org/abs/2301.06309},
	abstract = {With the explosive growth of web videos and emerging large-scale vision-language pre-training models, e.g., CLIP, retrieving videos of interest with text instructions has attracted increasing attention. A common practice is to transfer text-video pairs to the same embedding space and craft cross-modal interactions with certain entities in specific granularities for semantic correspondence. Unfortunately, the intrinsic uncertainties of optimal entity combinations in appropriate granularities for cross-modal queries are understudied, which is especially critical for modalities with hierarchical semantics, e.g., video, text, etc. In this paper, we propose an Uncertainty-Adaptive Text-Video Retrieval approach, termed UATVR, which models each look-up as a distribution matching procedure. Concretely, we add additional learnable tokens in the encoders to adaptively aggregate multi-grained semantics for flexible high-level reasoning. In the refined embedding space, we represent text-video pairs as probabilistic distributions where prototypes are sampled for matching evaluation. Comprehensive experiments on four benchmarks justify the superiority of our UATVR, which achieves new state-of-the-art results on MSR-VTT (50.8\%), VATEX (64.5\%), MSVD (49.7\%), and DiDeMo (45.8\%). The code is available in supplementary materials and will be released publicly soon.},
	urldate = {2023-07-25},
	publisher = {arXiv},
	author = {Fang, Bo and wu, Wenhao and Liu, Chang and Zhou, Yu and Yang, Min and Song, Yuxin and Li, Fu and Wang, Weiping and Ji, Xiangyang and Ouyang, Wanli},
	month = jan,
	year = {2023},
	note = {arXiv:2301.06309 [cs]},
	keywords = {FromKC, Machine Learning, Retrieval, Text, Video},
}

@misc{zhang_simple_2023,
	title = {A {Simple} {Framework} for {Open}-{Vocabulary} {Segmentation} and {Detection}},
	url = {http://arxiv.org/abs/2303.08131},
	abstract = {We present OpenSeeD, a simple Open-vocabulary Segmentation and Detection framework that jointly learns from different segmentation and detection datasets. To bridge the gap of vocabulary and annotation granularity, we first introduce a pre-trained text encoder to encode all the visual concepts in two tasks and learn a common semantic space for them. This gives us reasonably good results compared with the counterparts trained on segmentation task only. To further reconcile them, we locate two discrepancies: \$i\$) task discrepancy -- segmentation requires extracting masks for both foreground objects and background stuff, while detection merely cares about the former; \$ii\$) data discrepancy -- box and mask annotations are with different spatial granularity, and thus not directly interchangeable. To address these issues, we propose a decoupled decoding to reduce the interference between foreground/background and a conditioned mask decoding to assist in generating masks for given boxes. To this end, we develop a simple encoder-decoder model encompassing all three techniques and train it jointly on COCO and Objects365. After pre-training, our model exhibits competitive or stronger zero-shot transferability for both segmentation and detection. Specifically, OpenSeeD beats the state-of-the-art method for open-vocabulary instance and panoptic segmentation across 5 datasets, and outperforms previous work for open-vocabulary detection on LVIS and ODinW under similar settings. When transferred to specific tasks, our model achieves new SoTA for panoptic segmentation on COCO and ADE20K, and instance segmentation on ADE20K and Cityscapes. Finally, we note that OpenSeeD is the first to explore the potential of joint training on segmentation and detection, and hope it can be received as a strong baseline for developing a single model for both tasks in open world.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Zhang, Hao and Li, Feng and Zou, Xueyan and Liu, Shilong and Li, Chunyuan and Gao, Jianfeng and Yang, Jianwei and Zhang, Lei},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08131 [cs]},
	keywords = {FromKC},
}

@misc{jiang_busefficient_2023,
	title = {{BUS}:{Efficient} and {Effective} {Vision}-language {Pre}-training with {Bottom}-{Up} {Patch} {Summarization}},
	shorttitle = {{BUS}},
	url = {http://arxiv.org/abs/2307.08504},
	abstract = {Vision Transformer (ViT) based Vision-Language Pre-training (VLP) models have demonstrated impressive performance in various tasks. However, the lengthy visual token sequences fed into ViT can lead to training inefficiency and ineffectiveness. Existing efforts address the challenge by either bottom-level patch extraction in the ViT backbone or top-level patch abstraction outside, not balancing training efficiency and effectiveness well. Inspired by text summarization in natural language processing, we propose a Bottom-Up Patch Summarization approach named BUS, coordinating bottom-level extraction and top-level abstraction to learn a concise summary of lengthy visual token sequences efficiently. Specifically, We incorporate a Text-Semantics-Aware Patch Selector (TSPS) into the ViT backbone to perform a coarse-grained visual token extraction and then attach a flexible Transformer-based Patch Abstraction Decoder (PAD) upon the backbone for top-level visual abstraction. This bottom-up collaboration enables our BUS to yield high training efficiency while maintaining or even improving effectiveness. We evaluate our approach on various visual-language understanding and generation tasks and show competitive downstream task performance while boosting the training efficiency by 50{\textbackslash}\%. Additionally, our model achieves state-of-the-art performance on many downstream tasks by increasing input image resolution without increasing computational costs over baselines.},
	urldate = {2023-07-25},
	publisher = {arXiv},
	author = {Jiang, Chaoya and Xu, Haiyang and Ye, Wei and Ye, Qinghao and Li, Chenliang and Yan, Ming and Bi, Bin and Zhang, Shikun and Huang, Fei and Huang, Songfang},
	month = jul,
	year = {2023},
	note = {arXiv:2307.08504 [cs]},
}

@misc{yang_learning_2023,
	title = {Learning {Trajectory}-{Word} {Alignments} for {Video}-{Language} {Tasks}},
	url = {http://arxiv.org/abs/2301.01953},
	abstract = {In a video, an object usually appears as the trajectory, i.e., it spans over a few spatial but longer temporal patches, that contains abundant spatiotemporal contexts. However, modern Video-Language BERTs (VDL-BERTs) neglect this trajectory characteristic that they usually follow image-language BERTs (IL-BERTs) to deploy the patch-to-word (P2W) attention that may over-exploit trivial spatial contexts and neglect significant temporal contexts. To amend this, we propose a novel TW-BERT to learn Trajectory-Word alignment by a newly designed trajectory-to-word (T2W) attention for solving video-language tasks. Moreover, previous VDL-BERTs usually uniformly sample a few frames into the model while different trajectories have diverse graininess, i.e., some trajectories span longer frames and some span shorter, and using a few frames will lose certain useful temporal contexts. However, simply sampling more frames will also make pre-training infeasible due to the largely increased training burdens. To alleviate the problem, during the fine-tuning stage, we insert a novel Hierarchical Frame-Selector (HFS) module into the video encoder. HFS gradually selects the suitable frames conditioned on the text context for the later cross-modal encoder to learn better trajectory-word alignments. By the proposed T2W attention and HFS, our TW-BERT achieves SOTA performances on text-to-video retrieval tasks, and comparable performances on video question-answering tasks with some VDL-BERTs trained on much more data. The code will be available in the supplementary material.},
	urldate = {2023-07-25},
	publisher = {arXiv},
	author = {Yang, Xu and Li, Zhangzikang and Xu, Haiyang and Zhang, Hanwang and Ye, Qinghao and Li, Chenliang and Yan, Ming and Zhang, Yu and Huang, Fei and Huang, Songfang},
	month = mar,
	year = {2023},
	note = {arXiv:2301.01953 [cs]},
}

@misc{ye_hitea_2022,
	title = {{HiTeA}: {Hierarchical} {Temporal}-{Aware} {Video}-{Language} {Pre}-training},
	shorttitle = {{HiTeA}},
	url = {http://arxiv.org/abs/2212.14546},
	abstract = {Video-language pre-training has advanced the performance of various downstream video-language tasks. However, most previous methods directly inherit or adapt typical image-language pre-training paradigms to video-language pre-training, thus not fully exploiting the unique characteristic of video, i.e., temporal. In this paper, we propose a Hierarchical Temporal-Aware video-language pre-training framework, HiTeA, with two novel pre-training tasks for modeling cross-modal alignment between moments and texts as well as the temporal relations of video-text pairs. Specifically, we propose a cross-modal moment exploration task to explore moments in videos, which results in detailed video moment representation. Besides, the inherent temporal relations are captured by aligning video-text pairs as a whole in different time resolutions with multi-modal temporal relation exploration task. Furthermore, we introduce the shuffling test to evaluate the temporal reliance of datasets and video-language pre-training models. We achieve state-of-the-art results on 15 well-established video-language understanding and generation tasks, especially on temporal-oriented datasets (e.g., SSv2-Template and SSv2-Label) with 8.6\% and 11.1\% improvement respectively. HiTeA also demonstrates strong generalization ability when directly transferred to downstream tasks in a zero-shot manner. Models and demo will be available on ModelScope.},
	urldate = {2023-07-25},
	publisher = {arXiv},
	author = {Ye, Qinghao and Xu, Guohai and Yan, Ming and Xu, Haiyang and Qian, Qi and Zhang, Ji and Huang, Fei},
	month = dec,
	year = {2022},
	note = {arXiv:2212.14546 [cs]},
	keywords = {Machine Learning, Multimodal, Pretraining, Text, Video},
}

@inproceedings{zhu_dont_2020,
	title = {Don’t {Even} {Look} {Once}: {Synthesizing} {Features} for {Zero}-{Shot} {Detection}},
	shorttitle = {Don’t {Even} {Look} {Once}},
	doi = {10.1109/CVPR42600.2020.01171},
	abstract = {Zero-shot detection, namely, localizing both seen and unseen objects, increasingly gains importance for large-scale applications, with large number of object classes, since, collecting sufficient annotated data with ground truth bounding boxes is simply not scalable. While vanilla deep neural networks deliver high performance for objects available during training, unseen object detection degrades significantly. At a fundamental level, while vanilla detectors are capable of proposing bounding boxes, which include unseen objects, they are often incapable of assigning high-confidence to unseen objects, due to the inherent precision/recall tradeoffs that requires rejecting background objects. We propose a novel detection algorithm “Don't Even Look Once (DELO),” that synthesizes visual features for unseen objects and augments existing training algorithms to incorporate unseen object detection. Our proposed scheme is evaluated on PascalVOC and MSCOCO, and we demonstrate significant improvements in test accuracy over vanilla and other state-of-art zero-shot detectors.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhu, Pengkai and Wang, Hanxiao and Saligrama, Venkatesh},
	month = jun,
	year = {2020},
	note = {ISSN: 2575-7075},
	pages = {11690--11699},
}

@article{zhu_zero_2020,
	title = {Zero {Shot} {Detection}},
	volume = {30},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2019.2899569},
	abstract = {As we move toward large-scale object detection, it is unrealistic to expect annotated training data, in the form of bounding box annotations around objects, for all object classes at sufficient scale; therefore, the methods capable of unseen object detection are required. We propose a novel zero-shot method based on training an end-to-end model that fuses semantic attribute prediction with visual features to propose object bounding boxes for seen and unseen classes. While we utilize semantic features during training, our method is agnostic to semantic information for unseen classes at test-time. Our method retains the efficiency and effectiveness of YOLOv2 for objects seen during training, while improving its performance for novel and unseen objects. The ability of the state-of-the-art detection methods to learn discriminative object features to reject background proposals also limits their performance for unseen objects. We posit that, to detect unseen objects, we must incorporate semantic information into the visual domain so that the learned visual features reflect this information and lead to improved recall rates for unseen objects. We test our method on PASCAL VOC and MS COCO dataset and observed significant improvements on the average precision of unseen classes.},
	number = {4},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Zhu, Pengkai and Wang, Hanxiao and Saligrama, Venkatesh},
	month = apr,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	pages = {998--1010},
}

@inproceedings{ferrari_zero-shot_2018,
	address = {Cham},
	title = {Zero-{Shot} {Object} {Detection}},
	volume = {11205},
	isbn = {978-3-030-01245-8 978-3-030-01246-5},
	url = {https://link.springer.com/10.1007/978-3-030-01246-5_24},
	abstract = {We introduce and tackle the problem of zero-shot object detection (ZSD), which aims to detect object classes which are not observed during training. We work with a challenging set of object classes, not restricting ourselves to similar and/or ﬁne-grained categories as in prior works on zero-shot classiﬁcation. We present a principled approach by ﬁrst adapting visual-semantic embeddings for ZSD. We then discuss the problems associated with selecting a background class and motivate two background-aware approaches for learning robust detectors. One of these models uses a ﬁxed background class and the other is based on iterative latent assignments. We also outline the challenge associated with using a limited number of training classes and propose a solution based on dense sampling of the semantic label space using auxiliary data with a large number of categories. We propose novel splits of two standard detection datasets – MSCOCO and VisualGenome, and present extensive empirical results in both the traditional and generalized zero-shot settings to highlight the beneﬁts of the proposed methods. We provide useful insights into the algorithm and conclude by posing some open questions to encourage further research.},
	language = {en},
	urldate = {2023-07-18},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Bansal, Ankan and Sikka, Karan and Sharma, Gaurav and Chellappa, Rama and Divakaran, Ajay},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01246-5_24},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {397--414},
}

@inproceedings{frome_devise_2013,
	title = {{DeViSE}: {A} {Deep} {Visual}-{Semantic} {Embedding} {Model}},
	volume = {26},
	shorttitle = {{DeViSE}},
	url = {https://papers.nips.cc/paper_files/paper/2013/hash/7cce53cf90577442771720a370c3c723-Abstract.html},
	abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources -- such as text data -- both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions by up to 65\%, achieving hit rates of up to 10\% across thousands of novel labels never seen by the visual model.},
	urldate = {2023-07-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Frome, Andrea and Corrado, Greg S and Shlens, Jon and Bengio, Samy and Dean, Jeff and Ranzato, Marc' Aurelio and Mikolov, Tomas},
	year = {2013},
	keywords = {Detection, Image, Machine Learning, Text, Zero-shot detection},
}

@misc{lin_learning_2022,
	title = {Learning {Object}-{Language} {Alignments} for {Open}-{Vocabulary} {Object} {Detection}},
	url = {http://arxiv.org/abs/2211.14843},
	doi = {10.48550/arXiv.2211.14843},
	abstract = {Existing object detection methods are bounded in a fixed-set vocabulary by costly labeled data. When dealing with novel categories, the model has to be retrained with more bounding box annotations. Natural language supervision is an attractive alternative for its annotation-free attributes and broader object concepts. However, learning open-vocabulary object detection from language is challenging since image-text pairs do not contain fine-grained object-language alignments. Previous solutions rely on either expensive grounding annotations or distilling classification-oriented vision models. In this paper, we propose a novel open-vocabulary object detection framework directly learning from image-text pair data. We formulate object-language alignment as a set matching problem between a set of image region features and a set of word embeddings. It enables us to train an open-vocabulary object detector on image-text pairs in a much simple and effective way. Extensive experiments on two benchmark datasets, COCO and LVIS, demonstrate our superior performance over the competing approaches on novel categories, e.g. achieving 32.0\% mAP on COCO and 21.7\% mask mAP on LVIS. Code is available at: https://github.com/clin1223/VLDet.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Lin, Chuang and Sun, Peize and Jiang, Yi and Luo, Ping and Qu, Lizhen and Haffari, Gholamreza and Yuan, Zehuan and Cai, Jianfei},
	month = nov,
	year = {2022},
	note = {arXiv:2211.14843 [cs]},
}

@misc{chen_open_2022,
	title = {Open {Vocabulary} {Object} {Detection} with {Proposal} {Mining} and {Prediction} {Equalization}},
	url = {http://arxiv.org/abs/2206.11134},
	doi = {10.48550/arXiv.2206.11134},
	abstract = {Open-vocabulary object detection (OVD) aims to scale up vocabulary size to detect objects of novel categories beyond the training vocabulary. Recent work resorts to the rich knowledge in pre-trained vision-language models. However, existing methods are ineffective in proposal-level vision-language alignment. Meanwhile, the models usually suffer from confidence bias toward base categories and perform worse on novel ones. To overcome the challenges, we present MEDet, a novel and effective OVD framework with proposal mining and prediction equalization. First, we design an online proposal mining to refine the inherited vision-semantic knowledge from coarse to fine, allowing for proposal-level detection-oriented feature alignment. Second, based on causal inference theory, we introduce a class-wise backdoor adjustment to reinforce the predictions on novel categories to improve the overall OVD performance. Extensive experiments on COCO and LVIS benchmarks verify the superiority of MEDet over the competing approaches in detecting objects of novel categories, e.g., 32.6\% AP50 on COCO and 22.4\% mask mAP on LVIS.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Chen, Peixian and Sheng, Kekai and Zhang, Mengdan and Lin, Mingbao and Shen, Yunhang and Lin, Shaohui and Ren, Bo and Li, Ke},
	month = nov,
	year = {2022},
	note = {arXiv:2206.11134 [cs]},
}

@misc{long_p3ovd_2022,
	title = {P\${\textasciicircum}3\${OVD}: {Fine}-grained {Visual}-{Text} {Prompt}-{Driven} {Self}-{Training} for {Open}-{Vocabulary} {Object} {Detection}},
	shorttitle = {P\${\textasciicircum}3\${OVD}},
	url = {http://arxiv.org/abs/2211.00849},
	doi = {10.48550/arXiv.2211.00849},
	abstract = {Inspired by the success of visual-language methods (VLMs) in zero-shot classification, recent works attempt to extend this line of work into object detection by leveraging the localization ability of pre-trained VLMs and generating pseudo labels for unseen classes in a self-training manner. However, since the current VLMs are usually pre-trained with aligning sentence embedding with global image embedding, the direct use of them lacks fine-grained alignment for object instances, which is the core of detection. In this paper, we propose a simple but effective Pretrain-adaPt-Pseudo labeling paradigm for Open-Vocabulary Detection (P\${\textasciicircum}3\$OVD) that introduces a fine-grained visual-text prompt adapting stage to enhance the current self-training paradigm with a more powerful fine-grained alignment. During the adapting stage, we enable VLM to obtain fine-grained alignment by using learnable text prompts to resolve an auxiliary dense pixel-wise prediction task. Furthermore, we propose a visual prompt module to provide the prior task information (i.e., the categories need to be predicted) for the vision branch to better adapt the pretrained VLM to the downstream tasks. Experiments show that our method achieves the state-of-the-art performance for open-vocabulary object detection, e.g., 31.5\% mAP on unseen classes of COCO.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Long, Yanxin and Han, Jianhua and Huang, Runhui and Hang, Xu and Zhu, Yi and Xu, Chunjing and Liang, Xiaodan},
	month = nov,
	year = {2022},
	note = {arXiv:2211.00849 [cs]},
}

@inproceedings{DBLP:conf/nips/YaoHWLX0LXX22,
	title = {{DetCLIP}: {Dictionary}-enriched visual-concept paralleled pre-training for open-world detection},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/3ba960559212691be13fa81d9e5e0047-Abstract-Conference.html},
	booktitle = {{NeurIPS}},
	author = {Yao, Lewei and Han, Jianhua and Wen, Youpeng and Liang, Xiaodan and Xu, Dan and Zhang, Wei and Li, Zhenguo and Xu, Chunjing and Xu, Hang},
	year = {2022},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/nips/YaoHWLX0LXX22.bib
tex.timestamp: Thu, 11 May 2023 17:08:21 +0200},
}

@inproceedings{DBLP:conf/nips/Rasheed0K0K22,
	title = {Bridging the gap between object and image-level representations for open-vocabulary detection},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/dabf612543b97ea9c8f46d058d33cf74-Abstract-Conference.html},
	booktitle = {{NeurIPS}},
	author = {Rasheed, Hanoona Abdul and Maaz, Muhammad and Khattak, Muhammad Uzair and Khan, Salman H. and Khan, Fahad Shahbaz},
	year = {2022},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/nips/Rasheed0K0K22.bib
tex.timestamp: Thu, 11 May 2023 17:08:22 +0200},
}

@inproceedings{DBLP:conf/eccv/FengZJCRWXM22,
	series = {Lecture notes in computer science},
	title = {{PromptDet}: {Towards} open-vocabulary detection using uncurated images},
	volume = {13669},
	url = {https://doi.org/10.1007/978-3-031-20077-9_41},
	doi = {10.1007/978-3-031-20077-9\_41},
	booktitle = {Computer vision - {ECCV} 2022 - 17th european conference, tel aviv, israel, october 23-27, 2022, proceedings, part {IX}},
	publisher = {Springer},
	author = {Feng, Chengjian and Zhong, Yujie and Jie, Zequn and Chu, Xiangxiang and Ren, Haibing and Wei, Xiaolin and Xie, Weidi and Ma, Lin},
	editor = {Avidan, Shai and Brostow, Gabriel J. and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/eccv/FengZJCRWXM22.bib
tex.timestamp: Thu, 10 Nov 2022 10:31:50 +0100},
	pages = {701--717},
}

@inproceedings{10.1007/978-3-031-20080-9_42,
	address = {Berlin, Heidelberg},
	title = {Simple open-vocabulary object detection},
	isbn = {978-3-031-20079-3},
	url = {https://doi.org/10.1007/978-3-031-20080-9_42},
	doi = {10.1007/978-3-031-20080-9_42},
	abstract = {Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub .},
	booktitle = {Computer vision – {ECCV} 2022: 17th european conference, tel aviv, israel, october 23–27, 2022, proceedings, part {X}},
	publisher = {Springer-Verlag},
	author = {Minderer, Matthias and Gritsenko, Alexey and Stone, Austin and Neumann, Maxim and Weissenborn, Dirk and Dosovitskiy, Alexey and Mahendran, Aravindh and Arnab, Anurag and Dehghani, Mostafa and Shen, Zhuoran and Wang, Xiao and Zhai, Xiaohua and Kipf, Thomas and Houlsby, Neil},
	year = {2022},
	note = {Number of pages: 28
Place: Tel Aviv, Israel},
	keywords = {CLIP, Contrastive learning, Foundation models, Image-conditioned detection, Image-text models, One-shot object detection, Open-vocabulary detection, Transformer, Vision transformer, Zero-shot detection},
	pages = {728--755},
}

@inproceedings{DBLP:conf/eccv/ZhaoZSZKSCM22,
	series = {Lecture notes in computer science},
	title = {Exploiting unlabeled data with vision and language models for object detection},
	volume = {13669},
	url = {https://doi.org/10.1007/978-3-031-20077-9_10},
	doi = {10.1007/978-3-031-20077-9\_10},
	booktitle = {Computer vision - {ECCV} 2022 - 17th european conference, tel aviv, israel, october 23-27, 2022, proceedings, part {IX}},
	publisher = {Springer},
	author = {Zhao, Shiyu and Zhang, Zhixing and Schulter, Samuel and Zhao, Long and Kumar, B. G. Vijay and Stathopoulos, Anastasis and Chandraker, Manmohan and Metaxas, Dimitris N.},
	editor = {Avidan, Shai and Brostow, Gabriel J. and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/eccv/ZhaoZSZKSCM22.bib
tex.timestamp: Sun, 13 Nov 2022 17:52:09 +0100},
	pages = {159--175},
}

@inproceedings{avidan_detecting_2022,
	address = {Cham},
	title = {Detecting {Twenty}-{Thousand} {Classes} {Using} {Image}-{Level} {Supervision}},
	volume = {13669},
	isbn = {978-3-031-20076-2 978-3-031-20077-9},
	shorttitle = {Detic},
	url = {https://link.springer.com/10.1007/978-3-031-20077-9_21},
	abstract = {Current object detectors are limited in vocabulary size due to the small scale of detection datasets. Image classifiers, on the other hand, reason about much larger vocabularies, as their datasets are larger and easier to collect. We propose Detic, which simply trains the classifiers of a detector on image classification data and thus expands the vocabulary of detectors to tens of thousands of concepts. Unlike prior work, Detic does not need complex assignment schemes to assign image labels to boxes based on model predictions, making it much easier to implement and compatible with a range of detection architectures and backbones. Our results show that Detic yields excellent detectors even for classes without box annotations. It outperforms prior work on both open-vocabulary and long-tail detection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the open-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP when evaluated on all classes, or only rare classes, hence closing the gap in performance for object categories with few samples. For the first time, we train a detector with all the twenty-one-thousand classes of the ImageNet dataset and show that it generalizes to new datasets without finetuning. Code is available at https://github.com/facebookresearch/Detic.},
	language = {en},
	urldate = {2023-01-30},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Zhou, Xingyi and Girdhar, Rohit and Joulin, Armand and Krähenbühl, Philipp and Misra, Ishan},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-20077-9_21},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {ComputerVision, Detection, Image, Machine Learning, Untagged},
	pages = {350--368},
}

@inproceedings{DBLP:conf/eccv/ZangLZHL22,
	series = {Lecture notes in computer science},
	title = {Open-vocabulary {DETR} with conditional matching},
	volume = {13669},
	url = {https://doi.org/10.1007/978-3-031-20077-9_7},
	doi = {10.1007/978-3-031-20077-9\_7},
	booktitle = {Computer vision - {ECCV} 2022 - 17th european conference, tel aviv, israel, october 23-27, 2022, proceedings, part {IX}},
	publisher = {Springer},
	author = {Zang, Yuhang and Li, Wei and Zhou, Kaiyang and Huang, Chen and Loy, Chen Change},
	editor = {Avidan, Shai and Brostow, Gabriel J. and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/eccv/ZangLZHL22.bib
tex.timestamp: Thu, 08 Dec 2022 10:30:38 +0100},
	pages = {106--122},
}

@inproceedings{DBLP:conf/eccv/GaoXN0XLX22,
	series = {Lecture notes in computer science},
	title = {Open vocabulary object detection with pseudo bounding-box labels},
	volume = {13670},
	url = {https://doi.org/10.1007/978-3-031-20080-9_16},
	doi = {10.1007/978-3-031-20080-9\_16},
	booktitle = {Computer vision - {ECCV} 2022 - 17th european conference, tel aviv, israel, october 23-27, 2022, proceedings, part {X}},
	publisher = {Springer},
	author = {Gao, Mingfei and Xing, Chen and Niebles, Juan Carlos and Li, Junnan and Xu, Ran and Liu, Wenhao and Xiong, Caiming},
	editor = {Avidan, Shai and Brostow, Gabriel J. and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/eccv/GaoXN0XLX22.bib
tex.timestamp: Fri, 04 Nov 2022 14:25:40 +0100},
	pages = {266--282},
}

@inproceedings{Du_2022_CVPR,
	title = {Learning to prompt for open-vocabulary object detection with vision-language model},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Du, Yu and Wei, Fangyun and Zhang, Zihe and Shi, Miaojing and Gao, Yue and Li, Guoqi},
	month = jun,
	year = {2022},
	pages = {14084--14093},
}

@misc{zhong_regionclip_2021,
	title = {{RegionCLIP}: {Region}-based {Language}-{Image} {Pretraining}},
	shorttitle = {{RegionCLIP}},
	url = {http://arxiv.org/abs/2112.09106},
	abstract = {Contrastive language-image pretraining (CLIP) using image-text pairs has achieved impressive results on image classification in both zero-shot and transfer learning settings. However, we show that directly applying such models to recognize image regions for object detection leads to poor performance due to a domain shift: CLIP was trained to match an image as a whole to a text description, without capturing the fine-grained alignment between image regions and text spans. To mitigate this issue, we propose a new method called RegionCLIP that significantly extends CLIP to learn region-level visual representations, thus enabling fine-grained alignment between image regions and textual concepts. Our method leverages a CLIP model to match image regions with template captions and then pretrains our model to align these region-text pairs in the feature space. When transferring our pretrained model to the open-vocabulary object detection tasks, our method significantly outperforms the state of the art by 3.8 AP50 and 2.2 AP for novel categories on COCO and LVIS datasets, respectively. Moreoever, the learned region representations support zero-shot inference for object detection, showing promising results on both COCO and LVIS datasets. Our code is available at https://github.com/microsoft/RegionCLIP.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Zhong, Yiwu and Yang, Jianwei and Zhang, Pengchuan and Li, Chunyuan and Codella, Noel and Li, Liunian Harold and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Li, Yin and Gao, Jianfeng},
	month = dec,
	year = {2021},
	note = {arXiv:2112.09106 [cs]},
}

@inproceedings{gu_open-vocabulary_2022,
	title = {Open-vocabulary {Object} {Detection} via {Vision} and {Language} {Knowledge} {Distillation}},
	shorttitle = {{ViLD}},
	url = {https://openreview.net/forum?id=lL3lnMbR4WU},
	abstract = {We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask APr with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 APr. The model can directly transfer to other datasets without finetuning, achieving 72.2 AP50 on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-the-art (Zareian et al., 2021) by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild.},
	language = {en},
	urldate = {2023-01-30},
	author = {Gu, Xiuye and Lin, Tsung-Yi and Kuo, Weicheng and Cui, Yin},
	month = mar,
	year = {2022},
	keywords = {ComputerVision, Detection, FromKC, Image, Machine Learning, Multimodal, Open-vocabulary, Text, Untagged},
}

@article{carionEndtoEndObjectDetection2020,
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	shorttitle = {{DETR}},
	url = {http://arxiv.org/abs/2005.12872},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, eﬀectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a ﬁxed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the ﬁnal set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a uniﬁed manner. We show that it signiﬁcantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2005.12872 [cs]},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	month = may,
	year = {2020},
	note = {arXiv: 2005.12872},
	keywords = {Untagged},
}

@misc{wu_towards_2023,
	title = {Towards {Open} {Vocabulary} {Learning}: {A} {Survey}},
	shorttitle = {Towards {Open} {Vocabulary} {Learning}},
	url = {http://arxiv.org/abs/2306.15880},
	abstract = {In the field of visual scene understanding, deep neural networks have made impressive advancements in various core tasks like segmentation, tracking, and detection. However, most approaches operate on the close-set assumption, meaning that the model can only identify pre-defined categories that are present in the training set. Recently, open vocabulary settings were proposed due to the rapid progress of vision language pre-training. These new approaches seek to locate and recognize categories beyond the annotated label space. The open vocabulary approach is more general, practical, and effective compared to weakly supervised and zero-shot settings. This paper provides a thorough review of open vocabulary learning, summarizing and analyzing recent developments in the field. In particular, we begin by comparing it to related concepts such as zero-shot learning, open-set recognition, and out-of-distribution detection. Then, we review several closely related tasks in the case of segmentation and detection, including long-tail problems, few-shot, and zero-shot settings. For the method survey, we first present the basic knowledge of detection and segmentation in close-set as the preliminary knowledge. Next, we examine various scenarios in which open vocabulary learning is used, identifying common design elements and core ideas. Then, we compare the recent detection and segmentation approaches in commonly used datasets and benchmarks. Finally, we conclude with insights, issues, and discussions regarding future research directions. To our knowledge, this is the first comprehensive literature review of open vocabulary learning. We keep tracing related works at https://github.com/jianzongwu/Awesome-Open-Vocabulary.},
	urldate = {2023-07-17},
	publisher = {arXiv},
	author = {Wu, Jianzong and Li, Xiangtai and Xu, Shilin and Yuan, Haobo and Ding, Henghui and Yang, Yibo and Li, Xia and Zhang, Jiangning and Tong, Yunhai and Jiang, Xudong and Ghanem, Bernard and Tao, Dacheng},
	month = jul,
	year = {2023},
	note = {arXiv:2306.15880 [cs]},
}

@inproceedings{Xue_2023_CVPR,
	title = {{ULIP}: {Learning} a unified representation of language, images, and point clouds for {3D} understanding},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Xue, Le and Gao, Mingfei and Xing, Chen and Martín-Martín, Roberto and Wu, Jiajun and Xiong, Caiming and Xu, Ran and Niebles, Juan Carlos and Savarese, Silvio},
	month = jun,
	year = {2023},
	pages = {1179--1189},
}

@misc{hess_lidarclip_2023,
	title = {{LidarCLIP} or: {How} {I} {Learned} to {Talk} to {Point} {Clouds}},
	shorttitle = {{LidarCLIP} or},
	url = {http://arxiv.org/abs/2212.06858},
	doi = {10.48550/arXiv.2212.06858},
	abstract = {Research connecting text and images has recently seen several breakthroughs, with models like CLIP, DALL-E 2, and Stable Diffusion. However, the connection between text and other visual modalities, such as lidar data, has received less attention, prohibited by the lack of text-lidar datasets. In this work, we propose LidarCLIP, a mapping from automotive point clouds to a pre-existing CLIP embedding space. Using image-lidar pairs, we supervise a point cloud encoder with the image CLIP embeddings, effectively relating text and lidar data with the image domain as an intermediary. We show the effectiveness of LidarCLIP by demonstrating that lidar-based retrieval is generally on par with image-based retrieval, but with complementary strengths and weaknesses. By combining image and lidar features, we improve upon both single-modality methods and enable a targeted search for challenging detection scenarios under adverse sensor conditions. We also explore zero-shot classification and show that LidarCLIP outperforms existing attempts to use CLIP for point clouds by a large margin. Finally, we leverage our compatibility with CLIP to explore a range of applications, such as point cloud captioning and lidar-to-image generation, without any additional training. Code and pre-trained models are available at https://github.com/atonderski/lidarclip.},
	urldate = {2023-07-17},
	publisher = {arXiv},
	author = {Hess, Georg and Tonderski, Adam and Petersson, Christoffer and Åström, Kalle and Svensson, Lennart},
	month = may,
	year = {2023},
	note = {arXiv:2212.06858 [cs]},
}

@misc{zhu_pointclip_2022,
	title = {{PointCLIP} {V2}: {Adapting} {CLIP} for {Powerful} {3D} {Open}-world {Learning}},
	shorttitle = {{PointCLIP} {V2}},
	url = {http://arxiv.org/abs/2211.11682},
	doi = {10.48550/arXiv.2211.11682},
	abstract = {Contrastive Language-Image Pre-training (CLIP) has shown promising open-world performance on 2D image tasks, while its transferred capacity on 3D point clouds, i.e., PointCLIP, is still far from satisfactory. In this work, we propose PointCLIP V2, a powerful 3D open-world learner, to fully unleash the potential of CLIP on 3D point cloud data. First, we introduce a realistic shape projection module to generate more realistic depth maps for CLIP's visual encoder, which is quite efficient and narrows the domain gap between projected point clouds with natural images. Second, we leverage large-scale language models to automatically design a more descriptive 3D-semantic prompt for CLIP's textual encoder, instead of the previous hand-crafted one. Without introducing any training in 3D domains, our approach significantly surpasses PointCLIP by +42.90\%, +40.44\%, and +28.75\% accuracy on three datasets for zero-shot 3D classification. Furthermore, PointCLIP V2 can be extended to few-shot classification, zero-shot part segmentation, and zero-shot 3D object detection in a simple manner, demonstrating our superior generalization ability for 3D open-world learning. Code will be available at https://github.com/yangyangyang127/PointCLIP\_V2.},
	urldate = {2023-07-17},
	publisher = {arXiv},
	author = {Zhu, Xiangyang and Zhang, Renrui and He, Bowei and Zeng, Ziyao and Zhang, Shanghang and Gao, Peng},
	month = nov,
	year = {2022},
	note = {arXiv:2211.11682 [cs]},
}

@misc{huang_clip2point_2022,
	title = {{CLIP2Point}: {Transfer} {CLIP} to {Point} {Cloud} {Classification} with {Image}-{Depth} {Pre}-training},
	shorttitle = {{CLIP2Point}},
	url = {http://arxiv.org/abs/2210.01055},
	doi = {10.48550/arXiv.2210.01055},
	abstract = {Pre-training across 3D vision and language remains under development because of limited training data. Recent works attempt to transfer vision-language pre-training models to 3D vision. PointCLIP converts point cloud data to multi-view depth maps, adopting CLIP for shape classification. However, its performance is restricted by the domain gap between rendered depth maps and images, as well as the diversity of depth distributions. To address this issue, we propose CLIP2Point, an image-depth pre-training method by contrastive learning to transfer CLIP to the 3D domain, and adapt it to point cloud classification. We introduce a new depth rendering setting that forms a better visual effect, and then render 52,460 pairs of images and depth maps from ShapeNet for pre-training. The pre-training scheme of CLIP2Point combines cross-modality learning to enforce the depth features for capturing expressive visual and textual features and intra-modality learning to enhance the invariance of depth aggregation. Additionally, we propose a novel Dual-Path Adapter (DPA) module, i.e., a dual-path structure with simplified adapters for few-shot learning. The dual-path structure allows the joint use of CLIP and CLIP2Point, and the simplified adapter can well fit few-shot tasks without post-search. Experimental results show that CLIP2Point is effective in transferring CLIP knowledge to 3D vision. Our CLIP2Point outperforms PointCLIP and other self-supervised 3D networks, achieving state-of-the-art results on zero-shot and few-shot classification.},
	urldate = {2023-07-17},
	publisher = {arXiv},
	author = {Huang, Tianyu and Dong, Bowen and Yang, Yunhan and Huang, Xiaoshui and Lau, Rynson W. H. and Ouyang, Wanli and Zuo, Wangmeng},
	month = nov,
	year = {2022},
	note = {arXiv:2210.01055 [cs]},
}

@article{zeng_clip2_nodate,
	title = {{CLIP2}: {Contrastive} {Language}-{Image}-{Point} {Pretraining} {From} {Real}-{World} {Point} {Cloud} {Data}},
	language = {en},
	author = {Zeng, Yihan and Jiang, Chenhan and Mao, Jiageng and Han, Jianhua and Ye, Chaoqiang and Huang, Qingqiu and Yeung, Dit-Yan and Yang, Zhen and Liang, Xiaodan and Xu, Hang},
	keywords = {3D, ComputerVision, FromKC, Machine Learning, Multimodal, Point Clouds, Pretraining, Segmentation, Text},
}

@misc{liu_3d_2023,
	title = {{3D} {Open}-vocabulary {Segmentation} with {Foundation} {Models}},
	url = {http://arxiv.org/abs/2305.14093},
	abstract = {Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature significantly as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting the open-vocabulary multimodal knowledge and object reasoning capability of pre-trained foundation models CLIP and DINO, without necessitating any fine-tuning. Specifically, we distill open-vocabulary visual and textual knowledge from CLIP into a neural radiance field (NeRF) which effectively lifts 2D features into view-consistent 3D segmentation. Furthermore, we introduce the Relevancy-Distribution Alignment loss and Feature-Distribution Alignment loss to respectively mitigate the ambiguities of CLIP features and distill precise object boundaries from DINO features, eliminating the need for segmentation annotations during training. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs.},
	urldate = {2023-06-05},
	publisher = {arXiv},
	author = {Liu, Kunhao and Zhan, Fangneng and Zhang, Jiahui and Xu, Muyu and Yu, Yingchen and Saddik, Abdulmotaleb El and Theobalt, Christian and Xing, Eric and Lu, Shijian},
	month = may,
	year = {2023},
	note = {arXiv:2305.14093 [cs]},
	keywords = {3D, FromKC, Machine Learning, Multimodal, Open-vocabulary, Point Clouds, Segmentation},
}

@inproceedings{peng_openscene_2023,
	title = {{OpenScene}: {3D} {Scene} {Understanding} {With} {Open} {Vocabularies}},
	shorttitle = {{OpenScene}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Peng_OpenScene_3D_Scene_Understanding_With_Open_Vocabularies_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-05},
	author = {Peng, Songyou and Genova, Kyle and Jiang, Chiyu “Max” and Tagliasacchi, Andrea and Pollefeys, Marc and Funkhouser, Thomas},
	year = {2023},
	keywords = {3D, FromKC, Machine Learning, Multimodal, Open-vocabulary, Point Clouds, Scene Understanding},
	pages = {815--824},
}

@inproceedings{ding_pla_2023,
	title = {{PLA}: {Language}-{Driven} {Open}-{Vocabulary} {3D} {Scene} {Understanding}},
	shorttitle = {{PLA}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Ding_PLA_Language-Driven_Open-Vocabulary_3D_Scene_Understanding_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-01},
	author = {Ding, Runyu and Yang, Jihan and Xue, Chuhui and Zhang, Wenqing and Bai, Song and Qi, Xiaojuan},
	year = {2023},
	keywords = {3D, FromKC, Machine Learning, Multimodal, Open-vocabulary, Point Clouds, Scene Understanding},
	pages = {7010--7019},
}

@inproceedings{Chen_2023_CVPR,
	title = {{CLIP2Scene}: {Towards} label-efficient {3D} scene understanding by {CLIP}},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Chen, Runnan and Liu, Youquan and Kong, Lingdong and Zhu, Xinge and Ma, Yuexin and Li, Yikang and Hou, Yuenan and Qiao, Yu and Wang, Wenping},
	month = jun,
	year = {2023},
	keywords = {FromKC},
	pages = {7020--7030},
}

@misc{li_semantic-sam_2023,
	title = {Semantic-{SAM}: {Segment} and {Recognize} {Anything} at {Any} {Granularity}},
	shorttitle = {Semantic-{SAM}},
	url = {http://arxiv.org/abs/2307.04767},
	abstract = {In this paper, we introduce Semantic-SAM, a universal image segmentation model to enable segment and recognize anything at any desired granularity. Our model offers two key advantages: semantic-awareness and granularity-abundance. To achieve semantic-awareness, we consolidate multiple datasets across three granularities and introduce decoupled classification for objects and parts. This allows our model to capture rich semantic information. For the multi-granularity capability, we propose a multi-choice learning scheme during training, enabling each click to generate masks at multiple levels that correspond to multiple ground-truth masks. Notably, this work represents the first attempt to jointly train a model on SA-1B, generic, and part segmentation datasets. Experimental results and visualizations demonstrate that our model successfully achieves semantic-awareness and granularity-abundance. Furthermore, combining SA-1B training with other segmentation tasks, such as panoptic and part segmentation, leads to performance improvements. We will provide code and a demo for further exploration and evaluation.},
	urldate = {2023-07-17},
	publisher = {arXiv},
	author = {Li, Feng and Zhang, Hao and Sun, Peize and Zou, Xueyan and Liu, Shilong and Yang, Jianwei and Li, Chunyuan and Zhang, Lei and Gao, Jianfeng},
	month = jul,
	year = {2023},
	note = {arXiv:2307.04767 [cs]},
	keywords = {Untagged},
}

@misc{yin_center-based_2021,
	title = {Center-based {3D} {Object} {Detection} and {Tracking}},
	shorttitle = {{CenterPoint}},
	url = {http://arxiv.org/abs/2006.11275},
	abstract = {Three-dimensional objects are commonly represented as 3D boxes in a point-cloud. This representation mimics the well-studied image-based 2D bounding-box detection but comes with additional challenges. Objects in a 3D world do not follow any particular orientation, and box-based detectors have difficulties enumerating all orientations or fitting an axis-aligned bounding box to rotated objects. In this paper, we instead propose to represent, detect, and track 3D objects as points. Our framework, CenterPoint, first detects centers of objects using a keypoint detector and regresses to other attributes, including 3D size, 3D orientation, and velocity. In a second stage, it refines these estimates using additional point features on the object. In CenterPoint, 3D object tracking simplifies to greedy closest-point matching. The resulting detection and tracking algorithm is simple, efficient, and effective. CenterPoint achieved state-of-the-art performance on the nuScenes benchmark for both 3D detection and tracking, with 65.5 NDS and 63.8 AMOTA for a single model. On the Waymo Open Dataset, CenterPoint outperforms all previous single model method by a large margin and ranks first among all Lidar-only submissions. The code and pretrained models are available at https://github.com/tianweiy/CenterPoint.},
	urldate = {2023-07-17},
	publisher = {arXiv},
	author = {Yin, Tianwei and Zhou, Xingyi and Krähenbühl, Philipp},
	month = jan,
	year = {2021},
	note = {arXiv:2006.11275 [cs]},
	keywords = {3D, ComputerVision, Detection, Machine Learning, Point Clouds},
}

@inproceedings{mothilal_explaining_2020,
	title = {Explaining {Machine} {Learning} {Classifiers} through {Diverse} {Counterfactual} {Explanations}},
	shorttitle = {{DICE}},
	url = {http://arxiv.org/abs/1905.07697},
	doi = {10.1145/3351095.3372850},
	abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
	urldate = {2023-07-14},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Mothilal, Ramaravind Kommiya and Sharma, Amit and Tan, Chenhao},
	month = jan,
	year = {2020},
	note = {arXiv:1905.07697 [cs, stat]},
	pages = {607--617},
}

@misc{ribeiro_why_2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	shorttitle = {{LIME}},
	url = {http://arxiv.org/abs/1602.04938},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	urldate = {2023-07-14},
	publisher = {arXiv},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {arXiv:1602.04938 [cs, stat]
version: 3},
}

@article{deitke_objaverse-xl_nodate,
	title = {Objaverse-{XL}: {A} {Universe} of {10M}+ {3D} {Objects}},
	abstract = {Natural language processing and 2D vision models have attained remarkable proficiency on many tasks primarily by escalating the scale of training data. However, 3D vision tasks have not seen the same progress, in part due to the challenges of acquiring high-quality 3D data. In this work, we present Objaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises deduplicated 3D objects from a diverse set of sources, including manually designed objects, photogrammetry scans of landmarks and everyday items, and professional scans of historic and antique artifacts. Representing the largest scale and diversity in the realm of 3D datasets, Objaverse-XL enables significant new possibilities for 3D vision. Our experiments demonstrate the improvements enabled with the scale provided by Objaverse-XL. We show that by training Zero123 on novel view synthesis, utilizing over 100 million multi-view rendered images, we achieve strong zero-shot generalization abilities. We hope that releasing Objaverse-XL will enable further innovations in the field of 3D vision at scale.},
	language = {en},
	author = {Deitke, Matt and Liu, Ruoshi and Wallingford, Matthew and Ngo, Huong and Michel, Oscar and Kusupati, Aditya and Fan, Alan and Laforte, Christian and Voleti, Vikram and Gadre, Samir Yitzhak and VanderBilt, Eli and Kembhavi, Aniruddha and Vondrick, Carl and Gkioxari, Georgia and Ehsani, Kiana and Schmidt, Ludwig and Farhadi, Ali},
	keywords = {3D, Benchmarks},
}

@misc{chen_voxelnext_2023,
	title = {{VoxelNeXt}: {Fully} {Sparse} {VoxelNet} for {3D} {Object} {Detection} and {Tracking}},
	shorttitle = {{VoxelNeXt}},
	url = {http://arxiv.org/abs/2303.11301},
	abstract = {3D object detectors usually rely on hand-crafted proxies, e.g., anchors or centers, and translate well-studied 2D frameworks to 3D. Thus, sparse voxel features need to be densified and processed by dense prediction heads, which inevitably costs extra computation. In this paper, we instead propose VoxelNext for fully sparse 3D object detection. Our core insight is to predict objects directly based on sparse voxel features, without relying on hand-crafted proxies. Our strong sparse convolutional network VoxelNeXt detects and tracks 3D objects through voxel features entirely. It is an elegant and efficient framework, with no need for sparse-to-dense conversion or NMS post-processing. Our method achieves a better speed-accuracy trade-off than other mainframe detectors on the nuScenes dataset. For the first time, we show that a fully sparse voxel-based representation works decently for LIDAR 3D object detection and tracking. Extensive experiments on nuScenes, Waymo, and Argoverse2 benchmarks validate the effectiveness of our approach. Without bells and whistles, our model outperforms all existing LIDAR methods on the nuScenes tracking test benchmark.},
	urldate = {2023-07-11},
	publisher = {arXiv},
	author = {Chen, Yukang and Liu, Jianhui and Zhang, Xiangyu and Qi, Xiaojuan and Jia, Jiaya},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11301 [cs]},
}

@misc{fan_embracing_2021,
	title = {Embracing {Single} {Stride} {3D} {Object} {Detector} with {Sparse} {Transformer}},
	url = {http://arxiv.org/abs/2112.06375},
	abstract = {In LiDAR-based 3D object detection for autonomous driving, the ratio of the object size to input scene size is significantly smaller compared to 2D detection cases. Overlooking this difference, many 3D detectors directly follow the common practice of 2D detectors, which downsample the feature maps even after quantizing the point clouds. In this paper, we start by rethinking how such multi-stride stereotype affects the LiDAR-based 3D object detectors. Our experiments point out that the downsampling operations bring few advantages, and lead to inevitable information loss. To remedy this issue, we propose Single-stride Sparse Transformer (SST) to maintain the original resolution from the beginning to the end of the network. Armed with transformers, our method addresses the problem of insufficient receptive field in single-stride architectures. It also cooperates well with the sparsity of point clouds and naturally avoids expensive computation. Eventually, our SST achieves state-of-the-art results on the large scale Waymo Open Dataset. It is worth mentioning that our method can achieve exciting performance (83.8 LEVEL 1 AP on validation split) on small object (pedestrian) detection due to the characteristic of single stride. Codes will be released at https://github.com/TuSimple/SST},
	urldate = {2023-07-11},
	publisher = {arXiv},
	author = {Fan, Lue and Pang, Ziqi and Zhang, Tianyuan and Wang, Yu-Xiong and Zhao, Hang and Wang, Feng and Wang, Naiyan and Zhang, Zhaoxiang},
	month = dec,
	year = {2021},
	note = {arXiv:2112.06375 [cs]},
}

@misc{qi_pointnet_2017,
	title = {{PointNet}++: {Deep} {Hierarchical} {Feature} {Learning} on {Point} {Sets} in a {Metric} {Space}},
	shorttitle = {{PointNet}++},
	url = {http://arxiv.org/abs/1706.02413},
	abstract = {Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
	urldate = {2023-07-11},
	publisher = {arXiv},
	author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
	month = jun,
	year = {2017},
	note = {arXiv:1706.02413 [cs]},
}

@inproceedings{DBLP:conf/eccv/ZhangSYH20,
	series = {Lecture notes in computer science},
	title = {{H3DNet}: {3D} object detection using hybrid geometric primitives},
	volume = {12357},
	url = {https://doi.org/10.1007/978-3-030-58610-2_19},
	doi = {10.1007/978-3-030-58610-2\_19},
	booktitle = {Computer vision - {ECCV} 2020 - 16th european conference, glasgow, {UK}, august 23-28, 2020, proceedings, part {XII}},
	publisher = {Springer},
	author = {Zhang, Zaiwei and Sun, Bo and Yang, Haitao and Huang, Qixing},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/eccv/ZhangSYH20.bib
tex.timestamp: Thu, 08 Apr 2021 11:37:55 +0200},
	pages = {311--329},
}

@inproceedings{Qi_2019_ICCV,
	title = {Deep hough voting for {3D} object detection in point clouds},
	booktitle = {Proceedings of the {IEEE}/{CVF} international conference on computer vision ({ICCV})},
	author = {Qi, Charles R. and Litany, Or and He, Kaiming and Guibas, Leonidas J.},
	month = oct,
	year = {2019},
}

@inproceedings{lang_pointpillars_2019,
	address = {Long Beach, CA, USA},
	title = {{PointPillars}: {Fast} {Encoders} for {Object} {Detection} {From} {Point} {Clouds}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{PointPillars}},
	url = {https://ieeexplore.ieee.org/document/8954311/},
	doi = {10.1109/CVPR.2019.01298},
	abstract = {Object detection in point clouds is an important aspect of many robotics applications such as autonomous driving. In this paper, we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline. Recent literature suggests two types of encoders; ﬁxed encoders tend to be fast but sacriﬁce accuracy, while encoders that are learned from data are more accurate, but slower. In this work, we propose PointPillars, a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture, we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous encoders with respect to both speed and accuracy by a large margin. Despite only using lidar, our full detection pipeline signiﬁcantly outperforms the state of the art, even among fusion methods, with respect to both the 3D and bird’s eye view KITTI benchmarks. This detection performance is achieved while running at 62 Hz: a 2 - 4 fold runtime improvement. A faster version of our method matches the state of the art at 105 Hz. These benchmarks suggest that PointPillars is an appropriate encoding for object detection in point clouds.},
	language = {en},
	urldate = {2023-07-11},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lang, Alex H. and Vora, Sourabh and Caesar, Holger and Zhou, Lubing and Yang, Jiong and Beijbom, Oscar},
	month = jun,
	year = {2019},
	pages = {12689--12697},
}

@inproceedings{liu_ssd_2016,
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	volume = {9905},
	shorttitle = {{SSD}},
	url = {http://arxiv.org/abs/1512.02325},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For \$300{\textbackslash}times 300\$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for \$500{\textbackslash}times 500\$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
	urldate = {2023-07-11},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	year = {2016},
	doi = {10.1007/978-3-319-46448-0_2},
	note = {arXiv:1512.02325 [cs]},
	pages = {21--37},
}

@inproceedings{Zhou_2018_CVPR,
	title = {{VoxelNet}: {End}-to-end learning for point cloud based {3D} object detection},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition ({CVPR})},
	author = {Zhou, Yin and Tuzel, Oncel},
	month = jun,
	year = {2018},
}

@article{zou_object_2023,
	title = {Object {Detection} in 20 {Years}: {A} {Survey}},
	volume = {111},
	issn = {1558-2256},
	shorttitle = {Object {Detection} in 20 {Years}},
	doi = {10.1109/JPROC.2023.3238524},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Over the past two decades, we have seen a rapid technological evolution of object detection and its profound impact on the entire computer vision field. If we consider today’s object detection technique as a revolution driven by deep learning, then, back in the 1990s, we would see the ingenious thinking and long-term perspective design of early computer vision. This article extensively reviews this fast-moving research field in the light of technical evolution, spanning over a quarter-century’s time (from the 1990s to 2022). A number of topics have been covered in this article, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speedup techniques, and recent state-of-the-art detection methods.},
	number = {3},
	journal = {Proceedings of the IEEE},
	author = {Zou, Zhengxia and Chen, Keyan and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	month = mar,
	year = {2023},
	note = {Conference Name: Proceedings of the IEEE},
	pages = {257--276},
}

@inproceedings{Kim_2023_CVPR,
	title = {Region-aware pretraining for open-vocabulary object detection with vision transformers},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Kim, Dahun and Angelova, Anelia and Kuo, Weicheng},
	month = jun,
	year = {2023},
	keywords = {FromKC},
	pages = {11144--11154},
}

@inproceedings{Yao_2023_CVPR,
	title = {{DetCLIPv2}: {Scalable} open-vocabulary object detection pre-training via word-region alignment},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Yao, Lewei and Han, Jianhua and Liang, Xiaodan and Xu, Dan and Zhang, Wei and Li, Zhenguo and Xu, Hang},
	month = jun,
	year = {2023},
	keywords = {FromKC},
	pages = {23497--23506},
}

@inproceedings{Ma_2023_CVPR,
	title = {{CAT}: {LoCalization} and {IdentificAtion} cascade detection transformer for open-world object detection},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Ma, Shuailei and Wang, Yuefeng and Wei, Ying and Fan, Jiaqi and Li, Thomas H. and Liu, Hongli and Lv, Fanbing},
	month = jun,
	year = {2023},
	keywords = {FromKC},
	pages = {19681--19690},
}

@inproceedings{Wu_2023_CVPR,
	title = {{CORA}: {Adapting} {CLIP} for open-vocabulary detection with region prompting and anchor pre-matching},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Wu, Xiaoshi and Zhu, Feng and Zhao, Rui and Li, Hongsheng},
	month = jun,
	year = {2023},
	keywords = {FromKC},
	pages = {7031--7040},
}

@inproceedings{Wang_2023_CVPR,
	title = {Object-aware distillation pyramid for open-vocabulary object detection},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Wang, Luting and Liu, Yi and Du, Penghui and Ding, Zihan and Liao, Yue and Qi, Qiaosong and Chen, Biaolong and Liu, Si},
	month = jun,
	year = {2023},
	keywords = {FromKC},
	pages = {11186--11196},
}

@inproceedings{Xu_2023_CVPR,
	title = {Side adapter network for open-vocabulary semantic segmentation},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Xu, Mengde and Zhang, Zheng and Wei, Fangyun and Hu, Han and Bai, Xiang},
	month = jun,
	year = {2023},
	keywords = {FromKC},
	pages = {2945--2954},
}

@misc{kaul_multi-modal_2023,
	title = {Multi-{Modal} {Classifiers} for {Open}-{Vocabulary} {Object} {Detection}},
	url = {http://arxiv.org/abs/2306.05493},
	abstract = {The goal of this paper is open-vocabulary object detection (OVOD) \${\textbackslash}unicode\{x2013\}\$ building a model that can detect objects beyond the set of categories seen at training, thus enabling the user to specify categories of interest at inference without the need for model retraining. We adopt a standard two-stage object detector architecture, and explore three ways for specifying novel categories: via language descriptions, via image exemplars, or via a combination of the two. We make three contributions: first, we prompt a large language model (LLM) to generate informative language descriptions for object classes, and construct powerful text-based classifiers; second, we employ a visual aggregator on image exemplars that can ingest any number of images as input, forming vision-based classifiers; and third, we provide a simple method to fuse information from language descriptions and image exemplars, yielding a multi-modal classifier. When evaluating on the challenging LVIS open-vocabulary benchmark we demonstrate that: (i) our text-based classifiers outperform all previous OVOD works; (ii) our vision-based classifiers perform as well as text-based classifiers in prior work; (iii) using multi-modal classifiers perform better than either modality alone; and finally, (iv) our text-based and multi-modal classifiers yield better performance than a fully-supervised detector.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Kaul, Prannay and Xie, Weidi and Zisserman, Andrew},
	month = jun,
	year = {2023},
	note = {arXiv:2306.05493 [cs]},
	keywords = {ComputerVision, Detection, FromKC, Image, Machine Learning, Multimodal, Open-vocabulary, Text},
}

@inproceedings{wu_aligning_2023,
	title = {Aligning {Bag} of {Regions} for {Open}-{Vocabulary} {Object} {Detection}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Wu_Aligning_Bag_of_Regions_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-02},
	author = {Wu, Size and Zhang, Wenwei and Jin, Sheng and Liu, Wentao and Loy, Chen Change},
	year = {2023},
	keywords = {ComputerVision, Detection, FromKC, Image, Machine Learning, Multimodal, Open-vocabulary, Text},
	pages = {15254--15264},
}

@inproceedings{liang_unknown_2023,
	title = {Unknown {Sniffer} for {Object} {Detection}: {Don}'t {Turn} a {Blind} {Eye} to {Unknown} {Objects}},
	shorttitle = {Unknown {Sniffer} for {Object} {Detection}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Liang_Unknown_Sniffer_for_Object_Detection_Dont_Turn_a_Blind_Eye_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-02},
	author = {Liang, Wenteng and Xue, Feng and Liu, Yihao and Zhong, Guofeng and Ming, Anlong},
	year = {2023},
	keywords = {ComputerVision, Detection, FromKC, Image, Machine Learning, Multimodal, Open-vocabulary, Text},
	pages = {3230--3239},
}

@inproceedings{zareian_open-vocabulary_2021,
	address = {Nashville, TN, USA},
	title = {Open-{Vocabulary} {Object} {Detection} {Using} {Captions}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9577418/},
	doi = {10.1109/CVPR46437.2021.01416},
	abstract = {Despite the remarkable accuracy of deep neural networks in object detection, they are costly to train and scale due to supervision requirements. Particularly, learning more object categories typically requires proportionally more bounding box annotations. Weakly supervised and zero-shot learning techniques have been explored to scale object detectors to more categories with less supervision, but they have not been as successful and widely adopted as supervised models. In this paper, we put forth a novel formulation of the object detection problem, namely openvocabulary object detection, which is more general, more practical, and more effective than weakly supervised and zero-shot approaches. We propose a new method to train object detectors using bounding box annotations for a limited set of object categories, as well as image-caption pairs that cover a larger variety of objects at a signiﬁcantly lower cost. We show that the proposed method can detect and localize objects for which no bounding box annotation is provided during training, at a signiﬁcantly higher accuracy than zero-shot approaches. Meanwhile, objects with bounding box annotation can be detected almost as accurately as supervised methods, which is signiﬁcantly better than weakly supervised baselines. Accordingly, we establish a new state of the art for scalable object detection.},
	language = {en},
	urldate = {2023-01-30},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zareian, Alireza and Rosa, Kevin Dela and Hu, Derek Hao and Chang, Shih-Fu},
	month = jun,
	year = {2021},
	keywords = {ComputerVision, Detection, FromKC, Image, Machine Learning, Multimodal, Open-vocabulary, Text},
	pages = {14388--14397},
}

@inproceedings{mal_open-vocabulary_2022,
	address = {New Orleans, LA, USA},
	title = {Open-{Vocabulary} {One}-{Stage} {Detection} with {Hierarchical} {Visual}-{Language} {Knowledge} {Distillation}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9878533/},
	doi = {10.1109/CVPR52688.2022.01368},
	abstract = {Open-vocabulary object detection aims to detect novel object categories beyond the training set. The advanced open-vocabulary two-stage detectors employ instance-level visual-to-visual knowledge distillation to align the visual space of the detector with the semantic space of the Pretrained Visual-Language Model (PVLM). However, in the more efficient one-stage detector, the absence of classagnostic object proposals hinders the knowledge distillation on unseen objects, leading to severe performance degradation. In this paper, we propose a hierarchical visual-language knowledge distillation method, i.e., HierKD, for open-vocabulary one-stage detection. Specifically, a global-level knowledge distillation is explored to transfer the knowledge of unseen categories from the PVLM to the detector. Moreover, we combine the proposed globallevel knowledge distillation and the common instance-level knowledge distillation to learn the knowledge of seen and unseen categories simultaneously. Extensive experiments on MS-COCO show that our method significantly surpasses the previous best one-stage detector with 11.9\% and 6.7\% AP50 gains under the zero-shot detection and generalized zero-shot detection settings, and reduces the AP50 performance gap from 14\% to 7.3\% compared to the best twostage detector. Code will be released at this url 1.},
	language = {en},
	urldate = {2023-07-10},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Mal, Zongyang and Luo, Guan and Gao, Jin and Li, Liang and Chen, Yuxin and Wang, Shaoru and Zhang, Congxuan and Hu, Weiming},
	month = jun,
	year = {2022},
	keywords = {ComputerVision, Detection, FromKC, Image, Machine Learning, Multimodal, One-Stage, Open-vocabulary, Text},
	pages = {14054--14063},
}

@inproceedings{Wang_2023_CVPR,
	title = {{DSVT}: {Dynamic} sparse voxel transformer with rotated sets},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Wang, Haiyang and Shi, Chen and Shi, Shaoshuai and Lei, Meng and Wang, Sen and He, Di and Schiele, Bernt and Wang, Liwei},
	month = jun,
	year = {2023},
	keywords = {3D, Detection, FromKC, Machine Learning, Outdoor, Point Clouds},
	pages = {13520--13529},
}

@inproceedings{Brooks_2023_CVPR,
	title = {{InstructPix2Pix}: {Learning} to follow image editing instructions},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Brooks, Tim and Holynski, Aleksander and Efros, Alexei A.},
	month = jun,
	year = {2023},
	keywords = {Editing, FromKC, Image, Machine Learning, Manipulation, Multimodal, Text},
	pages = {18392--18402},
}

@misc{ding_longnet_2023,
	title = {{LongNet}: {Scaling} {Transformers} to 1,000,000,000 {Tokens}},
	shorttitle = {{LongNet}},
	url = {http://arxiv.org/abs/2307.02486},
	abstract = {Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. In this work, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between tokens; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.},
	urldate = {2023-07-08},
	publisher = {arXiv},
	author = {Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Wei, Furu},
	month = jul,
	year = {2023},
	note = {arXiv:2307.02486 [cs]},
	keywords = {LLMs, Language, LongRange, Machine Learning, NLP},
}

@inproceedings{misra_end--end_2021,
	address = {Montreal, QC, Canada},
	title = {An {End}-to-{End} {Transformer} {Model} for {3D} {Object} {Detection}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9711345/},
	doi = {10.1109/ICCV48922.2021.00290},
	abstract = {We propose 3DETR, an end-to-end Transformer based object detection model for 3D point clouds. Compared to existing detection methods that employ a number of 3Dspeciﬁc inductive biases, 3DETR requires minimal modiﬁcations to the vanilla Transformer block. Speciﬁcally, we ﬁnd that a standard Transformer with non-parametric queries and Fourier positional embeddings is competitive with specialized architectures that employ libraries of 3Dspeciﬁc operators with hand-tuned hyperparameters. Nevertheless, 3DETR is conceptually simple and easy to implement, enabling further improvements by incorporating 3D domain knowledge. Through extensive experiments, we show 3DETR outperforms the well-established and highly optimized VoteNet baselines on the challenging ScanNetV2 dataset by 9.5\%. Furthermore, we show 3DETR is applicable to 3D tasks beyond detection, and can serve as a building block for future research.},
	language = {en},
	urldate = {2023-07-08},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Misra, Ishan and Girdhar, Rohit and Joulin, Armand},
	month = oct,
	year = {2021},
	keywords = {3D, Detection, FromKC, Machine Learning, Point Clouds},
	pages = {2886--2897},
}

@misc{lee_diffusion_2023,
	title = {Diffusion {Probabilistic} {Models} for {Scene}-{Scale} {3D} {Categorical} {Data}},
	url = {http://arxiv.org/abs/2301.00527},
	abstract = {In this paper, we learn a diffusion model to generate 3D data on a scene-scale. Specifically, our model crafts a 3D scene consisting of multiple objects, while recent diffusion research has focused on a single object. To realize our goal, we represent a scene with discrete class labels, i.e., categorical distribution, to assign multiple objects into semantic categories. Thus, we extend discrete diffusion models to learn scene-scale categorical distributions. In addition, we validate that a latent diffusion model can reduce computation costs for training and deploying. To the best of our knowledge, our work is the first to apply discrete and latent diffusion for 3D categorical data on a scene-scale. We further propose to perform semantic scene completion (SSC) by learning a conditional distribution using our diffusion model, where the condition is a partial observation in a sparse point cloud. In experiments, we empirically show that our diffusion models not only generate reasonable scenes, but also perform the scene completion task better than a discriminative model. Our code and models are available at https://github.com/zoomin-lee/scene-scale-diffusion},
	urldate = {2023-07-08},
	publisher = {arXiv},
	author = {Lee, Jumin and Im, Woobin and Lee, Sebin and Yoon, Sung-Eui},
	month = jan,
	year = {2023},
	note = {arXiv:2301.00527 [cs]},
	keywords = {3D, Machine Learning, Point Clouds, Scene Generation},
}

@inproceedings{zheng_se-ssd_2021,
	address = {Nashville, TN, USA},
	title = {{SE}-{SSD}: {Self}-{Ensembling} {Single}-{Stage} {Object} {Detector} {From} {Point} {Cloud}},
	isbn = {978-1-66544-509-2},
	shorttitle = {{SE}-{SSD}},
	url = {https://ieeexplore.ieee.org/document/9578242/},
	doi = {10.1109/CVPR46437.2021.01426},
	abstract = {We present Self-Ensembling Single-Stage object Detector (SE-SSD) for accurate and efﬁcient 3D object detection in outdoor point clouds. Our key focus is on exploiting both soft and hard targets with our formulated constraints to jointly optimize the model, without introducing extra computation in the inference. Speciﬁcally, SE-SSD contains a pair of teacher and student SSDs, in which we design an effective IoU-based matching strategy to ﬁlter soft targets from the teacher and formulate a consistency loss to align student predictions with them. Also, to maximize the distilled knowledge for ensembling the teacher, we design a new augmentation scheme to produce shape-aware augmented samples to train the student, aiming to encourage it to infer complete object shapes. Lastly, to better exploit hard targets, we design an ODIoU loss to supervise the student with constraints on the predicted box centers and orientations. Our SE-SSD attains top performance compared with all prior published works. Also, it attains top precisions for car detection in the KITTI benchmark (ranked 1st and 2nd on the BEV and 3D leaderboards1, respectively) with an ultra-high inference speed. The code is available at https://github.com/Vegeta2020/SE-SSD.},
	language = {en},
	urldate = {2023-07-08},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zheng, Wu and Tang, Weiliang and Jiang, Li and Fu, Chi-Wing},
	month = jun,
	year = {2021},
	keywords = {3D, Detection, FromKC, Machine Learning, Point Clouds},
	pages = {14489--14498},
}

@misc{noauthor_deci-aisuper-gradientsyolo-nas_2023,
	title = {Deci-{AI}/super-gradients/{YOLO}-{NAS}},
	copyright = {Apache-2.0},
	url = {https://github.com/Deci-AI/super-gradients},
	abstract = {Easily train or fine-tune SOTA computer vision models with one open source training library. The home of Yolo-NAS.},
	urldate = {2023-07-08},
	publisher = {deci.ai},
	month = jul,
	year = {2023},
	note = {original-date: 2021-11-28T07:58:02Z},
}

@inproceedings{dokania_idd-3d_2023,
	address = {Waikoloa, HI, USA},
	title = {{IDD}-{3D}: {Indian} {Driving} {Dataset} for {3D} {Unstructured} {Road} {Scenes}},
	isbn = {978-1-66549-346-8},
	shorttitle = {{IDD}-{3D}},
	url = {https://ieeexplore.ieee.org/document/10030865/},
	doi = {10.1109/WACV56688.2023.00446},
	abstract = {Autonomous driving and assistance systems rely on annotated data from traffic and road scenarios to model and learn the various object relations in complex real-world scenarios. Preparation and training of deploy-able deep learning architectures require the models to be suited to different traffic scenarios and adapt to different situations. Currently, existing datasets, while large-scale, lack such diversities and are geographically biased towards mainly developed cities. An unstructured and complex driving layout found in several developing countries such as India poses a challenge to these models due to the sheer degree of variations in the object types, densities, and locations. To facilitate better research toward accommodating such scenarios, we build a new dataset, IDD-3D, which consists of multimodal data from multiple cameras and LiDAR sensors with 12k annotated driving LiDAR frames across various traffic scenarios. We discuss the need for this dataset through statistical comparisons with existing datasets and highlight benchmarks on standard 3D object detection and tracking tasks in complex layouts. Code and data available 1.},
	language = {en},
	urldate = {2023-07-08},
	booktitle = {2023 {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Dokania, Shubham and Hafez, A. H. Abdul and Subramanian, Anbumani and Chandraker, Manmohan and Jawahar, C.V.},
	month = jan,
	year = {2023},
	pages = {4471--4480},
}

@inproceedings{Zhang_2023_CVPR,
	title = {Towards unsupervised object detection from {LiDAR} point clouds},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Zhang, Lunjun and Yang, Anqi Joyce and Xiong, Yuwen and Casas, Sergio and Yang, Bin and Ren, Mengye and Urtasun, Raquel},
	month = jun,
	year = {2023},
	pages = {9317--9328},
}

@incollection{vedaldi_improving_2020,
	address = {Cham},
	title = {Improving {3D} {Object} {Detection} {Through} {Progressive} {Population} {Based} {Augmentation}},
	volume = {12366},
	isbn = {978-3-030-58588-4 978-3-030-58589-1},
	url = {https://link.springer.com/10.1007/978-3-030-58589-1_17},
	abstract = {Data augmentation has been widely adopted for object detection in 3D point clouds. However, all previous related eﬀorts have focused on manually designing speciﬁc data augmentation methods for individual architectures. In this work, we present the ﬁrst attempt to automate the design of data augmentation policies for 3D object detection. We introduce the Progressive Population Based Augmentation (PPBA) algorithm, which learns to optimize augmentation strategies by narrowing down the search space and adopting the best parameters discovered in previous iterations. On the KITTI 3D detection test set, PPBA improves the StarNet detector by substantial margins on the moderate diﬃculty category of cars, pedestrians, and cyclists, outperforming all current state-of-the-art single-stage detection models. Additional experiments on the Waymo Open Dataset indicate that PPBA continues to eﬀectively improve the StarNet and PointPillars detectors on a 20x larger dataset compared to KITTI. The magnitude of the improvements may be comparable to advances in 3D perception architectures and the gains come without an incurred cost at inference time. In subsequent experiments, we ﬁnd that PPBA may be up to 10x more data eﬃcient than baseline 3D detection models without augmentation, highlighting that 3D detection models may achieve competitive accuracy with far fewer labeled examples.},
	language = {en},
	urldate = {2023-07-08},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Cheng, Shuyang and Leng, Zhaoqi and Cubuk, Ekin Dogus and Zoph, Barret and Bai, Chunyan and Ngiam, Jiquan and Song, Yang and Caine, Benjamin and Vasudevan, Vijay and Li, Congcong and Le, Quoc V. and Shlens, Jonathon and Anguelov, Dragomir},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58589-1_17},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {3D, Augmentation, Detection, Machine Learning, Point Clouds},
	pages = {279--294},
}

@incollection{avidan_image2point_2022,
	address = {Cham},
	title = {{Image2Point}: {3D} {Point}-{Cloud} {Understanding} with {2D} {Image} {Pretrained} {Models}},
	volume = {13697},
	isbn = {978-3-031-19835-9 978-3-031-19836-6},
	shorttitle = {{Image2Point}},
	url = {https://link.springer.com/10.1007/978-3-031-19836-6_36},
	language = {en},
	urldate = {2023-07-08},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Xu, Chenfeng and Yang, Shijia and Galanti, Tomer and Wu, Bichen and Yue, Xiangyu and Zhai, Bohan and Zhan, Wei and Vajda, Peter and Keutzer, Kurt and Tomizuka, Masayoshi},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-19836-6_36},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {3D, ComputerVision, Machine Learning, Point Clouds, VFM},
	pages = {638--656},
}

@article{an_rs-aug_2023,
	title = {{RS}-{Aug}: {Improve} {3D} {Object} {Detection} on {LiDAR} {With} {Realistic} {Simulator} {Based} {Data} {Augmentation}},
	issn = {1558-0016},
	shorttitle = {{RS}-{Aug}},
	doi = {10.1109/TITS.2023.3266727},
	abstract = {Light detection and ranging (LiDAR) is an essential sensor for three dimensional (3D) object detection via generating 3D point cloud of the surroundings, and it has been widely used in the various visual applications, especially autonomous driving. However, limited numbers of labeled LiDAR datasets brutally restrain the development of 3D object detector, and this situation breeds an urgent demand on data augmentation in this field. By far, most of the traditional methods reuse the labeled samples, while those unlabeled are hastily untaken. Motivated by this, we propose a Realistic Simulator based data augmentation (RS-Aug). It aims to construct augmented real scenes to enrich the diversity of training dataset. To train 3D object detector in a supervised learning way, the first step of RS-Aug is auto-annotation. Time-continuous LiDAR frames are used to construct the dense scene, which is beneficial to annotation and the subsequent rendering augmentation. However, 3D points with incorrect semantic labels are naturally gathered during multi-view reconstruction, causing the negative effect on auto-annotation. We propose an algorithm of cluster guided k -nearest neighbor (c- k NN). It emphasizes on de-nosing semantic labels of clustered points using distance and intensity constraints. Then, the next step of RS-Aug is rendering augmentation on the real scene. To enhance the rendering quality using collision and distance constraints with the less computation complexity, we propose a scheme of heuristic search (HS) based object insertion. It estimates the proper position of the inserted object from 2D bird’s eye view (BEV). Experiments demonstrate the de-noising accuracy of c- k NN, rendering quality of HS based object insertion, and improvement of RS-Aug on object detection.},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {An, Pei and Liang, Junxiong and Ma, Jie and Chen, Yanfei and Wang, Liheng and Yang, You and Liu, Qiong},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {3D, Augmentation, Detection, Machine Learning, Point Clouds},
	pages = {1--12},
}

@misc{zhu_pointclip_2022-1,
	title = {{PointCLIP} {V2}: {Adapting} {CLIP} for {Powerful} {3D} {Open}-world {Learning}},
	shorttitle = {{PointCLIP} {V2}},
	url = {http://arxiv.org/abs/2211.11682},
	abstract = {Contrastive Language-Image Pre-training (CLIP) has shown promising open-world performance on 2D image tasks, while its transferred capacity on 3D point clouds, i.e., PointCLIP, is still far from satisfactory. In this work, we propose PointCLIP V2, a powerful 3D open-world learner, to fully unleash the potential of CLIP on 3D point cloud data. First, we introduce a realistic shape projection module to generate more realistic depth maps for CLIP's visual encoder, which is quite efficient and narrows the domain gap between projected point clouds with natural images. Second, we leverage large-scale language models to automatically design a more descriptive 3D-semantic prompt for CLIP's textual encoder, instead of the previous hand-crafted one. Without introducing any training in 3D domains, our approach significantly surpasses PointCLIP by +42.90\%, +40.44\%, and +28.75\% accuracy on three datasets for zero-shot 3D classification. Furthermore, PointCLIP V2 can be extended to few-shot classification, zero-shot part segmentation, and zero-shot 3D object detection in a simple manner, demonstrating our superior generalization ability for 3D open-world learning. Code will be available at https://github.com/yangyangyang127/PointCLIP\_V2.},
	urldate = {2023-07-08},
	publisher = {arXiv},
	author = {Zhu, Xiangyang and Zhang, Renrui and He, Bowei and Zeng, Ziyao and Zhang, Shanghang and Gao, Peng},
	month = nov,
	year = {2022},
	note = {arXiv:2211.11682 [cs]},
	keywords = {3D, Classification, ComputerVision, Machine Learning, Point Clouds, Text},
}

@misc{qi_pointnet_2017-1,
	title = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	shorttitle = {{PointNet}},
	url = {http://arxiv.org/abs/1612.00593},
	abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	urldate = {2022-10-06},
	publisher = {arXiv},
	author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
	month = apr,
	year = {2017},
	note = {arXiv:1612.00593 [cs]},
	keywords = {3D, Classification, ComputerVision, Machine Learning, Point Clouds},
}

@misc{lu_open-vocabulary_2022,
	title = {Open-{Vocabulary} {3D} {Detection} via {Image}-level {Class} and {Debiased} {Cross}-modal {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2207.01987},
	abstract = {Current point-cloud detection methods have difficulty detecting the open-vocabulary objects in the real world, due to their limited generalization capability. Moreover, it is extremely laborious and expensive to collect and fully annotate a point-cloud detection dataset with numerous classes of objects, leading to the limited classes of existing point-cloud datasets and hindering the model to learn general representations to achieve open-vocabulary point-cloud detection. As far as we know, we are the first to study the problem of open-vocabulary 3D point-cloud detection. Instead of seeking a point-cloud dataset with full labels, we resort to ImageNet1K to broaden the vocabulary of the point-cloud detector. We propose OV-3DETIC, an Open-Vocabulary 3D DETector using Image-level Class supervision. Specifically, we take advantage of two modalities, the image modality for recognition and the point-cloud modality for localization, to generate pseudo labels for unseen classes. Then we propose a novel debiased cross-modal contrastive learning method to transfer the knowledge from image modality to point-cloud modality during training. Without hurting the latency during inference, OV-3DETIC makes the point-cloud detector capable of achieving open-vocabulary detection. Extensive experiments demonstrate that the proposed OV-3DETIC achieves at least 10.77 \% mAP improvement (absolute value) and 9.56 \% mAP improvement (absolute value) by a wide range of baselines on the SUN-RGBD dataset and ScanNet dataset, respectively. Besides, we conduct sufficient experiments to shed light on why the proposed OV-3DETIC works.},
	urldate = {2023-07-08},
	publisher = {arXiv},
	author = {Lu, Yuheng and Xu, Chenfeng and Wei, Xiaobao and Xie, Xiaodong and Tomizuka, Masayoshi and Keutzer, Kurt and Zhang, Shanghang},
	month = jul,
	year = {2022},
	note = {arXiv:2207.01987 [cs]},
}

@misc{sanchez_domain_2023,
	title = {Domain generalization of {3D} semantic segmentation in autonomous driving},
	url = {http://arxiv.org/abs/2212.04245},
	abstract = {Using deep learning, 3D autonomous driving semantic segmentation has become a well-studied subject, with methods that can reach very high performance. Nonetheless, because of the limited size of the training datasets, these models cannot see every type of object and scene found in real-world applications. The ability to be reliable in these various unknown environments is called domain generalization. Despite its importance, domain generalization is relatively unexplored in the case of 3D autonomous driving semantic segmentation. To fill this gap, this paper presents the first benchmark for this application by testing state-of-the-art methods and discussing the difficulty of tackling Laser Imaging Detection and Ranging (LiDAR) domain shifts. We also propose the first method designed to address this domain generalization, which we call 3DLabelProp. This method relies on leveraging the geometry and sequentiality of the LiDAR data to enhance its generalization performances by working on partially accumulated point clouds. It reaches a mean Intersection over Union (mIoU) of 50.4\% on SemanticPOSS and of 55.2\% on PandaSet solid-state LiDAR while being trained only on SemanticKITTI, making it the state-of-the-art method for generalization (+5\% and +33\% better, respectively, than the second best method). The code for this method will be available on GitHub.},
	urldate = {2023-07-08},
	publisher = {arXiv},
	author = {Sanchez, Jules and Deschaud, Jean-Emmanuel and Goulette, Francois},
	month = mar,
	year = {2023},
	note = {arXiv:2212.04245 [cs]},
	keywords = {3D, ComputerVision, Generalization, Machine Learning, Outdoor, Point Clouds, Segmentation},
}

@article{xiao_3d_nodate,
	title = {{3D} {Semantic} {Segmentation} in the {Wild}: {Learning} {Generalized} {Models} for {Adverse}-{Condition} {Point} {Clouds}},
	language = {en},
	author = {Xiao, Aoran and Huang, Jiaxing and Xuan, Weihao and Ren, Ruijie and Liu, Kangcheng and Guan, Dayan and Saddik, Abdulmotaleb El and Lu, Shijian and Xing, Eric P},
	keywords = {3D, Adversarial, AdverseWeather, Augmentation, ComputerVision, Machine Learning, Point Clouds, Segmentation},
}

@inproceedings{lehner_3d-vfield_2022,
	address = {New Orleans, LA, USA},
	title = {{3D}-{VField}: {Adversarial} {Augmentation} of {Point} {Clouds} for {Domain} {Generalization} in {3D} {Object} {Detection}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{3D}-{VField}},
	url = {https://ieeexplore.ieee.org/document/9879166/},
	doi = {10.1109/CVPR52688.2022.01678},
	abstract = {As 3D object detection on point clouds relies on the geometrical relationships between the points, non-standard object shapes can hinder a method’s detection capability. However, in safety-critical settings, robustness to out-ofdomain and long-tail samples is fundamental to circumvent dangerous issues, such as the misdetection of damaged or rare cars. In this work, we substantially improve the generalization of 3D object detectors to out-of-domain data by deforming point clouds during training. We achieve this with 3D-VField: a novel data augmentation method that plausibly deforms objects via vector ﬁelds learned in an adversarial fashion. Our approach constrains 3D points to slide along their sensor view rays while neither adding nor removing any of them. The obtained vectors are transferable, sample-independent and preserve shape and occlusions. Despite training only on a standard dataset, such as KITTI, augmenting with our vector ﬁelds signiﬁcantly improves the generalization to differently shaped objects and scenes. Towards this end, we propose and share CrashD: a synthetic dataset of realistic damaged and rare cars, with a variety of crash scenarios. Extensive experiments on KITTI, Waymo, our CrashD and SUN RGB-D show the generalizability of our techniques to out-of-domain data, different models and sensors, namely LiDAR and ToF cameras, for both indoor and outdoor scenes. Our CrashD dataset is available at https://crashd-cars.github.io.},
	language = {en},
	urldate = {2023-07-08},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lehner, Alexander and Gasperini, Stefano and Marcos-Ramiro, Alvaro and Schmidt, Michael and Mahani, Mohammad-Ali Nikouei and Navab, Nassir and Busam, Benjamin and Tombari, Federico},
	month = jun,
	year = {2022},
	keywords = {3D, Adversarial, Augmentation, ComputerVision, Detection, Machine Learning, Point Clouds},
	pages = {17274--17283},
}

@misc{chen_towards_2023,
	title = {Towards {Label}-free {Scene} {Understanding} by {Vision} {Foundation} {Models}},
	url = {http://arxiv.org/abs/2306.03899},
	doi = {10.48550/arXiv.2306.03899},
	abstract = {Vision foundation models such as Contrastive Vision-Language Pre-training (CLIP) and Segment Anything (SAM) have demonstrated impressive zero-shot performance on image classification and segmentation tasks. However, the incorporation of CLIP and SAM for label-free scene understanding has yet to be explored. In this paper, we investigate the potential of vision foundation models in enabling networks to comprehend 2D and 3D worlds without labelled data. The primary challenge lies in effectively supervising networks under extremely noisy pseudo labels, which are generated by CLIP and further exacerbated during the propagation from the 2D to the 3D domain. To tackle these challenges, we propose a novel Cross-modality Noisy Supervision (CNS) method that leverages the strengths of CLIP and SAM to supervise 2D and 3D networks simultaneously. In particular, we introduce a prediction consistency regularization to co-train 2D and 3D networks, then further impose the networks' latent space consistency using the SAM's robust feature representation. Experiments conducted on diverse indoor and outdoor datasets demonstrate the superior performance of our method in understanding 2D and 3D open environments. Our 2D and 3D network achieves label-free semantic segmentation with 28.4\% and 33.5\% mIoU on ScanNet, improving 4.7\% and 7.9\%, respectively. And for nuScenes dataset, our performance is 26.8\% with an improvement of 6\%. Code will be released (https://github.com/runnanchen/Label-Free-Scene-Understanding).},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Chen, Runnan and Liu, Youquan and Kong, Lingdong and Chen, Nenglun and Zhu, Xinge and Ma, Yuexin and Liu, Tongliang and Wang, Wenping},
	month = jun,
	year = {2023},
	note = {arXiv:2306.03899 [cs]},
}

@misc{yang_sam3d_2023,
	title = {{SAM3D}: {Segment} {Anything} in {3D} {Scenes}},
	shorttitle = {{SAM3D}},
	url = {http://arxiv.org/abs/2306.03908},
	doi = {10.48550/arXiv.2306.03908},
	abstract = {In this work, we propose SAM3D, a novel framework that is able to predict masks in 3D point clouds by leveraging the Segment-Anything Model (SAM) in RGB images without further training or finetuning. For a point cloud of a 3D scene with posed RGB images, we first predict segmentation masks of RGB images with SAM, and then project the 2D masks into the 3D points. Later, we merge the 3D masks iteratively with a bottom-up merging approach. At each step, we merge the point cloud masks of two adjacent frames with the bidirectional merging approach. In this way, the 3D masks predicted from different frames are gradually merged into the 3D masks of the whole 3D scene. Finally, we can optionally ensemble the result from our SAM3D with the over-segmentation results based on the geometric information of the 3D scenes. Our approach is experimented with ScanNet dataset and qualitative results demonstrate that our SAM3D achieves reasonable and fine-grained 3D segmentation results without any training or finetuning of SAM.},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Yang, Yunhan and Wu, Xiaoyang and He, Tong and Zhao, Hengshuang and Liu, Xihui},
	month = jun,
	year = {2023},
	note = {arXiv:2306.03908 [cs]},
}

@misc{cen_sad_2023,
	title = {{SAD}: {Segment} {Any} {RGBD}},
	shorttitle = {{SAD}},
	url = {http://arxiv.org/abs/2305.14207},
	doi = {10.48550/arXiv.2305.14207},
	abstract = {The Segment Anything Model (SAM) has demonstrated its effectiveness in segmenting any part of 2D RGB images. However, SAM exhibits a stronger emphasis on texture information while paying less attention to geometry information when segmenting RGB images. To address this limitation, we propose the Segment Any RGBD (SAD) model, which is specifically designed to extract geometry information directly from images. Inspired by the natural ability of humans to identify objects through the visualization of depth maps, SAD utilizes SAM to segment the rendered depth map, thus providing cues with enhanced geometry information and mitigating the issue of over-segmentation. We further include the open-vocabulary semantic segmentation in our framework, so that the 3D panoptic segmentation is fulfilled. The project is available on https://github.com/Jun-CEN/SegmentAnyRGBD.},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Cen, Jun and Wu, Yizheng and Wang, Kewei and Li, Xingyi and Yang, Jingkang and Pei, Yixuan and Kong, Lingdong and Liu, Ziwei and Chen, Qifeng},
	month = may,
	year = {2023},
	note = {arXiv:2305.14207 [cs]},
}

@misc{sautier_image--lidar_2022,
	title = {Image-to-{Lidar} {Self}-{Supervised} {Distillation} for {Autonomous} {Driving} {Data}},
	url = {http://arxiv.org/abs/2203.16258},
	abstract = {Segmenting or detecting objects in sparse Lidar point clouds are two important tasks in autonomous driving to allow a vehicle to act safely in its 3D environment. The best performing methods in 3D semantic segmentation or object detection rely on a large amount of annotated data. Yet annotating 3D Lidar data for these tasks is tedious and costly. In this context, we propose a self-supervised pre-training method for 3D perception models that is tailored to autonomous driving data. Specifically, we leverage the availability of synchronized and calibrated image and Lidar sensors in autonomous driving setups for distilling self-supervised pre-trained image representations into 3D models. Hence, our method does not require any point cloud nor image annotations. The key ingredient of our method is the use of superpixels which are used to pool 3D point features and 2D pixel features in visually similar regions. We then train a 3D network on the self-supervised task of matching these pooled point features with the corresponding pooled image pixel features. The advantages of contrasting regions obtained by superpixels are that: (1) grouping together pixels and points of visually coherent regions leads to a more meaningful contrastive task that produces features well adapted to 3D semantic segmentation and 3D object detection; (2) all the different regions have the same weight in the contrastive loss regardless of the number of 3D points sampled in these regions; (3) it mitigates the noise produced by incorrect matching of points and pixels due to occlusions between the different sensors. Extensive experiments on autonomous driving datasets demonstrate the ability of our image-to-Lidar distillation strategy to produce 3D representations that transfer well on semantic segmentation and object detection tasks.},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Sautier, Corentin and Puy, Gilles and Gidaris, Spyros and Boulch, Alexandre and Bursuc, Andrei and Marlet, Renaud},
	month = mar,
	year = {2022},
	note = {arXiv:2203.16258 [cs]},
}

@misc{takmaz_openmask3d_2023,
	title = {{OpenMask3D}: {Open}-{Vocabulary} {3D} {Instance} {Segmentation}},
	shorttitle = {{OpenMask3D}},
	url = {http://arxiv.org/abs/2306.13631},
	abstract = {We introduce the task of open-vocabulary 3D instance segmentation. Traditional approaches for 3D instance segmentation largely rely on existing 3D annotated datasets, which are restricted to a closed-set of object categories. This is an important limitation for real-life applications where one might need to perform tasks guided by novel, open-vocabulary queries related to objects from a wide variety. Recently, open-vocabulary 3D scene understanding methods have emerged to address this problem by learning queryable features per each point in the scene. While such a representation can be directly employed to perform semantic segmentation, existing methods have limitations in their ability to identify object instances. In this work, we address this limitation, and propose OpenMask3D, which is a zero-shot approach for open-vocabulary 3D instance segmentation. Guided by predicted class-agnostic 3D instance masks, our model aggregates per-mask features via multi-view fusion of CLIP-based image embeddings. We conduct experiments and ablation studies on the ScanNet200 dataset to evaluate the performance of OpenMask3D, and provide insights about the open-vocabulary 3D instance segmentation task. We show that our approach outperforms other open-vocabulary counterparts, particularly on the long-tail distribution. Furthermore, OpenMask3D goes beyond the limitations of close-vocabulary approaches, and enables the segmentation of object instances based on free-form queries describing object properties such as semantics, geometry, affordances, and material properties.},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Takmaz, Ayça and Fedele, Elisabetta and Sumner, Robert W. and Pollefeys, Marc and Tombari, Federico and Engelmann, Francis},
	month = jun,
	year = {2023},
	note = {arXiv:2306.13631 [cs]},
}

@misc{liu_segment_2023,
	title = {Segment {Any} {Point} {Cloud} {Sequences} by {Distilling} {Vision} {Foundation} {Models}},
	url = {http://arxiv.org/abs/2306.09347},
	abstract = {Recent advancements in vision foundation models (VFMs) have opened up new possibilities for versatile and efficient visual perception. In this work, we introduce Seal, a novel framework that harnesses VFMs for segmenting diverse automotive point cloud sequences. Seal exhibits three appealing properties: i) Scalability: VFMs are directly distilled into point clouds, eliminating the need for annotations in either 2D or 3D during pretraining. ii) Consistency: Spatial and temporal relationships are enforced at both the camera-to-LiDAR and point-to-segment stages, facilitating cross-modal representation learning. iii) Generalizability: Seal enables knowledge transfer in an off-the-shelf manner to downstream tasks involving diverse point clouds, including those from real/synthetic, low/high-resolution, large/small-scale, and clean/corrupted datasets. Extensive experiments conducted on eleven different point cloud datasets showcase the effectiveness and superiority of Seal. Notably, Seal achieves a remarkable 45.0\% mIoU on nuScenes after linear probing, surpassing random initialization by 36.9\% mIoU and outperforming prior arts by 6.1\% mIoU. Moreover, Seal demonstrates significant performance gains over existing methods across 20 different few-shot fine-tuning tasks on all eleven tested point cloud datasets.},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Liu, Youquan and Kong, Lingdong and Cen, Jun and Chen, Runnan and Zhang, Wenwei and Pan, Liang and Chen, Kai and Liu, Ziwei},
	month = jun,
	year = {2023},
	note = {arXiv:2306.09347 [cs]},
}

@misc{zhang_sam3d_2023,
	title = {{SAM3D}: {Zero}-{Shot} {3D} {Object} {Detection} via {Segment} {Anything} {Model}},
	shorttitle = {{SAM3D}},
	url = {http://arxiv.org/abs/2306.02245},
	abstract = {With the development of large language models, many remarkable linguistic systems like ChatGPT have thrived and achieved astonishing success on many tasks, showing the incredible power of foundation models. In the spirit of unleashing the capability of foundation models on vision tasks, the Segment Anything Model (SAM), a vision foundation model for image segmentation, has been proposed recently and presents strong zero-shot ability on many downstream 2D tasks. However, whether SAM can be adapted to 3D vision tasks has yet to be explored, especially 3D object detection. With this inspiration, we explore adapting the zero-shot ability of SAM to 3D object detection in this paper. We propose a SAM-powered BEV processing pipeline to detect objects and get promising results on the large-scale Waymo open dataset. As an early attempt, our method takes a step toward 3D object detection with vision foundation models and presents the opportunity to unleash their power on 3D vision tasks. The code is released at https://github.com/DYZhang09/SAM3D.},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Zhang, Dingyuan and Liang, Dingkang and Yang, Hongcheng and Zou, Zhikang and Ye, Xiaoqing and Liu, Zhe and Bai, Xiang},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02245 [cs, eess]},
}

@article{Kan_2023_arxiv,
	title = {{WOMD}-{LiDAR}: {Raw} sensor dataset benchmark for motion forecasting},
	journal = {arXiv preprint arXiv:2304.03834},
	author = {Chen, Kan and Ge, Runzhou and Qiu, Hang and Ai-Rfou, Rami and Qi, Charles R. and Zhou, Xuanyu and Yang, Zoey and Ettinger, Scott and Sun, Pei and Leng, Zhaoqi and Mustafa, Mustafa and Bogun, Ivan and Wang, Weiyue and Tan, Mingxing and Anguelov, Dragomir},
	month = apr,
	year = {2023},
}

@inproceedings{Sun_2020_CVPR,
	title = {Scalability in perception for autonomous driving: {Waymo} open dataset},
	shorttitle = {Waymo},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
	month = jun,
	year = {2020},
}

@article{fong2021panoptic,
	title = {Panoptic {nuScenes}: {A} large-scale benchmark for {LiDAR} panoptic segmentation and tracking},
	journal = {arXiv preprint arXiv:2109.03805},
	author = {Fong, Whye Kit and Mohan, Rohit and Hurtado, Juana Valeria and Zhou, Lubing and Caesar, Holger and Beijbom, Oscar and Valada, Abhinav},
	year = {2021},
}

@article{nuscenes2019,
	title = {{nuScenes}: {A} multimodal dataset for autonomous driving},
	journal = {arXiv preprint arXiv:1903.11027},
	author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
	year = {2019},
}

@inproceedings{niemeyer_giraffe_2021,
	address = {Nashville, TN, USA},
	title = {{GIRAFFE}: {Representing} {Scenes} as {Compositional} {Generative} {Neural} {Feature} {Fields}},
	isbn = {978-1-66544-509-2},
	shorttitle = {{GIRAFFE}},
	url = {https://ieeexplore.ieee.org/document/9577414/},
	doi = {10.1109/CVPR46437.2021.01129},
	abstract = {Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature ﬁelds allows us to disentangle one or multiple objects from the background as well as individual objects’ shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.},
	language = {en},
	urldate = {2023-07-03},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Niemeyer, Michael and Geiger, Andreas},
	month = jun,
	year = {2021},
	keywords = {3D, Generation, Image, Machine Learning, NERF},
	pages = {11448--11459},
}

@misc{li_generate_2023,
	title = {Generate {Anything} {Anywhere} in {Any} {Scene}},
	url = {http://arxiv.org/abs/2306.17154},
	abstract = {Text-to-image diffusion models have attracted considerable interest due to their wide applicability across diverse fields. However, challenges persist in creating controllable models for personalized object generation. In this paper, we first identify the entanglement issues in existing personalized generative models, and then propose a straightforward and efficient data augmentation training strategy that guides the diffusion model to focus solely on object identity. By inserting the plug-and-play adapter layers from a pre-trained controllable diffusion model, our model obtains the ability to control the location and size of each generated personalized object. During inference, we propose a regionally-guided sampling technique to maintain the quality and fidelity of the generated images. Our method achieves comparable or superior fidelity for personalized objects, yielding a robust, versatile, and controllable text-to-image diffusion model that is capable of generating realistic and personalized images. Our approach demonstrates significant potential for various applications, such as those in art, entertainment, and advertising design.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Li, Yuheng and Liu, Haotian and Wen, Yangming and Lee, Yong Jae},
	month = jun,
	year = {2023},
	note = {arXiv:2306.17154 [cs]},
	keywords = {Generation, Image, Machine Learning, Text},
}

@misc{huang_out--distribution_2022,
	title = {Out-of-{Distribution} {Detection} for {LiDAR}-based {3D} {Object} {Detection}},
	url = {http://arxiv.org/abs/2209.14435},
	abstract = {3D object detection is an essential part of automated driving, and deep neural networks (DNNs) have achieved state-of-the-art performance for this task. However, deep models are notorious for assigning high confidence scores to out-of-distribution (OOD) inputs, that is, inputs that are not drawn from the training distribution. Detecting OOD inputs is challenging and essential for the safe deployment of models. OOD detection has been studied extensively for the classification task, but it has not received enough attention for the object detection task, specifically LiDAR-based 3D object detection. In this paper, we focus on the detection of OOD inputs for LiDAR-based 3D object detection. We formulate what OOD inputs mean for object detection and propose to adapt several OOD detection methods for object detection. We accomplish this by our proposed feature extraction method. To evaluate OOD detection methods, we develop a simple but effective technique of generating OOD objects for a given object detection model. Our evaluation based on the KITTI dataset shows that different OOD detection methods have biases toward detecting specific OOD objects. It emphasizes the importance of combined OOD detection methods and more research in this direction.},
	urldate = {2023-07-01},
	publisher = {arXiv},
	author = {Huang, Chengjie and Nguyen, Van Duong and Abdelzad, Vahdat and Mannes, Christopher Gus and Rowe, Luke and Therien, Benjamin and Salay, Rick and Czarnecki, Krzysztof},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14435 [cs, eess]},
}

@misc{yang_generalized_2022,
	title = {Generalized {Out}-of-{Distribution} {Detection}: {A} {Survey}},
	shorttitle = {Generalized {Out}-of-{Distribution} {Detection}},
	url = {http://arxiv.org/abs/2110.11334},
	abstract = {Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen during training time and cannot make a safe decision. The term, OOD detection, first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD), are closely related to OOD detection in terms of motivation and methodology. Despite common goals, these topics develop in isolation, and their subtle differences in definition and problem setting often confuse readers and practitioners. In this survey, we first present a unified framework called generalized OOD detection, which encompasses the five aforementioned problems, i.e., AD, ND, OSR, OOD detection, and OD. Under our framework, these five problems can be seen as special cases or sub-tasks, and are easier to distinguish. We then review each of these five areas by summarizing their recent technical developments, with a special focus on OOD detection methodologies. We conclude this survey with open challenges and potential research directions.},
	urldate = {2023-07-01},
	publisher = {arXiv},
	author = {Yang, Jingkang and Zhou, Kaiyang and Li, Yixuan and Liu, Ziwei},
	month = aug,
	year = {2022},
	note = {arXiv:2110.11334 [cs]},
}

@misc{nichol_point-e_2022,
	title = {Point-{E}: {A} {System} for {Generating} {3D} {Point} {Clouds} from {Complex} {Prompts}},
	shorttitle = {Point-{E}},
	url = {http://arxiv.org/abs/2212.08751},
	abstract = {While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https://github.com/openai/point-e.},
	urldate = {2023-07-01},
	publisher = {arXiv},
	author = {Nichol, Alex and Jun, Heewoo and Dhariwal, Prafulla and Mishkin, Pamela and Chen, Mark},
	month = dec,
	year = {2022},
	note = {arXiv:2212.08751 [cs]},
	keywords = {3D, FromKC, Generation, Machine Learning, Multimodal, Shape, Text},
}

@article{fu_shapecrafter_nodate,
	title = {{ShapeCrafter}: {A} {Recursive} {Text}-{Conditioned} {3D} {Shape} {Generation} {Model}},
	abstract = {We present ShapeCrafter, a neural network for recursive text-conditioned 3D shape generation. Existing methods that generate text-conditioned 3D shapes consume an entire text prompt to generate a 3D shape in a single step. However, humans tend to describe shapes recursively—we may start with an initial description and progressively add details based on intermediate results. To capture this recursive process, we introduce a method to generate a 3D shape distribution, conditioned on an initial phrase, that gradually evolves as more phrases are added. Since existing datasets are insufficient for training under this approach, we present Text2Shape++, a large dataset of 369K shape–text pairs that supports recursive shape generation. To capture local details that are often used to refine shape descriptions, we build upon vector-quantized deep implicit functions that generate a distribution of high-quality shapes. Results show that our method can generate shapes consistent with text descriptions, and shapes evolve gradually as more phrases are added. Our method supports shape editing, extrapolation, and can enable new applications in human–machine collaboration for creative design.},
	language = {en},
	author = {Fu, Rao and Zhan, Xiao and Chen, Yiwen and Ritchie, Daniel and Sridhar, Srinath},
	keywords = {3D, Generation, Machine Learning, ShapeGeneration, Text},
}

@misc{zhuang_dreameditor_2023,
	title = {{DreamEditor}: {Text}-{Driven} {3D} {Scene} {Editing} with {Neural} {Fields}},
	shorttitle = {{DreamEditor}},
	url = {http://arxiv.org/abs/2306.13455},
	abstract = {Neural fields have achieved impressive advancements in view synthesis and scene reconstruction. However, editing these neural fields remains challenging due to the implicit encoding of geometry and texture information. In this paper, we propose DreamEditor, a novel framework that enables users to perform controlled editing of neural fields using text prompts. By representing scenes as mesh-based neural fields, DreamEditor allows localized editing within specific regions. DreamEditor utilizes the text encoder of a pretrained text-to-Image diffusion model to automatically identify the regions to be edited based on the semantics of the text prompts. Subsequently, DreamEditor optimizes the editing region and aligns its geometry and texture with the text prompts through score distillation sampling [29]. Extensive experiments have demonstrated that DreamEditor can accurately edit neural fields of real-world scenes according to the given text prompts while ensuring consistency in irrelevant areas. DreamEditor generates highly realistic textures and geometry, significantly surpassing previous works in both quantitative and qualitative evaluations.},
	language = {en},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Zhuang, Jingyu and Wang, Chen and Liu, Lingjie and Lin, Liang and Li, Guanbin},
	month = jun,
	year = {2023},
	note = {arXiv:2306.13455 [cs]},
	keywords = {FromKC},
}

@article{achlioptas_shapetalk_nodate,
	title = {{ShapeTalk}: {A} {Language} {Dataset} and {Framework} for {3D} {Shape} {Edits} and {Deformations}},
	abstract = {Editing 3D geometry is a challenging task requiring specialized skills. In this work, we aim to facilitate the task of editing the geometry of 3D models through the use of natural language. For example, we may want to modify a 3D chair model to “make its legs thinner” or to “open a hole in its back”. To tackle this problem in a manner that promotes open-ended language use and enables fine-grained shape edits, we introduce the most extensive existing corpus of natural language utterances describing shape differences: ShapeTalk. ShapeTalk contains over half a million discriminative utterances produced by contrasting the shapes of common 3D objects for a variety of object classes and degrees of similarity. We also introduce a generic framework, ChangeIt3D, which builds on ShapeTalk and can use an arbitrary 3D generative model of shapes to produce edits that align the output better with the edit or deformation description. Finally, we introduce metrics for the quantitative evaluation of language-assisted shape editing methods that reflect key desiderata within this editing setup. We note that ShapeTalk allows methods to be trained with explicit 3D-to-language data, bypassing the necessity of “lifting” 2D to 3D using methods like neural rendering, as required by extant 2D image-language foundation models. Our code and data are publicly available at https://changeit3d.github.io/.},
	language = {en},
	author = {Achlioptas, Panos and Huang, Ian and Sung, Minhyuk and Tulyakov, Sergey and Guibas, Leonidas},
	keywords = {FromKC},
}

@article{ma_language-driven_2018,
	title = {Language-driven synthesis of {3D} scenes from scene databases},
	volume = {37},
	issn = {0730-0301},
	url = {https://dl.acm.org/doi/10.1145/3272127.3275035},
	doi = {10.1145/3272127.3275035},
	abstract = {We introduce a novel framework for using natural language to generate and edit 3D indoor scenes, harnessing scene semantics and text-scene grounding knowledge learned from large annotated 3D scene databases. The advantage of natural language editing interfaces is strongest when performing semantic operations at the sub-scene level, acting on groups of objects. We learn how to manipulate these sub-scenes by analyzing existing 3D scenes. We perform edits by first parsing a natural language command from the user and transforming it into a semantic scene graph that is used to retrieve corresponding sub-scenes from the databases that match the command. We then augment this retrieved sub-scene by incorporating other objects that may be implied by the scene context. Finally, a new 3D scene is synthesized by aligning the augmented sub-scene with the user's current scene, where new objects are spliced into the environment, possibly triggering appropriate adjustments to the existing scene arrangement. A suggestive modeling interface with multiple interpretations of user commands is used to alleviate ambiguities in natural language. We conduct studies comparing our approach against both prior text-to-scene work and artist-made scenes and find that our method significantly outperforms prior work and is comparable to handmade scenes even when complex and varied natural sentences are used.},
	number = {6},
	urldate = {2023-06-30},
	journal = {ACM Transactions on Graphics},
	author = {Ma, Rui and Patil, Akshay Gadi and Fisher, Matthew and Li, Manyi and Pirk, Sören and Hua, Binh-Son and Yeung, Sai-Kit and Tong, Xin and Guibas, Leonidas and Zhang, Hao},
	month = dec,
	year = {2018},
	keywords = {FromKC},
	pages = {212:1--212:16},
}

@inproceedings{tang_compressible-composable_2022,
	title = {Compressible-composable {NeRF} via {Rank}-residual {Decomposition}},
	url = {https://openreview.net/forum?id=aPXMGv7aeOn},
	abstract = {Neural Radiance Field (NeRF) has emerged as a compelling method to represent 3D objects and scenes for photo-realistic rendering. However, its implicit representation causes difficulty in manipulating the models like the explicit mesh representation. Several recent advances in NeRF manipulation are usually restricted by a shared renderer network, or suffer from large model size. To circumvent the hurdle, in this paper, we present a neural field representation that enables efficient and convenient manipulation of models. To achieve this goal, we learn a hybrid tensor rank decomposition of the scene without neural networks. Motivated by the low-rank approximation property of the SVD algorithm, we propose a rank-residual learning strategy to encourage the preservation of primary information in lower ranks. The model size can then be dynamically adjusted by rank truncation to control the levels of detail, achieving near-optimal compression without extra optimization. Furthermore, different models can be arbitrarily transformed and composed into one scene by concatenating along the rank dimension. The growth of storage cost can also be mitigated by compressing the unimportant objects in the composed scene. We demonstrate that our method is able to achieve comparable rendering quality to state-of-the-art methods, while enabling extra capability of compression and composition. Code is available at https://github.com/ashawkey/CCNeRF.},
	language = {en},
	urldate = {2023-06-30},
	author = {Tang, Jiaxiang and Chen, Xiaokang and Wang, Jingbo and Zeng, Gang},
	month = may,
	year = {2022},
}

@inproceedings{Zhang_2023_CVPR,
	title = {Frequency-modulated point cloud rendering with easy editing},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Zhang, Yi and Huang, Xiaoyang and Ni, Bingbing and Li, Teng and Zhang, Wenjun},
	month = jun,
	year = {2023},
	pages = {119--129},
}

@inproceedings{Chen_2023_CVPR,
	title = {{NeuralEditor}: {Editing} neural radiance fields via manipulating point clouds},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Chen, Jun-Kun and Lyu, Jipeng and Wang, Yu-Xiong},
	month = jun,
	year = {2023},
	pages = {12439--12448},
}

@inproceedings{kim_pointinverter_2023,
	address = {Waikoloa, HI, USA},
	title = {{PointInverter}: {Point} {Cloud} {Reconstruction} and {Editing} via a {Generative} {Model} with {Shape} {Priors}},
	isbn = {978-1-66549-346-8},
	shorttitle = {{PointInverter}},
	url = {https://ieeexplore.ieee.org/document/10030251/},
	doi = {10.1109/WACV56688.2023.00066},
	abstract = {In this paper, we propose a new method for mapping a 3D point cloud to the latent space of a 3D generative adversarial network. Our generative model for 3D point clouds is based on SP-GAN, a state-of-the-art sphere-guided 3D point cloud generator. We derive an efﬁcient way to encode an input 3D point cloud to the latent space of the SP-GAN. Our point cloud encoder can resolve the point ordering issue during inversion, and thus can determine the correspondences between points in the generated 3D point cloud and those in the canonical sphere used by the generator. We show that our method outperforms previous GAN inversion methods for 3D point clouds, achieving state-of-the-art results both quantitatively and qualitatively. Our code is available at https: //github.com/hkust-vgd/point\_inverter.},
	language = {en},
	urldate = {2023-06-30},
	booktitle = {2023 {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Kim, Jaeyeon and Hua, Binh-Son and Nguyen, Duc Thanh and Yeung, Sai-Kit},
	month = jan,
	year = {2023},
	pages = {592--601},
}

@article{deng_nerdi_nodate,
	title = {{NeRDi}: {Single}-{View} {NeRF} {Synthesis} {With} {Language}-{Guided} {Diffusion} {As} {General} {Image} {Priors}},
	language = {en},
	author = {Deng, Congyue},
}

@misc{xu_instructp2p_2023,
	title = {{InstructP2P}: {Learning} to {Edit} {3D} {Point} {Clouds} with {Text} {Instructions}},
	shorttitle = {{InstructP2P}},
	url = {http://arxiv.org/abs/2306.07154},
	abstract = {Enhancing AI systems to perform tasks following human instructions can significantly boost productivity. In this paper, we present InstructP2P, an end-to-end framework for 3D shape editing on point clouds, guided by high-level textual instructions. InstructP2P extends the capabilities of existing methods by synergizing the strengths of a text-conditioned point cloud diffusion model, Point-E, and powerful language models, enabling color and geometry editing using language instructions. To train InstructP2P, we introduce a new shape editing dataset, constructed by integrating a shape segmentation dataset, off-the-shelf shape programs, and diverse edit instructions generated by a large language model, ChatGPT. Our proposed method allows for editing both color and geometry of specific regions in a single forward pass, while leaving other regions unaffected. In our experiments, InstructP2P shows generalization capabilities, adapting to novel shape categories and instructions, despite being trained on a limited amount of data.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Xu, Jiale and Wang, Xintao and Cao, Yan-Pei and Cheng, Weihao and Shan, Ying and Gao, Shenghua},
	month = jun,
	year = {2023},
	note = {arXiv:2306.07154 [cs]},
	keywords = {FromKC},
}

@article{xu_open-vocabulary_nodate,
	title = {Open-{Vocabulary} {Panoptic} {Segmentation} {With} {Text}-to-{Image} {Diffusion} {Models}},
	language = {en},
	author = {Xu, Jiarui and Liu, Sifei and Vahdat, Arash and Byeon, Wonmin and Wang, Xiaolong and Mello, Shalini De},
	keywords = {DiffusionModel, Image, Machine Learning, Multimodal, Open-vocabulary, Panoptic, Segmentation, Text},
}

@misc{brust_integrating_2020,
	title = {Integrating domain knowledge: using hierarchies to improve deep classifiers},
	shorttitle = {Integrating domain knowledge},
	url = {http://arxiv.org/abs/1811.07125},
	abstract = {One of the most prominent problems in machine learning in the age of deep learning is the availability of sufficiently large annotated datasets. For specific domains, e.g. animal species, a long-tail distribution means that some classes are observed and annotated insufficiently. Additional labels can be prohibitively expensive, e.g. because domain experts need to be involved. However, there is more information available that is to the best of our knowledge not exploited accordingly. In this paper, we propose to make use of preexisting class hierarchies like WordNet to integrate additional domain knowledge into classification. We encode the properties of such a class hierarchy into a probabilistic model. From there, we derive a novel label encoding and a corresponding loss function. On the ImageNet and NABirds datasets our method offers a relative improvement of 10.4\% and 9.6\% in accuracy over the baseline respectively. After less than a third of training time, it is already able to match the baseline's fine-grained recognition performance. Both results show that our suggested method is efficient and effective.},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Brust, Clemens-Alexander and Denzler, Joachim},
	month = jan,
	year = {2020},
	note = {arXiv:1811.07125 [cs]},
}

@misc{novack_chils_2023,
	title = {{CHiLS}: {Zero}-{Shot} {Image} {Classification} with {Hierarchical} {Label} {Sets}},
	shorttitle = {{CHiLS}},
	url = {http://arxiv.org/abs/2302.02551},
	abstract = {Open vocabulary models (e.g. CLIP) have shown strong performance on zero-shot classification through their ability generate embeddings for each class based on their (natural language) names. Prior work has focused on improving the accuracy of these models through prompt engineering or by incorporating a small amount of labeled downstream data (via finetuning). However, there has been little focus on improving the richness of the class names themselves, which can pose issues when class labels are coarsely-defined and are uninformative. We propose Classification with Hierarchical Label Sets (or CHiLS), an alternative strategy for zero-shot classification specifically designed for datasets with implicit semantic hierarchies. CHiLS proceeds in three steps: (i) for each class, produce a set of subclasses, using either existing label hierarchies or by querying GPT-3; (ii) perform the standard zero-shot CLIP procedure as though these subclasses were the labels of interest; (iii) map the predicted subclass back to its parent to produce the final prediction. Across numerous datasets with underlying hierarchical structure, CHiLS leads to improved accuracy in situations both with and without ground-truth hierarchical information. CHiLS is simple to implement within existing zero-shot pipelines and requires no additional training cost. Code is available at: https://github.com/acmi-lab/CHILS.},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Novack, Zachary and McAuley, Julian and Lipton, Zachary C. and Garg, Saurabh},
	month = may,
	year = {2023},
	note = {arXiv:2302.02551 [cs]},
}

@inproceedings{yuan_nerf-editing_2022,
	address = {New Orleans, LA, USA},
	title = {{NeRF}-{Editing}: {Geometry} {Editing} of {Neural} {Radiance} {Fields}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{NeRF}-{Editing}},
	url = {https://ieeexplore.ieee.org/document/9879133/},
	doi = {10.1109/CVPR52688.2022.01781},
	abstract = {Implicit neural rendering, especially Neural Radiance Field (NeRF), has shown great potential in novel view synthesis of a scene. However, current NeRF-based methods cannot enable users to perform user-controlled shape deformation in the scene. While existing works have proposed some approaches to modify the radiance ﬁeld according to the user’s constraints, the modiﬁcation is limited to color editing or object translation and rotation. In this paper, we propose a method that allows users to perform controllable shape deformation on the implicit representation of the scene, and synthesizes the novel view images of the edited scene without re-training the network. Speciﬁcally, we establish a correspondence between the extracted explicit mesh representation and the implicit neural representation of the target scene. Users can ﬁrst utilize welldeveloped mesh-based deformation methods to deform the mesh representation of the scene. Our method then utilizes user edits from the mesh representation to bend the camera rays by introducing a tetrahedra mesh as a proxy, obtaining the rendering results of the edited scene. Extensive experiments demonstrate that our framework can achieve ideal editing results not only on synthetic data, but also on real scenes captured by users.},
	language = {en},
	urldate = {2023-06-28},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yuan, Yu-Jie and Sun, Yang-Tian and Lai, Yu-Kun and Ma, Yuewen and Jia, Rongfei and Gao, Lin},
	month = jun,
	year = {2022},
	pages = {18332--18343},
}

@misc{lin_componerf_2023,
	title = {{CompoNeRF}: {Text}-guided {Multi}-object {Compositional} {NeRF} with {Editable} {3D} {Scene} {Layout}},
	shorttitle = {{CompoNeRF}},
	url = {http://arxiv.org/abs/2303.13843},
	abstract = {Recent research endeavors have shown that combining neural radiance fields (NeRFs) with pre-trained diffusion models holds great potential for text-to-3D generation.However, a hurdle is that they often encounter guidance collapse when rendering complex scenes from multi-object texts. Because the text-to-image diffusion models are inherently unconstrained, making them less competent to accurately associate object semantics with specific 3D structures. To address this issue, we propose a novel framework, dubbed CompoNeRF, that explicitly incorporates an editable 3D scene layout to provide effective guidance at the single object (i.e., local) and whole scene (i.e., global) levels. Firstly, we interpret the multi-object text as an editable 3D scene layout containing multiple local NeRFs associated with the object-specific 3D box coordinates and text prompt, which can be easily collected from users. Then, we introduce a global MLP to calibrate the compositional latent features from local NeRFs, which surprisingly improves the view consistency across different local NeRFs. Lastly, we apply the text guidance on global and local levels through their corresponding views to avoid guidance ambiguity. This way, our CompoNeRF allows for flexible scene editing and re-composition of trained local NeRFs into a new scene by manipulating the 3D layout or text prompt. Leveraging the open-source Stable Diffusion model, our CompoNeRF can generate faithful and editable text-to-3D results while opening a potential direction for text-guided multi-object composition via the editable 3D scene layout.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Lin, Yiqi and Bai, Haotian and Li, Sijia and Lu, Haonan and Lin, Xiaodong and Xiong, Hui and Wang, Lin},
	month = mar,
	year = {2023},
	note = {arXiv:2303.13843 [cs]},
}

@misc{sun_nerfeditor_2022,
	title = {{NeRFEditor}: {Differentiable} {Style} {Decomposition} for {Full} {3D} {Scene} {Editing}},
	shorttitle = {{NeRFEditor}},
	url = {http://arxiv.org/abs/2212.03848},
	abstract = {We present NeRFEditor, an efficient learning framework for 3D scene editing, which takes a video captured over 360\{{\textbackslash}deg\} as input and outputs a high-quality, identity-preserving stylized 3D scene. Our method supports diverse types of editing such as guided by reference images, text prompts, and user interactions. We achieve this by encouraging a pre-trained StyleGAN model and a NeRF model to learn from each other mutually. Specifically, we use a NeRF model to generate numerous image-angle pairs to train an adjustor, which can adjust the StyleGAN latent code to generate high-fidelity stylized images for any given angle. To extrapolate editing to GAN out-of-domain views, we devise another module that is trained in a self-supervised learning manner. This module maps novel-view images to the hidden space of StyleGAN that allows StyleGAN to generate stylized images on novel views. These two modules together produce guided images in 360\{{\textbackslash}deg\}views to finetune a NeRF to make stylization effects, where a stable fine-tuning strategy is proposed to achieve this. Experiments show that NeRFEditor outperforms prior work on benchmark and real-world scenes with better editability, fidelity, and identity preservation.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Sun, Chunyi and Liu, Yanbin and Han, Junlin and Gould, Stephen},
	month = dec,
	year = {2022},
	note = {arXiv:2212.03848 [cs, eess]},
}

@article{kong_vmap_nodate,
	title = {{vMAP}: {Vectorised} {Object} {Mapping} for {Neural} {Field} {SLAM}},
	abstract = {We present vMAP, an object-level dense SLAM system using neural field representations. Each object is represented by a small MLP, enabling efficient, watertight object modelling without the need for 3D priors.},
	language = {en},
	author = {Kong, Xin and Liu, Shikun and Taher, Marwan and Davison, Andrew J},
	keywords = {3D, Compositional, Generation, Machine Learning, NERF},
}

@misc{wang_prolificdreamer_2023,
	title = {{ProlificDreamer}: {High}-{Fidelity} and {Diverse} {Text}-to-{3D} {Generation} with {Variational} {Score} {Distillation}},
	shorttitle = {{ProlificDreamer}},
	url = {http://arxiv.org/abs/2305.16213},
	abstract = {Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing, and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present variational score distillation (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation. We show that SDS is a special case of VSD and leads to poor samples with both small and large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i.e., \$7.5\$). We further present various improvements in the design space for text-to-3D such as distillation time schedule and density initialization, which are orthogonal to the distillation algorithm yet not well explored. Our overall approach, dubbed ProlificDreamer, can generate high rendering resolution (i.e., \$512{\textbackslash}times512\$) and high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and photo-realistic. Project page: https://ml.cs.tsinghua.edu.cn/prolificdreamer/},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Wang, Zhengyi and Lu, Cheng and Wang, Yikai and Bao, Fan and Li, Chongxuan and Su, Hang and Zhu, Jun},
	month = may,
	year = {2023},
	note = {arXiv:2305.16213 [cs]},
	keywords = {3D, Generation, Machine Learning, ShapeGeneration, Text},
}

@misc{wang_one-peace_2023,
	title = {{ONE}-{PEACE}: {Exploring} {One} {General} {Representation} {Model} {Toward} {Unlimited} {Modalities}},
	shorttitle = {{ONE}-{PEACE}},
	url = {http://arxiv.org/abs/2305.11172},
	abstract = {In this work, we explore a scalable way for building a general representation model toward unlimited modalities. We release ONE-PEACE, a highly extensible model with 4B parameters that can seamlessly align and integrate representations across vision, audio, and language modalities. The architecture of ONE-PEACE comprises modality adapters, shared self-attention layers, and modality FFNs. This design allows for the easy extension of new modalities by adding adapters and FFNs, while also enabling multi-modal fusion through self-attention layers. To pretrain ONE-PEACE, we develop two modality-agnostic pretraining tasks, cross-modal aligning contrast and intra-modal denoising contrast, which align the semantic space of different modalities and capture fine-grained details within modalities concurrently. With the scaling-friendly architecture and pretraining tasks, ONE-PEACE has the potential to expand to unlimited modalities. Without using any vision or language pretrained model for initialization, ONE-PEACE achieves leading results on a wide range of uni-modal and multi-modal tasks, including image classification (ImageNet), semantic segmentation (ADE20K), audio-text retrieval (AudioCaps, Clotho), audio classification (ESC-50, FSD50K, VGGSound), audio question answering (AVQA), image-text retrieval (MSCOCO, Flickr30K), and visual grounding (RefCOCO/+/g). Code is available at https://github.com/OFA-Sys/ONE-PEACE.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Wang, Peng and Wang, Shijie and Lin, Junyang and Bai, Shuai and Zhou, Xiaohuan and Zhou, Jingren and Wang, Xinggang and Zhou, Chang},
	month = may,
	year = {2023},
	note = {arXiv:2305.11172 [cs, eess]},
	keywords = {Machine Learning, Multimodal, Retrieval},
}

@inproceedings{DBLP:conf/nips/SahariaCSLWDGLA22,
	title = {Photorealistic text-to-image diffusion models with deep language understanding},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/ec795aeadae0b7d230fa35cbaf04c041-Abstract-Conference.html},
	booktitle = {{NeurIPS}},
	author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L. and Ghasemipour, Seyed Kamyar Seyed and Lopes, Raphael Gontijo and Ayan, Burcu Karagol and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
	year = {2022},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/nips/SahariaCSLWDGLA22.bib
tex.timestamp: Thu, 11 May 2023 17:08:22 +0200},
	keywords = {DDIM, Generation, Image, Machine Learning, Text},
}

@misc{kato_differentiable_2020,
	title = {Differentiable {Rendering}: {A} {Survey}},
	shorttitle = {Differentiable {Rendering}},
	url = {http://arxiv.org/abs/2006.12057},
	abstract = {Deep neural networks (DNNs) have shown remarkable performance improvements on vision-related tasks such as object detection or image segmentation. Despite their success, they generally lack the understanding of 3D objects which form the image, as it is not always possible to collect 3D information about the scene or to easily annotate it. Differentiable rendering is a novel ﬁeld which allows the gradients of 3D objects to be calculated and propagated through images. It also reduces the requirement of 3D data collection and annotation, while enabling higher success rate in various applications. This paper reviews existing literature and discusses the current state of differentiable rendering, its applications and open research problems.},
	language = {en},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Kato, Hiroharu and Beker, Deniz and Morariu, Mihai and Ando, Takahiro and Matsuoka, Toru and Kehl, Wadim and Gaidon, Adrien},
	month = jul,
	year = {2020},
	note = {arXiv:2006.12057 [cs]},
	keywords = {Graphics, Survey},
}

@misc{tang_any--any_2023,
	title = {Any-to-{Any} {Generation} via {Composable} {Diffusion}},
	url = {http://arxiv.org/abs/2305.11846},
	abstract = {We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Tang, Zineng and Yang, Ziyi and Zhu, Chenguang and Zeng, Michael and Bansal, Mohit},
	month = may,
	year = {2023},
	note = {arXiv:2305.11846 [cs, eess]},
	keywords = {3D, Audio, Generation, Image, Machine Learning, Multimodal, Text, Video},
}

@inproceedings{khalid_clip-mesh_2022,
	title = {{CLIP}-{Mesh}: {Generating} textured meshes from text using pretrained image-text models},
	shorttitle = {{CLIP}-{Mesh}},
	url = {http://arxiv.org/abs/2203.13333},
	doi = {10.1145/3550469.3555392},
	abstract = {We present a technique for zero-shot generation of a 3D model using only a target text prompt. Without any 3D supervision our method deforms the control shape of a limit subdivided surface along with its texture map and normal map to obtain a 3D asset that corresponds to the input text prompt and can be easily deployed into games or modeling applications. We rely only on a pre-trained CLIP model that compares the input text prompt with differentiably rendered images of our 3D model. While previous works have focused on stylization or required training of generative models we perform optimization on mesh parameters directly to generate shape, texture or both. To constrain the optimization to produce plausible meshes and textures we introduce a number of techniques using image augmentations and the use of a pretrained prior that generates CLIP image embeddings given a text embedding.},
	urldate = {2023-06-28},
	booktitle = {{SIGGRAPH} {Asia} 2022 {Conference} {Papers}},
	author = {Khalid, Nasir Mohammad and Xie, Tianhao and Belilovsky, Eugene and Popa, Tiberiu},
	month = nov,
	year = {2022},
	note = {arXiv:2203.13333 [cs]},
	keywords = {3D, Generation, Machine Learning, Multimodal, Scene, Text},
	pages = {1--8},
}

@inproceedings{poole_dreamfusion_2022,
	title = {{DreamFusion}: {Text}-to-{3D} using {2D} {Diffusion}},
	shorttitle = {{DreamFusion}},
	url = {https://openreview.net/forum?id=FjNys5c7VyY},
	abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D or multiview data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
	language = {en},
	urldate = {2023-06-28},
	author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
	month = sep,
	year = {2022},
	keywords = {3D, Generation, Machine Learning, Multimodal, Scene, Text},
}

@inproceedings{michel_text2mesh_2022,
	address = {New Orleans, LA, USA},
	title = {{Text2Mesh}: {Text}-{Driven} {Neural} {Stylization} for {Meshes}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{Text2Mesh}},
	url = {https://ieeexplore.ieee.org/document/9880225/},
	doi = {10.1109/CVPR52688.2022.01313},
	abstract = {In this work, we develop intuitive controls for editing the style of 3D objects. Our framework, Text2Mesh, stylizes a 3D mesh by predicting color and local geometric details which conform to a target text prompt. We consider a disentangled representation of a 3D object using a ﬁxed mesh input (content) coupled with a learned neural network, which we term a neural style ﬁeld network (NSF). In order to modify style, we obtain a similarity score between a text prompt (describing style) and a stylized mesh by harnessing the representational power of CLIP. Text2Mesh requires neither a pre-trained generative model nor a specialized 3D mesh dataset. It can handle low-quality meshes (non-manifold, boundaries, etc.) with arbitrary genus, and does not require UV parameterization. We demonstrate the ability of our technique to synthesize a myriad of styles over a wide variety of 3D meshes. Our code and results are available in our project webpage: https://threedle.github.io/text2mesh/.},
	language = {en},
	urldate = {2023-06-28},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Michel, Oscar and Bar-On, Roi and Liu, Richard and Benaim, Sagie and Hanocka, Rana},
	month = jun,
	year = {2022},
	keywords = {3D, Generation, Machine Learning, ShapeGeneration, Text},
	pages = {13482--13492},
}

@misc{hong_avatarclip_2022,
	title = {{AvatarCLIP}: {Zero}-{Shot} {Text}-{Driven} {Generation} and {Animation} of {3D} {Avatars}},
	shorttitle = {{AvatarCLIP}},
	url = {http://arxiv.org/abs/2205.08535},
	abstract = {3D avatar creation plays a crucial role in the digital age. However, the whole production process is prohibitively time-consuming and labor-intensive. To democratize this technology to a larger audience, we propose AvatarCLIP, a zero-shot text-driven framework for 3D avatar generation and animation. Unlike professional software that requires expert knowledge, AvatarCLIP empowers layman users to customize a 3D avatar with the desired shape and texture, and drive the avatar with the described motions using solely natural languages. Our key insight is to take advantage of the powerful vision-language model CLIP for supervising neural human generation, in terms of 3D geometry, texture and animation. Specifically, driven by natural language descriptions, we initialize 3D human geometry generation with a shape VAE network. Based on the generated 3D human shapes, a volume rendering model is utilized to further facilitate geometry sculpting and texture generation. Moreover, by leveraging the priors learned in the motion VAE, a CLIP-guided reference-based motion synthesis method is proposed for the animation of the generated 3D avatar. Extensive qualitative and quantitative experiments validate the effectiveness and generalizability of AvatarCLIP on a wide range of avatars. Remarkably, AvatarCLIP can generate unseen 3D avatars with novel animations, achieving superior zero-shot capability.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Hong, Fangzhou and Zhang, Mingyuan and Pan, Liang and Cai, Zhongang and Yang, Lei and Liu, Ziwei},
	month = may,
	year = {2022},
	note = {arXiv:2205.08535 [cs]},
	keywords = {3D, Generation, Machine Learning, ShapeGeneration, Text},
}

@misc{fukamizu_generation_2019,
	title = {Generation {High} resolution {3D} model from natural language by {Generative} {Adversarial} {Network}},
	url = {http://arxiv.org/abs/1901.07165},
	doi = {10.48550/arXiv.1901.07165},
	abstract = {We present a method of generating high resolution 3D shapes from natural language descriptions. To achieve this goal, we propose two steps that generating low resolution shapes which roughly reflect texts and generating high resolution shapes which reflect the detail of texts. In a previous paper, the authors have shown a method of generating low resolution shapes. We improve it to generate 3D shapes more faithful to natural language and test the effectiveness of the method. To generate high resolution 3D shapes, we use the framework of Conditional Wasserstein GAN. We propose two roles of Critic separately, which calculate the Wasserstein distance between two probability distribution, so that we achieve generating high quality shapes or acceleration of learning speed of model. To evaluate our approach, we performed quantitive evaluation with several numerical metrics for Critic models. Our method is first to realize the generation of high quality model by propagating text embedding information to high resolution task when generating 3D model.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Fukamizu, Kentaro and Kondo, Masaaki and Sakamoto, Ryuichi},
	month = jan,
	year = {2019},
	note = {arXiv:1901.07165 [cs, stat]},
	keywords = {3D, Generation, Machine Learning, ShapeGeneration, Text},
}

@inproceedings{jain_zero-shot_2022,
	address = {New Orleans, LA, USA},
	title = {Zero-{Shot} {Text}-{Guided} {Object} {Generation} with {Dream} {Fields}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9880330/},
	doi = {10.1109/CVPR52688.2022.00094},
	abstract = {We combine neural rendering with multi-modal image and text representations to synthesize diverse 3D objects solely from natural language descriptions. Our method, Dream Fields, can generate the geometry and color of a wide range of objects without 3D supervision. Due to the scarcity of diverse, captioned 3D data, prior methods only generate objects from a handful of categories, such as ShapeNet. Instead, we guide generation with image-text models pre-trained on large datasets of captioned images from the web. Our method optimizes a Neural Radiance Field from many camera views so that rendered images score highly with a target caption according to a pre-trained CLIP model. To improve ﬁdelity and visual quality, we introduce simple geometric priors, including sparsity-inducing transmittance regularization, scene bounds, and new MLP architectures. In experiments, Dream Fields produce realistic, multi-view consistent object geometry and color from a variety of natural language captions.},
	language = {en},
	urldate = {2023-06-28},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Jain, Ajay and Mildenhall, Ben and Barron, Jonathan T. and Abbeel, Pieter and Poole, Ben},
	month = jun,
	year = {2022},
	keywords = {3D, Generation, Machine Learning, ShapeGeneration, Text},
	pages = {857--866},
}

@misc{jetchev_clipmatrix_2021,
	title = {{ClipMatrix}: {Text}-controlled {Creation} of {3D} {Textured} {Meshes}},
	shorttitle = {{ClipMatrix}},
	url = {http://arxiv.org/abs/2109.12922},
	abstract = {If a picture is worth thousand words, a moving 3d shape must be worth a million. We build upon the success of recent generative methods that create images fitting the semantics of a text prompt, and extend it to the controlled generation of 3d objects. We present a novel algorithm for the creation of textured 3d meshes, controlled by text prompts. Our method creates aesthetically pleasing high resolution articulated 3d meshes, and opens new possibilities for automation and AI control of 3d assets. We call it "ClipMatrix" because it leverages CLIP text embeddings to breed new digital 3d creatures, a nod to the Latin meaning of the word "matrix" - "mother". See the online gallery for a full impression of our method's capability.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Jetchev, Nikolay},
	month = sep,
	year = {2021},
	note = {arXiv:2109.12922 [cs]},
	keywords = {3D, Generation, Machine Learning, ShapeGeneration, Text},
}

@misc{chen_text2shape_2018,
	title = {{Text2Shape}: {Generating} {Shapes} from {Natural} {Language} by {Learning} {Joint} {Embeddings}},
	shorttitle = {{Text2Shape}},
	url = {http://arxiv.org/abs/1803.08495},
	abstract = {We present a method for generating colored 3D shapes from natural language. To this end, we first learn joint embeddings of freeform text descriptions and colored 3D shapes. Our model combines and extends learning by association and metric learning approaches to learn implicit cross-modal connections, and produces a joint representation that captures the many-to-many relations between language and physical properties of 3D shapes such as color and shape. To evaluate our approach, we collect a large dataset of natural language descriptions for physical 3D objects in the ShapeNet dataset. With this learned joint embedding we demonstrate text-to-shape retrieval that outperforms baseline approaches. Using our embeddings with a novel conditional Wasserstein GAN framework, we generate colored 3D shapes from text. Our method is the first to connect natural language text with realistic 3D objects exhibiting rich variations in color, texture, and shape detail. See video at https://youtu.be/zraPvRdl13Q},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Chen, Kevin and Choy, Christopher B. and Savva, Manolis and Chang, Angel X. and Funkhouser, Thomas and Savarese, Silvio},
	month = mar,
	year = {2018},
	note = {arXiv:1803.08495 [cs]},
	keywords = {3D, Generation, Machine Learning, ShapeGeneration, Text},
}

@inproceedings{sanghi_clip-forge_2022,
	address = {New Orleans, LA, USA},
	title = {{CLIP}-{Forge}: {Towards} {Zero}-{Shot} {Text}-to-{Shape} {Generation}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{CLIP}-{Forge}},
	url = {https://ieeexplore.ieee.org/document/9878592/},
	doi = {10.1109/CVPR52688.2022.01805},
	abstract = {Generating shapes using natural language can enable new ways of imagining and creating the things around us. While significant recent progress has been made in text-to-image generation, text-to-shape generation remains a challenging problem due to the unavailability of paired text and shape data at a large scale. We present a simple yet effective method for zero-shot text-to-shape generation that circumvents such data scarcity. Our proposed method, named CLIP-Forge, is based on a two-stage training process, which only depends on an unlabelled shape dataset and a pre-trained image-text network such as CLIP. Our method has the benefits of avoiding expensive inference time optimization, as well as the ability to generate multiple shapes for a given text. We not only demonstrate promising zero-shot generalization of the CLIP-Forge model qualitatively and quantitatively, but also provide extensive comparative evaluations to better understand its behavior.},
	language = {en},
	urldate = {2023-06-28},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Sanghi, Aditya and Chu, Hang and Lambourne, Joseph G. and Wang, Ye and Cheng, Chin-Yi and Fumero, Marco and Malekshan, Kamal Rahimi},
	month = jun,
	year = {2022},
	keywords = {3D, Generation, Machine Learning, ShapeGeneration, Text},
	pages = {18582--18592},
}

@misc{zhang_complete_2023,
	title = {A {Complete} {Survey} on {Generative} {AI} ({AIGC}): {Is} {ChatGPT} from {GPT}-4 to {GPT}-5 {All} {You} {Need}?},
	shorttitle = {A {Complete} {Survey} on {Generative} {AI} ({AIGC})},
	url = {http://arxiv.org/abs/2303.11717},
	abstract = {As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible for us to miss the opportunity to glimpse AIGC from a certain angle. In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many people are wondering about its limits: can GPT-5 (or other future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? Toward answering this question, a comprehensive review of existing AIGC tasks is needed. As such, our work comes to fill this gap promptly by offering a first look at AIGC, ranging from its techniques to applications. Modern generative AI relies on various technical foundations, ranging from model architecture and self-supervised pretraining to generative modeling methods (like GAN and diffusion models). After introducing the fundamental techniques, this work focuses on the technological development of various AIGC tasks based on their output type, including text, images, videos, 3D content, etc., which depicts the full potential of ChatGPT's future. Moreover, we summarize their significant applications in some mainstream industries, such as education and creativity content. Finally, we discuss the challenges currently faced and present an outlook on how generative AI might evolve in the near future.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Zhang, Chaoning and Zhang, Chenshuang and Zheng, Sheng and Qiao, Yu and Li, Chenghao and Zhang, Mengchun and Dam, Sumit Kumar and Thwal, Chu Myaet and Tun, Ye Lin and Huy, Le Luang and kim, Donguk and Bae, Sung-Ho and Lee, Lik-Hang and Yang, Yang and Shen, Heng Tao and Kweon, In So and Hong, Choong Seon},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11717 [cs]},
	keywords = {3D, Generation, Machine Learning, Survey},
}

@inproceedings{park_normalized_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Normalized {Contrastive} {Learning} for {Text}-{Video} {Retrieval}},
	shorttitle = {{NCL}},
	url = {https://aclanthology.org/2022.emnlp-main.17},
	abstract = {Cross-modal contrastive learning has led the recent advances in multimodal retrieval with its simplicity and effectiveness. In this work, however, we reveal that cross-modal contrastive learning suffers from incorrect normalization of the sum retrieval probabilities of each text or video instance. Specifically, we show that many test instances are either over- or under-represented during retrieval, significantly hurting the retrieval performance. To address this problem, we propose Normalized Contrastive Learning (NCL) which utilizes the Sinkhorn-Knopp algorithm to compute the instance-wise biases that properly normalize the sum retrieval probabilities of each instance so that every text and video instance is fairly represented during cross-modal retrieval. Empirical study shows that NCL brings consistent and significant gains in text-video retrieval on different model architectures, with new state-of-the-art multimodal retrieval metrics on the ActivityNet, MSVD, and MSR-VTT datasets without any architecture engineering.},
	urldate = {2023-05-10},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Park, Yookoon and Azab, Mahmoud and Moon, Seungwhan and Xiong, Bo and Metze, Florian and Kundu, Gourab and Ahmed, Kirmani},
	month = dec,
	year = {2022},
	keywords = {Untagged},
	pages = {248--260},
}

@inproceedings{9710829,
	title = {Z-score normalization, hubness, and few-shot learning},
	doi = {10.1109/ICCV48922.2021.00021},
	booktitle = {2021 {IEEE}/{CVF} international conference on computer vision ({ICCV})},
	author = {Fei, Nanyi and Gao, Yizhao and Lu, Zhiwu and Xiang, Tao},
	year = {2021},
	pages = {142--151},
}

@inproceedings{liu_cross-modal_2022,
	address = {Dublin, Ireland},
	title = {Cross-{Modal} {Discrete} {Representation} {Learning}},
	url = {https://aclanthology.org/2022.acl-long.215},
	doi = {10.18653/v1/2022.acl-long.215},
	abstract = {In contrast to recent advances focusing on high-level representation learning across modalities, in this work we present a self-supervised learning framework that is able to learn a representation that captures finer levels of granularity across different modalities such as concepts or events represented by visual objects or spoken words. Our framework relies on a discretized embedding space created via vector quantization that is shared across different modalities. Beyond the shared embedding space, we propose a Cross-Modal Code Matching objective that forces the representations from different views (modalities) to have a similar distribution over the discrete embedding space such that cross-modal objects/actions localization can be performed without direct supervision. We show that the proposed discretized multi-modal fine-grained representation (e.g., pixel/word/frame) can complement high-level summary representations (e.g., video/sentence/waveform) for improved performance on cross-modal retrieval tasks. We also observe that the discretized representation uses individual clusters to represent the same semantic concept across modalities.},
	urldate = {2022-07-09},
	booktitle = {Annual {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Alexander and Jin, SouYoung and Lai, Cheng-I and Rouditchenko, Andrew and Oliva, Aude and Glass, James},
	year = {2022},
	keywords = {Sparse},
	pages = {3013--3035},
}

@inproceedings{cao_visual_2022,
	title = {Visual {Consensus} {Modeling} for {Video}-{Text} {Retrieval}},
	volume = {36},
	shorttitle = {{VCM}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/19891},
	doi = {10.1609/aaai.v36i1.19891},
	abstract = {In this paper, we propose a novel method to mine the commonsense knowledge shared between the video and text modalities for video-text retrieval, namely visual consensus modeling. Different from the existing works, which learn the video and text representations and their complicated relationships solely based on the pairwise video-text data, we make the first attempt to model the visual consensus by mining the visual concepts from videos and exploiting their co-occurrence patterns within the video and text modalities with no reliance on any additional concept annotations. Specifically, we build a shareable and learnable graph as the visual consensus, where the nodes denoting the mined visual concepts and the edges connecting the nodes representing the co-occurrence relationships between the visual concepts. Extensive experimental results on the public benchmark datasets demonstrate that our proposed method, with the ability to effectively model the visual consensus, achieves the state-of-the-art performances on the bidirectional video-text retrieval task. Our code is available at https://github.com/sqiangcao99/VCM.},
	language = {en},
	urldate = {2022-10-04},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Cao, Shuqiang and Wang, Bairui and Zhang, Wei and Ma, Lin},
	year = {2022},
	keywords = {Sparse},
	pages = {167--175},
}

@inproceedings{wang_object-aware_2022,
	address = {New Orleans, LA, USA},
	title = {Object-aware {Video}-language {Pre}-training for {Retrieval}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{OA}-{Trans}},
	url = {https://ieeexplore.ieee.org/document/9878421/},
	doi = {10.1109/CVPR52688.2022.00331},
	abstract = {Recently, by introducing large-scale dataset and strong transformer network, video-language pre-training has shown great success especially for retrieval. Yet, existing video-language transformer models do not explicitly finegrained semantic align. In this work, we present Objectaware Transformers, an object-centric approach that extends video-language transformer to incorporate object representations. The key idea is to leverage the bounding boxes and object tags to guide the training process. We evaluate our model on three standard sub-tasks of videotext matching on four widely used benchmarks. We also provide deep analysis and detailed ablation about the proposed method. We show clear improvement in performance across all tasks and datasets considered, demonstrating the value of a model that incorporates object representations into a video-language architecture. The code has been released in https://github.com/FingerRec/OATransformer.},
	language = {en},
	urldate = {2022-10-19},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, Alex Jinpeng and Ge, Yixiao and Cai, Guanyu and Yan, Rui and Lin, Xudong and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
	month = jun,
	year = {2022},
	keywords = {Language, Machine Learning, Multimodal, Retrieval, Sparse, Video},
	pages = {3303--3312},
}

@inproceedings{wang_boosting_2022,
	address = {New York, NY, USA},
	series = {{MM} '22},
	title = {Boosting {Video}-{Text} {Retrieval} with {Explicit} {High}-{Level} {Semantics}},
	isbn = {978-1-4503-9203-7},
	shorttitle = {{HiSE}},
	url = {https://doi.org/10.1145/3503161.3548010},
	doi = {10.1145/3503161.3548010},
	abstract = {Video-text retrieval (VTR) is an attractive yet challenging task for multi-modal understanding, which aims to search for relevant video (text) given a query (video). Existing methods typically employ completely heterogeneous visual-textual information to align video and text, whilst lacking the awareness of homogeneous high-level semantic information residing in both modalities. To fill this gap, in this work, we propose a novel visual-linguistic aligning model named HiSE for VTR, which improves the cross-modal representation by incorporating explicit high-level semantics. First, we explore the hierarchical property of explicit high-level semantics, and further decompose it into two levels, i.e. discrete semantics and holistic semantics. Specifically, for visual branch, we exploit an off-the-shelf semantic entity predictor to generate discrete high-level semantics. In parallel, a trained video captioning model is employed to output holistic high-level semantics. As for the textual modality, we parse the text into three parts including occurrence, action and entity. In particular, the occurrence corresponds to the holistic high-level semantics, meanwhile both action and entity represent the discrete ones. Then, different graph reasoning techniques are utilized to promote the interaction between holistic and discrete high-level semantics. Extensive experiments demonstrate that, with the aid of explicit high-level semantics, our method achieves the superior performance over state-of-the-art methods on three benchmark datasets, including MSR-VTT, MSVD and DiDeMo.},
	urldate = {2022-10-19},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Haoran and Xu, Di and He, Dongliang and Li, Fu and Ji, Zhong and Han, Jungong and Ding, Errui},
	year = {2022},
	keywords = {Sparse},
	pages = {4887--4898},
}

@inproceedings{chen_tagging_2023,
	title = {Tagging before {Alignment}: {Integrating} {Multi}-{Modal} {Tags} for {Video}-{Text} {Retrieval}},
	shorttitle = {{TABLE}},
	url = {http://arxiv.org/abs/2301.12644},
	abstract = {Vision-language alignment learning for video-text retrieval arouses a lot of attention in recent years. Most of the existing methods either transfer the knowledge of image-text pretraining model to video-text retrieval task without fully exploring the multi-modal information of videos, or simply fuse multi-modal features in a brute force manner without explicit guidance. In this paper, we integrate multi-modal information in an explicit manner by tagging, and use the tags as the anchors for better video-text alignment. Various pretrained experts are utilized for extracting the information of multiple modalities, including object, person, motion, audio, etc. To take full advantage of these information, we propose the TABLE (TAgging Before aLignmEnt) network, which consists of a visual encoder, a tag encoder, a text encoder, and a tag-guiding cross-modal encoder for jointly encoding multi-frame visual features and multi-modal tags information. Furthermore, to strengthen the interaction between video and text, we build a joint cross-modal encoder with the triplet input of [vision, tag, text] and perform two additional supervised tasks, Video Text Matching (VTM) and Masked Language Modeling (MLM). Extensive experimental results demonstrate that the TABLE model is capable of achieving State-Of-The-Art (SOTA) performance on various video-text retrieval benchmarks, including MSR-VTT, MSVD, LSMDC and DiDeMo.},
	urldate = {2023-06-05},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {arXiv},
	author = {Chen, Yizhen and Wang, Jie and Lin, Lijian and Qi, Zhongang and Ma, Jin and Shan, Ying},
	month = jan,
	year = {2023},
	note = {arXiv:2301.12644 [cs]},
	keywords = {Sparse},
}

@inproceedings{chen_litevl_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{LiteVL}: {Efficient} {Video}-{Language} {Learning} with {Enhanced} {Spatial}-{Temporal} {Modeling}},
	shorttitle = {{LiteVL}},
	url = {https://aclanthology.org/2022.emnlp-main.545},
	abstract = {Recent large-scale video-language pre-trained models have shown appealing performance on various downstream tasks. However, the pre-training process is computationally expensive due to the requirement of millions of video-text pairs and the redundant data structure of each video. To mitigate these problems, we propose LiteVL, which adapts a pre-trained image-language model BLIP into a video-text model directly on downstream tasks, without heavy pre-training. To enhance the temporal modeling lacking in the image-language model, we propose to add temporal attention modules in the image encoder of BLIP with dynamic temporal scaling. Besides the model-wise adaptation, we also propose a non-parametric pooling mechanism to adaptively reweight the fine-grained video embedding conditioned on the text. Experimental results on text-video retrieval and video question answering show that the proposed LiteVL even outperforms previous video-language pre-trained models by a clear margin, though without any video-language pre-training.},
	urldate = {2023-05-10},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Dongsheng and Tao, Chaofan and Hou, Lu and Shang, Lifeng and Jiang, Xin and Liu, Qun},
	month = dec,
	year = {2022},
	keywords = {Untagged},
	pages = {7985--7997},
}

@misc{ost_neural_2021,
	title = {Neural {Scene} {Graphs} for {Dynamic} {Scenes}},
	url = {http://arxiv.org/abs/2011.10379},
	abstract = {Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efficient representations of static scenes that encode all scene objects into a single neural network, and lack the ability to represent dynamic scenes and decompositions into individual scene objects. In this work, we present the first neural rendering method that decomposes dynamic scenes into scene graphs. We propose a learned scene graph representation, which encodes object transformation and radiance, to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes -- only by observing a video of this scene -- and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Ost, Julian and Mannan, Fahim and Thuerey, Nils and Knodt, Julian and Heide, Felix},
	month = mar,
	year = {2021},
	note = {arXiv:2011.10379 [cs]},
	keywords = {3D, ComputerVision, Machine Learning, Scene Generation},
}

@article{yang_unisim_nodate,
	title = {{UniSim}: {A} {Neural} {Closed}-{Loop} {Sensor} {Simulator}},
	language = {en},
	author = {Yang, Ze and Chen, Yun and Wang, Jingkang and Manivasagam, Sivabalan and Ma, Wei-Chiu and Yang, Anqi Joyce and Urtasun, Raquel},
	keywords = {3D, ComputerVision, Machine Learning, Scene Generation},
}

@misc{manivasagam_lidarsim_2020,
	title = {{LiDARsim}: {Realistic} {LiDAR} {Simulation} by {Leveraging} the {Real} {World}},
	shorttitle = {{LiDARsim}},
	url = {http://arxiv.org/abs/2006.09348},
	abstract = {We tackle the problem of producing realistic simulations of LiDAR point clouds, the sensor of preference for most self-driving vehicles. We argue that, by leveraging real data, we can simulate the complex world more realistically compared to employing virtual worlds built from CAD/procedural models. Towards this goal, we first build a large catalog of 3D static maps and 3D dynamic objects by driving around several cities with our self-driving fleet. We can then generate scenarios by selecting a scene from our catalog and "virtually" placing the self-driving vehicle (SDV) and a set of dynamic objects from the catalog in plausible locations in the scene. To produce realistic simulations, we develop a novel simulator that captures both the power of physics-based and learning-based simulation. We first utilize ray casting over the 3D scene and then use a deep neural network to produce deviations from the physics-based simulation, producing realistic LiDAR point clouds. We showcase LiDARsim's usefulness for perception algorithms-testing on long-tail events and end-to-end closed-loop evaluation on safety-critical scenarios.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Manivasagam, Sivabalan and Wang, Shenlong and Wong, Kelvin and Zeng, Wenyuan and Sazanovich, Mikita and Tan, Shuhan and Yang, Bin and Ma, Wei-Chiu and Urtasun, Raquel},
	month = jun,
	year = {2020},
	note = {arXiv:2006.09348 [cs]},
	keywords = {3D, ComputerVision, Machine Learning, Scene Generation},
}

@misc{riegler_free_2020,
	title = {Free {View} {Synthesis}},
	url = {http://arxiv.org/abs/2008.05511},
	abstract = {We present a method for novel view synthesis from input images that are freely distributed around a scene. Our method does not rely on a regular arrangement of input views, can synthesize images for free camera movement through the scene, and works for general scenes with unconstrained geometric layouts. We calibrate the input images via SfM and erect a coarse geometric scaffold via MVS. This scaffold is used to create a proxy depth map for a novel view of the scene. Based on this depth map, a recurrent encoder-decoder network processes reprojected features from nearby views and synthesizes the new view. Our network does not need to be optimized for a given scene. After training on a dataset, it works in previously unseen environments with no fine-tuning or per-scene optimization. We evaluate the presented approach on challenging real-world datasets, including Tanks and Temples, where we demonstrate successful view synthesis for the first time and substantially outperform prior and concurrent work.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Riegler, Gernot and Koltun, Vladlen},
	month = aug,
	year = {2020},
	note = {arXiv:2008.05511 [cs]},
	keywords = {3D, ComputerVision, Machine Learning, Scene Generation},
}

@misc{girdhar_imagebind_2023,
	title = {{ImageBind}: {One} {Embedding} {Space} {To} {Bind} {Them} {All}},
	shorttitle = {{ImageBind}},
	url = {http://arxiv.org/abs/2305.05665},
	abstract = {We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
	month = may,
	year = {2023},
	note = {arXiv:2305.05665 [cs]},
	keywords = {Generation, Machine Learning, Multimodal, Retrieval},
}

@article{chen_neuraleditor_nodate,
	title = {{NeuralEditor}: {Editing} {Neural} {Radiance} {Fields} via {Manipulating} {Point} {Clouds}},
	abstract = {This paper proposes NeuralEditor that enables neural radiance fields (NeRFs) natively editable for general shape editing tasks. Despite their impressive results on novel-view synthesis, it remains a fundamental challenge for NeRFs to edit the shape of the scene. Our key insight is to exploit the explicit point cloud representation as the underlying structure to construct NeRFs, inspired by the intuitive interpretation of NeRF rendering as a process that projects or “plots” the associated 3D point cloud to a 2D image plane. To this end, NeuralEditor introduces a novel rendering scheme based on deterministic integration within K-D tree-guided density-adaptive voxels, which produces both high-quality rendering results and precise point clouds through optimization. NeuralEditor then performs shape editing via mapping associated points between point clouds. Extensive evaluation shows that NeuralEditor achieves state-ofthe-art performance in both shape deformation and scene morphing tasks. Notably, NeuralEditor supports both zeroshot inference and further fine-tuning over the edited scene. Our code, benchmark, and demo video are available at immortalco.github.io/NeuralEditor.},
	language = {en},
	author = {Chen, Jun-Kun and Lyu, Jipeng and Wang, Yu-Xiong},
	keywords = {3D, ComputerVision, Editing, Machine Learning},
}

@misc{gu_learning_2023,
	title = {Learning {Controllable} {3D} {Diffusion} {Models} from {Single}-view {Images}},
	url = {http://arxiv.org/abs/2304.06700},
	abstract = {Diffusion models have recently become the de-facto approach for generative modeling in the 2D domain. However, extending diffusion models to 3D is challenging due to the difficulties in acquiring 3D ground truth data for training. On the other hand, 3D GANs that integrate implicit 3D representations into GANs have shown remarkable 3D-aware generation when trained only on single-view image datasets. However, 3D GANs do not provide straightforward ways to precisely control image synthesis. To address these challenges, We present Control3Diff, a 3D diffusion model that combines the strengths of diffusion models and 3D GANs for versatile, controllable 3D-aware image synthesis for single-view datasets. Control3Diff explicitly models the underlying latent distribution (optionally conditioned on external inputs), thus enabling direct control during the diffusion process. Moreover, our approach is general and applicable to any type of controlling input, allowing us to train it with the same diffusion objective without any auxiliary supervision. We validate the efficacy of Control3Diff on standard image generation benchmarks, including FFHQ, AFHQ, and ShapeNet, using various conditioning inputs such as images, sketches, and text prompts. Please see the project website ({\textbackslash}url\{https://jiataogu.me/control3diff\}) for video comparisons.},
	urldate = {2023-06-02},
	publisher = {arXiv},
	author = {Gu, Jiatao and Gao, Qingzhe and Zhai, Shuangfei and Chen, Baoquan and Liu, Lingjie and Susskind, Josh},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06700 [cs]},
}

@inproceedings{zhu_curricular_2023,
	title = {Curricular {Object} {Manipulation} in {LiDAR}-{Based} {Object} {Detection}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Curricular_Object_Manipulation_in_LiDAR-Based_Object_Detection_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-02},
	author = {Zhu, Ziyue and Meng, Qiang and Wang, Xiao and Wang, Ke and Yan, Liujiang and Yang, Jian},
	year = {2023},
	pages = {1125--1135},
}

@inproceedings{sung_deep_2017,
	title = {Deep multimodal embedding: {Manipulating} novel objects with point-clouds, language and trajectories},
	shorttitle = {Deep multimodal embedding},
	doi = {10.1109/ICRA.2017.7989325},
	abstract = {A robot operating in a real-world environment needs to perform reasoning over a variety of sensor modalities such as vision, language and motion trajectories. However, it is extremely challenging to manually design features relating such disparate modalities. In this work, we introduce an algorithm that learns to embed point-cloud, natural language, and manipulation trajectory data into a shared embedding space with a deep neural network. To learn semantically meaningful spaces throughout our network, we use a loss-based margin to bring embeddings of relevant pairs closer together while driving less-relevant cases from different modalities further apart. We use this both to pre-train its lower layers and fine-tune our final embedding space, leading to a more robust representation. We test our algorithm on the task of manipulating novel objects and appliances based on prior experience with other objects. On a large dataset, we achieve significant improvements in both accuracy and inference time over the previous state of the art. We also perform end-to-end experiments on a PR2 robot utilizing our learned embedding space.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Sung, Jaeyong and Lenz, Ian and Saxena, Ashutosh},
	month = may,
	year = {2017},
	keywords = {3D, Generation, Machine Learning, Multimodal, ShapeGeneration, Text},
	pages = {2794--2801},
}

@article{xu_dream3d_nodate,
	title = {{Dream3D}: {Zero}-{Shot} {Text}-to-{3D} {Synthesis} {Using} {3D} {Shape} {Prior} and {Text}-to-{Image} {Diffusion} {Models}},
	language = {en},
	author = {Xu, Jiale and Wang, Xintao and Cheng, Weihao and Cao, Yan-Pei and Shan, Ying and Qie, Xiaohu and Gao, Shenghua},
	keywords = {3D, Generation, Machine Learning, Multimodal, ShapeGeneration, Text},
}

@misc{zhan_multimodal_2023,
	title = {Multimodal {Image} {Synthesis} and {Editing}: {A} {Survey}},
	shorttitle = {Multimodal {Image} {Synthesis} and {Editing}},
	url = {http://arxiv.org/abs/2112.13592},
	abstract = {As information exists in various modalities in real world, effective interaction and fusion among multimodal information plays a key role for the creation and perception of multimodal data in computer vision and deep learning research. With superb power in modeling the interaction among multimodal information, multimodal image synthesis and editing has become a hot research topic in recent years. Instead of providing explicit guidance for network training, multimodal guidance offers intuitive and flexible means for image synthesis and editing. On the other hand, this field is also facing several challenges in alignment of multimodal features, synthesis of high-resolution images, faithful evaluation metrics, etc. In this survey, we comprehensively contextualize the advance of the recent multimodal image synthesis and editing and formulate taxonomies according to data modalities and model types. We start with an introduction to different guidance modalities in image synthesis and editing, and then describe multimodal image synthesis and editing approaches extensively according to their model types. After that, we describe benchmark datasets and evaluation metrics as well as corresponding experimental results. Finally, we provide insights about the current research challenges and possible directions for future research. A project associated with this survey is available at https://github.com/fnzhan/MISE.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Zhan, Fangneng and Yu, Yingchen and Wu, Rongliang and Zhang, Jiahui and Lu, Shijian and Liu, Lingjie and Kortylewski, Adam and Theobalt, Christian and Xing, Eric},
	month = apr,
	year = {2023},
	note = {arXiv:2112.13592 [cs]},
	keywords = {Generation, Image, Machine Learning, Multimodal, Text},
}

@misc{hess_lidarclip_2023,
	title = {{LidarCLIP} or: {How} {I} {Learned} to {Talk} to {Point} {Clouds}},
	shorttitle = {{LidarCLIP} or},
	url = {http://arxiv.org/abs/2212.06858},
	abstract = {Research connecting text and images has recently seen several breakthroughs, with models like CLIP, DALL-E 2, and Stable Diffusion. However, the connection between text and other visual modalities, such as lidar data, has received less attention, prohibited by the lack of text-lidar datasets. In this work, we propose LidarCLIP, a mapping from automotive point clouds to a pre-existing CLIP embedding space. Using image-lidar pairs, we supervise a point cloud encoder with the image CLIP embeddings, effectively relating text and lidar data with the image domain as an intermediary. We show the effectiveness of LidarCLIP by demonstrating that lidar-based retrieval is generally on par with image-based retrieval, but with complementary strengths and weaknesses. By combining image and lidar features, we improve upon both single-modality methods and enable a targeted search for challenging detection scenarios under adverse sensor conditions. We also explore zero-shot classification and show that LidarCLIP outperforms existing attempts to use CLIP for point clouds by a large margin. Finally, we leverage our compatibility with CLIP to explore a range of applications, such as point cloud captioning and lidar-to-image generation, without any additional training. Code and pre-trained models are available at https://github.com/atonderski/lidarclip.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Hess, Georg and Tonderski, Adam and Petersson, Christoffer and Åström, Kalle and Svensson, Lennart},
	month = may,
	year = {2023},
	note = {arXiv:2212.06858 [cs]},
}

@article{sokhandan_domain-compatible_nodate,
	title = {Domain-{Compatible} {Synthetic} {Data} {Generation} for {Infrequent} {Objects} {Detection}},
	abstract = {The recent advances in generative models have resulted in massive progress in the quality of the generated images to the point that in many cases they cannot be easily distinguished from real images. Despite this quality improvement, using AI generated images for the purpose of training robust down-steam computer vision models for real-world applications has proven to be very challenging. The AI generated images usually lack the required diversity and scene complexity that is crucial for many real-world applications, speciﬁcally the ones with safety concerns. The di culty of this challenge grows signiﬁcantly when the underlying application involves detection of some speciﬁc objects that appear with critically low frequency in the available real datasets. This paper studies a new approach for generating diverse, complex and domain-compatible synthetic images for detecting infrequent objects by employing a di↵usion-based generative model pretrained on a generic dataset. More speciﬁcally, the impact of using the generated synthetic images with the proposed approach in solving the real world problem of detecting emergency vehicles in road scenes is investigated. Furthermore, the challenges of generating synthetic datasets with the proposed approach will be thoroughly discussed.},
	language = {en},
	author = {Sokhandan, Negin and Kulkarni, Ninad and Shah, Yash and Sathyanarayana, Suchitra},
	keywords = {3D, Generation, Machine Learning, Multimodal, ShapeGeneration, Text},
}

@inproceedings{liu_towards_2022,
	address = {New Orleans, LA, USA},
	title = {Towards {Implicit} {Text}-{Guided} {3D} {Shape} {Generation}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9878629/},
	doi = {10.1109/CVPR52688.2022.01737},
	abstract = {In this work, we explore the challenging task of generating 3D shapes from text. Beyond the existing works, we propose a new approach for text-guided 3D shape generation, capable of producing high-fidelity shapes with colors that match the given text description. This work has several technical contributions. First, we decouple the shape and color predictions for learning features in both texts and shapes, and propose the word-level spatial transformer to correlate word features from text with spatial features from shape. Also, we design a cyclic loss to encourage consistency between text and shape, and introduce the shape IMLE to diversify the generated shapes. Further, we extend the framework to enable text-guided shape manipulation. Extensive experiments on the largest existing text-shape benchmark [10] manifest the superiority of this work. The code and the models are available at https://github.com/liuzhengzhe/Towards- ImplicitText-Guided-Shape-Generation.},
	language = {en},
	urldate = {2023-06-13},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Liu, Zhengzhe and Wang, Yi and Qi, Xiaojuan and Fu, Chi-Wing},
	month = jun,
	year = {2022},
	keywords = {3D, Generation, Machine Learning, Multimodal, ShapeGeneration, Text},
	pages = {17875--17885},
}

@inproceedings{bai_transfusion_2022,
	address = {New Orleans, LA, USA},
	title = {{TransFusion}: {Robust} {LiDAR}-{Camera} {Fusion} for {3D} {Object} {Detection} with {Transformers}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{TransFusion}},
	url = {https://ieeexplore.ieee.org/document/9879824/},
	doi = {10.1109/CVPR52688.2022.00116},
	abstract = {LiDAR and camera are two important sensors for 3D object detection in autonomous driving. Despite the increasing popularity of sensor fusion in this field, the robustness against inferior image conditions, e.g., bad illumination and sensor misalignment, is under-explored. Existing fusion methods are easily affected by such conditions, mainly due to a hard association of LiDAR points and image pixels, established by calibration matrices.},
	language = {en},
	urldate = {2023-06-13},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Bai, Xuyang and Hu, Zeyu and Zhu, Xinge and Huang, Qingqiu and Chen, Yilun and Fu, Hangbo and Tai, Chiew-Lan},
	month = jun,
	year = {2022},
	keywords = {3D, ComputerVision, Detection, Image, Machine Learning},
	pages = {1080--1089},
}

@misc{stan_ldm3d_2023,
	title = {{LDM3D}: {Latent} {Diffusion} {Model} for {3D}},
	shorttitle = {{LDM3D}},
	url = {http://arxiv.org/abs/2305.10853},
	abstract = {This research paper proposes a Latent Diffusion Model for 3D (LDM3D) that generates both image and depth map data from a given text prompt, allowing users to generate RGBD images from text prompts. The LDM3D model is fine-tuned on a dataset of tuples containing an RGB image, depth map and caption, and validated through extensive experiments. We also develop an application called DepthFusion, which uses the generated RGB images and depth maps to create immersive and interactive 360-degree-view experiences using TouchDesigner. This technology has the potential to transform a wide range of industries, from entertainment and gaming to architecture and design. Overall, this paper presents a significant contribution to the field of generative AI and computer vision, and showcases the potential of LDM3D and DepthFusion to revolutionize content creation and digital experiences. A short video summarizing the approach can be found at https://t.ly/tdi2.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Stan, Gabriela Ben Melech and Wofk, Diana and Fox, Scottie and Redden, Alex and Saxton, Will and Yu, Jean and Aflalo, Estelle and Tseng, Shao-Yen and Nonato, Fabio and Muller, Matthias and Lal, Vasudev},
	month = may,
	year = {2023},
	note = {arXiv:2305.10853 [cs]},
	keywords = {3D, ComputerVision, DiffusionModel, Generation, Machine Learning},
}

@misc{gu_learning_2023-1,
	title = {Learning {Controllable} {3D} {Diffusion} {Models} from {Single}-view {Images}},
	url = {http://arxiv.org/abs/2304.06700},
	abstract = {Diffusion models have recently become the de-facto approach for generative modeling in the 2D domain. However, extending diffusion models to 3D is challenging due to the difficulties in acquiring 3D ground truth data for training. On the other hand, 3D GANs that integrate implicit 3D representations into GANs have shown remarkable 3D-aware generation when trained only on single-view image datasets. However, 3D GANs do not provide straightforward ways to precisely control image synthesis. To address these challenges, We present Control3Diff, a 3D diffusion model that combines the strengths of diffusion models and 3D GANs for versatile, controllable 3D-aware image synthesis for single-view datasets. Control3Diff explicitly models the underlying latent distribution (optionally conditioned on external inputs), thus enabling direct control during the diffusion process. Moreover, our approach is general and applicable to any type of controlling input, allowing us to train it with the same diffusion objective without any auxiliary supervision. We validate the efficacy of Control3Diff on standard image generation benchmarks, including FFHQ, AFHQ, and ShapeNet, using various conditioning inputs such as images, sketches, and text prompts. Please see the project website ({\textbackslash}url\{https://jiataogu.me/control3diff\}) for video comparisons.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Gu, Jiatao and Gao, Qingzhe and Zhai, Shuangfei and Chen, Baoquan and Liu, Lingjie and Susskind, Josh},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06700 [cs]},
}

@inproceedings{luo_diffusion_2021,
	address = {Nashville, TN, USA},
	title = {Diffusion {Probabilistic} {Models} for {3D} {Point} {Cloud} {Generation}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578791/},
	doi = {10.1109/CVPR46437.2021.00286},
	abstract = {We present a probabilistic model for point cloud generation, which is fundamental for various 3D vision tasks such as shape completion, upsampling, synthesis and data augmentation. Inspired by the diffusion process in nonequilibrium thermodynamics, we view points in point clouds as particles in a thermodynamic system in contact with a heat bath, which diffuse from the original distribution to a noise distribution. Point cloud generation thus amounts to learning the reverse diffusion process that transforms the noise distribution to the distribution of a desired shape. Speciﬁcally, we propose to model the reverse diffusion process for point clouds as a Markov chain conditioned on certain shape latent. We derive the variational bound in closed form for training and provide implementations of the model. Experimental results demonstrate that our model achieves competitive performance in point cloud generation and auto-encoding. The code is available at https://github.com/luost26/diffusionpoint-cloud.},
	language = {en},
	urldate = {2023-06-02},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Luo, Shitong and Hu, Wei},
	month = jun,
	year = {2021},
	keywords = {3D, ComputerVision, DiffusionModel, Generation, Machine Learning},
	pages = {2836--2844},
}

@inproceedings{kim_diffusionclip_2022,
	address = {New Orleans, LA, USA},
	title = {{DiffusionCLIP}: {Text}-{Guided} {Diffusion} {Models} for {Robust} {Image} {Manipulation}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{DiffusionCLIP}},
	url = {https://ieeexplore.ieee.org/document/9879284/},
	doi = {10.1109/CVPR52688.2022.00246},
	language = {en},
	urldate = {2023-06-13},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Kim, Gwanghyun and Kwon, Taesung and Ye, Jong Chul},
	month = jun,
	year = {2022},
	keywords = {ComputerVision, DiffusionModel, Machine Learning, Multimodal, Text, Untagged},
	pages = {2416--2425},
}

@misc{gallego_personalizing_2022,
	title = {Personalizing {Text}-to-{Image} {Generation} via {Aesthetic} {Gradients}},
	url = {http://arxiv.org/abs/2209.12330},
	abstract = {This work proposes aesthetic gradients, a method to personalize a CLIP-conditioned diffusion model by guiding the generative process towards custom aesthetics defined by the user from a set of images. The approach is validated with qualitative and quantitative experiments, using the recent stable diffusion model and several aesthetically-filtered datasets. Code is released at https://github.com/vicgalle/stable-diffusion-aesthetic-gradients},
	urldate = {2022-10-24},
	publisher = {arXiv},
	author = {Gallego, Victor},
	month = sep,
	year = {2022},
	note = {arXiv:2209.12330 [cs]},
	keywords = {ComputerVision, DiffusionModel, Machine Learning, Multimodal, Text, Untagged},
}

@misc{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2105.05233},
	abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\${\textbackslash}times\$128, 4.59 on ImageNet 256\${\textbackslash}times\$256, and 7.72 on ImageNet 512\${\textbackslash}times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\${\textbackslash}times\$256 and 3.85 on ImageNet 512\${\textbackslash}times\$512. We release our code at https://github.com/openai/guided-diffusion},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	month = jun,
	year = {2021},
	note = {arXiv:2105.05233 [cs, stat]},
}

@misc{trabucco_effective_2023,
	title = {Effective {Data} {Augmentation} {With} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2302.07944},
	abstract = {Data augmentation is one of the most prevalent tools in deep learning, underpinning many recent advances, including those from classification, generative models, and representation learning. The standard approach to data augmentation combines simple transformations like rotations and flips to generate new images from existing ones. However, these new images lack diversity along key semantic axes present in the data. Current augmentations cannot alter the high-level semantic attributes, such as animal species present in a scene, to enhance the diversity of data. We address the lack of diversity in data augmentation with image-to-image transformations parameterized by pre-trained text-to-image diffusion models. Our method edits images to change their semantics using an off-the-shelf diffusion model, and generalizes to novel visual concepts from a few labelled examples. We evaluate our approach on few-shot image classification tasks, and on a real-world weed recognition task, and observe an improvement in accuracy in tested domains.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Trabucco, Brandon and Doherty, Kyle and Gurinas, Max and Salakhutdinov, Ruslan},
	month = may,
	year = {2023},
	note = {arXiv:2302.07944 [cs]},
	keywords = {ComputerVision, DiffusionModel, Generation, Machine Learning},
}

@misc{zhang_recognize_2023,
	title = {Recognize {Anything}: {A} {Strong} {Image} {Tagging} {Model}},
	shorttitle = {Recognize {Anything}},
	url = {http://arxiv.org/abs/2306.03514},
	abstract = {We present the Recognize Anything Model (RAM): a strong foundation model for image tagging. RAM makes a substantial step for large models in computer vision, demonstrating the zero-shot ability to recognize any common category with high accuracy. RAM introduces a new paradigm for image tagging, leveraging large-scale image-text pairs for training instead of manual annotations. The development of RAM comprises four key steps. Firstly, annotation-free image tags are obtained at scale through automatic text semantic parsing. Subsequently, a preliminary model is trained for automatic annotation by unifying the caption and tagging tasks, supervised by the original texts and parsed tags, respectively. Thirdly, a data engine is employed to generate additional annotations and clean incorrect ones. Lastly, the model is retrained with the processed data and fine-tuned using a smaller but higher-quality dataset. We evaluate the tagging capabilities of RAM on numerous benchmarks and observe impressive zero-shot performance, significantly outperforming CLIP and BLIP. Remarkably, RAM even surpasses the fully supervised manners and exhibits competitive performance with the Google tagging API. We are releasing the RAM at {\textbackslash}url\{https://recognize-anything.github.io/\} to foster the advancements of large models in computer vision.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Zhang, Youcai and Huang, Xinyu and Ma, Jinyu and Li, Zhaoyang and Luo, Zhaochuan and Xie, Yanchun and Qin, Yuzhuo and Luo, Tong and Li, Yaqian and Liu, Shilong and Guo, Yandong and Zhang, Lei},
	month = jun,
	year = {2023},
	note = {arXiv:2306.03514 [cs]},
}

@misc{dinu_improving_2015,
	title = {Improving zero-shot learning by mitigating the hubness problem},
	shorttitle = {{GC}},
	url = {http://arxiv.org/abs/1412.6568},
	abstract = {The zero-shot paradigm exploits vector-based word representations extracted from text corpora with unsupervised methods to learn general mapping functions from other feature spaces onto word space, where the words associated to the nearest neighbours of the mapped vectors are used as their linguistic labels. We show that the neighbourhoods of the mapped elements are strongly polluted by hubs, vectors that tend to be near a high proportion of items, pushing their correct labels down the neighbour list. After illustrating the problem empirically, we propose a simple method to correct it by taking the proximity distribution of potential neighbours across many mapped vectors into account. We show that this correction leads to consistent improvements in realistic zero-shot experiments in the cross-lingual, image labeling and image retrieval domains.},
	urldate = {2023-06-10},
	publisher = {arXiv},
	author = {Dinu, Georgiana and Lazaridou, Angeliki and Baroni, Marco},
	month = apr,
	year = {2015},
	note = {arXiv:1412.6568 [cs]},
	keywords = {Hubness, Machine Learning, MachineTranslation, NLP},
}

@article{tomasev_probabilistic_nodate,
	title = {A {Probabilistic} {Approach} to {Nearest}-{Neighbor} {Classiﬁcation}: {Naive} {Hubness} {Bayesian} k {NN}},
	abstract = {Most machine-learning tasks, including classiﬁcation, involve dealing with high-dimensional data. It was recently shown that the phenomenon of hubness, inherent to high-dimensional data, can be exploited to improve methods based on nearest neighbors (NNs). Hubness refers to the emergence of points (hubs) that appear among the k NNs of many other points in the data, and constitute inﬂuential points for kNN classiﬁcation. In this paper, we present a new probabilistic approach to kNN classiﬁcation, naive hubness Bayesian k-nearest neighbor (NHBNN), which employs hubness for computing class likelihood estimates. Experiments show that NHBNN compares favorably to diﬀerent variants of the kNN classiﬁer, including probabilistic kNN (PNN) which is often used as an underlying probabilistic framework for NN classiﬁcation, signifying that NHBNN is a promising alternative framework for developing probabilistic NN algorithms.},
	language = {en},
	author = {Tomašev, Nenad and Radovanovic, Miloš and Mladenic, Dunja},
}

@inproceedings{radovanovic_existence_2010,
	address = {Geneva Switzerland},
	title = {On the existence of obstinate results in vector space models},
	isbn = {978-1-4503-0153-4},
	url = {https://dl.acm.org/doi/10.1145/1835449.1835482},
	doi = {10.1145/1835449.1835482},
	abstract = {The vector space model (VSM) is a popular and widely applied model in information retrieval (IR). VSM creates vector spaces whose dimensionality is usually high (e.g., tens of thousands of terms). This may cause various problems, such as susceptibility to noise and difﬁculty in capturing the underlying semantic structure, which are commonly recognized as different aspects of the “curse of dimensionality.” In this paper, we investigate a novel aspect of the dimensionality curse, which is referred to as hubness and manifested by the tendency of some documents (called hubs) to be included in unexpectedly many search result lists. Hubness may impact VSM considerably since hubs can become obstinate results, irrelevant to a large number of queries, thus harming the performance of an IR system and the experience of its users. We analyze the origins of hubness, showing it is primarily a consequence of high (intrinsic) dimensionality of data, and not a result of other factors such as sparsity and skewness of the distribution of term frequencies. We describe the mechanisms through which hubness emerges by exploring the behavior of similarity measures in high-dimensional vector spaces. Our consideration begins with the classical VSM (tf-idf term weighting and cosine similarity), but the conclusions generalize to more advanced variations, such as Okapi BM25. Moreover, we explain why hubness may not be easily mitigated by dimensionality reduction, and propose a similarity adjustment scheme that takes into account the existence of hubs. Experimental results over real data indicate that signiﬁcant improvement can be obtained through consideration of hubness.},
	language = {en},
	urldate = {2023-06-10},
	booktitle = {Proceedings of the 33rd international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {ACM},
	author = {Radovanović, Milos and Nanopoulos, Alexandros and Ivanović, Mirjana},
	month = jul,
	year = {2010},
	keywords = {Hubness, Recommendation Systems, Retrieval},
	pages = {186--193},
}

@inproceedings{lample2018word,
	title = {Word translation without parallel data},
	shorttitle = {{CSLS}},
	url = {https://openreview.net/forum?id=H196sainb},
	booktitle = {International conference on learning representations},
	author = {Lample, Guillaume and Conneau, Alexis and Ranzato, Marc'Aurelio and Denoyer, Ludovic and Jégou, Hervé},
	year = {2018},
	keywords = {Hubness, Machine Learning, MachineTranslation, NLP, Untagged},
}

@inproceedings{suzuki_centering_2013,
	address = {Seattle, Washington, USA},
	title = {Centering {Similarity} {Measures} to {Reduce} {Hubs}},
	url = {https://aclanthology.org/D13-1058},
	urldate = {2023-02-19},
	booktitle = {Proceedings of the 2013 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Suzuki, Ikumi and Hara, Kazuo and Shimbo, Masashi and Saerens, Marco and Fukumizu, Kenji},
	month = oct,
	year = {2013},
	keywords = {Untagged},
	pages = {613--623},
}

@inproceedings{lazaridou_hubness_2015,
	address = {Beijing, China},
	title = {Hubness and {Pollution}: {Delving} into {Cross}-{Space} {Mapping} for {Zero}-{Shot} {Learning}},
	shorttitle = {Hubness and {Pollution}},
	url = {https://aclanthology.org/P15-1027},
	doi = {10.3115/v1/P15-1027},
	urldate = {2023-06-09},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Lazaridou, Angeliki and Dinu, Georgiana and Baroni, Marco},
	month = jul,
	year = {2015},
	pages = {270--280},
}

@inproceedings{liu_hal_2020,
	title = {{HAL}: {Improved} {Text}-{Image} {Matching} by {Mitigating} {Visual} {Semantic} {Hubs}},
	volume = {34},
	shorttitle = {{HAL}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6823},
	doi = {10.1609/aaai.v34i07.6823},
	abstract = {The hubness problem widely exists in high-dimensional embedding space and is a fundamental source of error for cross-modal matching tasks. In this work, we study the emergence of hubs in Visual Semantic Embeddings (VSE) with application to text-image matching. We analyze the pros and cons of two widely adopted optimization objectives for training VSE and propose a novel hubness-aware loss function (Hal) that addresses previous methods' defects. Unlike (Faghri et al. 2018) which simply takes the hardest sample within a mini-batch, Hal takes all samples into account, using both local and global statistics to scale up the weights of “hubs”. We experiment our method with various configurations of model architectures and datasets. The method exhibits exceptionally good robustness and brings consistent improvement on the task of text-image matching across all settings. Specifically, under the same model architectures as (Faghri et al. 2018) and (Lee et al. 2018), by switching only the learning objective, we report a maximum R@1 improvement of 7.4\% on MS-COCO and 8.3\% on Flickr30k.1},
	language = {en},
	urldate = {2023-06-09},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Liu, Fangyu and Ye, Rongtian and Wang, Xun and Li, Shuaipeng},
	month = apr,
	year = {2020},
	keywords = {Untagged},
	pages = {11563--11571},
}

@inproceedings{liu_strong_2019,
	address = {Florence, Italy},
	title = {A {Strong} and {Robust} {Baseline} for {Text}-{Image} {Matching}},
	url = {https://aclanthology.org/P19-2023},
	doi = {10.18653/v1/P19-2023},
	abstract = {We review the current schemes of text-image matching models and propose improvements for both training and inference. First, we empirically show limitations of two popular loss (sum and max-margin loss) widely used in training text-image embeddings and propose a trade-off: a kNN-margin loss which 1) utilizes information from hard negatives and 2) is robust to noise as all K-most hardest samples are taken into account, tolerating pseudo negatives and outliers. Second, we advocate the use of Inverted Softmax (IS) and Cross-modal Local Scaling (CSLS) during inference to mitigate the so-called hubness problem in high-dimensional embedding space, enhancing scores of all metrics by a large margin.},
	urldate = {2023-06-09},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Student} {Research} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Fangyu and Ye, Rongtian},
	month = jul,
	year = {2019},
	keywords = {Untagged},
	pages = {169--176},
}

@inproceedings{xue_clip-vip_2023,
	title = {{CLIP}-{ViP}: {Adapting} {Pre}-trained {Image}-{Text} {Model} to {Video}-{Language} {Alignment}},
	shorttitle = {{CLIP}-{ViP}},
	url = {https://openreview.net/forum?id=GNjzMAgawq},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Xue, Hongwei and Sun, Yuchong and Liu, Bei and Fu, Jianlong and Song, Ruihua and Li, Houqiang and Luo, Jiebo},
	year = {2023},
	keywords = {Image, Language, Machine Learning, Multimodal, Retrieval, Video},
}

@inproceedings{chenFineGrainedVideoTextRetrieval2020,
	address = {Seattle, WA, USA},
	title = {Fine-{Grained} {Video}-{Text} {Retrieval} {With} {Hierarchical} {Graph} {Reasoning}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9156961/},
	doi = {10.1109/CVPR42600.2020.01065},
	abstract = {Cross-modal retrieval between videos and texts has attracted growing attentions due to the rapid emergence of videos on the web. The current dominant approach is to learn a joint embedding space to measure cross-modal similarities. However, simple embeddings are insufﬁcient to represent complicated visual and textual details, such as scenes, objects, actions and their compositions. To improve ﬁne-grained video-text retrieval, we propose a Hierarchical Graph Reasoning (HGR) model, which decomposes video-text matching into global-to-local levels. The model disentangles text into a hierarchical semantic graph including three levels of events, actions, entities, and generates hierarchical textual embeddings via attention-based graph reasoning. Different levels of texts can guide the learning of diverse and hierarchical video representations for cross-modal matching to capture both global and local details. Experimental results on three video-text datasets demonstrate the advantages of our model. Such hierarchical decomposition also enables better generalization across datasets and improves the ability to distinguish ﬁne-grained semantic differences. Code will be released at https: //github.com/cshizhe/hgr\_v2t.},
	language = {en},
	urldate = {2021-03-18},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Chen, Shizhe and Zhao, Yida and Jin, Qin and Wu, Qi},
	year = {2020},
	keywords = {Language, Machine Learning, Multimodal, Retrieval, Video},
	pages = {10635--10644},
}

@inproceedings{patrick_support-set_2021,
	title = {Support-set bottlenecks for video-text representation learning},
	url = {https://openreview.net/forum?id=EqoXe2zmhrh},
	abstract = {The dominant paradigm for learning video-text representations – noise contrastive learning – increases the similarity of the representations of pairs of samples that are known to be related, such as text and video from the same sample, and pushes away the representations of all other pairs. We posit that this last behaviour is too strict, enforcing dissimilar representations even for samples that are semantically-related – for example, visually similar videos or ones that share the same depicted action. In this paper, we propose a novel method that alleviates this by leveraging a generative model to naturally push these related samples together: each sample’s caption must be reconstructed as a weighted combination of a support set of visual representations. This simple idea ensures that representations are not overly-specialized to individual samples, are reusable across the dataset, and results in representations that explicitly encode semantics shared between samples, unlike noise contrastive learning. Our proposed method outperforms others by a large margin on MSR-VTT, VATEX, ActivityNet, and MSVD for video-to-text and text-to-video retrieval.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Patrick, Mandela and Huang, Po-Yao and Asano, Yuki and Metze, Florian and Hauptmann, Alexander G. and Henriques, Joao F. and Vedaldi, Andrea},
	month = jan,
	year = {2021},
	keywords = {Language, Machine Learning, Multimodal, Retrieval, Video},
}

@inproceedings{NEURIPS2022_fc65fab8,
	title = {Text-adaptive multiple visual prototype matching for video-text retrieval},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/fc65fab891d83433bd3c8d966edde311-Paper-Conference.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Lin, Chengzhi and Wu, Ancong and Liang, Junwei and Zhang, Jun and Ge, Wenhang and Zheng, Wei-Shi and Shen, Chunhua},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	keywords = {Language, Machine Learning, Multimodal, Retrieval, Video},
	pages = {38655--38666},
}

@inproceedings{vedaldi_preserving_2020,
	address = {Cham},
	title = {Preserving {Semantic} {Neighborhoods} for {Robust} {Cross}-{Modal} {Retrieval}},
	volume = {12363},
	isbn = {978-3-030-58522-8 978-3-030-58523-5},
	url = {https://link.springer.com/10.1007/978-3-030-58523-5_19},
	abstract = {The abundance of multimodal data (e.g. social media posts) has inspired interest in cross-modal retrieval methods. Popular approaches rely on a variety of metric learning losses, which prescribe what the proximity of image and text should be, in the learned space. However, most prior methods have focused on the case where image and text convey redundant information; in contrast, real-world image-text pairs convey complementary information with little overlap. Further, images in news articles and media portray topics in a visually diverse fashion; thus, we need to take special care to ensure a meaningful image representation. We propose novel within-modality losses which encourage semantic coherency in both the text and image subspaces, which does not necessarily align with visual coherency. Our method ensures that not only are paired images and texts close, but the expected image-image and texttext relationships are also observed. Our approach improves the results of cross-modal retrieval on four datasets compared to ﬁve baselines.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Thomas, Christopher and Kovashka, Adriana},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58523-5_19},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Image, Language, Machine Learning, Multimodal, Untagged},
	pages = {317--335},
}

@inproceedings{li_svitt_2023,
	title = {{SViTT}: {Temporal} {Learning} of {Sparse} {Video}-{Text} {Transformers}},
	shorttitle = {{SViTT}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Li_SViTT_Temporal_Learning_of_Sparse_Video-Text_Transformers_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-02},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Li, Yi and Min, Kyle and Tripathi, Subarna and Vasconcelos, Nuno},
	year = {2023},
	keywords = {Untagged},
	pages = {18919--18929},
}

@inproceedings{li_hero_2020,
	address = {Online},
	title = {{HERO}: {Hierarchical} {Encoder} for {Video}+{Language} {Omni}-representation {Pre}-training},
	shorttitle = {{HERO}},
	url = {https://aclanthology.org/2020.emnlp-main.161},
	doi = {10.18653/v1/2020.emnlp-main.161},
	abstract = {We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer. In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains. We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities.},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Linjie and Chen, Yen-Chun and Cheng, Yu and Gan, Zhe and Yu, Licheng and Liu, Jingjing},
	month = nov,
	year = {2020},
	keywords = {Language, Machine Learning, Multimodal, Retrieval, Video},
	pages = {2046--2065},
}

@inproceedings{yun_time_2022,
	title = {Time {Is} {MattEr}: {Temporal} {Self}-supervision for {Video} {Transformers}},
	shorttitle = {Time {Is} {MattEr}},
	url = {https://proceedings.mlr.press/v162/yun22a.html},
	abstract = {Understanding temporal dynamics of video is an essential aspect of learning better video representations. Recently, transformer-based architectural designs have been extensively explored for video tasks due to their capability to capture long-term dependency of input sequences. However, we found that these Video Transformers are still biased to learn spatial dynamics rather than temporal ones, and debiasing the spurious correlation is critical for their performance. Based on the observations, we design simple yet effective self-supervised tasks for video models to learn temporal dynamics better. Specifically, for debiasing the spatial bias, our method learns the temporal order of video frames as extra self-supervision and enforces the randomly shuffled frames to have low-confidence outputs. Also, our method learns the temporal flow direction of video tokens among consecutive frames for enhancing the correlation toward temporal dynamics. Under various video action recognition tasks, we demonstrate the effectiveness of our method and its compatibility with state-of-the-art Video Transformers.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yun, Sukmin and Kim, Jaehyung and Han, Dongyoon and Song, Hwanjun and Ha, Jung-Woo and Shin, Jinwoo},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	keywords = {ComputerVision, Machine Learning, RepresentationLearning, Video},
	pages = {25804--25816},
}

@inproceedings{ging_coot_2020,
	title = {{COOT}: {Cooperative} {Hierarchical} {Transformer} for {Video}-{Text} {Representation} {Learning}},
	volume = {33},
	shorttitle = {{COOT}},
	url = {https://papers.nips.cc/paper_files/paper/2020/hash/ff0abbcc0227c9124a804b084d161a2d-Abstract.html},
	abstract = {Many real-world video-text tasks involve different levels of granularity, such as frames and words, clip and sentences or videos and paragraphs, each with distinct semantics. In this paper, we propose a Cooperative hierarchical Transformer (COOT) to leverage this hierarchy information and model the interactions between different levels of granularity and different modalities. The method consists of three major components: an attention-aware feature aggregation layer, which leverages the local temporal context (intra-level, e.g., within a clip), a contextual transformer to learn the interactions between low-level and high-level semantics (inter-level, e.g. clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss to connect video and text. The resulting method compares favorably to the state of the art on several benchmarks while having few parameters.},
	urldate = {2023-06-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ging, Simon and Zolfaghari, Mohammadreza and Pirsiavash, Hamed and Brox, Thomas},
	year = {2020},
	keywords = {Language, Machine Learning, Multimodal, Retrieval, Video},
	pages = {22605--22618},
}

@inproceedings{akbari_vatt_2021,
	title = {{VATT}: {Transformers} for {Multimodal} {Self}-{Supervised} {Learning} from {Raw} {Video}, {Audio} and {Text}},
	volume = {34},
	shorttitle = {{VATT}},
	url = {https://papers.nips.cc/paper_files/paper/2021/hash/cb3213ada48302953cb0f166464ab356-Abstract.html},
	abstract = {We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1\% on Kinetics-400, 83.6\% on Kinetics-600, 72.7\% on Kinetics-700, and 41.1\% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7\% top-1 accuracy on ImageNet compared to 64.7\% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4\% on AudioSet without any supervised pre-training.},
	urldate = {2023-06-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Akbari, Hassan and Yuan, Liangzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
	year = {2021},
	keywords = {Language, Machine Learning, Multimodal, Retrieval, Video},
	pages = {24206--24221},
}

@inproceedings{DBLP:conf/bmvc/ShiC21,
	title = {Efficient cross-modal retrieval via deep binary hashing and quantization},
	url = {https://www.bmvc2021-virtualconference.com/assets/papers/1202.pdf},
	booktitle = {32nd british machine vision conference 2021, {BMVC} 2021, online, november 22-25, 2021},
	publisher = {BMVA Press},
	author = {Shi, Yang and Chung, Young-joo},
	year = {2021},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/bmvc/ShiC21.bib
tex.timestamp: Wed, 22 Jun 2022 16:52:45 +0200},
	keywords = {Hashing, Image, Language, Machine Learning, Multimodal, Retrieval},
	pages = {409},
}

@inproceedings{DBLP:conf/bmvc/LuWZ0WX21,
	title = {{SwinFGHash}: {Fine}-grained image retrieval via transformer-based hashing network},
	url = {https://www.bmvc2021-virtualconference.com/assets/papers/1551.pdf},
	booktitle = {32nd british machine vision conference 2021, {BMVC} 2021, online, november 22-25, 2021},
	publisher = {BMVA Press},
	author = {Lu, Di and Wang, Jinpeng and Zeng, Ziyun and Chen, Bin and Wu, Shudeng and Xia, Shu-Tao},
	year = {2021},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/bmvc/LuWZ0WX21.bib
tex.timestamp: Wed, 22 Jun 2022 16:52:45 +0200},
	keywords = {ComputerVision, Fine-grained, Hashing, Image, Machine Learning},
	pages = {432},
}

@inproceedings{liSelfSupervisedVideoHashing2021,
	title = {Self-{Supervised} {Video} {Hashing} via {Bidirectional} {Transformers}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Li_Self-Supervised_Video_Hashing_via_Bidirectional_Transformers_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-07-02},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Li, Shuyan and Li, Xiu and Lu, Jiwen and Zhou, Jie},
	year = {2021},
	keywords = {ComputerVision, Hashing, Machine Learning, Retrieval, Video},
	pages = {13549--13558},
}

@inproceedings{zeng_motion-aware_2022,
	title = {Motion-{Aware} {Graph} {Reasoning} {Hashing} for {Self}-supervised {Video} {Retrieval}},
	abstract = {Unsupervised video hashing aims to learn a nonlinear hashing function to map videos into a similarity-preserving hamming space without label supervision. Different from static images, the motion information within videos is crucial for content understanding. However, most existing works merely extract general features from sparsely sampled frames and do not explore motion information adequately. On the other hand, directly extracting clip-wise motion features is not practical in inference because of the heavy computation overhead. In this paper, we propose Motion-Aware Graph Reasoning Hashing (MAGRH), an end-to-end framework that utilizes the motion information explicitly while keeping inference efficiency. Specifically, we design a dual-branch architecture consisting of a main branch and an auxiliary branch. During training, the main (auxiliary) branch receives frame-wise (clip-wise) inputs and produces general (motion) hash codes via delicately designed graph reasoning modules and hash layers. On top of the two branches, we develop a combination of intra- and inter-branch contrastive objectives to simultaneously learn branch-specific hashing functions as well as transfer motion knowledge from the auxiliary branch to the main branch. In inference, the hash codes are solely produced by the main branch, which only requires frame-wise inputs. Benefiting from motion guidance, our MAGRH yields superior performance on two public benchmarks, i.e., FCVID and ActivityNet, even with a small frame rate.},
	language = {en},
	booktitle = {British {Machine} {Vision} {Conference}},
	author = {Zeng, Ziyun},
	year = {2022},
	keywords = {ComputerVision, Hashing, Machine Learning, Retrieval, Video},
}

@inproceedings{gabeur_masking_2022,
	address = {Waikoloa, HI, USA},
	title = {Masking {Modalities} for {Cross}-modal {Video} {Retrieval}},
	isbn = {978-1-66540-915-5},
	url = {https://ieeexplore.ieee.org/document/9707016/},
	doi = {10.1109/WACV51458.2022.00217},
	abstract = {Pre-training on large scale unlabelled datasets has shown impressive performance improvements in the fields of computer vision and natural language processing. Given the advent of large-scale instructional video datasets, a common strategy for pre-training video encoders is to use the accompanying speech as weak supervision. However, as speech is used to supervise the pre-training, it is never seen by the video encoder, which does not learn to process that modality. We address this drawback of current pre-training methods, which fail to exploit the rich cues in spoken language. Our proposal is to pre-train a video encoder using all the available video modalities as supervision, namely, appearance, sound, and transcribed speech. We mask an entire modality in the input and predict it using the other two modalities. This encourages each modality to collaborate with the others, and our video encoder learns to process appearance and audio as well as speech. We show the superior performance of our ‘modality masking’ pre-training approach for video retrieval on the How2R, YouCook2 and Condensed Movies datasets.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {2022 {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Gabeur, Valentin and Nagrani, Arsha and Sun, Chen and Alahari, Karteek and Schmid, Cordelia},
	month = jan,
	year = {2022},
	keywords = {Language, Machine Learning, Multimodal, Retrieval, Video},
	pages = {2111--2120},
}

@inproceedings{yuan_central_2020,
	address = {Seattle, WA, USA},
	title = {Central {Similarity} {Quantization} for {Efficient} {Image} and {Video} {Retrieval}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9156349/},
	doi = {10.1109/CVPR42600.2020.00315},
	abstract = {Existing data-dependent hashing methods usually learn hash functions from pairwise or triplet data relationships, which only capture the data similarity locally, and often suffer from low learning efﬁciency and low collision rate. In this work, we propose a new global similarity metric, termed as central similarity, with which the hash codes of similar data pairs are encouraged to approach a common center and those for dissimilar pairs to converge to different centers, to improve hash learning efﬁciency and retrieval accuracy. We principally formulate the computation of the proposed central similarity metric by introducing a new concept, i.e., hash center that refers to a set of data points scattered in the Hamming space with a sufﬁcient mutual distance between each other. We then provide an efﬁcient method to construct well separated hash centers by leveraging the Hadamard matrix and Bernoulli distributions. Finally, we propose the Central Similarity Quantization (CSQ) that optimizes the central similarity between data points w.r.t. their hash centers instead of optimizing the local similarity. CSQ is generic and applicable to both image and video hashing scenarios. Extensive experiments on large-scale image and video retrieval tasks demonstrate that CSQ can generate cohesive hash codes for similar data pairs and dispersed hash codes for dissimilar pairs, achieving a noticeable boost in retrieval performance, i.e. 3\%20\% in mAP over the previous state-of-the-arts 1 .},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yuan, Li and Wang, Tao and Zhang, Xiaopeng and Tay, Francis Eh and Jie, Zequn and Liu, Wei and Feng, Jiashi},
	month = jun,
	year = {2020},
	keywords = {Hashing, Image, Language, Machine Learning, Multimodal, Video},
	pages = {3080--3089},
}

@inproceedings{avidan_dual-stream_2022,
	address = {Cham},
	title = {Dual-{Stream} {Knowledge}-{Preserving} {Hashing} for {Unsupervised} {Video} {Retrieval}},
	volume = {13674},
	isbn = {978-3-031-19780-2 978-3-031-19781-9},
	url = {https://link.springer.com/10.1007/978-3-031-19781-9_11},
	abstract = {Unsupervised video hashing usually optimizes binary codes by learning to reconstruct input videos. Such reconstruction constraint spends much effort on frame-level temporal context changes without focusing on video-level global semantics that are more useful for retrieval. Hence, we address this problem by decomposing video information into reconstruction-dependent and semantic-dependent information, which disentangles the semantic extraction from reconstruction constraint. Specifically, we first design a simple dual-stream structure, including a temporal layer and a hash layer. Then, with the help of semantic similarity knowledge obtained from self-supervision, the hash layer learns to capture information for semantic retrieval, while the temporal layer learns to capture the information for reconstruction. In this way, the model naturally preserves the disentangled semantics into binary codes. Validated by comprehensive experiments, our method consistently outperforms the state-of-the-arts on three video benchmarks.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Li, Pandeng and Xie, Hongtao and Ge, Jiannan and Zhang, Lei and Min, Shaobo and Zhang, Yongdong},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-19781-9_11},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Machine Learning, Retrieval, Video},
	pages = {181--197},
}

@inproceedings{avidan_multi-query_2022,
	address = {Cham},
	title = {Multi-query {Video} {Retrieval}},
	volume = {13674},
	isbn = {978-3-031-19780-2 978-3-031-19781-9},
	url = {https://link.springer.com/10.1007/978-3-031-19781-9_14},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Wang, Zeyu and Wu, Yu and Narasimhan, Karthik and Russakovsky, Olga},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-19781-9_14},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Language, Machine Learning, Multimodal, Retrieval, Video},
	pages = {233--249},
}

@inproceedings{avidan_deep_2022,
	address = {Cham},
	title = {Deep {Hash} {Distillation} for {Image} {Retrieval}},
	volume = {13674},
	isbn = {978-3-031-19780-2 978-3-031-19781-9},
	url = {https://link.springer.com/10.1007/978-3-031-19781-9_21},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Jang, Young Kyun and Gu, Geonmo and Ko, Byungsoo and Kang, Isaac and Cho, Nam Ik},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-19781-9_21},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {ComputerVision, Hashing, Image, Machine Learning, Retrieval},
	pages = {354--371},
}

@inproceedings{avidan_lightweight_2022,
	address = {Cham},
	title = {Lightweight {Attentional} {Feature} {Fusion}: {A} {New} {Baseline} for {Text}-to-{Video} {Retrieval}},
	volume = {13674},
	isbn = {978-3-031-19780-2 978-3-031-19781-9},
	shorttitle = {Lightweight {Attentional} {Feature} {Fusion}},
	url = {https://link.springer.com/10.1007/978-3-031-19781-9_26},
	abstract = {In this paper we revisit feature fusion, an old-fashioned topic, in the new context of text-to-video retrieval. Different from previous research that considers feature fusion only at one end, let it be video or text, we aim for feature fusion for both ends within a unified framework. We hypothesize that optimizing the convex combination of the features is preferred to modeling their correlations by computationally heavy multihead self attention. We propose Lightweight Attentional Feature Fusion (LAFF). LAFF performs feature fusion at both early and late stages and at both video and text ends, making it a powerful method for exploiting diverse (off-the-shelf) features. The interpretability of LAFF can be used for feature selection. Extensive experiments on five public benchmark sets (MSR-VTT, MSVD, TGIF, VATEX and TRECVID AVS 2016-2020) justify LAFF as a new baseline for text-to-video retrieval.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Hu, Fan and Chen, Aozhu and Wang, Ziyue and Zhou, Fangming and Dong, Jianfeng and Li, Xirong},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-19781-9_26},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Language, Machine Learning, Multimodal, Retrieval, Video},
	pages = {444--461},
}

@inproceedings{huang_tada_2022,
	title = {{TAda}! {Temporally}-{Adaptive} {Convolutions} for {Video} {Understanding}},
	url = {https://openreview.net/forum?id=izj68lUcBpt},
	abstract = {Spatial convolutions are widely used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding, which shows that adaptive weight calibration along the temporal dimension is an efficient way to facilitate modelling complex temporal dynamics in videos. Specifically, TAdaConv empowers the spatial convolutions with temporal modelling abilities by calibrating the convolution weights for each frame according to its local and global temporal context. Compared to previous temporal modelling operations, TAdaConv is more efficient as it operates over the convolution kernels instead of the features, whose dimension is an order of magnitude smaller than the spatial resolutions. Further, the kernel calibration brings an increased model capacity. We construct TAda2D and TAdaConvNeXt networks by replacing the 2D convolutions in ResNet and ConvNeXt with TAdaConv, which leads to at least on par or better performance compared to state-of-the-art approaches on multiple video action recognition and localization benchmarks. We also demonstrate that as a readily plug-in operation with negligible computation overhead, TAdaConv can effectively improve many existing video models with a convincing margin.},
	language = {en},
	urldate = {2023-06-06},
	author = {Huang, Ziyuan and Zhang, Shiwei and Pan, Liang and Qing, Zhiwu and Tang, Mingqian and Liu, Ziwei and Jr, Marcelo H. Ang},
	month = jan,
	year = {2022},
	keywords = {Classification, ComputerVision, Machine Learning, Video},
}

@inproceedings{chen_spatial-temporal_2021,
	title = {Spatial-temporal {Causal} {Inference} for {Partial} {Image}-to-video {Adaptation}},
	volume = {35},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16187},
	doi = {10.1609/aaai.v35i2.16187},
	abstract = {Image-to-video adaptation leverages off-the-shelf learned models in labeled images to help classiﬁcation in unlabeled videos, thus alleviating the high computation overhead of training a video classiﬁer from scratch. This task is very challenging since there exist two types of domain shifts between images and videos: 1) spatial domain shift caused by static appearance variance between images and video frames, and 2) temporal domain shift caused by the absence of dynamic motion in images. Moreover, for different video classes, these two domain shifts have different effects on the domain gap and should not be treated equally during adaptation. In this paper, we propose a spatial-temporal causal inference framework for image-to-video adaptation. We ﬁrst construct a spatial-temporal causal graph to infer the effects of the spatial and temporal domain shifts by performing counterfactual causality. We then learn causality-guided bidirectional heterogeneous mappings between images and videos to adaptively reduce the two domain shifts. Moreover, to relax the assumption that the label spaces of the image and video domains are the same by the existing methods, we incorporate class-wise alignment into the learning of image-video mappings to perform partial image-to-video adaptation where the image label space subsumes the video label space. Extensive experiments on several video datasets have validated the effectiveness of our proposed method.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Chen, Jin and Wu, Xinxiao and Hu, Yao and Luo, Jiebo},
	month = may,
	year = {2021},
	keywords = {ComputerVision, Image, Machine Learning, Video},
	pages = {1027--1035},
}

@article{chen_mind--gap_2021,
	title = {Mind-the-{Gap}! {Unsupervised} {Domain} {Adaptation} for {Text}-{Video} {Retrieval}},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16192},
	doi = {10.1609/aaai.v35i2.16192},
	abstract = {When can we expect a text-video retrieval system to work effectively on datasets that differ from its training domain? In this work, we investigate this question through the lens of unsupervised domain adaptation in which the objective is to match natural language queries and video content in the presence of domain shift at query-time. Such systems have signiﬁcant practical applications since they are capable generalising to new data sources without requiring corresponding text annotations. We make the following contributions: (1) We propose the UDAVR (Unsupervised Domain Adaptation for Video Retrieval) benchmark and employ it to study the performance of text-video retrieval in the presence of domain shift. (2) We propose Concept-Aware-Pseudo-Query (CAPQ), a method for learning discriminative and transferable features that bridge these cross-domain discrepancies to enable effective target domain retrieval using source domain supervision. (3) We show that CAPQ outperforms alternative domain adaptation strategies on UDAVR.},
	language = {en},
	number = {2},
	urldate = {2023-06-06},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Qingchao and Liu, Yang and Albanie, Samuel},
	month = may,
	year = {2021},
	keywords = {DomainAdaption, Machine Learning, Multimodal, Retrieval, Video-Text},
	pages = {1072--1080},
}

@inproceedings{qian_dual_2021,
	title = {Dual {Adversarial} {Graph} {Neural} {Networks} for {Multi}-label {Cross}-modal {Retrieval}},
	volume = {35},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16345},
	doi = {10.1609/aaai.v35i3.16345},
	abstract = {Cross-modal retrieval has become an active study ﬁeld with the expanding scale of multimodal data. To date, most existing methods transform multimodal data into a common representation space where semantic similarities between items can be directly measured across different modalities. However, these methods typically suffer from following limitations: 1) They usually attempt to bridge the modality gap by designing losses in the common representation space which may not be sufﬁcient to eliminate potential heterogeneity of different modalities in the common space. 2) They typically treat labels as independent individuals and ignore label relationships which are important for constructing semantic links between multimodal data. In this work, we propose a novel Dual Adversarial Graph Neural Networks (DAGNN) composed of the dual generative adversarial networks and the multi-hop graph neural networks, which learn modality-invariant and discriminative common representations for cross-modal retrieval. Firstly, we construct the dual generative adversarial networks to project multimodal data into a common representation space. Secondly, we leverage the multi-hop graph neural networks, in which a layer aggregation mechanism is proposed to exploit multi-hop propagation information, to capture the label correlation dependency and learn inter-dependent classiﬁers. Comprehensive experiments conducted on two cross-modal retrieval benchmark datasets, NUS-WIDE and MIRFlickr, indicate the superiority of DAGNN.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Qian, Shengsheng and Xue, Dizhan and Zhang, Huaiwen and Fang, Quan and Xu, Changsheng},
	month = may,
	year = {2021},
	keywords = {Graph, Machine Learning, Multimodal, Retrieval},
	pages = {2440--2448},
}

@inproceedings{yu_deep_2021,
	title = {Deep {Graph}-neighbor {Coherence} {Preserving} {Network} for {Unsupervised} {Cross}-modal {Hashing}},
	volume = {35},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16592},
	doi = {10.1609/aaai.v35i5.16592},
	abstract = {Unsupervised cross-modal hashing (UCMH) has become a hot topic recently. Current UCMH focuses on exploring data similarities. However, current UCMH methods calculate the similarity between two data, mainly relying on the two data’s cross-modal features. These methods suffer from inaccurate similarity problems that result in a suboptimal retrieval Hamming space, because the cross-modal features between the data are not sufﬁcient to describe the complex data relationships, such as situations where two data have different feature representations but share the inherent concepts. In this paper, we devise a deep graph-neighbor coherence preserving network (DGCPN). Speciﬁcally, DGCPN stems from graph models and explores graph-neighbor coherence by consolidating the information between data and their neighbors. DGCPN regulates comprehensive similarity preserving losses by exploiting three types of data similarities (i.e., the graph-neighbor coherence, the coexistent similarity, and the intra- and inter-modality consistency) and designs a half-real and half-binary optimization strategy to reduce the quantization errors during hashing. Essentially, DGCPN addresses the inaccurate similarity problem by exploring and exploiting the data’s intrinsic relationships in a graph. We conduct extensive experiments on three public UCMH datasets. The experimental results demonstrate the superiority of DGCPN, e.g., by improving the mean average precision from 0.722 to 0.751 on MIRFlickr-25K using 64-bit hashing codes to retrieve texts from images. We will release the source code package and the trained model on https://github.com/Atmegal/DGCPN.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Yu, Jun and Zhou, Hao and Zhan, Yibing and Tao, Dacheng},
	month = may,
	year = {2021},
	keywords = {Graph, Hashing, Machine Learning, Multimodal},
	pages = {4626--4634},
}

@inproceedings{mao_deep_2021,
	title = {Deep {Mutual} {Information} {Maximin} for {Cross}-{Modal} {Clustering}},
	volume = {35},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17076},
	doi = {10.1609/aaai.v35i10.17076},
	abstract = {Cross-modal clustering (CMC) aims to enhance the clustering performance by exploring complementary information from multiple modalities. However, the performances of existing CMC algorithms are still unsatisfactory due to the conﬂict of heterogeneous modalities and the high-dimensional non-linear property of individual modality. In this paper, a novel deep mutual information maximin (DMIM) method for cross-modal clustering is proposed to maximally preserve the shared information of multiple modalities while eliminating the superﬂuous information of individual modalities in an end-to-end manner. Speciﬁcally, a multi-modal shared encoder is ﬁrstly built to align the latent feature distributions by sharing parameters across modalities. Then, DMIM formulates the complementarity of multi-modalities representations as a mutual information maximin objective function, in which the shared information of multiple modalities and the superﬂuous information of individual modalities are identiﬁed by mutual information maximization and minimization respectively. To solve the DMIM objective function, we propose a variational optimization method to ensure it converge to a local optimal solution. Moreover, an auxiliary overclustering mechanism is employed to optimize the clustering structure by introducing more detailed clustering classes. Extensive experimental results demonstrate the superiority of DMIM method over the state-of-the-art cross-modal clustering methods on IAPR-TC12, ESP-Game, MIRFlickr and NUSWide datasets.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Mao, Yiqiao and Yan, Xiaoqiang and Guo, Qiang and Ye, Yangdong},
	month = may,
	year = {2021},
	keywords = {Machine Learning, Multimodal, MutualInfo},
	pages = {8893--8901},
}

@inproceedings{wang_enhancing_2021,
	title = {Enhancing {Unsupervised} {Video} {Representation} {Learning} by {Decoupling} the {Scene} and the {Motion}},
	volume = {35},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17215},
	doi = {10.1609/aaai.v35i11.17215},
	abstract = {One signiﬁcant factor we expect the video representation learning to capture, especially in contrast with the image representation learning, is the object motion. However, we found that in the current mainstream video datasets, some action categories are highly related with the scene where the action happens, making the model tend to degrade to a solution where only the scene information is encoded. For example, a trained model may predict a video as playing football simply because it sees the ﬁeld, neglecting that the subject is dancing as a cheerleader on the ﬁeld. This is against our original intention towards the video representation learning and may bring scene bias on a different dataset that can not be ignored. In order to tackle this problem, we propose to decouple the scene and the motion (DSM) with two simple operations, so that the model attention towards the motion information is better paid. Speciﬁcally, we construct a positive clip and a negative clip for each video. Compared to the original video, the positive/negative is motion-untouched/broken but scenebroken/untouched by Spatial Local Disturbance and Temporal Local Disturbance. Our objective is to pull the positive closer while pushing the negative farther to the original clip in the latent space. In this way, the impact of the scene is weakened while the temporal sensitivity of the network is further enhanced. We conduct experiments on two tasks with various backbones and different pre-training datasets, and ﬁnd that our method surpass the SOTA methods with a remarkable 8.1\% and 8.8\% improvement towards action recognition task on the UCF101 and HMDB51 datasets respectively using the same backbone.},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Wang, Jinpeng and Gao, Yuting and Li, Ke and Hu, Jianguo and Jiang, Xinyang and Guo, Xiaowei and Ji, Rongrong and Sun, Xing},
	month = may,
	year = {2021},
	keywords = {Classification, ComputerVision, Machine Learning, Video},
	pages = {10129--10137},
}

@inproceedings{yu_ghan_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{GHAN}: {Graph}-{Based} {Hierarchical} {Aggregation} {Network} for {Text}-{Video} {Retrieval}},
	shorttitle = {{GHAN}},
	url = {https://aclanthology.org/2022.emnlp-main.374},
	abstract = {Text-video retrieval focuses on two aspects: cross-modality interaction and video-language encoding. Currently, the mainstream approach is to train a joint embedding space for multimodal interactions. However, there are structural and semantic differences between text and video, making this approach challenging for fine-grained understanding. In order to solve this, we propose an end-to-end graph-based hierarchical aggregation network for text-video retrieval according to the hierarchy possessed by text and video. We design a token-level weighted network to refine intra-modality representations and construct a graph-based message passing attention network for global-local alignment across modality. We conduct experiments on the public datasets MSR-VTT-9K, MSR-VTT-7K and MSVD, and achieve Recall@1 of 73.0\%, 65.6\%, and 64.0\% , which is 25.7\%, 16.5\%, and 14.2\% better than the current state-of-the-art model.},
	urldate = {2023-05-10},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Yu, Yahan and Hu, Bojie and Li, Yu},
	month = dec,
	year = {2022},
	keywords = {Untagged},
	pages = {5547--5557},
}

@inproceedings{wu_rap_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{RaP}: {Redundancy}-aware {Video}-language {Pre}-training for {Text}-{Video} {Retrieval}},
	shorttitle = {{RaP}},
	url = {https://aclanthology.org/2022.findings-emnlp.221},
	abstract = {Video language pre-training methods have mainly adopted sparse sampling techniques to alleviate the temporal redundancy of videos. Though effective, sparse sampling still suffers inter-modal redundancy: visual redundancy and textual redundancy. Compared with highly generalized text, sparsely sampled frames usually contain text-independent portions, called visual redundancy. Sparse sampling is also likely to miss important frames corresponding to some text portions, resulting in textual redundancy. Inter-modal redundancy leads to a mismatch of video and text information, hindering the model from better learning the shared semantics across modalities. To alleviate it, we propose Redundancy-aware Video-language Pre-training. We design a redundancy measurement of video patches and text tokens by calculating the cross-modal minimum dis-similarity. Then, we penalize the high-redundant video patches and text tokens through a proposed redundancy-aware contrastive learning. We evaluate our method on four benchmark datasets, MSRVTT, MSVD, DiDeMo, and LSMDC, achieving a significant improvement over the previous state-of-the-art results.},
	urldate = {2023-05-10},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Xing and Gao, Chaochen and Lin, Zijia and Wang, Zhongyuan and Han, Jizhong and Hu, Songlin},
	month = dec,
	year = {2022},
	keywords = {Untagged},
	pages = {3036--3047},
}

@inproceedings{li_unimo_2021,
	address = {Online},
	title = {{UNIMO}: {Towards} {Unified}-{Modal} {Understanding} and {Generation} via {Cross}-{Modal} {Contrastive} {Learning}},
	shorttitle = {{UNIMO}},
	url = {https://aclanthology.org/2021.acl-long.202},
	doi = {10.18653/v1/2021.acl-long.202},
	abstract = {Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data (i.e., text or image) or limited multi-modal data (i.e., image-text pairs). In this work, we propose a UNIfied-MOdal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections are utilized to improve the capability of visual and textual understanding, and cross-modal contrastive learning (CMCL) is leveraged to align the textual and visual information into a unified semantic space, over a corpus of image-text pairs augmented with related images and texts. With the help of rich non-paired single-modal data, our model is able to learn more generalizable representations, by allowing textual knowledge and visual knowledge to enhance each other in the unified semantic space. The experimental results show that UNIMO greatly improves the performance of several single-modal and multi-modal downstream tasks. Our code and pre-trained models are public at https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO.},
	urldate = {2022-10-05},
	booktitle = {Annual {Meeting} of the {Association} for {Computational} {Linguistics} and {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Wei and Gao, Can and Niu, Guocheng and Xiao, Xinyan and Liu, Hao and Liu, Jiachen and Wu, Hua and Wang, Haifeng},
	year = {2021},
	keywords = {Untagged},
	pages = {2592--2607},
}

@inproceedings{alikhani_cross-modal_2022,
	title = {Cross-{Modal} {Coherence} for {Text}-to-{Image} {Retrieval}},
	volume = {36},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21285},
	doi = {10.1609/aaai.v36i10.21285},
	abstract = {Common image-text joint understanding techniques presume that images and the associated text can universally be characterized by a single implicit model. However, co-occurring images and text can be related in qualitatively different ways, and explicitly modeling it could improve the performance of current joint understanding models. In this paper, we train a Cross-Modal Coherence Model for text-to-image retrieval task. Our analysis shows that models trained with image–text coherence relations can retrieve images originally paired with target text more often than coherence-agnostic models. We also show via human evaluation that images retrieved by the proposed coherence-aware model are preferred over a coherenceagnostic baseline by a huge margin. Our findings provide insights into the ways that different modalities communicate and the role of coherence relations in capturing commonsense inferences in text and imagery.},
	language = {en},
	urldate = {2023-06-05},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Alikhani, Malihe and Han, Fangda and Ravi, Hareesh and Kapadia, Mubbasir and Pavlovic, Vladimir and Stone, Matthew},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {10427--10435},
}

@inproceedings{li_bi-cmr_2022,
	title = {Bi-{CMR}: {Bidirectional} {Reinforcement} {Guided} {Hashing} for {Effective} {Cross}-{Modal} {Retrieval}},
	volume = {36},
	shorttitle = {Bi-{CMR}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21268},
	doi = {10.1609/aaai.v36i9.21268},
	abstract = {Cross-modal hashing has attracted considerable attention for large-scale multimodal data. Recent supervised cross-modal hashing methods using multi-label networks utilize the semantics of multi-labels to enhance retrieval accuracy, where label hash codes are learned independently. However, all these methods assume that label annotations reliably reﬂect the relevance between their corresponding instances, which is not true in real applications. In this paper, we propose a novel framework called Bidirectional Reinforcement Guided Hashing for Effective Cross-Modal Retrieval (Bi-CMR), which exploits a bidirectional learning to relieve the negative impact of this assumption. Speciﬁcally, in the forward learning procedure, we highlight the representative labels and learn the reinforced multi-label hash codes by intra-modal semantic information, and further adjust similarity matrix. In the backward learning procedure, the reinforced multi-label hash codes and adjusted similarity matrix are used to guide the matching of instances. We construct two datasets with explicit relevance labels that reﬂect the semantic relevance of instance pairs based on two benchmark datasets. The Bi-CMR is evaluated by conducting extensive experiments over these two datasets. Experimental results prove the superiority of Bi-CMR over four state-of-the-art methods in terms of effectiveness.},
	language = {en},
	urldate = {2023-06-05},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Li, Tieying and Yang, Xiaochun and Wang, Bin and Xi, Chong and Zheng, Hanzhong and Zhou, Xiangmin},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {10275--10282},
}

@inproceedings{xu_videoclip_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {{VideoCLIP}: {Contrastive} {Pre}-training for {Zero}-shot {Video}-{Text} {Understanding}},
	shorttitle = {{VideoCLIP}},
	url = {https://aclanthology.org/2021.emnlp-main.544},
	doi = {10.18653/v1/2021.emnlp-main.544},
	abstract = {We present VideoCLIP, a contrastive approach to pre-train a unified model for zero-shot video and text understanding, without using any labels on downstream tasks. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives from nearest neighbor retrieval. Our experiments on a diverse series of downstream tasks, including sequence-level text-video retrieval, VideoQA, token-level action localization, and action segmentation reveal state-of-the-art performance, surpassing prior work, and in some cases even outperforming supervised approaches. Code is made available at https://github.com/pytorch/fairseq/examples/MMPT.},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Hu and Ghosh, Gargi and Huang, Po-Yao and Okhonko, Dmytro and Aghajanyan, Armen and Metze, Florian and Zettlemoyer, Luke and Feichtenhofer, Christoph},
	month = nov,
	year = {2021},
	keywords = {Untagged},
	pages = {6787--6800},
}

@inproceedings{xu_vlm_2021,
	address = {Online},
	title = {{VLM}: {Task}-agnostic {Video}-{Language} {Model} {Pre}-training for {Video} {Understanding}},
	shorttitle = {{VLM}},
	url = {https://aclanthology.org/2021.findings-acl.370},
	doi = {10.18653/v1/2021.findings-acl.370},
	urldate = {2023-06-06},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Hu and Ghosh, Gargi and Huang, Po-Yao and Arora, Prahal and Aminzadeh, Masoumeh and Feichtenhofer, Christoph and Metze, Florian and Zettlemoyer, Luke},
	month = aug,
	year = {2021},
	keywords = {Untagged},
	pages = {4227--4239},
}

@inproceedings{ren_learning_2021,
	address = {Online},
	title = {Learning {Relation} {Alignment} for {Calibrated} {Cross}-modal {Retrieval}},
	url = {https://aclanthology.org/2021.acl-long.43},
	doi = {10.18653/v1/2021.acl-long.43},
	abstract = {Despite the achievements of large-scale multimodal pre-training approaches, cross-modal retrieval, e.g., image-text retrieval, remains a challenging task. To bridge the semantic gap between the two modalities, previous studies mainly focus on word-region alignment at the object level, lacking the matching between the linguistic relation among the words and the visual relation among the regions. The neglect of such relation consistency impairs the contextualized representation of image-text pairs and hinders the model performance and the interpretability. In this paper, we first propose a novel metric, Intra-modal Self-attention Distance (ISD), to quantify the relation consistency by measuring the semantic distance between linguistic and visual relations. In response, we present Inter-modal Alignment on Intra-modal Self-attentions (IAIS), a regularized training method to optimize the ISD and calibrate intra-modal self-attentions from the two modalities mutually via inter-modal alignment. The IAIS regularizer boosts the performance of prevailing models on Flickr30k and MS COCO datasets by a considerable margin, which demonstrates the superiority of our approach.},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ren, Shuhuai and Lin, Junyang and Zhao, Guangxiang and Men, Rui and Yang, An and Zhou, Jingren and Sun, Xu and Yang, Hongxia},
	month = aug,
	year = {2021},
	keywords = {Untagged},
	pages = {514--524},
}

@inproceedings{xu_e2e-vlp_2021,
	address = {Online},
	title = {{E2E}-{VLP}: {End}-to-{End} {Vision}-{Language} {Pre}-training {Enhanced} by {Visual} {Learning}},
	shorttitle = {{E2E}-{VLP}},
	url = {https://aclanthology.org/2021.acl-long.42},
	doi = {10.18653/v1/2021.acl-long.42},
	abstract = {Vision-language pre-training (VLP) on large-scale image-text pairs has achieved huge success for the cross-modal downstream tasks. The most existing pre-training methods mainly adopt a two-step training procedure, which firstly employs a pre-trained object detector to extract region-based visual features, then concatenates the image representation and text embedding as the input of Transformer to train. However, these methods face problems of using task-specific visual representation of the specific object detector for generic cross-modal understanding, and the computation inefficiency of two-stage pipeline. In this paper, we propose the first end-to-end vision-language pre-trained model for both V+L understanding and generation, namely E2E-VLP, where we build a unified Transformer framework to jointly learn visual representation, and semantic alignments between image and text. We incorporate the tasks of object detection and image captioning into pre-training with a unified Transformer encoder-decoder architecture for enhancing visual learning. An extensive set of experiments have been conducted on well-established vision-language downstream tasks to demonstrate the effectiveness of this novel VLP paradigm.},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Haiyang and Yan, Ming and Li, Chenliang and Bi, Bin and Huang, Songfang and Xiao, Wenming and Huang, Fei},
	month = aug,
	year = {2021},
	keywords = {Untagged},
	pages = {503--513},
}

@misc{lu_open-vocabulary_2022,
	title = {Open-{Vocabulary} {3D} {Detection} via {Image}-level {Class} and {Debiased} {Cross}-modal {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2207.01987},
	doi = {10.48550/arXiv.2207.01987},
	abstract = {Current point-cloud detection methods have difficulty detecting the open-vocabulary objects in the real world, due to their limited generalization capability. Moreover, it is extremely laborious and expensive to collect and fully annotate a point-cloud detection dataset with numerous classes of objects, leading to the limited classes of existing point-cloud datasets and hindering the model to learn general representations to achieve open-vocabulary point-cloud detection. As far as we know, we are the first to study the problem of open-vocabulary 3D point-cloud detection. Instead of seeking a point-cloud dataset with full labels, we resort to ImageNet1K to broaden the vocabulary of the point-cloud detector. We propose OV-3DETIC, an Open-Vocabulary 3D DETector using Image-level Class supervision. Specifically, we take advantage of two modalities, the image modality for recognition and the point-cloud modality for localization, to generate pseudo labels for unseen classes. Then we propose a novel debiased cross-modal contrastive learning method to transfer the knowledge from image modality to point-cloud modality during training. Without hurting the latency during inference, OV-3DETIC makes the point-cloud detector capable of achieving open-vocabulary detection. Extensive experiments demonstrate that the proposed OV-3DETIC achieves at least 10.77 \% mAP improvement (absolute value) and 9.56 \% mAP improvement (absolute value) by a wide range of baselines on the SUN-RGBD dataset and ScanNet dataset, respectively. Besides, we conduct sufficient experiments to shed light on why the proposed OV-3DETIC works.},
	urldate = {2023-06-03},
	publisher = {arXiv},
	author = {Lu, Yuheng and Xu, Chenfeng and Wei, Xiaobao and Xie, Xiaodong and Tomizuka, Masayoshi and Keutzer, Kurt and Zhang, Shanghang},
	month = jul,
	year = {2022},
	note = {arXiv:2207.01987 [cs]},
	keywords = {3D, Detection, Machine Learning, Multimodal, Open-vocabulary, Point Clouds},
}

@inproceedings{lu_open-vocabulary_2023,
	title = {Open-{Vocabulary} {Point}-{Cloud} {Object} {Detection} {Without} {3D} {Annotation}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-01},
	author = {Lu, Yuheng and Xu, Chenfeng and Wei, Xiaobao and Xie, Xiaodong and Tomizuka, Masayoshi and Keutzer, Kurt and Zhang, Shanghang},
	year = {2023},
	keywords = {3D, Detection, Machine Learning, Multimodal, Open-vocabulary, Point Clouds},
	pages = {1190--1199},
}

@inproceedings{DBLP:conf/kdd/YiCLSY13,
	title = {Predictive model performance: offline and online evaluations},
	url = {https://doi.org/10.1145/2487575.2488215},
	doi = {10/ggf5c5},
	booktitle = {The 19th {ACM} {SIGKDD} international conference on knowledge discovery and data mining, {KDD} 2013, chicago, {IL}, {USA}, august 11-14, 2013},
	publisher = {ACM},
	author = {Yi, Jeonghee and Chen, Ye and Li, Jie and Sett, Swaraj and Yan, Tak W.},
	editor = {Dhillon, Inderjit S. and Koren, Yehuda and Ghani, Rayid and Senator, Ted E. and Bradley, Paul and Parekh, Rajesh and He, Jingrui and Grossman, Robert L. and Uthurusamy, Ramasamy},
	year = {2013},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/kdd/YiCLSY13.bib
tex.timestamp: Tue, 06 Nov 2018 00:00:00 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {1294--1302},
}

@inproceedings{DBLP:conf/recsys/BeelGLNG13,
	address = {Hong Kong, China},
	title = {A comparative analysis of offline and online evaluations and discussion of research paper recommender system evaluation},
	url = {https://doi.org/10.1145/2532508.2532511},
	doi = {10/ggcj6p},
	booktitle = {International workshop on reproducibility and replication in recommender systems evaluation},
	publisher = {ACM},
	author = {Beel, Jöran and Genzmehr, Marcel and Langer, Stefan and Nürnberger, Andreas and Gipp, Bela},
	editor = {Bellogín, Alejandro and Castells, Pablo and Said, Alan and Tikk, Domonkos},
	year = {2013},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/recsys/BeelGLNG13.bib
tex.timestamp: Wed, 25 Sep 2019 01:00:00 +0200},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {7--14},
}

@inproceedings{DBLP:conf/pepm/Hatcliff98,
	series = {Lecture notes in computer science},
	title = {An introduction to online and offline partial evaluation using a simple flowchart language},
	volume = {1706},
	url = {https://doi.org/10.1007/3-540-47018-2_2},
	doi = {10.1007/3-540-47018-2\_2},
	booktitle = {Partial evaluation - practice and theory, {DIKU} 1998 international summer school, copenhagen, denmark, june 29 - july 10, 1998},
	publisher = {Springer},
	author = {Hatcliff, John},
	editor = {Hatcliff, John and Mogensen, Torben Æ. and Thiemann, Peter},
	year = {1998},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/pepm/Hatcliff98.bib
tex.timestamp: Mon, 23 Mar 2020 12:22:51 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {20--82},
}

@inproceedings{DBLP:conf/pepm/SumiiK00,
	title = {Online-and-offline partial evaluation: {A} mixed approach (extended abstract)},
	url = {https://doi.org/10.1145/328690.328694},
	doi = {10/bjv5sc},
	booktitle = {Proceedings of the 2000 {ACM} {SIGPLAN} workshop on partial evaluation and semantics-based program manipulation ({PEPM} '00), boston, massachusetts, {USA}, january 22-23, 2000},
	publisher = {ACM},
	author = {Sumii, Eijiro and Kobayashi, Naoki},
	editor = {Lawall, Julia L.},
	year = {2000},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/pepm/SumiiK00.bib
tex.timestamp: Thu, 08 Jul 2021 16:04:02 +0200},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {12--21},
}

@article{DBLP:journals/lisp/SumiiK01,
	title = {A hybrid approach to online and offline partial evaluation},
	volume = {14},
	url = {https://doi.org/10.1023/A:1012984529382},
	doi = {10/fnmrkq},
	number = {2-3},
	journal = {High. Order Symb. Comput.},
	author = {Sumii, Eijiro and Kobayashi, Naoki},
	year = {2001},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/lisp/SumiiK01.bib
tex.timestamp: Thu, 05 Mar 2020 00:00:00 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {101--142},
}

@inproceedings{DBLP:conf/lopstr/LeuschelCE06,
	series = {Lecture notes in computer science},
	title = {Supervising offline partial evaluation of logic programs using online techniques},
	volume = {4407},
	url = {https://doi.org/10.1007/978-3-540-71410-1_5},
	doi = {10.1007/978-3-540-71410-1\_5},
	booktitle = {Logic-based program synthesis and transformation, 16th international symposium, {LOPSTR} 2006, venice, italy, july 12-14, 2006, revised selected papers},
	publisher = {Springer},
	author = {Leuschel, Michael and Craig, Stephen-John and Elphick, Daniel},
	editor = {Puebla, Germán},
	year = {2006},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/lopstr/LeuschelCE06.bib
tex.timestamp: Tue, 14 May 2019 10:00:41 +0200},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {43--59},
}

@inproceedings{DBLP:conf/recsys/MoreiraSC15,
	address = {Vienna, Austria},
	series = {{CEUR} workshop proceedings},
	title = {Comparing offline and online recommender system evaluations on long-tail distributions},
	volume = {1441},
	url = {http://ceur-ws.org/Vol-1441/recsys2015_poster4.pdf},
	booktitle = {{ACM} {Conference} on {Recommender} {Systems}},
	publisher = {CEUR-WS.org},
	author = {de Souza Pereira Moreira, Gabriel and de Souza, Gilmar Alves and da Cunha, Adilson Marques},
	editor = {Castells, Pablo},
	year = {2015},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/recsys/MoreiraSC15.bib
tex.timestamp: Wed, 12 Feb 2020 16:45:18 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
}

@inproceedings{DBLP:conf/sigir/Kharitonov14,
	title = {Improving offline and online web search evaluation by modelling the user behaviour},
	url = {https://doi.org/10.1145/2600428.2610379},
	doi = {10/gmxkv7},
	booktitle = {The 37th international {ACM} {SIGIR} conference on research and development in information retrieval, {SIGIR} '14, gold coast , {QLD}, australia - july 06 - 11, 2014},
	publisher = {ACM},
	author = {Kharitonov, Eugene},
	editor = {Geva, Shlomo and Trotman, Andrew and Bruza, Peter and Clarke, Charles L. A. and Järvelin, Kalervo},
	year = {2014},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/sigir/Kharitonov14.bib
tex.timestamp: Tue, 06 Nov 2018 11:07:24 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {1278},
}

@inproceedings{DBLP:conf/ercimdl/BeelL15,
	series = {Lecture notes in computer science},
	title = {A comparison of offline evaluations, online evaluations, and user studies in the context of research-paper recommender systems},
	volume = {9316},
	url = {https://doi.org/10.1007/978-3-319-24592-8_12},
	doi = {10.1007/978-3-319-24592-8\_12},
	booktitle = {Research and advanced technology for digital libraries - 19th international conference on theory and practice of digital libraries, {TPDL} 2015, poznań, poland, september 14-18, 2015. {Proceedings}},
	publisher = {Springer},
	author = {Beel, Jöran and Langer, Stefan},
	editor = {Kapidakis, Sarantos and Mazurek, Cezary and Werla, Marcin},
	year = {2015},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/ercimdl/BeelL15.bib
tex.timestamp: Tue, 14 May 2019 10:00:40 +0200},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {153--168},
}

@inproceedings{DBLP:conf/clef/KilleLTSLBSH15,
	series = {Lecture notes in computer science},
	title = {Stream-based recommendations: {Online} and offline evaluation as a service},
	volume = {9283},
	url = {https://doi.org/10.1007/978-3-319-24027-5_48},
	doi = {10.1007/978-3-319-24027-5\_48},
	booktitle = {Experimental {IR} meets multilinguality, multimodality, and interaction - 6th international conference of the {CLEF} association, {CLEF} 2015, toulouse, france, september 8-11, 2015, proceedings},
	publisher = {Springer},
	author = {Kille, Benjamin and Lommatzsch, Andreas and Turrin, Roberto and Serény, András and Larson, Martha A. and Brodt, Torben and Seiler, Jonas and Hopfgartner, Frank},
	editor = {Mothe, Josiane and Savoy, Jacques and Kamps, Jaap and Pinel-Sauvagnat, Karen and Jones, Gareth J. F. and SanJuan, Eric and Cappellato, Linda and Ferro, Nicola},
	year = {2015},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/clef/KilleLTSLBSH15.bib
tex.timestamp: Fri, 06 Dec 2019 00:00:00 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {497--517},
}

@inproceedings{DBLP:conf/recsys/GarcinFDABH14,
	address = {Silicon Valley, CA},
	title = {Offline and online evaluation of news recommender systems at swissinfo.ch},
	url = {https://doi.org/10.1145/2645710.2645745},
	doi = {10/ghh3r6},
	booktitle = {{ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Garcin, Florent and Faltings, Boi and Donatsch, Olivier and Alazzawi, Ayar and Bruttin, Christophe and Huber, Amr},
	editor = {Kobsa, Alfred and Zhou, Michelle X. and Ester, Martin and Koren, Yehuda},
	year = {2014},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/recsys/GarcinFDABH14.bib
tex.timestamp: Tue, 06 Nov 2018 00:00:00 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {169--176},
}

@inproceedings{DBLP:conf/sigir/ChenZL0M17,
	title = {Meta-evaluation of online and offline web search evaluation metrics},
	url = {https://doi.org/10.1145/3077136.3080804},
	doi = {10/gmxkwc},
	booktitle = {Proceedings of the 40th international {ACM} {SIGIR} conference on research and development in information retrieval, shinjuku, tokyo, japan, august 7-11, 2017},
	publisher = {ACM},
	author = {Chen, Ye and Zhou, Ke and Liu, Yiqun and Zhang, Min and Ma, Shaoping},
	editor = {Kando, Noriko and Sakai, Tetsuya and Joho, Hideo and Li, Hang and de Vries, Arjen P. and White, Ryen W.},
	year = {2017},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/sigir/ChenZL0M17.bib
tex.timestamp: Wed, 16 Sep 2020 01:00:00 +0200},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {15--24},
}

@inproceedings{DBLP:conf/recsys/AbelDEK17,
	address = {Como, Italy},
	title = {{RecSys} challenge 2017: {Offline} and online evaluation},
	url = {https://doi.org/10.1145/3109859.3109954},
	doi = {10/gmxkv5},
	booktitle = {{ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Abel, Fabian and Deldjoo, Yashar and Elahi, Mehdi and Kohlsdorf, Daniel},
	editor = {Cremonesi, Paolo and Ricci, Francesco and Berkovsky, Shlomo and Tuzhilin, Alexander},
	year = {2017},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/recsys/AbelDEK17.bib
tex.timestamp: Mon, 16 Sep 2019 01:00:00 +0200},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {372--373},
}

@article{DBLP:journals/nms/Verboord14,
	title = {The impact of peer-produced criticism on cultural evaluation: {A} multilevel analysis of discourse employment in online and offline film reviews},
	volume = {16},
	url = {https://doi.org/10.1177/1461444813495164},
	doi = {10/gmxkv8},
	number = {6},
	journal = {New Media \& Society},
	author = {Verboord, Marc},
	year = {2014},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/nms/Verboord14.bib
tex.timestamp: Mon, 26 Oct 2020 00:00:00 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {921--940},
}

@inproceedings{DBLP:conf/www/Chapelle15,
	title = {Offline evaluation of response prediction in online advertising auctions},
	url = {https://doi.org/10.1145/2740908.2742566},
	doi = {10/gmxkv9},
	booktitle = {Proceedings of the 24th international conference on world wide web companion, {WWW} 2015, florence, italy, may 18-22, 2015 - companion volume},
	publisher = {ACM},
	author = {Chapelle, Olivier},
	editor = {Gangemi, Aldo and Leonardi, Stefano and Panconesi, Alessandro},
	year = {2015},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/www/Chapelle15.bib
tex.timestamp: Tue, 06 Nov 2018 00:00:00 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {919--922},
}

@inproceedings{DBLP:conf/clef/GebremeskelV16,
	series = {{CEUR} workshop proceedings},
	title = {Recommender systems evaluations : {Offline}, online, time and {A}/{A} test},
	volume = {1609},
	url = {http://ceur-ws.org/Vol-1609/16090642.pdf},
	booktitle = {Working notes of {CLEF} 2016 - conference and labs of the evaluation forum, évora, portugal, 5-8 september, 2016},
	publisher = {CEUR-WS.org},
	author = {Gebremeskel, Gebrekirstos G. and de Vries, Arjen P.},
	editor = {Balog, Krisztian and Cappellato, Linda and Ferro, Nicola and Macdonald, Craig},
	year = {2016},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/clef/GebremeskelV16.bib
tex.timestamp: Wed, 12 Feb 2020 16:44:30 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {642--656},
}

@inproceedings{DBLP:conf/aaai/MandelLBP16,
	title = {Offline evaluation of online reinforcement learning algorithms},
	url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12475},
	booktitle = {Proceedings of the thirtieth {AAAI} conference on artificial intelligence, february 12-17, 2016, phoenix, arizona, {USA}},
	publisher = {AAAI Press},
	author = {Mandel, Travis and Liu, Yun-En and Brunskill, Emma and Popovic, Zoran},
	editor = {Schuurmans, Dale and Wellman, Michael P.},
	year = {2016},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/aaai/MandelLBP16.bib
tex.timestamp: Wed, 10 Feb 2021 08:44:59 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {1926--1933},
}

@inproceedings{DBLP:conf/ecir/BampoulidisPLBH17,
	series = {Lecture notes in computer science},
	title = {Does online evaluation correspond to offline evaluation in query auto completion?},
	volume = {10193},
	url = {https://doi.org/10.1007/978-3-319-56608-5_70},
	doi = {10.1007/978-3-319-56608-5\_70},
	booktitle = {Advances in information retrieval - 39th european conference on {IR} research, {ECIR} 2017, aberdeen, {UK}, april 8-13, 2017, proceedings},
	author = {Bampoulidis, Alexandros and Palotti, João R. M. and Lupu, Mihai and Brassey, Jon and Hanbury, Allan},
	editor = {Jose, Joemon M. and Hauff, Claudia and Altingövde, Ismail Sengör and Song, Dawei and Albakour, Dyaa and Watt, Stuart N. K. and Tait, John},
	year = {2017},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/ecir/BampoulidisPLBH17.bib
tex.timestamp: Tue, 29 Dec 2020 18:27:42 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {713--719},
}

@inproceedings{DBLP:conf/russir/Kanoulas15,
	series = {Communications in computer and information science},
	title = {A short survey on online and offline methods for search quality evaluation},
	volume = {573},
	url = {https://doi.org/10.1007/978-3-319-41718-9_3},
	doi = {10.1007/978-3-319-41718-9\_3},
	booktitle = {Information retrieval - 9th russian summer school, {RuSSIR} 2015, saint petersburg, russia, august 24-28, 2015, revised selected papers},
	publisher = {Springer},
	author = {Kanoulas, Evangelos},
	editor = {Braslavski, Pavel and Markov, Ilya and Pardalos, Panos M. and Volkovich, Yana and Ignatov, Dmitry I. and Koltsov, Sergei and Koltsova, Olessia},
	year = {2015},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/russir/Kanoulas15.bib
tex.timestamp: Wed, 14 Nov 2018 00:00:00 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {38--87},
}

@inproceedings{DBLP:conf/clef/KilleLHLSMSB16,
	series = {{CEUR} workshop proceedings},
	title = {{CLEF} {NewsREEL} 2016: {Comparing} multi-dimensional offline and online evaluation of news recommender systems},
	volume = {1609},
	url = {http://ceur-ws.org/Vol-1609/16090593.pdf},
	booktitle = {Working notes of {CLEF} 2016 - conference and labs of the evaluation forum, évora, portugal, 5-8 september, 2016},
	publisher = {CEUR-WS.org},
	author = {Kille, Benjamin and Lommatzsch, Andreas and Hopfgartner, Frank and Larson, Martha A. and Seiler, Jonas and Malagoli, Davide and Serény, András and Brodt, Torben},
	editor = {Balog, Krisztian and Cappellato, Linda and Ferro, Nicola and Macdonald, Craig},
	year = {2016},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/clef/KilleLHLSMSB16.bib
tex.timestamp: Fri, 06 Dec 2019 00:00:00 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {593--605},
}

@phdthesis{DBLP:phd/ethos/Kharitonov16,
	title = {Using interaction data for improving the offline and online evaluation of search engines},
	url = {http://theses.gla.ac.uk/7750/},
	school = {University of Glasgow, UK},
	author = {Kharitonov, Evgeny},
	year = {2016},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/phd/ethos/Kharitonov16.bib
tex.timestamp: Wed, 05 Apr 2017 01:00:00 +0200},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
}

@inproceedings{mogenetPredictingOnlinePerformance2019,
	address = {New York, NY, USA},
	series = {{RecSys} '19},
	title = {Predicting online performance of job recommender systems with offline evaluation},
	isbn = {978-1-4503-6243-6},
	url = {https://doi.org/10.1145/3298689.3347032},
	doi = {10/gmxkv6},
	abstract = {At Indeed, recommender systems are used to recommend jobs. In this context, implicit and explicit feedback signals we can collect are rare events, making the task of evaluation more complex. Online evaluation (A/B testing) is usually the most reliable way to measure the results from our experiments, but it is a slow process. In contrast, the offline evaluation process is faster, but it is critical to make it reliable as it informs our decision to roll out new improvements in production. In this paper, we review the comparative offline and online performances of three recommendations models, we describe the evaluation metrics we use and analyze how the offline performance metrics correlate with online metrics to understand how an offline evaluation process can be leveraged to inform the decisions.},
	urldate = {2021-08-18},
	booktitle = {{ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Mogenet, Adrien and Pham, Tuan Anh Nguyen and Kazama, Masahiro and Kong, Jialin},
	month = sep,
	year = {2019},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {477--480},
}

@inproceedings{DBLP:conf/sigir/ZhangZSLZM18,
	title = {How well do offline and online evaluation metrics measure user satisfaction in web image search?},
	url = {https://doi.org/10.1145/3209978.3210059},
	doi = {10/gmxkwb},
	booktitle = {The 41st international {ACM} {SIGIR} conference on research \& development in information retrieval, {SIGIR} 2018, ann arbor, {MI}, {USA}, july 08-12, 2018},
	publisher = {ACM},
	author = {Zhang, Fan and Zhou, Ke and Shao, Yunqiu and Luo, Cheng and Zhang, Min and Ma, Shaoping},
	editor = {Collins-Thompson, Kevyn and Mei, Qiaozhu and Davison, Brian D. and Liu, Yiqun and Yilmaz, Emine},
	year = {2018},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/sigir/ZhangZSLZM18.bib
tex.timestamp: Sun, 25 Jul 2021 01:00:00 +0200},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {615--624},
}

@inproceedings{DBLP:conf/sigir/CarteretteC18,
	title = {Offline comparative evaluation with incremental, minimally-invasive online feedback},
	url = {https://doi.org/10.1145/3209978.3210050},
	doi = {10/gmxkwd},
	booktitle = {The 41st international {ACM} {SIGIR} conference on research \& development in information retrieval, {SIGIR} 2018, ann arbor, {MI}, {USA}, july 08-12, 2018},
	publisher = {ACM},
	author = {Carterette, Ben and Chandar, Praveen},
	editor = {Collins-Thompson, Kevyn and Mei, Qiaozhu and Davison, Brian D. and Liu, Yiqun and Yilmaz, Emine},
	year = {2018},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/sigir/CarteretteC18.bib
tex.timestamp: Wed, 16 Sep 2020 13:34:22 +0200},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {705--714},
}

@inproceedings{DBLP:conf/clef/KilleLHLB17,
	series = {{CEUR} workshop proceedings},
	title = {{CLEF} 2017 {NewsREEL} overview: {Offline} and online evaluation of stream-based news recommender systems},
	volume = {1866},
	url = {http://ceur-ws.org/Vol-1866/invited_paper_17.pdf},
	booktitle = {Working notes of {CLEF} 2017 - conference and labs of the evaluation forum, dublin, ireland, september 11-14, 2017},
	publisher = {CEUR-WS.org},
	author = {Kille, Benjamin and Lommatzsch, Andreas and Hopfgartner, Frank and Larson, Martha A. and Brodt, Torben},
	editor = {Cappellato, Linda and Ferro, Nicola and Goeuriot, Lorraine and Mandl, Thomas},
	year = {2017},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/clef/KilleLHLB17.bib
tex.timestamp: Wed, 12 Feb 2020 16:44:30 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
}

@article{DBLP:journals/cogsr/Yu18,
	title = {Sports activity detection, organization and evaluation in online to offline sports community},
	volume = {52},
	url = {https://doi.org/10.1016/j.cogsys.2018.09.002},
	doi = {10/gfq4kn},
	journal = {Cognitive Systems Research},
	author = {Yu, Lan},
	year = {2018},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/cogsr/Yu18.bib
tex.timestamp: Wed, 19 Feb 2020 00:00:00 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {785--792},
}

@article{DBLP:journals/mktsci/DzyaburaJM19,
	title = {Accounting for discrepancies between online and offline product evaluations},
	volume = {38},
	url = {https://doi.org/10.1287/mksc.2018.1124},
	doi = {10/ghrktf},
	number = {1},
	journal = {Mark. Sci.},
	author = {Dzyabura, Daria and Jagabathula, Srikanth and Muller, Eitan},
	year = {2019},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/mktsci/DzyaburaJM19.bib
tex.timestamp: Thu, 14 May 2020 01:00:00 +0200},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {88--106},
}

@article{DBLP:journals/fgcs/ChenYJ19,
	title = {Evaluation model for business sites planning based on online and offline datasets},
	volume = {91},
	url = {https://doi.org/10.1016/j.future.2018.08.024},
	doi = {10/gmxkwf},
	journal = {Future Gener. Comput. Syst.},
	author = {Chen, Jun and Yu, Chen and Jin, Hai},
	year = {2019},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/fgcs/ChenYJ19.bib
tex.timestamp: Wed, 19 Feb 2020 00:00:00 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {465--474},
}

@article{DBLP:journals/cogsr/Yu19,
	title = {Corrigendum to "{Sports} activity detection, organization and evaluation in online to offline sports community" [{Cogn}. {Syst}. {Res}. 52 (2018) 785-792]},
	volume = {56},
	url = {https://doi.org/10.1016/j.cogsys.2019.03.016},
	doi = {10/gf6pxn},
	journal = {Cognitive Systems Research},
	author = {Yu, Lan},
	year = {2019},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/cogsr/Yu19.bib
tex.timestamp: Wed, 19 Feb 2020 00:00:00 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {23},
}

@incollection{DBLP:books/ws/18/BelloginS18,
	title = {Offline and online evaluation of recommendations},
	url = {https://doi.org/10.1142/9789813275355_0009},
	booktitle = {Collaborative recommendations - algorithms, practical challenges and applications},
	publisher = {WorldScientific},
	author = {Bellogín, Alejandro and Said, Alan},
	editor = {Berkovsky, Shlomo and Cantador, Iván and Tikk, Domonkos},
	year = {2018},
	doi = {10.1142/9789813275355\_0009},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/books/ws/18/BelloginS18.bib
tex.timestamp: Sat, 13 Jul 2019 10:25:45 +0200},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {295--328},
}

@inproceedings{DBLP:conf/ntcir/AroraJ19,
	series = {Lecture notes in computer science},
	title = {Studying online and offline evaluation measures: {A} case study based on the {NTCIR}-14 {OpenLiveQ}-2 task},
	volume = {11966},
	url = {https://doi.org/10.1007/978-3-030-36805-0_6},
	doi = {10.1007/978-3-030-36805-0\_6},
	booktitle = {{NII} testbeds and community for information access research - 14th international conference, {NTCIR} 2019, tokyo, japan, june 10-13, 2019, revised selected papers},
	publisher = {Springer},
	author = {Arora, Piyush and Jones, Gareth J. F.},
	editor = {Kato, Makoto P. and Liu, Yiqun and Kando, Noriko and Clarke, Charles L. A.},
	year = {2019},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/ntcir/AroraJ19.bib
tex.timestamp: Wed, 16 Sep 2020 13:34:20 +0200},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {70--82},
}

@article{DBLP:journals/cbsn/ParkGMKR19,
	title = {Male adolescents' and young adults' evaluations of interracial exclusion in offline and online settings},
	volume = {22},
	url = {https://doi.org/10.1089/cyber.2019.0102},
	doi = {10/gmxkwg},
	number = {10},
	journal = {Cyberpsychology Behav. Soc. Netw.},
	author = {Park, Henry and Gönültas, Seçil and Mulvey, Kelly Lynn and Killen, Melanie and Ruck, Martin D.},
	year = {2019},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/cbsn/ParkGMKR19.bib
tex.timestamp: Mon, 26 Oct 2020 00:00:00 +0100},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {641--647},
}

@inproceedings{symeonidisRecommendingVideoWatch2020,
	address = {New York, NY, USA},
	series = {{RecSys} '20},
	title = {Recommending the {Video} to {Watch} {Next}: {An} {Offline} and {Online} {Evaluation} at {YOUTV}.de},
	isbn = {978-1-4503-7583-2},
	shorttitle = {Recommending the {Video} to {Watch} {Next}},
	url = {https://doi.org/10.1145/3383313.3412257},
	doi = {10/gmxksn},
	abstract = {The task “recommend a video to watch next?” has been in the focus of recommender systems’ research for a long time. However, adequately exploiting the clues hidden in the sequences of actions of user sessions in order to reveal users’ short-term intentions moved only recently into the focus of research. Based on a real-world application scenario, in this paper, we propose a Markov Chain-based transition probability matrix to efficiently reveal the short-term preferences of individuals. We experimentally evaluated our proposed method by comparing it against state-of-the-art algorithms in an offline as well as a live evaluation setting. In both cases our method not only demonstrated its superiority over its competitors, but exposed a clearly stronger engagement of users on the platform. In the online setting, our method improved the click-through rate by up to 93.61\%. This paper therefore contributes real-world evidence for improving the recommendation effectiveness, by considering sequence-awareness, since capturing the short-term preferences of users is crucial in the light of items with a short life span such as tv programs (news, tv shows, etc.).},
	urldate = {2021-08-18},
	booktitle = {{ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Symeonidis, Panagiotis and Janes, Andrea and Chaltsev, Dmitry and Giuliani, Philip and Morandini, Daniel and Unterhuber, Andreas and Coba, Ludovik and Zanker, Markus},
	year = {2020},
	keywords = {R - Offline Eval, R - Online Eval, Recommendation Systems},
	pages = {299--308},
}

@article{brittman_most_nodate,
	title = {The {Most} {Common} {Habits} from more than 200 {English} {Papers} written by {Graduate} {Chinese} {Engineering} {Students}},
	language = {en},
	author = {Brittman, Felicia},
	keywords = {Untagged},
}

@article{carlini_secret_nodate,
	title = {The {Secret} {Sharer}: {Evaluating} and {Testing} {Unintended} {Memorization} in {Neural} {Networks}},
	abstract = {This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models—a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users’ private messages), this methodology can beneﬁt privacy by allowing deep-learning practitioners to select means of training that minimize such memorization.},
	language = {en},
	author = {Carlini, Nicholas and Liu, Chang and Erlingsson, Úlfar and Kos, Jernej and Song, Dawn},
	keywords = {Untagged},
	pages = {19},
}

@article{harwath_unsupervised_nodate,
	title = {Unsupervised {Learning} of {Spoken} {Language} with {Visual} {Context}},
	abstract = {Humans learn to speak before they can read or write, so why can’t computers do the same? In this paper, we present a deep neural network model capable of rudimentary spoken language acquisition using untranscribed audio training data, whose only supervision comes in the form of contextually relevant visual images. We describe the collection of our data comprised of over 120,000 spoken audio captions for the Places image dataset and evaluate our model on an image search and annotation task. We also provide some visualizations which suggest that our model is learning to recognize meaningful words within the caption spectrograms.},
	language = {en},
	author = {Harwath, David and Torralba, Antonio and Glass, James},
	keywords = {Untagged},
	pages = {9},
}

@article{heControlBatchSize,
	title = {Control {Batch} {Size} and {Learning} {Rate} to {Generalize} {Well}: {Theoretical} and {Empirical} {Evidence}},
	abstract = {Deep neural networks have received dramatic success based on the optimization method of stochastic gradient descent (SGD). However, it is still not clear how to tune hyper-parameters, especially batch size and learning rate, to ensure good generalization. This paper reports both theoretical and empirical evidence of a training strategy that we should control the ratio of batch size to learning rate not too large to achieve a good generalization ability. Speciﬁcally, we prove a PAC-Bayes generalization bound for neural networks trained by SGD, which has a positive correlation with the ratio of batch size to learning rate. This correlation builds the theoretical foundation of the training strategy. Furthermore, we conduct a largescale experiment to verify the correlation and training strategy. We trained 1,600 models based on architectures ResNet-110, and VGG-19 with datasets CIFAR-10 and CIFAR-100 while strictly control unrelated variables. Accuracies on the test sets are collected for the evaluation. Spearman’s rank-order correlation coefﬁcients and the corresponding p values on 164 groups of the collected data demonstrate that the correlation is statistically signiﬁcant, which fully supports the training strategy.},
	language = {en},
	author = {He, Fengxiang and Liu, Tongliang and Tao, Dacheng},
	keywords = {Untagged},
	pages = {15},
}

@article{leinoStolenMemoriesLeveraging,
	title = {Stolen {Memories}: {Leveraging} {Model} {Memorization} for {Calibrated} {White}-{Box} {Membership} {Inference}},
	abstract = {Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overﬁtting occurs in deep neural networks, we show how a model’s idiosyncratic use of features can provide evidence for membership to white-box attackers—even when the model’s black-box behavior appears to generalize well—and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide conﬁdent positive inferences, we ﬁnd that previous attacks do not often provide a meaningful basis for conﬁdently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, ﬁnding that (1) smaller generalization error is not sufﬁcient to prevent attacks on real models, and (2) while small-ε-differential privacy reduces the attack’s effectiveness, this often comes at a signiﬁcant cost to the model’s accuracy; and for larger ε that are sometimes used in practice (e.g., ε = 16 [43]), the attack can achieve nearly the same accuracy as on the unprotected model.},
	language = {en},
	author = {Leino, Klas and Fredrikson, Matt},
	keywords = {Untagged},
	pages = {19},
}

@article{li_all_nodate,
	title = {All in {One} {Bad} {Weather} {Removal} {Using} {Architectural} {Search}},
	abstract = {Many methods have set state-of-the-art performance on restoring images degraded by bad weather such as rain, haze, fog, and snow, however they are designed speciﬁcally to handle one type of degradation. In this paper, we propose a method that can handle multiple bad weather degradations: rain, fog, snow and adherent raindrops using a single network. To achieve this, we ﬁrst design a generator with multiple task-speciﬁc encoders, each of which is associated with a particular bad weather degradation type. We utilize a neural architecture search to optimally process the image features extracted from all encoders. Subsequently, to convert degraded image features to clean background features, we introduce a series of tensor-based operations encapsulating the underlying physics principles behind the formation of rain, fog, snow and adherent raindrops. These operations serve as the basic building blocks for our architectural search. Finally, our discriminator simultaneously assesses the correctness and classiﬁes the degradation type of the restored image. We design a novel adversarial learning scheme that only backpropagates the loss of a degradation type to the respective task-speciﬁc encoder. Despite being designed to handle different types of bad weather, extensive experiments demonstrate that our method performs competitively to the individual and dedicated state-of-theart image restoration methods.},
	language = {en},
	author = {Li, Ruoteng and Tan, Robby T and Cheong, Loong-Fah},
	keywords = {Untagged},
	pages = {11},
}

@article{liu_use_nodate,
	title = {Use {What} {You} {Have}: {Video} {Retrieval} {Using} {Representations} {From} {Collaborative} {Experts}},
	shorttitle = {{CE}+},
	abstract = {The rapid growth of video on the internet has made searching for video content using natural language queries a significant challenge. Human-generated queries for video datasets ‘in the wild’ vary a lot in terms of degree of specificity, with some queries describing ‘specific details’ such as the names of famous identities, content from speech, or text available on the screen. Our goal is to condense the multi-modal, extremely high dimensional information from videos into a single, compact video representation for the task of video retrieval using free-form text queries, where the degree of specificity is open-ended.},
	language = {en},
	author = {Liu, Yang},
	keywords = {Untagged},
}

@article{lu_learn_nodate,
	title = {Learn to {Explain}: {Multimodal} {Reasoning} via {Thought} {Chains} for {Science} {Question} {Answering}},
	language = {en},
	author = {Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
	keywords = {Untagged},
}

@book{mohri_foundations_nodate,
	title = {Foundations of {Machine} {Learning} (second edition)},
	language = {en},
	author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
	keywords = {Untagged},
}

@article{nayak_zero-shot_nodate,
	title = {Zero-{Shot} {Learning} with {Common} {Sense} {Knowledge} {Graphs}},
	abstract = {Zero-shot learning relies on semantic class representations such as hand-engineered attributes or learned embeddings to predict classes without any labeled examples. We propose to learn class representations by embedding nodes from common sense knowledge graphs in a vector space. Common sense knowledge graphs are an untapped source of explicit high-level knowledge that requires little human eﬀort to apply to a range of tasks. To capture the knowledge in the graph, we introduce ZSL-KG, a general-purpose framework with a novel transformer graph convolutional network (TrGCN) for generating class representations. Our proposed TrGCN architecture computes non-linear combinations of node neighbourhoods. Our results show that ZSL-KG improves over existing WordNet-based methods on ﬁve out of six zero-shot benchmark datasets in language and vision. The code is available at https://github.com/BatsResearch/zsl-kg.},
	language = {en},
	author = {Nayak, Nihal V and Bach, Stephen H},
	keywords = {Untagged},
}

@misc{ren_chatgptai_nodate,
	title = {{ChatGPT能生成全新自然语言}，华人学者正研发同类{AI生成式模型}，计划用于药物发现领域},
	url = {http://mp.weixin.qq.com/s?__biz=MzA3NTIyODUzNA==&mid=2649693889&idx=1&sn=180ddddd228f3ba6c9824e24ffd5f68b&chksm=87680ad8b01f83ced2685d1ee309bd81b4cc55dc82e1c4e3c16b2ba6ad1e4188799d32f8803b#rd},
	abstract = {有了类似的 ChatGPT 模型，可以直接生成新的分子，比如新的蛋白质、新的抗体序列等。},
	urldate = {2023-04-06},
	journal = {Weixin Official Accounts Platform},
	author = {Ren、多加},
	keywords = {Untagged},
}

@misc{scienceai_deeptarget_nodate,
	title = {{DeepTarget}：筑波大学提出从蛋白序列生成分子的端到端方法},
	url = {http://mp.weixin.qq.com/s?__biz=MzI3MjM3ODk0NQ==&mid=2247494810&idx=1&sn=4f98a4b26d9ddf6d193b8d871a7e54cf&chksm=eb31d1b4dc4658a28e222032c79df1bed9390ddfc207cb0f0f1c2ebf01b4575549dd8ba6447c#rd},
	abstract = {设计生成靶向药物技术的新进展。},
	urldate = {2023-04-11},
	journal = {Weixin Official Accounts Platform},
	author = {ScienceAI},
	keywords = {Untagged},
}

@misc{scienceai__nodate,
	title = {蛋白质领域的 {ChatGPT}，首次使用对比学习准确预测酶功能},
	url = {http://mp.weixin.qq.com/s?__biz=MzI3MjM3ODk0NQ==&mid=2247494721&idx=1&sn=59fcc2269b266a21ee00a81500d7189a&chksm=eb31d16fdc4658791f178092b538f4f69e6289eb6cdf7c82d8ad59f3a2a5d8686c446948c2e6#rd},
	abstract = {CLEAN在准确性、可靠性和灵敏度方面超过了最先进的工具。},
	urldate = {2023-04-11},
	journal = {Weixin Official Accounts Platform},
	author = {ScienceAI},
	keywords = {Untagged},
}

@techreport{songmeiSTAT260MeanField,
	address = {Berkeley},
	title = {{STAT260}: {Mean} {Field} {Asymptotics} in {Statistical} {Learning}},
	url = {https://www.stat.berkeley.edu/~songmei/Teaching/STAT260_Spring2021/index.html},
	urldate = {2022-03-04},
	author = {{Song Mei}},
	keywords = {Untagged},
}

@article{stimpson_nash_nodate,
	title = {Nash {Equilibrium} or {Nash} {Bargaining}? {Choosing} a {Solution} {Concept} for {Multi}-{Agent} {Learning}},
	abstract = {Learning in many multi-agent settings is inherently repeated play. This calls into question the naive application of Nash equilibria in multi-agent learning and suggests, instead, the application of give-and-take principles of bargaining. We present an M action, N player social dilemma that encodes the key elements of the Prisoner’s Dilemma and thereby serves to highlight the importance of cooperation in multiagent systems. This game is instructive because it characterizes social dilemmas with more than two agents and more than two choices. We show how several diﬀerent multi-agent learning algorithms behave in this social dilemma, including a satisﬁcing algorithm based on [16] that is compatible with the bargaining perspective. This algorithm is a form of relaxation search that converges to a satisﬁcing equilibrium without knowledge of other agents actions and payoﬀs. Finally, we present theoretical results that characterize the behavior of the algorithm.},
	language = {en},
	author = {Stimpson, Jeffrey L and Goodrich, Michael A},
	keywords = {Untagged},
}

@article{wu_generative_nodate,
	title = {Generative {Visual} {Prompt}: {Unifying} {Distributional} {Control} of {Pre}-{Trained} {Generative} {Models}},
	language = {en},
	author = {Wu, Chen Henry and Motamed, Saman and Srivastava, Shaunak},
	keywords = {Untagged},
}

@article{yuHowDoesData,
	title = {How {Does} {Data} {Augmentation} {Affect} {Privacy} in {Machine} {Learning}?},
	abstract = {It is observed in the literature that data augmentation can signiﬁcantly mitigate membership inference (MI) attack. However, in this work, we challenge this observation by proposing new MI attacks to utilize the information of augmented data. MI attack is widely used to measure the model’s information leakage of the training set. We establish the optimal membership inference when the model is trained with augmented data, which inspires us to formulate the MI attack as a set classiﬁcation problem, i.e., classifying a set of augmented instances instead of a single data point, and design input permutation invariant features. Empirically, we demonstrate that the proposed approach universally outperforms original methods when the model is trained with data augmentation. Even further, we show that the proposed approach can achieve higher MI attack success rates on models trained with some data augmentation than the existing methods on models trained without data augmentation. Notably, we achieve 70.1\% MI attack success rate on CIFAR10 against a wide residual network while previous best approach only attains 61.9\%. This suggests the privacy risk of models trained with data augmentation could be largely underestimated.},
	language = {en},
	author = {Yu, Da and Zhang, Huishuai and Chen, Wei and Yin, Jian and Liu, Tie-Yan},
	keywords = {Untagged},
	pages = {8},
}

@misc{_chatgpt_nodate,
	title = {颠覆蛋白设计，生物界的{ChatGPT要来了}？},
	url = {http://mp.weixin.qq.com/s?__biz=MzAwMDA5NTIxNQ==&mid=2650046419&idx=1&sn=c82c4e804eaa9ac1493f02ec019ea6fb&chksm=82eef953b5997045b86d40f8d902588610b4787359d73c7244e6b3219a9c086bba884978b426#rd},
	abstract = {颠覆创新蛋白的生成模式},
	urldate = {2023-04-11},
	journal = {Weixin Official Accounts Platform},
	author = {药明康德},
	keywords = {Untagged},
}

@misc{noauthor_36-708_nodate,
	title = {36-708 {Statistical} {Machine} {Learning}, {Spring} 2018},
	url = {https://www.stat.cmu.edu/~larry/=sml/},
	urldate = {2022-05-24},
	keywords = {Untagged},
}

@misc{noauthor_50_nodate,
	title = {(50 封私信 / 80 条消息) 推荐系统embedding过大如何处理？有相关论文吗？ - 知乎},
	url = {https://www.zhihu.com/question/499895625/answer/2994025589},
	urldate = {2023-04-24},
	keywords = {Untagged},
}

@misc{AGCorpusNews,
	title = {{AG}'s corpus of news articles},
	url = {http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html},
	urldate = {2022-03-16},
	keywords = {Untagged},
}

@misc{noauthor_agents_nodate,
	title = {Agents — 🦜🔗 {LangChain} 0.0.139},
	url = {https://python.langchain.com/en/latest/modules/agents.html},
	urldate = {2023-04-14},
	keywords = {Untagged},
}

@inproceedings{noauthor_bridging_nodate,
	title = {Bridging the {Gap} {Between} {Adversarial} {ML} {Research} and {Practice}},
	booktitle = {{SatML}},
	keywords = {Untagged},
}

@misc{noauthor_cs_nodate,
	title = {{CS} 885 {Project} - {Online} {LaTeX} {Editor} {Overleaf}},
	url = {https://www.overleaf.com/project/6355ee67f23f02f074c4d8fd},
	urldate = {2022-12-10},
	keywords = {Untagged},
}

@misc{CVZhongDeAttentionJiZhiDCANetJieDuPdf,
	title = {【{CV中的Attention机制}】{DCANet解读}.pdf},
	keywords = {Untagged},
}

@misc{noauthor_defining_nodate,
	title = {Defining a {Graph} {Schema} - {GSQL} {Language} {Reference}},
	url = {https://docs.tigergraph.com/gsql-ref/current/ddl-and-loading/defining-a-graph-schema},
	abstract = {GSQL commands used to define a graph schema.},
	language = {en},
	urldate = {2023-04-11},
	journal = {TigerGraph Documentation},
	keywords = {Untagged},
}

@misc{noauthor_dinov2_nodate,
	title = {{DINOv2} by {Meta} {AI}},
	url = {https://dinov2.metademolab.com/},
	abstract = {A self-supervised vision transformer model by Meta AI},
	language = {en},
	urldate = {2023-04-24},
	keywords = {Untagged},
}

@misc{noauthor_downloads_nodate,
	title = {Downloads {\textbar} {UniProt} help {\textbar} {UniProt}},
	url = {https://www.uniprot.org/help/downloads},
	urldate = {2023-04-08},
	keywords = {Untagged},
}

@misc{noauthor_graph_nodate,
	title = {Graph {Modeling} {Guidelines} - {Developer} {Guides}},
	url = {https://neo4j.com/developer/guide-data-modeling/},
	abstract = {[object Object]},
	language = {en},
	urldate = {2023-04-11},
	journal = {Neo4j Graph Data Platform},
	keywords = {Untagged},
}

@book{noauthor_idea_nodate,
	title = {Idea},
	keywords = {Untagged},
}

@misc{noauthor_information_nodate,
	title = {Information {Retrieval}: {Implementing} and {Evaluating} {Search} {Engines}},
	url = {https://plg.uwaterloo.ca/~ir/ir/book/},
	urldate = {2023-04-21},
	keywords = {Untagged},
}

@misc{noauthor_minigpt-4_nodate,
	title = {Minigpt-4},
	url = {https://minigpt-4.github.io/},
	urldate = {2023-04-24},
	keywords = {Untagged},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {WebQuestionsSP} {Benchmark} ({Semantic} {Parsing})},
	url = {https://paperswithcode.com/sota/semantic-parsing-on-webquestionssp},
	abstract = {The current state-of-the-art on WebQuestionsSP is ReaRev. See a full comparison of 5 papers with code.},
	language = {en},
	urldate = {2023-04-16},
	keywords = {Untagged},
}

@misc{noauthor_resisting_nodate,
	title = {Resisting membership inference attacks through knowledge distillation - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/abs/pii/S0925231221006329},
	urldate = {2022-04-03},
	keywords = {Untagged},
}

@misc{noauthor_segment_nodate,
	title = {Segment {Anything} {\textbar} {Meta} {AI}},
	url = {https://segment-anything.com/},
	language = {en},
	urldate = {2023-04-06},
	keywords = {Untagged},
}

@misc{noauthor_ukplabmmt-retrieval_nodate,
	title = {{UKPLab}/{MMT}-{Retrieval}},
	url = {https://github.com/UKPLab/MMT-Retrieval},
	urldate = {2023-02-23},
	keywords = {Untagged},
}

@misc{noauthor_untitled_nodate,
	title = {Untitled - {Jupyter} {Notebook}},
	url = {http://localhost:8888/notebooks/Downloads/Untitled.ipynb},
	urldate = {2022-10-15},
	keywords = {Untagged},
}

@misc{RenLianShiBieZhongSoftmaxbasedLossDeYanHuaShiPdf,
	title = {{人脸识别中Softmax}-based {Loss的演化史}.pdf},
	keywords = {Untagged},
}

@misc{Cong2019AIDingHuiZuiJiaLunWenKanShenDuXueXiDeLiLunJiChu,
	title = {从2019 {AI顶会最佳论文}，看深度学习的理论基础.pdf},
	keywords = {Untagged},
}

@misc{CongPinYuJiaoDuChongXinSiKaoZhuYiLiJiZhiFcaNetPdf,
	title = {从频域角度重新思考注意力机制——{FcaNet}.pdf},
	keywords = {Untagged},
}

@techreport{YiLiao,
	title = {医疗},
	keywords = {Untagged},
}

@misc{JuanJiShenJingWangLuoDeFuZaDuFenXiPdf,
	title = {卷积神经网络的复杂度分析.pdf},
	keywords = {Untagged},
}

@misc{CaChuTiShengCNN,
	title = {擦除：提升 {CNN} 特征可视化的 3 种重要手段.pdf},
	keywords = {Untagged},
}

@misc{ShenJingWangLuoKeJieShiXingShenDuXueXiXinFangFa2020,
	title = {神经网络可解释性、深度学习新方法， 2020 年有哪些势不可挡的研究趋势？.pdf},
	keywords = {Untagged},
}

@misc{KeJiWenXinShouXuZhi,
	title = {科技文新手须知},
	keywords = {Untagged},
}

@misc{JieGouChongCanShuHuaLiYongCanShuZhuanHuanJieOuXunLianHeTuiLiJieGou,
	title = {结构重参数化：利用参数转换解耦训练和推理结构},
	keywords = {Untagged},
}

@techreport{ZongShuYiWenKanJinShenJingWangLuoZhongBuTongZhongLeiDeJuanJiCengPdf,
	title = {综述 \_ 一文看尽神经网络中不同种类的卷积层.pdf},
	keywords = {Untagged},
}

@techreport{ZongShuJiSuanJiShiJueZhongDeZhuYiLiJiZhiPdf,
	title = {综述｜计算机视觉中的注意力机制.pdf},
	keywords = {Untagged},
}

@techreport{WangLuoYaSuoZuiXinJinZhan2019NianZuiXinWenZhangGaiLanPdf,
	title = {网络压缩最新进展：2019年最新文章概览.pdf},
	keywords = {Untagged},
}

@misc{LianBangXueXiCoggleShuJuKeXue,
	title = {联邦学习 - {Coggle数据科学}},
	url = {https://coggle.club/note/dl/federated-learning},
	urldate = {2021-12-06},
	keywords = {Untagged},
}

@techreport{YingWeiDaKaiYuanImaginaireJiuDaTuXiangJiShiPinHeChengFangFa,
	title = {英伟达开源「{Imaginaire}」：九大图像及视频合成方法，你学fei了吗？.pdf},
	keywords = {Untagged},
}

@misc{DaoLiWoDuDongDanShiShenJingWangLuoFanXiangChuanBoShiDeTiDuDaoDiZenMeQiuPdf,
	title = {道理我都懂，但是神经网络反向传播时的梯度到底怎么求？.pdf},
	keywords = {Untagged},
}

@article{robbins1951stochastic,
	title = {A stochastic approximation method},
	shorttitle = {{SGD}},
	journal = {The annals of mathematical statistics},
	author = {Robbins, Herbert and Monro, Sutton},
	year = {1951},
	note = {Publisher: JSTOR},
	keywords = {Untagged},
	pages = {400--407},
}

@article{warnerRandomizedResponseSurvey1965,
	title = {Randomized response: {A} survey technique for eliminating evasive answer bias},
	volume = {60},
	doi = {10/gfxhsr},
	number = {309},
	journal = {Journal of the American Statistical Association},
	author = {Warner, Stanley L},
	year = {1965},
	note = {Publisher: Taylor \& Francis},
	keywords = {Untagged},
	pages = {63--69},
}

@article{schonemannGeneralizedSolutionOrthogonal1966,
	title = {A generalized solution of the orthogonal procrustes problem},
	volume = {31},
	number = {1},
	journal = {Psychometrika},
	author = {Schönemann, Peter H},
	year = {1966},
	note = {Publisher: Springer},
	keywords = {Untagged},
	pages = {1--10},
}

@article{lecam1973convergence,
	title = {Convergence of estimates under dimensionality restrictions},
	journal = {The Annals of Statistics},
	author = {LeCam, Lucien},
	year = {1973},
	note = {Publisher: JSTOR},
	keywords = {Untagged},
	pages = {38--53},
}

@article{Rivest1978,
	title = {On data banks and privacy homomorphisms},
	journal = {Foundations of Secure Computation, Academia Press},
	author = {Rivest, R L and Adleman, L and Dertouzos, M L},
	year = {1978},
	note = {tex.added-at: 2011-02-14T16:52:54.000+0100
tex.biburl: https://www.bibsonomy.org/bibtex/2df214f59ac3cc15fbfaf6d0606eb340c/fohv
tex.date-modified: 2006-08-22 11:34:23 +0200
tex.interhash: c880dde05ce4210752a154a7c0b6fd31
tex.intrahash: df214f59ac3cc15fbfaf6d0606eb340c
tex.timestamp: 2011-02-14T16:53:02.000+0100},
	keywords = {Untagged},
	pages = {169--179},
}

@inproceedings{NIPS1991_ff4d5fbb,
	title = {Principles of risk minimization for learning theory},
	volume = {4},
	shorttitle = {{ERM}},
	url = {https://proceedings.neurips.cc/paper/1991/file/ff4d5fbbafdf976cfdc032e3bde78de5-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Morgan-Kaufmann},
	author = {Vapnik, V.},
	editor = {Moody, J. and Hanson, S. and Lippmann, R.P.},
	year = {1991},
	keywords = {Untagged},
}

@incollection{hotellingRelationsTwoSets1992,
	title = {Relations between two sets of variates},
	booktitle = {Breakthroughs in statistics},
	publisher = {Springer},
	author = {Hotelling, Harold},
	year = {1992},
	keywords = {Untagged},
	pages = {162--190},
}

@inproceedings{kuniyoshiQualitativeRecognitionOngoing1993,
	address = {Chambéry, France},
	title = {Qualitative {Recognition} of {Ongoing} {Human} {Action} {Sequences}},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann},
	author = {Kuniyoshi, Yasuo and Inoue, Hirochika},
	editor = {Bajcsy, Ruzena},
	year = {1993},
	keywords = {Untagged},
	pages = {1600--1609},
}

@incollection{thomson_chapter_1994,
	title = {Chapter 35 {Cooperative} models of bargaining},
	volume = {2},
	booktitle = {Handbook of {Game} {Theory} with {Economic} {Applications}},
	publisher = {Elsevier},
	author = {Thomson, William},
	year = {1994},
	doi = {10.1016/S1574-0005(05)80067-0},
	keywords = {Untagged},
	pages = {1237--1284},
}

@article{lecunGradientbasedLearningApplied1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	shorttitle = {{MNIST}},
	doi = {10/d89c25},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick and {others}},
	year = {1998},
	note = {Publisher: Taipei, Taiwan},
	keywords = {Untagged},
	pages = {2278--2324},
}

@techreport{Samarati98protectingprivacy,
	title = {Protecting privacy when disclosing information: k-{Anonymity} and its enforcement through generalization and suppression},
	author = {Samarati, Pierangela and Sweeney, Latanya},
	year = {1998},
	keywords = {Untagged},
}

@book{DBLP:books/daglib/0097035,
	title = {Statistical learning theory},
	isbn = {978-0-471-03003-4},
	publisher = {Wiley},
	author = {Vapnik, Vladimir},
	year = {1998},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/books/daglib/0097035.bib
tex.timestamp: Thu, 21 Apr 2011 19:59:45 +0200},
	keywords = {Untagged},
}

@article{azzalini_statistical_1999,
	title = {Statistical applications of the multivariate skew-normal distribution},
	volume = {61},
	issn = {1369-7412, 1467-9868},
	url = {http://arxiv.org/abs/0911.2093},
	doi = {10.1111/1467-9868.00194},
	abstract = {Azzalini \& Dalla Valle (1996) have recently discussed the multivariate skew-normal distribution which extends the class of normal distributions by the addition of a shape parameter. The first part of the present paper examines further probabilistic properties of the distribution, with special emphasis on aspects of statistical relevance. Inferential and other statistical issues are discussed in the following part, with applications to some multivariate statistics problems, illustrated by numerical examples. Finally, a further extension is described which introduces a skewing factor of an elliptical density.},
	number = {3},
	urldate = {2023-02-15},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Azzalini, Adelchi and Capitanio, Antonella},
	month = aug,
	year = {1999},
	note = {arXiv:0911.2093 [stat]},
	keywords = {Untagged},
	pages = {579--602},
}

@article{barron_risk_1999,
	title = {Risk bounds for model selection via penalization},
	volume = {113},
	issn = {1432-2064},
	url = {https://doi.org/10.1007/s004400050210},
	doi = {10.1007/s004400050210},
	abstract = {Performance bounds for criteria for model selection are developed using recent theory for sieves. The model selection criteria are based on an empirical loss or contrast function with an added penalty term motivated by empirical process theory and roughly proportional to the number of parameters needed to describe the model divided by the number of observations. Most of our examples involve density or regression estimation settings and we focus on the problem of estimating the unknown density or regression function. We show that the quadratic risk of the minimum penalized empirical contrast estimator is bounded by an index of the accuracy of the sieve. This accuracy index quantifies the trade-off among the candidate models between the approximation error and parameter dimension relative to sample size.},
	language = {en},
	number = {3},
	urldate = {2022-05-26},
	journal = {Probability Theory and Related Fields},
	author = {Barron, Andrew and Birgé, Lucien and Massart, Pascal},
	month = feb,
	year = {1999},
	keywords = {Untagged},
	pages = {301--413},
}

@inproceedings{gionisSimilaritySearchHigh1999,
	title = {Similarity search in high dimensions via hashing},
	booktitle = {{VLDB}},
	author = {Gionis, Aristides and Indyk, Piotr and Motwani, Rajeev},
	year = {1999},
	keywords = {Untagged},
	pages = {518--529},
}

@inproceedings{auerAdaptiveSelfConfidentOnLine2000,
	title = {Adaptive and {Self}-{Confident} {On}-{Line} {Learning} {Algorithms}},
	shorttitle = {{lemmaAMS0}},
	booktitle = {Annual {Conference} on {Learning} {Theory}},
	author = {Auer, Peter and Gentile, Claudio},
	year = {2000},
	keywords = {Untagged},
	pages = {107--117},
}

@article{fliege_steepest_2000,
	title = {Steepest descent methods for multicriteria optimization},
	volume = {51},
	issn = {1432-5217},
	url = {https://doi.org/10.1007/s001860000043},
	doi = {10.1007/s001860000043},
	abstract = {We propose a steepest descent method for unconstrained multicriteria optimization and a “feasible descent direction” method for the constrained case. In the unconstrained case, the objective functions are assumed to be continuously differentiable. In the constrained case, objective and constraint functions are assumed to be Lipshitz-continuously differentiable and a constraint qualification is assumed. Under these conditions, it is shown that these methods converge to a point satisfying certain first-order necessary conditions for Pareto optimality. Both methods do not scalarize the original vector optimization problem. Neither ordering information nor weighting factors for the different objective functions are assumed to be known. In the single objective case, we retrieve the Steepest descent method and Zoutendijk's method of feasible directions, respectively.},
	language = {en},
	number = {3},
	urldate = {2022-06-25},
	journal = {Mathematical Methods of Operations Research},
	author = {Fliege, Jörg and Svaiter, Benar Fux},
	month = aug,
	year = {2000},
	keywords = {Untagged},
	pages = {479--494},
}

@inproceedings{papineniBLEUMethodAutomatic2001,
	address = {Philadelphia, Pennsylvania},
	title = {{BLEU}: a method for automatic evaluation of machine translation},
	shorttitle = {{BLEU}},
	url = {http://portal.acm.org/citation.cfm?doid=1073083.1073135},
	doi = {10/dmgshg},
	language = {en},
	urldate = {2020-11-09},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}  - {ACL} '02},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	year = {2001},
	keywords = {Untagged},
	pages = {311},
}

@techreport{penaIntegerProgrammingPart2001,
	title = {Integer programming (part 1)},
	language = {en},
	author = {Pena, Javier},
	year = {2001},
	keywords = {Untagged},
	pages = {30},
}

@article{auerAdaptiveSelfConfidentOnLine2002,
	title = {Adaptive and {Self}-{Confident} {On}-{Line} {Learning} {Algorithms}},
	volume = {64},
	issn = {0022-0000},
	url = {http://www.sciencedirect.com/science/article/pii/S0022000001917957},
	doi = {10/bstgct},
	abstract = {We study on-line learning in the linear regression framework. Most of the performance bounds for on-line algorithms in this framework assume a constant learning rate. To achieve these bounds the learning rate must be optimized based on a posteriori information. This information depends on the whole sequence of examples and thus it is not available to any strictly on-line algorithm. We introduce new techniques for adaptively tuning the learning rate as the data sequence is progressively revealed. Our techniques allow us to prove essentially the same bounds as if we knew the optimal learning rate in advance. Moreover, such techniques apply to a wide class of on-line algorithms, including p-norm algorithms for generalized linear regression and Weighted Majority for linear regression with absolute loss. Our adaptive tunings are radically different from previous techniques, such as the so-called doubling trick. Whereas the doubling trick restarts the on-line algorithm several times using a constant learning rate for each run, our methods save information by changing the value of the learning rate very smoothly. In fact, for Weighted Majority over a finite set of experts our analysis provides a better leading constant than the doubling trick.},
	number = {1},
	journal = {Journal of Computer and System Sciences},
	author = {Auer, Peter and Cesa-Bianchi, Nicolò and Gentile, Claudio},
	year = {2002},
	keywords = {Untagged},
	pages = {48 -- 75},
}

@inproceedings{charikarSimilarityEstimationTechniques2002,
	title = {Similarity estimation techniques from rounding algorithms},
	url = {https://doi.org/10.1145/509907.509965},
	doi = {10/ctxkwj},
	booktitle = {Proceedings on 34th {Annual} {ACM} {Symposium} on {Theory} of {Computing}, {May} 19-21, 2002, {Montréal}, {Québec}, {Canada}},
	publisher = {ACM},
	author = {Charikar, Moses},
	editor = {Reif, John H.},
	year = {2002},
	keywords = {Untagged},
	pages = {380--388},
}

@inproceedings{zinkevichOnlineConvexProgramming2003,
	address = {Washington, DC},
	title = {Online {Convex} {Programming} and {Generalized} {Infinitesimal} {Gradient} {Ascent}},
	url = {http://www.aaai.org/Library/ICML/2003/icml03-120.php},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {AAAI Press},
	author = {Zinkevich, Martin},
	editor = {Fawcett, Tom and Mishra, Nina},
	year = {2003},
	keywords = {Untagged},
	pages = {928--936},
}

@book{boydConvexOptimization2004,
	title = {Convex {Optimization}},
	publisher = {Cambridge University Press},
	author = {Boyd, Stephen and Vandenberghe, Lieven},
	year = {2004},
	doi = {10.1017/CBO9780511804441},
	keywords = {Untagged},
}

@inproceedings{datarLocalitysensitiveHashingScheme2004,
	title = {Locality-sensitive hashing scheme based on p-stable distributions},
	booktitle = {{SoCG}},
	author = {Datar, Mayur and Immorlica, Nicole and Indyk, Piotr and Mirrokni, Vahab S},
	year = {2004},
	keywords = {Untagged},
	pages = {253--262},
}

@article{DBLP:journals/neco/HardoonSS04,
	title = {Canonical correlation analysis: {An} overview with application to learning methods},
	volume = {16},
	url = {https://doi.org/10.1162/0899766042321814},
	doi = {10.1162/0899766042321814},
	number = {12},
	journal = {Neural Computation},
	author = {Hardoon, David R. and Szedmák, Sándor and Shawe-Taylor, John},
	year = {2004},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/neco/HardoonSS04.bib
tex.timestamp: Tue, 01 Sep 2020 13:12:34 +0200},
	keywords = {Untagged},
	pages = {2639--2664},
}

@inproceedings{jarvisIdentifyingTerroristActivity2004,
	address = {San Jose, California},
	title = {Identifying {Terrorist} {Activity} with {AI} {Plan} {Recognition} {Technology}},
	booktitle = {National {Conference} on {Artificial} {Intelligence}, {Sixteenth} {Conference} on {Innovative} {Applications} of {Artificial} {Intelligence}},
	publisher = {AAAI Press / The MIT Press},
	author = {Jarvis, Peter and Lunt, Teresa F. and Myers, Karen L.},
	editor = {McGuinness, Deborah L. and Ferguson, George},
	year = {2004},
	keywords = {Untagged},
	pages = {858--863},
}

@article{kraskovEstimatingMutualInformation2004,
	title = {Estimating mutual information},
	volume = {69},
	issn = {1539-3755, 1550-2376},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.69.066138},
	doi = {10/bffj69},
	language = {en},
	number = {6},
	urldate = {2021-01-27},
	journal = {Physical Review E},
	author = {Kraskov, Alexander and Stögbauer, Harald and Grassberger, Peter},
	month = jun,
	year = {2004},
	keywords = {Untagged},
	pages = {066138},
}

@article{loweDistinctiveImageFeatures2004,
	title = {Distinctive {Image} {Features} from {Scale}-{Invariant} {Keypoints}},
	volume = {60},
	issn = {1573-1405},
	url = {https://doi.org/10.1023/B:VISI.0000029664.99615.94},
	doi = {10/bqrmsp},
	abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
	number = {2},
	journal = {IJCV},
	author = {Lowe, David G.},
	year = {2004},
	keywords = {Untagged},
	pages = {91--110},
}

@inproceedings{aggarwalKanonymityCurseDimensionality2005,
	title = {On k-anonymity and the curse of dimensionality},
	booktitle = {{VLDB}},
	author = {Aggarwal, Charu C},
	year = {2005},
	keywords = {Untagged},
	pages = {901--909},
}

@inproceedings{buadesNonLocalAlgorithmImage2005,
	address = {San Diego, CA},
	title = {A {Non}-{Local} {Algorithm} for {Image} {Denoising}},
	url = {https://doi.org/10.1109/CVPR.2005.38},
	doi = {10/cgprz3},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Buades, Antoni and Coll, Bartomeu and Morel, Jean-Michel},
	year = {2005},
	keywords = {Untagged},
	pages = {60--65},
}

@inproceedings{dalalHistogramsOrientedGradients2005,
	title = {Histograms of oriented gradients for human detection},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Dalal, Navneet and Triggs, Bill},
	year = {2005},
	keywords = {Untagged},
	pages = {886--893},
}

@article{kalaiEfficientAlgorithmsOnline2005,
	title = {Efficient algorithms for online decision problems},
	volume = {71},
	doi = {10/bs62hb},
	number = {3},
	journal = {Journal of Computer and System Sciences},
	author = {Kalai, Adam and Vempala, Santosh},
	year = {2005},
	note = {Publisher: Elsevier},
	keywords = {Untagged},
	pages = {291--307},
}

@inproceedings{liaoLocationBasedActivityRecognition2005,
	address = {Edinburgh, Scotland, UK},
	title = {Location-{Based} {Activity} {Recognition} using {Relational} {Markov} {Networks}},
	url = {http://ijcai.org/Proceedings/05/Papers/1572.pdf},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {Professional Book Center},
	author = {Liao, Lin and Fox, Dieter and Kautz, Henry A.},
	editor = {Kaelbling, Leslie Pack and Saffiotti, Alessandro},
	year = {2005},
	keywords = {Untagged},
	pages = {773--778},
}

@article{nesterovSmoothMinimizationNonsmooth2005,
	title = {Smooth minimization of non-smooth functions},
	volume = {103},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/10.1007/s10107-004-0552-5},
	doi = {10.1007/s10107-004-0552-5},
	language = {en},
	number = {1},
	urldate = {2022-02-28},
	journal = {Mathematical Programming},
	author = {Nesterov, Yu.},
	month = may,
	year = {2005},
	keywords = {Untagged},
	pages = {127--152},
}

@inproceedings{raviActivityRecognitionAccelerometer2005,
	address = {Pittsburgh, Pennsylvania},
	title = {Activity {Recognition} from {Accelerometer} {Data}},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press / The MIT Press},
	author = {Ravi, Nishkam and Dandekar, Nikhil and Mysore, Preetham and Littman, Michael L.},
	editor = {Veloso, Manuela M. and Kambhampati, Subbarao},
	year = {2005},
	keywords = {Untagged},
	pages = {1541--1546},
}

@inproceedings{wyattUnsupervisedActivityRecognition2005,
	address = {Pittsburgh, Pennsylvania},
	title = {Unsupervised {Activity} {Recognition} {Using} {Automatically} {Mined} {Common} {Sense}},
	url = {http://www.aaai.org/Library/AAAI/2005/aaai05-004.php},
	booktitle = {National {Conference} on {Artificial} {Intelligence} and {Innovative} {Applications} of {Artificial} {Intelligence} {Conference}},
	publisher = {AAAI Press / The MIT Press},
	author = {Wyatt, Danny and Philipose, Matthai and Choudhury, Tanzeem},
	editor = {Veloso, Manuela M. and Kambhampati, Subbarao},
	year = {2005},
	keywords = {Untagged},
	pages = {21--27},
}

@inproceedings{yinActivityRecognitionGoalBased2005,
	address = {Pittsburgh, Pennsylvania},
	title = {Activity {Recognition} through {Goal}-{Based} {Segmentation}},
	url = {http://www.aaai.org/Library/AAAI/2005/aaai05-005.php},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press / The MIT Press},
	author = {Yin, Jie and Shen, Dou and Yang, Qiang and Li, Ze-Nian},
	editor = {Veloso, Manuela M. and Kambhampati, Subbarao},
	year = {2005},
	keywords = {Untagged},
	pages = {28--34},
}

@book{bishopPatternRecognitionMachine2006,
	address = {New York},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Untagged},
}

@article{massart_risk_2006,
	title = {Risk bounds for statistical learning},
	volume = {34},
	url = {https://doi.org/10.1214/009053606000000786},
	doi = {10.1214/009053606000000786},
	number = {5},
	journal = {The Annals of Statistics},
	author = {Massart, Pascal and Nédélec, Élodie},
	year = {2006},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Untagged},
	pages = {2326 -- 2366},
}

@techreport{MIT972Algebraic2006,
	title = {{MIT} 6.972 {Algebraic} techniques and semidefinite optimization},
	institution = {MIT},
	year = {2006},
	keywords = {Untagged},
}

@inproceedings{ben-davidAnalysisRepresentationsDomain2007,
	title = {Analysis of representations for domain adaptation},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ben-David, S. and Blitzer, J. and Crammer, K. and Pereira, F.},
	year = {2007},
	keywords = {Untagged},
	pages = {137--144},
}

@article{hazanLogarithmicRegretAlgorithms2007,
	title = {Logarithmic regret algorithms for online convex optimization},
	volume = {69},
	doi = {10/b9shmj},
	number = {2-3},
	journal = {Machine Learning},
	author = {Hazan, Elad and Agarwal, Amit and Kale, Satyen},
	year = {2007},
	note = {Publisher: Springer},
	keywords = {Untagged},
	pages = {169--192},
}

@book{picardConcentrationInequalitiesModel2007,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Mathematics}},
	title = {Concentration {Inequalities} and {Model} {Selection}},
	volume = {1896},
	isbn = {978-3-540-48497-4},
	url = {http://link.springer.com/10.1007/978-3-540-48503-2},
	language = {en},
	urldate = {2021-04-09},
	publisher = {Springer Berlin Heidelberg},
	editor = {Picard, Jean},
	year = {2007},
	doi = {10.1007/978-3-540-48503-2},
	keywords = {Untagged},
}

@article{abernethyOptimalStrategiesMinimax2008,
	title = {Optimal {Strategies} and {Minimax} {Lower} {Bounds} for {Online} {Convex} {Games}},
	abstract = {A number of learning problems can be cast as an Online Convex Game: on each round, a learner makes a prediction x from a convex set, the environment plays a loss function f , and the learner’s long-term goal is to minimize regret. Algorithms have been proposed by Zinkevich, when f is assumed to be convex, and Hazan et al., when f is assumed to be strongly convex, that have provably low regret. We consider these two settings and analyze such games from a minimax perspective, proving minimax strategies and lower bounds in each case. These results prove that the existing algorithms are essentially optimal.},
	language = {en},
	author = {Abernethy, Jacob and Rakhlin, Alexander and Bartlett, Peter L and Tewari, Ambuj},
	year = {2008},
	keywords = {Untagged},
	pages = {9},
}

@inproceedings{akdemirOntologyBasedApproach2008,
	address = {Vancouver, British Columbia, Canada},
	title = {An ontology based approach for activity recognition from video},
	url = {https://doi.org/10.1145/1459359.1459466},
	doi = {10.1145/1459359.1459466},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Akdemir, Umut and Turaga, Pavan K. and Chellappa, Rama},
	editor = {El-Saddik, Abdulmotaleb and Vuong, Son and Griwodz, Carsten and Bimbo, Alberto Del and Candan, K. Selçuk and Jaimes, Alejandro},
	year = {2008},
	keywords = {Untagged},
	pages = {709--712},
}

@incollection{anderssonFundamentalsAnonymityMetrics2008,
	address = {Boston, MA},
	title = {On the {Fundamentals} of {Anonymity} {Metrics}},
	isbn = {978-1-4419-4629-4 978-0-387-79026-8},
	url = {http://link.springer.com/10.1007/978-0-387-79026-8_23},
	abstract = {In recent years, a handful of anonymity metrics have been proposed that are either based on (i) the number participants in the given scenario, (ii) the probability distribution in an anonymous network regarding which participant is the sender / receiver, or (iii) a combination thereof. In this paper, we discuss elementary properties of metrics, and evaluate the behavior of a recent anonymity metrics in a set of application scenarios. Then, we deﬁne criteria for anonymity metrics and show than none of the studied metrics fulﬁll all criteria. Lastly, based on previous work on entropy-based anonymity metrics, we propose a new metric designed to fulﬁll these criteria – the so-called scaled anonymity set size.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {The {Future} of {Identity} in the {Information} {Society}},
	publisher = {Springer US},
	author = {Andersson, Christer and Lundin, Reine},
	editor = {Fischer-Hübner, Simone and Duquenoy, Penny and Zuccato, Albin and Martucci, Leonardo},
	year = {2008},
	doi = {10.1007/978-0-387-79026-8_23},
	keywords = {Untagged},
	pages = {325--341},
}

@article{andoniNearoptimalHashingAlgorithms2008,
	title = {Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions},
	volume = {51},
	doi = {10/b5dkc4},
	number = {1},
	journal = {Communications of the ACM},
	author = {Andoni, Alexandr and Indyk, Piotr},
	year = {2008},
	keywords = {Untagged},
	pages = {117},
}

@inproceedings{buiHiddenPermutationModel2008,
	address = {Chicago, Illinois},
	title = {The {Hidden} {Permutation} {Model} and {Location}-{Based} {Activity} {Recognition}},
	url = {http://www.aaai.org/Library/AAAI/2008/aaai08-213.php},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Bui, Hung Hai and Phung, Dinh Q. and Venkatesh, Svetha and Phan, Hai},
	editor = {Fox, Dieter and Gomes, Carla P.},
	year = {2008},
	keywords = {Untagged},
	pages = {1345--1350},
}

@inproceedings{hazanAdaptiveOnlineGradient2008,
	title = {Adaptive online gradient descent},
	shorttitle = {{AOGD}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hazan, Elad and Rakhlin, Alexander and Bartlett, Peter L},
	year = {2008},
	keywords = {Untagged},
	pages = {65--72},
}

@inproceedings{hoogsVideoActivityRecognition2008,
	address = {Chicago, Illinois},
	title = {Video {Activity} {Recognition} in the {Real} {World}},
	url = {http://www.aaai.org/Library/AAAI/2008/aaai08-260.php},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Hoogs, Anthony and Perera, A. G. Amitha},
	editor = {Fox, Dieter and Gomes, Carla P.},
	year = {2008},
	keywords = {Untagged},
	pages = {1551--1554},
}

@inproceedings{huCIGARConcurrentInterleaving2008,
	address = {Chicago, Illinois},
	title = {{CIGAR}: {Concurrent} and {Interleaving} {Goal} and {Activity} {Recognition}},
	url = {http://www.aaai.org/Library/AAAI/2008/aaai08-216.php},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Hu, Derek Hao and Yang, Qiang},
	editor = {Fox, Dieter and Gomes, Carla P.},
	year = {2008},
	keywords = {Untagged},
	pages = {1363--1368},
}

@inproceedings{huiskesMIRFlickrRetrieval2008,
	address = {New York, NY, USA},
	series = {{MIR} '08},
	title = {The {MIR} {Flickr} {Retrieval} {Evaluation}},
	isbn = {978-1-60558-312-9},
	url = {https://doi.org/10.1145/1460096.1460104},
	doi = {10/bbwcpg},
	abstract = {In most well known image retrieval test sets, the imagery typically cannot be freely distributed or is not representative of a large community of users. In this paper we present a collection for the MIR community comprising 25000 images from the Flickr website which are redistributable for research purposes and represent a real community of users both in the image content and image tags. We have extracted the tags and EXIF image metadata, and also make all of these publicly available. In addition we discuss several challenges for benchmarking retrieval and classification methods.},
	booktitle = {Proceedings of the 1st {ACM} {International} {Conference} on {Multimedia} {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Huiskes, Mark J. and Lew, Michael S.},
	year = {2008},
	note = {event-place: Vancouver, British Columbia, Canada},
	keywords = {Untagged},
	pages = {39--43},
}

@inproceedings{jiangCrossdomainLearningMethods2008,
	title = {Cross-domain learning methods for high-level visual concept classification},
	booktitle = {{ICIP}},
	author = {Jiang, W. and Zavesky, E. and Chang, S.-F. and Loui, A.},
	year = {2008},
	keywords = {Untagged},
	pages = {161--164},
}

@inproceedings{kultimaDesigningGameIdea2008,
	address = {Toronto, Ontario, Canada},
	title = {Designing game idea generation games},
	isbn = {978-1-60558-218-4},
	url = {http://portal.acm.org/citation.cfm?doid=1496984.1497007},
	doi = {10/crvpjp},
	abstract = {This paper introduces idea generation games designed for the use of game designers. Three games designed especially for generating new game ideas were developed in the GameSpace project that studies methods for design and evaluation of casual mobile multiplayer games. GameSpace idea generation games have been developed through an iterative process and in close cooperation with the end users, game industry professionals. According to our workshop experiences and tentative results from a pilot study, idea generation games can be successful devices for creative work of game designers. Game-based idea generation techniques provide an easily facilitated, focused but playful setting for coming up with new ideas. However, our experiences indicate that idea generation games feature special challenges which must be taken into consideration when designing such games.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {Proceedings of the 2008 {Conference} on {Future} {Play} {Research}, {Play}, {Share} - {Future} {Play} '08},
	publisher = {ACM Press},
	author = {Kultima, Annakaisa and Niemelä, Johannes and Paavilainen, Janne and Saarenpää, Hannamari},
	year = {2008},
	keywords = {Untagged},
	pages = {137},
}

@inproceedings{leighUsingCoevolutionUnderstand2008,
	address = {Atlanta, GA, USA},
	title = {Using coevolution to understand and validate game balance in continuous games},
	isbn = {978-1-60558-130-9},
	url = {http://portal.acm.org/citation.cfm?doid=1389095.1389394},
	doi = {10/fm3d2d},
	abstract = {We attack the problem of game balancing by using a coevolutionary algorithm to explore the space of possible game strategies and counter strategies. We deﬁne balanced games as games which have no single dominating strategy. Balanced games are more fun and provide a more interesting strategy space for players to explore. However, proving that a game is balanced mathematically may not be possible and industry commonly uses extensive and expensive human testing to balance games. We show how a coevolutionary algorithm can be used to test game balance and use the publicly available continuous state, capture-the-ﬂag CaST game as our testbed. Our results show that we can use coevolution to highlight game imbalances in CaST and provide intuition towards balancing this game. This aids in eliminating dominating strategies, thus making the game more interesting as players must constantly adapt to opponent strategies.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {Proceedings of the 10th annual conference on {Genetic} and evolutionary computation - {GECCO} '08},
	publisher = {ACM Press},
	author = {Leigh, Ryan and Schonfeld, Justin and Louis, Sushil J.},
	year = {2008},
	keywords = {Untagged},
	pages = {1563},
}

@inproceedings{liRealtimeHumanAction2008,
	address = {Vancouver, British Columbia, Canada},
	title = {Real-time human action recognition by luminance field trajectory analysis},
	url = {https://doi.org/10.1145/1459359.1459456},
	doi = {10.1145/1459359.1459456},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Li, Zhu and Fu, Yun and Huang, Thomas S. and Yan, Shuicheng},
	editor = {El-Saddik, Abdulmotaleb and Vuong, Son and Griwodz, Carsten and Bimbo, Alberto Del and Candan, K. Selçuk and Jaimes, Alejandro},
	year = {2008},
	keywords = {Untagged},
	pages = {671--676},
}

@inproceedings{lianChattingActivityRecognition2008,
	address = {Chicago, Illinois},
	title = {Chatting {Activity} {Recognition} in {Social} {Occasions} {Using} {Factorial} {Conditional} {Random} {Fields} with {Iterative} {Classification}},
	url = {http://www.aaai.org/Library/AAAI/2008/aaai08-297.php},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Lian, Chia-chun and Hsu, Jane Yung-jen},
	editor = {Fox, Dieter and Gomes, Carla P.},
	year = {2008},
	keywords = {Untagged},
	pages = {1814--1815},
}

@inproceedings{padoyOnlineRecognitionSurgical2008,
	address = {Chicago, Illinois},
	title = {On-line {Recognition} of {Surgical} {Activity} for {Monitoring} in the {Operating} {Room}},
	url = {http://www.aaai.org/Library/IAAI/2008/iaai08-015.php},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Padoy, Nicolas and Blum, Tobias and Feußner, Hubertus and Berger, Marie-Odile and Navab, Nassir},
	editor = {Fox, Dieter and Gomes, Carla P.},
	year = {2008},
	keywords = {Untagged},
	pages = {1718--1724},
}

@article{tsengAcceleratedProximalGradient2008,
	title = {On accelerated proximal gradient methods for convex-concave optimization},
	volume = {2},
	journal = {IAM Journal on Optimization},
	author = {Tseng, Paul},
	year = {2008},
	keywords = {Untagged},
	pages = {3},
}

@inproceedings{vailFeatureSelectionActivity2008,
	address = {Chicago, Illinois},
	title = {Feature {Selection} for {Activity} {Recognition} in {Multi}-{Robot} {Domains}},
	url = {http://www.aaai.org/Library/AAAI/2008/aaai08-224.php},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Vail, Douglas L. and Veloso, Manuela M.},
	editor = {Fox, Dieter and Gomes, Carla P.},
	year = {2008},
	keywords = {Untagged},
	pages = {1415--1420},
}

@inproceedings{vankasterenAccurateActivityRecognition2008,
	address = {New York, NY, USA},
	series = {{UbiComp} '08},
	title = {Accurate activity recognition in a home setting},
	isbn = {978-1-60558-136-1},
	url = {https://doi.org/10.1145/1409635.1409637},
	doi = {10/bxvfsk},
	abstract = {A sensor system capable of automatically recognizing activities would allow many potential ubiquitous applications. In this paper, we present an easy to install sensor network and an accurate but inexpensive annotation method. A recorded dataset consisting of 28 days of sensor data and its annotation is described and made available to the community. Through a number of experiments we show how the hidden Markov model and conditional random fields perform in recognizing activities. We achieve a timeslice accuracy of 95.6\% and a class accuracy of 79.4\%.},
	urldate = {2021-09-27},
	booktitle = {Proceedings of the 10th international conference on {Ubiquitous} computing},
	publisher = {Association for Computing Machinery},
	author = {van Kasteren, Tim and Noulas, Athanasios and Englebienne, Gwenn and Kröse, Ben},
	month = sep,
	year = {2008},
	keywords = {Untagged},
	pages = {1--9},
}

@article{acquisti_predicting_2009,
	title = {Predicting {Social} {Security} numbers from public data},
	volume = {106},
	url = {https://www.pnas.org/doi/10.1073/pnas.0904891106},
	doi = {10.1073/pnas.0904891106},
	number = {27},
	urldate = {2022-10-25},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Acquisti, Alessandro and Gross, Ralph},
	month = jul,
	year = {2009},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	keywords = {Untagged},
	pages = {10975--10980},
}

@inproceedings{chaudhuriPrivacypreservingLogisticRegression2009,
	title = {Privacy-preserving logistic regression},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chaudhuri, Kamalika and Monteleoni, Claire},
	editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
	year = {2009},
	keywords = {Untagged},
	pages = {289--296},
}

@inproceedings{chuaNUSWIDERealworldWeb2009,
	title = {{NUS}-{WIDE}: a real-world web image database from {National} {University} of {Singapore}},
	shorttitle = {{NUS}-{WIDE}},
	doi = {10/dzgqch},
	booktitle = {{CIVR}},
	author = {Chua, Tat-Seng and Tang, Jinhui and Hong, Richang and Li, Haojie and Luo, Zhiping and Zheng, Yantao},
	year = {2009},
	keywords = {Untagged},
	pages = {1--9},
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	doi = {10/cvc7xp},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year = {2009},
	keywords = {Untagged},
	pages = {248--255},
}

@inproceedings{doProximalRegularizationOnline2009,
	title = {Proximal regularization for online and batch learning},
	doi = {10/b43b29},
	booktitle = {{ICML}},
	author = {Do, Chuong B and Le, Quoc V and Foo, Chuan-Sheng},
	year = {2009},
	keywords = {Untagged},
	pages = {257--264},
}

@inproceedings{duanDomainTransferSVM2009,
	title = {Domain transfer {SVM} for video concept detection},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Duan, L. and Tsang, I. W and Xu, D. and Maybank, S. J},
	year = {2009},
	keywords = {Untagged},
	pages = {1375--1381},
}

@article{JMLR:v10:duchi09a,
	title = {Efficient online and batch learning using forward backward splitting},
	volume = {10},
	url = {http://jmlr.org/papers/v10/duchi09a.html},
	number = {99},
	journal = {Journal of Machine Learning Research},
	author = {Duchi, John and Singer, Yoram},
	year = {2009},
	keywords = {Untagged},
	pages = {2899--2934},
}

@inproceedings{dworkDifferentialPrivacyRobust2009,
	address = {New York, NY, USA},
	series = {{STOC} '09},
	title = {Differential privacy and robust statistics},
	isbn = {978-1-60558-506-2},
	url = {https://doi.org/10.1145/1536414.1536466},
	doi = {10/bgjfm2},
	abstract = {We show by means of several examples that robust statistical estimators present an excellent starting point for differentially private estimators. Our algorithms use a new paradigm for differentially private mechanisms, which we call Propose-Test-Release (PTR), and for which we give a formal definition and general composition theorems.},
	urldate = {2021-09-28},
	booktitle = {Proceedings of the forty-first annual {ACM} symposium on {Theory} of computing},
	publisher = {Association for Computing Machinery},
	author = {Dwork, Cynthia and Lei, Jing},
	month = may,
	year = {2009},
	keywords = {Untagged},
	pages = {371--380},
}

@inproceedings{elhamifarSparseSubspaceClustering2009,
	title = {Sparse subspace clustering},
	url = {https://doi.org/10.1109/CVPR.2009.5206547},
	doi = {10.1109/CVPR.2009.5206547},
	booktitle = {2009 {IEEE} computer society conference on computer vision and pattern recognition ({CVPR} 2009), 20-25 june 2009, miami, florida, {USA}},
	publisher = {IEEE Computer Society},
	author = {Elhamifar, Ehsan and Vidal, René},
	year = {2009},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/cvpr/ElhamifarV09.bib
tex.timestamp: Fri, 14 Feb 2020 11:46:23 +0100},
	keywords = {Untagged},
	pages = {2790--2797},
}

@inproceedings{gabaldonActivityRecognitionIntended2009,
	address = {Pasadena, California},
	title = {Activity {Recognition} with {Intended} {Actions}},
	url = {http://ijcai.org/Proceedings/09/Papers/283.pdf},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Gabaldon, Alfredo},
	editor = {Boutilier, Craig},
	year = {2009},
	keywords = {Untagged},
	pages = {1696--1701},
}

@inproceedings{huAbnormalActivityRecognition2009,
	address = {Pasadena, California},
	title = {Abnormal {Activity} {Recognition} {Based} on {HDP}-{HMM} {Models}},
	url = {http://ijcai.org/Proceedings/09/Papers/286.pdf},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Hu, Derek Hao and Zhang, Xian-Xing and Yin, Jie and Zheng, Vincent Wenchen and Yang, Qiang},
	editor = {Boutilier, Craig},
	year = {2009},
	keywords = {Untagged},
	pages = {1715--1720},
}

@techreport{krizhevskyLearningMultipleLayers2009,
	title = {Learning multiple layers of features from tiny images},
	shorttitle = {Cifar},
	institution = {Citeseer},
	author = {Krizhevsky, Alex and Hinton, Geoffrey},
	year = {2009},
	keywords = {Untagged},
}

@inproceedings{kulisLearningHashBinary2009,
	title = {Learning to {Hash} with {Binary} {Reconstructive} {Embeddings}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kulis, Brian and Darrell, Trevor},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	year = {2009},
	keywords = {Untagged},
	pages = {1042--1050},
}

@inproceedings{lianProbabilisticModelsConcurrent2009,
	address = {Pasadena, California},
	title = {Probabilistic {Models} for {Concurrent} {Chatting} {Activity} {Recognition}},
	url = {http://ijcai.org/Proceedings/09/Papers/192.pdf},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Lian, Chia-chun and Hsu, Jane Yung-jen},
	editor = {Boutilier, Craig},
	year = {2009},
	keywords = {Untagged},
	pages = {1138--1143},
}

@article{nesterovPrimaldualSubgradientMethods2009,
	title = {Primal-dual subgradient methods for convex problems},
	volume = {120},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/10.1007/s10107-007-0149-x},
	doi = {10/d63rjd},
	abstract = {In this paper we present a new approach for constructing subgradient schemes for different types of nonsmooth problems with convex structure. Our methods are primal-dual since they are always able to generate a feasible approximation to the optimum of an appropriately formulated dual problem. Besides other advantages, this useful feature provides the methods with a reliable stopping criterion. The proposed schemes differ from the classical approaches (divergent series methods, mirror descent methods) by presence of two control sequences. The ﬁrst sequence is responsible for aggregating the support functions in the dual space, and the second one establishes a dynamically updated scale between the primal and dual spaces. This additional ﬂexibility allows to guarantee a boundedness of the sequence of primal test points even in the case of unbounded feasible set (however, we always assume the uniform boundedness of subgradients). We present the variants of subgradient schemes for nonsmooth convex minimization, minimax problems, saddle point problems, variational inequalities, and stochastic optimization. In all situations our methods are proved to be optimal from the view point of worst-case black-box lower complexity bounds.},
	language = {en},
	number = {1},
	urldate = {2021-01-27},
	journal = {Mathematical Programming},
	author = {Nesterov, Yurii},
	month = aug,
	year = {2009},
	keywords = {Untagged},
	pages = {221--259},
}

@article{rubinsteinLearningLargeFunction2009,
	title = {Learning in a {Large} {Function} {Space}: {Privacy}-{Preserving} {Mechanisms} for {SVM} {Learning}},
	journal = {CoRR},
	author = {Rubinstein, Benjamin I. P. and Bartlett, Peter L. and Huang, Ling and Taft, Nina},
	year = {2009},
	keywords = {Untagged},
}

@inproceedings{wangHumanActivityEncoding2009,
	address = {Pasadena, California},
	title = {Human {Activity} {Encoding} and {Recognition} {Using} {Low}-level {Visual} {Features}},
	url = {http://ijcai.org/Proceedings/09/Papers/311.pdf},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Wang, Zheshen and Li, Baoxin},
	editor = {Boutilier, Craig},
	year = {2009},
	keywords = {Untagged},
	pages = {1876--1883},
}

@inproceedings{weinbergerFeatureHashingLarge2009,
	series = {{ACM} {International} {Conference} {Proceeding} {Series}},
	title = {Feature hashing for large scale multitask learning},
	volume = {382},
	url = {https://doi.org/10.1145/1553374.1553516},
	doi = {10/c2f9jw},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}, {ICML} 2009, {Montreal}, {Quebec}, {Canada}, {June} 14-18, 2009},
	publisher = {ACM},
	author = {Weinberger, Kilian Q. and Dasgupta, Anirban and Langford, John and Smola, Alexander J. and Attenberg, Josh},
	editor = {Danyluk, Andrea Pohoreckyj and Bottou, Léon and Littman, Michael L.},
	year = {2009},
	keywords = {Untagged},
	pages = {1113--1120},
}

@inproceedings{yangActivityRecognitionLinking2009,
	address = {Pasadena, California},
	title = {Activity {Recognition}: {Linking} {Low}-level {Sensors} to {High}-level {Intelligence}},
	url = {http://ijcai.org/Proceedings/09/Papers/015.pdf},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Yang, Qiang},
	editor = {Boutilier, Craig},
	year = {2009},
	keywords = {Untagged},
	pages = {20--25},
}

@inproceedings{yuLearningStructuralSVMs2009,
	address = {Montreal, Quebec, Canada},
	title = {Learning structural {SVMs} with latent variables},
	isbn = {978-1-60558-516-1},
	url = {http://portal.acm.org/citation.cfm?doid=1553374.1553523},
	doi = {10/cx86cj},
	abstract = {We present a large-margin formulation and algorithm for structured output prediction that allows the use of latent variables. Our proposal covers a large range of application problems, with an optimization problem that can be solved eﬃciently using ConcaveConvex Programming. The generality and performance of the approach is demonstrated through three applications including motifﬁnding, noun-phrase coreference resolution, and optimizing precision at k in information retrieval.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning} - {ICML} '09},
	publisher = {ACM Press},
	author = {Yu, Chun-Nam John and Joachims, Thorsten},
	year = {2009},
	keywords = {Untagged},
	pages = {1--8},
}

@inproceedings{a.bergamoExploitingWeaklylabeledWeb2010,
	title = {Exploiting weakly-labeled {Web} images to improve object classification: a domain adaptation approach},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {A. Bergamo, L. Torresani},
	year = {2010},
	keywords = {Untagged},
	pages = {181--189},
}

@article{ben-davidTheoryLearningDifferent2010,
	title = {A theory of learning from different domains},
	volume = {79},
	doi = {10/c5bhdk},
	number = {1},
	journal = {Machine learning},
	author = {Ben-David, S. and Blitzer, J. and Crammer, K. and Kulesza, A. and Pereira, F. and Vaughan, J. W.},
	year = {2010},
	keywords = {Untagged},
	pages = {151--175},
}

@article{chechikLargeScaleOnline2010,
	title = {Large {Scale} {Online} {Learning} of {Image} {Similarity} {Through} {Ranking}},
	volume = {11},
	url = {https://dl.acm.org/citation.cfm?id=1756042},
	journal = {J. Mach. Learn. Res.},
	author = {Chechik, Gal and Sharma, Varun and Shalit, Uri and Bengio, Samy},
	year = {2010},
	keywords = {Untagged},
	pages = {1109--1135},
}

@inproceedings{conaireCombiningInertialVisual2010,
	address = {Firenze, Italy},
	title = {Combining inertial and visual sensing for human action recognition in tennis},
	url = {https://doi.org/10.1145/1877868.1877882},
	doi = {10.1145/1877868.1877882},
	booktitle = {{ACM} {International} {Conference} on {Multimedia} {Workshop}},
	publisher = {ACM},
	author = {Conaire, Ciarán Ó and Connaghan, Damien and Kelly, Philip and O'Connor, Noel E. and Gaffney, Mark and Buckley, John},
	editor = {Doulamis, Anastasios D. and Gonzàlez, Jordi and Bertini, Marco and Gool, Luc Van and Matsatsinis, Nikolaos F. and Moeslund, Thomas B. and Nixon, Mark S. and Wang, Liang},
	year = {2010},
	keywords = {Untagged},
	pages = {51--56},
}

@inproceedings{duchiCompositeObjectiveMirror2010,
	title = {Composite {Objective} {Mirror} {Descent}},
	booktitle = {{COLT}},
	author = {Duchi, John C and Shalev-Shwartz, Shai and Singer, Yoram and Tewari, Ambuj},
	year = {2010},
	keywords = {Untagged},
	pages = {14--26},
}

@inproceedings{dworkDifferentialPrivacyContinual2010,
	address = {Cambridge, Massachusetts},
	title = {Differential {Privacy} under {Continual} {Observation}},
	isbn = {978-1-4503-0050-6},
	shorttitle = {{TreeBasedAlgorithm}},
	url = {https://doi.org/10.1145/1806689.1806787},
	doi = {10/cs855m},
	abstract = {Differential privacy is a recent notion of privacy tailored to privacy-preserving data analysis [11]. Up to this point, research on differentially private data analysis has focused on the setting of a trusted curator holding a large, static, data set; thus every computation is a "one-shot" object: there is no point in computing something twice, since the result will be unchanged, up to any randomness introduced for privacy. However, many applications of data analysis involve repeated computations, either because the entire goal is one of monitoring, e.g., of traffic conditions, search trends, or incidence of influenza, or because the goal is some kind of adaptive optimization, e.g., placement of data to minimize access costs. In these cases, the algorithm must permit continual observation of the system's state. We therefore initiate a study of differential privacy under continual observation. We identify the problem of maintaining a counter in a privacy preserving manner and show its wide applicability to many different problems.},
	booktitle = {{ACM} {Symposium} on {Theory} of {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Dwork, Cynthia and Naor, Moni and Pitassi, Toniann and Rothblum, Guy N.},
	year = {2010},
	keywords = {Untagged},
	pages = {715--724},
}

@inproceedings{dworkBoostingDifferentialPrivacy2010,
	address = {Las Vegas, Nevada},
	title = {Boosting and differential privacy},
	shorttitle = {Advanced {Composition} of {Differential} {Privacy}},
	booktitle = {{IEEE} {Symposium} on {Foundations} of {Computer} {Science}},
	publisher = {IEEE},
	author = {Dwork, Cynthia and Rothblum, Guy N and Vadhan, Salil},
	year = {2010},
	keywords = {Untagged},
	pages = {51--60},
}

@article{escalanteSegmentedAnnotatedIAPR2010,
	title = {The segmented and annotated {IAPR} {TC}-12 benchmark},
	volume = {114},
	issn = {1077-3142},
	shorttitle = {{IAPR}},
	url = {http://www.sciencedirect.com/science/article/pii/S1077314209000575},
	doi = {10/ck49r7},
	abstract = {Automatic image annotation (AIA), a highly popular topic in the field of information retrieval research, has experienced significant progress within the last decade. Yet, the lack of a standardized evaluation platform tailored to the needs of AIA, has hindered effective evaluation of its methods, especially for region-based AIA. Therefore in this paper, we introduce the segmented and annotated IAPR TC-12 benchmark; an extended resource for the evaluation of AIA methods as well as the analysis of their impact on multimedia information retrieval. We describe the methodology adopted for the manual segmentation and annotation of images, and present statistics for the extended collection. The extended collection is publicly available and can be used to evaluate a variety of tasks in addition to image annotation. We also propose a soft measure for the evaluation of annotation performance and identify future research areas in which this extended test collection is likely to make a contribution.},
	number = {4},
	journal = {Computer Vision and Image Understanding},
	author = {Escalante, Hugo Jair and Hernández, Carlos A. and Gonzalez, Jesus A. and López-López, A. and Montes, Manuel and Morales, Eduardo F. and Sucar, L. Enrique and Villaseñor, Luis and Grubinger, Michael},
	year = {2010},
	keywords = {Untagged},
	pages = {419 -- 428},
}

@inproceedings{frankActivityGaitRecognition2010,
	address = {Atlanta, Georgia},
	title = {Activity and {Gait} {Recognition} with {Time}-{Delay} {Embeddings}},
	url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1666},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Frank, Jordan and Mannor, Shie and Precup, Doina},
	editor = {Fox, Maria and Poole, David},
	year = {2010},
	keywords = {Untagged},
}

@book{glasman-deal_science_2010,
	address = {London ; Hackensack, NJ},
	title = {Science research writing for non-native speakers of {English}},
	isbn = {978-1-84816-309-6 978-1-84816-310-2},
	language = {en},
	publisher = {Imperial College Press},
	author = {Glasman-Deal, Hilary},
	year = {2010},
	keywords = {Untagged},
}

@inproceedings{guptaUsingClosedCaptions2010,
	address = {Atlanta, Georgia},
	title = {Using {Closed} {Captions} as {Supervision} for {Video} {Activity} {Recognition}},
	url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1588},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Gupta, Sonal and Mooney, Raymond J.},
	editor = {Fox, Maria and Poole, David},
	year = {2010},
	keywords = {Untagged},
}

@inproceedings{moghaddamHumanActionRecognition2010,
	address = {Firenze, Italy},
	title = {Human action recognition with {MPEG}-7 descriptors and architectures},
	url = {https://doi.org/10.1145/1877868.1877885},
	doi = {10.1145/1877868.1877885},
	booktitle = {{ACM} {International} {Conference} on {Multimedia} {Workshop}},
	publisher = {ACM},
	author = {Moghaddam, Zia and Piccardi, Massimo},
	editor = {Doulamis, Anastasios D. and Gonzàlez, Jordi and Bertini, Marco and Gool, Luc Van and Matsatsinis, Nikolaos F. and Moeslund, Thomas B. and Nixon, Mark S. and Wang, Liang},
	year = {2010},
	keywords = {Untagged},
	pages = {63--68},
}

@article{panSurveyTransferLearning2010,
	title = {A {Survey} on {Transfer} {Learning}},
	volume = {22},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/5288526/},
	doi = {10/bc4vws},
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classiﬁcation task in one domain of interest, but we only have sufﬁcient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classiﬁcation, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.},
	language = {en},
	number = {10},
	urldate = {2021-01-27},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	month = oct,
	year = {2010},
	keywords = {Untagged},
	pages = {1345--1359},
}

@article{radovanovi263_hubs_2010,
	title = {Hubs in {Space}: {Popular} {Nearest} {Neighbors} in {High}-{Dimensional} {Data}},
	volume = {11},
	url = {http://jmlr.org/papers/v11/radovanovic10a.html},
	number = {86},
	journal = {Journal of Machine Learning Research},
	author = {Radovanovi\&\#263;, Miloš and Nanopoulos, Alexandros and Ivanovi\&\#263;, Mirjana},
	year = {2010},
	keywords = {Untagged},
	pages = {2487--2531},
}

@inproceedings{saenkoAdaptingVisualCategory2010,
	title = {Adapting visual category models to new domains},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Saenko, K. and Kulis, B. and Fritz, M. and Darrell, T.},
	year = {2010},
	keywords = {Untagged},
	pages = {213--226},
}

@inproceedings{wangSequentialProjectionLearning2010,
	title = {Sequential {Projection} {Learning} for {Hashing} with {Compact} {Codes}},
	url = {https://icml.cc/Conferences/2010/papers/178.pdf},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Machine} {Learning} ({ICML}-10), {June} 21-24, 2010, {Haifa}, {Israel}},
	publisher = {Omnipress},
	author = {Wang, Jun and Kumar, Sanjiv and Chang, Shih-Fu},
	editor = {Fürnkranz, Johannes and Joachims, Thorsten},
	year = {2010},
	keywords = {Untagged},
	pages = {1127--1134},
}

@article{xiaoDualAveragingMethods2010,
	title = {Dual averaging methods for regularized stochastic learning and online optimization},
	volume = {11},
	number = {Oct},
	journal = {Journal of Machine Learning Research},
	author = {Xiao, Lin},
	year = {2010},
	keywords = {Untagged},
	pages = {2543--2596},
}

@book{bubeckIntroductionOnlineOptimization2011,
	title = {Introduction to {Online} {Optimization}},
	language = {en},
	author = {Bubeck, Sebastien},
	month = dec,
	year = {2011},
	keywords = {Untagged},
}

@inproceedings{bullingRecognitionVisualMemory2011,
	address = {New York, NY, USA},
	series = {{UbiComp} '11},
	title = {Recognition of visual memory recall processes using eye movement analysis},
	isbn = {978-1-4503-0630-0},
	url = {https://doi.org/10.1145/2030112.2030172},
	doi = {10/d27j4v},
	abstract = {Physical activity, location, as well as a person's psychophysiological and affective state are common dimensions for developing context-aware systems in ubiquitous computing. An important yet missing contextual dimension is the cognitive context that comprises all aspects related to mental information processing, such as perception, memory, knowledge, or learning. In this work we investigate the feasibility of recognising visual memory recall. We use a recognition methodology that combines minimum redundancy maximum relevance feature selection (mRMR) with a support vector machine (SVM) classifier. We validate the methodology in a dual user study with a total of fourteen participants looking at familiar and unfamiliar pictures from four picture categories: abstract, landscapes, faces, and buildings. Using person-independent training, we are able to discriminate between familiar and unfamiliar abstract pictures with a top recognition rate of 84.3\% (89.3\% recall, 21.0\% false positive rate) over all participants. We show that eye movement analysis is a promising approach to infer the cognitive context of a person and discuss the key challenges for the real-world implementation of eye-based cognition-aware systems.},
	urldate = {2021-09-27},
	booktitle = {Proceedings of the 13th international conference on {Ubiquitous} computing},
	publisher = {Association for Computing Machinery},
	author = {Bulling, Andreas and Roggen, Daniel},
	month = sep,
	year = {2011},
	keywords = {Untagged},
	pages = {455--464},
}

@article{chanPrivateContinualRelease2011,
	title = {Private and continual release of statistics},
	volume = {14},
	shorttitle = {{TreeBasedAlgorithm}},
	doi = {10/fq49nz},
	number = {3},
	journal = {ACM Transactions on Information and System Security},
	author = {Chan, T-H Hubert and Shi, Elaine and Song, Dawn},
	year = {2011},
	note = {Publisher: ACM},
	keywords = {Untagged},
	pages = {26},
}

@article{chaudhuriDifferentiallyPrivateEmpirical2011,
	title = {Differentially private empirical risk minimization},
	volume = {12},
	number = {Mar},
	journal = {Journal of Machine Learning Research},
	author = {Chaudhuri, Kamalika and Monteleoni, Claire and Sarwate, Anand D},
	year = {2011},
	keywords = {Untagged},
	pages = {1069--1109},
}

@inproceedings{chengMultitaskLowrankAffinity2011,
	address = {Barcelona, Spain},
	title = {Multi-task low-rank affinity pursuit for image segmentation},
	isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
	url = {http://ieeexplore.ieee.org/document/6126528/},
	doi = {10/fxjbwh},
	abstract = {This paper investigates how to boost region-based image segmentation by pursuing a new solution to fuse multiple types of image features. A collaborative image segmentation framework, called multi-task low-rank afﬁnity pursuit, is presented for such a purpose. Given an image described with multiple types of features, we aim at inferring a uniﬁed afﬁnity matrix that implicitly encodes the segmentation of the image. This is achieved by seeking the sparsityconsistent low-rank afﬁnities from the joint decompositions of multiple feature matrices into pairs of sparse and lowrank matrices, the latter of which is expressed as the production of the image feature matrix and its corresponding image afﬁnity matrix. The inference process is formulated as a constrained nuclear norm and 2,1-norm minimization problem, which is convex and can be solved efﬁciently with the Augmented Lagrange Multiplier method. Compared to previous methods, which are usually based on a single type of features, the proposed method seamlessly integrates multiple types of features to jointly produce the afﬁnity matrix within a single inference step, and produces more accurate and reliable segmentation results. Experiments on the MSRC dataset and Berkeley segmentation dataset well validate the superiority of using multiple features over single feature and also the superiority of our method over conventional methods for feature fusion. Moreover, our method is shown to be very competitive while comparing to other state-of-the-art methods.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {International {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Cheng, Bin and Liu, Guangcan and Wang, Jingdong and {Zhongyang Huang} and Yan, Shuicheng},
	year = {2011},
	keywords = {Untagged},
	pages = {2439--2446},
}

@article{duchiAdaptiveSubgradientMethods2011,
	title = {Adaptive {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization}},
	volume = {12},
	shorttitle = {{AdaGrad}},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	number = {61},
	journal = {Journal of Machine Learning Research},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	year = {2011},
	keywords = {Untagged},
	pages = {2121--2159},
}

@inproceedings{frankActivityRecognitionTimeDelay2011,
	title = {Activity {Recognition} with {Time}-{Delay} {Emobeddings}},
	url = {http://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2446},
	booktitle = {Computational {Physiology}, {Papers} from the 2011 {AAAI} {Spring} {Symposium}, {Technical} {Report} {SS}-11-04, {Stanford}, {California}, {USA}, {March} 21-23, 2011},
	publisher = {AAAI},
	author = {Frank, Jordan and Mannor, Shie and Precup, Doina},
	year = {2011},
	keywords = {Untagged},
}

@inproceedings{gopalanDomainAdaptationObject2011,
	title = {Domain adaptation for object recognition: {An} unsupervised approach},
	booktitle = {International {Conference} on {Computer} {Vision}},
	author = {Gopalan, R. and Li, R. and Chellappa, R.},
	year = {2011},
	keywords = {Untagged},
	pages = {999--1006},
}

@inproceedings{huTransferLearningActivity2011,
	address = {Barcelona, Catalonia, Spain},
	title = {Transfer {Learning} for {Activity} {Recognition} via {Sensor} {Mapping}},
	url = {https://doi.org/10.5591/978-1-57735-516-8/IJCAI11-328},
	doi = {10.5591/978-1-57735-516-8/IJCAI11-328},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {IJCAI/AAAI},
	author = {Hu, Derek Hao and Yang, Qiang},
	editor = {Walsh, Toby},
	year = {2011},
	keywords = {Untagged},
	pages = {1962--1967},
}

@article{jegouProductQuantizationNearest2011,
	title = {Product {Quantization} for {Nearest} {Neighbor} {Search}},
	volume = {33},
	issn = {0162-8828},
	url = {http://ieeexplore.ieee.org/document/5432202/},
	doi = {10/dgf6zs},
	abstract = {This paper introduces a product quantization based approach for approximate nearest neighbor search. The idea is to decomposes the space into a Cartesian product of low dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The Euclidean distance between two vectors can be efﬁciently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code.},
	language = {en},
	number = {1},
	urldate = {2021-01-27},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Jégou, H and Douze, M and Schmid, C},
	month = jan,
	year = {2011},
	keywords = {Untagged},
	pages = {117--128},
}

@inproceedings{kerrActivityRecognitionFinite2011,
	address = {Barcelona, Spain},
	title = {Activity {Recognition} with {Finite} {State} {Machines}},
	url = {https://doi.org/10.5591/978-1-57735-516-8/IJCAI11-228},
	doi = {10.5591/978-1-57735-516-8/IJCAI11-228},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {IJCAI/AAAI},
	author = {Kerr, Wesley and Tran, Anh and Cohen, Paul R.},
	editor = {Walsh, Toby},
	year = {2011},
	keywords = {Untagged},
	pages = {1348--1353},
}

@inproceedings{khoslaNovelDatasetFinegrained2011,
	title = {Novel dataset for fine-grained image categorization},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshop}},
	author = {Khosla, A. and Jayadevaprakash, N. and Yao, B. and Fei-Fei, L.},
	year = {2011},
	keywords = {Untagged},
	pages = {1--2},
}

@article{kwapiszActivityRecognitionUsing2011,
	title = {Activity recognition using cell phone accelerometers},
	volume = {12},
	issn = {1931-0145, 1931-0153},
	shorttitle = {Dataset: {WISDOM}},
	url = {https://dl.acm.org/doi/10.1145/1964897.1964918},
	doi = {10.1145/1964897.1964918},
	abstract = {Mobile devices are becoming increasingly sophisticated and the latest generation of smart cell phones now incorporates many diverse and powerful sensors. These sensors include GPS sensors, vision sensors (i.e., cameras), audio sensors (i.e., microphones), light sensors, temperature sensors, direction sensors (i.e., magnetic compasses), and acceleration sensors (i.e., accelerometers). The availability of these sensors in mass-marketed communication devices creates exciting new opportunities for data mining and data mining applications. In this paper we describe and evaluate a system that uses phone-based accelerometers to perform activity recognition, a task which involves identifying the physical activity a user is performing. To implement our system we collected labeled accelerometer data from twenty-nine users as they performed daily activities such as walking, jogging, climbing stairs, sitting, and standing, and then aggregated this time series data into examples that summarize the user activity over 10second intervals. We then used the resulting training data to induce a predictive model for activity recognition. This work is significant because the activity recognition model permits us to gain useful knowledge about the habits of millions of users passively—just by having them carry cell phones in their pockets. Our work has a wide range of applications, including automatic customization of the mobile device’s behavior based upon a user’s activity (e.g., sending calls directly to voicemail if a user is jogging) and generating a daily/weekly activity profile to determine if a user (perhaps an obese child) is performing a healthy amount of exercise.},
	language = {en},
	number = {2},
	urldate = {2021-07-02},
	journal = {ACM SIGKDD Explorations Newsletter},
	author = {Kwapisz, Jennifer R. and Weiss, Gary M. and Moore, Samuel A.},
	month = mar,
	year = {2011},
	keywords = {Untagged},
	pages = {74--82},
}

@inproceedings{liuHashingGraphs2011,
	title = {Hashing with graphs},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Liu, Wei and Wang, Jun and Kumar, Sanjiv and Chang, Shih-Fu},
	year = {2011},
	keywords = {Untagged},
	pages = {1--8},
}

@inproceedings{masatoAgentOrientedIncrementalTeam2011,
	address = {Barcelona, Catalonia, Spain},
	title = {Agent-{Oriented} {Incremental} {Team} and {Activity} {Recognition}},
	url = {https://doi.org/10.5591/978-1-57735-516-8/IJCAI11-237},
	doi = {10.5591/978-1-57735-516-8/IJCAI11-237},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {IJCAI/AAAI},
	author = {Masato, Daniele and Norman, Timothy J. and Vasconcelos, Wamberto Weber and Sycara, Katia P.},
	editor = {Walsh, Toby},
	year = {2011},
	keywords = {Untagged},
	pages = {1402--1407},
}

@inproceedings{norouziMinimalLossHashing2011,
	title = {Minimal loss hashing for compact binary codes},
	booktitle = {{ICML}},
	publisher = {Citeseer},
	author = {Norouzi, Mohammad and Blei, David M},
	year = {2011},
	keywords = {Untagged},
	pages = {353--360},
}

@inproceedings{plotzFeatureLearningActivity2011,
	address = {Barcelona, Catalonia, Spain},
	title = {Feature {Learning} for {Activity} {Recognition} in {Ubiquitous} {Computing}},
	url = {https://doi.org/10.5591/978-1-57735-516-8/IJCAI11-290},
	doi = {10.5591/978-1-57735-516-8/IJCAI11-290},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {IJCAI/AAAI},
	author = {Plötz, Thomas and Hammerla, Nils Y. and Olivier, Patrick},
	editor = {Walsh, Toby},
	year = {2011},
	keywords = {Untagged},
	pages = {1729--1734},
}

@article{shalev-shwartzOnlineLearningOnline2011,
	title = {Online {Learning} and {Online} {Convex} {Optimization}},
	volume = {4},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com/article/Details/MAL-018},
	doi = {10/gc7rf4},
	language = {en},
	number = {2},
	urldate = {2021-01-27},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Shalev-Shwartz, Shai},
	year = {2011},
	keywords = {Untagged},
	pages = {107--194},
}

@incollection{vankasterenHumanActivityRecognition2011,
	address = {Paris},
	title = {Human {Activity} {Recognition} from {Wireless} {Sensor} {Network} {Data}: {Benchmark} and {Software}},
	volume = {4},
	isbn = {978-90-78677-42-0 978-94-91216-05-3},
	shorttitle = {Dataset: {VanKasteren} benchmark},
	url = {http://link.springer.com/10.2991/978-94-91216-05-3_8},
	abstract = {Although activity recognition is an active area of research no common benchmark for evaluating the performance of activity recognition methods exists. In this chapter we present the state of the art probabilistic models used in activity recognition and show their performance on several real world datasets. Our results can be used as a baseline for comparing the performance of other pattern recognition methods (both probabilistic and non-probabilistic). The datasets used in this chapter are made public, together with the source code of the probabilistic models used.},
	language = {en},
	urldate = {2021-07-02},
	booktitle = {Activity {Recognition} in {Pervasive} {Intelligent} {Environments}},
	publisher = {Atlantis Press},
	author = {van Kasteren, T. L. M. and Englebienne, G. and Kröse, B. J. A.},
	editor = {Chen, Liming and Nugent, Chris D. and Biswas, Jit and Hoey, Jesse},
	year = {2011},
	doi = {10.2991/978-94-91216-05-3_8},
	note = {Series Title: Atlantis Ambient and Pervasive Intelligence},
	keywords = {Untagged},
	pages = {165--186},
}

@techreport{wahCaltechUCSDBirds2002011Dataset2011,
	title = {The {Caltech}-{UCSD} {Birds}-200-2011 {Dataset}},
	number = {CNS-TR-2011-001},
	institution = {California Institute of Technology},
	author = {Wah, Catherine and Branson, Steve and Welinder, Peter and Pietro Perona, Serge Belongie},
	year = {2011},
	keywords = {Untagged},
}

@inproceedings{zhaoCrossPeopleMobilePhoneBased2011,
	address = {Barcelona, Catalonia, Spain},
	title = {Cross-{People} {Mobile}-{Phone} {Based} {Activity} {Recognition}},
	url = {https://doi.org/10.5591/978-1-57735-516-8/IJCAI11-423},
	doi = {10.5591/978-1-57735-516-8/IJCAI11-423},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {IJCAI/AAAI},
	author = {Zhao, Zhongtang and Chen, Yiqiang and Liu, Junfa and Shen, Zhiqi and Liu, Mingjie},
	editor = {Walsh, Toby},
	year = {2011},
	keywords = {Untagged},
	pages = {2545--2550},
}

@inproceedings{zhengUserDependentAspectModel2011,
	address = {Barcelona, Catalonia, Spain},
	title = {User-{Dependent} {Aspect} {Model} for {Collaborative} {Activity} {Recognition}},
	url = {https://doi.org/10.5591/978-1-57735-516-8/IJCAI11-348},
	doi = {10.5591/978-1-57735-516-8/IJCAI11-348},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {IJCAI/AAAI},
	author = {Zheng, Vincent Wenchen and Yang, Qiang},
	editor = {Walsh, Toby},
	year = {2011},
	keywords = {Untagged},
	pages = {2085--2090},
}

@inproceedings{zontakInternalStatisticsSingle2011,
	address = {Colorado Springs, CO, USA},
	title = {Internal statistics of a single natural image},
	isbn = {978-1-4577-0394-2},
	url = {http://ieeexplore.ieee.org/document/5995401/},
	doi = {10.1109/CVPR.2011.5995401},
	abstract = {Statistics of ‘natural images’ provides useful priors for solving under-constrained problems in Computer Vision. Such statistics is usually obtained from large collections of natural images. We claim that the substantial internal data redundancy within a single natural image (e.g., recurrence of small image patches), gives rise to powerful internal statistics, obtained directly from the image itself. While internal patch recurrence has been used in various applications, we provide a parametric quantiﬁcation of this property. We show that the likelihood of an image patch to recur at another image location can be expressed parametricly as a function of the spatial distance from the patch, and its gradient content. This “internal parametric prior” is used to improve existing algorithms that rely on patch recurrence. Moreover, we show that internal image-speciﬁc statistics is often more powerful than general external statistics, giving rise to more powerful image-speciﬁc priors. In particular: (i) Patches tend to recur much more frequently (densely) inside the same image, than in any random external collection of natural images. (ii) To ﬁnd an equally good external representative patch for all the patches of an image, requires an external database of hundreds of natural images. (iii) Internal statistics often has stronger predictive power than external statistics, indicating that it may potentially give rise to more powerful image-speciﬁc priors.},
	language = {en},
	urldate = {2021-03-18},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zontak, Maria and Irani, Michal},
	year = {2011},
	keywords = {Untagged},
	pages = {977--984},
}

@inproceedings{abdullahPopulationScaleActivity2012,
	address = {Toronto, Ontario, Canada},
	title = {Towards {Population} {Scale} {Activity} {Recognition}: {A} {Framework} for {Handling} {Data} {Diversity}},
	url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5169},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Abdullah, Saeed and Lane, Nicholas D. and Choudhury, Tanzeem},
	editor = {Hoffmann, Jörg and Selman, Bart},
	year = {2012},
	keywords = {Untagged},
}

@inproceedings{chuNewHeatmapbasedAlgorithm2012,
	address = {Nara, Japan},
	title = {A new heat-map-based algorithm for human group activity recognition},
	url = {https://doi.org/10.1145/2393347.2396385},
	doi = {10.1145/2393347.2396385},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Chu, Hang and Lin, Weiyao and Wu, Jianxin and Zhou, Xingtong and Chen, Yuanzhe and Li, Hongxiang},
	editor = {Babaguchi, Noboru and Aizawa, Kiyoharu and Smith, John R. and Satoh, Shin'ichi and Plagemann, Thomas and Hua, Xian-Sheng and Yan, Rong},
	year = {2012},
	keywords = {Untagged},
	pages = {1069--1072},
}

@inproceedings{deanLargeScaleDistributed2012,
	address = {Lake Tahoe, Nevada},
	title = {Large {Scale} {Distributed} {Deep} {Networks}},
	url = {https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V. and Mao, Mark Z. and Ranzato, Marc'Aurelio and Senior, Andrew W. and Tucker, Paul A. and Yang, Ke and Ng, Andrew Y.},
	editor = {Bartlett, Peter L. and Pereira, Fernando C. N. and Burges, Christopher J. C. and Bottou, Léon and Weinberger, Kilian Q.},
	year = {2012},
	keywords = {Untagged},
	pages = {1232--1240},
}

@article{desideri_multiple-gradient_2012,
	title = {Multiple-gradient descent algorithm ({MGDA}) for multiobjective optimization},
	volume = {350},
	issn = {1631-073X},
	shorttitle = {{MGDA}},
	url = {https://www.sciencedirect.com/science/article/pii/S1631073X12000738},
	doi = {10.1016/j.crma.2012.03.014},
	abstract = {One considers the context of the concurrent optimization of several criteria Ji(Y) (i=1,…,n), supposed to be smooth functions of the design vector Y∈RN (n⩽N). An original constructive solution is given to the problem of identifying a descent direction common to all criteria when the current design-point Y0 is not Pareto-optimal. This leads us to generalize the classical steepest-descent method to the multiobjective context by utilizing this direction for the descent. The algorithm is then proved to converge to a Pareto-stationary design-point.
Résumé
On se place dans le contexte de lʼoptimisation concourante de plusieurs critères Ji(Y) (i=1,…,n), fonctions régulières du vecteur de conception Y∈RN (n⩽N). On donne une solution constructive originale au problème de lʼidentification dʼune direction de descente commune à tous les critères en un point Y0 non optimal au sens de Pareto. On est conduit à généraliser la méthode classique du gradient au contexte multiobjectif en utilisant cette direction pour la descente. On prouve que lʼalgorithme converge alors vers un point de conception Pareto-stationnaire.},
	language = {en},
	number = {5},
	urldate = {2022-06-25},
	journal = {Comptes Rendus Mathematique},
	author = {Désidéri, Jean-Antoine},
	month = mar,
	year = {2012},
	keywords = {Untagged},
	pages = {313--318},
}

@inproceedings{duanDiscoveringLocalizedAttributes2012,
	title = {Discovering localized attributes for fine-grained recognition},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Duan, K. and Parikh, D. and Crandall, D. and Grauman, K.},
	year = {2012},
	keywords = {Untagged},
	pages = {3474--3481},
}

@article{grettonKernelTwoSampleTest2012,
	title = {A {Kernel} {Two}-{Sample} {Test}},
	volume = {13},
	issn = {1533-7928},
	shorttitle = {{MMD}},
	url = {http://jmlr.org/papers/v13/gretton12a.html},
	abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD). We present two distribution-free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
	number = {25},
	urldate = {2022-05-15},
	journal = {Journal of Machine Learning Research},
	author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
	year = {2012},
	keywords = {Untagged},
	pages = {723--773},
}

@article{gupta_financial_2012,
	title = {Financial {Statement} {Fraud} {Detection} using {Text} {Mining}},
	volume = {3},
	issn = {2156-5570},
	url = {https://thesai.org/Publications/ViewPaper?Volume=3&Issue=12&Code=IJACSA&SerialNo=30},
	doi = {10.14569/IJACSA.2012.031230},
	abstract = {Data mining techniques have been used enormously by the researchers’ community in detecting financial statement fraud. Most of the research in this direction has used the numbers (quantitative information) i.e. financial ratios present in the financial statements for detecting fraud. There is very little or no research on the analysis of text such as auditor’s comments or notes present in published reports. In this study we propose a text mining approach for detecting financial statement fraud by analyzing the hidden clues in the qualitative information (text) present in financial statements.},
	language = {en},
	number = {12},
	urldate = {2022-11-07},
	journal = {International Journal of Advanced Computer Science and Applications (IJACSA)},
	author = {Gupta, Rajan and Gill, Nasib Singh},
	year = {2012},
	note = {Number: 12
Publisher: The Science and Information (SAI) Organization Limited},
	keywords = {Untagged},
}

@inproceedings{hazanProjectionfreeOnlineLearning2012,
	title = {Projection-free {Online} {Learning}},
	url = {http://icml.cc/2012/papers/292.pdf},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Machine} {Learning}, {ICML} 2012, {Edinburgh}, {Scotland}, {UK}, {June} 26 - {July} 1, 2012},
	publisher = {icml.cc / Omnipress},
	author = {Hazan, Elad and Kale, Satyen},
	year = {2012},
	keywords = {Untagged},
}

@inproceedings{jainDifferentiallyPrivateOnline2012,
	address = {Edinburgh, Scotland},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Differentially {Private} {Online} {Learning}},
	volume = {23},
	url = {http://proceedings.mlr.press/v23/jain12.html},
	abstract = {In this paper, we consider the problem of preserving privacy in the context of online learning. Online learning involves learning from data in real-time, due to which the learned model as well as its predictions are continuously changing. This makes preserving privacy of each data point significantly more challenging as its effect on the learned model can be easily tracked by observing changes in the subsequent predictions. Furthermore, with more and more online systems (e.g. search engines like Bing, Google etc.) trying to learn their customers’ behavior by leveraging their access to sensitive customer data (through cookies etc.), the problem of privacy preserving online learning has become critical. We study the problem in the framework of online convex programming (OCP) – a popular online learning setting with several theoretical and practical implications – while using differential privacy as the formal measure of privacy. For this problem, we provide a generic framework that can be used to convert any given OCP algorithm into a private OCP algorithm with provable privacy as well as regret guarantees (utility), provided that the given OCP algorithm satisfies the following two criteria: 1) linearly decreasing sensitivity, i.e., the effect of the new data points on the learned model decreases linearly, 2) sub-linear regret. We then illustrate our approach by converting two popular OCP algorithms into corresponding differentially private algorithms while guaranteeing {\textbackslash}emphÕ(√T) regret for strongly convex functions. Next, we consider the practically important class of online linear regression problems, for which we generalize the approach by Dwork et al. (2010a) to provide a differentially private algorithm with just poly-log regret. Finally, we show that our online learning framework can be used to provide differentially private algorithms for the offline learning problem as well. For the offline learning problem, our approach guarantees {\textbackslash}emphbetter error bounds and is more practical than the existing state-of-the-art methods (Chaudhuri et al., 2011; Rubinstein et al., 2009).},
	booktitle = {Annual {Conference} on {Learning} {Theory}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Jain, Prateek and Kothari, Pravesh and Thakurta, Abhradeep},
	editor = {Mannor, Shie and Srebro, Nathan and Williamson, Robert C.},
	year = {2012},
	keywords = {Untagged},
	pages = {24.1--24.34},
}

@inproceedings{krizhevskyImagenetClassificationDeep2012,
	title = {Imagenet classification with deep convolutional neural networks},
	booktitle = {{NeurIPS}},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
	keywords = {Untagged},
	pages = {1097--1105},
}

@article{lacoste-julienSimplerApproachObtaining2012,
	title = {A simpler approach to obtaining an {O}(1/t) convergence rate for the projected stochastic subgradient method},
	url = {http://arxiv.org/abs/1212.2002},
	abstract = {In this note, we present a new averaging technique for the projected stochastic subgradient method. By using a weighted average with a weight of t + 1 for each iterate wt at iteration t, we obtain the convergence rate of O(1/t) with both an easy proof and an easy implementation. The new scheme is compared empirically to existing techniques, with similar performance behavior.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1212.2002 [cs, math, stat]},
	author = {Lacoste-Julien, Simon and Schmidt, Mark and Bach, Francis},
	month = dec,
	year = {2012},
	note = {arXiv: 1212.2002},
	keywords = {Untagged},
}

@inproceedings{linActionRecognitionHumanmarionette2012,
	address = {Nara, Japan},
	title = {Action recognition for human-marionette interaction},
	url = {https://doi.org/10.1145/2393347.2393360},
	doi = {10.1145/2393347.2393360},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Lin, Shih-Yao and Shie, Chuen-Kai and Chen, Shen-Chi and Hung, Yi-Ping},
	editor = {Babaguchi, Noboru and Aizawa, Kiyoharu and Smith, John R. and Satoh, Shin'ichi and Plagemann, Thomas and Hua, Xian-Sheng and Yan, Rong},
	year = {2012},
	keywords = {Untagged},
	pages = {39--48},
}

@inproceedings{linHumanActionRecognition2012,
	address = {Nara, Japan},
	title = {Human action recognition and retrieval using sole depth information},
	url = {https://doi.org/10.1145/2393347.2396381},
	doi = {10.1145/2393347.2396381},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Lin, Yan-Ching and Hu, Min-Chun and Cheng, Wen-Huang and Hsieh, Yung-Huan and Chen, Hong-Ming},
	editor = {Babaguchi, Noboru and Aizawa, Kiyoharu and Smith, John R. and Satoh, Shin'ichi and Plagemann, Thomas and Hua, Xian-Sheng and Yan, Rong},
	year = {2012},
	keywords = {Untagged},
	pages = {1053--1056},
}

@inproceedings{norouziHammingDistanceMetric2012,
	title = {Hamming {Distance} {Metric} {Learning}},
	url = {https://proceedings.neurips.cc/paper/2012/hash/59b90e1005a220e2ebc542eb9d950b1e-Abstract.html},
	booktitle = {{NIPS}},
	author = {Norouzi, Mohammad and Fleet, David J. and Salakhutdinov, Ruslan},
	editor = {Bartlett, Peter L. and Pereira, Fernando C. N. and Burges, Christopher J. C. and Bottou, Léon and Weinberger, Kilian Q.},
	year = {2012},
	keywords = {Untagged},
	pages = {1070--1078},
}

@article{rubinsteinLearningLargeFunction2012,
	title = {Learning in a {Large} {Function} {Space}: {Privacy}-{Preserving} {Mechanisms} for {SVM} {Learning}},
	volume = {4},
	url = {https://journalprivacyconfidentiality.org/index.php/jpc/article/view/612},
	doi = {10/ghqpgg},
	number = {1},
	journal = {Journal of Privacy and Confidentiality},
	author = {Rubinstein, Benjamin I. P. and Bartlett, Peter L. and Huang, Ling and Taft, Nina},
	month = jul,
	year = {2012},
	keywords = {Untagged},
}

@article{schnitzer_local_2012,
	title = {Local and {Global} {Scaling} {Reduce} {Hubs} in {Space}},
	volume = {13},
	url = {http://jmlr.org/papers/v13/schnitzer12a.html},
	number = {92},
	journal = {Journal of Machine Learning Research},
	author = {Schnitzer, Dominik and Flexer, Arthur and Schedl, Markus and Widmer, Gerhard},
	year = {2012},
	keywords = {Untagged},
	pages = {2871--2902},
}

@book{taoTopicsRandomMatrix2012,
	title = {Topics in random matrix theory},
	volume = {132},
	publisher = {American Mathematical Soc.},
	author = {Tao, Terence},
	year = {2012},
	keywords = {Untagged},
}

@article{tielemanLecture5rmspropDivide2012,
	title = {Lecture 6.5-rmsprop: {Divide} the gradient by a running average of its recent magnitude},
	volume = {4},
	number = {2},
	journal = {COURSERA: Neural networks for machine learning},
	author = {Tieleman, Tijmen and Hinton, Geoffrey},
	year = {2012},
	keywords = {Untagged},
	pages = {26--31},
}

@inproceedings{weiliuSupervisedHashingKernels2012,
	address = {Providence, RI},
	title = {Supervised hashing with kernels},
	isbn = {978-1-4673-1228-8 978-1-4673-1226-4 978-1-4673-1227-1},
	url = {http://ieeexplore.ieee.org/document/6247912/},
	doi = {10/ghqpgq},
	abstract = {Recent years have witnessed the growing popularity of hashing in large-scale vision problems. It has been shown that the hashing quality could be boosted by leveraging supervised information into hash function learning. However, the existing supervised methods either lack adequate performance or often incur cumbersome model training. In this paper, we propose a novel kernel-based supervised hashing model which requires a limited amount of supervised information, i.e., similar and dissimilar data pairs, and a feasible training cost in achieving high quality hashing. The idea is to map the data to compact binary codes whose Hamming distances are minimized on similar pairs and simultaneously maximized on dissimilar pairs. Our approach is distinct from prior works by utilizing the equivalence between optimizing the code inner products and the Hamming distances. This enables us to sequentially and efﬁciently train the hash functions one bit at a time, yielding very short yet discriminative codes. We carry out extensive experiments on two image benchmarks with up to one million samples, demonstrating that our approach signiﬁcantly outperforms the state-of-the-arts in searching both metric distance neighbors and semantically similar neighbors, with accuracy gains ranging from 13\% to 46\%.},
	language = {en},
	urldate = {2021-01-28},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {{Wei Liu} and {Jun Wang} and {Rongrong Ji} and {Yu-Gang Jiang} and {Shih-Fu Chang}},
	year = {2012},
	keywords = {Untagged},
	pages = {2074--2081},
}

@inproceedings{wuHybridGenerativediscriminativeRecognition2012,
	address = {Nara, Japan},
	title = {Hybrid generative-discriminative recognition of human action in {3D} joint space},
	url = {https://doi.org/10.1145/2393347.2396388},
	doi = {10.1145/2393347.2396388},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Wu, Zhe and Li, Xiong and Zhao, Xu and Liu, Yuncai},
	editor = {Babaguchi, Noboru and Aizawa, Kiyoharu and Smith, John R. and Satoh, Shin'ichi and Plagemann, Thomas and Hua, Xian-Sheng and Yan, Rong},
	year = {2012},
	keywords = {Untagged},
	pages = {1081--1084},
}

@inproceedings{yangUnsupervisedTemplateLearning2012,
	title = {Unsupervised template learning for fine-grained object recognition},
	booktitle = {{NeurIPS}},
	author = {Yang, Shulin and Bo, Liefeng and Wang, Jue and Shapiro, Linda},
	year = {2012},
	keywords = {Untagged},
	pages = {3122--3130},
}

@article{zeilerAdadeltaAdaptiveLearning2012,
	title = {Adadelta: an adaptive learning rate method},
	journal = {arXiv preprint arXiv:1212.5701},
	author = {Zeiler, Matthew D},
	year = {2012},
	keywords = {Untagged},
}

@article{zhangFunctionalMechanismRegression2012,
	title = {Functional {Mechanism}: {Regression} {Analysis} under {Differential} {Privacy}},
	volume = {5},
	url = {https://doi.org/10.14778/2350229.2350253},
	doi = {10/ghqpf3},
	number = {11},
	journal = {Proceedings of the VLDB Endowment},
	author = {Zhang, Jun and Zhang, Zhenjie and Xiao, Xiaokui and Yang, Yin and Winslett, Marianne},
	year = {2012},
	note = {Publisher: VLDB Endowment},
	keywords = {Untagged},
	pages = {1364--1375},
}

@inproceedings{DBLP:conf/icml/AndrewABL13,
	series = {{JMLR} workshop and conference proceedings},
	title = {Deep canonical correlation analysis},
	volume = {28},
	url = {http://proceedings.mlr.press/v28/andrew13.html},
	booktitle = {Proceedings of the 30th international conference on machine learning, {ICML} 2013, atlanta, {GA}, {USA}, 16-21 june 2013},
	publisher = {JMLR.org},
	author = {Andrew, Galen and Arora, Raman and Bilmes, Jeff A. and Livescu, Karen},
	year = {2013},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/icml/AndrewABL13.bib
tex.timestamp: Wed, 29 May 2019 08:41:45 +0200},
	keywords = {Untagged},
	pages = {1247--1255},
}

@inproceedings{Anguita2013APD,
	title = {A public domain dataset for human activity recognition using smartphones},
	booktitle = {{ESANN}},
	author = {Anguita, D. and Ghio, Alessandro and Oneto, L. and Parra, Xavier and Reyes-Ortiz, Jorge Luis},
	year = {2013},
	keywords = {Untagged},
}

@inproceedings{avgerinakisActivityDetectionRecognition2013,
	address = {Barcelona, Spain},
	title = {Activity detection and recognition of daily living events},
	url = {https://doi.org/10.1145/2505323.2505327},
	doi = {10.1145/2505323.2505327},
	booktitle = {{ACM} {International} {Conference} on {Multimedia} {Workshop}},
	publisher = {ACM},
	author = {Avgerinakis, Konstantinos and Briassouli, Alexia and Kompatsiaris, Ioannis},
	editor = {Briassouli, Alexia and Benois-Pineau, Jenny and Hauptmann, Alexander G.},
	year = {2013},
	keywords = {Untagged},
	pages = {3--10},
}

@inproceedings{aydayPersonalUseGenomic2013,
	title = {Personal use of the genomic data: privacy vs. storage cost},
	booktitle = {{GLOBECOM}},
	author = {Ayday, Erman and Raisaro, Jean Louis and Hubaux, Jean-Pierre},
	year = {2013},
	keywords = {Untagged},
	pages = {2723--2729},
}

@article{bengioEstimatingPropagatingGradients2013,
	title = {Estimating or {Propagating} {Gradients} {Through} {Stochastic} {Neurons} for {Conditional} {Computation}},
	url = {http://arxiv.org/abs/1308.3432},
	abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we “back-propagate” through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to ﬁrst order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of conditional computation, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1308.3432},
	author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
	month = aug,
	year = {2013},
	note = {arXiv: 1308.3432},
	keywords = {Untagged},
}

@inproceedings{bergPOOFPartBasedOnevs2013,
	address = {Portland, OR, USA},
	title = {{POOF}: {Part}-{Based} {One}-vs.-{One} {Features} for {Fine}-{Grained} {Categorization}, {Face} {Verification}, and {Attribute} {Estimation}},
	isbn = {978-0-7695-4989-7},
	shorttitle = {{POOF}},
	url = {http://ieeexplore.ieee.org/document/6618972/},
	doi = {10/ghv3t5},
	abstract = {From a set of images in a particular domain, labeled with part locations and class, we present a method to automatically learn a large and diverse set of highly discriminative intermediate features that we call Part-based One-vs-One Features (POOFs). Each of these features specializes in discrimination between two particular classes based on the appearance at a particular part. We demonstrate the particular usefulness of these features for ﬁne-grained visual categorization with new state-of-the-art results on bird species identiﬁcation using the Caltech UCSD Birds (CUB) dataset and parity with the best existing results in face veriﬁcation on the Labeled Faces in the Wild (LFW) dataset. Finally, we demonstrate the particular advantage of POOFs when training data is scarce.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Berg, Thomas and Belhumeur, Peter N.},
	year = {2013},
	keywords = {Untagged},
	pages = {955--962},
}

@inproceedings{chaiSymbioticSegmentationPart2013,
	title = {Symbiotic segmentation and part localization for fine-grained categorization},
	booktitle = {International {Conference} on {Computer} {Vision}},
	author = {Chai, Yuning and Lempitsky, Victor and Zisserman, Andrew},
	year = {2013},
	keywords = {Untagged},
	pages = {321--328},
}

@inproceedings{10.1007/978-3-642-39077-7_5,
	address = {Berlin, Heidelberg},
	title = {Broadening the scope of differential privacy using metrics},
	isbn = {978-3-642-39077-7},
	abstract = {Differential Privacy is one of the most prominent frameworks used to deal with disclosure prevention in statistical databases. It provides a formal privacy guarantee, ensuring that sensitive information relative to individuals cannot be easily inferred by disclosing answers to aggregate queries. If two databases are adjacent, i.e. differ only for an individual, then the query should not allow to tell them apart by more than a certain factor. This induces a bound also on the distinguishability of two generic databases, which is determined by their distance on the Hamming graph of the adjacency relation.},
	booktitle = {Privacy enhancing technologies},
	publisher = {Springer Berlin Heidelberg},
	author = {Chatzikokolakis, Konstantinos and Andrés, Miguel E. and Bordenabe, Nicolás Emilio and Palamidessi, Catuscia},
	editor = {De Cristofaro, Emiliano and Wright, Matthew},
	year = {2013},
	keywords = {Untagged},
	pages = {82--102},
}

@article{chaudhuriNearoptimalAlgorithmDifferentiallyprivate2013,
	title = {A near-optimal algorithm for differentially-private principal components},
	volume = {14},
	number = {1},
	journal = {Journal of Machine Learning Research},
	author = {Chaudhuri, Kamalika and Sarwate, Anand D and Sinha, Kaushik},
	year = {2013},
	note = {Publisher: JMLR. org},
	keywords = {Untagged},
	pages = {2905--2943},
}

@article{elhamifarSparseSubspaceClustering2013,
	title = {Sparse {Subspace} {Clustering}: {Algorithm}, {Theory}, and {Applications}},
	shorttitle = {Sparse {Subspace} {Clustering}},
	url = {http://arxiv.org/abs/1203.1005},
	abstract = {Many real-world problems deal with collections of high-dimensional data, such as images, videos, text and web documents, DNA microarray data, and more. Often, such high-dimensional data lie close to low-dimensional structures corresponding to several classes or categories to which the data belong. In this paper, we propose and study an algorithm, called Sparse Subspace Clustering (SSC), to cluster data points that lie in a union of low-dimensional subspaces. The key idea is that, among the inﬁnitely many possible representations of a data point in terms of other points, a sparse representation corresponds to selecting a few points from the same subspace. This motivates solving a sparse optimization program whose solution is used in a spectral clustering framework to infer the clustering of the data into subspaces. Since solving the sparse optimization program is in general NP-hard, we consider a convex relaxation and show that, under appropriate conditions on the arrangement of the subspaces and the distribution of the data, the proposed minimization program succeeds in recovering the desired sparse representations. The proposed algorithm is efﬁcient and can handle data points near the intersections of subspaces. Another key advantage of the proposed algorithm with respect to the state of the art is that it can deal directly with data nuisances, such as noise, sparse outlying entries, and missing entries, by incorporating the model of the data into the sparse optimization program. We demonstrate the effectiveness of the proposed algorithm through experiments on synthetic data as well as the two real-world problems of motion segmentation and face clustering.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1203.1005 [cs, math, stat]},
	author = {Elhamifar, Ehsan and Vidal, Rene},
	month = feb,
	year = {2013},
	note = {arXiv: 1203.1005},
	keywords = {Untagged},
}

@inproceedings{fang_unbiased_2013,
	address = {Sydney, Australia},
	title = {Unbiased {Metric} {Learning}: {On} the {Utilization} of {Multiple} {Datasets} and {Web} {Images} for {Softening} {Bias}},
	shorttitle = {{VLCS}},
	url = {https://doi.org/10.1109/ICCV.2013.208},
	doi = {10.1109/ICCV.2013.208},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE Computer Society},
	author = {Fang, Chen and Xu, Ye and Rockmore, Daniel N.},
	year = {2013},
	keywords = {Untagged},
	pages = {1657--1664},
}

@inproceedings{DBLP:conf/nips/FromeCSBDRM13,
	title = {{DeViSE}: {A} deep visual-semantic embedding model},
	url = {https://proceedings.neurips.cc/paper/2013/hash/7cce53cf90577442771720a370c3c723-Abstract.html},
	booktitle = {Advances in neural information processing systems 26: 27th annual conference on neural information processing systems 2013. {Proceedings} of a meeting held december 5-8, 2013, lake tahoe, nevada, united states},
	author = {Frome, Andrea and Corrado, Gregory S. and Shlens, Jonathon and Bengio, Samy and Dean, Jeffrey and Ranzato, Marc'Aurelio and Mikolov, Tomás},
	editor = {Burges, Christopher J. C. and Bottou, Léon and Ghahramani, Zoubin and Weinberger, Kilian Q.},
	year = {2013},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/nips/FromeCSBDRM13.bib
tex.timestamp: Thu, 21 Jan 2021 15:15:23 +0100},
	keywords = {Untagged},
	pages = {2121--2129},
}

@article{gongIterativeQuantizationProcrustean2013,
	title = {Iterative {Quantization}: {A} {Procrustean} {Approach} to {Learning} {Binary} {Codes} for {Large}-{Scale} {Image} {Retrieval}},
	volume = {35},
	issn = {0162-8828, 2160-9292},
	shorttitle = {{ITQ}},
	url = {http://ieeexplore.ieee.org/document/6296665/},
	doi = {10/f5gmfv},
	abstract = {This paper addresses the problem of learning similarity-preserving binary codes for efﬁcient similarity search in large-scale image collections. We formulate this problem in terms of ﬁnding a rotation of zero-centered data so as to minimize the quantization error of mapping this data to the vertices of a zero-centered binary hypercube, and propose a simple and efﬁcient alternating minimization algorithm to accomplish this task. This algorithm, dubbed iterative quantization (ITQ), has connections to multi-class spectral clustering and to the orthogonal Procrustes problem, and it can be used both with unsupervised data embeddings such as PCA and supervised embeddings such as canonical correlation analysis (CCA). The resulting binary codes signiﬁcantly outperform several other state-of-the-art methods. We also show that further performance improvements can result from transforming the data with a nonlinear kernel mapping prior to PCA or CCA. Finally, we demonstrate an application of ITQ to learning binary attributes or “classemes” on the ImageNet dataset.},
	language = {en},
	number = {12},
	urldate = {2021-01-28},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gong, Yunchao and Lazebnik, Svetlana and Gordo, Albert and Perronnin, Florent},
	month = dec,
	year = {2013},
	keywords = {Untagged},
	pages = {2916--2929},
}

@inproceedings{goodfellowMaxoutNetworks2013,
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {Maxout {Networks}},
	volume = {28},
	url = {http://proceedings.mlr.press/v28/goodfellow13.html},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}, {ICML} 2013, {Atlanta}, {GA}, {USA}, 16-21 {June} 2013},
	publisher = {JMLR.org},
	author = {Goodfellow, Ian J. and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron C. and Bengio, Yoshua},
	year = {2013},
	keywords = {Untagged},
	pages = {1319--1327},
}

@inproceedings{gowayyedHistogramOrientedDisplacements2013,
	address = {Beijing, China},
	title = {Histogram of {Oriented} {Displacements} ({HOD}): {Describing} {Trajectories} of {Human} {Joints} for {Action} {Recognition}},
	url = {http://www.aaai.org/ocs/index.php/IJCAI/IJCAI13/paper/view/6967},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {IJCAI/AAAI},
	author = {Gowayyed, Mohammad Abdelaziz and Torki, Marwan and Hussein, Mohamed Elsayed and El-Saban, Motaz},
	editor = {Rossi, Francesca},
	year = {2013},
	keywords = {Untagged},
	pages = {1351--1357},
}

@inproceedings{guhathakurtaNearlyOptimalAlgorithms2013,
	title = {({Nearly}) {Optimal} {Algorithms} for {Private} {Online} {Learning} in {Full}-information and {Bandit} {Settings}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/file/c850371fda6892fbfd1c5a5b457e5777-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Guha Thakurta, Abhradeep and Smith, Adam},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	keywords = {Untagged},
	pages = {2733--2741},
}

@inproceedings{haoHumanActionRecognition2013,
	address = {Barcelona, Catalonia, Spain},
	title = {Human action recognition by fast dense trajectories},
	url = {https://doi.org/10.1145/2502081.2508123},
	doi = {10.1145/2502081.2508123},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Hao, Zong Bo and Zhang, Qianni and Izquierdo, Ebroul and Sang, Nan},
	editor = {Jaimes, Alejandro and Sebe, Nicu and Boujemaa, Nozha and Gatica-Perez, Daniel and Shamma, David A. and Worring, Marcel and Zimmermann, Roger},
	year = {2013},
	keywords = {Untagged},
	pages = {377--380},
}

@inproceedings{husseinHumanActionRecognition2013,
	address = {Beijing, China},
	title = {Human {Action} {Recognition} {Using} a {Temporal} {Hierarchy} of {Covariance} {Descriptors} on {3D} {Joint} {Locations}},
	url = {http://www.aaai.org/ocs/index.php/IJCAI/IJCAI13/paper/view/6869},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {IJCAI/AAAI},
	author = {Hussein, Mohamed E. and Torki, Marwan and Gowayyed, Mohammad Abdelaziz and El-Saban, Motaz},
	editor = {Rossi, Francesca},
	year = {2013},
	keywords = {Untagged},
	pages = {2466--2472},
}

@inproceedings{iscenKnivesArePicked2013,
	address = {Barcelona, Catalonia, Spain},
	title = {Knives are picked before slices are cut: recognition through activity sequence analysis},
	url = {https://doi.org/10.1145/2506023.2506025},
	doi = {10.1145/2506023.2506025},
	booktitle = {{ACM} {International} {Conference} on {Multimedia} {Workshop}},
	publisher = {ACM},
	author = {Iscen, Ahmet and Duygulu, Pinar},
	editor = {Aizawa, Kiyoharu and Yamakata, Yoko and Funatomi, Takuya},
	year = {2013},
	keywords = {Untagged},
	pages = {3--8},
}

@inproceedings{krause3DObjectRepresentations2013,
	title = {{3D} object representations for fine-grained categorization},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision} {Workshops}},
	publisher = {IEEE Computer Society},
	author = {Krause, Jonathan and Stark, Michael and Deng, Jia and Fei-Fei, Li},
	year = {2013},
	keywords = {Untagged},
	pages = {554--561},
}

@inproceedings{kunzeReallifeActivityRecognition2013,
	address = {Beijing, China},
	title = {Real-life activity recognition: recognizing reading activities},
	url = {https://doi.org/10.1145/2516911.2516912},
	doi = {10.1145/2516911.2516912},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence} {Workshop}},
	publisher = {ACM},
	author = {Kunze, Kai},
	editor = {Bader, Sebastian and Schumann, Anika and Sigg, Stephan and Lécué, Freddy and Srivastava, Biplav and Nie, Zaiqing and Guttmann, Christian},
	year = {2013},
	keywords = {Untagged},
	pages = {3},
}

@inproceedings{liangLearningLatentSpatiotemporal2013,
	address = {Barcelona, Catalonia, Spain},
	title = {Learning latent spatio-temporal compositional model for human action recognition},
	url = {https://doi.org/10.1145/2502081.2502089},
	doi = {10.1145/2502081.2502089},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Liang, Xiaodan and Lin, Liang and Cao, Liangliang},
	editor = {Jaimes, Alejandro and Sebe, Nicu and Boujemaa, Nozha and Gatica-Perez, Daniel and Shamma, David A. and Worring, Marcel and Zimmermann, Roger},
	year = {2013},
	keywords = {Untagged},
	pages = {263--272},
}

@incollection{borgelt_hubness_2013,
	address = {Berlin, Heidelberg},
	title = {The {Hubness} {Phenomenon}: {Fact} or {Artifact}?},
	volume = {285},
	isbn = {978-3-642-30277-0 978-3-642-30278-7},
	shorttitle = {The {Hubness} {Phenomenon}},
	url = {https://link.springer.com/10.1007/978-3-642-30278-7_21},
	abstract = {The hubness phenomenon, as it was recently described, consists in the observation that for increasing dimensionality of a data set the distribution of the number of times a data point occurs among the k nearest neighbors of other data points becomes increasingly skewed to the right. As a consequence, so-called hubs emerge, that is, data points that appear in the lists of the k nearest neighbors of other data points much more often than others. In this paper we challenge the hypothesis that the hubness phenomenon is an eﬀect of the dimensionality of the data set and provide evidence that it is rather a boundary eﬀect or, more generally, an eﬀect of a density gradient. As such, it may be seen as an artifact that results from the process in which the data is generated that is used to demonstrate this phenomenon. We report experiments showing that the hubness phenomenon need not occur in high-dimensional data and can be made to occur in low-dimensional data.},
	language = {en},
	urldate = {2023-02-19},
	booktitle = {Towards {Advanced} {Data} {Analysis} by {Combining} {Soft} {Computing} and {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Low, Thomas and Borgelt, Christian and Stober, Sebastian and Nürnberger, Andreas},
	editor = {Borgelt, Christian and Gil, María Ángeles and Sousa, João M.C. and Verleysen, Michel},
	year = {2013},
	doi = {10.1007/978-3-642-30278-7_21},
	note = {Series Title: Studies in Fuzziness and Soft Computing},
	keywords = {Untagged},
	pages = {267--278},
}

@techreport{majiFineGrainedVisualClassification2013,
	title = {Fine-{Grained} {Visual} {Classification} of {Aircraft}},
	author = {Maji, Subhransu and Rahtu, Esa and Kannala, Juho and Blaschko, Matthew and Vedaldi, Andrea},
	year = {2013},
	note = {\_eprint: 1306.5151},
	keywords = {Untagged},
}

@inproceedings{DBLP:conf/acl/MoranLO13,
	title = {Variable bit quantisation for {LSH}},
	url = {https://aclanthology.org/P13-2132/},
	booktitle = {Proceedings of the 51st annual meeting of the association for computational linguistics, {ACL} 2013, 4-9 august 2013, sofia, bulgaria, volume 2: {Short} papers},
	publisher = {The Association for Computer Linguistics},
	author = {Moran, Sean and Lavrenko, Victor and Osborne, Miles},
	year = {2013},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/acl/MoranLO13.bib
tex.timestamp: Fri, 06 Aug 2021 00:41:02 +0200},
	keywords = {Untagged},
	pages = {753--758},
}

@article{nesterovGradientMethodsMinimizing2013,
	title = {Gradient methods for minimizing composite functions},
	volume = {140},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-012-0629-5},
	doi = {10.1007/s10107-012-0629-5},
	abstract = {In this paper we analyze several new methods for solving optimization problems with the objective function formed as a sum of two terms: one is smooth and given by a black-box oracle, and another is a simple general convex function with known structure. Despite the absence of good properties of the sum, such problems, both in convex and nonconvex cases, can be solved with efficiency typical for the first part of the objective. For convex problems of the above structure, we consider primal and dual variants of the gradient method (with convergence rate \$\$O{\textbackslash}left(\{1 {\textbackslash}over k\}{\textbackslash}right)\$\$), and an accelerated multistep version with convergence rate \$\$O{\textbackslash}left(\{1 {\textbackslash}over k{\textasciicircum}2\}{\textbackslash}right)\$\$, where \$\$k\$\$is the iteration counter. For nonconvex problems with this structure, we prove convergence to a point from which there is no descent direction. In contrast, we show that for general nonsmooth, nonconvex problems, even resolving the question of whether a descent direction exists from a point is NP-hard. For all methods, we suggest some efficient “line search” procedures and show that the additional computational work necessary for estimating the unknown problem class parameters can only multiply the complexity of each iteration by a small constant factor. We present also the results of preliminary computational experiments, which confirm the superiority of the accelerated scheme.},
	number = {1},
	journal = {Mathematical Programming},
	author = {Nesterov, Yu.},
	year = {2013},
	keywords = {Untagged},
	pages = {125--161},
}

@article{sarwateSignalProcessingMachine2013,
	title = {Signal processing and machine learning with differential privacy: {Algorithms} and challenges for continuous data},
	volume = {30},
	doi = {10/ghnhfb},
	number = {5},
	journal = {IEEE signal processing magazine},
	author = {Sarwate, Anand D and Chaudhuri, Kamalika},
	year = {2013},
	note = {Publisher: IEEE},
	keywords = {Untagged},
	pages = {86--94},
}

@techreport{songStochasticGradientDescent2013a,
	title = {Stochastic {Gradient} {Descent} with {Differential} {Privacy}},
	language = {en},
	author = {Song, Shuang and Chaudhuri, Kamalika and Sarwate, Anand D},
	month = dec,
	year = {2013},
	keywords = {Untagged},
	pages = {44},
}

@inproceedings{songStochasticGradientDescent2013,
	address = {Austin, TX, USA},
	title = {Stochastic gradient descent with differentially private updates},
	isbn = {978-1-4799-0248-4},
	url = {http://ieeexplore.ieee.org/document/6736861/},
	doi = {10/ghv3nv},
	abstract = {Differential privacy is a recent framework for computation on sensitive data, which has shown considerable promise in the regime of large datasets. Stochastic gradient methods are a popular approach for learning in the data-rich regime because they are computationally tractable and scalable. In this paper, we derive differentially private versions of stochastic gradient descent, and test them empirically. Our results show that standard SGD experiences high variability due to differential privacy, but a moderate increase in the batch size can improve performance signiﬁcantly.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Global} {Conference} on {Signal} and {Information} {Processing}},
	publisher = {IEEE},
	author = {Song, Shuang and Chaudhuri, Kamalika and Sarwate, Anand D.},
	year = {2013},
	keywords = {Untagged},
	pages = {245--248},
}

@inproceedings{teschExpensiveFunctionOptimization2013,
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {Expensive {Function} {Optimization} with {Stochastic} {Binary} {Outcomes}},
	volume = {28},
	url = {http://proceedings.mlr.press/v28/tesch13.html},
	urldate = {2022-02-25},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}, {ICML} 2013, {Atlanta}, {GA}, {USA}, 16-21 {June} 2013},
	publisher = {JMLR.org},
	author = {Tesch, Matthew and Schneider, Jeff G. and Choset, Howie},
	year = {2013},
	keywords = {Untagged},
	pages = {1283--1291},
}

@inproceedings{wangLearningHashCodes2013,
	address = {Sydney, Australia},
	title = {Learning {Hash} {Codes} with {Listwise} {Supervision}},
	isbn = {978-1-4799-2840-8},
	url = {http://ieeexplore.ieee.org/document/6751488/},
	doi = {10/ghwfw9},
	abstract = {Hashing techniques have been intensively investigated in the design of highly efﬁcient search engines for largescale computer vision applications. Compared with prior approximate nearest neighbor search approaches like treebased indexing, hashing-based search schemes have prominent advantages in terms of both storage and computational efﬁciencies. Moreover, the procedure of devising hash functions can be easily incorporated into sophisticated machine learning tools, leading to data-dependent and task-speciﬁc compact hash codes. Therefore, a number of learning paradigms, ranging from unsupervised to supervised, have been applied to compose appropriate hash functions. However, most of the existing hash function learning methods either treat hash function design as a classiﬁcation problem or generate binary codes to satisfy pairwise supervision, and have not yet directly optimized the search accuracy. In this paper, we propose to leverage listwise supervision into a principled hash function learning framework. In particular, the ranking information is represented by a set of rank triplets that can be used to assess the quality of ranking. Simple linear projection-based hash functions are solved efﬁciently through maximizing the ranking quality over the training data. We carry out experiments on large image datasets with size up to one million and compare with the state-of-the-art hashing techniques. The extensive results corroborate that our learned hash codes via listwise supervision can provide superior search accuracy without incurring heavy computational overhead.},
	language = {en},
	urldate = {2021-01-28},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Wang, Jun and Liu, Wei and Sun, Andy X. and Jiang, Yu-Gang},
	year = {2013},
	keywords = {Untagged},
	pages = {3032--3039},
}

@inproceedings{xieHierarchicalPartMatching2013,
	title = {Hierarchical part matching for fine-grained visual categorization},
	booktitle = {{ICCV}},
	author = {Xie, L. and Tian, Q. and Hong, R. and Yan, S. and Zhang, B.},
	year = {2013},
	keywords = {Untagged},
	pages = {1641--1648},
}

@inproceedings{zhangCounterexampleValidityUsing2013,
	title = {A counterexample for the validity of using nuclear norm as a convex surrogate of rank},
	booktitle = {Joint {European} {Conference} on {Machine} {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer},
	author = {Zhang, Hongyang and Lin, Zhouchen and Zhang, Chao},
	year = {2013},
	keywords = {Untagged},
	pages = {226--241},
}

@inproceedings{agarwalTamingMonsterFast2014,
	series = {{JMLR} workshop and conference proceedings},
	title = {Taming the monster: {A} fast and simple algorithm for contextual bandits},
	volume = {32},
	url = {http://proceedings.mlr.press/v32/agarwalb14.html},
	booktitle = {Proceedings of the 31th international conference on machine learning, {ICML} 2014, beijing, china, 21-26 june 2014},
	publisher = {JMLR.org},
	author = {Agarwal, Alekh and Hsu, Daniel J. and Kale, Satyen and Langford, John and Li, Lihong and Schapire, Robert E.},
	year = {2014},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/icml/AgarwalHKLLS14.bib
tex.timestamp: Wed, 29 May 2019 08:41:46 +0200},
	keywords = {Untagged},
	pages = {1638--1646},
}

@inproceedings{bossardFood101MiningDiscriminative2014,
	title = {Food-101 – {Mining} {Discriminative} {Components} with {Random} {Forests}},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},
	year = {2014},
	keywords = {Untagged},
	pages = {446--461},
}

@article{bransonBirdSpeciesCategorization2014,
	title = {Bird {Species} {Categorization} {Using} {Pose} {Normalized} {Deep} {Convolutional} {Nets}},
	volume = {abs/1406.2952},
	url = {http://arxiv.org/abs/1406.2952},
	journal = {CoRR},
	author = {Branson, Steve and Horn, Grant Van and Belongie, Serge J. and Perona, Pietro},
	year = {2014},
	note = {\_eprint: 1406.2952},
	keywords = {Untagged},
}

@inproceedings{chatfieldReturnDevilDetails2014,
	title = {Return of the {Devil} in the {Details}: {Delving} {Deep} into {Convolutional} {Nets}},
	shorttitle = {{CNNF}},
	doi = {10/gdz798},
	booktitle = {British {Machine} {Vision} {Conference}},
	author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	year = {2014},
	keywords = {Untagged},
}

@inproceedings{davisJointlyOptimizing3D2014,
	title = {Jointly optimizing {3D} model fitting and fine-grained classification},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Davis, Yen-Liang LinVlad I. MorariuWinston HsuLarry S.},
	year = {2014},
	keywords = {Untagged},
	pages = {466--480},
}

@article{duchiPrivacyAwareLearning2014,
	title = {Privacy aware learning},
	volume = {61},
	doi = {10/gf3q6t},
	number = {6},
	journal = {Journal of the ACM},
	author = {Duchi, John C and Jordan, Michael I and Wainwright, Martin J},
	year = {2014},
	note = {Publisher: ACM New York, NY, USA},
	keywords = {Untagged},
	pages = {1--57},
}

@article{dworkAlgorithmicFoundationsDifferential2014,
	title = {The algorithmic foundations of differential privacy},
	volume = {9},
	number = {3–4},
	journal = {Foundations and Trends® in Theoretical Computer Science},
	author = {Dwork, Cynthia and Roth, Aaron and {others}},
	year = {2014},
	note = {Publisher: Now Publishers, Inc.},
	keywords = {Untagged},
	pages = {211--407},
}

@article{ganinUnsupervisedDomainAdaptation2014,
	title = {Unsupervised domain adaptation by backpropagation},
	journal = {arXiv:1409.7495},
	author = {Ganin, Y. and Lempitsky, V.},
	year = {2014},
	keywords = {Untagged},
}

@inproceedings{ghifaryDomainAdaptiveNeural2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Domain {Adaptive} {Neural} {Networks} for {Object} {Recognition}},
	volume = {8862},
	url = {https://doi.org/10.1007/978-3-319-13560-1_76},
	doi = {10/ghwngv},
	booktitle = {{PRICAI} 2014: {Trends} in {Artificial} {Intelligence} - 13th {Pacific} {Rim} {International} {Conference} on {Artificial} {Intelligence}, {Gold} {Coast}, {QLD}, {Australia}, {December} 1-5, 2014. {Proceedings}},
	publisher = {Springer},
	author = {Ghifary, Muhammad and Kleijn, W. Bastiaan and Zhang, Mengjie},
	editor = {Pham, Duc Nghia and Park, Seong-Bae},
	year = {2014},
	keywords = {Untagged},
	pages = {898--904},
}

@inproceedings{girshickRichFeatureHierarchies2014,
	address = {Columbus, OH},
	title = {Rich {Feature} {Hierarchies} for {Accurate} {Object} {Detection} and {Semantic} {Segmentation}},
	url = {https://doi.org/10.1109/CVPR.2014.81},
	doi = {10/gc7rmd},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Girshick, Ross B. and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	year = {2014},
	keywords = {Untagged},
	pages = {580--587},
}

@inproceedings{DBLP:conf/eccv/GongWHHL14,
	series = {Lecture notes in computer science},
	title = {Improving image-sentence embeddings using large weakly annotated photo collections},
	volume = {8692},
	url = {https://doi.org/10.1007/978-3-319-10593-2_35},
	doi = {10.1007/978-3-319-10593-2\_35},
	booktitle = {Computer vision - {ECCV} 2014 - 13th european conference, zurich, switzerland, september 6-12, 2014, proceedings, part {IV}},
	publisher = {Springer},
	author = {Gong, Yunchao and Wang, Liwei and Hodosh, Micah and Hockenmaier, Julia and Lazebnik, Svetlana},
	editor = {Fleet, David J. and Pajdla, Tomás and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/eccv/GongWHHL14.bib
tex.timestamp: Tue, 14 May 2019 10:00:45 +0200},
	keywords = {Untagged},
	pages = {529--545},
}

@article{JMLR:v15:hazan14a,
	title = {Beyond the regret minimization barrier: {Optimal} algorithms for stochastic strongly-convex optimization},
	volume = {15},
	url = {http://jmlr.org/papers/v15/hazan14a.html},
	number = {71},
	journal = {Journal of Machine Learning Research},
	author = {Hazan, Elad and Kale, Satyen},
	year = {2014},
	keywords = {Untagged},
	pages = {2489--2512},
}

@inproceedings{jiaCaffeConvolutionalArchitecture2014,
	title = {Caffe: {Convolutional} architecture for fast feature embedding},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	author = {Jia, Y. and Shelhamer, E. and Donahue, J. and Karayev, S. and Long, J. and Girshick, R. and Guadarrama, S. and Darrell, T.},
	year = {2014},
	keywords = {Untagged},
	pages = {675--678},
}

@inproceedings{kananFinegrainedObjectRecognition2014,
	title = {Fine-grained object recognition with {Gnostic} {Fields}},
	doi = {10/ghqpfz},
	booktitle = {{WACV}},
	author = {Kanan, C.},
	year = {2014},
	note = {ISSN: 1550-5790},
	keywords = {Untagged},
	pages = {23--30},
}

@inproceedings{DBLP:conf/nips/KarpathyJL14,
	title = {Deep fragment embeddings for bidirectional image sentence mapping},
	url = {https://proceedings.neurips.cc/paper/2014/hash/84d2004bf28a2095230e8e14993d398d-Abstract.html},
	booktitle = {Advances in neural information processing systems 27: {Annual} conference on neural information processing systems 2014, december 8-13 2014, montreal, quebec, canada},
	author = {Karpathy, Andrej and Joulin, Armand and Fei-Fei, Li},
	editor = {Ghahramani, Zoubin and Welling, Max and Cortes, Corinna and Lawrence, Neil D. and Weinberger, Kilian Q.},
	year = {2014},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/nips/KarpathyJL14.bib
tex.timestamp: Wed, 15 Sep 2021 14:13:01 +0200},
	keywords = {Untagged},
	pages = {1889--1897},
}

@article{kellarisDifferentiallyPrivateEvent2014,
	title = {Differentially private event sequences over infinite streams},
	volume = {7},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/2732977.2732989},
	doi = {10/ghnhfj},
	abstract = {Numerous applications require continuous publication of statistics for monitoring purposes, such as real-time trafﬁc analysis, timely disease outbreak discovery, and social trends observation. These statistics may be derived from sensitive user data and, hence, necessitate privacy preservation. A notable paradigm for offering strong privacy guarantees in statistics publishing is ϵ-differential privacy. However, there is limited literature that adapts this concept to settings where the statistics are computed over an inﬁnite stream of “events” (i.e., data items generated by the users), and published periodically. These works aim at hiding a single event over the entire stream. We argue that, in most practical scenarios, sensitive information is revealed from multiple events occurring at contiguous time instances. Towards this end, we put forth the novel notion of w-event privacy over inﬁnite streams, which protects any event sequence occurring in w successive time instants. We ﬁrst formulate our privacy concept, motivate its importance, and introduce a methodology for achieving it. We next design two instantiations, whose utility is independent of the stream length. Finally, we conﬁrm the practicality of our solutions experimenting with real data.},
	language = {en},
	number = {12},
	urldate = {2021-01-27},
	journal = {Proceedings of the VLDB Endowment},
	author = {Kellaris, Georgios and Papadopoulos, Stavros and Xiao, Xiaokui and Papadias, Dimitris},
	month = aug,
	year = {2014},
	keywords = {Untagged},
	pages = {1155--1166},
}

@article{DBLP:journals/corr/KleinLSW14,
	title = {Fisher vectors derived from hybrid gaussian-laplacian mixture models for image annotation},
	volume = {abs/1411.7399},
	url = {http://arxiv.org/abs/1411.7399},
	journal = {CoRR},
	author = {Klein, Benjamin and Lev, Guy and Sadeh, Gil and Wolf, Lior},
	year = {2014},
	note = {arXiv: 1411.7399
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/corr/KleinLSW14.bib
tex.timestamp: Mon, 13 Aug 2018 16:46:23 +0200},
	keywords = {Untagged},
}

@article{linNetworkNetwork2014,
	title = {Network {In} {Network}},
	url = {http://arxiv.org/abs/1312.4400},
	abstract = {We propose a novel deep network structure called “Network In Network”(NIN) to enhance model discriminability for local patches within the receptive ﬁeld. The conventional convolutional layer uses linear ﬁlters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive ﬁeld. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classiﬁcation layer, which is easier to interpret and less prone to overﬁtting than traditional fully connected layers. We demonstrated the state-of-the-art classiﬁcation performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1312.4400 [cs]},
	author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
	month = mar,
	year = {2014},
	note = {arXiv: 1312.4400},
	keywords = {Untagged},
}

@article{linSampleComplexityRandom2014,
	title = {On the {Sample} {Complexity} of {Random} {Fourier} {Features} for {Online} {Learning}: {How} {Many} {Random} {Fourier} {Features} {Do} {We} {Need}?},
	volume = {8},
	issn = {1556-4681, 1556-472X},
	shorttitle = {On the {Sample} {Complexity} of {Random} {Fourier} {Features} for {Online} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/2611378},
	doi = {10/ghv3jx},
	language = {en},
	number = {3},
	urldate = {2021-01-27},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Lin, Ming and Weng, Shifeng and Zhang, Changshui},
	month = jun,
	year = {2014},
	keywords = {Untagged},
	pages = {1--19},
}

@inproceedings{linMicrosoftCOCOCommon2014,
	address = {Cham},
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	isbn = {978-3-319-10602-1},
	shorttitle = {Microsoft {COCO}},
	doi = {10.1007/978-3-319-10602-1_48},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {Untagged},
	pages = {740--755},
}

@inproceedings{linJointlyOptimizing3D2014,
	title = {Jointly optimizing {3D} model fitting and fine-grained classification},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Lin, Y.-L. and Morariu, V. I and Hsu, W. and Davis, L. S},
	year = {2014},
	keywords = {Untagged},
	pages = {466--480},
}

@article{mirzaConditionalGenerativeAdversarial2014,
	title = {Conditional {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1411.1784 [cs, stat]},
	author = {Mirza, Mehdi and Osindero, Simon},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.1784},
	keywords = {Untagged},
}

@inproceedings{penningtonGloVeGlobalVectors2014,
	address = {Doha, Qatar},
	title = {{GloVe}: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {{GloVe}},
	url = {https://aclanthology.org/D14-1162},
	doi = {10.3115/v1/D14-1162},
	urldate = {2022-03-01},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	keywords = {Untagged},
	pages = {1532--1543},
}

@article{pimentelReviewNoveltyDetection2014,
	title = {A review of novelty detection},
	volume = {99},
	issn = {01651684},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016516841300515X},
	doi = {10/f5wndg},
	abstract = {Novelty detection is the task of classifying test data that differ in some respect from the data that are available during training. This may be seen as “one-class classification”, in which a model is constructed to describe “normal” training data. The novelty detection approach is typically used when the quantity of available “abnormal” data is insufficient to construct explicit models for non-normal classes. Application includes inference in datasets from critical systems, where the quantity of available normal data is very large, such that “normality” may be accurately modelled. In this review we aim to provide an updated and structured investigation of novelty detection research papers that have appeared in the machine learning literature during the last decade.},
	language = {en},
	urldate = {2021-01-27},
	journal = {Signal Processing},
	author = {Pimentel, Marco A.F. and Clifton, David A. and Clifton, Lei and Tarassenko, Lionel},
	month = jun,
	year = {2014},
	keywords = {Untagged},
	pages = {215--249},
}

@book{shalev-shwartzUnderstandingMachineLearning2014,
	address = {Cambridge},
	title = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
	isbn = {978-1-107-29801-9},
	shorttitle = {Understanding {Machine} {Learning}},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781107298019},
	language = {en},
	urldate = {2022-05-07},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	year = {2014},
	doi = {10.1017/CBO9781107298019},
	keywords = {Untagged},
}

@article{shoaibFusionSmartphoneMotion2014,
	title = {Fusion of {Smartphone} {Motion} {Sensors} for {Physical} {Activity} {Recognition}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/14/6/10146},
	doi = {10.3390/s140610146},
	abstract = {For physical activity recognition, smartphone sensors, such as an accelerometer and a gyroscope, are being utilized in many research studies. So far, particularly, the accelerometer has been extensively studied. In a few recent studies, a combination of a gyroscope, a magnetometer (in a supporting role) and an accelerometer (in a lead role) has been used with the aim to improve the recognition performance. How and when are various motion sensors, which are available on a smartphone, best used for better recognition performance, either individually or in combination? This is yet to be explored. In order to investigate this question, in this paper, we explore how these various motion sensors behave in different situations in the activity recognition process. For this purpose, we designed a data collection experiment where ten participants performed seven different activities carrying smart phones at different positions. Based on the analysis of this data set, we show that these sensors, except the magnetometer, are each capable of taking the lead roles individually, depending on the type of activity being recognized, the body position, the used data features and the classification method employed (personalized or generalized). We also show that their combination only improves the overall recognition performance when their individual performances are not very high, so that there is room for performance improvement. We have made our data set and our data collection application publicly available, thereby making our experiments reproducible.},
	language = {en},
	number = {6},
	urldate = {2021-07-02},
	journal = {Sensors},
	author = {Shoaib, Muhammad and Bosch, Stephan and Incel, Ozlem Durmaz and Scholten, Hans and Havinga, Paul J. M.},
	month = jun,
	year = {2014},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Untagged},
	pages = {10146--10176},
}

@article{simonyanVeryDeepConvolutional2014,
	title = {Very deep convolutional networks for large-scale image recognition},
	shorttitle = {{VGG16}},
	journal = {arXiv:1409.1556},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2014},
	keywords = {Untagged},
}

@inproceedings{szegedy_intriguing_2014,
	address = {banff, AB, canada},
	title = {Intriguing properties of neural networks},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian J. and Fergus, Rob},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2014},
	keywords = {Untagged},
}

@article{tzengDeepDomainConfusion2014,
	title = {Deep {Domain} {Confusion}: {Maximizing} for {Domain} {Invariance}},
	volume = {abs/1412.3474},
	url = {http://arxiv.org/abs/1412.3474},
	journal = {CoRR},
	author = {Tzeng, Eric and Hoffman, Judy and Zhang, Ning and Saenko, Kate and Darrell, Trevor},
	year = {2014},
	note = {\_eprint: 1412.3474},
	keywords = {Untagged},
}

@inproceedings{vedaldiUnderstandingObjectsDetail2014,
	title = {Understanding objects in detail with fine-grained attributes},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Vedaldi, Andrea and Mahendran, Siddharth and Tsogkas, Stavros and Maji, Subhransu and Girshick, Ross and Kannala, Juho and Rahtu, Esa and Kokkinos, Iasonas and Blaschko, Matthew B. and Weiss, David and Taskar, Ben and Simonyan, Karen and Saphra, Naomi and Mohamed, Sammy},
	year = {2014},
	keywords = {Untagged},
	pages = {3622--3629},
}

@article{youngImageDescriptionsVisual2014,
	title = {From image descriptions to visual denotations: {New} similarity metrics for semantic inference over event descriptions},
	volume = {2},
	shorttitle = {From image descriptions to visual denotations},
	url = {https://aclanthology.org/Q14-1006},
	doi = {10.1162/tacl_a_00166},
	abstract = {We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.},
	urldate = {2022-03-16},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
	year = {2014},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	keywords = {Untagged},
	pages = {67--78},
}

@inproceedings{zeilerVisualizingUnderstandingConvolutional2014,
	address = {Zurich, Switzerland},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	volume = {8689},
	url = {https://doi.org/10.1007/978-3-319-10590-1_53},
	doi = {10/gfrkjf},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	editor = {Fleet, David J. and Pajdla, Tomás and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {Untagged},
	pages = {818--833},
}

@article{zhangRobustLatentLow2014,
	title = {Robust latent low rank representation for subspace clustering},
	volume = {145},
	doi = {10.1016/j.neucom.2014.05.022},
	journal = {Neurocomputing},
	author = {Zhang, Hongyang and Lin, Zhouchen and Zhang, Chao and Gao, Junbin},
	year = {2014},
	note = {Publisher: Elsevier},
	keywords = {Untagged},
	pages = {369--373},
}

@inproceedings{zhangPartbasedRCNNsFinegrained2014,
	title = {Part-based {R}-{CNNs} for fine-grained category detection},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Zhang, N. and Donahue, J. and Girshick, R. and Darrell, T.},
	year = {2014},
	keywords = {Untagged},
	pages = {834--849},
}

@inproceedings{zhouLearningDeepFeatures2014,
	title = {Learning deep features for scene recognition using places database},
	booktitle = {{NeurIPS}},
	author = {Zhou, B. and Lapedriza, A. and Xiao, J. and Torralba, A. and Oliva, A.},
	year = {2014},
	keywords = {Untagged},
	pages = {487--495},
}

@inproceedings{andoniOptimalDatadependentHashing2015,
	title = {Optimal data-dependent hashing for approximate near neighbors},
	doi = {10/ghqpgn},
	booktitle = {{STOC}},
	author = {Andoni, Alexandr and Razenshteyn, Ilya},
	year = {2015},
	keywords = {Untagged},
	pages = {793--801},
}

@article{blundellWeightUncertaintyNeural2015,
	title = {Weight {Uncertainty} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.05424},
	abstract = {We introduce a new, efﬁcient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classiﬁcation. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1505.05424 [cs, stat]},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = may,
	year = {2015},
	note = {arXiv: 1505.05424},
	keywords = {Untagged},
}

@inproceedings{caiAttributeMiningScalable2015,
	address = {Brisbane, Australia},
	title = {Attribute {Mining} for {Scalable} {3D} {Human} {Action} {Recognition}},
	url = {https://doi.org/10.1145/2733373.2806285},
	doi = {10.1145/2733373.2806285},
	booktitle = {Proceedings of the 23rd {Annual} {ACM} {Conference} on {Multimedia} {Conference}, {MM} '15, {Brisbane}, {Australia}, {October} 26 - 30, 2015},
	publisher = {ACM},
	author = {Cai, Xingyang and Zhou, Wengang and Li, Houqiang},
	editor = {Zhou, Xiaofang and Smeaton, Alan F. and Tian, Qi and Bulterman, Dick C. A. and Shen, Heng Tao and Mayer-Patel, Ketan and Yan, Shuicheng},
	year = {2015},
	keywords = {Untagged},
	pages = {1075--1078},
}

@inproceedings{chenSelectivePoolingVector2015,
	title = {Selective {Pooling} {Vector} for {Fine}-{Grained} {Recognition}},
	url = {https://doi.ieeecomputersociety.org/10.1109/WACV.2015.119},
	doi = {10/ghqpf2},
	booktitle = {{WACV}},
	author = {Chen, G. and Yang, J. and Jin, H. and Shechtman, E. and Brandt, J. and Han, T. X.},
	year = {2015},
	keywords = {Untagged},
	pages = {860--867},
}

@book{cholletKeras2015,
	title = {Keras},
	url = {https://keras.io},
	author = {Chollet, François and {others}},
	year = {2015},
	keywords = {Untagged},
}

@inproceedings{courbariauxBinaryConnectTrainingDeep2015,
	title = {{BinaryConnect}: {Training} {Deep} {Neural} {Networks} with binary weights during propagations},
	volume = {28},
	shorttitle = {{BinaryConnect}},
	url = {https://proceedings.neurips.cc/paper/2015/hash/3e15cc11f979ed25912dff5b0669f2cd-Abstract.html},
	urldate = {2022-02-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
	year = {2015},
	keywords = {Untagged},
}

@inproceedings{fredriksonModelInversionAttacks2015,
	address = {Denver Colorado USA},
	title = {Model {Inversion} {Attacks} that {Exploit} {Confidence} {Information} and {Basic} {Countermeasures}},
	isbn = {978-1-4503-3832-5},
	url = {https://dl.acm.org/doi/10.1145/2810103.2813677},
	doi = {10.1145/2810103.2813677},
	abstract = {Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classiﬁers in personalized medicine by Fredrikson et al. [13], adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown.},
	language = {en},
	urldate = {2022-03-14},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
	month = oct,
	year = {2015},
	keywords = {Untagged},
	pages = {1322--1333},
}

@inproceedings{freedmanTemporalObjectRelations2015,
	address = {Arlington, Virginia},
	title = {Temporal and {Object} {Relations} in {Unsupervised} {Plan} and {Activity} {Recognition}},
	url = {http://www.aaai.org/ocs/index.php/FSS/FSS15/paper/view/11694},
	booktitle = {{AAAI} {Fall} {Symposia}},
	publisher = {AAAI Press},
	author = {Freedman, Richard G. and Jung, Hee-Tae and Zilberstein, Shlomo},
	year = {2015},
	keywords = {Untagged},
	pages = {51--59},
}

@inproceedings{ghifary_domain_2015,
	address = {Santiago, Chile},
	title = {Domain {Generalization} for {Object} {Recognition} with {Multi}-task {Autoencoders}},
	isbn = {978-1-4673-8391-2},
	shorttitle = {{RotatedMNIST}},
	url = {http://ieeexplore.ieee.org/document/7410650/},
	doi = {10.1109/ICCV.2015.293},
	abstract = {The problem of domain generalization is to take knowledge acquired from a number of related domains where training data is available, and to then successfully apply it to previously unseen domains. We propose a new feature learning algorithm, Multi-Task Autoencoder (MTAE), that provides good generalization performance for crossdomain object recognition.},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Ghifary, Muhammad and Kleijn, W. Bastiaan and Zhang, Mengjie and Balduzzi, David},
	year = {2015},
	keywords = {Untagged},
	pages = {2551--2559},
}

@inproceedings{goodfellowExplainingHarnessingAdversarial2015,
	address = {San Diego, CA},
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2015},
	keywords = {Untagged},
}

@inproceedings{guptaDeepLearningLimited2015,
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {Deep {Learning} with {Limited} {Numerical} {Precision}},
	volume = {37},
	url = {http://proceedings.mlr.press/v37/gupta15.html},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}, {ICML} 2015, {Lille}, {France}, 6-11 {July} 2015},
	publisher = {JMLR.org},
	author = {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
	editor = {Bach, Francis R. and Blei, David M.},
	year = {2015},
	keywords = {Untagged},
	pages = {1737--1746},
}

@phdthesis{gustavssonTopicsConvexMixed2015,
	type = {{PhD}},
	title = {Topics in convex and mixed binary linear optimization},
	abstract = {This thesis concerns theory, algorithms, and applications for two problem classes within the realm of mathematical optimization; convex optimization and mixed binary linear optimization. To the thesis is appended ﬁve papers containing its main contributions.},
	language = {en},
	school = {Chalmers University of Technology and University of Gothenburg},
	author = {Gustavsson, Emil},
	year = {2015},
	keywords = {Untagged},
}

@inproceedings{hanLearningBothWeights2015,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'15},
	title = {Learning {Both} {Weights} and {Connections} for {Efficient} {Neural} {Networks}},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 1},
	publisher = {MIT Press},
	author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
	year = {2015},
	note = {event-place: Montreal, Canada},
	keywords = {Untagged},
	pages = {1135--1143},
}

@article{hastieMatrixCompletionLowrank2015,
	title = {Matrix completion and low-rank {SVD} via fast alternating least squares},
	volume = {16},
	url = {http://dl.acm.org/citation.cfm?id=2912106},
	journal = {J. Mach. Learn. Res.},
	author = {Hastie, Trevor and Mazumder, Rahul and Lee, Jason D. and Zadeh, Reza},
	year = {2015},
	keywords = {Untagged},
	pages = {3367--3402},
}

@inproceedings{10.5555/2969239.2969417,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'15},
	title = {Beyond convexity: {Stochastic} quasi-convex optimization},
	abstract = {Stochastic convex optimization is a basic and well studied primitive in machine learning. It is well known that convex and Lipschitz functions can be minimized efficiently using Stochastic Gradient Descent (SGD).The Normalized Gradient Descent (NGD) algorithm, is an adaptation of Gradient Descent, which updates according to the direction of the gradients, rather than the gradients themselves. In this paper we analyze a stochastic version of NGD and prove its convergence to a global minimum for a wider class of functions: we require the functions to be quasi-convex and locally-Lipschitz. Quasi-convexity broadens the concept of unimodality to multidimensions and allows for certain types of saddle points, which are a known hurdle for first-order optimization methods such as gradient descent. Locally-Lipschitz functions are only required to be Lipschitz in a small region around the optimum. This assumption circumvents gradient explosion, which is another known hurdle for gradient descent variants. Interestingly, unlike the vanilla SGD algorithm, the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size.},
	booktitle = {Proceedings of the 28th international conference on neural information processing systems - volume 1},
	publisher = {MIT Press},
	author = {Hazan, Elad and Levy, Kfir Y. and Shalev-Shwartz, Shai},
	year = {2015},
	note = {Number of pages: 9
Place: Montreal, Canada},
	keywords = {Untagged},
	pages = {1594--1602},
}

@article{heSpatialPyramidPooling2015,
	title = {Spatial {Pyramid} {Pooling} in {Deep} {Convolutional} {Networks} for {Visual} {Recognition}},
	volume = {37},
	url = {https://doi.org/10.1109/TPAMI.2015.2389824},
	doi = {10/f7mt3q},
	number = {9},
	journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
	keywords = {Untagged},
	pages = {1904--1916},
}

@inproceedings{hornBuildingBirdRecognition2015,
	title = {Building a bird recognition app and large scale dataset with citizen scientists: {The} fine print in fine-grained dataset collection},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Horn, Grant Van and Branson, Steve and Farrell, Ryan and Haber, Scott},
	year = {2015},
	keywords = {Untagged},
	pages = {595--604},
}

@inproceedings{jaderbergSpatialTransformerNetworks2015,
	title = {Spatial {Transformer} {Networks}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/file/33ceb07bf4eeb3da587e268d663aba1a-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and kavukcuoglu, koray},
	editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	keywords = {Untagged},
}

@inproceedings{jiangHumanActivityRecognition2015,
	address = {Brisbane, Australia},
	title = {Human {Activity} {Recognition} {Using} {Wearable} {Sensors} by {Deep} {Convolutional} {Neural} {Networks}},
	url = {https://doi.org/10.1145/2733373.2806333},
	doi = {10.1145/2733373.2806333},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Jiang, Wenchao and Yin, Zhaozheng},
	editor = {Zhou, Xiaofang and Smeaton, Alan F. and Tian, Qi and Bulterman, Dick C. A. and Shen, Heng Tao and Mayer-Patel, Ketan and Yan, Shuicheng},
	year = {2015},
	keywords = {Untagged},
	pages = {1307--1310},
}

@inproceedings{jinCollaboratingLocalGlobal2015,
	address = {Melbourne Australia},
	title = {Collaborating between {Local} and {Global} {Learning} for {Distributed} {Online} {Multiple} {Tasks}},
	isbn = {978-1-4503-3794-6},
	url = {https://dl.acm.org/doi/10.1145/2806416.2806553},
	doi = {10/ghv3hm},
	abstract = {This paper studies the novel learning scenarios of Distributed Online Multi-tasks (DOM), where the learning individuals with continuously arriving data are distributed separately and meanwhile they need to learn individual models collaboratively. It has three characteristics: distributed learning, online learning and multi-task learning. It is motivated by the emerging applications of wearable devices, which aim to provide intelligent monitoring services, such as health emergency alarming and movement recognition.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {Proceedings of the 24th {ACM} {International} on {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {ACM},
	author = {Jin, Xin and Luo, Ping and Zhuang, Fuzhen and He, Jia and He, Qing},
	year = {2015},
	keywords = {Untagged},
	pages = {113--122},
}

@inproceedings{kingmaAdamMethodStochastic2015,
	address = {San Diego, CA},
	title = {Adam: {A} method for stochastic optimization},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Kingma, Diederik P and Ba, Jimmy},
	year = {2015},
	keywords = {Untagged},
}

@inproceedings{krauseFinegrainedRecognitionPart2015,
	title = {Fine-grained recognition without part annotations},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Krause, J. and Jin, H. and Yang, J. and Fei-Fei, L.},
	year = {2015},
	keywords = {Untagged},
	pages = {5546--5555},
}

@inproceedings{laiSimultaneousFeatureLearning2015,
	title = {Simultaneous feature learning and hash coding with deep neural networks},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Lai, Hanjiang and Pan, Yan and Liu, Ye and Yan, Shuicheng},
	year = {2015},
	keywords = {Untagged},
	pages = {3270--3278},
}

@article{liDifferentiallyPrivateDistributed2015,
	title = {Differentially {Private} {Distributed} {Online} {Learning}},
	url = {http://arxiv.org/abs/1505.06556},
	abstract = {In this paper, we propose a novel distributed online learning algorithm to handle massive data in Big Data era. Comparing to the typical centralized scenario, our proposed distributed online learning has multi-learners. Each learner optimizes its own learning parameter based on local data source and communicates timely with neighbors. We study the regret of the distributed online learning algorithm. However, communications among the learners may lead to privacy breaches. Thus, we use differential privacy to preserve the privacy of the learners, and study the inﬂuence of guaranteeing differential privacy on the regret of the algorithm. Furthermore, our online learning algorithm can be used to achieve fast convergence rates for ofﬂine learning algorithms in distributed scenarios. We demonstrate that the differentially private ofﬂine learning algorithm has high variance, but we can use mini-batch to improve the performance. The simulations show that our proposed theorems are correct and our differentially private distributed online learning algorithm is a general framework.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1505.06556 [cs]},
	author = {Li, Chencheng and Zhou, Pan},
	month = jun,
	year = {2015},
	note = {arXiv: 1505.06556},
	keywords = {Untagged},
}

@techreport{liLearningHashBig2015,
	title = {Learning to {Hash} for {Big} {Data}},
	institution = {NJU},
	author = {Li, Wujun},
	year = {2015},
	keywords = {Untagged},
}

@inproceedings{linDeepLACDeep2015,
	address = {Boston, MA},
	title = {Deep {LAC}: {Deep} localization, alignment and classification for fine-grained recognition},
	isbn = {978-1-4673-6964-0},
	shorttitle = {Deep {LAC}},
	url = {http://ieeexplore.ieee.org/document/7298775/},
	doi = {10/gkzww3},
	abstract = {We propose a ﬁne-grained recognition system that incorporates part localization, alignment, and classiﬁcation in one deep neural network. This is a nontrivial process, as the input to the classiﬁcation module should be functions that enable back-propagation in constructing the solver. Our major contribution is to propose a valve linkage function (VLF) for back-propagation chaining and form our deep localization, alignment and classiﬁcation (LAC) system. The VLF can adaptively compromise the errors of classiﬁcation and alignment when training the LAC model. It in turn helps update localization. The performance on ﬁne-grained object data bears out the effectiveness of our LAC system.},
	language = {en},
	urldate = {2021-06-30},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Lin, Di and Shen, Xiaoyong and Lu, Cewu and Jia, Jiaya},
	month = jun,
	year = {2015},
	keywords = {Untagged},
	pages = {1666--1674},
}

@inproceedings{linDeepLearningBinary2015,
	address = {Boston, MA, USA},
	title = {Deep learning of binary hash codes for fast image retrieval},
	isbn = {978-1-4673-6759-2},
	url = {http://ieeexplore.ieee.org/document/7301269/},
	doi = {10/ghv44t},
	abstract = {Approximate nearest neighbor search is an efﬁcient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs), we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available, binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels. The utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning, our method learns hash codes and image representations in a point-wised manner, making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-ofthe-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate its scalability and efﬁcacy on a large-scale dataset of 1 million clothing images.},
	language = {en},
	urldate = {2020-11-05},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	publisher = {IEEE},
	author = {Lin, Kevin and Yang, Huei-Fang and Hsiao, Jen-Hao and Chen, Chu-Song},
	year = {2015},
	keywords = {Untagged},
	pages = {27--35},
}

@inproceedings{liuIncorporatingDomainSentiment2015,
	title = {Incorporating {Domain} and {Sentiment} {Supervision} in {Representation} {Learning} for {Domain} {Adaptation}},
	url = {http://ijcai.org/Abstract/15/184},
	booktitle = {Proceedings of the {Twenty}-{Fourth} {International} {Joint} {Conference} on {Artificial} {Intelligence}, {IJCAI} 2015, {Buenos} {Aires}, {Argentina}, {July} 25-31, 2015},
	publisher = {AAAI Press},
	author = {Liu, Biao and Huang, Minlie and Sun, Jiashen and Zhu, Xuan},
	editor = {Yang, Qiang and Wooldridge, Michael J.},
	year = {2015},
	keywords = {Untagged},
	pages = {1277--1283},
}

@inproceedings{liu2015faceattributes,
	title = {Deep learning face attributes in the wild},
	booktitle = {Proceedings of international conference on computer vision ({ICCV})},
	author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
	month = dec,
	year = {2015},
	keywords = {Untagged},
}

@inproceedings{longLearningTransferableFeatures2015,
	title = {Learning transferable features with deep adaptation networks},
	booktitle = {{ICML}},
	author = {Long, M. and Cao, Y. and Wang, J. and Jordan, M. I},
	year = {2015},
	keywords = {Untagged},
	pages = {97--105},
}

@inproceedings{luong-etal-2015-effective,
	address = {Lisbon, Portugal},
	title = {Effective approaches to attention-based neural machine translation},
	url = {https://aclanthology.org/D15-1166},
	doi = {10.18653/v1/D15-1166},
	booktitle = {Proceedings of the 2015 conference on empirical methods in natural language processing},
	publisher = {Association for Computational Linguistics},
	author = {Luong, Thang and Pham, Hieu and Manning, Christopher D.},
	month = sep,
	year = {2015},
	keywords = {Untagged},
	pages = {1412--1421},
}

@inproceedings{DBLP:conf/iccv/RanjanRJ15,
	title = {Multi-label cross-modal retrieval},
	url = {https://doi.org/10.1109/ICCV.2015.466},
	doi = {10.1109/ICCV.2015.466},
	booktitle = {2015 {IEEE} international conference on computer vision, {ICCV} 2015, santiago, chile, december 7-13, 2015},
	publisher = {IEEE Computer Society},
	author = {Ranjan, Viresh and Rasiwasia, Nikhil and Jawahar, C. V.},
	year = {2015},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/iccv/RanjanRJ15.bib
tex.timestamp: Sun, 25 Oct 2020 23:16:01 +0100},
	keywords = {Untagged},
	pages = {4094--4102},
}

@inproceedings{ronnebergerUNetConvolutionalNetworks2015,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	volume = {9351},
	url = {https://doi.org/10.1007/978-3-319-24574-4_28},
	doi = {10/gcgk7j},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} - {MICCAI} 2015 - 18th {International} {Conference} {Munich}, {Germany}, {October} 5 - 9, 2015, {Proceedings}, {Part} {III}},
	publisher = {Springer},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and III, William M. Wells and Frangi, Alejandro F.},
	year = {2015},
	keywords = {Untagged},
	pages = {234--241},
}

@inproceedings{sainingxieHyperclassAugmentedRegularized2015,
	title = {Hyper-class augmented and regularized deep learning for fine-grained image classification},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Saining Xie, Tianbao Yang, Xiaoyu Wang, Yuanqing Lin},
	year = {2015},
	keywords = {Untagged},
	pages = {2645--2654},
}

@inproceedings{schroffFaceNetUnifiedEmbedding2015,
	address = {Boston, MA, USA},
	title = {{FaceNet}: {A} unified embedding for face recognition and clustering},
	isbn = {978-1-4673-6964-0},
	shorttitle = {{FaceNet}},
	url = {http://ieeexplore.ieee.org/document/7298682/},
	doi = {10/gdcfkq},
	abstract = {Despite signiﬁcant recent advances in the ﬁeld of face recognition [10, 14, 15, 17], implementing face veriﬁcation and recognition efﬁciently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, veriﬁcation and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	year = {2015},
	keywords = {Untagged},
	pages = {815--823},
}

@book{shiManifoldRegularizedSelectableFactor2015,
	title = {Manifold-{Regularized} {Selectable} {Factor} {Extraction} for {Semi}-supervised {Image} {Classification}},
	author = {Shi, Xin and Zhang, Chao and Wei, Fangyun and Zhang, Hongyang and She, Yiyuan},
	year = {2015},
	doi = {10.5244/C.29.132},
	note = {Pages: 132.11},
	keywords = {Untagged},
}

@inproceedings{shokriPrivacyPreservingDeepLearning2015,
	address = {Denver Colorado USA},
	title = {Privacy-{Preserving} {Deep} {Learning}},
	isbn = {978-1-4503-3832-5},
	url = {https://dl.acm.org/doi/10.1145/2810103.2813687},
	doi = {10/gfvdzx},
	abstract = {Deep learning based on artiﬁcial neural networks is a very popular approach to modeling, classifying, and recognizing complex data such as images, speech, and text. The unprecedented accuracy of deep learning methods has turned them into the foundation of new AI-based services on the Internet. Commercial companies that collect user data on a large scale have been the main beneﬁciaries of this trend since the success of deep learning techniques is directly proportional to the amount of data available for training.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Shokri, Reza and Shmatikov, Vitaly},
	month = oct,
	year = {2015},
	keywords = {Untagged},
	pages = {1310--1321},
}

@inproceedings{songTopRankSupervised2015,
	address = {Santiago, Chile},
	title = {Top {Rank} {Supervised} {Binary} {Coding} for {Visual} {Search}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410580/},
	doi = {10/ghwfxb},
	abstract = {In recent years, binary coding techniques are becoming increasingly popular because of their high efﬁciency in handling large-scale computer vision applications. It has been demonstrated that supervised binary coding techniques that leverage supervised information can signiﬁcantly enhance the coding quality, and hence greatly beneﬁt visual search tasks. Typically, a modern binary coding method seeks to learn a group of coding functions which compress data samples into binary codes. However, few methods pursued the coding functions such that the precision at the top of a ranking list according to Hamming distances of the generated binary codes is optimized. In this paper, we propose a novel supervised binary coding approach, namely Top Rank Supervised Binary Coding (Top-RSBC), which explicitly focuses on optimizing the precision of top positions in a Hamming-distance ranking list towards preserving the supervision information. The core idea is to train the disciplined coding functions, by which the mistakes at the top of a Hamming-distance ranking list are penalized more than those at the bottom. To solve such coding functions, we relax the original discrete optimization objective with a continuous surrogate, and derive a stochastic gradient descent to optimize the surrogate objective. To further reduce the training time cost, we also design an online learning algorithm to optimize the surrogate objective more efﬁciently. Empirical studies based upon three benchmark image datasets demonstrate that the proposed binary coding approach achieves superior image search accuracy over the state-of-the-arts.},
	language = {en},
	urldate = {2021-01-28},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Song, Dongjin and Liu, Wei and Ji, Rongrong and Meyer, David A. and Smith, John R.},
	year = {2015},
	keywords = {Untagged},
	pages = {1922--1930},
}

@article{springenbergStrivingSimplicityAll2015,
	title = {Striving for {Simplicity}: {The} {All} {Convolutional} {Net}},
	shorttitle = {Striving for {Simplicity}},
	url = {http://arxiv.org/abs/1412.6806},
	abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We ﬁnd that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this ﬁnding – and building on other recent work for ﬁnding simple network structures – we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the “deconvolution approach” for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1412.6806 [cs]},
	author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
	month = apr,
	year = {2015},
	note = {arXiv: 1412.6806},
	keywords = {Untagged},
}

@inproceedings{tremblayExploitingEnvironmentalSounds2015,
	address = {Austin, Texas},
	series = {{AAAI} {Workshops}},
	title = {Exploiting {Environmental} {Sounds} for {Activity} {Recognition} in {Smart} {Homes}},
	volume = {WS-16-03},
	url = {http://aaai.org/ocs/index.php/WS/AAAIW15/paper/view/9697},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence} {Workshop}},
	publisher = {AAAI Press},
	author = {Tremblay, Sébastien and Fortin-Simard, Dany and Blackburn-Verreault, Erika and Gaboury, Sébastien and Bouchard, Bruno and Bouzouane, Abdenour},
	editor = {Bouchard, Bruno and Giroux, Sylvain and Bouzouane, Abdenour and Guillet, Sébastien},
	year = {2015},
	keywords = {Untagged},
}

@article{troppIntroductionMatrixConcentration2015,
	title = {An {Introduction} to {Matrix} {Concentration} {Inequalities}},
	volume = {8},
	issn = {1935-8237},
	url = {https://doi.org/10.1561/2200000048},
	doi = {10/gc8zfs},
	abstract = {Random matrices now play a role in many areas of theoretical, applied,and computational mathematics. Therefore, it is desirable to have toolsfor studying random matrices that are flexible, easy to use, and powerful.Over the last fifteen years, researchers have developed a remarkablefamily of results, called matrix concentration inequalities, that achieveall of these goals.This monograph offers an invitation to the field of matrix concentrationinequalities. It begins with some history of random matrix theory;it describes a flexible model for random matrices that is suitablefor many problems; and it discusses the most important matrix concentrationresults. To demonstrate the value of these techniques, thepresentation includes examples drawn from statistics, machine learning,optimization, combinatorics, algorithms, scientific computing, andbeyond.},
	number = {1–2},
	journal = {Found. Trends Mach. Learn.},
	author = {Tropp, Joel A.},
	year = {2015},
	note = {Place: Hanover, MA, USA
Publisher: Now Publishers Inc.},
	keywords = {Untagged},
	pages = {1--230},
}

@inproceedings{tzengSimultaneousDeepTransfer2015,
	title = {Simultaneous deep transfer across domains and tasks},
	booktitle = {International {Conference} on {Computer} {Vision}},
	author = {Tzeng, E. and Hoffman, J. and Darrell, T. and Saenko, K.},
	year = {2015},
	keywords = {Untagged},
	pages = {4068--4076},
}

@inproceedings{venugopalan_sequence_2015,
	title = {Sequence to {Sequence} – {Video} to {Text}},
	doi = {10.1109/ICCV.2015.515},
	abstract = {Real-world videos often have complex dynamics, methods for generating open-domain video descriptions should be sensitive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, i.e. a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Venugopalan, Subhashini and Rohrbach, Marcus and Donahue, Jeffrey and Mooney, Raymond and Darrell, Trevor and Saenko, Kate},
	month = dec,
	year = {2015},
	note = {ISSN: 2380-7504},
	keywords = {Untagged},
	pages = {4534--4542},
}

@inproceedings{venugopalan_translating_2015,
	address = {Denver, Colorado},
	title = {Translating {Videos} to {Natural} {Language} {Using} {Deep} {Recurrent} {Neural} {Networks}},
	shorttitle = {{S2VT}},
	url = {https://aclanthology.org/N15-1173},
	doi = {10.3115/v1/N15-1173},
	urldate = {2023-05-30},
	booktitle = {Proceedings of the 2015 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Venugopalan, Subhashini and Xu, Huijuan and Donahue, Jeff and Rohrbach, Marcus and Mooney, Raymond and Saenko, Kate},
	month = may,
	year = {2015},
	keywords = {Untagged},
	pages = {1494--1504},
}

@inproceedings{vinyalsShowTellNeural2015,
	address = {Boston, MA, USA},
	title = {Show and tell: {A} neural image caption generator},
	isbn = {978-1-4673-6964-0},
	shorttitle = {Show and tell},
	url = {http://ieeexplore.ieee.org/document/7298935/},
	doi = {10/gffdbh},
	abstract = {Automatically describing the content of an image is a fundamental problem in artiﬁcial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the ﬂuency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
	year = {2015},
	keywords = {Untagged},
	pages = {3156--3164},
}

@inproceedings{wangLearningHashStructured2015,
	title = {Learning to {Hash} on {Structured} {Data}},
	url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9410},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {AAAI} {Conference} on {Artificial} {Intelligence}, {January} 25-30, 2015, {Austin}, {Texas}, {USA}},
	publisher = {AAAI Press},
	author = {Wang, Qifan and Si, Luo and Shen, Bin},
	editor = {Bonet, Blai and Koenig, Sven},
	year = {2015},
	keywords = {Untagged},
	pages = {3066--3072},
}

@inproceedings{xiaoApplicationTwolevelAttention2015,
	title = {The application of two-level attention models in deep convolutional neural network for fine-grained image classification},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Xiao, T. and Xu, Y. and Yang, K. and Zhang, J. and Peng, Y. and Zhang, Z.},
	year = {2015},
	keywords = {Untagged},
	pages = {842--850},
}

@article{xieFineGrainedImageSearch2015,
	title = {Fine-{Grained} {Image} {Search}},
	volume = {17},
	number = {5},
	journal = {TMM},
	author = {Xie, Lingxi and Wang, Jingdong and Zhang, Bo and Tian, Qi},
	year = {2015},
	keywords = {Untagged},
	pages = {636--647},
}

@inproceedings{xieHyperclassAugmentedRegularized2015,
	title = {Hyper-class augmented and regularized deep learning for fine-grained image classification},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Xie, S. and Yang, T. and Wang, X. and Lin, Y.},
	year = {2015},
	keywords = {Untagged},
	pages = {2645--2654},
}

@inproceedings{xuShowAttendTell2015,
	address = {Lille, France},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Show, {Attend} and {Tell}: {Neural} {Image} {Caption} {Generation} with {Visual} {Attention}},
	volume = {37},
	url = {http://proceedings.mlr.press/v37/xuc15.html},
	abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
	editor = {Bach, Francis and Blei, David},
	month = jul,
	year = {2015},
	keywords = {Untagged},
	pages = {2048--2057},
}

@inproceedings{xuMultimodalMultiviewInteractive2015,
	title = {Multi-modal \& {Multi}-view \& {Interactive} {Benchmark} {Dataset} for {Human} {Action} {Recognition}},
	url = {https://doi.org/10.1145/2733373.2806315},
	doi = {10.1145/2733373.2806315},
	booktitle = {Proceedings of the 23rd {Annual} {ACM} {Conference} on {Multimedia} {Conference}, {MM} '15, {Brisbane}, {Australia}, {October} 26 - 30, 2015},
	publisher = {ACM},
	author = {Xu, Ning and Liu, Anan and Nie, Weizhi and Wong, Yongkang and Li, Fuwu and Su, Yuting},
	editor = {Zhou, Xiaofang and Smeaton, Alan F. and Tian, Qi and Bulterman, Dick C. A. and Shen, Heng Tao and Mayer-Patel, Ketan and Yan, Shuicheng},
	year = {2015},
	keywords = {Untagged},
	pages = {1195--1198},
}

@inproceedings{DBLP:conf/mm/XuYSTH15,
	title = {Semi-supervised coupled dictionary learning for cross-modal retrieval in internet images and texts},
	url = {https://doi.org/10.1145/2733373.2806346},
	doi = {10.1145/2733373.2806346},
	booktitle = {Proceedings of the 23rd annual {ACM} conference on multimedia conference, {MM} '15, brisbane, australia, october 26 - 30, 2015},
	publisher = {ACM},
	author = {Xu, Xing and Yang, Yang and Shimada, Atsushi and Taniguchi, Rin-Ichiro and He, Li},
	editor = {Zhou, Xiaofang and Smeaton, Alan F. and Tian, Qi and Bulterman, Dick C. A. and Shen, Heng Tao and Mayer-Patel, Ketan and Yan, Shuicheng},
	year = {2015},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/mm/XuYSTH15.bib
tex.timestamp: Wed, 17 Feb 2021 15:51:19 +0100},
	keywords = {Untagged},
	pages = {847--850},
}

@inproceedings{yangDeepConvolutionalNeural2015,
	address = {Buenos Aires, Argentina},
	title = {Deep {Convolutional} {Neural} {Networks} on {Multichannel} {Time} {Series} for {Human} {Activity} {Recognition}},
	url = {http://ijcai.org/Abstract/15/561},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Yang, Jianbo and Nguyen, Minh Nhut and San, Phyo Phyo and Li, Xiaoli and Krishnaswamy, Shonali},
	editor = {Yang, Qiang and Wooldridge, Michael J.},
	year = {2015},
	keywords = {Untagged},
	pages = {3995--4001},
}

@inproceedings{yangLargescaleCarDataset2015,
	address = {Boston, MA},
	title = {A large-scale car dataset for fine-grained categorization and verification},
	url = {https://doi.org/10.1109/CVPR.2015.7299023},
	doi = {10/ggnb3p},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Yang, Linjie and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
	year = {2015},
	keywords = {Untagged},
	pages = {3973--3981},
}

@article{zarembaRecurrentNeuralNetwork2015,
	title = {Recurrent {Neural} {Network} {Regularization}},
	url = {http://arxiv.org/abs/1409.2329},
	abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
	urldate = {2021-09-27},
	journal = {arXiv:1409.2329 [cs]},
	author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
	month = feb,
	year = {2015},
	note = {arXiv: 1409.2329},
	keywords = {Untagged},
}

@inproceedings{zhangExactRecoverabilityRobust2015,
	title = {Exact recoverability of robust {PCA} via outlier pursuit with tight recovery bounds},
	booktitle = {Twenty-{Ninth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zhang, Hongyang and Lin, Zhouchen and Zhang, Chao and Chang, Edward Y.},
	year = {2015},
	keywords = {Untagged},
}

@article{zhangRelationsLowrankSubspace2015,
	title = {Relations among some low-rank subspace recovery models},
	volume = {27},
	doi = {10.1162/NECO_a_00762},
	number = {9},
	journal = {Neural computation},
	author = {Zhang, Hongyang and Lin, Zhouchen and Zhang, Chao and Gao, Junbin},
	year = {2015},
	note = {Publisher: MIT Press},
	keywords = {Untagged},
	pages = {1915--1950},
}

@inproceedings{zhangMultiSourceDomainAdaptation2015,
	title = {Multi-{Source} {Domain} {Adaptation}: {A} {Causal} {View}},
	url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/10052},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {AAAI} {Conference} on {Artificial} {Intelligence}, {January} 25-30, 2015, {Austin}, {Texas}, {USA}},
	publisher = {AAAI Press},
	author = {Zhang, Kun and Gong, Mingming and Schölkopf, Bernhard},
	editor = {Bonet, Blai and Koenig, Sven},
	year = {2015},
	keywords = {Untagged},
	pages = {3150--3157},
}

@inproceedings{Zhang2015CharacterlevelCN,
	title = {Character-level convolutional networks for text classification},
	shorttitle = {{AG}\_NEWS},
	booktitle = {{NIPS}},
	author = {Zhang, Xiang and Zhao, Junbo Jake and LeCun, Yann},
	year = {2015},
	keywords = {Untagged},
}

@inproceedings{zhenCrossModalSimilarityLearning2015,
	title = {Cross-{Modal} {Similarity} {Learning} via {Pairs}, {Preferences}, and {Active} {Supervision}},
	url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9648},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Zhen, Yi and Rai, Piyush and Zha, Hongyuan and Carin, Lawrence},
	editor = {Bonet, Blai and Koenig, Sven},
	year = {2015},
	keywords = {Untagged},
	pages = {3203--3209},
}

@article{zhuWearableSensorBasedBehavioral2015,
	title = {Wearable {Sensor}-{Based} {Behavioral} {Anomaly} {Detection} in {Smart} {Assisted} {Living} {Systems}},
	volume = {12},
	url = {https://doi.org/10.1109/TASE.2015.2474743},
	doi = {10.1109/TASE.2015.2474743},
	number = {4},
	journal = {IEEE Trans Autom. Sci. Eng.},
	author = {Zhu, Chun and Sheng, Weihua and Liu, Meiqin},
	year = {2015},
	keywords = {Untagged},
	pages = {1225--1234},
}

@inproceedings{abadiDeepLearningDifferential2016,
	address = {Vienna Austria},
	title = {Deep {Learning} with {Differential} {Privacy}},
	isbn = {978-1-4503-4139-4},
	url = {https://dl.acm.org/doi/10.1145/2976749.2978318},
	doi = {10/gcrnp3},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
	month = oct,
	year = {2016},
	keywords = {Untagged},
	pages = {308--318},
}

@inproceedings{alsheikhDeepActivityRecognition2016,
	address = {Phoenix, Arizona},
	series = {{AAAI} {Workshops}},
	title = {Deep {Activity} {Recognition} {Models} with {Triaxial} {Accelerometers}},
	volume = {WS-16-01},
	url = {http://www.aaai.org/ocs/index.php/WS/AAAIW16/paper/view/12627},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence} {Workshops}},
	publisher = {AAAI Press},
	author = {Alsheikh, Mohammad Abu and Selim, Ahmed and Niyato, Dusit and Doyle, Linda and Lin, Shaowei and Tan, Hwee-Pink},
	editor = {Bouchard, Bruno and Giroux, Sylvain and Bouzouane, Abdenour and Gaboury, Sébastien},
	year = {2016},
	keywords = {Untagged},
}

@inproceedings{andrychowiczLearningLearnGradient2016,
	address = {Barcelona, Spain},
	title = {Learning to learn by gradient descent by gradient descent},
	url = {https://proceedings.neurips.cc/paper/2016/hash/fb87582825f9d28a8d42c5e5e5e8b23d-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Andrychowicz, Marcin and Denil, Misha and Colmenarejo, Sergio Gomez and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Freitas, Nando de},
	editor = {Lee, Daniel D. and Sugiyama, Masashi and Luxburg, Ulrike von and Guyon, Isabelle and Garnett, Roman},
	year = {2016},
	keywords = {Untagged},
	pages = {3981--3989},
}

@inproceedings{awasthiLearning1bitCompressed2016,
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {Learning and 1-bit {Compressed} {Sensing} under {Asymmetric} {Noise}},
	volume = {49},
	url = {http://proceedings.mlr.press/v49/awasthi16.html},
	booktitle = {Proceedings of the 29th {Conference} on {Learning} {Theory}, {COLT} 2016, {New} {York}, {USA}, {June} 23-26, 2016},
	publisher = {JMLR.org},
	author = {Awasthi, Pranjal and Balcan, Maria-Florina and Haghtalab, Nika and Zhang, Hongyang},
	editor = {Feldman, Vitaly and Rakhlin, Alexander and Shamir, Ohad},
	year = {2016},
	keywords = {Untagged},
	pages = {152--192},
}

@inproceedings{balcanNoisetolerantLifelongMatrix2016,
	title = {Noise-tolerant life-long matrix completion via adaptive sampling},
	volume = {29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Balcan, Maria-Florina F. and Zhang, Hongyang},
	year = {2016},
	keywords = {Untagged},
}

@inproceedings{bilenWeaklySupervisedDeep2016,
	address = {Las Vegas, NV},
	title = {Weakly {Supervised} {Deep} {Detection} {Networks}},
	url = {https://doi.org/10.1109/CVPR.2016.311},
	doi = {10/ggn89j},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Bilen, Hakan and Vedaldi, Andrea},
	year = {2016},
	keywords = {Untagged},
	pages = {2846--2854},
}

@inproceedings{booth3DMorphableModel2016,
	address = {Las Vegas, NV},
	title = {A {3D} {Morphable} {Model} {Learnt} from 10,000 {Faces}},
	isbn = {978-1-4673-8851-1},
	url = {https://ieeexplore.ieee.org/document/7780967/},
	doi = {10/ggssnx},
	abstract = {We present Large Scale Facial Model (LSFM) — a 3D Morphable Model (3DMM) automatically constructed from 9,663 distinct facial identities. To the best of our knowledge LSFM is the largest-scale Morphable Model ever constructed, containing statistical information from a huge variety of the human population. To build such a large model we introduce a novel fully automated and robust Morphable Model construction pipeline. The dataset that LSFM is trained on includes rich demographic information about each subject, allowing for the construction of not only a global 3DMM but also models tailored for speciﬁc age, gender or ethnicity groups. As an application example, we utilise the proposed model to perform age classiﬁcation from 3D shape alone. Furthermore, we perform a systematic analysis of the constructed 3DMMs that showcases their quality and descriptive power. The presented extensive qualitative and quantitative evaluations reveal that the proposed 3DMM achieves state-of-the-art results, outperforming existing models by a large margin. Finally, for the beneﬁt of the research community, we make publicly available the source code of the proposed automatic 3DMM construction pipeline. In addition, the constructed global 3DMM and a variety of bespoke models tailored by age, gender and ethnicity are available on application to researchers involved in medically oriented research.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Booth, James and Roussos, Anastasios and Zafeiriou, Stefanos and Ponniah, Allan and Dunaway, David},
	year = {2016},
	keywords = {Untagged},
	pages = {5543--5552},
}

@inproceedings{brandsaeterApplicationSensorbasedAnomaly2016,
	address = {Ottawa, ON},
	title = {An application of sensor-based anomaly detection in the maritime industry},
	isbn = {978-1-5090-0382-2},
	url = {https://ieeexplore.ieee.org/document/7811910/},
	doi = {10.1109/ICPHM.2016.7811910},
	abstract = {In this paper we present an application of sensorbased anomaly detection in maritime transport. The study is based on real sensor data streamed from a ship to shore, where the data is analysed through a big data analytics platform. The novelty of this work originates in the use of data from sensors covering different aspects of the ship operation, exempliﬁed here by propulsion power, speed over ground and ship motion in four different degrees of freedom. The developed method employs Auto Associative Kernel Regression (AAKR) for signal reconstruction, and the Sequential Probability Ratio Test (SPRT) technique for anomaly detection, where different hypothesis tests looking both at mean and variance deviations have been tested. In order to compare different settings, formal state of the art performance metrics have been used.},
	language = {en},
	urldate = {2021-06-04},
	booktitle = {{IEEE} {International} {Conference} on {Prognostics} and {Health} {Management}},
	publisher = {IEEE},
	author = {Brandsæter, Andreas and Manno, Gabriele and Vanem, Erik and Glad, Ingrid Kristine},
	year = {2016},
	keywords = {Untagged},
	pages = {1--8},
}

@inproceedings{bunConcentratedDifferentialPrivacy2016,
	address = {Beijing, China},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Concentrated {Differential} {Privacy}: {Simplifications}, {Extensions}, and {Lower} {Bounds}},
	volume = {9985},
	url = {https://doi.org/10.1007/978-3-662-53641-4_24},
	doi = {10/gj45dp},
	booktitle = {Theory of {Cryptography}},
	author = {Bun, Mark and Steinke, Thomas},
	editor = {Hirt, Martin and Smith, Adam D.},
	year = {2016},
	keywords = {Untagged},
	pages = {635--658},
}

@inproceedings{caoCorrelationAutoencoderHashing2016,
	address = {New York, New York, USA},
	title = {Correlation {Autoencoder} {Hashing} for {Supervised} {Cross}-{Modal} {Search}},
	isbn = {978-1-4503-4359-6},
	url = {http://dl.acm.org/citation.cfm?doid=2911996.2912000},
	doi = {10/ghhxhz},
	language = {en},
	urldate = {2020-11-05},
	booktitle = {International {Conference} on {Multimedia} {Retrieval}},
	publisher = {ACM Press},
	author = {Cao, Yue and Long, Mingsheng and Wang, Jianmin and Zhu, Han},
	year = {2016},
	keywords = {Untagged},
	pages = {197--204},
}

@article{chenOnlineSequentialProjection2016,
	title = {Online {Sequential} {Projection} {Vector} {Machine} with {Adaptive} {Data} {Mean} {Update}},
	volume = {2016},
	issn = {1687-5265, 1687-5273},
	url = {http://www.hindawi.com/journals/cin/2016/5197932/},
	doi = {10/f8whpd},
	abstract = {We propose a simple online learning algorithm especial for high-dimensional data. The algorithm is referred to as online sequential projection vector machine (OSPVM) which derives from projection vector machine and can learn from data in one-by-one or chunk-by-chunk mode. In OSPVM, data centering, dimension reduction, and neural network training are integrated seamlessly. In particular, the model parameters including
              
                (
                1
                )
              
              the projection vectors for dimension reduction,
              
                (
                2
                )
              
              the input weights, biases, and output weights, and
              
                (
                3
                )
              
              the number of hidden nodes can be updated simultaneously. Moreover, only one parameter, the number of hidden nodes, needs to be determined manually, and this makes it easy for use in real applications. Performance comparison was made on various high-dimensional classification problems for OSPVM against other fast online algorithms including budgeted stochastic gradient descent (BSGD) approach, adaptive multihyperplane machine (AMM), primal estimated subgradient solver (Pegasos), online sequential extreme learning machine (OSELM), and SVD + OSELM (feature selection based on SVD is performed before OSELM). The results obtained demonstrated the superior generalization performance and efficiency of the OSPVM.},
	language = {en},
	urldate = {2021-01-27},
	journal = {Computational Intelligence and Neuroscience},
	author = {Chen, Lin and Jia, Ji-Ting and Zhang, Qiong and Deng, Wan-Yu and Wei, Wei},
	year = {2016},
	keywords = {Untagged},
	pages = {1--13},
}

@article{chenXGBoostScalableTree2016,
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	shorttitle = {{XGBoost}},
	url = {http://arxiv.org/abs/1603.02754},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly eﬀective and widely used machine learning method. In this paper, we describe a scalable endto-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	language = {en},
	urldate = {2021-03-18},
	journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	author = {Chen, Tianqi and Guestrin, Carlos},
	year = {2016},
	note = {arXiv: 1603.02754},
	keywords = {Untagged},
	pages = {785--794},
}

@inproceedings{chenInfoGANInterpretableRepresentation2016,
	address = {Barcelona, Spain},
	title = {{InfoGAN}: {Interpretable} {Representation} {Learning} by {Information} {Maximizing} {Generative} {Adversarial} {Nets}},
	url = {https://proceedings.neurips.cc/paper/2016/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	editor = {Lee, Daniel D. and Sugiyama, Masashi and Luxburg, Ulrike von and Guyon, Isabelle and Garnett, Roman},
	year = {2016},
	keywords = {Untagged},
	pages = {2172--2180},
}

@article{dentonSemiSupervisedLearningContextConditional2016,
	title = {Semi-{Supervised} {Learning} with {Context}-{Conditional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.06430},
	abstract = {We introduce a simple semi-supervised learning approach for images based on in-painting using an adversarial loss. Images with random patches removed are presented to a generator whose task is to ﬁll in the hole, based on the surrounding pixels. The in-painted images are then presented to a discriminator network that judges if they are real (unaltered training images) or not. This task acts as a regularizer for standard supervised training of the discriminator. Using our approach we are able to directly train large VGG-style networks in a semi-supervised fashion. We evaluate on STL-10 and PASCAL datasets, where our approach obtains performance comparable or superior to existing methods.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1611.06430 [cs]},
	author = {Denton, Emily and Gross, Sam and Fergus, Rob},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.06430},
	keywords = {Untagged},
}

@inproceedings{doLearningHashBinary2016,
	title = {Learning to {Hash} with {Binary} {Deep} {Neural} {Network}},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Do, Thanh-Toan and Doan, Anh-Dzung and Cheung, Ngai-Man},
	year = {2016},
	keywords = {Untagged},
	pages = {219--234},
}

@article{drozdzalImportanceSkipConnections2016,
	title = {The {Importance} of {Skip} {Connections} in {Biomedical} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1608.04117},
	abstract = {In this paper, we study the inﬂuence of both long and short skip connections on Fully Convolutional Networks (FCN) for biomedical image segmentation. In standard FCNs, only long skip connections are used to skip features from the contracting path to the expanding path in order to recover spatial information lost during downsampling. We extend FCNs by adding short skip connections, that are similar to the ones introduced in residual networks, in order to build very deep FCNs (of hundreds of layers). A review of the gradient ﬂow conﬁrms that for a very deep FCN it is beneﬁcial to have both long and short skip connections. Finally, we show that a very deep FCN can achieve near-to-state-of-the-art results on the EM dataset without any further post-processing.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1608.04117 [cs]},
	author = {Drozdzal, Michal and Vorontsov, Eugene and Chartrand, Gabriel and Kadoury, Samuel and Pal, Chris},
	month = sep,
	year = {2016},
	note = {arXiv: 1608.04117},
	keywords = {Untagged},
}

@inproceedings{duNovelQuantizationStrategies2016,
	title = {Novel quantization strategies for linear prediction with guarantees},
	booktitle = {Proceedings of {ICML} 2016 {Workshop} on {On}-{Device} {Intelligence}},
	author = {Du, S. and Xu, Yichong and Zhang, H. and Li, C. and Grover, P. and Singh, Aarti},
	year = {2016},
	keywords = {Untagged},
}

@inproceedings{ervenMetaGradMultipleLearning2016,
	address = {Barcelona, Spain},
	title = {{MetaGrad}: {Multiple} {Learning} {Rates} in {Online} {Learning}},
	url = {https://proceedings.neurips.cc/paper/2016/hash/14cfdb59b5bda1fc245aadae15b1984a-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Erven, Tim van and Koolen, Wouter M.},
	editor = {Lee, Daniel D. and Sugiyama, Masashi and Luxburg, Ulrike von and Guyon, Isabelle and Garnett, Roman},
	year = {2016},
	keywords = {Untagged},
	pages = {3666--3674},
}

@article{ganin_domain-adversarial_2016,
	title = {Domain-{Adversarial} {Training} of {Neural} {Networks}},
	volume = {17},
	shorttitle = {{DANN}},
	url = {http://jmlr.org/papers/v17/15-239.html},
	number = {59},
	journal = {Journal of Machine Learning Research},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and March, Mario and Lempitsky, Victor},
	year = {2016},
	keywords = {Untagged},
	pages = {1--35},
}

@inproceedings{gaoCompactBilinearPooling2016,
	title = {Compact {Bilinear} {Pooling}},
	doi = {10/gj45dq},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Gao, Yang and Beijbom, Oscar and Zhang, Ning and Darrell, Trevor},
	year = {2016},
	keywords = {Untagged},
	pages = {317--326},
}

@article{garipovUltimateTensorizationCompressing2016,
	title = {Ultimate tensorization: compressing convolutional and {FC} layers alike},
	shorttitle = {Ultimate tensorization},
	url = {http://arxiv.org/abs/1611.03214},
	abstract = {Convolutional neural networks excel in image recognition tasks, but this comes at the cost of high computational and memory complexity. To tackle this problem, [1] developed a tensor factorization framework to compress fully-connected layers. In this paper, we focus on compressing convolutional layers. We show that while the direct application of the tensor framework [1] to the 4-dimensional kernel of convolution does compress the layer, we can do better. We reshape the convolutional kernel into a tensor of higher order and factorize it. We combine the proposed approach with the previous work to compress both convolutional and fully-connected layers of a network and achieve 80x network compression rate with 1.1\% accuracy drop on the CIFAR-10 dataset.},
	urldate = {2022-02-28},
	journal = {arXiv:1611.03214 [cs]},
	author = {Garipov, Timur and Podoprikhin, Dmitry and Novikov, Alexander and Vetrov, Dmitry},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.03214},
	keywords = {Untagged},
}

@inproceedings{gatysImageStyleTransfer2016,
	address = {Las Vegas, NV, USA},
	title = {Image {Style} {Transfer} {Using} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780634/},
	doi = {10/gfpg66},
	abstract = {Rendering the semantic content of an image in different styles is a difﬁcult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic information and, thus, allow to separate image content from style. Here we use image representations derived from Convolutional Neural Networks optimised for object recognition, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can separate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an arbitrary photograph with the appearance of numerous wellknown artworks. Our results provide new insights into the deep image representations learned by Convolutional Neural Networks and demonstrate their potential for high level image synthesis and manipulation.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	year = {2016},
	keywords = {Untagged},
	pages = {2414--2423},
}

@inproceedings{geFinegrainedClassificationMixture2016,
	title = {Fine-grained classification via mixture of deep convolutional neural networks},
	doi = {10/ghqpfx},
	booktitle = {{IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision}},
	author = {Ge, Z. and Bewley, A. and McCool, C. and Corke, P. and Upcroft, B. and Sanderson, C.},
	year = {2016},
	keywords = {Untagged},
	pages = {1--6},
}

@inproceedings{ghourchianLocationBasedActivityRecognition2016,
	address = {New York City, NY},
	title = {Location-{Based} {Activity} {Recognition} with {Hierarchical} {Dirichlet} {Process}},
	url = {http://www.ijcai.org/Abstract/16/569},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {IJCAI/AAAI Press},
	author = {Ghourchian, Negar},
	editor = {Kambhampati, Subbarao},
	year = {2016},
	keywords = {Untagged},
	pages = {3990--3991},
}

@inproceedings{guanEfficientMultiInstanceLearning2016,
	address = {New York City, NY},
	title = {Efficient {Multi}-{Instance} {Learning} for {Activity} {Recognition} from {Time} {Series} {Data} {Using} an {Auto}-{Regressive} {Hidden} {Markov} {Model}},
	volume = {48},
	url = {http://proceedings.mlr.press/v48/guan16.html},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Guan, Xinze and Raich, Raviv and Wong, Weng-Keen},
	editor = {Balcan, Maria-Florina and Weinberger, Kilian Q.},
	year = {2016},
	keywords = {Untagged},
	pages = {2330--2339},
}

@inproceedings{hammerlaDeepConvolutionalRecurrent2016,
	address = {New York, NY},
	title = {Deep, {Convolutional}, and {Recurrent} {Models} for {Human} {Activity} {Recognition} {Using} {Wearables}},
	url = {http://www.ijcai.org/Abstract/16/220},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {IJCAI/AAAI Press},
	author = {Hammerla, Nils Y. and Halloran, Shane and Plötz, Thomas},
	editor = {Kambhampati, Subbarao},
	year = {2016},
	keywords = {Untagged},
	pages = {1533--1540},
}

@inproceedings{hanDeepCompressionCompressing2016,
	address = {San Juan, Puerto Rico},
	title = {Deep {Compression}: {Compressing} {Deep} {Neural} {Network} with {Pruning}, {Trained} {Quantization} and {Huffman} {Coding}},
	url = {http://arxiv.org/abs/1510.00149},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Han, Song and Mao, Huizi and Dally, William J.},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2016},
	keywords = {Untagged},
}

@inproceedings{Hasan2016HumanAR,
	title = {Human activity recognition using smartphone sensors with context filtering},
	booktitle = {{ACHI} 2016},
	author = {Hasan, Shah Md. Shihab and Masnad, Mohshi and Khan, Md Mohiuddin and Mahmud, Hasan and Hasan, Md. Kamrul},
	year = {2016},
	keywords = {Untagged},
}

@article{hazanIntroductionOnlineConvex2016,
	title = {Introduction to online convex optimization},
	volume = {2},
	doi = {10/ggmmss},
	number = {3-4},
	journal = {Foundations and Trends® in Optimization},
	author = {Hazan, Elad},
	year = {2016},
	note = {Publisher: Now Publishers, Inc.},
	keywords = {Untagged},
	pages = {157--325},
}

@inproceedings{heDeepResidualLearning2016,
	address = {Las Vegas, NV},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	shorttitle = {{ResNet}},
	url = {https://doi.org/10.1109/CVPR.2016.90},
	doi = {10/gdcfkn},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	keywords = {Untagged},
	pages = {770--778},
}

@inproceedings{hersheyDeepClusteringDiscriminative2016,
	address = {Shanghai, China},
	title = {Deep clustering: {Discriminative} embeddings for segmentation and separation},
	url = {https://doi.org/10.1109/ICASSP.2016.7471631},
	doi = {10/ggm5q3},
	booktitle = {{IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Hershey, John R. and Chen, Zhuo and Roux, Jonathan Le and Watanabe, Shinji},
	year = {2016},
	keywords = {Untagged},
	pages = {31--35},
}

@inproceedings{huangUnsupervisedLearningDiscriminative2016,
	address = {Las Vegas, NV, USA},
	title = {Unsupervised {Learning} of {Discriminative} {Attributes} and {Visual} {Representations}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780928/},
	doi = {10/ghmjdv},
	abstract = {Attributes offer useful mid-level features to interpret visual data. While most attribute learning methods are supervised by costly human-generated labels, we introduce a simple yet powerful unsupervised approach to learn and predict visual attributes directly from data. Given a large unlabeled image collection as input, we train deep Convolutional Neural Networks (CNNs) to output a set of discriminative, binary attributes often with semantic meanings. Speciﬁcally, we ﬁrst train a CNN coupled with unsupervised discriminative clustering, and then use the cluster membership as a soft supervision to discover shared attributes from the clusters while maximizing their separability. The learned attributes are shown to be capable of encoding rich imagery properties from both natural images and contour patches. The visual representations learned in this way are also transferrable to other tasks such as object detection. We show other convincing results on the related tasks of image retrieval and classiﬁcation, and contour detection.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Huang, Chen and Loy, Chen Change and Tang, Xiaoou},
	year = {2016},
	keywords = {Untagged},
	pages = {5175--5184},
}

@inproceedings{hubaraBinarizedNeuralNetworks2016,
	title = {Binarized neural networks},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/file/d8330f857a17c53d217014ee776bfd50-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
	editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
	year = {2016},
	keywords = {Untagged},
}

@article{inoueDeepRecurrentNeural2016,
	title = {Deep {Recurrent} {Neural} {Network} for {Mobile} {Human} {Activity} {Recognition} with {High} {Throughput}},
	shorttitle = {{LSTM}},
	url = {http://arxiv.org/abs/1611.03607},
	abstract = {In this paper, we propose a method of human activity recognition with high throughput from raw accelerometer data applying a deep recurrent neural network (DRNN), and investigate various architectures and its combination to find the best parameter values. The "high throughput" refers to short time at a time of recognition. We investigated various parameters and architectures of the DRNN by using the training dataset of 432 trials with 6 activity classes from 7 people. The maximum recognition rate was 95.42\% and 83.43\% against the test data of 108 segmented trials each of which has single activity class and 18 multiple sequential trials, respectively. Here, the maximum recognition rates by traditional methods were 71.65\% and 54.97\% for each. In addition, the efficiency of the found parameters was evaluated by using additional dataset. Further, as for throughput of the recognition per unit time, the constructed DRNN was requiring only 1.347 [ms], while the best traditional method required 11.031 [ms] which includes 11.027 [ms] for feature calculation. These advantages are caused by the compact and small architecture of the constructed real time oriented DRNN.},
	urldate = {2021-09-27},
	journal = {arXiv:1611.03607 [cs]},
	author = {Inoue, Masaya and Inoue, Sozo and Nishida, Takeshi},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.03607},
	keywords = {Untagged},
}

@inproceedings{jenattonAdaptiveAlgorithmsOnline2016,
	address = {New York City, NY},
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {Adaptive {Algorithms} for {Online} {Convex} {Optimization} with {Long}-term {Constraints}},
	volume = {48},
	url = {http://proceedings.mlr.press/v48/jenatton16.html},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Jenatton, Rodolphe and Huang, Jim C. and Archambeau, Cédric},
	editor = {Balcan, Maria-Florina and Weinberger, Kilian Q.},
	year = {2016},
	keywords = {Untagged},
	pages = {402--411},
}

@inproceedings{johnsonDenseCapFullyConvolutional2016,
	address = {Las Vegas, NV, USA},
	title = {{DenseCap}: {Fully} {Convolutional} {Localization} {Networks} for {Dense} {Captioning}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {{DenseCap}},
	url = {http://ieeexplore.ieee.org/document/7780863/},
	doi = {10/ghmh9p},
	abstract = {We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efﬁcient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li},
	year = {2016},
	keywords = {Untagged},
	pages = {4565--4574},
}

@inproceedings{larsenAutoencodingPixelsUsing2016,
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {Autoencoding beyond pixels using a learned similarity metric},
	volume = {48},
	url = {http://proceedings.mlr.press/v48/larsen16.html},
	booktitle = {Proceedings of the 33nd {International} {Conference} on {Machine} {Learning}, {ICML} 2016, {New} {York} {City}, {NY}, {USA}, {June} 19-24, 2016},
	publisher = {JMLR.org},
	author = {Larsen, Anders Boesen Lindbo and Sønderby, Søren Kaae and Larochelle, Hugo and Winther, Ole},
	editor = {Balcan, Maria-Florina and Weinberger, Kilian Q.},
	year = {2016},
	keywords = {Untagged},
	pages = {1558--1566},
}

@inproceedings{leeGeneralizingPoolingFunctions2016,
	address = {Cadiz, Spain},
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {Generalizing {Pooling} {Functions} in {Convolutional} {Neural} {Networks}: {Mixed}, {Gated}, and {Tree}},
	volume = {51},
	url = {http://proceedings.mlr.press/v51/lee16a.html},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR.org},
	author = {Lee, Chen-Yu and Gallagher, Patrick W. and Tu, Zhuowen},
	editor = {Gretton, Arthur and Robert, Christian C.},
	year = {2016},
	keywords = {Untagged},
	pages = {464--472},
}

@article{liTernaryWeightNetworks2016,
	title = {Ternary {Weight} {Networks}},
	url = {http://arxiv.org/abs/1605.04711},
	abstract = {We introduce ternary weight networks (TWNs) - neural networks with weights constrained to +1, 0 and -1. The Euclidian distance between full (ﬂoat or double) precision weights and the ternary weights along with a scaling factor is minimized. Besides, a threshold-based ternary function is optimized to get an approximated solution which can be fast and easily computed. TWNs have stronger expressive abilities than recently proposed binary precision counterparts and are more effective than the latter. Meanwhile, TWNs achieve up to 16× or 32× model compression rate and need fewer multiplications compared with the full precision counterparts. Benchmarks on MNIST, CIFAR-10, and large scale ImageNet datasets show that the performance of TWNs is only slightly worse than the full precision counterparts but outperforms the analogous binary precision counterparts a lot.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:1605.04711 [cs]},
	author = {Li, Fengfu and Zhang, Bo and Liu, Bin},
	month = nov,
	year = {2016},
	note = {arXiv: 1605.04711},
	keywords = {Untagged},
}

@inproceedings{linLearningCompactBinary2016,
	address = {Las Vegas, NV, USA},
	title = {Learning {Compact} {Binary} {Descriptors} with {Unsupervised} {Deep} {Neural} {Networks}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780502/},
	doi = {10/gcmdh5},
	abstract = {In this paper, we propose a new unsupervised deep learning approach called DeepBit to learn compact binary descriptor for efﬁcient visual object matching. Unlike most existing binary descriptors which were designed with random projections or linear hash functions, we develop a deep neural network to learn binary descriptors in an unsupervised manner. We enforce three criterions on binary codes which are learned at the top layer of our network: 1) minimal loss quantization, 2) evenly distributed codes and 3) uncorrelated bits. Then, we learn the parameters of the networks with a back-propagation technique. Experimental results on three different visual analysis tasks including image matching, image retrieval, and object recognition clearly demonstrate the effectiveness of the proposed approach.},
	language = {en},
	urldate = {2021-01-28},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Lin, Kevin and Lu, Jiwen and Chen, Chu-Song and Zhou, Jie},
	year = {2016},
	keywords = {Untagged},
	pages = {1183--1192},
}

@inproceedings{liuOptimalBinaryCode2016,
	title = {Towards optimal binary code learning via ordinal embedding},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Liu, Hong and Ji, Rongrong and Wu, Yongjian and Liu, Wei},
	year = {2016},
	keywords = {Untagged},
	pages = {1258--1265},
}

@inproceedings{liuTwoStreamContextualizedCNN2016,
	title = {Two-{Stream} {Contextualized} {CNN} for {Fine}-{Grained} {Image} {Classification}},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Liu, Jiang and Gao, Chenqiang and Meng, Deyu and Zuo, Wangmeng},
	year = {2016},
	keywords = {Untagged},
	pages = {4232--4233},
}

@inproceedings{liuCoupledGenerativeAdversarial2016,
	title = {Coupled generative adversarial networks},
	booktitle = {{NeurIPS}},
	author = {Liu, M.-Y. and Tuzel, O.},
	year = {2016},
	keywords = {Untagged},
	pages = {469--477},
}

@inproceedings{longUnsupervisedDomainAdaptation2016,
	title = {Unsupervised domain adaptation with residual transfer networks},
	booktitle = {{NIPS}},
	author = {Long, M. and Zhu, H. and Wang, J. and Jordan, M. I},
	year = {2016},
	keywords = {Untagged},
	pages = {136--144},
}

@article{lucSemanticSegmentationUsing2016,
	title = {Semantic {Segmentation} using {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.08408},
	abstract = {Adversarial training has been shown to produce state of the art results for generative image modeling. In this paper we propose an adversarial training approach to train semantic segmentation models. We train a convolutional semantic segmentation network along with an adversarial network that discriminates segmentation maps coming either from the ground truth or from the segmentation network. The motivation for our approach is that it can detect and correct higher-order inconsistencies between ground truth segmentation maps and the ones produced by the segmentation net. Our experiments show that our adversarial training approach leads to improved accuracy on the Stanford Background and PASCAL VOC 2012 datasets.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1611.08408 [cs]},
	author = {Luc, Pauline and Couprie, Camille and Chintala, Soumith and Verbeek, Jakob},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.08408},
	keywords = {Untagged},
}

@inproceedings{maDiscreteCrossModalHashing2016,
	title = {Discrete {Cross}-{Modal} {Hashing} for {Efficient} {Multimedia} {Retrieval}},
	url = {https://doi.org/10.1109/ISM.2016.0017},
	doi = {10/ghjr5j},
	booktitle = {{IEEE} {International} {Symposium} on {Multimedia}},
	publisher = {IEEE Computer Society},
	author = {Ma, Dekui and Liang, Jian and Kong, Xiangwei and He, Ran and Li, Ying},
	year = {2016},
	keywords = {Untagged},
	pages = {38--43},
}

@article{makhzaniAdversarialAutoencoders2016,
	title = {Adversarial {Autoencoders}},
	url = {http://arxiv.org/abs/1511.05644},
	abstract = {In this paper, we propose the “adversarial autoencoder” (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classiﬁcation, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classiﬁcation tasks.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1511.05644 [cs]},
	author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
	month = may,
	year = {2016},
	note = {arXiv: 1511.05644},
	keywords = {Untagged},
}

@inproceedings{maoImageRestorationUsing2016,
	address = {Barcelona, Spain},
	title = {Image {Restoration} {Using} {Very} {Deep} {Convolutional} {Encoder}-{Decoder} {Networks} with {Symmetric} {Skip} {Connections}},
	url = {https://proceedings.neurips.cc/paper/2016/hash/0ed9422357395a0d4879191c66f4faa2-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Mao, Xiao-Jiao and Shen, Chunhua and Yang, Yu-Bin},
	editor = {Lee, Daniel D. and Sugiyama, Masashi and Luxburg, Ulrike von and Guyon, Isabelle and Garnett, Roman},
	year = {2016},
	keywords = {Untagged},
	pages = {2802--2810},
}

@inproceedings{medinaNoRegretAlgorithmsHeavyTailed2016,
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {No-{Regret} {Algorithms} for {Heavy}-{Tailed} {Linear} {Bandits}},
	volume = {48},
	url = {http://proceedings.mlr.press/v48/medina16.html},
	booktitle = {Proceedings of the 33nd {International} {Conference} on {Machine} {Learning}, {ICML} 2016, {New} {York} {City}, {NY}, {USA}, {June} 19-24, 2016},
	publisher = {JMLR.org},
	author = {Medina, Andres Muñoz and Yang, Scott},
	editor = {Balcan, Maria-Florina and Weinberger, Kilian Q.},
	year = {2016},
	keywords = {Untagged},
	pages = {1642--1650},
}

@inproceedings{moosavi-dezfooliDeepFoolSimpleAccurate2016,
	address = {Las Vegas, NV, USA},
	title = {{DeepFool}: {A} {Simple} and {Accurate} {Method} to {Fool} {Deep} {Neural} {Networks}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {{DeepFool}},
	url = {http://ieeexplore.ieee.org/document/7780651/},
	doi = {10.1109/CVPR.2016.282},
	language = {en},
	urldate = {2022-03-12},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
	month = jun,
	year = {2016},
	keywords = {Untagged},
	pages = {2574--2582},
}

@inproceedings{niepertLearningConvolutionalNeural2016,
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {Learning {Convolutional} {Neural} {Networks} for {Graphs}},
	volume = {48},
	url = {http://proceedings.mlr.press/v48/niepert16.html},
	urldate = {2022-02-25},
	booktitle = {Proceedings of the 33nd {International} {Conference} on {Machine} {Learning}, {ICML} 2016, {New} {York} {City}, {NY}, {USA}, {June} 19-24, 2016},
	publisher = {JMLR.org},
	author = {Niepert, Mathias and Ahmed, Mohamed and Kutzkov, Konstantin},
	editor = {Balcan, Maria-Florina and Weinberger, Kilian Q.},
	year = {2016},
	keywords = {Untagged},
	pages = {2014--2023},
}

@inproceedings{nowozinFGANTrainingGenerative2016,
	address = {Barcelona, Spain},
	title = {f-{GAN}: {Training} {Generative} {Neural} {Samplers} using {Variational} {Divergence} {Minimization}},
	url = {https://proceedings.neurips.cc/paper/2016/hash/cedebb6e872f539bef8c3f919874e9d7-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
	editor = {Lee, Daniel D. and Sugiyama, Masashi and Luxburg, Ulrike von and Guyon, Isabelle and Garnett, Roman},
	year = {2016},
	keywords = {Untagged},
	pages = {271--279},
}

@article{ordonezDeepConvolutionalLSTM2016,
	title = {Deep {Convolutional} and {LSTM} {Recurrent} {Neural} {Networks} for {Multimodal} {Wearable} {Activity} {Recognition}},
	volume = {16},
	issn = {1424-8220},
	shorttitle = {{DeepConvLSTM}},
	url = {https://www.mdpi.com/1424-8220/16/1/115},
	doi = {10.3390/s16010115},
	abstract = {Human activity recognition (HAR) tasks have traditionally been solved using engineered features obtained by heuristic processes. Current research suggests that deep convolutional neural networks are suited to automate feature extraction from raw sensor inputs. However, human activities are made of complex sequences of motor movements, and capturing this temporal dynamics is fundamental for successful HAR. Based on the recent success of recurrent neural networks for time series domains, we propose a generic deep framework for activity recognition based on convolutional and LSTM recurrent units, which: (i) is suitable for multimodal wearable sensors; (ii) can perform sensor fusion naturally; (iii) does not require expert knowledge in designing features; and (iv) explicitly models the temporal dynamics of feature activations. We evaluate our framework on two datasets, one of which has been used in a public activity recognition challenge. Our results show that our framework outperforms competing deep non-recurrent networks on the challenge dataset by 4\% on average; outperforming some of the previous reported results by up to 9\%. Our results show that the framework can be applied to homogeneous sensor modalities, but can also fuse multimodal sensors to improve performance. We characterise key architectural hyperparameters’ influence on performance to provide insights about their optimisation.},
	number = {1},
	journal = {Sensors},
	author = {Ordóñez, Francisco Javier and Roggen, Daniel},
	year = {2016},
	keywords = {Untagged},
}

@inproceedings{papernot_distillation_2016,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	booktitle = {{arXiv}:1511.04508},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	year = {2016},
	keywords = {Untagged},
}

@article{perarnauInvertibleConditionalGANs2016,
	title = {Invertible {Conditional} {GANs} for image editing},
	url = {http://arxiv.org/abs/1611.06355},
	abstract = {Generative Adversarial Networks (GANs) have recently demonstrated to successfully approximate complex data distributions. A relevant extension of this model is conditional GANs (cGANs), where the introduction of external information allows to determine speciﬁc representations of the generated images. In this work, we evaluate encoders to inverse the mapping of a cGAN, i.e., mapping a real image into a latent space and a conditional representation. This allows, for example, to reconstruct and modify real images of faces conditioning on arbitrary attributes. Additionally, we evaluate the design of cGANs. The combination of an encoder with a cGAN, which we call Invertible cGAN (IcGAN), enables to re-generate real images with deterministic complex modiﬁcations.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1611.06355 [cs]},
	author = {Perarnau, Guim and van de Weijer, Joost and Raducanu, Bogdan and Álvarez, Jose M.},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.06355},
	keywords = {Untagged},
}

@inproceedings{radenovicCNNImageRetrieval2016,
	address = {Cham},
	title = {{CNN} {Image} {Retrieval} {Learns} from {BoW}: {Unsupervised} {Fine}-{Tuning} with {Hard} {Examples}},
	volume = {9905},
	isbn = {978-3-319-46447-3 978-3-319-46448-0},
	shorttitle = {{CNN} {Image} {Retrieval} {Learns} from {BoW}},
	url = {http://link.springer.com/10.1007/978-3-319-46448-0_1},
	doi = {10.1007/978-3-319-46448-0_1},
	abstract = {Convolutional Neural Networks (CNNs) achieve state-of-theart performance in many computer vision tasks. However, this achievement is preceded by extreme manual annotation in order to perform either training from scratch or ﬁne-tuning for the target task. In this work, we propose to ﬁne-tune CNN for image retrieval from a large collection of unordered images in a fully automated manner. We employ state-of-the-art retrieval and Structure-from-Motion (SfM) methods to obtain 3D models, which are used to guide the selection of the training data for CNN ﬁne-tuning. We show that both hard positive and hard negative examples enhance the ﬁnal performance in particular object retrieval with compact codes.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer International Publishing},
	author = {Radenović, Filip and Tolias, Giorgos and Chum, Ondřej},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {3--20},
}

@inproceedings{radfordUnsupervisedRepresentationLearning2016,
	address = {San Juan, Puerto Rico},
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2016},
	keywords = {Untagged},
}

@inproceedings{rastegariXNORNetImageNetClassification2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{XNOR}-{Net}: {ImageNet} {Classification} {Using} {Binary} {Convolutional} {Neural} {Networks}},
	isbn = {978-3-319-46493-0},
	shorttitle = {{XNOR}-{Net}},
	doi = {10.1007/978-3-319-46493-0_32},
	abstract = {We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32××{\textbackslash}times memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58××{\textbackslash}times faster convolutional operations (in terms of number of the high precision operations) and 32××{\textbackslash}times memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than 16\%16\%16{\textbackslash},{\textbackslash}\% in top-1 accuracy. Our code is available at: http://allenai.org/plato/xnornet.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Untagged},
	pages = {525--542},
}

@inproceedings{reedGenerativeAdversarialText2016,
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {Generative {Adversarial} {Text} to {Image} {Synthesis}},
	volume = {48},
	url = {http://proceedings.mlr.press/v48/reed16.html},
	booktitle = {Proceedings of the 33nd {International} {Conference} on {Machine} {Learning}, {ICML} 2016, {New} {York} {City}, {NY}, {USA}, {June} 19-24, 2016},
	publisher = {JMLR.org},
	author = {Reed, Scott E. and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
	editor = {Balcan, Maria-Florina and Weinberger, Kilian Q.},
	year = {2016},
	keywords = {Untagged},
	pages = {1060--1069},
}

@article{renFasterRCNNRealTime2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with “attention” mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv: 1506.01497},
	keywords = {Untagged},
}

@inproceedings{scottreedLearningDeepRepresentations2016,
	title = {Learning deep representations of fine-grained visual descriptions},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Scott Reed, Zeynep Akata, Honglak Lee and Schiele, Bernt},
	year = {2016},
	keywords = {Untagged},
	pages = {49--58},
}

@article{shenRobustCrossviewHashing2016,
	title = {Robust {Cross}-view {Hashing} for {Multimedia} {Retrieval}},
	volume = {23},
	url = {https://doi.org/10.1109/LSP.2016.2517093},
	doi = {10/ggcvxd},
	number = {6},
	journal = {IEEE Signal Process. Lett.},
	author = {Shen, Xiao-Bo and Shen, Fumin and Sun, Quan-Sen and Yuan, Yunhao and Shen, Heng Tao},
	year = {2016},
	keywords = {Untagged},
	pages = {893--897},
}

@article{shiEdgeComputingVision2016,
	title = {Edge {Computing}: {Vision} and {Challenges}},
	volume = {3},
	issn = {2327-4662},
	shorttitle = {Edge {Computing}},
	doi = {10.1109/JIOT.2016.2579198},
	abstract = {The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.},
	number = {5},
	journal = {IEEE Internet of Things Journal},
	author = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
	year = {2016},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Untagged},
	pages = {637--646},
}

@inproceedings{sochorBoxcars3DBoxes2016,
	title = {Boxcars: {3D} boxes as cnn input for improved fine-grained vehicle recognition},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Sochor, J. and Herout, A. and Havel, J.},
	year = {2016},
	keywords = {Untagged},
	pages = {3006--3015},
}

@article{JMLR:v17:15-084,
	title = {A differential equation for modeling nesterov's accelerated gradient method: {Theory} and insights},
	volume = {17},
	url = {http://jmlr.org/papers/v17/15-084.html},
	number = {153},
	journal = {Journal of Machine Learning Research},
	author = {Su, Weijie and Boyd, Stephen and Candès, Emmanuel J.},
	year = {2016},
	keywords = {Untagged},
	pages = {1--43},
}

@inproceedings{sunDeepCoralCorrelation2016,
	title = {Deep coral: {Correlation} alignment for deep domain adaptation},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Sun, B. and Saenko, K.},
	year = {2016},
	keywords = {Untagged},
	pages = {443--450},
}

@inproceedings{tranRichImageCaptioning2016,
	address = {Las Vegas, NV, USA},
	title = {Rich {Image} {Captioning} in the {Wild}},
	isbn = {978-1-5090-1437-8},
	url = {http://ieeexplore.ieee.org/document/7789551/},
	doi = {10/ghv3nr},
	abstract = {We present an image caption system that addresses new challenges of automatically describing images in the wild. The challenges include high quality caption quality with respect to human judgments, out-of-domain data handling, and low latency required in many applications. Built on top of a state-of-the-art framework, we developed a deep vision model that detects a broad range of visual concepts, an entity recognition model that identiﬁes celebrities and landmarks, and a conﬁdence model for the caption output. Experimental results show that our caption engine outperforms previous state-of-the-art systems signiﬁcantly on both in-domain dataset (i.e. MS COCO) and out-of-domain datasets.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshop}},
	publisher = {IEEE},
	author = {Tran, Kenneth and He, Xiaodong and Zhang, Lei and Sun, Jian},
	year = {2016},
	keywords = {Untagged},
	pages = {434--441},
}

@inproceedings{ustinovaLearningDeepEmbeddings2016,
	address = {Barcelona, Spain},
	title = {Learning {Deep} {Embeddings} with {Histogram} {Loss}},
	url = {https://proceedings.neurips.cc/paper/2016/hash/325995af77a0e8b06d1204a171010b3a-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ustinova, Evgeniya and Lempitsky, Victor S.},
	editor = {Lee, Daniel D. and Sugiyama, Masashi and Luxburg, Ulrike von and Guyon, Isabelle and Garnett, Roman},
	year = {2016},
	keywords = {Untagged},
	pages = {4170--4178},
}

@inproceedings{vinyalsMatchingNetworksOne2016,
	address = {Barcelona, Spain},
	title = {Matching {Networks} for {One} {Shot} {Learning}},
	url = {https://proceedings.neurips.cc/paper/2016/hash/90e1357833654983612fb05e3ec9148c-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Tim and Kavukcuoglu, Koray and Wierstra, Daan},
	editor = {Lee, Daniel D. and Sugiyama, Masashi and Luxburg, Ulrike von and Guyon, Isabelle and Garnett, Roman},
	year = {2016},
	keywords = {Untagged},
	pages = {3630--3638},
}

@inproceedings{wangCNNRNNUnifiedFramework2016,
	address = {Las Vegas, NV},
	title = {{CNN}-{RNN}: {A} {Unified} {Framework} for {Multi}-label {Image} {Classification}},
	url = {https://doi.org/10.1109/CVPR.2016.251},
	doi = {10/gfzwdb},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Wang, Jiang and Yang, Yi and Mao, Junhua and Huang, Zhiheng and Huang, Chang and Xu, Wei},
	year = {2016},
	keywords = {Untagged},
	pages = {2285--2294},
}

@article{wangSemanticBoostingCrossModal2016,
	title = {Semantic {Boosting} {Cross}-{Modal} {Hashing} for efficient multimedia retrieval},
	volume = {330},
	url = {https://doi.org/10.1016/j.ins.2015.10.028},
	doi = {10/f75j9p},
	journal = {Information Sciences},
	author = {Wang, Ke and Tang, Jun and Wang, Nian and Shao, Ling},
	year = {2016},
	keywords = {Untagged},
	pages = {199--210},
}

@inproceedings{DBLP:conf/cvpr/WangLL16,
	title = {Learning deep structure-preserving image-text embeddings},
	url = {https://doi.org/10.1109/CVPR.2016.541},
	doi = {10.1109/CVPR.2016.541},
	booktitle = {2016 {IEEE} conference on computer vision and pattern recognition, {CVPR} 2016, las vegas, {NV}, {USA}, june 27-30, 2016},
	publisher = {IEEE Computer Society},
	author = {Wang, Liwei and Li, Yin and Lazebnik, Svetlana},
	year = {2016},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/cvpr/WangLL16.bib
tex.timestamp: Thu, 16 Dec 2021 17:36:03 +0100},
	keywords = {Untagged},
	pages = {5005--5013},
}

@article{wangDatabaseMeetsDeep2016,
	title = {Database {Meets} {Deep} {Learning}: {Challenges} and {Opportunities}},
	volume = {45},
	issn = {0163-5808},
	shorttitle = {Database {Meets} {Deep} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3003665.3003669},
	doi = {10/f866mr},
	abstract = {Deep learning has recently become very popular on account of its incredible success in many complex datadriven applications, including image classiﬁcation and speech recognition. The database community has worked on data-driven applications for many years, and therefore should be playing a lead role in supporting this new wave. However, databases and deep learning are different in terms of both techniques and applications. In this paper, we discuss research problems at the intersection of the two ﬁelds. In particular, we discuss possible improvements for deep learning systems from a database perspective, and analyze database applications that may beneﬁt from deep learning techniques.},
	language = {en},
	number = {2},
	urldate = {2021-01-27},
	journal = {ACM SIGMOD Record},
	author = {Wang, Wei and Zhang, Meihui and Chen, Gang and Jagadish, H. V. and Ooi, Beng Chin and Tan, Kian-Lee},
	month = sep,
	year = {2016},
	keywords = {Untagged},
	pages = {17--22},
}

@article{wangNoisySparseSubspace2016,
	title = {Noisy {Sparse} {Subspace} {Clustering}},
	volume = {17},
	url = {http://jmlr.org/papers/v17/13-354.html},
	journal = {J. Mach. Learn. Res.},
	author = {Wang, Yu-Xiang and Xu, Huan},
	year = {2016},
	keywords = {Untagged},
	pages = {12:1--12:41},
}

@inproceedings{wangUsingRandomizedResponse2016,
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Using {Randomized} {Response} for {Differential} {Privacy} {Preserving} {Data} {Collection}},
	volume = {1558},
	url = {http://ceur-ws.org/Vol-1558/paper35.pdf},
	booktitle = {Proceedings of the {Workshops} of the {EDBT}/{ICDT} 2016 {Joint} {Conference}, {EDBT}/{ICDT} {Workshops} 2016, {Bordeaux}, {France}, {March} 15, 2016},
	publisher = {CEUR-WS.org},
	author = {Wang, Yue and Wu, Xintao and Hu, Donghui},
	editor = {Palpanas, Themis and Stefanidis, Kostas},
	year = {2016},
	keywords = {Untagged},
}

@incollection{wasedaAnalyzingRandomizedResponse2016,
	address = {Cham},
	title = {Analyzing {Randomized} {Response} {Mechanisms} {Under} {Differential} {Privacy}},
	volume = {9866},
	isbn = {978-3-319-45870-0 978-3-319-45871-7},
	url = {http://link.springer.com/10.1007/978-3-319-45871-7_17},
	abstract = {The randomized response technique was ﬁrst introduced by Warner in 1965 [27] as a technique to survey sensitive questions. Since it is considered to protect the respondent’s privacy, many variants and applications have been proposed in the literature. Unfortunately, the randomized response and its variants have not been well evaluated from the privacy viewpoint historically. In this paper, we evaluate them by using diﬀerential privacy. Speciﬁcally, we show that some variants have a tradeoﬀ between the privacy and utility, and that the “negative” survey technique obtains negative results.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {Information {Security}},
	publisher = {Springer International Publishing},
	author = {Waseda, Atsushi and Nojima, Ryo},
	editor = {Bishop, Matt and Nascimento, Anderson C A},
	year = {2016},
	doi = {10.1007/978-3-319-45871-7_17},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {271--282},
}

@inproceedings{weiDeepNonlinearFeature2016,
	title = {Deep {Nonlinear} {Feature} {Coding} for {Unsupervised} {Domain} {Adaptation}.},
	booktitle = {{IJCAI}},
	author = {Wei, P. and Ke, Y. and Goh, C.-K.},
	year = {2016},
	keywords = {Untagged},
	pages = {2189--2195},
}

@article{whiteSamplingGenerativeNetworks2016,
	title = {Sampling {Generative} {Networks}},
	url = {http://arxiv.org/abs/1609.04468},
	abstract = {We introduce several techniques for sampling and visualizing the latent spaces of generative models. Replacing linear interpolation with spherical linear interpolation prevents diverging from a model’s prior distribution and produces sharper samples. J-Diagrams and MINE grids are introduced as visualizations of manifolds created by analogies and nearest neighbors. We demonstrate two new techniques for deriving attribute vectors: bias-corrected vectors with data replication and synthetic vectors with data augmentation. Binary classiﬁcation using attribute vectors is presented as a technique supporting quantitative analysis of the latent space. Most techniques are intended to be independent of model type and examples are shown on both Variational Autoencoders and Generative Adversarial Networks.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1609.04468 [cs, stat]},
	author = {White, Tom},
	month = dec,
	year = {2016},
	note = {arXiv: 1609.04468},
	keywords = {Untagged},
}

@article{wuRobustHashingMultiView2016,
	title = {Robust {Hashing} for {Multi}-{View} {Data}: {Jointly} {Learning} {Low}-{Rank} {Kernelized} {Similarity} {Consensus} and {Hash} {Functions}},
	shorttitle = {Robust {Hashing} for {Multi}-{View} {Data}},
	url = {http://arxiv.org/abs/1611.05521},
	abstract = {Learning hash functions/codes for similarity search over multi-view data is attracting increasing attention, where similar hash codes are assigned to the data objects characterizing consistently neighborhood relationship across views. Traditional methods in this category inherently suffer three limitations: 1) they commonly adopt a two-stage scheme where similarity matrix is ﬁrst constructed, followed by a subsequent hash function learning; 2) these methods are commonly developed on the assumption that data samples with multiple representations are noise-free,which is not practical in reallife applications; 3) they often incur cumbersome training model caused by the neighborhood graph construction using all N points in the database (O(N )). In this paper, we motivate the problem of jointly and efﬁciently training the robust hash functions over data objects with multi-feature representations which may be noise corrupted. To achieve both the robustness and training efﬁciency, we propose an approach to effectively and efﬁciently learning low-rank kernelized 1 hash functions shared across views. Speciﬁcally, we utilize landmark graphs to construct tractable similarity matrices in multi-views to automatically discover neighborhood structure in the data. To learn robust hash functions, a latent low-rank kernel function is used to construct hash functions in order to accommodate linearly inseparable data. In particular, a latent kernelized similarity matrix is recovered by rank minimization on multiple kernel-based similarity matrices. Extensive experiments on realworld multi-view datasets validate the efﬁcacy of our method in the presence of error corruptions.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1611.05521 [cs]},
	author = {Wu, Lin and Wang, Yang},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.05521},
	keywords = {Untagged},
}

@article{xieUnsupervisedMultigraphCrossmodal2016,
	title = {Unsupervised multi-graph cross-modal hashing for large-scale multimedia retrieval},
	volume = {75},
	url = {https://doi.org/10.1007/s11042-016-3432-0},
	doi = {10/f839ck},
	number = {15},
	journal = {Multim. Tools Appl.},
	author = {Xie, Liang and Zhu, Lei and Chen, Guoqi},
	year = {2016},
	keywords = {Untagged},
	pages = {9185--9204},
}

@inproceedings{xu_msr-vtt_2016,
	address = {Las Vegas, NV, USA},
	title = {{MSR}-{VTT}: {A} {Large} {Video} {Description} {Dataset} for {Bridging} {Video} and {Language}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {{MSR}-{VTT}},
	url = {http://ieeexplore.ieee.org/document/7780940/},
	doi = {10.1109/CVPR.2016.571},
	abstract = {While there has been increasing interest in the task of describing video with natural language, current computer vision algorithms are still severely limited in terms of the variability and complexity of the videos and their associated language that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on speciﬁc ﬁne-grained domains with limited videos and simple descriptions. While researchers have provided several benchmark datasets for image captioning, we are not aware of any large-scale video description dataset with comprehensive categories yet diverse video content.},
	language = {en},
	urldate = {2022-07-10},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
	month = jun,
	year = {2016},
	keywords = {Untagged},
	pages = {5288--5296},
}

@inproceedings{yangExploitBoundingBox2016,
	address = {Las Vegas, NV},
	title = {Exploit {Bounding} {Box} {Annotations} for {Multi}-{Label} {Object} {Recognition}},
	url = {https://doi.org/10.1109/CVPR.2016.37},
	doi = {10/ghv3n3},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Yang, Hao and Zhou, Joey Tianyi and Zhang, Yu and Gao, Bin-Bin and Wu, Jianxin and Cai, Jianfei},
	year = {2016},
	keywords = {Untagged},
	pages = {280--288},
}

@inproceedings{yehMatrixProfileAll2016,
	address = {Barcelona, Spain},
	title = {Matrix {Profile} {I}: {All} {Pairs} {Similarity} {Joins} for {Time} {Series}: {A} {Unifying} {View} {That} {Includes} {Motifs}, {Discords} and {Shapelets}},
	url = {https://doi.org/10.1109/ICDM.2016.0179},
	doi = {10.1109/ICDM.2016.0179},
	booktitle = {International {Conference} on {Data} {Mining}},
	publisher = {IEEE Computer Society},
	author = {Yeh, Chin-Chia Michael and Zhu, Yan and Ulanova, Liudmila and Begum, Nurjahan and Ding, Yifei and Dau, Hoang Anh and Silva, Diego Furtado and Mueen, Abdullah and Keogh, Eamonn J.},
	editor = {Bonchi, Francesco and Domingo-Ferrer, Josep and Baeza-Yates, Ricardo and Zhou, Zhi-Hua and Wu, Xindong},
	year = {2016},
	keywords = {Untagged},
	pages = {1317--1322},
}

@inproceedings{zagoruykoWideResidualNetworks2016,
	title = {Wide {Residual} {Networks}},
	isbn = {1-901725-59-6},
	shorttitle = {{WIdeResNet}},
	url = {https://dx.doi.org/10.5244/C.30.87},
	doi = {10/ggn9dm},
	booktitle = {British {Machine} {Vision} {Conference}},
	publisher = {BMVA Press},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	editor = {Richard C. Wilson, Edwin R. Hancock and Smith, William A. P.},
	year = {2016},
	keywords = {Untagged},
	pages = {87.1--87.12},
}

@article{zhangCompletingLowrankMatrices2016,
	title = {Completing low-rank matrices with corrupted samples from few coefficients in general basis},
	volume = {62},
	doi = {10.1109/TIT.2016.2573311},
	number = {8},
	journal = {IEEE Transactions on Information Theory},
	author = {Zhang, Hongyang and Lin, Zhouchen and Zhang, Chao},
	year = {2016},
	note = {Publisher: IEEE},
	keywords = {Untagged},
	pages = {4748--4768},
}

@inproceedings{zhangOnlineStochasticLinear2016,
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {Online {Stochastic} {Linear} {Optimization} under {One}-bit {Feedback}},
	volume = {48},
	url = {http://proceedings.mlr.press/v48/zhangb16.html},
	urldate = {2022-02-25},
	booktitle = {Proceedings of the 33nd {International} {Conference} on {Machine} {Learning}, {ICML} 2016, {New} {York} {City}, {NY}, {USA}, {June} 19-24, 2016},
	publisher = {JMLR.org},
	author = {Zhang, Lijun and Yang, Tianbao and Jin, Rong and Xiao, Yichi and Zhou, Zhi-Hua},
	editor = {Balcan, Maria-Florina and Weinberger, Kilian Q.},
	year = {2016},
	keywords = {Untagged},
	pages = {392--401},
}

@inproceedings{zhangEmbeddingLabelStructures2016,
	title = {Embedding label structures for fine-grained feature representation},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhang, Xiaofan and Zhou, Feng and Lin, Yuanqing and Zhang, Shaoting},
	year = {2016},
	keywords = {Untagged},
	pages = {1114--1123},
}

@article{zhengPersonReidentificationPresent2016,
	title = {Person {Re}-identification: {Past}, {Present} and {Future}},
	shorttitle = {Person {Re}-identification},
	url = {http://arxiv.org/abs/1610.02984},
	abstract = {Person re-identiﬁcation (re-ID) has become increasingly popular in the community due to its application and research signiﬁcance. It aims at spotting a person of interest in other cameras. In the early days, hand-crafted algorithms and small-scale evaluation were predominantly reported. Recent years have witnessed the emergence of large-scale datasets and deep learning systems which make use of large data volumes. Considering different tasks, we classify most current re-ID methods into two classes, i.e., image-based and video-based; in both tasks, hand-crafted and deep learning systems will be reviewed. Moreover, two new re-ID tasks which are much closer to real-world applications are described and discussed, i.e., end-to-end re-ID and fast re-ID in very large galleries. This paper: 1) introduces the history of person re-ID and its relationship with image classiﬁcation and instance retrieval; 2) surveys a broad selection of the hand-crafted systems and the large-scale methods in both image- and video-based re-ID; 3) describes critical future directions in end-to-end re-ID and fast retrieval in large galleries; and 4) ﬁnally briefs some important yet under-developed issues.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1610.02984 [cs]},
	author = {Zheng, Liang and Yang, Yi and Hauptmann, Alexander G.},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.02984},
	keywords = {Untagged},
}

@inproceedings{zhouLearningDeepFeatures2016,
	address = {Las Vegas, NV, USA},
	title = {Learning {Deep} {Features} for {Discriminative} {Localization}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780688/},
	doi = {10/gf2j8c},
	abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we ﬁnd that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation.We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classiﬁcation task1.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
	year = {2016},
	keywords = {Untagged},
	pages = {2921--2929},
}

@inproceedings{agarwalPriceDifferentialPrivacy2017,
	address = {Sydney, NSW, Australia},
	title = {The price of differential privacy for online learning},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Agarwal, Naman and Singh, Karan},
	year = {2017},
	keywords = {Untagged},
	pages = {32--40},
}

@inproceedings{antipovBoostingCrossageFace2017,
	address = {Denver, CO},
	title = {Boosting cross-age face verification via generative age normalization},
	isbn = {978-1-5386-1124-1},
	url = {http://ieeexplore.ieee.org/document/8272698/},
	doi = {10/ghv4cp},
	abstract = {Despite the tremendous progress in face veriﬁcation performance as a result of Deep Learning, the sensitivity to human age variations remains an Achilles’ heel of the majority of the contemporary face veriﬁcation software. A promising solution to this problem consists in synthetic aging/rejuvenation of the input face images to some predeﬁned age categories prior to face veriﬁcation. We recently proposed [3] Age-cGAN aging/rejuvenation method based on generative adversarial neural networks allowing to synthesize more plausible and realistic faces than alternative non-generative methods. However, in this work, we show that Age-cGAN cannot be directly used for improving face veriﬁcation due to its slightly imperfect preservation of the original identities in aged/rejuvenated faces. We therefore propose Local Manifold Adaptation (LMA) approach which resolves the stated issue of Age-cGAN resulting in the novel Age-cGAN+LMA aging/rejuvenation method. Based on Age-cGAN+LMA, we design an age normalization algorithm which boosts the accuracy of an off-the-shelf face veriﬁcation software in the cross-age evaluation scenario.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {2017 {IEEE} {International} {Joint} {Conference} on {Biometrics} ({IJCB})},
	publisher = {IEEE},
	author = {Antipov, Grigory and Baccouche, Moez and Dugelay, Jean-Luc},
	year = {2017},
	keywords = {Untagged},
	pages = {191--199},
}

@inproceedings{antipovFaceAgingConditional2017,
	title = {Face aging with conditional generative adversarial networks},
	url = {https://doi.org/10.1109/ICIP.2017.8296650},
	doi = {10/ghfq66},
	booktitle = {2017 {IEEE} {International} {Conference} on {Image} {Processing}, {ICIP} 2017, {Beijing}, {China}, {September} 17-20, 2017},
	publisher = {IEEE},
	author = {Antipov, Grigory and Baccouche, Moez and Dugelay, Jean-Luc},
	year = {2017},
	keywords = {Untagged},
	pages = {2089--2093},
}

@inproceedings{arjovskyPrincipledMethodsTraining2017,
	address = {Toulon, France},
	title = {Towards {Principled} {Methods} for {Training} {Generative} {Adversarial} {Networks}},
	url = {https://openreview.net/forum?id=Hk4_qw5xe},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Arjovsky, Martín and Bottou, Léon},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{arjovskyWassersteinGenerativeAdversarial2017,
	address = {International Convention Centre, Sydney, Australia},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Wasserstein {Generative} {Adversarial} {Networks}},
	volume = {70},
	url = {http://proceedings.mlr.press/v70/arjovsky17a.html},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = aug,
	year = {2017},
	keywords = {Untagged},
	pages = {214--223},
}

@inproceedings{aroraGeneralizationEquilibriumGenerative2017,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Generalization and {Equilibrium} in {Generative} {Adversarial} {Nets} ({GANs})},
	volume = {70},
	url = {http://proceedings.mlr.press/v70/arora17a.html},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}, {ICML} 2017, {Sydney}, {NSW}, {Australia}, 6-11 {August} 2017},
	publisher = {PMLR},
	author = {Arora, Sanjeev and Ge, Rong and Liang, Yingyu and Ma, Tengyu and Zhang, Yi},
	editor = {Precup, Doina and Teh, Yee Whye},
	year = {2017},
	keywords = {Untagged},
	pages = {224--232},
}

@inproceedings{balcanDifferentiallyPrivateClustering2017,
	title = {Differentially private clustering in high-dimensional euclidean spaces},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Balcan, Maria-Florina and Dick, Travis and Liang, Yingyu and Mou, Wenlong and Zhang, Hongyang},
	year = {2017},
	keywords = {Untagged},
	pages = {322--331},
}

@inproceedings{balcanSampleComputationallyEfficient2017,
	title = {Sample and computationally efficient learning algorithms under s-concave distributions},
	volume = {30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Balcan, Maria-Florina F. and Zhang, Hongyang},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{baoCVAEGANFineGrainedImage2017,
	address = {Venice},
	title = {{CVAE}-{GAN}: {Fine}-{Grained} {Image} {Generation} through {Asymmetric} {Training}},
	isbn = {978-1-5386-1032-9},
	shorttitle = {{CVAE}-{GAN}},
	url = {http://ieeexplore.ieee.org/document/8237561/},
	doi = {10/ghr7ks},
	abstract = {We present variational generative adversarial networks, a general learning framework that combines a variational auto-encoder with a generative adversarial network, for synthesizing images in ﬁne-grained categories, such as faces of a speciﬁc person or objects in a category. Our approach models an image as a composition of label and latent attributes in a probabilistic model. By varying the ﬁne-grained category label fed into the resulting generative model, we can generate images in a speciﬁc category with randomly drawn values on a latent attribute vector. Our approach has two novel aspects. First, we adopt a cross entropy loss for the discriminative and classiﬁer network, but a mean discrepancy objective for the generative network. This kind of asymmetric loss function makes the GAN training more stable. Second, we adopt an encoder network to learn the relationship between the latent space and the real image space, and use pairwise feature matching to keep the structure of generated images. We experiment with natural images of faces, ﬂowers, and birds, and demonstrate that the proposed models are capable of generating realistic and diverse samples with ﬁne-grained category labels. We further show that our models can be applied to other tasks, such as image inpainting, super-resolution, and data augmentation for training better face recognition models.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Bao, Jianmin and Chen, Dong and Wen, Fang and Li, Houqiang and Hua, Gang},
	year = {2017},
	keywords = {Untagged},
	pages = {2764--2773},
}

@inproceedings{bertasiusConvolutionalRandomWalk2017,
	address = {Honolulu, Hawaii},
	title = {Convolutional {Random} {Walk} {Networks} for {Semantic} {Image} {Segmentation}},
	url = {https://doi.org/10.1109/CVPR.2017.650},
	doi = {10/ghvcqs},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Bertasius, Gedas and Torresani, Lorenzo and Yu, Stella X. and Shi, Jianbo},
	year = {2017},
	keywords = {Untagged},
	pages = {6137--6145},
}

@inproceedings{bousmalisUnsupervisedPixellevelDomain2017,
	title = {Unsupervised pixel-level domain adaptation with generative adversarial networks},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Bousmalis, K. and Silberman, N. and Dohan, D. and Erhan, D. and Krishnan, D.},
	year = {2017},
	keywords = {Untagged},
	pages = {3722--3731},
}

@article{bressonSimultaneousLocalizationMapping2017,
	title = {Simultaneous {Localization} and {Mapping}: {A} {Survey} of {Current} {Trends} in {Autonomous} {Driving}},
	volume = {2},
	issn = {2379-8904},
	shorttitle = {Simultaneous {Localization} and {Mapping}},
	doi = {10.1109/TIV.2017.2749181},
	abstract = {In this paper, we propose a survey of the Simultaneous Localization And Mapping (SLAM) field when considering the recent evolution of autonomous driving. The growing interest regarding self-driving cars has given new directions to localization and mapping techniques. In this survey, we give an overview of the different branches of SLAM before going into the details of specific trends that are of interest when considered with autonomous applications in mind. We first present the limits of classical approaches for autonomous driving and discuss the criteria that are essential for this kind of application. We then review the methods where the identified challenges are tackled. We mostly focus on approaches building and reusing long-term maps in various conditions (weather, season, etc.). We also go through the emerging domain of multivehicle SLAM and its link with self-driving cars. We survey the different paradigms of that field (centralized and distributed) and the existing solutions. Finally, we conclude by giving an overview of the various large-scale experiments that have been carried out until now and discuss the remaining challenges and future orientations.},
	number = {3},
	journal = {IEEE Transactions on Intelligent Vehicles},
	author = {Bresson, Guillaume and Alsayed, Zayed and Yu, Li and Glaser, Sébastien},
	month = sep,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Intelligent Vehicles},
	keywords = {Untagged},
	pages = {194--220},
}

@inproceedings{caiHigherOrderIntegrationHierarchical2017,
	address = {Venice, Italy},
	title = {Higher-{Order} {Integration} of {Hierarchical} {Convolutional} {Activations} for {Fine}-{Grained} {Visual} {Categorization}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237325/},
	doi = {10/ggzw7w},
	abstract = {The success of ﬁne-grained visual categorization (FGVC) extremely relies on the modeling of appearance and interactions of various semantic parts. This makes FGVC very challenging because: (i) part annotation and detection require expert guidance and are very expensive; (ii) parts are of different sizes; and (iii) the part interactions are complex and of higher-order. To address these issues, we propose an end-to-end framework based on higherorder integration of hierarchical convolutional activations for FGVC. By treating the convolutional activations as local descriptors, hierarchical convolutional activations can serve as a representation of local parts from different scales. A polynomial kernel based predictor is proposed to capture higher-order statistics of convolutional activations for modeling part interaction. To model inter-layer part interactions, we extend polynomial predictor to integrate hierarchical activations via kernel fusion. Our work also provides a new perspective for combining convolutional activations from multiple layers. While hypercolumns simply concatenate maps from different layers, and holistically-nested network uses weighted fusion to combine side-outputs, our approach exploits higher-order intra-layer and inter-layer relations for better integration of hierarchical convolutional features. The proposed framework yields more discriminative representation and achieves competitive results on the widely used FGVC datasets.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Cai, Sijia and Zuo, Wangmeng and Zhang, Lei},
	year = {2017},
	keywords = {Untagged},
	pages = {511--520},
}

@inproceedings{caoHashNetDeepLearning2017,
	address = {Venice, Italy},
	title = {{HashNet}: {Deep} {Learning} to {Hash} by {Continuation}},
	isbn = {978-1-5386-1032-9},
	shorttitle = {{HashNet}},
	url = {http://ieeexplore.ieee.org/document/8237860/},
	doi = {10/gdm5d5},
	abstract = {Learning to hash has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval, due to its computation efﬁciency and retrieval quality. Deep learning to hash, which improves retrieval quality by end-to-end representation learning and hash encoding, has received increasing attention recently. Subject to the illposed gradient difﬁculty in the optimization with sign activations, existing deep learning to hash methods need to ﬁrst learn continuous representations and then generate binary hash codes in a separated binarization step, which suffer from substantial loss of retrieval quality. This work presents HashNet, a novel deep architecture for deep learning to hash by continuation method with convergence guarantees, which learns exactly binary hash codes from imbalanced similarity data. The key idea is to attack the ill-posed gradient problem in optimizing deep networks with non-smooth binary activations by continuation method, in which we begin from learning an easier network with smoothed activation function and let it evolve during the training, until it eventually goes back to being the original, difﬁcult to optimize, deep network with the sign activation function. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art multimedia retrieval performance on standard benchmarks.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Cao, Zhangjie and Long, Mingsheng and Wang, Jianmin and Yu, Philip S.},
	year = {2017},
	keywords = {Untagged},
	pages = {5609--5618},
}

@article{carlini_magnet_2017,
	title = {{MagNet} and "{Efficient} {Defenses} {Against} {Adversarial} {Attacks}" are {Not} {Robust} to {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1711.08478},
	doi = {10.48550/arXiv.1711.08478},
	abstract = {MagNet and "Efficient Defenses..." were recently proposed as a defense to adversarial examples. We find that we can construct adversarial examples that defeat these defenses with only a slight increase in distortion.},
	urldate = {2022-05-29},
	journal = {arXiv:1711.08478},
	author = {Carlini, Nicholas and Wagner, David},
	month = nov,
	year = {2017},
	note = {arXiv:1711.08478 [cs]
type: article},
	keywords = {Untagged},
}

@inproceedings{carlini_towards_2017,
	address = {san jose, CA},
	title = {Towards evaluating the robustness of neural networks},
	booktitle = {{IEEE} symposium on security and privacy},
	publisher = {IEEE Computer Society},
	author = {Carlini, Nicholas and Wagner, David A.},
	year = {2017},
	keywords = {Untagged},
	pages = {39--57},
}

@article{carreira-perpinanModelCompressionConstrained2017a,
	title = {Model compression as constrained optimization, with application to neural nets. {Part} {I}: general framework},
	shorttitle = {Model compression as constrained optimization, with application to neural nets. {Part} {I}},
	url = {http://arxiv.org/abs/1707.01209},
	abstract = {Compressing neural nets is an active research problem, given the large size of state-of-the-art nets for tasks such as object recognition, and the computational limits imposed by mobile devices. We give a general formulation of model compression as constrained optimization. This includes many types of compression: quantization, low-rank decomposition, pruning, lossless compression and others. Then, we give a general algorithm to optimize this nonconvex problem based on the augmented Lagrangian and alternating optimization. This results in a "learning-compression" algorithm, which alternates a learning step of the uncompressed model, independent of the compression type, with a compression step of the model parameters, independent of the learning task. This simple, efficient algorithm is guaranteed to find the best compressed model for the task in a local sense under standard assumptions. We present separately in several companion papers the development of this general framework into specific algorithms for model compression based on quantization, pruning and other variations, including experimental results on compressing neural nets and other models.},
	urldate = {2022-02-28},
	journal = {arXiv:1707.01209 [cs, math, stat]},
	author = {Carreira-Perpiñán, Miguel Á},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.01209},
	keywords = {Untagged},
}

@article{carreira-perpinanModelCompressionConstrained2017,
	title = {Model compression as constrained optimization, with application to neural nets. {Part} {II}: quantization},
	shorttitle = {Model compression as constrained optimization, with application to neural nets. {Part} {II}},
	url = {http://arxiv.org/abs/1707.04319},
	abstract = {We consider the problem of deep neural net compression by quantization: given a large, reference net, we want to quantize its real-valued weights using a codebook with \$K\$ entries so that the training loss of the quantized net is minimal. The codebook can be optimally learned jointly with the net, or fixed, as for binarization or ternarization approaches. Previous work has quantized the weights of the reference net, or incorporated rounding operations in the backpropagation algorithm, but this has no guarantee of converging to a loss-optimal, quantized net. We describe a new approach based on the recently proposed framework of model compression as constrained optimization {\textbackslash}citep\{Carreir17a\}. This results in a simple iterative "learning-compression" algorithm, which alternates a step that learns a net of continuous weights with a step that quantizes (or binarizes/ternarizes) the weights, and is guaranteed to converge to local optimum of the loss for quantized nets. We develop algorithms for an adaptive codebook or a (partially) fixed codebook. The latter includes binarization, ternarization, powers-of-two and other important particular cases. We show experimentally that we can achieve much higher compression rates than previous quantization work (even using just 1 bit per weight) with negligible loss degradation.},
	urldate = {2022-02-28},
	journal = {arXiv:1707.04319 [cs, math, stat]},
	author = {Carreira-Perpiñán, Miguel Á and Idelbayev, Yerlan},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.04319},
	keywords = {Untagged},
}

@article{chaurasiaLinkNetExploitingEncoder2017,
	title = {{LinkNet}: {Exploiting} {Encoder} {Representations} for {Efficient} {Semantic} {Segmentation}},
	shorttitle = {{LinkNet}},
	url = {http://arxiv.org/abs/1707.03718},
	doi = {10/gdq9vq},
	abstract = {Pixel-wise semantic segmentation for visual scene understanding not only needs to be accurate, but also efﬁcient in order to ﬁnd any use in real-time application. Existing algorithms even though are accurate but they do not focus on utilizing the parameters of neural network efﬁciently. As a result they are huge in terms of parameters and number of operations; hence slow too. In this paper, we propose a novel deep neural network architecture which allows it to learn without any signiﬁcant increase in number of parameters. Our network uses only 11.5 million parameters and 21.2 GFLOPs for processing an image of resolution 3 × 640 × 360. It gives state-of-the-art performance on CamVid and comparable results on Cityscapes dataset. We also compare our networks processing time on NVIDIA GPU and embedded system device with existing state-of-the-art architectures for different image resolutions.},
	language = {en},
	urldate = {2021-01-27},
	journal = {2017 IEEE Visual Communications and Image Processing (VCIP)},
	author = {Chaurasia, Abhishek and Culurciello, Eugenio},
	year = {2017},
	keywords = {Untagged},
	pages = {1--4},
}

@inproceedings{chenFastPersonReidentification2017,
	address = {Honolulu, HI},
	title = {Fast {Person} {Re}-identification via {Cross}-{Camera} {Semantic} {Binary} {Transformation}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100049/},
	doi = {10/ghv22s},
	abstract = {Numerous methods have been proposed for person reidentiﬁcation, most of which however neglect the matching efﬁciency. Recently, several hashing based approaches have been developed to make re-identiﬁcation more scalable for large-scale gallery sets. Despite their efﬁciency, these works ignore cross-camera variations, which severely deteriorate the ﬁnal matching accuracy. To address the above issues, we propose a novel hashing based method for fast person re-identiﬁcation, namely Cross-camera Semantic Binary Transformation (CSBT). CSBT aims to transform original high-dimensional feature vectors into compact identitypreserving binary codes. To this end, CSBT ﬁrst employs a subspace projection to mitigate cross-camera variations, by maximizing intra-person similarities and inter-person discrepancies. Subsequently, a binary coding scheme is proposed via seamlessly incorporating both the semantic pairwise relationships and local afﬁnity information. Finally, a joint learning framework is proposed for simultaneous subspace projection learning and binary coding based on discrete alternating optimization. Experimental results on four benchmarks clearly demonstrate the superiority of CSBT over the state-of-the-art methods.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Chen, Jiaxin and Wang, Yunhong and Qin, Jie and Liu, Li and Shao, Ling},
	year = {2017},
	keywords = {Untagged},
	pages = {5330--5339},
}

@inproceedings{chenOutlierDetectionAutoencoder2017,
	address = {Houston, Texas},
	title = {Outlier {Detection} with {Autoencoder} {Ensembles}},
	url = {https://doi.org/10.1137/1.9781611974973.11},
	doi = {10.1137/1.9781611974973.11},
	booktitle = {{SIAM} {International} {Conference} on {Data} {Mining}},
	publisher = {SIAM},
	author = {Chen, Jinghui and Sathe, Saket and Aggarwal, Charu C. and Turaga, Deepak S.},
	editor = {Chawla, Nitesh V. and Wang, Wei},
	year = {2017},
	keywords = {Untagged},
	pages = {90--98},
}

@misc{chen_rethinking_2017,
	title = {Rethinking {Atrous} {Convolution} for {Semantic} {Image} {Segmentation}},
	shorttitle = {{DeepLabv3}},
	url = {http://arxiv.org/abs/1706.05587},
	doi = {10.48550/arXiv.1706.05587},
	abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	month = dec,
	year = {2017},
	note = {arXiv:1706.05587 [cs]},
	keywords = {Untagged},
}

@inproceedings{chenShowAdaptTell2017,
	address = {Venice},
	title = {Show, {Adapt} and {Tell}: {Adversarial} {Training} of {Cross}-{Domain} {Image} {Captioner}},
	isbn = {978-1-5386-1032-9},
	shorttitle = {Show, {Adapt} and {Tell}},
	url = {http://ieeexplore.ieee.org/document/8237326/},
	doi = {10/gf5bpr},
	urldate = {2020-11-05},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Chen, Tseng-Hung and Liao, Yuan-Hong and Chuang, Ching-Yao and Hsu, Wan-Ting and Fu, Jianlong and Sun, Min},
	year = {2017},
	keywords = {Untagged},
	pages = {521--530},
}

@inproceedings{chenPeGaSusDataAdaptiveDifferentially2017,
	address = {Dallas Texas USA},
	title = {{PeGaSus}: {Data}-{Adaptive} {Differentially} {Private} {Stream} {Processing}},
	isbn = {978-1-4503-4946-8},
	shorttitle = {{PeGaSus}},
	url = {https://dl.acm.org/doi/10.1145/3133956.3134102},
	doi = {10/ghv3jz},
	abstract = {Individuals are continually observed by an ever-increasing number of sensors that make up the Internet of Things. The resulting streams of data, which are analyzed in real time, can reveal sensitive personal information about individuals. Hence, there is an urgent need for stream processing solutions that can analyze these data in real time with provable guarantees of privacy and low error.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {Proceedings of the 2017 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Chen, Yan and Machanavajjhala, Ashwin and Hay, Michael and Miklau, Gerome},
	month = oct,
	year = {2017},
	keywords = {Untagged},
	pages = {1375--1388},
}

@inproceedings{chenLearningLearnGradient2017,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Learning to {Learn} without {Gradient} {Descent} by {Gradient} {Descent}},
	volume = {70},
	url = {http://proceedings.mlr.press/v70/chen17e.html},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}, {ICML} 2017, {Sydney}, {NSW}, {Australia}, 6-11 {August} 2017},
	publisher = {PMLR},
	author = {Chen, Yutian and Hoffman, Matthew W. and Colmenarejo, Sergio Gomez and Denil, Misha and Lillicrap, Timothy P. and Botvinick, Matthew and Freitas, Nando de},
	editor = {Precup, Doina and Teh, Yee Whye},
	year = {2017},
	keywords = {Untagged},
	pages = {748--756},
}

@inproceedings{cuiKernelPoolingConvolutional2017,
	title = {Kernel pooling for convolutional neural networks},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Cui, Y. and Zhou, F. and Wang, J. and Liu, X. and Lin, Y. and Belongie, S. J},
	year = {2017},
	keywords = {Untagged},
	pages = {2921--2930},
}

@techreport{cummingsLecturePropertiesDifferential2017,
	title = {Lecture 5: {Properties} of {Diﬀerential} {Privacy} \& {Sparse} {Vector}},
	language = {en},
	author = {Cummings, Rachel and Kim, Nayeon},
	month = sep,
	year = {2017},
	keywords = {Untagged},
	pages = {4},
}

@inproceedings{cutkoskyStochasticAdversarialOnline2017,
	address = {Long Beach, CA},
	title = {Stochastic and {Adversarial} {Online} {Learning} without {Hyperparameters}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/6aed000af86a084f9cb0264161e29dd3-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Cutkosky, Ashok and Boahen, Kwabena A.},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {5059--5067},
}

@inproceedings{daiStochasticGenerativeHashing2017,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Stochastic {Generative} {Hashing}},
	volume = {70},
	url = {http://proceedings.mlr.press/v70/dai17a.html},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}, {ICML} 2017, {Sydney}, {NSW}, {Australia}, 6-11 {August} 2017},
	publisher = {PMLR},
	author = {Dai, Bo and Guo, Ruiqi and Kumar, Sanjiv and He, Niao and Song, Le},
	editor = {Precup, Doina and Teh, Yee Whye},
	year = {2017},
	keywords = {Untagged},
	pages = {913--922},
}

@inproceedings{donahueAdversarialFeatureLearning2017,
	address = {Toulon, France},
	title = {Adversarial {Feature} {Learning}},
	url = {https://openreview.net/forum?id=BJtNZAFgg},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Donahue, Jeff and Krähenbühl, Philipp and Darrell, Trevor},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{dumoulinAdversariallyLearnedInference2017,
	address = {Toulon, France},
	title = {Adversarially {Learned} {Inference}},
	url = {https://openreview.net/forum?id=B1ElR4cgg},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Lamb, Alex and Arjovsky, Martín and Mastropietro, Olivier and Courville, Aaron C.},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{dumoulinLearnedRepresentationArtistic2017,
	address = {Toulon, France},
	title = {A {Learned} {Representation} {For} {Artistic} {Style}},
	url = {https://openreview.net/forum?id=BJO-BuT1g},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Dumoulin, Vincent and Shlens, Jonathon and Kudlur, Manjunath},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{dwibediCutPasteLearn2017,
	title = {Cut, {Paste} and {Learn}: {Surprisingly} {Easy} {Synthesis} for {Instance} {Detection}},
	url = {https://doi.org/10.1109/ICCV.2017.146},
	doi = {10/ghn8j8},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}, {ICCV} 2017, {Venice}, {Italy}, {October} 22-29, 2017},
	publisher = {IEEE Computer Society},
	author = {Dwibedi, Debidatta and Misra, Ishan and Hebert, Martial},
	year = {2017},
	keywords = {Untagged},
	pages = {1310--1319},
}

@article{dworkCalibratingNoiseSensitivity2017,
	title = {Calibrating {Noise} to {Sensitivity} in {Private} {Data} {Analysis}},
	volume = {7},
	url = {https://journalprivacyconfidentiality.org/index.php/jpc/article/view/405},
	doi = {10/ghf8xc},
	number = {3},
	journal = {Journal of Privacy and Confidentiality},
	author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
	year = {2017},
	keywords = {Untagged},
	pages = {17--51},
}

@inproceedings{enUnsupervisedDeepHashing2017,
	address = {Beijing},
	title = {Unsupervised deep hashing with stacked convolutional autoencoders},
	isbn = {978-1-5090-2175-8},
	url = {http://ieeexplore.ieee.org/document/8296917/},
	doi = {10/ghv3cz},
	abstract = {Learning-based image hashing consists in turning highdimensional image features into compact binary codes, while preserving their semantic similarity (i.e., if two images are close in terms of content, their codes should be close as well). In this context, many existing hashing techniques rely on supervision for preserving these semantic properties. In this paper, we aim at learning such binary codes by exploiting the underlying structure of unlabeled data, using deep learning. The proposed deep network is based on a stacked convolutional autoencoder which hierarchically maps input images into a low-dimensional space. A binary relaxation constraint applied to the middle layer of the network – the one containing the code – makes the codes sparse and binary. To demonstrate the competitiveness of the proposed architecture, we evaluate the so produced hash codes on image retrieval and image classiﬁcation tasks on the MNIST dataset, and compare its performance with state-of-the-art approaches.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {2017 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	publisher = {IEEE},
	author = {En, Sovann and Cremilleux, Bruno and Jurie, Frederic},
	year = {2017},
	keywords = {Untagged},
	pages = {3420--3424},
}

@inproceedings{fangRMPERegionalMultiperson2017,
	title = {{RMPE}: {Regional} {Multi}-person {Pose} {Estimation}},
	url = {https://doi.org/10.1109/ICCV.2017.256},
	doi = {10/ggn89s},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}, {ICCV} 2017, {Venice}, {Italy}, {October} 22-29, 2017},
	publisher = {IEEE Computer Society},
	author = {Fang, Haoshu and Xie, Shuqin and Tai, Yu-Wing and Lu, Cewu},
	year = {2017},
	keywords = {Untagged},
	pages = {2353--2362},
}

@inproceedings{finn_model-agnostic_2017,
	title = {Model-{Agnostic} {Meta}-{Learning} for {Fast} {Adaptation} of {Deep} {Networks}},
	shorttitle = {{MAML}},
	url = {https://proceedings.mlr.press/v70/finn17a.html},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	year = {2017},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {1126--1135},
}

@inproceedings{franceschiForwardReverseGradientBased2017,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Forward and {Reverse} {Gradient}-{Based} {Hyperparameter} {Optimization}},
	volume = {70},
	url = {http://proceedings.mlr.press/v70/franceschi17a.html},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}, {ICML} 2017, {Sydney}, {NSW}, {Australia}, 6-11 {August} 2017},
	publisher = {PMLR},
	author = {Franceschi, Luca and Donini, Michele and Frasconi, Paolo and Pontil, Massimiliano},
	editor = {Precup, Doina and Teh, Yee Whye},
	year = {2017},
	keywords = {Untagged},
	pages = {1165--1173},
}

@inproceedings{fuLookCloserSee2017,
	title = {Look closer to see better: {Recurrent} attention convolutional neural network for fine-grained image recognition},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Fu, Jianlong and Zheng, Heliang and Mei, Tao},
	year = {2017},
	keywords = {Untagged},
	pages = {4438--4446},
}

@inproceedings{gadelha3DShapeInduction2017,
	address = {Qingdao, China},
	title = {{3D} {Shape} {Induction} from {2D} {Views} of {Multiple} {Objects}},
	url = {https://doi.org/10.1109/3DV.2017.00053},
	doi = {10/ghwnhk},
	booktitle = {International {Conference} on {3D} {Vision}},
	publisher = {IEEE Computer Society},
	author = {Gadelha, Matheus and Maji, Subhransu and Wang, Rui},
	year = {2017},
	keywords = {Untagged},
	pages = {402--411},
}

@inproceedings{gebruFinegrainedRecognitionWild2017,
	title = {Fine-grained recognition in the wild: {A} multi-task domain adaptation approach},
	booktitle = {International {Conference} on {Computer} {Vision}},
	author = {Gebru, T. and Hoffman, J. and Fei-Fei, L.},
	year = {2017},
	keywords = {Untagged},
	pages = {1358--1367},
}

@inproceedings{gebruFineGrainedCarDetection2017,
	title = {Fine-{Grained} {Car} {Detection} for {Visual} {Census} {Estimation}},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Gebru, Timnit and Krause, Jonathan and Wang, Yilun and Chen, Duyun and Deng, Jia and Fei-Fei, Li},
	year = {2017},
	keywords = {Untagged},
	pages = {4502--4508},
}

@inproceedings{gomezReversibleResidualNetwork2017,
	title = {The {Reversible} {Residual} {Network}: {Backpropagation} {Without} {Storing} {Activations}},
	volume = {30},
	shorttitle = {The {Reversible} {Residual} {Network}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html},
	urldate = {2021-10-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},
	year = {2017},
	keywords = {Untagged},
}

@article{goodfellowNIPS2016Tutorial2017,
	title = {{NIPS} 2016 {Tutorial}: {Generative} {Adversarial} {Networks}},
	shorttitle = {{NIPS} 2016 {Tutorial}},
	url = {http://arxiv.org/abs/1701.00160},
	abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1701.00160 [cs]},
	author = {Goodfellow, Ian},
	month = apr,
	year = {2017},
	note = {arXiv: 1701.00160},
	keywords = {Untagged},
}

@inproceedings{goriMultiTypeActivityRecognition2017,
	address = {Melbourne, Australia},
	title = {Multi-{Type} {Activity} {Recognition} from a {Robot}'s {Viewpoint}},
	url = {https://doi.org/10.24963/ijcai.2017/680},
	doi = {10.24963/ijcai.2017/680},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {ijcai.org},
	author = {Gori, Ilaria and Aggarwal, J. K. and Matthies, Larry H. and Ryoo, Michael S.},
	editor = {Sierra, Carles},
	year = {2017},
	keywords = {Untagged},
	pages = {4849--4853},
}

@article{goyalACtuALActorCriticAdversarial2017,
	title = {{ACtuAL}: {Actor}-{Critic} {Under} {Adversarial} {Learning}},
	shorttitle = {{ACtuAL}},
	url = {http://arxiv.org/abs/1711.04755},
	abstract = {Generative Adversarial Networks (GANs) are a powerful framework for deep generative modeling. Posed as a two-player minimax problem, GANs are typically trained end-to-end on real-valued data and can be used to train a generator of high-dimensional and realistic images. However, a major limitation of GANs is that training relies on passing gradients from the discriminator through the generator via back-propagation. This makes it fundamentally difﬁcult to train GANs with discrete data, as generation in this case typically involves a non-differentiable function. These difﬁculties extend to the reinforcement learning setting when the action space is composed of discrete decisions. We address these issues by reframing the GAN framework so that the generator is no longer trained using gradients through the discriminator, but is instead trained using a learned critic in the actorcritic framework with a Temporal Difference (TD) objective. This is a natural ﬁt for sequence modeling and we use it to achieve improvements on language modeling tasks over the standard Teacher-Forcing methods.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1711.04755 [cs, stat]},
	author = {Goyal, Anirudh and Ke, Nan Rosemary and Lamb, Alex and Hjelm, R. Devon and Pal, Chris and Pineau, Joelle and Bengio, Yoshua},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.04755},
	keywords = {Untagged},
}

@article{GRAVINA201768,
	title = {Multi-sensor fusion in body sensor networks: {State}-of-the-art and research challenges},
	volume = {35},
	issn = {1566-2535},
	url = {https://www.sciencedirect.com/science/article/pii/S156625351630077X},
	doi = {10/gf69m8},
	abstract = {Body Sensor Networks (BSNs) have emerged as a revolutionary technology in many application domains in health-care, fitness, smart cities, and many other compelling Internet of Things (IoT) applications. Most commercially available systems assume that a single device monitors a plethora of user information. In reality, BSN technology is transitioning to multi-device synchronous measurement environments; fusion of the data from multiple, potentially heterogeneous, sensor sources is therefore becoming a fundamental yet non-trivial task that directly impacts application performance. Nevertheless, only recently researchers have started developing technical solutions for effective fusion of BSN data. To the best of our knowledge, the community is currently lacking a comprehensive review of the state-of-the-art techniques on multi-sensor fusion in the area of BSN. This survey discusses clear motivations and advantages of multi-sensor data fusion and particularly focuses on physical activity recognition, aiming at providing a systematic categorization and common comparison framework of the literature, by identifying distinctive properties and parameters affecting data fusion design choices at different levels (data, feature, and decision). The survey also covers data fusion in the domains of emotion recognition and general-health and introduce relevant directions and challenges of future research on multi-sensor fusion in the BSN domain.},
	journal = {Information Fusion},
	author = {Gravina, Raffaele and Alinia, Parastoo and Ghasemzadeh, Hassan and Fortino, Giancarlo},
	year = {2017},
	keywords = {Untagged},
	pages = {68--80},
}

@inproceedings{gulrajaniImprovedTrainingWasserstein2017,
	address = {Long Beach, CA},
	title = {Improved {Training} of {Wasserstein} {GANs}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martín and Dumoulin, Vincent and Courville, Aaron C.},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {5767--5777},
}

@inproceedings{hausserAssociativeDomainAdaptation2017,
	address = {Venice, Italy},
	title = {Associative {Domain} {Adaptation}},
	url = {https://doi.org/10.1109/ICCV.2017.301},
	doi = {10/gfx48r},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE Computer Society},
	author = {Häusser, Philip and Frerix, Thomas and Mordvintsev, Alexander and Cremers, Daniel},
	year = {2017},
	keywords = {Untagged},
	pages = {2784--2792},
}

@inproceedings{hausserLearningAssociationVersatile2017,
	address = {Honolulu, HI},
	title = {Learning by {Association} - {A} {Versatile} {Semi}-{Supervised} {Training} {Method} for {Neural} {Networks}},
	url = {https://doi.org/10.1109/CVPR.2017.74},
	doi = {10/gf3s5s},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Häusser, Philip and Mordvintsev, Alexander and Cremers, Daniel},
	year = {2017},
	keywords = {Untagged},
	pages = {626--635},
}

@inproceedings{heFineGrainedImageClassification2017,
	address = {Honolulu, HI},
	title = {Fine-{Grained} {Image} {Classification} via {Combining} {Vision} and {Language}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100258/},
	doi = {10/ggzw7b},
	abstract = {Fine-grained image classiﬁcation is a challenging task due to the large intra-class variance and small inter-class variance, aiming at recognizing hundreds of sub-categories belonging to the same basic-level category. Most existing ﬁne-grained image classiﬁcation methods generally learn part detection models to obtain the semantic parts for better classiﬁcation accuracy. Despite achieving promising results, these methods mainly have two limitations: (1) not all the parts which obtained through the part detection models are beneﬁcial and indispensable for classiﬁcation, and (2) ﬁne-grained image classiﬁcation requires more detailed visual descriptions which could not be provided by the part locations or attribute annotations. For addressing the above two limitations, this paper proposes the two-stream model combining vision and language (CVL) for learning latent semantic representations. The vision stream learns deep representations from the original visual information via deep convolutional neural network. The language stream utilizes the natural language descriptions which could point out the discriminative parts or characteristics for each image, and provides a ﬂexible and compact way of encoding the salient visual aspects for distinguishing sub-categories. Since the two streams are complementary, combining the two streams can further achieves better classiﬁcation accuracy. Comparing with 12 state-ofthe-art methods on the widely used CUB-200-2011 dataset for ﬁne-grained image classiﬁcation, the experimental results demonstrate our CVL approach achieves the best performance.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {He, Xiangteng and Peng, Yuxin},
	month = jul,
	year = {2017},
	keywords = {Untagged},
	pages = {7332--7340},
}

@inproceedings{hendricks_localizing_2017,
	address = {Venice},
	title = {Localizing {Moments} in {Video} with {Natural} {Language}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237880/},
	doi = {10.1109/ICCV.2017.618},
	abstract = {We consider retrieving a speciﬁc temporal segment, or moment, from a video given a natural language text description. Methods designed to retrieve whole video clips with natural language determine what occurs in a video but not when. To address this issue, we propose the Moment Context Network (MCN) which effectively localizes natural language queries in videos by integrating local and global video features over time. A key obstacle to training our MCN model is that current video datasets do not include pairs of localized video segments and referring expressions, or text descriptions which uniquely identify a corresponding moment. Therefore, we collect the Distinct Describable Moments (DiDeMo) dataset which consists of over 10,000 unedited, personal videos in diverse visual settings with pairs of localized video segments and referring expressions. We demonstrate that MCN outperforms several baseline methods and believe that our initial results together with the release of DiDeMo will inspire further research on localizing video moments with natural language.},
	language = {en},
	urldate = {2023-05-22},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Hendricks, Lisa Anne and Wang, Oliver and Shechtman, Eli and Sivic, Josef and Darrell, Trevor and Russell, Bryan},
	month = oct,
	year = {2017},
	keywords = {Untagged},
	pages = {5804--5813},
}

@inproceedings{heuselGANsTrainedTwo2017,
	address = {Long Beach, CA},
	title = {{GANs} {Trained} by a {Two} {Time}-{Scale} {Update} {Rule} {Converge} to a {Local} {Nash} {Equilibrium}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {6626--6637},
}

@article{hoangMultiGeneratorGenerativeAdversarial2017,
	title = {Multi-{Generator} {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1708.02556},
	abstract = {We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classiﬁer, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classiﬁer speciﬁes which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as ﬁnal output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efﬁciently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1708.02556 [cs, stat]},
	author = {Hoang, Quan and Nguyen, Tu Dinh and Le, Trung and Phung, Dinh},
	month = oct,
	year = {2017},
	note = {arXiv: 1708.02556},
	keywords = {Untagged},
}

@article{holohanOptimalDifferentiallyPrivate2017,
	title = {Optimal {Differentially} {Private} {Mechanisms} for {Randomised} {Response}},
	volume = {12},
	url = {https://doi.org/10.1109/TIFS.2017.2718487},
	doi = {10/ghv4n2},
	number = {11},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Holohan, Naoise and Leith, Douglas J. and Mason, Oliver},
	year = {2017},
	keywords = {Untagged},
	pages = {2726--2735},
}

@inproceedings{houLossawareBinarizationDeep2017,
	title = {Loss-aware {Binarization} of {Deep} {Networks}},
	url = {https://openreview.net/forum?id=S1oWlN9ll},
	abstract = {Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the...},
	language = {en},
	urldate = {2022-02-28},
	booktitle = {{ICLR}},
	author = {Hou, Lu and Yao, Quanming and Kwok, James T.},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{houVegFruDomainSpecificDataset2017,
	address = {Venice},
	title = {{VegFru}: {A} {Domain}-{Specific} {Dataset} for {Fine}-{Grained} {Visual} {Categorization}},
	isbn = {978-1-5386-1032-9},
	shorttitle = {{VegFru}},
	url = {http://ieeexplore.ieee.org/document/8237328/},
	doi = {10/gf5bpg},
	abstract = {In this paper, we propose a novel domain-speciﬁc dataset named VegFru for ﬁne-grained visual categorization (FGVC). While the existing datasets for FGVC are mainly focused on animal breeds or man-made objects with limited labelled data, VegFru is a larger dataset consisting of vegetables and fruits which are closely associated with the daily life of everyone. Aiming at domestic cooking and food management, VegFru categorizes vegetables and fruits according to their eating characteristics, and each image contains at least one edible part of vegetables or fruits with the same cooking usage. Particularly, all the images are labelled hierarchically. The current version covers vegetables and fruits of 25 upper-level categories and 292 subordinate classes. And it contains more than 160,000 images in total and at least 200 images for each subordinate class. Accompanying the dataset, we also propose an effective framework called HybridNet to exploit the label hierarchy for FGVC. Speciﬁcally, multiple granularity features are ﬁrst extracted by dealing with the hierarchical labels separately. And then they are fused through explicit operation, e.g., Compact Bilinear Pooling, to form a uniﬁed representation for the ultimate recognition. The experimental results on the novel VegFru, the public FGVC-Aircraft and CUB-200-2011 indicate that HybridNet achieves one of the top performance on these datasets. The dataset and code are available at https://github.com/ustc-vim/vegfru.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Hou, Saihui and Feng, Yushan and Wang, Zilei},
	year = {2017},
	keywords = {Untagged},
	pages = {541--549},
}

@article{howardMobileNetsEfficientConvolutional2017,
	title = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
	shorttitle = {{MobileNets}},
	url = {http://arxiv.org/abs/1704.04861},
	abstract = {We present a class of efﬁcient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depthwise separable convolutions to build light weight deep neural networks. We introduce two simple global hyperparameters that efﬁciently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classiﬁcation. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, ﬁnegrain classiﬁcation, face attributes and large scale geo-localization.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1704.04861 [cs]},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.04861},
	keywords = {Untagged},
}

@inproceedings{huangFaceRotationGlobal2017,
	title = {Beyond {Face} {Rotation}: {Global} and {Local} {Perception} {GAN} for {Photorealistic} and {Identity} {Preserving} {Frontal} {View} {Synthesis}},
	url = {https://doi.org/10.1109/ICCV.2017.267},
	doi = {10/ghwnht},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}, {ICCV} 2017, {Venice}, {Italy}, {October} 22-29, 2017},
	publisher = {IEEE Computer Society},
	author = {Huang, Rui and Zhang, Shu and Li, Tianyu and He, Ran},
	year = {2017},
	keywords = {Untagged},
	pages = {2458--2467},
}

@inproceedings{huangCoarseFineNetworkKeypoint2017,
	address = {Venice},
	title = {A {Coarse}-{Fine} {Network} for {Keypoint} {Localization}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237591/},
	doi = {10/gfsjfw},
	abstract = {We propose a coarse-ﬁne network (CFN) that exploits multi-level supervisions for keypoint localization. Recently, convolutional neural networks (CNNs)-based methods have achieved great success due to the powerful hierarchical features in CNNs. These methods typically use conﬁdence maps generated from ground-truth keypoint locations as supervisory signals. However, while some keypoints can be easily located with high accuracy, many of them are hard to localize due to appearance ambiguity. Thus, using strict supervision often fails to detect keypoints that are difﬁcult to locate accurately. To target this problem, we develop a keypoint localization network composed of several coarse detector branches, each of which is built on top of a feature layer in a CNN, and a ﬁne detector branch built on top of multiple feature layers. We supervise each branch by a speciﬁed label map to explicate a certain supervision strictness level. All the branches are uniﬁed principally to produce the ﬁnal accurate keypoint locations. We demonstrate the efﬁcacy, efﬁciency, and generality of our method on several benchmarks for multiple tasks including bird part localization and human body pose estimation. Especially, our method achieves 72.2\% AP on the 2016 COCO Keypoints Challenge dataset, which is an 18\% improvement over the winning entry.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Huang, Shaoli and Gong, Mingming and Tao, Dacheng},
	year = {2017},
	keywords = {Untagged},
	pages = {3047--3056},
}

@article{iizukaGloballyLocallyConsistent2017,
	title = {Globally and locally consistent image completion},
	volume = {36},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3072959.3073659},
	doi = {10/gbxf4k},
	language = {en},
	number = {4},
	urldate = {2021-01-27},
	journal = {ACM Transactions on Graphics},
	author = {Iizuka, Satoshi and Simo-Serra, Edgar and Ishikawa, Hiroshi},
	month = jul,
	year = {2017},
	keywords = {Untagged},
	pages = {1--14},
}

@article{ioffeBatchRenormalizationReducing2017,
	title = {Batch {Renormalization}: {Towards} {Reducing} {Minibatch} {Dependence} in {Batch}-{Normalized} {Models}},
	shorttitle = {Batch {Renormalization}},
	url = {http://arxiv.org/abs/1702.03275},
	abstract = {Batch Normalization is quite effective at accelerating and improving the training of deep models. However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples. We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference. We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-i.i.d. minibatches. At the same time, Batch Renormalization retains the beneﬁts of batchnorm such as insensitivity to initialization and training efﬁciency.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:1702.03275 [cs]},
	author = {Ioffe, Sergey},
	month = mar,
	year = {2017},
	note = {arXiv: 1702.03275},
	keywords = {Untagged},
}

@inproceedings{isolaImagetoImageTranslationConditional2017,
	address = {Honolulu, HI},
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {https://doi.org/10.1109/CVPR.2017.632},
	doi = {10/gfrfv9},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	year = {2017},
	keywords = {Untagged},
	pages = {5967--5976},
}

@inproceedings{iwasawaPrivacyIssuesRegarding2017,
	address = {Melbourne, Australia},
	title = {Privacy {Issues} {Regarding} the {Application} of {DNNs} to {Activity}-{Recognition} using {Wearables} and {Its} {Countermeasures} by {Use} of {Adversarial} {Training}},
	url = {https://doi.org/10.24963/ijcai.2017/268},
	doi = {10.24963/ijcai.2017/268},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {ijcai.org},
	author = {Iwasawa, Yusuke and Nakayama, Kotaro and Yairi, Ikuko and Matsuo, Yutaka},
	editor = {Sierra, Carles},
	year = {2017},
	keywords = {Untagged},
	pages = {1930--1936},
}

@inproceedings{jia_adversarial_2017,
	address = {Copenhagen, Denmark},
	title = {Adversarial {Examples} for {Evaluating} {Reading} {Comprehension} {Systems}},
	url = {https://aclanthology.org/D17-1215},
	doi = {10.18653/v1/D17-1215},
	abstract = {Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75\% F1 score to 36\%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7\%. We hope our insights will motivate the development of new models that understand language more precisely.},
	urldate = {2022-11-08},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Jia, Robin and Liang, Percy},
	month = sep,
	year = {2017},
	keywords = {Untagged},
	pages = {2021--2031},
}

@misc{jigsaw_introducing_2017,
	title = {Introducing the {False} {Positive}},
	url = {https://medium.com/@JigsawTeam/introducing-the-false-positive-dcaef45b9a72},
	abstract = {Hi folks. We’re a group of researchers, scientists, and technologists who work at Google and Jigsaw on a research effort called…},
	language = {en},
	urldate = {2022-11-07},
	journal = {Medium},
	author = {Jigsaw},
	month = sep,
	year = {2017},
	keywords = {Untagged},
}

@article{kairouzCompositionTheoremDifferential2017,
	title = {The {Composition} {Theorem} for {Differential} {Privacy}},
	volume = {63},
	issn = {0018-9448, 1557-9654},
	url = {http://ieeexplore.ieee.org/document/7883827/},
	doi = {10/gbgxq5},
	abstract = {Sequential querying of differentially private mechanisms degrades the overall privacy level. In this paper, we answer the fundamental question of characterizing the level of overall privacy degradation as a function of the number of queries and the privacy levels maintained by each privatization mechanism. Our solution is complete: we prove an upper bound on the overall privacy level and construct a sequence of privatization mechanisms that achieves this bound. The key innovation is the introduction of an operational interpretation of differential privacy (involving hypothesis testing) and the use of new data processing inequalities. Our result improves over the state-of-the-art and has immediate applications to several problems studied in the literature.},
	language = {en},
	number = {6},
	urldate = {2021-01-27},
	journal = {IEEE Transactions on Information Theory},
	author = {Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},
	month = jun,
	year = {2017},
	keywords = {Untagged},
	pages = {4037--4049},
}

@article{kaiserOneModelLearn2017,
	title = {One {Model} {To} {Learn} {Them} {All}},
	url = {http://arxiv.org/abs/1706.05137},
	abstract = {Deep learning yields great results across many ﬁelds, from speech recognition, image classiﬁcation, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data beneﬁt largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1706.05137 [cs, stat]},
	author = {Kaiser, Lukasz and Gomez, Aidan N. and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.05137},
	keywords = {Untagged},
}

@inproceedings{karlinskyFineGrainedRecognitionThousands2017,
	address = {Honolulu, HI},
	title = {Fine-{Grained} {Recognition} of {Thousands} of {Object} {Categories} with {Single}-{Example} {Training}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099592/},
	doi = {10/gfxcxb},
	abstract = {We approach the problem of fast detection and recognition of a large number (thousands) of object categories while training on a very limited amount of examples, usually one per category. Examples of this task include: (i) detection of retail products, where we have only one studio image of each product available for training; (ii) detection of brand logos; and (iii) detection of 3D objects and their respective poses within a static 2D image, where only a sparse subset of (partial) object views is available for training, with a single example for each view. Building a detector based on so few examples presents a signiﬁcant challenge for the current top-performing (deep) learning based techniques, which require large amounts of data to train. Our approach for this task is based on a non-parametric probabilistic model for initial detection, CNN-based reﬁnement and temporal integration where applicable. We successfully demonstrate its usefulness in a variety of experiments on both existing and our own benchmarks achieving state-ofthe-art performance.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Karlinsky, Leonid and Shtok, Joseph and Tzur, Yochay and Tzadok, Asaf},
	year = {2017},
	keywords = {Untagged},
	pages = {965--974},
}

@inproceedings{keLightGBMHighlyEfficient2017,
	title = {{LightGBM}: {A} {Highly} {Efficient} {Gradient} {Boosting} {Decision} {Tree}},
	shorttitle = {{LightGBM}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html},
	urldate = {2022-02-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2017, {December} 4-9, 2017, {Long} {Beach}, {CA}, {USA}},
	author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {3146--3154},
}

@inproceedings{kendallWhatUncertaintiesWe2017,
	address = {Long Beach, CA},
	title = {What {Uncertainties} {Do} {We} {Need} in {Bayesian} {Deep} {Learning} for {Computer} {Vision}?},
	url = {https://proceedings.neurips.cc/paper/2017/hash/2650d6089a6d640c5e85b2b88265dc2b-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Kendall, Alex and Gal, Yarin},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {5574--5584},
}

@inproceedings{keskarImprovingGeneralizationPerformance2017,
	title = {Improving {Generalization} {Performance} by {Switching} from {Adam} to {SGD}},
	shorttitle = {{SWATS}},
	url = {http://arxiv.org/abs/1712.07628},
	abstract = {Despite superior training outcomes, adaptive optimization methods such as Adam, Adagrad or RMSprop have been found to generalize poorly compared to Stochastic gradient descent (SGD). These methods tend to perform well in the initial portion of training but are outperformed by SGD at later stages of training. We investigate a hybrid strategy that begins training with an adaptive method and switches to SGD when appropriate. Concretely, we propose SWATS, a simple strategy which Switches from Adam to SGD when a triggering condition is satisﬁed. The condition we propose relates to the projection of Adam steps on the gradient subspace. By design, the monitoring process for this condition adds very little overhead and does not increase the number of hyperparameters in the optimizer. We report experiments on several standard benchmarks such as: ResNet, SENet, DenseNet and PyramidNet for the CIFAR-10 and CIFAR-100 data sets, ResNet on the tiny-ImageNet data set and language modeling with recurrent networks on the PTB and WT2 data sets. The results show that our strategy is capable of closing the generalization gap between SGD and Adam on a majority of the tasks.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{arXiv}:1712.07628 [cs, math]},
	author = {Keskar, Nitish Shirish and Socher, Richard},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.07628},
	keywords = {Untagged},
}

@inproceedings{klambauerSelfNormalizingNeuralNetworks2017,
	title = {Self-{Normalizing} {Neural} {Networks}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html},
	urldate = {2022-02-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2017, {December} 4-9, 2017, {Long} {Beach}, {CA}, {USA}},
	author = {Klambauer, Günter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {971--980},
}

@inproceedings{kongLowrankBilinearPooling2017,
	title = {Low-rank bilinear pooling for fine-grained classification},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Kong, S. and Fowlkes, C.},
	year = {2017},
	keywords = {Untagged},
	pages = {365--374},
}

@inproceedings{koppelParsimoniousOnlineLearning2017,
	address = {New Orleans, LA},
	title = {Parsimonious {Online} {Learning} with {Kernels} via sparse projections in function space},
	isbn = {978-1-5090-4117-6},
	url = {http://ieeexplore.ieee.org/document/7953042/},
	doi = {10/ggv957},
	abstract = {Despite their attractiveness, popular perception is that techniques for nonparametric function approximation do not scale to streaming data due to an intractable growth in the amount of storage they require. To solve this problem in a memory-aﬀordable way, we propose an online technique based on functional stochastic gradient descent in tandem with supervised sparsiﬁcation based on greedy function subspace projections. The method, called parsimonious online learning with kernels (POLK), provides a controllable tradeoﬀ between its solution accuracy and the amount of memory it requires. We derive conditions under which the generated function sequence converges almost surely to the optimal function, and we establish that the memory requirement remains ﬁnite. We evaluate POLK for kernel multi-class logistic regression and kernel hinge-loss classiﬁcation on three canonical data sets: a synthetic Gaussian mixture model, the MNIST hand-written digits, and the Brodatz texture database. On all three tasks, we observe a favorable trade-oﬀ of objective function evaluation, classiﬁcation performance, and complexity of the nonparametric regressor extracted by the proposed method.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {2017 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Koppel, Alec and Warnell, Garrett and Stump, Ethan and Ribeiro, Alejandro},
	year = {2017},
	keywords = {Untagged},
	pages = {4671--4675},
}

@article{krishnaVisualGenomeConnecting2017,
	title = {Visual genome: {Connecting} language and vision using crowdsourced dense image annotations},
	volume = {123},
	doi = {10/f96kc4},
	number = {1},
	journal = {IJCV},
	author = {Krishna, R. and Zhu, Y. and Groth, O. and Johnson, J. and Hata, K. and Kravitz, J. and Chen, S. and Kalantidis, Y. and Li, L.-J. and Shamma, D. A. and Bernstein, M.-S. and L, Fei-Fei},
	year = {2017},
	keywords = {Untagged},
	pages = {32--73},
}

@article{krizhevskyImageNetClassificationDeep2017,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {60},
	issn = {0001-0782},
	doi = {10/gbhhxs},
	number = {6},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2017},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Untagged},
	pages = {84--90},
}

@inproceedings{lamFineGrainedRecognitionHSnet2017,
	address = {Honolulu, HI},
	title = {Fine-{Grained} {Recognition} as {HSnet} {Search} for {Informative} {Image} {Parts}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100171/},
	doi = {10/gfxcxc},
	abstract = {This work addresses ﬁne-grained image classiﬁcation. Our work is based on the hypothesis that when dealing with subtle differences among object classes it is critical to identify and only account for a few informative image parts, as the remaining image context may not only be uninformative but may also hurt recognition. This motivates us to formulate our problem as a sequential search for informative parts over a deep feature map produced by a deep Convolutional Neural Network (CNN). A state of this search is a set of proposal bounding boxes in the image, whose “informativeness” is evaluated by the heuristic function (H), and used for generating new candidate states by the successor function (S). The two functions are uniﬁed via a Long Short-Term Memory network (LSTM) into a new deep recurrent architecture, called HSnet. Thus, HSnet (i) generates proposals of informative image parts and (ii) fuses all proposals toward ﬁnal ﬁne-grained recognition. We specify both supervised and weakly supervised training of HSnet depending on the availability of object part annotations. Evaluation on the benchmark Caltech-UCSD Birds 2002011 and Cars-196 datasets demonstrate our competitive performance relative to the state of the art.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Lam, Michael and Mahasseni, Behrooz and Todorovic, Sinisa},
	year = {2017},
	keywords = {Untagged},
	pages = {6497--6506},
}

@inproceedings{ledigPhotoRealisticSingleImage2017,
	address = {Honolulu, HI},
	title = {Photo-{Realistic} {Single} {Image} {Super}-{Resolution} {Using} a {Generative} {Adversarial} {Network}},
	url = {https://doi.org/10.1109/CVPR.2017.19},
	doi = {10/ggmhhp},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew P. and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	year = {2017},
	keywords = {Untagged},
	pages = {105--114},
}

@inproceedings{li_deeper_2017,
	address = {Venice, Italy},
	title = {Deeper, {Broader} and {Artier} {Domain} {Generalization}},
	shorttitle = {{PACS}},
	url = {https://doi.org/10.1109/ICCV.2017.591},
	doi = {10.1109/ICCV.2017.591},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE Computer Society},
	author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M.},
	year = {2017},
	keywords = {Untagged},
	pages = {5543--5551},
}

@inproceedings{liTrainingQuantizedNets2017,
	title = {Training {Quantized} {Nets}: {A} {Deeper} {Understanding}},
	volume = {30},
	shorttitle = {Training {Quantized} {Nets}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/1c303b0eed3133200cf715285011b4e4-Abstract.html},
	urldate = {2022-02-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Hao and De, Soham and Xu, Zheng and Studer, Christoph and Samet, Hanan and Goldstein, Tom},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{liLearningOptimize2017,
	address = {Toulon, France},
	title = {Learning to {Optimize}},
	url = {https://openreview.net/forum?id=ry4Vrt5gl},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Li, Ke and Malik, Jitendra},
	year = {2017},
	keywords = {Untagged},
}

@article{liLearningOptimizeNeural2017,
	title = {Learning to {Optimize} {Neural} {Nets}},
	url = {http://arxiv.org/abs/1703.00441},
	abstract = {Learning to Optimize (Li \& Malik, 2016) is a recently proposed framework for learning optimization algorithms using reinforcement learning. In this paper, we explore learning an optimization algorithm for training shallow neural nets. Such high-dimensional stochastic optimization problems present interesting challenges for existing reinforcement learning algorithms. We develop an extension that is suited to learning optimization algorithms in this setting and demonstrate that the learned optimization algorithm consistently outperforms other known optimization algorithms even on unseen tasks and is robust to changes in stochasticity of gradients and the neural net architecture. More speciﬁcally, we show that an optimization algorithm trained with the proposed method on the problem of training a neural net on MNIST generalizes to the problems of training neural nets on the Toronto Faces Dataset, CIFAR-10 and CIFAR100.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1703.00441 [cs, math, stat]},
	author = {Li, Ke and Malik, Jitendra},
	month = nov,
	year = {2017},
	note = {arXiv: 1703.00441},
	keywords = {Untagged},
}

@article{liHyperbandNovelBanditBased2017,
	title = {Hyperband: {A} {Novel} {Bandit}-{Based} {Approach} to {Hyperparameter} {Optimization}},
	volume = {18},
	url = {http://jmlr.org/papers/v18/16-558.html},
	journal = {J. Mach. Learn. Res.},
	author = {Li, Lisha and Jamieson, Kevin G. and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
	year = {2017},
	keywords = {Untagged},
	pages = {185:1--185:52},
}

@inproceedings{liRegionbasedActivityRecognition2017,
	address = {Mountain View, CA},
	title = {Region-based {Activity} {Recognition} {Using} {Conditional} {GAN}},
	url = {https://doi.org/10.1145/3123266.3123365},
	doi = {10.1145/3123266.3123365},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Li, Xinyu and Zhang, Yanyi and Zhang, Jianyu and Chen, Yueyang and Li, Huangcan and Marsic, Ivan and Burd, Randall S.},
	editor = {Liu, Qiong and Lienhart, Rainer and Wang, Haohong and Chen, Sheng-Wei "Kuan-Ta" and Boll, Susanne and Chen, Yi-Ping Phoebe and Friedland, Gerald and Li, Jia and Yan, Shuicheng},
	year = {2017},
	keywords = {Untagged},
	pages = {1059--1067},
}

@inproceedings{liRevisitingBatchNormalization2017,
	title = {Revisiting batch normalization for practical domain adaptation},
	booktitle = {{ICLRW}},
	author = {Li, Y. and Wang, N. and Shi, J. and Liu, J. and Hou, X.},
	year = {2017},
	keywords = {Untagged},
	pages = {1--12},
}

@inproceedings{liGenerativeFaceCompletion2017,
	address = {Honolulu, HI},
	title = {Generative {Face} {Completion}},
	url = {https://doi.org/10.1109/CVPR.2017.624},
	doi = {10/ghv4b8},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Li, Yijun and Liu, Sifei and Yang, Jimei and Yang, Ming-Hsuan},
	year = {2017},
	keywords = {Untagged},
	pages = {5892--5900},
}

@inproceedings{liConvergenceAnalysisTwolayer2017,
	address = {Long Beach, CA},
	title = {Convergence {Analysis} of {Two}-layer {Neural} {Networks} with {ReLU} {Activation}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/a96b65a721e561e1e3de768ac819ffbb-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Li, Yuanzhi and Yuan, Yang},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {597--607},
}

@inproceedings{liDualingGANs2017,
	address = {Long Beach, CA},
	title = {Dualing {GANs}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/12a1d073d5ed3fa12169c67c4e2ce415-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Li, Yujia and Schwing, Alexander G. and Wang, Kuan-Chieh and Zemel, Richard S.},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {5606--5616},
}

@inproceedings{liEndtoEndAdversarialMemory2017,
	title = {End-to-{End} {Adversarial} {Memory} {Network} for {Cross}-domain {Sentiment} {Classification}.},
	booktitle = {{IJCAI}},
	author = {Li, Z. and Zhang, Y. and Wei, Y. and Wu, Y. and Yang, Q.},
	year = {2017},
	keywords = {Untagged},
	pages = {2237--2243},
}

@article{liangGenerativeSemanticManipulation2017,
	title = {Generative {Semantic} {Manipulation} with {Contrasting} {GAN}},
	url = {http://arxiv.org/abs/1708.00315},
	abstract = {Generative Adversarial Networks (GANs) have recently achieved signiﬁcant improvement on paired/unpaired image-to-image translation, such as photo→ sketch and artist painting style transfer. However, existing models can only be capable of transferring the low-level information (e.g. color or texture changes), but fail to edit high-level semantic meanings (e.g., geometric structure or content) of objects. On the other hand, while some researches can synthesize compelling real-world images given a class label or caption, they cannot condition on arbitrary shapes or structures, which largely limits their application scenarios and interpretive capability of model results. In this work, we focus on a more challenging semantic manipulation task, which aims to modify the semantic meaning of an object while preserving its own characteristics (e.g. viewpoints and shapes), such as cow→sheep, motor→ bicycle, cat→dog. To tackle such large semantic changes, we introduce a contrasting GAN (contrast-GAN) with a novel adversarial contrasting objective. Instead of directly making the synthesized samples close to target data as previous GANs did, our adversarial contrasting objective optimizes over the distance comparisons between samples, that is, enforcing the manipulated data be semantically closer to the real data with target category than the input data. Equipped with the new contrasting objective, a novel mask-conditional contrast-GAN architecture is proposed to enable disentangle image background with object semantic changes. Experiments on several semantic manipulation tasks on ImageNet and MSCOCO dataset show considerable performance gain by our contrast-GAN over other conditional GANs. Quantitative results further demonstrate the superiority of our model on generating manipulated results with high visual ﬁdelity and reasonable object semantics.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1708.00315 [cs]},
	author = {Liang, Xiaodan and Zhang, Hao and Xing, Eric P.},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.00315},
	keywords = {Untagged},
}

@inproceedings{linFeaturePyramidNetworks2017,
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.html},
	urldate = {2021-11-14},
	author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	year = {2017},
	keywords = {Untagged},
	pages = {2117--2125},
}

@book{linLowrankModelsVisual2017,
	title = {Low-rank models in visual analysis: {Theories}, algorithms, and applications},
	shorttitle = {Low-rank models in visual analysis},
	publisher = {Academic Press},
	author = {Lin, Zhouchen and Zhang, Hongyang},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{liongLearningCrossmodalHashing2017,
	title = {Learning a cross-modal hashing network for multimedia search},
	url = {https://doi.org/10.1109/ICIP.2017.8296973},
	doi = {10/ghjr5g},
	booktitle = {{IEEE} {International} {Conference} on {Image} {Processing}},
	publisher = {IEEE},
	author = {Liong, Venice Erin and Lu, Jiwen and Tan, Yap-Peng},
	year = {2017},
	keywords = {Untagged},
	pages = {3700--3704},
}

@article{liuCrossConvolutionalLayerPoolingImage2017,
	title = {Cross-{Convolutional}-{Layer} {Pooling} for {Image} {Recognition}},
	volume = {39},
	url = {https://doi.org/10.1109/TPAMI.2016.2637921},
	doi = {10/gb2259},
	number = {11},
	journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	author = {Liu, Lingqiao and Shen, Chunhua and Hengel, Anton van den},
	year = {2017},
	keywords = {Untagged},
	pages = {2305--2313},
}

@inproceedings{liuUnsupervisedImagetoImageTranslation2017,
	address = {Long Beach, CA},
	title = {Unsupervised {Image}-to-{Image} {Translation} {Networks}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/dc6a6489640ca02b0d42dabeb8e46bb7-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {700--708},
}

@inproceedings{liuFaceAgingContextual2017,
	title = {Face {Aging} with {Contextual} {Generative} {Adversarial} {Nets}},
	url = {https://doi.org/10.1145/3123266.3123431},
	doi = {10/ghwnm2},
	booktitle = {Proceedings of the 2017 {ACM} on {Multimedia} {Conference}, {MM} 2017, {Mountain} {View}, {CA}, {USA}, {October} 23-27, 2017},
	publisher = {ACM},
	author = {Liu, Si and Sun, Yao and Zhu, Defa and Bao, Renda and Wang, Wei and Shu, Xiangbo and Yan, Shuicheng},
	editor = {Liu, Qiong and Lienhart, Rainer and Wang, Haohong and Chen, Sheng-Wei "Kuan-Ta" and Boll, Susanne and Chen, Yi-Ping Phoebe and Friedland, Gerald and Li, Jia and Yan, Shuicheng},
	year = {2017},
	keywords = {Untagged},
	pages = {82--90},
}

@inproceedings{liuLocalizingDescribingAttributeGuided2017,
	title = {Localizing by {Describing}: {Attribute}-{Guided} {Attention} {Localization} for {Fine}-{Grained} {Recognition}},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Liu, Xiao and Wang, Jiang and Wen, Shilei and Ding, Errui and Lin, Yuanqing},
	year = {2017},
	keywords = {Untagged},
	pages = {4190--4196},
}

@inproceedings{longDeepTransferLearning2017,
	title = {Deep transfer learning with joint adaptation networks},
	booktitle = {{ICML}},
	author = {Long, M. and Zhu, H. and Wang, J. and Jordan, M. I},
	year = {2017},
	keywords = {Untagged},
	pages = {2208--2217},
}

@inproceedings{longLearningMultipleTasks2017,
	address = {Long Beach, CA},
	title = {Learning {Multiple} {Tasks} with {Multilinear} {Relationship} {Networks}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/03e0704b5690a2dee1861dc3ad3316c9-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Long, Mingsheng and Cao, Zhangjie and Wang, Jianmin and Yu, Philip S.},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {1594--1603},
}

@article{luDeepBinaryRepresentation2017,
	title = {Deep {Binary} {Representation} for {Efficient} {Image} {Retrieval}},
	volume = {2017},
	issn = {1687-5680, 1687-5699},
	url = {https://www.hindawi.com/journals/am/2017/8961091/},
	doi = {10.1155/2017/8961091},
	abstract = {With the fast growing number of images uploaded every day, efficient content-based image retrieval becomes important. Hashing method, which means representing images in binary codes and using Hamming distance to judge similarity, is widely accepted for its advantage in storage and searching speed. A good binary representation method for images is the determining factor of image retrieval. In this paper, we propose a new deep hashing method for efficient image retrieval. We propose an algorithm to calculate the target hash code which indicates the relationship between images of different contents. Then the target hash code is fed to the deep network for training. Two variants of deep network, DBR and DBR-v3, are proposed for different size and scale of image database. After training, our deep network can produce hash codes with large Hamming distance for images of different contents. Experiments on standard image retrieval benchmarks show that our method outperforms other state-of-the-art methods including unsupervised, supervised, and deep hashing methods.},
	language = {en},
	urldate = {2021-07-13},
	journal = {Advances in Multimedia},
	author = {Lu, Xuchao and Song, Li and Xie, Rong and Yang, Xiaokang and Zhang, Wenjun},
	year = {2017},
	keywords = {Untagged},
	pages = {1--10},
}

@inproceedings{lundbergUnifiedApproachInterpreting2017,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html},
	urldate = {2022-01-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lundberg, Scott M and Lee, Su-In},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{luoLearningInverseMapping2017,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning {Inverse} {Mapping} by {AutoEncoder} {Based} {Generative} {Adversarial} {Nets}},
	volume = {10635},
	url = {https://doi.org/10.1007/978-3-319-70096-0_22},
	doi = {10/ghwnnw},
	booktitle = {Neural {Information} {Processing} - 24th {International} {Conference}, {ICONIP} 2017, {Guangzhou}, {China}, {November} 14-18, 2017, {Proceedings}, {Part} {II}},
	publisher = {Springer},
	author = {Luo, Junyu and Xu, Yong and Tang, Chenwei and Lv, Jiancheng},
	editor = {Liu, Derong and Xie, Shengli and Li, Yuanqing and Zhao, Dongbin and El-Alfy, El-Sayed M.},
	year = {2017},
	keywords = {Untagged},
	pages = {207--216},
}

@inproceedings{maPoseGuidedPerson2017,
	address = {Long Beach, CA},
	title = {Pose {Guided} {Person} {Image} {Generation}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/34ed066df378efacc9b924ec161e7639-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ma, Liqian and Jia, Xu and Sun, Qianru and Schiele, Bernt and Tuytelaars, Tinne and Gool, Luc Van},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {406--416},
}

@inproceedings{maiaUsingMonteCarlo2017,
	address = {New York, NY, USA},
	title = {Using {Monte} {Carlo} tree search and google maps to improve game balancing in location-based games},
	isbn = {978-1-5386-3233-8},
	url = {http://ieeexplore.ieee.org/document/8080438/},
	doi = {10/ghv3nx},
	abstract = {Location-Based games (LBGs) are a subtype of digital games that uses the location of players as a key component for playability, including changes to the game state. However, a signiﬁcant challenge that threatens the development and popularization of LBGs is the game balancing. Since LBGs rely on players’ location, it is hard to manually design interactions, challenges, and game scenarios for each part of the world. Thus, the same LBG is likely to present varying difﬁculty levels depending on the player’s location due to differences in terrain, distance, and transport availability. As a result, even modern LBGs show huge balancing differences between regions and they do not explore competition between players like other game genres. In this paper, we present measurements to estimate game balancing in modern LBGs and introduce a method that uses Monte Carlo Tree Search (MCTS) to automatically edit instances of these games to minimize differences in game balancing. Additionally, we present a study detailing the improvements in game balancing when using the proposed method in today’s two most popular LBGs (Ingress and Poke´mon Go).},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computational} {Intelligence} and {Games}},
	publisher = {IEEE},
	author = {Maia, Luis Fernando and Viana, Windson and Trinta, Fernando},
	year = {2017},
	keywords = {Untagged},
	pages = {215--222},
}

@inproceedings{makhzaniPixelGANAutoencoders2017,
	address = {Long Beach, CA},
	title = {{PixelGAN} {Autoencoders}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/7e7e69ea3384874304911625ac34321c-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Makhzani, Alireza and Frey, Brendan J.},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {1975--1985},
}

@inproceedings{mcmahanCommunicationEfficientLearningDeep2017,
	title = {Communication-{Efficient} {Learning} of {Deep} {Networks} from {Decentralized} {Data}},
	shorttitle = {{FedAvg}},
	url = {https://proceedings.mlr.press/v54/mcmahan17a.html},
	abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches.  We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.  We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
	urldate = {2021-11-30},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Aguera y},
	year = {2017},
	keywords = {Untagged},
	pages = {1273--1282},
}

@article{mendesPrivacyPreservingDataMining2017,
	title = {Privacy-{Preserving} {Data} {Mining}: {Methods}, {Metrics}, and {Applications}},
	volume = {5},
	issn = {2169-3536},
	shorttitle = {Privacy-{Preserving} {Data} {Mining}},
	url = {http://ieeexplore.ieee.org/document/7950921/},
	doi = {10/gfvfv5},
	abstract = {The collection and analysis of data are continuously growing due to the pervasiveness of computing devices. The analysis of such information is fostering businesses and contributing beneﬁcially to the society in many different ﬁelds. However, this storage and ﬂow of possibly sensitive data poses serious privacy concerns. Methods that allow the knowledge extraction from data, while preserving privacy, are known as privacy-preserving data mining (PPDM) techniques. This paper surveys the most relevant PPDM techniques from the literature and the metrics used to evaluate such techniques and presents typical applications of PPDM methods in relevant ﬁelds. Furthermore, the current challenges and open issues in PPDM are discussed.},
	language = {en},
	urldate = {2021-01-27},
	journal = {IEEE Access},
	author = {Mendes, Ricardo and Vilela, Joao P.},
	year = {2017},
	keywords = {Untagged},
	pages = {10562--10582},
}

@inproceedings{meschederAdversarialVariationalBayes2017,
	address = {Sydney, NSW, Australia},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Adversarial {Variational} {Bayes}: {Unifying} {Variational} {Autoencoders} and {Generative} {Adversarial} {Networks}},
	volume = {70},
	url = {http://proceedings.mlr.press/v70/mescheder17a.html},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mescheder, Lars M. and Nowozin, Sebastian and Geiger, Andreas},
	editor = {Precup, Doina and Teh, Yee Whye},
	year = {2017},
	keywords = {Untagged},
	pages = {2391--2400},
}

@inproceedings{meschederNumericsGANs2017,
	address = {Long Beach, CA},
	title = {The {Numerics} of {GANs}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/4588e674d3f0faf985047d4c3f13ed0d-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Mescheder, Lars M. and Nowozin, Sebastian and Geiger, Andreas},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {1825--1835},
}

@inproceedings{mironovRenyiDifferentialPrivacy2017,
	title = {Renyi {Differential} {Privacy}},
	url = {http://arxiv.org/abs/1702.07476},
	doi = {10/gf3cr9},
	abstract = {We propose a natural relaxation of differential privacy based on the Re´nyi divergence. Closely related notions have appeared in several recent papers that analyzed composition of differentially private mechanisms. We argue that the useful analytical tool can be used as a privacy deﬁnition, compactly and accurately representing guarantees on the tails of the privacy loss. We demonstrate that the new deﬁnition shares many important properties with the standard deﬁnition of differential privacy, while additionally allowing tighter analysis of composite heterogeneous mechanisms.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Computer} {Security} {Foundations} {Symposium}},
	author = {Mironov, Ilya},
	year = {2017},
	note = {arXiv: 1702.07476},
	keywords = {Untagged},
	pages = {263--275},
}

@inproceedings{miyatoAdversarialTrainingMethods2017,
	address = {Toulon, France},
	title = {Adversarial {Training} {Methods} for {Semi}-{Supervised} {Text} {Classification}},
	url = {https://openreview.net/forum?id=r1X3g2_xl},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Miyato, Takeru and Dai, Andrew M. and Goodfellow, Ian J.},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{movshovitz-attiasNoFussDistance2017,
	title = {No {Fuss} {Distance} {Metric} {Learning} {Using} {Proxies}},
	url = {https://doi.org/10.1109/ICCV.2017.47},
	doi = {10/gf6mxp},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}, {ICCV} 2017, {Venice}, {Italy}, {October} 22-29, 2017},
	publisher = {IEEE Computer Society},
	author = {Movshovitz-Attias, Yair and Toshev, Alexander and Leung, Thomas K. and Ioffe, Sergey and Singh, Saurabh},
	year = {2017},
	keywords = {Untagged},
	pages = {360--368},
}

@inproceedings{mukkamalaVariantsRMSPropAdagrad2017,
	address = {Sydney, NSW, Australia},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Variants of {RMSProp} and {Adagrad} with {Logarithmic} {Regret} {Bounds}},
	volume = {70},
	url = {http://proceedings.mlr.press/v70/mukkamala17a.html},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mukkamala, Mahesh Chandra and Hein, Matthias},
	editor = {Precup, Doina and Teh, Yee Whye},
	year = {2017},
	keywords = {Untagged},
	pages = {2545--2553},
}

@inproceedings{nguyenDualDiscriminatorGenerative2017,
	address = {Long Beach, CA},
	title = {Dual {Discriminator} {Generative} {Adversarial} {Nets}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/e60e81c4cbe5171cd654662d9887aec2-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Nguyen, Tu Dinh and Le, Trung and Vu, Hung and Phung, Dinh Q.},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {2670--2680},
}

@inproceedings{odenaConditionalImageSynthesis2017,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Conditional {Image} {Synthesis} with {Auxiliary} {Classifier} {GANs}},
	volume = {70},
	url = {http://proceedings.mlr.press/v70/odena17a.html},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}, {ICML} 2017, {Sydney}, {NSW}, {Australia}, 6-11 {August} 2017},
	publisher = {PMLR},
	author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
	editor = {Precup, Doina and Teh, Yee Whye},
	year = {2017},
	keywords = {Untagged},
	pages = {2642--2651},
}

@inproceedings{papernotSemisupervisedKnowledgeTransfer2017,
	address = {Toulon, France},
	title = {Semi-supervised {Knowledge} {Transfer} for {Deep} {Learning} from {Private} {Training} {Data}},
	shorttitle = {{PATE}},
	url = {https://openreview.net/forum?id=HkwoSDPgg},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Papernot, Nicolas and Abadi, Martín and Erlingsson, Úlfar and Goodfellow, Ian J. and Talwar, Kunal},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{papernot_practical_2017,
	address = {Abu Dhabi United Arab Emirates},
	title = {Practical {Black}-{Box} {Attacks} against {Machine} {Learning}},
	isbn = {978-1-4503-4944-4},
	url = {https://dl.acm.org/doi/10.1145/3052973.3053009},
	doi = {10.1145/3052973.3053009},
	abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modiﬁed to yield erroneous model outputs, while appearing unmodiﬁed to human observers. Potential attacks include having malicious content like malware identiﬁed as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the ﬁrst practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and ﬁnd that they are misclassiﬁed by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We ﬁnd that their DNN misclassiﬁes 84.24\% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassiﬁed by Amazon and Google at rates of 96.19\% and 88.94\%. We also ﬁnd that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
	language = {en},
	urldate = {2022-05-29},
	booktitle = {{ACM} {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
	month = apr,
	year = {2017},
	keywords = {Untagged},
	pages = {506--519},
}

@inproceedings{paszkeAutomaticDifferentiationPyTorch2017,
	title = {Automatic {Differentiation} in {PyTorch}},
	booktitle = {{NeurIPSW}},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{phanAdaptiveLaplaceMechanism2017,
	address = {New Orleans, LA},
	title = {Adaptive {Laplace} {Mechanism}: {Differential} {Privacy} {Preservation} in {Deep} {Learning}},
	isbn = {978-1-5386-3835-4},
	shorttitle = {Adaptive {Laplace} {Mechanism}},
	url = {http://ieeexplore.ieee.org/document/8215511/},
	doi = {10/ghv3f2},
	abstract = {In this paper, we focus on developing a novel mechanism to preserve differential privacy in deep neural networks, such that: (1) The privacy budget consumption is totally independent of the number of training steps; (2) It has the ability to adaptively inject noise into features based on the contribution of each to the output; and (3) It could be applied in a variety of different deep neural networks. To achieve this, we ﬁgure out a way to perturb afﬁne transformations of neurons, and loss functions used in deep neural networks. In addition, our mechanism intentionally adds “more noise” into features which are “less relevant” to the model output, and vice-versa. Our theoretical analysis further derives the sensitivities and error bounds of our mechanism. Rigorous experiments conducted on MNIST and CIFAR-10 datasets show that our mechanism is highly effective and outperforms existing solutions.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {International}  {Conference} on {Data} {Mining}},
	publisher = {IEEE},
	author = {Phan, NhatHai and Wu, Xintao and Hu, Han and Dou, Dejing},
	year = {2017},
	keywords = {Untagged},
	pages = {385--394},
}

@article{poirion_descent_2017,
	title = {Descent algorithm for nonsmooth stochastic multiobjective optimization},
	volume = {68},
	issn = {0926-6003, 1573-2894},
	url = {http://link.springer.com/10.1007/s10589-017-9921-x},
	doi = {10.1007/s10589-017-9921-x},
	abstract = {An algorithm for solving the expectation formulation of stochastic nonsmooth multiobjective optimization problems is proposed. The proposed method is an extension of the classical stochastic gradient algorithm to multi10 objective optimization using the properties of a common descent vector deﬁned in the deterministic context. The mean square and the almost sure convergence of the algorithm are proven. The algorithm eﬃciency is illustrated and assessed on an academic example.},
	language = {en},
	number = {2},
	urldate = {2023-05-11},
	journal = {Computational Optimization and Applications},
	author = {Poirion, Fabrice and Mercier, Quentin and Désidéri, Jean-Antoine},
	month = nov,
	year = {2017},
	keywords = {Untagged},
	pages = {317--331},
}

@article{roscaVariationalApproachesAutoEncoding2017,
	title = {Variational {Approaches} for {Auto}-{Encoding} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1706.04987},
	abstract = {Auto-encoding generative adversarial networks (GANs) combine the standard GAN algorithm, which discriminates between real and model-generated data, with a reconstruction loss given by an auto-encoder. Such models aim to prevent mode collapse in the learned generative model by ensuring that it is grounded in all the available training data. In this paper, we develop a principle upon which autoencoders can be combined with generative adversarial networks by exploiting the hierarchical structure of the generative model. The underlying principle shows that variational inference can be used a basic tool for learning, but with the intractable likelihood replaced by a synthetic likelihood, and the unknown posterior distribution replaced by an implicit distribution; both synthetic likelihoods and implicit posterior distributions can be learned using discriminators. This allows us to develop a natural fusion of variational auto-encoders and generative adversarial networks, combining the best of both these methods. We describe a uniﬁed objective for optimization, discuss the constraints needed to guide learning, connect to the wide range of existing work, and use a battery of tests to systematically and quantitatively assess the performance of our method.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1706.04987 [cs, stat]},
	author = {Rosca, Mihaela and Lakshminarayanan, Balaji and Warde-Farley, David and Mohamed, Shakir},
	month = oct,
	year = {2017},
	note = {arXiv: 1706.04987},
	keywords = {Untagged},
}

@article{ruderOverviewMultiTaskLearning2017,
	title = {An {Overview} of {Multi}-{Task} {Learning} in {Deep} {Neural} {Networks}},
	volume = {abs/1706.05098},
	url = {http://arxiv.org/abs/1706.05098},
	journal = {CoRR},
	author = {Ruder, Sebastian},
	year = {2017},
	note = {\_eprint: 1706.05098},
	keywords = {Untagged},
}

@inproceedings{ruiz3DCNNsDistance2017,
	address = {Mountain View, CA},
	title = {{3D} {CNNs} on {Distance} {Matrices} for {Human} {Action} {Recognition}},
	url = {https://doi.org/10.1145/3123266.3123299},
	doi = {10.1145/3123266.3123299},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Ruiz, Alejandro Hernandez and Porzi, Lorenzo and Bulò, Samuel Rota and Moreno-Noguer, Francesc},
	editor = {Liu, Qiong and Lienhart, Rainer and Wang, Haohong and Chen, Sheng-Wei "Kuan-Ta" and Boll, Susanne and Chen, Yi-Ping Phoebe and Friedland, Gerald and Li, Jia and Yan, Shuicheng},
	year = {2017},
	keywords = {Untagged},
	pages = {1087--1095},
}

@inproceedings{ryooPrivacyPreservingHumanActivity2017,
	address = {Mountain View, CA},
	title = {Privacy-{Preserving} {Human} {Activity} {Recognition} from {Extreme} {Low} {Resolution}},
	url = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14847},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {AAAI Press},
	author = {Ryoo, Michael S. and Rothrock, Brandon and Fleming, Charles and Yang, Hyun Jong},
	editor = {Singh, Satinder P. and Markovitch, Shaul},
	year = {2017},
	keywords = {Untagged},
	pages = {4255--4262},
}

@inproceedings{saitoAsymmetricTritrainingUnsupervised2017,
	address = {Sydney, NSW, Australia},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Asymmetric {Tri}-training for {Unsupervised} {Domain} {Adaptation}},
	volume = {70},
	url = {http://proceedings.mlr.press/v70/saito17a.html},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Saito, Kuniaki and Ushiku, Yoshitaka and Harada, Tatsuya},
	editor = {Precup, Doina and Teh, Yee Whye},
	year = {2017},
	keywords = {Untagged},
	pages = {2988--2997},
}

@article{selaUnrestrictedFacialGeometry2017,
	title = {Unrestricted {Facial} {Geometry} {Reconstruction} {Using} {Image}-to-{Image} {Translation}},
	url = {http://arxiv.org/abs/1703.10131},
	abstract = {It has been recently shown that neural networks can recover the geometric structure of a face from a single given image. A common denominator of most existing face geometry reconstruction methods is the restriction of the solution space to some low-dimensional subspace. While such a model signiﬁcantly simpliﬁes the reconstruction problem, it is inherently limited in its expressiveness. As an alternative, we propose an Image-to-Image translation network that jointly maps the input image to a depth image and a facial correspondence map. This explicit pixel-based mapping can then be utilized to provide high quality reconstructions of diverse faces under extreme expressions, using a purely geometric reﬁnement process. In the spirit of recent approaches, the network is trained only with synthetic data, and is then evaluated on “in-the-wild” facial images. Both qualitative and quantitative analyses demonstrate the accuracy and the robustness of our approach.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1703.10131 [cs]},
	author = {Sela, Matan and Richardson, Elad and Kimmel, Ron},
	month = sep,
	year = {2017},
	note = {arXiv: 1703.10131},
	keywords = {Untagged},
}

@inproceedings{8237336,
	title = {Grad-cam: {Visual} explanations from deep networks via gradient-based localization},
	doi = {10.1109/ICCV.2017.74},
	booktitle = {2017 {IEEE} international conference on computer vision ({ICCV})},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	year = {2017},
	keywords = {Untagged},
	pages = {618--626},
}

@inproceedings{shangVIGANMissingView2017,
	title = {{VIGAN}: {Missing} view imputation with generative adversarial networks},
	url = {https://doi.org/10.1109/BigData.2017.8257992},
	doi = {10/ghwnpr},
	booktitle = {2017 {IEEE} {International} {Conference} on {Big} {Data}, {BigData} 2017, {Boston}, {MA}, {USA}, {December} 11-14, 2017},
	publisher = {IEEE Computer Society},
	author = {Shang, Chao and Palmer, Aaron and Sun, Jiangwen and Chen, Ko-Shin and Lu, Jin and Bi, Jinbo},
	editor = {Nie, Jian-Yun and Obradovic, Zoran and Suzumura, Toyotaro and Ghosh, Rumi and Nambiar, Raghunath and Wang, Chonggang and Zang, Hui and Baeza-Yates, Ricardo and Hu, Xiaohua and Kepner, Jeremy and Cuzzocrea, Alfredo and Tang, Jian and Toyoda, Masashi},
	year = {2017},
	keywords = {Untagged},
	pages = {766--775},
}

@inproceedings{shokriMembershipInferenceAttacks2017,
	address = {San Jose, CA, USA},
	title = {Membership {Inference} {Attacks} {Against} {Machine} {Learning} {Models}},
	isbn = {978-1-5090-5533-3},
	url = {http://ieeexplore.ieee.org/document/7958568/},
	doi = {10/cwdq},
	abstract = {We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model’s training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model’s predictions on the inputs that it trained on versus the inputs that it did not train on.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {2017 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
	year = {2017},
	keywords = {Untagged},
	pages = {3--18},
}

@inproceedings{shuchangzhouGeneGANLearningObject2017,
	title = {{GeneGAN}: {Learning} {Object} {Transfiguration} and {Object} {Subspace} from {Unpaired} {Data}},
	isbn = {1-901725-60-X},
	url = {https://dx.doi.org/10.5244/C.31.111},
	doi = {10/ghwnm3},
	booktitle = {British {Machine} {Vision} {Conference}},
	publisher = {BMVA Press},
	author = {Shuchang Zhou, Taihong Xiao, Yi Yang, Dieqiao Feng, Qinyao He and He, Weiran},
	editor = {Tae-Kyun Kim, Stefanos Zafeiriou, Gabriel Brostow and Mikolajczyk, Krystian},
	year = {2017},
	keywords = {Untagged},
	pages = {111.1--111.13},
}

@inproceedings{simonGeneralizedOrderlessPooling2017,
	title = {Generalized orderless pooling performs implicit salient matching},
	booktitle = {International {Conference} on {Computer} {Vision}},
	author = {Simon, M. and Gao, Y. and Darrell, T. and Denzler, J. and Rodner, E.},
	year = {2017},
	keywords = {Untagged},
	pages = {4960--4969},
}

@inproceedings{DBLP:conf/iclr/SmithTHH17,
	title = {Offline bilingual word vectors, orthogonal transformations and the inverted softmax},
	shorttitle = {{IS}},
	url = {https://openreview.net/forum?id=r1Aab85gg},
	booktitle = {5th international conference on learning representations, {ICLR} 2017, toulon, france, april 24-26, 2017, conference track proceedings},
	publisher = {OpenReview.net},
	author = {Smith, Samuel L. and Turban, David H. P. and Hamblin, Steven and Hammerla, Nils Y.},
	year = {2017},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/iclr/SmithTHH17.bib
tex.timestamp: Thu, 25 Jul 2019 14:25:47 +0200},
	keywords = {Untagged},
}

@inproceedings{snellPrototypicalNetworksFewshot2017,
	address = {Long Beach, CA},
	title = {Prototypical {Networks} for {Few}-shot {Learning}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {4077--4087},
}

@article{songHierarchicalDeepHashing2017,
	title = {Hierarchical deep hashing for image retrieval},
	volume = {11},
	issn = {2095-2228, 2095-2236},
	url = {http://link.springer.com/10.1007/s11704-017-6537-3},
	doi = {10/f95nfv},
	abstract = {We present a new method to generate eﬃcient multi-level hashing codes for image retrieval based on the deep siamese convolutional neural network (DSCNN). Conventional deep hashing methods trade oﬀ the capability of capturing highly complex and nonlinear semantic information of images against very compact hash codes, usually leading to high retrieval eﬃciency but with deteriorated accuracy. We alleviate the restrictive compactness requirement of hash codes by extending them to a two-level hierarchical coding scheme, in which the ﬁrst level aims to capture the high-level semantic information extracted by the deep network using a rich encoding strategy, while the subsequent level squeezes them to more global and compact codes. At running time, we adopt an attention-based mechanism to select some of its most essential bits speciﬁc to each query image for retrieval instead of using the full hash codes of the ﬁrst level. The attention-based mechanism is based on the guides of hash codes generated by the second level, taking advantage of both local and global properties of deep features. Experimental results on various popular datasets demonstrate the advantages of the proposed method compared to several state-of-the-art methods.},
	language = {en},
	number = {2},
	urldate = {2021-01-27},
	journal = {Frontiers of Computer Science},
	author = {Song, Ge and Tan, Xiaoyang},
	month = apr,
	year = {2017},
	keywords = {Untagged},
	pages = {253--265},
}

@inproceedings{songDeepSpatialSemanticAttention2017,
	address = {Venice, Italy},
	title = {Deep {Spatial}-{Semantic} {Attention} for {Fine}-{Grained} {Sketch}-{Based} {Image} {Retrieval}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237854/},
	doi = {10/gfsvgm},
	abstract = {Human sketches are unique in being able to capture both the spatial topology of a visual object, as well as its subtle appearance details. Fine-grained sketch-based image retrieval (FG-SBIR) importantly leverages on such ﬁne-grained characteristics of sketches to conduct instancelevel retrieval of photos. Nevertheless, human sketches are often highly abstract and iconic, resulting in severe misalignments with candidate photos which in turn make subtle visual detail matching difﬁcult. Existing FG-SBIR approaches focus only on coarse holistic matching via deep cross-domain representation learning, yet ignore explicitly accounting for ﬁne-grained details and their spatial context. In this paper, a novel deep FG-SBIR model is proposed which differs signiﬁcantly from the existing models in that: (1) It is spatially aware, achieved by introducing an attention module that is sensitive to the spatial position of visual details; (2) It combines coarse and ﬁne semantic information via a shortcut connection fusion block; and (3) It models feature correlation and is robust to misalignments between the extracted features across the two domains by introducing a novel higher-order learnable energy function (HOLEF) based loss. Extensive experiments show that the proposed deep spatial-semantic attention model signiﬁcantly outperforms the state-of-the-art.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Song, Jifei and Yu, Qian and Song, Yi-Zhe and Xiang, Tao and Hospedales, Timothy M.},
	year = {2017},
	keywords = {Untagged},
	pages = {5552--5561},
}

@inproceedings{staib2017distributionally,
	title = {Distributionally robust deep learning as a generalization of adversarial training},
	volume = {1},
	booktitle = {{NIPS} workshop on machine learning and computer security},
	author = {Staib, Matthew and Jegelka, Stefanie},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{suPoseDrivenDeepConvolutional2017,
	address = {Venice, Italy},
	title = {Pose-{Driven} {Deep} {Convolutional} {Model} for {Person} {Re}-identification},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237689/},
	doi = {10/gdcfkz},
	abstract = {Feature extraction and matching are two crucial components in person Re-Identiﬁcation (ReID). The large pose deformations and the complex view variations exhibited by the captured person images signiﬁcantly increase the difﬁculty of learning and matching of the features from person images. To overcome these difﬁculties, in this work we propose a Pose-driven Deep Convolutional (PDC) model to learn improved feature extraction and matching models from end to end. Our deep architecture explicitly leverages the human part cues to alleviate the pose variations and learn robust feature representations from both the global image and different local parts. To match the features from global human body and local body parts, a pose driven feature weighting sub-network is further designed to learn adaptive feature fusions. Extensive experimental analyses and results on three popular datasets demonstrate signiﬁcant performance improvements of our model over all published stateof-the-art methods.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Su, Chi and Li, Jianing and Zhang, Shiliang and Xing, Junliang and Gao, Wen and Tian, Qi},
	year = {2017},
	keywords = {Untagged},
	pages = {3980--3989},
}

@inproceedings{suReasoningFineGrainedAttribute2017,
	address = {Venice, Italy},
	title = {Reasoning {About} {Fine}-{Grained} {Attribute} {Phrases} {Using} {Reference} {Games}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237315/},
	doi = {10/ghv3vb},
	abstract = {We present a framework for learning to describe ﬁnegrained visual differences between instances using attribute phrases. Attribute phrases capture distinguishing aspects of an object (e.g., “propeller on the nose” or “door near the wing” for airplanes) in a compositional manner. Instances within a category can be described by a set of these phrases and collectively they span the space of semantic attributes for a category. We collect a large dataset of such phrases by asking annotators to describe several visual differences between a pair of instances within a category. We then learn to describe and ground these phrases to images in the context of a reference game between a speaker and a listener. The goal of a speaker is to describe attributes of an image that allows the listener to correctly identify it within a pair. Data collected in a pairwise manner improves the ability of the speaker to generate, and the ability of the listener to interpret visual descriptions. Moreover, due to the compositionality of attribute phrases, the trained listeners can interpret descriptions not seen during training for image retrieval, and the speakers can generate attribute-based explanations for differences between previously unseen categories. We also show that embedding an image into the semantic space of attribute phrases derived from listeners offers 20\% improvement in accuracy over existing attributebased representations on the FGVC-aircraft dataset.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Su, Jong-Chyi and Wu, Chenyun and Jiang, Huaizu and Maji, Subhransu},
	year = {2017},
	keywords = {Untagged},
	pages = {418--427},
}

@incollection{sunCorrelationAlignmentUnsupervised2017,
	series = {Advances in {Computer} {Vision} and {Pattern} {Recognition}},
	title = {Correlation {Alignment} for {Unsupervised} {Domain} {Adaptation}},
	url = {https://doi.org/10.1007/978-3-319-58347-1_8},
	booktitle = {Domain {Adaptation} in {Computer} {Vision} {Applications}},
	publisher = {Springer},
	author = {Sun, Baochen and Feng, Jiashi and Saenko, Kate},
	editor = {Csurka, Gabriela},
	year = {2017},
	doi = {10.1007/978-3-319-58347-1_8},
	keywords = {Untagged},
	pages = {153--171},
}

@inproceedings{taigmanUnsupervisedCrossDomainImage2017,
	address = {Toulon, France},
	title = {Unsupervised {Cross}-{Domain} {Image} {Generation}},
	url = {https://openreview.net/forum?id=Sk2Im59ex},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Taigman, Yaniv and Polyak, Adam and Wolf, Lior},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{tianSymmetryBreakingConvergenceAnalysis2017,
	address = {Toulon, France},
	title = {Symmetry-{Breaking} {Convergence} {Analysis} of {Certain} {Two}-layered {Neural} {Networks} with {ReLU} nonlinearity},
	url = {https://openreview.net/forum?id=r1lVgRNtx},
	booktitle = {International {Conference} on {Learning} {Representations} {Workshop}},
	publisher = {OpenReview.net},
	author = {Tian, Yuandong},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{tolstikhinAdaGANBoostingGenerative2017,
	address = {Long Beach, CA},
	title = {{AdaGAN}: {Boosting} {Generative} {Models}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/d0010a6f34908640a4a6da2389772a78-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Tolstikhin, Ilya O. and Gelly, Sylvain and Bousquet, Olivier and Simon-Gabriel, Carl-Johann and Schölkopf, Bernhard},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {5424--5433},
}

@inproceedings{tzengAdversarialDiscriminativeDomain2017,
	address = {Honolulu, Hawaii},
	title = {Adversarial {Discriminative} {Domain} {Adaptation}},
	url = {https://doi.org/10.1109/CVPR.2017.316},
	doi = {10/gf5br3},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
	year = {2017},
	keywords = {Untagged},
	pages = {2962--2971},
}

@article{vanhornDevilTailsFinegrained2017,
	title = {The {Devil} is in the {Tails}: {Fine}-grained {Classification} in the {Wild}},
	shorttitle = {The {Devil} is in the {Tails}},
	url = {http://arxiv.org/abs/1709.01450},
	abstract = {The world is long-tailed. What does this mean for computer vision and visual recognition? The main two implications are (1) the number of categories we need to consider in applications can be very large, and (2) the number of training examples for most categories can be very small. Current visual recognition algorithms have achieved excellent classiﬁcation accuracy. However, they require many training examples to reach peak performance, which suggests that long-tailed distributions will not be dealt with well. We analyze this question in the context of eBird, a large ﬁne-grained classiﬁcation dataset, and a state-ofthe-art deep network classiﬁcation algorithm. We ﬁnd that (a) peak classiﬁcation performance on well-represented categories is excellent, (b) given enough data, classiﬁcation performance suffers only minimally from an increase in the number of classes, (c) classiﬁcation performance decays precipitously as the number of training examples decreases, (d) surprisingly, transfer learning is virtually absent in current methods. Our ﬁndings suggest that our community should come to grips with the question of long tails.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1709.01450 [cs]},
	author = {Van Horn, Grant and Perona, Pietro},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.01450},
	keywords = {Untagged},
}

@inproceedings{vaswaniAttentionAllYou2017,
	address = {Long Beach, CA},
	title = {Attention is {All} you {Need}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {5998--6008},
}

@inproceedings{vuSelfGatedRecurrentNeural2017,
	address = {Mountain View, CA},
	title = {Self-{Gated} {Recurrent} {Neural} {Networks} for {Human} {Activity} {Recognition} on {Wearable} {Devices}},
	url = {https://doi.org/10.1145/3126686.3126764},
	doi = {10.1145/3126686.3126764},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Vu, Toan H. and Dang, An and Dung, Le and Wang, Jia-Ching},
	editor = {Wu, Wanmin and Yang, Jianchao and Tian, Qi and Zimmermann, Roger},
	year = {2017},
	keywords = {Untagged},
	pages = {179--185},
}

@inproceedings{wangUntrimmedNetsWeaklySupervised2017,
	address = {Honolulu, HI},
	title = {{UntrimmedNets} for {Weakly} {Supervised} {Action} {Recognition} and {Detection}},
	url = {https://doi.org/10.1109/CVPR.2017.678},
	doi = {10/ghwngn},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Wang, Limin and Xiong, Yuanjun and Lin, Dahua and Gool, Luc Van},
	year = {2017},
	keywords = {Untagged},
	pages = {6402--6411},
}

@inproceedings{wangLocallyDifferentiallyPrivate2017,
	address = {Vancouver, BC, Canada},
	title = {Locally {Differentially} {Private} {Protocols} for {Frequency} {Estimation}},
	url = {https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/wang-tianhao},
	booktitle = {{USENIX} {Security} {Symposium}},
	publisher = {USENIX Association},
	author = {Wang, Tianhao and Blocki, Jeremiah and Li, Ninghui and Jha, Somesh},
	editor = {Kirda, Engin and Ristenpart, Thomas},
	year = {2017},
	keywords = {Untagged},
	pages = {729--745},
}

@inproceedings{wangMultilabelImageRecognition2017,
	address = {Venice},
	title = {Multi-label {Image} {Recognition} by {Recurrently} {Discovering} {Attentional} {Regions}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237320/},
	doi = {10/ghptqn},
	abstract = {This paper proposes a novel deep architecture to address multi-label image recognition, a fundamental and practical task towards general visual understanding. Current solutions for this task usually rely on an extra step of extracting hypothesis regions (i.e., region proposals), resulting in redundant computation and sub-optimal performance. In this work, we achieve the interpretable and contextualized multi-label image classiﬁcation by developing a recurrent memorized-attention module. This module consists of two alternately performed components: i) a spatial transformer layer to locate attentional regions from the convolutional feature maps in a region-proposal-free way and ii) an LSTM (Long-Short Term Memory) sub-network to sequentially predict semantic labeling scores on the located regions while capturing the global dependencies of these regions. The LSTM also output the parameters for computing the spatial transformer. On large-scale benchmarks of multi-label image classiﬁcation (e.g., MS-COCO and PASCAL VOC 07), our approach demonstrates superior performances over other existing state-of-the-arts in both accuracy and efﬁciency.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Wang, Zhouxia and Chen, Tianshui and Li, Guanbin and Xu, Ruijia and Lin, Liang},
	year = {2017},
	keywords = {Untagged},
	pages = {464--472},
}

@article{weiSelectiveConvolutionalDescriptor2017,
	title = {Selective convolutional descriptor aggregation for fine-grained image retrieval},
	volume = {26},
	number = {6},
	journal = {TIP},
	author = {Wei, Xiu-Shen and Luo, Jian-Hao and Wu, Jianxin and Zhou, Zhi-Hua},
	year = {2017},
	note = {Publisher: IEEE},
	keywords = {Untagged},
	pages = {2868--2881},
}

@inproceedings{wilsonMarginalValueAdaptive2017,
	address = {Long Beach, CA},
	title = {The {Marginal} {Value} of {Adaptive} {Gradient} {Methods} in {Machine} {Learning}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wilson, Ashia C. and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	keywords = {Untagged},
	pages = {4148--4158},
}

@article{winograd-cortFrameworkAdaptiveDifferential2017,
	title = {A {Framework} for {Adaptive} {Differential} {Privacy}},
	volume = {1},
	url = {https://doi.org/10.1145/3110254},
	doi = {10/ghv4mf},
	abstract = {Differential privacy is a widely studied theory for analyzing sensitive data with a strong privacy guarantee—any change in an individual's data can have only a small statistical effect on the result—and a growing number of programming languages now support differentially private data analysis. A common shortcoming of these languages is poor support for adaptivity. In practice, a data analyst rarely wants to run just one function over a sensitive database, nor even a predetermined sequence of functions with fixed privacy parameters; rather, she wants to engage in an interaction where, at each step, both the choice of the next function and its privacy parameters are informed by the results of prior functions. Existing languages support this scenario using a simple composition theorem, which often gives rather loose bounds on the actual privacy cost of composite functions, substantially reducing how much computation can be performed within a given privacy budget. The theory of differential privacy includes other theorems with much better bounds, but these have not yet been incorporated into programming languages. We propose a novel framework for adaptive composition that is elegant, practical, and implementable. It consists of a reformulation based on typed functional programming of privacy filters, together with a concrete realization of this framework in the design and implementation of a new language, called Adaptive Fuzz. Adaptive Fuzz transplants the core static type system of Fuzz to the adaptive setting by wrapping the Fuzz typechecker and runtime system in an outer adaptive layer, allowing Fuzz programs to be conveniently constructed and typechecked on the fly. We describe an interpreter for Adaptive Fuzz and report results from two case studies demonstrating its effectiveness for implementing common statistical algorithms over real data sets.},
	number = {ICFP},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Winograd-Cort, Daniel and Haeberlen, Andreas and Roth, Aaron and Pierce, Benjamin C.},
	year = {2017},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Untagged},
}

@inproceedings{xieDeepDeterminantalPoint2017,
	address = {Venice},
	title = {Deep {Determinantal} {Point} {Process} for {Large}-{Scale} {Multi}-label {Classification}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237321/},
	doi = {10/ghv3n2},
	abstract = {We study large-scale multi-label classiﬁcation (MLC) on two recently released datasets: Youtube-8M and Open Images that contain millions of data instances and thousands of classes. The unprecedented problem scale poses great challenges for MLC. First, ﬁnding out the correct label subset out of exponentially many choices incurs substantial ambiguity and uncertainty. Second, the large data-size and class-size entail considerable computational cost. To address the ﬁrst challenge, we investigate two strategies: capturing label-correlations from the training data and incorporating label co-occurrence relations obtained from external knowledge, which effectively eliminate semantically inconsistent labels and provide contextual clues to differentiate visually ambiguous labels. Speciﬁcally, we propose a Deep Determinantal Point Process (DDPP) model which seamlessly integrates a DPP with deep neural networks (DNNs) and supports end-to-end multi-label learning and deep representation learning. The DPP is able to capture label-correlations of any order with a polynomial computational cost, while the DNNs learn hierarchical features of images/videos and capture the dependency between input data and labels. To incorporate external knowledge about label co-occurrence relations, we impose relational regularization over the kernel matrix in DDPP. To address the second challenge, we study an efﬁcient low-rank kernel learning algorithm based on inducing point methods. Experiments on the two datasets demonstrate the efﬁcacy and efﬁciency of the proposed methods.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Xie, Pengtao and Salakhutdinov, Ruslan and Mou, Luntian and Xing, Eric P.},
	year = {2017},
	keywords = {Untagged},
	pages = {473--482},
}

@inproceedings{xieAggregatedResidualTransformations2017,
	title = {Aggregated {Residual} {Transformations} for {Deep} {Neural} {Networks}},
	shorttitle = {{ResNeXt}},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Xie, Saining and Girshick, Ross and Dollar, Piotr and Tu, Zhuowen and He, Kaiming},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{xuDeepImageMatting2017,
	address = {Honolulu, HI},
	title = {Deep {Image} {Matting}},
	url = {https://doi.org/10.1109/CVPR.2017.41},
	doi = {10/ghdg2b},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Xu, Ning and Price, Brian L. and Cohen, Scott and Huang, Thomas S.},
	year = {2017},
	keywords = {Untagged},
	pages = {311--320},
}

@inproceedings{xuNoisetolerantInteractiveLearning2017,
	title = {Noise-tolerant interactive learning using pairwise comparisons},
	volume = {30},
	booktitle = {Advances in neural information processing systems},
	author = {Xu, Yichong and Zhang, Hongyang and Miller, Kyle and Singh, Aarti and Dubrawski, Artur},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{yanFrameworkOnlineLearning2017,
	title = {A {Framework} of {Online} {Learning} with {Imbalanced} {Streaming} {Data}},
	url = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14487},
	urldate = {2022-02-25},
	booktitle = {Proceedings of the {Thirty}-{First} {AAAI} {Conference} on {Artificial} {Intelligence}, {February} 4-9, 2017, {San} {Francisco}, {California}, {USA}},
	publisher = {AAAI Press},
	author = {Yan, Yan and Yang, Tianbao and Yang, Yi and Chen, Jianhui},
	editor = {Singh, Satinder P. and Markovitch, Shaul},
	year = {2017},
	keywords = {Untagged},
	pages = {2817--2823},
}

@inproceedings{yangMIMLFCNMultiInstanceMultiLabel2017,
	address = {Honolulu, HI},
	title = {{MIML}-{FCN}+: {Multi}-{Instance} {Multi}-{Label} {Learning} via {Fully} {Convolutional} {Networks} with {Privileged} {Information}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {{MIML}-{FCN}+},
	url = {http://ieeexplore.ieee.org/document/8100118/},
	doi = {10/ghv3n4},
	abstract = {Multi-instance multi-label (MIML) learning has many interesting applications in computer visions, including multi-object recognition and automatic image tagging. In these applications, additional information such as bounding-boxes, image captions and descriptions is often available during training phrase, which is referred as privileged information (PI). However, as existing works on learning using PI only consider instance-level PI (privileged instances), they fail to make use of bag-level PI (privileged bags) available in MIML learning. Therefore, in this paper, we propose a two-stream fully convolutional network, named MIML-FCN+, uniﬁed by a novel PI loss to solve the problem of MIML learning with privileged bags. Compared to the previous works on PI, the proposed MIML-FCN+ utilizes the readily available privileged bags, instead of hard-to-obtain privileged instances, making the system more general and practical in real world applications. As the proposed PI loss is convex and SGDcompatible and the framework itself is a fully convolutional network, MIML-FCN+ can be easily integrated with stateof-the-art deep learning networks. Moreover, the ﬂexibility of convolutional layers allows us to exploit structured correlations among instances to facilitate more effective training and testing. Experimental results on three benchmark datasets demonstrate the effectiveness of the proposed MIML-FCN+, outperforming state-of-the-art methods in the application of multi-object recognition.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Yang, Hao and Zhou, Joey Tianyi and Cai, Jianfei and Ong, Yew Soon},
	year = {2017},
	keywords = {Untagged},
	pages = {5996--6004},
}

@inproceedings{yangMidiNetConvolutionalGenerative2017,
	title = {{MidiNet}: {A} {Convolutional} {Generative} {Adversarial} {Network} for {Symbolic}-{Domain} {Music} {Generation}},
	url = {https://ismir2017.smcnus.org/wp-content/uploads/2017/10/226_Paper.pdf},
	booktitle = {Proceedings of the 18th {International} {Society} for {Music} {Information} {Retrieval} {Conference}, {ISMIR} 2017, {Suzhou}, {China}, {October} 23-27, 2017},
	author = {Yang, Li-Chia and Chou, Szu-Yu and Yang, Yi-Hsuan},
	editor = {Cunningham, Sally Jo and Duan, Zhiyao and Hu, Xiao and Turnbull, Douglas},
	year = {2017},
	keywords = {Untagged},
	pages = {324--331},
}

@inproceedings{yehSemanticImageInpainting2017,
	address = {Honolulu, HI},
	title = {Semantic {Image} {Inpainting} with {Deep} {Generative} {Models}},
	url = {https://doi.org/10.1109/CVPR.2017.728},
	doi = {10/gg5cd8},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Yeh, Raymond A. and Chen, Chen and Lim, Teck-Yian and Schwing, Alexander G. and Hasegawa-Johnson, Mark and Do, Minh N.},
	year = {2017},
	keywords = {Untagged},
	pages = {6882--6890},
}

@inproceedings{yiDualGANUnsupervisedDual2017,
	title = {{DualGAN}: {Unsupervised} {Dual} {Learning} for {Image}-to-{Image} {Translation}},
	url = {https://doi.org/10.1109/ICCV.2017.310},
	doi = {10/ggv876},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}, {ICCV} 2017, {Venice}, {Italy}, {October} 22-29, 2017},
	publisher = {IEEE Computer Society},
	author = {Yi, Zili and Zhang, Hao (Richard) and Tan, Ping and Gong, Minglun},
	year = {2017},
	keywords = {Untagged},
	pages = {2868--2876},
}

@inproceedings{yordanovaTextToHBMGeneralisedApproach2017,
	address = {San Francisco, California},
	series = {{AAAI} {Workshops}},
	title = {{TextToHBM}: {A} {Generalised} {Approach} to {Learning} {Models} of {Human} {Behaviour} for {Activity} {Recognition} from {Textual} {Instructions}},
	volume = {WS-17},
	url = {http://aaai.org/ocs/index.php/WS/AAAIW17/paper/view/15110},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Yordanova, Kristina Y.},
	year = {2017},
	keywords = {Untagged},
}

@article{youLargeBatchTraining2017,
	title = {Large {Batch} {Training} of {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1708.03888},
	abstract = {A common way to speed up training of large convolutional networks is to add computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. But training with large batch size often results in the lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome this optimization difficulties we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.},
	urldate = {2021-07-29},
	journal = {arXiv:1708.03888 [cs]},
	author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
	month = sep,
	year = {2017},
	note = {arXiv: 1708.03888},
	keywords = {Untagged},
}

@inproceedings{yuSeqGANSequenceGenerative2017,
	title = {{SeqGAN}: {Sequence} {Generative} {Adversarial} {Nets} with {Policy} {Gradient}},
	url = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14344},
	booktitle = {Proceedings of the {Thirty}-{First} {AAAI} {Conference} on {Artificial} {Intelligence}, {February} 4-9, 2017, {San} {Francisco}, {California}, {USA}},
	publisher = {AAAI Press},
	author = {Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong},
	editor = {Singh, Satinder P. and Markovitch, Shaul},
	year = {2017},
	keywords = {Untagged},
	pages = {2852--2858},
}

@article{yuMultimodalFactorizedBilinear2017,
	title = {Multi-modal {Factorized} {Bilinear} {Pooling} with {Co}-{Attention} {Learning} for {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/1708.01471},
	abstract = {Visual question answering (VQA) is challenging because it requires a simultaneous understanding of both the visual content of images and the textual content of questions. The approaches used to represent the images and questions in a ﬁne-grained manner and questions and to fuse these multimodal features play key roles in performance. Bilinear pooling based models have been shown to outperform traditional linear models for VQA, but their high-dimensional representations and high computational complexity may seriously limit their applicability in practice. For multimodal feature fusion, here we develop a Multi-modal Factorized Bilinear (MFB) pooling approach to efﬁciently and effectively combine multi-modal features, which results in superior performance for VQA compared with other bilinear pooling approaches. For ﬁne-grained image and question representation, we develop a ‘co-attention’ mechanism using an end-to-end deep network architecture to jointly learn both the image and question attentions. Combining the proposed MFB approach with co-attention learning in a new network architecture provides a uniﬁed model for VQA. Our experimental results demonstrate that the single MFB with co-attention model achieves new state-of-theart performance on the real-world VQA dataset. Code available at https://github.com/yuzcccc/mfb.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1708.01471 [cs]},
	author = {Yu, Zhou and Yu, Jun and Fan, Jianping and Tao, Dacheng},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.01471},
	keywords = {Untagged},
}

@inproceedings{yuanExactPenaltyMethod2017,
	title = {An {Exact} {Penalty} {Method} for {Binary} {Optimization} {Based} on {MPEC} {Formulation}},
	url = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14310},
	urldate = {2022-02-25},
	booktitle = {Proceedings of the {Thirty}-{First} {AAAI} {Conference} on {Artificial} {Intelligence}, {February} 4-9, 2017, {San} {Francisco}, {California}, {USA}},
	publisher = {AAAI Press},
	author = {Yuan, Ganzhao and Ghanem, Bernard},
	editor = {Singh, Satinder P. and Markovitch, Shaul},
	year = {2017},
	keywords = {Untagged},
	pages = {2867--2875},
}

@inproceedings{zellingerCentralMomentDiscrepancy2017,
	title = {Central {Moment} {Discrepancy} ({CMD}) for {Domain}-{Invariant} {Representation} {Learning}},
	booktitle = {{ICLR}},
	author = {Zellinger, W. and Grubinger, T. and Lughofer, E. and Natschläger, T. and Saminger-Platz, S.},
	year = {2017},
	keywords = {Untagged},
}

@article{zhangUnderstandingDeepLearning2017,
	title = {Understanding deep learning requires rethinking generalization},
	url = {http://arxiv.org/abs/1611.03530},
	abstract = {Despite their massive size, successful deep artiﬁcial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1611.03530 [cs]},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	month = feb,
	year = {2017},
	note = {arXiv: 1611.03530},
	keywords = {Untagged},
}

@inproceedings{zhangFastCompressivePhase2017,
	title = {Fast compressive phase retrieval under bounded noise},
	booktitle = {Thirty-first {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zhang, Hongyang and You, Shan and Lin, Zhouchen and Xu, Chao},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{DBLP:conf/ijcai/ZhangMHLHT17,
	title = {Adaptively unified semi-supervised learning for cross-modal retrieval},
	url = {https://doi.org/10.24963/ijcai.2017/476},
	doi = {10.24963/ijcai.2017/476},
	booktitle = {Proceedings of the twenty-sixth international joint conference on artificial intelligence, {IJCAI} 2017, melbourne, australia, august 19-25, 2017},
	publisher = {ijcai.org},
	author = {Zhang, Liang and Ma, Bingpeng and He, Jianfeng and Li, Guorong and Huang, Qingming and Tian, Qi},
	editor = {Sierra, Carles},
	year = {2017},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/ijcai/ZhangMHLHT17.bib
tex.timestamp: Tue, 20 Aug 2019 16:16:54 +0200},
	keywords = {Untagged},
	pages = {3406--3412},
}

@inproceedings{zhangProjectionfreeDistributedOnline2017,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Projection-free {Distributed} {Online} {Learning} in {Networks}},
	volume = {70},
	url = {http://proceedings.mlr.press/v70/zhang17g.html},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}, {ICML} 2017, {Sydney}, {NSW}, {Australia}, 6-11 {August} 2017},
	publisher = {PMLR},
	author = {Zhang, Wenpeng and Zhao, Peilin and Zhu, Wenwu and Hoi, Steven C. H. and Zhang, Tong},
	editor = {Precup, Doina and Teh, Yee Whye},
	year = {2017},
	keywords = {Untagged},
	pages = {4054--4062},
}

@article{zhangSurveyMultiTaskLearning2017,
	title = {A {Survey} on {Multi}-{Task} {Learning}},
	volume = {abs/1707.08114},
	url = {http://arxiv.org/abs/1707.08114},
	journal = {CoRR},
	author = {Zhang, Yu and Yang, Qiang},
	year = {2017},
	note = {\_eprint: 1707.08114},
	keywords = {Untagged},
}

@inproceedings{zhangAgeProgressionRegression2017,
	address = {Honolulu, HI},
	title = {Age {Progression}/{Regression} by {Conditional} {Adversarial} {Autoencoder}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099946/},
	doi = {10/ghvg2x},
	abstract = {If I provide you a face image of mine (without telling you the actual age when I took the picture) and a large amount of face images that I crawled (containing labeled faces of different ages but not necessarily paired), can you show me what I would look like when I am 80 or what I was like when I was 5?” The answer is probably a “No.” Most existing face aging works attempt to learn the transformation between age groups and thus would require the paired samples as well as the labeled query image. In this paper, we look at the problem from a generative modeling perspective such that no paired samples is required. In addition, given an unlabeled image, the generative model can directly produce the image with desired age attribute. We propose a conditional adversarial autoencoder (CAAE) that learns a face manifold, traversing on which smooth age progression and regression can be realized simultaneously. In CAAE, the face is ﬁrst mapped to a latent vector through a convolutional encoder, and then the vector is projected to the face manifold conditional on age through a deconvolutional generator. The latent vector preserves personalized face features (i.e., personality) and the age condition controls progression vs. regression. Two adversarial networks are imposed on the encoder and generator, respectively, forcing to generate more photo-realistic faces. Experimental results demonstrate the appealing performance and ﬂexibility of the proposed framework by comparing with the state-of-the-art and ground truth.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhang, Zhifei and Song, Yang and Qi, Hairong},
	month = jul,
	year = {2017},
	keywords = {Untagged},
	pages = {4352--4360},
}

@inproceedings{zhao_open_2017,
	address = {Venice},
	title = {Open {Vocabulary} {Scene} {Parsing}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237483/},
	doi = {10.1109/ICCV.2017.221},
	abstract = {Recognizing arbitrary objects in the wild has been a challenging problem due to the limitations of existing classiﬁcation models and datasets. In this paper, we propose a new task that aims at parsing scenes with a large and open vocabulary, and several evaluation metrics are explored for this problem. Our approach is a joint image pixel and word concept embeddings framework, where word concepts are connected by semantic relations. We validate the open vocabulary prediction ability of our framework on ADE20K dataset which covers a wide variety of scenes and objects. We further explore the trained joint embedding space to show its interpretability.},
	language = {en},
	urldate = {2023-04-07},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhao, Hang and Puig, Xavier and Zhou, Bolei and Fidler, Sanja and Torralba, Antonio},
	month = oct,
	year = {2017},
	keywords = {Untagged},
	pages = {2021--2029},
}

@inproceedings{zhaoDependentDifferentialPrivacy2017,
	address = {Singapore},
	title = {Dependent {Differential} {Privacy} for {Correlated} {Data}},
	isbn = {978-1-5386-3920-7},
	url = {http://ieeexplore.ieee.org/document/8269219/},
	doi = {10/ghv3hb},
	abstract = {Many organizations maintain large collections of personal information. Clearly, sharing personal data may endanger the privacy of users whose data is shared. To rigorously evaluate the privacy and the utility of data publishing, the notion of differential privacy (DP) has received much attention. However, it has been shown that DP may not work well for databases with tuple correlations, which arise from various behavioral, social, and genetic relationships between users. DP masks only the presence of those records received from each user, but does not mask statistical trends that may reveal information about each user. This distinction leads to the degradation of privacy expectations in a social sense, since an adversary may still combine a query response with tuple correlation to learn about a user’s data. To extend differential privacy for correlated data, prior work has investigated various privacy metrics. Nevertheless, these privacy metrics either assume background knowledge of the adversary or lack effective and general achieving mechanisms. To overcome these limitations, this paper formalizes the notion of dependent differential privacy (DDP) such that under any tuple correlation, almost no sensitive information about any user can be leaked because of answering a query. This DDP guarantee applies to any data correlation and is independent of the adversary’s knowledge. It is shown that this DDP notion can be quantitatively deduced by DP with a stronger privacy parameter, where the difference between the privacy parameters of DDP and DP depends on the correlation between data tuples. Further, various mechanisms for achieving DDP are presented. These mechanisms are computationally efﬁcient and achieve higher utilities than mechanisms introduced in prior work to address tuple correlations. As a representative example, for data correlations modeled by an ������tuple Markov chain and a query with constant global sensitivity, the amount of noise in a Laplace mechanism proposed here does not scale with ������, whereas the level of the noise added by the state-of-the-art mechanism of Liu et al. scales linearly with ������, so the proposed mechanism achieves higher utility than that of the state-of-the-art.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {2017 {IEEE} {Globecom} {Workshops} ({GC} {Wkshps})},
	publisher = {IEEE},
	author = {Zhao, Jun and Zhang, Junshan and Poor, H. Vincent},
	year = {2017},
	keywords = {Untagged},
	pages = {1--7},
}

@article{zhaoLearningMonitorMachine2017,
	title = {Learning to {Monitor} {Machine} {Health} with {Convolutional} {Bi}-{Directional} {LSTM} {Networks}},
	volume = {17},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/17/2/273},
	doi = {10/gfth36},
	abstract = {In modern manufacturing systems and industries, more and more research efforts have been made in developing effective machine health monitoring systems. Among various machine health monitoring approaches, data-driven methods are gaining in popularity due to the development of advanced sensing and data analytic techniques. However, considering the noise, varying length and irregular sampling behind sensory data, this kind of sequential data cannot be fed into classiﬁcation and regression models directly. Therefore, previous work focuses on feature extraction/fusion methods requiring expensive human labor and high quality expert knowledge. With the development of deep learning methods in the last few years, which redeﬁne representation learning from raw data, a deep neural network structure named Convolutional Bi-directional Long Short-Term Memory networks (CBLSTM) has been designed here to address raw sensory data. CBLSTM ﬁrstly uses CNN to extract local features that are robust and informative from the sequential input. Then, bi-directional LSTM is introduced to encode temporal information. Long Short-Term Memory networks(LSTMs) are able to capture long-term dependencies and model sequential data, and the bi-directional structure enables the capture of past and future contexts. Stacked, fully-connected layers and the linear regression layer are built on top of bi-directional LSTMs to predict the target value. Here, a real-life tool wear test is introduced, and our proposed CBLSTM is able to predict the actual tool wear based on raw sensory data. The experimental results have shown that our model is able to outperform several state-of-the-art baseline methods.},
	language = {en},
	number = {2},
	urldate = {2021-09-27},
	journal = {Sensors},
	author = {Zhao, Rui and Yan, Ruqiang and Wang, Jinjiang and Mao, Kezhi},
	month = feb,
	year = {2017},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Untagged},
	pages = {273},
}

@inproceedings{zhengLearningMultiattentionConvolutional2017,
	address = {Venice, Italy},
	title = {Learning {Multi}-attention {Convolutional} {Neural} {Network} for {Fine}-{Grained} {Image} {Recognition}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237819/},
	doi = {10/gfsvgc},
	abstract = {Recognizing ﬁne-grained categories (e.g., bird species) highly relies on discriminative part localization and partbased ﬁne-grained feature learning. Existing approaches predominantly solve these challenges independently, while neglecting the fact that part localization (e.g., head of a bird) and ﬁne-grained feature learning (e.g., head shape) are mutually correlated. In this paper, we propose a novel part learning approach by a multi-attention convolutional neural network (MA-CNN), where part generation and feature learning can reinforce each other. MA-CNN consists of convolution, channel grouping and part classiﬁcation sub-networks. The channel grouping network takes as input feature channels from convolutional layers, and generates multiple parts by clustering, weighting and pooling from spatially-correlated channels. The part classiﬁcation network further classiﬁes an image by each individual part, through which more discriminative ﬁne-grained features can be learned. Two losses are proposed to guide the multi-task learning of channel grouping and part classiﬁcation, which encourages MA-CNN to generate more discriminative parts from feature channels and learn better ﬁne-grained features from parts in a mutual reinforced way. MA-CNN does not need bounding box/part annotation and can be trained end-to-end. We incorporate the learned parts from MA-CNN with part-CNN for recognition, and show the best performances on three challenging published ﬁne-grained datasets, e.g., CUB-Birds, FGVC-Aircraft and Stanford-Cars.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Zheng, Heliang and Fu, Jianlong and Mei, Tao and Luo, Jiebo},
	year = {2017},
	keywords = {Untagged},
	pages = {5219--5227},
}

@inproceedings{zhengUnlabeledSamplesGenerated2017,
	address = {Venice, Italy},
	title = {Unlabeled {Samples} {Generated} by {GAN} {Improve} the {Person} {Re}-identification {Baseline} in {Vitro}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237667/},
	doi = {10/ggv9qz},
	abstract = {The main contribution of this paper is a simple semisupervised pipeline that only uses the original training set without collecting extra data. It is challenging in 1) how to obtain more training data only from the training set and 2) how to use the newly generated data. In this work, the generative adversarial network (GAN) is used to generate unlabeled samples. We propose the label smoothing regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Zheng, Zhedong and Zheng, Liang and Yang, Yi},
	year = {2017},
	keywords = {Untagged},
	pages = {3774--3782},
}

@inproceedings{zhong_re-ranking_2017,
	address = {Honolulu, HI},
	title = {Re-ranking {Person} {Re}-identification with k-{Reciprocal} {Encoding}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099872/},
	doi = {10.1109/CVPR.2017.389},
	abstract = {When considering person re-identiﬁcation (re-ID) as a retrieval process, re-ranking is a critical step to improve its accuracy. Yet in the re-ID community, limited effort has been devoted to re-ranking, especially those fully automatic, unsupervised solutions. In this paper, we propose a ���-reciprocal encoding method to re-rank the re-ID results. Our hypothesis is that if a gallery image is similar to the probe in the ���-reciprocal nearest neighbors, it is more likely to be a true match. Speciﬁcally, given an image, a ���reciprocal feature is calculated by encoding its ���-reciprocal nearest neighbors into a single vector, which is used for reranking under the Jaccard distance. The ﬁnal distance is computed as the combination of the original distance and the Jaccard distance. Our re-ranking method does not require any human interaction or any labeled data, so it is applicable to large-scale datasets. Experiments on the largescale Market-1501, CUHK03, MARS, and PRW datasets conﬁrm the effectiveness of our method1.},
	language = {en},
	urldate = {2023-02-03},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhong, Zhun and Zheng, Liang and Cao, Donglin and Li, Shaozi},
	month = jul,
	year = {2017},
	keywords = {Untagged},
	pages = {3652--3661},
}

@article{zhouGeneGANLearningObject2017,
	title = {{GeneGAN}: {Learning} {Object} {Transfiguration} and {Attribute} {Subspace} from {Unpaired} {Data}},
	shorttitle = {{GeneGAN}},
	url = {http://arxiv.org/abs/1705.04932},
	abstract = {Object Transﬁguration replaces an object in an image with another object from a second image. For example it can perform tasks like “putting exactly those eyeglasses from image A on the nose of the person in image B”. Usage of exemplar images allows more precise speciﬁcation of desired modiﬁcations and improves the diversity of conditional image generation. However, previous methods that rely on feature space operations, require paired data and/or appearance models for training or disentangling objects from background. In this work, we propose a model that can learn object transﬁguration from two unpaired sets of images: one set containing images that “have” that kind of object, and the other set being the opposite, with the mild constraint that the objects be located approximately at the same place. For example, the training data can be one set of reference face images that have eyeglasses, and another set of images that have not, both of which spatially aligned by face landmarks. Despite the weak 0/1 labels, our model can learn an “eyeglasses” subspace that contain multiple representatives of different types of glasses. Consequently, we can perform ﬁne-grained control of generated images, like swapping the glasses in two images by swapping the projected components in the “eyeglasses” subspace, to create novel images of people wearing eyeglasses.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1705.04932 [cs]},
	author = {Zhou, Shuchang and Xiao, Taihong and Yang, Yi and Feng, Dieqiao and He, Qinyao and He, Weiran},
	month = may,
	year = {2017},
	note = {arXiv: 1705.04932},
	keywords = {Untagged},
}

@article{zhouPersonalizedOccupationalawareAge2017,
	title = {Personalized and {Occupational}-aware {Age} {Progression} by {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1711.09368},
	abstract = {Face age progression, which aims to predict the future looks, is important for various applications and has been received considerable attentions. Existing methods and datasets are limited in exploring the effects of occupations which may inﬂuence the personal appearances. In this paper, we ﬁrstly introduce an occupational face aging dataset for studying the inﬂuences of occupations on the appearances. It includes ﬁve occupations, which enables the development of new algorithms for age progression and facilitate future researches. Second, we propose a new occupationalaware adversarial face aging network, which learns human aging process under different occupations. Two factors are taken into consideration in our aging process: personalitypreserving and visually plausible texture change for different occupations. We propose personalized network with personalized loss in deep autoencoder network for keeping personalized facial characteristics, and occupationalaware adversarial network with occupational-aware adversarial loss for obtaining more realistic texture changes. Experimental results well demonstrate the advantages of the proposed method by comparing with other state-of-the-arts age progression methods.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1711.09368 [cs]},
	author = {Zhou, Siyu and Zhao, Weiqiang and Feng, Jiashi and Lai, Hanjiang and Pan, Yan and Yin, Jian and Yan, Shuicheng},
	month = dec,
	year = {2017},
	note = {arXiv: 1711.09368},
	keywords = {Untagged},
}

@inproceedings{zhuLearningSpatialRegularization2017,
	address = {Honolulu, HI},
	title = {Learning {Spatial} {Regularization} with {Image}-{Level} {Supervisions} for {Multi}-label {Image} {Classification}},
	url = {https://doi.org/10.1109/CVPR.2017.219},
	doi = {10/ggfcwb},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Zhu, Feng and Li, Hongsheng and Ouyang, Wanli and Yu, Nenghai and Wang, Xiaogang},
	year = {2017},
	keywords = {Untagged},
	pages = {2027--2036},
}

@inproceedings{zhuUnpairedImagetoImageTranslation2017,
	title = {Unpaired {Image}-to-{Image} {Translation} {Using} {Cycle}-{Consistent} {Adversarial} {Networks}},
	url = {https://doi.org/10.1109/ICCV.2017.244},
	doi = {10/gfhw33},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}, {ICCV} 2017, {Venice}, {Italy}, {October} 22-29, 2017},
	publisher = {IEEE Computer Society},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	year = {2017},
	keywords = {Untagged},
	pages = {2242--2251},
}

@techreport{DataMining20172017,
	title = {Data {Mining} 2017: {Privacy} and {Differential} {Privacy} {Slides} stolen from {Kobbi} {Nissim} (with permission)},
	year = {2017},
	keywords = {Untagged},
}

@techreport{NeuralNetworkApproximation2017,
	address = {Megvii},
	title = {Neural {Network} {Approximation}},
	year = {2017},
	keywords = {Untagged},
}

@inproceedings{ahmedBlindDeconvolutionalPhase2018,
	address = {Montréal, Canada},
	title = {Blind {Deconvolutional} {Phase} {Retrieval} via {Convex} {Programming}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/d5b3d8dadd770c460b1cde910a711987-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ahmed, Ali and Aghasi, Alireza and Hand, Paul},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	keywords = {Untagged},
	pages = {10051--10061},
}

@inproceedings{ahmedMaskConnectConnectivityLearning2018,
	address = {Munich, Germany},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{MaskConnect}: {Connectivity} {Learning} by {Gradient} {Descent}},
	volume = {11209},
	url = {https://doi.org/10.1007/978-3-030-01228-1_22},
	doi = {10/ghwnfw},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Ahmed, Karim and Torresani, Lorenzo},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Untagged},
	pages = {362--378},
}

@article{akhtar_threat_2018,
	title = {Threat of {Adversarial} {Attacks} on {Deep} {Learning} in {Computer} {Vision}: {A} {Survey}},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Threat of {Adversarial} {Attacks} on {Deep} {Learning} in {Computer} {Vision}},
	doi = {10.1109/ACCESS.2018.2807385},
	abstract = {Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.},
	journal = {IEEE Access},
	author = {Akhtar, Naveed and Mian, Ajmal},
	year = {2018},
	note = {Conference Name: IEEE Access},
	keywords = {Untagged},
	pages = {14410--14430},
}

@article{albadawy2018deep,
	title = {Deep learning for segmentation of brain tumors: {Impact} of cross-institutional training and testing},
	volume = {45},
	number = {3},
	journal = {Medical physics},
	author = {AlBadawy, Ehab A and Saha, Ashirbani and Mazurowski, Maciej A},
	year = {2018},
	note = {Publisher: Wiley Online Library},
	keywords = {Untagged},
	pages = {1150--1158},
}

@inproceedings{alizadehEmpiricalStudyBinary2018,
	title = {An {Empirical} study of {Binary} {Neural} {Networks}' {Optimisation}},
	url = {https://openreview.net/forum?id=rJfUCoR5KX},
	abstract = {Binary neural networks using the Straight-Through-Estimator (STE) have been shown to achieve state-of-the-art results, but their training process is not well-founded. This is due to the discrepancy...},
	language = {en},
	urldate = {2022-02-23},
	author = {Alizadeh, Milad and Fernández-Marqués, Javier and Lane, Nicholas D. and Gal, Yarin},
	month = sep,
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{allen-zhuNatashaFasterNonConvex2018,
	address = {Montréal, Canada},
	title = {Natasha 2: {Faster} {Non}-{Convex} {Optimization} {Than} {SGD}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/79a49b3e3762632813f9e35f4ba53d6c-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Allen-Zhu, Zeyuan},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	keywords = {Untagged},
	pages = {2680--2691},
}

@inproceedings{alzantot_generating_2018,
	address = {Brussels, Belgium},
	title = {Generating {Natural} {Language} {Adversarial} {Examples}},
	url = {https://aclanthology.org/D18-1316},
	doi = {10.18653/v1/D18-1316},
	abstract = {Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations can often be made virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97\% and 70\%, respectively. We additionally demonstrate that 92.3\% of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.},
	urldate = {2023-02-27},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Alzantot, Moustafa and Sharma, Yash and Elgohary, Ahmed and Ho, Bo-Jhang and Srivastava, Mani and Chang, Kai-Wei},
	month = oct,
	year = {2018},
	keywords = {Untagged},
	pages = {2890--2896},
}

@inproceedings{andersonHighDimensionalGeometryBinary2018,
	title = {The {High}-{Dimensional} {Geometry} of {Binary} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=B1IDRdeCW},
	abstract = {Recent successes of Binary Neural Networks can be understood based on the geometry of high-dimensional binary vectors},
	language = {en},
	urldate = {2022-02-28},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Anderson, Alexander G. and Berg, Cory P.},
	month = feb,
	year = {2018},
	keywords = {Untagged},
}

@article{andersonBottomUpTopDownAttention2018,
	title = {Bottom-{Up} and {Top}-{Down} {Attention} for {Image} {Captioning} and {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/1707.07998},
	abstract = {Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through ﬁne-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and topdown attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain ﬁrst place in the 2017 VQA Challenge.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1707.07998 [cs]},
	author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
	month = mar,
	year = {2018},
	note = {arXiv: 1707.07998},
	keywords = {Untagged},
}

@inproceedings{andersonVisionandLanguageNavigationInterpreting2018,
	address = {Salt Lake City, UT},
	title = {Vision-and-{Language} {Navigation}: {Interpreting} {Visually}-{Grounded} {Navigation} {Instructions} in {Real} {Environments}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Vision-and-{Language} {Navigation}},
	url = {https://ieeexplore.ieee.org/document/8578485/},
	doi = {10/ggjfsc},
	abstract = {A robot that can carry out a natural-language instruction has been a dream since before the Jetsons cartoon series imagined a life of leisure mediated by a ﬂeet of attentive robot helpers. It is a dream that remains stubbornly distant. However, recent advances in vision and language methods have made incredible progress in closely related areas. This is signiﬁcant because a robot interpreting a naturallanguage navigation instruction on the basis of what it sees is carrying out a vision and language process that is similar to Visual Question Answering. Both tasks can be interpreted as visually grounded sequence-to-sequence translation problems, and many of the same methods are applicable. To enable and encourage the application of vision and language methods to the problem of interpreting visuallygrounded navigation instructions, we present the Matterport3D Simulator – a large-scale reinforcement learning environment based on real imagery [11]. Using this simulator, which can in future support a range of embodied vision and language tasks, we provide the ﬁrst benchmark dataset for visually-grounded natural language navigation in real buildings – the Room-to-Room (R2R) dataset1.},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Anderson, Peter and Wu, Qi and Teney, Damien and Bruce, Jake and Johnson, Mark and Sunderhauf, Niko and Reid, Ian and Gould, Stephen and van den Hengel, Anton},
	month = jun,
	year = {2018},
	keywords = {Untagged},
	pages = {3674--3683},
}

@article{angiulli_behavior_2018,
	title = {On the {Behavior} of {Intrinsically} {High}-{Dimensional} {Spaces}: {Distances}, {Direct} and {Reverse} {Nearest} {Neighbors}, and {Hubness}},
	volume = {18},
	url = {http://jmlr.org/papers/v18/17-151.html},
	number = {170},
	journal = {Journal of Machine Learning Research},
	author = {Angiulli, Fabrizio},
	year = {2018},
	keywords = {Untagged},
	pages = {1--60},
}

@inproceedings{NEURIPS2018_a9813e95,
	title = {Differentially private robust low-rank approximation},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/file/a9813e9550fee3110373c21fa012eee7-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Arora, Raman and braverman, Vladimir and Upadhyay, Jalaj},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{ashokN2NLearningNetwork2018,
	address = {Vancouver, BC, Canada},
	title = {{N2N} learning: {Network} to {Network} {Compression} via {Policy} {Gradient} {Reinforcement} {Learning}},
	url = {https://openreview.net/forum?id=B1hcZZ-AW},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Ashok, Anubhav and Rhinehart, Nicholas and Beainy, Fares and Kitani, Kris M.},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{athalye_obfuscated_2018,
	series = {Proceedings of machine learning research},
	title = {Obfuscated gradients give a false sense of security: {Circumventing} defenses to adversarial examples},
	volume = {80},
	url = {https://proceedings.mlr.press/v80/athalye18a.html},
	abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	note = {tex.pdf: http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf},
	keywords = {Untagged},
	pages = {274--283},
}

@inproceedings{athalye_synthesizing_2018,
	title = {Synthesizing {Robust} {Adversarial} {Examples}},
	url = {https://proceedings.mlr.press/v80/athalye18b.html},
	abstract = {Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.},
	language = {en},
	urldate = {2022-05-29},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {284--293},
}

@inproceedings{azadiMultiContentGANFewShot2018,
	title = {Multi-{Content} {GAN} for {Few}-{Shot} {Font} {Style} {Transfer}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Azadi_Multi-Content_GAN_for_CVPR_2018_paper.html},
	doi = {10/gf5bqb},
	booktitle = {2018 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2018, {Salt} {Lake} {City}, {UT}, {USA}, {June} 18-22, 2018},
	publisher = {IEEE Computer Society},
	author = {Azadi, Samaneh and Fisher, Matthew and Kim, Vladimir G. and Wang, Zhaowen and Shechtman, Eli and Darrell, Trevor},
	year = {2018},
	keywords = {Untagged},
	pages = {7564--7573},
}

@article{bagherinezhadLabelRefineryImproving2018,
	title = {Label {Refinery}: {Improving} {ImageNet} {Classification} through {Label} {Progression}},
	shorttitle = {Label {Refinery}},
	url = {http://arxiv.org/abs/1805.02641},
	abstract = {Among the three main components (data, labels, and models) of any supervised learning system, data and models have been the main subjects of active research. However, studying labels and their properties has received very little attention. Current principles and paradigms of labeling impose several challenges to machine learning algorithms. Labels are often incomplete, ambiguous, and redundant. In this paper we study the eﬀects of various properties of labels and introduce the Label Reﬁnery: an iterative procedure that updates the ground truth labels after examining the entire dataset. We show signiﬁcant gain using reﬁned labels across a wide range of models. Using a Label Reﬁnery improves the state-of-the-art top-1 accuracy of (1) AlexNet from 59.3 to 67.2, (2) MobileNet1 from 70.6 to 73.39, (3) MobileNet0.25 from 50.6 to 55.59, (4) VGG19 from 72.7 to 75.46, and (5) Darknet19 from 72.9 to 74.47.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1805.02641 [cs]},
	author = {Bagherinezhad, Hessam and Horton, Maxwell and Rastegari, Mohammad and Farhadi, Ali},
	month = may,
	year = {2018},
	note = {arXiv: 1805.02641},
	keywords = {Untagged},
}

@inproceedings{bakDomainAdaptationSynthesis2018,
	address = {Munich, Germany},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Domain {Adaptation} {Through} {Synthesis} for {Unsupervised} {Person} {Re}-identification},
	volume = {11217},
	url = {https://doi.org/10.1007/978-3-030-01261-8_12},
	doi = {10/ggthsm},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Bak, Slawomir and Carr, Peter and Lalonde, Jean-François},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Untagged},
	pages = {193--209},
}

@inproceedings{balcanMatrixCompletionRelated2018,
	address = {Cambridge, MA},
	series = {{LIPIcs}},
	title = {Matrix {Completion} and {Related} {Problems} via {Strong} {Duality}},
	volume = {94},
	url = {https://doi.org/10.4230/LIPIcs.ITCS.2018.5},
	doi = {10/ghwnr3},
	booktitle = {Innovations in {Theoretical} {Computer} {Science} {Conference}},
	publisher = {Schloss Dagstuhl - Leibniz-Zentrum für Informatik},
	author = {Balcan, Maria-Florina and Liang, Yingyu and Woodruff, David P. and Zhang, Hongyang},
	editor = {Karlin, Anna R.},
	year = {2018},
	keywords = {Untagged},
	pages = {5:1--5:22},
}

@techreport{balleShortTutorialDifferential2018,
	address = {Alan Turing Institute},
	title = {A {Short} {Tutorial} on {Differential} {Privacy}},
	language = {en},
	institution = {Amazon Research Cambridge},
	author = {Balle, Borja},
	month = jan,
	year = {2018},
	keywords = {Untagged},
	pages = {75},
}

@inproceedings{beery_recognition_2018,
	address = {Munich, Germany},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Recognition in {Terra} {Incognita}},
	volume = {11220},
	shorttitle = {Terra {Incognita}},
	url = {https://doi.org/10.1007/978-3-030-01270-0\_28},
	doi = {10.1007/978-3-030-01270-0_28},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Beery, Sara and Horn, Grant Van and Perona, Pietro},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Untagged},
	pages = {472--489},
}

@inproceedings{baoOpenSetIdentityPreserving2018,
	title = {Towards {Open}-{Set} {Identity} {Preserving} {Face} {Synthesis}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Bao_Towards_Open-Set_Identity_CVPR_2018_paper.html},
	doi = {10/gg2v96},
	booktitle = {2018 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2018, {Salt} {Lake} {City}, {UT}, {USA}, {June} 18-22, 2018},
	publisher = {IEEE Computer Society},
	author = {Bao, Jianmin and Chen, Dong and Wen, Fang and Li, Houqiang and Hua, Gang},
	year = {2018},
	keywords = {Untagged},
	pages = {6713--6722},
}

@inproceedings{belinkov2018synthetic,
	title = {Synthetic and natural noise both break neural machine translation},
	url = {https://openreview.net/forum?id=BJ8vJebC-},
	booktitle = {International conference on learning representations},
	author = {Belinkov, Yonatan and Bisk, Yonatan},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{benderUnderstandingSimplifyingOneShot2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Understanding and {Simplifying} {One}-{Shot} {Architecture} {Search}},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/bender18a.html},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018, {Stockholmsmässan}, {Stockholm}, {Sweden}, {July} 10-15, 2018},
	publisher = {PMLR},
	author = {Bender, Gabriel and Kindermans, Pieter-Jan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc V.},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	keywords = {Untagged},
	pages = {549--558},
}

@inproceedings{bernsteinSIGNSGDCompressedOptimisation2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{SIGNSGD}: {Compressed} {Optimisation} for {Non}-{Convex} {Problems}},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/bernstein18a.html},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018, {Stockholmsmässan}, {Stockholm}, {Sweden}, {July} 10-15, 2018},
	publisher = {PMLR},
	author = {Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	keywords = {Untagged},
	pages = {559--568},
}

@inproceedings{bjorckUnderstandingBatchNormalization2018,
	address = {Montréal, Canada},
	title = {Understanding {Batch} {Normalization}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/36072923bfc3cf47745d704feb489480-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bjorck, Johan and Gomes, Carla P. and Selman, Bart and Weinberger, Kilian Q.},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	keywords = {Untagged},
	pages = {7705--7716},
}

@article{bottouOptimizationMethodsLargeScale2018,
	title = {Optimization {Methods} for {Large}-{Scale} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1606.04838},
	abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classiﬁcation and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:1606.04838 [cs, math, stat]},
	author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
	month = feb,
	year = {2018},
	note = {arXiv: 1606.04838},
	keywords = {Untagged},
}

@article{bouwmansApplicationsRobustPCA2018,
	title = {On the applications of robust {PCA} in image and video processing},
	volume = {106},
	doi = {10.1109/JPROC.2018.2853589},
	number = {8},
	journal = {Proceedings of the IEEE},
	author = {Bouwmans, Thierry and Javed, Sajid and Zhang, Hongyang and Lin, Zhouchen and Otazo, Ricardo},
	year = {2018},
	note = {Publisher: IEEE},
	keywords = {Untagged},
	pages = {1427--1457},
}

@article{bouzaraaLearnableExposureFusion2018,
	title = {Learnable {Exposure} {Fusion} for {Dynamic} {Scenes}},
	url = {http://arxiv.org/abs/1804.01611},
	abstract = {In this paper, we focus on Exposure Fusion (EF) [16] for dynamic scenes. The task is to fuse multiple images obtained by exposure bracketing to create an image which comprises a high level of details. Typically, such images are not possible to obtain directly from a camera due to hardware limitations, e.g., a limited dynamic range of the sensor. A major problem of such tasks is that the images may not be spatially aligned due to scene motion or camera motion. It is known that the required alignment by image registration problems is ill-posed. In this case, the images to be aligned vary in their intensity range, which makes the problem even more difﬁcult.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1804.01611 [cs]},
	author = {Bouzaraa, Fahd and Halfaoui, Ibrahim and Urfalioglu, Onay},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.01611},
	keywords = {Untagged},
}

@inproceedings{brendel_approximating_2018,
	title = {Approximating {CNNs} with {Bag}-of-local-{Features} models works surprisingly well on {ImageNet}},
	url = {https://openreview.net/forum?id=SkfMWhAqYQ},
	abstract = {Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6\% top-5 for 32 x 32 px features and Alexnet performance for 16 x16 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.},
	language = {en},
	urldate = {2023-06-02},
	author = {Brendel, Wieland and Bethge, Matthias},
	month = dec,
	year = {2018},
	keywords = {Untagged},
}

@article{brendelAdversarialVisionChallenge2018,
	title = {Adversarial vision challenge},
	journal = {arXiv preprint arXiv:1808.01976},
	author = {Brendel, Wieland and Rauber, Jonas and Kurakin, Alexey and Papernot, Nicolas and Veliqi, Behar and Salathé, Marcel and Mohanty, Sharada P. and Bethge, Matthias},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{buckman_thermometer_2018,
	title = {Thermometer {Encoding}: {One} {Hot} {Way} {To} {Resist} {Adversarial} {Examples}},
	shorttitle = {Thermometer {Encoding}},
	url = {https://openreview.net/forum?id=S18Su--CW},
	abstract = {Input discretization leads to robustness against adversarial examples},
	language = {en},
	urldate = {2022-05-29},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Buckman, Jacob and Roy, Aurko and Raffel, Colin and Goodfellow, Ian},
	month = feb,
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{caesarCOCOStuffThingStuff2018,
	title = {{COCO}-{Stuff}: {Thing} and {Stuff} {Classes} in {Context}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Caesar_COCO-Stuff_Thing_and_CVPR_2018_paper.html},
	doi = {10/ggn89h},
	booktitle = {2018 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2018, {Salt} {Lake} {City}, {UT}, {USA}, {June} 18-22, 2018},
	publisher = {IEEE Computer Society},
	author = {Caesar, Holger and Uijlings, Jasper R. R. and Ferrari, Vittorio},
	year = {2018},
	keywords = {Untagged},
	pages = {1209--1218},
}

@inproceedings{caiPathLevelNetworkTransformation2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Path-{Level} {Network} {Transformation} for {Efficient} {Architecture} {Search}},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/cai18a.html},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018, {Stockholmsmässan}, {Stockholm}, {Sweden}, {July} 10-15, 2018},
	publisher = {PMLR},
	author = {Cai, Han and Yang, Jiacheng and Zhang, Weinan and Han, Song and Yu, Yong},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	keywords = {Untagged},
	pages = {677--686},
}

@inproceedings{caiPathLevelNetworkTransformation2018a,
	title = {Path-{Level} {Network} {Transformation} for {Efﬁcient} {Architecture} {Search}},
	language = {en},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {Stockholm}, {Sweden},},
	author = {Cai, Han and Yang, Jiacheng and Zhang, Weinan and Han, Song and Yu, Yong},
	year = {2018},
	keywords = {Untagged},
	pages = {10},
}

@article{caoLoadBalancedGANs2018,
	title = {Load {Balanced} {GANs} for {Multi}-view {Face} {Image} {Synthesis}},
	url = {http://arxiv.org/abs/1802.07447},
	abstract = {Multi-view face synthesis from a single image is an ill-posed problem and often suffers from serious appearance distortion. Producing photo-realistic and identity preserving multi-view results is still a not well deﬁned synthesis problem. This paper proposes Load Balanced Generative Adversarial Networks (LB-GAN) to precisely rotate the yaw angle of an input face image to any speciﬁed angle. LBGAN decomposes the challenging synthesis problem into two well constrained subtasks that correspond to a face normalizer and a face editor respectively. The normalizer ﬁrst frontalizes an input image, and then the editor rotates the frontalized image to a desired pose guided by a remote code. In order to generate photo-realistic local details, the normalizer and the editor are trained in a two-stage manner and regulated by a conditional self-cycle loss and an attention based L2 loss. Exhaustive experiments on controlled and uncontrolled environments demonstrate that the proposed method not only improves the visual realism of multi-view synthetic images, but also preserves identity information well.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1802.07447 [cs]},
	author = {Cao, Jie and Hu, Yibo and Yu, Bing and He, Ran and Sun, Zhenan},
	month = mar,
	year = {2018},
	note = {arXiv: 1802.07447},
	keywords = {Untagged},
}

@inproceedings{caoHashGANDeepLearning2018,
	address = {Salt Lake City, UT},
	title = {{HashGAN}: {Deep} {Learning} to {Hash} with {Pair} {Conditional} {Wasserstein} {GAN}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{HashGAN}},
	url = {https://ieeexplore.ieee.org/document/8578238/},
	doi = {10/ghv44r},
	abstract = {Deep learning to hash improves image retrieval performance by end-to-end representation learning and hash coding from training data with pairwise similarity information. Subject to the scarcity of similarity information that is often expensive to collect for many application domains, existing deep learning to hash methods may overﬁt the training data and result in substantial loss of retrieval quality. This paper presents HashGAN, a novel architecture for deep learning to hash, which learns compact binary hash codes from both real images and diverse images synthesized by generative models. The main idea is to augment the training data with nearly real images synthesized from a new Pair Conditional Wasserstein GAN (PC-WGAN) conditioned on the pairwise similarity information. Extensive experiments demonstrate that HashGAN can generate high-quality binary hash codes and yield state-of-the-art image retrieval performance on three benchmarks, NUS-WIDE, CIFAR-10, and MS-COCO.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Cao, Yue and Liu, Bin and Long, Mingsheng and Wang, Jianmin},
	month = jun,
	year = {2018},
	keywords = {Untagged},
	pages = {1287--1296},
}

@inproceedings{carreira-perpinanLearningCompressionAlgorithmsNeural2018,
	address = {Salt Lake City, UT},
	title = {"{Learning}-{Compression}" {Algorithms} for {Neural} {Net} {Pruning}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578988/},
	doi = {10/ggffs9},
	abstract = {Pruning a neural net consists of removing weights without degrading its performance. This is an old problem of renewed interest because of the need to compress ever larger nets so they can run in mobile devices. Pruning has been traditionally done by ranking or penalizing weights according to some criterion (such as magnitude), removing low-ranked weights and retraining the remaining ones. We formulate pruning as an optimization problem of ﬁnding the weights that minimize the loss while satisfying a pruning cost condition. We give a generic algorithm to solve this which alternates “learning” steps that optimize a regularized, data-dependent loss and “compression” steps that mark weights for pruning in a data-independent way. Magnitude thresholding arises naturally in the compression step, but unlike existing magnitude pruning approaches, our algorithm explores subsets of weights rather than committing irrevocably to a speciﬁc subset from the beginning. It is also able to learn automatically the best number of weights to prune in each layer of the net without incurring an exponentially costly model selection. Using a single pruninglevel user parameter, we achieve state-of-the-art pruning in LeNet and ResNets of various sizes.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Carreira-Perpinan, Miguel A. and Idelbayev, Yerlan},
	month = jun,
	year = {2018},
	keywords = {Untagged},
	pages = {8532--8541},
}

@inproceedings{cesa-bianchiNonstochasticBanditsComposite2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Nonstochastic {Bandits} with {Composite} {Anonymous} {Feedback}},
	volume = {75},
	url = {http://proceedings.mlr.press/v75/cesa-bianchi18a.html},
	urldate = {2022-02-25},
	booktitle = {Conference {On} {Learning} {Theory}, {COLT} 2018, {Stockholm}, {Sweden}, 6-9 {July} 2018},
	publisher = {PMLR},
	author = {Cesa-Bianchi, Nicolò and Gentile, Claudio and Mansour, Yishay},
	editor = {Bubeck, Sébastien and Perchet, Vianney and Rigollet, Philippe},
	year = {2018},
	keywords = {Untagged},
	pages = {750--773},
}

@article{chakrabortyAdversarialAttacksDefences2018,
	title = {Adversarial {Attacks} and {Defences}: {A} {Survey}},
	shorttitle = {Adversarial {Attacks} and {Defences}},
	url = {http://arxiv.org/abs/1810.00069},
	abstract = {Deep learning has emerged as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. In the last few years, deep learning has advanced radically in such a way that it can surpass human-level performance on a number of tasks. As a consequence, deep learning is being extensively used in most of the recent day-to-day applications. However, security of deep learning systems are vulnerable to crafted adversarial examples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where adversaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. In this paper, we attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate the efficiency and challenges of recent countermeasures against them.},
	language = {en},
	urldate = {2022-01-25},
	journal = {arXiv:1810.00069},
	author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
	month = sep,
	year = {2018},
	note = {arXiv: 1810.00069},
	keywords = {Untagged},
}

@inproceedings{chaoRethinkingFasterRCNN2018,
	title = {Rethinking the {Faster} {R}-{CNN} {Architecture} for {Temporal} {Action} {Localization}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Chao_Rethinking_the_Faster_CVPR_2018_paper.html},
	doi = {10/ghwngk},
	booktitle = {2018 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2018, {Salt} {Lake} {City}, {UT}, {USA}, {June} 18-22, 2018},
	publisher = {IEEE Computer Society},
	author = {Chao, Yu-Wei and Vijayanarasimhan, Sudheendra and Seybold, Bryan and Ross, David A. and Deng, Jia and Sukthankar, Rahul},
	year = {2018},
	keywords = {Untagged},
	pages = {1130--1139},
}

@inproceedings{ferrari_encoder-decoder_2018,
	address = {Cham},
	title = {Encoder-{Decoder} with {Atrous} {Separable} {Convolution} for {Semantic} {Image} {Segmentation}},
	volume = {11211},
	isbn = {978-3-030-01233-5 978-3-030-01234-2},
	shorttitle = {{DeepLabv3}+},
	url = {https://link.springer.com/10.1007/978-3-030-01234-2_49},
	abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with ﬁlters or pooling operations at multiple rates and multiple eﬀective ﬁelds-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Speciﬁcally, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet eﬀective decoder module to reﬁne the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89\% and 82.1\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorﬂow at https: //github.com/tensorflow/models/tree/master/research/deeplab.},
	language = {en},
	urldate = {2022-11-21},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01234-2_49},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {833--851},
}

@inproceedings{chenProjectionFreeOnlineOptimization2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Projection-{Free} {Online} {Optimization} with {Stochastic} {Gradient}: {From} {Convexity} to {Submodularity}},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/chen18c.html},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018, {Stockholmsmässan}, {Stockholm}, {Sweden}, {July} 10-15, 2018},
	publisher = {PMLR},
	author = {Chen, Lin and Harshaw, Christopher and Hassani, Hamed and Karbasi, Amin},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	keywords = {Untagged},
	pages = {813--822},
}

@article{chenDifferentiallyPrivateData2018,
	title = {Differentially {Private} {Data} {Generative} {Models}},
	url = {http://arxiv.org/abs/1812.02274},
	abstract = {Deep neural networks (DNNs) have recently been widely adopted in various applications, and such success is largely due to a combination of algorithmic breakthroughs, computation resource improvements, and access to a large amount of data. However, the large-scale data collections required for deep learning often contain sensitive information, therefore raising many privacy concerns. Prior research has shown several successful attacks in inferring sensitive training data information, such as model inversion, membership inference, and generative adversarial networks (GAN) based leakage attacks against collaborative deep learning. In this paper, to enable learning efficiency as well as to generate data with privacy guarantees and high utility, we propose a differentially private autoencoder-based generative model (DP-AuGM) and a differentially private variational autoencoder-based generative model (DP-VaeGM). We evaluate the robustness of two proposed models. We show that DP-AuGM can effectively defend against the model inversion, membership inference, and GAN-based attacks. We also show that DP-VaeGM is robust against the membership inference attack. We conjecture that the key to defend against the model inversion and GAN-based attacks is not due to differential privacy but the perturbation of training data. Finally, we demonstrate that both DP-AuGM and DP-VaeGM can be easily integrated with real-world machine learning applications, such as machine learning as a service and federated learning, which are otherwise threatened by the membership inference attack and the GAN-based attack, respectively.},
	urldate = {2022-03-21},
	journal = {arXiv:1812.02274 [cs]},
	author = {Chen, Qingrong and Xiang, Chong and Xue, Minhui and Li, Bo and Borisov, Nikita and Kaarfar, Dali and Zhu, Haojin},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.02274},
	keywords = {Untagged},
}

@incollection{chenMobileFaceNetsEfficientCNNs2018,
	address = {Cham},
	title = {{MobileFaceNets}: {Efficient} {CNNs} for {Accurate} {Real}-{Time} {Face} {Verification} on {Mobile} {Devices}},
	volume = {10996},
	isbn = {978-3-319-97908-3 978-3-319-97909-0},
	shorttitle = {{MobileFaceNets}},
	url = {http://link.springer.com/10.1007/978-3-319-97909-0_46},
	abstract = {In this paper, we present a class of extremely efficient CNN models called MobileFaceNets, which use no more than 1 million parameters and specifically tailored for high-accuracy real-time face verification on mobile and embedded devices. We also make a simple analysis on the weakness of common mobile networks for face verification. The weakness has been well overcome by our specifically designed MobileFaceNets. Under the same experimental conditions, our MobileFaceNets achieve significantly superior accuracy as well as more than 2 times actual speedup over MobileNetV2. After trained by ArcFace loss on the refined MS-Celeb-1M from scratch, our single MobileFaceNet model of 4.0MB size achieves 99.55\% face verification accuracy on LFW and 92.59\% TAR (FAR1e-6) on MegaFace Challenge 1, which is even comparable to state-of-the-art big CNN models of hundreds MB size. The fastest one of our MobileFaceNets has an actual inference time of 18 milliseconds on a mobile phone. Our experiments on LFW, AgeDB, and MegaFace show that our MobileFaceNets achieve significantly improved efficiency compared with the state-of-the-art lightweight and mobile CNNs for face verification.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {Biometric {Recognition}},
	publisher = {Springer International Publishing},
	author = {Chen, Sheng and Liu, Yang and Gao, Xiang and Han, Zhen},
	editor = {Zhou, Jie and Wang, Yunhong and Sun, Zhenan and Jia, Zhenhong and Feng, Jianjiang and Shan, Shiguang and Ubul, Kurban and Guo, Zhenhua},
	year = {2018},
	doi = {10.1007/978-3-319-97909-0_46},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {428--438},
}

@inproceedings{chenAdversarialMetricLearning2018,
	address = {Stockholm, Sweden},
	title = {Adversarial {Metric} {Learning}},
	isbn = {978-0-9992411-2-7},
	url = {https://www.ijcai.org/proceedings/2018/279},
	doi = {10/ghv4dc},
	abstract = {In the past decades, intensive efforts have been put to design various loss functions and metric forms for metric learning problem. These improvements have shown promising results when the test data is similar to the training data. However, the trained models often fail to produce reliable distances on the ambiguous test pairs due to the different samplings between training set and test set. To address this problem, the Adversarial Metric Learning (AML) is proposed in this paper, which automatically generates adversarial pairs to remedy the sampling bias and facilitate robust metric learning. Speciﬁcally, AML consists of two adversarial stages, i.e. confusion and distinguishment. In confusion stage, the ambiguous but critical adversarial data pairs are adaptively generated to mislead the learned metric. In distinguishment stage, a metric is exhaustively learned to try its best to distinguish both adversarial pairs and original training pairs. Thanks to the challenges posed by the confusion stage in such competing process, the AML model is able to grasp plentiful difﬁcult knowledge that has not been contained by the original training pairs, so the discriminability of AML can be signiﬁcantly improved. The entire model is formulated into optimization framework, of which the global convergence is theoretically proved. The experimental results on toy data and practical datasets clearly demonstrate the superiority of AML to representative state-of-the-art metric learning models.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Chen, Shuo and Gong, Chen and Yang, Jian and Li, Xiang and Wei, Yang and Li, Jun},
	month = jul,
	year = {2018},
	keywords = {Untagged},
	pages = {2021--2027},
}

@inproceedings{chenSketchyGANDiverseRealistic2018,
	title = {{SketchyGAN}: {Towards} {Diverse} and {Realistic} {Sketch} to {Image} {Synthesis}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Chen_SketchyGAN_Towards_Diverse_CVPR_2018_paper.html},
	doi = {10/ghwnpn},
	booktitle = {2018 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2018, {Salt} {Lake} {City}, {UT}, {USA}, {June} 18-22, 2018},
	publisher = {IEEE Computer Society},
	author = {Chen, Wengling and Hays, James},
	year = {2018},
	keywords = {Untagged},
	pages = {9416--9425},
}

@inproceedings{chenDomainAdaptiveFaster2018,
	title = {Domain {Adaptive} {Faster} {R}-{CNN} for {Object} {Detection} in the {Wild}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Domain_Adaptive_Faster_CVPR_2018_paper.html},
	doi = {10/gf4kzk},
	booktitle = {2018 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2018, {Salt} {Lake} {City}, {UT}, {USA}, {June} 18-22, 2018},
	publisher = {IEEE Computer Society},
	author = {Chen, Yuhua and Li, Wen and Sakaridis, Christos and Dai, Dengxin and Gool, Luc Van},
	year = {2018},
	keywords = {Untagged},
	pages = {3339--3348},
}

@article{chenHighResolutionFace2018,
	title = {High {Resolution} {Face} {Completion} with {Multiple} {Controllable} {Attributes} via {Fully} {End}-to-{End} {Progressive} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1801.07632},
	abstract = {We present a deep learning approach for high resolution face completion with multiple controllable attributes (e.g., male and smiling) under arbitrary masks. Face completion entails understanding both structural meaningfulness and appearance consistency locally and globally to fill in "holes" whose content do not appear elsewhere in an input image. It is a challenging task with the difficulty level increasing significantly with respect to high resolution, the complexity of "holes" and the controllable attributes of filled-in fragments. Our system addresses the challenges by learning a fully end-to-end framework that trains generative adversarial networks (GANs) progressively from low resolution to high resolution with conditional vectors encoding controllable attributes. We design novel network architectures to exploit information across multiple scales effectively and efficiently. We introduce new loss functions encouraging sharp completion. We show that our system can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.007 seconds for images at 1024 x 1024 resolution. We also perform a pilot human study that shows our approach outperforms state-of-the-art face completion methods in terms of rank analysis. The code will be released upon publication.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1801.07632 [cs]},
	author = {Chen, Zeyuan and Nie, Shaoliang and Wu, Tianfu and Healey, Christopher G.},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.07632},
	keywords = {Untagged},
}

@inproceedings{chengLearningDatumWiseSampling2018,
	address = {New Orleans, Louisiana},
	title = {Learning {Datum}-{Wise} {Sampling} {Frequency} for {Energy}-{Efficient} {Human} {Activity} {Recognition}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16448},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Cheng, Weihao and Erfani, Sarah M. and Zhang, Rui and Kotagiri, Ramamohanarao},
	editor = {McIlraith, Sheila A. and Weinberger, Kilian Q.},
	year = {2018},
	keywords = {Untagged},
	pages = {2143--2150},
}

@inproceedings{chengAONArbitrarilyOrientedText2018,
	address = {Salt Lake City, UT, USA},
	title = {{AON}: {Towards} {Arbitrarily}-{Oriented} {Text} {Recognition}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{AON}},
	url = {https://ieeexplore.ieee.org/document/8578682/},
	doi = {10/ggv74h},
	abstract = {Recognizing text from natural images is a hot research topic in computer vision due to its various applications. Despite the enduring research of several decades on optical character recognition (OCR), recognizing texts from natural images is still a challenging task. This is because scene texts are often in irregular (e.g. curved, arbitrarilyoriented or seriously distorted) arrangements, which have not yet been well addressed in the literature. Existing methods on text recognition mainly work with regular (horizontal and frontal) texts and cannot be trivially generalized to handle irregular texts. In this paper, we develop the arbitrary orientation network (AON) to directly capture the deep features of irregular texts, which are combined into an attention-based decoder to generate character sequence. The whole network can be trained end-to-end by using only images and word-level annotations. Extensive experiments on various benchmarks, including the CUTE80, SVTPerspective, IIIT5k, SVT and ICDAR datasets, show that the proposed AON-based method achieves the-state-of-theart performance in irregular datasets, and is comparable to major existing methods in regular datasets.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Cheng, Zhanzhan and Xu, Yangliu and Bai, Fan and Niu, Yi and Pu, Shiliang and Zhou, Shuigeng},
	month = jun,
	year = {2018},
	keywords = {Untagged},
	pages = {5571--5579},
}

@inproceedings{choiStarGANUnifiedGenerative2018,
	title = {{StarGAN}: {Unified} {Generative} {Adversarial} {Networks} for {Multi}-{Domain} {Image}-to-{Image} {Translation}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.html},
	doi = {10/gfsmz8},
	booktitle = {2018 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2018, {Salt} {Lake} {City}, {UT}, {USA}, {June} 18-22, 2018},
	publisher = {IEEE Computer Society},
	author = {Choi, Yunjey and Choi, Min-Je and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
	year = {2018},
	keywords = {Untagged},
	pages = {8789--8797},
}

@article{claes_genome-wide_2018,
	title = {Genome-wide mapping of global-to-local genetic effects on human facial shape},
	volume = {50},
	issn = {1061-4036, 1546-1718},
	url = {http://www.nature.com/articles/s41588-018-0057-4},
	doi = {10.1038/s41588-018-0057-4},
	language = {en},
	number = {3},
	urldate = {2022-08-24},
	journal = {Nature Genetics},
	author = {Claes, Peter and Roosenboom, Jasmien and White, Julie D. and Swigut, Tomek and Sero, Dzemila and Li, Jiarui and Lee, Myoung Keun and Zaidi, Arslan and Mattern, Brooke C. and Liebowitz, Corey and Pearson, Laurel and González, Tomás and Leslie, Elizabeth J. and Carlson, Jenna C. and Orlova, Ekaterina and Suetens, Paul and Vandermeulen, Dirk and Feingold, Eleanor and Marazita, Mary L. and Shaffer, John R. and Wysocka, Joanna and Shriver, Mark D. and Weinberg, Seth M.},
	month = mar,
	year = {2018},
	keywords = {Untagged},
	pages = {414--423},
}

@inproceedings{cohenCrossDomainRegularization2018,
	title = {Cross {Domain} {Regularization} for {Neural} {Ranking} {Models} using {Adversarial} {Learning}},
	url = {https://doi.org/10.1145/3209978.3210141},
	doi = {10/gdvj7j},
	booktitle = {The 41st {International} {ACM} {SIGIR} {Conference} on {Research} \& {Development} in {Information} {Retrieval}, {SIGIR} 2018, {Ann} {Arbor}, {MI}, {USA}, {July} 08-12, 2018},
	publisher = {ACM},
	author = {Cohen, Daniel and Mitra, Bhaskar and Hofmann, Katja and Croft, W. Bruce},
	editor = {Collins-Thompson, Kevyn and Mei, Qiaozhu and Davison, Brian D. and Liu, Yiqun and Yilmaz, Emine},
	year = {2018},
	keywords = {Untagged},
	pages = {1025--1028},
}

@inproceedings{cuiLargeScaleFineGrained2018,
	address = {Salt Lake City, UT, USA},
	title = {Large {Scale} {Fine}-{Grained} {Categorization} and {Domain}-{Specific} {Transfer} {Learning}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578530/},
	doi = {10/gf5bqk},
	abstract = {Transferring the knowledge learned from large scale datasets (e.g., ImageNet) via ﬁne-tuning offers an effective solution for domain-speciﬁc ﬁne-grained visual categorization (FGVC) tasks (e.g., recognizing bird species or car make \& model). In such scenarios, data annotation often calls for specialized domain knowledge and thus is difﬁcult to scale. In this work, we ﬁrst tackle a problem in large scale FGVC. Our method won ﬁrst place in iNaturalist 2017 large scale species classiﬁcation challenge. Central to the success of our approach is a training scheme that uses higher image resolution and deals with the long-tailed distribution of training data. Next, we study transfer learning via ﬁne-tuning from large scale datasets to small scale, domainspeciﬁc FGVC datasets. We propose a measure to estimate domain similarity via Earth Mover’s Distance and demonstrate that transfer learning beneﬁts from pre-training on a source domain that is similar to the target domain by this measure. Our proposed transfer learning outperforms ImageNet pre-training and obtains state-of-the-art results on multiple commonly used FGVC datasets.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Cui, Yin and Song, Yang and Sun, Chen and Howard, Andrew and Belongie, Serge},
	month = jun,
	year = {2018},
	keywords = {Untagged},
	pages = {4109--4118},
}

@inproceedings{debFaceRecognitionPrimates2018,
	address = {Redondo Beach, CA},
	title = {Face {Recognition}: {Primates} in the {Wild}},
	url = {https://doi.org/10.1109/BTAS.2018.8698538},
	doi = {10/ghwnhc},
	booktitle = {{IEEE} {International} {Conference} on {Biometrics} {Theory}, {Applications} and {Systems}},
	publisher = {IEEE},
	author = {Deb, Debayan and Wiper, Susan and Gong, Sixue and Shi, Yichun and Tymoszek, Cori and Fletcher, Alison and Jain, Anil K.},
	year = {2018},
	keywords = {Untagged},
	pages = {1--10},
}

@inproceedings{dehzangiIMUBasedRobustHuman2018,
	title = {{IMU}-{Based} {Robust} {Human} {Activity} {Recognition} using {Feature} {Analysis}, {Extraction}, and {Reduction}},
	doi = {10.1109/ICPR.2018.8546311},
	abstract = {In recent years, research investigations on recognizing human activities to assess the physical and cognitive capability of humans have gained importance. This paper presents the development of a robust recognition system for Human Activity Recognition under real-world conditions. The activities considered are walking, walking upstairs (walk-up), walking downstairs (walk-dn), sitting, standing and sleeping. The proposed system consists of 3 main elements - a feature extraction from an IMU (Inertial Measurement Unit) based on the spectral and temporal analysis; a feature dimensionality reduction techniques to reduce the high dimensional feature representation, and; various model training algorithms to recognize the human activities. Different methods for feature extraction based on time and frequency domain signal properties are evaluated. The high dimensionality of extracted features results in complex model training and suffers from the curse of dimensionality. Therefore, we evaluated feature selection and transformation algorithms to improve robustness without decreasing the prediction accuracy. Our results finding shows that Random forest feature selection method, when used with Ensemble bagged classifier, provides an accuracy of 96.9\% with 15 features compared to the current benchmark system that employs 561 features. We further obtained a less complex activity recognition system via Neighborhood component analysis along with Ensemble bagged classifier that yields a classification accuracy of 96.3\% with only 9 features.},
	booktitle = {2018 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Dehzangi, Omid and Sahu, Vaishali},
	month = aug,
	year = {2018},
	note = {ISSN: 1051-4651},
	keywords = {Untagged},
	pages = {1402--1407},
}

@inproceedings{dengImageImageDomainAdaptation2018,
	address = {Salt Lake City, UT},
	title = {Image-{Image} {Domain} {Adaptation} {With} {Preserved} {Self}-{Similarity} and {Domain}-{Dissimilarity} for {Person} {Re}-{Identification}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Deng_Image-Image_Domain_Adaptation_CVPR_2018_paper.html},
	doi = {10/gf5bqq},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Deng, Weijian and Zheng, Liang and Ye, Qixiang and Kang, Guoliang and Yang, Yi and Jiao, Jianbin},
	year = {2018},
	keywords = {Untagged},
	pages = {994--1003},
}

@article{deutsch_graph_2018,
	title = {Graph data models, query languages and programming paradigms},
	volume = {11},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3229863.3229879},
	doi = {10.14778/3229863.3229879},
	abstract = {Numerous databases support semi-structured, schemaless and heterogeneous data, typically in the form of graphs (often restricted to trees and nested data). They also provide corresponding high-level query languages or graph-tailored programming paradigms.
            The evolving query languages present multiple variations: Some are superficial syntactic ones, while other ones are genuine differences in modeling, language capabilities and semantics. Incompatibility with SQL presents a learning challenge for graph databases, while table orientation often leads to cumbersome syntactic/semantic structures that are contrary to graph data. Furthermore, the query languages often fall short of full-fledged semistructured and graph query language capabilities, when compared to the yardsticks set by prior academic efforts.
            We survey features, the designers' options and differences in the approaches taken by current systems. We cover both declarative query languages, whose semantics is independent of the underlying model of computation, as well as languages with an operational semantics that is more tightly coupled with the model of computation. For the declarative languages over both general graphs and tree-shaped graphs (as motivated by XML and the recent generation of nested formats, such as JSON and Parquet) we compare to an SQL baseline and present SQL reductions and extensions that capture the essentials of such database systems. More precisely, rather than presenting a single SQL extension, we present multiple configuration options whereas multiple possible (and different) semantics are formally captured by the multiple options that the language's semantic configuration options can take. We show how appropriate setting of the configuration options morphs the semantics into the semantics of multiple surveyed languages, hence providing a compact and formal tool to understand the essential semantic differences between different systems.
            Finally we compare with prior nested and graph query languages (notably OQL, XQuery, Lorel, StruQL, PigLatin) and we transfer into the modern graph database context lessons from the semistructured query processing research of the 90s and 00s, combining them with insights on current graph databases.},
	language = {en},
	number = {12},
	urldate = {2023-04-11},
	journal = {Proceedings of the VLDB Endowment},
	author = {Deutsch, Alin and Papakonstantinou, Yannis},
	month = aug,
	year = {2018},
	keywords = {Untagged},
	pages = {2106--2109},
}

@inproceedings{dhillon_stochastic_2018,
	title = {Stochastic {Activation} {Pruning} for {Robust} {Adversarial} {Defense}},
	url = {https://openreview.net/forum?id=H1uR4GZRZ},
	abstract = {Neural networks are known to be vulnerable to adversarial examples. Carefully chosen perturbations to real images, while imperceptible to humans, induce misclassification and threaten the...},
	language = {en},
	urldate = {2022-05-29},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Dhillon, Guneet S. and Azizzadenesheli, Kamyar and Lipton, Zachary C. and Bernstein, Jeremy D. and Kossaifi, Jean and Khanna, Aran and Anandkumar, Animashree},
	month = feb,
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{duHierarchicalNonlinearOrthogonal2018,
	address = {New Orleans, Louisiana},
	title = {Hierarchical {Nonlinear} {Orthogonal} {Adaptive}-{Subspace} {Self}-{Organizing} {Map} {Based} {Feature} {Extraction} for {Human} {Action} {Recognition}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16332},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Du, Yang and Yuan, Chunfeng and Li, Bing and Hu, Weiming and Yang, Hao and Fu, Zhikang and Zhao, Lili},
	editor = {McIlraith, Sheila A. and Weinberger, Kilian Q.},
	year = {2018},
	keywords = {Untagged},
	pages = {6805--6812},
}

@inproceedings{dubeyMaximumEntropyFineGrained2018,
	title = {Maximum-{Entropy} {Fine} {Grained} {Classification}},
	booktitle = {{NeurIPS}},
	author = {Dubey, A. and Gupta, O. and Raskar, R. and Naik, N.},
	year = {2018},
	keywords = {Untagged},
	pages = {637--647},
}

@inproceedings{dubeyPairwiseConfusionFineGrained2018,
	title = {Pairwise {Confusion} for {Fine}-{Grained} {Visual} {Classification}},
	doi = {10/ghqpgr},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Dubey, Abhimanyu and Gupta, Otkrist and Guo, Pei and Raskar, Ramesh and Farrell, Ryan and Naik, Nikhil},
	year = {2018},
	keywords = {Untagged},
}

@article{dumoulinGuideConvolutionArithmetic2018,
	title = {A guide to convolution arithmetic for deep learning},
	url = {http://arxiv.org/abs/1603.07285},
	abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1603.07285 [cs, stat]},
	author = {Dumoulin, Vincent and Visin, Francesco},
	month = jan,
	year = {2018},
	note = {arXiv: 1603.07285},
	keywords = {Untagged},
}

@inproceedings{ebrahimi_hotflip_2018,
	address = {Melbourne, Australia},
	title = {{HotFlip}: {White}-{Box} {Adversarial} {Examples} for {Text} {Classification}},
	shorttitle = {{HotFlip}},
	url = {https://aclanthology.org/P18-2006},
	doi = {10.18653/v1/P18-2006},
	abstract = {We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ebrahimi, Javid and Rao, Anyi and Lowd, Daniel and Dou, Dejing},
	month = jul,
	year = {2018},
	keywords = {Untagged},
	pages = {31--36},
}

@inproceedings{DBLP:conf/cvpr/EngilbergeCPC18,
	title = {Finding beans in burgers: {Deep} semantic-visual embedding with localization},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Engilberge_Finding_Beans_in_CVPR_2018_paper.html},
	doi = {10.1109/CVPR.2018.00419},
	booktitle = {2018 {IEEE} conference on computer vision and pattern recognition, {CVPR} 2018, salt lake city, {UT}, {USA}, june 18-22, 2018},
	publisher = {Computer Vision Foundation / IEEE Computer Society},
	author = {Engilberge, Martin and Chevallier, Louis and Pérez, Patrick and Cord, Matthieu},
	year = {2018},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/cvpr/EngilbergeCPC18.bib
tex.timestamp: Tue, 31 Aug 2021 14:00:32 +0200},
	keywords = {Untagged},
	pages = {3984--3993},
}

@inproceedings{engstrom_evaluating_2018,
	title = {Evaluating and {Understanding} the {Robustness} of {Adversarial} {Logit} {Pairing}},
	booktitle = {{arXiv}:1807.10272},
	author = {Engstrom, Logan and Ilyas, Andrew and Athalye, Anish},
	year = {2018},
	keywords = {Untagged},
}

@article{engstrom_rotation_2018,
	title = {A {Rotation} and a {Translation} {Suffice}: {Fooling} {CNNs} with {Simple} {Transformations}},
	author = {Engstrom, Logan and Tran, Brandon and Tsipras, Dimitris and Schmidt, Ludwig and Madry, Aleksander},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{esserVariationalUNetConditional2018,
	address = {Salt Lake City, UT},
	title = {A {Variational} {U}-{Net} for {Conditional} {Appearance} and {Shape} {Generation}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Esser_A_Variational_U-Net_CVPR_2018_paper.html},
	doi = {10/ggv7z5},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Esser, Patrick and Sutter, Ekaterina and Ommer, Björn},
	year = {2018},
	keywords = {Untagged},
	pages = {8857--8866},
}

@inproceedings{fabbriEnhancingUnderwaterImagery2018,
	title = {Enhancing {Underwater} {Imagery} {Using} {Generative} {Adversarial} {Networks}},
	url = {https://doi.org/10.1109/ICRA.2018.8460552},
	doi = {10/ggsgv3},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation}, {ICRA} 2018, {Brisbane}, {Australia}, {May} 21-25, 2018},
	publisher = {IEEE},
	author = {Fabbri, Cameron and Islam, Md Jahidul and Sattar, Junaed},
	year = {2018},
	keywords = {Untagged},
	pages = {7159--7165},
}

@inproceedings{faghriVSEImprovingVisualSemantic2018,
	title = {{VSE}++: {Improving} {Visual}-{Semantic} {Embeddings} with {Hard} {Negatives}},
	shorttitle = {{VSE}++},
	url = {http://bmvc2018.org/contents/papers/0344.pdf},
	urldate = {2022-02-25},
	booktitle = {British {Machine} {Vision} {Conference} 2018, {BMVC} 2018, {Newcastle}, {UK}, {September} 3-6, 2018},
	publisher = {BMVA Press},
	author = {Faghri, Fartash and Fleet, David J. and Kiros, Jamie Ryan and Fidler, Sanja},
	year = {2018},
	keywords = {Untagged},
	pages = {12},
}

@inproceedings{follmannMVTecD2SDensely2018,
	address = {Munich, Germany},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{MVTec} {D2S}: {Densely} {Segmented} {Supermarket} {Dataset}},
	volume = {11214},
	url = {https://doi.org/10.1007/978-3-030-01249-6_35},
	doi = {10/gfddts},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Follmann, Patrick and Böttger, Tobias and Härtinger, Philipp and König, Rebecca and Ulrich, Markus},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Untagged},
	pages = {581--597},
}

@inproceedings{friedSpeakerFollowerModelsVisionandLanguage2018,
	title = {Speaker-{Follower} {Models} for {Vision}-and-{Language} {Navigation}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/6a81681a7af700c6385d36577ebec359-Abstract.html},
	urldate = {2021-11-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Fried, Daniel and Hu, Ronghang and Cirik, Volkan and Rohrbach, Anna and Andreas, Jacob and Morency, Louis-Philippe and Berg-Kirkpatrick, Taylor and Saenko, Kate and Klein, Dan and Darrell, Trevor},
	year = {2018},
	keywords = {Untagged},
}

@article{geMatrixCompletionHas2018,
	title = {Matrix {Completion} has {No} {Spurious} {Local} {Minimum}},
	url = {http://arxiv.org/abs/1605.07272},
	abstract = {Matrix completion is a basic machine learning problem that has wide applications, especially in collaborative ﬁltering and recommender systems. Simple non-convex optimization algorithms are popular and effective in practice. Despite recent progress in proving various non-convex algorithms converge from a good initial point, it remains unclear why random or arbitrary initialization sufﬁces in practice. We prove that the commonly used non-convex objective function for positive semideﬁnite matrix completion has no spurious local minima – all local minima must also be global. Therefore, many popular optimization algorithms such as (stochastic) gradient descent can provably solve positive semideﬁnite matrix completion with arbitrary initialization in polynomial time. The result can be generalized to the setting when the observed entries contain noise. We believe that our main proof strategy can be useful for understanding geometric properties of other statistical problems involving partial or noisy observations.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1605.07272 [cs, stat]},
	author = {Ge, Rong and Lee, Jason D. and Ma, Tengyu},
	month = jul,
	year = {2018},
	note = {arXiv: 1605.07272},
	keywords = {Untagged},
}

@inproceedings{gidarisUnsupervisedRepresentationLearning2018,
	address = {Vancouver, BC, Canada},
	title = {Unsupervised {Representation} {Learning} by {Predicting} {Image} {Rotations}},
	url = {https://openreview.net/forum?id=S1v4N2l0-},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{gilmerAdversarialSpheres2018,
	address = {Vancouver, BC, Canada},
	title = {Adversarial {Spheres}},
	url = {https://openreview.net/forum?id=SkthlLkPf},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Gilmer, Justin and Metz, Luke and Faghri, Fartash and Schoenholz, Samuel S. and Raghu, Maithra and Wattenberg, Martin and Goodfellow, Ian J.},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{gongDeepSemanticCorrelation2018,
	title = {Deep {Semantic} {Correlation} {Learning} {Based} {Hashing} for {Multimedia} {Cross}-{Modal} {Retrieval}},
	url = {https://doi.org/10.1109/ICDM.2018.00027},
	doi = {10/ghjr5f},
	booktitle = {Proceedings of the {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {IEEE Computer Society},
	author = {Gong, Xiaolong and Huang, Linpeng and Wang, Fuwei},
	year = {2018},
	keywords = {Untagged},
	pages = {117--126},
}

@inproceedings{guo_countering_2018,
	title = {Countering {Adversarial} {Images} using {Input} {Transformations}},
	url = {https://openreview.net/forum?id=SyJ7ClWCb},
	abstract = {We apply a model-agnostic defense strategy against adversarial examples and achieve 60\% white-box accuracy and 90\% black-box accuracy against major attack algorithms.},
	language = {en},
	urldate = {2022-05-29},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Guo, Chuan and Rana, Mayank and Cisse, Moustapha and Maaten, Laurens van der},
	month = feb,
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{guoAlignedObjectNot2018,
	title = {Aligned to the {Object}, {Not} to the {Image}: {A} {Unified} {Pose}-{Aligned} {Representation} for {Fine}-{Grained} {Recognition}},
	booktitle = {{IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision}},
	author = {Guo, Pei and Farrell, Ryan},
	year = {2018},
	note = {Journal Abbreviation: WACV},
	keywords = {Untagged},
	pages = {1876--1885},
}

@inproceedings{guoDialogbasedInteractiveImage2018,
	address = {Montréal, Canada},
	title = {Dialog-based {Interactive} {Image} {Retrieval}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/a01a0380ca3c61428c26a231f0e49a09-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Guo, Xiaoxiao and Wu, Hui and Cheng, Yu and Rennie, Steven and Tesauro, Gerald and Feris, Rogério Schmidt},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	keywords = {Untagged},
	pages = {676--686},
}

@inproceedings{haarnojaSoftActorCriticOffPolicy2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/haarnoja18b.html},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018, {Stockholmsmässan}, {Stockholm}, {Sweden}, {July} 10-15, 2018},
	publisher = {PMLR},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	keywords = {Untagged},
	pages = {1856--1865},
}

@inproceedings{handPhaseRetrievalGenerative2018,
	address = {Montréal, Canada},
	title = {Phase {Retrieval} {Under} a {Generative} {Prior}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/1bc2029a8851ad344a8d503930dfd7f7-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hand, Paul and Leong, Oscar and Voroninski, Vladislav},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	keywords = {Untagged},
	pages = {9154--9164},
}

@article{hayes_logan_2018,
	title = {{LOGAN}: {Membership} {Inference} {Attacks} {Against} {Generative} {Models}},
	shorttitle = {{LOGAN}},
	url = {http://arxiv.org/abs/1705.07663},
	abstract = {Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator's capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality.},
	urldate = {2022-04-03},
	journal = {arXiv:1705.07663 [cs]},
	author = {Hayes, Jamie and Melis, Luca and Danezis, George and De Cristofaro, Emiliano},
	month = aug,
	year = {2018},
	note = {arXiv: 1705.07663},
	keywords = {Untagged},
}

@article{heBagTricksImage2018,
	title = {Bag of {Tricks} for {Image} {Classification} with {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1812.01187},
	abstract = {Much of the recent progress made in image classiﬁcation research can be credited to training procedure reﬁnements, such as changes in data augmentations and optimization methods. In the literature, however, most reﬁnements are either brieﬂy mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such reﬁnements and empirically evaluate their impact on the ﬁnal model accuracy through ablation study. We will show that, by combining these reﬁnements together, we are able to improve various CNN models signiﬁcantly. For example, we raise ResNet-50’s top-1 validation accuracy from 75.3\% to 79.29\% on ImageNet. We will also demonstrate that improvement on image classiﬁcation accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1812.01187 [cs]},
	author = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.01187},
	keywords = {Untagged},
}

@inproceedings{heljakkaRecursiveChainingReversible2018,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Recursive {Chaining} of {Reversible} {Image}-to-{Image} {Translators} for {Face} {Aging}},
	volume = {11182},
	url = {https://doi.org/10.1007/978-3-030-01449-0_26},
	doi = {10/ghwnnz},
	booktitle = {Advanced {Concepts} for {Intelligent} {Vision} {Systems}, 19th {International} {Conference}, {ACIVS} 2018, {Poitiers}, {France}, {September} 24-27, 2018, {Proceedings}},
	publisher = {Springer},
	author = {Heljakka, Ari and Solin, Arno and Kannala, Juho},
	editor = {Blanc-Talon, Jacques and Helbert, David and Philips, Wilfried and Popescu, Dan C. and Scheunders, Paul},
	year = {2018},
	keywords = {Untagged},
	pages = {309--320},
}

@inproceedings{henschelFusionHeadFullBody2018,
	address = {Salt Lake City, UT},
	title = {Fusion of {Head} and {Full}-{Body} {Detectors} for {Multi}-{Object} {Tracking}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018_workshops/w29/html/Henschel_Fusion_of_Head_CVPR_2018_paper.html},
	doi = {10/ghwngb},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshop}},
	publisher = {IEEE Computer Society},
	author = {Henschel, Roberto and Leal-Taixé, Laura and Cremers, Daniel and Rosenhahn, Bodo},
	year = {2018},
	keywords = {Untagged},
	pages = {1428--1437},
}

@inproceedings{houLossawareWeightQuantization2018,
	title = {Loss-aware {Weight} {Quantization} of {Deep} {Networks}},
	url = {https://openreview.net/forum?id=BkrSv0lA-},
	abstract = {A loss-aware weight quantization algorithm that directly considers its effect on the loss is proposed.},
	language = {en},
	urldate = {2022-02-28},
	author = {Hou, Lu and Kwok, James T.},
	month = feb,
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{huSqueezeandExcitationNetworks2018,
	title = {Squeeze-and-{Excitation} {Networks}},
	shorttitle = {{SENet}},
	doi = {10/gftn9k},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Hu, J. and Shen, L. and Sun, G.},
	year = {2018},
	keywords = {Untagged},
	pages = {7132--7141},
}

@inproceedings{huDuplexGenerativeAdversarial2018,
	address = {Salt Lake City, UT},
	title = {Duplex {Generative} {Adversarial} {Network} for {Unsupervised} {Domain} {Adaptation}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578260/},
	doi = {10/gf5bq2},
	abstract = {Domain adaptation attempts to transfer the knowledge obtained from the source domain to the target domain, i.e., the domain where the testing data are. The main challenge lies in the distribution discrepancy between source and target domain. Most existing works endeavor to learn domain invariant representation usually by minimizing a distribution distance, e.g., MMD and the discriminator in the recently proposed generative adversarial network (GAN). Following the similar idea of GAN, this work proposes a novel GAN architecture with duplex adversarial discriminators (referred to as DupGAN), which can achieve domain-invariant representation and domain transformation. Speciﬁcally, our proposed network consists of three parts, an encoder, a generator and two discriminators. The encoder embeds samples from both domains into the latent representation, and the generator decodes the latent representation to both source and target domains respectively conditioned on a domain code, i.e., achieves domain transformation. The generator is pitted against duplex discriminators, one for source domain and the other for target, to ensure the reality of domain transformation, the latent representation domain invariant and the category information of it preserved as well. Our proposed work achieves the state-of-the-art performance on unsupervised domain adaptation of digit classiﬁcation and object recognition.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Hu, Lanqing and Kan, Meina and Shan, Shiguang and Chen, Xilin},
	year = {2018},
	keywords = {Untagged},
	pages = {1498--1507},
}

@article{huExposureWhiteBoxPhoto2018,
	title = {Exposure: {A} {White}-{Box} {Photo} {Post}-{Processing} {Framework}},
	volume = {37},
	url = {https://doi.org/10.1145/3181974},
	doi = {10/gd52qk},
	number = {2},
	journal = {ACM Trans. Graph.},
	author = {Hu, Yuanming and He, Hao and Xu, Chenxi and Wang, Baoyuan and Lin, Stephen},
	year = {2018},
	keywords = {Untagged},
	pages = {26:1--26:17},
}

@article{huangIntroductionImageSynthesis2018,
	title = {An {Introduction} to {Image} {Synthesis} with {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1803.04469},
	abstract = {There has been a drastic growth of research in Generative Adversarial Nets (GANs) in the past few years. Proposed in 2014, GAN has been applied to various applications such as computer vision and natural language processing, and achieves impressive performance. Among the many applications of GAN, image synthesis is the most well-studied one, and research in this area has already demonstrated the great potential of using GAN in image synthesis. In this paper, we provide a taxonomy of methods used in image synthesis, review different models for text-to-image synthesis and image-to-image translation, and discuss some evaluation metrics as well as possible future research directions in image synthesis with GAN.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1803.04469 [cs]},
	author = {Huang, He and Yu, Philip S. and Wang, Changhu},
	month = nov,
	year = {2018},
	note = {arXiv: 1803.04469},
	keywords = {Untagged},
}

@inproceedings{huangCostEffectiveTrainingDeep2018,
	title = {Cost-{Effective} {Training} of {Deep} {CNNs} with {Active} {Model} {Adaptation}},
	url = {https://doi.org/10.1145/3219819.3220026},
	doi = {10/ggsdcs},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}, {KDD} 2018, {London}, {UK}, {August} 19-23, 2018},
	publisher = {ACM},
	author = {Huang, Sheng-Jun and Zhao, Jia-Wei and Liu, Zhao-Yang},
	editor = {Guo, Yike and Farooq, Faisal},
	year = {2018},
	keywords = {Untagged},
	pages = {1580--1588},
}

@inproceedings{huang_deep_2018,
	address = {Salt Lake City, UT},
	title = {Deep {Cross}-{Media} {Knowledge} {Transfer}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8579019/},
	doi = {10.1109/CVPR.2018.00921},
	abstract = {Cross-media retrieval is a research hotspot in multimedia area, which aims to perform retrieval across diﬀerent media types such as image and text. The performance of existing methods usually relies on labeled data for model training. However, cross-media data is very labor consuming to collect and label, so how to transfer valuable knowledge in existing data to new data is a key problem towards application. For achieving the goal, this paper proposes deep cross-media knowledge transfer (DCKT) approach, which transfers knowledge from a large-scale cross-media dataset to promote the model training on another smallscale cross-media dataset. The main contributions of DCKT are: (1) Two-level transfer architecture is proposed to jointly minimize the media-level and correlation-level domain discrepancies, which allows two important and complementary aspects of knowledge to be transferred: intramedia semantic and inter-media correlation knowledge. It can enrich the training information and boost the retrieval accuracy. (2) Progressive transfer mechanism is proposed to iteratively select training samples with ascending transfer diﬃculties, via the metric of cross-media domain consistency with adaptive feedback. It can drive the transfer process to gradually reduce vast cross-media domain discrepancy, so as to enhance the robustness of model training. For verifying the eﬀectiveness of DCKT, we take the largescale dataset XMediaNet as source domain, and 3 widelyused datasets as target domain for cross-media retrieval. Experimental results show that DCKT achieves promising improvement on retrieval accuracy.},
	language = {en},
	urldate = {2022-10-05},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Huang, Xin and Peng, Yuxin},
	year = {2018},
	keywords = {Untagged},
	pages = {8837--8846},
}

@inproceedings{huangMultimodalUnsupervisedImagetoImage2018,
	address = {Munich, Germany},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multimodal {Unsupervised} {Image}-to-{Image} {Translation}},
	volume = {11207},
	url = {https://doi.org/10.1007/978-3-030-01219-9_11},
	doi = {10/ggvxmz},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Huang, Xun and Liu, Ming-Yu and Belongie, Serge J. and Kautz, Jan},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Untagged},
	pages = {179--196},
}

@inproceedings{huiyangDifferentialPrivacyInformation2018,
	address = {Marina Del Rey, CA},
	title = {Differential {Privacy} for {Information} {Retrieval}},
	isbn = {978-1-4503-5581-0},
	url = {https://doi.org/10.1145/3159652.3162006},
	doi = {10.1145/3159652.3162006},
	abstract = {The concern for privacy is real for any research that uses user data. Information Retrieval (IR) is not an exception. Many IR algorithms and applications require the use of users' personal information, contextual information and other sensitive and private information.The extensive use of personalization in IR has become a double-edged sword. Sometimes, the concern becomes so overwhelming that IR research has to stop to avoid privacy leaks. The good news is that recently there have been increasing attentions paid on the joint field of privacy and IR – privacy-preserving IR. As part of the effort, this tutorial offers an introduction to differential privacy (DP), one of the most advanced techniques in privacy research, and provides necessary set of theoretical knowledge for applying privacy techniques in IR. Differential privacy is a technique that provides strong privacy guarantees for data protection. Theoretically, it aims to maximize the data utility in statistical datasets while minimizing the risk of exposing individual data entries to any adversary. Differential privacy has been successfully applied to a wide range of applications in database (DB) and data mining (DM). The research in privacy-preserving IR is relatively new, however, research has shown that DP is also effective in supporting multiple IR tasks. This tutorial aims to lay a theoretical foundation of DP and explains how it can be applied to IR. It highlights the differences in IR tasks and DB and DM tasks and how DP connects to IR. We hope the attendees of this tutorial will have a good understanding of DP and other necessary knowledge to work on the newly minted joint research field of privacy and IR.},
	booktitle = {{ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Hui Yang, Grace and Zhang, Sicong},
	year = {2018},
	note = {event-place: Marina Del Rey, CA, USA},
	keywords = {Untagged},
	pages = {777--778},
}

@inproceedings{hungAdversarialLearningSemisupervised2018,
	address = {Newcastle, UK},
	title = {Adversarial {Learning} for {Semi}-supervised {Semantic} {Segmentation}},
	url = {http://bmvc2018.org/contents/papers/0200.pdf},
	booktitle = {British {Machine} {Vision} {Conference}},
	publisher = {BMVA Press},
	author = {Hung, Wei-Chih and Tsai, Yi-Hsuan and Liou, Yan-Ting and Lin, Yen-Yu and Yang, Ming-Hsuan},
	year = {2018},
	keywords = {Untagged},
	pages = {65},
}

@inproceedings{jacobQuantizationTrainingNeural2018,
	title = {Quantization and {Training} of {Neural} {Networks} for {Efficient} {Integer}-{Arithmetic}-{Only} {Inference}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Jacob_Quantization_and_Training_CVPR_2018_paper.html},
	urldate = {2021-11-04},
	author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
	year = {2018},
	keywords = {Untagged},
	pages = {2704--2713},
}

@inproceedings{jaiswalCapsuleGANGenerativeAdversarial2018,
	address = {Munich, Germany},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{CapsuleGAN}: {Generative} {Adversarial} {Capsule} {Network}},
	volume = {11131},
	url = {https://doi.org/10.1007/978-3-030-11015-4_38},
	doi = {10/ghwnhv},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Jaiswal, Ayush and AbdAlmageed, Wael and Wu, Yue and Natarajan, Premkumar},
	editor = {Leal-Taixé, Laura and Roth, Stefan},
	year = {2018},
	keywords = {Untagged},
	pages = {526--535},
}

@inproceedings{jetleyLearnPayAttention2018,
	address = {Vancouver, BC, Canada},
	title = {Learn to {Pay} {Attention}},
	url = {https://openreview.net/forum?id=HyzbhfWRW},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Jetley, Saumya and Lord, Nicholas A. and Lee, Namhoon and Torr, Philip H. S.},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{jiLargescaleRGBDDatabase2018,
	address = {Seoul, Republic of Korea},
	title = {A {Large}-scale {RGB}-{D} {Database} for {Arbitrary}-view {Human} {Action} {Recognition}},
	url = {https://doi.org/10.1145/3240508.3240675},
	doi = {10.1145/3240508.3240675},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Ji, Yanli and Xu, Feixiang and Yang, Yang and Shen, Fumin and Shen, Heng Tao and Zheng, Wei-Shi},
	editor = {Boll, Susanne and Lee, Kyoung Mu and Luo, Jiebo and Zhu, Wenwu and Byun, Hyeran and Chen, Chang Wen and Lienhart, Rainer and Mei, Tao},
	year = {2018},
	keywords = {Untagged},
	pages = {1510--1518},
}

@techreport{jiangLoglossAnalysisDifferent2018,
	title = {Logloss {Analysis} with {Diﬀerent} {Activation}},
	language = {en},
	author = {Jiang, Qing-Yuan},
	year = {2018},
	keywords = {Untagged},
	pages = {8},
}

@inproceedings{kanehiraViewpointAwareVideoSummarization2018,
	title = {Viewpoint-{Aware} {Video} {Summarization}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Kanehira_Viewpoint-Aware_Video_Summarization_CVPR_2018_paper.html},
	doi = {10/ghwnfv},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Kanehira, Atsushi and Gool, Luc Van and Ushiku, Yoshitaka and Harada, Tatsuya},
	year = {2018},
	keywords = {Untagged},
	pages = {7435--7444},
}

@inproceedings{kangDeepAdversarialAttention2018,
	title = {Deep {Adversarial} {Attention} {Alignment} for {Unsupervised} {Domain} {Adaptation}: the {Benefit} of {Target} {Expectation} {Maximization}},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Kang, Guoliang and Zheng, Liang and Yan, Yan and Yang, Yi},
	year = {2018},
	keywords = {Untagged},
	pages = {401--416},
}

@inproceedings{kannan_adversarial_2018,
	title = {Adversarial {Logit} {Pairing}},
	booktitle = {{arXiv}:1803.06373},
	author = {Kannan, Harini and Kurakin, Alexey and Goodfellow, Ian},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{10.5555/3327345.3327448,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'18},
	title = {Differentially {Private} k-{Means} with {Constant} {Multiplicative} {Error}},
	abstract = {We design new differentially private algorithms for the Euclidean k-means problem, both in the centralized model and in the local model of differential privacy. In both models, our algorithms achieve significantly improved error guarantees than the previous state-of-the-art. In addition, in the local model, our algorithm significantly reduces the number of interaction rounds.Although the problem has been widely studied in the context of differential privacy, all of the existing constructions achieve only super constant approximation factors. We present—for the first time—efficient private algorithms for the problem with constant multiplicative error. Furthermore, we show how to modify our algorithms so they compute private coresets for k-means clustering in both models.},
	booktitle = {Proceedings of the 32nd international conference on neural information processing systems},
	publisher = {Curran Associates Inc.},
	author = {Kaplan, Haim and Stemmer, Uri},
	year = {2018},
	note = {Number of pages: 11
Place: Montréal, Canada},
	keywords = {Untagged},
	pages = {5436--5446},
}

@inproceedings{kaplanisContinualReinforcementLearning2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Continual {Reinforcement} {Learning} with {Complex} {Synapses}},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/kaplanis18a.html},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018, {Stockholmsmässan}, {Stockholm}, {Sweden}, {July} 10-15, 2018},
	publisher = {PMLR},
	author = {Kaplanis, Christos and Shanahan, Murray and Clopath, Claudia},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	keywords = {Untagged},
	pages = {2502--2511},
}

@inproceedings{karrasProgressiveGrowingGANs2018,
	address = {Vancouver, BC, Canada},
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	url = {https://openreview.net/forum?id=Hk99zCeAb},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{khrulkovGeometryScoreMethod2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Geometry {Score}: {A} {Method} {For} {Comparing} {Generative} {Adversarial} {Networks}},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/khrulkov18a.html},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018, {Stockholmsmässan}, {Stockholm}, {Sweden}, {July} 10-15, 2018},
	publisher = {PMLR},
	author = {Khrulkov, Valentin and Oseledets, Ivan V.},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	keywords = {Untagged},
	pages = {2626--2634},
}

@inproceedings{kieuOutlierDetectionMultidimensional2018,
	address = {Aalborg, Denmark},
	title = {Outlier {Detection} for {Multidimensional} {Time} {Series} {Using} {Deep} {Neural} {Networks}},
	isbn = {978-1-5386-4133-0},
	shorttitle = {{CNN}-{AE}},
	url = {https://ieeexplore.ieee.org/document/8411269/},
	doi = {10.1109/MDM.2018.00029},
	abstract = {Due to the continued digitization of industrial and societal processes, including the deployment of networked sensors, we are witnessing a rapid proliferation of time-ordered observations, known as time series. For example, the behavior of drivers can be captured by GPS or accelerometer as a time series of speeds, directions, and accelerations. We propose a framework for outlier detection in time series that, for example, can be used for identifying dangerous driving behavior and hazardous road locations. Speciﬁcally, we ﬁrst propose a method that generates statistical features to enrich the feature space of raw time series. Next, we utilize an autoencoder to reconstruct the enriched time series. The autoencoder performs dimensionality reduction to capture, using a small feature space, the most representative features of the enriched time series. As a result, the reconstructed time series only capture representative features, whereas outliers often have non-representative features. Therefore, deviations of the enriched time series from the reconstructed time series can be taken as indicators of outliers. We propose and study autoencoders based on convolutional neural networks and longshort term memory neural networks. In addition, we show that embedding of contextual information into the framework has the potential to further improve the accuracy of identifying outliers. We report on empirical studies with multiple time series data sets, which offers insight into the design properties of the proposed framework, indicating that it is effective at detecting outliers.},
	language = {en},
	urldate = {2021-06-02},
	booktitle = {{IEEE} {International} {Conference} on {Mobile} {Data} {Management}},
	publisher = {IEEE},
	author = {Kieu, Tung and Yang, Bin and Jensen, Christian S.},
	year = {2018},
	keywords = {Untagged},
	pages = {125--134},
}

@article{koniuszMuseumExhibitIdentification2018a,
	title = {Museum {Exhibit} {Identification} {Challenge} for {Domain} {Adaptation} and {Beyond}},
	url = {http://arxiv.org/abs/1802.01093},
	abstract = {In this paper, we approach an open problem of artwork identiﬁcation and propose a new dataset dubbed Open Museum Identiﬁcation Challenge (Open MIC). It contains photos of exhibits captured in 10 distinct exhibition spaces of several museums which showcase paintings, timepieces, sculptures, glassware, relics, science exhibits, natural history pieces, ceramics, pottery, tools and indigenous crafts. The goal of Open MIC is to stimulate research in domain adaptation, egocentric recognition and few-shot learning by providing a testbed complementary to the famous Ofﬁce dataset which reaches ∼90\% accuracy [15]. To form our dataset, we captured a number of images per art piece with a mobile phone and wearable cameras to form the source and target data splits, respectively. To achieve robust baselines, we build on a recent approach that aligns per-class scatter matrices of the source and target CNN streams [15]. Moreover, we exploit the positive deﬁnite nature of such representations by using end-to-end Bregman divergences and the Riemannian metric. We present baselines such as training/evaluation per exhibition and training/evaluation on the combined set covering 866 exhibit identities. As each exhibition poses distinct challenges e.g., quality of lighting, motion blur, occlusions, clutter, viewpoint and scale variations, rotations, glares, transparency, non-planarity, clipping, we break down results w.r.t. these factors.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1802.01093 [cs]},
	author = {Koniusz, Piotr and Tas, Yusuf and Zhang, Hongguang and Harandi, Mehrtash and Porikli, Fatih and Zhang, Rui},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.01093},
	keywords = {Untagged},
}

@inproceedings{koniuszMuseumExhibitIdentification2018,
	title = {Museum {Exhibit} {Identification} {Challenge} for the {Supervised} {Domain} {Adaptation} and {Beyond}},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Koniusz, Piotr and Tas, Yusuf and Zhang, Hongguang and Harandi, Mehrtash and Porikli, Fatih and Zhang, Rui},
	year = {2018},
	keywords = {Untagged},
	pages = {788--804},
}

@article{koreinMultiArmedBanditAlgorithms2018,
	title = {Multi-{Armed} {Bandit} {Algorithms} for {Spare} {Time} {Planning} of a {Mobile} {Service} {Robot}},
	abstract = {We assume that service robots will have spare time in between scheduled user requests, which they could use to perform additional unrequested services in order to learn a model of users’ preferences and receive reward. However, a mobile service robot is constrained by the need to travel through the environment to reach a user in order to perform a service for them, as well as the need to carry out scheduled user requests. We present modified versions of Thompson Sampling and UCB1, existing algorithms used in multiarmed bandit problems, which plan ahead considering the time and location constraints of a mobile service robot. We compare them to existing versions of Thompson Sampling and UCB1 and find that our modified planning algorithms outperform the original versions in terms of both reward received and the effectiveness of the model learned in a simulation.},
	language = {en},
	author = {Korein, Max and Veloso, Manuela},
	year = {2018},
	keywords = {Untagged},
	pages = {3},
}

@inproceedings{laradjiWhereAreBlobs2018,
	address = {Cham},
	title = {Where {Are} the {Blobs}: {Counting} by {Localization} with {Point} {Supervision}},
	volume = {11206},
	isbn = {978-3-030-01215-1 978-3-030-01216-8},
	shorttitle = {Where {Are} the {Blobs}},
	url = {http://link.springer.com/10.1007/978-3-030-01216-8_34},
	doi = {10.1007/978-3-030-01216-8_34},
	urldate = {2021-01-27},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer International Publishing},
	author = {Laradji, Issam H. and Rostamzadeh, Negar and Pinheiro, Pedro O. and Vazquez, David and Schmidt, Mark},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {560--576},
}

@article{lecuyerCertifiedRobustnessAdversarial2018,
	title = {Certified {Robustness} to {Adversarial} {Examples} with {Differential} {Privacy}},
	shorttitle = {{PixelDP}},
	url = {http://arxiv.org/abs/1802.03471},
	abstract = {Adversarial examples that fool machine learning models, particularly deep neural networks, have been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best effort and have been shown to be vulnerable to sophisticated attacks. Recently a set of certified defenses have been introduced, which provide guarantees of robustness to norm-bounded attacks, but they either do not scale to large datasets or are limited in the types of models they can support. This paper presents the first certified defense that both scales to large networks and datasets (such as Google's Inception network for ImageNet) and applies broadly to arbitrary model types. Our defense, called PixelDP, is based on a novel connection between robustness against adversarial examples and differential privacy, a cryptographically-inspired formalism, that provides a rigorous, generic, and flexible foundation for defense.},
	urldate = {2021-09-28},
	journal = {arXiv:1802.03471 [cs, stat]},
	author = {Lecuyer, Mathias and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Jana, Suman},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.03471
version: 1},
	keywords = {Untagged},
}

@inproceedings{leeHierarchicalNoveltyDetection2018,
	address = {Salt Lake City, UT},
	title = {Hierarchical {Novelty} {Detection} for {Visual} {Object} {Recognition}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Lee_Hierarchical_Novelty_Detection_CVPR_2018_paper.html},
	doi = {10/ghwngj},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Lee, Kibok and Lee, Kimin and Min, Kyle and Zhang, Yuting and Shin, Jinwoo and Lee, Honglak},
	year = {2018},
	keywords = {Untagged},
	pages = {1034--1042},
}

@inproceedings{leeWassersteinIntrospectiveNeural2018,
	address = {Salt Lake City, UT},
	title = {Wasserstein {Introspective} {Neural} {Networks}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Lee_Wasserstein_Introspective_Neural_CVPR_2018_paper.html},
	doi = {10/ghwnpt},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Lee, Kwonjoon and Xu, Weijian and Fan, Fan and Tu, Zhuowen},
	year = {2018},
	keywords = {Untagged},
	pages = {3702--3711},
}

@inproceedings{lengExtremelyLowBit2018,
	address = {New Orleans, Louisiana},
	title = {Extremely {Low} {Bit} {Neural} {Network}: {Squeeze} the {Last} {Bit} {Out} {With} {ADMM}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16767},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Leng, Cong and Dou, Zesheng and Li, Hao and Zhu, Shenghuo and Jin, Rong},
	editor = {McIlraith, Sheila A. and Weinberger, Kilian Q.},
	year = {2018},
	keywords = {Untagged},
	pages = {3466--3473},
}

@inproceedings{lettryDARNDeepAdversarial2018,
	address = {Los Alamitos, CA, USA},
	title = {{DARN}: {A} {Deep} {Adversarial} {Residual} {Network} for {Intrinsic} {Image} {Decomposition}},
	url = {https://doi.ieeecomputersociety.org/10.1109/WACV.2018.00153},
	doi = {10/ggknkx},
	booktitle = {{IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision}},
	publisher = {IEEE Computer Society},
	author = {Lettry, L. and Vanhoey, K. and Gool, L. van},
	year = {2018},
	keywords = {Untagged},
	pages = {1359--1367},
}

@inproceedings{li_learning_2018,
	title = {Learning to {Generalize}: {Meta}-{Learning} for {Domain} {Generalization}},
	volume = {32},
	copyright = {Copyright (c)},
	shorttitle = {{MLDG}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11596},
	doi = {10.1609/aaai.v32i1.11596},
	abstract = {Domain shift refers to the well known problem that a model trained in one source domain performs poorly when appliedto a target domain with different statistics. Domain Generalization (DG) techniques attempt to alleviate this issue by producing models which by design generalize well to novel testing domains. We propose a novel meta-learning method for domain generalization. Rather than designing a specific model that is robust to domain shift as in most previous DG work, we propose a model agnostic training procedure for DG. Our algorithm simulates train/test domain shift during training by synthesizing virtual testing domains within each mini-batch. The meta-optimization objective requires that steps to improve training domain performance should also improve testing domain performance. This meta-learning procedure trains models with good generalization ability to novel domains. We evaluate our method and achieve state of the art results on a recent cross-domain image classification benchmark, as well demonstrating its potential on two classic reinforcement learning tasks.},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy},
	year = {2018},
	note = {Number: 1},
	keywords = {Untagged},
}

@inproceedings{liVisualizingLossLandscape2018,
	address = {Montréal, Canada},
	title = {Visualizing the {Loss} {Landscape} of {Neural} {Nets}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/a41b3bb3e6b050b6c9067c67f663b915-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	keywords = {Untagged},
	pages = {6391--6401},
}

@inproceedings{liDomainGeneralizationAdversarial2018a,
	address = {Salt Lake City, UT},
	title = {Domain {Generalization} with {Adversarial} {Feature} {Learning}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{MMD}},
	url = {https://ieeexplore.ieee.org/document/8578664/},
	doi = {10.1109/CVPR.2018.00566},
	abstract = {In this paper, we tackle the problem of domain generalization: how to learn a generalized feature representation for an “unseen” target domain by taking the advantage of multiple seen source-domain data. We present a novel framework based on adversarial autoencoders to learn a generalized latent feature representation across domains for domain generalization. To be speciﬁc, we extend adversarial autoencoders by imposing the Maximum Mean Discrepancy (MMD) measure to align the distributions among different domains, and matching the aligned distribution to an arbitrary prior distribution via adversarial feature learning. In this way, the learned feature representation is supposed to be universal to the seen source domains because of the MMD regularization, and is expected to generalize well on the target domain because of the introduction of the prior distribution. We proposed an algorithm to jointly train different components of our proposed framework. Extensive experiments on various vision tasks demonstrate that our proposed framework can learn better generalized features for the unseen target domain compared with state-of-the-art domain generalization methods.},
	language = {en},
	urldate = {2022-05-15},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Li, Haoliang and Pan, Sinno Jialin and Wang, Shiqi and Kot, Alex C.},
	year = {2018},
	keywords = {Untagged},
	pages = {5400--5409},
}

@inproceedings{liBruteForceFacialLandmark2018,
	address = {New Orleans, Louisiana},
	title = {Brute-{Force} {Facial} {Landmark} {Analysis} {With} a 140, 000-{Way} {Classifier}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16899},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Li, Mengtian and Jeni, László A. and Ramanan, Deva},
	editor = {McIlraith, Sheila A. and Weinberger, Kilian Q.},
	year = {2018},
	keywords = {Untagged},
	pages = {7032--7040},
}

@inproceedings{liSemanticawareGradGANVirtualtoReal2018,
	title = {Semantic-aware {Grad}-{GAN} for {Virtual}-to-{Real} {Urban} {Scene} {Adaption}},
	url = {http://bmvc2018.org/contents/papers/0239.pdf},
	booktitle = {British {Machine} {Vision} {Conference} 2018, {BMVC} 2018, {Newcastle}, {UK}, {September} 3-6, 2018},
	publisher = {BMVA Press},
	author = {Li, Peilun and Liang, Xiaodan and Jia, Daoyuan and Xing, Eric P.},
	year = {2018},
	keywords = {Untagged},
	pages = {73},
}

@inproceedings{liGlobalLocalConsistent2018,
	address = {Beijing, China},
	title = {Global and {Local} {Consistent} {Age} {Generative} {Adversarial} {Networks}},
	url = {https://doi.org/10.1109/ICPR.2018.8545119},
	doi = {10/ghwnm6},
	booktitle = {International {Conference} on {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Li, Peipei and Hu, Yibo and Li, Qi and He, Ran and Sun, Zhenan},
	year = {2018},
	keywords = {Untagged},
	pages = {1073--1078},
}

@article{liOptimalControlApproach2018,
	title = {An {Optimal} {Control} {Approach} to {Deep} {Learning} and {Applications} to {Discrete}-{Weight} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1803.01299},
	abstract = {Deep learning is formulated as a discrete-time optimal control problem. This allows one to characterize necessary conditions for optimality and develop training algorithms that do not rely on gradients with respect to the trainable parameters. In particular, we introduce the discrete-time method of successive approximations (MSA), which is based on the Pontryagin’s maximum principle, for training neural networks. A rigorous error estimate for the discrete MSA is obtained, which sheds light on its dynamics and the means to stabilize the algorithm. The developed methods are applied to train, in a rather principled way, neural networks with weights that are constrained to take values in a discrete set. We obtain competitive performance and interestingly, very sparse weights in the case of ternary networks, which may be useful in model deployment in low-memory devices.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1803.01299 [cs]},
	author = {Li, Qianxiao and Hao, Shuji},
	month = jun,
	year = {2018},
	note = {arXiv: 1803.01299},
	keywords = {Untagged},
}

@article{liDomainInvariantClass2018,
	title = {Domain {Invariant} and {Class} {Discriminative} {Feature} {Learning} for {Visual} {Domain} {Adaptation}},
	volume = {27},
	issn = {1057-7149, 1941-0042},
	url = {https://ieeexplore.ieee.org/document/8362753/},
	doi = {10/ghqpfw},
	abstract = {Domain adaptation manages to build an effective target classiﬁer or regression model for unlabeled target data by utilizing the well-labeled source data but lying different distributions. Intuitively, to address domain shift problem, it is crucial to learn domain invariant features across domains, and most existing approaches have concentrated on it. However, they often do not directly constrain the learned features to be class discriminative for both source and target data, which is of vital importance for the ﬁnal classiﬁcation. Therefore, in this paper, we put forward a novel feature learning method for domain adaptation to construct both domain invariant and class discriminative representations, referred to as DICD. Speciﬁcally, DICD is to learn a latent feature space with important data properties preserved, which reduces the domain difference by jointly matching the marginal and class-conditional distributions of both domains, and simultaneously maximizes the inter-class dispersion and minimizes the intra-class scatter as much as possible. Experiments in this paper have demonstrated that the class discriminative properties will dramatically alleviate the cross-domain distribution inconsistency, which further boosts the classiﬁcation performance. Moreover, we show that exploring both domain invariance and class discriminativeness of the learned representations can be integrated into one optimization framework, and the optimal solution can be derived effectively by solving a generalized eigen-decomposition problem. Comprehensive experiments on several visual cross-domain classiﬁcation tasks verify that DICD can outperform the competitors signiﬁcantly.},
	language = {en},
	number = {9},
	urldate = {2021-01-27},
	journal = {IEEE Transactions on Image Processing},
	author = {Li, Shuang and Song, Shiji and Huang, Gao and Ding, Zhengming and Wu, Cheng},
	month = sep,
	year = {2018},
	keywords = {Untagged},
	pages = {4260--4273},
}

@inproceedings{liHarmoniousAttentionNetwork2018,
	address = {Salt Lake City, UT},
	title = {Harmonious {Attention} {Network} for {Person} {Re}-{Identification}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Li_Harmonious_Attention_Network_CVPR_2018_paper.html},
	doi = {10/gfvvnb},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Li, Wei and Zhu, Xiatian and Gong, Shaogang},
	year = {2018},
	keywords = {Untagged},
	pages = {2285--2294},
}

@inproceedings{ferrari_deep_2018,
	address = {Cham},
	title = {Deep {Domain} {Generalization} via {Conditional} {Invariant} {Adversarial} {Networks}},
	volume = {11219},
	isbn = {978-3-030-01266-3 978-3-030-01267-0},
	shorttitle = {{CDANN}},
	url = {http://link.springer.com/10.1007/978-3-030-01267-0_38},
	doi = {10.1007/978-3-030-01267-0_38},
	abstract = {Domain generalization aims to learn a classiﬁcation model from multiple source domains and generalize it to unseen target domains. A critical problem in domain generalization involves learning domaininvariant representations. Let X and Y denote the features and the labels, respectively. Under the assumption that the conditional distribution P (Y {\textbar}X) remains unchanged across domains, earlier approaches to domain generalization learned the invariant representation T (X) by minimizing the discrepancy of the marginal distribution P (T (X)). However, such an assumption of stable P (Y {\textbar}X) does not necessarily hold in practice. In addition, the representation learning function T (X) is usually constrained to a simple linear transformation or shallow networks. To address the above two drawbacks, we propose an end-to-end conditional invariant deep domain generalization approach by leveraging deep neural networks for domain-invariant representation learning. The domain-invariance property is guaranteed through a conditional invariant adversarial network that can learn domain-invariant representations w.r.t. the joint distribution P (T (X), Y ) if the target domain data are not severely class unbalanced. We perform various experiments to demonstrate the eﬀectiveness of the proposed method.},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer International Publishing},
	author = {Li, Ya and Tian, Xinmei and Gong, Mingming and Liu, Yajing and Liu, Tongliang and Zhang, Kun and Tao, Dacheng},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {647--663},
}

@inproceedings{liAdaptationReIdentificationNetwork2018,
	address = {Salt Lake City, UT},
	title = {Adaptation and {Re}-{Identification} {Network}: {An} {Unsupervised} {Deep} {Transfer} {Learning} {Approach} to {Person} {Re}-{Identification}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018_workshops/w6/html/Li_Adaptation_and_Re-Identification_CVPR_2018_paper.html},
	doi = {10/ggcwnp},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshop}},
	publisher = {IEEE Computer Society},
	author = {Li, Yu-Jhe and Yang, Fu-En and Liu, Yen-Cheng and Yeh, Yu-Ying and Du, Xiaofei and Wang, Yu-Chiang Frank},
	year = {2018},
	keywords = {Untagged},
	pages = {172--178},
}

@inproceedings{liHybridRetrievalGenerationReinforced2018,
	address = {Montréal, Canada},
	title = {Hybrid {Retrieval}-{Generation} {Reinforced} {Agent} for {Medical} {Image} {Report} {Generation}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/e07413354875be01a996dc560274708e-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Li, Yuan and Liang, Xiaodan and Hu, Zhiting and Xing, Eric P.},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	keywords = {Untagged},
	pages = {1537--1547},
}

@inproceedings{liDetNetDesignBackbone2018,
	title = {{DetNet}: {Design} {Backbone} for {Object} {Detection}},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Li, Zeming and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Deng, Yangdong and Sun, Jian},
	month = sep,
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{lin_supervised_2018,
	address = {Seoul Republic of Korea},
	title = {Supervised {Online} {Hashing} via {Hadamard} {Codebook} {Learning}},
	isbn = {978-1-4503-5665-7},
	url = {https://dl.acm.org/doi/10.1145/3240508.3240519},
	doi = {10.1145/3240508.3240519},
	abstract = {In recent years, binary code learning, a.k.a. hashing, has received extensive attention in large-scale multimedia retrieval. It aims to encode high-dimensional data points into binary codes, hence the original high-dimensional metric space can be e ciently approximated via Hamming space. However, most existing hashing methods adopted o ine batch learning, which is not suitable to handle incremental datasets with streaming data or new instances. In contrast, the robustness of the existing online hashing remains as an open problem, while the embedding of supervised/semantic information hardly boosts the performance of the online hashing, mainly due to the defect of unknown category numbers in supervised learning. In this paper, we propose an online hashing scheme, termed Hadamard Codebook based Online Hashing (HCOH), which aims to solving the above problems towards robust and supervised online hashing. In particular, we rst assign an appropriate highdimensional binary codes to each class label, which is generated randomly by Hadamard codes. Subsequently, LSH is adopted to reduce the length of such Hadamard codes in accordance with the hash bits, which can adapt the prede ned binary codes online, and theoretically guarantee the semantic similarity. Finally, we consider the setting of stochastic data acquisition, which facilitates our method to e ciently learn the corresponding hashing functions via stochastic gradient descend (SGD) online. Notably, the proposed HCOH can be embedded with supervised labels and is not limited to a prede ned category number. Extensive experiments on three widely-used benchmarks demonstrate the merits of the proposed scheme over the state-of-the-art methods.},
	language = {en},
	urldate = {2023-06-02},
	booktitle = {Proceedings of the 26th {ACM} international conference on {Multimedia}},
	publisher = {ACM},
	author = {Lin, Mingbao and Ji, Rongrong and Liu, Hong and Wu, Yongjian},
	month = oct,
	year = {2018},
	keywords = {Untagged},
	pages = {1635--1643},
}

@article{linBilinearConvolutionalNeural2018,
	title = {Bilinear {Convolutional} {Neural} {Networks} for {Fine}-{Grained} {Visual} {Recognition}},
	volume = {40},
	shorttitle = {{BCNN}},
	doi = {10/gdjg84},
	number = {6},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Lin, Tsung-Yu and RoyChowdhury, Aruni and Maji, Subhransu},
	month = jun,
	year = {2018},
	keywords = {Untagged},
	pages = {1309--1322},
}

@inproceedings{linPacGANPowerTwo2018,
	address = {Montréal, Canada},
	title = {{PacGAN}: {The} power of two samples in generative adversarial networks},
	url = {https://proceedings.neurips.cc/paper/2018/hash/288cc0ff022877bd3df94bc9360b9c5d-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Lin, Zinan and Khetan, Ashish and Fanti, Giulia C. and Oh, Sewoong},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	keywords = {Untagged},
	pages = {1505--1514},
}

@inproceedings{liuFeatureSpaceTransfer2018,
	address = {Salt Lake City, UT},
	title = {Feature {Space} {Transfer} for {Data} {Augmentation}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Feature_Space_Transfer_CVPR_2018_paper.html},
	doi = {10/gf5bqx},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Liu, Bo and Wang, Xudong and Dixit, Mandar and Kwitt, Roland and Vasconcelos, Nuno},
	year = {2018},
	keywords = {Untagged},
	pages = {9090--9098},
}

@inproceedings{liuHierarchicalRepresentationsEfficient2018,
	address = {Vancouver, BC, Canada},
	title = {Hierarchical {Representations} for {Efficient} {Architecture} {Search}},
	url = {https://openreview.net/forum?id=BJQRKzbA-},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{liuIndexRetrieveMultimedia2018,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Index and {Retrieve} {Multimedia} {Data}: {Cross}-{Modal} {Hashing} by {Learning} {Subspace} {Relation}},
	volume = {10828},
	url = {https://doi.org/10.1007/978-3-319-91458-9_37},
	doi = {10/ghjr5d},
	booktitle = {Database {Systems} for {Advanced} {Applications}},
	publisher = {Springer},
	author = {Liu, Luchen and Yang, Yang and Hu, Mengqiu and Xu, Xing and Shen, Fumin and Xie, Ning and Huang, Zi},
	editor = {Pei, Jian and Manolopoulos, Yannis and Sadiq, Shazia W. and Li, Jianxin},
	year = {2018},
	keywords = {Untagged},
	pages = {606--621},
}

@article{longUnderstandingMembershipInferences2018,
	title = {Understanding {Membership} {Inferences} on {Well}-{Generalized} {Learning} {Models}},
	url = {http://arxiv.org/abs/1802.04889},
	abstract = {Membership Inference Attack (MIA) determines the presence of a record in a machine learning model's training data by querying the model. Prior work has shown that the attack is feasible when the model is overfitted to its training data or when the adversary controls the training algorithm. However, when the model is not overfitted and the adversary does not control the training algorithm, the threat is not well understood. In this paper, we report a study that discovers overfitting to be a sufficient but not a necessary condition for an MIA to succeed. More specifically, we demonstrate that even a well-generalized model contains vulnerable instances subject to a new generalized MIA (GMIA). In GMIA, we use novel techniques for selecting vulnerable instances and detecting their subtle influences ignored by overfitting metrics. Specifically, we successfully identify individual records with high precision in real-world datasets by querying black-box machine learning models. Further we show that a vulnerable record can even be indirectly attacked by querying other related records and existing generalization techniques are found to be less effective in protecting the vulnerable instances. Our findings sharpen the understanding of the fundamental cause of the problem: the unique influences the training instance may have on the model.},
	urldate = {2022-03-21},
	journal = {arXiv:1802.04889 [cs, stat]},
	author = {Long, Yunhui and Bindschaedler, Vincent and Wang, Lei and Bu, Diyue and Wang, Xiaofeng and Tang, Haixu and Gunter, Carl A. and Chen, Kai},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.04889},
	keywords = {Untagged},
}

@inproceedings{luAdaptiveDifferentialPrivacy2018,
	address = {Xi'an, China},
	title = {Adaptive {Differential} {Privacy} {Interactive} {Publishing} {Model} {Based} on {Dynamic} {Feedback}},
	url = {https://doi.org/10.1109/NANA.2018.8648706},
	doi = {10/ghwng6},
	booktitle = {International {Conference} on {Networking} and {Network} {Applications}},
	publisher = {IEEE},
	author = {Lu, Laifeng and Li, Yanping and Zhou, Yihui and Tian, Feng and Liu, Hai},
	year = {2018},
	keywords = {Untagged},
	pages = {218--222},
}

@inproceedings{luAttributeGuidedFaceGeneration2018,
	address = {Munich, Germany},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Attribute-{Guided} {Face} {Generation} {Using} {Conditional} {CycleGAN}},
	volume = {11216},
	url = {https://doi.org/10.1007/978-3-030-01258-8_18},
	doi = {10/ghwnhr},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Lu, Yongyi and Tai, Yu-Wing and Tang, Chi-Keung},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Untagged},
	pages = {293--308},
}

@inproceedings{luoAsymmetricDiscreteCrossModal2018,
	address = {Yokohama Japan},
	title = {Asymmetric {Discrete} {Cross}-{Modal} {Hashing}},
	isbn = {978-1-4503-5046-4},
	url = {https://dl.acm.org/doi/10.1145/3206025.3206034},
	doi = {10/ghv3c9},
	abstract = {Recently, cross-modal hashing (CMH) methods have attracted much attention. Many methods have been explored; however, there are still some issues that need to be further considered. 1) How to efficiently construct the correlations among heterogeneous modalities. 2) How to solve the NP-hard optimization problem and avoid the large quantization errors generated by relaxation. 3) How to handle the complex and difficult problem in most CMH methods that simultaneously learning the hash codes and hash functions. To address these challenges, we present a novel cross-modal hashing algorithm, named Asymmetric Discrete Cross-Modal Hashing (ADCH). Specifically, it leverages the collective matrix factorization technique to learn the common latent representations while preserving not only the cross-correlation from different modalities but also the semantic similarity. Instead of relaxing the binary constraints, it generates the hash codes directly using an iterative optimization algorithm proposed in this work. Based the learnt hash codes, ADCH further learns a series of binary classifiers as hash functions, which is flexible and effective. Extensive experiments are conducted on three real-world datasets. The results demonstrate that ADCH outperforms several state-of-the-art cross-modal hashing baselines.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {International {Conference} on {Multimedia} {Retrieval}},
	publisher = {ACM},
	author = {Luo, Xin and Zhang, Peng-Fei and Wu, Ye and Chen, Zhen-Duo and Huang, Hua-Junjie and Xu, Xin-Shun},
	month = jun,
	year = {2018},
	keywords = {Untagged},
	pages = {204--212},
}

@inproceedings{maShuffleNetV2Practical2018,
	address = {Munich, Germany},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{ShuffleNet} {V2}: {Practical} {Guidelines} for {Efficient} {CNN} {Architecture} {Design}},
	volume = {11218},
	url = {https://doi.org/10.1007/978-3-030-01264-9_8},
	doi = {10/ggv7q6},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Untagged},
	pages = {122--138},
}

@inproceedings{maDAGANInstanceLevelImage2018,
	address = {Salt Lake City, UT},
	title = {{DA}-{GAN}: {Instance}-{Level} {Image} {Translation} by {Deep} {Attention} {Generative} {Adversarial} {Networks}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Ma_DA-GAN_Instance-Level_Image_CVPR_2018_paper.html},
	doi = {10/ghwnj8},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Ma, Shuang and Fu, Jianlong and Chen, Chang Wen and Mei, Tao},
	year = {2018},
	keywords = {Untagged},
	pages = {5657--5666},
}

@inproceedings{ma_characterizing_2018,
	title = {Characterizing {Adversarial} {Subspaces} {Using} {Local} {Intrinsic} {Dimensionality}},
	url = {https://openreview.net/forum?id=B1gJ1L2aW},
	abstract = {We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID).},
	language = {en},
	urldate = {2022-05-29},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Ma, Xingjun and Li, Bo and Wang, Yisen and Erfani, Sarah M. and Wijewickrema, Sudanthi and Schoenebeck, Grant and Song, Dawn and Houle, Michael E. and Bailey, James},
	month = feb,
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{madryDeepLearningModels2018,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	shorttitle = {{PGD}},
	url = {https://openreview.net/forum?id=rJzIBfZAb},
	abstract = {We provide a principled, optimization-based re-look at the notion of adversarial examples, and develop methods that produce models that are adversarially robust against a wide range of adversaries.},
	language = {en},
	urldate = {2022-03-17},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	month = feb,
	year = {2018},
	keywords = {Untagged},
}

@article{mcallisterRobustnessOutofDistributionInputs2018,
	title = {Robustness to {Out}-of-{Distribution} {Inputs} via {Task}-{Aware} {Generative} {Uncertainty}},
	url = {http://arxiv.org/abs/1812.10687},
	abstract = {Deep learning provides a powerful tool for machine perception when the observations resemble the training data. However, real-world robotic systems, especially mobile robots or autonomous vehicles, must react intelligently to their observations even in unexpected circumstances. This requires a system to reason about its own uncertainty given unfamiliar, out-of-distribution observations. Approximate Bayesian approaches are commonly used to estimate uncertainty for neural network predictions, but can struggle with out-of-distribution observations. Generative models can in principle detect out-ofdistribution observations as those with a low estimated density. However, the mere presence of an out-of-distribution input does not by itself indicate an unsafe situation. Intuitively, we would like a perception system that can detect when task-salient parts of the image are unfamiliar or uncertain, while ignoring taskirrelevant features. In this paper, we present a method for uncertainty-aware robotic perception that combines generative modeling and model uncertainty to cope with uncertainty stemming from out-of-distribution states, undersampling, and noisy data. Our method estimates an uncertainty measure about the model’s prediction, taking into account an explicit (generative) model of the observation distribution to handle outof-distribution inputs. This is accomplished by probabilistically projecting observations onto the training distribution, such that out-of-distribution inputs map to uncertain in-distribution observations, which in turn produce uncertain task-related predictions, but only if task-relevant parts of the image change. For example, a change of wall color should not confuse a ground robot, while an unfamiliar obstacle should trigger an increase in collision prediction uncertainty. We evaluate our method on an action-conditioned collision prediction task with both simulated and real data, and demonstrate that our method of projecting out-of-distribution observations improves the performance of four standard Bayesian and non-Bayesian neural network approaches, offering more favorable trade-offs between the proportion of time a robot can remain autonomous and the proportion of impending crashes successfully avoided.},
	language = {en},
	urldate = {2021-10-29},
	journal = {arXiv:1812.10687 [cs, stat]},
	author = {McAllister, Rowan and Kahn, Gregory and Clune, Jeff and Levine, Sergey},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.10687},
	keywords = {Untagged},
}

@inproceedings{mcmahanLearningDifferentiallyPrivate2018,
	title = {Learning {Differentially} {Private} {Recurrent} {Language} {Models}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {McMahan, H. Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
	year = {2018},
	keywords = {Untagged},
}

@article{melisExploitingUnintendedFeature2018,
	title = {Exploiting {Unintended} {Feature} {Leakage} in {Collaborative} {Learning}},
	url = {http://arxiv.org/abs/1805.04049},
	abstract = {Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points -- for example, specific locations -- in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses.},
	urldate = {2022-03-21},
	journal = {arXiv:1805.04049 [cs]},
	author = {Melis, Luca and Song, Congzheng and De Cristofaro, Emiliano and Shmatikov, Vitaly},
	month = nov,
	year = {2018},
	note = {arXiv: 1805.04049},
	keywords = {Untagged},
}

@inproceedings{miyatoCGANsProjectionDiscriminator2018,
	address = {Vancouver, BC, Canada},
	title = {{cGANs} with {Projection} {Discriminator}},
	url = {https://openreview.net/forum?id=ByS1VpgRZ},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Miyato, Takeru and Koyama, Masanori},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{mosbach_logit_2018,
	title = {Logit {Pairing} {Methods} {Can} {Fool} {Gradient}-{Based} {Attacks}},
	booktitle = {{arXiv}:1810.12042},
	author = {Mosbach, Marius and Andriushchenko, Maksym and Trost, Thomas and Hein, Matthias and Klakow, Dietrich},
	year = {2018},
	keywords = {Untagged},
}

@article{mu_all-but--top_2018,
	title = {{ALL}-{BUT}-{THE}-{TOP}: {SIMPLE} {AND} {EFFECTIVE} {POST}- {PROCESSING} {FOR} {WORD} {REPRESENTATIONS}},
	abstract = {Real-valued word representations have transformed NLP applications; popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a very simple, and yet counter-intuitive, postprocessing technique – eliminate the common mean vector and a few top dominating directions from the word vectors – that renders off-the-shelf representations even stronger. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level tasks (semantic textural similarity and text classiﬁcation) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages; in each case, the processed representations are consistently better than the original ones.},
	language = {en},
	author = {Mu, Jiaqi and Viswanath, Pramod},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{murezImageImageTranslation2018,
	address = {Salt Lake City, UT},
	title = {Image to {Image} {Translation} for {Domain} {Adaptation}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Murez_Image_to_Image_CVPR_2018_paper.html},
	doi = {10/ghcmbv},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Murez, Zak and Kolouri, Soheil and Kriegman, David J. and Ramamoorthi, Ravi and Kim, Kyungnam},
	year = {2018},
	keywords = {Untagged},
	pages = {4500--4509},
}

@inproceedings{na_cascade_2018,
	title = {Cascade {Adversarial} {Machine} {Learning} {Regularized} with a {Unified} {Embedding}},
	url = {https://openreview.net/forum?id=HyRVBzap-},
	abstract = {Cascade adversarial training + low level similarity learning improve robustness against both white box and black box attacks.},
	language = {en},
	urldate = {2022-05-29},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Na, Taesik and Ko, Jong Hwan and Mukhopadhyay, Saibal},
	month = feb,
	year = {2018},
	keywords = {Untagged},
}

@article{nakosImprovedAlgorithmsAdaptive2018,
	title = {Improved algorithms for adaptive compressed sensing},
	journal = {arXiv preprint arXiv:1804.09673},
	author = {Nakos, Vasileios and Shi, Xiaofei and Woodruff, David P. and Zhang, Hongyang},
	year = {2018},
	keywords = {Untagged},
}

@techreport{naorDifferentialPrivacyWhat2018,
	address = {Weizmann Institute of Science},
	title = {Differential {Privacy}: {What}, {Why} and {When}},
	language = {en},
	author = {Naor, Moni},
	month = nov,
	year = {2018},
	keywords = {Untagged},
	pages = {71},
}

@article{nasrMachineLearningMembership2018,
	title = {Machine {Learning} with {Membership} {Privacy} using {Adversarial} {Regularization}},
	url = {http://arxiv.org/abs/1807.05852},
	abstract = {Machine learning models leak information about the datasets on which they are trained. An adversary can build an algorithm to trace the individual members of a model's training dataset. As a fundamental inference attack, he aims to distinguish between data points that were part of the model's training set and any other data points from the same distribution. This is known as the tracing (and also membership inference) attack. In this paper, we focus on such attacks against black-box models, where the adversary can only observe the output of the model, but not its parameters. This is the current setting of machine learning as a service in the Internet. We introduce a privacy mechanism to train machine learning models that provably achieve membership privacy: the model's predictions on its training data are indistinguishable from its predictions on other data points from the same distribution. We design a strategic mechanism where the privacy mechanism anticipates the membership inference attacks. The objective is to train a model such that not only does it have the minimum prediction error (high utility), but also it is the most robust model against its corresponding strongest inference attack (high privacy). We formalize this as a min-max game optimization problem, and design an adversarial training algorithm that minimizes the classification loss of the model as well as the maximum gain of the membership inference attack against it. This strategy, which guarantees membership privacy (as prediction indistinguishability), acts also as a strong regularizer and significantly generalizes the model. We evaluate our privacy mechanism on deep neural networks using different benchmark datasets. We show that our min-max strategy can mitigate the risk of membership inference attacks (close to the random guess) with a negligible cost in terms of the classification error.},
	urldate = {2022-03-14},
	journal = {arXiv:1807.05852 [cs, stat]},
	author = {Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.05852},
	keywords = {Untagged},
}

@inproceedings{nazeriImageColorizationUsing2018,
	address = {Cham},
	title = {Image {Colorization} {Using} {Generative} {Adversarial} {Networks}},
	isbn = {978-3-319-94544-6},
	abstract = {Over the last decade, the process of automatic image colorization has been of significant interest for several application areas including restoration of aged or degraded images. This problem is highly ill-posed due to the large degrees of freedom during the assignment of color information. Many of the recent developments in automatic colorization involve images that contain a common theme or require highly processed data such as semantic maps as input. In our approach, we attempt to fully generalize the colorization procedure using a conditional Deep Convolutional Generative Adversarial Network (DCGAN). The network is trained over datasets that are publicly available such as CIFAR-10 and Places365. The results between the generative model and traditional deep neural networks are compared.},
	booktitle = {Articulated {Motion} and {Deformable} {Objects}},
	publisher = {Springer International Publishing},
	author = {Nazeri, Kamyar and Ng, Eric and Ebrahimi, Mehran},
	editor = {Perales, Francisco José and Kittler, Josef},
	year = {2018},
	keywords = {Untagged},
	pages = {85--94},
}

@book{nesterovLecturesConvexOptimization2018,
	address = {Cham},
	series = {Springer {Optimization} and {Its} {Applications}},
	title = {Lectures on {Convex} {Optimization}},
	volume = {137},
	isbn = {978-3-319-91577-7 978-3-319-91578-4},
	url = {http://link.springer.com/10.1007/978-3-319-91578-4},
	language = {en},
	urldate = {2021-07-13},
	publisher = {Springer International Publishing},
	author = {Nesterov, Yurii},
	year = {2018},
	doi = {10.1007/978-3-319-91578-4},
	keywords = {Untagged},
}

@inproceedings{nguyenWeaklySupervisedAction2018,
	address = {Salt Lake City, UT},
	title = {Weakly {Supervised} {Action} {Localization} by {Sparse} {Temporal} {Pooling} {Network}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Nguyen_Weakly_Supervised_Action_CVPR_2018_paper.html},
	doi = {10/ghwngp},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Nguyen, Phuc and Liu, Ting and Prasad, Gautam and Han, Bohyung},
	year = {2018},
	keywords = {Untagged},
	pages = {6752--6761},
}

@inproceedings{niuWeblySupervisedLearning2018,
	address = {Salt Lake City, UT},
	title = {Webly {Supervised} {Learning} {Meets} {Zero}-shot {Learning}: {A} {Hybrid} {Approach} for {Fine}-{Grained} {Classification}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Webly {Supervised} {Learning} {Meets} {Zero}-shot {Learning}},
	url = {https://ieeexplore.ieee.org/document/8578847/},
	doi = {10/ghv3t6},
	abstract = {Fine-grained image classiﬁcation, which targets at distinguishing subtle distinctions among various subordinate categories, remains a very difﬁcult task due to the high annotation cost of enormous ﬁne-grained categories. To cope with the scarcity of well-labeled training images, existing works mainly follow two research directions: 1) utilize freely available web images without human annotation; 2) only annotate some ﬁne-grained categories and transfer the knowledge to other ﬁne-grained categories, which falls into the scope of zero-shot learning (ZSL). However, the above two directions have their own drawbacks. For the ﬁrst direction, the labels of web images are very noisy and the data distribution between web images and test images are considerably different. For the second direction, the performance gap between ZSL and traditional supervised learning is still very large. The drawbacks of the above two directions motivate us to design a new framework which can jointly leverage both web data and auxiliary labeled categories to predict the test categories that are not associated with any well-labeled training images. Comprehensive experiments on three benchmark datasets demonstrate the effectiveness of our proposed framework.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Niu, Li and Veeraraghavan, Ashok and Sabharwal, Ashu},
	year = {2018},
	keywords = {Untagged},
	pages = {7171--7180},
}

@inproceedings{niu_adversarial_2018,
	address = {Brussels, Belgium},
	title = {Adversarial {Over}-{Sensitivity} and {Over}-{Stability} {Strategies} for {Dialogue} {Models}},
	url = {https://aclanthology.org/K18-1047},
	doi = {10.18653/v1/K18-1047},
	abstract = {We present two categories of model-agnostic adversarial strategies that reveal the weaknesses of several generative, task-oriented dialogue models: Should-Not-Change strategies that evaluate over-sensitivity to small and semantics-preserving edits, as well as Should-Change strategies that test if a model is over-stable against subtle yet semantics-changing modifications. We next perform adversarial training with each strategy, employing a max-margin approach for negative generative examples. This not only makes the target dialogue model more robust to the adversarial inputs, but also helps it perform significantly better on the original inputs. Moreover, training on all strategies combined achieves further improvements, achieving a new state-of-the-art performance on the original task (also verified via human evaluation). In addition to adversarial training, we also address the robustness task at the model-level, by feeding it subword units as both inputs and outputs, and show that the resulting model is equally competitive, requires only 1/4 of the original vocabulary size, and is robust to one of the adversarial strategies (to which the original model is vulnerable) even without adversarial training.},
	urldate = {2023-04-11},
	booktitle = {Proceedings of the 22nd {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Niu, Tong and Bansal, Mohit},
	month = oct,
	year = {2018},
	keywords = {Untagged},
	pages = {486--496},
}

@inproceedings{nutterDesignNovelDeep2018,
	title = {Design of {Novel} {Deep} {Learning} {Models} for {Real}-time {Human} {Activity} {Recognition} with {Mobile} {Phones}},
	doi = {10.1109/IJCNN.2018.8489319},
	abstract = {In this paper we present deep learning based techniques for human activity classification that are designed to run in real time on mobile devices. Our methods minimize the size of the model and computational overhead in order to run on the embedded processor and preserve battery life. Prior work shows that the inertial measurement unit (IMU) data from waist-mounted mobile phones can be used to develop accurate classification models for various human activities such as walking, running, stair-climbing, etc. However, these models have largely been based on hand crafted features derived from temporal and spectral statistics. More recently, deep learning has been applied to IMU sensor data, but have not been optimized for resourceconstrained devices. We present a detailed study of the traditional hand-crafted features used for shallow/statistical models that consist of a over 561 manually chosen set of dimensions. We show, through principal component analysis (PCA) and application of a published support vector machine (SVM) pipeline, that the number of features can be significantly reduced - less than 100 features that give the same performance. In addition, we show that features derived from frequency-domain transformations do not contribute to the accuracy of these models. Finally, we provide details of our learning technique which creates 2D signal images from windowed samples of IMU data. Our pipeline includes a convolutional neural network (CNN) with several layers (1 convolutional layer and 1 averaging layer and a fully connected layer). We show that by removing the steps in the pipeline and layers in the CNN, we can still achieve 0.98 F1 score but with a much smaller memory footprint and corresponding computational cost. To increase the classification accuracy of our pipeline we added a hybrid bi-class support vector machine (SVM) that was trained using the labeled and flattened convolutional layer after each training image was processed. The learned feature set is almost half the size of the original hand crafted feature set and combining the CNN with the SVM results in 0.99 F1 score. We also investigate a novel application of transfer learning by using the time series 2D signal images to re-train two different publicly available networks, Inception/ImageNet and MobileNet. We find that re-trained ImageNet networks could be created \${\textless}; 5.5\$ MB (suitable for mobile phones) and classification accuracy ranging from 0.83 to 0.93 (F1 score), thus indicating that retraining can be a useful future direction to build new classifiers for continuously evolving activities quickly while also being applicable to mobile device classification. Finally, we show that these deep learning models may be generalizable enough such that classifiers built from a given set of users for a specified set of activities can be used for a new user/subject as well.},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Nutter, Mark and Crawford, Catherine H. and Ortiz, Jorge},
	month = jul,
	year = {2018},
	note = {ISSN: 2161-4407},
	keywords = {Untagged},
	pages = {1--8},
}

@article{nwekeDeepLearningAlgorithms2018,
	title = {Deep learning algorithms for human activity recognition using mobile and wearable sensor networks: {State} of the art and research challenges},
	volume = {105},
	issn = {09574174},
	shorttitle = {Deep learning algorithms for human activity recognition using mobile and wearable sensor networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417418302136},
	doi = {10.1016/j.eswa.2018.03.056},
	language = {en},
	urldate = {2021-04-09},
	journal = {Expert Systems with Applications},
	author = {Nweke, Henry Friday and Teh, Ying Wah and Al-garadi, Mohammed Ali and Alo, Uzoma Rita},
	month = sep,
	year = {2018},
	keywords = {Untagged},
	pages = {233--261},
}

@article{oordNeuralDiscreteRepresentation2018,
	title = {Neural {Discrete} {Representation} {Learning}},
	shorttitle = {{VQ}-{VAE}},
	url = {http://arxiv.org/abs/1711.00937},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	urldate = {2021-08-11},
	journal = {arXiv:1711.00937 [cs]},
	author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
	month = may,
	year = {2018},
	note = {arXiv: 1711.00937},
	keywords = {Untagged},
}

@inproceedings{oreshkinTADAMTaskDependent2018,
	address = {Montréal, Canada},
	title = {{TADAM}: {Task} dependent adaptive metric for improved few-shot learning},
	url = {https://proceedings.neurips.cc/paper/2018/hash/66808e327dc79d135ba18e051673d906-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Oreshkin, Boris N. and López, Pau Rodríguez and Lacoste, Alexandre},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	keywords = {Untagged},
	pages = {719--729},
}

@inproceedings{panRecurrentResidualModule2018,
	address = {Salt Lake City, UT},
	title = {Recurrent {Residual} {Module} for {Fast} {Inference} in {Videos}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Pan_Recurrent_Residual_Module_CVPR_2018_paper.html},
	doi = {10/gfvmk3},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Pan, Bowen and Lin, Wuwei and Fang, Xiaolin and Huang, Chaoqin and Zhou, Bolei and Lu, Cewu},
	year = {2018},
	keywords = {Untagged},
	pages = {1536--1545},
}

@inproceedings{papiniStochasticVarianceReducedPolicy2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Stochastic {Variance}-{Reduced} {Policy} {Gradient}},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/papini18a.html},
	urldate = {2022-02-25},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018, {Stockholmsmässan}, {Stockholm}, {Sweden}, {July} 10-15, 2018},
	publisher = {PMLR},
	author = {Papini, Matteo and Binaghi, Damiano and Canonaco, Giuseppe and Pirotta, Matteo and Restelli, Marcello},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	keywords = {Untagged},
	pages = {4023--4032},
}

@inproceedings{peiMultiadversarialDomainAdaptation2018,
	title = {Multi-adversarial domain adaptation},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Pei, Z. and Cao, Z. and Long, M. and Wang, J.},
	year = {2018},
	keywords = {Untagged},
	pages = {3934--3941},
}

@inproceedings{pengMegDetLargeMiniBatch2018,
	address = {Salt Lake City, UT},
	title = {{MegDet}: {A} {Large} {Mini}-{Batch} {Object} {Detector}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Peng_MegDet_A_Large_CVPR_2018_paper.html},
	doi = {10/ggcxcm},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Peng, Chao and Xiao, Tete and Li, Zeming and Jiang, Yuning and Zhang, Xiangyu and Jia, Kai and Yu, Gang and Sun, Jian},
	year = {2018},
	keywords = {Untagged},
	pages = {6181--6189},
}

@inproceedings{periDisguiseNetContrastiveApproach2018,
	title = {{DisguiseNet}: {A} {Contrastive} {Approach} for {Disguised} {Face} {Verification} in the {Wild}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018_workshops/w1/html/Peri_DisguiseNet_A_Contrastive_CVPR_2018_paper.html},
	doi = {10/ghwngt},
	booktitle = {2018 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}, {CVPR} {Workshops} 2018, {Salt} {Lake} {City}, {UT}, {USA}, {June} 18-22, 2018},
	publisher = {IEEE Computer Society},
	author = {Peri, Skand Vishwanath and Dhall, Abhinav},
	year = {2018},
	keywords = {Untagged},
	pages = {25--31},
}

@inproceedings{phamEfficientNeuralArchitecture2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Efficient {Neural} {Architecture} {Search} via {Parameter} {Sharing}},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/pham18a.html},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018, {Stockholmsmässan}, {Stockholm}, {Sweden}, {July} 10-15, 2018},
	publisher = {PMLR},
	author = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	keywords = {Untagged},
	pages = {4092--4101},
}

@inproceedings{pmlr-v80-pham18a,
	series = {Proceedings of machine learning research},
	title = {Efficient neural architecture search via parameters sharing},
	volume = {80},
	url = {https://proceedings.mlr.press/v80/pham18a.html},
	abstract = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. ENAS constructs a large computational graph, where each subgraph represents a neural network architecture, hence forcing all architectures to share their parameters. A controller is trained with policy gradient to search for a subgraph that maximizes the expected reward on a validation set. Meanwhile a model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Sharing parameters among child models allows ENAS to deliver strong empirical performances, whilst using much fewer GPU-hours than existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On Penn Treebank, ENAS discovers a novel architecture that achieves a test perplexity of 56.3, on par with the existing state-of-the-art among all methods without post-training processing. On CIFAR-10, ENAS finds a novel architecture that achieves 2.89\% test error, which is on par with the 2.65\% test error of NASNet (Zoph et al., 2018).},
	booktitle = {Proceedings of the 35th international conference on machine learning},
	publisher = {PMLR},
	author = {Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	note = {tex.pdf: http://proceedings.mlr.press/v80/pham18a/pham18a.pdf},
	keywords = {Untagged},
	pages = {4095--4104},
}

@inproceedings{pidhorskyiDeepSupervisedHashing2018,
	address = {Perth, Australia},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Deep {Supervised} {Hashing} with {Spherical} {Embedding}},
	volume = {11364},
	url = {https://doi.org/10.1007/978-3-030-20870-7_26},
	doi = {10/ghwnqh},
	booktitle = {Asian {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Pidhorskyi, Stanislav and Jones, Quinn and Motiian, Saeid and Adjeroh, Donald A. and Doretto, Gianfranco},
	editor = {Jawahar, C. V. and Li, Hongdong and Mori, Greg and Schindler, Konrad},
	year = {2018},
	keywords = {Untagged},
	pages = {417--434},
}

@inproceedings{pinheiroUnsupervisedDomainAdaptation2018,
	address = {Salt Lake City, UT},
	title = {Unsupervised {Domain} {Adaptation} with {Similarity} {Learning}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578933/},
	doi = {10/ghv25v},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Pinheiro, Pedro O.},
	year = {2018},
	keywords = {Untagged},
	pages = {8004--8013},
}

@article{puerto_minimax_2018,
	series = {Special {Issue} on {Convex} {Analysis} and {Optimization}: {New} {Trends} in {Theory} and {Applications}},
	title = {On minimax and {Pareto} optimal security payoffs in multicriteria games},
	volume = {457},
	issn = {0022-247X},
	url = {https://www.sciencedirect.com/science/article/pii/S0022247X17300136},
	doi = {10.1016/j.jmaa.2017.01.002},
	abstract = {In this paper, we characterize minimax and Pareto-optimal security payoff vectors for general multicriteria zero-sum matrix games, using properties similar to the ones that have been used in the single criterion case. Our results show that these two solution concepts are rather similar, since they can be characterized with nearly the same sets of properties. Their main difference is the form of consistency that each solution concept satisfies. We also prove that both solution concepts can transform into each other, in their corresponding domains.},
	language = {en},
	number = {2},
	urldate = {2023-05-11},
	journal = {Journal of Mathematical Analysis and Applications},
	author = {Puerto, Justo and Perea, Federico},
	month = jan,
	year = {2018},
	keywords = {Untagged},
	pages = {1634--1648},
}

@inproceedings{qianSensorBasedActivityRecognition2018,
	address = {New Orleans, Louisiana},
	title = {Sensor-{Based} {Activity} {Recognition} via {Learning} {From} {Distributions}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16305},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Qian, Hangwei and Pan, Sinno Jialin and Miao, Chunyan},
	editor = {McIlraith, Sheila A. and Weinberger, Kilian Q.},
	year = {2018},
	keywords = {Untagged},
	pages = {6262--6269},
}

@inproceedings{qianPoseNormalizedImageGeneration2018,
	address = {Munich, Germany},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Pose-{Normalized} {Image} {Generation} for {Person} {Re}-identification},
	volume = {11213},
	url = {https://doi.org/10.1007/978-3-030-01240-3_40},
	doi = {10/ggv9qw},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Qian, Xuelin and Fu, Yanwei and Xiang, Tao and Wang, Wenxuan and Qiu, Jie and Wu, Yang and Jiang, Yu-Gang and Xue, Xiangyang},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Untagged},
	pages = {661--678},
}

@article{qiaoGeometryContrastiveGANFacial2018,
	title = {Geometry-{Contrastive} {GAN} for {Facial} {Expression} {Transfer}},
	url = {http://arxiv.org/abs/1802.01822},
	abstract = {In this paper, we propose a geometry-contrastive generative adversarial network GC-GAN for generating facial expression images conditioned on geometry information. Speciﬁcally, given an input face and a target expression designated by a set of facial landmarks, an identity-preserving face can be generated guided by the target expression. In order to embed facial geometry onto a semantic manifold, we incorporate contrastive learning into conditional GANs. Experiment results demonstrate that the manifold is sensitive to the changes of facial geometry both globally and locally. Beneﬁted from the semantic manifold, dynamic smooth transitions between different facial expressions are exhibited via geometry interpolation. Furthermore, our method can also be applied in facial expression transfer even there exist big differences in face shape between target faces and driving faces.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1802.01822 [cs]},
	author = {Qiao, Fengchun and Yao, Naiming and Jiao, Zirui and Li, Zhihao and Chen, Hui and Wang, Hongan},
	month = oct,
	year = {2018},
	note = {arXiv: 1802.01822},
	keywords = {Untagged},
}

@article{rahmanMembershipInferenceAttack2018,
	title = {Membership {Inference} {Attack} against {Differentially} {Private} {Deep} {Learning} {Model}},
	abstract = {The unprecedented success of deep learning is largely dependent on the availability of massive amount of training data. In many cases, these data are crowd-sourced and may contain sensitive and conﬁdential information, therefore, pose privacy concerns. As a result, privacy-preserving deep learning has been gaining increasing focus nowadays. One of the promising approaches for privacy-preserving deep learning is to employ differential privacy during model training which aims to prevent the leakage of sensitive information about the training data via the trained model. While these models are considered to be immune to privacy attacks, with the advent of recent and sophisticated attack models, it is not clear how well these models trade-off utility for privacy. In this paper, we systematically study the impact of a sophisticated machine learning based privacy attack called the membership inference attack against a state-of-the-art differentially private deep model. More speciﬁcally, given a differentially private deep model with its associated utility, we investigate how much we can infer about the model’s training data. Our experimental results show that differentially private deep models may keep their promise to provide privacy protection against strong adversaries by only offering poor model utility, while exhibit moderate vulnerability to the membership inference attack when they offer an acceptable utility. For evaluating our experiments, we use the CIFAR-10 and MNIST datasets and the corresponding classiﬁcation tasks.},
	language = {en},
	author = {Rahman, Atiqur and Rahman, Tanzila and Laganiere, Robert and Mohammed, Noman and Wang, Yang},
	year = {2018},
	keywords = {Untagged},
	pages = {19},
}

@inproceedings{reddiConvergenceAdam2018,
	title = {On the {Convergence} of {Adam} and {Beyond}},
	shorttitle = {{AmsGrad}},
	booktitle = {{ICLR}},
	author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{renRAN4IQARestorativeAdversarial2018,
	title = {{RAN4IQA}: {Restorative} {Adversarial} {Nets} for {No}-{Reference} {Image} {Quality} {Assessment}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16423},
	booktitle = {Proceedings of the {Thirty}-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}, ({AAAI}-18), the 30th innovative {Applications} of {Artificial} {Intelligence} ({IAAI}-18), and the 8th {AAAI} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence} ({EAAI}-18), {New} {Orleans}, {Louisiana}, {USA}, {February} 2-7, 2018},
	publisher = {AAAI Press},
	author = {Ren, Hongyu and Chen, Diqi and Wang, Yizhou},
	editor = {McIlraith, Sheila A. and Weinberger, Kilian Q.},
	year = {2018},
	keywords = {Untagged},
	pages = {7308--7314},
}

@inproceedings{renMetaLearningSemiSupervisedFewShot2018,
	address = {Vancouver, BC, Canada},
	title = {Meta-{Learning} for {Semi}-{Supervised} {Few}-{Shot} {Classification}},
	url = {https://openreview.net/forum?id=HJcSzz-CZ},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Ren, Mengye and Triantafillou, Eleni and Ravi, Sachin and Snell, Jake and Swersky, Kevin and Tenenbaum, Joshua B. and Larochelle, Hugo and Zemel, Richard S.},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{renCrossDomainSelfSupervisedMultiTask2018,
	address = {Salt Lake City, UT},
	title = {Cross-{Domain} {Self}-{Supervised} {Multi}-{Task} {Feature} {Learning} {Using} {Synthetic} {Imagery}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Ren_Cross-Domain_Self-Supervised_Multi-Task_CVPR_2018_paper.html},
	doi = {10/gf5brc},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Ren, Zhongzheng and Lee, Yong Jae},
	year = {2018},
	keywords = {Untagged},
	pages = {762--771},
}

@inproceedings{ribeiro_semantically_2018,
	address = {Melbourne, Australia},
	title = {Semantically {Equivalent} {Adversarial} {Rules} for {Debugging} {NLP} models},
	url = {https://aclanthology.org/P18-1079},
	doi = {10.18653/v1/P18-1079},
	abstract = {Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) – semantic-preserving perturbations that induce changes in the model's predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) – simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.},
	urldate = {2022-11-08},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = jul,
	year = {2018},
	keywords = {Untagged},
	pages = {856--865},
}

@inproceedings{richardActionSetsWeakly2018,
	address = {Salt Lake City, UT},
	title = {Action {Sets}: {Weakly} {Supervised} {Action} {Segmentation} {Without} {Ordering} {Constraints}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Action {Sets}},
	url = {https://ieeexplore.ieee.org/document/8578725/},
	doi = {10/ghv3q4},
	abstract = {Action detection and temporal segmentation of actions in videos are topics of increasing interest. While fully supervised systems have gained much attention lately, full annotation of each action within the video is costly and impractical for large amounts of video data. Thus, weakly supervised action detection and temporal segmentation methods are of great importance. While most works in this area assume an ordered sequence of occurring actions to be given, our approach only uses a set of actions. Such action sets provide much less supervision since neither action ordering nor the number of action occurrences are known. In exchange, they can be easily obtained, for instance, from meta-tags, while ordered sequences still require human annotation. We introduce a system that automatically learns to temporally segment and label actions in a video, where the only supervision that is used are action sets. An evaluation on three datasets shows that our method still achieves good results although the amount of supervision is signiﬁcantly smaller than for other related methods.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Richard, Alexander and Kuehne, Hilde and Gall, Juergen},
	month = jun,
	year = {2018},
	keywords = {Untagged},
	pages = {5987--5996},
}

@inproceedings{rokniPersonalizedHumanActivity2018,
	address = {New Orleans, Louisiana},
	title = {Personalized {Human} {Activity} {Recognition} {Using} {Convolutional} {Neural} {Networks}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16989},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Rokni, Seyed Ali and Nourollahi, Marjan and Ghasemzadeh, Hassan},
	editor = {McIlraith, Sheila A. and Weinberger, Kilian Q.},
	year = {2018},
	keywords = {Untagged},
	pages = {8143--8144},
}

@article{rosenfeldElephantRoom2018,
	title = {The {Elephant} in the {Room}},
	url = {http://arxiv.org/abs/1808.03305},
	abstract = {We showcase a family of common failures of state-of-the art object detectors. These are obtained by replacing image sub-regions by another sub-image that contains a trained object. We call this “object transplanting”. Modifying an image in this manner is shown to have a non-local impact on object detection. Slight changes in object position can affect its identity according to an object detector as well as that of other objects in the image. We provide some analysis and suggest possible reasons for the reported phenomena.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1808.03305 [cs]},
	author = {Rosenfeld, Amir and Zemel, Richard and Tsotsos, John K.},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.03305},
	keywords = {Untagged},
}

@inproceedings{ruedaLearningAttributeRepresentation2018,
	title = {Learning {Attribute} {Representation} for {Human} {Activity} {Recognition}},
	doi = {10.1109/ICPR.2018.8545146},
	abstract = {Attribute representations became relevant in image recognition and word spotting, providing support under the presence of unbalance and disjoint datasets. However, for human activity recognition using sequential data from on-body sensors, human-labeled attributes are lacking. This paper introduces a search for attributes that represent favorably signal segments for recognizing human activities. It presents three deep architectures, including temporal-convolutions and an IMU centered design, for predicting attributes. An empiric evaluation of random and learned attribute representations, and as well as the networks is carried out on two datasets, outperforming the state-of-the art.},
	booktitle = {2018 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Rueda, Fernando Moya and Fink, Gernot A.},
	month = aug,
	year = {2018},
	note = {ISSN: 1051-4651},
	keywords = {Untagged},
	pages = {523--528},
}

@inproceedings{ryooExtremeLowResolution2018,
	address = {New Orleans, Louisiana},
	title = {Extreme {Low} {Resolution} {Activity} {Recognition} {With} {Multi}-{Siamese} {Embedding} {Learning}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16790},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Ryoo, Michael S. and Kim, Kiyoon and Yang, Hyun Jong},
	editor = {McIlraith, Sheila A. and Weinberger, Kilian Q.},
	year = {2018},
	keywords = {Untagged},
	pages = {7315--7322},
}

@inproceedings{sGrayBoxAdversarialTraining2018,
	address = {Munich, Germany},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Gray-{Box} {Adversarial} {Training}},
	volume = {11219},
	url = {https://doi.org/10.1007/978-3-030-01267-0_13},
	doi = {10/ghv2mq},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {S, Vivek B. and Mopuri, Konda Reddy and Babu, R. Venkatesh},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Untagged},
	pages = {213--228},
}

@inproceedings{saitoMaximumClassifierDiscrepancy2018,
	address = {Salt Lake City, UT},
	title = {Maximum {Classifier} {Discrepancy} for {Unsupervised} {Domain} {Adaptation}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Saito_Maximum_Classifier_Discrepancy_CVPR_2018_paper.html},
	doi = {10/gf2ffb},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Saito, Kuniaki and Watanabe, Kohei and Ushiku, Yoshitaka and Harada, Tatsuya},
	year = {2018},
	keywords = {Untagged},
	pages = {3723--3732},
}

@inproceedings{sakaridis_model_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Model {Adaptation} with {Synthetic} and {Real} {Data} for {Semantic} {Dense} {Foggy} {Scene} {Understanding}},
	isbn = {978-3-030-01261-8},
	doi = {10.1007/978-3-030-01261-8_42},
	abstract = {This work addresses the problem of semantic scene understanding under dense fog. Although considerable progress has been made in semantic scene understanding, it is mainly related to clear-weather scenes. Extending recognition methods to adverse weather conditions such as fog is crucial for outdoor applications. In this paper, we propose a novel method, named Curriculum Model Adaptation (CMAda), which gradually adapts a semantic segmentation model from light synthetic fog to dense real fog in multiple steps, using both synthetic and real foggy data. In addition, we present three other main stand-alone contributions: (1) a novel method to add synthetic fog to real, clear-weather scenes using semantic input; (2) a new fog density estimator; (3) the Foggy Zurich dataset comprising 3808 real foggy images, with pixel-level semantic annotations for 16 images with dense fog. Our experiments show that (1) our fog simulation slightly outperforms a state-of-the-art competing simulation with respect to the task of semantic foggy scene understanding (SFSU); (2) CMAda improves the performance of state-of-the-art models for SFSU significantly by leveraging unlabeled real foggy data. The datasets and code will be made publicly available.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Sakaridis, Christos and Dai, Dengxin and Hecker, Simon and Van Gool, Luc},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Untagged},
	pages = {707--724},
}

@article{sakaridis_semantic_2018,
	title = {Semantic {Foggy} {Scene} {Understanding} with {Synthetic} {Data}},
	volume = {126},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-018-1072-8},
	doi = {10.1007/s11263-018-1072-8},
	abstract = {This work addresses the problem of semantic foggy scene understanding (SFSU). Although extensive research has been performed on image dehazing and on semantic scene understanding with clear-weather images, little attention has been paid to SFSU. Due to the difficulty of collecting and annotating foggy images, we choose to generate synthetic fog on real images that depict clear-weather outdoor scenes, and then leverage these partially synthetic data for SFSU by employing state-of-the-art convolutional neural networks (CNN). In particular, a complete pipeline to add synthetic fog to real, clear-weather images using incomplete depth information is developed. We apply our fog synthesis on the Cityscapes dataset and generate Foggy Cityscapes with 20,550 images. SFSU is tackled in two ways: (1) with typical supervised learning, and (2) with a novel type of semi-supervised learning, which combines (1) with an unsupervised supervision transfer from clear-weather images to their synthetic foggy counterparts. In addition, we carefully study the usefulness of image dehazing for SFSU. For evaluation, we present Foggy Driving, a dataset with 101 real-world images depicting foggy driving scenes, which come with ground truth annotations for semantic segmentation and object detection. Extensive experiments show that (1) supervised learning with our synthetic data significantly improves the performance of state-of-the-art CNN for SFSU on Foggy Driving; (2) our semi-supervised learning strategy further improves performance; and (3) image dehazing marginally advances SFSU with our learning strategy. The datasets, models and code are made publicly available.},
	language = {en},
	number = {9},
	urldate = {2022-12-06},
	journal = {International Journal of Computer Vision},
	author = {Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
	month = sep,
	year = {2018},
	keywords = {Untagged},
	pages = {973--992},
}

@article{salemMLLeaksModelData2018,
	title = {{ML}-{Leaks}: {Model} and {Data} {Independent} {Membership} {Inference} {Attacks} and {Defenses} on {Machine} {Learning} {Models}},
	shorttitle = {{ML}-{Leaks}},
	url = {http://arxiv.org/abs/1806.01246},
	abstract = {Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. However, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. In addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.},
	urldate = {2022-03-21},
	journal = {arXiv:1806.01246 [cs]},
	author = {Salem, Ahmed and Zhang, Yang and Humbert, Mathias and Berrang, Pascal and Fritz, Mario and Backes, Michael},
	month = dec,
	year = {2018},
	note = {arXiv: 1806.01246},
	keywords = {Untagged},
}

@inproceedings{samangoueiDefenseGANProtectingClassifiers2018,
	address = {Vancouver, BC, Canada},
	title = {Defense-{GAN}: {Protecting} {Classifiers} {Against} {Adversarial} {Attacks} {Using} {Generative} {Models}},
	url = {https://openreview.net/forum?id=BkJ3ibb0-},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Samangouei, Pouya and Kabkab, Maya and Chellappa, Rama},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{sandlerMobileNetV2InvertedResiduals2018,
	address = {Salt Lake City, UT},
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.html},
	doi = {10/gfxgjz},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Sandler, Mark and Howard, Andrew G. and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	year = {2018},
	keywords = {Untagged},
	pages = {4510--4520},
}

@inproceedings{sankaranarayananGenerateAdaptAligning2018,
	address = {Salt Lake City, UT},
	title = {Generate to {Adapt}: {Aligning} {Domains} {Using} {Generative} {Adversarial} {Networks}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Sankaranarayanan_Generate_to_Adapt_CVPR_2018_paper.html},
	doi = {10/ggv9rb},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Sankaranarayanan, Swami and Balaji, Yogesh and Castillo, Carlos Domingo and Chellappa, Rama},
	year = {2018},
	keywords = {Untagged},
	pages = {8503--8512},
}

@inproceedings{sener_multi-task_2018,
	title = {Multi-{Task} {Learning} as {Multi}-{Objective} {Optimization}},
	volume = {31},
	url = {https://papers.nips.cc/paper/2018/hash/432aca3a1e345e339f35a30c8f65edce-Abstract.html},
	abstract = {In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training.},
	urldate = {2022-07-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sener, Ozan and Koltun, Vladlen},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{senguptaSfSNetLearningShape2018,
	address = {Salt Lake City, UT},
	title = {{SfSNet}: {Learning} {Shape}, {Reflectance} and {Illuminance} of {Faces} 'in the {Wild}'},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Sengupta_SfSNet_Learning_Shape_CVPR_2018_paper.html},
	doi = {10/gfxjqn},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Sengupta, Soumyadip and Kanazawa, Angjoo and Castillo, Carlos Domingo and Jacobs, David W.},
	year = {2018},
	keywords = {Untagged},
	pages = {6296--6305},
}

@inproceedings{sewardFirstOrderGenerative2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {First {Order} {Generative} {Adversarial} {Networks}},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/seward18a.html},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018, {Stockholmsmässan}, {Stockholm}, {Sweden}, {July} 10-15, 2018},
	publisher = {PMLR},
	author = {Seward, Calvin and Unterthiner, Thomas and Bergmann, Urs and Jetchev, Nikolay and Hochreiter, Sepp},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	keywords = {Untagged},
	pages = {4574--4583},
}

@inproceedings{shariffDifferentiallyPrivateContextual2018,
	address = {Montr\{{\textbackslash}'\{e\}\}al, Canada},
	series = {{NIPS}'18},
	title = {Differentially {Private} {Contextual} {Linear} {Bandits}},
	abstract = {We study the contextual linear bandit problem, a version of the standard stochastic multi-armed bandit (MAB) problem where a learner sequentially selects actions to maximize a reward which depends also on a user provided per-round context. Though the context is chosen arbitrarily or adversarially, the reward is assumed to be a stochastic function of a feature vector that encodes the context and selected action. Our goal is to devise private learners for the contextual linear bandit problem.We first show that using the standard definition of differential privacy results in linear regret. So instead, we adopt the notion of joint differential privacy, where we assume that the action chosen on day t is only revealed to user t and thus needn't be kept private that day, only on following days. We give a general scheme converting the classic linear-UCB algorithm into a joint differentially private algorithm using the tree-based algorithm [10, 18]. We then apply either Gaussian noise or Wishart noise to achieve joint-differentially private algorithms and bound the resulting algorithms' regrets. In addition, we give the first lower bound on the additional regret any private algorithms for the MAB problem must incur.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Shariff, Roshan and Sheffet, Or},
	year = {2018},
	note = {event-place: Montréal, Canada},
	keywords = {Untagged},
	pages = {4301--4311},
}

@article{shenUnsupervisedDeepHashing2018,
	title = {Unsupervised {Deep} {Hashing} with {Similarity}-{Adaptive} and {Discrete} {Optimization}},
	volume = {40},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/8247210/},
	doi = {10/gfkx5z},
	abstract = {Recent vision and learning studies show that learning compact hash codes can facilitate massive data processing with signiﬁcantly reduced storage and computation. Particularly, learning deep hash functions has greatly improved the retrieval performance, typically under the semantic supervision. In contrast, current unsupervised deep hashing algorithms can hardly achieve satisfactory performance due to either the relaxed optimization or absence of similarity-sensitive objective. In this work, we propose a simple yet effective unsupervised hashing framework, named Similarity-Adaptive Deep Hashing (SADH), which alternatingly proceeds over three training modules: deep hash model training, similarity graph updating and binary code optimization. The key difference from the widely-used two-step hashing method is that the output representations of the learned deep model help update the similarity graph matrix, which is then used to improve the subsequent code optimization. In addition, for producing high-quality binary codes, we devise an effective discrete optimization algorithm which can directly handle the binary constraints with a general hashing loss. Extensive experiments validate the efﬁcacy of SADH, which consistently outperforms the state-of-the-arts by large gaps.},
	language = {en},
	number = {12},
	urldate = {2021-01-28},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Shen, Fumin and Xu, Yan and Liu, Li and Yang, Yang and Huang, Zi and Shen, Heng Tao},
	month = dec,
	year = {2018},
	keywords = {Untagged},
	pages = {3034--3044},
}

@inproceedings{shenWassersteinDistanceGuided2018,
	title = {Wasserstein {Distance} {Guided} {Representation} {Learning} for {Domain} {Adaptation}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17155},
	booktitle = {Proceedings of the {Thirty}-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}, ({AAAI}-18), the 30th innovative {Applications} of {Artificial} {Intelligence} ({IAAI}-18), and the 8th {AAAI} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence} ({EAAI}-18), {New} {Orleans}, {Louisiana}, {USA}, {February} 2-7, 2018},
	publisher = {AAAI Press},
	author = {Shen, Jian and Qu, Yanru and Zhang, Weinan and Yu, Yong},
	editor = {McIlraith, Sheila A. and Weinberger, Kilian Q.},
	year = {2018},
	keywords = {Untagged},
	pages = {4058--4065},
}

@inproceedings{shenCrowdCountingAdversarial2018,
	address = {Salt Lake City, UT},
	title = {Crowd {Counting} via {Adversarial} {Cross}-{Scale} {Consistency} {Pursuit}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578648/},
	doi = {10/ggv9bx},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Shen, Zan and Xu, Yi and Ni, Bingbing and Wang, Minsi and Hu, Jianguo and Yang, Xiaokang},
	year = {2018},
	keywords = {Untagged},
	pages = {5245--5254},
}

@inproceedings{shengQuantizationFriendlySeparableConvolution2018,
	title = {A {Quantization}-{Friendly} {Separable} {Convolution} for {MobileNets}},
	doi = {10.1109/EMC2.2018.00011},
	booktitle = {2018 1st {Workshop} on {Energy} {Efficient} {Machine} {Learning} and {Cognitive} {Computing} for {Embedded} {Applications} ({EMC2})},
	author = {Sheng, Tao and Feng, Chen and Zhuo, Shaojie and Zhang, Xiaopeng and Shen, Liang and Aleksic, Mickey},
	year = {2018},
	keywords = {Untagged},
	pages = {14--18},
}

@inproceedings{siarohinDeformableGANsPoseBased2018,
	address = {Salt Lake City, UT},
	title = {Deformable {GANs} for {Pose}-{Based} {Human} {Image} {Generation}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Siarohin_Deformable_GANs_for_CVPR_2018_paper.html},
	doi = {10/ggv7w3},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Siarohin, Aliaksandr and Sangineto, Enver and Lathuilière, Stéphane and Sebe, Nicu},
	year = {2018},
	keywords = {Untagged},
	pages = {3408--3416},
}

@inproceedings{smithBayesianPerspectiveGeneralization2018,
	address = {Vancouver, BC, Canada},
	title = {A {Bayesian} {Perspective} on {Generalization} and {Stochastic} {Gradient} {Descent}},
	url = {https://openreview.net/forum?id=BJij4yg0Z},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Smith, Samuel L. and Le, Quoc V.},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{songDeepAdaptiveTemporal2018,
	address = {Seoul, Republic of Korea},
	title = {Deep {Adaptive} {Temporal} {Pooling} for {Activity} {Recognition}},
	url = {https://doi.org/10.1145/3240508.3240713},
	doi = {10.1145/3240508.3240713},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Song, Sibo and Cheung, Ngai-Man and Chandrasekhar, Vijay and Mandal, Bappaditya},
	editor = {Boll, Susanne and Lee, Kyoung Mu and Luo, Jiebo and Zhu, Wenwu and Byun, Hyeran and Chen, Chang Wen and Lienhart, Rainer and Mei, Tao},
	year = {2018},
	keywords = {Untagged},
	pages = {1829--1837},
}

@inproceedings{song_pixeldefend_2018,
	title = {{PixelDefend}: {Leveraging} {Generative} {Models} to {Understand} and {Defend} against {Adversarial} {Examples}},
	shorttitle = {{PixelDefend}},
	url = {https://openreview.net/forum?id=rJUYGxbCW},
	abstract = {Adversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. What makes them so special in the eyes of...},
	language = {en},
	urldate = {2022-05-29},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Song, Yang and Kim, Taesup and Nowozin, Sebastian and Ermon, Stefano and Kushman, Nate},
	month = feb,
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{suGreedyHashFast2018,
	address = {Montréal, Canada},
	title = {Greedy {Hash}: {Towards} {Fast} {Optimization} for {Accurate} {Hash} {Coding} in {CNN}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/13f3cf8c531952d72e5847c4183e6910-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Su, Shupeng and Zhang, Chao and Han, Kai and Tian, Yonghong},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	keywords = {Untagged},
	pages = {806--815},
}

@article{sunMultiAttentionMultiClassConstraint2018,
	title = {Multi-{Attention} {Multi}-{Class} {Constraint} for {Fine}-grained {Image} {Recognition}},
	url = {http://arxiv.org/abs/1806.05372},
	abstract = {Attention-based learning for ﬁne-grained image recognition remains a challenging task, where most of the existing methods treat each object part in isolation, while neglecting the correlations among them. In addition, the multi-stage or multi-scale mechanisms involved make the existing methods less eﬃcient and hard to be trained end-to-end. In this paper, we propose a novel attention-based convolutional neural network (CNN) which regulates multiple object parts among diﬀerent input images. Our method ﬁrst learns multiple attention region features of each input image through the one-squeeze multi-excitation (OSME) module, and then apply the multi-attention multi-class constraint (MAMC) in a metric learning framework. For each anchor feature, the MAMC functions by pulling same-attention same-class features closer, while pushing diﬀerent-attention or diﬀerent-class features away. Our method can be easily trained end-to-end, and is highly eﬃcient which requires only one training stage. Moreover, we introduce Dogs-in-the-Wild, a comprehensive dog species dataset that surpasses similar existing datasets by category coverage, data volume and annotation quality. This dataset will be released upon acceptance to facilitate the research of ﬁne-grained image recognition. Extensive experiments are conducted to show the substantial improvements of our method on four benchmark datasets.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1806.05372 [cs]},
	author = {Sun, Ming and Yuan, Yuchen and Zhou, Feng and Ding, Errui},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.05372},
	keywords = {Untagged},
}

@article{tanSupervisedHashingEndtoEnd2018,
	title = {Supervised {Hashing} with {End}-to-{End} {Binary} {Deep} {Neural} {Network}},
	url = {http://arxiv.org/abs/1711.08901},
	abstract = {Image hashing is a popular technique applied to large scale content-based visual retrieval due to its compact and efficient binary codes. Our work proposes a new end-to-end deep network architecture for supervised hashing which directly learns binary codes from input images and maintains good properties over binary codes such as similarity preservation, independence, and balancing. Furthermore, we also propose a new learning scheme that can cope with the binary constrained loss function. The proposed algorithm not only is scalable for learning over large-scale datasets but also outperforms state-of-the-art supervised hashing methods, which are illustrated throughout extensive experiments from various image retrieval benchmarks.},
	urldate = {2021-06-30},
	journal = {arXiv:1711.08901 [cs]},
	author = {Tan, Dang-Khoa Le and Do, Thanh-Toan and Cheung, Ngai-Man},
	month = oct,
	year = {2018},
	note = {arXiv: 1711.08901},
	keywords = {Untagged},
}

@inproceedings{tangMiningSemanticsPreservingAttention2018,
	address = {Seoul, Republic of Korea},
	title = {Mining {Semantics}-{Preserving} {Attention} for {Group} {Activity} {Recognition}},
	url = {https://doi.org/10.1145/3240508.3240576},
	doi = {10.1145/3240508.3240576},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Tang, Yansong and Wang, Zian and Li, Peiyang and Lu, Jiwen and Yang, Ming and Zhou, Jie},
	editor = {Boll, Susanne and Lee, Kyoung Mu and Luo, Jiebo and Zhu, Wenwu and Byun, Hyeran and Chen, Chang Wen and Lienhart, Rainer and Mei, Tao},
	year = {2018},
	keywords = {Untagged},
	pages = {1283--1291},
}

@inproceedings{tolstikhinWassersteinAutoEncoders2018,
	address = {Vancouver, BC, Canada},
	title = {Wasserstein {Auto}-{Encoders}},
	url = {https://openreview.net/forum?id=HkL7n1-0b},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Tolstikhin, Ilya O. and Bousquet, Olivier and Gelly, Sylvain and Schölkopf, Bernhard},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{tukuljacMULANBlindOffGrid2018,
	address = {Montréal, Canada},
	title = {{MULAN}: {A} {Blind} and {Off}-{Grid} {Method} for {Multichannel} {Echo} {Retrieval}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/c9f95a0a5af052bffce5c89917335f67-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Tukuljac, Helena Peic and Deleforge, Antoine and Gribonval, Rémi},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	keywords = {Untagged},
	pages = {2186--2196},
}

@inproceedings{tungCLIPQDeepNetwork2018,
	address = {Salt Lake City, UT},
	title = {{CLIP}-{Q}: {Deep} {Network} {Compression} {Learning} by {In}-parallel {Pruning}-{Quantization}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{CLIP}-{Q}},
	url = {https://ieeexplore.ieee.org/document/8578919/},
	doi = {10/ghv3pq},
	abstract = {Deep neural networks enable state-of-the-art accuracy on visual recognition tasks such as image classiﬁcation and object detection. However, modern deep networks contain millions of learned weights; a more efﬁcient utilization of computation resources would assist in a variety of deployment scenarios, from embedded platforms with resource constraints to computing clusters running ensembles of networks. In this paper, we combine network pruning and weight quantization in a single learning framework that performs pruning and quantization jointly, and in parallel with ﬁne-tuning. This allows us to take advantage of the complementary nature of pruning and quantization and to recover from premature pruning errors, which is not possible with current two-stage approaches. Our proposed CLIP-Q method (Compression Learning by InParallel Pruning-Quantization) compresses AlexNet by 51fold, GoogLeNet by 10-fold, and ResNet-50 by 15-fold, while preserving the uncompressed network accuracies on ImageNet.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Tung, Frederick and Mori, Greg},
	year = {2018},
	keywords = {Untagged},
	pages = {7873--7882},
}

@article{veenThreeToolsPractical2018,
	title = {Three {Tools} for {Practical} {Differential} {Privacy}},
	volume = {abs/1812.02890},
	url = {http://arxiv.org/abs/1812.02890},
	journal = {CoRR},
	author = {Veen, Koen Lennart van der and Seggers, Ruben and Bloem, Peter and Patrini, Giorgio},
	year = {2018},
	note = {\_eprint: 1812.02890},
	keywords = {Untagged},
}

@inproceedings{volpiAdversarialFeatureAugmentation2018,
	address = {Salt Lake City, UT},
	title = {Adversarial {Feature} {Augmentation} for {Unsupervised} {Domain} {Adaptation}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper.html},
	doi = {10/ghwnhp},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Volpi, Riccardo and Morerio, Pietro and Savarese, Silvio and Murino, Vittorio},
	year = {2018},
	keywords = {Untagged},
	pages = {5495--5504},
}

@inproceedings{wanDiscriminativeLatentSemantic2018,
	title = {Discriminative {Latent} {Semantic} {Regression} for {Cross}-{Modal} {Hashing} of {Multimedia} {Retrieval}},
	url = {https://doi.org/10.1109/BigMM.2018.8499071},
	doi = {10/ghjr5c},
	booktitle = {{IEEE} {International} {Conference} on {Multimedia} {Big} {Data}},
	publisher = {IEEE},
	author = {Wan, Jianwu and Wang, Yi},
	year = {2018},
	keywords = {Untagged},
	pages = {1--7},
}

@inproceedings{wanEfficientAdaptiveOnline2018,
	address = {Stockholm, Sweden},
	title = {Efficient {Adaptive} {Online} {Learning} via {Frequent} {Directions}},
	url = {https://doi.org/10.24963/ijcai.2018/381},
	doi = {10/gj45dn},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Wan, Yuanyu and Wei, Nan and Zhang, Lijun},
	editor = {Lang, Jérôme},
	year = {2018},
	keywords = {Untagged},
	pages = {2748--2754},
}

@inproceedings{wangMinimizingAdaptiveRegret2018,
	address = {Stockholm, Sweden},
	title = {Minimizing {Adaptive} {Regret} with {One} {Gradient} per {Iteration}},
	isbn = {978-0-9992411-2-7},
	url = {https://www.ijcai.org/proceedings/2018/383},
	doi = {10/ghv3js},
	abstract = {To cope with non-stationary environments, recent advances in online optimization have introduced the notion of adaptive regret, which measures the performance of an online learner against different comparators within different time intervals. Previous studies have proposed various algorithms to yield low adaptive regret under different scenarios. However, all of existing algorithms need to query the gradient of the loss function at least O(log t) times in every iteration t, which hinders their applications to broad domains, especially when the evaluation of gradients is expensive. To address this limitation, we propose a series of computationally efﬁcient algorithms for minimizing the adaptive regret of general convex, strongly convex and exponentially concave functions respectively. The key idea is to replace each loss function with a carefully designed surrogate loss, which bounds the original loss function from below. We show that the proposed algorithms only query the gradient once per iteration, and attain the same theoretical guarantees as previous optimal algorithms. Empirical results demonstrate the efﬁciency and effectiveness of our methods.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Wang, Guanghui and Zhao, Dakuan and Zhang, Lijun},
	month = jul,
	year = {2018},
	keywords = {Untagged},
	pages = {2762--2768},
}

@article{wangSurveyLearningHash2018,
	title = {A {Survey} on {Learning} to {Hash}},
	volume = {40},
	doi = {10/gcp3w9},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wang, Jingdong and Zhang, Ting and Song, Jingkuan and Sebe, Nicu and Shen, Heng Tao},
	year = {2018},
	keywords = {Untagged},
	pages = {769--790},
}

@inproceedings{wangSensoryGANsEffectiveGenerative2018,
	title = {{SensoryGANs}: {An} {Effective} {Generative} {Adversarial} {Framework} for {Sensor}-based {Human} {Activity} {Recognition}},
	shorttitle = {{SensoryGANs}},
	doi = {10.1109/IJCNN.2018.8489106},
	abstract = {This study focuses on improving the performance of human activity recognition when a small number of sensor data are available under some special practical scenarios and resource-limited environments, such as some high-risk projects, anomaly monitoring and actual tactical scenarios. The Human Activity Recognition (HAR) based on wearable sensors is an attractive research topic in machine learning and ubiquitous computing over the last few decades, and has extremely practicality in health surveillance, medical assistance, personalized services, etc. However, with the limitation of sensor sampling rate, sustainability, deployment, and other restricted conditions, it is difficult to collect enough and resultful sensor data anywhere, anytime. Therefore, the HAR based on wearable sensors always faces the challenges of the low-data regime under some practical scenarios, which leads to a low accuracy of activity recognition and needs to be solved urgently. Currently, the Generative Adversarial Networks (GANs) provide a powerful method for training resultful generative models that could generate very convincing verisimilar images. The framework of GANs and its variants shed many lights on improving the performance of HAR. In this paper, we propose a new generative adversarial networks framework called SensoryGANs that can effectively generate available sensor data used for HAR. To the best of our knowledge, SensoryGANs is the first unbroken generative adversarial networks applied in generating sensor data in the HAR research field. Firstly, we tried exploring and devising three activity-special GANs models for three human daily activities. Secondly, these specific models are trained with the guidance of unbroken vanilla GANs. Thirdly, the trained generators from adversarial optimization process are used to generate synthetic sensor data. Finally, the synthetic sensor data from SensoryGANs are used to enrich the original authentic sensor datasets, which can improve the performance of target activity recognition model. Meanwhile, we propose three visual evaluation methods for assessing synthetic sensor data produced by the trained generators in SensoryGANs models. Experimental results show that SensoryGANs models have the capability of capturing the implicit distribution of real sensor data of human activity, and then the synthetic sensor data generated by SensoryGANs models Have a potential for improving human activity recognition.},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	author = {Wang, Jiwei and Chen, Yiqiang and Gu, Yang and Xiao, Yunlong and Pan, Haonan},
	year = {2018},
	note = {ISSN: 2161-4407},
	keywords = {Untagged},
	pages = {1--8},
}

@article{wangDeepVisualDomain2018,
	title = {Deep {Visual} {Domain} {Adaptation}: {A} {Survey}},
	volume = {312},
	issn = {09252312},
	shorttitle = {Deep {Visual} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1802.03601},
	doi = {10/gf6m39},
	abstract = {Deep domain adaptation has emerged as a new learning technique to address the lack of massive amounts of labeled data. Compared to conventional methods, which learn shared feature subspaces or reuse important source instances with shallow representations, deep domain adaptation methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning. There have been comprehensive surveys for shallow domain adaptation, but few timely reviews the emerging deep learning based methods. In this paper, we provide a comprehensive survey of deep domain adaptation methods for computer vision applications with four major contributions. First, we present a taxonomy of different deep domain adaptation scenarios according to the properties of data that deﬁne how two domains are diverged. Second, we summarize deep domain adaptation approaches into several categories based on training loss, and analyze and compare brieﬂy the state-of-the-art methods under these categories. Third, we overview the computer vision applications that go beyond image classiﬁcation, such as face recognition, semantic segmentation and object detection. Fourth, some potential deﬁciencies of current methods and several future directions are highlighted.},
	language = {en},
	urldate = {2021-06-10},
	journal = {Neurocomputing},
	author = {Wang, Mei and Deng, Weihong},
	month = oct,
	year = {2018},
	note = {arXiv: 1802.03601},
	keywords = {Untagged},
	pages = {135--153},
}

@inproceedings{wangHighResolutionImageSynthesis2018,
	address = {Salt Lake City, UT},
	title = {High-{Resolution} {Image} {Synthesis} and {Semantic} {Manipulation} {With} {Conditional} {GANs}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper.html},
	doi = {10/ggv75b},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
	year = {2018},
	keywords = {Untagged},
	pages = {8798--8807},
}

@inproceedings{wangNonlocalNeuralNetworks2018,
	address = {Salt Lake City, UT},
	title = {Non-local {Neural} {Networks}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578911/},
	doi = {10/gftc9c},
	abstract = {Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method [4] in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classiﬁcation, even without any bells and whistles, our nonlocal models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
	year = {2018},
	keywords = {Untagged},
	pages = {7794--7803},
}

@article{wangUnsupervisedDomainAdaptation2018,
	title = {Unsupervised {Domain} {Adaptation} with {Coupled} {Generative} {Adversarial} {Autoencoders}},
	volume = {8},
	issn = {2076-3417},
	url = {http://www.mdpi.com/2076-3417/8/12/2529},
	doi = {10/ghv3pf},
	abstract = {When large-scale annotated data are not available for certain image classiﬁcation tasks, training a deep convolutional neural network model becomes challenging. Some recent domain adaptation methods try to solve this problem using generative adversarial networks and have achieved promising results. However, these methods are based on a shared latent space assumption and they do not consider the situation when shared high level representations in different domains do not exist or are not ideal as they assumed. To overcome this limitation, we propose a neural network structure called coupled generative adversarial autoencoders (CGAA) that allows a pair of generators to learn the high-level differences between two domains by sharing only part of the high-level layers. Additionally, by introducing a class consistent loss calculated by a stand-alone classiﬁer into the generator optimization, our model is able to generate class invariant style-transferred images suitable for classiﬁcation tasks in domain adaptation. We apply CGAA to several domain transferred image classiﬁcation scenarios including several benchmark datasets. Experiment results have shown that our method can achieve state-of-the-art classiﬁcation results.},
	language = {en},
	number = {12},
	urldate = {2021-01-27},
	journal = {Applied Sciences},
	author = {Wang, Xiaoqing and Wang, Xiangjun},
	month = dec,
	year = {2018},
	keywords = {Untagged},
	pages = {2529},
}

@inproceedings{wangLookYouLeap2018,
	title = {Look {Before} {You} {Leap}: {Bridging} {Model}-{Free} and {Model}-{Based} {Reinforcement} {Learning} for {Planned}-{Ahead} {Vision}-and-{Language} {Navigation}},
	shorttitle = {Look {Before} {You} {Leap}},
	url = {https://arxiv.org/abs/1803.07729v2},
	abstract = {Existing research studies on vision and language grounding for robot navigation focus on improving model-free deep reinforcement learning (DRL) models in synthetic environments. However, model-free DRL models do not consider the dynamics in the real-world environments, and they often fail to generalize to new scenes. In this paper, we take a radical approach to bridge the gap between synthetic studies and real-world practices---We propose a novel, planned-ahead hybrid reinforcement learning model that combines model-free and model-based reinforcement learning to solve a real-world vision-language navigation task. Our look-ahead module tightly integrates a look-ahead policy model with an environment model that predicts the next state and the reward. Experimental results suggest that our proposed method significantly outperforms the baselines and achieves the best on the real-world Room-to-Room dataset. Moreover, our scalable method is more generalizable when transferring to unseen environments.},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {{ECCV}},
	author = {Wang, Xin and Xiong, Wenhan and Wang, Hongmin and Wang, William Yang},
	month = mar,
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{wangLearningDiscriminativeFilter2018,
	address = {Salt Lake City, UT},
	title = {Learning a {Discriminative} {Filter} {Bank} {Within} a {CNN} for {Fine}-{Grained} {Recognition}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578534/},
	doi = {10/ggzw7c},
	abstract = {Compared to earlier multistage frameworks using CNN features, recent end-to-end deep approaches for ﬁnegrained recognition essentially enhance the mid-level learning capability of CNNs. Previous approaches achieve this by introducing an auxiliary network to infuse localization information into the main classiﬁcation network, or a sophisticated feature encoding method to capture higher order feature statistics. We show that mid-level representation learning can be enhanced within the CNN framework, by learning a bank of convolutional ﬁlters that capture class-speciﬁc discriminative patches without extra part or bounding box annotations. Such a ﬁlter bank is well structured, properly initialized and discriminatively learned through a novel asymmetric multi-stream architecture with convolutional ﬁlter supervision and a non-random layer initialization. Experimental results show that our approach achieves state-of-the-art on three publicly available ﬁne-grained recognition datasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies and visualizations are provided to understand our approach.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Wang, Yaming and Morariu, Vlad I. and Davis, Larry S.},
	year = {2018},
	keywords = {Untagged},
	pages = {4148--4157},
}

@article{wangInferringClassRepresentatives2018,
	title = {Beyond {Inferring} {Class} {Representatives}: {User}-{Level} {Privacy} {Leakage} {From} {Federated} {Learning}},
	shorttitle = {Beyond {Inferring} {Class} {Representatives}},
	url = {http://arxiv.org/abs/1812.00535},
	abstract = {Federated learning, i.e., a mobile edge computing framework for deep learning, is a recent advance in privacy-preserving machine learning, where the model is trained in a decentralized manner by the clients, i.e., data curators, preventing the server from directly accessing those private data from the clients. This learning mechanism significantly challenges the attack from the server side. Although the state-of-the-art attacking techniques that incorporated the advance of Generative adversarial networks (GANs) could construct class representatives of the global data distribution among all clients, it is still challenging to distinguishably attack a specific client (i.e., user-level privacy leakage), which is a stronger privacy threat to precisely recover the private data from a specific client. This paper gives the first attempt to explore user-level privacy leakage against the federated learning by the attack from a malicious server. We propose a framework incorporating GAN with a multi-task discriminator, which simultaneously discriminates category, reality, and client identity of input samples. The novel discrimination on client identity enables the generator to recover user specified private data. Unlike existing works that tend to interfere the training process of the federated learning, the proposed method works "invisibly" on the server side. The experimental results demonstrate the effectiveness of the proposed attacking approach and the superior to the state-of-the-art.},
	language = {en},
	urldate = {2021-11-19},
	journal = {arXiv:1812.00535 [cs]},
	author = {Wang, Zhibo and Song, Mengkai and Zhang, Zhifei and Song, Yang and Wang, Qian and Qi, Hairong},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.00535},
	keywords = {Untagged},
}

@inproceedings{weiPersonTransferGAN2018,
	address = {Salt Lake City, UT},
	title = {Person {Transfer} {GAN} to {Bridge} {Domain} {Gap} for {Person} {Re}-{Identification}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Wei_Person_Transfer_GAN_CVPR_2018_paper.html},
	doi = {10/gf5bp5},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Wei, Longhui and Zhang, Shiliang and Gao, Wen and Tian, Qi},
	year = {2018},
	keywords = {Untagged},
	pages = {79--88},
}

@article{weiMaskCNNLocalizingParts2018,
	title = {Mask-{CNN}: {Localizing} parts and selecting descriptors for fine-grained image recognition},
	volume = {76},
	journal = {PR},
	author = {Wei, X.-S. and Xie, C.-W. and Wu, J.},
	year = {2018},
	keywords = {Untagged},
	pages = {704--714},
}

@inproceedings{weiImprovingImprovedTraining2018,
	address = {Vancouver, BC, Canada},
	title = {Improving the {Improved} {Training} of {Wasserstein} {GANs}: {A} {Consistency} {Term} and {Its} {Dual} {Effect}},
	url = {https://openreview.net/forum?id=SJx9GQb0-},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Wei, Xiang and Gong, Boqing and Liu, Zixia and Lu, Wei and Wang, Liqiang},
	year = {2018},
	keywords = {Untagged},
}

@article{weng2018attention,
	title = {Attention? {Attention}!},
	url = {http://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html},
	journal = {lilianweng.github.io/lil-log},
	author = {Weng, Lilian},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{wong_provable_2018,
	title = {Provable {Defenses} against {Adversarial} {Examples} via the {Convex} {Outer} {Adversarial} {Polytope}},
	urldate = {2022-09-09},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wong, Eric and Kolter, Zico},
	month = jul,
	year = {2018},
	keywords = {Untagged},
	pages = {5286--5295},
}

@inproceedings{wooCBAMConvolutionalBlock2018,
	address = {Munich, Germany},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{CBAM}: {Convolutional} {Block} {Attention} {Module}},
	volume = {11211},
	url = {https://doi.org/10.1007/978-3-030-01234-2_1},
	doi = {10/gfscb7},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Untagged},
	pages = {3--19},
}

@inproceedings{wuReinforcedCoTraining2018,
	title = {Reinforced {Co}-{Training}},
	url = {https://doi.org/10.18653/v1/n18-1113},
	doi = {10/gf3s5t},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL}-{HLT} 2018, {New} {Orleans}, {Louisiana}, {USA}, {June} 1-6, 2018, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Jiawei and Li, Lei and Wang, William Yang},
	editor = {Walker, Marilyn A. and Ji, Heng and Stent, Amanda},
	year = {2018},
	keywords = {Untagged},
	pages = {1252--1262},
}

@inproceedings{wuPrivacyPreservingVisualRecognition2018,
	address = {Munich, Germany},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Towards {Privacy}-{Preserving} {Visual} {Recognition} via {Adversarial} {Training}: {A} {Pilot} {Study}},
	volume = {11220},
	url = {https://doi.org/10.1007/978-3-030-01270-0_37},
	doi = {10/ghwng7},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Wu, Zhenyu and Wang, Zhangyang and Wang, Zhaowen and Jin, Hailin},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Untagged},
	pages = {627--645},
}

@inproceedings{xiDeepDilatedConvolution2018,
	title = {Deep {Dilated} {Convolution} on {Multimodality} {Time} {Series} for {Human} {Activity} {Recognition}},
	doi = {10.1109/IJCNN.2018.8489540},
	abstract = {Convolutional Neural Networks (CNNs) is capable of automatically learning feature representations, CNN-based recognition algorithm has been an alternative method for human activity recognition. Even though general convolution operation followed by pooling could expand the receptive fields for extracting features, it will bring about information loss in feature representation. Due to that dilated convolutions not only could expand receptive field exponentially without changing the size of field map or pooling, but it also will not cause information loss, hence, we propose D2CL, a novel deep learning framework for human activity recognition using multi-model wearable sensors. This framework consists of dilated convolutional neural networks and recurrent neural networks. At first, learning from previous works, we add a general convolutional layer to map inputs into a hidden space for improving the capability of nonlinear representations. Subsequently, a stacked dilated convolutional networks automatically learn feature representations for inter-sensors and intra-sensors from hidden space. Then, given these learned features, two RNNs are applied to model their latent temporal dependencies. Finally, a softmax classifier at the topmost layer is utilized to recognize activities. To evaluate the performance of D2CL on activity recognition, we select two open datasets OPPORTUNITY and PAMAP2 for training and testing. Results show that our proposed model achieves a higher classification performance than the state-of-the-art DeepConvLSTM.},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Xi, Rui and Hou, Mengshu and Fu, Mingsheng and Qu, Hong and Liu, Daibo},
	month = jul,
	year = {2018},
	note = {ISSN: 2161-4407},
	keywords = {Untagged},
	pages = {1--8},
}

@inproceedings{xiaoGeneratingAdversarialExamples2018,
	title = {Generating {Adversarial} {Examples} with {Adversarial} {Networks}},
	url = {https://doi.org/10.24963/ijcai.2018/543},
	doi = {10/gfx5xq},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence}, {IJCAI} 2018, {July} 13-19, 2018, {Stockholm}, {Sweden}},
	publisher = {ijcai.org},
	author = {Xiao, Chaowei and Li, Bo and Zhu, Jun-Yan and He, Warren and Liu, Mingyan and Song, Dawn},
	editor = {Lang, Jérôme},
	year = {2018},
	keywords = {Untagged},
	pages = {3905--3911},
}

@inproceedings{xie_mitigating_2018,
	title = {Mitigating {Adversarial} {Effects} {Through} {Randomization}},
	url = {https://openreview.net/forum?id=Sk9yuql0Z},
	abstract = {Convolutional neural networks have demonstrated high accuracy on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. For example, imperceptible...},
	language = {en},
	urldate = {2022-05-29},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Xie, Cihang and Wang, Jianyu and Zhang, Zhishuai and Ren, Zhou and Yuille, Alan},
	month = feb,
	year = {2018},
	keywords = {Untagged},
}

@article{xieDifferentiallyPrivateGenerative2018,
	title = {Differentially {Private} {Generative} {Adversarial} {Network}},
	url = {http://arxiv.org/abs/1802.06739},
	abstract = {Generative Adversarial Network (GAN) and its variants have recently attracted intensive research interests due to their elegant theoretical foundation and excellent empirical performance as generative models. These tools provide a promising direction in the studies where data availability is limited. One common issue in GANs is that the density of the learned generative distribution could concentrate on the training data points, meaning that they can easily remember training samples due to the high model complexity of deep networks. This becomes a major concern when GANs are applied to private or sensitive data such as patient medical records, and the concentration of distribution may divulge critical patient information. To address this issue, in this paper we propose a differentially private GAN (DPGAN) model, in which we achieve differential privacy in GANs by adding carefully designed noise to gradients during the learning procedure. We provide rigorous proof for the privacy guarantee, as well as comprehensive empirical evidence to support our analysis, where we demonstrate that our method can generate high quality data points at a reasonable privacy level.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1802.06739 [cs, stat]},
	author = {Xie, Liyang and Lin, Kaixiang and Wang, Shu and Wang, Fei and Zhou, Jiayu},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.06739},
	keywords = {Untagged},
}

@inproceedings{xinImprovingNeuralFineGrained2018,
	title = {Improving {Neural} {Fine}-{Grained} {Entity} {Typing} {With} {Knowledge} {Attention}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16321},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Xin, Ji and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong},
	editor = {McIlraith, Sheila A. and Weinberger, Kilian Q.},
	year = {2018},
	keywords = {Untagged},
	pages = {5997--6004},
}

@article{xingWalkSGD2018,
	title = {A {Walk} with {SGD}},
	url = {http://arxiv.org/abs/1802.08770},
	abstract = {We present novel empirical observations regarding how stochastic gradient descent (SGD) navigates the loss landscape of over-parametrized deep neural networks (DNNs). These observations expose the qualitatively different roles of learning rate and batch-size in DNN optimization and generalization. Speciﬁcally we study the DNN loss surface along the trajectory of SGD by interpolating the loss surface between parameters from consecutive iterations and tracking various metrics during training. We ﬁnd that the loss interpolation between parameters before and after each training iteration’s update is roughly convex with a minimum (valley ﬂoor) in between for most of the training. Based on this and other metrics, we deduce that for most of the training update steps, SGD moves in valley like regions of the loss surface by jumping from one valley wall to another at a height above the valley ﬂoor. This ’bouncing between walls at a height’ mechanism helps SGD traverse larger distance for small batch sizes and large learning rates which we ﬁnd play qualitatively different roles in the dynamics. While a large learning rate maintains a large height from the valley ﬂoor, a small batch size injects noise facilitating exploration. We ﬁnd this mechanism is crucial for generalization because the valley ﬂoor has barriers and this exploration above the valley ﬂoor allows SGD to quickly travel far away from the initialization point (without being affected by barriers) and ﬁnd ﬂatter regions, corresponding to better generalization.},
	language = {en},
	urldate = {2021-01-24},
	journal = {arXiv:1802.08770 [cs, stat]},
	author = {Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua},
	month = may,
	year = {2018},
	note = {arXiv: 1802.08770},
	keywords = {Untagged},
}

@inproceedings{xiuPoseFlowEfficient2018,
	title = {Pose {Flow}: {Efficient} {Online} {Pose} {Tracking}},
	url = {http://bmvc2018.org/contents/papers/0096.pdf},
	booktitle = {British {Machine} {Vision} {Conference} 2018, {BMVC} 2018, {Newcastle}, {UK}, {September} 3-6, 2018},
	publisher = {BMVA Press},
	author = {Xiu, Yuliang and Li, Jiefeng and Wang, Haoyu and Fang, Yinghong and Lu, Cewu},
	year = {2018},
	keywords = {Untagged},
	pages = {53},
}

@article{xuOnlineProductQuantization2018,
	title = {Online {Product} {Quantization}},
	issn = {1041-4347},
	url = {http://arxiv.org/abs/1711.10775},
	doi = {10/ghv43x},
	abstract = {Approximate nearest neighbor (ANN) search has achieved great success in many tasks. However, existing popular methods for ANN search, such as hashing and quantization methods, are designed for static databases only. They cannot handle well the database with data distribution evolving dynamically, due to the high computational effort for retraining the model based on the new database. In this paper, we address the problem by developing an online product quantization (online PQ) model and incrementally updating the quantization codebook that accommodates to the incoming streaming data. Moreover, to further alleviate the issue of large scale computation for the online PQ update, we design two budget constraints for the model to update partial PQ codebook instead of all. We derive a loss bound which guarantees the performance of our online PQ model. Furthermore, we develop an online PQ model over a sliding window with both data insertion and deletion supported, to reﬂect the real-time behaviour of the data. The experiments demonstrate that our online PQ model is both time-efﬁcient and effective for ANN search in dynamic large scale databases compared with baseline methods and the idea of partial PQ codebook update further reduces the update cost.},
	language = {en},
	urldate = {2021-01-27},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Xu, Donna and Tsang, Ivor W. and Zhang, Ying},
	year = {2018},
	note = {arXiv: 1711.10775},
	keywords = {Untagged},
	pages = {1--1},
}

@inproceedings{xuFinegrainedImageClassification2018,
	address = {Stockholm, Sweden},
	title = {Fine-grained {Image} {Classification} by {Visual}-{Semantic} {Embedding}},
	isbn = {978-0-9992411-2-7},
	url = {https://www.ijcai.org/proceedings/2018/145},
	doi = {10/ghv3tk},
	abstract = {This paper investigates a challenging problem, which is known as ﬁne-grained image classiﬁcation (FGIC). Different from conventional computer vision problems, FGIC suffers from the large intraclass diversities and subtle inter-class differences. Existing FGIC approaches are limited to explore only the visual information embedded in the images. In this paper, we present a novel approach which can use handy prior knowledge from either structured knowledge bases or unstructured text to facilitate FGIC. Speciﬁcally, we propose a visualsemantic embedding model which explores semantic embedding from knowledge bases and text, and further trains a novel end-to-end CNN framework to linearly map image features to a rich semantic embedding space. Experimental results on a challenging large-scale UCSD Bird-200-2011 dataset verify that our approach outperforms several stateof-the-art methods with signiﬁcant advances.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Xu, Huapeng and Qi, Guilin and Li, Jingjing and Wang, Meng and Xu, Kang and Gao, Huan},
	month = jul,
	year = {2018},
	keywords = {Untagged},
	pages = {1043--1049},
}

@inproceedings{xuHumanActivityRecognition2018,
	title = {Human {Activity} {Recognition} {Based} {On} {Convolutional} {Neural} {Network}},
	shorttitle = {{CNN1d}},
	doi = {10.1109/ICPR.2018.8545435},
	abstract = {Smartphones are ubiquitous and becoming increasingly sophisticated, with ever-growing sensing powers. Recent years, more and more applications of activity recognition based on sensors are developed for routine behavior monitoring and helping the users form a healthy habit. In this field, finding an efficient method of recognizing the physical activities (e.g., sitting, walking, jogging, etc) becomes the pivotal, core and urgent issue. In this study, we construct a Convolutional Neural Network (CNN) to identify human activities using the data collected from the three-axis accelerometer integrated in users' smartphones. The daily human activities that are chosen to be recognized include walking, jogging, sitting, standing, upstairs and downstairs. The three-dimensional (3D) raw accelerometer data is directly used as the input for training the CNN without any complex pretreatment. The performance of our CNN-based method for multi human activity recognition showed 91.97\% accuracy, which outperformed the Support Vector Machine (SVM) approach of 82.27\% trained and tested with six kinds of features extracted from the 3D raw accelerometer data. Therefore, our proposed approach achieved high recognition accuracy with low computational cost.},
	booktitle = {2018 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Xu, Wenchao and Pang, Yuxin and Yang, Yanqin and Liu, Yanbo},
	month = aug,
	year = {2018},
	note = {ISSN: 1051-4651},
	keywords = {Untagged},
	pages = {165--170},
}

@article{xuFirstorderStochasticAlgorithms2018,
	title = {First-order {Stochastic} {Algorithms} for {Escaping} {From} {Saddle} {Points} in {Almost} {Linear} {Time}},
	url = {http://arxiv.org/abs/1711.01944},
	abstract = {Two classes of methods have been proposed for escaping from saddle points with one using the second-order information carried by the Hessian and the other adding the noise into the first-order information. The existing analysis for algorithms using noise in the first-order information is quite involved and hides the essence of added noise, which hinder further improvements of these algorithms. In this paper, we present a novel perspective of noise-adding technique, i.e., adding the noise into the first-order information can help extract the negative curvature from the Hessian matrix, and provide a formal reasoning of this perspective by analyzing a simple first-order procedure. More importantly, the proposed procedure enables one to design purely first-order stochastic algorithms for escaping from non-degenerate saddle points with a much better time complexity (almost linear time in terms of the problem's dimensionality). In particular, we develop a \{{\textbackslash}bf first-order stochastic algorithm\} based on our new technique and an existing algorithm that only converges to a first-order stationary point to enjoy a time complexity of \{\vphantom{\}}\${\textbackslash}widetilde O(d/{\textbackslash}epsilon{\textasciicircum}\{3.5\})\$ for finding a nearly second-order stationary point \${\textbackslash}bf\{x\}\$ such that \${\textbackslash}{\textbar}{\textbackslash}nabla F(bf\{x\}){\textbackslash}{\textbar}{\textbackslash}leq {\textbackslash}epsilon\$ and \${\textbackslash}nabla{\textasciicircum}2 F(bf\{x\}){\textbackslash}geq -{\textbackslash}sqrt\{{\textbackslash}epsilon\}I\$ (in high probability), where \$F({\textbackslash}cdot)\$ denotes the objective function and \$d\$ is the dimensionality of the problem. To the best of our knowledge, this is the best theoretical result of first-order algorithms for stochastic non-convex optimization, which is even competitive with if not better than existing stochastic algorithms hinging on the second-order information.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1711.01944 [math, stat]},
	author = {Xu, Yi and Jin, Rong and Yang, Tianbao},
	month = mar,
	year = {2018},
	note = {arXiv: 1711.01944},
	keywords = {Untagged},
}

@inproceedings{yanParticipationContributedTemporalDynamic2018,
	address = {Seoul, Republic of Korea},
	title = {Participation-{Contributed} {Temporal} {Dynamic} {Model} for {Group} {Activity} {Recognition}},
	url = {https://doi.org/10.1145/3240508.3240572},
	doi = {10.1145/3240508.3240572},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Yan, Rui and Tang, Jinhui and Shu, Xiangbo and Li, Zechao and Tian, Qi},
	editor = {Boll, Susanne and Lee, Kyoung Mu and Luo, Jiebo and Zhu, Wenwu and Byun, Hyeran and Chen, Chang Wen and Lienhart, Rainer and Mei, Tao},
	year = {2018},
	keywords = {Untagged},
	pages = {1292--1300},
}

@article{yangPerceptualImageHashing2018,
	title = {Perceptual {Image} {Hashing} {Using} {Latent} {Low}-{Rank} {Representation} and {Uniform} {LBP}},
	volume = {8},
	issn = {2076-3417},
	url = {http://www.mdpi.com/2076-3417/8/2/317},
	doi = {10/ghv3j8},
	abstract = {Robustness and discriminability are the two most important features of perceptual image hashing (PIH) schemes. In order to achieve a good balance between perceptual robustness and discriminability, a novel PIH algorithm is proposed by combining latent low-rank representation (LLRR) and rotation invariant uniform local binary patterns (RiuLBP). LLRR is ﬁrst applied on resized original images to the principal feature matrix and to the salient feature matrix, since it can automatically extract salient features from corrupted images. Following this, Riulocal bin features are extracted from each non-overlapping block of the principal feature matrix and of the salient feature matrix, respectively. All features are concatenated and scrambled to generate ﬁnal binary hash code. Experimental results show that the proposed hashing algorithm is robust against many types of distortions and attacks, such as noise addition, low-pass ﬁltering, rotation, scaling, and JPEG compression. It outperforms other local binary patterns (LBP) based image hashing schemes in terms of perceptual robustness and discriminability.},
	language = {en},
	number = {2},
	urldate = {2021-01-27},
	journal = {Applied Sciences},
	author = {Yang, Hengfu and Yin, Jianping and Jiang, Mingfang},
	month = feb,
	year = {2018},
	keywords = {Untagged},
	pages = {317},
}

@inproceedings{yangLearningFaceAge2018,
	address = {Salt Lake City, UT},
	title = {Learning {Face} {Age} {Progression}: {A} {Pyramid} {Architecture} of {GANs}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Learning_Face_Age_CVPR_2018_paper.html},
	doi = {10/ghwnnv},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Yang, Hongyu and Huang, Di and Wang, Yunhong and Jain, Anil K.},
	year = {2018},
	keywords = {Untagged},
	pages = {31--39},
}

@article{yangSupervisedLearningSemanticsPreserving2018,
	title = {Supervised {Learning} of {Semantics}-{Preserving} {Hash} via {Deep} {Convolutional} {Neural} {Networks}},
	volume = {40},
	issn = {0162-8828, 2160-9292},
	url = {http://arxiv.org/abs/1507.00101},
	doi = {10/gcvzjs},
	abstract = {This paper presents a simple yet effective supervised deep hash approach that constructs binary hash codes from labeled data for large-scale image search. We assume that the semantic labels are governed by several latent attributes with each attribute on or off, and classiﬁcation relies on these attributes. Based on this assumption, our approach, dubbed supervised semantics-preserving deep hashing (SSDH), constructs hash functions as a latent layer in a deep network and the binary codes are learned by minimizing an objective function deﬁned over classiﬁcation error and other desirable hash codes properties. With this design, SSDH has a nice characteristic that classiﬁcation and retrieval are uniﬁed in a single learning model. Moreover, SSDH performs joint learning of image representations, hash codes, and classiﬁcation in a point-wised manner, and thus is scalable to large-scale datasets. SSDH is simple and can be realized by a slight enhancement of an existing deep architecture for classiﬁcation; yet it is effective and outperforms other hashing approaches on several benchmarks and large datasets. Compared with state-of-the-art approaches, SSDH achieves higher retrieval accuracy, while the classiﬁcation performance is not sacriﬁced.},
	language = {en},
	number = {2},
	urldate = {2021-01-27},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Yang, Huei-Fang and Lin, Kevin and Chen, Chu-Song},
	month = feb,
	year = {2018},
	note = {arXiv: 1507.00101},
	keywords = {Untagged},
	pages = {437--451},
}

@inproceedings{yangConvolutionalNeuralNetworks2018,
	address = {Salt Lake City, UT},
	title = {Convolutional {Neural} {Networks} {With} {Alternately} {Updated} {Clique}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Convolutional_Neural_Networks_CVPR_2018_paper.html},
	doi = {10/ghwnj6},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Yang, Yibo and Zhong, Zhisheng and Shen, Tiancheng and Lin, Zhouchen},
	year = {2018},
	keywords = {Untagged},
	pages = {2413--2422},
}

@inproceedings{pmlr-v84-yang18c,
	series = {Proceedings of machine learning research},
	title = {Dimensionality reduced ⁰-{Sparse} subspace clustering},
	volume = {84},
	url = {https://proceedings.mlr.press/v84/yang18c.html},
	abstract = {Subspace clustering partitions the data that lie on a union of subspaces. ⁰-Sparse Subspace Clustering (⁰-SSC), which belongs to the subspace clustering methods with sparsity prior, guarantees the correctness of subspace clustering under less restrictive assumptions compared to its ¹ counterpart such as Sparse Subspace Clustering (SSC, Elhamifar et al., 2013) with demonstrated effectiveness in practice. In this paper, we present Dimensionality Reduced ⁰-Sparse Subspace Clustering (DR-⁰-SSC). DR-⁰-SSC first projects the data onto a lower dimensional space by linear transformation, then performs ⁰-SSC on the dimensionality reduced data. The correctness of DR-⁰-SSC in terms of the subspace detection property is proved, therefore DR-⁰-SSC recovers the underlying subspace structure in the original data from the dimensionality reduced data. Experimental results demonstrate the effectiveness of DR-⁰-SSC.},
	booktitle = {Proceedings of the twenty-first international conference on artificial intelligence and statistics},
	publisher = {PMLR},
	author = {Yang, Yingzhen},
	editor = {Storkey, Amos and Perez-Cruz, Fernando},
	month = apr,
	year = {2018},
	note = {tex.pdf: http://proceedings.mlr.press/v84/yang18c/yang18c.pdf},
	keywords = {Untagged},
	pages = {2065--2074},
}

@inproceedings{yangLearningNavigateFinegrained2018,
	title = {Learning to {Navigate} for {Fine}-grained {Classification}},
	doi = {10/ghqpgs},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Yang, Ze and Luo, Tiange and Wang, Dong and Hu, Zhiqiang and Gao, Jun and Wang, Liwei},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{yaoHumanBehaviorUnderstanding2018,
	address = {Seoul Republic of Korea},
	title = {Human {Behavior} {Understanding}: {From} {Action} {Recognition} to {Complex} {Event} {Detection}},
	isbn = {978-1-4503-5665-7},
	shorttitle = {Human {Behavior} {Understanding}},
	url = {https://dl.acm.org/doi/10.1145/3240508.3241474},
	doi = {10/gkx7rd},
	abstract = {Analyzing human behaviour in videos is one of the fundamental problems of computer vision and multimedia understanding. The task is very challenging as video is an information-intensive media with large variations and complexities in content. With the development of deep learning techniques, researchers have strived to push the limits of human behaviour understanding in a wide variety of applications from action recognition to event detection. This tutorial will present recent advances under the umbrella of human behaviour understanding, which range from the fundamental problem of how to learn “good” video representations, to the challenges of categorizing video content into human action classes, finally to multimedia event detection and surveillance event detection in complex scenarios.},
	language = {en},
	urldate = {2021-06-10},
	booktitle = {Proceedings of the 26th {ACM} international conference on {Multimedia}},
	publisher = {ACM},
	author = {Yao, Ting and Liu, Jingen},
	month = oct,
	year = {2018},
	keywords = {Untagged},
	pages = {2104--2105},
}

@inproceedings{yazdaniLinearBackpropNonlinear2018,
	address = {Vancouver, BC, Canada},
	title = {Linear {Backprop} in non-linear networks},
	url = {https://openreview.net/forum?id=ByfPDyrYim},
	abstract = {Backprop is the primary learning algorithm used in many machine learning algorithms. In practice, however, Backprop in deep neural networks is a highly sensitive learning algorithm and successful learning depends on numerous conditions and constraints. One set of constraints is to avoid weights that lead to saturated units. The motivation for avoiding unit saturation is that gradients vanish and as a result learning comes to a halt. Careful weight initialization and re-scaling schemes such as batch normalization ensure that input activity to the neuron is within the linear regime where gradients are not vanished and can ﬂow. Here we investigate backpropagating error terms only linearly. That is, we ignore the saturation that arise by ensuring gradients always ﬂow. We refer to this learning rule as Linear Backprop since in the backward pass the network appears to be linear. In addition to ensuring persistent gradient ﬂow, Linear Backprop is also favorable when computation is expensive since gradients are never computed. Our early results suggest that learning with Linear Backprop is competitive with Backprop and saves expensive gradient computations.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Yazdani, Mehrdad},
	year = {2018},
	keywords = {Untagged},
	pages = {6},
}

@article{yeomPrivacyRiskMachine2018,
	title = {Privacy {Risk} in {Machine} {Learning}: {Analyzing} the {Connection} to {Overfitting}},
	shorttitle = {Privacy {Risk} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1709.01604},
	abstract = {Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.},
	urldate = {2022-03-21},
	journal = {arXiv:1709.01604 [cs, stat]},
	author = {Yeom, Samuel and Giacomelli, Irene and Fredrikson, Matt and Jha, Somesh},
	month = may,
	year = {2018},
	note = {arXiv: 1709.01604},
	keywords = {Untagged},
}

@article{yinBinaryRelaxRelaxationApproach2018,
	title = {{BinaryRelax}: {A} {Relaxation} {Approach} for {Training} {Deep} {Neural} {Networks} with {Quantized} {Weights}},
	volume = {11},
	shorttitle = {{BinaryRelax}},
	url = {https://epubs.siam.org/doi/10.1137/18M1166134},
	doi = {10.1137/18M1166134},
	abstract = {We propose BinaryRelax, a simple two-phase algorithm, for training deep neural networks with quantized weights. The set constraint that characterizes the quantization of weights is not imposed until the late stage of training, and a sequence of pseudo quantized weights is maintained. Specifically, we relax the hard constraint into a continuous regularizer via a Moreau envelope, which turns out to be the squared Euclidean distance to the set of quantized weights. The pseudo quantized weights are obtained by linearly interpolating between the float weights and their quantizations. A continuation strategy is adopted to push the weights toward the quantized state by gradually increasing the regularization parameter. In the second phase, an exact quantization scheme with a small learning rate is invoked to guarantee fully quantized weights. We test BinaryRelax on the benchmark CIFAR and ImageNet color image datasets to demonstrate the superiority of the relaxed quantization approach and the improved accuracy over the state-of-the-art training methods. Finally, we prove the convergence of BinaryRelax under an approximate orthogonality condition.},
	number = {4},
	urldate = {2022-02-28},
	journal = {SIAM Journal on Imaging Sciences},
	author = {Yin, Penghang and Zhang, Shuai and Lyu, Jiancheng and Osher, Stanley and Qi, Yingyong and Xin, Jack},
	year = {2018},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {Untagged},
	pages = {2205--2223},
}

@article{yoonLifelongLearningDynamically2018,
	title = {Lifelong {Learning} with {Dynamically} {Expandable} {Networks}},
	url = {http://arxiv.org/abs/1708.01547},
	abstract = {We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efﬁciently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets under lifelong learning scenarios, on which it not only signiﬁcantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch counterparts with substantially fewer number of parameters. Further, the obtained network ﬁne-tuned on all tasks obtained signiﬁcantly better performance over the batch models, which shows that it can be used to estimate the optimal network structure even when all tasks are available in the ﬁrst place.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1708.01547 [cs]},
	author = {Yoon, Jaehong and Yang, Eunho and Lee, Jeongtae and Hwang, Sung Ju},
	month = jun,
	year = {2018},
	note = {arXiv: 1708.01547},
	keywords = {Untagged},
}

@inproceedings{yoonRadialGANLeveragingMultiple2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{RadialGAN}: {Leveraging} multiple datasets to improve target-specific predictive models using {Generative} {Adversarial} {Networks}},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/yoon18b.html},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018, {Stockholmsmässan}, {Stockholm}, {Sweden}, {July} 10-15, 2018},
	publisher = {PMLR},
	author = {Yoon, Jinsung and Jordon, James and Schaar, Mihaela van der},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	keywords = {Untagged},
	pages = {5685--5693},
}

@inproceedings{yuDeepLayerAggregation2018,
	address = {Salt Lake City, UT},
	title = {Deep {Layer} {Aggregation}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{DLA}},
	url = {https://ieeexplore.ieee.org/document/8578353/},
	doi = {10/gf5zxs},
	abstract = {Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from ﬁne to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been “shallow” themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes.},
	language = {en},
	urldate = {2021-06-28},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Yu, Fisher and Wang, Dequan and Shelhamer, Evan and Darrell, Trevor},
	year = {2018},
	keywords = {Untagged},
	pages = {2403--2412},
}

@inproceedings{yuFineGrainedVideoCaptioning2018,
	address = {Salt Lake City, UT},
	title = {Fine-{Grained} {Video} {Captioning} for {Sports} {Narrative}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578727/},
	doi = {10/ghq56f},
	abstract = {Despite recent emergence of video caption methods, how to generate ﬁne-grained video descriptions (i.e., long and detailed commentary about individual movements of multiple subjects as well as their frequent interactions) is far from being solved, which however has great applications such as automatic sports narrative. To this end, this work makes the following contributions. First, to facilitate this novel research of ﬁne-grained video caption, we collected a novel dataset called Fine-grained Sports Narrative dataset (FSN) that contains 2K sports videos with ground-truth narratives from YouTube.com. Second, we develop a novel performance evaluation metric named Fine-grained Captioning Evaluation (FCE) to cope with this novel task. Considered as an extension of the widely used METEOR, it measures not only the linguistic performance but also whether the action details and their temporal orders are correctly described. Third, we propose a new framework for ﬁnegrained sports narrative task. This network features three branches: 1) a spatio-temporal entity localization and role discovering sub-network; 2) a ﬁne-grained action modeling sub-network for local skeleton motion description; and 3) a group relationship modeling sub-network to model interactions between players. We further fuse the features and decode them into long narratives by a hierarchically recurrent structure. Extensive experiments on the FSN dataset demonstrates the validity of the proposed framework for ﬁne-grained video caption.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Yu, Huanyu and Cheng, Shuo and Ni, Bingbing and Wang, Minsi and Zhang, Jian and Yang, Xiaokang},
	year = {2018},
	keywords = {Untagged},
	pages = {6006--6015},
}

@inproceedings{yuPUNetPointCloud2018,
	address = {Salt Lake City, UT},
	title = {{PU}-{Net}: {Point} {Cloud} {Upsampling} {Network}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Yu_PU-Net_Point_Cloud_CVPR_2018_paper.html},
	doi = {10/ggn9dk},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Yu, Lequan and Li, Xianzhi and Fu, Chi-Wing and Cohen-Or, Daniel and Heng, Pheng-Ann},
	year = {2018},
	keywords = {Untagged},
	pages = {2790--2799},
}

@inproceedings{NEURIPS2018_907edb0a,
	title = {Compact generalized non-local network},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/file/907edb0aa6986220dbffb79a788596ee-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Yue, Kaiyu and Sun, Ming and Yuan, Yuchen and Zhou, Feng and Ding, Errui and Xu, Fuxin},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{ferrari_cross-modal_2018,
	address = {Cham},
	title = {Cross-{Modal} and {Hierarchical} {Modeling} of {Video} and {Text}},
	volume = {11217},
	isbn = {978-3-030-01260-1 978-3-030-01261-8},
	shorttitle = {{FSE}},
	url = {https://link.springer.com/10.1007/978-3-030-01261-8_23},
	abstract = {Visual data and text data are composed of information at multiple granularities. A video can describe a complex scene that is composed of multiple clips or shots, where each depicts a semantically coherent event or action. Similarly, a paragraph may contain sentences with diﬀerent topics, which collectively conveys a coherent message or story. In this paper, we investigate the modeling techniques for such hierarchical sequential data where there are correspondences across multiple modalities. Speciﬁcally, we introduce hierarchical sequence embedding (hse), a generic model for embedding sequential data of diﬀerent modalities into hierarchically semantic spaces, with either explicit or implicit correspondence information. We perform empirical studies on large-scale video and paragraph retrieval datasets and demonstrated superior performance by the proposed methods. Furthermore, we examine the eﬀectiveness of our learned embeddings when applied to downstream tasks. We show its utility in zero-shot action recognition and video captioning.},
	language = {en},
	urldate = {2023-05-30},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Zhang, Bowen and Hu, Hexiang and Sha, Fei},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01261-8_23},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {385--401},
}

@article{zhangFindingBetterTopologies2018,
	title = {Finding {Better} {Topologies} for {Deep} {Convolutional} {Neural} {Networks} by {Evolution}},
	url = {http://arxiv.org/abs/1809.03242},
	abstract = {Due to the nonlinearity of artiﬁcial neural networks, designing topologies for deep convolutional neural networks (CNN) is a challenging task and often only heuristic approach, such as trial and error, can be applied. Evolutionary algorithm can solve optimization problems where the ﬁtness landscape is unknown. However, evolutionary algorithm is computing resource intensive, which makes it difﬁcult for problems when deep CNNs are involved. In this paper we propose an evolutionary strategy to ﬁnd better topologies for deep CNNs. Incorporating the concept of knowledge inheritance and knowledge learning, our evolutionary algorithm can be executed with limited computing resources. We applied the proposed algorithm in ﬁnding effective topologies of deep CNNs for the image classiﬁcation task using CIFAR10 dataset. After the evolution, we analyzed the topologies that performed well for this task. Our studies verify the techniques that have been commonly used in human designed deep CNNs. We also discovered that some of the graph properties greatly affect the system performance. We applied the guidelines learned from the evolution and designed new network topologies that outperform Residual Net with less layers on CIFAR-10, CIFAR100 and SVHN dataset.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1809.03242 [cs]},
	author = {Zhang, Honglei and Kiranyaz, Serkan and Gabbouj, Moncef},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.03242},
	keywords = {Untagged},
}

@article{zhangMedicalMissingData2018,
	title = {Medical missing data imputation by stackelberg gan},
	journal = {Carnegie Mellon University},
	author = {Zhang, Hongyang and Woodruff, David P.},
	year = {2018},
	keywords = {Untagged},
}

@article{zhangStackelbergGanProvable2018,
	title = {Stackelberg gan: {Towards} provable minimax equilibrium via multi-generator architectures},
	shorttitle = {Stackelberg gan},
	journal = {arXiv preprint arXiv:1811.08010},
	author = {Zhang, Hongyang and Xu, Susu and Jiao, Jiantao and Xie, Pengtao and Salakhutdinov, Ruslan and Xing, Eric P.},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{zhangAudioVisualAttribute2018,
	title = {Audio {Visual} {Attribute} {Discovery} for {Fine}-{Grained} {Object} {Recognition}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16740},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Zhang, Hua and Cao, Xiaochun and Wang, Rui},
	editor = {McIlraith, Sheila A. and Weinberger, Kilian Q.},
	year = {2018},
	keywords = {Untagged},
	pages = {7542--7549},
}

@article{zhangSCHGANSemisupervisedCrossmodal2018,
	title = {{SCH}-{GAN}: {Semi}-supervised {Cross}-modal {Hashing} by {Generative} {Adversarial} {Network}},
	shorttitle = {{SCH}-{GAN}},
	url = {http://arxiv.org/abs/1802.02488},
	abstract = {Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space, which can realize fast and ﬂexible retrieval across different modalities. Supervised cross-modal hashing methods have achieved considerable progress by incorporating semantic side information. However, they mainly have two limitations: (1) Heavily rely on large-scale labeled cross-modal training data which are labor intensive and hard to obtain, since multiple modalities are involved. (2) Ignore the rich information contained in the large amount of unlabeled data across different modalities, especially the margin examples from another modality that are easily to be incorrectly retrieved, which can help to model the correlations between different modalities. To address these problems, in this paper we propose a novel Semi-supervised Cross-Modal Hashing approach by Generative Adversarial Network (SCH-GAN). We aim to take advantage of GAN’s ability for modeling data distributions, so that SCH-GAN can model the distribution across different modalities, and select informative margin examples from large amount of unlabeled data to promote cross-modal hashing learning in an adversarial way. The main contributions can be summarized as follows: (1) We propose a novel generative adversarial network for cross-modal hashing. In our proposed SCH-GAN, the generative model tries to select margin examples of one modality from unlabeled data when giving a query of another modality (e.g. giving a text query to retrieve images and vice versa). While the discriminative model tries to distinguish the selected examples and true positive examples of the query. These two models play a minimax game so that the generative model can promote the hashing performance of discriminative model. (2) We propose a reinforcement learning based algorithm to drive the training of proposed SCH-GAN. The generative model takes the correlation score predicted by discriminative model as a reward, and tries to select the examples close to the margin to promote discriminative model by maximizing the margin between positive and negative data. Extensive experiments compared with 8 state-of-the-art methods on 3 widely-used datasets verify the effectiveness of our proposed approach.},
	language = {en},
	urldate = {2020-11-12},
	journal = {arXiv:1802.02488 [cs]},
	author = {Zhang, Jian and Peng, Yuxin and Yuan, Mingkuan},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.02488},
	keywords = {Untagged},
}

@inproceedings{zhangSparselyGroupedMultiTask2018,
	title = {Sparsely {Grouped} {Multi}-{Task} {Generative} {Adversarial} {Networks} for {Facial} {Attribute} {Manipulation}},
	url = {https://doi.org/10.1145/3240508.3240594},
	doi = {10/ghwnpp},
	booktitle = {2018 {ACM} {Multimedia} {Conference} on {Multimedia} {Conference}, {MM} 2018, {Seoul}, {Republic} of {Korea}, {October} 22-26, 2018},
	publisher = {ACM},
	author = {Zhang, Jichao and Shu, Yezhi and Xu, Songhua and Cao, Gongze and Zhong, Fan and Liu, Meng and Qin, Xueying},
	editor = {Boll, Susanne and Lee, Kyoung Mu and Luo, Jiebo and Zhu, Wenwu and Byun, Hyeran and Chen, Chang Wen and Lienhart, Rainer and Mei, Tao},
	year = {2018},
	keywords = {Untagged},
	pages = {392--401},
}

@inproceedings{zhangImportanceWeightedAdversarial2018,
	address = {Salt Lake City, UT},
	title = {Importance {Weighted} {Adversarial} {Nets} for {Partial} {Domain} {Adaptation}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578949/},
	doi = {10/gg3tqn},
	abstract = {This paper proposes an importance weighted adversarial nets-based method for unsupervised domain adaptation, speciﬁc for partial domain adaptation where the target domain has less number of classes compared to the source domain. Previous domain adaptation methods generally assume the identical label spaces, such that reducing the distribution divergence leads to feasible knowledge transfer. However, such an assumption is no longer valid in a more realistic scenario that requires adaptation from a larger and more diverse source domain to a smaller target domain with less number of classes. This paper extends the adversarial nets-based domain adaptation and proposes a novel adversarial nets-based partial domain adaptation method to identify the source samples that are potentially from the outlier classes and, at the same time, reduce the shift of shared classes between domains.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhang, Jing and Ding, Zewei and Li, Wanqing and Ogunbona, Philip},
	year = {2018},
	keywords = {Untagged},
	pages = {8156--8164},
}

@inproceedings{zhangAdaptiveOnlineLearning2018,
	title = {Adaptive {Online} {Learning} in {Dynamic} {Environments}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zhang, Lijun and Lu, Shiyin and Zhou, Zhi-Hua},
	year = {2018},
	keywords = {Untagged},
	pages = {1323--1333},
}

@inproceedings{zhangInterpretableConvolutionalNeural2018,
	title = {Interpretable {Convolutional} {Neural} {Networks}},
	doi = {10/ggcb4r},
	abstract = {This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable CNN, in order to clarify knowledge representations in high conv-layers of the CNN. In an interpretable CNN, each filter in a high conv-layer represents a specific object part. Our interpretable CNNs use the same training data as ordinary CNNs without a need for any annotations of object parts or textures for supervision. The interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. We can apply our method to different types of CNNs with various structures. The explicit knowledge representation in an interpretable CNN can help people understand the logic inside a CNN, i.e. what patterns are memorized by the CNN for prediction. Experiments have shown that filters in an interpretable CNN are more semantically meaningful than those in a traditional CNN. The code is available at https://github.com/zqs1022/interpretableCNN.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhang, Quanshi and Wu, Ying Nian and Zhu, Song-Chun},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {Untagged},
	pages = {8827--8836},
}

@article{zhangPrivacypreservingMachineLearning2018,
	title = {Privacy-preserving {Machine} {Learning} through {Data} {Obfuscation}},
	url = {http://arxiv.org/abs/1807.01860},
	abstract = {As machine learning becomes a practice and commodity, numerous cloud-based services and frameworks are provided to help customers develop and deploy machine learning applications. While it is prevalent to outsource model training and serving tasks in the cloud, it is important to protect the privacy of sensitive samples in the training dataset and prevent information leakage to untrusted third parties. Past work have shown that a malicious machine learning service provider or end user can easily extract critical information about the training samples, from the model parameters or even just model outputs. In this paper, we propose a novel and generic methodology to preserve the privacy of training data in machine learning applications. Specifically we introduce an obfuscate function and apply it to the training data before feeding them to the model training task. This function adds random noise to existing samples, or augments the dataset with new samples. By doing so sensitive information about the properties of individual samples, or statistical properties of a group of samples, is hidden. Meanwhile the model trained from the obfuscated dataset can still achieve high accuracy. With this approach, the customers can safely disclose the data or models to third-party providers or end users without the need to worry about data privacy. Our experiments show that this approach can effective defeat four existing types of machine learning privacy attacks at negligible accuracy cost.},
	urldate = {2022-03-21},
	journal = {arXiv:1807.01860 [cs]},
	author = {Zhang, Tianwei and He, Zecheng and Lee, Ruby B.},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.01860},
	keywords = {Untagged},
}

@inproceedings{zhangCollaborativeAdversarialNetwork2018,
	address = {Salt Lake City, UT},
	title = {Collaborative and {Adversarial} {Network} for {Unsupervised} {Domain} {Adaptation}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578498/},
	doi = {10/gfx3c8},
	abstract = {In this paper, we propose a new unsupervised domain adaptation approach called Collaborative and Adversarial Network (CAN) through domain-collaborative and domainadversarial training of neural networks. We add several domain classiﬁers on multiple CNN feature extraction blocks1, in which each domain classiﬁer is connected to the hidden representations from one block and one loss function is deﬁned based on the hidden presentation and the domain labels (e.g., source and target). We design a new loss function by integrating the losses from all blocks in order to learn domain informative representations from lower blocks through collaborative learning and learn domain uninformative representations from higher blocks through adversarial learning. We further extend our CAN method as Incremental CAN (iCAN), in which we iteratively select a set of pseudolabelled target samples based on the image classiﬁer and the last domain classiﬁer from the previous training epoch and re-train our CAN model by using the enlarged training set. Comprehensive experiments on two benchmark datasets Ofﬁce and ImageCLEF-DA clearly demonstrate the effectiveness of our newly proposed approaches CAN and iCAN for unsupervised domain adaptation.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhang, Weichen and Ouyang, Wanli and Li, Wen and Xu, Dong},
	year = {2018},
	keywords = {Untagged},
	pages = {3801--3809},
}

@inproceedings{zhangShuffleNetExtremelyEfficient2018,
	address = {Salt Lake City, UT},
	title = {{ShuffleNet}: {An} {Extremely} {Efficient} {Convolutional} {Neural} {Network} for {Mobile} {Devices}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.html},
	doi = {10/gf2v49},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
	year = {2018},
	keywords = {Untagged},
	pages = {6848--6856},
}

@inproceedings{zhangZigzagLearningWeakly2018,
	address = {Salt Lake City, UT},
	title = {Zigzag {Learning} for {Weakly} {Supervised} {Object} {Detection}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578546/},
	doi = {10/ghv3q6},
	abstract = {This paper addresses weakly supervised object detection with only image-level supervision at training stage. Previous approaches train detection models with entire images all at once, making the models prone to being trapped in sub-optimums due to the introduced false positive examples. Unlike them, we propose a zigzag learning strategy to simultaneously discover reliable object instances and prevent the model from overﬁtting initial seeds. Towards this goal, we ﬁrst develop a criterion named mean Energy Accumulation Scores (mEAS) to automatically measure and rank localization difﬁculty of an image containing the target object, and accordingly learn the detector progressively by feeding examples with increasing difﬁculty. In this way, the model can be well prepared by training on easy examples for learning from more difﬁcult ones and thus gain a stronger detection ability more efﬁciently. Furthermore, we introduce a novel masking regularization strategy over the high level convolutional feature maps to avoid overﬁtting initial samples. These two modules formulate a zigzag learning process, where progressive learning endeavors to discover reliable object instances, and masking regularization increases the difﬁculty of ﬁnding object instances properly. We achieve 47.6\% mAP on PASCAL VOC 2007, surpassing the state-of-the-arts by a large margin.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhang, Xiaopeng and Feng, Jiashi and Xiong, Hongkai and Tian, Qi},
	year = {2018},
	keywords = {Untagged},
	pages = {4262--4270},
}

@inproceedings{zhangFineGrainedVisualCategorization2018,
	address = {Munich, Germany},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Fine-{Grained} {Visual} {Categorization} {Using} {Meta}-learning {Optimization} with {Sample} {Selection} of {Auxiliary} {Data}},
	volume = {11212},
	url = {https://doi.org/10.1007/978-3-030-01237-3_15},
	doi = {10/ghwnhg},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Zhang, Yabin and Tang, Hui and Jia, Kui},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Untagged},
	pages = {241--256},
}

@article{zhangOverviewMultitaskLearning2018,
	title = {An overview of multi-task learning},
	volume = {5},
	issn = {2095-5138, 2053-714X},
	url = {https://academic.oup.com/nsr/article/5/1/30/4101432},
	doi = {10/gc897k},
	abstract = {As a promising area in machine learning, multi-task learning (MTL) aims to improve the performance of multiple related learning tasks by leveraging useful information among them. In this paper, we give an overview of MTL by first giving a definition of MTL. Then several different settings of MTL are introduced, including multi-task supervised learning, multi-task unsupervised learning, multi-task semi-supervised learning, multi-task active learning, multi-task reinforcement learning, multi-task online learning and multi-task multi-view learning. For each setting, representative MTL models are presented. In order to speed up the learning process, parallel and distributed MTL models are introduced. Many areas, including computer vision, bioinformatics, health informatics, speech, natural language processing, web applications and ubiquitous computing, use MTL to improve the performance of the applications involved and some representative works are reviewed. Finally, recent theoretical analyses for MTL are presented.},
	language = {en},
	number = {1},
	urldate = {2021-01-24},
	journal = {National Science Review},
	author = {Zhang, Yu and Yang, Qiang},
	month = jan,
	year = {2018},
	keywords = {Untagged},
	pages = {30--43},
}

@inproceedings{zhaoInPrivateDiggingEnabling2018,
	address = {Honolulu, HI},
	title = {{InPrivate} {Digging}: {Enabling} {Tree}-based {Distributed} {Data} {Mining} with {Differential} {Privacy}},
	url = {https://doi.org/10.1109/INFOCOM.2018.8486352},
	doi = {10/ghdxkw},
	booktitle = {{IEEE} {Conference} on {Computer} {Communications}},
	publisher = {IEEE},
	author = {Zhao, Lingchen and Ni, Lihao and Hu, Shengshan and Chen, Yanjiao and Zhou, Pan and Xiao, Fu and Wu, Libing},
	year = {2018},
	keywords = {Untagged},
	pages = {2087--2095},
}

@article{zhaoProximalOnlineGradient2018,
	title = {Proximal {Online} {Gradient} is {Optimum} for {Dynamic} {Regret}},
	volume = {abs/1810.03594},
	url = {http://arxiv.org/abs/1810.03594},
	urldate = {2022-02-25},
	journal = {CoRR},
	author = {Zhao, Yawei and Qiu, Shuang and Liu, Ji},
	year = {2018},
	note = {arXiv: 1810.03594},
	keywords = {Untagged},
}

@inproceedings{zhaoAdversarialApproachHard2018,
	address = {Cham},
	title = {An {Adversarial} {Approach} to {Hard} {Triplet} {Generation}},
	volume = {11213},
	isbn = {978-3-030-01239-7 978-3-030-01240-3},
	url = {http://link.springer.com/10.1007/978-3-030-01240-3_31},
	abstract = {While deep neural networks have demonstrated competitive results for many visual recognition and image retrieval tasks, the major challenge lies in distinguishing similar images from diﬀerent categories (i.e., hard negative examples) while clustering images with large variations from the same category (i.e., hard positive examples). The current state-of-the-art is to mine the most hard triplet examples from the minibatch to train the network. However, mining-based methods tend to look into these triplets that are hard in terms of the current estimated network, rather than deliberately generating those hard triplets that really matter in globally optimizing the network. For this purpose, we propose an adversarial network for Hard Triplet Generation (HTG) to optimize the network ability in distinguishing similar examples of diﬀerent categories as well as grouping varied examples of the same categories. We evaluate our method on the real-world challenging datasets, such as CUB200-2011, CARS196, DeepFashion and VehicleID datasets, and show that our method outperforms the state-of-the-art methods signiﬁcantly.},
	language = {en},
	urldate = {2021-06-30},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Zhao, Yiru and Jin, Zhongming and Qi, Guo-jun and Lu, Hongtao and Hua, Xian-sheng},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01240-3_31},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {508--524},
}

@inproceedings{zhengCentralizedRankingLoss2018,
	address = {Stockholm, Sweden},
	title = {Centralized {Ranking} {Loss} with {Weakly} {Supervised} {Localization} for {Fine}-{Grained} {Object} {Retrieval}},
	isbn = {978-0-9992411-2-7},
	url = {https://www.ijcai.org/proceedings/2018/171},
	doi = {10/ghv43w},
	abstract = {Fine-grained object retrieval has attracted extensive research focus recently. Its state-of-the-art schemes are typically based upon convolutional neural network (CNN) features. Despite the extensive progress, two issues remain open. On one hand, the deep features are coarsely extracted at image level rather than precisely at object level, which are interrupted by background clutters. On the other hand, training CNN features with a standard triplet loss is time consuming and incapable to learn discriminative features. In this paper, we present a novel ﬁne-grained object retrieval scheme that conquers these issues in a uniﬁed framework. Firstly, we introduce a novel centralized ranking loss (CRL), which achieves a very efﬁcient (1,000 times training speedup comparing to the triplet loss) and discriminative feature learning by a “centralized” global pooling. Secondly, a weakly supervised attractive feature extraction is proposed, which segments object contours with top-down saliency. Consequently, the contours are integrated into the CNN response map to precisely extract features “within” the target object. Interestingly, we have discovered that the combination of CRL and weakly supervised learning can reinforce each other. We evaluate the performance of the proposed scheme on widely-used benchmarks including CUB200-2011 and CARS196. We have reported signiﬁcant gains over the state-of-the-art schemes, e.g., 5.4\% over SCDA [Wei et al., 2017] on CARS196, and 3.7\% on CUB200-2011.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Zheng, Xiawu and Ji, Rongrong and Sun, Xiaoshuai and Wu, Yongjian and Huang, Feiyue and Yang, Yanhua},
	month = jul,
	year = {2018},
	keywords = {Untagged},
	pages = {1226--1233},
}

@article{zhouDeepMetaLearningLearning2018,
	title = {Deep {Meta}-{Learning}: {Learning} to {Learn} in the {Concept} {Space}},
	shorttitle = {Deep {Meta}-{Learning}},
	url = {http://arxiv.org/abs/1802.03596},
	abstract = {Few-shot learning remains challenging for metalearning that learns a learning algorithm (metalearner) from many related tasks. In this work, we argue that this is due to the lack of a good representation for meta-learning, and propose deep metalearning to integrate the representation power of deep learning into meta-learning. The framework is composed of three modules, a concept generator, a meta-learner, and a concept discriminator, which are learned jointly. The concept generator, e.g. a deep residual net, extracts a representation for each instance that captures its high-level concept, on which the meta-learner performs fewshot learning, and the concept discriminator recognizes the concepts. By learning to learn in the concept space rather than in the complicated instance space, deep meta-learning can substantially improve vanilla meta-learning, which is demonstrated on various few-shot image recognition problems. For example, on 5-way-1-shot image recognition on CIFAR-100 and CUB-200, it improves Matching Nets from 50.53\% and 56.53\% to 58.18\% and 63.47\%, improves MAML from 49.28\% and 50.45\% to 56.65\% and 64.63\%, and improves Meta-SGD from 53.83\% and 53.34\% to 61.62\% and 66.95\%, respectively.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1802.03596 [cs]},
	author = {Zhou, Fengwei and Wu, Bin and Li, Zhenguo},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.03596},
	keywords = {Untagged},
}

@inproceedings{zhouDLinkNetLinkNetPretrained2018,
	address = {Salt Lake City, UT, USA},
	title = {D-{LinkNet}: {LinkNet} with {Pretrained} {Encoder} and {Dilated} {Convolution} for {High} {Resolution} {Satellite} {Imagery} {Road} {Extraction}},
	isbn = {978-1-5386-6100-0},
	shorttitle = {D-{LinkNet}},
	url = {https://ieeexplore.ieee.org/document/8575492/},
	doi = {10/gftk8h},
	abstract = {Road extraction is a fundamental task in the ﬁeld of remote sensing which has been a hot research topic in the past decade. In this paper, we propose a semantic segmentation neural network, named D-LinkNet, which adopts encoderdecoder structure, dilated convolution and pretrained encoder for road extraction task. The network is built with LinkNet architecture and has dilated convolution layers in its center part. Linknet architecture is efﬁcient in computation and memory. Dilation convolution is a powerful tool that can enlarge the receptive ﬁeld of feature points without reducing the resolution of the feature maps. In the CVPR DeepGlobe 2018 Road Extraction Challenge, our best IoU scores on the validation set and the test set are 0.6466 and 0.6342 respectively.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Zhou, Lichen and Zhang, Chuang and Wu, Ming},
	month = jun,
	year = {2018},
	keywords = {Untagged},
	pages = {192--1924},
}

@inproceedings{zhouDeepAdversarialSubspace2018,
	address = {Salt Lake City, UT},
	title = {Deep {Adversarial} {Subspace} {Clustering}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578270/},
	doi = {10/gg9xrj},
	abstract = {Most existing subspace clustering methods hinge on selfexpression of handcrafted representations and are unaware of potential clustering errors. Thus they perform unsatisfactorily on real data with complex underlying subspaces. To solve this issue, we propose a novel deep adversarial subspace clustering (DASC) model, which learns more favorable sample representations by deep learning for subspace clustering, and more importantly introduces adversarial learning to supervise sample representation learning and subspace clustering. Speciﬁcally, DASC consists of a subspace clustering generator and a quality-verifying discriminator, which learn against each other. The generator produces subspace estimation and sample clustering. The discriminator evaluates current clustering performance by inspecting whether the re-sampled data from estimated subspaces have consistent subspace properties, and supervises the generator to progressively improve subspace clustering. Experimental results on the handwritten recognition, face and object clustering tasks demonstrate the advantages of DASC over shallow and few deep subspace clustering models. Moreover, to our best knowledge, this is the ﬁrst successful application of GAN-alike model for unsupervised subspace clustering, which also paves the way for deep learning to solve other unsupervised learning problems.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhou, Pan and Hou, Yunqing and Feng, Jiashi},
	year = {2018},
	keywords = {Untagged},
	pages = {1596--1604},
}

@inproceedings{zhouSFCNOPIDetectionFineGrained2018,
	address = {New Orleans, Louisiana},
	title = {{SFCN}-{OPI}: {Detection} and {Fine}-{Grained} {Classification} of {Nuclei} {Using} {Sibling} {FCN} {With} {Objectness} {Prior} {Interaction}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17088},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Zhou, Yanning and Dou, Qi and Chen, Hao and Qin, Jing and Heng, Pheng-Ann},
	editor = {McIlraith, Sheila A. and Weinberger, Kilian Q.},
	year = {2018},
	keywords = {Untagged},
	pages = {2652--2659},
}

@inproceedings{zhuSparselyAggregatedConvolutional2018,
	address = {Munich, Germany},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Sparsely {Aggregated} {Convolutional} {Networks}},
	volume = {11216},
	url = {https://doi.org/10.1007/978-3-030-01258-8_12},
	doi = {10/ghwngm},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Zhu, Ligeng and Deng, Ruizhi and Maire, Michael and Deng, Zhiwei and Mori, Greg and Tan, Ping},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Untagged},
	pages = {192--208},
}

@inproceedings{zhuHighPerformanceVideo2018,
	address = {Salt Lake City, UT},
	title = {Towards {High} {Performance} {Video} {Object} {Detection}},
	doi = {10/ghwngc},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhu, X. and Dai, J. and Yuan, L. and Wei, Y.},
	year = {2018},
	keywords = {Untagged},
	pages = {7210--7218},
}

@inproceedings{zhuangDiscriminationawareChannelPruning2018,
	address = {Montréal, Canada},
	title = {Discrimination-aware {Channel} {Pruning} for {Deep} {Neural} {Networks}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/55a7cf9c71f1c9c495413f934dd1a158-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zhuang, Zhuangwei and Tan, Mingkui and Zhuang, Bohan and Liu, Jing and Guo, Yong and Wu, Qingyao and Huang, Junzhou and Zhu, Jin-Hui},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	keywords = {Untagged},
	pages = {883--894},
}

@inproceedings{ziebaBinGANLearningCompact2018,
	title = {{BinGAN}: {Learning} {Compact} {Binary} {Descriptors} with a {Regularized} {GAN}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/f442d33fa06832082290ad8544a8da27-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2018, {NeurIPS} 2018, {December} 3-8, 2018, {Montréal}, {Canada}},
	author = {Zieba, Maciej and Semberecki, Piotr and El-Gaaly, Tarek and Trzcinski, Tomasz},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	keywords = {Untagged},
	pages = {3612--3622},
}

@inproceedings{zolfaghariECOEfficientConvolutional2018,
	address = {Munich, Germany},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{ECO}: {Efficient} {Convolutional} {Network} for {Online} {Video} {Understanding}},
	volume = {11206},
	url = {https://doi.org/10.1007/978-3-030-01216-8_43},
	doi = {10/ghwnf9},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Zolfaghari, Mohammadreza and Singh, Kamaljeet and Brox, Thomas},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Untagged},
	pages = {713--730},
}

@inproceedings{zophLearningTransferableArchitectures2018,
	address = {Salt Lake City, UT},
	title = {Learning {Transferable} {Architectures} for {Scalable} {Image} {Recognition}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.html},
	doi = {10/gfxhnx},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
	year = {2018},
	keywords = {Untagged},
	pages = {8697--8710},
}

@techreport{noauthor_minimax_2018,
	title = {Minimax {Theory}-{Statistical} {Machine} {Learning}, {Spring} 2018},
	url = {https://www.stat.cmu.edu/~larry/=sml/},
	urldate = {2022-05-24},
	year = {2018},
	keywords = {Untagged},
}

@inproceedings{abdu-aguyeCompetitiveFeatureExtraction2019,
	title = {Competitive {Feature} {Extraction} for {Activity} {Recognition} based on {Wavelet} {Transforms} and {Adaptive} {Pooling}},
	doi = {10.1109/IJCNN.2019.8852299},
	abstract = {Any application of machine learning requires feature extraction, whereupon the source data is processed to yield representations that are germane to obtaining the desired output. Traditional approaches to feature extraction include the estimation of statistical and structural properties of the data, the choice of which is mainly influenced by domain knowledge. However, Deep Learning has made it possible to learn the best features directly from the data itself, when sufficient data is available. For domains such as activity recognition where such plentiful data may be lacking, the application of deep learning may be limited. Therefore, methods which can yield deep learning-like performance without the need for massive amounts of data remain of interest.In this work we present a novel approach to feature extraction for sensor-generated activity recognition data. We first process the data using wavelet transforms, and subsequently use an adaptive pooling operator on the generated decomposition to obtain a compact, fixed-length representation of the data. Our experiments on seven different activity recognition datasets yield results comparable to those obtained from a deep neural network for all the considered datasets without the need for large amounts of data or any training overhead.},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	author = {Abdu-Aguye, Mubarak G. and Gomaa, Walid},
	year = {2019},
	note = {ISSN: 2161-4407},
	keywords = {Untagged},
	pages = {1--8},
}

@inproceedings{abedinSparseSenseHumanActivity2019,
	address = {Macao, China},
	title = {{SparseSense}: {Human} {Activity} {Recognition} from {Highly} {Sparse} {Sensor} {Data}-streams {Using} {Set}-based {Neural} {Networks}},
	isbn = {978-0-9992411-4-1},
	shorttitle = {{SparseSense}},
	url = {https://www.ijcai.org/proceedings/2019/801},
	doi = {10/gj45dg},
	abstract = {Batteryless or so called passive wearables are providing new and innovative methods for human activity recognition (HAR), especially in healthcare applications for older people. Passive sensors are low cost, lightweight, unobtrusive and desirably disposable; attractive attributes for healthcare applications in hospitals and nursing homes. Despite the compelling propositions for sensing applications, the data streams from these sensors are characterized by high sparsity—the time intervals between sensor readings are irregular while the number of readings per unit time are often limited. In this paper, we rigorously explore the problem of learning activity recognition models from temporally sparse data. We describe how to learn directly from sparse data using a deep learning paradigm in an end-to-end manner. We demonstrate signiﬁcant classiﬁcation performance improvements on realworld passive sensor datasets from older people over the state-of-the-art deep learning human activity recognition models. Further, we provide insights into the model’s behaviour through complementary experiments on a benchmark dataset and visualization of the learned activity feature spaces.},
	language = {en},
	urldate = {2021-04-09},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Abedin, Alireza and Rezatofighi, S. Hamid and Shi, Qinfeng and Ranasinghe, Damith C.},
	year = {2019},
	keywords = {Untagged},
	pages = {5780--5786},
}

@inproceedings{abernethyOnlineLearningDifferential2019,
	title = {Online {Learning} via the {Differential} {Privacy} {Lens}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/file/c36b1132ac829ece87dda55d77ac06a4-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Abernethy, Jacob D and Jung, Young Hun and Lee, Chansoo and McMillan, Audra and Tewari, Ambuj},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	keywords = {Untagged},
	pages = {8894--8904},
}

@inproceedings{abnar_blackbox_2019,
	address = {Florence, Italy},
	title = {Blackbox {Meets} {Blackbox}: {Representational} {Similarity} \& {Stability} {Analysis} of {Neural} {Language} {Models} and {Brains}},
	shorttitle = {Blackbox {Meets} {Blackbox}},
	url = {https://aclanthology.org/W19-4820},
	doi = {10.18653/v1/W19-4820},
	abstract = {In this paper, we define and apply representational stability analysis (ReStA), an intuitive way of analyzing neural language models. ReStA is a variant of the popular representational similarity analysis (RSA) in cognitive neuroscience. While RSA can be used to compare representations in models, model components, and human brains, ReStA compares instances of the same model, while systematically varying single model parameter. Using ReStA, we study four recent and successful neural language models, and evaluate how sensitive their internal representations are to the amount of prior context. Using RSA, we perform a systematic study of how similar the representational spaces in the first and second (or higher) layers of these models are to each other and to patterns of activation in the human brain. Our results reveal surprisingly strong differences between language models, and give insights into where the deep linguistic processing, that integrates information over multiple sentences, is happening in these models. The combination of ReStA and RSA on models and brains allows us to start addressing the important question of what kind of linguistic processes we can hope to observe in fMRI brain imaging data. In particular, our results suggest that the data on story reading from Wehbe et al./ (2014) contains a signal of shallow linguistic processing, but show no evidence on the more interesting deep linguistic processing.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 2019 {ACL} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Abnar, Samira and Beinborn, Lisa and Choenni, Rochelle and Zuidema, Willem},
	month = aug,
	year = {2019},
	keywords = {Untagged},
	pages = {191--203},
}

@article{agarwalEncryptedDatabasesDifferential2019,
	title = {Encrypted {Databases} for {Differential} {Privacy}},
	volume = {2019},
	issn = {2299-0984},
	url = {https://content.sciendo.com/doi/10.2478/popets-2019-0042},
	doi = {10/ghv3hs},
	abstract = {The problem of privatizing statistical databases is a well-studied topic that has culminated with the notion of diﬀerential privacy. The complementary problem of securing these databases, however, has—as far as we know—not been considered in the past. While the security of private databases is in theory orthogonal to the problem of private statistical analysis (e.g., in the central model of diﬀerential privacy the curator is trusted) the recent real-world deployments of diﬀerentially-private systems suggest that it will become a problem of increasing importance. In this work, we consider the problem of designing encrypted databases (EDB) that support diﬀerentially-private statistical queries. More precisely, these EDBs should support a set of encrypted operations with which a curator can securely query and manage its data, and a set of private operations with which an analyst can privately analyze the data. Using such an EDB, a curator can securely outsource its database to an untrusted server (e.g., on-premise or in the cloud) while still allowing an analyst to privately query it.},
	language = {en},
	number = {3},
	urldate = {2021-01-27},
	journal = {Proceedings on Privacy Enhancing Technologies},
	author = {Agarwal, Archita and Herlihy, Maurice and Kamara, Seny and Moataz, Tarik},
	month = jul,
	year = {2019},
	keywords = {Untagged},
	pages = {170--190},
}

@inproceedings{NEURIPS2019_9ce3c52f,
	title = {Differentiable convex optimization layers},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/file/9ce3c52fc54362e22053399d3181c638-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Agrawal, Akshay and Amos, Brandon and Barratt, Shane and Boyd, Stephen and Diamond, Steven and Kolter, J. Zico},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and dAlché-Buc, F. and Fox, E. and Garnett, R.},
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{agustssonGenerativeAdversarialNetworks2019,
	address = {Seoul, Korea},
	title = {Generative {Adversarial} {Networks} for {Extreme} {Learned} {Image} {Compression}},
	url = {https://doi.org/10.1109/ICCV.2019.00031},
	doi = {10/ggwb5j},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Agustsson, Eirikur and Tschannen, Michael and Mentzer, Fabian and Timofte, Radu and Gool, Luc Van},
	year = {2019},
	keywords = {Untagged},
	pages = {221--231},
}

@misc{ahmad_how_2019,
	title = {How {Can} {We} {Be} {So} {Dense}? {The} {Benefits} of {Using} {Highly} {Sparse} {Representations}},
	shorttitle = {How {Can} {We} {Be} {So} {Dense}?},
	url = {http://arxiv.org/abs/1903.11257},
	abstract = {Most artiﬁcial networks today rely on dense representations, whereas biological networks rely on sparse representations. In this paper we show how sparse representations can be more robust to noise and interference, as long as the underlying dimensionality is sufﬁciently high. A key intuition that we develop is that the ratio of the operable volume around a sparse vector divided by the volume of the representational space decreases exponentially with dimensionality. We then analyze computationally efﬁcient sparse networks containing both sparse weights and activations. Simulations on MNIST and the Google Speech Command Dataset show that such networks demonstrate signiﬁcantly improved robustness and stability compared to dense networks, while maintaining competitive accuracy. We discuss the potential beneﬁts of sparsity on accuracy, noise robustness, hyperparameter tuning, learning speed, computational efﬁciency, and power requirements.},
	language = {en},
	urldate = {2023-06-02},
	publisher = {arXiv},
	author = {Ahmad, Subutai and Scheinkman, Luiz},
	month = apr,
	year = {2019},
	note = {arXiv:1903.11257 [cs, stat]},
	keywords = {Untagged},
}

@inproceedings{ajanthanProximalMeanFieldNeural2019,
	address = {Seoul, Korea (South)},
	title = {Proximal {Mean}-{Field} for {Neural} {Network} {Quantization}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010338/},
	doi = {10.1109/ICCV.2019.00497},
	abstract = {Compressing large Neural Networks (NN) by quantizing the parameters, while maintaining the performance is highly desirable due to reduced memory and time complexity. In this work, we cast NN quantization as a discrete labelling problem, and by examining relaxations, we design an efﬁcient iterative optimization procedure that involves stochastic gradient descent followed by a projection. We prove that our simple projected gradient descent approach is, in fact, equivalent to a proximal version of the wellknown mean-ﬁeld method. These ﬁndings would allow the decades-old and theoretically grounded research on MRF optimization to be used to design better network quantization schemes. Our experiments on standard classiﬁcation datasets (MNIST, CIFAR10/100, TinyImageNet) with convolutional and residual architectures show that our algorithm obtains fully-quantized networks with accuracies very close to the ﬂoating-point reference networks.},
	language = {en},
	urldate = {2022-03-04},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Ajanthan, Thalaiyasingam and Dokania, Puneet and Hartley, Richard and Torr, Philip},
	month = oct,
	year = {2019},
	keywords = {Untagged},
	pages = {4870--4879},
}

@article{akimotoAdaptiveStochasticNatural2019,
	title = {Adaptive {Stochastic} {Natural} {Gradient} {Method} for {One}-{Shot} {Neural} {Architecture} {Search}},
	url = {http://arxiv.org/abs/1905.08537},
	abstract = {High sensitivity of neural architecture search (NAS) methods against their input such as stepsize (i.e., learning rate) and search space prevents practitioners from applying them out-of-the-box to their own problems, albeit its purpose is to automate a part of tuning process. Aiming at a fast, robust, and widely-applicable NAS, we develop a generic optimization framework for NAS. We turn a coupled optimization of connection weights and neural architecture into a differentiable optimization by means of stochastic relaxation. It accepts arbitrary search space (widely-applicable) and enables to employ a gradient-based simultaneous optimization of weights and architecture (fast). We propose a stochastic natural gradient method with an adaptive step-size mechanism built upon our theoretical investigation (robust). Despite its simplicity and no problem-dependent parameter tuning, our method exhibited near state-of-theart performances with low computational budgets both on image classiﬁcation and inpainting tasks.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1905.08537 [cs, stat]},
	author = {Akimoto, Youhei and Shirakawa, Shinichi and Yoshinari, Nozomu and Uchida, Kento and Saito, Shota and Nishida, Kouhei},
	month = may,
	year = {2019},
	note = {arXiv: 1905.08537},
	keywords = {Untagged},
}

@inproceedings{allen-zhuCanSGDLearn2019,
	address = {Vancouver, BC, Canada},
	title = {Can {SGD} {Learn} {Recurrent} {Neural} {Networks} with {Provable} {Generalization}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/67fe0f66449e31fdafdc3505c37d6acb-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
	editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily B. and Garnett, Roman},
	year = {2019},
	keywords = {Untagged},
	pages = {10331--10341},
}

@inproceedings{allen-zhuWhatCanResNet2019,
	address = {Vancouver, BC, Canada},
	title = {What {Can} {ResNet} {Learn} {Efficiently}, {Going} {Beyond} {Kernels}?},
	url = {https://proceedings.neurips.cc/paper/2019/hash/5857d68cd9280bc98d079fa912fd6740-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
	editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily B. and Garnett, Roman},
	year = {2019},
	keywords = {Untagged},
	pages = {9015--9025},
}

@inproceedings{allen-zhuLearningGeneralizationOverparameterized2019,
	title = {Learning and {Generalization} in {Overparameterized} {Neural} {Networks}, {Going} {Beyond} {Two} {Layers}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/62dad6e273d32235ae02b7d321578ee8-Abstract.html},
	booktitle = {{NIPS}},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
	editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily B. and Garnett, Roman},
	year = {2019},
	keywords = {Untagged},
	pages = {6155--6166},
}

@inproceedings{allen-zhuConvergenceTheoryDeep2019,
	address = {Long Beach, California},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {A {Convergence} {Theory} for {Deep} {Learning} via {Over}-{Parameterization}},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/allen-zhu19a.html},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	year = {2019},
	keywords = {Untagged},
	pages = {242--252},
}

@inproceedings{allen-zhuConvergenceRateTraining2019,
	title = {On the {Convergence} {Rate} of {Training} {Recurrent} {Neural} {Networks}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/0ee8b85a85a49346fdff9665312a5cc4-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
	editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily B. and Garnett, Roman},
	year = {2019},
	keywords = {Untagged},
	pages = {6673--6685},
}

@inproceedings{andersonChasingGhostsInstruction2019,
	title = {Chasing {Ghosts}: {Instruction} {Following} as {Bayesian} {State} {Tracking}},
	volume = {32},
	shorttitle = {Chasing {Ghosts}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/82161242827b703e6acf9c726942a1e4-Abstract.html},
	urldate = {2021-11-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Anderson, Peter and Shrivastava, Ayush and Parikh, Devi and Batra, Dhruv and Lee, Stefan},
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{azarConvolutionalRelationalMachine2019,
	address = {Long Beach, CA, USA},
	title = {Convolutional {Relational} {Machine} for {Group} {Activity} {Recognition}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953390/},
	doi = {10/ghns2k},
	abstract = {We present an end-to-end deep Convolutional Neural Network called Convolutional Relational Machine (CRM) for recognizing group activities that utilizes the information in spatial relations between individual persons in image or video. It learns to produce an intermediate spatial representation (activity map) based on individual and group activities. A multi-stage reﬁnement component is responsible for decreasing the incorrect predictions in the activity map. Finally, an aggregation component uses the reﬁned information to recognize group activities. Experimental results demonstrate the constructive contribution of the information extracted and represented in the form of the activity map. CRM shows advantages over state-of-the-art models on Volleyball and Collective Activity datasets.},
	language = {en},
	urldate = {2021-04-09},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Azar, Sina Mokhtarzadeh and Atigh, Mina Ghadimi and Nickabadi, Ahmad and Alahi, Alexandre},
	year = {2019},
	keywords = {Untagged},
	pages = {7884--7893},
}

@article{baekCharacterRegionAwareness2019,
	title = {Character {Region} {Awareness} for {Text} {Detection}},
	url = {http://arxiv.org/abs/1904.01941},
	abstract = {Scene text detection methods based on neural networks have emerged recently and have shown promising results. Previous methods trained with rigid word-level bounding boxes exhibit limitations in representing the text region in an arbitrary shape. In this paper, we propose a new scene text detection method to effectively detect text area by exploring each character and afﬁnity between characters. To overcome the lack of individual character level annotations, our proposed framework exploits both the given characterlevel annotations for synthetic images and the estimated character-level ground-truths for real images acquired by the learned interim model. In order to estimate afﬁnity between characters, the network is trained with the newly proposed representation for afﬁnity. Extensive experiments on six benchmarks, including the TotalText and CTW-1500 datasets which contain highly curved texts in natural images, demonstrate that our character-level text detection signiﬁcantly outperforms the state-of-the-art detectors. According to the results, our proposed method guarantees high ﬂexibility in detecting complicated scene text images, such as arbitrarily-oriented, curved, or deformed texts.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1904.01941 [cs]},
	author = {Baek, Youngmin and Lee, Bado and Han, Dongyoon and Yun, Sangdoo and Lee, Hwalsuk},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.01941},
	keywords = {Untagged},
}

@inproceedings{NEURIPS2019_fc0de4e0,
	title = {Differential privacy has disparate impact on model accuracy},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/file/fc0de4e0396fff257ea362983c2dda5a-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Bagdasaryan, Eugene and Poursaeed, Omid and Shmatikov, Vitaly},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and dAlché-Buc, F. and Fox, E. and Garnett, R.},
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{baiMotion2VectorUnsupervisedLearning2019,
	address = {London United Kingdom},
	title = {{Motion2Vector}: unsupervised learning in human activity recognition using wrist-sensing data},
	isbn = {978-1-4503-6869-8},
	shorttitle = {{Motion2Vector}},
	url = {https://dl.acm.org/doi/10.1145/3341162.3349335},
	doi = {10/gj45dd},
	abstract = {With the increasing popularity of consumer wearable devices augmented with sensing capabilities (smart bands, smart watches), there is a signi�cant focus on extracting meaningful information about human behaviour through large scale real-world wearable sensor data. The focus of this work is to develop techniques to detect human activities, utilising a large dataset of wearable data where no ground truth has been produced on the actual activities performed. We propose a deep learning variational auto encoder activity recognition model - Motion2Vector. The model is trained using large amounts of unlabelled human activity data to learn a representation of a time period of activity data. The learned activity representations can be mapped into an embedded activity space and grouped with regards to the nature of the activity type. In order to evaluate the proposed model, we have applied our method on public dataset - The Heterogeneity Human Activity Recognition (HHAR) dataset. The results showed that our method can achieve improved result over the HHAR dataset. In addition, we have collected our own lab-based activity dataset. Our experimental results show that our system achieves good accuracy in detecting such activities, and has the potential to provide additional insights in understanding the real-world activity in the situations where there is no ground truth available.},
	language = {en},
	urldate = {2021-04-09},
	booktitle = {{ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing}},
	publisher = {ACM},
	author = {Bai, Lu and Yeung, Chris and Efstratiou, Christos and Chikomo, Moyra},
	year = {2019},
	keywords = {Untagged},
	pages = {537--542},
}

@inproceedings{baiReRankingMetricFusion2019,
	address = {Long Beach, CA, USA},
	title = {Re-{Ranking} via {Metric} {Fusion} for {Object} {Retrieval} and {Person} {Re}-{Identification}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953719/},
	doi = {10/ghv432},
	abstract = {This work studies the unsupervised re-ranking procedure for object retrieval and person re-identiﬁcation with a speciﬁc concentration on an ensemble of multiple metrics (or similarities). While the re-ranking step is involved by running a diffusion process on the underlying data manifolds, the fusion step can leverage the complementarity of multiple metrics.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Bai, Song and Tang, Peng and Torr, Philip H.S. and Latecki, Longin Jan},
	year = {2019},
	keywords = {Untagged},
	pages = {740--749},
}

@inproceedings{baiProxQuantQuantizedNeural2019,
	title = {{ProxQuant}: {Quantized} {Neural} {Networks} via {Proximal} {Operators}},
	shorttitle = {{ProxQuant}},
	url = {https://openreview.net/forum?id=HyzMyhCcK7},
	abstract = {A principled framework for model quantization using the proximal gradient method, with empirical evaluation and theoretical convergence analyses.},
	language = {en},
	urldate = {2022-02-25},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Bai, Yu and Wang, Yu-Xiang and Liberty, Edo},
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{balcanTestingMatrixRank2019,
	title = {Testing matrix rank, optimally},
	doi = {10.1137/1.9781611975482.46},
	booktitle = {Proceedings of the {Thirtieth} {Annual} {ACM}-{SIAM} {Symposium} on {Discrete} {Algorithms}},
	publisher = {SIAM},
	author = {Balcan, Maria-Florina and Li, Yi and Woodruff, David P. and Zhang, Hongyang},
	year = {2019},
	keywords = {Untagged},
	pages = {727--746},
}

@article{balcanNonConvexMatrixCompletion2019,
	title = {Non-{Convex} {Matrix} {Completion} and {Related} {Problems} via {Strong} {Duality}},
	volume = {20},
	url = {http://jmlr.org/papers/v20/17-611.html},
	journal = {J. Mach. Learn. Res.},
	author = {Balcan, Maria-Florina and Liang, Yingyu and Song, Zhao and Woodruff, David P. and Zhang, Hongyang},
	year = {2019},
	keywords = {Untagged},
	pages = {102:1--102:56},
}

@article{belkinReconcilingModernMachinelearning2019,
	title = {Reconciling modern machine-learning practice and the classical bias–variance trade-off},
	volume = {116},
	copyright = {© 2019 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/116/32/15849},
	doi = {10.1073/pnas.1903070116},
	abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
	language = {en},
	number = {32},
	urldate = {2022-02-22},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	month = aug,
	year = {2019},
	pmid = {31341078},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	keywords = {Untagged},
	pages = {15849--15854},
}

@article{benhamouOperatorNormUpper2019,
	title = {Operator norm upper bound for sub-{Gaussian} tailed random matrices},
	url = {http://arxiv.org/abs/1812.09618},
	abstract = {This paper investigates an upper bound of the operator norm for subGaussian tailed random matrices. A lot of attention has been put on uniformly bounded sub-Gaussian tailed random matrices with independent coeﬃcients. However, little has been done for sub-Gaussian tailed random matrices whose matrix coeﬃcients variance are not equal or for matrix for which coeﬃcients are not independent. This is precisely the subject of this paper. After proving that random matrices with uniform sub-Gaussian tailed independent coeﬃcients satisfy the Tracy W√idom bound, that is, their matrix operator norm remains bounded by O( n) with overwhelming probability, we prove that a less stringent condition is that the matrix rows are independent and uniformly sub-Gaussian. This does not impose in particular that all matrix coeﬃcients are independent, but only their rows, which is a weaker condition.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1812.09618 [math]},
	author = {Benhamou, Eric and Atif, Jamal and Laraki, Rida},
	month = jan,
	year = {2019},
	note = {arXiv: 1812.09618},
	keywords = {Untagged},
}

@article{bhandareEfficient8BitQuantization2019,
	title = {Efficient 8-{Bit} {Quantization} of {Transformer} {Neural} {Machine} {Language} {Translation} {Model}},
	url = {http://arxiv.org/abs/1906.00532},
	abstract = {In this work, we quantize a trained Transformer machine language translation model to lower precision 8-bit integers. We leverage the high performance Intel R Math Kernel Library martix multiplication kernels optimized with INT8/VNNI instructions in the latest Intel R Xeon R Cascade Lake processors to improve inference efﬁciency while maintaining less than 0.5 drop in BLEU score accuracy. To the best of our knowledge, this is the ﬁrst attempt in the industry to quantize the Transformer model. We present novel quantization techniques directly in TensorFlow to opportunistically replace 32-bit ﬂoating point (FP32) computations with 8-bit integers (INT8) and transform the FP32 computational graph. We also present a parallel batching technique to maximize CPU utilization during inference. Our optimizations improved performance of both FP32 and INT8-quantized model resulting in a net improvement of 1.5X of the best quantized model over the best FP32 performance. Furthermore, we reveal opportunities and challenges of quantizing emerging deep learning model inference on Intel CPUs and establish best practices to do so.},
	language = {en},
	urldate = {2021-11-04},
	journal = {arXiv:1906.00532 [cs]},
	author = {Bhandare, Aishwarya and Sripathi, Vamsi and Karkada, Deepthi and Menon, Vivek and Choi, Sun and Datta, Kushal and Saletore, Vikram},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.00532},
	keywords = {Untagged},
}

@inproceedings{biettiInductiveBiasNeural2019,
	address = {Vancouver, BC, Canada},
	title = {On the {Inductive} {Bias} of {Neural} {Tangent} {Kernels}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/c4ef9c39b300931b69a36fb3dbb8d60e-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bietti, Alberto and Mairal, Julien},
	editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily B. and Garnett, Roman},
	year = {2019},
	keywords = {Untagged},
	pages = {12873--12884},
}

@inproceedings{NEURIPS2019_885fe656,
	title = {Accurate, reliable and fast robustness evaluation},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/file/885fe656777008c335ac96072a45be15-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Brendel, Wieland and Rauber, Jonas and Kümmerer, Matthias and Ustyuzhaninov, Ivan and Bethge, Matthias},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and dAlché-Buc, F. and Fox, E. and Garnett, R.},
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{bucher_zero-shot_2019,
	title = {Zero-{Shot} {Semantic} {Segmentation}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/0266e33d3f546cb5436a10798e657d97-Abstract.html},
	abstract = {Semantic segmentation models are limited in their ability to scale to large numbers of object classes. In this paper, we introduce the new task of zero-shot semantic segmentation: learning pixel-wise classifiers for never-seen object categories with zero training examples. To this end, we present a novel architecture, ZS3Net, combining a deep visual  segmentation model with an approach to generate visual representations from semantic word embeddings. By this way, ZS3Net addresses pixel classification tasks where both seen and unseen categories are faced at test time (so called generalized zero-shot classification). Performance is further improved by a self-training step that relies on automatic pseudo-labeling of pixels from unseen classes. On the two standard segmentation datasets, Pascal-VOC and Pascal-Context, we propose zero-shot benchmarks and set competitive baselines. For complex scenes as ones in the Pascal-Context dataset, we extend our approach by using a graph-context encoding to fully leverage spatial context priors coming from class-wise segmentation maps.},
	urldate = {2023-01-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bucher, Maxime and VU, Tuan-Hung and Cord, Matthieu and Pérez, Patrick},
	year = {2019},
	keywords = {Untagged},
}

@article{caiDisentangledImageMatting2019,
	title = {Disentangled {Image} {Matting}},
	url = {http://arxiv.org/abs/1909.04686},
	abstract = {Most previous image matting methods require a roughlyspeciﬁced trimap as input, and estimate fractional alpha values for all pixels that are in the unknown region of the trimap. In this paper, we argue that directly estimating the alpha matte from a coarse trimap is a major limitation of previous methods, as this practice tries to address two difﬁcult and inherently different problems at the same time: identifying true blending pixels inside the trimap region, and estimate accurate alpha values for them. We propose AdaMatting, a new end-to-end matting framework that disentangles this problem into two sub-tasks: trimap adaptation and alpha estimation. Trimap adaptation is a pixelwise classiﬁcation problem that infers the global structure of the input image by identifying deﬁnite foreground, background, and semi-transparent image regions. Alpha estimation is a regression problem that calculates the opacity value of each blended pixel. Our method separately handles these two sub-tasks within a single deep convolutional neural network (CNN). Extensive experiments show that AdaMatting has additional structure awareness and trimap fault-tolerance. Our method achieves the state-ofthe-art performance on Adobe Composition-1k dataset both qualitatively and quantitatively. It is also the current bestperforming method on the alphamatting.com online evaluation for all commonly-used metrics.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1909.04686 [cs]},
	author = {Cai, Shaofan and Zhang, Xiaoshuai and Fan, Haoqiang and Huang, Haibin and Liu, Jiangyu and Liu, Jiaming and Liu, Jiaying and Wang, Jue and Sun, Jian},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.04686},
	keywords = {Untagged},
}

@article{cakirHashingMutualInformation2019,
	title = {Hashing with {Mutual} {Information}},
	volume = {41},
	url = {https://doi.org/10.1109/TPAMI.2019.2914897},
	doi = {10/ghwnqt},
	number = {10},
	journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	author = {Çakir, Fatih and He, Kun and Bargal, Sarah Adel and Sclaroff, Stan},
	year = {2019},
	keywords = {Untagged},
	pages = {2424--2437},
}

@article{carbonnelleLayerRotationSurprisingly2019,
	title = {Layer rotation: a surprisingly powerful indicator of generalization in deep networks?},
	shorttitle = {Layer rotation},
	url = {http://arxiv.org/abs/1806.01603},
	abstract = {Our work presents extensive empirical evidence that layer rotation, i.e. the evolution across training of the cosine distance between each layer’s weight vector and its initialization, constitutes an impressively consistent indicator of generalization performance. In particular, larger cosine distances between ﬁnal and initial weights of each layer consistently translate into better generalization performance of the ﬁnal model. Interestingly, this relation admits a network independent optimum: training procedures during which all layers’ weights reach a cosine distance of 1 from their initialization consistently outperform other conﬁgurations -by up to 30\% test accuracy. Moreover, we show that layer rotations are easily monitored and controlled (helpful for hyperparameter tuning) and potentially provide a uniﬁed framework to explain the impact of learning rate tuning, weight decay, learning rate warmups and adaptive gradient methods on generalization and training speed. In an attempt to explain the surprising properties of layer rotation, we show on a 1-layer MLP trained on MNIST that layer rotation correlates with the degree to which features of intermediate layers have been trained.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1806.01603 [cs, stat]},
	author = {Carbonnelle, Simon and De Vleeschouwer, Christophe},
	month = jul,
	year = {2019},
	note = {arXiv: 1806.01603},
	keywords = {Untagged},
}

@article{caronDeepClusteringUnsupervised2019,
	title = {Deep {Clustering} for {Unsupervised} {Learning} of {Visual} {Features}},
	url = {http://arxiv.org/abs/1807.05520},
	abstract = {Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, kmeans, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a signiﬁcant margin on all the standard benchmarks.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1807.05520 [cs]},
	author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
	month = mar,
	year = {2019},
	note = {arXiv: 1807.05520},
	keywords = {Untagged},
}

@inproceedings{changExploreExploitGraphTraversal2019,
	address = {Long Beach, CA, USA},
	title = {Explore-{Exploit} {Graph} {Traversal} for {Image} {Retrieval}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954266/},
	doi = {10/ghv433},
	abstract = {We propose a novel graph-based approach for image retrieval. Given a nearest neighbor graph produced by the global descriptor model, we traverse it by alternating between exploit and explore steps. The exploit step maximally utilizes the immediate neighborhood of each vertex, while the explore step traverses vertices that are farther away in the descriptor space. By combining these two steps we can better capture the underlying image manifold, and successfully retrieve relevant images that are visually dissimilar to the query. Our traversal algorithm is conceptually simple, has few tunable parameters and can be implemented with basic data structures. This enables fast real-time inference for previously unseen queries with minimal memory overhead. Despite relative simplicity, we show highly competitive results on multiple public benchmarks, including the largest image retrieval dataset that is currently publicly available. Full code for this work is available here: https://github.com/ layer6ai-labs/egt.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Chang, Cheng and Yu, Guangwei and Liu, Chundi and Volkovs, Maksims},
	year = {2019},
	keywords = {Untagged},
	pages = {9415--9423},
}

@article{changCronusRobustHeterogeneous2019,
	title = {Cronus: {Robust} and {Heterogeneous} {Collaborative} {Learning} with {Black}-{Box} {Knowledge} {Transfer}},
	shorttitle = {Cronus},
	url = {http://arxiv.org/abs/1912.11279},
	abstract = {Collaborative (federated) learning enables multiple parties to train a global model without sharing their private data, but through repeated sharing of the parameters of their local models. Each party updates its local model using the aggregation of all parties’ parameters, before each round of local training.},
	language = {en},
	urldate = {2021-11-19},
	journal = {arXiv:1912.11279 [cs, stat]},
	author = {Chang, Hongyan and Shejwalkar, Virat and Shokri, Reza and Houmansadr, Amir},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.11279},
	keywords = {Untagged},
}

@inproceedings{chenHybridAttentionBasedDecoupled2019,
	address = {Long Beach, CA, USA},
	title = {Hybrid-{Attention} {Based} {Decoupled} {Metric} {Learning} for {Zero}-{Shot} {Image} {Retrieval}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953975/},
	doi = {10/ghjffk},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Chen, Binghui and Deng, Weihong},
	year = {2019},
	keywords = {Untagged},
	pages = {2745--2754},
}

@article{chenProgressiveFeatureAlignment2019,
	title = {Progressive {Feature} {Alignment} for {Unsupervised} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1811.08585},
	abstract = {Unsupervised domain adaptation (UDA) transfers knowledge from a label-rich source domain to a fullyunlabeled target domain. To tackle this task, recent approaches resort to discriminative domain transfer in virtue of pseudo-labels to enforce the class-level distribution alignment across the source and target domains. These methods, however, are vulnerable to the error accumulation and thus incapable of preserving cross-domain category consistency, as the pseudo-labeling accuracy is not guaranteed explicitly. In this paper, we propose the Progressive Feature Alignment Network (PFAN) to align the discriminative features across domains progressively and effectively, via exploiting the intra-class variation in the target domain. To be speciﬁc, we ﬁrst develop an Easyto-Hard Transfer Strategy (EHTS) and an Adaptive Prototype Alignment (APA) step to train our model iteratively and alternatively. Moreover, upon observing that a good domain adaptation usually requires a non-saturated source classiﬁer, we consider a simple yet efﬁcient way to retard the convergence speed of the source classiﬁcation loss by further involving a temperature variate into the soft-max function. The extensive experimental results reveal that the proposed PFAN exceeds the state-of-the-art performance on three UDA datasets.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1811.08585 [cs]},
	author = {Chen, Chaoqi and Xie, Weiping and Huang, Wenbing and Rong, Yu and Ding, Xinghao and Huang, Yue and Xu, Tingyang and Huang, Junzhou},
	month = may,
	year = {2019},
	note = {arXiv: 1811.08585},
	keywords = {Untagged},
}

@article{chenDataFreeLearningStudent2019,
	title = {Data-{Free} {Learning} of {Student} {Networks}},
	url = {http://arxiv.org/abs/1904.01186},
	abstract = {Learning portable neural networks is very essential for computer vision for the purpose that pre-trained heavy deep models can be well applied on edge devices such as mobile phones and micro sensors. Most existing deep neural network compression and speed-up methods are very effective for training compact deep models, when we can directly access the training dataset. However, training data for the given deep network are often unavailable due to some practice problems (e.g. privacy, legal issue, and transmission), and the architecture of the given network are also unknown except some interfaces. To this end, we propose a novel framework for training efﬁcient deep neural networks by exploiting generative adversarial networks (GANs). To be speciﬁc, the pre-trained teacher networks are regarded as a ﬁxed discriminator and the generator is utilized for derivating training samples which can obtain the maximum response on the discriminator. Then, an efﬁcient network with smaller model size and computational complexity is trained using the generated data and the teacher network, simultaneously. Efﬁcient student networks learned using the proposed Data-Free Learning (DAFL) method achieve 92.22\% and 74.47\% accuracies using ResNet-18 without any training data on the CIFAR-10 and CIFAR-100 datasets, respectively. Meanwhile, our student network obtains an 80.56\% accuracy on the CelebA benchmark.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1904.01186 [cs, stat]},
	author = {Chen, Hanting and Wang, Yunhe and Xu, Chang and Yang, Zhaohui and Liu, Chuanjian and Shi, Boxin and Xu, Chunjing and Xu, Chao and Tian, Qi},
	month = dec,
	year = {2019},
	note = {arXiv: 1904.01186},
	keywords = {Untagged},
}

@inproceedings{chenDistributionallyRobustSemiSupervised2019,
	title = {Distributionally {Robust} {Semi}-{Supervised} {Learning} for {People}-{Centric} {Sensing}},
	volume = {33},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/4205},
	doi = {10/ghkjpq},
	abstract = {Semi-supervised learning is crucial for alleviating labelling burdens in people-centric sensing. However, humangenerated data inherently suffer from distribution shift in semi-supervised learning due to the diverse biological conditions and behavior patterns of humans. To address this problem, we propose a generic distributionally robust model for semi-supervised learning on distributionally shifted data. Considering both the discrepancy and the consistency between the labeled data and the unlabeled data, we learn the latent features that reduce person-speciﬁc discrepancy and preserve task-speciﬁc consistency. We evaluate our model in a variety of people-centric recognition tasks on real-world datasets, including intention recognition, activity recognition, muscular movement recognition and gesture recognition. The experiment results demonstrate that the proposed model outperforms the state-of-the-art methods.},
	language = {en},
	urldate = {2021-04-09},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Chen, Kaixuan and Yao, Lina and Zhang, Dalin and Chang, Xiaojun and Long, Guodong and Wang, Sen},
	year = {2019},
	keywords = {Untagged},
	pages = {3321--3328},
}

@inproceedings{chenMultiagentAttentionalActivity2019,
	address = {Macao, China},
	title = {Multi-agent {Attentional} {Activity} {Recognition}},
	url = {https://doi.org/10.24963/ijcai.2019/186},
	doi = {10.24963/ijcai.2019/186},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {ijcai.org},
	author = {Chen, Kaixuan and Yao, Lina and Zhang, Dalin and Guo, Bin and Yu, Zhiwen},
	editor = {Kraus, Sarit},
	year = {2019},
	keywords = {Untagged},
	pages = {1344--1350},
}

@article{chenNeuralOrdinaryDifferential2019,
	title = {Neural {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing ﬂows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1806.07366 [cs, stat]},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	month = dec,
	year = {2019},
	note = {arXiv: 1806.07366},
	keywords = {Untagged},
}

@inproceedings{chenLearningActiveContour2019,
	address = {Long Beach, CA, USA},
	title = {Learning {Active} {Contour} {Models} for {Medical} {Image} {Segmentation}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953484/},
	doi = {10/ghv3jb},
	abstract = {Image segmentation is an important step in medical image processing and has been widely studied and developed for reﬁnement of clinical analysis and applications. New models based on deep learning have improved results but are restricted to pixel-wise tting of the segmentation map. Our aim was to tackle this limitation by developing a new model based on deep learning which takes into account the area inside as well as outside the region of interest as well as the size of boundaries during learning. Specically, we propose a new loss function which incorporates area and size information and integrates this into a dense deep learning model. We evaluated our approach on a dataset of more than 2,000 cardiac MRI scans. Our results show that the proposed loss function outperforms other mainstream loss function Cross-entropy on two common segmentation networks. Our loss function is robust while using different hyperparameter λ.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Chen, Xu and Williams, Bryan M. and Vallabhaneni, Srinivasa R. and Czanner, Gabriela and Williams, Rachel and Zheng, Yalin},
	year = {2019},
	keywords = {Untagged},
	pages = {11624--11632},
}

@article{chenPixelHopSuccessiveSubspace2019,
	title = {{PixelHop}: {A} {Successive} {Subspace} {Learning} ({SSL}) {Method} for {Object} {Classification}},
	shorttitle = {{PixelHop}},
	url = {http://arxiv.org/abs/1909.08190},
	abstract = {A new machine learning methodology, called successive subspace learning (SSL), is introduced in this work. SSL contains four key ingredients: 1) successive near-to-far neighborhood expansion; 2) unsupervised dimension reduction via subspace approximation; 3) supervised dimension reduction via label-assisted regression (LAG); and 4) feature concatenation and decision making. An image-based object classiﬁcation method, called PixelHop, is proposed to illustrate the SSL design. It is shown by experimental results that the PixelHop method outperforms the classic CNN model of similar model complexity in three benchmarking datasets (MNIST, Fashion MNIST and CIFAR-10). Although SSL and deep learning (DL) have some high-level concept in common, they are fundamentally different in model formulation, the training process and training complexity. Extensive discussion on the comparison of SSL and DL is made to provide further insights into the potential of SSL.},
	language = {en},
	urldate = {2021-01-24},
	journal = {arXiv:1909.08190 [cs, stat]},
	author = {Chen, Yueru and Kuo, C.-C. Jay},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.08190},
	keywords = {Untagged},
}

@inproceedings{chenRENASReinforcedEvolutionary2019,
	title = {{RENAS}: {Reinforced} {Evolutionary} {Neural} {Architecture} {Search}},
	url = {http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_RENAS_Reinforced_Evolutionary_Neural_Architecture_Search_CVPR_2019_paper.html},
	doi = {10/ghv236},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2019, {Long} {Beach}, {CA}, {USA}, {June} 16-20, 2019},
	publisher = {Computer Vision Foundation / IEEE},
	author = {Chen, Yukang and Meng, Gaofeng and Zhang, Qian and Xiang, Shiming and Huang, Chang and Mu, Lisen and Wang, Xinggang},
	year = {2019},
	keywords = {Untagged},
	pages = {4787--4796},
}

@inproceedings{pmlr-v97-cohen19c,
	series = {Proceedings of machine learning research},
	title = {Certified adversarial robustness via randomized smoothing},
	volume = {97},
	url = {https://proceedings.mlr.press/v97/cohen19c.html},
	abstract = {We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the L2 norm. While this "randomized smoothing" technique has been proposed before in the literature, we are the first to provide a tight analysis, which establishes a close connection between L2 robustness and Gaussian noise. We use the technique to train an ImageNet classifier with e.g. a certified top-1 accuracy of 49\% under adversarial perturbations with L2 norm less than 0.5 (=127/255). Smoothing is the only approach to certifiably robust classification which has been shown feasible on full-resolution ImageNet. On smaller-scale datasets where competing approaches to certified L2 robustness are viable, smoothing delivers higher certified accuracies. The empirical success of the approach suggests that provable methods based on randomization at prediction time are a promising direction for future research into adversarially robust classification.},
	booktitle = {Proceedings of the 36th international conference on machine learning},
	publisher = {PMLR},
	author = {Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	note = {tex.pdf: http://proceedings.mlr.press/v97/cohen19c/cohen19c.pdf},
	keywords = {Untagged},
	pages = {1310--1320},
}

@inproceedings{cuiClassBalancedLossBased2019,
	title = {Class-{Balanced} {Loss} {Based} on {Effective} {Number} of {Samples}},
	url = {http://openaccess.thecvf.com/content_CVPR_2019/html/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.html},
	doi = {10/ggnkz6},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2019, {Long} {Beach}, {CA}, {USA}, {June} 16-20, 2019},
	publisher = {Computer Vision Foundation / IEEE},
	author = {Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge J.},
	year = {2019},
	keywords = {Untagged},
	pages = {9268--9277},
}

@article{daiVideoPersonReidentification2019,
	title = {Video {Person} {Re}-identification by {Temporal} {Residual} {Learning}},
	volume = {28},
	issn = {1057-7149, 1941-0042},
	url = {http://arxiv.org/abs/1802.07918},
	doi = {10/ghv22q},
	abstract = {In this paper, we propose a novel feature learning framework for video person re-identiﬁcation (re-ID). The proposed framework largely aims to exploit the adequate temporal information of video sequences and tackle the poor spatial alignment of moving pedestrians. More speciﬁcally, for exploiting the temporal information, we design a temporal residual learning (TRL) module to simultaneously extract the generic and speciﬁc features of consecutive frames. The TRL module is equipped with two bi-directional LSTM (BiLSTM), which are respectively responsible to describe a moving person in different aspects, providing complementary information for better feature representations. To deal with the poor spatial alignment in video reID datasets, we propose a spatial-temporal transformer network (ST2N) module. Transformation parameters in the ST2N module are learned by leveraging the high-level semantic information of the current frame as well as the temporal context knowledge from other frames. The proposed ST2N module with less learnable parameters allows effective person alignments under signiﬁcant appearance changes. Extensive experimental results on the largescale MARS, PRID2011, ILIDS-VID and SDU-VID datasets demonstrate that the proposed method achieves consistently superior performance and outperforms most of the very recent state-of-the-art methods.},
	language = {en},
	number = {3},
	urldate = {2021-01-27},
	journal = {IEEE Transactions on Image Processing},
	author = {Dai, Ju and Zhang, Pingping and Lu, Huchuan and Wang, Hongyu},
	month = mar,
	year = {2019},
	note = {arXiv: 1802.07918},
	keywords = {Untagged},
	pages = {1366--1377},
}

@inproceedings{danOptimalAnalysisSubsetselection2019,
	title = {Optimal analysis of subset-selection based l\_p low-rank approximation},
	volume = {32},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Dan, Chen and Wang, Hong and Zhang, Hongyang and Zhou, Yuchen and Ravikumar, Pradeep K.},
	year = {2019},
	keywords = {Untagged},
}

@article{devlinBERTPretrainingDeep2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2021-06-04},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Untagged},
}

@inproceedings{deyDoodleSearchPractical2019,
	address = {Long Beach, CA, USA},
	title = {Doodle to {Search}: {Practical} {Zero}-{Shot} {Sketch}-{Based} {Image} {Retrieval}},
	isbn = {978-1-72813-293-8},
	shorttitle = {Doodle to {Search}},
	url = {https://ieeexplore.ieee.org/document/8953251/},
	doi = {10/ghv435},
	abstract = {In this paper, we investigate the problem of zeroshot sketch-based image retrieval (ZS-SBIR), where human sketches are used as queries to conduct retrieval of photos from unseen categories. We importantly advance prior arts by proposing a novel ZS-SBIR scenario that represents a ﬁrm step forward in its practical application. The new setting uniquely recognizes two important yet often neglected challenges of practical ZS-SBIR, (i) the large domain gap between amateur sketch and photo, and (ii) the necessity for moving towards large-scale retrieval. We ﬁrst contribute to the community a novel ZS-SBIR dataset, QuickDrawExtended, that consists of 330, 000 sketches and 204, 000 photos spanning across 110 categories. Highly abstract amateur human sketches are purposefully sourced to maximize the domain gap, instead of ones included in existing datasets that can often be semi-photorealistic. We then formulate a ZS-SBIR framework to jointly model sketches and photos into a common embedding space. A novel strategy to mine the mutual information among domains is specifically engineered to alleviate the domain gap. External semantic knowledge is further embedded to aid semantic transfer. We show that, rather surprisingly, retrieval performance signiﬁcantly outperforms that of state-of-the-art on existing datasets that can already be achieved using a reduced version of our model. We further demonstrate the superior performance of our full model by comparing with a number of alternatives on the newly proposed dataset. The new dataset, plus all training and testing code of our model, will be publicly released to facilitate future research†.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Dey, Sounak and Riba, Pau and Dutta, Anjan and Llados, Josep Llados and Song, Yi-Zhe},
	year = {2019},
	keywords = {Untagged},
	pages = {2174--2183},
}

@inproceedings{dingRegularizingActivationDistribution2019,
	title = {Regularizing {Activation} {Distribution} for {Training} {Binarized} {Deep} {Networks}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Ding_Regularizing_Activation_Distribution_for_Training_Binarized_Deep_Networks_CVPR_2019_paper.html},
	urldate = {2021-07-12},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Ding, Ruizhou and Chin, Ting-Wu and Liu, Zeye and Marculescu, Diana},
	year = {2019},
	note = {Github:https://github.com/ruizhoud/DistributionLoss},
	keywords = {Untagged},
	pages = {11408--11417},
}

@inproceedings{dingACNetStrengtheningKernel2019,
	address = {Seoul, Republic of Korea},
	title = {{ACNet}: {Strengthening} the {Kernel} {Skeletons} for {Powerful} {CNN} via {Asymmetric} {Convolution} {Blocks}},
	url = {https://doi.org/10.1109/ICCV.2019.00200},
	doi = {10/ggsmjf},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Ding, Xiaohan and Guo, Yuchen and Ding, Guiguang and Han, Jungong},
	year = {2019},
	keywords = {Untagged},
	pages = {1911--1920},
}

@inproceedings{dongDualEncodingZeroExample2019,
	address = {Long Beach, CA, USA},
	title = {Dual {Encoding} for {Zero}-{Example} {Video} {Retrieval}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953675/},
	doi = {10/ghv436},
	abstract = {This paper attacks the challenging problem of zeroexample video retrieval. In such a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc queries described in natural language text with no visual example provided. Given videos as sequences of frames and queries as sequences of words, an effective sequence-to-sequence cross-modal matching is required. The majority of existing methods are concept based, extracting relevant concepts from queries and videos and accordingly establishing associations between the two modalities. In contrast, this paper takes a concept-free approach, proposing a dual deep encoding network that encodes videos and queries into powerful dense representations of their own. Dual encoding is conceptually simple, practically effective and endto-end. As experiments on three benchmarks, i.e. MSRVTT, TRECVID 2016 and 2017 Ad-hoc Video Search show, the proposed solution establishes a new state-of-the-art for zero-example video retrieval.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Dong, Jianfeng and Li, Xirong and Xu, Chaoxi and Ji, Shouling and He, Yuan and Yang, Gang and Wang, Xun},
	year = {2019},
	keywords = {Untagged},
	pages = {9338--9347},
}

@article{duGRADIENTDESCENTPROVABLY2019,
	title = {{GRADIENT} {DESCENT} {PROVABLY} {OPTIMIZES} {OVER}-{PARAMETERIZED} {NEURAL} {NETWORKS}},
	abstract = {One of the mysteries in the success of neural networks is randomly initialized ﬁrst order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystiﬁes this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an m hidden node shallow neural network with ReLU activation and n training data, we show as long as m is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function.},
	language = {en},
	author = {Du, Simon S and Zhai, Xiyu},
	year = {2019},
	keywords = {Untagged},
	pages = {19},
}

@inproceedings{duGradientDescentFinds2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Gradient {Descent} {Finds} {Global} {Minima} of {Deep} {Neural} {Networks}},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/du19c.html},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}, {ICML} 2019, 9-15 {June} 2019, {Long} {Beach}, {California}, {USA}},
	publisher = {PMLR},
	author = {Du, Simon S. and Lee, Jason D. and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	year = {2019},
	keywords = {Untagged},
	pages = {1675--1685},
}

@inproceedings{duttaSemanticallyTiedPaired2019,
	address = {Long Beach, CA, USA},
	title = {Semantically {Tied} {Paired} {Cycle} {Consistency} for {Zero}-{Shot} {Sketch}-{Based} {Image} {Retrieval}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954217/},
	doi = {10/ghv437},
	abstract = {Zero-shot sketch-based image retrieval (SBIR) is an emerging task in computer vision, allowing to retrieve natural images relevant to sketch queries that might not been seen in the training phase. Existing works either require aligned sketch-image pairs or inefﬁcient memory fusion layer for mapping the visual information to a semantic space. In this work, we propose a semantically aligned paired cycle-consistent generative (SEM-PCYC) model for zero-shot SBIR, where each branch maps the visual information to a common semantic space via an adversarial training. Each of these branches maintains a cycle consistency that only requires supervision at category levels, and avoids the need of highly-priced aligned sketch-image pairs. A classiﬁcation criteria on the generators’ outputs ensures the visual to semantic space mapping to be discriminating. Furthermore, we propose to combine textual and hierarchical side information via a feature selection autoencoder that selects discriminating side information within a same end-to-end model. Our results demonstrate a significant boost in zero-shot SBIR performance over the state-ofthe-art on the challenging Sketchy and TU-Berlin datasets.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Dutta, Anjan and Akata, Zeynep},
	year = {2019},
	keywords = {Untagged},
	pages = {5084--5093},
}

@inproceedings{eghbaliDeepSphericalQuantization2019,
	address = {Long Beach, CA, USA},
	title = {Deep {Spherical} {Quantization} for {Image} {Search}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953784/},
	doi = {10/ghv45h},
	abstract = {Hashing methods, which encode high-dimensional images with compact discrete codes, have been widely applied to enhance large-scale image retrieval. In this paper, we put forward Deep Spherical Quantization (DSQ), a novel method to make deep convolutional neural networks generate supervised and compact binary codes for efﬁcient image search. Our approach simultaneously learns a mapping that transforms the input images into a low-dimensional discriminative space, and quantizes the transformed data points using multi-codebook quantization. To eliminate the negative effect of norm variance on codebook learning, we force the network to L2 normalize the extracted features and then quantize the resulting vectors using a new supervised quantization technique speciﬁcally designed for points lying on a unit hypersphere. Furthermore, we introduce an easyto-implement extension of our quantization technique that enforces sparsity on the codebooks. Extensive experiments demonstrate that DSQ and its sparse variant can generate semantically separable compact binary codes outperforming many state-of-the-art image retrieval methods on three benchmarks.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Eghbali, Sepehr and Tahvildari, Ladan},
	year = {2019},
	keywords = {Untagged},
	pages = {11682--11691},
}

@article{elskenNeuralArchitectureSearch2019,
	title = {Neural {Architecture} {Search}: {A} {Survey}},
	volume = {20},
	url = {http://jmlr.org/papers/v20/18-598.html},
	journal = {J. Mach. Learn. Res.},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	year = {2019},
	keywords = {Untagged},
	pages = {55:1--55:21},
}

@article{elzanatyLimitsSparseData2019,
	title = {Limits on {Sparse} {Data} {Acquisition}: {RIC} {Analysis} of {Finite} {Gaussian} {Matrices}},
	volume = {65},
	issn = {0018-9448, 1557-9654},
	shorttitle = {Limits on {Sparse} {Data} {Acquisition}},
	url = {http://arxiv.org/abs/1802.03195},
	doi = {10/ghv4n4},
	abstract = {One of the key issues in the acquisition of sparse data by means of compressed sensing (CS) is the design of the measurement matrix. Gaussian matrices have been proven to be information-theoretically optimal in terms of minimizing the required number of measurements for sparse recovery. In this paper we provide a new approach for the analysis of the restricted isometry constant (RIC) of ﬁnite dimensional Gaussian measurement matrices. The proposed method relies on the exact distributions of the extreme eigenvalues for Wishart matrices. First, we derive the probability that the restricted isometry property is satisﬁed for a given sufﬁcient recovery condition on the RIC, and propose a probabilistic framework to study both the symmetric and asymmetric RICs. Then, we analyze the recovery of compressible signals in noise through the statistical characterization of stability and robustness. The presented framework determines limits on various sparse recovery algorithms for ﬁnite size problems. In particular, it provides a tight lower bound on the maximum sparsity order of the acquired data allowing signal recovery with a given target probability. Also, we derive simple approximations for the RICs based on the Tracy-Widom distribution.},
	language = {en},
	number = {3},
	urldate = {2021-01-27},
	journal = {IEEE Transactions on Information Theory},
	author = {Elzanaty, Ahmed and Giorgetti, Andrea and Chiani, Marco},
	month = mar,
	year = {2019},
	note = {arXiv: 1802.03195},
	keywords = {Untagged},
	pages = {1578--1588},
}

@article{engelDeepLocalizationLandmarkbasedSelfLocalization2019,
	title = {{DeepLocalization}: {Landmark}-based {Self}-{Localization} with {Deep} {Neural} {Networks}},
	shorttitle = {{DeepLocalization}},
	url = {http://arxiv.org/abs/1904.09007},
	abstract = {We address the problem of landmark-based vehicle self-localization by relying on multi-modal sensory information. Our goal is to determine the autonomous vehicle’s pose based on landmark measurements and map landmarks. The map is built by extracting landmarks from the vehicle’s ﬁeld of view in an off-line way, while the measurements are collected in the same way during inference. To map the measurements and map landmarks to the vehicle’s pose, we propose DeepLocalization, a deep neural network that copes with dynamic input. Our network is robust to missing landmarks that occur due to the dynamic environment and handles unordered and adaptive input. In real-world experiments, we evaluate two inference approaches to show that DeepLocalization can be combined with GPS-sensors and is complementary to ﬁltering approaches such as an extended Kalman ﬁlter. We show that our approach achieves state-of-the-art accuracy and is about ten times faster than the related work.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1904.09007 [cs, stat]},
	author = {Engel, Nico and Hoermann, Stefan and Horn, Markus and Belagiannis, Vasileios and Dietmayer, Klaus},
	month = jul,
	year = {2019},
	note = {arXiv: 1904.09007},
	keywords = {Untagged},
}

@inproceedings{fuDualAttentionNetwork2019,
	title = {Dual {Attention} {Network} for {Scene} {Segmentation}},
	url = {http://openaccess.thecvf.com/content_CVPR_2019/html/Fu_Dual_Attention_Network_for_Scene_Segmentation_CVPR_2019_paper.html},
	doi = {10/ggpcpn},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2019, {Long} {Beach}, {CA}, {USA}, {June} 16-20, 2019},
	publisher = {Computer Vision Foundation / IEEE},
	author = {Fu, Jun and Liu, Jing and Tian, Haijie and Li, Yong and Bao, Yongjun and Fang, Zhiwei and Lu, Hanqing},
	year = {2019},
	keywords = {Untagged},
	pages = {3146--3154},
}

@article{gaierWeightAgnosticNeural2019,
	title = {Weight {Agnostic} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1906.04358},
	abstract = {Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:1906.04358 [cs, stat]},
	author = {Gaier, Adam and Ha, David},
	month = sep,
	year = {2019},
	note = {arXiv: 1906.04358},
	keywords = {Untagged},
}

@inproceedings{gao2018representation,
	title = {Representation degeneration problem in training natural language generation models},
	url = {https://openreview.net/forum?id=SkEYojRqtm},
	booktitle = {International conference on learning representations},
	author = {Gao, Jun and He, Di and Tan, Xu and Qin, Tao and Wang, Liwei and Liu, Tieyan},
	year = {2019},
	keywords = {Untagged},
}

@article{garneloReconcilingDeepLearning2019,
	title = {Reconciling deep learning with symbolic artificial intelligence: representing objects and relations},
	volume = {29},
	issn = {23521546},
	shorttitle = {Reconciling deep learning with symbolic artificial intelligence},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352154618301943},
	doi = {10/gf7d9s},
	language = {en},
	urldate = {2021-01-27},
	journal = {Current Opinion in Behavioral Sciences},
	author = {Garnelo, Marta and Shanahan, Murray},
	month = oct,
	year = {2019},
	keywords = {Untagged},
	pages = {17--23},
}

@inproceedings{gavrilinAcrossSensorFeatureLearning2019,
	title = {Across-{Sensor} {Feature} {Learning} for {Energy}-{Efficient} {Activity} {Recognition} on {Mobile} {Devices}},
	doi = {10.1109/IJCNN.2019.8851977},
	abstract = {In this paper we propose across-sensor representation learning framework for improving power-accuracy trade-off in multi-sensor human activity recognition (HAR). The goal of the study is to achieve the level of performance comparable to one of multi-sensor HAR systems by using fewer or even single sensor. Such performance is achieved by learning relations between these sensors at training time and utilizing them at test time. These relations are learned by supervised deep models which use multi-sensor data during training only. The absence of need for having multiple sensors during test time allows turning these sensors off and replacing them with a single sensor coupled with learned across-sensor relations. These across-sensor relations make up for the information lost from the turned-off sensors. Using fewer sensors reduces energy consumption of HAR systems deployed on a smartphone. Moreover, it allows building HAR systems for situations when collection of multi-sensor data is possible only during training. This work presents preliminary results achieved with the proposed approach on the SHL dataset. Obtained results show an improvement of up to 14\% in classification accuracy of single-sensor HAR.},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	author = {Gavrilin, Yuriy and Khan, Adil},
	year = {2019},
	note = {ISSN: 2161-4407},
	keywords = {Untagged},
	pages = {1--7},
}

@inproceedings{geWeaklySupervisedComplementary2019,
	title = {Weakly supervised complementary parts models for fine-grained image classification from the bottom up},
	url = {http://openaccess.thecvf.com/content_CVPR_2019/html/Ge_Weakly_Supervised_Complementary_Parts_Models_for_Fine-Grained_Image_Classification_From_CVPR_2019_paper.html},
	doi = {10.1109/CVPR.2019.00315},
	booktitle = {{IEEE} conference on computer vision and pattern recognition, {CVPR} 2019, long beach, {CA}, {USA}, june 16-20, 2019},
	publisher = {Computer Vision Foundation / IEEE},
	author = {Ge, Weifeng and Lin, Xiangru and Yu, Yizhou},
	year = {2019},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/cvpr/GeLY19.bib
tex.timestamp: Mon, 30 Aug 2021 17:01:14 +0200},
	keywords = {Untagged},
	pages = {3034--3043},
}

@article{gengOptimalNoiseAddingMechanism2019,
	title = {Optimal {Noise}-{Adding} {Mechanism} in {Additive} {Differential} {Privacy}},
	url = {http://arxiv.org/abs/1809.10224},
	abstract = {We derive the optimal \$(0, {\textbackslash}delta)\$-differentially private query-output independent noise-adding mechanism for single real-valued query function under a general cost-minimization framework. Under a mild technical condition, we show that the optimal noise probability distribution is a uniform distribution with a probability mass at the origin. We explicitly derive the optimal noise distribution for general \${\textbackslash}ell{\textasciicircum}p\$ cost functions, including \${\textbackslash}ell{\textasciicircum}1\$ (for noise magnitude) and \${\textbackslash}ell{\textasciicircum}2\$ (for noise power) cost functions, and show that the probability concentration on the origin occurs when \${\textbackslash}delta {\textgreater} {\textbackslash}frac\{p\}\{p+1\}\$. Our result demonstrates an improvement over the existing Gaussian mechanisms by a factor of two and three for \$(0,{\textbackslash}delta)\$-differential privacy in the high privacy regime in the context of minimizing the noise magnitude and noise power, and the gain is more pronounced in the low privacy regime. Our result is consistent with the existing result for \$(0,{\textbackslash}delta)\$-differential privacy in the discrete setting, and identifies a probability concentration phenomenon in the continuous setting.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1809.10224 [cs]},
	author = {Geng, Quan and Ding, Wei and Guo, Ruiqi and Kumar, Sanjiv},
	month = feb,
	year = {2019},
	note = {arXiv: 1809.10224},
	keywords = {Untagged},
}

@inproceedings{ginart_making_2019,
	title = {Making {AI} {Forget} {You}: {Data} {Deletion} in {Machine} {Learning}},
	volume = {32},
	shorttitle = {Making {AI} {Forget} {You}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/cb79f8fa58b91d3af6c9c991f63962d3-Abstract.html},
	urldate = {2022-06-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ginart, Antonio and Guan, Melody and Valiant, Gregory and Zou, James Y},
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{guAsymmetricDeepCrossmodal2019,
	address = {Faro, Portugal},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Asymmetric {Deep} {Cross}-modal {Hashing}},
	volume = {11540},
	url = {https://doi.org/10.1007/978-3-030-22750-0_4},
	doi = {10/ghhxhv},
	publisher = {Springer},
	author = {Gu, Jingzi and Zhang, Jinchao and Lin, Zheng and Li, Bo and Wang, Weiping and Meng, Dan},
	editor = {Rodrigues, João M. F. and Cardoso, Pedro J. S. and Monteiro, Jânio M. and Lam, Roberto and Krzhizhanovskaya, Valeria V. and Lees, Michael Harold and Dongarra, Jack J. and Sloot, Peter M. A.},
	year = {2019},
	keywords = {Untagged},
	pages = {41--54},
}

@inproceedings{haeusserAssociativeDeepClustering2019,
	address = {Cham},
	title = {Associative {Deep} {Clustering}: {Training} a {Classification} {Network} with {No} {Labels}},
	isbn = {978-3-030-12939-2},
	abstract = {We propose a novel end-to-end clustering training schedule for neural networks that is direct, i.e. the output is a probability distribution over cluster memberships. A neural network maps images to embeddings. We introduce centroid variables that have the same shape as image embeddings. These variables are jointly optimized with the network’s parameters. This is achieved by a cost function that associates the centroid variables with embeddings of input images. Finally, an additional layer maps embeddings to logits, allowing for the direct estimation of the respective cluster membership. Unlike other methods, this does not require any additional classifier to be trained on the embeddings in a separate step. The proposed approach achieves state-of-the-art results in unsupervised classification and we provide an extensive ablation study to demonstrate its capabilities.},
	booktitle = {Pattern {Recognition}},
	publisher = {Springer International Publishing},
	author = {Haeusser, Philip and Plapp, Johannes and Golkov, Vladimir and Aljalbout, Elie and Cremers, Daniel},
	editor = {Brox, Thomas and Bruhn, Andrés and Fritz, Mario},
	year = {2019},
	keywords = {Untagged},
	pages = {18--32},
}

@inproceedings{halder_physics-based_2019,
	address = {Seoul, Korea (South)},
	title = {Physics-{Based} {Rendering} for {Improving} {Robustness} to {Rain}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010641/},
	doi = {10.1109/ICCV.2019.01030},
	abstract = {To improve the robustness to rain, we present a physicallybased rain rendering pipeline for realistically inserting rain into clear weather images. Our rendering relies on a physical particle simulator, an estimation of the scene lighting and an accurate rain photometric modeling to augment images with arbitrary amount of realistic rain or fog. We validate our rendering with a user study, proving our rain is judged 40\% more realistic that state-of-the-art. Using our generated weather augmented Kitti and Cityscapes dataset, we conduct a thorough evaluation of deep object detection and semantic segmentation algorithms and show that their performance decreases in degraded weather, on the order of 15\% for object detection and 60\% for semantic segmentation. Furthermore, we show reﬁning existing networks with our augmented images improves the robustness of both object detection and semantic segmentation algorithms. We experiment on nuScenes and measure an improvement of 15\% for object detection and 35\% for semantic segmentation compared to original rainy performance. Augmented databases and code are available on the project page.},
	language = {en},
	urldate = {2022-10-27},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Halder, Shirsendu and Lalonde, Jean-Francois and Charette, Raoul De},
	month = oct,
	year = {2019},
	keywords = {Untagged},
	pages = {10202--10211},
}

@inproceedings{heNewBenchmarkApproach2019,
	address = {Nice, France},
	title = {A {New} {Benchmark} and {Approach} for {Fine}-grained {Cross}-media {Retrieval}},
	url = {https://doi.org/10.1145/3343031.3350974},
	doi = {10/ggzw66},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {He, Xiangteng and Peng, Yuxin and Xie, Liu},
	editor = {Amsaleg, Laurent and Huet, Benoit and Larson, Martha A. and Gravier, Guillaume and Hung, Hayley and Ngo, Chong-Wah and Ooi, Wei Tsang},
	year = {2019},
	keywords = {Untagged},
	pages = {1740--1748},
}

@inproceedings{heBoundingBoxRegression2019,
	address = {Long Beach, CA, USA},
	title = {Bounding {Box} {Regression} {With} {Uncertainty} for {Accurate} {Object} {Detection}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953889/},
	doi = {10/ghv3gp},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {He, Yihui and Zhu, Chenchen and Wang, Jianren and Savvides, Marios and Zhang, Xiangyu},
	year = {2019},
	keywords = {Untagged},
	pages = {2883--2892},
}

@article{heAttGANFacialAttribute2019,
	title = {{AttGAN}: {Facial} {Attribute} {Editing} by {Only} {Changing} {What} {You} {Want}},
	volume = {28},
	url = {https://doi.org/10.1109/TIP.2019.2916751},
	doi = {10/ggvxm2},
	number = {11},
	journal = {IEEE Trans. Image Process.},
	author = {He, Zhenliang and Zuo, Wangmeng and Kan, Meina and Shan, Shiguang and Chen, Xilin},
	year = {2019},
	keywords = {Untagged},
	pages = {5464--5478},
}

@inproceedings{henriquesSmallStepsGiant2019,
	address = {Seoul, Korea},
	title = {Small {Steps} and {Giant} {Leaps}: {Minimal} {Newton} {Solvers} for {Deep} {Learning}},
	url = {https://doi.org/10.1109/ICCV.2019.00486},
	doi = {10/ghfh4t},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Henriques, João F. and Ehrhardt, Sébastien and Albanie, Samuel and Vedaldi, Andrea},
	year = {2019},
	keywords = {Untagged},
	pages = {4762--4771},
}

@article{hjelmLearningDeepRepresentations2019,
	title = {Learning deep representations by mutual information estimation and maximization},
	url = {http://arxiv.org/abs/1808.06670},
	abstract = {This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can signiﬁcantly improve a representation’s suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classiﬁcation tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards ﬂexible formulations of representation learning objectives for speciﬁc end-goals.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1808.06670 [cs, stat]},
	author = {Hjelm, R. Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
	month = feb,
	year = {2019},
	note = {arXiv: 1808.06670},
	keywords = {Untagged},
}

@inproceedings{hossainActiveDeepLearning2019,
	address = {Anchorage AK USA},
	title = {Active {Deep} {Learning} for {Activity} {Recognition} with {Context} {Aware} {Annotator} {Selection}},
	isbn = {978-1-4503-6201-6},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330688},
	doi = {10.1145/3292500.3330688},
	abstract = {Machine learning models are bounded by the credibility of ground truth data used for both training and testing. Regardless of the problem domain, this ground truth annotation is objectively manual and tedious as it needs considerable amount of human intervention. With the advent of Active Learning with multiple annotators, the burden can be somewhat mitigated by actively acquiring labels of most informative data instances. However, multiple annotators with varying degrees of expertise poses new set of challenges in terms of quality of the label received and availability of the annotator. Due to limited amount of ground truth information addressing the variabilities of Activity of Daily Living (ADLs), activity recognition models using wearable and mobile devices are still not robust enough for real-world deployment. In this paper, we rst propose an active learning combined deep model which updates its network parameters based on the optimization of a joint loss function. We then propose a novel annotator selection model by exploiting the relationships among the users while considering their heterogeneity with respect to their expertise, physical and spatial context. Our proposed model leverages model-free deep reinforcement learning in a partially observable environment setting to capture the actionreward interaction among multiple annotators. Our experiments in real-world settings exhibit that our active deep model converges to optimal accuracy with fewer labeled instances and achieves ≈8\% improvement in accuracy in fewer iterations.},
	language = {en},
	urldate = {2021-04-09},
	booktitle = {{ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Hossain, H M Sajjad and Roy, Nirmalya},
	year = {2019},
	keywords = {Untagged},
	pages = {1862--1870},
}

@article{hossainComprehensiveSurveyDeep2019,
	title = {A {Comprehensive} {Survey} of {Deep} {Learning} for {Image} {Captioning}},
	volume = {51},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3295748},
	doi = {10/ghdkft},
	abstract = {Generating a description of an image is called image captioning. Image captioning requires recognizing the important objects, their attributes, and their relationships in an image. It also needs to generate syntactically and semantically correct sentences. Deep-learning-based techniques are capable of handling the complexities and challenges of image captioning. In this survey article, we aim to present a comprehensive review of existing deep-learning-based image captioning techniques. We discuss the foundation of the techniques to analyze their performances, strengths, and limitations. We also discuss the datasets and the evaluation metrics popularly used in deep-learning-based automatic image captioning.},
	number = {6},
	journal = {ACM Computing Surveys},
	author = {Hossain, MD. Zakir and Sohel, Ferdous and Shiratuddin, Mohd Fairuz and Laga, Hamid},
	month = feb,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Untagged},
}

@inproceedings{huAreYouLooking2019,
	address = {Florence, Italy},
	title = {Are {You} {Looking}? {Grounding} to {Multiple} {Modalities} in {Vision}-and-{Language} {Navigation}},
	shorttitle = {Are {You} {Looking}?},
	url = {https://www.aclweb.org/anthology/P19-1655},
	doi = {10/gnbckb},
	abstract = {Vision-and-Language Navigation (VLN) requires grounding instructions, such as turn right and stop at the door, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. stop at the door might ground into visual objects, while turn right might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-ofthe-art models on the VLN task.},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Hu, Ronghang and Fried, Daniel and Rohrbach, Anna and Klein, Dan and Darrell, Trevor and Saenko, Kate},
	year = {2019},
	keywords = {Untagged},
	pages = {6551--6557},
}

@article{huWeaklySupervisedBilinear2019,
	title = {Weakly {Supervised} {Bilinear} {Attention} {Network} for {Fine}-{Grained} {Visual} {Classification}},
	url = {http://arxiv.org/abs/1808.02152},
	abstract = {In ﬁne-grained visual classiﬁcation task, objects usually share similar geometric structure but present different part distribution and variant local features. Therefore, localizing and extracting discriminative local features play a crucial role in obtaining accurate performance. Existing work that ﬁrst locates speciﬁc several object parts and then extract further local features either require additional location annotation or need to train multiple independent networks. In this paper. We propose Weakly Supervised Local Attention Network (WS-LAN) to solve the problem, which jointly generates a great many attention maps (region-ofinterest maps) to indicate the location of object parts and extract sequential local features by Local Attention Pooling (LAP). Besides, we adopt attention center loss and attention dropout so that each attention map will focus on a unique object part. WS-LAN can be trained end-to-end and achieves the state-of-the-art performance on multiple ﬁnegrained classiﬁcation datasets, including CUB-200-2011, Stanford Car and FGVC-Aircraft, which demonstrated its effectiveness.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1808.02152 [cs]},
	author = {Hu, Tao and Xu, Jizheng and Huang, Cong and Qi, Honggang and Huang, Qingming and Lu, Yan},
	month = feb,
	year = {2019},
	note = {arXiv: 1808.02152},
	keywords = {Untagged},
}

@article{huangMultimodalDiscriminativeModel2019,
	title = {Multi-modal {Discriminative} {Model} for {Vision}-and-{Language} {Navigation}},
	url = {https://arxiv.org/abs/1905.13358v1},
	abstract = {Vision-and-Language Navigation (VLN) is a natural language grounding task where agents have to interpret natural language instructions in the context of visual scenes in a dynamic environment to achieve prescribed navigation goals. Successful agents must have the ability to parse natural language of varying linguistic styles, ground them in potentially unfamiliar scenes, plan and react with ambiguous environmental feedback. Generalization ability is limited by the amount of human annotated data. In particular, {\textbackslash}emph\{paired\} vision-language sequence data is expensive to collect. We develop a discriminator that evaluates how well an instruction explains a given path in VLN task using multi-modal alignment. Our study reveals that only a small fraction of the high-quality augmented data from {\textbackslash}citet\{Fried:2018:Speaker\}, as scored by our discriminator, is useful for training VLN agents with similar performance on previously unseen environments. We also show that a VLN agent warm-started with pre-trained components from the discriminator outperforms the benchmark success rates of 35.5 by 10{\textbackslash}\% relative measure on previously unseen environments.},
	language = {en},
	urldate = {2021-11-03},
	author = {Huang, Haoshuo and Jain, Vihan and Mehta, Harsh and Baldridge, Jason and Ie, Eugene},
	month = may,
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{ilyas_adversarial_2019,
	title = {Adversarial {Examples} {Are} {Not} {Bugs}, {They} {Are} {Features}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/e2c420d928d4bf8ce0ff2ec19b371514-Abstract.html},
	abstract = {Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features (derived from patterns in the data distribution) that are highly predictive, yet brittle and (thus) incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a \{{\textbackslash}em misalignment\} between the (human-specified) notion of robustness and the inherent geometry of the data.},
	urldate = {2022-11-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
	year = {2019},
	keywords = {Untagged},
}

@article{irvineNeuralNetworkEnsembles2019,
	title = {Neural {Network} {Ensembles} for {Sensor}-{Based} {Human} {Activity} {Recognition} {Within} {Smart} {Environments}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/1/216},
	doi = {10.3390/s20010216},
	abstract = {In this paper, we focus on data-driven approaches to human activity recognition (HAR). Data-driven approaches rely on good quality data during training, however, a shortage of high quality, large-scale, and accurately annotated HAR datasets exists for recognizing activities of daily living (ADLs) within smart environments. The contributions of this paper involve improving the quality of an openly available HAR dataset for the purpose of data-driven HAR and proposing a new ensemble of neural networks as a data-driven HAR classiﬁer. Speciﬁcally, we propose a homogeneous ensemble neural network approach for the purpose of recognizing activities of daily living within a smart home setting. Four base models were generated and integrated using a support function fusion method which involved computing an output decision score for each base classiﬁer. The contribution of this work also involved exploring several approaches to resolving conﬂicts between the base models. Experimental results demonstrated that distributing data at a class level greatly reduces the number of conﬂicts that occur between the base models, leading to an increased performance prior to the application of conﬂict resolution techniques. Overall, the best HAR performance of 80.39\% was achieved through distributing data at a class level in conjunction with a conﬂict resolution approach, which involved calculating the diﬀerence between the highest and second highest predictions per conﬂicting model and awarding the ﬁnal decision to the model with the highest diﬀerential value.},
	language = {en},
	number = {1},
	urldate = {2021-06-04},
	journal = {Sensors},
	author = {Irvine, Naomi and Nugent, Chris and Zhang, Shuai and Wang, Hui and Ng, Wing W. Y.},
	month = dec,
	year = {2019},
	keywords = {Untagged},
	pages = {216},
}

@article{jainMakingLastIterate2019,
	title = {Making the {Last} {Iterate} of {SGD} {Information} {Theoretically} {Optimal}},
	url = {http://arxiv.org/abs/1904.12443},
	abstract = {Stochastic gradient descent (SGD) is one of the most widely used algorithms for large scale optimization problems. While classical theoretical analysis of SGD for convex problems studies (suffix) {\textbackslash}emph\{averages\} of iterates and obtains information theoretically optimal bounds on suboptimality, the {\textbackslash}emph\{last point\} of SGD is, by far, the most preferred choice in practice. The best known results for last point of SGD {\textbackslash}cite\{shamir2013stochastic\} however, are suboptimal compared to information theoretic lower bounds by a \${\textbackslash}log T\$ factor, where \$T\$ is the number of iterations. {\textbackslash}cite\{harvey2018tight\} shows that in fact, this additional \${\textbackslash}log T\$ factor is tight for standard step size sequences of \${\textbackslash}OTheta\{{\textbackslash}frac\{1\}\{{\textbackslash}sqrt\{t\}\}\}\$ and \${\textbackslash}OTheta\{{\textbackslash}frac\{1\}\{t\}\}\$ for non-strongly convex and strongly convex settings, respectively. Similarly, even for subgradient descent (GD) when applied to non-smooth, convex functions, the best known step-size sequences still lead to \$O({\textbackslash}log T)\$-suboptimal convergence rates (on the final iterate). The main contribution of this work is to design new step size sequences that enjoy information theoretically optimal bounds on the suboptimality of {\textbackslash}emph\{last point\} of SGD as well as GD. We achieve this by designing a modification scheme, that converts one sequence of step sizes to another so that the last point of SGD/GD with modified sequence has the same suboptimality guarantees as the average of SGD/GD with original sequence. We also show that our result holds with high-probability. We validate our results through simulations which demonstrate that the new step size sequence indeed improves the final iterate significantly compared to the standard step size sequences.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1904.12443 [cs, math]},
	author = {Jain, Prateek and Nagaraj, Dheeraj and Netrapalli, Praneeth},
	month = may,
	year = {2019},
	note = {arXiv: 1904.12443},
	keywords = {Untagged},
}

@inproceedings{jangLearningWhatWhere2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Learning {What} and {Where} to {Transfer}},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/jang19b.html},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}, {ICML} 2019, 9-15 {June} 2019, {Long} {Beach}, {California}, {USA}},
	publisher = {PMLR},
	author = {Jang, Yunhun and Lee, Hankook and Hwang, Sung Ju and Shin, Jinwoo},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	year = {2019},
	keywords = {Untagged},
	pages = {3030--3039},
}

@inproceedings{jayaramanEvaluatingDifferentiallyPrivate2019,
	title = {Evaluating {Differentially} {Private} {Machine} {Learning} in {Practice}},
	isbn = {978-1-939133-06-9},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/jayaraman},
	language = {en},
	urldate = {2022-03-23},
	author = {Jayaraman, Bargav and Evans, David},
	year = {2019},
	keywords = {Untagged},
	pages = {1895--1912},
}

@inproceedings{jessethomasonShiftingBaselineSingle2019,
	address = {Minneapolis, Minnesota},
	title = {Shifting the {Baseline}: {Single} {Modality} {Performance} on {Visual} {Navigation} \& {QA}},
	shorttitle = {Shifting the {Baseline}},
	url = {https://aclanthology.org/N19-1197},
	doi = {10/gnbcg9},
	abstract = {We demonstrate the surprising strength of unimodal baselines in multimodal domains, and make concrete recommendations for best practices in future research. Where existing work often compares against random or majority class baselines, we argue that unimodal approaches better capture and reflect dataset biases and therefore provide an important comparison when assessing the performance of multimodal techniques. We present unimodal ablations on three recent datasets in visual navigation and QA, seeing an up to 29\% absolute gain in performance over published baselines.},
	urldate = {2021-11-03},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {{Jesse Thomason} and Gordon, Daniel and Bisk, Yonatan},
	month = jun,
	year = {2019},
	keywords = {Untagged},
	pages = {1977--1983},
}

@article{jiaMemGuardDefendingBlackBox2019,
	title = {{MemGuard}: {Defending} against {Black}-{Box} {Membership} {Inference} {Attacks} via {Adversarial} {Examples}},
	shorttitle = {{MemGuard}},
	url = {http://arxiv.org/abs/1909.10594},
	abstract = {In a membership inference attack, an attacker aims to infer whether a data sample is in a target classifier's training dataset or not. Specifically, given a black-box access to the target classifier, the attacker trains a binary classifier, which takes a data sample's confidence score vector predicted by the target classifier as an input and predicts the data sample to be a member or non-member of the target classifier's training dataset. Membership inference attacks pose severe privacy and security threats to the training dataset. Most existing defenses leverage differential privacy when training the target classifier or regularize the training process of the target classifier. These defenses suffer from two key limitations: 1) they do not have formal utility-loss guarantees of the confidence score vectors, and 2) they achieve suboptimal privacy-utility tradeoffs. In this work, we propose MemGuard, the first defense with formal utility-loss guarantees against black-box membership inference attacks. Instead of tampering the training process of the target classifier, MemGuard adds noise to each confidence score vector predicted by the target classifier. Our key observation is that attacker uses a classifier to predict member or non-member and classifier is vulnerable to adversarial examples. Based on the observation, we propose to add a carefully crafted noise vector to a confidence score vector to turn it into an adversarial example that misleads the attacker's classifier. Our experimental results on three datasets show that MemGuard can effectively defend against membership inference attacks and achieve better privacy-utility tradeoffs than existing defenses. Our work is the first one to show that adversarial examples can be used as defensive mechanisms to defend against membership inference attacks.},
	urldate = {2022-03-21},
	journal = {arXiv:1909.10594 [cs]},
	author = {Jia, Jinyuan and Salem, Ahmed and Backes, Michael and Zhang, Yang and Gong, Neil Zhenqiang},
	month = dec,
	year = {2019},
	note = {arXiv: 1909.10594},
	keywords = {Untagged},
}

@article{jiangEvaluationMetricHashing2019,
	title = {On the {Evaluation} {Metric} for {Hashing}},
	volume = {abs/1905.10951},
	url = {http://arxiv.org/abs/1905.10951},
	journal = {arXiv},
	author = {Jiang, Qing-Yuan and Li, Ming-Wei and Li, Wu-Jun},
	year = {2019},
	note = {\_eprint: 1905.10951},
	keywords = {Untagged},
}

@article{jiangDisentangledRepresentationLearning2019,
	title = {Disentangled {Representation} {Learning} for {3D} {Face} {Shape}},
	url = {http://arxiv.org/abs/1902.09887},
	abstract = {In this paper, we present a novel strategy to design disentangled 3D face shape representation. Speciﬁcally, a given 3D face shape is decomposed into identity part and expression part, which are both encoded in a nonlinear way. To solve this problem, we propose an attribute decomposition framework for 3D face mesh. To better represent face shapes which are usually nonlinear deformed between each other, the face shapes are represented by a vertex based deformation representation rather than Euclidean coordinates. The experimental results demonstrate that our method has better performance than existing methods on decomposing the identity and expression parts. Moreover, more natural expression transfer results can be achieved with our method than existing methods.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1902.09887 [cs]},
	author = {Jiang, Zi-Hang and Wu, Qianyi and Chen, Keyu and Zhang, Juyong},
	month = mar,
	year = {2019},
	note = {arXiv: 1902.09887},
	keywords = {Untagged},
}

@inproceedings{jinAPEGANAdversarialPerturbation2019,
	address = {Brighton, United Kingdom},
	title = {{APE}-{GAN}: {Adversarial} {Perturbation} {Elimination} with {GAN}},
	url = {https://doi.org/10.1109/ICASSP.2019.8683044},
	doi = {10/ghwnhq},
	booktitle = {{IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Jin, Guoqing and Shen, Shiwei and Zhang, Dongming and Dai, Feng and Zhang, Yongdong},
	year = {2019},
	keywords = {Untagged},
	pages = {3842--3846},
}

@article{jinDeepSemanticPreservingOrdinal2019,
	title = {Deep {Semantic}-{Preserving} {Ordinal} {Hashing} for {Cross}-{Modal} {Similarity} {Search}},
	volume = {30},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/8478207/},
	doi = {10/ghhxhx},
	number = {5},
	urldate = {2020-11-05},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Jin, Lu and Li, Kai and Li, Zechao and Xiao, Fu and Qi, Guo-Jun and Tang, Jinhui},
	month = may,
	year = {2019},
	keywords = {Untagged},
	pages = {1429--1440},
}

@article{kairouzAdvancesOpenProblems2019,
	title = {Advances and {Open} {Problems} in {Federated} {Learning}},
	url = {https://arxiv.org/abs/1912.04977v3},
	abstract = {Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.},
	language = {en},
	urldate = {2021-11-25},
	author = {Kairouz, Peter and McMahan, H. Brendan and Avent, Brendan and Bellet, Aurélien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D'Oliveira, Rafael G. L. and Eichner, Hubert and Rouayheb, Salim El and Evans, David and Gardner, Josh and Garrett, Zachary and Gascón, Adrià and Ghazi, Badih and Gibbons, Phillip B. and Gruteser, Marco and Harchaoui, Zaid and He, Chaoyang and He, Lie and Huo, Zhouyuan and Hutchinson, Ben and Hsu, Justin and Jaggi, Martin and Javidi, Tara and Joshi, Gauri and Khodak, Mikhail and Konečný, Jakub and Korolova, Aleksandra and Koushanfar, Farinaz and Koyejo, Sanmi and Lepoint, Tancrède and Liu, Yang and Mittal, Prateek and Mohri, Mehryar and Nock, Richard and Özgür, Ayfer and Pagh, Rasmus and Raykova, Mariana and Qi, Hang and Ramage, Daniel and Raskar, Ramesh and Song, Dawn and Song, Weikang and Stich, Sebastian U. and Sun, Ziteng and Suresh, Ananda Theertha and Tramèr, Florian and Vepakomma, Praneeth and Wang, Jianyu and Xiong, Li and Xu, Zheng and Yang, Qiang and Yu, Felix X. and Yu, Han and Zhao, Sen},
	month = dec,
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{kang_transfer_2019,
	title = {Transfer of {Adversarial} {Robustness} {Between} {Perturbation} {Types}},
	booktitle = {arxiv:1905.01034},
	author = {Kang, Daniel and Sun, Yi and Brown, Tom and Hendrycks, Dan and Steinhardt, Jacob},
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{karamanUnsupervisedRankPreservingHashing2019,
	title = {Unsupervised {Rank}-{Preserving} {Hashing} for {Large}-{Scale} {Image} {Retrieval}},
	url = {https://doi.org/10.1145/3323873.3325038},
	doi = {10/ghwnq2},
	booktitle = {Proceedings of the 2019 on {International} {Conference} on {Multimedia} {Retrieval}, {ICMR} 2019, {Ottawa}, {ON}, {Canada}, {June} 10-13, 2019},
	publisher = {ACM},
	author = {Karaman, Svebor and Lin, Xudong and Hu, Xuefeng and Chang, Shih-Fu},
	editor = {El-Saddik, Abdulmotaleb and Bimbo, Alberto Del and Zhang, Zhongfei and Hauptmann, Alexander G. and Candan, K. Selçuk and Bertini, Marco and Xie, Lexing and Wei, Xiao-Yong},
	year = {2019},
	keywords = {Untagged},
	pages = {192--196},
}

@inproceedings{keTacticalRewindSelfCorrection2019,
	address = {Long Beach, CA, USA},
	title = {Tactical {Rewind}: {Self}-{Correction} via {Backtracking} in {Vision}-{And}-{Language} {Navigation}},
	isbn = {978-1-72813-293-8},
	shorttitle = {Tactical {Rewind}},
	url = {https://ieeexplore.ieee.org/document/8953538/},
	doi = {10/ggjfsv},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Ke, Liyiming and Li, Xiujun and Bisk, Yonatan and Holtzman, Ari and Gan, Zhe and Liu, Jingjing and Gao, Jianfeng and Choi, Yejin and Srinivasa, Siddhartha},
	month = jun,
	year = {2019},
	keywords = {Untagged},
	pages = {6734--6742},
}

@inproceedings{Kirillov_2019_CVPR,
	title = {Panoptic segmentation},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollar, Piotr},
	month = jun,
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{kitaevReformerEfficientTransformer2019,
	title = {Reformer: {The} {Efficient} {Transformer}},
	shorttitle = {Reformer},
	url = {https://openreview.net/forum?id=rkgNKkHtvB},
	abstract = {Efficient Transformer with locality-sensitive hashing and reversible layers},
	language = {en},
	urldate = {2021-10-19},
	author = {Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
	month = sep,
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{kleinEndToEndSupervisedProduct2019,
	address = {Long Beach, CA, USA},
	title = {End-{To}-{End} {Supervised} {Product} {Quantization} for {Image} {Search} and {Retrieval}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953697/},
	doi = {10/ghv438},
	abstract = {Product Quantization, a dictionary based hashing method, is one of the leading unsupervised hashing techniques. While it ignores the labels, it harnesses the features to construct look up tables that can approximate the feature space. In recent years, several works have achieved state of the art results on hashing benchmarks by learning binary representations in a supervised manner. This work presents Deep Product Quantization (DPQ), a technique that leads to more accurate retrieval and classiﬁcation than the latest state of the art methods, while having similar computational complexity and memory footprint as the Product Quantization method. To our knowledge, this is the ﬁrst work to introduce a dictionary-based representation that is inspired by Product Quantization and which is learned end-to-end, and thus beneﬁts from the supervised signal. DPQ explicitly learns soft and hard representations to enable an efﬁcient and accurate asymmetric search, by using a straightthrough estimator. Our method obtains state of the art results on an extensive array of retrieval and classiﬁcation experiments.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Klein, Benjamin and Wolf, Lior},
	year = {2019},
	keywords = {Untagged},
	pages = {5036--5045},
}

@article{koskelaLearningRateAdaptation2019,
	title = {Learning {Rate} {Adaptation} for {Federated} and {Differentially} {Private} {Learning}},
	url = {http://arxiv.org/abs/1809.03832},
	abstract = {We propose an algorithm for the adaptation of the learning rate for stochastic gradient descent (SGD) that avoids the need for validation set use. The idea for the adaptiveness comes from the technique of extrapolation: to get an estimate for the error against the gradient ﬂow which underlies SGD, we compare the result obtained by one full step and two half-steps. The algorithm is applied in two separate frameworks: federated and differentially private learning. Using examples of deep neural networks we empirically show that the adaptive algorithm is competitive with manually tuned commonly used optimisation methods for differentially privately training. We also show that it works robustly in the case of federated learning unlike commonly used optimisation methods.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1809.03832 [cs, stat]},
	author = {Koskela, Antti and Honkela, Antti},
	month = may,
	year = {2019},
	note = {arXiv: 1809.03832},
	keywords = {Untagged},
}

@article{kuoInterpretableConvolutionalNeural2019,
	title = {Interpretable convolutional neural networks via feedforward design},
	volume = {60},
	url = {https://doi.org/10.1016/j.jvcir.2019.03.010},
	doi = {10/ggbzqd},
	journal = {J. Vis. Commun. Image Represent.},
	author = {Kuo, C.-C. Jay and Zhang, Min and Li, Siyang and Duan, Jiali and Chen, Yueru},
	year = {2019},
	keywords = {Untagged},
	pages = {346--359},
}

@inproceedings{kurmiAttendingDiscriminativeCertainty2019,
	address = {Long Beach, CA, USA},
	title = {Attending to {Discriminative} {Certainty} for {Domain} {Adaptation}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953446/},
	doi = {10/ghv248},
	abstract = {In this paper, we aim to solve for unsupervised domain adaptation of classiﬁers where we have access to label information for the source domain while these are not available for a target domain. While various methods have been proposed for solving these including adversarial discriminator based methods, most approaches have focused on the entire image based domain adaptation. In an image, there would be regions that can be adapted better, for instance, the foreground object may be similar in nature. To obtain such regions, we propose methods that consider the probabilistic certainty estimate of various regions and specify focus on these during classiﬁcation for adaptation. We observe that just by incorporating the probabilistic certainty of the discriminator while training the classiﬁer, we are able to obtain state of the art results on various datasets as compared against all the recent methods. We provide a thorough empirical analysis of the method by providing ablation analysis, statistical signiﬁcance test, and visualization of the attention maps and t-SNE embeddings. These evaluations convincingly demonstrate the effectiveness of the proposed approach.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Kurmi, Vinod Kumar and Kumar, Shanu and Namboodiri, Vinay P.},
	month = jun,
	year = {2019},
	keywords = {Untagged},
	pages = {491--500},
}

@inproceedings{laputSensingFineGrainedHand2019,
	address = {Glasgow Scotland Uk},
	title = {Sensing {Fine}-{Grained} {Hand} {Activity} with {Smartwatches}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300568},
	doi = {10.1145/3290605.3300568},
	abstract = {Capturing ﬁne-grained hand activity could make computational experiences more powerful and contextually aware. Indeed, philosopher Immanuel Kant argued, "the hand is the visible part of the brain." However, most prior work has focused on detecting whole-body activities, such as walking, running and bicycling. In this work, we explore the feasibility of sensing hand activities from commodity smartwatches, which are the most practical vehicle for achieving this vision. Our investigations started with a 50 participant, in-the-wild study, which captured hand activity labels over nearly 1000 worn hours. We then studied this data to scope our research goals and inform our technical approach. We conclude with a second, in-lab study that evaluates our classiﬁcation stack, demonstrating 95.2\% accuracy across 25 hand activities. Our work highlights an underutilized, yet highly complementary contextual channel that could unlock a wide range of promising applications.},
	language = {en},
	urldate = {2021-06-04},
	booktitle = {{ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Laput, Gierad and Harrison, Chris},
	year = {2019},
	keywords = {Untagged},
	pages = {1--13},
}

@inproceedings{lee_tight_2019,
	title = {Tight {Certificates} of {Adversarial} {Robustness} for {Randomly} {Smoothed} {Classifiers}},
	volume = {32},
	shorttitle = {l0},
	url = {https://papers.nips.cc/paper/2019/hash/fa2e8c4385712f9a1d24c363a2cbe5b8-Abstract.html},
	urldate = {2022-06-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lee, Guang-He and Yuan, Yang and Chang, Shiyu and Jaakkola, Tommi},
	year = {2019},
	keywords = {Untagged},
	pages = {4911--4922},
}

@inproceedings{leeSetTransformerFramework2019,
	title = {Set {Transformer}: {A} {Framework} for {Attention}-based {Permutation}-{Invariant} {Neural} {Networks}},
	shorttitle = {Set {Transformer}},
	url = {https://proceedings.mlr.press/v97/lee19d.html},
	abstract = {Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.},
	language = {en},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {3744--3753},
}

@inproceedings{li_certified_2019,
	title = {Certified {Adversarial} {Robustness} with {Additive} {Noise}},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/335cd1b90bfa4ee70b39d08a4ae0cf2d-Abstract.html},
	abstract = {The existence of adversarial data examples has drawn significant attention in the deep-learning community; such data are seemingly minimally perturbed relative to the original data, but lead to very different outputs from a deep-learning algorithm. Although a significant body of work on developing defense models has been developed, most such models are heuristic and are often vulnerable to adaptive attacks. Defensive methods that provide theoretical robustness guarantees have been studied intensively, yet most fail to obtain non-trivial robustness when a large-scale model and data are present. To address these limitations, we introduce a framework that is scalable and provides certified bounds on the norm of the input manipulation for constructing adversarial examples. We establish a connection between robustness against adversarial perturbation and additive random noise, and propose a training strategy that can significantly improve the certified bounds. Our evaluation on MNIST, CIFAR-10 and ImageNet suggests that our method is scalable to complicated models and large data sets, while providing competitive robustness to state-of-the-art provable defense methods.},
	urldate = {2022-06-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Bai and Chen, Changyou and Wang, Wenlin and Carin, Lawrence},
	year = {2019},
	keywords = {Untagged},
	pages = {9459--9469},
}

@article{liFedMDHeterogenousFederated2019,
	title = {{FedMD}: {Heterogenous} {Federated} {Learning} via {Model} {Distillation}},
	shorttitle = {{FedMD}},
	url = {https://arxiv.org/abs/1910.03581v1},
	abstract = {Federated learning enables the creation of a powerful centralized model without compromising data privacy of multiple participants. While successful, it does not incorporate the case where each participant independently designs its own model. Due to intellectual property concerns and heterogeneous nature of tasks and data, this is a widespread requirement in applications of federated learning to areas such as health care and AI as a service. In this work, we use transfer learning and knowledge distillation to develop a universal framework that enables federated learning when each agent owns not only their private data, but also uniquely designed models. We test our framework on the MNIST/FEMNIST dataset and the CIFAR10/CIFAR100 dataset and observe fast improvement across all participating models. With 10 distinct participants, the final test accuracy of each model on average receives a 20\% gain on top of what's possible without collaboration and is only a few percent lower than the performance each model would have obtained if all private datasets were pooled and made directly available for all participants.},
	language = {en},
	urldate = {2021-12-06},
	author = {Li, Daliang and Wang, Junpu},
	month = oct,
	year = {2019},
	keywords = {Untagged},
}

@article{liAnomalyDetectionGenerative2019,
	title = {Anomaly {Detection} with {Generative} {Adversarial} {Networks} for {Multivariate} {Time} {Series}},
	url = {http://arxiv.org/abs/1809.04758},
	abstract = {Today’s Cyber-Physical Systems (CPSs) are large, complex, and afﬁxed with networked sensors and actuators that are targets for cyber-attacks. Conventional detection techniques are unable to deal with the increasingly dynamic and complex nature of the CPSs. On the other hand, the networked sensors and actuators generate large amounts of data streams that can be continuously monitored for intrusion events. Unsupervised machine learning techniques can be used to model the system behaviour and classify deviant behaviours as possible attacks. In this work, we proposed a novel Generative Adversarial Networks-based Anomaly Detection (GAN-AD) method for such complex networked CPSs. We used LSTM-RNN in our GAN to capture the distribution of the multivariate time series of the sensors and actuators under normal working conditions of a CPS. Instead of treating each sensor’s and actuator’s time series independently, we model the time series of multiple sensors and actuators in the CPS concurrently to take into account of potential latent interactions between them. To exploit both the generator and the discriminator of our GAN, we deployed the GAN-trained discriminator together with the residuals between generator-reconstructed data and the actual samples to detect possible anomalies in the complex CPS. We used our GAN-AD to distinguish abnormal attacked situations from normal working conditions for a complex six-stage Secure Water Treatment (SWaT) system. Experimental results showed that the proposed strategy is effective in identifying anomalies caused by various attacks with high detection rate and low false positive rate as compared to existing methods.},
	language = {en},
	urldate = {2021-06-04},
	journal = {arXiv:1809.04758 [cs, stat]},
	author = {Li, Dan and Chen, Dacheng and Goh, Jonathan and Ng, See-kiong},
	month = jan,
	year = {2019},
	note = {arXiv: 1809.04758},
	keywords = {Untagged},
}

@article{liDeepGCNsCanGCNs2019,
	title = {{DeepGCNs}: {Can} {GCNs} {Go} as {Deep} as {CNNs}?},
	shorttitle = {{DeepGCNs}},
	url = {http://arxiv.org/abs/1904.03751},
	abstract = {Convolutional Neural Networks (CNNs) achieve impressive performance in a wide variety of ﬁelds. Their success beneﬁted from a massive boost when very deep CNN models were able to be reliably trained. Despite their merits, CNNs fail to properly address problems with non-Euclidean data. To overcome this challenge, Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean data, borrow concepts from CNNs, and apply them in training. GCNs show promising results, but they are usually limited to very shallow models due to the vanishing gradient problem (see Figure 1). As a result, most state-of-the-art GCN models are no deeper than 3 or 4 layers. In this work, we present new ways to successfully train very deep GCNs. We do this by borrowing concepts from CNNs, speciﬁcally residual/dense connections and dilated convolutions, and adapting them to GCN architectures. Extensive experiments show the positive effect of these deep GCN frameworks. Finally, we use these new concepts to build a very deep 56-layer GCN, and show how it signiﬁcantly boosts performance (+3.7\% mIoU over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly beneﬁt from this work, as it opens up many opportunities for advancing GCN-based research.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1904.03751 [cs]},
	author = {Li, Guohao and Müller, Matthias and Thabet, Ali and Ghanem, Bernard},
	month = aug,
	year = {2019},
	note = {arXiv: 1904.03751},
	keywords = {Untagged},
}

@techreport{liGeneralizationErrorImplicit2019,
	address = {NJU},
	title = {Generalization {Error} and {Implicit} {Bias} of {Gradient} {Methods} for {Deep} {Learning}},
	language = {en},
	institution = {Tsinghua University},
	author = {Li, Jian},
	year = {2019},
	keywords = {Untagged},
	pages = {59},
}

@article{liTwostreamedNetworkEstimating2019,
	title = {A two-streamed network for estimating fine-scaled depth maps from single {RGB} images},
	volume = {186},
	url = {https://doi.org/10.1016/j.cviu.2019.06.002},
	doi = {10/ghwng9},
	journal = {Comput. Vis. Image Underst.},
	author = {Li, Jun and Yuce, Can and Klein, Reinhard and Yao, Angela},
	year = {2019},
	keywords = {Untagged},
	pages = {25--36},
}

@inproceedings{DBLP:conf/iccv/LiZLLF19,
	title = {Visual semantic reasoning for image-text matching},
	url = {https://doi.org/10.1109/ICCV.2019.00475},
	doi = {10.1109/ICCV.2019.00475},
	booktitle = {2019 {IEEE}/{CVF} international conference on computer vision, {ICCV} 2019, seoul, korea (south), october 27 - november 2, 2019},
	publisher = {IEEE},
	author = {Li, Kunpeng and Zhang, Yulun and Li, Kai and Li, Yuanyuan and Fu, Yun},
	year = {2019},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/iccv/LiZLLF19.bib
tex.timestamp: Tue, 17 Mar 2020 15:23:29 +0100},
	keywords = {Untagged},
	pages = {4653--4661},
}

@article{liFaceRecognitionLow2019,
	title = {Face {Recognition} in {Low} {Quality} {Images}: {A} {Survey}},
	shorttitle = {Face {Recognition} in {Low} {Quality} {Images}},
	url = {http://arxiv.org/abs/1805.11519},
	abstract = {Low-resolution face recognition (LRFR) has received increasing attention over the past few years. Its applications lie widely in the real-world environment when highresolution or high-quality images are hard to capture. One of the biggest demands for LRFR technologies is video surveillance. As the the number of surveillance cameras in the city increases, the videos that captured will need to be processed automatically. However, those videos or images are usually captured with large standoffs, arbitrary illumination condition, and diverse angles of view. Faces in these images are generally small in size. Several studies addressed this problem employed techniques like super resolution, deblurring, or learning a relationship between different resolution domains. In this paper, we provide a comprehensive review of approaches to low-resolution face recognition in the past ﬁve years. First, a general problem deﬁnition is given. Later, systematically analysis of the works on this topic is presented by catogory. In addition to describing the methods, we also focus on datasets and experiment settings. We further address the related works on unconstrained lowresolution face recognition and compare them with the result that use synthetic low-resolution data. Finally, we summarized the general limitations and speculate a priorities for the future effort.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1805.11519 [cs]},
	author = {Li, Pei and Prieto, Loreto and Mery, Domingo and Flynn, Patrick},
	month = mar,
	year = {2019},
	note = {arXiv: 1805.11519},
	keywords = {Untagged},
}

@article{liLowResolutionFaceRecognition2019,
	title = {On {Low}-{Resolution} {Face} {Recognition} in the {Wild}: {Comparisons} and {New} {Techniques}},
	volume = {14},
	url = {https://doi.org/10.1109/TIFS.2018.2890812},
	doi = {10/gf5ctr},
	number = {8},
	journal = {IEEE Trans. Inf. Forensics Secur.},
	author = {Li, Pei and Prieto, Loreto and Mery, Domingo and Flynn, Patrick J.},
	year = {2019},
	keywords = {Untagged},
	pages = {2000--2012},
}

@article{liSurveyFederatedLearning2019,
	title = {A {Survey} on {Federated} {Learning} {Systems}: {Vision}, {Hype} and {Reality} for {Data} {Privacy} and {Protection}},
	shorttitle = {A {Survey} on {Federated} {Learning} {Systems}},
	url = {https://arxiv.org/abs/1907.09693v6},
	abstract = {Federated learning has been a hot research topic in enabling the collaborative training of machine learning models among different organizations under the privacy restrictions. As researchers try to support more machine learning models with different privacy-preserving approaches, there is a requirement in developing systems and infrastructures to ease the development of various federated learning algorithms. Similar to deep learning systems such as PyTorch and TensorFlow that boost the development of deep learning, federated learning systems (FLSs) are equivalently important, and face challenges from various aspects such as effectiveness, efficiency, and privacy. In this survey, we conduct a comprehensive review on federated learning systems. To achieve smooth flow and guide future research, we introduce the definition of federated learning systems and analyze the system components. Moreover, we provide a thorough categorization for federated learning systems according to six different aspects, including data distribution, machine learning model, privacy mechanism, communication architecture, scale of federation and motivation of federation. The categorization can help the design of federated learning systems as shown in our case studies. By systematically summarizing the existing federated learning systems, we present the design factors, case studies, and future research opportunities.},
	language = {en},
	urldate = {2021-11-25},
	author = {Li, Qinbin and Wen, Zeyi and Wu, Zhaomin and Hu, Sixu and Wang, Naibo and Li, Yuan and Liu, Xu and He, Bingsheng},
	month = jul,
	year = {2019},
	keywords = {Untagged},
}

@article{liAttentiveNormalization2019,
	title = {Attentive {Normalization}},
	url = {http://arxiv.org/abs/1908.01259},
	abstract = {In state-of-the-art deep neural networks, both feature normalization and feature attention have become ubiquitous with signiﬁcant performance improvement shown in a vast amount of tasks. They are usually studied as separate modules, however. In this paper, we propose a light-weight integration between, and thus harness the best of, the two schema. We present Attentive Normalization (AN) which generalizes the common afﬁne transformation component in the vanilla feature normalization. Instead of learning a single afﬁne transformation, AN learns a mixture of afﬁne transformations and utilizes their weighted-sum as the ﬁnal afﬁne transformation applied to re-calibrate features in an instance-speciﬁc way. The weights are learned by leveraging feature attention. AN introduces negligible extra parameters and computational cost (i.e., light-weight). AN can be used as a drop-in replacement for any feature normalization technique which includes the afﬁne transformation component. In experiments, we test the proposed AN using three representative neural architectures (ResNets [11], MobileNets-v2 [36] and AOGNets [24]) in the ImageNet1000 classiﬁcation benchmark [34] and the MS-COCO 2107 object detection and instance segmentation benchmark [26]. AN obtains consistent performance improvement for different neural architectures in both benchmarks with absolute increase of top-1 accuracy in ImageNet-1000 between 0.5\% and 2.0\%, and absolute increase up to 1.8\% and 2.2\% for bounding box and mask AP in MS-COCO respectively. The source codes are publicly available 1.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1908.01259 [cs]},
	author = {Li, Xilai and Sun, Wei and Wu, Tianfu},
	month = nov,
	year = {2019},
	note = {arXiv: 1908.01259},
	keywords = {Untagged},
}

@inproceedings{li_implicit_2019,
	title = {Implicit {Bias} of {Gradient} {Descent} based {Adversarial} {Training} on {Separable} {Data}},
	url = {https://openreview.net/forum?id=HkgTTh4FDH},
	abstract = {Adversarial training is a principled approach for training robust neural networks. Despite of tremendous successes in practice, its theoretical properties still remain largely unexplored. In this paper, we provide new theoretical insights of gradient descent based adversarial training by studying its computational properties, specifically on its implicit bias. We take the binary classification task on linearly separable data as an illustrative example, where the loss asymptotically attains its infimum as the parameter diverges to infinity along certain directions. Specifically, we show that for any fixed iteration \$T\$, when the adversarial perturbation during training has proper bounded L2 norm, the classifier learned by gradient descent based adversarial training converges in direction to the maximum L2 norm margin classifier at the rate of \$O(1/{\textbackslash}sqrt\{T\vphantom{\{}\})\$, significantly faster than the rate \$O(1/{\textbackslash}log T\}\$ of training with clean data. In addition, when the adversarial perturbation during training has bounded Lq norm, the resulting classifier converges in direction to a maximum mixed-norm margin classifier, which has a natural interpretation of robustness, as being the maximum L2 norm margin classifier under worst-case bounded Lq norm perturbation to the data. Our findings provide theoretical backups for adversarial training that it indeed promotes robustness against adversarial perturbation.},
	language = {en},
	urldate = {2023-05-11},
	author = {Li, Yan and X.Fang, Ethan and Xu, Huan and Zhao, Tuo},
	month = dec,
	year = {2019},
	keywords = {Untagged},
}

@article{liangFederatedTransferReinforcement2019,
	title = {Federated {Transfer} {Reinforcement} {Learning} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/1910.06001},
	abstract = {Reinforcement learning (RL) is widely used in autonomous driving tasks and training RL models typically involves in a multi-step process: pre-training RL models on simulators, uploading the pre-trained model to real-life robots, and ﬁne-tuning the weight parameters on robot vehicles. This sequential process is extremely time-consuming and more importantly, knowledge from the ﬁne-tuned model stays local and can not be re-used or leveraged collaboratively. To tackle this problem, we present an online federated RL transfer process for real-time knowledge extraction where all the participant agents make corresponding actions with the knowledge learned by others, even when they are acting in very different environments. To validate the effectiveness of the proposed approach, we constructed a real-life collision avoidance system with Microsoft Airsim simulator and NVIDIA JetsonTX2 car agents, which cooperatively learn from scratch to avoid collisions in indoor environment with obstacle objects. We demonstrate that with the proposed framework, the simulator car agents can transfer knowledge to the RC cars in real-time, with 27\% increase in the average distance with obstacles and 42\% decrease in the collision counts.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1910.06001 [cs]},
	author = {Liang, Xinle and Liu, Yang and Chen, Tianjian and Liu, Ming and Yang, Qiang},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.06001},
	keywords = {Untagged},
}

@inproceedings{linDonUseLarge2019,
	title = {Don't {Use} {Large} {Mini}-batches, {Use} {Local} {SGD}},
	url = {https://openreview.net/forum?id=B1eyO1BFPr},
	abstract = {Mini-batch stochastic gradient methods (SGD) are state of the art for distributed training of deep neural networks. 
Drastic increases in the mini-batch sizes have lead to key efficiency and...},
	language = {en},
	urldate = {2022-03-04},
	author = {Lin, Tao and Stich, Sebastian U. and Patel, Kumar Kshitij and Jaggi, Martin},
	month = sep,
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{lin_pareto_2019,
	title = {Pareto {Multi}-{Task} {Learning}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/685bfde03eb646c27ed565881917c71c-Abstract.html},
	abstract = {Multi-task learning is a powerful method for solving multiple correlated tasks simultaneously. However, it is often impossible to find one single solution to optimize all the tasks, since different tasks might conflict with each other. Recently, a novel method is proposed to find one single Pareto optimal solution with good trade-off among different tasks by casting multi-task learning as multiobjective optimization. In this paper, we generalize this idea and propose a novel Pareto multi-task learning algorithm (Pareto MTL) to find a set of well-distributed Pareto solutions which can represent different trade-offs among different tasks. The proposed algorithm first formulates a multi-task learning problem as a multiobjective optimization problem, and then decomposes the multiobjective optimization problem into a set of constrained subproblems with different trade-off preferences. By solving these subproblems in parallel, Pareto MTL can find a set of well-representative Pareto optimal solutions with different trade-off among all tasks. Practitioners can easily select their preferred solution from these Pareto solutions, or use different trade-off solutions for different situations.  Experimental results confirm that the proposed algorithm can generate well-representative solutions and outperform some state-of-the-art algorithms on many multi-task learning applications.},
	urldate = {2022-07-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lin, Xi and Zhen, Hui-Ling and Li, Zhenhua and Zhang, Qing-Fu and Kwong, Sam},
	year = {2019},
	keywords = {Untagged},
}

@article{liuDifferentialPrivacyEyeTracking2019,
	title = {Differential {Privacy} for {Eye}-{Tracking} {Data}},
	url = {http://arxiv.org/abs/1904.06809},
	doi = {10/ghv4nz},
	abstract = {As large eye-tracking datasets are created, data privacy is a pressing concern for the eye-tracking community. De-identifying data does not guarantee privacy because multiple datasets can be linked for inferences. A common belief is that aggregating individuals’ data into composite representations such as heatmaps protects the individual. However, we analytically examine the privacy of (noise-free) heatmaps and show that they do not guarantee privacy. We further propose two noise mechanisms that guarantee privacy and analyze their privacy-utility tradeoff. Analysis reveals that our Gaussian noise mechanism is an elegant solution to preserve privacy for heatmaps. Our results have implications for interdisciplinary research to create differentially private mechanisms for eye tracking.},
	language = {en},
	urldate = {2021-01-27},
	journal = {Proceedings of the 11th ACM Symposium on Eye Tracking Research \& Applications},
	author = {Liu, Ao and Xia, Lirong and Duchowski, Andrew and Bailey, Reynold and Holmqvist, Kenneth and Jain, Eakta},
	month = jun,
	year = {2019},
	note = {arXiv: 1904.06809},
	keywords = {Untagged},
	pages = {1--10},
}

@article{liuSocInfMembershipInference2019,
	title = {{SocInf}: {Membership} {Inference} {Attacks} on {Social} {Media} {Health} {Data} {With} {Machine} {Learning}},
	volume = {6},
	issn = {2329-924X, 2373-7476},
	shorttitle = {{SocInf}},
	url = {https://ieeexplore.ieee.org/document/8728167/},
	doi = {10.1109/TCSS.2019.2916086},
	abstract = {Social media networks have shown rapid growth in the past, and massive social data are generated which can reveal behavior or emotion propensities of users. Numerous social researchers leverage machine learning technology to build social media analytic models which can detect the abnormal behaviors or mental illnesses from the social media data effectively. Although the researchers only public the prediction interfaces of the machine learning models, in general, these interfaces may leak information about the individual data records on which the models were trained. Knowing a certain user’s social media record was used to train a model can breach user privacy. In this paper, we present SocInf and focus on the fundamental problem known as membership inference. The key idea of SocInf is to construct a mimic model which has a similar prediction behavior with the public model, and then we can disclose the prediction differences between the training and testing data set by abusing the mimic model. With elaborated analytics on the predictions of the mimic model, SocInf can thus infer whether a given record is in the victim model’s training set or not. We empirically evaluate the attack performance of SocInf on machine learning models trained by Xgboost, logistics, and online cloud platform. Using the realistic data, the experiment results show that SocInf can achieve an inference accuracy and precision of 73\% and 84\%, respectively, in average, and of 83\% and 91\% at best.},
	language = {en},
	number = {5},
	urldate = {2022-03-21},
	journal = {IEEE Transactions on Computational Social Systems},
	author = {Liu, Gaoyang and Wang, Chen and Peng, Kai and Huang, Haojun and Li, Yutong and Cheng, Wenqing},
	month = oct,
	year = {2019},
	keywords = {Untagged},
	pages = {907--921},
}

@inproceedings{liuFastLowrankMetric2019,
	title = {Fast {Low}-rank {Metric} {Learning} for {Large}-scale and {High}-dimensional {Data}},
	abstract = {Low-rank metric learning aims to learn better discrimination of data subject to low-rank constraints. It keeps the intrinsic low-rank structure of datasets and reduces the time cost and memory usage in metric learning. However, it is still a challenge for current methods to handle datasets with both high dimensions and large numbers of samples. To address this issue, we present a novel fast low-rank metric learning (FLRML) method. FLRML casts the low-rank metric learning problem into an unconstrained optimization on the Stiefel manifold, which can be efﬁciently solved by searching along the descent curves of the manifold. FLRML signiﬁcantly reduces the complexity and memory usage in optimization, which makes the method scalable to both high dimensions and large numbers of samples. Furthermore, we introduce a mini-batch version of FLRML to make the method scalable to larger datasets which are hard to be loaded and decomposed in limited memory. The outperforming experimental results show that our method is with high accuracy and much faster than the state-of-the-art methods under several benchmarks with large numbers of high-dimensional data. Code has been made available at https://github.com/highan911/FLRML.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Liu, Han and Han, Zhizhong and Liu, Yu-Shen and Gu, Ming},
	year = {2019},
	keywords = {Untagged},
	pages = {11},
}

@inproceedings{liuJointDynamicPose2019,
	title = {Joint {Dynamic} {Pose} {Image} and {Space} {Time} {Reversal} for {Human} {Action} {Recognition} from {Videos}},
	url = {https://doi.org/10.1609/aaai.v33i01.33018762},
	doi = {10.1609/aaai.v33i01.33018762},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Liu, Mengyuan and Meng, Fanyang and Chen, Chen and Wu, Songtao},
	year = {2019},
	keywords = {Untagged},
	pages = {8762--8769},
}

@inproceedings{liuEndToEndMultiTaskLearning2019,
	title = {End-{To}-{End} {Multi}-{Task} {Learning} {With} {Attention}},
	url = {http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_End-To-End_Multi-Task_Learning_With_Attention_CVPR_2019_paper.html},
	doi = {10/gghmkj},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2019, {Long} {Beach}, {CA}, {USA}, {June} 16-20, 2019},
	publisher = {Computer Vision Foundation / IEEE},
	author = {Liu, Shikun and Johns, Edward and Davison, Andrew J.},
	year = {2019},
	keywords = {Untagged},
	pages = {1871--1880},
}

@inproceedings{luViLBERTPretrainingTaskAgnostic2019,
	title = {{ViLBERT}: {Pretraining} {Task}-{Agnostic} {Visiolinguistic} {Representations} for {Vision}-and-{Language} {Tasks}},
	volume = {32},
	shorttitle = {{ViLBERT}},
	url = {https://papers.nips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html},
	language = {en},
	urldate = {2021-07-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{luCoTCooperativeTraining2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{CoT}: {Cooperative} {Training} for {Generative} {Modeling} of {Discrete} {Data}},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/lu19d.html},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}, {ICML} 2019, 9-15 {June} 2019, {Long} {Beach}, {California}, {USA}},
	publisher = {PMLR},
	author = {Lu, Sidi and Yu, Lantao and Feng, Siyuan and Zhu, Yaoming and Zhang, Weinan},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	year = {2019},
	keywords = {Untagged},
	pages = {4164--4172},
}

@inproceedings{luoAdaptiveGradientMethods2019,
	address = {New Orleans, LA},
	title = {Adaptive {Gradient} {Methods} with {Dynamic} {Bound} of {Learning} {Rate}},
	shorttitle = {{AdaBound}},
	url = {https://openreview.net/forum?id=Bkg3g2R9FX},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{lyuGradientDescentMaximizes2019,
	title = {Gradient {Descent} {Maximizes} the {Margin} of {Homogeneous} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=SJeLIgBKPS},
	abstract = {We study the implicit bias of gradient descent and prove under a minimal set of assumptions that the parameter direction of homogeneous models converges to KKT points of a natural margin...},
	language = {en},
	urldate = {2022-03-10},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Lyu, Kaifeng and Li, Jian},
	month = sep,
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{maSELFMONITORINGNAVIGATIONAGENT2019,
	title = {{SELF}-{MONITORING} {NAVIGATION} {AGENT} {VIA} {AUXIL}- {IARY} {PROGRESS} {ESTIMATION}},
	abstract = {The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reﬂects the navigation progress. We test our selfmonitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a signiﬁcant margin (8\% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Ma, Chih-Yao and Lu, Jiasen and Wu, Zuxuan and AlRegib, Ghassan and Kira, Zsolt and Socher, Richard and Xiong, Caiming},
	year = {2019},
	keywords = {Untagged},
	pages = {18},
}

@inproceedings{maRegretfulAgentHeuristicAided2019,
	address = {Long Beach, CA, USA},
	title = {The {Regretful} {Agent}: {Heuristic}-{Aided} {Navigation} {Through} {Progress} {Estimation}},
	isbn = {978-1-72813-293-8},
	shorttitle = {The {Regretful} {Agent}},
	url = {https://ieeexplore.ieee.org/document/8954045/},
	doi = {10/gjzmvp},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Ma, Chih-Yao and Wu, Zuxuan and AlRegib, Ghassan and Xiong, Caiming and Kira, Zsolt},
	month = jun,
	year = {2019},
	keywords = {Untagged},
	pages = {6725--6733},
}

@inproceedings{maAttnSenseMultilevelAttention2019,
	address = {Macao, China},
	title = {{AttnSense}: {Multi}-level {Attention} {Mechanism} {For} {Multimodal} {Human} {Activity} {Recognition}},
	isbn = {978-0-9992411-4-1},
	shorttitle = {{AttnSense}},
	url = {https://www.ijcai.org/proceedings/2019/431},
	doi = {10/gjgc7v},
	language = {en},
	urldate = {2021-04-09},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Ma, HaoJie and Li, Wenzhong and Zhang, Xiao and Gao, Songcheng and Lu, Sanglu},
	year = {2019},
	keywords = {Untagged},
	pages = {3109--3115},
}

@inproceedings{maLabelForestNonParametricSemiSupervised2019,
	title = {{LabelForest}: {Non}-{Parametric} {Semi}-{Supervised} {Learning} for {Activity} {Recognition}},
	volume = {33},
	shorttitle = {{LabelForest}},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/4369},
	doi = {10/ghkjt9},
	abstract = {Activity recognition is central to many motion analysis applications ranging from health assessment to gaming. However, the need for obtaining sufﬁciently large amounts of labeled data has limited the development of personalized activity recognition models. Semi-supervised learning has traditionally been a promising approach in many application domains to alleviate reliance on large amounts of labeled data by learning the label information from a small set of seed labels. Nonetheless, existing approaches perform poorly in highly dynamic settings, such as wearable systems, because some algorithms rely on predeﬁned hyper-parameters or distribution models that needs to be tuned for each user or context. To address these challenges, we introduce LabelForest 1, a novel non-parametric semi-supervised learning framework for activity recognition. LabelForest has two algorithms at its core: (1) a spanning forest algorithm for sample selection and label inference; and (2) a silhouette-based ﬁltering method to ﬁnalize label augmentation for machine learning model training. Our thorough analysis on three human activity datasets demonstrate that LabelForest achieves a labeling accuracy of 90.1\% in presence of a skewed label distribution in the seed data. Compared to self-training and other sequential learning algorithms, LabelForest achieves up to 56.9\% and 175.3\% improvement in the accuracy on balanced and unbalanced seed data, respectively.},
	language = {en},
	urldate = {2021-04-09},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Ma, Yuchao and Ghasemzadeh, Hassan},
	year = {2019},
	keywords = {Untagged},
	pages = {4520--4527},
}

@inproceedings{maDataPoisoningDifferentiallyPrivate2019,
	address = {Macao, China},
	title = {Data {Poisoning} against {Differentially}-{Private} {Learners}: {Attacks} and {Defenses}},
	isbn = {978-0-9992411-4-1},
	shorttitle = {Data {Poisoning} against {Differentially}-{Private} {Learners}},
	url = {https://www.ijcai.org/proceedings/2019/657},
	doi = {10/ghv3gs},
	abstract = {Data poisoning attacks aim to manipulate the model produced by a learning algorithm by adversarially modifying the training set. We consider differential privacy as a defensive measure against this type of attack. We show that private learners are resistant to data poisoning attacks when the adversary is only able to poison a small number of items. However, this protection degrades as the adversary is allowed to poison more data. We emprically evaluate this protection by designing attack algorithms targeting objective and output perturbation learners, two standard approaches to differentially-private machine learning. Experiments show that our methods are effective when the attacker is allowed to poison sufﬁciently many training items.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Ma, Yuzhe and Zhu, Xiaojin and Hsu, Justin},
	month = aug,
	year = {2019},
	keywords = {Untagged},
	pages = {4732--4738},
}

@inproceedings{mithunWeaklySupervisedVideo2019,
	address = {Long Beach, CA, USA},
	title = {Weakly {Supervised} {Video} {Moment} {Retrieval} {From} {Text} {Queries}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954297/},
	doi = {10/ghv439},
	abstract = {There have been a few recent methods proposed in text to video moment retrieval using natural language queries, but requiring full supervision during training. However, acquiring a large number of training videos with temporal boundary annotations for each text description is extremely timeconsuming and often not scalable. In order to cope with this issue, in this work, we introduce the problem of learning from weak labels for the task of text to video moment retrieval. The weak nature of the supervision is because, during training, we only have access to the video-text pairs rather than the temporal extent of the video to which different text descriptions relate. We propose a joint visualsemantic embedding based framework that learns the notion of relevant segments from video using only video-level sentence descriptions. Speciﬁcally, our main idea is to utilize latent alignment between video frames and sentence descriptions using Text-Guided Attention (TGA). TGA is then used during the test phase to retrieve relevant moments. Experiments on two benchmark datasets demonstrate that our method achieves comparable performance to state-of-theart fully supervised approaches.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Mithun, Niluthpol Chowdhury and Paul, Sujoy and Roy-Chowdhury, Amit K.},
	year = {2019},
	keywords = {Untagged},
	pages = {11584--11593},
}

@inproceedings{mohriAgnosticFederatedLearning2019,
	title = {Agnostic {Federated} {Learning}},
	url = {https://proceedings.mlr.press/v97/mohri19a.html},
	abstract = {A key learning scenario in large-scale applications is that of federated learning, where a centralized model is trained based on data originating from a large number of clients. We argue that, with the existing training and inference, federated models can be biased towards different clients. Instead, we propose a new framework of agnostic federated learning, where the centralized model is optimized for any target distribution formed by a mixture of the client distributions. We further show that this framework naturally yields a notion of fairness. We present data-dependent Rademacher complexity guarantees for learning with this objective, which guide the definition of an algorithm for agnostic federated learning. We also give a fast stochastic optimization algorithm for solving the corresponding optimization problem, for which we prove convergence bounds, assuming a convex loss function and a convex hypothesis set. We further empirically demonstrate the benefits of our approach in several datasets. Beyond federated learning, our framework and algorithm can be of interest to other learning scenarios such as cloud computing, domain adaptation, drifting, and other contexts where the training and test distributions do not coincide.},
	language = {en},
	urldate = {2022-03-04},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mohri, Mehryar and Sivek, Gary and Suresh, Ananda Theertha},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {4615--4625},
}

@inproceedings{morcosOneTicketWin2019,
	title = {One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
	url = {https://proceedings.neurips.cc/paper/2019/hash/a4613e8d72a61b3b69b32d040f89ad81-Abstract.html},
	booktitle = {Advances in neural information processing systems 32: {Annual} conference on neural information processing systems 2019, {NeurIPS} 2019, december 8-14, 2019, vancouver, {BC}, canada},
	author = {Morcos, Ari S. and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
	editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily B. and Garnett, Roman},
	year = {2019},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/nips/MorcosYPT19.bib
tex.timestamp: Thu, 21 Jan 2021 15:15:19 +0100},
	keywords = {Untagged},
	pages = {4933--4943},
}

@inproceedings{murrugarra-llerenaCrossModalityPersonalizationRetrieval2019,
	address = {Long Beach, CA, USA},
	title = {Cross-{Modality} {Personalization} for {Retrieval}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953895/},
	doi = {10/ghv44c},
	abstract = {Existing captioning and gaze prediction approaches do not consider the multiple facets of personality that affect how a viewer extracts meaning from an image. While there are methods that consider personalized captioning, they do not consider personalized perception across modalities, i.e. how a person’s way of looking at an image (gaze) affects the way they describe it (captioning). In this work, we propose a model for modeling cross-modality personalized retrieval. In addition to modeling gaze and captions, we also explicitly model the personality of the users providing these samples. We incorporate constraints that encourage gaze and caption samples on the same image to be close in a learned space; we refer to this as content modeling. We also model style: we encourage samples provided by the same user to be close in a separate embedding space, regardless of the image on which they were provided. To leverage the complementary information that content and style constraints provide, we combine the embeddings from both networks. We show that our combined embeddings achieve better performance than existing approaches for cross-modal retrieval.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Murrugarra-Llerena, Nils and Kovashka, Adriana},
	year = {2019},
	keywords = {Untagged},
	pages = {6422--6431},
}

@inproceedings{nagarajanUniformConvergenceMay2019,
	address = {Vancouver, BC, Canada},
	title = {Uniform convergence may be unable to explain generalization in deep learning},
	url = {https://proceedings.neurips.cc/paper/2019/hash/05e97c207235d63ceb1db43c60db7bbb-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Nagarajan, Vaishnavh and Kolter, J. Zico},
	editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily B. and Garnett, Roman},
	year = {2019},
	keywords = {Untagged},
	pages = {11611--11622},
}

@inproceedings{nagelDataFreeQuantizationWeight2019,
	address = {Seoul, Korea},
	title = {Data-{Free} {Quantization} {Through} {Weight} {Equalization} and {Bias} {Correction}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9008784/},
	doi = {10.1109/ICCV.2019.00141},
	abstract = {We introduce a data-free quantization method for deep neural networks that does not require ﬁne-tuning or hyperparameter selection. It achieves near-original model performance on common computer vision architectures and tasks. 8-bit ﬁxed-point quantization is essential for efﬁcient inference on modern deep learning hardware. However, quantizing models to run in 8-bit is a non-trivial task, frequently leading to either signiﬁcant performance reduction or engineering time spent on training a network to be amenable to quantization. Our approach relies on equalizing the weight ranges in the network by making use of a scale-equivariance property of activation functions. In addition the method corrects biases in the error that are introduced during quantization. This improves quantization accuracy performance, and can be applied to many common computer vision architectures with a straight forward API call. For common architectures, such as the MobileNet family, we achieve state-of-the-art quantized model performance. We further show that the method also extends to other computer vision architectures and tasks such as semantic segmentation and object detection.},
	language = {en},
	urldate = {2021-03-18},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Nagel, Markus and Baalen, Mart Van and Blankevoort, Tijmen and Welling, Max},
	year = {2019},
	keywords = {Untagged},
	pages = {1325--1334},
}

@article{nakkiranDeepDoubleDescent2019,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	shorttitle = {Deep {Double} {Descent}},
	url = {http://arxiv.org/abs/1912.02292},
	abstract = {We show that a variety of modern deep learning tasks exhibit a “double-descent” phenomenon where, as we increase model size, performance ﬁrst gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by deﬁning a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	language = {en},
	urldate = {2022-02-22},
	journal = {arXiv:1912.02292 [cs, stat]},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.02292},
	keywords = {Untagged},
}

@inproceedings{nasrComprehensivePrivacyAnalysis2019,
	title = {Comprehensive {Privacy} {Analysis} of {Deep} {Learning}: {Passive} and {Active} {White}-box {Inference} {Attacks} against {Centralized} and {Federated} {Learning}},
	shorttitle = {Comprehensive {Privacy} {Analysis} of {Deep} {Learning}},
	doi = {10.1109/SP.2019.00065},
	abstract = {Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.},
	booktitle = {2019 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
	year = {2019},
	note = {ISSN: 2375-1207},
	keywords = {Untagged},
	pages = {739--753},
}

@article{naymanXNASNeuralArchitecture2019,
	title = {{XNAS}: {Neural} {Architecture} {Search} with {Expert} {Advice}},
	shorttitle = {{XNAS}},
	url = {http://arxiv.org/abs/1906.08031},
	abstract = {This paper introduces a novel optimization method for differential neural architecture search, based on the theory of prediction with expert advice. Its optimization criterion is well ﬁtted for an architecture-selection, i.e., it minimizes the regret incurred by a sub-optimal selection of operations. Unlike previous search relaxations, that require hard pruning of architectures, our method is designed to dynamically wipe out inferior architectures and enhance superior ones. It achieves an optimal worst-case regret bound and suggests the use of multiple learning-rates, based on the amount of information carried by the backward gradients. Experiments show that our algorithm achieves a strong performance over several image classiﬁcation datasets. Speciﬁcally, it obtains an error rate of 1.6\% for CIFAR-10, 24\% for ImageNet under mobile settings, and achieves state-of-the-art results on three additional datasets.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1906.08031 [cs, math, stat]},
	author = {Nayman, Niv and Noy, Asaf and Ridnik, Tal and Friedman, Itamar and Jin, Rong and Zelnik-Manor, Lihi},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.08031},
	keywords = {Untagged},
}

@inproceedings{nguyenGenerativeAdversarialNetworks2019,
	address = {Kingston, ON, Canada},
	title = {Generative {Adversarial} {Networks} {Using} {Adaptive} {Convolution}},
	url = {https://doi.org/10.1109/CRV.2019.00025},
	doi = {10/ghwnm4},
	booktitle = {Conference on {Computer} and {Robot} {Vision}},
	publisher = {IEEE},
	author = {Nguyen, Nhat M. and Ray, Nilanjan},
	year = {2019},
	keywords = {Untagged},
	pages = {129--134},
}

@inproceedings{nooriFusionMultipleRepresentations2019,
	title = {Fusion of {Multiple} {Representations} {Extracted} from a {Single} {Sensor}’s {Data} for {Activity} {Recognition} {Using} {CNNs}},
	doi = {10.1109/IJCNN.2019.8851898},
	abstract = {With the emerging ubiquitous sensing field, it has become possible to build assistive technologies for persons during their daily life activities to provide personalized feedback and services. For instance, it is possible to detect an individual's behavioral information (e.g. physical activity, location, and mood) by using sensors embedded in smartwatches and smartphones. To detect human's daily life activities, accelerometers have been widely used in wearable devices. In the current research, usually a single data representation is used, i.e., either image or feature vector representations. In this paper, a novel method is proposed to address two key aspects for the future development of robust deep learning methods for Human Activity Recognition (HAR): (1) multiple representations of a single sensor's data and (2) fusion of these multiple representations. The presented method utilizes Deep Convolutional Neural Networks (CNNs) and was evaluated using a publicly available HAR dataset. The proposed method showed promising performance, with the best result reaching an overall accuracy of 0.97, which outperforms current conventional approaches.},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	author = {Noori, Farzan Majeed and Garcia-Ceja, Enrique and Uddin, Md. Zia and Riegler, Michael and Tørresen, Jim},
	year = {2019},
	note = {ISSN: 2161-4407},
	keywords = {Untagged},
	pages = {1--6},
}

@article{oordRepresentationLearningContrastive2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2021-07-29},
	journal = {arXiv:1807.03748 [cs, stat]},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv: 1807.03748},
	keywords = {Untagged},
}

@inproceedings{ottFairseqFastExtensible2019,
	address = {Minneapolis, Minnesota},
	title = {fairseq: {A} {Fast}, {Extensible} {Toolkit} for {Sequence} {Modeling}},
	shorttitle = {fairseq},
	url = {https://aclanthology.org/N19-4009},
	doi = {10/gf7fdf},
	abstract = {fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics} ({Demonstrations})},
	publisher = {Association for Computational Linguistics},
	author = {Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
	month = jun,
	year = {2019},
	keywords = {Untagged},
	pages = {48--53},
}

@article{otterSurveyUsagesDeep2019,
	title = {A {Survey} of the {Usages} of {Deep} {Learning} in {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/1807.10854},
	abstract = {Over the last several years, the ﬁeld of natural language processing has been propelled forward by an explosion in the use of deep learning models. This survey provides a brief introduction to the ﬁeld and a quick overview of deep learning architectures and methods. It then sifts through the plethora of recent studies and summarizes a large assortment of relevant contributions. Analyzed research areas include several core linguistic processing issues in addition to a number of applications of computational linguistics. A discussion of the current state of the art is then provided along with recommendations for future research in the ﬁeld.},
	language = {en},
	urldate = {2021-11-24},
	journal = {arXiv:1807.10854 [cs]},
	author = {Otter, Daniel W. and Medina, Julian R. and Kalita, Jugal K.},
	month = dec,
	year = {2019},
	note = {arXiv: 1807.10854},
	keywords = {Untagged},
}

@inproceedings{pangGeneralisingFineGrainedSketchBased2019,
	address = {Long Beach, CA, USA},
	title = {Generalising {Fine}-{Grained} {Sketch}-{Based} {Image} {Retrieval}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953777/},
	doi = {10/ghv44d},
	abstract = {Fine-grained sketch-based image retrieval (FG-SBIR) addresses matching speciﬁc photo instance using free-hand sketch as a query modality. Existing models aim to learn an embedding space in which sketch and photo can be directly compared. While successful, they require instancelevel pairing within each coarse-grained category as annotated training data. Since the learned embedding space is domain-speciﬁc, these models do not generalise well across categories. This limits the practical applicability of FGSBIR. In this paper, we identify cross-category generalisation for FG-SBIR as a domain generalisation problem, and propose the ﬁrst solution. Our key contribution is a novel unsupervised learning approach to model a universal manifold of prototypical visual sketch traits. This manifold can then be used to paramaterise the learning of a sketch/photo representation. Model adaptation to novel categories then becomes automatic via embedding the novel sketch in the manifold and updating the representation and retrieval function accordingly. Experiments on the two largest FG-SBIR datasets, Sketchy and QMUL-Shoe-V2, demonstrate the efﬁcacy of our approach in enabling crosscategory generalisation of FG-SBIR.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Pang, Kaiyue and Li, Ke and Yang, Yongxin and Zhang, Honggang and Hospedales, Timothy M. and Xiang, Tao and Song, Yi-Zhe},
	year = {2019},
	keywords = {Untagged},
	pages = {677--686},
}

@inproceedings{parkRelationalKnowledgeDistillation2019,
	address = {Long Beach, CA},
	title = {Relational {Knowledge} {Distillation}},
	url = {http://openaccess.thecvf.com/content_CVPR_2019/html/Park_Relational_Knowledge_Distillation_CVPR_2019_paper.html},
	doi = {10/ghn5bn},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {Computer Vision Foundation / IEEE},
	author = {Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
	year = {2019},
	keywords = {Untagged},
	pages = {3967--3976},
}

@inproceedings{paszkePyTorchImperativeStyle2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	keywords = {Untagged},
	pages = {8024--8035},
}

@inproceedings{pengMomentMatchingMultiSource2019,
	address = {Seoul, Korea},
	title = {Moment {Matching} for {Multi}-{Source} {Domain} {Adaptation}},
	isbn = {978-1-72814-803-8},
	shorttitle = {{DomainNEt}},
	url = {https://ieeexplore.ieee.org/document/9010750/},
	doi = {10/ghfhmt},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Peng, Xingchao and Bai, Qinxun and Xia, Xide and Huang, Zijun and Saenko, Kate and Wang, Bo},
	year = {2019},
	keywords = {Untagged},
	pages = {1406--1415},
}

@article{perone2019unsupervised,
	title = {Unsupervised domain adaptation for medical imaging segmentation with self-ensembling},
	volume = {194},
	journal = {NeuroImage},
	author = {Perone, Christian S and Ballester, Pedro and Barros, Rodrigo C and Cohen-Adad, Julien},
	year = {2019},
	note = {Publisher: Elsevier},
	keywords = {Untagged},
	pages = {1--11},
}

@techreport{peterBenignOverfitting2019,
	address = {UC Berkeley},
	title = {Benign {Overfitting}},
	language = {en},
	institution = {UC Berkeley},
	author = {Peter, Bartlett},
	month = aug,
	year = {2019},
	keywords = {Untagged},
	pages = {34},
}

@article{pinotUnifiedViewDifferential2019,
	title = {A unified view on differential privacy and robustness to adversarial examples},
	url = {http://arxiv.org/abs/1906.07982},
	abstract = {This short note highlights some links between two lines of research within the emerging topic of trustworthy machine learning: differential privacy and robustness to adversarial examples. By abstracting the definitions of both notions, we show that they build upon the same theoretical ground and hence results obtained so far in one domain can be transferred to the other. More precisely, our analysis is based on two key elements: probabilistic mappings (also called randomized algorithms in the differential privacy community), and the Renyi divergence which subsumes a large family of divergences. We first generalize the definition of robustness against adversarial examples to encompass probabilistic mappings. Then we observe that Renyi-differential privacy (a generalization of differential privacy recently proposed in{\textasciitilde}{\textbackslash}cite\{Mironov2017RenyiDP\}) and our definition of robustness share several similarities. We finally discuss how can both communities benefit from this connection to transfer technical tools from one research field to the other.},
	urldate = {2021-09-27},
	journal = {arXiv:1906.07982 [cs, stat]},
	author = {Pinot, Rafael and Yger, Florian and Gouy-Pailler, Cédric and Atif, Jamal},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.07982},
	keywords = {Untagged},
}

@inproceedings{pruthi_combating_2019,
	address = {Florence, Italy},
	title = {Combating {Adversarial} {Misspellings} with {Robust} {Word} {Recognition}},
	url = {https://aclanthology.org/P19-1561},
	doi = {10.18653/v1/P19-1561},
	abstract = {To combat adversarial spelling mistakes, we propose placing a word recognition model in front of the downstream classifier. Our word recognition models build upon the RNN semi-character architecture, introducing several new backoff strategies for handling rare and unseen words. Trained to recognize words corrupted by random adds, drops, swaps, and keyboard mistakes, our method achieves 32\% relative (and 3.3\% absolute) error reduction over the vanilla semi-character model. Notably, our pipeline confers robustness on the downstream classifier, outperforming both adversarial training and off-the-shelf spell checkers. Against a BERT model fine-tuned for sentiment analysis, a single adversarially-chosen character attack lowers accuracy from 90.3\% to 45.8\%. Our defense restores accuracy to 75\%. Surprisingly, better word recognition does not always entail greater robustness. Our analysis reveals that robustness also depends upon a quantity that we denote the sensitivity.},
	urldate = {2023-02-27},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Pruthi, Danish and Dhingra, Bhuwan and Lipton, Zachary C.},
	month = jul,
	year = {2019},
	keywords = {Untagged},
	pages = {5582--5591},
}

@inproceedings{qianNovelDistributionEmbeddedNeural2019,
	address = {Macao, China},
	title = {A {Novel} {Distribution}-{Embedded} {Neural} {Network} for {Sensor}-{Based} {Activity} {Recognition}},
	isbn = {978-0-9992411-4-1},
	url = {https://www.ijcai.org/proceedings/2019/779},
	doi = {10.24963/ijcai.2019/779},
	abstract = {Feature-engineering-based machine learning models and deep learning models have been explored for wearable-sensor-based human activity recognition. For both types of methods, one crucial research issue is how to extract proper features from the partitioned segments of multivariate sensor readings. Existing methods have different drawbacks: 1) feature-engineering-based methods are able to extract meaningful features, such as statistical or structural information underlying the segments, but usually require manual designs of features for different applications, which is time consuming, and 2) deep learning models are able to learn temporal and/or spatial features from the sensor data automatically, but fail to capture statistical information. In this paper, we propose a novel deep learning model to automatically learn meaningful features including statistical features, temporal features and spatial correlation features for activity recognition in a uniﬁed framework. Extensive experiments are conducted on four datasets to demonstrate the effectiveness of our proposed method compared with state-of-the-art baselines.},
	language = {en},
	urldate = {2021-06-02},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Qian, Hangwei and Pan, Sinno Jialin and Da, Bingshui and Miao, Chunyan},
	year = {2019},
	keywords = {Untagged},
	pages = {5614--5620},
}

@inproceedings{qianDistributionBasedSemiSupervisedLearning2019,
	address = {Honolulu, Hawaii},
	title = {Distribution-{Based} {Semi}-{Supervised} {Learning} for {Activity} {Recognition}},
	url = {https://doi.org/10.1609/aaai.v33i01.33017699},
	doi = {10.1609/aaai.v33i01.33017699},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Qian, Hangwei and Pan, Sinno Jialin and Miao, Chunyan},
	year = {2019},
	keywords = {Untagged},
	pages = {7699--7706},
}

@inproceedings{qianSoftTripleLossDeep2019,
	address = {Seoul, Korea},
	title = {{SoftTriple} {Loss}: {Deep} {Metric} {Learning} {Without} {Triplet} {Sampling}},
	url = {https://doi.org/10.1109/ICCV.2019.00655},
	doi = {10/ghfhng},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Qian, Qi and Shang, Lei and Sun, Baigui and Hu, Juhua and Tacoma, Tacoma and Li, Hao and Jin, Rong},
	year = {2019},
	keywords = {Untagged},
	pages = {6449--6457},
}

@inproceedings{raeCompressiveTransformersLongRange2019,
	title = {Compressive {Transformers} for {Long}-{Range} {Sequence} {Modelling}},
	url = {https://openreview.net/forum?id=SylKikSYDH},
	abstract = {Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.},
	language = {en},
	urldate = {2021-11-04},
	author = {Rae, Jack W. and Potapenko, Anna and Jayakumar, Siddhant M. and Hillier, Chloe and Lillicrap, Timothy P.},
	month = sep,
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{ren_generating_2019,
	address = {Florence, Italy},
	title = {Generating {Natural} {Language} {Adversarial} {Examples} through {Probability} {Weighted} {Word} {Saliency}},
	url = {https://aclanthology.org/P19-1103},
	doi = {10.18653/v1/P19-1103},
	abstract = {We address the problem of adversarial attacks on text classification, which is rarely studied comparing to attacks on image classification. The challenge of this task is to generate adversarial examples that maintain lexical correctness, grammatical correctness and semantic similarity. Based on the synonyms substitution strategy, we introduce a new word replacement order determined by both the word saliency and the classification probability, and propose a greedy algorithm called probability weighted word saliency (PWWS) for text adversarial attack. Experiments on three popular datasets using convolutional as well as LSTM models show that PWWS reduces the classification accuracy to the most extent, and keeps a very low word substitution rate. A human evaluation study shows that our generated adversarial examples maintain the semantic similarity well and are hard for humans to perceive. Performing adversarial training using our perturbed datasets improves the robustness of the models. At last, our method also exhibits a good transferability on the generated adversarial examples.},
	urldate = {2023-02-27},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Ren, Shuhuai and Deng, Yihe and He, Kun and Che, Wanxiang},
	month = jul,
	year = {2019},
	keywords = {Untagged},
	pages = {1085--1097},
}

@article{ribaKorniaOpenSource2019,
	title = {Kornia: an {Open} {Source} {Differentiable} {Computer} {Vision} {Library} for {PyTorch}},
	shorttitle = {Kornia},
	url = {http://arxiv.org/abs/1910.02190},
	abstract = {This work presents Kornia – an open source computer vision library which consists of a set of differentiable routines and modules to solve generic computer vision problems. At its core, the package uses PyTorch as its main backend both for efﬁciency and to take advantage of the reverse-mode auto-differentiation to deﬁne and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be inserted inside neural networks to train models to perform image transformations, camera calibration, epipolar geometry, and low level image processing techniques such as ﬁltering and edge detection that operate directly on high dimensional tensor representations. Examples of classical vision problems implemented using our framework are also provided including a benchmark comparing to existing vision libraries.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1910.02190 [cs]},
	author = {Riba, Edgar and Mishkin, Dmytro and Ponsa, Daniel and Rublee, Ethan and Bradski, Gary},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.02190},
	keywords = {Untagged},
}

@inproceedings{sablayrollesWhiteboxVsBlackbox2019,
	title = {White-box vs {Black}-box: {Bayes} {Optimal} {Strategies} for {Membership} {Inference}},
	shorttitle = {White-box vs {Black}-box},
	url = {https://proceedings.mlr.press/v97/sablayrolles19a.html},
	abstract = {Membership inference determines, given a sample and trained parameters of a machine learning model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few assumptions on the distribution of the parameters. We show that optimal attacks only depend on the loss function, and thus black-box attacks are as good as white-box attacks. As the optimal strategy is not tractable, we provide approximations of it leading to several inference methods, and show that existing membership inference methods are coarser approximations of this optimal strategy. Our membership attacks outperform the state of the art in various settings, ranging from a simple logistic regression to more complex architectures and datasets, such as ResNet-101 and Imagenet.},
	language = {en},
	urldate = {2022-03-21},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sablayrolles, Alexandre and Douze, Matthijs and Schmid, Cordelia and Ollivier, Yann and Jegou, Herve},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {5558--5567},
}

@article{salinasDeepARProbabilisticForecasting2019,
	title = {{DeepAR}: {Probabilistic} {Forecasting} with {Autoregressive} {Recurrent} {Networks}},
	shorttitle = {{DeepAR}},
	url = {http://arxiv.org/abs/1704.04110},
	abstract = {Probabilistic forecasting, i.e. estimating the probability distribution of a time series' future given its past, is a key enabler for optimizing business processes. In retail businesses, for example, forecasting demand is crucial for having the right inventory available at the right time at the right place. In this paper we propose DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an auto regressive recurrent network model on a large number of related time series. We demonstrate how by applying deep learning techniques to forecasting, one can overcome many of the challenges faced by widely-used classical approaches to the problem. We show through extensive empirical evaluation on several real-world forecasting data sets accuracy improvements of around 15\% compared to state-of-the-art methods.},
	urldate = {2021-07-23},
	journal = {arXiv:1704.04110 [cs, stat]},
	author = {Salinas, David and Flunkert, Valentin and Gasthaus, Jan},
	month = feb,
	year = {2019},
	note = {arXiv: 1704.04110},
	keywords = {Untagged},
}

@inproceedings{salman_provably_2019,
	title = {Provably {Robust} {Deep} {Learning} via {Adversarially} {Trained} {Smoothed} {Classifiers}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/3a24b25a7b092a252166a1641ae953e7-Abstract.html},
	urldate = {2022-06-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Salman, Hadi and Li, Jerry and Razenshteyn, Ilya and Zhang, Pengchuan and Zhang, Huan and Bubeck, Sebastien and Yang, Greg},
	year = {2019},
	keywords = {Untagged},
	pages = {11289--11300},
}

@article{savareseConvergenceAdaBoundIts2019,
	title = {On the {Convergence} of {AdaBound} and its {Connection} to {SGD}},
	url = {http://arxiv.org/abs/1908.04457},
	abstract = {Adaptive gradient methods such as Adam have gained extreme popularity due to their success in training complex neural networks and less sensitivity to hyperparameter tuning compared to SGD. However, it has been recently shown that Adam can fail to converge and might cause poor generalization – this lead to the design of new, sophisticated adaptive methods which attempt to generalize well while being theoretically reliable.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1908.04457 [cs, math, stat]},
	author = {Savarese, Pedro},
	month = dec,
	year = {2019},
	note = {arXiv: 1908.04457},
	keywords = {Untagged},
}

@article{schlemperAttentionGatedNetworks2019,
	title = {Attention gated networks: {Learning} to leverage salient regions in medical images},
	volume = {53},
	issn = {13618415},
	shorttitle = {Attention gated networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841518306133},
	doi = {10/ggrpct},
	language = {en},
	urldate = {2021-01-27},
	journal = {Medical Image Analysis},
	author = {Schlemper, Jo and Oktay, Ozan and Schaap, Michiel and Heinrich, Mattias and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
	month = apr,
	year = {2019},
	keywords = {Untagged},
	pages = {197--207},
}

@inproceedings{schott2018towards,
	title = {Towards the first adversarially robust neural network model on {MNIST}},
	url = {https://openreview.net/forum?id=S1EHOsC9tX},
	booktitle = {International conference on learning representations},
	author = {Schott, Lukas and Rauber, Jonas and Bethge, Matthias and Brendel, Wieland},
	year = {2019},
	keywords = {Untagged},
}

@article{sero_facial_2019,
	title = {Facial recognition from {DNA} using face-to-{DNA} classifiers},
	volume = {10},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/s41467-019-10617-y},
	doi = {10.1038/s41467-019-10617-y},
	language = {en},
	number = {1},
	urldate = {2022-08-24},
	journal = {Nature Communications},
	author = {Sero, Dzemila and Zaidi, Arslan and Li, Jiarui and White, Julie D. and Zarzar, Tomás B. González and Marazita, Mary L. and Weinberg, Seth M. and Suetens, Paul and Vandermeulen, Dirk and Wagner, Jennifer K. and Shriver, Mark D. and Claes, Peter},
	month = dec,
	year = {2019},
	keywords = {Untagged},
	pages = {2557},
}

@inproceedings{shafahi_adversarial_2019,
	title = {Adversarial training for free!},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/7503cfacd12053d309b6bed5c89de212-Abstract.html},
	abstract = {Adversarial training, in which a network is trained on adversarial examples, is one of the few defenses against adversarial attacks that withstands strong attacks. Unfortunately, the high cost of generating strong adversarial examples makes standard adversarial training impractical on large-scale problems like ImageNet. We present an algorithm that eliminates the overhead cost of generating adversarial examples by recycling the gradient information computed when updating model parameters. Our "free" adversarial training algorithm achieves comparable robustness to PGD adversarial training on the CIFAR-10 and CIFAR-100 datasets at negligible additional cost compared to natural training, and can be 7 to 30 times faster than other strong adversarial training methods. Using a single workstation with 4 P100 GPUs and 2 days of runtime, we can train a robust model for the large-scale ImageNet classification task that maintains 40\% accuracy against PGD attacks.},
	urldate = {2022-06-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Shafahi, Ali and Najibi, Mahyar and Ghiasi, Mohammad Amin and Xu, Zheng and Dickerson, John and Studer, Christoph and Davis, Larry S and Taylor, Gavin and Goldstein, Tom},
	year = {2019},
	keywords = {Untagged},
}

@article{shamirExponentialConvergenceTime2019,
	title = {Exponential {Convergence} {Time} of {Gradient} {Descent} for {One}-{Dimensional} {Deep} {Linear} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1809.08587},
	abstract = {We study the dynamics of gradient descent on objective functions of the form \$f({\textbackslash}prod\_\{i=1\}{\textasciicircum}\{k\} w\_i)\$ (with respect to scalar parameters \$w\_1,{\textbackslash}ldots,w\_k\$), which arise in the context of training depth-\$k\$ linear neural networks. We prove that for standard random initializations, and under mild assumptions on \$f\$, the number of iterations required for convergence scales exponentially with the depth \$k\$. We also show empirically that this phenomenon can occur in higher dimensions, where each \$w\_i\$ is a matrix. This highlights a potential obstacle in understanding the convergence of gradient-based methods for deep linear neural networks, where \$k\$ is large.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1809.08587 [cs, math, stat]},
	author = {Shamir, Ohad},
	month = jun,
	year = {2019},
	note = {arXiv: 1809.08587},
	keywords = {Untagged},
}

@inproceedings{siminelakisRehashingKernelEvaluation2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Rehashing {Kernel} {Evaluation} in {High} {Dimensions}},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/siminelakis19a.html},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}, {ICML} 2019, 9-15 {June} 2019, {Long} {Beach}, {California}, {USA}},
	publisher = {PMLR},
	author = {Siminelakis, Paris and Rong, Kexin and Bailis, Peter and Charikar, Moses and Levis, Philip},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	year = {2019},
	keywords = {Untagged},
	pages = {5789--5798},
}

@article{sochorBoxCarsImprovingFineGrained2019,
	title = {{BoxCars}: {Improving} {Fine}-{Grained} {Recognition} of {Vehicles} using 3-{D} {Bounding} {Boxes} in {Traffic} {Surveillance}},
	volume = {20},
	issn = {1524-9050, 1558-0016},
	shorttitle = {{BoxCars}},
	url = {http://arxiv.org/abs/1703.00686},
	doi = {10/gftzzr},
	abstract = {In this paper, we focus on ﬁne-grained recognition of vehicles mainly in trafﬁc surveillance applications. We propose an approach that is orthogonal to recent advancements in ﬁnegrained recognition (automatic part discovery, bilinear pooling). Also, in contrast to other methods focused on ﬁne-grained recognition of vehicles, we do not limit ourselves to a frontal/rear viewpoint, but allow the vehicles to be seen from any viewpoint. Our approach is based on 3D bounding boxes built around the vehicles. The bounding box can be automatically constructed from trafﬁc surveillance data. For scenarios where it is not possible to use precise construction, we propose a method for an estimation of the 3D bounding box. The 3D bounding box is used to normalize the image viewpoint by “unpacking” the image into a plane. We also propose to randomly alter the color of the image and add a rectangle with random noise to a random position in the image during the training of Convolutional Neural Networks. We have collected a large ﬁne-grained vehicle dataset BoxCars116k, with 116k images of vehicles from various viewpoints taken by numerous surveillance cameras. We performed a number of experiments which show that our proposed method signiﬁcantly improves CNN classiﬁcation accuracy (the accuracy is increased by up to 12 percentage points and the error is reduced by up to 50 \% compared to CNNs without the proposed modiﬁcations). We also show that our method outperforms stateof-the-art methods for ﬁne-grained recognition.},
	language = {en},
	number = {1},
	urldate = {2021-01-27},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Sochor, Jakub and Špaňhel, Jakub and Herout, Adam},
	month = jan,
	year = {2019},
	note = {arXiv: 1703.00686},
	keywords = {Untagged},
	pages = {97--108},
}

@article{soltanolkotabiTheoreticalInsightsOptimization2019,
	title = {Theoretical {Insights} {Into} the {Optimization} {Landscape} of {Over}-{Parameterized} {Shallow} {Neural} {Networks}},
	volume = {65},
	issn = {0018-9448, 1557-9654},
	url = {https://ieeexplore.ieee.org/document/8409482/},
	doi = {10/ggjnrf},
	abstract = {In this paper, we study the problem of learning a shallow artiﬁcial neural network that best ﬁts a training data set. We study this problem in the over-parameterized regime where the numbers of observations are fewer than the number of parameters in the model. We show that with the quadratic activations, the optimization landscape of training, such shallow neural networks, has certain favorable characteristics that allow globally optimal models to be found efﬁciently using a variety of local search heuristics. This result holds for an arbitrary training data of input/output pairs. For differentiable activation functions, we also show that gradient descent, when suitably initialized, converges at a linear rate to a globally optimal model. This result focuses on a realizable model where the inputs are chosen i.i.d. from a Gaussian distribution and the labels are generated according to planted weight coefﬁcients.},
	language = {en},
	number = {2},
	urldate = {2021-01-27},
	journal = {IEEE Transactions on Information Theory},
	author = {Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D.},
	month = feb,
	year = {2019},
	keywords = {Untagged},
	pages = {742--769},
}

@article{songAuditingDataProvenance2019,
	title = {Auditing {Data} {Provenance} in {Text}-{Generation} {Models}},
	url = {http://arxiv.org/abs/1811.00513},
	abstract = {To help enforce data-protection regulations such as GDPR and detect unauthorized uses of personal data, we develop a new {\textbackslash}emph\{model auditing\} technique that helps users check if their data was used to train a machine learning model. We focus on auditing deep-learning models that generate natural-language text, including word prediction and dialog generation. These models are at the core of popular online services and are often trained on personal data such as users' messages, searches, chats, and comments. We design and evaluate a black-box auditing method that can detect, with very few queries to a model, if a particular user's texts were used to train it (among thousands of other users). We empirically show that our method can successfully audit well-generalized models that are not overfitted to the training data. We also analyze how text-generation models memorize word sequences and explain why this memorization makes them amenable to auditing.},
	urldate = {2022-03-21},
	journal = {arXiv:1811.00513 [cs, stat]},
	author = {Song, Congzheng and Shmatikov, Vitaly},
	month = may,
	year = {2019},
	note = {arXiv: 1811.00513},
	keywords = {Untagged},
}

@inproceedings{songMembershipInferenceAttacks2019,
	address = {San Francisco, CA, USA},
	title = {Membership {Inference} {Attacks} {Against} {Adversarially} {Robust} {Deep} {Learning} {Models}},
	isbn = {978-1-72813-508-3},
	url = {https://ieeexplore.ieee.org/document/8844607/},
	doi = {10.1109/SPW.2019.00021},
	abstract = {In recent years, the research community has increasingly focused on understanding the security and privacy challenges posed by deep learning models. However, the security domain and the privacy domain have typically been considered separately. It is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain. In this paper, we take a step towards enhancing our understanding of deep learning models when the two domains are combined together. We do this by measuring the success of membership inference attacks against two state-of-the-art adversarial defense methods that mitigate evasion attacks: adversarial training and provable defense. On the one hand, membership inference attacks aim to infer an individual’s participation in the target model’s training dataset and are known to be correlated with target model’s overﬁtting. On the other hand, adversarial defense methods aim to enhance the robustness of target models by ensuring that model predictions are unchanged for a small area around each sample in the training dataset. Intuitively, adversarial defenses may rely more on the training dataset and be more vulnerable to membership inference attacks. By performing empirical membership inference attacks on both adversarially robust models and corresponding undefended models, we ﬁnd that the adversarial training method is indeed more susceptible to membership inference attacks, and the privacy leakage is directly correlated with model robustness. We also ﬁnd that the provable defense approach does not lead to enhanced success of membership inference attacks. However, this is achieved by signiﬁcantly sacriﬁcing the accuracy of the model on benign data points, indicating that privacy, security, and prediction accuracy are not jointly achieved in these two approaches.},
	language = {en},
	urldate = {2022-03-21},
	booktitle = {2019 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	publisher = {IEEE},
	author = {Song, Liwei and Shokri, Reza and Mittal, Prateek},
	month = may,
	year = {2019},
	keywords = {Untagged},
	pages = {50--56},
}

@article{songPrivacyRisksSecuring2019,
	title = {Privacy {Risks} of {Securing} {Machine} {Learning} {Models} against {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1905.10291},
	doi = {10.1145/3319535.3354211},
	abstract = {The arms race between attacks and defenses for machine learning models has come to a forefront in recent years, in both the security community and the privacy community. However, one big limitation of previous research is that the security domain and the privacy domain have typically been considered separately. It is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain. In this paper, we take a step towards resolving this limitation by combining the two domains. In particular, we measure the success of membership inference attacks against six state-of-the-art defense methods that mitigate the risk of adversarial examples (i.e., evasion attacks). Membership inference attacks determine whether or not an individual data record has been part of a model's training set. The accuracy of such attacks reflects the information leakage of training algorithms about individual members of the training set. Adversarial defense methods against adversarial examples influence the model's decision boundaries such that model predictions remain unchanged for a small area around each input. However, this objective is optimized on training data. Thus, individual data records in the training set have a significant influence on robust models. This makes the models more vulnerable to inference attacks. To perform the membership inference attacks, we leverage the existing inference methods that exploit model predictions. We also propose two new inference methods that exploit structural properties of robust models on adversarially perturbed data. Our experimental evaluation demonstrates that compared with the natural training (undefended) approach, adversarial defense methods can indeed increase the target model's risk against membership inference attacks.},
	urldate = {2022-03-21},
	journal = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
	author = {Song, Liwei and Shokri, Reza and Mittal, Prateek},
	month = nov,
	year = {2019},
	note = {arXiv: 1905.10291},
	keywords = {Untagged},
	pages = {241--257},
}

@inproceedings{songPolysemousVisualSemanticEmbedding2019,
	address = {Long Beach, CA, USA},
	title = {Polysemous {Visual}-{Semantic} {Embedding} for {Cross}-{Modal} {Retrieval}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{PVSE}},
	url = {https://ieeexplore.ieee.org/document/8953306/},
	doi = {10/gg3wgw},
	abstract = {Visual-semantic embedding aims to ﬁnd a shared latent space where related visual and textual instances are close to each other. Most current methods learn injective embedding functions that map an instance to a single point in the shared space. Unfortunately, injective embedding cannot effectively handle polysemous instances with multiple possible meanings; at best, it would ﬁnd an average representation of different meanings. This hinders its use in real-world scenarios where individual instances and their cross-modal associations are often ambiguous. In this work, we introduce Polysemous Instance Embedding Networks (PIE-Nets) that compute multiple and diverse representations of an instance by combining global context with locally-guided features via multi-head self-attention and residual learning. To learn visual-semantic embedding, we tie-up two PIE-Nets and optimize them jointly in the multiple instance learning framework. Most existing work on cross-modal retrieval focus on image-text pairs of data. Here, we also tackle a more challenging case of video-text retrieval. To facilitate further research in video-text retrieval, we release a new dataset of 50K video-sentence pairs collected from social media, dubbed MRW (my reaction when). We demonstrate our approach on both image-text and video-text retrieval scenarios using MS-COCO, TGIF, and our new MRW dataset.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Song, Yale and Soleymani, Mohammad},
	year = {2019},
	keywords = {Untagged},
	pages = {1979--1988},
}

@inproceedings{songEfficientSymmetricNorm2019,
	title = {Efficient symmetric norm regression via linear sketching},
	volume = {32},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Song, Zhao and Wang, Ruosong and Yang, Lin and Zhang, Hongyang and Zhong, Peilin},
	year = {2019},
	keywords = {Untagged},
}

@article{sousalimaHumanActivityRecognition2019,
	title = {Human {Activity} {Recognition} {Using} {Inertial} {Sensors} in a {Smartphone}: {An} {Overview}},
	volume = {19},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Human {Activity} {Recognition} {Using} {Inertial} {Sensors} in a {Smartphone}},
	url = {https://www.mdpi.com/1424-8220/19/14/3213},
	doi = {10/gf5fg6},
	abstract = {The ubiquity of smartphones and the growth of computing resources, such as connectivity, processing, portability, and power of sensing, have greatly changed people\&rsquo;s lives. Today, many smartphones contain a variety of powerful sensors, including motion, location, network, and direction sensors. Motion or inertial sensors (e.g., accelerometer), specifically, have been widely used to recognize users\&rsquo; physical activities. This has opened doors for many different and interesting applications in several areas, such as health and transportation. In this perspective, this work provides a comprehensive, state of the art review of the current situation of human activity recognition (HAR) solutions in the context of inertial sensors in smartphones. This article begins by discussing the concepts of human activities along with the complete historical events, focused on smartphones, which shows the evolution of the area in the last two decades. Next, we present a detailed description of the HAR methodology, focusing on the presentation of the steps of HAR solutions in the context of inertial sensors. For each step, we cite the main references that use the best implementation practices suggested by the scientific community. Finally, we present the main results about HAR solutions from the perspective of the inertial sensors embedded in smartphones.},
	language = {en},
	number = {14},
	urldate = {2021-09-27},
	journal = {Sensors},
	author = {Sousa Lima, Wesllen and Souto, Eduardo and El-Khatib, Khalil and Jalali, Roozbeh and Gama, Joao},
	month = jan,
	year = {2019},
	note = {Number: 14
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Untagged},
	pages = {3213},
}

@inproceedings{specialePrivacyPreservingImageBased2019,
	address = {Long Beach, CA, USA},
	title = {Privacy {Preserving} {Image}-{Based} {Localization}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953346/},
	doi = {10/ghv3kk},
	abstract = {Image-based localization is a core component of many augmented/mixed reality (AR/MR) and autonomous robotic systems. Current localization systems rely on the persistent storage of 3D point clouds of the scene to enable camera pose estimation, but such data reveals potentially sensitive scene information. This gives rise to signiﬁcant privacy risks, especially as for many applications 3D mapping is a background process that the user might not be fully aware of. We pose the following question: How can we avoid disclosing conﬁdential information about the captured 3D scene, and yet allow reliable camera pose estimation? This paper proposes the ﬁrst solution to what we call privacy preserving image-based localization. The key idea of our approach is to lift the map representation from a 3D point cloud to a 3D line cloud. This novel representation obfuscates the underlying scene geometry while providing sufﬁcient geometric constraints to enable robust and accurate 6-DOF camera pose estimation. Extensive experiments on several datasets and localization scenarios underline the high practical relevance of our proposed approach.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Speciale, Pablo and Schonberger, Johannes L. and Kang, Sing Bing and Sinha, Sudipta N. and Pollefeys, Marc},
	year = {2019},
	keywords = {Untagged},
	pages = {5488--5498},
}

@inproceedings{suHDLHierarchicalDeep2019,
	title = {{HDL}: {Hierarchical} {Deep} {Learning} {Model} based {Human} {Activity} {Recognition} using {Smartphone} {Sensors}},
	shorttitle = {{HDL}},
	doi = {10.1109/IJCNN.2019.8851889},
	abstract = {With the development and popularization of smart-phones, human activity recognition methods based on contact perception are proposed. The smartphones which are embedded with various sensors can be used as a platform of mobile sensing for human activity recognition. In this paper, we propose an automated human activity recognition network HDL with smartphone motion sensor units. The HDL network combines DBLSTM (Deep Bidirectional Long Short-Term Memory) model and CNN (Convolutional neural network) model. The DBLSTM model is first used to model long sequence data and ultimately generate a bidirectional output vector in a abstract way. The DBLSTM model is good at dealing with serialization tasks but poor in the ability to extract features. Hence, the CNN model is then used to extract features from the abstract vector. Finally, the output layer employs a softmax function to classify human activities. We conduct experiments on the Public domain UCI dataset. The experimental results show that the proposed HDL network achieves reliable results with accuracy and F1 score as high as 97.95\% and 97.27\%. Compared with other networks based on the same smartphone dataset, the accuracy of HDL is higher than S-LSTM and Dropout CNN network by 2.14\% and 6.97\% respectively.},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	author = {Su, Tongtong and Sun, Huazhi and Ma, Chunmei and Jiang, Lifen and Xu, Tongtong},
	year = {2019},
	note = {ISSN: 2161-4407},
	keywords = {Untagged},
	pages = {1--8},
}

@article{sunFinegrainedRecognitionAccounting2019,
	title = {Fine-grained {Recognition}: {Accounting} for {Subtle} {Differences} between {Similar} {Classes}},
	shorttitle = {Fine-grained {Recognition}},
	url = {http://arxiv.org/abs/1912.06842},
	abstract = {The main requisite for ﬁne-grained recognition task is to focus on subtle discriminative details that make the subordinate classes different from each other. We note that existing methods implicitly address this requirement and leave it to a datadriven pipeline to ﬁgure out what makes a subordinate class different from the others. This results in two major limitations: First, the network focuses on the most obvious distinctions between classes and overlooks more subtle inter-class variations. Second, the chance of misclassifying a given sample in any of the negative classes is considered equal, while in fact, confusions generally occur among only the most similar classes. Here, we propose to explicitly force the network to ﬁnd the subtle differences among closely related classes. In this pursuit, we introduce two key novelties that can be easily plugged into existing end-to-end deep learning pipelines. On one hand, we introduce “diversiﬁcation block” which masks the most salient features for an input to force the network to use more subtle cues for its correct classiﬁcation. Concurrently, we introduce a “gradient-boosting” loss function that focuses only on the confusing classes for each sample and therefore moves swiftly along the direction on the loss surface that seeks to resolve these ambiguities. The synergy between these two blocks helps the network to learn more effective feature representations. Comprehensive experiments are performed on ﬁve challenging datasets. Our approach outperforms existing methods using similar experimental setting on all ﬁve datasets.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1912.06842 [cs]},
	author = {Sun, Guolei and Cholakkal, Hisham and Khan, Salman and Khan, Fahad Shahbaz and Shao, Ling},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.06842},
	keywords = {Untagged},
}

@article{sunHighResolutionRepresentationsLabeling2019,
	title = {High-{Resolution} {Representations} for {Labeling} {Pixels} and {Regions}},
	url = {http://arxiv.org/abs/1904.04514},
	abstract = {High-resolution representation learning plays an essential role in many vision problems, e.g., pose estimation and semantic segmentation. The high-resolution network (HRNet) [91], recently developed for human pose estimation, maintains high-resolution representations through the whole process by connecting high-to-low resolution convolutions in parallel and produces strong high-resolution representations by repeatedly conducting fusions across parallel convolutions.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1904.04514 [cs]},
	author = {Sun, Ke and Zhao, Yang and Jiang, Borui and Cheng, Tianheng and Xiao, Bin and Liu, Dong and Mu, Yadong and Wang, Xinggang and Liu, Wenyu and Wang, Jingdong},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.04514},
	keywords = {Untagged},
}

@article{sunOptimizationDeepLearning2019,
	title = {Optimization for deep learning: theory and algorithms},
	shorttitle = {Optimization for deep learning},
	url = {http://arxiv.org/abs/1912.08957},
	abstract = {When and why can a neural network be successfully trained? This article provides an overview of optimization algorithms and theory for training neural networks. First, we discuss the issue of gradient explosion/vanishing and the more general issue of undesirable spectrum, and then discuss practical solutions including careful initialization and normalization methods. Second, we review generic optimization methods used in training neural networks, such as SGD, adaptive gradient methods and distributed methods, and existing theoretical results for these algorithms. Third, we review existing research on the global issues of neural network training, including results on bad local minima, mode connectivity, lottery ticket hypothesis and inﬁnitewidth analysis.},
	language = {en},
	urldate = {2022-02-25},
	journal = {arXiv:1912.08957 [cs, math, stat]},
	author = {Sun, Ruoyu},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.08957},
	keywords = {Untagged},
}

@inproceedings{sunPatientKnowledgeDistillation2019,
	title = {Patient {Knowledge} {Distillation} for {BERT} {Model} {Compression}},
	shorttitle = {{PatientKD}},
	url = {https://doi.org/10.18653/v1/D19-1441},
	doi = {10/ghn6fz},
	booktitle = {Empirical {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	year = {2019},
	keywords = {Untagged},
	pages = {4322--4331},
}

@inproceedings{tanLearningNavigateUnseen2019,
	address = {Minneapolis, Minnesota},
	title = {Learning to {Navigate} {Unseen} {Environments}: {Back} {Translation} with {Environmental} {Dropout}},
	shorttitle = {Learning to {Navigate} {Unseen} {Environments}},
	url = {https://aclanthology.org/N19-1268},
	doi = {10/gjzmvj},
	abstract = {A grand goal in AI is to build a robot that can accurately navigate based on natural language instructions, which requires the agent to perceive the scene, understand and ground language, and act in the real-world environment. One key challenge here is to learn to navigate in new environments that are unseen during training. Most of the existing approaches perform dramatically worse in unseen environments as compared to seen ones. In this paper, we present a generalizable navigational agent. Our agent is trained in two stages. The first stage is training via mixed imitation and reinforcement learning, combining the benefits from both off-policy and on-policy optimization. The second stage is fine-tuning via newly-introduced `unseen' triplets (environment, path, instruction). To generate these unseen triplets, we propose a simple but effective `environmental dropout' method to mimic unseen environments, which overcomes the problem of limited seen environment variability. Next, we apply semi-supervised learning (via back-translation) on these dropout environments to generate new paths and instructions. Empirically, we show that our agent is substantially better at generalizability when fine-tuned with these triplets, outperforming the state-of-art approaches by a large margin on the private unseen test set of the Room-to-Room task, and achieving the top rank on the leaderboard.},
	urldate = {2021-11-03},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Tan, Hao and Yu, Licheng and Bansal, Mohit},
	month = jun,
	year = {2019},
	keywords = {Untagged},
	pages = {2610--2621},
}

@inproceedings{torkzadehmahaniDPCGANDifferentiallyPrivate2019,
	address = {Long Beach, CA, USA},
	title = {{DP}-{CGAN}: {Differentially} {Private} {Synthetic} {Data} and {Label} {Generation}},
	isbn = {978-1-72812-506-0},
	shorttitle = {{DP}-{CGAN}},
	url = {https://ieeexplore.ieee.org/document/9025394/},
	doi = {10/ghfcx3},
	abstract = {Generative Adversarial Networks (GANs) are one of the well-known models to generate synthetic data including images, especially for research communities that cannot use original sensitive datasets because they are not publicly accessible. One of the main challenges in this area is to preserve the privacy of individuals who participate in the training of the GAN models. To address this challenge, we introduce a Differentially Private Conditional GAN (DP-CGAN) training framework based on a new clipping and perturbation strategy, which improves the performance of the model while preserving privacy of the training dataset. DP-CGAN generates both synthetic data and corresponding labels and leverages the recently introduced Re´nyi differential privacy accountant to track the spent privacy budget. The experimental results show that DP-CGAN can generate visually and empirically promising results on the MNIST dataset with a single-digit epsilon parameter in differential privacy.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshop}},
	publisher = {IEEE},
	author = {Torkzadehmahani, Reihaneh and Kairouz, Peter and Paten, Benedict},
	month = jun,
	year = {2019},
	keywords = {Untagged},
	pages = {98--104},
}

@inproceedings{tramer_adversarial_2019,
	title = {Adversarial {Training} and {Robustness} for {Multiple} {Perturbations}},
	volume = {32},
	shorttitle = {{SLIDE}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/5d4ae76f053f8f2516ad12961ef7fe97-Abstract.html},
	urldate = {2022-06-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tramer, Florian and Boneh, Dan},
	year = {2019},
	keywords = {Untagged},
}

@article{truexDemystifyingMembershipInference2019,
	title = {Towards {Demystifying} {Membership} {Inference} {Attacks}},
	url = {http://arxiv.org/abs/1807.09173},
	abstract = {Membership inference attacks seek to infer membership of individual training instances of a model to which an adversary has black-box access through a machine learning-as-a-service API. In providing an in-depth characterization of membership privacy risks against machine learning models, this paper presents a comprehensive study towards demystifying membership inference attacks from two complimentary perspectives. First, we provide a generalized formulation of the development of a black-box membership inference attack model. Second, we characterize the importance of model choice on model vulnerability through a systematic evaluation of a variety of machine learning models and model combinations using multiple datasets. Through formal analysis and empirical evidence from extensive experimentation, we characterize under what conditions a model may be vulnerable to such black-box membership inference attacks. We show that membership inference vulnerability is data-driven and corresponding attack models are largely transferable. Though different model types display different vulnerabilities to membership inference, so do different datasets. Our empirical results additionally show that (1) using the type of target model under attack within the attack model may not increase attack effectiveness and (2) collaborative learning exposes vulnerabilities to membership inference risks when the adversary is a participant. We also discuss countermeasure and mitigation strategies.},
	urldate = {2022-03-21},
	journal = {arXiv:1807.09173 [cs]},
	author = {Truex, Stacey and Liu, Ling and Gursoy, Mehmet Emre and Yu, Lei and Wei, Wenqi},
	month = feb,
	year = {2019},
	note = {arXiv: 1807.09173},
	keywords = {Untagged},
}

@article{truexEffectsDifferentialPrivacy2019,
	title = {Effects of {Differential} {Privacy} and {Data} {Skewness} on {Membership} {Inference} {Vulnerability}},
	url = {http://arxiv.org/abs/1911.09777},
	abstract = {Membership inference attacks seek to infer the membership of individual training instances of a privately trained model. This paper presents a membership privacy analysis and evaluation system, called MPLens, with three unique contributions. First, through MPLens, we demonstrate how membership inference attack methods can be leveraged in adversarial machine learning. Second, through MPLens, we highlight how the vulnerability of pre-trained models under membership inference attack is not uniform across all classes, particularly when the training data itself is skewed. We show that risk from membership inference attacks is routinely increased when models use skewed training data. Finally, we investigate the effectiveness of differential privacy as a mitigation technique against membership inference attacks. We discuss the trade-offs of implementing such a mitigation strategy with respect to the model complexity, the learning task complexity, the dataset complexity and the privacy parameter settings. Our empirical results reveal that (1) minority groups within skewed datasets display increased risk for membership inference and (2) differential privacy presents many challenging trade-offs as a mitigation technique to membership inference risk.},
	urldate = {2022-03-23},
	journal = {arXiv:1911.09777 [cs, stat]},
	author = {Truex, Stacey and Liu, Ling and Gursoy, Mehmet Emre and Wei, Wenqi and Yu, Lei},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.09777},
	keywords = {Untagged},
}

@inproceedings{tsipras_robustness_2019,
	title = {Robustness {May} {Be} at {Odds} with {Accuracy}},
	urldate = {2022-06-22},
	booktitle = {{arXiv}:1805.12152},
	publisher = {arXiv},
	author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
	month = sep,
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{uittenbogaardPrivacyProtectionStreetView2019,
	address = {Long Beach, CA, USA},
	title = {Privacy {Protection} in {Street}-{View} {Panoramas} {Using} {Depth} and {Multi}-{View} {Imagery}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954238/},
	doi = {10/ghbmnn},
	abstract = {The current paradigm in privacy protection in streetview images is to detect and blur sensitive information. In this paper, we propose a framework that is an alternative to blurring, which automatically removes and inpaints moving objects (e.g. pedestrians, vehicles) in street-view imagery. We propose a novel moving object segmentation algorithm exploiting consistencies in depth across multiple street-view images that are later combined with the results of a segmentation network. The detected moving objects are removed and inpainted with information from other views, to obtain a realistic output image such that the moving object is not visible anymore. We evaluate our results on a dataset of 1000 images to obtain a peak noise-to-signal ratio (PSNR) and L1 loss of 27.2 dB and 2.5\%, respectively. To assess overall quality, we also report the results of a survey conducted on 35 professionals, asked to visually inspect the images whether object removal and inpainting had taken place. The inpainting dataset will be made publicly available for scientiﬁc benchmarking purposes at https://research.cyclomedia.com/.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Uittenbogaard, Ries and Sebastian, Clint and Vijverberg, Julien and Boom, Bas and Gavrila, Dariu M. and de With, Peter H.N.},
	year = {2019},
	keywords = {Untagged},
	pages = {10573--10582},
}

@inproceedings{vanderhoevenUserSpecifiedLocalDifferential2019,
	title = {User-{Specified} {Local} {Differential} {Privacy} in {Unconstrained} {Adaptive} {Online} {Learning}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/file/ddb4955263e6c08179393d1beaf18602-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {van der Hoeven, Dirk},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	keywords = {Untagged},
	pages = {14103--14112},
}

@phdthesis{vijayarajuImageRetrievalUsing2019,
	address = {San Jose, CA, USA},
	type = {Master of {Science}},
	title = {Image {Retrieval} {Using} {Image} {Captioning}},
	url = {https://scholarworks.sjsu.edu/etd_projects/687},
	abstract = {The rapid growth in the availability of the Internet and smartphones have resulted in the increase in usage of social media in recent years. This increased usage has thereby resulted in the exponential growth of digital images which are available. Therefore, image retrieval systems play a major role in fetching images relevant to the query provided by the users. These systems should also be able to handle the massive growth of data and take advantage of the emerging technologies, like deep learning and image captioning. This report aims at understanding the purpose of image retrieval and various research held in image retrieval in the past. This report will also analyze various gaps in the past research and it will state the role of image captioning in these systems. Additionally, this report proposes a new methodology using image captioning to retrieve images and presents the results of this method, along with comparing the results with past research.},
	language = {en},
	urldate = {2021-01-27},
	school = {San Jose State University},
	author = {Vijayaraju, Nivetha},
	month = may,
	year = {2019},
	doi = {10.31979/etd.vm9n-39ed},
	keywords = {Untagged},
}

@inproceedings{voComposingTextImage2019,
	address = {Long Beach, CA, USA},
	title = {Composing {Text} and {Image} for {Image} {Retrieval} - an {Empirical} {Odyssey}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953387/},
	doi = {10/ghv44f},
	abstract = {In this paper, we study the task of image retrieval, where the input query is speciﬁed in the form of an image plus some text that describes desired modiﬁcations to the input image. For example, we may present an image of the Eiffel tower, and ask the system to ﬁnd images which are visually similar, but are modiﬁed in small ways, such as being taken at nighttime instead of during the day. To tackle this task, we embed the query (reference image plus modiﬁcation text) and the target (images). The encoding function of the image text query learns a representation, such that the similarity with the target image representation is high iff it is a “positive match”. We propose a new way to combine image and text through residual connection, that is designed for this retrieval task. We show this outperforms existing approaches on 3 different datasets, namely Fashion-200k, MIT-States and a new synthetic dataset we create based on CLEVR. We also show that our approach can be used to perform image classiﬁcation with compositionally novel labels, and we outperform previous methods on MIT-States on this task.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Vo, Nam and Jiang, Lu and Sun, Chen and Murphy, Kevin and Li, Li-Jia and Fei-Fei, Li and Hays, James},
	year = {2019},
	keywords = {Untagged},
	pages = {6432--6441},
}

@inproceedings{wallace_universal_2019,
	address = {Hong Kong, China},
	title = {Universal {Adversarial} {Triggers} for {Attacking} and {Analyzing} {NLP}},
	url = {https://aclanthology.org/D19-1221},
	doi = {10.18653/v1/D19-1221},
	abstract = {Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94\% to 0.55\%, 72\% of “why” questions in SQuAD to be answered “to kill american people”, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.},
	urldate = {2023-04-12},
	booktitle = {Empirical {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
	month = nov,
	year = {2019},
	keywords = {Untagged},
	pages = {2153--2162},
}

@inproceedings{wangAdaptivityOptimalityUniversal2019,
	address = {Tel Aviv, Israel},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Adaptivity and {Optimality}: {A} {Universal} {Algorithm} for {Online} {Convex} {Optimization}},
	volume = {115},
	url = {http://proceedings.mlr.press/v115/wang20e.html},
	booktitle = {Conference on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {AUAI Press},
	author = {Wang, Guanghui and Lu, Shiyin and Zhang, Lijun},
	editor = {Globerson, Amir and Silva, Ricardo},
	year = {2019},
	keywords = {Untagged},
	pages = {659--668},
}

@inproceedings{wangPhysicalActivityRecognition2019,
	title = {Physical {Activity} {Recognition} {Using} {Multi}-{Sensor} {Fusion} and {Extreme} {Learning} {Machines}},
	doi = {10.1109/IJCNN.2019.8852175},
	abstract = {Sensor based Physical Activity (PA) recognition is an imperative research topic due to its wide applications in many areas such as mobile healthcare, elderly care and personalized recommendation. However, PA recognition based on single sensor suffers from issues such as limited spatial coverage, imprecision and uncertainty. In contrast, fusion of heterogeneous sensor sources brings improved resolution, precision and robustness. In this paper, a multi-sensor fusion method based on variants of Extreme Learning Machine (ELM) is presented to improve the recognition performance in terms of speed and accuracy. The Kernel ELM, Weighted ELM, Regularized ELM are tasked directly to handle the multi-activity classification problem using the real-world test data with 46 test subjects. Their performance has been evaluated against previously published methods based on Support Vector Machines (SVM) and K-Nearest Neighbor (KNN) using a two-step subject-wise cross validation scheme. The multi-sensor fusion based Kernel ELM has shown favorable performance with 88.4\% average testing accuracy, outperforming SVM by 1.1\%. And for real-time PA recognition, Kernel ELM is 30\% faster than SVM. The ELMs trained with the two-step, subject-wise cross validation approach, combined with feature level data fusion also led to a better cross-person generalization than SVM and KNN.},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	author = {Wang, Honggang and Yan, Weizhong and Liu, Shaopeng},
	year = {2019},
	note = {ISSN: 2161-4407},
	keywords = {Untagged},
	pages = {1--7},
}

@article{wangDeepLearningSensorbased2019,
	title = {Deep learning for sensor-based activity recognition: {A} survey},
	volume = {119},
	issn = {01678655},
	shorttitle = {Deep learning for sensor-based activity recognition},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016786551830045X},
	doi = {10.1016/j.patrec.2018.02.010},
	language = {en},
	urldate = {2021-04-09},
	journal = {Pattern Recognition Letters},
	author = {Wang, Jindong and Chen, Yiqiang and Hao, Shuji and Peng, Xiaohui and Hu, Lisha},
	month = mar,
	year = {2019},
	keywords = {Untagged},
	pages = {3--11},
}

@inproceedings{wangReinforcedCrossModalMatching2019,
	address = {Long Beach, CA, USA},
	title = {Reinforced {Cross}-{Modal} {Matching} and {Self}-{Supervised} {Imitation} {Learning} for {Vision}-{Language} {Navigation}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953608/},
	doi = {10/ggjfsg},
	abstract = {Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model signiﬁcantly outperforms previous methods by 10\% on SPL and achieves the new state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efﬁcient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7\% to 11.7\%).},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Wang, Xin and Huang, Qiuyuan and Celikyilmaz, Asli and Gao, Jianfeng and Shen, Dinghan and Wang, Yuan-Fang and Wang, William Yang and Zhang, Lei},
	year = {2019},
	keywords = {Untagged},
	pages = {6622--6631},
}

@inproceedings{wangSubsampledRenyiDifferential2019,
	address = {Naha, Okinawa, Japan},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Subsampled {Renyi} {Differential} {Privacy} and {Analytical} {Moments} {Accountant}},
	volume = {89},
	url = {http://proceedings.mlr.press/v89/wang19b.html},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Wang, Yu-Xiang and Balle, Borja and Kasiviswanathan, Shiva Prasad},
	editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
	year = {2019},
	keywords = {Untagged},
	pages = {1226--1235},
}

@article{wardAdaGradStepsizesSharp2019,
	title = {{AdaGrad} stepsizes: {Sharp} convergence over nonconvex landscapes, from any initialization},
	shorttitle = {{AdaGrad} stepsizes},
	url = {http://arxiv.org/abs/1806.01811},
	abstract = {Adaptive gradient methods such as AdaGrad and its variants update the stepsize in stochastic gradient descent on the ﬂy according to the gradients received along the way; such methods have gained widespread use in large-scale optimization for their ability to converge robustly, without the need to ﬁne-tune the stepsize schedule. Yet, the theoretical guarantees to date for AdaGrad are for online and convex optimization. We bridge this gap by providing theoretical guarantees for the convergence of AdaGrad for smooth, nonconvex functions. We show that the nor√m version of AdaGrad (AdaGrad-Norm) converges to a stationary point at the O(log(N )/ N ) rate in the stochastic setting, and at the optimal O(1/N ) rate in the batch (non-stochastic) setting – in this sense, our convergence guarantees are “sharp”. In particular, the convergence of AdaGrad-Norm is robust to the choice of all hyper-parameters of the algorithm, in contrast to stochastic gradient descent whose convergence depends crucially on tuning the step-size to the (generally unknown) Lipschitz smoothness constant and level of stochastic noise on the gradient. Extensive numerical experiments are provided to corroborate our theory; moreover, the experiments suggest that the robustness of AdaGrad-Norm extends to state-of-the-art models in deep learning, without sacriﬁcing generalization.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1806.01811 [cs, stat]},
	author = {Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
	month = apr,
	year = {2019},
	note = {arXiv: 1806.01811},
	keywords = {Untagged},
}

@article{weiRPCLargeScaleRetail2019,
	title = {{RPC}: {A} {Large}-{Scale} {Retail} {Product} {Checkout} {Dataset}},
	journal = {arXiv preprint arXiv:1901.07249},
	author = {Wei, Xiu-Shen and Cui, Quan and Yang, Lei and Wang, Peng and Liu, Lingqiao},
	year = {2019},
	keywords = {Untagged},
}

@article{weiPiecewiseClassifierMappings2019,
	title = {Piecewise {Classifier} {Mappings}: {Learning} {Fine}-{Grained} {Learners} for {Novel} {Categories} {With} {Few} {Examples}},
	volume = {28},
	issn = {1057-7149, 1941-0042},
	shorttitle = {Piecewise {Classifier} {Mappings}},
	url = {https://ieeexplore.ieee.org/document/8752297/},
	doi = {10/ggzxdv},
	abstract = {Humans are capable of learning a new ﬁne-grained concept with very little supervision, e.g., few exemplary images for a species of bird, yet our best deep learning systems need hundreds or thousands of labeled examples. In this paper, we try to reduce this gap by studying the ﬁne-grained image recognition problem in a challenging few-shot learning setting, termed fewshot ﬁne-grained recognition (FSFG). The task of FSFG requires the learning systems to build classiﬁers for novel ﬁne-grained categories from few examples (only one or less than ﬁve). To solve this problem, we propose an end-to-end trainable deep network which is inspired by the state-of-the-art ﬁne-grained recognition model and is tailored for the FSFG task.},
	language = {en},
	number = {12},
	urldate = {2021-01-27},
	journal = {IEEE Transactions on Image Processing},
	author = {Wei, Xiu-Shen and Wang, Peng and Liu, Lingqiao and Shen, Chunhua and Wu, Jianxin},
	month = dec,
	year = {2019},
	keywords = {Untagged},
	pages = {6116--6125},
}

@article{weiDeepLearningFineGrained2019,
	title = {Deep {Learning} for {Fine}-{Grained} {Image} {Analysis}: {A} {Survey}},
	shorttitle = {Deep {Learning} for {Fine}-{Grained} {Image} {Analysis}},
	url = {http://arxiv.org/abs/1907.03069},
	abstract = {Computer vision (CV) is the process of using machines to understand and analyze imagery, which is an integral branch of artiﬁcial intelligence. Among various research areas of CV, ﬁne-grained image analysis (FGIA) is a longstanding and fundamental problem, and has become ubiquitous in diverse real-world applications. The task of FGIA targets analyzing visual objects from subordinate categories, e.g., species of birds or models of cars. The small inter-class variations and the large intra-class variations caused by the ﬁne-grained nature makes it a challenging problem. During the booming of deep learning, recent years have witnessed remarkable progress of FGIA using deep learning techniques. In this paper, we aim to give a survey on recent advances of deep learning based FGIA techniques in a systematic way. Speciﬁcally, we organize the existing studies of FGIA techniques into three major categories: ﬁne-grained image recognition, ﬁne-grained image retrieval and ﬁne-grained image generation. In addition, we also cover some other important issues of FGIA, such as publicly available benchmark datasets and its related domain speciﬁc applications. Finally, we conclude this survey by highlighting several directions and open problems which need be further explored by the community in the future.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1907.03069 [cs]},
	author = {Wei, Xiu-Shen and Wu, Jianxin and Cui, Quan},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.03069},
	keywords = {Untagged},
}

@article{wuCharacterizingMembershipPrivacy2019,
	title = {Characterizing {Membership} {Privacy} in {Stochastic} {Gradient} {Langevin} {Dynamics}},
	url = {http://arxiv.org/abs/1910.02249},
	abstract = {Bayesian deep learning is recently regarded as an intrinsic way to characterize the weight uncertainty of deep neural networks{\textasciitilde}(DNNs). Stochastic Gradient Langevin Dynamics{\textasciitilde}(SGLD) is an effective method to enable Bayesian deep learning on large-scale datasets. Previous theoretical studies have shown various appealing properties of SGLD, ranging from the convergence properties to the generalization bounds. In this paper, we study the properties of SGLD from a novel perspective of membership privacy protection (i.e., preventing the membership attack). The membership attack, which aims to determine whether a specific sample is used for training a given DNN model, has emerged as a common threat against deep learning algorithms. To this end, we build a theoretical framework to analyze the information leakage (w.r.t. the training dataset) of a model trained using SGLD. Based on this framework, we demonstrate that SGLD can prevent the information leakage of the training dataset to a certain extent. Moreover, our theoretical analysis can be naturally extended to other types of Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods. Empirical results on different datasets and models verify our theoretical findings and suggest that the SGLD algorithm can not only reduce the information leakage but also improve the generalization ability of the DNN models in real-world applications.},
	urldate = {2022-03-21},
	journal = {arXiv:1910.02249 [cs, stat]},
	author = {Wu, Bingzhe and Chen, Chaochao and Zhao, Shiwan and Chen, Cen and Yao, Yuan and Sun, Guangyu and Wang, Li and Zhang, Xiaolu and Zhou, Jun},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.02249},
	keywords = {Untagged},
}

@inproceedings{wuP3SGDPatientPrivacy2019,
	address = {Long Beach, CA, USA},
	title = {{P3SGD}: {Patient} {Privacy} {Preserving} {SGD} for {Regularizing} {Deep} {CNNs} in {Pathological} {Image} {Classification}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{P3SGD}},
	url = {https://ieeexplore.ieee.org/document/8953573/},
	doi = {10/ghv286},
	abstract = {Recently, deep convolutional neural networks (CNNs) have achieved great success in pathological image classiﬁcation. However, due to the limited number of labeled pathological images, there are still two challenges to be addressed: (1) overﬁtting: the performance of a CNN model is undermined by the overﬁtting due to its huge amounts of parameters and the insufﬁciency of labeled training data. (2) privacy leakage: the model trained using a conventional method may involuntarily reveal the private information of the patients in the training dataset. The smaller the dataset, the worse the privacy leakage.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Wu, Bingzhe and Zhao, Shiwan and Sun, Guangyu and Zhang, Xiaolu and Su, Zhong and Zeng, Caihong and Liu, Zhihong},
	month = jun,
	year = {2019},
	keywords = {Untagged},
	pages = {2094--2103},
}

@inproceedings{wuMutuallyReinforcedSpatioTemporal2019,
	address = {Macao, China},
	title = {Mutually {Reinforced} {Spatio}-{Temporal} {Convolutional} {Tube} for {Human} {Action} {Recognition}},
	url = {https://doi.org/10.24963/ijcai.2019/136},
	doi = {10.24963/ijcai.2019/136},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {ijcai.org},
	author = {Wu, Haoze and Liu, Jiawei and Zha, Zheng-Jun and Chen, Zhenzhong and Sun, Xiaoyan},
	editor = {Kraus, Sarit},
	year = {2019},
	keywords = {Untagged},
	pages = {968--974},
}

@inproceedings{xiaoSharingAttentionWeights2019,
	address = {Macao, China},
	title = {Sharing {Attention} {Weights} for {Fast} {Transformer}},
	isbn = {978-0-9992411-4-1},
	url = {https://www.ijcai.org/proceedings/2019/735},
	doi = {10/gncvcd},
	abstract = {Recently, the Transformer machine translation system has shown strong results by stacking attention layers on both the source and target-language sides. But the inference of this model is slow due to the heavy use of dot-product attention in auto-regressive decoding. In this paper we speed up Transformer via a fast and lightweight attention model. More speciﬁcally, we share attention weights in adjacent layers and enable the efﬁcient re-use of hidden states in a vertical manner. Moreover, the sharing policy can be jointly learned with the MT model. We test our approach on ten WMT and NIST OpenMT tasks. Experimental results show that it yields an average of 1.3X speedup (with almost no decrease in BLEU) on top of a state-of-the-art implementation that has already adopted a cache for fast inference. Also, our approach obtains a 1.8X speed-up when it works with the AAN model. This is even 16 times faster than the baseline with no use of the attention cache.},
	language = {en},
	urldate = {2021-11-09},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Xiao, Tong and Li, Yinqiao and Zhu, Jingbo and Yu, Zhengtao and Liu, Tongran},
	month = aug,
	year = {2019},
	keywords = {Untagged},
	pages = {5292--5298},
}

@article{xieSNASSTOCHASTICNEURAL2019,
	title = {{SNAS}: {STOCHASTIC} {NEURAL} {ARCHITECTURE} {SEARCH}},
	abstract = {We propose Stochastic Neural Architecture Search (SNAS), an economical endto-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of backpropagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efﬁciently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efﬁcient constraint. In experiments on CIFAR-10, SNAS takes fewer epochs to ﬁnd a cell architecture with state-of-theart accuracy than non-differentiable evolution-based and reinforcement-learningbased NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efﬁcient NAS on big datasets.},
	language = {en},
	author = {Xie, Sirui and Zheng, Hehui and Liu, Chunxiao and Lin, Liang},
	year = {2019},
	keywords = {Untagged},
	pages = {17},
}

@article{xuHybridCompositionIdleBlock2019,
	title = {Hybrid {Composition} with {IdleBlock}: {More} {Efficient} {Networks} for {Image} {Recognition}},
	shorttitle = {Hybrid {Composition} with {IdleBlock}},
	url = {http://arxiv.org/abs/1911.08609},
	abstract = {We propose a new building block, IdleBlock, which naturally prunes connections within the block. To fully utilize the IdleBlock we break the tradition of monotonic design in state-of-the-art networks, and introducing hybrid composition with IdleBlock. We study hybrid composition on MobileNet v3 [9] and EfﬁcientNet-B0 [24], two of the most efﬁcient networks. Without any neural architecture search, the deeper MobileNet v3 with hybrid composition design surpasses possibly all state-of-the-art image recognition network designed by human experts or neural architecture search algorithms. Similarly, the hybridized EfﬁcientNetB0 networks are more efﬁcient than previous state-of-theart networks with similar computation budgets. These results suggest a new simpler and more efﬁcient direction for network design and neural architecture search.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1911.08609 [cs]},
	author = {Xu, Bing and Tulloch, Andrew and Chen, Yunpeng and Yang, Xiaomeng and Qiao, Lin},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.08609},
	keywords = {Untagged},
}

@inproceedings{DBLP:conf/uai/XuGG19,
	series = {Proceedings of machine learning research},
	title = {An improved convergence analysis of stochastic variance-reduced policy gradient},
	volume = {115},
	url = {http://proceedings.mlr.press/v115/xu20a.html},
	booktitle = {Proceedings of the thirty-fifth conference on uncertainty in artificial intelligence, {UAI} 2019, tel aviv, israel, july 22-25, 2019},
	publisher = {AUAI Press},
	author = {Xu, Pan and Gao, Felicia and Gu, Quanquan},
	editor = {Globerson, Amir and Silva, Ricardo},
	year = {2019},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/uai/XuGG19.bib
tex.timestamp: Tue, 15 Dec 2020 17:40:18 +0100},
	keywords = {Untagged},
	pages = {541--551},
}

@inproceedings{xuCokeCommunicationCensoredKernel2019,
	address = {Minneapolis, MN, USA},
	title = {Coke: {Communication}-{Censored} {Kernel} {Learning} {Via} {Random} {Features}},
	isbn = {978-1-72810-708-0},
	shorttitle = {Coke},
	url = {https://ieeexplore.ieee.org/document/8755802/},
	doi = {10/ggv959},
	abstract = {Distributed kernel-based methods are attractive in nonlinear learning tasks where either a dataset is too large to be processed on a single machine or the data are only locally available to geographically-located sites. For the ﬁrst case, we propose to split the large dataset into multiple mini-batches and distribute them to distinct sites for parallel learning through the alternating direction method of multipliers (ADMM). For the second case, we develop a decentralized ADMM so that each site can solve the learning task collaboratively through one-hop communications. To circumvent the curse of dimensionality in kernel-based methods, we leverage the random feature approximation to map the large-volume data into a smaller feature space. This also results in a common set of decision parameters that can be exchanged among sites. Motivated by the need to conserve energy and reduce communication overheads, we apply a censoring strategy to evaluate the updated parameter at each site and decide if this update is worth transmitting. The proposed COmmunicationcensored KErnel learning (COKE) algorithms are corroborated to be communication-efﬁcient and learning-effective by simulations on both synthetic and real datasets.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {2019 {IEEE} {Data} {Science} {Workshop} ({DSW})},
	publisher = {IEEE},
	author = {Xu, Ping and Tian, Zhi and Zhang, Zhe and Wang, Yue},
	month = jun,
	year = {2019},
	keywords = {Untagged},
	pages = {32--36},
}

@inproceedings{xueDeepFusionDeepLearning2019,
	address = {Catania Italy},
	title = {{DeepFusion}: {A} {Deep} {Learning} {Framework} for the {Fusion} of {Heterogeneous} {Sensory} {Data}},
	isbn = {978-1-4503-6764-6},
	shorttitle = {{DeepFusion}},
	url = {https://dl.acm.org/doi/10.1145/3323679.3326513},
	doi = {10/gf3sxh},
	abstract = {In recent years, significant research efforts have been spent towards building intelligent and user-friendly IoT systems to enable a new generation of applications capable of performing complex sensing and recognition tasks. In many of such applications, there are usually multiple different sensors monitoring the same object. Each of these sensors can be regarded as an information source and provides us a unique “view” of the observed object. Intuitively, if we can combine the complementary information carried by multiple sensors, we will be able to improve the sensing performance. Towards this end, we propose DeepFusion, a unified multi-sensor deep learning framework, to learn informative representations of heterogeneous sensory data. DeepFusion can combine different sensors’ information weighted by the quality of their data and incorporate cross-sensor correlations, and thus can benefit a wide spectrum of IoT applications. To evaluate the proposed DeepFusion model, we set up two real-world human activity recognition testbeds using commercialized wearable and wireless sensing devices. Experiment results show that DeepFusion can outperform the state-of-the-art human activity recognition methods.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {Proceedings of the {Twentieth} {ACM} {International} {Symposium} on {Mobile} {Ad} {Hoc} {Networking} and {Computing}},
	publisher = {ACM},
	author = {Xue, Hongfei and Jiang, Wenjun and Miao, Chenglin and Yuan, Ye and Ma, Fenglong and Ma, Xin and Wang, Yijiang and Yao, Shuochao and Xu, Wenyao and Zhang, Aidong and Su, Lu},
	month = jul,
	year = {2019},
	keywords = {Untagged},
	pages = {151--160},
}

@article{yamadaShakedropRegularizationDeep2019,
	title = {Shakedrop {Regularization} for {Deep} {Residual} {Learning}},
	volume = {7},
	url = {https://doi.org/10.1109/ACCESS.2019.2960566},
	doi = {10/ghv387},
	journal = {IEEE Access},
	author = {Yamada, Yoshihiro and Iwamura, Masakazu and Akiba, Takuya and Kise, Koichi},
	year = {2019},
	keywords = {Untagged},
	pages = {186126--186136},
}

@inproceedings{yangProxSGDTrainingStructured2019,
	title = {{ProxSGD}: {Training} {Structured} {Neural} {Networks} under {Regularization} and {Constraints}},
	shorttitle = {{ProxSGD}},
	url = {https://openreview.net/forum?id=HygpthEtvr},
	abstract = {We propose a convergent proximal-type stochastic gradient descent algorithm for constrained nonsmooth nonconvex optimization problems},
	language = {en},
	urldate = {2022-02-23},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Yang, Yang and Yuan, Yaxiong and Chatzimichailidis, Avraam and Sloun, Ruud JG van and Lei, Lei and Chatzinotas, Symeon},
	month = sep,
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{yangFeaturePyramidHashing2019,
	title = {Feature {Pyramid} {Hashing}},
	url = {https://doi.org/10.1145/3323873.3325015},
	doi = {10/ghwnqm},
	booktitle = {Proceedings of the 2019 on {International} {Conference} on {Multimedia} {Retrieval}, {ICMR} 2019, {Ottawa}, {ON}, {Canada}, {June} 10-13, 2019},
	publisher = {ACM},
	author = {Yang, Yifan and Geng, Libing and Lai, Hanjiang and Pan, Yan and Yin, Jian},
	editor = {El-Saddik, Abdulmotaleb and Bimbo, Alberto Del and Zhang, Zhongfei and Hauptmann, Alexander G. and Candan, K. Selçuk and Bertini, Marco and Xie, Lexing and Wei, Xiao-Yong},
	year = {2019},
	keywords = {Untagged},
	pages = {114--122},
}

@article{yeBPTransformerModellingLongRange2019,
	title = {{BP}-{Transformer}: {Modelling} {Long}-{Range} {Context} via {Binary} {Partitioning}},
	shorttitle = {{BP}-{Transformer}},
	url = {http://arxiv.org/abs/1911.04070},
	abstract = {The Transformer model is widely successful on many natural language processing tasks. However, the quadratic complexity of self-attention limit its application on long text. In this paper, adopting a fine-to-coarse attention mechanism on multi-scale spans via binary partitioning (BP), we propose BP-Transformer (BPT for short). BPT yields \$O(k{\textbackslash}cdot n{\textbackslash}log (n/k))\$ connections where \$k\$ is a hyperparameter to control the density of attention. BPT has a good balance between computation complexity and model capacity. A series of experiments on text classification, machine translation and language modeling shows BPT has a superior performance for long text than previous self-attention models. Our code, hyperparameters and CUDA kernels for sparse attention are available in PyTorch.},
	urldate = {2021-11-04},
	journal = {arXiv:1911.04070 [cs]},
	author = {Ye, Zihao and Guo, Qipeng and Gan, Quan and Qiu, Xipeng and Zhang, Zheng},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.04070},
	keywords = {Untagged},
}

@inproceedings{yoonTimeseriesGenerativeAdversarial2019,
	address = {Vancouver, BC, Canada},
	title = {Time-series {Generative} {Adversarial} {Networks}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/c9efe5f26cd17ba6216bbe2a7d26d490-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Yoon, Jinsung and Jarrett, Daniel and van der Schaar, Mihaela},
	editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily B. and Garnett, Roman},
	year = {2019},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/nips/YoonJS19.bib
tex.timestamp: Thu, 21 Jan 2021 15:15:19 +0100},
	keywords = {Untagged},
	pages = {5509--5519},
}

@article{yuAcceleratingDeepUnsupervised2019,
	title = {Accelerating {Deep} {Unsupervised} {Domain} {Adaptation} with {Transfer} {Channel} {Pruning}},
	url = {http://arxiv.org/abs/1904.02654},
	abstract = {Deep unsupervised domain adaptation (UDA) has recently received increasing attention from researchers. However, existing methods are computationally intensive due to the computation cost of Convolutional Neural Networks (CNN) adopted by most work. To date, there is no effective network compression method for accelerating these models. In this paper, we propose a uniﬁed Transfer Channel Pruning (TCP) approach for accelerating UDA models. TCP is capable of compressing the deep UDA model by pruning less important channels while simultaneously learning transferable features by reducing the crossdomain distribution divergence. Therefore, it reduces the impact of negative transfer and maintains competitive performance on the target task. To the best of our knowledge, TCP is the ﬁrst approach that aims at accelerating deep UDA models. TCP is validated on two benchmark datasets Ofﬁce-31 and ImageCLEFDA with two common backbone networks VGG16 and ResNet50. Experimental results demonstrate that TCP achieves comparable or better classiﬁcation accuracy than other comparison methods while signiﬁcantly reducing the computational cost. To be more speciﬁc, in VGG16, we get even higher accuracy after pruning 26\% ﬂoating point operations (FLOPs); in ResNet50, we also get higher accuracy on half of the tasks after pruning 12\% FLOPs. We hope that TCP will open a new door for future research on accelerating transfer learning models.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1904.02654 [cs]},
	author = {Yu, Chaohui and Wang, Jindong and Chen, Yiqiang and Wu, Zijing},
	month = mar,
	year = {2019},
	note = {arXiv: 1904.02654},
	keywords = {Untagged},
}

@inproceedings{yuMultimappingImagetoImageTranslation2019,
	address = {Vancouver, BC, Canada},
	title = {Multi-mapping {Image}-to-{Image} {Translation} via {Learning} {Disentanglement}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/5a142a55461d5fef016acfb927fee0bd-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Yu, Xiaoming and Chen, Yuanqi and Liu, Shan and Li, Thomas H. and Li, Ge},
	editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily B. and Garnett, Roman},
	year = {2019},
	keywords = {Untagged},
	pages = {2990--2999},
}

@article{zhangCrossingGenerativeAdversarial2019,
	title = {Crossing generative adversarial networks for cross-view person re-identification},
	volume = {340},
	url = {https://doi.org/10.1016/j.neucom.2019.01.093},
	doi = {10/ghwnj7},
	journal = {Neurocomputing},
	author = {Zhang, Chengyuan and Wu, Lin and Wang, Yang},
	year = {2019},
	keywords = {Untagged},
	pages = {259--269},
}

@inproceedings{zhangDeepNeuralNetwork2019,
	address = {Honolulu, Hawaii},
	title = {A {Deep} {Neural} {Network} for {Unsupervised} {Anomaly} {Detection} and {Diagnosis} in {Multivariate} {Time} {Series} {Data}},
	url = {https://doi.org/10.1609/aaai.v33i01.33011409},
	doi = {10.1609/aaai.v33i01.33011409},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Zhang, Chuxu and Song, Dongjin and Chen, Yuncong and Feng, Xinyang and Lumezanu, Cristian and Cheng, Wei and Ni, Jingchao and Zong, Bo and Chen, Haifeng and Chawla, Nitesh V.},
	year = {2019},
	keywords = {Untagged},
	pages = {1409--1416},
}

@inproceedings{zhangMANMomentAlignment2019,
	address = {Long Beach, CA, USA},
	title = {{MAN}: {Moment} {Alignment} {Network} for {Natural} {Language} {Moment} {Retrieval} via {Iterative} {Graph} {Adjustment}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{MAN}},
	url = {https://ieeexplore.ieee.org/document/8953783/},
	doi = {10/ghv44g},
	abstract = {This research strives for natural language moment retrieval in long, untrimmed video streams. The problem is not trivial especially when a video contains multiple moments of interests and the language describes complex temporal dependencies, which often happens in real scenarios. We identify two crucial challenges: semantic misalignment and structural misalignment. However, existing approaches treat different moments separately and do not explicitly model complex moment-wise temporal relations. In this paper, we present Moment Alignment Network (MAN), a novel framework that uniﬁes the candidate moment encoding and temporal structural reasoning in a single-shot feed-forward network. MAN naturally assigns candidate moment representations aligned with language semantics over different temporal locations and scales. Most importantly, we propose to explicitly model moment-wise temporal relations as a structured graph and devise an iterative graph adjustment network to jointly learn the best structure in an end-to-end manner. We evaluate the proposed approach on two challenging public benchmarks DiDeMo and Charades-STA, where our MAN signiﬁcantly outperforms the state-of-the-art by a large margin.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhang, Da and Dai, Xiyang and Wang, Xin and Wang, Yuan-Fang and Davis, Larry S.},
	year = {2019},
	keywords = {Untagged},
	pages = {1247--1257},
}

@inproceedings{zhang_you_2019,
	title = {You {Only} {Propagate} {Once}: {Accelerating} {Adversarial} {Training} via {Maximal} {Principle}},
	volume = {32},
	shorttitle = {You {Only} {Propagate} {Once}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html},
	urldate = {2022-06-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Dinghuai and Zhang, Tianyuan and Lu, Yiping and Zhu, Zhanxing and Dong, Bin},
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{zhangSelfAttentionGenerativeAdversarial2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Self-{Attention} {Generative} {Adversarial} {Networks}},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/zhang19d.html},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}, {ICML} 2019, 9-15 {June} 2019, {Long} {Beach}, {California}, {USA}},
	publisher = {PMLR},
	author = {Zhang, Han and Goodfellow, Ian J. and Metaxas, Dimitris N. and Odena, Augustus},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	year = {2019},
	keywords = {Untagged},
	pages = {7354--7363},
}

@phdthesis{zhangNewAdvancesSparse2019,
	title = {New {Advances} in {Sparse} {Learning}, {Deep} {Networks}, and {Adversarial} {Learning}: {Theory} and {Applications}},
	language = {en},
	school = {CMU},
	author = {Zhang, Hongyang},
	year = {2019},
	keywords = {Untagged},
}

@inproceedings{zhangDeepNeuralNetworks2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Deep {Neural} {Networks} with {Multi}-{Branch} {Architectures} {Are} {Less} {Non}-{Convex}},
	volume = {89},
	url = {http://proceedings.mlr.press/v89/zhang19d.html},
	booktitle = {The 22nd {International} {Conference} on {Artificial} {Intelligence} and {Statistics}, {AISTATS} 2019, 16-18 {April} 2019, {Naha}, {Okinawa}, {Japan}},
	publisher = {PMLR},
	author = {Zhang, Hongyang and Shao, Junru and Salakhutdinov, Ruslan},
	editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
	year = {2019},
	keywords = {Untagged},
	pages = {1099--1109},
}

@inproceedings{zhangTheoreticallyPrincipledTradeoff2019,
	series = {Proceedings of machine learning research},
	title = {Theoretically principled trade-off between robustness and accuracy},
	volume = {97},
	shorttitle = {{TRADES}},
	url = {http://proceedings.mlr.press/v97/zhang19p.html},
	booktitle = {Proceedings of the 36th international conference on machine learning, {ICML} 2019, 9-15 june 2019, long beach, california, {USA}},
	publisher = {PMLR},
	author = {Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric P. and Ghaoui, Laurent El and Jordan, Michael I.},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	year = {2019},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/icml/ZhangYJXGJ19.bib
tex.timestamp: Mon, 22 Jul 2019 16:49:12 +0200},
	keywords = {Untagged},
	pages = {7472--7482},
}

@inproceedings{zhangMetaCleanerLearningHallucinate2019,
	address = {Long Beach, CA, USA},
	title = {{MetaCleaner}: {Learning} to {Hallucinate} {Clean} {Representations} for {Noisy}-{Labeled} {Visual} {Recognition}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{MetaCleaner}},
	url = {https://ieeexplore.ieee.org/document/8953770/},
	doi = {10.1109/CVPR.2019.00755},
	abstract = {Deep Neural Networks (DNNs) have achieved remarkable successes in large-scale visual recognition. However, they often suffer from overﬁtting under noisy labels. To alleviate this problem, we propose a conceptually simple but effective MetaCleaner, which can learn to hallucinate a clean representation of an object category, according to a small noisy subset from the same category. Specially, MetaCleaner consists of two ﬂexible submodules. The ﬁrst submodule, namely Noisy Weighting, can estimate the conﬁdence scores of all the images in the noisy subset, by analyzing their deep features jointly. The second submodule, namely Clean Hallucinating, can generate a clean representation from the noisy subset, by summarizing the noisy images with their conﬁdence scores. Via MetaCleaner, DNNs can strengthen its robustness to noisy labels, as well as enhance its generalization capacity with richer data diversity. Moreover, MetaCleaner can be easily integrated into the standard training procedure of DNNs, which promotes its value for real-life applications. We conduct extensive experiments on two popular benchmarks in noisylabeled recognition, i.e., Food-101N and Clothing1M. For both datasets, our MetaCleaner signiﬁcantly outperforms baselines, and achieves the state-of-the-art performance.},
	language = {en},
	urldate = {2021-03-18},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhang, Weihe and Wang, Yali and Qiao, Yu},
	year = {2019},
	keywords = {Untagged},
	pages = {7365--7374},
}

@inproceedings{zhangPCAN3DAttention2019,
	address = {Long Beach, CA, USA},
	title = {{PCAN}: {3D} {Attention} {Map} {Learning} {Using} {Contextual} {Information} for {Point} {Cloud} {Based} {Retrieval}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{PCAN}},
	url = {https://ieeexplore.ieee.org/document/8953951/},
	doi = {10/ghv44h},
	abstract = {Point cloud based retrieval for place recognition is an emerging problem in vision ﬁeld. The main challenge is how to ﬁnd an efﬁcient way to encode the local features into a discriminative global descriptor. In this paper, we propose a Point Contextual Attention Network (PCAN), which can predict the signiﬁcance of each local point feature based on point context. Our network makes it possible to pay more attention to the task-relevent features when aggregating local features. Experiments on various benchmark datasets show that the proposed network can provide outperformance than current state-of-the-art approaches.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhang, Wenxiao and Xiao, Chunxia},
	year = {2019},
	keywords = {Untagged},
	pages = {12428--12437},
}

@inproceedings{zhangHIBERTDocumentLevel2019,
	address = {Florence, Italy},
	title = {{HIBERT}: {Document} {Level} {Pre}-training of {Hierarchical} {Bidirectional} {Transformers} for {Document} {Summarization}},
	shorttitle = {{HIBERT}},
	url = {https://www.aclweb.org/anthology/P19-1499},
	doi = {10/gg33gk},
	abstract = {Neural extractive summarization models usually employ a hierarchical encoder for document encoding and they are trained using sentence-level labels, which are created heuristically using rule-based methods. Training the hierarchical encoder with these inaccurate labels is challenging. Inspired by the recent work on pre-training transformer sentence encoders (Devlin et al., 2018), we propose HIBERT (as shorthand for HIerachical Bidirectional Encoder Representations from Transformers) for document encoding and a method to pre-train it using unlabeled data. We apply the pre-trained HIBERT to our summarization model and it outperforms its randomly initialized counterpart by 1.25 ROUGE on the CNN/Dailymail dataset and by 2.0 ROUGE on a version of New York Times dataset. We also achieve the state-of-the-art performance on these two datasets.},
	language = {en},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Xingxing and Wei, Furu and Zhou, Ming},
	year = {2019},
	keywords = {Untagged},
	pages = {5059--5069},
}

@inproceedings{pmlr-v97-zhang19i,
	series = {Proceedings of machine learning research},
	title = {Bridging theory and algorithm for domain adaptation},
	volume = {97},
	url = {https://proceedings.mlr.press/v97/zhang19i.html},
	abstract = {This paper addresses the problem of unsupervised domain adaption from theoretical and algorithmic perspectives. Existing domain adaptation theories naturally imply minimax optimization algorithms, which connect well with the domain adaptation methods based on adversarial learning. However, several disconnections still exist and form the gap between theory and algorithm. We extend previous theories (Mansour et al., 2009c; Ben-David et al., 2010) to multiclass classification in domain adaptation, where classifiers based on the scoring functions and margin loss are standard choices in algorithm design. We introduce Margin Disparity Discrepancy, a novel measurement with rigorous generalization bounds, tailored to the distribution comparison with the asymmetric margin loss, and to the minimax optimization for easier training. Our theory can be seamlessly transformed into an adversarial learning algorithm for domain adaptation, successfully bridging the gap between theory and algorithm. A series of empirical studies show that our algorithm achieves the state of the art accuracies on challenging domain adaptation tasks.},
	booktitle = {Proceedings of the 36th international conference on machine learning},
	publisher = {PMLR},
	author = {Zhang, Yuchen and Liu, Tianle and Long, Mingsheng and Jordan, Michael},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	note = {tex.pdf: http://proceedings.mlr.press/v97/zhang19i/zhang19i.pdf},
	keywords = {Untagged},
	pages = {7404--7413},
}

@article{zhaoExplicitSparseTransformer2019,
	title = {Explicit {Sparse} {Transformer}: {Concentrated} {Attention} {Through} {Explicit} {Selection}},
	shorttitle = {Explicit {Sparse} {Transformer}},
	url = {http://arxiv.org/abs/1912.11637},
	abstract = {Self-attention based Transformer has demonstrated the state-of-the-art performances in a number of natural language processing tasks. Self-attention is able to model long-term dependencies, but it may suffer from the extraction of irrelevant information in the context. To tackle the problem, we propose a novel model called {\textbackslash}textbf\{Explicit Sparse Transformer\}. Explicit Sparse Transformer is able to improve the concentration of attention on the global context through an explicit selection of the most relevant segments. Extensive experimental results on a series of natural language processing and computer vision tasks, including neural machine translation, image captioning, and language modeling, all demonstrate the advantages of Explicit Sparse Transformer in model performance. We also show that our proposed sparse attention method achieves comparable or better results than the previous sparse attention method, but significantly reduces training and testing time. For example, the inference speed is twice that of sparsemax in Transformer model. Code will be available at {\textbackslash}url\{https://github.com/lancopku/Explicit-Sparse-Transformer\}},
	language = {en},
	urldate = {2021-11-04},
	journal = {arXiv:1912.11637 [cs]},
	author = {Zhao, Guangxiang and Lin, Junyang and Zhang, Zhiyuan and Ren, Xuancheng and Su, Qi and Sun, Xu},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.11637},
	keywords = {Untagged},
}

@inproceedings{zhao_learning_2019,
	title = {On {Learning} {Invariant} {Representations} for {Domain} {Adaptation}},
	url = {https://proceedings.mlr.press/v97/zhao19a.html},
	abstract = {Due to the ability of deep neural nets to learn rich representations, recent advances in unsupervised domain adaptation have focused on learning domain-invariant features that achieve a small error on the source domain. The hope is that the learnt representation, together with the hypothesis learnt from the source domain, can generalize to the target domain. In this paper, we first construct a simple counterexample showing that, contrary to common belief, the above conditions are not sufficient to guarantee successful domain adaptation. In particular, the counterexample exhibits conditional shift: the class-conditional distributions of input features change between source and target domains. To give a sufficient condition for domain adaptation, we propose a natural and interpretable generalization upper bound that explicitly takes into account the aforementioned shift. Moreover, we shed new light on the problem by proving an information-theoretic lower bound on the joint error of any domain adaptation method that attempts to learn invariant representations. Our result characterizes a fundamental tradeoff between learning invariant representations and achieving small joint error on both domains when the marginal label distributions differ from source to target. Finally, we conduct experiments on real-world datasets that corroborate our theoretical findings. We believe these insights are helpful in guiding the future design of domain adaptation and representation learning algorithms.},
	language = {en},
	urldate = {2022-11-14},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhao, Han and Combes, Remi Tachet Des and Zhang, Kun and Gordon, Geoffrey},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {7523--7532},
}

@inproceedings{zhaoRetrievalAugmentedConvolutionalNeural2019,
	address = {Long Beach, CA, USA},
	title = {Retrieval-{Augmented} {Convolutional} {Neural} {Networks} {Against} {Adversarial} {Examples}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953314/},
	doi = {10/ghv44j},
	abstract = {We propose a retrieval-augmented convolutional network (RaCNN) and propose to train it with local mixup, a novel variant of the recently proposed mixup algorithm. The proposed hybrid architecture combining a convolutional network and an off-the-shelf retrieval engine was designed to mitigate the adverse effect of off-manifold adversarial examples, while the proposed local mixup addresses on-manifold ones by explicitly encouraging the classiﬁer to locally behave linearly on the data manifold. Our evaluation of the proposed approach against seven readilyavailable adversarial attacks on three datasets–CIFAR-10, SVHN and ImageNet–demonstrate the improved robustness compared to a vanilla convolutional network, and comparable performance with the state-of-the-art reactive defense approaches.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhao, Jake Junbo and Cho, Kyunghyun},
	year = {2019},
	keywords = {Untagged},
	pages = {11555--11563},
}

@article{zhao2019decentralized,
	title = {Decentralized online learning: {Take} benefits from others' data without sharing your own to track global trend},
	author = {Zhao, Yawei and Yu, Chen and Zhao, Peilin and Tang, Hanlin and Qiu, Shuang and Liu, Ji},
	year = {2019},
	note = {arXiv: 1901.10593 [cs.LG]},
	keywords = {Untagged},
}

@article{zhaoObjectDetectionDeep2019,
	title = {Object {Detection} {With} {Deep} {Learning}: {A} {Review}},
	volume = {30},
	url = {https://doi.org/10.1109/TNNLS.2018.2876865},
	doi = {10/gf3w39},
	number = {11},
	journal = {IEEE Trans. Neural Networks Learn. Syst.},
	author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-tao and Wu, Xindong},
	year = {2019},
	keywords = {Untagged},
	pages = {3212--3232},
}

@inproceedings{zhenDeepSupervisedCrossModal2019,
	address = {Long Beach, CA, USA},
	title = {Deep {Supervised} {Cross}-{Modal} {Retrieval}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953669/},
	doi = {10/gg3wgx},
	abstract = {Cross-modal retrieval aims to enable ﬂexible retrieval across different modalities. The core of cross-modal retrieval is how to measure the content similarity between different types of data. In this paper, we present a novel cross-modal retrieval method, called Deep Supervised Cross-modal Retrieval (DSCMR). It aims to ﬁnd a common representation space, in which the samples from different modalities can be compared directly. Speciﬁcally, DSCMR minimises the discrimination loss in both the label space and the common representation space to supervise the model learning discriminative features. Furthermore, it simultaneously minimises the modality invariance loss and uses a weight sharing strategy to eliminate the cross-modal discrepancy of multimedia data in the common representation space to learn modality-invariant features. Comprehensive experimental results on four widely-used benchmark datasets demonstrate that the proposed method is effective in cross-modal learning and signiﬁcantly outperforms the state-of-the-art cross-modal retrieval methods.},
	language = {en},
	urldate = {2021-11-22},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhen, Liangli and Hu, Peng and Wang, Xu and Peng, Dezhong},
	month = jun,
	year = {2019},
	keywords = {Untagged},
	pages = {10386--10395},
}

@inproceedings{zhengLearningDeepBilinear2019,
	address = {Vancouver, BC, Canada},
	title = {Learning {Deep} {Bilinear} {Transformation} for {Fine}-grained {Image} {Representation}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/959ef477884b6ac2241b19ee4fb776ae-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zheng, Heliang and Fu, Jianlong and Zha, Zheng-Jun and Luo, Jiebo},
	editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily B. and Garnett, Roman},
	year = {2019},
	keywords = {Untagged},
	pages = {4279--4288},
}

@inproceedings{zhengOptimalFineGrained2019,
	title = {Towards {Optimal} {Fine} {Grained} {Retrieval} via {Decorrelated} {Centralized} {Loss} with {Normalize}-{Scale} {Layer}},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zheng, Xiawu and Ji, Rongrong and Sun, Xiaoshuai and Zhang, Baochang and Wu, Yongjian and Huang, Feiyue},
	year = {2019},
	keywords = {Untagged},
	pages = {9291--9298},
}

@article{zhengPedestrianAlignmentNetwork2019,
	title = {Pedestrian {Alignment} {Network} for {Large}-scale {Person} {Re}-identification},
	volume = {29},
	issn = {1051-8215, 1558-2205},
	url = {http://arxiv.org/abs/1707.00408},
	doi = {10/gfxgzz},
	abstract = {Person re-identification (person re-ID) is mostly viewed as an image retrieval problem. This task aims to search a query person in a large image pool. In practice, person re-ID usually adopts automatic detectors to obtain cropped pedestrian images. However, this process suffers from two types of detector errors: excessive background and part missing. Both errors deteriorate the quality of pedestrian alignment and may compromise pedestrian matching due to the position and scale variances. To address the misalignment problem, we propose that alignment can be learned from an identification procedure. We introduce the pedestrian alignment network (PAN) which allows discriminative embedding learning and pedestrian alignment without extra annotations. Our key observation is that when the convolutional neural network (CNN) learns to discriminate between different identities, the learned feature maps usually exhibit strong activations on the human body rather than the background. The proposed network thus takes advantage of this attention mechanism to adaptively locate and align pedestrians within a bounding box. Visual examples show that pedestrians are better aligned with PAN. Experiments on three large-scale re-ID datasets confirm that PAN improves the discriminative ability of the feature embeddings and yields competitive accuracy with the state-of-the-art methods.},
	language = {en},
	number = {10},
	urldate = {2021-01-27},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Zheng, Zhedong and Zheng, Liang and Yang, Yi},
	month = oct,
	year = {2019},
	note = {arXiv: 1707.00408},
	keywords = {Untagged},
	pages = {3037--3045},
}

@article{zhouObjectsPoints2019,
	title = {Objects as {Points}},
	url = {http://arxiv.org/abs/1904.07850},
	abstract = {Detection identiﬁes objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefﬁcient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point — the center point of its bounding box. Our detector uses keypoint estimation to ﬁnd center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1\% AP at 142 FPS, 37.4\% AP at 52 FPS, and 45.1\% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1904.07850 [cs]},
	author = {Zhou, Xingyi and Wang, Dequan and Krähenbühl, Philipp},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.07850},
	keywords = {Untagged},
}

@inproceedings{zhouCollaborativeLearningSemiSupervised2019,
	address = {Long Beach, CA, USA},
	title = {Collaborative {Learning} of {Semi}-{Supervised} {Segmentation} and {Classification} for {Medical} {Images}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953911/},
	doi = {10/ghv3gr},
	abstract = {Medical image analysis has two important research areas: disease grading and ﬁne-grained lesion segmentation. Although the former problem often relies on the latter, the two are usually studied separately. Disease severity grading can be treated as a classiﬁcation problem, which only requires image-level annotations, while the lesion segmentation requires stronger pixel-level annotations. However, pixel-wise data annotation for medical images is highly time-consuming and requires domain experts. In this paper, we propose a collaborative learning method to jointly improve the performance of disease grading and lesion segmentation by semi-supervised learning with an attention mechanism. Given a small set of pixel-level annotated data, a multi-lesion mask generation model ﬁrst performs the traditional semantic segmentation task. Then, based on initially predicted lesion maps for large quantities of imagelevel annotated data, a lesion attentive disease grading model is designed to improve the severity classiﬁcation accuracy. Meanwhile, the lesion attention model can reﬁne the lesion maps using class-speciﬁc information to ﬁne-tune the segmentation model in a semi-supervised manner. An adversarial architecture is also integrated for training. With extensive experiments on a representative medical problem called diabetic retinopathy (DR), we validate the effectiveness of our method and achieve consistent improvements over state-of-the-art methods on three public datasets.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhou, Yi and He, Xiaodong and Huang, Lei and Liu, Li and Zhu, Fan and Cui, Shanshan and Shao, Ling},
	year = {2019},
	keywords = {Untagged},
	pages = {2074--2083},
}

@inproceedings{zhuGANCrossModalRecipe2019,
	address = {Long Beach, CA, USA},
	title = {R²{GAN}: {Cross}-{Modal} {Recipe} {Retrieval} {With} {Generative} {Adversarial} {Network}},
	isbn = {978-1-72813-293-8},
	shorttitle = {R²{GAN}},
	url = {https://ieeexplore.ieee.org/document/8953293/},
	doi = {10/gg3wgz},
	abstract = {Representing procedure text such as recipe for crossmodal retrieval is inherently a difﬁcult problem, not mentioning to generate image from recipe for visualization. This paper studies a new version of GAN, named Recipe Retrieval Generative Adversarial Network (R2GAN ), to explore the feasibility of generating image from procedure text for retrieval problem. The motivation of using GAN is twofold: learning compatible cross-modal features in an adversarial way, and explanation of search results by showing the images generated from recipes. The novelty of R2GAN comes from architecture design, speciﬁcally a GAN with one generator and dual discriminators is used, which makes the generation of image from recipe a feasible idea. Furthermore, empowered by the generated images, a two-level ranking loss in both embedding and image spaces are considered. These add-ons not only result in excellent retrieval performance, but also generate close-to-realistic food images useful for explaining ranking of recipes. On recipe1M dataset, R2GAN demonstrates high scalability to data size, outperforms all the existing approaches, and generates images intuitive for human to interpret the search results.},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhu, Bin and Ngo, Chong-Wah and Chen, Jingjing and Hao, Yanbin},
	year = {2019},
	keywords = {Untagged},
	pages = {11469--11478},
}

@inproceedings{zhuPoissionSubsampledRenyi2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Poission {Subsampled} {Rényi} {Differential} {Privacy}},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/zhu19c.html},
	abstract = {We consider the problem of privacy-amplification by under the Renyi Differential Privacy framework. This is the main technique underlying the moments accountants (Abadi et al., 2016) for differentially private deep learning. Unlike previous attempts on this problem which deals with Sampling with Replacement, we consider the Poisson subsampling scheme which selects each data point independently with a coin toss. This allows us to significantly simplify and tighten the bounds for the RDP of subsampled mechanisms and derive numerically stable approximation schemes. In particular, for subsampled Gaussian mechanism and subsampled Laplace mechanism, we prove an analytical formula of their RDP that exactly matches the lower bound. The result is the first of its kind and we numerically demonstrate an order of magnitude improvement in the privacy-utility tradeoff.},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhu, Yuqing and Wang, Yu-Xiang},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	keywords = {Untagged},
	pages = {7634--7642},
}

@article{ainslieETCEncodingLong2020,
	title = {{ETC}: {Encoding} {Long} and {Structured} {Inputs} in {Transformers}},
	shorttitle = {{ETC}},
	url = {http://arxiv.org/abs/2004.08483},
	abstract = {Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pretraining objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.},
	language = {en},
	urldate = {2021-11-04},
	journal = {arXiv:2004.08483 [cs, stat]},
	author = {Ainslie, Joshua and Ontanon, Santiago and Alberti, Chris and Cvicek, Vaclav and Fisher, Zachary and Pham, Philip and Ravula, Anirudh and Sanghai, Sumit and Wang, Qifan and Yang, Li},
	month = oct,
	year = {2020},
	note = {arXiv: 2004.08483},
	keywords = {Untagged},
}

@inproceedings{alaniClassifyingImbalancedMultimodal2020,
	title = {Classifying {Imbalanced} {Multi}-modal {Sensor} {Data} for {Human} {Activity} {Recognition} in a {Smart} {Home} using {Deep} {Learning}},
	doi = {10.1109/IJCNN48605.2020.9207697},
	abstract = {In smart homes, data generated from real-time sensors for human activity recognition is complex, noisy and imbalanced. It is a significant challenge to create machine learning models that can classify activities which are not as commonly occurring as other activities. Machine learning models designed to classify imbalanced data are biased towards learning the more commonly occurring classes. Such learning bias occurs naturally, since the models better learn classes which contain more records. This paper examines whether fusing real-world imbalanced multi-modal sensor data improves classification results as opposed to using unimodal data; and compares deep learning approaches to dealing with imbalanced multi-modal sensor data when using various resampling methods and deep learning models. Experiments were carried out using a large multi-modal sensor dataset generated from the Sensor Platform for HEalthcare in a Residential Environment (SPHERE). The data comprises 16104 samples, where each sample comprises 5608 features and belongs to one of 20 activities (classes). Experimental results using SPHERE demonstrate the challenges of dealing with imbalanced multi-modal data and highlight the importance of having a suitable number of samples within each class for sufficiently training and testing deep learning models. Furthermore, the results revealed that when fusing the data and using the Synthetic Minority Oversampling Technique (SMOTE) to correct class imbalance, CNN-LSTM achieved the highest classification accuracy of 93.67\% followed by CNN, 93.55\%, and LSTM, i.e. 92.98\%.},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	author = {Alani, Ali A. and Cosma, Georgina and Taherkhani, Aboozar},
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {Untagged},
	pages = {1--8},
}

@inproceedings{alharbiSyntheticSensorData2020,
	title = {Synthetic {Sensor} {Data} for {Human} {Activity} {Recognition}},
	doi = {10.1109/IJCNN48605.2020.9206624},
	abstract = {Human activity recognition (HAR) based on wearable sensors has emerged as an active topic of research in machine learning and human behavior analysis because of its applications in several fields, including health, security and surveillance, and remote monitoring. Machine learning algorithms are frequently applied in HAR systems to learn from labeled sensor data. The effectiveness of these algorithms generally relies on having access to lots of accurately labeled training data. But labeled data for HAR is hard to come by and is often heavily imbalanced in favor of one or other dominant classes, which in turn leads to poor recognition performance. In this study we introduce a generative adversarial network (GAN)-based approach for HAR that we use to automatically synthesize balanced and realistic sensor data. GANs are robust generative networks, typically used to create synthetic images that cannot be distinguished from real images. Here we explore and construct a model for generating several types of human activity sensor data using a Wasserstein GAN (WGAN). We assess the synthetic data using two commonly-used classifier models, Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM). We evaluate the quality and diversity of the synthetic data by training on synthetic data and testing on real sensor data, and vice versa. We then use synthetic sensor data to oversample the imbalanced training set. We demonstrate the efficacy of the proposed method on two publicly available human activity datasets, the Sussex-Huawei Locomotion (SHL) and Smoking Activity Dataset (SAD). We achieve improvements of using WGAN augmented training data over the imbalanced case, for both SHL (0.85 to 0.95 F1-score), and for SAD (0.70 to 0.77 F1-score) when using a CNN activity classifier.},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	author = {Alharbi, Fayez and Ouarbya, Lahcen and Ward, Jamie A},
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {Untagged},
	pages = {1--9},
}

@inproceedings{alvesSensorDataHuman2020,
	title = {Sensor {Data} for {Human} {Activity} {Recognition}: {Feature} {Representation} and {Benchmarking}},
	shorttitle = {Sensor {Data} for {Human} {Activity} {Recognition}},
	doi = {10.1109/IJCNN48605.2020.9207068},
	abstract = {The field of Human Activity Recognition (HAR) focuses on obtaining and analysing data captured from monitoring devices (e.g. sensors). There is a wide range of applications within the field; for instance, assisted living, security surveillance, and intelligent transportation. In HAR, the development of Activity Recognition models is dependent upon the data captured by these devices and the methods used to analyse them, which directly affect performance metrics. In this work, we address the issue of accurately recognising human activities using different Machine Learning (ML) techniques. We propose a new feature representation based on consecutive occurring observations and compare it against previously used feature representations using a wide range of classification methods. Experimental results demonstrate that techniques based on the proposed representation outperform the baselines and a better accuracy was achieved for both highly and less frequent actions. We also investigate how the addition of further features and their pre-processing techniques affect performance results leading to state-of-the-art accuracy on a Human Activity Recognition dataset.},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	author = {Alves, Flávia and Gairing, Martin and Oliehoek, Frans A. and Do, Thanh-Toan},
	month = jul,
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {Untagged},
	pages = {1--8},
}

@inproceedings{vedaldi_square_2020,
	address = {Cham},
	title = {Square {Attack}: {A} {Query}-{Efficient} {Black}-{Box} {Adversarial} {Attack} via {Random} {Search}},
	volume = {12368},
	isbn = {978-3-030-58591-4 978-3-030-58592-1},
	shorttitle = {Square {Attack}},
	url = {https://link.springer.com/10.1007/978-3-030-58592-1_29},
	doi = {10.1007/978-3-030-58592-1_29},
	abstract = {We propose the Square Attack, a score-based black-box l2and l∞-adversarial attack that does not rely on local gradient information and thus is not aﬀected by gradient masking. Square Attack is based on a randomized search scheme which selects localized squareshaped updates at random positions so that at each iteration the perturbation is situated approximately at the boundary of the feasible set. Our method is signiﬁcantly more query eﬃcient and achieves a higher success rate compared to the state-of-the-art methods, especially in the untargeted setting. In particular, on ImageNet we improve the average query eﬃciency in the untargeted setting for various deep networks by a factor of at least 1.8 and up to 3 compared to the recent state-ofthe-art l∞-attack of Al-Dujaili \& O’Reilly (2020). Moreover, although our attack is black-box, it can also outperform gradient-based white-box attacks on the standard benchmarks achieving a new state-of-the-art in terms of the success rate. The code of our attack is available at https://github.com/max-andr/square-attack.},
	language = {en},
	urldate = {2022-05-27},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas and Hein, Matthias},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {484--501},
}

@inproceedings{arjovsky_invariant_2020,
	title = {Invariant {Risk} {Minimization}},
	shorttitle = {{ColoredMNIST}},
	url = {http://arxiv.org/abs/1907.02893},
	doi = {10.48550/arXiv.1907.02893},
	abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
	urldate = {2022-06-13},
	booktitle = {{arXiv}:1907.02893},
	publisher = {arXiv},
	author = {Arjovsky, Martin and Bottou, Léon and Gulrajani, Ishaan and Lopez-Paz, David},
	month = mar,
	year = {2020},
	note = {arXiv:1907.02893 [cs, stat]
type: article},
	keywords = {Untagged},
}

@inproceedings{augustinAdversarialRobustnessOutDistribution2020,
	address = {Cham},
	title = {Adversarial {Robustness} on {In}- and {Out}-{Distribution} {Improves} {Explainability}},
	volume = {12371},
	isbn = {978-3-030-58573-0 978-3-030-58574-7},
	url = {https://link.springer.com/10.1007/978-3-030-58574-7_14},
	abstract = {Neural networks have led to major improvements in image classiﬁcation but suﬀer from being non-robust to adversarial changes, unreliable uncertainty estimates on out-distribution samples and their inscrutable black-box decisions. In this work we propose RATIO, a training procedure for Robustness via Adversarial Training on In- and Outdistribution, which leads to robust models with reliable and robust conﬁdence estimates on the out-distribution. RATIO has similar generative properties to adversarial training so that visual counterfactuals produce class speciﬁc features. While adversarial training comes at the price of lower clean accuracy, RATIO achieves state-of-the-art l2-adversarial robustness on CIFAR10 and maintains better clean accuracy.},
	language = {en},
	urldate = {2021-10-29},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Augustin, Maximilian and Meinke, Alexander and Hein, Matthias},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58574-7_14},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {228--245},
}

@article{azimiReviewUseArtificial2020,
	title = {A {Review} on the {Use} of {Artificial} {Intelligence} in {Spinal} {Diseases}},
	volume = {14},
	issn = {1976-1902},
	doi = {10.31616/asj.2020.0147},
	abstract = {Artificial neural networks (ANNs) have been used in a wide variety of real-world applications and it emerges as a promising field across various branches of medicine. This review aims to identify the role of ANNs in spinal diseases. Literature were searched from electronic databases of Scopus and Medline from 1993 to 2020 with English publications reported on the application of ANNs in spinal diseases. The search strategy was set as the combinations of the following keywords: "artificial neural networks," "spine," "back pain," "prognosis," "grading," "classification," "prediction," "segmentation," "biomechanics," "deep learning," and "imaging." The main findings of the included studies were summarized, with an emphasis on the recent advances in spinal diseases and its application in the diagnostic and prognostic procedures. According to the search strategy, a set of 3,653 articles were retrieved from Medline and Scopus databases. After careful evaluation of the abstracts, the full texts of 89 eligible papers were further examined, of which 79 articles satisfied the inclusion criteria of this review. Our review indicates several applications of ANNs in the management of spinal diseases including (1) diagnosis and assessment of spinal disease progression in the patients with low back pain, perioperative complications, and readmission rate following spine surgery; (2) enhancement of the clinically relevant information extracted from radiographic images to predict Pfirrmann grades, Modic changes, and spinal stenosis grades on magnetic resonance images automatically; (3) prediction of outcomes in lumbar spinal stenosis, lumbar disc herniation and patient-reported outcomes in lumbar fusion surgery, and preoperative planning and intraoperative assistance; and (4) its application in the biomechanical assessment of spinal diseases. The evidence suggests that ANNs can be successfully used for optimizing the diagnosis, prognosis and outcome prediction in spinal diseases. Therefore, incorporation of ANNs into spine clinical practice may improve clinical decision making.},
	language = {eng},
	number = {4},
	journal = {Asian Spine Journal},
	author = {Azimi, Parisa and Yazdanian, Taravat and Benzel, Edward C. and Aghaei, Hossein Nayeb and Azhari, Shirzad and Sadeghi, Sohrab and Montazeri, Ali},
	month = aug,
	year = {2020},
	pmid = {32326672},
	pmcid = {PMC7435304},
	keywords = {Untagged},
	pages = {543--571},
}

@inproceedings{bacharidisImprovingDeepLearning2020,
	title = {Improving {Deep} {Learning} {Approaches} for {Human} {Activity} {Recognition} based on {Natural} {Language} {Processing} of {Action} {Labels}},
	doi = {10.1109/IJCNN48605.2020.9207397},
	abstract = {Human activity recognition has always been an appealing research topic in computer vision due its theoretic interest and vast range of applications. In recent years, machine learning has dominated computer vision and human activity recognition research. Supervised learning methods and especially deep learning-based ones are considered to provide the best solutions for this task, achieving state-of-the art results. However, the performance of deep learning-based approaches depends greatly on the modelling capabilities of the spatio-temporal neural network architecture and the learning goals of the training process. Moreover, the design complexity is task-depended. In this paper, we show that we can exploit the information contained in the label description of action classes (action labels) to extract information regarding their similarity which can then be used to steer the learning process and improve the activity recognition performance. Moreover, we experimentally verify that the adopted strategy can be useful in both single and multi-stream architectures, providing better scalability on the training of the network in more complex datasets featuring activity classes with larger intra- and inter-class similarities.},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	author = {Bacharidis, Konstantinos and Argyros, Antonis},
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {Untagged},
	pages = {1--8},
}

@article{balcanPowerAbstentionDatadriven2020,
	title = {On the power of abstention and data-driven decision making for adversarial robustness},
	journal = {arXiv preprint arXiv:2010.06154},
	author = {Balcan, Maria-Florina and Blum, Avrim and Sharma, Dravyansh and Zhang, Hongyang},
	year = {2020},
	keywords = {Untagged},
}

@article{bartlettBenignOverfittingLinear2020,
	title = {Benign {Overfitting} in {Linear} {Regression}},
	url = {http://arxiv.org/abs/1906.11300},
	abstract = {The phenomenon of benign overﬁtting is one of the key mysteries uncovered by deep learning methodology: deep neural networks seem to predict well, even with a perfect ﬁt to noisy training data. Motivated by this phenomenon, we consider when a perfect ﬁt to training data in linear regression is compatible with accurate prediction. We give a characterization of linear regression problems for which the minimum norm interpolating prediction rule has near-optimal prediction accuracy. The characterization is in terms of two notions of the eﬀective rank of the data covariance. It shows that overparameterization is essential for benign overﬁtting in this setting: the number of directions in parameter space that are unimportant for prediction must signiﬁcantly exceed the sample size. By studying examples of data covariance properties that this characterization shows are required for benign overﬁtting, we ﬁnd an important role for ﬁnite-dimensional data: the accuracy of the minimum norm interpolating prediction rule approaches the best possible accuracy for a much narrower range of properties of the data distribution when the data lies in an inﬁnite dimensional space versus when the data lies in a ﬁnite dimensional space whose dimension grows faster than the sample size.},
	language = {en},
	urldate = {2022-01-25},
	journal = {arXiv:1906.11300 [cs, math, stat]},
	author = {Bartlett, Peter L. and Long, Philip M. and Lugosi, Gábor and Tsigler, Alexander},
	month = jan,
	year = {2020},
	note = {arXiv: 1906.11300},
	keywords = {Untagged},
}

@article{behjatiOverNetLightweightMultiScale2020,
	title = {{OverNet}: {Lightweight} {Multi}-{Scale} {Super}-{Resolution} with {Overscaling} {Network}},
	shorttitle = {{OverNet}},
	url = {http://arxiv.org/abs/2008.02382},
	abstract = {Super-resolution (SR) has achieved great success due to the development of deep convolutional neural networks (CNNs). However, as the depth and width of the networks increase, CNN-based SR methods have been faced with the challenge of computational complexity in practice. Moreover, most SR methods train a dedicated model for each target resolution, losing generality and increasing memory requirements. To address these limitations we introduce OverNet, a deep but lightweight convolutional network to solve SISR at arbitrary scale factors with a single model. We make the following contributions: ﬁrst, we introduce a lightweight feature extractor that enforces efﬁcient reuse of information through a novel recursive structure of skip and dense connections. Second, to maximize the performance of the feature extractor, we propose a model agnostic reconstruction module that generates accurate high-resolution images from overscaled feature maps obtained from any SR architecture. Third, we introduce a multi-scale loss function to achieve generalization across scales. Experiments show that our proposal outperforms previous state-of-theart approaches in standard benchmarks, while maintaining relatively low computation and memory requirements.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2008.02382 [cs, eess]},
	author = {Behjati, Parichehr and Rodriguez, Pau and Mehri, Armin and Hupont, Isabelle and Gonzalez, Jordi and Tena, Carles Fernandez},
	month = nov,
	year = {2020},
	note = {arXiv: 2008.02382},
	keywords = {Untagged},
}

@article{beltagyLongformerLongDocumentTransformer2020,
	title = {Longformer: {The} {Long}-{Document} {Transformer}},
	shorttitle = {Longformer},
	url = {http://arxiv.org/abs/2004.05150},
	abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
	language = {en},
	urldate = {2021-11-04},
	journal = {arXiv:2004.05150 [cs]},
	author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
	month = dec,
	year = {2020},
	note = {arXiv: 2004.05150},
	keywords = {Untagged},
}

@misc{bhambri_survey_2020,
	title = {A {Survey} of {Black}-{Box} {Adversarial} {Attacks} on {Computer} {Vision} {Models}},
	url = {http://arxiv.org/abs/1912.01667},
	abstract = {Machine learning has seen tremendous advances in the past few years, which has lead to deep learning models being deployed in varied applications of day-to-day life. Attacks on such models using perturbations, particularly in real-life scenarios, pose a severe challenge to their applicability, pushing research into the direction which aims to enhance the robustness of these models. After the introduction of these perturbations by Szegedy et al. [1], significant amount of research has focused on the reliability of such models, primarily in two aspects - white-box, where the adversary has access to the targeted model and related parameters; and the black-box, which resembles a real-life scenario with the adversary having almost no knowledge of the model to be attacked. To provide a comprehensive security cover, it is essential to identify, study, and build defenses against such attacks. Hence, in this paper, we propose to present a comprehensive comparative study of various black-box adversarial attacks and defense techniques.},
	urldate = {2022-12-04},
	publisher = {arXiv},
	author = {Bhambri, Siddhant and Muku, Sumanyu and Tulasi, Avinash and Buduru, Arun Balaji},
	month = feb,
	year = {2020},
	note = {arXiv:1912.01667 [cs, stat]},
	keywords = {Untagged},
}

@inproceedings{bijelic_seeing_2020,
	address = {Seattle, WA, USA},
	title = {Seeing {Through} {Fog} {Without} {Seeing} {Fog}: {Deep} {Multimodal} {Sensor} {Fusion} in {Unseen} {Adverse} {Weather}},
	isbn = {978-1-72817-168-5},
	shorttitle = {Seeing {Through} {Fog} {Without} {Seeing} {Fog}},
	url = {https://ieeexplore.ieee.org/document/9157107/},
	doi = {10.1109/CVPR42600.2020.01170},
	abstract = {The fusion of multimodal sensor streams, such as camera, lidar, and radar measurements, plays a critical role in object detection for autonomous vehicles, which base their decision making on these inputs. While existing methods exploit redundant information in good environmental conditions, they fail in adverse weather where the sensory streams can be asymmetrically distorted. These rare “edgecase” scenarios are not represented in available datasets, and existing fusion architectures are not designed to handle them. To address this challenge we present a novel multimodal dataset acquired in over 10,000 km of driving in northern Europe. Although this dataset is the ﬁrst large multimodal dataset in adverse weather, with 100k labels for lidar, camera, radar, and gated NIR sensors, it does not facilitate training as extreme weather is rare. To this end, we present a deep fusion network for robust fusion without a large corpus of labeled training data covering all asymmetric distortions. Departing from proposal-level fusion, we propose a single-shot model that adaptively fuses features, driven by measurement entropy. We validate the proposed method, trained on clean data, on our extensive validation dataset. Code and data are available here https:// github.com/princeton-computational-imaging/ SeeingThroughFog.},
	language = {en},
	urldate = {2022-10-27},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Bijelic, Mario and Gruber, Tobias and Mannan, Fahim and Kraus, Florian and Ritter, Werner and Dietmayer, Klaus and Heide, Felix},
	month = jun,
	year = {2020},
	keywords = {Untagged},
	pages = {11679--11689},
}

@article{blumRandomSmoothingMight2020,
	title = {Random {Smoothing} {Might} be {Unable} to {Certify} \${\textbackslash}ell\_{\textbackslash}infty\$ {Robustness} for {High}-{Dimensional} {Images}},
	url = {http://arxiv.org/abs/2002.03517},
	abstract = {We show a hardness result for random smoothing to achieve certiﬁed adversarial robustness against attacks in the p ball of radius when p {\textgreater} 2. Although random smoothing has been well understood for the 2 case using the Gaussian distribution, much remains unknown concerning the existence of a noise distribution that works for the case of p {\textgreater} 2. This has been posed as an open problem by Cohen et al. (2019) and includes many signiﬁcant paradigms such as the ∞ threat model. In this work, we show that any noise distribution D over Rd that provides p robustness for all base classiﬁers with p {\textgreater} 2 must satisfy E ηi2 = Ω(d1−2/p 2(1 − δ)/δ2) for 99\% of the features (pixels) of vector η ∼ D, where is the robust radius and δ is the score gap between the highest-scored class and the runner-up. Therefore, for high-dimensional images with pixel values bounded in [0, 255], the required noise will eventually dominate the useful information in the images, leading to trivial smoothed classiﬁers.},
	language = {en},
	urldate = {2022-02-22},
	journal = {arXiv:2002.03517 [cs, stat]},
	author = {Blum, Avrim and Dick, Travis and Manoj, Naren and Zhang, Hongyang},
	month = mar,
	year = {2020},
	note = {arXiv: 2002.03517},
	keywords = {Untagged},
}

@article{blumRandomSmoothingMight2020a,
	title = {Random {Smoothing} {Might} be {Unable} to {Certify} {L}∞ {Robustness} for {High}-{Dimensional} {Images}.},
	volume = {21},
	journal = {J. Mach. Learn. Res.},
	author = {Blum, Avrim and Dick, Travis and Manoj, Naren and Zhang, Hongyang},
	year = {2020},
	keywords = {Untagged},
	pages = {211--1},
}

@article{bouchabouFullyConvolutionalNetwork2020,
	title = {Fully {Convolutional} {Network} {Bootstrapped} by {Word} {Encoding} and {Embedding} for {Activity} {Recognition} in {Smart} {Homes}},
	url = {http://arxiv.org/abs/2012.02300},
	abstract = {Activity recognition in smart homes is essential when we wish to propose automatic services for the inhabitants. However, it is a challenging problem in terms of environments’ variability, sensory-motor systems, user habits, but also sparsity of signals and redundancy of models . Therefore, end-to-end systems fail at automatically extracting key features, and need to access context and domain knowledge. We propose to tackle feature extraction for activity recognition in smart homes by merging methods of Natural Language Processing (NLP) and Time Series Classiﬁcation (TSC) domains.},
	language = {en},
	urldate = {2021-04-09},
	journal = {arXiv:2012.02300 [cs, eess]},
	author = {Bouchabou, Damien and Nguyen, Sao and Lohr, Christophe and Leduc, Benoit and Kanellos, Ioannis},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.02300},
	keywords = {Untagged},
}

@inproceedings{bourtoule_machine_2020,
	title = {Machine {Unlearning}},
	url = {http://arxiv.org/abs/1912.03817},
	abstract = {Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult. We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning. Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63x, and 2.45x for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36x in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.},
	urldate = {2022-06-06},
	booktitle = {{IEEE} {S}\&{P} 2021},
	publisher = {arXiv},
	author = {Bourtoule, Lucas and Chandrasekaran, Varun and Choquette-Choo, Christopher A. and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas},
	month = dec,
	year = {2020},
	note = {arXiv:1912.03817 [cs]
type: article},
	keywords = {Untagged},
}

@inproceedings{brixSuccessfullyApplyingStabilized2020,
	title = {Successfully {Applying} the {Stabilized} {Lottery} {Ticket} {Hypothesis} to the {Transformer} {Architecture}},
	url = {http://arxiv.org/abs/2005.03454},
	abstract = {Sparse models require less memory for storage and enable a faster inference by reducing the necessary number of FLOPs. This is relevant both for time-critical and on-device computations using neural networks. The stabilized lottery ticket hypothesis states that networks can be pruned after none or few training iterations, using a mask computed based on the unpruned converged model. On the transformer architecture and the WMT 2014 English→German and English→French tasks, we show that stabilized lottery ticket pruning performs similar to magnitude pruning for sparsity levels of up to 85\%, and propose a new combination of pruning techniques that outperforms all other techniques for even higher levels of sparsity. Furthermore, we conﬁrm that the parameter’s initial sign and not its speciﬁc value is the primary factor for successful training, and show that magnitude pruning could be used to ﬁnd winning lottery tickets.},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {{ACL}},
	author = {Brix, Christopher and Bahar, Parnia and Ney, Hermann},
	year = {2020},
	note = {arXiv: 2005.03454},
	keywords = {Untagged},
}

@inproceedings{brownSmoothAPSmoothingPath2020,
	address = {Glasgow, Uk},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Smooth-{AP}: {Smoothing} the {Path} {Towards} {Large}-{Scale} {Image} {Retrieval}},
	volume = {12354},
	url = {https://doi.org/10.1007/978-3-030-58545-7_39},
	doi = {10/gkzhft},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Brown, Andrew and Xie, Weidi and Kalogeiton, Vicky and Zisserman, Andrew},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Untagged},
	pages = {677--694},
}

@article{buDeepLearningGaussian2020,
	title = {Deep {Learning} with {Gaussian} {Differential} {Privacy}},
	url = {http://arxiv.org/abs/1911.11607},
	abstract = {Deep learning models are often trained on datasets that contain sensitive information such as individuals’ shopping transactions, personal contacts, and medical records. An increasingly important line of work therefore has sought to train neural networks subject to privacy constraints that are speciﬁed by diﬀerential privacy or its divergence-based relaxations. These privacy deﬁnitions, however, have weaknesses in handling certain important primitives (composition and subsampling), thereby giving loose or complicated privacy analyses of training neural networks. In this paper, we consider a recently proposed privacy deﬁnition termed f -diﬀerential privacy [17] for a reﬁned privacy analysis of training neural networks. Leveraging the appealing properties of f -diﬀerential privacy in handling composition and subsampling, this paper derives analytically tractable expressions for the privacy guarantees of both stochastic gradient descent and Adam used in training deep neural networks, without the need of developing sophisticated techniques as [3] did. Our results demonstrate that the f -diﬀerential privacy framework allows for a new privacy analysis that improves on the prior analysis [3], which in turn suggests tuning certain parameters of neural networks for a better prediction accuracy without violating the privacy budget. These theoretically derived improvements are conﬁrmed by our experiments in a range of tasks in image classiﬁcation, text classiﬁcation, and recommender systems.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1911.11607 [cs, stat]},
	author = {Bu, Zhiqi and Dong, Jinshuo and Long, Qi and Su, Weijie J.},
	month = jul,
	year = {2020},
	note = {arXiv: 1911.11607},
	keywords = {Untagged},
}

@inproceedings{NEURIPS2020_34609bdc,
	title = {Network size and size of the weights in memorization with two-layers neural networks},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/34609bdc08a07ace4e1526bbb1777673-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Bubeck, Sebastien and Eldan, Ronen and Lee, Yin Tat and Mikulincer, Dan},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	keywords = {Untagged},
	pages = {4977--4986},
}

@article{caiUnsupervisedDomainAdaptation2020,
	title = {Unsupervised {Domain} {Adaptation} {With} {Adversarial} {Residual} {Transform} {Networks}},
	volume = {31},
	url = {https://doi.org/10.1109/TNNLS.2019.2935384},
	doi = {10/ghv244},
	number = {8},
	journal = {IEEE Trans. Neural Networks Learn. Syst.},
	author = {Cai, Guanyu and Wang, Yuqin and He, Lianghua and Zhou, MengChu},
	year = {2020},
	keywords = {Untagged},
	pages = {3073--3086},
}

@article{castroCausalityMattersMedical2020,
	title = {Causality matters in medical imaging},
	volume = {11},
	issn = {2041-1723},
	url = {https://doi.org/10.1038/s41467-020-17478-w},
	doi = {10.1038/s41467-020-17478-w},
	abstract = {Causal reasoning can shed new light on the major challenges in machine learning for medical imaging: scarcity of high-quality annotated data and mismatch between the development dataset and the target environment. A causal perspective on these issues allows decisions about data collection, annotation, preprocessing, and learning strategies to be made and scrutinized more transparently, while providing a detailed categorisation of potential biases and mitigation techniques. Along with worked clinical examples, we highlight the importance of establishing the causal relationship between images and their annotations, and offer step-by-step recommendations for future studies.},
	number = {1},
	journal = {Nature Communications},
	author = {Castro, Daniel C. and Walker, Ian and Glocker, Ben},
	month = jul,
	year = {2020},
	keywords = {Untagged},
	pages = {3673},
}

@inproceedings{pmlr-v119-chen20s,
	series = {Proceedings of machine learning research},
	title = {Generative pretraining from pixels},
	volume = {119},
	url = {https://proceedings.mlr.press/v119/chen20s.html},
	abstract = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3\% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0\% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0\% top-1 accuracy on a linear probe of our features.},
	booktitle = {Proceedings of the 37th international conference on machine learning},
	publisher = {PMLR},
	author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
	editor = {III, Hal Daumé and Singh, Aarti},
	month = jul,
	year = {2020},
	note = {tex.pdf: http://proceedings.mlr.press/v119/chen20s/chen20s.pdf},
	keywords = {Untagged},
	pages = {1691--1703},
}

@inproceedings{chenLotteryTicketHypothesis2020,
	title = {The lottery ticket hypothesis for pre-trained {BERT} networks},
	url = {https://proceedings.neurips.cc/paper/2020/hash/b6af2c9703f203a2794be03d443af2e3-Abstract.html},
	booktitle = {Advances in neural information processing systems 33: {Annual} conference on neural information processing systems 2020, {NeurIPS} 2020, december 6-12, 2020, virtual},
	author = {Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and Carbin, Michael},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/nips/ChenFC0ZWC20.bib
tex.timestamp: Tue, 19 Jan 2021 15:56:59 +0100},
	keywords = {Untagged},
}

@inproceedings{chenUnderstandingGradientClipping2020,
	address = {virtual},
	title = {Understanding {Gradient} {Clipping} in {Private} {SGD}: {A} {Geometric} {Perspective}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/9ecff5455677b38d19f49ce658ef0608-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Chen, Xiangyi and Wu, Zhiwei Steven and Hong, Mingyi},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
	keywords = {Untagged},
}

@inproceedings{chenLocallyDifferentiallyPrivate2020,
	address = {Virtual Event},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {({Locally}) {Differentially} {Private} {Combinatorial} {Semi}-{Bandits}},
	volume = {119},
	url = {http://proceedings.mlr.press/v119/chen20y.html},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Xiaoyu and Zheng, Kai and Zhou, Zixin and Yang, Yunchang and Chen, Wei and Wang, Liwei},
	year = {2020},
	keywords = {Untagged},
	pages = {1757--1767},
}

@inproceedings{chenUNITERUNiversalImageTExt2020,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{UNITER}: {UNiversal} {Image}-{TExt} {Representation} {Learning}},
	volume = {12375},
	url = {https://doi.org/10.1007/978-3-030-58577-8\_7},
	doi = {10/gm5tjx},
	booktitle = {Computer {Vision} - {ECCV} 2020 - 16th {European} {Conference}, {Glasgow}, {UK}, {August} 23-28, 2020, {Proceedings}, {Part} {XXX}},
	publisher = {Springer},
	author = {Chen, Yen-Chun and Li, Linjie and Yu, Licheng and Kholy, Ahmed El and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Untagged},
	pages = {104--120},
}

@article{chengRobustUnsupervisedCrossmodal2020,
	title = {Robust {Unsupervised} {Cross}-modal {Hashing} for {Multimedia} {Retrieval}},
	volume = {38},
	url = {https://doi.org/10.1145/3389547},
	doi = {10/gg97xm},
	number = {3},
	journal = {ACM Trans. Inf. Syst.},
	author = {Cheng, Miaomiao and Jing, Liping and Ng, Michael K.},
	year = {2020},
	keywords = {Untagged},
	pages = {30:1--30:25},
}

@article{chengExplainingKnowledgeDistillation2020,
	title = {Explaining {Knowledge} {Distillation} by {Quantifying} the {Knowledge}},
	url = {http://arxiv.org/abs/2003.03622},
	abstract = {This paper presents a method to interpret the success of knowledge distillation by quantifying and analyzing taskrelevant and task-irrelevant visual concepts that are encoded in intermediate layers of a deep neural network (DNN). More speciﬁcally, three hypotheses are proposed as follows. 1. Knowledge distillation makes the DNN learn more visual concepts than learning from raw data. 2. Knowledge distillation ensures that the DNN is prone to learning various visual concepts simultaneously. Whereas, in the scenario of learning from raw data, the DNN learns visual concepts sequentially. 3. Knowledge distillation yields more stable optimization directions than learning from raw data. Accordingly, we design three types of mathematical metrics to evaluate feature representations of the DNN. In experiments, we diagnosed various DNNs, and above hypotheses were veriﬁed.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:2003.03622 [cs, stat]},
	author = {Cheng, Xu and Rao, Zhefan and Chen, Yilan and Zhang, Quanshi},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.03622},
	keywords = {Untagged},
}

@inproceedings{chrysakisOnlineContinualLearning2020,
	series = {Proceedings of machine learning research},
	title = {Online continual learning from imbalanced data},
	volume = {119},
	url = {http://proceedings.mlr.press/v119/chrysakis20a.html},
	booktitle = {Proceedings of the 37th international conference on machine learning, {ICML} 2020, 13-18 july 2020, virtual event},
	publisher = {PMLR},
	author = {Chrysakis, Aristotelis and Moens, Marie-Francine},
	year = {2020},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/icml/ChrysakisM20.bib
tex.timestamp: Tue, 15 Dec 2020 17:40:18 +0100},
	keywords = {Untagged},
	pages = {1952--1961},
}

@inproceedings{chungExtremelyLowBit2020,
	address = {Online},
	title = {Extremely {Low} {Bit} {Transformer} {Quantization} for {On}-{Device} {Neural} {Machine} {Translation}},
	url = {https://aclanthology.org/2020.findings-emnlp.433},
	doi = {10/gnbmkt},
	abstract = {The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quantization is an effective technique to address such challenges. Our analysis shows that for a given number of quantization bits, each block of Transformer contributes to translation quality and inference computations in different manners. Moreover, even inside an embedding block, each word presents vastly different contributions. Correspondingly, we propose a mixed precision quantization strategy to represent Transformer weights by an extremely low number of bits (e.g., under 3 bits). For example, for each word in an embedding block, we assign different quantization bits based on statistical property. Our quantized Transformer model achieves 11.8{\textbackslash}mbox\${\textbackslash}times\$ smaller model size than the baseline model, with less than -0.5 BLEU. We achieve 8.3{\textbackslash}mbox\${\textbackslash}times\$ reduction in run-time memory footprints and 3.5{\textbackslash}mbox\${\textbackslash}times\$ speed up (Galaxy N10+) such that our proposed compression strategy enables efficient implementation for on-device NMT.},
	urldate = {2021-11-04},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Chung, Insoo and Kim, Byeongwook and Choi, Yoonjung and Kwon, Se Jung and Jeon, Yongkweon and Park, Baeseong and Kim, Sangha and Lee, Dongsoo},
	month = nov,
	year = {2020},
	keywords = {Untagged},
	pages = {4812--4826},
}

@article{cordonnierRelationshipSelfAttentionConvolutional2020,
	title = {On the {Relationship} between {Self}-{Attention} and {Convolutional} {Layers}},
	url = {http://arxiv.org/abs/1911.03584},
	abstract = {Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Speciﬁcally, we prove that a multi-head self-attention layer with sufﬁcient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available1.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1911.03584 [cs, stat]},
	author = {Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
	month = jan,
	year = {2020},
	note = {arXiv: 1911.03584},
	keywords = {Untagged},
}

@article{croce_minimally_2020,
	title = {Minimally distorted {Adversarial} {Examples} with a {Fast} {Adaptive} {Boundary} {Attack}},
	shorttitle = {{FAB}},
	url = {http://arxiv.org/abs/1907.02044},
	doi = {10.48550/arXiv.1907.02044},
	abstract = {The evaluation of robustness against adversarial manipulation of neural networks-based classifiers is mainly tested with empirical attacks as methods for the exact computation, even when available, do not scale to large networks. We propose in this paper a new white-box adversarial attack wrt the \$l\_p\$-norms for \$p {\textbackslash}in {\textbackslash}\{1,2,{\textbackslash}infty{\textbackslash}\}\$ aiming at finding the minimal perturbation necessary to change the class of a given input. It has an intuitive geometric meaning, yields quickly high quality results, minimizes the size of the perturbation (so that it returns the robust accuracy at every threshold with a single run). It performs better or similar to state-of-the-art attacks which are partially specialized to one \$l\_p\$-norm, and is robust to the phenomenon of gradient masking.},
	urldate = {2022-05-27},
	author = {Croce, Francesco and Hein, Matthias},
	month = jul,
	year = {2020},
	note = {arXiv:1907.02044 [cs, stat]
type: article},
	keywords = {Untagged},
}

@inproceedings{croce_provable_2020,
	title = {Provable robustness against all adversarial \$l\_p\$-perturbations for \$p{\textbackslash}geq 1\$},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Croce, Francesco and Hein, Matthias},
	year = {2020},
	keywords = {Untagged},
}

@inproceedings{croceReliableEvaluationAdversarial2020,
	title = {Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
	shorttitle = {{AutoAttack}},
	url = {http://arxiv.org/abs/2003.01690},
	abstract = {The ﬁeld of defense strategies against adversarial attacks has signiﬁcantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufﬁcient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difﬁcult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we ﬁrst propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than 10\%, identifying several broken defenses.},
	language = {en},
	urldate = {2022-04-07},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Croce, Francesco and Hein, Matthias},
	month = aug,
	year = {2020},
	note = {arXiv: 2003.01690},
	keywords = {Untagged},
	pages = {2206--2216},
}

@article{dai_curriculum_2020,
	title = {Curriculum {Model} {Adaptation} with {Synthetic} and {Real} {Data} for {Semantic} {Foggy} {Scene} {Understanding}},
	volume = {128},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-019-01182-4},
	doi = {10.1007/s11263-019-01182-4},
	abstract = {This work addresses the problem of semantic scene understanding under fog. Although marked progress has been made in semantic scene understanding, it is mainly concentrated on clear-weather scenes. Extending semantic segmentation methods to adverse weather conditions such as fog is crucial for outdoor applications. In this paper, we propose a novel method, named Curriculum Model Adaptation (CMAda), which gradually adapts a semantic segmentation model from light synthetic fog to dense real fog in multiple steps, using both labeled synthetic foggy data and unlabeled real foggy data. The method is based on the fact that the results of semantic segmentation in moderately adverse conditions (light fog) can be bootstrapped to solve the same problem in highly adverse conditions (dense fog). CMAda is extensible to other adverse conditions and provides a new paradigm for learning with synthetic data and unlabeled real data. In addition, we present four other main stand-alone contributions: (1) a novel method to add synthetic fog to real, clear-weather scenes using semantic input; (2) a new fog density estimator; (3) a novel fog densification method for real foggy scenes without known depth; and (4) the Foggy Zurich dataset comprising 3808 real foggy images, with pixel-level semantic annotations for 40 images with dense fog. Our experiments show that (1) our fog simulation and fog density estimator outperform their state-of-the-art counterparts with respect to the task of semantic foggy scene understanding (SFSU); (2) CMAda improves the performance of state-of-the-art models for SFSU significantly, benefiting both from our synthetic and real foggy data. The foggy datasets and code are publicly available.},
	language = {en},
	number = {5},
	urldate = {2022-12-06},
	journal = {International Journal of Computer Vision},
	author = {Dai, Dengxin and Sakaridis, Christos and Hecker, Simon and Van Gool, Luc},
	month = may,
	year = {2020},
	keywords = {Untagged},
	pages = {1182--1204},
}

@article{daiAttentionalFeatureFusion2020,
	title = {Attentional {Feature} {Fusion}},
	url = {http://arxiv.org/abs/2009.14082},
	abstract = {Feature fusion, the combination of features from different layers or branches, is an omnipresent part of modern network architectures. It is often implemented via simple operations, such as summation or concatenation, but this might not be the best choice. In this work, we propose a uniform and general scheme, namely attentional feature fusion, which is applicable for most common scenarios, including feature fusion induced by short and long skip connections as well as within Inception layers. To better fuse features of inconsistent semantics and scales, we propose a multiscale channel attention module, which addresses issues that arise when fusing features given at different scales. We also demonstrate that the initial integration of feature maps can become a bottleneck and that this issue can be alleviated by adding another level of attention, which we refer to as iterative attentional feature fusion. With fewer layers or parameters, our models outperform state-of-the-art networks on both CIFAR-100 and ImageNet datasets, which suggests that more sophisticated attention mechanisms for feature fusion hold great potential to consistently yield better results compared to their direct counterparts. Our codes and trained models are available online1.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2009.14082 [cs]},
	author = {Dai, Yimian and Gieseke, Fabian and Oehmcke, Stefan and Wu, Yiquan and Barnard, Kobus},
	month = nov,
	year = {2020},
	note = {arXiv: 2009.14082},
	keywords = {Untagged},
}

@inproceedings{deanSeeHearExplore2020,
	title = {See, {Hear}, {Explore}: {Curiosity} via {Audio}-{Visual} {Association}},
	shorttitle = {See, {Hear}, {Explore}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/ab6b331e94c28169d15cca0cb3bbc73e-Abstract.html},
	urldate = {2022-02-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 33: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2020, {NeurIPS} 2020, {December} 6-12, 2020, virtual},
	author = {Dean, Victoria and Tulsiani, Shubham and Gupta, Abhinav},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
	keywords = {Untagged},
}

@article{doCompactHashCode2020,
	title = {Compact {Hash} {Code} {Learning} {With} {Binary} {Deep} {Neural} {Network}},
	volume = {22},
	doi = {10/gkzww8},
	number = {4},
	journal = {IEEE Transactions on Multimedia},
	author = {Do, Thanh-Toan and Hoang, Tuan and Le Tan, Dang-Khoa and Doan, Anh-Dzung and Cheung, Ngai-Man},
	year = {2020},
	keywords = {Untagged},
	pages = {992--1004},
}

@article{dongCentripetalNetPursuingHighquality2020,
	title = {{CentripetalNet}: {Pursuing} {High}-quality {Keypoint} {Pairs} for {Object} {Detection}},
	shorttitle = {{CentripetalNet}},
	url = {http://arxiv.org/abs/2003.09119},
	abstract = {Keypoint-based detectors have achieved pretty-well performance. However, incorrect keypoint matching is still widespread and greatly affects the performance of the detector. In this paper, we propose CentripetalNet which uses centripetal shift to pair corner keypoints from the same instance. CentripetalNet predicts the position and the centripetal shift of the corner points and matches corners whose shifted results are aligned. Combining position information, our approach matches corner points more accurately than the conventional embedding approaches do. Corner pooling extracts information inside the bounding boxes onto the border. To make this information more aware at the corners, we design a cross-star deformable convolution network to conduct feature adaption. Furthermore, we explore instance segmentation on anchor-free detectors by equipping our CentripetalNet with a mask prediction module. On MS-COCO test-dev, our CentripetalNet not only outperforms all existing anchor-free detectors with an AP of 48.0\% but also achieves comparable performance to the state-of-the-art instance segmentation approaches with a 40.2\% M askAP . Code will be available at https: //github.com/KiveeDong/CentripetalNet.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:2003.09119 [cs]},
	author = {Dong, Zhiwei and Li, Guoxuan and Liao, Yue and Wang, Fei and Ren, Pengju and Qian, Chen},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.09119},
	keywords = {Untagged},
}

@inproceedings{drossos_clotho_2020,
	title = {Clotho: an {Audio} {Captioning} {Dataset}},
	shorttitle = {Clotho},
	doi = {10.1109/ICASSP40776.2020.9052990},
	abstract = {Audio captioning is the novel task of general audio content description using free text. It is an intermodal translation task (not speech-to-text), where a system accepts as an input an audio signal and outputs the textual description (i.e. the caption) of that signal. In this paper we present Clotho, a dataset for audio captioning consisting of 4981 audio samples of 15 to 30 seconds duration and 24 905 captions of eight to 20 words length, and a baseline method to provide initial results. Clotho is built with focus on audio content and caption diversity, and the splits of the data are not hampering the training or evaluation of methods. All sounds are from the Freesound platform, and captions are crowdsourced using Amazon Mechanical Turk and annotators from English speaking countries. Unique words, named entities, and speech transcription are removed with post-processing. Clotho is freely available online 1.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Drossos, Konstantinos and Lipping, Samuel and Virtanen, Tuomas},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {Untagged},
	pages = {736--740},
}

@article{duanNGBoostNaturalGradient2020,
	title = {{NGBoost}: {Natural} {Gradient} {Boosting} for {Probabilistic} {Prediction}},
	shorttitle = {{NGBoost}},
	url = {http://arxiv.org/abs/1910.03225},
	abstract = {We present Natural Gradient Boosting (NGBoost), an algorithm which brings probabilistic prediction capability to gradient boosting in a generic way. Predictive uncertainty estimation is crucial in many applications such as healthcare and weather forecasting. Probabilistic prediction, which is the approach where the model outputs a full probability distribution over the entire outcome space, is a natural way to quantify those uncertainties. Gradient Boosting Machines have been widely successful in prediction tasks on structured input data, but a simple boosting solution for probabilistic prediction of real valued outputs is yet to be made. NGBoost is a gradient boosting approach which uses the Natural Gradient to address technical challenges that makes generic probabilistic prediction hard with existing gradient boosting methods. Our approach is modular with respect to the choice of base learner, probability distribution, and scoring rule. We show empirically on several regression datasets that NGBoost provides competitive predictive performance of both uncertainty estimates and traditional metrics.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1910.03225 [cs, stat]},
	author = {Duan, Tony and Avati, Anand and Ding, Daisy Yi and Thai, Khanh K. and Basu, Sanjay and Ng, Andrew Y. and Schuler, Alejandro},
	month = jun,
	year = {2020},
	note = {arXiv: 1910.03225},
	keywords = {Untagged},
}

@article{dudite;kOracleefficientOnlineLearning2020,
	title = {Oracle-efficient {Online} {Learning} and {Auction} {Design}},
	volume = {67},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/3402203},
	doi = {10/ghv3cs},
	language = {en},
	number = {5},
	urldate = {2021-01-27},
	journal = {Journal of the ACM},
	author = {Dudíte;k, Miroslav and Haghtalab, Nika and Luo, Haipeng and Schapire, Robert E. and Syrgkanis, Vasilis and Vaughan, Jennifer Wortman},
	month = oct,
	year = {2020},
	keywords = {Untagged},
	pages = {1--57},
}

@article{dwivediBenchmarkingGraphNeural2020,
	title = {Benchmarking {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2003.00982},
	abstract = {Graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. They have been successfully applied to a myriad of domains including chemistry, physics, social sciences, knowledge graphs, recommendation, and neuroscience. As the ﬁeld grows, it becomes critical to identify the architectures and key mechanisms which generalize across graphs sizes, enabling us to tackle larger, more complex datasets and domains. Unfortunately, it has been increasingly difﬁcult to gauge the effectiveness of new GNNs and compare models in the absence of a standardized benchmark with consistent experimental settings and large datasets. In this paper, we propose a reproducible GNN benchmarking framework6, with the facility for researchers to add new datasets and models conveniently. We apply this benchmarking framework to novel medium-scale graph datasets from mathematical modeling, computer vision, chemistry and combinatorial problems to establish key operations when designing effective GNNs. Precisely, graph convolutions, anisotropic diffusion, residual connections and normalization layers are universal building blocks for developing robust and scalable GNNs.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:2003.00982 [cs, stat]},
	author = {Dwivedi, Vijay Prakash and Joshi, Chaitanya K. and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
	month = jul,
	year = {2020},
	note = {arXiv: 2003.00982},
	keywords = {Untagged},
}

@article{falconFrameworkContrastiveSelfSupervised2020,
	title = {A {Framework} {For} {Contrastive} {Self}-{Supervised} {Learning} {And} {Designing} {A} {New} {Approach}},
	url = {http://arxiv.org/abs/2009.00104},
	abstract = {Contrastive self-supervised learning (CSL) is an approach to learn useful representations by solving a pretext task which selects and compares anchor, negative and positive (APN) features from an unlabeled dataset. We present a conceptual framework which characterizes CSL approaches in ﬁve aspects (1) data augmentation pipeline, (2) encoder selection, (3) representation extraction, (4) similarity measure, and (5) loss function. We analyze three leading CSL approaches–AMDIM, CPC and SimCLR–, and show that despite different motivations, they are special cases under this framework. We show the utility of our framework by designing Yet Another DIM (YADIM) which achieves competitive results on CIFAR-10, STL-10 and ImageNet, and is more robust to the choice of encoder and the representation extraction strategy. To support ongoing CSL research, we release the PyTorch implementation of this conceptual framework along with standardized implementations of AMDIM, CPC (V2), SimCLR, BYOL, Moco (V2) and YADIM.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2009.00104 [cs]},
	author = {Falcon, William and Cho, Kyunghyun},
	month = aug,
	year = {2020},
	note = {arXiv: 2009.00104},
	keywords = {Untagged},
}

@article{fengExploringMultiScaleFeature2020,
	title = {Exploring {Multi}-{Scale} {Feature} {Propagation} and {Communication} for {Image} {Super} {Resolution}},
	url = {http://arxiv.org/abs/2008.00239},
	abstract = {Multi-scale techniques have achieved great success in a wide range of computer vision tasks. However, while this technique is incorporated in existing works, there still lacks a comprehensive investigation on variants of multi-scale convolution in image super resolution. In this work, we present a uniﬁed formulation over widely-used multiscale structures. With this framework, we systematically explore the two factors of multi-scale convolution – feature propagation and cross-scale communication. Based on the investigation, we propose a generic and eﬃcient multi-scale convolution unit – Multi-Scale cross-Scale Share-weights convolution (MS3-Conv). Extensive experiments demonstrate that the proposed MS3-Conv can achieve better SR performance than the standard convolution with less parameters and computational cost. Beyond quantitative analysis, we comprehensively study the visual quality, which show that MS3-Conv behave better to recover high-frequency details.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2008.00239 [cs, eess]},
	author = {Feng, Ruicheng and Guan, Weipeng and Qiao, Yu and Dong, Chao},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.00239},
	keywords = {Untagged},
}

@article{fernandesLocalitySensitiveHashing2020,
	title = {Locality {Sensitive} {Hashing} with {Extended} {Differential} {Privacy}},
	url = {http://arxiv.org/abs/2010.09393},
	abstract = {Extended differential privacy, a generalization of standard differential privacy (DP) using a general metric rather than the Hamming metric, has been widely studied to provide rigorous privacy guarantees while keeping high utility. However, existing works on extended DP focus on a specific metric such as the Euclidean metric, the ������1 metric, and the Earth Mover’s metric, and cannot be applied to other metrics. Consequently, existing extended DP mechanisms are limited to a small number of applications such as location-based services and document processing.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2010.09393 [cs, math]},
	author = {Fernandes, Natasha and Kawamoto, Yusuke and Murakami, Takao},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.09393},
	keywords = {Untagged},
}

@inproceedings{forbesRepresentingTemporalDependencies2020,
	title = {Representing {Temporal} {Dependencies} in {Smart} {Home} {Activity} {Recognition} for {Health} {Monitoring}},
	doi = {10.1109/IJCNN48605.2020.9207480},
	abstract = {Long term health conditions, such as fall risk, are traditionally diagnosed through testing performed in hospital environments. Smart Homes offer the opportunity to perform continuous, long-term behavioural and vitals monitoring of residents, which may be employed to aid diagnosis and management of chronic conditions without placing additional strain on health services. A profile of the resident's behaviour can be produced from sensor data, and then compared over time. Activity Recognition is a primary challenge for profile generation, however many of the approaches adopted fail to take full advantage of the inherent temporal dependencies that exist in the activities taking place. Long Short Term Memory (LSTM) is a form of recurrent neural network that uses previously learned examples to inform classification decisions. In this paper we present a variety of approaches to human activity recognition using LSTMs which consider the temporal dependencies present in the sensor data in order to produce richer representations and improved classification accuracy. The LSTM approaches are compared to the performance of a selection of baseline classification algorithms on several real world datasets. In general, it was found that accuracy in LSTMs improved as additional temporal information was presented to the classifier.},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	author = {Forbes, Glenn and Massie, Stewart and Craw, Susan and Fraser, Lucy and Hamilton, Graeme},
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {Untagged},
	pages = {1--8},
}

@inproceedings{fuCounterfactualVisionandLanguageNavigation2020,
	address = {Cham},
	title = {Counterfactual {Vision}-and-{Language} {Navigation} via {Adversarial} {Path} {Sampler}},
	volume = {12351},
	isbn = {978-3-030-58538-9 978-3-030-58539-6},
	url = {https://link.springer.com/10.1007/978-3-030-58539-6_5},
	doi = {10.1007/978-3-030-58539-6_5},
	abstract = {Vision-and-Language Navigation (VLN) is a task where agents must decide how to move through a 3D environment to reach a goal by grounding natural language instructions to the visual surroundings. One of the problems of the VLN task is data scarcity since it is diﬃcult to collect enough navigation paths with human-annotated instructions for interactive environments. In this paper, we explore the use of counterfactual thinking as a human-inspired data augmentation method that results in robust models. Counterfactual thinking is a concept that describes the human propensity to create possible alternatives to life events that have already occurred. We propose an adversarial-driven counterfactual reasoning model that can consider eﬀective conditions instead of low-quality augmented data. In particular, we present a model-agnostic adversarial path sampler (APS) that learns to sample challenging paths that force the navigator to improve based on the navigation performance. APS also serves to do pre-exploration of unseen environments to strengthen the model’s ability to generalize. We evaluate the inﬂuence of APS on the performance of diﬀerent VLN baseline models using the room-to-room dataset (R2R). The results show that the adversarial training process with our proposed APS beneﬁts VLN models under both seen and unseen environments. And the pre-exploration process can further gain additional improvements under unseen environments.},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Fu, Tsu-Jui and Wang, Xin Eric and Peterson, Matthew F. and Grafton, Scott T. and Eckstein, Miguel P. and Wang, William Yang},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {71--86},
}

@inproceedings{gabeur_multi-modal_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multi-modal {Transformer} for {Video} {Retrieval}},
	isbn = {978-3-030-58548-8},
	shorttitle = {{MMT}},
	doi = {10.1007/978-3-030-58548-8_13},
	abstract = {The task of retrieving video content relevant to natural language queries plays a critical role in effectively handling internet-scale datasets. Most of the existing methods for this caption-to-video retrieval problem do not fully exploit cross-modal cues present in video. Furthermore, they aggregate per-frame visual features with limited or no temporal information. In this paper, we present a multi-modal transformer to jointly encode the different modalities in video, which allows each of them to attend to the others. The transformer architecture is also leveraged to encode and model the temporal information. On the natural language side, we investigate the best practices to jointly optimize the language embedding together with the multi-modal transformer. This novel framework allows us to establish state-of-the-art results for video retrieval on three datasets. More details are available at http://thoth.inrialpes.fr/research/MMT.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Gabeur, Valentin and Sun, Chen and Alahari, Karteek and Schmid, Cordelia},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Untagged},
	pages = {214--229},
}

@inproceedings{gaoRevisitingBilinearPooling2020,
	title = {Revisiting {Bilinear} {Pooling}: {A} {Coding} {Perspective}},
	shorttitle = {Revisiting {Bilinear} {Pooling}},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/5811},
	urldate = {2022-02-25},
	booktitle = {The {Thirty}-{Fourth} {AAAI} {Conference} on {Artificial} {Intelligence}, {AAAI} 2020, {The} {Thirty}-{Second} {Innovative} {Applications} of {Artificial} {Intelligence} {Conference}, {IAAI} 2020, {The} {Tenth} {AAAI} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}, {EAAI} 2020, {New} {York}, {NY}, {USA}, {February} 7-12, 2020},
	publisher = {AAAI Press},
	author = {Gao, Zhi and Wu, Yuwei and Zhang, Xiaoxun and Dai, Jindou and Jia, Yunde and Harandi, Mehrtash},
	year = {2020},
	keywords = {Untagged},
	pages = {3954--3961},
}

@inproceedings{pmlr-v108-garber20a,
	series = {Proceedings of machine learning research},
	title = {Improved regret bounds for projection-free bandit convex optimization},
	volume = {108},
	url = {https://proceedings.mlr.press/v108/garber20a.html},
	abstract = {We revisit the challenge of designing online algorithms for the bandit convex optimization problem (BCO) which are also scalable to high dimensional problems. Hence, we consider algorithms that are \textit{projection-free}, i.e., based on the conditional gradient method whose only access to the feasible decision set, is through a linear optimization oracle (as opposed to other methods which require potentially much more computationally-expensive subprocedures, such as computing Euclidean projections). We present the first such algorithm that attains O(T$^{\textrm{3/4}}$) expected regret using only O(T) overall calls to the linear optimization oracle, in expectation, where T in the number of prediction rounds. This improves over the O(T$^{\textrm{4/5}}$) expected regret bound recently obtained by Karbasi19, and actually matches the current best regret bound for projection-free online learning in the \textit{full information} setting.},
	booktitle = {Proceedings of the twenty third international conference on artificial intelligence and statistics},
	publisher = {PMLR},
	author = {Garber, Dan and Kretzu, Ben},
	editor = {Chiappa, Silvia and Calandra, Roberto},
	month = aug,
	year = {2020},
	note = {tex.pdf: http://proceedings.mlr.press/v108/garber20a/garber20a.pdf},
	keywords = {Untagged},
	pages = {2196--2206},
}

@article{garcia-gonzalezPublicDomainDataset2020,
	title = {A {Public} {Domain} {Dataset} for {Real}-{Life} {Human} {Activity} {Recognition} {Using} {Smartphone} {Sensors}},
	volume = {20},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/20/8/2200},
	doi = {10/ggtdjq},
	abstract = {In recent years, human activity recognition has become a hot topic inside the scientific community. The reason to be under the spotlight is its direct application in multiple domains, like healthcare or fitness. Additionally, the current worldwide use of smartphones makes it particularly easy to get this kind of data from people in a non-intrusive and cheaper way, without the need for other wearables. In this paper, we introduce our orientation-independent, placement-independent and subject-independent human activity recognition dataset. The information in this dataset is the measurements from the accelerometer, gyroscope, magnetometer, and GPS of the smartphone. Additionally, each measure is associated with one of the four possible registered activities: inactive, active, walking and driving. This work also proposes asupport vector machine (SVM) model to perform some preliminary experiments on the dataset. Considering that this dataset was taken from smartphones in their actual use, unlike other datasets, the development of a good model on such data is an open problem and a challenge for researchers. By doing so, we would be able to close the gap between the model and a real-life application.},
	language = {en},
	number = {8},
	urldate = {2021-09-27},
	journal = {Sensors},
	author = {Garcia-Gonzalez, Daniel and Rivero, Daniel and Fernandez-Blanco, Enrique and Luaces, Miguel R.},
	month = jan,
	year = {2020},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Untagged},
	pages = {2200},
}

@inproceedings{gardner_evaluating_2020,
	address = {Online},
	title = {Evaluating {Models}' {Local} {Decision} {Boundaries} via {Contrast} {Sets}},
	url = {https://aclanthology.org/2020.findings-emnlp.117},
	doi = {10.18653/v1/2020.findings-emnlp.117},
	abstract = {Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture the abilities a dataset is intended to test. We propose a more rigorous annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, and IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets—up to 25\% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.},
	urldate = {2022-11-08},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Gardner, Matt and Artzi, Yoav and Basmov, Victoria and Berant, Jonathan and Bogin, Ben and Chen, Sihao and Dasigi, Pradeep and Dua, Dheeru and Elazar, Yanai and Gottumukkala, Ananth and Gupta, Nitish and Hajishirzi, Hannaneh and Ilharco, Gabriel and Khashabi, Daniel and Lin, Kevin and Liu, Jiangming and Liu, Nelson F. and Mulcaire, Phoebe and Ning, Qiang and Singh, Sameer and Smith, Noah A. and Subramanian, Sanjay and Tsarfaty, Reut and Wallace, Eric and Zhang, Ally and Zhou, Ben},
	month = nov,
	year = {2020},
	keywords = {Untagged},
	pages = {1307--1323},
}

@inproceedings{garg_bae_2020,
	address = {Online},
	title = {{BAE}: {BERT}-based {Adversarial} {Examples} for {Text} {Classification}},
	shorttitle = {{BAE}},
	url = {https://aclanthology.org/2020.emnlp-main.498},
	doi = {10.18653/v1/2020.emnlp-main.498},
	abstract = {Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.},
	urldate = {2023-02-26},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Garg, Siddhant and Ramakrishnan, Goutham},
	month = nov,
	year = {2020},
	keywords = {Untagged},
	pages = {6174--6181},
}

@article{geirhosShortcutLearningDeep2020,
	title = {Shortcut {Learning} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2004.07780},
	abstract = {Deep learning has triggered the current rise of artiﬁcial intelligence and is the workhorse of today’s machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distil how many of deep learning’s problem can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artiﬁcial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2004.07780 [cs, q-bio]},
	author = {Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
	month = may,
	year = {2020},
	note = {arXiv: 2004.07780},
	keywords = {Untagged},
}

@article{gholamiUnsupervisedMultiTargetDomain2020,
	title = {Unsupervised {Multi}-{Target} {Domain} {Adaptation}: {An} {Information} {Theoretic} {Approach}},
	volume = {29},
	issn = {1057-7149, 1941-0042},
	shorttitle = {Unsupervised {Multi}-{Target} {Domain} {Adaptation}},
	url = {https://ieeexplore.ieee.org/document/8970464/},
	doi = {10/ghv3nw},
	abstract = {Unsupervised domain adaptation (uDA) models focus on pairwise adaptation settings where there is a single, labeled, source and a single target domain. However, in many real-world settings one seeks to adapt to multiple, but somewhat similar, target domains. Applying pairwise adaptation approaches to this setting may be suboptimal, as they fail to leverage shared information among multiple domains. In this work, we propose an information theoretic approach for domain adaptation in the novel context of multiple target domains with unlabeled instances and one source domain with labeled instances. Our model aims to ﬁnd a shared latent space common to all domains, while simultaneously accounting for the remaining private, domain-speciﬁc factors. Disentanglement of shared and private information is accomplished using a uniﬁed information-theoretic approach, which also serves to establish a stronger link between the latent representations and the observed data. The resulting model, accompanied by an efﬁcient optimization algorithm, allows simultaneous adaptation from a single source to multiple target domains. We test our approach on three challenging publicly-available datasets, showing that it outperforms several popular domain adaptation methods.},
	language = {en},
	urldate = {2021-01-27},
	journal = {IEEE Transactions on Image Processing},
	author = {Gholami, Behnam and Sahu, Pritish and Rudovic, Ognjen and Bousmalis, Konstantinos and Pavlovic, Vladimir},
	year = {2020},
	keywords = {Untagged},
	pages = {3993--4002},
}

@article{goodfellowGenerativeAdversarialNetworks2020,
	title = {Generative adversarial networks},
	volume = {63},
	url = {https://doi.org/10.1145/3422622},
	doi = {10/ghsbsw},
	number = {11},
	journal = {Commun. ACM},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron C. and Bengio, Yoshua},
	year = {2020},
	keywords = {Untagged},
	pages = {139--144},
}

@inproceedings{hammoudehLearningPositiveUnlabeled2020,
	title = {Learning from {Positive} and {Unlabeled} {Data} with {Arbitrary} {Positive} {Shift}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/98b297950041a42470269d56260243a1-Abstract.html},
	urldate = {2022-02-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 33: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2020, {NeurIPS} 2020, {December} 6-12, 2020, virtual},
	author = {Hammoudeh, Zayd and Lowd, Daniel},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
	keywords = {Untagged},
}

@article{hanGhostNetMoreFeatures2020,
	title = {{GhostNet}: {More} {Features} from {Cheap} {Operations}},
	shorttitle = {{GhostNet}},
	url = {http://arxiv.org/abs/1911.11907},
	abstract = {Deploying convolutional neural networks (CNNs) on embedded devices is difficult due to the limited memory and computation resources. The redundancy in feature maps is an important characteristic of those successful CNNs, but has rarely been investigated in neural architecture design. This paper proposes a novel Ghost module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. Ghost bottlenecks are designed to stack Ghost modules, and then the lightweight GhostNet can be easily established. Experiments conducted on benchmarks demonstrate that the proposed Ghost module is an impressive alternative of convolution layers in baseline models, and our GhostNet can achieve higher recognition performance (e.g. \$75.7{\textbackslash}\%\$ top-1 accuracy) than MobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012 classification dataset. Code is available at https://github.com/huawei-noah/ghostnet},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1911.11907 [cs]},
	author = {Han, Kai and Wang, Yunhe and Tian, Qi and Guo, Jianyuan and Xu, Chunjing and Xu, Chang},
	month = mar,
	year = {2020},
	note = {arXiv: 1911.11907},
	keywords = {Untagged},
}

@article{hanLearningtoRankBERTTFRanking2020,
	title = {Learning-to-{Rank} with {BERT} in {TF}-{Ranking}},
	url = {http://arxiv.org/abs/2004.08476},
	abstract = {This paper describes a machine learning algorithm for document (re)ranking, in which queries and documents are firstly encoded using BERT [1], and on top of that a learning-to-rank (LTR) model constructed with TF-Ranking (TFR) [2] is applied to further optimize the ranking performance. This approach is proved to be effective in a public MS MARCO benchmark [3]. Our first two submissions achieve the best performance for the passage re-ranking task [4], and the second best performance for the passage full-ranking task as of April 10, 2020 [5]. To leverage the lately development of pre-trained language models, we recently integrate RoBERTa [6] and ELECTRA [7]. Our latest submissions improve our previously state-of-the-art re-ranking performance by 4.3\% [8], and achieve the third best performance for the full-ranking task [9] as of June 8, 2020. Both of them demonstrate the effectiveness of combining ranking losses with BERT representations for document ranking.},
	urldate = {2021-08-25},
	journal = {arXiv:2004.08476 [cs]},
	author = {Han, Shuguang and Wang, Xuanhui and Bendersky, Mike and Najork, Marc},
	month = jun,
	year = {2020},
	note = {arXiv: 2004.08476},
	keywords = {Untagged},
}

@inproceedings{hartvigsenRecurrentHaltingChain2020,
	address = {Virtual Event CA USA},
	title = {Recurrent {Halting} {Chain} for {Early} {Multi}-label {Classification}},
	isbn = {978-1-4503-7998-4},
	url = {https://dl.acm.org/doi/10.1145/3394486.3403191},
	doi = {10/gkx7rc},
	abstract = {Early multi-label classification of time series, the assignment of a label set to a time series before the series is entirely observed, is critical for time-sensitive domains such as healthcare. In such cases, waiting too long to classify can render predictions useless, regardless of their accuracy, while predicting prematurely can result in potentially costly erroneous results. When predicting multiple labels (for example, types of infections), dependencies between labels can be learned and leveraged to improve overall accuracy. Together, reliably predicting the correct label set of a time series while observing as few timesteps as possible is challenging because these goals are contradictory in that fewer timesteps often means worse accuracy. To achieve early yet sufficiently accurate predictions, correlations between labels must be accounted for since direct evidence of some labels may only appear late in the series. We design an effective solution to this open problem, the Recurrent Halting Chain (RHC), that for the first time integrates key innovations in both Early and Multi-label Classification into one multi-objective model. RHC uses a recurrent neural network to jointly model raw time series as well as correlations between labels, resulting in a novel order-free classifier chain that tackles this time-sensitive multi-label learning task. Further, RHC employs a reinforcement learning-based halting network to decide at each timestep which, if any, classes should be predicted, learning to build the label set over time. Using two real-world time-sensitive datasets and popular multi-label metrics, we show that RHC outperforms recent alternatives by predicting more-accurate label sets earlier.},
	language = {en},
	urldate = {2021-06-18},
	booktitle = {{ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Hartvigsen, Thomas and Sen, Cansu and Kong, Xiangnan and Rundensteiner, Elke},
	year = {2020},
	keywords = {Untagged},
	pages = {1382--1392},
}

@inproceedings{hayesExtensionsLimitationsRandomized2020,
	address = {Seattle, WA, USA},
	title = {Extensions and limitations of randomized smoothing for robustness guarantees},
	isbn = {978-1-72819-360-1},
	url = {https://ieeexplore.ieee.org/document/9150620/},
	doi = {10.1109/CVPRW50498.2020.00401},
	abstract = {Randomized smoothing, a method to certify a classiﬁer’s decision on an input is invariant under adversarial noise, offers attractive advantages over other certiﬁcation methods. It operates in a black-box and so certiﬁcation is not constrained by the size of the classiﬁer’s architecture. Here, we extend the work of Li et al. [26], studying how the choice of divergence between smoothing measures affects the ﬁnal robustness guarantee, and how the choice of smoothing measure itself can lead to guarantees in differing threat models. To this end, we develop a method to certify robustness against any `p (p 2 N{\textgreater}0) minimized adversarial perturbation. We then demonstrate a negative result, that randomized smoothing suffers from the curse of dimensionality; as p increases, the effective radius around an input one can certify vanishes.},
	language = {en},
	urldate = {2022-02-22},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Hayes, Jamie},
	month = jun,
	year = {2020},
	keywords = {Untagged},
	pages = {3413--3421},
}

@inproceedings{hazanFasterProjectionfreeOnline2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Faster {Projection}-free {Online} {Learning}},
	volume = {125},
	url = {http://proceedings.mlr.press/v125/hazan20a.html},
	booktitle = {Conference on {Learning} {Theory}, {COLT} 2020, 9-12 {July} 2020, {Virtual} {Event} [{Graz}, {Austria}]},
	publisher = {PMLR},
	author = {Hazan, Elad and Minasyan, Edgar},
	editor = {Abernethy, Jacob D. and Agarwal, Shivani},
	year = {2020},
	keywords = {Untagged},
	pages = {1877--1893},
}

@inproceedings{heGroupKnowledgeTransfer2020,
	title = {Group {Knowledge} {Transfer}: {Federated} {Learning} of {Large} {CNNs} at the {Edge}},
	shorttitle = {{FedGKT}},
	url = {http://arxiv.org/abs/2007.14513},
	abstract = {Scaling up the convolutional neural network (CNN) size (e.g., width, depth, etc.) is known to effectively improve model accuracy. However, the large model size impedes training on resource-constrained edge devices. For instance, federated learning (FL) may place undue burden on the compute capability of edge nodes, even though there is a strong practical need for FL due to its privacy and confidentiality properties. To address the resource-constrained reality of edge devices, we reformulate FL as a group knowledge transfer training algorithm, called FedGKT. FedGKT designs a variant of the alternating minimization approach to train small CNNs on edge nodes and periodically transfer their knowledge by knowledge distillation to a large server-side CNN. FedGKT consolidates several advantages into a single framework: reduced demand for edge computation, lower communication bandwidth for large CNNs, and asynchronous training, all while maintaining model accuracy comparable to FedAvg. We train CNNs designed based on ResNet-56 and ResNet-110 using three distinct datasets (CIFAR-10, CIFAR-100, and CINIC-10) and their non-I.I.D. variants. Our results show that FedGKT can obtain comparable or even slightly higher accuracy than FedAvg. More importantly, FedGKT makes edge training affordable. Compared to the edge training using FedAvg, FedGKT demands 9 to 17 times less computational power (FLOPs) on edge devices and requires 54 to 105 times fewer parameters in the edge CNN. Our source code is released at FedML (https://fedml.ai).},
	urldate = {2022-03-15},
	booktitle = {{arXiv}:2007.14513 [cs]},
	author = {He, Chaoyang and Annavaram, Murali and Avestimehr, Salman},
	month = nov,
	year = {2020},
	note = {arXiv: 2007.14513},
	keywords = {Untagged},
}

@article{heRobustnessPrivacyGeneralization2020,
	title = {Robustness, {Privacy}, and {Generalization} of {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2012.13573},
	abstract = {Adversarial training can considerably robustify deep neural networks to resist adversarial attacks. However, some works suggested that adversarial training might comprise the privacy-preserving and generalization abilities. This paper establishes and quantifies the privacy-robustness trade-off and generalization-robustness trade-off in adversarial training from both theoretical and empirical aspects. We first define a notion, \{{\textbackslash}it robustified intensity\} to measure the robustness of an adversarial training algorithm. This measure can be approximate empirically by an asymptotically consistent empirical estimator, \{{\textbackslash}it empirical robustified intensity\}. Based on the robustified intensity, we prove that (1) adversarial training is \$({\textbackslash}varepsilon, {\textbackslash}delta)\$-differentially private, where the magnitude of the differential privacy has a positive correlation with the robustified intensity; and (2) the generalization error of adversarial training can be upper bounded by an \${\textbackslash}mathcal O({\textbackslash}sqrt\{{\textbackslash}log N\}/N)\$ on-average bound and an \${\textbackslash}mathcal O(1/{\textbackslash}sqrt\{N\})\$ high-probability bound, both of which have positive correlations with the robustified intensity. Additionally, our generalization bounds do not explicitly rely on the parameter size which would be prohibitively large in deep learning. Systematic experiments on standard datasets, CIFAR-10 and CIFAR-100, are in full agreement with our theories. The source code package is available at {\textbackslash}url\{https://github.com/fshp971/RPG\}.},
	urldate = {2021-09-27},
	journal = {arXiv:2012.13573 [cs, stat]},
	author = {He, Fengxiang and Fu, Shaopeng and Wang, Bohan and Tao, Dacheng},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.13573},
	keywords = {Untagged},
}

@article{heMomentumContrastUnsupervised2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1911.05722},
	abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning [27] as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-ﬂy that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classiﬁcation. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1911.05722 [cs]},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	month = mar,
	year = {2020},
	note = {arXiv: 1911.05722},
	keywords = {Untagged},
}

@article{heRealFormerTransformerLikes2020,
	title = {{RealFormer}: {Transformer} {Likes} {Residual} {Attention}},
	shorttitle = {{RealFormer}},
	url = {http://arxiv.org/abs/2012.11747},
	abstract = {Transformer is the backbone of modern NLP models. In this paper, we propose RealFormer, a simple Residual Attention Layer Transformer architecture that signiﬁcantly outperforms canonical Transformers on a spectrum of tasks including Masked Language Modeling, GLUE, and SQuAD. Qualitatively, RealFormer is easy to implement and requires minimal hyper-parameter tuning. It also stabilizes training and leads to models with sparser attentions. Code will be open-sourced upon paper acceptance.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2012.11747 [cs]},
	author = {He, Ruining and Ravula, Anirudh and Kanagal, Bhargav and Ainslie, Joshua},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.11747},
	keywords = {Untagged},
}

@article{henaffDataEfficientImageRecognition2020,
	title = {Data-{Efficient} {Image} {Recognition} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1905.09272},
	abstract = {Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers.},
	urldate = {2021-07-29},
	journal = {arXiv:1905.09272 [cs]},
	author = {Hénaff, Olivier J. and Srinivas, Aravind and De Fauw, Jeffrey and Razavi, Ali and Doersch, Carl and Eslami, S. M. Ali and Oord, Aaron van den},
	month = jul,
	year = {2020},
	note = {arXiv: 1905.09272},
	keywords = {Untagged},
}

@article{hongTrainingSpikingNeural2020,
	title = {Training {Spiking} {Neural} {Networks} for {Cognitive} {Tasks}: {A} {Versatile} {Framework} {Compatible} {With} {Various} {Temporal} {Codes}},
	volume = {31},
	doi = {10/ghv3pd},
	number = {4},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Hong, C. and Wei, X. and Wang, J. and Deng, B. and Yu, H. and Che, Y.},
	year = {2020},
	keywords = {Untagged},
	pages = {1285--1296},
}

@inproceedings{hongSubInstructionAwareVisionandLanguage2020,
	address = {Online},
	title = {Sub-{Instruction} {Aware} {Vision}-and-{Language} {Navigation}},
	url = {https://aclanthology.org/2020.emnlp-main.271},
	doi = {10/gjzmx4},
	abstract = {Vision-and-language navigation requires an agent to navigate through a real 3D environment following natural language instructions. Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences. Meanwhile, due to the lack of intermediate supervision, the agent's performance at following each part of the instruction cannot be assessed during navigation. In this work, we focus on the granularity of the visual and language sequences as well as the traceability of agents through the completion of an instruction. We provide agents with fine-grained annotations during training and find that they are able to follow the instruction better and have a higher chance of reaching the target at test time. We enrich the benchmark dataset Room-to-Room (R2R) with sub-instructions and their corresponding paths. To make use of this data, we propose effective sub-instruction attention and shifting modules that select and attend to a single sub-instruction at each time-step. We implement our sub-instruction modules in four state-of-the-art agents, compare with their baseline models, and show that our proposed method improves the performance of all four agents. We release the Fine-Grained R2R dataset (FGR2R) and the code at https://github.com/YicongHong/Fine-Grained-R2R.},
	urldate = {2021-11-03},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Hong, Yicong and Rodriguez, Cristian and Wu, Qi and Gould, Stephen},
	month = nov,
	year = {2020},
	keywords = {Untagged},
	pages = {3360--3376},
}

@inproceedings{hongLanguageVisualEntity2020,
	title = {Language and {Visual} {Entity} {Relationship} {Graph} for {Agent} {Navigation}},
	abstract = {Vision-and-Language Navigation (VLN) requires an agent to navigate in a realworld environment following natural language instructions. From both the textual and visual perspectives, we ﬁnd that the relationships among the scene, its objects, and directional clues are essential for the agent to interpret complex instructions and correctly perceive the environment. To capture and utilize the relationships, we propose a novel Language and Visual Entity Relationship Graph for modelling the inter-modal relationships between text and vision, and the intra-modal relationships among visual entities. We propose a message passing algorithm for propagating information between language elements and visual entities in the graph, which we then combine to determine the next action to take. Experiments show that by taking advantage of the relationships we are able to improve over state-of-the-art. On the Room-to-Room (R2R) benchmark, our method achieves the new best performance on the test unseen split with success rate weighted by path length (SPL) of 52\%. On the Room-for-Room (R4R) dataset, our method signiﬁcantly improves the previous best from 13\% to 34\% on the success weighted by normalized dynamic time warping (SDTW).},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hong, Yicong and Rodriguez-Opazo, Cristian and Qi, Yuankai and Wu, Qi and Gould, Stephen},
	year = {2020},
	keywords = {Untagged},
	pages = {12},
}

@article{houStripPoolingRethinking2020,
	title = {Strip {Pooling}: {Rethinking} {Spatial} {Pooling} for {Scene} {Parsing}},
	shorttitle = {Strip {Pooling}},
	url = {http://arxiv.org/abs/2003.13328},
	abstract = {Spatial pooling has been proven highly effective in capturing long-range contextual information for pixel-wise prediction tasks, such as scene parsing. In this paper, beyond conventional spatial pooling that usually has a regular shape of N × N , we rethink the formulation of spatial pooling by introducing a new pooling strategy, called strip pooling, which considers a long but narrow kernel, i.e., 1 × N or N × 1. Based on strip pooling, we further investigate spatial pooling architecture design by 1) introducing a new strip pooling module that enables backbone networks to efﬁciently model long-range dependencies, 2) presenting a novel building block with diverse spatial pooling as a core, and 3) systematically comparing the performance of the proposed strip pooling and conventional spatial pooling techniques. Both novel pooling-based designs are lightweight and can serve as an efﬁcient plugand-play module in existing scene parsing networks. Extensive experiments on popular benchmarks (e.g., ADE20K and Cityscapes) demonstrate that our simple approach establishes new state-of-the-art results. Code is available at https://github.com/Andrew-Qibin/SPNet.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:2003.13328 [cs]},
	author = {Hou, Qibin and Zhang, Li and Cheng, Ming-Ming and Feng, Jiashi},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.13328},
	keywords = {Untagged},
}

@article{huDSNASDirectNeural2020,
	title = {{DSNAS}: {Direct} {Neural} {Architecture} {Search} without {Parameter} {Retraining}},
	shorttitle = {{DSNAS}},
	url = {http://arxiv.org/abs/2002.09128},
	abstract = {If NAS methods are solutions, what is the problem? Most existing NAS methods require two-stage parameter optimization. However, performance of the same architecture in the two stages correlates poorly. In this work, we propose a new problem deﬁnition for NAS, task-speciﬁc endto-end, based on this observation. We argue that given a computer vision task for which a NAS method is expected, this deﬁnition can reduce the vaguely-deﬁned NAS evaluation to i) accuracy of this task and ii) the total computation consumed to ﬁnally obtain a model with satisfying accuracy. Seeing that most existing methods do not solve this problem directly, we propose DSNAS, an efﬁcient differentiable NAS framework that simultaneously optimizes architecture and parameters with a low-biased Monte Carlo estimate. Child networks derived from DSNAS can be deployed directly without parameter retraining. Comparing with two-stage methods, DSNAS successfully discovers networks with comparable accuracy (74.4\%) on ImageNet in 420 GPU hours, reducing the total time by more than 34\%.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2002.09128 [cs, stat]},
	author = {Hu, Shoukang and Xie, Sirui and Zheng, Hehui and Liu, Chunxiao and Shi, Jianping and Liu, Xunying and Lin, Dahua},
	month = mar,
	year = {2020},
	note = {arXiv: 2002.09128},
	keywords = {Untagged},
}

@inproceedings{huangSelfadaptiveTrainingEmpirical2020,
	title = {Self-adaptive training: beyond empirical risk minimization},
	volume = {33},
	shorttitle = {Self-adaptive training},
	booktitle = {Advances in neural information processing systems},
	author = {Huang, Lang and Zhang, Chao and Zhang, Hongyang},
	year = {2020},
	keywords = {Untagged},
	pages = {19365--19376},
}

@inproceedings{huangAdversarialRobustnessStabilized2020,
	title = {Adversarial robustness of stabilized neuralodes might be from obfuscated gradients},
	booktitle = {{arXiv} preprint {arXiv}:2009.13145},
	author = {Huang, Yifei and Yu, Yaodong and Zhang, Hongyang and Ma, Yi and Yao, Yuan},
	year = {2020},
	keywords = {Untagged},
}

@inproceedings{huangFederatedLearningSpoken2020,
	address = {Barcelona, Spain (Online)},
	title = {Federated {Learning} for {Spoken} {Language} {Understanding}},
	url = {https://aclanthology.org/2020.coling-main.310},
	doi = {10/gngt3f},
	abstract = {Recently, spoken language understanding (SLU) has attracted extensive research interests, and various SLU datasets have been proposed to promote the development. However, most of the existing methods focus on a single individual dataset, the efforts to improve the robustness of models and obtain better performance by combining the merits of various datasets are not well studied. In this paper, we argue that if these SLU datasets are considered together, different knowledge from different datasets could be learned jointly, and there are high chances to promote the performance of each dataset. At the same time, we further attempt to prevent data leakage when unifying multiple datasets which, arguably, is more useful in an industry setting. To this end, we propose a federated learning framework, which could unify various types of datasets as well as tasks to learn and fuse various types of knowledge, i.e., text representations, from different datasets and tasks, without the sharing of downstream task data. The fused text representations merge useful features from different SLU datasets and tasks and are thus much more powerful than the original text representations alone in individual tasks. At last, in order to provide multi-granularity text representations for our framework, we propose a novel Multi-view Encoder (MV-Encoder) as the backbone of our federated learning framework. Experiments on two SLU benchmark datasets, including two tasks (intention detection and slot filling) and federated learning settings (horizontal federated learning, vertical federated learning and federated transfer learning), demonstrate the effectiveness and universality of our approach. Specifically, we are able to get 1.53\% improvement on the intent detection metric accuracy. And we could also boost the performance of a strong baseline by up to 5.29\% on the slot filling metric F1. Furthermore, by leveraging BERT as an additional encoder, we establish new state-of-the-art results on SNIPS and ATIS datasets, where we get 99.33\% and 98.28\% in terms of accuracy on intent detection task as well as 97.20\% and 96.41\% in terms of F1 score on slot filling task, respectively.},
	urldate = {2021-11-16},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Huang, Zhiqi and Liu, Fenglin and Zou, Yuexian},
	month = dec,
	year = {2020},
	keywords = {Untagged},
	pages = {3467--3478},
}

@inproceedings{pmlr-v119-ishida20a,
	series = {Proceedings of machine learning research},
	title = {Do we need zero training loss after achieving zero training error?},
	volume = {119},
	url = {https://proceedings.mlr.press/v119/ishida20a.html},
	abstract = {Overparameterized deep networks have the capacity to memorize training data with zero \textit{training error}. Even after memorization, the \textit{training loss} continues to approach zero, making the model overconfident and the test performance degraded. Since existing regularizers do not directly aim to avoid zero training loss, it is hard to tune their hyperparameters in order to maintain a fixed/preset level of training loss. We propose a direct solution called \textit{flooding} that intentionally prevents further reduction of the training loss when it reaches a reasonably small value, which we call the \textit{flood level}. Our approach makes the loss float around the flood level by doing mini-batched gradient descent as usual but gradient ascent if the training loss is below the flood level. This can be implemented with one line of code and is compatible with any stochastic optimizer and other regularizers. With flooding, the model will continue to “random walk” with the same non-zero training loss, and we expect it to drift into an area with a flat loss landscape that leads to better generalization. We experimentally show that flooding improves performance and, as a byproduct, induces a double descent curve of the test loss.},
	booktitle = {Proceedings of the 37th international conference on machine learning},
	publisher = {PMLR},
	author = {Ishida, Takashi and Yamane, Ikko and Sakai, Tomoya and Niu, Gang and Sugiyama, Masashi},
	editor = {III, Hal Daumé and Singh, Aarti},
	month = jul,
	year = {2020},
	note = {tex.pdf: http://proceedings.mlr.press/v119/ishida20a/ishida20a.pdf},
	keywords = {Untagged},
	pages = {4604--4614},
}

@article{jiangAutomaticNormalPositioning2020,
	title = {Automatic {Normal} {Positioning} of {Robotic} {Ultrasound} {Probe} {Based} {Only} on {Confidence} {Map} {Optimization} and {Force} {Measurement}},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	url = {https://ieeexplore.ieee.org/document/8963620/},
	doi = {10/ghv3gk},
	abstract = {Acquiring good image quality is one of the main challenges for fully-automatic robot-assisted ultrasound systems (RUSS). The presented method aims at overcoming this challenge for orthopaedic applications by optimizing the orientation of the robotic ultrasound (US) probe, i.e. aligning the central axis of the US probe to the tissue’s surface normal at the point of contact in order to improve sound propagation within the tissue. We ﬁrst optimize the in-plane orientation of the probe by analyzing the conﬁdence map [1] of the US image. We then carry out a fan motion and analyze the resulting forces estimated from joint torques to align the central axis of the probe to the normal within the plane orthogonal to the initial image plane. This results in the ﬁnal 3D alignment of the probe’s main axis with the normal to the anatomical surface at the point of contact without using external sensors for surface reconstruction or localizing the point of contact in an anatomical atlas. The algorithm is evaluated both on a phantom and on human tissues (forearm, upper arm and lower back). The mean absolute angular difference (± STD) between true and estimated normal on stationary phantom, forearm, upper arm and lower back was 3.1 ± 1.0◦, 3.7 ± 1.7◦, 5.3 ± 1.3◦ and 6.9 ± 3.5◦, respectively. In comparison, six human operators obtained errors of 3.2 ± 1.7◦ on the phantom. Hence the method is able to automatically position the probe normal to the scanned tissue at the point of contact and thus improve the quality of automatically acquired ultrasound images.},
	language = {en},
	number = {2},
	urldate = {2021-01-27},
	journal = {IEEE Robotics and Automation Letters},
	author = {Jiang, Zhongliang and Grimm, Matthias and Zhou, Mingchuan and Esteban, Javier and Simson, Walter and Zahnd, Guillaume and Navab, Nassir},
	month = apr,
	year = {2020},
	keywords = {Untagged},
	pages = {1342--1349},
}

@inproceedings{jin_is_2020,
	title = {Is {BERT} {Really} {Robust}? {A} {Strong} {Baseline} for {Natural} {Language} {Attack} on {Text} {Classification} and {Entailment}},
	volume = {34},
	shorttitle = {Is {BERT} {Really} {Robust}?},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6311},
	doi = {10.1609/aaai.v34i05.6311},
	abstract = {Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective—it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving—it preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficient—it generates adversarial text with computational complexity linear to the text length.1},
	language = {en},
	urldate = {2023-06-02},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
	month = apr,
	year = {2020},
	keywords = {Untagged},
	pages = {8018--8025},
}

@inproceedings{jinAdaBitsNeuralNetwork2020,
	address = {Seattle, WA, USA},
	title = {{AdaBits}: {Neural} {Network} {Quantization} {With} {Adaptive} {Bit}-{Widths}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{AdaBits}},
	url = {https://ieeexplore.ieee.org/document/9157698/},
	doi = {10.1109/CVPR42600.2020.00222},
	abstract = {Deep neural networks with adaptive conﬁgurations have gained increasing attention due to the instant and ﬂexible deployment of these models on platforms with diﬀerent resource budgets. In this paper, we investigate a novel option to achieve this goal by enabling adaptive bit-widths of weights and activations in the model. We ﬁrst examine the beneﬁts and challenges of training quantized model with adaptive bit-widths, and then experiment with several approaches including direct adaptation, progressive training and joint training. We discover that joint training is able to produce comparable performance on the adaptive model as individual models. We also propose a new technique named Switchable Clipping Level (S-CL) to further improve quantized models at the lowest bit-width. With our proposed techniques applied on a bunch of models including MobileNet V1/V2 and ResNet50, we demonstrate that bit-width of weights and activations is a new option for adaptively executable deep neural networks, oﬀering a distinct opportunity for improved accuracy-eﬃciency trade-oﬀ as well as instant adaptation according to the platform constraints in real-world applications.},
	language = {en},
	urldate = {2022-03-04},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Jin, Qing and Yang, Linjie and Liao, Zhenyu},
	month = jun,
	year = {2020},
	keywords = {Untagged},
	pages = {2143--2153},
}

@article{kamathHowSGDHyperparameters2020,
	title = {How do {SGD} hyperparameters in natural training affect adversarial robustness?},
	url = {https://arxiv.org/abs/2006.11604v1},
	abstract = {Learning rate, batch size and momentum are three important hyperparameters in the SGD algorithm. It is known from the work of Jastrzebski et al. arXiv:1711.04623 that large batch size training of neural networks yields models which do not generalize well. Yao et al. arXiv:1802.08241 observe that large batch training yields models that have poor adversarial robustness. In the same paper, the authors train models with different batch sizes and compute the eigenvalues of the Hessian of loss function. They observe that as the batch size increases, the dominant eigenvalues of the Hessian become larger. They also show that both adversarial training and small-batch training leads to a drop in the dominant eigenvalues of the Hessian or lowering its spectrum. They combine adversarial training and second order information to come up with a new large-batch training algorithm and obtain robust models with good generalization. In this paper, we empirically observe the effect of the SGD hyperparameters on the accuracy and adversarial robustness of networks trained with unperturbed samples. Jastrzebski et al. considered training models with a fixed learning rate to batch size ratio. They observed that higher the ratio, better is the generalization. We observe that networks trained with constant learning rate to batch size ratio, as proposed in Jastrzebski et al., yield models which generalize well and also have almost constant adversarial robustness, independent of the batch size. We observe that momentum is more effective with varying batch sizes and a fixed learning rate than with constant learning rate to batch size ratio based SGD training.},
	language = {en},
	urldate = {2022-02-23},
	author = {Kamath, Sandesh and Deshpande, Amit and Subrahmanyam, K. V.},
	month = jun,
	year = {2020},
	keywords = {Untagged},
}

@article{kangFedMVTSemisupervisedVertical2020,
	title = {{FedMVT}: {Semi}-supervised {Vertical} {Federated} {Learning} with {MultiView} {Training}},
	shorttitle = {{FedMVT}},
	url = {http://arxiv.org/abs/2008.10838},
	abstract = {Federated learning allows many parties to collaboratively build a model without exposing data. Particularly, vertical federated learning (VFL) enables parties to build a robust shared machine learning model based upon distributed features about the same samples. However, VFL requires all parties to share a sufficient amount of overlapping samples. In reality, the set of overlapping samples may be small, leaving the majority of the non-overlapping data unutilized. In this paper, we propose Federated Multi-View Training (FedMVT), a semi-supervised learning approach that improves the performance of VFL with limited overlapping samples. FedMVT estimates representations for missing features and predicts pseudo-labels for unlabeled samples to expand training set, and trains three classifiers jointly based upon different views of the input to improve model's representation learning. FedMVT does not require parties to share their original data and model parameters, thus preserving data privacy. We conduct experiments on the NUS-WIDE and the CIFAR10. The experimental results demonstrate that FedMVT significantly outperforms vanilla VFL that only utilizes overlapping samples, and improves the performance of the local model in the party that owns labels.},
	urldate = {2022-03-04},
	journal = {arXiv:2008.10838 [cs, stat]},
	author = {Kang, Yan and Liu, Yang and Chen, Tianjian},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.10838},
	keywords = {Untagged},
}

@misc{karpukhin_dense_2020,
	title = {Dense {Passage} {Retrieval} for {Open}-{Domain} {Question} {Answering}},
	shorttitle = {{DPR}},
	url = {http://arxiv.org/abs/2004.04906},
	abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9\%-19\% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Karpukhin, Vladimir and Oğuz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
	month = sep,
	year = {2020},
	note = {arXiv:2004.04906 [cs]},
	keywords = {Untagged},
}

@inproceedings{karrasTrainingGenerativeAdversarial2020,
	title = {Training generative adversarial networks with limited data},
	url = {https://proceedings.neurips.cc/paper/2020/hash/8d30aa96e72440759f74bd2306c1fa3d-Abstract.html},
	booktitle = {Advances in neural information processing systems 33: {Annual} conference on neural information processing systems 2020, {NeurIPS} 2020, december 6-12, 2020, virtual},
	author = {Karras, Tero and Aittala, Miika and Hellsten, Janne and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/nips/KarrasAHLLA20.bib
tex.timestamp: Tue, 19 Jan 2021 15:56:48 +0100},
	keywords = {Untagged},
}

@article{katharopoulosTransformersAreRNNs2020,
	title = {Transformers are {RNNs}: {Fast} {Autoregressive} {Transformers} with {Linear} {Attention}},
	shorttitle = {Transformers are {RNNs}},
	url = {http://arxiv.org/abs/2006.16236},
	abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input’s length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from O N 2 to O (N ), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
	language = {en},
	urldate = {2021-11-04},
	journal = {arXiv:2006.16236 [cs, stat]},
	author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, François},
	month = aug,
	year = {2020},
	note = {arXiv: 2006.16236},
	keywords = {Untagged},
}

@article{kawaguchiEliminationAllBad2020,
	title = {Elimination of {All} {Bad} {Local} {Minima} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1901.00279},
	abstract = {In this paper, we theoretically prove that adding one special neuron per output unit eliminates all suboptimal local minima of any deep neural network, for multi-class classiﬁcation, binary classiﬁcation, and regression with an arbitrary loss function, under practical assumptions. At every local minimum of any deep neural network with these added neurons, the set of parameters of the original neural network (without added neurons) is guaranteed to be a global minimum of the original neural network. The eﬀects of the added neurons are proven to automatically vanish at every local minimum. Moreover, we provide a novel theoretical characterization of a failure mode of eliminating suboptimal local minima via an additional theorem and several examples. This paper also introduces a novel proof technique based on the perturbable gradient basis (PGB) necessary condition of local minima, which provides new insight into the elimination of local minima and is applicable to analyze various models and transformations of objective functions beyond the elimination of local minima.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1901.00279 [cs, math, stat]},
	author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack},
	month = jan,
	year = {2020},
	note = {arXiv: 1901.00279},
	keywords = {Untagged},
}

@article{khandelwal_generalization_2020,
	title = {{GENERALIZATION} {THROUGH} {MEMORIZATION}: {NEAREST} {NEIGHBOR} {LANGUAGE} {MODELS}},
	abstract = {We introduce kNN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a k-nearest neighbors (kNN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong WIKITEXT-103 LM, with neighbors drawn from the original training set, our kNN-LM achieves a new stateof-the-art perplexity of 15.79 – a 2.9 point improvement with no additional training. We also show that this approach has implications for efﬁciently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.},
	language = {en},
	author = {Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
	year = {2020},
	keywords = {Untagged},
}

@inproceedings{kim_adaptive_2020,
	address = {Online},
	title = {Adaptive {Compression} of {Word} {Embeddings}},
	url = {https://aclanthology.org/2020.acl-main.364},
	doi = {10.18653/v1/2020.acl-main.364},
	abstract = {Distributed representations of words have been an indispensable component for natural language processing (NLP) tasks. However, the large memory footprint of word embeddings makes it challenging to deploy NLP models to memory-constrained devices (e.g., self-driving cars, mobile devices). In this paper, we propose a novel method to adaptively compress word embeddings. We fundamentally follow a code-book approach that represents words as discrete codes such as (8, 5, 2, 4). However, unlike prior works that assign the same length of codes to all words, we adaptively assign different lengths of codes to each word by learning downstream tasks. The proposed method works in two steps. First, each word directly learns to select its code length in an end-to-end manner by applying the Gumbel-softmax tricks. After selecting the code length, each word learns discrete codes through a neural network with a binary constraint. To showcase the general applicability of the proposed method, we evaluate the performance on four different downstream tasks. Comprehensive evaluation results clearly show that our method is effective and makes the highly compressed word embeddings without hurting the task accuracy. Moreover, we show that our model assigns word to each code-book by considering the significance of tasks.},
	urldate = {2023-04-24},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Yeachan and Kim, Kang-Min and Lee, SangKeun},
	month = jul,
	year = {2020},
	keywords = {Untagged},
	pages = {3950--3959},
}

@article{koBenchmarkTricksLargescale2020,
	title = {A {Benchmark} on {Tricks} for {Large}-scale {Image} {Retrieval}},
	url = {http://arxiv.org/abs/1907.11854},
	abstract = {Many studies have been performed on metric learning, which has become a key ingredient in top-performing methods of instance-level image retrieval. Meanwhile, less attention has been paid to pre-processing and post-processing tricks that can signiﬁcantly boost performance. Furthermore, we found that most previous studies used small scale datasets to simplify processing. Because the behavior of a feature representation in a deep learning model depends on both domain and data, it is important to understand how model behave in large-scale environments when a proper combination of retrieval tricks is used. In this paper, we extensively analyze the effect of well-known pre-processing, post-processing tricks, and their combination for largescale image retrieval. We found that proper use of these tricks can signiﬁcantly improve model performance without necessitating complex architecture or introducing loss, as conﬁrmed by achieving a competitive result on the Google Landmark Retrieval Challenge 2019.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1907.11854 [cs]},
	author = {Ko, Byungsoo and Shin, Minchul and Gu, Geonmo and Jun, HeeJae and Lee, Tae Kwan and Kim, Youngjoon},
	month = apr,
	year = {2020},
	note = {arXiv: 1907.11854},
	keywords = {Untagged},
}

@inproceedings{koskelaLearningRateAdaptation2020,
	address = {Palermo, Sicily, Italy},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Learning {Rate} {Adaptation} for {Differentially} {Private} {Learning}},
	volume = {108},
	url = {http://proceedings.mlr.press/v108/koskela20a.html},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Koskela, Antti and Honkela, Antti},
	editor = {Chiappa, Silvia and Calandra, Roberto},
	year = {2020},
	keywords = {Untagged},
	pages = {2465--2475},
}

@inproceedings{kurita_weight_2020,
	address = {Online},
	title = {Weight {Poisoning} {Attacks} on {Pretrained} {Models}},
	url = {https://aclanthology.org/2020.acl-main.249},
	doi = {10.18653/v1/2020.acl-main.249},
	abstract = {Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct “weight poisoning” attacks where pre-trained weights are injected with vulnerabilities that expose “backdoors” after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks.},
	urldate = {2022-11-07},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Kurita, Keita and Michel, Paul and Neubig, Graham},
	month = jul,
	year = {2020},
	keywords = {Untagged},
	pages = {2793--2806},
}

@article{lawCornerNetDetectingObjects2020,
	title = {{CornerNet}: {Detecting} {Objects} as {Paired} {Keypoints}},
	volume = {128},
	url = {https://doi.org/10.1007/s11263-019-01204-1},
	doi = {10/ggwbc3},
	number = {3},
	journal = {Int. J. Comput. Vis.},
	author = {Law, Hei and Deng, Jia},
	year = {2020},
	keywords = {Untagged},
	pages = {642--656},
}

@inproceedings{liSequentialLearningDomain2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Sequential {Learning} for {Domain} {Generalization}},
	isbn = {978-3-030-66415-2},
	doi = {10.1007/978-3-030-66415-2_39},
	abstract = {In this paper we propose a sequential learning framework for Domain Generalization (DG), the problem of training a model that is robust to domain shift by design. Various DG approaches have been proposed with different motivating intuitions, but they typically optimize for a single step of domain generalization – training on one set of domains and generalizing to one other. Our sequential learning is inspired by the idea lifelong learning, where accumulated experience means that learning the \$\$n{\textasciicircum}\{th\}\$\$nththing becomes easier than the \$\$1{\textasciicircum}\{st\}\$\$1stthing. In DG this means encountering a sequence of domains and at each step training to maximise performance on the next domain. The performance at domain n then depends on the previous \$\$n-1\$\$n-1learning problems. Thus backpropagating through the sequence means optimizing performance not just for the next domain, but all following domains. Training on all such sequences of domains provides dramatically more ‘practice’ for a base DG learner compared to existing approaches, thus improving performance on a true testing domain. This strategy can be instantiated for different base DG algorithms, but we focus on its application to the recently proposed Meta-Learning Domain generalization (MLDG). We show that for MLDG it leads to a simple to implement and fast algorithm that provides consistent performance improvement on a variety of DG benchmarks.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy},
	editor = {Bartoli, Adrien and Fusiello, Andrea},
	year = {2020},
	keywords = {Untagged},
	pages = {603--619},
}

@inproceedings{li_bert-attack_2020,
	address = {Online},
	title = {{BERT}-{ATTACK}: {Adversarial} {Attack} {Against} {BERT} {Using} {BERT}},
	shorttitle = {{BERT}-{ATTACK}},
	url = {https://aclanthology.org/2020.emnlp-main.500},
	doi = {10.18653/v1/2020.emnlp-main.500},
	abstract = {Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose BERT-Attack, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at https://github.com/LinyangLee/BERT-Attack.},
	urldate = {2023-02-26},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Linyang and Ma, Ruotian and Guo, Qipeng and Xue, Xiangyang and Qiu, Xipeng},
	month = nov,
	year = {2020},
	keywords = {Untagged},
	pages = {6193--6202},
}

@article{liSoKCertifiedRobustness2020,
	title = {{SoK}: {Certified} {Robustness} for {Deep} {Neural} {Networks}},
	shorttitle = {{SoK}},
	url = {http://arxiv.org/abs/2009.04131},
	abstract = {Great advancement in deep neural networks (DNNs) has led to state-of-the-art performance on a wide range of tasks. However, recent studies have shown that DNNs are vulnerable to adversarial attacks, which have brought great concerns when deploying these models to safety-critical applications such as autonomous driving. Different defense approaches have been proposed against adversarial attacks, including: 1) empirical defenses, which can be adaptively attacked again without providing robustness certiﬁcation; and 2) certiﬁably robust approaches, which consist of robustness veriﬁcation providing the lower bound of robust accuracy against any attacks under certain conditions and corresponding robust training approaches. In this paper, we focus on these certiﬁably robust approaches and provide the ﬁrst work to perform large-scale systematic analysis of different robustness veriﬁcation and training approaches. In particular, we 1) provide a taxonomy for the robustness veriﬁcation and training approaches, as well as discuss the detailed methodologies for representative algorithms, 2) reveal the fundamental connections among these approaches, 3) discuss current research progresses, theoretical barriers, main challenges, and several promising future directions for certiﬁed defenses for DNNs, and 4) provide an open-sourced uniﬁed platform to evaluate 20+ representative veriﬁcation and corresponding robust training approaches on a wide range of DNNs.},
	language = {en},
	urldate = {2021-11-04},
	journal = {arXiv:2009.04131},
	author = {Li, Linyi and Qi, Xiangyu and Xie, Tao and Li, Bo},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.04131},
	keywords = {Untagged},
}

@article{liMultipleCodeHashing2020,
	title = {Multiple {Code} {Hashing} for {Efficient} {Image} {Retrieval}},
	url = {http://arxiv.org/abs/2008.01503},
	abstract = {Due to its low storage cost and fast query speed, hashing has been widely used in large-scale image retrieval tasks. Hash bucket search returns data points within a given Hamming radius to each query, which can enable search at a constant or sub-linear time cost. However, existing hashing methods cannot achieve satisfactory retrieval performance for hash bucket search in complex scenarios, since they learn only one hash code for each image. More speciﬁcally, by using one hash code to represent one image, existing methods might fail to put similar image pairs to the buckets with a small Hamming distance to the query when the semantic information of images is complex. As a result, a large number of hash buckets need to be visited for retrieving similar images, based on the learned codes. This will deteriorate the eﬃciency of hash bucket search. In this paper, we propose a novel hashing framework, called multiple code hashing (MCH), to improve the performance of hash bucket search. The main idea of MCH is to learn multiple hash codes for each image, with each code representing a diﬀerent region of the image. Furthermore, we propose a deep reinforcement learning algorithm to learn the parameters in MCH. To the best of our knowledge, this is the ﬁrst work that proposes to learn multiple hash codes for each image in image retrieval. Experiments demonstrate that MCH can achieve a signiﬁcant improvement in hash bucket search, compared with existing methods that learn only one hash code for each image.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2008.01503 [cs, stat]},
	author = {Li, Ming-Wei and Jiang, Qing-Yuan and Li, Wu-Jun},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.01503},
	keywords = {Untagged},
}

@inproceedings{liGroupSkeletonBasedHumanAction2020,
	address = {Seattle, WA},
	title = {Group-{Skeleton}-{Based} {Human} {Action} {Recognition} in {Complex} {Events}},
	url = {https://doi.org/10.1145/3394171.3416280},
	doi = {10.1145/3394171.3416280},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Li, Tingtian and Sun, Zixun and Chen, Xiao},
	editor = {Chen, Chang Wen and Cucchiara, Rita and Hua, Xian-Sheng and Qi, Guo-Jun and Ricci, Elisa and Zhang, Zhengyou and Zimmermann, Roger},
	year = {2020},
	keywords = {Untagged},
	pages = {4703--4707},
}

@article{liAdaXAdaptiveGradient2020,
	title = {{AdaX}: {Adaptive} {Gradient} {Descent} with {Exponential} {Long} {Term} {Memory}},
	shorttitle = {{AdaX}},
	url = {http://arxiv.org/abs/2004.09740},
	abstract = {Although adaptive optimization algorithms such as Adam show fast convergence in many machine learning tasks, this paper identiﬁes a problem of Adam by analyzing its performance in a simple non-convex synthetic problem, showing that Adam’s fast convergence would possibly lead the algorithm to local minimums. To address this problem, we improve Adam by proposing a novel adaptive gradient descent algorithm named AdaX. Unlike Adam that ignores the past gradients, AdaX exponentially accumulates the long-term gradient information in the past during training, to adaptively tune the learning rate. We thoroughly prove the convergence of AdaX in both the convex and non-convex settings. Extensive experiments show that AdaX outperforms Adam in various tasks of computer vision and natural language processing and can catch up with Stochastic Gradient Descent.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:2004.09740 [cs, stat]},
	author = {Li, Wenjie and Zhang, Zhaoyang and Wang, Xinjiang and Luo, Ping},
	month = may,
	year = {2020},
	note = {arXiv: 2004.09740},
	keywords = {Untagged},
}

@article{liGeneralizedFocalLoss2020,
	title = {Generalized {Focal} {Loss}: {Learning} {Qualified} and {Distributed} {Bounding} {Boxes} for {Dense} {Object} {Detection}},
	shorttitle = {Generalized {Focal} {Loss}},
	url = {http://arxiv.org/abs/2006.04388},
	abstract = {One-stage detector basically formulates object detection as dense classiﬁcation and localization (i.e., bounding box regression). The classiﬁcation is usually optimized by Focal Loss and the box location is commonly learned under Dirac delta distribution. A recent trend for one-stage detectors is to introduce an individual prediction branch to estimate the quality of localization, where the predicted quality facilitates the classiﬁcation to improve detection performance. This paper delves into the representations of the above three fundamental elements: quality estimation, classiﬁcation and localization. Two problems are discovered in existing practices, including (1) the inconsistent usage of the quality estimation and classiﬁcation between training and inference (i.e., separately trained but compositely used in test) and (2) the inﬂexible Dirac delta distribution for localization when there is ambiguity and uncertainty which is often the case in complex scenes. To address the problems, we design new representations for these elements. Speciﬁcally, we merge the quality estimation into the class prediction vector to form a joint representation of localization quality and classiﬁcation, and use a vector to represent arbitrary distribution of box locations. The improved representations eliminate the inconsistency risk and accurately depict the ﬂexible distribution in real data, but contain continuous labels, which is beyond the scope of Focal Loss. We then propose Generalized Focal Loss (GFL) that generalizes Focal Loss from its discrete form to the continuous version for successful optimization. On COCO test-dev, GFL achieves 45.0\% AP using ResNet-101 backbone, surpassing state-of-the-art SAPD (43.5\%) and ATSS (43.6\%) with higher or comparable inference speed, under the same backbone and training settings. Notably, our best model can achieve a single-model single-scale AP of 48.2\%, at 10 FPS on a single 2080Ti GPU. Code and pretrained models are available at https://github.com/implus/GFocal.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2006.04388 [cs]},
	author = {Li, Xiang and Wang, Wenhai and Wu, Lijun and Chen, Shuo and Hu, Xiaolin and Li, Jun and Tang, Jinhui and Yang, Jian},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.04388},
	keywords = {Untagged},
}

@inproceedings{liSimpleFastAlgorithm2020,
	title = {Simple and {Fast} {Algorithm} for {Binary} {Integer} and {Online} {Linear} {Programming}},
	url = {https://openreview.net/forum?id=8satAeuaK_},
	language = {en},
	urldate = {2022-02-22},
	author = {Li, Xiaocheng and Sun, Chunlin and Ye, Yinyu},
	month = jan,
	year = {2020},
	keywords = {Untagged},
}

@article{liOscarObjectSemanticsAligned2020,
	title = {Oscar: {Object}-{Semantics} {Aligned} {Pre}-training for {Vision}-{Language} {Tasks}},
	shorttitle = {Oscar},
	url = {http://arxiv.org/abs/2004.06165},
	abstract = {Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing methods simply concatenate image region features and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner, in this paper, we propose a new learning method Oscar (Object-Semantics Aligned Pre-training), which uses object tags detected in images as anchor points to significantly ease the learning of alignments. Our method is motivated by the observation that the salient objects in an image can be accurately detected, and are often mentioned in the paired text. We pre-train an Oscar model on the public corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating new state-of-the-arts on six well-established vision-language understanding and generation tasks.},
	language = {en},
	urldate = {2021-10-21},
	journal = {arXiv:2004.06165 [cs]},
	author = {Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and Choi, Yejin and Gao, Jianfeng},
	month = jul,
	year = {2020},
	note = {arXiv: 2004.06165},
	keywords = {Untagged},
}

@article{liDADADifferentiableAutomatic2020,
	title = {{DADA}: {Differentiable} {Automatic} {Data} {Augmentation}},
	shorttitle = {{DADA}},
	url = {http://arxiv.org/abs/2003.03780},
	abstract = {Data augmentation (DA) techniques aim to increase data variability, and thus train deep networks with better generalisation. The pioneering AutoAugment automated the search for optimal DA policies with reinforcement learning. However, AutoAugment is extremely computationally expensive, limiting its wide applicability. Followup works such as Population Based Augmentation (PBA) and Fast AutoAugment improved eﬃciency, but their optimization speed remains a bottleneck. In this paper, we propose Diﬀerentiable Automatic Data Augmentation (DADA) which dramatically reduces the cost. DADA relaxes the discrete DA policy selection to a diﬀerentiable optimization problem via Gumbel-Softmax. In addition, we introduce an unbiased gradient estimator, RELAX, leading to an eﬃcient and eﬀective one-pass optimization strategy to learn an eﬃcient and accurate DA policy. We conduct extensive experiments on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Furthermore, we demonstrate the value of Auto DA in pre-training for downstream detection problems. Results show our DADA is at least one order of magnitude faster than the state-of-theart while achieving very comparable accuracy. The code is available at https://github.com/VDIGPKU/DADA.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2003.03780 [cs]},
	author = {Li, Yonggang and Hu, Guosheng and Wang, Yongtao and Hospedales, Timothy and Robertson, Neil M. and Yang, Yongxin},
	month = jul,
	year = {2020},
	note = {arXiv: 2003.03780},
	keywords = {Untagged},
}

@article{liLearningDisentanglingFusing2020,
	title = {Learning disentangling and fusing networks for face completion under structured occlusions},
	volume = {99},
	url = {https://doi.org/10.1016/j.patcog.2019.107073},
	doi = {10/ggbz3s},
	journal = {Pattern Recognit.},
	author = {Li, Zhihang and Hu, Yibo and He, Ran and Sun, Zhenan},
	year = {2020},
	keywords = {Untagged},
}

@article{lillicrapBackpropagationBrain2020,
	title = {Backpropagation and the brain},
	volume = {21},
	issn = {1471-003X, 1471-0048},
	url = {http://www.nature.com/articles/s41583-020-0277-3},
	doi = {10.1038/s41583-020-0277-3},
	abstract = {During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.},
	language = {en},
	number = {6},
	urldate = {2021-03-18},
	journal = {Nature Reviews Neuroscience},
	author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	keywords = {Untagged},
	pages = {335--346},
}

@article{limTemporalFusionTransformers2020,
	title = {Temporal {Fusion} {Transformers} for {Interpretable} {Multi}-horizon {Time} {Series} {Forecasting}},
	url = {http://arxiv.org/abs/1912.09363},
	abstract = {Multi-horizon forecasting problems often contain a complex mix of inputs -- including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed historically -- without any prior information on how they interact with the target. While several deep learning models have been proposed for multi-step prediction, they typically comprise black-box models which do not account for the full range of inputs present in common scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) -- a novel attention-based architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, the TFT utilizes recurrent layers for local processing and interpretable self-attention layers for learning long-term dependencies. The TFT also uses specialized components for the judicious selection of relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of regimes. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and showcase three practical interpretability use-cases of TFT.},
	urldate = {2021-07-19},
	journal = {arXiv:1912.09363 [cs, stat]},
	author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
	month = sep,
	year = {2020},
	note = {arXiv: 1912.09363},
	keywords = {Untagged},
}

@inproceedings{NEURIPS2020_18df51b9,
	title = {Ensemble distillation for robust model fusion in federated learning},
	volume = {33},
	shorttitle = {{FedFD}},
	url = {https://proceedings.neurips.cc/paper/2020/file/18df51b97ccd68128e994804f3eccc87-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Lin, Tao and Kong, Lingjing and Stich, Sebastian U and Jaggi, Martin},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	keywords = {Untagged},
	pages = {2351--2363},
}

@inproceedings{liuFederatedLearningVisionandLanguage2020,
	title = {Federated {Learning} for {Vision}-and-{Language} {Grounding} {Problems}},
	volume = {34},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/6824},
	doi = {10/gmznx3},
	abstract = {Recently, vision-and-language grounding problems, e.g., image captioning and visual question answering (VQA), has attracted extensive interests from both academic and industrial worlds. However, given the similarity of these tasks, the efforts to obtain better results by combining the merits of their algorithms are not well studied. Inspired by the recent success of federated learning, we propose a federated learning framework to obtain various types of image representations from different tasks, which are then fused together to form ﬁnegrained image representations. The representations merge useful features from different vision-and-language grounding problems, and are thus much more powerful than the original representations alone in individual tasks. To learn such image representations, we propose the Aligning, Integrating and Mapping Network (aimNet). The aimNet is validated on three federated learning settings, which include horizontal federated learning, vertical federated learning, and federated transfer learning. Experiments of aimNet-based federated learning framework on two representative tasks, i.e., image captioning and VQA, demonstrate the effective and universal improvements of all metrics over the baselines. In image captioning, we are able to get 14\% and 13\% relative gain on the taskspeciﬁc metrics CIDEr and SPICE, respectively. In VQA, we could also boost the performance of strong baselines by up to 3\%.},
	language = {en},
	urldate = {2021-11-16},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Liu, Fenglin and Wu, Xian and Ge, Shen and Fan, Wei and Zou, Yuexian},
	month = apr,
	year = {2020},
	keywords = {Untagged},
	pages = {11572--11579},
}

@article{liuUnderstandingWhyNeural2020,
	title = {Understanding {Why} {Neural} {Networks} {Generalize} {Well} {Through} {GSNR} of {Parameters}},
	url = {http://arxiv.org/abs/2001.07384},
	abstract = {As deep neural networks (DNNs) achieve tremendous success across many application domains, researchers tried to explore in many aspects on why they generalize well. In this paper, we provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is deﬁned as the ratio between its gradient’s squared mean and variance, over the data distribution. Based on several approximations, we establish a quantitative relationship between model parameters’ GSNR and the generalization gap. This relationship indicates that larger GSNR during training process leads to better generalization performance. Moreover, we show that, different from that of shallow models (e.g. logistic regression, support vector machines), the gradient descent optimization dynamics of DNNs naturally produces large GSNR during training, which is probably the key to DNNs’ remarkable generalization ability.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:2001.07384 [cs, stat]},
	author = {Liu, Jinlong and Jiang, Guoqing and Bai, Yunzhi and Chen, Ting and Wang, Huayan},
	month = feb,
	year = {2020},
	note = {arXiv: 2001.07384},
	keywords = {Untagged},
}

@inproceedings{liuBAMSProdStepGeneralizing2020,
	address = {Seattle, WA, USA},
	title = {{BAMSProd}: {A} {Step} towards {Generalizing} the {Adaptive} {Optimization} {Methods} to {Deep} {Binary} {Model}},
	isbn = {978-1-72819-360-1},
	shorttitle = {{BAMSProd}},
	url = {https://ieeexplore.ieee.org/document/9150627/},
	doi = {10.1109/CVPRW50498.2020.00345},
	abstract = {Recent methods have signiﬁcantly reduced the performance degradation of Binary Neural Networks (BNNs), but guaranteeing the effective and efﬁcient training of BNNs is an unsolved problem. The main reason is that the estimated gradients produced by the Straight-ThroughEstimator (STE) mismatches with the gradients of the real derivatives. In this paper, we provide an explicit convex optimization example where training the BNNs with the traditionally adaptive optimization methods still faces the risk of non-convergence, and identify that constraining the range of gradients is critical for optimizing the deep binary model to avoid highly suboptimal solutions. Besides, we propose a BAMSProd algorithm with a key observation that the convergence property of optimizing deep binary model is strongly related to the quantization errors. In brief, it employs an adaptive range constraint via an errors measurement for smoothing the gradients transition while follows the exponential moving strategy from AMSGrad to avoid errors accumulation during the optimization. The experiments verify the corollary of theoretical convergence analysis, and further demonstrate that our optimization method can speed up the convergence about 1.2× and boost the performance of BNNs to a signiﬁcant level than the speciﬁc binary optimizer about 3.7\%, even in a highly non-convex optimization problem.},
	language = {en},
	urldate = {2022-02-24},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Liu, Junjie and Wen, Dongchao and Wang, Deyu and Tao, Wei and Chen, Tse-Wei and Osa, Kinya and Kato, Masami},
	month = jun,
	year = {2020},
	keywords = {Untagged},
	pages = {2880--2889},
}

@article{liuDeepLearningGeneric2020,
	title = {Deep {Learning} for {Generic} {Object} {Detection}: {A} {Survey}},
	volume = {128},
	url = {https://doi.org/10.1007/s11263-019-01247-4},
	doi = {10/ggcd9z},
	number = {2},
	journal = {Int. J. Comput. Vis.},
	author = {Liu, Li and Ouyang, Wanli and Wang, Xiaogang and Fieguth, Paul W. and Chen, Jie and Liu, Xinwang and Pietikäinen, Matti},
	year = {2020},
	keywords = {Untagged},
	pages = {261--318},
}

@article{liuRCSSFLRobustCommunicationefficient2020,
	title = {{RC}-{SSFL}: {Towards} {Robust} and {Communication}-efficient {Semi}-supervised {Federated} {Learning} {System}},
	shorttitle = {{RC}-{SSFL}},
	url = {http://arxiv.org/abs/2012.04432},
	abstract = {Federated Learning (FL) is an emerging decentralized artificial intelligence paradigm, which promises to train a shared global model in high-quality while protecting user data privacy. However, the current systems rely heavily on a strong assumption: all clients have a wealth of ground truth labeled data, which may not be always feasible in the real life. In this paper, we present a practical Robust, and Communication-efficient Semi-supervised FL (RC-SSFL) system design that can enable the clients to jointly learn a high-quality model that is comparable to typical FL's performance. In this setting, we assume that the client has only unlabeled data and the server has a limited amount of labeled data. Besides, we consider malicious clients can launch poisoning attacks to harm the performance of the global model. To solve this issue, RC-SSFL employs a minimax optimization-based client selection strategy to select the clients who hold high-quality updates and uses geometric median aggregation to robustly aggregate model updates. Furthermore, RC-SSFL implements a novel symmetric quantization method to greatly improve communication efficiency. Extensive case studies on two real-world datasets demonstrate that RC-SSFL can maintain the performance comparable to typical FL in the presence of poisoning attacks and reduce communication overhead by \$2 {\textbackslash}times {\textbackslash}sim 4 {\textbackslash}times \$.},
	urldate = {2022-03-04},
	journal = {arXiv:2012.04432 [cs]},
	author = {Liu, Yi and Yuan, Xingliang and Zhao, Ruihui and Zheng, Yifeng and Zheng, Yefeng},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.04432},
	keywords = {Untagged},
}

@inproceedings{liuReActNetPreciseBinary2020,
	address = {Cham},
	title = {{ReActNet}: {Towards} {Precise} {Binary} {Neural} {Network} with {Generalized} {Activation} {Functions}},
	volume = {12359},
	isbn = {978-3-030-58567-9 978-3-030-58568-6},
	shorttitle = {{ReActNet}},
	url = {https://link.springer.com/10.1007/978-3-030-58568-6_9},
	abstract = {In this paper, we propose several ideas for enhancing a binary network to close its accuracy gap from real-valued networks without incurring any additional computational cost. We ﬁrst construct a baseline network by modifying and binarizing a compact real-valued network with parameter-free shortcuts, bypassing all the intermediate convolutional layers including the downsampling layers. This baseline network strikes a good trade-oﬀ between accuracy and eﬃciency, achieving superior performance than most of existing binary networks at approximately half of the computational cost. Through extensive experiments and analysis, we observed that the performance of binary networks is sensitive to activation distribution variations. Based on this important observation, we propose to generalize the traditional Sign and PReLU functions, denoted as RSign and RPReLU for the respective generalized functions, to enable explicit learning of the distribution reshape and shift at near-zero extra cost. Lastly, we adopt a distributional loss to further enforce the binary network to learn similar output distributions as those of a real-valued network. We show that after incorporating all these ideas, the proposed ReActNet outperforms all the state-of-thearts by a large margin. Speciﬁcally, it outperforms Real-to-Binary Net and MeliusNet29 by 4.0\% and 3.6\% respectively for the top-1 accuracy and also reduces the gap to its real-valued counterpart to within 3.0\% top-1 accuracy on ImageNet dataset. Code and models are available at: https://github.com/liuzechun/ReActNet.},
	language = {en},
	urldate = {2022-02-22},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Liu, Zechun and Shen, Zhiqiang and Savvides, Marios and Cheng, Kwang-Ting},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58568-6_9},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {143--159},
}

@techreport{longTransferLearningTheories2020,
	address = {CCDM},
	title = {Transfer {Learning}: {Theories} and {Algorithms}},
	language = {en},
	author = {Long, Mingsheng},
	year = {2020},
	keywords = {Untagged},
	pages = {50},
}

@inproceedings{longPragmaticApproachMembership2020,
	address = {Genoa, Italy},
	title = {A {Pragmatic} {Approach} to {Membership} {Inferences} on {Machine} {Learning} {Models}},
	isbn = {978-1-72815-087-1},
	url = {https://ieeexplore.ieee.org/document/9230385/},
	doi = {10.1109/EuroSP48549.2020.00040},
	abstract = {Membership Inference Attacks (MIAs) aim to determine the presence of a record in a machine learning model’s training data by querying the model. Recent work has demonstrated the effectiveness of MIA on various machine learning models and corresponding defenses have been proposed. However, both attacks and defenses have focused on an adversary that indiscriminately attacks all the records without regard to the cost of false positives or negatives. In this work, we revisit membership inference attacks from the perspective of a pragmatic adversary who carefully selects targets and make predictions conservatively. We design a new evaluation methodology that allows us to evaluate the membership privacy risk at the level of individuals and not only in aggregate. We experimentally demonstrate that highly vulnerable records exist even when the aggregate attack precision is close to 50\% (baseline). Speciﬁcally, on the MNIST dataset, our pragmatic adversary achieves a precision of 95.05\% whereas the prior attack only achieves a precision of 51.7\%.},
	language = {en},
	urldate = {2022-04-03},
	booktitle = {2020 {IEEE} {European} {Symposium} on {Security} and {Privacy} ({EuroS}\&{P})},
	publisher = {IEEE},
	author = {Long, Yunhui and Wang, Lei and Bu, Diyue and Bindschaedler, Vincent and Wang, Xiaofeng and Tang, Haixu and Gunter, Carl A. and Chen, Kai},
	month = sep,
	year = {2020},
	keywords = {Untagged},
	pages = {521--534},
}

@article{luInvariantCausalRepresentation2020,
	title = {Invariant {Causal} {Representation} {Learning}},
	url = {https://openreview.net/forum?id=K4wkUp5xNK},
	abstract = {Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either...},
	language = {en},
	urldate = {2022-05-15},
	author = {Lu, Chaochao and Wu, Yuhuai and Hernández-Lobato, José Miguel and Schölkopf, Bernhard},
	month = sep,
	year = {2020},
	keywords = {Untagged},
}

@article{luLearningVeryFew2020,
	title = {Learning from {Very} {Few} {Samples}: {A} {Survey}},
	shorttitle = {Learning from {Very} {Few} {Samples}},
	url = {http://arxiv.org/abs/2009.02653},
	abstract = {Few sample learning (FSL) is signiﬁcant and challenging in the ﬁeld of machine learning. The capability of learning and generalizing from very few samples successfully is a noticeable demarcation separating artiﬁcial intelligence and human intelligence since humans can readily establish their cognition to novelty from just a single or a handful of examples whereas machine learning algorithms typically entail hundreds or thousands of supervised samples to guarantee generalization ability. Despite the long history dated back to the early 2000s and the widespread attention in recent years with booming deep learning technologies, little surveys or reviews for FSL are available until now. In this context, we extensively review 300+ papers of FSL spanning from the 2000s to 2019 and provide a timely and comprehensive survey for FSL. In this survey, we review the evolution history as well as the current progress on FSL, categorize FSL approaches into the generative model based and discriminative model based kinds in principle, and emphasize particularly on the meta learning based FSL approaches. We also summarize several recently emerging extensional topics of FSL and review the latest advances on these topics. Furthermore, we highlight the important FSL applications covering many research hotspots in computer vision, natural language processing, audio and speech, reinforcement learning and robotic, data analysis, etc. Finally, we conclude the survey with a discussion on promising trends in the hope of providing guidance and insights to follow-up researches.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2009.02653 [cs, stat]},
	author = {Lu, Jiang and Gong, Pinghua and Ye, Jieping and Zhang, Changshui},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.02653},
	keywords = {Untagged},
}

@article{luCrossmodalityPersonReidentification2020,
	title = {Cross-modality {Person} re-identification with {Shared}-{Specific} {Feature} {Transfer}},
	url = {http://arxiv.org/abs/2002.12489},
	abstract = {Cross-modality person re-identiﬁcation (cm-ReID) is a challenging but key technology for intelligent video analysis. Existing works mainly focus on learning modalityshared representation by embedding different modalities into a same feature space, lowering the upper bound of feature distinctiveness. In this paper, we tackle the above limitation by proposing a novel cross-modality shared-speciﬁc feature transfer algorithm (termed cm-SSFT) to explore the potential of both the modality-shared information and the modality-speciﬁc characteristics.We model the afﬁnities of different modality samples according to the shared features and then transfer both shared and speciﬁc features among and across modalities. We also propose a complementary feature learning strategy including modality adaption, project adversarial learning and reconstruction enhancement to learn discriminative and complementary shared and speciﬁc features of each modality, respectively. The entire cm-SSFT algorithm can be trained in an end-to-end manner. We conducted comprehensive experiments to validate the superiority of the overall algorithm and the effectiveness of each component. The proposed algorithm significantly outperforms state-of-the-arts by 22.5\% and 19.3\% mAP on the two mainstream benchmark datasets SYSUMM01 and RegDB, respectively.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:2002.12489 [cs]},
	author = {Lu, Yan and Wu, Yue and Liu, Bin and Zhang, Tianzhu and Li, Baopu and Chu, Qi and Yu, Nenghai},
	month = mar,
	year = {2020},
	note = {arXiv: 2002.12489},
	keywords = {Untagged},
}

@article{luoSurveyDeepHashing2020,
	title = {A {Survey} on {Deep} {Hashing} {Methods}},
	url = {http://arxiv.org/abs/2003.03369},
	abstract = {Nearest neighbor search is to find the data points in the database such that the distances from them to the query are the smallest, which is a fundamental problem in various domains, such as computer vision, recommendation systems and machine learning. Hashing is one of the most widely used method for its computational and storage efficiency. With the development of deep learning, deep hashing methods show more advantages than traditional methods. In this paper, we present a comprehensive survey of the deep hashing algorithms. Based on the loss function, we categorize deep supervised hashing methods according to the manners of preserving the similarities into: pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving, as well as quantization. In addition, we also introduce some other topics such as deep unsupervised hashing and multi-modal deep hashing methods. Meanwhile, we also present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally, we discussed some potential research directions in the conclusion.},
	language = {en},
	urldate = {2021-04-09},
	journal = {arXiv:2003.03369 [cs]},
	author = {Luo, Xiao and Chen, Chong and Zhong, Huasong and Zhang, Hao and Deng, Minghua and Huang, Jianqiang and Hua, Xiansheng},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.03369},
	keywords = {Untagged},
}

@article{maTwoBirdsOne2020,
	title = {Two birds with one stone: {Transforming} and generating facial images with iterative {GAN}},
	volume = {396},
	url = {https://doi.org/10.1016/j.neucom.2018.10.093},
	doi = {10/ggbzzk},
	journal = {Neurocomputing},
	author = {Ma, Dan and Liu, Bin and Kang, Zhao and Zhou, Jiayu and Zhu, Jianke and Xu, Zenglin},
	year = {2020},
	keywords = {Untagged},
	pages = {278--290},
}

@article{maWeightNetRevisitingDesign2020,
	title = {{WeightNet}: {Revisiting} the {Design} {Space} of {Weight} {Networks}},
	shorttitle = {{WeightNet}},
	url = {http://arxiv.org/abs/2007.11823},
	abstract = {We present a conceptually simple, ﬂexible and eﬀective framework for weight generating networks. Our approach is general that uniﬁes two current distinct and extremely eﬀective SENet and CondConv into the same framework on weight space. The method, called W eightN et, generalizes the two methods by simply adding one more grouped fullyconnected layer to the attention activation layer. We use the WeightNet, composed entirely of (grouped) fully-connected layers, to directly output the convolutional weight. WeightNet is easy and memory-conserving to train, on the kernel space instead of the feature space. Because of the ﬂexibility, our method outperforms existing approaches on both ImageNet and COCO detection tasks, achieving better Accuracy-FLOPs and Accuracy-Parameter trade-oﬀs. The framework on the ﬂexible weight space has the potential to further improve the performance. Code is available at https://github.com/megvii-model/WeightNet.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2007.11823 [cs]},
	author = {Ma, Ningning and Zhang, Xiangyu and Huang, Jiawei and Sun, Jian},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.11823},
	keywords = {Untagged},
}

@inproceedings{maFunnelActivationVisual2020,
	address = {Cham},
	title = {Funnel {Activation} for {Visual} {Recognition}},
	volume = {12356},
	isbn = {978-3-030-58620-1 978-3-030-58621-8},
	url = {http://link.springer.com/10.1007/978-3-030-58621-8_21},
	doi = {10.1007/978-3-030-58621-8_21},
	abstract = {We present a conceptually simple but eﬀective funnel activation for image recognition tasks, called Funnel activation (FReLU), that extends ReLU and PReLU to a 2D activation by adding a negligible overhead of spatial condition. The forms of ReLU and PReLU are y = max(x, 0) and y = max(x, px), respectively, while FReLU is in the form of y = max(x, T(x)), where T(·) is the 2D spatial condition. Moreover, the spatial condition achieves a pixel-wise modeling capacity in a simple way, capturing complicated visual layouts with regular convolutions. We conduct experiments on ImageNet, COCO detection, and semantic segmentation tasks, showing great improvements and robustness of FReLU in the visual recognition tasks. Code is available at https://github.com/megvii-model/FunnelAct.},
	language = {en},
	urldate = {2021-03-18},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer International Publishing},
	author = {Ma, Ningning and Zhang, Xiangyu and Sun, Jian},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {351--368},
}

@inproceedings{mahmudHumanActivityRecognition2020,
	address = {Santiago de Compostela, Spain},
	series = {Frontiers in {Artificial} {Intelligence} and {Applications}},
	title = {Human {Activity} {Recognition} from {Wearable} {Sensor} {Data} {Using} {Self}-{Attention}},
	volume = {325},
	url = {https://doi.org/10.3233/FAIA200236},
	doi = {10/gkx7rb},
	booktitle = {European {Conference} on {Artificial} {Intelligence}},
	publisher = {IOS Press},
	author = {Mahmud, Saif and Tonmoy, M. Tanjid Hasan and Bhaumik, Kishor Kumar and Rahman, A. K. M. Mahbubur and Amin, M. Ashraful and Shoyaib, Mohammad and Khan, Muhammad Asif Hossain and Ali, Amin Ahsan},
	editor = {Giacomo, Giuseppe De and Catalá, Alejandro and Dilkina, Bistra and Milano, Michela and Barro, Senén and Bugarín, Alberto and Lang, Jérôme},
	year = {2020},
	keywords = {Untagged},
	pages = {1332--1339},
}

@inproceedings{maini_adversarial_2020,
	title = {Adversarial {Robustness} {Against} the {Union} of {Multiple} {Perturbation} {Models}},
	shorttitle = {Multi steepest descent, {MSD}},
	url = {http://arxiv.org/abs/1909.04068},
	doi = {10.48550/arXiv.1909.04068},
	abstract = {Owing to the susceptibility of deep learning systems to adversarial attacks, there has been a great deal of work in developing (both empirically and certifiably) robust classifiers. While most work has defended against a single type of attack, recent work has looked at defending against multiple perturbation models using simple aggregations of multiple attacks. However, these methods can be difficult to tune, and can easily result in imbalanced degrees of robustness to individual perturbation models, resulting in a sub-optimal worst-case loss over the union. In this work, we develop a natural generalization of the standard PGD-based procedure to incorporate multiple perturbation models into a single attack, by taking the worst-case over all steepest descent directions. This approach has the advantage of directly converging upon a trade-off between different perturbation models which minimizes the worst-case performance over the union. With this approach, we are able to train standard architectures which are simultaneously robust against \${\textbackslash}ell\_{\textbackslash}infty\$, \${\textbackslash}ell\_2\$, and \${\textbackslash}ell\_1\$ attacks, outperforming past approaches on the MNIST and CIFAR10 datasets and achieving adversarial accuracy of 47.0\% against the union of (\${\textbackslash}ell\_{\textbackslash}infty\$, \${\textbackslash}ell\_2\$, \${\textbackslash}ell\_1\$) perturbations with radius = (0.03, 0.5, 12) on the latter, improving upon previous approaches which achieve 40.6\% accuracy.},
	urldate = {2022-06-05},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Maini, Pratyush and Wong, Eric and Kolter, J. Zico},
	month = jul,
	year = {2020},
	note = {arXiv:1909.04068 [cs, stat]
type: article},
	keywords = {Untagged},
	pages = {6640--6650},
}

@inproceedings{martinez_minimax_2020,
	title = {Minimax {Pareto} {Fairness}: {A} {Multi} {Objective} {Perspective}},
	shorttitle = {Minimax {Pareto} {Fairness}},
	url = {https://proceedings.mlr.press/v119/martinez20a.html},
	abstract = {In this work we formulate and formally characterize group fairness as a multi-objective optimization problem, where each sensitive group risk is a separate objective. We propose a fairness criterion where a classifier achieves minimax risk and is Pareto-efficient w.r.t. all groups, avoiding unnecessary harm, and can lead to the best zero-gap model if policy dictates so. We provide a simple optimization algorithm compatible with deep neural networks to satisfy these constraints. Since our method does not require test-time access to sensitive attributes, it can be applied to reduce worst-case classification errors between outcomes in unbalanced classification problems. We test the proposed methodology on real case-studies of predicting income, ICU patient mortality, skin lesions classification, and assessing credit risk, demonstrating how our framework compares favorably to other approaches.},
	language = {en},
	urldate = {2023-05-11},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Martinez, Natalia and Bertran, Martin and Sapiro, Guillermo},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {6755--6764},
}

@inproceedings{matsuuraDomainGeneralizationUsing2020,
	title = {Domain {Generalization} {Using} a {Mixture} of {Multiple} {Latent} {Domains}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6846},
	urldate = {2022-05-15},
	booktitle = {The {Thirty}-{Fourth} {AAAI} {Conference} on {Artificial} {Intelligence}, {AAAI} 2020, {The} {Thirty}-{Second} {Innovative} {Applications} of {Artificial} {Intelligence} {Conference}, {IAAI} 2020, {The} {Tenth} {AAAI} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}, {EAAI} 2020, {New} {York}, {NY}, {USA}, {February} 7-12, 2020},
	publisher = {AAAI Press},
	author = {Matsuura, Toshihiko and Harada, Tatsuya},
	year = {2020},
	keywords = {Untagged},
	pages = {11749--11756},
}

@inproceedings{mengTrainingBinaryNeural2020,
	title = {Training {Binary} {Neural} {Networks} using the {Bayesian} {Learning} {Rule}},
	url = {https://proceedings.mlr.press/v119/meng20a.html},
	abstract = {Neural networks with binary weights are computation-efficient and hardware-friendly, but their training is challenging because it involves a discrete optimization problem. Surprisingly, ignoring the discrete nature of the problem and using gradient-based methods, such as the Straight-Through Estimator, still works well in practice. This raises the question: are there principled approaches which justify such methods? In this paper, we propose such an approach using the Bayesian learning rule. The rule, when applied to estimate a Bernoulli distribution over the binary weights, results in an algorithm which justifies some of the algorithmic choices made by the previous approaches. The algorithm not only obtains state-of-the-art performance, but also enables uncertainty estimation and continual learning to avoid catastrophic forgetting. Our work provides a principled approach for training binary neural networks which also justifies and extends existing approaches.},
	language = {en},
	urldate = {2022-02-23},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Meng, Xiangming and Bachmann, Roman and Khan, Mohammad Emtiyaz},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {6852--6861},
}

@inproceedings{menonLongtailLearningLogit2020,
	title = {Long-tail learning via logit adjustment},
	url = {https://openreview.net/forum?id=37nvvqkCo5},
	abstract = {Real-world classification problems typically exhibit an imbalanced or long-tailed label distribution, wherein many labels have only a few associated samples. This poses a challenge for...},
	language = {en},
	urldate = {2021-07-02},
	author = {Menon, Aditya Krishna and Jayasumana, Sadeep and Rawat, Ankit Singh and Jain, Himanshu and Veit, Andreas and Kumar, Sanjiv},
	month = sep,
	year = {2020},
	keywords = {Untagged},
}

@misc{miech_learning_2020,
	title = {Learning a {Text}-{Video} {Embedding} from {Incomplete} and {Heterogeneous} {Data}},
	url = {http://arxiv.org/abs/1804.02516},
	doi = {10.48550/arXiv.1804.02516},
	abstract = {Joint understanding of video and language is an active research area with many applications. Prior work in this domain typically relies on learning text-video embeddings. One difficulty with this approach, however, is the lack of large-scale annotated video-caption datasets for training. To address this issue, we aim at learning text-video embeddings from heterogeneous data sources. To this end, we propose a Mixture-of-Embedding-Experts (MEE) model with ability to handle missing input modalities during training. As a result, our framework can learn improved text-video embeddings simultaneously from image and video datasets. We also show the generalization of MEE to other input modalities such as face descriptors. We evaluate our method on the task of video retrieval and report results for the MPII Movie Description and MSR-VTT datasets. The proposed MEE model demonstrates significant improvements and outperforms previously reported methods on both text-to-video and video-to-text retrieval tasks. Code is available at: https://github.com/antoine77340/Mixture-of-Embedding-Experts},
	urldate = {2023-05-22},
	author = {Miech, Antoine and Laptev, Ivan and Sivic, Josef},
	month = jan,
	year = {2020},
	note = {arXiv:1804.02516 [cs]},
	keywords = {Untagged},
}

@misc{mildenhall_nerf_2020,
	title = {{NeRF}: {Representing} {Scenes} as {Neural} {Radiance} {Fields} for {View} {Synthesis}},
	shorttitle = {{NeRF}},
	url = {http://arxiv.org/abs/2003.08934},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	urldate = {2022-10-27},
	publisher = {arXiv},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	month = aug,
	year = {2020},
	note = {arXiv:2003.08934 [cs]},
	keywords = {Untagged},
}

@article{minCuriousCaseAdversarially2020,
	title = {The {Curious} {Case} of {Adversarially} {Robust} {Models}: {More} {Data} {Can} {Help}, {Double} {Descend}, or {Hurt} {Generalization}},
	shorttitle = {The {Curious} {Case} of {Adversarially} {Robust} {Models}},
	url = {http://arxiv.org/abs/2002.11080},
	abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classiﬁcation problems. We ﬁrst investigate the Gaussian mixture classiﬁcation with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we move to the analysis of a two-dimensional classiﬁcation problem with a 0-1 loss. We prove that more data always hurts the generalization performance of adversarially trained models with large perturbations. To complement our theoretical results, we conduct empirical studies on Gaussian mixture classiﬁcation, support vector machines (SVMs), and linear regression.},
	language = {en},
	urldate = {2022-02-23},
	journal = {arXiv:2002.11080 [cs, stat]},
	author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	month = jun,
	year = {2020},
	note = {arXiv: 2002.11080},
	keywords = {Untagged},
}

@inproceedings{minhImprovingSelfAdaptationMultiSensor2020,
	title = {Improving {Self}-{Adaptation} {For} {Multi}-{Sensor} {Activity} {Recognition} with {Active} {Learning}},
	doi = {10.1109/IJCNN48605.2020.9206873},
	abstract = {Heterogeneous domain adaptation adapts a machine learning model, here classification model, from a source domain to a target domain to leverage data from both domains. Thereby, supervised heterogeneous domain adaptation expects labeled data from the target domain, while unsupervised heterogeneous domain adaptation does not. In this article, we study the inclusion of active learning to bridge unsupervised and supervised domain adaptation. The active learning approach iteratively queries the most useful instances from the target domain, which are then labeled and used to improve the classification model. Using active learning, the selection of training instances can focus on areas where ambiguity in the source domain resolves in the target domain. Hence, we achieve the same performance with fewer labels. Experiments on real activity recognition data confirm our claims.},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	author = {Minh, Tuan Pham and Kottke, Daniel and Tsarenko, Anna and Gruhl, Christian and Sick, Bernhard},
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {Untagged},
	pages = {1--8},
}

@article{mireshghallahPrivacyDeepLearning2020,
	title = {Privacy in {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Privacy in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2004.12254},
	abstract = {The ever-growing advances of deep learning in many areas including vision, recommendation systems, natural language processing, etc., have led to the adoption of Deep Neural Networks (DNNs) in production systems. The availability of large datasets and high computational power are the main contributors to these advances. The datasets are usually crowdsourced and may contain sensitive information. This poses serious privacy concerns as this data can be misused or leaked through various vulnerabilities. Even if the cloud provider and the communication link is trusted, there are still threats of inference attacks where an attacker could speculate properties of the data used for training, or find the underlying model architecture and parameters. In this survey, we review the privacy concerns brought by deep learning, and the mitigating techniques introduced to tackle these issues. We also show that there is a gap in the literature regarding test-time inference privacy, and propose possible future research directions.},
	urldate = {2022-03-14},
	journal = {arXiv:2004.12254 [cs, stat]},
	author = {Mireshghallah, Fatemehsadat and Taram, Mohammadkazem and Vepakomma, Praneeth and Singh, Abhishek and Raskar, Ramesh and Esmaeilzadeh, Hadi},
	month = nov,
	year = {2020},
	note = {arXiv: 2004.12254},
	keywords = {Untagged},
}

@inproceedings{morris_reevaluating_2020,
	address = {Online},
	title = {Reevaluating {Adversarial} {Examples} in {Natural} {Language}},
	url = {https://aclanthology.org/2020.findings-emnlp.341},
	doi = {10.18653/v1/2020.findings-emnlp.341},
	abstract = {State-of-the-art attacks on NLP models lack a shared definition of a what constitutes a successful attack. We distill ideas from past work into a unified framework: a successful natural language adversarial example is a perturbation that fools the model and follows some linguistic constraints. We then analyze the outputs of two state-of-the-art synonym substitution attacks. We find that their perturbations often do not preserve semantics, and 38\% introduce grammatical errors. Human surveys reveal that to successfully preserve semantics, we need to significantly increase the minimum cosine similarities between the embeddings of swapped words and between the sentence encodings of original and perturbed sentences.With constraints adjusted to better preserve semantics and grammaticality, the attack success rate drops by over 70 percentage points.},
	urldate = {2022-11-08},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Morris, John and Lifland, Eli and Lanchantin, Jack and Ji, Yangfeng and Qi, Yanjun},
	month = nov,
	year = {2020},
	keywords = {Untagged},
	pages = {3829--3839},
}

@article{munroMultiModalDomainAdaptation2020,
	title = {Multi-{Modal} {Domain} {Adaptation} for {Fine}-{Grained} {Action} {Recognition}},
	url = {http://arxiv.org/abs/2001.09691},
	abstract = {Fine-grained action recognition datasets exhibit environmental bias, where multiple video sequences are captured from a limited number of environments. Training a model in one environment and deploying in another results in a drop in performance due to an unavoidable domain shift. Unsupervised Domain Adaptation (UDA) approaches have frequently utilised adversarial training between the source and target domains. However, these approaches have not explored the multi-modal nature of video within each domain. In this work we exploit the correspondence of modalities as a self-supervised alignment approach for UDA in addition to adversarial alignment (Fig. 1).},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:2001.09691 [cs]},
	author = {Munro, Jonathan and Damen, Dima},
	month = mar,
	year = {2020},
	note = {arXiv: 2001.09691},
	keywords = {Untagged},
}

@inproceedings{nizar-kobren-2020-leveraging,
	address = {Online},
	title = {Leveraging extracted model adversaries for improved black box attacks},
	url = {https://aclanthology.org/2020.blackboxnlp-1.6},
	doi = {10.18653/v1/2020.blackboxnlp-1.6},
	abstract = {We present a method for adversarial input generation against black box models for reading comprehension based question answering. Our approach is composed of two steps. First, we approximate a victim black box model via model extraction. Second, we use our own white box method to generate input perturbations that cause the approximate model to fail. These perturbed inputs are used against the victim. In experiments we find that our method improves on the efficacy of the ADDANY—a white box attack—performed on the approximate model by 25\% F1, and the ADDSENT attack—a black box attack—by 11\% F1.},
	booktitle = {Proceedings of the third {BlackboxNLP} workshop on analyzing and interpreting neural networks for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Nizar, Naveen Jafer and Kobren, Ari},
	month = nov,
	year = {2020},
	keywords = {Untagged},
	pages = {57--67},
}

@article{papyanPrevalenceNeuralCollapse2020,
	title = {Prevalence of neural collapse during the terminal phase of deep learning training},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.2015509117},
	doi = {10.1073/pnas.2015509117},
	abstract = {Modern practice for training classification deepnets involves a terminal phase of training (TPT), which begins at the epoch where training error first vanishes. During TPT, the training error stays effectively zero, while training loss is pushed toward zero. Direct measurements of TPT, for three prototypical deepnet architectures and across seven canonical classification datasets, expose a pervasive inductive bias we call
              neural collapse
              (NC), involving four deeply interconnected phenomena. (NC1) Cross-example within-class variability of last-layer training activations collapses to zero, as the individual activations themselves collapse to their class means. (NC2) The class means collapse to the vertices of a simplex equiangular tight frame (ETF). (NC3) Up to rescaling, the last-layer classifiers collapse to the class means or in other words, to the simplex ETF (i.e., to a self-dual configuration). (NC4) For a given activation, the classifier’s decision collapses to simply choosing whichever class has the closest train class mean (i.e., the nearest class center [NCC] decision rule). The symmetric and very simple geometry induced by the TPT confers important benefits, including better generalization performance, better robustness, and better interpretability.},
	language = {en},
	number = {40},
	urldate = {2022-03-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Papyan, Vardan and Han, X. Y. and Donoho, David L.},
	month = oct,
	year = {2020},
	keywords = {Untagged},
	pages = {24652--24663},
}

@inproceedings{pengLearningGraphConvolutional2020,
	address = {New York City, NY},
	title = {Learning {Graph} {Convolutional} {Network} for {Skeleton}-{Based} {Human} {Action} {Recognition} by {Neural} {Searching}},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/5652},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Peng, Wei and Hong, Xiaopeng and Chen, Haoyu and Zhao, Guoying},
	year = {2020},
	keywords = {Untagged},
	pages = {2669--2676},
}

@inproceedings{pengSuppressingMislabeledData2020,
	address = {Cham},
	title = {Suppressing {Mislabeled} {Data} via {Grouping} and {Self}-attention},
	volume = {12361},
	isbn = {978-3-030-58516-7 978-3-030-58517-4},
	url = {http://link.springer.com/10.1007/978-3-030-58517-4_46},
	doi = {10.1007/978-3-030-58517-4_46},
	abstract = {Deep networks achieve excellent results on large-scale clean data but degrade signiﬁcantly when learning from noisy labels. To suppressing the impact of mislabeled data, this paper proposes a conceptually simple yet eﬃcient training block, termed as Attentive Feature Mixup (AFM), which allows paying more attention to clean samples and less to mislabeled ones via sample interactions in small groups. Specifically, this plug-and-play AFM ﬁrst leverages a group-to-attend module to construct groups and assign attention weights for group-wise samples, and then uses a mixup module with the attention weights to interpolate massive noisy-suppressed samples. The AFM has several appealing beneﬁts for noise-robust deep learning. (i) It does not rely on any assumptions and extra clean subset. (ii) With massive interpolations, the ratio of useless samples is reduced dramatically compared to the original noisy ratio. (iii) It jointly optimizes the interpolation weights with classiﬁers, suppressing the inﬂuence of mislabeled data via low attention weights. (iv) It partially inherits the vicinal risk minimization of mixup to alleviate over-ﬁtting while improves it by sampling fewer feature-target vectors around mislabeled data from the mixup vicinal distribution. Extensive experiments demonstrate that AFM yields state-of-the-art results on two challenging real-world noisy datasets: Food101N and Clothing1M.},
	language = {en},
	urldate = {2021-03-18},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer International Publishing},
	author = {Peng, Xiaojiang and Wang, Kai and Zeng, Zhaoyang and Li, Qing and Yang, Jianfei and Qiao, Yu},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {786--802},
}

@inproceedings{phanScalableDifferentialPrivacy2020,
	title = {Scalable {Differential} {Privacy} with {Certified} {Robustness} in {Adversarial} {Learning}},
	url = {https://proceedings.mlr.press/v119/phan20a.html},
	abstract = {In this paper, we aim to develop a scalable algorithm to preserve differential privacy (DP) in adversarial learning for deep neural networks (DNNs), with certified robustness to adversarial examples. By leveraging the sequential composition theory in DP, we randomize both input and latent spaces to strengthen our certified robustness bounds. To address the trade-off among model utility, privacy loss, and robustness, we design an original adversarial objective function, based on the post-processing property in DP, to tighten the sensitivity of our model. A new stochastic batch training is proposed to apply our mechanism on large DNNs and datasets, by bypassing the vanilla iterative batch-by-batch training in DP DNNs. An end-to-end theoretical analysis and evaluations show that our mechanism notably improves the robustness and scalability of DP DNNs.},
	language = {en},
	urldate = {2021-09-28},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Phan, Hai and Thai, My T. and Hu, Han and Jin, Ruoming and Sun, Tong and Dou, Dejing},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {7683--7694},
}

@inproceedings{ponceComparativeAnalysisArtificial2020,
	title = {Comparative {Analysis} of {Artificial} {Hydrocarbon} {Networks} versus {Convolutional} {Neural} {Networks} in {Human} {Activity} {Recognition}},
	doi = {10.1109/IJCNN48605.2020.9206757},
	abstract = {Human activity recognition (HAR) has gained interest in the research communities in order to know the behavior and context of users for medical, sports performance evaluation, ambient assisted living and security applications. Recent works suggest that convolutional neural networks (CNN) are very competitive machine learning techniques for HAR. Nevertheless, CNN require many computational resources, high number of parameter tuning, and many data samples for training. In this paper, we present a comparative analysis of a novel technique, artificial hydrocarbon networks (AHN), with CNN on human activity recognition classification task. We choose to compare AHN with CNN given that it is a very well-suited machine learning technique for HAR. We show that AHN architecture is simpler to set up than CNN, it needs less hyper-parameter configuration and has a slightly better accuracy performance.},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	author = {Ponce, Hiram and Martínez-Villaseñor, Lourdes},
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {Untagged},
	pages = {1--7},
}

@inproceedings{pratoFullyQuantizedTransformer2020,
	address = {Online},
	title = {Fully {Quantized} {Transformer} for {Machine} {Translation}},
	url = {https://aclanthology.org/2020.findings-emnlp.1},
	doi = {10/gnbmkr},
	abstract = {State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing computational costs of such methods without affecting performance has been up to this point unsuccessful. To this end, we propose FullyQT: an all-inclusive quantization strategy for the Transformer. To the best of our knowledge, we are the first to show that it is possible to avoid any loss in translation quality with a fully quantized Transformer. Indeed, compared to full-precision, our 8-bit models score greater or equal BLEU on most tasks. Comparing ourselves to all previously proposed methods, we achieve state-of-the-art quantization results.},
	urldate = {2021-11-04},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Prato, Gabriele and Charlaix, Ella and Rezagholizadeh, Mehdi},
	month = nov,
	year = {2020},
	keywords = {Untagged},
	pages = {1--14},
}

@inproceedings{pustozerovaInformationLeaksFederated2020,
	address = {San Diego, CA},
	title = {Information {Leaks} in {Federated} {Learning}},
	isbn = {978-1-891562-64-8},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2020/04/diss2020-23004-paper.pdf},
	doi = {10/gnh49h},
	abstract = {With the surge in data collection and analytics, concerns are raised with regards to the privacy of the individuals represented by the data. In settings where the data is distributed over several data holders, federated learning offers an alternative to learn from the data without the need to centralize it in the ﬁrst place. This is achieved by exchanging only model parameters learned locally at each data holder. This greatly limits the amount of data to be transferred, reduces the impact of data breaches, and helps to preserve the individual’s privacy. Federated learning thus becomes a viable alternative in IoT and Edge Computing settings, especially if the data collected is sensitive.},
	language = {en},
	urldate = {2021-11-19},
	booktitle = {Proceedings 2020 {Workshop} on {Decentralized} {IoT} {Systems} and {Security}},
	publisher = {Internet Society},
	author = {Pustozerova, Anastassiya and Mayer, Rudolf},
	year = {2020},
	keywords = {Untagged},
}

@article{qianThinkingFrequencyFace2020,
	title = {Thinking in {Frequency}: {Face} {Forgery} {Detection} by {Mining} {Frequency}-aware {Clues}},
	shorttitle = {Thinking in {Frequency}},
	url = {http://arxiv.org/abs/2007.09355},
	abstract = {As realistic facial manipulation technologies have achieved remarkable progress, social concerns about potential malicious abuse of these technologies bring out an emerging research topic of face forgery detection. However, it is extremely challenging since recent advances are able to forge faces beyond the perception ability of human eyes, especially in compressed images and videos. We ﬁnd that mining forgery patterns with the awareness of frequency could be a cure, as frequency provides a complementary viewpoint where either subtle forgery artifacts or compression errors could be well described. To introduce frequency into the face forgery detection, we propose a novel Frequency in Face Forgery Network (F3-Net), taking advantages of two diﬀerent but complementary frequency-aware clues, 1) frequency-aware decomposed image components, and 2) local frequency statistics, to deeply mine the forgery patterns via our two-stream collaborative learning framework. We apply DCT as the applied frequency-domain transformation. Through comprehensive studies, we show that the proposed F3-Net signiﬁcantly outperforms competing state-of-the-art methods on all compression qualities in the challenging FaceForensics++ dataset, especially wins a big lead upon low-quality media.},
	language = {en},
	urldate = {2022-03-22},
	journal = {arXiv:2007.09355 [cs]},
	author = {Qian, Yuyang and Yin, Guojun and Sheng, Lu and Chen, Zixuan and Shao, Jing},
	month = oct,
	year = {2020},
	note = {arXiv: 2007.09355},
	keywords = {Untagged},
}

@article{qinBinaryNeuralNetworks2020,
	title = {Binary {Neural} {Networks}: {A} {Survey}},
	volume = {105},
	issn = {00313203},
	shorttitle = {Binary {Neural} {Networks}},
	url = {http://arxiv.org/abs/2004.03333},
	doi = {10/ggs3g4},
	abstract = {The binary neural network, largely saving the storage and computation, serves as a promising technique for deploying deep models on resource-limited devices. However, the binarization inevitably causes severe information loss, and even worse, its discontinuity brings difficulty to the optimization of the deep network. To address these issues, a variety of algorithms have been proposed, and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these algorithms, mainly categorized into the native solutions directly conducting binarization, and the optimized ones using techniques like minimizing the quantization error, improving the network loss function, and reducing the gradient error. We also investigate other practical aspects of binary neural networks such as the hardware-friendly design and the training tricks. Then, we give the evaluation and discussions on different tasks, including image classification, object detection and semantic segmentation. Finally, the challenges that may be faced in future research are prospected.},
	urldate = {2021-06-30},
	journal = {Pattern Recognition},
	author = {Qin, Haotong and Gong, Ruihao and Liu, Xianglong and Bai, Xiao and Song, Jingkuan and Sebe, Nicu},
	year = {2020},
	note = {arXiv: 2004.03333},
	keywords = {Untagged},
	pages = {107281},
}

@inproceedings{qinForwardBackwardInformation2020,
	title = {Forward and {Backward} {Information} {Retention} for {Accurate} {Binary} {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Qin_Forward_and_Backward_Information_Retention_for_Accurate_Binary_Neural_Networks_CVPR_2020_paper.html},
	urldate = {2021-07-09},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Qin, Haotong and Gong, Ruihao and Liu, Xianglong and Shen, Mingzhu and Wei, Ziran and Yu, Fengwei and Song, Jingkuan},
	year = {2020},
	keywords = {Untagged},
	pages = {2250--2259},
}

@article{rahimianSamplingAttacksAmplification2020,
	title = {Sampling {Attacks}: {Amplification} of {Membership} {Inference} {Attacks} by {Repeated} {Queries}},
	shorttitle = {Sampling {Attacks}},
	url = {http://arxiv.org/abs/2009.00395},
	abstract = {Machine learning models have been shown to leak information violating the privacy of their training set. We focus on membership inference attacks on machine learning models which aim to determine whether a data point was used to train the victim model. Our work consists of two sides: We introduce sampling attack, a novel membership inference technique that unlike other standard membership adversaries is able to work under severe restriction of no access to scores of the victim model. We show that a victim model that only publishes the labels is still susceptible to sampling attacks and the adversary can recover up to 100\% of its performance compared to when posterior vectors are provided. The other sides of our work includes experimental results on two recent membership inference attack models and the defenses against them. For defense, we choose differential privacy in the form of gradient perturbation during the training of the victim model as well as output perturbation at prediction time. We carry out our experiments on a wide range of datasets which allows us to better analyze the interaction between adversaries, defense mechanism and datasets. We find out that our proposed fast and easy-to-implement output perturbation technique offers good privacy protection for membership inference attacks at little impact on utility.},
	urldate = {2022-03-23},
	journal = {arXiv:2009.00395 [cs]},
	author = {Rahimian, Shadi and Orekondy, Tribhuvanesh and Fritz, Mario},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.00395},
	keywords = {Untagged},
}

@article{realAutoMLZeroEvolvingMachine2020,
	title = {{AutoML}-{Zero}: {Evolving} {Machine} {Learning} {Algorithms} {From} {Scratch}},
	shorttitle = {{AutoML}-{Zero}},
	url = {http://arxiv.org/abs/2003.03384},
	abstract = {Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made signiﬁcant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks—or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that signiﬁcantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the ﬁeld.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:2003.03384 [cs, stat]},
	author = {Real, Esteban and Liang, Chen and So, David R. and Le, Quoc V.},
	month = jun,
	year = {2020},
	note = {arXiv: 2003.03384},
	keywords = {Untagged},
}

@inproceedings{ribeiro_beyond_2020,
	address = {Online},
	title = {Beyond {Accuracy}: {Behavioral} {Testing} of {NLP} {Models} with {CheckList}},
	shorttitle = {Beyond {Accuracy}},
	url = {https://aclanthology.org/2020.acl-main.442},
	doi = {10.18653/v1/2020.acl-main.442},
	abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.},
	urldate = {2023-02-26},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
	month = jul,
	year = {2020},
	keywords = {Untagged},
	pages = {4902--4912},
}

@inproceedings{riceOverfittingAdversariallyRobust2020,
	title = {Overfitting in adversarially robust deep learning},
	abstract = {It is common practice in deep learning to use overparameterized networks and train for as long as possible; there are numerous studies that show, both theoretically and empirically, that such practices surprisingly do not unduly harm the generalization performance of the classiﬁer. In this paper, we empirically study this phenomenon in the setting of adversarially trained deep networks, which are trained to minimize the loss under worst-case adversarial perturbations. We ﬁnd that overﬁtting to the training set does in fact harm robust performance to a very large degree in adversarially robust training across multiple datasets (SVHN, CIFAR-10, CIFAR-100, and ImageNet) and perturbation models (`∞ and `2). Based upon this observed effect, we show that the performance gains of virtually all recent algorithmic improvements upon adversarial training can be matched by simply using early stopping. We also show that effects such as the double descent curve do still occur in adversarially trained models, yet fail to explain the observed overﬁtting. Finally, we study several classical and modern deep learning remedies for overﬁtting, including regularization and data augmentation, and ﬁnd that no approach in isolation improves signiﬁcantly upon the gains achieved by early stopping. All code for reproducing the experiments as well as pretrained model weights and training logs can be found at https://github.com/ locuslab/robust\_overfitting.},
	language = {en},
	author = {Rice, Leslie and Wong, Eric and Kolter, J Zico},
	year = {2020},
	keywords = {Untagged},
	pages = {12},
}

@inproceedings{rizwan_hate-speech_2020,
	address = {Online},
	title = {Hate-{Speech} and {Offensive} {Language} {Detection} in {Roman} {Urdu}},
	url = {https://aclanthology.org/2020.emnlp-main.197},
	doi = {10.18653/v1/2020.emnlp-main.197},
	abstract = {The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion. Existing research in this area, however, is mainly focused on the English language, limiting the applicability to particular demographics. Despite its prevalence, Roman Urdu (RU) lacks language resources, annotated datasets, and language models for this task. In this study, we: (1) Present a lexicon of hateful words in RU, (2) Develop an annotated dataset called RUHSOLD consisting of 10,012 tweets in RU with both coarse-grained and fine-grained labels of hate-speech and offensive language, (3) Explore the feasibility of transfer learning of five existing embedding models to RU, (4) Propose a novel deep learning architecture called CNN-gram for hate-speech and offensive language detection and compare its performance with seven current baseline approaches on RUHSOLD dataset, and (5) Train domain-specific embeddings on more than 4.7 million tweets and make them publicly available. We conclude that transfer learning is more beneficial as compared to training embedding from scratch and that the proposed model exhibits greater robustness as compared to the baselines.},
	urldate = {2022-11-08},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Rizwan, Hammad and Shakeel, Muhammad Haroon and Karim, Asim},
	month = nov,
	year = {2020},
	keywords = {Untagged},
	pages = {2512--2522},
}

@article{rongDropEdgeDeepGraph2020,
	title = {{DropEdge}: {Towards} {Deep} {Graph} {Convolutional} {Networks} on {Node} {Classification}},
	shorttitle = {{DropEdge}},
	url = {http://arxiv.org/abs/1907.10903},
	abstract = {Over-ﬁtting and over-smoothing are two main obstacles of developing deep Graph Convolutional Networks (GCNs) for node classiﬁcation. In particular, over-ﬁtting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes DropEdge, a novel and ﬂexible technique to alleviate both issues. At its core, DropEdge randomly removes a certain number of edges from the input graph at each training epoch, acting like a data augmenter and also a message passing reducer. Furthermore, we theoretically demonstrate that DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by it. More importantly, our DropEdge is a general skill that can be equipped with many other backbone models (e.g. GCN, ResGCN, GraphSAGE, and JKNet) for enhanced performance. Extensive experiments on several benchmarks verify that DropEdge consistently improves the performance on a variety of both shallow and deep GCNs. The effect of DropEdge on preventing over-smoothing is empirically visualized and validated as well. Codes are released on https://github.com/DropEdge/DropEdge.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:1907.10903 [cs, stat]},
	author = {Rong, Yu and Huang, Wenbing and Xu, Tingyang and Huang, Junzhou},
	month = mar,
	year = {2020},
	note = {arXiv: 1907.10903},
	keywords = {Untagged},
}

@inproceedings{sagawa_distributionally_2020,
	title = {Distributionally {Robust} {Neural} {Networks}},
	shorttitle = {{DRO}},
	url = {https://openreview.net/forum?id=ryxGuJrFvS},
	abstract = {Overparameterized neural networks can be distributionally robust, but only when you account for generalization.},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Sagawa, Shiori and Koh, Pang Wei and Hashimoto, Tatsunori B. and Liang, Percy},
	year = {2020},
	keywords = {Untagged},
}

@article{shafiqueRobustMachineLearning2020,
	title = {Robust {Machine} {Learning} {Systems}: {Challenges}, {Current} {Trends}, {Perspectives}, and the {Road} {Ahead}},
	volume = {37},
	issn = {2168-2356, 2168-2364},
	shorttitle = {Robust {Machine} {Learning} {Systems}},
	url = {http://arxiv.org/abs/2101.02559},
	doi = {10/gg4tzd},
	abstract = {Machine Learning (ML) techniques have been rapidly adopted by smart Cyber-Physical Systems (CPS) and Internet-of-Things (IoT) due to their powerful decision-making capabilities. However, they are vulnerable to various security and reliability threats, at both hardware and software levels, that compromise their accuracy. These threats get aggravated in emerging edge ML devices that have stringent constraints in terms of resources (e.g., compute, memory, power/energy), and that therefore cannot employ costly security and reliability measures. Security, reliability, and vulnerability mitigation techniques span from network security measures to hardware protection, with an increased interest towards formal veriﬁcation of trained ML models.},
	language = {en},
	number = {2},
	urldate = {2022-01-25},
	journal = {IEEE Design \& Test},
	author = {Shafique, Muhammad and Naseer, Mahum and Theocharides, Theocharis and Kyrkou, Christos and Mutlu, Onur and Orosa, Lois and Choi, Jungwook},
	month = apr,
	year = {2020},
	note = {arXiv: 2101.02559},
	keywords = {Untagged},
	pages = {30--57},
}

@article{shanahanExplicitlyRelationalNeural2020,
	title = {An {Explicitly} {Relational} {Neural} {Network} {Architecture}},
	url = {http://arxiv.org/abs/1905.10307},
	abstract = {With a view to bridging the gap between deep learning and symbolic AI, we present a novel endto-end neural network architecture that learns to form propositional representations with an explicitly relational structure from raw pixel data. In order to evaluate and analyse the architecture, we introduce a family of simple visual relational reasoning tasks of varying complexity. We show that the proposed architecture, when pre-trained on a curriculum of such tasks, learns to generate reusable representations that better facilitate subsequent learning on previously unseen tasks when compared to a number of baseline architectures. The workings of a successfully trained model are visualised to shed some light on how the architecture functions.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1905.10307 [cs, stat]},
	author = {Shanahan, Murray and Nikiforou, Kyriacos and Creswell, Antonia and Kaplanis, Christos and Barrett, David and Garnelo, Marta},
	month = jun,
	year = {2020},
	note = {arXiv: 1905.10307},
	keywords = {Untagged},
}

@article{shaoFineGymHierarchicalVideo2020,
	title = {{FineGym}: {A} {Hierarchical} {Video} {Dataset} for {Fine}-grained {Action} {Understanding}},
	shorttitle = {{FineGym}},
	url = {http://arxiv.org/abs/2004.06704},
	abstract = {On public benchmarks, current action recognition techniques have achieved great success. However, when used in real-world applications, e.g. sport analysis, which requires the capability of parsing an activity into phases and differentiating between subtly different actions, their performances remain far from being satisfactory. To take action recognition to a new level, we develop FineGym, a new dataset built on top of gymnastic videos. Compared to existing action recognition datasets, FineGym is distinguished in richness, quality, and diversity. In particular, it provides temporal annotations at both action and sub-action levels with a three-level semantic hierarchy. For example, a "balance beam" event will be annotated as a sequence of elementary sub-actions derived from five sets: "leap-jump-hop", "beam-turns", "flight-salto", "flight-handspring", and "dismount", where the sub-action in each set will be further annotated with finely defined class labels. This new level of granularity presents significant challenges for action recognition, e.g. how to parse the temporal structures from a coherent action, and how to distinguish between subtly different action classes. We systematically investigate representative methods on this dataset and obtain a number of interesting findings. We hope this dataset could advance research towards action understanding.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2004.06704 [cs]},
	author = {Shao, Dian and Zhao, Yue and Dai, Bo and Lin, Dahua},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.06704},
	keywords = {Untagged},
}

@article{shenInterFaceGANInterpretingDisentangled2020,
	title = {{InterFaceGAN}: {Interpreting} the {Disentangled} {Face} {Representation} {Learned} by {GANs}},
	shorttitle = {{InterFaceGAN}},
	url = {http://arxiv.org/abs/2005.09635},
	abstract = {Although Generative Adversarial Networks (GANs) have made signiﬁcant progress in face synthesis, there lacks enough understanding of what GANs have learned in the latent representation to map a random code to a photo-realistic image. In this work, we propose a framework called InterFaceGAN to interpret the disentangled face representation learned by the state-of-the-art GAN models and study the properties of the facial semantics encoded in the latent space. We ﬁrst ﬁnd that GANs learn various semantics in some linear subspaces of the latent space. After identifying these subspaces, we can realistically manipulate the corresponding facial attributes without retraining the model. We then conduct a detailed study on the correlation between different semantics and manage to better disentangle them via subspace projection, resulting in more precise control of the attribute manipulation. Besides manipulating the gender, age, expression, and presence of eyeglasses, we can even alter the face pose and ﬁx the artifacts accidentally made by GANs. Furthermore, we perform an in-depth face identity analysis and a layer-wise analysis to evaluate the editing results quantitatively. Finally, we apply our approach to real face editing by employing GAN inversion approaches and explicitly training feed-forward models based on the synthetic data established by InterFaceGAN. Extensive experimental results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable face representation.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2005.09635 [cs, eess]},
	author = {Shen, Yujun and Yang, Ceyuan and Tang, Xiaoou and Zhou, Bolei},
	month = oct,
	year = {2020},
	note = {arXiv: 2005.09635},
	keywords = {Untagged},
}

@inproceedings{shenInvertibleZeroShotRecognition2020,
	address = {Cham},
	title = {Invertible {Zero}-{Shot} {Recognition} {Flows}},
	volume = {12361},
	isbn = {978-3-030-58516-7 978-3-030-58517-4},
	url = {http://link.springer.com/10.1007/978-3-030-58517-4_36},
	doi = {10.1007/978-3-030-58517-4_36},
	abstract = {Deep generative models have been successfully applied to Zero-Shot Learning (ZSL) recently. However, the underlying drawbacks of GANs and VAEs (e.g., the hardness of training with ZSL-oriented regularizers and the limited generation quality) hinder the existing generative ZSL models from fully bypassing the seen-unseen bias. To tackle the above limitations, for the ﬁrst time, this work incorporates a new family of generative models (i.e., ﬂow-based models) into ZSL. The proposed Invertible Zero-shot Flow (IZF) learns factorized data embeddings (i.e., the semantic factors and the non-semantic ones) with the forward pass of an invertible ﬂow network, while the reverse pass generates data samples. This procedure theoretically extends conventional generative ﬂows to a factorized conditional scheme. To explicitly solve the bias problem, our model enlarges the seen-unseen distributional discrepancy based on a negative sample-based distance measurement. Notably, IZF works ﬂexibly with either a naive Bayesian classiﬁer or a held-out trainable one for zero-shot recognition. Experiments on widely-adopted ZSL benchmarks demonstrate the signiﬁcant performance gain of IZF over existing methods, in both classic and generalized settings.},
	language = {en},
	urldate = {2021-03-18},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer International Publishing},
	author = {Shen, Yuming and Qin, Jie and Huang, Lei and Liu, Li and Zhu, Fan and Shao, Ling},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {614--631},
}

@article{silvaOpportunitiesChallengesDeep2020,
	title = {Opportunities and {Challenges} in {Deep} {Learning} {Adversarial} {Robustness}: {A} {Survey}},
	shorttitle = {Opportunities and {Challenges} in {Deep} {Learning} {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/2007.00753},
	abstract = {As we seek to deploy machine learning models beyond virtual and controlled domains, it is critical to analyze not only the accuracy or the fact that it works most of the time, but if such a model is truly robust and reliable. This paper studies strategies to implement adversary robustly trained algorithms towards guaranteeing safety in machine learning algorithms. We provide a taxonomy to classify adversarial attacks and defenses, formulate the Robust Optimization problem in a min-max setting, and divide it into 3 subcategories, namely: Adversarial (re)Training, Regularization Approach, and Certiﬁed Defenses. We survey the most recent and important results in adversarial example generation, defense mechanisms with adversarial (re)Training as their main defense against perturbations. We also survey mothods that add regularization terms which change the behavior of the gradient, making it harder for attackers to achieve their objective. Alternatively, we’ve surveyed methods which formally derive certiﬁcates of robustness by exactly solving the optimization problem or by approximations using upper or lower bounds. In addition we discuss the challenges faced by most of the recent algorithms presenting future research perspectives.},
	language = {en},
	urldate = {2022-01-25},
	journal = {arXiv:2007.00753 [cs, stat]},
	author = {Silva, Samuel Henrique and Najafirad, Peyman},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.00753},
	keywords = {Untagged},
}

@inproceedings{stockTrainingQuantizationNoise2020,
	title = {Training with {Quantization} {Noise} for {Extreme} {Model} {Compression}},
	url = {https://openreview.net/forum?id=dV19Yyi1fS3},
	abstract = {We tackle the problem of producing compact models, maximizing their accuracy for a given model size. A standard solution is to train networks with Quantization Aware Training, where the weights are...},
	language = {en},
	urldate = {2021-11-04},
	author = {Stock, Pierre and Fan, Angela and Graham, Benjamin and Grave, Edouard and Gribonval, Rémi and Jegou, Herve and Joulin, Armand},
	month = sep,
	year = {2020},
	keywords = {Untagged},
}

@article{suVLBERTPretrainingGeneric2020,
	title = {{VL}-{BERT}: {Pre}-training of {Generic} {Visual}-{Linguistic} {Representations}},
	shorttitle = {{VL}-{BERT}},
	url = {http://arxiv.org/abs/1908.08530},
	abstract = {We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to ﬁt for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues and beneﬁt the downstream tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. It is worth noting that VL-BERT achieved the ﬁrst place of single model on the leaderboard of the VCR benchmark. Code is released at https://github.com/jackroos/VL-BERT.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:1908.08530 [cs]},
	author = {Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng},
	month = feb,
	year = {2020},
	note = {arXiv: 1908.08530},
	keywords = {Untagged},
}

@article{sunMiningCrossImageSemantics2020,
	title = {Mining {Cross}-{Image} {Semantics} for {Weakly} {Supervised} {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/2007.01947},
	abstract = {This paper studies the problem of learning semantic segmentation from image-level supervision only. Current popular solutions leverage object localization maps from classiﬁers as supervision signals, and struggle to make the localization maps capture more complete object content. Rather than previous eﬀorts that primarily focus on intra-image information, we address the value of cross-image semantic relations for comprehensive object pattern mining. To achieve this, two neural coattentions are incorporated into the classiﬁer to complimentarily capture cross-image semantic similarities and diﬀerences. In particular, given a pair of training images, one co-attention enforces the classiﬁer to recognize the common semantics from co-attentive objects, while the other one, called contrastive co-attention, drives the classiﬁer to identify the unshared semantics from the rest, uncommon objects. This helps the classiﬁer discover more object patterns and better ground semantics in image regions. In addition to boosting object pattern learning, the co-attention can leverage context from other related images to improve localization map inference, hence eventually beneﬁting semantic segmentation learning. More essentially, our algorithm provides a uniﬁed framework that handles well diﬀerent WSSS settings, i.e., learning WSSS with (1) precise image-level supervision only, (2) extra simple single-label data, and (3) extra noisy web data. It sets new state-of-the-arts on all these settings, demonstrating well its eﬃcacy and generalizability. Moreover, our approach ranked 1st place in the Weakly-Supervised Semantic Segmentation Track of CVPR2020 Learning from Imperfect Data Challenge.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2007.01947 [cs, eess]},
	author = {Sun, Guolei and Wang, Wenguan and Dai, Jifeng and Van Gool, Luc},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.01947},
	keywords = {Untagged},
}

@misc{sun_generic_2020,
	title = {A {Generic} {Network} {Compression} {Framework} for {Sequential} {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2004.13139},
	doi = {10.48550/arXiv.2004.13139},
	abstract = {Sequential recommender systems (SRS) have become the key technology in capturing user's dynamic interests and generating high-quality recommendations. Current state-of-the-art sequential recommender models are typically based on a sandwich-structured deep neural network, where one or more middle (hidden) layers are placed between the input embedding layer and output softmax layer. In general, these models require a large number of parameters (such as using a large embedding dimension or a deep network architecture) to obtain their optimal performance. Despite the effectiveness, at some point, further increasing model size may be harder for model deployment in resource-constraint devices, resulting in longer responding time and larger memory footprint. To resolve the issues, we propose a compressed sequential recommendation framework, termed as CpRec, where two generic model shrinking techniques are employed. Specifically, we first propose a block-wise adaptive decomposition to approximate the input and softmax matrices by exploiting the fact that items in SRS obey a long-tailed distribution. To reduce the parameters of the middle layers, we introduce three layer-wise parameter sharing schemes. We instantiate CpRec using deep convolutional neural network with dilated kernels given consideration to both recommendation accuracy and efficiency. By the extensive ablation studies, we demonstrate that the proposed CpRec can achieve up to 4\${\textbackslash}sim\$8 times compression rates in real-world SRS datasets. Meanwhile, CpRec is faster during training{\textbackslash}inference, and in most cases outperforms its uncompressed counterpart.},
	urldate = {2023-04-24},
	publisher = {arXiv},
	author = {Sun, Yang and Yuan, Fajie and Yang, Min and Wei, Guoao and Zhao, Zhou and Liu, Duo},
	month = may,
	year = {2020},
	note = {arXiv:2004.13139 [cs]},
	keywords = {Untagged},
}

@article{taySparseSinkhornAttention2020,
	title = {Sparse {Sinkhorn} {Attention}},
	url = {https://arxiv.org/abs/2002.11296v1},
	abstract = {We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.},
	language = {en},
	urldate = {2021-11-04},
	author = {Tay, Yi and Bahri, Dara and Yang, Liu and Metzler, Donald and Juan, Da-Cheng},
	month = feb,
	year = {2020},
	keywords = {Untagged},
}

@article{tayEfficientTransformersSurvey2020,
	title = {Efficient {Transformers}: {A} {Survey}},
	shorttitle = {Efficient {Transformers}},
	url = {http://arxiv.org/abs/2009.06732},
	abstract = {Transformer model architectures have garnered immense interest lately due to their eﬀectiveness across a range of domains like language, vision and reinforcement learning. In the ﬁeld of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of “X-former” models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory eﬃciency. With the aim of helping the avid researcher navigate this ﬂurry, this paper characterizes a large and thoughtful selection of recent eﬃciency-ﬂavored “X-former” models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
	language = {en},
	urldate = {2021-10-21},
	journal = {arXiv:2009.06732 [cs]},
	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.06732},
	keywords = {Untagged},
}

@inproceedings{topleAlleviatingPrivacyAttacks2020,
	title = {Alleviating {Privacy} {Attacks} via {Causal} {Learning}},
	url = {https://proceedings.mlr.press/v119/tople20a.html},
	abstract = {Machine learning models, especially deep neural networks are known to be susceptible to privacy attacks such as membership inference where an adversary can detect whether a data point was used to train a model. Such privacy risks are exacerbated when a model is used for predictions on an unseen data distribution. To alleviate privacy attacks, we demonstrate the benefit of predictive models that are based on the causal relationships between input features and the outcome. We first show that models learnt using causal structure generalize better to unseen data, especially on data from different distributions than the train distribution. Based on this generalization property, we establish a theoretical link between causality and privacy: compared to associational models, causal models provide stronger differential privacy guarantees and are more robust to membership inference attacks. Experiments on simulated Bayesian networks and the colored-MNIST dataset show that associational models exhibit upto 80\% attack accuracy under different test distributions and sample sizes whereas causal models exhibit attack accuracy close to a random guess.},
	language = {en},
	urldate = {2022-03-21},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tople, Shruti and Sharma, Amit and Nori, Aditya},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {9537--9547},
}

@book{torlattimoreBanditAlgorithms2020,
	title = {Bandit {Algorithms}},
	publisher = {Cambridge University Press},
	author = {{Tor Lattimore} and {Csaba Szepesv ́ari}},
	year = {2020},
	keywords = {Untagged},
}

@article{touvronFixingTraintestResolution2020,
	title = {Fixing the train-test resolution discrepancy: {FixEfficientNet}},
	shorttitle = {Fixing the train-test resolution discrepancy},
	url = {http://arxiv.org/abs/2003.08237},
	abstract = {This paper provides an extensive analysis of the performance of the EfﬁcientNet image classiﬁers with several recent training procedures, in particular one that corrects the discrepancy between train and test images [1]. The resulting network, called FixEfﬁcientNet, signiﬁcantly outperforms the initial architecture with the same number of parameters.},
	language = {en},
	urldate = {2021-11-04},
	journal = {arXiv:2003.08237 [cs]},
	author = {Touvron, Hugo and Vedaldi, Andrea and Douze, Matthijs and Jégou, Hervé},
	month = nov,
	year = {2020},
	note = {arXiv: 2003.08237},
	keywords = {Untagged},
}

@article{tuDeepCrossModalHashing2020,
	title = {Deep {Cross}-{Modal} {Hashing} with {Hashing} {Functions} and {Unified} {Hash} {Codes} {Jointly} {Learning}},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {{DCHUC}},
	url = {https://ieeexplore.ieee.org/document/9069300/},
	doi = {10/ghhxh8},
	urldate = {2020-11-05},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Tu, Rong-Cheng and Mao, Xian-Ling and Ma, Bing and Hu, Yong and Yan, Tan and Wei, Wei and Huang, Heyan},
	year = {2020},
	keywords = {Untagged},
	pages = {1--1},
}

@article{tuDeepCrossmodalProxy2020,
	title = {Deep {Cross}-modal {Proxy} {Hashing}},
	url = {http://arxiv.org/abs/2011.03451},
	abstract = {Due to their high retrieval efﬁciency and low storage cost for cross-modal search task, cross-modal hashing methods have attracted considerable attention. For supervised crossmodal hashing methods, how to make the learned hash codes preserve semantic structure information sufﬁciently is a key point to further enhance the retrieval performance. As far as we know, almost all supervised cross-modal hashing methods preserve semantic structure information depending on atleast-one similarity deﬁnition fully or partly, i.e., it deﬁnes two datapoints as similar ones if they share at least one common category otherwise they are dissimilar. Obviously, the at-least-one similarity misses abundant semantic structure information. To tackle this problem, in this paper, we propose a novel Deep Cross-modal Proxy Hashing, called DCPH. Speciﬁcally, DCPH ﬁrst learns a proxy hashing network to generate a discriminative proxy hash code for each category. Then, by utilizing the learned proxy hash code as supervised information, a novel M argin-Sof tM ax-like loss is proposed without deﬁning the at-least-one similarity between datapoints. By minimizing the novel M argin-Sof tM axlike loss, the learned hash codes will simultaneously preserve the cross-modal similarity and abundant semantic structure information well. Extensive experiments on two benchmark datasets show that the proposed method outperforms the state-of-the-art baselines in cross-modal retrieval task.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2011.03451 [cs]},
	author = {Tu, Rong-Cheng and Mao, Xian-Ling and Tu, Rongxin and Wei, Wei and Huang, Heyan},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.03451},
	keywords = {Untagged},
}

@inproceedings{uroojMMFTBERTMultimodalFusion2020,
	address = {Online},
	title = {{MMFT}-{BERT}: {Multimodal} {Fusion} {Transformer} with {BERT} {Encodings} for {Visual} {Question} {Answering}},
	shorttitle = {{MMFT}-{BERT}},
	url = {https://aclanthology.org/2020.findings-emnlp.417},
	doi = {10/gmxksm},
	abstract = {We present MMFT-BERT(MultiModal FusionTransformer with BERT encodings), to solve Visual Question Answering (VQA) ensuring individual and combined processing of multiple input modalities. Our approach benefits from processing multimodal data (video and text) adopting the BERT encodings individually and using a novel transformer-based fusion method to fuse them together. Our method decomposes the different sources of modalities, into different BERT instances with similar architectures, but variable weights. This achieves SOTA results on the TVQA dataset. Additionally, we provide TVQA-Visual, an isolated diagnostic subset of TVQA, which strictly requires the knowledge of visual (V) modality based on a human annotator's judgment. This set of questions helps us to study the model's behavior and the challenges TVQA poses to prevent the achievement of super human performance. Extensive experiments show the effectiveness and superiority of our method.},
	urldate = {2021-09-10},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Urooj, Aisha and Mazaheri, Amir and Da vitoria lobo, Niels and Shah, Mubarak},
	month = nov,
	year = {2020},
	keywords = {Untagged},
	pages = {4648--4660},
}

@inproceedings{vanberloFederatedUnsupervisedRepresentation2020,
	address = {New York, NY, USA},
	series = {{EdgeSys} '20},
	title = {Towards federated unsupervised representation learning},
	isbn = {978-1-4503-7132-2},
	url = {https://doi.org/10.1145/3378679.3394530},
	doi = {10.1145/3378679.3394530},
	abstract = {Making deep learning models efficient at inferring nowadays requires training with an extensive number of labeled data that are gathered in a centralized system. However, gathering labeled data is an expensive and time-consuming process, centralized systems cannot aggregate an ever-increasing amount of data and aggregating user data is raising privacy concerns. Federated learning solves data volume and privacy issues by leaving user data on devices, but is limited to use cases where labeled data can be generated from user interaction. Unsupervised representation learning reduces the amount of labeled data required for model training, but previous work is limited to centralized systems. This work introduces federated unsupervised representation learning, a novel software architecture that uses unsupervised representation learning to pre-train deep neural networks using unlabeled data in a federated setting. Pre-trained networks can be used to extract discriminative features. The features help learn a down-stream task of interest with a reduced amount of labeled data. Based on representation performance experiments with human activity detection it is recommended to pre-train with unlabeled data originating from more users performing a bigger set of activities compared to data used with the down-stream task of interest. As a result, competitive or superior performance compared to supervised deep learning is achieved.},
	urldate = {2022-03-04},
	booktitle = {Proceedings of the {Third} {ACM} {International} {Workshop} on {Edge} {Systems}, {Analytics} and {Networking}},
	publisher = {Association for Computing Machinery},
	author = {van Berlo, Bram and Saeed, Aaqib and Ozcelebi, Tanir},
	year = {2020},
	keywords = {Untagged},
	pages = {31--36},
}

@inproceedings{wangSAdamVariantAdam2020,
	address = {Addis Ababa, Ethiopia},
	title = {{SAdam}: {A} {Variant} of {Adam} for {Strongly} {Convex} {Functions}},
	url = {https://openreview.net/forum?id=rye5YaEtPr},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {OpenReview.net},
	author = {Wang, Guanghui and Lu, Shiyin and Cheng, Quan and Tu, Weiwei and Zhang, Lijun},
	year = {2020},
	keywords = {Untagged},
}

@inproceedings{wangSoftExpertReward2020,
	address = {Cham},
	title = {Soft {Expert} {Reward} {Learning} for {Vision}-and-{Language} {Navigation}},
	volume = {12354},
	isbn = {978-3-030-58544-0 978-3-030-58545-7},
	url = {https://link.springer.com/10.1007/978-3-030-58545-7_8},
	doi = {10.1007/978-3-030-58545-7_8},
	abstract = {Vision-and-Language Navigation (VLN) requires an agent to ﬁnd a speciﬁed spot in an unseen environment by following natural language instructions. Dominant methods based on supervised learning clone expert’s behaviours and thus perform better on seen environments, while showing restricted performance on unseen ones. Reinforcement Learning (RL) based models show better generalisation ability but have issues as well, requiring large amount of manual reward engineering is one of which. In this paper, we introduce a Soft Expert Reward Learning (SERL) model to overcome the reward engineering designing and generalisation problems of the VLN task. Our proposed method consists of two complementary components: Soft Expert Distillation (SED) module encourages agents to behave like an expert as much as possible, but in a soft fashion; Self Perceiving (SP) module targets at pushing the agent towards the ﬁnal destination as fast as possible. Empirically, we evaluate our model on the VLN seen, unseen and test splits and the model outperforms the state-of-the-art methods on most of the evaluation metrics.},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Wang, Hu and Wu, Qi and Shen, Chunhua},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {126--141},
}

@article{wangDeepHighResolutionRepresentation2020,
	title = {Deep {High}-{Resolution} {Representation} {Learning} for {Visual} {Recognition}},
	url = {http://arxiv.org/abs/1908.07919},
	abstract = {High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks ﬁrst encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions in series (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams in parallel; (ii) Repeatedly exchange the information across resolutions. The beneﬁt is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at https://github.com/HRNet.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:1908.07919 [cs]},
	author = {Wang, Jingdong and Sun, Ke and Cheng, Tianheng and Jiang, Borui and Deng, Chaorui and Zhao, Yang and Liu, Dong and Mu, Yadong and Tan, Mingkui and Wang, Xinggang and Liu, Wenyu and Xiao, Bin},
	month = mar,
	year = {2020},
	note = {arXiv: 1908.07919},
	keywords = {Untagged},
}

@article{wangEfficientPrivacyPreservingStochastic2020,
	title = {Efficient {Privacy}-{Preserving} {Stochastic} {Nonconvex} {Optimization}},
	url = {http://arxiv.org/abs/1910.13659},
	abstract = {While many solutions for privacy-preserving convex empirical risk minimization (ERM) have been developed, privacy-preserving nonconvex ERM remains under challenging. In this paper, we study nonconvex ERM, which takes the form of minimizing a ﬁnite-sum of nonconvex loss functions over a training set. To achieve both efﬁciency and strong privacy guarantees with efﬁciency, we propose a differentially-private stochastic gradient descent algorithm for nonconvex ERM, and provide a tight analysis of its privacy and utility guarantees, as well as its gradient complexity. We show that our proposed algorithm can substantially reduce gradient complexity while matching the best-known utility guarantee obtained by Wang et al. (2017). We extend our algorithm to the distributed setting using secure multi-party computation, and show that it is possible for a distributed algorithm to match the privacy and utility guarantees of a centralized algorithm in this setting. Our experiments on benchmark nonconvex ERM problems and real datasets demonstrate superior performance in terms of both training time and utility gains compared with previous differentially-private methods using the same privacy budgets.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1910.13659 [cs, math, stat]},
	author = {Wang, Lingxiao and Jayaraman, Bargav and Evans, David and Gu, Quanquan},
	month = oct,
	year = {2020},
	note = {arXiv: 1910.13659},
	keywords = {Untagged},
}

@article{wangLinformerSelfAttentionLinear2020,
	title = {Linformer: {Self}-{Attention} with {Linear} {Complexity}},
	shorttitle = {Linformer},
	url = {http://arxiv.org/abs/2006.04768},
	abstract = {Large transformer models have shown extraordinary success in achieving state-ofthe-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses O(n2) time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this ﬁnding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from O(n2) to O(n) in both time and space. The resulting linear transformer, the Linformer, performs on par with standard Transformer models, while being much more memory- and time-efﬁcient.},
	language = {en},
	urldate = {2021-11-04},
	journal = {arXiv:2006.04768 [cs, stat]},
	author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.04768},
	keywords = {Untagged},
}

@article{wangTaskadaptiveAsymmetricDeep2020,
	title = {Task-adaptive {Asymmetric} {Deep} {Cross}-modal {Hashing}},
	url = {http://arxiv.org/abs/2004.00197},
	abstract = {Supervised cross-modal hashing aims to embed the semantic correlations of heterogeneous modality data into the binary hash codes with discriminative semantic labels. Because of its advantages on retrieval and storage efficiency, it is widely used for solving efficient cross-modal retrieval. However, existing researches equally handle the different tasks of cross-modal retrieval, and simply learn the same couple of hash functions in a symmetric way for them. Under such circumstance, the uniqueness of different cross-modal retrieval tasks are ignored and sub-optimal performance may be brought. Motivated by this, we present a Task-adaptive Asymmetric Deep Cross-modal Hashing (TA-ADCMH) method in this paper. It can learn task-adaptive hash functions for two sub-retrieval tasks via simultaneous modality representation and asymmetric hash learning. Unlike previous cross-modal hashing approaches, our learning framework jointly optimizes semantic preserving that transforms deep features of multimedia data into binary hash codes, and the semantic regression which directly regresses query modality representation to explicit label. With our model, the binary codes can effectively preserve semantic correlations across different modalities, meanwhile, adaptively capture the query semantics. The superiority of TA-ADCMH is proved on two standard datasets from many aspects.},
	urldate = {2020-11-05},
	journal = {arXiv},
	author = {Wang, Tong and Zhu, Lei and Cheng, Zhiyong and Li, Jingjing and Zhang, Huaxiang},
	month = mar,
	year = {2020},
	note = {arXiv: 2004.00197},
	keywords = {Untagged},
}

@inproceedings{wangV2VNetVehicletoVehicleCommunication2020,
	address = {Cham},
	title = {{V2VNet}: {Vehicle}-to-{Vehicle} {Communication} for {Joint} {Perception} and {Prediction}},
	volume = {12347},
	isbn = {978-3-030-58535-8 978-3-030-58536-5},
	shorttitle = {{V2VNet}},
	url = {https://link.springer.com/10.1007/978-3-030-58536-5_36},
	abstract = {In this paper, we explore the use of vehicle-to-vehicle (V2V) communication to improve the perception and motion forecasting performance of self-driving vehicles. By intelligently aggregating the information received from multiple nearby vehicles, we can observe the same scene from diﬀerent viewpoints. This allows us to see through occlusions and detect actors at long range, where the observations are very sparse or non-existent. We also show that our approach of sending compressed deep feature map activations achieves high accuracy while satisfying communication bandwidth requirements.},
	language = {en},
	urldate = {2021-11-19},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Wang, Tsun-Hsuan and Manivasagam, Sivabalan and Liang, Ming and Yang, Bin and Zeng, Wenyuan and Urtasun, Raquel},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58536-5_36},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {605--621},
}

@article{wangTiedBlockConvolution2020,
	title = {Tied {Block} {Convolution}: {Leaner} and {Better} {CNNs} with {Shared} {Thinner} {Filters}},
	shorttitle = {Tied {Block} {Convolution}},
	url = {http://arxiv.org/abs/2009.12021},
	abstract = {Convolution is the main building block of convolutional neural networks (CNN). We observe that an optimized CNN often has highly correlated ﬁlters as the number of channels increases with depth, reducing the expressive power of feature representations. We propose Tied Block Convolution (TBC) that shares the same thinner ﬁlters over equal blocks of channels and produces multiple responses with a single ﬁlter. The concept of TBC can also be extended to group convolution and fully connected layers, and can be applied to various backbone networks and attention modules.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2009.12021 [cs]},
	author = {Wang, Xudong and Yu, Stella X.},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.12021},
	keywords = {Untagged},
}

@article{wangMultiSimilarityLossGeneral2020,
	title = {Multi-{Similarity} {Loss} with {General} {Pair} {Weighting} for {Deep} {Metric} {Learning}},
	url = {http://arxiv.org/abs/1904.06627},
	abstract = {A family of loss functions built on pair-based computation have been proposed in the literature which provide a myriad of solutions for deep metric learning. In this paper, we provide a general weighting framework for understanding recent pair-based loss functions. Our contributions are three-fold: (1) we establish a General Pair Weighting (GPW) framework, which casts the sampling problem of deep metric learning into a unified view of pair weighting through gradient analysis, providing a powerful tool for understanding recent pair-based loss functions; (2) we show that with GPW, various existing pair-based methods can be compared and discussed comprehensively, with clear differences and key limitations identified; (3) we propose a new loss called multi-similarity loss (MS loss) under the GPW, which is implemented in two iterative steps (i.e., mining and weighting). This allows it to fully consider three similarities for pair weighting, providing a more principled approach for collecting and weighting informative pairs. Finally, the proposed MS loss obtains new state-of-the-art performance on four image retrieval benchmarks, where it outperforms the most recent approaches, such as ABE{\textbackslash}cite\{Kim\_2018\_ECCV\} and HTL by a large margin: 60.6\% to 65.7\% on CUB200, and 80.9\% to 88.0\% on In-Shop Clothes Retrieval dataset at Recall@1. Code is available at https://github.com/MalongTech/research-ms-loss.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1904.06627 [cs]},
	author = {Wang, Xun and Han, Xintong and Huang, Weilin and Dong, Dengke and Scott, Matthew R.},
	month = mar,
	year = {2020},
	note = {arXiv: 1904.06627},
	keywords = {Untagged},
}

@article{wangSemiPairedAsymmetricDeep2020,
	title = {Semi-{Paired} {Asymmetric} {Deep} {Cross}-{Modal} {Hashing} {Learning}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9119427/},
	doi = {10/ghhxhq},
	urldate = {2020-11-05},
	journal = {IEEE Access},
	author = {Wang, Yi and Shen, Xiaobo and Tang, Zhenmin and Zhang, Tao and Lv, Jianyong},
	year = {2020},
	keywords = {Untagged},
	pages = {113814--113825},
}

@inproceedings{wangSearchingPrivatelyImperceptible2020,
	address = {Virtual Event / Seattle, WA},
	title = {Searching {Privately} by {Imperceptible} {Lying}: {A} {Novel} {Private} {Hashing} {Method} with {Differential} {Privacy}},
	copyright = {All rights reserved},
	url = {https://doi.org/10.1145/3394171.3413882},
	doi = {10/gj45dv},
	booktitle = {{ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Wang, Yimu and Lu, Shiyin and Zhang, Lijun},
	editor = {Chen, Chang Wen and Cucchiara, Rita and Hua, Xian-Sheng and Qi, Guo-Jun and Ricci, Elisa and Zhang, Zhengyou and Zimmermann, Roger},
	year = {2020},
	keywords = {Untagged},
	pages = {2700--2709},
}

@inproceedings{wangAdversarialDomainAdaptation2020,
	address = {Snowmass Village, CO, USA},
	title = {An {Adversarial} {Domain} {Adaptation} {Network} {For} {Cross}-{Domain} {Fine}-{Grained} {Recognition}},
	copyright = {All rights reserved},
	url = {https://doi.org/10.1109/WACV45572.2020.9093306},
	doi = {10/gj45dx},
	booktitle = {{IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision}},
	publisher = {IEEE},
	author = {Wang, Yimu and Song, Ren-Jie and Wei, Xiu-Shen and Zhang, Lijun},
	year = {2020},
	keywords = {Untagged},
	pages = {1217--1225},
}

@inproceedings{wangPiecewiseHashingDeep2020,
	address = {Nanjing, China},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Piecewise {Hashing}: {A} {Deep} {Hashing} {Method} for {Large}-{Scale} {Fine}-{Grained} {Search}},
	volume = {12306},
	copyright = {All rights reserved},
	url = {https://doi.org/10.1007/978-3-030-60639-8_36},
	doi = {10/gj45dt},
	booktitle = {Pattern {Recognition} and {Computer} {Vision}},
	publisher = {Springer},
	author = {Wang, Yimu and Wei, Xiu-Shen and Xue, Bo and Zhang, Lijun},
	editor = {Peng, Yuxin and Liu, Qingshan and Lu, Huchuan and Sun, Zhenan and Liu, Chenglin and Chen, Xilin and Zha, Hongbin and Yang, Jian},
	year = {2020},
	keywords = {Untagged},
	pages = {432--444},
}

@article{wangBATCHScalableAsymmetric2020,
	title = {{BATCH}: {A} {Scalable} {Asymmetric} {Discrete} {Cross}-{Modal} {Hashing}},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {{BATCH}},
	url = {https://ieeexplore.ieee.org/document/9001235/},
	doi = {10/gg7fwc},
	urldate = {2020-11-05},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Wang, Yongxin and Luo, Xin and Nie, Liqiang and Song, Jingkuan and Zhang, Wei and Xu, Xin-Shun},
	year = {2020},
	keywords = {Untagged},
	pages = {1--1},
}

@inproceedings{wangHierarchicalBayesianModel2020,
	address = {Yokohama, Japan},
	title = {Towards a {Hierarchical} {Bayesian} {Model} of {Multi}-{View} {Anomaly} {Detection}},
	isbn = {978-0-9992411-6-5},
	url = {https://www.ijcai.org/proceedings/2020/335},
	doi = {10/gjq36j},
	abstract = {Traditional anomaly detectors examine a single view of instances and cannot discover multi-view anomalies, i.e., instances that exhibit inconsistent behaviors across different views. To tackle the problem, several multi-view anomaly detectors have been developed recently, but they are all transductive and unsupervised thus may suffer some challenges. In this paper, we propose a novel inductive semi-supervised Bayesian multi-view anomaly detector. Speciﬁcally, we ﬁrst present a generative model for normal data. Then, we build a hierarchical Bayesian model, by ﬁrst assigning priors to all parameters and latent variables, and then assigning priors over the priors. Finally, we employ variational inference to approximate the posterior of the model and evaluate anomalous scores of multi-view instances. In the experiment, we show the proposed Bayesian detector consistently outperforms state-of-the-art counterparts across several public data sets and three well-known types of multi-view anomalies. In theory, we prove the inferred Bayesian estimator is consistent and derive a proximate sample complexity for the proposed anomaly detector.},
	language = {en},
	urldate = {2021-04-09},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Wang, Zhen and Lan, Chao},
	year = {2020},
	keywords = {Untagged},
	pages = {2420--2426},
}

@book{wenchenProceedings28thACM2020,
	title = {Proceedings of the 28th {ACM} {International} {Conference} on {Multimedia}.},
	isbn = {978-1-4503-7988-5},
	url = {https://dl.acm.org/action/showBook?doi=10.1145/3394171},
	language = {en},
	urldate = {2021-01-27},
	author = {Wen Chen, Chang},
	year = {2020},
	note = {OCLC: 1227082425},
	keywords = {Untagged},
}

@inproceedings{pmlr-v100-wong20a,
	series = {Proceedings of machine learning research},
	title = {Identifying unknown instances for autonomous driving},
	volume = {100},
	url = {https://proceedings.mlr.press/v100/wong20a.html},
	abstract = {In the past few years, we have seen great progress in perception algorithms, particular through the use of deep learning. However, most existing approaches focus on a few categories of interest, which represent only a small fraction of the potential categories that robots need to handle in the real-world. Thus, identifying objects from unknown classes remains a challenging yet crucial task. In this paper, we develop a novel open-set instance segmentation algorithm for point clouds which can segment objects from both known and unknown classes in a holistic way. Our method uses a deep convolutional neural network to project points into a category-agnostic embedding space in which they can be clustered into instances irrespective of their semantics. Experiments on two large-scale self-driving datasets validate the effectiveness of our proposed method.},
	booktitle = {Proceedings of the conference on robot learning},
	publisher = {PMLR},
	author = {Wong, Kelvin and Wang, Shenlong and Ren, Mengye and Liang, Ming and Urtasun, Raquel},
	editor = {Kaelbling, Leslie Pack and Kragic, Danica and Sugiura, Komei},
	month = nov,
	year = {2020},
	note = {tex.pdf: http://proceedings.mlr.press/v100/wong20a/wong20a.pdf},
	keywords = {Untagged},
	pages = {384--393},
}

@inproceedings{wuMultiScaleSpatialTemporalIntegration2020,
	address = {Yokohama, Japan},
	title = {Multi-{Scale} {Spatial}-{Temporal} {Integration} {Convolutional} {Tube} for {Human} {Action} {Recognition}},
	url = {https://doi.org/10.24963/ijcai.2020/105},
	doi = {10.24963/ijcai.2020/105},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {ijcai.org},
	author = {Wu, Haoze and Liu, Jiawei and Zhu, Xierong and Wang, Meng and Zha, Zheng-Jun},
	editor = {Bessiere, Christian},
	year = {2020},
	keywords = {Untagged},
	pages = {753--759},
}

@article{xiaMultiViewLearningVisionandLanguage2020,
	title = {Multi-{View} {Learning} for {Vision}-and-{Language} {Navigation}},
	url = {http://arxiv.org/abs/2003.00857},
	abstract = {Learning to navigate in a visual environment following natural language instructions is a challenging task because natural language instructions are highly variable, ambiguous, and under-specified. In this paper, we present a novel training paradigm, Learn from EveryOne (LEO), which leverages multiple instructions (as different views) for the same trajectory to resolve language ambiguity and improve generalization. By sharing parameters across instructions, our approach learns more effectively from limited training data and generalizes better in unseen environments. On the recent Room-to-Room (R2R) benchmark dataset, LEO achieves 16\% improvement (absolute) over a greedy agent as the base agent (25.3\% \${\textbackslash}rightarrow\$ 41.4\%) in Success Rate weighted by Path Length (SPL). Further, LEO is complementary to most existing models for vision-and-language navigation, allowing for easy integration with the existing techniques, leading to LEO+, which creates the new state of the art, pushing the R2R benchmark to 62\% (9\% absolute improvement).},
	urldate = {2021-11-03},
	journal = {arXiv:2003.00857 [cs]},
	author = {Xia, Qiaolin and Li, Xiujun and Li, Chunyuan and Bisk, Yonatan and Sui, Zhifang and Gao, Jianfeng and Choi, Yejin and Smith, Noah A.},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.00857},
	keywords = {Untagged},
}

@article{xiangInterpretableComplexValuedNeural2020,
	title = {Interpretable {Complex}-{Valued} {Neural} {Networks} for {Privacy} {Protection}},
	url = {http://arxiv.org/abs/1901.09546},
	abstract = {Previous studies have found that an adversary attacker can often infer unintended input information from intermediate-layer features. We study the possibility of preventing such adversarial inference, yet without too much accuracy degradation. We propose a generic method to revise the neural network to boost the challenge of inferring input attributes from features, while maintaining highly accurate outputs. In particular, the method transforms real-valued features into complex-valued ones, in which the input is hidden in a randomized phase of the transformed features. The knowledge of the phase acts like a key, with which any party can easily recover the output from the processing result, but without which the party can neither recover the output nor distinguish the original input. Preliminary experiments on various datasets and network structures have shown that our method signiﬁcantly diminishes the adversary’s ability in inferring about the input while largely preserves the resulting accuracy.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1901.09546 [cs, stat]},
	author = {Xiang, Liyao and Ma, Haotian and Zhang, Hao and Zhang, Yifan and Ren, Jie and Zhang, Quanshi},
	month = jan,
	year = {2020},
	note = {arXiv: 1901.09546},
	keywords = {Untagged},
}

@inproceedings{xieEfficientProjectionFreeOnline2020,
	title = {Efficient {Projection}-{Free} {Online} {Methods} with {Stochastic} {Recursive} {Gradient}},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/6116},
	booktitle = {The {Thirty}-{Fourth} {AAAI} {Conference} on {Artificial} {Intelligence}, {AAAI} 2020, {The} {Thirty}-{Second} {Innovative} {Applications} of {Artificial} {Intelligence} {Conference}, {IAAI} 2020, {The} {Tenth} {AAAI} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}, {EAAI} 2020, {New} {York}, {NY}, {USA}, {February} 7-12, 2020},
	publisher = {AAAI Press},
	author = {Xie, Jiahao and Shen, Zebang and Zhang, Chao and Wang, Boyu and Qian, Hui},
	year = {2020},
	keywords = {Untagged},
	pages = {6446--6453},
}

@inproceedings{vedaldi_pointcontrast_2020,
	address = {Cham},
	title = {{PointContrast}: {Unsupervised} {Pre}-training for {3D} {Point} {Cloud} {Understanding}},
	volume = {12348},
	isbn = {978-3-030-58579-2 978-3-030-58580-8},
	shorttitle = {{PointContrast}},
	url = {https://link.springer.com/10.1007/978-3-030-58580-8_34},
	abstract = {Arguably one of the top success stories of deep learning is transfer learning. The ﬁnding that pre-training a network on a rich source set (e.g., ImageNet) can help boost performance once ﬁne-tuned on a usually much smaller target set, has been instrumental to many applications in language and vision. Yet, very little is known about its usefulness in 3D point cloud understanding. We see this as an opportunity considering the eﬀort required for annotating data in 3D. In this work, we aim at facilitating research on 3D representation learning. Diﬀerent from previous works, we focus on high-level scene understanding tasks. To this end, we select a suit of diverse datasets and tasks to measure the eﬀect of unsupervised pre-training on a large source set of 3D scenes. Our ﬁndings are extremely encouraging: using a uniﬁed triplet of architecture, source dataset, and contrastive loss for pre-training, we achieve improvement over recent best results in segmentation and detection across 6 diﬀerent benchmarks for indoor and outdoor, real and synthetic datasets – demonstrating that the learned representation can generalize across domains. Furthermore, the improvement was similar to supervised pre-training, suggesting that future eﬀorts should favor scaling data collection over more detailed annotation. We hope these ﬁndings will encourage more research on unsupervised pretext task design for 3D deep learning.},
	language = {en},
	urldate = {2022-10-06},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Xie, Saining and Gu, Jiatao and Guo, Demi and Qi, Charles R. and Guibas, Leonidas and Litany, Or},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58580-8_34},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {574--591},
}

@inproceedings{xu_adversarial_2020,
	title = {Adversarial {Domain} {Adaptation} with {Domain} {Mixup}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	shorttitle = {{MixUp}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6123},
	doi = {10.1609/aaai.v34i04.6123},
	abstract = {Recent works on domain adaptation reveal the effectiveness of adversarial learning on filling the discrepancy between source and target domains. However, two common limitations exist in current adversarial-learning-based methods. First, samples from two domains alone are not sufficient to ensure domain-invariance at most part of latent space. Second, the domain discriminator involved in these methods can only judge real or fake with the guidance of hard label, while it is more reasonable to use soft scores to evaluate the generated images or features, i.e., to fully utilize the inter-domain information. In this paper, we present adversarial domain adaptation with domain mixup (DM-ADA), which guarantees domain-invariance in a more continuous latent space and guides the domain discriminator in judging samples' difference relative to source and target domains. Domain mixup is jointly conducted on pixel and feature level to improve the robustness of models. Extensive experiments prove that the proposed approach can achieve superior performance on tasks with various degrees of domain shift and data complexity.},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Xu, Minghao and Zhang, Jian and Ni, Bingbing and Li, Teng and Wang, Chengjie and Tian, Qi and Zhang, Wenjun},
	year = {2020},
	note = {Number: 04},
	keywords = {Untagged},
	pages = {6502--6509},
}

@article{xuDifferentiallyPrivateAdversarial2020,
	title = {Differentially {Private} {Adversarial} {Robustness} {Through} {Randomized} {Perturbations}},
	url = {http://arxiv.org/abs/2009.12718},
	abstract = {Deep Neural Networks, despite their great success in diverse domains, are provably sensitive to small perturbations on correctly classified examples and lead to erroneous predictions. Recently, it was proposed that this behavior can be combatted by optimizing the worst case loss function over all possible substitutions of training examples. However, this can be prone to weighing unlikely substitutions higher, limiting the accuracy gain. In this paper, we study adversarial robustness through randomized perturbations, which has two immediate advantages: (1) by ensuring that substitution likelihood is weighted by the proximity to the original word, we circumvent optimizing the worst case guarantees and achieve performance gains; and (2) the calibrated randomness imparts differentially-private model training, which additionally improves robustness against adversarial attacks on the model outputs. Our approach uses a novel density-based mechanism based on truncated Gumbel noise, which ensures training on substitutions of both rare and dense words in the vocabulary while maintaining semantic similarity for model robustness.},
	urldate = {2021-09-27},
	journal = {arXiv:2009.12718 [cs, stat]},
	author = {Xu, Nan and Feyisetan, Oluwaseyi and Aggarwal, Abhinav and Xu, Zekun and Teissier, Nathanael},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.12718},
	keywords = {Untagged},
}

@article{xuCOKECommunicationCensoredKernel2020,
	title = {{COKE}: {Communication}-{Censored} {Kernel} {Learning} for {Decentralized} {Non}-parametric {Learning}},
	shorttitle = {{COKE}},
	url = {http://arxiv.org/abs/2001.10133},
	abstract = {This paper studies the decentralized optimization and learning problem where multiple interconnected agents aim to learn an optimal decision function deﬁned over a reproducing kernel Hilbert (RKH) space by jointly minimizing a global objective function, with access to locally observed data only. As a non-parametric approach, kernel learning faces a major challenge in distributed implementation: the decision variables of local objective functions are data-dependent with diﬀerent sizes and thus cannot be optimized under the decentralized consensus framework without any raw data exchange among agents. To circumvent this major challenge and preserve data privacy, we leverage the random feature (RF) approximation approach to map the large-volume data represented in the RKH space into a smaller RF space, which facilitates the same-size parameter exchange and enables distributed agents to reach consensus on the function decided by the parameters in the RF space. For fast convergent implementation, we design an iterative algorithm for Decentralized Kernel Learning via Alternating direction method of multipliers (DKLA). Further, we develop a COmmunication-censored KErnel learning (COKE) algorithm to reduce the communication load in DKLA. To do so, we apply a communication-censoring strategy, which prevents an agent from transmitting at every iteration unless its local updates are deemed informative. Theoretical results in terms of linear convergence guarantee and generalization performance analysis of DKLA and COKE are provided. Comprehensive tests with both synthetic and real datasets are conducted to verify the communication eﬃciency and learning eﬀectiveness of COKE.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:2001.10133 [cs, stat]},
	author = {Xu, Ping and Wang, Yue and Chen, Xiang and Zhi, Tian},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.10133},
	keywords = {Untagged},
}

@inproceedings{xueNearlyOptimalRegret2020,
	address = {Yokohama, Japan},
	title = {Nearly {Optimal} {Regret} for {Stochastic} {Linear} {Bandits} with {Heavy}-{Tailed} {Payoffs}},
	copyright = {All rights reserved},
	url = {https://doi.org/10.24963/ijcai.2020/406},
	doi = {10/gj45dw},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {ijcai.org},
	author = {Xue, Bo and Wang, Guanghui and Wang, Yimu and Zhang, Lijun},
	editor = {Bessiere, Christian},
	year = {2020},
	keywords = {Untagged},
	pages = {2936--2942},
}

@inproceedings{yangLearningTextureTransformer2020,
	address = {Seattle, WA, USA},
	title = {Learning {Texture} {Transformer} {Network} for {Image} {Super}-{Resolution}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157061/},
	doi = {10.1109/CVPR42600.2020.00583},
	abstract = {We study on image super-resolution (SR), which aims to recover realistic textures from a low-resolution (LR) image. Recent progress has been made by taking high-resolution images as references (Ref), so that relevant textures can be transferred to LR images. However, existing SR approaches neglect to use attention mechanisms to transfer high-resolution (HR) textures from Ref images, which limits these approaches in challenging cases. In this paper, we propose a novel Texture Transformer Network for Image Super-Resolution (TTSR), in which the LR and Ref images are formulated as queries and keys in a transformer, respectively. TTSR consists of four closely-related modules optimized for image generation tasks, including a learnable texture extractor by DNN, a relevance embedding module, a hard-attention module for texture transfer, and a softattention module for texture synthesis. Such a design encourages joint feature learning across LR and Ref images, in which deep feature correspondences can be discovered by attention, and thus accurate texture features can be transferred. The proposed texture transformer can be further stacked in a cross-scale way, which enables texture recovery from different levels (e.g., from 1× to 4× magniﬁcation). Extensive experiments show that TTSR achieves signiﬁcant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations.},
	language = {en},
	urldate = {2021-03-18},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Yang, Fuzhi and Yang, Huan and Fu, Jianlong and Lu, Hongtao and Guo, Baining},
	year = {2020},
	keywords = {Untagged},
	pages = {5790--5799},
}

@article{yangMutualNetAdaptiveConvNet2020,
	title = {{MutualNet}: {Adaptive} {ConvNet} via {Mutual} {Learning} from {Network} {Width} and {Resolution}},
	shorttitle = {{MutualNet}},
	url = {http://arxiv.org/abs/1909.12978},
	abstract = {We propose the width-resolution mutual learning method (MutualNet) to train a network that is executable at dynamic resource constraints to achieve adaptive accuracy-efficiency trade-offs at runtime. Our method trains a cohort of sub-networks with different widths using different input resolutions to mutually learn multi-scale representations for each sub-network. It achieves consistently better ImageNet top-1 accuracy over the state-of-the-art adaptive network US-Net under different computation constraints, and outperforms the best compound scaled MobileNet in EfficientNet by 1.5\%. The superiority of our method is also validated on COCO object detection and instance segmentation as well as transfer learning. Surprisingly, the training strategy of MutualNet can also boost the performance of a single network, which substantially outperforms the powerful AutoAugmentation in both efficiency (GPU search hours: 15000 vs. 0) and accuracy (ImageNet: 77.6\% vs. 78.6\%). Code is available at {\textbackslash}url\{https://github.com/taoyang1122/MutualNet\}.},
	urldate = {2020-11-05},
	journal = {arXiv},
	author = {Yang, Taojiannan and Zhu, Sijie and Chen, Chen and Yan, Shen and Zhang, Mi and Willis, Andrew},
	month = mar,
	year = {2020},
	note = {arXiv: 1909.12978},
	keywords = {Untagged},
}

@inproceedings{yangDesignInterpretationUniversal2020,
	title = {Design and interpretation of universal adversarial patches in face detection},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Yang, Xiao and Wei, Fangyun and Zhang, Hongyang and Zhu, Jun},
	year = {2020},
	keywords = {Untagged},
	pages = {174--191},
}

@inproceedings{yangInstanceWiseDynamicSensor2020,
	title = {Instance-{Wise} {Dynamic} {Sensor} {Selection} for {Human} {Activity} {Recognition}},
	volume = {34},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5461},
	doi = {10.1609/aaai.v34i01.5461},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Yang, Xiaodong and Chen, Yiqiang and Yu, Hanchao and Zhang, Yingwei and Lu, Wang and Sun, Ruizhe},
	year = {2020},
	keywords = {Untagged},
	pages = {1104--1111},
}

@inproceedings{yangCloserLookAccuracy2020,
	title = {A closer look at accuracy vs. robustness},
	volume = {33},
	booktitle = {Advances in neural information processing systems},
	author = {Yang, Yao-Yuan and Rashtchian, Cyrus and Zhang, Hongyang and Salakhutdinov, Russ R. and Chaudhuri, Kamalika},
	year = {2020},
	keywords = {Untagged},
	pages = {8588--8601},
}

@inproceedings{yangAdversarialVulnerabilityDopplerbased2020,
	title = {Adversarial {Vulnerability} in {Doppler}-based {Human} {Activity} {Recognition}},
	doi = {10.1109/IJCNN48605.2020.9207686},
	abstract = {Human activity recognition (HAR) is an important task in many internet of things (IoT) applications. In recent years, significant efforts have been made towards achieving the highest possible recognition performance (accuracy and robustness) by using advanced machine learning techniques, including deep learning. However, to the best of our knowledge, the adversarial vulnerability of the Doppler sensor-based HAR systems has not been studied. In other domains such as computer vision, the vulnerability of deep learning algorithms to adversarial samples has attracted tremendous research interests in the past few years. In this work, we investigate the adversarial vulnerability of the Doppler-based human activity recognition system. Using a case study we demonstrate that the adversarial examples can significantly degrade the performance of the human activity recognition. Specifically, the basic iterative method (BIM) attack can reduce classification accuracy by as much as 85\%. We also discuss different types of attacks, e.g., data poisoning attacks and potential strategies of protecting the Doppler-based HAR systems against adversarial attacks.},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	author = {Yang, Zhaoyuan and Zhao, Yang and Yan, Weizhong},
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {Untagged},
	pages = {1--7},
}

@article{yangDefendingModelInversion2020,
	title = {Defending {Model} {Inversion} and {Membership} {Inference} {Attacks} via {Prediction} {Purification}},
	url = {http://arxiv.org/abs/2005.03915},
	abstract = {Neural networks are susceptible to data inference attacks such as the model inversion attack and the membership inference attack, where the attacker could infer the reconstruction and the membership of a data sample from the confidence scores predicted by the target classifier. In this paper, we propose a unified approach, namely purification framework, to defend data inference attacks. It purifies the confidence score vectors predicted by the target classifier by reducing their dispersion. The purifier can be further specialized in defending a particular attack via adversarial learning. We evaluate our approach on benchmark datasets and classifiers. We show that when the purifier is dedicated to one attack, it naturally defends the other one, which empirically demonstrates the connection between the two attacks. The purifier can effectively defend both attacks. For example, it can reduce the membership inference accuracy by up to 15\% and increase the model inversion error by a factor of up to 4. Besides, it incurs less than 0.4\% classification accuracy drop and less than 5.5\% distortion to the confidence scores.},
	urldate = {2022-03-21},
	journal = {arXiv:2005.03915 [cs]},
	author = {Yang, Ziqi and Shao, Bin and Xuan, Bohan and Chang, Ee-Chien and Zhang, Fan},
	month = aug,
	year = {2020},
	note = {arXiv: 2005.03915},
	keywords = {Untagged},
}

@article{yaoCrossIterationBatchNormalization2020,
	title = {Cross-{Iteration} {Batch} {Normalization}},
	url = {http://arxiv.org/abs/2002.05712},
	abstract = {A well-known issue of Batch Normalization is its signiﬁcantly reduced effectiveness in the case of small mini-batch sizes. When a mini-batch contains few examples, the statistics upon which the normalization is deﬁned cannot be reliably estimated from it during a training iteration. To address this problem, we present Cross-Iteration Batch Normalization (CBN), in which examples from multiple recent iterations are jointly utilized to enhance estimation quality. A challenge of computing statistics over multiple iterations is that the network activations from different iterations are not comparable to each other due to changes in network weights. We thus compensate for the network weight changes via a proposed technique based on Taylor polynomials, so that the statistics can be accurately estimated and batch normalization can be effectively applied. On object detection and image classiﬁcation with small mini-batch sizes, CBN is found to outperform the original batch normalization and a direct calculation of statistics over previous iterations without the proposed compensation technique. Code is available at https://github.com/Howal/ Cross-iterationBatchNorm.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2002.05712 [cs, stat]},
	author = {Yao, Zhuliang and Cao, Yue and Zheng, Shuxin and Huang, Gao and Lin, Stephen},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.05712},
	keywords = {Untagged},
}

@article{youLARGEBATCHOPTIMIZATION2020,
	title = {{LARGE} {BATCH} {OPTIMIZATION} {FOR} {DEEP} {LEARNING}: {TRAINING} {BERT} {IN} 76 {MINUTES}},
	abstract = {Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains RESNET on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we ﬁrst study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and RESNET-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes (Table 1). The LAMB implementation is available online1.},
	language = {en},
	author = {You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
	year = {2020},
	keywords = {Untagged},
	pages = {38},
}

@article{yuLearningDiscriminativeHashing2020,
	title = {Learning discriminative hashing codes for cross-modal retrieval based on multi-view features},
	volume = {23},
	issn = {1433-7541, 1433-755X},
	url = {http://link.springer.com/10.1007/s10044-020-00870-z},
	doi = {10/ghv3nz},
	abstract = {Hashing techniques have been applied broadly in retrieval tasks due to their low storage requirements and high speed of processing. Many hashing methods based on a single view have been extensively studied for information retrieval. However, the representation capacity of a single view is insufficient and some discriminative information is not captured, which results in limited improvement. In this paper, we employ multiple views to represent images and texts for enriching the feature information. Our framework exploits the complementary information among multiple views to better learn the discriminative compact hash codes. A discrete hashing learning framework that jointly performs classifier learning and subspace learning is proposed to complete multiple search tasks simultaneously. Our framework includes two stages, namely a kernelization process and a quantization process. Kernelization aims to find a common subspace where multi-view features can be fused. The quantization stage is designed to learn discriminative unified hashing codes. Extensive experiments are performed on single-label datasets (WiKi and MMED) and multi-label datasets (MIRFlickr and NUS-WIDE), and the experimental results indicate the superiority of our method compared with the state-of-the-art methods.},
	language = {en},
	number = {3},
	urldate = {2021-01-27},
	journal = {Pattern Analysis and Applications},
	author = {Yu, Jun and Wu, Xiao-Jun and Kittler, Josef},
	month = aug,
	year = {2020},
	keywords = {Untagged},
	pages = {1421--1438},
}

@article{yuanHSResNetHierarchicalSplitBlock2020,
	title = {{HS}-{ResNet}: {Hierarchical}-{Split} {Block} on {Convolutional} {Neural} {Network}},
	shorttitle = {{HS}-{ResNet}},
	url = {http://arxiv.org/abs/2010.07621},
	abstract = {This paper addresses representational block named Hierarchical-Split Block, which can be taken as a plug-and-play block to upgrade existing convolutional neural networks, improves model performance significantly in a network. Hierarchical-Split Block contains many hierarchical split and concatenate connections within one single residual block. We find multi-scale features is of great importance for numerous vision tasks. Moreover, Hierarchical-Split block is very flexible and efficient, which provides a large space of potential network architectures for different applications. In this work, we present a common backbone based on Hierarchical-Split block for tasks: image classification, object detection, instance segmentation and semantic image segmentation/parsing. Our approach shows significant improvements over all these core tasks in comparison with the baseline. As shown in Figure1, for image classification, our 50-layers network(HS-ResNet50) achieves 81.28\% top-1 accuracy with competitive latency on ImageNet-1k dataset. It also outperforms most state-of-the-art models. The source code and models will be available on: https://github.com/PaddlePaddle/PaddleClas},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2010.07621 [cs]},
	author = {Yuan, Pengcheng and Lin, Shufei and Cui, Cheng and Du, Yuning and Guo, Ruoyu and He, Dongliang and Ding, Errui and Han, Shumin},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.07621},
	keywords = {Untagged},
}

@article{yurtseverSurveyAutonomousDriving2020,
	title = {A {Survey} of {Autonomous} {Driving}: {Common} {Practices} and {Emerging} {Technologies}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {A {Survey} of {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/1906.05113},
	doi = {10.1109/ACCESS.2020.2983149},
	abstract = {Automated driving systems (ADSs) promise a safe, comfortable and efﬁcient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, highlevel system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many stateof-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.},
	language = {en},
	urldate = {2021-03-18},
	journal = {IEEE Access},
	author = {Yurtsever, Ekim and Lambert, Jacob and Carballo, Alexander and Takeda, Kazuya},
	year = {2020},
	note = {arXiv: 1906.05113},
	keywords = {Untagged},
	pages = {58443--58469},
}

@inproceedings{zang_word-level_2020,
	address = {Online},
	title = {Word-level {Textual} {Adversarial} {Attacking} as {Combinatorial} {Optimization}},
	url = {https://aclanthology.org/2020.acl-main.540},
	doi = {10.18653/v1/2020.acl-main.540},
	abstract = {Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets. Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.},
	urldate = {2023-02-26},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zang, Yuan and Qi, Fanchao and Yang, Chenghao and Liu, Zhiyuan and Zhang, Meng and Liu, Qun and Sun, Maosong},
	month = jul,
	year = {2020},
	keywords = {Untagged},
	pages = {6066--6080},
}

@article{zhaiMACERATTACKFREESCALABLE2020,
	title = {{MACER}: {ATTACK}-{FREE} {AND} {SCALABLE} {ROBUST} {TRAINING} {VIA} {MAXIMIZING} {CERTIFIED} {RADIUS}},
	abstract = {Adversarial training is one of the most popular ways to learn robust models but is usually attack-dependent and time costly. In this paper, we propose the MACER algorithm, which learns robust models without using adversarial training but performs better than all existing provable l2-defenses. Recent work (Cohen et al., 2019) shows that randomized smoothing can be used to provide a certiﬁed l2 radius to smoothed classiﬁers, and our algorithm trains provably robust smoothed classiﬁers via MAximizing the CErtiﬁed Radius (MACER). The attack-free characteristic makes MACER faster to train and easier to optimize. In our experiments, we show that our method can be applied to modern deep neural networks on a wide range of datasets, including Cifar-10, ImageNet, MNIST, and SVHN. For all tasks, MACER spends less training time than state-of-the-art adversarial training algorithms, and the learned models achieve larger average certiﬁed radii. Our code is available at https://github.com/RuntianZ/macer.},
	language = {en},
	author = {Zhai, Runtian and Dan, Chen and He, Di and Zhang, Huan and Gong, Boqing and Ravikumar, Pradeep and Hsieh, Cho-Jui and Wang, Liwei},
	year = {2020},
	keywords = {Untagged},
	pages = {20},
}

@article{zhangFeaturePyramidTransformer2020,
	title = {Feature {Pyramid} {Transformer}},
	url = {http://arxiv.org/abs/2007.09451},
	abstract = {Feature interactions across space and scales underpin modern visual recognition systems because they introduce beneﬁcial visual contexts. Conventionally, spatial contexts are passively hidden in the CNN’s increasing receptive ﬁelds or actively encoded by non-local convolution. Yet, the non-local spatial interactions are not across scales, and thus they fail to capture the non-local contexts of objects (or parts) residing in diﬀerent scales. To this end, we propose a fully active feature interaction across both space and scales, called Feature Pyramid Transformer (FPT). It transforms any feature pyramid into another feature pyramid of the same size but with richer contexts, by using three specially designed transformers in self-level, top-down, and bottom-up interaction fashion. FPT serves as a generic visual backbone with fair computational overhead. We conduct extensive experiments in both instance-level (i.e., object detection and instance segmentation) and pixel-level segmentation tasks, using various backbones and head networks, and observe consistent improvement over all the baselines and the state-of-the-art methods1.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:2007.09451 [cs]},
	author = {Zhang, Dong and Zhang, Hanwang and Tang, Jinhui and Wang, Meng and Hua, Xiansheng and Sun, Qianru},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.09451},
	keywords = {Untagged},
}

@article{zhangFineGrainedAgeEstimation2020,
	title = {Fine-{Grained} {Age} {Estimation} in the {Wild} {With} {Attention} {LSTM} {Networks}},
	volume = {30},
	url = {https://doi.org/10.1109/TCSVT.2019.2936410},
	doi = {10/ghwnhd},
	number = {9},
	journal = {IEEE Trans. Circuits Syst. Video Technol.},
	author = {Zhang, Ke and Liu, Na and Yuan, Xingfang and Guo, Xinyao and Gao, Ce and Zhao, Zhenbing and Ma, Zhanyu},
	year = {2020},
	keywords = {Untagged},
	pages = {3140--3152},
}

@article{zhangBifurcationSpikingNeural2020,
	title = {Bifurcation {Spiking} {Neural} {Network}},
	url = {http://arxiv.org/abs/1909.08341},
	abstract = {Spiking neural networks (SNNs) has attracted much attention due to its great potential of modeling time-dependent signals. The firing rate of spiking neurons is decided by control rate which is fixed manually in advance, and thus, whether the firing rate is adequate for modeling actual time series relies on fortune. Though it is demanded to have an adaptive control rate, it is a non-trivial task because the control rate and the connection weights learned during the training process are usually entangled. In this paper, we show that the firing rate is related to the eigenvalue of the spike generation function. Inspired by this insight, by enabling the spike generation function to have adaptable eigenvalues rather than parametric control rates, we develop the Bifurcation Spiking Neural Network (BSNN), which has an adaptive firing rate and is insensitive to the setting of control rates. Experiments validate the effectiveness of BSNN on a broad range of tasks, showing that BSNN achieves superior performance to existing SNNs and is robust to the setting of control rates.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1909.08341 [cs, q-bio]},
	author = {Zhang, Shao-Qun and Zhang, Zhao-Yu and Zhou, Zhi-Hua},
	month = feb,
	year = {2020},
	note = {arXiv: 1909.08341},
	keywords = {Untagged},
}

@article{zhang_adversarial_2020,
	title = {Adversarial {Attacks} on {Deep}-learning {Models} in {Natural} {Language} {Processing}: {A} {Survey}},
	volume = {11},
	issn = {2157-6904},
	shorttitle = {Adversarial {Attacks} on {Deep}-learning {Models} in {Natural} {Language} {Processing}},
	url = {https://doi.org/10.1145/3374217},
	doi = {10.1145/3374217},
	abstract = {With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs are vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations, but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples against DNNs in Computer Vision (CV), research efforts on attacking DNNs for Natural Language Processing (NLP) applications have emerged in recent years. However, the intrinsic difference between image (CV) and text (NLP) renders challenges to directly apply attacking methods in CV to NLP. Various methods are proposed addressing this difference and attack a wide range of NLP applications. In this article, we present a systematic survey on these works. We collect all related academic works since the first appearance in 2017. We then select, summarize, discuss, and analyze 40 representative works in a comprehensive way. To make the article self-contained, we cover preliminary knowledge of NLP and discuss related seminal works in computer vision. We conclude our survey with a discussion on open issues to bridge the gap between the existing progress and more robust adversarial attacks on NLP DNNs.},
	number = {3},
	urldate = {2022-12-04},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Zhang, Wei Emma and Sheng, Quan Z. and Alhazmi, Ahoud and Li, Chenliang},
	month = apr,
	year = {2020},
	keywords = {Untagged},
	pages = {24:1--24:41},
}

@article{zhangOceanObjectawareAnchorfree2020,
	title = {Ocean: {Object}-aware {Anchor}-free {Tracking}},
	shorttitle = {Ocean},
	url = {http://arxiv.org/abs/2006.10721},
	abstract = {Anchor-based Siamese trackers have achieved remarkable advancements in accuracy, yet the further improvement is restricted by the lagged tracking robustness. We ﬁnd the underlying reason is that the regression network in anchor-based methods is only trained on the positive anchor boxes (i.e., IoU ≥ 0.6). This mechanism makes it diﬃcult to reﬁne the anchors whose overlap with the target objects are small. In this paper, we propose a novel object-aware anchor-free network to address this issue. First, instead of reﬁning the reference anchor boxes, we directly predict the position and scale of target objects in an anchor-free fashion. Since each pixel in groundtruth boxes is well trained, the tracker is capable of rectifying inexact predictions of target objects during inference. Second, we introduce a feature alignment module to learn an object-aware feature from predicted bounding boxes. The object-aware feature can further contribute to the classiﬁcation of target objects and background. Moreover, we present a novel tracking framework based on the anchor-free model. The experiments show that our anchor-free tracker achieves state-of-the-art performance on ﬁve benchmarks, including VOT-2018, VOT-2019, OTB-100, GOT-10k and LaSOT. The source code is available at https://github.com/researchmm/TracKit.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2006.10721 [cs]},
	author = {Zhang, Zhipeng and Peng, Houwen and Fu, Jianlong and Li, Bing and Hu, Weiming},
	month = jul,
	year = {2020},
	note = {arXiv: 2006.10721},
	keywords = {Untagged},
}

@inproceedings{zhang_revisiting_2020,
	address = {Online},
	title = {Revisiting {Representation} {Degeneration} {Problem} in {Language} {Modeling}},
	url = {https://aclanthology.org/2020.findings-emnlp.46},
	doi = {10.18653/v1/2020.findings-emnlp.46},
	abstract = {Weight tying is now a common setting in many language generation tasks such as language modeling and machine translation. However, a recent study reveals that there is a potential flaw in weight tying. They find that the learned word embeddings are likely to degenerate and lie in a narrow cone when training a language model. They call it the representation degeneration problem and propose a cosine regularization to solve it. Nevertheless, we prove that the cosine regularization is insufficient to solve the problem, as the degeneration is still likely to happen under certain conditions. In this paper, we revisit the representation degeneration problem and theoretically analyze the limitations of the previously proposed solution. Afterward, we propose an alternative regularization method called Laplacian regularization to tackle the problem. Experiments on language modeling demonstrate the effectiveness of the proposed Laplacian regularization.},
	urldate = {2023-04-16},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Zhong and Gao, Chongming and Xu, Cong and Miao, Rui and Yang, Qinli and Shao, Junming},
	month = nov,
	year = {2020},
	keywords = {Untagged},
	pages = {518--527},
}

@article{zhaoDeepResidualShrinkage2020,
	title = {Deep {Residual} {Shrinkage} {Networks} for {Fault} {Diagnosis}},
	volume = {16},
	issn = {1551-3203, 1941-0050},
	url = {https://ieeexplore.ieee.org/document/8850096/},
	doi = {10/ggx5mf},
	abstract = {This paper develops new deep learning methods, namely, deep residual shrinkage networks, to improve the feature learning ability from highly noised vibration signals and achieve a high fault diagnosing accuracy. Soft thresholding is inserted as nonlinear transformation layers into the deep architectures to eliminate unimportant features. Moreover, considering that it is generally challenging to set proper values for the thresholds, the developed deep residual shrinkage networks integrate a few specialized neural networks as trainable modules to automatically determine the thresholds, so that professional expertise on signal processing is not required. The efficacy of the developed methods is validated through experiments with various types of noise.},
	language = {en},
	number = {7},
	urldate = {2021-01-27},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Zhao, Minghang and Zhong, Shisheng and Fu, Xuyun and Tang, Baoping and Pecht, Michael},
	month = jul,
	year = {2020},
	keywords = {Untagged},
	pages = {4681--4690},
}

@inproceedings{zhengLocallyDifferentiallyPrivate2020,
	address = {Vitural},
	title = {Locally {Differentially} {Private} ({Contextual}) {Bandits} {Learning}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/908c9a564a86426585b29f5335b619bc-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zheng, Kai and Cai, Tianle and Huang, Weiran and Li, Zhenguo and Wang, Liwei},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
	keywords = {Untagged},
}

@article{zhouLearningGeneralisableOmniScale2020,
	title = {Learning {Generalisable} {Omni}-{Scale} {Representations} for {Person} {Re}-{Identification}},
	url = {http://arxiv.org/abs/1910.06827},
	abstract = {An effective person re-identiﬁcation (re-ID) model should learn feature representations that are both discriminative, for distinguishing similar-looking people, and generalisable, for deployment across datasets without any adaptation. In this paper, we develop novel CNN architectures to address both challenges. First, we present a re-ID CNN termed omni-scale network (OSNet) to learn features that not only capture different spatial scales but also encapsulate a synergistic combination of multiple scales, namely omni-scale features. The basic building block consists of multiple convolutional streams, each detecting features at a certain scale. For omni-scale feature learning, a uniﬁed aggregation gate is introduced to dynamically fuse multi-scale features with channel-wise weights. OSNet is lightweight as its building blocks comprise factorised convolutions. Second, to improve generalisable feature learning, we introduce instance normalisation (IN) layers into OSNet to cope with cross-dataset discrepancies. Further, to determine the optimal placements of these IN layers in the architecture, we formulate an efﬁcient differentiable architecture search algorithm. Extensive experiments show that, in the conventional same-dataset setting, OSNet achieves state-of-the-art performance, despite being much smaller than existing re-ID models. In the more challenging yet practical cross-dataset setting, OSNet beats most recent unsupervised domain adaptation methods without requiring any target data for model adaptation. Our code and models are released at https://github.com/KaiyangZhou/deep-person-reid.},
	language = {en},
	urldate = {2021-01-27},
	journal = {arXiv:1910.06827 [cs]},
	author = {Zhou, Kaiyang and Yang, Yongxin and Cavallaro, Andrea and Xiang, Tao},
	month = mar,
	year = {2020},
	note = {arXiv: 1910.06827},
	keywords = {Untagged},
}

@article{zhouCrossScaleInternalGraph2020,
	title = {Cross-{Scale} {Internal} {Graph} {Neural} {Network} for {Image} {Super}-{Resolution}},
	url = {http://arxiv.org/abs/2006.16673},
	abstract = {Non-local self-similarity in natural images has been well studied as an effective prior in image restoration. However, for single image super-resolution (SISR), most existing deep non-local methods (e.g., non-local neural networks) only exploit similar patches within the same scale of the low-resolution (LR) input image. Consequently, the restoration is limited to using the same-scale information while neglecting potential high-resolution (HR) cues from other scales. In this paper, we explore the cross-scale patch recurrence property of a natural image, i.e., similar patches tend to recur many times across different scales. This is achieved using a novel cross-scale internal graph neural network (IGNN). Speciﬁcally, we dynamically construct a cross-scale graph by searching k-nearest neighboring patches in the downsampled LR image for each query patch in the LR image. We then obtain the corresponding k HR neighboring patches in the LR image and aggregate them adaptively in accordance to the edge label of the constructed graph. In this way, the HR information can be passed from k HR neighboring patches to the LR query patch to help it recover more detailed textures. Besides, these internal image-speciﬁc LR/HR exemplars are also signiﬁcant complements to the external information learned from the training dataset. Extensive experiments demonstrate the effectiveness of IGNN against the state-of-the-art SISR methods including existing non-local networks on standard benchmarks.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2006.16673 [cs]},
	author = {Zhou, Shangchen and Zhang, Jiawei and Zuo, Wangmeng and Loy, Chen Change},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.16673},
	keywords = {Untagged},
}

@article{zhouGraphConvolutionalNetwork2020,
	title = {Graph {Convolutional} {Network} {Hashing}},
	volume = {50},
	issn = {2168-2267, 2168-2275},
	url = {https://ieeexplore.ieee.org/document/8576642/},
	doi = {10/ghv45s},
	abstract = {Recently, graph-based hashing that learns similarity-preserving binary codes via an afﬁnity graph has been extensively studied for large-scale image retrieval. However, most graph-based hashing methods resort to intractable binary quadratic programs, making them unscalable to massive data. In this paper, we propose a novel graph convolutional network-based hashing framework, dubbed GCNH, which directly carries out spectral convolution operations on both an image set and an afﬁnity graph built over the set, naturally yielding similarity-preserving binary embedding. GCNH fundamentally differs from conventional graph hashing methods which adopt an afﬁnity graph as the only learning guidance in an objective function to pursue the binary embedding. As the core ingredient of GCNH, we introduce an intuitive asymmetric graph convolutional (AGC) layer to simultaneously convolve the anchor graph, input data, and convolutional ﬁlters. By virtue of the AGC layer, GCNH well addresses the issues of scalability and out-of-sample extension when leveraging afﬁnity graphs for hashing. As a use case of our GCNH, we particularly study the semisupervised hashing scenario in this paper. Comprehensive image retrieval evaluations on the CIFAR-10, NUS-WIDE, and ImageNet datasets demonstrate the consistent advantages of GCNH over the state-of-the-art methods given limited labeled data.},
	language = {en},
	number = {4},
	urldate = {2021-01-27},
	journal = {IEEE Transactions on Cybernetics},
	author = {Zhou, Xiang and Shen, Fumin and Liu, Li and Liu, Wei and Nie, Liqiang and Yang, Yang and Shen, Heng Tao},
	month = apr,
	year = {2020},
	keywords = {Untagged},
	pages = {1460--1472},
}

@inproceedings{zhu_actbert_2020,
	address = {Seattle, WA, USA},
	title = {{ActBERT}: {Learning} {Global}-{Local} {Video}-{Text} {Representations}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{ActBERT}},
	url = {https://ieeexplore.ieee.org/document/9156847/},
	doi = {10.1109/CVPR42600.2020.00877},
	abstract = {In this paper, we introduce ActBERT for self-supervised learning of joint video-text representations from unlabeled data. First, we leverage global action information to catalyze mutual interactions between linguistic texts and local regional objects. It uncovers global and local visual clues from paired video sequences and text descriptions for detailed visual and text relation modeling. Second, we introduce a TaNgled Transformer block (TNT) to encode three sources of information, i.e., global actions, local regional objects, and linguistic descriptions. Global-local correspondences are discovered via judicious clues extraction from contextual information. It enforces the joint video-text representation to be aware of ﬁne-grained objects as well as global human intention. We validate the generalization capability of ActBERT on downstream video-and-language tasks, i.e., text-video clip retrieval, video captioning, video question answering, action segmentation, and action step localization. ActBERT signiﬁcantly outperforms the stateof-the-art, demonstrating its superiority in video-text representation learning.},
	language = {en},
	urldate = {2023-05-30},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhu, Linchao and Yang, Yi},
	month = jun,
	year = {2020},
	keywords = {Untagged},
	pages = {8743--8752},
}

@article{zhuangAdaBeliefOptimizerAdapting2020,
	title = {{AdaBelief} {Optimizer}: {Adapting} {Stepsizes} by the {Belief} in {Observed} {Gradients}},
	shorttitle = {{AdaBelief} {Optimizer}},
	url = {http://arxiv.org/abs/2010.07468},
	abstract = {Most popular optimizers for deep learning can be broadly categorized as adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient descent (SGD) with momentum). For many models such as convolutional neural networks (CNNs), adaptive methods typically converge faster but generalize worse compared to SGD; for complex settings such as generative adversarial networks (GANs), adaptive methods are typically the default because of their stability.We propose AdaBelief to simultaneously achieve three goals: fast convergence as in adaptive methods, good generalization as in SGD, and training stability. The intuition for AdaBelief is to adapt the stepsize according to the "belief" in the current gradient direction. Viewing the exponential moving average (EMA) of the noisy gradient as the prediction of the gradient at the next time step, if the observed gradient greatly deviates from the prediction, we distrust the current observation and take a small step; if the observed gradient is close to the prediction, we trust it and take a large step. We validate AdaBelief in extensive experiments, showing that it outperforms other methods with fast convergence and high accuracy on image classification and language modeling. Specifically, on ImageNet, AdaBelief achieves comparable accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief demonstrates high stability and improves the quality of generated samples compared to a well-tuned Adam optimizer. Code is available at https://github.com/juntang-zhuang/Adabelief-Optimizer},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2010.07468 [cs, stat]},
	author = {Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James S.},
	month = dec,
	year = {2020},
	note = {arXiv: 2010.07468},
	keywords = {Untagged},
}

@inproceedings{zhuPrivatekNNPracticalDifferential2020,
	address = {Seattle, WA, USA},
	title = {Private-{kNN}: {Practical} {Differential} {Privacy} for {Computer} {Vision}},
	isbn = {978-1-72817-168-5},
	shorttitle = {Private-{kNN}},
	url = {https://ieeexplore.ieee.org/document/9156598/},
	doi = {10/gg99wj},
	language = {en},
	urldate = {2021-01-27},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhu, Yuqing and Yu, Xiang and Chandraker, Manmohan and Wang, Yu-Xiang},
	month = jun,
	year = {2020},
	keywords = {Untagged},
	pages = {11851--11859},
}

@inproceedings{amraniPersonalizedModelsHuman2021,
	title = {Personalized {Models} in {Human} {Activity} {Recognition} using {Deep} {Learning}},
	doi = {10/gkzww7},
	abstract = {Current sensor-based human activity recognition techniques that rely on a user-independent model struggle to generalize to new users and on to changes that a person may make over time to his or her way of carrying out activities. Incremental learning is a technique that allows to obtain personalized models which may improve the performance on the classifiers thanks to a continuous learning based on user data. Finally, deep learning techniques have been proven to be more effective with respect to traditional ones in the generation of user-independent models. The aim of our work is therefore to put together deep learning techniques with incremental learning in order to obtain personalized models that perform better with respect to user-independent model and personalized model obtained using traditional machine learning techniques. The experimentation was done by comparing the results obtained by a technique in the state of the art with those obtained by two neural networks (ResNet and a simplified CNN) on three datasets. The experimentation showed that neural networks adapt faster to a new user than the baseline.},
	booktitle = {International {Conference} on {Pattern} {Recognition}},
	author = {Amrani, Hamza and Micucci, Daniela and Napoletano, Paolo},
	year = {2021},
	note = {ISSN: 1051-4651},
	keywords = {Untagged},
	pages = {9682--9688},
}

@article{abdel-salamHumanActivityRecognition2021,
	title = {Human {Activity} {Recognition} using {Wearable} {Sensors}: {Review}, {Challenges}, {Evaluation} {Benchmark}},
	shorttitle = {Human {Activity} {Recognition} using {Wearable} {Sensors}},
	url = {http://arxiv.org/abs/2101.01665},
	abstract = {Recognizing human activity plays a signiﬁcant role in the advancements of human-interaction applications in healthcare, personal ﬁtness, and smart devices. Many papers presented various techniques for human activity representation that resulted in distinguishable progress. In this study, we conduct an extensive literature review on recent, top-performing techniques in human activity recognition based on wearable sensors. Due to the lack of standardized evaluation and to assess and ensure a fair comparison between the state-of-the-art techniques, we applied a standardized evaluation benchmark on the state-of-the-art techniques using six publicly available data-sets: MHealth, USCHAD, UTDMHAD, WISDM, WHARF, and OPPORTUNITY. Also, we propose an experimental, improved approach that is a hybrid of enhanced handcrafted features and a neural network architecture which outperformed top-performing techniques with the same standardized evaluation benchmark applied concerning MHealth, USCHAD, UTD-MHAD data-sets.},
	language = {en},
	urldate = {2021-04-09},
	journal = {arXiv:2101.01665 [cs]},
	author = {Abdel-Salam, Reem and Mostafa, Rana and Hadhood, Mayada},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.01665},
	keywords = {Untagged},
}

@article{MultiagentCommunicationBased2020,
	title = {Multi-agent {Communication} based on {Shared} {Attention}},
	abstract = {How to deal with social relationship is a huge challenge not only for human. The Multi-Agent System (MAS) suffers from social dilemmas as well. Social dilemmas mean when an environment is affected by joint actions of multiple agents and returns different rewards back to each agent, self-interest agents is likely to ignore even destroy long-term collective-interest on the behalf of itself. Besides the social dilemmas, the increasing dynamic of environment is another problem, bringing great challenges to the Multi-Agent Reinforcement Learning (MARL) . Therefore, MARL is devoted to obtaining long-term rewards in spite of the awful dynamic. Early works take stability from game theory, but they are not suitable for complex environment. Although recent works share similarity in a deep, opponent sensitive kind of frameworks, it is still far from constructing effective communication between each other. We synthesize Shared Attention and Reused Attention module , to enhance efficiency of communication between agents. The Shared Attention helps building shared intention, while the Reused attention helps extracting information on their opponents. We evaluate the proposed modules in several MAS environments. Experimental results prove that Q network equipped with the proposed modules can balance self-interest and collective-interest, learn stably and achieve high scores of MAS tasks.},
	language = {en},
	journal = {New Zealand},
	year = {2020},
	keywords = {Untagged},
	pages = {9},
}

@inproceedings{pmlr-v139-asi21a,
	series = {Proceedings of machine learning research},
	title = {Private adaptive gradient methods for convex optimization},
	volume = {139},
	url = {http://proceedings.mlr.press/v139/asi21a.html},
	abstract = {We study adaptive methods for differentially private convex optimization, proposing and analyzing differentially private variants of a Stochastic Gradient Descent (SGD) algorithm with adaptive stepsizes, as well as the AdaGrad algorithm. We provide upper bounds on the regret of both algorithms and show that the bounds are (worst-case) optimal. As a consequence of our development, we show that our private versions of AdaGrad outperform adaptive SGD, which in turn outperforms traditional SGD in scenarios with non-isotropic gradients where (non-private) Adagrad provably outperforms SGD. The major challenge is that the isotropic noise typically added for privacy dominates the signal in gradient geometry for high-dimensional problems; approaches to this that effectively optimize over lower-dimensional subspaces simply ignore the actual problems that varying gradient geometries introduce. In contrast, we study non-isotropic clipping and noise addition, developing a principled theoretical approach; the consequent procedures also enjoy significantly stronger empirical performance than prior approaches.},
	booktitle = {Proceedings of the 38th international conference on machine learning},
	publisher = {PMLR},
	author = {Asi, Hilal and Duchi, John and Fallah, Alireza and Javidbakht, Omid and Talwar, Kunal},
	editor = {Meila, Marina and Zhang, Tong},
	month = jul,
	year = {2021},
	note = {tex.pdf: http://proceedings.mlr.press/v139/asi21a/asi21a.pdf},
	keywords = {Untagged},
	pages = {383--392},
}

@phdthesis{arjovskyOutDistributionGeneralization2021,
	title = {Out of {Distribution} {Generalization} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/2103.02667},
	abstract = {Machine learning has achieved tremendous success in a variety of domains in recent years. However, a lot of these success stories have been in places where the training and the testing distributions are extremely similar to each other. In everyday situations when models are tested in slightly different data than they were trained on, ML algorithms can fail spectacularly. This research attempts to formally define this problem, what sets of assumptions are reasonable to make in our data and what kind of guarantees we hope to obtain from them. Then, we focus on a certain class of out of distribution problems, their assumptions, and introduce simple algorithms that follow from these assumptions that are able to provide more reliable generalization. A central topic in the thesis is the strong link between discovering the causal structure of the data, finding features that are reliable (when using them to predict) regardless of their context, and out of distribution generalization.},
	language = {en},
	urldate = {2021-09-27},
	author = {Arjovsky, Martin},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.02667},
	keywords = {Untagged},
}

@inproceedings{anonymousImageBERTPretraining2021,
	title = {Image {BERT} {Pre}-training with {Online} {Tokenizer}},
	url = {https://openreview.net/forum?id=ydopy-e6Dg},
	abstract = {The success of language Transformers is primarily attributed to the pretext task of masked language modeling (MLM), where texts are first tokenized into semantically meaningful pieces.
In this work...},
	language = {en},
	urldate = {2021-12-24},
	author = {Anonymous},
	month = sep,
	year = {2021},
	keywords = {Untagged},
}

@inproceedings{bain_frozen_2021,
	address = {Montreal, QC, Canada},
	title = {Frozen in {Time}: {A} {Joint} {Video} and {Image} {Encoder} for {End}-to-{End} {Retrieval}},
	isbn = {978-1-66542-812-5},
	shorttitle = {Frozen in {Time}},
	url = {https://ieeexplore.ieee.org/document/9711165/},
	doi = {10.1109/ICCV48922.2021.00175},
	abstract = {Our objective in this work is video-text retrieval – in particular a joint embedding that enables efficient text-to-video retrieval. The challenges in this area include the design of the visual architecture and the nature of the training data, in that the available large scale video-text training datasets, such as HowTo100M, are noisy and hence competitive performance is achieved only at scale through large amounts of compute.},
	language = {en},
	urldate = {2023-05-30},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Bain, Max and Nagrani, Arsha and Varol, Gul and Zisserman, Andrew},
	month = oct,
	year = {2021},
	keywords = {Untagged},
	pages = {1708--1718},
}

@article{bain_frozen_2021-1,
	title = {Frozen in {Time}: {A} {Joint} {Video} and {Image} {Encoder} for {End}-to-{End} {Retrieval}},
	shorttitle = {Frozen in {Time}},
	url = {https://arxiv.org/abs/2104.00650v2},
	doi = {10.48550/arXiv.2104.00650},
	abstract = {Our objective in this work is video-text retrieval - in particular a joint embedding that enables efficient text-to-video retrieval. The challenges in this area include the design of the visual architecture and the nature of the training data, in that the available large scale video-text training datasets, such as HowTo100M, are noisy and hence competitive performance is achieved only at scale through large amounts of compute. We address both these challenges in this paper. We propose an end-to-end trainable model that is designed to take advantage of both large-scale image and video captioning datasets. Our model is an adaptation and extension of the recent ViT and Timesformer architectures, and consists of attention in both space and time. The model is flexible and can be trained on both image and video text datasets, either independently or in conjunction. It is trained with a curriculum learning schedule that begins by treating images as 'frozen' snapshots of video, and then gradually learns to attend to increasing temporal context when trained on video datasets. We also provide a new video-text pretraining dataset WebVid-2M, comprised of over two million videos with weak captions scraped from the internet. Despite training on datasets that are an order of magnitude smaller, we show that this approach yields state-of-the-art results on standard downstream video-retrieval benchmarks including MSR-VTT, MSVD, DiDeMo and LSMDC.},
	language = {en},
	urldate = {2022-10-20},
	author = {Bain, Max and Nagrani, Arsha and Varol, Gül and Zisserman, Andrew},
	month = apr,
	year = {2021},
	keywords = {Untagged},
}

@article{balestrieroLearningHighDimension2021,
	title = {Learning in {High} {Dimension} {Always} {Amounts} to {Extrapolation}},
	url = {http://arxiv.org/abs/2110.09485},
	abstract = {The notion of interpolation and extrapolation is fundamental in various fields from deep learning to function approximation. Interpolation occurs for a sample \$x\$ whenever this sample falls inside or on the boundary of the given dataset's convex hull. Extrapolation occurs when \$x\$ falls outside of that convex hull. One fundamental (mis)conception is that state-of-the-art algorithms work so well because of their ability to correctly interpolate training data. A second (mis)conception is that interpolation happens throughout tasks and datasets, in fact, many intuitions and theories rely on that assumption. We empirically and theoretically argue against those two points and demonstrate that on any high-dimensional (\${\textgreater}\$100) dataset, interpolation almost surely never happens. Those results challenge the validity of our current interpolation/extrapolation definition as an indicator of generalization performances.},
	urldate = {2021-11-01},
	journal = {arXiv:2110.09485 [cs]},
	author = {Balestriero, Randall and Pesenti, Jerome and LeCun, Yann},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.09485},
	keywords = {Untagged},
}

@article{beyerKnowledgeDistillationGood2021,
	title = {Knowledge distillation: {A} good teacher is patient and consistent},
	shorttitle = {Knowledge distillation},
	url = {http://arxiv.org/abs/2106.05237},
	abstract = {There is a growing discrepancy in computer vision between large-scale models that achieve state-of-the-art performance and models that are affordable in practical applications. In this paper we address this issue and significantly bridge the gap between these two types of models. Throughout our empirical investigation we do not aim to necessarily propose a new method, but strive to identify a robust and effective recipe for making state-of-the-art large scale models affordable in practice. We demonstrate that, when performed correctly, knowledge distillation can be a powerful tool for reducing the size of large models without compromising their performance. In particular, we uncover that there are certain implicit design choices, which may drastically affect the effectiveness of distillation. Our key contribution is the explicit identification of these design choices, which were not previously articulated in the literature. We back up our findings by a comprehensive empirical study, demonstrate compelling results on a wide range of vision datasets and, in particular, obtain a state-of-the-art ResNet-50 model for ImageNet, which achieves 82.8{\textbackslash}\% top-1 accuracy.},
	urldate = {2021-07-02},
	journal = {arXiv:2106.05237 [cs]},
	author = {Beyer, Lucas and Zhai, Xiaohua and Royer, Amélie and Markeeva, Larisa and Anil, Rohan and Kolesnikov, Alexander},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.05237},
	keywords = {Untagged},
}

@article{baninoPonderNetLearningPonder2021,
	title = {{PonderNet}: {Learning} to {Ponder}},
	shorttitle = {{PonderNet}},
	url = {http://arxiv.org/abs/2107.05407},
	abstract = {In standard neural networks the amount of computation used grows with the size of the inputs, but not with the complexity of the problem being learnt. To overcome this limitation we introduce PonderNet, a new algorithm that learns to adapt the amount of computation based on the complexity of the problem at hand. PonderNet learns end-to-end the number of computational steps to achieve an effective compromise between training prediction accuracy, computational cost and generalization. On a complex synthetic problem, PonderNet dramatically improves performance over previous adaptive computation methods and additionally succeeds at extrapolation tests where traditional neural networks fail. Also, our method matched the current state of the art results on a real world question and answering dataset, but using less compute. Finally, PonderNet reached state of the art results on a complex task designed to test the reasoning capabilities of neural networks.1},
	urldate = {2021-08-26},
	journal = {arXiv:2107.05407 [cs]},
	author = {Banino, Andrea and Balaguer, Jan and Blundell, Charles},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.05407},
	keywords = {Untagged},
}

@article{bardesVICRegVarianceInvarianceCovarianceRegularization2021,
	title = {{VICReg}: {Variance}-{Invariance}-{Covariance} {Regularization} for {Self}-{Supervised} {Learning}},
	shorttitle = {{VICReg}},
	url = {http://arxiv.org/abs/2105.04906},
	abstract = {Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements.},
	urldate = {2021-08-19},
	journal = {arXiv:2105.04906 [cs]},
	author = {Bardes, Adrien and Ponce, Jean and LeCun, Yann},
	month = may,
	year = {2021},
	note = {arXiv: 2105.04906},
	keywords = {Untagged},
}

@article{belloLAMBDANETWORKSMODELINGLONGRANGE2021,
	title = {{LAMBDANETWORKS}: {MODELING} {LONG}-{RANGE} {INTERACTIONS} {WITHOUT} {ATTENTION}},
	abstract = {We present lambda layers – an alternative framework to self-attention – for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Lambda layers capture such interactions by transforming available contexts into linear functions, termed lambdas, and applying these linear functions to each input separately. Similar to linear attention, lambda layers bypass expensive attention maps, but in contrast, they model both content and position-based interactions which enables their application to large structured inputs such as images. The resulting neural network architectures, LambdaNetworks, signiﬁcantly outperform their convolutional and attentional counterparts on ImageNet classiﬁcation, COCO object detection and COCO instance segmentation, while being more computationally efﬁcient. Additionally, we design LambdaResNets, a family of hybrid architectures across different scales, that considerably improves the speed-accuracy tradeoff of image classiﬁcation models. LambdaResNets reach excellent accuracies on ImageNet while being 3.2 - 4.4x faster than the popular EfﬁcientNets on modern machine learning accelerators. When training with an additional 130M pseudo-labeled images, LambdaResNets achieve up to a 9.5x speed-up over the corresponding EfﬁcientNet checkpoints1.},
	language = {en},
	author = {Bello, Irwan},
	year = {2021},
	keywords = {Untagged},
	pages = {31},
}

@article{blum_fishyscapes_2021,
	title = {The {Fishyscapes} {Benchmark}: {Measuring} {Blind} {Spots} in {Semantic} {Segmentation}},
	volume = {129},
	issn = {0920-5691, 1573-1405},
	shorttitle = {The {Fishyscapes} {Benchmark}},
	url = {https://link.springer.com/10.1007/s11263-021-01511-6},
	doi = {10.1007/s11263-021-01511-6},
	abstract = {Deep learning has enabled impressive progress in the accuracy of semantic segmentation. Yet, the ability to estimate uncertainty and detect failure is key for safety-critical applications like autonomous driving. Existing uncertainty estimates have mostly been evaluated on simple tasks, and it is unclear whether these methods generalize to more complex scenarios. We present Fishyscapes, the ﬁrst public benchmark for anomaly detection in a real-world task of semantic segmentation for urban driving. It evaluates pixel-wise uncertainty estimates towards the detection of anomalous objects. We adapt state-of-the-art methods to recent semantic segmentation models and compare uncertainty estimation approaches based on softmax conﬁdence, Bayesian learning, density estimation, image resynthesis, as well as supervised anomaly detection methods. Our results show that anomaly detection is far from solved even for ordinary situations, while our benchmark allows measuring advancements beyond the state-of-the-art. Results, data and submission information can be found at https://ﬁshyscapes.com/.},
	language = {en},
	number = {11},
	urldate = {2023-06-02},
	journal = {International Journal of Computer Vision},
	author = {Blum, Hermann and Sarlin, Paul-Edouard and Nieto, Juan and Siegwart, Roland and Cadena, Cesar},
	month = nov,
	year = {2021},
	keywords = {Untagged},
	pages = {3119--3135},
}

@article{bigazziExploreExplainSelfsupervised2021,
	title = {Explore and {Explain}: {Self}-supervised {Navigation} and {Recounting}},
	shorttitle = {Explore and {Explain}},
	url = {http://arxiv.org/abs/2007.07268},
	doi = {10/gnbbxx},
	abstract = {Embodied AI has been recently gaining attention as it aims to foster the development of autonomous and intelligent agents. In this paper, we devise a novel embodied setting in which an agent needs to explore a previously unknown environment while recounting what it sees during the path. In this context, the agent needs to navigate the environment driven by an exploration goal, select proper moments for description, and output natural language descriptions of relevant objects and scenes. Our model integrates a novel self-supervised exploration module with penalty, and a fully-attentive captioning model for explanation. Also, we investigate different policies for selecting proper moments for explanation, driven by information coming from both the environment and the navigation. Experiments are conducted on photorealistic environments from the Matterport3D dataset and investigate the navigation and explanation capabilities of the agent as well as the role of their interactions.},
	language = {en},
	urldate = {2021-11-03},
	journal = {2020 25th International Conference on Pattern Recognition (ICPR)},
	author = {Bigazzi, Roberto and Landi, Federico and Cornia, Marcella and Cascianelli, Silvia and Baraldi, Lorenzo and Cucchiara, Rita},
	month = jan,
	year = {2021},
	note = {arXiv: 2007.07268},
	keywords = {Untagged},
	pages = {1152--1159},
}

@inproceedings{bubeckLawRobustnessTwoLayers2021,
	title = {A {Law} of {Robustness} for {Two}-{Layers} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v134/bubeck21a.html},
	abstract = {We initiate the study of the inherent tradeoffs between the size of a neural network and its robustness, as measured by its Lipschitz constant. We make a precise conjecture that, for any Lipschitz activation function and for most datasets, any two-layers neural network with \$k\$ neurons that perfectly fit the data must have its Lipschitz constant larger (up to a constant) than \${\textbackslash}sqrt\{n/k\}\$ where \$n\$ is the number of datapoints. In particular, this conjecture implies that overparametrization is necessary for robustness, since it means that one needs roughly one neuron per datapoint to ensure a \$O(1)\$-Lipschitz network, while mere data fitting of \$d\$-dimensional data requires only one neuron per \$d\$ datapoints. We prove a weaker version of this conjecture when the Lipschitz constant is replaced by an upper bound on it based on the spectral norm of the weight matrix. We also prove the conjecture in the high-dimensional regime \$n {\textbackslash}approx d\$ (which we also refer to as the undercomplete case, since only \$k {\textbackslash}leq d\$ is relevant here). Finally we prove the conjecture for polynomial activation functions of degree \$p\$ when \$n {\textbackslash}approx d{\textasciicircum}p\$. We complement these findings with experimental evidence supporting the conjecture.},
	language = {en},
	urldate = {2021-12-02},
	booktitle = {Proceedings of {Thirty} {Fourth} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Bubeck, Sebastien and Li, Yuanzhi and Nagaraj, Dheeraj M.},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {804--820},
}

@article{bronsteinGeometricDeepLearning2021,
	title = {Geometric {Deep} {Learning}: {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	shorttitle = {Geometric {Deep} {Learning}},
	url = {http://arxiv.org/abs/2104.13478},
	abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
	urldate = {2022-02-25},
	journal = {arXiv:2104.13478 [cs, stat]},
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
	month = may,
	year = {2021},
	note = {arXiv: 2104.13478},
	keywords = {Untagged},
}

@inproceedings{carlini_extracting_2021,
	title = {Extracting {Training} {Data} from {Large} {Language} {Models}},
	isbn = {978-1-939133-24-3},
	url = {https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting},
	language = {en},
	urldate = {2022-10-15},
	author = {Carlini, Nicholas and Tramèr, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Úlfar and Oprea, Alina and Raffel, Colin},
	year = {2021},
	keywords = {Untagged},
	pages = {2633--2650},
}

@article{carliniMembershipInferenceAttacks2021,
	title = {Membership {Inference} {Attacks} {From} {First} {Principles}},
	url = {http://arxiv.org/abs/2112.03570},
	abstract = {A membership inference attack allows an adversary to query a trained machine learning model to predict whether or not a particular example was contained in the model's training dataset. These attacks are currently evaluated using average-case "accuracy" metrics that fail to characterize whether the attack can confidently identify any members of the training set. We argue that attacks should instead be evaluated by computing their true-positive rate at low (e.g., {\textless}0.1\%) false-positive rates, and find most prior attacks perform poorly when evaluated in this way. To address this we develop a Likelihood Ratio Attack (LiRA) that carefully combines multiple ideas from the literature. Our attack is 10x more powerful at low false-positive rates, and also strictly dominates prior attacks on existing metrics.},
	urldate = {2022-03-21},
	journal = {arXiv:2112.03570 [cs]},
	author = {Carlini, Nicholas and Chien, Steve and Nasr, Milad and Song, Shuang and Terzis, Andreas and Tramer, Florian},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.03570},
	keywords = {Untagged},
}

@inproceedings{bubeckUniversalLawRobustness2021,
	title = {A {Universal} {Law} of {Robustness} via {Isoperimetry}},
	url = {https://openreview.net/forum?id=z71OSKqTFh7},
	abstract = {Classically, data interpolation with a parametrized model class is possible as long as the number of parameters is larger than the number of equations to be satisfied. A puzzling phenomenon in the...},
	language = {en},
	urldate = {2022-02-25},
	author = {Bubeck, Sebastien and Sellke, Mark},
	month = may,
	year = {2021},
	keywords = {Untagged},
}

@article{caiRevisitHashingAlgorithms2021,
	title = {A {Revisit} of {Hashing} {Algorithms} for {Approximate} {Nearest} {Neighbor} {Search}},
	volume = {33},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2019.2953897},
	abstract = {Approximate Nearest Neighbor Search (ANNS) is a fundamental problem in many areas of machine learning and data mining. During the past decade, numerous hashing algorithms are proposed to solve this problem. Every proposed algorithm claims to outperform Locality Sensitive Hashing (LSH), which is the most popular hashing method. However, the evaluation of these hashing article was not thorough enough, and the claim should be re-examined. If implemented correctly, almost all the hashing methods will have their performance improved as the code length increases. However, many existing hashing article only report the performance with the code length shorter than 128. In this article, we carefully revisit the problem of search-with-a-hash-index and analyze the pros and cons of two popular hash index search procedures. Then we proposed a simple but effective novel hash index search approach and made a thorough comparison of eleven popular hashing algorithms. Surprisingly, the random-projection-based Locality Sensitive Hashing ranked the first, which is in contradiction to the claims in all the other 10 hashing article. Despite the extreme simplicity of random-projection-based LSH, our results show that the capability of this algorithm has been far underestimated. For the sake of reproducibility, all the codes used in the article are released on GitHub, which can be used as a testing platform for a fair comparison between various hashing algorithms.},
	number = {6},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Cai, Deng},
	month = jun,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Untagged},
	pages = {2337--2348},
}

@article{chenWhenMachineUnlearning2021,
	title = {When {Machine} {Unlearning} {Jeopardizes} {Privacy}},
	url = {http://arxiv.org/abs/2005.02205},
	doi = {10.1145/3460120.3484756},
	abstract = {The right to be forgotten states that a data owner has the right to erase their data from an entity storing it. In the context of machine learning (ML), the right to be forgotten requires an ML model owner to remove the data owner's data from the training set used to build the ML model, a process known as machine unlearning. While originally designed to protect the privacy of the data owner, we argue that machine unlearning may leave some imprint of the data in the ML model and thus create unintended privacy risks. In this paper, we perform the first study on investigating the unintended information leakage caused by machine unlearning. We propose a novel membership inference attack that leverages the different outputs of an ML model's two versions to infer whether a target sample is part of the training set of the original model but out of the training set of the corresponding unlearned model. Our experiments demonstrate that the proposed membership inference attack achieves strong performance. More importantly, we show that our attack in multiple cases outperforms the classical membership inference attack on the original ML model, which indicates that machine unlearning can have counterproductive effects on privacy. We notice that the privacy degradation is especially significant for well-generalized ML models where classical membership inference does not perform well. We further investigate four mechanisms to mitigate the newly discovered privacy risks and show that releasing the predicted label only, temperature scaling, and differential privacy are effective. We believe that our results can help improve privacy protection in practical implementations of machine unlearning. Our code is available at https://github.com/MinChen00/UnlearningLeaks.},
	urldate = {2022-03-21},
	journal = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
	author = {Chen, Min and Zhang, Zhikun and Wang, Tianhao and Backes, Michael and Humbert, Mathias and Zhang, Yang},
	month = nov,
	year = {2021},
	note = {arXiv: 2005.02205},
	keywords = {Untagged},
	pages = {896--911},
}

@article{chenDeepLearningSensorbased2021,
	title = {Deep {Learning} for {Sensor}-based {Human} {Activity} {Recognition}: {Overview}, {Challenges} and {Opportunities}},
	volume = {54},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3447744},
	doi = {10.1145/3447744},
	abstract = {The vast proliferation of sensor devices and Internet of Things enables the applications of sensor-based activity recognition. However, there exist substantial challenges that could influence the performance of the recognition system in practical scenarios. Recently, as deep learning has demonstrated its effectiveness in many areas, plenty of deep methods have been investigated to address the challenges in activity recognition. In this study, we present a survey of the state-of-the-art deep learning methods for sensor-based human activity recognition. We first introduce the multi-modality of the sensory data and provide information for public datasets that can be used for evaluation in different challenge tasks. We then propose a new taxonomy to structure the deep methods by challenges. Challenges and challenge-related deep methods are summarized and analyzed to form an overview of the current research progress. At the end of this work, we discuss the open issues and provide some insights for future directions.},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Chen, Kaixuan and Zhang, Dalin and Yao, Lina and Guo, Bin and Yu, Zhiwen and Liu, Yunhao},
	month = may,
	year = {2021},
	note = {Number of pages: 40
Place: New York, NY, USA
Publisher: Association for Computing Machinery
tex.articleno: 77
tex.issue\_date: May 2021},
	keywords = {Untagged},
}

@article{cheferGenericAttentionmodelExplainability2021,
	title = {Generic {Attention}-model {Explainability} for {Interpreting} {Bi}-{Modal} and {Encoder}-{Decoder} {Transformers}},
	url = {http://arxiv.org/abs/2103.15679},
	abstract = {Transformers are increasingly dominating multi-modal reasoning tasks, such as visual question answering, achieving state-of-the-art results thanks to their ability to contextualize information using the self-attention and co-attention mechanisms. These attention modules also play a role in other computer vision tasks including object detection and image segmentation. Unlike Transformers that only use self-attention, Transformers with co-attention require to consider multiple attention maps in parallel in order to highlight the information that is relevant to the prediction in the model's input. In this work, we propose the first method to explain prediction by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions. We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. We show that our method is superior to all existing methods which are adapted from single modality explainability.},
	urldate = {2021-08-25},
	journal = {arXiv:2103.15679 [cs]},
	author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.15679},
	keywords = {Untagged},
}

@article{caronEmergingPropertiesSelfSupervised2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2104.14294},
	abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	urldate = {2021-08-12},
	journal = {arXiv:2104.14294 [cs]},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = may,
	year = {2021},
	note = {arXiv: 2104.14294},
	keywords = {Untagged},
}

@article{chenPSViTBetterVision2021,
	title = {{PSViT}: {Better} {Vision} {Transformer} via {Token} {Pooling} and {Attention} {Sharing}},
	shorttitle = {{PSViT}},
	url = {http://arxiv.org/abs/2108.03428},
	abstract = {In this paper, we observe two levels of redundancies when applying vision transformers (ViT) for image recognition. First, fixing the number of tokens through the whole network produces redundant features at the spatial level. Second, the attention maps among different transformer layers are redundant. Based on the observations above, we propose a PSViT: a ViT with token Pooling and attention Sharing to reduce the redundancy, effectively enhancing the feature representation ability, and achieving a better speed-accuracy trade-off. Specifically, in our PSViT, token pooling can be defined as the operation that decreases the number of tokens at the spatial level. Besides, attention sharing will be built between the neighboring transformer layers for reusing the attention maps having a strong correlation among adjacent layers. Then, a compact set of the possible combinations for different token pooling and attention sharing mechanisms are constructed. Based on the proposed compact set, the number of tokens in each layer and the choices of layers sharing attention can be treated as hyper-parameters that are learned from data automatically. Experimental results show that the proposed scheme can achieve up to 6.6\% accuracy improvement in ImageNet classification compared with the DeiT.},
	urldate = {2021-08-18},
	journal = {arXiv:2108.03428 [cs]},
	author = {Chen, Boyu and Li, Peixia and Li, Baopu and Li, Chuming and Bai, Lei and Lin, Chen and Sun, Ming and Yan, Junjie and Ouyang, Wanli},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.03428},
	keywords = {Untagged},
}

@article{chen_robust_2021,
	title = {{ROBUST} {OVERFITTING} {MAY} {BE} {MITIGATED} {BY} {PROP}- {ERLY} {LEARNED} {SMOOTHENING}},
	abstract = {A recent study (Rice et al., 2020) revealed overﬁtting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overﬁtting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overﬁtting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by 3.72\% ∼ 6.68\% and robust accuracy by 0.22\% ∼ 2.03\%, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ( ∞ and 2), and robustiﬁed methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models’ robustness against transfer attacks. Codes are available at https: //github.com/VITA-Group/Alleviate-Robust-Overfitting.},
	language = {en},
	author = {Chen, Tianlong and Zhang, Zhenyu and Liu, Sijia and Chang, Shiyu and Wang, Zhangyang},
	year = {2021},
	keywords = {Untagged},
}

@article{chenElasticLotteryTicket2021,
	title = {The {Elastic} {Lottery} {Ticket} {Hypothesis}},
	volume = {abs/2103.16547},
	url = {https://arxiv.org/abs/2103.16547},
	journal = {CoRR},
	author = {Chen, Xiaohan and Cheng, Yu and Wang, Shuohang and Gan, Zhe and Liu, Jingjing and Wang, Zhangyang},
	year = {2021},
	note = {arXiv: 2103.16547},
	keywords = {Untagged},
}

@article{chenDataEfficientGANTraining2021,
	title = {Data-{Efficient} {GAN} {Training} {Beyond} ({Just}) {Augmentations}: {A} {Lottery} {Ticket} {Perspective}},
	shorttitle = {Data-{Efficient} {GAN} {Training} {Beyond} ({Just}) {Augmentations}},
	url = {http://arxiv.org/abs/2103.00397},
	abstract = {Training generative adversarial networks (GANs) with limited data generally results in deteriorated performance and collapsed models. To conquer this challenge, we are inspired by the latest observation of Kalibhat et al. (2020); Chen et al. (2021d), that one can discover independently trainable and highly sparse subnetworks (a.k.a., lottery tickets) from GANs. Treating this as an inductive prior, we decompose the data-hungry GAN training into two sequential sub-problems: (i) identifying the lottery ticket from the original GAN; then (ii) training the found sparse subnetwork with aggressive data and feature augmentations. Both sub-problems re-use the same small training set of real images. Such a coordinated framework enables us to focus on lower-complexity and more data-efﬁcient sub-problems, effectively stabilizing training and improving convergence. Comprehensive experiments endorse the effectiveness of our proposed ultra-data-efﬁcient training framework, across various GAN architectures (SNGAN, BigGAN, and StyleGAN2) and diverse datasets (CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet). Besides, our training framework also displays powerful few-shot generalization ability, i.e., generating high-ﬁdelity images by training from scratch with just 100 real images, without any pre-training. Codes are available at: https://github.com/VITA-Group/ Ultra-Data-Efficient-GAN-Training.},
	language = {en},
	urldate = {2021-10-20},
	journal = {arXiv:2103.00397 [cs]},
	author = {Chen, Tianlong and Cheng, Yu and Gan, Zhe and Liu, Jingjing and Wang, Zhangyang},
	month = may,
	year = {2021},
	note = {arXiv: 2103.00397},
	keywords = {Untagged},
}

@article{chenInputSpecificRobustnessCertification2021,
	title = {Input-{Specific} {Robustness} {Certification} for {Randomized} {Smoothing}},
	url = {http://arxiv.org/abs/2112.12084},
	abstract = {Although randomized smoothing has demonstrated high certiﬁed robustness and superior scalability to other certiﬁed defenses, the high computational overhead of the robustness certiﬁcation bottlenecks the practical applicability, as it depends heavily on the large sample approximation for estimating the conﬁdence interval. In existing works, the sample size for the conﬁdence interval is universally set and agnostic to the input for prediction. This Input-Agnostic Sampling (IAS) scheme may yield a poor Average Certiﬁed Radius (ACR)-runtime trade-off which calls for improvement. In this paper, we propose Input-Speciﬁc Sampling (ISS) acceleration to achieve the cost-effectiveness for robustness certiﬁcation, in an adaptive way of reducing the sampling size based on the input characteristic. Furthermore, our method universally controls the certiﬁed radius decline from the ISS sample size reduction. The empirical results on CIFAR-10 and ImageNet show that ISS can speed up the certiﬁcation by more than three times at a limited cost of 0.05 certiﬁed radius. Meanwhile, ISS surpasses IAS on the average certiﬁed radius across the extensive hyperparameter settings. Speciﬁcally, ISS achieves ACR=0.958 on ImageNet (σ = 1.0) in 250 minutes, compared to ACR=0.917 by IAS under the same condition. We release our code in https://github.com/roy-ch/Input-Speciﬁc-Certiﬁcation.},
	language = {en},
	urldate = {2022-02-22},
	journal = {arXiv:2112.12084 [cs]},
	author = {Chen, Ruoxin and Li, Jie and Yan, Junchi and Li, Ping and Sheng, Bin},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.12084},
	keywords = {Untagged},
}

@article{chenUnrestrictedAdversarialAttacks2021,
	title = {Unrestricted {Adversarial} {Attacks} on {ImageNet} {Competition}},
	journal = {arXiv preprint arXiv:2110.09903},
	author = {Chen, Yuefeng and Mao, Xiaofeng and He, Yuan and Xue, Hui and Li, Chao and Dong, Yinpeng and Fu, Qi-An and Yang, Xiao and Xiang, Wenzhao and Pang, Tianyu},
	year = {2021},
	keywords = {Untagged},
}

@article{chenDPTDeformablePatchbased2021,
	title = {{DPT}: {Deformable} {Patch}-based {Transformer} for {Visual} {Recognition}},
	shorttitle = {{DPT}},
	url = {http://arxiv.org/abs/2107.14467},
	doi = {10.1145/3474085.3475467},
	abstract = {Transformer has achieved great success in computer vision, while how to split patches in an image remains a problem. Existing methods usually use a fixed-size patch embedding which might destroy the semantics of objects. To address this problem, we propose a new Deformable Patch (DePatch) module which learns to adaptively split the images into patches with different positions and scales in a data-driven way rather than using predefined fixed patches. In this way, our method can well preserve the semantics in patches. The DePatch module can work as a plug-and-play module, which can easily be incorporated into different transformers to achieve an end-to-end training. We term this DePatch-embedded transformer as Deformable Patch-based Transformer (DPT) and conduct extensive evaluations of DPT on image classification and object detection. Results show DPT can achieve 81.9\% top-1 accuracy on ImageNet classification, and 43.7\% box mAP with RetinaNet, 44.3\% with Mask R-CNN on MSCOCO object detection. Code has been made available at: https://github.com/CASIA-IVA-Lab/DPT .},
	urldate = {2021-08-18},
	journal = {arXiv:2107.14467 [cs]},
	author = {Chen, Zhiyang and Zhu, Yousong and Zhao, Chaoyang and Hu, Guosheng and Zeng, Wei and Wang, Jinqiao and Tang, Ming},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.14467},
	keywords = {Untagged},
}

@article{chenTransHashTransformerbasedHamming2021,
	title = {{TransHash}: {Transformer}-based {Hamming} {Hashing} for {Efficient} {Image} {Retrieval}},
	shorttitle = {{TransHash}},
	url = {http://arxiv.org/abs/2105.01823},
	abstract = {Deep hamming hashing has gained growing popularity in approximate nearest neighbour search for large-scale image retrieval. Until now, the deep hashing for the image retrieval community has been dominated by convolutional neural network architectures, e.g. Resnet[21]. In this paper, inspired by the recent advancements of vision transformers, we present Transhash, a pure transformerbased framework for deep hashing learning. Concretely, our framework is composed of two major modules: (1) Based on Vision Transformer (ViT), we design a siamese vision transformer backbone for image feature extraction. To learn fine-grained features, we innovate a dual-stream feature learning on top of the transformer to learn discriminative global and local features. (2) Besides, we adopt a Bayesian learning scheme with a dynamically constructed similarity matrix to learn compact binary hash codes. The entire framework is jointly trained in an end-to-end manner. To the best of our knowledge, this is the first work to tackle deep hashing learning problems without convolutional neural networks (CNNs). We perform comprehensive experiments on three widely-studied datasets: CIFAR-10, NUSWIDE and IMAGENET. The experiments have evidenced our superiority against the existing state-of-the-art deep hashing methods. Specifically, we achieve 8.2\%, 2.6\%, 12.7\% performance gains in terms of average mAP for different hash bit lengths on three public datasets, respectively.},
	language = {en},
	urldate = {2021-10-21},
	journal = {arXiv:2105.01823 [cs]},
	author = {Chen, Yongbiao and Zhang, Sheng and Liu, Fangxin and Chang, Zhigang and Ye, Mang and Qi, Zhengwei},
	month = may,
	year = {2021},
	note = {arXiv: 2105.01823},
	keywords = {Untagged},
}

@article{chenGANsCanPlay2021,
	title = {{GANs} {Can} {Play} {Lottery} {Tickets} {Too}},
	url = {http://arxiv.org/abs/2106.00134},
	abstract = {Deep generative adversarial networks (GANs) have gained growing popularity in numerous scenarios, while usually suffer from high parameter complexities for resource-constrained real-world applications. However, the compression of GANs has less been explored. A few works show that heuristically applying compression techniques normally leads to unsatisfactory results, due to the notorious training instability of GANs. In parallel, the lottery ticket hypothesis shows prevailing success on discriminative models, in locating sparse matching subnetworks capable of training in isolation to full model performance. In this work, we for the ﬁrst time study the existence of such trainable matching subnetworks in deep GANs. For a range of GANs, we certainly ﬁnd matching subnetworks at 67\%-74\% sparsity. We observe that with or without pruning discriminator has a minor effect on the existence and quality of matching subnetworks, while the initialization weights used in the discriminator plays a signiﬁcant role. We then show the powerful transferability of these subnetworks to unseen tasks. Furthermore, extensive experimental results demonstrate that our found subnetworks substantially outperform previous state-of-the-art GAN compression approaches in both image generation (e.g. SNGAN) and image-to-image translation GANs (e.g. CycleGAN). Codes available at https://github.com/VITA-Group/GAN-LTH.},
	language = {en},
	urldate = {2021-10-21},
	journal = {arXiv:2106.00134 [cs]},
	author = {Chen, Xuxi and Zhang, Zhenyu and Sui, Yongduo and Chen, Tianlong},
	month = may,
	year = {2021},
	note = {arXiv: 2106.00134},
	keywords = {Untagged},
}

@misc{cheng_improving_2021,
	title = {Improving {Video}-{Text} {Retrieval} by {Multi}-{Stream} {Corpus} {Alignment} and {Dual} {Softmax} {Loss}},
	shorttitle = {{CAMoE}},
	url = {http://arxiv.org/abs/2109.04290},
	doi = {10.48550/arXiv.2109.04290},
	abstract = {Employing large-scale pre-trained model CLIP to conduct video-text retrieval task (VTR) has become a new trend, which exceeds previous VTR methods. Though, due to the heterogeneity of structures and contents between video and text, previous CLIP-based models are prone to overfitting in the training phase, resulting in relatively poor retrieval performance. In this paper, we propose a multi-stream Corpus Alignment network with single gate Mixture-of-Experts (CAMoE) and a novel Dual Softmax Loss (DSL) to solve the two heterogeneity. The CAMoE employs Mixture-of-Experts (MoE) to extract multi-perspective video representations, including action, entity, scene, etc., then align them with the corresponding part of the text. In this stage, we conduct massive explorations towards the feature extraction module and feature alignment module. DSL is proposed to avoid the one-way optimum-match which occurs in previous contrastive methods. Introducing the intrinsic prior of each pair in a batch, DSL serves as a reviser to correct the similarity matrix and achieves the dual optimal match. DSL is easy to implement with only one-line code but improves significantly. The results show that the proposed CAMoE and DSL are of strong efficiency, and each of them is capable of achieving State-of-The-Art (SOTA) individually on various benchmarks such as MSR-VTT, MSVD, and LSMDC. Further, with both of them, the performance is advanced to a big extend, surpassing the previous SOTA methods for around 4.6{\textbackslash}\% R@1 in MSR-VTT.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Cheng, Xing and Lin, Hezheng and Wu, Xiangyu and Yang, Fan and Shen, Dong},
	month = nov,
	year = {2021},
	note = {arXiv:2109.04290 [cs]},
	keywords = {Untagged},
}

@inproceedings{choquette-chooLabelOnlyMembershipInference2021,
	title = {Label-{Only} {Membership} {Inference} {Attacks}},
	url = {http://arxiv.org/abs/2007.14321},
	abstract = {Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.},
	urldate = {2022-03-21},
	booktitle = {{ICML}},
	author = {Choquette-Choo, Christopher A. and Tramer, Florian and Carlini, Nicholas and Papernot, Nicolas},
	month = dec,
	year = {2021},
	note = {arXiv: 2007.14321},
	keywords = {Untagged},
}

@inproceedings{DBLP:conf/iclr/Choquette-ChooD21,
	title = {{CaPC} learning: {Confidential} and private collaborative learning},
	url = {https://openreview.net/forum?id=h2EbJ4_wMVq},
	booktitle = {9th international conference on learning representations, {ICLR} 2021, virtual event, austria, may 3-7, 2021},
	publisher = {OpenReview.net},
	author = {Choquette-Choo, Christopher A. and Dullerud, Natalie and Dziedzic, Adam and Zhang, Yunxiang and Jha, Somesh and Papernot, Nicolas and Wang, Xiao},
	year = {2021},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/iclr/Choquette-ChooD21.bib
tex.timestamp: Sat, 04 Dec 2021 18:14:03 +0100},
	keywords = {Untagged},
}

@article{chengFedGEMSFederatedLearning2021,
	title = {{FedGEMS}: {Federated} {Learning} of {Larger} {Server} {Models} via {Selective} {Knowledge} {Fusion}},
	shorttitle = {{FedGEMS}},
	url = {http://arxiv.org/abs/2110.11027},
	abstract = {Today data is often scattered among billions of resource-constrained edge devices with security and privacy constraints. Federated Learning (FL) has emerged as a viable solution to learn a global model while keeping data private, but the model complexity of FL is impeded by the computation resources of edge nodes. In this work, we investigate a novel paradigm to take advantage of a powerful server model to break through model capacity in FL. By selectively learning from multiple teacher clients and itself, a server model develops in-depth knowledge and transfers its knowledge back to clients in return to boost their respective performance. Our proposed framework achieves superior performance on both server and client models and provides several advantages in a uniﬁed framework, including ﬂexibility for heterogeneous client architectures, robustness to poisoning attacks, and communication efﬁciency between clients and server. By bridging FL effectively with larger server model training, our proposed paradigm paves ways for robust and continual knowledge accumulation from distributed and private data.},
	language = {en},
	urldate = {2021-11-15},
	journal = {arXiv:2110.11027 [cs]},
	author = {Cheng, Sijie and Wu, Jingwen and Xiao, Yanghua and Liu, Yang and Liu, Yang},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.11027},
	keywords = {Untagged},
}

@inproceedings{chenTVQVCTransformerBased2021,
	title = {{TVQVC}: {Transformer} {Based} {Vector} {Quantized} {Variational} {Autoencoder} with {CTC} {Loss} for {Voice} {Conversion}},
	shorttitle = {{TVQVC}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/chen21e_interspeech.html},
	doi = {10/gnbm22},
	abstract = {Techniques of voice conversion(VC) aim to modify the speaker identity and style of an utterance while preserving the linguistic content. Although there are lots of VC methods, the state of the art of VC is still cascading automatic speech recognition(ASR) and text-to-speech(TTS). This paper presents a new structure of vector-quantized autoencoder based on transformer with CTC loss for non-parallel VC, which inspired by cascading ASR and TTS VC method. Our proposed method combines CTC loss and vector quantization to get high-level linguistic information without speaker information. Objective and subjective evaluations on the mandarin datasets show that the converted speech of our proposed model is better than baselines on naturalness, rhythm and speaker similarity.},
	language = {en},
	urldate = {2021-11-04},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Chen, Ziyi and Zhang, Pengyuan},
	month = aug,
	year = {2021},
	keywords = {Untagged},
	pages = {826--830},
}

@inproceedings{croitoru_teachtext_2021,
	address = {Montreal, QC, Canada},
	title = {{TeachText}: {CrossModal} {Generalized} {Distillation} for {Text}-{Video} {Retrieval}},
	isbn = {978-1-66542-812-5},
	shorttitle = {{TT}-{CE}+},
	url = {https://ieeexplore.ieee.org/document/9710750/},
	doi = {10.1109/ICCV48922.2021.01138},
	abstract = {In recent years, considerable progress on the task of text-video retrieval has been achieved by leveraging largescale pretraining on visual and audio datasets to construct powerful video encoders. By contrast, despite the natural symmetry, the design of effective algorithms for exploiting large-scale language pretraining remains under-explored. In this work, we are the first to investigate the design of such algorithms and propose a novel generalized distillation method, TEACHTEXT, which leverages complementary cues from multiple text encoders to provide an enhanced supervisory signal to the retrieval model. Moreover, we extend our method to video side modalities and show that we can effectively reduce the number of used modalities at test time without compromising performance. Our approach advances the state of the art on several video retrieval benchmarks by a significant margin and adds no computational overhead at test time. Last but not least, we show an effective application of our method for eliminating noise from retrieval datasets. Code and data can be found at https://www.robots.ox.ac.uk/˜vgg/ research/teachtext/.},
	language = {en},
	urldate = {2023-05-30},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Croitoru, Ioana and Bogolin, Simion-Vlad and Leordeanu, Marius and Jin, Hailin and Zisserman, Andrew and Albanie, Samuel and Liu, Yang},
	month = oct,
	year = {2021},
	keywords = {Untagged},
	pages = {11563--11573},
}

@article{croce_adversarial_2021,
	title = {Adversarial robustness against multiple \$l\_p\$-threat models at the price of one and how to quickly fine-tune robust models to another threat model},
	url = {http://arxiv.org/abs/2105.12508},
	abstract = {Adversarial training (AT) in order to achieve adversarial robustness wrt single \$l\_p\$-threat models has been discussed extensively. However, for safety-critical systems adversarial robustness should be achieved wrt all \$l\_p\$-threat models simultaneously. In this paper we develop a simple and efficient training scheme to achieve adversarial robustness against the union of \$l\_p\$-threat models. Our novel \$l\_1+l\_{\textbackslash}infty\$-AT scheme is based on geometric considerations of the different \$l\_p\$-balls and costs as much as normal adversarial training against a single \$l\_p\$-threat model. Moreover, we show that using our \$l\_1+l\_{\textbackslash}infty\$-AT scheme one can fine-tune with just 3 epochs any \$l\_p\$-robust model (for \$p {\textbackslash}in {\textbackslash}\{1,2,{\textbackslash}infty{\textbackslash}\}\$) and achieve multiple norm adversarial robustness. In this way we boost the previous state-of-the-art reported for multiple-norm robustness by more than \$6{\textbackslash}\%\$ on CIFAR-10 and report up to our knowledge the first ImageNet models with multiple norm robustness. Moreover, we study the general transfer of adversarial robustness between different threat models and in this way boost the previous SOTA \$l\_1\$-robustness on CIFAR-10 by almost \$10{\textbackslash}\%\$.},
	urldate = {2022-06-05},
	journal = {arXiv:2105.12508},
	author = {Croce, Francesco and Hein, Matthias},
	month = may,
	year = {2021},
	note = {arXiv:2105.12508 [cs]
type: article},
	keywords = {Untagged},
}

@inproceedings{chunProbabilisticEmbeddingsCrossModal2021,
	title = {Probabilistic {Embeddings} for {Cross}-{Modal} {Retrieval}},
	shorttitle = {{PCME}},
	url = {http://arxiv.org/abs/2101.05068},
	abstract = {Cross-modal retrieval methods build a common representation space for samples from multiple modalities, typically from the vision and the language domains. For images and their captions, the multiplicity of the correspondences makes the task particularly challenging. Given an image (respectively a caption), there are multiple captions (respectively images) that equally make sense. In this paper, we argue that deterministic functions are not sufﬁciently powerful to capture such one-to-many correspondences. Instead, we propose to use Probabilistic Cross-Modal Embedding (PCME), where samples from the different modalities are represented as probabilistic distributions in the common embedding space. Since common benchmarks such as COCO suffer from non-exhaustive annotations for crossmodal matches, we propose to additionally evaluate retrieval on the CUB dataset, a smaller yet clean database where all possible image-caption pairs are annotated. We extensively ablate PCME and demonstrate that it not only improves the retrieval performance over its deterministic counterpart but also provides uncertainty estimates that render the embeddings more interpretable. Code is available at https://github.com/naver-ai/pcme.},
	language = {en},
	urldate = {2021-12-04},
	booktitle = {{arXiv}:2101.05068 [cs]},
	author = {Chun, Sanghyuk and Oh, Seong Joon and de Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},
	month = jun,
	year = {2021},
	note = {arXiv: 2101.05068},
	keywords = {Untagged},
}

@inproceedings{croceRobustBenchStandardizedAdversarial2021,
	title = {{RobustBench}: a standardized adversarial robustness benchmark},
	shorttitle = {{RobustBench}},
	url = {http://arxiv.org/abs/2010.09670},
	abstract = {As a research community, we are still lacking a systematic understanding of the progress on adversarial robustness which often makes it hard to identify the most promising ideas in training robust models. A key challenge in benchmarking robustness is that its evaluation is often error-prone leading to robustness overestimation. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reﬂects the robustness of the considered models within a reasonable computational budget. To this end, we start by considering the image classiﬁcation task and introduce restrictions (possibly loosened in the future) on the allowed models. We evaluate adversarial robustness with AutoAttack [28], an ensemble of white- and black-box attacks, which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. To prevent overadaptation of new defenses to AutoAttack, we welcome external evaluations based on adaptive attacks [142], especially where AutoAttack ﬂags a potential overestimation of robustness. Our leaderboard, hosted at https://robustbench.github.io/, contains evaluations of 120+ models and aims at reﬂecting the current state of the art in image classiﬁcation on a set of well-deﬁned tasks in ∞- and 2-threat models and on common corruptions, with possible extensions in the future. Additionally, we open-source the library https://github.com/RobustBench/robustbench that provides uniﬁed access to 80+ robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze the impact of robustness on the performance on distribution shifts, calibration, out-of-distribution detection, fairness, privacy leakage, smoothness, and transferability.},
	language = {en},
	urldate = {2022-04-07},
	booktitle = {{NeurIPS}},
	author = {Croce, Francesco and Andriushchenko, Maksym and Sehwag, Vikash and Debenedetti, Edoardo and Flammarion, Nicolas and Chiang, Mung and Mittal, Prateek and Hein, Matthias},
	month = oct,
	year = {2021},
	note = {arXiv: 2010.09670},
	keywords = {Untagged},
}

@inproceedings{DBLP:conf/aaai/DengH21,
	title = {Graph neural network-based anomaly detection in multivariate time series},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16523},
	booktitle = {Thirty-fifth {AAAI} conference on artificial intelligence, {AAAI} 2021, thirty-third conference on innovative applications of artificial intelligence, {IAAI} 2021, the eleventh symposium on educational advances in artificial intelligence, {EAAI} 2021, virtual event, february 2-9, 2021},
	publisher = {AAAI Press},
	author = {Deng, Ailin and Hooi, Bryan},
	year = {2021},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/aaai/DengH21.bib
tex.timestamp: Wed, 02 Jun 2021 18:09:11 +0200},
	keywords = {Untagged},
	pages = {4027--4035},
}

@article{dhariwalDiffusionModelsBeat2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2105.05233},
	abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\${\textbackslash}times\$128, 4.59 on ImageNet 256\${\textbackslash}times\$256, and 7.72 on ImageNet 512\${\textbackslash}times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\${\textbackslash}times\$256 and 3.85 on ImageNet 512\${\textbackslash}times\$512. We release our code at https://github.com/openai/guided-diffusion},
	urldate = {2021-08-19},
	journal = {arXiv:2105.05233 [cs, stat]},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	month = jun,
	year = {2021},
	note = {arXiv: 2105.05233},
	keywords = {Untagged},
}

@inproceedings{daiDynamicHeadUnifying2021,
	title = {Dynamic {Head}: {Unifying} {Object} {Detection} {Heads} {With} {Attentions}},
	shorttitle = {Dynamic {Head}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Dai_Dynamic_Head_Unifying_Object_Detection_Heads_With_Attentions_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-07-06},
	author = {Dai, Xiyang and Chen, Yinpeng and Xiao, Bin and Chen, Dongdong and Liu, Mengchen and Yuan, Lu and Zhang, Lei},
	year = {2021},
	note = {Github: https://github.com/microsoft/DynamicHead},
	keywords = {Untagged},
	pages = {7373--7382},
}

@article{dattaMachineLearningExplainability2021,
	title = {Machine {Learning} {Explainability} and {Robustness}: {Connected} at the {Hip}},
	doi = {10/gn8x9g},
	abstract = {This tutorial examines the synergistic relationship between explainability methods for machine learning and a significant problem related to model quality: robustness against adversarial perturbations. We begin with a broad overview of approaches to explainable AI, before narrowing our focus to post-hoc explanation methods for predictive models. We discuss perspectives on what constitutes a “good” explanation in various settings, with an emphasis on axiomatic justifications for various explanation methods. In doing so, we will highlight the importance of an explanation method’s faithfulness to the target model, as this property allows one to distinguish between explanations that are unintelligible because of the method used to produce them, and cases where a seemingly poor explanation points to model quality issues. Next, we introduce concepts surrounding adversarial robustness, including adversarial attacks as well as a range of corresponding state-of-the-art defenses. Finally, building on the knowledge presented thus far, we present key insights from the recent literature on the connections between explainability and robustness, showing that many commonly-perceived explainability issues may be caused by non-robust model behavior. Accordingly, a careful study of adversarial examples and robustness can lead to models whose explanations better appeal to human intuition and domain knowledge.},
	language = {en},
	author = {Datta, Anupam},
	year = {2021},
	keywords = {Untagged},
	pages = {2},
}

@inproceedings{dockhornDemystifyingGeneralizingBinaryConnect2021,
	title = {Demystifying and {Generalizing} {BinaryConnect}},
	url = {https://openreview.net/forum?id=FyaSaEbNm1W},
	abstract = {We generalize BinaryConnect and prove convergence for the resulting algorithm.},
	language = {en},
	urldate = {2022-02-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Dockhorn, Tim and Yu, Yaoliang and Sari, Eyyub and Zolnouri, Mahdi and Nia, Vahid Partovi},
	month = may,
	year = {2021},
	keywords = {Untagged},
}

@article{dingRepMLPReparameterizingConvolutions2021,
	title = {{RepMLP}: {Re}-parameterizing {Convolutions} into {Fully}-connected {Layers} for {Image} {Recognition}},
	shorttitle = {{RepMLP}},
	url = {http://arxiv.org/abs/2105.01883},
	abstract = {We propose RepMLP, a multi-layer-perceptron-style neural network building block for image recognition, which is composed of a series of fully-connected (FC) layers. Compared to convolutional layers, FC layers are more efficient, better at modeling the long-range dependencies and positional patterns, but worse at capturing the local structures, hence usually less favored for image recognition. We propose a structural re-parameterization technique that adds local prior into an FC to make it powerful for image recognition. Specifically, we construct convolutional layers inside a RepMLP during training and merge them into the FC for inference. On CIFAR, a simple pure-MLP model shows performance very close to CNN. By inserting RepMLP in traditional CNN, we improve ResNets by 1.8\% accuracy on ImageNet, 2.9\% for face recognition, and 2.3\% mIoU on Cityscapes with lower FLOPs. Our intriguing findings highlight that combining the global representational capacity and positional perception of FC with the local prior of convolution can improve the performance of neural network with faster speed on both the tasks with translation invariance (e.g., semantic segmentation) and those with aligned images and positional patterns (e.g., face recognition). The code and models are available at https://github.com/DingXiaoH/RepMLP.},
	urldate = {2021-08-19},
	journal = {arXiv:2105.01883 [cs]},
	author = {Ding, Xiaohan and Zhang, Xiangyu and Han, Jungong and Ding, Guiguang},
	month = may,
	year = {2021},
	note = {arXiv: 2105.01883},
	keywords = {Untagged},
}

@inproceedings{dingRepVGGMakingVGGStyle2021,
	title = {{RepVGG}: {Making} {VGG}-{Style} {ConvNets} {Great} {Again}},
	shorttitle = {{RepVGG}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Ding_RepVGG_Making_VGG-Style_ConvNets_Great_Again_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-08-19},
	author = {Ding, Xiaohan and Zhang, Xiangyu and Ma, Ningning and Han, Jungong and Ding, Guiguang and Sun, Jian},
	year = {2021},
	keywords = {Untagged},
	pages = {13733--13742},
}

@inproceedings{dosovitskiyImageWorth16x162021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	year = {2021},
	keywords = {Untagged},
}

@article{dongAttentionNotAll2021,
	title = {Attention is {Not} {All} {You} {Need}: {Pure} {Attention} {Loses} {Rank} {Doubly} {Exponentially} with {Depth}},
	shorttitle = {Attention is {Not} {All} {You} {Need}},
	url = {http://arxiv.org/abs/2103.03404},
	abstract = {Attention-based architectures have become ubiquitous in machine learning, yet our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms, each involving the operation of a sequence of attention heads across layers. Using this decomposition, we prove that self-attention possesses a strong inductive bias towards "token uniformity". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the identified convergence phenomena on different variants of standard transformer architectures.},
	urldate = {2021-08-25},
	journal = {arXiv:2103.03404 [cs]},
	author = {Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.03404
version: 1},
	keywords = {Untagged},
}

@misc{faghri_bridging_2021,
	title = {Bridging the {Gap} {Between} {Adversarial} {Robustness} and {Optimization} {Bias}},
	url = {http://arxiv.org/abs/2102.08868},
	doi = {10.48550/arXiv.2102.08868},
	abstract = {We demonstrate that the choice of optimizer, neural network architecture, and regularizer significantly affect the adversarial robustness of linear neural networks, providing guarantees without the need for adversarial training. To this end, we revisit a known result linking maximally robust classifiers and minimum norm solutions, and combine it with recent results on the implicit bias of optimizers. First, we show that, under certain conditions, it is possible to achieve both perfect standard accuracy and a certain degree of robustness, simply by training an overparametrized model using the implicit bias of the optimization. In that regime, there is a direct relationship between the type of the optimizer and the attack to which the model is robust. To the best of our knowledge, this work is the first to study the impact of optimization methods such as sign gradient descent and proximal methods on adversarial robustness. Second, we characterize the robustness of linear convolutional models, showing that they resist attacks subject to a constraint on the Fourier-\${\textbackslash}ell\_{\textbackslash}infty\$ norm. To illustrate these findings we design a novel Fourier-\${\textbackslash}ell\_{\textbackslash}infty\$ attack that finds adversarial examples with controllable frequencies. We evaluate Fourier-\${\textbackslash}ell\_{\textbackslash}infty\$ robustness of adversarially-trained deep CIFAR-10 models from the standard RobustBench benchmark and visualize adversarial perturbations.},
	urldate = {2023-05-11},
	publisher = {arXiv},
	author = {Faghri, Fartash and Gowal, Sven and Vasconcelos, Cristina and Fleet, David J. and Pedregosa, Fabian and Roux, Nicolas Le},
	month = jun,
	year = {2021},
	note = {arXiv:2102.08868 [cs, stat]},
	keywords = {Untagged},
}

@misc{evans_protein_2021,
	title = {Protein complex prediction with {AlphaFold}-{Multimer}},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1},
	doi = {10.1101/2021.10.04.463034},
	abstract = {While the vast majority of well-structured single protein chains can now be predicted to high accuracy due to the recent AlphaFold [1] model, the prediction of multi-chain protein complexes remains a challenge in many cases. In this work, we demonstrate that an AlphaFold model trained specifically for multimeric inputs of known stoichiometry, which we call AlphaFold-Multimer, significantly increases accuracy of predicted multimeric interfaces over input-adapted single-chain AlphaFold while maintaining high intra-chain accuracy. On a benchmark dataset of 17 heterodimer proteins without templates (introduced in [2]) we achieve at least medium accuracy (DockQ [3] ≥ 0.49) on 14 targets and high accuracy (DockQ ≥ 0.8) on 6 targets, compared to 9 targets of at least medium accuracy and 4 of high accuracy for the previous state of the art system (an AlphaFold-based system from [2]). We also predict structures for a large dataset of 4,433 recent protein complexes, from which we score all non-redundant interfaces with low template identity. For heteromeric interfaces we successfully predict the interface (DockQ ≥ 0.23) in 67\% of cases, and produce high accuracy predictions (DockQ ≥ 0.8) in 23\% of cases, an improvement of +25 and +11 percentage points over the flexible linker modification of AlphaFold [4] respectively. For homomeric interfaces we successfully predict the interface in 69\% of cases, and produce high accuracy predictions in 34\% of cases, an improvement of +5 percentage points in both instances.},
	language = {en},
	urldate = {2023-04-16},
	publisher = {bioRxiv},
	author = {Evans, Richard and O’Neill, Michael and Pritzel, Alexander and Antropova, Natasha and Senior, Andrew and Green, Tim and Žídek, Augustin and Bates, Russ and Blackwell, Sam and Yim, Jason and Ronneberger, Olaf and Bodenstein, Sebastian and Zielinski, Michal and Bridgland, Alex and Potapenko, Anna and Cowie, Andrew and Tunyasuvunakool, Kathryn and Jain, Rishub and Clancy, Ellen and Kohli, Pushmeet and Jumper, John and Hassabis, Demis},
	month = oct,
	year = {2021},
	note = {Pages: 2021.10.04.463034
Section: New Results},
	keywords = {Untagged},
}

@misc{edward_j_hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full ﬁne-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁnetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	language = {en},
	urldate = {2023-04-08},
	publisher = {arXiv},
	author = {{Edward J. Hu} and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Untagged},
}

@article{el-noubyXCiTCrossCovarianceImage2021,
	title = {{XCiT}: {Cross}-{Covariance} {Image} {Transformers}},
	shorttitle = {{XCiT}},
	url = {http://arxiv.org/abs/2106.09681},
	abstract = {Following their success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens ,i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a "transposed" version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efficient processing of high-resolution images. Our cross-covariance image transformer (XCiT) is built upon XCA. It combines the accuracy of conventional transformers with the scalability of convolutional architectures. We validate the effectiveness and generality of XCiT by reporting excellent results on multiple vision benchmarks, including image classification and self-supervised feature learning on ImageNet-1k, object detection and instance segmentation on COCO, and semantic segmentation on ADE20k.},
	urldate = {2021-08-12},
	journal = {arXiv:2106.09681 [cs]},
	author = {El-Nouby, Alaaeldin and Touvron, Hugo and Caron, Mathilde and Bojanowski, Piotr and Douze, Matthijs and Joulin, Armand and Laptev, Ivan and Neverova, Natalia and Synnaeve, Gabriel and Verbeek, Jakob and Jegou, Hervé},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.09681},
	keywords = {Untagged},
}

@misc{formal_splade_2021,
	title = {{SPLADE} v2: {Sparse} {Lexical} and {Expansion} {Model} for {Information} {Retrieval}},
	shorttitle = {{SPLADE} v2},
	url = {http://arxiv.org/abs/2109.10086},
	abstract = {In neural Information Retrieval (IR), ongoing research is directed towards improving the first retriever in ranking pipelines. Learning dense embeddings to conduct retrieval using efficient approximate nearest neighbors methods has proven to work well. Meanwhile, there has been a growing interest in learning sparse representations for documents and queries, that could inherit from the desirable properties of bag-of-words models such as the exact matching of terms and the efficiency of inverted indexes. Introduced recently, the SPLADE model provides highly sparse representations and competitive results with respect to state-of-the-art dense and sparse approaches. In this paper, we build on SPLADE and propose several significant improvements in terms of effectiveness and/or efficiency. More specifically, we modify the pooling mechanism, benchmark a model solely based on document expansion, and introduce models trained with distillation. We also report results on the BEIR benchmark. Overall, SPLADE is considerably improved with more than 9\% gains on NDCG@10 on TREC DL 2019, leading to state-of-the-art results on the BEIR benchmark.},
	language = {en},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Formal, Thibault and Lassance, Carlos and Piwowarski, Benjamin and Clinchant, Stéphane},
	month = sep,
	year = {2021},
	note = {arXiv:2109.10086 [cs]},
	keywords = {Untagged},
}

@misc{fangCLIP2VideoMasteringVideoText2021,
	title = {{CLIP2Video}: {Mastering} {Video}-{Text} {Retrieval} via {Image} {CLIP}},
	shorttitle = {{CLIP2Video}},
	url = {http://arxiv.org/abs/2106.11097},
	doi = {10.48550/arXiv.2106.11097},
	abstract = {We present CLIP2Video network to transfer the image-language pre-training model to video-text retrieval in an end-to-end manner. Leading approaches in the domain of video-and-language learning try to distill the spatio-temporal video features and multi-modal interaction between videos and languages from a large-scale video-text dataset. Different from them, we leverage pretrained image-language model, simplify it as a two-stage framework with co-learning of image-text and enhancing temporal relations between video frames and video-text respectively, make it able to train on comparatively small datasets. Specifically, based on the spatial semantics captured by Contrastive Language-Image Pretraining (CLIP) model, our model involves a Temporal Difference Block to capture motions at fine temporal video frames, and a Temporal Alignment Block to re-align the tokens of video clips and phrases and enhance the multi-modal correlation. We conduct thorough ablation studies, and achieve state-of-the-art performance on major text-to-video and video-to-text retrieval benchmarks, including new records of retrieval accuracy on MSR-VTT, MSVD and VATEX.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Fang, Han and Xiong, Pengfei and Xu, Luhui and Chen, Yu},
	month = jun,
	year = {2021},
	note = {arXiv:2106.11097 [cs]},
	keywords = {Untagged},
}

@inproceedings{fangCompressingVisualLinguisticModel2021,
	title = {Compressing {Visual}-{Linguistic} {Model} via {Knowledge} {Distillation}},
	abstract = {Despite exciting progress in pre-training for visuallinguistic (VL) representations, very few aspire to a small VL model. In this paper, we study knowledge distillation (KD) to effectively compress a transformer based large VL model into a small VL model. The major challenge arises from the inconsistent regional visual tokens extracted from different detectors of Teacher and Student, resulting in the misalignment of hidden representations and attention distributions. To address the problem, we retrain and adapt the Teacher by using the same region proposals from Student’s detector while the features are from Teacher’s own object detector. With aligned network inputs, the adapted Teacher is capable of transferring the knowledge through the intermediate representations. Specifically, we use the mean square error loss to mimic the attention distribution inside the transformer block, and present a token-wise noise contrastive loss to align the hidden state by contrasting with negative representations stored in a sample queue. To this end, we show that our proposed distillation significantly improves the performance of small VL models on image captioning and visual question answering tasks. It reaches 120.8 in CIDEr score on COCO captioning, an improvement of 5.1 over its non-distilled counterpart; and an accuracy of 69.8 on VQA 2.0, a 0.8 gain from the baseline. Our extensive experiments and ablations confirm the effectiveness of VL distillation in both pre-training and fine-tuning stages.},
	language = {en},
	booktitle = {{ICCV}},
	author = {Fang, Zhiyuan and Wang, Jianfeng and Hu, Xiaowei and Wang, Lijuan and Yang, Yezhou and Liu, Zicheng},
	year = {2021},
	keywords = {Untagged},
	pages = {11},
}

@article{fangExploringDeepNeural2021,
	title = {Exploring {Deep} {Neural} {Networks} via {Layer}-{Peeled} {Model}: {Minority} {Collapse} in {Imbalanced} {Training}},
	shorttitle = {Exploring {Deep} {Neural} {Networks} via {Layer}-{Peeled} {Model}},
	url = {http://arxiv.org/abs/2101.12699},
	abstract = {In this paper, we introduce the {\textbackslash}textit\{Layer-Peeled Model\}, a nonconvex yet analytically tractable optimization program, in a quest to better understand deep neural networks that are trained for a sufficiently long time. As the name suggests, this new model is derived by isolating the topmost layer from the remainder of the neural network, followed by imposing certain constraints separately on the two parts of the network. We demonstrate that the Layer-Peeled Model, albeit simple, inherits many characteristics of well-trained neural networks, thereby offering an effective tool for explaining and predicting common empirical patterns of deep learning training. First, when working on class-balanced datasets, we prove that any solution to this model forms a simplex equiangular tight frame, which in part explains the recently discovered phenomenon of neural collapse {\textbackslash}cite\{papyan2020prevalence\}. More importantly, when moving to the imbalanced case, our analysis of the Layer-Peeled Model reveals a hitherto unknown phenomenon that we term {\textbackslash}textit\{Minority Collapse\}, which fundamentally limits the performance of deep learning models on the minority classes. In addition, we use the Layer-Peeled Model to gain insights into how to mitigate Minority Collapse. Interestingly, this phenomenon is first predicted by the Layer-Peeled Model before being confirmed by our computational experiments.},
	language = {en},
	urldate = {2022-02-22},
	journal = {arXiv:2101.12699 [cs, math, stat]},
	author = {Fang, Cong and He, Hangfeng and Long, Qi and Su, Weijie J.},
	month = sep,
	year = {2021},
	note = {arXiv: 2101.12699},
	keywords = {Untagged},
}

@inproceedings{gao_coil_2021,
	address = {Online},
	title = {{COIL}: {Revisit} {Exact} {Lexical} {Match} in {Information} {Retrieval} with {Contextualized} {Inverted} {List}},
	shorttitle = {{COIL}},
	url = {https://aclanthology.org/2021.naacl-main.241},
	doi = {10.18653/v1/2021.naacl-main.241},
	language = {en},
	urldate = {2023-04-06},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Gao, Luyu and Dai, Zhuyun and Callan, Jamie},
	year = {2021},
	keywords = {Untagged},
	pages = {3030--3042},
}

@article{gorslineAdversarialRobustnessQuantized2021,
	title = {On the {Adversarial} {Robustness} of {Quantized} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2105.00227},
	doi = {10.1145/3453688.3461755},
	abstract = {Reducing the size of neural network models is a critical step in moving AI from a cloud-centric to an edge-centric (i.e. on-device) compute paradigm. This shift from cloud to edge is motivated by a number of factors including reduced latency, improved security, and higher flexibility of AI algorithms across several application domains (e.g. transportation, healthcare, defense, etc.). However, it is currently unclear how model compression techniques may affect the robustness of AI algorithms against adversarial attacks. This paper explores the effect of quantization, one of the most common compression techniques, on the adversarial robustness of neural networks. Specifically, we investigate and model the accuracy of quantized neural networks on adversarially-perturbed images. Results indicate that for simple gradient-based attacks, quantization can either improve or degrade adversarial robustness depending on the attack strength.},
	urldate = {2022-02-28},
	journal = {Proceedings of the 2021 on Great Lakes Symposium on VLSI},
	author = {Gorsline, Micah and Smith, James and Merkel, Cory},
	month = jun,
	year = {2021},
	note = {arXiv: 2105.00227},
	keywords = {Untagged},
	pages = {189--194},
}

@article{fuDeepMomentumUncertainty2021,
	title = {Deep {Momentum} {Uncertainty} {Hashing}},
	url = {http://arxiv.org/abs/2009.08012},
	abstract = {Discrete optimization is one of the most intractable problems in deep hashing. Previous methods usually mitigate this problem by binary approximation, substituting binary codes for real-values via activation functions or regularizations. However, such approximation leads to uncertainty between real-values and binary ones, degrading retrieval performance. In this paper, we propose a novel Deep Momentum Uncertainty Hashing (DMUH). It explicitly estimates the uncertainty during training and leverages the uncertainty information to guide the approximation process. Specifically, we model bit-level uncertainty via measuring the discrepancy between the output of a hashing network and that of a momentum-updated network. The discrepancy of each bit indicates the uncertainty of the hashing network to the approximate output of that bit. Meanwhile, the mean discrepancy of all bits in a hashing code can be regarded as image-level uncertainty. It embodies the uncertainty of the hashing network to the corresponding input image. The hashing bit and image with higher uncertainty are paid more attention during optimization. To the best of our knowledge, this is the first work to study the uncertainty in hashing bits. Extensive experiments are conducted on four datasets to verify the superiority of our method, including CIFAR-10, NUS-WIDE, MS-COCO, and a million-scale dataset Clothing1M. Our method achieves the best performance on all of the datasets and surpasses existing state-of-the-art methods by a large margin, especially on Clothing1M.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2009.08012 [cs]},
	author = {Fu, Chaoyou and Wang, Guoli and Wu, Xiang and Zhang, Qian and He, Ran},
	month = jan,
	year = {2021},
	note = {arXiv: 2009.08012},
	keywords = {Untagged},
}

@article{ganPlayingLotteryTickets2021,
	title = {Playing {Lottery} {Tickets} with {Vision} and {Language}},
	volume = {abs/2104.11832},
	url = {https://arxiv.org/abs/2104.11832},
	journal = {CoRR},
	author = {Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Chen, Tianlong and Cheng, Yu and Wang, Shuohang and Liu, Jingjing},
	year = {2021},
	note = {arXiv: 2104.11832},
	keywords = {Untagged},
}

@inproceedings{gaoPrivacyPreservingCollaborativeLearning2021,
	title = {Privacy-{Preserving} {Collaborative} {Learning} {With} {Automatic} {Transformation} {Search}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Gao_Privacy-Preserving_Collaborative_Learning_With_Automatic_Transformation_Search_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-11-19},
	author = {Gao, Wei and Guo, Shangwei and Zhang, Tianwei and Qiu, Han and Wen, Yonggang and Liu, Yang},
	year = {2021},
	keywords = {Untagged},
	pages = {114--123},
}

@article{gupta_bert_2021,
	title = {{BERT} \& {Family} {Eat} {Word} {Salad}: {Experiments} with {Text} {Understanding}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{BERT} \& {Family} {Eat} {Word} {Salad}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17531},
	doi = {10.1609/aaai.v35i14.17531},
	abstract = {In this paper, we study the response of large models from the BERT family to incoherent inputs that should confuse any model that claims to understand natural language. We define simple heuristics to construct such examples. Our experiments show that state-of-the-art models consistently fail to recognize them as ill-formed, and instead produce high confidence predictions on them. As a consequence of this phenomenon, models trained on sentences with randomly permuted word order perform close to state-of-the-art models. To alleviate these issues, we show that if models are explicitly trained to recognize invalid inputs, they can be robust to such attacks without a drop in performance.},
	language = {en},
	number = {14},
	urldate = {2023-04-12},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Gupta, Ashim and Kvernadze, Giorgi and Srikumar, Vivek},
	month = may,
	year = {2021},
	note = {Number: 14},
	keywords = {Untagged},
	pages = {12946--12954},
}

@inproceedings{gulrajani2021in,
	title = {In search of lost domain generalization},
	url = {https://openreview.net/forum?id=lQdXeXDoWtI},
	booktitle = {International conference on learning representations},
	author = {Gulrajani, Ishaan and Lopez-Paz, David},
	year = {2021},
	keywords = {Untagged},
}

@article{guoSelfattentionExternalAttention2021,
	title = {Beyond {Self}-attention: {External} {Attention} using {Two} {Linear} {Layers} for {Visual} {Tasks}},
	shorttitle = {Beyond {Self}-attention},
	url = {http://arxiv.org/abs/2105.02358},
	abstract = {Attention mechanisms, especially self-attention, have played an increasingly important role in deep feature representation for visual tasks. Self-attention updates the feature at each position by computing a weighted sum of features using pair-wise affinities across all positions to capture the long-range dependency within a single sample. However, self-attention has quadratic complexity and ignores potential correlation between different samples. This paper proposes a novel attention mechanism which we call external attention, based on two external, small, learnable, shared memories, which can be implemented easily by simply using two cascaded linear layers and two normalization layers; it conveniently replaces self-attention in existing popular architectures. External attention has linear complexity and implicitly considers the correlations between all data samples. We further incorporate the multi-head mechanism into external attention to provide an all-MLP architecture, external attention MLP (EAMLP), for image classification. Extensive experiments on image classification, object detection, semantic segmentation, instance segmentation, image generation, and point cloud analysis reveal that our method provides results comparable or superior to the self-attention mechanism and some of its variants, with much lower computational and memory costs.},
	urldate = {2021-08-18},
	journal = {arXiv:2105.02358 [cs]},
	author = {Guo, Meng-Hao and Liu, Zheng-Ning and Mu, Tai-Jiang and Hu, Shi-Min},
	month = may,
	year = {2021},
	note = {arXiv: 2105.02358},
	keywords = {Untagged},
}

@article{gouKnowledgeDistillationSurvey2021,
	title = {Knowledge {Distillation}: {A} {Survey}},
	volume = {129},
	issn = {1573-1405},
	shorttitle = {Knowledge {Distillation}},
	url = {https://doi.org/10.1007/s11263-021-01453-z},
	doi = {10/gksj5g},
	abstract = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher–student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.},
	language = {en},
	number = {6},
	urldate = {2021-12-23},
	journal = {International Journal of Computer Vision},
	author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},
	month = jun,
	year = {2021},
	keywords = {Untagged},
	pages = {1789--1819},
}

@misc{he_open_2021,
	title = {Open {Set} {Domain} {Recognition} via {Attention}-{Based} {GCN} and {Semantic} {Matching} {Optimization}},
	url = {http://arxiv.org/abs/2105.04967},
	abstract = {Open set domain recognition has got the attention in recent years. The task aims to specifically classify each sample in the practical unlabeled target domain, which consists of all known classes in the manually labeled source domain and target-specific unknown categories. The absence of annotated training data or auxiliary attribute information for unknown categories makes this task especially difficult. Moreover, exiting domain discrepancy in label space and data distribution further distracts the knowledge transferred from known classes to unknown classes. To address these issues, this work presents an end-to-end model based on attention-based GCN and semantic matching optimization, which first employs the attention mechanism to enable the central node to learn more discriminating representations from its neighbors in the knowledge graph. Moreover, a coarse-to-fine semantic matching optimization approach is proposed to progressively bridge the domain gap. Experimental results validate that the proposed model not only has superiority on recognizing the images of known and unknown classes, but also can adapt to various openness of the target domain.},
	urldate = {2023-05-11},
	publisher = {arXiv},
	author = {He, Xinxing and Yuan, Yuan and Jiang, Zhiyu},
	month = may,
	year = {2021},
	note = {arXiv:2105.04967 [cs]},
	keywords = {Untagged},
}

@inproceedings{hanzlikMLCapsuleGuardedOffline2021,
	address = {Nashville, TN, USA},
	title = {{MLCapsule}: {Guarded} {Offline} {Deployment} of {Machine} {Learning} as a {Service}},
	isbn = {978-1-66544-899-4},
	shorttitle = {{MLCapsule}},
	url = {https://ieeexplore.ieee.org/document/9523029/},
	doi = {10.1109/CVPRW53098.2021.00368},
	abstract = {Machine Learning as a Service (MLaaS) is a popular and convenient way to access a trained machine learning (ML) model trough an API. However, if the user’s input is sensitive, sending it to the server is not an option. Equally, the service provider does not want to share the model by sending it to the client for protecting its intellectual property and pay-per-query business model. As a solution, we propose MLCapsule, a guarded oﬄine deployment of MLaaS. MLCapsule executes the machine learning model locally on the user’s client and therefore the data never leaves the client. Meanwhile, we show that MLCapsule is able to oﬀer the service provider the same level of control and security of its model as the commonly used server-side execution. Beyond protecting against direct model access, we demonstrate that MLCapsule allows for implementing defenses against advanced attacks on machine learning models such as model stealing, reverse engineering and membership inference.},
	language = {en},
	urldate = {2022-03-21},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Hanzlik, Lucjan and Zhang, Yang and Grosse, Kathrin and Salem, Ahmed and Augustin, Maximilian and Backes, Michael and Fritz, Mario},
	month = jun,
	year = {2021},
	keywords = {Untagged},
	pages = {3295--3304},
}

@article{heRecentAdvancesDeep2021,
	title = {Recent advances in deep learning theory},
	url = {http://arxiv.org/abs/2012.10931},
	abstract = {Deep learning is usually described as an experiment-driven field under continuous criticizes of lacking theoretical foundations. This problem has been partially fixed by a large volume of literature which has so far not been well organized. This paper reviews and organizes the recent advances in deep learning theory. The literature is categorized in six groups: (1) complexity and capacity-based approaches for analyzing the generalizability of deep learning; (2) stochastic differential equations and their dynamic systems for modelling stochastic gradient descent and its variants, which characterize the optimization and generalization of deep learning, partially inspired by Bayesian inference; (3) the geometrical structures of the loss landscape that drives the trajectories of the dynamic systems; (4) the roles of over-parameterization of deep neural networks from both positive and negative perspectives; (5) theoretical foundations of several special structures in network architectures; and (6) the increasingly intensive concerns in ethics and security and their relationships with generalizability.},
	urldate = {2021-08-02},
	journal = {arXiv:2012.10931 [cs, stat]},
	author = {He, Fengxiang and Tao, Dacheng},
	month = mar,
	year = {2021},
	note = {arXiv: 2012.10931},
	keywords = {Untagged},
}

@article{hanSurveyVisualTransformer2021,
	title = {A {Survey} on {Visual} {Transformer}},
	url = {http://arxiv.org/abs/2012.12556},
	abstract = {Transformer, ﬁrst applied to the ﬁeld of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent networks. Given its high performance and no need for human-deﬁned inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these visual transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Furthermore, we include efﬁcient transformer methods for pushing transformer into real device-based applications. Toward the end of this paper, we discuss the challenges and provide several further research directions for visual transformers.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2012.12556 [cs]},
	author = {Han, Kai and Wang, Yunhe and Chen, Hanting and Chen, Xinghao and Guo, Jianyuan and Liu, Zhenhua and Tang, Yehui and Xiao, An and Xu, Chunjing and Xu, Yixing and Yang, Zhaohui and Zhang, Yiman and Tao, Dacheng},
	month = jan,
	year = {2021},
	note = {arXiv: 2012.12556},
	keywords = {Untagged},
}

@article{huSourceInferenceAttacks2021,
	title = {Source {Inference} {Attacks} in {Federated} {Learning}},
	url = {http://arxiv.org/abs/2109.05659},
	abstract = {Federated learning (FL) has emerged as a promising privacy-aware paradigm that allows multiple clients to jointly train a model without sharing their private data. Recently, many studies have shown that FL is vulnerable to membership inference attacks (MIAs) that can distinguish the training members of the given model from the non-members. However, existing MIAs ignore the source of a training member, i.e., the information of which client owns the training member, while it is essential to explore source privacy in FL beyond membership privacy of examples from all clients. The leakage of source information can lead to severe privacy issues. For example, identification of the hospital contributing to the training of an FL model for COVID-19 pandemic can render the owner of a data record from this hospital more prone to discrimination if the hospital is in a high risk region. In this paper, we propose a new inference attack called source inference attack (SIA), which can derive an optimal estimation of the source of a training member. Specifically, we innovatively adopt the Bayesian perspective to demonstrate that an honest-but-curious server can launch an SIA to steal non-trivial source information of the training members without violating the FL protocol. The server leverages the prediction loss of local models on the training members to achieve the attack effectively and non-intrusively. We conduct extensive experiments on one synthetic and five real datasets to evaluate the key factors in an SIA, and the results show the efficacy of the proposed source inference attack.},
	urldate = {2022-03-21},
	journal = {arXiv:2109.05659 [cs]},
	author = {Hu, Hongsheng and Salcic, Zoran and Sun, Lichao and Dobbie, Gillian and Zhang, Xuyun},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.05659},
	keywords = {Untagged},
}

@inproceedings{huAdCoAdversarialContrast2021,
	title = {{AdCo}: {Adversarial} {Contrast} for {Efficient} {Learning} of {Unsupervised} {Representations} {From} {Self}-{Trained} {Negative} {Adversaries}},
	shorttitle = {{AdCo}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Hu_AdCo_Adversarial_Contrast_for_Efficient_Learning_of_Unsupervised_Representations_From_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-07-06},
	author = {Hu, Qianjiang and Wang, Xiao and Hu, Wei and Qi, Guo-Jun},
	year = {2021},
	keywords = {Untagged},
	pages = {1074--1083},
}

@article{heClassificationNeurofibromatosisrelatedDystrophic2021,
	title = {Classification of neurofibromatosis-related dystrophic or nondystrophic scoliosis based on image features using {Bilateral} {CNN}},
	volume = {48},
	url = {https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.14719},
	doi = {10/gj45ds},
	abstract = {Purpose We developed a system that can automatically classify cases of scoliosis secondary to neurofibromatosis type 1 (NF1-S) using deep learning algorithms (DLAs) and improve the accuracy and effectiveness of classification, thereby assisting surgeons with the auxiliary diagnosis. Methods Comprehensive experiments in NF1 classification were performed based on a dataset consisting 211 NF1-S (131 dystrophic and 80 nondystrophic NF1-S) patients. Additionally, 100 congenital scoliosis (CS), 100 adolescent idiopathic scoliosis (AIS) patients, and 114 normal controls were used for experiments in primary classification. For identification of NF1-S with nondystrophic or dystrophic curves, we devised a novel network (i.e., Bilateral convolutional neural network [CNN]) utilizing a bilinear-like operation to discover the similar interest features between whole spine AP and lateral x-ray images. The performance of Bilateral CNN was compared with spine surgeons, conventional DLAs (i.e., VGG-16, ResNet-50, and Bilinear CNN [BCNN]), recently proposed DLAs (i.e., ShuffleNet, MobileNet, and EfficientNet), and Two-path BCNN which was the extension of BCNN using AP and lateral x-ray images as inputs. Results In NF1 classification, our proposed Bilateral CNN with 80.36\% accuracy outperformed the other seven DLAs ranging from 61.90\% to 76.19\% with fivefold cross-validation. It also outperformed the spine surgeons (with an average accuracy of 77.5\% for the senior surgeons and 65.0\% for the junior surgeons). Our method is highly generalizable due to the proposed methodology and data augmentation. Furthermore, the heatmaps extracted by Bilateral CNN showed curve pattern and morphology of ribs and vertebrae contributing most to the classification results. In primary classification, our proposed method with an accuracy of 87.92\% also outperformed all the other methods with varied accuracies between 52.58\% and 83.35\% with fivefold cross-validation. Conclusions The proposed Bilateral CNN can automatically capture representative features for classifying NF1-S utilizing AP and lateral x-ray images, leading to a relatively good performance. Moreover, the proposed method can identify other spine deformities for auxiliary diagnosis.},
	number = {4},
	journal = {Medical Physics},
	author = {He, Zhong and Wang, Yimu and Qin, Xiaodong and Yin, Rui and Qiu, Yong and He, Kelei and Zhu, Zezhang},
	year = {2021},
	note = {\_eprint: https://aapm.onlinelibrary.wiley.com/doi/pdf/10.1002/mp.14719},
	keywords = {Untagged},
	pages = {1571--1583},
}

@article{hongFederatedLearningDynamic2021,
	title = {Federated {Learning} with {Dynamic} {Transformer} for {Text} to {Speech}},
	url = {http://arxiv.org/abs/2107.08795},
	abstract = {Text to speech (TTS) is a crucial task for user interaction, but TTS model training relies on a sizable set of high-quality original datasets. Due to privacy and security issues, the original datasets are usually unavailable directly. Recently, federated learning proposes a popular distributed machine learning paradigm with an enhanced privacy protection mechanism. It offers a practical and secure framework for data owners to collaborate with others, thus obtaining a better global model trained on the larger dataset. However, due to the high complexity of transformer models, the convergence process becomes slow and unstable in the federated learning setting. Besides, the transformer model trained in federated learning is costly communication and limited computational speed on clients, impeding its popularity. To deal with these challenges, we propose the federated dynamic transformer. On the one hand, the performance is greatly improved comparing with the federated transformer, approaching centralize-trained Transformer-TTS when increasing clients number. On the other hand, it achieves faster and more stable convergence in the training phase and signiﬁcantly reduces communication time. Experiments on the LJSpeech dataset also strongly prove our method’s advantage.},
	language = {en},
	urldate = {2021-11-16},
	journal = {arXiv:2107.08795 [cs]},
	author = {Hong, Zhenhou and Wang, Jianzong and Qu, Xiaoyang and Liu, Jie and Zhao, Chendong and Xiao, Jing},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.08795},
	keywords = {Untagged},
}

@article{hongRecurrentVisionandLanguageBERT2021,
	title = {A {Recurrent} {Vision}-and-{Language} {BERT} for {Navigation}},
	url = {http://arxiv.org/abs/2011.13922},
	abstract = {Accuracy of many visiolinguistic tasks has beneﬁted signiﬁcantly from the application of vision-and-language (V\&L) BERT. However, its application for the task of visionand-language navigation (VLN) remains limited. One reason for this is the difﬁculty adapting the BERT architecture to the partially observable Markov decision process present in VLN, requiring history-dependent attention and decision making. In this paper we propose a recurrent BERT model that is time-aware for use in VLN. Speciﬁcally, we equip the BERT model with a recurrent function that maintains cross-modal state information for the agent. Through extensive experiments on R2R and REVERIE we demonstrate that our model can replace more complex encoder-decoder models to achieve state-of-the-art results. Moreover, our approach can be generalised to other transformer-based architectures, supports pre-training, and is capable of solving navigation and referring expression tasks simultaneously.},
	language = {en},
	urldate = {2021-10-22},
	journal = {arXiv:2011.13922 [cs]},
	author = {Hong, Yicong and Wu, Qi and Qi, Yuankai and Rodriguez-Opazo, Cristian and Gould, Stephen},
	month = mar,
	year = {2021},
	note = {arXiv: 2011.13922},
	keywords = {Untagged},
}

@inproceedings{huang_seeing_2021,
	address = {Nashville, TN, USA},
	title = {Seeing {Out} of {tHe} {bOx}: {End}-to-{End} {Pre}-training for {Vision}-{Language} {Representation} {Learning}},
	isbn = {978-1-66544-509-2},
	shorttitle = {Seeing {Out} of {tHe} {bOx}},
	url = {https://ieeexplore.ieee.org/document/9577775/},
	doi = {10.1109/CVPR46437.2021.01278},
	abstract = {We study joint learning of Convolutional Neural Network (CNN) and Transformer for vision-language pre-training (VLPT) which aims to learn cross-modal alignments from millions of image-text pairs. State-of-the-art approaches extract salient image regions and align regions with words step-by-step. As region-based visual features usually represent parts of an image, it is challenging for existing visionlanguage models to fully understand the semantics from paired natural languages. In this paper, we propose SOHO to “Seeing Out of tHe bOx” that takes a whole image as input, and learns vision-language representation in an endto-end manner. SOHO does not require bounding box annotations which enables inference 10 times faster than regionbased approaches. In particular, SOHO learns to extract comprehensive yet compact image features through a visual dictionary (VD) that facilitates cross-modal understanding. VD is designed to represent consistent visual abstractions of similar semantics. It is updated on-the-ﬂy and utilized in our proposed pre-training task Masked Visual Modeling (MVM). We conduct experiments on four well-established vision-language tasks by following standard VLPT settings. In particular, SOHO achieves absolute gains of 2.0\% R@1 score on MSCOCO text retrieval 5k test split, 1.5\% accuracy on NLVR2 test-P split, 6.7\% accuracy on SNLI-VE test split, respectively.},
	language = {en},
	urldate = {2022-10-05},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Huang, Zhicheng and Zeng, Zhaoyang and Huang, Yupan and Liu, Bei and Fu, Dongmei and Fu, Jianlong},
	year = {2021},
	keywords = {Untagged},
	pages = {12971--12980},
}

@article{huiPracticalBlindMembership2021,
	title = {Practical {Blind} {Membership} {Inference} {Attack} via {Differential} {Comparisons}},
	url = {http://arxiv.org/abs/2101.01341},
	abstract = {Membership inference (MI) attacks affect user privacy by inferring whether given data samples have been used to train a target learning model, e.g., a deep neural network. There are two types of MI attacks in the literature, i.e., these with and without shadow models. The success of the former heavily depends on the quality of the shadow model, i.e., the transferability between the shadow and the target; the latter, given only blackbox probing access to the target model, cannot make an effective inference of unknowns, compared with MI attacks using shadow models, due to the insufficient number of qualified samples labeled with ground truth membership information. In this paper, we propose an MI attack, called BlindMI, which probes the target model and extracts membership semantics via a novel approach, called differential comparison. The high-level idea is that BlindMI first generates a dataset with nonmembers via transforming existing samples into new samples, and then differentially moves samples from a target dataset to the generated, non-member set in an iterative manner. If the differential move of a sample increases the set distance, BlindMI considers the sample as non-member and vice versa. BlindMI was evaluated by comparing it with state-of-the-art MI attack algorithms. Our evaluation shows that BlindMI improves F1-score by nearly 20\% when compared to state-of-the-art on some datasets, such as Purchase-50 and Birds-200, in the blind setting where the adversary does not know the target model's architecture and the target dataset's ground truth labels. We also show that BlindMI can defeat state-of-the-art defenses.},
	urldate = {2022-03-21},
	journal = {arXiv:2101.01341 [cs]},
	author = {Hui, Bo and Yang, Yuchen and Yuan, Haolin and Burlina, Philippe and Gong, Neil Zhenqiang and Cao, Yinzhi},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.01341},
	keywords = {Untagged},
}

@article{huangSelfadaptiveTrainingBridging2021,
	title = {Self-adaptive training: bridging the supervised and self-supervised learning},
	shorttitle = {Self-adaptive training},
	journal = {arXiv preprint arXiv:2101.08732},
	author = {Huang, Lang and Zhang, Chao and Zhang, Hongyang},
	year = {2021},
	keywords = {Untagged},
}

@inproceedings{Hu_2021_ICCV,
	title = {{UniT}: {Multimodal} multitask learning with a unified transformer},
	booktitle = {Proceedings of the {IEEE}/{CVF} international conference on computer vision ({ICCV})},
	author = {Hu, Ronghang and Singh, Amanpreet},
	month = oct,
	year = {2021},
	keywords = {Untagged},
	pages = {1439--1449},
}

@inproceedings{hwang_adversarial_2021,
	title = {Adversarial {Training} {With} {Stochastic} {Weight} {Average}},
	doi = {10.1109/ICIP42928.2021.9506548},
	abstract = {Although adversarial training is the most reliable method to train robust deep neural networks so far, adversarially trained networks still show large gap between their accuracies on clean images and those on adversarial images. In conventional classification problem, one can gain higher accuracy by ensembling multiple networks. However, in adversarial training, there are obstacles to adopt such ensemble method. First, as inner maximization is expensive, training multiple networks adversarially becomes overburden. Moreover, the naive ensemble faces dilemma on choosing target model to generate adversarial examples with. Training adversarial examples of the members causes covariate shift, while training those of ensemble diminishes the benefit of ensembling. With these insights, we adopt stochastic weight average methods and improve it by considering overfitting nature of adversarial training. Our method take the benefit of ensemble while avoiding the described problems. Experiments on CIFAR10 and CIFAR100 shows our method improves the robustness effectively.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Hwang, Joong-won and Lee, Youngwan and Oh, Sungchan and Bae, Yuseok},
	month = sep,
	year = {2021},
	note = {ISSN: 2381-8549},
	keywords = {Untagged},
	pages = {814--818},
}

@inproceedings{hwang_exemplar-based_2021,
	title = {Exemplar-{Based} {Open}-{Set} {Panoptic} {Segmentation} {Network}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Hwang_Exemplar-Based_Open-Set_Panoptic_Segmentation_Network_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-11-15},
	author = {Hwang, Jaedong and Oh, Seoung Wug and Lee, Joon-Young and Han, Bohyung},
	year = {2021},
	keywords = {Untagged},
	pages = {1175--1184},
}

@article{jayaramanRevisitingMembershipInference2021,
	title = {Revisiting {Membership} {Inference} {Under} {Realistic} {Assumptions}},
	url = {http://arxiv.org/abs/2005.10881},
	abstract = {We study membership inference in settings where some of the assumptions typically used in previous research are relaxed. First, we consider skewed priors, to cover cases such as when only a small fraction of the candidate pool targeted by the adversary are actually members and develop a PPV-based metric suitable for this setting. This setting is more realistic than the balanced prior setting typically considered by researchers. Second, we consider adversaries that select inference thresholds according to their attack goals and develop a threshold selection procedure that improves inference attacks. Since previous inference attacks fail in imbalanced prior setting, we develop a new inference attack based on the intuition that inputs corresponding to training set members will be near a local minimum in the loss function, and show that an attack that combines this with thresholds on the per-instance loss can achieve high PPV even in settings where other attacks appear to be ineffective. Code for our experiments can be found here: https://github.com/bargavj/EvaluatingDPML.},
	urldate = {2022-03-21},
	journal = {arXiv:2005.10881 [cs, stat]},
	author = {Jayaraman, Bargav and Wang, Lingxiao and Knipmeyer, Katherine and Gu, Quanquan and Evans, David},
	month = jan,
	year = {2021},
	note = {arXiv: 2005.10881},
	keywords = {Untagged},
}

@article{jeongFederatedSemiSupervisedLearning2021,
	title = {Federated {Semi}-{Supervised} {Learning} with {Inter}-{Client} {Consistency} \& {Disjoint} {Learning}},
	url = {http://arxiv.org/abs/2006.12097},
	abstract = {While existing federated learning approaches mostly require that clients have fully-labeled data to train on, in realistic settings, data obtained at the client-side often comes without any accompanying labels. Such deficiency of labels may result from either high labeling cost, or difficulty of annotation due to the requirement of expert knowledge. Thus the private data at each client may be either partly labeled, or completely unlabeled with labeled data being available only at the server, which leads us to a new practical federated learning problem, namely Federated Semi-Supervised Learning (FSSL). In this work, we study two essential scenarios of FSSL based on the location of the labeled data. The first scenario considers a conventional case where clients have both labeled and unlabeled data (labels-at-client), and the second scenario considers a more challenging case, where the labeled data is only available at the server (labels-at-server). We then propose a novel method to tackle the problems, which we refer to as Federated Matching (FedMatch). FedMatch improves upon naive combinations of federated learning and semi-supervised learning approaches with a new inter-client consistency loss and decomposition of the parameters for disjoint learning on labeled and unlabeled data. Through extensive experimental validation of our method in the two different scenarios, we show that our method outperforms both local semi-supervised learning and baselines which naively combine federated learning with semi-supervised learning. The code is available at https://github.com/wyjeong/FedMatch.},
	urldate = {2022-03-04},
	journal = {arXiv:2006.12097 [cs, stat]},
	author = {Jeong, Wonyong and Yoon, Jaehong and Yang, Eunho and Hwang, Sung Ju},
	month = mar,
	year = {2021},
	note = {arXiv: 2006.12097},
	keywords = {Untagged},
}

@inproceedings{jia_scaling_2021,
	title = {Scaling {Up} {Visual} and {Vision}-{Language} {Representation} {Learning} {With} {Noisy} {Text} {Supervision}},
	url = {https://proceedings.mlr.press/v139/jia21b.html},
	abstract = {Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.},
	language = {en},
	urldate = {2023-05-30},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {4904--4916},
}

@article{jumper_highly_2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Abstract
            
              Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort
              1–4
              , the structures of around 100,000 unique proteins have been determined
              5
              , but this represents a small fraction of the billions of known protein sequences
              6,7
              . Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’
              8
              —has been an important open research problem for more than 50 years
              9
              . Despite recent progress
              10–14
              , existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)
              15
              , demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2023-04-06},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	keywords = {Untagged},
	pages = {583--589},
}

@article{kairouzAdvancesOpenProblems2021,
	title = {Advances and {Open} {Problems} in {Federated} {Learning}},
	volume = {14},
	doi = {10.1561/2200000083},
	number = {1-2},
	journal = {Found. Trends Mach. Learn.},
	author = {Kairouz, Peter and McMahan, H. Brendan and Avent, Brendan and Bellet, Aurélien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista A. and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D'Oliveira, Rafael G. L. and Eichner, Hubert and Rouayheb, Salim El and Evans, David and Gardner, Josh and Garrett, Zachary and Gascón, Adrià and Ghazi, Badih and Gibbons, Phillip B. and Gruteser, Marco and Harchaoui, Zaïd and He, Chaoyang and He, Lie and Huo, Zhouyuan and Hutchinson, Ben and Hsu, Justin and Jaggi, Martin and Javidi, Tara and Joshi, Gauri and Khodak, Mikhail and Konečný, Jakub and Korolova, Aleksandra and Koushanfar, Farinaz and Koyejo, Sanmi and Lepoint, Tancrède and Liu, Yang and Mittal, Prateek and Mohri, Mehryar and Nock, Richard and Özgür, Ayfer and Pagh, Rasmus and Qi, Hang and Ramage, Daniel and Raskar, Ramesh and Raykova, Mariana and Song, Dawn and Song, Weikang and Stich, Sebastian U. and Sun, Ziteng and Suresh, Ananda Theertha and Tramèr, Florian and Vepakomma, Praneeth and Wang, Jianyu and Xiong, Li and Xu, Zheng and Yang, Qiang and Yu, Felix X. and Yu, Han and Zhao, Sen},
	year = {2021},
	keywords = {Untagged},
	pages = {1--210},
}

@article{jiangAllTokensMatter2021,
	title = {All {Tokens} {Matter}: {Token} {Labeling} for {Training} {Better} {Vision} {Transformers}},
	shorttitle = {All {Tokens} {Matter}},
	url = {http://arxiv.org/abs/2104.10858},
	abstract = {In this paper, we present token labeling -- a new training objective for training high-performance vision transformers (ViTs). Different from the standard training objective of ViTs that computes the classification loss on an additional trainable class token, our proposed one takes advantage of all the image patch tokens to compute the training loss in a dense manner. Specifically, token labeling reformulates the image classification problem into multiple token-level recognition problems and assigns each patch token with an individual location-specific supervision generated by a machine annotator. Experiments show that token labeling can clearly and consistently improve the performance of various ViT models across a wide spectrum. For a vision transformer with 26M learnable parameters serving as an example, with token labeling, the model can achieve 84.4\% Top-1 accuracy on ImageNet. The result can be further increased to 86.4\% by slightly scaling the model size up to 150M, delivering the minimal-sized model among previous models (250M+) reaching 86\%. We also show that token labeling can clearly improve the generalization capability of the pre-trained models on downstream tasks with dense prediction, such as semantic segmentation. Our code and all the training details will be made publicly available at https://github.com/zihangJiang/TokenLabeling.},
	urldate = {2021-08-12},
	journal = {arXiv:2104.10858 [cs]},
	author = {Jiang, Zihang and Hou, Qibin and Yuan, Li and Zhou, Daquan and Shi, Yujun and Jin, Xiaojie and Wang, Anran and Feng, Jiashi},
	month = jun,
	year = {2021},
	note = {arXiv: 2104.10858},
	keywords = {Untagged},
}

@inproceedings{pmlr-v139-kaya21a,
	series = {Proceedings of machine learning research},
	title = {When does data augmentation help with membership inference attacks?},
	volume = {139},
	url = {https://proceedings.mlr.press/v139/kaya21a.html},
	abstract = {Deep learning models often raise privacy concerns as they leak information about their training data. This leakage enables membership inference attacks (MIA) that can identify whether a data point was in a model’s training set. Research shows that some ’data augmentation’ mechanisms may reduce the risk by combatting a key factor increasing the leakage, overfitting. While many mechanisms exist, their effectiveness against MIAs and privacy properties have not been studied systematically. Employing two recent MIAs, we explore the lower bound on the risk in the absence of formal upper bounds. First, we evaluate 7 mechanisms and differential privacy, on three image classification tasks. We find that applying augmentation to increase the model’s utility does not mitigate the risk and protection comes with a utility penalty. Further, we also investigate why popular label smoothing mechanism consistently amplifies the risk. Finally, we propose ’loss-rank-correlation’ (LRC) metric to assess how similar the effects of different mechanisms are. This, for example, reveals the similarity of applying high-intensity augmentation against MIAs to simply reducing the training time. Our findings emphasize the utility-privacy trade-off and provide practical guidelines on using augmentation to manage the trade-off.},
	booktitle = {Proceedings of the 38th international conference on machine learning},
	publisher = {PMLR},
	author = {Kaya, Yigitcan and Dumitras, Tudor},
	editor = {Meila, Marina and Zhang, Tong},
	month = jul,
	year = {2021},
	note = {tex.pdf: http://proceedings.mlr.press/v139/kaya21a/kaya21a.pdf},
	keywords = {Untagged},
	pages = {5345--5355},
}

@inproceedings{kairouzFastDimensionIndependent2021,
	title = {Fast {Dimension} {Independent} {Private} {AdaGrad} on {Publicly} {Estimated} {Subspaces}},
	url = {http://arxiv.org/abs/2008.06570},
	abstract = {We revisit the problem of empirical risk minimziation (ERM) with differential privacy. We show that noisy AdaGrad, given appropriate knowledge and conditions on the subspace from which gradients can be drawn, achieves a regret comparable to traditional AdaGrad plus a well-controlled term due to noise. We show a convergence rate of \$O({\textbackslash}text\{Tr\}(G\_T)/T)\$, where \$G\_T\$ captures the geometry of the gradient subspace. Since \${\textbackslash}text\{Tr\}(G\_T)=O({\textbackslash}sqrt\{T\})\$ we can obtain faster rates for convex and Lipschitz functions, compared to the \$O(1/{\textbackslash}sqrt\{T\})\$ rate achieved by known versions of noisy (stochastic) gradient descent with comparable noise variance. In particular, we show that if the gradients lie in a known constant rank subspace, and assuming algorithmic access to an envelope which bounds decaying sensitivity, one can achieve faster convergence to an excess empirical risk of \${\textbackslash}tilde O(1/{\textbackslash}epsilon n)\$, where \${\textbackslash}epsilon\$ is the privacy budget and \$n\$ the number of samples. Letting \$p\$ be the problem dimension, this result implies that, by running noisy Adagrad, we can bypass the DP-SGD bound \${\textbackslash}tilde O({\textbackslash}sqrt\{p\}/{\textbackslash}epsilon n)\$ in \$T=({\textbackslash}epsilon n){\textasciicircum}\{2/(1+2{\textbackslash}alpha)\}\$ iterations, where \${\textbackslash}alpha {\textbackslash}geq 0\$ is a parameter controlling gradient norm decay, instead of the rate achieved by SGD of \$T={\textbackslash}epsilon{\textasciicircum}2n{\textasciicircum}2\$. Our results operate with general convex functions in both constrained and unconstrained minimization. Along the way, we do a perturbation analysis of noisy AdaGrad of independent interest. Our utility guarantee for the private ERM problem follows as a corollary to the regret guarantee of noisy AdaGrad.},
	urldate = {2021-07-16},
	booktitle = {Conference on {Learning} {Theory}},
	author = {Kairouz, Peter and Ribero, Mónica and Rush, Keith and Thakurta, Abhradeep},
	month = jan,
	year = {2021},
	note = {arXiv: 2008.06570},
	keywords = {Untagged},
}

@article{kasaiFinetuningPretrainedTransformers2021,
	title = {Finetuning {Pretrained} {Transformers} into {RNNs}},
	url = {http://arxiv.org/abs/2103.13076},
	abstract = {Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. This comes with a significant computational overhead, as the attention mechanism scales with a quadratic complexity in sequence length. Efficient transformer variants have received increasing interest from recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train or yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving the efficiency while retaining the accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process needs lower training cost than training these recurrent variants from scratch. As many recent models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.},
	urldate = {2021-08-19},
	journal = {arXiv:2103.13076 [cs]},
	author = {Kasai, Jungo and Peng, Hao and Zhang, Yizhe and Yogatama, Dani and Ilharco, Gabriel and Pappas, Nikolaos and Mao, Yi and Chen, Weizhu and Smith, Noah A.},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.13076},
	keywords = {Untagged},
}

@article{khanTransformersVisionSurvey2021,
	title = {Transformers in {Vision}: {A} {Survey}},
	shorttitle = {Transformers in {Vision}},
	url = {http://arxiv.org/abs/2101.01169},
	abstract = {Astounding results from transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. This has led to exciting progress on a number of tasks while requiring minimal inductive biases in the model design. This survey aims to provide a comprehensive overview of the transformer models in the computer vision discipline and assumes little to no prior background in the ﬁeld. We start with an introduction to fundamental concepts behind the success of transformer models i.e., self-supervision and self-attention. Transformer architectures leverage self-attention mechanisms to encode long-range dependencies in the input domain which makes them highly expressive. Since they assume minimal prior knowledge about the structure of the problem, self-supervision using pretext tasks is applied to pre-train transformer models on large-scale (unlabelled) datasets. The learned representations are then ﬁne-tuned on the downstream tasks, typically leading to excellent performance due to the generalization and expressivity of encoded features. We cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classiﬁcation, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classiﬁcation and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges towards the application of transformer models in computer vision.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2101.01169 [cs]},
	author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
	month = feb,
	year = {2021},
	note = {arXiv: 2101.01169},
	keywords = {Untagged},
}

@inproceedings{kalibhatWinningLotteryTickets2021,
	title = {Winning lottery tickets in deep generative models},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16980},
	booktitle = {Thirty-fifth {AAAI} conference on artificial intelligence, {AAAI} 2021, thirty-third conference on innovative applications of artificial intelligence, {IAAI} 2021, the eleventh symposium on educational advances in artificial intelligence, {EAAI} 2021, virtual event, february 2-9, 2021},
	publisher = {AAAI Press},
	author = {Kalibhat, Neha Mukund and Balaji, Yogesh and Feizi, Soheil},
	year = {2021},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/aaai/KalibhatBF21.bib
tex.timestamp: Wed, 02 Jun 2021 18:09:11 +0200},
	keywords = {Untagged},
	pages = {8038--8046},
}

@inproceedings{kim_vilt_2021,
	title = {{ViLT}: {Vision}-and-{Language} {Transformer} {Without} {Convolution} or {Region} {Supervision}},
	shorttitle = {{ViLT}},
	url = {https://proceedings.mlr.press/v139/kim21k.html},
	abstract = {Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.},
	language = {en},
	urldate = {2023-05-30},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {5583--5594},
}

@inproceedings{kohWILDSBenchmarkIntheWild2021,
	title = {{WILDS}: {A} {Benchmark} of in-the-{Wild} {Distribution} {Shifts}},
	shorttitle = {{WILDS}},
	url = {https://proceedings.mlr.press/v139/koh21a.html},
	abstract = {Distribution shifts—where the training distribution differs from the test distribution—can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. The full paper, code, and leaderboards are available at https://wilds.stanford.edu.},
	language = {en},
	urldate = {2022-05-16},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and Lee, Tony and David, Etienne and Stavness, Ian and Guo, Wei and Earnshaw, Berton and Haque, Imran and Beery, Sara M. and Leskovec, Jure and Kundaje, Anshul and Pierson, Emma and Levine, Sergey and Finn, Chelsea and Liang, Percy},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {5637--5664},
}

@inproceedings{kruegerOutofDistributionGeneralizationRisk2021,
	title = {Out-of-{Distribution} {Generalization} via {Risk} {Extrapolation} ({REx})},
	url = {https://proceedings.mlr.press/v139/krueger21a.html},
	abstract = {Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model’s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing robustness to changes in the input distribution (“covariate shift”). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.},
	language = {en},
	urldate = {2022-05-15},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Krueger, David and Caballero, Ethan and Jacobsen, Joern-Henrik and Zhang, Amy and Binas, Jonathan and Zhang, Dinghuai and Priol, Remi Le and Courville, Aaron},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {5815--5826},
}

@inproceedings{DBLP:conf/icml/KimGYMK21,
	series = {Proceedings of machine learning research},
	title = {I-{BERT}: {Integer}-only {BERT} quantization},
	volume = {139},
	url = {http://proceedings.mlr.press/v139/kim21d.html},
	booktitle = {Proceedings of the 38th international conference on machine learning, {ICML} 2021, 18-24 july 2021, virtual event},
	publisher = {PMLR},
	author = {Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W. and Keutzer, Kurt},
	editor = {Meila, Marina and Zhang, Tong},
	year = {2021},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/icml/KimGYMK21.bib
tex.timestamp: Wed, 25 Aug 2021 17:11:17 +0200},
	keywords = {Untagged},
	pages = {5506--5518},
}

@article{kulynychDisparateVulnerabilityMembership2021,
	title = {Disparate {Vulnerability} to {Membership} {Inference} {Attacks}},
	url = {http://arxiv.org/abs/1906.00389},
	abstract = {A membership inference attack (MIA) against a machine-learning model enables an attacker to determine whether a given data record was part of the model's training data or not. In this paper, we provide an in-depth study of the phenomenon of disparate vulnerability against MIAs: unequal success rate of MIAs against different population subgroups. We first establish necessary and sufficient conditions for MIAs to be prevented, both on average and for population subgroups, using a notion of distributional generalization. Second, we derive connections of disparate vulnerability to algorithmic fairness and to differential privacy. We show that fairness can only prevent disparate vulnerability against limited classes of adversaries. Differential privacy bounds disparate vulnerability but can significantly reduce the accuracy of the model. We show that estimating disparate vulnerability to MIAs by na{\textbackslash}"ively applying existing attacks can lead to overestimation. We then establish which attacks are suitable for estimating disparate vulnerability, and provide a statistical framework for doing so reliably. We conduct experiments on synthetic and real-world data finding statistically significant evidence of disparate vulnerability in realistic settings. The code is available at https://github.com/spring-epfl/disparate-vulnerability},
	urldate = {2022-03-21},
	journal = {arXiv:1906.00389 [cs, stat]},
	author = {Kulynych, Bogdan and Yaghini, Mohammad and Cherubin, Giovanni and Veale, Michael and Troncoso, Carmela},
	month = sep,
	year = {2021},
	note = {arXiv: 1906.00389},
	keywords = {Untagged},
}

@techreport{learned-millerVectorMatrixTensor2021,
	title = {Vector, {Matrix}, and {Tensor} {Derivatives}},
	language = {en},
	author = {Learned-Miller, Erik},
	year = {2021},
	keywords = {Untagged},
	pages = {7},
}

@inproceedings{leeMachineLearningRobustness2021,
	address = {Virtual Event Singapore},
	title = {Machine {Learning} {Robustness}, {Fairness}, and their {Convergence}},
	isbn = {978-1-4503-8332-5},
	url = {https://dl.acm.org/doi/10.1145/3447548.3470799},
	doi = {10/gn3r7m},
	abstract = {Responsible AI becomes critical where robustness and fairness must be satisfied together. Traditionally, the two topics have been studied by different communities for different applications. Robust training is designed for noisy or poisoned data where image data is typically considered. In comparison, fair training primarily deals with biased data where structured data is typically considered. Nevertheless, robust training and fair training are fundamentally similar in considering that both of them aim at fixing the inherent flaws of real-world data. In this tutorial, we first cover state-of-the-art robust training techniques where most of the research is on combating various label noises. In particular, we cover label noise modeling, robust training approaches, and real-world noisy data sets. Then, proceeding to the related fairness literature, we discuss pre-processing, in-processing, and post-processing unfairness mitigation techniques, depending on whether the mitigation occurs before, during, or after the model training. Finally, we cover the recent trend emerged to combine robust and fair training in two flavors: the former is to make the fair training more robust (i.e., robust fair training), and the latter is to consider robustness and fairness as two equals to incorporate them into a holistic framework. This tutorial is indeed timely and novel because the convergence of the two topics is increasingly common, but yet to be addressed in tutorials. The tutors have extensive experience publishing papers in top-tier machine learning and data mining venues and developing machine learning platforms.},
	language = {en},
	urldate = {2022-01-25},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Lee, Jae-Gil and Roh, Yuji and Song, Hwanjun and Whang, Steven Euijong},
	month = aug,
	year = {2021},
	keywords = {Untagged},
	pages = {4046--4047},
}

@techreport{lamda8BanditsRoadMap2021,
	title = {Bandits {RoadMap}},
	author = {{Lamda 8}},
	year = {2021},
	keywords = {Untagged},
}

@inproceedings{levine_deep_2021,
	title = {Deep {Partition} {Aggregation}: {Provable} {Defense} against {General} {Poisoning} {Attacks}},
	shorttitle = {Deep {Partition} {Aggregation}},
	url = {http://arxiv.org/abs/2006.14768},
	abstract = {Adversarial poisoning attacks distort training data in order to corrupt the test-time behavior of a classifier. A provable defense provides a certificate for each test sample, which is a lower bound on the magnitude of any adversarial distortion of the training set that can corrupt the test sample's classification. We propose two novel provable defenses against poisoning attacks: (i) Deep Partition Aggregation (DPA), a certified defense against a general poisoning threat model, defined as the insertion or deletion of a bounded number of samples to the training set -- by implication, this threat model also includes arbitrary distortions to a bounded number of images and/or labels; and (ii) Semi-Supervised DPA (SS-DPA), a certified defense against label-flipping poisoning attacks. DPA is an ensemble method where base models are trained on partitions of the training set determined by a hash function. DPA is related to both subset aggregation, a well-studied ensemble method in classical machine learning, as well as to randomized smoothing, a popular provable defense against evasion attacks. Our defense against label-flipping attacks, SS-DPA, uses a semi-supervised learning algorithm as its base classifier model: each base classifier is trained using the entire unlabeled training set in addition to the labels for a partition. SS-DPA significantly outperforms the existing certified defense for label-flipping attacks on both MNIST and CIFAR-10: provably tolerating, for at least half of test images, over 600 label flips (vs. {\textless} 200 label flips) on MNIST and over 300 label flips (vs. 175 label flips) on CIFAR-10. Against general poisoning attacks, where no prior certified defenses exists, DPA can certify {\textgreater}= 50\% of test images against over 500 poison image insertions on MNIST, and nine insertions on CIFAR-10. These results establish new state-of-the-art provable defenses against poisoning attacks.},
	urldate = {2022-06-11},
	booktitle = {{arXiv}:2006.14768},
	publisher = {arXiv},
	author = {Levine, Alexander and Feizi, Soheil},
	month = mar,
	year = {2021},
	note = {arXiv:2006.14768 [cs, stat]
type: article},
	keywords = {Untagged},
}

@inproceedings{liMetaHARFederatedRepresentation2021,
	address = {Ljubljana,Slovenia},
	title = {Meta-{HAR}: {Federated} {Representation} {Learning} for {Human} {Activity} {Recognition}},
	isbn = {978-1-4503-8312-7},
	shorttitle = {Meta-{HAR}},
	url = {https://dl.acm.org/doi/10.1145/3442381.3450006},
	doi = {10/gkzww4},
	abstract = {Human activity recognition (HAR) based on mobile sensors plays an important role in ubiquitous computing. However, the rise of data regulatory constraints precludes collecting private and labeled signal data from personal devices at scale. Thanks to the growth of computational power on mobile devices, federated learning has emerged as a decentralized alternative solution to model training, which iteratively aggregates locally updated models into a shared global model, therefore being able to leverage decentralized, private data without central collection. However, the effectiveness of federated learning for HAR is affected by the fact that each user has different activity types and even a different signal distribution for the same activity type. Furthermore, it is uncertain if a single global model trained can generalize well to individual users or new users with heterogeneous data. In this paper, we propose Meta-HAR, a federated representation learning framework, in which a signal embedding network is meta-learned in a federated manner, while the learned signal representations are further fed into a personalized classification network at each user for activity prediction. In order to boost the representation ability of the embedding network, we treat the HAR problem at each user as a different task and train the shared embedding network through a Model-Agnostic Meta-learning framework, such that the embedding network can generalize to any individual user. Personalization is further achieved on top of the robustly learned representations in an adaptation procedure. We conducted extensive experiments based on two publicly available HAR datasets as well as a newly created HAR dataset. Results verify that Meta-HAR is effective at maintaining high test accuracies for individual users, including new users, and significantly outperforms several baselines, including Federated Averaging, Reptile and even centralized learning in certain cases. Our collected dataset will be open-sourced to facilitate future development in the field of sensor-based human activity recognition.},
	language = {en},
	urldate = {2021-06-30},
	booktitle = {Web {Conference}},
	publisher = {ACM},
	author = {Li, Chenglin and Niu, Di and Jiang, Bei and Zuo, Xiao and Yang, Jianming},
	year = {2021},
	keywords = {Untagged},
	pages = {912--922},
}

@inproceedings{liTwoStreamConvolutionAugmented2021,
	title = {Two-{Stream} {Convolution} {Augmented} {Transformer} for {Human} {Activity} {Recognition}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16103},
	language = {en},
	urldate = {2021-06-30},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Li, Bing and Cui, Wei and Wang, Wei and Zhang, Le and Chen, Zhenghua and Wu, Min},
	year = {2021},
	note = {Github:https://github.com/windofshadow/THAT},
	keywords = {Untagged},
	pages = {286--293},
}

@inproceedings{leiLessMoreClipBERT2021,
	address = {Virtual},
	title = {Less {Is} {More}: {ClipBERT} for {Video}-and-{Language} {Learning} via {Sparse} {Sampling}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Lei\_Less\_Is\_More\_ClipBERT\_for\_Video-and-Language\_Learning\_via\_Sparse\_Sampling\_CVPR\_2021\_paper.html},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {Computer Vision Foundation / IEEE},
	author = {Lei, Jie and Li, Linjie and Zhou, Luowei and Gan, Zhe and Berg, Tamara L. and Bansal, Mohit and Liu, Jingjing},
	year = {2021},
	keywords = {Untagged},
	pages = {7331--7341},
}

@article{lesage-landryOnlineConvexOptimization2021,
	title = {Online {Convex} {Optimization} with {Binary} {Constraints}},
	volume = {66},
	issn = {0018-9286, 1558-2523, 2334-3303},
	url = {http://arxiv.org/abs/2005.02274},
	doi = {10.1109/TAC.2021.3061625},
	abstract = {We consider online optimization with binary decision variables and convex loss functions. We design a new algorithm, binary online gradient descent (bOGD) and bound its expected dynamic regret. We provide a regret bound that holds for any time horizon and a specialized bound for ﬁnite time horizons. First, we present the regret as the sum of the relaxed, continuous round optimum tracking error and the rounding error of our update in which the former asymptomatically decreases with time under certain conditions. Then, we derive a ﬁnite-time bound that is sublinear in time and linear in the cumulative variation of the relaxed, continuous round optima. We apply bOGD to demand response with thermostatically controlled loads, in which binary constraints model discrete on/off settings. We also model uncertainty and varying load availability, which depend on temperature deadbands, lockout of cooling units and manual overrides. We test the performance of bOGD in several simulations based on demand response. The simulations corroborate that the use of randomization in bOGD does not signiﬁcantly degrade performance while making the problem more tractable.},
	language = {en},
	number = {12},
	urldate = {2022-02-22},
	journal = {IEEE Transactions on Automatic Control},
	author = {Lesage-Landry, Antoine and Taylor, Joshua A. and Callaway, Duncan S.},
	month = dec,
	year = {2021},
	note = {arXiv: 2005.02274},
	keywords = {Untagged},
	pages = {6164--6170},
}

@inproceedings{li_contextualized_2021,
	address = {Online},
	title = {Contextualized {Perturbation} for {Textual} {Adversarial} {Attack}},
	url = {https://aclanthology.org/2021.naacl-main.400},
	doi = {10.18653/v1/2021.naacl-main.400},
	abstract = {Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a context-aware manner. We propose three contextualized perturbations, Replace, Insert and Merge, that allow for generating outputs of varied lengths. CLARE can flexibly combine these perturbations and apply them at any position in the inputs, and is thus able to attack the victim model more effectively with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality.},
	urldate = {2023-02-26},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Dianqi and Zhang, Yizhe and Peng, Hao and Chen, Liqun and Brockett, Chris and Sun, Ming-Ting and Dolan, Bill},
	month = jun,
	year = {2021},
	keywords = {Untagged},
	pages = {5053--5069},
}

@article{liFrequencyawareDiscriminativeFeature2021,
	title = {Frequency-aware {Discriminative} {Feature} {Learning} {Supervised} by {Single}-{Center} {Loss} for {Face} {Forgery} {Detection}},
	url = {http://arxiv.org/abs/2103.09096},
	abstract = {Face forgery detection is raising ever-increasing interest in computer vision since facial manipulation technologies cause serious worries. Though recent works have reached sound achievements, there are still unignorable problems: a) learned features supervised by softmax loss are separable but not discriminative enough, since softmax loss does not explicitly encourage intra-class compactness and interclass separability; and b) ﬁxed ﬁlter banks and hand-crafted features are insufﬁcient to capture forgery patterns of frequency from diverse inputs. To compensate for such limitations, a novel frequency-aware discriminative feature learning framework is proposed in this paper. Speciﬁcally, we design a novel single-center loss (SCL) that only compresses intra-class variations of natural faces while boosting interclass differences in the embedding space. In such a case, the network can learn more discriminative features with less optimization difﬁculty. Besides, an adaptive frequency feature generation module is developed to mine frequency clues in a completely data-driven fashion. With the above two modules, the whole framework can learn more discriminative features in an end-to-end manner. Extensive experiments demonstrate the effectiveness and superiority of our framework on three versions of the FF++ dataset.},
	language = {en},
	urldate = {2022-03-22},
	journal = {arXiv:2103.09096 [cs]},
	author = {Li, Jiaming and Xie, Hongtao and Li, Jiahong and Wang, Zhongyuan and Zhang, Yongdong},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.09096},
	keywords = {Untagged},
}

@article{liMembershipInferenceAttacks2021,
	title = {Membership {Inference} {Attacks} and {Defenses} in {Classification} {Models}},
	url = {http://arxiv.org/abs/2002.12062},
	doi = {10.1145/3422337.3447836},
	abstract = {We study the membership inference (MI) attack against classifiers, where the attacker's goal is to determine whether a data instance was used for training the classifier. Through systematic cataloging of existing MI attacks and extensive experimental evaluations of them, we find that a model's vulnerability to MI attacks is tightly related to the generalization gap -- the difference between training accuracy and test accuracy. We then propose a defense against MI attacks that aims to close the gap by intentionally reduces the training accuracy. More specifically, the training process attempts to match the training and validation accuracies, by means of a new \{{\textbackslash}em set regularizer\} using the Maximum Mean Discrepancy between the softmax output empirical distributions of the training and validation sets. Our experimental results show that combining this approach with another simple defense (mix-up training) significantly improves state-of-the-art defense against MI attacks, with minimal impact on testing accuracy.},
	urldate = {2022-04-03},
	journal = {Proceedings of the Eleventh ACM Conference on Data and Application Security and Privacy},
	author = {Li, Jiacheng and Li, Ninghui and Ribeiro, Bruno},
	month = apr,
	year = {2021},
	note = {arXiv: 2002.12062},
	keywords = {Untagged},
	pages = {5--16},
}

@inproceedings{li2021align,
	title = {Align before fuse: {Vision} and language representation learning with momentum distillation},
	url = {https://openreview.net/forum?id=OJLaKwiXSbx},
	booktitle = {Advances in neural information processing systems},
	author = {Li, Junnan and Selvaraju, Ramprasaath R. and Gotmare, Akhilesh Deepak and Joty, Shafiq and Xiong, Caiming and Hoi, Steven},
	editor = {Beygelzimer, A. and Dauphin, Y. and Liang, P. and Vaughan, J. Wortman},
	year = {2021},
	keywords = {Untagged},
}

@article{liUnifiedFoundationModel2021,
	title = {Towards a {Unified} {Foundation} {Model}: {Jointly} {Pre}-{Training} {Transformers} on {Unpaired} {Images} and {Text}},
	shorttitle = {Towards a {Unified} {Foundation} {Model}},
	url = {http://arxiv.org/abs/2112.07074},
	abstract = {In this paper, we explore the possibility of building a uniﬁed foundation model that can be adapted to both visiononly and text-only tasks. Starting from BERT and ViT, we design a uniﬁed transformer consisting of modality-speciﬁc tokenizers, a shared transformer encoder, and task-speciﬁc output heads. To efﬁciently pre-train the proposed model jointly on unpaired images and text, we propose two novel techniques: (i) We employ the separately-trained BERT and ViT models as teachers and apply knowledge distillation to provide additional, accurate supervision signals for the joint training; (ii) We propose a novel gradient masking strategy to balance the parameter updates from the image and text pre-training losses. We evaluate the jointly pretrained transformer by ﬁne-tuning it on image classiﬁcation tasks and natural language understanding tasks, respectively. The experiments show that the resultant uniﬁed foundation transformer works surprisingly well on both the vision-only and text-only tasks, and the proposed knowledge distillation and gradient masking strategy can effectively lift the performance to approach the level of separately-trained models.},
	language = {en},
	urldate = {2022-01-12},
	journal = {arXiv:2112.07074 [cs]},
	author = {Li, Qing and Gong, Boqing and Cui, Yin and Kondratyuk, Dan and Du, Xianzhi and Yang, Ming-Hsuan and Brown, Matthew},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.07074},
	keywords = {Untagged},
}

@misc{lin_few_2021,
	title = {A {Few} {Brief} {Notes} on {DeepImpact}, {COIL}, and a {Conceptual} {Framework} for {Information} {Retrieval} {Techniques}},
	shorttitle = {{UNICOIL}},
	url = {http://arxiv.org/abs/2106.14807},
	abstract = {Recent developments in representational learning for information retrieval can be organized in a conceptual framework that establishes two pairs of contrasts: sparse vs. dense representations and unsupervised vs. learned representations. Sparse learned representations can further be decomposed into expansion and term weighting components. This framework allows us to understand the relationship between recently proposed techniques such as DPR, ANCE, DeepCT, DeepImpact, and COIL, and furthermore, gaps revealed by our analysis point to “low hanging fruit” in terms of techniques that have yet to be explored. We present a novel technique dubbed “uniCOIL”, a simple extension of COIL that achieves to our knowledge the current state-of-the-art in sparse retrieval on the popular MS MARCO passage ranking dataset. Our implementation using the Anserini IR toolkit is built on the Lucene search library and thus fully compatible with standard inverted indexes.},
	language = {en},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Lin, Jimmy and Ma, Xueguang},
	month = jun,
	year = {2021},
	note = {arXiv:2106.14807 [cs]},
	keywords = {Untagged},
}

@article{li2021understanding,
	title = {Towards an understanding of benign overfitting in neural networks},
	author = {Li, Zhu and Zhou, Zhi-Hua and Gretton, Arthur},
	year = {2021},
	note = {arXiv: 2106.03212 [stat.ML]},
	keywords = {Untagged},
}

@article{liangRDropRegularizedDropout2021,
	title = {R-{Drop}: {Regularized} {Dropout} for {Neural} {Networks}},
	shorttitle = {R-{Drop}},
	url = {http://arxiv.org/abs/2106.14448},
	abstract = {Dropout is a powerful and widely used technique to regularize the training of deep neural networks. In this paper, we introduce a simple regularization strategy upon dropout in model training, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the freedom of the model parameters and complements dropout. Experiments on \${\textbackslash}bf\{5\}\$ widely used deep learning tasks (\${\textbackslash}bf\{18\}\$ datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 English\${\textbackslash}to\$German translation (\${\textbackslash}bf\{30.91\}\$ BLEU) and WMT14 English\${\textbackslash}to\$French translation (\${\textbackslash}bf\{43.95\}\$ BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models. Our code is available at GitHub\{{\textbackslash}url\{https://github.com/dropreg/R-Drop\}\}.},
	urldate = {2021-08-25},
	journal = {arXiv:2106.14448 [cs]},
	author = {Liang, Xiaobo and Wu, Lijun and Li, Juntao and Wang, Yue and Meng, Qi and Qin, Tao and Chen, Wei and Zhang, Min and Liu, Tie-Yan},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.14448},
	keywords = {Untagged},
}

@inproceedings{liangLearningLanguageMultimodal2021,
	address = {Online},
	title = {Learning {Language} and {Multimodal} {Privacy}-{Preserving} {Markers} of {Mood} from {Mobile} {Data}},
	url = {https://aclanthology.org/2021.acl-long.322},
	doi = {10/gmdjjv},
	abstract = {Mental health conditions remain underdiagnosed even in countries with common access to advanced medical care. The ability to accurately and efficiently predict mood from easily collectible data has several important implications for the early detection, intervention, and treatment of mental health disorders. One promising data source to help monitor human behavior is daily smartphone usage. However, care must be taken to summarize behaviors without identifying the user through personal (e.g., personally identifiable information) or protected (e.g., race, gender) attributes. In this paper, we study behavioral markers of daily mood using a recent dataset of mobile behaviors from adolescent populations at high risk of suicidal behaviors. Using computational models, we find that language and multimodal representations of mobile typed text (spanning typed characters, words, keystroke timings, and app usage) are predictive of daily mood. However, we find that models trained to predict mood often also capture private user identities in their intermediate representations. To tackle this problem, we evaluate approaches that obfuscate user identity while remaining predictive. By combining multimodal representations with privacy-preserving learning, we are able to push forward the performance-privacy frontier.},
	urldate = {2021-11-15},
	booktitle = {Annual {Meeting} of the {Association} for {Computational} {Linguistics} and {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Liang, Paul Pu and Liu, Terrance and Cai, Anna and Muszynski, Michal and Ishii, Ryo and Allen, Nick and Auerbach, Randy and Brent, David and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
	month = aug,
	year = {2021},
	keywords = {Untagged},
	pages = {4170--4187},
}

@inproceedings{liu_conflict-averse_2021,
	title = {Conflict-{Averse} {Gradient} {Descent} for {Multi}-task {Learning}},
	abstract = {The goal of multi-task learning is to enable more efficient learning than single task learning by sharing model structures for a diverse set of tasks. A standard multi-task learning objective is to minimize the average loss across all tasks. While straightforward, using this objective often results in much worse final performance for each task than learning them independently. A major challenge in optimizing a multi-task model is the conflicting gradients, where gradients of different task objectives are not well aligned so that following the average gradient direction can be detrimental to specific tasks’ performance. Previous work has proposed several heuristics to manipulate the task gradients for mitigating this problem. But most of them lack convergence guarantee and/or could converge to any Pareto-stationary point. In this paper, we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the average loss function, while leveraging the worst local improvement of individual tasks to regularize the algorithm trajectory. CAGrad balances the objectives automatically and still provably converges to a minimum over the average loss. It includes the regular gradient descent (GD) and the multiple gradient descent algorithm (MGDA) in the multi-objective optimization (MOO) literature as special cases. On a series of challenging multi-task supervised learning and reinforcement learning tasks, CAGrad achieves improved performance over prior state-of-the-art multi-objective gradient manipulation methods. Code is available at https://github.com/Cranial-XIX/CAGrad.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Liu, Bo and Liu, Xingchao and Jin, Xiaojie and Stone, Peter and Liu, Qiang},
	year = {2021},
	keywords = {Untagged},
	pages = {13},
}

@inproceedings{liuLearningCausalSemantic2021,
	title = {Learning {Causal} {Semantic} {Representation} for {Out}-of-{Distribution} {Prediction}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/310614fca8fb8e5491295336298c340f-Abstract.html},
	abstract = {Conventional supervised learning methods, especially deep ones, are found to be sensitive to out-of-distribution (OOD) examples, largely because the learned representation mixes the semantic factor with the variation factor due to their domain-specific correlation, while only the semantic factor causes the output. To address the problem, we propose a Causal Semantic Generative model (CSG) based on a causal reasoning so that the two factors are modeled separately, and develop methods for OOD prediction from a single training domain, which is common and challenging. The methods are based on the causal invariance principle, with a novel design in variational Bayes for both efficient learning and easy prediction. Theoretically, we prove that under certain conditions, CSG can identify the semantic factor by fitting training data, and this semantic-identification guarantees the boundedness of OOD generalization error and the success of adaptation. Empirical study shows improved OOD performance over prevailing baselines.},
	urldate = {2022-05-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Chang and Sun, Xinwei and Wang, Jindong and Tang, Haoyue and Li, Tao and Qin, Tao and Chen, Wei and Liu, Tie-Yan},
	year = {2021},
	keywords = {Untagged},
	pages = {6155--6170},
}

@inproceedings{liuTransferableAdversarialPerturbations2021,
	title = {Towards transferable adversarial perturbations with minimum norm},
	booktitle = {{ICML} 2021 {Workshop} on {Adversarial} {Machine} {Learning}},
	author = {Liu, Fangcheng and Zhang, Chao and Zhang, Hongyang},
	year = {2021},
	keywords = {Untagged},
}

@inproceedings{liu_hit_2021,
	address = {Montreal, QC, Canada},
	title = {{HiT}: {Hierarchical} {Transformer} with {Momentum} {Contrast} for {Video}-{Text} {Retrieval}},
	isbn = {978-1-66542-812-5},
	shorttitle = {{HiT}},
	url = {https://ieeexplore.ieee.org/document/9710620/},
	doi = {10.1109/ICCV48922.2021.01170},
	abstract = {Video-Text Retrieval has been a hot research topic with the growth of multimedia data on the internet. Transformer for video-text learning has attracted increasing attention due to its promising performance. However, existing crossmodal transformer approaches typically suffer from two major limitations: 1) Exploitation of the transformer architecture where different layers have different feature characteristics is limited; 2) End-to-end training mechanism limits negative sample interactions in a mini-batch. In this paper, we propose a novel approach named Hierarchical Transformer (HiT) for video-text retrieval. HiT performs Hierarchical Cross-modal Contrastive Matching in both featurelevel and semantic-level, achieving multi-view and comprehensive retrieval results. Moreover, inspired by MoCo, we propose Momentum Cross-modal Contrast for cross-modal learning to enable large-scale negative sample interactions on-the-ﬂy, which contributes to the generation of more precise and discriminative representations. Experimental results on the three major Video-Text Retrieval benchmark datasets demonstrate the advantages of our method.},
	language = {en},
	urldate = {2023-05-30},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Liu, Song and Fan, Haoqi and Qian, Shengsheng and Chen, Yiru and Ding, Wenkui and Wang, Zhongyuan},
	month = oct,
	year = {2021},
	keywords = {Untagged},
	pages = {11895--11905},
}

@article{liuInfoBehaviorSelfsupervisedRepresentation2021,
	title = {{InfoBehavior}: {Self}-supervised {Representation} {Learning} for {Ultra}-long {Behavior} {Sequence} via {Hierarchical} {Grouping}},
	shorttitle = {{InfoBehavior}},
	url = {http://arxiv.org/abs/2106.06905},
	abstract = {E-commerce companies have to face abnormal sellers who sell potentially-risky products. Typically, the risk can be identified by jointly considering product content (e.g., title and image) and seller behavior. This work focuses on behavior feature extraction as behavior sequences can provide valuable clues for the risk discovery by reflecting the sellers' operation habits. Traditional feature extraction techniques heavily depend on domain experts and adapt poorly to new tasks. In this paper, we propose a self-supervised method InfoBehavior to automatically extract meaningful representations from ultra-long raw behavior sequences instead of the costly feature selection procedure. InfoBehavior utilizes Bidirectional Transformer as feature encoder due to its excellent capability in modeling long-term dependency. However, it is intractable for commodity GPUs because the time and memory required by Transformer grow quadratically with the increase of sequence length. Thus, we propose a hierarchical grouping strategy to aggregate ultra-long raw behavior sequences to length-processable high-level embedding sequences. Moreover, we introduce two types of pretext tasks. Sequence-related pretext task defines a contrastive-based training objective to correctly select the masked-out coarse-grained/fine-grained behavior sequences against other "distractor" behavior sequences; Domain-related pretext task designs a classification training objective to correctly predict the domain-specific statistical results of anomalous behavior. We show that behavior representations from the pre-trained InfoBehavior can be directly used or integrated with features from other side information to support a wide range of downstream tasks. Experimental results demonstrate that InfoBehavior significantly improves the performance of Product Risk Management and Intellectual Property Protection.},
	urldate = {2021-06-30},
	journal = {arXiv:2106.06905 [cs]},
	author = {Liu, Runshi and Qin, Pengda and Li, Yuhong and Wen, Weigao and Li, Dong and Deng, Kefeng and Wu, Qiang},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.06905},
	keywords = {Untagged},
}

@article{liuPretrainPromptPredict2021,
	title = {Pre-train, {Prompt}, and {Predict}: {A} {Systematic} {Survey} of {Prompting} {Methods} in {Natural} {Language} {Processing}},
	shorttitle = {Pre-train, {Prompt}, and {Predict}},
	url = {http://arxiv.org/abs/2107.13586},
	abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y{\textbar}x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
	urldate = {2021-08-18},
	journal = {arXiv:2107.13586 [cs]},
	author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.13586},
	keywords = {Untagged},
}

@misc{luo_clip4clip_2021,
	title = {{CLIP4Clip}: {An} {Empirical} {Study} of {CLIP} for {End} to {End} {Video} {Clip} {Retrieval}},
	shorttitle = {{CLIP4Clip}},
	url = {http://arxiv.org/abs/2104.08860},
	abstract = {Video-text retrieval plays an essential role in multi-modal research and has been widely used in many real-world web applications. The CLIP (Contrastive Language-Image Pre-training), an image-language pre-training model, has demonstrated the power of visual concepts learning from web collected image-text datasets. In this paper, we propose a CLIP4Clip model to transfer the knowledge of the CLIP model to video-language retrieval in an end-to-end manner. Several questions are investigated via empirical studies: 1) Whether image feature is enough for video-text retrieval? 2) How a post-pretraining on a large-scale video-text dataset based on the CLIP affect the performance? 3) What is the practical mechanism to model temporal dependency between video frames? And 4) The Hyper-parameters sensitivity of the model on video-text retrieval task. Extensive experimental results present that the CLIP4Clip model transferred from the CLIP can achieve SOTA results on various video-text retrieval datasets, including MSR-VTT, MSVC, LSMDC, ActivityNet, and DiDeMo. We release our code at https://github.com/ArrowLuo/CLIP4Clip.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Luo, Huaishao and Ji, Lei and Zhong, Ming and Chen, Yang and Lei, Wen and Duan, Nan and Li, Tianrui},
	month = may,
	year = {2021},
	note = {arXiv:2104.08860 [cs]},
	keywords = {Untagged},
}

@article{longFedSiamAdaptiveFederated2021,
	title = {{FedSiam}: {Towards} {Adaptive} {Federated} {Semi}-{Supervised} {Learning}},
	shorttitle = {{FedSiam}},
	url = {http://arxiv.org/abs/2012.03292},
	abstract = {Federated learning (FL) has emerged as an effective technique to co-training machine learning models without actually sharing data and leaking privacy. However, most existing FL methods focus on the supervised setting and ignore the utilization of unlabeled data. Although there are a few existing studies trying to incorporate unlabeled data into FL, they all fail to maintain performance guarantees or generalization ability in various real-world settings. In this paper, we focus on designing a general framework FedSiam to tackle different scenarios of federated semi-supervised learning, including four settings in the labels-at-client scenario and two setting in the labels-at-server scenario. FedSiam is built upon a siamese network into FL with a momentum update to handle the non-IID challenges introduced by unlabeled data. We further propose a new metric to measure the divergence of local model layers within the siamese network. Based on the divergence, FedSiam can automatically select layer-level parameters to be uploaded to the server in an adaptive manner. Experimental results on three datasets under two scenarios with different data distribution settings demonstrate that the proposed FedSiam framework outperforms state-of-the-art baselines.},
	urldate = {2022-03-04},
	journal = {arXiv:2012.03292 [cs]},
	author = {Long, Zewei and Che, Liwei and Wang, Yaqing and Ye, Muchao and Luo, Junyu and Wu, Jinze and Xiao, Houping and Ma, Fenglong},
	month = jul,
	year = {2021},
	note = {arXiv: 2012.03292},
	keywords = {Untagged},
}

@inproceedings{longGenerativeImaginationElevates2021,
	address = {Online},
	title = {Generative {Imagination} {Elevates} {Machine} {Translation}},
	url = {https://aclanthology.org/2021.naacl-main.457},
	doi = {10.18653/v1/2021.naacl-main.457},
	abstract = {There are common semantics shared across text and images. Given a sentence in a source language, whether depicting the visual scene helps translation into a target language? Existing multimodal neural machine translation methods (MNMT) require triplets of bilingual sentence - image for training and tuples of source sentence - image for inference. In this paper, we propose ImagiT, a novel machine translation method via visual imagination. ImagiT first learns to generate visual representation from the source sentence, and then utilizes both source sentence and the “imagined representation” to produce a target translation. Unlike previous methods, it only needs the source sentence at the inference time. Experiments demonstrate that ImagiT benefits from visual imagination and significantly outperforms the text-only neural machine translation baselines. Further analysis reveals that the imagination process in ImagiT helps fill in missing information when performing the degradation strategy.},
	urldate = {2021-07-06},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Long, Quanyu and Wang, Mingxuan and Li, Lei},
	month = jun,
	year = {2021},
	keywords = {Untagged},
	pages = {5738--5748},
}

@inproceedings{liuHowAdamTraining2021,
	title = {How {Do} {Adam} and {Training} {Strategies} {Help} {BNNs} {Optimization}?},
	url = {http://arxiv.org/abs/2106.11309},
	abstract = {The best performing Binary Neural Networks (BNNs) are usually attained using Adam optimization and its multi-step training variants. However, to the best of our knowledge, few studies explore the fundamental reasons why Adam is superior to other optimizers like SGD for BNN optimization or provide analytical explanations that support specific training strategies. To address this, in this paper we first investigate the trajectories of gradients and weights in BNNs during the training process. We show the regularization effect of second-order momentum in Adam is crucial to revitalize the weights that are dead due to the activation saturation in BNNs. We find that Adam, through its adaptive learning rate strategy, is better equipped to handle the rugged loss surface of BNNs and reaches a better optimum with higher generalization ability. Furthermore, we inspect the intriguing role of the real-valued weights in binary networks, and reveal the effect of weight decay on the stability and sluggishness of BNN optimization. Through extensive experiments and analysis, we derive a simple training scheme, building on existing Adam-based optimization, which achieves 70.5\% top-1 accuracy on the ImageNet dataset using the same architecture as the state-of-the-art ReActNet while achieving 1.1\% higher accuracy. Code and models are available at https://github.com/liuzechun/AdamBNN.},
	urldate = {2021-07-23},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Liu, Zechun and Shen, Zhiqiang and Li, Shichao and Helwegen, Koen and Huang, Dong and Cheng, Kwang-Ting},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.11309},
	keywords = {Untagged},
}

@article{liuQuantitativeMetricPrivacy2021,
	title = {A {Quantitative} {Metric} for {Privacy} {Leakage} in {Federated} {Learning}},
	url = {http://arxiv.org/abs/2102.13472},
	abstract = {In the federated learning system, parameter gradients are shared among participants and the central modulator, while the original data never leave their protected source domain. However, the gradient itself might carry enough information for precise inference of the original data. By reporting their parameter gradients to the central server, client datasets are exposed to inference attacks from adversaries. In this paper, we propose a quantitative metric based on mutual information for clients to evaluate the potential risk of information leakage in their gradients. Mutual information has received increasing attention in the machine learning and data mining community over the past few years. However, existing mutual information estimation methods cannot handle highdimensional variables. In this paper, we propose a novel method to approximate the mutual information between the high-dimensional gradients and batched input data. Experimental results show that the proposed metric reliably reﬂect the extent of information leakage in federated learning. In addition, using the proposed metric, we investigate the inﬂuential factors of risk level. It is proven that, the risk of information leakage is related to the status of the task model, as well as the inherent data distribution.},
	language = {en},
	urldate = {2021-11-19},
	journal = {arXiv:2102.13472 [cs]},
	author = {Liu, Yong and Zhu, Xinghua and Wang, Jianzong and Xiao, Jing},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.13472},
	keywords = {Untagged},
}

@misc{mallia_learning_2021,
	title = {Learning {Passage} {Impacts} for {Inverted} {Indexes}},
	shorttitle = {{DeepImpact}},
	url = {http://arxiv.org/abs/2104.12016},
	abstract = {Neural information retrieval systems typically use a cascading pipeline, in which a first-stage model retrieves a candidate set of documents and one or more subsequent stages re-rank this set using contextualized language models such as BERT. In this paper, we propose DeepImpact, a new document term-weighting scheme suitable for efficient retrieval using a standard inverted index. Compared to existing methods, DeepImpact improves impact-score modeling and tackles the vocabulary-mismatch problem. In particular, DeepImpact leverages DocT5Query to enrich the document collection and, using a contextualized language model, directly estimates the semantic importance of tokens in a document, producing a single-value representation for each token in each document. Our experiments show that DeepImpact significantly outperforms prior first-stage retrieval approaches by up to 17\% on effectiveness metrics w.r.t. DocT5Query, and, when deployed in a re-ranking scenario, can reach the same effectiveness of state-of-the-art approaches with up to 5.1× speedup in efficiency.},
	language = {en},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Mallia, Antonio and Khattab, Omar and Tonellotto, Nicola and Suel, Torsten},
	month = apr,
	year = {2021},
	note = {arXiv:2104.12016 [cs]},
	keywords = {Untagged},
}

@article{mai_3d_2021,
	title = {{3D} {Object} {Detection} with {SLS}-{Fusion} {Network} in {Foggy} {Weather} {Conditions}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/21/20/6711},
	doi = {10.3390/s21206711},
	abstract = {The role of sensors such as cameras or LiDAR (Light Detection and Ranging) is crucial for the environmental awareness of self-driving cars. However, the data collected from these sensors are subject to distortions in extreme weather conditions such as fog, rain, and snow. This issue could lead to many safety problems while operating a self-driving vehicle. The purpose of this study is to analyze the effects of fog on the detection of objects in driving scenes and then to propose methods for improvement. Collecting and processing data in adverse weather conditions is often more difficult than data in good weather conditions. Hence, a synthetic dataset that can simulate bad weather conditions is a good choice to validate a method, as it is simpler and more economical, before working with a real dataset. In this paper, we apply fog synthesis on the public KITTI dataset to generate the Multifog KITTI dataset for both images and point clouds. In terms of processing tasks, we test our previous 3D object detector based on LiDAR and camera, named the Spare LiDAR Stereo Fusion Network (SLS-Fusion), to see how it is affected by foggy weather conditions. We propose to train using both the original dataset and the augmented dataset to improve performance in foggy weather conditions while keeping good performance under normal conditions. We conducted experiments on the KITTI and the proposed Multifog KITTI datasets which show that, before any improvement, performance is reduced by 42.67\% in 3D object detection for Moderate objects in foggy weather conditions. By using a specific strategy of training, the results significantly improved by 26.72\% and keep performing quite well on the original dataset with a drop only of 8.23\%. In summary, fog often causes the failure of 3D detection on driving scenes. By additional training with the augmented dataset, we significantly improve the performance of the proposed 3D object detection algorithm for self-driving cars in foggy weather conditions.},
	language = {en},
	number = {20},
	urldate = {2022-12-05},
	journal = {Sensors},
	author = {Mai, Nguyen Anh Minh and Duthon, Pierre and Khoudour, Louahdi and Crouzil, Alain and Velastin, Sergio A.},
	month = jan,
	year = {2021},
	note = {Number: 20
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Untagged},
	pages = {6711},
}

@article{mao_composite_2021,
	title = {Composite {Adversarial} {Attacks}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17075},
	abstract = {Adversarial attack is a technique for deceiving Machine Learning (ML) models, which provides a way to evaluate the adversarial robustness. In practice, attack algorithms are artificially selected and tuned by human experts to break a ML system. However, manual selection of attackers tends to be sub-optimal, leading to a mistakenly assessment of model security. In this paper, a new procedure called Composite Adversarial Attack (CAA) is proposed for automatically searching the best combination of attack algorithms and their hyper-parameters from a candidate pool of 32 base attackers. We design a search space where attack policy is represented as an attacking sequence, i.e., the output of the previous attacker is used as the initialization input for successors. Multi-objective NSGA-II genetic algorithm is adopted for finding the strongest attack policy with minimum complexity. The experimental result shows CAA beats 10 top attackers on 11 diverse defenses with less elapsed time (6 × faster than AutoAttack), and achieves the new state-of-the-art on linf, l2 and unrestricted adversarial attacks.},
	language = {en},
	number = {10},
	urldate = {2022-05-30},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Mao, Xiaofeng and Chen, Yuefeng and Wang, Shuhui and Su, Hang and He, Yuan and Xue, Hui},
	month = may,
	year = {2021},
	note = {Number: 10},
	keywords = {Untagged},
	pages = {8884--8892},
}

@techreport{monfort_spoken_2021,
	title = {Spoken {Moments}: {Learning} {Joint} {Audio}-{Visual} {Representations} from {Video} {Descriptions}},
	shorttitle = {Spoken {Moments}},
	url = {http://arxiv.org/abs/2105.04489},
	abstract = {When people observe events, they are able to abstract key information and build concise summaries of what is happening. These summaries include contextual and semantic information describing the important high-level details (what, where, who and how) of the observed event and exclude background information that is deemed unimportant to the observer. With this in mind, the descriptions people generate for videos of different dynamic events can greatly improve our understanding of the key information of interest in each video. These descriptions can be captured in captions that provide expanded attributes for video labeling (e.g. actions/objects/scenes/sentiment/etc.) while allowing us to gain new insight into what people find important or necessary to summarize specific events. Existing caption datasets for video understanding are either small in scale or restricted to a specific domain. To address this, we present the Spoken Moments (S-MiT) dataset of 500k spoken captions each attributed to a unique short video depicting a broad range of different events. We collect our descriptions using audio recordings to ensure that they remain as natural and concise as possible while allowing us to scale the size of a large classification dataset. In order to utilize our proposed dataset, we present a novel Adaptive Mean Margin (AMM) approach to contrastive learning and evaluate our models on video/caption retrieval on multiple datasets. We show that our AMM approach consistently improves our results and that models trained on our Spoken Moments dataset generalize better than those trained on other video-caption datasets.},
	number = {arXiv:2105.04489},
	urldate = {2022-07-10},
	institution = {arXiv},
	author = {Monfort, Mathew and Jin, SouYoung and Liu, Alexander and Harwath, David and Feris, Rogerio and Glass, James and Oliva, Aude},
	month = may,
	year = {2021},
	note = {arXiv:2105.04489 [cs, eess]
type: article},
	keywords = {Untagged},
}

@techreport{meiLecture16Double2021,
	title = {Lecture 16: {Double} {Descent} and {Generalized} {Linear} {Models}},
	language = {en},
	author = {Mei, Song and Agrawal, Kumar Krishna},
	month = mar,
	year = {2021},
	keywords = {Untagged},
	pages = {6},
}

@article{melas-kyriaziYouEvenNeed2021,
	title = {Do {You} {Even} {Need} {Attention}? {A} {Stack} of {Feed}-{Forward} {Layers} {Does} {Surprisingly} {Well} on {ImageNet}},
	shorttitle = {Do {You} {Even} {Need} {Attention}?},
	url = {http://arxiv.org/abs/2105.02723},
	abstract = {The strong performance of vision transformers on image classification and other vision tasks is often attributed to the design of their multi-head attention layers. However, the extent to which attention is responsible for this strong performance remains unclear. In this short report, we ask: is the attention layer even necessary? Specifically, we replace the attention layer in a vision transformer with a feed-forward layer applied over the patch dimension. The resulting architecture is simply a series of feed-forward layers applied over the patch and feature dimensions in an alternating fashion. In experiments on ImageNet, this architecture performs surprisingly well: a ViT/DeiT-base-sized model obtains 74.9{\textbackslash}\% top-1 accuracy, compared to 77.9{\textbackslash}\% and 79.9{\textbackslash}\% for ViT and DeiT respectively. These results indicate that aspects of vision transformers other than attention, such as the patch embedding, may be more responsible for their strong performance than previously thought. We hope these results prompt the community to spend more time trying to understand why our current models are as effective as they are.},
	urldate = {2021-08-25},
	journal = {arXiv:2105.02723 [cs]},
	author = {Melas-Kyriazi, Luke},
	month = may,
	year = {2021},
	note = {arXiv: 2105.02723},
	keywords = {Untagged},
}

@inproceedings{miechThinkingFastSlow2021,
	address = {Nashville, TN, USA},
	title = {Thinking {Fast} and {Slow}: {Efficient} {Text}-to-{Visual} {Retrieval} with {Transformers}},
	isbn = {978-1-66544-509-2},
	shorttitle = {Thinking {Fast} and {Slow}},
	url = {https://ieeexplore.ieee.org/document/9577976/},
	doi = {10/gnjfs5},
	abstract = {Our objective is language-based search of large-scale image and video datasets. For this task, the approach that consists of independently mapping text and vision to a joint embedding space, a.k.a. dual encoders, is attractive as retrieval scales and is efﬁcient for billions of images using approximate nearest neighbour search. An alternative approach of using vision-text transformers with cross-attention gives considerable improvements in accuracy over the joint embeddings, but is often inapplicable in practice for large-scale retrieval given the cost of the crossattention mechanisms required for each sample at test time. This work combines the best of both worlds. We make the following three contributions. First, we equip transformerbased models with a new ﬁne-grained cross-attention architecture, providing signiﬁcant improvements in retrieval accuracy whilst preserving scalability. Second, we introduce a generic approach for combining a Fast dual encoder model with our Slow but accurate transformer-based model via distillation and re-ranking. Finally, we validate our approach on the Flickr30K image dataset where we show an increase in inference speed by several orders of magnitude while having results competitive to the state of the art. We also extend our method to the video domain, improving the state of the art on the VATEX dataset.},
	language = {en},
	urldate = {2022-01-24},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Miech, Antoine and Alayrac, Jean-Baptiste and Laptev, Ivan and Sivic, Josef and Zisserman, Andrew},
	month = jun,
	year = {2021},
	keywords = {Untagged},
	pages = {9821--9831},
}

@article{muFedProcPrototypicalContrastive2021,
	title = {{FedProc}: {Prototypical} {Contrastive} {Federated} {Learning} on {Non}-{IID} data},
	shorttitle = {{FedProc}},
	url = {http://arxiv.org/abs/2109.12273},
	abstract = {Federated learning allows multiple clients to collaborate to train high-performance deep learning models while keeping the training data locally. However, when the local data of all clients are not independent and identically distributed (i.e., non-IID), it is challenging to implement this form of efficient collaborative learning. Although significant efforts have been dedicated to addressing this challenge, the effect on the image classification task is still not satisfactory. In this paper, we propose FedProc: prototypical contrastive federated learning, which is a simple and effective federated learning framework. The key idea is to utilize the prototypes as global knowledge to correct the local training of each client. We design a local network architecture and a global prototypical contrastive loss to regulate the training of local models, which makes local objectives consistent with the global optima. Eventually, the converged global model obtains a good performance on non-IID data. Experimental results show that, compared to state-of-the-art federated learning methods, FedProc improves the accuracy by \$1.6{\textbackslash}\%{\textbackslash}sim7.9{\textbackslash}\%\$ with acceptable computation cost.},
	urldate = {2022-04-28},
	journal = {arXiv:2109.12273 [cs]},
	author = {Mu, Xutong and Shen, Yulong and Cheng, Ke and Geng, Xueli and Fu, Jiaxuan and Zhang, Tao and Zhang, Zhiwei},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.12273},
	keywords = {Untagged},
}

@article{nhoUIGANGenerativeAdversarial2021,
	title = {{UI}-{GAN}: {Generative} {Adversarial} {Network}-{Based} {Anomaly} {Detection} {Using} {User} {Initial} {Information} for {Wearable} {Devices}},
	volume = {21},
	issn = {1530-437X, 1558-1748, 2379-9153},
	shorttitle = {{UI}-{GAN}},
	url = {https://ieeexplore.ieee.org/document/9335651/},
	doi = {10.1109/JSEN.2021.3054394},
	language = {en},
	number = {8},
	urldate = {2021-06-04},
	journal = {IEEE Sensors Journal},
	author = {Nho, Young-Hoon and Ryu, Semin and Kwon, Dong-Soo},
	month = apr,
	year = {2021},
	keywords = {Untagged},
	pages = {9949--9958},
}

@article{nairAWACAcceleratingOnline2021,
	title = {{AWAC}: {Accelerating} {Online} {Reinforcement} {Learning} with {Offline} {Datasets}},
	shorttitle = {{AWAC}},
	url = {http://arxiv.org/abs/2006.09359},
	abstract = {Reinforcement learning (RL) provides an appealing formalism for learning control policies from experience. However, the classic active formulation of RL necessitates a lengthy active exploration process for each behavior, making it difﬁcult to apply in real-world settings such as robotic control. If we can instead allow RL algorithms to effectively use previously collected data to aid the online learning process, such applications could be made substantially more practical: the prior data would provide a starting point that mitigates challenges due to exploration and sample complexity, while the online training enables the agent to perfect the desired skill. Such prior data could either constitute expert demonstrations or, more generally, sub-optimal prior data that illustrates potentially useful transitions. While a number of prior methods have either used optimal demonstrations to bootstrap reinforcement learning, or have used sub-optimal data to train purely ofﬂine, it remains exceptionally difﬁcult to train a policy with potentially sub-optimal ofﬂine data and actually continue to improve it further with online RL. In this paper we systematically analyze why this problem is so challenging, and propose an algorithm that combines sampleefﬁcient dynamic programming with maximum likelihood policy updates, providing a simple and effective framework that is able to leverage large amounts of ofﬂine data and then quickly perform online ﬁne-tuning of RL policies. We show that our method, advantage weighted actor critic (AWAC), enables rapid learning of skills with a combination of prior demonstration data and online experience. We demonstrate these beneﬁts on a variety of simulated and real-world robotics domains, including dexterous manipulation with a real multi-ﬁngered hand, drawer opening with a robotic arm, and rotating a valve. Our results show that incorporating prior data can reduce the time required to learn a range of robotic skills to practical time-scales.},
	language = {en},
	urldate = {2021-11-02},
	journal = {arXiv:2006.09359 [cs, stat]},
	author = {Nair, Ashvin and Gupta, Abhishek and Dalal, Murtaza and Levine, Sergey},
	month = apr,
	year = {2021},
	note = {arXiv: 2006.09359},
	keywords = {Untagged},
}

@article{neimarkVideoTransformerNetwork2021,
	title = {Video {Transformer} {Network}},
	url = {http://arxiv.org/abs/2102.00719},
	abstract = {This paper presents VTN, a transformer-based framework for video recognition. Inspired by recent developments in vision transformers, we ditch the standard approach in video action recognition that relies on 3D ConvNets and introduce a method that classiﬁes actions by attending to the entire video sequence information. Our approach is generic and builds on top of any given 2D spatial network. In terms of wall runtime, it trains 16.1× faster and runs 5.1× faster during inference while maintaining competitive accuracy compared to other state-of-the-art methods. It enables whole video analysis, via a single end-to-end pass, while requiring 1.5× fewer GFLOPs. We report competitive results on Kinetics-400 and Moments in Time benchmarks and present an ablation study of VTN properties and the trade-off between accuracy and inference speed. We hope our approach will serve as a new baseline and start a fresh line of research in the video recognition domain. Code and models are available at: https://github.com/bomri/SlowFast/blob/ master/projects/vtn/README.md.},
	language = {en},
	urldate = {2021-11-04},
	journal = {arXiv:2102.00719 [cs]},
	author = {Neimark, Daniel and Bar, Omri and Zohar, Maya and Asselmann, Dotan},
	month = aug,
	year = {2021},
	note = {arXiv: 2102.00719},
	keywords = {Untagged},
}

@inproceedings{pelicon_zero-shot_2021,
	address = {Online},
	title = {Zero-shot {Cross}-lingual {Content} {Filtering}: {Offensive} {Language} and {Hate} {Speech} {Detection}},
	shorttitle = {Zero-shot {Cross}-lingual {Content} {Filtering}},
	url = {https://aclanthology.org/2021.hackashop-1.5},
	abstract = {We present a system for zero-shot cross-lingual offensive language and hate speech classification. The system was trained on English datasets and tested on a task of detecting hate speech and offensive social media content in a number of languages without any additional training. Experiments show an impressive ability of both models to generalize from English to other languages. There is however an expected gap in performance between the tested cross-lingual models and the monolingual models. The best performing model (offensive content classifier) is available online as a REST API.},
	urldate = {2022-11-08},
	booktitle = {Proceedings of the {EACL} {Hackashop} on {News} {Media} {Content} {Analysis} and {Automated} {Report} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Pelicon, Andraž and Shekhar, Ravi and Martinc, Matej and Škrlj, Blaž and Purver, Matthew and Pollak, Senja},
	month = apr,
	year = {2021},
	keywords = {Untagged},
	pages = {30--34},
}

@article{paulDefendingMedicalImage2021,
	title = {Defending {Medical} {Image} {Diagnostics} against {Privacy} {Attacks} using {Generative} {Methods}},
	url = {http://arxiv.org/abs/2103.03078},
	abstract = {Machine learning (ML) models used in medical imaging diagnostics can be vulnerable to a variety of privacy attacks, including membership inference attacks, that lead to violations of regulations governing the use of medical data and threaten to compromise their effective deployment in the clinic. In contrast to most recent work in privacy-aware ML that has been focused on model alteration and post-processing steps, we propose here a novel and complementary scheme that enhances the security of medical data by controlling the data sharing process. We develop and evaluate a privacy defense protocol based on using a generative adversarial network (GAN) that allows a medical data sourcer (e.g. a hospital) to provide an external agent (a modeler) a proxy dataset synthesized from the original images, so that the resulting diagnostic systems made available to model consumers is rendered resilient to privacy attackers. We validate the proposed method on retinal diagnostics AI used for diabetic retinopathy that bears the risk of possibly leaking private information. To incorporate concerns of both privacy advocates and modelers, we introduce a metric to evaluate privacy and utility performance in combination, and demonstrate, using these novel and classical metrics, that our approach, by itself or in conjunction with other defenses, provides state of the art (SOTA) performance for defending against privacy attacks.},
	urldate = {2022-04-03},
	journal = {arXiv:2103.03078 [cs, eess]},
	author = {Paul, William and Cao, Yinzhi and Zhang, Miaomiao and Burlina, Phil},
	month = aug,
	year = {2021},
	note = {arXiv: 2103.03078},
	keywords = {Untagged},
}

@article{pangBagTricksAdversarial2021,
	title = {Bag of {Tricks} for {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2010.00467},
	abstract = {Adversarial training (AT) is one of the most effective strategies for promoting model robustness. However, recent benchmarks show that most of the proposed improvements on AT are less effective than simply early stopping the training procedure. This counter-intuitive fact motivates us to investigate the implementation details of tens of AT methods. Surprisingly, we ﬁnd that the basic settings (e.g., weight decay, training schedule, etc.) used in these methods are highly inconsistent. In this work, we provide comprehensive evaluations on CIFAR-10, focusing on the effects of mostly overlooked training tricks and hyperparameters for adversarially trained models. Our empirical observations suggest that adversarial robustness is much more sensitive to some basic training settings than we thought. For example, a slightly different value of weight decay can reduce the model robust accuracy by more than 7\%, which is probable to override the potential promotion induced by the proposed methods. We conclude a baseline training setting and re-implement previous defenses to achieve new state-of-the-art results1. These facts also appeal to more concerns on the overlooked confounders when benchmarking defenses.},
	language = {en},
	urldate = {2021-11-04},
	journal = {arXiv:2010.00467 [cs, stat]},
	author = {Pang, Tianyu and Yang, Xiao and Dong, Yinpeng and Su, Hang and Zhu, Jun},
	month = mar,
	year = {2021},
	note = {arXiv: 2010.00467},
	keywords = {Untagged},
}

@article{panVideoMoCoContrastiveVideo2021,
	title = {{VideoMoCo}: {Contrastive} {Video} {Representation} {Learning} with {Temporally} {Adversarial} {Examples}},
	shorttitle = {{VideoMoCo}},
	url = {http://arxiv.org/abs/2103.05905},
	abstract = {MoCo [11] is effective for unsupervised image representation learning. In this paper, we propose VideoMoCo for unsupervised video representation learning. Given a video sequence as an input sample, we improve the temporal feature representations of MoCo from two perspectives. First, we introduce a generator to drop out several frames from this sample temporally. The discriminator is then learned to encode similar feature representations regardless of frame removals. By adaptively dropping out different frames during training iterations of adversarial learning, we augment this input sample to train a temporally robust encoder. Second, we use temporal decay to model key attenuation in the memory queue when computing the contrastive loss. As the momentum encoder updates after keys enqueue, the representation ability of these keys degrades when we use the current input sample for contrastive learning. This degradation is reﬂected via temporal decay to attend the input sample to recent keys in the queue. As a result, we adapt MoCo to learn video representations without empirically designing pretext tasks. By empowering the temporal robustness of the encoder and modeling the temporal decay of the keys, our VideoMoCo improves MoCo temporally based on contrastive learning. Experiments on benchmark datasets including UCF101 and HMDB51 show that VideoMoCo stands as a state-of-the-art video representation learning method.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2103.05905 [cs]},
	author = {Pan, Tian and Song, Yibing and Yang, Tianyu and Jiang, Wenhao and Liu, Wei},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.05905},
	keywords = {Untagged},
}

@article{ouRefiningBERTEmbeddings2021,
	title = {Refining {BERT} {Embeddings} for {Document} {Hashing} via {Mutual} {Information} {Maximization}},
	url = {http://arxiv.org/abs/2109.02867},
	abstract = {Existing unsupervised document hashing methods are mostly established on generative models. Due to the difficulties of capturing long dependency structures, these methods rarely model the raw documents directly, but instead to model the features extracted from them (e.g. bag-of-words (BOW), TFIDF). In this paper, we propose to learn hash codes from BERT embeddings after observing their tremendous successes on downstream tasks. As a first try, we modify existing generative hashing models to accommodate the BERT embeddings. However, little improvement is observed over the codes learned from the old BOW or TFIDF features. We attribute this to the reconstruction requirement in the generative hashing, which will enforce irrelevant information that is abundant in the BERT embeddings also compressed into the codes. To remedy this issue, a new unsupervised hashing paradigm is further proposed based on the mutual information (MI) maximization principle. Specifically, the method first constructs appropriate global and local codes from the documents and then seeks to maximize their mutual information. Experimental results on three benchmark datasets demonstrate that the proposed method is able to generate hash codes that outperform existing ones learned from BOW features by a substantial margin.},
	language = {en},
	urldate = {2021-10-21},
	journal = {arXiv:2109.02867 [cs]},
	author = {Ou, Zijing and Su, Qinliang and Yu, Jianxing and Zhao, Ruihui and Zheng, Yefeng and Liu, Bang},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.02867},
	keywords = {Untagged},
}

@inproceedings{prasad_what_2021,
	address = {Punta Cana, Dominican Republic},
	title = {To what extent do human explanations of model behavior align with actual model behavior?},
	url = {https://aclanthology.org/2021.blackboxnlp-1.1},
	doi = {10.18653/v1/2021.blackboxnlp-1.1},
	abstract = {Given the increasingly prominent role NLP models (will) play in our lives, it is important for human expectations of model behavior to align with actual model behavior. Using Natural Language Inference (NLI) as a case study, we investigate the extent to which human-generated explanations of models' inference decisions align with how models actually make these decisions. More specifically, we define three alignment metrics that quantify how well natural language explanations align with model sensitivity to input words, as measured by integrated gradients. Then, we evaluate eight different models (the base and large versions of BERT,RoBERTa and ELECTRA, as well as anRNN and bag-of-words model), and find that the BERT-base model has the highest alignment with human-generated explanations, for all alignment metrics. Focusing in on transformers, we find that the base versions tend to have higher alignment with human-generated explanations than their larger counterparts, suggesting that increasing the number of model parameters leads, in some cases, to worse alignment with human explanations. Finally, we find that a model's alignment with human explanations is not predicted by the model's accuracy, suggesting that accuracy and alignment are complementary ways to evaluate models.},
	urldate = {2022-12-11},
	booktitle = {Proceedings of the {Fourth} {BlackboxNLP} {Workshop} on {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Prasad, Grusha and Nie, Yixin and Bansal, Mohit and Jia, Robin and Kiela, Douwe and Williams, Adina},
	month = nov,
	year = {2021},
	keywords = {Untagged},
	pages = {1--14},
}

@inproceedings{10.1007/978-3-030-77004-4_1,
	address = {Cham},
	title = {A straightforward framework for video retrieval using {CLIP}},
	isbn = {978-3-030-77004-4},
	abstract = {Video Retrieval is a challenging task where the task aims at matching a text query to a video or vice versa. Most of the existing approaches for addressing such a problem rely on annotations made by the users. Although simple, this approach is not always feasible in practice. In this work, we explore the application of the language-image model, CLIP, to obtain video representations without the need for said annotations. This model was explicitly trained to learn a common space where images and text can be compared. Using various techniques described in this document, we extended its application to videos, obtaining state-of-the-art results on the MSR-VTT and MSVD benchmarks.},
	booktitle = {Pattern recognition},
	publisher = {Springer International Publishing},
	author = {Portillo-Quintero, Jesús Andrés and Ortiz-Bayliss, José Carlos and Terashima-Marín, Hugo},
	editor = {Roman-Rangel, Edgar and Kuri-Morales, Ángel Fernando and Martínez-Trinidad, José Francisco and Carrasco-Ochoa, Jesús Ariel and Olvera-López, José Arturo},
	year = {2021},
	keywords = {Untagged},
	pages = {3--12},
}

@inproceedings{qianLatentIndependentExcitation2021,
	title = {Latent {Independent} {Excitation} for {Generalizable} {Sensor}-based {Cross}-{Person} {Activity} {Recognition}},
	abstract = {In wearable-sensor-based activity recognition, it is often assumed that the training and the test samples follow the same data distribution. This assumption neglects practical scenarios where the activity patterns inevitably vary from person to person. To solve this problem, transfer learning and domain adaptation approaches are often leveraged to reduce the gaps between different participants. Nevertheless, these approaches require additional information (i.e., labeled or unlabeled data, meta-information) from the target domain during the training stage. In this paper, we introduce a novel method named Generalizable Independent Latent Excitation (GILE) for human activity recognition, which greatly enhances the cross-person generalization capability of the model. Our proposed method is superior to existing methods in the sense that it does not require any access to the target domain information. Besides, this novel model can be directly applied to various target domains without re-training or ﬁne-tuning. Speciﬁcally, the proposed model learns to automatically disentangle domain-agnostic and domain-speciﬁc features, the former of which are expected to be invariant across various persons. To further remove correlations between the two types of features, a novel Independent Excitation mechanism is incorporated in the latent feature space. Comprehensive experimental evaluations are conducted on three benchmark datasets to demonstrate the superiority of the proposed method over state-ofthe-art solutions.},
	language = {en},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Qian, Hangwei and Pan, Sinno Jialin and Miao, Chunyan},
	year = {2021},
	note = {Github: https://github.com/Hangwei12358/cross-person-HAR},
	keywords = {Untagged},
	pages = {9},
}

@inproceedings{prakashMultiModalFusionTransformer2021,
	title = {Multi-{Modal} {Fusion} {Transformer} for {End}-to-{End} {Autonomous} {Driving}},
	booktitle = {Conference on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Prakash, Aditya and Chitta, Kashyap and Geiger, Andreas},
	year = {2021},
	note = {Github:https://github.com/autonomousvision/transfuser},
	keywords = {Untagged},
}

@inproceedings{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {https://proceedings.mlr.press/v139/radford21a.html},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
	language = {en},
	urldate = {2023-05-12},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {8748--8763},
}

@inproceedings{qinBiPointNetBinaryNeural2021,
	title = {{BiPointNet}: {Binary} {Neural} {Network} for {Point} {Clouds}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Qin, Haotong and Cai, Zhongang and Zhang, Mingyuan and Ding, Yifu and Zhao, Haiyu and Yi, Shuai and Liu, Xianglong and Su, Hao},
	year = {2021},
	keywords = {Untagged},
}

@article{qinFcaNetFrequencyChannel2021,
	title = {{FcaNet}: {Frequency} {Channel} {Attention} {Networks}},
	shorttitle = {{FcaNet}},
	url = {http://arxiv.org/abs/2012.11879},
	abstract = {Attention mechanism, especially channel attention, has gained great success in the computer vision ﬁeld. Many works focus on how to design efﬁcient channel attention mechanisms while ignoring a fundamental problem, i.e., using global average pooling (GAP) as the unquestionable pre-processing method. In this work, we start from a different view and rethink channel attention using frequency analysis. Based on the frequency analysis, we mathematically prove that the conventional GAP is a special case of the feature decomposition in the frequency domain. With the proof, we naturally generalize the pre-processing of channel attention mechanism in the frequency domain and propose FcaNet with novel multi-spectral channel attention. The proposed method is simple but effective. We can change only one line of code in the calculation to implement our method within existing channel attention methods. Moreover, the proposed method achieves state-of-the-art results compared with other channel attention methods on image classiﬁcation, object detection, and instance segmentation tasks. Our method could improve by 1.8\% in terms of Top-1 accuracy on ImageNet compared with the baseline SENet50, with the same number of parameters and the same computational cost. Our code and models are publicly available at https://github.com/cfzd/FcaNet.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2012.11879 [cs]},
	author = {Qin, Zequn and Zhang, Pengyi and Wu, Fei and Li, Xi},
	month = feb,
	year = {2021},
	note = {arXiv: 2012.11879},
	keywords = {Untagged},
}

@inproceedings{rajaee_cluster-based_2021,
	address = {Online},
	title = {A {Cluster}-based {Approach} for {Improving} {Isotropy} in {Contextual} {Embedding} {Space}},
	url = {https://aclanthology.org/2021.acl-short.73},
	doi = {10.18653/v1/2021.acl-short.73},
	abstract = {The representation degeneration problem in Contextual Word Representations (CWRs) hurts the expressiveness of the embedding space by forming an anisotropic cone where even unrelated words have excessively positive correlations. Existing techniques for tackling this issue require a learning process to re-train models with additional objectives and mostly employ a global assessment to study isotropy. Our quantitative analysis over isotropy shows that a local assessment could be more accurate due to the clustered structure of CWRs. Based on this observation, we propose a local cluster-based method to address the degeneration issue in contextual embedding spaces. We show that in clusters including punctuations and stop words, local dominant directions encode structural information, removing which can improve CWRs performance on semantic tasks. Moreover, we find that tense information in verb representations dominates sense semantics. We show that removing dominant directions of verb representations can transform the space to better suit semantic applications. Our experiments demonstrate that the proposed cluster-based method can mitigate the degeneration problem on multiple tasks.},
	urldate = {2023-04-16},
	booktitle = {{ACL}},
	publisher = {Association for Computational Linguistics},
	author = {Rajaee, Sara and Pilehvar, Mohammad Taher},
	month = aug,
	year = {2021},
	keywords = {Untagged},
	pages = {575--584},
}

@inproceedings{ribeiro_investigating_2021,
	address = {Online},
	title = {Investigating {Pretrained} {Language} {Models} for {Graph}-to-{Text} {Generation}},
	url = {https://aclanthology.org/2021.nlp4convai-1.20},
	doi = {10.18653/v1/2021.nlp4convai-1.20},
	abstract = {Graph-to-text generation aims to generate fluent texts from graph-based data. In this paper, we investigate two recent pretrained language models (PLMs) and analyze the impact of different task-adaptive pretraining strategies for PLMs in graph-to-text generation. We present a study across three graph domains: meaning representations, Wikipedia knowledge graphs (KGs) and scientific KGs. We show that approaches based on PLMs BART and T5 achieve new state-of-the-art results and that task-adaptive pretraining strategies improve their performance even further. We report new state-of-the-art BLEU scores of 49.72 on AMR-LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative improvement of 31.8\%, 4.5\%, and 42.4\%, respectively, with our models generating significantly more fluent texts than human references. In an extensive analysis, we identify possible reasons for the PLMs' success on graph-to-text tasks. Our findings suggest that the PLMs benefit from similar facts seen during pretraining or fine-tuning, such that they perform well even when the input graph is reduced to a simple bag of node and edge labels.},
	urldate = {2023-04-11},
	booktitle = {Proceedings of the 3rd {Workshop} on {Natural} {Language} {Processing} for {Conversational} {AI}},
	publisher = {Association for Computational Linguistics},
	author = {Ribeiro, Leonardo F. R. and Schmitt, Martin and Schütze, Hinrich and Gurevych, Iryna},
	month = nov,
	year = {2021},
	keywords = {Untagged},
	pages = {211--227},
}

@inproceedings{rezaeiDifficultyMembershipInference2021,
	address = {Nashville, TN, USA},
	title = {On the {Difficulty} of {Membership} {Inference} {Attacks}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9577318/},
	doi = {10.1109/CVPR46437.2021.00780},
	abstract = {Recent studies propose membership inference (MI) attacks on deep models, where the goal is to infer if a sample has been used in the training process. Despite their apparent success, these studies only report accuracy, precision, and recall of the positive class (member class). Hence, the performance of these attacks have not been clearly reported on negative class (non-member class). In this paper, we show that the way the MI attack performance has been reported is often misleading because they suffer from high false positive rate or false alarm rate (FAR) that has not been reported. FAR shows how often the attack model mislabel non-training samples (non-member) as training (member) ones. The high FAR makes MI attacks fundamentally impractical, which is particularly more signiﬁcant for tasks such as membership inference where the majority of samples in reality belong to the negative (non-training) class. Moreover, we show that the current MI attack models can only identify the membership of misclassiﬁed samples with mediocre accuracy at best, which only constitute a very small portion of training samples.},
	language = {en},
	urldate = {2022-03-21},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Rezaei, Shahbaz and Liu, Xin},
	month = jun,
	year = {2021},
	keywords = {Untagged},
	pages = {7888--7896},
}

@article{reidSubformerExploringWeight2021,
	title = {Subformer: {Exploring} {Weight} {Sharing} for {Parameter} {Efficiency} in {Generative} {Transformers}},
	shorttitle = {Subformer},
	url = {http://arxiv.org/abs/2101.00234},
	abstract = {Transformers have shown improved performance when compared to previous architectures for sequence processing such as RNNs. Despite their sizeable performance gains, as recently suggested, the model is computationally expensive to train and with a high parameter budget. In light of this, we explore parameter-sharing methods in Transformers with a specific focus on generative models. We perform an analysis of different parameter sharing/reduction methods and develop the Subformer. Our model combines sandwich-style parameter sharing, which overcomes naive cross-layer parameter sharing in generative models, and self-attentive embedding factorization (SAFE). Experiments on machine translation, abstractive summarization and language modeling show that the Subformer can outperform the Transformer even when using significantly fewer parameters.},
	language = {en},
	urldate = {2021-11-09},
	journal = {arXiv:2101.00234 [cs]},
	author = {Reid, Machel and Marrese-Taylor, Edison and Matsuo, Yutaka},
	month = sep,
	year = {2021},
	note = {arXiv: 2101.00234},
	keywords = {Untagged},
}

@misc{roldao_3d_2021,
	title = {{3D} {Semantic} {Scene} {Completion}: a {Survey}},
	shorttitle = {{3D} {Semantic} {Scene} {Completion}},
	url = {http://arxiv.org/abs/2103.07466},
	doi = {10.48550/arXiv.2103.07466},
	abstract = {Semantic Scene Completion (SSC) aims to jointly estimate the complete geometry and semantics of a scene, assuming partial sparse input. In the last years following the multiplication of large-scale 3D datasets, SSC has gained significant momentum in the research community because it holds unresolved challenges. Specifically, SSC lies in the ambiguous completion of large unobserved areas and the weak supervision signal of the ground truth. This led to a substantially increasing number of papers on the matter. This survey aims to identify, compare and analyze the techniques providing a critical analysis of the SSC literature on both methods and datasets. Throughout the paper, we provide an in-depth analysis of the existing works covering all choices made by the authors while highlighting the remaining avenues of research. SSC performance of the SoA on the most popular datasets is also evaluated and analyzed.},
	urldate = {2022-11-09},
	publisher = {arXiv},
	author = {Roldao, Luis and de Charette, Raoul and Verroust-Blondet, Anne},
	month = jul,
	year = {2021},
	note = {arXiv:2103.07466 [cs]},
	keywords = {Untagged},
}

@inproceedings{robey_model-based_2021,
	title = {Model-{Based} {Domain} {Generalization}},
	shorttitle = {{MBDG}},
	url = {https://openreview.net/forum?id=JOxB9h40A-1},
	abstract = {We derive a principled primal-dual style algorithm for domain generalization which improves over all known algorithms by up to 30\%.},
	language = {en},
	urldate = {2022-08-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Robey, Alexander and Pappas, George J. and Hassani, Hamed},
	month = oct,
	year = {2021},
	keywords = {Untagged},
}

@inproceedings{rizveDEFENSEPSEUDOLABELINGUNCERTAINTYAWARE2021,
	title = {{IN} {DEFENSE} {OF} {PSEUDO}-{LABELING}: {AN} {UNCERTAINTY}-{AWARE} {PSEUDO}-{LABEL} {SELEC}- {TION} {FRAMEWORK} {FOR} {SEMI}-{SUPERVISED} {LEARNING}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Rizve, Mamshad Nayeem and Duarte, Kevin and Rawat, Yogesh S. and Shah, Mubarak},
	year = {2021},
	keywords = {Untagged},
}

@inproceedings{rosenfeld_risks_2021,
	title = {The {Risks} of {Invariant} {Risk} {Minimization}},
	url = {https://openreview.net/forum?id=BbNIbVPJ-42},
	abstract = {Invariant Causal Prediction (Peters et al., 2016) is a technique for out-of-distribution generalization which assumes that some aspects of the data distribution vary across the training set but that the underlying causal mechanisms remain constant. Recently, Arjovsky et al. (2019) proposed Invariant Risk Minimization (IRM), an objective based on this idea for learning deep, invariant features of data which are a complex function of latent variables; many alternatives have subsequently been suggested. However, formal guarantees for all of these works are severely lacking. In this paper, we present the first analysis of classification under the IRM objective—as well as these recently proposed alternatives—under a fairly natural and general model. In the linear case, we show simple conditions under which the optimal solution succeeds or, more often, fails to recover the optimal invariant predictor. We furthermore present the very first results in the non-linear regime: we demonstrate that IRM can fail catastrophically unless the test data is sufficiently similar to the training distribution—this is precisely the issue that it was intended to solve. Thus, in this setting we find that IRM and its alternatives fundamentally do not improve over standard Empirical Risk Minimization.},
	language = {en},
	urldate = {2022-11-14},
	author = {Rosenfeld, Elan and Ravikumar, Pradeep Kumar and Risteski, Andrej},
	month = mar,
	year = {2021},
	keywords = {Untagged},
}

@inproceedings{sagawaExtendingWILDSBenchmark2021,
	title = {Extending the {WILDS} {Benchmark} for {Unsupervised} {Adaptation}},
	url = {https://openreview.net/forum?id=z7p2V6KROOV},
	abstract = {Machine learning systems deployed in the wild are often trained on a source distribution but deployed on a different target distribution. Unlabeled data can be a powerful point of leverage for...},
	language = {en},
	urldate = {2022-05-16},
	author = {Sagawa, Shiori and Koh, Pang Wei and Lee, Tony and Gao, Irena and Xie, Sang Michael and Shen, Kendrick and Kumar, Ananya and Hu, Weihua and Yasunaga, Michihiro and Marklund, Henrik and Beery, Sara and David, Etienne and Stavness, Ian and Guo, Wei and Leskovec, Jure and Saenko, Kate and Hashimoto, Tatsunori and Levine, Sergey and Finn, Chelsea and Liang, Percy},
	month = sep,
	year = {2021},
	keywords = {Untagged},
}

@inproceedings{ruedaHumanPoseOnBody2021,
	title = {From {Human} {Pose} to {On}-{Body} {Devices} for {Human}-{Activity} {Recognition}},
	doi = {10/gkzww6},
	abstract = {Human Activity Recognition (HAR), using inertial measurements from on-body devices, has not seen a great advantage from deep architectures. This drawback is mainly due to the lack of annotated data, diversity of on-body device configurations, the class-unbalance problem, and non-standard human activity definitions. Approaches for improving the performance of such architectures, e.g., transfer learning, are therefore difficult to apply. This paper introduces a method for transfer learning from human-pose estimations as a source for improving HAR using inertial measurements obtained from on-body devices. We propose to fine-tune deep architectures, trained using sequences of human poses from a large dataset and their derivatives, for solving HAR on inertial measurements from on-body devices. Derivatives of human poses will be considered as a sort of synthetic data for HAR. We deploy two different temporal-convolutional architectures as classifiers. An evaluation of the method is carried out on three benchmark datasets improving the classification performance.},
	booktitle = {International {Conference} on {Pattern} {Recognition}},
	author = {Rueda, Fernando Moya and Fink, Gernot A.},
	year = {2021},
	note = {ISSN: 1051-4651},
	keywords = {Untagged},
	pages = {10066--10073},
}

@inproceedings{shen_umec_2021,
	title = {{UMEC}: {Unified} model and embedding compression for efficient recommendation systems},
	shorttitle = {{UMEC}},
	url = {https://openreview.net/forum?id=BM---bH_RSh},
	abstract = {The recommendation system (RS) plays an important role in the content recommendation and retrieval scenarios. The core part of the system is the Ranking neural network, which is usually a bottleneck of whole system performance during online inference. In this work, we propose a unified model and embedding compression (UMEC) framework to hammer an efficient neural network-based recommendation system. Our framework jointly learns input feature selection and neural network compression together, and solve them as an end-to-end resource-constrained optimization problem using ADMM. Our method outperforms other baselines in terms of neural network Flops, sparse embedding feature size and the number of sparse embedding features. We evaluate our method on the public benchmark of DLRM, trained over the Kaggle Criteo dataset. The codes can be found at https://github.com/VITA-Group/UMEC.},
	language = {en},
	urldate = {2023-04-24},
	author = {Shen, Jiayi and Wang, Haotao and Gui, Shupeng and Tan, Jianchao and Wang, Zhangyang and Liu, Ji},
	month = mar,
	year = {2021},
	keywords = {Untagged},
}

@inproceedings{shafranMembershipInferenceAttacks2021,
	title = {Membership {Inference} {Attacks} {Are} {Easier} on {Difficult} {Problems}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Shafran_Membership_Inference_Attacks_Are_Easier_on_Difficult_Problems_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-03-30},
	booktitle = {{ICCV}},
	author = {Shafran, Avital and Peleg, Shmuel and Hoshen, Yedid},
	year = {2021},
	keywords = {Untagged},
	pages = {14820--14829},
}

@inproceedings{shejwalkarMembershipPrivacyMachine2021,
	title = {Membership {Privacy} for {Machine} {Learning} {Models} {Through} {Knowledge} {Transfer}},
	abstract = {Large capacity machine learning (ML) models are prone to membership inference attacks (MIAs), which aim to infer whether the target sample is a member of the target model’s training dataset. The serious privacy concerns due to the membership inference have motivated multiple defenses against MIAs, e.g., differential privacy and adversarial regularization. Unfortunately, these defenses produce ML models with unacceptably low classiﬁcation performances.},
	language = {en},
	booktitle = {{AAAI}},
	author = {Shejwalkar, Virat and Houmansadr, Amir},
	year = {2021},
	keywords = {Untagged},
	pages = {9},
}

@inproceedings{shi_efcient_2021,
	title = {Efﬁcient {Cross}-{Modal} {Retrieval} via {Deep} {Binary} {Hashing} and {Quantization}},
	abstract = {Cross-modal retrieval aims to search for data with similar semantic meanings across different content modalities. However, cross-modal retrieval requires huge amounts of storage and retrieval time since it needs to process data in multiple modalities. Existing works focused on learning single-source compact features such as binary hash codes that preserve similarities between different modalities. In this work, we propose a jointly learned deep hashing and quantization network (HQ) for cross-modal retrieval. We simultaneously learn binary hash codes and quantization codes to preserve semantic information in multiple modalities by an end-to-end deep learning architecture. At the retrieval step, binary hashing is used to retrieve a subset of items from the search space, then quantization is used to re-rank the retrieved items. We theoretically and empirically show that this two-stage retrieval approach provides faster retrieval results while preserving accuracy. Experimental results on the NUS-WIDE, MIR-Flickr, and Amazon datasets demonstrate that HQ achieves boosts of more than 7\% in precision compared to supervised neural network-based compact coding models.},
	language = {en},
	booktitle = {{BMVC}},
	author = {Shi, Yang},
	year = {2021},
	keywords = {Untagged},
}

@article{shimLearningDomainAgnosticVisual2021,
	title = {Learning a {Domain}-{Agnostic} {Visual} {Representation} for {Autonomous} {Driving} via {Contrastive} {Loss}},
	url = {http://arxiv.org/abs/2103.05902},
	abstract = {Deep neural networks have been widely studied in autonomous driving applications such as semantic segmentation or depth estimation. However, training a neural network in a supervised manner requires a large amount of annotated labels which are expensive and time-consuming to collect. Recent studies leverage synthetic data collected from a virtual environment which are much easier to acquire and more accurate compared to data from the real world, but they usually suffer from poor generalization due to the inherent domain shift problem. In this paper, we propose a Domain-Agnostic Contrastive Learning (DACL) which is a two-stage unsupervised domain adaptation framework with cyclic adversarial training and contrastive loss. DACL leads the neural network to learn domain-agnostic representation to overcome performance degradation when there exists a difference between training and test data distribution. Our proposed approach achieves better performance in the monocular depth estimation task compared to previous state-of-the-art methods and also shows effectiveness in the semantic segmentation task. The ofﬁcial implementation of the paper in PyTorch is available at https://github.com/dsshim0125/dacl.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2103.05902 [cs]},
	author = {Shim, Dongseok and Kim, H. Jin},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.05902},
	keywords = {Untagged},
}

@inproceedings{shenTimeSeriesAnomaly2021,
	title = {Time {Series} {Anomaly} {Detection} with {Multiresolution} {Ensemble} {Decoding}},
	volume = {35},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17152},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Shen, Lifeng and Yu, Zhongzhong and Ma, Qianli and Kwok, James T.},
	year = {2021},
	keywords = {Untagged},
	pages = {9567--9575},
}

@techreport{shiyinluOCORoadMap2021,
	title = {{OCO} {RoadMap}},
	author = {{Shiyin Lu}},
	year = {2021},
	keywords = {Untagged},
}

@inproceedings{sinha_unnatural_2021,
	address = {Online},
	title = {{UnNatural} {Language} {Inference}},
	url = {https://aclanthology.org/2021.acl-long.569},
	doi = {10.18653/v1/2021.acl-long.569},
	abstract = {Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Understanding (NLU) models indicate that they appear to understand human-like syntax, at least to some extent. We provide novel evidence that complicates this claim: we find that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are invariant to random word-order permutations. This behavior notably differs from that of humans; we struggle to understand the meaning of ungrammatical sentences. To measure the severity of this issue, we propose a suite of metrics and investigate which properties of particular permutations lead models to be word order invariant. For example, in MNLI dataset we find almost all (98.7\%) examples contain at least one permutation which elicits the gold label. Models are even able to assign gold labels to permutations that they originally failed to predict correctly. We provide a comprehensive empirical evaluation of this phenomenon, and further show that this issue exists in pre-Transformer RNN / ConvNet based encoders, as well as across multiple languages (English and Chinese). Our code and data are available at https://github.com/facebookresearch/unlu.},
	urldate = {2023-04-12},
	booktitle = {{ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Sinha, Koustuv and Parthasarathi, Prasanna and Pineau, Joelle and Williams, Adina},
	month = aug,
	year = {2021},
	keywords = {Untagged},
	pages = {7329--7346},
}

@article{shokriPrivacyRisksModel2021,
	title = {On the {Privacy} {Risks} of {Model} {Explanations}},
	url = {http://arxiv.org/abs/1907.00164},
	abstract = {Privacy and transparency are two key foundations of trustworthy machine learning. Model explanations offer insights into a model's decisions on input data, whereas privacy is primarily concerned with protecting information about the training data. We analyze connections between model explanations and the leakage of sensitive information about the model's training set. We investigate the privacy risks of feature-based model explanations using membership inference attacks: quantifying how much model predictions plus their explanations leak information about the presence of a datapoint in the training set of a model. We extensively evaluate membership inference attacks based on feature-based model explanations, over a variety of datasets. We show that backpropagation-based explanations can leak a significant amount of information about individual training datapoints. This is because they reveal statistical information about the decision boundaries of the model about an input, which can reveal its membership. We also empirically investigate the trade-off between privacy and explanation quality, by studying the perturbation-based model explanations.},
	urldate = {2022-03-21},
	journal = {arXiv:1907.00164 [cs, stat]},
	author = {Shokri, Reza and Strobel, Martin and Zick, Yair},
	month = feb,
	year = {2021},
	note = {arXiv: 1907.00164},
	keywords = {Untagged},
}

@article{singhFLAVAFoundationalLanguage2021,
	title = {{FLAVA}: {A} {Foundational} {Language} {And} {Vision} {Alignment} {Model}},
	shorttitle = {{FLAVA}},
	url = {http://arxiv.org/abs/2112.04482},
	abstract = {State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal (with earlier fusion) but not both; and they often only target speciﬁc modalities or tasks. A promising direction would be to use a single holistic universal model, as a “foundation”, that targets all modalities at once—a true vision and language foundation model should be good at vision tasks, language tasks, and cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate impressive performance on a wide range of 35 tasks spanning these target modalities.},
	language = {en},
	urldate = {2022-01-12},
	journal = {arXiv:2112.04482 [cs]},
	author = {Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.04482},
	keywords = {Untagged},
}

@inproceedings{sinha_perturbing_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Perturbing {Inputs} for {Fragile} {Interpretations} in {Deep} {Natural} {Language} {Processing}},
	url = {https://aclanthology.org/2021.blackboxnlp-1.33},
	doi = {10.18653/v1/2021.blackboxnlp-1.33},
	abstract = {Interpretability methods like Integrated Gradient and LIME are popular choices for explaining natural language model predictions with relative word importance scores. These interpretations need to be robust for trustworthy NLP applications in high-stake areas like medicine or finance. Our paper demonstrates how interpretations can be manipulated by making simple word perturbations on an input text. Via a small portion of word-level swaps, these adversarial perturbations aim to make the resulting text semantically and spatially similar to its seed input (therefore sharing similar interpretations). Simultaneously, the generated examples achieve the same prediction label as the seed yet are given a substantially different explanation by the interpretation methods. Our experiments generate fragile interpretations to attack two SOTA interpretation methods, across three popular Transformer models and on three different NLP datasets. We observe that the rank order correlation and top-K intersection score drops by over 20\% when less than 10\% of words are perturbed on average. Further, rank-order correlation keeps decreasing as more words get perturbed. Furthermore, we demonstrate that candidates generated from our method have good quality metrics.},
	urldate = {2022-12-11},
	booktitle = {Proceedings of the {Fourth} {BlackboxNLP} {Workshop} on {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Sinha, Sanchit and Chen, Hanjie and Sekhon, Arshdeep and Ji, Yangfeng and Qi, Yanjun},
	month = nov,
	year = {2021},
	keywords = {Untagged},
	pages = {420--434},
}

@techreport{soSecuringSecureAggregation2021,
	title = {Securing {Secure} {Aggregation}: {Mitigating} {Multi}-{Round} {Privacy} {Leakage} in {Federated} {Learning}},
	shorttitle = {Securing {Secure} {Aggregation}},
	url = {http://eprint.iacr.org/2021/771},
	abstract = {Secure aggregation is a critical component in federated learning, which enables the server to learn the aggregate model of the users without observing their local models. Conventionally, secure aggregation algorithms focus only on ensuring the privacy of individual users in a single training round. We contend that such designs can lead to significant privacy leakages over multiple training rounds, due to partial user selection/participation at each round of federated learning. In fact, we empirically show that the conventional random user selection strategies for federated learning lead to leaking users' individual models within number of rounds linear in the number of users. To address this challenge, we introduce a secure aggregation framework with multi-round privacy guarantees. In particular, we introduce a new metric to quantify the privacy guarantees of federated learning over multiple training rounds, and develop a structured user selection strategy that guarantees the long-term privacy of each user (over any number of training rounds). Our framework also carefully accounts for the fairness and the average number of participating users at each round. We perform several experiments on MNIST and CIFAR-10 datasets in the IID and the non-IID settings to demonstrate the performance improvement over the baseline algorithms, both in terms of privacy protection and test accuracy.},
	number = {771},
	urldate = {2021-11-19},
	author = {So, Jinhyun and Ali, Ramy E. and Guler, Basak and Jiao, Jiantao and Avestimehr, Salman},
	year = {2021},
	keywords = {Untagged},
}

@misc{su_whitening_2021,
	title = {Whitening {Sentence} {Representations} for {Better} {Semantics} and {Faster} {Retrieval}},
	url = {http://arxiv.org/abs/2103.15316},
	abstract = {Pre-training models such as BERT have achieved great success in many natural language processing tasks. However, how to obtain better sentence representation through these pre-training models is still worthy to exploit. Previous work has shown that the anisotropy problem is an critical bottleneck for BERT-based sentence representation which hinders the model to fully utilize the underlying semantic features. Therefore, some attempts of boosting the isotropy of sentence distribution, such as flow-based model, have been applied to sentence representations and achieved some improvement. In this paper, we find that the whitening operation in traditional machine learning can similarly enhance the isotropy of sentence representations and achieve competitive results. Furthermore, the whitening technique is also capable of reducing the dimensionality of the sentence representation. Our experimental results show that it can not only achieve promising performance but also significantly reduce the storage cost and accelerate the model retrieval speed.},
	urldate = {2023-04-16},
	publisher = {arXiv},
	author = {Su, Jianlin and Cao, Jiarun and Liu, Weijie and Ou, Yangyiwen},
	month = mar,
	year = {2021},
	note = {arXiv:2103.15316 [cs]},
	keywords = {Untagged},
}

@inproceedings{stevens_investigation_2021,
	address = {Punta Cana, Dominican Republic},
	title = {An {Investigation} of {Language} {Model} {Interpretability} via {Sentence} {Editing}},
	url = {https://aclanthology.org/2021.blackboxnlp-1.34},
	doi = {10.18653/v1/2021.blackboxnlp-1.34},
	abstract = {Pre-trained language models (PLMs) like BERT are being used for almost all language-related tasks, but interpreting their behavior still remains a significant challenge and many important questions remain largely unanswered. In this work, we re-purpose a sentence editing dataset, where faithful high-quality human rationales can be automatically extracted and compared with extracted model rationales, as a new testbed for interpretability. This enables us to conduct a systematic investigation on an array of questions regarding PLMs' interpretability, including the role of pre-training procedure, comparison of rationale extraction methods, and different layers in the PLM. The investigation generates new insights, for example, contrary to the common understanding, we find that attention weights correlate well with human rationales and work better than gradient-based saliency in extracting model rationales. Both the dataset and code will be released to facilitate future interpretability research.},
	urldate = {2022-12-12},
	booktitle = {Proceedings of the {Fourth} {BlackboxNLP} {Workshop} on {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Stevens, Samuel and Su, Yu},
	month = nov,
	year = {2021},
	keywords = {Untagged},
	pages = {435--446},
}

@inproceedings{songSystematicEvaluationPrivacy2021,
	title = {Systematic evaluation of privacy risks of machine learning models},
	url = {https://www.usenix.org/conference/usenixsecurity21/presentation/song},
	booktitle = {30th {USENIX} security symposium, {USENIX} security 2021, august 11-13, 2021},
	publisher = {USENIX Association},
	author = {Song, Liwei and Mittal, Prateek},
	editor = {Bailey, Michael and Greenstadt, Rachel},
	year = {2021},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/uss/SongM21.bib
tex.timestamp: Thu, 16 Sep 2021 17:32:10 +0200},
	keywords = {Untagged},
	pages = {2615--2632},
}

@article{sunFSCEFewShotObject2021,
	title = {{FSCE}: {Few}-{Shot} {Object} {Detection} via {Contrastive} {Proposal} {Encoding}},
	shorttitle = {{FSCE}},
	url = {http://arxiv.org/abs/2103.05950},
	abstract = {Emerging interests have been brought to recognize previously unseen objects given very few training examples, known as few-shot object detection (FSOD). Recent researches demonstrate that good feature embedding is the key to reach favorable few-shot learning performance. We observe object proposals with different Intersection-of-Union (IoU) scores are analogous to the intra-image augmentation used in contrastive approaches. And we exploit this analogy and incorporate supervised contrastive learning to achieve more robust objects representations in FSOD. We present Few-Shot object detection via Contrastive proposals Encoding (FSCE), a simple yet effective approach to learning contrastive-aware object proposal encodings that facilitate the classification of detected objects. We notice the degradation of average precision (AP) for rare objects mainly comes from misclassifying novel instances as confusable classes. And we ease the misclassification issues by promoting instance level intra-class compactness and inter-class variance via our contrastive proposal encoding loss (CPE loss). Our design outperforms current state-of-the-art works in any shot and all data splits, with up to +8.8\% on standard benchmark PASCAL VOC and +2.7\% on challenging COCO benchmark. Code is available at: https: //github.com/MegviiDetection/FSCE},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2103.05950 [cs]},
	author = {Sun, Bo and Li, Banghuai and Cai, Shengcai and Yuan, Ye and Zhang, Chi},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.05950},
	keywords = {Untagged},
}

@article{tanakaFakeimageDetectionRobust2021,
	title = {Fake-image detection with {Robust} {Hashing}},
	url = {http://arxiv.org/abs/2102.01313},
	abstract = {In this paper, we investigate whether robust hashing has a possibility to robustly detect fake-images even when multiple manipulation techniques such as JPEG compression are applied to images for the ﬁrst time. In an experiment, the proposed fake detection with robust hashing is demonstrated to outperform state-of-the-art one under the use of various datasets including fake images generated with GANs.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2102.01313 [cs]},
	author = {Tanaka, Miki and Kiya, Hitoshi},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.01313},
	keywords = {Untagged},
}

@inproceedings{sunLightningDOTPretrainingVisualSemantic2021,
	title = {{LightningDOT}: {Pre}-training {Visual}-{Semantic} {Embeddings} for {Real}-{Time} {Image}-{Text} {Retrieval}},
	url = {https://doi.org/10.18653/v1/2021.naacl-main.77},
	doi = {10/gm5tjs},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL}-{HLT} 2021, {Online}, {June} 6-11, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Sun, Siqi and Chen, Yen-Chun and Li, Linjie and Wang, Shuohang and Fang, Yuwei and Liu, Jingjing},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tür, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	year = {2021},
	keywords = {Untagged},
	pages = {982--997},
}

@inproceedings{sunSoteriaProvableDefense2021,
	title = {Soteria: {Provable} {Defense} {Against} {Privacy} {Leakage} in {Federated} {Learning} {From} {Representation} {Perspective}},
	shorttitle = {Soteria},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Sun_Soteria_Provable_Defense_Against_Privacy_Leakage_in_Federated_Learning_From_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-11-19},
	author = {Sun, Jingwei and Li, Ang and Wang, Binghui and Yang, Huanrui and Li, Hai and Chen, Yiran},
	year = {2021},
	keywords = {Untagged},
	pages = {9311--9319},
}

@article{tangMitigatingMembershipInference2021,
	title = {Mitigating {Membership} {Inference} {Attacks} by {Self}-{Distillation} {Through} a {Novel} {Ensemble} {Architecture}},
	url = {http://arxiv.org/abs/2110.08324},
	abstract = {Membership inference attacks are a key measure to evaluate privacy leakage in machine learning (ML) models. These attacks aim to distinguish training members from non-members by exploiting differential behavior of the models on member and non-member inputs. The goal of this work is to train ML models that have high membership privacy while largely preserving their utility; we therefore aim for an empirical membership privacy guarantee as opposed to the provable privacy guarantees provided by techniques like differential privacy, as such techniques are shown to deteriorate model utility. Specifically, we propose a new framework to train privacy-preserving models that induces similar behavior on member and non-member inputs to mitigate membership inference attacks. Our framework, called SELENA, has two major components. The first component and the core of our defense is a novel ensemble architecture for training. This architecture, which we call Split-AI, splits the training data into random subsets, and trains a model on each subset of the data. We use an adaptive inference strategy at test time: our ensemble architecture aggregates the outputs of only those models that did not contain the input sample in their training data. We prove that our Split-AI architecture defends against a large family of membership inference attacks, however, it is susceptible to new adaptive attacks. Therefore, we use a second component in our framework called Self-Distillation to protect against such stronger attacks. The Self-Distillation component (self-)distills the training dataset through our Split-AI ensemble, without using any external public datasets. Through extensive experiments on major benchmark datasets we show that SELENA presents a superior trade-off between membership privacy and utility compared to the state of the art.},
	urldate = {2022-03-21},
	journal = {arXiv:2110.08324 [cs]},
	author = {Tang, Xinyu and Mahloujifar, Saeed and Song, Liwei and Shejwalkar, Virat and Nasr, Milad and Houmansadr, Amir and Mittal, Prateek},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.08324},
	keywords = {Untagged},
}

@article{taySynthesizerRethinkingSelfAttention2021,
	title = {Synthesizer: {Rethinking} {Self}-{Attention} in {Transformer} {Models}},
	shorttitle = {Synthesizer},
	url = {http://arxiv.org/abs/2005.00743},
	abstract = {The dot product self-attention is known to be central and indispensable to state-of-the-art Transformer models. But is it really required? This paper investigates the true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models. Via extensive experiments, we ﬁnd that (1) random alignment matrices surprisingly perform quite competitively and (2) learning attention weights from token-token (query-key) interactions is useful but not that important after all. To this end, we propose SYNTHESIZER, a model that learns synthetic attention weights without token-token interactions. In our experiments, we ﬁrst show that simple Synthesizers achieve highly competitive performance when compared against vanilla Transformer models across a range of tasks, including machine translation, language modeling, text generation and GLUE/SuperGLUE benchmarks. When composed with dot product attention, we ﬁnd that Synthesizers consistently outperform Transformers. Moreover, we conduct additional comparisons of Synthesizers against Dynamic Convolutions, showing that simple Random Synthesizer is not only 60\% faster but also improves perplexity by a relative 3.5\%. Finally, we show that simple factorized Synthesizers can outperform Linformers on encoding only tasks.},
	language = {en},
	urldate = {2021-11-04},
	journal = {arXiv:2005.00743 [cs]},
	author = {Tay, Yi and Bahri, Dara and Metzler, Donald and Juan, Da-Cheng and Zhao, Zhe and Zheng, Che},
	month = may,
	year = {2021},
	note = {arXiv: 2005.00743},
	keywords = {Untagged},
}

@inproceedings{tang1bitAdamCommunication2021,
	title = {1-bit {Adam}: {Communication} {Efficient} {Large}-{Scale} {Training} with {Adam}’s {Convergence} {Speed}},
	shorttitle = {1-bit {Adam}},
	url = {https://proceedings.mlr.press/v139/tang21a.html},
	abstract = {Scalable training of large models (like BERT and GPT-3) requires careful optimization rooted in model design, architecture, and system capabilities. From a system standpoint, communication has become a major bottleneck, especially on commodity systems with standard TCP interconnects that offer limited network bandwidth. Communication compression is an important technique to reduce training time on such systems. One of the most effective ways to compress communication is via error compensation compression, which offers robust convergence speed, even under 1-bit compression. However, state-of-the-art error compensation techniques only work with basic optimizers like SGD and momentum SGD, which are linearly dependent on the gradients. They do not work with non-linear gradient-based optimizers like Adam, which offer state-of-the-art convergence efficiency and accuracy for models like BERT. In this paper, we propose 1-bit Adam that reduces the communication volume by up to 5x, offers much better scalability, and provides the same convergence speed as uncompressed Adam. Our key finding is that Adam’s variance becomes stable (after a warmup phase) and can be used as a fixed precondition for the rest of the training (compression phase). We performed experiments on up to 256 GPUs and show that 1-bit Adam enables up to 3.3x higher throughput for BERT-Large pre-training and up to 2.9x higher throughput for SQuAD fine-tuning. In addition, we provide theoretical analysis for 1-bit Adam.},
	language = {en},
	urldate = {2022-02-22},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tang, Hanlin and Gan, Shaoduo and Awan, Ammar Ahmad and Rajbhandari, Samyam and Li, Conglong and Lian, Xiangru and Liu, Ji and Zhang, Ce and He, Yuxiong},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {10118--10129},
}

@article{tolstikhinMLPMixerAllMLPArchitecture2021,
	title = {{MLP}-{Mixer}: {An} all-{MLP} {Architecture} for {Vision}},
	shorttitle = {{MLP}-{Mixer}},
	url = {http://arxiv.org/abs/2105.01601},
	abstract = {Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.},
	urldate = {2021-08-19},
	journal = {arXiv:2105.01601 [cs]},
	author = {Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
	month = jun,
	year = {2021},
	note = {arXiv: 2105.01601},
	keywords = {Untagged},
}

@article{touvronResMLPFeedforwardNetworks2021,
	title = {{ResMLP}: {Feedforward} networks for image classification with data-efficient training},
	shorttitle = {{ResMLP}},
	url = {http://arxiv.org/abs/2105.03404},
	abstract = {We present ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity trade-offs on ImageNet. We also train ResMLP models in a self-supervised setup, to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results. We share pre-trained models and our code based on the Timm library.},
	urldate = {2021-08-19},
	journal = {arXiv:2105.03404 [cs]},
	author = {Touvron, Hugo and Bojanowski, Piotr and Caron, Mathilde and Cord, Matthieu and El-Nouby, Alaaeldin and Grave, Edouard and Izacard, Gautier and Joulin, Armand and Synnaeve, Gabriel and Verbeek, Jakob and Jégou, Hervé},
	month = jun,
	year = {2021},
	note = {arXiv: 2105.03404},
	keywords = {Untagged},
}

@article{torfiNaturalLanguageProcessing2021,
	title = {Natural {Language} {Processing} {Advancements} {By} {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Natural {Language} {Processing} {Advancements} {By} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2003.01200},
	abstract = {Natural Language Processing (NLP) helps empower intelligent machines by enhancing a better understanding of the human language for linguistic-based human-computer communication. Recent developments in computational power and the advent of large amounts of linguistic data have heightened the need and demand for automating semantic analysis using data-driven approaches. The utilization of data-driven strategies is pervasive now due to the significant improvements demonstrated through the usage of deep learning methods in areas such as Computer Vision, Automatic Speech Recognition, and in particular, NLP. This survey categorizes and addresses the different aspects and applications of NLP that have benefited from deep learning. It covers core NLP tasks and applications and describes how deep learning methods and models advance these areas. We further analyze and compare different approaches and state-of-the-art models.},
	urldate = {2021-11-22},
	journal = {arXiv:2003.01200 [cs]},
	author = {Torfi, Amirsina and Shirvani, Rouzbeh A. and Keneshloo, Yaser and Tavaf, Nader and Fox, Edward A.},
	month = feb,
	year = {2021},
	note = {arXiv: 2003.01200},
	keywords = {Untagged},
}

@inproceedings{touvronTrainingDataefficientImage2021,
	title = {Training data-efficient image transformers \& distillation through attention},
	url = {https://proceedings.mlr.press/v139/touvron21a.html},
	abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2\% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {10347--10357},
}

@inproceedings{wallace_concealed_2021,
	address = {Online},
	title = {Concealed {Data} {Poisoning} {Attacks} on {NLP} {Models}},
	url = {https://aclanthology.org/2021.naacl-main.13},
	doi = {10.18653/v1/2021.naacl-main.13},
	abstract = {Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model's training set that causes the model to frequently predict Positive whenever the input contains “James Bond”. Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (“Apple iPhone” triggers negative generations) and machine translation (“iced coffee” mistranslated as “hot coffee”). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.},
	urldate = {2022-11-07},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Wallace, Eric and Zhao, Tony and Feng, Shi and Singh, Sameer},
	month = jun,
	year = {2021},
	keywords = {Untagged},
	pages = {139--150},
}

@article{tursynbekRobustnessThreatsDifferential2021,
	title = {Robustness {Threats} of {Differential} {Privacy}},
	url = {http://arxiv.org/abs/2012.07828},
	abstract = {Differential privacy (DP) is a gold-standard concept of measuring and guaranteeing privacy in data analysis. It is well-known that the cost of adding DP to deep learning model is its accuracy. However, it remains unclear how it affects robustness of the model. Standard neural networks are not robust to different input perturbations: either adversarial attacks or common corruptions. In this paper, we empirically observe an interesting trade-off between privacy and robustness of neural networks. We experimentally demonstrate that networks, trained with DP, in some settings might be even more vulnerable in comparison to non-private versions. To explore this, we extensively study different robustness measurements, including FGSM and PGD adversaries, distance to linear decision boundaries, curvature profile, and performance on a corrupted dataset. Finally, we study how the main ingredients of differentially private neural networks training, such as gradient clipping and noise addition, affect (decrease and increase) the robustness of the model.},
	urldate = {2021-09-27},
	journal = {arXiv:2012.07828 [cs]},
	author = {Tursynbek, Nurislam and Petiushko, Aleksandr and Oseledets, Ivan},
	month = aug,
	year = {2021},
	note = {arXiv: 2012.07828},
	keywords = {Untagged},
}

@article{touvronGoingDeeperImage2021,
	title = {Going deeper with {Image} {Transformers}},
	url = {http://arxiv.org/abs/2103.17239},
	abstract = {Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of image transformers has been little studied so far. In this work, we build and optimize deeper transformer networks for image classification. In particular, we investigate the interplay of architecture and optimization of such dedicated transformers. We make two transformers architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models whose performance does not saturate early with more depth, for instance we obtain 86.5\% top-1 accuracy on Imagenet when training with no external data, we thus attain the current SOTA with less FLOPs and parameters. Moreover, our best model establishes the new state of the art on Imagenet with Reassessed labels and Imagenet-V2 / match frequency, in the setting with no additional training data. We share our code and models.},
	urldate = {2021-08-12},
	journal = {arXiv:2103.17239 [cs]},
	author = {Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and Jégou, Hervé},
	month = apr,
	year = {2021},
	note = {arXiv: 2103.17239},
	keywords = {Untagged},
}

@inproceedings{pmlr-v134-vanerven21a,
	series = {Proceedings of machine learning research},
	title = {Robust online convex optimization in the presence of outliers},
	volume = {134},
	url = {https://proceedings.mlr.press/v134/vanerven21a.html},
	abstract = {We consider online convex optimization when a number k of data points are outliers that may be corrupted. We model this by introducing the notion of robust regret, which measures the regret only on rounds that are not outliers. The aim for the learner is to achieve small robust regret, without knowing where the outliers are. If the outliers are chosen adversarially, we show that a simple filtering strategy on extreme gradients incurs O(k) overhead compared to the usual regret bounds, and that this is unimprovable, which means that k needs to be sublinear in the number of rounds. We further ask which additional assumptions would allow for a linear number of outliers. It turns out that the usual benign cases of independently, identically distributed (i.i.d.) observations or strongly convex losses are not sufficient. However, combining i.i.d. observations with the assumption that outliers are those observations that are in an extreme quantile of the distribution, does lead to sublinear robust regret, even though the expected number of outliers is linear.},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {van Erven, Tim and Sachs, Sarah and Koolen, Wouter M and Kotlowski, Wojciech},
	editor = {Belkin, Mikhail and Kpotufe, Samory},
	month = aug,
	year = {2021},
	note = {tex.pdf: http://proceedings.mlr.press/v134/vanerven21a/vanerven21a.pdf},
	keywords = {Untagged},
	pages = {4174--4194},
}

@inproceedings{wangImplicitBiasAdaptive2021,
	title = {The {Implicit} {Bias} for {Adaptive} {Optimization} {Algorithms} on {Homogeneous} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v139/wang21q.html},
	abstract = {Despite their overwhelming capacity to overfit, deep neural networks trained by specific optimization algorithms tend to generalize relatively well to unseen data. Recently, researchers explained it by investigating the implicit bias of optimization algorithms. A remarkable progress is the work (Lyu \& Li, 2019), which proves gradient descent (GD) maximizes the margin of homogeneous deep neural networks. Except the first-order optimization algorithms like GD, adaptive algorithms such as AdaGrad, RMSProp and Adam are popular owing to their rapid training process. Mean-while, numerous works have provided empirical evidence that adaptive methods may suffer from poor generalization performance. However, theoretical explanation for the generalization of adaptive optimization algorithms is still lacking. In this paper, we study the implicit bias of adaptive optimization algorithms on homogeneous neural networks. In particular, we study the convergent direction of parameters when they are optimizing the logistic loss. We prove that the convergent direction of Adam and RMSProp is the same as GD, while for AdaGrad, the convergent direction depends on the adaptive conditioner. Technically, we provide a unified framework to analyze convergent direction of adaptive optimization algorithms by constructing novel and nontrivial adaptive gradient flow and surrogate margin. The theoretical findings explain the superiority on generalization of exponential moving average strategy that is adopted by RMSProp and Adam. To the best of knowledge, it is the first work to study the convergent direction of adaptive optimizations on non-linear deep neural networks},
	language = {en},
	urldate = {2022-03-10},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wang, Bohan and Meng, Qi and Chen, Wei and Liu, Tie-Yan},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {10849--10858},
}

@article{wangRepresentativeForgeryMining2021,
	title = {Representative {Forgery} {Mining} for {Fake} {Face} {Detection}},
	url = {http://arxiv.org/abs/2104.06609},
	abstract = {Although vanilla Convolutional Neural Network (CNN) based detectors can achieve satisfactory performance on fake face detection, we observe that the detectors tend to seek forgeries on a limited region of face, which reveals that the detectors is short of understanding of forgery. Therefore, we propose an attention-based data augmentation framework to guide detector reﬁne and enlarge its attention. Speciﬁcally, our method tracks and occludes the Top-N sensitive facial regions, encouraging the detector to mine deeper into the regions ignored before for more representative forgery. Especially, our method is simple-touse and can be easily integrated with various CNN models. Extensive experiments show that the detector trained with our method is capable to separately point out the representative forgery of fake faces generated by different manipulation techniques, and our method enables a vanilla CNN-based detector to achieve state-of-the-art performance without structure modiﬁcation. Our code is available at https://github.com/crywang/RFM .},
	language = {en},
	urldate = {2021-06-04},
	journal = {arXiv:2104.06609 [cs]},
	author = {Wang, Chengrui and Deng, Weihong},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.06609},
	keywords = {Untagged},
}

@article{wang_pre-training_2021,
	title = {Pre-{Training} by {Completing} {Point} {Clouds}},
	url = {https://openreview.net/forum?id=jPSYH47QSZL&referrer=%5Bthe%20profile%20of%20Xiangyu%20Yue%5D(%2Fprofile%3Fid%3D~Xiangyu_Yue1)},
	abstract = {There has recently been a flurry of exciting advances in deep learning models on point clouds. However, these advances have been hampered by the difficulty of creating labelled point cloud...},
	language = {en},
	urldate = {2022-10-06},
	author = {Wang, Hanchen and Liu, Qi and Yue, Xiangyu and Lasenby, Joan and Kusner, Matt},
	month = mar,
	year = {2021},
	keywords = {Untagged},
}

@inproceedings{wang_unsupervised_2021,
	address = {Montreal, QC, Canada},
	title = {Unsupervised {Point} {Cloud} {Pre}-training via {Occlusion} {Completion}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710667/},
	doi = {10.1109/ICCV48922.2021.00964},
	language = {en},
	urldate = {2022-10-06},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Wang, Hanchen and Liu, Qi and Yue, Xiangyu and Lasenby, Joan and Kusner, Matt J.},
	month = oct,
	year = {2021},
	keywords = {Untagged},
	pages = {9762--9772},
}

@article{wangGeneralizingUnseenDomains2021,
	title = {Generalizing to {Unseen} {Domains}: {A} {Survey} on {Domain} {Generalization}},
	shorttitle = {Generalizing to {Unseen} {Domains}},
	url = {http://arxiv.org/abs/2103.03097},
	abstract = {Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets and applications. Finally, we summarize existing literature and present some potential research topics for the future.},
	urldate = {2021-08-25},
	journal = {arXiv:2103.03097 [cs]},
	author = {Wang, Jindong and Lan, Cuiling and Liu, Chang and Ouyang, Yidong and Zeng, Wenjun and Qin, Tao},
	month = jul,
	year = {2021},
	note = {arXiv: 2103.03097},
	keywords = {Untagged},
}

@inproceedings{wangDenseContrastiveLearning2021,
	title = {Dense {Contrastive} {Learning} for {Self}-{Supervised} {Visual} {Pre}-{Training}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Dense_Contrastive_Learning_for_Self-Supervised_Visual_Pre-Training_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-08-19},
	author = {Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},
	year = {2021},
	keywords = {Untagged},
	pages = {3024--3033},
}

@inproceedings{9413946,
	title = {Benign overfitting in binary classification of gaussian mixtures},
	doi = {10.1109/ICASSP39728.2021.9413946},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} international conference on acoustics, speech and signal processing ({ICASSP})},
	author = {Wang, Ke and Thrampoulidis, Christos},
	year = {2021},
	keywords = {Untagged},
	pages = {4030--4034},
}

@inproceedings{wangClusterFormerClusteringbasedSparse2021,
	series = {Findings of {ACL}},
	title = {Cluster-{Former}: {Clustering}-based {Sparse} {Transformer} for {Question} {Answering}},
	volume = {ACL/IJCNLP 2021},
	url = {https://doi.org/10.18653/v1/2021.findings-acl.346},
	doi = {10/gm5tjq},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}/{IJCNLP} 2021, {Online} {Event}, {August} 1-6, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Shuohang and Zhou, Luowei and Gan, Zhe and Chen, Yen-Chun and Fang, Yuwei and Sun, Siqi and Cheng, Yu and Liu, Jingjing},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	year = {2021},
	keywords = {Untagged},
	pages = {3958--3968},
}

@inproceedings{wangMembershipInferenceAttack2021,
	address = {Montreal, Canada},
	title = {Against {Membership} {Inference} {Attack}: {Pruning} is {All} {You} {Need}},
	isbn = {978-0-9992411-9-6},
	shorttitle = {Against {Membership} {Inference} {Attack}},
	url = {https://www.ijcai.org/proceedings/2021/432},
	doi = {10.24963/ijcai.2021/432},
	abstract = {The large model size, high computational operations, and vulnerability against membership inference attack (MIA) have impeded deep learning or deep neural networks (DNNs) popularity, especially on mobile devices. To address the challenge, we envision that the weight pruning technique will help DNNs against MIA while reducing model storage and computational operation. In this work, we propose a pruning algorithm, and we show that the proposed algorithm can ﬁnd a subnetwork that can prevent privacy leakage from MIA and achieves competitive accuracy with the original DNNs. We also verify our theoretical insights with experiments. Our experimental results illustrate that the attack accuracy using model compression is up to 13.6\% and 10\% lower than that of the baseline and Min-Max game, accordingly.},
	language = {en},
	urldate = {2022-03-21},
	booktitle = {Proceedings of the {Thirtieth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Wang, Yijue and Wang, Chenghong and Wang, Zigeng and Zhou, Shanglin and Liu, Hang and Bi, Jinbo and Ding, Caiwen and Rajasekaran, Sanguthevar},
	month = aug,
	year = {2021},
	keywords = {Untagged},
	pages = {3141--3147},
}

@inproceedings{wangPrototypeSupervisedAdversarialNetwork2021,
	title = {Prototype-{Supervised} {Adversarial} {Network} for {Targeted} {Attack} of {Deep} {Hashing}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Prototype-Supervised_Adversarial_Network_for_Targeted_Attack_of_Deep_Hashing_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-07-02},
	author = {Wang, Xunguang and Zhang, Zheng and Wu, Baoyuan and Shen, Fumin and Lu, Guangming},
	year = {2021},
	keywords = {Untagged},
	pages = {16357--16366},
}

@inproceedings{wangDeepUnifiedCrossModality2021,
	title = {Deep {Unified} {Cross}-{Modality} {Hashing} by {Pairwise} {Data} {Alignment}},
	url = {https://doi.org/10.24963/ijcai.2021/156},
	doi = {10/gnf8rs},
	booktitle = {Proceedings of the {Thirtieth} {International} {Joint} {Conference} on {Artificial} {Intelligence}, {IJCAI}-21},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Wang, Yimu and Xue, Bo and Cheng, Quan and Chen, Yuhui and Zhang, Lijun},
	editor = {Zhou, Zhi-Hua},
	month = aug,
	year = {2021},
	keywords = {Untagged},
	pages = {1129--1135},
}

@article{wuVisualandLanguageNavigationSurvey2021,
	title = {Visual-and-{Language} {Navigation}: {A} {Survey} and {Taxonomy}},
	shorttitle = {Visual-and-{Language} {Navigation}},
	url = {http://arxiv.org/abs/2108.11544},
	abstract = {An agent that can understand natural-language instruction and carry out corresponding actions in the visual world is one of the long-term challenges of Artiﬁcial Intelligent (AI). Due to multifarious instructions from humans, it requires the agent can combine natural language to vision and action in unstructured, previously unseen environments. If the instruction given by human is a navigation task, this challenge is called Visual-and-Language Navigation (VLN). It is a booming multidisciplinary ﬁeld of increasing importance and with extraordinary practicality. Instead of focusing on the details of speciﬁc methods, this paper provides a comprehensive survey on VLN tasks and makes a classiﬁcation carefully according the different characteristics of language instructions in these tasks. According to when the instructions are given, the tasks can be divided into single-turn and multi-turn. For single-turn tasks, we further divided them into goal-orientation and route-orientation based on whether the instructions contain a route. For multi-turn tasks, we divided them into imperative task and interactive task based on whether the agent responses to the instructions. This taxonomy enable researchers to better grasp the key point of a speciﬁc task and identify directions for future research.},
	language = {en},
	urldate = {2021-10-21},
	journal = {arXiv:2108.11544 [cs]},
	author = {Wu, Wansen and Chang, Tao and Li, Xinmeng},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.11544},
	keywords = {Untagged},
}

@inproceedings{wuCompletingPictureRandomized2021,
	title = {Completing the {Picture}: {Randomized} {Smoothing} {Suffers} from the {Curse} of {Dimensionality} for a {Large} {Family} of {Distributions}},
	shorttitle = {Completing the {Picture}},
	url = {https://proceedings.mlr.press/v130/wu21d.html},
	abstract = {Randomized smoothing is currently the most competitive technique for providing provable robustness guarantees. Since this approach is model-agnostic and inherently scalable we can certify arbitrary classifiers. Despite its success, recent works show that for a small class of i.i.d. distributions, the largest \$l\_p\$ radius that can be certified using randomized smoothing decreases as \$O(1/d{\textasciicircum}\{1/2-1/p\})\$ with dimension \$d\$ for \$p {\textgreater} 2\$. We complete the picture and show that similar no-go results hold for the \$l\_2\$ norm for a much more general family of distributions which are continuous and symmetric about the origin. Specifically, we calculate two different upper bounds of the \$l\_2\$ certified radius which have a constant multiplier of order \${\textbackslash}Theta(1/d{\textasciicircum}\{1/2\})\$. Moreover, we extend our results to \$l\_p (p{\textgreater}2)\$ certification with spherical symmetric distributions solidifying the limitations of randomized smoothing. We discuss the implications of our results for how accuracy and robustness are related, and why robust training with noise augmentation can alleviate some of the limitations in practice. We also show that on real-world data the gap between the certified radius and our upper bounds is small.},
	language = {en},
	urldate = {2022-02-22},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Wu, Yihan and Bojchevski, Aleksandar and Kuvshinov, Aleksei and Günnemann, Stephan},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {3763--3771},
}

@article{xiaoEarlyConvolutionsHelp2021,
	title = {Early {Convolutions} {Help} {Transformers} {See} {Better}},
	url = {http://arxiv.org/abs/2106.14881},
	abstract = {Vision transformer (ViT) models exhibit substandard optimizability. In particular, they are sensitive to the choice of optimizer (AdamW vs. SGD), optimizer hyperparameters, and training schedule length. In comparison, modern convolutional neural networks are far easier to optimize. Why is this the case? In this work, we conjecture that the issue lies with the patchify stem of ViT models, which is implemented by a stride-p pxp convolution (p=16 by default) applied to the input image. This large-kernel plus large-stride convolution runs counter to typical design choices of convolutional layers in neural networks. To test whether this atypical design choice causes an issue, we analyze the optimization behavior of ViT models with their original patchify stem versus a simple counterpart where we replace the ViT stem by a small number of stacked stride-two 3x3 convolutions. While the vast majority of computation in the two ViT designs is identical, we find that this small change in early visual processing results in markedly different training behavior in terms of the sensitivity to optimization settings as well as the final model accuracy. Using a convolutional stem in ViT dramatically increases optimization stability and also improves peak performance (by {\textasciitilde}1-2\% top-1 accuracy on ImageNet-1k), while maintaining flops and runtime. The improvement can be observed across the wide spectrum of model complexities (from 1G to 36G flops) and dataset scales (from ImageNet-1k to ImageNet-21k). These findings lead us to recommend using a standard, lightweight convolutional stem for ViT models as a more robust architectural choice compared to the original ViT model design.},
	urldate = {2021-08-25},
	journal = {arXiv:2106.14881 [cs]},
	author = {Xiao, Tete and Singh, Mannat and Mintun, Eric and Darrell, Trevor and Dollár, Piotr and Girshick, Ross},
	month = jul,
	year = {2021},
	note = {arXiv: 2106.14881},
	keywords = {Untagged},
}

@article{xuHOWNEURALNETWORKS2021,
	title = {{HOW} {NEURAL} {NETWORKS} {EXTRAPOLATE}: {FROM} {FEEDFORWARD} {TO} {GRAPH} {NEURAL} {NETWORKS}},
	abstract = {We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of the training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while feedforward neural networks, a.k.a. multilayer perceptrons (MLPs), do not extrapolate well in certain simple tasks, Graph Neural Networks (GNNs), a structured network with MLP modules, have shown some success in more complex tasks. Working towards a theoretical explanation, we identify conditions under which MLPs and GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly converge to linear functions along any direction from the origin, which implies that ReLU MLPs do not extrapolate most nonlinear functions. But, they can provably learn a linear target function when the training distribution is sufﬁciently “diverse”. Second, in connection to analyzing the successes and limitations of GNNs, these results suggest a hypothesis for which we provide theoretical and empirical evidence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or edge weights) relies on encoding task-speciﬁc non-linearities in the architecture or features. Our theoretical analysis builds on a connection of over-parameterized networks to the neural tangent kernel. Empirically, our theory holds across different training settings.},
	language = {en},
	author = {Xu, Keyulu and Zhang, Mozhi and Li, Jingling and Du, Simon S and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
	year = {2021},
	keywords = {Untagged},
	pages = {52},
}

@article{xieSelfSupervisedLearningSwin2021,
	title = {Self-{Supervised} {Learning} with {Swin} {Transformers}},
	url = {http://arxiv.org/abs/2105.04553},
	abstract = {We are witnessing a modeling shift from CNN to Transformers in computer vision. In this work, we present a self-supervised learning approach called MoBY, with Vision Transformers as its backbone architecture. The approach basically has no new inventions, which is combined from MoCo v2 and BYOL and tuned to achieve reasonably high accuracy on ImageNet-1K linear evaluation: 72.8\% and 75.0\% top-1 accuracy using DeiT-S and Swin-T, respectively, by 300-epoch training. The performance is slightly better than recent works of MoCo v3 and DINO which adopt DeiT as the backbone, but with much lighter tricks. More importantly, the general-purpose Swin Transformer backbone enables us to also evaluate the learnt representations on downstream tasks such as object detection and semantic segmentation, in contrast to a few recent approaches built on ViT/DeiT which only report linear evaluation results on ImageNet-1K due to ViT/DeiT not tamed for these dense prediction tasks. We hope our results can facilitate more comprehensive evaluation of self-supervised learning methods designed for Transformer architectures. Our code and models are available at https://github.com/SwinTransformer/Transformer-SSL, which will be continually enriched.},
	urldate = {2021-08-12},
	journal = {arXiv:2105.04553 [cs]},
	author = {Xie, Zhenda and Lin, Yutong and Yao, Zhuliang and Zhang, Zheng and Dai, Qi and Cao, Yue and Hu, Han},
	month = may,
	year = {2021},
	note = {arXiv: 2105.04553},
	keywords = {Untagged},
}

@inproceedings{yan_consert_2021,
	address = {Online},
	title = {{ConSERT}: {A} {Contrastive} {Framework} for {Self}-{Supervised} {Sentence} {Representation} {Transfer}},
	shorttitle = {{ConSERT}},
	url = {https://aclanthology.org/2021.acl-long.393},
	doi = {10.18653/v1/2021.acl-long.393},
	abstract = {Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8\% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Yan, Yuanmeng and Li, Rumei and Wang, Sirui and Zhang, Fuzheng and Wu, Wei and Xu, Weiran},
	month = aug,
	year = {2021},
	keywords = {Untagged},
	pages = {5065--5075},
}

@inproceedings{xu_spg_2021,
	address = {Montreal, QC, Canada},
	title = {{SPG}: {Unsupervised} {Domain} {Adaptation} for {3D} {Object} {Detection} via {Semantic} {Point} {Generation}},
	isbn = {978-1-66542-812-5},
	shorttitle = {{SPG}},
	url = {https://ieeexplore.ieee.org/document/9711355/},
	doi = {10.1109/ICCV48922.2021.01516},
	language = {en},
	urldate = {2022-12-01},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Xu, Qiangeng and Zhou, Yin and Wang, Weiyue and Qi, Charles R. and Anguelov, Dragomir},
	month = oct,
	year = {2021},
	keywords = {Untagged},
	pages = {15426--15436},
}

@misc{yang_st3d_2021,
	title = {{ST3D}++: {Denoised} {Self}-training for {Unsupervised} {Domain} {Adaptation} on {3D} {Object} {Detection}},
	shorttitle = {{ST3D}++},
	url = {http://arxiv.org/abs/2108.06682},
	abstract = {In this paper, we present a self-training method, named ST3D++, with a holistic pseudo label denoising pipeline for unsupervised domain adaptation on 3D object detection. ST3D++ aims at reducing noise in pseudo label generation as well as alleviating the negative impacts of noisy pseudo labels on model training. First, ST3D++ pre-trains the 3D object detector on the labeled source domain with random object scaling (ROS) which is designed to reduce target domain pseudo label noise arising from object scale bias of the source domain. Then, the detector is progressively improved through alternating between generating pseudo labels and training the object detector with pseudo-labeled target domain data. Here, we equip the pseudo label generation process with a hybrid quality-aware triplet memory to improve the quality and stability of generated pseudo labels. Meanwhile, in the model training stage, we propose a source data assisted training strategy and a curriculum data augmentation policy to effectively rectify noisy gradient directions and avoid model over-fitting to noisy pseudo labeled data. These specific designs enable the detector to be trained on meticulously refined pseudo labeled target data with denoised training signals, and thus effectively facilitate adapting an object detector to a target domain without requiring annotations. Finally, our method is assessed on four 3D benchmark datasets (i.e., Waymo, KITTI, Lyft, and nuScenes) for three common categories (i.e., car, pedestrian and bicycle). ST3D++ achieves state-of-the-art performance on all evaluated settings, outperforming the corresponding baseline by a large margin (e.g., 9.6\% \${\textbackslash}sim\$ 38.16\% on Waymo \${\textbackslash}rightarrow\$ KITTI in terms of AP\$\_\{{\textbackslash}text\{3D\}\}\$), and even surpasses the fully supervised oracle results on the KITTI 3D object detection benchmark with target prior. Code will be available.},
	urldate = {2023-02-15},
	publisher = {arXiv},
	author = {Yang, Jihan and Shi, Shaoshuai and Wang, Zhe and Li, Hongsheng and Qi, Xiaojuan},
	month = aug,
	year = {2021},
	note = {arXiv:2108.06682 [cs]},
	keywords = {Untagged},
}

@inproceedings{yang_3d-man_2021,
	address = {Nashville, TN, USA},
	title = {{3D}-{MAN}: {3D} {Multi}-frame {Attention} {Network} for {Object} {Detection}},
	isbn = {978-1-66544-509-2},
	shorttitle = {{3D}-{MAN}},
	url = {https://ieeexplore.ieee.org/document/9577561/},
	doi = {10.1109/CVPR46437.2021.00190},
	language = {en},
	urldate = {2023-06-01},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yang, Zetong and Zhou, Yin and Chen, Zhifeng and Ngiam, Jiquan},
	month = jun,
	year = {2021},
	keywords = {Untagged},
	pages = {1863--1872},
}

@article{yangUnderstandingRobustnessTeacherStudent2021,
	title = {Understanding {Robustness} in {Teacher}-{Student} {Setting}: {A} {New} {Perspective}},
	language = {en},
	author = {Yang, Zhuolin and Chen, Zhaoxi},
	year = {2021},
	keywords = {Untagged},
	pages = {11},
}

@inproceedings{yaoJoSRCContrastiveApproach2021,
	title = {Jo-{SRC}: {A} {Contrastive} {Approach} for {Combating} {Noisy} {Labels}},
	shorttitle = {Jo-{SRC}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Yao_Jo-SRC_A_Contrastive_Approach_for_Combating_Noisy_Labels_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-07-02},
	author = {Yao, Yazhou and Sun, Zeren and Zhang, Chuanyi and Shen, Fumin and Wu, Qi and Zhang, Jian and Tang, Zhenmin},
	year = {2021},
	keywords = {Untagged},
	pages = {5192--5201},
}

@article{yeEnhancedMembershipInference2021,
	title = {Enhanced {Membership} {Inference} {Attacks} against {Machine} {Learning} {Models}},
	url = {http://arxiv.org/abs/2111.09679},
	abstract = {How much does a given trained model leak about each individual data record in its training set? Membership inference attacks are used as an auditing tool to quantify the private information that a model leaks about the individual data points in its training set. The attacks are inﬂuenced by different uncertainties that an attacker has to resolve about training data, the training algorithm, and the underlying data distribution. Thus attack success rates, of many attacks in the literature, do not precisely capture the information leakage of models about their data, as they also reﬂect other uncertainties that the attack algorithm has. In this paper, we explain the implicit assumptions and also the simpliﬁcations made in prior work using the framework of hypothesis testing. We also derive new attack algorithms from the framework that can achieve a high AUC score while also highlighting the different factors that affect their performance. Our algorithms capture a very precise approximation of privacy loss in models, and can be used as a tool to perform an accurate and informed estimation of privacy risk in machine learning models. We provide a thorough empirical evaluation of our attack strategies on various machine learning tasks and benchmark datasets.},
	language = {en},
	urldate = {2021-11-26},
	journal = {arXiv:2111.09679 [cs, stat]},
	author = {Ye, Jiayuan and Maddi, Aadyaa and Murakonda, Sasi Kumar and Shokri, Reza},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.09679},
	keywords = {Untagged},
}

@article{yinComprehensiveSurveyPrivacypreserving2021,
	title = {A {Comprehensive} {Survey} of {Privacy}-preserving {Federated} {Learning}: {A} {Taxonomy}, {Review}, and {Future} {Directions}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {A {Comprehensive} {Survey} of {Privacy}-preserving {Federated} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3460427},
	doi = {10/gnh5d3},
	abstract = {The past four years have witnessed the rapid development of federated learning (FL). However, new privacy concerns have also emerged during the aggregation of the distributed intermediate results. The emerging privacy-preserving FL (PPFL) has been heralded as a solution to generic privacy-preserving machine learning. However, the challenge of protecting data privacy while maintaining the data utility through machine learning still remains. In this article, we present a comprehensive and systematic survey on the PPFL based on our proposed 5W-scenario-based taxonomy. We analyze the privacy leakage risks in the FL from five aspects, summarize existing methods, and identify future research directions.},
	language = {en},
	number = {6},
	urldate = {2021-11-24},
	journal = {ACM Computing Surveys},
	author = {Yin, Xuefei and Zhu, Yanming and Hu, Jiankun},
	month = jul,
	year = {2021},
	keywords = {Untagged},
	pages = {1--36},
}

@inproceedings{yin_defending_2021,
	address = {New York, NY, USA},
	series = {{KDD} '21},
	title = {Defending privacy against more knowledgeable membership inference attackers},
	isbn = {978-1-4503-8332-5},
	url = {https://doi.org/10.1145/3447548.3467444},
	doi = {10.1145/3447548.3467444},
	abstract = {Membership Inference Attack (MIA) in deep learning is a common form of privacy attack which aims to infer whether a data sample is in a target classifier's training dataset or not. Previous studies of MIA typically tackle either a black-box or a white-box adversary model, assuming an attacker not knowing (or knowing) the structure and parameters of the target classifier while having access to the confidence vector of the query output. With the popularity of privacy protection methods such as differential privacy, it is increasingly easier for an attacker to obtain the defense method adopted by the target classifier, which poses extra challenge to privacy protection. In this paper, we name such attacker a crystal-box adversary. We present definitions for utility and privacy of target classifier, and formulate the design goal of the defense method as an optimization problem. We also conduct theoretical analysis on the respective forms of the optimization for three adversary models, namely black-box, white-box, and crystal-box, and prove that the optimization problem is NP-hard. Thereby we solve a surrogate problem and propose three defense methods, which, if used together, can make trade-off between utility and privacy. A notable advantage of our approach is that it can be used to resist attacks from three adversary models, namely black-box, white-box, and crystal-box, simultaneously. Evaluation results show effectiveness of our proposed approach for defending privacy against MIA and better performance compared to previous defense methods.},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} conference on knowledge discovery \&amp; data mining},
	publisher = {Association for Computing Machinery},
	author = {Yin, Yu and Chen, Ke and Shou, Lidan and Chen, Gang},
	year = {2021},
	note = {Number of pages: 11
Place: Virtual Event, Singapore},
	keywords = {Untagged},
	pages = {2026--2036},
}

@article{yuSelfsupervisedAsymmetricDeep2021,
	title = {Self-supervised asymmetric deep hashing with margin-scalable constraint for image retrieval},
	url = {http://arxiv.org/abs/2012.03820},
	abstract = {Due to its eﬀectivity and eﬃciency, image retrieval based on deep hashing approaches is widely used especially for large-scale visual search. However, many existing deep hashing methods inadequately utilize label information as guidance for feature learning networks without more advanced exploration of the semantic space. Besides the similarity correlations in the Hamming space are not fully discovered and embedded into hash codes, by which the retrieval quality is diminished with ineﬃcient preservation of pairwise correlations and multi-label semantics. To cope with these problems, we propose a novel self-supervised asymmetric deep hashing method with a margin-scalable constraint(SADH) approach for image retrieval. SADH implements a self-supervised network to preserve semantic information in a semantic feature map and a semantic code map for the semantics of the given dataset, which eﬃciently and precisely guides a feature learning network to preserve multi-label semantic information using an asymmetric learning strategy. Moreover, for the feature learning part, by further exploiting semantic maps, a new margin-scalable constraint is employed for both highly-accurate construction of pairwise correlations in the hamming space and a more discriminative hash code representation. Extensive empirical research on three benchmark datasets validates the proposed method and shows it outperforms several state-of-the-art approaches. The source codes URL of our SADH is: http://github.com/SWU-CS-MediaLab/SADH.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2012.03820 [cs]},
	author = {Yu, Zhengyang and Dou, Zhihao and Bakker, Erwin M. and Wu, Song},
	month = jan,
	year = {2021},
	note = {arXiv: 2012.03820},
	keywords = {Untagged},
}

@article{yuanComprehensiveReviewBinary2021,
	title = {A comprehensive review of {Binary} {Neural} {Network}},
	url = {https://arxiv.org/abs/2110.06804v2},
	abstract = {Binary Neural Network (BNN) method is an extreme application of convolutional neural network (CNN) parameter quantization. As opposed to the original CNN methods which employed floating-point computation with full-precision weights and activations, BBN uses 1-bit activations and weights. With BBNs, a significant amount of storage, network complexity and energy consumption can be reduced, and neural networks can be implemented more efficiently in embedded applications. Unfortunately, binarization causes severe information loss. A gap still exists between full-precision CNN models and their binarized counterparts. The recent developments in BNN have led to a lot of algorithms and solutions that have helped address this issue. This article provides a full overview of recent developments in BNN. The present paper focuses exclusively on 1-bit activations and weights networks, as opposed to previous surveys in which low-bit works are mixed in. In this paper, we conduct a complete investigation of BNN's development from their predecessors to the latest BNN algorithms and techniques, presenting a broad design pipeline, and discussing each module's variants. Along the way, this paper examines BNN (a) purpose: their early successes and challenges; (b) BNN optimization: selected representative works that contain key optimization techniques; (c) deployment: open-source frameworks for BNN modeling and development; (d) terminal: efficient computing architectures and devices for BNN and (e) applications: diverse applications with BNN. Moreover, this paper discusses potential directions and future research opportunities for the latest BNN algorithms and techniques, presents a broad design pipeline, and discusses each module's variants.},
	language = {en},
	urldate = {2022-02-23},
	author = {Yuan, Chunyu and Agaian, Sos S.},
	month = oct,
	year = {2021},
	keywords = {Untagged},
}

@article{yuanIncorporatingConvolutionDesigns2021,
	title = {Incorporating {Convolution} {Designs} into {Visual} {Transformers}},
	url = {http://arxiv.org/abs/2103.11816},
	abstract = {Motivated by the success of Transformers in natural language processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new {\textbackslash}textbf\{Convolution-enhanced image Transformer (CeiT)\} which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: {\textbackslash}textbf\{1)\} instead of the straightforward tokenization from raw input images, we design an {\textbackslash}textbf\{Image-to-Tokens (I2T)\} module that extracts patches from generated low-level features; {\textbackslash}textbf\{2)\} the feed-froward network in each encoder block is replaced with a {\textbackslash}textbf\{Locally-enhanced Feed-Forward (LeFF)\} layer that promotes the correlation among neighboring tokens in the spatial dimension; {\textbackslash}textbf\{3)\} a {\textbackslash}textbf\{Layer-wise Class token Attention (LCA)\} is attached at the top of the Transformer that utilizes the multi-level representations. Experimental results on ImageNet and seven downstream tasks show the effectiveness and generalization ability of CeiT compared with previous Transformers and state-of-the-art CNNs, without requiring a large amount of training data and extra CNN teachers. Besides, CeiT models also demonstrate better convergence with \$3{\textbackslash}times\$ fewer training iterations, which can reduce the training cost significantly{\textbackslash}footnote\{Code and models will be released upon acceptance.\}.},
	urldate = {2021-08-12},
	journal = {arXiv:2103.11816 [cs]},
	author = {Yuan, Kun and Guo, Shaopeng and Liu, Ziwei and Zhou, Aojun and Yu, Fengwei and Wu, Wei},
	month = apr,
	year = {2021},
	note = {arXiv: 2103.11816},
	keywords = {Untagged},
}

@inproceedings{yueCounterfactualZeroShotOpenSet2021,
	title = {Counterfactual {Zero}-{Shot} and {Open}-{Set} {Visual} {Recognition}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Yue_Counterfactual_Zero-Shot_and_Open-Set_Visual_Recognition_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-08-19},
	author = {Yue, Zhongqi and Wang, Tan and Sun, Qianru and Hua, Xian-Sheng and Zhang, Hanwang},
	year = {2021},
	keywords = {Untagged},
	pages = {15404--15414},
}

@article{zaheerBigBirdTransformers2021,
	title = {Big {Bird}: {Transformers} for {Longer} {Sequences}},
	shorttitle = {Big {Bird}},
	url = {http://arxiv.org/abs/2007.14062},
	abstract = {Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BIGBIRD, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BIGBIRD is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the beneﬁts of having O(1) global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BIGBIRD drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.},
	language = {en},
	urldate = {2021-11-04},
	journal = {arXiv:2007.14062 [cs, stat]},
	author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
	month = jan,
	year = {2021},
	note = {arXiv: 2007.14062},
	keywords = {Untagged},
}

@inproceedings{zhangCertifyingLinfinityRobustness2021,
	title = {Towards {Certifying} {L}-infinity {Robustness} using {Neural} {Networks} with {L}-inf-dist {Neurons}},
	url = {http://arxiv.org/abs/2102.05363},
	abstract = {It is well-known that standard neural networks, even with a high classification accuracy, are vulnerable to small \${\textbackslash}ell\_{\textbackslash}infty\$-norm bounded adversarial perturbations. Although many attempts have been made, most previous works either can only provide empirical verification of the defense to a particular attack method, or can only develop a certified guarantee of the model robustness in limited scenarios. In this paper, we seek for a new approach to develop a theoretically principled neural network that inherently resists \${\textbackslash}ell\_{\textbackslash}infty\$ perturbations. In particular, we design a novel neuron that uses \${\textbackslash}ell\_{\textbackslash}infty\$-distance as its basic operation (which we call \${\textbackslash}ell\_{\textbackslash}infty\$-dist neuron), and show that any neural network constructed with \${\textbackslash}ell\_{\textbackslash}infty\$-dist neurons (called \${\textbackslash}ell\_\{{\textbackslash}infty\}\$-dist net) is naturally a 1-Lipschitz function with respect to \${\textbackslash}ell\_{\textbackslash}infty\$-norm. This directly provides a rigorous guarantee of the certified robustness based on the margin of prediction outputs. We then prove that such networks have enough expressive power to approximate any 1-Lipschitz function with robust generalization guarantee. We further provide a holistic training strategy that can greatly alleviate optimization difficulties. Experimental results show that using \${\textbackslash}ell\_\{{\textbackslash}infty\}\$-dist nets as basic building blocks, we consistently achieve state-of-the-art performance on commonly used datasets: 93.09\% certified accuracy on MNIST (\${\textbackslash}epsilon=0.3\$), 35.42\% on CIFAR-10 (\${\textbackslash}epsilon=8/255\$) and 16.31\% on TinyImageNet (\${\textbackslash}epsilon=1/255\$).},
	urldate = {2021-10-29},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Zhang, Bohang and Cai, Tianle and Lu, Zhou and He, Di and Wang, Liwei},
	year = {2021},
	note = {arXiv: 2102.05363},
	keywords = {Untagged},
}

@article{zhangBoostingCertifiedRobustness2021,
	title = {Boosting the {Certified} {Robustness} of {L}-infinity {Distance} {Nets}},
	url = {http://arxiv.org/abs/2110.06850},
	abstract = {Recently, Zhang et al. (2021) developed a new neural network architecture based on ℓ∞-distance functions, which naturally possesses certiﬁed robustness by its construction. Despite the excellent theoretical properties, the model so far can only achieve comparable performance to conventional networks. In this paper, we signiﬁcantly boost the certiﬁed robustness of ℓ∞-distance nets through a careful analysis of its training process. In particular, we show the ℓp-relaxation, a crucial way to overcome the non-smoothness of the model, leads to an unexpected large Lipschitz constant at the early training stage. This makes the optimization insufﬁcient using hinge loss and produces sub-optimal solutions. Given these ﬁndings, we propose a simple approach to address the issues above by using a novel objective function which combines a scaled cross-entropy loss with clipped hinge loss. Our experiments show that using the proposed training strategy, the certiﬁed accuracy of ℓ∞-distance net can be dramatically improved from 33.30\% to 40.06\% on CIFAR-10 (ǫ = 8/255), meanwhile signiﬁcantly outperforming other approaches in this area. Such result clearly demonstrates the effectiveness and potential of ℓ∞-distance net for certiﬁed robustness.},
	language = {en},
	urldate = {2021-10-29},
	journal = {arXiv:2110.06850 [cs, stat]},
	author = {Zhang, Bohang and Jiang, Du and He, Di and Wang, Liwei},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.06850},
	keywords = {Untagged},
}

@inproceedings{zhang_unifying_2021,
	title = {Unifying {Likelihood}-free {Inference} with {Black}-box {Optimization} and {Beyond}},
	url = {https://openreview.net/forum?id=1HxTO6CTkz},
	abstract = {Black-box optimization formulations for biological sequence design have drawn recent attention due to their promising potential impact on the pharmaceutical industry. In this work, we propose to...},
	language = {en},
	urldate = {2022-06-05},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhang, Dinghuai and Fu, Jie and Bengio, Yoshua and Courville, Aaron},
	year = {2021},
	keywords = {Untagged},
}

@article{zhangInterpretingMultivariateShapley2021,
	title = {Interpreting {Multivariate} {Shapley} {Interactions} in {DNNs}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17299},
	language = {en},
	number = {12},
	urldate = {2021-07-16},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhang, Hao and Xie, Yichen and Zheng, Longjie and Zhang, Die and Zhang, Quanshi},
	month = may,
	year = {2021},
	note = {Number: 12},
	keywords = {Untagged},
	pages = {10877--10886},
}

@inproceedings{zhangConditionalUNetConditionawareDeep2021,
	title = {Conditional-{UNet}: {A} {Condition}-aware {Deep} {Model} for {Coherent} {Human} {Activity} {Recognition} {From} {Wearables}},
	shorttitle = {Conditional-{UNet}},
	doi = {10/gkzww5},
	abstract = {Recognizing human activities from multi-channel time series data collected from wearable sensors has become an important practical application of machine learning. A serious challenge comes from the presence of coherent activities or body movements, such as movements of the head while walking or sitting, since signals representing these movements are mixed and interfere with each other. Basic multi-label classification typically assumes independence within the multiple activities. This is oversimplified and reduces modeling power even when using state-of-the-art deep learning methods. In this paper, we investigate this new problem, which we name “Coherent Human Activity Recognition (Co-HAR)”, that keeps complete conditional dependency between the multiple labels. Additionally, we treat Co-HAR as a dense labelling problem that classifies each sample on a time step with multiple coherent labels to provide high-fidelity and duration-sensitive support to high-precision applications. To explicitly model conditional dependency, a novel condition-aware deep architecture “Conditional-UNet” is developed to allow for multiple dense labeling for Co-HAR. We also contribute a first-of-its-kind Co-HAR dataset for head gesture recognition associated with a user's activity, walking or sitting, to the research community. Extensive experiments on this dataset show that our model outperforms state-of-the-art deep learning methods and achieves up to 92\% accuracy on context-based head gesture classification.},
	booktitle = {International {Conference} on {Pattern} {Recognition}},
	author = {Zhang, Liming and Zhang, Wenbin and Japkowicz, Nathalie},
	year = {2021},
	note = {ISSN: 1051-4651},
	keywords = {Untagged},
	pages = {5889--5896},
}

@inproceedings{DBLP:conf/mm/ZhangDHY21,
	title = {Joint-teaching: {Learning} to refine knowledge for resource-constrained unsupervised cross-modal retrieval},
	url = {https://doi.org/10.1145/3474085.3475286},
	doi = {10.1145/3474085.3475286},
	booktitle = {{MM} '21: {ACM} multimedia conference, virtual event, china, october 20 - 24, 2021},
	publisher = {ACM},
	author = {Zhang, Peng-Fei and Duan, Jiasheng and Huang, Zi and Yin, Hongzhi},
	editor = {Shen, Heng Tao and Zhuang, Yueting and Smith, John R. and Yang, Yang and Cesar, Pablo and Metze, Florian and Prabhakaran, Balakrishnan},
	year = {2021},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/mm/ZhangDHY21.bib
tex.timestamp: Wed, 20 Oct 2021 12:40:01 +0200},
	keywords = {Untagged},
	pages = {1517--1525},
}

@article{zhangGroupCAMGroupScoreWeighted2021,
	title = {Group-{CAM}: {Group} {Score}-{Weighted} {Visual} {Explanations} for {Deep} {Convolutional} {Networks}},
	shorttitle = {Group-{CAM}},
	url = {http://arxiv.org/abs/2103.13859},
	abstract = {In this paper, we propose an efficient saliency map generation method, called Group score-weighted Class Activation Mapping (Group-CAM), which adopts the "split-transform-merge" strategy to generate saliency maps. Specifically, for an input image, the class activations are firstly split into groups. In each group, the sub-activations are summed and de-noised as an initial mask. After that, the initial masks are transformed with meaningful perturbations and then applied to preserve sub-pixels of the input (i.e., masked inputs), which are then fed into the network to calculate the confidence scores. Finally, the initial masks are weighted summed to form the final saliency map, where the weights are confidence scores produced by the masked inputs. Group-CAM is efficient yet effective, which only requires dozens of queries to the network while producing target-related saliency maps. As a result, Group-CAM can be served as an effective data augment trick for fine-tuning the networks. We comprehensively evaluate the performance of Group-CAM on common-used benchmarks, including deletion and insertion tests on ImageNet-1k, and pointing game tests on COCO2017. Extensive experimental results demonstrate that Group-CAM achieves better visual performance than the current state-of-the-art explanation approaches. The code is available at https://github.com/wofmanaf/Group-CAM.},
	urldate = {2021-07-06},
	journal = {arXiv:2103.13859 [cs]},
	author = {Zhang, Qinglong and Rao, Lu and Yang, Yubin},
	month = jun,
	year = {2021},
	note = {Github: https://github.com/wofmanaf/Group-CAM},
	keywords = {Untagged},
}

@inproceedings{zhangDeepStableLearning2021,
	title = {Deep {Stable} {Learning} for {Out}-of-{Distribution} {Generalization}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Deep_Stable_Learning_for_Out-of-Distribution_Generalization_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-07-06},
	author = {Zhang, Xingxuan and Cui, Peng and Xu, Renzhe and Zhou, Linjun and He, Yue and Shen, Zheyan},
	year = {2021},
	keywords = {Untagged},
	pages = {5372--5382},
}

@inproceedings{zhang_self-supervised_2021,
	address = {Montreal, QC, Canada},
	title = {Self-{Supervised} {Pretraining} of {3D} {Features} on any {Point}-{Cloud}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710368/},
	doi = {10.1109/ICCV48922.2021.01009},
	abstract = {Pretraining on large labeled datasets is a prerequisite to achieve good performance in many computer vision tasks like image recognition, video understanding etc. However, pretraining is not widely used for 3D recognition tasks where state-of-the-art methods train models from scratch. A primary reason is the lack of large annotated datasets because 3D data labelling is time-consuming. Recent work shows that self-supervised learning is useful to pretrain models in 3D but requires multi-view data and point correspondences. We present a simple self-supervised pretraining method that can work with single-view depth scans acquired by varied sensors, without 3D registration and point correspondences. We pretrain standard point cloud and voxel based model architectures, and show that joint pretraining further improves performance. We evaluate our models on 9 benchmarks for object detection, semantic segmentation, and object classification, where they achieve state-of-the-art results. Most notably, we set a new state-ofthe-art for object detection on ScanNet (69.0\% mAP) and SUNRGBD (63.5\% mAP). Our pretrained models are label efficient and improve performance for classes with few examples.},
	language = {en},
	urldate = {2022-10-06},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhang, Zaiwei and Girdhar, Rohit and Joulin, Armand and Misra, Ishan},
	month = oct,
	year = {2021},
	keywords = {Untagged},
	pages = {10232--10243},
}

@article{zhangImprovingSemisupervisedFederated2021,
	title = {Improving {Semi}-supervised {Federated} {Learning} by {Reducing} the {Gradient} {Diversity} of {Models}},
	url = {http://arxiv.org/abs/2008.11364},
	abstract = {Federated learning (FL) is a promising way to use the computing power of mobile devices while maintaining the privacy of users. Current work in FL, however, makes the unrealistic assumption that the users have ground-truth labels on their devices, while also assuming that the server has neither data nor labels. In this work, we consider the more realistic scenario where the users have only unlabeled data, while the server has some labeled data, and where the amount of labeled data is smaller than the amount of unlabeled data. We call this learning problem semi-supervised federated learning (SSFL). For SSFL, we demonstrate that a critical issue that affects the test accuracy is the large gradient diversity of the models from different users. Based on this, we investigate several design choices. First, we find that the so-called consistency regularization loss (CRL), which is widely used in semi-supervised learning, performs reasonably well but has large gradient diversity. Second, we find that Batch Normalization (BN) increases gradient diversity. Replacing BN with the recently-proposed Group Normalization (GN) can reduce gradient diversity and improve test accuracy. Third, we show that CRL combined with GN still has a large gradient diversity when the number of users is large. Based on these results, we propose a novel grouping-based model averaging method to replace the FedAvg averaging method. Overall, our grouping-based averaging, combined with GN and CRL, achieves better test accuracy than not just a contemporary paper on SSFL in the same settings ({\textgreater}10{\textbackslash}\%), but also four supervised FL algorithms.},
	urldate = {2022-03-04},
	journal = {arXiv:2008.11364 [cs, stat]},
	author = {Zhang, Zhengming and Yang, Yaoqing and Yao, Zhewei and Yan, Yujun and Gonzalez, Joseph E. and Mahoney, Michael W.},
	month = mar,
	year = {2021},
	note = {arXiv: 2008.11364},
	keywords = {Untagged},
}

@article{zhaoMultiattentionalDeepfakeDetection2021,
	title = {Multi-attentional {Deepfake} {Detection}},
	url = {http://arxiv.org/abs/2103.02406},
	abstract = {Face forgery by deepfake is widely spread over the internet and has raised severe societal concerns. Recently, how to detect such forgery contents has become a hot research topic and many deepfake detection methods have been proposed. Most of them model deepfake detection as a vanilla binary classiﬁcation problem, i.e, ﬁrst use a backbone network to extract a global feature and then feed it into a binary classiﬁer (real/fake). But since the difference between the real and fake images in this task is often subtle and local, we argue this vanilla solution is not optimal. In this paper, we instead formulate deepfake detection as a ﬁne-grained classiﬁcation problem and propose a new multi-attentional deepfake detection network. Speciﬁcally, it consists of three key components: 1) multiple spatial attention heads to make the network attend to different local parts; 2) textural feature enhancement block to zoom in the subtle artifacts in shallow features; 3) aggregate the low-level textural feature and high-level semantic features guided by the attention maps. Moreover, to address the learning difﬁculty of this network, we further introduce a new regional independence loss and an attention guided data augmentation strategy. Through extensive experiments on different datasets, we demonstrate the superiority of our method over the vanilla binary classiﬁer counterparts, and achieve state-of-the-art performance. The models will be released recently at https://github.com/yoctta/ multiple-attention.},
	language = {en},
	urldate = {2022-03-22},
	journal = {arXiv:2103.02406 [cs]},
	author = {Zhao, Hanqing and Zhou, Wenbo and Chen, Dongdong and Wei, Tianyi and Zhang, Weiming and Yu, Nenghai},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.02406},
	keywords = {Untagged},
}

@article{zhaoTrTrVisualTracking2021,
	title = {{TrTr}: {Visual} {Tracking} with {Transformer}},
	shorttitle = {{TrTr}},
	url = {http://arxiv.org/abs/2105.03817},
	abstract = {Template-based discriminative trackers are currently the dominant tracking methods due to their robustness and accuracy, and the Siamese-network-based methods that depend on cross-correlation operation between features extracted from template and search images show the state-of-the-art tracking performance. However, general cross-correlation operation can only obtain relationship between local patches in two feature maps. In this paper, we propose a novel tracker network based on a powerful attention mechanism called Transformer encoder-decoder architecture to gain global and rich contextual interdependencies. In this new architecture, features of the template image is processed by a self-attention module in the encoder part to learn strong context information, which is then sent to the decoder part to compute cross-attention with the search image features processed by another self-attention module. In addition, we design the classification and regression heads using the output of Transformer to localize target based on shape-agnostic anchor. We extensively evaluate our tracker TrTr, on VOT2018, VOT2019, OTB-100, UAV, NfS, TrackingNet, and LaSOT benchmarks and our method performs favorably against state-of-the-art algorithms. Training code and pretrained models are available at https://github.com/tongtybj/TrTr.},
	urldate = {2021-08-19},
	journal = {arXiv:2105.03817 [cs]},
	author = {Zhao, Moju and Okada, Kei and Inaba, Masayuki},
	month = may,
	year = {2021},
	note = {arXiv: 2105.03817},
	keywords = {Untagged},
}

@article{zhaoRescuingDeepHashing2021,
	title = {Rescuing {Deep} {Hashing} from {Dead} {Bits} {Problem}},
	url = {http://arxiv.org/abs/2102.00648},
	abstract = {Deep hashing methods have shown great retrieval accuracy and efﬁciency in large-scale image retrieval. How to optimize discrete hash bits is always the focus in deep hashing methods. A common strategy in these methods is to adopt an activation function, e.g. sigmoid(·) or tanh(·), and minimize a quantization loss to approximate discrete values. However, this paradigm may make more and more hash bits stuck into the wrong saturated area of the activation functions and never escaped. We call this problem “Dead Bits Problem (DBP)”. Besides, the existing quantization loss will aggravate DBP as well. In this paper, we propose a simple but effective gradient ampliﬁer which acts before activation functions to alleviate DBP. Moreover, we devise an error-aware quantization loss to further alleviate DBP. It avoids the negative effect of quantization loss based on the similarity between two images. The proposed gradient ampliﬁer and error-aware quantization loss are compatible with a variety of deep hashing methods. Experimental results on three datasets demonstrate the efﬁciency of the proposed gradient ampliﬁer and the error-aware quantization loss.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2102.00648 [cs]},
	author = {Zhao, Shu and Wu, Dayan and Zhou, Yucan and Li, Bo and Wang, Weiping},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.00648},
	keywords = {Untagged},
}

@inproceedings{zhao_sparta_2021,
	address = {Online},
	title = {{SPARTA}: {Efficient} {Open}-{Domain} {Question} {Answering} via {Sparse} {Transformer} {Matching} {Retrieval}},
	shorttitle = {{SPARTA}},
	url = {https://aclanthology.org/2021.naacl-main.47},
	doi = {10.18653/v1/2021.naacl-main.47},
	language = {en},
	urldate = {2023-04-06},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Tiancheng and Lu, Xiaopeng and Lee, Kyusong},
	year = {2021},
	keywords = {Untagged},
	pages = {565--575},
}

@article{zhaoImprovingLongTailedClassification2021,
	title = {Improving {Long}-{Tailed} {Classification} from {Instance} {Level}},
	url = {http://arxiv.org/abs/2104.06094},
	abstract = {Data in the real world tends to exhibit a long-tailed label distribution, which poses great challenges for neural networks in classification. Existing methods tackle this problem mainly from the coarse-grained class level, ignoring the difference among instances, e.g., hard samples vs. easy samples. In this paper, we revisit the long-tailed problem from the instance level and propose two instance-level components to improve long-tailed classification. The first one is an Adaptive Logit Adjustment (ALA) loss, which applies an adaptive adjusting term to the logit. Different from the adjusting terms in existing methods that are class-dependent and only focus on tail classes, we carefully design an instance-specific term and add it on the class-dependent term to make the network pay more attention to not only tailed class, but more importantly hard samples. The second one is a Mixture-of-Experts (MoE) network, which contains a multi-expert module and an instance-aware routing module. The routing module is designed to dynamically integrate the results of multiple experts according to each input instance, and is trained jointly with the experts network in an end-to-end manner.Extensive experiment results show that our method outperforms the state-of-the-art methods by 1\% to 5\% on common long-tailed benchmarks including ImageNet-LT and iNaturalist.},
	urldate = {2021-07-02},
	journal = {arXiv:2104.06094 [cs]},
	author = {Zhao, Yan and Chen, Weicong and Tan, Xu and Huang, Kai and Xu, Jin and Wang, Changhu and Zhu, Jihong},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.06094},
	keywords = {Untagged},
}

@article{zhaoSemisupervisedFederatedLearning2021,
	title = {Semi-supervised {Federated} {Learning} for {Activity} {Recognition}},
	url = {http://arxiv.org/abs/2011.00851},
	abstract = {Training deep learning models on in-home IoT sensory data is commonly used to recognise human activities. Recently, federated learning systems that use edge devices as clients to support local human activity recognition have emerged as a new paradigm to combine local (individual-level) and global (group-level) models. This approach provides better scalability and generalisability and also offers better privacy compared with the traditional centralised analysis and learning models. The assumption behind federated learning, however, relies on supervised learning on clients. This requires a large volume of labelled data, which is difficult to collect in uncontrolled IoT environments such as remote in-home monitoring. In this paper, we propose an activity recognition system that uses semi-supervised federated learning, wherein clients conduct unsupervised learning on autoencoders with unlabelled local data to learn general representations, and a cloud server conducts supervised learning on an activity classifier with labelled data. Our experimental results show that using a long short-term memory autoencoder and a Softmax classifier, the accuracy of our proposed system is higher than that of both centralised systems and semi-supervised federated learning using data augmentation. The accuracy is also comparable to that of supervised federated learning systems. Meanwhile, we demonstrate that our system can reduce the number of needed labels and the size of local models, and has faster local activity recognition speed than supervised federated learning does.},
	urldate = {2022-03-04},
	journal = {arXiv:2011.00851 [cs]},
	author = {Zhao, Yuchen and Liu, Hanyang and Li, Honglin and Barnaghi, Payam and Haddadi, Hamed},
	month = mar,
	year = {2021},
	note = {arXiv: 2011.00851},
	keywords = {Untagged},
}

@article{DBLP:journals/ijon/ZhengCW21,
	title = {Resisting membership inference attacks through knowledge distillation},
	volume = {452},
	url = {https://doi.org/10.1016/j.neucom.2021.04.082},
	doi = {10.1016/j.neucom.2021.04.082},
	journal = {Neurocomputing},
	author = {Zheng, Junxiang and Cao, Yongzhi and Wang, Hanpin},
	year = {2021},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/ijon/ZhengCW21.bib
tex.timestamp: Thu, 17 Jun 2021 18:15:53 +0200},
	keywords = {Untagged},
	pages = {114--126},
}

@inproceedings{zhouInformerEfficientTransformer2021,
	title = {Informer: {Beyond} {Efficient} {Transformer} for {Long} {Sequence} {Time}-{Series} {Forecasting}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	shorttitle = {Informer},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17325},
	language = {en},
	urldate = {2021-07-20},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
	month = may,
	year = {2021},
	note = {Number: 12},
	keywords = {Untagged},
	pages = {11106--11115},
}

@article{zhouUC2UniversalCrosslingual2021,
	title = {{UC2}: {Universal} {Cross}-lingual {Cross}-modal {Vision}-and-{Language} {Pre}-training},
	volume = {abs/2104.00332},
	url = {https://arxiv.org/abs/2104.00332},
	journal = {CoRR},
	author = {Zhou, Mingyang and Zhou, Luowei and Wang, Shuohang and Cheng, Yu and Li, Linjie and Yu, Zhou and Liu, Jingjing},
	year = {2021},
	note = {arXiv: 2104.00332},
	keywords = {Untagged},
}

@article{zhou_isobn_2021,
	title = {{IsoBN}: {Fine}-{Tuning} {BERT} with {Isotropic} {Batch} {Normalization}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{IsoBN}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17718},
	doi = {10.1609/aaai.v35i16.17718},
	abstract = {Fine-tuning pre-trained language models (PTLMs), such as BERT and its better variant RoBERTa, has been a common practice for advancing performance in natural language understanding (NLU) tasks. Recent advance in representation learning shows that isotropic (i.e., unit-variance and uncorrelated) embeddings can significantly improve performance on downstream tasks with faster convergence and better generalization. The isotropy of the pre-trained embeddings in PTLMs, however, is relatively under-explored. In this paper, we analyze the isotropy of the pre-trained [CLS] embeddings of PTLMs with straightforward visualization, and point out two major issues: high variance in their standard deviation, and high correlation between different dimensions. We also propose a new network regularization method, isotropic batch normalization (IsoBN) to address the issues, towards learning more isotropic representations in fine-tuning by dynamically penalizing dominating principal components. This simple yet effective fine-tuning method yields about 1.0 absolute increment on the average of seven NLU tasks.},
	language = {en},
	number = {16},
	urldate = {2023-04-16},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhou, Wenxuan and Lin, Bill Yuchen and Ren, Xiang},
	month = may,
	year = {2021},
	note = {Number: 16},
	keywords = {Untagged},
	pages = {14621--14629},
}

@article{zhuDeepLearningEmbodied2021,
	title = {Deep {Learning} for {Embodied} {Vision} {Navigation}: {A} {Survey}},
	shorttitle = {Deep {Learning} for {Embodied} {Vision} {Navigation}},
	url = {http://arxiv.org/abs/2108.04097},
	abstract = {Embodied visual navigation” problem requires an agent to navigate in a 3D environment mainly rely on its ﬁrst-person observation. This problem has attracted rising attention in recent years due to its wide application in vacuum cleaner, and rescue robot, etc. A navigation agent is supposed to have various intelligent skills, such as visual perceiving, mapping, planning, exploring and reasoning, etc. Building such an agent that observes, thinks, and acts is a key to real intelligence. The remarkable learning ability of deep learning methods empowered the agents to accomplish embodied visual navigation tasks. Despite this, embodied visual navigation is still in its infancy since a lot of advanced skills are required, including perceiving partially observed visual input, exploring unseen areas, memorizing and modeling seen scenarios, understanding cross-modal instructions, and adapting to a new environment, etc. Recently, embodied visual navigation has attracted rising attention of the community, and numerous works has been proposed to learn these skills. This paper attempts to establish an outline of the current works in the ﬁeld of embodied visual navigation by providing a comprehensive literature survey. We summarize the benchmarks and metrics, review different methods, analysis the challenges, and highlight the state-of-the-art methods. Finally, we discuss unresolved challenges in the ﬁeld of embodied visual navigation and give promising directions in pursuing future research.},
	language = {en},
	urldate = {2021-10-29},
	journal = {arXiv:2108.04097 [cs]},
	author = {Zhu, Fengda and Zhu, Yi and Lee, Vincent CS and Liang, Xiaodan and Chang, Xiaojun},
	month = oct,
	year = {2021},
	note = {arXiv: 2108.04097},
	keywords = {Untagged},
}

@article{zhuResidualAttentionSimple2021,
	title = {Residual {Attention}: {A} {Simple} but {Effective} {Method} for {Multi}-{Label} {Recognition}},
	shorttitle = {Residual {Attention}},
	url = {http://arxiv.org/abs/2108.02456},
	abstract = {Multi-label image recognition is a challenging computer vision task of practical use. Progresses in this area, however, are often characterized by complicated methods, heavy computations, and lack of intuitive explanations. To effectively capture different spatial regions occupied by objects from different categories, we propose an embarrassingly simple module, named class-specific residual attention (CSRA). CSRA generates class-specific features for every category by proposing a simple spatial attention score, and then combines it with the class-agnostic average pooling feature. CSRA achieves state-of-the-art results on multilabel recognition, and at the same time is much simpler than them. Furthermore, with only 4 lines of code, CSRA also leads to consistent improvement across many diverse pretrained models and datasets without any extra training. CSRA is both easy to implement and light in computations, which also enjoys intuitive explanations and visualizations.},
	urldate = {2021-08-26},
	journal = {arXiv:2108.02456 [cs]},
	author = {Zhu, Ke and Wu, Jianxin},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.02456},
	keywords = {Untagged},
}

@article{zhuDeformableDETRDeformable2021,
	title = {Deformable {DETR}: {Deformable} {Transformers} for {End}-to-{End} {Object} {Detection}},
	shorttitle = {Deformable {DETR}},
	url = {http://arxiv.org/abs/2010.04159},
	abstract = {DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10× less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https:// github.com/fundamentalvision/Deformable-DETR.},
	language = {en},
	urldate = {2021-03-18},
	journal = {arXiv:2010.04159 [cs]},
	author = {Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
	month = mar,
	year = {2021},
	note = {arXiv: 2010.04159},
	keywords = {Untagged},
}

@inproceedings{zhuDataFreeKnowledgeDistillation2021,
	title = {Data-{Free} {Knowledge} {Distillation} for {Heterogeneous} {Federated} {Learning}},
	url = {https://proceedings.mlr.press/v139/zhu21b.html},
	abstract = {Federated Learning (FL) is a decentralized machine-learning paradigm, in which a global server iteratively averages the model parameters of local users without accessing their data. User heterogeneity has imposed significant challenges to FL, which can incur drifted global models that are slow to converge. Knowledge Distillation has recently emerged to tackle this issue, by refining the server model using aggregated knowledge from heterogeneous users, other than directly averaging their model parameters. This approach, however, depends on a proxy dataset, making it impractical unless such a prerequisite is satisfied. Moreover, the ensemble knowledge is not fully utilized to guide local model learning, which may in turn affect the quality of the aggregated model. Inspired by the prior art, we propose a data-free knowledge distillation approach to address heterogeneous FL, where the server learns a lightweight generator to ensemble user information in a data-free manner, which is then broadcasted to users, regulating local training using the learned knowledge as an inductive bias. Empirical studies powered by theoretical implications show that our approach facilitates FL with better generalization performance using fewer communication rounds, compared with the state-of-the-art.},
	language = {en},
	urldate = {2022-04-28},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhu, Zhuangdi and Hong, Junyuan and Zhou, Jiayu},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {12878--12889},
}

@misc{zhuang_fast_2021,
	title = {Fast {Passage} {Re}-ranking with {Contextualized} {Exact} {Term} {Matching} and {Efficient} {Passage} {Expansion}},
	shorttitle = {{TILDEv2}},
	url = {http://arxiv.org/abs/2108.08513},
	abstract = {BERT-based information retrieval models are expensive, in both time (query latency) and computational resources (energy, hardware cost), making many of these models impractical especially under resource constraints. The reliance on a query encoder that only performs tokenization and on the pre-processing of passage representations at indexing, has allowed the recently proposed TILDE method to overcome the high query latency issue typical of BERT-based models. This however is at the expense of a lower effectiveness compared to other BERT-based re-rankers and dense retrievers. In addition, the original TILDE method is characterised by indexes with a very high memory footprint, as it expands each passage into the size of the BERT vocabulary.},
	language = {en},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Zhuang, Shengyao and Zuccon, Guido},
	month = sep,
	year = {2021},
	note = {arXiv:2108.08513 [cs]},
	keywords = {Untagged},
}

@inproceedings{zhuang_tilde_2021,
	address = {Virtual Event Canada},
	title = {{TILDE}: {Term} {Independent} {Likelihood} {moDEl} for {Passage} {Re}-ranking},
	isbn = {978-1-4503-8037-9},
	shorttitle = {{TILDE}},
	url = {https://dl.acm.org/doi/10.1145/3404835.3462922},
	doi = {10.1145/3404835.3462922},
	abstract = {Deep language models (deep LMs) are increasingly being used for full text retrieval or within cascade retrieval pipelines as later-stage re-rankers. A problem with using deep LMs is that, at query time, a slow inference step needs to be performed – this hinders the practical adoption of these powerful retrieval models, or limits sensibly how many documents can be considered for re-ranking.},
	language = {en},
	urldate = {2023-04-06},
	booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Zhuang, Shengyao and Zuccon, Guido},
	month = jul,
	year = {2021},
	keywords = {Untagged},
	pages = {1483--1492},
}

@inproceedings{zhugeKaleidoBERTVisionLanguagePreTraining2021,
	title = {Kaleido-{BERT}: {Vision}-{Language} {Pre}-{Training} on {Fashion} {Domain}},
	shorttitle = {Kaleido-{BERT}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Zhuge_Kaleido-BERT_Vision-Language_Pre-Training_on_Fashion_Domain_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-07-06},
	author = {Zhuge, Mingchen and Gao, Dehong and Fan, Deng-Ping and Jin, Linbo and Chen, Ben and Zhou, Haoming and Qiu, Minghui and Shao, Ling},
	year = {2021},
	keywords = {Untagged},
	pages = {12647--12657},
}

@inproceedings{zouBenignOverfittingConstantStepsize2021,
	title = {Benign {Overfitting} of {Constant}-{Stepsize} {SGD} for {Linear} {Regression}},
	url = {http://www.learningtheory.org/colt2021/virtual/poster_1333.html},
	abstract = {There is an increasing realization that algorithmic inductive biases are central in preventing overfitting; empirically, we often see a benign overfitting phenomenon in overparameterized settings for natural learning algorithms, such as stochastic gradient descent (SGD), where little to no explicit regularization has been employed. This work considers this issue in arguably the most basic setting: constant-stepsize SGD (with iterate averaging) for linear regression in the overparameterized regime. Our main result provides a sharp excess risk bound, stated in terms of the full eigenspectrum of the data covariance matrix, that reveals a bias-variance decomposition characterizing when generalization is possible: (i) the variance bound is characterized in terms of an effective dimension and (ii) the bias bound provides a sharp geometric characterization in terms of the location of the initial iterate (and how it aligns with the data covariance matrix). We reflect on a number of notable differences between the algorithmic regularization afforded by (unregularized) SGD in comparison to ordinary least squares (minimum-norm interpolation) and ridge regression.},
	language = {en},
	urldate = {2022-02-22},
	booktitle = {Proceedings of the {Thirty}-fourth {Annual} {Conference} on {Learning} {Theory}},
	author = {Zou, Difan and Wu, Jingfeng and Braverman, Vladimir and Gu, Quanquan and Kakade, Sham},
	month = aug,
	year = {2021},
	keywords = {Untagged},
}

@techreport{EricJangDon2021,
	title = {Eric {Jang}\_ {Don}'t {Mess} with {Backprop}\_ {Doubts} about {Biologically} {Plausible} {Deep} {Learning}.pdf},
	month = feb,
	year = {2021},
	keywords = {Untagged},
}

@techreport{LiangYePPTGongShiJiaPingZhu2021,
	title = {{两页PPT}(公式加评注)},
	month = dec,
	year = {2021},
	keywords = {Untagged},
}

@techreport{TuYouHuaA2021,
	title = {{凸优化A}},
	month = dec,
	year = {2021},
	keywords = {Untagged},
}

@techreport{TuYouHuaB2021,
	title = {{凸优化B}},
	month = dec,
	year = {2021},
	keywords = {Untagged},
}

@techreport{TuYouHuaC2021,
	title = {{凸优化C}},
	month = dec,
	year = {2021},
	keywords = {Untagged},
}

@techreport{TuYouHuaD2021,
	title = {{凸优化D}},
	month = dec,
	year = {2021},
	keywords = {Untagged},
}

@techreport{TuYouHuaE2021a,
	title = {{凸优化E}(1)},
	month = dec,
	year = {2021},
	keywords = {Untagged},
}

@techreport{TuYouHuaE2021,
	title = {{凸优化E}},
	month = dec,
	year = {2021},
	keywords = {Untagged},
}

@misc{bai_constitutional_2022,
	title = {Constitutional {AI}: {Harmlessness} from {AI} {Feedback}},
	shorttitle = {{RLAIF}},
	url = {http://arxiv.org/abs/2212.08073},
	abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
	language = {RLAIF},
	urldate = {2023-04-11},
	publisher = {arXiv},
	author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
	month = dec,
	year = {2022},
	note = {arXiv:2212.08073 [cs]},
	keywords = {Untagged},
}

@inproceedings{bai_transfusion_2022,
	address = {New Orleans, LA, USA},
	title = {{TransFusion}: {Robust} {LiDAR}-{Camera} {Fusion} for {3D} {Object} {Detection} with {Transformers}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{TransFusion}},
	url = {https://ieeexplore.ieee.org/document/9879824/},
	doi = {10.1109/CVPR52688.2022.00116},
	abstract = {LiDAR and camera are two important sensors for 3D object detection in autonomous driving. Despite the increasing popularity of sensor fusion in this field, the robustness against inferior image conditions, e.g., bad illumination and sensor misalignment, is under-explored. Existing fusion methods are easily affected by such conditions, mainly due to a hard association of LiDAR points and image pixels, established by calibration matrices.},
	language = {en},
	urldate = {2022-10-06},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Bai, Xuyang and Hu, Zeyu and Zhu, Xinge and Huang, Qingqiu and Chen, Yilun and Fu, Hangbo and Tai, Chiew-Lan},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {1080--1089},
}

@techreport{JiSuanJiXueYuanBaoGaoZhaiYao2021,
	title = {计算机学院报告摘要},
	month = dec,
	year = {2021},
	keywords = {Untagged},
}

@techreport{TuYouHuaF2021,
	title = {{凸优化F}},
	month = dec,
	year = {2021},
	keywords = {Untagged},
}

@misc{besta_demystifying_2022,
	title = {Demystifying {Graph} {Databases}: {Analysis} and {Taxonomy} of {Data} {Organization}, {System} {Designs}, and {Graph} {Queries}},
	shorttitle = {Demystifying {Graph} {Databases}},
	url = {http://arxiv.org/abs/1910.09017},
	abstract = {Graph processing has become an important part of multiple areas of computer science, such as machine learning, computational sciences, medical applications, social network analysis, and many others. Numerous graphs such as web or social networks may contain up to trillions of edges. Often, these graphs are also dynamic (their structure changes over time) and have domain-specific rich data associated with vertices and edges. Graph database systems such as Neo4j enable storing, processing, and analyzing such large, evolving, and rich datasets. Due to the sheer size of such datasets, combined with the irregular nature of graph processing, these systems face unique design challenges. To facilitate the understanding of this emerging domain, we present the first survey and taxonomy of graph database systems. We focus on identifying and analyzing fundamental categories of these systems (e.g., triple stores, tuple stores, native graph database systems, or object-oriented systems), the associated graph models (e.g., RDF or Labeled Property Graph), data organization techniques (e.g., storing graph data in indexing structures or dividing data into records), and different aspects of data distribution and query execution (e.g., support for sharding and ACID). 51 graph database systems are presented and compared, including Neo4j, OrientDB, or Virtuoso. We outline graph database queries and relationships with associated domains (NoSQL stores, graph streaming, and dynamic graph algorithms). Finally, we describe research and engineering challenges to outline the future of graph databases.},
	urldate = {2023-04-11},
	publisher = {arXiv},
	author = {Besta, Maciej and Peter, Emanuel and Gerstenberger, Robert and Fischer, Marc and Podstawski, Michał and Barthels, Claude and Alonso, Gustavo and Hoefler, Torsten},
	month = nov,
	year = {2022},
	note = {arXiv:1910.09017 [cs]},
	keywords = {Untagged},
}

@article{baker_deep_2022,
	title = {Deep learning models fail to capture the configural nature of human shape perception},
	volume = {25},
	issn = {25890042},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2589004222011853},
	doi = {10.1016/j.isci.2022.104913},
	abstract = {A hallmark of human object perception is sensitivity to the holistic conﬁguration of the local shape features of an object. Deep convolutional neural networks (DCNNs) are currently the dominant models for object recognition processing in the visual cortex, but do they capture this conﬁgural sensitivity? To answer this question, we employed a dataset of animal silhouettes and created a variant of this dataset that disrupts the conﬁguration of each object while preserving local features. While human performance was impacted by this manipulation, DCNN performance was not, indicating insensitivity to object conﬁguration. Modiﬁcations to training and architecture to make networks more brain-like did not lead to conﬁgural processing, and none of the networks were able to accurately predict trial-by-trial human object judgements. We speculate that to match human conﬁgural sensitivity, networks must be trained to solve a broader range of object tasks beyond category recognition.},
	language = {en},
	number = {9},
	urldate = {2022-11-26},
	journal = {iScience},
	author = {Baker, Nicholas and Elder, James H.},
	month = sep,
	year = {2022},
	keywords = {Untagged},
	pages = {104913},
}

@techreport{bellotAccountingUnobservedConfounding2022,
	title = {Accounting for {Unobserved} {Confounding} in {Domain} {Generalization}},
	url = {http://arxiv.org/abs/2007.10653},
	abstract = {This paper investigates the problem of learning robust, generalizable prediction models from a combination of multiple datasets and qualitative assumptions about the underlying data-generating model. Part of the challenge of learning robust models lies in the influence of unobserved confounders that void many of the invariances and principles of minimum error presently used for this problem. Our approach is to define a different invariance property of causal solutions in the presence of unobserved confounders which, through a relaxation of this invariance, can be connected with an explicit distributionally robust optimization problem over a set of affine combination of data distributions. Concretely, our objective takes the form of a standard loss, plus a regularization term that encourages partial equality of error derivatives with respect to model parameters. We demonstrate the empirical performance of our approach on healthcare data from different modalities, including image, speech and tabular data.},
	number = {arXiv:2007.10653},
	urldate = {2022-05-15},
	institution = {arXiv},
	author = {Bellot, Alexis and van der Schaar, Mihaela},
	month = feb,
	year = {2022},
	doi = {10.48550/arXiv.2007.10653},
	note = {arXiv:2007.10653 [cs, stat]
type: article},
	keywords = {Untagged},
}

@article{blumBoostingBarelyRobust2022,
	title = {Boosting {Barely} {Robust} {Learners}: {A} {New} {Perspective} on {Adversarial} {Robustness}},
	shorttitle = {Boosting {Barely} {Robust} {Learners}},
	url = {http://arxiv.org/abs/2202.05920},
	abstract = {We present an oracle-efficient algorithm for boosting the adversarial robustness of barely robust learners. Barely robust learning algorithms learn predictors that are adversarially robust only on a small fraction \${\textbackslash}beta {\textbackslash}ll 1\$ of the data distribution. Our proposed notion of barely robust learning requires robustness with respect to a "larger" perturbation set; which we show is necessary for strongly robust learning, and that weaker relaxations are not sufficient for strongly robust learning. Our results reveal a qualitative and quantitative equivalence between two seemingly unrelated problems: strongly robust learning and barely robust learning.},
	urldate = {2022-03-04},
	journal = {arXiv:2202.05920 [cs, stat]},
	author = {Blum, Avrim and Montasser, Omar and Shakhnarovich, Greg and Zhang, Hongyang},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.05920},
	keywords = {Untagged},
}

@article{bryant_improved_2022,
	title = {Improved prediction of protein-protein interactions using {AlphaFold2}},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-28865-w},
	doi = {10.1038/s41467-022-28865-w},
	abstract = {Predicting the structure of interacting protein chains is a fundamental step towards understanding protein function. Unfortunately, no computational method can produce accurate structures of protein complexes. AlphaFold2, has shown unprecedented levels of accuracy in modelling single chain protein structures. Here, we apply AlphaFold2 for the prediction of heterodimeric protein complexes. We find that the AlphaFold2 protocol together with optimised multiple sequence alignments, generate models with acceptable quality (DockQ ≥ 0.23) for 63\% of the dimers. From the predicted interfaces we create a simple function to predict the DockQ score which distinguishes acceptable from incorrect models as well as interacting from non-interacting proteins with state-of-art accuracy. We find that, using the predicted DockQ scores, we can identify 51\% of all interacting pairs at 1\% FPR.},
	language = {en},
	number = {1},
	urldate = {2023-04-06},
	journal = {Nature Communications},
	author = {Bryant, Patrick and Pozzati, Gabriele and Elofsson, Arne},
	month = mar,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Untagged},
	pages = {1265},
}

@inproceedings{brendel__decision-based_2022,
	title = {Decision-{Based} {Adversarial} {Attacks}: {Reliable} {Attacks} {Against} {Black}-{Box} {Machine} {Learning} {Models}},
	shorttitle = {Decision-{Based} {Adversarial} {Attacks}},
	url = {https://openreview.net/forum?id=SyZI0GWCZ},
	abstract = {Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox (https://github.com/bethgelab/foolbox).},
	language = {en},
	urldate = {2023-02-24},
	author = {Brendel *, Wieland and Rauber *, Jonas and Bethge, Matthias},
	month = feb,
	year = {2022},
	keywords = {Untagged},
}

@inproceedings{bogolin_cross_2022,
	address = {New Orleans, LA, USA},
	title = {Cross {Modal} {Retrieval} with {Querybank} {Normalisation}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{QBNorm}},
	url = {https://ieeexplore.ieee.org/document/9878857/},
	doi = {10.1109/CVPR52688.2022.00513},
	abstract = {Profiting from large-scale training datasets, advances in neural architecture design and efficient inference, joint embeddings have become the dominant approach for tackling cross-modal retrieval. In this work we first show that, despite their effectiveness, state-of-the-art joint embeddings suffer significantly from the longstanding “hubness problem” in which a small number of gallery embeddings form the nearest neighbours of many queries. Drawing inspiration from the NLP literature, we formulate a simple but effective framework called Querybank Normalisation (QB-NORM) that re-normalises query similarities to account for hubs in the embedding space. QB-NORM improves retrieval performance without requiring retraining. Differently from prior work, we show that QB-NORM fl works effectively without concurrent access to any test set queries. Within the QB-NORM framework, we also propose a novel similarity normalisation method, the Dynamic Inverted Softmax, that is significantly more robust than existing approaches. We showcase QB-NORM across a range of cross modal retrieval models and benchmarks where it consistently enhances strong baselines beyond the state of the art. Code is available at https://vladbogo. github.io/QB-Norm/.},
	language = {en},
	urldate = {2023-02-02},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Bogolin, Simion-Vlad and Croitoru, Ioana and Jin, Hailin and Liu, Yang and Albanie, Samuel},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {5184--5195},
}

@misc{burns_discovering_2022,
	title = {Discovering {Latent} {Knowledge} in {Language} {Models} {Without} {Supervision}},
	url = {http://arxiv.org/abs/2212.03827},
	abstract = {Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can’t detect. We propose circumventing this issue by directly ﬁnding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Speciﬁcally, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by ﬁnding a direction in activation space that satisﬁes logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 questionanswering datasets, it outperforms zero-shot accuracy by 4\% on average. We also ﬁnd that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don’t have access to explicit ground truth labels.},
	language = {en},
	urldate = {2023-06-02},
	publisher = {arXiv},
	author = {Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
	month = dec,
	year = {2022},
	note = {arXiv:2212.03827 [cs]},
	keywords = {Untagged},
}

@misc{cai_x-detr_2022,
	title = {X-{DETR}: {A} {Versatile} {Architecture} for {Instance}-wise {Vision}-{Language} {Tasks}},
	shorttitle = {X-{DETR}},
	url = {http://arxiv.org/abs/2204.05626},
	abstract = {In this paper, we study the challenging instance-wise vision-language tasks, where the free-form language is required to align with the objects instead of the whole image. To address these tasks, we propose X-DETR, whose architecture has three major components: an object detector, a language encoder, and vision-language alignment. The vision and language streams are independent until the end and they are aligned using an efﬁcient dot-product operation. The whole network is trained end-to-end, such that the detector is optimized for the vision-language tasks instead of an off-the-shelf component. To overcome the limited size of paired object-language annotations, we leverage other weak types of supervision to expand the knowledge coverage. This simple yet effective architecture of X-DETR shows good accuracy and fast speeds for multiple instance-wise vision-language tasks, e.g., 16.4 AP on LVIS detection of 1.2K categories at ∼20 frames per second without using any LVIS annotation during training.},
	language = {en},
	urldate = {2023-06-02},
	publisher = {arXiv},
	author = {Cai, Zhaowei and Kwon, Gukyeong and Ravichandran, Avinash and Bas, Erhan and Tu, Zhuowen and Bhotika, Rahul and Soatto, Stefano},
	month = apr,
	year = {2022},
	note = {arXiv:2204.05626 [cs]},
	keywords = {Untagged},
}

@article{chaturvedi_ignorance_2022,
	title = {Ignorance is {Bliss}: {Exploring} {Defenses} {Against} {Invariance}-{Based} {Attacks} on {Neural} {Machine} {Translation} {Systems}},
	volume = {3},
	issn = {2691-4581},
	shorttitle = {Ignorance is {Bliss}},
	doi = {10.1109/TAI.2021.3123931},
	abstract = {This article addresses an invariance-based attack on the transformer, a state-of-the-art neural machine translation (NMT) system. Such attacks make multiple changes to the source sentence with the goal of keeping the predicted translation unchanged. Since the gold translation is not available for the adversarial sentences, tackling invariance-based attacks is a challenging task. We propose two contrasting defense strategies for the same, learn to deal and learn to ignore. In learn to deal, NMT system is trained not to predict the same translation for a clean text and its noisy counterpart, whereas in learn to ignore, NMT system is trained to output a dummy sentence in the target language whenever it encounters a noisy text. The experiments on two language pairs, English–German (en–de) and English–French (en–fr), show that learn to deal strategy reduces the attack success rate from 84.0\% to 62.2\% for en–de and from 84.6\% to 73.8\% for en–fr, whereas learn to ignore strategy reduces the attack success rate from 84.0\% to 27.2\% for en–de and from 84.6\% to 37.0\% for en–fr.},
	number = {4},
	journal = {IEEE Transactions on Artificial Intelligence},
	author = {Chaturvedi, Akshay and Chakrabarty, Abhisek and Utiyama, Masao and Sumita, Eiichiro and Garain, Utpal},
	month = aug,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Artificial Intelligence},
	keywords = {Untagged},
	pages = {518--525},
}

@article{caoBenignOverfittingTwolayer2022,
	title = {Benign {Overfitting} in {Two}-layer {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2202.06526},
	abstract = {Modern neural networks often have great expressive power and can be trained to overﬁt the training data, while still achieving a good test performance. This phenomenon is referred to as “benign overﬁtting”. Recently, there emerges a line of works studying “benign overﬁtting” from the theoretical perspective. However, they are limited to linear models or kernel/random feature models, and there is still a lack of theoretical understanding about when and how benign overﬁtting occurs in neural networks. In this paper, we study the benign overﬁtting phenomenon in training a two-layer convolutional neural network (CNN). We show that when the signal-tonoise ratio satisﬁes a certain condition, a two-layer CNN trained by gradient descent can achieve arbitrarily small training and test loss. On the other hand, when this condition does not hold, overﬁtting becomes harmful and the obtained CNN can only achieve constant level test loss. These together demonstrate a sharp phase transition between benign overﬁtting and harmful overﬁtting, driven by the signal-to-noise ratio. To the best of our knowledge, this is the ﬁrst work that precisely characterizes the conditions under which benign overﬁtting can occur in training convolutional neural networks.},
	language = {en},
	urldate = {2022-02-22},
	journal = {arXiv:2202.06526 [cs, math, stat]},
	author = {Cao, Yuan and Chen, Zixiang and Belkin, Mikhail and Gu, Quanquan},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.06526},
	keywords = {Untagged},
}

@inproceedings{chen_balanced_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Balanced {Adversarial} {Training}: {Balancing} {Tradeoffs} between {Fickleness} and {Obstinacy} in {NLP} {Models}},
	shorttitle = {Balanced {Adversarial} {Training}},
	url = {https://aclanthology.org/2022.emnlp-main.40},
	abstract = {Traditional (fickle) adversarial examples involve finding a small perturbation that does not change an input's true label but confuses the classifier into outputting a different prediction. Conversely, obstinate adversarial examples occur when an adversary finds a small perturbation that preserves the classifier's prediction but changes the true label of an input.Adversarial training and certified robust training have shown some effectiveness in improving the robustness of machine learnt models to fickle adversarial examples. We show that standard adversarial training methods focused on reducing vulnerability to fickle adversarial examples may make a model more vulnerable to obstinate adversarial examples, with experiments for both natural language inference and paraphrase identification tasks. To counter this phenomenon, we introduce Balanced Adversarial Training, which incorporates contrastive learning to increase robustness against both fickle and obstinate adversarial examples.},
	urldate = {2023-04-11},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Hannah and Ji, Yangfeng and Evans, David},
	month = dec,
	year = {2022},
	keywords = {Untagged},
	pages = {632--647},
}

@inproceedings{chenRelaxLossDefendingMembership2022,
	title = {{RelaxLoss}: {Defending} {Membership} {Inference} {Attacks} without {Losing} {Utility}},
	shorttitle = {{RelaxLoss}},
	url = {https://openreview.net/forum?id=FEDfGWVZYIn},
	abstract = {As a long-term threat to the privacy of training data, membership inference attacks (MIAs) emerge ubiquitously in machine learning models.
Existing works evidence strong connection between the...},
	language = {en},
	urldate = {2022-03-24},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Chen, Dingfan and Yu, Ning and Fritz, Mario},
	year = {2022},
	keywords = {Untagged},
}

@misc{chen_generalist_2022,
	title = {A {Generalist} {Framework} for {Panoptic} {Segmentation} of {Images} and {Videos}},
	url = {http://arxiv.org/abs/2210.06366},
	abstract = {Panoptic segmentation assigns semantic and instance ID labels to every pixel of an image. As permutations of instance IDs are also valid solutions, the task requires learning of high-dimensional one-to-many mapping. As a result, state-of-the-art approaches use customized architectures and task-specific loss functions. We formulate panoptic segmentation as a discrete data generation problem, without relying on inductive bias of the task. A diffusion model based on analog bits is used to model panoptic masks, with a simple, generic architecture and loss function. By simply adding past predictions as a conditioning signal, our method is capable of modeling video (in a streaming setting) and thereby learns to track object instances automatically. With extensive experiments, we demonstrate that our generalist approach can perform competitively to state-of-the-art specialist methods in similar settings.},
	urldate = {2023-02-19},
	publisher = {arXiv},
	author = {Chen, Ting and Li, Lala and Saxena, Saurabh and Hinton, Geoffrey and Fleet, David J.},
	month = dec,
	year = {2022},
	note = {arXiv:2210.06366 [cs]},
	keywords = {Untagged},
}

@misc{chen_expose_2022,
	title = {Expose {Backdoors} on the {Way}: {A} {Feature}-{Based} {Efficient} {Defense} against {Textual} {Backdoor} {Attacks}},
	shorttitle = {Expose {Backdoors} on the {Way}},
	url = {http://arxiv.org/abs/2210.07907},
	doi = {10.48550/arXiv.2210.07907},
	abstract = {Natural language processing (NLP) models are known to be vulnerable to backdoor attacks, which poses a newly arisen threat to NLP models. Prior online backdoor defense methods for NLP models only focus on the anomalies at either the input or output level, still suffering from fragility to adaptive attacks and high computational cost. In this work, we take the first step to investigate the unconcealment of textual poisoned samples at the intermediate-feature level and propose a feature-based efficient online defense method. Through extensive experiments on existing attacking methods, we find that the poisoned samples are far away from clean samples in the intermediate feature space of a poisoned NLP model. Motivated by this observation, we devise a distance-based anomaly score (DAN) to distinguish poisoned samples from clean samples at the feature level. Experiments on sentiment analysis and offense detection tasks demonstrate the superiority of DAN, as it substantially surpasses existing online defense methods in terms of defending performance and enjoys lower inference costs. Moreover, we show that DAN is also resistant to adaptive attacks based on feature-level regularization. Our code is available at https://github.com/lancopku/DAN.},
	urldate = {2022-11-07},
	publisher = {arXiv},
	author = {Chen, Sishuo and Yang, Wenkai and Zhang, Zhiyuan and Bi, Xiaohan and Sun, Xu},
	month = oct,
	year = {2022},
	note = {arXiv:2210.07907 [cs]},
	keywords = {Untagged},
}

@inproceedings{chen_learning_2022,
	address = {New Orleans, LA, USA},
	title = {Learning {Multiple} {Adverse} {Weather} {Removal} via {Two}-stage {Knowledge} {Learning} and {Multi}-contrastive {Regularization}: {Toward} a {Unified} {Model}},
	isbn = {978-1-66546-946-3},
	shorttitle = {Learning {Multiple} {Adverse} {Weather} {Removal} via {Two}-stage {Knowledge} {Learning} and {Multi}-contrastive {Regularization}},
	url = {https://ieeexplore.ieee.org/document/9879902/},
	doi = {10.1109/CVPR52688.2022.01713},
	abstract = {In this paper, an ill-posed problem of multiple adverse weather removal is investigated. Our goal is to train a model with a ’unified’ architecture and only one set of pretrained weights that can tackle multiple types of adverse weathers such as haze, snow, and rain simultaneously. To this end, a two-stage knowledge learning mechanism including knowledge collation (KC) and knowledge examination (KE) based on a multi-teacher and student architecture is proposed. At the KC, the student network aims to learn the comprehensive bad weather removal problem from multiple well-trained teacher networks where each of them is specialized in a specific bad weather removal problem. To accomplish this process, a novel collaborative knowledge transfer is proposed. At the KE, the student model is trained without the teacher networks and examined by challenging pixel loss derived by the ground truth. Moreover, to improve the performance of our training framework, a novel loss function called multi-contrastive knowledge regularization (MCR) loss is proposed. Experiments on several datasets show that our student model can achieve promising results on different bad weather removal tasks simultaneously. The code is available in our project page.},
	language = {en},
	urldate = {2022-10-27},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chen, Wei-Ting and Huang, Zhi-Kai and Tsai, Cheng-Che and Yang, Hao-Hsiang and Ding, Jian-Jiun and Kuo, Sy-Yen},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {17632--17641},
}

@article{chenProvableRegretBounds2022,
	title = {Provable {Regret} {Bounds} for {Deep} {Online} {Learning} and {Control}},
	url = {http://arxiv.org/abs/2110.07807},
	abstract = {The theory of deep learning focuses almost exclusively on supervised learning, non-convex optimization using stochastic gradient descent, and overparametrized neural networks. It is common belief that the optimizer dynamics, network architecture, initialization procedure, and other factors tie together and are all components of its success. This presents theoretical challenges for analyzing state-based and/or online deep learning.},
	language = {en},
	urldate = {2022-02-24},
	journal = {arXiv:2110.07807 [cs]},
	author = {Chen, Xinyi and Minasyan, Edgar and Lee, Jason D. and Hazan, Elad},
	month = feb,
	year = {2022},
	note = {arXiv: 2110.07807},
	keywords = {Untagged},
}

@misc{deitke_objaverse_2022,
	title = {Objaverse: {A} {Universe} of {Annotated} {3D} {Objects}},
	shorttitle = {Objaverse},
	url = {http://arxiv.org/abs/2212.08051},
	abstract = {Massive data corpora like WebText, Wikipedia, Conceptual Captions, WebImageText, and LAION have propelled recent dramatic progress in AI. Large neural models trained on such datasets produce impressive results and top many of today's benchmarks. A notable omission within this family of large-scale datasets is 3D data. Despite considerable interest and potential applications in 3D vision, datasets of high-fidelity 3D models continue to be mid-sized with limited diversity of object categories. Addressing this gap, we present Objaverse 1.0, a large dataset of objects with 800K+ (and growing) 3D models with descriptive captions, tags, and animations. Objaverse improves upon present day 3D repositories in terms of scale, number of categories, and in the visual diversity of instances within a category. We demonstrate the large potential of Objaverse via four diverse applications: training generative 3D models, improving tail category segmentation on the LVIS benchmark, training open-vocabulary object-navigation models for Embodied AI, and creating a new benchmark for robustness analysis of vision models. Objaverse can open new directions for research and enable new applications across the field of AI.},
	urldate = {2023-06-02},
	publisher = {arXiv},
	author = {Deitke, Matt and Schwenk, Dustin and Salvador, Jordi and Weihs, Luca and Michel, Oscar and VanderBilt, Eli and Schmidt, Ludwig and Ehsani, Kiana and Kembhavi, Aniruddha and Farhadi, Ali},
	month = dec,
	year = {2022},
	note = {arXiv:2212.08051 [cs]},
	keywords = {Untagged},
}

@inproceedings{ding_isotropy_2022,
	address = {Dublin, Ireland},
	title = {On {Isotropy} {Calibration} of {Transformer} {Models}},
	url = {https://aclanthology.org/2022.insights-1.1},
	doi = {10.18653/v1/2022.insights-1.1},
	abstract = {Different studies of the embedding space of transformer models suggest that the distribution of contextual representations is highly anisotropic - the embeddings are distributed in a narrow cone. Meanwhile, static word representations (e.g., Word2Vec or GloVe) have been shown to benefit from isotropic spaces. Therefore, previous work has developed methods to calibrate the embedding space of transformers in order to ensure isotropy. However, a recent study (Cai et al. 2021) shows that the embedding space of transformers is locally isotropic, which suggests that these models are already capable of exploiting the expressive capacity of their embedding space. In this work, we conduct an empirical evaluation of state-of-the-art methods for isotropy calibration on transformers and find that they do not provide consistent improvements across models and tasks. These results support the thesis that, given the local isotropy, transformers do not benefit from additional isotropy calibration.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the {Third} {Workshop} on {Insights} from {Negative} {Results} in {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Ding, Yue and Martinkus, Karolis and Pascual, Damian and Clematide, Simon and Wattenhofer, Roger},
	month = may,
	year = {2022},
	keywords = {Untagged},
	pages = {1--9},
}

@inproceedings{avidan_eccv_2022,
	address = {Cham},
	title = {{ECCV} {Caption}: {Correcting} {False} {Negatives} by {Collecting} {Machine}-and-{Human}-verified {Image}-{Caption} {Associations} for {MS}-{COCO}},
	volume = {13668},
	isbn = {978-3-031-20073-1 978-3-031-20074-8},
	shorttitle = {{ECCV} {Caption}},
	url = {https://link.springer.com/10.1007/978-3-031-20074-8_1},
	language = {en},
	urldate = {2023-04-16},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk and Oh, Seong Joon},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-20074-8_1},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {1--19},
}

@inproceedings{chen_iterative_2022,
	title = {Iterative {Feature} {Matching}: {Toward} {Provable} {Domain} {Generalization} with {Logarithmic} {Environments}},
	shorttitle = {Iterative {Feature} {Matching}},
	url = {https://openreview.net/forum?id=CF1ThuQ8vpG},
	abstract = {Domain generalization aims at performing well on unseen test environments with data from a limited number of training environments. Despite a proliferation of proposed algorithms for this task, assessing their performance both theoretically and empirically is still very challenging. Distributional matching algorithms such as (Conditional) Domain Adversarial Networks [Ganin et al., 2016, Long et al., 2018] are popular and enjoy empirical success, but they lack formal guarantees. Other approaches such as Invariant Risk Minimization (IRM) require a prohibitively large number of training environments---linear in the dimension of the spurious feature space \$d\_s\$---even on simple data models like the one proposed by [Rosenfeld et al., 2021]. Under a variant of this model, we show that ERM and IRM can fail to find the optimal invariant predictor with \$o(d\_s)\$ environments. We then present an iterative feature matching algorithm that is guaranteed with high probability to find the optimal invariant predictor after seeing only \$O({\textbackslash}log d\_s)\$ environments. Our results provide the first theoretical justification for distribution-matching algorithms widely used in practice under a concrete nontrivial data model.},
	language = {en},
	urldate = {2022-11-14},
	author = {Chen, Yining and Rosenfeld, Elan and Sellke, Mark and Ma, Tengyu and Risteski, Andrej},
	month = oct,
	year = {2022},
	keywords = {Untagged},
}

@misc{dong_peco_2022,
	title = {{PeCo}: {Perceptual} {Codebook} for {BERT} {Pre}-training of {Vision} {Transformers}},
	shorttitle = {{PeCo}},
	url = {http://arxiv.org/abs/2111.12710},
	abstract = {This paper explores a better codebook for BERT pre-training of vision transformers. The recent work BEiT successfully transfers BERT pre-training from NLP to the vision field. It directly adopts one simple discrete VAE as the visual tokenizer, but has not considered the semantic level of the resulting visual tokens. By contrast, the discrete tokens in NLP field are naturally highly semantic. This difference motivates us to learn a perceptual codebook. And we surprisingly find one simple yet effective idea: enforcing perceptual similarity during the dVAE training. We demonstrate that the visual tokens generated by the proposed perceptual codebook do exhibit better semantic meanings, and subsequently help pre-training achieve superior transfer performance in various downstream tasks. For example, we achieve 84.5\% Top-1 accuracy on ImageNet-1K with ViT-B backbone, outperforming the competitive method BEiT by +1.3 with the same pre-training epochs. It can also improve the performance of object detection and segmentation tasks on COCO val by +1.3 box AP and +1.0 mask AP, semantic segmentation on ADE20k by +1.0 mIoU. Equipped with a larger backbone ViT-H, we achieve the state-of-the-art performance (88.3\% Top-1 accuracy) among the methods using only ImageNet-1K data. The code and models will be available at https://github.com/microsoft/PeCo.},
	urldate = {2022-11-19},
	publisher = {arXiv},
	author = {Dong, Xiaoyi and Bao, Jianmin and Zhang, Ting and Chen, Dongdong and Zhang, Weiming and Yuan, Lu and Chen, Dong and Wen, Fang and Yu, Nenghai},
	month = jan,
	year = {2022},
	note = {arXiv:2111.12710 [cs]},
	keywords = {Untagged},
}

@inproceedings{dong_partially_2022,
	address = {Lisboa Portugal},
	title = {Partially {Relevant} {Video} {Retrieval}},
	isbn = {978-1-4503-9203-7},
	url = {https://dl.acm.org/doi/10.1145/3503161.3547976},
	doi = {10.1145/3503161.3547976},
	language = {en},
	urldate = {2022-10-20},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Dong, Jianfeng and Chen, Xianke and Zhang, Minsong and Yang, Xun and Chen, Shujie and Li, Xirong and Wang, Xun},
	month = oct,
	year = {2022},
	keywords = {Untagged},
	pages = {246--257},
}

@inproceedings{dou_empirical_2022,
	address = {New Orleans, LA, USA},
	title = {An {Empirical} {Study} of {Training} {End}-to-{End} {Vision}-and-{Language} {Transformers}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9879595/},
	doi = {10.1109/CVPR52688.2022.01763},
	abstract = {Vision-and-language (VL) pre-training has proven to be highly effective on various VL downstream tasks. While recent work has shown that fully transformer-based VL models can be more efficient than previous region-featurebased methods, their performance on downstream tasks often degrades significantly. In this paper, we present METER, a Multimodal End-to-end TransformER framework, through which we investigate how to design and pre-train a fully transformer-based VL model in an endto-end manner. Specifically, we dissect the model designs along multiple dimensions: vision encoders (e.g., CLIPViT, Swin transformer), text encoders (e.g., RoBERTa, DeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention), architectural design (e.g., encoder-only vs. encoder-decoder), and pre-training objectives (e.g., masked image modeling). We conduct comprehensive experiments and provide insights on how to train a performant VL transformer. METER achieves an accuracy of 77.64\% on the VQAv2 test-std set using only 4M images for pre-training, surpassing the state-of-the-art region-featurebased model by 1.04\%, and outperforming the previous best fully transformer-based model by 1.6\%. Notably, when further scaled up, our best VQA model achieves an accuracy of 80.54\%. Code and pre-trained models are released at https://github.com/zdou0830/METER.},
	language = {en},
	urldate = {2022-10-05},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Dou, Zi-Yi and Xu, Yichong and Gan, Zhe and Wang, Jianfeng and Wang, Shuohang and Wang, Lijuan and Zhu, Chenguang and Zhang, Pengchuan and Yuan, Lu and Peng, Nanyun and Liu, Zicheng and Zeng, Michael},
	year = {2022},
	keywords = {Untagged},
	pages = {18145--18155},
}

@misc{drews_deepfusion_2022,
	title = {{DeepFusion}: {A} {Robust} and {Modular} {3D} {Object} {Detector} for {Lidars}, {Cameras} and {Radars}},
	shorttitle = {{DeepFusion}},
	url = {http://arxiv.org/abs/2209.12729},
	abstract = {We propose DeepFusion, a modular multi-modal architecture to fuse lidars, cameras and radars in different combinations for 3D object detection. Specialized feature extractors take advantage of each modality and can be exchanged easily, making the approach simple and flexible. Extracted features are transformed into bird's-eye-view as a common representation for fusion. Spatial and semantic alignment is performed prior to fusing modalities in the feature space. Finally, a detection head exploits rich multi-modal features for improved 3D detection performance. Experimental results for lidar-camera, lidar-camera-radar and camera-radar fusion show the flexibility and effectiveness of our fusion approach. In the process, we study the largely unexplored task of faraway car detection up to 225 meters, showing the benefits of our lidar-camera fusion. Furthermore, we investigate the required density of lidar points for 3D object detection and illustrate implications at the example of robustness against adverse weather conditions. Moreover, ablation studies on our camera-radar fusion highlight the importance of accurate depth estimation.},
	urldate = {2022-12-05},
	publisher = {arXiv},
	author = {Drews, Florian and Feng, Di and Faion, Florian and Rosenbaum, Lars and Ulrich, Michael and Gläser, Claudius},
	month = sep,
	year = {2022},
	note = {arXiv:2209.12729 [cs]},
	keywords = {Untagged},
}

@inproceedings{duan_multi-modal_2022,
	address = {New Orleans, LA, USA},
	title = {Multi-modal {Alignment} using {Representation} {Codebook}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9878982/},
	doi = {10.1109/CVPR52688.2022.01520},
	abstract = {Aligning signals from different modalities is an important step in vision-language representation learning as it affects the performance of later stages such as cross-modality fusion. Since image and text typically reside in different regions of the feature space, directly aligning them at instance level is challenging especially when features are still evolving during training. In this paper, we propose to align at a higher and more stable level using cluster representation. Specifically, we treat image and text as two “views” of the same entity, and encode them into a joint vision-language coding space spanned by a dictionary of cluster centers (codebook). We contrast positive and negative samples via their cluster assignments while simultaneously optimizing the cluster centers. To further smooth out the learning process, we adopt a teacher-student distillation paradigm, where the momentum teacher of one view guides the student learning of the other. We evaluated our approach on common vision language benchmarks and obtain new SoTA on zero-shot cross modality retrieval while being competitive on various other transfer tasks.},
	language = {en},
	urldate = {2022-10-05},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Duan, Jiali and Chen, Liqun and Tran, Son and Yang, Jinyu and Xu, Yi and Zeng, Belinda and Chilimbi, Trishul},
	year = {2022},
	keywords = {Untagged},
	pages = {15630--15639},
}

@article{freiBenignOverfittingLinearity2022,
	title = {Benign {Overfitting} without {Linearity}: {Neural} {Network} {Classifiers} {Trained} by {Gradient} {Descent} for {Noisy} {Linear} {Data}},
	shorttitle = {Benign {Overfitting} without {Linearity}},
	url = {https://arxiv.org/abs/2202.05928v1},
	abstract = {Benign overfitting, the phenomenon where interpolating models generalize well in the presence of noisy data, was first observed in neural network models trained with gradient descent. To better understand this empirical observation, we consider the generalization error of two-layer neural networks trained to interpolation by gradient descent on the logistic loss following random initialization. We assume the data comes from well-separated class-conditional log-concave distributions and allow for a constant fraction of the training labels to be corrupted by an adversary. We show that in this setting, neural networks exhibit benign overfitting: they can be driven to zero training error, perfectly fitting any noisy training labels, and simultaneously achieve test error close to the Bayes-optimal error. In contrast to previous work on benign overfitting that require linear or kernel-based predictors, our analysis holds in a setting where both the model and learning dynamics are fundamentally nonlinear.},
	language = {en},
	urldate = {2022-02-22},
	author = {Frei, Spencer and Chatterji, Niladri S. and Bartlett, Peter L.},
	month = feb,
	year = {2022},
	keywords = {Untagged},
}

@inproceedings{gao_kernel-whitening_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Kernel-{Whitening}: {Overcome} {Dataset} {Bias} with {Isotropic} {Sentence} {Embedding}},
	shorttitle = {Kernel-{Whitening}},
	url = {https://aclanthology.org/2022.emnlp-main.275},
	abstract = {Dataset bias has attracted increasing attention recently for its detrimental effect on the generalization ability of fine-tuned models. The current mainstream solution is designing an additional shallow model to pre-identify biased instances. However, such two-stage methods scale up the computational complexity of training process and obstruct valid feature information while mitigating bias.To address this issue, we utilize the representation normalization method which aims at disentangling the correlations between features of encoded sentences. We find it also promising in eliminating the bias problem by providing isotropic data distribution. We further propose Kernel-Whitening, a Nystrom kernel approximation method to achieve more thorough debiasing on nonlinear spurious correlations. Our framework is end-to-end with similar time consumption to fine-tuning. Experiments show that Kernel-Whitening significantly improves the performance of BERT on out-of-distribution datasets while maintaining in-distribution accuracy.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Gao, SongYang and Dou, Shihan and Zhang, Qi and Huang, Xuanjing},
	month = dec,
	year = {2022},
	keywords = {Untagged},
	pages = {4112--4122},
}

@misc{gao_simcse_2022,
	title = {{SimCSE}: {Simple} {Contrastive} {Learning} of {Sentence} {Embeddings}},
	shorttitle = {{SimCSE}},
	url = {http://arxiv.org/abs/2104.08821},
	abstract = {This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using "entailment" pairs as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3\% and 81.6\% Spearman's correlation respectively, a 4.2\% and 2.2\% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.},
	urldate = {2023-04-16},
	publisher = {arXiv},
	author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
	month = may,
	year = {2022},
	note = {arXiv:2104.08821 [cs]},
	keywords = {Untagged},
}

@misc{gao_clip2tv_2022,
	title = {{CLIP2TV}: {Align}, {Match} and {Distill} for {Video}-{Text} {Retrieval}},
	shorttitle = {{CLIP2TV}},
	url = {http://arxiv.org/abs/2111.05610},
	doi = {10.48550/arXiv.2111.05610},
	abstract = {Modern video-text retrieval frameworks basically consist of three parts: video encoder, text encoder and the similarity head. With the success on both visual and textual representation learning, transformer based encoders and fusion methods have also been adopted in the field of video-text retrieval. In this report, we present CLIP2TV, aiming at exploring where the critical elements lie in transformer based methods. To achieve this, We first revisit some recent works on multi-modal learning, then introduce some techniques into video-text retrieval, finally evaluate them through extensive experiments in different configurations. Notably, CLIP2TV achieves 52.9@R1 on MSR-VTT dataset, outperforming the previous SOTA result by 4.1\%.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Gao, Zijian and Liu, Jingyu and Sun, Weiqi and Chen, Sheng and Chang, Dedan and Zhao, Lili},
	month = jul,
	year = {2022},
	note = {arXiv:2111.05610 [cs]},
	keywords = {Untagged},
}

@inproceedings{ge_bridging_2022,
	address = {New Orleans, LA, USA},
	title = {Bridging {Video}-text {Retrieval} with {Multiple} {Choice} {Questions}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9880071/},
	doi = {10.1109/CVPR52688.2022.01569},
	language = {en},
	urldate = {2023-05-10},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Ge, Yuying and Ge, Yixiao and Liu, Xihui and Li, Dian and Shan, Ying and Qie, Xiaohu and Luo, Ping},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {16146--16155},
}

@inproceedings{geirhos_imagenet-trained_2022,
	title = {{ImageNet}-trained {CNNs} are biased towards texture; increasing shape bias improves accuracy and robustness},
	url = {https://openreview.net/forum?id=Bygh9j09KX},
	abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.},
	language = {en},
	urldate = {2022-11-27},
	author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
	month = feb,
	year = {2022},
	keywords = {Untagged},
}

@misc{gasperini_holistic_2022,
	title = {Holistic {Segmentation}},
	url = {http://arxiv.org/abs/2209.05407},
	doi = {10.48550/arXiv.2209.05407},
	abstract = {As panoptic segmentation provides a prediction for every pixel in input, non-standard and unseen objects systematically lead to wrong outputs. However, in safety-critical settings, robustness against out-of-distribution samples and corner cases is crucial to avoid dangerous behaviors, such as ignoring an animal or a lost cargo on the road. Since driving datasets cannot contain enough data points to properly sample the long tail of the underlying distribution, a method must deal with unknown and unseen scenarios to be deployed safely. Previous methods targeted part of this issue, by re-identifying already seen unlabeled objects. In this work, we broaden the scope proposing holistic segmentation: a task to identify and separate unseen unknown objects into instances, without learning from unknowns, while performing panoptic segmentation of known classes. We tackle this new problem with U3HS, which first finds unknowns as highly uncertain regions, then clusters the corresponding instance-aware embeddings into individual objects. By doing so, for the first time in panoptic segmentation with unknown objects, our U3HS is not trained with unknown data, thus leaving the settings unconstrained with respect to the type of objects and allowing for a holistic scene understanding. Extensive experiments and comparisons on two public datasets, namely Cityscapes and Lost\&Found as a transfer, demonstrate the effectiveness of U3HS in the challenging task of holistic segmentation, with competitive closed-set panoptic segmentation performance.},
	urldate = {2022-11-09},
	publisher = {arXiv},
	author = {Gasperini, Stefano and Winkelmann, Frithjof and Marcos-Ramiro, Alvaro and Schmidt, Micheal and Navab, Nassir and Busam, Benjamin and Tombari, Federico},
	month = sep,
	year = {2022},
	note = {arXiv:2209.05407 [cs]},
	keywords = {Untagged},
}

@misc{ghiasi_scaling_2022,
	title = {Scaling {Open}-{Vocabulary} {Image} {Segmentation} with {Image}-{Level} {Labels}},
	url = {http://arxiv.org/abs/2112.12143},
	abstract = {We design an open-vocabulary image segmentation model to organize an image into meaningful regions indicated by arbitrary texts. Recent works (CLIP and ALIGN), despite attaining impressive open-vocabulary classification accuracy with image-level caption labels, are unable to segment visual concepts with pixels. We argue that these models miss an important step of visual grouping, which organizes pixels into groups before learning visual-semantic alignments. We propose OpenSeg to address the above issue while still making use of scalable image-level supervision of captions. First, it learns to propose segmentation masks for possible organizations. Then it learns visual-semantic alignments by aligning each word in a caption to one or a few predicted masks. We find the mask representations are the key to support learning image segmentation from captions, making it possible to scale up the dataset and vocabulary sizes. OpenSeg significantly outperforms the recent open-vocabulary method of LSeg by +19.9 mIoU on PASCAL dataset, thanks to its scalability.},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Ghiasi, Golnaz and Gu, Xiuye and Cui, Yin and Lin, Tsung-Yi},
	month = jul,
	year = {2022},
	note = {arXiv:2112.12143 [cs]},
	keywords = {Untagged},
}

@inproceedings{gorti_x-pool_2022,
	address = {New Orleans, LA, USA},
	title = {X-{Pool}: {Cross}-{Modal} {Language}-{Video} {Attention} for {Text}-{Video} {Retrieval}},
	isbn = {978-1-66546-946-3},
	shorttitle = {X-{Pool}},
	url = {https://ieeexplore.ieee.org/document/9879391/},
	doi = {10.1109/CVPR52688.2022.00495},
	abstract = {In text-video retrieval, the objective is to learn a crossmodal similarity function between a text and a video that ranks relevant text-video pairs higher than irrelevant pairs. However, videos inherently express a much wider gamut of information than texts. Instead, texts often capture subregions of entire videos and are most semantically similar to certain frames within videos. Therefore, for a given text, a retrieval model should focus on the text’s most semantically similar video sub-regions to make a more relevant comparison. Yet, most existing works aggregate entire videos without directly considering text. Common text-agnostic aggregations schemes include mean-pooling or self-attention over the frames, but these are likely to encode misleading visual information not described in the given text. To address this, we propose a cross-modal attention model called XPool that reasons between a text and the frames of a video. Our core mechanism is a scaled dot product attention for a text to attend to its most semantically similar frames. We then generate an aggregated video representation conditioned on the text’s attention weights over the frames. We evaluate our method on three benchmark datasets of MSRVTT, MSVD and LSMDC, achieving new state-of-the-art results by up to 12\% in relative improvement in Recall@1. Our ﬁndings thereby highlight the importance of joint textvideo reasoning to extract important visual cues according to text. Full code and demo can be found at: layer6ailabs.github.io/xpool/.},
	language = {en},
	urldate = {2022-10-20},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Gorti, Satya Krishna and Vouitsis, Noel and Ma, Junwei and Golestan, Keyvan and Volkovs, Maksims and Garg, Animesh and Yu, Guangwei},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {4996--5005},
}

@inproceedings{goldwasser_planting_2022,
	title = {Planting {Undetectable} {Backdoors} in {Machine} {Learning} {Models}},
	url = {http://arxiv.org/abs/2204.06974},
	doi = {10.48550/arXiv.2204.06974},
	abstract = {Given the computational cost and technical expertise required to train machine learning models, users may delegate the task of learning to a service provider. We show how a malicious learner can plant an undetectable backdoor into a classifier. On the surface, such a backdoored classifier behaves normally, but in reality, the learner maintains a mechanism for changing the classification of any input, with only a slight perturbation. Importantly, without the appropriate "backdoor key", the mechanism is hidden and cannot be detected by any computationally-bounded observer. We demonstrate two frameworks for planting undetectable backdoors, with incomparable guarantees. First, we show how to plant a backdoor in any model, using digital signature schemes. The construction guarantees that given black-box access to the original model and the backdoored version, it is computationally infeasible to find even a single input where they differ. This property implies that the backdoored model has generalization error comparable with the original model. Second, we demonstrate how to insert undetectable backdoors in models trained using the Random Fourier Features (RFF) learning paradigm or in Random ReLU networks. In this construction, undetectability holds against powerful white-box distinguishers: given a complete description of the network and the training data, no efficient distinguisher can guess whether the model is "clean" or contains a backdoor. Our construction of undetectable backdoors also sheds light on the related issue of robustness to adversarial examples. In particular, our construction can produce a classifier that is indistinguishable from an "adversarially robust" classifier, but where every input has an adversarial example! In summary, the existence of undetectable backdoors represent a significant theoretical roadblock to certifying adversarial robustness.},
	urldate = {2022-06-11},
	booktitle = {{arXiv}:2204.06974},
	publisher = {arXiv},
	author = {Goldwasser, Shafi and Kim, Michael P. and Vaikuntanathan, Vinod and Zamir, Or},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06974 [cs]
type: article},
	keywords = {Untagged},
}

@article{higgins_symmetry-based_2022,
	title = {Symmetry-{Based} {Representations} for {Artificial} and {Biological} {General} {Intelligence}},
	volume = {16},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2022.836498/full},
	doi = {10.3389/fncom.2022.836498},
	abstract = {Biological intelligence is remarkable in its ability to produce complex behavior in many diverse situations through data efﬁcient, generalizable, and transferable skill acquisition. It is believed that learning “good” sensory representations is important for enabling this, however there is little agreement as to what a good representation should look like. In this review article we are going to argue that symmetry transformations are a fundamental principle that can guide our search for what makes a good representation. The idea that there exist transformations (symmetries) that affect some aspects of the system but not others, and their relationship to conserved quantities has become central in modern physics, resulting in a more uniﬁed theoretical framework and even ability to predict the existence of new particles. Recently, symmetries have started to gain prominence in machine learning too, resulting in more data efﬁcient and generalizable algorithms that can mimic some of the complex behaviors produced by biological intelligence. Finally, ﬁrst demonstrations of the importance of symmetry transformations for representation learning in the brain are starting to arise in neuroscience. Taken together, the overwhelming positive effect that symmetries bring to these disciplines suggest that they may be an important general framework that determines the structure of the universe, constrains the nature of natural tasks and consequently shapes both biological and artiﬁcial intelligence.},
	language = {en},
	urldate = {2023-06-02},
	journal = {Frontiers in Computational Neuroscience},
	author = {Higgins, Irina and Racanière, Sébastien and Rezende, Danilo},
	month = apr,
	year = {2022},
	keywords = {Untagged},
	pages = {836498},
}

@inproceedings{guo_unixcoder_2022,
	address = {Dublin, Ireland},
	title = {{UniXcoder}: {Unified} {Cross}-{Modal} {Pre}-training for {Code} {Representation}},
	shorttitle = {{UniXcoder}},
	url = {https://aclanthology.org/2022.acl-long.499},
	doi = {10.18653/v1/2022.acl-long.499},
	abstract = {Pre-trained models for programming languages have recently demonstrated great success on code intelligence. To support both code-related understanding and generation tasks, recent works attempt to pre-train unified encoder-decoder models. However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference. In this paper, we present UniXcoder, a unified cross-modal pre-trained model for programming language. The model utilizes mask attention matrices with prefix adapters to control the behavior of the model and leverages cross-modal contents like AST and code comment to enhance code representation. To encode AST that is represented as a tree in parallel, we propose a one-to-one mapping method to transform AST in a sequence structure that retains all structural information from the tree. Furthermore, we propose to utilize multi-modal contents to learn representation of code fragment with contrastive learning, and then align representations among programming languages using a cross-modal generation task. We evaluate UniXcoder on five code-related tasks over nine datasets. To further evaluate the performance of code fragment representation, we also construct a dataset for a new task, called zero-shot code-to-code search. Results show that our model achieves state-of-the-art performance on most tasks and analysis reveals that comment and AST can both enhance UniXcoder.},
	urldate = {2023-05-10},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Guo, Daya and Lu, Shuai and Duan, Nan and Wang, Yanlin and Zhou, Ming and Yin, Jian},
	month = may,
	year = {2022},
	keywords = {Untagged},
	pages = {7212--7225},
}

@article{hongyangzhangBoostingBarelyRobust2022,
	title = {Boosting {Barely} {Robust} {Learners}: {A} {New} {Perspective} on {Adversarial} {Robustness}},
	author = {{Hongyang Zhang}},
	year = {2022},
	keywords = {Untagged},
}

@article{hu_afdetv2_2022,
	title = {{AFDetV2}: {Rethinking} the {Necessity} of the {Second} {Stage} for {Object} {Detection} from {Point} {Clouds}},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{AFDetV2}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/19980},
	doi = {10.1609/aaai.v36i1.19980},
	abstract = {There have been two streams in the 3D detection from point clouds: single-stage methods and two-stage methods. While the former is more computationally efﬁcient, the latter usually provides better detection accuracy. By carefully examining the two-stage approaches, we have found that if appropriately designed, the ﬁrst stage can produce accurate box regression. In this scenario, the second stage mainly rescores the boxes such that the boxes with better localization get selected. From this observation, we have devised a single-stage anchor-free network that can fulﬁll these requirements. This network, named AFDetV2, extends the previous work by incorporating a self-calibrated convolution block in the backbone, a keypoint auxiliary supervision, and an IoU prediction branch in the multi-task head. We take a simple product of the predicted IoU score with the classiﬁcation heatmap to form the ﬁnal classiﬁcation conﬁdence. The enhanced backbone strengthens the box localization capability, and the rescoring approach effectively joins the object presence conﬁdence and the box regression accuracy. As a result, the detection accuracy is drastically boosted in the single-stage. To evaluate our approach, we have conducted extensive experiments on the Waymo Open Dataset and the nuScenes Dataset. We have observed that our AFDetV2 achieves the state-of-theart results on these two datasets, superior to all the prior arts, including both the single-stage and the two-stage 3D detectors. AFDetV2 won the 1st place in the Real-Time 3D Detection of the Waymo Open Dataset Challenge 2021. In addition, a variant of our model AFDetV2-Base was entitled the “Most Efﬁcient Model” by the Challenge Sponsor, showing a superior computational efﬁciency. To demonstrate the generality of this single-stage method, we have also applied it to the ﬁrst stage of the two-stage networks. Without exception, the results show that with the strengthened backbone and the rescoring approach, the second stage reﬁnement is no longer needed.},
	language = {en},
	number = {1},
	urldate = {2023-06-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Hu, Yihan and Ding, Zhuangzhuang and Ge, Runzhou and Shao, Wenxin and Huang, Li and Li, Kun and Liu, Qiang},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {969--979},
}

@article{huMembershipInferenceAttacks2022,
	title = {Membership {Inference} {Attacks} on {Machine} {Learning}: {A} {Survey}},
	shorttitle = {Membership {Inference} {Attacks} on {Machine} {Learning}},
	url = {http://arxiv.org/abs/2103.07853},
	abstract = {Machine learning (ML) models have been widely applied to various applications, including image classification, text generation, audio recognition, and graph data analysis. However, recent studies have shown that ML models are vulnerable to membership inference attacks (MIAs), which aim to infer whether a data record was used to train a target model or not. MIAs on ML models can directly lead to a privacy breach. For example, via identifying the fact that a clinical record that has been used to train a model associated with a certain disease, an attacker can infer that the owner of the clinical record has the disease with a high chance. In recent years, MIAs have been shown to be effective on various ML models, e.g., classification models and generative models. Meanwhile, many defense methods have been proposed to mitigate MIAs. Although MIAs on ML models form a newly emerging and rapidly growing research area, there has been no systematic survey on this topic yet. In this paper, we conduct the first comprehensive survey on membership inference attacks and defenses. We provide the taxonomies for both attacks and defenses, based on their characterizations, and discuss their pros and cons. Based on the limitations and gaps identified in this survey, we point out several promising future research directions to inspire the researchers who wish to follow this area. This survey not only serves as a reference for the research community but also provides a clear description for researchers outside this research domain. To further help the researchers, we have created an online resource repository, which we will keep updated with future relevant work. Interested readers can find the repository at https://github.com/HongshengHu/membership-inference-machine-learning-literature.},
	urldate = {2022-03-17},
	journal = {arXiv:2103.07853 [cs]},
	author = {Hu, Hongsheng and Salcic, Zoran and Sun, Lichao and Dobbie, Gillian and Yu, Philip S. and Zhang, Xuyun},
	month = feb,
	year = {2022},
	note = {arXiv: 2103.07853},
	keywords = {Untagged},
}

@inproceedings{hongyangzhangHowManyData2022,
	title = {How {Many} {Data} {Are} {Needed} for {Robust} {Learning}?},
	author = {{Hongyang Zhang}},
	year = {2022},
	keywords = {Untagged},
}

@misc{jia_visual_2022,
	title = {Visual {Prompt} {Tuning}},
	url = {http://arxiv.org/abs/2203.12119},
	abstract = {The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, ie, full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1\% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
	month = jul,
	year = {2022},
	note = {arXiv:2203.12119 [cs]},
	keywords = {Untagged},
}

@inproceedings{huynh_open-vocabulary_2022,
	address = {New Orleans, LA, USA},
	title = {Open-{Vocabulary} {Instance} {Segmentation} via {Robust} {Cross}-{Modal} {Pseudo}-{Labeling}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9878700/},
	doi = {10.1109/CVPR52688.2022.00689},
	language = {en},
	urldate = {2023-01-30},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Huynh, Dat and Kuen, Jason and Lin, Zhe and Gu, Jiuxiang and Elhamifar, Ehsan},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {7010--7021},
}

@article{koepke_audio_2022,
	title = {Audio {Retrieval} with {Natural} {Language} {Queries}: {A} {Benchmark} {Study}},
	issn = {1941-0077},
	shorttitle = {Audio {Retrieval} with {Natural} {Language} {Queries}},
	doi = {10.1109/TMM.2022.3149712},
	abstract = {The objectives of this work are cross-modal text-audio and audio-text retrieval, in which the goal is to retrieve the audio content from a pool of candidates that best matches a given written description and vice versa. Text-audio retrieval enables users to search large databases through an intuitive interface: they simply issue free-form natural language descriptions of the sound they would like to hear. To study the tasks of text-audio and audio-text retrieval, which have received limited attention in the existing literature, we introduce three challenging new benchmarks. We first construct text-audio and audio-text retrieval benchmarks from the AudioCaps and Clotho audio captioning datasets. Additionally, we introduce the SoundDescs benchmark, which consists of paired audio and natural language descriptions for a diverse collection of sounds that are complementary to those found in AudioCaps and Clotho. We employ these three benchmarks to establish baselines for cross-modal text-audio and audio-text retrieval, where we demonstrate the benefits of pre-training on diverse audio tasks. We hope that our benchmarks will inspire further research into audio retrieval with free-form text queries. Code, audio features for all datasets used, and the SoundDescs dataset are publicly available at https://github.com/akoepke/audio-retrieval-benchmark.},
	journal = {IEEE Transactions on Multimedia},
	author = {Koepke, A. Sophia and Oncescu, Andreea-Maria and Henriques, Joao and Akata, Zeynep and Albanie, Samuel},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Multimedia},
	keywords = {Untagged},
	pages = {1--1},
}

@misc{kim_large-scale_2022,
	title = {Large-{Scale} {Bidirectional} {Training} for {Zero}-{Shot} {Image} {Captioning}},
	shorttitle = {{BITTERS}},
	url = {http://arxiv.org/abs/2211.06774},
	doi = {10.48550/arXiv.2211.06774},
	abstract = {When trained on large-scale datasets, image captioning models can understand the content of images from a general domain but often fail to generate accurate, detailed captions. To improve performance, pretraining-and-finetuning has been a key strategy for image captioning. However, we find that large-scale bidirectional training between image and text enables zero-shot image captioning. In this paper, we introduce Bidirectional Image Text Training in largER Scale, BITTERS, an efficient training and inference framework for zero-shot image captioning. We also propose a new evaluation benchmark which comprises of high quality datasets and an extensive set of metrics to properly evaluate zero-shot captioning accuracy and societal bias. We additionally provide an efficient finetuning approach for keyword extraction. We show that careful selection of large-scale training set and model architecture is the key to achieving zero-shot image captioning.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Kim, Taehoon and Marsden, Mark and Ahn, Pyunghwan and Kim, Sangyun and Lee, Sihaeng and Sala, Alessandra and Kim, Seung Hwan},
	month = nov,
	year = {2022},
	note = {arXiv:2211.06774 [cs]},
	keywords = {Untagged},
}

@article{kim_camera-lidar_2022,
	title = {Camera-{LiDAR} {Fusion} {Method} with {Feature} {Switch} {Layer} for {Object} {Detection} {Networks}},
	volume = {22},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/19/7163},
	doi = {10.3390/s22197163},
	abstract = {Object detection is an important factor in the autonomous driving industry. Object detection for autonomous vehicles requires robust results, because various situations and environments must be considered. A sensor fusion method is used to implement robust object detection. A sensor fusion method using a network should effectively meld two features, otherwise, there is concern that the performance is substantially degraded. To effectively use sensors in autonomous vehicles, data analysis is required. We investigated papers in which the camera and LiDAR data change for effective fusion. We propose a feature switch layer for a sensor fusion network for object detection in cameras and LiDAR. Object detection performance was improved by designing a feature switch layer that can consider its environment during network feature fusion. The feature switch layer extracts and fuses features while considering the environment in which the sensor data changes less than during the learning network. We conducted an evaluation experiment using the Dense Dataset and confirmed that the proposed method improves the object detection performance.},
	language = {en},
	number = {19},
	urldate = {2022-12-05},
	journal = {Sensors},
	author = {Kim, Taek-Lim and Park, Tae-Hyoung},
	month = jan,
	year = {2022},
	note = {Number: 19
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Untagged},
	pages = {7163},
}

@inproceedings{jose_valanarasu_transweather_2022,
	address = {New Orleans, LA, USA},
	title = {{TransWeather}: {Transformer}-based {Restoration} of {Images} {Degraded} by {Adverse} {Weather} {Conditions}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{TransWeather}},
	url = {https://ieeexplore.ieee.org/document/9878823/},
	doi = {10.1109/CVPR52688.2022.00239},
	abstract = {Removing adverse weather conditions like rain, fog, and snow from images is an important problem in many applications. Most methods proposed in the literature have been designed to deal with just removing one type of degradation. Recently, a CNN-based method using neural architecture search (All-in-One) was proposed to remove all the weather conditions at once. However, it has a large number of parameters as it uses multiple encoders to cater to each weather removal task and still has scope for improvement in its performance. In this work, we focus on developing an efﬁcient solution for the all adverse weather removal problem. To this end, we propose TransWeather, a transformer-based end-to-end model with just a single encoder and a decoder that can restore an image degraded by any weather condition. Speciﬁcally, we utilize a novel transformer encoder using intra-patch transformer blocks to enhance attention inside the patches to effectively remove smaller weather degradations. We also introduce a transformer decoder with learnable weather type embeddings to adjust to the weather degradation at hand. TransWeather achieves signiﬁcant improvements across multiple test datasets over both All-in-One network as well as methods ﬁne-tuned for speciﬁc tasks. TransWeather is also validated on real world test images and found to be more effective than previous methods. Implementation code can be found in the supplementary document. Code is available at https://github.com/jeya-maria-jose/TransWeather.},
	language = {en},
	urldate = {2022-10-27},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Jose Valanarasu, Jeya Maria and Yasarla, Rajeev and Patel, Vishal M.},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {2343--2353},
}

@inproceedings{li_language-driven_2022,
	title = {Language-driven {Semantic} {Segmentation}},
	url = {https://openreview.net/forum?id=RriDjddCLN},
	abstract = {We present LSeg, a novel model for language-driven semantic image segmentation. LSeg uses a text encoder to compute embeddings of descriptive input labels (e.g., ``grass'' or ``building'') together with a transformer-based image encoder that computes dense per-pixel embeddings of the input image. The image encoder is trained with a contrastive objective to align pixel embeddings to the text embedding of the corresponding semantic class. The text embeddings provide a flexible label representation in which semantically similar labels map to similar regions in the embedding space (e.g., ``cat'' and ``furry''). This allows LSeg to generalize to previously unseen categories at test time, without retraining or even requiring a single additional training sample. We demonstrate that our approach achieves highly competitive zero-shot performance compared to existing zero- and few-shot semantic segmentation methods, and even matches the accuracy of traditional segmentation algorithms when a fixed label set is provided. Code and demo are available at https://github.com/isl-org/lang-seg.},
	language = {en},
	urldate = {2023-01-30},
	author = {Li, Boyi and Weinberger, Kilian Q. and Belongie, Serge and Koltun, Vladlen and Ranftl, Rene},
	month = feb,
	year = {2022},
	keywords = {Untagged},
}

@inproceedings{lee_fifo_2022,
	address = {New Orleans, LA, USA},
	title = {{FIFO}: {Learning} {Fog}-invariant {Features} for {Foggy} {Scene} {Segmentation}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{FIFO}},
	url = {https://ieeexplore.ieee.org/document/9880253/},
	doi = {10.1109/CVPR52688.2022.01834},
	abstract = {Robust visual recognition under adverse weather conditions is of great importance in real-world applications. In this context, we propose a new method for learning semantic segmentation models robust against fog. Its key idea is to consider the fog condition of an image as its style and close the gap between images with different fog conditions in neural style spaces of a segmentation model. In particular, since the neural style of an image is in general affected by other factors as well as fog, we introduce a fog-pass filter module that learns to extract a fog-relevant factor from the style. Optimizing the fog-pass filter and the segmentation model alternately gradually closes the style gap between different fog conditions and allows to learn fog-invariant features in consequence. Our method substantially outperforms previous work on three real foggy image datasets. Moreover, it improves performance on both foggy and clear weather images, while existing methods often degrade performance on clear scenes.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lee, Sohyun and Son, Taeyoung and Kwak, Suha},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {18889--18899},
}

@misc{li_blip_2022,
	title = {{BLIP}: {Bootstrapping} {Language}-{Image} {Pre}-training for {Unified} {Vision}-{Language} {Understanding} and {Generation}},
	shorttitle = {{BLIP}},
	url = {http://arxiv.org/abs/2201.12086},
	doi = {10.48550/arXiv.2201.12086},
	abstract = {Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7\% in average recall@1), image captioning (+2.8\% in CIDEr), and VQA (+1.6\% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
	month = feb,
	year = {2022},
	note = {arXiv:2201.12086 [cs]},
	keywords = {Untagged},
}

@inproceedings{li_clop_2022,
	address = {Lisboa Portugal},
	title = {{CLOP}: {Video}-and-{Language} {Pre}-{Training} with {Knowledge} {Regularizations}},
	isbn = {978-1-4503-9203-7},
	shorttitle = {{CLOP}},
	url = {https://dl.acm.org/doi/10.1145/3503161.3548346},
	doi = {10.1145/3503161.3548346},
	abstract = {Video-and-language pre-training has shown promising results for learning generalizable representations. Most existing approaches usually model video and text in an implicit manner, without considering explicit structural representations of the multi-modal content. We denote such form of representations as “structural knowledge”, which express rich semantics of multiple granularities. There are related works that propose object-aware approaches to inject similar knowledge as inputs. However, the existing methods usually fail to effectively utilize such knowledge as “regularizations” to shape a superior cross-modal representation space. To this end, we propose a Cross-modaL knOwledge-enhanced Pre-training (CLOP) method with Knowledge Regularizations. There are two key designs of ours: 1) a simple yet effective Structural Knowledge Prediction (SKP) task to pull together the latent representations of similar videos; and 2) a novel Knowledge-guided sampling approach for Contrastive Learning (KCL) to push apart cross-modal hard negative samples. We evaluate our method on four text-video retrieval tasks and one multi-choice QA task. The experiments show clear improvements, outperforming prior works by a substantial margin. Besides, we provide ablations and insights of how our methods affect the latent representation space, demonstrating the value of incorporating knowledge regularizations into video-and-language pre-training.},
	language = {en},
	urldate = {2022-10-20},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Li, Guohao and Yang, Hu and He, Feng and Feng, Zhifan and Lyu, Yajuan and Wu, Hua and Wang, Haifeng},
	month = oct,
	year = {2022},
	keywords = {Untagged},
	pages = {4584--4593},
}

@inproceedings{li_align_2022,
	address = {New Orleans, LA, USA},
	title = {Align and {Prompt}: {Video}-and-{Language} {Pre}-training with {Entity} {Prompts}},
	isbn = {978-1-66546-946-3},
	shorttitle = {Align and {Prompt}},
	url = {https://ieeexplore.ieee.org/document/9878891/},
	doi = {10.1109/CVPR52688.2022.00490},
	abstract = {Video-and-language pre-training has shown promising improvements on various downstream tasks. Most previous methods capture cross-modal interactions with a standard transformer-based multimodal encoder, not fully addressing the misalignment between unimodal video and text features. Besides, learning fine-grained visual-language alignment usually requires off-the-shelf object detectors to provide object information, which is bottlenecked by the detector’s limited vocabulary and expensive computation cost.},
	language = {en},
	urldate = {2022-10-19},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Dongxu and Li, Junnan and Li, Hongdong and Niebles, Juan Carlos and Hoi, Steven C.H.},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {4943--4953},
}

@inproceedings{li_grounded_2022,
	address = {New Orleans, LA, USA},
	title = {Grounded {Language}-{Image} {Pre}-training},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9879567/},
	doi = {10.1109/CVPR52688.2022.01069},
	abstract = {This paper presents a grounded language-image pretraining (GLIP) model for learning object-level, languageaware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representations semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines.1 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code will be released at https://github.com/microsoft/GLIP.},
	language = {en},
	urldate = {2023-01-30},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and Chang, Kai-Wei and Gao, Jianfeng},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {10955--10965},
}

@inproceedings{avidan_homogeneous_2022,
	address = {Cham},
	title = {Homogeneous {Multi}-modal {Feature} {Fusion} and {Interaction} for {3D} {Object} {Detection}},
	volume = {13698},
	isbn = {978-3-031-19838-0 978-3-031-19839-7},
	url = {https://link.springer.com/10.1007/978-3-031-19839-7_40},
	doi = {10.1007/978-3-031-19839-7_40},
	abstract = {Multi-modal 3D object detection has been an active research topic in autonomous driving. Nevertheless, it is non-trivial to explore the cross-modal feature fusion between sparse 3D points and dense 2D pixels. Recent approaches either fuse the image features with the point cloud features that are projected onto the 2D image plane or combine the sparse point cloud with dense image pixels. These fusion approaches often suffer from severe information loss, thus causing sub-optimal performance. To address these problems, we construct the homogeneous structure between the point cloud and images to avoid projective information loss by transforming the camera features into the LiDAR 3D space. In this paper, we propose a homogeneous multi-modal feature fusion and interaction method (HMFI) for 3D object detection. Specifically, we first design an image voxel lifter module (IVLM) to lift 2D image features into the 3D space and generate homogeneous image voxel features. Then, we fuse the voxelized point cloud features with the image features from different regions by introducing the self-attention based query fusion mechanism (QFM). Next, we propose a voxel feature interaction module (VFIM) to enforce the consistency of semantic information from identical objects in the homogeneous point cloud and image voxel representations, which can provide object-level alignment guidance for cross-modal feature fusion and strengthen the discriminative ability in complex backgrounds. We conduct extensive experiments on the KITTI and Waymo Open Dataset, and the proposed HMFI achieves better performance compared with the state-of-the-art multi-modal methods. Particularly, for the 3D detection of cyclist on the KITTI benchmark, HMFI surpasses all the published algorithms by a large margin.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Li, Xin and Shi, Botian and Hou, Yuenan and Wu, Xingjiao and Ma, Tianlong and Li, Yikang and He, Liang},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {691--707},
}

@inproceedings{li_deep_2022,
	title = {Deep {Hierarchical} {Semantic} {Segmentation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Li_Deep_Hierarchical_Semantic_Segmentation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-11-09},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Li, Liulei and Zhou, Tianfei and Wang, Wenguan and Li, Jianwu and Yang, Yi},
	year = {2022},
	keywords = {Untagged},
	pages = {1246--1257},
}

@inproceedings{liang2022mind,
	title = {Mind the gap: {Understanding} the modality gap in multi-modal contrastive representation learning},
	url = {https://openreview.net/forum?id=S7Evzt9uit3},
	booktitle = {Advances in neural information processing systems},
	author = {Liang, Weixin and Zhang, Yuhui and Kwon, Yongchan and Yeung, Serena and Zou, James},
	editor = {Oh, Alice H. and Agarwal, Alekh and Belgrave, Danielle and Cho, Kyunghyun},
	year = {2022},
	keywords = {Untagged},
}

@misc{liang_exploring_2022,
	title = {Exploring and {Exploiting} {Hubness} {Priors} for {High}-{Quality} {GAN} {Latent} {Sampling}},
	url = {http://arxiv.org/abs/2206.06014},
	abstract = {Despite the extensive studies on Generative Adversarial Networks (GANs), how to reliably sample high-quality images from their latent spaces remains an under-explored topic. In this paper, we propose a novel GAN latent sampling method by exploring and exploiting the hubness priors of GAN latent distributions. Our key insight is that the high dimensionality of the GAN latent space will inevitably lead to the emergence of hub latents that usually have much larger sampling densities than other latents in the latent space. As a result, these hub latents are better trained and thus contribute more to the synthesis of high-quality images. Unlike the a posterior "cherry-picking", our method is highly efficient as it is an a priori method that identifies high-quality latents before the synthesis of images. Furthermore, we show that the well-known but purely empirical truncation trick is a naive approximation to the central clustering effect of hub latents, which not only uncovers the rationale of the truncation trick, but also indicates the superiority and fundamentality of our method. Extensive experimental results demonstrate the effectiveness of the proposed method.},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {Liang, Yuanbang and Wu, Jing and Lai, Yu-Kun and Qin, Yipeng},
	month = jun,
	year = {2022},
	note = {arXiv:2206.06014 [cs]},
	keywords = {Untagged},
}

@misc{liu_bevfusion_2022,
	title = {{BEVFusion}: {Multi}-{Task} {Multi}-{Sensor} {Fusion} with {Unified} {Bird}'s-{Eye} {View} {Representation}},
	shorttitle = {{BEVFusion}},
	url = {http://arxiv.org/abs/2205.13542},
	doi = {10.48550/arXiv.2205.13542},
	abstract = {Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. However, the camera-to-LiDAR projection throws away the semantic density of camera features, hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). In this paper, we break this deeply-rooted convention with BEVFusion, an efficient and generic multi-task multi-sensor fusion framework. It unifies multi-modal features in the shared bird's-eye view (BEV) representation space, which nicely preserves both geometric and semantic information. To achieve this, we diagnose and lift key efficiency bottlenecks in the view transformation with optimized BEV pooling, reducing latency by more than 40x. BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. It establishes the new state of the art on nuScenes, achieving 1.3\% higher mAP and NDS on 3D object detection and 13.6\% higher mIoU on BEV map segmentation, with 1.9x lower computation cost. Code to reproduce our results is available at https://github.com/mit-han-lab/bevfusion.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Liu, Zhijian and Tang, Haotian and Amini, Alexander and Yang, Xinyu and Mao, Huizi and Rus, Daniela and Han, Song},
	month = jun,
	year = {2022},
	note = {arXiv:2205.13542 [cs]},
	keywords = {Untagged},
}

@inproceedings{liu2022ts2net,
	title = {{TS2}-{Net}: {Token} shift and selection transformer for text-video retrieval},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Liu, Yuqi and Xiong, Pengfei and Xu, Luhui and Cao, Shengming and Jin, Qin},
	year = {2022},
	keywords = {Untagged},
}

@article{liuTransferableUnrestrictedAdversarial2022,
	title = {Towards {Transferable} {Unrestricted} {Adversarial} {Examples} with {Minimum} {Changes}},
	url = {http://arxiv.org/abs/2201.01102},
	abstract = {Transfer-based adversarial example is one of the most important classes of black-box attacks. However, there is a trade-off between transferability and imperceptibility of the adversarial perturbation. Prior work in this direction often requires a fixed but large \${\textbackslash}ell\_p\$-norm perturbation budget to reach a good transfer success rate, leading to perceptible adversarial perturbations. On the other hand, most of the current unrestricted adversarial attacks that aim to generate semantic-preserving perturbations suffer from weaker transferability to the target model. In this work, we propose a geometry-aware framework to generate transferable adversarial examples with minimum changes. Analogous to model selection in statistical machine learning, we leverage a validation model to select the optimal perturbation budget for each image under both the \${\textbackslash}ell\_\{{\textbackslash}infty\}\$-norm and unrestricted threat models. Extensive experiments verify the effectiveness of our framework on balancing imperceptibility and transferability of the crafted adversarial examples. The methodology is the foundation of our entry to the CVPR'21 Security AI Challenger: Unrestricted Adversarial Attacks on ImageNet, in which we ranked 1st place out of 1,559 teams and surpassed the runner-up submissions by 4.59\% and 23.91\% in terms of final score and average image quality level, respectively. Code is available at https://github.com/Equationliu/GA-Attack.},
	urldate = {2022-03-04},
	journal = {arXiv:2201.01102 [cs]},
	author = {Liu, Fangcheng and Zhang, Chao and Zhang, Hongyang},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.01102},
	keywords = {Untagged},
}

@misc{ma_probing_2022,
	title = {Probing {Cross}-modal {Semantics} {Alignment} {Capability} from the {Textual} {Perspective}},
	url = {http://arxiv.org/abs/2210.09550},
	abstract = {In recent years, vision and language pre-training (VLP) models have advanced the state-of-the-art results in a variety of cross-modal downstream tasks. Aligning cross-modal semantics is claimed to be one of the essential capabilities of VLP models. However, it still remains unclear about the inner working mechanism of alignment in VLP models. In this paper, we propose a new probing method that is based on image captioning to first empirically study the cross-modal semantics alignment of VLP models. Our probing method is built upon the fact that given an image-caption pair, the VLP models will give a score, indicating how well two modalities are aligned; maximizing such scores will generate sentences that VLP models believe are of good alignment. Analyzing these sentences thus will reveal in what way different modalities are aligned and how well these alignments are in VLP models. We apply our probing method to five popular VLP models, including UNITER, ROSITA, ViLBERT, CLIP, and LXMERT, and provide a comprehensive analysis of the generated captions guided by these models. Our results show that VLP models (1) focus more on just aligning objects with visual words, while neglecting global semantics; (2) prefer fixed sentence patterns, thus ignoring more important textual information including fluency and grammar; and (3) deem the captions with more visual words are better aligned with images. These findings indicate that VLP models still have weaknesses in cross-modal semantics alignment and we hope this work will draw researchers' attention to such problems when designing a new VLP model.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Ma, Zheng and Zong, Shi and Pan, Mianzhi and Zhang, Jianbing and Huang, Shujian and Dai, Xinyu and Chen, Jiajun},
	month = oct,
	year = {2022},
	note = {arXiv:2210.09550 [cs]},
	keywords = {Untagged},
}

@inproceedings{10.1145/3503161.3547910,
	address = {New York, NY, USA},
	series = {{MM} '22},
	title = {X-{CLIP}: {End}-to-end multi-grained contrastive learning for video-text retrieval},
	isbn = {978-1-4503-9203-7},
	url = {https://doi.org/10.1145/3503161.3547910},
	doi = {10.1145/3503161.3547910},
	abstract = {Video-text retrieval has been a crucial and fundamental task in multi-modal research. The development of video-text retrieval has been considerably promoted by large-scale multi-modal contrastive pre-training, which primarily focuses on coarse-grained or fine-grained contrast. However, cross-grained contrast, which is the contrast between coarse-grained representations and fine-grained representations, has rarely been explored in prior research. Compared with fine-grained or coarse-grained contrasts, cross-grained contrast calculate the correlation between coarse-grained features and each fine-grained feature, and is able to filter out the unnecessary fine-grained features guided by the coarse-grained feature during similarity calculation, thus improving the accuracy of retrieval. To this end, this paper presents a novel multi-grained contrastive model, namely X-CLIP, for video-text retrieval. However, another challenge lies in the similarity aggregation problem, which aims to aggregate fine-grained and cross-grained similarity matrices to instance-level similarity. To address this challenge, we propose the Attention Over Similarity Matrix (AOSM) module to make the model focus on the contrast between essential frames and words, thus lowering the impact of unnecessary frames and words on retrieval results. With multi-grained contrast and the proposed AOSM module, X-CLIP achieves outstanding performance on five widely-used video-text retrieval datasets, including MSR-VTT (49.3 R@1), MSVD (50.4 R@1), LSMDC (26.1 R@1), DiDeMo (47.8 R@1) and ActivityNet (46.2 R@1).},
	booktitle = {{ACM} international conference on multimedia},
	publisher = {Association for Computing Machinery},
	author = {Ma, Yiwei and Xu, Guohai and Sun, Xiaoshuai and Yan, Ming and Zhang, Ji and Ji, Rongrong},
	year = {2022},
	note = {Number of pages: 10
Place: Lisboa, Portugal},
	keywords = {Untagged},
	pages = {638--647},
}

@misc{lu_lgdn_2022,
	title = {{LGDN}: {Language}-{Guided} {Denoising} {Network} for {Video}-{Language} {Modeling}},
	shorttitle = {{LGDN}},
	url = {http://arxiv.org/abs/2209.11388},
	abstract = {Video-language modeling has attracted much attention with the rapid growth of web videos. Most existing methods assume that the video frames and text description are semantically correlated, and focus on video-language modeling at video level. However, this hypothesis often fails for two reasons: (1) With the rich semantics of video contents, it is difficult to cover all frames with a single video-level description; (2) A raw video typically has noisy/meaningless information (e.g., scenery shot, transition or teaser). Although a number of recent works deploy attention mechanism to alleviate this problem, the irrelevant/noisy information still makes it very difficult to address. To overcome such challenge, we thus propose an efficient and effective model, termed Language-Guided Denoising Network (LGDN), for video-language modeling. Different from most existing methods that utilize all extracted video frames, LGDN dynamically filters out the misaligned or redundant frames under the language supervision and obtains only 2--4 salient frames per video for cross-modal token-level alignment. Extensive experiments on five public datasets show that our LGDN outperforms the state-of-the-arts by large margins. We also provide detailed ablation study to reveal the critical importance of solving the noise issue, in hope of inspiring future video-language work.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Lu, Haoyu and Ding, Mingyu and Fei, Nanyi and Huo, Yuqi and Lu, Zhiwu},
	month = oct,
	year = {2022},
	note = {arXiv:2209.11388 [cs]},
	keywords = {Untagged},
}

@misc{marbut_reliable_2022,
	title = {Reliable {Measures} of {Spread} in {High} {Dimensional} {Latent} {Spaces}},
	url = {http://arxiv.org/abs/2212.08172},
	abstract = {Understanding geometric properties of natural language processing models' latent spaces allows the manipulation of these properties for improved performance on downstream tasks. One such property is the amount of data spread in a model's latent space, or how fully the available latent space is being used. In this work, we define data spread and demonstrate that the commonly used measures of data spread, Average Cosine Similarity and a partition function min/max ratio I(V), do not provide reliable metrics to compare the use of latent space across models. We propose and examine eight alternative measures of data spread, all but one of which improve over these current metrics when applied to seven synthetic data distributions. Of our proposed measures, we recommend one principal component-based measure and one entropy-based measure that provide reliable, relative measures of spread and can be used to compare models of different sizes and dimensionalities.},
	urldate = {2023-04-16},
	publisher = {arXiv},
	author = {Marbut, Anna C. and McKinney-Bock, Katy and Wheeler, Travis J.},
	month = dec,
	year = {2022},
	note = {arXiv:2212.08172 [cs]},
	keywords = {Untagged},
}

@misc{min_hunyuan_tvr_2022,
	title = {{HunYuan}\_tvr for {Text}-{Video} {Retrieval}},
	url = {http://arxiv.org/abs/2204.03382},
	doi = {10.48550/arXiv.2204.03382},
	abstract = {Text-Video Retrieval plays an important role in multi-modal understanding and has attracted increasing attention in recent years. Most existing methods focus on constructing contrastive pairs between whole videos and complete caption sentences, while ignoring fine-grained cross-modal relationships, e.g., short clips and phrases or single frame and word. In this paper, we propose a novel method, named HunYuan{\textbackslash}\_tvr, to explore hierarchical cross-modal interactions by simultaneously exploring video-sentence, clip-phrase, and frame-word relationships. Considering intrinsic semantic relations between frames, HunYuan{\textbackslash}\_tvr first performs self-attention to explore frame-wise correlations and adaptively clusters correlated frames into clip-level representations. Then, the clip-wise correlation is explored to aggregate clip representations into a compact one to describe the video globally. In this way, we can construct hierarchical video representations for frame-clip-video granularities, and also explore word-wise correlations to form word-phrase-sentence embeddings for the text modality. Finally, hierarchical contrastive learning is designed to explore cross-modal relationships,{\textasciitilde}{\textbackslash}emph\{i.e.,\} frame-word, clip-phrase, and video-sentence, which enables HunYuan{\textbackslash}\_tvr to achieve a comprehensive multi-modal understanding. Further boosted by adaptive label denoising and marginal sample enhancement, HunYuan{\textbackslash}\_tvr obtains new state-of-the-art results on various benchmarks, e.g., Rank@1 of 55.0\%, 57.8\%, 29.7\%, 52.1\%, and 57.3\% on MSR-VTT, MSVD, LSMDC, DiDemo, and ActivityNet respectively.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Min, Shaobo and Kong, Weijie and Tu, Rong-Cheng and Gong, Dihong and Cai, Chengfei and Zhao, Wenzhe and Liu, Chenyang and Zheng, Sixiao and Wang, Hongfa and Li, Zhifeng and Liu, Wei},
	month = oct,
	year = {2022},
	note = {arXiv:2204.03382 [cs]},
	keywords = {Untagged},
}

@inproceedings{navon_multi-task_2022,
	title = {Multi-{Task} {Learning} as a {Bargaining} {Game}},
	url = {http://arxiv.org/abs/2202.01017},
	abstract = {In Multi-task learning (MTL), a joint model is trained to simultaneously make predictions for several tasks. Joint training reduces computation costs and improves data efficiency; however, since the gradients of these different tasks may conflict, training a joint model for MTL often yields lower performance than its corresponding single-task counterparts. A common method for alleviating this issue is to combine per-task gradients into a joint update direction using a particular heuristic. In this paper, we propose viewing the gradients combination step as a bargaining game, where tasks negotiate to reach an agreement on a joint direction of parameter update. Under certain assumptions, the bargaining problem has a unique solution, known as the Nash Bargaining Solution, which we propose to use as a principled approach to multi-task learning. We describe a new MTL optimization procedure, Nash-MTL, and derive theoretical guarantees for its convergence. Empirically, we show that Nash-MTL achieves state-of-the-art results on multiple MTL benchmarks in various domains.},
	urldate = {2022-06-07},
	booktitle = {{arXiv}:2202.01017},
	publisher = {arXiv},
	author = {Navon, Aviv and Shamsian, Aviv and Achituve, Idan and Maron, Haggai and Kawaguchi, Kenji and Chechik, Gal and Fetaya, Ethan},
	month = feb,
	year = {2022},
	note = {arXiv:2202.01017 [cs]
type: article},
	keywords = {Untagged},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	shorttitle = {{InstructGPT}},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Untagged},
}

@inproceedings{park_exposing_2022,
	address = {Seattle, United States},
	title = {Exposing the {Limits} of {Video}-{Text} {Models} through {Contrast} {Sets}},
	url = {https://aclanthology.org/2022.naacl-main.261},
	doi = {10.18653/v1/2022.naacl-main.261},
	abstract = {Recent video-text models can retrieve relevant videos based on text with a high accuracy, but to what extent do they comprehend the semantics of the text? Can they discriminate between similar entities and actions? To answer this, we propose an evaluation framework that probes video-text models with hard negatives. We automatically build contrast sets, where true textual descriptions are manipulated in ways that change their semantics while maintaining plausibility. Specifically, we leverage a pre-trained language model and a set of heuristics to create verb and person entity focused contrast sets. We apply these in the multiple choice video to-text classification setting. We test the robustness of recent methods on the proposed automatic contrast sets, and compare them to additionally collected human-generated counterparts, to assess their effectiveness. We see that model performance suffers across all methods, erasing the gap between recent CLIP-based methods vs. the earlier methods.},
	urldate = {2022-10-19},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Park, Jae Sung and Shen, Sheng and Farhadi, Ali and Darrell, Trevor and Choi, Yejin and Rohrbach, Anna},
	month = jul,
	year = {2022},
	keywords = {Untagged},
	pages = {3574--3586},
}

@inproceedings{qiu_contrastive_2022,
	address = {Virtual Event AZ USA},
	title = {Contrastive {Learning} for {Representation} {Degeneration} {Problem} in {Sequential} {Recommendation}},
	isbn = {978-1-4503-9132-0},
	url = {https://dl.acm.org/doi/10.1145/3488560.3498433},
	doi = {10.1145/3488560.3498433},
	language = {en},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the {Fifteenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Qiu, Ruihong and Huang, Zi and Yin, Hongzhi and Wang, Zijian},
	month = feb,
	year = {2022},
	keywords = {Untagged},
	pages = {813--823},
}

@misc{penha_sparse_2022,
	title = {Sparse and {Dense} {Approaches} for the {Full}-rank {Retrieval} of {Responses} for {Dialogues}},
	url = {http://arxiv.org/abs/2204.10558},
	abstract = {Ranking responses for a given dialogue context is a popular benchmark in which the setup is to re-rank the ground-truth response over a limited set of 𝑛 responses, where 𝑛 is typically 10. The predominance of this setup in conversation response ranking has lead to a great deal of attention to building neural re-rankers, while the first-stage retrieval step has been overlooked. Since the correct answer is always available in the candidate list of 𝑛 responses, this artificial evaluation setup assumes that there is a first-stage retrieval step which is always able to rank the correct response in its top-𝑛 list. In this paper we focus on the more realistic task of full-rank retrieval of responses, where 𝑛 can be up to millions of responses. We investigate both dialogue context and response expansion techniques for sparse retrieval, as well as zero-shot and fine-tuned dense retrieval approaches. Our findings—based on three different informationseeking dialogue datasets—reveal that a learned response expansion technique is a solid baseline for sparse retrieval. We find the best performing method overall to be dense retrieval with intermediate training—a step after the language model pre-training where sentence representations are learned—followed by fine-tuning on the target conversational data. We also investigate the intriguing phenomena that harder negatives sampling techniques lead to worse results for the fine-tuned dense retrieval models. The code and datasets are available at https://github.com/Guzpenha/transformer\_ rankers/tree/full\_rank\_retrieval\_dialogues.},
	language = {en},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Penha, Gustavo and Hauff, Claudia},
	month = apr,
	year = {2022},
	note = {arXiv:2204.10558 [cs]},
	keywords = {Untagged},
}

@article{ravindran_camera_2022,
	title = {Camera, {LiDAR}, and {Radar} {Sensor} {Fusion} {Based} on {Bayesian} {Neural} {Network} ({CLR}-{BNN})},
	volume = {22},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2022.3154980},
	abstract = {Perception in automated vehicles (AV) is the main factor in achieving safe driving. In this perception task, multi-object detection (MOD) in diverse driving situations is the main challenge. Our recent survey [Ravindran et al. (2021)] shows the limitations of deep neural networks (DNN) in predicting the uncertainties of object detection in MOD. This research proposed a camera, LiDAR and RADAR sensor fusion Bayesian neural network (CLR-BNN) to improve detection accuracy and reduce uncertainties in diverse driving situations using these three primary sensing devices. The experiments were performed using the nuScence dataset with incorporation of various noises. The CLR-BNN performed better than its deterministic sensor fusion model (CLR-DNN) in terms of mAP. The CLR-BNN also showed improvement in categorical and bounding box location uncertainty using sensor fusion in diverse driving conditions. The uncertainty predictions of the CLR-BNN were validated using the calibration curve and other performance metrics.},
	number = {7},
	journal = {IEEE Sensors Journal},
	author = {Ravindran, Ratheesh and Santora, Michael J. and Jamali, Mohsin M.},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Sensors Journal},
	keywords = {Untagged},
	pages = {6964--6974},
}

@inproceedings{raghunathan_certified_2022,
	title = {Certified {Defenses} against {Adversarial} {Examples}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Raghunathan, Aditi and Steinhardt, Jacob and Liang, Percy},
	year = {2022},
	keywords = {Untagged},
}

@misc{satar_rome_2022,
	title = {{RoME}: {Role}-aware {Mixture}-of-{Expert} {Transformer} for {Text}-to-{Video} {Retrieval}},
	shorttitle = {{RoME}},
	url = {http://arxiv.org/abs/2206.12845},
	abstract = {Seas of videos are uploaded daily with the popularity of social channels; thus, retrieving the most related video contents with user textual queries plays a more crucial role. Most methods consider only one joint embedding space between global visual and textual features without considering the local structures of each modality. Some other approaches consider multiple embedding spaces consisting of global and local features separately, ignoring rich inter-modality correlations. We propose a novel mixture-of-expert transformer RoME that disentangles the text and the video into three levels; the roles of spatial contexts, temporal contexts, and object contexts. We utilize a transformer-based attention mechanism to fully exploit visual and text embeddings at both global and local levels with mixture-of-experts for considering inter-modalities and structures' correlations. The results indicate that our method outperforms the state-of-the-art methods on the YouCook2 and MSR-VTT datasets, given the same visual backbone without pre-training. Finally, we conducted extensive ablation studies to elucidate our design choices.},
	urldate = {2023-05-30},
	publisher = {arXiv},
	author = {Satar, Burak and Zhu, Hongyuan and Zhang, Hanwang and Lim, Joo Hwee},
	month = jun,
	year = {2022},
	note = {arXiv:2206.12845 [cs]
version: 1},
	keywords = {Untagged},
}

@misc{shanahan_talking_2022,
	title = {Talking {About} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2212.03551},
	abstract = {Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as "knows", "believes", and "thinks", when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.},
	urldate = {2022-12-12},
	publisher = {arXiv},
	author = {Shanahan, Murray},
	month = dec,
	year = {2022},
	note = {arXiv:2212.03551 [cs]},
	keywords = {Untagged},
}

@inproceedings{reddy_master_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Master of {All}: {Simultaneous} {Generalization} of {Urban}-{Scene} {Segmentation} to {All} {Adverse} {Weather} {Conditions}},
	isbn = {978-3-031-19842-7},
	shorttitle = {Master of {All}},
	doi = {10.1007/978-3-031-19842-7_4},
	abstract = {Computer vision systems for autonomous navigation must generalize well in adverse weather and illumination conditions expected in the real world. However, semantic segmentation of images captured in such conditions remains a challenging task for current state-of-the-art (SOTA) methods trained on broad daylight images, due to the associated distribution shift. On the other hand, domain adaptation techniques developed for the purpose rely on the availability of the source data, (un)labeled target data and/or its auxiliary information (e.g., GPS). Even then, they typically adapt to a single(specific) target domain(s). To remedy this, we propose a novel, fully test time, adaptation technique, named Master of ALL (MALL), for simultaneous generalization to multiple target domains. MALL learns to generalize on unseen adverse weather images from multiple target domains directly at the inference time. More specifically, given a pre-trained model and its parameters, MALL enforces edge consistency prior at the inference stage and updates the model based on (a) a single test sample at a time (MALL-sample), or (b) continuously for the whole test domain (MALL-domain). Not only the target data, MALL also does not need access to the source data and thus, can be used with any pre-trained model. Using a simple model pre-trained on daylight images, MALL outperforms specially designed adverse weather semantic segmentation methods, both in domain generalization and test-time adaptation settings. Our experiments on foggy, snow, night, cloudy, overcast, and rainy conditions demonstrate the target domain-agnostic effectiveness of our approach. We further show that MALL can improve the performance of a model on an adverse weather condition, even when the model is already pre-trained for the specific condition.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Reddy, Nikhil and Singhal, Abhinav and Kumar, Abhishek and Baktashmotlagh, Mahsa and Arora, Chetan},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {Untagged},
	pages = {51--69},
}

@article{rezaeiAccuracyPrivacyTradeoffDeep2022,
	title = {Accuracy-{Privacy} {Trade}-off in {Deep} {Ensemble}: {A} {Membership} {Inference} {Perspective}},
	shorttitle = {Accuracy-{Privacy} {Trade}-off in {Deep} {Ensemble}},
	url = {https://openreview.net/forum?id=wxVpa5z4DU1},
	abstract = {Deep ensemble learning has been shown to improve accuracy by training multiple neural networks and fusing their outputs. Ensemble learning has also been used to defend against membership inference...},
	language = {en},
	urldate = {2022-03-30},
	journal = {ICLR Rejection},
	author = {Rezaei, Shahbaz and Shafiq, Zubair and Liu, Xin},
	year = {2022},
	keywords = {Untagged},
}

@misc{shi_knn-prompt_2022,
	title = {{kNN}-{Prompt}: {Nearest} {Neighbor} {Zero}-{Shot} {Inference}},
	shorttitle = {{kNN}-{Prompt}},
	url = {http://arxiv.org/abs/2205.13792},
	abstract = {Retrieval-augmented language models (LMs) use non-parametric memory to substantially outperform their non-retrieval counterparts on perplexity-based evaluations, but it is an open question whether they achieve similar gains in few- and zero-shot end-task accuracy. We extensively study one such model, the k-nearest neighbor LM (kNN-LM), showing that the gains marginally transfer. The main challenge is to achieve coverage of the verbalizer tokens that define the different end-task class labels. To address this challenge, we also introduce kNN-Prompt, a simple and effective kNN-LM with automatically expanded fuzzy verbalizers (e.g. to expand terrible to also include silly and other task-specific synonyms for sentiment classification). Across nine diverse end-tasks, using kNN-Prompt with GPT-2 large yields significant performance boosts over strong zero-shot baselines (13.4\% absolute improvement over the base LM on average). We also show that other advantages of non-parametric augmentation hold for end tasks; kNN-Prompt is effective for domain adaptation with no further training, and gains increase with the size of the retrieval model.},
	urldate = {2023-05-03},
	publisher = {arXiv},
	author = {Shi, Weijia and Michael, Julian and Gururangan, Suchin and Zettlemoyer, Luke},
	month = nov,
	year = {2022},
	note = {arXiv:2205.13792 [cs]},
	keywords = {Untagged},
}

@inproceedings{shvetsova_everything_2022,
	address = {New Orleans, LA, USA},
	title = {Everything at {Once} – {Multi}-modal {Fusion} {Transformer} for {Video} {Retrieval}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9879495/},
	doi = {10.1109/CVPR52688.2022.01939},
	language = {en},
	urldate = {2022-10-19},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Shvetsova, Nina and Chen, Brian and Rouditchenko, Andrew and Thomas, Samuel and Kingsbury, Brian and Feris, Rogerio and Harwath, David and Glass, James and Kuehne, Hilde},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {19988--19997},
}

@article{thudiBoundingMembershipInference2022,
	title = {Bounding {Membership} {Inference}},
	url = {https://openreview.net/forum?id=Mh40mAxxAUz},
	abstract = {Differential Privacy (DP) is the de facto standard for reasoning about the privacy guarantees of a training algorithm. Despite the empirical observation that DP reduces the vulnerability of models...},
	language = {en},
	urldate = {2022-03-30},
	journal = {ICLR Rejection},
	author = {Thudi, Anvith and Shumailov, I. and Boenisch, Franziska and Papernot, Nicolas},
	year = {2022},
	keywords = {Untagged},
}

@inproceedings{togashi_axiou_2022,
	address = {New Orleans, LA, USA},
	title = {{AxIoU}: {An} {Axiomatically} {Justified} {Measure} for {Video} {Moment} {Retrieval}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{AxIoU}},
	url = {https://ieeexplore.ieee.org/document/9879518/},
	doi = {10.1109/CVPR52688.2022.02040},
	abstract = {Evaluation measures have a crucial impact on the direction of research. Therefore, it is of utmost importance to develop appropriate and reliable evaluation measures for new applications where conventional measures are not well suited. Video Moment Retrieval (VMR) is one such application, and the current practice is to use R@K, θ for evaluating VMR systems. However, this measure has two disadvantages. First, it is rank-insensitive: It ignores the rank positions of successfully localised moments in the top-K ranked list by treating the list as a set. Second, it binarizes the Intersection over Union (IoU) of each retrieved video moment using the threshold θ and thereby ignoring fine-grained localisation quality of ranked moments.},
	language = {en},
	urldate = {2023-05-10},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Togashi, Riku and Otani, Mayu and Nakashima, Yuta and Rahtu, Esa and Heikkila, Janne and Sakai, Tetsuya},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {21044--21053},
}

@inproceedings{tomy_fusing_2022,
	title = {Fusing {Event}-based and {RGB} camera for {Robust} {Object} {Detection} in {Adverse} {Conditions}},
	doi = {10.1109/ICRA46639.2022.9812059},
	abstract = {The ability to detect objects, under image corruptions and different weather conditions is vital for deep learning models especially when applied to real-world applications such as autonomous driving. Traditional RGB-based detection fails under these conditions and it is thus important to design a sensor suite that is redundant to failures of the primary frame-based detection. Event-based cameras can complement frame-based cameras in low-light conditions and high dynamic range scenarios that an autonomous vehicle can encounter during navigation. Accordingly, we propose a redundant sensor fusion model of event-based and frame-based cameras that is robust to common image corruptions. The method utilizes a voxel grid representation for events as input and proposes a two-parallel feature extractor network for frames and events. Our sensor fusion approach is more robust to corruptions by over 30\% compared to only frame-based detections and outperforms the only event-based detection. The model is trained and evaluated on the publicly released DSEC dataset.},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Tomy, Abhishek and Paigwar, Anshul and Mann, Khushdeep S. and Renzaglia, Alessandro and Laugier, Christian},
	month = may,
	year = {2022},
	keywords = {Untagged},
	pages = {933--939},
}

@article{vardiGradientMethodsProvably2022,
	title = {Gradient {Methods} {Provably} {Converge} to {Non}-{Robust} {Networks}},
	url = {http://arxiv.org/abs/2202.04347},
	abstract = {Despite a great deal of research, it is still unclear why neural networks are so susceptible to adversarial examples. In this work, we identify natural settings where depth-2 ReLU networks trained with gradient ﬂow are provably non-robust (susceptible to small adversarial 2-perturbations), even when robust networks that classify the training dataset correctly exist. Perhaps surprisingly, we show that the well-known implicit bias towards margin maximization induces bias towards non-robust networks, by proving that every network which satisﬁes the KKT conditions of the max-margin problem is non-robust.},
	language = {en},
	urldate = {2022-03-10},
	journal = {arXiv:2202.04347 [cs]},
	author = {Vardi, Gal and Yehudai, Gilad and Shamir, Ohad},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.04347},
	keywords = {Untagged},
}

@article{wang_momentum_2022,
	title = {Momentum {Doesn}'t {Change} {The} {Implicit} {Bias}},
	url = {https://openreview.net/forum?id=yzDTTtlIlMr},
	abstract = {The momentum acceleration technique is widely adopted in many optimization algorithms. However, the theoretical understanding on how the momentum affects the generalization performance of the optimization algorithms is still unknown. In this paper, we answer this question through analyzing the implicit bias of momentum-based optimization. We prove that both SGD with momentum and Adam converge to the \$L\_2\$ max-margin solution for exponential-tailed loss, which is the same as vanilla gradient descent. That means, these optimizers with momentum acceleration still converge to a model with low complexity, which provides guarantees on their generalization. Technically, to overcome the difficulty brought by the error accumulation in analyzing the momentum, we construct new Lyapunov functions as a tool to analyze the gap between the model parameter and the max-margin solution.},
	language = {en},
	urldate = {2023-05-11},
	author = {Wang, Bohan and Meng, Qi and Zhang, Huishuai and Sun, Ruoyu and Chen, Wei and Ma, Zhi-Ming},
	month = jan,
	year = {2022},
	keywords = {Untagged},
}

@misc{wang_adversarial_2022,
	title = {Adversarial {GLUE}: {A} {Multi}-{Task} {Benchmark} for {Robustness} {Evaluation} of {Language} {Models}},
	shorttitle = {Adversarial {GLUE}},
	url = {http://arxiv.org/abs/2111.02840},
	abstract = {Large-scale pre-trained language models have achieved tremendous success across a wide range of natural language understanding (NLU) tasks, even surpassing human performance. However, recent studies reveal that the robustness of these models can be challenged by carefully crafted textual adversarial examples. While several individual datasets have been proposed to evaluate model robustness, a principled and comprehensive benchmark is still missing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task benchmark to quantitatively and thoroughly explore and evaluate the vulnerabilities of modern large-scale language models under various types of adversarial attacks. In particular, we systematically apply 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, which is further validated by humans for reliable annotations. Our findings are summarized as follows. (i) Most existing adversarial attack algorithms are prone to generating invalid or ambiguous adversarial examples, with around 90\% of them either changing the original semantic meanings or misleading human annotators as well. Therefore, we perform a careful filtering process to curate a high-quality benchmark. (ii) All the language models and robust training methods we tested perform poorly on AdvGLUE, with scores lagging far behind the benign accuracy. We hope our work will motivate the development of new adversarial attacks that are more stealthy and semantic-preserving, as well as new robust language models against sophisticated adversarial attacks. AdvGLUE is available at https://adversarialglue.github.io.},
	urldate = {2022-11-08},
	publisher = {arXiv},
	author = {Wang, Boxin and Xu, Chejian and Wang, Shuohang and Gan, Zhe and Cheng, Yu and Gao, Jianfeng and Awadallah, Ahmed Hassan and Li, Bo},
	month = jan,
	year = {2022},
	note = {arXiv:2111.02840 [cs]},
	keywords = {Untagged},
}

@misc{wang_discrete_2022,
	title = {Discrete {Cross}-{Modal} {Alignment} {Enables} {Zero}-{Shot} {Speech} {Translation}},
	url = {http://arxiv.org/abs/2210.09556},
	abstract = {End-to-end Speech Translation (ST) aims at translating the source language speech into target language text without generating the intermediate transcriptions. However, the training of end-to-end methods relies on parallel ST data, which are difficult and expensive to obtain. Fortunately, the supervised data for automatic speech recognition (ASR) and machine translation (MT) are usually more accessible, making zero-shot speech translation a potential direction. Existing zero-shot methods fail to align the two modalities of speech and text into a shared semantic space, resulting in much worse performance compared to the supervised ST methods. In order to enable zero-shot ST, we propose a novel Discrete Cross-Modal Alignment (DCMA) method that employs a shared discrete vocabulary space to accommodate and match both modalities of speech and text. Specifically, we introduce a vector quantization module to discretize the continuous representations of speech and text into a finite set of virtual tokens, and use ASR data to map corresponding speech and text to the same virtual token in a shared codebook. This way, source language speech can be embedded in the same semantic space as the source language text, which can be then transformed into target language text with an MT module. Experiments on multiple language pairs demonstrate that our zero-shot ST method significantly improves the SOTA, and even performers on par with the strong supervised ST baselines.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Wang, Chen and Liu, Yuchen and Chen, Boxing and Zhang, Jiajun and Luo, Wei and Huang, Zhongqiang and Zong, Chengqing},
	month = oct,
	year = {2022},
	note = {arXiv:2210.09556 [cs, eess]},
	keywords = {Untagged},
}

@inproceedings{wang_provable_2022,
	title = {Provable {Domain} {Generalization} via {Invariant}-{Feature} {Subspace} {Recovery}},
	url = {https://proceedings.mlr.press/v162/wang22x.html},
	abstract = {Domain generalization asks for models trained over a set of training environments to perform well in unseen test environments. Recently, a series of algorithms such as Invariant Risk Minimization (IRM) has been proposed for domain generalization. However, Rosenfeld et al. (2021) shows that in a simple linear data model, even if non-convexity issues are ignored, IRM and its extensions cannot generalize to unseen environments with less than \$d\_s+1\$ training environments, where \$d\_s\$ is the dimension of the spurious-feature subspace. In this paper, we propose to achieve domain generalization with Invariant-feature Subspace Recovery (ISR). Our first algorithm, ISR-Mean, can identify the subspace spanned by invariant features from the first-order moments of the class-conditional distributions, and achieve provable domain generalization with \$d\_s+1\$ training environments under the data model of Rosenfeld et al. (2021). Our second algorithm, ISR-Cov, further reduces the required number of training environments to \$O(1)\$ using the information of second-order moments. Notably, unlike IRM, our algorithms bypass non-convexity issues and enjoy global convergence guarantees. Empirically, our ISRs can obtain superior performance compared with IRM on synthetic benchmarks. In addition, on three real-world image and text datasets, we show that both ISRs can be used as simple yet effective post-processing methods to improve the worst-case accuracy of (pre-)trained models against spurious correlations and group shifts.},
	language = {en},
	urldate = {2022-11-14},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wang, Haoxiang and Si, Haozhe and Li, Bo and Zhao, Han},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {23018--23033},
}

@misc{wang_disentangled_2022,
	title = {Disentangled {Representation} {Learning} for {Text}-{Video} {Retrieval}},
	url = {http://arxiv.org/abs/2203.07111},
	doi = {10.48550/arXiv.2203.07111},
	abstract = {Cross-modality interaction is a critical component in Text-Video Retrieval (TVR), yet there has been little examination of how different influencing factors for computing interaction affect performance. This paper first studies the interaction paradigm in depth, where we find that its computation can be split into two terms, the interaction contents at different granularity and the matching function to distinguish pairs with the same semantics. We also observe that the single-vector representation and implicit intensive function substantially hinder the optimization. Based on these findings, we propose a disentangled framework to capture a sequential and hierarchical representation. Firstly, considering the natural sequential structure in both text and video inputs, a Weighted Token-wise Interaction (WTI) module is performed to decouple the content and adaptively exploit the pair-wise correlations. This interaction can form a better disentangled manifold for sequential inputs. Secondly, we introduce a Channel DeCorrelation Regularization (CDCR) to minimize the redundancy between the components of the compared vectors, which facilitate learning a hierarchical representation. We demonstrate the effectiveness of the disentangled representation on various benchmarks, e.g., surpassing CLIP4Clip largely by +2.9\%, +3.1\%, +7.9\%, +2.3\%, +2.8\% and +6.5\% R@1 on the MSR-VTT, MSVD, VATEX, LSMDC, AcitivityNet, and DiDeMo, respectively.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Wang, Qiang and Zhang, Yanhao and Zheng, Yun and Pan, Pan and Hua, Xian-Sheng},
	month = mar,
	year = {2022},
	note = {arXiv:2203.07111 [cs]},
	keywords = {Untagged},
}

@inproceedings{wang_ofa_2022,
	title = {{OFA}: {Unifying} {Architectures}, {Tasks}, and {Modalities} {Through} a {Simple} {Sequence}-to-{Sequence} {Learning} {Framework}},
	shorttitle = {{OFA}},
	url = {https://proceedings.mlr.press/v162/wang22al.html},
	abstract = {In this work, we pursue a unified paradigm for multimodal pretraining to break the shackles of complex task/modality-specific customization. We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks, including image generation, visual grounding, image captioning, image classification, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA follows the instruction-based learning in both pretraining and finetuning stages, requiring no extra task-specific layers for downstream tasks. In comparison with the recent state-of-the-art vision \& language models that rely on extremely large cross-modal datasets, OFA is pretrained on only 20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training data, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive performances on uni-modal tasks. Our further analysis indicates that OFA can also effectively transfer to unseen tasks and unseen domains. Our code and models are publicly available at https://github.com/OFA-Sys/OFA.},
	language = {en},
	urldate = {2022-10-05},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
	year = {2022},
	note = {ISSN: 2640-3498},
	keywords = {Untagged},
	pages = {23318--23340},
}

@inproceedings{wang_measure_2022,
	address = {Seattle, United States},
	title = {Measure and {Improve} {Robustness} in {NLP} {Models}: {A} {Survey}},
	shorttitle = {Measure and {Improve} {Robustness} in {NLP} {Models}},
	url = {https://aclanthology.org/2022.naacl-main.339},
	doi = {10.18653/v1/2022.naacl-main.339},
	abstract = {As NLP models achieved state-of-the-art performances over benchmarks and gained wide applications, it has been increasingly important to ensure the safe deployment of these models in the real world, e.g., making sure the models are robust against unseen or challenging scenarios. Despite robustness being an increasingly studied topic, it has been separately explored in applications like vision and NLP, with various definitions, evaluation and mitigation strategies in multiple lines of research. In this paper, we aim to provide a unifying survey of how to define, measure and improve robustness in NLP. We first connect multiple definitions of robustness, then unify various lines of work on identifying robustness failures and evaluating models' robustness. Correspondingly, we present mitigation strategies that are data-driven, model-driven, and inductive-prior-based, with a more systematic view of how to effectively improve robustness in NLP models. Finally, we conclude by outlining open challenges and future directions to motivate further research in this area.},
	urldate = {2022-11-07},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Xuezhi and Wang, Haohan and Yang, Diyi},
	month = jul,
	year = {2022},
	keywords = {Untagged},
	pages = {4569--4586},
}

@article{wang_scalable_2022,
	title = {A {Scalable} and {Accurate} {De}-{Snowing} {Algorithm} for {LiDAR} {Point} {Clouds} in {Winter}},
	volume = {14},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/6/1468},
	doi = {10.3390/rs14061468},
	abstract = {Accurate and efﬁcient environmental awareness is a fundamental capability of autonomous driving technology and the real-time data collected by sensors offer autonomous vehicles an intuitive impression of their environment. Unfortunately, the ambient noise caused by varying weather conditions immediately affects the ability of autonomous vehicles to accurately understand their environment and its expected impact. In recent years, researchers have improved the environmental perception capabilities of simultaneous localization and mapping (SLAM), object detection and tracking, semantic segmentation and panoptic segmentation, but relatively few studies have focused on enhancing environmental perception capabilities in adverse weather conditions, such as rain, snow and fog. To enhance the environmental perception of autonomous vehicles in adverse weather, we developed a dynamic ﬁltering method called Dynamic Distance–Intensity Outlier Removal (DDIOR), which integrates the distance and intensity of points based on the systematic and accurate analysis of LiDAR point cloud data characteristics in snowy weather. Experiments on the publicly available WADS dataset (Winter Adverse Driving dataSet) showed that our method can efﬁciently remove snow noise while fully preserving the detailed features of the environment.},
	language = {en},
	number = {6},
	urldate = {2023-06-02},
	journal = {Remote Sensing},
	author = {Wang, Weiqi and You, Xiong and Chen, Lingyu and Tian, Jiangpeng and Tang, Fen and Zhang, Lantian},
	month = mar,
	year = {2022},
	keywords = {Untagged},
	pages = {1468},
}

@article{wang_align_2022,
	title = {Align and {Tell}: {Boosting} {Text}-{Video} {Retrieval} {With} {Local} {Alignment} and {Fine}-{Grained} {Supervision}},
	issn = {1941-0077},
	shorttitle = {Align and {Tell}},
	doi = {10.1109/TMM.2022.3204444},
	abstract = {Text-video retrieval is one of the basic tasks for multimodal research and has been widely harnessed in many real-world systems. Most existing approaches directly compare the global representation between videos and text descriptions and utilize the global contrastive loss to train the model. These designs overlook the local alignment and the word-level supervision signal. In this paper, we propose a new framework, called Align and Tell, for text-video retrieval. Compared to the previous work, our framework contains additional modules, i.e., two transformer decoders for local alignment and one captioning head to enhance the representation learning. First, we introduce a set of learnable queries to interact with both textual representations and video representations and project them to a fixed number of local features. After that, local contrastive learning is performed to complement the global comparison. Moreover, we design a video captioning head to provide additional supervision signals during training. This word-level supervision can enhance the visual presentation and alleviate the cross-modal gap. The captioning head can be removed during inference and does not introduce extra computational costs. Extensive empirical results demonstrate that our Align and Tell model can achieve state-of-the-art performance on four text-video retrieval datasets, including MSR-VTT, MSVD, LSMDC, and ActivityNet-Captions.},
	journal = {IEEE Transactions on Multimedia},
	author = {Wang, Xiaohan and Zhu, Linchao and Zheng, Zhedong and Xu, Mingliang and Yang, Yi},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Multimedia},
	keywords = {Untagged},
	pages = {1--11},
}

@misc{wang_self-instruct_2022,
	title = {Self-{Instruct}: {Aligning} {Language} {Model} with {Self} {Generated} {Instructions}},
	shorttitle = {Self-{Instruct}},
	url = {http://arxiv.org/abs/2212.10560},
	abstract = {Large "instruction-tuned" language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model. Applying our method to vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT\_001, which is trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT\_001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
	urldate = {2023-04-11},
	publisher = {arXiv},
	author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10560 [cs]},
	keywords = {Untagged},
}

@inproceedings{wang_learn_2022,
	address = {Lisboa Portugal},
	title = {Learn to {Understand} {Negation} in {Video} {Retrieval}},
	isbn = {978-1-4503-9203-7},
	url = {https://dl.acm.org/doi/10.1145/3503161.3547968},
	doi = {10.1145/3503161.3547968},
	language = {en},
	urldate = {2022-10-20},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Wang, Ziyue and Chen, Aozhu and Hu, Fan and Li, Xirong},
	month = oct,
	year = {2022},
	keywords = {Untagged},
	pages = {434--443},
}

@misc{wang_vqa-gnn_2022,
	title = {{VQA}-{GNN}: {Reasoning} with {Multimodal} {Semantic} {Graph} for {Visual} {Question} {Answering}},
	shorttitle = {{VQA}-{GNN}},
	url = {http://arxiv.org/abs/2205.11501},
	abstract = {Visual understanding requires seamless integration between recognition and reasoning: beyond image-level recognition (e.g., detecting objects), systems must perform concept-level reasoning (e.g., inferring the context of objects and intents of people). However, existing methods only model the image-level features, and do not ground them and reason with background concepts such as knowledge graphs (KGs). In this work, we propose a novel visual question answering method, VQA-GNN, which unifies the image-level information and conceptual knowledge to perform joint reasoning of the scene. Specifically, given a question-image pair, we build a scene graph from the image, retrieve a relevant linguistic subgraph from ConceptNet and visual subgraph from VisualGenome, and unify these three graphs and the question into one joint graph, multimodal semantic graph. Our VQA-GNN then learns to aggregate messages and reason across different modalities captured by the multimodal semantic graph. In the evaluation on the VCR task, our method outperforms the previous scene graph-based Trans-VL models by over 4\%, and VQA-GNN-Large, our model that fuses a Trans-VL further improves the state of the art by 2\%, attaining the top of the VCR leaderboard at the time of submission. This result suggests the efficacy of our model in performing conceptual reasoning beyond image-level recognition for visual understanding. Finally, we demonstrate that our model is the first work to provide interpretability across visual and textual knowledge domains for the VQA task.},
	urldate = {2022-10-15},
	publisher = {arXiv},
	author = {Wang, Yanan and Yasunaga, Michihiro and Ren, Hongyu and Wada, Shinya and Leskovec, Jure},
	month = may,
	year = {2022},
	note = {arXiv:2205.11501 [cs]},
	keywords = {Untagged},
}

@inproceedings{watsonImportanceDifficultyCalibration2022,
	title = {On the {Importance} of {Difficulty} {Calibration} in {Membership} {Inference} {Attacks}},
	url = {https://openreview.net/forum?id=3eIrli0TwQ},
	abstract = {The vulnerability of machine learning models to membership inference attacks has received much attention in recent years. However, existing attacks mostly remain impractical due to having high...},
	language = {en},
	urldate = {2022-03-30},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Watson, Lauren and Guo, Chuan and Cormode, Graham and Sablayrolles, Alexandre},
	year = {2022},
	keywords = {Untagged},
}

@misc{wu_memorizing_2022,
	title = {Memorizing {Transformers}},
	url = {http://arxiv.org/abs/2203.08913},
	abstract = {Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.},
	urldate = {2023-05-03},
	publisher = {arXiv},
	author = {Wu, Yuhuai and Rabe, Markus N. and Hutchins, DeLesley and Szegedy, Christian},
	month = mar,
	year = {2022},
	note = {arXiv:2203.08913 [cs]},
	keywords = {Untagged},
}

@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = oct,
	year = {2022},
	note = {arXiv:2206.07682 [cs]},
	keywords = {Untagged},
}

@misc{wu_entity-focused_2022,
	title = {Entity-{Focused} {Dense} {Passage} {Retrieval} for {Outside}-{Knowledge} {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2210.10176},
	abstract = {Most Outside-Knowledge Visual Question Answering (OK-VQA) systems employ a two-stage framework that first retrieves external knowledge given the visual question and then predicts the answer based on the retrieved content. However, the retrieved knowledge is often inadequate. Retrievals are frequently too general and fail to cover specific knowledge needed to answer the question. Also, the naturally available supervision (whether the passage contains the correct answer) is weak and does not guarantee question relevancy. To address these issues, we propose an Entity-Focused Retrieval (EnFoRe) model that provides stronger supervision during training and recognizes question-relevant entities to help retrieve more specific knowledge. Experiments show that our EnFoRe model achieves superior retrieval performance on OK-VQA, the currently largest outside-knowledge VQA dataset. We also combine the retrieved knowledge with state-of-the-art VQA models, and achieve a new state-of-the-art performance on OK-VQA.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Wu, Jialin and Mooney, Raymond J.},
	month = oct,
	year = {2022},
	note = {arXiv:2210.10176 [cs]},
	keywords = {Untagged},
}

@misc{xiao_isotropy_2022,
	title = {On {Isotropy} and {Learning} {Dynamics} of {Contrastive}-based {Sentence} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2212.09170},
	abstract = {Incorporating contrastive learning objectives in sentence representation learning (SRL) has yielded significant improvements on many sentence-level NLP tasks. However, It is not well understood why contrastive learning works for learning sentence-level semantics. In this paper, we take a closer look at contrastive sentence representation learning through the lens of isotropy and learning dynamics. We interpret its success stories through the geometry of the representation shifts. We show that contrastive learning brings isotropy, and surprisingly learns to converge tokens to similar positions in the semantic space if given the signal that they are in the same sentence. Also, what we formalize as "spurious contextualization" is mitigated for semantically meaningful tokens, while augmented for functional ones. The embedding space is pushed toward the origin during training, with more areas now better defined. We ablate these findings by observing the learning dynamic with different training temperatures, batch sizes and pooling methods. With these findings, we aim to shed light on future designs of sentence representation learning methods.},
	urldate = {2023-04-16},
	publisher = {arXiv},
	author = {Xiao, Chenghao and Long, Yang and Moubayed, Noura Al},
	month = dec,
	year = {2022},
	note = {arXiv:2212.09170 [cs]},
	keywords = {Untagged},
}

@inproceedings{xie_unifiedskg_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{UnifiedSKG}: {Unifying} and {Multi}-{Tasking} {Structured} {Knowledge} {Grounding} with {Text}-to-{Text} {Language} {Models}},
	shorttitle = {{UnifiedSKG}},
	url = {https://aclanthology.org/2022.emnlp-main.39},
	abstract = {Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UnifiedSKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UnifiedSKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UnifiedSKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg.},
	urldate = {2023-04-13},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Xie, Tianbao and Wu, Chen Henry and Shi, Peng and Zhong, Ruiqi and Scholak, Torsten and Yasunaga, Michihiro and Wu, Chien-Sheng and Zhong, Ming and Yin, Pengcheng and Wang, Sida I. and Zhong, Victor and Wang, Bailin and Li, Chengzu and Boyle, Connor and Ni, Ansong and Yao, Ziyu and Radev, Dragomir and Xiong, Caiming and Kong, Lingpeng and Zhang, Rui and Smith, Noah A. and Zettlemoyer, Luke and Yu, Tao},
	month = dec,
	year = {2022},
	keywords = {Untagged},
	pages = {602--631},
}

@inproceedings{yasarla_art-ss_2022,
	title = {{ART}-{SS}: {An} {Adaptive} {Rejection} {Technique} for {Semi}-{Supervised} restoration for adverse weather-affected images},
	shorttitle = {{ART}-{SS}},
	url = {http://arxiv.org/abs/2203.09275},
	abstract = {In recent years, convolutional neural network-based single image adverse weather removal methods have achieved significant performance improvements on many benchmark datasets. However, these methods require large amounts of clean-weather degraded image pairs for training, which is often difficult to obtain in practice. Although various weather degradation synthesis methods exist in the literature, the use of synthetically generated weather degraded images often results in sub-optimal performance on the real weather degraded images due to the domain gap between synthetic and real-world images. To deal with this problem, various semi-supervised restoration (SSR) methods have been proposed for deraining or dehazing which learn to restore the clean image using synthetically generated datasets while generalizing better using unlabeled real-world images. The performance of a semi-supervised method is essentially based on the quality of the unlabeled data. In particular, if the unlabeled data characteristics are very different from that of the labeled data, then the performance of a semi-supervised method degrades significantly. We theoretically study the effect of unlabeled data on the performance of an SSR method and develop a technique that rejects the unlabeled images that degrade the performance. Extensive experiments and ablation study show that the proposed sample rejection method increases the performance of existing SSR deraining and dehazing methods significantly. Code is available at :https://github.com/rajeevyasarla/ART-SS},
	urldate = {2022-10-27},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Yasarla, Rajeev and Priebe, Carey E. and Patel, Vishal},
	month = mar,
	year = {2022},
	note = {arXiv:2203.09275 [cs]},
	keywords = {Untagged},
}

@article{xiongUnifiedFrameworkMultimodal2022,
	title = {A unified framework for multi-modal federated learning},
	volume = {480},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231222000820},
	doi = {10.1016/j.neucom.2022.01.063},
	abstract = {Federated Learning (FL) is a machine learning setting that separates data and protects user privacy. Clients learn global models together without data interaction. However, due to the lack of high-quality labeled data collected from the real world, most of the existing FL methods still rely on single-modal data. In this paper, we consider a new problem of multimodal federated learning. Although multimodal data always beneﬁts from the complementarity of different modalities, it is difﬁcult to solve the multimodal FL problem with traditional FL methods due to the modality discrepancy. Therefore, we propose a uniﬁed framework to solve it. In our framework, we use the co-attention mechanism to fuse the complementary information of different modalities. Our enhanced FL algorithm can learn useful global features of different modalities to jointly train common models for all clients. In addition, we use a personalization method based on Model-Agnostic Meta-Learning(MAML) to adapt the ﬁnal model for each client. Extensive experimental results on multimodal activity recognition tasks demonstrate the effectiveness of the proposed method.},
	language = {en},
	urldate = {2022-03-08},
	journal = {Neurocomputing},
	author = {Xiong, Baochen and Yang, Xiaoshan and Qi, Fan and Xu, Changsheng},
	month = apr,
	year = {2022},
	keywords = {Untagged},
	pages = {110--118},
}

@inproceedings{yin_sensitivity_2022,
	address = {Dublin, Ireland},
	title = {On the {Sensitivity} and {Stability} of {Model} {Interpretations} in {NLP}},
	url = {https://aclanthology.org/2022.acl-long.188},
	doi = {10.18653/v1/2022.acl-long.188},
	abstract = {Recent years have witnessed the emergence of a variety of post-hoc interpretations that aim to uncover how natural language processing (NLP) models make predictions. Despite the surge of new interpretation methods, it remains an open problem how to define and quantitatively measure the faithfulness of interpretations, i.e., to what extent interpretations reflect the reasoning process by a model. We propose two new criteria, sensitivity and stability, that provide complementary notions of faithfulness to the existed removal-based criteria. Our results show that the conclusion for how faithful interpretations are could vary substantially based on different notions. Motivated by the desiderata of sensitivity and stability, we introduce a new class of interpretation methods that adopt techniques from adversarial robustness. Empirical results show that our proposed methods are effective under the new criteria and overcome limitations of gradient-based methods on removal-based criteria. Besides text classification, we also apply interpretation methods and metrics to dependency parsing. Our results shed light on understanding the diverse set of interpretations.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Yin, Fan and Shi, Zhouxing and Hsieh, Cho-Jui and Chang, Kai-Wei},
	month = may,
	year = {2022},
	keywords = {Untagged},
	pages = {2631--2647},
}

@misc{yin_proposalcontrast_2022,
	title = {{ProposalContrast}: {Unsupervised} {Pre}-training for {LiDAR}-based {3D} {Object} {Detection}},
	shorttitle = {{ProposalContrast}},
	url = {http://arxiv.org/abs/2207.12654},
	doi = {10.48550/arXiv.2207.12654},
	abstract = {Existing approaches for unsupervised point cloud pre-training are constrained to either scene-level or point/voxel-level instance discrimination. Scene-level methods tend to lose local details that are crucial for recognizing the road objects, while point/voxel-level methods inherently suffer from limited receptive field that is incapable of perceiving large objects or context environments. Considering region-level representations are more suitable for 3D object detection, we devise a new unsupervised point cloud pre-training framework, called ProposalContrast, that learns robust 3D representations by contrasting region proposals. Specifically, with an exhaustive set of region proposals sampled from each point cloud, geometric point relations within each proposal are modeled for creating expressive proposal representations. To better accommodate 3D detection properties, ProposalContrast optimizes with both inter-cluster and inter-proposal separation, i.e., sharpening the discriminativeness of proposal representations across semantic classes and object instances. The generalizability and transferability of ProposalContrast are verified on various 3D detectors (i.e., PV-RCNN, CenterPoint, PointPillars and PointRCNN) and datasets (i.e., KITTI, Waymo and ONCE).},
	urldate = {2022-10-06},
	publisher = {arXiv},
	author = {Yin, Junbo and Zhou, Dingfu and Zhang, Liangjun and Fang, Jin and Xu, Cheng-Zhong and Shen, Jianbing and Wang, Wenguan},
	month = sep,
	year = {2022},
	note = {arXiv:2207.12654 [cs]},
	keywords = {Untagged},
}

@article{yePrivacyAuditingMachine2022,
	title = {Privacy {Auditing} of {Machine} {Learning} using {Membership} {Inference} {Attacks}},
	url = {https://openreview.net/forum?id=EG5Pgd7-MY},
	abstract = {Membership inference attacks determine if a given data point is used for training a target model. Thus, this attack could be used as an auditing tool to quantify the private information that a...},
	language = {en},
	urldate = {2022-03-30},
	journal = {ICLR Rejection},
	author = {Ye, Jiayuan and Maddi, Aadyaa and Murakonda, Sasi Kumar and Shokri, Reza},
	year = {2022},
	keywords = {Untagged},
}

@inproceedings{yu_rare_2022,
	address = {Dublin, Ireland},
	title = {Rare {Tokens} {Degenerate} {All} {Tokens}: {Improving} {Neural} {Text} {Generation} via {Adaptive} {Gradient} {Gating} for {Rare} {Token} {Embeddings}},
	shorttitle = {Rare {Tokens} {Degenerate} {All} {Tokens}},
	url = {https://aclanthology.org/2022.acl-long.3},
	doi = {10.18653/v1/2022.acl-long.3},
	abstract = {Recent studies have determined that the learned token embeddings of large-scale neural language models are degenerated to be anisotropic with a narrow-cone shape. This phenomenon, called the representation degeneration problem, facilitates an increase in the overall similarity between token embeddings that negatively affect the performance of the models. Although the existing methods that address the degeneration problem based on observations of the phenomenon triggered by the problem improves the performance of the text generation, the training dynamics of token embeddings behind the degeneration problem are still not explored. In this study, we analyze the training dynamics of the token embeddings focusing on rare token embedding. We demonstrate that the specific part of the gradient for rare token embeddings is the key cause of the degeneration problem for all tokens during training stage. Based on the analysis, we propose a novel method called, adaptive gradient gating(AGG). AGG addresses the degeneration problem by gating the specific part of the gradient for rare token embeddings. Experimental results from language modeling, word similarity, and machine translation tasks quantitatively and qualitatively verify the effectiveness of AGG.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Yu, Sangwon and Song, Jongyoon and Kim, Heeseung and Lee, Seongmin and Ryu, Woo-Jong and Yoon, Sungroh},
	month = may,
	year = {2022},
	keywords = {Untagged},
	pages = {29--45},
}

@inproceedings{zhang_fine-tuning_2022,
	address = {Seattle, United States},
	title = {Fine-tuning {Pre}-trained {Language} {Models} for {Few}-shot {Intent} {Detection}: {Supervised} {Pre}-training and {Isotropization}},
	shorttitle = {Fine-tuning {Pre}-trained {Language} {Models} for {Few}-shot {Intent} {Detection}},
	url = {https://aclanthology.org/2022.naacl-main.39},
	doi = {10.18653/v1/2022.naacl-main.39},
	abstract = {It is challenging to train a good intent classifier for a task-oriented dialogue system with only a few annotations. Recent studies have shown that fine-tuning pre-trained language models with a small set of labeled utterances from public benchmarks in a supervised manner is extremely helpful. However, we find that supervised pre-training yields an anisotropic feature space, which may suppress the expressive power of the semantic representations. Inspired by recent research in isotropization, we propose to improve supervised pre-training by regularizing the feature space towards isotropy. We propose two regularizers based on contrastive learning and correlation matrix respectively, and demonstrate their effectiveness through extensive experiments. Our main finding is that it is promising to regularize supervised pre-training with isotropization to further improve the performance of few-shot intent detection. The source code can be found at https://github.com/fanolabs/isoIntentBert-main.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Haode and Liang, Haowen and Zhang, Yuwei and Zhan, Li-Ming and Wu, Xiao-Ming and Lu, Xiaolei and Lam, Albert},
	month = jul,
	year = {2022},
	keywords = {Untagged},
	pages = {532--542},
}

@misc{yu_coca_2022,
	title = {{CoCa}: {Contrastive} {Captioners} are {Image}-{Text} {Foundation} {Models}},
	shorttitle = {{CoCa}},
	url = {http://arxiv.org/abs/2205.01917},
	doi = {10.48550/arXiv.2205.01917},
	abstract = {Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3\% zero-shot top-1 accuracy, 90.6\% with a frozen encoder and learned classification head, and new state-of-the-art 91.0\% top-1 accuracy on ImageNet with a finetuned encoder.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
	month = jun,
	year = {2022},
	note = {arXiv:2205.01917 [cs]},
	keywords = {Untagged},
}

@inproceedings{zhang_multi-lingual_2022,
	title = {Multi-{Lingual} {Acquisition} on {Multimodal} {Pre}-training for {Cross}-modal {Retrieval}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/bfadef437ed27372648714c930c3a77a-Abstract-Conference.html},
	language = {en},
	urldate = {2023-05-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zhang, Liang and Hu, Anwen and Jin, Qin},
	month = dec,
	year = {2022},
	keywords = {Untagged},
	pages = {29691--29704},
}

@article{zhang_multiple_2022,
	title = {Multiple {Adverse} {Weather} {Conditions} {Adaptation} for {Object} {Detection} via {Causal} {Intervention}},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2022.3166765},
	abstract = {Most state-of-the-art object detection methods have achieved impressive perfomrace on several public benchmarks, which are trained with high definition images. However, existing detectors are often sensitive to the visual variations and out-of-distribution data due to the domain gap caused by various confounders, e.g. the adverse weathre conditions. To bridge the gap, previous methods have been mainly exploring domain alignment, which requires to collect an amount of domain-specific training samples. In this paper, we introduce a novel domain adaptation model to discover a weather condition invariant feature representation. Specifically, we first employ a memory network to develop a confounder dictionary, which stores prototypes of object features under various scenarios. To guarantee the representativeness of each prototype in the dictionary, a dynamic item extraction strategy is used to update the memory dictionary. After that, we introduce a causal intervention reasoning module to explore the invariant representation of a specific object under different weather conditions. Finally, a categorical consistency regularization is used to constrain the similarities between categories in order to automatically search for the aligned instances among distinct domains. Experiments are conducted on several public benchmarks (RTTS, Foggy-Cityscapes, RID, and BDD 100K) with state-of-the-art performance achieved under multiple weather conditions.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhang, Hua and Xiao, Liqiang and Cao, Xiaochun and Foroosh, Hassan},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Untagged},
	pages = {1--1},
}

@article{zhangFinetuningGlobalModel2022,
	title = {Fine-tuning {Global} {Model} via {Data}-{Free} {Knowledge} {Distillation} for {Non}-{IID} {Federated} {Learning}},
	url = {http://arxiv.org/abs/2203.09249},
	abstract = {Federated Learning (FL) is an emerging distributed learning paradigm under privacy constraint. Data heterogeneity is one of the main challenges in FL, which results in slow convergence and degraded performance. Most existing approaches only tackle the heterogeneity challenge by restricting the local model update in client, ignoring the performance drop caused by direct global model aggregation. Instead, we propose a data-free knowledge distillation method to fine-tune the global model in the server (FedFTG), which relieves the issue of direct model aggregation. Concretely, FedFTG explores the input space of local models through a generator, and uses it to transfer the knowledge from local models to the global model. Besides, we propose a hard sample mining scheme to achieve effective knowledge distillation throughout the training. In addition, we develop customized label sampling and class-level ensemble to derive maximum utilization of knowledge, which implicitly mitigates the distribution discrepancy across clients. Extensive experiments show that our FedFTG significantly outperforms the state-of-the-art (SOTA) FL algorithms and can serve as a strong plugin for enhancing FedAvg, FedProx, FedDyn, and SCAFFOLD.},
	urldate = {2022-04-28},
	journal = {arXiv:2203.09249 [cs]},
	author = {Zhang, Lin and Shen, Li and Ding, Liang and Tao, Dacheng and Duan, Ling-Yu},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.09249},
	keywords = {Untagged},
}

@techreport{zhangAcceleratingTrainingNeural2022,
	title = {Accelerating {Training} of {Neural} {Networks}},
	language = {en},
	institution = {Huawei},
	author = {Zhang, Hongyang},
	year = {2022},
	keywords = {Untagged},
	pages = {6},
}

@inproceedings{zhang_pointclip_2022,
	address = {New Orleans, LA, USA},
	title = {{PointCLIP}: {Point} {Cloud} {Understanding} by {CLIP}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{PointCLIP}},
	url = {https://ieeexplore.ieee.org/document/9878980/},
	doi = {10.1109/CVPR52688.2022.00836},
	abstract = {Recently, zero-shot and few-shot learning via Contrastive Vision-Language Pre-training (CLIP) have shown inspirational performance on 2D visual recognition, which learns to match images with their corresponding texts in openvocabulary settings. However, it remains under explored that whether CLIP, pre-trained by large-scale image-text pairs in 2D, can be generalized to 3D recognition. In this paper, we identify such a setting is feasible by proposing PointCLIP, which conducts alignment between CLIPencoded point clouds and 3D category texts. Specifically, we encode a point cloud by projecting it onto multi-view depth maps and aggregate the view-wise zero-shot prediction in an end-to-end manner, which achieves efficient knowledge transfer from 2D to 3D. We further design an inter-view adapter to better extract the global feature and adaptively fuse the 3D few-shot knowledge into CLIP pre-trained in 2D. By just fine-tuning the adapter under few-shot settings, the performance of PointCLIP could be largely improved. In addition, we observe the knowledge complementary property between PointCLIP and classical 3D-supervised networks. Via simple ensemble during inference, PointCLIP contributes to favorable performance enhancement over state-of-the-art 3D networks. Therefore, PointCLIP is a promising alternative for effective 3D point cloud understanding under low data regime with marginal resource cost. We conduct thorough experiments on ModelNet10, ModelNet40 and ScanObjectNN to demonstrate the effectiveness of PointCLIP. Code is available at https: //github.com/ZrrSkywalker/PointCLIP.},
	language = {en},
	urldate = {2023-06-01},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhang, Renrui and Guo, Ziyu and Zhang, Wei and Li, Kunchang and Miao, Xupeng and Cui, Bin and Qiao, Yu and Gao, Peng and Li, Hongsheng},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {8542--8552},
}

@inproceedings{zhang_complicate_2022,
	address = {Gyeongju, Republic of Korea},
	title = {Complicate {Then} {Simplify}: {A} {Novel} {Way} to {Explore} {Pre}-trained {Models} for {Text} {Classification}},
	shorttitle = {Complicate {Then} {Simplify}},
	url = {https://aclanthology.org/2022.coling-1.97},
	abstract = {With the development of pre-trained models (PTMs), the performance of text classification has been continuously improved by directly employing the features generated by PTMs. However such way might not fully explore the knowledge in PTMs as it is constrained by the difficulty of the task. Compared to difficult task, the learning algorithms tend to saturate early on the simple task. Moreover, the native sentence representations derived from BERT are prone to be collapsed and directly employing such representation for text classification might fail to fully capture discriminative features. In order to address these issues, in this paper we propose a novel framework for text classification which implements a two-stage training strategy. In the pre-training stage, auxiliary labels are introduced to increase the task difficulties and to fully exploit the knowledge in the pre-trained model. In the fine-tuning stage, the textual representation learned in the pre-training stage is employed and the classifier is fine-tuned to obtain better classification performance. Experiments were conducted on six text classification corpora and the results showed that the proposed framework outperformed several state-of-the-art baselines.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Zhang, Xu and Liu, Zejie and Xiang, Yanzheng and Zhou, Deyu},
	month = oct,
	year = {2022},
	keywords = {Untagged},
	pages = {1136--1145},
}

@inproceedings{avidan_exploiting_2022,
	address = {Cham},
	title = {Exploiting {Unlabeled} {Data} with {Vision} and {Language} {Models} for {Object} {Detection}},
	volume = {13669},
	isbn = {978-3-031-20076-2 978-3-031-20077-9},
	url = {https://link.springer.com/10.1007/978-3-031-20077-9_10},
	abstract = {Building robust and generic object detection frameworks requires scaling to larger label spaces and bigger training datasets. However, it is prohibitively costly to acquire annotations for thousands of categories at a large scale. We propose a novel method that leverages the rich semantics available in recent vision and language models to localize and classify objects in unlabeled images, effectively generating pseudo labels for object detection. Starting with a generic and class-agnostic region proposal mechanism, we use vision and language models to categorize each region of an image into any object category that is required for downstream tasks. We demonstrate the value of the generated pseudo labels in two specific tasks, open-vocabulary detection, where a model needs to generalize to unseen object categories, and semi-supervised object detection, where additional unlabeled images can be used to improve the model. Our empirical evaluation shows the effectiveness of the pseudo labels in both tasks, where we outperform competitive baselines and achieve a novel state-of-the-art for open-vocabulary object detection. Our code is available at https://github.com/xiaofeng94/VL-PLM.},
	language = {en},
	urldate = {2023-01-30},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Zhao, Shiyu and Zhang, Zhixing and Schulter, Samuel and Zhao, Long and Vijay Kumar, B.G and Stathopoulos, Anastasis and Chandraker, Manmohan and Metaxas, Dimitris N.},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-20077-9_10},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Untagged},
	pages = {159--175},
}

@inproceedings{zhong_regionclip_2022,
	address = {New Orleans, LA, USA},
	title = {{RegionCLIP}: {Region}-based {Language}-{Image} {Pretraining}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{RegionCLIP}},
	url = {https://ieeexplore.ieee.org/document/9878561/},
	doi = {10.1109/CVPR52688.2022.01629},
	abstract = {Contrastive language-image pretraining (CLIP) using image-text pairs has achieved impressive results on image classification in both zero-shot and transfer learning settings. However, we show that directly applying such models to recognize image regions for object detection leads to unsatisfactory performance due to a major domain shift: CLIP was trained to match an image as a whole to a text description, without capturing the fine-grained alignment between image regions and text spans. To mitigate this issue, we propose a new method called RegionCLIP that significantly extends CLIP to learn region-level visual representations, thus enabling fine-grained alignment between image regions and textual concepts. Our method leverages a CLIP model to match image regions with template captions, and then pretrains our model to align these region-text pairs in the feature space. When transferring our pretrained model to the open-vocabulary object detection task, our method outperforms the state of the art by 3.8 AP50 and 2.2 AP for novel categories on COCO and LVIS datasets, respectively. Further, the learned region representations support zero-shot inference for object detection, showing promising results on both COCO and LVIS datasets. Our code is available at https://github.com/microsoft/RegionCLIP.},
	language = {en},
	urldate = {2023-01-30},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhong, Yiwu and Yang, Jianwei and Zhang, Pengchuan and Li, Chunyuan and Codella, Noel and Li, Liunian Harold and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Li, Yin and Gao, Jianfeng},
	month = jun,
	year = {2022},
	keywords = {Untagged},
	pages = {16772--16782},
}

@inproceedings{zhao_centerclip_2022,
	title = {{CenterCLIP}: {Token} {Clustering} for {Efficient} {Text}-{Video} {Retrieval}},
	shorttitle = {{CenterCLIP}},
	url = {http://arxiv.org/abs/2205.00823},
	doi = {10.1145/3477495.3531950},
	abstract = {Recently, large-scale pre-training methods like CLIP have made great progress in multi-modal research such as text-video retrieval. In CLIP, transformers are vital for modeling complex multi-modal relations. However, in the vision transformer of CLIP, the essential visual tokenization process, which produces discrete visual token sequences, generates many homogeneous tokens due to the redundancy nature of consecutive and similar frames in videos. This significantly increases computation costs and hinders the deployment of video retrieval models in web applications. In this paper, to reduce the number of redundant video tokens, we design a multi-segment token clustering algorithm to find the most representative tokens and drop the non-essential ones. As the frame redundancy occurs mostly in consecutive frames, we divide videos into multiple segments and conduct segment-level clustering. Center tokens from each segment are later concatenated into a new sequence, while their original spatial-temporal relations are well maintained. We instantiate two clustering algorithms to efficiently find deterministic medoids and iteratively partition groups in high dimensional space. Through this token clustering and center selection procedure, we successfully reduce computation costs by removing redundant visual tokens. This method further enhances segment-level semantic alignment between video and text representations, enforcing the spatio-temporal interactions of tokens from within-segment frames. Our method, coined as CenterCLIP, surpasses existing state-of-the-art by a large margin on typical text-video benchmarks, while reducing the training memory cost by 35{\textbackslash}\% and accelerating the inference speed by 14{\textbackslash}\% at the best case. The code is available at {\textbackslash}href\{\{https://github.com/mzhaoshuai/CenterCLIP\}\}\{\{https://github.com/mzhaoshuai/CenterCLIP\}\}.},
	urldate = {2022-10-04},
	booktitle = {International {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	author = {Zhao, Shuai and Zhu, Linchao and Wang, Xiaohan and Yang, Yi},
	year = {2022},
	note = {arXiv:2205.00823 [cs]},
	keywords = {Untagged},
	pages = {970--981},
}

@inproceedings{zhaoMultimodalFederatedLearning2022,
	address = {Milan, Italy},
	title = {Multimodal {Federated} {Learning} on {IoT} {Data}},
	url = {http://arxiv.org/abs/2109.04833},
	abstract = {Federated learning is proposed as an alternative to centralized machine learning since its client-server structure provides better privacy protection and scalability in real-world applications. In many applications, such as smart homes with IoT devices, local data on clients are generated from different modalities such as sensory, visual, and audio data. Existing federated learning systems only work on local data from a single modality, which limits the scalability of the systems.},
	language = {en},
	urldate = {2021-11-16},
	booktitle = {{IoTDI}},
	author = {Zhao, Yuchen and Barnaghi, Payam and Haddadi, Hamed},
	month = may,
	year = {2022},
	note = {arXiv: 2109.04833},
	keywords = {Untagged},
}

@misc{zhou_non-contrastive_2022,
	title = {Non-{Contrastive} {Learning} {Meets} {Language}-{Image} {Pre}-{Training}},
	url = {http://arxiv.org/abs/2210.09304},
	abstract = {Contrastive language-image pre-training (CLIP) serves as a de-facto standard to align images and texts. Nonetheless, the loose correlation between images and texts of webcrawled data renders the contrastive objective data inefﬁcient and craving for a large training batch size. In this work, we explore the validity of non-contrastive languageimage pre-training (nCLIP), and study whether nice properties exhibited in visual self-supervised models can emerge. We empirically observe that the non-contrastive objective nourishes representation learning while sufﬁciently underperforming under zero-shot recognition. Based on the above study, we further introduce xCLIP, a multi-tasking framework combining CLIP and nCLIP, and show that nCLIP aids CLIP in enhancing feature semantics. The synergy between two objectives lets xCLIP enjoy the best of both worlds: superior performance in both zero-shot transfer and representation learning. Systematic evaluation is conducted spanning a wide variety of downstream tasks including zero-shot classiﬁcation, out-of-domain classiﬁcation, retrieval, visual representation learning, and textual representation learning, showcasing a consistent performance gain and validating the effectiveness of xCLIP.},
	language = {en},
	urldate = {2023-06-02},
	publisher = {arXiv},
	author = {Zhou, Jinghao and Dong, Li and Gan, Zhe and Wang, Lijuan and Wei, Furu},
	month = oct,
	year = {2022},
	note = {arXiv:2210.09304 [cs]},
	keywords = {Untagged},
}

@misc{azaria_internal_2023,
	title = {The {Internal} {State} of an {LLM} {Knows} {When} its {Lying}},
	url = {http://arxiv.org/abs/2304.13734},
	abstract = {While Large Language Models (LLMs) have shown exceptional performance in various tasks, their (arguably) most prominent drawback is generating inaccurate or false information with a confident tone. In this paper, we hypothesize that the LLM's internal state can be used to reveal the truthfulness of a statement. Therefore, we introduce a simple yet effective method to detect the truthfulness of LLM-generated statements, which utilizes the LLM's hidden layer activations to determine the veracity of statements. To train and evaluate our method, we compose a dataset of true and false statements in six different topics. A classifier is trained to detect which statement is true or false based on an LLM's activation values. Specifically, the classifier receives as input the activation values from the LLM for each of the statements in the dataset. Our experiments demonstrate that our method for detecting statement veracity significantly outperforms even few-shot prompting methods, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.},
	urldate = {2023-05-03},
	publisher = {arXiv},
	author = {Azaria, Amos and Mitchell, Tom},
	month = apr,
	year = {2023},
	note = {arXiv:2304.13734 [cs]},
	keywords = {Untagged},
}

@article{boiko_emergent_2023,
	title = {Emergent autonomous scientific research capabilities of large language models},
	abstract = {Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent’s scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.},
	language = {en},
	author = {Boiko, Daniil A and MacKnight, Robert and Gomes, Gabe},
	year = {2023},
	keywords = {Untagged},
}

@misc{bran_chemcrow_2023,
	title = {{ChemCrow}: {Augmenting} large-language models with chemistry tools},
	shorttitle = {{ChemCrow}},
	url = {http://arxiv.org/abs/2304.05376},
	abstract = {Large-language models (LLMs) have recently shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these models lack access to external knowledge sources, limiting their usefulness in scientific applications. In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. By integrating 13 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our evaluation, including both LLM and expert human assessments, demonstrates ChemCrow's effectiveness in automating a diverse set of chemical tasks. Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and GPT-4 + ChemCrow performance. There is a significant risk of misuse of tools like ChemCrow and we discuss their potential harms. Employed responsibly, ChemCrow not only aids expert chemists and lowers barriers for non-experts, but also fosters scientific advancement by bridging the gap between experimental and computational chemistry.},
	urldate = {2023-04-23},
	publisher = {arXiv},
	author = {Bran, Andres M. and Cox, Sam and White, Andrew D. and Schwaller, Philippe},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05376 [physics, stat]},
	keywords = {Untagged},
}

@misc{chan_ic3_2023,
	title = {\${IC}{\textasciicircum}3\$: {Image} {Captioning} by {Committee} {Consensus}},
	shorttitle = {\${IC}{\textasciicircum}3\$},
	url = {http://arxiv.org/abs/2302.01328},
	abstract = {If you ask a human to describe an image, they might do so in a thousand different ways. Traditionally, image captioning models are trained to approximate the reference distribution of image captions, however, doing so encourages captions that are viewpoint-impoverished. Such captions often focus on only a subset of the possible details, while ignoring potentially useful information in the scene. In this work, we introduce a simple, yet novel, method: "Image Captioning by Committee Consensus" (\$IC{\textasciicircum}3\$), designed to generate a single caption that captures high-level details from several viewpoints. Notably, humans rate captions produced by \$IC{\textasciicircum}3\$ at least as helpful as baseline SOTA models more than two thirds of the time, and \$IC{\textasciicircum}3\$ captions can improve the performance of SOTA automated recall systems by up to 84\%, indicating significant material improvements over existing SOTA approaches for visual description. Our code is publicly available at https://github.com/DavidMChan/caption-by-committee},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Chan, David M. and Myers, Austin and Vijayanarasimhan, Sudheendra and Ross, David A. and Canny, John},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01328 [cs]},
	keywords = {Untagged},
}

@misc{chen_stair_2023,
	title = {{STAIR}: {Learning} {Sparse} {Text} and {Image} {Representation} in {Grounded} {Tokens}},
	shorttitle = {{STAIR}},
	url = {http://arxiv.org/abs/2301.13081},
	abstract = {Image and text retrieval is one of the foundational tasks in the vision and language domain with multiple real-world applications. State-of-theart approaches, e.g. CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), represent images and texts as dense embeddings and calculate the similarity in the dense embedding space as the matching score. On the other hand, sparse semantic features like bag-of-words models are more interpretable, but believed to suffer from inferior accuracy than dense representations. In this work, we show that it is possible to build a sparse semantic representation that is as powerful as, or even better than, dense presentations. We extend the CLIP model and build a sparse text and image representation (STAIR), where the image and text are mapped to a sparse token space. Each token in the space is a (sub-)word in the vocabulary, which is not only interpretable but also easy to integrate with existing information retrieval systems. STAIR model signiﬁcantly outperforms a CLIP model with +4.9\% and +4.3\% absolute Recall@1 improvement on COCO-5k text→image and image→text retrieval respectively. It also achieved better performance on both of ImageNet zero-shot and linear probing compared to CLIP.},
	language = {en},
	urldate = {2023-06-02},
	publisher = {arXiv},
	author = {Chen, Chen and Zhang, Bowen and Cao, Liangliang and Shen, Jiguang and Gunter, Tom and Jose, Albin Madappally and Toshev, Alexander and Shlens, Jonathon and Pang, Ruoming and Yang, Yinfei},
	month = feb,
	year = {2023},
	note = {arXiv:2301.13081 [cs]},
	keywords = {Untagged},
}

@misc{ge_openagi_2023,
	title = {{OpenAGI}: {When} {LLM} {Meets} {Domain} {Experts}},
	shorttitle = {{OpenAGI}},
	url = {http://arxiv.org/abs/2304.04370},
	abstract = {Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language queries, serving as input to the LLM. The LLM subsequently selects, synthesizes, and executes models provided by OpenAGI to address the task. Furthermore, we propose a Reinforcement Learning from Task Feedback (RLTF) mechanism, which uses the task-solving result as feedback to improve the LLM's task-solving ability. Thus, the LLM is responsible for synthesizing various external models for solving complex tasks, while RLTF provides feedback to improve its task-solving ability, enabling a feedback loop for self-improving AI. We believe that the paradigm of LLMs operating various expert models for complex task-solving is a promising approach towards AGI. To facilitate the community's long-term improvement and evaluation of AGI's ability, we open-source the code, benchmark, and evaluation methods of the OpenAGI project at https://github.com/agiresearch/OpenAGI.},
	urldate = {2023-04-24},
	publisher = {arXiv},
	author = {Ge, Yingqiang and Hua, Wenyue and Ji, Jianchao and Tan, Juntao and Xu, Shuyuan and Zhang, Yongfeng},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04370 [cs]},
	keywords = {Untagged},
}

@misc{dong_survey_2023,
	title = {A {Survey} on {In}-context {Learning}},
	url = {http://arxiv.org/abs/2301.00234},
	abstract = {With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.},
	urldate = {2023-04-01},
	publisher = {arXiv},
	author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Li, Lei and Sui, Zhifang},
	month = feb,
	year = {2023},
	note = {arXiv:2301.00234 [cs]},
	keywords = {Untagged},
}

@inproceedings{hao_dual_2023,
	title = {Dual {Alignment} {Unsupervised} {Domain} {Adaptation} for {Video}-{Text} {Retrieval}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Hao_Dual_Alignment_Unsupervised_Domain_Adaptation_for_Video-Text_Retrieval_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-02},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Hao, Xiaoshuai and Zhang, Wanqian and Wu, Dayan and Zhu, Fei and Li, Bo},
	year = {2023},
	keywords = {Untagged},
	pages = {18962--18972},
}

@inproceedings{guo_contranorm_2023,
	title = {{ContraNorm}: {A} {Contrastive} {Learning} {Perspective} on {Oversmoothing} and {Beyond}},
	shorttitle = {{ContraNorm}},
	url = {https://openreview.net/forum?id=SM7XkJouWHm},
	abstract = {Oversmoothing is a common phenomenon in a wide range of Graph Neural Networks (GNNs) and Transformers, where performance degenerates as the layer goes deeper. Instead of characterizing oversmoothing from the view of complete collapse in which representations converge to a single point, we dive into a more general perspective dimensional collapse in which representations lie in a narrow cone. Accordingly, inspired by the power of contrastive learning in preventing dimensional collapse, we propose a novel normalization layer ContraNorm. Intuitively, ContraNorm implicitly shatters representations in the embedding space, leading to a more uniform distribution and slighter dimensional collapse. On the theoretical analysis, we prove that ContraNorm can alleviate both complete collapse and dimensional collapse under some conditions. Our proposed normalization layer can be easily inserted into GNNs and Transformers with negligible parameter overhead. Experiments on various real-world datasets verify the effectiveness of our method.},
	language = {en},
	urldate = {2023-04-16},
	author = {Guo, Xiaojun and Wang, Yifei and Du, Tianqi and Wang, Yisen},
	month = feb,
	year = {2023},
	keywords = {Untagged},
}

@misc{guo_images_2023,
	title = {From {Images} to {Textual} {Prompts}: {Zero}-shot {VQA} with {Frozen} {Large} {Language} {Models}},
	shorttitle = {From {Images} to {Textual} {Prompts}},
	url = {http://arxiv.org/abs/2212.10846},
	doi = {10.48550/arXiv.2212.10846},
	abstract = {Large language models (LLMs) have demonstrated excellent zero-shot generalization to new language tasks. However, effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging, primarily due to the modality disconnection and task disconnection between LLM and VQA task. End-to-end training on vision and language data may bridge the disconnections, but is inflexible and computationally expensive. To address this issue, we propose {\textbackslash}emph\{Img2Prompt\}, a plug-and-play module that provides the prompts that can bridge the aforementioned modality and task disconnections, so that LLMs can perform zero-shot VQA tasks without end-to-end training. In order to provide such prompts, we further employ LLM-agnostic models to provide prompts that can describe image content and self-constructed question-answer pairs, which can effectively guide LLM to perform zero-shot VQA tasks. Img2Prompt offers the following benefits: 1) It can flexibly work with various LLMs to perform VQA. 2){\textasciitilde}Without the needing of end-to-end training, it significantly reduces the cost of deploying LLM for zero-shot VQA tasks. 3) It achieves comparable or better performance than methods relying on end-to-end training. For example, we outperform Flamingo {\textbackslash}cite\{Deepmind:Flamingo2022\} by 5.6{\textbackslash}\% on VQAv2. On the challenging A-OKVQA dataset, our method even outperforms few-shot methods by as much as 20{\textbackslash}\%.},
	urldate = {2023-04-11},
	publisher = {arXiv},
	author = {Guo, Jiaxian and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Li, Boyang and Tao, Dacheng and Hoi, Steven C. H.},
	month = mar,
	year = {2023},
	note = {arXiv:2212.10846 [cs]},
	keywords = {Untagged},
}

@inproceedings{huang_vop_2023,
	title = {{VoP}: {Text}-{Video} {Co}-{Operative} {Prompt} {Tuning} for {Cross}-{Modal} {Retrieval}},
	shorttitle = {{VoP}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Huang_VoP_Text-Video_Co-Operative_Prompt_Tuning_for_Cross-Modal_Retrieval_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-02},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Huang, Siteng and Gong, Biao and Pan, Yulin and Jiang, Jianwen and Lv, Yiliang and Li, Yuyuan and Wang, Donglin},
	year = {2023},
	keywords = {Untagged},
	pages = {6565--6574},
}

@misc{hu_federated_2023,
	title = {Federated {Learning} {Meets} {Multi}-objective {Optimization}},
	url = {http://arxiv.org/abs/2006.11489},
	abstract = {Federated learning has emerged as a promising, massively distributed way to train a joint deep model over large amounts of edge devices while keeping private user data strictly on device. In this work, motivated from ensuring fairness among users and robustness against malicious adversaries, we formulate federated learning as multi-objective optimization and propose a new algorithm FedMGDA+ that is guaranteed to converge to Pareto stationary solutions. FedMGDA+ is simple to implement, has fewer hyperparameters to tune, and refrains from sacrificing the performance of any participating user. We establish the convergence properties of FedMGDA+ and point out its connections to existing approaches. Extensive experiments on a variety of datasets confirm that FedMGDA+ compares favorably against state-of-the-art.},
	urldate = {2023-05-11},
	publisher = {arXiv},
	author = {Hu, Zeou and Shaloudegi, Kiarash and Zhang, Guojun and Yu, Yaoliang},
	month = jan,
	year = {2023},
	note = {arXiv:2006.11489 [cs, stat]},
	keywords = {Untagged},
}

@misc{jin_genegpt_2023,
	title = {{GeneGPT}: {Teaching} {Large} {Language} {Models} to {Use} {NCBI} {Web} {APIs}},
	shorttitle = {{GeneGPT}},
	url = {http://arxiv.org/abs/2304.09667},
	abstract = {In this paper, we present GeneGPT, a novel method for teaching large language models (LLMs) to use the Web Application Programming Interfaces (APIs) of the National Center for Biotechnology Information (NCBI) and answer genomics questions. Specifically, we prompt Codex (code-davinci-002) to solve the GeneTuring tests with few-shot URL requests of NCBI API calls as demonstrations for in-context learning. During inference, we stop the decoding once a call request is detected and make the API call with the generated URL. We then append the raw execution results returned by NCBI APIs to the generated texts and continue the generation until the answer is found or another API call is detected. Our preliminary results show that GeneGPT achieves state-of-the-art results on three out of four one-shot tasks and four out of five zero-shot tasks in the GeneTuring dataset. Overall, GeneGPT achieves a macro-average score of 0.76, which is much higher than retrieval-augmented LLMs such as the New Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as well as other LLMs such as GPT-3 (0.16) and ChatGPT (0.12).},
	urldate = {2023-04-24},
	publisher = {arXiv},
	author = {Jin, Qiao and Yang, Yifan and Chen, Qingyu and Lu, Zhiyong},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09667 [cs, q-bio]},
	keywords = {Untagged},
}

@inproceedings{li_rethinking_2023,
	title = {Rethinking {Out}-of-{Distribution} ({OOD}) {Detection}: {Masked} {Image} {Modeling} {Is} {All} {You} {Need}},
	shorttitle = {Rethinking {Out}-of-{Distribution} ({OOD}) {Detection}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Li_Rethinking_Out-of-Distribution_OOD_Detection_Masked_Image_Modeling_Is_All_You_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-02},
	author = {Li, Jingyao and Chen, Pengguang and He, Zexin and Yu, Shaozuo and Liu, Shu and Jia, Jiaya},
	year = {2023},
	keywords = {Untagged},
	pages = {11578--11589},
}

@misc{lei_conditional_2023,
	title = {Conditional {Adapters}: {Parameter}-efficient {Transfer} {Learning} with {Fast} {Inference}},
	shorttitle = {Conditional {Adapters}},
	url = {http://arxiv.org/abs/2304.04947},
	abstract = {We propose Conditional Adapter (CoDA), a parameter-efficient transfer learning method that also improves inference efficiency. CoDA generalizes beyond standard adapter approaches to enable a new way of balancing speed and accuracy using conditional computation. Starting with an existing dense pretrained model, CoDA adds sparse activation together with a small number of new parameters and a light-weight training phase. Our experiments demonstrate that the CoDA approach provides an unexpectedly efficient way to transfer knowledge. Across a variety of language, vision, and speech tasks, CoDA achieves a 2x to 8x inference speed-up compared to the state-of-the-art Adapter approach with moderate to no accuracy loss and the same parameter efficiency.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Lei, Tao and Bai, Junwen and Brahma, Siddhartha and Ainslie, Joshua and Lee, Kenton and Zhou, Yanqi and Du, Nan and Zhao, Vincent Y. and Wu, Yuexin and Li, Bo and Zhang, Yu and Chang, Ming-Wei},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04947 [cs]},
	keywords = {Untagged},
}

@misc{kooshiar_kooshiar_end--end_2023,
	type = {Tweet},
	title = {End-to-end generative {AI} for protein design by http://310.ai \#{GenerativeAI} \#{ChatGPT} \#ai https://t.co/{wiGwVts7yo}},
	url = {https://twitter.com/kooshiar/status/1628121444457803778},
	language = {en},
	urldate = {2023-04-08},
	journal = {Twitter},
	author = {{kooshiar [@kooshiar]}},
	month = feb,
	year = {2023},
	keywords = {Untagged},
}

@misc{li_blip-2_2023,
	title = {{BLIP}-2: {Bootstrapping} {Language}-{Image} {Pre}-training with {Frozen} {Image} {Encoders} and {Large} {Language} {Models}},
	shorttitle = {{BLIP}-2},
	url = {http://arxiv.org/abs/2301.12597},
	doi = {10.48550/arXiv.2301.12597},
	abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
	month = jan,
	year = {2023},
	note = {arXiv:2301.12597 [cs]},
	keywords = {Untagged},
}

@inproceedings{li_decap_2023,
	title = {{DeCap}: {Decoding} {CLIP} {Latents} for {Zero}-{Shot} {Captioning} via {Text}-{Only} {Training}},
	shorttitle = {{DeCap}},
	url = {https://openreview.net/forum?id=Lt8bMlhiwx2},
	abstract = {Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong zero-shot transfer capability in many discriminative tasks, e.g., image classification. Their adaptation to zero-shot image-conditioned text generation tasks has drawn increasing interest. Prior arts approach to zero-shot captioning by either utilizing the existing large language models (e.g., GPT-2) or pre-training the encoder-decoder network in an end-to-end manner. However, the large language models may not generate sensible descriptions due to the task discrepancy between captioning and language modeling, while the end-to-end pre-training requires paired data and extensive computational resources. In this work, we propose a simple framework, named DeCap, for zero-shot captioning. We introduce a lightweight visual-aware language decoder. This decoder is both data-efficient and computation-efficient: 1) it only requires the {\textbackslash}textit\{text\} data for training, easing the burden on the collection of paired data. 2) it does not require end-to-end training. When trained with text-only data, the decoder takes the text embedding extracted from the off-the-shelf CLIP encoder as a prefix embedding. The challenge is that the decoder is trained on the text corpus but at the inference stage, it needs to generate captions based on visual inputs. Though the CLIP text embedding and the visual embedding are correlated, the {\textbackslash}textit\{modality gap\} issue is widely observed in multi-modal contrastive models that prevents us from directly taking the visual embedding as the prefix embedding. We propose a training-free mechanism to reduce the modality gap. We project the visual embedding into the CLIP text embedding space, while the projected embedding retains the information of the visual input. Taking the projected embedding as the prefix embedding, the decoder generates high-quality descriptions that match the visual input. The experiments show that DeCap outperforms other zero-shot captioning methods and unpaired captioning methods by a large margin on the typical image captioning benchmarks, i.e., MSCOCO and NoCaps. We apply DeCap to video captioning and achieve state-of-the-art zero-shot performance on MSR-VTT and ActivityNet-Captions. The code is available at https://github.com/dhg-wei/DeCap.},
	language = {en},
	urldate = {2023-06-02},
	author = {Li, Wei and Zhu, Linchao and Wen, Longyin and Yang, Yi},
	month = feb,
	year = {2023},
	keywords = {Untagged},
}

@misc{li_scaling_2023,
	title = {Scaling {Language}-{Image} {Pre}-training via {Masking}},
	url = {http://arxiv.org/abs/2212.00794},
	abstract = {We present Fast Language-Image Pre-training (FLIP), a simple and more efficient method for training CLIP. Our method randomly masks out and removes a large portion of image patches during training. Masking allows us to learn from more image-text pairs given the same wall-clock time and contrast more samples per iteration with similar memory footprint. It leads to a favorable trade-off between accuracy and training time. In our experiments on 400 million image-text pairs, FLIP improves both accuracy and speed over the no-masking baseline. On a large diversity of downstream tasks, FLIP dominantly outperforms the CLIP counterparts trained on the same data. Facilitated by the speedup, we explore the scaling behavior of increasing the model size, data size, or training length, and report encouraging results and comparisons. We hope that our work will foster future research on scaling vision-language learning.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Li, Yanghao and Fan, Haoqi and Hu, Ronghang and Feichtenhofer, Christoph and He, Kaiming},
	month = mar,
	year = {2023},
	note = {arXiv:2212.00794 [cs]},
	keywords = {Untagged},
}

@misc{li_cancergpt_2023,
	title = {{CancerGPT}: {Few}-shot {Drug} {Pair} {Synergy} {Prediction} using {Large} {Pre}-trained {Language} {Models}},
	shorttitle = {{CancerGPT}},
	url = {http://arxiv.org/abs/2304.10946},
	abstract = {Large pre-trained language models (LLMs) have been shown to have significant potential in few-shot learning across various fields, even with minimal training data. However, their ability to generalize to unseen tasks in more complex fields, such as biology, has yet to be fully evaluated. LLMs can offer a promising alternative approach for biological inference, particularly in cases where structured data and sample size are limited, by extracting prior knowledge from text corpora. Our proposed few-shot learning approach uses LLMs to predict the synergy of drug pairs in rare tissues that lack structured data and features. Our experiments, which involved seven rare tissues from different cancer types, demonstrated that the LLM-based prediction model achieved significant accuracy with very few or zero samples. Our proposed model, the CancerGPT (with \${\textbackslash}sim\$ 124M parameters), was even comparable to the larger fine-tuned GPT-3 model (with \${\textbackslash}sim\$ 175B parameters). Our research is the first to tackle drug pair synergy prediction in rare tissues with limited data. We are also the first to utilize an LLM-based prediction model for biological reaction prediction tasks.},
	urldate = {2023-04-24},
	publisher = {arXiv},
	author = {Li, Tianhao and Shetty, Sandesh and Kamath, Advaith and Jaiswal, Ajay and Jiang, Xianqian and Ding, Ying and Kim, Yejin},
	month = apr,
	year = {2023},
	note = {arXiv:2304.10946 [cs, q-bio]},
	keywords = {Untagged},
}

@misc{lievin_can_2023,
	title = {Can large language models reason about medical questions?},
	url = {http://arxiv.org/abs/2207.08143},
	abstract = {Although large language models (LLMs) often produce impressive outputs, it remains unclear how they perform in real-world scenarios requiring strong reasoning skills and expert domain knowledge. We set out to investigate whether GPT-3.5 (Codex and InstructGPT) can be applied to answer and reason about difficult real-world-based questions. We utilize two multiple-choice medical exam questions (USMLE and MedMCQA) and a medical reading comprehension dataset (PubMedQA). We investigate multiple prompting scenarios: Chain-of-Thought (CoT, think step-by-step), zero- and few-shot (prepending the question with question-answer exemplars) and retrieval augmentation (injecting Wikipedia passages into the prompt). For a subset of the USMLE questions, a medical expert reviewed and annotated the model's CoT. We found that InstructGPT can often read, reason and recall expert knowledge. Failure are primarily due to lack of knowledge and reasoning errors and trivial guessing heuristics are observed, e.g.{\textbackslash} too often predicting labels A and D on USMLE. Sampling and combining many completions overcome some of these limitations. Using 100 samples, Codex 5-shot CoT not only gives close to well-calibrated predictive probability but also achieves human-level performances on the three datasets. USMLE: 60.2\%, MedMCQA: 62.7\% and PubMedQA: 78.2\%.},
	urldate = {2023-04-24},
	publisher = {arXiv},
	author = {Liévin, Valentin and Hother, Christoffer Egeberg and Winther, Ole},
	month = jan,
	year = {2023},
	note = {arXiv:2207.08143 [cs]},
	keywords = {Untagged},
}

@inproceedings{liu_learnable_2023,
	title = {Learnable {Embedding} sizes for {Recommender} {Systems}},
	url = {https://openreview.net/forum?id=vQzcqQWIS0q},
	abstract = {The embedding-based representation learning is commonly used in deep learning recommendation models to map the raw sparse features to dense vectors. The traditional embedding manner that assigns a uniform size to all features has two issues. First, the numerous features inevitably lead to a gigantic embedding table that causes a high memory usage cost. Second, it is likely to cause the over-fitting problem for those features that do not require too large representation capacity. Existing works that try to address the problem always cause a significant drop in recommendation performance or suffers from the limitation of unaffordable training time cost. In this paper, we proposed a novel approach, named PEP (short for Plug-in Embedding Pruning), to reduce the size of the embedding table while avoiding the drop of recommendation accuracy. PEP prunes embedding parameter where the pruning threshold(s) can be adaptively learned from data. Therefore we can automatically obtain a mixed-dimension embedding-scheme by pruning redundant parameters for each feature. PEP is a general framework that can plug in various base recommendation models. Extensive experiments demonstrate it can efficiently cut down embedding parameters and boost the base model's performance. Specifically, it achieves strong recommendation performance while reducing 97-99\% parameters. As for the computation cost, PEP only brings an additional 20-30\% time cost compare with base models.},
	language = {en},
	urldate = {2023-04-24},
	author = {Liu, Siyi and Gao, Chen and Chen, Yihong and Jin, Depeng and Li, Yong},
	month = apr,
	year = {2023},
	keywords = {Untagged},
}

@misc{liu_text-guided_2023,
	title = {A {Text}-guided {Protein} {Design} {Framework}},
	url = {http://arxiv.org/abs/2302.04611},
	abstract = {Current AI-assisted protein design mainly utilizes protein sequential and structural information. Meanwhile, there exists tremendous knowledge curated by humans in the text format describing proteins' high-level properties. Yet, whether the incorporation of such text data can help protein design tasks has not been explored. To bridge this gap, we propose ProteinDT, a multi-modal framework that leverages textual descriptions for protein design. ProteinDT consists of three subsequent steps: ProteinCLAP that aligns the representation of two modalities, a facilitator that generates the protein representation from the text modality, and a decoder that generates the protein sequences from the representation. To train ProteinDT, we construct a large dataset, SwissProtCLAP, with 441K text and protein pairs. We empirically verify the effectiveness of ProteinDT from three aspects: (1) consistently superior performance on four out of six protein property prediction benchmarks; (2) over 90\% accuracy for text-guided protein generation; and (3) promising results for zero-shot text-guided protein editing.},
	urldate = {2023-04-08},
	publisher = {arXiv},
	author = {Liu, Shengchao and Zhu, Yutao and Lu, Jiarui and Xu, Zhao and Nie, Weili and Gitter, Anthony and Xiao, Chaowei and Tang, Jian and Guo, Hongyu and Anandkumar, Anima},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04611 [cs, q-bio, stat]},
	keywords = {Untagged},
}

@article{lu_unified-io_2023,
	title = {{UNIFIED}-{IO}: {A} {UNIFIED} {MODEL} {FOR} {VISION}, {LANGUAGE}, {AND} {MULTI}-{MODAL} {TASKS}},
	language = {en},
	author = {Lu, Jiasen and Clark, Christopher and Zellers, Rowan and Mottaghi, Roozbeh and Kembhavi, Aniruddha},
	year = {2023},
	keywords = {Untagged},
}

@article{madani_large_2023,
	title = {Large language models generate functional protein sequences across diverse families},
	issn = {1546-1696},
	url = {https://doi.org/10.1038/s41587-022-01618-2},
	doi = {10.1038/s41587-022-01618-2},
	abstract = {Deep-learning language models have shown promise in various biotechnological applications, including protein design and engineering. Here we describe ProGen, a language model that can generate protein sequences with a predictable function across large protein families, akin to generating grammatically and semantically correct natural language sentences on diverse topics. The model was trained on 280 million protein sequences from {\textgreater}19,000 families and is augmented with control tags specifying protein properties. ProGen can be further fine-tuned to curated sequences and tags to improve controllable generation performance of proteins from families with sufficient homologous samples. Artificial proteins fine-tuned to five distinct lysozyme families showed similar catalytic efficiencies as natural lysozymes, with sequence identity to natural proteins as low as 31.4\%. ProGen is readily adapted to diverse protein families, as we demonstrate with chorismate mutase and malate dehydrogenase.},
	journal = {Nature Biotechnology},
	author = {Madani, Ali and Krause, Ben and Greene, Eric R. and Subramanian, Subu and Mohr, Benjamin P. and Holton, James M. and Olmos, Jose Luis and Xiong, Caiming and Sun, Zachary Z. and Socher, Richard and Fraser, James S. and Naik, Nikhil},
	month = jan,
	year = {2023},
	keywords = {Untagged},
}

@misc{more_awesome-text-based-image-manipulation_2023,
	title = {awesome-text-based-image-manipulation},
	copyright = {CC0-1.0},
	url = {https://github.com/martinduartemore/awesome-text-based-image-manipulation},
	abstract = {A curated list of text-based image manipulation methods.},
	urldate = {2023-06-02},
	author = {More, Martin Duarte},
	month = may,
	year = {2023},
	note = {original-date: 2020-08-19T21:00:27Z},
	keywords = {Untagged},
}

@article{mao_3d_2023,
	title = {{3D} {Object} {Detection} for {Autonomous} {Driving}: {A} {Comprehensive} {Survey}},
	issn = {0920-5691, 1573-1405},
	shorttitle = {{3D} {Object} {Detection} for {Autonomous} {Driving}},
	url = {https://link.springer.com/10.1007/s11263-023-01790-1},
	doi = {10.1007/s11263-023-01790-1},
	abstract = {Autonomous driving, in recent years, has been receiving increasing attention for its potential to relieve drivers’ burdens and improve the safety of driving. In modern autonomous driving pipelines, the perception system is an indispensable component, aiming to accurately estimate the status of surrounding environments and provide reliable observations for prediction and planning. 3D object detection, which aims to predict the locations, sizes, and categories of the 3D objects near an autonomous vehicle, is an important part of a perception system. This paper reviews the advances in 3D object detection for autonomous driving. First, we introduce the background of 3D object detection and discuss the challenges in this task. Second, we conduct a comprehensive survey of the progress in 3D object detection from the aspects of models and sensory inputs, including LiDAR-based, camera-based, and multi-modal detection approaches. We also provide an in-depth analysis of the potentials and challenges in each category of methods. Additionally, we systematically investigate the applications of 3D object detection in driving systems. Finally, we conduct a performance analysis of the 3D object detection approaches, and we further summarize the research trends over the years and prospect the future directions of this area.},
	language = {en},
	urldate = {2023-06-02},
	journal = {International Journal of Computer Vision},
	author = {Mao, Jiageng and Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
	month = apr,
	year = {2023},
	keywords = {Untagged},
}

@article{mohoney_high-throughput_2023,
	title = {High-{Throughput} {Vector} {Similarity} {Search} in {Knowledge} {Graphs}},
	abstract = {There is an increasing adoption of machine learning for encoding data into vectors to serve online recommendation and search use cases. As a result, recent data management systems propose augmenting query processing with online vector similarity search. In this work, we explore vector similarity search in the context of Knowledge Graphs (KGs). Motivated by the tasks of ﬁnding related KG queries and entities for past KG query workloads, we focus on hybrid vector similarity search (hybrid queries for short) where part of the query corresponds to vector similarity search and part of the query corresponds to predicates over relational attributes associated with the underlying data vectors. For example, given past KG queries for a song entity, we want to construct new queries for new song entities whose vector representations are close to the vector representation of the entity in the past KG query. But entities in a KG also have non-vector attributes such as a song associated with an artist, a genre, and a release date. Therefore, suggested entities must also satisfy query predicates over non-vector attributes beyond a vector-based similarity predicate. While these tasks are central to KGs, our contributions are generally applicable to hybrid queries. In contrast to prior works that optimize online queries, we focus on enabling eﬃcient batch processing of past hybrid query workloads. We present our system, HQI, for high-throughput batch processing of hybrid queries. We introduce a workload-aware vector data partitioning scheme to tailor the vector index layout to the given workload and describe a multi-query optimization technique to reduce the overhead of vector similarity computations. We evaluate our methods on industrial workloads and demonstrate that HQI yields a 31× improvement in throughput for ﬁnding related KG queries compared to existing hybrid query processing approaches.},
	language = {en},
	author = {Mohoney, Jason and Pacaci, Anil and Chowdhury, Shihabur Rahman and Mousavi, Ali and Ilyas, Ihab F and Minhas, Umar Farooq and Pound, Jeﬀrey and Rekatsinas, Theodoros},
	year = {2023},
	keywords = {Untagged},
}

@misc{oquab_dinov2_2023,
	title = {{DINOv2}: {Learning} {Robust} {Visual} {Features} without {Supervision}},
	shorttitle = {{DINOv2}},
	url = {http://arxiv.org/abs/2304.07193},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	urldate = {2023-04-24},
	publisher = {arXiv},
	author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07193 [cs]},
	keywords = {Untagged},
}

@misc{nguyen_unified_2023,
	title = {A {Unified} {Framework} for {Learned} {Sparse} {Retrieval}},
	url = {http://arxiv.org/abs/2303.13416},
	abstract = {Learned sparse retrieval (LSR) is a family of first-stage retrieval methods that are trained to generate sparse lexical representations of queries and documents for use with an inverted index. Many LSR methods have been recently introduced, with Splade models achieving state-of-the-art performance on MSMarco. Despite similarities in their model architectures, many LSR methods show substantial differences in effectiveness and efficiency. Differences in the experimental setups and configurations used make it difficult to compare the methods and derive insights. In this work, we analyze existing LSR methods and identify key components to establish an LSR framework that unifies all LSR methods under the same perspective. We then reproduce all prominent methods using a common codebase and re-train them in the same environment, which allows us to quantify how components of the framework affect effectiveness and efficiency. We find that (1) including document term weighting is most important for a method's effectiveness, (2) including query weighting has a small positive impact, and (3) document expansion and query expansion have a cancellation effect. As a result, we show how removing query expansion from a state-of-the-art model can reduce latency significantly while maintaining effectiveness on MSMarco and TripClick benchmarks. Our code is publicly available at https://github.com/thongnt99/learned-sparse-retrieval},
	language = {en},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {Nguyen, Thong and MacAvaney, Sean and Yates, Andrew},
	month = mar,
	year = {2023},
	note = {arXiv:2303.13416 [cs]},
	keywords = {Untagged},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Untagged},
}

@inproceedings{park_dual-path_2023,
	title = {Dual-{Path} {Adaptation} {From} {Image} to {Video} {Transformers}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Park_Dual-Path_Adaptation_From_Image_to_Video_Transformers_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-02},
	author = {Park, Jungin and Lee, Jiyoung and Sohn, Kwanghoon},
	year = {2023},
	keywords = {Untagged},
	pages = {2203--2213},
}

@inproceedings{pei_clipping_2023,
	title = {{CLIPPING}: {Distilling} {CLIP}-{Based} {Models} {With} a {Student} {Base} for {Video}-{Language} {Retrieval}},
	shorttitle = {{CLIPPING}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Pei_CLIPPING_Distilling_CLIP-Based_Models_With_a_Student_Base_for_Video-Language_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-02},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Pei, Renjing and Liu, Jianzhuang and Li, Weimian and Shao, Bin and Xu, Songcen and Dai, Peng and Lu, Juwei and Yan, Youliang},
	year = {2023},
	keywords = {Untagged},
	pages = {18983--18992},
}

@misc{peng_instruction_2023,
	title = {Instruction {Tuning} with {GPT}-4},
	url = {http://arxiv.org/abs/2304.03277},
	abstract = {Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.},
	urldate = {2023-04-11},
	publisher = {arXiv},
	author = {Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03277 [cs]},
	keywords = {Untagged},
}

@inproceedings{shen_lexmae_2023,
	title = {{LEXMAE}: {LEXICON}-{BOTTLENECKED} {PRETRAINING} {FOR} {LARGE}-{SCALE} {RETRIEVAL}},
	abstract = {In large-scale retrieval, the lexicon-weighting paradigm, learning weighted sparse representations in vocabulary space, has shown promising results with high quality and low latency. Despite it deeply exploiting the lexicon-representing capability of pre-trained language models, a crucial gap remains between language modeling and lexicon-weighting retrieval – the former preferring certain or low-entropy words whereas the latter favoring pivot or high-entropy words – becoming the main barrier to lexicon-weighting performance for large-scale retrieval. To bridge this gap, we propose a brand-new pre-training framework, lexicon-bottlenecked masked autoencoder (LexMAE), to learn importance-aware lexicon representations. Essentially, we present a lexicon-bottlenecked module between a normal language modeling encoder and a weakened decoder, where a continuous bag-of-words bottleneck is constructed to learn a lexicon-importance distribution in an unsupervised fashion. The pre-trained LexMAE is readily transferred to the lexicon-weighting retrieval via fine-tuning. On the ad-hoc retrieval benchmark, MS-Marco, it achieves 42.6\% MRR@10 with 45.8 QPS for the passage dataset and 44.4\% MRR@100 with 134.8 QPS for the document dataset, by a CPU machine. And LexMAE shows state-of-the-art zero-shot transfer capability on BEIR benchmark with 12 datasets.},
	language = {en},
	author = {Shen, Tao and Geng, Xiubo and Tao, Chongyang and Xu, Can and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin},
	year = {2023},
	keywords = {Untagged},
}

@misc{qu_continuous_2023,
	title = {Continuous {Input} {Embedding} {Size} {Search} {For} {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2304.03501},
	doi = {10.48550/arXiv.2304.03501},
	abstract = {Latent factor models are the most popular backbones for today's recommender systems owing to their prominent performance. Latent factor models represent users and items as real-valued embedding vectors for pairwise similarity computation, and all embeddings are traditionally restricted to a uniform size that is relatively large (e.g., 256-dimensional). With the exponentially expanding user base and item catalog in contemporary e-commerce, this design is admittedly becoming memory-inefficient. To facilitate lightweight recommendation, reinforcement learning (RL) has recently opened up opportunities for identifying varying embedding sizes for different users/items. However, challenged by search efficiency and learning an optimal RL policy, existing RL-based methods are restricted to highly discrete, predefined embedding size choices. This leads to a largely overlooked potential of introducing finer granularity into embedding sizes to obtain better recommendation effectiveness under a given memory budget. In this paper, we propose continuous input embedding size search (CIESS), a novel RL-based method that operates on a continuous search space with arbitrary embedding sizes to choose from. In CIESS, we further present an innovative random walk-based exploration strategy to allow the RL policy to efficiently explore more candidate embedding sizes and converge to a better decision. CIESS is also model-agnostic and hence generalizable to a variety of latent factor RSs, whilst experiments on two real-world datasets have shown state-of-the-art performance of CIESS under different memory budgets when paired with three popular recommendation models.},
	urldate = {2023-04-24},
	publisher = {arXiv},
	author = {Qu, Yunke and Chen, Tong and Zhao, Xiangyu and Cui, Lizhen and Zheng, Kai and Yin, Hongzhi},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03501 [cs]},
	keywords = {Untagged},
}

@misc{saeed_querying_2023,
	title = {Querying {Large} {Language} {Models} with {SQL}},
	url = {http://arxiv.org/abs/2304.00472},
	abstract = {In many use-cases, information is stored in text but not available in structured data. However, extracting data from natural language text to precisely fit a schema, and thus enable querying, is a challenging task. With the rise of pre-trained Large Language Models (LLMs), there is now an effective solution to store and use information extracted from massive corpora of text documents. Thus, we envision the use of SQL queries to cover a broad range of data that is not captured by traditional databases by tapping the information in LLMs. To ground this vision, we present Galois, a prototype based on a traditional database architecture, but with new physical operators for querying the underlying LLM. The main idea is to execute some operators of the the query plan with prompts that retrieve data from the LLM. For a large class of SQL queries, querying LLMs returns well structured relations, with encouraging qualitative results. Preliminary experimental results make pre-trained LLMs a promising addition to the field of database systems, introducing a new direction for hybrid query processing. However, we pinpoint several research challenges that must be addressed to build a DBMS that exploits LLMs. While some of these challenges necessitate integrating concepts from the NLP literature, others offer novel research avenues for the DB community.},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {Saeed, Mohammed and De Cao, Nicola and Papotti, Paolo},
	month = apr,
	year = {2023},
	note = {arXiv:2304.00472 [cs]},
	keywords = {Untagged},
}

@misc{shen_hugginggpt_2023,
	title = {{HuggingGPT}: {Solving} {AI} {Tasks} with {ChatGPT} and its {Friends} in {HuggingFace}},
	shorttitle = {{HuggingGPT}},
	url = {http://arxiv.org/abs/2303.17580},
	abstract = {Solving complicated AI tasks with different domains and modalities is a key step toward advanced artificial intelligence. While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards advanced artificial intelligence.},
	urldate = {2023-04-11},
	publisher = {arXiv},
	author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
	month = apr,
	year = {2023},
	note = {arXiv:2303.17580 [cs]},
	keywords = {Untagged},
}

@article{shi_pv-rcnn_2023,
	title = {{PV}-{RCNN}++: {Point}-{Voxel} {Feature} {Set} {Abstraction} {With} {Local} {Vector} {Representation} for {3D} {Object} {Detection}},
	volume = {131},
	issn = {0920-5691, 1573-1405},
	shorttitle = {{PV}-{RCNN}++},
	url = {https://link.springer.com/10.1007/s11263-022-01710-9},
	doi = {10.1007/s11263-022-01710-9},
	abstract = {Abstract
            
              3D object detection is receiving increasing attention from both industry and academia thanks to its wide applications in various fields. In this paper, we propose Point-Voxel Region-based Convolution Neural Networks (PV-RCNNs) for 3D object detection on point clouds. First, we propose a novel 3D detector, PV-RCNN, which boosts the 3D detection performance by deeply integrating the feature learning of both point-based set abstraction and voxel-based sparse convolution through two novel steps,
              i.e.
              , the voxel-to-keypoint scene encoding and the keypoint-to-grid RoI feature abstraction. Second, we propose an advanced framework, PV-RCNN++, for more efficient and accurate 3D object detection. It consists of two major improvements: sectorized proposal-centric sampling for efficiently producing more representative keypoints, and VectorPool aggregation for better aggregating local point features with much less resource consumption. With these two strategies, our PV-RCNN++ is about
              
                
                  \$\$3{\textbackslash}times \$\$
                  
                    
                      3
                      ×
                    
                  
                
              
              faster than PV-RCNN, while also achieving better performance. The experiments demonstrate that our proposed PV-RCNN++ framework achieves state-of-the-art 3D detection performance on the large-scale and highly-competitive Waymo Open Dataset with 10 FPS inference speed on the detection range of
              
                
                  \$\$150m {\textbackslash}times 150m\$\$
                  
                    
                      150
                      m
                      ×
                      150
                      m
                    
                  
                
              
              .},
	language = {en},
	number = {2},
	urldate = {2023-06-02},
	journal = {International Journal of Computer Vision},
	author = {Shi, Shaoshuai and Jiang, Li and Deng, Jiajun and Wang, Zhe and Guo, Chaoxu and Shi, Jianping and Wang, Xiaogang and Li, Hongsheng},
	month = feb,
	year = {2023},
	keywords = {Untagged},
	pages = {531--551},
}

@inproceedings{song_consistency_2023,
	title = {Consistency {Models}},
	url = {http://arxiv.org/abs/2303.01469},
	abstract = {Diffusion models have made signiﬁcant breakthroughs in image, audio, and video generation, but they depend on an iterative generation process that causes slow sampling speed and caps their potential for real-time applications. To overcome this limitation, we propose consistency models, a new family of generative models that achieve high sample quality without adversarial training. They support fast one-step generation by design, while still allowing for few-step sampling to trade compute for sample quality. They also support zero-shot data editing, like image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either as a way to distill pre-trained diffusion models, or as standalone generative models. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in oneand few-step generation. For example, we achieve the new state-of-the-art FID of 3.55 on CIFAR10 and 6.20 on ImageNet 64 ˆ 64 for one-step generation. When trained as standalone generative models, consistency models also outperform single-step, non-adversarial generative models on standard benchmarks like CIFAR-10, ImageNet 64 ˆ 64 and LSUN 256 ˆ 256.},
	language = {en},
	urldate = {2023-06-02},
	booktitle = {{ICML} 2023},
	publisher = {arXiv},
	author = {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
	month = may,
	year = {2023},
	note = {arXiv:2303.01469 [cs, stat]},
	keywords = {Untagged},
}

@misc{wang_seggpt_2023,
	title = {{SegGPT}: {Segmenting} {Everything} {In} {Context}},
	shorttitle = {{SegGPT}},
	url = {http://arxiv.org/abs/2304.03284},
	abstract = {We present SegGPT, a generalist model for segmenting everything in context. We unify various segmentation tasks into a generalist in-context learning framework that accommodates different kinds of segmentation data by transforming them into the same format of images. The training of SegGPT is formulated as an in-context coloring problem with random color mapping for each data sample. The objective is to accomplish diverse tasks according to the context, rather than relying on specific colors. After training, SegGPT can perform arbitrary segmentation tasks in images or videos via in-context inference, such as object instance, stuff, part, contour, and text. SegGPT is evaluated on a broad range of tasks, including few-shot semantic segmentation, video object segmentation, semantic segmentation, and panoptic segmentation. Our results show strong capabilities in segmenting in-domain and out-of-domain targets, either qualitatively or quantitatively.},
	urldate = {2023-04-11},
	publisher = {arXiv},
	author = {Wang, Xinlong and Zhang, Xiaosong and Cao, Yue and Wang, Wen and Shen, Chunhua and Huang, Tiejun},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03284 [cs]},
	keywords = {Untagged},
}

@misc{wang_images_2023,
	title = {Images {Speak} in {Images}: {A} {Generalist} {Painter} for {In}-{Context} {Visual} {Learning}},
	shorttitle = {Images {Speak} in {Images}},
	url = {http://arxiv.org/abs/2212.02499},
	abstract = {In-context learning, as a new paradigm in NLP, allows the model to rapidly adapt to various tasks with only a handful of prompts and examples. But in computer vision, the difficulties for in-context learning lie in that tasks vary significantly in the output representations, thus it is unclear how to define the general-purpose task prompts that the vision model can understand and transfer to out-of-domain tasks. In this work, we present Painter, a generalist model which addresses these obstacles with an "image"-centric solution, that is, to redefine the output of core vision tasks as images, and specify task prompts as also images. With this idea, our training process is extremely simple, which performs standard masked image modeling on the stitch of input and output image pairs. This makes the model capable of performing tasks conditioned on visible image patches. Thus, during inference, we can adopt a pair of input and output images from the same task as the input condition, to indicate which task to perform. Without bells and whistles, our generalist Painter can achieve competitive performance compared to well-established task-specific models, on seven representative vision tasks ranging from high-level visual understanding to low-level image processing. In addition, Painter significantly outperforms recent generalist models on several challenging tasks.},
	urldate = {2023-04-11},
	publisher = {arXiv},
	author = {Wang, Xinlong and Wang, Wen and Cao, Yue and Shen, Chunhua and Huang, Tiejun},
	month = mar,
	year = {2023},
	note = {arXiv:2212.02499 [cs]},
	keywords = {Untagged},
}

@inproceedings{wu_cap4video_2023,
	title = {{Cap4Video}: {What} {Can} {Auxiliary} {Captions} {Do} for {Text}-{Video} {Retrieval}?},
	shorttitle = {{Cap4Video}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Wu_Cap4Video_What_Can_Auxiliary_Captions_Do_for_Text-Video_Retrieval_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-02},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wu, Wenhao and Luo, Haipeng and Fang, Bo and Wang, Jingdong and Ouyang, Wanli},
	year = {2023},
	keywords = {Untagged},
	pages = {10704--10713},
}

@misc{wang_video-text_2023,
	title = {Video-{Text} {Retrieval} by {Supervised} {Multi}-{Space} {Multi}-{Grained} {Alignment}},
	url = {http://arxiv.org/abs/2302.09473},
	doi = {10.48550/arXiv.2302.09473},
	abstract = {While recent progress in video-text retrieval has been advanced by the exploration of better representation learning, in this paper, we present a novel multi-space multi-grained supervised learning framework, SUMA, to learn an aligned representation space shared between the video and the text for video-text retrieval. The shared aligned space is initialized with a finite number of concept clusters, each of which refers to a number of basic concepts (words). With the text data at hand, we are able to update the shared aligned space in a supervised manner using the proposed similarity and alignment losses. Moreover, to enable multi-grained alignment, we incorporate frame representations for better modeling the video modality and calculating fine-grained and coarse-grained similarity. Benefiting from learned shared aligned space and multi-grained similarity, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of SUMA over existing methods.},
	urldate = {2023-05-12},
	publisher = {arXiv},
	author = {Wang, Yimu and Shi, Peng},
	month = feb,
	year = {2023},
	note = {arXiv:2302.09473 [cs]},
	keywords = {Untagged},
}

@misc{wu_visual_2023,
	title = {Visual {ChatGPT}: {Talking}, {Drawing} and {Editing} with {Visual} {Foundation} {Models}},
	shorttitle = {Visual {ChatGPT}},
	url = {http://arxiv.org/abs/2303.04671},
	doi = {10.48550/arXiv.2303.04671},
	abstract = {ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called {\textbackslash}textbf\{Visual ChatGPT\}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at {\textbackslash}url\{https://github.com/microsoft/visual-chatgpt\}.},
	urldate = {2023-04-11},
	publisher = {arXiv},
	author = {Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
	month = mar,
	year = {2023},
	note = {arXiv:2303.04671 [cs]},
	keywords = {Untagged},
}

@misc{yang_atomic_2023,
	title = {{AToMiC}: {An} {Image}/{Text} {Retrieval} {Test} {Collection} to {Support} {Multimedia} {Content} {Creation}},
	shorttitle = {{AToMiC}},
	url = {http://arxiv.org/abs/2304.01961},
	abstract = {This paper presents the AToMiC (Authoring Tools for Multimedia Content) dataset, designed to advance research in image/text cross-modal retrieval. While vision-language pretrained transformers have led to significant improvements in retrieval effectiveness, existing research has relied on image-caption datasets that feature only simplistic image-text relationships and underspecified user models of retrieval tasks. To address the gap between these oversimplified settings and real-world applications for multimedia content creation, we introduce a new approach for building retrieval test collections. We leverage hierarchical structures and diverse domains of texts, styles, and types of images, as well as large-scale image-document associations embedded in Wikipedia. We formulate two tasks based on a realistic user model and validate our dataset through retrieval experiments using baseline models. AToMiC offers a testbed for scalable, diverse, and reproducible multimedia retrieval research. Finally, the dataset provides the basis for a dedicated track at the 2023 Text Retrieval Conference (TREC), and is publicly available at https://github.com/TREC-AToMiC/AToMiC.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Yang, Jheng-Hong and Lassance, Carlos and de Rezende, Rafael Sampaio and Srinivasan, Krishna and Redi, Miriam and Clinchant, Stéphane and Lin, Jimmy},
	month = apr,
	year = {2023},
	note = {arXiv:2304.01961 [cs]},
	keywords = {Untagged},
}

@article{yu_alphapulldownpython_2023,
	title = {{AlphaPulldown}—a python package for protein–protein interaction screens using {AlphaFold}-{Multimer}},
	volume = {39},
	issn = {1367-4811},
	url = {https://doi.org/10.1093/bioinformatics/btac749},
	doi = {10.1093/bioinformatics/btac749},
	abstract = {The artificial intelligence-based structure prediction program AlphaFold-Multimer enabled structural modelling of protein complexes with unprecedented accuracy. Increasingly, AlphaFold-Multimer is also used to discover new protein–protein interactions (PPIs). Here, we present AlphaPulldown, a Python package that streamlines PPI screens and high-throughput modelling of higher-order oligomers using AlphaFold-Multimer. It provides a convenient command-line interface, a variety of confidence scores and a graphical analysis tool.AlphaPulldown is freely available at https://www.embl-hamburg.de/AlphaPulldown.Supplementary note is available at Bioinformatics online.},
	number = {1},
	urldate = {2023-04-06},
	journal = {Bioinformatics},
	author = {Yu, Dingquan and Chojnowski, Grzegorz and Rosenthal, Maria and Kosinski, Jan},
	month = jan,
	year = {2023},
	keywords = {Untagged},
	pages = {btac749},
}

@misc{yu_zero-shot_2023,
	title = {Zero-shot {Referring} {Image} {Segmentation} with {Global}-{Local} {Context} {Features}},
	url = {http://arxiv.org/abs/2303.17811},
	abstract = {Referring image segmentation (RIS) aims to find a segmentation mask given a referring expression grounded to a region of the input image. Collecting labelled datasets for this task, however, is notoriously costly and labor-intensive. To overcome this issue, we propose a simple yet effective zero-shot referring image segmentation method by leveraging the pre-trained cross-modal knowledge from CLIP. In order to obtain segmentation masks grounded to the input text, we propose a mask-guided visual encoder that captures global and local contextual information of an input image. By utilizing instance masks obtained from off-the-shelf mask proposal techniques, our method is able to segment fine-detailed Istance-level groundings. We also introduce a global-local text encoder where the global feature captures complex sentence-level semantics of the entire input expression while the local feature focuses on the target noun phrase extracted by a dependency parser. In our experiments, the proposed method outperforms several zero-shot baselines of the task and even the weakly supervised referring expression segmentation method with substantial margins. Our code is available at https://github.com/Seonghoon-Yu/Zero-shot-RIS.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Yu, Seonghoon and Seo, Paul Hongsuck and Son, Jeany},
	month = apr,
	year = {2023},
	note = {arXiv:2303.17811 [cs]
version: 2},
	keywords = {Untagged},
}

@misc{zhang_llama-adapter_2023,
	title = {{LLaMA}-{Adapter}: {Efficient} {Fine}-tuning of {Language} {Models} with {Zero}-init {Attention}},
	shorttitle = {{LLaMA}-{Adapter}},
	url = {http://arxiv.org/abs/2303.16199},
	abstract = {We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapter.},
	urldate = {2023-04-13},
	publisher = {arXiv},
	author = {Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16199 [cs]},
	keywords = {Untagged},
}

@inproceedings{zhou_large_2023,
	title = {Large {Language} {Models} are {Human}-{Level} {Prompt} {Engineers}},
	url = {https://openreview.net/forum?id=92gvk82DE-},
	abstract = {By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 21/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts.},
	language = {en},
	urldate = {2023-04-06},
	author = {Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
	month = feb,
	year = {2023},
	keywords = {Untagged},
}

@misc{zhu_chatgpt_2023,
	title = {{ChatGPT} {Asks}, {BLIP}-2 {Answers}: {Automatic} {Questioning} {Towards} {Enriched} {Visual} {Descriptions}},
	shorttitle = {{ChatGPT} {Asks}, {BLIP}-2 {Answers}},
	url = {http://arxiv.org/abs/2303.06594},
	doi = {10.48550/arXiv.2303.06594},
	abstract = {Asking insightful questions is crucial for acquiring knowledge and expanding our understanding of the world. However, the importance of questioning has been largely overlooked in AI research, where models have been primarily developed to answer questions. With the recent advancements of large language models (LLMs) like ChatGPT, we discover their capability to ask high-quality questions when provided with a suitable prompt. This discovery presents a new opportunity to develop an automatic questioning system. In this paper, we introduce ChatCaptioner, a novel automatic-questioning method deployed in image captioning. Here, ChatGPT is prompted to ask a series of informative questions about images to BLIP-2, a strong vision question-answering model. By keeping acquiring new visual information from BLIP-2's answers, ChatCaptioner is able to generate more enriched image descriptions. We conduct human-subject evaluations on common image caption datasets such as COCO, Conceptual Caption, and WikiArt, and compare ChatCaptioner with BLIP-2 as well as ground truth. Our results demonstrate that ChatCaptioner's captions are significantly more informative, receiving three times as many votes from human evaluators for providing the most image information. Besides, ChatCaptioner identifies 53\% more objects within the image than BLIP-2 alone measured by WordNet synset matching. Code is available at https://github.com/Vision-CAIR/ChatCaptioner},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Zhu, Deyao and Chen, Jun and Haydarov, Kilichbek and Shen, Xiaoqian and Zhang, Wenxuan and Elhoseiny, Mohamed},
	month = mar,
	year = {2023},
	note = {arXiv:2303.06594 [cs]},
	keywords = {Untagged},
}

@misc{noauthor_hiring-msr_2023,
	title = {Hiring-{MSR}},
	copyright = {MIT},
	url = {https://github.com/microsoft/unilm},
	abstract = {Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities},
	urldate = {2023-04-24},
	publisher = {Microsoft},
	month = apr,
	year = {2023},
	note = {original-date: 2019-07-23T04:15:28Z},
	keywords = {Untagged},
}

@misc{noauthor_cs_2023,
	title = {{CS} 886: {Graph} {Neural} {Networks}},
	shorttitle = {{CS} 886},
	url = {https://github.com/opallab/cs886-winter-2023},
	urldate = {2023-04-11},
	publisher = {Opal Lab},
	month = apr,
	year = {2023},
	note = {original-date: 2022-11-29T11:46:02Z},
	keywords = {Untagged},
}

@misc{noauthor_segment_2023,
	title = {Segment {Anything}},
	copyright = {Apache-2.0},
	url = {https://github.com/facebookresearch/segment-anything},
	abstract = {The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model.},
	urldate = {2023-04-06},
	publisher = {Meta Research},
	month = apr,
	year = {2023},
	note = {original-date: 2023-03-23T17:03:03Z},
	keywords = {Untagged},
}

@misc{noauthor_soap_2023,
	title = {{SOAP}},
	year = {2023},
	keywords = {Untagged},
}
