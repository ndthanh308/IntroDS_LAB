

\TODO{To perform field monitoring we make use of Mask-RCNN in conjunction with re-projection as described in our prior work by Halstead et al.~\cite{halstead2021crop}. 
Mask-RCNN provides instance-based semantic segmentation which includes the class information (e.g. weed species) and viewable surface area from the RGB-D camera.
The strength of the approach lies in being able to estimate how far the camera has moved, for instance Halstead~et~al. used wheel odometry, to re-project information between frames and conduct better object tracking as well as counting.
Despite the success of the proposed approach with wheel odometry, the extra localization information (e.g. GPS and IMU) available on \bbot\ should further enhance this technique.}

\TODO{In this paper we enhance demonstrate that the extra sensors on \bbot enhance the localization information which also enhances field monitoring.
In particular, we fuse the available odomtery and GPS information with an Extended Kalman Filter(EKF)~\cite{wei2011intelligent}.
This algorithm recursively estimates the state of a non-linear system in an optimal way.
Furthermore, adding local source of motion estimation can considerably reduce the risk of outage due to lack of proper satellite observations~\cite{ahmadi2021towards}.}

\TODO{The classic EKF method consists of two steps: the prediction and the correction. 
In prediction step the algorithm tries to predict future state of the system based on motion model, while the correction step improves predicted state's accuracy using real measurements from sensors. 
Using this principle, the state of the system can be determined recursively in real time.
In our system, the wheel odometry is used as control data~$u_t=(d_t^{t+1}, w_t)$ in the prediction step and position and orientation of the embedded EKF solution for the SBG system provides corrections.
As the robot velocity is always controlled with a differential controller model within crop-rows we express its kinematics model in x-y plane as:}
% 
\begin{equation}
    \begin{array}{lr}
        x_{t+1} = x_t + d_t^{t+1} \cdot cos(\theta_{t+1}), \\
        y_{t+1} = y_t + d_t^{t+1} \cdot sin(\theta_{t+1}). \\
        \theta_{t+1} = \theta_t + w_t
    \end{array}
    \label{eq:ekf_motionModel}
\end{equation}
% 
where $d_t^{t+1}$ is circular arc traveled within $t$ to $t+1$ and $w_t^{t+1}$ is the rotation angle around z-axis.
So, let the vector $\Vec{\mathbf{X}}_t = [x_t, y_t, z_t, \phi_t, \psi_t, \theta_t]^T$ be the state vector of the \bbot\ at time $t$.
where $x_t, y_t, z_t$ denote the current position in \textit{ENU} coordinate frame, and $\phi_t, \psi_t, \theta_t$ specify roll, pitch and yaw angles of the robot in world coordinate frame $\mathcal{F}_w$.

\TODO{In our prior work~\cite{halstead2021crop} some of the most challenging plants were grasses and so we propose to evaluate improvements to field monitoring (via improved localization with more sensors) on a grass crop. 
In particular, our evaluations for field monitoring in Section~\ref{sec:exp} are conducted on corn fields which have similar weed species to our prior work but the crop (the dominant plant) has a thinner leaf structure which makes it more difficult to track.}

%The trained model is used to evaluate the tracking performance of the BonnBon-I platform using varying types of odometry information.
%Specifically to improve the accuracy of image-based tracking results we tend to use Extended Kalman Filter(EKF) to fuse odomtery and GPS information which is explained in the following.
%
%However, wheel odometry is a coarse
%methodand tracking is counting is achieving by performing tracking which uses the re-projection of masks.
%This re-projection can benefit from the 

%then performedas our based method for instance-based semantic segmentation with conjunction with reprojection mathod explained in~\secref{subsec:intraCameraTracking}. 

%Similar to~\cite{halstead2018fruit, halstead2021crop}, we augment a Mask-RCNN network to incorporate super-class and sub-class classification heads. 
%The super-class represents generalized object recognition as a binary background versus plant classifier, while the sub-class outputs species level classifications of the crop and weed types.
%The overall benefit of this approach is the ability to accurately segment objects (super-class) while providing fine-grained classification with minimal extra overhead.

\begin{comment}
The performance of this approach is outlined in~\cite{halstead2021crop}, where its benefit of being crop and environment agnostic was shown for both arable farmland and a glasshouse setting.
The strength of this approach is further outlined in~\cite{halstead2020} where the parallel network structure was compared to a standard $N$-class network.
The parallel structure was able to more accurately detect the presence of objects in the scene while still supplying relevant species level information.

This approach is used on the two datasets mentioned in~\secref{subsec:dataset}.
For SB20 we train a model using the same parameters outlined in~\cite{halstead2021crop} and store the fine-grained and bounding box location, sub-class label, and size information.
These characteristics are used for the intervention experiments.
%
% % Figure environment removed 
%
We once again show the flexibility of this approach by applying it to a novel crop - corn.
While the corn fields have similar weed species to the sugar beet dataset of~\cite{halstead2021crop} the thinner leaf structure of the early growth corn makes it a complex crop to classify and segment.
The trained model is used to evaluate the tracking performance of the BonnBon-I platform using varying types of odometry information.
Specifically to improve the accuracy of image-based tracking results we tend to use Extended Kalman Filter(EKF) to fuse odomtery and GPS information which is explained in the following.
\end{comment}

\begin{comment}
In~\cite{halstead2021crop} we showed that by augmenting Mask-RCNN architecture with super-class and sub-class heads we can avoid miss-classifications of less represented sub-classes. 
Specially for agriculture we can consider that their is a primary class (plant) with sub-classes (fine-grained classification) which in most of the cases the distribution of species appearance are not uniform.
The benefit of this approach is demonstrated in~\cite{halstead2021crop} for both glasshouses, and arable farmland and outlined the benefit of having crop, environment, and platform invariant approaches.
Figure~\ref{fig:subcls} outlines the basic sub-class structure inserted into the Mask-RCNN network.
This super-class (plant) and sub-class (species) alignment ensures that the super-class generalises to what a ``plant'' is while the sub-class provides the species level classification required for weeding~\cite{halstead2018fruit, halstead2020}.
While these less represented classes may be missed entirely in an $N$-class method the generalised super-class approach is still able to identify them as a ``plant''.


% Mask-RCNN is designed to be an $N$-class classifier with an bounding box regression and instance based segmentation within the bounded region.
% The strength of this is that multiple classes can be classified, regressed, and segmented within the same network.
% However, this approach is not always optimal.
% For agriculture we can consider that their is a primary class (plant) with sub-classes (fine-grained classification).

% This super-class (plant) and sub-class (species) alignment ensures that the super-class generalises to what a ``plant'' is while the sub-class provides the species level classification required for weeding.
% A key benefit of this technique~\cite{halstead2018fruit, halstead2020, halstead2021crop} is the ability to still recognise less represented classes.
% While these less represented classes may be missed entirely in an $N$-class method the generalised super-class approach is still able to identify them as a ``plant''.

% As a species level classifier~\cite{halstead2021crop} showed the benefit of this approach for both glasshouses, and arable farmland and outlined the benefit of having crop, environment, and platform invariant approaches.
% Figure~\ref{fig:subcls} outlines the basic sub-class structure inserted into the Mask-RCNN network.
% Generally, this network has a classification head, and regression head, and a mask head.
% In our approach we insert a sub-class classifier in the same network location as the classifier and regressor, which utilise the same embedding layer as these two.



% BonnBot-I is a arable farmland monitoring platform that works in row-crop fields.
% This allows it to capture data of any crop type that in these fields. 
Previously we have used sugar beet data captured on this platform in~\cite{halstead2021crop}, to show the flexibility of this approach we will utilise corn data captured on the same platform.

In this case the plants as a whole represent the super-class and the species represent the sub-class.

\begin{table}[!h]
    \vspace{-2mm}
	\centering
	\caption{\TODO{Dataset Sub Categories ... - MAH I don't think we need this table, we have already given the corn annotations previously.}}
	\begin{tabular}{l cc cc cc cc c}
	\toprule
	%&& \multicolumn{2}{c}{BUP} \\\hline
	  & \textbf{SB} & \textbf{CN} & \textbf{Bi} & \textbf{An} & \textbf{Chy} & \textbf{Pe} & \textbf{Th} & \textbf{Ch} & \textbf{Un} \\\hline
	 \midrule
    SB20 & 768 & - & 241 & 19 & 64 & 620 & 775 & 232 & 206 \\ 
    CN20 & - &  &  &  &  &  &  &  & \\
    \bottomrule
    \end{tabular}
    % \vspace{-4mm}
	\label{tab:dataset_cats}
\end{table}
\end{comment}
% Mask-RCNN with species level weed classification.

% \cite{halstead2018fruit, halstead2020, halstead2021crop}

% \begin{itemize}
%     \item Standard mask and faster pipeline. Talk about N Classes.
%     \item Refer to \cite{halstead2018fruit} and \cite{halstead2020} for an overview of why a super-class sub-class relationship does a better job when object counts are limited compared to an N-super-class classifier.
%     \item Describe the sub-class layer, maybe with a NEW figure.
%     \item refer to \cite{halstead2021crop} to show it working as a species-level classifier.
%     \item Talk about how the platform enables this stuff.
%     \item Corn seg
% \end{itemize}

% % Figure environment removed



\begin{comment}
\subsubsection{Precise in Field localization}
\label{sec:fieldLocalization}
The accuracy of in-field localization directly influences the accuracy of monitoring system and interventions. 
Hence to maximize weeding operations resolution a millimeter-level accurate position determination is crucial.
To increase the accuracy of robot localization from couple of centimeters (provided by SBG system) to level of a few millimeters we fuse SBG measurement with robot's wheel odometry using Extended Kalman Filter (EKF)~\cite{wei2011intelligent} which is an algorithm to recursively estimate the state of a non-linear system in an optimal way.
Furthermore, adding local source of motion estimation can considerably reduce the risk of outage due to lack of proper satellite observations~\cite{ahmadi2021towards}.

The classic EKF method consist of two steps:the prediction and the correction. 
In prediction step the algorithm tries to predict future state of the system based on motion model, while the correction step improves predicted state's accuracy using real measurements from sensors. 
Using this principle, the state of the system can be determined recursively in real time.
In our system, the wheel odometry is used as control data~ $u_t=(d_t^{t+1}, w_t)$ in prediction step and position and orientation of embedded EKF solution of SBG system provides corrections.
As the robot velocity is always controlled with a differential controller model within crop-rows we express its kinematics model in x-y plane as: 
% 
\begin{equation}
    \begin{array}{lr}
        x_{t+1} = x_t + d_t^{t+1} \cdot cos(\theta_{t+1}), \\
        y_{t+1} = y_t + d_t^{t+1} \cdot sin(\theta_{t+1}). \\
        \theta_{t+1} = \theta_t + w_t
    \end{array}
    \label{eq:ekf_motionModel}
\end{equation}
% 
where $d_t^{t+1}$ is circular arc traveled within $t$ to $t+1$ and $w_t^{t+1}$ is the rotation angle around z-axis.
So, let the vector $\Vec{\mathbf{X}}_t = [x_t, y_t, z_t, \phi_t, \psi_t, \theta_t]^T$ be the state vector of the \bbot\ at time $t$.
where $x_t, y_t, z_t$ denote the current position in \textit{ENU} coordinate frame, and $\phi_t, \psi_t, \theta_t$ specify roll, pitch and yaw angles of the robot in world coordinate frame $\mathcal{F}_w$.



% In the correction step, using SBG module output and wheel Odomtery readings we determine travelled distance based on odometry, GPS and O which are contained in measurement vector as $\mathbf{d}_{xy}^{o/s}$ in row. 
% This leads to the following measurement vector as time $t$:
% \begin{equation}
%   \label{eq:ekf_measurementVec}
%   \Vec{\mathbf{z}}_t = [x^s, y^s, z^s, \mathbf{d}_{xy}^{o/s}, a_z^i, \phi^i, \psi^i, \theta^i, \boldsymbol{\omega}_x^{i}, \boldsymbol{\omega}_y^{i}, \boldsymbol{\omega}_z^{i}]
% \end{equation}
% where, SBG ($s$) ensure the absolute positioning is obtained, 
% as , the IMU ($i$) provides absolute roll, pitch and yaw angles 
% where, $a_z^t$ represent the acceleration in z-direction and $\boldsymbol{\omega}_x^{i/v}, \boldsymbol{\omega}_y^{i/v}, \boldsymbol{\omega}_z^{i/v}$ indicate the angular velocities in x,y and z directions estimated from VO and IMU units.

% EKF explanation + VO version !! 
\begin{comment}
\subsubsection{Extended Kalman Filter (EKF)}: 
To fuse measurement of different systems we use the Extended Kalman filter (EKF)~\cite{??} which is an algorithm to recursively estimate the state of a non-linear system in an optimal way.
This classic method consist of two steps:the prediction and the correction. 
In prediction step the algorithm tries to predict future state of the system based on the system model, while the correction step improves this prediction using real measurements or additional information. 
By applying this principle, the state of the system can be determined recursively in real time.
Let $\Vec{\mathbf{X}}_t = [x_t, y_t, \phi_t, \psi_t, \theta_t]^T$ denote the state vector of the \bbot\ at time $t$.
where $x_t, y_t, z_t$ denote the current position in \textit{NEU} coordinate frame, and $\phi_t, \psi_t, \theta_t$ specifies the roll, pitch and yaw angles of the vehicle in world coordinate frame $\mathcal{F}_w$.
As the robot is always controlled with differential model within crop-rows we express its kinematic model in x-y plane as: 
\begin{equation}
    \begin{array}{lr}
        x_{t+1} = x_t + d_t^{t+1} \cdot cos(\theta_{t+1}), \\
        y_{t+1} = y_t + d_t^{t+1} \cdot sin(\theta_{t+1}). \\
        \theta_{t+1} = \theta_t + w_t^{t+1}
    \end{array}
    \label{eq:ekf_motionModel}
\end{equation}
Where $d_t^{t+1}$ is xy-distance traveled within $t$ to $t+1$ and $w_t^{t+1}$ is the rotation angle around z-axis.

In the correction step, using SBG module embedded EKF output and Visual-Odomtery we determine travelled distance based on odometry, GPS and VO which are contained in measurement vector as $\mathbf{d}_{xy}^{o/g/v}$ in row. 
This leads to the following measurement vector as time $t$:
\begin{equation}
  \label{eq:ekf_measurementVec}
  \Vec{\mathbf{z}}_t = [x^g, y^g, \mathbf{d}_{xy}^{o/g/v}, a_z^i, \phi^i, \psi^i, \theta^i, \boldsymbol{\omega}_x^{i/v}, \boldsymbol{\omega}_y^{i/v}, \boldsymbol{\omega}_z^{i/v}]
\end{equation}

as GPS ($g$) we ensure the absolute positioning is obtained, the IMU ($i$) provides absolute roll, pitch and yaw angles 
where, $a_z^t$ represent the acceleration in z-direction and $\boldsymbol{\omega}_x^{i/v}, \boldsymbol{\omega}_y^{i/v}, \boldsymbol{\omega}_z^{i/v}$ indicate the angular velocities in x,y and z directions estimated from VO and IMU units. 
\end{comment}

%  VO part asnd tracking part
\begin{comment}
\subsubsection{Intra-Camera Target Tracking}
Tracking-via-segmentation aims to exploit known properties of the agricultural scene as a robotic platform traverses a row.
As the scene remains relatively static between captures (from the camera on the platform) it can be assumed that the plants are both temporally and spatially static.
Assuming these properties we are able to conclude that an object captured at $t$ will be close in image location at $t+1$.
The platform movement and the frames-per-second of the camera allow us to make this assumption as we are not moving too rapidly that the scence changes dramatically.
Halstead et al.~\cite{halstead2021crop} provides a detailed outline of this approach for the original intersection-over-union based approach.

Early tracking-via-segmentation~\cite{halstead2018fruit} relies heavily on the  assumption of spatial and temporal consistancy. 
If the captured images are too far apart objects can not be matched which duplicates tracklets.
However, in~\cite{smitt2020pathobot} we showed that even with larger movements between captures can be aggregated together through reprojection using the wheel odometry of the platform and the depth information.
One problem with these early approaches was the matching criterion: IoU.
In~\cite{halstead2021crop} it was shown that for small discrepancies in image captures, even with reprojection, this criterion struggles to match objects, particularly small objects.

To allow for this~\cite{halstead2021crop} outlined a new matching criterion and compared it directly to the IoU.
This dynamic radius was able to match objects, even small objects, due to the underlying technique of using the Euclidean distance compare the center of mass between two objects, if this value is below a threshold, based on a radius value, the objects are aggregated into a single tracklet.
There is a single issue with this approach in that it can match 360$^o$ around the center of mass meaning incorrect matches do occur.
\TODO{total plant number od corn dataset is: row 7 & 545, row 9 & 802, row 11 & 713}
% \begin{table}
%     \centering
%     \caption{\TODO{Ground truth of the corn tracking data - MAH Put this in the text no reason for a table.}}
%     \label{tab:gttrack}
%     \begin{tabular}{|l|c|}
%         \hline
%         row id & total plants \\
%         \hline\hline
%         row 7 & 545 \\
%         row 9 & 802 \\
%         row 11 & 713 \\
%         \hline
%     \end{tabular}
% \end{table}

% \cite{smitt2020pathobot, halstead2021crop}
% \begin{itemize}
%     \item Describe tracking via segmentation with a figure.
%     \item Talk about \cite{smitt2020pathobot} as the initial trials to get this working.
%     \item Introduce \cite{halstead2021crop} and the reasons for DR over IoU and the benefits of it.
%     \item Talk about how the platform enables this type of thing.
%     \item Corn tracking?
% \end{itemize}
\begin{itemize}
    \item explain the necessity of VO and improvement of the tracking
    \item Describe VO approach briefly!
    \item explain relevance of VO output to prev. section (intra-camera tracking).
    \item explain ekf where VO gets fused with GPS output to take positioning accuracy from couple of centimeters to millimeter accuracy! (must be proved ... maybe be with simple map or orthomosaic!)
\end{itemize}
% Using Depth information of D455 sensor we are able to accurately localize and geo-reference each crops/weed on the ground, ??? increasing the intervention accuracy.
% Accessing consistent input modalities with such methods can reveal helpful representations of data and aid in estimating phenotypic information at without any extra costs for the system. 
% A like the depth modality which can be used for estimating growth-stage of crops and geometric based data association between consequent frames.
Visual Odometry~\cite{ahmadi2021registration} 
\end{comment}
\end{comment}