
%The main use case of BonnBot-I is to manage weeds in arable fields.
%To perform this task it is equipped with a novel weeding tool described in \secref{subsec:weedingImplements}.
%The actual treatment controlled based on an advanced DNN driven monitoring approach whcih is explained in \secref{subsec:cropWeedMonitoring}. 
%The monitoring approach estimates critical phenotypic information like crop/weed classification and segmentation, stem location and pixel-wise area of plants (or BBCH) of crops located underneath the robot in real-time while robot is moving through crop-row.

%The scarcity of proper agricultural datasets suited for precision weed management still is a challenge in agriculture.
%Hence we gathered and annotated two datasets which are being used for our plant monitoring system which is more elaborated in the following.




%\subsection{Dataset}
%\label{subsec:dataset}
%\input{contribs/dataset}

%\subsection{Crop/Weed Instance Monitoring and Tracking}
%\label{subsec:cropWeedMonitoring}
%\input{contribs/cropWeedMonitoring}


% \bbot\ performs not only weed management but also field monitoring.
One of the key benefits of \bbot\ as a platform is that it can monitor the state of the field while traversing it, this supplements its key function as a weeding platform.
We demonstrate the potential of \bbot\ for field monitoring by illustrating how the extra localization sensors can enhance the existing tracking algorithms in our prior work~\cite{halstead2021crop}.
This approach used Mask-RCNN to provide instance-based segmentation, species-level information (e.g. crop and weed species) and the viewable surface area. % from the RGB-D camera.
This approach included a tracking-via-segmentation technique that outlined the benefit of of spatial matching operator, coined dynamic radius (DR), over a pixel-wise version, referred to as intersection over union (IoU).
This technique also exploited re-projection between frames using wheel odometry and camera parameters.
This enabled more accurate tracking of objects in the scene, however, wheel odometry is often prone to errors.
Using the extra GPS sensors available on \bbot\ has the potential to increase performance by re-projecting more accurately between frames. %, especially over large frame skips.
% The strength of the approach lied in being able to estimate how far the camera had moved, for instance Halstead~et~al. used wheel odometry, to re-project information between frames and conduct better object tracking as well as counting.
% Despite the success of the proposed approach with wheel odometry, the extra localization information (e.g. GPS and IMU) available on \bbot\ should further enhance such an approach.

In this paper we demonstrate that the extra localization sensors on \bbot\ can be used to enhance the performance of field monitoring.
In particular, we fuse the available odometery and GPS information with an EKF~\cite{wei2011intelligent}.
This algorithm recursively estimates the state of a non-linear system in an optimal way.
Furthermore, adding local source of motion estimation can considerably reduce the risk of outage due to lack of proper satellite observations~\cite{ahmadi2021towards}.

\begin{comment}
The classic EKF method consists of two steps: the prediction and the correction. 
In prediction step the algorithm tries to predict future state of the system based on motion model, while the correction step improves predicted state's accuracy using real measurements from sensors. 
Using this principle, the state of the system can be determined recursively in real time.
In our system, the wheel odometry is used as control data~$u_t=(d_t^{t+1}, w_t)$ in the prediction step and position and orientation of the embedded EKF solution for the SBG system provides corrections.
As the robot velocity is always controlled with a differential controller model within crop-rows we express its kinematics model in x-y plane as:
% 
\begin{equation}
    \begin{array}{lr}
        x_{t+1} = x_t + d_t^{t+1} \cdot cos(\theta_{t+1}), \\
        y_{t+1} = y_t + d_t^{t+1} \cdot sin(\theta_{t+1}). \\
        \theta_{t+1} = \theta_t + w_t
    \end{array}
    \label{eq:ekf_motionModel}
\end{equation}
% 
where $d_t^{t+1}$ is circular arc traveled within $t$ to $t+1$ and $w_t^{t+1}$ is the rotation angle around z-axis.
So, let the vector $\Vec{\mathbf{X}}_t = [x_t, y_t, z_t, \phi_t, \psi_t, \theta_t]^T$ be the state vector of the \bbot\ at time $t$.
where $x_t, y_t, z_t$ denote the current position in \textit{ENU} coordinate frame, and $\phi_t, \psi_t, \theta_t$ specify roll, pitch and yaw angles of the robot in world coordinate frame $\mathcal{F}_w$.
\end{comment}

In our prior work~\cite{halstead2021crop} we concentrated on sweet pepper in a horticultural setting and sugar beet in arable farmland.
In both cases the objects we aim to detect are somewhat robust to external influences such as weather conditions.
However, some of the weed species witnessed in the arable farmland were grasses and their accurate localization proved difficult.
In this paper we further outline the ability for our approach to be crop agnostic by performing monitoring on a novel grass crop dataset consisting of corn.
Corn has a long leaf structure which makes it susceptible to weather conditions resulting in a difficult crop to localize, due to this, in~\secref{subsec:cropWeedMonitoring} we outline our performance using both the pixel-wise and spatial matching criteria from~\cite{halstead2021crop}.
% \TODO{MAH this needs some work, also missing reference:
% In our prior work~\cite{halstead2021crop}, some of the most challenging plants were grasses and so we propose to evaluate improvements to field monitoring (via improved localization with more sensors) on a grass crop. 
% In particular, our evaluations for field monitoring in Section~\ref{subsec:cropWeedMonitoring} are conducted on corn fields which is a grass crop that has a thin and long leaf structure which makes it more difficult to track.
% }

This corn data set (CN20) was acquired using \bbot\ from a phenotyping field at campus Klein-Altendorf (CKA) of the University of Bonn.
The data was captured using an Intel RealSense D435i sensor with a nadir view of the ground in front of the robot and resolution of $1280\times720$ with a frame-rate of $15Hz$. 
The non-overlapping training, validation, and evaluation data includes RGB-D frames for $170$, $43$ and $70$ images respectively.
This data comes from six different rows providing unique crop and weed distributions due to the non-homogeneous growth stage of the weeds.
% The data comes with $283$ RGB and depth frames covering six rows of crop providing a large distribution of crops sizes due to non-homogeneous growth stage and several weed types.
In total there are nine different categories of weeds containing a total of $2566$ and $1261$ instances of crop and weeds respectively. 
The data is annotated to include instance based pixel-wise segmentation, bounding boxes and stem locations of each instance in Coco format~\cite{lin2014microsoft}.
% To train out DNN-based monitoring system we divided the images of dataset in to sets of training, validation and evaluation of size $170$, $43$ and $70$, respectively.
% $149$, $39$ and $44$,
\figref{fig:datasetSample} shows an example annotated image of CN20 dataset.

% Figure environment removed 
