

We equipped BonnBot-I with a range of sensors to perform both in-field weed management and crop monitoring.
There are two sets of sensors, the first set is for ``localization and navigation'' and the second set is for ``robotic vision''.
%This infrastructure enables the platform the achieve state-of-the-art capabilities in arable fields to perform critical tasks like: autonomous navigation, localization, and environment perception encouraging selective and plant-level real-time interventions.
%The sensors range from intertial measurement units (IMUs) and GNSS sensors for accurate localization through to 2D and 3D vision sensors such as RGB-D cameras and lidar; we will expand on the specific components here.
%Below we briefly these describe these components.

\subsubsection{Localization}
\label{subsubsec:navigation}

The localization of BonnBot-I is performed with a compact Inertial Navigation System (INS), Ellipse2-D SBG Systems~\cite{sbg} which includes an IMU and a dual-antenna receiver, multi-band GNSS receivers fixed at the front and back of the robot at height of $1.85m$ above the ground.
%Benefiting an on-board high-frequency Extended-Kalman filter fusion of IMU and GPS data providing a horizontal and vertical position accuracy of $4cm$ and $3cm$, respectively. 
Using an on-board high-frequency extended-Kalman filter fusion of IMU and GPS data provides us with a horizontal and vertical position accuracy of $4cm$ and $3cm$ respectively. 
Furthermore, the heading of the platform can be determined with an accuracy of $0.1^{\circ}$ and $0.3^{\circ}$ in roll-pitch and yaw directions respectively.
%
% To provide a homogeneous view of the environment we added an Ouster-OS1 lidar to the front. %at the front of the robot. 
% The OS-1 lidar is a 64 beam lidar covering $360$ and $45$ degree field of view in the horizontal and vertical directions, with a maximum range of $120m$.
%degrees horizontal and vertical field of views with maximum range of $120m$ meters.
% This provides long-range sparse depth data to to enable 3D mapping, navigation, and for safety.

%Even-though, in \cite{ahmadi2021towards} we showed how BonnBot-I is able to automatically traverse multi-crop-row field without any human intervention and independent of any global localization service using only two symmetrically fixed Intel RealSense-D435i in front and back of the robot denoted by $C_{f}$ and $C_b$ in \figref{fig:robot}.

% Additionally, to enable automated row-crop field traversing we use two RGB-D Intel RealSense-D435i cameras, one on front and one on the back of the platform with a fixed tilt angle.
% In~\cite{ahmadi2021towards} we showed that this sensor configuration could reliably navigate a field regardless of the number of crop-rows under the robot or the crop type; this was achieved without using any global localization service. 

%we showed that this sensor configuration could be used for automatic navigation in arable fields with diverse crop types as well as multiple crop-rows achieving reliable performance without using any global localization service.

\subsubsection{Robotic Vision}
\label{subsubsec:detectionInference}
BonnBot-I is equipped a nadir-view cameras (Intel Real-sense D455) on the front of the platform used for monitoring purposes.
% \RNum{1} \textit{JAI Camera:}
% The AD-130GE is a Rolling shutter CCD multi-spectral GigE Vision compliant camera. 
% This device employs 2 CCD sensors, one for Bayer color and the other for NIR monochrome utilizing prism optics so that the AD-130GE can inspect the objects by visible color sensor and Near IR sensor with the same angle of view.
% \RNum{2} \textit{Real-sense D455 Camera:}
The Real-sense D455 is a global shutter camera which provides RGB and registered depth images at $15 Hz$.
On \bbot\ it is fixed at a height of $0.78m$ providing a view-able area of $1.4m\times0.78m$ covering the gap between the two front wheels. %, with a $15 Hz$ frame-rate. 
This is the sensor used to perform field monitoring.
% This RGB-D camera is the primary sensor to perform field monitoring.
%used to perform field monitoring
%This sensor obtains depth information from two NIR cameras. 
%It has a NIR projector that is used to project points onto the scene so that depth can be estimated on visually texture-less surfaces such as walls.

% As this is not a problem with the setting for an agricultural robot to avoid interference with other sensors (e.g. NIR channel of the JAI camera) we disable the projector and consider this to be an RGB+D+NIR sensor.