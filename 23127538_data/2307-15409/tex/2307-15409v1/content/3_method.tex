\section{Methodology}
\label{sec:method}

\subsection{Overview}
\label{sec:method_fmwk}

As shown in~\cref{fig:method_fmwk}, the proposed unsupervised MOT framework is trained with the widely-used contrastive learning technique~\cite{chen2020simple,he2020momentum}. 
\lk{Specifically, for multi-object tracking}, objects within the tracklet ($\boldsymbol{k}_{+}$) should be pulled together and different tracklets ($\boldsymbol{k}_{-}$) should be separated. It can be mathematically formulated as:

\begin{equation}
% \begin{split}
    \mathcal{L}_{cl}( \boldsymbol{q}; \boldsymbol{k}_{+}; \boldsymbol{k}_{-} )= 
    - \log \frac{\exp(\boldsymbol{q} \cdot \boldsymbol{k}_{+} / \epsilon)}{\sum_{i}\exp(\boldsymbol{q} \cdot \boldsymbol{k}_{i} / \epsilon)}
  \label{eq:method_nce}
% \end{split}  
\end{equation}

\noindent where $\mathcal{L}_{cl}$ denotes the InfoNCE~\cite{oord2018representation} loss function, and $\epsilon$ is the temperature hyper-parameter~\cite{wu2018unsupervised}. 
Within a video, following the unsupervised tracking fashion~\cite{liu2022online,shuai2022id}, the positive and negative keys mainly come from two sources, \ie pseudo-labeled historical frame and self-augmented frame. 

\lk{However, two issues occur: (1) the uncertainty reduces the accuracy of pseudo-tracklets and (2) the randomly augmented samples fail to learn the inter-frame consistency.} 
We argue the above issues are not independent. 
\lk{By leveraging the uncertainty in turn,} the accurate pseudo-tracklets can guide the qualified positive and negative augmentations.

To address these two issues, we propose an uncertainty-aware pseudo-tracklet labeling strategy in \cref{sec:method_uoap}, which integrates a verification-and-rectification mechanism into the tracklet generation. Our method significantly improves the accuracy of pseudo-identities, especially in long-term interval. 
Then we propose a tracklet-guided augmentation strategy in \cref{sec:method_ada_aug}, which brings the temporary information into spatial augmentation. The augmented samples simulates the objects' motion. A hierarchical uncertainty-based sampling strategy is proposed for hard sample mining. More details are described in the following section.


\subsection{Uncertainty-aware Tracklet-Labeling}
\label{sec:method_uoap}

Accurate pseudo tracklet is critical in \liuk{learning feature consistency}. 
However, without manual annotation, \lk{the aggravated uncertainty makes} the tracklet-labeling a huge challenge due to various interference factors, including similar appearance among objects, frequent object cross and occlusions, \etc. 
\lk{In fact, the uncertainty can also be leveraged to improve the pseudo-accuracy in turn.}
In this section, we propose an \textbf{U}ncertainty-aware \textbf{T}racklet-\textbf{L}abeling (\textbf{UTL}) strategy for better pseudo-tracklets.

Given an input video sequence $V = \{I^{1}, I^{2}, \cdots, I^{N}\}$, each frame $I^{t}$ is annotated with the bounding boxes $B^{t} = \{b_{1}^{t}, b_{2}^{t}, \cdots, b_{M^{t}}^{t}\}$ of $M^{t}$ objects in $t_{th}$ frame, where $b_{i}^{t} = (cx_{i}^{t}, cy_{i}^{t}, w_{i}^{t}, h_{i}^{t})$ is the center coordinate and shape of the $i_{th}$ object $o_{i}^{t}$. As shown in~\cref{fig:method_fmwk}, \mywork~generates accurate pseudo-tracklets in four main steps:

1) \textbf{Association}. For a certain object $o_{i}^{t}$ in frame $I^{t}$, the $\ell_2$-normalized representation $\boldsymbol{f}_{i}^{t}$ can be expressed as $\boldsymbol{f}_{i}^{t} = {\phi}(I^{t}, b_{i}^{t})$, 
% \begin{equation}
%   \boldsymbol{f}_{i}^{t} = {\phi}(I^{t}, b_{i}^{t})
%   % / {\Vert {\phi}(I^{t}, b_{i}^{t}) \Vert}_{2}
%   \label{eq:method_feat}
% \end{equation}
where the embedding encoder is denoted as $\phi$.

To associate the objects in frame $I^{t}$ with the objects or trajectories in previous $I^{t \minus 1}$, a similarity matrix is constructed with their appearance embeddings:

\begin{equation}
  \boldsymbol{C} \in \mathbb{R}^{M^{t} \times M^{t \minus 1}}, \;
  c_{i,j} = {\boldsymbol{f}_{i}^{t}} \cdot  \boldsymbol{f}_{j}^{t \minus 1}
  \label{eq:method_matrix}
\end{equation}

\noindent where $c_{i,j}$ represents the cosine similarity between the $i_{th}$ object in frame $I^{t}$ and the $j_{th}$ object (or trajectory) in frame $I^{t \minus 1}$. Then the Hungarian algorithm~\cite{kuhn1955hungarian} is adopted to generate the identity association results.

2) \textbf{Verification}. However, the appearance representations are sometimes unreliable, especially in the unsupervised scenario. To solve this issue, an uncertainty metric is proposed to evaluate the association after the first stage.

% For an object $o_{i}^{t}$ in frame $I^{t}$, the similarities against the $M^{t \minus 1}$ objects in the previous frame can be expressed as:

% \begin{equation}
%   \boldsymbol{s}_{i} = \boldsymbol{C}_{i} = [c_{i,1}, c_{i,2}, \cdots, c_{i,M^{t \minus 1}}]^T
%   \label{eq:method_svec}
% \end{equation}

% Inspired by margin-based OOD detection~\cite{hendrycks2016baseline}, we assume that the assignment ($o_{i}^{t} \!\sim\! o_{j}^{t \minus 1}$) in the association stage is not convincing under the following circumstances:

% \begin{itemize}
%     \setlength{\itemsep}{0pt}
%     \item The assigned similarity between $o_{i}^{t}$ and $o_{j}^{t \minus 1}$ is relatively low (\ie, $c_{i,j} < m_1$).
%     \item The second-highest similarity with others ($c_{i,j_{2}}$) is close to the assigned $o_{j}^{t \minus 1}$ (\ie, $c_{i,j} - c_{i,j_{2}} < m_2$).
% \end{itemize}

% Based on these assumptions, we define an association-level uncertainty metric, which is formulated as:



Object association can be viewed as multi-category classification.
And confidence-score has been proved efficient and effective on detecting mis-classified examples~\cite{hendrycks2016baseline}.
Inspired by this, we propose to detect the mis-associated objects through the similarity-scores.


Given an object $o_{i}^{t}$ associated with $o_{j}^{t \minus 1}$ in the previous frame based on \cref{eq:method_matrix}, the association ($o_{i}^{t} \!\sim\! o_{j}^{t \minus 1}$) is unconvincing in two cases: 
1) the assigned similarity $c_{i,j}$ is relatively low (\eg, partial occlusion or motion blur) and 
2) there are other objects whose similarities are close to the assigned $c_{i,j}$ (\eg, similar appearance or indistinguishable embedding).
It can be formulated as:

\begin{equation}
  c_{i,j} < m_1; \quad c_{i,j_{2}} > c_{i,j} - m_2
  \label{eq:method_margin}
\end{equation}


\noindent 
where $m_1,m_2$ are constant margins. Only the second-highest similarity with others ($c_{i,j_{2}}$) is considered for simplicity.
In an ideal association, $c_{i,j}$ should be close to 1 and $c_{i,j_{2}}$ close to 0.
We thus proposed to estimate the association \lk{risk} as:

% \sigma_{i,j} = - \left( 
% \log c_{i,j} + \log \left( 1 - c_{i,j_{2}} \right)
% + \overline{\log \left( 1 - c_{i,l} \right) }
% \right)  
\begin{equation}
  \sigma_{i,j} = - \log c_{i,j} - \log \left( 1 - c_{i,j_{2}} \right)
  \label{eq:method_energy}
\end{equation}

Detailed derivation process refers to the supplementary materials.
Combining with \cref{eq:method_margin} and \cref{eq:method_energy} , an adaptive threshold is proposed:

\begin{equation}
  % \gamma_{i,j} = -\log \left( 1 + m_2 - c_{i,j} \right) -\log m_1 \left( 1 - m_3 \right)
  \gamma_{i,j} =  -\log m_1 - \log \left( 1 + m_2 - c_{i,j} \right)
  \label{eq:method_border}
\end{equation}

As shown in~\cref{fig:method_verify}, when the \lk{risk} $\sigma_{i,j}$ is higher than the threshold $\gamma_{i,j}$, the assignment ($o_{i}^{t} \!\sim\! o_{j}^{t \minus 1}$) should be re-considered. 
\lk{The \textbf{association uncertainty} is quantified as:}

\begin{equation}
  \delta_{i,j} = \sigma_{i,j} - \gamma_{i,j}
  \label{eq:method_uncertain}
\end{equation}

The results are not sensitive to the exact margins. We set $m_1 = 0.5$ and $m_2 = 0.05$ for a slightly better performance.
% More experimental details are shown in the supplementary materials.

The uncertain pairs after the verification stage and unmatched objects after the association stage are gathered as uncertain candidates for the rectification stage.


3) \textbf{Rectification}. 
The rectification stage is performed among the uncertain candidate. The similarities between two adjacent frames are no longer convincing.
% due to irregular motion, severe occlusion, and so on. 
More information should be taken into account, including motion \lk{estimation} and appearance \lk{variation} within a tracklet. 
% Specifically, intersection-over-union (IoU)~\cite{bewley2016simple} is the widely-used motion metric. At the same time, the tracklet embeddings can provide complementary appearance information.

For the uncertain candidates, \mywork~constructs another similarity matrix for the secondary rectification. 
First, \lk{the motion constraints should be relaxed}, so the association shares overlap \lk{higher than} $\beta$ 
% in adjacent frames 
\lk{are preserved}. 
Second, \lk{the appearance should not vary extremely fast}, so we adopt the averaged similarity between object $o_{i}^{t}$ and tracklet $trk_{j} = \{o_{j}^{t \minus K}, \cdots, o_{j}^{t \minus 1}\}$ within previous $K$ frames. 
In this stage, we solve the sub-problem of global identity assignments, which can be formulated as:

\begin{equation}
\begin{split}
  \boldsymbol{C}^\prime \in \mathbb{R}^{{M^{t}}^\prime \times {M^{t \minus 1}}^\prime} & \\
  c^\prime_{i,j} = \left( \frac{1}{K} \sum_{\hat{t} = t \minus K}^{t \minus 1} {\boldsymbol{f}_{i}^{t}} \cdot  \boldsymbol{f}_{j}^{\hat{t}} \right) 
            \times \mathbb{I} & \left( \text{IoU} \left( b_{i}^{t}, b_{j}^{t \minus 1} \right) > \beta \right) 
  \label{eq:method_recti}
\end{split}
\end{equation}

\noindent where $\mathbb{I}(*)$ is the indicator function. Then the match set is updated based on the Hungarian algorithm.

\lk{
\textit{Remark.} Our core contribution is the uncertainty-based verification mechanism, rather than the specific rectification, which shall be adjusted in practice. Empirically we set $\beta=0.1$ and $K=5$.
}

% Figure environment removed

4) \textbf{Propagation}. The pseudo-tracklets are propagated frame-by-frame. As shown in~\cref{fig:method_reidacc}, our strategy brings \lk{consistently} accurate pseudo-identities, \lk{\eg, reaching 97\% accuracy across 100 frames}.
% The pseudo-tracklets are progressively updated during the training process.
The long-term intra-tracklet consistency is successfully maintained.
% by the accurate pseudo-identities.

It is worth mentioning that the \lk{verification and rectification} stages can be naturally applied to the inference process to boost the performance, \lk{which does not conflict with existing association methods}.

\subsection{Tracklet-Guided Augmentation}
\label{sec:method_ada_aug}

The accurate pseudo-tracklets can guide the sample augmentation in the unsupervised MOT framework.
To learn the \liuk{inter-frame consistency}~\cite{chen2020simple,zhang2021fairmot}, good training samples should be diverse and \liuk{temporal-aware}. 
However, as illustrated in~\cref{fig:method_ada_aug}, existing methods usually treat augmentation and multi-object tracking as two isolated tasks, leading to ineffective augmentations. 
Instead, this paper utilizes the tracklet's spatial displacements to guide the augmentation process. 
According to a properly selected anchor pair, the proposed strategy makes the augmented frames aligned to the historical frames, simulating realistic tracklet movements. The proposed method concurrently focuses on the hard negative samples.
Details \lk{of the \textbf{T}racklet-\textbf{G}uided \textbf{A}ugmentation (TGA)} are given below.

% Figure environment removed

We introduce the temporal information into spatial transformation. 
First, given a current frame $I^{t}$ with $M^{t}$ objects, we select a source-anchor object $o_{a}^{t}$, whose bounding box is denoted as $b_{a}^{t} = (cx_{a}^{t}, cy_{a}^{t}, w_{a}^{t}, h_{a}^{t})$. Then, we choose a target-anchor $o_{a}^{t \minus \tau}$ in $(t \minus \tau)_{th}$  historical frame from the pseudo-tracklet $trk_{a} = \{o_{a}^{t_0}, o_{a}^{t_1}, \cdots, o_{a}^{t}\}$. 
Finally, to augment the current $I^{t}$ to align with historical $I^{t \minus \tau}$,  a tracklet-guided affine transformation can be expressed as:

\begin{equation}
  \begin{bmatrix}
      x^{t \minus \tau} \\ y^{t \minus \tau} \\ 1
  \end{bmatrix}
  =
  \boldsymbol{M}_{t}^{t \minus \tau}
  \begin{bmatrix}
      x^{t} \\ y^{t} \\ 1
  \end{bmatrix}
  =
  \begin{bmatrix}
      m_{11} & m_{12} & m_{13} \\
      m_{21} & m_{22} & m_{23} \\
      0      & 0      & 1
  \end{bmatrix}
  \begin{bmatrix}
      x^{t} \\ y^{t} \\ 1
  \end{bmatrix}
  \label{eq:method_affine}
\end{equation}

\noindent where $x^*,y^*$ are spatial coordinates, and $\boldsymbol{M}_{t}^{t \minus \tau}$ can be solved by direct linear transform (DLT) algorithm ~\cite{detone2016deep}. 
% with the corner locations of the anchor pair $(o_{a}^{t} \!\sim\! o_{a}^{t \minus \tau})$. 
Then an augmented frame $\tilde{I}^{t}$ is generated based on the tracklet-guided affine transformation with perspective jitter, which can be expressed as $\tilde{I}^{t} = \mathcal{T}\left(I^{t}, M_{t}^{t \minus \tau} \right)$.
% \begin{equation}
%   \tilde{I}^{t} = \mathcal{T}\left(I^{t}, M_{t}^{t \minus \tau} \right)
%   \label{eq:method_aug}
% \end{equation}

Intuitively, a proper anchor-selection is vitally important for our augmentation strategy. 

First, the identity accuracy of anchor pair $(o_{a}^{t} \!\sim\! o_{a}^{t \minus \tau})$ is important. In other words, the consistency of anchor tracklet $trk_{a}$ should be guaranteed. We thus design a tracklet-level uncertain metric based on the propagated association-level uncertainty defined in \cref{eq:method_uncertain}, which is formulated as:

\begin{equation}
  \Omega_{i} = \frac{1}{n} \sum_{s=t_0}^{t} \exp (\delta_{i}^{s})
  % \Omega_{i} = \sqrt[n]{\sigma_{i}^{t_0} \cdot \sigma_{i}^{t_1} \cdots \sigma_{i}^{t}}
  \label{eq:method_tenergy}
\end{equation}

\noindent where $\Omega_{i}$ represents the uncertainty of tracklet $trk_{i}$, \lk{and $n$ is the tracklet length}.
An uncertainty-based sampling strategy is designed to select the source anchor $o_{a}^{t}$ (along with the anchor $trk_{a}$) from the $M^{t}$ objects in frame $I^{t}$, which can be formulated as:

\begin{equation}
  p\left(a=i \mid t \right) 
  % = softmax\left(-\Omega_{i}\right)
  = \frac{\exp{\left(-\Omega_{i}\right)}}{\sum_{\hat{i}=1}^{M^{t}}\exp{\left(-\Omega_{\hat{i}}\right)}}
  \label{eq:method_sel_an_src}
\end{equation}

\noindent where $p\left(a=i \mid t \right)$ represents the probability to choose the $i_{th}$ tracklet $trk_{i}$ as the anchor $trk_{a}$.
The uncertain tracklet with high $\Omega$ is less likely to be selected, avoiding dramatic augmentations from erroneous pseudo-tracklets.

Second, hard negative samples matters in discriminablity learning. We tend to choose an indistinguishable (or, high uncertain) target anchor $o_{a}^{t \minus \tau}$ along the tracklet $trk_{i}$. The selection probability can be formulated as:

\begin{equation}
  p\left(\pi=t \minus \tau \mid a \right) 
  = \frac{\exp{\left(\delta_{a}^{t \minus \tau}\right)}}{\sum_{\hat{\tau}=t_0}^{t-1}\exp{\left(\delta_{a}^{t-\hat{\tau}}\right)}}
  \label{eq:method_sel_an_tgt}
\end{equation}

\lk{A visualization example are displayed in the supplementary material to illustrate the hierarchical sampling process.}

Compared with conventional random transformation, the proposed tracklet-guided augmentation is well-directed and tracking-related. 
\lk{Together with accurate pseudo-tracklets, \mywork~successfully improves the inter-frame consistency, as shown in \cref{fig:method_disc_vis}. }


% Figure environment removed

% \subsection{Momentum Memory Dictionary}
% \label{sec:method_md}


%To reuse the encoded samples from the intermediate mini-batches, we maintain a queue for each video in the memory dictionary by enqueueing the $M^{t}$ objects in the current frame and removing the oldest samples.
%In representation learning, high-quality negative samples play an essential role~\cite{chen2020simple,he2020momentum}. However, existing unsupervised trackers only take negative samples from adjacent frames, augmented frames, and the current frame itself. The lack of negative sample diversity prevents trackers from learning discriminative representations. \mywork~adopts a momentum dictionary mechanism to alleviate this problem.

%As shown in~\cref{fig:method_fmwk}, we build a memory dictionary for each \textit{independent} video input during training. Given an input image $I^{t}$ from video $V$, we randomly sample a number of negative object samples from other videos in the memory dictionary, so as to enrich the negative sample diversity. To reuse the encoded samples from the intermediate mini-batches, we maintain a queue for each video in the memory dictionary by enqueueing the $M^{t}$ objects in the current frame and removing the oldest samples.