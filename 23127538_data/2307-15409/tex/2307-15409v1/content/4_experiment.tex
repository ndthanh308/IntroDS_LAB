\section{Experiment}
\label{sec:exp}

\subsection{Datasets and Evaluation Metrics}
\label{sec:exp_setup}

\textbf{Datasets.}
 Experiments are performed on three common MOT benchmarks: MOT17~\cite{milan2016mot16}, MOT20~\cite{dendorfer2020mot20}, and the challenging VisDrone-MOT~\cite{zhu2021visdrone}.  MOT17 contains 14 videos captured from diverse viewpoints and weather conditions, while MOT20 consists of 8 videos on crowded scenes with heavier occlusion. \lk{Both of them are evaluated under the ``private detection'' protocol}. 
VisDrone-MOT is captured by UAVs in various scenes, which comprises 79 sequences with 10 object categories. Only five common categories (\ie, car, bus, truck, pedestrian, and van)~\cite{liu2022multi} are considered during evaluation. \lk{The multi-class tracking with the irregular camera motion makes VisDrone-MOT a chellanging benchmark.}

\textbf{Metrics.} Following the previous MOT methods~\cite{li2022unsupervised,zhang2021fairmot,zhang2022bytetrack}, the HOTA~\cite{luiten2021hota} and CLEAR~\cite{bernardin2008evaluating} metrics are adopted to evaluate the trackers. Specifically, CLEAR mainly includes multiple object tracking accuracy (MOTA), ID F1 score (IDF1), and identity switches (IDS).

\subsection{Implementation Details}
\label{sec:exp_impl}

To show the effectiveness of our proposed unsupervised MOT framework, we implement~\mywork~\lk{on YOLOX~\cite{ge2021yolox}} with a ReID head integrated.
Specifically, the detection branch is trained in the same way as ByteTrack~\cite{zhang2022bytetrack}, while the new ReID head is learned with our proposed \mywork. 
The model is trained with SGD optimizer and the initial learning rate of $1 \times 10^{-3}$ \lk{with cosine annealing schedule}.

\lk{
For VisDrone-MOT, \mywork~is trained for 40 epochs, with no extra dataset added. A pre-trained UniTrack~\cite{wang2021different} ReID model is added for ByteTrack to handle the multi-class multi-object tracking~\cite{zhang2022bytetrack}.
For MOT17 and MOT20, the model is trained with the same setting as ByteTrack. Taking MOT20 for example, we train \mywork~for 80 epochs with an extra CrowdHuman~\cite{shao2018crowdhuman} dataset. 
% Specifically, \mywork~can also apply TGA on discrete images (\eg, CrowdHuman) according to the tracklet movements from continuous sequences (\eg, MOT20). Data is fully utilized.
The input image size is $1440 \times 800$ for MOT-challenges and $1600 \times 896$ for VisDrone-MOT.
}

\lk{Identity labels are unused in ALL training datasets.}


\begin{table}[tb]
    \centering
    \small
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{c|lc|cccc}
        \toprule
        Dataset & Tracker & \textit{Sup.} & HOTA$\uparrow$ & MOTA$\uparrow$ & IDF1$\uparrow$ & IDS$\downarrow$  \\  % & FPS$\uparrow$
        \midrule
        \multirow{12}{*}{MOT17} 
        & TrkFormer~\cite{meinhardt2022trackformer} & \CheckmarkBold & 57.3 & 74.1 & 68.0 & 2829 \\
        & MOTR~\cite{zeng2021motr} & \CheckmarkBold & 57.8 & 73.4 & 68.6 & 2439 \\
        & TraDeS~\cite{wu2021track} & \CheckmarkBold & 52.7 & 69.1 & 63.9 & 3555 \\
        & CorrTrack~\cite{wang2021multiple} & \CheckmarkBold & 60.7 & 76.5 & 73.6 & 3369 \\
        & MTrack~\cite{yu2022towards} & \CheckmarkBold & 60.5 & 72.1 & 73.5 & 2028 \\
        % \cline{2-7}
        & OUTrack~\cite{liu2022online} & \XSolidBrush & 58.7 & 73.5 & 70.2 & 4122 \\
        & PointID~\cite{shuai2022id} & \XSolidBrush & $-$ & 74.2 & 72.4 & 2748 \\
        & UEANet~\cite{li2022unsupervised} & \XSolidBrush & 62.7 & 77.2 & 77.0 & \topb{1533} \\
        & ByteTrack~\cite{zhang2022bytetrack} & \XSolidBrush & \topb{63.1} & \topa{80.3} & \topb{77.3} & 2196 \\
        & \textbf{\mywork~}(Ours) & \XSolidBrush & \topa{64.2} & \topb{79.7} & \topa{78.2} & \topa{1506} \\
        \midrule
        \multirow{7}{*}{MOT20} 
        & CorrTrack~\cite{wang2021multiple} & \CheckmarkBold & $-$ & 65.2 & 69.1 & 5183 \\
        & MTrack~\cite{yu2022towards} & \CheckmarkBold & 55.3 & 63.5 & 69.2 & 6031 \\
        % \cline{2-7}
        & OUTrack~\cite{liu2022online} & \XSolidBrush & 56.2 & 68.6 & 69.4 & 2223 \\
        & UEANet~\cite{li2022unsupervised} & \XSolidBrush & 58.6 & 73.0 & \topb{75.6} & 1423 \\
        & ByteTrack~\cite{zhang2022bytetrack} & \XSolidBrush & \topb{61.3} & \topa{77.8} & 75.2 & \topa{1223} \\
        & \textbf{\mywork~}(Ours) & \XSolidBrush & \topa{62.7} & \topb{77.1} & \topa{76.2} & \topb{1379} \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Performance comparison against SOTA trackers on MOT-Challenge test sets.} 
    % `Pub.\&Year' denotes the paper publisher and publication years. 
    % `id-label' indicates whether the tracker is trained with identity labels or not. 
    `$\uparrow$'/`$\downarrow$' indicates higher/lower values are better, respectively. 
    \topa{Bold} numbers are superior results. %and \topb{bold} numbers are the second highest results. 
    }
    \label{tab:res_mot}
\end{table}



% \begin{table}[tb]
%     \centering
%     \small
%     \setlength{\tabcolsep}{1pt}
%     \begin{tabular}{c|lc|cccc}
%         \toprule
%         Dataset & Tracker & \textit{Sup.} & mMOTA$\uparrow$ & mIDF1$\uparrow$ & MOTA$\uparrow$ & IDF1$\uparrow$  \\  % & FPS$\uparrow$
%         \midrule
%         \multirow{5}{*}{VisDrone} 
%         & MOTR~\cite{zeng2021motr}  & \CheckmarkBold & $-$ & $-$  & 22.8 & 41.4  \\
%         & TrkFormer~\cite{meinhardt2022trackformer}  & \CheckmarkBold & $-$ & $-$ & 24.0 & 30.5  \\
%         & UavMOT~\cite{liu2022multi}  & \CheckmarkBold & $-$ & $-$ & 36.1 & 51.0  \\
%         & ByteTrack~\cite{zhang2022bytetrack} & \XSolidBrush& $-$ & $-$ & 52.3 & 68.3 \\
%         % \cline{2-7}
%         & \textbf{\mywork~}(Ours) & \XSolidBrush & $-$ & $-$ & \topa{52.3} & \topa{69.0} \\
%         \midrule
%         \multirow{7}{*}{BDD100K} 
%         & bdd100k~\cite{yu2020bdd100k} & \CheckmarkBold & 26.3 & 44.7 & 58.3 & 68.2 \\
%         & DeepBlueAI & \CheckmarkBold & 31.6 & 38.7 & 56.9 & 56.0 \\
%         & madamada & \CheckmarkBold & 33.6 & 43.0 & 59.8 & 55.7 \\
%         & QDTrack~\cite{pang2021quasi} & \CheckmarkBold & 35.5 & 52.3 & 64.3 & 72.3 \\
%         & ByteTrack~\cite{zhang2022bytetrack} & \CheckmarkBold & 40.1 & 55.8 & 69.6 & 71.3 \\
%         % \cline{2-7}
%         & \textbf{\mywork~}(Ours) & \XSolidBrush &  & \topb{} & \topa{} & \topb{} \\
%         \bottomrule
%     \end{tabular}
%     \caption{\textbf{Performance comparison against SOTA trackers on challenging benchmarks.} 
%     Both of VisDrone~\cite{zhu2018vision} and BDD100K~\cite{yu2020bdd100k} require multi-class multi-object tracking with large camera motions.
%     }
%     \label{tab:res_mcmot}
% \end{table}

\begin{table}[t]
    \centering
    \small
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{lc|ccccc}
        \toprule
        Method & \textit{Sup.}  & MOTA$\uparrow$ & IDF1$\uparrow$ & IDS$\downarrow$ & FPS$\uparrow$ \\
        \midrule
        MOTR~\cite{zeng2021motr}  & \CheckmarkBold  & 22.8 & 41.4 & 959 & 7.5 \\
        TrkFormer~\cite{meinhardt2022trackformer} & \CheckmarkBold & 24.0 & 30.5 & 4840 & 7.4 \\
        UavMOT~\cite{liu2022multi}  & \CheckmarkBold & 36.1 & 51.0 & 2775 & 12.0 \\
        ByteTrack~\cite{zhang2022bytetrack} & \XSolidBrush & 52.3 & 68.3 & 1232 & 11.4 \\
        \textbf{\mywork~}(Ours) & \XSolidBrush & \topa{52.3} & \topa{69.0} & \topb{1052} & \topa{19.4} \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Performance on VisDrone-MOT test-dev set.}}
    \label{tab:res_mcmot}
\end{table}

\subsection{Main Results}
\label{sec:exp_result}
\textbf{MOT-Challanges}. 
Evaluated by the official server, the results on MOT17 and MOT20 benchmarks are illustrated in \cref{tab:res_mot}, which shows \mywork~beats all of the SOTA supervised and unsupervised methods \lk{on HOTA and IDF1 metrics}. Specifically, it outperforms the SOTA unsupervised UEANet~\cite{li2022unsupervised} by a large margin (\eg, 1.2\% HOTA on MOT17). 
With the assist of ReID head, \mywork~consistently performs better in terms of HOTA and IDF1 against ByteTrack~\cite{zhang2022bytetrack}.
\lk{However, the IDS increases on MOT20. It is mainly because the extracted feature embedding is naturally biased in such severe scenarios, which causes the close similarities among occluded objects. The embedding-based unsupervised methods (including our \mywork) are inferior to this problem, leading to the IDS increase. Some occlusion-aware optimizations~\cite{he2020guided,yan2021occluded} might alleviate this problem.
% ByteTrack only use the motion clues (\ie, the Kalman Filter) for tracking, which won't suffer from the appearance confusion in severe occlusions, but our \mywork~will. 
% Some occlusion-aware optimizations~\cite{he2020guided,yan2021occluded} shall alleviate this problem.
}
% , which demonstrates appearance representation still plays an essential role in multi-object tracking besides motion clues (\ie, the Kalman Filter). 
\liuk{Besides, the MOTA of \mywork~is slightly decreased, which implies that the competition between detection and re-identification tasks should be further explored.} 

In addition, \mywork~does not involve network structure evolution, so the performance gains brought by \mywork~is uncorrelated with those enhancement modules proposed by advanced trackers in~\cref{tab:res_mot}. Combining \mywork~with these methods would lead to even better tracking performance.


\textbf{VisDrone-MOT}.
For the videos captured in UAV views, the IoU information (or motion model) is unreliable due to the irregular camera motion. 
To deal with this issue, camera motion compensation~\cite{zhang2022bytetrack} and objects' positional relation~\cite{liu2022multi} are mainly adopted, which are effective but computational expensive. This work provides another solution, \lk{\ie, using an uncertainty metric and relaxed motion constraints for refined association results, which is robust to large camera motion as well as low frame rate}.
As shown in \cref{tab:res_mcmot}, \mywork~ substantially outperforms all the comparison methods,
\lk{including ByteTrack, who shares the same detector with our~\mywork~while utilizes the pre-trained self-supervised UniTrack model~\cite{wang2021different} for ReID. Also without identity annotation, the IDF1 gains demonstrate our learned task-specific appearance embedding head beats the pre-trained model.}
\lk{Besides, the improved FPS mainly comes from \mywork's jointly-trained ReID head, rather than an extra ReID model that requires another thorough inference on the raw images.}
\lk{The results demonstrate the effectiveness and efficiency of our proposed \mywork~tracker.}



\subsection{Ablation Studies}
\label{sec:exp_ablation}

In this section, we conduct extensive ablation studies to elaborate on the effectiveness of the proposed approach. Following the previous methods~\cite{zhang2021fairmot,li2022unsupervised,wu2021track}, the first half of each video of the MOT17 training set is used for training, and the second half is for validation. All the models are trained for 30 epochs.
\lk{Beside the results below, we also conduct ablation studies by training and testing on separate videos with cross-validation. The conclusion remains unchanged. More details refer to the supplementary material.}


\begin{table}[t]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        Method & HOTA$\uparrow$ & MOTA$\uparrow$ & IDF1$\uparrow$ & IDS$\downarrow$  \\
        \midrule
        baseline & 63.40 & 73.73 & 74.51 & 207 \\
        % +ExtraN  & 63.54 & 73.81 & 74.69 & 205 \\
        % \midrule
        +LTD & 63.43 & 73.74 & 74.64 & 202 \\
        +UTL & 63.84 & 73.78 & 75.19 & 203 \\
        +TGA & \topa{64.08} & 73.79 & \topa{75.42} & \topa{197} \\
        \midrule
        \textit{supervised}     & \textit{63.96} & \textit{73.79} & \textit{75.32} & \textit{196} \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Evaluation of the proposed modules.} 
    % `\textit{id-label}' means ground-truth identity labels are used.
    }
    \label{tab:ablation_component}
\end{table}


% Figure environment removed

\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{cc|cccc}
        \toprule
        TGA-src & TGA-tgt & HOTA$\uparrow$ & MOTA$\uparrow$ & IDF1$\uparrow$ & IDS$\downarrow$  \\
        \midrule
        $-$ & $-$    & 63.84 & 73.78 & 75.19 & 203 \\
        \midrule 
        random    & random    & 63.93 & 73.79 & 75.31 & 204 \\
        uncertain & random    & 63.92 & 73.77 & 75.36 & 194 \\
        random    & uncertain & 64.01 & 73.79 & 75.35 & 201 \\
        uncertain & uncertain & \topa{64.08} & 73.79 & \topa{75.42} & \topb{197} \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Ablation on the anchor-selection mechanism in TGA (\cref{sec:method_ada_aug}).} The ``$-$'' indicates TGA is not applied.
    }
    \label{tab:ablation_aug}
\end{table}

\textbf{Effectiveness of the modules proposed in \mywork.} 
\lk{Our unsupervised framework} proposes two major components: uncertainty-aware tracklet-labeling (UTL) and tracklet-guided augmentation (TGA).
To evaluate each component, we conduct an ablation study on the tracking performance. The results are shown in \cref{tab:ablation_component}. 
We first construct a baseline model by training on adjacent frames. \lk{To introduce long-term dependency (LTD), the vanilla} similarity-based association on historical frames is conducted for pseudo identities. However, it results in \lk{negligible gains in terms of HOTA and MOTA} due to the noisy pseudo-labels, \lk{meanwhile the IDF1 and IDS obtain slight increases}.
Instead, the proposed UTL strategy improves the tracking performance in \lk{most of the metrics (\eg, 0.4\% HOTA)}, which evidences the fact that long-term temporal consistency is well preserved. 
Finally, the TGA strategy results in increases of 0.2\% HOTA and 0.2\% IDF1, \lk{as well as decreased IDS}, demonstrating that our task-specific augmentation \lk{assists in learning} the inter-frame consistency.
Equipped with the proposed components, unsupervised \mywork~\lk{even achieves better HOTA and IDF1} against the identity-supervised model, which \lk{validates the effectiveness of our method and indicates the potential to leverage large-scale unlabeled videos}. % TODO: the latter may not be mentained

\textbf{Uncertain margins.} 
\lk{Since the uncertainty metric is vital, we investigate the performance variance caused by different uncertainty margins when verifying the associations. As shown in \cref{fig:abl_uncertain}, different combinations of $m_1,m_2$ consistently improve the tracking performance. And the improvement is relatively not sensitive to the exact value of these two hyper-parameters. 
It indicates that wrong associations usually occur in candidates with comparable similarities and relatively lower confidence, which are possible to be filtered out and rectified.
We choose $m_1=0.5,m_2=0.05$ for a slightly better performance.}


\textbf{Augmentation strategies.} 
\lk{The customized tracklet-guided augmentation is mainly explored in \cref{tab:ablation_aug}, where the hierarchical uncertainty-based anchor-sampling mechanism is further evaluated.
First, TGA benefits the tracking performance even with totally random anchor-selections.
Meanwhile, the selected source anchor tracklet with low-uncertainty avoids dramatic transformation, which brings a slight decrease on IDS. Since most of the pseudo-tracklets are accurate enough after the training, this mechanism mostly serves as stabilizing the training in early stages.
Moreover, the selected target anchor object with high-uncertainty along the tracklet brings qualified hard negative examples, leading to 0.1\% HOTA increase.
Ultimately, the combined hierarchical uncertainty-based anchor-sampling mechanism results in better performance, demonstrating the effectiveness of TGA.}



\begin{table}[t]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        Tracker & HOTA$\uparrow$ & MOTA$\uparrow$ & IDF1$\uparrow$ & IDS$\downarrow$  \\
        \midrule
        \textbf{\mywork}   & 64.08 & 73.79 & 75.42 & 197 \\
        \textbf{+UTL}       & \topa{64.90} & \topa{74.09} & \topa{76.66} & 212 \\
        \midrule
        ByteTrack  & 63.32 & 73.72 & 74.32 & 207 \\
        \textbf{+UTL}       & \topa{64.90} & \topa{74.09} & \topa{76.66} & 212 \\
        \midrule
        FairMOT    & 62.03 & 72.65 & 72.83 & 618 \\
        \textbf{+UTL}       & \topa{64.01} & \topa{73.35} & \topa{75.05} & \topa{427} \\
        \midrule
        DeepSORT   & 58.48 & 70.81 & 66.20 & 526 \\
        \textbf{+UTL}       & \topa{59.75} & \topa{70.97} & \topa{67.60} & \topa{512} \\
        \midrule
        MOTDT      & 60.49 & 71.95 & 69.87 & 622 \\
        \textbf{+UTL}       & \topa{61.46} & \topa{72.55} & \topa{71.40} & \topa{353} \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Inference boosting.} Results are obtained by different association strategies with the SAME model.}
    \label{tab:ablation_infer_boost}
\end{table}

\textbf{Inference boosting.} The proposed uncertainty-aware tracklet labeling (UTL) strategy does not conflict with existing matching strategies. On the contrary, combined with our method, existing methods achieve better tracking performance. As shown in ~\cref{tab:ablation_infer_boost}, \lk{we first set our method as the comparison baseline, which simply adds ReID embeddings to the ByteTrack~\cite{zhang2022bytetrack}, and our UTL can thus be equipped. Besides, we integrate UTL to other three popular MOT trackers, including FairMOT~\cite{zhang2021fairmot}, DeepSORT~\cite{wojke2017simple}, and MOTDT~\cite{chen2018real}. It shows that the UTL consistently boosts all of these trackers by a large margin on most of the metrics, especially in HOTA and IDF1. The IDS of FairMOT and MOTDT is significantly decreased. The training-free UTL shows its effective and generalized application prospects. }

Some typical visualization results are shown in \cref{fig:visualize}, which is consistent with ~\cref{tab:ablation_infer_boost}. 
First, when IoU information is unreliable in irregular camera motions, our method is robust to spatial prediction noise with the uncertainty-based verification stage. 
Second, in the rectification stage, the tracklet appearance embedding provides important supplementary information to confront the \lk{transient occlusions}.


% Figure environment removed

