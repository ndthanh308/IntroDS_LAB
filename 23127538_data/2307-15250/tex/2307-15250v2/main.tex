
\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{svg}
\usepackage{caption}
\usepackage{multirow}
\usepackage{graphics}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{mathtools}
% \usepackage{ulem}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}  
\SetKwRepeat{Do}{do}{while}%
% *** CITATION PACKAGES ***
%



\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "% Figure environment removed


\section{Proposed Approach} \label{proposed_method}
\textbf{Motivation}: Sparse keypoints associated with descriptors are the most widely-used fundamental elements in visual localization problems. These descriptors are used to generate a sparse map through structure from motion (SfM) or visual SLAM and are subsequently used for localization by finding the nearest descriptor positions. Although various methods are available for solving this search problem \cite{sattler2016efficient, liu2017efficient, sarlin2019coarse, sarlin2021back}, direct end-to-end learning from the descriptor to its 3D position is yet to be explored in this research field. This is because individual descriptors contain limited contextual awareness of the entire map, making learning directly from them impractical. Additionally, although an image frame often contains thousands of keypoint descriptors, only a subset of them are projections of salient 3D points or robust features. Therefore, the formulation of a direct learning method for 2D--3D correspondences must adhere to certain physical constraints: \textcolor{red}{i)} several descriptors may be unreliable for localization because they belong to dynamic objects or are the result of detector failures, \textcolor{red}{ii)} individual descriptors may have minimal or no significance to the entire image and global environment, and \textcolor{red}{iii)} latent descriptors may have a distribution variance associated with the changes in the environment over time since the map was created.  We formulated D2S (see Figure. \ref{fig:network_architecture}) as a learnable method for 2D--3D correspondences using single image descriptors. Our proposed approach offers a robust and lightweight alternative to sparse SfM or visual SLAM-based 3D models. 



\textbf{Formulation}: Given a set of images $\{\mathbf{I}^{i}_{\mathcal{T}}\}_{i=1}^{n}$ and its reconstructed SfM model $\mathcal{E}$,  we aim to develop a sparse regression module, which can encode the entire environment $\mathcal{E}$ using a compact function $\mathfrak{F}(.)$, where $\mathfrak{F}$ is a deep neural network. The proposed function $\mathfrak{F}(.)$ inputs local descriptors $\mathbf{D}^{i}$ extracted from $\mathbf{I}^{i}$ and outputs its corresponding 3D global cloud coordinates $\mathbf{W}^{i}$. The ultimate goal of the proposed module is to perform \textit{visual localization}, a task of estimating camera pose $\mathbf{T} \in \mathbb{R}^{4\times4}$ of the query image $\mathbf{I}_{q}$.

\textbf{Overview}: To unfold the constraints of unordered sets and different reliable descriptors, constraint \textcolor{red}{i}, we initially propose a straightforward regression module capable of accurately generating 3D positions for individual descriptors, as well as 1D channels representing reliability probabilities (Section \ref{simple_function}). As the relationship between all image descriptors is crucial in understanding the implied global visual and geometric context, as stated in constraint \textcolor{red}{ii}, we propose a middle-optimization of D2S using a graph-attention module to learn both global and local context (Section \ref{graph_attension}). Although this module is similar to the works of feature matching \cite{sarlin2020superglue, sun2021loftr, jiang2021cotr}, it is simpler and more lightweight, and thereby more suitable for the scene regression task. Given a successful representative method of scene coordinates and visual descriptors, we design an extension algorithm for learning with any unlabeled observations (Section \ref{unlabel_learning}). This can make D2S adopt the variance of descriptors from a free data source, constraint \textcolor{red}{iii}. As the nature of the proposed self-supervised learning in Section \ref{unlabel_learning} can be further applied for data augmentation, we present this procedure in Section \ref{aumgent_pipe}. The loss functions used to train the proposed system are described in Section \ref{loss_function}. Finally, we use PnP+RANSAC \cite{kukelova2013real,chum2003locally}, as in previous works \cite{li2020hierarchical, brachmann2021visual}, to estimate the camera pose (Section \ref{pose_estimation}).


\subsection{Representing Descriptors and Scene Coordinates as D2S} \label{simple_function}

Inspired by the universal continuous set function approximator \cite{qi2017pointnet,bui2022fast}, we first propose a simple element set learning function $\mathfrak{F}(.)$. It receives a set of descriptors $\{\mathbf{d}^{i}\}_{i=0}^{k}$ as input and outputs corresponding scene coordinates $\{\mathbf{w}^{i}\}_{i=0}^{k}$. The descriptors extractor can be a hand-crafted or learning-based method such as SIFT \cite{lowe2004distinctive} or SuperPoint \cite{detone2018superpoint}, respectively. Overall, the proposed function can be described as follows: 


\begin{equation}
\begin{aligned}
\mathfrak{F}\left(\{\mathbf{d}^{i}\}_{i=1}^{k}\right) & = \left\{f(\mathbf{d}^{i})\right\}_{i=1}^{k}\\
& = \{\mathbf{w}^{i}\}_{i=1}^{k},
\label{ori_equation}
\end{aligned}
\end{equation}

where $\mathfrak{F}:\mathbb{R}^{K \times D} \to \mathbb{R}^{K \times 4}$ and $f:\mathbb{R}^{D} \to \mathbb{R}^{4}$. $K$ is the number descriptors, and $D$ is its description dimension. The function $f(.)$ is a shared nonlinear function. It receives a descriptor vector $\mathbf{d} \in \mathbb{R}^{D}$ and outputs a scene coordinate vector $\mathbf{w} = (x,y,z,p)^T$. We introduce an additional dimension $p$ for the scene coordinate, which represents the reliability probability for localization of the input descriptor $\mathbf{d}$. To ensure that the range of the reliability probability output lies within $[0,1]$ while enabling function $f(.)$ to produce $p\in \mathbb{R}$, we compute the final prediction for $\hat{p}$ using the following equation: 

\begin{equation}
\begin{aligned}
\hat{z} = \frac{1}{1+|\beta p|} \in (0,1],
\end{aligned}
\label{reliable_equation}
\end{equation}

where $\beta$ is a scale factor chosen to make the expected value of reliability prediction $\hat{z}$ easy to reach a small value when the input descriptors belong to high uncertainty regions. 


The proposed module is theoretically simple. As illustrated in Eq. \ref{ori_equation}, it requires only a non-linear function $f$ to compute the scene coordinates of given descriptors.  In practice, we approximate $f$ using a multi-layer perceptron (MLP) network. This approach has been shown to work well in experiments, particularly for small-scale indoor environments.  




\subsection{Optimizing D2S with Graph Attention} \label{graph_attension}
In the previous section, we described the core component for regressing scene coordinates from sparse descriptors. Although this simple component is advantageous, it is insufficient for achieving state-of-the-art accuracy, as demonstrated in Section \ref{graph_attention_results}. This is due to the nature of local descriptors, which are designed to describe local regions or patches within a single frame. Therefore, relying solely on individual descriptors to regress scene coordinates can result in a lack of understanding of the global context and the distinctiveness of several feature clusters. To address this issue, we draw inspiration from recent works on feature matching \cite{sarlin2020superglue, sun2021loftr, jiang2021cotr}, which leverage attention aggregation \cite{vaswani2017attention} to learn both local and global attributes of sparse features. We employ a simplified version for the scenes regression task, which can be described as follows:


\begin{equation}
\begin{aligned}
\mathfrak{F}\left(\{\mathbf{d}^{i}\}_{i=1}^{k}\right) & = f \circ  \mathcal{A}^{l}\circ... \mathcal{A}^{1}\left(\{\mathbf{d}^{i} \}_{i=1}^{k}\right)  \\
& = \{\mathbf{w}^{i}\}_{i=1}^{k},
\end{aligned}
\label{ori_grap_re}
\end{equation}

where $\mathcal{A}:\mathbb{R}^{K \times D} \to \mathbb{R}^{K \times D}$, and $\mathcal{A}^{l}\circ... \mathcal{A}^{1}(.)$ represents a nested residual attention update for each descriptor $\mathbf{d}^{i}$ across $l$ graph layers. Given a coarse local descriptor $\mathbf{d}^{i}$, $\mathcal{A}^{l}(.)$ is used to compute its fine \textit{regression 
descriptor} $\prescript{(l)}{}{\mathbf{d}^{i}}$  by letting it communicate with each other across $l$ attention layers. This implicitly incorporates visual relationships and contextual cues with other descriptors in the same image, enabling it to extract robust features automatically through an end-to-end learning pipeline from scene coordinates. In addition, the proposed reliability-aware detection, as described in Eq. \ref{reliable_equation}, can also encourage our attention module to favor features from reliable areas (e.g. buildings, stable objects) and suppress those from unreliable regions (e.g. trees, sky, human). 



As $\prescript{(l)}{}{\mathbf{d}^{i}}$ is the representation for the output \textit{regression descriptor} of element $i$ at layer $l$, Eq. \ref{ori_grap_re} can be simplified as follows: 

\begin{equation}
\begin{aligned}
\mathfrak{F}(\{\mathbf{d}^{i}\}_{i=1}^{k}) & = \left\{f(\prescript{(l)}{}{\mathbf{d}^{i}}) \right\}_{i=1}^{k},
\end{aligned}
\end{equation}

which now has a similar form to the original Eq. \ref{ori_equation}, where the final regression layer $f$ is simply a shared MLP. 

Similar to a previous study \cite{sarlin2020superglue}, we also consider the attentional module as a multi-layer graph neural network. However, we only leverage the self-edge, which is based on self-attention \cite{vaswani2017attention}, to connect the descriptor $i$ to all others in the same image. The message passing formulation \cite{gilmer2017neural} for element $i$ at the layer $(l+1)$ is described as

\begin{equation}
    \prescript{(l+1)}{}{\mathbf{d}^{i}}  = \prescript{(l)}{}{\mathbf{d}^{i}} + MLP\left(\left[\prescript{(l)}{}{\mathbf{d}^{i}} || \mathbf{m}_{\mathfrak{I}\rightarrow i}\right]\right),
\end{equation}



where $\mathbf{m}_{\mathfrak{I}\rightarrow i}$ denotes the aggregated message result from all descriptors $\{\prescript{(l)}{}{\mathbf{d}^{i}}\}_{i=1}^{k}$, with  $\mathfrak{I}=\{1,...,k\}$ indicating the set of descriptor indices, and $[.||.]$ denoting concatenation. This module consists of $L$ chained layers with different parameters. Therefore, the message $\mathbf{m}_{\mathfrak{I}\rightarrow i}$ is different in each layer. 

The self-attention mechanism performs an interaction to map the query vector $\mathbf{q}_{i}$ against a set of key vectors $\{\mathbf{k}_{j}\}_{j \in \mathfrak{I}}$, associated with candidate descriptors, to compute the attention score $\alpha_{ij}=$ Softmax$_{j}(\mathbf{q}_{i}^{T}\mathbf{k}_{j})$. It then presents the best-matched descriptors with their value vectors $\{\mathbf{v}_{j}\}_{j \in \mathfrak{I}}$ represented by a scored average of values, which in practice is the computation of message $\mathbf{m}_{\mathfrak{I}\rightarrow i}$:


\begin{equation}
\begin{aligned}
\mathbf{m}_{\mathfrak{I}\rightarrow i} = \sum_{j:(i,j)\in\mathfrak{I}}{\alpha_{ij}\mathbf{v}_{j}}
\end{aligned}
\end{equation}



The query, key, and value are derived from linear projections between the descriptor $\prescript{(l)}{}{\mathbf{d}}$ with three different weight matrices $\mathbf{W}_{q}$, $\mathbf{W}_{k}$, and $\mathbf{W}_{v}$. This linear projection for all descriptors is expressed as



\begin{equation}
\begin{aligned}
% \mathbf{q} = \mathbf{W}_{q}\mathbf{d} + \mathbf{b}_{q}  \\
\begin{bmatrix}
\mathbf{q} \\
\mathbf{k} \\
\mathbf{v} 
\end{bmatrix} = \begin{bmatrix}
\mathbf{W}_{q} \\
\mathbf{W}_{k} \\
\mathbf{W}_{v} 
\end{bmatrix} \prescript{(l)}{}{\mathbf{d}} + \begin{bmatrix}
\mathbf{b}_{q} \\
\mathbf{b}_{k} \\
\mathbf{b}_{v}
\end{bmatrix}.
\end{aligned}
\end{equation}

Note that the weight parameters are different in each graph layer. In practice, we also applied multi-head attention \cite{vaswani2017attention} in each layer to improve the expressivity from different representation subspaces of different weight parameters. Fig. \ref{fig_attention_1} presents an example result of self-attention on predicted robust descriptors.


% Figure environment removed


\subsection{Updating D2S with Unlabeled Observations} \label{unlabel_learning}

\begin{algorithm}[t]
% \DontPrintSemicolon
  
  \KwInput{$\mathcal{T} $, $\mathcal{D}_{\mathcal{T}}$, $\mathcal{W}_{\mathcal{T}}$, $\mathcal{U}$\\
  \begin{itemize}
    \item[-] $\mathcal{T} = \{\mathbf{I}^{i}_{\mathcal{T}}\}_{i=1}^{n}$ training images,
    \item[-] $\mathcal{D}_{\mathcal{T}}=\{\mathbf{D}^{i}_{\mathcal{T}}\}_{i=1}^{n}$ training descriptors,
    \item[-] $\mathcal{W}_{\mathcal{T}} = \{\mathbf{W}^{i}_{\mathcal{T}}\}_{i=1}^{n}$ training scene coordinates, 
    \item[-] $\mathcal{U}=\{\mathbf{I}^{i}_{\mathcal{U}}\}_{i=1}^{m}$ unlabeled images,
\end{itemize}
  }
  \KwOutput{ $ \mathcal{A}=\{\mathbf{D}^{i}_{\mathcal{U}}, \mathbf{W}^{i}_{\mathcal{U}}\}_{i=1}^{v}$
    \begin{itemize}
    \item[-] $\mathcal{D}_{\mathcal{U}}=\{\mathbf{D}^{i}_{\mathcal{U}}\}_{i=1}^{m}$ unlabeled descriptors, 
    \item[-] $\mathcal{W}_{\mathcal{U}} = \{\mathbf{W}^{i}_{\mathcal{U}}\}_{i=1}^{m}$ pseudo scene coordinates,
    \item[-] $ v \leq m$.
\end{itemize}}
    $\mathcal{D}_{\mathcal{U}} \leftarrow$ ExtractDescs$(\mathcal{U});$ \\
    $i:=1; \mathcal{A} \leftarrow \{\};$ \\
    \Do{$i \leq m, i++$}{
      $\mathbf{ids}_{\mathcal{T}} \leftarrow$ NearestVLAD$(\mathbf{I}_{\mathcal{U}}^{i}, \mathcal{T});$ \# nearest indices. \\
      $\mathcal{M} \leftarrow$ Matching$(\mathbf{D}_{\mathcal{U}}^{i}, \mathcal{D}_{\mathcal{T}}^{ \mathbf{ids}_{\mathcal{T}}});$ \\
      $s, \mathbf{W}^{i}_{\mathcal{U}} \leftarrow $ Copy$(\mathcal{M}, \mathcal{W}_{\mathcal{T}}^{\mathbf{ids}_{\mathcal{T}}});$ \\ 
      \If{$s \geq 50$}{
        $\mathcal{A} \leftarrow \mathcal{A} \cup \{\mathbf{D}^{i}_{\mathcal{U}}, \mathbf{W}^{i}_{\mathcal{U}}\};$ 
        }
        
    }
    

\caption{Pseudo-labelling}
\label{algo1}
\end{algorithm}


% Figure environment removed



Due to the significant changes that can occur in environments over time since map creation, the ability to update from unlabeled data is crucial for most localization systems. Previous SCR approaches \cite{brachmann2018learning, li2020hierarchical, zhou2020kfnet, brachmann2021visual} are offline methods, where the learned network weights are fixed after training. In addition, these methods require absolute camera poses, camera parameters, and pixel-level scene coordinates to supervise the network. However, obtaining this information often requires reconstructing the 3D map. Therefore, updating the system with new observations is challenging. In this section, we introduce a straightforward algorithm to update the proposed D2S with additional unlabeled data in a self-supervised learning manner without the need to reconstruct the 3D map. Fig. \ref{fig:unlabel_learning} illustrates the proposed data flow for updating D2S with available unlabeled data. 


Given a set of training images $\mathcal{T}$ and its labeled database of correspondent 2D--3D descriptors $\mathcal{D}_{\mathcal{T}}$ and $\mathcal{W}_{\mathcal{T}}$, we aim to find a set of pseudo-labels for new observation images $\mathcal{U}$. As D2S receives descriptors and produces world coordinates, the pseudo-labels accordingly denote the newly generated 2D--3D correspondences of the new data $\mathcal{U}$. In detail, we first extract the local descriptors of all images in $\mathcal{U}$, which can be denoted as $\mathcal{D}_{\mathcal{U}}$. For each query image $\mathbf{I}_{\mathcal{U}}^{i}$, the top 10 nearest training images are listed in $\mathcal{T}$ before matching the local descriptors. In this searching step, image retrieval NetVLAD\cite{arandjelovic2016netvlad} is used to determine the most similar images, which is described in line 4 of the algorithm \ref{algo1}. Subsequently, for each matching pair, we simply match two descriptor sources using a specific matching algorithm such as the nearest neighbor search. In particular, recent advancements in keypoints matching methods, such as SuperGlue [1] and LoFTR [2], can serve as a reliable tool for generating matching keypoints between two images. Thus, we used SuperGlue and the nearest neighbor method to evaluate our algorithm. 


Finally, we merged the top 10 matching pairs by copying their ground truth world coordinates from $\mathcal{T}$ to $\mathcal{U}$. This step is shown as line 6 of the algorithm \ref{algo1}, where the $\mathbf{W}^{i}_{\mathcal{U}}$ is the obtained pseudo scene coordinates of descriptor matrix $\mathbf{D}^{i}_{\mathcal{U}}$ and $s$ is the number of valid world coordinates. Only the obtained pseudo-labels are added to the final training source with $s \geq 50$. The whole process of generating pseudo-labels for unlabeled data is summarized in algorithm \ref{algo1}. 



\subsection{Data Augmentation} \label{aumgent_pipe}

General approaches for data augmentation include color changing, rotation, distortions, and cropping. Applying these methods to the training data can enrich the DNN model with unseen data. However, in our study, these augmentation techniques could affect the camera pose and latent descriptors. Using algorithm \ref{algo1}, we can derive a simple procedure for data augmentation of D2S with any transformation technique. To achieve this outcome, we transform a given image $\mathbf{I}_{\mathcal{T}}$ by changing brightness, cropping, or distorting, which result in a new image $\mathbf{I}^{'}_{\mathcal{T}}$. Thereafter, we simply assume that these two images are nearby and apply algorithm \ref{algo1} for the matching and copying operations (lines 5 and 6). This simple augmentation step successively improves performance, as explained in Section \ref{experiments}.

% \usepackage{graphicx}
% \usepackage{multirow}


% \usepackage{multirow}

% Figure environment removed

\subsection{Loss Function} \label{loss_function}
According to Eq. \ref{ori_equation}, the output of sparse scene coordinates $\mathbf{w}$, predicted by D2S, includes four variables $(x,y,z,p)^{T}$. This output is rewritten as $(\hat{\mathbf{y}},p)^{T}$, where $\hat{\mathbf{y}}$ is the abbreviation of the estimated world coordinates vector and $p$ is the reliability probability. The loss function to minimize the estimated scene coordinates can be defined as 
\begin{equation}
    \mathcal{L}_{m} = \frac{1}{N}\sum^{N}\sum_{i=1}^{K} z_{i}\lVert \mathbf{y}_{i}-\hat{\mathbf{y}}_{i} \lVert^{2}_{2},
\label{loss_m}
\end{equation}

where $N$ is the number of mini-batch sizes, $K$ is the number of descriptors in the current frame, and $\mathbf{y}_{i}$ is the ground truth scene coordinate. We multiply the difference between estimated and ground truth world coordinates by $z_{i} \in \{0,1\}$, which is the ground truth reliability of descriptors. The goal is to ensure that the optimization focuses on robust descriptors while learning to ignore unreliable descriptors. 

Because the loss Eq. \ref{loss_m} is used to optimize only the robust features, we propose an additional loss function to simultaneously learn to be aware of this assumption, as follows:

\begin{equation}
    \mathcal{L}_{u} = \frac{1}{N}\sum^{N}\sum_{i=1}^{K}  \lVert z_{i}- \frac{1}{1+|\beta p|} \lVert^{2}_{2}.
\end{equation}

As described in Section \ref{simple_function}, $\beta$ is a scale factor chosen to enable the optimization to focus on the descriptors belonging to high uncertainty regions or failures of the detector. The combination of $\mathcal{L}_{m}$ and $\mathcal{L}_{u}$ can result in a stable optimization on two different criteria of robust descriptors. In practice, we chose the factor value as $\beta=100$ for successively learning to classify unreliable descriptors and focus on robust features (refer to Fig. \ref{fig_attention_1} for an example result).

In addition, as the camera poses are known for the training data, we also apply the re-projection loss for each 2D descriptor's position. The aim of this loss function is to align the general scene coordinates following the correct camera rays. This loss function can be described as follows:

\begin{equation}
    \mathcal{L}_{r} = \frac{1}{N}\sum_{j=1}^{N}\sum_{i=1}^{K}z_{i}\lVert \pi(\mathbf{R}_{j} \mathbf{y}_{i}+\mathbf{t}_{j}) - \mathbf{u}_{i}\lVert^{2}_{2},
    \label{reproject_loss}
\end{equation} 

where $\mathbf{R}_{j}$ is the rotation matrix and $\mathbf{t}_{j}$ is the translation vector of camera pose $j$. $\pi(.)$ is a function to convert the estimated 3D camera coordinates to 2D pixel position, with $u_{i}$ as its ground truth pixel coordinate. 

Finally, these loss functions are combined with the scale factor as follows: 


\begin{equation}
    \mathcal{L}=\alpha_{m}\mathcal{L}_{m}+\alpha_{u}\mathcal{L}_{u}+\alpha_{r}\mathcal{L}_{r}
    \label{total_loss}
\end{equation}
\subsection{Camera Pose Estimation} \label{pose_estimation}
The proposed method directly learns to compress 2D--3D correspondences into a D2S network. A sparse set of scene coordinates is estimated based on D2S with its corresponding reliability values. To estimate the camera pose, we filtered out the most robust predicted scene coordinates based on its reliability. The visualization results are illustrated in Fig. \ref{Uncertainties_results}. Moreover, a robust minimal solver (PnP\cite{kukelova2013real}+RANSAC\cite{chum2003locally}) is used, followed by a Levenberg--Marquardt-based nonlinear refinement, to compute the camera pose. 


In contrast to DSAC$^{*}$ \cite{brachmann2021visual} and several SCR-based works \cite{brachmann2017dsac, brachmann2018learning, li2020hierarchical}, which only focus on estimating dense 3D scene coordinates of the input image, our D2S can simultaneously focus on the detection of salient keypoints of the scene and output its coordinates. We believe that the SCR network should learn to pay more attention to salient features (the most crucial solution to localization problems); this can lead to a better localization performance \cite{do2022learning, altillawi2022pixselect, sarlin2021back}. Furthermore, directly learning scene coordinates from an RGB image may result in decreased performance when the test images differ significantly from training images. This finding is supported by empirical evidence presented in Section \ref{outdoor_results_BKC}. Notably, the proposed D2S demomnstrates robustness against these challenges. 




\section{Experiments} \label{experiments}



% Figure environment removed





We evaluate the proposed re-localization method on four different datasets: two indoor and two outdoor datasets. First, we present the experimental settings used in our experiments, including network architecture, datasets, and competitors (Section \ref{ex_settings}). Second, we report the results of the two different categories: indoor and outdoor environments, in Sections \ref{indoor_results} and \ref{outdoor_results_BKC}, respectively. 
%Finally, we present an extensive ablation study to further characterize the proposed methods in Section \ref{ab_study}. 

\subsection{Experimental Settings} \label{ex_settings}
\subsubsection{Settings}
First, we describe the network architecture used in our experiments. As described in Section \ref{simple_function} and Section \ref{graph_attension}, the proposed network includes two different parts: a graph attention module and a shared multi-layer perception (MLP). For the graph module, we used five different layers $L=5$, starting from coarse descriptors extracted by any specific local feature extractors (e.g. SIFT \cite{lowe2004distinctive} and SuperPoint \cite{detone2018superpoint}). At each graph layer, we used the same number of four attention heads. The second module is an MLP with the following setting: $MLP(D, 512, 1024, 1024, 512, 4)$, where $D$ is the number of dimensions of the input descriptors. Our architecture for regressing sparse scene coordinates incurs a parameter count that depends on the local feature dimension. For example, the SuperPoint \cite{detone2018superpoint} descriptor, which has 256 dimensions, uses 5.5 million parameters, whereas only 3.0 million parameters are required for SIFT descriptors, which have 128 dimensions. However, in this study, we primarily experiment with the proposed method using SuperPoint features, due to its posterior performance compared with other local features \cite{detone2018superpoint, sarlin2020superglue, sarlin2019coarse}. We also verified this result by conducting experiments with other local feature extractors (see Section \ref{different_local_extractors}). 

Second, we present the hyperparameters and implementation details of our experiments. We used Pytorch \cite{paszke2019pytorch} to implement the proposed method and trained it with a mini-batch size of eight images. The training of D2S was separated into two steps. We first trained the network with 300K iterations, where the hyperparameters of the loss Eq. \ref{total_loss} are established as follows: $\alpha_{m} = 1$, $\alpha_{u} = 1$, and $\alpha_{r} = 0$. In this step, we used a learning rate of $10^{-4}$ with the Adam optimizer \cite{kingma2014adam}. The learning rate reduces with a decay of 0.5 for every one-fourth of the total iterations. In the second training step, we trained the network with another 100K iterations, with a smaller learning rate of $10^{-5}$, where the $\alpha_{r}$ is increased to $\alpha_{r}=10$. The aim of the second training step is to enhance the network to focus on the re-projection loss Eq. \ref{reproject_loss}, which depends on the ground truth camera poses. For data augmentation, we applied the algorithm \ref{algo1} and the method described in Section \ref{aumgent_pipe} to obtain the pseudo-labels of augmented descriptors. In particular, we used Pytorch \cite{paszke2019pytorch} transformation to randomly adjust the brightness and contrast of the input images within the range $[0.5, 1.5]$ and $[0, 0.5]$ respectively. We also applied a random perspective transformation to the training images with a probability of 0.3 and a distortion scale of 0.4. Finally, for camera pose estimation using the predicted sparse scene coordinates, we used PnP+RANSAC implemented in pyColmap \cite{pycolmap} with an inlier threshold of 12 px.

For the self-supervised updating of the model with unlabeled data, we applied Algorithm \ref{algo1} to generate pseudo-labels for the unlabeled descriptors. We then updated the pre-trained models with an additional 50K iterations. In this step, we used $\alpha_{m} = 1$, $\alpha_{u} = 1$, and $\alpha_{r} = 1$. In each mini-batch, we sampled half of the training data and half of the unlabeled data. This step ensures that the updating is not biased toward only the unlabeled data.


\subsubsection{Datasets} \label{sec_datasets}
We evaluated the proposed pipeline on two indoor and two outdoor camera re-localization datasets. For the indoor environment, we used a standard small-scale 7-Scenes dataset \cite{shotton2013scene}, and the recently released Indoor6 \cite{do2022learning}. For the outdoor environment, we used the Cambridge Landmarks \cite{kendall2015posenet} dataset. However, we observed that the performance of various camera localization methods is already saturated on two benchmarks of 7-Scenes and Cambridge\cite{kendall2015posenet, brachmann2017dsac,brachmann2018learning,bach2022featloc,chen2022dfnet, zhou2020kfnet}. In addition, the properties of these datasets are not completely suitable to evaluate scene-specific generality and availability of unlabeled observations. Therefore, we proposed a new outdoor dataset named Ritsumeikan BKC for the evaluation of our hypothesis on generality and the proposed self-supervised pipeline. Details of the datasets used in our experiments are as follows: 

\textbf{7-Scenes \cite{shotton2013scene}:} An RGB-D dataset of seven small-scale indoor environments ranging in size from $1m^{3}$ to $18m^{3}$. This dataset features challenges such as repeating structures, motion blur, and texture-less areas. Each scene consists of several thousand RGB images recorded by KinectFusion \cite{newcombe2011kinectfusion}, which also provides the ground truth for camera poses. The depth channels and camera poses are available for generating the ground truth scene coordinates for our training of D2S. The training and validation sets are split by the authors. In addition, 7-Scenes is a stationary dataset of images captured under fixed illumination. Consequently, validating localization methods against the challenges of changing environments is difficult. 

% Figure environment removed

\begin{table*}
\centering
\caption{\textbf{7-scenes results.} We report the median errors in cm for position and degree ($^{\circ}$) for the orientation of the 7-scenes dataset. We also display the obtained recall for each method, which is the percentage of frames that have the error within the threshold of 5 cm and 5$^{\circ}$. The best results are in bold.}
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|cc|cc|cc|cc|cc|cc|cc} 
\hline
\multicolumn{16}{c}{7scenes}                                                                                                                                                                                                                                                                                                                                                           \\ 
\hline
\multicolumn{2}{c|}{\multirow{3}{*}{Method}} & \multicolumn{2}{c|}{Chess}                 & \multicolumn{2}{c|}{Fire}                  & \multicolumn{2}{c|}{Heads}                                  & \multicolumn{2}{c|}{Office}                & \multicolumn{2}{c|}{Pumpkin}               & \multicolumn{2}{c|}{Kitchen}               & \multicolumn{2}{c}{Stairs}               \\ 
\cline{3-16}
\multicolumn{2}{c|}{}                        & Err.                       & Acc.          & Err.                       & Acc.          & Err.                                         & Acc.         & Err.                       & Acc.          & Err.                       & Acc.          & Err.                       & Acc.          & Err.                    & Acc.           \\
\multicolumn{2}{c|}{}                        & (cm/deg.)                  & (\%)          & (cm/deg.)                  & (\%)          & (cm/deg.)                                    & (\%)         & (cm/deg.)                  & (\%)          & (cm/deg.)                  & (\%)          & (cm/deg.)                  & (\%)          & (cm/deg.)               & (\%)           \\ 
\hline
\multirow{3}{*}{APR} & MapNet\cite{brahmbhatt2018geometry}                & 9/3.24                     & -             & 20/9.29                    & -             & 12/8.45                                      & -            & 19/5.42                    & -             & 19/3.96                    & -             & 20/4.94                    & -             & 27/10.6                 & -              \\
                     & FeatLoc\cite{bach2022featloc}               & 7/3.66                     & -             & 17/5.95                    & -             & 10/7.57                                      & -            & 16/5.2                     & -             & 11/3.86                    & -             & 20/6.43                    & -             & 16/8.57                 & -              \\
                     & DFNet\cite{chen2022dfnet}                 & 5/1.88                     & -             & 17/6.45                    & -             & 6/3.63                                       & -            & 8/2.48                     & -             & 10/2.78                    & -             & 22/5.45                    & -             & 16/3.29                 & -              \\ 
\hline
\multirow{3}{*}{FM}  & AS\cite{sattler2016efficient}                    & 2.6/0.9                    & 86.2          & 2.3/1.02                   & 86.6          & 1.1/0.8                                      & 95.7         & 4.0/1.2                    & 65.9          & 6.5/1.7                    & 32.5          & 5.3/1.7                    & 46.2          & 3.8/1.1                 & 68.1           \\
                     & Inloc\cite{taira2018inloc}                 & 3/1.05                     & -             & 3/1.07                     & -             & 2/1.16                                       & -            & 3/1.05                     & -             & 5/1.55                     & -             & 4/1.31                     & -             & 9/2.47                  & -              \\
                     & Hloc\cite{sarlin2019coarse,sarlin2020superglue}                  & 2.4/0.77                   & 94.2          & 1.8/0.75                   & 93.7          & 0.9/0.59                                     & 99.7         & 2.6/0.77                   & 83.2          & 4.4/1.15                   & 55.1          & 4.0/1.38                   & 61.2          & 5.1/1.46                & 49.4           \\ 
\hline
\multirow{5}{*}{SCR} & SCoCR\cite{li2020hierarchical}                 & 2/0.7                      & 97.5          & 2/0.9                      & \textbf{96.7} & 1/0.9                                        & \textbf{100} & 3/0.8                      & 86.5          & 4/1.0                      & 59.9          & 4/1.2                      & 65.5          & \textbf{3/\textbf{0.8}} & \textbf{87.5}  \\
                     & KFNet\cite{zhou2020kfnet}                 & 1.8/0.65                   & -             & 2.3/0.90                   & -             & 1.4/0.82                                     & -            & 2.5/0.69                   & -             & 3.7/1.02                   & -             & 3.8/1.16                   & -             & 3.3/0.94                & -              \\
                     & NBE+SLD\cite{do2022learning}               & 2.2/0.75                   & 93.7          & 1.8/\textbf{\textbf{0.73}} & 94.1          & \textbf{0.9/}0.68                            & 96.6         & 3.2/0.91                   & 74.8          & 5.6/1.55                   & 44.6          & 5.3/1.52                   & 45.7          & 5.5/1.41                & 44.6           \\
                     & DSAC*\cite{brachmann2021visual}                 & 1.8/0.59                   & 97.8          & \textbf{\textbf{1.7/}}0.77 & 94.5          & 1.0/\textbf{\textbf{\textbf{\textbf{0.66}}}} & 98.8         & 2.7/0.79                   & 83.9          & 3.9/1.05                   & 62.0          & 3.9/1.24                   & 65.5          & 3.5/0.93                & 78.0           \\
                     & D2S (ours)            & \textbf{1.7/\textbf{0.57}} & \textbf{98.6} & 1.8/0.74                   & 92.9          & 1.2/0.75                                     & 98.7         & \textbf{2.1/\textbf{0.62}} & \textbf{91.3} & \textbf{2.9/\textbf{0.83}} & \textbf{72.8} & \textbf{3.0/\textbf{1.04}} & \textbf{77.9} & 13/2.02                 & 24.2           \\
\hline
\end{tabular}
}
\label{7scene_results}
\end{table*}



% Figure environment removed


% Figure environment removed





\textbf{Indoor6 \cite{do2022learning}:} In contrast to the 7-Scenes dataset, Indoor6 is a non-stationary dataset. The images are captured in much larger environments at different times and days and exhibit strong illumination variations. It contains six different indoor environments, each with multiple rooms. The ground truth of the camera poses and 3D point clouds are labeled using COLMAP \cite{schonberger2016structure}.  We used the split of training and testing data, as well as the 3D SfM models provided by the authors. 

\textbf{Cambridge \cite{kendall2015posenet}:} An RGB outdoor dataset scaling from $875m^{2}$ to $5600m^{2}$. Each scene contains 200--1500 training and validation samples, which are split by the authors. The ground truth of camera poses is obtained through SfM reconstruction. We used this rendered 3D model to generate the ground truth scene coordinates for the training of D2S. 



\textbf{Ritsumeikan BKC:} We introduce a novel outdoor RGB dataset for evaluating the generalization capabilities of visual localization methods. In contrast to the Cambridge dataset, our dataset possesses training trajectories that are distinct from those used for testing. As illustrated in Fig. \ref{BKC_train_test}, the data were collected at different times of day and night. In particular, it includes two different scenes named WestWing and CentralArc. In the WestWing scene (Fig. \ref{BKC_train_test}a), a total of seven sequences are present: one for training (daytime), three for evaluation (nighttime and far-distance daytime), and the remaining three sequences are unlabeled. Note that we do not visualize the unlabeled trajectories here as we do not know their camera poses. The CentralArc (Fig. \ref{BKC_train_test}c) is similar but has an additional training sequence. In Fig. \ref{BKC_train_test}b and d, we visualize the 3D cloud map generated when using only training data. Note that the training data was recorded to cover most of the important areas for localization. Our dataset also involves certain challenges such as changing conditions, domain shifts, and scanty training data. For comparison, Fig. \ref{kingscollege_traintest} visualizes the training and testing data of Cambridge KingsCollege.

In this work, we used Hloc \cite{sarlin2019coarse, sarlin2020superglue} to pre-process the data for the evaluation of D2S. We used the SfM pseudo ground truth (pGT) generated from COLMAP \cite{schonberger2016structure} to triangulate 3D points from SuperPoint features. Subsequently, we split the extracted features and their labels of 3D coordinates for training and testing. The evaluation of D2S with other local feature extractors (e.g., SIFT \cite{lowe2004distinctive}, R2D2 \cite{revaud2019r2d2}) also used the aforementioned pre-processing procedure.


\begin{table*}
\centering
\caption{\textbf{Indoor6 results.} We report the median errors in cm for position and degree ($^{\circ}$) for the orientation of the Indoor6 dataset. We also show the percentage of frame errors within the threshold of 5 cm and 5$^{\circ}$.}
\label{indoor6_results}
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|cc|cc|cc|cc|cc|cc} 
\hline
\multicolumn{14}{c}{Indoor6~~}                                                                                                                                                                                                                                                                                \\ 
\hline
\multirow{2}{*}{Method} & Size           & \multicolumn{2}{c|}{scene1}                & \multicolumn{2}{c|}{scene2a}               & \multicolumn{2}{c|}{scene3}                & \multicolumn{2}{c|}{scene4a}      & \multicolumn{2}{c|}{scene5}                & \multicolumn{2}{c}{scene6}                  \\
                        & (GB)           & (cm/deg.)                  & (\%)          & (cm/deg.)                  & (\%)          & (cm/deg.)                  & (\%)          & (cm/deg.)         & (\%)          & (cm/deg.)                  & (\%.)         & (cm.)                      & (\%)           \\ 
\hline
PoseNet\cite{kendall2017geometric}                 & 0.050          & 159.0/7.46                 & 0.0           & -/-                        & -             & 141.0/9.26                 & 0.0           & -/-               & -             & 179.3/9.37                 & 0.0           & 118.2/9.26                 & 0.0            \\
DSAC*\cite{brachmann2021visual}                   & 0.028          & 12.3/2.06                  & 18.7          & 7.9/0.9                    & 28.0          & 13.1/2.34                  & 19.7          & \textbf{3.7/}0.95 & 60.8          & 40.7/6.72                  & 10.6          & 6.0/1.40                   & 44.3           \\
NBE+SLD(E)\cite{do2022learning}              & 0.029          & 7.5/1.15                   & 28.4          & 7.3/0.7                    & 30.4          & 6.2/1.28                   & 43.5          & 4.6/1.01          & 54.4          & 6.3/0.96                   & 37.5          & 5.8/1.3                    & 44.6           \\
NBE+SLD\cite{do2022learning}                 & 0.132          & 6.5/0.90                   & 38.4          & 7.2/0.68                   & \textbf{32.7} & 4.4/0.91                   & 53.0          & 3.8/0.94          & 66.5          & \textbf{6.0/\textbf{0.91}} & \textbf{40.0} & 5.0/0.99                   & 50.5           \\ 
\cline{2-14}
D2S(ours)               & \textbf{0.022} & 5.6/0.90                   & 44.6          & 8.7/0.8                    & 22.0          & 4.8/0.92                   & 52.4          & \textbf{3.7}/0.88 & 69.0          & 8.6/1.26                   & 28.5          & 4.9/1.06                   & 53.3           \\
D2S+(ours)              & \textbf{0.022} & \textbf{5.0/\textbf{0.82}} & \textbf{50.6} & \textbf{6.9/\textbf{0.62}} & 27.2          & \textbf{4.3/\textbf{0.74}} & \textbf{59.1} & \textbf{3.7/0.86} & \textbf{70.3} & 8.5/1.16                   & 29.3          & \textbf{3.8/\textbf{0.79}} & \textbf{62.9}  \\
\hline
\end{tabular}
}
\end{table*}


\begin{table*}[t]
\centering
\caption{Median position and rotation errors of different relocalization methods on \textit{Cambridge} dataset.}
\label{all_results_cambridge}

\begin{tabular}{c|c|cc|cc|cc|cc|cc} 
\hline
\multicolumn{12}{c}{Cambridge Landmarks}                                                                                                                                                                                                                                                                                                                                             \\ 
\hline
\multicolumn{2}{c|}{\multirow{3}{*}{Method}} & \multicolumn{2}{c|}{King's College}       & \multicolumn{2}{c|}{Old Hospital}        & \multicolumn{2}{c|}{Shop Facade}          & \multicolumn{2}{c|}{St Mary's Church}                       & \multicolumn{2}{c}{Great Court}                                                                                                      \\ 
\cline{3-12}
\multicolumn{2}{c|}{}                        & Size        & Error                       & Size        & Error                      & Size        & Error                       & Size        & Error                                         & Size        & Error                                                                                                                  \\
\multicolumn{2}{c|}{}                        & (MB)        & (m/deg.)                    & (MB)        & (m/deg.)                   & (MB)        & (m/deg.)                    & (MB)        & (m/deg.)                                      & (MB)        & (m/deg.)                                                                                                               \\ 
\hline
\multirow{3}{*}{APR} & PoseNet\cite{kendall2017geometric}             & 50          & 0.88/1.04                   & 50          & 3.2/3.29                   & 50          & 0.88/3.78                   & 50          & 1.6/3.32                                      & 50          & 6.83/3.47                                                                                                              \\
                     & FeatLoc\cite{bach2022featloc}               & 34          & 1.3/3.84                    & 34          & 2.1/6.1                    & 34          & 0.91/7.5                    & 34          & 3.0/10.4                                      & -           & -/-                                                                                                                    \\
                     & DFNet\cite{chen2022dfnet}                 & 60          & 0.73/2.37                   & 60          & 2.0/2.98                   & 60          & 0.67/2.21                   & 60          & 1.37/4.03                                     & -           & -/-                                                                                                                    \\ 
\hline
\multirow{3}{*}{FM}  & Hloc\cite{sarlin2019coarse, sarlin2020superglue}                  & 1877        & 0.11/0.20                   & 1335        & 0.15/0.30                  & 316         & 0.04/0.19                   & 2009        & 0.07/\textcolor[rgb]{0.122,0.122,0.157}{0.22} & 1746        & \textcolor[rgb]{0.122,0.125,0.157}{0.17}\textcolor[rgb]{0.122,0.125,0.157}{/}\textcolor[rgb]{0.122,0.125,0.157}{0.11}  \\
                     & AS\cite{sattler2016efficient}                    & 275         & 0.57/0.70                   & 140         & 0.52/1.12                  & 37.7        & 0.12/0.41                   & 359         & 0.22/0.62                                     & -           & \textcolor[rgb]{0.122,0.125,0.157}{0.24/0.13}                                                                          \\
                     & SS\cite{yang2022scenesqueezer}                    & 0.3         & 0.27/0.38                   & 0.53        & 0.37/0.53                  & 0.13        & 0.11/0.38                   & 0.95        & 0.15/0.37                                     & -           & -/-                                                                                                                    \\ 
\hline
\multirow{5}{*}{SCR} & DSAC++\cite{brachmann2018learning}                & 104         & 0.18/0.3                    & 104         & 0.20/\textbf{\textbf{0.3}} & 104         & 0.06/0.3                    & 104         & 0.13/0.4                                      & 104         & 0.40/0.2                                                                                                               \\
                     & SCoCR\cite{li2020hierarchical}                 & 165         & 0.18/0.3                    & 165         & \textbf{0.19/\textbf{0.3}} & 165         & 0.06/0.3                    & 165         & \textbf{0.09/\textbf{0.3}}                    & 165         & 0.28/0.2                                                                                                               \\
                     & DSAC*\cite{brachmann2021visual}                 & 28          & \textbf{0.15/}0.3           & 28          & 0.21/0.4                   & 28          & \textbf{0.05/}0.3           & 28          & 0.13/0.4                                      & 28          & \textcolor[rgb]{0.122,0.125,0.157}{0.49/0.25}                                                                          \\ 
\cline{2-12}
                     & D2S (ours)            & \textbf{22} & \textbf{0.15/\textbf{0.24}} & \textbf{22} & 0.21/0.40                  & \textbf{22} & 0.06/0.32                   & \textbf{22} & 0.16/0.50                                     & \textbf{22} & 0.38/0.18                                                                                                              \\
                     & D2S+ (ours)           & \textbf{22} & \textbf{0.15/}0.25          & \textbf{22} & \textbf{0.19/}0.34         & \textbf{22} & \textbf{0.05/\textbf{0.26}} & \textbf{22} & 0.13/0.40                                     & \textbf{22} & \textbf{0.25/0.15}                                                                                                     \\
\hline
\end{tabular}

\end{table*}


\subsubsection{Competitors}
We compared our methods with other re-localization methods. For absolute pose regression methods, we performed comparisons with PoseNet \cite{kendall2017geometric} (the updated version), MapNet \cite{brahmbhatt2018geometry}, FeatLoc \cite{bach2022featloc}, and DFNet \cite{chen2022dfnet}. For feature-matching-based competitors, we reported the results of Inloc \cite{taira2018inloc}, Active Search \cite{sattler2016efficient}, and the state-of-the-art pipeline Hloc \cite{sarlin2020superglue, sarlin2019coarse}. For scene coordinates regression approaches, we performed comparisons with SCoCR \cite{li2020hierarchical}, KFNet \cite{zhou2020kfnet}, the state-of-the-art pipeline DSAC* \cite{brachmann2021visual}, DSAC++ \cite{brachmann2017dsac} (the former version of DSAC*), and a recent landmarks-based regression method NBE+SLD \cite{do2022learning}.


\subsection{Results for Indoor Localization (7-Scenes)} \label{indoor_results}






We trained D2S on 7-Scenes using the experimental settings described in Section \ref{ex_settings}. However, we did not apply any augmentation step on this dataset considering its stationary nature, where the illumination remains consistent between the training and testing data. The complete results are presented in Table \ref{7scene_results}. Notably, our D2S outperforms the previous baselines of SCR methods including SCoCR\cite{li2020hierarchical}, DSAC*\cite{brachmann2021visual}, NBE+SLD\cite{do2022learning}, and KFNet \cite{zhou2020kfnet} on Chess, Office, Pumpkin, and Kitchen scenes. Note that all these baselines are CNN-based regression methods: KFNet\cite{zhou2020kfnet} is a temporal relocalization approach that requires a sequence of input frames for high-accuracy localization, whereas the proposed D2S is a one-shot frame method. Particularly, the proposed D2S borrows the SuperPoint \cite{detone2018superpoint} extractor, which is similar to Hloc \cite{sarlin2019coarse, sarlin2020superglue}. Our D2S still offers better accuracy on this dataset compared to Hloc. In addition, Hloc requires several GB of memory for localization for these scenes, whereas D2S requires only 22 MB for encoding the scene. For a clearer difference in obtained results, we visualize the cumulative distributions of pose error in Fig. \ref{dslam_acc_perror}. On average across six scenes, we obtained the best accuracy among these baselines. However, in the Stairs scene, D2S and Hloc exhibited the worst performance due to the textureless nature of this scene, which challenges feature-based methods. Fig. \ref{estimated_trajectories_7scenes} visualizes the estimated trajectories of the proposed methods, including Hloc and DSAC*.


\subsection{Results for Indoor Localization (Indoor6)} \label{indoor_results_indoor6}

Indoor6 \cite{do2022learning} is a challenging indoor dataset consisting of images collected at different times and days. Therefore, we trained our D2S with each scene of Indoor6 using the entire experimental settings demonstrated in Section \ref{ex_settings}. Note that we performed an offline data augmentation step before training, which required a runtime of approximately 8 minutes for pre-processing of 1000 unlabeled images. Subsequently, we proceeded with simultaneous training using both the total augmented and original training data. 


The results for this dataset are listed in Table \ref{indoor6_results}. Our D2S significantly outperforms DSAC* \cite{brachmann2021visual}, PoseNet\cite{kendall2017geometric}, and NBE+SLD(E) \cite{do2022learning} (lightweight version). However, in certain cases, its performance is considerably lower than that of NBE+SLD \cite{do2022learning}. NBE+SLD trains two different models of landmarks detector (SLD) and scene coordinate regression network (NBE) for each scene. However, the proposed method can implement both criteria in the same D2S network. Consequently, our D2S requires only a small memory for final localization, whereas NBE+SLD demands six times more memory for both networks (132 MB).


The Indoor6 \cite{do2022learning} dataset provides a small amount of validation data that is distinct from the training and test data. We used this data to evaluate our proposed self-supervised learning method with unlabeled data. As shown in Table \ref{indoor6_results}, the results for D2S+ (the version that updates with unlabeled data) demonstrate that our proposed self-supervised learning procedure successfully improved the localization performance of D2S by up to 9.6\%.


\begin{table*}
\centering

\caption{Median errors for BKC dataset, where the accuracy is calculated using the threshold of 0.5m and 10$^{\circ}$.}
\begin{tabular}{c|ccc|ccc|ccc|ccc} 
\hline
\multirow{4}{*}{Method} & \multicolumn{12}{c}{WestWing}                                                                                                                                                                                                                                                      \\ 
\cline{2-13}
                        & \multicolumn{3}{c|}{Seq-2 (daytime - one view)}                                         & \multicolumn{3}{c|}{Seq-3 (daytime - two views)}                                         & \multicolumn{3}{c|}{Seq-4 (nighttime)}                                         & \multicolumn{3}{c}{Average}                                         \\ 
\cline{2-13}
                        & Trans. Err           & Rot. Err             & Acc.                 & Trans. Err           & Rot. Err             & Acc.                 & Trans. Err           & Rot. Err             & Acc.                 & Trans. Err           & Rot. Err             & Acc.                  \\
                        & (m)                  & ($^{\circ}$)                & (\%)                 & (m)                  & ($^{\circ}$)                & (\%)                 & (m)                  & ($^{\circ}$)                & (\%)                 & (m)                  & ($^{\circ}$)                & (\%)                  \\ 
\hline
DSAC*\cite{brachmann2021visual}                   & 29.6                 & 67.1                 & 0.0                  & 8.92                 & 28.5                 & 0.0                  & 8.85                 & 57.0                 & 0.0                  & 15.8                 & 50.8                 & 0.0                   \\
D2S(ours)                     & 0.43                 & 3.62                 & 51.5                 & 0.36                 & 2.24                 & 56.0                 & 0.18                 & 2.12                 & 63.0                 & 0.32                 & 2.6                  & 56.8                  \\
D2S+(ours)                    & 0.16                 & 1.34                 & 75.7                 & 0.09                 & 0.48                 & 93.6                 & 0.08                 & 1.29                 & 70.4                 & \textbf{0.11}        & 1.04                 & \textbf{79.9}         \\ 
\hline
Hloc\cite{sarlin2019coarse, sarlin2020superglue}                    & 0.25                 & 2.12                 & 57.3                 & 0.04                 & 0.19                 & 83.5                 & 0.03                 & 0.30                 & 74.1                 & \textbf{0.11}        & \textbf{0.87}        & 71.6                  \\ 
\hline
\multicolumn{1}{l}{}    & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}  \\ 
\hline
\multirow{4}{*}{Method} & \multicolumn{12}{c}{CentralArc}                                                                                                                                                                                                                                                    \\ 
\cline{2-13}
                        & \multicolumn{3}{c|}{Seq-3 (daytime - rain)}                                         & \multicolumn{3}{c|}{Seq-4 (daytime - sunny)}                                         & \multicolumn{3}{c|}{Seq-5 (nighttime)}                                         & \multicolumn{3}{c}{Average}                                         \\ 
\cline{2-13}
                        & Trans. Err           & Rot. Err             & Acc.                 & Trans. Err           & Rot. Err             & Acc.                 & Trans. Err           & Rot. Err             & Acc.                 & Trans. Err           & Rot. Err             & Acc.                  \\
                        & (m)                  & ($^{\circ}$)                & (\%)                 & (m)                  & ($^{\circ}$)                & (\%)                 & (m)                  & ($^{\circ}$)                & (\%)                 & (m)                  & ($^{\circ}$)                & (\%)                  \\ 
\hline
DSAC*\cite{brachmann2021visual}                   & 226.7                & 15.5                 & 0.0                  & 97.2                 & 6.7                  & 0.0                  & 104.8                & 11.4                 & 0.0                  & 142.7                & 11.2                 & 0.0                   \\
D2S(ours)                     & 1.26                 & 5.97                 & 29.6                 & 0.66                 & 3.19                 & 46.7                 & 10.5                 & 82.3                 & 4.65                 & 4.14                 & 30.5                 & 27.0                  \\
D2S+(ours)                    & 0.73                 & 3.43                 & 33.7                 & 0.23                 & 1.15                 & 72.8                 & 0.93                 & 7.25                 & 30.2                 & \textbf{0.63}        & \textbf{3.91}        & 45.6                  \\ 
\hline
Hloc\cite{sarlin2019coarse, sarlin2020superglue}                    & 3.86                 & 14.3                 & 25.5                 & 0.04                 & 0.14                 & 77.2                 & 1.16                 & 4.69                 & 41.9                 & 1.69                 & 6.38                 & \textbf{48.2}         \\
\hline
\end{tabular}
\label{BKC_median_results}
\end{table*}
  
% Figure environment removed



\subsection{Results for Outdoor Localization (Cambridge)} \label{outdoor_results_cambridge}
To evaluate the quality of re-localization performance on the Cambridge dataset, we computed the median pose error for each scene, and the results are reported in Table \ref{all_results_cambridge}. Among the evaluated approaches, APR-based approaches, including PoseNet \cite{kendall2017geometric}, FeatLoc \cite{bach2022featloc}, and DFNet \cite{chen2022dfnet}, achieved the lowest performance with low memory demand for all scenes. In contrast, the FM-based Hloc \cite{sarlin2020superglue, sarlin2019coarse} pipeline achieved the highest re-localization accuracy. However, Hloc requires saving global and local descriptors for training images, leading to a high memory demand for each scene, as indicated in Table \ref{all_results_cambridge}. We also report the results of the compressed version  SceneSqueezer (SS) \cite{yang2022scenesqueezer} on this dataset, which has a significantly low memory footprint. However, its accuracy is much lower than that of Hloc.

Moreover, we report the results of SRC-based approaches as well as the storage demand of each scene. Overall, all the SRC-based methods reported in Table \ref{all_results_cambridge} achieved a similar accuracy. However, considering the trade-off between memory footprint and re-localization accuracy, the proposed method of D2S achieves the best trade-off compared to other methods. Note that DSAC* also has a compact version measuring 4 MB in size, but it yields significantly higher localization errors. Therefore, we only reported the most accurate version of DSAC* herein.  


Finally, we evaluated the proposed self-supervised learning of D2S+ on the Cambridge dataset. As this dataset does not provide unlabeled data, we used half of the validation data as unlabeled data, which is similar to the approach used in previous works \cite{brahmbhatt2018geometry, chen2022dfnet, chen2021direct}. Subsequently, we updated the model using algorithm 1. The results are presented in Table \ref{all_results_cambridge}; notably, D2S+ also successfully improves the localization performance of D2S.





\subsection{Results for Outdoor Localization (BKC)} \label{outdoor_results_BKC}

This section reports the performance of the proposed method and other baselines on the BKC dataset. This dataset has not been evaluated by previous localization methods; therefore, we selected two state-of-the-art baselines, FM-based Hloc \cite{sarlin2019coarse, sarlin2020superglue} and SCR-based DSAC* \cite{brachmann2021visual}, to draw a comparison with the proposed D2S.

For the evaluation of Hloc on this dataset, we used the following procedure. In the offline phase, we used the SfM pseudo ground truths (pGT) generated from COLMAP \cite{schonberger2016structure} to triangulate 3D points from SuperPoint features. In the online phase, we used NetVLAD \cite{arandjelovic2016netvlad} to retrieve the top-10 nearest images from the training database, followed by feature matching between the query
and retrieved images to determine 2D-3D correspondences. Finally, we computed the camera pose using pyColmap \cite{pycolmap} with an inlier threshold of 12 px. 

% Figure environment removed

For DSAC*+3D model (SfM) mode, we used the same configurations stated by the authors. We trained the model with 1M iterations for an initial training step and maintained an end-to-end training of differentiable RANSAC and pose estimation with an additional 100K iterations.


Table \ref{BKC_median_results} reports the median localization errors of DSAC*, Hloc, and the proposed D2S on different test sequences of the BKC dataset. For a more detailed understanding, we also visualized the cumulative distribution of errors on this dataset in Fig. \ref{cumulative_bkc}. The BKC dataset features several challenging properties for evaluating the visual localization methods on their capacity to generalize beyond the training data. Thus, it is particularly challenging for CNN-based SCR approaches as significant domain shifts can result in substantial differences between training and test images. This is evident in the DSAC* results on all testing sequences (Table \ref{BKC_median_results} and in Fig. \ref{cumulative_bkc}). In the WestWing scene, as an example, DSAC* obtained the worst median localization errors on Seq-2, which were 29.6m and 67.1$^{\circ}$, whereas the proposed D2S achieved localization errors of 0.43m and 3.62$^{\circ}$. On average for three test sequences, Hloc achieves the highest accuracy within 50cm and 10$^{\circ}$, which is 71.6\%, compared to 56.8\% for D2S and 0\% for DSAC*. The same result is obtained when evaluating BKC CentralArc, where Hloc still demonstrates its state-of-the-art FM-based localization methods. In particular, Hloc achieves an accuracy of 48.2\%, D2S achieves an accuracy of 27\%, whereas DSAC* still fails in these testing situations. These results demonstrate that the proposed sparse SCR-based D2S outperforms the dense CNN-based DSAC* in generalizing beyond training data, including transitioning from day to night and adapting to domain shifts even in the absence of training data sources.


In addition, the BKC dataset is also used for the evaluation of visualization methods on updating with unlabeled observations. Each test sequence in WestWing and CentralArc has a corresponding unlabeled sequence. We recorded these data at different times and near the test sequence. Fig. \ref{bkcwestwing_testandunlabeled} shows a comparison of test and unlabeled images for the BKC WestWing scene.


% Figure environment removed

We report the results of D2S+ when updated with these unlabeled data in Table \ref{BKC_median_results} and Fig. \ref{cumulative_bkc}. Notably, the improvement in accuracy obtained through D2S, from 56.8\% to 79.9\%, within the threshold of 50cm and 10$^{\circ}$ on the WestWing scene surpasses Hlocs accuracy of 71.6\%. A large margin of improvement with 18.6\% is also observed on the CentralArc scene. Particularly, when observing the results with a higher threshold of up to 2m and 20$^{\circ}$, D2S+ outperforms Hloc. Fig. \ref{bkc_seq2_est} shows an example of estimated trajectories of Hloc, D2S, and D2S+ on WestWing Seq-2, where D2S+ successfully predicts the camera position, even at the positions located far from the scene. 


\section{Ablation Study} \label{ab_study}

This section presents various ablation studies to further understand the underlying mechanism of D2S. 
\subsection{Different Local Extractors} \label{different_local_extractors}
We measure the performance of D2S when working on different local extractors. We selected three popular feature extractors: SIFT \cite{lowe2004distinctive}, R2D2 \cite{revaud2019r2d2}, and SuperPoint \cite{detone2018superpoint}. SIFT and R2D2 possess the same descriptor dimension of 128, which results in a minimal memory footprint of 12MB for D2S. In contrast, SuperPoint has 254 dimensions for its descriptors, requiring 22MB of memory consumption. Additionally, we also report the results of Hloc when using these different feature extractors. 

\begin{table}[bp]
\centering
\caption{Median localization errors of D2S when learning on different local feature extractors. We also report the results of Hloc for using different extractors as a comparison to D2S.}
\label{all_results}
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|cc|cc} 
\hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Localization\\~Method\end{tabular}} & \multirow{3}{*}{Extractor} & \multirow{2}{*}{Size} & 7scenes                  & \multirow{2}{*}{Size} & Cambridge~          \\
                                                                               &                            &                       & Chess                    &                       & KingsCollege        \\
                                                                               &                            & (MB)                  & (m/deg./\%)              & (MB)                  & (m/deg.)            \\ 
\hline
\multirow{3}{*}{Hloc\cite{sarlin2019coarse,sarlin2020superglue}}                                                          & SIFT\cite{lowe2004distinctive}~                      & 8.8K                  & 0.025/0.86/90.0          & 1.2K                  & 0.12/0.22           \\
                                                                               & R2D2\cite{revaud2019r2d2}                       & \textbf{2.1K}         & 0.028/0.95/84.8          & \textbf{0.7K}         & 0.13/0.22           \\
                                                                               & SuperPoint\cite{detone2018superpoint}                 & 4.5K                  & \textbf{0.024/0.77/94.2} & 1.9K                  & \textbf{0.11/0.20}  \\ 
\hline
\multirow{3}{*}{D2S (ours)}                                                    & SIFT\cite{lowe2004distinctive}                       & \textbf{12}           & 0.039/1.46/64.7          & \textbf{12}           & 0.52/0.72           \\
                                                                               & R2D2\cite{revaud2019r2d2}                       & \textbf{12}           & 0.023/0.77/89.1~         & \textbf{12}           & 0.33/0.44           \\
                                                                               & SuperPoint\cite{detone2018superpoint}                 & 22                    & \textbf{0.017/0.57/98.6} & 22                    & \textbf{0.15/0.24}  \\
\hline
\end{tabular}
}
\label{different_extractors_results}
\end{table}

We conducted experiments on two scenes, one indoor and one outdoor, using the 7-scenes Chess and the Cambridge KingsCollege. Table \ref{different_extractors_results} reports the results of D2S and Hloc when using different local features. For the indoor chess scene, both D2S and Hloc achieved the highest re-localization accuracy when using SuperPoint as the local feature extractor. However, considering the learning-based features of R2D2 and SuperPoint, D2S obtained a better accuracy while requiring only 12 and 22MB, compared to 2.1K and 4.5K MB used by Hloc, respectively. For the outdoor scene, Hloc obtained the best localization accuracy but also suffered from a huge memory requirement. The proposed D2S performed better when using learning-based descriptors compared to hand-crafted features.

% Figure environment removed

Furthermore, we also examined the behavior of D2S when learning with different descriptors for reliability detection. Fig. \ref{uncertainty_ablation} shows the predicted uncertainty results on different descriptors of SIFT\cite{lowe2004distinctive}, R2D2\cite{revaud2019r2d2}, and SuperPoint\cite{detone2018superpoint} by the proposed D2S with an uncertain threshold of 0.5. Notably, D2S demonstrates its state-of-the-art classification performance when using SuperPoint features. In Section \ref{train_inference}, Table \ref{pnp_inference}, we further report the efficient effects of this factor on re-localization performance in both accuracy and inference time. 




% \usepackage{graphicx}
% \usepackage{multirow}

\subsection{Graph Attention} \label{graph_attention_results}


This section presents a further examination of the manner in which graph attention operates in D2S architecture. Fig. \ref{attention_full} displays the attention results of both indoor and outdoor data. D2S shows a concentration on all existing descriptors in the first layers, but finally (in layer 5), it shows special interest only in robust keypoints. This trend illustrates that the proposed D2S has succeeded at regressing the scene's coordinates from noisy descriptors. 

To further understand the influence of graph attention on the improvement in final re-localization accuracy, we conducted an ablation study by changing the number of graph layers. The results are reported in Table \ref{change_graph_results}, where the proposed D2S using all five graph layers achieves a large margin of enhancement compared to D2S (without using graph attention), under both indoor and outdoor environments. 
 


% Figure environment removed


\begin{table}[t]
\centering
\caption{Median position and rotation errors of different graph attention settings on both indoor and outdoor scenes. The best results are in bold.}
% \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c} 
\hline
                     & \multirow{2}{*}{+Config.} & 7scenes~                     & Cambridge~            \\
                     &                           & Chess                       & KingsCollege          \\ 
\hline
\multirow{3}{*}{D2S} & No graph network          & 0.022m/0.79$^{\circ}$/93.2\%          & 0.82m/1.10$^{\circ}$           \\
                     & Smaller (2 layers)        & 0.019m/0.62$^{\circ}$/97.0\%          & 0.22m/0.36$^{\circ}$           \\
                     & Full (5 layers)           & \textbf{0.017m/0.57$^{\circ}$/98.6\%} & \textbf{0.15m/0.24$^{\circ}$}  \\
\hline
\end{tabular}

\label{change_graph_results}
\end{table}


% Figure environment removed



\begin{table*}
\centering
\caption{\textbf{Uncertainty Effects}. We report the effects of uncertainty prediction on re-localization performance using the Cambridge \cite{kendall2015posenet} dataset. The error is reported as a median error of position and rotation (m and degree). The PnP RANSAC \cite{pycolmap} estimation times (ms) are calculated by averaging over all the test images for each scene.}
\begin{tabular}{c|cc|cc|cc|cc|cc} 
\hline
\multirow{3}{*}{D2S Uncer.} & \multicolumn{2}{c|}{Kings College} & \multicolumn{2}{c|}{Old Hospital} & \multicolumn{2}{c|}{Shop Facade} & \multicolumn{2}{c|}{St Mary's Church} & \multicolumn{2}{c}{Great Court}   \\ 
\cline{2-11}
                            & Error              & PnP time      & Error              & PnP time     & Error              & PnP time    & Error              & PnP time         & Error              & PnP time     \\
                            & (m/deg.)           & (ms)          & (m/deg.)           & (ms)         & (m/deg.)           & (ms)        & (m/deg.)           & (ms)             & (m/deg.)           & (ms)         \\ 
\hline
No                          & 0.16/0.25          & 76            & 0.27/0.44          & 302          & 0.08/0.35          & 195         & 0.18/0.54          & 548              & 0.42/0.21          & 257          \\
Yes                         & \textbf{0.15/0.24} & \textbf{30}   & \textbf{0.21/0.40} & \textbf{22}  & \textbf{0.06/0.32} & \textbf{27} & \textbf{0.16/0.50} & \textbf{106}     & \textbf{0.38/0.18} & \textbf{61}  \\
\hline
\end{tabular}
\label{pnp_inference}
\end{table*}

\begin{table}
\centering
\caption{\textbf{Inference time}. We used an image size of 1024 for outdoor evaluation and 640 for indoor evaluation. In D2S, we report the feedforward times with two different numbers of input descriptors, 2048 and 1024. The inference time of PnP RANSAC \cite{pycolmap} is dependent on the quality of predicted 3D coordinates (see Table \ref{pnp_inference}).}
\begin{tabular}{c|cc|cc} 
\hline
\multirow{3}{*}{Image size} & \multicolumn{2}{c|}{SuperPoint} & \multicolumn{2}{c}{D2S~}                                \\
                            & \multicolumn{2}{c|}{}           & \multicolumn{2}{c}{(2048/1024)}                         \\ 
\cline{2-5}
                            & GPU  & CPU                      & GPU                      & CPU                          \\ 
\hline
1024                        & 19ms & 735ms                    & \multirow{2}{*}{4ms/4ms} & \multirow{2}{*}{244ms/76ms}  \\
640                         & 10ms & 278ms                    &                          &                              \\
\hline
\end{tabular}

\label{cpu_gpu}
\end{table}



\subsection{Hloc and D2S+ Comparison}
This section presents a comparison between Hloc and D2S+. As shown in Section \ref{outdoor_results_BKC}, D2S+ performed better in locations lacking training data. Notably, both D2S+ and Hloc used the same amount of reference/training data. We found several failed cases in the image retrieval step, as illustrated in Fig. \ref{netvlad}. This resulted in an inferior performance of Hloc at locations lacking reference data. However, D2S+ overcomes this drawback by learning the general hints from unlabeled data even when facing this difficulty. Note that the unlabeled data contains no information about the reconstructed SfM models.  

\subsection{Training and Inference Time} \label{train_inference}
This section reports the training time and inference performance of the proposed D2S. In our experiments, we used an Nvidia GeForce GTX 1080ti GPU and an Intel Core i9-9900 CPU for both training and inference. For each environment, D2S required 1.5 days to train using the aforementioned hardware. In comparison, DSAC* \cite{brachmann2021visual} required 2.5 days to train, and DSAC++ \cite{brachmann2018learning} even required up to 6 days to train on Tesla K80. However, we verified the training time of DSAC* on our hardware, which requires the same duration of 1.5 days for each scene, similar to the proposed D2S. SCR-based approaches encounter protracted training times, but recent work using gradient decorrelation has efficiently solved this issue \cite{brachmann2023accelerated}. Thus, we plan to optimize this in future work.

Additionally, we report the inference performance of D2S in Table \ref{pnp_inference} and Table \ref{cpu_gpu}. Herein, Table \ref{pnp_inference} reports the average estimation times of the RANSAC PnP \cite{pycolmap} loop on two different settings (without and with D2S uncertainty). Table \ref{cpu_gpu} reports the feedforward times of SuperPoint \cite{detone2018superpoint} and D2S when using the GPU or CPU. Overall, the proposed D2S infers with an average of 53 ms from an RGB image to a 6DoF camera pose with the KingsCollege scene while requiring only 45 ms with Old Hospital. Note that the total estimation time depends on the quality of predicted scene coordinates. 




% % Figure environment removed

\section{Conclusion} \label{conclusion}
In this study, we introduced D2S, a novel direct sparse regression approach for establishing 2D--3D correspondences in visual localization. Our proposed pipeline is both simple and cost-effective, enabling the efficient representation of complex descriptors in 3D visual cloud maps. By utilizing an attention graph and straightforward loss functions, D2S was able to accurately classify the reliability of descriptors, leading to significant improvements in re-localization performance. We conducted a comprehensive evaluation of our proposed D2S method on four distinct indoor and outdoor datasets, demonstrating its superior re-localization accuracy compared to state-of-the-art baselines. Furthermore, we introduced a new dataset for assessing the generalization capabilities of visual localization approaches. Our results demonstrate that D2S is able to generalize beyond its training data, even in challenging domain shifts such as day-to-night transitions and in locations lacking training data. Additionally, D2S is capable of self-supervised updating with new observations, eliminating the need for camera poses, camera parameters, and ground truth scene coordinates. Overall, our approach represents a promising step forward in the field of visual localization, particularly in the context of sparse descriptors. Future work could focus on further optimizing the training time and exploring other self-supervised learning strategies to enhance localization performance in challenging scenarios.

\bibliographystyle{unsrt}
\bibliography{reference.bib}


% \begin{IEEEbiography}
% [{% Figure removed}]{Bach-Thuan Bui} received the B.E. degree in Information Science and Engineering from Hanoi University of Science and Engineering, Vietnam in 2019, and the M.E. degree from Ritsumeikan University, Japan in 2022. He is currently pursuing a Ph.D. degree in Information Science and Engineering at Ritsumeikan University, Japan. His current research interests include computer vision, visual localization systems, and learning-based localization techniques.
% \end{IEEEbiography}

% \begin{IEEEbiography}
% [{% Figure removed}]{Dinh-Tuan Tran} received the B.E., M.E., and Ph.D. degrees in Information Science and Engineering from Ritsumeikan University, Japan, in 2012, 2016, and 2019, respectively. From 2019 to 2020, he worked as a Postdoctoral Researcher with the College of Information Science and Engineering at Ritsumeikan University, Japan. He is currently an Assistant Professor in the College of Information Science and Engineering at Ritsumeikan University, Japan. Between 2017 and 2019, he was a short-term visiting researcher with the Technical University of Munich (Munich, Germany, 2017), the Budapest University of Technology and Economics (Budapest, Hungary, 2018), the University of Auckland (Auckland, New Zealand, 2018), the University of Greenwich (London, UK, 2018), the Cardiff University (Cardiff, UK, 2018), and the University of Melbourne (Melbourne, Australia, 2019). He won a hackathon named StuDev for Game at the Summer Internship of DeNA Co., Ltd., Japan, in 2015. His research interests include machine learning, image processing, computer vision, robot vision, reinforcement learning, imitation learning, human process modeling, and medical imaging.
% \end{IEEEbiography}

% \begin{IEEEbiography}
% [{% Figure removed}]{Joo-Ho Lee} received the B.E. and M.E. degrees from Korea University, Seoul, Korea, in 1993 and 1995, respectively, and the Ph.D. degree from The University of Tokyo, Tokyo, Japan, in 1999, all in electrical engineering. He is currently a Professor in Dept. of Information Science and Engineering at Ritsumeikan University, Shiga, Japan. From 1999 to 2003, he was a JSPS Postdoctoral Researcher at the Institute of Industrial Science, The University of Tokyo. From 2003 to 2004, he was Research Associate at Tokyo University of Science, Japan and from 2004 he joined Ritsumeikan University as an Associate Professor. From 2008 to 2009, he was a Visiting Scholar at the Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA. In 2017, he was a Research Professor at Dept. of Mechanical Engineering, Korea University, Seoul, Korea. His research interests include intelligent environments, intelligent robots, computer vision, machine learning, and medical/healthcare applications. He is a Senior Member of IEEE and a Member of the RSJ, JSME, SICE, HIS, IEICE, KROS, and IEEJ.
% \end{IEEEbiography}


% if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}



