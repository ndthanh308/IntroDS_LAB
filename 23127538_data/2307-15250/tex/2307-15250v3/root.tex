\pdfinfo{/myversion (0.00.00)}
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

% \documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{svg}
\usepackage{caption}
% \usepackage{tabularray}
\usepackage{multirow}
% \usepackage{float}


\usepackage{amsfonts}
% \usepackage{algorithmic}
% \usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{soul}

\usepackage{graphics}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{mathtools}

\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}  
\SetKwRepeat{Do}{do}{while}%

\title{\LARGE \bf
D2S: Representing sparse descriptors and 3D coordinates for camera relocalization
}


\author{Bach-Thuan Bui$^{1}$, Huy-Hoang Bui$^{1}$, Dinh-Tuan Tran$^{2}$, and Joo-Ho Lee$^{2}$% <-this % stops a space
% \thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{$^{1}$Graduate School of Information Science and Engineering, Ritsumeikan University, Japan}%
\thanks{$^{2}$College of Information Science and Engineering, Ritsumeikan University, Japan.}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

State-of-the-art visual localization methods mostly rely on complex procedures to match local descriptors and 3D point clouds. However, these procedures can incur significant costs in terms of inference, storage, and updates over time. In this study, we propose a direct learning-based approach that utilizes a simple network named D2S to represent complex local descriptors and their scene coordinates. Our method is characterized by its simplicity and cost-effectiveness. It solely leverages a single RGB image for localization during the testing phase and only requires a lightweight model to encode a complex sparse scene. The proposed D2S employs a combination of a simple loss function and graph attention to selectively focus on robust descriptors while disregarding areas such as clouds, trees, and several dynamic objects. This selective attention enables D2S to effectively perform a binary-semantic classification for sparse descriptors. Additionally, we propose a simple outdoor dataset to evaluate the capabilities of visual localization methods in scene-specific generalization and self-updating from unlabeled observations. Our approach outperforms the state-of-the-art CNN-based methods in scene coordinate regression in indoor and outdoor environments. It demonstrates the ability to generalize beyond training data, including scenarios involving transitions from day to night and adapting to domain shifts, even in the absence of the labeled data sources. The source code, trained models, dataset, and demo videos are available at the following link: https://thpjp.github.io/d2s.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
% Visual re-localization plays a crucial role in numerous applications including robotics and computer vision. The most common methods rely on sparse reconstruction models and local feature matching (FM) \cite{detone2018superpoint, revaud2019r2d2, sarlin2019coarse} owing to their robustness in handling appearance changes introduced by different devices and viewpoints. However, these algorithms tend to be complex, since the image-matching process requires a detailed 3D cloud map along with local and global visual descriptors \cite{detone2018superpoint, lowe2004distinctive, arandjelovic2016netvlad, sarlin2019coarse}. In addition, storing 3D maps can be resource-intensive, as they often contain up to a million points. This hinders their execution on consumer devices with limited computing capacity. 

Visual re-localization plays a crucial role in numerous applications including robotics and computer vision. The majority of methods rely on sparse reconstruction models, and local feature matching \cite{detone2018superpoint, sarlin2019coarse}. The 3D reconstructed model, commonly built using Structure from Motion (SfM), stores 3D points cloud map along with local and global visual descriptors \cite{detone2018superpoint, lowe2004distinctive, arandjelovic2016netvlad, sarlin2019coarse}. While showing superior performance in handling appearance changes introduced by different devices and viewpoints, these methods suffer from several drawbacks. First, point clouds and visual descriptors need to be stored explicitly, which hinders their practicality to large-scale scenes due to storage requirements. Second, the involvement of multiple components such as feature extraction and feature matching make these approaches computation-extensive which is not suitable for execution on consumer or edge devices. 

Conversely, end-to-end learning of the implied 2D--3D correspondences offers a better trade-off between accuracy and computational efficiency \cite{brachmann2017dsac, li2020hierarchical, brachmann2021visual}. These approaches, commonly referred to as scene coordinate regression (SCR), directly regressing scene coordinates from corresponding 2D pixels of the input image. Thus, SCRs naturally establish the 2D--3D matches without the need for the 3D models but it is worth mentioning that they are prone to overfitting if the training data does not adequately represent the environment \cite{li2018full}. Although previous SCR-based approaches have demonstrated high accuracy for re-localization in small-scale and stationary environments \cite{brachmann2017dsac,brachmann2018learning,brachmann2021visual}, their performance are not on par with FM-based methods in environments that contain variations over a long span of time \cite{do2022learning} and high domain shift scenarios.

% Figure environment removed


% In this study, we introduce a new approach to establish sparse 2D-3D correspondences from keypoint descriptors which is robust against environment changes. Instead of learning directly from RGB images, we initiate the regression network from sparse salient descriptors. However, sparse descriptors are more suitable for matching pipelines, as they are unordered and locally separated. Due to their unorderness and isolation, spare descriptors are suitable candidate for matching pipelines. We thus propose a \ul{regression network$\rightarrow$novel model} named D2S \ul{as a representation of$\rightarrow$ that learns to map keypoint} \textbf{D}escriptors and their \textbf{S}cene coordinates. \textcolor{blue}{\ul{The D2S thus can$\rightarrow$ As the result, D2S} inherit the robustness of salient features, and additionally learn the global context by communicating all features using self-attention}. Additionally, we believe that the re-localizer should be able to exploit the redundancy of new, unlabeled data from various devices, to make it highly adaptable. \textcolor{red}{However, previous SRC methods \cite{brachmann2021visual, brachmann2023accelerated} are not designed to handle these changing conditions}. By leveraging existing robust sparse matching pipelines, we additionally propose a simple self-supervised mechanism for updating the re-localizer with unlabeled observations, further enhancing D2S in adapting to \ul{non-stationary$\rightarrow$dynamic} environments. In Fig. \ref{fig1}, we present a summary of the proposed pipeline. In detail, \ul{we summarized our contributions to this work as follows$\rightarrow$ our contributions are as follows}: 


In this study, we introduce a new visual localization approach named D2S that leverages keypoint descriptors to address the aforementioned issues of previous SCR methods. Instead of learning directly from RGB images, we initiate the regression network from sparse salient descriptors. This allows the network to learn the mapping between keypoint descriptors and their corresponding scene coordinates. By exploiting the self-attention mechanism, we also enable D2S to learn the global context by communicating between features. As a result, D2S inherits the robustness of salient features, making it resilient against drastic changes in the environment. In addition, we explore an aspect that previous SCR research \cite{brachmann2021visual, brachmann2023accelerated} lacks, which is the ability of the model to utilize new, unlabeled data from various devices. We propose a simple yet efficient self-supervised technique for updating the re-localizer with unlabeled observations, which further enhances D2S's ability to accommodate dynamic environments. In detail, our contributions are as follows:


\begin{itemize}
    \item We present a novel approach for representing the sparse descriptor and its 3D coordinate through a lightweight regression network, termed D2S (Descriptors to Scenes). Our approach achieves re-localization with remarkable precision, with errors as low as a few centimeters and a fraction of a degree in real-time, in both indoor and outdoor environments. 
    \item Our D2S leverages a combination of graph attention and a simple loss function for learning to identify reliable descriptors. Consequently, D2S can strongly focus on robust descriptors, leading to significant enhancements in re-localization performance. This also renders D2S a natural filter for the appearances of dynamic objects. 
    \item We propose a self-supervised learning procedure for updating the D2S with new observations without requiring camera poses, camera parameters, and ground truth scene coordinates. 
    \item We introduce a simple dataset to evaluate the performance of visual re-localizers under conditions of high domain shift and locations with limited training data. We also provide additional unlabeled data to facilitate the evaluation of methods with self-supervised updating. 
    \item We report a comprehensive evaluation of the proposed D2S on three different indoor and outdoor datasets. The results demonstrate that our method outperforms the state of the art.
\end{itemize}


\section{Related Work} \label{related_work}
\subsection{Image Retrieval and Absolute Pose Regression}

Image-based visual re-localization methods\cite{schindler2007city} were arguably the simplest solution, which constructing maps just by storing a set of images and their camera pose in a database. Given a query image, we seek for the most similar images in the database by comparing their global descriptor \cite{arandjelovic2016netvlad, torii201524}. With the matched images representing a location in a scene that is close to the query image \cite{torii201524, arandjelovic2016netvlad}, the pose of the query image can be derived from the top retrieved images pose. Despite demonstrating scalability in large environments, these approaches have limited accuracy in determining camera poses. Thus they are often used as initialization for later pose refinements.

6DoF absolute camera pose regression approaches (APR) have been studied extensively to overcome the drawbacks of image retrieval. Notably, PoseNet\cite{kendall2015posenet} was first proposed, along with subsequent developments \cite{kendall2015posenet, bach2022featloc, chen2022dfnet}. APR learns to directly encode the 6DoF camera pose from the image, resulting in a lightweight system where memory demand and estimation time are independent of the reference data size. However, theoretical conclusions \cite{sattler2019understanding} suggest that APR behaviors are more closely related to image retrieval than methods that estimate poses by 3D geometry. Thus APR could hardly outperform a handcraft retrieval baseline.

% Figure environment removed

\subsection{Sparse Feature Matching}

Features matching-based (FM) re-localizers compute the camera pose from 2D-3D correspondences between 2D pixels and 3D scene maps, constructed using Structure-from-Motion (SfM) \cite{schonberger2016structure, sarlin2019coarse}. Early FM-based solutions match local descriptors in 2D images to 3D points in the SfM model \cite{svarm2016city, toft2018semantic, sattler2016efficient, liu2017efficient}. Despite achieving accurate pose estimations, exhaustive matching creates a substantial computational bottleneck. Additionally, as SfM models grow in size, direct matching becomes ambiguous and difficult to establish under significant appearance changes. Leveraging the robustness of image retrieval in scalability and handling appearance changes, \cite{sarlin2019coarse, sarlin2020superglue} proposed a hierarchical re-localization approach that uses image retrieval as an initialization step before performing 2D-3D direct matching. Instead of performing complex 2D-3D matching, \cite{sarlin2021back} and \cite{pietrantoni2023segloc} proposed deep feature alignment, reducing system complexity while preserving accurate estimation.


\subsection{Scene Coordinate Regression}


Compared to the FM-based pipeline, scene coordinate regression (SCR) approaches are more straightforward. By employing a single regression network, 3D coordinates can be directly extracted from 2D pixels \cite{valentin2015exploiting, brachmann2017dsac, brachmann2018learning, li2020hierarchical, brachmann2021visual, brachmann2023accelerated}. These correspondences then serve as the input for RANSAC-based pose optimization. SCR methods have demonstrated superior accuracy in camera pose estimation in small-scale and stationary environments. However, scaling these methods to larger, dynamic environments remains challenging. They struggle with adapting to novel viewpoints and handling the increased complexity that comes with larger-scale environments \cite{do2022learning}. Our proposed model, adopting similar idea to SCR but aim to address the aforementioned problem. By utilizing the robustness of local keypoint descriptors, our model seeks to enhance performance in high viewpoint changes and dynamic scenarios.

\subsection{Self-Supervised Updating with Unlabeled Data}


In constrast to visual Simultaneous Localization and Mapping (SLAM) systems \cite{sumikura2019openvslam}, the map of most visual re-localization approaches, including FMs \cite{sarlin2019coarse, yang2022scenesqueezer} and SCR-based methods \cite{brachmann2018learning, brachmann2019expert, li2020hierarchical, brachmann2021visual, brachmann2023accelerated} are built in advanced. Although this feature allow visual-relocalizer to be lightweight and fast compared to SLAM, it poses obstacles when working with new or constantly changing environments since the map need to be rebuilt. To overcome this challenge and further push the performace of the proposed pipeline, we introduce a simple approach for self-supervised updating for the D2S with unlabeled observations. This enables the method to learn with minimal amount of intitial training data and subsequently update itself during operation, which significantly reduce the labor-intensive for data collecting and labeling. 

\section{Proposed Approach} \label{proposed_method}
Given a set of images $\{\mathbf{I}^{i}_{\mathcal{T}}\}_{i=1}^{n}$ and its reconstructed SfM model $\mathcal{E}$,  we aim to develop a sparse regression module, which can encode the entire environment $\mathcal{E}$ using a compact function $\mathfrak{F}(.)$, where $\mathfrak{F}$ is a deep neural network. The proposed function $\mathfrak{F}(.)$ inputs local descriptors $\mathbf{D}^{i}$ extracted from $\mathbf{I}^{i}$ and outputs its corresponding 3D global cloud coordinates $\mathbf{W}^{i}$. The ultimate goal of the proposed module is to perform \textit{visual localization}, a task of estimating camera pose $\mathbf{T} \in \mathbb{R}^{4\times4}$ of the query image $\mathbf{I}_{q}$.

\subsection{Representing Sparse Descriptors and 3D Coordinates} \label{simple_function}
Inspired by the universal continuous set function approximator \cite{qi2017pointnet,bui2022fast}, we first propose a simple element set learning function $\mathfrak{F}(.)$. It receives a set of descriptors $\{\mathbf{d}^{i}\}_{i=0}^{k}$ as input and outputs corresponding scene coordinates $\{\mathbf{w}^{i}\}_{i=0}^{k}$. Overall, the proposed function can be described as follows: 


\begin{equation}
\begin{aligned}
\mathfrak{F}\left(\{\mathbf{d}^{i}\}_{i=1}^{k}\right) & = \left\{f(\mathbf{d}^{i})\right\}_{i=1}^{k}\\
& = \{\mathbf{w}^{i}\}_{i=1}^{k},
\label{ori_equation}
\end{aligned}
\end{equation}

where $\mathfrak{F}:\mathbb{R}^{K \times D} \to \mathbb{R}^{K \times 4}$ and $f:\mathbb{R}^{D} \to \mathbb{R}^{4}$. $K$ is the number descriptors, and $D$ is its dimension. The function $f(.)$ is a shared nonlinear function. It receives a descriptor vector $\mathbf{d} \in \mathbb{R}^{D}$ and outputs a scene coordinate vector $\mathbf{w} = (x,y,z,p)^T$. We introduce an additional dimension $p$ for the scene coordinate, which represents the reliability probability for localization of the input descriptor $\mathbf{d}$. To ensure that the range of the reliability probability output lies within $[0,1]$ while enabling function $f(.)$ to produce $p\in \mathbb{R}$, we compute the final prediction for $\hat{p}$ using the following equation: 

\begin{equation}
\begin{aligned}
\hat{z} = \frac{1}{1+|\beta p|} \in (0,1],
\end{aligned}
\label{reliable_equation}
\end{equation}

where $\beta$ is a scale factor chosen to make the expected value of reliability prediction $\hat{z}$ easy to reach a small value when the input descriptors belong to high uncertainty regions. 


The proposed module is theoretically simple. As illustrated in Eq. \ref{ori_equation}, it requires only a non-linear function $f$ to compute the scene coordinates of given descriptors.  In practice, we approximate $f$ using a multi-layer perceptron (MLP) network, which is similar to \cite{brachmann2023accelerated}.  


% Figure environment removed


\subsection{Optimizing with Graph Attention} \label{graph_attension}
Solely relying on independent descriptors to regress scene coordinates may lead to an insufficient understanding of the global context and the distinctiveness of feature clusters. To address this issue, we draw inspiration from recent works on feature matching \cite{sarlin2020superglue, sun2021loftr}, which leverage attention aggregation \cite{vaswani2017attention} to learn both local and global attributes of sparse features. We employ a simplified version for the scenes regression task, which can be described as follows:

\begin{equation}
\begin{aligned}
\mathfrak{F}\left(\{\mathbf{d}^{i}\}_{i=1}^{k}\right) & = f \circ  \mathcal{A}^{l}\circ... \mathcal{A}^{1}\left(\{\mathbf{d}^{i} \}_{i=1}^{k}\right)  \\
& = \{\mathbf{w}^{i}\}_{i=1}^{k},
\end{aligned}
\label{ori_grap_re}
\end{equation}

where $\mathcal{A}:\mathbb{R}^{K \times D} \to \mathbb{R}^{K \times D}$, and $\mathcal{A}^{l}\circ... \mathcal{A}^{1}(.)$ represents a nested residual attention update for each descriptor $\mathbf{d}^{i}$ across $l$ graph layers. Given a coarse local descriptor $\mathbf{d}^{i}$, $\mathcal{A}^{l}(.)$ is used to compute its \textit{fine regression 
descriptor} $\prescript{(l)}{}{\mathbf{d}^{i}}$  by letting it communicate with each other across $l$ attention layers. This implicitly incorporates visual relationships and contextual cues with other descriptors in the same image, enabling it to extract robust features automatically through an end-to-end learning pipeline from scene coordinates. In addition, the proposed reliability-aware detection, as described in Eq. \ref{reliable_equation}, can also encourage our attention module to favor features from reliable areas (e.g. buildings, stable objects) and suppress those from unreliable regions (e.g. trees, sky, human). 



As $\prescript{(l)}{}{\mathbf{d}^{i}}$ is the representation for the output \textit{regression descriptor} of element $i$ at layer $l$, Eq. \ref{ori_grap_re} can be simplified as follows: 

\begin{equation}
\begin{aligned}
\mathfrak{F}(\{\mathbf{d}^{i}\}_{i=1}^{k}) & = \left\{f(\prescript{(l)}{}{\mathbf{d}^{i}}) \right\}_{i=1}^{k},
\end{aligned}
\end{equation}

which now has a similar form to the original Eq. \ref{ori_equation}, where the final regression layer $f$ is simply a shared MLP. 

Similar to a previous study \cite{sarlin2020superglue}, we also consider the attentional module as a multi-layer graph neural network. However, we only leverage the self-edge, which is based on self-attention \cite{vaswani2017attention}, to connect the descriptor $i$ to all others in the same image. The message passing formulation for element $i$ at the layer $(l+1)$ is described as

\begin{equation}
    \prescript{(l+1)}{}{\mathbf{d}^{i}}  = \prescript{(l)}{}{\mathbf{d}^{i}} + MLP\left(\left[\prescript{(l)}{}{\mathbf{d}^{i}} || \mathbf{m}_{\mathfrak{I}\rightarrow i}\right]\right),
\end{equation}



where $\mathbf{m}_{\mathfrak{I}\rightarrow i}$ denotes the aggregated message result from all descriptors $\{\prescript{(l)}{}{\mathbf{d}^{i}}\}_{i=1}^{k}$, with  $\mathfrak{I}=\{1,...,k\}$ indicating the set of descriptor indices, and $[.||.]$ denoting concatenation. This module consists of $L$ chained layers with different parameters. Therefore, the message $\mathbf{m}_{\mathfrak{I}\rightarrow i}$ is different in each layer. 

The self-attention mechanism performs an interaction to map the query vector $\mathbf{q}_{i}$ against a set of key vectors $\{\mathbf{k}_{j}\}_{j \in \mathfrak{I}}$, associated with candidate descriptors, to compute the attention score $\alpha_{ij}=$ Softmax$_{j}(\mathbf{q}_{i}^{T}\mathbf{k}_{j})$. It then presents the best-matched descriptors with their value vectors $\{\mathbf{v}_{j}\}_{j \in \mathfrak{I}}$ represented by a scored average of values, which in practice is the computation of message $\mathbf{m}_{\mathfrak{I}\rightarrow i}$:


\begin{equation}
\begin{aligned}
\mathbf{m}_{\mathfrak{I}\rightarrow i} = \sum_{j:(i,j)\in\mathfrak{I}}{\alpha_{ij}\mathbf{v}_{j}}.
\end{aligned}
\end{equation}



The query, key, and value are derived from linear projections between the descriptor $\prescript{(l)}{}{\mathbf{d}}$ with three different weight matrices $\mathbf{W}_{q}$, $\mathbf{W}_{k}$, and $\mathbf{W}_{v}$. This linear projection for all descriptors is expressed as



\begin{equation}
\begin{aligned}
% \mathbf{q} = \mathbf{W}_{q}\mathbf{d} + \mathbf{b}_{q}  \\
\begin{bmatrix}
\mathbf{q} \\
\mathbf{k} \\
\mathbf{v} 
\end{bmatrix} = \begin{bmatrix}
\mathbf{W}_{q} \\
\mathbf{W}_{k} \\
\mathbf{W}_{v} 
\end{bmatrix} \prescript{(l)}{}{\mathbf{d}} + \begin{bmatrix}
\mathbf{b}_{q} \\
\mathbf{b}_{k} \\
\mathbf{b}_{v}
\end{bmatrix}.
\end{aligned}
\end{equation}

Note that the weight parameters are different in each graph layer. In practice, we also applied multi-head attention \cite{vaswani2017attention} to improve the expressivity from different representation subspaces of different weight parameters. Fig. \ref{fig_attention_1} presents example results of attention on robust descriptors.


\subsection{Updating D2S with Unlabeled Observations} \label{unlabel_learning}
\begin{algorithm}[t]
% \DontPrintSemicolon
  
  \KwInput{$\mathcal{T} $, $\mathcal{D}_{\mathcal{T}}$, $\mathcal{W}_{\mathcal{T}}$, $\mathcal{U}$\\
  \begin{itemize}
    \item[-] $\mathcal{T} = \{\mathbf{I}^{i}_{\mathcal{T}}\}_{i=1}^{n}$ training images,
    \item[-] $\mathcal{D}_{\mathcal{T}}=\{\mathbf{D}^{i}_{\mathcal{T}}\}_{i=1}^{n}$ training descriptors,
    \item[-] $\mathcal{W}_{\mathcal{T}} = \{\mathbf{W}^{i}_{\mathcal{T}}\}_{i=1}^{n}$ training scene coordinates, 
    \item[-] $\mathcal{U}=\{\mathbf{I}^{i}_{\mathcal{U}}\}_{i=1}^{m}$ unlabeled images,
\end{itemize}
  }
  \KwOutput{ $ \mathcal{A}=\{\mathbf{D}^{i}_{\mathcal{U}}, \mathbf{W}^{i}_{\mathcal{U}}\}_{i=1}^{v}$
    \begin{itemize}
    \item[-] $\mathcal{D}_{\mathcal{U}}=\{\mathbf{D}^{i}_{\mathcal{U}}\}_{i=1}^{m}$ unlabeled descriptors, 
    \item[-] $\mathcal{W}_{\mathcal{U}} = \{\mathbf{W}^{i}_{\mathcal{U}}\}_{i=1}^{m}$ pseudo scene coordinates,
    \item[-] $ v \leq m$.
\end{itemize}}
    $\mathcal{D}_{\mathcal{U}} \leftarrow$ ExtractDescs$(\mathcal{U});$ \\
    $i:=1; \mathcal{A} \leftarrow \{\};$ \\
    \Do{$i \leq m, i++$}{
      $\mathbf{ids}_{\mathcal{T}} \leftarrow$ NearestVLAD$(\mathbf{I}_{\mathcal{U}}^{i}, \mathcal{T});$ \# nearest indices. \\
      $\mathcal{M} \leftarrow$ Matching$(\mathbf{D}_{\mathcal{U}}^{i}, \mathcal{D}_{\mathcal{T}}^{ \mathbf{ids}_{\mathcal{T}}});$ \\
      $s, \mathbf{W}^{i}_{\mathcal{U}} \leftarrow $ Copy$(\mathcal{M}, \mathcal{W}_{\mathcal{T}}^{\mathbf{ids}_{\mathcal{T}}});$ \\ 
      \If{$s \geq 50$}{
        $\mathcal{A} \leftarrow \mathcal{A} \cup \{\mathbf{D}^{i}_{\mathcal{U}}, \mathbf{W}^{i}_{\mathcal{U}}\};$ 
        }
        
    }
\caption{Pseudo-labelling for self-supervision}
\label{algo1}
\end{algorithm}

Due to the significant changes that can occur in environments over time since map creation, the ability to update from unlabeled data is crucial for most localization systems. We thus introduce a simple algorithm to update the proposed D2S with additional unlabeled data in a self-supervised learning manner.

Given a set of training images $\mathcal{T}$ and its labeled database of correspondent 2D--3D descriptors $\mathcal{D}_{\mathcal{T}}$ and $\mathcal{W}_{\mathcal{T}}$, we aim to find a set of pseudo-labels for new observation images $\mathcal{U}$. The pseudo-labels denote the newly generated 2D--3D correspondences of the unlabeled data $\mathcal{U}$. In detail, we extract the local descriptors of all images in $\mathcal{U}$, resulting in $\mathcal{D}_{\mathcal{U}}$. For each unlabeled image $\mathbf{I}_{\mathcal{U}}^{i}$, we then retrieve 10 nearest training images in the training database $\mathcal{T}$. Subsequently, we match two descriptor sources using a specific matching algorithm such as SuperGlue \cite{sarlin2020superglue}. Finally, we merge the top 10 matching pairs by copying their ground truth world coordinates from $\mathcal{T}$ to $\mathcal{U}$. This step is shown as line 6 of the algorithm \ref{algo1}, where the $\mathbf{W}^{i}_{\mathcal{U}}$ is the obtained pseudo scene coordinates of descriptor matrix $\mathbf{D}^{i}_{\mathcal{U}}$ and $s$ is the number of valid world coordinates. This process is summarized in algorithm \ref{algo1}. 

% \usepackage{graphicx}
% \usepackage{multirow}
% \usepackage{multirow}

\subsection{Loss Function} \label{loss_function}
According to Eq. \ref{ori_equation}, the output of sparse scene coordinates $\mathbf{w}$, predicted by D2S, includes four variables $(x,y,z,p)^{T}$. This output is rewritten as $(\hat{\mathbf{y}},p)^{T}$, where $\hat{\mathbf{y}}$ is the abbreviation of the estimated world coordinates vector and $p$ is the reliability probability. The loss function to minimize the estimated scene coordinates can be defined as 
\begin{equation}
    \mathcal{L}_{m} = \frac{1}{N}\sum^{N}\sum_{i=1}^{K} z_{i}\lVert \mathbf{y}_{i}-\hat{\mathbf{y}}_{i} \lVert^{2}_{2},
\label{loss_m}
\end{equation}

where $N$ is the number of mini-batch sizes, $K$ is the number of descriptors in the current frame, and $\mathbf{y}_{i}$ is the ground truth scene coordinate. We multiply the difference between estimated and ground truth world coordinates by $z_{i} \in \{0,1\}$, which is the ground truth reliability of descriptors. The goal is to ensure that the optimization focuses on robust descriptors while learning to ignore unreliable descriptors. 

Because the loss Eq. \ref{loss_m} is used to optimize only the robust features, we propose an additional loss function to simultaneously learn to be aware of this assumption, as follows:

\begin{equation}
    \mathcal{L}_{u} = \frac{1}{N}\sum^{N}\sum_{i=1}^{K}  \lVert z_{i}- \frac{1}{1+|\beta p|} \lVert^{2}_{2}.
\end{equation}

As described in Section \ref{simple_function}, $\beta$ is a scale factor chosen to enable the optimization to focus on the descriptors belonging to high uncertainty regions or failure cases of the detector. The combination of $\mathcal{L}_{m}$ and $\mathcal{L}_{u}$ can result in a stable optimization on two different criteria of robust descriptors.

In addition, we also apply the re-projection error loss for each 2D descriptor's position. The loss function aims to align the general scene coordinates following the correct camera rays, which is described as follows:

\begin{equation}
    \mathcal{L}_{r} = \frac{1}{N}\sum_{j=1}^{N}\sum_{i=1}^{K}z_{i}\lVert \pi(\mathbf{R}_{j} \mathbf{y}_{i}+\mathbf{t}_{j}) - \mathbf{u}_{i}\lVert^{2}_{2},
    \label{reproject_loss}
\end{equation} 

where $\mathbf{R}_{j}$ is the rotation matrix and $\mathbf{t}_{j}$ is the translation vector of camera pose $j$. $\pi(.)$ is a function to convert the estimated 3D camera coordinates to 2D pixel position, with $u_{i}$ as its ground truth pixel coordinate. 

Finally, these loss functions are combined with the scale factor as follows: 


\begin{equation}
    \mathcal{L}=\alpha_{m}\mathcal{L}_{m}+\alpha_{u}\mathcal{L}_{u}+\alpha_{r}\tau(t) tanh\bigg(\frac{\mathcal{L}_{\pi}}{\tau(t)}\bigg),
    \label{total_loss}
\end{equation}
where $\tau(t) = \omega(t) \tau_{max} + \tau_{min},  \text{ with } \omega(t) = \sqrt{1-t^{2}}$, $t\in(0,1)$ denotes the relative training progress \cite{brachmann2023accelerated}. The $\alpha_{r} = 1$ if $\frac{c_{i}}{c_{total}}>0.8$, otherwise $\alpha_{r} = 0$, where $c_{i}$ and $c_{total}$ are current and total training iterations respectively. 
\subsection{Camera Pose Estimation} \label{pose_estimation}
We estimate the camera pose using the most robust 3D coordinates based on their predicted reliability $\hat{z}$. We visualize examples of predicted robust coordinates in Fig. \ref{Uncertainties_results}. Finally, a robust minimal solver (PnP+RANSAC\cite{PoseLib}) is used, followed by a Levenberg--Marquardt-based nonlinear refinement, to compute the camera pose. 


In contrast to DSAC$^{*}$ \cite{brachmann2021visual} and several SCR-based works \cite{brachmann2017dsac, brachmann2018learning, li2020hierarchical}, which only focus on estimating dense 3D scene coordinates of the input image, our D2S learn to detect of salient keypoints and its coordinates. We believe that the SCR network should learn to pay more attention to salient features; this can lead to a better localization performance \cite{do2022learning, sarlin2021back}. This is also supported by empirical evidence of D2S's results.

% \addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Figure environment removed


\section{Experiments} \label{experiments}

We evaluate the proposed re-localization method on three different datasets: one indoor\cite{do2022learning} and two outdoor datasets\cite{kendall2015posenet} including a proposed dataset. We set $L=5$ for the number of graph attention with a subsequent MLP of $(D, 512, 1024, 1024, 512, 4)$, where $D=256$ is the dimensional number of SuperPoint\cite{detone2018superpoint} features. We used Pytorch \cite{paszke2019pytorch} to implement the proposed method and trained the network with 1.5M iterations with the batch size of a single image. The learning rate is set at $0.0001$ with the Adam optimizer \cite{kingma2014adam}, being reduced with a decay of 0.5 for every one-seventh total iterations. For data augmentation, we randomly apply brightness and contrast change of $\pm15\%$ and rotate and resize the image $\pm 30^{\circ}$. 

\subsection{Localization Results on Indoor6} 
\label{sec_datasets}
Indoor6 \cite{do2022learning} is a challenging indoor dataset consisting of images collected at different times and days. The results on this dataset are listed in Table \ref{indoor6_results}. Our D2S significantly outperforms PoseNet\cite{kendall2015posenet}, DSAC* \cite{brachmann2021visual}, ACE\cite{brachmann2023accelerated} and NBE+SLD \cite{do2022learning}. In detail, compared with NBE+SLD\cite{do2022learning}, the proposed method shows the largest improvement of 18.7\% accuracy in scene2a which reduces the error from 7.2cm/0.68$^{\circ}$ to 5.0cm/0.49$^{\circ}$. The high improvements are also observed in scene1, scene4a, and scene5. On average over 6 scenes, D2S reached the errors of 5.1cm/0.75$^{\circ}$ improved by 7.8\% compared to NBE+SLD's errors of 5.5cm/0.89$^{\circ}$. In storage efficiency comparison, NBE+SLD needs two different models of landmarks detector (SLD) and scene coordinate regression network (NBE) which cost 132MB but our method can show even with large improvement can be done with a much smaller single network, which costs only 22MB (6$\times$ times smaller). The lowest memory footprint is observed in ACE \cite{brachmann2023accelerated}. However, compared to our method, ACE achieved significantly inferior performance, with a 17.6\% reduction. This demonstrates the strong adaptability of the proposed method in environments with substantial condition changes.

\begin{table*}
\centering
\caption{\textbf{Results on Indoor6.} We report the median errors in cm for position and degree ($^{\circ}$) for the orientation of the Indoor6 dataset. We also show the percentage of frame errors within the threshold of 5cm and 5$^{\circ}$. The best results are \textbf{in bold} and the second best in \underline{underlined}.}
\label{indoor6_results}
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|cc|cc|cc|cc|cc|cc|cc} 
\hline
\multicolumn{16}{c}{Indoor6~~}                                                                                                                                                                                                                                                                              \\ 
\hline
\multirow{2}{*}{Method} & Size       & \multicolumn{2}{c|}{scene1}       & \multicolumn{2}{c|}{scene2a}      & \multicolumn{2}{c|}{scene3}       & \multicolumn{2}{c|}{scene4a}      & \multicolumn{2}{c|}{scene5}                & \multicolumn{2}{c|}{scene6}       & \multicolumn{2}{c}{Average}         \\
                        & (MB)       & (cm/deg.)         & (\%)          & (cm/deg.)         & (\%)          & (cm/deg.)         & (\%)          & (cm/deg.)         & (\%)          & (cm/deg.)                  & (\%.)         & (cm/deg.)         & (\%)          & (cm/deg.)          & (\%)           \\ 
\hline
PoseNet\cite{kendall2015posenet}                 & 50         & 159.0/7.46        & 0.0           & -/-               & -             & 141.0/9.26        & 0.0           & -/-               & -             & 179.3/9.37                 & 0.0           & 118.2/9.26        & 0.0           & -/-                & -              \\
DSAC*\cite{brachmann2021visual}                   & 28         & 12.3/2.06         & 18.7          & 7.9/0.9           & 28.0          & 13.1/2.34         & 19.7          & 3.7/0.95          & 60.8          & 40.7/6.72                  & 10.6          & 6.0/1.40          & 44.3          & 13.9/2.39          & 30.4           \\
ACE\cite{brachmann2023accelerated}                     & \textbf{4} & 13.6/2.1          & 24.9          & 6.8/0.7           & 31.9          & 8.1/1.3           & 33.0          & 4.8/0.9           & 55.7          & 14.7/2.3                   & 17.9          & 6.1/1.1           & 45.5          & 9.02/1.40            & 34.8           \\
NBE+SLD(E)\cite{do2022learning}              & 29         & 7.5/1.15          & 28.4          & 7.3/0.7           & 30.4          & 6.2/1.28          & 43.5          & 4.6/1.01          & 54.4          & \underline{6.3/0.96}           & \underline{37.5}  & 5.8/1.3           & 44.6          & 6.28/1.07          & 39.8           \\
NBE+SLD\cite{do2022learning}                 & 132        & \underline{6.5/0.90}  & \underline{38.4}  & \underline{7.2/0.68}  & \underline{32.7}  & \textbf{4.4/0.91} & \textbf{53.0} & \underline{3.8/0.94}  & \underline{66.5}  & \textbf{6.0/\textbf{0.91}} & \textbf{40.0} & \underline{5.0/0.99}  & \underline{50.5}  & \underline{5.48/0.89}  & \underline{46.9}   \\
D2S (\textbf{ours})     & \underline{22} & \textbf{5.0/0.89} & \textbf{49.9} & \textbf{5.0/0.49} & \textbf{51.4} & \underline{5.5/1.15}  & \underline{46.0}  & \textbf{2.6/0.65} & \textbf{75.3} & 8.2/1.3                    & 35.5          & \textbf{4.2/0.78} & \textbf{56.4} & \textbf{5.08/0.88} & \textbf{52.4}  \\
\hline
\end{tabular}
}
\end{table*}

\begin{table*}[t]
\centering
\caption{\textbf{Results on Cambridge Landmarks}. We report the median position and rotation errors of different relocalization methods on \textit{Cambridge} dataset. The best method is highlighted in \textbf{bold} and the second best in \underline{underlined}.}
\label{all_results_cambridge}
\resizebox{0.88\linewidth}{!}{%
\begin{tabular}{c|c|cc|cc|cc|cc|cc|cc} 
\hline
\multicolumn{14}{c}{Cambridge Landmarks\cite{kendall2015posenet}}                                                                                                                                                                                                                                                                                                                                                           \\ 
\hline
\multicolumn{2}{c|}{\multirow{3}{*}{Method}} & \multicolumn{2}{c|}{King's College} & \multicolumn{2}{c|}{Old Hospital}      & \multicolumn{2}{c|}{Shop Facade} & \multicolumn{2}{c|}{St Mary's Church}                      & \multicolumn{2}{c|}{Great Court}                                                                                                  & \multicolumn{2}{c}{Average}      \\ 
\cline{3-14}
\multicolumn{2}{c|}{}                        & Size       & Error                  & Size       & Error                     & Size       & Error               & Size       & Error                                         & Size       & Error                                                                                                                & Size       & Error               \\
\multicolumn{2}{c|}{}                        & (MB)       & (m/deg.)               & (MB)       & (m/deg.)                  & (MB)       & (m/deg.)            & (MB)       & (m/deg.)                                      & (MB)       & (m/deg.)                                                                                                             & (MB)       & (m/deg.)            \\ 
\hline
\multirow{3}{*}{FM}  & Hloc\cite{sarlin2019coarse, sarlin2020superglue}                  & 1877       & 0.06/0.10              & 1335       & 0.13/0.23                 & 316        & 0.03/0.14           & 2009       & 0.04/\textcolor[rgb]{0.122,0.122,0.157}{0.13} & 1746       & \textcolor[rgb]{0.122,0.122,0.157}{0.1}\textcolor[rgb]{0.122,0.122,0.157}{/}\textcolor[rgb]{0.122,0.122,0.157}{0.05} & 1459       & 0.07/0.13           \\
                     & AS\cite{sattler2016efficient}                    & 275        & 0.57/0.70              & 140        & 0.52/1.12                 & 37.7       & 0.12/0.41           & 359        & 0.22/0.62                                     & -          & \textcolor[rgb]{0.122,0.122,0.157}{0.24/0.13}                                                                        & -          & 0.33/0.60           \\
                     & SS\cite{yang2022scenesqueezer}                    & 0.3        & 0.27/0.38              & 0.53       & 0.37/0.53                 & 0.13       & 0.11/0.38           & 0.95       & 0.15/0.37                                     & -          & -/-                                                                                                                  & -          & -/-                 \\ 
\hline
\multirow{3}{*}{APR} & PoseNet\cite{kendall2015posenet}               & 50         & 0.88/1.04              & 50         & 3.2/3.29                  & 50         & 0.88/3.78           & 50         & 1.6/3.32                                      & 50         & 6.83/3.47                                                                                                            & 50         & 2.78/2.98           \\
                     & FeatLoc\cite{bach2022featloc}               & 34         & 1.3/3.84               & 34         & 2.1/6.1                   & 34         & 0.91/7.5            & 34         & 3.0/10.4                                      & -          & -/-                                                                                                                  & -          & -/-                 \\
                     & DFNet\cite{chen2022dfnet}                 & 60         & 0.73/2.37              & 60         & 2.0/2.98                  & 60         & 0.67/2.21           & 60         & 1.37/4.03                                     & -          & -/-                                                                                                                  & -          & -/-                 \\ 
\hline
\multirow{6}{*}{SCR} & DSAC++\cite{brachmann2018learning}                & 104        & 0.18/\underline{0.3}       & 104        & 0.20/\textbf{0.3}         & 104        & 0.06/\underline{0.3}    & 104        & 0.13/0.4                                      & 104        & 0.40/0.2                                                                                                             & 104        & 0.19/ 0.30          \\
                     & SCoCR\cite{li2020hierarchical}                 & 165        & 0.18/\underline{0.3}       & 165        & \underline{0.19}/\textbf{0.3} & 165        & 0.06/\underline{0.3}    & 165        & \textbf{0.09}/\underline{0.3}                     & 165        & \textbf{0.28}/\underline{0.2}                                                                                            & 165        & \underline{0.16/0.28}   \\
                     & DSAC*\cite{brachmann2021visual}                 & 28         & \underline{0.15/0.3}       & 28         & 0.21/0.4                  & 28         & \underline{0.05/0.3}    & 28         & 0.13/0.4                                      & 28         & \textcolor[rgb]{0.122,0.122,0.157}{0.49/0.3}                                                                         & 28         & 0.21/0.34           \\
                     & ACE\cite{brachmann2023accelerated}                   & \textbf{4} & 0.28/0.4               & \textbf{4} & 0.31/0.6                  & \textbf{4} & \underline{0.05/0.3}    & \textbf{4} & 0.18/0.6                                      & \textbf{4} & 0.43/\underline{0.2}                                                                                                     & \textbf{4} & 0.25/0.42           \\
                     & HSCNet++\cite{wang2024hscnet++}              & 85         & 0.19/0.34              & 85         & 0.20/\underline{0.31}         & 85         & 0.06/0.24           & 85         & \textbf{0.09/0.27}                            & 85         & 0.39/0.23                                                                                                            & 85         & 0.19/\underline{0.28}   \\
                     & D2S (\textbf{ours})   & \underline{22} & \textbf{0.08/0.12}     & \underline{22} & \textbf{0.16/0.30}        & \underline{22} & \textbf{0.04/0.17}  & \underline{22} & \underline{0.10/0.30}                            & \underline{22} & \underline{0.34}\textbf{/0.17}                                                                                           & \underline{22} & \textbf{0.14/0.21}  \\
\hline
\end{tabular}
}
\end{table*}


\subsection{Localization Results on Cambridge Landmarks}
\label{outdoor_results_cambridge}
To evaluate the quality of re-localization performance on the Cambridge Landmarks\cite{kendall2015posenet} dataset, we report the median pose error for each scene, and the results are displayed in Table \ref{all_results_cambridge}. In comparison, APR-based approaches including PoseNet \cite{kendall2015posenet}, FeatLoc \cite{bach2022featloc}, and DFNet \cite{chen2022dfnet}, achieved the lowest performance with low memory demand for all scenes. In contrast, the FM-based Hloc \cite{sarlin2020superglue, sarlin2019coarse} pipeline achieved the highest re-localization accuracy but requires a high memory footprint. We also report the results of the compressed version  SceneSqueezer (SS) \cite{yang2022scenesqueezer} on this dataset, which has a significantly low memory footprint. However, its accuracy is much worse compared to Hloc.

Importantly, we report the results of SRC-based approaches as well as the storage demand of each scene. The proposed D2S achieved the highest accuracy in King's College, Old Hospital, and Shop Facade while St Mary's Church reached second best and Great Court had the lowest rotation error. In detail, for example, in the King's College scene, our method shows 47\% position improvement compared to the second-best method of DSAC*\cite{brachmann2021visual}. The reduction of error is from 0.15m/0.3$^{\circ}$ to 0.08m/0.12$^{\circ}$. 

On average, our method marks the top-1 accuracy among learning-based localization methods, which improved by 12.5\% in positional error compared to the second-best method of SCoCR \cite{li2020hierarchical}.

% Figure environment removed

\begin{table}
\centering

\caption{\textbf{Results on Ritsumeikan BKC.} We report the median position and rotation errors in comparison with other methods on \textit{BKC} dataset. The accuracy is calculated within 0.5m and 10$^{\circ}$. The best results in SCR-based are highlighted in \textbf{bold} and the second best in \underline{underlined}.}
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|cc|cc|cc|cc} 
\hline
\multicolumn{2}{c|}{\multirow{5}{*}{Method}} & \multicolumn{8}{c}{Ritsumeikan BKC}                                                                                                                            \\ 
\cline{3-10}
\multicolumn{2}{c|}{}                        & \multicolumn{2}{c|}{Seq-2~}         & \multicolumn{2}{c|}{Seq-3~}         & \multicolumn{2}{c|}{Seq-4~~}       & \multicolumn{2}{c}{\multirow{2}{*}{Average}}  \\
\multicolumn{2}{c|}{}                        & \multicolumn{2}{c|}{(domain-shift)} & \multicolumn{2}{c|}{(domain-shift)} & \multicolumn{2}{c|}{(nighttime)}   & \multicolumn{2}{c}{}                          \\ 
\cline{3-10}
\multicolumn{2}{c|}{}                        & Errors             & Acc.           & Errors             & Acc.           & Errors             & Acc.          & Errors             & Acc.                     \\
\multicolumn{2}{c|}{}                        & (m/deg.)           & (\%)           & (m/deg.)           & (\%)           & (m/deg.)           & (\%)          & (m/deg.)           & (\%)                     \\ 
\hline
FM                   & Hloc                  & 0.25/2.12          & 57.3           & 0.04/0.19          & 83.5           & 0.03/0.30          & 74.1          & 0.11/0.87          & 71.6                     \\ 
\hline
\multirow{4}{*}{SCR} & DSAC*\cite{brachmann2021visual}                 & 29.6/67.1          & 0.0            & 8.92/28.5          & 0.0            & 8.85/57.0          & 0.0           & 15.8/50.8          & 0.0                      \\
                     & ACE\cite{brachmann2023accelerated}                   & \underline{0.18/1.7}   & \underline{53.4}   & 8.64/60.2          & 18.3           & 3.89/59.5          & 2.5           & 4.24/40.5          & 24.7                     \\
                     & D2S(\textbf{ours})    & 0.43/3.62          & 51.5           & \underline{0.36/2.24}  & \underline{56.0}   & \underline{0.18/2.12}  & \underline{63.0}  & \underline{0.32/2.6}   & \underline{56.8}             \\
                     & D2S+(\textbf{ours})   & \textbf{0.16/1.34} & \textbf{75.7}  & \textbf{0.09/0.48} & \textbf{93.6}  & \textbf{0.08/1.29} & \textbf{70.4} & \textbf{0.11/1.04} & \textbf{79.9}            \\
\hline
\end{tabular}
}
\label{BKC_median_results}
\end{table}



\subsection{Results for Outdoor Localization on BKC Dataset} \label{outdoor_results_BKC}

This section reports the performance of the proposed method and other baselines on the proposed Ritsumeikan BKC dataset, illustrated in Fig. \ref{BKC_train_test}. This dataset has not been evaluated by previous localization methods; therefore, we selected three state-of-the-art baselines, FM-based Hloc \cite{sarlin2019coarse, sarlin2020superglue}, SCR-based DSAC* \cite{brachmann2021visual} and ACE\cite{brachmann2023accelerated}, to draw a comparison with the proposed D2S. We used the same training and localization configurations stated in the original papers\cite{sarlin2019coarse, brachmann2021visual,brachmann2023accelerated}. 

Table \ref{BKC_median_results} reports the median localization errors of ACE, DSAC*, Hloc, and the proposed D2S on different test sequences of the BKC dataset. The BKC dataset features several challenges for visual localization methods on their capacity to generalize beyond the training data (high domain shifts and transition from day to night). Thus, it is particularly challenging for SCR approaches because significant domain shifts can result in substantial differences between training and test images. This is evident in the DSAC* and ACE's worse results on testing sequences. On the other hand, the proposed D2S outperforms DSAC* and ACE with a remarkable improvement in localization accuracy of 32.1\%.


\textbf{Self-update with unlabeled data (D2S+).} Importantly, the BKC dataset is proposed to evaluate localization methods in the capability of updating with unlabeled observations. Note that the previous datasets \cite{ kendall2015posenet, do2022learning} do not offer any available new observations for evaluating self-supervision. In the BKC dataset, each test sequence in has a corresponding unlabeled sequence recorded at different times. For self-supervision, we update the model with an additional 50k iterations using data generated by Algorithm \ref{algo1}.

% Figure environment removed

We also report the results of D2S+ when updated with unlabeled data in Table \ref{BKC_median_results}. Notably, the improvement in accuracy obtained through D2S, is from 56.8\% to 79.9\%, within the threshold of 50cm and 10$^{\circ}$. D2S+ even surpasses Hlocs accuracy by 8.3\%. Fig. \ref{bkc_seq2_est} shows an example of estimated trajectories of Hloc, D2S, and D2S+ on WestWing Seq-2, where D2S+ successfully predicts the camera position, even at the positions located far from the scene. 

% Figure environment removed

\textbf{Hloc and D2S+ Comparison.} As shown in the reported results, D2S+ performed better in locations lacking training data aided by self-supervision capability. Notably, both D2S+ and Hloc used the same amount of reference/training data. We found several failed cases in the image retrieval step (an important step in the Hloc pipeline), as illustrated in Fig. \ref{netvlad}. This results in an inferior performance of Hloc at locations lacking reference data. However, D2S+ overcomes this drawback by learning the general hints from unlabeled data even when facing this difficulty. Note that the unlabeled data contains no information about the reconstructed SfM models.

\subsection{Additional Results}
We evaluated our method using an Nvidia GeForce GTX 1080ti GPU and an Intel Core i9-9900, where the SuperPoint\cite{detone2018superpoint} feedforward takes 10ms for the indoor image size of 640, and 19ms for outdoor image size of 1024, and D2S's 3D coordinates computing takes 4ms. The camera pose estimation time varies depending on the quality of predicted coordinates and reliabilities. We report this estimation time in the table \ref{pnp_inference}. The proposed D2S shows a significant enhancement in reducing PnP RANSAC times \cite{PoseLib} while also improving localization accuracy. In Table \ref{change_graph_results}, we additionally report the results on different graph settings. 

\begin{table*}
\centering
\caption{\textbf{Reliability Effects}. We report the effects of reliability prediction on the re-localization performance on the Cambridge \cite{kendall2015posenet} dataset using the metric of median errors on cm and $^{\circ}$, and accuracy on the threshold of 5cm/5$^{\circ}$. The best results are in \textbf{bold}.}
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|cc|cc|cc|cc|cc} 
\hline
\multirow{3}{*}{D2S Reli.} & \multicolumn{2}{c|}{Kings College}    & \multicolumn{2}{c|}{Old Hospital}    & \multicolumn{2}{c|}{Shop Facade}               & \multicolumn{2}{c|}{St Mary's Church}         & \multicolumn{2}{c}{Great Court}                   \\ 
\cline{2-11}
                            & Error                   & Avg. PnP time    & Error                  & Avg. PnP time    & Error                            & Avg. PnP time    & Error                           & Avg. PnP time    & Error                              & Avg. PnP time     \\
                            & (cm/deg./\%)            & (ms)        & (cm/deg./\%)           & (ms)        & (cm/deg./\%)                     & (ms)        & (cm/deg./\%)                    & (ms)        & (cm/deg./\%)                       & (ms)         \\ 
\hline
No                          & 7.9/0.132/30.0          & 26          & 16.6/0.314/\textbf{10.4} & 34          & 3.8/\textbf{0.171}/67.0          & 23          & 10.0/0.311/\textbf{13.7} & 72         & 34.4/\textbf{0.163}/0.26          & 30           \\
Yes                         & \textbf{7.6/0.125/34.1} & \textbf{21} & \textbf{15.9/0.303/10.4} & \textbf{18} & \textbf{3.6/}0.173/\textbf{69.9} & \textbf{18} & \textbf{9.8/0.305/}13.2          & \textbf{48} & \textbf{34.0/}0.170/\textbf{0.66} & \textbf{18}  \\
\hline
\end{tabular}
}
\label{pnp_inference}
\end{table*}


\begin{table}[t]
\centering
\caption{Results on the different number of graph attention.}
\resizebox{0.7\linewidth}{!}{%
\begin{tabular}{c|c|c} 
\hline
                     & \multirow{2}{*}{+Config.} & Cambridge\cite{kendall2015posenet}                             \\
                     &                           & King's College                        \\ 
\hline
\multirow{3}{*}{D2S} & No graph network          & 22.2cm/0.31$^\circ$/3.53\%                        \\
                     & Smaller (2 layers)        & 9.2cm/0.16$^\circ$/23.5\%             \\
                     & Full (5 layers)           & \textbf{7.6cm/0.12$^{\circ}$/34.1\%}  \\
\hline
\end{tabular}
}
\label{change_graph_results}
\end{table}


\section{Conclusion} \label{conclusion}
In this study, we introduced D2S, a novel direct sparse regression approach for establishing 2D--3D correspondences in visual localization. Our proposed pipeline is both simple and cost-effective, enabling the efficient representation of complex descriptors in 3D visual cloud maps. By utilizing an attention graph and straightforward loss functions, D2S was able to accurately classify and learn to focus on the most reliable descriptors, leading to significant improvements in re-localization performance. We conducted a comprehensive evaluation of our proposed D2S method on three distinct indoor and outdoor datasets, demonstrating its superior re-localization accuracy compared to state-of-the-art baselines. Furthermore, we introduced a new dataset for assessing the generalization capabilities of visual localization approaches. The obtained results demonstrate that D2S can generalize beyond its training data, even in challenging domain shifts such as day-to-night transitions and in locations lacking training data. Additionally, D2S is capable of being self-supervised with new observations, eliminating the need for camera poses, camera parameters, and ground truth scene coordinates.


\bibliographystyle{IEEEtran}
\bibliography{reference.bib}




\end{document}
