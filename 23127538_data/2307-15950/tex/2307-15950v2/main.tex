%% 
%% Copyright 2019-2020 Elsevier Ltd
%% 
%% This file is part of the 'CAS Bundle'.
%% --------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'CAS Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for cas-sc documentclass for 
%% double column output.

%\documentclass[a4paper,fleqn,longmktitle]{cas-sc}
\documentclass[a4paper,fleqn]{cas-sc}

% \usepackage[numbers]{natbib}
%\usepackage[authoryear]{natbib}
\usepackage[authoryear,longnamesfirst]{natbib}
\usepackage{siunitx}
\usepackage{color}
\usepackage{tabularx}
\usepackage{booktabs}
%%%Author definitions
\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}
\tsc{EP}
\tsc{PMS}
\tsc{BEC}
\tsc{DE}
%%%

% Uncomment and use as if needed
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}[theorem]{Lemma}
%\newdefinition{rmk}{Remark}
%\newproof{pf}{Proof}
%\newproof{pot}{Proof of Theorem \ref{thm}}


\begin{document}
% 

\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}

% Short title
\shorttitle{ }

% Short author
\shortauthors{J. Liu, et~al.}

% Main title of the paper
\title [mode = title]{Teaching Autonomous Vehicles to Express Interaction Intent during Unprotected Left Turns: A Human-Driving-Prior-Based Trajectory Planning Approach}                      

% First author
\author{Jiaqi Liu}[orcid=0000-0002-6171-6155]
% Email id of the first author
\ead{liujiaqi13@tongji.edu.cn}
\author{Xiao Qi}
\ead{qixiaog@tongji.edu.cn}
\author{Ying Ni}
\ead{ying_ni@tongji.edu.cn}
\author{Jian Sun}
\ead{sunjian@tongji.edu.cn}
\author{Peng Hang}
\ead{hangpeng@tongji.edu.cn}
% Corresponding author indication
\cormark[1]
% Address/affiliation
\affiliation{organization={Department of Traffic Engineering \& Key Laboratory of Road and Traffic Engineering, Ministry of Education, Tongji University},
    country={China}}

% Corresponding author text
\cortext[cor1]{Corresponding author}
% \cortext[cor2]{Principal corresponding author}

% Footnote text
% \fntext[fn1]{This is the first author footnote. but is common to third
%   author as well.}
% \fntext[fn2]{Another author footnote, this is a very long footnote and
%   it should be a really long footnote. But this footnote is not yet
%   sufficiently long enough to make two lines of footnote text.}

% For a title note without a number/mark
% \nonumnote{This work was supported in part by the National Natural Science Foundation of China (52125208, 52232015, 52272313), the Young Elite Scientists Sponsorship Program by CAST (2022QNRC001) and the Fundamental Research Funds for the Central Universities.
%   }

% Here goes the abstract
\begin{abstract}
Incorporating Autonomous Vehicles (AVs) into existing transportation systems necessitates examining their coexistence with Human-driven Vehicles (HVs) in mixed traffic environments. Central to this coexistence is the AVs' ability to emulate human-like interaction intentions within traffic scenarios. 
We introduce a novel framework for planning unprotected left-turn trajectories for AVs, designed to mirror human driving behaviors and effectively communicate social intentions. This framework consists of three phases: trajectory generation, evaluation, and selection.
In the trajectory generation phase, we utilize real human-driving trajectory data to establish constraints for a predicted trajectory space, creating candidate motion trajectories that reflect intent. The evaluation phase incorporates maximum entropy inverse reinforcement learning (ME-IRL) to gauge human trajectory preferences, considering aspects like traffic efficiency, driving comfort, and interactive safety.
During the selection phase, a Boltzmann distribution-based approach is employed to assign rewards and probabilities to the candidate trajectories, promoting human-like decision-making. We validate our framework using an authentic trajectory dataset and conduct a comparative analysis with various baseline methods.
Our results, derived from simulator tests and human-in-the-loop driving experiments, affirm our framework's superiority in mimicking human-like driving, expressing intent, and computational efficiency. 
For additional information of this research, please visit \url{https://shorturl.at/jqu35}.

\end{abstract}

% Use if graphical abstract is present
% \begin{graphicalabstract}
% % Figure removed
% \end{graphicalabstract}
% Research highlights
\begin{highlights}
\item A left-turn trajectory planning framework capable of expressing social intentions is proposed.
\item A method based on maximum entropy inverse reinforcement learning is introduced to learn and evaluate human trajectory preferences.
\item Prior information and latent interaction norms from human experts are learned and embedded into the trajectory planning process to enable the expression of latent intent communication.
%\item Simulation experiments and human-in-the-loop driving experiments demonstrate the superiority in intent expression capabilities and computational efficiency of our methods.
\end{highlights}


% Keywords
% Each keyword is seperated by \sep
\begin{keywords}
Autonomous Vehicles, Human-like Trajectory Planning, Intent Expression, Inverse Reinforcement Learning
\end{keywords}


\maketitle

\section{Introduction}
The emergence of Autonomous Vehicles (AVs) marks a revolutionary era in transportation, promising extensive transformations throughout the entire system. In the dynamic landscape of mixed-traffic scenarios, ensuring the harmonious coexistence of AVs and Human-Driven Vehicles (HVs) stands as a matter of paramount significance(\cite{toghi2021cooperative,wang2022social}). 
The ability of AVs to engage in fluid, understandable interactions with humans is a key factor in their integration into the existing traffic system. Unlike human drivers who can engage in flexible and varied forms of social interaction to communicate, current AVs still lack capabilities in interactive communication(\cite{toghi2022social}). In recent years, enhancing the social interaction abilities of autonomous driving has become a focal point for many researchers(\cite{schwarting2019social,crosato2022interaction}). Most studies, however, have concentrated on high-level decision-making capabilities. In the real-world traffic scenario, movement trajectories also serve as a vital means for drivers to interact and communicate (e.g., trajectory deviation, speed adjustment) (\cite{de2013road,lee2021road}). Present-day AVs lack the capacity to express social intentions at the level of trajectory behavior. Their driving actions often appear ambiguous and difficult for humans to understand (\cite{wang2022social}), posing greater challenges for interaction safety in mixed driving environments (\cite{wang2022social,valiente2023prediction}).

% Unlike the implicit communication and interaction observed among human drivers through nuanced driving trajectories, such as trajectory deviation and speed adjustment (\cite{de2013road,lee2021road}), AVs face challenges in expressing social intent at the trajectory behavior level (\cite{wang2022social}). This lack of clarity in their driving behaviors poses a significant challenge, raising substantial concerns about interactive safety in mixed-driving environments (\cite{wang2022social}).

Among existing trajectory planning algorithms, those based on discrete optimization can meet requirements such as ease of construction, smooth trajectory, and continuous derivability. These algorithms can execute simple trajectory actions like collision avoidance and lane changing in high-speed following and lane-changing scenarios. However, they still struggle to accurately represent the real driving behaviors of human drivers, leading to notable differences between the planned and actual human trajectories.
In response to this incongruity, prior research (\cite{huang2021driving, Song1797}) has introduced trajectory planning techniques designed to emulate human-like behaviors, drawing inspiration from and imitating expert human trajectories.

Despite the potential of these methods to leverage real HV trajectories for instructing AV trajectory planning, they leave several critical issues unresolved. A key challenge lies in the fact that high-level decision-making intentions are pivotal in constraining the trajectory planning space. Existing learning methods often derive AV decisions from trajectory outcomes, contradicting the fundamental principle of 'decision precedes execution.' Moreover, these methods tend to overlook the encapsulated intent expression information within trajectory movements influenced by high-level decisions. This oversight is particularly pronounced in complex intersection interaction scenarios with diverse trajectories (\cite{wang2022social}). The intent information embedded within trajectory movements plays a crucial role during the interaction between both parties, leading to noticeable differences in trajectory motion strategies and anticipated spaces under varying decisions (\cite{Song1797}).

These challenges escalate in complexity and importance when an AV navigates an intersection, one of the most demanding interaction scenarios (\cite{zhao2023unprotected}). This complexity often obstructs an AV from executing unprotected left turns with the finesse of a human driver.

In response to these challenges, we select the most representative unprotected left-turn scenario and proposes a trajectory planning framework that not only learns from human trajectory prior knowledge but also effectively expresses social intent. This framework unfolds in three meticulously designed stages: trajectory generation, evaluation, and selection.
In the trajectory generation stage, we extract implicit interaction intent rules from human trajectory data. Here, we establish constraints on the expected trajectory space under various decisions, drawing insights from human implicit intent experiences. The outcome is the generation of candidate motion trajectories that purposefully consider intent expression.
Moving into the evaluation stage, a robust reward function is constructed, taking into account efficiency, comfort, and safety. Leveraging the power of maximum entropy inverse reinforcement learning (ME-IRL), we learn and evaluate human trajectory preferences within this framework.
In the selection stage, we intricately set up a probability distribution of reward gains and candidate trajectories, guided by the Boltzmann distribution. This enables a human-like action selection process, enriching the framework with the nuanced decision-making dynamics observed in human drivers.

The efficacy of our trajectory planning framework is rigorously substantiated through a comprehensive validation process employing a real trajectory dataset. Comparative analyses against multiple baseline methods are conducted, drawing insights from simulator-based assessments and real-world human-in-loop driving experiments.

In summary, our contributions are shown as follows:
\begin{itemize}
    \item We propose a left-turn trajectory planning framework capable of expressing social intentions, comprising three stages: trajectory generation, trajectory evaluation, and trajectory selection. This framework learns prior information from human expert trajectories and embeds learned human latent interaction norms into the trajectory planning process, enabling the expression of latent intent communication.
    \item In the trajectory generation phase, we analyze real human trajectory data to uncover the implicit prior information of human left-turn driving in interactive scenarios. We establish expected trajectory space constraints under different decision-making scenarios and propose a trajectory generation method that considers intent expression methods.
    \item We introduce a method for learning and evaluating human trajectory preferences based on Maximum Entropy Inverse Reinforcement Learning, with action selection humanized through the Boltzmann distribution. The algorithm has been validated through both simulated experiments and human-in-the-loop driving simulation platforms, demonstrating superiority in intent expression capabilities and computational efficiency.
\end{itemize}

\section{Related Works}
% In the hierarchical architecture of autonomous driving interaction strategies, once the intent decision is formulated, corresponding action execution methods are essential to ensure the effective implementation of the decision. The methods for executing actions after intent decision can be categorized into motion planning algorithms and trajectory planning algorithms, based on different planning philosophies\cite{claussmann2019review}. Motion planning algorithms typically consider constraints related to decision intent and the current interaction state, involving calculations of acceleration, heading angles, and other kinematic aspects. On the other hand, trajectory planning algorithms primarily start from a global perspective, deriving a collision-free executable trajectory based on the decision intent to ensure the vehicle safely and efficiently reaches its destination from the starting point.

% \subsection{Trajectory Planning Methods for AV}
Once a decision intent is formulated during the motion process, AVs necessitate a suitable trajectory planning algorithm to create a collision-free, executable path. This planning ensures the vehicle can traverse from the start point to the destination efficiently and safely.

Discrete optimization-based trajectory planning methods, such as the Frenet trajectory planning method  (\cite{werling2010optimal}), are widely employed. This method operates on the Frenet coordinate system, converting the problem into two-dimensional S-T and L-T problems. Building on this, several studies  (\cite{zhang2020optimal,hu2022probabilistic,hu2018dynamic}) have enhanced and optimized the Frenet trajectory planning method. Apollo researchers  (\cite{zhang2020optimal}), drawing from the Frenet trajectory planning method, proposed a quadratic optimization method to cater to the non-holonomic constraints of the vehicle. Hu et al. (\cite{hu2018dynamic}) suggested a real-time dynamic path planning method for autonomous driving, computing collision risk via the Gaussian convolution algorithm to ensure safe interaction with both static and dynamic obstacles.
Other methods, like the Archimedean spiral  (\cite{ma2017two}) and the Bezier curve  (\cite{zhou2022autonomous}), are also used to characterize motion trajectories during intersection turns or high-speed lane changes. However, these methods necessitate more parameters such as trajectory control points and control parameters, and unlike polynomial methods that already incorporate the planning results of linear speed and acceleration, these methods need to further resolve motion planning parameters.

In comparison to motion planning methods that directly output discrete instantaneous kinematic control parameters, trajectory planning algorithms based on discrete optimization generate and plan reasonable operational trajectories through a controllable trajectory generation space, addressing issues such as the lack of global planning, non-smooth trajectories, and the neglect of trajectory diversity and intent expression features.

% \subsection{Behavioral Intent Expression in Planning Algorithms}
While trajectory planning methods based on discrete optimization can fulfill requirements such as easy control and construction of candidate trajectories, as well as smooth and continuously differentiable trajectories, they still struggle to depict the real driving behaviors of human drivers. This discrepancy between planned trajectories and actual human trajectory behaviors has been noted (\cite{Song1797}), leading to significant differences between planned trajectories for AVs and human trajectory selection methods. Furthermore, it results in a lack of expressive capabilities for intent communication under social norms.

On one hand, current research (\cite{hu2022probabilistic,werling2010optimal}) often employs subjective methods, such as manually setting feature weights in trajectory planning reward functions or iteratively experimenting to optimize them. The constructed settings lack alignment with the cognitive characteristics of drivers and exhibit significant differences from human trajectory selection methods. Inverse Reinforcement Learning (IRL) provides a potential solution to this problem. It is an approach designed to reconstruct reward functions by learning from expert example trajectories (\cite{arora2021survey}).

IRL has been widely applied to problems such as route planning (\cite{ziebart2008maximum,wulfmeier2017large}) and learning human behavior trajectory features (\cite{abbeel2004apprenticeship}). For instance, Abbeel et al. (\cite{abbeel2004apprenticeship}) propose an IRL algorithm based on apprenticeship learning and apply it to learn different driving styles. Molly et al. (\cite{molloy2020online}), using optimization control theory, design an IRL algorithm and utilize real-time collected data to learn human driving behavior. Common IRL methods include Maximum Entropy IRL (ME-IRL) (\cite{ziebart2008maximum,wulfmeier2017large}), apprenticeship IRL (\cite{abbeel2004apprenticeship}), and Bayesian IRL (\cite{ramachandran2007bayesian}), among which ME-IRL, in comparison to other IRL methods, possesses significant advantages in studying human expert trajectory behavior. It excels in non-deterministic modeling, learning behavioral diversity, and strong generalization capabilities.
% Wu et al. (\cite{wu2020efficient}) propose a sampling-based continuous domain ME-IRL algorithm, enhancing the sampling efficiency of ME-IRL through temporal-spatial decoupling and elastic sampling, effectively learning human driving behavior. Huang et al. (\cite{hang2020human}), considering workplace interaction, further introduce Boltzmann noise theory to describe trajectory distribution, assuming an exponential relationship between trajectory probability and reward. Finally, they establish a highway driving trajectory planning model based on ME-IRL.

Therefore, some studies have combined ME-IRL with discrete optimization trajectory planning methods to learn the human driving behavior trajectory selection mechanism. Wu et al. (\cite{wu2020efficient}) proposed a sampling-based continuous domain ME-IRL algorithm, improving ME-IRL's sampling efficiency through spatiotemporal decoupling and elastic sampling, effectively learning human driving behavior. Huang et al. (\cite{hang2020human}) introduced inter-vehicle interaction considerations, describing trajectory distributions through Boltzmann noise theory, assuming an exponential relationship between trajectory probability and rewards, and ultimately establishing a highway driving trajectory planning model based on ME-IRL. Song Dongjian et al. (\cite{Song1797}) proposed a driving behavior strategy framework with a matched driving behavior generation mechanism, learning a human-like reward function through ME-IRL and establishing a mapping between behavior probability and cumulative rewards using the Boltzmann rational noise model. By discretizing the expected trajectory space, they avoided the dimensionality disaster in continuous high-dimensional space integration, compressing and pruning the expected trajectory space based on statistical laws and safety constraints, thus enhancing trajectory sampling efficiency.

On the other hand, solely relying on inverse reinforcement learning may be insufficient to reproduce implicit intent expression features in trajectory behaviors under different decisions, especially in complex scenarios like unprotected left turns at intersections (\cite{xu2019simulation,ma2017two2}). There is significant implicit intent communication in HV-HV interaction processes (\cite{de2013road}), and typically, human drivers prefer and trust implicit information communication methods for interactive driving tasks (\cite{lee2021road}). 
Studies have found that AVs can optimize interaction processes by implicitly expressing intentions through adjustments in approach speed, distance, lateral deviation, and braking positions. De Winter et al. (\cite{de2009left}) found that speed, distance, and time of arrival significantly influence the likelihood of road users crossing in front of AVs. Dey (\cite{dey2021communicating}) demonstrated that the braking position and deceleration taken by AVs significantly influence pedestrians' judgment of AV intent. Dietrich et al. \cite{dietrich2020implicit} explored the impact of different deceleration modes and vehicle pitch values on pedestrian crossing behavior, finding that scenarios under defensive deceleration modes prompted earlier crossing by pedestrians. Rettenmaier et al. (\cite{rettenmaier2021communication}) studied the impact of speed and lateral deviation of AVs in bottleneck sections on vehicle interaction, showing that when AV motion includes lateral deviation, participants' throughput efficiency and safety are improved. Left-turning vehicles at intersections can also express their interactive intentions through implicit communication methods such as early (\cite{YangLan2022Human}) and delayed (\cite{ma2017two}) lateral deviations or early heading angle deviations (\cite{YangLan2022Human}), making the intentions of both parties more explicit and understandable. Although the aforementioned studies have explored the impact of implicit intent expression, few have considered incorporating intent expression methods into trajectory planning work. Ma et al. (\cite{ma2017two}) modeled trajectory motion features from a human-like perspective but overlooked the intrinsic correlation between trajectory operational characteristics and intent decisions, and failed to comprehensively construct intent expression methods under all decision-making scenarios.
% Currently, there is limited work considering intent expression methods in trajectory planning. Ma et al. (\cite{ma2017two}), from a humanization perspective, model certain features of trajectory motion but overlook the inherent correlation between trajectory behavior and intent decision. They also fail to comprehensively construct intent expression methods for trajectory behaviors under all decisions.

To address the traditional trajectory planning algorithms' deficiency in expressing social intent, we utilize ME-IRL to learn the human driving behavior trajectory selection mechanism and construct the expected trajectory space considering the intent expression mode under varying decisions.

% \subsection{Motion Planning Methods for AV}
% Research methodologies for driving motion planning commonly fall into three major categories: mathematical models, machine learning, and interactive game theory. Mathematical model-based motion planning methods include potential fields(~\cite{xu2018simulation}), risk fields(~\cite{wang2016driving}), and social forces~(\cite{ma2017two,li2021modified}), employing computational models with physical and mathematical significance. These models assign or calculate the driving force for the vehicle, describe interaction risks through field strength or forces, compute relevant parameters for motion, and facilitate collision avoidance during the journey, ensuring the vehicle reaches its destination safely. Machine learning-based motion planning (~\cite{qi2019autonomous,ye2019automated}) takes environmental interaction states as inputs, generating vehicle motion parameters for planning, setting reward functions, and obtaining optimal motion reaction actions through iterative learning processes. Similarly, interactive motion planning research based on game theory principles (\cite{schwarting2019social,zhao2021yield,wang2021competitive,yang2016cooperative}) constructs a game framework for interacting parties, defines payoff functions for both sides, and solves motion planning parameters through optimization. It is noteworthy that some motion planning methods only output kinematic parameters at specific time instances, exhibiting clear discretization and localization features. These approaches lack a global planning process and require constraints to ensure the executability of consecutive actions, potentially resulting in non-smooth trajectories that cannot be effectively tracked and applied by the vehicle control system. While another set of motion planning methods may select the lane centerline (virtual lane centerline) (\cite{qi2019autonomous,ye2019automated}) or pre-plan a reasonable linear trajectory line (\cite{ma2017two,xu2018simulation}), they do not generate diverse trajectory clusters and overlook implicit intent expression in the motion trajectory (\cite{de2013road}).

\section{Methodology}
In this section, we introduce a social trajectory planning method framework with inherent intent expression capabilities. We begin with an overview of the entire left-turn trajectory planning framework, followed by an analysis of the interaction features of human drivers during left turns. Subsequently, we elaborate on our trajectory generation method, which incorporates intent expression.

\subsection{Overview of Framework}
The comprehensive left-turn trajectory planning framework proposed in this study is illustrated in Figure~\ref{fig:framework}. The specific process and underlying philosophy of our trajectory planning method are as follows:

We scrutinize the characteristics of human trajectory intent expression in left-turn scenarios based on the human driving dataset, SIND  (\cite{xu2022drone}), providing prior knowledge of human drivers for trajectory generation.
Given the decision-making intent of proceeding or yielding, coupled with prior human knowledge of trajectory intent expression, we generate multiple candidate trajectories to form an expected trajectory space for decision-making.
We then formulate a trajectory reward function to evaluate the features of all generated candidate trajectories considering aspects such as traffic efficiency, driving comfort, and dynamic interactive safety.
Subsequently, we establish a probability distribution of candidate trajectories based on the Boltzmann distribution, employing the Boltzmann noise rational model to emulate driver trajectory selection behavior.
Lastly, we learn the criteria for trajectory evaluation and selection during human driving via the method of maximum entropy inverse reinforcement learning.
Upon completing the algorithm training, the algorithm will be tested and validated through simulated experiments and human-in-the-loop driving simulation experiments.

% Figure environment removed

\subsection{Analysis of Human Trajectory Intent Expression Characteristics}
Unlike existing autonomous driving algorithms, human drivers often subtly express their intentions to interactive objects through speed adjustments and trajectory deviations in some interactive scenarios. As the movement trajectories in the intersection driving space are less confined and more flexible, left-turning HVs can communicate their intent decisions through distinct trajectory progression methods during interaction. We select unprotected left-turn interaction as a representative scenario and analyze the features of human trajectory intent expression using the SIND dataset. The distribution of unprotected left-turn interaction trajectories in the SIND dataset is shown in Figure~\ref{fig:Traj_HV_Prior}, where the orange line symbolizes the yielding left-turn trajectory and the blue line signifies the proceeding left-turn trajectory. It is evident that human drivers employ diverse intent expression methods for left-turn trajectories under varying decisions:
\begin{itemize}
\item As depicted in the left part of Figure~\ref{fig:Traj_HV_Prior}, the preceding trajectory employs a pre-turn behavior to pivot the vehicle head in advance and laterally displace it in the upstream section, swiftly concluding the turn in a direct turn manner within the intersection.
\item As illustrated in the right part of Figure~\ref{fig:Traj_HV_Prior}, the yielding trajectory maintains a straight course with no notable deviation in the upstream section, completing the yielding turn in a two-stage turn manner, i.e., "proceed straight first and then complete the turn," within the intersection.
\end{itemize}

By integrating the intent expression methods of these differing decisions, we will establish an expected trajectory space constraint capable of realizing implicit intent expression. This enables the trajectory to operate within the human expectation space, bestowing it with intent expression capabilities.

% Figure environment removed

\subsection{Generating Left-turn Candidate Trajectories at Intersections Based on the Frenet Coordinate System}

To efficiently generate smooth and flexible trajectories at intersections, we assume that drivers typically make short-term plans based on the current interaction motion state, considering both lateral and longitudinal expected target points. Since the polynomial trajectory generation method can independently establish expressions for longitudinal and lateral displacement, and it has advantages like smooth trajectories, curvature continuity, and derivability of mathematical expressions, we employ the polynomial method to generate left-turn candidate trajectories based on the Frenet coordinate system.

The boundary conditions for trajectory generation using the polynomial method can be represented as follows:

\begin{align}
\begin{cases}
s\left(t_0\right)=s_0, \dot{s}\left(t_0\right)=v_{s0}, \ddot{s}\left(t_0\right)=a_{s0}, \dot{s}\left(T\right)=v_{sT}, \ddot{s}\left(T\right)=a_{sT}\\
l\left(t_0\right)=l_0, \dot{l}\left(t_0\right)=v_{l0}, \ddot{l}\left(t_0\right)=a_{l0}, l\left(T\right)=l_T, \dot{l}\left(T\right)=v_{lT}, \ddot{l}\left(T\right)=a_{lT}
\end{cases}
\end{align}
where $s_0$, $v_{s_0}$, and $a_{s_0}$ represent the longitudinal displacement, velocity, and acceleration at the initial state,  respectively. $l_0$, $v_{l0}$, and $a_{l_0}$ denote the lateral displacement, velocity, and acceleration at the initial state. $v_{s_T}$, $a_{s_T}$ represent the longitudinal velocity and acceleration at the terminal state. Similarly, $l_T$, $v_{l_T}$, and $a_{l_T}$ denote the lateral displacement, velocity, and acceleration at the terminal state.

Meanwhile, left-turning usually requires determining a reasonable sampling space to generate a set of trajectories, from which the optimal trajectory is selected. The sampling space for generating the trajectories is determined by the range of the terminal state parameters in the boundary conditions. To avoid the dimension explosion problem caused by too many parameters, we simplify the variables in the sampling space by setting the terminal moment longitudinal acceleration $a_{s_T}=0$ and lateral acceleration $a_{l_T}=0$. Then, we sample the trajectories over the time length $T$. The state variables that actually affect the trajectory sampling space are as follows:

\begin{align}
\begin{cases}
v_{sT}\in\left[v_{s0}-\Delta v_s,v_{s0}+\Delta v_s\right]\\
v_{lT}\in\left[v_{l0}-\Delta v_l,v_{l0}+\Delta v_l\right]\\
l_T=f_l\left(s_T\right)\\
T=f_T\left(s_T\right)
\end{cases}
\end{align}

The change range of the longitudinal and lateral velocities, $v_{s_T}$ and $v_{l_T}$, under the trajectory terminal state is constrained based on the longitudinal and lateral velocities, $v_{s_0}$ and $v_{l_0}$, under the initial state of the trajectory. The lateral deviation $l_T$ under the terminal state is constrained based on the feature distribution of the expected trajectory space in subsequent research on human left-turn trajectory decisions. In determining the terminal position, we generate trajectories using an indefinite time length $T$. When the terminal position is uncertain, we generate trajectories using a fixed time length $T=5s$. These two methods are applied to upstream road sections and internal intersection trajectory generation, respectively.

\subsection{Constraint of Desired Trajectory Space Based on the Expression of Interaction Intent}
\subsubsection{Desired Trajectory Space Constraint for Upstream Sections}
It was analyzed in the previous sections that for left turn trajectories in the upstream sections, the preceding trajectories tend to make a pre-turn while the yielding trajectories continue to maintain straight-line motion along the lane. As shown in Figure~\ref{fig:pre_turning}, the position, speed, and acceleration $(s_0,l_0,v_{s0},v_{l0},a_{s0},a_{l0})$ are determined as the initial states of the scene (not trajectory). Based on the analysis of pre-turning behavior characteristics, it is determined that the trajectory will experience a lateral deviation and a change in heading angle when reaching the stop line. Hence, the pre-turning state of the trajectory at the stop line can be represented as $(s_{pre},l_{pre},\theta_{pre})$
Here, $s_{pre}=s_{stop}$, the longitudinal position of the stop line on the reference line, is known. The trajectory's lateral deviation $l_{pre}$ and heading angle $\theta_{pre}$ at the stop line are determined as follows:
\begin{equation}
    \left\{\begin{matrix}\theta_0=\frac{v_{l0}}{v_{s0}}\\\theta_{pre}=\frac{v_{lpre}}{v_{spre}}\\l_{pre}\in\left[l_0,l_{stop}^{max}\right]\\\theta_{pre}\in\left[\theta_0,\theta_{stop}^{max}\right]\\\end{matrix}\right.
\end{equation}
where $l_{stop}^{max}$ is the maximum trajectory deviation at that point. $\theta_{pre}$ determines the ratio of the lateral speed $v_{lpre}$ to the longitudinal speed $v_{spre}$ of the trajectory at the stop line. $\theta_{stop}^{max}$ is the maximum heading angle of the trajectory at that point. Lateral deviation and heading angle are determined through a uniformly distributed random sampling method.

Therefore, assuming the initial trajectory state at time $(s_t,l_t,v_{st},v_{lt},a_{st},a_{lt})$, the trajectory sampling time length is set as $T_{pre}$. The pre-turning behavior characteristic state $(s_{pre},l_{pre},\theta_{pre})$ corresponds to the trajectory generation control terminal state $(s_{pre},l_{pre},v_{spre},v_{lpre},a_{spre},a_{lpre})$ at this moment, where acceleration items are simplified to default zero. Then, when controlling the trajectory generation space variable duration $T_{pre}$, longitudinal speed $v_{spre}$, and lateral speed $v_{lpre}$, they can be determined as the following calculation results:
\begin{equation}
    \left\{\begin{matrix}\mathrm{\ }\mathrm{T}_{pre}\in\left[\frac{s_{pre}-s_t}{v_{st}}-\Delta T_{pre},\frac{s_{pre}-s_t}{v_{st}}+\Delta T_{pre}\right]\\\sqrt{{v_{\mathrm{spre\ }}}^2+{v_{\mathrm{lpre\ }}}^2}\in\left[\sqrt{{v_{st}}^2+{v_{lt}}^2}-\Delta v_{pre},\sqrt{{v_{st}}^2+{v_{lt}}^2}+\Delta v_{pre}\right]\\\frac{v_{\mathrm{lpre\ }}}{v_{\mathrm{spre\ }}}=\theta_{pre}\\\end{matrix}\right.
\end{equation}
where $\Delta T_{pre}$ and $\Delta v_{pre}$ are control variables of trajectory duration and speed variation interval, respectively. The space variables of the yielding trajectory generation are consistent with the preceding trajectory, both controlling the trajectory sampling duration $T_{pre}$ , longitudinal speed $v_{spre}$ , and lateral speed $v_{lpre}$. The calculation method is consistent with the aforementioned one.
% Figure environment removed

\subsubsection{Constraining the Desired Trajectory Space within Intersections}
Analysis from previous sections determined that, within an intersection, left-turning vehicles following a trajectory that has the right of way will complete a rapid turn directly, while those yielding will adopt a two-phase turning method, maintaining a straight path before completing the turn.

As shown in Figure~\ref{fig:Traj_HV_Prior}, different decision trajectories lead to distinctly different trajectory distribution spaces within the intersection due to the different turning methods employed.
In this study, we model the relationship between the longitudinal displacement, s, and lateral displacement, l, of a trajectory using a continuously varying normal distribution. The mean and standard deviation of the preemptive trajectory's normal distribution are denoted as $\mu_{\text{preempt}}(s)$ and $\sigma_{\text{preempt}}(s)$ respectively. The mean and standard deviation of the yielding trajectory's normal distribution are denoted as $\mu_{\text{yield}}(s)$ and $\sigma_{\text{yield}}(s)$ respectively.

By determining the longitudinal displacement at time t, $s(t)$, we can deduce that $l_t \sim N(\mu(s(t)),\sigma(s(t)))$. Further, based on the 3-sigma rule of normal distribution, we can determine the range of the trajectory's lateral displacement at the end, $l_t \in [\mu(s(t)) - 2\sigma(s(t)), \mu(s(t)) + 2\sigma(s(t))]$.

Simultaneously, the longitudinal speed, $v_{sT}$, and the lateral speed, $v_{lT}$, at the end of the trajectory are determined within a changing interval based on the initial speeds $v_{s0}$ and $v_{l0}$. These speeds are uniformly sampled within the interval to determine $v_{sT}$ and $v_{lT}$.
Assuming an initial trajectory state within the intersection at a particular time, $(s_0, l_0, v_{s0}, v_{l0}, a_{s0}, a_{l0})$, the longitudinal displacement at the end of the trajectory, $s_T$, can be computed given $v_{sT}$. With $s_T$ determined, the lateral displacement at the end of the trajectory, $l_T$, can be decided based on the established relationship between $s$ and $l$.

Therefore, the expected trajectory space within the intersection can be controlled and generated by the following variables:
\begin{equation}
    \left\{
\begin{matrix}
v_{sT} \in [v_{s0} - \Delta v_s, v_{s0} + \Delta v_s]\\
v_{lT} \in [v_{l0} - \Delta v_l, v_{l0} + \Delta v_l]\\
l_T \in [\mu(s_T) - 2\sigma(s_T), \mu(s_T) + 2\sigma(s_T)]
\end{matrix}
\right.
\end{equation}
where $\Delta v_s$ and $\Delta v_l$ are the control variables for the changing interval of longitudinal and lateral speeds respectively.

In order to ensure that all generated trajectories reflect normal kinematic characteristics exhibited by human driving and avoid collisions, we apply the following constraints to the dynamics of the trajectory:

\begin{equation}
    \left\{
\begin{matrix}
v_{min} \le v_t \le v_{max}\\
a_{min} \le a_t \le a_{max}\\
c_{min} \le c_t \le c_{max}
\end{matrix}
\right.
\end{equation}
where $v_t$, $a_t$, and $c_t$ represent the speed, acceleration, and curvature of a generated trajectory $\zeta_I^i \in \zeta_I$ at time $t \in [0, T]$. The $(v_{\text{max}}, v_{\text{min}})$, $(a_{\text{max}}, a_{\text{min}})$, and $(c_{\text{max}}, c_{\text{min}})$ are the minimum and maximum speeds, accelerations, and curvatures respectively, as determined from the statistical analysis of real human trajectories.

Furthermore, we conduct collision checks to ensure that the generated trajectories do not result in collisions with interactive objects at any time. This study includes the consideration of a safety margin, determining the safe bounding box $Boundingbox_{\text{safe}}$. The collision constraint can be expressed as:

\begin{equation}
Boundingbox_{\text{safe}}^t\left(\text{left}\right) \cap Boundingbox_{\text{safe}}^t\left(\text{straight}\right) = \emptyset \quad 
\end{equation}
where $Boundingbox_{\text{safe}}^t$ is the safe bounding box at time t, which is calculated by adding 0.5m to the front and rear spaces and 0.3m to the left and right spaces of the vehicle's boundary rectangle. If, at any point, a trajectory $\zeta_{I,K}^i \in \zeta_{I,K}$ fails to meet the collision constraint, that trajectory is removed from the set $\zeta_{I,K}$. The final set of desired trajectories that meet the collision constraints is $\zeta_{I,K,C}$.

\subsection{Human Prior Learning Based on ME-IRL}
In traditional trajectory planning research, after defining the trajectory feature function, the optimal trajectory selection is achieved by manually setting or experimentally determining the trajectory reward function feature weights. However, in real-world interaction scenarios, it's challenging to accurately specify a reward function that captures all aspects of safe and efficient driving. Inverse Reinforcement Learning (IRL) can solve this problem. In this section, we consider the trajectory generation method expressing intentions and use ME-IRL to learn the human expert's trajectory behavior selection strategy under different decisions.

\subsubsection{Maximum Entropy Inverse Reinforcement Learning}
We assume that the total reward of a trajectory is a linear expression of the trajectory reward function, which is the weighted sum of selected features. Furthermore, we postulate that human drivers' preferences or behaviors under the same decision do not exhibit noticeable time variability and individual heterogeneity. Therefore, the total reward $R\left(\zeta\right)$ of a trajectory $\zeta$ can be expressed as:
\begin{equation}
R\left(\zeta\right)=\theta^T\cdot\mathbf{f}\zeta
\end{equation}
where $\theta=\left[\theta_1,\theta_2,\cdots,\theta_K\right]$ is the weight vector, and $K$ depends on the number of trajectory reward functions $\mathbf{f}\zeta$.
Given a human driving demonstration dataset $D=\{ \zeta_1,\zeta_2,\cdots,\zeta_N \}$ composed of $N$ trajectories, the ME-IRL algorithm is used to infer the reward weight $\theta$, which can then be used to generate driving strategies that match the human expert demonstration trajectories.

Simultaneously, to simulate the randomness of human driver's trajectory selection, we employ the Boltzmann noise theory model to construct the candidate trajectory distribution. Under the Boltzmann distribution, all features expected to match the expert's demonstration have the maximum entropy principle, corresponding to the maximum entropy IRL. Therefore, the probability of a trajectory is proportional to the return of that trajectory,
\begin{equation}
P\left(\zeta\left|\mathbf{\theta}\right.\right)=\frac{e^{R\left(\zeta\right)}}{Z\left(\theta\right)}=\frac{e^{\mathbf{\theta}^T\mathbf{f}\zeta}}{Z\left(\theta\right)}
\end{equation}
where $\theta$ is the feature weight, $\mathbf{f}\zeta$ is the feature vector of trajectory $\zeta$, $P\left(\zeta\left|\mathbf{\theta}\right.\right)$ is the probability of trajectory $\zeta$, and $Z\left(\theta\right)$ is the partition function. As the partition function $Z\left(\theta\right)$ represents the integral sum of the rewards of all possible trajectories, it is challenging to directly calculate in continuous and high-dimensional spaces. We reduce the trajectory space to the previously researched and mined human prior expected trajectory space $\zeta_{I,K,C}$ and use a finite number of discretely generated feasible trajectories to approximate the partition function. Therefore, the probability expression of a trajectory yields that
\begin{equation}
P\left(\zeta\left|\mathbf{\theta}\right.\right)\approx\frac{e^{\mathbf{\theta}^T\mathbf{f}\zeta}}{\sum_{i=1}^{N}e^{\mathbf{\theta}^T\mathbf{f}{{\widetilde{\zeta}}^i}}}
\end{equation}
where ${\widetilde{\zeta}}^i\in\zeta{I,K,C}$ is a generated trajectory with the same initial state as trajectory $\zeta$, $\mathbf{f}_{{\widetilde{\zeta}}^i}$ is the feature vector of trajectory ${\widetilde{\zeta}}^i$, and $N$ is the total number of generated trajectories.

The aim of maximum entropy IRL is to maximize the likelihood of expert demonstration trajectories by adjusting the feature weight $\theta$. The optimization objective function can be expressed as:
\begin{equation}
\max_{\theta} \mathcal{J}(\mathbf{\theta})= \max_{\theta} 
\sum_{\zeta \in \mathcal{D}} 
 \log P(\zeta | \theta)
\end{equation}
where $\mathcal{D}=\{ \zeta_i \}{i=1}^N$ represents the set of human expert demonstration trajectories. By substituting $P\left(\zeta\left|\mathbf{\theta}\right.\right)$ into the above equation, we obtain the optimized objective function $\mathcal{J}\left(\theta\right)$ as follows:
\begin{equation}
\mathcal{J}(\mathbf{\theta})=\sum_{\zeta\in\mathcal{D}}\left[\mathbf{\theta}^T\mathbf{f}_\zeta-log{\sum_{i=1}^{M}e^{\mathbf{\theta}^T\mathbf{f}_{{\widetilde{\zeta}}^i}}} \right]
\end{equation}

The above equation can be optimized using gradient-based methods. 
The gradient of the optimization objective function $\mathcal{J}\left(\theta\right)$, $\nabla\theta\mathcal{J}\left(\theta\right)$, can be expressed as follows:
\begin{equation}
\nabla_\mathbf{\theta}\mathcal{J}(\mathbf{\theta})=\sum_{\zeta\in\mathcal{D}}\left[\mathbf{f}_\zeta-\sum_{i=1}^{M}\frac{e^{\mathbf{\theta}^T\mathbf{f}_{{\widetilde{\zeta}}^i}}}{\sum_{i=1}^{M}e^{\mathbf{\theta}^T\mathbf{f}_{{\widetilde{\zeta}}^i}}}\mathbf{f}_{{\widetilde{\zeta}}^i}\right]
\end{equation}
The gradient can be viewed as the difference in feature expectations between human demonstration trajectories and generated trajectories:
\begin{equation}
\nabla_\mathbf{\theta}\mathcal{J}(\mathbf{\theta})=\sum_{\zeta\in\mathcal{D}}\left[\mathbf{f}_\zeta-\sum_{i=1}^{M}P\left({\widetilde{\zeta}}^i\left|\mathbf{\theta}\right.\right)\mathbf{f}_{{\widetilde{\zeta}}^i}\right]
\end{equation}
Following the process outlined by  (\cite{huang2021driving}), we use a gradient ascent method to iteratively update the trajectory feature weights and compute the optimization objective until the loss converges. To prevent overfitting, we incorporate L2 regularization into the objective function $\mathcal{J}\left(\theta\right)$. Consequently, the gradient $\nabla\theta\mathcal{J}\left(\theta\right)$ includes the difference in feature expectations plus a regularization term, as shown below:
\begin{equation}
\nabla_\mathbf{\theta}\mathcal{J}(\mathbf{\theta})=\sum_{\zeta\in\mathcal{D}}\left[\mathbf{f}_\zeta-\sum_{i=1}^{M}P\left({\widetilde{\zeta}}^i\left|\mathbf{\theta}\right.\right)\mathbf{f}_{{\widetilde{\zeta}}^i}\right]-2\lambda\theta
\end{equation}
where $\lambda > 0$ is the regularization parameter.

\subsubsection{Reward Function Design}
When designing the reward function, we considered three aspects: traffic efficiency, driving comfort, and interactive safety.

\textbf{(1) Traffic Efficiency}

We set the reward function for traffic efficiency as the loss in speed, which is the difference between the trajectory speed and the expected speed. We determine the traffic efficiency feature $f_{\text{efficiency}}\left(\zeta\right)$ of the trajectory $\zeta$ to be the mean speed loss at all times, represented as follows:
\begin{equation}
f_{\text{efficiency}}\left(\zeta\right)=-\frac{\sqrt{\sum_{t=0}^{T}\left(v_t-v_{\text{target}}\right)^2}}{T}
\end{equation}
where $v_t$ is the scalar speed of the trajectory $\zeta$ at time $t$, $T$ is the total duration of trajectory sampling, and $v_{\text{target}}$ is the expected speed of the vehicle turning left.

\textbf{(2) Driving Comfort}

We select the jerk (rate of change of acceleration) to establish the comfort feature function $f_{\text{comfort}}\left(\zeta\right)$ for assessing whether the trajectory $\zeta$ is comfortable. This feature is determined as the mean of the jerk vector sum longitudinally and laterally at all times for the trajectory $\zeta$, calculated as follows:

\begin{equation}
    \left\{\begin{matrix}
    f_{\text{comfort}}\left(\zeta\right)= - \frac{\sum_{t=0}^{T}\sqrt{((Jerk_s^t)^2 + (Jerk_l^t)^2)}}{T}\\
    Jerk_s^t = s'''(t)\\
    Jerk_l^t = l'''(t)\\
    \end{matrix}
    \right.
\end{equation}
where $Jerk_s^t$ and $Jerk_l^t$ are the longitudinal and lateral jerks of the trajectory $\zeta$ at time $t$ respectively. The reward function $f_{\text{comfort}}\left(\zeta\right)$ takes into account the smoothness in both the longitudinal and lateral directions.

\textbf{(3) Interaction Safety}

We quantify interaction safety by calculating the time difference $\Delta TTCP$ between the interacting parties reaching the conflict point. $\Delta TTCP$ takes into account the relative relationships of the positions and speeds of the interacting parties, characterizing the time interval for both parties to leave the conflict point under the current state.

We deconstruct interaction safety into longitudinal progress and lateral deviation. The impact on interaction safety from the longitudinal progress at time $t$ is determined as the time difference for the interacting parties to reach the conflict point without considering the lateral deviation of the left-turning vehicle, denoted as $\Delta TTCP_{st}$. Its calculation is as follows:

\begin{equation}
\Delta TTCP_{st}=TTCP_{st}^{left}-TTCP_{st}^{straight}
\end{equation}
\begin{align}
\begin{cases}
TTCP_{st}^{left}=\frac{s_t^{left}-s_{cp}^{left}}{v_{st}^{left}}\\
TTCP_{st}^{straight}=\frac{s_t^{straight}-s_{cp}^{straight}}{v_{st}^{straight}}
\end{cases}
\end{align}
where $TTCP_{st}^{left}$ and $TTCP_{st}^{straight}$ are the times for the left-turning and straight-driving vehicles to pass the conflict point at the longitudinal level at time $t$ respectively.
$s_t^{left}$ and $s_t^{straight}$ are the longitudinal positions of the left-turning and straight-driving vehicles at time $t$, $s_{cp}^{left}$ and $s_{cp}^{straight}$ are the longitudinal positions of the reference conflict points on the reference lines for the left-turning and straight-driving vehicles respectively, $v_{st}^{left}$ and $v_{st}^{straight}$ are the longitudinal velocities of the left-turning and straight-driving vehicles at time $t$.

% Figure environment removed

After calculating $\Delta TTCP_{st}$, we further determine the feature function $f_{safe,s}\left(\zeta\right)$ on the longitudinal level of interaction safety as the mean of $\Delta TTCP_{st}$ at all times on trajectory $\zeta$, represented as follows:
\begin{equation}
f_{safe,s}\left(\zeta\right)=\frac{\sum_{t=0}^{T}\left|\Delta TTCP_{st}\right|}{T}
\end{equation}

On the other hand, the impact of lateral deviation at time $t$ on interaction safety, as analyzed above, is determined by the impact of the left-turning vehicle's lateral deviation on the time difference for both interacting parties to reach the conflict point, denoted as $\Delta TTCP_{lt}$. Its calculation is as follows:

\begin{equation}
\Delta TTCP_{lt}=\Delta TTCP_l^{left}+\Delta TTCP_l^{straight}
\end{equation}
where $\Delta TTCP_l^{left}$ and $\Delta TTCP_l^{straight}$ are the impacts caused by the left-turn vehicle's lateral deviation on the times for itself and the oncoming straight-driving vehicle to pass through the conflict point, respectively. Their calculations consider the variables as shown in Figure~\ref{fig:IRL_Safety_Term} and are as follows:

\begin{align}
\begin{cases}
\Delta TTCP_l^{left}=\frac{l_t \cdot tan{\theta_t}}{v_{st}^{left}}\\
\Delta TTCP_l^{straight}=\frac{l_t \cdot cos{\theta_t}}{v_{st}^{straight}}
\end{cases}
\end{align}
where $l_t$ is the lateral deviation of the left-turn vehicle at time $t$, and $\theta_t$ is the angle produced by the expected trajectory of the oncoming straight-driving vehicle and the projection point of the expected conflict point on the reference line along the $l$ axis at time $t$.

After calculating $\Delta TTCP_{lt}$, we further determine the feature function $f_{safe,l}\left(\zeta\right)$ on the longitudinal level of interaction safety as the mean of $\Delta TTCP_{lt}$ at all times on trajectory $\zeta$, represented as follows:

\begin{equation}
f_{safe,l}\left(\zeta\right)=\frac{\sum_{t=0}^{T}\left|\Delta TTCP_{lt}\right|}{T}
\end{equation}

The interaction safety feature function of trajectory $\zeta$ is characterized from both longitudinal and lateral perspectives, represented as $f_{safe,s}\left(\zeta\right)$ and $f_{safe,l}\left(\zeta\right)$.


\section{Experiment and Analysis on the simulation platform}
This section begins by introducing the dataset employed and providing a detailed description of the implementation. Subsequently, the conducted experiments within the simulation environment are meticulously analyzed.
% Subsequently, the conducted experiments within the simulation environment and the human-in-the-loop driving platform are meticulously analyzed and systematically compared.

\subsection{Dataset}
We employ the SIND (\cite{xu2022drone}), a drone dataset, for our analysis of human driver interaction behavior. The SIND dataset, curated by the SOTIF research team at Tsinghua University, represents an intersection dataset collected from a two-phase traffic signal-controlled intersection in Tianjin, China. This two-phase signal control scheme enables simultaneous movement of left-turning and straight-going vehicles, thereby resulting in pronounced interactions and frequent conflicts. After meticulous selection, we extract 268 sets of one-on-one interaction events between left-turning and straight-going vehicles. Out of these events, 132 instances feature a yielding left-turning vehicle, and 136 instances involve a left-turning vehicle proceeding first.

\subsection{Implementation Details}
In the inverse reinforcement learning process, we initially extract the left-turning trajectory of human experts from the SIND dataset. The duration of these trajectories varies between approximately 8 to 15 seconds. Beginning from the initial moment of the left-turning and straight-moving interaction event, we choose a trajectory segment of length T=5s every 0.5 seconds as the demonstration trajectory, denoted as $\zeta$, of the human expert. Based on the initial state of this trajectory segment, we generate a trajectory, denoted as $\zeta_{I,K,C}$, which considers intent expression. We randomly select 80\% of the generated trajectories and their corresponding human expert demonstration trajectories as training data for learning the reward function, while the remaining 20\% serve as test data.

During the training process, the trajectory generation sample space is determined by three control variables, namely, the longitudinal speed $v_{sT}$ at the end time, the lateral speed $v_{lT}$, and the lateral displacement $l_T$. Here, $v_{sT} \in [v_{s0}-\Delta v_s, v_{s0}+\Delta v_s]$, where $\Delta v_s =$ \SI{3}{m/s}, and within this interval, we uniformly select 6 values for $v_{sT}$. $v_{lT} \in [v_{l0}-\Delta v_l, v_{l0}+\Delta v_l]$, where $\Delta v_l =$ \SI{1}{m/s}, and within this interval, we uniformly select 5 values for $v_{lT}$. $l_T \in [\mu(s_T)-2\sigma(s_T),\mu(s_T)+2\sigma(s_T)]$, within this interval, we uniformly select 10 values for $l_T$. The time length of the trajectory planning is T=5s, and the time interval between each trajectory point is 0.1s. 
For the learning process of ME-IRL, we set the number of iterations E to 1000, with $\alpha = 0.05$ and $\lambda = 0.01$.

For comparative experiments with our proposed method, we select three kinds of trajectory planning methods, namely, \textbf{Frenet trajectory planning method} (\cite{werling2010optimal,zhang2020optimal}), \textbf{decision-based ME-IRL (DB-ME-IRL) trajectory learning method} (\cite{Song1797}) that does not consider intent expression space, and \textbf{motion planning method based on potential field} (\cite{xu2019simulation}). To compare the differences among these trajectory planning methods, we select the decision results of real interactive events as high-level decision information. More details about the method and results can be found at the site.\footnote{See \url{https://shorturl.at/jqu35}}

% Figure environment removed

\subsection{Analysis of Inverse Reinforcement Learning Results}
The training progression of the Maximum Entropy Inverse Reinforcement Learning (ME-IRL) model is depicted in Figure~\ref{fig:IRL_Result}. Figure~\ref{fig:IRL_Result} (a) illustrates the iterative process that shows the average feature discrepancy between the reward function policy learned under yield and priority decisions and human drivers, as well as the change in the average log likelihood value of the human expert demonstration trajectory. These correspond to the gradient $\nabla_\theta\mathcal{J}\left(\theta\right)$ and the optimization target function $\mathcal{J}\left(\theta\right)$. The changing curves in the figure indicate that the average log likelihood value of the human expert demonstration trajectory steadily increases with the number of iterations under different decision trajectories, eventually reaching convergence.

We employ the Average Human Trajectory Similarity to gauge the closeness of the trajectory planning results to the human driving trajectory. We define the Average Human Trajectory Similarity (denoted as AHL) as the average final displacement error of the n trajectories with the highest selection probability in the generated trajectory distribution:
\begin{equation}
\text{AHL} = \frac{1}{n} \min_{i=1}^{n} |\hat{\zeta}_i(T) - \zeta(T)|_2
\end{equation}
where $\hat{\zeta}_i(T)$ $(i=1,2,...,n)$ are the trajectories with the highest selection probability in the generated trajectory distribution, and $\zeta(T)$ is the real trajectory of human drivers. The curves in Figure~\ref{fig:IRL_Result} (b) respectively represent the accuracy of ME-IRL under the preceding and yielding decisions. 
The trajectory planning results of the priority and yield decisions have average errors of only 0.39m and 0.26m respectively when compared to the actual human trajectories, thus substantiating the effectiveness of our method.

\subsection{Evaluation of Planning Results}
We perform a comprehensive evaluation of our method across four dimensions: overall trajectory distribution, intent expression capability based on lateral offset, safety and efficiency of trajectory motion interaction features, and computational efficiency and learning effect.

\subsubsection{Overall Trajectory Distribution}
The social capability of trajectory planning can be analyzed from the overall distribution and spatial expansion of the trajectory planning. Trajectory planning methods with weaker sociality lack consideration of intent expression, resulting in a more single-choice trajectory selection and smaller trajectory space. The difference in trajectories under priority and yield decisions is not pronounced, making it difficult for interactive objects to determine their intent through trajectory behavior.

To intuitively compare the differences in the trajectory planning space of different methods, we drew comparisons between the trajectory planning distribution of four methods and the actual trajectory distribution, as shown in Figure~\ref{fig:planning_results_compare}. Through qualitative comparison, it can be found that the trajectory space distribution of our method is closest to the actual human trajectory distribution. The potential field method and DB-ME-IRL method's trajectory planning space distribution is slightly inferior to our method, with the Frenet planning method showing the most significant discrepancy from actual human trajectories.
\begin{table} %[!htbp]
    \centering
    \caption{Comparison of trajectory coverage of different methods}
    \label{table:coverage compare}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{*{6}{c}}
    \toprule
        \multirow{2}{*}{Distribution characteristic index} & \multicolumn{5}{c}{Trajectory planning method}\\ 
        \cmidrule(lr){2-6}
        {} & Real trajectory & Our method & Frenet Planning Method & DB-ME-IRL & Potential field method \\
    \midrule
        Meta area coverage & 1066 & 824 & 576 & 710 & 737 \\ \hline
        Ratio to true trajectory & - & 77\% & 54\% & 67\% & 69\% \\
    \bottomrule
    \end{tabular}
    }
\end{table}

% Figure environment removed

We quantitatively analyze the differences in trajectory distribution using trajectory coverage. The specific calculation of trajectory coverage involves dividing the upstream entrance lane and the area inside the intersection into multiple subregions at 0.5m intervals in all directions. If there is a trajectory point in a subregion, it is determined that the subregion has been covered. Based on this concept, we calculated the coverage of actual human trajectories and the trajectories of the four planning methods, as shown in Table \ref{table:coverage compare}. The calculation results show that our method can achieve 77\% coverage of the actual trajectory distribution. The potential field method and DB-ME-IRL method show a decline compared to our method, while the Frenet planning method performs the worst, covering only 54\% of the actual trajectory distribution. Our method has improved the trajectory coverage rate by 12\% compared to the potential field method.


\subsubsection{Intent Expression Ability}
When interacting with oncoming straight-line vehicles, left-turning vehicles usually adopt lateral offset behavior to clearly express their decision intent to the interactive object. Therefore, the intent expression ability of a trajectory can be characterized by analyzing the degree of lateral offset.

As shown in Figure~\ref{fig:SL_traj_distribution}, the red (blue) line segment is the center line of the virtual left-turn lane, and the gray line segments are the left and right boundary lines of the virtual left-turn lane. The offset to the left of the center line is negative, and the offset to the right is positive. The different color curves in Figure~\ref{fig:SL_traj_distribution}(a) - Figure~\ref{fig:SL_traj_distribution}(e) represent the normalized SL trajectories under different methods, and the corresponding colored areas represent the enclosed area between the normalized SL trajectories and the center line of the virtual lane. By comparing and observing Figure~\ref{fig:SL_traj_distribution}(a) - Figure~\ref{fig:SL_traj_distribution}(e), it can be seen that the offset degree exhibited by the SL trajectory of our method is closest to the real trajectory, while the other three comparison methods all have significant differences with our method and the real trajectory. 
Although the potential field method can reasonably reproduce the behavior of priority trajectories quickly turning by significantly offsetting to the left of the centerline, it still cannot achieve a similar behavior to human real trajectories when planning yield trajectories, i.e., completing a longer straight line to indicate the intent to yield before turning. Both DB-ME-IRL and Frenet planning methods are inferior to our method in intent expression ability. 

% Figure environment removed



Further, we calculate the enclosed area $S_{SL}$ between the normalized SL trajectory and the center line of the virtual lane to represent the total lateral offset of a single trajectory. 
The total offset of each planned trajectory under different methods is calculated and drawn as a violin plot as shown in Figure~\ref{fig:SL_Trajectory_Offset}. 
The real trajectory offset average is 0.75, our method's average is 0.64, the potential field's average is next at 0.59, and the smallest average is the Frenet planning method at 0.13. Our method's average is 85\% of the real trajectory, an improvement of 8\% compared to the comparison methods. In terms of maximum offset ability, the maximum offset of the real trajectory is 2.2, our method's maximum is 1.7, DB-ME-IRL's maximum is next at 1.2, and the smallest maximum is the Frenet planning method at 0.7. Our method's maximum offset is 77\% of the real trajectory, an improvement of 42\% compared to the comparison methods.

% Figure environment removed

\subsubsection{Motion Interaction Feature Analysis}
We evaluate the safety and efficiency of different methods using two indicators: Post-Encroachment Time (PET) and Travel Time.
% The travel time within an intersection can reflect the efficiency characteristics of the trajectory. 
We calculate the time required for the planned trajectories under different methods to enter and leave the intersection, as shown in Figure~\ref{fig:travel_time_distri}. Figure~\ref{fig:travel_time_distri}(a) compares the distribution of travel time within the intersection for the planned trajectories of our method and real trajectories. The average travel time for our method's planned trajectories is 7.4s, with a standard deviation of 2.4s. The average travel time for the real trajectories is 7.2s, with a standard deviation of 2.9s.
% A rank-sum test with p=0.034 indicates a significant difference in the distribution of travel times between the planned and real trajectories. 
Figure~\ref{fig:travel_time_distri} (b) shows the travel time statistical results for the four different methods and the real trajectories, and we can see that the travel time of our method's planned trajectories is closest to the real trajectories. 
% The travel time of the potential field and DB-ME-IRL methods show a more significant increase compared to the real trajectories. The Frenet planning method has the most significant difference in travel time with the real trajectories, with the distribution concentrated between 6s and 8s, and the outliers also indicate a high degree of dispersion in the travel time distribution.
% Figure environment removed

Post-Encroachment Time (PET) has been proven to be suitable for representing the severity of conflicts during left turns. The PET distribution statistical results for different methods are shown in Figure~\ref{fig:result_PET_distri}. Figure~\ref{fig:result_PET_distri} (a) compares the PET distribution for our method's planned trajectories and real trajectories. The average PET for our method's planned trajectories is 4.3s, with a standard deviation of 1.5s. The average travel time for real trajectories is 4.5s, with a standard deviation of 1.4s. 
% A rank-sum test with p=0.80 indicates no significant difference in the PET distribution between the planned and real trajectories. 
Figure~\ref{fig:result_PET_distri} (b) shows the PET statistical results for four different methods and the real trajectories. We can see that our method's planned trajectories have the best PET performance. Specifically, both the DB-ME-IRL method and the Frenet planning method show a significant overall decrease in PET compared to our method, indicating an increased risk of trajectory interaction safety.
% Figure environment removed

\subsubsection{Computational Efficiency and Learning Performance}
In order to validate the improvement of the trajectory planning algorithm's computational efficiency by the decision expectation space constraint, we select the average number of candidate trajectories, the average computation time (including feature computation and trajectory search), and learning performance. We conduct experimental comparisons with the DB-ME-IRL method, and the comparison results are shown in Table\ref{table:calculate_efficiency_compare}. The experimental device configuration used for comparison is an i7-8700 processor and 16GB memory. Our method reduces the average number of generated candidate trajectories by 52.5\% after considering the expected trajectory space constraint and reduces the average computation time by 41.1\%.

Further, we compared the trajectory learning performance through the Average Human Likeness (AHL) index. Our method and DB-ME-IRL were trained for 1000 generations simultaneously, and the process results are shown in Table\ref{table:calculate_efficiency_compare} and Figure~\ref{fig:IRL_effect_compare}. After 1000 generations of simultaneous learning, the AHL of the trajectories under our method and DB-ME-IRL for priority decision are 0.39 and 0.45, respectively, and the AHL of the trajectories under our method and DB-ME-IRL for yielding decision are 0.26 and 0.57, respectively. The learning performance of our method has improved by 13\% and 54\% under the two types of decisions compared to the comparison method.

% In summary, by mining the trajectory movement characteristics of human intention expression and constructing the decision expectation trajectory space constraint, the trajectories not only have the ability to express intentions, but also significantly improve the trajectory planning efficiency and human-like learning performance.

\begin{table}%[!htbp]
    \centering
    \caption{Trajectory Planning Efficiency and Effect Comparison}
    \label{table:calculate_efficiency_compare}
    % \resizebox{\textwidth}{!}{
    % \begin{tabular}{*{5}{c}}
    \begin{tabular}{>{\centering\arraybackslash}p{3cm} >{\centering\arraybackslash}p{3cm} >{\centering\arraybackslash}p{3cm} >{\centering\arraybackslash}p{3cm} >{\centering\arraybackslash}p{3cm}}
    \toprule
        Method & Average Number of Candidate Trajectories & Average Calculation Time & Learning Effect (Preceding) & Learning Effect (Yielding)\\ 
        % \cmidrule(lr){4-5}
    \midrule
        Our Method & 273 & 0.079s & 0.39 & 0.26 \\ \hline
        Decision-based ME-IRL & 581 & 0.134s & 0.45 & 0.57 \\
    \bottomrule
    \end{tabular}
    % }
\end{table}

% Figure environment removed

% Figure environment removed
% Figure environment removed
% Figure environment removed

\subsection{Case Analysis}

To ascertain the degree to which our method aligns with human characteristics in conveying intentions during the interaction process, we conducted tests on our algorithm within a simulation platform.
Initially, a 2D intersection simulator is constructed, mirroring the size and shape of the intersection found in the SIND dataset from the real world. Within this simulator, the trajectory of the straight-going High-Velocity (HV) vehicle is log-replayed based on real-world data, while the left-turn vehicle can be controlled either by our algorithm or by human-expert data.
Our algorithm is rigorously tested within the simulator, focusing on two selected scenarios: a left-turn vehicle yielding case and a proceeding-first case. Subsequently, we compare the actual human trajectories with those planned by our method and trajectories devised by the Maximum Entropy Inverse Reinforcement Learning (ME-IRL) methods (\cite{werling2010optimal}).
For a visual representation of the two interaction cases, animations are available on the provided website.\footnote{Refer to \url{https://shorturl.at/jqu35}}

\subsubsection{Case 1: Left-turn Vehicle Proceeding}
Figure~\ref{fig:case_preceding_trajectory_our} illustrates a case where the left-turn vehicle is proceeding. At T=3s, the trajectory planned by our method begins to laterally offset prior to entering the intersection. Upon entering the intersection, the trajectory maintains a turning approach that passes through the conflict point early by significantly shifting left relative to the reference line, closely resembling the actual human trajectory. In contrast, the comparison method as shown in Figure \ref{fig:case_preceding_trajectory_baseline} plans trajectories on both sides of the reference line, striving to minimize any lateral offset relative to the reference line. 
By comparison, it can be found that this method differs greatly from the actual human trajectory and fails to achieve intention communication at the trajectory behavior level.

Furthermore, our method reduces the trajectory generation search space significantly by adding the constraint of the expected space, whereas the comparison method uniformly generates candidate trajectories based on both sides of the reference line. Throughout the planning process, the maximum number of trajectories planned by our method is 300, while the comparison method needs to generate and search for up to 800 candidate trajectories. Our method reduces the search space by 62.5\%.

We further quantitatively analyze the behavioral performance difference between the two methods by examining the changes in motion states during the interaction process, as demonstrated in Figure \ref{fig:case_preceding_analysis}. Figure \ref{fig:case_preceding_analysis} (a) depicts the relationship between the heading angle of the left-turning vehicle and X. It reveals that our method exhibits a noticeable pre-turning behavior before the stop line of the lane, whereas the comparison method only commences turning after entering the intersection. Figure \ref{fig:case_preceding_analysis} (b) displays the relationship between the lateral offset of the left-turning vehicle and X. It shows that our method passes through the conflict point first by significantly and swiftly shifting laterally during the turning process, whereas the comparison method adheres to the reference line, resulting in a trajectory with negligible lateral offset.

\subsubsection{Case 2: Left-turn Vehicle Yielding}
% Figure environment removed
% Figure environment removed

% Figure environment removed
In the scenario where the left-turn vehicle yields, the trajectories planned by our method and the comparison method are illustrated as the red trajectory lines in Figure~\ref{fig:case_yielding_trajectory_our} and Figure~\ref{fig:case_yielding_trajectory_baseline}, respectively.
Our method, as depicted in Figure \ref{fig:case_yielding_trajectory_our}, can implement a two-stage turning approach of 'proceed straight then turn' under the yielding scenario when making a left turn. This both communicates its decision to yield to the interacting object and prevents the efficiency loss of stopping and restarting. 

Figure~\ref{fig:case_yielding_analysis}(a) illustrates the relationship between the heading angle and X. If we consider a heading angle of 15 as a significant deviation, the real trajectory can maintain a straight phase up to x=10.7m, and our method can maintain a straight phase up to x=7.0m. After entering the intersection, it clearly yields and maintains a straight path. However, the comparison method can maintain a straight phase up to x=8.2m under the judgment standard of 15. Nonetheless, the comparison method exhibits decision inconsistency; it decides to proceed first upon entering the intersection but fails, and then corrects its decision through the steering angle, resulting in a later turning time for the comparison method. 

Figure \ref{fig:case_yielding_analysis}(b) shows the relationship between the lateral offset and X. To quantify the difference between the planned trajectory and the actual trajectory during the straight phase, the area composed of the X-axis coordinates and the lateral offset is described as the degree of offset while going straight. During the straight phase of the real trajectory, our method's process offset degree is greater than that of the comparison method, thus appearing closer to the actual trajectory during the straight phase. The trajectory straight offset degree of our method is 15.6\% different from the real trajectory, and the trajectory straight offset degree of the traditional method decreases by 22.1\% compared to our method.

\section{Evaluation on the Human-in-loop Driving Iteraction Platform}
Although low-cost, efficient, and rapid experiments can be conducted in simulated environments, vehicles in simulators typically exhibit fixed driving styles and behavior patterns, lacking the ability to perceive and understand socio-interactional cues. To validate the algorithm's socio-interactional capabilities, in this section, we establish a "Human-in-Loop" driving simulation platform and design simulation experiments involving human drivers to assess the algorithm's interpretability of intent and timing of intent clarification.

\subsection{Platform Construction and Experimental Design}
We initially constructed the "Human-in-Loop" driving simulation platform, as depicted in Figure~\ref{fig:human-in-loop driving platform}. The platform consists of three main components: hardware environment, software environment, and interaction strategies.
% Figure environment removed

Fourteen participants with valid driver's licenses and at least one year of driving experience were recruited for the experiments. We designed an intersection interaction environment with east-west lanes as bidirectional four-lane roads and north-south lanes as bidirectional two-lane roads. During the experiments, specific constraints were applied to interacting vehicles: left-turn vehicles (controlled by our algorithm) entered the intersection from the west entrance in the left-turn lane, while straight-going vehicles (controlled by participants) entered the intersection from the east entrance in the straight/left lane. 
ME-IRL algorithm, as we mentioned above, is emplyed as the baseline algorithm. The high-level decision making of both our algorithm and baseline algorithm are generated by decision-making tree algorithm~\cite{ma2017two}.
Each participant engaged in 20 scenarios, resulting in a total of 280 sets of interaction experiment data.

Before, during, and after the experiments, we designed corresponding subjective perception questions. Prior to the experiments, we assessed participants' trust in currently operating AVs on real roads. During the experiments, for each interaction scenario, subjective feedback on different AV interaction strategies was collected in terms of interpretability of interaction strategies and the timing of intent clarification. After each experiment, participants provided feedback on the acceptability of the proposed interaction strategy and changes in trust level. Specific subjective perception questions are detailed in Table~\ref{tab:subjective_evaluation}, utilizing a 5-point Likert scale for measurement, except for decision consistency, which involves a binary metric during the experimental process.
Mmore detailed information regarding the driving simulation platform and the design of simulated experiments in this study can be accessed at this site.\footnote{See \url{https://shorturl.at/jqu35}}


\begin{table}%[h]
    \centering
    \caption{Subjective Evaluation using a 5-Point Likert Scale at Different Experimental Stages}
    \label{tab:subjective_evaluation}
    \begin{tabularx}{1.0\textwidth}{lXl}
        \toprule
        \textbf{Experimental Phase} & \textbf{Subjective Perception Question} & \textbf{5-Point Likert Scale} \\
        \midrule
        Before Experiment & \textbf{Question about Trust Level} & Not at all Trusting  Completely Trusting \\
        & During the current stage of driving and interaction with the AV, do you trust the decision-making behavior of the AV? & \\
        \midrule
        During Experiment & \textbf{Question about Interpretability} & Not at all Clear  Completely Clear \\
        & During this driving process, do you think the left-turning vehicle (AV) expresses its intent clearly? & \\
        & \textbf{Question about Timing of Intent Clarification} & Very Late  Very Early \\
        & During the interaction process, at which stage can you fully understand the intent of the left-turning vehicle? & \\
        \midrule
        After Experiment & \textbf{Question about Approval Level} & Not Necessary  Very Necessary \\
        & Do you think it is necessary for the AV to showcase its intent through different actions? & \\
        & \textbf{Question about Trust Level} & Not at all Helpful  Extremely Helpful \\
        & Does this socio-interaction strategy help improve your trust in the AV? & \\
        \bottomrule
    \end{tabularx}
\end{table}

\subsection{Experiment Result}
We primarily analyze the algorithm's social performance from two perspectives: interpretability of intent and timing of intent clarification.

\subsubsection{Interpretability of Intent}

Subjective evaluations of interpretability of intent for the proposed strategy and comparative strategies were statistically compared across different priority scenarios. Box plots depicting these evaluations are shown in Figure~\ref{fig:human interpretability evaluation}. Furthermore, Wilcoxon non-parametric tests were employed to analyze the differences in subjective perception scores for interpretability of intent. Pearson correlation coefficient analysis was then conducted to measure the impact of different conditions on participants' subjective ratings, categorized as no impact, minor impact, moderate impact, and significant impact~\cite{rettenmaier2021communication}. Descriptive statistical results are presented in Table\ref{tab:interpretability_results}.
% Figure environment removed

From the comprehensive results in Figure~\ref{fig:human interpretability evaluation}, it is evident that the average interpretability of intent for our strategy surpasses that of the comparative strategies in all five priority scenarios. Except for the fuzzy scenario, where interpretability ranges from "general" to "somewhat clear," the interpretability for the remaining four scenarios is above the "somewhat clear" level. This indicates a significant enhancement in the ability of our interactive strategy to express implicit intent. In contrast, the comparative strategy achieves "somewhat clear" interpretability only in the straight-going priority scenario, while in other scenarios, it remains at the "general" level.

Analyzing the results in Table~\ref{tab:interpretability_results}, Wilcoxon tests reveal significant differences in the interpretability of intent between our strategy and the comparative strategy in scenarios with straight-going priority, left-turn priority, and left-turn advantage. These findings suggest that our strategy better communicates its intent to interacting entities. Pearson correlation coefficient calculations indicate that different strategies have a considerable impact on interpretability of intent in scenarios with left-turn priority and left-turn advantage, a moderate impact in scenarios with straight-going priority, and a minor impact in scenarios with fuzzy boundaries and straight-going advantage. This demonstrates that our strategy has the greatest impact on improving interpretability of intent in scenarios with left-turn priority and left-turn advantage, followed by straight-going priority, and lastly, scenarios with fuzzy boundaries and straight-going advantage. In a comprehensive analysis of all scenarios, our strategy's median interpretability score is 5.0, classified as "very clear," with a mean of 4.23 falling between "somewhat clear" and "very clear." The mean improvement over the comparative strategy is 24\%, showing a significant difference ($p=0.000$). Our strategy achieves a moderate level of improvement ($r=0.36$) in interpretability of intent compared to the comparative strategy.
\begin{table}%[h]
    \centering
    \caption{Descriptive Statistical Results of Subjective Evaluation on the Interpretability of Intent for Our Strategy and Comparative Strategy. The values outside and inside the brackets in the Median and Mean columns represent the respective values for our strategy and the comparative strategy.}
    \label{tab:interpretability_results}
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Scenario} & \textbf{Median (Mdn)} & \textbf{Mean} & \textbf{Test Statistic (W)} & \textbf{p-value} & \textbf{Effect Size (r)} \\
        \midrule
        Fuzzy Boundary (n=28) & 4.0 (3.0) & 3.64 & 85.0 & 0.057 & 0.24 \\
        Straight Advantage (n=28) & 4.0 (3.0) & 4.11 & 37.5 & 0.010 & 0.35 \\
        Straight Priority (n=28) & 4.5 (4.0) & 4.43 & 24.0 & 0.058 & 0.22 \\
        Left Turn Advantage (n=28) & 4.0 (3.0) & 4.32 & 22.5 & 0.001 & 0.51 \\
        Left Turn Priority (n=28) & 5.0 (3.5) & 4.64 & 22.5 & 0.000 & 0.52 \\
        All Scenarios (n=140) & 5.0 (3.0) & 4.23 & 847.5 & 0.000 & 0.36 \\
        \bottomrule
    \end{tabular}
    \smallskip
\end{table}

\subsubsection{Timing of Intent Clarification}

In addition to subjective evaluations of interpretability, the timing of intent clarification is another dimension to measure the expressive capability of intent. If participants clarify the AV's intent earlier, it indicates that the social intent expression timing of the interaction strategy is earlier and more explicit.

Subjective evaluations of the timing of intent clarification for our strategy and the comparative strategy were statistically compared across different priority scenarios. Box plots depicting these evaluations are shown in Figure~\ref{fig:human_time_clarification_evaluation}. Similarly, Wilcoxon tests were applied to analyze the differences in subjective perception scores for the timing of intent clarification. Pearson correlation coefficient analysis was then conducted to measure the impact of different conditions on participants' subjective ratings. Descriptive statistical results are presented in Table~\ref{tab:timing_results}.
% Figure environment removed

Analyzing the comprehensive results in Figure~\ref{fig:human_time_clarification_evaluation}, it is observed that the average timing of intent clarification for our strategy is earlier than that of the comparative strategy in all five priority scenarios. In scenarios with fuzzy boundaries and straight-going advantage, the clarification timing is within the "general" to "somewhat early" range, while in straight-going priority, left-turn priority, and left-turn advantage scenarios, it is above the "somewhat early" level. In contrast, the comparative strategy's timing of intent clarification remains below the "somewhat early" level in all scenarios. This indicates that our interactive strategy achieves earlier and more explicit intent expression.

Analyzing the results in Table~\ref{tab:timing_results}, Wilcoxon tests reveal significant differences in the timing of intent clarification between our strategy and the comparative strategy in all five scenarios, suggesting that our strategy can express its intent to interacting entities earlier and more explicitly. Pearson correlation coefficient calculations indicate that different strategies have a considerable impact on the timing of intent clarification in scenarios with left-turn advantage, a moderate impact in scenarios with left-turn priority, and a minor impact in scenarios with fuzzy boundaries, straight-going advantage, and straight-going priority. This demonstrates that our strategy has the greatest impact on improving the timing of intent clarification in scenarios with left-turn advantage, followed by left-turn priority, and lastly, scenarios with fuzzy boundaries, straight-going advantage, and straight-going priority. In a comprehensive analysis of all scenarios, our strategy's median timing of intent clarification is 4.0, classified as "somewhat early," with a mean of 4.00 falling between "somewhat early" and "very early." The mean improvement over the comparative strategy is 23\%, showing a significant difference ($p=0.000$). Our strategy achieves a moderate level of improvement ($r=0.33$) in the timing of intent clarification compared to the comparative strategy.

\begin{table}%[h]
    \centering
    \caption{Descriptive Statistical Results of Timing of Intent Clarification for Our Strategy and Comparative Strategy. The values outside and inside the brackets in the Median and Mean columns represent the respective values for our strategy and the comparative strategy.}
    \label{tab:timing_results}
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Scenario} & \textbf{Median (Mdn)} & \textbf{Mean} & \textbf{Test Statistic (W)} & \textbf{p-value} & \textbf{Effect Size (r)} \\
        \midrule
        Fuzzy Boundary (n=28) & 4.0 (3.0) & 3.50 & 59.0 & 0.044 & 0.23 \\
        Straight Advantage (n=28) & 4.0 (3.0) & 3.79 & 52.5 & 0.022 & 0.25 \\
        Straight Priority (n=28) & 4.0 (4.0) & 4.14 & 21.0 & 0.009 & 0.25 \\
        Left Turn Advantage (n=28) & 4.0 (3.0) & 4.04 & 6.0 & 0.000 & 0.48 \\
        Left Turn Priority (n=28) & 5.0 (3.0) & 4.50 & 19.5 & 0.000 & 0.52 \\
        All Scenarios (n=140) & 4.0 (3.0) & 4.00 & 736.0 & 0.000 & 0.33 \\
        \bottomrule
    \end{tabular}
    \smallskip
\end{table}

\section{Conclusion}
In an endeavor to narrow the divide between AVs and HVs and ensure that AVs can implicitly convey social intent in a manner understandable to HVs in mixed-traffic scenarios, we have introduced an innovative framework for socially-compliant trajectory planning that is robust in implicit intent expression at the unprotected left-turn scenarios.

Our proposed framework is organized into three components: trajectory generation, trajectory evaluation, and trajectory selection. The experimental results substantiate the efficacy of our framework, demonstrating a strong resemblance to actual human trajectories, considerable enhancements in intent expression, safety, and efficiency, along with improved computational efficiency and learning outcomes. Our method shows a 77\% match with the actual trajectory distribution, an average offset of 85\% from the real trajectory, an average travel time of 7.4 seconds within the intersection, and a decrease in the average computation time by 41.1\%.
Concurrently, experiments conducted within the simulation platform and human-in-loop driving platform unequivocally showcase the effectiveness and precedence of the algorithm.

For future research, our research focus will expand the applicability of our trajectory planning methodology to include a broader range of interactive scenarios. Furthermore, we aim to confirm the effectiveness and scalability of our methodology through driving simulation experiments and real-vehicle interactions.

\section{CRediT authorship contribution statement}
Jiaqi Liu: Methodology, Writing - original draft. Xiao Qi: Data curation, Writing - review \& editing. Jian Sun: Writing - review \& editing, Proiect administration. Ying Ni: Writing - review \& editing. PengHang: Conceptualization, Writing - review \& editing, Supervision, Project administration.

\section{Declaration of Competing Interest}
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

\section{Acknowledgments}
This work was supported in part by the National Natural Science Foundation of China (52125208, 52232015, 52272313), the Young Elite Scientists Sponsorship Program by CAST (2022QNRC001) and the Fundamental Research Funds for the Central Universities.

\newpage

\bibliographystyle{cas-model2-names}
\bibliography{reference}
\end{document}


