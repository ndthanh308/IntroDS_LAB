% \begin{table}
%   \centering
%   \caption{
%   % \xd{move to experiment as we have many analysis figures before method already} 
%   % We evaluate existing works of multi-task learning in our baseline. We empirically find that GradNorm and xx, though is able to reduce the discrepancy of gradient magnitude, are unfavorable to our model. IMTL can alleviate the task conflict considerably. We conjecture that this is because IMTL also attends the gradient direction. 
%   We qualitatively and quantitatively evaluate existing works of multi-task learning in our baseline. They are able to reduce the discrepancy of gradient magnitude and improve the subsequent performance. Throughout the paper, IMTL is adopted as the technique for inter-gradient calibration.\xd{list the average number of multi-task in table}
%   }
%     \resizebox{1.0\columnwidth}{!}
%     {\tablestyle{3pt}{1.0}
%   \begin{tabular}{@{}lccccc@{}}
%     \toprule
%     Method & $\gamma_{\rm task}$ &mAP(\%) & NDS &mIoU(\%) &$\Delta_{\rm MTL}$\\
%     \midrule
%     Baseline  &16.87 & 59.09 &65.03 &43.90 &-\\
%     GradNorm~\cite{chen2018gradnorm}  &12.54 & 58.33 & 64.41 & 57.19&\\
%     DWA~\cite{liu2019end} &-  &\textbf{59.29} & \textbf{65.14} & 57.72 &\\
%     IMTL~\cite{liu2021towards} &1.07  &57.07 &63.34  &\textbf{59.40} &\\
%     \bottomrule
%   \end{tabular}}
%   \label{tab:mtl_result}
% \end{table}


% \begin{table}
%   \centering
%   \caption{
%   We qualitatively and quantitatively evaluate existing works of multi-task learning in our baseline. They are able to reduce the discrepancy of gradient magnitude and improve the subsequent performance. 
%   \lsh{Note that DWA has to..}
%   % Throughout the paper, IMTL is adopted as the technique for inter-gradient calibration.
%   }\vspace{-4mm}
%     \resizebox{1.0\columnwidth}{!}
%     {\tablestyle{5pt}{1.0}
%   \begin{tabular}{@{}lccccc@{}}
%     \toprule
%     Method & $\gamma_{\rm task}$ $^\downarrow$ &mAP(\%)$^\uparrow$ 
%     & NDS$^\uparrow$ &mIoU(\%)$^\uparrow$ &$\Delta_{\rm MTL}$(\%) $^\downarrow$\\
%     \midrule
%     Baseline  &16.87 & 59.1 &65.0 &44.0 &36.6\\
%     GradNorm~\cite{chen2018gradnorm}  &12.54 & 58.3 & 64.4 & 57.2&17.6\\
%     DWA~\cite{liu2019end} &-  &59.3 & 65.1 & 57.7 &14.1\\
%     IMTL\_G~\cite{liu2021towards} &1.13  &57.1 &63.3 &\textbf{59.4} &17.7\\
%     \midrule
%     Fuller (Ours) &1.07 &\textbf{60.5} &\textbf{65.3} &58.4 &\textbf{10.8} \\
    
%     \bottomrule
%   \end{tabular}}
%   \label{tab:mtl_result}
% \end{table}

\begin{table}
  \centering
  % \caption{We compare existing works of multi-task learning.}
  \caption{DWA needs to empirically select the initial loss weights while GradNorm uses extra learnable parameters. Considering scalability and performance, we select the IMTL\_G as the inter-gradient calibration.}
  \vspace{-0mm}
  \resizebox{1.0\columnwidth}{!}
  {\tablestyle{8pt}{1.0}
  \begin{tabular}{@{}lcccc@{}}
    \toprule
    Method &mAP(\%)$\uparrow$ 
    & NDS$\uparrow$ &mIoU(\%)$\uparrow$ &$\Delta_{\rm MTL}$(\%) $\downarrow$\\
    \midrule
    Baseline  & 59.1 &65.0 &44.0 &36.6\\
    GradNorm~\cite{chen2018gradnorm}  & 58.3 & 64.4 & 57.2&17.6\\
    DWA~\cite{liu2019end}  &59.3 & 65.1 & 57.7 &14.1\\
    IMTL\_G~\cite{liu2021towards}  &57.1 &63.3 &\textbf{59.4} &17.7\\
    \midrule
    Fuller (Ours) &\textbf{60.5} &\textbf{65.3} &58.4 &\textbf{10.8} \\
    
    \bottomrule
  \end{tabular}}
  \label{tab:mtl_result}
\end{table}