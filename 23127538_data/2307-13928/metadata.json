{
  "title": "Beyond Strict Competition: Approximate Convergence of Multi Agent Q-Learning Dynamics",
  "authors": [
    "Aamal Hussain",
    "Francesco Belardinelli",
    "Georgios Piliouras"
  ],
  "submission_date": "2023-07-26T02:55:33+00:00",
  "revised_dates": [],
  "abstract": "The behaviour of multi-agent learning in competitive settings is often considered under the restrictive assumption of a zero-sum game. Only under this strict requirement is the behaviour of learning well understood; beyond this, learning dynamics can often display non-convergent behaviours which prevent fixed-point analysis. Nonetheless, many relevant competitive games do not satisfy the zero-sum assumption.\n  Motivated by this, we study a smooth variant of Q-Learning, a popular reinforcement learning dynamics which balances the agents' tendency to maximise their payoffs with their propensity to explore the state space. We examine this dynamic in games which are `close' to network zero-sum games and find that Q-Learning converges to a neighbourhood around a unique equilibrium. The size of the neighbourhood is determined by the `distance' to the zero-sum game, as well as the exploration rates of the agents. We complement these results by providing a method whereby, given an arbitrary network game, the `nearest' network zero-sum game can be found efficiently. As our experiments show, these guarantees are independent of whether the dynamics ultimately reach an equilibrium, or remain non-convergent.",
  "categories": [
    "cs.GT"
  ],
  "primary_category": "cs.GT",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.13928",
  "pdf_url": null,
  "comment": "Presented at IJCAI 2023",
  "num_versions": null,
  "size_before_bytes": 2564405,
  "size_after_bytes": 384317
}