\section{Our Approach}

\subsection{Framework Overview}

In this paper, we provide a general solution for cross-domain generalization, joint generalization, and adaptation generalization jointly by digging into uncertainty estimation in stereo matching. The overall architecture of our method is shown in Fig. \ref{fig: UCFNet}, which can divide into three steps:

\textbf{1) Training a robust stereo matching network on source domain:} Given stereo image pairs and corresponding ground-truth disparity of source domain (synthetic dataset), we first propose to employ the synthetic data to train a robust pre-train model with strong cross-domain generalization. Specifically, an \textbf{U}ncertainty-based \textbf{C}ascade and \textbf{F}used cost volume representation (UCFNet) is proposed to alleviate the unbalanced disparity distribution and large domain shifts across different datasets. 
% The detailed network architecture is shown in Figure ??, which consists of four parts: pyramid feature extraction, fused cost volume, cascade cost volume, and attention-based disparity refinement.

\textbf{2) Pseudo-label generation on target domain:} After getting the pre-training model, we propose an uncertainty-based pseudo-label generation method to generate reliable pseudo-labels for domain adaptation. Specifically, the proposed method can be divided into two steps: (a) Given stereo image pairs of the target domain (real dataset), the pre-trained model is employed to predict the corresponding disparity estimation. (b) Two terms of uncertainty estimation, i.e., pixel-level and area-level are proposed to filter out the high-uncertainty pixels of the disparity estimation and generate sparse while reliable disparity maps as pseudo-label.

\textbf{3) Domain adaptation with generated Pseudo-label:} After getting the generated pseudo-label, we can employ it as supervision to adapt the pre-train model to the target domain. In addition, the generated pseudo-label can also be employed as supervision to train the Monocular depth estimation network in an unsupervised way. Experiments show the superiority of the proposed method on both monocular and binocular depth estimation tasks.

% Figure environment removed

The structure of this paper is organized as follows. In Sec.~\ref{sec:stereo_network}, we present the details about how to employ the \textbf{U}ncertainty-based \textbf{C}ascade and \textbf{F}used cost volume representation (UCFNet) for robust disparity estimation. Sec. \ref{sec:uncertainty} introduces the design of uncertainty estimation, which can filter out unreliable points of current estimations and generate reliable and sparse pseudo-label for subsequent domain adaptation. Sec. \ref{sec:domain_adaptation} introduces the mechanism of domain adaptation, i.e., how to employ the generated pseudo-label adapting the binocular/monocular depth estimation network to the new domain. Finally, we evaluate the results of our algorithms on both stereo matching and monocular depth estimation tasks in Sec. \ref{sec:stereo_experinment} and Sec. \ref{sec:monocular}, respectively.


\subsection{Uncertainty based cascade and fused cost volume for disparity estimation}
\label{sec:stereo_network}

To achieve robust stereo matching, we propose an \textbf{U}ncertainty-based \textbf{C}ascade and \textbf{F}used cost volume representation (UCFNet) to alleviate the unbalanced disparity distribution and large domain shifts across different datasets. As shown in Fig. \ref{fig: CFNet} and \ref{fig:channel_attention}, the proposed UCFNet consists of four parts, including pyramid feature extraction, fused cost volume, cascade cost volume, and attention-based disparity refinement. 

\subsubsection{Pyramid feature extraction}

%Given an image pair, we first employ an unet-like \cite{hsm,unet} encoder-decoder architecture with skip connections to extract multi-scale image features. The encoder consists of five residual blocks, followed by an SPP \cite{psmnet} module to better incorporate hierarchical context information. Compared with the widely used Resnet-like network \cite{cascade,gwcnet}, our method is more efficient and still contains sufficient information for cost aggregation. Then, we divide the multi-scale features into fused and cascade cost volume and predict multi-resolution disparity, respectively. %The details of cost volume fusion and cost volume cascade will be discussed in the next section.

Given an image pair, an unet-like \cite{hsm,unet} encoder-decoder architecture is first proposed to extract multi-scale image features. Specifically, the encoder consists of five residual blocks, followed by an SPP \cite{psmnet} module to better extract hierarchical context information. Compared with the widely used Resnet-like network \cite{cascade,gwcnet}, experiments show that the proposed unet-like feature extraction can preserve sufficient information with lower computational complexity. Then, the extracted multi-scale features can be divided into fused and cascade cost volumes and predict corresponding resolution disparity, respectively.

% Figure environment removed

\subsubsection{Fused Cost Volume}

%We propose to fuse multiple low-resolution dense cost volumes (smaller than 1/4 of the original input image resolution in our paper) to reduce the domain shifts between different datasets for initial disparity estimation. Existing approaches \cite{cascade,uscnet,cvpmvsnet} have realized the importance of employing multi-scale cost volumes. However, these methods generally abandon small-resolution cost volumes because such cost volumes don’t contain sufficient information to generate an accurate disparity map respectively. Instead, we argue that different scale small-resolution cost volumes can cover multi-scale receptive fields, guiding the network to look at different scale image regions. Thus, they can be fused together to extract global and structural representations and generate a more accurate initial disparity map than higher-resolution sparse cost volume. More precisely, we first construct low-resolution cost volumes at each scale respectively, and then design a cost volume fusion module to integrate them in an encoder-decoder process. We provide details about these two steps below.

In this section, multiple low-resolution dense cost volumes are fused together to reduce the domain shifts across different datasets for initial disparity estimation. Our method is motivated by a simple observation that multi-scale cost volume can cover multi-scale receptive fields and drive the network to extract multi-level information, e.g., edges and areas are easier to be captured by low-resolution cost volume. Moreover, edges and areas are no-local information, which is less sensitive to domain changes. Hence, we can fuse multiple low-resolution dense cost volumes to incorporate hierarchical structural representations and generate a more accurate initial disparity estimation. Specifically, we first employ the input multi-scale features (smaller than 1/4 of the original input image resolution) to construct each scale cost volume respectively and then design a cost volume fusion module to integrate them. Details of the two steps will be provided below.

% In this section, multiple low-resolution dense cost volumes are fused together to reduce the domain shifts across different datasets for initial disparity estimation. The usage of multi-scale cost volume is a common operation in learning-based stereo matching methods \cite{cascade,uscnet,cvpmvsnet}. However, small-resolution cost volumes are usually abandoned by these methods due to the lack of sufficient information, i.e., cannot generate an accurate disparity map separately. In this paper, we argue that different scale small-resolution cost volumes can cover multi-scale receptive fields and push the network to see different scale regions of original images. Hence, we can fuse them together to incorporate hierarchical context information and generate a more accurate initial disparity map than a higher-resolution sparse cost volume. Specifically, we first employ the input multi-scale features (smaller than 1/4 of the original input image resolution) to construct each scale cost volume respectively and then design a cost volume fusion module to integrate them. Details of the two steps will be provided below.

\textbf{Cost volume construction:} Inspired by \cite{msmdnet,gwcnet}, feature concatenation and group-wise correlation are employed to generate corresponding combination volume as follows:
\begin{eqnarray}
\begin{array}{c}
V_{concat}^i({d^i},x,y,f) = f_L^i(x,y)||f_R^i(x - {d^i},y) \\
\\
V_{gwc}^i({d^i},x,y,g) = \frac{1}{{N_c^i/{N_g}}}\left\langle {f_l^{ig}(x,y),f_r^{ig}(x - {d^i},y)} \right\rangle \\
\\
V_{combine}^i = V_{concat}^i||V_{gwc}^i
\end{array}
\label{eq:volume construction}
\end{eqnarray}
where $||$ denotes the vector concatenation operation. ${N_c}$ represents the channels of extracted features. ${N_g}$ is the amount of group. 
$\left\langle {{\rm{ }},{\rm{ }}} \right\rangle$ represents the inner product. ${f^i}$ denotes the extracted feature at scale (stage) $i$ and  $i = 0$ represents the original input image resolution.
 
Note that the disparity searching index ${d^i}$ is defined as 
${d^i} \in \{ 0,1,2 \ldots \frac{{{D_{\max }}}}{{{2^i}}} - 1\}$ and the hypothesis plane interval equals to 1 in the fused cost volume representation. That is, these cost volumes are all dense cost volumes with the size of  $\frac{H}{{{2^i}}} \times \frac{W}{{{2^i}}} \times \frac{{{D_{\max }}}}{{{2^i}}} \times F$. By densely sampling the whole disparity range in small resolution, we can efficiently generate the coarsest disparity map. Then pixel-level uncertainty estimation is employed to narrow down the disparity searching space at higher resolution and refine the disparity estimation in a coarse-to-fine manner. Please refer to Section 3.2.3 for more detail.

%\textbf{Cost Volume fusion:} Following the method proposed in \cite{msmdnet}, we use an improved encoder-decoder architecture to fuse low-resolution cost volumes. The architecture is shown in Fig. \ref{fig: fusion}. Specifically, we first employ four 3D convolution layers with skip connections to regularize each cost volume and use a 3D convolution layer (stride of two) to down-sample the combination volume of scale 3 from 1/8 to 1/16 of the original input image resolution. Next, we concatenate them (the down-sampled cost volume and the next stage combination volumes) at the feature dimension and then decrease the feature channel to a fixed size via one additional 3D convolution layer. Then, we apply a similar operation to progressively down-sample the cost volume to 1/32 of the original input image resolution and adopt 3-D transposed convolution to up-sample the volume in the decoder. In addition, we utilize one 3-D hourglass network to further regularize and refine the volume. Finally, an output module is applied to predict the disparity. The output module contains two more 3D convolution layers, aiming at obtaining a 1-channel 4D volume. To transform volume into probability, we apply soft argmin \cite{gcnet} operation to generate initial disparity map ${D^3}$. The soft argmin operation is defined as:

\textbf{Cost Volume fusion:} The architecture of cost volume fusion is shown in Fig. \ref{fig: fusion}. Specifically, we first employ four 3D convolution layers with skip connections to regularize each cost volume. Then, a 3D convolution layer (stride of two) is employed to downsample the scale 3 cost volume from 1/8 to 1/16 of the original input image size. Next, we concatenate the down-sampled cost volume and the next scale combination volume at the feature dimension and use one additional 3D convolution layer to decrease the feature channel to a fixed size. Similar operations are progressively employed until we downsample the cost volume to 1/32 of the original input image size. Finally, a 3D transposed convolution is adopted to up-sample the volume in the decoder and one 3-D hourglass network is further employed to aggregate the cost volume. Moreover, an output module is applied to predict the disparity from the fused cost volume. Specifically, we first employ two more 3D convolution layers to obtain a 1-channel 4D volume. Then, soft argmin \cite{gcnet} operation is applied to transform volume into probability and generate the initial disparity map ${D^3}$. The soft argmin operation is defined as:
\begin{eqnarray}
\widehat {{d^i}}{\rm{ = }}\sum\limits_{d = 0}^{\frac{{{D_{\max }}}}{{{2^i}}} - 1} {d \times \sigma ( - c_d^i)},
\label{softmax}
\end{eqnarray}
where $\sigma$ denotes the softmax operation and $c$ represents the predicted 1-channel 4D volume. ${\sigma ( - c_d)}$ denotes the discrete disparity probability distribution and the estimated disparity map is susceptible to all disparity indexes.



%\end{figure}
%% Figure environment removed



\subsubsection{Cascade Cost Volume}
\label{cascade_cost}
%Given the initial disparity estimation, the next step is to construct a fine-grained cost volume and refine disparity maps in a coarse-to-fine manner. Considering the next stage disparity searching range, uniform sampling a pre-defined range is the most straightforward way \cite{cascade}. However, such an approach assumes that all pixels are the same and cannot make adaptive pixel-level adjustments. For example, we should expand the searching range of ill-posed and occluded pixels. Furthermore, the disparity distribution of datasets with different characteristics is usually unbalanced. Thus, a question arises, how to avoid being affected by invalid disparity indexes in a large initial disparity range and capture more possible pixel-level disparity searching space with the prior knowledge of the last stage’s disparity estimation.

Given the initial disparity estimation, the next step is to construct a fine-grained cost volume and refine disparity maps in a coarse-to-fine manner. One naive way to construct the next stage disparity searching range is uniform sampling a pre-defined searching range \cite{cascade}. However, such a method treats all pixels equally and cannot make pixel-level adjustments. Furthermore, the unbalanced disparity distribution across different datasets requests networks to adjust the disparity searching range according to the input image adaptively. Hence, a question arises, can we drive the network to filter out invalid disparity indexes in a large disparity searching range and capture more possible pixel-level disparity searching space with prior knowledge of the last stage’s disparity estimation?

% Figure environment removed

% % Figure environment removed
%To tackle this problem, we propose an pixel-level uncertainty estimation to adaptively adjust the disparity searching range. As introduced in related work, discrete disparity probability distribution reflects the similarities between candidate matching pixel pairs and the final predicted disparity is a weighted sum of all disparity indexes according to their probability. Thus, the ideal disparity probability distribution should be unimodal peaked at true disparities. However, the actual probability distribution is predominantly unimodal or even multi-modal at some pixels, e.g., ill-posed and occluded. Previous work \cite{acfnet,gcnet} has discovered that the degree of multimodal distribution is highly correlated with the probability of prediction error. In addition, ill-posed areas, texture-less regions, and occlusions tend to be multimodal distribution as well as high estimation error rate. Therefore, we propose to define an pixel-level uncertainty estimation to quantify the degree to which the cost volume tends to be multi-modal distribution and employ it to evaluate the confidence of the current estimation. The pixel-level uncertainty is defined as:

To tackle this problem, we propose a pixel-level uncertainty estimation to adaptively adjust the disparity searching range. As mentioned in Eq. \ref{softmax}, the final predicted disparity can be obtained by softly weighting indices according to their probability. Thus, the discrete disparity probability distribution indeed reflects the similarities between candidate matching pixel pairs and the ideal disparity probability distribution should be unimodal peaked at true disparities. However, the actual probability distribution is predominantly unimodal or even multi-modal at some pixels, e.g., ill-posed and occluded areas. Moreover, existing methods \cite{acfnet,gcnet} have observed that the degree of multimodal distribution is highly correlated with the probability of prediction error. Hence, we propose to define a pixel-level uncertainty estimation to quantify the degree to which the cost volume tends to be multi-modal distribution and employ it to evaluate the confidence of the current estimation. The pixel-level uncertainty is defined as:
\begin{eqnarray}
\begin{array}{c}
{{\rm{U}}^i}{\rm{ = }}\sum\limits_{\forall {d^i}} {{{(d - {{\hat d}^i})}^2} \times \sigma ( - c_d^i)} \\
\\
\widehat {{d^i}}{\rm{ = }}\sum\limits_{\forall {d^i}} {d \times \sigma ( - c_d^i)} 
\end{array}
\end{eqnarray}
where $\sigma$ denotes the softmax operation and $c$ represents the predicted 1-channel 4D volume. Fig. \ref{fig: uncertainty estimation sample} gives a toy sample to show the effectiveness of pixel-level uncertainty estimation. As shown, the uncertainty of unimodal distribution equals to 0 and the more the distribution tends to be multimodal, the higher the error and uncertainty.
%We also notice that some special multi-modal distribution (\autoref{fig: uncertainty estimation sample} (d)) can achieve accurate estimation with high uncertainty. However, such a situation rarely happens and most of the multi-modal distribution leads to inaccurate estimation \cite{acfnet}. 
Thus, we can employ pixel-level uncertainty to evaluate the confidence of disparity estimation, higher uncertainty implies a higher probability of prediction error and a wider disparity searching space to correct the wrong estimation. Then, the next stage’s disparity searching range can be defined as:
% (visualization can be seen in Figure \ref{fig: ue visual})
\begin{eqnarray}
\begin{array}{l}
d_{\max }^{i - 1} = \delta (\widehat {{d^i}} + \left( {{\alpha ^i} + 1} \right)\sqrt {{U^i}}  + {\beta ^i})\\
\\
d_{\min }^{i - 1} = \delta (\widehat {{d^i}} - \left( {{\alpha ^i} + 1} \right)\sqrt {{U^i}}  - {\beta ^i})
\end{array}
\end{eqnarray}
where $\delta$ denotes bilinear interpolation. $\alpha$ and $\beta$ are normalization factors, which are initialized as 0 and gradually learn a weight. Then, uniform sampling can be employed to get the next stage discrete hypothesis disparity indexes ${d^{i - 1}}$:
\begin{eqnarray}
\begin{array}{c}
{d^{i - 1}} = d_{\min }^{i - 1} + n(d_{\max }^{i - 1} - d_{\min }^{i - 1})/\left( {{N^{i - 1}} - 1} \right)\\
\\
n \in \{ 0,1,2 \ldots {N^{i-1}} - 1\}
\end{array}
\end{eqnarray}
where ${N^{i - 1}}$ is the number of hypothesis planes at stage $i-1$. Then, a sparse while fine-grained cost volume at stage $i-1$ ($\frac{H}{{{2^{i - 1}}}} \times \frac{W}{{{2^{i - 1}}}} \times {N^{i - 1}} \times F$) can be constructed based on Eq.\ref{eq:volume construction}.  After
getting the next stage cost volume, a similar cost aggregation network (omitting the solid line in Fig. \ref{fig: fusion}) can be employed to predict the corresponding stage disparity map. By iteratively narrowing down the disparity range and higher the cost volume resolution, we can refine the disparity in a coarser to fine manner. Note that the final output of cascade cost volume $\hat{d^{1}}$ is half resolution of the original image. Thus, an up-sampling operation is necessary to up-sample $\hat{d^{1}}$ to the same size of original images, i.e., $\hat{d^{0}} = up(\hat{d^{1}})$, where the up-sampling operation \emph{up} is implemented by bilinear interpolation.

In summary, the proposed UCFNet outperforms previous cascade-based approaches, i.e., casstereo \cite{cascade} in the following three aspects: First, we propose to fuse multiple dense low-resolution cost volumes to generate a more accurate initial disparity estimation at lower resolution (see the comparison between the estimation of casstereo at stage 2 and our UCFNet at stage 3 in Fig. \ref{fig: cost volume distribution} (a) and (b)). Second, pixel-level uncertainty estimation is proposed to adaptively adjust the next stage disparity searching range which can push disparity distribution to be more predominantly unimodal (Fig. \ref{fig: cost volume distribution}(b)). Third, our method can better cover the corresponding ground truth value in the final stage disparity searching range by the proposed Uncertainty-based Cascade and Fused cost volume representation and corrects some biased results in casstereo (Fig. \ref{fig: cost volume distribution}(a)).

%\textcolor{blue}{Remove or not?    While it is noteworthy that the insight of cascade cost volume has also been investigated in \cite{cascade}, our work differs from theirs in the following three main aspects: First, casstereo constructs sparse cost volume to predict an initial disparity estimation. Instead, we propose to fuse multiple dense low-resolution cost volumes to generate more accurate initial disparity estimation at lower resolution (As shown by the comparison between the estimation of casstereo at stage 2 and estimation of our CFNet at stage 3 in Fig. \ref{fig: cost volume distribution} (a) and (b)). Second, cascade stereo only uniform sampling a pre-defined range to generate the next stage disparity searching range. In contrast, we develop a variance-based uncertainty estimation to adaptively adjust the next stage disparity searching range which can push disparity distribution to be more predominantly unimodal (As illustrated in Fig. \ref{fig: cost volume distribution}(b)). Third, by the cooperation of cascade and fused cost volume representation, our method is more likely to cover the corresponding ground truth value in the final stage disparity searching range and corrects some biased results in cascade stereo (as illustrated in Fig. \ref{fig: cost volume distribution}(a)).}


% Figure environment removed

\subsubsection{Attention-based disparity refinement}
%As mentioned in Section \ref{cascade_cost}, the output of our cascade cost volume $\hat{d^{0}}$ is up-sampled from the half-resolution output $\hat{d^{1}}$ by bilinear interpolation. However, such direct upsampling operations will lead to significant degradation of texture information. One naive solution for such degradation is to extend the proposed cascade cost volume to the original resolution while such an operation will bring unbearable GPU memory and time consumption. Hence, a lightweight attention-based disparity refinement module is proposed to make up for the missing details of half resolution output $\hat{d^{1}}$.

As mentioned in Section \ref{cascade_cost}, the output of our cascade cost volume $\hat{d^{0}}$ is up-sampled from the half-resolution disparity map $\hat{d^{1}}$ by bilinear interpolation. However, such direct upsampling operations will lead to the degradation of texture information, which indeed hinders both the finetuning performance and generalization of the proposed method. Hence, we propose a lightweight attention-based disparity refinement module to make up for the missing details. Experiments in Tab.1\&2\&3 demonstrate the proposed refinement network can achieve consistent improvement in both generalization and finetuning performance across multiple datasets. 

The pipeline of the attention mechanism is shown in Fig.~\ref{fig:channel_attention}. Taking up-sampled disparity $\hat{d^{0}}$ as input, we first employ multiple stacked convolution layers to extract the deep feature representation $f_{input}$. Then, three encoding operations \emph{P(.)}, \emph{Q(.)} and \emph{V(.)} are used to convert $f_{input}$ to three components ${f_p}$, ${f_Q}$ and ${f_V}$, in which reshape operation is utilized to convert the shape of ${f_Q}$, ${f_V}$ to ${f_Q} \in [C \times HW]$ and ${f_V} \in [HW \times C]$: 
\begin{eqnarray}
\begin{array}{l}
{f_{input}} = Conv({\hat{d^{0}}}),\\
{f_p} = P({f_{input}}),\\
{f_Q} = reshape(Q({f_{input}})),\\
{f_V} = reshape(V({f_{input}})),
\end{array}
\end{eqnarray}
where \emph{Conv} means convolution operation. Then, a matrix multiplication $\otimes$ and a softmax operation are introduced to generate the attention weight, which reflects the similarity between each channel position of input feature map $f_{input}$. Next, we employ a matrix multiplication operation between weights and $f_p$ with a 2D convolution layer to generate the residual disparity:
%\begin{eqnarray}
%Weight = \frac{{\exp ({f_Q} \cdot {f_V})}}{{\sum\nolimits_{i = 1}^N {\exp ({f_Q} \cdot {f_V})} }}
%\end{eqnarray}
%Where N denotes the channel number of input feature maps $f_{input}$. And the attention weight reflects the similarity between each channel position of input feature map $f_{input}$. Then, we employ a matrix multiplication operation between weights and $f_p$ with a 2D convolution layer to generate the residual disparity:
\begin{eqnarray}
\hat {d_{residual}^0} = Conv(Weight \otimes {f_p} + {f_{input}}),
\end{eqnarray}
where \emph{Conv} means convolution operation. Finally, an element-wise addition operation is employed to generate the refined disparity:
\begin{eqnarray}
\hat {d_{refine}^0} = \hat {d_{residual}^0} + \hat {{d^0}}
\end{eqnarray}
%Generally, the proposed attention mechanism can be understood as a non-local convolution operation, which aims to enhance the channels with much more effective information. The non-local operation in the proposed attention-based disparity refinement module can obtain effective attention weights for each channel by exploiting all the position information of the feature maps, which provides an effective score to describe the tendency of two feature maps at different channels. Visualization results are shown in Fig. \ref{fig: residual disparity}. As shown, the original disparity estimation results lack texture information, e.g., missing of thin structures (see red dash boxes in the picture) and unsmooth regions due to the segmentation of lane lines (see yellow dash boxes in the picture). Instead, the proposed attention-based disparity refinement module can learn such missing details by the residual disparity (sub-figure (b)) and generate a better refined disparity map.

Visualization results are shown in Fig. \ref{fig: residual disparity}. All methods are only trained on the Scene Flow datatest and tested on unseen KITTI datasets. As shown, the original disparity estimation results lack texture information, e.g., the missing thin structures (see red dash boxes in the picture) and unsmooth regions due to the segmentation of lane lines (see yellow dash boxes in the picture). Instead, the proposed attention-based disparity refinement module can learn such missing details by the residual disparity (sub-figure (b)) and generate a better refined disparity map.

% In this section, we use attention mechanism to further refine the disparity. In specific, after obtaining disparity $\hat{d_i}$, $i=1$, which is half size of input left and right images, a up-sampling operation is utilized to up-sample $\hat{d_i}$, $i=1$, to the size of left and right images, thus initial disparity $\hat{d} = up(\hat{d_1})$ can  be obtained, where \emph{up} means the up-sampling operation and we use bilinear here. Then, taking initial disparity $\hat{d}$ as input, a convolution operation is utilized and three encoding operations \emph{P(.)}, \emph{Q(.)} and \emph{V(.)} are used to convert $f_{input}$ to three components: $f_p$ = $P(Conv(\hat{d}))$, $Q(Conv(\hat{d}))$ and $V(Conv(\hat{d}))$, where \emph{Conv} means convolution operation. Then reshape operation is utilized to convert $Q(Conv(\hat{d}))$, $V(Conv(\hat{d}))$ to $f_Q=(c\times (hw))$ and $f_V=((hw) \times c)$. softmax and matrix multiplication ($\otimes$) are employed to obtain weights by $weights=softmax(f_Q) \otimes softmax(f_V)$. Then the residual disparity can be obtained by $\hat{d_{rd}} = Conv(weights \otimes f_p$), where \emph{Conv} means convolution operation. The final refined disparity $\hat{d_r}$ can be obtained by $\hat{d_r} = \hat{d} + \hat{d_{rd}}$. The pipeline of the attention mechanism is shown in Fig.~\ref{fig:channel_attention}, and the process can be formulated as:

% \begin{equation}
%     \begin{split}
%         &f_p = P(Conv(\hat{d}))\\
%         &f_Q = reshape(Q(Conv(\hat{d}))\\
%         &f_V = reshape(V(Conv(\hat{d}))) \\
%         &weights=softmax(f_Q) \otimes softmax(f_V) \\
%         &\hat{d_{rd}} = Conv(weights \otimes f_{p}) \\
%         &\hat{d_r} = \hat{d} + \hat{d_{rd}} \\
%     \end{split}
% \end{equation}

% The attention can be understood as non-local convolution process, which aims to enhance the channels with much more effective information. The non-local operation in the proposed channel attention based reconstruction can obtain effective attention weights for each channel by exploiting all the position information of the feature maps, which provides an effective score to describe the tendency of two feature maps at different channels. 

% Figure environment removed

\subsubsection{Loss function}
Inspired by previous work \cite{psmnet}, we employ smooth $L_1$ loss function \cite{smoothl1} to train the proposed stereo matching network. Specifically, the loss function is described as:
\begin{eqnarray}
L\left( {\hat D,{D_{gt}}} \right) = \frac{1}{{{N_{gt}}}}\sum\limits_{i = 1}^{N_{gt}} {smoot{h_{{L_1}}}({D_{gt}} - \hat D)}
\end{eqnarray}
in which
\begin{eqnarray}
Smoot{h_{{L_1}}}(x) = \left\{ \begin{array}{l}
0.5{x^2},{\rm{    if }}\left| x \right| < 1\\
\left| x \right| - 0.5,{\rm{ otherwise}}
\end{array} \right.
\end{eqnarray}
where $N_{gt}$ denotes the number of available pixels in the provided ground truth disparity of source domain and $\hat D$ represents the predicted disparity.

\subsection{Uncertainty estimation for pseudo-label generation}
\label{sec:uncertainty}
We propose an uncertainty based pseudo-label generation method to generate low-noise disparity maps and leverage them as supervision to adapt the pre-trained model to the target domain. A key observation behind our method is that deep stereo matching methods can be successfully adapted to a new domain by only deploying sparse ground-truth labels or even sparse noisy predictions \cite{unsuperviseddomainadaptation, self-supervisedeccv}. Based on the above observations, we propose to employ the target domain image pairs $({I_L},{I_R})$ and the pre-trained model ${M_{{\rm{pre}}}}$ to generate dense disparity maps ${D_{{\rm{pre}}}}$. Then we can leverage the uncertainty estimation to filter out unreliable points of ${D_{{\rm{pre}}}}$ and generate reliable and sparse disparity maps ${D_u}$. Specifically, two terms of uncertainty estimation, i.e., pixel-level and area-level are employed to generate low-noise disparity maps. Below we will introduce each term of uncertainty estimation for more details.

\textbf{Pixel-level Uncertainty Estimation:}  
% As mentioned in Sec. \ref{cascade_cost}, we propose an pixel-level uncertainty estimation to adaptively adjust the next stage disparity searching range. By defining a pixel-level uncertainty estimation to quantify the degree to which the cost volume tends to be multi-modal distribution, we can evaluate the confidence of the current estimation reasonably. Hence, intuitively, we can directly introduce the pixel-level uncertainty estimation to generate the corresponding pseudo-label ${D_{pixel}}$ as follows:
As mentioned in Sec. \ref{cascade_cost}, we propose a pixel-level uncertainty estimation to adaptively adjust the next stage disparity searching range. As mentioned before, the proposed pixel-level uncertainty estimation is according to the sharpness of cost volume distribution to evaluate the confidence of current estimations. That is the proposed pixel-level uncertainty estimation has no learnable parameters and is totally decided by the input cost volume distribution. As the geometry of cost volume distribution is domain-invariant, we can ensure the generalization ability of pixel-level uncertainty estimation. Hence, intuitively, we can directly introduce the pixel-level uncertainty estimation to generate the corresponding pseudo-label ${D_{pixel}}$ as follows:
\begin{eqnarray}
\begin{array}{c}
{D_{{\rm{pre}}}} = {M_{{\rm{pre}}}}({I_L},{I_R}),\\
\\
{D_{pixel}} = \{ d \in {D_{pre}}:\sqrt U  < t\}, 
\end{array}
\end{eqnarray}
where $t$ is the threshold that controls the density and reliability of the filtered disparity map ${D_{pixel}}$. A lower value of t will filter out more mistakes in ${D_{{\rm{pre}}}}$ and generating more sparse disparity maps ${D_{pixel}}$. Thus, by setting a reasonable threshold, we can utilize filtered disparity maps ${D_u}$  as if they were ground truth to supervise the fine-tuning of the pre-trained model ${M_{{\rm{pre}}}}$. As shown in Fig. \ref{fig: uncertainty estimation}, the proposed pixel-level uncertainty estimation can filet out most error of the cross-domain disparity estimation ${D_{{\rm{pre}}}}$ and generate low-noise disparity maps ${D_{pixel}}$. However, the proposed pixel-level uncertainty estimation still has the following two shortcomings: 1) it only evaluates pixel-level confidence and doesn’t consider the influence of neighboring pixels or global information. 2) it only employs the cost volume as input curs while ignoring the usage of multi-input, i.e., reference image and disparity maps. Thus, area-level uncertainty estimation is essential for the further refinement of pixel-level uncertainty maps. 

% Figure environment removed

\textbf{Area-level Uncertainty Estimation:} We propose an area-level uncertainty estimation to leverage the information of multi-modal input and neighboring pixels. Intuitively, both neighborhood and multi-modal information can better guide the pixel-level uncertainty map to identify the prediction correct region. For example, assuming that a pixel has high pixel-level uncertainty while the pixel-level uncertainty of surrounding similar pixels is relatively low, then we can judge it is the correct prediction to preserve more valid labels. In this case, the neighborhood information can guide the network to discover surrounding pixels and the multi-modal information can help the network to distinguish whether these pixels are similar and can be considered to have close uncertainty. Specifically, let us denote the input initial uncertainty map as ${U_{pixel}}$. Our goal is to recover from ${U_{pixel}}$ an improved uncertainty map ${U_{area}}$ that can more accurately identify the region where the disparity estimation is wrong. Such a task can be seen as a binary classification mission, where the output of the area-level uncertainty estimation will be constrained in the range of $(0,1)$. A higher value denotes a higher possibility of prediction error, i.e., higher uncertainty. Actually, such a task is very similar to the setting of salient object detection, which also employs a binary classification network to identify the visually distinctive regions or objects in a scene. Moreover, the usage of multi-modal input and neighboring information is an essential topic in salient object detection and has drawn great attention from the community. Hence, we propose to employ some network design ideas in salient object detection \cite{salient1,salient2,salient3, gatanet} to construct our area-level uncertainty estimation network. The overall architecture of our area-level uncertainty estimation network is shown in Fig. \ref{fig: area-level uncertainty}, which consists of six parts: left image encoder blocks $E_l^i$, uncertainty map encoder blocks $E_U^i$, fusion blocks $F^i$, gate unit $G^i$, main decoder block $D_m^i$ and residual decoder block $D_r^i$  ($i$ denotes different scales and $i = 0$ represents the original input image resolution). Below we will introduce each part in more detail.

\textbf{Encoder blocks:} Our encoder blocks can be divided into two parts: the reference image encoder block $E_l^i$ and uncertainty map encoder block $E_U^i$, which characterizes the information of the reference image and the concatenation of pixel-level uncertainty map and predicted disparity map, respectively. Specifically, we propose to employ the commonly used pre-trained backbone network ResNet-34 to construct our encoder block. Similar to previous work, we remove the last fully-connected and pooling layers of the employed backbone network.

\textbf{Fusion blocks:} Our fusion block has two main inputs: 1) left image encoder blocks, which represent the multi-scale information of reference images. 2) uncertainty map encoder block, which stores the multi-scale information of pixel-level uncertainty map and predicted disparity map. By employing the fusion blocks, we can integrate scale-matching reference image encoder blocks and uncertainty map encoder blocks to extract robust multi-modal input representation. Specifically, the fusion process can be formulated as:
\begin{equation}
{F^i} = \delta({ E_l^i }||{ E_U^i }),
    \label{eq:decoder block}
\end{equation}
where $||$ denotes the concatenation operation and $\delta$ refers to the convolution layer.

\textbf{Gate unit based decoder block: } Inspired by previous work \cite{salient3,gated2}, we propose the gate unit based decoder block to control the message passing between scale-matching fusion and decoder blocks. In specific, our gate unit is formulated as:
\begin{scriptsize}
\begin{equation}
{G^i} = [G_m^i,G_r^i] = \left\{ \begin{array}{l}
AvgPool(S(\delta ({F^i}\parallel {D_m^{i + 1}}))){\rm{               if }}\;i = 1,2,3,4\\
AvgPool(S(\delta ({F^i}||FASPP({F^i})))){\rm{   if }}\;i  =  5
\end{array} \right.
\end{equation}
\end{scriptsize}
where $AvgPool$ denotes the global average pooling, $S$ is the sigmoid function, $||$ represents the concatenation operation, $\delta$ refers to the convolution layer and $FASPP$ is the Fold-ASPP operation \cite{gatanet}. Note that the output channel of $\delta$ is 2. Hence, the proposed gate unit has two outputs, i.e., $G_m^i$ and $G_r^i$ , which will be employed to control the message passing in main decoder block $D_m^i$ and residual decoder block $D_r^i$, respectively. Specifically,  $G_m^i$  is employed to balance the contribution between upsampled main decoder blocks $D_m^i$ and corresponding fusion blocks $F^i$. The whole process can be written as:
\begin{equation}
D_m^i = \left\{ \begin{array}{l}
\delta (G_m^i \times \delta ({F^i}) + Up(D_m^{i + 1})){\rm{  if }}\;i = 1,2,3,4\\
\delta (G_m^i \times FASPP({F^i})){\rm{          if}}\; i  =  5
\end{array} \right.
\end{equation}
where $\times$ is the element-wise multiplication operation and $Up$ refers to the up-sampling operation which is implemented by bilinear interpolation. Then we further introduce the residual decoder block $D_r^i$ to recover the missed details of the main decoder blocks and also employ the gate unit $G_r^i$ to balance the information flow. The whole process can be formulated as:
\begin{equation}
D_r^i = \left\{ \begin{array}{l}
G_r^i \times \delta ({F^i})||Up(D_r^{i - 1}{\rm{)  if }}\;i = 1,2,3,4\\
G_r^i \times \delta ({F^i}){\rm{           if}}\; i  =  5
\end{array} \right.
\end{equation}
Finally, we can fuse the output of two terms of decoder blocks to generate the area-level uncertainty map ${U_{area}}$:
\begin{equation}
{U_{area}} = S(\delta (D_r^1||D_m^1) + D_m^1)
\end{equation}
where $S$ is the sigmoid function. Thus, the output of the area-level uncertainty estimation ${U_{area}}$ is in the range of $(0,1)$ and a higher value denotes a higher possibility of prediction error. Then, we can use the same operation introduced in pixel-level uncertainty estimation to generate the corresponding pseudo-label ${D_{area}}$ as follows:
\begin{eqnarray}
\begin{array}{c}
{D_{area}} = \{ d \in {D_{pre}}:{U_{area}}  < t\} 
\end{array}
\end{eqnarray}
where $t$ is the threshold that controls the density and reliability of the filtered disparity map ${D_{area}}$.


\subsubsection{Loss function}
We employ the cross-entropy loss to train the proposed area-level uncertainty estimation network. Specifically, the cross-entropy loss can be defined as:
\begin{eqnarray}
l = U_{gt}\log {U_{area}} + (1 - U_{gt})\log (1 - {U_{area}})
\end{eqnarray}
where $U_{area}$ and $U_{gt}$ denote the predicted area-level uncertainty map and ground truth uncertainty mask, respectively. As the ground truth uncertainty mask is not provided in the source domain(synthetic dataset) and indeed the value of it will change as the convergence of the stereo matching network. Here, we specify how to obtain $U_{gt}$ on source domain according to the provided ground truth disparity $D_{gt}$ and predicted disparity $\hat D$. Specifically, our ground truth uncertainty mask is defined as:
\begin{eqnarray}
{U_{gt}} = \left\{ \begin{array}{l}
1,{\rm{  if }}\left| {{D_{gt}} - \hat D} \right|{\rm{ > }}\delta {\rm{ }}\\
0,{\rm{  otherwise}}
\end{array} \right.
\end{eqnarray}
where $\delta$ is the threshold that controls the strictness of uncertainty estimation, e.g., a higher $\delta$ denotes a larger gap between $D_{gt}$ and $\hat D$ can be seen as a correct prediction.


\subsection{Domain Adaptation with supervision of pseudo-label  }
\label{sec:domain_adaptation}
% We propose to employ our uncertainty estimation to generate low-noise disparity maps and leverage them as supervision to adapt the pre-trained model to the target domain. A key observation behind our method is that deep stereo matching methods can be successfully adapted to a new domain by only deploying sparse ground-truth labels or even sparse noisy predictions \cite{unsuperviseddomainadaptation, self-supervisedeccv}. 

% Based on the above observations, we propose to employ the target domain image pairs $({I_L},{I_R})$ and the pre-trained model ${M_{{\rm{pre}}}}$ to generate dense disparity maps ${D_{{\rm{pre}}}}$. Then we leverage the uncertainty estimation to filter out unreliable points of ${D_{{\rm{pre}}}}$ and generate reliable and sparse disparity maps ${D_u}$, defined as:
% \begin{eqnarray}
% \begin{array}{c}
% {D_{{\rm{pre}}}} = {M_{{\rm{pre}}}}({I_L},{I_R})\\
% \\
% {D_u} = \{ d \in {D_{pre}}:\sqrt U  < t\} 
% \end{array}
% \end{eqnarray}
% where $t$ is the threshold that controls the density and reliability of the filtered disparity map ${D_u}$. A lower value of t will filter out more mistakes in ${D_{{\rm{pre}}}}$ though generating more sparse disparity maps ${D_u}$. Thus, by setting a reasonable threshold, we can utilize filtered disparity maps ${D_u}$  as if they were ground truth to supervise the fine-tuning of the pre-trained model ${M_{{\rm{pre}}}}$.
After getting the generated pseudo-label ${D_{area}}$, we can employ it to adapt the pre-trained binocular depth estimation network to the new domain. The loss function is defined as: 
%\begin{eqnarray}
%\begin{array}{l}
%L\left( {D,{D_u}} \right) = \frac{1}{{{N_u}}}\sum\limits_{i = 1}^N {smoot{h_{{L_1}}}({D_u} - \hat D)} \\
%\\
%Smoot{h_{{L_1}}}(x) = \left\{ \begin{array}{l}
%0.5{x^2},{\rm{    if }}\left| x \right| < 1\\
%\left| x \right| - 0.5,{\rm{ otherwise}}
%\end{array} \right.
%\end{array}
%\end{eqnarray}
\begin{eqnarray}
L\left( {D,{D_{area}}} \right) = \frac{1}{{{N_{area}}}}\sum\limits_{i = 1}^{N_{area}} {smoot{h_{{L_1}}}({D_{area}} - \hat D)}
\end{eqnarray}
in which
\begin{eqnarray}
Smoot{h_{{L_1}}}(x) = \left\{ \begin{array}{l}
0.5{x^2},{\rm{    if }}\left| x \right| < 1\\
\left| x \right| - 0.5,{\rm{ otherwise}}
\end{array} \right.
\end{eqnarray}
where $N_{area}$ denotes the number of available pixels in the filtered disparity map ${D_{area}}$ and $\hat D$ represents the predicted disparity. Besides, we can also employ the generated pseudo-label as supervision to train the monocular depth estimation network in an unsupervised way. The loss function is defined as:
\begin{eqnarray}
L({\hat D_{mono}},{D_{area}}) = \sqrt {\frac{1}{{{N_{area}}}}\sum\limits_{i = 1}^{{N_{area}}} {d_i^2 - \frac{\lambda }{{{n^2}}}} {{(\sum\limits_{i = 1}^{{N_{area}}} {{d_i}} )}^2}}
\end{eqnarray}
in which
\begin{eqnarray}
{d_i} = \log {{\hat D}_{mono}} - \log {D_{area}}
\end{eqnarray}
where $N_{area}$ denotes the number of available pixels in the filtered disparity map ${D_{area}}$ and $\hat D_{mono}$ represents the predicted disparity map by monocular depth estimation networks. The balancing factor $\lambda$ is set to 0.85. Then the disparity can be converted to depth by triangulation:
\begin{eqnarray}
Depth = \frac{{fB}}{d}
\end{eqnarray}
where f denotes the camera's focal length and B is the baseline, i.e., the distance between two camera centers.


% In addition, by replacing ${D_u}$ with ground truth disparity ${D_gt}$, we can train our network in a supervised manner.