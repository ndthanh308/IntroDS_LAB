\section{Stereo matching Experiments}
\label{sec:stereo_experinment}
% In this section, we discuss the datasets we used, data argumentation and implementation details. The datasets and data argumentation are described in Section 2.1 and 2.2. The implementation details are described in Section 2.3.  
\subsection{Dataset}
%\begin{itemize}

\noindent \textbf{SceneFlow:} This is a large synthetic dataset including 35,454 training and 4,370 test images with a resolution of $960\times 540$ for optical flow and stereo matching. We use it to pre-train our network. 

\noindent \textbf{Middlebury:} Middlebury \cite{mid} is an indoor dataset with 28 training image pairs (13 of them are additional training images) and 15 testing image pairs with full, half, and quarter resolutions. It has the highest resolution among the three datasets and the disparity range of half-resolution image pairs is 0-400. 

\noindent \textbf{KITTI 2012\&2015:} They are both real-world datasets collected from a driving car. KITTI 2015 \cite{kitti2} contains 200 training and another 200 testing image pairs while KITTI 2012 \cite{kitti1} contains 194 training and another 195 testing image pairs. Both training image pairs provide sparse ground-truth disparity and the disparity range of them is 0-230.

\noindent \textbf{ETH3D:} ETH3D \cite{eth3d} is the only grayscale image dataset with both indoor and outdoor scenes. It contains 27 training and 20 testing image pairs with sparsely labeled ground truth. It has the smallest disparity range among the three datasets, which is just in the range of 0-64.

\begin{table*}[!htb]
\caption{\footnotesize Joint Generalization comparison on ETH3D, Middlebury, and KITTI2015 datasets. 	\textbf{Top:} Joint Generalization comparison with methods who participated in the Robust Vision Challenge 2020, our method achieves the best overall performance. \textbf{Bottom:} Joint Generalization comparison with the top 3 methods in the past three years, our method surpasses previous work on all three datasets by a noteworthy margin. All methods are tested on three datasets without adaptation. We highlight the best result in \textbf{bold} and the second-best result in \B{blue} for each column. The overall rank is obtained by Schulze Proportional Ranking \cite{rank} to join multiple rankings into one.
% Note that other than typically ranking the performance with a single metric, to construct a robust system, every sub ranking is decided by a number of metrics per dataset.
}
\vspace{-0.1in}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{Method}   & \multicolumn{4}{c|}{KITTI}                                 & \multicolumn{4}{c|}{Middlebury}                            & \multicolumn{4}{c|}{ETH3D}                                 & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Overall\\ Rank\end{tabular}} \\ \cline{2-13}
                          & D1\_bg        & D1\_fg        & D1\_all       & Rank & bad 1.0       & bad 2.0       & avg error     & Rank & bad 1.0       & bad 2.0       & avg error     & Rank &                                  \\ \hline
NLCANet\_V2\_RVC \cite{nlcanet}         & \textbf{1.51} & \textbf{\B{3.97}}          & \textbf{1.92} & \textbf{1} & \textbf{\B{29.4}}          & \textbf{\B{16.4}}          & 5.60          & 3          & \textbf{\B{4.11}}          & \textbf{\B{1.2}}           & \textbf{\B{0.29}}          & \textbf{\B{2}}          & \textbf{\B{2}}                                \\ 
HSMNet\_RVC \cite{hsm}               & 2.74          & 8.73          & 3.74          & 6          & 31.2          & 16.5          & \textbf{3.44} & \textbf{1} & 4.40          & 1.51          & 0.28          & 3          & 3                                \\ 
CVANet\_RVC               & 1.74          & 4.98          & 2.28          & 3          & 58.5          & 38.5          & 8.64          & 5          & 4.68          & 1.37          & 0.34          & 4          & 4                                \\ 
AANet\_RVC \cite{aanet}                & 2.23          & 4.89          & 2.67          & 5          & 42.9          & 31.8          & 12.8          & 6          & 5.41          & 1.95          & 0.33          & 5          & 5                                \\ 
GANet\_RVC \cite{ganet}                & 1.88          & 4.58          & 2.33          & 4          & 43.1          & 24.9          & 15.8          & 7          & 6.97          & 1.25          & 0.45          & 6          & 6                                \\ 
\textbf{CFNet\_RVC(ours)} & \textbf{\B{1.65}}          & \textbf{3.53} & \textbf{\B{1.96}}          & \textbf{\B{2}}          & \textbf{26.2} & \textbf{16.1} & \textbf{\B{5.07}}          & \textbf{\B{2}}          & \textbf{3.7}  & \textbf{0.97} & \textbf{0.26} & \textbf{1} & \textbf{1}                       \\ \hline
iResNet\_ROB \cite{iresnet, mcvmfc}                   & 2.27          & 4.89          & 2.71          & 4          & 45.9          & 31.7          & 6.56          & 3          & 4.67          & 1.22          & 0.27          & 4          & 4                                \\ 
Deeppruner\_ROB \cite{deeppruner}                & -             & -             & 2.23          & 3          & 57.1          & 36.4          & 6.56          & 4          & 3.82          & 1.04          & 0.28          & 3          & 3                                \\ 
\textbf{CFNet\_RVC(ours)} & \textbf{\B{1.65}} & \textbf{\B{3.53}} & \textbf{\B{1.96}} & \textbf{\B{2}}          & \textbf{26.2} & \textbf{16.1} & \textbf{5.07} & \textbf{1}          &\textbf{\B{3.7}}           & \textbf{\B{0.97}}          & \textbf{\B{0.26}}          & \textbf{\B{2}}          & \textbf{\B{2}}                               \\ 
\textbf{UCFNet\_RVC(ours)} & \textbf{1.57} & \textbf{3.33} & \textbf{1.86} & \textbf{1}          & \textbf{\B{31.6}} & \textbf{\B{16.7}} & \textbf{\B{5.96}} & \textbf{\B{2}}          &\textbf{3.37}           & \textbf{0.78}          & \textbf{0.25}          & \textbf{1}          & \textbf{1}                               \\ \hline
\end{tabular}
}
\label{tab: robust challenge}
\vspace{-0.15in}
\end{table*}

\subsection{Implementation Details}
% We use PyTorch to implement our 3-stage network and employ Adam (${\beta _{\rm{1}}} = 0.9,{\beta _2} = 0.999$) to train the whole network in an end-to-end way. The batch size is set to 8 for training on 2 Tesla V100 GPUs and the whole disparity search range is fixed to 256 during the training and testing process. We employ the smooth L1 loss function to train our network and include all intermediate outputs in the loss weight. ${N^1}$ and ${N^2}$ are set as 12 and 16, respectively. Asymmetric chromatic augmentation and asymmetric occlusion \cite{hsm} are employed for data augmentation. 

% % Specifically, we apply different chromatic augmentation to the image pairs hoping our network can improve its robustness when stereo cameras are under different lighting and exposure conditions. We also randomly replace a rectangle region at the target image with the RGB means of the whole picture so that our network can learn to predict the disparity without correspondences.

% Inspired by the two-stage finetuning strategy \cite{mcvmfc}, we propose a three-stage finetune strategy to train our network. First, following the method proposed in \cite{msmdnet}, we use switch training strategy to pre-train our model in the SceneFlow dataset. Specifically, we first use ReLU to train our network from scratch for 20 epochs, then we switch the activation function to Mish and prolong the pre-training process in the SceneFlow dataset for another 15 epochs. Second, we jointly finetune our pre-train model on four datasets, i.e., KITTI 2015, KITTI2012, ETH3D, and Middlebury for 400 epochs. The initial learning rate is 0.001 and is down-scaled by 10 after epoch 300. Third, we augment Middlebury and ETH3D to the same size as KITTI 2015 and finetune our model for 50 epochs with a learning rate of 0.0001. The core idea of our three-stage finetune strategy is to prevent the small datasets from being overwhelmed by large datasets. One selection is to directly augment small datasets to the size of the large dataset at the beginning. But such an approach may lead to overfitting on small datasets because of repeating the training process on the same picture with a large learning rate. So we propose to augment small datasets at stage three and train our model with a small learning rate. Consequently, our strategy makes a better trade-off between generalization capability and fitting capability on three datasets.

We use PyTorch to implement our network and employ Adam (${\beta _{\rm{1}}} = 0.9,{\beta _2} = 0.999$) to train the whole network in an end-to-end way. The batch size is set to 16 for training on 4 Tesla V100 GPUs and the whole disparity search range is fixed to 256 during the training and testing process. ${N^1}$ and ${N^2}$ are set as 12 and 16, respectively. Threshold $\delta$ of the ground truth uncertainty mask is 1. Asymmetric chromatic augmentation and asymmetric occlusion \cite{hsm} are employed for data augmentation. Below we will introduce our training process for each term of generalization in detail.  

\noindent \textbf{Cross-domain generalization training:} Our pre-training process on the synthetic dataset (source domain) can be broken down into three steps. Firstly, we use switch training strategy to pre-train our UCFNet in the SceneFlow dataset. Specifically, we first use ReLU to train our network from scratch for 20 epochs, then we switch the activation function to Mish and prolong the pre-training process in the SceneFlow dataset for another 15 epochs. Secondly, we fix the weights of the UCFNet (except the refinement module) and train the attention-based refinement network individually for 20 epochs with a 0.0001 learning rate. Thirdly, we fix the weights of the UCFNet and train the area-level uncertainty estimation network alone for 15 epochs. The initial learning rate is 0.001 and is down-scaled by 2 after epochs 10,12,14.

\noindent \textbf{Adapt generalization training:} After obtaining a strong pre-training model, we can further adapt our pre-trained model to the new domain with the generated pseudo-labels. Specifically, the training process of domain adaptation can be broken down into two steps. First, we feed the synchronized stereo images of the target domain into the pre-trained model and employ the proposed uncertainty-based pseudo-label generation method to generate corresponding pseudo-labels. Secondly, we employ the generated pseudo-labels as supervision to adapt the pre-trained model to the new domain. Specifically, we first fix the weights of the refinement network and train UCFNet on the target domain for 50 epochs with a 0.001 learning rate. Then, we fix the weights of the UCFNet (except the refinement module) and train the attention-based refinement network individually for 50 epochs with a 0.0001 learning rate.

\noindent \textbf{Joint generalization training:} We propose a three-stage finetuning strategy for joint generalization training. First, as mentioned in the cross-domain generalization training, we employ the switch training strategy to pre-train our model in the SceneFlow dataset. Second, we jointly finetune our pre-train model on four datasets, i.e., KITTI 2015, KITTI2012, ETH3D, and Middlebury for 400 epochs. The initial learning rate is 0.001 and is down-scaled by 10 after epoch 300. Third, we augment Middlebury and ETH3D to the same size as KITTI 2015 and finetune our model for 50 epochs with a learning rate of 0.0001. The core idea of our three-stage finetune strategy is to prevent the small datasets from being overwhelmed by large datasets. By augmenting small datasets at stage three and training our model with a small learning rate, our strategy makes a better trade-off between generalization capability and fitting capability on three datasets.

% \begin{table}[!htb]
% \centering
% \resizebox{0.42\textwidth}{!}{
% \begin{tabular}{c|c|c|c|c}
% \hline
% Method    & \begin{tabular}[c]{@{}c@{}}KITTI2012\\ D1\_all(\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}KITTI2015\\ D1\_all(\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Middlebury\\ bad 2.0(\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}ETH3D\\ bad 1.0(\%)\end{tabular} \\ \hline
% PSMNet \cite{psmnet}   & 15.1                                                            & 16.3                                                            & 39.5                                                             & 23.8                                                        \\ 
% GWCNet \cite{gwcnet}    & 12.0                                                            & 12.2                                                            & 37.4                                                             & 11.0                                                        \\ 
% CasStereo \cite{cascade} & 11.8                                                            & 11.9                                                            & 40.6                                                             & 7.8                                                         \\ 
% GANet \cite{ganet}    & 10.1                                                            & 11.7                                                            & 32.2                                                             & 14.1                                                        \\ 
% DSMNet \cite{dsmnet}   & 6.2                                                             & 6.5                                                             & \textbf{21.8}                                                    & 6.2                                                         \\ \hline
% CFNet     & \textbf{4.7}                                                    & \textbf{5.8}                                                    & 28.2                                                             & \textbf{5.8}                                                \\ \hline
% \end{tabular}
% }
% \caption{Cross-domain generalization evaluation on ETH3D, Middlebury, and KITTI training sets. All methods are only trained on the Scene Flow datatest and tested on full-resolution training images of three real datasets.}
% \label{tab: cross-domain generalization}
% \end{table}




\begin{table}[!t]
\caption{\footnotesize \textbf{Top}: Cross-domain generalization evaluation on ETH3D, Middlebury, and KITTI training sets. All methods are only trained on the Scene Flow datatest and tested on full-resolution training images of three real datasets. \textbf{Bottom}: Adaptation generalization evaluation on ETH3D, Middlebury, and KITTI training sets. TDD: target domain data. All methods are finetuned on the unlabeled target domain data. We highlight the best result in \textbf{bold} and the second-best result in \B{blue} for each column.}
\vspace{-0.1in}
\centering
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c}
\hline
Method       & Training Set         & \begin{tabular}[c]{@{}c@{}}KITTI2012\\ D1\_all(\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}KITTI2015\\ D1\_all(\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Middlebury\\ bad 2.0(\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}ETH3D\\ bad 1.0(\%)\end{tabular} \\ \hline
\multicolumn{6}{c}{Cross-domain Generalization Evaluation}                                                                                                                                                                                                                                                        \\ \hline
PSMNet \cite{psmnet}       & synthetic            & 15.1                                                            & 16.3                                                            & 39.5                                                             & 23.8                                                        \\ 
GWCNet \cite{gwcnet}      & synthetic            & 12.0                                                            & 12.2                                                            & 37.4                                                             & 11.0                                                        \\ 
CasStereo \cite{cascade}    & synthetic            & 11.8                                                            & 11.9                                                            & 40.6                                                             & 7.8                                                         \\ 
GANet \cite{ganet}       & synthetic            & 10.1                                                            & 11.7                                                            & 32.2                                                             & 14.1                                                        \\ 
DSMNet \cite{dsmnet}       & synthetic            & 6.2                                                             & 6.5                                                             & \B{\textbf{21.8}}                                                    & 6.2                                                         \\
ITSA-CFNet \cite{itsa}       & synthetic            & \textbf{4.2}                                                             & \textbf{4.7}                                                             & \textbf{20.7}                                                    & \B{\textbf{5.1}}                                                         \\ 
% Graft-Ganet \cite{graftnet}       & synthetic            & \textbf{4.2}                                                             & 4.9                                                             & \textbf{-/9.8}                                                    & 6.2                                                         \\
% FC-Ganet \cite{fcnet}       & synthetic            & 4.6                                                             & 5.3                                                             & \textbf{-/10.2}                                                    & 5.8                                                         \\ 
\textbf{CFNet(ours)\cite{cfnet}}       & synthetic            & 4.7                                                    & 5.8                                                    & 28.2                                                             & 5.8                                                \\ 
\textbf{UCFNet\_pretrain}       & synthetic            & \B{\textbf{4.5}}                                                    & \B{\textbf{5.2}}                                                    & 26.0                                                             & \textbf{\textbf{4.8}}                                                \\ \hline
\multicolumn{6}{c}{Adaptation Generalization Evaluation}                                                                                                                                                                                                                                                          \\ \hline
AOHNet \cite{aohnet}       & synthetic+TDD(no gt) & 8.6                                                             & 7.8                                                             & -                                                                & -                                                           \\ 
ZOLE \cite{zoom}     & synthetic+TDD(no gt) & -                                                               & 6.8                                                             & -                                                                & -                                                           \\ 
MADNet \cite{madnet}      & synthetic+TDD(no gt) & 9.3                                                             & 8.5                                                             & -                                                                & -                                                           \\
AdaStereo \cite{adastereo,adastereo_ijcv}      & synthetic+TDD(no gt) & \B{\textbf{3.6}}                                                             & \B{\textbf{3.5}}                                                             & \textbf{\textbf{18.5}}                                                                & \B{\textbf{4.1}}                                                           \\ 
\textbf{UCFNet\_adapt} & synthetic+TDD(no gt) & \textbf{2.8}                                                    & \textbf{3.1}                                                    & \B{\textbf{20.8}}                                                             & \textbf{3.0} \\\hline
\end{tabular}
}
\label{tab: cross-domain generalization}
	\vspace{-0.1in}
\end{table}





\subsection{Robustness Evaluation}
\label{Sec. Robustness Evaluation}
% As mentioned before, we define robustness as the network’s ability to perform well on a variety of datasets with different characteristics. Such generalization is essential for current methods, which are limited to specific domains and cannot get comparable results on other datasets. This is also the goal of Robust Vision Challenge 2020. Towards this end, we evaluate methods' robustness by their performance on three real datasets (KITTI, ETH3D, and Middlebury) without finetuning. 

% The top six methods in Robust Vision Challenge 2020 are listed in \autoref{tab: robust challenge}. It can be seen from this table that HSMNet\_RVC \cite{hsm} ranks first on the Middlebury dataset. But it can’t get comparable results on the other two datasets (3rd on ETH3D 2017 and 6th on KITTI 2015). In particular, its performance on KITTI 2015 dataset is far worse than the other five. This is because this method is specially designed for high-resolution datasets and can’t generalize well on other datasets. GANet \cite{ganet} is the best open-source method in the KITTI dataset. However, the error rate of D1\_all increased by 28.73\% (from 1.81\% to 2.33\%) after adding training images of the other two datasets and only ranks 4th on KITTI 2015. In addition, it still cannot get a good result on the other two datasets (6th on ETH3D and 7th on Middlebury). The similar situation also appeared on other methods (CVANet\_RVC, NLCANet\_V2\_RVC, and AANet\_RVC). In contrast, our method shows great generalization ability and perform well on all three datasets (2nd on KITTI 2015, 1st on ETH3D 2017, and 2nd on Middlebury 2014) and won first place on the stereo task. We also compare our methods with the top two methods on previous robust vision challenge benchmark. As shown in \autoref{tab: robust challenge}, our approach outperforms Deeppruner and iResNet on all three datasets with a remarkable margin.

% We also notice that some previous work defines robustness as the network’s ability to perform well on unseen scenes.  To further emphasize the effectiveness of our method, we compare our method with some state-of-the-art methods by training on synthetic images and testing on real images. As shown in \autoref{tab: cross-domain generalization}, our method far outperforms domain-specific methods \cite{psmnet,gwcnet,cascade,ganet} on all four datasets. DSMnet \cite{dsmnet} is specially designed for the network’s generalization to unseen scenes. Our method can surpass it on three datasets which further shows our fused and cascade cost volume representation is an efficient approach for robust stereo matching.


In this section, we evaluate our method on three terms of generalization and compare it with state-of-the-art methods in each category.

\noindent \textbf{Cross-domain Generalization:} As the target domain data cannot be easily obtained in many real scenarios, the network’s ability to perform well on unseen scenes is indispensable for robust stereo matching. Towards this end, we evaluate methods' cross-domain generalization by training on synthetic images and testing on real images. As shown in Tab. \ref{tab: cross-domain generalization}, our method far outperforms domain-specific methods \cite{psmnet,gwcnet,cascade,ganet} and our conference version CFNet on all four datasets with a large margin. Specifically, the error rate on KITTI 2012, KITTI 2015, Middlebury, and ETH3D has been decreased by 4.26\%, 10.34\%, 7.80\%, and 17.24\%, respectively compared to CFNet. Moreover, ITAS-CFNet \cite{itsa}, a specially designed stereo matching method developed from our conference version for cross-domain generalization, is the current best-published method. Our method can achieve comparable performance with it on most datasets, which further verifies our uncertainty-based cascade and fused cost volume representation is an efficient approach for robust stereo matching. 

\noindent \textbf{Adaptation Generalization:}  
% In addition, current end-to-end networks only use an image pair as one sample. The lack of labeled training data greatly limits the performance of the end-to-end network in small datasets, e.g., Eth3d and Middlebury. Instead, our method can employ the filtered disparity maps $D_u$ as ground truth to supervise the fine-tuning process of the pre-trained model,  which greatly alleviated the problem of limited ground-truth labels. As a result, as shown in Table \ref{tab: benchmark evaluation}, our method can surpass our jointly-finetuned version or even some dataset-specific supervised approaches in small datasets after adaptation.
Comparing with collecting accurate ground-truth disparities, unlabeled target data is much easier to obtain. Thus, how to employ the knowledge of unlabeled target data adapting pre-trained models to the new domain is also essential. We evaluate such adapt generalization by training on synthetic images and finetuning on unlabeled real images. As shown in Tab. \ref{tab: cross-domain generalization}, although the generalization of our pre-trained model has outperformed most domain generalization methods, using the knowledge of unlabeled target data can still achieve a tremendous gain and surpass all domain generalization methods. Specifically, compared to our pre-trained model UCFNet\_pretrain, UCFNet\_adapt achieves 37.78\%,  40.38\%,  20\%,  37.5\% error reduction on KITTI2012, KITTI2015, Middlebury, and ETH3D, respectively. Moreover, compared to the current best-published domain adaptation method AdaStereo \cite{adastereo, adastereo_ijcv}, our method can still outperform it on three of four datasets, which further proves the effectiveness of the proposed method. Note that our method doesn’t employ the non-adversarial progressive color transfer and cost normalization proposed in AdaStereo, thus, the performance of our method has the potential for further improvement. 

% The Qualitative comparison among GANet, UCFNet\_pretrain, and UCFNet\_adapt on three real datasets is shown in Fig. \ref{fig:  adapt generalization intro}. It can be seen from the figure that the generalization of current dataset-specific methods is limited to unseen real scenes, while our pre-training method can correct most errors and generate a more reasonable result. Moreover, compared with the pre-training model UCFNet\_pretraining, the proposed UCFNet\_adapt can further make tremendous improvements on both textureless area of foreground (red dash boxes)
% and unlabeled area of background (green dash boxes). The visualization results further support our claim that uncertainty-based pseudo-labels generation is an effective way to narrow down the domain gap between the source domain and target domain and we can employ it to improve the estimation accuracy by solely feeding the synchronized stereo images of the target domain, i.e., without the need of ground truth.

\noindent \textbf{Joint Generalization:} Learning-based methods are usually limited to specific domains and cannot get comparable results on other datasets. Thus, the network’s ability to perform well on a variety of datasets with the same model parameters is essential for current methods. This is also the goal of Robust Vision Challenge 2020. Towards this end, we evaluate methods' joint generalization by their performance on three real datasets (KITTI, ETH3D, and Middlebury) without finetuning. We list the result of Robust Vision Challenge 2020 in the upper section of Tab. \ref{tab: robust challenge}. It can be seen from this table that HSMNet\_RVC \cite{hsm} ranks first on the Middlebury dataset. But it can’t get comparable results on the other two datasets (3rd on ETH3D 2017 and 6th on KITTI 2015). In particular, its performance on KITTI 2015 dataset is far worse than the other five. This is because this method is specially designed for high-resolution datasets and can’t generalize well to other datasets. The similar situation also appeared on other methods (GANet\_RVC, CVANet\_RVC, and AANet\_RVC). In contrast, our conference version CFNet\_RVC shows great generalization ability and performs well on all three datasets (2nd on KITTI 2015, 1st on ETH3D 2017, and 2nd on Middlebury 2014) and achieves the best overall performance. Additionally, we further compare the proposed UCFNet\_RVC with the top three methods in the previous Robust Vision Challenge in the lower part of the tab. \ref{tab: robust challenge}. As shown, our approach outperforms Deeppruner\_ROB and iResNet\_ROB on all three datasets with a remarkable margin. Compared with our conference version CFNet\_RVC, the proposed UCFNet\_RVC can achieve similar performance on Middlebury and surpass it on the other two datasets, which further verifies the effectiveness of the proposed refinement module. See corresponding visualization results in Fig. \ref{fig: robust comparison}.

% Figure environment removed

% Visualization results are shown in Fig. \ref{fig: robust comparison}.  It can be seen from the figure that other methods, i.e., HSMNet \cite{hsm} can achieve good performance on one specific dataset but perform poorly on the other two even if they have included targeted domain images in the training process while our method performs well on all three datasets and achieves state-of-the-art overall performance without adaptation. 

\subsection{Results on KITTI Benchmark}
Although our focus is not on domain-specific performance, we still fine-tune our model on KITTI 2012 and KITTI 2015 benchmarks to show the efficiency of our method. Note that the
training strategy is same with our conference paper and the only difference is the proposed attention-based disparity refinement network (please see our conference paper for more details). Specifically, some state-of-the-art real-time methods and best-performing approaches are listed in Tab. \ref{tab: KITTI benchmark}. We find that our method achieves a 1.49\% three-pixel error rate on KITTI2012, a 6\% error reduction from our conference version\cite{cfnet} with a similar running time. Moreover, Lac-GaNet is the best published method on KITTI2012 and the proposed method can achieve comparable performance with 9 times faster speed, i.e., 1.42\% (1.8s) vs 1.49\% (0.21s), which implies the effectiveness and efficiency of the proposed method. A similar situation can also be observed in the KITTI2015 benchmark. 
%Note that GANet-deep, AANet, Deeppruner, and HSMNet all include in the generalization evaluation and cannot achieve a good generalization.

\begin{table}[!htb]
\caption{\footnotesize Results on KITTI benchmark. \textbf{Top:} Comparison with best-performing methods. \textbf{Bottom:} Comparison with real-time methods. All methods are finetuned on specific datasets.}
\vspace{-0.1in}
\centering
\resizebox{0.4\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}KITTI2012\\ 3px(\%)\end{tabular}} & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}KITTI2015\\ D1\_all(\%)\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}time\\  (s)\end{tabular}} \\ \cline{2-5}
                        & Noc                                     & All                                    & Noc                                       & All                                      &                                                                      \\ \hline
LaCGANet \cite{lacnet} & \textbf{1.05}  & \textbf{1.42}    & \textbf{1.49}  & \textbf{1.67}  & 1.8 \\
LEAStereo   \cite{lea}            & 1.13                           & 1.45                          & 1.51                            & 1.65                            & 0.3                                                                  \\
CREStereo \cite{cre}    & 1.14  & 1.46  & 1.54 & 1.69 & 0.41 \\
GANet-deep \cite{ganet}             & 1.19                                    & 1.60                                   & 1.63                                      & 1.81                                     & 1.8                                                                  \\ 
AcfNet \cite{acfnet}                 & 1.17                                    & 1.54                                   & 1.72                                      & 1.89                                     & 0.48                                                                 \\ 
Casstereo \cite{cascade}                & -                                       & -                                      & 1.78                                      & 2.0                                      & 0.6                                                                  \\ \hline
HITNet \cite{hitnet}                  & 1.41                                    & 1.89                                   & 1.74                                      & 1.98                                     & \textbf{0.015}                                                       \\ 
HD\textasciicircum{}3 \cite{hd3}   & 1.40                                    & 1.80                                   & 1.87                                      & 2.02                                     & 0.14                                                                 \\ 
AANet+ \cite{aanet}                  & 1.55                                    & 2.04                                   & 1.85                                      & 2.03                                     & 0.06                                                                 \\ 
HSMNet \cite{hsm}                  & 1.53                                    & 1.99                                   & 1.92                                      & 2.14                                     & 0.14                                                                 \\ 
Deeppruner \cite{deeppruner}              & -                                       & -                                      & 1.95                                      & 2.15                                     & 0.18                                                                 \\ 
\textbf{CFNet(conference version)}    & 1.23                           & 1.58                          &  1.73                                        & 1.88                            & 0.18  \\ \textbf{UCFNet(ours)}    & \textbf{1.12}                           & \textbf{1.49}                          &  \textbf{1.61}                                        & \textbf{1.77}                            & 0.21                                                                 \\ \hline                        
\end{tabular}
}
\label{tab: KITTI benchmark}
	\vspace{0.1in}
% \end{table}

% \begin{table}[!htb]
% \vspace{0.1in}
\caption{\footnotesize Ablation study of attention-based disparity refinement module.  All methods are trained on the Sceneflow dataset with ground truth and tested on the KITTI2015 dataset. $U_{pixel}$ denotes the pixel-level uncertainty estimation result. The approach which is used in our final model is underlined.}
\vspace{-0.1in}
\centering
\resizebox{0.4\textwidth}{!}{
\begin{tabular}{c|c}
\hline
Method                                            & \textbf{D1\_all} \\ \hline
No refinement                                     & 5.8\%            \\ 
{ \ul Refinement with disparity input}                   & \textbf{5.2\%}            \\ 
Refinement with disparity + left image input      & 5.4\%            \\ 
Refinement with disparity + $U_{pixel}$ input  & 5.4\%            \\ 
refinement with disparity + left feature input    & \textbf{5.2\%}            \\ \hline
\end{tabular}
}
\label{tab: abstudy_refinement}
\vspace{0.1in}
% \end{table}

% \begin{table}[!htb]
\caption{ \footnotesize Ablation study results of the proposed network on KITTI 2015 training set. All methods are trained on the Sceneflow dataset with ground truth and fintuned on the unlabeled target domain data. SF and K denotes Sceneflow and kitti dataset, respectively. $D_{pixel}$ and $D_{area}$ are the pixel-level proxy label and area-level proxy label. The iteration number is set to one.}
\vspace{-0.1in}
\centering
%\resizebox{\textwidth}{!}
{
\begin{tabular}{c|c|c|c}
\hline
Backbone      & Supervision & Proxy label & D1\_all \\ \hline
CFNet         & SF                   & -           & 5.8     \\ 
CFNet         & SF+K(no gt)                 & $D_{pixel}$          & 4.5     \\ 
CFNet         & SF+K(no gt)                 & $D_{area}$   & \textbf{3.88}    \\ \hline
UCFNet & SF                   & -           & 5.2     \\ 
UCFNet & SF+K(no gt)                  & $D_{pixel}$          & 3.89    \\ 
UCFNet & SF+K(no gt)                  & $D_{area}$   & \textbf{3.64}    \\ \hline
\end{tabular}
}
\label{tab: abstudy_pixel_area}
\vspace{-0.1in}
\end{table}

\begin{table*}[!t]
\caption{ \footnotesize Ablation study of different proxy label generation methods. We will filter out all the pixels whose uncertainty is larger than the threshold. All methods are trained on the Sceneflow dataset with ground truth and fintuned on the unlabeled KITTI2015 dataset. $U_{pixel}$ and ${U_{area}}$ are pixel-level uncertainty estimation and area-level uncertainty estimation, respectively. Overlap denotes the overlap percentage between the generated proxy label and the ground truth. The iteration number is set to one.}
\vspace{-0.1in}
\centering
%\resizebox{0.8\textwidth}{!}
{
% \begin{tabular}{cccccc|c}
% \hline
% \multicolumn{6}{c|}{Proxy label generation}                                                                                                                                                   & \multicolumn{1}{l}{Domain adaptation} \\ \hline
% \multicolumn{1}{c|}{Filter}                       & \multicolumn{1}{c|}{threshold} & \multicolumn{1}{c|}{D1\_all(\%)} & \multicolumn{1}{c|}{bad 1.0} & \multicolumn{1}{c|}{density} & overlap & D1\_all                                \\ \hline
% \multicolumn{1}{c|}{\multirow{4}{*}{$U_{pixel}$}} & \multicolumn{1}{c|}{0.7}       & \multicolumn{1}{c|}{0.74}        & \multicolumn{1}{c|}{15.22}   & \multicolumn{1}{c|}{47.62}   & 47.22   & 4.01                                   \\  
% \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{0.8}       & \multicolumn{1}{c|}{1.8}         & \multicolumn{1}{c|}{19.27}   & \multicolumn{1}{c|}{73.54}   & 79.17   & 3.96                                   \\ 
% \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{0.9}       & \multicolumn{1}{c|}{2.5}         & \multicolumn{1}{c|}{21.62}   & \multicolumn{1}{c|}{84.01}   & 89.04   & \textbf{3.89}                          \\  
% \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{1.0}       & \multicolumn{1}{c|}{3.08}        & \multicolumn{1}{c|}{22.9}    & \multicolumn{1}{c|}{88.88}   & 93.95   & 4.17                                   \\ \hline

% \multicolumn{1}{c|}{\multirow{5}{*}{$U_{area}$}}                             & \multicolumn{1}{c|}{0.1}       & \multicolumn{1}{c|}{0.30}        & \multicolumn{1}{c|}{11.25}   & \multicolumn{1}{c|}{24.97}   & 26.55   & 3.76                          \\ 
% \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{0.2}       & \multicolumn{1}{c|}{0.46}        & \multicolumn{1}{c|}{13.78}   & \multicolumn{1}{c|}{45.07}   & 51.29   & \textbf{3.64}                 \\
% \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{0.3}       & \multicolumn{1}{c|}{0.69}        & \multicolumn{1}{c|}{16.09}   & \multicolumn{1}{c|}{59.49}   & 68.19   & 3.78                                   \\
% \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{0.4}       & \multicolumn{1}{c|}{1.00}        & \multicolumn{1}{c|}{18.18}   & \multicolumn{1}{c|}{70.13}   & 79.66   & 3.82  \\
% \multicolumn{1}{c|}{}  & \multicolumn{1}{c|}{0.5}       & \multicolumn{1}{c|}{1.45}        & \multicolumn{1}{c|}{20.08}   & \multicolumn{1}{c|}{78.09}   & 87.42   & 3.91                                   \\  \hline


% \end{tabular}
\begin{tabular}{ccc|ccccc|cl}
\cline{1-9}
\multicolumn{3}{c|}{Pretraining model}                                                                          & \multicolumn{5}{c|}{Proxy label generation}                                                                                                                      & Domain adaptation &  \\ \cline{1-9}
\multicolumn{1}{c|}{Backbone}                & \multicolumn{1}{c|}{D1\_all(\%)}          & density(\%)          & \multicolumn{1}{c|}{Filter}                 & \multicolumn{1}{c|}{threshold} & \multicolumn{1}{c|}{D1\_all(\%)} & \multicolumn{1}{c|}{density(\%)} & overlap(\%) & D1\_all(\%)       &  \\ \cline{1-9}
\multicolumn{1}{c|}{\multirow{9}{*}{UCFNet}} & \multicolumn{1}{c|}{\multirow{9}{*}{5.2}} & \multirow{9}{*}{100} & \multicolumn{1}{c|}{\multirow{4}{*}{$U_{pixel}$}} & \multicolumn{1}{c|}{0.7}       & \multicolumn{1}{c|}{0.74}        & \multicolumn{1}{c|}{47.62}       & 47.22       & 4.01              &  \\ 
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{}                     &                      & \multicolumn{1}{c|}{}                       & \multicolumn{1}{c|}{0.8}       & \multicolumn{1}{c|}{1.8}         & \multicolumn{1}{c|}{73.54}       & 79.17       & 3.96              &  \\ 
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{}                     &                      & \multicolumn{1}{c|}{}                       & \multicolumn{1}{c|}{0.9}       & \multicolumn{1}{c|}{2.5}         & \multicolumn{1}{c|}{84.01}       & 89.04       & \textbf{3.89}     &  \\ 
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{}                     &                      & \multicolumn{1}{c|}{}                       & \multicolumn{1}{c|}{1.0}       & \multicolumn{1}{c|}{3.08}        & \multicolumn{1}{c|}{88.88}       & 93.95       & 4.17              &  \\ \cline{4-9}
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{}                     &                      & \multicolumn{1}{c|}{\multirow{5}{*}{$U_{areal}$}}  & \multicolumn{1}{c|}{0.1}       & \multicolumn{1}{c|}{0.30}        & \multicolumn{1}{c|}{24.97}       & 26.55       & 3.76              &  \\ 
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{}                     &                      & \multicolumn{1}{c|}{}                       & \multicolumn{1}{c|}{0.2}       & \multicolumn{1}{c|}{0.46}        & \multicolumn{1}{c|}{45.07}       & 51.29       & \textbf{3.64}     &  \\ 
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{}                     &                      & \multicolumn{1}{c|}{}                       & \multicolumn{1}{c|}{0.3}       & \multicolumn{1}{c|}{0.69}        & \multicolumn{1}{c|}{59.49}       & 68.19       & 3.78              &  \\ 
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{}                     &                      & \multicolumn{1}{c|}{}                       & \multicolumn{1}{c|}{0.4}       & \multicolumn{1}{c|}{1.00}        & \multicolumn{1}{c|}{70.13}       & 79.66       & 3.82              &  \\ 
\multicolumn{1}{c|}{}                        & \multicolumn{1}{c|}{}                     &                      & \multicolumn{1}{c|}{}                       & \multicolumn{1}{c|}{0.5}       & \multicolumn{1}{c|}{1.45}        & \multicolumn{1}{c|}{78.09}       & 87.42       & 3.91              &  \\ \cline{1-9}
\end{tabular}
}
\label{tab: abstudy_threshold}
\vspace{-0.15in}
\end{table*}

	
%     % Figure removed&
%     % Figure removed&
% 	% Figure removed&
% 	% Figure removed\\

%     % Figure removed&
%     % Figure removed&
% 	% Figure removed&
% 	% Figure removed\\

% 	{(a) Left image} &  {(b) CFNet} &	{(c) GANet }	&  {(d) HSMNet }	  	\\
% 	\end{tabular}
	
% 	\caption{Visualization of some state-of-the-art methods’ performance on three real-world dataset testsets (from top to bottom: KITTI, Middlebury, and ETH3D). All methods are trained with a combination of KITTI, Middlebury, and ETH3D train images. GANet \cite{ganet} and HSMNet \cite{hsm} can achieve good performance on one specific dataset but perform poorly on the other two datasets even if they have included targeted domain images in the training process. Our CFNet achieves SOTA or near SOTA performance on all three datasets without any adaptation.}
% 	\label{fig: joint generalization}
% \end{figure*}

\begin{table}[!htb]
\caption{ \footnotesize Ablation study of different iteration numbers. All methods are trained on the Sceneflow dataset with ground truth and fintuned on the unlabeled KITTI2015 dataset. $U_{area}$ denotes the area-level uncertainty estimation.}
\vspace{-0.1in}
\centering
%\resizebox{0.49\textwidth}{!}
{
\begin{tabular}{c|c|c|c}
\hline
Backbone                         & Filter                       & iteration & D1\_all \\ \hline
% \multirow{8}{*}{UCFnet} & \multirow{4}{*}{$U_{pixel}$} & 0         & 5.2     \\  
%                               &                              & 1         & 3.89    \\  
%                               &                              & 2         & 3.91    \\  
%                               &                              & 3         & -       \\ \cline{2-4} 
\multirow{4}{*}{UCFnet}                               & \multirow{4}{*}{$U_{area}$}  & 0         & 5.2     \\ 
                               &                              & 1         & 3.64    \\ 
                               &                              & 2         & 3.14    \\  
                               &                              & 3         & 3.16    \\ \hline
\end{tabular}
}
\label{tab: iteration}
\vspace{0.1in}
% \end{table}

% \begin{table}[!t]
% \vspace{0.1in}
\caption{ \footnotesize Ablation study of area-level uncertainty estimation network module.  All methods are trained on the Sceneflow dataset with ground truth and tested on the KITTI2015 dataset. Density denotes the percentage of valid pixels in the generated proxy label. The approach which is used in our final model is underlined.}
\vspace{-0.1in}
\centering
\resizebox{0.44\textwidth}{!}{
\begin{tabular}{c|ccc}
\hline
\multirow{2}{*}{Method}                                           & \multicolumn{3}{c}{Kitti}                                               \\ \cline{2-4} 
& \multicolumn{1}{c|}{D1\_all} & \multicolumn{1}{c|}{density} & overlap \\  \hline
{\ul Multi-modal input}                         & \multicolumn{1}{c|}{\textbf{0.46\%}}  & \multicolumn{1}{c|}{45.07\%}    & 51.29\% \\ 
Multi-modal input without uncertainty map  & \multicolumn{1}{c|}{0.81\%}  & \multicolumn{1}{c|}{48.16\%}    & 55.56\% \\ 
Multi-modal input without disp            & \multicolumn{1}{c|}{0.73\%}  & \multicolumn{1}{c|}{49.02\%}    & 56.83\% \\ 
Multi-modal input without left image     & \multicolumn{1}{c|}{0.55\%}  & \multicolumn{1}{c|}{45.30\%}    & 49.54\% \\ \hline
\end{tabular}
}
\label{tab: abstudy_area-level}
\vspace{-0.15in}
\end{table}

% \begin{table*}[!htb]
% \caption{ \small Ablation study of different proxy label generation methods. We will filter out all the pixels whose uncertainty is larger than the threshold. All methods are trained on the Sceneflow dataset with ground truth and fintuned on the unlabeled KITTI2015 dataset. $U_{pixel}$ and ${U_area}$ are pixel-level uncertainty estimation and area-level uncertainty estimation, respectively. Overlap denotes the overlap percentage between the generated proxy label and the ground truth.}
% \centering
% %\resizebox{0.8\textwidth}{!}
% {
% \begin{tabular}{cccccc|c}
% \hline
% \multicolumn{6}{c|}{Proxy label generation}                                                                                                                                                   & \multicolumn{1}{l}{Domain adaptation} \\ \hline
% \multicolumn{1}{c|}{Filter}                       & \multicolumn{1}{c|}{threshold} & \multicolumn{1}{c|}{D1\_all(\%)} & \multicolumn{1}{c|}{bad 1.0} & \multicolumn{1}{c|}{density} & overlap & D1\_all                                \\ \hline
% \multicolumn{1}{c|}{\multirow{4}{*}{$U_{pixel}$}} & \multicolumn{1}{c|}{0.7}       & \multicolumn{1}{c|}{0.74}        & \multicolumn{1}{c|}{15.22}   & \multicolumn{1}{c|}{47.62}   & 47.22   & 4.01                                   \\  
% \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{0.8}       & \multicolumn{1}{c|}{1.8}         & \multicolumn{1}{c|}{19.27}   & \multicolumn{1}{c|}{73.54}   & 79.17   & 3.96                                   \\ 
% \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{0.9}       & \multicolumn{1}{c|}{2.5}         & \multicolumn{1}{c|}{21.62}   & \multicolumn{1}{c|}{84.01}   & 89.04   & \textbf{3.89}                          \\  
% \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{1.0}       & \multicolumn{1}{c|}{3.08}        & \multicolumn{1}{c|}{22.9}    & \multicolumn{1}{c|}{88.88}   & 93.95   & 4.17                                   \\ \hline

% \multicolumn{1}{c|}{\multirow{5}{*}{$U_{area}$}}                             & \multicolumn{1}{c|}{0.1}       & \multicolumn{1}{c|}{0.30}        & \multicolumn{1}{c|}{11.25}   & \multicolumn{1}{c|}{24.97}   & 26.55   & 3.76                          \\ 
% \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{0.2}       & \multicolumn{1}{c|}{0.46}        & \multicolumn{1}{c|}{13.78}   & \multicolumn{1}{c|}{45.07}   & 51.29   & \textbf{3.64}                 \\
% \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{0.3}       & \multicolumn{1}{c|}{0.69}        & \multicolumn{1}{c|}{16.09}   & \multicolumn{1}{c|}{59.49}   & 68.19   & 3.78                                   \\
% \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{0.4}       & \multicolumn{1}{c|}{1.00}        & \multicolumn{1}{c|}{18.18}   & \multicolumn{1}{c|}{70.13}   & 79.66   & 3.82  \\
% \multicolumn{1}{c|}{}  & \multicolumn{1}{c|}{0.5}       & \multicolumn{1}{c|}{1.45}        & \multicolumn{1}{c|}{20.08}   & \multicolumn{1}{c|}{78.09}   & 87.42   & 3.91                                   \\  \hline


% \end{tabular}
% }
% \label{tab: abstudy_threshold}
% \end{table*}


\subsection{Ablation Study}
To verify the effectiveness of different modules, we set a series of experiments in this section. Note that all methods are trained on the synthetic dataset first and then finetuned on the unlabeled target dataset with the proposed pseudo-labels. Generally, seven types of experiments have been executed here.

\noindent \textbf{Attention-based refinement module:} In the attention-based disparity refinement module, we only employ initial disparity estimation as input. Here, we test the impact of adding multi-modal input, i.e., left image, left image feature, and pixel-level uncertainty map. As shown in Tab. \ref{tab: abstudy_refinement}, adding multi-modal input doesn’t bring noticeable gain and we can obtain the best performance by only using initial disparity estimation as input.

\noindent \textbf{Domain adaptation with self-generated pseudo-label:} Two terms of proxy labels, i.e., ${D_{pixel}}$ and ${D_{area}}$ can be generated by pixel-level uncertainty estimation and area-level uncertainty estimation, respectively. Here, we test the impact of each proxy label for domain adaptation individually. As shown in the Tab. \ref{tab: abstudy_pixel_area}, the two terms of proxy labels can both promote the performance of the pre-training model on the target dataset and the improvement is consistent on both CFNet and UCFNet, which verifies the effectiveness of the proposed uncertainty-based pseudo-labels generation method. Moreover, the proposed area-level proxy label can achieve a larger gain due to the leveraging of multi-modal input and neighboring pixel information, e.g., the d1\_all error rate of CFNet can further decrease from 4.5\% to 3.88\% after employing ${D_{area}}$ as supervision.

% \begin{table}[!htb]
% \caption{ \footnotesize Ablation study of area-level uncertainty estimation network module.  Density denotes the percentage of valid pixels in the generated proxy label and overlap denotes the overlap percentage between the generated proxy label and the ground truth. The approach which is used in our final model is underlined.}
% \centering
% \resizebox{0.49\textwidth}{!}{
% \begin{tabular}{c|ccc|ccc}
% \hline
% \multirow{2}{*}{Method}                   & \multicolumn{3}{c|}{Sceneflow}                                           & \multicolumn{3}{c}{Kitti}                                               \\ \cline{2-7} 
%                                           & \multicolumn{1}{c|}{D1\_all} & \multicolumn{1}{c|}{density} & overlap & \multicolumn{1}{c|}{D1\_all} & \multicolumn{1}{c|}{dense rate} & overlap \\  \hline
% {\ul Multi-modal input}                         & \multicolumn{1}{c|}{\textbf{0.17\%}}  & \multicolumn{1}{c|}{85.70\%}     & 86.09\% & \multicolumn{1}{c|}{\textbf{0.46\%}}  & \multicolumn{1}{c|}{45.07\%}    & 51.29\% \\ 
% Multi-modal input without uncertainty map & \multicolumn{1}{c|}{0.25\%}  & \multicolumn{1}{c|}{84.24\%}     & 84.73\% & \multicolumn{1}{c|}{0.81\%}  & \multicolumn{1}{c|}{48.16\%}    & 55.56\% \\ 
% Multi-modal input without disp            & \multicolumn{1}{c|}{0.18\%}  & \multicolumn{1}{c|}{82.45\%}     & 82.93\%  & \multicolumn{1}{c|}{0.73\%}  & \multicolumn{1}{c|}{49.02\%}    & 56.83\% \\ 
% Multi-modal input without left image      & \multicolumn{1}{c|}{0.18\%}  & \multicolumn{1}{c|}{83.50\%}    & 83.98\% & \multicolumn{1}{c|}{0.55\%}  & \multicolumn{1}{c|}{45.30\%}    & 49.54\% \\ \hline
% \end{tabular}
% }
% \label{tab: abstudy_area-level}
% \end{table}

\noindent \textbf{Threshold of proxy label generation:} Threshold $t$ is an essential hyperparameter in proxy label generation, which controls the density and reliability of the filtered proxy labels. Hence, we made a detailed ablation study to analyze the impact of different threshold settings for domain adaptation. As shown in Tab. \ref{tab: abstudy_threshold}, we show the tradeoff between accuracy and density of generated proxy labels in different threshold settings and the corresponding domain adaptation results. It can be seen from the table that the proposed uncertainty estimation can effectively evaluate the confidence of current disparity estimations, e.g., by removing 21.91\% of uncertain pixels ($U_{area}>0.5$), we decrease the D1\_all error rate by 72.12\% (from 5.2\% to 1.45\% in KITTI 2015 training set). Moreover, we can find that a more accurate while sparser pseudo-label is not always good for final domain adaptation performance and we should seek a suitable balance between accuracy and density, i.e., threshold 0.9 for pixel-level uncertainty estimation and threshold 0.2 for area-level uncertainty estimation in our experiment setting.

\noindent \textbf{Pixel-level uncertainty estimation vs area-level uncertainty estimation:}
Inspired by the standard evaluation metric in confidence estimation \cite{confidencesurvey, confidencesurvey2}, we propose to use the ROC curve and its area under curve (AUC) to quantitatively evaluate the performance of the proposed pixel-level and area-level uncertainty estimation. Specifically, we firstly sort pixels in the predicted disparity map following decreasing order of uncertainty. Then, we compute the D1\_all error rate on sparse maps obtained by iterative filtering (e.g., 5\% of pixels with higher uncertainty each time) from the dense map and plot the ROC curve, whose AUC quantitatively assesses the confidence effectiveness(the lower, the better). The ROC curve of the proposed method is shown in Fig. \ref{fig: roc_compare}. Note that we also plot the roc curve of the traditional uncertainty estimation method, i.e., image-level and feature-level left-right consistency check \cite{mcvmfc, crl} to further show the effectiveness of the proposed method. As shown in Fig. \ref{fig: roc_compare}, the proposed pixel-level and area-level uncertainty estimation can both generate a more accurate disparity map than the traditional left-right consistency check at any density and the area-level uncertainty estimation can achieve the best performance. Visualization of generated proxy labels is shown in Fig. \ref{fig: uncertainty estimation}. As shown, the generated two terms of uncertainty map (sub-figs (d) and (g)) are highly correlated with the error map. Hence, we can employ the proposed uncertainty estimation to filter out the high-uncertainty pixels of the original estimation and generate reliable pseudo labels. Moreover, the comparison between sub-figs (e) and (h) further shows the superiority of the proposed area-level uncertainty estimation. As shown, our area-level uncertainty estimation ${D_{area}}$ can employ the neighboring pixel information to better preserve the instance-level correct disparity estimation result, e.g., the pedestrian and pole of the input image.

\noindent \textbf{Iteration number of domain adaptation:} Our result can further be improved by iterative domain adaptation. Take two times of iteration as an example. At iteration 1, we employ the proxy label generated by the pre-training model to finetune the pre-training model. Then, at iteration 2, we can further finetune the pre-training model by employing the finetuned model to generate better proxy labels. More specifically, the generated pseudo-labels are employed to adapt the main network, refinement module, and area-level uncertainty estimation network one by one in each iteration. It can be seen from Tab. \ref{tab: iteration} that iterative domain adaptation can significantly improve the performance of disparity estimation on the target dataset, e.g., employing two iterations can decrease the D1\_all error rate from 3.64\% to 3.14\%. Note that, using more iterations cannot further improve estimation accuracy due to the optimization of pseudo-label has an upper limit by iterative domain adaptation.       

% Figure environment removed


% Figure environment removed

\begin{table}[!t]
\caption{\footnotesize \textbf{Top}: Cross-domain generalization evaluation on ETH3D, and KITTI training sets. All methods are only trained on the Scene Flow datatest and tested on full-resolution training images of two real datasets. \textbf{Bottom}: Adaptation generalization evaluation on ETH3D, and KITTI training sets. TDD: target domain data. All methods are finetuned on the unlabeled target domain data. 
% \textbf{Bottom}: Joint generalization evaluation on ETH3D and KITTI testing sets. All methods are
% tested on two datasets with one final model. 
* denotes not using additional data augmentation. We highlight the best result in \textbf{bold} and the second-best result in \B{blue} for each column.}
\vspace{-0.1in}
\centering
\resizebox{0.4\textwidth}{!}{
\begin{tabular}{c|c|c|c}
\hline
Method       & Training Set          & \begin{tabular}[c]{@{}c@{}}KITTI2015\\ D1\_all(\%)\end{tabular}  & \begin{tabular}[c]{@{}c@{}}ETH3D\\ bad 1.0(\%)\end{tabular} \\ \hline
\multicolumn{4}{c}{Cross-domain Generalization Evaluation}                                                                                                                                                                                                                                                        \\ \hline
% GWCNet* \cite{gwcnet}      & synthetic                                                                        & 12.2                                                                                                                         & 11.0                                                        \\ 
CasStereo* \cite{cascade}    & synthetic                                                                        & 11.9                                                                                                                         & 7.8                                                         \\ 
% GANet* \cite{ganet}       & synthetic                                                                     & 11.7                                                                                                                        & 14.1                                                        \\ 
DSMNet* \cite{dsmnet}       & synthetic                                                                      & 6.5                                                                                                                 & 6.2                                                         \\
ITSA-CFNet \cite{itsa}       & synthetic                                                                         & \textbf{4.7}                                                                                                               & \B{\textbf{5.1}}                                                         \\ 
% Graft-Ganet \cite{graftnet}       & synthetic            & \textbf{4.2}                                                             & 4.9                                                             & \textbf{-/9.8}                                                    & 6.2                                                         \\
% FC-Ganet \cite{fcnet}       & synthetic            & 4.6                                                             & 5.3                                                             & \textbf{-/10.2}                                                    & 5.8                                                         \\ 
\textbf{CFNet(ours)\cite{cfnet}}       & synthetic                                                              & 5.8                                                                                                               & 5.8                                                \\
\textbf{UCFNet\_pretrain*}       & synthetic                                                               & 7.8                                                                                                                & 5.9   \\
\textbf{UCFNet\_pretrain}       & synthetic                                                               & \B{\textbf{5.2}}                                                                                                                & \textbf{\textbf{4.8}}                                                \\ \hline
\multicolumn{4}{c}{Adaptation Generalization Evaluation}                                                                                                                                                                                                                                                          \\ \hline
% AOHNet* \cite{aohnet}       & synthetic+TDD(no gt)                                                              & 7.8                                                                                                                           & -                                                           \\ 
% MADNet* \cite{madnet}      & synthetic+TDD(no gt)                                                             & 8.5                                                                                                                            & -                                                           \\
AdaStereo \cite{adastereo,adastereo_ijcv}      & synthetic+TDD(no gt)                                                             & \B{\textbf{3.5}}                                                                                                                         & 4.1                                                           \\
\textbf{UCFNet\_adapt*} & synthetic+TDD(no gt)                                                     & 3.6                                                                                                                & \B{\textbf{4.0}} \\
\textbf{UCFNet\_adapt} & synthetic+TDD(no gt)                                                     & \textbf{3.1}                                                                                                                & \textbf{3.0} \\ \hline
% \multicolumn{4}{c}{Joint Generalization Evaluation} 
% \\ \hline
% NLCANet\_V2\_RVC \cite{nlcanet} & synthetic+TDD & 1.92 & 4.11 \\
% \textbf{CFNet\_RVC} \cite{cfnet} & synthetic+TDD & 1.96 & 3.7 \\
% \textbf{UCFNet\_RVC*} & synthetic+TDD & \B{\textbf{1.92}} & \B{\textbf{3.44}} \\
% \textbf{UCFNet\_RVC} & synthetic+TDD & \textbf{1.86} & \textbf{3.37} \\ \hline
\end{tabular}
}
\label{tab: data_augmentation}
	\vspace{-0.1in}
\end{table}
\noindent \textbf{Area-level uncertainty estimation:} In the area-level uncertainty estimation module, we employ multi-model input, i.e., pixel-level uncertainty map, initial disparity map, and left image to drive our network better evaluate the uncertainty of current disparity estimation. Here, we test the impact of each input individually. All methods are trained on the scene flow training dataset and then tested on the unseen kitti2015 dataset. As shown in Tab. \ref{tab: abstudy_area-level}, the result verifies all the multi-modal inputs work positively to filter out matching-error points and compared with other inputs, the pixel-level uncertainty map achieves the largest gain.

% Figure environment removed

\noindent \textbf{Data argumentation:} We also test the influence of removing additional data augmentation, e.g., asymmetric chromatic augmentation and occlusion on two real datasets. As shown in Tab. \ref{tab: data_augmentation}, the proposed method can still achieve a comparable result with the state-of-the-art method on both two terms of generalization even without data augmentation. For example, AdaStereo is the best-published domain adaptation method and indeed uses additional data augmentation. However, the proposed no data augmentation version UCFNet\_adapt* can still obtain comparable results on both ETH3D and KITTI datasets. Moreover, data augmentation works positively to improve the performance on all evaluation metrics and we recommend deploying it in the final model.

Additionally, as our original cascade and fused cost volume representation (CFNet) is mainly designed for joint generalization, we also perform various ablation studies to show the effectiveness of each network design in joint generalization evaluation. We divide 5 images from each real dataset (KITTI 2015, Middlebury, and ETH3D) as a validation set and use the rest of them as a training set to finetune our pretrain model. Results are shown in Tab. \ref{tab: ablation study}. Below we describe each component in more detail.



\noindent \textbf{Feature extraction:} We compare our pyramid feature extraction with the most widely used Resnet-like-network \cite{cascade,gwcnet}. As shown, our pyramid feature extraction can achieve similar performance with a faster speed, likely because the employing of small scale features is also helpful in feature extraction.

\noindent \textbf{Cost volume fusion:} We fuse three small-resolution cost volumes to generate the initial disparity map. Here, we test the impact when only a single volume is used. Cost volume fusion can achieve better performance with a slight additional computational cost.

\noindent \textbf{Cost volume cascade:} We test three ways of generating the next stage’s disparity searching space in cascade cost volume representation. As shown, learned parameters based pixel-level uncertainty estimation achieves the best performance with tiny additional computation complexity.

% To further emphasize the effectiveness of our uncertainty estimation, we visualize the error map and uncertainty map in Figure \ref{fig: ue visual}. As shown, the error map is highly correlated with the uncertainty map. Experimentally, by removing 1\% of uncertain pixels ($\sqrt U  >  = 2.5$), we decrease the D1\_all error rate by 29.68\% (from 1.55\% to 1.09\% in KITTI 2015 validation set). Similar situations can be observed on the other two datasets. Thus, it is reasonable to employ uncertainty estimation to evaluate the pixel-level confidence of disparity estimation. 

% \noindent \textbf{Loss weight:} The N-stages model outputs N disparity maps. We test three different settings of loss weight in our three-stage model. Experiments show that later stages shall set a larger loss weight which matches our intuition.

\noindent \textbf{Finetuning strategy:} We test three terms of finetuning strategy. As shown,  neither directly augmenting small datasets at the beginning (two-stages) nor only extending the number of iterations (three stages\_no augment) can improve the accuracy of predictions on small datasets. Instead, our strategy can greatly alleviate the problem of small datasets being overwhelmed by large ones.

\renewcommand\arraystretch{1.3}
\begin{table}[!t]
% \vspace{-0.1in}
\centering
\caption{\footnotesize Ablation study results of the proposed network on KITTI 2015, Middlebury, and ETH3D validation set. PUE: pixel-level uncertainty estimation. We test a component of our method individually in each section of the table and the approach which is used in our final model (UCFNet without the refinement module) is underlined. Time is measured on the KITTI dataset by a single Tesla V100 GPU.}
\vspace{-0.1in}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c}
\hline
Experiment                           & Method                           & \begin{tabular}[c]{@{}c@{}}KITTI\\ D1\_all\end{tabular} & \begin{tabular}[c]{@{}c@{}}Middlebury\\ bad 2.0\end{tabular} & \begin{tabular}[c]{@{}c@{}}ETH3D\\ bad 1.0\end{tabular} & \begin{tabular}[c]{@{}c@{}}time\\ (s)\end{tabular} \\ \hline
\multirow{2}{*}{Feature Extraction}  & Resnet-like-network              & 1.76                                                    & 22.81                                                         & \textbf{3.49}                                           & 0.270                                              \\  
                                     & \underline{Pyramid Feature Extraction} & \textbf{1.71}                                           & \textbf{22.27}                                               & 3.57                                                    & \textbf{0.225}                                     \\ \hline
\multirow{2}{*}{Cost Volume Fusion}  & Not Fuse                         & 1.79                                                    & 22.65                                                        & 3.67                                                    & \textbf{0.220}                                     \\  
                                     & \underline{Fuse}                       & \textbf{1.71}                                           & \textbf{22.27}                                               & \textbf{3.57}                                           & 0.225                                              \\ \hline
\multirow{3}{*}{Cost Volume Cascade} & Uniform Sample                   & 1.92                                                    & 23.8                                                         & 3.97                                                    & 0.225                                              \\  
                                     & PUE + Hyperparameters             & 1.78                                                    & 23.13                                                        & 3.83                                                    & 0.225                                              \\  
                                     & \underline{PUE + Learned Parameters}    & \textbf{1.71}                                           & \textbf{22.27}                                               & \textbf{3.57}                                           & \textbf{0.225}                                     \\ \hline
% \multirow{3}{*}{Loss weight}         & Loss1:1,Loss2:1,Loss3:1          & 1.84                                                    & 23.87                                                        & \textbf{3.47}                                           & 0.225                                              \\ 
%                                      & Loss1:1,Loss2:0.7,Loss3:0.5      & 1.79                                                    & 22.97                                                        & 3.62                                                    & 0.225                                              \\  
%                                      & \underline{Loss1:2,Loss2:1,Loss3:0.5}  & \textbf{1.71}                                           & \textbf{22.27}                                               & 3.57                                                    & \textbf{0.225}                                     \\ \hline
  \multirow{3}{*}{Fine-tuning strategy} & two stages               & \textbf{1.70}                                                    & 22.77                                                        & 3.99                                           & 0.225                                              \\ 
                                      & three stages\_no augment & \textbf{1.70}                                                    & 22.57                                                        & 3.92                                                    & 0.225                                              \\  
                                      & \underline {three stages}       & 1.71                                           & \textbf{22.27}                                               & \textbf{3.57}                                                    & \textbf{0.225}                                     \\ \hline
\end{tabular}
}
\label{tab: ablation study}
\vspace{-0.1in}
\end{table}

\renewcommand\arraystretch{1}

\subsection{Extreme Situation}
As mentioned in Sec. \ref{Sec. Robustness Evaluation}, our pre-training model has strong cross-domain generalization and can generate relatively reasonable results for the subsequent pseudo-label generation. However, it still cannot work in some extreme situations, e.g., pictures taken inside a tunnel. As shown in Fig. \ref{fig: extreme situation}, we give some extreme cases, in which our pre-training method predicts a totally wrong result. As for such cases, the proposed pixel-level uncertainty estimation (sub-figs (f)) method is difficult to filter out all errors in the disparity estimation while area-level uncertainty estimation (sub-figs (i)) can achieve this goal and generate a disparity map without valid pixels, making the noisy labels don’t affect subsequent domain adaptation. Moreover, the proposed UCF\_adapt can indeed predict a reasonable result in such extreme situations (sub-figs (m)) even if we don’t have corresponding valid proxy labels for these extreme cases, which further verifies the effectiveness of the proposed domain adaptation method.

% \renewcommand\arraystretch{1.3}
% \begin{table}[!htb]
% \centering
% \resizebox{0.48\textwidth}{!}{
% \begin{tabular}{cccc}
% \hline
% Experiment                           & Method                           & \begin{tabular}[c]{@{}c@{}}Sceneflow\\ EPE(\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}time\\ (s)\end{tabular} \\ \hline
% \multirow{2}{*}{Feature Extraction}  & Resnet-like-network              & 1.05                                                        & 0.278                                              \\  
%                                      & \underline {Pyramid Feature Extraction} & \textbf{1.04}                                               & \textbf{0.234}                                              \\ \hline
% \multirow{2}{*}{Cost Volume Fusion}  & Not Fuse                         & 1.07                                                        & \textbf{0.229}                                              \\ 
%                                      & \underline {Fuse}                       & \textbf{1.04}                                               & 0.234                                              \\ \hline
% \multirow{3}{*}{Cost Volume Cascade} & Uniform Sample                   & 1.10                                                        & 0.234                                              \\ 
%                                      & UE + Hyperparameters             & 1.08                                                        & 0.234                                              \\  
%                                      & \underline {UE + Learned Parameters}    & \textbf{1.04}                                               & \textbf{0.234}                                              \\ \hline
% \multirow{3}{*}{Loss weight}         & Loss1:1,Loss2:1,Loss3:1          & 1.08                                                        & 0.234                                              \\ 
%                                      & Loss1:1,Loss2:0.7,Loss3:0.5      & 1.07                                                        & 0.234                                              \\  
%                                      & \underline {Loss1:2,Loss2:1,Loss3:0.5}  & \textbf{1.04}                                               & \textbf{0.234}                                              \\ \hline
% \end{tabular}
% }
% \caption{Ablation study results of the proposed network on the SceneFlow dataset. UE: uncertainty estimation. Loss i: the loss weight at stage i. We test a component of our method individually in each section of the table and the approach which is used in our final model is underlined. The time is measured on a single Tesla V100 GPU.}
% \label{tab: ablation study}
% \end{table}



% \begin{table*}[!htb]
% \caption{Quantitative evaluations on the kitti dataset using the test split of Eigen et al. * denotes the performance is evaluated using the official annotated ground truth (default: using raw velodyne data). Note that we use the cropping strategy introduced by Garg et al.}
% \centering
% %\resizebox{0.95\textwidth}{!}
% {
% \begin{tabular}{c|c|ccc|cccc}
% \hline
% \multirow{2}{*}{Method} & \multirow{2}{*}{GT} & \multicolumn{1}{c|}{$\delta  < {1.25}$} & \multicolumn{1}{c|}{$\delta  < {1.25^2}$} & $\delta  < {1.25^3}$ & \multicolumn{1}{c|}{Abs Rel} & \multicolumn{1}{c|}{Sq Rel} & \multicolumn{1}{c|}{RMSE}  & RMSE log \\ \cline{3-9} 
%                   &     & \multicolumn{3}{c|}{Higher value is better}                                                        & \multicolumn{4}{c}{Lower value is better}                                                         \\ \hline
                        
% {Godard \textit{ et al.}\cite{godard2017unsupervised}}        & N & \multicolumn{1}{c|}{0.861}            & \multicolumn{1}{c|}{0.949}             & 0.976             & \multicolumn{1}{c|}{0.114}   & \multicolumn{1}{c|}{0.898}  & \multicolumn{1}{c|}{4.935} & 0.206    \\
% {Kuznietsov \textit{ et al.}  \cite{Kuznietsov}}                & N & \multicolumn{1}{c|}{0.862}            & \multicolumn{1}{c|}{0.960}             & 0.986             & \multicolumn{1}{c|}{0.113}   & \multicolumn{1}{c|}{0.741}  & \multicolumn{1}{c|}{4.621} & 0.189    \\
% {Monodepth2~\cite{monodepth2_iccv2019}} & N &\multicolumn{1}{c|}{0.876} & \multicolumn{1}{c|}{0.957} & 0.980 & \multicolumn{1}{c|}{0.106} & \multicolumn{1}{c|}{0.806} & \multicolumn{1}{c|}{4.630} & 0.193  \\
% {Monodepth2 dh~\cite{depth_hint_iccv2019}} & N &\multicolumn{1}{c|}{0.888} & \multicolumn{1}{c|}{0.962} & 0.982 & \multicolumn{1}{c|}{0.100} & \multicolumn{1}{c|}{0.757} & \multicolumn{1}{c|}{4.490} & 0.185 \\
% {MLDA-Net~\cite{song2021mlda}} & N &\multicolumn{1}{c|}{0.887} & \multicolumn{1}{c|}{0.963} & 0.983 & \multicolumn{1}{c|}{0.099} & \multicolumn{1}{c|}{0.724} & \multicolumn{1}{c|}{4.415} & 0.183 \\ \hline                        

% {Eigen\textit{ et al.} \cite{eigen}}                & Y & \multicolumn{1}{c|}{0.692}            & \multicolumn{1}{c|}{0.899}             & 0.967             & \multicolumn{1}{c|}{0.190}   & \multicolumn{1}{c|}{1.515}  & \multicolumn{1}{c|}{7.156} & 0.270    \\
% {Liu \textit{ et al.}\cite{liu}}             &   Y  & \multicolumn{1}{c|}{0.647}            & \multicolumn{1}{c|}{0.882}             & 0.961             & \multicolumn{1}{c|}{0.217}   & \multicolumn{1}{c|}{1.841}  & \multicolumn{1}{c|}{6.986} & 0.289    \\


% {Gan et al \textit{ et al.}   \cite{gan}}              & Y & \multicolumn{1}{c|}{0.890}            & \multicolumn{1}{c|}{0.964}             & 0.985             & \multicolumn{1}{c|}{0.098}   & \multicolumn{1}{c|}{0.666}  & \multicolumn{1}{c|}{3.933} & 0.173    \\
% {DORN     \cite{dorn}}           & Y & \multicolumn{1}{c|}{0.897}            & \multicolumn{1}{c|}{0.966}             & 0.986             & \multicolumn{1}{c|}{0.099}   & \multicolumn{1}{c|}{0.593}  & \multicolumn{1}{c|}{3.714} & 0.161    \\ \hline
% % Lee et al                  & \multicolumn{1}{c|}{0.904}            & \multicolumn{1}{c|}{0.967}             & 0.984             & \multicolumn{1}{c|}{0.091}   & \multicolumn{1}{c|}{0.555}  & \multicolumn{1}{c|}{4.033} & 0.174    \\ \hline
% %bts\_wetrain~\cite{song2021monocular}                  & \multicolumn{1}{c|}{0.913}            & \multicolumn{1}{c|}{0.970}             & 0.985             & \multicolumn{1}{c|}{0.082}   & \multicolumn{1}{c|}{0.484}  & \multicolumn{1}{c|}{3.694} & 0.166 \\
% {Bts~\cite{bts}}                & Y & \multicolumn{1}{c|}{0.913}            & \multicolumn{1}{c|}{0.970}             & 0.985             & \multicolumn{1}{c|}{0.082}   & \multicolumn{1}{c|}{0.484}  & \multicolumn{1}{c|}{3.694} & 0.166 \\
% % {{\rowcolor{gbygreen}}}{bts\_pseudo\_label}                & N  & \multicolumn{1}{c|}{0.909}            & \multicolumn{1}{c|}{0.970}             & 0.986             & \multicolumn{1}{c|}{0.087}   & \multicolumn{1}{c|}{0.526}  & \multicolumn{1}{c|}{3.716} & 0.166 \\
% {bts\_pseudo\_label}                & N  & \multicolumn{1}{c|}{0.910}            & \multicolumn{1}{c|}{0.970}             & 0.986             & \multicolumn{1}{c|}{0.094}   & \multicolumn{1}{c|}{0.554}  & \multicolumn{1}{c|}{3.730} & 0.167 \\
% % {{\rowcolor{gbygreen}}}{bts\_pseudo\_label\_full}              &   N  & \multicolumn{1}{c|}{0.921}            & \multicolumn{1}{c|}{0.973}             & 0.987             & \multicolumn{1}{c|}{0.084}   & \multicolumn{1}{c|}{0.476}  & \multicolumn{1}{c|}{3.424} & 0.157 \\ \hline
% {bts\_pseudo\_label\_full}              &   N  & \multicolumn{1}{c|}{\textbf{0.939}}            & \multicolumn{1}{c|}{\textbf{0.975}}             & \textbf{0.987}             & \multicolumn{1}{c|}{\textbf{0.072}}   & \multicolumn{1}{c|}{\textbf{0.454}}  & \multicolumn{1}{c|}{\textbf{3.222}} &\textbf{ 0.147} \\ \hline
% %lapdepth\_wetrain~\cite{song2021monocular}                   & \multicolumn{1}{c|}{0.915}            & \multicolumn{1}{c|}{0.970}             & 0.985             & \multicolumn{1}{c|}{0.083}   & \multicolumn{1}{c|}{0.481}  & \multicolumn{1}{c|}{3.658} & 0.165 
% {lapdepth~\cite{lapdepth}}              &   Y   & \multicolumn{1}{c|}{0.915}            & \multicolumn{1}{c|}{0.970}             & 0.985             & \multicolumn{1}{c|}{0.083}   & \multicolumn{1}{c|}{0.481}  & \multicolumn{1}{c|}{3.658} & 0.165 \\
% % lapdepth\_wetrain\_693                  & \multicolumn{1}{c|}{0.915}            & \multicolumn{1}{c|}{0.970}             & 0.985             & \multicolumn{1}{c|}{0.091}   & \multicolumn{1}{c|}{0.502}  & \multicolumn{1}{c|}{3.665} & 0.169 \\
% % {{\rowcolor{gbygreen}}}{lapdepth\_pseudo\_label}               &  N  & \multicolumn{1}{c|}{0.908}            & \multicolumn{1}{c|}{0.970}             & 0.987             & \multicolumn{1}{c|}{0.092}   & \multicolumn{1}{c|}{0.550}  & \multicolumn{1}{c|}{3.653} & 0.162    \\
% {lapdepth\_pseudo\_label}               &  N  & \multicolumn{1}{c|}{0.909}            & \multicolumn{1}{c|}{0.972}             & 0.989             & \multicolumn{1}{c|}{0.089}   & \multicolumn{1}{c|}{0.520}  & \multicolumn{1}{c|}{3.623} & 0.162    \\ 
% % {{\rowcolor{gbygreen}}}{lapdepth\_pseudo\_label\_full}              &  N   & \multicolumn{1}{c|}{0.936}            & \multicolumn{1}{c|}{0.975}             & 0.987             & \multicolumn{1}{c|}{0.069}   & \multicolumn{1}{c|}{0.448}  & \multicolumn{1}{c|}{3.236} & 0.145    \\
% {lapdepth\_pseudo\_label\_full}               &  N  & \multicolumn{1}{c|}{\textbf{0.936}}            & \multicolumn{1}{c|}{\textbf{0.975}}             & \textbf{0.987}             & \multicolumn{1}{c|}{\textbf{0.075}}   & \multicolumn{1}{c|}{\textbf{0.443}}  & \multicolumn{1}{c|}{\textbf{3.160}} & \textbf{0.147}    \\ 
% \hline
% % lapdepth\_pseudo\_label\_full\_monogt                  & \multicolumn{1}{c|}{0.939}            & \multicolumn{1}{c|}{0.976}             & 0.988             & \multicolumn{1}{c|}{0.074}   & \multicolumn{1}{c|}{0.481}  & \multicolumn{1}{c|}{3.287} & 0.145    \\ \hline
% % lapdepth\_pseudo\_label\_full\_monogt693                  & \multicolumn{1}{c|}{0.938}            & \multicolumn{1}{c|}{0.975}             & 0.987             & \multicolumn{1}{c|}{0.075}   & \multicolumn{1}{c|}{0.503}  & \multicolumn{1}{c|}{3.320} & 0.147    \\ \hline
% % __________________________________________________________________________________________________           
% {Godard* \textit{ et al.}} \cite{godard2017unsupervised}              &  N & \multicolumn{1}{c|}{0.916}            & \multicolumn{1}{c|}{0.980}             & 0.994             & \multicolumn{1}{c|}{0.085}   & \multicolumn{1}{c|}{0.584}  & \multicolumn{1}{c|}{3.938} & 0.135    \\ 
% {Kuznietsov* \textit{ et al.}} \cite{Kuznietsov}           & N & \multicolumn{1}{c|}{0.906}            & \multicolumn{1}{c|}{0.980}             & 0.995             & \multicolumn{1}{c|}{0.138}   & \multicolumn{1}{c|}{0.478}  & \multicolumn{1}{c|}{3.60}  & 0.138    \\ \hline
% {Amiri*  \textit{ et al.}}  \cite{amiri}             &  Y & \multicolumn{1}{c|}{0.923}            & \multicolumn{1}{c|}{0.984}             & 0.995             & \multicolumn{1}{c|}{0.078}   & \multicolumn{1}{c|}{0.417}  & \multicolumn{1}{c|}{3.464} & 0.126    \\ 
% {DORN*  \textit{ et al.}}   \cite{dorn}             & Y  & \multicolumn{1}{c|}{0.932}            & \multicolumn{1}{c|}{0.984}             & 0.994             & \multicolumn{1}{c|}{0.072}   & \multicolumn{1}{c|}{0.307}  & \multicolumn{1}{c|}{2.727} & 0.120    \\ \hline
% % Bts\_paper*              & \multicolumn{1}{c|}{0.955}            & \multicolumn{1}{c|}{0.993}             & 0.998             & \multicolumn{1}{c|}{0.060}   & \multicolumn{1}{c|}{0.249}  & \multicolumn{1}{c|}{2.798} & 0.096    \\ 
% {Bts* \cite{bts}}          & Y & \multicolumn{1}{c|}{0.960}            & \multicolumn{1}{c|}{0.994}          & 0.999             & \multicolumn{1}{c|}{0.060}   & \multicolumn{1}{c|}{0.210}  & \multicolumn{1}{c|}{2.459} & 0.093    \\ 
% % {{\rowcolor{gbygreen}}}{Bts\_pseudo\_label*}     & N & \multicolumn{1}{c|}{0.956}                & \multicolumn{1}{c|}{0.993}                 & 0.998                 & \multicolumn{1}{c|}{0.064}   & \multicolumn{1}{c|}{0.247}  & \multicolumn{1}{c|}{2.589} & 0.098    \\
% {Bts\_pseudo\_label*}     & N & \multicolumn{1}{c|}{0.956}                & \multicolumn{1}{c|}{0.993}                 & 0.998                 & \multicolumn{1}{c|}{0.064}   & \multicolumn{1}{c|}{0.243}  & \multicolumn{1}{c|}{2.576} & 0.099    \\ 
% % {{\rowcolor{gbygreen}}}{Bts\_pseudo\_label\_full*}     & N & \multicolumn{1}{c|}{0.970}                & \multicolumn{1}{c|}{0.995}                 & 0.999                 & \multicolumn{1}{c|}{0.058}   & \multicolumn{1}{c|}{0.185}  & \multicolumn{1}{c|}{2.230} & 0.087    \\ \hline
% {Bts\_pseudo\_label\_full*}     & N & \multicolumn{1}{c|}{\textbf{0.982}}                & \multicolumn{1}{c|}{\textbf{0.997}}                 & \textbf{0.999}                 & \multicolumn{1}{c|}{\textbf{0.043}}   & \multicolumn{1}{c|}{\textbf{0.127}}  & \multicolumn{1}{c|}{\textbf{1.845}} & \textbf{0.070}    \\ \hline
% % lapdepth\_paper*         & \multicolumn{1}{c|}{0.965}            & \multicolumn{1}{c|}{0.995}             & 0.999             & \multicolumn{1}{c|}{0.059}   & \multicolumn{1}{c|}{0.201}  & \multicolumn{1}{c|}{2.397} & 0.090    \\ 
% {lapdepth* \cite{lapdepth}}     & Y & \multicolumn{1}{c|}{0.961}            & \multicolumn{1}{c|}{0.994}             & 0.999             & \multicolumn{1}{c|}{0.061}   & \multicolumn{1}{c|}{0.206}  & \multicolumn{1}{c|}{2.398} & 0.092    \\ 
% % {{\rowcolor{gbygreen}}}{lapdepth\_pseudo\_label*} &  N & \multicolumn{1}{c|}{0.955}           & \multicolumn{1}{c|}{0.993}             & 0.998             & \multicolumn{1}{c|}{0.066}   & \multicolumn{1}{c|}{0.260}  & \multicolumn{1}{c|}{2.555} & 0.099
% % \\
% {lapdepth\_pseudo\_label*} &  N & \multicolumn{1}{c|}{0.952}           & \multicolumn{1}{c|}{0.992}             & 0.998             & \multicolumn{1}{c|}{0.067}   & \multicolumn{1}{c|}{0.263}  & \multicolumn{1}{c|}{2.598} & 0.100
% \\ 
% % {{\rowcolor{gbygreen}}}{lapdepth\_pseudo\_label\_full*}  & N & \multicolumn{1}{c|}{0.981}          & \multicolumn{1}{c|}{0.996}             & 0.999             & \multicolumn{1}{c|}{0.043}   & \multicolumn{1}{c|}{0.139}  & \multicolumn{1}{c|}{1.896} & 0.071 \\
% {lapdepth\_pseudo\_label\_full*} &  N & \multicolumn{1}{c|}{\textbf{0.979}}           & \multicolumn{1}{c|}{\textbf{0.996}}             & \textbf{0.999}             & \multicolumn{1}{c|}{\textbf{0.046}}   & \multicolumn{1}{c|}{\textbf{0.134}}  & \multicolumn{1}{c|}{\textbf{1.847}} & \textbf{0.074}
% \\ \hline
% \end{tabular}
% }
% \label{tab: KITTI benchmark}
% \end{table*}

% \begin{table}[!htb]
% \centering
% \resizebox{0.49\textwidth}{!}{
% \begin{tabular}{c|c|c|c|c|c}
% \hline
% Backbone & Supervision  & Abs Rel & Sq Rel & RMSE  & RMSE log \\ \hline
% BTS      & ground truth          & 0.060    & 0.210  & 2.459 & 0.093    \\ \hline
% BTS      & raw disparity         & -       & -      & -     & -        \\ 
% BTS      & raw disparity\_refine & -       & -      & -     & -        \\ 
% BTS      & UE/SF                 & -       & -      & -     & -        \\ 
% BTS      & CE/SF                 & 0.06824 & 0.2802 & 2.726 & 0.1054   \\ 
% BTS      & UE/SF+K               & 0.063   & 0.253  & 2.67  & 0.0992   \\ 
% BTS      & CE/SF+K               & 0.06314 & 0.2489 & 2.618 & 0.0982   \\ 
% BTS      & CE/SF+K/refine        & -       & -      & -     & -        \\ \hline
% \end{tabular}
% }
% \caption{to write}
% \label{tab: BTS backbone}
% \end{table}


% \begin{table*}[!htb]
% \centering
% \resizebox{0.8\textwidth}{!}{
% \begin{tabular}{c|c|c|c|c|c|c|c}
% \hline
% Method        & \textbf{Filter} & Dataset & RMSE & RMSE log & Sq Rel & Abs Rel & dense rate \\ \hline
% CFNet         & -               & SF      & 3.65 & 0.094    & 15.50  & 0.036   & 100\%      \\ 
% CFNet\_refine & -               & SF      & -    & -        & -      & -       & -          \\ 
% CFNet         & UE              & SF      & -    & -        & -      & -       & -          \\ 
% CFNet         & CE              & SF      & 2.22 & 0.040    & 0.316  & 0.025   & 54.92\%    \\ 
% CFNet\_refine & CE              & SF      & -    & -        & -      & -       & -          \\ \hline
% CFNet         & UE              & SF+K    & 1.95 & 0.035    & 0.146  & 0.021   & 62.59\%    \\ 
% CFNet         & CE              & SF+K    & 1.92 & 0.036    & 0.099  & 0.021   & 85.38\%    \\ 
% CFNet\_refine & CE              & SF+K    & -    & -        & -      & -       & -          \\ \hline
% \end{tabular}
% }
% \caption{to write}
% \label{tab: BTS backbone}
% \end{table*}


\section{Monocular depth estimation Experiments}
\label{sec:monocular}
% To further prove the effectiveness of our generated pseudo-labels, we also evaluate our approach under the monocular depth estimation setting, that is, we replace the ground truth with our generated pseudo-labels to supervise monocular depth estimation methods. Note that both state-of-the-art supervised and self-supervised monocular depth estimation approaches are compared here. 
Recall our goal is to push methods to be robust and perform well across different datasets without using the ground truth of the target domain. This is same in the monocular depth estimation setting. Indeed, as the monocular depth estimation is an ill-posed problem, it even needs more annotated data. However, acquiring labeled real-world data is cumbersome and costly in most practical settings, e.g., expensive LiDAR with careful calibration is required to obtain depth ground truth in outdoor scenes. Instead, stereo matching is a cheaper option. Hence, we propose to explore the feasibility of eliminating the need for lidar and using stereo matching methods to collect ground truth data. That is the proposed stereo matching method is served as the offline ground truth collection system and the monocular depth estimation network is the deployed online depth estimation module. Following this motivation, we select to use the stereo matching model trained on the synthetic dataset and unlabeled target domain data (UCFNet\_adapt) to generate the pseudo-label for the training of monocular depth estimation. Note that both state-of-the-art supervised and self-supervised monocular depth estimation approaches are compared here.



\subsection{Dataset}

We use the KITTI dataset~\cite{kitti1} as the training dataset which consists of calibrated videos registered to LiDAR measurements of city scenarios. The depth evaluation is done on the LiDAR pointcloud. Following~\cite{eigen2014depth}\cite{monodepth2_iccv2019}\cite{depth_hint_iccv2019}, seven standard metrics, named ''Abs Rel'', ''Sq Rel'', ''RMSE'', ''RMSE log'', ''$\delta < 1.25 $'', ''$\delta < 1.25^2 $'' and ''$\delta < 1.25^3 $'' are used to evaluate the performance of the predicted depth information. Tab.~\ref{tab:metrics} demonstrates the definition of each evaluation metric, and please see~\cite{eigen2014depth} for evaluation details. %Following~\cite{monodepth2_iccv2019}, to compare fairly, we use the Eigen split of KITTI dataset~\cite{kitti1} as the validation dataset using the standard cap of 80m~\cite{Left-Right-Consistency:cvpr2017}. In specific, $39810$ monocular triplets are used for training, $4424$ for testing and $697$ for validation.

\textbf{Training mode:} According to different training modes, three kinds of results are provided here. (1). Results of self-supervised monocular depth estimation approaches, which do not need the supervision of ground truth; (2). Results of supervised monocular depth estimation approaches with ground truth as supervision; (3). Results of supervised monocular depth estimation approaches with generated pseudo-labels as supervision. Moreover, as these methods eliminate the need for ground truth, they can be regarded as \textbf{unsupervised approaches}.%To well evaluate the performance of our approach, different modes of models are trained and evaluated, including: monocular only (M), stereo only (S), and monocular plus stereo (MS). To train the model of stereo (S) and monocular plus stereo (MS), we set the transformation between the two stereo frames to be a pure horizontal translation of fixed length. For models trained under monocular (M) mode, results are obtained by using the per-image median ground truth scaling introduced by~\cite{Depth-Ego-Motion-from-Video:cvpr2017}, and for models trained under stereo (S) and monocular plus stereo (MS) modes, we use scale that inferred from the known camera baseline during training, which is $5.4$ in our experiment for KITTI dataset~\cite{kitti1}. 


\begin{table}[!t]
    \centering
    \caption{\footnotesize Detailed evaluation metrics of monocular depth estimation, where $n$ is the number of pixels, $y_{pred}$ and $y_{gt}$ are the estimated depth and ground truth depth, respectively. $y_{{pred}_i}$ and $y_{{gt}_i}$ denote the $i_{th}$ pixel in the estimated and ground truth depth map. $T$ is the threshold.}
    \vspace{-0.1in}
    \begin{tabular}{|c|c|} \hline
        Abs Rel & $\frac{1}{n} \sum \frac{y_{pred} - y_{gt}}{y_{gt}}$ \\ \hline
        Sq Rel & $\frac{1}{n} \sum (\frac{y_{pred} - y_{gt}}{y_{gt}})^2$ \\ \hline
        RMSE & $\sqrt{\frac{1}{n}\sum{(y_{pred} - y_{gt})^2}}$  \\ \hline 
        RMSE log & $\sqrt{\frac{1}{n}\sum{(log(y_{pred}) - log(y_{gt}))^2}}$ \\ \hline
        $\delta$ & $max(\frac{y_{{pred}_i}}{y_{{gt}_i}}, \frac{y_{{gt}_i}}{y_{{pred}_i}}) < T$\\ \hline
    \end{tabular}
    
    \label{tab:metrics}
    \vspace{-0.1in}
\end{table}

\begin{table*}[!t]
\caption{\footnotesize Quantitative evaluations on the kitti dataset using the test split of Eigen et al. * denotes the performance is evaluated using the official annotated ground truth (default: using raw velodyne data). Note that we use the cropping strategy introduced by Garg et al.}
\vspace{-0.1in}
\centering
\footnotesize
%\resizebox{0.95\textwidth}{!}
{
\begin{tabular}{c|c|ccc|cccc}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{GT} & \multicolumn{1}{c|}{$\delta  < {1.25}$} & \multicolumn{1}{c|}{$\delta  < {1.25^2}$} & $\delta  < {1.25^3}$ & \multicolumn{1}{c|}{Abs Rel} & \multicolumn{1}{c|}{Sq Rel} & \multicolumn{1}{c|}{RMSE}  & RMSE log \\ \cline{3-9} 
                  &     & \multicolumn{3}{c|}{Higher value is better}                                                        & \multicolumn{4}{c}{Lower value is better}                                                         \\ \hline
                        
\rowcolor{gbypink}{Godard \textit{ et al.}\cite{godard2017unsupervised}}        & N & \multicolumn{1}{c|}{0.861}            & \multicolumn{1}{c|}{0.949}             & 0.976             & \multicolumn{1}{c|}{0.114}   & \multicolumn{1}{c|}{0.898}  & \multicolumn{1}{c|}{4.935} & 0.206    \\
\rowcolor{gbypink}{Kuznietsov \textit{ et al.}  \cite{Kuznietsov}}                & N & \multicolumn{1}{c|}{0.862}            & \multicolumn{1}{c|}{0.960}             & 0.986             & \multicolumn{1}{c|}{0.113}   & \multicolumn{1}{c|}{0.741}  & \multicolumn{1}{c|}{4.621} & 0.189    \\
\rowcolor{gbypink}{Monodepth2~\cite{monodepth2_iccv2019}} & N &\multicolumn{1}{c|}{0.876} & \multicolumn{1}{c|}{0.957} & 0.980 & \multicolumn{1}{c|}{0.106} & \multicolumn{1}{c|}{0.806} & \multicolumn{1}{c|}{4.630} & 0.193  \\
\rowcolor{gbypink}{Monodepth2 dh~\cite{depth_hint_iccv2019}} & N &\multicolumn{1}{c|}{0.888} & \multicolumn{1}{c|}{0.962} & 0.982 & \multicolumn{1}{c|}{0.100} & \multicolumn{1}{c|}{0.757} & \multicolumn{1}{c|}{4.490} & 0.185 \\
\rowcolor{gbypink}{MLDA-Net~\cite{song2021mlda}} & N &\multicolumn{1}{c|}{0.887} & \multicolumn{1}{c|}{0.963} & 0.983 & \multicolumn{1}{c|}{0.099} & \multicolumn{1}{c|}{0.724} & \multicolumn{1}{c|}{4.415} & 0.183 \\
\rowcolor{gbypink}{monoResMatch~\cite{monoresmatch}} & N &\multicolumn{1}{c|}{0.890} & \multicolumn{1}{c|}{0.961} & 0.981 & \multicolumn{1}{c|}{0.096} & \multicolumn{1}{c|}{0.673} & \multicolumn{1}{c|}{4.351} & 0.184 \\ 
\rowcolor{gbypink}{SD-SSMDE~\cite{SD-SSMDE}} & N &\multicolumn{1}{c|}{0.902} & \multicolumn{1}{c|}{0.968} & 0.985 & \multicolumn{1}{c|}{0.098} & \multicolumn{1}{c|}{0.674} & \multicolumn{1}{c|}{4.187} & 0.170 \\ \hline  

\rowcolor{gbygray}{Eigen\textit{ et al.} \cite{eigen}}                & Y & \multicolumn{1}{c|}{0.692}            & \multicolumn{1}{c|}{0.899}             & 0.967             & \multicolumn{1}{c|}{0.190}   & \multicolumn{1}{c|}{1.515}  & \multicolumn{1}{c|}{7.156} & 0.270    \\
\rowcolor{gbygray}{Liu \textit{ et al.}\cite{liu}}             &   Y  & \multicolumn{1}{c|}{0.647}            & \multicolumn{1}{c|}{0.882}             & 0.961             & \multicolumn{1}{c|}{0.217}   & \multicolumn{1}{c|}{1.841}  & \multicolumn{1}{c|}{6.986} & 0.289    \\


\rowcolor{gbygray}{Gan et al \textit{ et al.}   \cite{gan}}              & Y & \multicolumn{1}{c|}{0.890}            & \multicolumn{1}{c|}{0.964}             & 0.985             & \multicolumn{1}{c|}{0.098}   & \multicolumn{1}{c|}{0.666}  & \multicolumn{1}{c|}{3.933} & 0.173    \\
\rowcolor{gbygray}{DORN     \cite{dorn}}           & Y & \multicolumn{1}{c|}{0.897}            & \multicolumn{1}{c|}{0.966}             & 0.986             & \multicolumn{1}{c|}{0.099}   & \multicolumn{1}{c|}{0.593}  & \multicolumn{1}{c|}{3.714} & 0.161    \\ \hline
% Lee et al                  & \multicolumn{1}{c|}{0.904}            & \multicolumn{1}{c|}{0.967}             & 0.984             & \multicolumn{1}{c|}{0.091}   & \multicolumn{1}{c|}{0.555}  & \multicolumn{1}{c|}{4.033} & 0.174    \\ \hline
%bts\_wetrain~\cite{song2021monocular}                  & \multicolumn{1}{c|}{0.913}            & \multicolumn{1}{c|}{0.970}             & 0.985             & \multicolumn{1}{c|}{0.082}   & \multicolumn{1}{c|}{0.484}  & \multicolumn{1}{c|}{3.694} & 0.166 \\
\rowcolor{gbygreen}{Bts~\cite{bts}}                & Y & \multicolumn{1}{c|}{0.913}            & \multicolumn{1}{c|}{0.970}             & 0.985             & \multicolumn{1}{c|}{0.082}   & \multicolumn{1}{c|}{0.484}  & \multicolumn{1}{c|}{3.694} & 0.166 \\
% {{\rowcolor{gbygreen}}}{bts\_pseudo\_label}                & N  & \multicolumn{1}{c|}{0.909}            & \multicolumn{1}{c|}{0.970}             & 0.986             & \multicolumn{1}{c|}{0.087}   & \multicolumn{1}{c|}{0.526}  & \multicolumn{1}{c|}{3.716} & 0.166 \\
\rowcolor{gbygreen}{bts\_pseudo\_label}                & N  & \multicolumn{1}{c|}{0.910}            & \multicolumn{1}{c|}{0.970}             & 0.986             & \multicolumn{1}{c|}{0.094}   & \multicolumn{1}{c|}{0.554}  & \multicolumn{1}{c|}{3.730} & 0.167 \\
% {{\rowcolor{gbygreen}}}{bts\_pseudo\_label\_full}              &   N  & \multicolumn{1}{c|}{0.921}            & \multicolumn{1}{c|}{0.973}             & 0.987             & \multicolumn{1}{c|}{0.084}   & \multicolumn{1}{c|}{0.476}  & \multicolumn{1}{c|}{3.424} & 0.157 \\ \hline
\rowcolor{gbygreen}{bts\_pseudo\_label\_full}              &   N  & \multicolumn{1}{c|}{\textbf{0.939}}            & \multicolumn{1}{c|}{\textbf{0.975}}             & \textbf{0.987}             & \multicolumn{1}{c|}{\textbf{0.072}}   & \multicolumn{1}{c|}{\textbf{0.454}}  & \multicolumn{1}{c|}{\textbf{3.222}} &\textbf{ 0.147} \\ \hline
%lapdepth\_wetrain~\cite{song2021monocular}                   & \multicolumn{1}{c|}{0.915}            & \multicolumn{1}{c|}{0.970}             & 0.985             & \multicolumn{1}{c|}{0.083}   & \multicolumn{1}{c|}{0.481}  & \multicolumn{1}{c|}{3.658} & 0.165 
\rowcolor{gbygreen}{lapdepth~\cite{lapdepth}}              &   Y   & \multicolumn{1}{c|}{0.915}            & \multicolumn{1}{c|}{0.970}             & 0.985             & \multicolumn{1}{c|}{0.083}   & \multicolumn{1}{c|}{0.481}  & \multicolumn{1}{c|}{3.658} & 0.165 \\
% lapdepth\_wetrain\_693                  & \multicolumn{1}{c|}{0.915}            & \multicolumn{1}{c|}{0.970}             & 0.985             & \multicolumn{1}{c|}{0.091}   & \multicolumn{1}{c|}{0.502}  & \multicolumn{1}{c|}{3.665} & 0.169 \\
% {{\rowcolor{gbygreen}}}{lapdepth\_pseudo\_label}               &  N  & \multicolumn{1}{c|}{0.908}            & \multicolumn{1}{c|}{0.970}             & 0.987             & \multicolumn{1}{c|}{0.092}   & \multicolumn{1}{c|}{0.550}  & \multicolumn{1}{c|}{3.653} & 0.162    \\
\rowcolor{gbygreen}{lapdepth\_pseudo\_label}               &  N  & \multicolumn{1}{c|}{0.909}            & \multicolumn{1}{c|}{0.972}             & \textbf{0.989}             & \multicolumn{1}{c|}{0.089}   & \multicolumn{1}{c|}{0.520}  & \multicolumn{1}{c|}{3.623} & 0.162    \\ 
% {{\rowcolor{gbygreen}}}{lapdepth\_pseudo\_label\_full}              &  N   & \multicolumn{1}{c|}{0.936}            & \multicolumn{1}{c|}{0.975}             & 0.987             & \multicolumn{1}{c|}{0.069}   & \multicolumn{1}{c|}{0.448}  & \multicolumn{1}{c|}{3.236} & 0.145    \\
\rowcolor{gbygreen}{lapdepth\_pseudo\_label\_full}               &  N  & \multicolumn{1}{c|}{\textbf{0.936}}            & \multicolumn{1}{c|}{\textbf{0.975}}             & 0.987             & \multicolumn{1}{c|}{\textbf{0.075}}   & \multicolumn{1}{c|}{\textbf{0.443}}  & \multicolumn{1}{c|}{\textbf{3.160}} & \textbf{0.147}    \\ 
\hline
% lapdepth\_pseudo\_label\_full\_monogt                  & \multicolumn{1}{c|}{0.939}            & \multicolumn{1}{c|}{0.976}             & 0.988             & \multicolumn{1}{c|}{0.074}   & \multicolumn{1}{c|}{0.481}  & \multicolumn{1}{c|}{3.287} & 0.145    \\ \hline
% lapdepth\_pseudo\_label\_full\_monogt693                  & \multicolumn{1}{c|}{0.938}            & \multicolumn{1}{c|}{0.975}             & 0.987             & \multicolumn{1}{c|}{0.075}   & \multicolumn{1}{c|}{0.503}  & \multicolumn{1}{c|}{3.320} & 0.147    \\ \hline
% __________________________________________________________________________________________________           
\rowcolor{gbypink}{Godard* \textit{ et al.}} \cite{godard2017unsupervised}              &  N & \multicolumn{1}{c|}{0.916}            & \multicolumn{1}{c|}{0.980}             & 0.994             & \multicolumn{1}{c|}{0.085}   & \multicolumn{1}{c|}{0.584}  & \multicolumn{1}{c|}{3.938} & 0.135    \\ 
\rowcolor{gbypink}{Kuznietsov* \textit{ et al.}} \cite{Kuznietsov}           & N & \multicolumn{1}{c|}{0.906}            & \multicolumn{1}{c|}{0.980}             & 0.995             & \multicolumn{1}{c|}{0.138}   & \multicolumn{1}{c|}{0.478}  & \multicolumn{1}{c|}{3.60}  & 0.138    \\ \hline
\rowcolor{gbygray}{Amiri*  \textit{ et al.}}  \cite{amiri}             &  Y & \multicolumn{1}{c|}{0.923}            & \multicolumn{1}{c|}{0.984}             & 0.995             & \multicolumn{1}{c|}{0.078}   & \multicolumn{1}{c|}{0.417}  & \multicolumn{1}{c|}{3.464} & 0.126    \\ 
\rowcolor{gbygray}{DORN*  \textit{ et al.}}   \cite{dorn}             & Y  & \multicolumn{1}{c|}{0.932}            & \multicolumn{1}{c|}{0.984}             & 0.994             & \multicolumn{1}{c|}{0.072}   & \multicolumn{1}{c|}{0.307}  & \multicolumn{1}{c|}{2.727} & 0.120    \\ \hline
% Bts\_paper*              & \multicolumn{1}{c|}{0.955}            & \multicolumn{1}{c|}{0.993}             & 0.998             & \multicolumn{1}{c|}{0.060}   & \multicolumn{1}{c|}{0.249}  & \multicolumn{1}{c|}{2.798} & 0.096    \\ 
\rowcolor{gbygreen}{Bts* \cite{bts}}          & Y & \multicolumn{1}{c|}{0.960}            & \multicolumn{1}{c|}{0.994}          & 0.999             & \multicolumn{1}{c|}{0.060}   & \multicolumn{1}{c|}{0.210}  & \multicolumn{1}{c|}{2.459} & 0.093    \\ 
% {{\rowcolor{gbygreen}}}{Bts\_pseudo\_label*}     & N & \multicolumn{1}{c|}{0.956}                & \multicolumn{1}{c|}{0.993}                 & 0.998                 & \multicolumn{1}{c|}{0.064}   & \multicolumn{1}{c|}{0.247}  & \multicolumn{1}{c|}{2.589} & 0.098    \\
\rowcolor{gbygreen}{Bts\_pseudo\_label*}     & N & \multicolumn{1}{c|}{0.956}                & \multicolumn{1}{c|}{0.993}                 & 0.998                 & \multicolumn{1}{c|}{0.064}   & \multicolumn{1}{c|}{0.243}  & \multicolumn{1}{c|}{2.576} & 0.099    \\ 
% {{\rowcolor{gbygreen}}}{Bts\_pseudo\_label\_full*}     & N & \multicolumn{1}{c|}{0.970}                & \multicolumn{1}{c|}{0.995}                 & 0.999                 & \multicolumn{1}{c|}{0.058}   & \multicolumn{1}{c|}{0.185}  & \multicolumn{1}{c|}{2.230} & 0.087    \\ \hline
\rowcolor{gbygreen}{Bts\_pseudo\_label\_full*}     & N & \multicolumn{1}{c|}{\textbf{0.982}}                & \multicolumn{1}{c|}{\textbf{0.997}}                 & \textbf{0.999}                 & \multicolumn{1}{c|}{\textbf{0.043}}   & \multicolumn{1}{c|}{\textbf{0.127}}  & \multicolumn{1}{c|}{\textbf{1.845}} & \textbf{0.070}    \\ \hline
% lapdepth\_paper*         & \multicolumn{1}{c|}{0.965}            & \multicolumn{1}{c|}{0.995}             & 0.999             & \multicolumn{1}{c|}{0.059}   & \multicolumn{1}{c|}{0.201}  & \multicolumn{1}{c|}{2.397} & 0.090    \\ 
\rowcolor{gbygreen}{lapdepth* \cite{lapdepth}}     & Y & \multicolumn{1}{c|}{0.961}            & \multicolumn{1}{c|}{0.994}             & 0.999             & \multicolumn{1}{c|}{0.061}   & \multicolumn{1}{c|}{0.206}  & \multicolumn{1}{c|}{2.398} & 0.092    \\ 
% {{\rowcolor{gbygreen}}}{lapdepth\_pseudo\_label*} &  N & \multicolumn{1}{c|}{0.955}           & \multicolumn{1}{c|}{0.993}             & 0.998             & \multicolumn{1}{c|}{0.066}   & \multicolumn{1}{c|}{0.260}  & \multicolumn{1}{c|}{2.555} & 0.099
% \\
\rowcolor{gbygreen}{lapdepth\_pseudo\_label*} &  N & \multicolumn{1}{c|}{0.952}           & \multicolumn{1}{c|}{0.992}             & 0.998             & \multicolumn{1}{c|}{0.067}   & \multicolumn{1}{c|}{0.263}  & \multicolumn{1}{c|}{2.598} & 0.100
\\ 
% {{\rowcolor{gbygreen}}}{lapdepth\_pseudo\_label\_full*}  & N & \multicolumn{1}{c|}{0.981}          & \multicolumn{1}{c|}{0.996}             & 0.999             & \multicolumn{1}{c|}{0.043}   & \multicolumn{1}{c|}{0.139}  & \multicolumn{1}{c|}{1.896} & 0.071 \\
\rowcolor{gbygreen}{lapdepth\_pseudo\_label\_full*} &  N & \multicolumn{1}{c|}{\textbf{0.979}}           & \multicolumn{1}{c|}{\textbf{0.996}}             & \textbf{0.999}             & \multicolumn{1}{c|}{\textbf{0.046}}   & \multicolumn{1}{c|}{\textbf{0.134}}  & \multicolumn{1}{c|}{\textbf{1.847}} & \textbf{0.074}
\\ \hline
\end{tabular}
}
\label{tab: mono_result}
\vspace{-0.1in}
\end{table*}

\subsection{Implementation Details}
We select two representative supervised monocular depth estimation models LapDepth$\footnote{\href{https://github.com/tjqansthd/LapDepth-release/} {LapDepth} uses GPL-3.0 License and we download the code from their official GitHub website.}$ and BTS$\footnote{\href{ https://github.com/cleinc/bts/}{BTS} uses GPL-3.0 License and we download the code from their official GitHub website.}$ to verify the effectiveness of the proposed pseudo label in monocular depth estimation. All settings remain the same as in their original paper except that we employ the generated pseudo-labels rather than ground truth to supervise the network. Specifically, the whole training process can be divided into three steps: Firstly, we employ the synthetic data (source domain) and the unlabeled stereo images of the real dataset (target domain) to train a robust stereo matching network UCFNet\_adapt. Secondly, we feed the synchronized stereo images of the KITTI raw dataset into the UCFNet\_adapt and employ the proposed uncertainty-based pseudo-label generation method to generate corresponding pseudo-labels. Thirdly, we employ the generated pseudo-labels as supervision to train the selected two representative supervised monocular depth estimation methods: LapDepth and BTS.

\subsection{Comparisons among different training modes}
Tab. \ref{tab: mono_result} demonstrates the results among different training modes. In specific, pink areas mean results obtained by self-supervised approaches, purple areas mean results obtained by supervised approaches with ground truth supervision, and green areas mean results obtained by supervised approaches with our generated pseudo-labels. ''*'' means results evaluated by the official annotated ground truth of KITTI, where the ground truth is obtained by combing multi-frame of the point cloud, while default (without ''*'') means results evaluated by raw point cloud data of KITTI.

Note that, the ground truth is not needed in the generation of pseudo-labels, therefore, approaches supervised with our generated pseudo-labels (\emph{Bts\_pseudo\_label}, \emph{Bts\_pseudo\_label\_full}, \emph{lapdepth\_pseudo\_label} and \emph{lapdepth\_pseudo\_label\_full} in Tab.~\ref{tab: mono_result}) can be regarded as unsupervised approaches.

% Figure environment removed

The comparison between state-of-the-art self-supervised and supervised monocular depth estimation methods is shown in Tab.~\ref{tab: mono_result}. It can be seen from the table that the proposed pseudo-label-based method (green areas) outperforms self-supervised monocular depth estimation approaches (pink areas) by a large margin, which proves that our generated pseudo-labels can well supervise monocular depth estimation approaches.  Moreover, due to the usage of ground truth, the performances of supervised depth estimation approaches (purple areas), such as DORN~\cite{dorn}, Bts~\cite{bts}, and lapdepth~\cite{lapdepth}, commonly outperform self-supervised based approaches (pink areas). In this paper, we claim that pseudo-label-based methods can achieve comparable or even better results than supervised approaches without the need for ground truth. Specifically, as shown in the green areas of Tab. \ref{tab: mono_result}, Bts~\cite{bts} and lapdepth~\cite{lapdepth} denote the original result of two representative supervised monocular depth estimation models trained by ourselves with the official implementation.  \emph{Bts\_pseudo\_label} and \emph{lapdepth\_pseudo\_label} denote we employ the pseudo-labels of the training dataset to supervise the two representative supervised monocular depth estimation models. Note that the only difference between the proposed pseudo-label-based methods and the corresponding original implementation is the supervision signals, i.e., pseudo-labels vs ground truth. As shown, \emph{Bts\_pseudo\_label} and \emph{lapdepth\_pseudo\_label} can achieve comparable results with the supervised version, e.g., the rmse of \emph{Bts\_pseudo\_label} in default evaluation is 3.730, which is only 0.97\% higher than  \emph{BTS}. Similar situations can also be observed in \emph{lapdepth\_pseudo\_label}. Note that we also observe that some previous work, such as SD-SSMDE \cite{SD-SSMDE} and monoResMatch \cite{monoresmatch} also explore using pseudo-label generated by traditional stereo matching methods \cite{monoresmatch} or self-distillation \cite{SD-SSMDE} to supervise the monocular depth estimation method. However, these methods still have a large gap between supervised depth estimation approaches and cannot achieve comparable results with the proposed uncertainty-based pseudo-label. Additionally, as the generation of pseudo-labels is not dependent on ground truth, pseudo-labels of the testing dataset can also be generated, which can be combined into the training dataset for better performance.  \emph{Bts\_pseudo\_label\_full} and \emph{lapdepth\_pseudo\_label\_full} in Tab. \ref{tab: mono_result} demonstrate the corresponding results. We can see that \emph{Bts\_pseudo\_label\_full} and \emph{lapdepth\_pseudo\_label\_full} can greatly improve the performances of \emph{Bts\_pseudo\_label} and \emph{lapdepth\_pseudo\_label} on all evaluation metrics. Moreover, they can even outperform the fully supervised method Bts~\cite{bts} and lapdepth~\cite{lapdepth} with large margins, which further verifies our hypothesis that stereo matching can be a viable alternative to reduce the cost of ground truth collection in the monocular depth estimation setting.

Qualitative comparison results on the KITTI Eigen test split are shown in Fig.~\ref{fig: monodepth}. Specifically, scenes with different depths of field are provided in which red denotes a smaller depth. As shown, our method can better distinguish objects in both foreground and background areas (see dash boxes in the pictures). Moreover, supervised methods generally cannot generate reasonable results on unlabeled areas, e.g., the sky region and the upper part of the scenes. This is mainly caused by the limitation of LIDAR, e.g., the ground truth obtained by lidar is very sparse and cannot collect valid data in the upper region of the scene. Instead, the proposed method can provide denser pseudo-labels and cover all regions in the image, thus significantly improving the visualization results on unlabeled areas. (see green dash boxes in the picture). 

% All in all, the visualization results further support our claim that pseudo-label-based methods can achieve comparable or even better results than supervised approaches without the need for ground truth.
% More visualization results can be seen in the video demo of supplementary.

% Table.~\ref{tab: KITTI benchmark} demonstrates the results among different training mode. In specific, pink areas mean results obtained by self-supervised approaches, blue areas mean results obtained by supervised approaches with ground truth supervision, and gray areas mean results obtained by supervised approaches with our generated pseudo-labels. ''*'' means results evaluated by the official annotated ground truth, where the ground truth is obtained by combing multi-frame of pointcloud, while default (without ''*'') means results evaluated by ground truth obtained by single raw pointcloud.

% Note that, the ground truth is not needed in the generation of pseudo-labels, therefore, approaches supervised with our generated pseudo-labels (\emph{Bts\_pseudo\_label}, \emph{Bts\_pseudo\_label\_full}, \emph{lapdepth\_pseudo\_label} and \emph{lapdepth\_pseudo\_label\_full} in Table.~\ref{tab: KITTI benchmark}) can be regarded as unsupervised approaches.

% As shown in Table.~\ref{tab: KITTI benchmark}, though the ground truth data is not needed, results obtained by approaches with our pseudo-labels (gray areas) outperform self-supervised monocular depth estimation approaches (pink areas), which proves that our generated pseudo-labels can well supervise monocular depth estimation approaches. Meanwhile, due to lacking of the supervisions of ground truth, the performances of supervised depth estimation approaches (pink areas), such as DORN~\cite{dorn}, Bts~\cite{bts} and lapdepth~\cite{lapdepth}, commonly outperform self-supervised based approaches (blue areas). And as shown in Table.~\ref{tab: KITTI benchmark}, comparing with supervised approaches, Bts~\cite{bts} and lapdepth~\cite{lapdepth}, comparable results can be obtained with supervisions of our generated pseudo-labels,  \emph{Bts\_pseudo\_label} and \emph{lapdepth\_pseudo\_label}, which also sufficiently proves the effectiveness of our generated pseudo-labels.

% Meanwhile, due to the generation of pseudo-labels is not dependent on ground truth, therefore, the pseudo-labels of testing dataset can also be generated, which can be combined into the training dataset, thus the models can be re-trained, and Table.~\ref{tab: KITTI benchmark} \emph{Bts\_pseudo\_label\_full} and \emph{lapdepth\_pseudo\_label\_full} demonstrate the corresponding results. We can see that \emph{Bts\_pseudo\_label\_full} and \emph{lapdepth\_pseudo\_label\_full} can further improve the performances on all evaluation metrics, and outperform Bts~\cite{bts} and lapdepth~\cite{lapdepth} with large margins, which proves that our generated pseudo-labels are more flexible and effective.
