
\section{Introduction}

%Stereo matching, \emph{i.e.}, estimating a disparity/depth map from a pair of stereo images, is fundamental to various applications, such as autonomous driving \cite{autonomousdriving}, robot navigation \cite{roboticsnavigation}, SLAM \cite{slam1,slam2}, etc. Recently, many deep learning-based stereo methods have been developed and achieved impressive performance on most of the standard benchmarks.

Stereo matching is a classical research topic in computer vision, which aims to estimate a disparity/depth map from a pair of rectified stereo images. It is a key enabling technique for various applications, such as autonomous driving \cite{autonomousdriving}, robot navigation \cite{roboticsnavigation}, SLAM \cite{slam1,slam2}, etc. Currently, impressive performances have been achieved by many deep learning-based stereo methods on most of the standard benchmarks.

%without substantial adaptation (visualization comparison can be seen in Figure \ref{fig: introduction comparison}),

%which leads to the learned features distorted and noisy \cite{dsmnet}

However, significant domain shifts commonly exist among different datasets, which limits the generalization abilities of current state-of-the-art stereo matching methods. For example, the Middlebury \cite{mid} dataset mainly contains indoor high-resolution scenes while the KITTI dataset \cite{kitti1,kitti2} mainly consists of real-world urban driving scenarios. More specifically, as illustrated in Fig.~\ref{fig:  adapt generalization intro} (a), there are significant differences among various datasets, e.g., indoors vs outdoors, color vs gray, and real vs synthetic. In addition, the disparity ranges are different among various datasets. As illustrated in Fig.~\ref{fig: disparity distribution}, the disparity range of half-resolution images in Middlebury \cite{mid} is even more than 6 times larger than full-resolution images in ETH3D \cite{eth3d} (400 vs 64). Such unbalanced disparity distributions make the current approaches trained with a fixed disparity range difficult to cover the whole disparity range of another dataset without substantial finetuning.

%It is difficult to design a robust stereo matching system due to the large domain differences and unbalanced disparity distribution between a variety of datasets. 

% Figure environment removed

% Figure environment removed

% Figure environment removed

Consequently, methods with state-of-the-art performance on one dataset often cannot achieve comparable results on other datasets without substantial adaptation. To relieve the problem, our conference paper CFNet\cite{cfnet} proposes a cascade and fused cost volume representation to narrow down the domain difference.
By employing the cascade cost volume representation to alleviate the unbalanced disparity distribution, the method can eliminate the need for adaptation and performs well across a variety of datasets with fixed model parameters and hyperparameters, i.e., joint generalization.
% to construct a robust stereo matching system. By proposing a fused cost volume representation to narrow down the domain difference.
% By alleviating the unbalanced disparity distribution and eliminating the need for adaptation, the method performs well across a variety of datasets with the fixed model parameters and hyperparameters, i.e., joint generalization.
Unfortunately, such a method still needs suitable labeled target domain data, which cannot be easily obtained in most practical settings. Moreover, the labeled ground truth data is commonly obtained by expensive sensors (e.g. LiDAR) alongside careful calibration, which is cumbersome and costly, limiting the applicability in practical settings. Thus, we need to push methods to be robust and perform well across different datasets without using the groundtruth labels from the target domain.

%However, current state-of-the-art methods are generally limited to a specific dataset, which is caused by the significant domain shifts between different datasets. For example, the KITTI dataset \cite{kitti1,kitti2} focuses on real-world urban driving scenarios while Middlebury \cite{mid} concentrates on indoor high-resolution scenes. Consequently, methods that are state-of-the-art on one dataset often can’t achieve comparable performance on a unseen different one without substantial adaptation (visualization comparison can be seen in Figure \ref{fig: introduction comparison}). To address the domain-shift issues, current methods usually select to first pretrain their model on large-scale synthetic dataset and then finetune it on annotated target-domain data. However, suitable labeled data cannot be easily obtained in most practical settings, for example, we need to deploy expensive sensors alongside with careful calibration. As such expensive sensors (e.g. LiDAR) is cumbersome and costly, collecting enough labeled target domain images to solve the domain-shift issues may not be an effective solution in practical settings. Thus, we need to push methods to be robust and perform well across different datasets without using the groundtruth labels from the target domain.

	
%     % Figure removed&
%     % Figure removed&
% 	% Figure removed&
% 	% Figure removed\\

%     % Figure removed&
%     % Figure removed&
% 	% Figure removed&
% 	% Figure removed\\

% 	{(a) Left image} &  {(b) CFNet} &	{(c) GANet }	&  {(d) HSMNet }	  	\\
% 	\end{tabular}
	
% 	\caption{Visualization of some state-of-the-art methods’ performance on three real-world dataset testsets (from top to bottom: KITTI, Middlebury, and ETH3D). All methods are trained with a combination of KITTI, Middlebury, and ETH3D train images. GANet \cite{ganet} and HSMNet \cite{hsm} can achieve good performance on one specific dataset but perform poorly on the other two datasets even if they have included targeted domain images in the training process. Our CFNet achieves SOTA or near SOTA performance on all three datasets without any adaptation.}
% 	\label{fig: introduction comparison}
% \end{figure*}

% % Figure environment removed
%The difficulties in designing a robust stereo matching system come from the large domain differences and unbalanced disparity distribution between a variety of datasets.

In this paper, an Uncertainty-based Cascade and Fused cost volume representation (UCFNet) is proposed to dig into uncertainty estimation for robust stereo matching. Specifically, an uncertainty-based pseudo-labels generation method is proposed to adapt the pre-trained model to the new domain, i.e., domain adaptation. A key observation behind our method is that learning-based models can be successfully adapted to new domains even by deploying only a few sparse groundtruth annotations. For example, learning-based models can achieve state-of-the-art performance on KITTI datasets with limited sparse groundtruth (less than 1/3 pixels is annotated for totally 200 images). Thus, we can employ the proposed uncertainty estimation to filter out the high-uncertainty pixels of the pre-trained model and generate sparse while reliable disparity maps as pseudo-labels to adapt the pre-trained model. As shown in Fig.~\ref{fig: intro all visual}, the provided ground truth data is sparse and cannot provide valid annotation in the upper region of the scene. Instead, the proposed method can generate a denser disparity map as pseudo-labels, which can filter out most errors of UCFNet\_pretrain and cover all regions of the input picture. Consequently, the proposed method can tremendously improve the performance of our pre-training model on textureless area of foreground (red dash boxes) and unlabeled area of background (green dash boxes) by solely employing self-generated proxy labels as ground truth. More specifically, pixel-level and area-level uncertainty estimation are employed to generate reliable pseudo-labels. Given current disparity estimation results, we first employ pixel-level uncertainty estimation to quantify the degree to which the current disparity probability distribution tends to be multi-modal and employ it to evaluate the pixel-level confidence of current estimations. Then, area-level uncertainty estimation is proposed to leverage the multi-modal input and neighboring pixel information to further refine the initial uncertainty map. By the cooperation between pixel-level and area-level uncertainty estimation, we can obtain a denser and more robust pseudo label for domain adaptation without requiring cumbersome and expensive depth annotations. 


Experimentally, we perform extensive experimental evaluations on various benchmarks to verify the generalization of the proposed method. When trained on synthetic datasets and generalized to unseen real-world datasets, our pre-trained model shows strong cross-domain generalization and can generate a good initial value for subsequent adaptation. Then, our model can further promote its performance by solely feeding the target domain synchronized stereo images and generated pseudo-labels, i.e., without the need for ground truth. In specific, the proposed method outperforms other domain generalization/adaptation methods by a noteworthy margin on various stereo matching benchmarks. The Qualitative comparison among GANet\_pretrain, UCFNet\_pretrain, and UCFNet\_adapt on three real datasets is shown in Fig. \ref{fig:  adapt generalization intro}. It can be seen from the figure that the generalization of current dataset-specific methods is limited to unseen real scenes, while our pre-training method can correct most errors and generate a more reasonable result. Moreover, compared with the pre-training model UCFNet\_pretraining, the proposed UCFNet\_adapt can achieve consistent improvement on multiple datasets with different characteristics, which further verifies the effectiveness of the generated pseudo-labels. More visualization results can be seen in the video demo of supplementary. Additionally, our uncertainty-based pseudo-labels can further be extended to replace the ground truth of monocular depth estimation networks and train these networks in an unsupervised way. Experiments show that the deep monocular depth estimation network trained by our pseudo-labels can outperform all self- supervised monocular depth estimation algorithms by a noteworthy margin and even achieves comparable performance with supervised methods. The code will be available at   \href{https://github.com/gallenszl/UCFNet}{https://github.com/gallenszl/UCFNet}. 


% $\footnote{\href{https://github.com/tjqansthd/LapDepth-release/} {LapDepth} uses GPL-3.0 License and we download the code from their official GitHub website.}$ and BTS$\footnote{\href{ https://github.com/cleinc/bts/}{BTS} uses GPL-3.0 License and we download the code from their official GitHub website.}$

In summary, our main contributions are:
\begin{itemize}[leftmargin=*]%[wide, labelwidth=!, labelindent=0pt]
\item We propose an uncertainty-based cascade and fused cost volume representation to reduce the domain differences and balance different disparity distributions across a variety of datasets. Thus, a robust pre-trained model with strong cross-domain generalization can be obtained.
\item We propose an uncertainty-based pseudo-labels generation method to further narrow down the domain gap. By employing the generated pseudo-labels to adapt our pre-trained model to the new domain, we can greatly promote the performance of our method.
\item Our method shows strong cross-domain and adapt generalization and outperforms other domain generalization/domain adaptation methods by a noteworthy margin on various stereo matching benchmarks.
\item Our method can perform well on multiple datasets with fixed model parameters and hyperparameters and obtains 1st place on the stereo task of Robust Vision Challenge 2020 \footnote{\href{http://www.robustvision.net/rvc2020.php}{http://www.robustvision.net/rvc2020.php}}.
\item Our uncertainty-based pseudo-labels can further be extended to train monocular depth estimation networks in an unsupervised way and even achieves comparable performance with supervised methods.
\end{itemize}

%This is because our approach only needs to apply a shallow encoder-decoder architecture at a small resolution to predict an initial disparity map while DSMNet needs to repeatedly apply learnable no-local layers and 3D convolution layers at high resolution. 

\textbf{Differences with conference version \cite{cfnet}:} This paper extends the early ideas and findings presented in CFNet\cite{cfnet}. The differences with our conference paper can be summarized as follows:
\begin{itemize}[leftmargin=*]
\item In our previous work, we only focus on cross-domain generalization and joint generalization of stereo matching tasks. Here, we provide a general solution for cross-domain, joint, and adaptation generalization jointly by digging into uncertainty estimation in stereo matching. Hence, a more complete and standard solution is presented for robust stereo matching.  
\item In our previous work, the final disparity estimation is just half-resolution of the input image and needs to be upsampled to the original image size. Thus, we propose a simple, yet effective attention-based refinement module to recover the details loss caused by the bilinear sampling.
\item We extend our uncertainty-based pseudo-labels to train the monocular depth estimation network in an unsupervised way. Experiments show that the deep monocular depth estimation network trained by our pseudo-labels can outperform all self-supervised monocular depth estimation algorithms by a noteworthy margin and even achieves comparable performance with the supervised methods.
\end{itemize}
