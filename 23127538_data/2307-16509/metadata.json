{
  "title": "Digging Into Uncertainty-based Pseudo-label for Robust Stereo Matching",
  "authors": [
    "Zhelun Shen",
    "Xibin Song",
    "Yuchao Dai",
    "Dingfu Zhou",
    "Zhibo Rao",
    "Liangjun Zhang"
  ],
  "submission_date": "2023-07-31T09:11:31+00:00",
  "revised_dates": [],
  "abstract": "Due to the domain differences and unbalanced disparity distribution across multiple datasets, current stereo matching approaches are commonly limited to a specific dataset and generalize poorly to others. Such domain shift issue is usually addressed by substantial adaptation on costly target-domain ground-truth data, which cannot be easily obtained in practical settings. In this paper, we propose to dig into uncertainty estimation for robust stereo matching. Specifically, to balance the disparity distribution, we employ a pixel-level uncertainty estimation to adaptively adjust the next stage disparity searching space, in this way driving the network progressively prune out the space of unlikely correspondences. Then, to solve the limited ground truth data, an uncertainty-based pseudo-label is proposed to adapt the pre-trained model to the new domain, where pixel-level and area-level uncertainty estimation are proposed to filter out the high-uncertainty pixels of predicted disparity maps and generate sparse while reliable pseudo-labels to align the domain gap. Experimentally, our method shows strong cross-domain, adapt, and joint generalization and obtains \\textbf{1st} place on the stereo task of Robust Vision Challenge 2020. Additionally, our uncertainty-based pseudo-labels can be extended to train monocular depth estimation networks in an unsupervised way and even achieves comparable performance with the supervised methods. The code will be available at https://github.com/gallenszl/UCFNet.",
  "categories": [
    "cs.CV"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.16509",
  "pdf_url": "https://arxiv.org/pdf/2307.16509v1",
  "comment": "Accepted by TPAMI",
  "num_versions": null,
  "size_before_bytes": 24768118,
  "size_after_bytes": 547034
}