
\label{sec:methods}

% Figure environment removed


For a dataset $D$ containing $n$ images, a set of image embeddings $F = \{ f_i \}_{i=1}^n$ are extracted by a feature extractor. Both inlier and outlier images are present in $D$, with inliers representing the majority. The goal of RANSAC-NN is to assign outlier scores $\sigma$ approaching $1$ to the outlier images and $\sigma$ approaching $0$ to the inlier images. 


\subsection{RANSAC-NN}
\label{methods-ransacnn_algo}
The RANSAC-NN algorithm (see Figure \ref{fig:ransac-nn-fig}) consists of two main components: Inlier Score Prediction (ISP) and Threshold Sampling (TS). 
ISP helps identify potential inliers in the dataset when the data distribution is initially unknown. By sampling the set of embeddings $F = \{ f_i \}_{i=1}^n$ repetitively and aggregating the sampled results, ISP assigns a value $\eta_i \in [0, 1 ]$ to each embedding $f_i$ that indicates the likelihood of the $i$-th image to be an inlier. This value $\eta_i$ is referred to as the inlier score of the $i$-th image. 

The inlier scores $\boldsymbol{\eta} = \{ \eta_i \}_{i=1}^n$ predicted by ISP estimates the inlier distribution of $F$. TS leverages the inlier scores $\boldsymbol{\eta}$ as a prior, and it samples $F$ in a similar manner to ISP. However, TS can avoid sampling embeddings with low inlier scores by using an enhanced sampling strategy conditioned on $\boldsymbol{\eta}$. This enhanced sampling strategy ultimately enables TS to predict outlier scores $\boldsymbol{\sigma}$ with greater precision. While it is much simpler to consider the inverse of the inlier score as an outlier score measure, our experiments show that the outlier scores $\boldsymbol{\sigma}$ predicted by TS provide much better inlier-outlier separation in comparison.

\paragraph{Inlier Score Prediction (ISP)}
\label{isp_stage}

The objective of ISP is to predict a set of inlier scores $\boldsymbol{\eta} = \{ \eta_i \}_{i=1}^n$ for a given set of image embeddings $F = \{ f_i \}_{i=1}^n$. Since the inlier distribution is initially unknown, ISP samples $F$ uniformly in an attempt to obtain a set of embeddings $\Tilde{I}$ consisting of all inliers, which we refer to as a \textit{clean} set. 
In total, ISP samples $F$ for $s$ iterations, each time taking $m$ embeddings without repetition. This set of $m$ sampled embeddings, denoted as $\Tilde{I}$, are assumed to all be inliers, and they are compared against the remaining embeddings in $F$. Comparison is made by taking the cosine similarities for each $f_i \in F$ to its nearest-neighbor (NN) embedding in the sample set $\Tilde{I}$. 
A similarity score $\alpha_i \in [-1, 1]$ stores this value for every $f_i \in F$.
An embedding $f_i$ with an $\alpha_i$ approaching $1$ indicates that $f_i$ is very similar to one of the sampled $\Tilde{f} \in \Tilde{I}$. Moreover, if $\Tilde{I}$ is a \textit{clean} set, inliers in $F$ would all have a high value of $\alpha_i$. 

In reality, however, the sample set $\Tilde{I}$ may contain outliers. Thus, by repeatedly sampling $F$, we can increase the odds of obtaining one clean set $\Tilde{I}$ during one of the $s$ sampling iterations. For each $f_i \in F$, the similarity scores $\alpha_i$ obtained in each iteration are aggregated by taking the minimum $\alpha_i$ obtained across all $s$ iterations. This minimum $\alpha_i$ of each $f_i \in F$ essentially measures the worst-performing similarity score from all $s$ iterations. As long as an inlier is present in every sampled $\Tilde{I}$, a lower bound of the minimum $\alpha_i$ for $f_i \in F_\text{in}$ exists. In the contrary, if an outlier $f_i \in F_\text{out}$ ever attains a low $\alpha_i$ during one of the $s$ iterations, which would happen if $\Tilde{I}$ is a \textit{clean} set, the minimum $\alpha_i$ would account for this drop in similarity score. This minimum $\alpha_i$ is referred to as the inlier score $\eta_i$ for image $i$. The details of ISP are provided in Algorithm \ref{alg:inlier_score_prediction}. 

\input{alg-inlier_score_pred}

\paragraph{Threshold Sampling}
\label{threshold_sampling}

TS leverages $\boldsymbol{\eta}$ from ISP to predict the final set of outlier scores $\boldsymbol{\sigma} = \{ \sigma_i \}_{i=1}^n$ for each embedding in $F = \{ f_i \}_{i=1}^n$. Unlike ISP, where the inlier distribution is initially unknown, TS has the advantage of using the inlier scores $\boldsymbol{ \eta }$ as a prior. The idea behind TS is to compare each $f_i \in F$ starting from a loosely assembled set of inlier candidates to a gradually refined set. This gradual transition allows for inliers that might have a relatively lower $\eta_i$ to still be included in the sample set $\Tilde{I}$ during early sampling iterations. Inliers with larger $\eta_i$, or embeddings that resemble most of the inliers, would be sampled frequently. 

In each sampling iteration, a threshold $\tau \in [0, 1]$ filters out embeddings in $F$ that have an inlier score $\eta_i \leq \tau$. This leaves a set of eligible embeddings $\Omega = \{f_i \mid \eta_i > \tau \} \subseteq F$ available for sampling. Similar to ISP, the algorithm takes at most $m$ samples uniformly from $\Omega$ and assembles a set $\Tilde{I}$. In contrast to ISP, $\Tilde{I}$ in TS would contain a higher concentration of actual inliers since $\Omega$ has already filtered out embeddings with low $\eta_i$. 
Each embedding $f_i \in F$ is then compared against the sample set $\Tilde{I}$, and the similarity score $\alpha_i$ is computed for each $f_i \in F$. 
For each embedding $i$, a running mean $\sigma_i$ keeps track of the average number of times the embedding $f_i$ obtains an $\alpha_i < \tau$. 

This sampling and comparison process is repeated for every increase in threshold $\tau$. The final $\sigma_i \in [0, 1]$ is the outlier score predicted by TS, which equals the average number of times each $f_i \in F$ fails to find a close-enough NN during TS. An $f_i \in F$ with $\sigma_i$ approaching $1$ implies that the embedding failed during the majority of TS sampling iterations, which is a common characteristic of outliers. Details of TS are listed in Algorithm \ref{alg:threshold_sampling}.

\input{alg-thresold_sampling}

\subsection{Properties Analysis}
\label{methods:sample_size_iteration_properties}
In this section, we explore the properties of RANSAC-NN in detail. Assumptions regarding the inlier and outlier embeddings are presented, and we analyze the different possibilities of obtaining a sample set $\Tilde{I}$ during ISP. We then analyze the influence of the sample size $m$ and sampling iterations $s$ on RANSAC-NN. The sample size $m$ determines how large of a set $\Tilde{I}$ to sample for, and the sampling iteration $s$ controls the probability of obtaining a \textit{clean} inlier set. Since $m$ and $s$ mainly influence the inlier scores $\boldsymbol{ \eta }$, we focus on ISP for most of this section.

For the purpose of analysis, we assume that $F = F_{\text{in}} \cup F_{\text{out}}$ and $F_\text{in} \cap F_\text{out} = \varnothing$ where $F_{\text{in}}$ and $F_{\text{out}}$ are the inlier and outlier subsets of $F$. Assume further that all embeddings $f_i \in F$ satisfy the following bounds:
\begin{equation}
    \label{eq-assumption1}
    \cos{(f_1, f_2 )} > g \quad \forall \Hquad f_1, f_2 \in F_{\text{in}}
\end{equation}
\begin{equation}
    \label{eq-assumption2}
    \cos{(f_1, f_3 )} < h \quad \forall \Hquad f_1 \in F_{\text{in}}, f_3 \in F_{\text{out}}
\end{equation}
where $-1 \leq h \ll g \leq 1$. The bound in Equation \ref{eq-assumption1} presumes that all inlier embeddings in $F_\text{in}$ have a pair-wise cosine similarity greater than $g$. Equation \ref{eq-assumption2} entails that the cosine similarity of an inlier embedding and an outlier embedding does not exceed the value of $h$. To account for similar outliers, we do not assume any bounds on the pair-wise similarity between outlier embeddings. 

\paragraph{Influence of sample sets on ISP}

During ISP, sets of embeddings $\Tilde{I}$ are randomly sampled from $F$. We consider the different cases of $\Tilde{I}$, and we examine their influence on the predicted inlier scores during ISP.

% Figure environment removed

% \vspace{1mm}
\subparagraph{(1) Samples are all inliers}
In the case where $\Tilde{I}$ is a \textit{clean} set, the similarity score $\alpha_i$ effectively seperates inliers from outliers by a large margin because $\alpha_i > g \Hquad \forall \Hquad f_i \in F_\text{in}$ and $\alpha_i < h \Hquad \forall \Hquad f_i \in F_\text{out}$ (Line $6$, Algorithm \ref{alg:inlier_score_prediction}). Furthermore, if an inlier is present in every sampled $\Tilde{I}$ throughout all $s$ iterations during ISP, the inlier scores $\eta_i \Hquad \forall \Hquad f_i \in F_\text{in}$ would be lower-bounded by $g$, and $\eta_i \Hquad \forall \Hquad f_i \in F_\text{out}$ would be upper-bounded by $h$. This is because ISP takes the minimum $\alpha_i$ across all $s$ iterations as $\eta_i$ (Line $7$, Algorithm \ref{alg:inlier_score_prediction}). Thus, if a sampled $\Tilde{I}$ is ever a \textit{clean} set in one of the $s$ iterations during ISP, inliers and outliers can be distinguishable from the predicted inlier scores $\boldsymbol{\eta}$.

% \vspace{1mm}
\subparagraph{(2) Samples contain both inliers and outliers}
Most of the sampled $\Tilde{I}$ during ISP fall under this case. When $\Tilde{I}$ contains both inlier and outlier samples, the similarity score $\alpha_i \Hquad \forall \Hquad f_i \in F_\text{in}$ are lower bounded by $g$ (Line $6$, Algorithm \ref{alg:inlier_score_prediction}). The outliers $f_i \in F_\text{out}$, however, are not necessarily upper bounded by $h$ due to the presence of outlier samples in $\Tilde{I}$. But this is fine because the inlier score $\eta_i$ predicted by ISP is the minimum $\alpha_i$ from all $s$ sampling iterations. As long as ISP samples a \textit{clean} set during one of the $s$ iterations (Case 1), the inlier scores $\eta_i$ of the outliers $f_i \in F_\text{out}$ will still be upper-bounded by $h$. 


\subparagraph{(3) Samples are all outliers}
In the case where $\Tilde{I}$ contains only outliers, ISP would fail to produce meaningful inlier scores $\boldsymbol{\eta}$. The reason is that the similarity score $\alpha_i$ for all the inliers $f_i \in F_\text{in}$ would be upper bounded by $h$ when compared against a sample set $\Tilde{I}$ containing all outliers. The outliers $f_i \in F_\text{out}$, on the other hand, are not necessarily upper-bounded by $h$. The predicted inlier scores $\eta_i \Hquad \forall \Hquad f \in F_\text{in}$ by ISP, which is the minimum $\alpha_i$ across all $s$ sampling iterations, would thus be upper-bounded by $h$. This implies that the inlier scores $\boldsymbol{\eta}$ have failed to produce a meaningful distinction between inliers and outliers. Furthermore, the inliers $f_i \in F_\text{in}$ would be filtered out when the threshold $\tau$ exceeds $h$ during TS, leading to deteriorating outlier prediction performance. The situation where $\Tilde{I}$ contains all outliers should be avoided at all costs during ISP sampling.

\paragraph{Sample Size}
Now that all possible cases of $\Tilde{I}$ have been explored, we explain how setting the sample size $m$ can avoid sampling an $\Tilde{I}$ containing all outliers (Case 3) and improve the odds to obtaining a \textit{clean} set (Case 1). We denote the probability to sample a \textit{clean} set as $p_\text{clean}$ and the probability to sample a set containing all outliers as $p_\text{out}$. Suppose there exists $l$ outliers in a dataset of size $n$. During each sampling iteration, the probability of sampling a \textit{clean} set is given by:
\begin{equation}
    \label{eq-pclean}
    p_\text{clean} = \frac{{n-l \choose m}}{{n \choose m}} \approx \left( 1-\frac{l}{n} \right)^m
\end{equation}
and the probability of sampling $\Tilde{I}$ with all outliers is given by:
\begin{equation}
    \label{eq-outliers}
    p_\text{out} = \frac{{l \choose m}{n-l \choose n-m}}{{n \choose m}} \approx \left( \frac{l}{n} \right)^m
\end{equation}

From Equations \ref{eq-pclean} and \ref{eq-outliers}, we notice that when $l < n/2$, $p_\text{out} < p_\text{clean} \Hquad \forall \Hquad m > 0$. This implies that in the case where outliers constitute the minority, the probability of sampling a \textit{clean} set is always greater than that of sampling a set with all outliers. For large sample sizes $m$, $p_\text{out}$ converges to $0$ at a fast rate, which implies a reduced chance of outliers populating $\Tilde{I}$. $p_\text{clean}$ also approaches $0$, but at a slower rate than $p_\text{out}$, implying a reduced probability of sampling a \textit{clean} set.

On the contrary, if $m$ is assigned an extremely small value, then $p_\text{clean}$ would greatly increase. This indicates an increased probability of obtaining a \text{clean} set. But doing so would also increase $p_\text{out}$. Since sampling an $\Tilde{I}$ containing all outliers is detrimental to ISP, we avoid setting extremely small values of $m$.

\paragraph{Sampling Iteration}
For a given sample size $m$, the probability to sample a \textit{clean} set $p_\text{clean}$ is fixed for each iteration. Then for a certain confidence $c$ (e.g. $c = 0.95$), the minimum number of sampling iterations $s_\text{min}$ such that at least one \textit{clean} set has been sampled is given by:

\begin{equation}
    \label{eq-sample_iter}
    s_\text{min} \geq \lceil \log_{1-p_\text{clean}}(1 - c) \rceil
\end{equation}

Thus, as long as $m$ is set at a reasonable sample size, increasing the number of sampling iterations $s$ only improves the odds of sampling a \text{clean} set. In Section \ref{exp:hyperparameters} of the experiments, we demonstrate the above principles on actual image datasets.

\paragraph{Computational Complexity}
RANSAC-NN has a time complexity of $\mathcal{O}\big(snm \big)$ from ISP and $\mathcal{O}\big(tnm\big)$ from TS. 
