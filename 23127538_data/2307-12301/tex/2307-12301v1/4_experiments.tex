\label{sec:experiments}
We provide a comprehensive analysis of our proposed algorithm in the following. We begin with a motivational experiment to assess the impact of outlier presence in the image classification task. Next, we compare the OD performance of RANSAC-NN to other SOTA OD algorithms. We then perform an ablation study to understand the impact of each component in RANSAC-NN. Lastly, we demonstrate several potential use cases of our OD algorithm in image mislabeled detection.

% Figure environment removed

\subsection{Datasets}
\label{exp_datasets}

We employ $15$ publicly available datasets (listed in the Supplementary Materials) to evaluate the effectiveness of our algorithm.
An image-level label is provided for each image by their respective authors, and we adopt the provided labels as the ground truth. Prior to their usage, each dataset was checked visually to ensure coherence among images belonging to the same class. Images that were visually distinct were discarded (e.g., several monochrome inverted x-ray images were removed from MURA 1.1\cite{data-mura11}). 
We focus our experiments on at most $10$ classes from each dataset containing the most sample images. This is to ensure that a reasonable amount of images are available to which applying OD would be necessary. The maximum number of images each class contains is set to 5000 samples for all evaluations. 


For each dataset listed in Table \ref{table-datasets_list}, we employ a synthetic method to generate outlier images. Assume a dataset $D = \bigcup_{i=1}^c S_i$ containing $c$ disjoint subsets $S_i$ which resembles the set of images belonging to class $i$. We introduce outliers to each set $S_i$ by adding $l_i$ images sampled from $D \setminus \{S_i\}$ into the set $S_i$. The number of outliers $l_i$ we add to each set $S_i$ is proportional to $|S_i|$. This proportion is referred as the outlier percentage. For example, an outlier percentage of $0.1$ equates to adding outliers amounting to $10\%$ of each set $S_i$ from other sets $S_j$ where $i \neq j$ for $i,j \in \{1, \ldots, c\}$. Hence, the inlier images in each set $S_i$ remain intact, and the amount of outliers introduced to each set increases according to the specified outlier percentage.


\subsection{Impact of Outliers in Image Classification}
\label{exp-motivation_exp}

In this experiment, we explore the possibility of performance decrease when outliers are introduced in an image classification task. We conduct the image classification experiments on 4 diverse datasets: Food101 \cite{data-food101}, RESISC45 \cite{data-resisc45}, SUN397 \cite{data-sun397}, and Caltech101 \cite{data-caltech101}. For each dataset and outlier percentage setting, we train $10$ ResNet-18 \cite{cnn-resnet} models from scratch and record their average test set performance. 

As shown in Figure \ref{fig:performance_drop}, we observe an overall performance decrease across all datasets as the outlier percentage increases. Since the inliers were kept intact throughout all experiments, this implies that the presence of outliers do pose a negative impact on the training task in terms of both model performance and computation time. Hence, the removal of outliers is essential for improving training performance and efficiency.


\subsection{Comparison of Image Outlier Detection}
\label{exp-image_od}

This section presents a comparative analysis of our proposed method with several existing approaches including LOF, INNE, Isolation Forest, LUNAR, LODA, and KNN-Distance algorithms \cite{local_outlier_factor, algo-inne, isolation_forest, algo-lunar, loda, knn-distance-algo}\footnote{LOF, LODA,t INNE, Isolation Forest, and LUNAR were implemented in PyOD\cite{lib-pyod} and KNN-Distance was implemented in CleanLab\cite{cleanlab-library}.}. Throughout all experiments, the sample size $m$ of RANSAC-NN was set to be $n/10$ where $n$ is the number of images in the dataset. The sampling iteration $s$ was set to $10$, and the number of threshold iterations $t$ was set to $500$. To increase the robustness of our evaluation, we repeat the experiment $10$ times, with each iteration utilizing a distinct set of outlier images. The average results obtained from the $10$ iterations across all datasets are then aggregated to provide an overall assessment of the algorithms' performance.


\paragraph{Varying Outlier Percentage Levels}
\label{exp:vary_outlier_percentage}
The first experiment compares the performance of each OD algorithm on the 15 datasets specified in Table \ref{table-datasets_list} under different outlier percentage levels. 
We employed a shared feature extractor, ResNet-50, to extract features for each of the algorithms. For each dataset, with a specified outlier percentage, we applied each OD algorithm to each class and computed the average performance across all classes.

Figure \ref{fig:pertubation_pr_auc} shows the average PR-AUCs of each algorithm.

RANSAC-NN demonstrates significant overall performance in contrast to the other OD algorithms across the range of outlier percentages levels\footnote{The largest $p$-value from the $60$ ANOVA tests (4 outlier percentage levels, 15 datasets) conducted was $0.0002$. Details of additional Tukey post-hoc tests  can be founded in the Supplementary Materials.}. In each outlier percentage level, RANSAC-NN maintained the top-performing algorithm in each dataset category except in LC25000 \cite{data-lc25000}, where it ranked $3$rd. Additional plots highlighting the average ROC-AUC, Precision@$k$, and Recall@$k$ have been included in the Supplementary Materials. 



% Figure environment removed

\paragraph{Difference in Feature Extractors}
\label{exp:feature_extractor_sweep}

In the second experiment, we analyze the OD performance of each algorithm under a variety of features extracted by different feature extractors. Specifically, we consider ImageNet pre-trained ResNet-18, ResNet-50, VGG-11, VGG-16, EfficientNetv2-B0, EfficientNetv2-B2, ViT-Base-Patch16-224, Swin-Transformer-V2-Small, and Swin-Transformer-V2-Tiny models \cite{cnn-resnet, cnn-vgg, cnn-efficientnetv2, cnn-vit, cnn-swinv2}. 


Figure \ref{fig:feat_extractor} shows the average performance of each algorithm evaluated on the $15$ datasets with the outlier percentage at $0.2$.
RANSAC-NN performs consistently well in contrast to other algorithms across a wide variety of feature extractors. In addition, our method outperforms other algorithms in average performance on all datasets except LC25000\footnote{The largest $p$-value from the 135 ANOVA tests (9 feature extractors, 15 datasets) conducted was $0.012$. Details of Tukey post-hoc tests are included in the Supplementary Materials.}. From the observed results, we can attribute the improved OD performance achieved by RANSAC-NN to the design of ISP and TS, which is independent of feature extractor choice.


\subsection{Ablation Studies}
\label{exp-ablation}

% Figure environment removed

\paragraph{Improvements from Threshold Sampling}
In this experiment, we analyze the performance improvements contributed by Threshold Sampling (TS). We invert the inlier scores predicted by ISP and compare them to the predicted outlier scores by TS. In Figure \ref{fig:threshold-sampling-improvement}, we show the performance improvements delivered by the TS in the $15$ datasets with an outlier percentage of $0.2$. In almost every dataset evaluated, we observe a noticeable performance gain with the addition of TS. This observation reflects the importance of TS in predicting outlier scores that result in better inlier-outlier separation in contrast to using the inverted inlier scores alone.

\paragraph{Sample Size and Sampling Iteration}
\label{exp:hyperparameters}

% Figure environment removed

The two primary hyperparameters of RANSAC-NN are the sample size $m$ and the sampling iterations $s$. As described in Section \ref{methods:sample_size_iteration_properties}, the sample size $m$ influences the probability to sample a \textit{clean} inlier set, and the sampling iteration $s$ impacts the odds of sampling at least one \textit{clean} inlier set. In this experiment, we study their influence on the performance of RANSAC-NN. 

For the purpose of evaluating our algorithm under heavy outlier presence, we introduce outliers to each dataset with an outlier percentage of $0.4$, and we evaluate RANSAC-NN with varying sample sizes $m$ and sampling iterations $s$. We consider $m$ in terms of dataset percentage (e.g., the sample size of $0.05$ indicates sampling $5\%$ of the dataset) with values ranging from $0.01$ to $0.5$. We also consider $s$ ranging from $1$ to $1024$, where larger $s$ implies longer compute time. For each dataset, we repeat the experiment 5 times, and we record the mean PR-AUCs at each hyperparameter setting.

Figure \ref{fig:hyperparameter_ablation} shows the results evaluated on the Caltech101 dataset\footnote{The results for additional datasets are included in the Supplementary Materials.}. First, it is noteworthy that there is a consistent decrease in performance as the sample size $m$ increases. This observation coincides with our analysis where a large value of $m$ causes $p_\text{clean}$ to decrease (see Equation \ref{eq-pclean}). On the contrary, we notice that setting $m$ too small (e.g., $s=0.01$) can negatively impact the performance, especially as $s$ increases. The reason is that $p_\text{out}$ increases as the value of $m$ decreases to an extremely small value (see Equation \ref{eq-outliers}). This case, where the sampled set $\Tilde{I}$ are all outliers, should be avoided at all costs.
Second, we observed that for a reasonable sample size $m$, the performance tends to increase as the sampling iteration $s$ increases.
By increasing $s$, we can improve the probability of obtaining a \textit{clean} inlier sets during one of the sampling iterations (see Equation \ref{eq-sample_iter}), thus enhancing the inlier scores and improving overall performance. Depending on the computing constraints, we suggest setting $s$ as large as possible for optimal performance.  


\subsection{Applications in Image Mislabeled Detection}
In this section, we explore the use of RANSAC-NN for image mislabeled detection on two popular computer vision tasks: image classification and object detection. We detail their setup in the following.

\paragraph{Image Classification.} Labels in image classification tasks are image-level labels. A mislabeled image would take the form as an outlier image among the set of images it was mislabeled as. Thus, by applying RANSAC-NN to the set of images belonging to each category, we can effectively detect images that have been mislabeled. Several case studies are provided in the Supplementary Materials.

\paragraph{Object Detection.} Labels in object detection tasks are bounding boxes, which contain cropped image contents and a crop label. A mislabeled bounding box could either be a box that is incorrectly labeled or a poorly cropped box. For this task, we explore the use of Faster-RCNN \cite{fasterrcnn} in combination with RANSAC-NN for bounding box mislabeled detection. An illustration of the algorithm and several example detections are provided in the Supplementary Materials. 

