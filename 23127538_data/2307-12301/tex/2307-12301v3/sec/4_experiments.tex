\section{Experiments}
\label{sec:experiments}

We provide a comprehensive analysis of RANSAC-NN in the following. To begin, we conduct a benchmark of RANSAC-NN in comparison to other well-established OD methods in the setting where a clean inlier set is provided. Next, we consider the event where outlier contamination occurs in the training set, and we compare the performance difference caused by contaminated training. We then explore the use of RANSAC-NN for outlier removal in contaminated sets prior to OD model training. Lastly, we perform ablation studies to understand the components behind our algorithm.

\subsection{Experiment Setup}
\label{exp:experiment_setup}

\paragraph{Data and Metrics.}
For all our experiments, we follow the one-class classification setup \cite{2018_deepsvdd, one_class_svm, one_class_classification}. Benchmarks were conducted on natural image datasets SUN397 \cite{data-sun397}, Caltech101 \cite{data-caltech101}, and ImageNet21K\footnote{The original 1000 classes from ImageNet-1K were excluded.} \cite{data-imagenet21k}. 
For each dataset, we consider the top 10 classes with the most images, and we separate a train and test set for each class. We perform OD on each class, and samples from other classes are considered outliers. The ground truth labels from the datasets were adopted solely for performance evaluation. Following \cite{goad, 2021_exploring_limits_of_ood, 2019_mem_ae, knn-distance-algo, deecke2018anomaly}, we report the average ROC-AUC's from the 10 class evaluations. Each experiment is repeated 6 times to avoid over-fitting. 

\paragraph{Algorithms}
We evaluate RANSAC-NN alongside several well-established density estimation \cite{local_outlier_factor, algo-inne, isolation_forest, knn-distance-algo, 2018_deepsvdd, algo-lunar}, image reconstruction \cite{Outlier_Analysis_aggarwal, rca}, and self-supervised classification \cite{goad, 2021_neural_transformation} methods\footnote{Implemented by the PyOD \cite{zhao2019pyod} and DeepOD \cite{deepod} libraries.}. The RANSAC-NN sample size $m$ was set at $5\%$ of the dataset size for both ISP and TSP. The ISP sampling iterations $s$ was set at $\left \lceil{n/m}\right \rceil$ (each sample is expected to be sampled once). The TSP threshold iterations $t$ was kept constant at $500$.  

Prior to our experiments, we optimized the hyperparameters of every algorithm with $20$ automatic searches using HyperOpt \cite{hyperopt}. Image embeddings were extracted and normalized using a pre-trained ResNet-18 \cite{cnn-resnet} with the final fully-connected layer removed. Experiments on additional feature extractor choices and run-time performance have also been provided in the Appendix.

\paragraph{Terminology}
Outliers added to the training set are referred to as \textit{contamination}, and outliers added to the test set are referred to as \textit{perturbation}. Unless specified, it is assumed that the contamination level equals the perturbation level to ensure equal representation during evaluation. \textit{Algorithms} refer to the OD method, and \textit{models} are the results from training an algorithm on a dataset. 


\subsection{Outlier Detection with Clean Training}
\label{exp:outlier_detection_benchmark}
In this experiment, we evaluate the performance of RANSAC-NN alongside several well-known OD algorithms. For each existing algorithm, we train its model on a \textbf{clean inlier set} and compare their performance under different levels of outlier perturbations. Since RANSAC-NN does not require training, we directly apply our algorithm to the perturbed test set.
%The results are listed in Table \ref{table:od_benchmark}.

\begin{table}[t]
    \centering
    \begin{adjustbox}{width=0.47\textwidth}
    \begin{tabular}{lllllc}
\toprule
\multirow{2}{*}{Algorithm} & \multicolumn{4}{c}{Outlier Perturbation (ROC)} & \multirow{2}{*}{Max $\sigma$}\\
\cmidrule(lr){2-5} 
{} &         $5\%$ &     $10\%$ &     $20\%$ &     $40\%$ & {} \\
\midrule
Isolation Forest \cite{isolation_forest} &      $0.966$ &  $0.964$ &  $0.963$ &  $0.960$ &  $\pm0.02$ \\
INNE \cite{algo-inne}             &      $0.973$ &  $0.976$ &  $0.976$ &  $0.970$ &  $\pm0.01$ \\
LOF  \cite{local_outlier_factor}            &      $0.977$ &  $0.976$ & $0.976$  &   $\boldsymbol{0.976}^{*}$ &  $\pm0.01$ \\
AutoEncoder  \cite{Outlier_Analysis_aggarwal}    &      $0.968$ &  $0.965$ &  $0.968$ &  $0.959$ &  $\pm0.02$ \\
LUNAR  \cite{algo-lunar}      &      $0.970$ &  $0.971$ &  $0.970$ &  $0.967$ &  $\pm0.02$ \\
KNN Distance \cite{knn-distance-algo}    &      $\boldsymbol{0.983}^{+}$ &  $\boldsymbol{0.984}^{+}$ &  $\boldsymbol{0.983}^{+}$ &  $\boldsymbol{0.981}^{+}$ &  $\pm0.01$ \\
GOAD \cite{goad}            &      $0.967$ &  $0.967$ &  $0.965$ &  $0.964$ &  $\pm0.02$ \\
Deep SVDD  \cite{2018_deepsvdd}      &      $0.922$ &  $0.926$ &  $0.919$ &  $0.913$ &  $\pm0.03$ \\
RCA  \cite{rca}            &      $0.922$ &  $0.918$ &  $0.915$ &  $0.895$ &  $\pm0.06$ \\
NeuTraL  \cite{2021_neural_transformation}  &      $0.931$ &  $0.936$ &  $0.931$ &  $0.926$ &  $\pm0.04$ \\
RANSAC-NN (Ours)      &      $\boldsymbol{0.980}^{*}$ &  $\boldsymbol{0.978}^{*}$ &  $\boldsymbol{0.977}^{*}$ &  $0.941$ &  $\pm0.04$ \\

\bottomrule
\end{tabular}
\end{adjustbox}

    \caption{\textbf{Outlier Detection with Clean Training.} Shown above are the average ROC-AUC's obtained by models trained using the corresponding algorithm on a clean set of inlier images. Notice that all models maintain a relatively solid performance despite the varying perturbation levels. This implies that existing algorithms are capable of image OD given they are trained with quality data. Although RANSAC-NN was directly applied to the test set without training, it still maintained favorable performance compared to the models that had undergone training.}
    \label{table:od_benchmark}
\end{table}

Table \ref{table:od_benchmark} shows the experiment results.
The first observation is that the majority of OD algorithms perform almost equally well when trained on an inlier set. For every algorithm, the performance differences are relatively minor under all perturbation levels. This implies that most existing algorithms are capable of maintaining a perturbation-robust image OD performance as long as a clean inlier set is available for training.

The second observation is that RANSAC-NN, despite the lack of training, can achieve similar performance to models that had undergone training. This is important as it entails the possibility of applying RANSAC-NN directly to a contaminated training set prior to the training of other OD models. In the next experiment, we demonstrate the importance of training on a clean inlier set by analyzing the impact of outlier contamination.

\subsection{Influence of Contaminated Training}
\label{exp:impact_of_contaminated_training}

% Figure environment removed

In this experiment, we demonstrate the importance of maintaining an outlier-free dataset when training an OD model. We consider the same setup as in Section \ref{exp:outlier_detection_benchmark}, but we replace the clean training sets with \textbf{contaminated sets}. We then train a model for each algorithm at different contamination levels and compare their differences to the models that were trained on the clean sets. 

The performance difference due to outlier contamination is illustrated in Figure \ref{fig:contaminated_training}. We can observe that at a low contamination level, the performance drop is already noticeable in some algorithms. As the contamination level increases, the performance difference becomes more apparent. In comparison to the behavior of models that were trained on a clean inlier set (see Table \ref{table:od_benchmark}), this drop in performance further emphasizes the importance of maintaining a quality inlier set when training OD models.

Another observation is the relatively more robust performance delivered by RANSAC-NN under varying outlier perturbation levels. Since RANSAC-NN does not require training, its performance is not influenced by contamination in the training set. This observation suggests the possible use of RANSAC-NN as a filter to remove probable outliers from contaminated sets prior to model training.

\subsection{Outlier Filtering with RANSAC-NN}
\label{exp:outlier_filtering}

In this experiment, we assume that a contaminated set was provided, and we explore the use of outlier removal using RANSAC-NN. Before training an OD model for each algorithm, we applied RANSAC-NN on the contaminated set to obtain an outlier score distribution. Images were ranked according to their outlier scores in ascending order, and the top-$p$ percent of these images were selected for model training (see Figure~\ref{fig:outlier_filtering_dist}).
For each algorithm, a series of models were trained by taking different values of
$p$ (higher $p$ indicates more samples kept for training), and their results are detailed in Table~\ref{tab:outlier_filtering_results}.


\begin{table*}[t]
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{lllllll}
\toprule
\multirow{2}{*}{Algorithm} & \multicolumn{2}{c}{10\% Contamination} & \multicolumn{2}{c}{20\% Contamination} & \multicolumn{2}{c}{40\% Contamination}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
{} & Take Top-50\% & Take Top-90\% & Take Top-50\% & Take Top-80\% & Take Top-50\% & Take Top-60\%\\

\midrule
Isolation Forest \cite{isolation_forest} & $0.905\:(-4.23\%)$ & $\boldsymbol{0.959\:(+1.14\%)}$ & $0.921\:(-0.54\%)$ & $\boldsymbol{0.953\:(+2.69\%)}$ & $\boldsymbol{0.875\:(+7.98\%)}$ & $0.858\:(+6.29\%)$ \\ 
INNE \cite{algo-inne} & $0.855\:(-5.57\%)$ & $\boldsymbol{0.950\:(+3.98\%)}$ & $0.883\:(+3.58\%)$ & $\boldsymbol{0.930\:(+8.29\%)}$ & $0.862\:(+10.00\%)$ & $\boldsymbol{0.871\:(+10.91\%)}$ \\ 
LOF \cite{local_outlier_factor} & $0.869\:(+8.88\%)$ & $\boldsymbol{0.937\:(+15.69\%)}$ & $0.897\:(+32.14\%)$ & $\boldsymbol{0.904\:(+32.76\%)}$ & $\boldsymbol{0.869\:(+34.16\%)}$ & $0.853\:(+32.55\%)$ \\ 
AutoEncoder \cite{Outlier_Analysis_aggarwal} & $0.848\:(-11.02\%)$ & $\boldsymbol{0.961\:(+0.23\%)}$ & $0.874\:(-6.81\%)$ & $\boldsymbol{0.946\:(+0.35\%)}$ & $0.863\:(-1.11\%)$ & $\boldsymbol{0.891\:(+1.65\%)}$ \\ 
LUNAR \cite{algo-lunar} & $0.856\:(-4.70\%)$ & $\boldsymbol{0.953\:(+5.06\%)}$ & $0.887\:(+3.95\%)$ & $\boldsymbol{0.933\:(+8.51\%)}$ & $\boldsymbol{0.864\:(+14.01\%)}$ & $0.864\:(+14.00\%)$ \\ 
KNN Distance \cite{knn-distance-algo} & $\boldsymbol{0.977\:(+2.58\%)}$ & $0.976\:(+2.49\%)$ & $\boldsymbol{0.979\:(+7.80\%)}$ & $0.958\:(+5.76\%)$ & $\boldsymbol{0.900\:(+14.07\%)}$ & $0.891\:(+13.12\%)$ \\ 
GOAD \cite{goad} & $\boldsymbol{0.966\:(+0.69\%)}$ & $0.964\:(+0.47\%)$ & $\boldsymbol{0.963\:(+1.94\%)}$ & $0.956\:(+1.31\%)$ & $\boldsymbol{0.939\:(+5.39\%)}$ & $0.930\:(+4.53\%)$ \\ 
Deep SVDD \cite{2018_deepsvdd} & $\boldsymbol{0.950\:(+24.67\%)}$ & $0.873\:(+17.05\%)$ & $\boldsymbol{0.942\:(+28.24\%)}$ & $0.860\:(+19.96\%)$ & $\boldsymbol{0.861\:(+25.54\%)}$ & $0.825\:(+22.00\%)$ \\ 
RCA \cite{rca} & $0.896\:(-0.82\%)$ & $\boldsymbol{0.908\:(+0.35\%)}$ & $\boldsymbol{0.901\:(+0.94\%)}$ & $0.901\:(+0.94\%)$ & $\boldsymbol{0.868\:(+3.59\%)}$ & $0.862\:(+2.99\%)$ \\
NeuTraL \cite{2021_neural_transformation} & $\boldsymbol{0.933\:(+3.19\%)}$ & $0.922\:(+2.09\%)$ & $\boldsymbol{0.925\:(+5.77\%)}$ & $0.905\:(+3.77\%)$ & $\boldsymbol{0.881\:(+10.48\%)}$ & $0.867\:(+9.00\%)$ \\ 
\bottomrule
\end{tabular}
\end{adjustbox}
    \caption{\textbf{Outlier Filtering with RANSAC-NN.} Shown above are the performance of OD models trained on a RANSAC-NN filtered dataset (alongside their relative difference to contaminated training). The datasets were initially contaminated, and we applied RANSAC-NN to generate outlier scores for each training image. We then took the top-$p$ percent of images with the lowest outlier scores and trained the models using the filtered set. As demonstrated above, filtering the dataset using RANSAC-NN has led to significant performance gains in most OD algorithms, especially in the high contamination setting.}
    \label{tab:outlier_filtering_results}
\end{table*}

From the results, it is evident that models trained with a filtered set have resulted in significant performance gains compared to those trained directly on the contaminated set. At $10\%$ contamination, we can notice that the majority of the algorithms benefit more by removing fewer samples and keeping more for training. At a higher contamination of $40\%$, almost all algorithms experience a significantly better performance by removing more samples. Essentially, in cases where contamination in the training set is known to be severe, higher precision should favored for better OD performance during the outlier removal process.

Another interesting observation from this experiment is the characteristics of the OD algorithms. INNE \cite{algo-inne} and AutoEncoder \cite{Outlier_Analysis_aggarwal}, in particular, are much more dependent on the amount of training data. In each contaminated setting, both algorithms experienced better improvements by selecting a tighter threshold closer to the actual contamination level. In contrast, Deep SVDD \cite{2018_deepsvdd}, GOAD \cite{goad}, NeuTraL\cite{2021_neural_transformation}, and KNN Distance \cite{knn-distance-algo} were able to deliver better performance with less but potentially higher quality data. 

% Figure environment removed

\subsection{Ablation Studies}
\label{exp:ablation_studies}

\paragraph{Improvements from Threshold Sampling}
In this experiment, we analyze the performance improvements contributed by Threshold Sampling (TS). We invert the inlier scores predicted by ISP and compare them to the outlier scores produced by TS. In Figure \ref{fig:threshold-sampling-improvement}, we illustrate the performance obtained under both setups with varying outlier perturbation levels. From the results, we can observe that TS indeed enhances the predictive performance of RANSAC-NN. This is especially true when outlier perturbation is severe, indicating the importance of TS in increasing the robustness of RANSAC-NN. 
% Therefore, we can attribute TS as one of the primary reasons why RANSAC-NN is able to maintain robust performance 
% under high perturbation. 

% Figure environment removed


\paragraph{Sample Size and Sampling Iteration}
\label{exp:hyperparameters}

The two primary hyperparameters of RANSAC-NN are the sample size $m$ and the sampling iterations $s$. As described in Section \ref{methods:sample_size_iteration_properties}, the sample size $m$ influences the probability of sampling a clean inlier set, and the sampling iteration $s$ impacts the odds of sampling at least one clean inlier set. In this experiment, we compare the performance of RANSAC-NN under different sample sizes and sampling iteration configurations. 

% Figure environment removed

From the results in Figure \ref{fig:sample_size_iteration}, we can observe that an extreme sample size causes unstable performance, especially as the sampling iteration increases. This is because a small enough sample size increases the odds of sampling a set containing all outliers. With the addition of repeated sampling, this probability only increases, which is why we observe such unfavorable performance with sample size at $1\%$ of the dataset. 

On the other extreme, a large sample size reduces the probability of sampling an all-inlier set. This phenomenon can be seen in cases where the sample size exceeds $30\%$ of the dataset. The ideal sample size, therefore, falls within the $10-20\%$ range. With the addition of large sampling iterations, the performance continues improving as we increase the odds of sampling an all-inlier set. 
