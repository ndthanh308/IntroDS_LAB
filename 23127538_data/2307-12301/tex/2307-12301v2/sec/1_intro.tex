\section{Introduction}
\label{sec:intro}

Outlier detection (OD) refers to the task of identifying abnormal data that deviates from an expected inlier distribution. The abnormal data are known as \textit{outliers}, and the \textit{inliers} are data that belong within the expected distribution. In most cases, the outlier distributions are not known in advance, and the goal of OD methods is to classify data samples under such constraints. This is known as the \textit{one-class classification} objective \cite{one_class_classification, oc-svm, 2004_svdd, 2019_deep_feature_for_oneclass_cls, a_surve_of_one_class_khan}. 

The majority of OD algorithms can be categorized into three main categories: unsupervised, semi-supervised, or supervised methods \cite{od_survey, Outlier_Analysis_aggarwal, generalize_ood_survey, zhao2019pyod}. 
In unsupervised methods \cite{isolation_forest, local_outlier_factor, knn-distance-algo}, a \textit{contaminated} dataset comprising \textbf{both outliers and inliers} is provided. The underlying 
 assumption is that inliers constitute the majority of the dataset, whereas outliers reside in low-density regions within sample space. This allows unsupervised methods to identify outliers within the contaminated set by modeling the data distribution.
Semi-supervised methods \cite{2018_deepsvdd, algo-lunar, 2015_vae_anomaly_det, 2019_deep_feature_for_oneclass_cls, 2021_neural_transformation} utilize a \textit{clean} dataset consisting \textbf{exclusively of inlier samples}. A model is first trained on the clean inlier dataset before performing OD.
Supervised methods \cite{2021_exploring_limits_of_ood, 2018_outlier_exposure, 2021_ood_Abstention, 2021_dermatology_ood} assume an additional prior knowledge of the outlier distribution. These methods entail training a model on a labeled dataset comprised of both inlier and outlier samples before prediction. 
Given that supervised methods are more closely related to imbalanced classification, this paper focuses primarily on semi-supervised and unsupervised approaches.

%The majority of OD algorithms can be categorized under unsupervised, semi-supervised, or supervised methods \cite{od_survey, Outlier_Analysis_aggarwal, generalize_ood_survey, zhao2019pyod}. 
%In unsupervised methods, a \textit{contaminated} dataset containing outliers and inliers are given \cite{isolation_forest, local_outlier_factor}. The assumption is that inliers constitute the majority of the dataset, whereas outliers reside in low-density regions in the sample space. This allows unsupervised methods to identify outliers within the contaminated set by modeling the data distribution. 

%In semi-supervised methods, a \textit{clean} dataset containing strictly inlier samples is provided. A model is first trained on the clean inlier set before performing OD on future samples. 
%Supervised methods require the additional outlier distribution to be known in advance \cite{2021_exploring_limits_of_ood}. A model is trained on a labeled set of inlier and outlier samples before OD. Since supervised methods are more aligned with imbalanced classification, we focus our scope on semi-supervised and unsupervised methods in this paper. 


Image OD is a specific branch of OD targeted for computer vision \cite{od_survey, generalize_ood_survey, ImageOD-survey}. The goal is to identify images that are visually distinct from those of an expected distribution. In the context of image data, the source of abnormality could occur in the pixel or image level. Pixel-level abnormalities refer to the low-level pixel regions that are distinctive from regular regions. Image-level abnormalities are images that are visually different from regular images as a whole. For the remaining of this paper, we concentrate on outliers at the image level.

The variety of image OD methods ranges from traditional tabular techniques to recent deep learning-based approaches. Traditional techniques assumed the data to be low-dimensional, hand-crafted features \cite{isolation_forest, algo-inne, local_outlier_factor, loda}, which were effective primarily in low-resolution images.  However, these methods fall short when addressing the unstructured, high-dimensional pixel arrays seen in typical images. The introduction of deep learning enabled an automated means of extracting meaningful, resolution-independent features directly from image data. This has allowed the use of feature embeddings extracted by pre-trained neural networks to replace traditional features used by tabular methods \cite{knn-distance-algo, iForest_on_NN, 2019_deep_feature_for_oneclass_cls}. Additional model-based methods \cite{2018_deepsvdd, anogan, 2015_vae_anomaly_det, geom, goad} that represent the inlier image distribution using neural networks have also been proposed.

% The introduction of deep learning has been a game changer, enabling the direct extraction of meaningful, high-dimensional features from image data. This shift is evident in recent research, which promotes using feature embeddings from pre-trained neural networks, thereby enhancing the capacity of traditional methods to process more complex images. 

%Prior to deep learning, the majority of image OD algorithms have centered around methods designed for tabular data. Tabular OD algorithms assume their data to be low-dimensional hand-crafted features \cite{isolation_forest, algo-inne, local_outlier_factor, loda}. This is largely different from the high-dimensional unstructured pixel arrays commonly seen in images. Due to this difference, most tabular methods have only been successful with low-resolution images. 

%Deep learning in computer vision introduced a strategy to learn meaningful representations directly from data. Some works \cite{knn-distance-algo, iForest_on_NN, 2019_deep_feature_for_oneclass_cls} have suggested the use of feature embeddings from pre-trained neural networks in replacement of the hand-crafted features used by tabular methods. This modification has allowed tabular algorithms to handle images of a significantly larger scale without much compromise in performance. Others \cite{2018_deepsvdd, anogan, 2015_vae_anomaly_det, geom, goad} have suggested model-based methods that involve training a neural network to represent the inlier distribution. The trained model is then used to determine whether future samples are likely outliers.

The biggest challenge among existing image OD methods is the dependence on a \textbf{clean inlier set} for model training. To build a good representation of the inlier distribution, the training set must contain enough inlier samples while avoiding outlier contamination. Outlier presence causes actual outliers to be incorrectly treated as inliers during training, which leads to deteriorated OD performance during test time. In most cases, human inspection is required to ensure data quality, but this is an expensive, time-consuming, and labor-intensive process.  

In this paper, we present RANSAC-NN, an unsupervised image OD algorithm that addresses outlier contamination in image datasets. Inspired by RANSAC \cite{ref_ransac}, which was intended for fundamental matrix estimation, our algorithm applies a two-stage repeated nearest neighbor sub-sampling to predict outlier scores. Specifically, the first stage Inlier Score Prediction (ISP) generates a set of inlier score estimations, which are used by the second stage Threshold Sampling (TS) to predict the outlier score distribution. Outliers in the contaminated set can thus be removed by filtering out samples with scores exceeding a certain threshold. Unlike previous approaches, whose aim is to produce an OD model, the goal of RANSAC-NN is to dynamically detect outlier samples in a given contaminated set. As a result, our algorithm does not require any training or data preparation and can be applied straight out of the box.

% one class classification
We evaluate RANSAC-NN in comparison to previous methods on several OD tasks. The first apparent observation is that almost all existing methods achieve favorable performance when trained on a clean inlier set. However, when the training set is contaminated, a noticeable drop in performance occurs depending on the contamination level. We then attempt to filter out outliers by applying RANSAC-NN on the contaminated sets. Models for existing OD methods were trained using the filtered sets, and the results show significant improvements in all existing methods. Additional experiments regarding the RANSAC-NN setup and hyper-parameter settings have also been explored in our evaluations.

In summary, we make the following contributions:
\begin{itemize}
    \item We demonstrate that the majority of image OD algorithms can achieve favorable and robust performance when trained properly on a clean inlier set. 
    
    \item We emphasize the importance of training on a clean inlier set by examining the performance drop of existing methods under contaminated training. 

    \item We present an unsupervised image OD algorithm called RANSAC-NN that delivers promising performance in comparison to existing methods. Since our algorithm does not require model training, it maintains robust performance regardless of outlier contamination.

    \item We demonstrate improved robustness under contaminated training in all existing OD methods by applying RANSAC-NN as an outlier removal mechanism. 
\end{itemize} 

The remaining of this paper is organized as follows. In Section \ref{sec:methods}, we present the proposed RANSAC-NN algorithm. In Section \ref{sec:related_works}, we provide an overview of some well-known OD methods. In Section \ref{sec:experiments}, we detail the experimental results. Finally, we conclude the paper in Section \ref{sec:conclusions}.
    
% outlier contamination can impact the performance of existing image OD methods. 






% The results show that almost all existing methods can easily achieve near perfect performance when trained on a clean inlier set. However, a noticeable performance drop occurs when outliers are introduced into the training set. We then evaluate RANSAC-NN as pre-processing strategy, which we observe an overall improvement in performance. 


% The first apparent observation is that almost all existing methods achieve near perfect performance when trained on a clean inlier set. However, a decline in performance among existing algorithms can be observed, depending on the amout of 

% when outliers are present in the training set. 


% when a clean inlier set is provided, almost all existing methods achieved near perfect performance. However, when contamination 
% We first demonstrate that in cases where a clean inlier set is provided, we show that almost all existing OD algorithms perform equally well.

% The results show that almost all existing OD methods perform equally well when trained on a clean inlier set. However

% and we evaluate its performance in comparison to other state-of-the-art algorithms. The results show that our algorithm achieves favorable performance in comparison to the well

% RANSAC-NN does not estimate any model parameters or require any training to be applied. 

% neither any model or training is required to apply RANSAC-NN on a contaminated set. 

% that ranks the dataset samples by their outlier likelihood. Unlike the original RANSAC, our algorithm does not build or estimate any model parameters. Furthermore, RANSAC-NN can be applied directly on a contaminated set without any training.

% To address this issue, we present an unsupervised image OD algorithm that operates directly on a contaminated set. Rerank inspired by 


% forms a boundary while learning to identify real from generated data. 
% Energy based methods and diffusion based methods have also demonstrated potential in effective image OD.  


% Prior to the prominence of deep learning, the vast majority of OD algorithms attempted image OD by flattening the 2D pixel arrays into vectors and applying the respective algorithm. Most algorithms, however, were designed for tabular data, which are  structured concatenations of hand-crafted features. Unlike images, which are unstructured high dimensional pixel arrays, tabular data possess significantly lower feature dimensions with weakly correlated attributes. 

% often weakly correlated, and the feature dimension is comparatively smaller. This difference has 
% This has limited the application of most OD methods to only low-dimension images. 

% limited their applications to only low-dimension images. Since most methods were designed with the focus on the tabular data domain, transferring such methods to images have  

% This is a good paragraph, but maybe leave to related works since it talks about the details of tabular data
% Within the OD literature, most methods focus on detecting abnormality using hand-crafted features common in tabular data. The tabular features are often interpretable and weakly correlated, and the dimension size is relatively low. In contrast, images are high dimensional pixel arrays with strong locally correlating features. This has limited the application of most OD methods to only low-dimension images. 

% Deep learning provided a method to learning feature representations automatically. Specifically, a deep neural network is trained with large amounts of data for an objective, and the feature representations are the intermediate neuron outputs. Unlike tabular data, the feature representations (also known as ``embeddings'') are non-interpretable latent vectors.

% has been an active area of research within the computer vision community. Several studies have explored the use of auto-encoders to model the inlier image distribution. During inference, images that exhibit abnormally large reconstruction error in the latent space can be considered outliers. Another class of solutions have suggested the use of generative-adversarial networks (GAN's) as a method of modeling the inlier and outlier distributions. <something here about GANs> < Softmax probability method > <Cleanlab recent discovery> <all methods rely on a clean inlier set> 

% has also been an active area of research. As deep learning continues to gain prevalence in recent years, a number of studies have suggested reconstruction-based methods as well as generative-adversarial training for OD purposes.  

% remember, these guys are here to help


% TODO:
% leave outlier score to methods
% describe one class classification when introducing our method

% predicting an ``outlier score''. 


% P3: Challenge with starting with clean set to begin, Talk about image OD & 
% In computer vision (CV), image OD methods have been prevalent in applications involving xxx, yyy, and data cleaning. 
% plays an important role in ensuring the quality of image datasets, as well as robustness of  [image OD is common in CV]. [However, most of these image OD algorithms require a clean dataset to begin with.] [It is expensive, time-consuming to manually clean.] [In this work, we present...]
% P4: Talk about our method




