\section{Related Works}
\label{sec:related_works}

Image OD methods can be categorized by their detection mechanism into three main types: density estimation, image reconstruction, and self-supervised classification. We provide a brief introduction of each in the following.

\subsection{Density Estimation}
Density estimation relies on a set of inlier images to build a representation of the inlier distribution. During test time, images are compared to the representation to determine their outlier scores. While some methods have suggested to explicitly model the inlier distribution \cite{book-prml-bishop, a_surve_of_one_class_khan, 2018_dae_gmm}, others have explored non-parametric methods. 

One strategy is to combine tabular non-parametric methods with neural networks to address outlier images. In one study \cite{iForest_on_NN}, image embeddings extracted by a neural network were fed into the Isolation Forest \cite{isolation_forest} and Local Outlier Factor (LOF) \cite{local_outlier_factor} algorithms for outlier score prediction. Other studies \cite{knn-distance-algo, 2019_deep_feature_for_oneclass_cls} have considered the use of nearest neighbors between image embeddings as an outlier score measure. 

Deep learning-based density estimation methods rely on optimizing a neural network to learn characteristics specific to inlier images. DeepSVDD \cite{2018_deepsvdd} is a method inspired by \cite{oc-svm, 2004_svdd} that learns a mapping from the inlier image distribution to a minimum volume hypersphere in latent space. At test time, images that are mapped to points far from the center of the hypersphere are considered likely outliers. OC-CNN \cite{2018_oneclass_cnn} suggested a similar approach, but it leveraged a pre-trained convolutional neural network that can be trained in an end-to-end fashion. Adversarial techniques \cite{sogann} that combine synthetic outlier images with real inlier images to train a discriminator network have also been presented. Given the wide variety of density estimation methods, we refer to \cite{ImageOD-survey, od_survey} for a complete overview.

\subsection{Image Reconstruction}
Image reconstruction refers to a class of methods that measures the outlier score of an image by its reconstruction \cite{Outlier_Analysis_aggarwal}. Auto-encoders and generative adversarial networks (GAN's) are two common options given their ability to generate realistic images using latent representations \cite{2014_anomaly_detection, 2014_gans}. In \cite{2015_vae_anomaly_det, 2015_autoencoder_reconstruction_loss}, auto-encoders were trained on a set of inlier images, and the outlier scores of test images were measured by comparing their reconstruction loss with those of known inlier images. This method was further improved in \cite{2019_as_ae, 2019_mem_ae} by including additional latent heuristics as part of the comparison. Other works \cite{2020_attr, 2020_puzzle_ae} have suggested the addition of image augmentations to increase the distinction between latent representations.  

GAN-based reconstruction relies on the generative ability of a generator network to mimic the inlier image space. One approach \cite{deecke2018anomaly} is to search for a latent representation of a test image from a trained generator. At test time, images that fail to find a corresponding latent representation are considered outliers. Another method \cite{anogan} attempts to generate a replication of a given image using a trained generator. The image is considered an outlier if the generator fails to replicate a similar reconstruction.

\subsection{Self-Supervised Classification}
Self-supervised classification is a type of method that follows the same principles from self-supervised learning \cite{ssl_cookbook}. In self-supervised learning, a model is optimized on one or more auxiliary tasks using transformed data from an unlabeled dataset. The purpose is to learn meaningful representations that can potentially benefit downstream tasks by training on the auxiliary tasks. 

GEOM \cite{geom} proposed an auxiliary objective to train a classification model in identifying geometric transforms on unlabeled inlier images. During inference, the same transformations are applied to the test image, and by comparing the model's output with those seen during training, the outlier score can be determined. Additional transforms were introduced by \cite{2019_using_ssl_can_improve_robustness}, which have demonstrated improved OD performance. GOAD \cite{goad} builds upon GEOM by generalizing the types of transforms to include non-image data. Additional methods \cite{2020_ssl_ae, 2020_csi_ssl_ae, 2021_ssd_od, 2021_neural_transformation} have also shown promising performance using self-supervised classification methods.

% As demonstrated in \cite{geom}, training a model to identify geometric transforms on unlabeled inlier images has proved to be an effective auxiliary task. \cite{2019_using_ssl_can_improve_robustness} improved upon this by adding an additional translation task, which was shown to benefit the overall robustness. \cite{goad} expanded upon the types of transformation by including 



% The objective of self-supervised learning is to leverage auxiliary tasks in an attempt to extract available supervision information from large amounts of unsupervised data. 


% \begin{itemize}
  % \item Image OD Survey Paper: \cite{ImageOD-survey}
  % \item One-class classification
  %   \begin{itemize}
  %     \item Learn a decision boundary around normal images in feature space. Detect anomalies by checking if test images fall outside the boundary.
  %     \item Classical Paper: One class SVM \cite{2001_oneclass_svm}, SVDD \cite{2004_svdd}
  %     \item \cite{2019_deep_feature_for_oneclass_cls} FT the CNN to extract image features and then takes the nearest neighbor classification method to construct the one-class classifier
  %     \item \cite{2019_dl_nd} extend it to the problem of detecting anomalies for multiple known classes instead of one class
  %     \item \cite{2018_deepsvdd} proposed end-to-end deep support vector description model
  %     \item \cite{2018_oneclass_cnn} proposed one-class cnn (OCCNN)
  %   \end{itemize}
  % \item Image reconstruction (learn features from normal cases
    % \begin{itemize}
    %   \item Map images to latent vectors and reconstruct them. Assume reconstruction error is larger for anomalies.
      % \item Autoencoder for OD: \cite{1995_novelty_det_cls}, \cite{2014_anomaly_detection}, \cite{2015_vae_anomaly_det}
      % \item Autoencoder (+latent) for OD: \cite{2018_dae_gmm}, \cite{2019_as_ae}, \cite{2019_mem_ae}
      % \item Autoencoder (+data augmentation): \cite{2020_attr}, \cite{2020_puzzle_ae}
      % \item GAN \cite{2017_ad_gan}
      % \item GAN+Reconstruction \cite{2019_skip_ganomaly}, \cite{2018_gan_ae}, \cite{2019_ocgan}, \cite{2020_old_is_god}
    % \end{itemize}
%   \item SSL (learn features from normal cases
%     \begin{itemize}
%       \item Train model on pretext task using only normal images. Anomalies stand out as model fails on pretext task.
%       \item Example methods: Predict image rotations, translations
%       \item papers: \cite{2018_geometric_trans_ae}, \cite{2020_ssl_ae}, \cite{2020_csi_ssl_ae}, \cite{2021_ssd_od}
%   \end{itemize}
% \end{itemize}

% The problem of OD has been studied in a wide range of machine learning domains, including tabular data, time series, and graph networks. In this paper, we introduce some well-established methods that have targeted image data.

% \subsection{Tabular Methods}
% Prior to deep learning, tabular methods were the primary solutions for image OD. 
% Isolation Forest (iForest) \cite{isolation_forest} is an unsupervised OD algorithm that identifies outliers by partitioning the feature space. The objective is to build \textit{isolation trees} that isolate every data sample by their feature attributes. Densely-packed inliers would require trees of deeper depth for isolation, whereas shallow trees are enough to isolate most outliers. The outlier likelihood of a sample can then be measured by the tree depth for which it is isolated. Another unsupervised algorithm is Local Outlier Factor (LOF) \cite{local_outlier_factor}. In LOF, the \textit{local density} of each sample is computed by measuring its average distance to its \textit{k}-nearest neighbors. Samples with low \textit{local densities} are likely outliers.
% By comparing each sample's \textit{local density} to those of its neighbors, outlier samples can be identified as those with low \textit{local densities}.

% LODA \cite{loda} is a semi-supervised OD algorithm that utilizes random sparse projections to model the distribution of an inlier set. During training, input samples are projected onto a fixed set of sparse projection vectors, and a set of 1D histograms learns the projected distributions. During inference, input samples are projected in the same manner, and the outlier likelihood can be approximated by taking the joint probability of the projections. 

% Other existing works have suggested similar measures to handle outliers in the tabular domain.
% Despite their success, tabular methods often struggle with detecting outlier images due to the differences in data structure and dimensionality. 
% To this extent, some have explored the use of deep learning models to extract meaningful representations in replacement of raw image pixels.

% In a study by Luan \textit{et al.} \cite{iForest_on_NN}, the authors demonstrate significant improvements by feeding features extracted by neural networks into iForest and LOF. Another semi-supervised strategy called KNN-Distance \cite{knn-distance-algo} uses the cosine similarity between a sample and its \textit{k}-nearest neighbors as a measure of outlier likelihood. As demonstrated in their experiments, the transition from raw pixels to deep features has led to improved performance when adopting tabular OD methods. In this paper, we consider such an approach when evaluating existing tabular methods on image OD tasks. 

% \subsection{Deep Learning Methods}
% A fundamental aspect of deep learning methods is that the data is assumed to be homogeneous. Therefore, deep learning algorithms often involve a model that learns inlier data characteristics before generalizing to unseen data. One classic example of such models are auto-encoders for semi-supervised OD. 

% In the auto-encoder setup, an encoder is trained to map high-dimensional inlier samples to low-dimensional features. Using the encoded features, a decoder learns to reconstruct the original sample in its original dimensions. The assumption is that outliers are harder to encode, which leads to poor reconstructions. Thus, the reconstruction error can be an effective measure to determine the outlier likelihood of a sample.

% DeepSVDD \cite{2018_deepsvdd} is semi-supervised algorithm whose objective is to learn a neural network that maps the inlier data into a minimum volume hypersphere. To achieve this, the network learns a center point of a hypersphere such that inliers are distanced closer to the center. During inference, outlier samples that are distant from the center can be identified by their large radial distances. In comparison to previous kernel-based approaches \cite{oc-svm, 2004_svdd}, DeepSVDD offers the advantage of avoiding explicit feature engineering while ensuring scalability to high dimension signals. 


% Generative methods such as adversarial networks or diffusion models has also demonstrated potential for dealing with OD. In the work by Liu \textit{et al.}, SO-GAAL and MO-GAAL \cite{sogann} were presented with the objective of generating artificial outliers for classification training. The idea is to have a generator produce potential outliers that are similar to the inlier data. A discriminator is trained alongside the generator to classify real from fake data. During the optimization process, the discriminator indirectly learns a decision boundary around the inlier distribution, which can be used to evaluate future samples. Similar approaches \cite{anogan} have been demonstrated using other forms of generative methods.







% BELOW ARE OLD OLD TEXTS


% Auto-encoders are often considered in the context of reconstruction-based methods. In this setting, a model is trained to encode inlier samples to a lower dimension, and a reconstruction is generated using the encoded representation. Since outliers have distinct characteristics compared to inliers, the reconstruction loss can be a meaningful measure. 

% Others have suggested more sophisticated methods that include information compression approaches to boundary-based methods. Most information compression algorithms rely on auto-encoders, which generate a reconstruction of an input using the encoded latent variables. By training an auto-encoder on a set of inlier images, outliers can be detected by an abnormally large reconstruction loss. 
% Boundary-based image OD has also been demonstrated using generative adversarial networks. When the discriminator is trained to distinguish real inliers from generated outliers, it forms a decision boundary that encapsulates the inlier distribution. Additional methods have been proposed in the literature, and they all demonstrate promising performance in their respective settings.



% Deep learning OD methods differ from tabular methods in that 
% Common OD methods in deep learning have involved auto-encoders and generative models. 


% In the work by Golan \textit{et al.}, GEOM \cite{geom} was presented as a semi-supervised approach to building an anomaly classifier. During training, GEOM applies multiple distinct transforms to the inlier images, and a model learns to identify features useful for detecting anomalies. GOAD \cite{goad} further extends upon this approach by generalizing the transforms to include non-image data. As demonstrated in their paper, the addition of transformations has allowed the model to learn spatially-preserving features that are inherent among inliers. 

% Deep learning methods have the advantage of learning inlier characteristics directly from data. Once the model has been trained, future samples can be easily determined by simply applying the model regardless of dataset size. Unfortunately, the challenge with such methods is the difficulty in obtaining a quality training set with enough inlier samples. In our experiments, we demonstrate that outlier contamination can severely degrade the performance in both deep learning and tabular methods. 
% To mitigate the influence of outlier contamination, we introduce RANSAC-NN, an unsupervised image OD algorithm that can detect outliers in heavy contaminated datasets. 

% the goal of RANSAC-NN is to provide an unsupervised image OD algorithm that maintains strong performance under heavy contaminated settings. 


% graph networks, and 

% time-series data, graph networks, tabular data, etc.)

% OD in machine learning have spanned domains including time-series data, graph networks, tabular data, and images.   


% Due to the extent of OD methods in the ML literature, 

% OD methods throughout the machine learning literature have covered domains including time-series data, graph-based networks, tabular data, and 

% The vast majority of OD algorithms proposed throughout the literature can categorized by their domain of application and the amount of supervision required. Common domains include tabular, time-series, 

% Numerous OD methods have been proposed throughout the machine learning literature. 

% The vast majority of OD methods in machine learning can be categorized by their domain of application and level of supervision. Common domains include time-series, tabular, 

% Types of data The OD domain span 


% Here we highlight a few well-established methods....




% The scope of OD in machine learning encompass various domains including time-series data, 

% The scope of OD methods in machine learning have encompassed 
% Many different modalities time series, graph based, 

% OD algorithms within the machine learning literature can be categorized into 

% The goal of OD 

% One Class Classification definition