\clearpage
\setcounter{page}{1}
\maketitlesupplementary


\section{Variations in Feature Extractors}

We include additional experiments using MobileNet-v3 \cite{mobilenet-v3} and Fast-ViT \cite{fastvit} as the feature extractor of choice. We follow the same experiment setup in Section \ref{sec:experiments}, and the results are presented in the following. 

\subsection{Outlier Detection with Clean Training}
\label{suppl:clean_training_benchmark}

The same experiments from Section \ref{exp:outlier_detection_benchmark} are repeated using MobileNet-v3 and Fast-ViT, and their results are listed in Tables \ref{suppl:clean_train_mobilenet} and \ref{suppl:clean_train_swin}. As demonstrated below, the performance of models trained on a \textbf{clean} inlier set remain consistent regardless of the choice in feature extractor.

% MOBILENET-V3
\begin{table}[h]
    \centering
    \begin{adjustbox}{width=0.47\textwidth}
    \begin{tabular}{lllllc}
\toprule
\multirow{2}{*}{Algorithm} & \multicolumn{4}{c}{Outlier Perturbation (ROC)} & \multirow{2}{*}{Max $\sigma$}\\
\cmidrule(lr){2-5} 
{} &         $5\%$ &     $10\%$ &     $20\%$ &     $40\%$ & {} \\
\midrule

Isolation Forest \cite{isolation_forest} &      $0.962$ &  $0.964$ &  $0.962$ &  $0.961$ &  $\pm0.02$ \\
INNE  \cite{algo-inne}           &      $0.972$ &  $0.975$ &  $\boldsymbol{0.975}^{+}$ &  $\boldsymbol{0.970}^{+}$ &  $\pm0.01$ \\
LOF   \cite{local_outlier_factor}           &      $0.970$ &  $0.972$ &  $0.970$ &  $\boldsymbol{0.970}^{+}$ &  $\pm0.01$ \\
AutoEncoder  \cite{Outlier_Analysis_aggarwal}    &      $0.971$ &  $0.970$ &  $0.968$ &  $0.965$ &  $\pm0.02$ \\
LUNAR   \cite{algo-lunar}         &      $0.972$ &  $0.969$ &  $0.968$ &  $0.966$ &  $\pm0.02$ \\
KNN Distance  \cite{knn-distance-algo}   &      $\boldsymbol{0.977}^{*}$ &  $\boldsymbol{0.978}^{*}$ &  $\boldsymbol{0.978}^{*}$ &  $\boldsymbol{0.977}^{*}$ &  $\pm0.01$ \\
GOAD  \cite{goad}           &      $0.969$ &  $0.970$ &  $0.969$ &  $0.969$ &  $\pm0.01$ \\
Deep SVDD  \cite{2018_deepsvdd}      &      $0.958$ &  $0.963$ &  $0.959$ &  $0.961$ &  $\pm0.02$ \\
RCA   \cite{rca}           &      $0.769$ &  $0.756$ &  $0.763$ &  $0.759$ &  $\pm0.11$ \\
NeuTraL  \cite{2021_neural_transformation}        &      $0.967$ &  $0.970$ &  $0.968$ &  $0.964$ &  $\pm0.02$ \\
RANSAC-NN (Ours)       &      $\boldsymbol{0.976}^{+}$ &  $\boldsymbol{0.976}^{+}$ &  $0.971$ &  $0.935$ &  $\pm0.03$ \\

\bottomrule
\end{tabular}
\end{adjustbox}

    \caption{\textbf{Outlier Detection with Clean Training (MobileNet-v3).}}
    \label{suppl:clean_train_mobilenet}
\end{table}


% SWIN-V2
\begin{table}[h]
    \centering
    \begin{adjustbox}{width=0.47\textwidth}
    \begin{tabular}{lllllc}
\toprule
\multirow{2}{*}{Algorithm} & \multicolumn{4}{c}{Outlier Perturbation (ROC)} & \multirow{2}{*}{Max $\sigma$}\\
\cmidrule(lr){2-5} 
{} &         $5\%$ &     $10\%$ &     $20\%$ &     $40\%$ & {} \\
\midrule

Isolation Forest \cite{isolation_forest} &      $0.961$ &  $0.962$ &  $0.959$ &  $0.957$ &  $\pm0.02$ \\
INNE  \cite{algo-inne}           &      $0.974$ &  $0.972$ &  $0.971$ &  $0.968$ &  $\pm0.01$ \\
LOF  \cite{local_outlier_factor}            &      $0.966$ &  $0.963$ &  $0.965$ &  $0.963$ &  $\pm0.02$ \\
AutoEncoder \cite{Outlier_Analysis_aggarwal}     &      $0.974$ &  $0.974$ &  $0.972$ &  $\boldsymbol{0.970}^{+}$ &  $\pm0.02$ \\
LUNAR \cite{algo-lunar}           &      $0.973$ &  $0.973$ &  $\boldsymbol{0.974}^{+}$ &  $\boldsymbol{0.970}^{+}$ &  $\pm0.02$ \\
KNN Distance \cite{knn-distance-algo}    &      $\boldsymbol{0.980}^{+}$ &  $\boldsymbol{0.980}^{*}$ &  $\boldsymbol{0.979}^{*}$ &  $\boldsymbol{0.979}^{*}$ &  $\pm0.02$ \\
GOAD  \cite{goad}           &      $0.962$ &  $0.961$ &  $0.965$ &  $0.963$ &  $\pm0.02$ \\
Deep SVDD  \cite{2018_deepsvdd}      &      $0.923$ &  $0.925$ &  $0.931$ &  $0.929$ &  $\pm0.04$ \\
RCA  \cite{rca}            &      $0.831$ &  $0.832$ &  $0.809$ &  $0.792$ &  $\pm0.11$ \\
NeuTraL \cite{2021_neural_transformation}         &      $0.937$ &  $0.937$ &  $0.929$ &  $0.915$ &  $\pm0.06$ \\
RANSAC-NN (Ours)       &      $\boldsymbol{0.981}^{*}$ &  $\boldsymbol{0.976}^{+}$ &  $0.970$ &  $0.946$ &  $\pm0.03$ \\

\bottomrule
\end{tabular}
\end{adjustbox}

    \caption{\textbf{Outlier Detection with Clean Training (Fast-ViT).}}
    \label{suppl:clean_train_swin}
\end{table}


\subsection{Influence of Contaminated Training}
The same experiments from Section \ref{exp:impact_of_contaminated_training} are repeated below. As shown in Figures \ref{suppl:contam_train_mobilenet} and \ref{suppl:contam_train_fastvit}, the performance drop due to contaminated training can be observed regardless of the feature extractor choice.

% Figure environment removed

% Figure environment removed


\subsection{Outlier Filtering with RANSAC-NN}
\label{suppl:outlier_filtering}




\begin{table*}[h]
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{lllllll}
\toprule
\multirow{2}{*}{Algorithm} & \multicolumn{2}{c}{10\% Contamination} & \multicolumn{2}{c}{20\% Contamination} & \multicolumn{2}{c}{40\% Contamination}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
{} & Take Top-50\% & Take Top-90\% & Take Top-50\% & Take Top-80\% & Take Top-50\% & Take Top-60\%\\

\midrule
Isolation Forest & $0.937\:(-1.81\%)$ & $\boldsymbol{0.963\:(+0.79\%)}$ & $0.945\:(+0.01\%)$ & $\boldsymbol{0.958\:(+1.38\%)}$ & $\boldsymbol{0.944\:(+5.69\%)}$ & $0.930\:(+4.32\%)$ \\ 
INNE & $0.914\:(-0.83\%)$ & $\boldsymbol{0.962\:(+3.97\%)}$ & $0.929\:(+7.16\%)$ & $\boldsymbol{0.940\:(+8.35\%)}$ & $\boldsymbol{0.913\:(+14.33\%)}$ & $0.911\:(+14.15\%)$ \\ 
LOF & $0.935\:(+16.28\%)$ & $\boldsymbol{0.943\:(+17.05\%)}$ & $\boldsymbol{0.946\:(+37.24\%)}$ & $0.906\:(+33.29\%)$ & $\boldsymbol{0.911\:(+42.61\%)}$ & $0.884\:(+39.93\%)$ \\ 
AutoEncoder & $0.912\:(-5.61\%)$ & $\boldsymbol{0.970\:(+0.12\%)}$ & $0.923\:(-3.57\%)$ & $\boldsymbol{0.966\:(+0.73\%)}$ & $0.930\:(+3.97\%)$ & $\boldsymbol{0.935\:(+4.46\%)}$ \\ 
LUNAR & $0.900\:(-2.85\%)$ & $\boldsymbol{0.960\:(+3.16\%)}$ & $0.924\:(+4.10\%)$ & $\boldsymbol{0.952\:(+6.89\%)}$ & $\boldsymbol{0.922\:(+13.73\%)}$ & $0.922\:(+13.65\%)$ \\ 
KNN Distance & $\boldsymbol{0.975\:(+3.74\%)}$ & $0.968\:(+3.03\%)$ & $\boldsymbol{0.975\:(+10.35\%)}$ & $0.958\:(+8.65\%)$ & $\boldsymbol{0.942\:(+16.68\%)}$ & $0.940\:(+16.48\%)$ \\ 
GOAD & $\boldsymbol{0.970\:(+0.61\%)}$ & $0.969\:(+0.55\%)$ & $\boldsymbol{0.966\:(+0.94\%)}$ & $0.964\:(+0.76\%)$ & $\boldsymbol{0.958\:(+6.08\%)}$ & $0.950\:(+5.25\%)$ \\ 
Deep SVDD & $\boldsymbol{0.971\:(+20.00\%)}$ & $0.913\:(+14.22\%)$ & $\boldsymbol{0.966\:(+25.79\%)}$ & $0.894\:(+18.56\%)$ & $\boldsymbol{0.927\:(+28.41\%)}$ & $0.896\:(+25.33\%)$ \\ 
RCA & $\boldsymbol{0.786\:(+6.28\%)}$ & $0.746\:(+2.27\%)$ & $\boldsymbol{0.792\:(+8.69\%)}$ & $0.768\:(+6.36\%)$ & $\boldsymbol{0.774\:(+13.28\%)}$ & $0.754\:(+11.34\%)$ \\ 
NeuTraL & $\boldsymbol{0.962\:(+0.59\%)}$ & $0.961\:(+0.45\%)$ & $\boldsymbol{0.967\:(+2.81\%)}$ & $0.956\:(+1.76\%)$ & $\boldsymbol{0.953\:(+8.22\%)}$ & $0.942\:(+7.13\%)$ \\
\bottomrule
\end{tabular}
\end{adjustbox}
    \caption{\textbf{Performance Improvements from Outlier Filtering using RANSAC-NN (MobileNet-v3).}}
    \label{suppl:outlier_filtering_results_mbnet}
\end{table*}

\begin{table*}[h!]
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{lllllll}
\toprule
\multirow{2}{*}{Algorithm} & \multicolumn{2}{c}{10\% Contamination} & \multicolumn{2}{c}{20\% Contamination} & \multicolumn{2}{c}{40\% Contamination}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
{} & Take Top-50\% & Take Top-90\% & Take Top-50\% & Take Top-80\% & Take Top-50\% & Take Top-60\%\\

\midrule
Isolation Forest & $0.926\:(-2.29\%)$ & $\boldsymbol{0.954\:(+0.49\%)}$ & $0.929\:(+0.50\%)$ & $\boldsymbol{0.947\:(+2.27\%)}$ & $0.893\:(+4.25\%)$ & $\boldsymbol{0.896\:(+4.58\%)}$ \\ 
INNE & $0.916\:(-0.70\%)$ & $\boldsymbol{0.952\:(+2.86\%)}$ & $0.924\:(+6.10\%)$ & $\boldsymbol{0.926\:(+6.27\%)}$ & $\boldsymbol{0.885\:(+11.31\%)}$ & $0.882\:(+11.04\%)$ \\ 
LOF & $\boldsymbol{0.941\:(+15.57\%)}$ & $0.940\:(+15.46\%)$ & $\boldsymbol{0.946\:(+32.63\%)}$ & $0.894\:(+27.40\%)$ & $\boldsymbol{0.878\:(+35.95\%)}$ & $0.860\:(+34.16\%)$ \\ 
AutoEncoder & $0.918\:(-4.15\%)$ & $\boldsymbol{0.967\:(+0.82\%)}$ & $0.927\:(-1.10\%)$ & $\boldsymbol{0.959\:(+2.07\%)}$ & $0.905\:(+4.77\%)$ & $\boldsymbol{0.917\:(+5.98\%)}$ \\ 
LUNAR & $0.905\:(+1.22\%)$ & $\boldsymbol{0.954\:(+6.09\%)}$ & $0.920\:(+8.54\%)$ & $\boldsymbol{0.934\:(+9.93\%)}$ & $\boldsymbol{0.898\:(+16.74\%)}$ & $0.884\:(+15.32\%)$ \\ 
KNN Distance & $\boldsymbol{0.977\:(+2.94\%)}$ & $0.971\:(+2.37\%)$ & $\boldsymbol{0.971\:(+8.79\%)}$ & $0.952\:(+6.88\%)$ & $\boldsymbol{0.916\:(+13.32\%)}$ & $0.915\:(+13.26\%)$ \\ 
GOAD & $\boldsymbol{0.960\:(+0.65\%)}$ & $0.959\:(+0.53\%)$ & $\boldsymbol{0.964\:(+1.85\%)}$ & $0.959\:(+1.32\%)$ & $\boldsymbol{0.918\:(+4.57\%)}$ & $0.918\:(+4.57\%)$ \\ 
Deep SVDD & $\boldsymbol{0.953\:(+21.60\%)}$ & $0.857\:(+11.96\%)$ & $\boldsymbol{0.944\:(+26.04\%)}$ & $0.859\:(+17.55\%)$ & $\boldsymbol{0.876\:(+25.15\%)}$ & $0.839\:(+21.42\%)$ \\ 
RCA & $0.792\:(-1.01\%)$ & $\boldsymbol{0.821\:(+1.85\%)}$ & $0.781\:(+1.30\%)$ & $\boldsymbol{0.792\:(+2.45\%)}$ & $0.760\:(+6.11\%)$ & $\boldsymbol{0.763\:(+6.40\%)}$ \\ 
NeuTraL & $\boldsymbol{0.929\:(+3.04\%)}$ & $0.924\:(+2.58\%)$ & $\boldsymbol{0.920\:(+6.28\%)}$ & $0.910\:(+5.37\%)$ & $\boldsymbol{0.877\:(+9.49\%)}$ & $0.874\:(+9.18\%)$ \\ 
\bottomrule
\end{tabular}
\end{adjustbox}
    \caption{\textbf{Performance Improvements from Outlier Filtering using RANSAC-NN (Fast-ViT).}}
    \label{suppl:outlier_filtering_results_fastvit}
% \vspace*{-\baselineskip}
\end{table*}


% Figure environment removed

The same experiments from Section \ref{exp:outlier_filtering} are repeated in the following. For each algorithm, we train a model on a set of images that had been outlier-filtered by RANSAC-NN prior to model training (see Figures \ref{fig:outlier_filtering_dist} and \ref{fig:outlier_filtering_dist_comb} for example illustration). The resulting model performance are listed in Tables \ref{suppl:outlier_filtering_results_mbnet} and \ref{suppl:outlier_filtering_results_fastvit} for both feature extractors. 

We can notice that when the outlier contamination level is severe, almost all algorithms benefit from outlier-filtering with a smaller but higher quality training dataset. This demonstrates the importance of training on a clean inlier set. Furthermore, these observations are consistent across different feature extractor choices.


% \begin{table*}[t]
%     \centering
%     \begin{adjustbox}{width=\textwidth}
%     \begin{tabular}{lllllll}
% \toprule
% \multirow{2}{*}{Algorithm} & \multicolumn{2}{c}{10\% Contamination} & \multicolumn{2}{c}{20\% Contamination} & \multicolumn{2}{c}{40\% Contamination}\\
% \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
% {} & Take Top-50\% & Take Top-90\% & Take Top-50\% & Take Top-80\% & Take Top-50\% & Take Top-60\%\\

% \midrule
% Isolation Forest & $0.937\:(-1.81\%)$ & $\boldsymbol{0.963\:(+0.79\%)}$ & $0.945\:(+0.01\%)$ & $\boldsymbol{0.958\:(+1.38\%)}$ & $\boldsymbol{0.944\:(+5.69\%)}$ & $0.930\:(+4.32\%)$ \\ 
% INNE & $0.914\:(-0.83\%)$ & $\boldsymbol{0.962\:(+3.97\%)}$ & $0.929\:(+7.16\%)$ & $\boldsymbol{0.940\:(+8.35\%)}$ & $\boldsymbol{0.913\:(+14.33\%)}$ & $0.911\:(+14.15\%)$ \\ 
% LOF & $0.935\:(+16.28\%)$ & $\boldsymbol{0.943\:(+17.05\%)}$ & $\boldsymbol{0.946\:(+37.24\%)}$ & $0.906\:(+33.29\%)$ & $\boldsymbol{0.911\:(+42.61\%)}$ & $0.884\:(+39.93\%)$ \\ 
% AutoEncoder & $0.912\:(-5.61\%)$ & $\boldsymbol{0.970\:(+0.12\%)}$ & $0.923\:(-3.57\%)$ & $\boldsymbol{0.966\:(+0.73\%)}$ & $0.930\:(+3.97\%)$ & $\boldsymbol{0.935\:(+4.46\%)}$ \\ 
% LUNAR & $0.900\:(-2.85\%)$ & $\boldsymbol{0.960\:(+3.16\%)}$ & $0.924\:(+4.10\%)$ & $\boldsymbol{0.952\:(+6.89\%)}$ & $\boldsymbol{0.922\:(+13.73\%)}$ & $0.922\:(+13.65\%)$ \\ 
% KNN Distance & $\boldsymbol{0.975\:(+3.74\%)}$ & $0.968\:(+3.03\%)$ & $\boldsymbol{0.975\:(+10.35\%)}$ & $0.958\:(+8.65\%)$ & $\boldsymbol{0.942\:(+16.68\%)}$ & $0.940\:(+16.48\%)$ \\ 
% GOAD & $\boldsymbol{0.970\:(+0.61\%)}$ & $0.969\:(+0.55\%)$ & $\boldsymbol{0.966\:(+0.94\%)}$ & $0.964\:(+0.76\%)$ & $\boldsymbol{0.958\:(+6.08\%)}$ & $0.950\:(+5.25\%)$ \\ 
% Deep SVDD & $\boldsymbol{0.971\:(+20.00\%)}$ & $0.913\:(+14.22\%)$ & $\boldsymbol{0.966\:(+25.79\%)}$ & $0.894\:(+18.56\%)$ & $\boldsymbol{0.927\:(+28.41\%)}$ & $0.896\:(+25.33\%)$ \\ 
% RCA & $\boldsymbol{0.786\:(+6.28\%)}$ & $0.746\:(+2.27\%)$ & $\boldsymbol{0.792\:(+8.69\%)}$ & $0.768\:(+6.36\%)$ & $\boldsymbol{0.774\:(+13.28\%)}$ & $0.754\:(+11.34\%)$ \\ 
% NeuTraL & $\boldsymbol{0.962\:(+0.59\%)}$ & $0.961\:(+0.45\%)$ & $\boldsymbol{0.967\:(+2.81\%)}$ & $0.956\:(+1.76\%)$ & $\boldsymbol{0.953\:(+8.22\%)}$ & $0.942\:(+7.13\%)$ \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
%     \caption{\textbf{Performance Improvements from Outlier Filtering using RANSAC-NN (MobileNet-v3).}}
%     \label{suppl:outlier_filtering_results_mbnet}
% \end{table*}


% \begin{table*}[t]
%     \centering
%     \begin{adjustbox}{width=\textwidth}
%     \begin{tabular}{lllllll}
% \toprule
% \multirow{2}{*}{Algorithm} & \multicolumn{2}{c}{10\% Contamination} & \multicolumn{2}{c}{20\% Contamination} & \multicolumn{2}{c}{40\% Contamination}\\
% \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
% {} & Take Top-50\% & Take Top-90\% & Take Top-50\% & Take Top-80\% & Take Top-50\% & Take Top-60\%\\

% \midrule
% Isolation Forest & $0.926\:(-2.29\%)$ & $\boldsymbol{0.954\:(+0.49\%)}$ & $0.929\:(+0.50\%)$ & $\boldsymbol{0.947\:(+2.27\%)}$ & $0.893\:(+4.25\%)$ & $\boldsymbol{0.896\:(+4.58\%)}$ \\ 
% INNE & $0.916\:(-0.70\%)$ & $\boldsymbol{0.952\:(+2.86\%)}$ & $0.924\:(+6.10\%)$ & $\boldsymbol{0.926\:(+6.27\%)}$ & $\boldsymbol{0.885\:(+11.31\%)}$ & $0.882\:(+11.04\%)$ \\ 
% LOF & $\boldsymbol{0.941\:(+15.57\%)}$ & $0.940\:(+15.46\%)$ & $\boldsymbol{0.946\:(+32.63\%)}$ & $0.894\:(+27.40\%)$ & $\boldsymbol{0.878\:(+35.95\%)}$ & $0.860\:(+34.16\%)$ \\ 
% AutoEncoder & $0.918\:(-4.15\%)$ & $\boldsymbol{0.967\:(+0.82\%)}$ & $0.927\:(-1.10\%)$ & $\boldsymbol{0.959\:(+2.07\%)}$ & $0.905\:(+4.77\%)$ & $\boldsymbol{0.917\:(+5.98\%)}$ \\ 
% LUNAR & $0.905\:(+1.22\%)$ & $\boldsymbol{0.954\:(+6.09\%)}$ & $0.920\:(+8.54\%)$ & $\boldsymbol{0.934\:(+9.93\%)}$ & $\boldsymbol{0.898\:(+16.74\%)}$ & $0.884\:(+15.32\%)$ \\ 
% KNN Distance & $\boldsymbol{0.977\:(+2.94\%)}$ & $0.971\:(+2.37\%)$ & $\boldsymbol{0.971\:(+8.79\%)}$ & $0.952\:(+6.88\%)$ & $\boldsymbol{0.916\:(+13.32\%)}$ & $0.915\:(+13.26\%)$ \\ 
% GOAD & $\boldsymbol{0.960\:(+0.65\%)}$ & $0.959\:(+0.53\%)$ & $\boldsymbol{0.964\:(+1.85\%)}$ & $0.959\:(+1.32\%)$ & $\boldsymbol{0.918\:(+4.57\%)}$ & $0.918\:(+4.57\%)$ \\ 
% Deep SVDD & $\boldsymbol{0.953\:(+21.60\%)}$ & $0.857\:(+11.96\%)$ & $\boldsymbol{0.944\:(+26.04\%)}$ & $0.859\:(+17.55\%)$ & $\boldsymbol{0.876\:(+25.15\%)}$ & $0.839\:(+21.42\%)$ \\ 
% RCA & $0.792\:(-1.01\%)$ & $\boldsymbol{0.821\:(+1.85\%)}$ & $0.781\:(+1.30\%)$ & $\boldsymbol{0.792\:(+2.45\%)}$ & $0.760\:(+6.11\%)$ & $\boldsymbol{0.763\:(+6.40\%)}$ \\ 
% NeuTraL & $\boldsymbol{0.929\:(+3.04\%)}$ & $0.924\:(+2.58\%)$ & $\boldsymbol{0.920\:(+6.28\%)}$ & $0.910\:(+5.37\%)$ & $\boldsymbol{0.877\:(+9.49\%)}$ & $0.874\:(+9.18\%)$ \\ 
% \bottomrule
% \end{tabular}
% \end{adjustbox}
%     \caption{\textbf{Performance Improvements from Outlier Filtering using RANSAC-NN (Fast-ViT).}}
%     \label{suppl:outlier_filtering_results_fastvit}
% \vspace*{-\baselineskip}
% \end{table*}

% % Figure environment removed






\section{Run-Time Performance}
We provide a comparison of the run-time of each of the algorithms from Section \ref{exp:experiment_setup} using the PyOD \cite{zhao2019pyod} and DeepOD \cite{deepod} libraries. Nearest-neighbors in RANSAC-NN was implemented using FAISS \cite{faiss}. Experiments were carried out on an Intel Core i7-8700K @ 3.7GHz CPU. We consider a feature dimension of $256$, and we evaluate the algorithms on datasets containing $1000$, $10000$, and $100000$ samples. The average run-time from 4 repeated experiments are shown in Figure \ref{suppl:runtime-exp}.  
% Figure environment removed


% Figure environment removed


% \clearpage

\section{Applications In Image Mislabeled Detection}


Since RANSAC-NN operates as a one-class classifier, its applications in image mislabeled detection may also be explored. In the case of \textbf{image-level} labels, one may apply RANSAC-NN on the set of images belonging to every class. Images that are mislabeled may be detected with potentially high outlier scores. In the case of \textbf{bounding-box} labels, one may apply RANSAC-NN on the crops of bounding boxes from every object class. Mislabeled bounding boxes may be correlated with abnormally high outlier scores. A potential approach using the Faster-RCNN \cite{fasterrcnn} architecture is illustrated in Figure \ref{suppl:bbox_mislabeled_detection}.
