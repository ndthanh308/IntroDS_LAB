\begin{thebibliography}{21}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Boyle(1995)}]{boyle1995myers}
Gregory~J Boyle. 1995.
\newblock Myers-briggs type indicator (mbti): some psychometric limitations.
\newblock \emph{Australian Psychologist}, 30(1):71--74.

\bibitem[{Cabrera et~al.(2023)Cabrera, Loyola, Maga{\~{n}}a, and
  Rojas}]{cabrera2023ethical}
Johana Cabrera, Mar{\'{\i}}a~Soledad Loyola, Irene Maga{\~{n}}a, and Rodrigo
  Rojas. 2023.
\newblock Ethical dilemmas, mental health, artificial intelligence, and
  llm-based chatbots.
\newblock In \emph{Bioinformatics and Biomedical Engineering - 10th
  International Work-Conference, {IWBBIO} 2023, Meloneras, Gran Canaria, Spain,
  July 12-14, 2023 Proceedings, Part {II}}, volume 13920 of \emph{Lecture Notes
  in Computer Science}, pages 313--326. Springer.

\bibitem[{Han et~al.(2021)Han, Chen, Xiao, Zhang, Zeng, and Chen}]{han2021fine}
Ning Han, Jingjing Chen, Guangyi Xiao, Hao Zhang, Yawen Zeng, and Hao Chen.
  2021.
\newblock Fine-grained cross-modal alignment network for text-video retrieval.
\newblock In \emph{Proceedings of the 29th ACM International Conference on
  Multimedia}, pages 3826--3834. ACM.

\bibitem[{Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt}]{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt. 2021.
\newblock \href {http://arxiv.org/abs/2009.03300} {Measuring massive multitask
  language understanding}.

\bibitem[{Huang et~al.(2023)Huang, Bai, Zhu, Zhang, Zhang, Su, Liu, Lv, Zhang,
  Lei, Fu, Sun, and He}]{huang2023ceval}
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su,
  Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and
  Junxian He. 2023.
\newblock C-eval: A multi-level multi-discipline chinese evaluation suite for
  foundation models.
\newblock \emph{arXiv preprint arXiv:2305.08322}.

\bibitem[{Li et~al.(2023{\natexlab{a}})Li, Cheng, Zhao, Nie, and
  Wen}]{li2023halueval}
Junyi Li, Xiaoxue Cheng, Wayne~Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.
  2023{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2305.11747} {Halueval: A large-scale
  hallucination evaluation benchmark for large language models}.

\bibitem[{Li et~al.(2023{\natexlab{b}})Li, Li, Joty, Liu, Huang, Qiu, and
  Bing}]{li2023does}
Xingxuan Li, Yutong Li, Shafiq Joty, Linlin Liu, Fei Huang, Lin Qiu, and Lidong
  Bing. 2023{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2212.10529} {Does gpt-3 demonstrate
  psychopathy? evaluating large language models from a psychological
  perspective}.

\bibitem[{Mökander et~al.()Mökander, Schuett, Kirk, and
  Floridi}]{M_kander_2023}
Jakob Mökander, Jonas Schuett, Hannah~Rose Kirk, and Luciano Floridi.
\newblock Auditing large language models: a three-layered approach.
\newblock \emph{{AI} and Ethics}.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell,
  Welinder, Christiano, Leike, and Lowe}]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
\newblock \href {http://arxiv.org/abs/2203.02155} {Training language models to
  follow instructions with human feedback}.

\bibitem[{Rao et~al.(2023)Rao, Leung, and Miao}]{rao2023can}
Haocong Rao, Cyril Leung, and Chunyan Miao. 2023.
\newblock Can {ChatGPT} assess human personalities? a general evaluation
  framework.
\newblock \emph{arXiv preprint arXiv:2303.01248}.

\bibitem[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov}]{schulman2017ppo}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
  2017.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv:1707.06347}.

\bibitem[{Shi et~al.(2023)Shi, Suzgun, Freitag, Wang, Srivats, Vosoughi, Chung,
  Tay, Ruder, Zhou, Das, and Wei}]{shi2023cot}
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush
  Vosoughi, Hyung~Won Chung, Yi~Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das,
  and Jason Wei. 2023.
\newblock \href {https://openreview.net/pdf?id=fR3wGCk-IXp} {Language models
  are multilingual chain-of-thought reasoners}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net.

\bibitem[{Suzgun et~al.(2023)Suzgun, Scales, Sch{\"{a}}rli, Gehrmann, Tay,
  Chung, Chowdhery, Le, Chi, Zhou, and Wei}]{suzgun2023cot}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"{a}}rli, Sebastian Gehrmann,
  Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V. Le, Ed~Chi, Denny Zhou,
  and Jason Wei. 2023.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve
  them.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  {ACL} 2023, Toronto, Canada, July 9-14, 2023}, pages 13003--13051.
  Association for Computational Linguistics.

\bibitem[{Talmor et~al.(2019)Talmor, Herzig, Lourie, and
  Berant}]{talmor2019commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019.
\newblock \href {http://arxiv.org/abs/1811.00937} {Commonsenseqa: A question
  answering challenge targeting commonsense knowledge}.

\bibitem[{Varshney et~al.(2023)Varshney, Yao, Zhang, Chen, and
  Yu}]{varshney2023hallucination}
Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023.
\newblock \href {https://doi.org/10.48550/arXiv.2307.03987} {A stitch in time
  saves nine: Detecting and mitigating hallucinations of llms by validating
  low-confidence generation}.
\newblock \emph{CoRR}, abs/2307.03987.

\bibitem[{Wang et~al.(2022)Wang, Zeng, Chen, Zhao, and Chen}]{WANG2022117114}
Huanwen Wang, Yawen Zeng, Jianguo Chen, Zhouting Zhao, and Hao Chen. 2022.
\newblock \href {https://doi.org/https://doi.org/10.1016/j.eswa.2022.117114} {A
  spatiotemporal graph neural network for session-based recommendation}.
\newblock \emph{Expert Systems with Applications}, 202:117114.

\bibitem[{White et~al.(2023)White, Fu, Hays, Sandborn, Olea, Gilbert, Elnashar,
  Spencer-Smith, and Schmidt}]{white2023prompt}
Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert,
  Ashraf Elnashar, Jesse Spencer-Smith, and Douglas~C. Schmidt. 2023.
\newblock \href {http://arxiv.org/abs/2302.11382} {A prompt pattern catalog to
  enhance prompt engineering with chatgpt}.

\bibitem[{Xu et~al.(2023)Xu, Yang, Lin, Wang, Zhou, Zhang, and
  Mao}]{xu2023expertprompting}
Benfeng Xu, An~Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and
  Zhendong Mao. 2023.
\newblock \href {http://arxiv.org/abs/2305.14688} {Expertprompting: Instructing
  large language models to be distinguished experts}.

\bibitem[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi}]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.
\newblock \href {http://arxiv.org/abs/1905.07830} {Hellaswag: Can a machine
  really finish your sentence?}

\bibitem[{Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang,
  Dong, Du, Yang, Chen, Chen, Jiang, Ren, Li, Tang, Liu, Liu, Nie, and
  Wen}]{zhao2023survey}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
  Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,
  Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,
  Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.
\newblock \href {http://arxiv.org/abs/2303.18223} {A survey of large language
  models}.

\bibitem[{Zhao et~al.(2020)Zhao, Shang, Liu, Wang, and Liu}]{zhao2020ape210k}
Wei Zhao, Mingyue Shang, Yang Liu, Liang Wang, and Jingming Liu. 2020.
\newblock \href {http://arxiv.org/abs/2009.11506} {Ape210k: A large-scale and
  template-rich dataset of math word problems}.

\end{thebibliography}
