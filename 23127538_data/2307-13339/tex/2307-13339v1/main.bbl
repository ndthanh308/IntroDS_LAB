\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Black et~al.(2021)Black, Leo, Wang, Leahy, and Biderman]{gpt-neo}
Black, S., Leo, G., Wang, P., Leahy, C., and Biderman, S.
\newblock {GPT-Neo: Large Scale Autoregressive Language Modeling with
  Mesh-Tensorflow}, March 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5297715}.
\newblock {If you use this software, please cite it using these metadata.}

\bibitem[Bolukbasi et~al.(2021)Bolukbasi, Pearce, Yuan, Coenen, Reif, Viégas,
  and Wattenberg]{bolukbasi2021interpretability}
Bolukbasi, T., Pearce, A., Yuan, A., Coenen, A., Reif, E., Viégas, F., and
  Wattenberg, M.
\newblock An interpretability illusion for bert, 2021.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021gsm8k}
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert,
  M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Golovneva et~al.(2022)Golovneva, Chen, Poff, Corredor, Zettlemoyer,
  Fazel-Zarandi, and Celikyilmaz]{golovneva2022roscoe}
Golovneva, O., Chen, M., Poff, S., Corredor, M., Zettlemoyer, L.,
  Fazel-Zarandi, M., and Celikyilmaz, A.
\newblock Roscoe: A suite of metrics for scoring step-by-step reasoning, 2022.

\bibitem[Jacovi et~al.(2021)Jacovi, Swayamdipta, Ravfogel, Elazar, Choi, and
  Goldberg]{jacovi-etal-2021-contrastive}
Jacovi, A., Swayamdipta, S., Ravfogel, S., Elazar, Y., Choi, Y., and Goldberg,
  Y.
\newblock Contrastive explanations for model interpretability.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  1597--1611, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.120}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.120}.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and
  Iwasawa]{https://doi.org/10.48550/arxiv.2205.11916}
Kojima, T., Gu, S.~S., Reid, M., Matsuo, Y., and Iwasawa, Y.
\newblock Large language models are zero-shot reasoners, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.11916}.

\bibitem[Krishna et~al.(2022)Krishna, Han, Gu, Pombra, Jabbari, Wu, and
  Lakkaraju]{krishna2022disagreement}
Krishna, S., Han, T., Gu, A., Pombra, J., Jabbari, S., Wu, S., and Lakkaraju,
  H.
\newblock The disagreement problem in explainable machine learning: A
  practitioner's perspective, 2022.

\bibitem[Lampinen et~al.(2022)Lampinen, Dasgupta, Chan, Matthewson, Tessler,
  Creswell, McClelland, Wang, and Hill]{lampinen2022can}
Lampinen, A.~K., Dasgupta, I., Chan, S.~C., Matthewson, K., Tessler, M.~H.,
  Creswell, A., McClelland, J.~L., Wang, J.~X., and Hill, F.
\newblock Can language models learn from explanations in context?
\newblock \emph{arXiv preprint arXiv:2204.02329}, 2022.

\bibitem[Li et~al.(2016)Li, Chen, Hovy, and Jurafsky]{li-etal-2016-visualizing}
Li, J., Chen, X., Hovy, E., and Jurafsky, D.
\newblock Visualizing and understanding neural models in {NLP}.
\newblock In \emph{Proceedings of the 2016 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp.\  681--691, San Diego, California, June 2016. Association
  for Computational Linguistics.
\newblock \doi{10.18653/v1/N16-1082}.
\newblock URL \url{https://aclanthology.org/N16-1082}.

\bibitem[Madaan \& Yazdanbakhsh(2022)Madaan and Yazdanbakhsh]{madaan2022text}
Madaan, A. and Yazdanbakhsh, A.
\newblock Text and patterns: For effective chain of thought, it takes two to
  tango.
\newblock \emph{arXiv preprint arXiv:2209.07686}, 2022.

\bibitem[Madsen et~al.(2022)Madsen, Reddy, and Chandar]{Madsen_2022}
Madsen, A., Reddy, S., and Chandar, S.
\newblock Post-hoc interpretability for neural {NLP}: A survey.
\newblock \emph{{ACM} Computing Surveys}, 55\penalty0 (8):\penalty0 1--42, dec
  2022.
\newblock \doi{10.1145/3546577}.
\newblock URL \url{https://doi.org/10.1145%2F3546577}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher-etal-2013-recursive}
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.~D., Ng, A., and
  Potts, C.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Proceedings of the 2013 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  1631--1642, Seattle, Washington, USA,
  October 2013. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/D13-1170}.

\bibitem[Sundararajan et~al.(2017)Sundararajan, Taly, and
  Yan]{https://doi.org/10.48550/arxiv.1703.01365}
Sundararajan, M., Taly, A., and Yan, Q.
\newblock Axiomatic attribution for deep networks, 2017.
\newblock URL \url{https://arxiv.org/abs/1703.01365}.

\bibitem[Talmor et~al.(2019)Talmor, Herzig, Lourie, and
  Berant]{talmor-etal-2019-commonsenseqa}
Talmor, A., Herzig, J., Lourie, N., and Berant, J.
\newblock {C}ommonsense{QA}: A question answering challenge targeting
  commonsense knowledge.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  4149--4158,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1421}.
\newblock URL \url{https://aclanthology.org/N19-1421}.

\bibitem[Turpin et~al.(2023)Turpin, Michael, Perez, and
  Bowman]{turpin2023language}
Turpin, M., Michael, J., Perez, E., and Bowman, S.~R.
\newblock Language models don't always say what they think: Unfaithful
  explanations in chain-of-thought prompting, 2023.

\bibitem[Wallace et~al.(2019)Wallace, Tuyls, Wang, Subramanian, Gardner, and
  Singh]{allennlp}
Wallace, E., Tuyls, J., Wang, J., Subramanian, S., Gardner, M., and Singh, S.
\newblock Allennlp interpret: A framework for explaining predictions of nlp
  models.
\newblock \emph{arXiv preprint arXiv:1909.09251}, 2019.

\bibitem[Wang \& Komatsuzaki(2021)Wang and Komatsuzaki]{gpt-j}
Wang, B. and Komatsuzaki, A.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem[Wang et~al.(2022)Wang, Min, Deng, Shen, Wu, Zettlemoyer, and
  Sun]{wang2022towards}
Wang, B., Min, S., Deng, X., Shen, J., Wu, Y., Zettlemoyer, L., and Sun, H.
\newblock Towards understanding chain-of-thought prompting: An empirical study
  of what matters.
\newblock \emph{arXiv preprint arXiv:2212.10001}, 2022.

\bibitem[Wang et~al.(2023)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery,
  and Zhou]{wang2023selfconsistency}
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A.,
  and Zhou, D.
\newblock Self-consistency improves chain of thought reasoning in language
  models, 2023.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le,
  and Zhou]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E.~H.,
  Le, Q.~V., and Zhou, D.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=_VjQlMeSB_J}.

\bibitem[Yin \& Neubig(2022)Yin and Neubig]{yin2022interpreting}
Yin, K. and Neubig, G.
\newblock Interpreting language models with contrastive explanations.
\newblock \emph{arXiv preprint arXiv:2202.10419}, 2022.

\bibitem[Zelikman et~al.(2022{\natexlab{a}})Zelikman, Wu, Mu, and
  Goodman]{https://doi.org/10.48550/arxiv.2203.14465}
Zelikman, E., Wu, Y., Mu, J., and Goodman, N.~D.
\newblock Star: Bootstrapping reasoning with reasoning, 2022{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2203.14465}.

\bibitem[Zelikman et~al.(2022{\natexlab{b}})Zelikman, Wu, Mu, and
  Goodman]{star}
Zelikman, E., Wu, Y., Mu, J., and Goodman, N.~D.
\newblock Star: Bootstrapping reasoning with reasoning.
\newblock mar 2022{\natexlab{b}}.

\bibitem[Zhang et~al.(2022)Zhang, Zhang, Li, and Smola]{zhang2022automatic}
Zhang, Z., Zhang, A., Li, M., and Smola, A.
\newblock Automatic chain of thought prompting in large language models.
\newblock \emph{arXiv preprint arXiv:2210.03493}, 2022.

\end{thebibliography}
