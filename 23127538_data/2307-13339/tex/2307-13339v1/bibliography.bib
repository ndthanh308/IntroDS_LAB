@misc{https://doi.org/10.48550/arxiv.2206.01254,
  doi = {10.48550/ARXIV.2206.01254},
  
  url = {https://arxiv.org/abs/2206.01254},
  
  author = {Han, Tessa and Srinivas, Suraj and Lakkaraju, Himabindu},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post Hoc Explanations},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{madaan2022text,
  title={Text and patterns: For effective chain of thought, it takes two to tango},
  author={Madaan, Aman and Yazdanbakhsh, Amir},
  journal={arXiv preprint arXiv:2209.07686},
  year={2022}
}

@misc{https://doi.org/10.48550/arxiv.1706.03762,
  doi = {10.48550/ARXIV.1706.03762},
  
  url = {https://arxiv.org/abs/1706.03762},
  
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Attention Is All You Need},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{chatgpt,
  title={Smarter than Humans? Validating how OpenAI’s ChatGPT Model Explains Crowdfunding, Alternative Finance and Community Finance},
  author={Wenzlaff and Spaeth},
  journal={SSRN},
  year={2022}
}

@article{lampinen2022can,
  title={Can language models learn from explanations in context?},
  author={Lampinen, Andrew K and Dasgupta, Ishita and Chan, Stephanie CY and Matthewson, Kory and Tessler, Michael Henry and Creswell, Antonia and McClelland, James L and Wang, Jane X and Hill, Felix},
  journal={arXiv preprint arXiv:2204.02329},
  year={2022}
}

@article{zhang2022automatic,
  title={Automatic chain of thought prompting in large language models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  journal={arXiv preprint arXiv:2210.03493},
  year={2022}
}

@misc{https://doi.org/10.48550/arxiv.2203.14465,
  doi = {10.48550/ARXIV.2203.14465},
  
  url = {https://arxiv.org/abs/2203.14465},
  
  author = {Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah D.},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {STaR: Bootstrapping Reasoning With Reasoning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2205.11916,
  doi = {10.48550/ARXIV.2205.11916},
  
  url = {https://arxiv.org/abs/2205.11916},
  
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Large Language Models are Zero-Shot Reasoners},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{li-etal-2016-visualizing,
    title = "Visualizing and Understanding Neural Models in {NLP}",
    author = "Li, Jiwei  and
      Chen, Xinlei  and
      Hovy, Eduard  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1082",
    doi = "10.18653/v1/N16-1082",
    pages = "681--691",
}

@misc{https://doi.org/10.48550/arxiv.1703.01365,
  doi = {10.48550/ARXIV.1703.01365},
  
  url = {https://arxiv.org/abs/1703.01365},
  
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Axiomatic Attribution for Deep Networks},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Madsen_2022,
	doi = {10.1145/3546577},
  
	url = {https://doi.org/10.1145%2F3546577},
  
	year = 2022,
	month = {dec},
  
	publisher = {Association for Computing Machinery ({ACM})},
  
	volume = {55},
  
	number = {8},
  
	pages = {1--42},
  
	author = {Andreas Madsen and Siva Reddy and Sarath Chandar},
  
	title = {Post-hoc Interpretability for Neural {NLP}: A Survey},
  
	journal = {{ACM} Computing Surveys}
}

@inproceedings{jacovi-etal-2021-contrastive,
    title = "Contrastive Explanations for Model Interpretability",
    author = "Jacovi, Alon  and
      Swayamdipta, Swabha  and
      Ravfogel, Shauli  and
      Elazar, Yanai  and
      Choi, Yejin  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.120",
    doi = "10.18653/v1/2021.emnlp-main.120",
    pages = "1597--1611",
    abstract = "Contrastive explanations clarify why an event occurred in contrast to another. They are inherently intuitive to humans to both produce and comprehend. We propose a method to produce contrastive explanations in the latent space, via a projection of the input representation, such that only the features that differentiate two potential decisions are captured. Our modification allows model behavior to consider only contrastive reasoning, and uncover which aspects of the input are useful for and against particular decisions. Our contrastive explanations can additionally answer for which label, and against which alternative label, is a given input feature useful. We produce contrastive explanations via both high-level abstract concept attribution and low-level input token/span attribution for two NLP classification benchmarks. Our findings demonstrate the ability of label-contrastive explanations to provide fine-grained interpretability of model decisions.",
}

@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{wang2023selfconsistency,
      title={Self-Consistency Improves Chain of Thought Reasoning in Language Models}, 
      author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
      year={2023},
      eprint={2203.11171},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@software{gpt-neo,
  author       = {Black, Sid and
                  Leo, Gao and
                  Wang, Phil and
                  Leahy, Connor and
                  Biderman, Stella},
  title        = {{GPT-Neo: Large Scale Autoregressive Language 
                   Modeling with Mesh-Tensorflow}},
  month        = mar,
  year         = 2021,
  note         = {{If you use this software, please cite it using 
                   these metadata.}},
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5297715},
  url          = {https://doi.org/10.5281/zenodo.5297715}
}

@inproceedings{socher-etal-2013-recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1170",
    pages = "1631--1642",
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{talmor-etal-2019-commonsenseqa,
    title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
    author = "Talmor, Alon  and
      Herzig, Jonathan  and
      Lourie, Nicholas  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1421",
    doi = "10.18653/v1/N19-1421",
    pages = "4149--4158",
    archivePrefix = "arXiv",
    eprint        = "1811.00937",
    primaryClass  = "cs",
}

@inproceedings{
creswell2023selectioninference,
title={Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning},
author={Antonia Creswell and Murray Shanahan and Irina Higgins},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=3Pf3Wg6o-A4}
}

@article{shrikumar2016not,
  title={Not just a black box: Learning important features through propagating activation differences},
  author={Shrikumar, Avanti and Greenside, Peyton and Shcherbina, Anna and Kundaje, Anshul},
  journal={arXiv preprint arXiv:1605.01713},
  year={2016}
}

@article{allennlp,
  title={Allennlp interpret: A framework for explaining predictions of nlp models},
  author={Wallace, Eric and Tuyls, Jens and Wang, Junlin and Subramanian, Sanjay and Gardner, Matt and Singh, Sameer},
  journal={arXiv preprint arXiv:1909.09251},
  year={2019}
}

@inproceedings{
ye2022the,
title={The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning},
author={Xi Ye and Greg Durrett},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=Bct2f8fRd8S}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{gao2020making,
  title={Making pre-trained language models better few-shot learners},
  author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  journal={arXiv preprint arXiv:2012.15723},
  year={2020}
}

@inproceedings{ling-etal-2017-program,
    title = "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
    author = "Ling, Wang  and
      Yogatama, Dani  and
      Dyer, Chris  and
      Blunsom, Phil",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1015",
    doi = "10.18653/v1/P17-1015",
    pages = "158--167",
    abstract = "Solving algebraic word problems requires executing a series of arithmetic operations{---}a program{---}to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.",
}

@article{ding2021openprompt,
  title={OpenPrompt: An Open-source Framework for Prompt-learning},
  author={Ding, Ning and Hu, Shengding and Zhao, Weilin and Chen, Yulin and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong},
  journal={arXiv preprint arXiv:2111.01998},
  year={2021}
}

@article{yin2022interpreting,
  title={Interpreting language models with contrastive explanations},
  author={Yin, Kayo and Neubig, Graham},
  journal={arXiv preprint arXiv:2202.10419},
  year={2022}
}

@inproceedings{
wei2022chain,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=_VjQlMeSB_J}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{wang2022towards,
  title={Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters},
  author={Wang, Boshi and Min, Sewon and Deng, Xiang and Shen, Jiaming and Wu, You and Zettlemoyer, Luke and Sun, Huan},
  journal={arXiv preprint arXiv:2212.10001},
  year={2022}
}

@article{wang2020gradient,
  title={Gradient-based analysis of NLP models is manipulable},
  author={Wang, Junlin and Tuyls, Jens and Wallace, Eric and Singh, Sameer},
  journal={arXiv preprint arXiv:2010.05419},
  year={2020}
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{star,
    title={STaR: Bootstrapping Reasoning with Reasoning},
    author={Eric Zelikman and Yuhuai Wu and Jesse Mu and Noah D. Goodman},
    year={2022},
    month={mar},
    eprint={2203.14465},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{bolukbasi2021interpretability,
      title={An Interpretability Illusion for BERT}, 
      author={Tolga Bolukbasi and Adam Pearce and Ann Yuan and Andy Coenen and Emily Reif and Fernanda Viégas and Martin Wattenberg},
      year={2021},
      eprint={2104.07143},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{turpin2023language,
      title={Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting}, 
      author={Miles Turpin and Julian Michael and Ethan Perez and Samuel R. Bowman},
      year={2023},
      eprint={2305.04388},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{krishna2022disagreement,
      title={The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective}, 
      author={Satyapriya Krishna and Tessa Han and Alex Gu and Javin Pombra and Shahin Jabbari and Steven Wu and Himabindu Lakkaraju},
      year={2022},
      eprint={2202.01602},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{golovneva2022roscoe,
      title={ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning}, 
      author={Olga Golovneva and Moya Chen and Spencer Poff and Martin Corredor and Luke Zettlemoyer and Maryam Fazel-Zarandi and Asli Celikyilmaz},
      year={2022},
      eprint={2212.07919},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}