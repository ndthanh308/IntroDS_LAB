\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{alsubait2014generating}
Alsubait, T., Parsia, B., Sattler, U.: Generating multiple questions from
  ontologies: How far can we go? In: Proceedings from the First International
  Workshop on Educational Knowledge Management (EKM 2014), Link{\"o}ping,
  November 24, 2014. pp. 19--30. No.~104, Link{\"o}ping University Electronic
  Press (2014)

\bibitem{bitew-etal-2023-learning}
Bitew, S.K., Deleu, J., Dogru{\"o}z, A.S., Develder, C., Demeester, T.:
  Learning from partially annotated data: Example-aware creation of gap-filling
  exercises for language learning. In: Proceedings of the 18th Workshop on
  Innovative Use of NLP for Building Educational Applications (BEA 2023). pp.
  598--609. Association for Computational Linguistics, Toronto, Canada (Jul
  2023), \url{https://aclanthology.org/2023.bea-1.51}

\bibitem{bitew2022learning}
Bitew, S.K., Hadifar, A., Sterckx, L., Deleu, J., Develder, C., Demeester, T.:
  Learning to reuse distractors to support multiple choice question generation
  in education. IEEE Transactions on Learning Technologies  (2022).
  \doi{10.1109/TLT.2022.3226523}

\bibitem{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.: Language models
  are few-shot learners. Advances in neural information processing systems
  \textbf{33},  1877--1901 (2020)

\bibitem{cavalcanti2021automatic}
Cavalcanti, A.P., Barbosa, A., Carvalho, R., Freitas, F., Tsai, Y.S.,
  Ga{\v{s}}evi{\'c}, D., Mello, R.F.: Automatic feedback in online learning
  environments: A systematic literature review. Computers and Education:
  Artificial Intelligence  \textbf{2},  100027 (2021)

\bibitem{choi2023chatgpt}
Choi, J.H., Hickman, K.E., Monahan, A., Schwarcz, D.: Chatgpt goes to law
  school. Available at SSRN  (2023)

\bibitem{chung-etal-2020-bert}
Chung, H.L., Chan, Y.H., Fan, Y.C.: A {BERT}-based distractor generation scheme
  with multi-tasking and negative answer training strategies. In: Findings of
  the Association for Computational Linguistics: EMNLP 2020. pp. 4390--4400.
  Association for Computational Linguistics, Online (Nov 2020).
  \doi{10.18653/v1/2020.findings-emnlp.393}

\bibitem{faizan2018automatic}
Faizan, A., Lohmann, S.: Automatic generation of multiple choice questions from
  slide content using linked data. In: Proceedings of the 8th International
  Conference on Web Intelligence, Mining and Semantics. pp.~1--8 (2018)

\bibitem{gao2019generating}
Gao, Y., Bing, L., Li, P., King, I., Lyu, M.R.: Generating distractors for
  reading comprehension questions from real examinations. In: Proceedings of
  the AAAI Conference on Artificial Intelligence. vol.~33, pp. 6423--6430
  (2019)

\bibitem{gierl2017developing}
Gierl, M.J., Bulut, O., Guo, Q., Zhang, X.: Developing, analyzing, and using
  distractors for multiple-choice tests in education: A comprehensive review.
  Review of Educational Research  \textbf{87}(6),  1082--1116 (2017)

\bibitem{gilson2023does}
Gilson, A., Safranek, C.W., Huang, T., Socrates, V., Chi, L., Taylor, R.A.,
  Chartash, D., et~al.: How does chatgpt perform on the united states medical
  licensing examination? the implications of large language models for medical
  education and knowledge assessment. JMIR Medical Education  \textbf{9}(1),
  e45312 (2023)

\bibitem{guo2016questimator}
Guo, Q., Kulkarni, C., Kittur, A., Bigham, J.P., Brunskill, E.: Questimator:
  Generating knowledge assessments for arbitrary topics. In: IJCAI-16:
  Proceedings of the AAAI Twenty-Fifth International Joint Conference on
  Artificial Intelligence (2016)

\bibitem{jiang2017distractor}
Jiang, S., Lee, J.S.: Distractor generation for chinese fill-in-the-blank
  items. In: Proceedings of the 12th Workshop on Innovative Use of NLP for
  Building Educational Applications. pp. 143--148 (2017)

\bibitem{kalpakchi-boye-2021-bert}
Kalpakchi, D., Boye, J.: {BERT}-based distractor generation for {S}wedish
  reading comprehension questions using a small-scale dataset. In: Proceedings
  of the 14th International Conference on Natural Language Generation. pp.
  387--403. Association for Computational Linguistics, Aberdeen, Scotland, UK
  (Aug 2021), \url{https://aclanthology.org/2021.inlg-1.43}

\bibitem{kurdi2020systematic}
Kurdi, G., Leo, J., Parsia, B., Sattler, U., Al-Emari, S.: A systematic review
  of automatic question generation for educational purposes. International
  Journal of Artificial Intelligence in Education  \textbf{30}(1),  121--204
  (2020)

\bibitem{leo2019ontology}
Leo, J., Kurdi, G., Matentzoglu, N., Parsia, B., Sattler, U., Forge, S.,
  Donato, G., Dowling, W.: Ontology-based generation of medical, multi-term
  mcqs. International Journal of Artificial Intelligence in Education
  \textbf{29}(2),  145--188 (2019)

\bibitem{li2023can}
Li, Y., Sha, L., Yan, L., Lin, J., Rakovi{\'c}, M., Galbraith, K., Lyons, K.,
  Ga{\v{s}}evi{\'c}, D., Chen, G.: Can large language models write
  reflectively. Computers and Education: Artificial Intelligence  \textbf{4},
  100140 (2023)

\bibitem{liang2018distractor}
Liang, C., Yang, X., Dave, N., Wham, D., Pursel, B., Giles, C.L.: Distractor
  generation for multiple choice questions using learning to rank. In:
  Proceedings of the thirteenth workshop on innovative use of NLP for building
  educational applications. pp. 284--290 (2018)

\bibitem{mchugh2012interrater}
McHugh, M.L.: Interrater reliability: the kappa statistic. Biochemia medica
  \textbf{22}(3),  276--282 (2012)

\bibitem{miller1995wordnet}
Miller, G.A.: Wordnet: a lexical database for english. Communications of the
  ACM  \textbf{38}(11),  39--41 (1995)

\bibitem{mitkov2009semantic}
Mitkov, R., Varga, A., Rello, L., et~al.: Semantic similarity of distractors in
  multiple-choice tests: extrinsic evaluation. In: Proceedings of the workshop
  on geometrical models of natural language semantics. pp. 49--56 (2009)

\bibitem{openai2023gpt4}
OpenAI: Gpt-4 technical report (2023)

\bibitem{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,
  C., Agarwal, S., Slama, K., Ray, A., et~al.: Training language models to
  follow instructions with human feedback. Advances in Neural Information
  Processing Systems  \textbf{35},  27730--27744 (2022)

\bibitem{panda-etal-2022-automatic}
Panda, S., Palma~Gomez, F., Flor, M., Rozovskaya, A.: Automatic generation of
  distractors for fill-in-the-blank exercises with round-trip neural machine
  translation. In: Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics: Student Research Workshop. pp. 391--401.
  Association for Computational Linguistics, Dublin, Ireland (May 2022).
  \doi{10.18653/v1/2022.acl-srw.31}

\bibitem{papasalouros2008automatic}
Papasalouros, A., Kanaris, K., Kotis, K.: Automatic generation of multiple
  choice questions from domain ontologies. e-Learning  \textbf{1},  427--434
  (2008)

\bibitem{pino2008selection}
Pino, J., Heilman, M., Eskenazi, M.: A selection strategy to improve cloze
  question quality. In: Proceedings of the Workshop on Intelligent Tutoring
  Systems for Ill-Defined Domains. 9th International Conference on Intelligent
  Tutoring Systems, Montreal, Canada. pp. 22--32. Citeseer (2008)

\bibitem{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.:
  Language models are unsupervised multitask learners. OpenAI blog
  \textbf{1}(8), ~9 (2019)

\bibitem{ramesh2022automated}
Ramesh, D., Sanampudi, S.K.: An automated essay scoring systems: a systematic
  literature review. Artificial Intelligence Review  \textbf{55}(3),
  2495--2527 (2022)

\bibitem{ramsden2003learning}
Ramsden, P.: Learning to teach in higher education. Routledge (2003)

\bibitem{rodriguez2022end}
Rodriguez-Torrealba, R., Garcia-Lopez, E., Garcia-Cabot, A.: End-to-end
  generation of multiple-choice questions using text-to-text transfer
  transformer models. Expert Systems with Applications  \textbf{208},  118258
  (2022)

\bibitem{roediger2006test}
Roediger~III, H.L., Karpicke, J.D.: Test-enhanced learning: Taking memory tests
  improves long-term retention. Psychological science  \textbf{17}(3),
  249--255 (2006)

\bibitem{sakai2007evaluating}
Sakai, T.: Evaluating information retrieval metrics based on bootstrap
  hypothesis tests. IPSJ Digital Courier  \textbf{3},  625--642 (2007)

\bibitem{wang2023self}
Wang, R., Wang, H., Mi, F., Chen, Y., Xu, R., Wong, K.F.: Self-critique
  prompting with large language models for inductive instructions. arXiv
  preprint arXiv:2305.13733  (2023)

\bibitem{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E.H.,
  Le, Q.V., Zhou, D.: Chain of thought prompting elicits reasoning in large
  language models. In: Advances in Neural Information Processing Systems
  (2022), \url{https://openreview.net/forum?id=_VjQlMeSB_J}

\bibitem{xue-etal-2021-mt5}
Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua,
  A., Raffel, C.: m{T}5: A massively multilingual pre-trained text-to-text
  transformer. In: Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies. pp. 483--498. Association for Computational Linguistics, Online
  (Jun 2021). \doi{10.18653/v1/2021.naacl-main.41}

\bibitem{yeung2019difficulty}
Yeung, C.Y., Lee, J.S., Tsou, B.K.: Difficulty-aware distractor generation for
  gap-fill items. In: Proceedings of the The 17th Annual Workshop of the
  Australasian Language Technology Association. pp. 159--164 (2019)

\bibitem{zhou2020co}
Zhou, X., Luo, S., Wu, Y.: Co-attention hierarchical network: Generating
  coherent long distractors for reading comprehension. In: Proceedings of the
  AAAI Conference on Artificial Intelligence. vol.~34, pp. 9725--9732 (2020)

\end{thebibliography}
