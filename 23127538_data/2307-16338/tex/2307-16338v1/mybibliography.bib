% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {$L_1$}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
  url={https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005},
	url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

@article{ct1965,
  title={An algorithm for the machine calculation of complex {F}ourier series},
  author={Cooley, James W. and Tukey, John W.},
  journal={Mathematics of Computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  url={https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}

@article{dobrovolskii2021word,
  title={Word-level coreference resolution},
  author={Dobrovolskii, Vladimir},
  journal={arXiv preprint arXiv:2109.04127},
  year={2021}
}
@inproceedings{wu2020corefqa,
  title={CorefQA: Coreference resolution as query-based span prediction},
  author={Wu, Wei and Wang, Fei and Yuan, Arianna and Wu, Fei and Li, Jiwei},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={6953--6963},
  year={2020}
}
@article{otmazgin2022lingmess,
  title={Lingmess: Linguistically informed multi expert scorers for coreference resolution},
  author={Otmazgin, Shon and Cattan, Arie and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2205.12644},
  year={2022}
}
@article{kirstain2021coreference,
  title={Coreference resolution without span representations},
  author={Kirstain, Yuval and Ram, Ori and Levy, Omer},
  journal={arXiv preprint arXiv:2101.00434},
  year={2021}
}
@inproceedings{otmazgin-etal-2022-f, title = "{F}-coref: Fast, Accurate and Easy to Use Coreference Resolution", author = "Otmazgin, Shon and Cattan, Arie and Goldberg, Yoav", booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: System Demonstrations", month = nov, year = "2022", address = "Taipei, Taiwan", publisher = "Association for Computational Linguistics", url = "https://aclanthology.org/2022.aacl-demo.6", pages = "48--56", abstract = "We introduce fastcoref, a python package for fast, accurate, and easy-to-use English coreference resolution. The package is pip-installable, and allows two modes: an accurate mode based on the LingMess architecture, providing state-of-the-art coreference accuracy, and a substantially faster model, F-coref, which is the focus of this work. F-coref allows to process 2.8K OntoNotes documents in 25 seconds on a V100 GPU (compared to 6 minutes for the LingMess model, and to 12 minutes of the popular AllenNLP coreference model) with only a modest drop in accuracy. The fast speed is achieved through a combination of distillation of a compact model from the LingMess model, and an efficient batching implementation using a technique we call leftover batching. https://github.com/shon-otmazgin/fastcoref", }



@article{liu2022autoregressive,
  title={Autoregressive Structured Prediction with Language Models},
  author={Liu, Tianyu and Jiang, Yuchen and Monath, Nicholas and Cotterell, Ryan and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2210.14698},
  year={2022}
}
@article{bohnet2022coreference,
  title={Coreference Resolution through a seq2seq Transition-Based System},
  author={Bohnet, Bernd and Alberti, Chris and Collins, Michael},
  journal={arXiv preprint arXiv:2211.12142},
  year={2022}
}
@article{xu2020revealing,
  title={Revealing the myth of higher-order inference in coreference resolution},
  author={Xu, Liyan and Choi, Jinho D},
  journal={arXiv preprint arXiv:2009.12013},
  year={2020}
}

@article{shridhar2022longtonotes,
  title={Longtonotes: OntoNotes with Longer Coreference Chains},
  author={Shridhar, Kumar and Monath, Nicholas and Thirukovalluru, Raghuveer and Stolfo, Alessandro and Zaheer, Manzil and McCallum, Andrew and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2210.03650},
  year={2022}
}

@article{xia2021moving,
  title={Moving on from OntoNotes: Coreference resolution model transfer},
  author={Xia, Patrick and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:2104.08457},
  year={2021}
}

@ARTICLE{9969921,
  author={Bitew, Semere Kiros and Hadifar, Amir and Sterckx, Lucas and Deleu, Johannes and Develder, Chris and Demeester, Thomas},
  journal={IEEE Transactions on Learning Technologies}, 
  title={Learning to Reuse Distractors to Support Multiple Choice Question Generation in Education}, 
  year={2022},
  volume={},
  number={},
  pages={1-16},
  doi={10.1109/TLT.2022.3226523}}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}


@inproceedings{
wei2022chain,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
url={https://openreview.net/forum?id=_VjQlMeSB_J}
}

@article{miller1995wordnet,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM New York, NY, USA}
}
@article{papasalouros2008automatic,
  title={Automatic Generation Of Multiple Choice Questions From Domain Ontologies.},
  author={Papasalouros, Andreas and Kanaris, Konstantinos and Kotis, Konstantinos},
  journal={e-Learning},
  volume={1},
  pages={427--434},
  year={2008},
  publisher={Citeseer}
}
@inproceedings{faizan2018automatic,
  title={Automatic generation of multiple choice questions from slide content using linked data},
  author={Faizan, Ainuddin and Lohmann, Steffen},
  booktitle={Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics},
  pages={1--8},
  year={2018}
}
@article{leo2019ontology,
  title={Ontology-based generation of medical, multi-term MCQs},
  author={Leo, Jared and Kurdi, Ghader and Matentzoglu, Nicolas and Parsia, Bijan and Sattler, Ulrike and Forge, Sophie and Donato, Gina and Dowling, Will},
  journal={International Journal of Artificial Intelligence in Education},
  volume={29},
  number={2},
  pages={145--188},
  year={2019},
  publisher={Springer}
}
@inproceedings{alsubait2014generating,
  title={Generating Multiple Questions From Ontologies: How Far Can We Go?},
  author={Alsubait, Tahani and Parsia, Bijan and Sattler, Uli},
  booktitle={Proceedings from the First International Workshop on Educational Knowledge Management (EKM 2014), Link{\"o}ping, November 24, 2014},
  number={104},
  pages={19--30},
  year={2014},
  organization={Link{\"o}ping University Electronic Press}
}
@inproceedings{mitkov2009semantic,
  title={Semantic similarity of distractors in multiple-choice tests: extrinsic evaluation},
  author={Mitkov, Ruslan and Varga, Andrea and Rello, Luz and others},
  booktitle={Proceedings of the workshop on geometrical models of natural language semantics},
  pages={49--56},
  year={2009}
}
@inproceedings{pino2008selection,
  title={A selection strategy to improve cloze question quality},
  author={Pino, Juan and Heilman, Michael and Eskenazi, Maxine},
  booktitle={Proceedings of the Workshop on Intelligent Tutoring Systems for Ill-Defined Domains. 9th International Conference on Intelligent Tutoring Systems, Montreal, Canada},
  pages={22--32},
  year={2008},
  organization={Citeseer}
}
@inproceedings{jiang2017distractor,
  title={Distractor generation for chinese fill-in-the-blank items},
  author={Jiang, Shu and Lee, John SY},
  booktitle={Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications},
  pages={143--148},
  year={2017}
}
@inproceedings{guo2016questimator,
  title={Questimator: Generating knowledge assessments for arbitrary topics},
  author={Guo, Qi and Kulkarni, Chinmay and Kittur, Aniket and Bigham, Jeffrey P and Brunskill, Emma},
  booktitle={IJCAI-16: Proceedings of the AAAI Twenty-Fifth International Joint Conference on Artificial Intelligence},
  year={2016}
}
@inproceedings{liang2018distractor,
  title={Distractor generation for multiple choice questions using learning to rank},
  author={Liang, Chen and Yang, Xiao and Dave, Neisarg and Wham, Drew and Pursel, Bart and Giles, C Lee},
  booktitle={Proceedings of the thirteenth workshop on innovative use of NLP for building educational applications},
  pages={284--290},
  year={2018}
}
@inproceedings{gao2019generating,
  title={Generating distractors for reading comprehension questions from real examinations},
  author={Gao, Yifan and Bing, Lidong and Li, Piji and King, Irwin and Lyu, Michael R},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={6423--6430},
  year={2019}
}
@inproceedings{yeung2019difficulty,
  title={Difficulty-aware distractor generation for gap-fill items},
  author={Yeung, Chak Yan and Lee, John SY and Tsou, Benjamin K},
  booktitle={Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association},
  pages={159--164},
  year={2019}
}

@inproceedings{zhou2020co,
  title={Co-attention hierarchical network: Generating coherent long distractors for reading comprehension},
  author={Zhou, Xiaorui and Luo, Senlin and Wu, Yunfang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={9725--9732},
  year={2020}
}
@inproceedings{chung-etal-2020-bert,
    title = "A {BERT}-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies.",
    author = "Chung, Ho-Lam  and
      Chan, Ying-Hong  and
      Fan, Yao-Chung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2020.findings-emnlp.393",
    pages = "4390--4400",
    abstract = "In this paper, we investigate the following two limitations for the existing distractor generation (DG) methods. First, the quality of the existing DG methods are still far from practical use. There are still room for DG quality improvement. Second, the existing DG designs are mainly for single distractor generation. However, for practical MCQ preparation, multiple distractors are desired. Aiming at these goals, in this paper, we present a new distractor generation scheme with multi-tasking and negative answer training strategies for effectively generating \textit{multiple} distractors. The experimental results show that (1) our model advances the state-of-the-art result from 28.65 to 39.81 (BLEU 1 score) and (2) the generated multiple distractors are diverse and shows strong distracting power for multiple choice question.",
}

@article{rodriguez2022end,
  title={End-to-End generation of Multiple-Choice questions using Text-to-Text transfer Transformer models},
  author={Rodriguez-Torrealba, Ricardo and Garcia-Lopez, Eva and Garcia-Cabot, Antonio},
  journal={Expert Systems with Applications},
  volume={208},
  pages={118258},
  year={2022},
  publisher={Elsevier}
}

@article{kurdi2020systematic,
  title={A systematic review of automatic question generation for educational purposes},
  author={Kurdi, Ghader and Leo, Jared and Parsia, Bijan and Sattler, Uli and Al-Emari, Salam},
  journal={International Journal of Artificial Intelligence in Education},
  volume={30},
  number={1},
  pages={121--204},
  year={2020},
  publisher={Springer}
}
@book{davis2009tools,
  title={Tools for teaching},
  author={Davis, Barbara Gross},
  year={2009},
  publisher={John Wiley \& Sons}
}
@article{wang2020generalizing,
  title={Generalizing from a few examples: A survey on few-shot learning},
  author={Wang, Yaqing and Yao, Quanming and Kwok, James T and Ni, Lionel M},
  journal={ACM computing surveys (csur)},
  volume={53},
  number={3},
  pages={1--34},
  year={2020},
  publisher={ACM New York, NY, USA}
}
@inproceedings{jie2019better,
  title={Better modeling of incomplete annotations for named entity recognition},
  author={Jie, Zhanming and Xie, Pengjun and Lu, Wei and Ding, Ruixue and Li, Linlin},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={729--734},
  year={2019}
}
@inproceedings{bellare2007learning,
  title={Learning extractors from unlabeled text using relevant databases},
  author={Bellare, Kedar and McCallum, Andrew},
  booktitle={Sixth international workshop on information integration on the web},
  year={2007}
}
@inproceedings{al2011ontoque,
  title={OntoQue: a question generation engine for educational assesment based on domain ontologies},
  author={Al-Yahya, Maha},
  booktitle={2011 IEEE 11th International Conference on Advanced Learning Technologies},
  pages={393--395},
  year={2011},
  organization={IEEE}
}

@article{sun2018automatic,
  title={Automatic question tagging with deep neural networks},
  author={Sun, Bo and Zhu, Yunzong and Xiao, Yongkang and Xiao, Rong and Wei, Yungang},
  journal={IEEE Transactions on Learning Technologies},
  volume={12},
  number={1},
  pages={29--43},
  year={2018},
  publisher={IEEE}
}
@article{conejo2016siette,
  title={The SIETTE automatic assessment environment},
  author={Conejo, Ricardo and Guzm{\'a}n, Eduardo and Trella, Monica},
  journal={International Journal of Artificial Intelligence in Education},
  volume={26},
  number={1},
  pages={270--292},
  year={2016},
  publisher={Springer}
}
@inproceedings{stasaski2017multiple,
  title={Multiple choice question generation utilizing an ontology},
  author={Stasaski, Katherine and Hearst, Marti A},
  booktitle={Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications},
  pages={303--312},
  year={2017}
}
@article{pugh2016using,
  title={Using cognitive models to develop quality multiple-choice questions},
  author={Pugh, Debra and De Champlain, Andre and Gierl, Mark and Lai, Hollis and Touchie, Claire},
  journal={Medical teacher},
  volume={38},
  number={8},
  pages={838--843},
  year={2016},
  publisher={Taylor \& Francis}
}
@article{afzal2014automatic,
  title={Automatic generation of multiple choice questions using dependency-based semantic relations},
  author={Afzal, Naveed and Mitkov, Ruslan},
  journal={Soft Computing},
  volume={18},
  number={7},
  pages={1269--1281},
  year={2014},
  publisher={Springer}
}
@article{susanti2017evaluation,
  title={Evaluation of automatically generated english vocabulary questions},
  author={Susanti, Yuni and Tokunaga, Takenobu and Nishikawa, Hitoshi and Obari, Hiroyuki},
  journal={Research and practice in technology enhanced learning},
  volume={12},
  number={1},
  pages={1--21},
  year={2017},
  publisher={Springer}
}
@inproceedings{hill2016automatic,
  title={Automatic generation of context-based fill-in-the-blank exercises using co-occurrence likelihoods and Google n-grams},
  author={Hill, Jennifer and Simha, Rahul},
  booktitle={Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications},
  pages={23--30},
  year={2016}
}
@article{goto2010automatic,
  title={Automatic generation system of multiple-choice cloze questions and its evaluation},
  author={Goto, Takuya and Kojiri, Tomoko and Watanabe, Toyohide and Iwata, Tomoharu and Yamada, Takeshi},
  journal={Knowledge Management \& E-Learning: An International Journal},
  volume={2},
  number={3},
  pages={210--224},
  year={2010}
}
@article{bitew2022learning,
  title={Learning to Reuse Distractors to Support Multiple Choice Question Generation in Education},
  author={Bitew, Semere Kiros and Hadifar, Amir and Sterckx, Lucas and Deleu, Johannes and Develder, Chris and Demeester, Thomas},
  journal={IEEE Transactions on Learning Technologies},
  year={2022},
  publisher={IEEE},
  doi={10.1109/TLT.2022.3226523}
}
@inproceedings{malinova2016automatic,
  title={Automatic generation of english language test questions using mathematica},
  author={Malinova, Anna and Rahneva, Olga},
  booktitle={CBU International Conference Proceedings},
  volume={4},
  pages={906--909},
  year={2016}
}
@inproceedings{perez2012generating,
  title={Generating grammar exercises},
  author={Perez-Beltrachini, Laura and Gardent, Claire and Kruszewski, German},
  booktitle={The 7th Workshop on Innovative Use of NLP for Building Educational Applications, NAACL-HLT Worskhop 2012},
  pages={147--157},
  year={2012}
}
@article{liu2016automatic,
  title={Automatic chinese factual question generation},
  author={Liu, Ming and Rus, Vasile and Liu, Li},
  journal={IEEE Transactions on Learning Technologies},
  volume={10},
  number={2},
  pages={194--204},
  year={2016},
  publisher={IEEE}
}
@article{yang2017transfer,
  title={Transfer learning for sequence tagging with hierarchical recurrent networks},
  author={Yang, Zhilin and Salakhutdinov, Ruslan and Cohen, William W},
  journal={arXiv preprint arXiv:1703.06345},
  year={2017}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@misc{
liu2020roberta,
title={Ro{\{}BERT{\}}a: A Robustly Optimized {\{}BERT{\}} Pretraining Approach},
author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
year={2020},
url={https://openreview.net/forum?id=SyxS0T4tvS}
}

@inproceedings{du-etal-2017-learning,
    title = "Learning to Ask: Neural Question Generation for Reading Comprehension",
    author = "Du, Xinya  and
      Shao, Junru  and
      Cardie, Claire",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1123",
    doi = "10.18653/v1/P17-1123",
    pages = "1342--1352",
    abstract = "We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (\textit{i.e.,}, grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).",
}

@article{conneau2019unsupervised,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.02116},
  year={2019}
}
@article{daradoumis2019analyzing,
  title={Analyzing students' perceptions to improve the design of an automated assessment tool in online distributed programming},
  author={Daradoumis, Thanasis and Puig, Joan Manuel Marqu{\`e}s and Arguedas, Marta and Li{\~n}an, Laura Calvet},
  journal={Computers \& Education},
  volume={128},
  pages={159--170},
  year={2019},
  publisher={Elsevier}
}


@inproceedings{kalpakchi-boye-2021-bert,
    title = "{BERT}-based distractor generation for {S}wedish reading comprehension questions using a small-scale dataset",
    author = "Kalpakchi, Dmytro  and
      Boye, Johan",
    booktitle = "Proceedings of the 14th International Conference on Natural Language Generation",
    month = aug,
    year = "2021",
    address = "Aberdeen, Scotland, UK",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.inlg-1.43",
    pages = "387--403",
    abstract = "An important part when constructing multiple-choice questions (MCQs) for reading comprehension assessment are the distractors, the incorrect but preferably plausible answer options. In this paper, we present a new BERT-based method for automatically generating distractors using only a small-scale dataset. We also release a new such dataset of Swedish MCQs (used for training the model), and propose a methodology for assessing the generated distractors. Evaluation shows that from a student{'}s perspective, our method generated one or more plausible distractors for more than 50{\%} of the MCQs in our test set. From a teacher{'}s perspective, about 50{\%} of the generated distractors were deemed appropriate. We also do a thorough analysis of the results.",
}
@inproceedings{panda-etal-2022-automatic,
    title = "Automatic Generation of Distractors for Fill-in-the-Blank Exercises with Round-Trip Neural Machine Translation",
    author = "Panda, Subhadarshi  and
      Palma Gomez, Frank  and
      Flor, Michael  and
      Rozovskaya, Alla",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2022.acl-srw.31",
    pages = "391--401",
    abstract = "In a fill-in-the-blank exercise, a student is presented with a carrier sentence with one word hidden, and a multiple-choice list that includes the correct answer and several inappropriate options, called distractors. We propose to automatically generate distractors using round-trip neural machine translation: the carrier sentence is translated from English into another (pivot) language and back, and distractors are produced by aligning the original sentence and its round-trip translation. We show that using hundreds of translations for a given sentence allows us to generate a rich set of challenging distractors. Further, using multiple pivot languages produces a diverse set of candidates. The distractors are evaluated against a real corpus of cloze exercises and checked manually for validity. We demonstrate that the proposed method significantly outperforms two strong baselines.",
}
@inproceedings{xue-etal-2021-mt5,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498",
    abstract = "The recent {``}Text-to-Text Transfer Transformer{''} (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent {``}accidental translation{''} in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
}

@article{mchugh2012interrater,
  title={Interrater reliability: the kappa statistic},
  author={McHugh, Mary L},
  journal={Biochemia medica},
  volume={22},
  number={3},
  pages={276--282},
  year={2012},
  publisher={Medicinska naklada}
}
@book{ramsden2003learning,
  title={Learning to teach in higher education},
  author={Ramsden, Paul},
  year={2003},
  publisher={Routledge}
}

@article{gilson2023does,
  title={How does CHATGPT perform on the United States Medical Licensing Examination? the implications of large language models for medical education and knowledge assessment},
  author={Gilson, Aidan and Safranek, Conrad W and Huang, Thomas and Socrates, Vimig and Chi, Ling and Taylor, Richard Andrew and Chartash, David and others},
  journal={JMIR Medical Education},
  volume={9},
  number={1},
  pages={e45312},
  year={2023},
  publisher={JMIR Publications Inc., Toronto, Canada}
}
@article{choi2023chatgpt,
  title={Chatgpt goes to law school},
  author={Choi, Jonathan H and Hickman, Kristin E and Monahan, Amy and Schwarcz, Daniel},
  journal={Available at SSRN},
  year={2023}
}
@article{li2023can,
  title={Can large language models write reflectively},
  author={Li, Yuheng and Sha, Lele and Yan, Lixiang and Lin, Jionghao and Rakovi{\'c}, Mladen and Galbraith, Kirsten and Lyons, Kayley and Ga{\v{s}}evi{\'c}, Dragan and Chen, Guanliang},
  journal={Computers and Education: Artificial Intelligence},
  volume={4},
  pages={100140},
  year={2023},
  publisher={Elsevier}
}
@article{roediger2006test,
  title={Test-enhanced learning: Taking memory tests improves long-term retention},
  author={Roediger III, Henry L and Karpicke, Jeffrey D},
  journal={Psychological science},
  volume={17},
  number={3},
  pages={249--255},
  year={2006},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}
@article{sakai2007evaluating,
  title={Evaluating information retrieval metrics based on bootstrap hypothesis tests},
  author={Sakai, Tetsuya},
  journal={IPSJ Digital Courier},
  volume={3},
  pages={625--642},
  year={2007},
  publisher={Information Processing Society of Japan}
}
@article{gierl2017developing,
  title={Developing, analyzing, and using distractors for multiple-choice tests in education: A comprehensive review},
  author={Gierl, Mark J and Bulut, Okan and Guo, Qi and Zhang, Xinxin},
  journal={Review of Educational Research},
  volume={87},
  number={6},
  pages={1082--1116},
  year={2017},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{ramesh2022automated,
  title={An automated essay scoring systems: a systematic literature review},
  author={Ramesh, Dadi and Sanampudi, Suresh Kumar},
  journal={Artificial Intelligence Review},
  volume={55},
  number={3},
  pages={2495--2527},
  year={2022},
  publisher={Springer}
}

@article{cavalcanti2021automatic,
  title={Automatic feedback in online learning environments: A systematic literature review},
  author={Cavalcanti, Anderson Pinheiro and Barbosa, Arthur and Carvalho, Ruan and Freitas, Fred and Tsai, Yi-Shan and Ga{\v{s}}evi{\'c}, Dragan and Mello, Rafael Ferreira},
  journal={Computers and Education: Artificial Intelligence},
  volume={2},
  pages={100027},
  year={2021},
  publisher={Elsevier}
}
@article{bitew2023learning,
  title={Learning from Partially Annotated Data: Example-aware Creation of Gap-filling Exercises for Language Learning},
  author={Bitew, Semere Kiros and Deleu, Johannes and Dogru{\"o}z, A Seza and Develder, Chris and Demeester, Thomas},
  journal={arXiv preprint arXiv:2306.01584},
  doi={10.48550/arXiv.2306.01584},
  year={2023}
}
@inproceedings{bitew-etal-2023-learning,
    title = "Learning from Partially Annotated Data: Example-aware Creation of Gap-filling Exercises for Language Learning",
    author = "Bitew, Semere Kiros  and
      Deleu, Johannes  and
      Dogru{\"o}z, A. Seza  and
      Develder, Chris  and
      Demeester, Thomas",
    booktitle = "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.bea-1.51",
    pages = "598--609",
    abstract = "Since performing exercises (including, e.g.,practice tests) forms a crucial component oflearning, and creating such exercises requiresnon-trivial effort from the teacher. There is agreat value in automatic exercise generationin digital tools in education. In this paper, weparticularly focus on automatic creation of gap-filling exercises for language learning, specifi-cally grammar exercises. Since providing anyannotation in this domain requires human ex-pert effort, we aim to avoid it entirely and ex-plore the task of converting existing texts intonew gap-filling exercises, purely based on anexample exercise, without explicit instructionor detailed annotation of the intended gram-mar topics. We contribute (i) a novel neuralnetwork architecture specifically designed foraforementioned gap-filling exercise generationtask, and (ii) a real-world benchmark datasetfor French grammar. We show that our modelfor this French grammar gap-filling exercisegeneration outperforms a competitive baselineclassifier by 8{\%} in F1 percentage points, achiev-ing an average F1 score of 82{\%}. Our model im-plementation and the dataset are made publiclyavailable to foster future research, thus offeringa standardized evaluation and baseline solutionof the proposed partially annotated data predic-tion task in grammar exercise creation.",
}
@article{wang2023self,
  title={Self-Critique Prompting with Large Language Models for Inductive Instructions},
  author={Wang, Rui and Wang, Hongru and Mi, Fei and Chen, Yi and Xu, Ruifeng and Wong, Kam-Fai},
  journal={arXiv preprint arXiv:2305.13733},
  year={2023}
}