1- I think it would be a good idea if I add some content about double-decent to show how and why overparameterized models are actually working better. maybe somewhere in the intro.{
[Deep Double Descent (cross-posted on OpenAI blog) – Windows On Theory](https://windowsontheory.org/2019/12/05/deep-double-descent/) #[[Roam-Highlights]]

    - But it corresponds to the practice of having many more parameters than training samples in modern deep learning.
    - We still fit all the training points, but now we do so in a more controlled way which actually tracks quite closely the ground truth. We see that despite what we learn in statistics textbooks,
    - That is, overfitting doesn’t hurt us if we take the number of parameters to be much larger than what is needed to just fit the training set — and in fact, as we see in deep learning, larger models are often better.
    - The take-away from our work (and the prior works it builds on) is that neither the classical statisticians’ conventional wisdom that __“too large models are worse”__ nor the modern ML paradigm that __“bigger models are always better”__always hold
    - even the age-old adage of __“more data is always better”__ is violated!
}

2- The YOLO v5 architecture contains four main model structures: YOLO v5l, YOLO v5x, YOLO v5m, and YOLO v5s, and the complexity of their networks decreases in order. In order to adapt the solution to cell phones, the YOLO v5n model was later proposed, which has the same model depth and half the network width compared to the YOLO v5s, with only 1.9 MB parameters. % from (DA-ActNN-YOLOV5: Hybrid YOLO v5 Model with Data Augmentation and Activation of Compression Mechanism for Potato Disease Identification)

3- if u wanna add more content about yolo, check out this paper: A Deployment Scheme of YOLOv5 with Inference Optimizations Based on the Triton Inference Server

4- I u want to talk more about non-uniform quantization:
Logarithmic quantization (LQ) is a more aggressive form of quantization that works by rounding each value to the nearest powerof-two term as shown in Figure 2(c) [45, 61]. This allows for significantly more efficient inference, as fixed-point multiplication in UQ can be replaced with bit shift operations as each value has only a single power-of-two term. However, due to the aggressive form of quantization, LQ typically suffers from larger accuracy degradation than UQ, as the resolution decreases exponentially when the values gets larger. Power-of-two-term quantization (TQ) relaxes LQ by allowing a term budget of one or more terms for values [36]. Unlike prior work that applied TQ in a post-training quantization fashion [36], in this work, we propose a multi-resolution training paradigm using TQ as our quantization function for weights and data. We discuss TQ in greater detail in Section 3 % from {Training for multi-resolution inference using reusable quantization terms}

5- idea for the abstract: {Neural Network Quantization for Efficient Inference: A Survey}

6- if u wanna add more info about PTQ, check out section 3 of this paper {Benchmarking the Reliability of Post-training Quantization: a Particular Focus on Worst-case Performance}

7- (possibly) mention that ur focus is not on TensorRT and 

8- I just found it informative: R-CNN, Fast R-CNN, Faster R-CNN, YOLO — Object Detection Algorithms 
 at {https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e}

9- the pruned model can be further compressed by parameter quantization [16,30] or low rank factorization [17], and the accuracy can be further restored by knowledge distillation technology [5,22]. from {Pruned-YOLO: Learning Efficient Object Detector Using Model Pruning}