%
\documentclass[onefignum,onetabnum]{siamart220329}
%

%
%
%
%

\input{ex_shared}

%
\ifpdf
\hypersetup{
  pdftitle={Stochastic $p$th root approximation of a stochastic matrix},
  pdfauthor={F. Durastante, B. Meini}
}
\fi

%
%

%

%
%
%
%
%
%
%
%
%
%
%
%

\begin{document}

\maketitle

%
\begin{abstract}
We propose two approaches, based on Riemannian optimization, for computing
a stochastic approximation of the $p$th root of a stochastic matrix $A$. In the first approach, the approximation is found in the Riemannian manifold of
positive stochastic matrices.
In the second approach, we introduce the Riemannian manifold of
positive stochastic matrices sharing with $A$ the Perron eigenvector and we compute the approximation of the $p$th root of $A$ in such a manifold.
This way, differently from the available methods based on constrained optimization, $A$ and its $p$th root approximation share the Perron eigenvector.
Such a property is relevant, from a modeling point of view,  in the embedding problem for Markov chains.
The extended numerical experimentation shows that, in the first approach,  the Riemannian optimization methods are generally faster and more accurate than the available methods based on constrained optimization.
In the second approach, even though the stochastic approximation of the $p$th root is found in a smaller set, the approximation is generally more accurate than the one obtained by standard constrained optimization.
\end{abstract}

%
\begin{keywords}
Stochastic matrix, Matrix $p$th root, Riemannian optimization, Markov chains, Embedding problem
\end{keywords}

%
\begin{MSCcodes}
65C40, 65K05, 53B21, 65F60
\end{MSCcodes}

\section{Introduction}

Discrete and continuous-time Markov chains are used to model a range of different time-evolving phenomena such as queueing models \cite{blm:book}, physician's estimate of prognosis under alternative treatment plans~\cite{application1}, synthetic DNA~\cite{Ardiyansyah2021}, rating agencies predicting the evolution of a firm's rating in a given time interval~\cite{application2,Hughes2016}, studying diffusion and consensus on directed graphs~\cite{Veerman2019} or the analysis of daily rainfall occurrence~\cite{application3}. 
%
The evolution of a discrete-time Markov chain, with  a finite number $n$ of states, is described in terms of an $n\times n$ matrix $A$, called \emph{transition matrix}, whose $(i,j)$-th entry represents the probability to go from state $i$ to state $j$ in one unit of time. The matrix $A$ is \emph{stochastic}, i.e., belongs to the set
\[
\mathbb{S}_n^0 = \{ S \in \mathbb{R}^{n \times n} \,:\; S \mathbf{1} = \mathbf{1}, \; S \geq 0 \},
\]
where $\mathbf{1} = (1,1,\ldots,1)^T \in \mathbb{R}^n$, and the symbol ``$\geq$'' represents the element-wise ordering. 
For the Perron-Frobenius theorem, any stochastic matrix $A$ has a nonnegative Perron eigenvector, i.e., a nonnegative vector $\boldsymbol{\pi}\ne 0$ such that $\boldsymbol{\pi}^T A=\boldsymbol{\pi}^T$; when $\boldsymbol{\pi}$ is normalized so that $\boldsymbol{\pi}^T\mathbf{1}=1$, then $\boldsymbol{\pi}$ is called steady state vector, or stationary distribution, for the matrix $A$. If $A$ is irreducible, then the steady state vector  has positive entries and is unique, moreover $\lim_{k\to\infty}A^k=\mathbf{1} \boldsymbol{\pi}^T$.
%
%
%
%
%
%

In many applications, the entries of the matrix $A$ are estimated through the analysis of historical series over long time intervals. 
%
Therefore, the unit time at which transitions occur is generally larger, compared with the characteristic time of the phenomenon to be analyzed. To know the transition probabilities in the typical time step of the phenomenon, it would therefore be necessary to investigate what happens in a fraction of a unit of time: for instance, which are the transition probabilities in a half-time unit?
%
An attempt might be computing a matrix $X$ such that
$A  = X^2$,
or, in other terms, a \emph{square root} of the transition matrix $A$. More generally, we can inquire about any number of intermediate steps $p$ thus looking for a $p$th root $X$ of $A$,
$
A = X^p$, $p \in \mathbb{N}$.
However, for the matrix $X$ to be descriptive of a Markov process, we need it to be itself a transition matrix, that is, $X$ should satisfy 
\[
X^p = A, \; X \in\mathbb{S}_n^0.
\]
Unfortunately, such $X$ does not exist in general~\cite{HighamLijing2010}, and several pathological cases can be readily produced, e.g., the $p$th root may exist or not, it may exist and not be stochastic, and there can even be more than one stochastic $p$th roots. In some cases, we can exploit the fact that $f(z)= z^{\nicefrac{1}{p}}$ has more than one branch in the complex plane to define \emph{non-primary} matrix functions $A^{\nicefrac{1}{p}}$ by selecting different determinations of the function on repeated eigenvalues --- see~\cite[Section~1.4]{HighamBook} --- and look for a non-primary stochastic matrix $p$th root. Even with this added degree of freedom, stochastic $p$th roots of a stochastic matrix might not exist. 

The problem of the existence of a stochastic $p$th root of a stochastic matrix is also strictly related to the so called embedding problem for Markov chains (see \cite{vanbrunt}). Indeed, a Markov chain
with transition matrix $A$ is \emph{embeddable} if and only if there exists a rate matrix $Q$ such
that $A=\exp(Q)$. We recall that a rate matrix is a matrix with nonnegative off-diagonal entries, such that $Q\mathbf{1}=\mathbf{0}$. It is immediate to verify that, if a Markov chain is embeddable, then $X=\exp(Q/p)$ is a stochastic $p$th root of $A$, for any $p$. More precisely, in \cite{Kingman} it is shown that a Markov chain is embeddable if and only if the transition matrix $A$ is nonsingular and has stochastic $p$th roots of any order~$p$. To this regard, in \cite{vanbrunt} a characterization of embeddable Markov chains is given in terms of infinite divisibility properties of nonnegative matrices. In practice, these conditions are difficult to be verified and sufficient conditions for embeddability have been introduced for specific cases, as  equal-input, circulant, symmetric or doubly stochastic matrices~\cite{Baake2020}, small size matrices~\cite{Casanellas2020}, or in other frameworks \cite{Davies2010,Ekhosuehi2023,HighamLijing2010,Bhat2020}.

In the case where a stochastic $p$th root does not exist,  an alternative approach consists in finding an approximation, which is a stochastic matrix. 
%
%
%
%
%
%
%
%
To this end, there are some methods available in the literature relying on optimization strategies, see, e.g., the code package in~\cite{Pfeuffer2017}. 
Given $A\in\mathbb{S}_n^0$, the main approach consists in the computation of the solution $X$  of the following constrained optimization problem, where $\|\cdot\|_F$ is the Frobenius norm (see \cite{HighamLijing2010,https://doi.org/10.1002/sim.2970}):
\begin{enumerate}[label=(\emph{\alph*}),ref=(\emph{\alph*})]
	\item\label{alg:a2} find
	\[
	X = \arg\min_{ X  \in \mathbb{S}_n^0 } \frac{1}{2}\| X^p - A\|_F^2.
	\]
\end{enumerate}
Other, less used, strategies consist in
\begin{enumerate}[label=(\emph{\alph*}),ref=(\emph{\alph*})]
	\setcounter{enumi}{1}
	\item\label{alg:a1} find
	\[
	X = \arg\min_{ X  \in \mathbb{S}_n^0 } \frac{1}{2}\| X - A^{\nicefrac{1}{p}} \|_F^2,
	\]
	where $A^{\nicefrac{1}{p}}$ is the principal $p$th root of $A$;
	\item\label{alg:a3} find 
	\[
	\mathbf{h} = \arg\min_{ \mathbf{h} \in \Omega  } \left\| \left( \sum_{i=0}^{n-1} h_i A^i \right)^p - A \right\|_F^2,
	\]
	for
	\[
	\Omega = \{ \mathbf{h} \in \mathbb{R}^n \,:\,\mathbf{1}^T \mathbf{h} = 1,\; B \mathbf{h} \geq 0,  \; B = [\operatorname{vec}(I)|\operatorname{vec}(A)|\cdots|\operatorname{vec}(A^{n-1})] \},
	\]
	where $\operatorname{vec}(F)$ is the vector obtained by stacking the columns of the matrix $F$,
	and set $X = X(\mathbf{h}) = \sum_{i=0}^{n-1} h_i A^i$.
\end{enumerate}

Formulations \ref{alg:a2} and \ref{alg:a1} deliver an approximation $X$ that is not, in general, a matrix-function of $A$, while in \ref{alg:a3} the approximation is a primary matrix function by construction. 
However, as pointed out in \cite{HighamLijing2010}, there are situations where the stochastic $p$th root exists but it is not a matrix-function of $A$, therefore \ref{alg:a3} does not compute such a stochastic $p$th root. On the other hand,  since $X$ is a function of $A$, a nice feature of formulation \ref{alg:a3} is that the output matrix $X$ shares with $A$ the steady state vector.
If both $A$ and $X$ are irreducible, this implies that $\lim_{k\to\infty}X^k=\lim_{k\to\infty}A^k=\mathbf{1}\boldsymbol{\pi}^T$, i.e., the asymptotic behavior of the Markov chains with transition matrices $X$ and $A$, respectively, is the same. From the modeling point of view, this is a desirable property, since we expect that doing time steps of different ``lengths'' should always bring us to the same limit. 


In this paper, we propose to compute a stochastic approximation $X$ to a $p$th root of the stochastic matrix $A$ by relying on a Riemannian optimization approach. Indeed, it is well known that the set of positive stochastic matrices is a Riemannian manifold, called multinomial manifold \cite{Douik8861409}. Therefore, the first approach that we propose is to solve problem \ref{alg:a2} in this Riemannian optimization setting. 
%
However, as in standard constrained optimization, the computed matrix $X$ is a stochastic matrix that generally does not share with $A$ the steady state vector. 
To overcome this drawback, given a positive vector  $\boldsymbol{\pi} \in \mathbb{R}^{n}$ such that $\boldsymbol{\pi}^T \mathbf{1} = 1$, 
we introduce the Riemannian manifold $\mathbb{S}_n^{\boldsymbol{\pi}}$ of positive stochastic matrices, having $\boldsymbol{\pi}$ as steady state vector. Such Riemannian manifold can be seen as the generalization of the Riemannian manifold of doubly stochastic matrices,
which corresponds to the special case where $\boldsymbol{\pi}=\frac1n\mathbf{1}$. In order to apply the Riemannian optimization algorithms, we give an expression to the tangent space, to the orthogonal complement and orthogonal projection, and to the Riemannian gradient and Hessian, by extending the analog properties valid for doubly stochastic matrices. To define the retraction from the tangent bundle to the manifold, we use a generalization of the Sinkhorn-Knopp algorithm. Hence, given an irreducible stochastic matrix $A$ with stationary distribution $\boldsymbol{\pi}$, we approximate its stochastic $p$th root by solving~\ref{alg:a2} in the manifold $\mathbb{S}_n^{\boldsymbol{\pi}}$. In implementing the optimization algorithms we need to solve several singular symmetric linear systems; in this regard, we provide a lower and an upper bound to the nonzero eigenvalues of the matrix, which give information on the convergence of Conjugate Gradient-like methods. Moreover, we propose some preconditions to improve the convergence of iterative methods for the solution of such linear systems. 

The new Riemannian manifold $\mathbb{S}_n^{\boldsymbol{\pi}}$ has been implemented in Matlab, in a format compatible with the Manopt library \cite{manopt}. The code is available in the GIT repository \href{https://github.com/Cirdans-Home/pth-root-stochastic}{github.com/Cirdans-Home/pth-root-stochastic}. 

The proposed methods have been tested on a variety of stochastic matrices $A$, with different properties, in terms of size and embeddability. An application to finance, where $A$ represents the transitions in the credit classes \cite{application2,Hughes2016}, has been treated in detail.

In the cases where we are not interested in preserving the steady state vector, the comparisons between constrained optimization algorithms,  integrating together trust region and interior point techniques~\cite{interiorpoint}, and the Riemannian-based optimizers for formulation~\ref{alg:a2} on the multinomial manifold, show that the latter achieve smaller or equal residuals, and they are generally faster. 
When we are interested in preserving the steady state vector, the numerical experiments  on the Riemannian manifold $\mathbb{S}_n^{\boldsymbol{\pi}}$ show that, in some cases, the approximation of the $p$th root of $A$ has a higher residual with respect to the approximation in the manifold $\mathbb{S}_n$ of positive stochastic matrices. This is expected, since the set $\mathbb{S}_n^{\boldsymbol{\pi}}$ is smaller than the set $\mathbb{S}_n$. However, in general, optimization methods in set $\mathbb{S}_n$ provide an approximation having a stationary distribution far from the stationary distribution $\boldsymbol{\pi}$ of $A$.
In the application to credit ranking in finance, the matrix $A$ is reducible, therefore we apply our method to the irreducible stochastic matrix $\widetilde A=\gamma A+(1-\gamma)\frac1n \boldsymbol{1}\boldsymbol{1}^T$, $0<\gamma<1$, which resembles the Page Rank matrix. From the numerical experiments, the approximation of the $p$th root has a structure close to the reducible structure of $A$, and the numerical values of its entries are very realistic, from a modeling point of view.

The paper is organized as follows. In Section~\ref{sec:riem_opt_general} we recall the main definitions concerning Riemannian manifolds and Riemannian optimization. 
In Section~\ref{sec:stochastic-old-methods} we recall the properties of the multinomial manifold of positive stochastic matrices and solve problem~\ref{alg:a2} in the framework of Riemannian optimization in such a manifold. In Section~\ref{sec:stochastic-new-methods} we introduce the Riemannian manifold $\mathbb{S}_n^{\boldsymbol{\pi}}$ of stochastic matrices having a common steady state vector $\boldsymbol{\pi}$, and solve problem~\ref{alg:a2} in this manifold. We present the numerical experiments in Section~\ref{sec:numer_ex} and draw conclusions in Section~\ref{sec:conclusions}.

\subsection{Notation}

In the following, the symbols ``$\oslash$'' and ``$\odot$'' represent the Hadamard (entry-wise) matrix division and multiplication, respectively. Given a vector $\mathbf{x}$ -- always denoted in bold face -- the symbol ``$\operatorname{diag}(\mathbf{x})$'' denotes the diagonal matrix having the entries of $\mathbf{x}$ on the main diagonal; for notational simplicity, we will also denote $D_{\mathbf{x}} = \operatorname{diag}(\mathbf{x})$. If $A$ is a square matrix, then ``$\operatorname{diag}(A)$'' denotes the vector formed by the diagonal entries of~$A$, and $\lambda(A)$ its spectrum. The notation concerning Riemannian geometry will be introduced at the time of their~use. If $A\in\mathbb{R}^{m\times n}$, $A$ is said to be nonnegative (positive), and we write $A\ge 0$ ($A>0$), if all its entries are nonnegative (positive).

\section{Preliminaries on Riemannian optimization}\label{sec:riem_opt_general}
We start by recalling some definitions concerning Riemannian manifolds and Riemannian optimization. The interested reader may find more details on this subject in~\cite{Douik8861409,AbsilBook}.

\begin{definition}[Charts, Atlas and Manifolds]\label{ref:manifold-definition}
	Let $\mathcal{M}$ be a set. A bijection $\phi$ from $\mathcal{U} \subset \mathcal{M}$ to an open subset of $\mathbb{R}^{d}$ is called $d$-dimensional \emph{chart} of the set $\mathcal{M}$. A $\mathcal{C}^{\infty}$ \emph{atlas} of $\mathcal{M}$ into $\mathbb{R}^d$ is a collection of charts $\{\mathcal{U}_\alpha,\phi_\alpha \}_\alpha$ of the set $\mathcal{M}$ such that
	\begin{itemize}
		\item $\bigcup_\alpha \mathcal{U}_\alpha = \mathcal{M}$,
		\item for any pair $\alpha,\beta$ with $\mathcal{U}_\alpha \cap \mathcal{U}_\beta \neq \emptyset$ the set $\phi_\alpha(\mathcal{U}_\alpha \cap \mathcal{U}_\beta)$ and $\phi_\beta(\mathcal{U}_\alpha \cap \mathcal{U}_\beta)$ are open sets in $\mathbb{R}^d$ and the change of coordinates
		\[
		\phi_\beta \circ \phi_\alpha^{-1} \,:\mathbb{R}^d \rightarrow \mathbb{R}^d
		\]
		is of class $\mathcal{C}^{\infty}$ on $\phi_\alpha(\mathcal{U}_\alpha \cap \mathcal{U}_\beta)$. 
	\end{itemize}
	Two atlases are equivalent if their union is an atlas. For every atlas $\mathcal{A}$ we define a maximal atlas $\mathcal{A}^+$ as the atlas of all charts $(\mathcal{U},\phi)$ such that $\mathcal{A} \cup \{ (\mathcal{U},\phi \}$ is also an atlas. Then a \emph{$d$-dimensional manifold} is a couple $(\mathcal{M},\mathcal{A}^+)$ with $\mathcal{M}$ a set and $\mathcal{A}^+$ a maximal atlas of $\mathcal{M}$ into $\mathbb{R}^d$, such that the topology induced by $\mathcal{A}^+$ is Hausdorff and second-countable, i.e., induces a topological space in which for any two distinct points exists neighborhoods of each that are disjoint from each other, and whose topology admits a countable base. 
\end{definition}

A tangent vector to $\mathcal{M}$ at a point $x$ is defined as follows:

\begin{definition}[Tangent vector, tangent bundle]\label{def:tangent_vector}
	A tangent vector $\xi_x$ to a manifold $\mathcal{M}$ at a point $x$ is a mapping from the set of smooth real-valued functions defined on a neighborhood of $x$, $\mathfrak{F}_x(\mathcal{M})$ to $\mathbb{R}$ such that there exists a curve $\gamma$ on $\mathcal{M}$ realizing the tangent vector $\xi_x$, i.e., such that $\gamma(0)=x$, and
	\[
	\xi_x f = \dot{\gamma}(0)f \triangleq \left.\frac{\mathrm{d}(f(\gamma(t)))}{\mathrm{d}t}\right\rvert_{t=0}, \; \forall \, f \in  \mathfrak{F}_x(\mathcal{M});
	\]
	see, e.g., Figure~\ref{fig:tangent_space}. The \emph{tangent space} $\mathcal{T}_x \mathcal{M}$ at $x\in\mathcal M$ is then the set of all tangent vectors to $\mathcal{M}$ at a point $x$. While we call tangent bundle the manifold $\mathcal{T} \mathcal{M}$ that assembles all the tangent vectors, i.e., $\mathcal{T} \mathcal{M} = \bigcup_{x \in \mathcal{M}} \mathcal{T}_x \mathcal{M}$.
\end{definition}

% Figure environment removed
A Riemannian manifold $\mathcal{M}$ is a manifold equipped with a positive-definite inner product on its tangent space, i.e., $\langle \xi_X, \eta_X\rangle_X$, for any $\xi_X,\eta_X\in\mathcal{T}_X\mathcal{M}$.
Such a \emph{metric}, called Riemannian metric, induces the norm $\| \xi_X \|_X=\sqrt{\langle \xi_X, \xi_X\rangle_X}$, for any $\xi_X\in\mathcal{T}_X\mathcal{M}$



%
On the Riemannian manifold, we can define the 
minimization problem
\begin{equation}\label{eq:genopt}
	\displaystyle \arg\min_{ X  \in \mathcal{M}} f(X),
\end{equation}
where $f:\mathcal{M}\to \mathbb{R}$ is a suitable smooth function. 
\subsection{Optimization methods}\label{sec:optstoc}
Optimization methods for solving \eqref{eq:genopt} on Riemannian manifolds use local pull-back from the tangent spaces $\mathcal{T}_S \mathcal{M}$ to the manifold $\mathcal{M}$ to produce a sequence of iterates, which can be interpreted as iterates moving along specific curves on the manifold, see the representation in Figure~\ref{fig:basic_idea}.
% Figure environment removed
What distinguishes the different algorithms is how the new point on the tangent space is determined. In general, it is possible to adapt the different classes of optimization methods in this new context, consider, e.g., first order methods, Newton and Quasi-Newton methods, or Trust-Region methods, see, \cite[Chapters~6,7 and 8]{AbsilBook} for a complete discussion. 

In order to define these methods, we need to recall some differential structures for functions taking values on the manifold. 
We denote by $\mathrm{D} f(X)[\xi]$  the directional derivative of $f$ given by:
\[
\mathrm{D} f(X)[\xi]=\lim_{t\to 0}\frac{f(X+t\xi)-f(X)}{t}.
\]


\begin{definition}[Affine connection]\label{def:affine-connection}
	An \emph{affine connection} $\nabla \,:\,\mathcal{T}\mathcal{M} \times \mathcal{T}\mathcal{M} \to \mathcal{T}\mathcal{M}$ is a map that associates to each $(\eta,\xi)$ in the tangent bundle (Definition~\ref{def:tangent_vector}) the tangent vector $\nabla_\eta \xi$ satisfying for all  $a,b \in \mathbb{R}$, and smooth $f,g: \mathcal{M} \longrightarrow \mathbb{R}$:
	\begin{itemize}
		\item $\nabla_{f(\eta)+g(\chi)}\xi =  f(\nabla_\eta \xi)+ g(\nabla_\chi \xi)$
		\item $\nabla_{\eta}(a\xi+b\varphi) = a\nabla_{\eta}\xi+b\nabla_{\eta}\varphi$
		\item $\nabla_{\eta}(f(\xi)) = \xi(f) \eta + f(\nabla_\eta \xi)$,
	\end{itemize}
	wherein the vector field $\xi$ acts on the function $f$ by derivation, that is 
	\[\xi(f)=\mathrm{D}(f)[\xi].\]
	We call \emph{Levi-Civita connection} the affine connection that preserves the Riemannian metric, i.e., the affine connection such that
	\begin{enumerate}
		\item $\nabla_\eta \xi - \nabla_\xi \eta = [\eta,\xi] $ $\forall\,\eta,\xi \in \mathcal{T}\mathcal{M}$,
		\item $\chi \langle \eta,\xi \rangle = \langle \nabla_\chi \eta,\xi \rangle + \langle\eta ,  \nabla_\chi \xi \rangle$, $\forall\,\eta ,\xi ,\chi \in \mathcal{T} \mathcal{M}$,
	\end{enumerate}
	where we are denoting with $[\cdot,\cdot]$ the Lie bracket
	\[
	[\xi,\eta]g = \xi(\eta (g)) - \eta(\xi (g)).
	\]
\end{definition}

\begin{definition}[Riemannian Gradient and Hessian]\label{def:riemannian-gradient-and-hessian}
	The \emph{Riemannian gradient} of $f$ at $S$, denoted by $\operatorname{grad}f(S)$, of a manifold $\mathcal{M}$ is defined as the unique vector in $\mathcal{T}_S\mathcal{M}$ that satisfies:
	\begin{align*}
		\langle \operatorname{grad}f(S), \xi_S \rangle_S = \mathrm{D} f(S) [\xi_S],\ \forall \ \xi_S \in \mathcal{T}_S\mathcal{M}.
	\end{align*}
	The \emph{Riemannian Hessian} of $f$ at $S$, denoted by $\operatorname{Hessian}f(S)$, of a manifold $\mathcal{M}$ is a mapping from $\mathcal{T}_S\mathcal{M}$ into itself defined by:
	\begin{align*}
		\operatorname{Hessian}f(S)[\xi_S] = \nabla_{\xi_S} \operatorname{grad}f(S), \ \forall \ \xi_S \in \mathcal{T}_S\mathcal{M},
	\end{align*}
	where $\operatorname{grad}f(S)$ is the Riemannian gradient and $\nabla$ is the Riemannian connection on~$\mathcal{M}$.
\end{definition}



\begin{definition}[Retraction]\label{def:retraction}
	A retraction on a manifold $\mathcal{M}$ is a smooth map $R$ from the tangent bundle $\mathcal{T} \mathcal{M} = \bigcup_{S \in \mathcal{M}} \mathcal{T}_S\mathcal{M}$ onto $\mathcal{M}$. For all $S \in \mathcal{M}$ the restriction of $R$ to $\mathcal{T}_S \mathcal{M}$, denoted by $R_S$, satisfies the following properties:
	\begin{description}
		\item[Centering.] $R_S(0) = S$;
		\item[Local rigidity.] The curve $\gamma_{\xi_S}(\tau) = R_S(\tau \xi_S)$ satisfies
		\[
		\left.\frac{d \gamma_{\xi_S}(\tau)}{d\tau}\right|_{\tau = 0} = \xi_S, \quad\,\forall\, \xi_S \in \mathcal{T}_S \mathcal{M}.
		\]
	\end{description}
\end{definition}


The general sketch of Newton's method for solving~\eqref{eq:genopt} on a Riemannian manifold is synthesized in Algorithm~\ref{alg:newton}. For other optimization methods, we refer to \cite[Chapters 4,6--8]{AbsilBook}.
\begin{algorithm2e}[htbp]
	\caption{Newton's method for~\eqref{eq:genopt} on a Riemannian Manifold $\mathcal{M}$}\label{alg:newton}
	\KwData{Manifold $\mathcal{M}$, function $f$, retraction $R$, affine connection $\nabla$, and convergence tolerance $\epsilon$}
	Initial guess $A \in \mathcal{M}$\;
	\While{$||\operatorname{grad}f(S)||_{S} \geq \epsilon$}{
		Find descent direction $\xi_S \in \mathcal{T}_S\mathcal{M}$ such that:
		\begin{align*}
			\operatorname{Hessian}f(S)[\xi_S] = -\operatorname{grad}f(S),
		\end{align*}
		wherein $\operatorname{Hessian}f(S)[\xi_S] = \nabla_{\xi_S} \operatorname{grad}f(S)$\;
		Retract $A = R_S(\xi_S)$\;
	}
	\KwResult{Output $A$.}
\end{algorithm2e}

\section{Stochastic \texorpdfstring{$p$th}{pth} root approximation via Riemannian optimization}\label{sec:stochastic-old-methods}


Here we propose an approach based on Riemannian optimization, to numerically approximate the solution of problem~\ref{alg:a2}.

Indeed, by following \cite{Douik8861409}, we endow the set of  row-stochastic matrices with positive entries, namely
\[
\mathbb{S}_n = \{ S \in \mathbb{R}^{n \times n} \,:\; S \mathbf{1} = \mathbf{1}, \; S > 0 \},
\]
with both a manifold structure (in the sense of Definition~\ref{ref:manifold-definition}) and an \emph{intrinsic metric}, making it a Riemannian manifold~\cite{Douik8861409,7182334}, known as multinomial manifold. The solution of problem~\ref{alg:a2} is approximated within such a manifold.

On
$\mathbb{S}_n$, we need to define 
the \emph{tangent space} $\mathcal{T}_S \mathbb{S}_n$.
By applying the definition, we find that if $S(t)$ is a smooth curve such that $S(0) = S$  and  $S(t)\in \mathbb{S}_n $ for any $t$ in a neighborhood of the origin, then the curve satisfies
\[
S(t) \mathbf{1} = \mathbf{1} \, \Rightarrow \, \dot{S}(t)\mathbf{1} = \mathbf{0},
\]
thus
$
\mathcal{T}_S \mathbb{S}_n \subseteq \{ \xi_S \in \mathbb{R}^{n \times n}\,:\; \xi_S \mathbf{1} = \mathbf{0} \},
$
while the opposite inclusion holds by comparing the number of degrees of freedom of the full space, and of the tangent space (see \cite[Proposition~1]{Douik8861409}), so that
\[
\mathcal{T}_S \mathbb{S}_n = \{ \xi_S \in \mathbb{R}^{n \times n}\,:\; \xi_S \mathbf{1} = \mathbf{0} \}.
\]
Therefore, $\mathbb{S}_n$ can be extended to be a Riemannian manifold by adding a positive-definite inner product on its tangent space at every point. This metric is given by the \emph{Fisher
	information metric}
\begin{equation}\label{eq:fisher-metric}
	\begin{split}
		g_A(\xi_S,\eta_S) = 
		&\; \langle \xi_S, \eta_S \rangle_S 
		=  \sum_{i,j=1}^{n} \frac{ (\xi_S)_{i,j} (\eta_S)_{i,j} }{S_{i,j}} \\ = &\; \operatorname{Trace}( (\xi_S \oslash S) \eta_S^T ), \;\forall\, \xi_S,\eta_S \in \mathcal{T}_S \mathbb{S}_n.
	\end{split}
\end{equation}

%
On the multinomial manifold $\mathbb{S}_n$ , we can define the analogous of problem~\ref{alg:a2} as follows:
\begin{equation}\label{eq:the_rewritten_problem}
	\text{Given }A \in \mathbb{S}_n^0 \text{ find } X = 
	%
	%
	\displaystyle \arg\min_{ X  \in \mathbb{S}_n } \| X^p - A\|_F.
	%
\end{equation}
The two substantial differences with respect to the standard formulation are, on the one hand, the explicit request to have the elements of $X > 0$, on the other hand, the possibility of exploiting the Riemannian manifold structure to compute the solution of problem \eqref{eq:the_rewritten_problem}.

In particular, since the multinomial manifold is already defined in the MANOPT library~\cite{manopt},
to apply for instance the Riemannian version of the Trust Region optimization procedure, we can use a few lines of MANOPT code. 
Indeed, given a stochastic matrix \mintinline{matlab}{A} with \mintinline{matlab}{n = size(A,1)}, and  given an integer \mintinline{matlab}{p}, it is sufficient to write
\begin{minted}[bgcolor=bg,fontsize=\small]{matlab}
%
manifold = multinomialfactory(n,n); %
%
problem.M = manifold;
problem.cost = @(x) 0.5*cnormsqfro(mpower(x,p).'-A);
problem = manoptAD(problem);
options.tolgradnorm = 1e-7;
[x, xcost, info, options] = trustregions(problem,[],options);
\end{minted}
Some attention is needed since in the multinomial manifold of the MANOPT library, matrices are column stochastic instead of row stochastic. The variable \mintinline{matlab}{x}
contains an approximation to the solution of \eqref{eq:the_rewritten_problem}.

To illustrate the behavior of this approach, we consider the following example from \cite[Fact~4.10]{HighamLijing2010}.
\begin{example}\label{example:circulant-example}
	Let us consider the matrix 
	\[
	A(a) = \frac{1}{3} \begin{bmatrix}
		1-2a & 1+a & 1+a \\
		1+a & 1-2a & 1+a \\
		1+a & 1+a & 1-2a \\
	\end{bmatrix}, \quad 0 < a \leq \frac{1}{3}, 
	\]
	having eigenvalues $\{1,-a,-a\}$.
	This matrix is circulant (and therefore doubly stochastic),  and has only a stochastic square root, which is not a primary function, given by
	\[
	X = \frac{1}{3} \begin{bmatrix}
		1 & 1+\sqrt{3a} & 1-\sqrt{3a} \\
		1-\sqrt{3a} & 1 & 1 + \sqrt{3a} \\
		1+\sqrt{3a} & 1-\sqrt{3a} & 1 \\
	\end{bmatrix}. %
	\]
	Its eigenvalues are $\{1,i\sqrt{a},-i\sqrt{a}\}$.
	Setting $a=\nicefrac{1}{6}$, an application of the optimization strategy produces
	\[
	\tilde{X} =
	\begin{bmatrix}
		0.3179  &  0.1158  &  0.5663 \\
		0.5885 &   0.3299  &  0.0816 \\
		0.0936 &   0.5543  &  0.3522 \\
	\end{bmatrix}, \; \|\tilde{X}^2 - A\|_F = 1.3102\times 10^{-12}.
	\]
	In this case the steady state vector of $\tilde X$ has 
	an absolute difference with respect to the steady state vector of $A(a)$ of $1.5701 \times 10^{-16}$, indeed this is mostly due to the fact the matrix is bi-stochastic. 
\end{example}


\begin{example}
	We consider the matrix \verb|Pajek/GD96_c| from the SuiteSparse matrix collection as the adjacency matrix $A$ of an undirected graph normalized by the inverse of the sum of the row entries, then $A$ is a row-stochastic matrix, and we can apply the optimization strategy for $p=2$. In this case, there doesn't seem to be a stochastic square root matrix to converge to, since the residual of the optimization procedure is $\|X^2 - A\|_F = 0.52$. Furthermore, as shown in Figure~\ref{fig:eigenfailure}, the proposed approximation has a different stationary distribution with respect to $A$, therefore the ``half step'' linked to it cannot converge to the same stationary state of the global system. For this reason, in the next section we will focus on the computation of an approximation of the root that preserves the stationary distribution. 
	
	% Figure environment removed
\end{example}

\section{Stochastic \texorpdfstring{$p$th}{pth} root approximation preserving the stationary distribution}\label{sec:stochastic-new-methods}
In this section we first introduce the manifold of positive stochastic matrices, having the same stationary distribution $\boldsymbol{\pi} > 0$, $\boldsymbol{\pi}^T\mathbf{1} = 1$. Then, given a stochastic matrix $A$ with stationary distribution $\boldsymbol{\pi}$,  we approximate its stochastic $p$th root on such a manifold. This way, the stochastic $p$th root approximation of $A$ shares with $A$ the stationary distribution.



\subsection{A new Riemannian manifold}
Let $\boldsymbol{\pi} \in \mathbb{R}^{n}$ be a positive vector such that $\boldsymbol{\pi}^T \mathbf{1} = 1$, and
define the set
\[
\mathbb{S}_n^{\boldsymbol{\pi}} = \{ S \in \mathbb{R}^{n \times n} \,:\; S \mathbf{1} = \mathbf{1}, \; \boldsymbol{\pi}^T S = \boldsymbol{\pi}^T, \; S > 0 \},
\]
i.e., $\mathbb{S}_n^{\boldsymbol{\pi}}$ is the set  of $n\times n$ positive stochastic matrices, having the same stationary distribution~$\boldsymbol{\pi}$.
By following the approach used in~\cite{Douik8861409} for the manifold of doubly stochastic matrices,  we may prove that  $\mathbb{S}_n^{\boldsymbol{\pi}}$ is an embedded manifold of $\mathbb{R}^{n \times n}$ of dimension $(n-1)^2$, since it is indeed generated by $2n-1$ linearly independent equations.

The following result characterizes the tangent space:

\begin{lemma}\label{lem:tangent-space}
	The tangent space  to $\mathbb{S}_n^{\boldsymbol{\pi}}$ at $S\in\mathbb{S}_n^{\boldsymbol{\pi}}$ is given by
	\begin{equation}\label{eq:tangent_space}
		\mathcal{T}_S \mathbb{S}_n^{\boldsymbol{\pi}} = \{ \xi_S \in \mathbb{R}^{n \times n}\,:\; \xi_S \mathbf{1} = \mathbf{0}, \; \boldsymbol{\pi}^T \xi_S = \mathbf{0} \}.
	\end{equation}
\end{lemma}

\begin{proof}
	Let $S(t)$ be any smooth curve such that $S(0) = S$ and $S(t)\in \mathbb{S}_n^{\boldsymbol{\pi}}$ for $t$ in a  neighborhood of the origin. By differentiating, we find
	\begin{equation*}
	\begin{split}
	\phantom{\boldsymbol{\pi}}S(t)\mathbf{1} = \mathbf{1} \, \Rightarrow \, \phantom{\boldsymbol{\pi}^T}\dot{S}(t)\mathbf{1} = \mathbf{0},\\
	\boldsymbol{\pi}^T S(t)\phantom{\mathbf{1}} = \boldsymbol{\pi}^T \, \Rightarrow \, \boldsymbol{\pi}^T \dot{S}(t) \phantom{\mathbf{1}} = \mathbf{0},
\end{split}
	\end{equation*}
	thus we have
	\begin{equation*}
		\mathcal{T}_S \mathbb{S}_n^{\boldsymbol{\pi}} \subseteq \{ \xi_S \in \mathbb{R}^{n \times n}\,:\; \xi_S \mathbf{1} = \mathbf{0}, \; \boldsymbol{\pi}^T \xi_S = \mathbf{0} \}.
	\end{equation*}
	To prove the opposite inclusion, we observe that the space $\{ \xi_S \in \mathbb{R}^{n \times n}\,:\; \xi_S \mathbf{1} = \mathbf{0}, \; \boldsymbol{\pi}^T \xi_S = \mathbf{0} \}$ is defined by $2n-1$ independent equations. Since the whole space has size $n^2$, then the dimension is given by $n^2-(2n-1) = (n-1)^2$ which equals the size of the space of matrices with a given left- and right-eigenvector.  Therefore, the set we defined has the same dimension as the tangent space, so that they coincide.
\end{proof}

The manifold $\mathbb{S}_n^{\boldsymbol{\pi}} $, endowed with the Fisher metric \eqref{eq:fisher-metric}, 
is a Riemannian manifold.

To use any optimization strategy we need an expression for the projection operator on the tangent space
\[
\Pi_S \,:\,\mathbb{R}^{n \times n} \rightarrow \mathcal{T}_S \mathbb{S}_n^{\boldsymbol{\pi}}.
\]
For a $Z \in \mathbb{R}^{n \times n}$ and an $S \in \mathbb{S}_n^{\boldsymbol{\pi}}$, we can express the orthogonal projection by using the decomposition of any ambient vector into
\begin{equation}\label{eq:matrix-decomposition}
	Z = \Pi_S(Z) + \Pi_S^\perp(Z).
\end{equation}


\begin{lemma}\label{lem:orthogonal_complement}
	The orthogonal complement of the tangent space $\mathcal{T}_S \mathbb{S}_n^{\boldsymbol{\pi}}$ has the expression
	\[
	\mathcal{T}_S^\perp \mathbb{S}_n^{\boldsymbol{\pi}} = \{ \xi_S^\perp \in \mathbb{R}^{n\times n} \,:\, \xi_S^\perp = (\boldsymbol{\alpha} \mathbf{1}^T + \boldsymbol{\pi} \boldsymbol{\beta}^T) \odot S \},
	\]
	for some vectors $\boldsymbol{\alpha},\boldsymbol{\beta} \in \mathbb{R}^{n}$.
\end{lemma}

\begin{proof}
	%
	Let $z=(\boldsymbol{\alpha} \mathbf{1}^T + \boldsymbol{\pi} \boldsymbol{\beta}^T) \odot S $, for some vectors $\boldsymbol{\alpha},\boldsymbol{\beta} \in \mathbb{R}^{n}$. Then
	\begin{equation*}
	\begin{split}
		\langle  z , \xi_S  \rangle_S = & \operatorname{Trace}( (z \oslash S) \xi_S^T ) = \operatorname{Trace}( (\boldsymbol{\alpha} \mathbf{1}^T + \boldsymbol{\pi} \boldsymbol{\beta}^T) \xi_S^T ) \\
		= & \operatorname{Trace}(\boldsymbol{\alpha} \mathbf{1}^T \xi_S^T ) + \operatorname{Trace}( \boldsymbol{\pi} \boldsymbol{\beta}^T \xi_S^T) \\
		= & \boldsymbol{\alpha}^T \underbrace{\xi_S \mathbf{1}}_{= \mathbf{0}} + \boldsymbol{\beta}^T \underbrace{\xi_S^T \boldsymbol{\pi}}_{\substack{= (\boldsymbol{\pi}^T\xi_S)^T = \mathbf{0}}} = 0,
	\end{split}
	\end{equation*}
	since for a $\xi_S \in \mathcal{T}_S \mathbb{S}_n^{\boldsymbol{\pi}}$ we have $\xi_S \mathbf{1} = \mathbf{0}$ and $\boldsymbol{\pi}^T \xi_S = \mathbf{0}^T$. Therefore, we have $\langle  z , \xi_S  \rangle_S = 0$, $\forall\,\xi_S \in \mathcal{T}_S \mathbb{S}_n^{\boldsymbol{\pi}}$, i.e., $z\in \mathcal{T}_S^\perp \mathbb{S}_n^{\boldsymbol{\pi}}$. To prove the opposite inclusion we use a dimensionality argument. Let us introduce the non-singular diagonal matrix  $D_{\boldsymbol{\pi}} = \operatorname{diag}(\boldsymbol{\pi})$ and observe that
	\[
	D_{\boldsymbol{\pi}}^{-1} z = D_{\boldsymbol{\pi}}^{-1} (\boldsymbol{\alpha} \mathbf{1}^T + \boldsymbol{\pi} \boldsymbol{\beta}^T) \odot S = (\hat{\boldsymbol{\alpha}} \mathbf{1}^T + \mathbf{1} \boldsymbol{\beta}^T) \odot S, \quad \hat{\boldsymbol{\alpha}} = D_{\boldsymbol{\pi}}^{-1}\boldsymbol{\alpha}.
	\]
	Then, the quantity on the right depends only on the first row and column of $(\hat{\boldsymbol{\alpha}} \mathbf{1}^T + \mathbf{1} \boldsymbol{\beta}^T)$, since 
	\[
	(D_{\boldsymbol{\pi}}^{-1} z)_{i,j} = \left( \frac{(D_{\boldsymbol{\pi}}^{-1} z)_{i,1}}{S_{i,1}} + \frac{(D_{\boldsymbol{\pi}}^{-1} z)_{1,j}}{S_{1,j}} - \frac{(D_{\boldsymbol{\pi}}^{-1} z)_{1,1}}{S_{1,1}}  \right) \odot S_{i,j}.
	\]
	Therefore, the dimension of the orthogonal complement of the tangent space has the correct dimension $2n-1$. 
\end{proof}

The above result allows us to give an expression for the orthogonal projection:

\begin{proposition}
	The orthogonal projection $\Pi_S\,:\,\mathbb{R}^{n\times n} \rightarrow \mathcal{T}_S \mathbb{S}_n^{\boldsymbol{\pi}}$ of a matrix $Z$ has the following expression:
	\[
	\Pi_S(Z) = Z - (\boldsymbol{\alpha} \mathbf{1}^T + \boldsymbol{\pi}\boldsymbol{\beta}^T)\odot S,
	\]
	where the vectors $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$ are a solution to the following consistent linear system
	\begin{equation}\label{eq:alpha_and_beta_values}
		\begin{bmatrix}
			Z\mathbf{1} \\
			Z^T \boldsymbol{\pi}
		\end{bmatrix}=
		\begin{bmatrix}
			I & D_{\boldsymbol{\pi}} S \\
			S^T D_{\boldsymbol{\pi}} & \mathrm{diag}(S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi}) 
		\end{bmatrix} 
		\begin{bmatrix}
			\boldsymbol{\alpha}
			\\
			\boldsymbol{\beta}
		\end{bmatrix}, \quad D_{\boldsymbol{\pi}} = \operatorname{diag}(\boldsymbol{\pi}).
	\end{equation}
\end{proposition}

\begin{proof}
	The formula for the orthogonal projection follows from Lemma~\ref{lem:orthogonal_complement}. To find an expression for the vectors $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$, we 
	use~\eqref{eq:matrix-decomposition} and obtain
	\[
	Z\mathbf{1} = \Pi_S(Z)\mathbf{1} + \Pi_S^\perp(Z)\mathbf{1} = \Pi_S^\perp(Z)\mathbf{1}.
	\]
	From Lemma~\ref{lem:orthogonal_complement}, we find that
	\[
	Z \mathbf{1} = ( (\boldsymbol{\alpha}\mathbf{1}^T + \boldsymbol{\pi}\boldsymbol{\beta}^T) \odot S ) \mathbf{1},
	\]
	that is
	\begin{equation*}
		\sum_{j=1}^{n} Z_{i,j} =  \sum_{j=1}^{n} (\alpha_i + \pi_i \beta_j) S_{i,j}= \alpha_i \sum_{j=1}^{n} S_{i,j} + \pi_i \sum_{j=1}^{n} S_{i,j}\beta_j, 
	\end{equation*}
	i.e., in matrix form,
	\[
	Z \mathbf{1} =  \boldsymbol{\alpha} +  D_{\boldsymbol{\pi}} S \boldsymbol{\beta}, \qquad D_{\boldsymbol{\pi}} = \operatorname{diag}(\boldsymbol{\pi}).
	\]
	Similarly, by transposing \eqref{eq:matrix-decomposition}, we obtain
	\begin{equation*}
		\begin{split}
			Z^T \boldsymbol{\pi} = &\; ( (\boldsymbol{\alpha}\mathbf{1}^T + \boldsymbol{\pi}\boldsymbol{\beta}^T) \odot S )^T \boldsymbol{\pi}\\
			=&\;   ( (\mathbf{1}
			\boldsymbol{\alpha}^T) \odot S^T)\boldsymbol{\pi}
			+(\boldsymbol{\beta} \boldsymbol{\pi} ^T) \odot S^T) \boldsymbol{\pi}.  
		\end{split}
	\end{equation*}
	From the properties of the Hadamard product, we find that
	\[
	( (\mathbf{1}
	\boldsymbol{\alpha}^T) \odot S^T)\boldsymbol{\pi}=D_{\mathbf{1}} S^T D_{\boldsymbol{\alpha}} \boldsymbol{\pi}=
	S^T D_{\boldsymbol{\pi}} \boldsymbol{\alpha},
	\]
	and
	\[
	((\boldsymbol{\beta} \boldsymbol{\pi} ^T) \odot S^T) \boldsymbol{\pi}=
	\mathrm{diag}(\boldsymbol{\beta} \boldsymbol{\pi}^T D_{\boldsymbol{\pi}}S)= \mathrm{diag}( S^TD_{\boldsymbol{\pi}}\boldsymbol{\pi}) \boldsymbol{\beta}.
	\]
	Therefore, we conclude that
	\[
	Z^T \boldsymbol{\pi} = 
	S^T D_{\boldsymbol{\pi}} \boldsymbol{\alpha} +
	\mathrm{diag}( S^TD_{\boldsymbol{\pi}}\boldsymbol{\pi}) \boldsymbol{\beta},
	\]
	so that the vectors $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$ can be found as a solution of the linear system
	\[
	\begin{bmatrix}
		Z\mathbf{1} \\
		Z^T \boldsymbol{\pi}
	\end{bmatrix}=
	\begin{bmatrix}
		I & D_{\boldsymbol{\pi}} S \\
		S^T D_{\boldsymbol{\pi}} & \mathrm{diag}( S^TD_{\boldsymbol{\pi}}\boldsymbol{\pi}) 
	\end{bmatrix} 
	\begin{bmatrix}
		\boldsymbol{\alpha}
		\\
		\boldsymbol{\beta}
	\end{bmatrix}.
	\]
	The linear system is consistent with an affine space of solutions of dimension one, since
	\[
	\begin{bmatrix}
		Z\mathbf{1} \\
		Z^T \boldsymbol{\pi}
	\end{bmatrix}^T \begin{bmatrix}
		- \boldsymbol{\pi} \\
		\mathbf{1}
	\end{bmatrix} = - \mathbf{1}^T Z^T \boldsymbol{\pi} + \boldsymbol{\pi}^T Z \mathbf{1} = - \mathbf{1}^T \boldsymbol{\pi} + \boldsymbol{\pi}^T \mathbf{1} = -1 + 1 = 0,
	\]
	and 
	\[
	\begin{bmatrix}
		I & D_{\boldsymbol{\pi}} S \\
		S^T D_{\boldsymbol{\pi}} & \mathrm{diag}( S^TD_{\boldsymbol{\pi}}\boldsymbol{\pi}) 
	\end{bmatrix} \begin{bmatrix}
		- \boldsymbol{\pi} \\
		\mathbf{1}
	\end{bmatrix} = \begin{bmatrix}
		\mathbf{0} \\
		\mathbf{0}
	\end{bmatrix}.
	\]
\end{proof}

Let now $f : \mathbb{S}_n^{\boldsymbol{\pi}} \rightarrow \mathbb{R}$ be a smooth real function defined on the manifold, and denote by $\operatorname{Grad} f(S)$ its euclidean gradient with respect to the euclidean metric. Then we can express the Riemannian gradient as follows.
\begin{proposition}[Riemannian gradient]\label{pro:riemannian-gradient}
	The Riemannian gradient $\operatorname{grad}f(S)$ is expressed in terms of the Euclidean gradient $\operatorname{Grad}f(S)$ as:
	\begin{equation}\label{eq:riemannian-gradient-on-the-new-manifold}
		\operatorname{grad}f(S) = \Pi_S(\operatorname{Grad}f(S) \odot S).
	\end{equation}
\end{proposition}

\begin{proof}
	We prove \eqref{eq:riemannian-gradient-on-the-new-manifold} by directly applying the Definition~\ref{def:riemannian-gradient-and-hessian} of Riemannian gradient. Indeed, the Riemannian gradient is the \emph{unique} element of $\mathcal{T}_S \mathbb{S}_n^{\boldsymbol{\pi}}$ for which  the directional derivative and the Riemannian metric satisfy the equation
	\begin{align}
		\langle \operatorname{grad}f(S), \xi_S \rangle_S = \mathrm{D} f(S) [\xi_S],\ \forall \ \xi_S \in \mathcal{T}_S \mathbb{S}_n^{\boldsymbol{\pi}}.
		\label{eq:riemannian-gradient-definition}
	\end{align}
	The Riemannian gradient is unique, therefore if we find an element of $\mathcal{T}_S \mathbb{S}_n^{\boldsymbol{\pi}}$ for which~\eqref{eq:riemannian-gradient-definition} holds for all tangent vectors, then this is the Riemannian gradient. We start by writing the Euclidean gradient in terms of the directional derivative and the Euclidean scalar product as:
	\[\langle \operatorname{Grad}f(S), \xi \rangle = \mathrm{D} f(S) [\xi],\ \forall \ \xi \in \mathbb{R}^{n \times n}.\]
	Restricting the previous to $\mathcal{T}_S \mathbb{S}_n^{\boldsymbol{\pi}} \subset \mathbb{R}^{n \times n}$, and changing the inner product to the one induced by the Fisher metric~\eqref{eq:fisher-metric} we find
	\[
	\langle \operatorname{Grad}f(S) , \xi_S \rangle = \langle \operatorname{Grad}f(S) \odot S, \xi_S \rangle_S 
	= \mathrm{D} f(S) [\xi_S],\ \forall \ \xi_S \in \mathcal{T}_S \mathbb{S}_n^{\boldsymbol{\pi}}. 
	\]
	To reach the conclusion we now need to apply again Lemma~\ref{lem:orthogonal_complement} and~\eqref{eq:matrix-decomposition} to project the (scaled) Euclidean gradient
	\[ \operatorname{Grad}f(S) \odot S = \Pi_S(\operatorname{Grad}f(S) \odot S) + \Pi_S^\perp(\operatorname{Grad}f(S) \odot S).\]
	From this we find
	\[
	\langle \operatorname{Grad}f(S) \odot S, \xi_S \rangle_S = \langle \Pi_S(\operatorname{Grad}f(S) \odot S), \xi_S \rangle_S,
	\]
	having canceled out the second term $\langle \Pi^\perp_A(\operatorname{Grad}f(S) \odot S), \xi_S \rangle_S = 0$ by means of the definition of the orthogonal complement.
	Summarizing, we have thus shown that $\Pi_S(\operatorname{Grad}f(S) \odot S)$ is \emph{a} tangent vector that satisfies the condition~\eqref{eq:riemannian-gradient-definition}, that permits us to conclude by the uniqueness of the Riemannian gradient that:
	\begin{equation*}
		\operatorname{grad} f(S) = \Pi_S \left( \operatorname{Grad} f(S) \odot S \right).
	\end{equation*}
\end{proof}

To implement Algorithm~\ref{alg:newton} we also need an expression for the Riemannian Hessian. From Definition~\ref{def:riemannian-gradient-and-hessian}, the Riemannian Hessian is related to the Levi-Civita connection, thus we first need a way of expressing the Levi-Civita connection for the metric~\eqref{eq:fisher-metric}.


\begin{proposition}[Koszul formula, {\cite[Theorem~5.3.1]{AbsilBook}}]\label{pro:Koszul}
	The Levi-Civita connection (Definition~\ref{def:affine-connection}) on the Euclidean space $\mathbb{R}^{n \times n}$ endowed with the Fisher information metric~\eqref{eq:fisher-metric} is given by
	\begin{align*}
		\nabla_{\eta_S} \xi_S = \mathrm{D}(\xi_S)[\eta_S] - \cfrac{1}{2} (\eta_S \odot \xi_S) \oslash S
	\end{align*}
\end{proposition}

\begin{theorem}[Riemannian Hessian]
	The Riemannian Hessian $\mathrm{hess} f(S)[\xi_S]$ can be obtained from the Euclidean gradient $\operatorname{Grad} f(S)$ and the Euclidean Hessian $\operatorname{Hess} f(S)$ by using the identity
	\[
	\mathrm{hess} f(S)[\xi_S] = \Pi_S(\mathrm{D}(\mathrm{grad} f(S))[\xi_S])-\frac12 \Pi_S(( \Pi_S(\mathrm{Grad} f(S))\odot S) \odot \xi_S) \oslash S),
	\]
	where
	\[
	D(\mathrm{grad} f(S))[\xi_S] = \dot{\gamma}[\xi_S]- (\dot{\boldsymbol{\alpha}}[\xi_S] \mathbf{1}^T+\boldsymbol{\pi}\dot{\boldsymbol{\beta}}^T[\xi_S])\odot S -
	(\boldsymbol{\alpha} \mathbf{1}^T+\boldsymbol{\pi}\boldsymbol{\beta}^T)\odot \xi_S,
	\]
	and
	\begin{equation*}
	\begin{split}
		\gamma = & \;  \mathrm{Grad} f(S)\odot S,\\
		\dot{\gamma}[\xi_S] = & \; \mathrm{Hess}\; f(S)[\xi_S]\odot S + \mathrm{Grad}\; f(S)\odot \xi_S,\\
		\mathcal{A} = &\;     \begin{bmatrix}
			I & D_{\boldsymbol{\pi}} S \\
			S^T D_{\boldsymbol{\pi}} & \mathrm{diag}( S^TD_{\boldsymbol{\pi}}\boldsymbol{\pi})
		\end{bmatrix}, \\
		\boldsymbol{\alpha}, \boldsymbol{\beta} \,\text{s.t.}\,&\; \mathcal{A} \begin{bmatrix}
			\boldsymbol{\alpha} \\ \boldsymbol{\beta}
		\end{bmatrix} = \begin{bmatrix}
			\gamma \mathbf{1} \\ \gamma^T \boldsymbol{\pi}
		\end{bmatrix},\\
		\dot{\boldsymbol{\alpha}}[\xi_S], \dot{\boldsymbol{\beta}}[\xi_S] \,\text{s.t.}\,&\; \mathcal{A} \begin{bmatrix}
			\dot{\boldsymbol{\alpha}}[\xi_S] \\ \dot{\boldsymbol{\beta}}[\xi_S]
		\end{bmatrix} = \begin{bmatrix}
			\dot{\gamma}[\xi_S] \mathbf{1} \\ \dot{\gamma}^T[\xi_S] \boldsymbol{\pi}
		\end{bmatrix} -
		\begin{bmatrix}
			0 & D_{\boldsymbol{\pi}} \xi_S \\
			\xi_S^T D_{\boldsymbol{\pi}} & \mathrm{diag}(\xi_ S^TD_{\boldsymbol{\pi}}\boldsymbol{\pi})
		\end{bmatrix} 
		\begin{bmatrix}
			\boldsymbol{\alpha} \\ \boldsymbol{\beta}
		\end{bmatrix}.
	\end{split}
\end{equation*}
\end{theorem}

\begin{proof}
	To obtain the first identity it is sufficient to use the Koszul formula (Proposition~\ref{pro:Koszul})
	\begin{equation*}
		\begin{split}
			\mathrm{hess} f(S)[\xi_S] & =\Pi_S(\mathrm{D}(\mathrm{grad} f(S))[\xi_S])-\frac12 \Pi_S(( \mathrm{grad} f(S))\odot \xi_S)\oslash S)\\
			& = \Pi_S(\mathrm{D}(\mathrm{grad} f(S))[\xi_S]) \\ & \qquad -\frac12 \Pi_S(( \Pi_S(\mathrm{Grad} f(S))\odot S) \odot \xi_S) \oslash S).
		\end{split}
	\end{equation*}
	Then, we need to find an expression for $\mathrm{D}(\mathrm{grad} f(S))[\xi_S]$. To find it, we denote $\gamma= \mathrm{Grad} f(S)\odot S$;  from the properties of the Fr√©chet derivative we find that
	\begin{equation*}
	\begin{split}
		\mathrm{D}(\mathrm{grad} f(S))[\xi_S]& =\mathrm{D}(\Pi_S(\gamma))[\xi_S]=\mathrm{D}(\gamma-(\boldsymbol{\alpha} \mathbf{1}^T+\boldsymbol{\pi}\boldsymbol{\beta}^T)\odot S)[\xi_S]=\\
		& \mathrm{D}(\gamma)[\xi_S]-\mathrm{D}( (\boldsymbol{\alpha} \mathbf{1}^T+\boldsymbol{\pi}\boldsymbol{\beta}^T)\odot S)[\xi_S]=\\
		& \dot{\gamma}[\xi_S]- (\dot{\boldsymbol{\alpha}}[\xi_S] \mathbf{1}^T+\boldsymbol{\pi}\dot{\boldsymbol{\beta}}^T[\xi_S])\odot S -
		(\boldsymbol{\alpha} \mathbf{1}^T+\boldsymbol{\pi}\boldsymbol{\beta}^T)\odot \xi_S.
	\end{split}
	\end{equation*}
	Now we need an expression for $\dot{\gamma}[\xi_S]$, $\dot{\boldsymbol{\alpha}}[\xi_S]$, and $\dot{\boldsymbol{\beta}}[\xi_S]$. Concerning  $\dot{\gamma}[\xi_S]=\mathrm{D}(\gamma)[\xi_S]$, we have
	\begin{equation*}
		\begin{split}
			\dot{\gamma}[\xi_S]&=\mathrm{D}(\mathrm{Grad}\; f(S))[\xi_S]\odot S + \mathrm{Grad}\; f(S)\odot \xi_S\\
			&=\mathrm{Hess}\; f(S)[\xi_S]\odot S + \mathrm{Grad}\; f(S)\odot \xi_S.
		\end{split}
	\end{equation*}
	To find an expression for $\dot{\boldsymbol{\alpha}}[\xi_S]$ and $\dot{\boldsymbol{\beta}}[\xi_S]$, we compute the derivative along the direction $\xi_S$ of both sides  of the linear system
	\[
	\begin{bmatrix}
		\gamma \mathbf{1} \\ \gamma^T \boldsymbol{\pi}
	\end{bmatrix} =
	\begin{bmatrix}
		I & D_{\boldsymbol{\pi}} S \\
		S^T D_{\boldsymbol{\pi}} & \mathrm{diag}( S^TD_{\boldsymbol{\pi}}\boldsymbol{\pi})
	\end{bmatrix} 
	\begin{bmatrix}
		\boldsymbol{\alpha} \\ \boldsymbol{\beta}
	\end{bmatrix} \equiv \mathcal{A} \begin{bmatrix}
		\boldsymbol{\alpha} \\ \boldsymbol{\beta}
	\end{bmatrix}.
	\]
	Therefore, we obtain
	\[
	\begin{bmatrix}
		\dot{\gamma}[\xi_S] \mathbf{1} \\ \dot{\gamma}^T[\xi_S] \boldsymbol{\pi}
	\end{bmatrix} =
	\begin{bmatrix}
		0 & D_{\boldsymbol{\pi}} \xi_S \\
		\xi_S^T D_{\boldsymbol{\pi}} & \mathrm{diag}(\xi_ S^TD_{\boldsymbol{\pi}}\boldsymbol{\pi})
	\end{bmatrix} 
	\begin{bmatrix}
		\boldsymbol{\alpha} \\ \boldsymbol{\beta}
	\end{bmatrix}+
	\begin{bmatrix}
		I & D_{\boldsymbol{\pi}} S \\
		S^T D_{\boldsymbol{\pi}} & \mathrm{diag}( S^TD_{\boldsymbol{\pi}}\boldsymbol{\pi})
	\end{bmatrix} 
	\begin{bmatrix}
		\dot{\boldsymbol{\alpha}}[\xi_S] \\ \dot{\boldsymbol{\beta}}[\xi_S]
	\end{bmatrix}, 
	\]
	so that $\dot{\boldsymbol{\alpha}}[\xi_S]$ and $\dot{\boldsymbol{\beta}}[\xi_S]$ can be computed by solving the linear system (with the same system matrix)
	\begin{equation*}
		\mathcal{A}
		\begin{bmatrix}
			\dot{\boldsymbol{\alpha}}[\xi_S] \\ \dot{\boldsymbol{\beta}}[\xi_S]
		\end{bmatrix}= \begin{bmatrix}
			\dot{\gamma}[\xi_S] \mathbf{1} \\ \dot{\gamma}^T[\xi_S] \boldsymbol{\pi}
		\end{bmatrix} -
		\begin{bmatrix}
			0 & D_{\boldsymbol{\pi}} \xi_S \\
			\xi_S^T D_{\boldsymbol{\pi}} & \mathrm{diag}(\xi_S^TD_{\boldsymbol{\pi}}\boldsymbol{\pi})
		\end{bmatrix} 
		\begin{bmatrix}
			\boldsymbol{\alpha} \\ \boldsymbol{\beta}
		\end{bmatrix} 
	\end{equation*}
\end{proof}

To complete the construction of the Riemannian optimization algorithm, we also need to define the retraction from the tangent bundle to the manifold (Definition~\ref{def:retraction}). To obtain such a map, we apply a suitable modification of the generalized Sinkhorn-Knopp algorithm~\cite{Rothblum}, which is based on the following

\begin{theorem}[Sinkhorn generalization, {\cite[Theorem 2(a)-(b)]{Rothblum}}]\label{thm:sink}
	Let $A \in \mathbb{R}^{n \times n}$ be a nonnegative matrix. Then for any vectors $\mathbf{r},\mathbf{c} \in \mathbb{R}^{n}$ with nonnegative entries there exist diagonal matrices $D_1$ and $D_2$ such that
	\[
	D_1 A D_2 \mathbf{1} = \mathbf{r}, \qquad D_2 A^T D_1 \mathbf{1} = \mathbf{c},
	\]
	if and only if there exists a matrix $B$ such $B\mathbf{1} = \mathbf{r}$ and $B^T\mathbf{1} = \mathbf{c}$, and having the same nonzero pattern as~$A$. Furthermore, if the matrix $A$ is positive, then $D_1$ and $D_2$ are unique up to a constant factor.
\end{theorem}

From the previous result, we can obtain the following generalization which allows us to obtain a matrix on the manifold $\mathbb{S}_n^{\boldsymbol{\pi}}$ through suitable diagonal scaling.
\begin{proposition}\label{prop:extsk}
	Let $A \in \mathbb{R}^{n \times n}$ be a matrix with positive entries. Then  there exist diagonal matrices $D_1$ and $D_2$ such that
	\[
	D_1 A D_2 \mathbf{1} = \mathbf{1}, \qquad \boldsymbol{\pi}^T D_1 A D_2  = \boldsymbol{\pi}^T.
	\]
	Moreover, $D_1$ and $D_2$ are diagonal matrices such that
	$D_1\widehat A D_2 \mathbf{1}=\boldsymbol{\pi}$ and $\mathbf{1}^T D_1\widehat A D_2 =\boldsymbol{\pi}^T$, where $\widehat A=\mathrm{diag}(\boldsymbol{\pi})A$. 
\end{proposition}

\begin{proof}
	Consider the matrix $\widehat A=\mathrm{diag}(\boldsymbol{\pi})A$. 
	By setting $\mathbf{c}=\mathbf{r}=\boldsymbol{\pi}$, according to Theorem~\ref{thm:sink} applied to $\widehat A$, there exist diagonal matrices $D_1$ and $D_2$ such that $D_1\widehat A D_2 \mathbf{1}=\boldsymbol{\pi}$ and $\mathbf{1}^T D_1\widehat A D_2 =\boldsymbol{\pi}^T$.
	Since diagonal matrices commute, from the first equality we obtain
	$\mathrm{diag}(\boldsymbol{\pi})^{-1}D_1\widehat A D_2 \mathbf{1}=\mathbf{1}$, so that
	$D_1 A D_2 \mathbf{1}=\mathbf{1}$; from the second equality, we find that $\mathbf{1}^T \mathrm{diag}(\boldsymbol{\pi}) D_1 A D_2 =\boldsymbol{\pi}^T$, i.e., $\boldsymbol{\pi}^T D_1 A D_2  = \boldsymbol{\pi}^T$.   
\end{proof}

Then we can combine the above result with the following description of retractions for a manifold that can be embedded in the Euclidean space.

\begin{theorem}[{\cite[Proposition 4.1.2]{AbsilBook}}]\label{thm:retraction-on-embedded-manifold}
	Let $\mathcal{M}$ be an embedded manifold of the Euclidean space $\mathcal{E}$ and let $\mathcal{N}$ be an abstract manifold such that $\dim(\mathcal{M}) + \dim(\mathcal{N}) = \dim(\mathcal{E})$. Assume that there is a diffeomorphism
	\begin{align*}
		\phi: \mathcal{M} \times \mathcal{N} &\longrightarrow \mathcal{E}^*  \\
		(A,B) &\longmapsto \phi(A,B)
	\end{align*}
	where $\mathcal{E}^*$ is an open subset of $\mathcal{E}$, with a neutral element $I \in \mathcal{N}$ satisfying
	\begin{align*}
		\phi(A,I) = A, \ \forall \ A \in \mathcal{M}.
	\end{align*}
	Under the above assumption, the mapping 
	\begin{align*}
		R_x: \mathcal{T}_x\mathcal{M} &\longrightarrow \mathcal{M}  \\
		\xi_x  &\longmapsto  R_x(\xi_x) = \pi_1(\phi^{-1}(x+\xi_x)),
	\end{align*}
	where $\pi_1: \mathcal{M} \times \mathcal{N} \longrightarrow \mathcal{M}: (A,B) \longmapsto A$ is the projection onto the first component, defines a retraction on the manifold $\mathcal{M}$ for all $x \in \mathcal{M}$ and $\xi_x$ in the neighborhood of $0_x$.
\end{theorem}

Proposition~\ref{prop:extsk} and Theorem~\ref{thm:retraction-on-embedded-manifold} provide an expression for the retraction to the manifold $\mathbb{S}_n^{\boldsymbol{\pi}}$.
\begin{theorem}[Retraction]
	The map $R: \mathcal{T}\mathbb{S}_n^{\boldsymbol{\pi}} \longrightarrow \mathbb{S}_n^{\boldsymbol{\pi}} $ whose restriction $R_{S}$ to $ \mathcal{T}_S\mathbb{S}_n^{\boldsymbol{\pi}} $ is given by:
	\begin{align}
		R_{S}(\xi_S) = S + \xi_S,
	\end{align}
	is a well-defined retraction on $\mathbb{S}_n^{\boldsymbol{\pi}}$ in the sense of Definition~\ref{def:retraction} whenever $\xi_S$ is in a neighborhood of $\mathbf{0}_{S}$, i.e., whenever $S > - \xi_S$ entry-wise.
	\label{th4}
\end{theorem}

\begin{proof}
	Since $\mathbb{S}_n^{\boldsymbol{\pi}}$ an embedded manifold, we apply Theorem~\ref{thm:retraction-on-embedded-manifold}, where the diffeomorphism $\phi$ is obtained by means of the extension of Sinkhorn theorem given in Proposition~\ref{prop:extsk}.  Since we deal with matrices with positive entries, the result in Proposition~\ref{prop:extsk} is invariant with respect to the scaling $D_1$ and $D_2$. Thus we can assume, without loss of generality, that the first diagonal element $(D_1)_{11}=1$. Then, the map $\phi$ we need to construct is given by
	\begin{align*}
		\phi: \mathbb{S}_n^{\boldsymbol{\pi}} \times \myR^{2n-1} &\longrightarrow \myR^{n \times n} & \begin{array}{l}\myR^{n \times n} = \{ S \in \mathbb{R}^{n \times n} \ : \ S > 0\},\\
			\myR^{2n-1} = \{ \mathbf{x} \in \mathbb{R}^{2n-1} \,:\, \mathbf{x} > 0 \},\end{array} \\[-0.4em]
		\left(S,\begin{pmatrix}
			\mathbf{d}_1 \\ \mathbf{d}_2
		\end{pmatrix}\right) &\longmapsto \operatorname{diag}(1,\mathbf{d}_1) S \operatorname{diag}(\mathbf{d}_2).
	\end{align*}
	Such $\phi$ satisfies all the requirements of Theorem~\ref{thm:retraction-on-embedded-manifold}. Indeed, both $\myR^{2n-1}$ and $\myR^{n \times n}$ are manifold as open subsets of the manifolds $\mathbb{R}^{2n-1}$ and  $\mathbb{R}^{n \times n}$, respectively. Furthermore, they satisfy the dimensionality relation since \[\dim(\mathbb{S}_n^{\boldsymbol{\pi}}) + \dim( \myR^{2n-1}) = (n-1)^2+2n-1 = n^2 = \dim(\mathbb{R}^{n \times n}).\] 
	Finally, the identity element $I \equiv \mathbf{1}$ of $\mathbb{R}^{2n-1}$ satisfies $\phi(S,\mathbf{1}) = S$, and $\phi$ inherits the required regularity from the regularity of the matrix product. To build the projection $\pi_1$ in Theorem~\ref{thm:retraction-on-embedded-manifold} we need the existence of the inverse map $\phi^{-1}$. This amount to an application of the (modified) Sinkhorn-Knopp's algorithm scaling the rows and the columns of the matrix. Observe that this is again a smooth map for the regularity of the matrix product. We have therefore proved that $\phi$ is a diffeomorphism. By Theorem~\ref{thm:retraction-on-embedded-manifold}, this means that $\pi_1(\phi^{-1}(S+\xi_S))$ is a retraction for $\xi_S$ in the neighborhood of $\mathbf{0}_{S}$, i.e., $(S+\xi_S) \in \myR^{n \times n} $ which can explicitly written in an element-wise sense as $S_{ij} > - \xi_S$. Using the definition of $\mathbb{S}_n^{\boldsymbol{\pi}}$ and Lemma~\ref{lem:tangent-space} the inverse map is the identity, since
	\begin{align*}
		(S+\xi_S)\mathbf{1} &= S\mathbf{1}+\xi_S\mathbf{1} = \mathbf{1}+ \mathbf{0} =\mathbf{1}, \\
		(S+\xi_S)^T\boldsymbol{\pi} &= S^T\boldsymbol{\pi}+\xi_S^T\boldsymbol{\pi} = \boldsymbol{\pi} + \mathbf{0} = \boldsymbol{\pi},
	\end{align*}
	hence, the canonical retraction is defined as $R_{S}(\xi_S) = S + \xi_S$.
\end{proof}

To avoid the deterioration of the quality of the analogous retraction on $\mathbb{S}_n^{\mathbf{1}}$ in the presence of small modulus elements in the iterations of the Riemannian optimization algorithms, in~\cite{Douik8861409} a modification based on the combination of the entry-wise exponential of a matrix and the Sinkhorn-Knopp‚Äôs algorithm (Theorem~\ref{thm:sink}) is proposed. We adapt here such proposal to the manifold $\mathbb{S}_n^{\boldsymbol{\pi}}$. 

\begin{theorem}
	The map $R^{\exp}: \mathcal{T}\mathbb{S}_n^{\boldsymbol{\pi}} \longrightarrow \mathbb{S}_n^{\boldsymbol{\pi}} $ whose restriction $R^{\exp}_{S}$ to $ \mathcal{T}_S\mathbb{S}_n^{\boldsymbol{\pi}} $ is given by:
	\begin{align}\label{eq:exp-retraction}
		R_{S}^{\exp}(\xi_S) = \mathcal{S}\left( S \odot \exp(\xi_S \oslash S )\right),
	\end{align}
	is a first-order retraction on $\mathbb{S}_n^{\boldsymbol{\pi}}$, where $\mathcal{S}\left( \cdot \right)$ represents an application of the modified Sinkhorn-Knopp‚Äôs algorithm in Proposition~\ref{prop:extsk}, and $\exp(\cdot)$ the entry-wise exponential.
\end{theorem}

\begin{proof}
	We need to show Definition~\ref{def:retraction} is verified. The map~\eqref{eq:exp-retraction} is centered, since for an $S \in \mathbb{S}_n^{\boldsymbol{\pi}}$ 
	\[
	R_{S}^{\exp}(0) = \mathcal{S}\left( S \odot \exp(0 \oslash S )\right) = \mathcal{S}\left( S \odot \exp(0)\right) = \mathcal{S}\left( S \right) = S,
	\]
	having selected $D_1 = D_2 = I$ in Proposition~\ref{prop:extsk}. To prove the local rigidity, we need to show that the curve $\gamma_{\xi_s}(\tau) = R_S^{\exp}(\tau \xi_S)$ satisfies
	\[
	\left.\frac{\mathrm{d}\gamma_{\xi_s}(\tau) }{\mathrm{d}\tau}\right\rvert_{\tau = 0} = \xi_S, \quad \forall\,\xi_S \in \mathcal{T}_S\mathbb{S}_n^{\boldsymbol{\pi}}.
	\]
	By definition
	\[
	\left.\frac{\mathrm{d}\gamma_{\xi_s}(\tau) }{\mathrm{d}\tau}\right\rvert_{\tau = 0} = \lim_{\tau \rightarrow 0} \frac{\mathcal{S}\left( S \odot \exp(\tau \xi_S \oslash S )\right) - S}{\tau} = \lim_{\tau \rightarrow 0} \frac{\mathcal{S}( S + \tau \xi_S + \mathcal{O}(\tau^2)) - S}{\tau},
	\]    
	where the last equality follows from the first order Taylor expansion of the exponential
	\[
	S \odot \exp(\tau \xi_S \oslash S ) = S \odot \left( 1 + \tau \xi_S \oslash S + \mathcal{O}(\tau^2) \right) = S + \tau \xi_S + \mathcal{O}(\tau^2)\text{ for } \tau \rightarrow 0.
	\]
	As in the proof of Theorem~\ref{thm:retraction-on-embedded-manifold}, we can now select $\tau$ small enough for having $S + \tau \xi_S$ a matrix with all positive entries, and apply Proposition~\ref{prop:extsk} to write
	\begin{equation*}
	\begin{split}
	\mathcal{S}( S + \tau \xi_S) = & \, (D_1 + \delta D_1) ( S + \tau \xi_S ) (D_2 + \delta D_2) \\ = & \, D_1 S D_2 + D_1 ( \tau \xi_S ) D_2 + \delta D_1 S D_2 + D_1 S \delta D_2,
	\end{split}
	\end{equation*}
	since $S \in \mathbb{S}_n^{\boldsymbol{\pi}}$ we have $D_1 = D_2 = I$, hence
	\[
	\mathcal{S}( S + \tau \xi_S) =  S  + \tau \xi_S  + \delta D_1 S  + S \delta D_2.
	\]
	We exploit now that $\xi_S \in \mathcal{T}_S\mathbb{S}_n^{\boldsymbol{\pi}}$ (Lemma~\ref{lem:tangent-space}), and write
	\begin{equation*}
	\begin{split}
		\mathbf{1} \equiv \mathcal{S}( S + \tau \xi_S)\mathbf{1} = & \,  S\mathbf{1}  + \tau \xi_S\mathbf{1}  + \delta D_1 S \mathbf{1}  + S \delta D_2 \mathbf{1} \\ 
		= & \, \mathbf{1}  + \delta D_1 \mathbf{1} + S \delta D_2 \mathbf{1} \\
		\boldsymbol{\pi}^T \equiv \boldsymbol{\pi}^T \mathcal{S}( S + \tau \xi_S) = & \,  \boldsymbol{\pi}^TS  + \tau \boldsymbol{\pi}^T\xi_S  + \boldsymbol{\pi}^T \delta D_1 S  + \boldsymbol{\pi}^T S \delta D_2 \\ 
		= & \, \boldsymbol{\pi}^T  + \boldsymbol{\pi}^T \delta D_1 S  + \boldsymbol{\pi}^T \delta D_2,
	\end{split}
	\end{equation*}
	equivalently
	\[
	\hat{\mathcal{M}} \begin{bmatrix}
		\boldsymbol{\delta}_1 \\
		\boldsymbol{\delta_2} 
	\end{bmatrix} \equiv \begin{bmatrix}
		I & S \\
		S^T D_{\boldsymbol{\pi}} & D_{\boldsymbol{\pi}}
	\end{bmatrix} \begin{bmatrix}
		\delta D_1 \mathbf{1} \\
		\delta D_2 \mathbf{1}
	\end{bmatrix} = \begin{bmatrix}
		\mathbf{0} \\
		\mathbf{0}
	\end{bmatrix}.
	\]
	The null space of $\hat{\mathcal{M}}$ is generated by the $[\mathbf{1}^T,-\mathbf{1}^T]^T$ vector, equivalently $\boldsymbol{\delta}_1 = - \boldsymbol{\delta}_2 = c \mathbf{1}$, hence $\delta D_1 S  + S \delta D_2 = 0$. Therefore, we have just proved that $\mathcal{S}(S + \tau \xi_S + \mathcal{O}(\tau^2)) = S + \tau \xi_S + \mathcal{O}(\tau^2)$, and consequently
	\[
	\left.\frac{\mathrm{d}\gamma_{\xi_s}(\tau) }{\mathrm{d}\tau}\right\rvert_{\tau = 0} = \lim_{\tau \rightarrow 0} \frac{ S + \tau \xi_S + \mathcal{O}(\tau^2) - S}{\tau} = \xi_S.
	\]
\end{proof}

\subsection{Computational issues}\label{sec:computational_issues}
We have implemented this new Riemannian manifold in a format compatible with the Manopt library \cite{manopt}, i.e., we have produced a Matlab function that outputs a \mintinline{matlab}{struct} variable whose fields implement the operation on the manifold, i.e., the \mintinline{matlab}{function} with prototype
\begin{minted}[bgcolor=bg,fontsize=\small]{matlab}
function M = multinomialfixedstochasticfactory(pi,optionsolve)
%
%
end
\end{minted}
in which the \mintinline{matlab}{optionsolve} contains options concerning the solution of the auxiliary linear systems, that can be selected when instantiating the manifold. The implementation is based on the \verb|multinomialdoublystochasticfactory.m| code by A. Douik and N. Boumal; see~\cite{Douik8861409} and the relevant references in~\cite{manopt}. 

For the computation of the various projections on the tangent space and for the computation of the Riemannian Hessian we have to solve several compatible singular linear systems of the form
\begin{equation}\label{eq:atildes}
	\mathcal{A} \begin{bmatrix}
		\mathbf{x}\\
		\mathbf{y}
	\end{bmatrix} =
	\begin{bmatrix}
		\mathbf{c}\\
		\mathbf{d}
	\end{bmatrix},
	~~~
	\mathcal{A} = \begin{bmatrix}
		I & D_{\boldsymbol{\pi}} S \\
		S^T D_{\boldsymbol{\pi}} & \mathrm{diag}(S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi})
	\end{bmatrix}.
\end{equation}
To this purpose, we want to use an iterative method of the Krylov type to avoid assembling the $2 \times 2$ block matrix. Since the system is symmetrical, an indication of the convergence properties can be obtained starting from the spectral properties of the matrix $\mathcal{A}$. 

\begin{proposition}[Spectral properties]\label{pro:spectral_properties}
	Given $S \in \mathbb{S}_n^{\boldsymbol{\pi}}$, the $2 \times 2$ block matrix
	$\mathcal{A}$  defined in \eqref{eq:atildes}
	is such that
	\begin{itemize}
		\item $\mathcal{A}$ is similar to a singular M-matrix,
		\item $\lambda(\mathcal{A}) \in \{ 0 \} \cup \left[\frac{\left(\delta^*+1 -\sqrt{\delta^*  (\delta^* +4 r^*-2)+1}\right)}{2}, \max\{ 1+\| \boldsymbol{\pi} \|_\infty, 2\| \boldsymbol{\pi} \|_\infty\}\right]$, for
		\[
		r^* =       
		\min_{j=1,\ldots,n} \max_{i = 1,\ldots, n} s_{i,j}, \text{ and } \delta^* = \min_{ \substack{i=1,\ldots,n \\ i \neq k}} \left( S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi} \right)_i,
		\]
		and $k$ is such that $r^* =       
		\min_{i = 1,\ldots, n} s_{i,k}$;
		%
		%
		%
		%
		moreover, if $r^*+\delta^*< 1$, then
		$\lambda(\mathcal{A}) \in \{ 0 \} \cup \left[ \delta^* \left( 1 - \frac{r^*}{1-\delta^*}\right), \max\{ 1+\| \boldsymbol{\pi} \|_\infty, 2\| \boldsymbol{\pi} \|_\infty\}\right]$.
	\end{itemize}
\end{proposition}

\begin{proof}
	The matrix 
	\[
	\tilde{\mathcal{A}}=D^{-1} \mathcal{A} D=
	\begin{bmatrix}
		I & -S \\
		-S^T D_{\boldsymbol{\pi}}^2 & \mathrm{diag}(S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi}) 
	\end{bmatrix}
	, ~~
	D=\begin{bmatrix}
		D_{\boldsymbol{\pi}} & 0 \\ 0 & -I
	\end{bmatrix},
	\]
	is a Z-matrix. Since $\tilde{\mathcal{A}}\mathbf{1}=0$, then $\tilde{\mathcal{A}}$ is a singular M-matrix \cite{bp:book}. In particular, its eigenvalues     have non negative real part \cite{bp:book}. On the other hand, since $\mathcal{A}$ is symmetric, then its eigenvalues are real. Therefore the eigenvalues of $\mathcal{A}$ are $\lambda_1=0\le \lambda_2\le \cdots\le \lambda_{2n}$. Since $S$ is irreducible and $\boldsymbol{\pi}>0$, then the matrix $\tilde{\mathcal{A}}$ is irreducible as well, therefore $\lambda_2>0$. 
	Since 
	\[
	\mathcal{A}\begin{bmatrix}
		\mathbf{1} \\ \mathbf{1}
	\end{bmatrix}=
	\begin{bmatrix}
		\mathbf{1}+\boldsymbol{\pi} \\ \boldsymbol{\pi}+S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi}
	\end{bmatrix} \le \begin{bmatrix}
		\mathbf{1}+\boldsymbol{\pi} \\ 2\boldsymbol{\pi}
	\end{bmatrix}
	\]
	then
	$\| \mathcal{A}\|_\infty\le \max\{ 1+\| \boldsymbol{\pi} \|_\infty, 2\| \boldsymbol{\pi} \|_\infty\}$, which implies \[\lambda_{2n}\le \max\{ 1+\| \boldsymbol{\pi} \|_\infty, 2\| \boldsymbol{\pi} \|_\infty\}.\]
	
	%
	%
	%
	
	%
	To obtain a lower-bound we use the eigenvalue interlacing Theorem \cite[Theorem~4.3.6]{MR2978290} for symmetric matrices. Indeed, 
	by removing the $i$-th column and row from $\mathcal{A}$ we obtain a matrix $\mathcal{A}'$ whose eigenvalues are such that
	\[
	0 = \lambda_1 \le  \lambda_1' \leq \lambda_2 \leq \ldots \leq \lambda_{2n-1}' \leq \lambda_{2n}.
	\]
	Therefore, a lower bound to  $\lambda_1'$ gives a lower bound to $\lambda_2$. 
	For the moment, assume that $i=2n$.
	Since $S$ is a positive matrix, then $\mathcal{A}'$ is irreducible, therefore, by using the same arguments as for $\mathcal{A}$ to show that $\lambda_1>0$, we deduce that $\lambda_1'>0$. 
	To give a positive lower bound to $\lambda_1'$, consider the parametric similarity transformation for $\alpha \in (0,1)$
	\[
	\hat{\mathcal{A}} = \begin{bmatrix}
		\alpha^{-1} D_{\boldsymbol{\pi}}^{-1} & \\
		& I_{n-1}
	\end{bmatrix} \mathcal{A}' \begin{bmatrix}
		\alpha D_{\boldsymbol{\pi}} & \\
		& I_{n-1}
	\end{bmatrix} = \begin{bmatrix}
		I_n & \alpha^{-1} \hat{S} \\
		\alpha \hat{S}^T D_{\boldsymbol{\pi}}^2 & \mathrm{diag}(\hat{S}^T D_{\boldsymbol{\pi}} \boldsymbol{\pi}) 
	\end{bmatrix},
	\]
	where $\hat{S}$ is the $n\times(n-1)$ matrix obtained by removing the last column to the matrix $S$. Let us call $\boldsymbol{\delta} = \hat{S}^T D_{\boldsymbol{\pi}} \boldsymbol{\pi}$ the $n-1$-vector generating the diagonal matrix in the $(2,2)$ block, then we can estimate the smallest eigenvalue by Gershgorin Theorem~\cite[Chapter~6]{MR2978290}, that is, we can estimate $\lambda_1'$ by the intersection with the $x$ axis of the left-most circle. The circles for the first $n$ rows have center in $1$ and radii 
	\[
	\alpha^{-1} r_i = \alpha^{-1} (\hat{S} \mathbf{1})_i = \alpha^{-1} \left( \mathbf{1} - S \mathbf{e}_n \right)_i = \alpha^{-1} (1-s_{in}) < \alpha^{-1} 1, \; i = 1,\ldots,n,
	\]
	whilst the circles for the last $n-1$ rows have center $\delta_i$ and radii $\alpha \delta_i$. Thus we have  
	\[
	\lambda_1' > \min \left\lbrace 1 - \alpha^{-1} r, \delta (1-\alpha) \right\rbrace
	\]
	for 
	\[
	r = \alpha^{-1} \max_{i = 1,\ldots,n} r_i, \qquad \delta = \min_{i = 1,\ldots,n-1} \delta_i.
	\]
	Observe now that, instead of removing the last row and column in $\widetilde{A}$, we can optimize the bound by selecting the column $\hat{j}$ to which corresponds
	\[
	r^* = \min_{j=1,\ldots,n} \max_{i = 1,\ldots, n} (1-s_{i,j}),
	\]
	and the corresponding 
	\[
	\delta^* = \min_{ \substack{i=1,\ldots,n \\ i \neq {\hat j}}} \delta_i.
	\]
	We can then solve the problem by solving the minimization problem\pgfmathsetmacro\MathAxis{height("$\vcenter{}$")}
	\[
	\alpha^* = \arg\max\min_{\alpha \in (0,1)}\left\lbrace 1 - \alpha^{-1} r, \delta (1-\alpha) \right\rbrace = \begin{tikzpicture}[baseline={(0, 1cm-\MathAxis pt)}]
		\begin{axis}[
			width=1.65in,
			height=1.65in,
			axis lines=middle,
			xmin = -0.5,
			xmax = 1.5,
			ymin = -0.5,
			ymax = 1.5,
			ytick={0, 1},
			xtick={0, 1},
			]
			\addplot[domain=0:1.5,dashed] {1};
			\addplot[domain=0:1.5,smooth,samples=200] { 1 - 0.1/x )};
			\addplot[domain=0:1.5,smooth,samples=200] { 0.6*(1-x)};
			\addplot[red,mark=*] coordinates {(0.193713,0.483772)} node[pin=25:{$\alpha$}]{};
			\addplot[mark=-,thick] coordinates {(0.1,0.6)} node[pin=140:{$\delta$}]{};
			\addplot[mark=|,thick] coordinates {(0.1,0)} node[pin=150:{$r$}]{};
			\addplot[domain=0.1:0.193713,smooth,samples=200,thick,blue] { 1 - 0.1/x )};
			\addplot[domain=0.193713:1,smooth,samples=200,thick,blue] { 0.6*(1-x)};
		\end{axis}
	\end{tikzpicture}
	\]
	which is therefore equivalent to obtaining the positive root of the quadratic equation
	\[
	1-\frac{r}{\alpha }=(1-\alpha ) \delta \,\Leftrightarrow\,\alpha^* = \frac{\delta - 1 +\sqrt{\delta ^2+\delta  (4 r-2)+1}}{2 \delta }.
	\]
	The lower-bound is then given by
	\[
	\lambda_2 \geq \frac{1}{2} \left(\delta^*+1 -\sqrt{\delta^*  (\delta^* +4 r^*-2)+1}\right). 
	\]
	We can further elaborate on the above bound.
	Indeed, since $(1+x)^{1/2}\le 1+\frac12 x$, we obtain
	\begin{equation*}
	\begin{split}
		\sqrt{\delta^*  (\delta^* +4 r^*-2)+1}  = & \sqrt{ (1-\delta^*)^2(1+ 4 r^* \delta^* (1-\delta^*)^{-2})}\le \\
		& (1-\delta^*) (1+ 2 r^* \delta^* (1-\delta^*)^{-2}),
	\end{split}
	\end{equation*}
	so that
	\[
	\lambda_2\ge \delta^* \left( 1 - \frac{r^*}{1-\delta^*}\right).
	\]
	This latter inequality makes sense if $r^*+\delta^*< 1$. 
\end{proof}

These eigenvalue properties suggest different strategies to solve the linear system \eqref{eq:atildes}. Firstly, we can directly apply the Conjugate Gradient (CG) to the linear system, in fact, if the starting vector is not in the null space of the matrix, we do not undergo a breakdown; some preconditioning strategies are discussed in the sequel. Secondly, we can consider using the LSQR method on the system instead. To reduce the dimensionality of the problem, we can solve the system for the Schur complement with respect to the $(1,1)$-block, i.e. we solve instead
\begin{equation}\label{eq:schurversion}
	[ \mathrm{diag}(S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi}) - S^T D_{\boldsymbol{\pi}^2} S ]\mathbf{y} = \mathbf{c} - S^T D_{\boldsymbol{\pi}} \mathbf{b}, \quad \mathbf{x} = \mathbf{b} - D_{\boldsymbol{\pi}} S \mathbf{y}.
\end{equation}
The matrix in the above system is a symmetric irreducible singular M-matrix, therefore it is  semidefinite, with a simple eigenvalue equal to 0.

In both formulations \eqref{eq:atildes} and \eqref{eq:schurversion}, since the basis of the null space is known, we can consider the de-singularized version obtained through a rank 1 update of the matrix; so that, in small size problems, we can employ the $LU$-factorization to solve the associated linear systems. Specifically, for the $2 \times 2$-block formulation \eqref{eq:atildes}, this consists in solving the updated linear system
\[
\left( \begin{bmatrix}
	I & D_{\boldsymbol{\pi}} S \\
	S^T D_{\boldsymbol{\pi}} & \mathrm{diag}(S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi}) 
\end{bmatrix} + \frac{1}{\boldsymbol{\pi}^T\boldsymbol{\pi} + n} \begin{bmatrix}
	\boldsymbol{\pi}\\-\mathbf{1}
\end{bmatrix} [\boldsymbol{\pi}^T,-\mathbf{1}^T]\right) \begin{bmatrix}
	\hat{\mathbf{x}}\\
	\hat{\mathbf{y}}
\end{bmatrix} = \begin{bmatrix}
	\mathbf{b}\\
	\mathbf{c}
\end{bmatrix}.
\]
Indeed, we may easily observe that $[\hat{\mathbf{x}}^T,
\hat{\mathbf{y}}^T]^T$ solves also the original linear system \eqref{eq:atildes}.
The eigenvalues of the matrix in the above system are the eigenvalues of the matrix $\mathcal{A}$, except for the eigenvalue 0 which is replaced by 1. Since, for Proposition~\ref{pro:spectral_properties}, the upper bound on the eigenvalues of $\mathcal{A}$ is
$\max\{ 1+\| \boldsymbol{\pi} \|_\infty, 2\| \boldsymbol{\pi} \|_\infty\}\ge 1$,  we expect that the rank one update does not modify conditioning of the linear system \eqref{eq:atildes}.  
For the Schur complement version \eqref{eq:schurversion}, the updated system is given by
\[
\left[ \mathrm{diag}(S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi}) - S^T D_{\boldsymbol{\pi}^2} S + \sigma\frac{1}{n} \mathbf{1}\mathbf{1}^T \right]\hat{\mathbf{y}} = \mathbf{c} - S^T D_{\boldsymbol{\pi}} \mathbf{b}, \quad \hat{\mathbf{x}} = \hat{\mathbf{b}} - D_{\boldsymbol{\pi}} S \hat{\mathbf{y}}.
\]
where $\sigma>0$. As in the previous case $[\hat{\mathbf{x}}^T,
\hat{\mathbf{y}}^T]^T$ solves also the original linear system \eqref{eq:atildes}.
Moreover, the eigenvalues of the matrix in the above system are the eigenvalues of the matrix $\mathrm{diag}(S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi}) - S^T D_{\boldsymbol{\pi}^2} S$, except for the eigenvalue 0 which is replaced by $\sigma$. In order not to deteriorate the conditioning of the system, the parameter $\sigma$ should be chosen between the smallest nonzero eigenvalue and the largest eigenvalue of the matrix $\mathrm{diag}(S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi}) - S^T D_{\boldsymbol{\pi}^2} S$. By using Gershgorin theorem, an upper bound to the spectral radius is given by $2\| S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi} \|_\infty$, while a lower bound can be found by using the Cauchy interlacing theorem, as in the proof of Proposition~\ref{pro:spectral_properties}, by bounding the smallest eigenvalue of a principal $(n-1)\times (n-1)$ matrix. Therefore $\sigma$ might be chosen as the arithmetic mean between these two bounds.


The different strategies can be selected through the \mintinline[breaklines,breakanywhere]{matlab}{optionsolve} variable in the definition phase of the manifold. Default values can be generated through the \mintinline[breaklines,breakanywhere]{matlab}{initoptions()} function, i.e., \mintinline[breaklines,breakanywhere]{matlab}{optionsolve = initoptions()}. The user can select between the $2\times 2$-block formulation of the linear system (\mintinline[breaklines,breakanywhere]{matlab}{optionsolve.formulation = "block";}) and the reduction to the Schur complement (\mintinline[breaklines,breakanywhere]{matlab}{optionsolve.formulation = "schur";}). In both cases the desingularization strategy via the rank-1 update can be activated (\mintinline[breaklines,breakanywhere]{matlab}{options.correction = true;}). The solution method for the linear systems can then be selected between the direct strategy (\mintinline[breaklines,breakanywhere]{matlab}{options.method = "direct";}), the CG (\mintinline[breaklines,breakanywhere]{matlab}{options.method = "cg";}), and the LSQR method ((\mintinline[breaklines,breakanywhere]{matlab}{options.method = "lsqr";}). All the options are case-insensitive. Additional debugging and tracing options can be enabled through this facility and are discussed in the code.

To precondition the CG algorithm we start from an empirical observation, since the target matrix $X$ has to be stochastic we expect, as the dimension $n$ of the problem grows, to encounter a large number of small entries. This suggests using a \emph{diagonally compensated modified incomplete Cholesky}~\cite[Section~10.3.5]{MR1990645} directly on the system matrix. The diagonal compensation term, to avoid the presence of nonpositive pivots, can be taken to be $\min_{i=1,\ldots,n}\pi_i$. Another strategy consists in first scaling the system
\begin{equation*}
\begin{split}
	\left[ I -  \mathrm{diag}(S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi})^{-\nicefrac{1}{2}} S^T D_{\boldsymbol{\pi}^2} S \mathrm{diag}(S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi})^{-\nicefrac{1}{2}} \right]  \tilde{\mathbf{y}} = \\ \qquad \mathrm{diag}(S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi})^{-\nicefrac{1}{2}}  \left( \mathbf{c} - S^T D_{\boldsymbol{\pi}} \mathbf{b}\right),
\end{split}
\end{equation*}
and then recover 
\[
\mathbf{y} = \mathrm{diag}(S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi})^{-\nicefrac{1}{2}} \tilde{\mathbf{y}},  \quad \mathbf{x} = \mathbf{b} - D_{\boldsymbol{\pi}} S \mathbf{y}.
\]
By calling $\tilde{S} = \mathrm{diag}(S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi})^{-\nicefrac{1}{2}} S^T D_{\boldsymbol{\pi}^2} S \mathrm{diag}(S^T D_{\boldsymbol{\pi}} \boldsymbol{\pi})^{-\nicefrac{1}{2}}$, and observing that $\rho(\tilde{S})$ $=1$, we can use a truncated Neumann series as preconditioner, i.e.,
\begin{equation}\label{eq:neumann_preconditioner}
	P_{k,\tau} = I + \sum_{j = 1}^k \hat{S}^j, \qquad (\hat{S})_{p,q} = \begin{cases}
		(\tilde{S})_{p,q}, & |\tilde{S}_{i,j}| \geq \tau,\\
		0, & \text{otherwise}.
	\end{cases} 
\end{equation}
The two strategies can be selected by choosing \mintinline[breaklines,breakanywhere]{matlab}{options.method = "pcg";} or \mintinline[breaklines,breakanywhere]{matlab}{options.method = "pcg2";} respectively in the code. In both cases, the preconditioner must be regenerated anew at each outer iteration of the optimization method, i.e., whenever we move the tangent space.
\section{Numerical Examples}\label{sec:numer_ex}


In this section, we will compare the algorithms based on Riemannian optimization with the methods available in the literature. We will also validate the theoretical results discussed in Section~\ref{sec:computational_issues} concerning the location of eigenvalues. Specifically, in Section~\ref{sec:stochastic-old-methods-experiments} we compare the Riemannian optimization on the $\mathbb{S}_n$ manifold with the methods based on constrained optimization available in the literature. Then, in Section~\ref{sec:stochastic-new-methods-experiments} we test the new approach that preserves the stationary distribution, i.e., the Riemannian optimization routines on the $\mathbb{S}_n^{\boldsymbol{\pi}}$ manifold. Furthermore, we numerically investigate also the computational issues discussed in Section~\ref{sec:computational_issues}.  Finally, in Section~\ref{sec:reducible-chains} we test our algorithms on an application in finance, where $A$ is the transition matrix of a Markov chain which represents the dynamics of the different credit ratings. The peculiarity of this problem is that $A$ is reducible, therefore its invariant distribution $\boldsymbol{\pi}$ is not positive, so the manifold $\mathbb{S}_n^{\boldsymbol{\pi}}$ cannot be defined. 

The numerical examples have been executed on a Laptop with Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i7-8750H CPU @ 2.20GHz with 16~Gb of memory, and running MATLAB 2023a and Manopt v.7.1. Code and examples are available in the GitHub repository \href{https://github.com/Cirdans-Home/pth-root-stochastic}{github.com/Cirdans-Home/pth-root-stochastic} and can be reproduced.

\subsection{Stochastic \texorpdfstring{$p$th}{pth} root approximation via Riemannian optimization}\label{sec:stochastic-old-methods-experiments}
In this section, we address the problem of the stochastic \texorpdfstring{$p$th}{pth} root approximation via Riemannian optimization with the manifold described in Section~\ref{sec:stochastic-old-methods}. We compare the Riemannian algorithm with the \emph{interior point method} implemented in the MATLAB routine \mintinline{matlab}{fmincon}~\cite{interiorpoint}. For the construction of the test matrices, we have prepared a generator of stochastic matrices of different classes \mintinline[breaklines,breakanywhere]{matlab}{matrixgenerator(n,p,seed,classes,numbers)} that produces matrices of which we want to approximate the stochastic $p$th root;  the classes are described in the Table~\ref{tab:matrix-classes}.
\begin{table}[htbp]
	\centering
	\setminted[matlab]{fontsize=\footnotesize}
	\caption{Matrix classes produced by the \mintinline{matlab}{matrixgenerator} code. The routine also allows fixing the seed of the random number generator so that the set of matrices generated for equal parameters is always the same.}
	\label{tab:matrix-classes}
	\begin{tabular}{p{3cm}p{3.5cm}ccc}
		\toprule
		Name & Description & Size & Embeddability & Ref.\\
		\midrule
		uniform stochastic & %
\begin{tabminted}{matlab}
B = rand(n,n);
D = diag(sum(B,2));
A = D\B;
\end{tabminted} 
		& $n$ & unknown & \\ \cmidrule{2-4}
		$p$th power of uniform stochastic & %
\begin{tabminted}{matlab}
B = rand(n,n);
D = diag(sum(B,2));
A = mpower(D\B,p);
\end{tabminted} 
		& $n$ & yes & \\ \cmidrule{2-4}
		$\exp$ of intensity matrix & %
\begin{tabminted}{matlab}
B = rand(n,n);
B(1:1+n:end) = 0;
D = diag(sum(B,2));
A = expm(B - D);
\end{tabminted} 
		& $n$ & yes & \\ \cmidrule{2-4}
		K80 & %
\begin{tabminted}{matlab}
b = rand(1);
c = sqrt(b)-b;
a = 1-b-2*c;
A0= [a,b;a,b];
E = ones(2,2);
A = [A0,c*E;c*E,A0];
\end{tabminted} 
		& 4 & yes & \cite{Casanellas2020} \\ \cmidrule{2-4}
		& %
\begin{tabminted}{matlab}
b = 0.5*rand(1);
c = (1-2*b)/2;
a = 1-b-2*c;
A0 = [a,b;a,b];
E = ones(2,2);
A = [A0,c*E;c*E,A0];
\end{tabminted} 
		& 4 & no & \cite{Casanellas2020} \\ \cmidrule{2-4}
		Pei &
\begin{tabminted}{matlab}
I = eye(n,n);
J = ones(n,n);
alpha = rand(1)-(1/(n-1))^p;
beta = (1-alpha)/n;
A = alpha*I+beta*J;
\end{tabminted} 
		& $n$ & yes & \cite{LinThesis} \\
		\bottomrule
	\end{tabular}
\end{table}
To obtain the approximation we use the formulation~\ref{alg:a2} and generate $40$ stochastic test matrices for each class, where the dimension of the matrices with variable size is $n = 100$. As Riemannian optimization algorithms we consider the \mintinline{matlab}{trustregions} algorithm~\cite{genrtr} and the \mintinline{matlab}{rlbfgs} algorithm~\cite{Huang2016}. The \mintinline{matlab}{trustregions} algorithm uses both the cost functional, its gradient, and an approximation of the Hessian. On the other hand, the \mintinline{matlab}{rlbfgs} algorithm uses only the cost functional, and the gradient. To approximate Euclidean gradients and Hessian matrices we use automatic differentiation (AD) instead of defining them analytically. All methods are initialized from the same starting point generated as a random point on the manifold of stochastic matrices $\mathcal{S}_n$. We measure the quality of the obtained results in terms of the residual on the cost functional~\ref{alg:a2} and on the time necessary to carry out the optimization. We report the data in the form of a performance profile in Figure~\ref{fig:standard_results}.
% Figure environment removed
The results show that the Riemannian algorithms perform much better than their counterpart based on constrained optimization. Both the execution times and the residuals are practically always lower. Furthermore, no significant difference is observed between the use of the LBFGS algorithm and that of internal point, i.e., the two lines are practically overlapping and have little difference on an extremely low percentage of problems.

\subsection{Stochastic \texorpdfstring{$p$th}{pth} root approximation preserving the stationary distribution}\label{sec:stochastic-new-methods-experiments}

In this section, we compare the two Riemannian optimization algorithms on the manifolds $\mathbb{S}_n$ and $\mathbb{S}_n^{\boldsymbol{\pi}}$, for the different test matrices from Table~\ref{tab:matrix-classes}. Specifically, we compare the stationary distribution of the approximation $X$ obtained in the two manifolds, with the stationary distribution $\boldsymbol{\pi}$ of $A$, by using the same tolerance request on the gradient norm (\mintinline{matlab}{1e-4}) and the same starting point for the optimization algorithm (\mintinline{matlab}{X0 = M.rand()}) which, in this case, is the \texttt{trustregions} method.

% Figure environment removed

From Figure~\ref{fig:preserving-pi} we observe that in several cases the optimization routine on $\mathbb{S}_n^{\boldsymbol{\pi}}$ provides a larger or comparable residual for the approximation of the square root, furthermore in all the cases the stationary distribution is recovered to the floating-point relative accuracy. From this experiment, we observe a trade-off between the achievable accuracy with respect to the residual with the starting matrix and the recovery of the stationary vector. Analogous results are also obtained for the case in which we consider roots of higher order (Figure~\ref{fig:preserving-pi-5}) for which the behavior is essentially the same.

In the next sections, we investigate the behavior of the different numerical methods for the solution of the linear systems needed for computing the various projections between the tangent plane and the manifold discussed in Section~\ref{sec:computational_issues}.

\subsubsection{Properties and solution of the associated linear systems}

To validate the results of the bound given by the Proposition~\ref{pro:spectral_properties} we consider the test matrices from Table~\ref{tab:matrix-classes}. Specifically, we first build the manifold associated with the stationary distribution for each matrix in the 
class, then we generate a random point on that manifold and the associated $2 \times 2$ block matrix needed for computing the projection on the tangent space associated with that point, i.e., the matrix in~\eqref{eq:alpha_and_beta_values}. Figure~\ref{fig:bound-on-matrices} reports the result of such an experiment.
% Figure environment removed
We observe that the upper bound is more accurate than the lower one.

As a second exploration, we consider the different solution strategies of associated linear systems discussed in Section~\ref{sec:computational_issues}. Let us first consider a case outside the optimization algorithm. That is, we generate the manifold $\mathbb{S}_n^{\boldsymbol{\pi}}$, where $\boldsymbol{\pi}$ is the stationary distribution associated with the out-degree random walk on the graphs from Figure~\ref{tab:graph-example}.
% Figure environment removed
Specifically, if we call $G$ the adjacency matrix of the largest connected component of such graphs, then we consider the manifold $\mathbb{S}_n^{\boldsymbol{\pi}}$ for $\boldsymbol{\pi}$ the invariant vector of the stochastic matrix $A = \operatorname{diag}(G\mathbf{1})^{-1}\mathbf{G}$. To have comparable results in the various cases, we choose to always have the external Riemannian trust-region optimizer perform 50 iterations and all instances are run by re-initializing the random number generator to the same seed. From the results in Figure~\ref{fig:iteration} we observe that the $2 \times 2$-block formulation is the one for which the convergence is more prone to undergo oscillations. On the other hand, the reduced reformulation in terms of the Schur complement given in equation~\eqref{eq:schurversion} tends to improve the situation. It is also observed that the two preconditioning strategies proposed are able to consistently reduce the number of iterations necessary for convergence. In particular, the version that uses the modified Cholesky incomplete factorization is more sensitive to the choice of the tolerance on the discarded elements and on the size of the global matrix. On the other hand, the use of the Neumann series preconditioner $P_{k,\tau}$ seems to give more consistent results with a higher tolerance on the elements to be dropped.

\subsection{The case of reducible Markov chains}\label{sec:reducible-chains}

We consider here what is an \emph{edge case} for our approach, i.e.,  when the chain is reducible due to the existence of two communication classes; in other words, to the case in which the stationary distribution $\boldsymbol{\pi} = [0,0,\ldots,1]^T$. This case is motivated by the embedding problem for Markov models of the term structure of credit risk spreads~\cite{application2}. The Markov chain modeling represents the dynamics of the different credit rating states, i.e., evaluations of the relative ability of an entity or obligation to meet financial commitments over time and examining the probability of transitioning in between these states. The different levels go from \texttt{AAA}, the lowest expectation of default risk, to \texttt{D}, for an issuer who has entered into bankruptcy and cannot recover from it. In Table~\ref{tab:creditriskmatrix} we report sample data from~\cite{application2}.
\begin{table}[htbp]
	\centering
	\small
	\begin{tabular}{r|cccccccc}
		\toprule
		\mintinline{matlab}{data(i,j)} &	\texttt{AAA}	&	\texttt{AA}	&	\texttt{A}	&	\texttt{BBB}	&	\texttt{BB}	&	\texttt{B}	&	\texttt{CCC}	&	\texttt{D}	\\
		\midrule
		\texttt{AAA}	&	0,891	&	0,0963	&	0,0078	&	0,0019	&	0,003	&	0	&	0	&	0	\\
		\texttt{AA}	&	0,0086	&	0,901	&	0,0747	&	0,0099	&	0,0029	&	0,0029	&	0	&	0	\\
		\texttt{A}	&	0,0009	&	0,0291	&	0,8894	&	0,0649	&	0,0101	&	0,0045	&	0	&	0,0009	\\
		\texttt{BBB}	&	0,0006	&	0,0043	&	0,0656	&	0,8427	&	0,0644	&	0,016	&	0,0018	&	0,0045	\\
		\texttt{BB}	&	0,0004	&	0,0022	&	0,0079	&	0,0719	&	0,7764	&	0,1043	&	0,0127	&	0,0241	\\
		\texttt{B}	&	0	&	0,0019	&	0,0031	&	0,0066	&	0,0517	&	0,8246	&	0,0435	&	0,0685	\\
		\texttt{CCC}	&	0	&	0	&	0,0116	&	0,0116	&	0,0203	&	0,0754	&	0,6493	&	0,2319	\\
		\texttt{D}	&	0	&	0	&	0	&	0	&	0	&	0	&	0	&	1	\\
		\bottomrule
	\end{tabular}
	\cprotect\caption{Standard and Poor's Credit Review (1993) from \cite[Table~3]{application2}. The numbers are reported with four figures as in the original data, this means that the probabilities are not normalized to have sum one. To be able to use them in the code we renormalize the entries as: {\mintinline{matlab}{A = diag(sum(data,2))\data;}}.}
	\label{tab:creditriskmatrix}
\end{table}
In order to compute a stochastic approximation of the root of the matrix, we cannot use manifold-based optimization directly, since the stationary distribution is $\boldsymbol{\pi} = [0,0,\ldots,1]^T$ and therefore it cannot be used to construct the manifold. In a similar guise to the Page Rank problem, we  apply a perturbation to the data matrix to make it irreducible, as
\[
\tilde{A} = (1-\gamma) A +  \gamma(\mathbf{1}\mathbf{1}^T)/n, \qquad 0 < \gamma \ll 1,  
\]
which admits stationary distribution $\tilde{\boldsymbol{\pi}}>0$ with respect to which we can construct the manifold $\mathbb{S}_n^{\tilde{\boldsymbol{\pi}}}$. The approximate root can then be computed by solving for
\begin{equation}\label{eq:riskopt}
	\min_{X \in \mathbb{S}_n^{\tilde{\boldsymbol{\pi}}}} \frac{1}{2}\|X^2 - A \|_F^2.
\end{equation}
For $\gamma = 10^{-4}$, we obtain \[\tilde{\boldsymbol{\pi}} = [0.0002, 0.0007, 0.0012, 0.0009, 0.0005, 0.0006, 0.0001, 0.9957]^T. \]
For the construction of the starting point for the optimization we exploit the modified Sinkhorn algorithm (Proposition~\ref{prop:extsk}) to project the perturbed version $A$ of the upper triangular matrix on the manifold in the same way,
\begin{minted}[bgcolor=bg,fontsize=\small]{matlab}
E = ones(size(A));                %
X0 = diag(sum(triu(ones(size(A))),2))\triu(ones(size(A))); %
X0 = gam*E + (1-gam)*X0;          %
X0 = modifiedsinkhorn(X0,pi,100); %
\end{minted}
and start both the standard constrained optimization and the Riemannian optimization for~\eqref{eq:riskopt}. In Figure~\ref{fig:riskfigure} we report the two approximations of the root thus obtained, as it can be observed the approximation obtained through the Riemannian optimization has a structure that is much closer to what we would expect from the appropriate matrix function. In particular the extradiagonal values on the last row are negligible with respect to the weight of the transition probability of remaining in \texttt{D}.
% Figure environment removed
In addition, beyond the structural similarity, the Riemannian approximation preserves the stationary (perturbed) distribution by construction, while the one based solely on constrained optimization produces a probability distribution that is closer to being uniform than concentrated in the \texttt{D} state; see Figure~\ref{fig:riskfigure:stationary}.

\section{Conclusions and future directions}\label{sec:conclusions}

In this paper, we have dealt with the problem of approximating the $p$th root of a stochastic matrix with a stochastic matrix. In particular, by observing that stochastic matrices form a Riemannian manifold with respect to the Fisher metric, we have exploited several specific optimization algorithms which show better performance than their counterparts that use only constrained optimization. Furthermore, we have introduced a new Riemannian manifold---employing the same metric---on the set of stochastic matrices with fixed steady state. This allowed us to employ Riemannian optimization algorithms capable of obtaining approximations of stochastic $p$th roots which also preserve such vector, i.e., such that the Markov chain induced by them has the same steady state. We have also shown that we can apply the proposed strategy to the case of non-reducible Markov chains through a perturbation technique. 

In the future, we intend to study the geodetic structure of this new manifold to better characterize the obtained $p$th root approximations, and to further investigate the solution of the associated computational problems, e.g., the solution of the linear systems needed to calculate the projections from the tangent plane to the manifold and the representation of the Riemannian Hessian, in order to further improve the computational efficiency of the proposed methods.

\bibliographystyle{siamplain}
\bibliography{references}
\end{document}
