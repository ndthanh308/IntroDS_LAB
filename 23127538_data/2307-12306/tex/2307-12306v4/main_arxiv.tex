\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage[a4paper,top=3cm,bottom=2cm,left=2.3cm,right=2.3cm,marginparwidth=1.75cm]{geometry}

%\title{Scaling up Physics-Informed Neural Networks (PINNs) to Very High-Dimensions}
%\title{Tackling the curse-of-dimensionality with physics-informed neural networks via SDGD}
\title{Tackling the Curse of Dimensionality with Physics-Informed Neural Networks}


\date{}

\usepackage{etoolbox}
\makeatletter
\patchcmd{\@maketitle}{\LARGE}{\fontsize{15}{20}\selectfont}{}{}
\makeatother

\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\usepackage{subfigure}
\usepackage{times}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\tsigma}{\tilde \sigma}
\newcommand{\tw}{\tilde w}
\newcommand{\tth}{\hat h}
\newcommand{\tv}{\tilde v}
\newcommand{\hh}{\hat h}
\newcommand{\hv}{\hat w}
\newcommand{\hhm}{\hat m}
\newcommand{\bh}{\bar h}
\newcommand{\bv}{\bar v}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bt}{\boldsymbol{\theta}}

\graphicspath{{./Figures/}}
\begin{document}


%% Group authors per affiliation:
\author{Zheyuan Hu\thanks{Department of Computer Science, National University of Singapore, Singapore, 119077 (\href{mailto:e0792494@u.nus.edu}{e0792494@u.nus.edu},\href{mailto:kenji@nus.edu.sg}{kenji@nus.edu.sg})}
\and Khemraj Shukla\thanks{Division of Applied Mathematics, Brown University, Providence, RI 02912, USA (\href{mailto:khemraj\_shukla@brown.edu}{khemraj\_shukla@brown.edu}, \href{mailto:george\_karniadakis@brown.edu}{george\_karniadakis@brown.edu})}
\and George Em Karniadakis\footnotemark[2] \and  \linebreak Kenji Kawaguchi\footnotemark[1]}

\maketitle

\begin{abstract}
The curse-of-dimensionality taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs, as Richard E. Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. We develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and randomly samples a subset of these dimensional pieces in each iteration of training PINNs. We prove theoretically the convergence and other desired properties of the proposed method. We demonstrate in various diverse tests that the proposed method can solve many notoriously hard high-dimensional PDEs, including the Hamilton-Jacobi-Bellman (HJB) and the Schr\"{o}dinger equations in tens of thousands of dimensions very fast on a single GPU using the PINNs mesh-free approach. Notably, we solve nonlinear PDEs with nontrivial, anisotropic, and inseparable solutions in 100,000 effective dimensions in 12 hours on a single GPU using SDGD with PINNs. Since SDGD is a general training methodology of PINNs, it can be applied to any current and future variants of PINNs to scale them up for arbitrary high-dimensional PDEs.
\end{abstract}

% , hence addressing both memory limitations and slow convergence issues

% The new method involves decomposing and sampling PDE terms, a technique we have named Stochastic Dimension Gradient Descent (SDGD). 

%  in high-dimensional cases

%\maketitle

\begin{comment}
%% -- this is old abstract -- 

The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high dimensional partial differential equations (PDEs) as Richard Bellman first pointed out over 60 years ago. We propose to further develop Physics-Informed Neural Networks (PINNs) to tackle the CoD in the context of PDEs. Specifically, we develop a new method for training and scaling up PINNs to solve arbitrary high-dimensional PDEs. We resolve the computational and memory bottleneck, which is the backpropagation of the residual loss, by decomposing the PDE terms of each dimension. Through this decomposition, we can reduce memory costs even more than just reducing the number of residual points. This leads to an inexpensive and unbiased gradient estimator that can be combined with modern stochastic gradient descent algorithms to accelerate convergence.
Based on this, we propose further acceleration. In addition to using SGD over PDE terms in the backward pass as mentioned above, we also perform unbiased sampling on the forward pass independently, making both the forward and backward passes computationally efficient and with low memory cost. We also discuss how our approach can perform parallel computations for further speed-up and scale-up. We theoretically demonstrate that the stochastic gradients generated by SGD over PDE terms are unbiased, providing convergence guarantees for our method. Additionally, we conduct a theoretical analysis comparing SDGD with SGD over residual points. Under the same batch size and memory cost, there exist proper batch sizes for PDE terms and residual points that can lead to minimized gradient variance and faster convergence, which cannot be achieved for the conventional SGD over points solely. This finding establishes that SDGD represents a more refined and general form of SGD, enabling smaller batch sizes, reduced memory usage, and accelerated convergence with proper batch sizes.

To quantify our claims, we first conduct extensive experiments on several nonlinear high-dimensional PDEs and investigate the stability of the proposed SGD over terms that unbiasedly sample the backward pass. We demonstrate that, at the same memory cost, SDGD is as stable as the conventional SGD over collocation points, and even more efficient in some cases. At the same time, with the further speed-up method via sampling both forward and backward passes, we can train PINNs on nontrivial nonlinear PDEs in 100,000 dimensions in 6 hours of training time (on a single GPU), while the traditional training methods exceed the GPU memory limit directly.  Combined with parallel computing, the training time for this case can be further reduced to several minutes.

Subsequently, we compare the two proposed algorithms with other methods on multiple nonlinear PDEs, and show that SDGD outperforms all competitors in various metrics. Additionally, SDGD for PINNs enables mesh-free training and prediction across the entire domain, while other methods typically only provide predictions at a single point and are mesh-dependent. Furthermore, we combine our algorithm with adversarial training, notorious for its slowness in high-dimensional machine learning, and show that SDGD accelerates it significantly. Finally, we demonstrate the generality of SDGD by applying it to the Schr\"{o}dinger equation for quantum physics and chemistry.
Taken together, SDGD provides a new efficient paradigm for large-scale PINN training but can be readily extended to any other regression or even classification problems.
The code will be released upon acceptance in \url{https://github.com/zheyuanhu01/Scaling_up_PINN/}.

\end{comment}

\section{Introduction}
The curse-of-dimensionality (CoD) refers to the computational and memory challenges when dealing with high-dimensional problems that do not exist in low-dimensional settings.  The term was first introduced in 1957 by Richard E. Bellman \cite{bellman1966dynamic,hammer1962adaptive}, who was working on dynamic programming, to describe the exponentially increasing costs due to high dimensions. Later, the concept was adopted in various areas of science, including numerical partial differential equations, combinatorics, machine learning, probability, stochastic analysis, and data mining. 
%
In the context of numerically solving Partial Differential Equations (PDEs), an increase in the dimensionality of the PDE's independent variables tends to lead to a corresponding exponential rise in computational costs needed for obtaining an accurate solution. This poses a considerable challenge for all existing algorithms.

 Physics-Informed Neural Networks (PINNs) are highly practical in solving partial differential equation (PDE) problems \cite{raissi2017physics}. Compared to other methods, PINNs can solve various PDEs in their general form, enabling mesh-free solutions and handling complex geometries. Additionally, PINNs leverage the interpolation capability of neural networks to provide accurate predictions throughout the domain, unlike other methods for high-dimensional PDEs, which can only compute the value of PDEs at a single point \cite{beck2021deep, han2018solving, raissi2018forward}. Due to neural networks' flexibility and expressiveness, in principle, the set of functions representable by PINNs have the ability to approximate high-dimensional solutions. However, PINNs may also fail in tackling CoD due to insufficient memory and slow convergence when dealing with high-dimensional PDEs. For example, when the dimension of PDE is very high, even just using one collocation point can lead to an insufficient memory error. Unfortunately, the aforementioned high-dimensional and high-order cases often occur for very useful PDEs, such as the Hamilton-Jacobi-Bellman (HJB) equation in stochastic optimal control, the Fokker-Planck equation in stochastic analysis and high-dimensional probability, and the Black-Scholes equation in mathematical finance, etc. These PDE examples pose a grand challenge to the application of PINNs in effectively tackling real-world large-scale problems.


To scale up PINNs for arbitrary high-dimensional PDEs, we propose a training method of PINNs by decomposing and sampling gradients of PDEs, which we name {\em stochastic dimension gradient descent} (SDGD). It accelerates the training of PINNs while significantly reducing the memory cost, thereby realizing the training of any high-dimensional PDEs with acceleration. Specifically, we reduce the memory consumption in PINNs by decomposing the computational and memory bottleneck, namely the gradient of the residual loss that contains PDE terms. We propose a novel decomposition of the gradient of the residual loss into each piece corresponding to each dimension of PDEs. Then, at each training iteration, we sample a subset of these dimensional pieces to optimize PINNs. Although we use only a subset per iteration, this sampling process is ensured to be an unbiased estimator of the full gradient of all dimensions, which is then used to guarantee convergence for all dimensions. 

SDGD enables more efficient parallel computations, fully leveraging multi-GPU computing to scale up and expedite the speed of PINNs. Parallelizing the computation of mini-batches of different dimensional pieces across multiple devices can expedite convergence. This is akin to traditional SGD over data points, which allows simultaneous computation on different machines. SDGD also accommodates the use of gradient accumulation on resource-limited machines to enable larger batch sizes and reduce gradient variance. Our method also enables parallel computations, fully leveraging multi-GPU computing to scale up and expedite the speed of PINNs. Gradient accumulation involves accumulating gradients from multiple batches to form a larger batch size for stochastic gradients, reducing the variance of each stochastic gradient on a single GPU. Due to the refined and general partitioning offered by SDGD, our gradient accumulation can be executed on devices with limited resources, ideal for edge computing. 

We theoretically prove that the stochastic gradients generated by SDGD are unbiased. Based on this, we also provide convergence guarantees for our method. Additionally, our theoretical analysis shows that under the same batch size and memory cost, proper batch sizes for PDE terms and residual points can minimize gradient variance and accelerate convergence, an outcome not possible solely with conventional SGD over points. SDGD extends and generalizes traditional SGD over points, providing stable, low-variance stochastic gradients while enabling smaller batch sizes and reduced memory usage.

We conduct extensive experiments on several high-dimensional PDEs. We investigate the relationship between SDGD and SGD over collocation points, where the latter is stable and widely adopted to reduce memory cost. We vary the number of PDE terms and collocation points to study the stability and convergence speed of SDGD and SGD under the same memory budgets. Experimental results show that our proposed SDGD is as stable as SGD over points on multiple nonlinear high-dimensional PDEs and can accelerate convergence. In some cases, SDGD over terms can achieve faster convergence than SGD under the same memory cost for both methods. 

Furthermore, we showcase large-scale PDEs where traditional PINN training directly fails due to an out-of-memory (OOM) error. Concretely, with the further speed-up method via sampling both forward and backward passes, we can train PINNs on nontrivial nonlinear PDEs in 100,000 dimensions in 12 hours of training time, while the traditional training methods exceed the GPU memory limit directly. Combined with parallel computing, the training of the 100,000D nonlinear equation can be reduced to just a few minutes.

After validating the effectiveness of SDGD compared to SGD over collocation points, we compare the algorithms with other methods. On multiple nonlinear PDEs, our algorithm outperforms other methods in various metrics. Additionally, our method enables mesh-free training and prediction across the entire domain, while other methods typically only provide predictions at a single point. Furthermore, we combine our algorithm with adversarial training, notorious for its slowness in high-dimensional machine learning, and our algorithm significantly accelerates it. We also demonstrate the generality of our method by applying it to the Schr\"{o}dinger equation, which has broad connections with quantum physics and quantum chemistry. Overall, our method provides a paradigm shift for performing large-scale PINN training. 

The rest of this paper is arranged as follows. We present related work in Section 2, our main algorithm for accelerating and scaling up PINNs in Section 3, theoretical analysis of our algorithms' convergence in Section 4, numerical experiments in Section 5, and conclusion in Section 6.
In the Appendices, we include more examples and more details of our method.




\section{Related Work}
\subsection{Physics-Informed Machine Learning}
This paper is based on the concept of Physics-Informed Machine Learning \cite{karniadakis2021physics}. Specifically, PINNs \cite{raissi2019physics} utilize neural networks as surrogate solutions for PDEs and optimize the boundary loss and residual loss to approximate the PDEs' solutions. On the other hand, DeepONet \cite{lu2019deeponet} leverages the universal approximation theorem of infinite-dimensional operators and directly fits the PDE operators in a data-driven manner. It can also incorporate physical information by using residual loss. Therefore, our algorithms can also be flexibly combined with the training of physics-informed DeepONet. Since their inception, PINNs have succeeded in solving numerous practical problems in computational science, e.g., nonlinear PDEs \cite{raissi2018hidden}, solid mechanics \cite{haghighat2021physics}, uncertainty quantification \cite{yang2019adversarial}, inverse water waves problems \cite {jagtap2022deep}, and fluid mechanics \cite{cai2021physics}, to name just a few. DeepONets also have shown their great potential in solving stiff chemical mechanics \cite{goswami2023learning}, shape optimization \cite{shukla2023deep}, and materials science problems \cite{goswami2022physics}.


The success of PINNs has also attracted considerable attention in theoretical analysis. In the study conducted by Luo et al. \cite{Luo2020TwoLayerNN}, the authors delved into the exploration of PINNs' prior and posterior generalization bounds. Additionally, they utilized the neural tangent kernel to demonstrate the global convergence of Physics-Informed Neural Networks (PINNs). Mishra et al. \cite{mishra2020estimates} derived an estimate for the generalization error of PINNs, considering both the training error and the number of training samples. In a different approach, Shin et al. \cite{shin2020convergence} adapted the Schauder approach and the maximum principle to provide insights into the convergence behavior of the minimizer as the number of training samples tends towards infinity. They demonstrated that the minimizer converges to the solution in both $C^0$ and $H^1$. Furthermore, Lu et al. \cite{lu2021priori} employed the Barron space within the framework of two-layer neural networks to conduct a prior analysis on PINN with the softplus activation function. This analysis is made possible by drawing parallels between the softplus and ReLU activation functions. More recently, Hu et al. \cite{hu2021extended} employed the Rademacher complexity concept to measure the generalization error for both PINNs and the extended PINNs variant (XPINNs \cite{jagtap2020extended} and APINNs \cite{hu2022augmented}).


\subsection{Machine Learning PDEs Solvers in High-Dimensions}
Numerous attempts have been to tackle high-dimensional PDEs by deep learning to overcome the curse of dimensionality.
In the PINN literature, \cite{wang20222} showed that to learn a high-dimensional HJB equation, the $L^\infty$ loss is required. The authors of \cite{he2023learning} proposed to parameterize PINNs by the Gaussian smoothed model and to optimize PINNs without back-propagation by Stein's identity, avoiding the vast amount of differentiation in high-dimensional PDE operators to accelerate the convergence of PINNs. Separable PINN \cite{cho2022separable} considers a per-axis sampling of residual points instead of point-wise sampling in high-dimensional spaces, thereby reducing the computational cost of PINN. However, this method \cite{he2023learning, cho2022separable} focuses on acceleration on only modest (less than 4) dimensions. Specifically, separable PINNs \cite{cho2022separable} are designed mainly for increasing the number of collocation points in 3D PDEs, whose separation of sampling points becomes intractable in high-dimensional spaces.  Separable PINNs  \cite{cho2022separable} also struggle in PDE problems with non-separable solutions.
Han et al. \cite{han2018solving, han2017deep} proposed the DeepBSDE solver for high-dimensional PDEs based on the classical BSDE method and they used  deep learning to approximate unknown functions. Extensions of the DeepBSDE method were presented in \cite{beck2019machine, chan2019machine,henry2017deep,hure2020deep,ji2020three}. Becker et al. \cite{becker2021solving} solved high-dimensional optimal stopping problems via deep learning. The authors of \cite{beck2021deep} combined splitting methods and deep learning to solve high-dimensional PDEs. Raissi \cite{raissi2018forward} leveraged the relationship between high-dimensional PDEs and stochastic processes whose trajectories provide supervision for deep learning. 
Despite the effectiveness of the methods in \cite{beck2021deep, han2018solving, han2017deep, raissi2018forward}, they can only be applied to a certain restricted class of PDEs.
Wang et al. \cite{wang2022tensor, wang2022solving} proposed tensor neural networks with efficient numerical integration and separable DeepONet structures for solving high-dimensional Schr\"{o}dinger equations in quantum physics. Zhang et al. \cite{zhang2020learning} proposed the use of PINNs for solving stochastic differential equations by representing their solutions via spectral dynamically orthogonal and bi-orthogonal methods. Zang et al. \cite{zang2020weak} proposed a weak adversarial network that solves PDEs using the weak formulation.

In the numerical PDE literature, there have been attempts to scale numerical methods to high dimensions, e.g., proper generalized decomposition (PGD) \cite{chinesta2011short}, multi-fidelity information fusion algorithm \cite{perdikaris2016multifidelity}, and ANOVA \cite{martens2020neural,zhang2012error}. Darbon and Osher \cite{darbon2016algorithms} proposed a fast algorithm for solving high-dimensional Hamilton-Jacobi equations if the Hamiltonian is convex and positively homogeneous of degree one. The multilevel Picard method \cite{beck2020overcoming,beck2020overcoming_ac,becker2020numerical,hutzenthaler2020overcoming, hutzenthaler2021multilevel} is another approach for approximating solutions of high-dimensional parabolic PDEs, which reformulates the PDE problem as a stochastic fixed point
equation, which is then solved by multilevel and nonlinear Monte-Carlo. Similar to Beck et al. \cite{beck2021deep} and Han et al. \cite{han2018solving}, the multilevel Picard method can only output the solution's value on one test point and can only be applied to a certain restricted class of PDEs.

\subsection{Stochastic Gradient Descent}
This paper accelerates and scales up PINNs based on the idea of stochastic gradient descent (SGD). In particular, we propose the aforementioned SDGD method. SGD is a standard approach in machine learning for handling large-scale data. In general, SGD is an iterative optimization algorithm commonly used in machine learning for training models. It updates the model's parameters by computing the gradients on a small randomly selected subset of training examples, known as mini-batches. This randomness introduces stochasticity, hence enabling faster convergence and efficient utilization of large datasets, making SGD a popular choice for training deep learning models.

Among them, the works in \cite{fehrman2020convergence, lei2019stochastic, mertikopoulos2020almost} are remarkable milestones. Specifically, the condition presented in \cite{lei2019stochastic} stands as a minimum requirement within the general optimization literature, specifically pertaining to the Lipschitz continuity of the gradient estimator. Their proof can also be extended in establishing convergence for our particular case, necessitating the demonstration of an upper bound on the Lipschitz contact of our gradient estimator. On the other hand, \cite{fehrman2020convergence, mertikopoulos2020almost}  adopted a classical condition that entails a finite upper bound on the variance. In this case, it suffices to compute an upper bound for the variance term. While \cite{lei2019stochastic} assumed a more relaxed condition, the conclusion is comparably weaker, demonstrating proven convergence but with a notably inferior convergence rate. 
In the theoretical analysis presented in section 4 of this paper, we will utilize the aforementioned tools to provide convergence guarantees for our algorithm, with an emphasis on the effect of the choice of SDGD on stochastic gradient variance and PINNs convergence.

\section{Method}\label{sec:method}
\subsection{Physics-Informed Neural Networks (PINNs)}
In this paper, we focus on solving the following partial differential equations (PDEs) defined on a domain $\Omega \subset \mathbb{R}^d$:
\begin{equation}\label{eq:PDE}
\begin{aligned}
\mathcal{L}u(\bx)=R(\bx) \ \text{in}\ \Omega, \qquad
\mathcal{B}u(\bx)=B(\bx) \ \text{on}\ \Gamma,
\end{aligned}
\end{equation}
where $\mathcal{L}$ and $\mathcal{B}$ are the differential operators for the residual in $\Omega$ and for the boundary/initial condition on $\Gamma$.
PINNs \cite{raissi2019physics} is a neural network-based PDE solver via minimizing the following boundary and residual loss functions.
\begin{equation}
\begin{aligned}
\mathcal{L}(\theta) &= \lambda_b \mathcal{L}_b(\theta) + \lambda_r \mathcal{L}_r(\theta)\\
&=\frac{\lambda_b}{n_b}\sum_{i=1}^{n_b} {|\mathcal{B}u_{\theta}(\bx_{b,i})-B(\bx_{b,i})|}^2 + \frac{\lambda_r}{n_r}\sum_{i=1}^{n_r} {|\mathcal{L}u_{\theta}(\bx_{r,i})-R(\bx_{r,i})|}^2.
\end{aligned}
\end{equation}

\subsection{Methodology for High-Dimensional PDEs}
We first adopt the simple high-dimensional second-order Poisson's equation for illustration, then we move to the widely-used high-dimensional Fokker-Planck equation for the general case.
\subsubsection{Introductory Case of the High-Dimensional Poisson's Equation}
We consider a simple high-dimensional second-order Poisson's equation for illustration:
\begin{equation}
\Delta u(\bx) = \sum_{i=1}^d \frac{d^2}{d\bx_i^2}u(\bx) = R(\bx), \bx \in \Omega \subset \mathbb{R}^d,
\end{equation}
where $u$ is the solution, and $u_\theta$ is our PINN model parameterized by $\theta$. The memory scales linearly as $d$ increases. So, for extremely high-dimensional PDEs containing many second-order terms, using one collocation point can lead to insufficient memory.

However, the memory problem is solvable by inspecting the residual loss on the collocation point $\bx$:
\begin{equation}
\ell(\theta) = \frac{1}{2}\left(\sum_{i=1}^d \frac{d^2}{d\bx_i^2}u_\theta(\bx) - R(\bx)\right)^2,
\end{equation}
The gradient with respect to the model parameters $\theta$ for training the PINN is
\begin{equation}
\begin{aligned}
\text{grad}(\theta) := \frac{\partial\ell(\theta)}{\partial \theta} &= \textcolor{blue}{\left(\sum_{i=1}^d \frac{d^2}{d\bx_i^2}u_\theta(\bx) - R(\bx)\right)}\left(\sum_{i=1}^d \textcolor{red}{\frac{\partial}{\partial\theta}\frac{d^2}{d\bx_i^2}u_\theta(\bx)}\right).
\end{aligned}
\end{equation}
We are only differentiating with respect to parameters $\theta$ on the $d$ PDE terms $\textcolor{red}{\frac{d^2}{d\bx_i^2}u_\theta(\bx)}$, which is the memory bottleneck in the backward pass since the shape of the gradient is proportional to both the PDE dimension and the parameter count of the PINN. In contrast, the first part $\textcolor{blue}{\left(\sum_{i=1}^d \frac{d^2}{d\bx_i^2}u_\theta(\bx) - R(\bx)\right)}$ is a scalar, which can be precomputed and detached from the GPU since it is not involved in the backpropagation for $\theta$. Since the full gradient grad$(\theta)$ is the sum of $d$ independent terms, we can sample several terms for stochastic gradient descent (SGD) using the sampled unbiased gradient estimator. Concretely, our algorithm can be summarized as follows:
\begin{enumerate}
\item Choose random indices $I \subset \{1,2,\cdots,d\}$ where $|I|$ is the cardinality of the set $I$, which is the batch size over PDE terms, where we can set $|I| \ll d$ to minimize memory cost. 
\item For $i=1,2,\cdots,d$, compute $\frac{d^2}{d\bx_i^2}u_\theta(\bx)$.
If $i \in I$, then keep the gradient with respect to $\theta$, else we detach it from the GPU to save memory. After detachment, the term will not be involved in the costly backpropagation, and its gradient with respect to $\theta$ will not be computed.
\item Compute the unbiased stochastic gradient used to update the model
\begin{equation}
\text{grad}_I(\theta) = \textcolor{red}{\frac{d}{|I|}}\left(\sum_{i=1}^d \frac{d^2}{d\bx_i^2}u_\theta(\bx) - R(\bx)\right)\left(\textcolor{red}{\sum_{i \in I}} \frac{d^2}{d\bx_i^2}\frac{\partial}{\partial\theta}u_\theta(\bx)\right).
\end{equation}
\item If not converged, go to 1.
\end{enumerate}
Our algorithm enjoys the following good properties and extensions:
\begin{itemize}
\item Low memory cost: Since the main cost is from the backward pass, and we are only backpropagating over terms with $i \in I$, the cost is the same as the corresponding $|I|$-dimensional PDE.
\item Unbiased stochastic gradient: Our gradient is an {unbiased} estimator of the true full batch gradient, i.e., 
\begin{equation}
\mathbb{E}_I \left[\text{grad}_I(\theta)\right] = \text{grad}(\theta),
\end{equation}
so that modern SGD accelerators such as Adam \cite{kingma2014adam} can be adopted.
\item Accumulate gradient for full batch GD: For the full GD that exceeds the memory, we can select non-overlapping index sets $\{I_k\}_{k \in K}$ such that $\cup_k I_k = \{1,2,\cdots,d\}$, then we can combine these minibatch gradients to get the full gradient,
\begin{equation}
\frac{1}{|K|}\sum_{k} \text{grad}_{I_k}(\theta) = \text{grad}(\theta).
\end{equation}
This baseline process is memory-efficient but time-consuming. It is memory efficient since it divides the entire computationally intensive gradient into the sum of several stochastic gradients whose computations are memory-efficient. This operation is conducted one by one using the ``For loop from $i=1$ to $d$". But it is rather time-consuming because the ``For loop" cannot be parallelized.
\end{itemize}
A common practice in PINNs is to sample stochastic collocation points to reduce the batch size of points, which is a common way to reduce memory costs in previous methods and even the entire field of deep learning. Next, we compare SDGD with SGD over collocation points:
\begin{itemize}
\item {SGD on collocation points} has a minimum batch size of 1 point plus the $d$-dimensional equation.
\item {SDGD} can be combined with SGD on points so that its minimal batch size is 1 point plus 1D equation. 
\item Thus, our method can solve high-dimensional PDEs arbitrarily since its minimal computational cost will not grow as the dimension $d$ increases. In contrast, the minimum cost of SGD on points scales linearly as $d$ grows.
\item Empirically, it is interesting to see how the two SGDs affect convergence. In particular, $B$-point + $D$-term with the same $B \times D$ quantity has the same computational and memory cost, e.g., 50-point-100-terms and 500-point-10-terms.
\end{itemize}

\subsubsection{General Case}

We are basically performing the following decomposition:
\begin{equation}
\mathcal{L}u = \sum_{i=1}^{N_{\mathcal{L}}}\mathcal{L}_i u,
\end{equation}
where $\mathcal{L}_i$ are the $N_{\mathcal{L}}$ decomposed terms. We have the following examples:
\begin{itemize}
\item In the $d$-dimensional Poisson cases, $N_{\mathcal{L}} = d$ and $\mathcal{L}_i u = \frac{\partial^2}{\partial \bx_i^2} u(\bx)$. This observation also highlights the significance of decomposition methods for high-dimensional Laplacian operators. The computational bottleneck in many high-dimensional second-order PDEs often lies in the high-dimensional Laplacian operator.
\item Consider the $d$-dimensional Fokkerâ€“Planck (FP) equation, which has numerous real-world applications in optimal control and finance:
\begin{equation}
\frac{\partial u(\boldsymbol{x},t)}{\partial t} = -\sum_{i=1}^{d} \frac{\partial}{\partial \bx_i} \left[ F_i(\boldsymbol{x}) u(\bx,t) \right] + \frac{1}{2} \sum_{i,j=1}^{d} \frac{\partial^2}{\partial \bx_i \partial \bx_j} \left[ D_{ij}(\bx) u(\bx,t) \right],
\end{equation}
where $\bx = (\bx_1, \bx_2, \dots, \bx_n)$ is a $d$-dimensional vector, $u(\bx,t)$ is the probability density function of the stochastic process $\bx(t)$, $F_i(\boldsymbol{x})$ is the $i$-th component of the drift coefficient vector $\boldsymbol{F}(\bx)$, and $D_{ij}(\bx)$ is the $(i,j)$-th component of the diffusion coefficient matrix $\boldsymbol{D}(\bx)$. 

Then, our decomposition over the PDE terms will be
\begin{equation}
\begin{aligned}
\mathcal{L}u &= \frac{\partial u(\boldsymbol{x},t)}{\partial t} +\sum_{i=1}^{d} \frac{\partial}{\partial \bx_i} \left[ F_i(\boldsymbol{x}) u(\bx,t) \right] -\frac{1}{2} \sum_{i,j=1}^{d} \frac{\partial^2}{\partial \bx_i \partial \bx_j} \left[ D_{ij}(\bx) u(\bx,t) \right]\\
&= \sum_{i,j=1}^d  \left\{\frac{1}{d^2}\frac{\partial u(\boldsymbol{x},t)}{\partial t} + \frac{1}{d}\frac{\partial}{\partial \bx_i} \left[ F_i(\boldsymbol{x}) u(\bx,t) \right] - \frac{1}{2} \frac{\partial^2}{\partial \bx_i \partial \bx_j} \left[ D_{ij}(\bx) u(\bx,t) \right]\right\}\\
&= \sum_{i,j=1}^d \mathcal{L}_{ij}u(\bx),
\end{aligned}
\end{equation}
where we denoted 
\begin{equation}
\mathcal{L}_{ij}u(\bx) = \frac{1}{d^2}\frac{\partial u(\boldsymbol{x},t)}{\partial t} + \frac{1}{d}\frac{\partial}{\partial \bx_i} \left[ F_i(\boldsymbol{x}) u(\bx,t) \right] - \frac{1}{2} \frac{\partial^2}{\partial \bx_i \partial \bx_j} \left[ D_{ij}(\bx) u(\bx,t) \right].
\end{equation}
\item The $d$-dimensional bi-harmonic equation:
\begin{equation}
\Delta^2 u(\bx) = 0,\quad \bx \in \Omega \subset \mathbb{R}^d,
\end{equation}
where $\Delta = \sum_{i=1}^{d}\frac{\partial^2}{\partial \bx_i^2}$ is the Laplacian operator. For this high-dimensional fourth-order PDE, we can do the following decomposition for the PDE operator:
\begin{equation}
\mathcal{L}u = \Delta^2 u(\bx) = \sum_{i,j=1}^d \frac{\partial^4}{\partial \bx_i^2\partial \bx_j^2} u(\bx) = \sum_{i,j=1}^d \mathcal{L}_{ij}u(\bx),
\end{equation}
where
\begin{equation}
\mathcal{L}_{ij}u(\bx) = \sum_{i,j=1}^d \frac{\partial^4}{\partial \bx_i^2\partial \bx_j^2} u(\bx).
\end{equation}
%\item Consider the $d$-dimensional parabolic equations that related papers \cite{beck2021deep, han2018solving, raissi2018forward} aim to solve

\item For other high-dimensional PDEs, their complexity arises from the dimensionality itself. Therefore, it is always possible to find decomposition methods that can alleviate this complexity.
\end{itemize}

We go back to the algorithm after introducing these illustrative examples, the computational and memory bottleneck is the residual loss:
\begin{equation}
\ell(\theta) =\frac{1}{2} \left(\mathcal{L}u(\bx) - R(\bx)\right)^2.
\end{equation}
The gradient with respect to the model parameters $\theta$ for training the PINN is
\begin{equation}
\begin{aligned}
\text{grad}(\theta) = \frac{\partial\ell(\theta)}{\partial \theta} &= \textcolor{blue}{\left(\sum_{i=1}^{N_{\mathcal{L}}} \mathcal{L}_iu_\theta(\bx) - R(\bx)\right)}\left(\sum_{i=1}^{N_{\mathcal{L}} }\textcolor{red}{\frac{\partial}{\partial\theta} \mathcal{L}_iu_\theta(\bx)}\right).
\end{aligned}
\end{equation}
Subsequently, we are sampling the red part to reduce memory cost. Concretely, our algorithm can be summarized as follows:
\begin{enumerate}
\item Choose random indices $I \subset \{1,2,\cdots, N_{\mathcal{L}}\}$ where $|I|$ is the cardinality of the set $I$, which is the batch size over PDE terms, where we can set $|I| \ll N_{\mathcal{L}}\}$ to minimize memory cost. 
\item For $i=1,2,\cdots,N_{\mathcal{L}}$, compute $\mathcal{L}_iu_\theta(\bx)$.
If $i \in I$, then keep the gradient with respect to $\theta$, else we detach it from the GPU to save memory. After detachment, the term will not be involved in the costly backpropagation, and its gradient with respect to $\theta$ will not be computed, which saves GPU memory costs.
\item Compute the unbiased stochastic gradient used to update the model
\begin{equation}
\text{grad}_I(\theta) = {\frac{N_{\mathcal{L}}}{|I|}}\left(\sum_{i=1}^{N_\mathcal{L}}\mathcal{L}_iu_\theta(\bx) - R(\bx)\right)\left({\sum_{i \in I}} \frac{\partial}{\partial\theta}\mathcal{L}_iu_\theta(\bx)\right).
\end{equation}
\item If not converged, go to 1.
\end{enumerate}
The algorithm is summarized in \ref{algo:1}.
\begin{algorithm}
\caption{Training Algorithm for scaling-up by sampling the gradient in the backward pass.}
\begin{algorithmic}[1]
\While{NOT CONVERGED}
\State Choose random indices $I \subset \{1,2,\cdots, N_{\mathcal{L}}\}$ where $|I|$ is the cardinality of the set $I$, which is the batch size over PDE terms, where we can set $|I| \ll N_{\mathcal{L}}\}$ to minimize memory cost. 
\For{$i \in \{1,2,\cdots, N_{\mathcal{L}}\}$}
    \State compute $\mathcal{L}_iu_\theta(\bx)$.
If $i \in I$, then keep the gradient with respect to $\theta$, else we detach it from GPU to save memory. After detachment, the term will not be involved in the costly backpropagation, and its gradient with respect to $\theta$ will not be computed.
\EndFor
\State Compute the unbiased stochastic gradient used to update the model
\begin{equation*}
\text{grad}_I(\theta) = {\frac{N_{\mathcal{L}}}{|I|}}\left(\sum_{i=1}^{N_\mathcal{L}}\mathcal{L}_i u_\theta(\bx) - R(\bx)\right)\left({\sum_{i \in I}} \frac{\partial}{\partial\theta}\mathcal{L}_iu_\theta(\bx)\right).
\end{equation*}
%\If{not converged}
%    \State go to 1.
%\EndIf
\EndWhile
\end{algorithmic}
\label{algo:1}
\end{algorithm}



\subsection{Extensions: Gradient Accumulation and Parallel Computing}\label{sec:GAPC}
%So far, we assumed that the main computational bottleneck of PINN is backpropagation. In fact, for extremely large-scale problems, even the forward part can cause an OOM error. For example, in the FP equation, the largest calculation of the forward propagation lies in the Hessian matrix of the model. Further, we shall discuss how our method can be combined with parallel computing to achieve further speed-up and/or scale-up.
This subsection introduces two straightforward extensions of our Algorithm \ref{algo:1} for further speed-up and scale-up.

\textbf{Gradient Accumulation}. Since our method involves SDGD in large-scale problems, we have to reduce the batch size to fit it within the available GPU memory. However, a very small batch size can result in significant gradient variance. In such cases, gradient accumulation is a promising idea. Specifically, gradient accumulation involves sampling different index sets $I_1, I_2, \cdots, I_n$, computing the corresponding gradients $\text{grad}_{I_k}(\theta)$ for $k = 1,2,\cdots,n$, and then averaging them as the final unbiased stochastic gradient for one-step optimization to effectively increase the batch size. The entire process can be implemented on a single GPU, and we only need to be mindful of the tradeoff between computation time and gradient variance.

\textbf{Parallel Computing}. Just as parallel computing can be used in machine learning to increase batch size and accelerate convergence in SGD, our new SDGD also supports parallel computing. Recall that the stochastic gradient by sampling the PDE terms randomly is given by
\begin{equation*}
\text{grad}_I(\theta) = {\frac{N_{\mathcal{L}}}{|I|}}\left(\sum_{i=1}^{N_\mathcal{L}}\mathcal{L}_iu_\theta(\bx) - R(\bx)\right)\left({\sum_{i \in I}} \frac{\partial}{\partial\theta}\mathcal{L}_iu_\theta(\bx)\right).
\end{equation*}
We can compute the above gradient for different index sets $I_1, I_2, \cdots, I_n$ on various machines simultaneously and accumulate them to form a larger-batch stochastic gradient.


\subsection{Further Speed-up via Simultaneous Sampling in Forward and Backward Passes}
In this section, we discuss how to accelerate further large-scale PINN training based on our proposed memory reduction methods to make it both fast and memory efficient.

Although the proposed methods can significantly reduce memory cost and use SDGD to obtain some acceleration through gradient randomness, the speed is still slow for particularly large-scale problems because we need to calculate each term $\{\mathcal{L}_iu(\bx)\}_{i=1}^{N_{\mathcal{L}}}$ in the forward pass one by one, and we are only omitting some these terms in the backward pass to scale up. For example, in extremely high-dimensional cases, we may need to calculate thousands of second-order derivatives of PINN, and the calculation speed is clearly unacceptable.

To overcome this bottleneck, we can perform the same unbiased sampling on the forward pass to accelerate it while ensuring that the entire gradient is unbiased. In other words, we only select some indices for calculation in the forward pass and select another set of indices for the backward pass, combining them into a very cheap unbiased stochastic gradient.

Mathematically, consider the full gradient again:
\begin{equation}
\begin{aligned}
\text{grad}(\theta) = \frac{\partial\ell(\theta)}{\partial \theta} &= \textcolor{blue}{\left(\sum_{i=1}^{N_{\mathcal{L}}} \mathcal{L}_iu_\theta(\bx) - R(\bx)\right)}\left(\sum_{i=1}^{N_{\mathcal{L}} }\textcolor{red}{\frac{\partial}{\partial\theta} \mathcal{L}_iu_\theta(\bx)}\right).
\end{aligned}
\end{equation}
We choose two random and independent indices sets $I, J \subset \{1,2,\cdots, N_{\mathcal{L}}\}$ for the sampling of the backward and the forward passes, respectively. The corresponding stochastic gradient is:
\begin{equation}
\begin{aligned}
\text{grad}_{I,J}(\theta) =\frac{N_{\mathcal{L}}}{|I|}\left(\left(\frac{N_{\mathcal{L}}}{|J|}\sum_{j \in J} \mathcal{L}_iu_\theta(\bx)\right) - R(\bx)\right)\left(\sum_{i\in I}\frac{\partial}{\partial\theta} \mathcal{L}_iu_\theta(\bx)\right).
\end{aligned}
\end{equation}
Since $I, J$ are independent, the gradient estimator is unbiased:
\begin{equation}
\mathbb{E}_{I,J}\text{grad}_{I,J}(\theta) = \text{grad}(\theta).
\end{equation}
Concretely, our algorithm can be summarized as follows:
\begin{enumerate}
\item Choose random indices $I, J \subset \{1,2,\cdots, N_{\mathcal{L}}\}$, where we can set $|I| \ll N_{\mathcal{L}}$ to minimize memory cost and $|J| \ll N_{\mathcal{L}}$ to further speed up. 
\item For $i \in I$, compute $\mathcal{L}_iu_\theta(\bx)$ and keep the gradient with respect to $\theta$.
\item For $j \in J$, compute $\mathcal{L}_ju_\theta(\bx)$ and detach it to save memory cost.
\item Compute the unbiased stochastic gradient used to update the model
\begin{equation}
\begin{aligned}
\text{grad}_{I,J}(\theta) =\frac{N_{\mathcal{L}}}{|I|}\left(\left(\frac{N_{\mathcal{L}}}{|J|}\sum_{j \in J} \mathcal{L}_ju_\theta(\bx)\right) - R(\bx)\right)\left(\sum_{i\in I}\frac{\partial}{\partial\theta} \mathcal{L}_iu_\theta(\bx)\right).
\end{aligned}
\end{equation}
\item If not converged, go to 1.
\end{enumerate}
The algorithm is summarized in Algorithm \ref{algo:2}.

\begin{algorithm}
\caption{Training Algorithm for scaling-up and further speeding-up by sampling both the forward and backward passes.}
\begin{algorithmic}[1]
\While{NOT CONVERGED}
\State Choose random indices $I, J \subset \{1,2,\cdots, N_{\mathcal{L}}\}$, where we can set $|I| \ll N_{\mathcal{L}}$ to minimize memory cost and $|J| \ll N_{\mathcal{L}}$ to further speed up.
\For{$i \in I$}
    \State compute $\mathcal{L}_iu_\theta(\bx)$ and keep the gradient with respect to $\theta$.
\EndFor
\For{$j \in J$}
    \State compute $\mathcal{L}_ju_\theta(\bx)$ and detach it to save memory cost.
\EndFor
\State Compute the unbiased stochastic gradient used to update the model
\begin{equation*}
\begin{aligned}
\text{grad}_{I,J}(\theta) =\frac{N_{\mathcal{L}}}{|I|}\left(\left(\frac{N_{\mathcal{L}}}{|J|}\sum_{j \in J} \mathcal{L}_j u_\theta(\bx)\right) - R(\bx)\right)\left(\sum_{i\in I}\frac{\partial}{\partial\theta} \mathcal{L}_iu_\theta(\bx)\right).
\end{aligned}
\end{equation*}
%\If{not converged}
%    \State go to 1.
%\EndIf
\EndWhile
\end{algorithmic}
\label{algo:2}
\end{algorithm}


\textbf{Trading off Speed with Gradient Variance}.
Obviously, since we have done more sampling, the gradient variance of this method may be larger, and the convergence may be sub-optimal. However, at the initialization stage of the problem, an imprecise gradient is sufficient to make the loss drop significantly, which is the tradeoff between convergence quality and convergence speed. We will demonstrate in experiments that this method is particularly effective for extremely large-scale PINN training.

\subsection{Additional Speed-up via Sampling Only Once}
In the previous section, we introduced Algorithm \ref{algo:2}, which aims to accelerate SDGD by performing random sampling to both forward and backward passes. To ensure unbiased gradients, we independently sample the forward and backward passes. However, this leads to additional computational overhead. If we were to use the same sample for both the forward and backward passes, our computational cost would be approximately half of that in Algorithm \ref{algo:2}, resulting in further acceleration. Nonetheless, the gradients obtained in this manner are biased due to the correlations between the forward and backward pass samples. Nevertheless, in practice, this approach often converges rapidly. We introduce this in Algorithm \ref{algo:3}.

\begin{algorithm}
\caption{Training Algorithm for scaling-up and more speeding-up by sampling both the forward and backward passes only once.}
\begin{algorithmic}[1]
\While{NOT CONVERGED}
\State Choose random indices $I \subset \{1,2,\cdots, N_{\mathcal{L}}\}$, where we can set $|I| \ll N_{\mathcal{L}}$ to minimize memory cost and to further speed up.
\For{$i \in I$}
    \State compute $\mathcal{L}_iu_\theta(\bx)$ and keep the gradient with respect to $\theta$.
\EndFor
\State Compute the unbiased stochastic gradient used to update the model
\begin{equation*}
\begin{aligned}
\text{grad}_{I,I}(\theta) =\frac{N_{\mathcal{L}}}{|I|}\left(\left(\frac{N_{\mathcal{L}}}{|I|}\sum_{i \in I} \mathcal{L}_i u_\theta(\bx)\right) - R(\bx)\right)\left(\sum_{i\in I}\frac{\partial}{\partial\theta} \mathcal{L}_iu_\theta(\bx)\right).
\end{aligned}
\end{equation*}
%\If{not converged}
%    \State go to 1.
%\EndIf
\EndWhile
\end{algorithmic}
\label{algo:3}
\end{algorithm}


The empirical success of Algorithm \ref{algo:3} can be attributed to the \textbf{Bias-Variance Tradeoff}. While Algorithm \ref{algo:3} is indeed biased, its use of the same set of random indices for both forward and backward passes results in lower variance. In contrast, for Algorithm \ref{algo:2}, although it is unbiased, the utilization of different random indices for forward and backward passes increases its variance due to the two sets of random indices involved.

\section{Theory}
In this section, we analyze the convergence of our proposed Algorithms \ref{algo:1} and \ref{algo:2}. 
\subsection{SDGD is Unbiased}
In previous sections, we have shown that the stochastic gradients generated by our proposed SDGD are unbiased:
\begin{theorem}
The stochastic gradients $\text{grad}_I(\theta)$ and $\text{grad}_{I,J}(\theta)$ in our Algorithms \ref{algo:1} and \ref{algo:2}, respectively, parameterized by index sets $I, J$, are an unbiased estimator of the full-batch gradient $\text{grad}(\theta)$ using all PDE terms, i.e., the expected values of these estimators match that of the full-batch gradient, $\mathbb{E}_I[\text{grad}_I(\theta)]=\mathbb{E}_{I,J}[\text{grad}_{I,J}(\theta)] = \text{grad}(\theta)$.
\end{theorem}
\begin{proof}
We prove the theorem in Section \ref{sec:method}.
\end{proof}

\subsection{SDGD Reduces Gradient Variance}
In this subsection, we aim to show that SDGD can be regarded as another form of SGD over PDE terms, serving as a complement to the commonly used SGD over residual points. Specifically, $B$-point + $D$-term where $B, D \in \mathbb{Z}^+$ with the same $B \times D$ quantity has the same computational and memory cost, e.g., 50-point-100-terms and 500-point-10-terms. In particular, we shall demonstrate that properly choosing the batch sizes of residual points $B$ and PDE terms $D$ under the constant memory cost ($B \times D$) can lead to reduced stochastic gradient variance and accelerated convergence compared to previous practices that use SGD over points only.
 

We assume that the total full batch is with $N_r$ residual points $\{\bx_i\}_{i=1}^{N_r}$ and $N_{\mathcal{L}}$ PDE terms $\mathcal{L} = \sum_{i=1}^{N_{\mathcal{L}}}\mathcal{L}_i$, then the loss function for PINN optimization is
\begin{equation}
\frac{1}{2N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)^2,
\end{equation}
where we normalize over both the number of residual points and the number of PDE terms, which does not impact the directions of the gradients. The full batch gradient is:
\begin{equation}
\begin{aligned}
g(\theta) &:= \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\frac{\partial}{\partial \theta}\mathcal{L}u_\theta(\bx_i)\\
&= \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\sum_{j=1}^{N_{\mathcal{L}}}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right).
\end{aligned}
\end{equation}
If the random index sets $J \subset \{1,2,\cdots,N_{\mathcal{L}}\}$ and $B\subset \{1,2,\cdots,N_r\}$ are chosen, then the SGD gradient with $|B|$ residual points and $|J|$ PDE terms using these index sets is
\begin{equation}
g_{B, J}(\theta) = \frac{1}{|B||J|N_{\mathcal{L}}}\sum_{i \in B}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\sum_{j \in J}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right),
\end{equation}
where $|\cdot|$ computes the cardinality of a set. It is straightforward to show that $\mathbb{E}_{B,J}[g_{B,J}(\theta)] = g(\theta)$.
\begin{theorem}\label{thm:variance_1}
For the random index sets $(B, J)$ where $J\subset \{1,2,\cdots,N_{\mathcal{L}}\}$ is that for indices of PDE terms and $B\subset \{1,2,\cdots,N_r\}$ is that for indices of residual points, then
\begin{equation}
\mathbb{V}_{B,J}[g_{B,J}(\theta)] = \frac{C_1|J| + C_2|B| + C_3}{|B||J|},
\end{equation}
where $\mathbb{V}$ computes the variance of a random variable, $C_1, C_2, C_3$ are constants independent of $B,J$.
\end{theorem}
\begin{proof}
The theorem is proved in Appendix \ref{appendix:variance_1}
\end{proof}
Intuitively, the variance of the stochastic gradient tends to decrease as the batch sizes ($|B|, |J|$) increase, and it converges to zero for $|B|, |J| \rightarrow \infty$.

This verifies that SDGD can be regarded as another form of SGD over PDE terms, serving as a complement to the commonly used SGD over residual points. $|B|$-point + $|J|$-term with the same $|B| \times |J|$ quantity has the same computational and memory cost, e.g., 50-point-100-terms and 500-point-10-terms. According to Theorem \ref{thm:variance_1}, under the same memory budget, i.e., $|B| \times |J|$ is fixed, then there exists a particular choice of batch sizes $|B|$ and $|J|$ that minimizes the gradient variance, in turn accelerating and stabilizing convergence. This is because the stochastic gradient variance $\mathbb{V}_{B,J}[g_{B,J}(\theta)]$ is a function of finite batch sizes $|B|$ and $|J|$, which therefore can achieve its minimum value at a certain choice of batch sizes.

Let us take the extreme cases as illustrative examples. The first extreme case is when the PDE terms have no variance, meaning that the terms $\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)$ are identical for all $j$ after we fix $i$. In this case, if a memory budget of, for example, 100 units is given, the optimal choice would be to select 100 points and one PDE term with the minimum variance. Choosing more PDE terms would not decrease the gradient variance and would be less effective than using the entire memory budget for sampling points. Conversely, if the points have no variance in terms of the gradient they induce, the optimal choice would be one point and 100 PDE terms. In practice, the selection of residual points and PDE terms will inevitably introduce some variance. Therefore, in the case of a fixed memory cost, the choice of batch size for PDE terms and residual points involves a tradeoff. Increasing the number of PDE terms reduces the variance contributed by the PDE terms but also decreases the number of residual points, thereby increasing the variance of the points. Conversely, decreasing the number of PDE terms reduces the variance from the points but increases the variance from the PDE terms. Thus, there exists an optimal selection strategy to minimize the overall gradient variance since the choices of batch sizes are finite.

\subsection{Gradient Variance Bound and Convergence of SDGD}
To establish the convergence of unbiased stochastic gradient descent, we require either Lipschitz continuity of the gradients \cite{lei2019stochastic} or bounded variances \cite{fehrman2020convergence,mertikopoulos2020almost}, with the latter leading to faster convergence. To prove this property, we need to take the following steps. Firstly, we define the neural network serving as the surrogate model in the PINN framework.
\begin{definition}\label{def:DNN}
(Neural Network). A deep neural network (DNN) $u_\theta:\bx=(\bx_{1},\dots,\bx_d)\in\mathbb{R}^d\longmapsto u_\theta(\bx) \in \mathbb{R}$,
parameterized by $\theta$ of depth $L$ is the composition of $L$ linear functions with element-wise non-linearity $\sigma$, is expressed as below:
\begin{equation}\label{eq:DNN}
    u_\theta(\bx)=W_L \sigma (W_{L-1} \sigma(\cdots \sigma(W_1\bx)\cdots ),
\end{equation}
where $\bx\in\mathbb{R}^d$ is the input, and $W_l\in\mathbb{R}^{m_l \times m_{l-1}}$ is the weight matrix at $l$-th layer with $d=m_0$ and $m_L=1$. The parameter vector $\theta$ is the vectorization of the collection of all parameters. We denote $h$ as the maximal width of the neural network, i.e., $ h = \max(m_L, \cdots, m_0)$.
\end{definition}
For the nonlinear activation function $\sigma$, the residual ground truth $R(\bx)$, we assume the following:
\begin{assumption}
We assume that the activation function is smooth and $|\sigma^{(n)}(x)| \leq 1$ and $\sigma^{(n)}$ is 1-Lipschitz, e.g., the sine and cosine activations. We assume that $|R(\bx)| \leq R$ for all $\bx$.
\end{assumption}
\begin{remark}
The sine and cosine functions naturally satisfy the aforementioned conditions. As for the hyperbolic tangent (tanh) activation function, when the order $n$ of the PDE is determined, there exists a constant $C_n$ such that the activation function $C_n \tanh(x)$ satisfies the given assumptions. This constant can be absorbed into the weights of the neural network. This is because both the tanh function and its derivatives up to order $n$ are bounded.
\end{remark}
Recall that the full batch gradient is
\begin{equation}
\begin{aligned}
g(\theta) &= \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\sum_{j=1}^{N_{\mathcal{L}}}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right).
\end{aligned}
\end{equation}
The stochastic gradient produced by Algorithm \ref{algo:1} is
\begin{equation}
g_{B, J}(\theta) = \frac{1}{|B||J|N_{\mathcal{L}}}\sum_{i \in B}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\sum_{j \in J}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right),
\end{equation}
The stochastic gradient produced by Algorithm \ref{algo:2} is
\begin{equation}
g_{B, J, K}(\theta) = \frac{1}{|B||J|N_{\mathcal{L}}}\sum_{i \in B}\left(\frac{N_{\mathcal{L}}}{|K|}\left(\sum_{k \in K}\mathcal{L}_ku_\theta(\bx_i)\right) - R(\bx_i)\right)\left(\sum_{j \in J}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right),
\end{equation}
where $B$ is the index set sampling over the residual points, while $J$ and $K$ sample the PDE terms in the forward pass and the backward pass, respectively.

\begin{lemma}\label{lemma:variance}
(Bounding the Gradient Variance) Suppose that we consider the neural network function class:
\begin{equation}
\mathcal{NN}^L_{M} := \left\{\bx \mapsto W_L \sigma (W_{L-1} \sigma(\cdots \sigma(W_1\boldsymbol{x})\cdots )\ |\ \Vert W_l \Vert_{2} \leq M(l)\right\}.
\end{equation}
If the PDE under consideration has at most $n$th order derivative, then the gradient variance can be bounded by 
\begin{equation}
\begin{aligned}
\mathbb{V}_{B,J}(g_{B, J}(\theta)) &\leq \frac{2}{|B||J|N_{\mathcal{L}}}\left(n!(L-1)^n M(L) \prod_{l=1}^{L-1}  M(l)^n + R\right)^2\cdot\\
&\quad\left((n+1)!(L-1)^{n+1} M(L) \prod_{l=1}^{L-1}  M(l)^{n+1} \max_{\bx \in \Omega}\Vert\bx\Vert\right)^2,\\
\mathbb{V}_{B,J,K}(g_{B, J, K}(\theta)) &\leq \frac{2}{|B||J||K|}\left(n!(L-1)^n M(L) \prod_{l=1}^{L-1}  M(l)^n + R\right)^2\cdot\\
&\quad\left((n+1)!(L-1)^{n+1} M(L) \prod_{l=1}^{L-1}  M(l)^{n+1} \max_{\bx \in \Omega}\Vert\bx\Vert\right)^2,
\end{aligned}
\end{equation}
\end{lemma}
\begin{proof}
The proof of this lemma is provided in Appendix \ref{appendix:lemma}
\end{proof}
\begin{remark}
Intuitively, given the order of the PDE and the number of layers in the neural network, it is observed that both of them are finite. Consequently, the absolute values of the higher-order derivatives of the network are necessarily bounded by a quantity that is related to the norms of its weight matrices.
\end{remark}
Next, we define the SGD trajectory, and we will demonstrate the convergence of the SGD trajectory based on the stochastic gradients provided by our Algorithms \ref{algo:1} and \ref{algo:2}.

\begin{definition}
(SGD Trajectory) Given step sizes (learning rates) $\{\gamma_n\}_{n=1}^\infty$, and the initialized PINN parameters $\theta^1$, then the update rules of SGD using Algorithms \ref{algo:1} and \ref{algo:2} are
\begin{equation}\label{eq:SGD1}
\theta^{n+1} = \theta^{n} - \gamma^ng_{B,J}(\theta^{n}),
\end{equation}
\begin{equation}\label{eq:SGD2}
\theta^{n+1} = \theta^{n} - \gamma^ng_{B,J, K}(\theta^{n}),
\end{equation}
respectively.
\end{definition}

The following theorem demonstrates the convergence rate of SDGD in PINN training.

\begin{theorem}
(Convergence of SDGD under Bounded Gradient Variance) Fix some tolerance level $\delta >0$, suppose that the SGD trajectory given in equations (\ref{eq:SGD1}, \ref{eq:SGD2}) are bounded, i.e., $\Vert W_l^n \Vert\leq M(l)$ for all $n$ where the collection of all $\{W_l^n\}_{l=1}^L$ is $\theta^n$, and that the minimizer of the PINN loss is $\theta^*$, and that the SGD step size follows the form $\gamma_n = \gamma / (n + m)^p$ for some $p \in (1/2,1]$ and large enough $\gamma, m > 0$, then
\begin{enumerate}
\item There exist neighborhoods $\mathcal{U}$ and $\mathcal{U}_1$ of $\theta^*$ such that, if $\theta^1 \in \mathcal{U}_1$, the event
\begin{equation}
E_\infty = \left\{\theta^n \in \mathcal{U} \ \text{for all } n \in \mathbb{N}\right\}
\end{equation}
occurs with probability at least $1 - \delta$, i.e., $\mathbb{P}(E_\infty | \theta^1 \in\mathcal{U}_1) \geq 1 - \delta$.
\item Conditioned on $E_\infty$, we have
\begin{equation}
\mathbb{E}[\Vert\theta^n - \theta^*\Vert^2|E_\infty] \leq \mathcal{O}(1/n^p).
\end{equation}
\end{enumerate}
\end{theorem}
\begin{remark}
The convergence rate of stochastic gradient descent is $\mathcal{O}(1/n^p)$, where $\mathcal{O}$ represents the constant involved, including the variance of the stochastic gradient. A larger variance of the stochastic gradient leads to slower convergence, while a smaller variance leads to faster convergence. The selection of the learning rate parameters $\gamma$ and $m$ depends on the specific optimization problem itself. For example, in a PINN problem, suitable values of $\gamma$ and $m$ ensure that the initial learning rate is around 1e-4, which is similar to the common practice for selecting learning rates.
\end{remark}
\begin{remark}
In our convergence theorem, we have not made any unrealistic assumptions; all we require is the boundedness of the optimization trajectory of the PINN. Empirically, as long as the learning rate and initialization are properly chosen, PINN always converges, and the values of its weight matrices do not diverge to infinity. In the literature, many convergence theorems for SGD require various conditions to be satisfied by the loss function of the optimization problem itself, such as Lipschitz continuity. However, in our case, none of these conditions are required.
\end{remark}


\section{Experiments}
In this section, we conduct extensive experiments to demonstrate the stable and fast convergence of SDGD over several nonlinear PDEs. In particular, 
we compare our PINN-based method with other non-PINN mainstream approaches for solving high-dimensional PDEs \cite{beck2021deep, han2018solving, raissi2018forward}, especially in terms of accuracy. We also demonstrate SDGD's convergence in 100,000-dimensional nonlinear PDEs with complicated exact solutions.
All the experiments in the main text are done on an NVIDIA A100 GPU with 80GB memory.
We present additional experiments to compare SDGD with full batch gradient descent comprehensively in the Appendices.

\subsection{SDGD Can Deal with Inseparable and Effectively High-Dimensional PDE Solutions}

Herein, we test the ability of SDGD  to deal with linear/nonlinear PDEs with nonlinear, inseparable, and effectively high-dimensional PDE exact solutions in PINN's residual loss. Specifically, we consider the following exact solution:
\begin{equation}
u_{\text{exact}}(\bx) = \left(1 - \Vert \bx \Vert_2^2\right)\left(\sum_{i=1}^{d-1}  c_i \sin(\bx_i +\cos(\bx_{i+1})+\bx_{i+1}\cos(\bx_i))\right),
\end{equation}
where $c_i \sim \mathcal{N}(0, 1)$. Here the term $\left(1 - \Vert \bx \Vert_2^2\right)$ is for a zero boundary condition on the unit sphere and for preventing the boundary from leaking the information of the solution, as we want to test SDGD's ability to fit the residual where sampling over dimensions is employed. Furthermore, the following PDEs are considered in the unit ball $\mathbb{B}^d$, all of which are associated with a zero boundary condition on the unit sphere:
\begin{itemize}
\item Poisson equation
\begin{equation}
\Delta u(\bx) = g(\bx), \quad \bx\in \mathbb{B}^d,
\end{equation}
where $g(\bx) = \Delta u_{\text{exact}}(\bx)$.
\item Allen-Cahn equation
\begin{equation}
\Delta u(\bx) + u(\bx) - u(\bx)^3 = g(\bx), \quad \bx\in \mathbb{B}^d,
\end{equation}
where $g(\bx) = \Delta u_{\text{exact}}(\bx) + u_{\text{exact}}(\bx) - u_{\text{exact}}(\bx)^3$.
\item Sine-Gordon equation
\begin{equation}
\Delta u(\bx) + \sin\left(u(\bx) \right) = g(\bx), \quad \bx \in \mathbb{B}^d,
\end{equation}
where $g(\bx) = \Delta u_{\text{exact}}(\bx) + \sin\left(u_{\text{exact}}(\bx) \right)$.
\end{itemize}
We would like to emphasize that the PDEs and the exact PDE solutions used here are highly nontrivial and challenging.
\begin{itemize}
\item Nonlinear, inseparable, and effectively high-dimensional PDE exact solution: 
This PDE cannot be reduced to a lower-dimensional problem, and the exact solution of the PDE cannot be decomposed into lower-dimensional functions. It is worth noting that in the exact solution, all pairs of variables, both $\bx_i$ and $\bx_{i+1}$, exhibit pairwise interactions and are highly coupled.
\item Anisotropic and nontrivial PDE exact solution: In addition, due to the random coefficients $c_i \sim \mathcal{N}(0, 1)$ along different dimensions, the exact solution is anisotropic, which poses an additional challenge to SDGD. The exact solution is also highly nontrivial; its value's standard deviation is large in high-dimensional spaces.
\item PDEs with different levels of nonlinearities: We test both linear and nonlinear PDEs to verify SDGD's robustness.
\item Zero boundary condition to test SDGD on the residual part: In a $d$-dimensional problem, the boundary is typically $(d-1)$-dimensional. Therefore, if the boundary conditions are nontrivial, the boundary loss will leak information at a rate of $(d-1)/d$. To assess the impact of sampling the dimension in the residual loss in SDGD, we maintain a zero boundary condition to eliminate its interference. This also makes the PDE more challenging, as we have no knowledge of the exact solution from the boundary.
\end{itemize}

The training details are as follows. The model is a 4-layer multi-layer perceptron network with 128 hidden units, which is trained via Adam \cite{kingma2014adam} for 10K epochs, with an initial learning rate 1e-3, which linearly decays to zero at the end of the optimization. We select 100 random residual points at each Adam epoch and 20K fixed testing points uniformly from the unit ball. We utilize Algorithm \ref{algo:3} with a minibatch of 100 dimensions to randomly sample the second-order derivatives along various dimensions in the Laplacian operator in all the PDEs. We adopt the following model structure to automatically satisfy the zero boundary condition, which helps the model avoid boundary loss and avoid sampling the boundary points \cite{lu2021physics}:
\begin{equation}
u^{\text{SDGD}}_\theta(\bx) = (1 - \Vert\bx\Vert_2^2) u_\theta(\bx),
\end{equation}
where $u_\theta(\bx)$ is the neural network and $u^{\text{SDGD}}_\theta(\bx)$ is the boundary-augmented model. The hard constraint technique for PINN is popular to avoid the additional boundary loss \cite{lu2021physics}. We repeat our experiment 5 times with 5 independent random seeds.

\begin{table}[htbp]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
PDE & Metric & 100 & 1,000 & 10,000 & 100,000 \\ \hline
Poisson & Rel. $L_2$ Error & 7.189E-3 & 5.611E-4 & 1.850E-3 & 2.175E-3 \\ \hline
Allen-Cahn & Rel. $L_2$ Error & 7.187E-3 & 5.615E-4 & 1.864E-3 & 2.178E-3 \\ \hline
Sine-Gordon & Rel. $L_2$ Error & 7.192E-3 & 5.641E-4 & 1.854E-3 & 2.177E-3 \\ \hline
 & Time (Hour) & 0.05 & 0.75 & 1.5 & 12 \\ \hline
 & Memory (MB) & 1328 & 1788 & 4527 & 32777 \\ \hline
\end{tabular}
\caption{Relative $L_2$ error, time, and memory costs, for SDGD across different PDEs under various high dimensions.}
\label{tab:1}
\end{table}

% Figure environment removed

The test relative $L_2$ errors and the convergence time are shown in Table \ref{tab:1}, while the convergence curve of SDGD is shown in Figure \ref{fig:1}. The main observations from the results are as follows.
\begin{itemize}
\item Stable performances: 
SDGD demonstrates remarkable stability across various dimensions and different nonlinear PDEs, with relative $L_2$ errors testing at approximately 1e-3. 
Furthermore, given that we have provided the same exact solution for each PDE and have kept the training and testing point samples fixed by using the same random numbers, the convergence curves of SDGD for different PDEs are remarkably similar. This is especially noticeable for Poisson and Allen-Cahn, where the convergence curves exhibit minimal distinctions. This highlights the stability of SDGD in the face of PDE nonlinearity.
\item Linear computational cost: Furthermore, SDGD's memory and optimization time costs exhibit linear growth with respect to dimensionality. For instance, when transitioning from 10K dimensions to 100K dimensions, both memory and time costs increase by less than a factor of 10.
\item SDGD scales up PINNs to extremely high dimensions: In the case of 100K dimensions, we opted for 100 dimensions for SDGD. If we were to employ the conventional full-batch gradient descent across all dimensions, even with just one residual point, the memory cost would surpass SDGD's current setting by a factor of 10. This greatly exceeds the 80GB memory limit of an A100 GPU. Thus, SDGD allows for the scalability of PINN to handle large-scale PDE problems.
\item Smoother convergence curve in higher dimensions: We observe that the convergence curves of SDGD are smoother in high-dimensional scenarios, which could be attributed to the tendency of high-dimensional functions to be smoother thanks to the blessing of dimensionality.
\end{itemize}
In sum, SDGD with PINN can tackle the CoD in solving nonlinear high-dimensional PDEs with inseparable and complicated solutions.

\begin{table}[htbp]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Method & PDE  & Dim=$10^1$       & Dim=$10^2$   & Dim=$10^3$   & Dim=$10^4$   \\ \hline
FBSNN \cite{raissi2018forward} & Allen-Cahn & 1.339E-3	& 2.184E-3	&7.029E-3&	5.392E-2
 \\ \hline
DeepBSDE \cite{han2018solving} & Allen-Cahn & 4.581E-3	&2.516E-3&	2.980E-3	&2.975E-3
 \\ \hline
DeepSplitting \cite{beck2021deep} & Allen-Cahn & 3.642E-3	&1.563E-3	&2.369E-3&	2.962E-3
 \\ \hline
SDGD (Ours) & Allen-Cahn & \textbf{7.815E-4}&	\textbf{3.142E-4}&\textbf{7.042E-4}&	\textbf{2.477E-4} \\ \toprule\hline
Method & PDE  & Dim=$10^1$       & Dim=$10^2$   & Dim=$10^3$   & Dim=$10^4$   \\ \hline
FBSNN \cite{raissi2018forward} & Semilinear Heat & 3.872E-3	&8.323E-4	&7.705E-3	&1.426E-2
 \\ \hline
DeepBSDE \cite{han2018solving} & Semilinear Heat & 3.024E-3&	3.222E-3&	3.510E-3&	4.279E-3
 \\ \hline
DeepSplitting \cite{beck2021deep} & Semilinear Heat &  2.823E-3	&3.432E-3&	3.439E-3	&3.525E-3
 \\ \hline
SDGD (Ours) & Semilinear Heat & \textbf{1.052E-3}&	\textbf{5.263E-4}&	\textbf{6.910E-4}&	\textbf{1.598E-3}
  \\\hline
\toprule\hline
Method & PDE  & Dim=$10^1$       & Dim=$10^2$   & Dim=$10^3$   & Dim=$10^4$   \\ \hline
FBSNN \cite{raissi2018forward} & Sine-Gordon & 2.641E-3	&4.283E-3&2.343E-1&5.185E-1
 \\ \hline
DeepBSDE \cite{han2018solving} & Sine-Gordon & 3.217E-3&2.752E-3&2.181E-3&2.630E-3
 \\ \hline
DeepSplitting \cite{beck2021deep} & Sine-Gordon &  3.297E-3	&2.674E-3&	2.170E-3	&2.249E-3
\\ \hline
SDGD (Ours) & Sine-Gordon & \textbf{2.265E-3}	&\textbf{2.310E-3}&\textbf{1.674E-3}&\textbf{1.956E-3}
  \\\hline
\end{tabular}
\caption{Results of various methods on the Allen-Cahn, semilinear heat, and sine-Gordon PDEs with different dimensions. The evaluation metric is all relative $L_1$ error on one test point. SDGD (Ours) is the state-of-the-art (SOTA) method.}
\label{tab:nonlinear_more}
\end{table}

\subsection{Nonlinear Fokker-Planck PDEs and Comparison with Strong Baselines}
%\cite{beck2021deep, han2018solving, raissi2018forward}}
Next, we present further results comparing our method based on PINN and other strong non-PINN baselines \cite{beck2021deep, han2018solving, raissi2018forward} for several nonlinear PDEs without analytical solutions.
Thus, we adopt other methods' settings and evaluate the model on one test point, where we show that SDGD-PINNs are state-of-the-art (SOTA) models that still outperform other competitors.
Concretely, the following nonlinear PDEs are considered:
\begin{itemize}
\item Allen-Cahn equation.
\begin{equation}
\partial_t u(\bx,t) = \Delta u(\bx,t) + u(\bx,t) - u(\bx,t)^3, \quad (\bx,t) \in \mathbb{R}^d \times [0, 0.3],
\end{equation}
with the initial condition $u(\bx, t=0) = \arctan(\max_i \bx_i)$. We aim to approximate the solution's true value on the one test point $(\bx, t) = (0,\cdots,0,0.3)$.
\item Semilinear heat equation.
\begin{equation}
\partial_t u(\bx,t) = \Delta u(\bx,t) + \frac{1-u(\bx,t)^2}{1+u(\bx,t)^2}, \quad (\bx,t) \in \mathbb{R}^d \times [0, 0.3],
\end{equation}
with the initial condition $u(\bx, t=0) = 5 / (10 + 2\Vert\bx\Vert^2)$. We aim to approximate the solution's true value on the one test point $(\bx, t) = (0,\cdots,0,0.3)$.
\item Sine-Gordon equation.
\begin{equation}
\partial_t u(\bx,t) = \Delta u(\bx,t) + \sin\left(u(\bx,t) \right), \quad (\bx,t) \in \mathbb{R}^d \times [0, 0.3],
\end{equation}
with the initial condition $u(\bx, t=0) = 5 / (10 + 2\Vert\bx\Vert^2)$. We aim to approximate the solution's true value on the one test point $(\bx, t) = (0,\cdots,0,0.3)$.
\end{itemize}
The reference values of these PDEs on the test point are computed by the multilevel Picard method \cite{becker2020numerical, hutzenthaler2020overcoming} with sufficient theoretical accuracy. The exact solutions of these nonlinear PDEs are not solvable analytically and are highly nontrivial and nonseparable. The residual points of PINN are chosen along the trajectories of the stochastic processes that those PDEs correspond to:
\begin{equation}
t \sim \text{Unif}[0, 0.3], \bx \sim \mathcal{N}(0, (0.3-t) \cdot \boldsymbol{I}_{d\times d}).
\end{equation}
More training details are as follows. The model is a 4-layer multi-layer perceptron network with 1024 hidden units, which is trained via Adam \cite{kingma2014adam} for 10K epochs, with an initial learning rate 1e-3 which linearly exponentially with exponent 0.9995. We discretize the time into a stepsize of 0.015 and select boundary and residual points from 100 random SDE trajectories at each Adam epoch. We assign unity weight to the residual loss and 20 weight to the boundary loss, where in the latter we fit the terminal condition and its first-order derivative with respect to $\bx$. We utilize Algorithm \ref{algo:2} with a minibatch of 10 dimensions to randomly sample the second-order derivatives along various dimensions in the Laplacian operator in all the PDEs. We repeat our experiment 5 times with 5 independent random seeds.

The results for these three nonlinear PDEs are shown in Table \ref{tab:nonlinear_more}. PINN surpasses other methods in all dimensions and for all types of PDEs. Whether it is predicting the entire domain or the value at a single point, PINN is capable of handling both scenarios. This is attributed to the mesh-free nature of PINN and its powerful interpolation capabilities.




\subsection{SDGD Accelerates PINN's Adversarial Training in HJB PDEs}

In Wang et al. \cite{wang20222}, it was demonstrated that the class of Hamilton-Jacobi-Bellman (HJB) equation \cite{han2018solving,wang20222} could only be effectively solved using adversarial training, which approximates the $L^\infty$ loss. Specifically, they consider the HJB equation with linear-quadratic-Gaussian (LQG) control:
\begin{equation}
\begin{aligned}
&\partial_t u(\bx, t) + \Delta u(\bx, t) - \Vert \nabla_{\bx} u(\bx,t) \Vert^2 = 0, \quad \bx \in \mathbb{R}^d, t\in[0,1]\\
&u(\bx,T)=g(\bx),
\end{aligned}
\end{equation} 
where $g(\bx)$ is the terminal condition/cost to be chosen. We can use the exact solution for testing and benchmarking PINNs' performances:
\begin{equation}
u(\bx,t) = -\log\left(\int_{\mathbb{R}^d}(2\pi)^{-d/2}\exp(-\Vert \boldsymbol{y} \Vert^2/2)\exp(- g(\bx - \sqrt{2(1-t)}\boldsymbol{y}))d\boldsymbol{y}\right).
\end{equation}
We choose the following cost functions 
\begin{itemize}
\item Logarithm cost:
\begin{equation}
g(\bx) = \log\left(\frac{1+\Vert \bx \Vert^2}{2}\right)
\end{equation}
following many previous papers \cite{han2018solving,he2023learning,raissi2018forward, wang20222} to obtain the \textbf{HJB-Log} case. We demonstrate that SDGD is the state-of-the-art (SOTA)
high-dimensional PDE solver, where SDGD outperforms these previous methods on this classical benchmark PDE, especially the closely related PINN-based approach in \cite{he2023learning,wang20222}.
\item Rosenbrock cost:
\begin{equation}
g(\bx) = \log\left(\frac{1+\sum_{i=1}^{d-1}\left[c_{1,i}( \bx_{i} - \bx_{i+1})^2 + c_{2,i}\bx_{i+1}^2\right]}{2}\right),
\end{equation}
where $c_{1,i},c_{2,i} \sim \text{Unif}[0.5, 1.5]$. This nonconvex cost function will lead to a PDE exact solution that is anisotropic and asymmetric along dimensions, highly coupled and inseparable. Notable, in the exact solution, all pairs of variables, both $\bx_i$ and $\bx_{i+1}$, exhibit pairwise interactions and are highly coupled. The corresponding HJB PDE has an effective dimension of $d + 1$, i.e., it can not be reduced to low-dimensional subproblems. This case is called the \textbf{HJB-Rosenbrock} case.
\end{itemize}

The solution's integration in HJB PDE cannot be analytically solved, necessitating Monte Carlo integration for approximation. We use the relative $L_2$ error approximating $u$ as the evaluation metric for this equation.

In this example of the HJB equation with LQG control, we decompose the residual prediction of PINN along each dimension as follows:
\begin{equation}
\partial_t u(\bx, t) + \Delta u(\bx, t) - \mu \Vert \nabla_{\bx} u(\bx,t) \Vert^2 = \frac{1}{d}\sum_{i=1}^d \left\{\partial_t u(\bx, t)+ d\frac{\partial^2u}{\partial \bx_i^2}-\mu \Vert \nabla_{\bx} u(\bx,t) \Vert^2 \right\}.
\end{equation}

Despite its ability to maintain a low maximal memory cost during training, adversarial training approximating the $L^\infty$ loss is widely recognized for its slow and inefficient nature \cite{shafahi2019adversarial}, which poses challenges in applying it to high-dimensional HJB equations with LQG control. Adversarial training involves optimizing two loss functions: one for the PINN parameters, minimizing the loss through gradient descent, and another for the residual point coordinates, maximizing the PINN loss to approximate the $L^\infty$ loss. This adversarial minimax process, known as adversarial training, is computationally demanding, often requiring multiple rounds of optimization before the resulting residual points can effectively optimize the PINN parameters.

The current state-of-the-art (SOTA) research by Wang et al. \cite{wang20222} and He et al. \cite{he2023learning} has successfully scaled adversarial training to 250 dimensions. In this study, we use SDGD to enhance the scalability and efficiency of adversarial training of PINN in high-dimensional HJB equations.

The implementation details are given as follows. We use a four-layer PINN with 1024 hidden units, trained by an Adam optimizer \cite{kingma2014adam} with a learning
rate = 1e-3 at the beginning and decay linearly to zero. We modified the PINN structure
to make the terminal condition automatically satisfied \cite{lu2021physics}:
\begin{equation}
{u}^{\text{HJB-LQG}}_\theta(\bx, t) = u_\theta(\bx, t) (1-t)+ g(\bx),
\end{equation}
where $g(\bx)$ is the terminal condition.  Thus, we only need to focus on the residual loss.
We conduct training for a total of 10,000 epochs. In all dimensions, we employ Algorithm \ref{algo:3} with 100 batches of PDE terms (dimensions) in the loss function for gradient descent and Algorithm \ref{algo:3} with 10 batches of PDE terms (dimensions) in the loss function for adversarial training. For both our methods and the baseline methods proposed by Wang et al. \cite{wang20222}, we randomly sample 100 residual points per epoch, and 20K test points, both sampling from the distribution $t \sim \text{Unif[0,1]}, \bx \sim \mathcal{N}(0, \boldsymbol{I}_{d\times d})$. For generating the reference value for testing, we conduct a Monte Carlo simulation with 1e5 samples to estimate the integral in the exact solution. We repeat our experiment 5 times with 5 independent random seeds.

The results of adversarial training on the two HJB equations are presented in Table \ref{tab:adv}. For instance, in the 250D HJB-LQG case, achieving a relative $L_2$ error of 1e-2 using \cite{wang20222} requires a training time of 1 day. In contrast, our approach achieves a superior $L_2$ error in just 3.5 hours. In the 1000D case, full batch GD's adversarial training is exceedingly slow, demanding over 5 days of training time. However, our method effectively mitigates this issue and attains a relative $L_2$ error of 5.672E-3 in 217 minutes. Moreover, our approach enables adversarial training even in higher-dimensional HJB equations, specifically 10,000 and 100,000 dimensions, with resulting relative $L_2$ errors of 5.026E-2 and 3.838E-2, respectively. For the HJB-Rosenbrock case, our method is also consistently better than others, which demonstrates SDGD's strong capability to deal with a complicated solution that is inseparable, effectively high-dimensional, non-convex, and highly coupled.


\begin{table}[htbp]
\centering
\begin{tabular}{|c|cc|cc|cc|}
\hline
\multicolumn{7}{|c|}{HJB-Log Results for $u$}\\\hline
        & \multicolumn{2}{c|}{Wang et al. \cite{wang20222}}                            & \multicolumn{2}{c|}{He et al. \cite{he2023learning}}                              & \multicolumn{2}{c|}{SDGD (Ours)}                           \\ \hline
Dim     & \multicolumn{1}{c|}{Time}                & Rel. $L_2$ Error & \multicolumn{1}{c|}{Time}                & Rel. $L_2$ Error & \multicolumn{1}{c|}{Time}        & Rel. $L_2$ Error \\ \hline
250     & \multicolumn{1}{c|}{38 hours}            & 1.182E-2         & \multicolumn{1}{c|}{12 hours}            & 1.370E-2         & \multicolumn{1}{c|}{210 minutes} & 6.147E-3         \\ \hline
1,000   & \multicolumn{1}{c|}{\textgreater 5 days} & N.A.             & \multicolumn{1}{c|}{\textgreater 5 days} & N.A.             & \multicolumn{1}{c|}{217 minutes} & 5.852E-3         \\ \hline
10,000  & \multicolumn{1}{c|}{OOM}                 & N.A.             & \multicolumn{1}{c|}{OOM}                 & N.A.             & \multicolumn{1}{c|}{481 minutes} & 4.926E-2         \\ \hline
100,000 & \multicolumn{1}{c|}{OOM}                 & N.A.             & \multicolumn{1}{c|}{OOM}                 & N.A.             & \multicolumn{1}{c|}{855 minutes} & 4.852E-2         \\ \hline
\multicolumn{7}{|c|}{HJB-Rosenbrock Results for $u$}\\\hline
        & \multicolumn{2}{c|}{Wang et al. \cite{wang20222}}                            & \multicolumn{2}{c|}{He et al. \cite{he2023learning}}                              & \multicolumn{2}{c|}{SDGD (Ours)}                           \\ \hline
Dim     & \multicolumn{1}{c|}{Time}                & Rel. $L_2$ Error & \multicolumn{1}{c|}{Time}                & Rel. $L_2$ Error & \multicolumn{1}{c|}{Time}        & Rel. $L_2$ Error \\ \hline
250     & \multicolumn{1}{c|}{38 hours}            &    1.207E-2    & \multicolumn{1}{c|}{12 hours}            &     1.413E-2  & \multicolumn{1}{c|}{210 minutes} &    5.419E-3      \\ \hline
1,000   & \multicolumn{1}{c|}{\textgreater 5 days} & N.A.             & \multicolumn{1}{c|}{\textgreater 5 days} & N.A.             & \multicolumn{1}{c|}{217 minutes} &   4.153E-3\\ \hline
10,000  & \multicolumn{1}{c|}{OOM}                 & N.A.             & \multicolumn{1}{c|}{OOM}                 & N.A.             & \multicolumn{1}{c|}{481 minutes} &   4.168E-2  \\ \hline
100,000 & \multicolumn{1}{c|}{OOM}                 & N.A.             & \multicolumn{1}{c|}{OOM}                 & N.A.             & \multicolumn{1}{c|}{855 minutes} &   4.091E-2  \\ \hline
\end{tabular}
\caption{Relative $L_2$ error results, running time across different dimensions for the baselines and ours, for two HJB equations requiring adversarial training of PINNs. In all dimensions, our method significantly outperforms the other two baselines in terms of accuracy and speed. The baselines, employing full batch gradient descent, experience slower speed and increased memory requirements in higher-dimensional scenarios. In contrast, our approach utilizes Algorithm \ref{algo:2}, which employs stochastic gradient descent over PDE terms, resulting in faster computation and relatively lower memory consumption. Additionally, the symmetry of the PDE guarantees the accuracy of our method.}
% If no adversarial training 10000: 2.542E-3
\label{tab:adv}
\end{table}

\subsection{Schr\"{o}dinger Equation}
Here, we scale up the Tensor Neural Networks (TNNs) \cite{wang2022tensor,wang2022solving} for solving very high-dimensional Schr\"{o}dinger equations. Due to its unique characteristics, the Schr\"{o}dinger equation cannot be addressed by the traditional PINN framework and requires a specialized TNN network structure and corresponding loss function for high-dimensional eigenvalue problems. However, our method can still be flexibly integrated into this framework for scaling up Schr\"{o}dinger equations, demonstrating the versatility of our approach. In contrast, other methods specifically designed for high-dimensional equations exhibit much greater limitations.

\subsubsection{Tensor Neural Network (TNN) for Solving Schr\"{o}dinger Equation}
In general, the Schr\"{o}dinger equation can be viewed as a high-dimensional eigenvalue problem.
\begin{equation}\label{eq:schrodinger}
\begin{aligned}
-\Delta u(\bx) + v(\bx) u(\bx) &= \lambda u(\bx), \quad \bx
\in \Omega,\\
u(\bx) &= 0, \quad \bx \in \partial \Omega,
\end{aligned}
\end{equation}
where $ \Omega = \Omega_1 \times \Omega_2 \times \cdots \times \Omega_d \subset \mathbb{R}^d$ and each $\Omega_i = (a_i, b_i)$, $i = 1, Â· Â· Â· , d$ is a bounded interval in $\mathbb{R}$, $v(\bx)$ is a known potential function, $u(\bx)$ is the unknown solution and $\lambda$ is the unknown eigenvalue of the high-dimensional eigenvalue problem. This problem can be addressed via the variational principle:
\begin{align}
\lambda = \min_{\Phi(\bx)}\frac{\int_\Omega |\nabla \Phi(\bx)|^2d\bx + \int_\Omega v(\bx)\Phi(\bx)^2d\bx}{\int_\Omega \Phi(\bx)^2d\bx}.
\end{align}
The integrals are usually computed via the expensive quadrature rule, e.g., Gaussian quadrature:
\begin{equation}
\int_\Omega \Phi(\bx) d\bx \approx \sum_{n \in \mathcal{N}} w^{(n)}\Phi(\bx^{(n)}),
\end{equation}
where $\mathcal{N}$ is the index set for the quadrature points. The size $\mathcal{N}$ grows exponentially as the dimension increases, incurring the curse of dimensionality.
To overcome this issue, the Tensor Neural Network (TNN) adopts a separable function network structure to reduce the computational cost associated with integration:
\begin{align}
\Phi(\bx;\theta) = \sum_{j=1}^p \prod_{i=1}^d\phi_{j}(\bx_i;\theta_i),
\end{align}
where $\bx_i$ is the $i$th dimension of the input point $\bx$, and $\phi_{j}(\bx_i;\theta_i)$ is the $j$th output of the sub-wavefunction network parameterized by $\theta_i$, and $p$ is the predefined rank of the TNN model.
The integral by quadrature rule of the TNN can be calculated efficiently:
\begin{equation}
\int_\Omega \Phi(\bx; \theta) d\bx = \sum_{j=1}^p  \int_\Omega \prod_{i=1}^d\phi_{j}(\bx_i;\theta_i) d\bx = \sum_{j=1}^p  \prod_{i=1}^d\int_{\Omega_i} \phi_{j}(\bx_i;\theta_i) d\bx_i \approx \sum_{j=1}^p  \prod_{i=1}^d\sum_{n_i \in \mathcal{N}_i} w^{(n_i)}\phi_j(x^{(n_i)};\theta_i),
\end{equation}
where $\mathcal{N}_i$ is the index set for the $i$th dimensional numerical integral. Thus, the numerical integrals can be computed along each axes separately to reduce the exponential cost to linear cost.
We refer the readers to \cite{wang2022tensor, wang2022solving} for more details.
With the efficient quadrature achieved by TNN's separable structure, the loss function to be minimized is
\begin{equation}
\min_\theta\ell(\theta) = \frac{ \sum_{n \in \mathcal{N}} w^{(n)}|\nabla \Phi(\bx^{(n)}; \theta)|^2 +  \sum_{n \in \mathcal{N}} w^{(n)}v(\bx^{(n)})\Phi(\bx^{(n)}; \theta)^2}{ \sum_{n \in \mathcal{N}} w^{(n)} \Phi(\bx^{(n)}; \theta)^2},
\end{equation}
where $\Phi(\bx; \theta)$ is the neural network wave function with trainable parameters $\theta$.

The primary computational bottleneck of TNNs for this problem lies in the first-order derivatives in the loss function, while other terms only contain zero-order terms with lower computational and memory costs:
\begin{equation}
\frac{\partial}{\partial \bx_i} \Phi(\bx; \theta) = \sum_{j=1}^p \frac{\partial}{\partial \bx_i}\phi_{j}(\bx_i;\theta_i)\prod_{k=1, k\neq i}^d\phi_{j}(\bx_k;\theta_k)
\end{equation}
Since TNN is a separable architecture, the $d$-dimensional SchrÃ¶dinger equation requires computing the first-order derivatives and performing quadrature integration for each of the $d$ subnetworks. Consequently, the computational cost scales linearly with the dimension. In comparison to previous examples of PINN solving PDEs, TNN incurs larger memory consumption for first-order derivatives due to the non-sharing of network parameters across different dimensions, i.e., $d$ independent first-order derivatives of $d$ distinct subnetworks are required, resulting in increased computational requirements.

Mathematically, the computational and memory bottleneck is the gradient with respect to the first-order term of the neural network wave function $\Phi(\bx; \theta)$
\begin{equation}
\text{grad}(\theta) = \sum_{n \in \mathcal{N}} w^{(n)}\frac{\partial}{\partial\theta}|\nabla \Phi_\theta(\bx^{(n)})|^2 = \sum_{n \in \mathcal{N}} w^{(n)}\left(\sum_{i=1}^d\frac{\partial}{\partial\theta}\left|\frac{\partial}{\partial \bx_i} \Phi_\theta(\bx^{(n)})\right|^2\right).
\end{equation}
By employing the proposed SDGD approach, we can simplify the entire gradient computation:
\begin{equation}
\text{grad}_I(\theta) = \sum_{n \in \mathcal{N}} w^{(n)}\left(\frac{d}{|I|}\sum_{i \in I}\frac{\partial}{\partial\theta}\left|\frac{\partial}{\partial \bx_i} \Phi_\theta(\bx^{(n)})\right|^2\right),
\end{equation}
where $I \subset \{1,2,\cdots,d\}$ is a random index set.
Therefore, we can sample the entire gradient at the PDE term level to reduce memory and computational costs, and the resulting stochastic gradient is unbiased, i.e., $\mathbb{E}_I[\text{grad}_I(\theta)] =\text{grad}(\theta)$ ensuring the convergence of our method. The gradient accumulation can also be done for large-scale problems by our method; see Section \ref{sec:GAPC} for details.

It is important to note that our method is more general compared to the traditional approach of SGD solely based on quadrature points. In particular, in extremely high-dimensional cases, even computing the gradient for a single quadrature point may lead to an out-of-memory (OOM) error. This is because the traditional approach requires a minimum batch size of $d$ PDE terms and 1 quadrature point, where $d$ is the dimensionality of the PDE. However, our method further reduces the computational load per batch to only 1 PDE term and 1 quadrature point.


\subsubsection{Experimental Setup}
We consider the \textbf{Coupled Quantum Harmonic Oscillator (CQHO)} potential function: $v(\bx) = \sum_{i=1}^d \bx_i^2 - \sum_{i=1}^{d-1} \bx_i \bx_{i+1}$, where all pairs of variables, both $\bx_i$ and $\bx_{i+1}$, exhibit pairwise
interactions and are highly coupled. The original problem is defined on an infinite interval. Therefore, we truncate the entire domain to $[-5,5]^d$. The exact smallest eigenvalue is $\lambda = \sum_{i=1}^d\sqrt{1-\cos\left(\frac{i\pi}{d+1}\right)}$. We use the same model structure and hyperparameters as Wang et al. \cite{wang2022tensor}. For this problem, we test the effectiveness of the Gradient Accumulation method in Section \ref{sec:GAPC} based on our further decomposition of PINN's gradient.

The test metric we report is the $L_1$ relative error of the minimum eigenvalue, given by $\frac{| \lambda_{\text{true}} - \lambda_{\text{approx}} |}{| \lambda_{\text{true}} |}$. Due to the diminishing nature of the eigenfunctions in high dimensions, we only report the accuracy of the eigenvalues. It is important to note that the eigenvalues carry physical significance as they represent the ground state energy.

% Figure environment removed

\subsubsection{Experimental Results}
On the CQHO problem, our gradient accumulation method achieves training in $2*10^4$ dimensions within the limited GPU memory in Figure \ref{fig:qho}. In all dimensions, SDGD achieves a stable relative error of less than 1e-6. Note that gradient accumulation performs theoretically the same as full batch GD, so we do not compare their results, but the former can save GPU memory, thus scaling up PDEs to higher dimensions. These results demonstrate the scalability of our method for PINN and PDE problems and highlight its generality, making it applicable to various physics-informed machine-learning problems.

During the early stages of optimization, we observe quick convergence of TNNs. Since the model parameters are initialized randomly, there is more room for improvement. At this point, the gradients of the loss function with respect to the parameters can be relatively large. Larger gradients result in more significant updates to the parameters, leading to faster convergence.

Despite the satisfactory convergence results, these problems demonstrate one shortcoming of our proposed approach. Our method is primarily designed to address the computational bottleneck arising from high-dimensional differentiations in PDEs. However, if the problem itself is large-scale and does not stem from differential equations, our method cannot be directly applied. Specifically, in the case of the SchrÃ¶dinger equation, due to the separable nature of its network structure, we employ a separate neural network for each dimension. This results in significant memory consumption, even without involving differentials. Additionally, in this problem, we cannot sample the points of integration / residual points because there is an integral term in the denominator. If we sample the integral in the denominator, the resulting stochastic gradient will be biased, leading to poor convergence. Therefore, for such problems, designing an improved unbiased stochastic gradient remains an open question.


\section{Conclusions}
CoD has been an open problem for many decades, but recent progress has been made by several research groups using deep learning methods for specific classes of PDEs \cite{beck2021deep, han2018solving, raissi2018forward}. Herein, we proposed a general method based on the mesh-free PINNs method by introducing a new type of stochastic dimension gradient descent or SDGD as a generalization of the largely successful SGD method over subsets of the training set. In particular, we claim the following advantages of the SDGD-PINNs method over  other related methods:

\textbf{Generality to Arbitrary PDEs}. Most existing approaches \cite{beck2021deep, han2018solving, raissi2018forward} are restricted to specific forms of parabolic PDEs. They leverage the connection between parabolic PDEs and stochastic processes, employing Monte Carlo simulations of stochastic processes to train their models. Consequently, their methods are limited to a subset of PDEs. In contrast, our approach is based on PINNs capable of handling arbitrary PDEs. Notably, we can address challenging cases such as the wave equation, biharmonic equation, SchrÃ¶dinger equation, and other similar examples, while \cite{beck2021deep, han2018solving, raissi2018forward} cannot.

\textbf{Prediction on the Entire Domain}. Furthermore, existing methods \cite{beck2021deep, han2018solving, raissi2018forward} can only predict the values of PDE solutions at a single test point during each training instance. This limitation arises from the necessity of setting the starting point of the stochastic process (i.e., the test point for the PDE solution) before conducting Monte Carlo simulations. Consequently, the computational cost of their methods \cite{beck2021deep, han2018solving, raissi2018forward}  increases greatly as the number of test points grows. In contrast, our approach, based on PINNs, allows for predictions across the entire domain in a single training instance, achieving a ``once for all" capability.

\textbf{Dependency on the Mesh}. Moreover, precisely because these methods \cite{beck2021deep, han2018solving, raissi2018forward} require Monte Carlo simulations of stochastic processes, they necessitate temporal discretization. This introduces additional parameters and requires a sufficiently high discretization precision to obtain accurate solutions. For PDE problems that span long durations, these methods undoubtedly suffer from the burden of temporal discretization. However, our PINN-based approach can handle long-time PDEs effortlessly. Since PINNs are mesh-free methods, the training points can be randomly sampled. The interpolation capability of PINNs ensures their generalization performance on long-time PDE problems.

\textbf{PINNs can also be adapted for prediction at one point}. Although PINNs offer advantages such as being mesh-free and capable of predicting the entire domain, they still perform remarkably well in settings where other methods focus on single-point prediction. Specifically, if our interest lies in the value of the PDE at a single point, we can exploit the relationship between the PDE and stochastic processes. This single point corresponds to the initial point of the stochastic process, and the PDE corresponds to a specific stochastic process, such as the heat equation corresponding to simple Brownian motion. In this case, we only need to use the snapshots obtained from the trajectory of this stochastic process as the residual points for PINN.

We have proved the necessary theorems that show SDGD leads to an unbiased estimator and that it converges under mild assumptions, similar to the vanilla SGD over collocation points and mini-batches. SDGD can also be used in physics-informed neural operators such as the DeepOnet and can be extended to general regression and possibly classification problems in very high dimensions.

The potential limitation of SDGD is that it requires a relatively larger batch size for the dimension when dealing with extremely high-dimensional PDEs. A too-small batch size may incur high stochastic gradient variance that slows down the convergence. In addition, for PDEs not following the boundary+residual forms, like the SchrÃ¶dinger equation, a special loss function is generally required, which may cause the core memory bottleneck to not be in the dimension that SDGD focuses on. This causes the minimum memory that SDGD can obtain to grow faster with dimensions.

In summary, the new SDGD-PINN framework is a paradigm shift in the way we solve high-dimensional PDEs as it can tackle arbitrary nonlinear PDEs, of any dimension providing high accuracy at extremely fast speeds.

\section*{Acknowledgments}
The work of ZH and KK was supported by the Singapore Ministry of Education Academic Research Fund Tier 1 grant (Grant number: T1 251RES2207). 
The work of KS and GEK was supported by the MURI-AFOSR FA9550-20-1-0358 projects and by the DOE SEA-CROGS project (DE-SC0023191). GEK was also supported by the ONR Vannevar Bush Faculty Fellowship (N00014-22-1-2795). 

\newpage
\appendix
\section{Proof}
\subsection{Proof of Theorem \ref{thm:variance_1}}\label{appendix:variance_1}
\begin{lemma}\label{lemma:1}
Suppose that we have $N$ fixed numbers $a_1, a_2,\cdots,a_N$ and we choose $k$ random numbers $\{X_i\}_{i=1}^{k}$ from them, i.e., each $X_i$ is a random variable and $X_i = a_n$ with probability $1 / N$ for all $n \in \{1,2,\cdots,N\}$, and $X_i$ are independent. Then the variance of the unbiased estimator $\sum_{i=1}^k X_i / k$ for $\overline{a} = \sum_{n=1}^N a_n$ is $\frac{1}{kn}\sum_{i=1}^n(a_i - \Bar{a})^2$.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:1}]
Since the samples $X_i$ are i.i.d.,
\begin{equation}
\begin{aligned}
\mathbb{V}\left[\frac{\sum_{i=1}^k X_i}{k}\right]
&= \frac{1}{k}\mathbb{V}[X_i] = \frac{1}{kn}\sum_{i=1}^n(a_i - \Bar{a})^2
\end{aligned}
\end{equation}
\end{proof}
\begin{proof}[Proof of Theorem \ref{thm:variance_1}]
We assume that the total full batch is with $N_r$ residual points $\{\bx_i\}_{i=1}^{N_r}$ and $N_{\mathcal{L}}$ PDE terms $\mathcal{L} = \sum_{i=1}^{N_{\mathcal{L}}}\mathcal{L}_i$, then the residual loss of the PINN is
\begin{equation}
\frac{1}{2N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)^2,
\end{equation}
where we normalize over both the number of residual points and the number of PDE terms. The full batch gradient is:
\begin{equation}
\begin{aligned}
g(\theta) &= \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\frac{\partial}{\partial \theta}\mathcal{L}u_\theta(\bx_i)\\
&= \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\sum_{j=1}^{N_{\mathcal{L}}}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\\
&= \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\sum_{j=1}^{N_{\mathcal{L}}}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\\
&= \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\left(\left(\sum_{j=1}^{N_{\mathcal{L}}}\mathcal{L}_ju_\theta(\bx_i)\right) - R(\bx_i)\right)\left(\sum_{j=1}^{N_{\mathcal{L}}}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right).
\end{aligned}
\end{equation}
From the viewpoint of Algorithm \ref{algo:1}, the full batch gradient is the mean of $N_rN_{\mathcal{L}}$ terms:
\begin{equation}
g(\theta) = \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\sum_{j=1}^{N_{\mathcal{L}}}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right):= \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\sum_{j=1}^{N_{\mathcal{L}}}g_{i,j}(\theta),
\end{equation}
where we denote 
\begin{equation}
g_{i,j}(\theta) = \left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right).
\end{equation}
The stochastic gradient generated by Algorithm \ref{algo:1} is given by
\begin{equation}
g_{B, J}(\theta) = \frac{1}{|B||J|N_{\mathcal{L}}}\sum_{i \in B}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\sum_{j \in J}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right) = \frac{1}{|B||J|N_{\mathcal{L}}}\sum_{i \in B}\sum_{j \in J}g_{i,j}(\theta).
\end{equation}
We have the variance
\begin{equation}
\mathbb{V}_{B, J}[g_{B, J}(\theta)] =\mathbb{E}_{B, J}[\left(g_{B, J}(\theta) - g(\theta)\right)^2]= \mathbb{E}_{B, J}[g_{B, J}(\theta)^2] - \mathbb{E}_{B, J}[g(\theta)^2] = \mathbb{E}_{B, J}[g_{B, J}(\theta)^2] - g(\theta)^2.
\end{equation}
From high-level perspective, $\mathbb{V}_{B, J}[g_{B, J}(\theta)]$ should be a function related to $|B|$ and $|J|$. Thus, under the constraint that $|B| \cdot |J|$ is the same for different SGD schemes, we can choose $B$ and $J$ properly to minimize the variance of SGD to accelerate convergence. 
\begin{equation}
\begin{aligned}
\mathbb{E}_{B, J}[g_{B, J}(\theta)^2] &=\frac{1}{|B|^2|J|^2N_{\mathcal{L}}^2}\mathbb{E}_{B, J}\left[\sum_{i \in B}\sum_{j \in J}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\right]^2\\
&= \frac{1}{|B|^2|J|^2N_{\mathcal{L}}^2}\mathbb{E}_{B, J}\Bigg[\sum_{i,i' \in B}\sum_{j,j' \in J}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\cdot\\
&\quad \left(\mathcal{L}u_\theta(\bx_{i'}) - R(\bx_{i'})\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_{j'}u_\theta(\bx_{i'})\right)\Bigg].
\end{aligned}
\end{equation}
The entire expectation can be decomposed into four parts (1) $i=i', j=j'$, (2) $i \neq i', j=j'$, (3) $i=i', j\neq j'$, (4) $i\neq i', j\neq j'$.
For the first part
\begin{equation}
\begin{aligned}
&\quad\frac{1}{|B|^2|J|^2N_{\mathcal{L}}^2}\mathbb{E}_{B, J}\Bigg[\sum_{i\in B}\sum_{j\in J}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)^2\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)^2\Bigg]\\
&=\frac{1}{|B||J|N_{\mathcal{L}}^2}\mathbb{E}_{i,j}\Bigg[\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)^2\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)^2\Bigg]\\
&=\frac{1}{|B||J|N_{\mathcal{L}}^2}\mathbb{E}_{i,j}[g_{i,j}(\theta)^2]\\
&= \frac{1}{|B||J|N_rN_{\mathcal{L}}^3}\sum_{i=1}^{N_r}\sum_{j=1}^{N_{\mathcal{L}}} g_{i,j}(\theta)^2\\
&= \frac{C_1}{|B||J|},
\end{aligned}
\end{equation}
where we denote $C_1 = \frac{1}{N_rN_{\mathcal{L}}^3}\sum_{i=1}^{N_r}\sum_{j=1}^{N_{\mathcal{L}}} g_{i,j}(\theta)^2$ which is independent of $B, J$.

In the second case, since the $i$th sample and the $i'$th sample are independent for $i \neq i'$, we have
\begin{equation}
\begin{aligned}
&\quad\frac{1}{|B|^2|J|^2N_{\mathcal{L}}^2}\mathbb{E}_{B, J}\Bigg[\sum_{i\neq i'\in B}\sum_{j\in J}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\mathcal{L}u_\theta(\bx_{i'}) - R(\bx_{i'})\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_{j}u_\theta(\bx_{i'})\right)\Bigg]\\
&=\frac{|B|-1}{|B||J|N_{\mathcal{L}}^2}\mathbb{E}_{i,i',j}\Bigg[\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\mathcal{L}u_\theta(\bx_{i'}) - R(\bx_{i'})\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_{i'})\right)\Bigg]\\
&=C_2\cdot \frac{|B|-1}{|B||J|}.
\end{aligned}
\end{equation}
In the third case, due to the same reason,
\begin{equation}
\begin{aligned}
&\quad\frac{1}{|B|^2|J|^2N_{\mathcal{L}}^2}\mathbb{E}_{B, J}\Bigg[\sum_{i\in B}\sum_{j\neq j'\in J}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)^2\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_{j'}u_\theta(\bx_i)\right)\Bigg]\\
&=\frac{|J|-1}{|B||J|N_{\mathcal{L}}^2}\mathbb{E}_{i,j,j'}\Bigg[\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)^2\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_{j'}u_\theta(\bx_i)\right)\Bigg]\\
&=C_3\cdot \frac{|J|-1}{|B||J|}.
\end{aligned}
\end{equation}
In the last case,
\begin{equation}
\begin{aligned}
&\quad\frac{1}{|B|^2|J|^2N_{\mathcal{L}}^2}\mathbb{E}_{B, J}\Bigg[\sum_{i\neq i'\in B}\sum_{j\neq j'\in J}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\mathcal{L}u_\theta(\bx_{i'}) - R(\bx_{i'})\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_{j'}u_\theta(\bx_{i'})\right)\Bigg]\\
&=\frac{(|B|-1)(|J| - 1)}{|B||J|N_{\mathcal{L}}^2}\mathbb{E}_{i,i',j,j'}\Bigg[\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\mathcal{L}u_\theta(\bx_{i'}) - R(\bx_{i'})\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_{j'}u_\theta(\bx_{i'})\right)\Bigg]\\
&=\frac{(|B|-1)(|J| - 1)}{|B||J|N_{\mathcal{L}}^2}\left(\mathbb{E}_{i,j}\Bigg[\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\mathcal{L}u_\theta(\bx_{i'}) - R(\bx_{i'})\right)\Bigg]\right)^2\\
&=\frac{(|B|-1)(|J| - 1)}{|B||J|}g(\theta)^2
\end{aligned}
\end{equation}
Taking together
\begin{equation}
\begin{aligned}
\mathbb{V}_{B, J}[g_{B, J}(\theta)] &= \mathbb{E}_{B, J}[g_{B, J}(\theta)^2] - g(\theta)^2\\
&=\frac{C_1 + C_2(|B|-1) + C_3(|J| - 1) + (1-|B|-|J|)g(\theta)^2}{|B||J|}\\
&= \frac{C_4|J| + C_5|B| + C_6}{|B||J|}.
\end{aligned}
\end{equation}

\end{proof}

\subsection{Proof of Lemma \ref{lemma:variance}}\label{appendix:lemma}
\begin{proof}
Here, we bound the gradient produced by physics-informed loss functions, which further provides an upper bound for the gradient variance during PINN training using SDGD.

For the stochastic gradient produced by Algorithm \ref{algo:1},
\begin{align}
\mathbb{V}_{B,J}(g_{B, J}(\theta)) &\leq \frac{1}{|B||J|N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\sum_{j=1}^{N_{\mathcal{L}}}\Vert g_{i,j}(\theta) - g(\theta)\Vert^2 \leq \frac{2}{|B||J|N_{\mathcal{L}}}\max_{i,j} \Vert g_{i,j}(\theta) \Vert^2,
\end{align}
where
\begin{align}
g_{i,j}(\theta) := \left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right).
\end{align}

For the stochastic gradient produced by Algorithm \ref{algo:2},
\begin{equation}
\begin{aligned}
g_{B, J, K}(\theta) &= \frac{1}{|B||J|N_{\mathcal{L}}}\sum_{i \in B}\left(\frac{N_{\mathcal{L}}}{|K|}\left(\sum_{k \in K}\mathcal{L}_ku_\theta(\bx_i)\right) - R(\bx_i)\right)\left(\sum_{j \in J}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\\
&=\frac{1}{|B||J||K|}\sum_{i \in B}\sum_{j \in J}\sum_{k \in K}\left(\mathcal{L}_ku_\theta(\bx_i) - \frac{R(\bx_i)}{N_{\mathcal{L}}}\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right),
\end{aligned}
\end{equation}
\begin{align}
\mathbb{V}_{B,J,K}(g_{B, J, K}(\theta)) &\leq \frac{1}{|B||J||K|N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\sum_{j=1}^{N_{\mathcal{L}}}\sum_{k=1}^{N_{\mathcal{L}}}(g_{i,j,k}(\theta) - g(\theta))^2 \leq \frac{2}{|B||J||K|}\max_{i,j,k} \Vert g_{i,j,k}(\theta) \Vert^2,
\end{align}
where
\begin{align}
g_{i,j,k}(\theta) := \left(\mathcal{L}_ku_\theta(\bx_i) - \frac{R(\bx_i)}{N_{\mathcal{L}}}\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right).
\end{align}

We focus on bounding the following two quantities:
\begin{align}
\max_{i, j}|\mathcal{L}_ju_\theta(\bx_i)| &\leq n!(L-1)^n\Vert W_L \Vert \prod_{l=1}^{L-1} \Vert W_l \Vert^n,\\
\max_{i, j}\left\|\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right\|&\leq (n+1)!(L-1)^{n+1}\Vert W_L \Vert \prod_{l=1}^{L-1} \Vert W_l \Vert^{n+1} \max_{\bx \in \Omega}\Vert\bx\Vert.
\end{align}


Recall that the DNN structure:
\begin{equation*}
    u_\theta(\bx)=W_L \sigma (W_{L-1} \sigma(\cdots \sigma(W_1\bx)\cdots ).
\end{equation*}


The neural network $u_\theta(\bx)$ has only one part concerning $\bx$, and after taking the derivative with respect to $\bx$, there is only one term:
\begin{equation}
\frac{\partial u_{\theta}(\bx)}{\partial \bx} = W_L \cdot \Phi_{L-1}(\bx) W_{L-1} \cdot \dots \cdot \Phi_1(\bx) W_1 \in \mathbb{R}^{d},
\end{equation}
where
$\Phi_l(\bx) = \text{diag}[\sigma'(W_l\sigma(W_{l-1}\sigma(\cdots\sigma(W_1\bx))))] \in \mathbb{R}^{m_l\times m_l}$, and the first-order derivative has an upper bound of $\prod_{l=1}^L \Vert W_l \Vert$. 

After taking the second derivative, due to the nested structure of the neural network, there will be $L-1$ parts concerning $\bx$ induced by the derivative of each $\Phi_l(\bx)$ with respect to $\bx$, resulting in $L-1$ terms:
\begin{equation}
\begin{aligned}
\frac{\partial^2 u_{\theta}(\bx)}{\partial \bx^2} &=\left\{\sum_{l=1}^{L-1} 
(W_L\Phi_{L-1}(\bx)\cdots W_{l+1})
\text{diag}(\Psi_l(\bx)\cdots\Psi_1(\bx)(W_1)_{:,j})
(W_l\cdots \Phi_1(\bx)W_1)\right\}_{1 \leq j \leq d},
\end{aligned}
\end{equation}
where
$\Phi_l(\bx) = \text{diag}[\sigma'(W_l\sigma(W_{l-1}\sigma(\cdots\sigma(W_1\bx))))] \in \mathbb{R}^{m_l\times m_l}$, and
$\Psi_l(\bx) = \text{diag}[\sigma''(W_l\sigma(W_{l-1}\sigma(\cdots\sigma(W_1\bx))))] \in \mathbb{R}^{m_l\times m_l}$. Each of these $L-1$ terms has an upper bound of $\Vert W_L \Vert \prod_{l=1}^{L-1} \Vert W_l \Vert^2$, and possesses at most $2(L-1)$ sub-terms depending on $\bx$. Therefore, there are $2(L-1)^2$ sub-terms within the second-order derivative depending on $\bx$. 

Using the principle of induction, after taking the $n$th derivative, there will be $n!(L-1)^n$ parts and each term has an upper bound of $\Vert W_L \Vert \prod_{l=1}^{L-1} \Vert W_l \Vert^n$. If we further take the derivative with respect to $W_l$, then there will be at most $(n+1)!(L-1)^{n+1}$ parts concerning $W_l$, and each term will be upper bounded by $\Vert W_L \Vert \prod_{l=1}^{L-1} \Vert W_l \Vert^{n+1} \max_{\bx \in \Omega}\Vert\bx\Vert$.


Consequently,
\begin{equation}
\Vert g_{i,j}(\theta)\Vert\leq \left\|\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right\|\left\|\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right\|\leq \left(\max_{i, j}|\mathcal{L}_ju_\theta(\bx_i)| + R\right)\max_{i, j}\left\|\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right\|,
\end{equation}
\begin{equation}
\Vert g_{i,j,k}(\theta)\Vert\leq \left\|\mathcal{L}_ku_\theta(\bx_i) - \frac{R(\bx_i)}{N_{\mathcal{L}}}\right\|\left\|\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right\|\leq \left(\max_{i, j}|\mathcal{L}_ju_\theta(\bx_i)| + R\right)\max_{i, j}\left\|\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right\|.
\end{equation}

\end{proof}


\section{More Experiments and Parallel Computing}
Our main goal is to showcase the following properties and advantages of our method:
\begin{enumerate}
\item For Algorithm \ref{algo:1}, reducing the residual point batch size and the PDE term batch size yields a comparable acceleration effect under the same memory budget. In some cases, our SDGD can even outperform the traditional SGD, demonstrating that SDGD can be an alternative form of SGD applied to PDE terms that is stable and reliable.

\item The main objective of Algorithm \ref{algo:2} is to scale up and accelerate training for large-scale PINN problems. We will demonstrate that while Algorithm \ref{algo:2} exhibits poorer convergence on small-scale problems due to increased gradient variance, its convergence speed and accuracy improve significantly as the scale of the PDE problem grows, such as in cases with tens of thousands of dimensions. In many of these large-scale instances, Algorithm \ref{algo:2} achieves convergence, whereas Algorithm \ref{algo:1} and full training only reach a few epochs without convergence. Additionally, full training may lead to out-of-memory (OOM) errors directly.

\item We compare our PINN-based method with other non-PINN mainstream approaches for solving high-dimensional PDEs \cite{beck2021deep, han2018solving, raissi2018forward}, especially in terms of accuracy.
\end{enumerate}

All the experiments are done on an NVIDIA A100 GPU with 40GB memory.

\subsection{Hamilton-Jacobi-Bellman (HJB) Equation with Linear Solution and Black-Scholes-Barenblatt (BSB) Equation: 
Scaling-up to 100,000D, and Comparisons with Baselines}

This experiment will demonstrate how Algorithms \ref{algo:1} and \ref{algo:2} are superior to the commonly used full-batch gradient descent on training PINNs, and how our SDGD-PINNs method can surpass strong non-PINNs baselines, on several nonlinear PDEs.

\subsubsection{Experimental Design}
We consider the following two nonlinear PDEs:
\begin{itemize}
\item \textbf{Hamilton-Jacobi-Bellman equation with linear solution (HJB-Lin)} \cite{wang20222} with the terminal condition given below:
\begin{equation}
\begin{aligned}
&\partial_t u(\bx, t) + \Delta u(\bx, t) - \frac{1}{d}\sum_{i=1}^d |\partial_{\bx_i}u|^c = -2, \quad \bx \in \mathbb{R}^d, t\in[0,T]\\
&u(\bx,T)=\sum_{i=1}^d \bx_i,
\end{aligned}
\end{equation} 
where $c=1, T=1$ and the dimension $d$ can be arbitrary chosen. The analytic solution is:
\begin{equation}
u(\bx,t) = \sum_{i=1}^d \bx_i + T - t.
\end{equation}
In this example, we decompose the residual prediction of PINN along each dimension as follows:
\begin{equation}
\partial_t u(\bx, t) + \Delta u(\bx, t) - \frac{1}{d}\sum_{i=1}^d |\partial_{\bx_i}u|^c = \frac{1}{d}\sum_{i=1}^d \left\{\partial_t u(\bx, t) + d\partial^2_{\bx_i}u(\bx, t) - \frac{1}{d}\sum_{j=1}^d |\partial_{\bx_j}u|^c \right\},
\end{equation}
since the first-order derivative is much cheaper and is not the main bottleneck.
\item \textbf{Black-Scholes-Barenblatt (BSB) equation}
\begin{equation}
\begin{aligned}
&u_t = -\frac{1}{2}\sigma^2\sum_{i=1}^d\bx_i^2u_{\bx_i\bx_i} + r(u -\sum_{i=1}^d\bx_iu_{\bx_i}),\\
&u(\bx, T) = \Vert \bx \Vert^2,
\end{aligned}
\end{equation}
where $T = 1, \sigma = 0.4, r = 0.05$ with the following analytic solution
\begin{equation}
u(\bx, t) = \exp\left[(r + \sigma^2)(T - t)\right]\Vert \bx \Vert^2.
\end{equation}
In this example, we decompose the residual prediction of PINN along each dimension as follows:
\begin{equation}
u_t+\frac{1}{2}\sigma^2\sum_{i=1}^d\bx_i^2u_{\bx_i\bx_i} - r(u -\sum_{i=1}^d\bx_iu_{\bx_i}) = \frac{1}{d}\sum_{i=1}^d \left\{u_t+\frac{1}{2}\sigma^2d\bx_i^2u_{\bx_i\bx_i} - r(u -\sum_{j=1}^d\bx_ju_{\bx_j})\right\},
\end{equation}
\end{itemize}

Using consistent hyperparameter settings, we explore varying PDE dimensions in the range of ${10^2, 10^3, 10^4, 10^5}$. This allows us to assess the acceleration achieved by our algorithm and whether PINNs can overcome the CoD.
Specifically, for the hyperparameters of PINNs, we employ a four-layer PINN with 1024 hidden units, trained using the Adam optimizer \cite{kingma2014adam} with an initial learning rate of 1e-3, exponentially decaying with a factor of 0.9995. Our testing datasets consist of 2e4 points, sampled from $\bx \sim \mathcal{N}(0,\boldsymbol{I})$ and $t \sim \text{Unif}[0,1]$. The residual points are sampled from the same distribution. The testing criterion is the relative $L_2$ error between the model and the solution given by
\begin{equation}
\frac{\sqrt{\sum_{i=1}^{N_{\text{test}}}|u_{\text{pred}}(X_i, t_i) - u(X_i, t_i)|^2}}{\sqrt{\sum_{i=1}^{N_{\text{test}}}|u(X_i, t_i)|^2}},
\end{equation}
where $u_{\text{pred}}$ is the prediction model and $u$ is the true solution, $N_{\text{test}}$ is the number of test points and $(X_i, t_i)$ is the $i$th test point.

Due to the large scale of the terminal condition, such as in HJB-Lin, where it follows a Gaussian distribution with zero mean and variance of $d$, or in the BSB equation, where it follows a chi-squared distribution with a mean of $d$, there is a high probability of sampling points with large values in high-dimensional space. Directly learning the terminal condition using the boundary loss can result in numerical overflow and optimization difficulties.
Therefore, we modified the PINN structure to satisfy the terminal condition automatically. This allows us to focus solely on the residual loss. Specifically, for the HJB-Lin and BSB equations with the terminal condition $u(\bx, T) = g(\bx)$, we consider the following PINN models:
\begin{equation}\label{eq:modified_nn}
{u}^{\text{HJB-Lin}}_\theta(\bx, t) = u_\theta(\bx, t)(T - t) + \sum_{i=1}^d\bx_i, \quad {u}^{\text{BSB}}_\theta(\bx, t) = \left((T-t)u_\theta(\bx, t) + 1\right)\Vert\bx\Vert^2,
\end{equation}
where $u_\theta$ is a vanilla multi-layer perceptron network while ${u}^{\text{HJB-Lin}}_\theta$ and ${u}^{\text{BSB}}_\theta$ are the prediction models for HJB-Lin and BSB equations, respectively.
For the HJB-Lin equation, the initialization is already sufficiently accurate regarding the $L^2$ error of the prediction for $u$, as the $\sum_{i=1}^d \bx_i$ scale is much larger than $T-t$. Therefore, we focus on testing the accuracy of $\partial_t u$, which is a nontrivial task:
\begin{equation}
\frac{\sqrt{\sum_{i=1}^{N_{\text{test}}}|\partial_tu_{\text{pred}}(X_i, t_i) - \partial_tu(X_i, t_i)|^2}}{\sqrt{\sum_{i=1}^{N_{\text{test}}}|\partial_tu(X_i, t_i)|^2}},
\end{equation}
where $u_{\text{pred}}$ is the prediction model and $u$ is the true solution.

For the baselines \cite{beck2021deep, han2018solving, raissi2018forward}, it is necessary to specify the initial point of the stochastic process corresponding to the PDE. These methods can only make predictions at this initial point, hence reporting the relative $L_1$ error at a single point. We will also report the error of PINN at this initial point. We choose the initial points as $(X_0, t_0) = (\bx_1=1, \bx_2=1, \cdots, \bx_d=1, t=0)$ for the BSB equation and $(X_0, t_0) = (\bx_1=0, \bx_2=0, \cdots, \bx_d=0, t=0)$ for the HJB-Lin equation, and the corresponding error is
\begin{equation}
\frac{|u_{\text{pred}}(X_0, t_0) - u(X_0, t_0)|}{u(X_0, t_0)},
\end{equation}
where $u_{\text{pred}}$ is the prediction model and $u$ is the true solution.

% Figure environment removed

% Figure environment removed

\begin{table}[]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
PDE & $10^2$ & $10^3$ & $10^4$ & $10^5$ \\ \hline
HJB-Lin & 34min & 68min & 81min & 310min \\ \hline
BSB & 31min & 57min & 118min & 41min \\ \hline
\end{tabular}
\caption{This table presents the convergence time required by our SDGD for different PDEs. In the HJB-Lin equation, as the dimensionality increases from 100 to 100,000, the dimensionality grows by a factor of 1000, while the time only increases by a factor of ten. This indicates that our method can withstand the curse of dimensionality. In the second BSB equation, surprisingly, the high-dimensional case converges faster than the low-dimensional case, demonstrating the so-called blessing of dimensionality and that our method can harness the blessing of dimensionality.}
\end{table}

\subsubsection{Results of Our Method and Full-Batch GD on Training PINN}
This subsection presents the results comparing our Algorithms \ref{algo:1} and \ref{algo:2} with full batch gradient descent when training PINN for the BSB and HJB equations, which are demonstrated in Figures \ref{fig:HJB2_scale_up} and \ref{fig:BSB_scale_up}.

Specifically, the captions of Figures \ref{fig:HJB2_scale_up} and \ref{fig:BSB_scale_up} illustrate different algorithms' speeds and memory costs in the HJB and BSB equations across various dimensions. Regarding memory cost, full gradient descent encounters OOM problems in 100,000 dimensions even with only one residual point, while SDGD can handle even higher-dimensional problems. In terms of speed, under comparable memory costs, Algorithm \ref{algo:2} consistently exhibits the highest speed, followed by Algorithm \ref{algo:1}, while full batch GD is the slowest. Notably, in high-dimensional cases, our Algorithm \ref{algo:2} demonstrates significant acceleration compared to other algorithms. In lower dimensions, the acceleration effect of Algorithm \ref{algo:2} is less pronounced, and Algorithm \ref{algo:1} suffices.

In the case of the 1000-dimensional BSB equation, Algorithm \ref{algo:2} exhibits a shorter time per epoch compared to Algorithm \ref{algo:1}. However, the convergence results are inferior, suggesting that Algorithm \ref{algo:2} sacrifices speed for larger gradient variance due to increased random sampling in both forward and backward passes. Nevertheless, as the dimension of the PDE increases, the advantageous fast convergence of Algorithm \ref{algo:2} becomes more evident. For instance, on the largest 100,000-dimensional problem, Algorithm \ref{algo:2} achieved convergence with a relative error of less than 3.477e-5 (HJB) / 4.334e-3 (BSB) in under 4 hours. In contrast, full GD encountered OOM errors, and the speed of Algorithm \ref{algo:1} was extremely slow, running only a few epochs within the same time frame and still far from convergence.

Meanwhile, in the 100,000-dimensional HJB equation with a relatively small batch size (100 PDE terms and 100 residual points), we observed a significant increase in the relative $L_2$ error performance of Algorithm \ref{algo:2} during training, attributable to the heightened variance of the gradient.

Thus, Algorithm \ref{algo:1} effectively reduces the memory cost of PINN by minimizing gradient variance and accelerates convergence through gradient randomness. Algorithm \ref{algo:2} further enhances iteration speed while reducing memory requirements, albeit at the expense of increased gradient variance and compromised convergence results. However, in higher dimensional cases, the acceleration provided by Algorithm \ref{algo:2} outweighs the impact of gradient variance. 

%Previous best results were achieved with a relative error of 2.3E-3 \cite{raissi2018forward} for the 100D BSB equation, and 6.1E-3 \cite{wang20222} for the 100D HJB equation. We have surpassed these benchmarks in both dimensions and performance.

In addition to the stochastic gradient acceleration employed in our method, the symmetry of the PDE plays a crucial role in achieving convergence. These PDEs exhibit symmetry along each $\bx_i$ axis regardless of dimensionality. This symmetry reduces gradient variance in SDGD and random sampling during the forward pass in Algorithm \ref{algo:2}, facilitating rapid convergence in our approach.

In conclusion, the experimental results demonstrate that Algorithm \ref{algo:1} and Algorithm \ref{algo:2} exhibit significant advantages over full batch gradient descent in training PINNs. This is particularly evident in the case of very high dimensions, where their acceleration and computational efficiency are especially pronounced.

\subsubsection{Comparison Between SDGD-PINNs and Strong Baselines}

\begin{table}[]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Method & Evaluation Metric  & Dim=$10^2$       & Dim=$10^3$   & Dim=$10^4$   & Dim=$10^5$   \\ \hline
FBSNN \cite{raissi2018forward} & Relative $L_2$ error of $u$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
DeepBSDE \cite{han2018solving} & Relative $L_2$ error of $u$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
DeepSplitting \cite{beck2021deep} & Relative $L_2$ error of $u$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
Ours & Relative $L_2$ error of $u$ & \textbf{6.182E-4} & \textbf{1.284E-3} & \textbf{3.571E-3} & \textbf{4.486E-3}  \\ \toprule\hline
Method & Evaluation Metric  & Dim=$10^2$       & Dim=$10^3$   & Dim=$10^4$   & Dim=$10^5$   \\ \hline
FBSNN \cite{raissi2018forward} & Relative $L_1$ error on one point & 1.378E-5 & 8.755E-5 & 4.003E-5  & 3.559E-5 \\ \hline
DeepBSDE \cite{han2018solving} & Relative $L_1$ error on one point & 1.106E-5  & 5.184E-4  & 3.013E-4 & 4.519E-4 \\ \hline
DeepSplitting \cite{beck2021deep} & Relative $L_1$ error on one point  &  2.048E-3  & 1.942E-3 & 1.992E-3  & 2.109E-3 \\ \hline
Ours & Relative $L_1$ error on one point & \textbf{6.781E-6} & \textbf{4.915E-5} & \textbf{1.084E-5} & \textbf{1.972E-5}  \\\hline
\end{tabular}
\caption{Results of various methods on the BSB equations with different dimensions and evaluation metrics.}
\label{tab:BSB}
\end{table}

\begin{table}[]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Method & Evaluation Metric  & Dim=$10^2$       & Dim=$10^3$   & Dim=$10^4$   & Dim=$10^5$   \\ \hline
FBSNN \cite{raissi2018forward} & Relative $L_2$ error of $u$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
DeepBSDE \cite{han2018solving} & Relative $L_2$ error of $u$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
DeepSplitting \cite{beck2021deep} & Relative $L_2$ error of $u$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
Ours & Relative $L_2$ error of $u$ &\textbf{1.159E-4} & \textbf{3.508E-4} & \textbf{1.216E-5} & \textbf{1.937E-7} \\\toprule \hline
Method & Evaluation Metric  & Dim=$10^2$       & Dim=$10^3$   & Dim=$10^4$   & Dim=$10^5$   \\ \hline
FBSNN \cite{raissi2018forward} & Relative $L_2$ error of $u_t$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
DeepBSDE \cite{han2018solving} & Relative $L_2$ error of $u_t$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
DeepSplitting \cite{beck2021deep} & Relative $L_2$ error of $u_t$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
Ours & Relative $L_2$ error of $u_t$ & \textbf{4.918E-3} & \textbf{3.243E-3} & \textbf{2.655E-3} & \textbf{3.792E-5}  \\ \toprule\hline
Method & Evaluation Metric  & Dim=$10^2$       & Dim=$10^3$   & Dim=$10^4$   & Dim=$10^5$   \\ \hline
FBSNN \cite{raissi2018forward} & Relative $L_1$ error on one point  & 6.327E-5  & 7.062E-5 & 4.916E-5  & 5.218E-5 \\ \hline
DeepBSDE \cite{han2018solving} & Relative $L_1$ error on one point & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
DeepSplitting \cite{beck2021deep} & Relative $L_1$ error on one point  & 1.034E-2  & Diverge & Diverge  & Diverge \\ \hline
Ours & Relative $L_1$ error on one point & \textbf{2.711E-5} & \textbf{6.208E-6} & \textbf{2.513E-5
} & \textbf{3.614E-5}  \\\hline
\end{tabular}
\caption{Results of various methods on the HJB-Lin equations with different dimensions and evaluation metrics.}
\label{tab:HJB_Lin}
\end{table}

This subsection presents the results comparing SDGD based on PINN and other strong non-PINN baselines \cite{beck2021deep, han2018solving, raissi2018forward} for the BSB and HJB equations. 

Tables \ref{tab:BSB} and \ref{tab:HJB_Lin} showcase the comparative results between our algorithm and the baselines on the BSB and HJB equations, respectively. Specifically, we present the prediction performance of various models on both the solution field ($u$ and/or $u_t$) and individual points, which are introduced in the experimental setup. If predictions are to be made across the entire domain, only our PINN-based approach can achieve that, while other methods can only make predictions at a single point. This showcases the advantage of PINN in not requiring a mesh/grid, unlike other methods that rely on temporal discretization. Even in predicting at a single point, PINN performs exceptionally well and achieves the best results among various methods. This is because although PINN is trained across the entire domain, its neural network possesses strong interpolation capabilities, enabling accurate predictions at single points. Other methods perform poorly on these PDEs because the scale of the entire PDE solution is very large, leading to potential issues such as numerical overflow or gradient explosion. However, in the case of PINNs, we cleverly employ boundary/terminal condition augmentation to mitigate these issues.

\subsection{Parallel Computing}
To speed up the training, we explored two avenues for distributed training. First, we used the data-parallel approach \cite{goyal2017accurate}, and second we explored the 1D tensor parallel approach \cite{narayanan2021efficient}.

The programming model for the data-parallel approach is based on the single instruction and multiple data (SIMD), resulting in a weak scaling-driven performance model. In the data-parallel approach, the data is uniformly split into a number of chunks, equal to the number of GPUs. The neural network (NN) model is initialized with the same parameters on all the processes. These neural networks are working on different chunks of the data, and therefore, work on different loss functions. To ensure a consistent neural network model with the same weights and biases across all the processes and during each epoch or iteration of training, a distributed optimizer is used, which averages out the gradient of loss values stored at each processor through an ``all reduce" operation. Subsequently, the averaged gradient of the loss function is sent to all the processors in the communicator world through a broadcast operation. Thus, the parameters of the model are updated simultaneously with the same gradients.

% Figure environment removed

To show the performance of the algorithm presented in this paper, we chose the HJB-Lin problem in 100,000 dimensions. We perform all the computational experiments in this section on A100 GPUs with 80 GB memory. Since the performance of the data parallel approach is driven by weak scaling, we need to first find out the correct hyperparameters such as the depth and width of neural networks, number of collocation points, batch size, etc. We show a detailed Table \ref{tab:S3} showing the performance of the data-parallel approach by varying the hyperparameters, specifically dimensions and the number of collocation points in each dimension. To observe the accuracy of the parallel code we ran a case for solving the HJB-Lin equation up to 5000 iterations and the convergence plot is shown in Figure \ref{fig:S6}, which utilizes the hyperparameters DIM=100, $N_f$=100, where we denote the batch size for dimension in Algorithm 1 of SDGD as DIM and the number of collocation points used in each iteration as $N_f$ in the parallel computing section.  

\begin{table*}[htbp]
\centering
\begin{tabular}{ |p{4cm}||p{3cm}|p{3cm}|p{3cm}|  }
 \hline
 \multicolumn{4}{|c|}{Scaling for data-parallel approach for HJB-Lin equation in 100,000 dimensions} \\
 \hline
Hyperparameters & 1-GPU (Mem and Compute) & 2-GPUs (Mem and Compute)  & Time Per iteration\\
 \hline
 DIM=100, $N_f$=100 & 34 GB and 50 \%  &  34 GB and  25 \% & 1 GPU = 10 sec, ~~~ 2 GPU = 2.5 sec  \\
 \hline 
DIM=1200, $N_f$=100 & 67 GB and 25 \%  & 37 GB and 30 \% & 1 GPU = 75  sec, ~~~ 2 GPU =  25 sec \\
 \hline 
DIM=2000, $N_f$=100 &  OOM Error & 37 GB and   50 \% & 1 GPU = NA, ~~~~ 2 GPU = 38 sec \\
\hline
\end{tabular}
\caption{Scaling for data-parallel approach with 3 hidden layers and 1024 neurons in each layer. Experiments are performed on A100 GPUs with 80 GB memory. Here we denote the batch size for dimension in Algorithms 1 and/or 2 of SDGD as DIM and the number of collocation points used in each iteration as $N_f$ in the parallel computing section.}
\label{tab:S3}
\end{table*}

\begin{table}[htbp]
\centering
\begin{tabular}{ |p{4cm}||p{3cm}|p{3cm}|p{3cm}|  }
 \hline
 \multicolumn{4}{|c|}{Scaling for data-parallel for 100,000D HJB-Lin equation with well-balanced load} \\
 \hline
Hyperparameters & 1-GPU (Mem and Compute) & 2-GPUs (Mem and Compute)  & Time Per iteration\\
 \hline
DIM=2000, $N_f$=1000 &  OOM Error & 80 GB, and   100\% & 1 GPU = NA, ~~~~ 2 GPU = 100 sec \\
\hline
\end{tabular}
\caption{Scaling for data-parallel approach with optimal hyperparameters endowed with 9 hidden layers and 1024 neurons in each layer.}
\label{tab:S4}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{ |p{4cm}||p{3cm}|p{3cm}|  }
 \hline
 \multicolumn{3}{|c|}{Performance: Serial vs data parallel code for 100,000D HJB-Lin} \\
 \hline
Hyperparameters & 1-GPU  & 2-GPUs \\
 \hline
DIM=100, $N_f$=100 &  6.17 Hrs & - \\
\hline
DIM=2000, $N_f$=1000 & - & 3.14 Hrs \\
\hline
\end{tabular}
\caption{Scaling results for serial and parallel implementation.}
\label{tab:S5}
\end{table}

For all the cases shown in Table \ref{tab:S3}, the GPU is not saturated completely, and therefore scaling is affected by memory latency. In Table \ref{tab:S4}, we show an optimal set of hyperparameters required to saturate the GPUs. With these parameters, we ran the proposed algorithm till the full convergence for the HJB-Lin equation in 100,000 dimensions, and the wall times are shown in Table \ref{tab:S5}. We see a very good scaling but there is still scope for further optimization. Further optimization of code and scaling it across the nodes will be considered in future work.

% Figure environment removed

%%%% A para on Tensor parallel approach and Table
Next, we present the results of the tensor parallel algorithm \cite{narayanan2021efficient}. In the tensor parallel algorithm, we split the weight matrices of neural networks along the column  dimension as shown in Figure \ref{fig:TPara}. Thereafter, the forward pass and backward pass are performed using the pipeline approach.
Since in the present study, our model is not very large but to saturate the GPU, we have used 8 hidden layers and 2048 neurons and performed the parallelization on 2 GPUs. The tensor parallel approach did not scale as well due to the effect of pipe-lining deteriorating the performance. We show a comparison of the tensor and data parallel approaches in Table \ref{tab:S6}. 

\begin{table}[hbtp]
\centering
\begin{tabular}{ |p{4cm}||p{3cm}|p{3cm}|  }
 \hline
 \multicolumn{3}{|c|}{Performance: data vs tensor parallel code for 100,000D HJB equation on 2 GPUs} \\
 \hline
Hyperparameters & data parallel  & tensor parallel \\
 \hline
DIM=2000, $N_f$=1000 & 100 s/iteration & 105 s/iteration \\
\hline
\end{tabular}
\caption{Scaling results for data-parallel and tensor parallel implementation.}
\label{tab:S6}
\end{table}
\newpage
\bibliographystyle{plain}
\bibliography{main_arxiv}
%\bibliographystyle{apalike}



\end{document}

