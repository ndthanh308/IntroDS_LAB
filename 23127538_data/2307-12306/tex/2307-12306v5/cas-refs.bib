% scibib.bib

% This is the .bib file used to compile the document "A simple Science
% template" (scifile.tex).  It is not intended as an example of how to
% set up your BibTeX file.




@misc{tth, note = "The package is TTH, available at
http://hutchinson.belmont.ma.us/tth/ ."}

@misc{use2e, note = "As the mark-up of the \TeX\ source for this
document makes clear, your file should be coded in \LaTeX
2${\varepsilon}$, not \LaTeX\ 2.09 or an earlier release.  Also,
please use the \texttt{article} document class."}

@misc{inclme, note="Among whom are the author of this document.  The
``real'' references and notes contained herein were compiled using
B{\small{IB}}\TeX\ from the sample .bib file \texttt{scibib.bib}, the style
package \texttt{scicite.sty}, and the bibliography style file
\texttt{Science.bst}."}


@misc{nattex, note="One of the equation editors we use, Equation Magic
(MicroPress Inc., Forest Hills, NY; http://www.micropress-inc.com/),
interprets native \TeX\ source code and generates an equation as an
OLE picture object that can then be cut and pasted directly into Word.
This editor, however, does not handle \LaTeX\ environments (such as
\texttt{\{array\}} or \texttt{\{eqnarray\}}); it can interpret only
\TeX\ codes.  Thus, when there's a choice, we ask that you avoid these
\LaTeX\ calls in displayed math --- for example, that you use the
\TeX\ \verb+\matrix+ command for ordinary matrices, rather than the
\LaTeX\ \texttt{\{array\}} environment."}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{hu2023hutchinson,
title = {Hutchinson Trace Estimation for high-dimensional and high-order Physics-Informed Neural Networks},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {424},
pages = {116883},
year = {2024},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2024.116883},
url = {https://www.sciencedirect.com/science/article/pii/S0045782524001397},
author = {Zheyuan Hu and Zekun Shi and George Em Karniadakis and Kenji Kawaguchi},
keywords = {Physics-Informed Neural Networks, Curse of dimensionality, High-dimensional and high-order partial differential equation, Hutchinson trace estimation},
abstract = {Physics-Informed Neural Networks (PINNs) have proven effective in solving partial differential equations (PDEs), especially when some data are available by seamlessly blending data and physics. However, extending PINNs to high-dimensional and even high-order PDEs encounters significant challenges due to the computational cost associated with automatic differentiation in the residual loss function calculation. Herein, we address the limitations of PINNs in handling high-dimensional and high-order PDEs by introducing the Hutchinson Trace Estimation (HTE) method. Starting with the second-order high-dimensional PDEs, which are ubiquitous in scientific computing, HTE is applied to transform the calculation of the entire Hessian matrix into a Hessian vector product (HVP). This approach not only alleviates the computational bottleneck via Taylor-mode automatic differentiation but also significantly reduces memory consumption from the Hessian matrix to an HVP’s scalar output. We further showcase HTE’s convergence to the original PINN loss and its unbiased behavior under specific conditions. Comparisons with the Stochastic Dimension Gradient Descent (SDGD) highlight the distinct advantages of HTE, particularly in scenarios with significant variability and variance among dimensions. We further extend the application of HTE to higher-order and higher-dimensional PDEs, specifically addressing the biharmonic equation. By employing tensor-vector products (TVP), HTE efficiently computes the colossal tensor associated with the fourth-order high-dimensional biharmonic equation, saving memory and enabling rapid computation. The effectiveness of HTE is illustrated through experimental setups, demonstrating comparable convergence rates with SDGD under memory and speed constraints. Additionally, HTE proves valuable in accelerating the Gradient-Enhanced PINN (gPINN) version as well as the Biharmonic equation. Overall, HTE opens up a new capability in scientific machine learning for tackling high-order and high-dimensional PDEs.}
}

@InProceedings{He_2016_CVPR,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@article{sirignano2018dgm,
  title={DGM: A deep learning algorithm for solving partial differential equations},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={Journal of computational physics},
  volume={375},
  pages={1339--1364},
  year={2018},
  publisher={Elsevier}
}
@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{hu2023bias,
  title={Bias-Variance Trade-off in Physics-Informed Neural Networks with Randomized Smoothing for High-Dimensional PDEs},
  author={Hu, Zheyuan and Yang, Zhouhao and Wang, Yezhen and Karniadakis, George Em and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:2311.15283},
  year={2023}
}

@article{hu2024score,
  title={Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck Equations},
  author={Hu, Zheyuan and Zhang, Zhongqiang and Karniadakis, George Em and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:2402.07465},
  year={2024}
}

@article{Luo2020TwoLayerNN,
  title={Two-Layer Neural Networks for Partial Differential Equations: Optimization and Generalization Theory},
  author={Tao Luo and H. Yang},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.15733}
}

@article{Weinan2020SomeOO,
  title={Some observations on partial differential equations in Barron and multi-layer spaces},
  author={E. Weinan and Stephan Wojtowytsch},
  journal={ArXiv},
  year={2020},
  volume={abs/2012.01484}
}

@article{karniadakis2021physics,
  title={Physics-informed machine learning},
  author={Karniadakis, George Em and Kevrekidis, Ioannis G and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  journal={Nature Reviews Physics},
  volume={3},
  number={6},
  pages={422--440},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{Golowich2018SizeIndependentSC,
  title={Size-Independent Sample Complexity of Neural Networks},
  author={Noah Golowich and A. Rakhlin and O. Shamir},
  booktitle={COLT},
  year={2018}
}

@article{256500,
  author={Barron, A.R.},
  journal={IEEE Transactions on Information Theory}, 
  title={Universal approximation bounds for superpositions of a sigmoidal function}, 
  year={1993},
  volume={39},
  number={3},
  pages={930-945},
  doi={10.1109/18.256500}}
  
@article{Weinan2020OnTB,
  title={On the Banach spaces associated with multi-layer ReLU networks: Function representation, approximation theory and gradient descent dynamics},
  author={E. Weinan and Stephan Wojtowytsch},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.15623}
}

@article{Weinan2017TheDR,
  title={The Deep Ritz Method: A Deep Learning-Based Numerical Algorithm for Solving Variational Problems},
  author={E. Weinan and Ting Yu},
  journal={Communications in Mathematics and Statistics},
  year={2017},
  volume={6},
  pages={1-12}
}

@article{Weinan2018APE,
  title={A Priori Estimates of the Generalization Error for Two-layer Neural Networks},
  author={E. Weinan and Chao Ma and L. Wu},
  journal={ArXiv},
  year={2018},
  volume={abs/1810.06397}
}

@article{Lu2021APG,
  title={A Priori Generalization Analysis of the Deep Ritz Method for Solving High Dimensional Elliptic Equations},
  author={J. Lu and Y. Lu and M. Wang},
  journal={ArXiv},
  year={2021},
  volume={abs/2101.01708}
}

@article{Xu2020TheFN,
  title={The Finite Neuron Method and Convergence Analysis},
  author={J. Xu},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.01458}
}

@article{jagtap2020conservative,
  title={Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems},
  author={Jagtap, Ameya D and Kharazmi, Ehsan and Karniadakis, George Em},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={365},
  pages={113028},
  year={2020},
  publisher={Elsevier}
}

@article{raissi2019physics,
  title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
  journal={Journal of Computational Physics},
  volume={378},
  pages={686--707},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{neyshabur2015norm,
  title={Norm-based capacity control in neural networks},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={1376--1401},
  year={2015},
  organization={PMLR}
}

@inproceedings{sokolic2017generalization,
  title={Generalization error of invariant classifiers},
  author={Sokolic, Jure and Giryes, Raja and Sapiro, Guillermo and Rodrigues, Miguel},
  booktitle={Artificial Intelligence and Statistics},
  pages={1094--1103},
  year={2017},
  organization={PMLR}
}

@article{xu2012robustness,
  title={Robustness and generalization},
  author={Xu, Huan and Mannor, Shie},
  journal={Machine learning},
  volume={86},
  number={3},
  pages={391--423},
  year={2012},
  publisher={Springer}
}

@article{lu2019deeponet,
  title={Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators},
  author={Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em},
  journal={Nature machine intelligence},
  volume={3},
  number={3},
  pages={218--229},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{raissi2018hidden,
  title={Hidden physics models: Machine learning of nonlinear partial differential equations},
  author={Raissi, Maziar and Karniadakis, George Em},
  journal={Journal of Computational Physics},
  volume={357},
  pages={125--141},
  year={2018},
  publisher={Elsevier}
}

@article{yang2019adversarial,
  title={Adversarial uncertainty quantification in physics-informed neural networks},
  author={Yang, Yibo and Perdikaris, Paris},
  journal={Journal of Computational Physics},
  volume={394},
  pages={136--152},
  year={2019},
  publisher={Elsevier}
}
@article{jagtap2022deep,
  title={Deep learning of inverse water waves problems using multi-fidelity data: Application to Serre--Green--Naghdi equations},
  author={Jagtap, Ameya D and Mitsotakis, Dimitrios and Karniadakis, George Em},
  journal={Ocean Engineering},
  volume={248},
  pages={110775},
  year={2022},
  publisher={Elsevier}
}
@article{shukla2023deep,
  title={Deep neural operators can serve as accurate surrogates for shape optimization: a case study for airfoils},
  author={Shukla, Khemraj and Oommen, Vivek and Peyvan, Ahmad and Penwarden, Michael and Bravo, Luis and Ghoshal, Anindya and Kirby, Robert M and Karniadakis, George Em},
  journal={arXiv preprint arXiv:2302.00807},
  year={2023}
}
@article{cai2021physics,
  title={Physics-informed neural networks (PINNs) for fluid mechanics: A review},
  author={Cai, Shengze and Mao, Zhiping and Wang, Zhicheng and Yin, Minglang and Karniadakis, George Em},
  journal={Acta Mechanica Sinica},
  volume={37},
  number={12},
  pages={1727--1738},
  year={2021},
  publisher={Springer}
}
@article{goswami2022physics,
  title={A physics-informed variational DeepONet for predicting crack path in quasi-brittle materials},
  author={Goswami, Somdatta and Yin, Minglang and Yu, Yue and Karniadakis, George Em},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={391},
  pages={114587},
  year={2022},
  publisher={Elsevier}
}
@article{goswami2023learning,
  title={Learning stiff chemical kinetics using extended deep neural operators},
  author={Goswami, Somdatta and Jagtap, Ameya D and Babaee, Hessam and Susi, Bryan T and Karniadakis, George Em},
  journal={arXiv preprint arXiv:2302.12645},
  year={2023}
}
@article{haghighat2021physics,
  title={A physics-informed deep learning framework for inversion and surrogate modeling in solid mechanics},
  author={Haghighat, Ehsan and Raissi, Maziar and Moure, Adrian and Gomez, Hector and Juanes, Ruben},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={379},
  pages={113741},
  year={2021},
  publisher={Elsevier}
}
@article{mertikopoulos2020almost,
  title={On the almost sure convergence of stochastic gradient descent in non-convex problems},
  author={Mertikopoulos, Panayotis and Hallak, Nadav and Kavis, Ali and Cevher, Volkan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1117--1128},
  year={2020}
}
@article{fehrman2020convergence,
  title={Convergence rates for the stochastic gradient descent method for non-convex objective functions},
  author={Fehrman, Benjamin and Gess, Benjamin and Jentzen, Arnulf},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5354--5401},
  year={2020},
  publisher={JMLRORG}
}
@article{wang2022tensor,
  title={Tensor Neural Network and Its Numerical Integration},
  author={Wang, Yifan and Jin, Pengzhan and Xie, Hehu},
  journal={arXiv preprint arXiv:2207.02754},
  year={2022}
}
@article{wang2022solving,
  title={Solving Schr$\backslash$"$\{$o$\}$ dinger Equation Using Tensor Neural Network},
  author={Wang, Yifan and Liao, Yangfei and Xie, Hehu},
  journal={arXiv preprint arXiv:2209.12572},
  year={2022}
}
@article{lei2019stochastic,
  title={Stochastic gradient descent for nonconvex learning without bounded gradient assumptions},
  author={Lei, Yunwen and Hu, Ting and Li, Guiying and Tang, Ke},
  journal={IEEE transactions on neural networks and learning systems},
  volume={31},
  number={10},
  pages={4394--4400},
  year={2019},
  publisher={IEEE}
}
@article{li2020neural,
  title={Neural operator: Graph kernel network for partial differential equations},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2003.03485},
  year={2020}
}
@inproceedings{
li2021fourier,
title={Fourier Neural Operator for Parametric Partial Differential Equations},
author={Zongyi Li and Nikola Borislavov Kovachki and Kamyar Azizzadenesheli and Burigede liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=c8P9NQVtmnO}
}
@article{li2021physics,
  title={Physics-informed neural operator for learning partial differential equations},
  author={Li, Zongyi and Zheng, Hongkai and Kovachki, Nikola and Jin, David and Chen, Haoxuan and Liu, Burigede and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2111.03794},
  year={2021}
}
@article{JMLR:v24:21-1524,
  author  = {Nikola Kovachki and Zongyi Li and Burigede Liu and Kamyar Azizzadenesheli and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
  title   = {Neural Operator: Learning Maps Between Function Spaces With Applications to PDEs},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {89},
  pages   = {1--97},
  url     = {http://jmlr.org/papers/v24/21-1524.html}
}
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{goswami2022physicsinformed,
  title={Physics-informed neural operators},
  author={Goswami, Somdatta and Bora, Aniruddha and Yu, Yue and Karniadakis, George Em},
  journal={arXiv preprint arXiv:2207.05748},
  year={2022}
}
@article{neyshabur2017pac,
  title={A pac-bayesian approach to spectrally-normalized margin bounds for neural networks},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and Srebro, Nathan},
  journal={arXiv preprint arXiv:1707.09564},
  year={2017}
}

@article{lu2021comprehensive,
  title={A comprehensive and fair comparison of two neural operators (with practical extensions) based on FAIR data},
  author={Lu, Lu and Meng, Xuhui and Cai, Shengze and Mao, Zhiping and Goswami, Somdatta and Zhang, Zhongqiang and Karniadakis, George Em},
  journal={arXiv preprint arXiv:2111.05512},
  year={2021}
}

@article{wei2019data,
  title={Data-dependent sample complexity of deep neural networks via lipschitz augmentation},
  author={Wei, Colin and Ma, Tengyu},
  journal={arXiv preprint arXiv:1905.03684},
  year={2019}
}

@inproceedings{arora2018stronger,
  title={Stronger generalization bounds for deep nets via a compression approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  booktitle={International Conference on Machine Learning},
  pages={254--263},
  year={2018},
  organization={PMLR}
}

@article{pisier1981remarques,
  title={Remarques sur un r{\'e}sultat non publi{\'e} de B. Maurey},
  author={Pisier, Gilles},
  journal={S{\'e}minaire Analyse fonctionnelle (dit},
  pages={1--12},
  year={1981},
  publisher={Ecole Polytechnique, Centre de Mathematiques}
}

@article{han2018solving,
  title={Solving high-dimensional partial differential equations using deep learning},
  author={Han, Jiequn and Jentzen, Arnulf and E, Weinan},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={34},
  pages={8505--8510},
  year={2018},
  publisher={National Acad Sciences}
}

@inproceedings{
wang20222,
title={Is \$L{\textasciicircum}2\$ Physics Informed Loss Always Suitable for Training Physics Informed Neural Network?},
author={Chuwei Wang and Shanda Li and Di He and Liwei Wang},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=cy1TKLRAEML}
}

@inproceedings{he2023learning,
  title={Learning physics-informed neural networks without stacked back-propagation},
  author={He, Di and Li, Shanda and Shi, Wenlei and Gao, Xiaotian and Zhang, Jia and Bian, Jiang and Wang, Liwei and Liu, Tie-Yan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3034--3047},
  year={2023},
  organization={PMLR}
}

@article{chinesta2011short,
  title={A short review on model order reduction based on proper generalized decomposition},
  author={Chinesta, Francisco and Ladeveze, Pierre and Cueto, Elias},
  journal={Archives of Computational Methods in Engineering},
  volume={18},
  number={4},
  pages={395},
  year={2011},
  publisher={Springer}
}

@article{perdikaris2016multifidelity,
  title={Multifidelity information fusion algorithms for high-dimensional systems and massive data sets},
  author={Perdikaris, Paris and Venturi, Daniele and Karniadakis, George Em},
  journal={SIAM Journal on Scientific Computing},
  volume={38},
  number={4},
  pages={B521--B538},
  year={2016},
  publisher={SIAM}
}

@article{hu2022augmented,
  title={Augmented Physics-Informed Neural Networks (APINNs): A gating network-based soft domain decomposition methodology},
  author={Hu, Zheyuan and Jagtap, Ameya D and Karniadakis, George Em and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:2211.08939},
  year={2022}
}

@inproceedings{kawaguchi2022robustness,
  title={Robustness implies generalization via data-dependent generalization bounds},
  author={Kawaguchi, Kenji and Deng, Zhun and Luh, Kyle and Huang, Jiaoyang},
  booktitle={International Conference on Machine Learning},
  pages={10866--10894},
  year={2022},
  organization={PMLR}
}

@article{hu2021extended,
author = {Hu, Zheyuan and Jagtap, Ameya D. and Karniadakis, George Em and Kawaguchi, Kenji},
title = {When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?},
journal = {SIAM Journal on Scientific Computing},
volume = {44},
number = {5},
pages = {A3158-A3182},
year = {2022},
doi = {10.1137/21M1447039},

URL = { 
    
        https://doi.org/10.1137/21M1447039
    
    

},
eprint = { 
    
        https://doi.org/10.1137/21M1447039
    
    

}
,
    abstract = { Physics-informed neural networks (PINNs) have become a popular choice for solving high-dimensional partial differential equations (PDEs) due to their excellent approximation power and generalization ability. Recently, extended PINNs (XPINNs) based on domain decomposition methods have attracted considerable attention due to their effectiveness in modeling multiscale and multiphysics problems and their parallelization. However, theoretical understanding of their convergence and generalization properties remains unexplored. In this study, we take an initial step towards understanding how and when XPINNs outperform PINNs. Specifically, for general multilayer PINNs and XPINNs, we first provide a prior generalization bound via the complexity of the target functions in the PDE problem and a posterior generalization bound via the posterior matrix norms of the networks after optimization. Moreover, based on our bounds, we analyze the conditions under which XPINNs improve generalization. Concretely, our theory shows that the key building block of XPINN, namely, the domain decomposition, introduces a tradeoff for generalization. On the one hand, XPINNs decompose the complex PDE solution into several simple parts, which decreases the complexity needed to learn each part and boosts generalization. On the other hand, decomposition leads to less training data being available in each subdomain, and hence such a model is typically prone to overfitting and may become less generalizable. Empirically, we choose five PDEs to show when XPINNs perform better than, similar to, or worse than PINNs, hence demonstrating and justifying our new theory. }
}


@inproceedings{
anonymous2022multilevel,
title={Multilevel physics informed neural networks ({MPINN}s)},
author={Anonymous},
booktitle={Submitted to The Tenth International Conference on Learning Representations },
year={2022},
url={https://openreview.net/forum?id=g5odb-gVVZY},
note={under review}
}

@article{lu2021priori,
  title={A priori generalization analysis of the deep ritz method for solving high dimensional elliptic equations},
  author={Lu, Jianfeng and Lu, Yulong and Wang, Min},
  journal={arXiv preprint arXiv:2101.01708},
  year={2021}
}

@article{shin2020convergence,
  title={On the convergence of physics informed neural networks for linear second-order elliptic and parabolic type PDEs},
  author={Shin, Yeonjong and Darbon, Jerome and Karniadakis, George Em},
  journal={arXiv preprint arXiv:2004.01806},
  year={2020}
}
@article{shin2020error,
  title={Error estimates of residual minimization using neural networks for linear PDEs},
  author={Shin, Yeonjong and Zhang, Zhongqiang and Karniadakis, George Em},
  journal={arXiv preprint arXiv:2010.08019},
  year={2020}
}
@incollection{bochev2016least,
  title={Least-squares methods for hyperbolic problems},
  author={Bochev, P and Gunzburger, M},
  booktitle={Handbook of Numerical Analysis},
  volume={17},
  pages={289--317},
  year={2016},
  publisher={Elsevier}
}

@article{de2022error,
  title={Error estimates for physics informed neural networks approximating the Navier-Stokes equations},
  author={De Ryck, Tim and Jagtap, Ameya D and Mishra, Siddhartha},
  journal={arXiv preprint arXiv:2203.09346},
  year={2022}
}
@article{Ryck2021ErrorAF,
  title={Error analysis for physics informed neural networks (PINNs) approximating Kolmogorov PDEs},
  author={Tim De Ryck and Siddhartha Mishra},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.14473}
}
@article{bramble1970rayleigh,
  title={Rayleigh-Ritz-Galerkin methods for dirichlet's problem using subspaces without boundary conditions},
  author={Bramble, James H and Schatz, Alfred H},
  journal={Communications on Pure and Applied Mathematics},
  volume={23},
  number={4},
  pages={653--675},
  year={1970},
  publisher={Wiley Online Library}
}

@article{mishra2020estimates,
  title={Estimates on the generalization error of physics informed neural networks (PINNs) for approximating PDEs},
  author={Mishra, Siddhartha and Molinaro, Roberto},
  journal={arXiv preprint arXiv:2006.16144},
  year={2020}
}

@article{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
  journal={arXiv preprint arXiv:1706.08947},
  year={2017}
}

@article{bartlett2017spectrally,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter and Foster, Dylan J and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1706.08498},
  year={2017}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={ICLR},
  year={2015}
}

@inproceedings{martens2020neural,
  title={Neural decomposition: Functional anova with variational autoencoders},
  author={M{\"a}rtens, Kaspar and Yau, Christopher},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2917--2927},
  year={2020},
  organization={PMLR}
}

@article{zhang2012error,
  title={Error estimates for the ANOVA method with polynomial chaos interpolation: Tensor product functions},
  author={Zhang, Zhongqiang and Choi, Minseok and Karniadakis, George Em},
  journal={SIAM Journal on Scientific Computing},
  volume={34},
  number={2},
  pages={A1165--A1186},
  year={2012},
  publisher={SIAM}
}

@article{han2017deep,
  title={Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations},
  author={Han, Jiequn and Jentzen, Arnulf and others},
  journal={Communications in mathematics and statistics},
  volume={5},
  number={4},
  pages={349--380},
  year={2017},
  publisher={Springer}
}

@article{raissi2018forward,
  title={Forward-backward stochastic neural networks: Deep learning of high-dimensional partial differential equations},
  author={Raissi, Maziar},
  journal={arXiv preprint arXiv:1804.07010},
  year={2018}
}

@book{bellman1966dynamic,
  title={Dynamic programming},
  author={Bellman, Richard},
  journal={Princeton University Press},
  year={1957},
  publisher={Princeton University Press}
}

@article{donoho2000high,
  title={High-dimensional data analysis: The curses and blessings of dimensionality},
  author={Donoho, David L and others},
  journal={AMS math challenges lecture},
  volume={1},
  number={2000},
  pages={32},
  year={2000}
}

@misc{kainen1997utilizing,
  title={Utilizing geometric anomalies of high dimension: When complexity makes computation easier},
  author={Kainen, Paul C},
  year={1997},
  publisher={Springer}
}

@article{gorban2020high,
  title={High-dimensional brain in a high-dimensional world: Blessing of dimensionality},
  author={Gorban, Alexander N and Makarov, Valery A and Tyukin, Ivan Y},
  journal={Entropy},
  volume={22},
  number={1},
  pages={82},
  year={2020},
  publisher={MDPI}
}

@article{gorban2018blessing,
  title={Blessing of dimensionality: mathematical foundations of the statistical physics of data},
  author={Gorban, Alexander N and Tyukin, Ivan Yu},
  journal={Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume={376},
  number={2118},
  pages={20170237},
  year={2018},
  publisher={The Royal Society Publishing}
}

@article{zimek2012survey,
  title={A survey on unsupervised outlier detection in high-dimensional numerical data},
  author={Zimek, Arthur and Schubert, Erich and Kriegel, Hans-Peter},
  journal={Statistical Analysis and Data Mining: The ASA Data Science Journal},
  volume={5},
  number={5},
  pages={363--387},
  year={2012},
  publisher={Wiley Online Library}
}

@article{raissi2017physics,
  title={Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  journal={arXiv preprint arXiv:1711.10561},
  year={2017}
}

@book{bellman1961,
 ISBN = {9780691079011},
 URL = {http://www.jstor.org/stable/j.ctt183ph6v},
 abstract = {The book description for "Adaptive Control Processes" is currently unavailable.},
 author = {Richard Bellman},
 publisher = {Princeton University Press},
 title = {Adaptive Control Processes: A Guided Tour},
 urldate = {2023-07-17},
 year = {1961}
}


@misc{hammer1962adaptive,
  title={Adaptive control processes: a guided tour (R. Bellman)},
  author={Hammer, PC},
  year={1962},
  publisher={Society for Industrial and Applied Mathematics}
}

@article{zhang2020learning,
  title={Learning in modal space: Solving time-dependent stochastic PDEs using physics-informed neural networks},
  author={Zhang, Dongkun and Guo, Ling and Karniadakis, George Em},
  journal={SIAM Journal on Scientific Computing},
  volume={42},
  number={2},
  pages={A639--A665},
  year={2020},
  publisher={SIAM}
}

@article{hutzenthaler2021multilevel,
  title={Multilevel Picard iterations for solving smooth semilinear parabolic heat equations},
  author={Hutzenthaler, Martin and Jentzen, Arnulf and Kruse, Thomas and others},
  journal={Partial Differential Equations and Applications},
  volume={2},
  number={6},
  pages={1--31},
  year={2021},
  publisher={Springer}
}

@article{zang2020weak,
  title={Weak adversarial networks for high-dimensional partial differential equations},
  author={Zang, Yaohua and Bao, Gang and Ye, Xiaojing and Zhou, Haomin},
  journal={Journal of Computational Physics},
  volume={411},
  pages={109409},
  year={2020},
  publisher={Elsevier}
}

@article{beck2020overcoming,
  title={Overcoming the curse of dimensionality in the numerical approximation of high-dimensional semilinear elliptic partial differential equations},
  author={Beck, Christian and Gonon, Lukas and Jentzen, Arnulf},
  journal={arXiv preprint arXiv:2003.00596},
  year={2020}
}

@article{beck2020overcoming_ac,
  title={Overcoming the curse of dimensionality in the numerical approximation of Allen--Cahn partial differential equations via truncated full-history recursive multilevel Picard approximations},
  author={Beck, Christian and Hornung, Fabian and Hutzenthaler, Martin and Jentzen, Arnulf and Kruse, Thomas},
  journal={Journal of Numerical Mathematics},
  volume={28},
  number={4},
  pages={197--222},
  year={2020},
  publisher={De Gruyter}
}

@article{beck2019machine,
  title={Machine learning approximation algorithms for high-dimensional fully nonlinear partial differential equations and second-order backward stochastic differential equations},
  author={Beck, Christian and E, Weinan and Jentzen, Arnulf},
  journal={Journal of Nonlinear Science},
  volume={29},
  pages={1563--1619},
  year={2019},
  publisher={Springer}
}

@article{becker2021solving,
  title={Solving high-dimensional optimal stopping problems using deep learning},
  author={Becker, Sebastian and Cheridito, Patrick and Jentzen, Arnulf and Welti, Timo},
  journal={European Journal of Applied Mathematics},
  volume={32},
  number={3},
  pages={470--514},
  year={2021},
  publisher={Cambridge University Press}
}

@article{chan2019machine,
  title={Machine learning for semi linear PDEs},
  author={Chan-Wai-Nam, Quentin and Mikael, Joseph and Warin, Xavier},
  journal={Journal of scientific computing},
  volume={79},
  number={3},
  pages={1667--1712},
  year={2019},
  publisher={Springer}
}

@article{hure2020deep,
  title={Deep backward schemes for high-dimensional nonlinear PDEs},
  author={Hur{\'e}, C{\^o}me and Pham, Huy{\^e}n and Warin, Xavier},
  journal={Mathematics of Computation},
  volume={89},
  number={324},
  pages={1547--1579},
  year={2020}
}

@article{henry2017deep,
  title={Deep primal-dual algorithm for BSDEs: Applications of machine learning to CVA and IM},
  author={Henry-Labordere, Pierre},
  journal={Available at SSRN 3071506},
  year={2017}
}

@article{bungartz_griebel_2004, title={Sparse grids}, volume={13}, DOI={10.1017/S0962492904000182}, journal={Acta Numerica}, publisher={Cambridge University Press}, author={Bungartz, Hans-Joachim and Griebel, Michael}, year={2004}, pages={147–269}}

@article{berger1984adaptive,
  title={Adaptive mesh refinement for hyperbolic partial differential equations},
  author={Berger, Marsha J and Oliger, Joseph},
  journal={Journal of computational Physics},
  volume={53},
  number={3},
  pages={484--512},
  year={1984},
  publisher={Elsevier}
}

@article{ji2020three,
  title={Three algorithms for solving high-dimensional fully coupled FBSDEs through deep learning},
  author={Ji, Shaolin and Peng, Shige and Peng, Ying and Zhang, Xichuan},
  journal={IEEE Intelligent Systems},
  volume={35},
  number={3},
  pages={71--84},
  year={2020},
  publisher={IEEE}
}

@article{hutzenthaler2020overcoming,
  title={Overcoming the curse of dimensionality in the numerical approximation of semilinear parabolic partial differential equations},
  author={Hutzenthaler, Martin and Jentzen, Arnulf and Kruse, Thomas and Anh Nguyen, Tuan and von Wurstemberger, Philippe},
  journal={Proceedings of the Royal Society A},
  volume={476},
  number={2244},
  pages={20190630},
  year={2020},
  publisher={The Royal Society Publishing}
}

@article{becker2020numerical,
  title={Numerical simulations for full history recursive multilevel Picard approximations for systems of high-dimensional partial differential equations},
  author={Becker, Sebastian and Braunwarth, Ramon and Hutzenthaler, Martin and Jentzen, Arnulf and von Wurstemberger, Philippe},
  journal={arXiv preprint arXiv:2005.10206},
  year={2020}
}

@article{pham2021combined,
  title={Combined scaling for zero-shot transfer learning},
  author={Pham, Hieu and Dai, Zihang and Ghiasi, Golnaz and Liu, Hanxiao and Yu, Adams Wei and Luong, Minh-Thang and Tan, Mingxing and Le, Quoc V},
  journal={arXiv preprint arXiv:2111.10050},
  year={2021}
}

@article{darbon2016algorithms,
  title={Algorithms for overcoming the curse of dimensionality for certain Hamilton--Jacobi equations arising in control theory and elsewhere},
  author={Darbon, J{\'e}r{\^o}me and Osher, Stanley},
  journal={Research in the Mathematical Sciences},
  volume={3},
  number={1},
  pages={19},
  year={2016},
  publisher={Springer}
}

@article{shafahi2019adversarial,
  title={Adversarial training for free!},
  author={Shafahi, Ali and Najibi, Mahyar and Ghiasi, Mohammad Amin and Xu, Zheng and Dickerson, John and Studer, Christoph and Davis, Larry S and Taylor, Gavin and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{cho2022separable,
  title={Separable PINN: Mitigating the Curse of Dimensionality in Physics-Informed Neural Networks},
  author={Cho, Junwoo and Nam, Seungtae and Yang, Hyunmo and Yun, Seok-Bae and Hong, Youngjoon and Park, Eunbyung},
  journal={arXiv preprint arXiv:2211.08761},
  year={2022}
}

@inproceedings{
bettencourt2019taylormode,
title={Taylor-Mode Automatic Differentiation for Higher-Order Derivatives in {JAX}},
author={Jesse Bettencourt and Matthew J. Johnson and David Duvenaud},
booktitle={Program Transformations for ML Workshop at NeurIPS 2019},
year={2019},
url={https://openreview.net/forum?id=SkxEF3FNPH}
}

@inproceedings{gander2017definition,
  title={On the definition of Dirichlet and Neumann conditions for the biharmonic equation and its impact on associated Schwarz methods},
  author={Gander, Martin J and Liu, Yongxiang},
  booktitle={Domain Decomposition Methods in Science and Engineering XXIII},
  pages={303--311},
  year={2017},
  organization={Springer}
}

@article{karageorghis1987method,
  title={The method of fundamental solutions for the numerical solution of the biharmonic equation},
  author={Karageorghis, Andreas and Fairweather, Graeme},
  journal={Journal of Computational Physics},
  volume={69},
  number={2},
  pages={434--459},
  year={1987},
  publisher={Elsevier}
}

@article{jagtap2020extended,
  title={Extended Physics-Informed Neural Networks (XPINNs): A Generalized Space-Time Domain Decomposition Based Deep Learning Framework for Nonlinear Partial Differential Equations},
  author={Jagtap, Ameya D and Karniadakis, George Em},
  journal={Communications in Computational Physics},
  volume={28},
  number={5},
  pages={2002--2041},
  year={2020},
  publisher={GLOBAL SCIENCE PRESS ROOM 3208, CENTRAL PLAZA, 18 HARBOUR RD, WANCHAI, HONG~…}
}

@article{Jacot2018NeuralTK,
  title={Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  author={Arthur Jacot and F. Gabriel and Cl{\'e}ment Hongler},
  journal={ArXiv},
  year={2018},
  volume={abs/1806.07572}
}

@article{Du2019GradientDP,
  title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  author={S. Du and Xiyu Zhai and B. P{\'o}czos and A. Singh},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.02054}
}

@article{Yang2019AFS,
  title={A Fine-Grained Spectral Perspective on Neural Networks},
  author={Greg Yang and Hadi Salman},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.10599}
}

@inproceedings{Rahaman2019OnTS,
  title={On the Spectral Bias of Neural Networks},
  author={N. Rahaman and A. Baratin and D. Arpit and Felix Dr{\"a}xler and M. Lin and F. Hamprecht and Yoshua Bengio and Aaron C. Courville},
  booktitle={ICML},
  year={2019}
}

@inproceedings{
chen2021deep,
title={Deep Neural Tangent Kernel and Laplace Kernel Have the Same {\{}RKHS{\}}},
author={Lin Chen and Sheng Xu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=vK9WrZ0QYQ}
}

@article{beck2021deep,
  title={Deep splitting method for parabolic PDEs},
  author={Beck, Christian and Becker, Sebastian and Cheridito, Patrick and Jentzen, Arnulf and Neufeld, Ariel},
  journal={SIAM Journal on Scientific Computing},
  volume={43},
  number={5},
  pages={A3135--A3154},
  year={2021},
  publisher={SIAM}
}

@article{pang2019fpinns,
  title={fPINNs: Fractional physics-informed neural networks},
  author={Pang, Guofei and Lu, Lu and Karniadakis, George Em},
  journal={SIAM Journal on Scientific Computing},
  volume={41},
  number={4},
  pages={A2603--A2626},
  year={2019},
  publisher={SIAM}
}

@article{Yarotsky2017ErrorBF,
  title={Error bounds for approximations with deep ReLU networks},
  author={D. Yarotsky},
  journal={Neural networks : the official journal of the International Neural Network Society},
  year={2017},
  volume={94},
  pages={
          103-114
        }
}

@article{Chen2020AGN,
  title={A Generalized Neural Tangent Kernel Analysis for Two-layer Neural Networks},
  author={Z. Chen and Y. Cao and Quanquan Gu and Tong Zhang},
  journal={arXiv: Learning},
  year={2020}
}

@inproceedings{Chizat2018OnTG,
  title={On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport},
  author={L{\'e}na{\"i}c Chizat and F. Bach},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{kawaguchi2016deep,
title={Deep Learning without Poor Local Minima},
author={Kawaguchi, Kenji},
booktitle={Advances in neural information processing systems (NeurIPS)},
pages={586--594},
year={2016}
}

@inproceedings{kawaguchi2021theory,
   title={On the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers},
   author={Kawaguchi, Kenji},
   booktitle={International Conference on Learning Representations (ICLR)},
   year={2021}
}

@inproceedings{xu2021graph,
   title={Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth},
   author={Keyulu Xu and Mozhi Zhang and Stefanie Jegelka and Kenji Kawaguchi},
   booktitle={International Conference on Machine Learning (ICML)},
   year={2021}
}

@article{Nguyen2020ARF,
  title={A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks},
  author={Phan-Minh Nguyen and Huy Tuan Pham},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.11443}
}

@article{Arora2019ACA,
  title={A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks},
  author={Sanjeev Arora and N. Cohen and Noah Golowich and Wei Hu},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.02281}
}

@article{Chen2019HowMO,
  title={How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?},
  author={Z. Chen and Yuan Cao and Difan Zou and Quanquan Gu},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.12360}
}

@inproceedings{Mei2019MeanfieldTO,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Song Mei and Theodor Misiakiewicz and A. Montanari},
  booktitle={COLT},
  year={2019}
}

@article{Telgarsky2016BenefitsOD,
  title={Benefits of Depth in Neural Networks},
  author={Matus Telgarsky},
  journal={ArXiv},
  year={2016},
  volume={abs/1602.04485}
}
@inproceedings{He2018ReLUDN,
  title={ReLU Deep Neural Networks and Linear Finite Elements},
  author={Juncai He and L. Li and J. Xu and Chunyue Zheng},
  year={2018}
}
@inproceedings{Lu2017TheEP,
  title={The Expressive Power of Neural Networks: A View from the Width},
  author={Zhou Lu and Hongming Pu and F. Wang and Zhiqiang Hu and L. Wang},
  booktitle={NIPS},
  year={2017}
}
@article{Mo2014SupportVM,
  title={Support vector machine adapted Tikhonov regularization method to solve Dirichlet problem},
  author={Yan Mo and T. Qian},
  journal={Appl. Math. Comput.},
  year={2014},
  volume={245},
  pages={509-519}
}

@article{Geifman2020OnTS,
  title={On the Similarity between the Laplace and Neural Tangent Kernels},
  author={Amnon Geifman and A. Yadav and Yoni Kasten and M. Galun and D. Jacobs and R. Basri},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.01580}
}

@inproceedings{
bietti2021deep,
title={Deep Equals Shallow for Re{\{}LU{\}} Networks in Kernel Regimes},
author={Alberto Bietti and Francis Bach},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=aDjoksTpXOP}
}

@article{Wang2020ImplicitBW,
  title={Implicit bias with Ritz-Galerkin method in understanding deep learning for solving PDEs},
  author={J. Wang and Zhi-Qin John Xu and Jiwei Zhang and Yaoyu Zhang},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.07989}
}

@article{Cao2019TowardsUT,
  title={Towards Understanding the Spectral Bias of Deep Learning},
  author={Yuan Cao and Zhiying Fang and Yue Wu and Ding-Xuan Zhou and Quanquan Gu},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.01198}
}

@inproceedings{Arora2019FineGrainedAO,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Sanjeev Arora and S. Du and Wei Hu and Zhiyuan Li and Ruosong Wang},
  booktitle={ICML},
  year={2019}
}

@article{yue2018deep,
  title={Deep learning for genomics: A concise overview},
  author={Yue, Tianwei and Wang, Haohan},
  journal={arXiv preprint arXiv:1802.00810},
  year={2018}
}

@article{cohn2018enhancer,
  title={Enhancer identification using transfer and adversarial deep learning of DNA sequences},
  author={Cohn, Dikla and Zuk, Or and Kaplan, Tommy},
  journal={BioRxiv},
  pages={264200},
  year={2018},
  publisher={Cold Spring Harbor Laboratory}
}

@article{howard2018universal,
  title={Universal language model fine-tuning for text classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018}
}

@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of AAAI},
  year={2021}
}

@article{ji2021dnabert,
  title={DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome},
  author={Ji, Yanrong and Zhou, Zhihan and Liu, Han and Davuluri, Ramana V},
  journal={Bioinformatics},
  volume={37},
  number={15},
  pages={2112--2120},
  year={2021},
  publisher={Oxford University Press}
}

@article{wu2021phylotransformer,
  title={PhyloTransformer: A Discriminative Model for Mutation Prediction Based on a Multi-head Self-attention Mechanism},
  author={Wu, Yingying and Xu, Shusheng and Yau, Shing-Tung and Wu, Yi},
  journal={arXiv preprint arXiv:2111.01969},
  year={2021}
}

@article{kelley2016basset,
  title={Basset: learning the regulatory code of the accessible genome with deep convolutional neural networks},
  author={Kelley, David R and Snoek, Jasper and Rinn, John L},
  journal={Genome research},
  volume={26},
  number={7},
  pages={990--999},
  year={2016},
  publisher={Cold Spring Harbor Lab}
}

@article{quang2016danq,
  title={DanQ: a hybrid convolutional and recurrent deep neural network for quantifying the function of DNA sequences},
  author={Quang, Daniel and Xie, Xiaohui},
  journal={Nucleic acids research},
  volume={44},
  number={11},
  pages={e107--e107},
  year={2016},
  publisher={Oxford University Press}
}

@inproceedings{tan2014unsupervised,
  title={Unsupervised feature construction and knowledge extraction from genome-wide assays of breast cancer with denoising autoencoders},
  author={Tan, Jie and Ung, Matthew and Cheng, Chao and Greene, Casey S},
  booktitle={Pacific symposium on biocomputing co-chairs},
  pages={132--143},
  year={2014},
  organization={World Scientific}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{he2019rethinking,
  title={Rethinking imagenet pre-training},
  author={He, Kaiming and Girshick, Ross and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4918--4927},
  year={2019}
}

@inproceedings{xuhong2018explicit,
  title={Explicit inductive bias for transfer learning with convolutional networks},
  author={Xuhong, LI and Grandvalet, Yves and Davoine, Franck},
  booktitle={International Conference on Machine Learning},
  pages={2825--2834},
  year={2018},
  organization={PMLR}
}

@inproceedings{NEURIPS2019_c6bff625,
 author = {Chen, Xinyang and Wang, Sinan and Fu, Bo and Long, Mingsheng and Wang, Jianmin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Catastrophic Forgetting Meets Negative Transfer: Batch Spectral Shrinkage for Safe Transfer Learning},
 url = {https://proceedings.neurips.cc/paper/2019/file/c6bff625bdb0393992c9d4db0c6bbe45-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{NEURIPS2020_bc573864,
 author = {Kou, Zhi and You, Kaichao and Long, Mingsheng and Wang, Jianmin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {16304--16314},
 publisher = {Curran Associates, Inc.},
 title = {Stochastic Normalization},
 url = {https://proceedings.neurips.cc/paper/2020/file/bc573864331a9e42e4511de6f678aa83-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{NEURIPS2020_c8067ad1,
 author = {You, Kaichao and Kou, Zhi and Long, Mingsheng and Wang, Jianmin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {17236--17246},
 publisher = {Curran Associates, Inc.},
 title = {Co-Tuning for Transfer Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/c8067ad1937f728f51288b3eb986afaa-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{zhang2021drop,
  title={Drop redundant, shrink irrelevant: Selective knowledge injection for language pretraining},
  author={Zhang, Ningyu and Deng, Shumin and Cheng, Xu and Chen, Xi and Zhang, Yichi and Zhang, Wei and Chen, Huajun and Center, Hangzhou Innovation},
  booktitle={Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI},
  pages={4007--4014},
  year={2021}
}

@article{chen2020recall,
  title={Recall and learn: Fine-tuning deep pretrained language models with less forgetting},
  author={Chen, Sanyuan and Hou, Yutai and Cui, Yiming and Che, Wanxiang and Liu, Ting and Yu, Xiangzhan},
  journal={arXiv preprint arXiv:2004.12651},
  year={2020}
}
@article{gomez2017reversible,
  title={The reversible residual network: Backpropagation without storing activations},
  author={Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@inproceedings{yin2019rademacher,
  title={Rademacher complexity for adversarially robust generalization},
  author={Yin, Dong and Kannan, Ramchandran and Bartlett, Peter},
  booktitle={International conference on machine learning},
  pages={7085--7094},
  year={2019},
  organization={PMLR}
}
@article{solovyev2016prediction,
  title={Prediction of prokaryotic and eukaryotic promoters using convolutional deep learning neural networks},
  author={Solovyev, Victor and Umarov, Ramzan},
  journal={arXiv preprint arXiv:1610.00121},
  year={2016}
}
@article{hill2018deep,
  title={A deep recurrent neural network discovers complex biological rules to decipher RNA protein-coding potential},
  author={Hill, Steven T and Kuintzle, Rachael and Teegarden, Amy and Merrill III, Erich and Danaee, Padideh and Hendrix, David A},
  journal={Nucleic acids research},
  volume={46},
  number={16},
  pages={8105--8113},
  year={2018},
  publisher={Oxford University Press}
}
@article{wei2019improved,
  title={Improved sample complexities for deep networks and robust classification via an all-layer margin},
  author={Wei, Colin and Ma, Tengyu},
  journal={arXiv preprint arXiv:1910.04284},
  year={2019}
}

@article{hoffman2019robust,
  title={Robust learning with jacobian regularization},
  author={Hoffman, Judy and Roberts, Daniel A and Yaida, Sho},
  journal={arXiv preprint arXiv:1908.02729},
  year={2019}
}
@article{arjovsky2019invariant,
  title={Invariant risk minimization},
  author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1907.02893},
  year={2019}
}

@inproceedings{
zhang2021how,
title={How Does Mixup Help With Robustness and Generalization?},
author={Linjun Zhang and Zhun Deng and Kenji Kawaguchi and Amirata Ghorbani and James Zou},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=8yKEo06dKNo}
}
@article{carratino2020mixup,
  title={On mixup regularization},
  author={Carratino, Luigi and Ciss{\'e}, Moustapha and Jenatton, Rodolphe and Vert, Jean-Philippe},
  journal={arXiv preprint arXiv:2006.06049},
  year={2020}
}
@article{mcclenny2020self,
  title={Self-adaptive physics-informed neural networks using a soft attention mechanism},
  author={McClenny, Levi and Braga-Neto, Ulisses},
  journal={arXiv preprint arXiv:2009.04544},
  year={2020}
}
@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}
@article{moseley2021finite,
  title={Finite Basis Physics-Informed Neural Networks (FBPINNs): a scalable domain decomposition approach for solving differential equations},
  author={Moseley, Ben and Markham, Andrew and Nissen-Meyer, Tarje},
  journal={arXiv preprint arXiv:2107.07871},
  year={2021}
}
@article{sheng2022pfnn,
  title={PFNN-2: A Domain Decomposed Penalty-Free Neural Network Method for Solving Partial Differential Equations},
  author={Sheng, Hailong and Yang, Chao},
  journal={arXiv preprint arXiv:2205.00593},
  year={2022}
}
@inproceedings{stiller2020large,
  title={Large-scale neural solvers for partial differential equations},
  author={Stiller, Patrick and Bethke, Friedrich and B{\"o}hme, Maximilian and Pausch, Richard and Torge, Sunna and Debus, Alexander and Vorberger, Jan and Bussmann, Michael and Hoffmann, Nico},
  booktitle={Smoky Mountains Computational Sciences and Engineering Conference},
  pages={20--34},
  year={2020},
  organization={Springer}
}
@article{shukla2021parallel,
  title={Parallel physics-informed neural networks via domain decomposition},
  author={Shukla, Khemraj and Jagtap, Ameya D and Karniadakis, George Em},
  journal={Journal of Computational Physics},
  volume={447},
  pages={110683},
  year={2021},
  publisher={Elsevier}
}
@article{DWIVEDI2021299,
title = {Distributed learning machines for solving forward and inverse problems in partial differential equations},
journal = {Neurocomputing},
volume = {420},
pages = {299-316},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220314090},
author = {Vikas Dwivedi and Nishant Parashar and Balaji Srinivasan},
keywords = {Distributed physics informed neural networks, Transfer learning, Inverse problems, Burgers’ equation, Navier–Stokes equation},
abstract = {We conceptualize Distributed Learning Machines (DLMs) – a novel machine learning approach that integrates existing machine learning algorithms with traditional mesh-based numerical methods for solving forward and inverse problems in nonlinear partial differential equations (PDEs). In conventional numerical methods such as finite element method (FEM), the discretization of the computational domain is a standard technique to reduce the representation load of basis functions. Along the same lines, we propose a distributed neural network architecture that facilitates the simultaneous deployment of several localized neural networks to solve PDEs in a unified manner. The most critical requirement of the DLMs is the synchronization of the distributed neural networks. For this, we introduce a new physics-based interface regularization term to the cost function of the existing learning machines like the Physics Informed Neural Network (PINN) and the Physics Informed Extreme Learning Machine (PIELM). To evaluate the efficacy of this approach, we develop three distinct variants of DLM namely, time-marching Distributed PIELM (DPIELM), Distributed PINN (DPINN) and time-marching DPINN. We show that ideas of linearization and time-marching allow DPIELM to be able to solve nonlinear PDEs to some extent. Next, we show that DPINNs have potential advantages over existing PINNs to solve the inverse problems in heterogeneous media. Finally, we propose a rapid, time-marching version of DPINN which leverages the ideas of transfer learning to accelerate the training. Collectively, this framework leads towards the promise of hybrid Neural Network-FVM or Neural Network-FEM schemes in the future.}
}
@article{dong2021local,
  title={Local extreme learning machines and domain decomposition for solving linear and nonlinear partial differential equations},
  author={Dong, Suchuan and Li, Zongwei},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={387},
  pages={114129},
  year={2021},
  publisher={Elsevier}
}
@article{li2019d3m,
  title={D3M: A deep domain decomposition method for partial differential equations},
  author={Li, Ke and Tang, Kejun and Wu, Tianfan and Liao, Qifeng},
  journal={IEEE Access},
  volume={8},
  pages={5283--5294},
  year={2019},
  publisher={IEEE}
}
@article{kharazmi2021hp,
  title={hp-VPINNs: Variational physics-informed neural networks with domain decomposition},
  author={Kharazmi, Ehsan and Zhang, Zhongqiang and Karniadakis, George Em},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={374},
  pages={113547},
  year={2021},
  publisher={Elsevier}
}
@inproceedings{li2020deep,
  title={Deep domain decomposition method: Elliptic problems},
  author={Li, Wuyang and Xiang, Xueshuang and Xu, Yingxiang},
  booktitle={Mathematical and Scientific Machine Learning},
  pages={269--286},
  year={2020},
  organization={PMLR}
}
@article{mercier2021coarse,
  title={A coarse space acceleration of deep-DDM},
  author={Mercier, Valentin and Gratton, Serge and Boudier, Pierre},
  journal={arXiv preprint arXiv:2112.03732},
  year={2021}
}
@article{taghibakhshi2022learning,
  title={Learning Interface Conditions in Domain Decomposition Solvers},
  author={Taghibakhshi, Ali and Nytko, Nicolas and Zaman, Tareq and MacLachlan, Scott and Olson, Luke and West, Matthew},
  journal={arXiv preprint arXiv:2205.09833},
  year={2022}
}
@article{li2022deep,
  title={A deep domain decomposition method based on Fourier features},
  author={Li, Sen and Xia, Yingzhi and Liu, Yu and Liao, Qifeng},
  journal={arXiv preprint arXiv:2205.01884},
  year={2022}
}
@article{heinlein2021combining,
  title={Combining machine learning and domain decomposition methods for the solution of partial differential equations—A review},
  author={Heinlein, Alexander and Klawonn, Axel and Lanser, Martin and Weber, Janine},
  journal={GAMM-Mitteilungen},
  volume={44},
  number={1},
  pages={e202100001},
  year={2021},
  publisher={Wiley Online Library}
}
@article{liu2022adaptive,
  title={Adaptive Discrete Communication Bottlenecks with Dynamic Vector Quantization},
  author={Liu, Dianbo and Lamb, Alex and Ji, Xu and Notsawo, Pascal and Mozer, Mike and Bengio, Yoshua and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:2202.01334},
  year={2022}
}
@article{liu2021discrete,
  title={Discrete-valued neural communication},
  author={Liu, Dianbo and Lamb, Alex M and Kawaguchi, Kenji and ALIAS PARTH GOYAL, Anirudh Goyal and Sun, Chen and Mozer, Michael C and Bengio, Yoshua},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2109--2121},
  year={2021}
}
@article{meng2020ppinn,
  title={PPINN: Parareal physics-informed neural network for time-dependent PDEs},
  author={Meng, Xuhui and Li, Zhen and Zhang, Dongkun and Karniadakis, George Em},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={370},
  pages={113250},
  year={2020},
  publisher={Elsevier}
}
@article{wang2021understanding,
  title={Understanding and mitigating gradient flow pathologies in physics-informed neural networks},
  author={Wang, Sifan and Teng, Yujun and Perdikaris, Paris},
  journal={SIAM Journal on Scientific Computing},
  volume={43},
  number={5},
  pages={A3055--A3081},
  year={2021},
  publisher={SIAM}
}
@article{wang2022and,
  title={When and why PINNs fail to train: A neural tangent kernel perspective},
  author={Wang, Sifan and Yu, Xinling and Perdikaris, Paris},
  journal={Journal of Computational Physics},
  volume={449},
  pages={110768},
  year={2022},
  publisher={Elsevier}
}
@inproceedings{NEURIPS2020_55053683,
 author = {Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {7537--7547},
 publisher = {Curran Associates, Inc.},
 title = {Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains},
 url = {https://proceedings.neurips.cc/paper/2020/file/55053683268957697aa39fba6f231c68-Paper.pdf},
 volume = {33},
 year = {2020}
}
@article{lin2022two,
  title={A two-stage physics-informed neural network method based on conserved quantities and applications in localized wave solutions},
  author={Lin, Shuning and Chen, Yong},
  journal={Journal of Computational Physics},
  volume={457},
  pages={111053},
  year={2022},
  publisher={Elsevier}
}
@article{li2022fourier,
  title={Fourier Neural Operator with Learned Deformations for PDEs on General Geometries},
  author={Li, Zongyi and Huang, Daniel Zhengyu and Liu, Burigede and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2207.05209},
  year={2022}
}
@article{daw2022rethinking,
  title={Rethinking the importance of sampling in physics-informed neural networks},
  author={Daw, Arka and Bu, Jie and Wang, Sifan and Perdikaris, Paris and Karpatne, Anuj},
  journal={arXiv preprint arXiv:2207.02338},
  year={2022}
}
@article{krishnapriyan2021characterizing,
  title={Characterizing possible failure modes in physics-informed neural networks},
  author={Krishnapriyan, Aditi and Gholami, Amir and Zhe, Shandian and Kirby, Robert and Mahoney, Michael W},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={26548--26560},
  year={2021}
}
@article{wang2022respecting,
  title={Respecting causality is all you need for training physics-informed neural networks},
  author={Wang, Sifan and Sankaran, Shyam and Perdikaris, Paris},
  journal={arXiv preprint arXiv:2203.07404},
  year={2022}
}
@article{cao2021choose,
  title={Choose a transformer: Fourier or galerkin},
  author={Cao, Shuhao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24924--24940},
  year={2021}
}
@article{guibas2021adaptive,
  title={Adaptive fourier neural operators: Efficient token mixers for transformers},
  author={Guibas, John and Mardani, Morteza and Li, Zongyi and Tao, Andrew and Anandkumar, Anima and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2111.13587},
  year={2021}
}
@article{nabian2021efficient,
  title={Efficient training of physics-informed neural networks via importance sampling},
  author={Nabian, Mohammad Amin and Gladstone, Rini Jasmine and Meidani, Hadi},
  journal={Computer-Aided Civil and Infrastructure Engineering},
  volume={36},
  number={8},
  pages={962--977},
  year={2021},
  publisher={Wiley Online Library}
}
@article{ji2021stiff,
  title={Stiff-pinn: Physics-informed neural network for stiff chemical kinetics},
  author={Ji, Weiqi and Qiu, Weilun and Shi, Zhiyu and Pan, Shaowu and Deng, Sili},
  journal={The Journal of Physical Chemistry A},
  volume={125},
  number={36},
  pages={8098--8106},
  year={2021},
  publisher={ACS Publications}
}
@inproceedings{
song2021scorebased,
title={Score-Based Generative Modeling through Stochastic Differential Equations},
author={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=PxTIG12RRHS}
}

@article{lu2022dpm,
  title={Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps},
  author={Lu, Cheng and Zhou, Yuhao and Bao, Fan and Chen, Jianfei and Li, Chongxuan and Zhu, Jun},
  journal={arXiv preprint arXiv:2206.00927},
  year={2022}
}

@article{chen2021solving,
  title={Solving Inverse Stochastic Problems from Discrete Particle Observations Using the Fokker--Planck Equation and Physics-Informed Neural Networks},
  author={Chen, Xiaoli and Yang, Liu and Duan, Jinqiao and Karniadakis, George Em},
  journal={SIAM Journal on Scientific Computing},
  volume={43},
  number={3},
  pages={B811--B830},
  year={2021},
  publisher={SIAM}
}

@article{poggio2017and,
  title={Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review},
  author={Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
  journal={International Journal of Automation and Computing},
  volume={14},
  number={5},
  pages={503--519},
  year={2017},
  publisher={Springer}
}

@article{lai2021kolmogorov,
  title={The Kolmogorov Superposition Theorem can Break the Curse of Dimensionality When Approximating High Dimensional Functions},
  author={Lai, Ming-Jun and Shen, Zhaiming},
  journal={arXiv preprint arXiv:2112.09963},
  year={2021}
}


@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@inproceedings{narayanan2021efficient,
  title={Efficient large-scale language model training on gpu clusters using megatron-lm},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}

@article{uy2020neural,
  title={Neural network representation of the probability density function of diffusion processes},
  author={Uy, Wayne Isaac T and Grigoriu, Mircea D},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={30},
  number={9},
  year={2020},
  publisher={AIP Publishing}
}

@article{gu2021stationary,
  title={Stationary Density Estimation of It$\backslash$\^{} o Diffusions Using Deep Learning},
  author={Gu, Yiqi and Harlim, John and Liang, Senwei and Yang, Haizhao},
  journal={arXiv preprint arXiv:2109.03992},
  year={2021}
}

@inproceedings{zhai2022deep,
  title={A deep learning method for solving Fokker-Planck equations},
  author={Zhai, Jiayu and Dobson, Matthew and Li, Yao},
  booktitle={Mathematical and scientific machine learning},
  pages={568--597},
  year={2022},
  organization={PMLR}
}

@article{darbon2021some,
  title={On some neural network architectures that can represent viscosity solutions of certain high dimensional Hamilton--Jacobi partial differential equations},
  author={Darbon, J{\'e}r{\^o}me and Meng, Tingwei},
  journal={Journal of Computational Physics},
  volume={425},
  pages={109907},
  year={2021},
  publisher={Elsevier}
}

@article{lu2021physics,
  title={Physics-informed neural networks with hard constraints for inverse design},
  author={Lu, Lu and Pestourie, Raphael and Yao, Wenjie and Wang, Zhicheng and Verdugo, Francesc and Johnson, Steven G},
  journal={SIAM Journal on Scientific Computing},
  volume={43},
  number={6},
  pages={B1105--B1132},
  year={2021},
  publisher={SIAM}
}

@article{batlle2023error,
  title={Error Analysis of Kernel/GP Methods for Nonlinear and Parametric PDEs},
  author={Batlle, Pau and Chen, Yifan and Hosseini, Bamdad and Owhadi, Houman and Stuart, Andrew M},
  journal={arXiv preprint arXiv:2305.04962},
  year={2023}
}

@article{liu2022physics,
  title={Physics-informed neural networks based on adaptive weighted loss functions for Hamilton-Jacobi equations},
  author={Liu, Youqiong and Cai, Li and Chen, Yaping and Wang, Bin},
  journal={Mathematical Biosciences and Engineering},
  volume={19},
  number={12},
  pages={12866--12896},
  year={2022}
}