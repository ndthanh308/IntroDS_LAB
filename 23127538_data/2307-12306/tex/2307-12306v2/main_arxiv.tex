\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage[a4paper,top=3cm,bottom=2cm,left=2.3cm,right=2.3cm,marginparwidth=1.75cm]{geometry}

%\title{Scaling up Physics-Informed Neural Networks (PINNs) to Very High-Dimensions}
% \title{Tackling the curse-of-dimensionality with physics-informed neural networks}
\title{Tackling the Curse of Dimensionality with Physics-Informed Neural Networks}
%

\date{}

\usepackage{etoolbox}
\makeatletter
\patchcmd{\@maketitle}{\LARGE}{\fontsize{15}{20}\selectfont}{}{}
\makeatother

\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\usepackage{subfigure}
\usepackage{times}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\tsigma}{\tilde \sigma}
\newcommand{\tw}{\tilde w}
\newcommand{\tth}{\hat h}
\newcommand{\tv}{\tilde v}
\newcommand{\hh}{\hat h}
\newcommand{\hv}{\hat w}
\newcommand{\hhm}{\hat m}
\newcommand{\bh}{\bar h}
\newcommand{\bv}{\bar v}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bt}{\boldsymbol{\theta}}

%\graphicspath{{./Figures/}}
\begin{document}


%% Group authors per affiliation:
\author{Zheyuan Hu\thanks{Department of Computer Science, National University of Singapore, Singapore, 119077 (\href{mailto:e0792494@u.nus.edu}{e0792494@u.nus.edu},\href{mailto:kenji@nus.edu.sg}{kenji@nus.edu.sg})}
\and Khemraj Shukla\thanks{Division of Applied Mathematics, Brown University, Providence, RI 02912, USA (\href{mailto:khemraj\_shukla@brown.edu}{khemraj\_shukla@brown.edu}, \href{mailto:george\_karniadakis@brown.edu}{george\_karniadakis@brown.edu})}
\and George Em Karniadakis\footnotemark[2] \and  \linebreak Kenji Kawaguchi\footnotemark[1]}

\maketitle

\begin{abstract}
The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed method. We experimentally demonstrate that the proposed method allows us to solve many notoriously hard high-dimensional PDEs, including the Hamilton-Jacobi-Bellman (HJB) and the Schr\"{o}dinger equations in thousands of dimensions very fast on a single GPU using the PINNs mesh-free approach. For instance, we solve nontrivial nonlinear PDEs (one HJB equation and one Black-Scholes equation) in 100,000 dimensions in 6 hours on a single GPU using SDGD with PINNs. Since SDGD is a general training methodology of PINNs, SDGD can be applied to any current and future variants of PINNs to scale them up for arbitrary high-dimensional PDEs.
\end{abstract}

% , hence addressing both memory limitations and slow convergence issues

% The new method involves decomposing and sampling PDE terms, a technique we have named Stochastic Dimension Gradient Descent (SDGD). 

%  in high-dimensional cases

%\maketitle

\begin{comment}
%% -- this is old abstract -- 

The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high dimensional partial differential equations (PDEs) as Richard Bellman first pointed out over 60 years ago. We propose to further develop Physics-Informed Neural Networks (PINNs) to tackle the CoD in the context of PDEs. Specifically, we develop a new method for training and scaling up PINNs to solve arbitrary high-dimensional PDEs. We resolve the computational and memory bottleneck, which is the backpropagation of the residual loss, by decomposing the PDE terms of each dimension. Through this decomposition, we can reduce memory costs even more than just reducing the number of residual points. This leads to an inexpensive and unbiased gradient estimator that can be combined with modern stochastic gradient descent algorithms to accelerate convergence.
Based on this, we propose further acceleration. In addition to using SGD over PDE terms in the backward pass as mentioned above, we also perform unbiased sampling on the forward pass independently, making both the forward and backward passes computationally efficient and with low memory cost. We also discuss how our approach can perform parallel computations for further speed-up and scale-up. We theoretically demonstrate that the stochastic gradients generated by SGD over PDE terms are unbiased, providing convergence guarantees for our method. Additionally, we conduct a theoretical analysis comparing SDGD with SGD over residual points. Under the same batch size and memory cost, there exist proper batch sizes for PDE terms and residual points that can lead to minimized gradient variance and faster convergence, which cannot be achieved for the conventional SGD over points solely. This finding establishes that SDGD represents a more refined and general form of SGD, enabling smaller batch sizes, reduced memory usage, and accelerated convergence with proper batch sizes.

To quantify our claims, we first conduct extensive experiments on several nonlinear high-dimensional PDEs and investigate the stability of the proposed SGD over terms that unbiasedly sample the backward pass. We demonstrate that, at the same memory cost, SDGD is as stable as the conventional SGD over collocation points, and even more efficient in some cases. At the same time, with the further speed-up method via sampling both forward and backward passes, we can train PINNs on nontrivial nonlinear PDEs in 100,000 dimensions in 6 hours of training time (on a single GPU), while the traditional training methods exceed the GPU memory limit directly.  Combined with parallel computing, the training time for this case can be further reduced to several minutes.

Subsequently, we compare the two proposed algorithms with other methods on multiple nonlinear PDEs, and show that SDGD outperforms all competitors in various metrics. Additionally, SDGD for PINNs enables mesh-free training and prediction across the entire domain, while other methods typically only provide predictions at a single point and are mesh-dependent. Furthermore, we combine our algorithm with adversarial training, notorious for its slowness in high-dimensional machine learning, and show that SDGD accelerates it significantly. Finally, we demonstrate the generality of SDGD by applying it to the Schr\"{o}dinger equation for quantum physics and chemistry.
Taken together, SDGD provides a new efficient paradigm for large-scale PINN training but can be readily extended to any other regression or even classification problems.
The code will be released upon acceptance in \url{https://github.com/zheyuanhu01/Scaling_up_PINN/}.

\end{comment}

\section{Introduction}
The curse-of-dimensionality (CoD) refers to the computational and memory challenges when dealing with high-dimensional problems that do not exist in low-dimensional settings.  The term was first introduced in 1957 by Richard E. Bellman \cite{bellman1966dynamic,hammer1962adaptive}, who was working on dynamic programming, to describe the exponentially increasing costs due to high dimensions. Later, the concept was adopted in various areas of science, including numerical partial differential equations, combinatorics, machine learning, probability, stochastic analysis, and data mining. 
%
In the context of numerically solving Partial Differential Equations (PDEs), an increase in the dimensionality of the PDE's independent variables tends to lead to a corresponding exponential rise in computational costs needed for obtaining an accurate solution. This poses a considerable challenge for all existing algorithms.

 Physics-Informed Neural Networks (PINNs) are highly practical in solving partial differential equation (PDE) problems. Compared to other methods, PINNs can solve various PDEs in their general form, enabling mesh-free solutions and handling complex geometries. Additionally, PINNs leverage the interpolation capability of neural networks to provide accurate predictions throughout the domain, unlike other methods that can only compute the value of PDEs at a single point \cite{beck2021deep, han2018solving, raissi2018forward}. Due to neural networks' flexibility and expressiveness, in principle, the set of functions representable by PINNs have the ability to approximate high-dimensional solutions. However, PINNs may also fail in tackling CoD due to insufficient memory and slow convergence when dealing with high-dimensional PDEs. For example, when PDE's dimension is very high, even just using one collocation point can lead to an insufficient memory error. Unfortunately, the aforementioned high-dimensional and high-order cases often occur for very useful PDEs, such as the Hamilton-Jacobi-Bellman (HJB) equation in stochastic optimal control, the Fokker-Planck equation in stochastic analysis and high-dimensional probability, and the Black-Scholes equation in mathematical finance, etc. These PDE examples pose a grand challenge to the application of PINNs in effectively tackling real-world large-scale problems.


To scale up PINNs for arbitrary high-dimensional PDEs, we propose a training method of PINNs by decomposing and sampling gradients of PDEs, which we name {\em stochastic dimension gradient descent} (SDGD). It accelerates the training of PINNs while significantly reducing the memory cost, thereby realizing the training of any high-dimensional PDEs with acceleration. Specifically, we reduce the memory consumption in PINNs by decomposing the computational and memory bottleneck, namely the gradient of the residual loss that contains PDE terms. We propose a novel decomposition of the gradient of the residual loss into each piece corresponding to each dimension of PDEs. Then, at each iteration of training, we sample a subset of these dimensional pieces to optimize PINNs. Although we use only a subset per iteration, this sampling process is ensured to be an unbiased estimator of the full gradient of all dimensions, which is then used to guarantee convergence for all dimensions. 

SDGD enables more efficient parallel computations, fully leveraging multi-GPU computing to scale up and expedite the speed of PINNs. Parallelizing the computation of mini-batches of different dimensional pieces across multiple devices can expedite convergence. This is akin to traditional SGD over data points, which allows simultaneous computation on different machines. SDGD also accommodates the use of gradient accumulation on resource-limited machines to enable larger batch sizes and reduce gradient variance. Our method also enables parallel computations, fully leveraging multi-GPU computing to scale up and expedite the speed of PINNs. Gradient accumulation involves accumulating gradients from multiple batches to form a larger batch size for stochastic gradients, reducing the variance of each stochastic gradient on a single GPU. Due to the refined and general partitioning offered by SDGD, our gradient accumulation can be executed on devices with limited resources, ideal for edge computing. 

We theoretically prove that the stochastic gradients generated by SDGD are unbiased. Based on this, we also provide convergence guarantees for our method. Additionally, our theoretical analysis showed that, under the same batch size and memory cost, proper batch sizes for PDE terms and residual points can minimize gradient variance and accelerate convergence, an outcome not possible solely with conventional SGD over points. SDGD extends and generalizes traditional SGD over points, providing stable, low-variance stochastic gradients, while enabling smaller batch sizes and reduced memory usage.

We conduct extensive experiments on several high-dimensional PDEs. We investigate the relationship between SDGD and SGD over collocation points, where the latter is stable and widely adopted to reduce memory cost previously. We vary the number of PDE terms and collocation points to study the stability and convergence speed of SDGD and SGD under the same memory budgets. Experimental results show that our proposed SDGD is as stable as SGD over points on multiple nonlinear high-dimensional PDEs and can accelerate convergence. In some cases, SDGD over terms can achieve faster convergence than SGD, under the same memory cost for both methods. 

Furthermore, we showcase large-scale PDEs where traditional PINN training directly fails due to an out-of-memory (OOM) error. Concretely, with the further speed-up method via sampling both forward and backward passes, we can train PINNs on nontrivial nonlinear PDEs in 100,000 dimensions in 6 hours of training time, while the traditional training methods exceed the GPU memory limit directly. Combined with parallel computing, the training of the 100,000D nonlinear equation can be reduced to just a few minutes.

After validating the effectiveness of SDGD compared to SGD over collocation points, we compare the algorithms with other methods. On multiple nonlinear PDEs, our algorithm outperforms other methods in various metrics. Additionally, our method enables mesh-free training and prediction across the entire domain, while other methods typically only provide predictions at a single point. Furthermore, we combine our algorithm with adversarial training, notorious for its slowness in high-dimensional machine learning, and our algorithm significantly accelerates it. We also demonstrate the generality of our method by applying it to the Schr\"{o}dinger equation, which has broad connections with quantum physics and quantum chemistry. Overall, our method provides a paradigm shift for performing large-scale PINN training. 

The rest of this paper is arranged as follows. We present related work in Section 2, our main algorithm for accelerating and scaling up PINNs in Section 3, theoretical analysis of our algorithms' convergence in Section 4, numerical experiments in Section 5, and conclusion in Section 6.





\section{Related Work}
\subsection{Physics-Informed Machine Learning}
This paper is based on the concept of Physics-Informed Machine Learning \cite{karniadakis2021physics}. Specifically, PINNs \cite{raissi2019physics} utilize neural networks as surrogate solutions for PDEs and optimize the boundary loss and residual loss to approximate the PDEs' solutions. On the other hand, DeepONet \cite{lu2019deeponet} leverages the universal approximation theorem of infinite-dimensional operators and directly fits the PDE operators in a data-driven manner. It can also incorporate physical information by using residual loss. Therefore, our algorithms can also be flexibly combined with the training of physics-informed DeepONet. Since their inception, PINNs have succeeded in solving numerous practical problems in computational science, e.g., nonlinear PDEs \cite{raissi2018hidden}, solid mechanics \cite{haghighat2021physics}, uncertainty quantification \cite{yang2019adversarial}, inverse water waves problems \cite {jagtap2022deep}, and fluid mechanics \cite{cai2021physics}, etc. DeepONets also have shown their great potential in solving stiff chemical mechanics \cite{goswami2023learning}, shape optimization \cite{shukla2023deep}, and materials science problems \cite{goswami2022physics}.


The success of PINNs has also attracted considerable attention in theoretical analysis. In the study conducted by Luo et al. \cite{Luo2020TwoLayerNN}, the authors delve into the exploration of PINNs' prior and posterior generalization bounds. Additionally, they utilize the neural tangent kernel to demonstrate the global convergence of Physics-Informed Neural Networks (PINNs). Mishra et al. \cite{mishra2020estimates} derive an estimate for the generalization error of PINNs, considering both the training error and the number of training samples. In a different approach, Shin et al. \cite{shin2020convergence} adapt the Schauder approach and the maximum principle to provide insights into the convergence behavior of the minimizer as the number of training samples tends towards infinity. They demonstrate that the minimizer converges to the solution in both $C^0$ and $H^1$. Furthermore, Lu et al. \cite{lu2021priori} employ the Barron space within the framework of two-layer neural networks to conduct a prior analysis on PINN with the softplus activation function. This analysis is made possible by drawing parallels between the softplus and ReLU activation functions. More recently, Hu et al. \cite{hu2021extended} employ the Rademacher complexity concept to provide a measure of the generalization error for both PINNs and the extended PINNs variant.


\subsection{Machine Learning PDEs Solvers in High-Dimensions}
There have been numerous attempts to tackle high-dimensional PDEs by deep learning to overcome the curse of dimensionality.
In the PINN literature, \cite{wang20222} shows that to learn a high-dimensional HJB equation, the $L^\infty$ loss is required. The authors of \cite{he2023learning} propose to parameterize PINNs by the Gaussian smoothed model and to optimize PINNs without back-propagation by Stein's identity, avoiding the vast amount of differentiation in high-dimensional PDE operators to accelerate the convergence of PINNs. Separable PINN \cite{cho2022separable} considers a per-axis sampling of residual points instead of point-wise sampling in high-dimensional spaces, thereby reducing the computational cost of PINN. However, this method \cite{he2023learning, cho2022separable} focuses on acceleration on only modest (less than 4) dimensions. Specifically, separable PINNs \cite{cho2022separable} are designed mainly for increasing the number of collocation points in 3D PDEs, whose separation of sampling points becomes intractable in high-dimensional spaces.  Separable PINNs  \cite{cho2022separable} also struggle in PDE problems with non-separable solutions.
Han et al. \cite{han2018solving, han2017deep} proposed the DeepBSDE solver for high-dimensional PDEs based on the classical BSDE method and they use deep learning to approximate unknown functions. Extensions of the DeepBSDE method are presented in \cite{beck2019machine, chan2019machine,henry2017deep,hure2020deep,ji2020three}. Becker et al. \cite{becker2021solving} solve high-dimensional optimal stopping problems via deep learning. The authors of \cite{beck2021deep} combine splitting methods and deep learning to solve high-dimensional PDEs. Raissi \cite{raissi2018forward} leverages the relationship between high-dimensional PDEs and stochastic processes whose trajectories provide supervision for deep learning. 
Despite the effectiveness of the method in \cite{beck2021deep, han2018solving, han2017deep, raissi2018forward}, they can only be applied to a certain restricted class of PDEs.
Wang et al. \cite{wang2022tensor, wang2022solving} propose tensor neural networks with efficient numerical integration and separable DeepONet structures for solving high-dimensional Schr\"{o}dinger equations in quantum physics. Zhang et al. \cite{zhang2020learning} propose to use PINNs for solving stochastic differential equations by representing their solutions via spectral dynamically orthogonal and bi-orthogonal methods. Zang et al. \cite{zang2020weak} propose a weak adversarial network that solves PDEs by its weak formulation.

In the numerical PDE literature, attempts exist to scale numerical methods to high dimensions, e.g., proper generalized decomposition (PGD) \cite{chinesta2011short}, multi-fidelity information fusion algorithm \cite{perdikaris2016multifidelity}, and ANOVA \cite{martens2020neural,zhang2012error}. Darbon and Osher \cite{darbon2016algorithms} propose a fast algorithm for solving high-dimensional Hamilton-Jacobi equations if the Hamiltonian is convex and positively homogeneous of degree one. The multilevel Picard method \cite{beck2020overcoming,beck2020overcoming_ac,becker2020numerical,hutzenthaler2020overcoming, hutzenthaler2021multilevel} is another approach for approximating solutions of high-dimensional parabolic PDEs, which reformulates the PDE problem as a stochastic fixed point
equation, which is then solved by multilevel and nonlinear Monte-Carlo. Similar to Beck et al. \cite{beck2021deep} and Han et al. \cite{han2018solving}, the multilevel Picard method can only output the solution's value on one test point and can only be applied to a certain restricted class of PDEs.

\subsection{Stochastic Gradient Descent}
This paper accelerates and scales up PINNs based on the idea of stochastic gradient descent (SGD). In particular, we propose the aforementioned SDGD method. SGD is a common approach in machine learning for handling large-scale data. In general, SGD is an iterative optimization algorithm commonly used in machine learning for training models. It updates the model's parameters by computing the gradients on a small randomly selected subset of training examples, known as mini-batches. This randomness introduces stochasticity, hence enabling faster convergence and efficient utilization of large datasets, making SGD a popular choice for training deep learning models.

Among them, the works in \cite{fehrman2020convergence, lei2019stochastic, mertikopoulos2020almost} are remarkable milestones. Specifically, the condition presented in \cite{lei2019stochastic} stands as a minimum requirement within the general optimization literature, specifically pertaining to the Lipschitz continuity of the gradient estimator. Their proof can also be extended in establishing convergence for our particular case, necessitating the demonstration of an upper bound on the Lipschitz contact of our gradient estimator. On the other hand, \cite{fehrman2020convergence, mertikopoulos2020almost}  adopt a classical condition that entails a finite upper bound on the variance. In this case, it suffices to compute an upper bound for the variance term. While \cite{lei2019stochastic} assumes a more relaxed condition, the conclusion is comparably weaker, demonstrating proven convergence but with a notably inferior convergence rate. 
In the theoretical analysis presented in section 4 of this paper, we will utilize the aforementioned tools to provide convergence guarantees for our algorithm, with an emphasis on the effect of the choice of SDGD on stochastic gradient variance and PINNs convergence.

\section{Method}\label{sec:method}
\subsection{Physics-Informed Neural Networks (PINNs)}
In this paper, we focus on solving the following partial differential equations (PDEs) defined on a domain $\Omega \subset \mathbb{R}^d$:
\begin{equation}\label{eq:PDE}
\begin{aligned}
\mathcal{L}u(\bx)=R(\bx) \ \text{in}\ \Omega, \qquad
\mathcal{B}u(\bx)=B(\bx) \ \text{on}\ \Gamma,
\end{aligned}
\end{equation}
where $\mathcal{L}$ and $\mathcal{B}$ are the differential operators for the residual in $\Omega$ and for the boundary/initial condition on $\Gamma$.
PINNs \cite{raissi2019physics} is a neural network-based PDE solver via minimizing the following boundary and residual loss functions.
\begin{equation}
\begin{aligned}
\mathcal{L}(\theta) &= \lambda_b \mathcal{L}_b(\theta) + \lambda_r \mathcal{L}_r(\theta)\\
&=\frac{\lambda_b}{n_b}\sum_{i=1}^{n_b} {|\mathcal{B}u_{\theta}(\bx_{b,i})-B(\bx_{b,i})|}^2 + \frac{\lambda_r}{n_r}\sum_{i=1}^{n_r} {|\mathcal{L}u_{\theta}(\bx_{r,i})-R(\bx_{r,i})|}^2.
\end{aligned}
\end{equation}

\subsection{Methodology for High-Dimensional PDEs}
We first adopt the simple high-dimensional second-order Poisson's equation for illustration, then we move to the widely-used high-dimensional Fokker-Planck equation for the general case.
\subsubsection{Introductory Case of the High-Dimensional Poisson's Equation}
We consider a simple high-dimensional second-order Poisson's equation for illustration:
\begin{equation}
\Delta u(\bx) = \sum_{i=1}^d \frac{d^2}{d\bx_i^2}u(\bx) = R(\bx), \bx \in \Omega \subset \mathbb{R}^d,
\end{equation}
where $u$ is the solution, and $u_\theta$ is our PINN model parameterized by $\theta$. The memory and computational cost scale linearly as $d$ increases. So, for extremely high-dimensional PDEs containing many second-order terms, using one collocation point can lead to insufficient memory.

However, the memory problem is solvable by inspecting the residual loss on the collocation point $\bx$:
\begin{equation}
\ell(\theta) = \frac{1}{2}\left(\sum_{i=1}^d \frac{d^2}{d\bx_i^2}u_\theta(\bx) - R(\bx)\right)^2,
\end{equation}
The gradient with respect to the model parameters $\theta$ for training the PINN is
\begin{equation}
\begin{aligned}
\text{grad}(\theta) := \frac{\partial\ell(\theta)}{\partial \theta} &= \textcolor{blue}{\left(\sum_{i=1}^d \frac{d^2}{d\bx_i^2}u_\theta(\bx) - R(\bx)\right)}\left(\sum_{i=1}^d \textcolor{red}{\frac{\partial}{\partial\theta}\frac{d^2}{d\bx_i^2}u_\theta(\bx)}\right).
\end{aligned}
\end{equation}
We are only differentiating with respect to parameters $\theta$ on the $d$ PDE terms $\textcolor{red}{\frac{d^2}{d\bx_i^2}u_\theta(\bx)}$, which is the memory bottleneck in the backward pass since the shape of the gradient is proportional to both the PDE dimension and the parameter count of the PINN. In contrast, the first part $\textcolor{blue}{\left(\sum_{i=1}^d \frac{d^2}{d\bx_i^2}u_\theta(\bx) - R(\bx)\right)}$ is a scalar, which can be precomputed and detached from the GPU since it is not involved in the backpropagation for $\theta$. Since the full gradient grad$(\theta)$ is the sum of $d$ independent terms, we can sample several terms for stochastic gradient descent (SGD) using the sampled unbiased gradient estimator. Concretely, our algorithm can be summarized as follows:
\begin{enumerate}
\item Choose random indices $I \subset \{1,2,\cdots,d\}$ where $|I|$ is the cardinality of the set $I$, which is the batch size over PDE terms, where we can set $|I| \ll d$ to minimize memory cost. 
\item For $i=1,2,\cdots,d$, compute $\frac{d^2}{d\bx_i^2}u_\theta(\bx)$.
If $i \in I$, then keep the gradient with respect to $\theta$, else we detach it from GPU to save memory. After detachment, the term will not be involved in the costly backpropagation, and its gradient with respect to $\theta$ will not be computed.
\item Compute the unbiased stochastic gradient used to update the model
\begin{equation}
\text{grad}_I(\theta) = \textcolor{red}{\frac{d}{|I|}}\left(\sum_{i=1}^d \frac{d^2}{d\bx_i^2}u_\theta(\bx) - R(\bx)\right)\left(\textcolor{red}{\sum_{i \in I}} \frac{d^2}{d\bx_i^2}\frac{\partial}{\partial\theta}u_\theta(\bx)\right).
\end{equation}
\item If not converged, go to 1.
\end{enumerate}
Our algorithm enjoys the following good properties and extensions:
\begin{itemize}
\item Low memory cost: Since the main cost is from the backward pass, and we are only backpropagating over terms with $i \in I$, the cost is the same as the corresponding $|I|$-dimensional PDE.
\item Unbiased stochastic gradient: Our gradient is an {unbiased} estimator of the true full batch gradient, i.e., 
\begin{equation}
\mathbb{E}_I \left[\text{grad}_I(\theta)\right] = \text{grad}(\theta),
\end{equation}
so that modern SGD accelerators such as Adam \cite{kingma2014adam} can be adopted.
\item Accumulate gradient for full batch GD: For the full GD that exceeds the memory, we can select non-overlapping index sets $\{I_k\}_{k \in K}$ such that $\cup_k I_k = \{1,2,\cdots,d\}$, then we can combine these minibatch gradients to get the full gradient,
\begin{equation}
\frac{1}{|K|}\sum_{k} \text{grad}_{I_k}(\theta) = \text{grad}(\theta).
\end{equation}
This baseline process is memory-efficient but time-consuming. It is memory efficient since it divides the entire computationally intensive gradient into the sum of several stochastic gradients whose computations are memory-efficient. This operation is conducted one by one using the ``For loop from $i=1$ to $d$". But it is rather time-consuming because the ``For loop" cannot be parallelized.
\end{itemize}
A common practice in PINN is to sample stochastic collocation points to reduce the batch size of points, which is a common way to reduce memory costs in previous methods and even the entire field of deep learning. Next, we compare SDGD with SGD over collocation points:
\begin{itemize}
\item {SGD on collocation points} has a minimum batch size of 1 point plus the $d$-dimensional equation.
\item {SDGD} can be combined with SGD on points so that its minimal batch size is 1 point plus 1D equation. 
\item Thus, our method can solve high-dimensional PDEs arbitrarily since its minimal computational cost will not grow as the dimension $d$ increases. In contrast, the minimum cost of SGD on points scales linearly as $d$ grows.
\item Empirically, it is interesting to see how the two SGDs affect convergence. In particular, $B$-point + $D$-term with the same $B \times D$ quantity has the same computational and memory cost, e.g., 50-point-100-terms and 500-point-10-terms.
\end{itemize}

\subsubsection{General Case}

We are basically doing the decomposition:
\begin{equation}
\mathcal{L}u = \sum_{i=1}^{N_{\mathcal{L}}}\mathcal{L}_i u,
\end{equation}
where $\mathcal{L}_i$ are the $N_{\mathcal{L}}$ decomposed terms. We have the following examples:
\begin{itemize}
\item In the $d$-dimensional Poisson cases, $N_{\mathcal{L}} = d$ and $\mathcal{L}_i u = \frac{\partial^2}{\partial \bx_i^2} u(\bx)$. This observation also highlights the significance of decomposition methods for high-dimensional Laplacian operators. The computational bottleneck in many high-dimensional second-order PDEs often lies in the high-dimensional Laplacian operator.
\item Consider the $d$-dimensional Fokkerâ€“Planck (FP) equation, which has numerous real-world applications in optimal control and finance:
\begin{equation}
\frac{\partial u(\boldsymbol{x},t)}{\partial t} = -\sum_{i=1}^{d} \frac{\partial}{\partial \bx_i} \left[ F_i(\boldsymbol{x}) u(\bx,t) \right] + \frac{1}{2} \sum_{i,j=1}^{d} \frac{\partial^2}{\partial \bx_i \partial \bx_j} \left[ D_{ij}(\bx) u(\bx,t) \right],
\end{equation}
where $\bx = (\bx_1, \bx_2, \dots, \bx_n)$ is a $d$-dimensional vector, $u(\bx,t)$ is the probability density function of the stochastic process $\bx(t)$, $F_i(\boldsymbol{x})$ is the $i$-th component of the drift coefficient vector $\boldsymbol{F}(\bx)$, and $D_{ij}(\bx)$ is the $(i,j)$-th component of the diffusion coefficient matrix $\boldsymbol{D}(\bx)$. 

Then, our decomposition over the PDE terms will be
\begin{equation}
\begin{aligned}
\mathcal{L}u &= \frac{\partial u(\boldsymbol{x},t)}{\partial t} +\sum_{i=1}^{d} \frac{\partial}{\partial \bx_i} \left[ F_i(\boldsymbol{x}) u(\bx,t) \right] -\frac{1}{2} \sum_{i,j=1}^{d} \frac{\partial^2}{\partial \bx_i \partial \bx_j} \left[ D_{ij}(\bx) u(\bx,t) \right]\\
&= \sum_{i,j=1}^d  \left\{\frac{1}{d^2}\frac{\partial u(\boldsymbol{x},t)}{\partial t} + \frac{1}{d}\frac{\partial}{\partial \bx_i} \left[ F_i(\boldsymbol{x}) u(\bx,t) \right] - \frac{1}{2} \frac{\partial^2}{\partial \bx_i \partial \bx_j} \left[ D_{ij}(\bx) u(\bx,t) \right]\right\}\\
&= \sum_{i,j=1}^d \mathcal{L}_{ij}u(\bx),
\end{aligned}
\end{equation}
where we denoted 
\begin{equation}
\mathcal{L}_{ij}u(\bx) = \frac{1}{d^2}\frac{\partial u(\boldsymbol{x},t)}{\partial t} + \frac{1}{d}\frac{\partial}{\partial \bx_i} \left[ F_i(\boldsymbol{x}) u(\bx,t) \right] - \frac{1}{2} \frac{\partial^2}{\partial \bx_i \partial \bx_j} \left[ D_{ij}(\bx) u(\bx,t) \right].
\end{equation}
\item The $d$-dimensional bi-harmonic equation:
\begin{equation}
\Delta^2 u(\bx) = 0,\quad \bx \in \Omega \subset \mathbb{R}^d,
\end{equation}
where $\Delta = \sum_{i=1}^{d}\frac{\partial^2}{\partial \bx_i^2}$ is the Laplacian operator. For this high-dimensional fourth-order PDE, we can do the following decomposition for the PDE operator:
\begin{equation}
\mathcal{L}u = \Delta^2 u(\bx) = \sum_{i,j=1}^d \frac{\partial^4}{\partial \bx_i^2\partial \bx_j^2} u(\bx) = \sum_{i,j=1}^d \mathcal{L}_{ij}u(\bx),
\end{equation}
where
\begin{equation}
\mathcal{L}_{ij}u(\bx) = \sum_{i,j=1}^d \frac{\partial^4}{\partial \bx_i^2\partial \bx_j^2} u(\bx).
\end{equation}
%\item Consider the $d$-dimensional parabolic equations that related papers \cite{beck2021deep, han2018solving, raissi2018forward} aim to solve

\item For other high-dimensional PDEs, their complexity arises from the dimensionality itself. Therefore, it is always possible to find decomposition methods that can alleviate this complexity.
\end{itemize}

We go back to the algorithm after introducing these illustrative examples, the computational and memory bottleneck is the residual loss:
\begin{equation}
\ell(\theta) =\frac{1}{2} \left(\mathcal{L}u(\bx) - R(\bx)\right)^2.
\end{equation}
The gradient with respect to the model parameters $\theta$ for training the PINN is
\begin{equation}
\begin{aligned}
\text{grad}(\theta) = \frac{\partial\ell(\theta)}{\partial \theta} &= \textcolor{blue}{\left(\sum_{i=1}^{N_{\mathcal{L}}} \mathcal{L}_iu_\theta(\bx) - R(\bx)\right)}\left(\sum_{i=1}^{N_{\mathcal{L}} }\textcolor{red}{\frac{\partial}{\partial\theta} \mathcal{L}_iu_\theta(\bx)}\right).
\end{aligned}
\end{equation}
Subsequently, we are sampling the red part to reduce memory cost. Concretely, our algorithm can be summarized as follows:
\begin{enumerate}
\item Choose random indices $I \subset \{1,2,\cdots, N_{\mathcal{L}}\}$ where $|I|$ is the cardinality of the set $I$, which is the batch size over PDE terms, where we can set $|I| \ll N_{\mathcal{L}}\}$ to minimize memory cost. 
\item For $i=1,2,\cdots,N_{\mathcal{L}}$, compute $\mathcal{L}_iu_\theta(\bx)$.
If $i \in I$, then keep the gradient with respect to $\theta$, else we detach it from GPU to save memory. After detachment, the term will not be involved in the costly backpropagation, and its gradient with respect to $\theta$ will not be computed, which saves GPU memory costs.
\item Compute the unbiased stochastic gradient used to update the model
\begin{equation}
\text{grad}_I(\theta) = {\frac{N_{\mathcal{L}}}{|I|}}\left(\sum_{i=1}^{N_\mathcal{L}}\mathcal{L}_iu_\theta(\bx) - R(\bx)\right)\left({\sum_{i \in I}} \frac{\partial}{\partial\theta}\mathcal{L}_iu_\theta(\bx)\right).
\end{equation}
\item If not converged, go to 1.
\end{enumerate}
The algorithm is summarized in \ref{algo:1}.
\begin{algorithm}
\caption{Training Algorithm for scaling-up by sampling the gradient in the backward pass.}
\begin{algorithmic}[1]
\While{NOT CONVERGED}
\State Choose random indices $I \subset \{1,2,\cdots, N_{\mathcal{L}}\}$ where $|I|$ is the cardinality of the set $I$, which is the batch size over PDE terms, where we can set $|I| \ll N_{\mathcal{L}}\}$ to minimize memory cost. 
\For{$i \in \{1,2,\cdots, N_{\mathcal{L}}\}$}
    \State compute $\mathcal{L}_iu_\theta(\bx)$.
If $i \in I$, then keep the gradient with respect to $\theta$, else we detach it from GPU to save memory. After detachment, the term will not be involved in the costly backpropagation, and its gradient with respect to $\theta$ will not be computed.
\EndFor
\State Compute the unbiased stochastic gradient used to update the model
\begin{equation*}
\text{grad}_I(\theta) = {\frac{N_{\mathcal{L}}}{|I|}}\left(\sum_{i=1}^{N_\mathcal{L}}\mathcal{L}_i u_\theta(\bx) - R(\bx)\right)\left({\sum_{i \in I}} \frac{\partial}{\partial\theta}\mathcal{L}_iu_\theta(\bx)\right).
\end{equation*}
%\If{not converged}
%    \State go to 1.
%\EndIf
\EndWhile
\end{algorithmic}
\label{algo:1}
\end{algorithm}



\subsection{Further Scale-up via Gradient Accumulation and Parallel Computing}\label{sec:GAPC}
%So far, we assumed that the main computational bottleneck of PINN is backpropagation. In fact, for extremely large-scale problems, even the forward part can cause an OOM error. For example, in the FP equation, the largest calculation of the forward propagation lies in the Hessian matrix of the model. Further, we shall discuss how our method can be combined with parallel computing to achieve further speed-up and/or scale-up.
This subsection introduces two straightforward extensions of our Algorithm \ref{algo:1} for further speed-up and scale-up.

\textbf{Gradient Accumulation}. Since our method involves SDGD, in large-scale problems, we have to reduce the batch size to fit it within the available GPU memory. However, a very small batch size can result in significant gradient variance. In such cases, gradient accumulation is a promising idea. Specifically, gradient accumulation involves sampling different index sets $I_1, I_2, \cdots, I_n$, computing the corresponding gradients $\text{grad}_{I_k}(\theta)$ for $k = 1,2,\cdots,n$, and then averaging them as the final unbiased stochastic gradient for one-step optimization to effectively increase the batch size. The entire process can be implemented on a single GPU, and we only need to be mindful of the tradeoff between computation time and gradient variance.

\textbf{Parallel Computing}. Just as parallel computing can be used in machine learning to increase batch size and accelerate convergence in SGD, our new SDGD also supports parallel computing. Recall that the stochastic gradient by sampling the PDE terms randomly is given by
\begin{equation*}
\text{grad}_I(\theta) = {\frac{N_{\mathcal{L}}}{|I|}}\left(\sum_{i=1}^{N_\mathcal{L}}\mathcal{L}_iu_\theta(\bx) - R(\bx)\right)\left({\sum_{i \in I}} \frac{\partial}{\partial\theta}\mathcal{L}_iu_\theta(\bx)\right).
\end{equation*}
We can compute the above gradient for different index sets $I_1, I_2, \cdots, I_n$ on various machines simultaneously and accumulate them to form a larger-batch stochastic gradient.


\subsection{Further Speed-up via Simultaneous Sampling in Forward and Backward Passes}
In this section, we discuss how to accelerate further large-scale PINN training based on our proposed memory reduction methods to make it both fast and memory efficient.

Although the proposed methods can significantly reduce memory cost and use SDGD to obtain some acceleration through gradient randomness, the speed is still slow for particularly large-scale problems because we need to calculate each term $\{\mathcal{L}_iu(\bx)\}_{i=1}^{N_{\mathcal{L}}}$ in the forward pass one by one, and we are only omitting some these terms in the backward pass to scale up. For example, in extremely high-dimensional cases, we may need to calculate thousands of second-order derivatives of PINN, and the calculation speed is clearly unacceptable.

To overcome this bottleneck, we can perform the same unbiased sampling on the forward pass to accelerate it while ensuring that the entire gradient is unbiased. In other words, we only select some indices for calculation in the forward pass and select another set of indices for the backward pass, combining them into a very cheap unbiased stochastic gradient.

Mathematically, consider the full gradient again:
\begin{equation}
\begin{aligned}
\text{grad}(\theta) = \frac{\partial\ell(\theta)}{\partial \theta} &= \textcolor{blue}{\left(\sum_{i=1}^{N_{\mathcal{L}}} \mathcal{L}_iu_\theta(\bx) - R(\bx)\right)}\left(\sum_{i=1}^{N_{\mathcal{L}} }\textcolor{red}{\frac{\partial}{\partial\theta} \mathcal{L}_iu_\theta(\bx)}\right).
\end{aligned}
\end{equation}
We choose two random and independent indices sets $I, J \subset \{1,2,\cdots, N_{\mathcal{L}}\}$ for the sampling of the backward and the forward passes, respectively. The corresponding stochastic gradient is:
\begin{equation}
\begin{aligned}
\text{grad}_{I,J}(\theta) =\frac{N_{\mathcal{L}}}{|I|}\left(\left(\frac{N_{\mathcal{L}}}{|J|}\sum_{j \in J} \mathcal{L}_iu_\theta(\bx)\right) - R(\bx)\right)\left(\sum_{i\in I}\frac{\partial}{\partial\theta} \mathcal{L}_iu_\theta(\bx)\right).
\end{aligned}
\end{equation}
Since $I, J$ are independent, the gradient estimator is unbiased:
\begin{equation}
\mathbb{E}_{I,J}\text{grad}_{I,J}(\theta) = \text{grad}(\theta).
\end{equation}
Concretely, our algorithm can be summarized as follows:
\begin{enumerate}
\item Choose random indices $I, J \subset \{1,2,\cdots, N_{\mathcal{L}}\}$, where we can set $|I| \ll N_{\mathcal{L}}$ to minimize memory cost and $|J| \ll N_{\mathcal{L}}$ to further speed up. 
\item For $i \in I$, compute $\mathcal{L}_iu_\theta(\bx)$ and keep the gradient with respect to $\theta$.
\item For $j \in J$, compute $\mathcal{L}_ju_\theta(\bx)$ and detach it to save memory cost.
\item Compute the unbiased stochastic gradient used to update the model
\begin{equation}
\begin{aligned}
\text{grad}_{I,J}(\theta) =\frac{N_{\mathcal{L}}}{|I|}\left(\left(\frac{N_{\mathcal{L}}}{|J|}\sum_{j \in J} \mathcal{L}_ju_\theta(\bx)\right) - R(\bx)\right)\left(\sum_{i\in I}\frac{\partial}{\partial\theta} \mathcal{L}_iu_\theta(\bx)\right).
\end{aligned}
\end{equation}
\item If not converged, go to 1.
\end{enumerate}
The algorithm is summarized in Algorithm \ref{algo:2}.

\begin{algorithm}
\caption{Training Algorithm for scaling-up and further speeding-up by sampling both the forward and backward passes.}
\begin{algorithmic}[1]
\While{NOT CONVERGED}
\State Choose random indices $I, J \subset \{1,2,\cdots, N_{\mathcal{L}}\}$, where we can set $|I| \ll N_{\mathcal{L}}$ to minimize memory cost and $|J| \ll N_{\mathcal{L}}$ to further speed up.
\For{$i \in I$}
    \State compute $\mathcal{L}_iu_\theta(\bx)$ and keep the gradient with respect to $\theta$.
\EndFor
\For{$j \in J$}
    \State compute $\mathcal{L}_ju_\theta(\bx)$ and detach it to save memory cost.
\EndFor
\State Compute the unbiased stochastic gradient used to update the model
\begin{equation*}
\begin{aligned}
\text{grad}_{I,J}(\theta) =\frac{N_{\mathcal{L}}}{|I|}\left(\left(\frac{N_{\mathcal{L}}}{|J|}\sum_{j \in J} \mathcal{L}_j u_\theta(\bx)\right) - R(\bx)\right)\left(\sum_{i\in I}\frac{\partial}{\partial\theta} \mathcal{L}_iu_\theta(\bx)\right).
\end{aligned}
\end{equation*}
%\If{not converged}
%    \State go to 1.
%\EndIf
\EndWhile
\end{algorithmic}
\label{algo:2}
\end{algorithm}


\textbf{Trading off Speed with Gradient Variance}.
Obviously, since we have done more sampling, the gradient variance of this method may be larger, and the convergence may be sub-optimal. However, at the initialization stage of the problem, an imprecise gradient is sufficient to make the loss drop significantly, which is the tradeoff between convergence quality and convergence speed. We will demonstrate in experiments that this method is particularly effective for extremely large-scale PINNs training.

\subsection{Practical Consideration: Normalization for Stable Gradient}
In high-dimensional scenarios, the number of terms in the PDE increases with the dimensionality of the problem, leading to the possibility of a substantially large scale for the entire PDE residual. This is different from low-dimensional problems, necessitating the need to normalize both the PDE terms and their corresponding residual losses to prevent the occurrence of gradient explosion. Specifically, our approach is as follows.

For the general PDE we have considered, the full batch gradient and stochastic gradients generated by Algorithm \ref{algo:1} and Algorithm \ref{algo:2} are
\begin{equation*}
\begin{aligned}
\text{grad}(\theta) &= \left(\sum_{i=1}^{N_\mathcal{L}}\mathcal{L}_i u_\theta(\bx) - R(\bx)\right)\left({\sum_{i=1}^{N_{\mathcal{L}}}} \frac{\partial}{\partial\theta}\mathcal{L}_iu_\theta(\bx)\right).\\
\text{grad}_I(\theta) &= {\frac{N_{\mathcal{L}}}{|I|}}\left(\sum_{i=1}^{N_\mathcal{L}}\mathcal{L}_i u_\theta(\bx) - R(\bx)\right)\left({\sum_{i \in I}} \frac{\partial}{\partial\theta}\mathcal{L}_iu_\theta(\bx)\right).\\
\text{grad}_{I,J}(\theta) &=\frac{N_{\mathcal{L}}}{|I|}\left(\left(\frac{N_{\mathcal{L}}}{|J|}\sum_{j \in J} \mathcal{L}_j u_\theta(\bx)\right) - R(\bx)\right)\left(\sum_{i\in I}\frac{\partial}{\partial\theta} \mathcal{L}_iu_\theta(\bx)\right).
\end{aligned}
\end{equation*}
Since the full gradient is the sum of $N_{\mathcal{L}}$ subgradients along each dimension where $N_{\mathcal{L}}$ grows with $d$, the scale of this PDE inherently increases with dimension. If we do not control the learning rate or normalize appropriately, even with regular training, we may encounter the problem of exploding gradients. 
Compared with adjusting the learning rate, normalizing the loss or gradients is more versatile. We only need to normalize using $\frac{1}{N_{\mathcal{L}}^2}$ for both the forward and backward passes and the corresponding new gradients are given by:
\begin{equation*}
\begin{aligned}
\text{grad}(\theta) &= \left(\frac{1}{N_{\mathcal{L}}}\sum_{i=1}^{N_\mathcal{L}}\mathcal{L}_i u_\theta(\bx) - \frac{R(\bx)}{{N_{\mathcal{L}}}}\right)\left({\frac{1}{N_{\mathcal{L}}}\sum_{i=1}^{N_{\mathcal{L}}}} \frac{\partial}{\partial\theta}\mathcal{L}_iu_\theta(\bx)\right).\\
\text{grad}_I(\theta) &= \left(\frac{1}{N_{\mathcal{L}}}\sum_{i=1}^{N_\mathcal{L}}\mathcal{L}_i u_\theta(\bx) - \frac{R(\bx)}{N_{\mathcal{L}}}\right)\left({\frac{1}{|I|}\sum_{i \in I}} \frac{\partial}{\partial\theta}\mathcal{L}_iu_\theta(\bx)\right).\\
\text{grad}_{I,J}(\theta) &=\left(\left(\frac{1}{|J|}\sum_{j \in J} \mathcal{L}_j u_\theta(\bx)\right) - \frac{R(\bx)}{N_{\mathcal{L}}}\right)\left(\frac{1}{|I|}\sum_{i\in I}\frac{\partial}{\partial\theta} \mathcal{L}_iu_\theta(\bx)\right).
\end{aligned}
\end{equation*}


\section{Theory}
In this section, we analyze the convergence of our proposed algorithm. 
\subsection{SDGD is Unbiased}
In previous sections, we have shown that the stochastic gradients generated by our proposed SDGD are unbiased:
\begin{theorem}
The stochastic gradients $\text{grad}_I(\theta)$ and $\text{grad}_{I,J}(\theta)$ in our Algorithms \ref{algo:1} and \ref{algo:2}, respectively, parameterized by index sets $I, J$, are an unbiased estimator of the full-batch gradient $\text{grad}(\theta)$ using all PDE terms, i.e., the expected values of these estimators match that of the full-batch gradient, $\mathbb{E}_I[\text{grad}_I(\theta)]=\mathbb{E}_{I,J}[\text{grad}_{I,J}(\theta)] = \text{grad}(\theta)$.
\end{theorem}
\begin{proof}
We prove the theorem in Section \ref{sec:method}.
\end{proof}

\subsection{SDGD Reduces Gradient Variance}
In this subsection, we aim to show that SDGD can be regarded as another form of SGD over PDE terms, serving as a complement to the commonly used SGD over residual points. Specifically, $B$-point + $D$-term where $B, D \in \mathbb{Z}^+$ with the same $B \times D$ quantity has the same computational and memory cost, e.g., 50-point-100-terms and 500-point-10-terms. In particular, we shall demonstrate that properly choosing the batch sizes of residual points $B$ and PDE terms $D$ under the constant memory cost ($B \times D$) can lead to reduced stochastic gradient variance and accelerated convergence compared to previous practices that use SGD over points only.
 

We assume that the total full batch is with $N_r$ residual points $\{\bx_i\}_{i=1}^{N_r}$ and $N_{\mathcal{L}}$ PDE terms $\mathcal{L} = \sum_{i=1}^{N_{\mathcal{L}}}\mathcal{L}_i$, then the loss function for PINN optimization is
\begin{equation}
\frac{1}{2N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)^2,
\end{equation}
where we normalize over both the number of residual points and the number of PDE terms, which does not impact the directions of the gradients. The full batch gradient is:
\begin{equation}
\begin{aligned}
g(\theta) &:= \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\frac{\partial}{\partial \theta}\mathcal{L}u_\theta(\bx_i)\\
&= \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\sum_{j=1}^{N_{\mathcal{L}}}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right).
\end{aligned}
\end{equation}
If the random index sets $J \subset \{1,2,\cdots,N_{\mathcal{L}}\}$ and $B\subset \{1,2,\cdots,N_r\}$ are chosen, then the SGD gradient with $|B|$ residual points and $|J|$ PDE terms using these index sets is
\begin{equation}
g_{B, J}(\theta) = \frac{1}{|B||J|N_{\mathcal{L}}}\sum_{i \in B}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\sum_{j \in J}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right),
\end{equation}
where $|\cdot|$ computes the cardinality of a set. It is straightforward to show that $\mathbb{E}_{B,J}[g_{B,J}(\theta)] = g(\theta)$.
\begin{theorem}\label{thm:variance_1}
For the random index sets $(B, J)$ where $J\subset \{1,2,\cdots,N_{\mathcal{L}}\}$ is that for indices of PDE terms and $B\subset \{1,2,\cdots,N_r\}$ is that for indices of residual points, then
\begin{equation}
\mathbb{V}_{B,J}[g_{B,J}(\theta)] = \frac{C_1|J| + C_2|B| + C_3}{|B||J|},
\end{equation}
where $\mathbb{V}$ computes the variance of a random variable, $C_1, C_2, C_3$ are constants independent of $B,J$.
\end{theorem}
\begin{proof}
The theorem is proved in Appendix \ref{appendix:variance_1}
\end{proof}
Intuitively, the variance of the stochastic gradient tends to decrease as the batch sizes ($|B|, |J|$) increase, and it converges to zero for $|B|, |J| \rightarrow \infty$.

This verifies that SDGD can be regarded as another form of SGD over PDE terms, serving as a complement to the commonly used SGD over residual points. $|B|$-point + $|J|$-term with the same $|B| \times |J|$ quantity has the same computational and memory cost, e.g., 50-point-100-terms and 500-point-10-terms. According to Theorem \ref{thm:variance_1}, under the same memory budget, i.e., $|B| \times |J|$ is fixed, then there exists a particular choice of batch sizes $|B|$ and $|J|$ that minimizes the gradient variance, in turn accelerating and stabilizing convergence. This is because the stochastic gradient variance $\mathbb{V}_{B,J}[g_{B,J}(\theta)]$ is a function of finite batch sizes $|B|$ and $|J|$, which therefore can achieve its minimum value at a certain choice of batch sizes.

Let us take the extreme cases as illustrative examples. The first extreme case is when the PDE terms have no variance, meaning that the terms $\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)$ are identical for all $j$ after we fix $i$. In this case, if a memory budget of, for example, 100 units is given, the optimal choice would be to select 100 points and one PDE term with the minimum variance. Choosing more PDE terms would not decrease the gradient variance and would be less effective than using the entire memory budget for sampling points. Conversely, if the points have no variance in terms of the gradient they induce, the optimal choice would be one point and 100 PDE terms. In practice, the selection of residual points and PDE terms will inevitably introduce some variance. Therefore, in the case of a fixed memory cost, the choice of batch size for PDE terms and residual points involves a tradeoff. Increasing the number of PDE terms reduces the variance contributed by the PDE terms but also decreases the number of residual points, thereby increasing the variance of the points. Conversely, decreasing the number of PDE terms reduces the variance from the points but increases the variance from the PDE terms. Thus, there exists an optimal selection strategy to minimize the overall gradient variance since the choices of batch sizes are finite.

\subsection{Gradient Variance Bound and Convergence of SDGD}
To establish the convergence of unbiased stochastic gradient descent, we require either Lipschitz continuity of the gradients \cite{lei2019stochastic} or bounded variances \cite{fehrman2020convergence,mertikopoulos2020almost}, with the latter leading to faster convergence. To prove this property, we need to make the following steps. Firstly, we define the neural network serving as the surrogate model in the PINN framework.
\begin{definition}\label{def:DNN}
(Neural Network). A deep neural network (DNN) $u_\theta:\bx=(\bx_{1},\dots,\bx_d)\in\mathbb{R}^d\longmapsto u_\theta(\bx) \in \mathbb{R}$,
parameterized by $\theta$ of depth $L$ is the composition of $L$ linear functions with element-wise non-linearity $\sigma$, is expressed as below:
\begin{equation}\label{eq:DNN}
    u_\theta(\bx)=W_L \sigma (W_{L-1} \sigma(\cdots \sigma(W_1\bx)\cdots ),
\end{equation}
where $\bx\in\mathbb{R}^d$ is the input, and $W_l\in\mathbb{R}^{m_l \times m_{l-1}}$ is the weight matrix at $l$-th layer with $d=m_0$ and $m_L=1$. The parameter vector $\theta$ is the vectorization of the collection of all parameters. We denote $h$ as the maximal width of the neural network, i.e., $ h = \max(m_L, \cdots, m_0)$.
\end{definition}
For the nonlinear activation function $\sigma$, the residual ground truth $R(\bx)$, we assume the following:
\begin{assumption}
We assume that the activation function is smooth and $|\sigma^{(n)}(x)| \leq 1$ and $\sigma^{(n)}$ is 1-Lipschitz, e.g., the sine and cosine activations. We assume that $|R(\bx)| \leq R$ for all $\bx$.
\end{assumption}
\begin{remark}
The sine and cosine functions naturally satisfy the aforementioned conditions. As for the hyperbolic tangent (tanh) activation function, when the order $n$ of the PDE is determined, there exists a constant $C_n$ such that the activation function $C_n \tanh(x)$ satisfies the given assumptions. This constant can be absorbed into the weights of the neural network. This is because both the tanh function and its derivatives up to order $n$ are bounded.
\end{remark}
Recall that the full batch gradient is
\begin{equation}
\begin{aligned}
g(\theta) &= \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\sum_{j=1}^{N_{\mathcal{L}}}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right).
\end{aligned}
\end{equation}
The stochastic gradient produced by Algorithm \ref{algo:1} is
\begin{equation}
g_{B, J}(\theta) = \frac{1}{|B||J|N_{\mathcal{L}}}\sum_{i \in B}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\sum_{j \in J}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right),
\end{equation}
The stochastic gradient produced by Algorithm \ref{algo:2} is
\begin{equation}
g_{B, J, K}(\theta) = \frac{1}{|B||J|N_{\mathcal{L}}}\sum_{i \in B}\left(\frac{N_{\mathcal{L}}}{|K|}\left(\sum_{k \in K}\mathcal{L}_ku_\theta(\bx_i)\right) - R(\bx_i)\right)\left(\sum_{j \in J}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right),
\end{equation}
where $B$ is the index set sampling over the residual points, while $J$ and $K$ sample the PDE terms in the forward pass and the backward pass, respectively.

\begin{lemma}\label{lemma:variance}
(Bounding the Gradient Variance) Suppose that we consider the neural network function class:
\begin{equation}
\mathcal{NN}^L_{M} := \left\{\bx \mapsto W_L \sigma (W_{L-1} \sigma(\cdots \sigma(W_1\boldsymbol{x})\cdots )\ |\ \Vert W_l \Vert_{2} \leq M(l)\right\}.
\end{equation}
If the PDE under consideration has at most $n$th order derivative, then the gradient variance can be bounded by 
\begin{equation}
\begin{aligned}
\mathbb{V}_{B,J}(g_{B, J}(\theta)) &\leq \frac{2}{|B||J|N_{\mathcal{L}}}\left(n!(L-1)^n M(L) \prod_{l=1}^{L-1}  M(l)^n + R\right)^2\cdot\\
&\quad\left((n+1)!(L-1)^{n+1} M(L) \prod_{l=1}^{L-1}  M(l)^{n+1} \max_{\bx \in \Omega}\Vert\bx\Vert\right)^2,\\
\mathbb{V}_{B,J,K}(g_{B, J, K}(\theta)) &\leq \frac{2}{|B||J||K|}\left(n!(L-1)^n M(L) \prod_{l=1}^{L-1}  M(l)^n + R\right)^2\cdot\\
&\quad\left((n+1)!(L-1)^{n+1} M(L) \prod_{l=1}^{L-1}  M(l)^{n+1} \max_{\bx \in \Omega}\Vert\bx\Vert\right)^2,
\end{aligned}
\end{equation}
\end{lemma}
\begin{proof}
The proof of this lemma is provided in Appendix \ref{appendix:lemma}
\end{proof}
\begin{remark}
Intuitively, given the order of the PDE and the number of layers in the neural network, it is observed that both of them are finite. Consequently, the absolute values of the higher-order derivatives of the network are necessarily bounded by a quantity that is related to the norms of its weight matrices.
\end{remark}
Next, we define the SGD trajectory, and we will demonstrate the convergence of the SGD trajectory based on the stochastic gradients provided by our Algorithms \ref{algo:1} and \ref{algo:2}.

\begin{definition}
(SGD Trajectory) Given step sizes (learning rates) $\{\gamma_n\}_{n=1}^\infty$, and the initialized PINN parameters $\theta^1$, then the update rules of SGD using Algorithms \ref{algo:1} and \ref{algo:2} are
\begin{equation}\label{eq:SGD1}
\theta^{n+1} = \theta^{n} - \gamma^ng_{B,J}(\theta^{n}),
\end{equation}
\begin{equation}\label{eq:SGD2}
\theta^{n+1} = \theta^{n} - \gamma^ng_{B,J, K}(\theta^{n}),
\end{equation}
respectively.
\end{definition}

The following theorem demonstrates the convergence rate of SDGD in PINN training.

\begin{theorem}
(Convergence of SDGD under Bounded Gradient Variance) Fix some tolerance level $\delta >0$, suppose that the SGD trajectory given in equations (\ref{eq:SGD1}, \ref{eq:SGD2}) are bounded, i.e., $\Vert W_l^n \Vert\leq M(l)$ for all $n$ where the collection of all $\{W_l^n\}_{l=1}^L$ is $\theta^n$, and that the minimizer of the PINN loss is $\theta^*$, and that the SGD step size follows the form $\gamma_n = \gamma / (n + m)^p$ for some $p \in (1/2,1]$ and large enough $\gamma, m > 0$, then
\begin{enumerate}
\item There exist neighborhoods $\mathcal{U}$ and $\mathcal{U}_1$ of $\theta^*$ such that, if $\theta^1 \in \mathcal{U}_1$, the event
\begin{equation}
E_\infty = \left\{\theta^n \in \mathcal{U} \ \text{for all } n \in \mathbb{N}\right\}
\end{equation}
occurs with probability at least $1 - \delta$, i.e., $\mathbb{P}(E_\infty | \theta^1 \in\mathcal{U}_1) \geq 1 - \delta$.
\item Conditioned on $E_\infty$, we have
\begin{equation}
\mathbb{E}[\Vert\theta^n - \theta^*\Vert^2|E_\infty] \leq \mathcal{O}(1/n^p).
\end{equation}
\end{enumerate}
\end{theorem}
\begin{remark}
The convergence rate of stochastic gradient descent is $\mathcal{O}(1/n^p)$, where $\mathcal{O}$ represents the constant involved, including the variance of the stochastic gradient. A larger variance of the stochastic gradient leads to slower convergence, while a smaller variance leads to faster convergence. The selection of the learning rate parameters $\gamma$ and $m$ depends on the specific optimization problem itself. For example, in a PINN problem, suitable values of $\gamma$ and $m$ ensure that the initial learning rate is around 1e-4, which is similar to the common practice for selecting learning rates.
\end{remark}
\begin{remark}
In our convergence theorem, we have not made any unrealistic assumptions; all we require is the boundedness of the optimization trajectory of the PINN. Empirically, as long as the learning rate and initialization are properly chosen, PINN always converges, and the values of its weight matrices do not diverge to infinity. In the literature, many convergence theorems for SGD require various conditions to be satisfied by the loss function of the optimization problem itself, such as Lipschitz continuity. However, in our case, none of these conditions are required.
\end{remark}


\section{Experiments}
Our main goal is to showcase the following properties and advantages of our method:
\begin{enumerate}
\item For Algorithm \ref{algo:1}, reducing the residual point batch size and the PDE term batch size yields a comparable acceleration effect under the same memory budget. In some cases, our SDGD can even outperform the traditional SGD, demonstrating that SDGD can be an alternative form of SGD applied to PDE terms that is stable and reliable.

\item The main objective of Algorithm \ref{algo:2} is to scale up and accelerate training for large-scale PINN problems. We will demonstrate that while Algorithm \ref{algo:2} exhibits poorer convergence on small-scale problems due to increased gradient variance, its convergence speed and accuracy improve significantly as the scale of the PDE problem grows, such as in cases with tens of thousands of dimensions. In many of these large-scale instances, Algorithm \ref{algo:2} achieves convergence, whereas Algorithm \ref{algo:1} and full training only reach a few epochs without convergence. Additionally, full training may lead to out-of-memory (OOM) errors directly.

\item We compare our PINN-based method with other non-PINN mainstream approaches for solving high-dimensional PDEs \cite{beck2021deep, han2018solving, raissi2018forward}, especially in terms of accuracy.
\end{enumerate}

All the experiments are done on an NVIDIA A100 GPU with 40GB memory.

\subsection{Hamilton-Jacobi-Bellman (HJB) Equation with Linear Solution and Black-Scholes-Barenblatt (BSB) Equation: 
Scaling-up to 100,000D, and Comparisons with Baselines}

This experiment will demonstrate how Algorithms \ref{algo:1} and \ref{algo:2} are superior to the commonly used full-batch gradient descent on training PINNs, and how our SDGD-PINNs method can surpass strong non-PINNs baselines, on several nonlinear PDEs.

\subsubsection{Experimental Design}
We consider the following two nonlinear PDEs:
\begin{itemize}
\item \textbf{Hamilton-Jacobi-Bellman equation with linear solution (HJB-Lin)} \cite{wang20222} with the terminal condition given below:
\begin{equation}
\begin{aligned}
&\partial_t u(\bx, t) + \Delta u(\bx, t) - \frac{1}{d}\sum_{i=1}^d |\partial_{\bx_i}u|^c = -2, \quad \bx \in \mathbb{R}^d, t\in[0,T]\\
&u(\bx,T)=\sum_{i=1}^d \bx_i,
\end{aligned}
\end{equation} 
where $c=1, T=1$ and the dimension $d$ can be arbitrary chosen. The analytic solution is:
\begin{equation}
u(\bx,t) = \sum_{i=1}^d \bx_i + T - t.
\end{equation}
In this example, we decompose the residual prediction of PINN along each dimension as follows:
\begin{equation}
\partial_t u(\bx, t) + \Delta u(\bx, t) - \frac{1}{d}\sum_{i=1}^d |\partial_{\bx_i}u|^c = \frac{1}{d}\sum_{i=1}^d \left\{\partial_t u(\bx, t) + d\partial^2_{\bx_i}u(\bx, t) - \frac{1}{d}\sum_{j=1}^d |\partial_{\bx_j}u|^c \right\},
\end{equation}
since the first-order derivative is much cheaper and is not the main bottleneck.
\item \textbf{Black-Scholes-Barenblatt (BSB) equation}
\begin{equation}
\begin{aligned}
&u_t = -\frac{1}{2}\sigma^2\sum_{i=1}^d\bx_i^2u_{\bx_i\bx_i} + r(u -\sum_{i=1}^d\bx_iu_{\bx_i}),\\
&u(\bx, T) = \Vert \bx \Vert^2,
\end{aligned}
\end{equation}
where $T = 1, \sigma = 0.4, r = 0.05$ with the following analytic solution
\begin{equation}
u(\bx, t) = \exp\left[(r + \sigma^2)(T - t)\right]\Vert \bx \Vert^2.
\end{equation}
In this example, we decompose the residual prediction of PINN along each dimension as follows:
\begin{equation}
u_t+\frac{1}{2}\sigma^2\sum_{i=1}^d\bx_i^2u_{\bx_i\bx_i} - r(u -\sum_{i=1}^d\bx_iu_{\bx_i}) = \frac{1}{d}\sum_{i=1}^d \left\{u_t+\frac{1}{2}\sigma^2d\bx_i^2u_{\bx_i\bx_i} - r(u -\sum_{j=1}^d\bx_ju_{\bx_j})\right\},
\end{equation}
\end{itemize}

Using consistent hyperparameter settings, we explore varying PDE dimensions in the range of ${10^2, 10^3, 10^4, 10^5}$. This allows us to assess the acceleration achieved by our algorithm and whether PINNs can overcome the CoD.
Specifically, for the hyperparameters of PINNs, we employ a four-layer PINN with 1024 hidden units, trained using the Adam optimizer \cite{kingma2014adam} with an initial learning rate of 1e-3, exponentially decaying with a factor of 0.9995. Our testing datasets consist of 2e4 points, sampled from $\bx \sim \mathcal{N}(0,\boldsymbol{I})$ and $t \sim \text{Unif}[0,1]$. The residual points are sampled from the same distribution. The testing criterion is the relative $L_2$ error between the model and the solution given by
\begin{equation}
\frac{\sqrt{\sum_{i=1}^{N_{\text{test}}}|u_{\text{pred}}(X_i, t_i) - u(X_i, t_i)|^2}}{\sqrt{\sum_{i=1}^{N_{\text{test}}}|u(X_i, t_i)|^2}},
\end{equation}
where $u_{\text{pred}}$ is the prediction model and $u$ is the true solution, $N_{\text{test}}$ is the number of test points and $(X_i, t_i)$ is the $i$th test point.

Due to the large scale of the terminal condition, such as in HJB-Lin, where it follows a Gaussian distribution with zero mean and variance of $d$, or in the BSB equation, where it follows a chi-squared distribution with a mean of $d$, there is a high probability of sampling points with large values in high-dimensional space. Directly learning the terminal condition using the boundary loss can result in numerical overflow and optimization difficulties.
Therefore, we modified the PINN structure to satisfy the terminal condition automatically. This allows us to focus solely on the residual loss. Specifically, for the HJB-Lin and BSB equations with the terminal condition $u(\bx, T) = g(\bx)$, we consider the following PINN models:
\begin{equation}\label{eq:modified_nn}
{u}^{\text{HJB-Lin}}_\theta(\bx, t) = u_\theta(\bx, t)(T - t) + \sum_{i=1}^d\bx_i, \quad {u}^{\text{BSB}}_\theta(\bx, t) = \left((T-t)u_\theta(\bx, t) + 1\right)\Vert\bx\Vert^2,
\end{equation}
where $u_\theta$ is a vanilla multi-layer perceptron network while ${u}^{\text{HJB-Lin}}_\theta$ and ${u}^{\text{BSB}}_\theta$ are the prediction models for HJB-Lin and BSB equations, respectively.
For the HJB-Lin equation, the initialization is already sufficiently accurate regarding the $L^2$ error of the prediction for $u$, as the $\sum_{i=1}^d \bx_i$ scale is much larger than $T-t$. Therefore, we focus on testing the accuracy of $\partial_t u$, which is a nontrivial task:
\begin{equation}
\frac{\sqrt{\sum_{i=1}^{N_{\text{test}}}|\partial_tu_{\text{pred}}(X_i, t_i) - \partial_tu(X_i, t_i)|^2}}{\sqrt{\sum_{i=1}^{N_{\text{test}}}|\partial_tu(X_i, t_i)|^2}},
\end{equation}
where $u_{\text{pred}}$ is the prediction model and $u$ is the true solution.

For the baselines \cite{beck2021deep, han2018solving, raissi2018forward}, it is necessary to specify the initial point of the stochastic process corresponding to the PDE. These methods can only make predictions at this initial point, hence reporting the relative $L_1$ error at a single point. We will also report the error of PINN at this initial point. We choose the initial points as $(X_0, t_0) = (\bx_1=1, \bx_2=1, \cdots, \bx_d=1, t=0)$ for the BSB equation and $(X_0, t_0) = (\bx_1=0, \bx_2=0, \cdots, \bx_d=0, t=0)$ for the HJB-Lin equation, and the corresponding error is
\begin{equation}
\frac{|u_{\text{pred}}(X_0, t_0) - u(X_0, t_0)|}{u(X_0, t_0)},
\end{equation}
where $u_{\text{pred}}$ is the prediction model and $u$ is the true solution.

% Figure environment removed

% Figure environment removed

\begin{table}[]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
PDE & $10^2$ & $10^3$ & $10^4$ & $10^5$ \\ \hline
HJB-Lin & 34min & 68min & 81min & 310min \\ \hline
BSB & 31min & 57min & 118min & 41min \\ \hline
\end{tabular}
\caption{This table presents the convergence time required by our SDGD for different PDEs. In the HJB-Lin equation, as the dimensionality increases from 100 to 100,000, the dimensionality grows by a factor of 1000, while the time only increases by a factor of ten. This indicates that our method can withstand the curse of dimensionality. In the second BSB equation, surprisingly, the high-dimensional case converges faster than the low-dimensional case, demonstrating the so-called blessing of dimensionality and that our method can harness the blessing of dimensionality.}
\end{table}

\subsubsection{Results of Our Method and Full-Batch GD on Training PINN}
This subsection presents the results comparing our Algorithms \ref{algo:1} and \ref{algo:2} with full batch gradient descent when training PINN for the BSB and HJB equations, which are demonstrated in Figures \ref{fig:HJB2_scale_up} and \ref{fig:BSB_scale_up}.

Specifically, the captions of Figures \ref{fig:HJB2_scale_up} and \ref{fig:BSB_scale_up} illustrate different algorithms' speeds and memory costs in the HJB and BSB equations across various dimensions. Regarding memory cost, full gradient descent encounters OOM problems in 100,000 dimensions even with only one residual point, while SDGD can handle even higher-dimensional problems. In terms of speed, under comparable memory costs, Algorithm \ref{algo:2} consistently exhibits the highest speed, followed by Algorithm \ref{algo:1}, while full batch GD is the slowest. Notably, in high-dimensional cases, our Algorithm \ref{algo:2} demonstrates significant acceleration compared to other algorithms. In lower dimensions, the acceleration effect of Algorithm \ref{algo:2} is less pronounced, and Algorithm \ref{algo:1} suffices.

In the case of the 1000-dimensional BSB equation, Algorithm \ref{algo:2} exhibits a shorter time per epoch compared to Algorithm \ref{algo:1}. However, the convergence results are inferior, suggesting that Algorithm \ref{algo:2} sacrifices speed for larger gradient variance due to increased random sampling in both forward and backward passes. Nevertheless, as the dimension of the PDE increases, the advantageous fast convergence of Algorithm \ref{algo:2} becomes more evident. For instance, on the largest 100,000-dimensional problem, Algorithm \ref{algo:2} achieved convergence with a relative error of less than 3.477e-5 (HJB) / 4.334e-3 (BSB) in under 4 hours. In contrast, full GD encountered OOM errors, and the speed of Algorithm \ref{algo:1} was extremely slow, running only a few epochs within the same time frame and still far from convergence.

Meanwhile, in the 100,000-dimensional HJB equation with a relatively small batch size (100 PDE terms and 100 residual points), we observed a significant increase in the relative $L_2$ error performance of Algorithm \ref{algo:2} during training, attributable to the heightened variance of the gradient.

Thus, Algorithm \ref{algo:1} effectively reduces the memory cost of PINN by minimizing gradient variance and accelerates convergence through gradient randomness. Algorithm \ref{algo:2} further enhances iteration speed while reducing memory requirements, albeit at the expense of increased gradient variance and compromised convergence results. However, in higher dimensional cases, the acceleration provided by Algorithm \ref{algo:2} outweighs the impact of gradient variance. 

%Previous best results were achieved with a relative error of 2.3E-3 \cite{raissi2018forward} for the 100D BSB equation, and 6.1E-3 \cite{wang20222} for the 100D HJB equation. We have surpassed these benchmarks in both dimensions and performance.

In addition to the stochastic gradient acceleration employed in our method, the symmetry of the PDE plays a crucial role in achieving convergence. These PDEs exhibit symmetry along each $\bx_i$ axis regardless of dimensionality. This symmetry reduces gradient variance in SDGD and random sampling during the forward pass in Algorithm \ref{algo:2}, facilitating rapid convergence in our approach.

In conclusion, the experimental results demonstrate that Algorithm \ref{algo:1} and Algorithm \ref{algo:2} exhibit significant advantages over full batch gradient descent in training PINNs. This is particularly evident in the case of very high dimensions, where their acceleration and computational efficiency are especially pronounced.

\subsubsection{Comparison Between SDGD-PINNs and Strong Baselines}

\begin{table}[]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Method & Evaluation Metric  & Dim=$10^2$       & Dim=$10^3$   & Dim=$10^4$   & Dim=$10^5$   \\ \hline
FBSNN \cite{raissi2018forward} & Relative $L_2$ error of $u$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
DeepBSDE \cite{han2018solving} & Relative $L_2$ error of $u$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
DeepSplitting \cite{beck2021deep} & Relative $L_2$ error of $u$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
Ours & Relative $L_2$ error of $u$ & \textbf{6.182E-4} & \textbf{1.284E-3} & \textbf{3.571E-3} & \textbf{4.486E-3}  \\ \toprule\hline
Method & Evaluation Metric  & Dim=$10^2$       & Dim=$10^3$   & Dim=$10^4$   & Dim=$10^5$   \\ \hline
FBSNN \cite{raissi2018forward} & Relative $L_1$ error on one point & 1.378E-5 & 8.755E-5 & 4.003E-5  & 3.559E-5 \\ \hline
DeepBSDE \cite{han2018solving} & Relative $L_1$ error on one point & 1.106E-5  & 5.184E-4  & 3.013E-4 & 4.519E-4 \\ \hline
DeepSplitting \cite{beck2021deep} & Relative $L_1$ error on one point  &  2.048E-3  & 1.942E-3 & 1.992E-3  & 2.109E-3 \\ \hline
Ours & Relative $L_1$ error on one point & \textbf{6.781E-6} & \textbf{4.915E-5} & \textbf{1.084E-5} & \textbf{1.972E-5}  \\\hline
\end{tabular}
\caption{Results of various methods on the BSB equations with different dimensions and evaluation metrics.}
\label{tab:BSB}
\end{table}

\begin{table}[]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Method & Evaluation Metric  & Dim=$10^2$       & Dim=$10^3$   & Dim=$10^4$   & Dim=$10^5$   \\ \hline
FBSNN \cite{raissi2018forward} & Relative $L_2$ error of $u$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
DeepBSDE \cite{han2018solving} & Relative $L_2$ error of $u$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
DeepSplitting \cite{beck2021deep} & Relative $L_2$ error of $u$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
Ours & Relative $L_2$ error of $u$ &\textbf{1.159E-4} & \textbf{3.508E-4} & \textbf{1.216E-5} & \textbf{1.937E-7} \\\toprule \hline
Method & Evaluation Metric  & Dim=$10^2$       & Dim=$10^3$   & Dim=$10^4$   & Dim=$10^5$   \\ \hline
FBSNN \cite{raissi2018forward} & Relative $L_2$ error of $u_t$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
DeepBSDE \cite{han2018solving} & Relative $L_2$ error of $u_t$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
DeepSplitting \cite{beck2021deep} & Relative $L_2$ error of $u_t$ & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
Ours & Relative $L_2$ error of $u_t$ & \textbf{4.918E-3} & \textbf{3.243E-3} & \textbf{2.655E-3} & \textbf{3.792E-5}  \\ \toprule\hline
Method & Evaluation Metric  & Dim=$10^2$       & Dim=$10^3$   & Dim=$10^4$   & Dim=$10^5$   \\ \hline
FBSNN \cite{raissi2018forward} & Relative $L_1$ error on one point  & 6.327E-5  & 7.062E-5 & 4.916E-5  & 5.218E-5 \\ \hline
DeepBSDE \cite{han2018solving} & Relative $L_1$ error on one point & N.A.  & N.A.  & N.A.  & N.A. \\ \hline
DeepSplitting \cite{beck2021deep} & Relative $L_1$ error on one point  & 1.034E-2  & Diverge & Diverge  & Diverge \\ \hline
Ours & Relative $L_1$ error on one point & \textbf{2.711E-5} & \textbf{6.208E-6} & \textbf{2.513E-5
} & \textbf{3.614E-5}  \\\hline
\end{tabular}
\caption{Results of various methods on the HJB-Lin equations with different dimensions and evaluation metrics.}
\label{tab:HJB_Lin}
\end{table}

This subsection presents the results comparing SDGD based on PINN and other strong non-PINN baselines \cite{beck2021deep, han2018solving, raissi2018forward} for the BSB and HJB equations. 

Tables \ref{tab:BSB} and \ref{tab:HJB_Lin} showcase the comparative results between our algorithm and the baselines on the BSB and HJB equations, respectively. Specifically, we present the prediction performance of various models on both the solution field ($u$ and/or $u_t$) and individual points, which are introduced in the experimental setup. If predictions are to be made across the entire domain, only our PINN-based approach can achieve that, while other methods can only make predictions at a single point. This showcases the advantage of PINN in not requiring a mesh/grid, unlike other methods that rely on temporal discretization. Even in predicting at a single point, PINN performs exceptionally well and achieves the best results among various methods. This is because although PINN is trained across the entire domain, its neural network possesses strong interpolation capabilities, enabling accurate predictions at single points. Other methods perform poorly on these PDEs because the scale of the entire PDE solution is very large, leading to potential issues such as numerical overflow or gradient explosion. However, in the case of PINNs, we cleverly employ boundary/terminal condition augmentation to mitigate these issues.

\begin{table}[]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Method & PDE  & Dim=$10^2$       & Dim=$10^3$   & Dim=$10^4$   & Dim=$10^5$   \\ \hline
FBSNN \cite{raissi2018forward} & Allen-Cahn & 1.339E-3	& 2.184E-3	&7.029E-3&	5.392E-2
 \\ \hline
DeepBSDE \cite{han2018solving} & Allen-Cahn & 4.581E-3	&2.516E-3&	2.980E-3	&2.975E-3
 \\ \hline
DeepSplitting \cite{beck2021deep} & Allen-Cahn & 3.642E-3	&1.563E-3	&2.369E-3&	2.962E-3
 \\ \hline
Ours & Allen-Cahn & \textbf{7.815E-4}&	\textbf{3.142E-4}&\textbf{7.042E-4}&	\textbf{2.477E-4} \\ \toprule\hline
Method & PDE & Dim=$10^2$       & Dim=$10^3$   & Dim=$10^4$   & Dim=$10^5$   \\ \hline
FBSNN \cite{raissi2018forward} & Semilinear Heat & 3.872E-3	&8.323E-4	&7.705E-3	&1.426E-2
 \\ \hline
DeepBSDE \cite{han2018solving} & Semilinear Heat & 3.024E-3&	3.222E-3&	3.510E-3&	4.279E-3
 \\ \hline
DeepSplitting \cite{beck2021deep} & Semilinear Heat &  2.823E-3	&3.432E-3&	3.439E-3	&3.525E-3
 \\ \hline
Ours & Semilinear Heat & \textbf{1.052E-3}&	\textbf{5.263E-4}&	\textbf{6.910E-4}&	\textbf{1.598E-3}
  \\\hline
\toprule\hline
Method & PDE & Dim=$10^2$       & Dim=$10^3$   & Dim=$10^4$   & Dim=$10^5$   \\ \hline
FBSNN \cite{raissi2018forward} & Sine-Gordon & 2.641E-3	&4.283E-3&2.343E-1&5.185E-1
 \\ \hline
DeepBSDE \cite{han2018solving} & Sine-Gordon & 3.217E-3&2.752E-3&2.181E-3&2.630E-3
 \\ \hline
DeepSplitting \cite{beck2021deep} & Sine-Gordon &  3.297E-3	&2.674E-3&	2.170E-3	&2.249E-3
\\ \hline
Ours & Sine-Gordon & \textbf{2.265E-3}	&\textbf{2.310E-3}&\textbf{1.674E-3}&\textbf{1.956E-3}
  \\\hline
\end{tabular}
\caption{Results of various methods on the Allen-Cahn, semilinear heat, and sine-Gordon PDEs with different dimensions. The evaluation metric is all relative $L_1$ error on one test point.}
\label{tab:nonlinear_more}
\end{table}

\subsection{More Comparisons with Strong Baselines \cite{beck2021deep, han2018solving, raissi2018forward}}
This subsection presents further results comparing our method based on PINN and other strong non-PINN baselines \cite{beck2021deep, han2018solving, raissi2018forward} for several nonlinear PDEs without analytical solutions.
Thus, we adopt other methods' settings and evaluate the model on one test point, where we show that PINNs still outperform.
other competitors.
\begin{itemize}
\item Allen-Cahn equation.
\begin{equation}
\partial_t u(\bx,t) = \Delta u(\bx,t) + u(\bx,t) - u(\bx,t)^3, \quad (\bx,t) \in \mathbb{R}^d \times [0, 0.3],
\end{equation}
with the initial condition $u(\bx, t) = \arctan(\max_i \bx_i)$. We aim to approximate the solution's true value on the one test point $(\bx, t) = (0,\cdots,0,0.3)$.
\item Semilinear heat equation.
\begin{equation}
\partial_t u(\bx,t) = \Delta u(\bx,t) + \frac{1-u(\bx,t)^2}{1+u(\bx,t)^2}, \quad (\bx,t) \in \mathbb{R}^d \times [0, 0.3],
\end{equation}
with the initial condition $u(\bx, t) = 5 / (10 + 2\Vert\bx\Vert^2)$. We aim to approximate the solution's true value on the one test point $(\bx, t) = (0,\cdots,0,0.3)$.
\item Sine-Gordon equation.
\begin{equation}
\partial_t u(\bx,t) = \Delta u(\bx,t) + \sin\left(u(\bx,t) \right), \quad (\bx,t) \in \mathbb{R}^d \times [0, 0.3],
\end{equation}
with the initial condition $u(\bx, t) = 5 / (10 + 2\Vert\bx\Vert^2)$. We aim to approximate the solution's true value on the one test point $(\bx, t) = (0,\cdots,0,0.3)$.
\end{itemize}

The reference values of these PDEs on the test point are computed by the multilevel Picard method \cite{becker2020numerical, hutzenthaler2020overcoming} with sufficient theoretical accuracy. The residual points of PINN are chosen along the trajectories of the stochastic processes that those PDEs correspond to, see the last section for details.

The results for these three nonlinear PDEs are shown in Table \ref{tab:nonlinear_more}. PINN surpasses other methods in all dimensions and for all types of PDEs. Whether it is predicting the entire domain or the value at a single point, PINN is capable of handling both scenarios. This is attributed to the mesh-free nature of PINN and its powerful interpolation capabilities.


\subsection{PINN and Adversarial Training ($L^\infty$ Loss)}

\begin{table}[]
\centering
\begin{tabular}{|c|cc|cc|cc|}
\hline
\multicolumn{7}{|c|}{HJB-LQG Results for $u$}\\\hline
        & \multicolumn{2}{c|}{Wang et al. \cite{wang20222}}                            & \multicolumn{2}{c|}{He et al. \cite{he2023learning}}                              & \multicolumn{2}{c|}{Ours}                           \\ \hline
Dim     & \multicolumn{1}{c|}{Time}                & Rel. $L_2$ Error & \multicolumn{1}{c|}{Time}                & Rel. $L_2$ Error & \multicolumn{1}{c|}{Time}        & Rel. $L_2$ Error \\ \hline
250     & \multicolumn{1}{c|}{38 hours}            & 1.182E-2         & \multicolumn{1}{c|}{12 hours}            & 1.370E-2         & \multicolumn{1}{c|}{210 minutes} & 6.147E-3         \\ \hline
1,000   & \multicolumn{1}{c|}{\textgreater 5 days} & N.A.             & \multicolumn{1}{c|}{\textgreater 5 days} & N.A.             & \multicolumn{1}{c|}{217 minutes} & 5.852E-3         \\ \hline
10,000  & \multicolumn{1}{c|}{OOM}                 & N.A.             & \multicolumn{1}{c|}{OOM}                 & N.A.             & \multicolumn{1}{c|}{481 minutes} & 4.926E-2         \\ \hline
100,000 & \multicolumn{1}{c|}{OOM}                 & N.A.             & \multicolumn{1}{c|}{OOM}                 & N.A.             & \multicolumn{1}{c|}{855 minutes} & 4.852E-2         \\ \hline
\multicolumn{7}{|c|}{HJB-Lin Results for $\partial_t u$}\\\hline
        & \multicolumn{2}{c|}{Wang et al. \cite{wang20222}}                            & \multicolumn{2}{c|}{He et al. \cite{he2023learning}}                              & \multicolumn{2}{c|}{Ours}                           \\ \hline
Dim     & \multicolumn{1}{c|}{Time}                & Rel. $L_2$ Error & \multicolumn{1}{c|}{Time}                & Rel. $L_2$ Error & \multicolumn{1}{c|}{Time}        & Rel. $L_2$ Error \\ \hline
250     & \multicolumn{1}{c|}{38 hours}            &    1.874E-3    & \multicolumn{1}{c|}{12 hours}            &     1.769E-3  & \multicolumn{1}{c|}{210 minutes} &    4.710E-4      \\ \hline
1,000   & \multicolumn{1}{c|}{\textgreater 5 days} & N.A.             & \multicolumn{1}{c|}{\textgreater 5 days} & N.A.             & \multicolumn{1}{c|}{217 minutes} &   4.455E-4\\ \hline
10,000  & \multicolumn{1}{c|}{OOM}                 & N.A.             & \multicolumn{1}{c|}{OOM}                 & N.A.             & \multicolumn{1}{c|}{481 minutes} &   3.609E-4  \\ \hline
100,000 & \multicolumn{1}{c|}{OOM}                 & N.A.             & \multicolumn{1}{c|}{OOM}                 & N.A.             & \multicolumn{1}{c|}{855 minutes} &   1.305E-3  \\ \hline
\end{tabular}
\caption{Relative $L_2$ error results, running time across different dimensions for the baselines and ours, for two HJB equations requiring adversarial training of PINNs. In all dimensions, our method significantly outperforms the other two baselines in terms of accuracy and speed. The baselines, employing full batch gradient descent, experience slower speed and increased memory requirements in higher-dimensional scenarios. In contrast, our approach utilizes Algorithm \ref{algo:2}, which employs stochastic gradient descent over PDE terms, resulting in faster computation and relatively lower memory consumption. Additionally, the symmetry of the PDE guarantees the accuracy of our method.}
% If no adversarial training 10000: 2.542E-3
\label{tab:adv}
\end{table}

In Wang et al. \cite{wang20222}, it was demonstrated that the class of Hamilton-Jacobi-Bellman (HJB) equation \cite{han2018solving,wang20222} could only be effectively solved using adversarial training, which approximates the $L^\infty$ loss. The first one is the HJB equation with linear-quadratic-Gaussian (LQG) control:
\begin{equation}
\begin{aligned}
&\partial_t u(\bx, t) + \Delta u(\bx, t) - \Vert \nabla_{\bx} u(\bx,t) \Vert^2 = 0, \quad \bx \in \mathbb{R}^d, t\in[0,1]\\
&u(\bx,T)=g(\bx),
\end{aligned}
\end{equation} 
where $g(\bx)$ is the terminal condition to be chosen, with the analytic solution:
\begin{equation}
u(\bx,t) = -\log\left(\int_{\mathbb{R}^d}(2\pi)^{-d/2}\exp(-\Vert \boldsymbol{y} \Vert^2/2)\exp(- g(\bx - \sqrt{2(1-t)}\boldsymbol{y}))d\boldsymbol{y}\right).
\end{equation}
We choose the cost function $g(\bx) = \log((1+\Vert \bx \Vert^2) / 2)$ following \cite{han2018solving,he2023learning,raissi2018forward, wang20222} to obtain the \textbf{HJB-LQG} case.
It is worth noting that the solution's integration in HJB-LQG-Log cannot be analytically solved, necessitating Monte Carlo integration for approximation. We use the relative $L_2$ error approximating $u$ as the evaluation metric for this equation.

The second HJB equation requiring $L^\infty$ loss and adversarial training is the previous HJB-Lin equation but with a different PDE operator:
\begin{equation}
\begin{aligned}
&\partial_t u(\bx, t) + \Delta u(\bx, t) - \frac{1}{d}\sum_{i=1}^d |\partial_{\bx_i}u|^c = -2, \quad \bx \in \mathbb{R}^d, t\in[0,T]\\
&u(\bx,T)=\sum_{i=1}^d \bx_i,
\end{aligned}
\end{equation} 
where $c=1.75, T=1$ and the dimension $d$ can be arbitrarily chosen. The analytic solution is
$
u(\bx,t) = \sum_{i=1}^d \bx_i + T - t.
$
Specifically, Wang et al. \cite{wang20222} demonstrated that as $c$ increases within the range of 1-2, the effectiveness of the $L_2$ PINN training loss diminishes, and PINN relies more on the $L^\infty$ norm loss to achieve smaller testing error. For this equation, we also evaluate the more challenging $L_2$ error of $\partial_t u$.

In this example of the HJB equation with LQG control, we decompose the residual prediction of PINN along each dimension as follows:
\begin{equation}
\partial_t u(\bx, t) + \Delta u(\bx, t) - \mu \Vert \nabla_{\bx} u(\bx,t) \Vert^2 = \frac{1}{d}\sum_{i=1}^d \left\{\partial_t u(\bx, t)+ d\frac{\partial^2u}{\partial \bx_i^2}-\mu \Vert \nabla_{\bx} u(\bx,t) \Vert^2 \right\}.
\end{equation}
For the HJB equation with linear solution, we decompose the residual prediction of PINN along each dimension as in the previous example.

Despite its ability to maintain a low maximal memory cost during training, adversarial training approximating the $L^\infty$ loss is widely recognized for its slow and inefficient nature \cite{shafahi2019adversarial}, which poses challenges in applying it to high-dimensional HJB equations with LQG control. Adversarial training involves optimizing two loss functions: one for the PINN parameters, minimizing the loss through gradient descent, and another for the residual point coordinates, maximizing the PINN loss to approximate the $L^\infty$ loss. This adversarial minimax process, known as adversarial training, is computationally demanding, often requiring multiple rounds of optimization before the resulting residual points can effectively optimize the PINN parameters.

The current state-of-the-art research by Wang et al. \cite{wang20222} and He et al. \cite{he2023learning} has successfully scaled adversarial training to 250 dimensions. In this study, we integrate adversarial training with our proposed approach to enhance the scalability and efficiency of adversarial training in high-dimensional HJB equations requiring adversarial training of PINN.

We use a four-layer PINN with 1024 hidden units, trained by an Adam optimizer \cite{kingma2014adam} with a learning
rate = 1e-3 at the beginning and decay linearly to zero; also, 2e4 test points are employed. We modified the PINN structure as in equation (\ref{eq:modified_nn})
to make the terminal condition automatically satisfied:
\begin{equation}
{u}^{\text{HJB-LQG}}_\theta(\bx, t) = u_\theta(\bx, t) (1-t)+ g(\bx),
\end{equation}
where $g(\bx)$ is the terminal condition, which is $g(\bx) = \log((1+\Vert \bx \Vert^2) / 2)$ in the HJB-LQG-Log case, and $g(\bx) = \sum_{i=1}^d \bx_i$ in the HJB-LQG-Lin case.  Thus, we only need to focus on the residual loss.
We conduct training for a total of 10,000 epochs. In all dimensions, we employ Algorithm \ref{algo:2} with 100 batches of PDE terms in the loss function for gradient descent, and Algorithm \ref{algo:2} with 10 batches of PDE terms in the loss function for adversarial training. For both our methods and the baseline methods proposed by Wang et al. \cite{wang20222}, we randomly sample 100 residual points per epoch.

The results of adversarial training on the two HJB equations are presented in Table \ref{tab:adv}. For instance, in the 250D HJB-LQG case, achieving a relative $L_2$ error of 1e-2 using \cite{wang20222} requires a training time of 1 day. In contrast, our approach achieves a superior $L_2$ error in just 3.5 hours. In the 1000D case, full batch GD's adversarial training is exceedingly slow, demanding over 5 days of training time. However, our method effectively mitigates this issue and attains a relative $L_2$ error of 5.672E-3 in 217 minutes. Moreover, our approach enables adversarial training even in higher-dimensional HJB equations, specifically 10,000 and 100,000 dimensions, with resulting relative $L_2$ errors of 5.026E-2 and 3.838E-2, respectively. For the HJB-Lin case, our method is also consistently better than others.
The success of our method hinges on the accelerated random gradient and the equation's symmetry.


\subsection{Fokker-Planck PDE: Anisotropic Problem}

% Figure environment removed


So far, the PDEs we have tested are isotropic and symmetric across dimensions. This choice is motivated by commonly used benchmark PDEs in real-world applications that exhibit these properties \cite{han2018solving,wang20222}. However, it is worth exploring whether our method can converge on asymmetric and anisotropic PDEs. Therefore, we design an anisotropic Fokker-Planck (FP) equation corresponding to the Brownian motion as follows:
\begin{equation}
\begin{aligned}
&\partial_t p = -\sum_{i=1}^d \mu_i\partial_{\bx_i} p + \frac{1}{2}\sum_{i=1}^d \partial_{\bx_i}^2 p, \quad t \in [0,1], x\in \mathbb{B}^{d}.\\
&u(\bx, 0) = \Vert \bx \Vert^2.\\
&u(\bx, t)\ \text{given}, \quad t \in [0,1],\bx \in \mathbb{S}^{d-1}.
\end{aligned}
\end{equation}
where $\mathbb{B}^{d}$ and $\mathbb{S}^{d-1}$ are the $d$-dimensional unit ball and sphere, respectively. Its solution is
\begin{equation}
u(\bx, t) = \sum_{i=1}^d(\bx_i - \mu_it)^2 + dt.
\end{equation}
Here each $\mu_i$ is randomly sampled from $\mathcal{N}(1,1)$ for the \textbf{anisotropic case} and all $\mu_i = 1$ for the \textbf{isotropic case}. Specifically, we will compare the convergence results of our method in two different cases to observe the robustness of our approach to anisotropic problems. This anisotropic case is not only asymmetric in its form of differential operators but also features an anisotropic solution across different axes. 

We use the following network architecture to satisfy the initial condition automatically:
\begin{equation}
u^{\text{Brownian}}_\theta(\bx, t) = u_\theta(\bx, t)t+ \Vert \bx \Vert^2.
\end{equation}

In this example of the FP equation corresponding to the Brownian motion, we decompose the residual prediction of PINN along each dimension as follows:
\begin{equation}
\partial_t p +\sum_{i=1}^d\mu_i \partial_{\bx_i} p - \frac{1}{2}\sum_{i=1}^d \partial_{\bx_i}^2 p = \frac{1}{d}\sum_{i=1}^d \left\{\partial_t p +\sum_{j=1}^d\mu_j \partial_{\bx_j} p - \frac{d}{2} \partial_{\bx_i}^2 p\right\}.
\end{equation}



We employ a four-layer PINN with 1024 hidden units, trained using the Adam optimizer \cite{kingma2014adam} with an initial learning rate of 1e-3, exponentially decaying with a factor of 0.9995. Our testing datasets consist of 2e4 points. We sample random boundary points for all equations for the boundary condition and use the regular boundary loss. The points are samples uniformly from the bounded domain.

The results for the FP equation corresponding to the Brownian motion are presented in Figure \ref{fig:brownian}, where the first row presents the convergence curve under different dimensions in the isotropic problem, while the second row showcases those for the anisotropic problem.

\textbf{Regarding the speed of our algorithms compared to full batch gradient descent}, in the case of 100 dimensions (100D), we vary the PDE term batch size and the number of collocation points for Algorithm \ref{algo:1} to match the memory cost of the full batch gradient descent (GD) baseline, aiming to observe the stability of our algorithm with different batch sizes. Regardless of the chosen PDE term batch size, our Algorithm \ref{algo:1} consistently converges faster than the baseline, demonstrating the stability and fast convergence of the stochastic gradient descent (SGD) over the PDE terms approach. For higher dimensions, we include the results of Algorithm \ref{algo:2}, which consistently outperforms Algorithm \ref{algo:1}, while Algorithm \ref{algo:1}, in turn, outperforms the baseline. This highlights the acceleration capability of Algorithm \ref{algo:2} in high-dimensional scenarios. %In the case of 100,000 dimensions (100,000D), the full batch GD baseline encounters memory limitations, whereas Algorithm \ref{algo:2} can converge within a few hours. We also observe that the convergence deteriorates in the case of 100,000 dimensions (9.092E-2). In contrast, those in lower dimensions are always smaller than 1\%, indicating the challenges posed by high dimensionality in optimizing PINNs.

\textbf{Regarding our algorithm's robustness in the anisotropic problem}, by comparing the convergence curves of our method in isotropic and anisotropic cases at the same dimensionality, we observe that our approach maintains fast and stable convergence even in anisotropic cases. Furthermore, the final convergence results do not deteriorate significantly in the anisotropic case. Concretely, the relative $L_2$ errors of the isotropic problem under 100D, 1000D, and 10000D are (4.708E-3, 2.223E-3, 3.800E-3), respectively, while those for the anisotropic problem are (4.906E-3, 2.623E-3, 3.715E-3). Therefore, our method is applicable to both isotropic and anisotropic cases.

Note that this FP equation is solved on a bounded domain, specifically, the unit ball. Generally, problems on bounded domains are somewhat easier because PDEs on unbounded domains lack information at infinity.

\subsection{Schr\"{o}dinger Equation}
This subsection scales up the Tensor Neural Networks (TNNs) \cite{wang2022tensor,wang2022solving} for solving very high-dimensional Schr\"{o}dinger equations. Due to its unique characteristics, the Schr\"{o}dinger equation cannot be addressed by the traditional PINN framework and requires a specialized TNN network structure and corresponding loss function for high-dimensional eigenvalue problems. However, our method can still be flexibly integrated into this framework for scaling up Schr\"{o}dinger equations, demonstrating the versatility of our approach. In contrast, other methods specifically designed for high-dimensional equations exhibit much greater limitations.

\subsubsection{Tensor Neural Network (TNN) for Solving Schr\"{o}dinger Equation}
In general, the Schr\"{o}dinger equation can be viewed as a high-dimensional eigenvalue problem.
\begin{equation}\label{eq:schrodinger}
\begin{aligned}
-\Delta u(\bx) + v(\bx) u(\bx) &= \lambda u(\bx), \quad \bx
\in \Omega,\\
u(\bx) &= 0, \quad \bx \in \partial \Omega,
\end{aligned}
\end{equation}
where $ \Omega = \Omega_1 \times \Omega_2 \times \cdots \times \Omega_d \subset \mathbb{R}^d$ and each $\Omega_i = (a_i, b_i)$, $i = 1, Â· Â· Â· , d$ is a bounded interval in $\mathbb{R}$, $v(\bx)$ is a known potential function, $u(\bx)$ is the unknown solution and $\lambda$ is the unknown eigenvalue of the high-dimensional eigenvalue problem. This problem can be addressed via the variational principle:
\begin{align}
\lambda = \min_{\Phi(\bx)}\frac{\int_\Omega |\nabla \Phi(\bx)|^2d\bx + \int_\Omega v(\bx)\Phi(\bx)^2d\bx}{\int_\Omega \Phi(\bx)^2d\bx}.
\end{align}
The integrals are usually computed via the expensive quadrature rule, e.g., Gaussian quadrature:
\begin{equation}
\int_\Omega \Phi(\bx) d\bx \approx \sum_{n \in \mathcal{N}} w^{(n)}\Phi(\bx^{(n)}),
\end{equation}
where $\mathcal{N}$ is the index set for the quadrature points. The size $\mathcal{N}$ grows exponentially as the dimension increases, incurring the curse of dimensionality.
To overcome this issue, the Tensor Neural Network (TNN) adopts a separable function network structure to reduce the computational cost associated with integration:
\begin{align}
\Phi(\bx;\theta) = \sum_{j=1}^p \prod_{i=1}^d\phi_{j}(\bx_i;\theta_i),
\end{align}
where $\bx_i$ is the $i$th dimension of the input point $\bx$, and $\phi_{j}(\bx_i;\theta_i)$ is the $j$th output of the sub-wavefunction network parameterized by $\theta_i$, and $p$ is the predefined rank of the TNN model.
The integral by quadrature rule of the TNN can be calculated efficiently:
\begin{equation}
\int_\Omega \Phi(\bx; \theta) d\bx = \sum_{j=1}^p  \int_\Omega \prod_{i=1}^d\phi_{j}(\bx_i;\theta_i) d\bx = \sum_{j=1}^p  \prod_{i=1}^d\int_{\Omega_i} \phi_{j}(\bx_i;\theta_i) d\bx_i \approx \sum_{j=1}^p  \prod_{i=1}^d\sum_{n_i \in \mathcal{N}_i} w^{(n_i)}\phi_j(x^{(n_i)};\theta_i),
\end{equation}
where $\mathcal{N}_i$ is the index set for the $i$th dimensional numerical integral. Thus, the numerical integrals can be computed along each axes separately to reduce the exponential cost to linear cost.
We refer the readers to \cite{wang2022tensor, wang2022solving} for more details.

With the cheap quadrature achieved by TNN's separable structure, the loss function to be minimized is
\begin{equation}
\min_\theta\ell(\theta) = \frac{ \sum_{n \in \mathcal{N}} w^{(n)}|\nabla \Phi(\bx^{(n)}; \theta)|^2 +  \sum_{n \in \mathcal{N}} w^{(n)}v(\bx^{(n)})\Phi(\bx^{(n)}; \theta)^2}{ \sum_{n \in \mathcal{N}} w^{(n)} \Phi(\bx^{(n)}; \theta)^2},
\end{equation}
where $\Phi(\bx; \theta)$ is the neural network wave function with trainable parameters $\theta$.

The primary computational bottleneck of TNNs for this problem lies in the first-order derivatives in the loss function, while other terms only contain zero-order terms with lower computational and memory costs:
\begin{equation}
\frac{\partial}{\partial \bx_i} \Phi(\bx; \theta) = \sum_{j=1}^p \frac{\partial}{\partial \bx_i}\phi_{j}(\bx_i;\theta_i)\prod_{k=1, k\neq i}^d\phi_{j}(\bx_k;\theta_k)
\end{equation}
Since TNN is a separable architecture, the $d$-dimensional SchrÃ¶dinger equation requires computing the first-order derivatives and performing quadrature integration for each of the $d$ subnetworks. Consequently, the computational cost scales linearly with the dimension. In comparison to previous examples of PINN solving PDEs, TNN incurs larger memory consumption for first-order derivatives due to the non-sharing of network parameters across different dimensions, i.e., $d$ independent first-order derivatives of $d$ distinct subnetworks are required, resulting in increased computational requirements.

Mathematically, the computational and memory bottleneck is the gradient with respect to the first-order term of the neural network wave function $\Phi(\bx; \theta)$
\begin{equation}
\text{grad}(\theta) = \sum_{n \in \mathcal{N}} w^{(n)}\frac{\partial}{\partial\theta}|\nabla \Phi_\theta(\bx^{(n)})|^2 = \sum_{n \in \mathcal{N}} w^{(n)}\left(\sum_{i=1}^d\frac{\partial}{\partial\theta}\left|\frac{\partial}{\partial \bx_i} \Phi_\theta(\bx^{(n)})\right|^2\right).
\end{equation}
By employing the proposed SDGD approach, we can simplify the entire gradient computation:
\begin{equation}
\text{grad}_I(\theta) = \sum_{n \in \mathcal{N}} w^{(n)}\left(\frac{d}{|I|}\sum_{i \in I}\frac{\partial}{\partial\theta}\left|\frac{\partial}{\partial \bx_i} \Phi_\theta(\bx^{(n)})\right|^2\right),
\end{equation}
where $I \subset \{1,2,\cdots,d\}$ is a random index set.
Therefore, we can sample the entire gradient at the PDE term level to reduce memory and computational costs, and the resulting stochastic gradient is unbiased, i.e., $\mathbb{E}_I[\text{grad}_I(\theta)] =\text{grad}(\theta)$ ensuring the convergence of our method. The gradient accumulation can also be done for large-scale problems by our method, see Section \ref{sec:GAPC} for details.

It is important to note that our method is more general compared to the traditional approach of SGD solely based on quadrature points. In particular, in extremely high-dimensional cases, even computing the gradient for a single quadrature point may lead to an out-of-memory (OOM) error. This is because the traditional approach requires a minimum batch size of $d$ PDE terms and 1 quadrature point, where $d$ is the dimensionality of the PDE. However, our method further reduces the computational load per batch to only 1 PDE term and 1 quadrature point.


\subsubsection{Experimental Setup}
We consider two classical examples of the SchrÃ¶dinger equation: the Laplace eigenvalue problem and the quantum harmonic oscillator (QHO) problem.

\textbf{Laplace eigenvalue problem}. The potential function in the SchrÃ¶dinger equation in equation (\ref{eq:schrodinger}) is zero function $v(\bx) = 0$. The domain is chosen as $\Omega = \prod_{i=1}^d\Omega_i= [0,1]^d$ where each $\Omega_i = [0,1]$ and the exact smallest eigenvalue and eigenfunction are $\lambda = d\pi^2$ and $u(\bx) = \prod_{i=1}^d \sin(\pi \bx_i)$. Following \cite{wang2022tensor}, the quadrature scheme for TNN divides each $\Omega_i$ into ten identical subintervals, and sixteen Gauss quadrature points are selected for each subinterval. To train the TNN, the Adam optimizer \cite{kingma2014adam} is used with a learning rate of 1e-3. The TNN is with rank $p = 1$, depth 2, and width 50 for the sub-wavefunctions in each dimension. For this problem, we evaluate the performance of SDGD.

\textbf{Quantum harmonic oscillator (QHO)}. The potential function in the SchrÃ¶dinger equation in equation (\ref{eq:schrodinger}) is $v(\bx) = \Vert\bx\Vert^2$. The original problem is defined on an infinite interval. Therefore, we truncate the entire domain to $[-5,5]^d$. The exact smallest eigenvalue and eigenfunction are $\lambda = d$ and $u(\bx) =  \exp(-\Vert \bx_i \Vert^2 / 2)$. We use the same model structure and hyperparameters as the Laplace eigenvalue problem. For this problem, we test the effectiveness of the Gradient Accumulation method based on our further decomposition of PINN's gradient.

The test metric we report is the $L_1$ relative error of the minimum eigenvalue, given by $\frac{| \lambda_{\text{true}} - \lambda_{\text{approx}} |}{| \lambda_{\text{true}} |}$. Due to the diminishing nature of the eigenfunctions in high dimensions, we only report the accuracy of the eigenvalues. It is important to note that the eigenvalues carry physical significance as they represent the ground state energy.

\begin{table}[]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Dimension & $10^2$ & $10^3$ & $10^4$ & $2*10^4$ \\ \hline
Full Batch GD & 3.882E-8 & 3.889E-8 & 3.811E-8 & OOM \\ \hline
SDGD & 3.772E-8 & 3.439E-8 & 4.003E-8 & 5.139E-8 \\ \hline
\end{tabular}
\caption{Laplace eigenvalue problem: relative $L_1$ error of the predicted eigenvalues of full batch gradient descent (GD) and our SDGD. The results of our algorithm are similar to the full batch GD. In extremely high dimensions, e.g., $2*10^4$ dimensions, the full batch GD encounters out-of-memory (OOM) error, while our algorithm reduces the memory costs and ensures a good convergence result.}
\label{tab:laplace}
\end{table}

% Figure environment removed

% Figure environment removed

\subsubsection{Experimental Results}
On the Laplace eigenvalue problem, we demonstrate the stable convergence of SDGD in Figure \ref{fig:laplace}, and it can scale up to larger problem sizes in Table \ref{tab:laplace}. Specifically, the convergence results of our SDGD are as satisfactory as the full batch GD, and our algorithm can scale up the problem to high dimensions ($2*10^4$ dimensions) where full batch GD directly fails due to insufficient memory.

On the QHO problem, our gradient accumulation method achieves training in $2*10^4$ dimensions within the limited GPU memory in Figure \ref{fig:qho}. Note that gradient accumulation performs theoretically the same as full batch GD, so we do not compare their results, but the former can save GPU memory, thus scaling up PDEs to higher dimensions. These results demonstrate the scalability of our method for PINN and PDE problems and highlight its generality, making it applicable to various physics-informed machine-learning problems.

During the early stages of optimization, we observe quick convergence of TNNs. Since the model parameters are initialized randomly, so there is much room for improvement. At this point, the gradients of the loss function with respect to the parameters can be relatively large. Larger gradients result in more significant updates to the parameters, leading to faster convergence.

Despite the satisfactory convergence results, these problems demonstrate one shortcoming of our proposed approach. Our method is primarily designed to address the computational bottleneck arising from high-dimensional differentiations in PDEs. However, if the problem itself is large-scale and does not stem from differential equations, our method cannot be directly applied. Specifically, in the case of the SchrÃ¶dinger equation, due to the separable nature of its network structure, we employ a separate neural network for each dimension. This results in significant memory consumption, even without involving differentials. Additionally, in this problem, we cannot sample the points of integration / residual points because there is an integral term in the denominator. If we sample the integral in the denominator, the resulting stochastic gradient will be biased, leading to poor convergence. Therefore, for such problems, designing an improved unbiased stochastic gradient remains an open question.

% Figure environment removed

\subsection{On the Effective Dimension of the PDE Solution}
In practical applications, many high-dimensional equation solutions may exhibit some form of low-dimensional structure, meaning they have a low effective dimension. In this chapter, we aim to demonstrate that our method can efficiently capture the low-dimensional solutions of high-dimensional equations. If the equation possesses only a few effective dimensions, the solutions in numerous other dimensions are essentially zero. This may lead to a significant variance in gradients, but we prove that the gradients between different dimensions are actually transferable. Thus, optimizing one dimension effectively assists the learning in other dimensions, ensuring that even when our algorithm selects a batch size of 1, rapid convergence is still achievable.

In particular, we consider the high-dimensional wave equation within the unit ball:
\begin{equation}
\begin{aligned}
&\partial_{tt} u =\Delta u, \quad t \in [0,1], x\in \mathbb{B}^{d}.\\
&u(\bx, 0) = \sum_{i=1}^{eff}\sinh\bx_i, \quad u_t(\bx, 0) = 0.\\
&u(\bx, t)\ \text{given}, \quad t \in [0,1],\bx \in \mathbb{S}^{d-1}.
\end{aligned}
\end{equation}
Here $eff$ is the effective dimension of the solution. We choose $eff = 1$ and $eff = d$ as the low-dimensional and high-dimensional solutions, respectively. The boundary condition is obtained via the ground truth solution of the PDE:
\begin{equation}
u(\bx, t) = \cosh(t) (\sum_{i=1}^{eff} \sinh(\bx_i)).
\end{equation}
Actually, in the low-dimensional solution case where $eff = 1$, the solution is anisotropic which poses additional variance to the gradients of our algorithms. We use the following network architecture to satisfy the initial condition automatically:
\begin{equation}
u_\theta^{\text{Wave}} = u_\theta(\bx, t)t^2 + \sum_{i=1}^{eff} \sinh(\bx_i).
\end{equation}
In this example of the wave equation, we decompose the residual prediction of PINN along each dimension as follows:
\begin{equation}
\partial_{tt} u - \Delta u = \frac{1}{d}\sum_{i=1}^d\left\{u_{tt} - du_{\bx_i\bx_i}\right\}.
\end{equation}
Other experimental details including the choices of points and optimizers are the same as those in the Fokker-Planck equation.

The results for the wave are presented in Figure \ref{fig:wave}, where we present the convergence results with different batch sizes in the scenarios of 1000 and 10000 dimensions. The red line corresponds to cases with a high effective dimension, while the blue line represents cases with a low effective dimension of only one. Firstly, if the equation has a low effective dimension (blue line), our method surprisingly converges faster. This indicates that our approach can quickly capture the low-dimensional solutions within a high-dimensional equation.

Secondly, it's worth noting that we have chosen very small-dimension batch sizes, specifically the PDE terms' batch size. In the 1000-dimensional case, we use a minimum of one dimension, optimizing one dimension at a time. In the 10000-dimensional case, we use only ten dimensions per iteration for gradient descent. However, our method still achieves rapid convergence, suggesting the presence of transferability in the gradients. Optimizing one dimension effectively triggers a transfer learning effect on other dimensions, implying that the gradients between different dimensions are not entirely independent or orthogonal, but exhibit some form of transfer learning.

The transferability can also be validated by theory. Given the neural network structure:
\begin{equation}
u_\theta(\bx)=W_L \sigma (W_{L-1} \sigma(\cdots \sigma(W_1\bx)\cdots )).
\end{equation}
where $\bx\in\mathbb{R}^d$ is the input, and $W_l\in\mathbb{R}^{m_l \times m_{l-1}}$ is the weight matrix at $l$-th layer with $d=m_0$ and $m_L=1$. The first-order and second-order derivatives are given by
\begin{equation}
\frac{\partial u_{\theta}(\bx)}{\partial \bx} = W_L \cdot \Phi_{L-1}(\bx) W_{L-1} \cdot \dots \cdot \Phi_1(\bx) W_1 \in \mathbb{R}^{d},
\end{equation}
\begin{equation}
\frac{\partial^2 u_{\theta}(\bx)}{\partial \bx_i\partial \bx_j} =\sum_{l=1}^{L-1} 
(W_L\Phi_{L-1}(\bx)\cdots W_{l+1})
\text{diag}(\Psi_l(\bx)W_l\cdots\Psi_1(\bx)(W_1)_{:,j})
(W_l\cdots \Phi_1(\bx)(W_1)_{:, i})
\end{equation}
where
\begin{equation}
\Phi_l(\bx) = \text{Diag}\left[\sigma'(W_l\sigma(W_{l-1}\sigma(\cdots\sigma(W_1\bx)\cdots))) \right]\in\mathbb{R}^{m_l \times m_l}
\end{equation}
\begin{equation}
\Psi_l(\bx) = \text{Diag}\left[\sigma''(W_l\sigma(W_{l-1}\sigma(\cdots\sigma(W_1\bx)\cdots)))\right] \in \mathbb{R}^{m_l\times m_l}
\end{equation}
We observed that in the wave equation, the forms of second derivatives in different dimensions are actually mostly shared. Therefore, optimizing the gradient of one dimension also affects other dimensions, which is akin to transfer learning. Moreover, this form has different dependencies on various dimensions only in the parameters of the first layer (notice $(W_1)_i$ and $(W_1)_j$), which means it differs solely in the $i$th and $j$th rows of the first-layer parameters. The parameters of the subsequent layers from the second layer onwards do not actually have any impact.

\section{Conclusions}
The CoD has been an open problem for many decades but recent progress \cite{beck2021deep, han2018solving, raissi2018forward} has been made by several research groups using deep learning methods for specific classes of PDEs. Herein, we proposed a general method based on the mesh-free PINNs method by introducing a new type of stochastic dimension gradient descent or SDGD as a generalization of the largely successful SGD method over subsets of the training set. In particular, we claim the following advantages of the SDGD-PINNs method over  other related methods:

\textbf{Generality to Arbitrary PDEs}. Primarily, certain approaches \cite{beck2021deep, han2018solving, raissi2018forward} are restricted to specific forms of parabolic PDEs. They leverage the connection between parabolic PDEs and stochastic processes, employing Monte Carlo simulations of stochastic processes to train their models. Consequently, their methods are limited to a subset of PDEs. In contrast, our approach is based on PINNs, theoretically capable of handling arbitrary PDEs. Notably, we can address challenging cases such as the wave equation, biharmonic equation, SchrÃ¶dinger equation, and other similar examples, while \cite{beck2021deep, han2018solving, raissi2018forward} cannot.

\textbf{Prediction on the Entire Domain}. Furthermore, these methods \cite{beck2021deep, han2018solving, raissi2018forward} can only predict the values of PDE solutions at a single test point during each training instance. This limitation arises from the necessity of setting the starting point of the stochastic process (i.e., the test point for the PDE solution) before conducting Monte Carlo simulations. Consequently, the computational cost of their methods \cite{beck2021deep, han2018solving, raissi2018forward}  increases greatly as the number of test points grows. In contrast, our approach, based on PINNs, allows for predictions across the entire domain in a single training instance, achieving a ``once for all" capability.

\textbf{Dependency on the Mesh}. Moreover, precisely because these methods \cite{beck2021deep, han2018solving, raissi2018forward} require Monte Carlo simulations of stochastic processes, they necessitate temporal discretization. This introduces additional parameters and requires a sufficiently high discretization precision to obtain accurate solutions. For PDE problems that span long durations, these methods undoubtedly suffer from the burden of temporal discretization. However, our PINN-based approach can handle long-time PDEs effortlessly. Since PINNs are mesh-free methods, the training points can be randomly sampled. The interpolation capability of PINNs ensures their generalization performance on long-time PDE problems.

\textbf{PINNs can also be adapted for prediction at one point}. Although PINNs offer advantages such as being mesh-free and capable of predicting the entire domain, they still perform remarkably well in the setting where other methods focus on single-point prediction. Specifically, if our interest lies in the value of the PDE at a single point, we can exploit the relationship between the PDE and stochastic processes. This single point corresponds to the initial point of the stochastic process, and the PDE corresponds to a specific stochastic process, such as the heat equation corresponding to simple Brownian motion. In this case, we only need to use the snapshots obtained from the trajectory of this stochastic process as the residual points for PINN.

We have proved the necessary theorems that show SDGD leads to an unbiased estimator and that it converges under mild assumptions, similar to the vanilla SGD over collocation points and mini-batches. SDGD can also be used in physics-informed neural operators such as the DeepOnet and can be extended to general regression and possibly classification problems in very high dimensions.

In summary, the new SDGD-PINN framework is a paradigm shift in the way we solve high-dimensional PDEs as it can tackle arbitrary nonlinear PDEs of any dimension providing high accuracy at extremely fast speeds, e.g., the BSB  solution in 100,000 dimensions can be obtained in a few minutes even on a single GPU.

\newpage
\appendix
\section{Proof}
\subsection{Proof of Theorem \ref{thm:variance_1}}\label{appendix:variance_1}
\begin{lemma}\label{lemma:1}
Suppose that we have $N$ fixed numbers $a_1, a_2,\cdots,a_N$ and we choose $k$ random numbers $\{X_i\}_{i=1}^{k}$ from them, i.e., each $X_i$ is a random variable and $X_i = a_n$ with probability $1 / N$ for all $n \in \{1,2,\cdots,N\}$, and $X_i$ are independent. Then the variance of the unbiased estimator $\sum_{i=1}^k X_i / k$ for $\overline{a} = \sum_{n=1}^N a_n$ is $\frac{1}{kn}\sum_{i=1}^n(a_i - \Bar{a})^2$.
\end{lemma}
\begin{proof}
Since the samples $X_i$ are i.i.d.,
\begin{equation}
\begin{aligned}
\mathbb{V}\left[\frac{\sum_{i=1}^k X_i}{k}\right]
&= \frac{1}{k}\mathbb{V}[X_i] = \frac{1}{kn}\sum_{i=1}^n(a_i - \Bar{a})^2
\end{aligned}
\end{equation}
\end{proof}
We assume that the total full batch is with $N_r$ residual points $\{\bx_i\}_{i=1}^{N_r}$ and $N_{\mathcal{L}}$ PDE terms $\mathcal{L} = \sum_{i=1}^{N_{\mathcal{L}}}\mathcal{L}_i$, then the residual loss of the PINN is
\begin{equation}
\frac{1}{2N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)^2,
\end{equation}
where we normalize over both the number of residual points and the number of PDE terms. The full batch gradient is:
\begin{equation}
\begin{aligned}
g(\theta) &= \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\frac{\partial}{\partial \theta}\mathcal{L}u_\theta(\bx_i)\\
&= \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\sum_{j=1}^{N_{\mathcal{L}}}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\\
&= \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\sum_{j=1}^{N_{\mathcal{L}}}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\\
&= \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\left(\left(\sum_{j=1}^{N_{\mathcal{L}}}\mathcal{L}_ju_\theta(\bx_i)\right) - R(\bx_i)\right)\left(\sum_{j=1}^{N_{\mathcal{L}}}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right).
\end{aligned}
\end{equation}
From the viewpoint of Algorithm \ref{algo:1}, the full batch gradient is the mean of $N_rN_{\mathcal{L}}$ terms:
\begin{equation}
g(\theta) = \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\sum_{j=1}^{N_{\mathcal{L}}}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right):= \frac{1}{N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\sum_{j=1}^{N_{\mathcal{L}}}g_{i,j}(\theta),
\end{equation}
where we denote 
\begin{equation}
g_{i,j}(\theta) = \left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right).
\end{equation}
The stochastic gradient generated by Algorithm \ref{algo:1} is given by
\begin{equation}
g_{B, J}(\theta) = \frac{1}{|B||J|N_{\mathcal{L}}}\sum_{i \in B}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\sum_{j \in J}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right) = \frac{1}{|B||J|N_{\mathcal{L}}}\sum_{i \in B}\sum_{j \in J}g_{i,j}(\theta).
\end{equation}
We have the variance
\begin{equation}
\mathbb{V}_{B, J}[g_{B, J}(\theta)] =\mathbb{E}_{B, J}[\left(g_{B, J}(\theta) - g(\theta)\right)^2]= \mathbb{E}_{B, J}[g_{B, J}(\theta)^2] - \mathbb{E}_{B, J}[g(\theta)^2] = \mathbb{E}_{B, J}[g_{B, J}(\theta)^2] - g(\theta)^2.
\end{equation}
From high-level perspective, $\mathbb{V}_{B, J}[g_{B, J}(\theta)]$ should be a function related to $|B|$ and $|J|$. Thus, under the constraint that $|B| \cdot |J|$ is the same for different SGD schemes, we can choose $B$ and $J$ properly to minimize the variance of SGD to accelerate convergence. 
\begin{equation}
\begin{aligned}
\mathbb{E}_{B, J}[g_{B, J}(\theta)^2] &=\frac{1}{|B|^2|J|^2N_{\mathcal{L}}^2}\mathbb{E}_{B, J}\left[\sum_{i \in B}\sum_{j \in J}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\right]^2\\
&= \frac{1}{|B|^2|J|^2N_{\mathcal{L}}^2}\mathbb{E}_{B, J}\Bigg[\sum_{i,i' \in B}\sum_{j,j' \in J}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\cdot\\
&\quad \left(\mathcal{L}u_\theta(\bx_{i'}) - R(\bx_{i'})\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_{j'}u_\theta(\bx_{i'})\right)\Bigg].
\end{aligned}
\end{equation}
The entire expectation can be decomposed into four parts (1) $i=i', j=j'$, (2) $i \neq i', j=j'$, (3) $i=i', j\neq j'$, (4) $i\neq i', j\neq j'$.
For the first part
\begin{equation}
\begin{aligned}
&\quad\frac{1}{|B|^2|J|^2N_{\mathcal{L}}^2}\mathbb{E}_{B, J}\Bigg[\sum_{i\in B}\sum_{j\in J}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)^2\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)^2\Bigg]\\
&=\frac{1}{|B||J|N_{\mathcal{L}}^2}\mathbb{E}_{i,j}\Bigg[\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)^2\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)^2\Bigg]\\
&=\frac{1}{|B||J|N_{\mathcal{L}}^2}\mathbb{E}_{i,j}[g_{i,j}(\theta)^2]\\
&= \frac{1}{|B||J|N_rN_{\mathcal{L}}^3}\sum_{i=1}^{N_r}\sum_{j=1}^{N_{\mathcal{L}}} g_{i,j}(\theta)^2\\
&= \frac{C_1}{|B||J|},
\end{aligned}
\end{equation}
where we denote $C_1 = \frac{1}{N_rN_{\mathcal{L}}^3}\sum_{i=1}^{N_r}\sum_{j=1}^{N_{\mathcal{L}}} g_{i,j}(\theta)^2$ which is independent of $B, J$.

In the second case, since the $i$th sample and the $i'$th sample are independent for $i \neq i'$, we have
\begin{equation}
\begin{aligned}
&\quad\frac{1}{|B|^2|J|^2N_{\mathcal{L}}^2}\mathbb{E}_{B, J}\Bigg[\sum_{i\neq i'\in B}\sum_{j\in J}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\mathcal{L}u_\theta(\bx_{i'}) - R(\bx_{i'})\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_{j}u_\theta(\bx_{i'})\right)\Bigg]\\
&=\frac{|B|-1}{|B||J|N_{\mathcal{L}}^2}\mathbb{E}_{i,i',j}\Bigg[\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\mathcal{L}u_\theta(\bx_{i'}) - R(\bx_{i'})\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_{i'})\right)\Bigg]\\
&=C_2\cdot \frac{|B|-1}{|B||J|}.
\end{aligned}
\end{equation}
In the third case, due to the same reason,
\begin{equation}
\begin{aligned}
&\quad\frac{1}{|B|^2|J|^2N_{\mathcal{L}}^2}\mathbb{E}_{B, J}\Bigg[\sum_{i\in B}\sum_{j\neq j'\in J}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)^2\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_{j'}u_\theta(\bx_i)\right)\Bigg]\\
&=\frac{|J|-1}{|B||J|N_{\mathcal{L}}^2}\mathbb{E}_{i,j,j'}\Bigg[\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)^2\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_{j'}u_\theta(\bx_i)\right)\Bigg]\\
&=C_3\cdot \frac{|J|-1}{|B||J|}.
\end{aligned}
\end{equation}
In the last case,
\begin{equation}
\begin{aligned}
&\quad\frac{1}{|B|^2|J|^2N_{\mathcal{L}}^2}\mathbb{E}_{B, J}\Bigg[\sum_{i\neq i'\in B}\sum_{j\neq j'\in J}\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\mathcal{L}u_\theta(\bx_{i'}) - R(\bx_{i'})\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_{j'}u_\theta(\bx_{i'})\right)\Bigg]\\
&=\frac{(|B|-1)(|J| - 1)}{|B||J|N_{\mathcal{L}}^2}\mathbb{E}_{i,i',j,j'}\Bigg[\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\mathcal{L}u_\theta(\bx_{i'}) - R(\bx_{i'})\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_{j'}u_\theta(\bx_{i'})\right)\Bigg]\\
&=\frac{(|B|-1)(|J| - 1)}{|B||J|N_{\mathcal{L}}^2}\left(\mathbb{E}_{i,j}\Bigg[\left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\mathcal{L}u_\theta(\bx_{i'}) - R(\bx_{i'})\right)\Bigg]\right)^2\\
&=\frac{(|B|-1)(|J| - 1)}{|B||J|}g(\theta)^2
\end{aligned}
\end{equation}
Taking together
\begin{equation}
\begin{aligned}
\mathbb{V}_{B, J}[g_{B, J}(\theta)] &= \mathbb{E}_{B, J}[g_{B, J}(\theta)^2] - g(\theta)^2\\
&=\frac{C_1 + C_2(|B|-1) + C_3(|J| - 1) + (1-|B|-|J|)g(\theta)^2}{|B||J|}\\
&= \frac{C_4|J| + C_5|B| + C_6}{|B||J|}.
\end{aligned}
\end{equation}

\subsection{Proof of Lemma \ref{lemma:variance}}\label{appendix:lemma}
Here, we bound the gradient produced by physics-informed loss functions, which further provides an upper bound for the gradient variance during PINN training using SDGD.

For the stochastic gradient produced by Algorithm \ref{algo:1},
\begin{align}
\mathbb{V}_{B,J}(g_{B, J}(\theta)) &\leq \frac{1}{|B||J|N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\sum_{j=1}^{N_{\mathcal{L}}}\Vert g_{i,j}(\theta) - g(\theta)\Vert^2 \leq \frac{2}{|B||J|N_{\mathcal{L}}}\max_{i,j} \Vert g_{i,j}(\theta) \Vert^2,
\end{align}
where
\begin{align}
g_{i,j}(\theta) := \left(\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right).
\end{align}

For the stochastic gradient produced by Algorithm \ref{algo:2},
\begin{equation}
\begin{aligned}
g_{B, J, K}(\theta) &= \frac{1}{|B||J|N_{\mathcal{L}}}\sum_{i \in B}\left(\frac{N_{\mathcal{L}}}{|K|}\left(\sum_{k \in K}\mathcal{L}_ku_\theta(\bx_i)\right) - R(\bx_i)\right)\left(\sum_{j \in J}\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right)\\
&=\frac{1}{|B||J||K|}\sum_{i \in B}\sum_{j \in J}\sum_{k \in K}\left(\mathcal{L}_ku_\theta(\bx_i) - \frac{R(\bx_i)}{N_{\mathcal{L}}}\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right),
\end{aligned}
\end{equation}
\begin{align}
\mathbb{V}_{B,J,K}(g_{B, J, K}(\theta)) &\leq \frac{1}{|B||J||K|N_rN^2_{\mathcal{L}}}\sum_{i=1}^{N_r}\sum_{j=1}^{N_{\mathcal{L}}}\sum_{k=1}^{N_{\mathcal{L}}}(g_{i,j,k}(\theta) - g(\theta))^2 \leq \frac{2}{|B||J||K|}\max_{i,j,k} \Vert g_{i,j,k}(\theta) \Vert^2,
\end{align}
where
\begin{align}
g_{i,j,k}(\theta) := \left(\mathcal{L}_ku_\theta(\bx_i) - \frac{R(\bx_i)}{N_{\mathcal{L}}}\right)\left(\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right).
\end{align}

We focus on bounding the following two quantities:
\begin{align}
\max_{i, j}|\mathcal{L}_ju_\theta(\bx_i)| &\leq n!(L-1)^n\Vert W_L \Vert \prod_{l=1}^{L-1} \Vert W_l \Vert^n,\\
\max_{i, j}\left\|\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right\|&\leq (n+1)!(L-1)^{n+1}\Vert W_L \Vert \prod_{l=1}^{L-1} \Vert W_l \Vert^{n+1} \max_{\bx \in \Omega}\Vert\bx\Vert.
\end{align}


Recall that the DNN structure:
\begin{equation*}
    u_\theta(\bx)=W_L \sigma (W_{L-1} \sigma(\cdots \sigma(W_1\bx)\cdots ).
\end{equation*}


The neural network $u_\theta(\bx)$ has only one part concerning $\bx$, and after taking the derivative with respect to $\bx$, there is only one term:
\begin{equation}
\frac{\partial u_{\theta}(\bx)}{\partial \bx} = W_L \cdot \Phi_{L-1}(\bx) W_{L-1} \cdot \dots \cdot \Phi_1(\bx) W_1 \in \mathbb{R}^{d},
\end{equation}
where
$\Phi_l(\bx) = \text{diag}[\sigma'(W_l\sigma(W_{l-1}\sigma(\cdots\sigma(W_1\bx))))] \in \mathbb{R}^{m_l\times m_l}$, and the first-order derivative has an upper bound of $\prod_{l=1}^L \Vert W_l \Vert$. 

After taking the second derivative, due to the nested structure of the neural network, there will be $L-1$ parts concerning $\bx$ induced by the derivative of each $\Phi_l(\bx)$ with respect to $\bx$, resulting in $L-1$ terms:
\begin{equation}
\begin{aligned}
\frac{\partial^2 u_{\theta}(\bx)}{\partial \bx^2} &=\left\{\sum_{l=1}^{L-1} 
(W_L\Phi_{L-1}(\bx)\cdots W_{l+1})
\text{diag}(\Psi_l(\bx)\cdots\Psi_1(\bx)(W_1)_{:,j})
(W_l\cdots \Phi_1(\bx)W_1)\right\}_{1 \leq j \leq d},
\end{aligned}
\end{equation}
where
$\Phi_l(\bx) = \text{diag}[\sigma'(W_l\sigma(W_{l-1}\sigma(\cdots\sigma(W_1\bx))))] \in \mathbb{R}^{m_l\times m_l}$, and
$\Psi_l(\bx) = \text{diag}[\sigma''(W_l\sigma(W_{l-1}\sigma(\cdots\sigma(W_1\bx))))] \in \mathbb{R}^{m_l\times m_l}$. Each of these $L-1$ terms has an upper bound of $\Vert W_L \Vert \prod_{l=1}^{L-1} \Vert W_l \Vert^2$, and possesses at most $2(L-1)$ sub-terms depending on $\bx$. Therefore, there are $2(L-1)^2$ sub-terms within the second-order derivative depending on $\bx$. 

Using the principle of induction, after taking the $n$th derivative, there will be $n!(L-1)^n$ parts and each term has an upper bound of $\Vert W_L \Vert \prod_{l=1}^{L-1} \Vert W_l \Vert^n$. If we further take the derivative with respect to $W_l$, then there will be at most $(n+1)!(L-1)^{n+1}$ parts concerning $W_l$, and each term will be upper bounded by $\Vert W_L \Vert \prod_{l=1}^{L-1} \Vert W_l \Vert^{n+1} \max_{\bx \in \Omega}\Vert\bx\Vert$.


Consequently,
\begin{equation}
\Vert g_{i,j}(\theta)\Vert\leq \left\|\mathcal{L}u_\theta(\bx_i) - R(\bx_i)\right\|\left\|\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right\|\leq \left(\max_{i, j}|\mathcal{L}_ju_\theta(\bx_i)| + R\right)\max_{i, j}\left\|\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right\|,
\end{equation}
\begin{equation}
\Vert g_{i,j,k}(\theta)\Vert\leq \left\|\mathcal{L}_ku_\theta(\bx_i) - \frac{R(\bx_i)}{N_{\mathcal{L}}}\right\|\left\|\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right\|\leq \left(\max_{i, j}|\mathcal{L}_ju_\theta(\bx_i)| + R\right)\max_{i, j}\left\|\frac{\partial}{\partial \theta}\mathcal{L}_ju_\theta(\bx_i)\right\|.
\end{equation}

\section{Parallel Computing}
To speed up the training, we explored two avenues for distributed training. First, we used the data-parallel approach \cite{goyal2017accurate}, and second we explored the 1D tensor parallel approach \cite{narayanan2021efficient}.

The programming model for the data-parallel approach is based on the single instruction and multiple data (SIMD), resulting in a weak scaling-driven performance model. In the data-parallel approach, the data is uniformly split into a number of chunks, equal to the number of GPUs. The neural network (NN) model is initialized with the same parameters on all the processes. These neural networks are working on different chunks of the data, and therefore, work on different loss functions. To ensure a consistent neural network model with the same weights and biases across all the processes and during each epoch or iteration of training, a distributed optimizer is used, which averages out the gradient of loss values stored at each processor through an ``all reduce" operation. Subsequently, the averaged gradient of the loss function is sent to all the processors in the communicator world through a broadcast operation. Thus, the parameters of the model are updated simultaneously with the same gradients.

% Figure environment removed

To show the performance of the algorithm presented in this paper, we chose the HJB-Lin problem in 100,000 dimensions. We perform all the computational experiments in this section on A100 GPUs with 80 GB memory. Since the performance of the data parallel approach is driven by weak scaling, we need to first find out the correct hyperparameters such as the depth and width of neural networks, number of collocation points, batch size, etc. We show a detailed Table \ref{tab:S3} showing the performance of the data-parallel approach by varying the hyperparameters, specifically dimensions and the number of collocation points in each dimension. To observe the accuracy of the parallel code we ran a case for solving the HJB-Lin equation up to 5000 iterations and the convergence plot is shown in Figure \ref{fig:S6}, which utilizes the hyperparameters DIM=100, $N_f$=100, where we denote the batch size for dimension in Algorithm 1 of SDGD as DIM and the number of collocation points used in each iteration as $N_f$ in the parallel computing section.  

\begin{table*}[htbp]
\centering
\begin{tabular}{ |p{4cm}||p{3cm}|p{3cm}|p{3cm}|  }
 \hline
 \multicolumn{4}{|c|}{Scaling for data-parallel approach for HJB-Lin equation in 100,000 dimensions} \\
 \hline
Hyperparameter & 1-GPU (Mem and Compute) & 2-GPUs (Mem and Compute)  & Time Per iteration\\
 \hline
 DIM=100, $N_f$=100 & 34 GB and 50 \%  &  34 GB and  25 \% & 1 GPU = 10 sec, ~~~ 2 GPU = 2.5 sec  \\
 \hline 
DIM=1200, $N_f$=100 & 67 GB and 25 \%  & 37 GB and 30 \% & 1 GPU = 75  sec, ~~~ 2 GPU =  25 sec \\
 \hline 
DIM=2000, $N_f$=100 &  OOM Error & 37 GB and   50 \% & 1 GPU = NA, ~~~~ 2 GPU = 38 sec \\
\hline
\end{tabular}
\caption{Scaling for data-parallel approach with 3 hidden layers and 1024 neurons in each layer. Experiments are performed on A100 GPUs with 80 GB memory. Here we denote the batch size for dimension in Algorithm 1 of SDGD as DIM and the number of collocation points used in each iteration as $N_f$ in the parallel computing section}
\label{tab:S3}
\end{table*}

\begin{table}[htbp]
\centering
\begin{tabular}{ |p{4cm}||p{3cm}|p{3cm}|p{3cm}|  }
 \hline
 \multicolumn{4}{|c|}{Scaling for Data parallel for 100,000D HJB-Lin equation with well-balanced load} \\
 \hline
Hyperparameters & 1-GPU (Mem and Compute) & 2-GPUs (Mem and Compute)  & Time Per iteration\\
 \hline
DIM=2000, $N_f$=1000 &  OOM Error & 80 GB, and   100\% & 1 GPU = NA, ~~~~ 2 GPU = 100 sec \\
\hline
\end{tabular}
\caption{Scaling for data-parallel approach with optimal hyperparameters endowed with 9 hidden layers and 1024 neurons in each layer.}
\label{tab:S4}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{ |p{4cm}||p{3cm}|p{3cm}|  }
 \hline
 \multicolumn{3}{|c|}{Performance: Serial vs data parallel code for 100,000D HJB-Lin} \\
 \hline
Hyperparameters & 1-GPU  & 2-GPUs \\
 \hline
DIM=100, $N_f$=100 &  6.17 Hrs & - \\
\hline
DIM=2000, $N_f$=1000 & - & 3.14 Hrs \\
\hline
\end{tabular}
\caption{Scaling results for serial and parallel implementation.}
\label{tab:S5}
\end{table}

For all the cases shown in Table \ref{tab:S3}, the GPU is not saturated completely, and therefore scaling is affected by memory latency. In Table \ref{tab:S4}, we show an optimal set of hyperparameters required to saturate the GPUs. With these parameters, we ran the proposed algorithm till the full convergence for the HJB-Lin equation in 100,000 dimensions, and the wall times are shown in Table \ref{tab:S5}. We see a very good scaling but there is still scope for further optimization. Further optimization of code and scaling it across the nodes will be considered in future work.

% Figure environment removed

%%%% A para on Tensor parallel approach and Table
Next, we present the results of the tensor parallel algorithm \cite{narayanan2021efficient}. In the tensor parallel algorithm, we split the weight matrices of neural networks along the column  dimension as shown in Figure \ref{fig:TPara}. Thereafter, the forward pass and backward pass are performed using the pipeline approach.
Since in the present study, our model is not very large but to saturate the GPU, we have used 8 hidden layers and 2048 neurons and performed the parallelization on 2 GPUs. The tensor parallel approach did not scale as well due to the effect of pipe-lining deteriorating the performance. We show a comparison of the tensor and data parallel approaches in Table \ref{tab:S6}. 

\begin{table}[hbtp]
\centering
\begin{tabular}{ |p{4cm}||p{3cm}|p{3cm}|  }
 \hline
 \multicolumn{3}{|c|}{Performance: data vs tensor parallel code for 100,000D HJB equation on 2 GPUs} \\
 \hline
Hyperparameters & data parallel  & tensor parallel \\
 \hline
DIM=2000, $N_f$=1000 & 100 s/iteration & 105 s/iteration \\
\hline
\end{tabular}
\caption{Scaling results for data-parallel and tensor parallel implementation.}
\label{tab:S6}
\end{table}

\newpage
\bibliographystyle{plain}
\bibliography{main_arxiv}
%\bibliographystyle{apalike}



\end{document}

