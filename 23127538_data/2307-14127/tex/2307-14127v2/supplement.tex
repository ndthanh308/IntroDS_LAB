\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{9144} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Creative Birds: Self-supervised Single-view 3D Style Transfer\\
(Supplementary
Material)}

% \author{{Renke Wang$^{1*}$\, Guimin Que$^{1*}$, Shuo Chen$^{2\dagger}$, Xiang Li$^3$, Jun Li$^{1\dagger}$, Jian Yang$^1$}\\
% \\
% {$^1$PCA Lab, Nanjing University of Science and Technology, China}\\
% {$^2$RIKEN, $^3$Nankai University}\\
% {\tt\small \{wrk226,qgm226131,xiang.li.implus,junli,csjyang\}@njust.edu.cn 
%    shuo.chen.ya@riken.jp}\\
% {\tt\small $^{*}$Contributes equally\ \ \ \ \ \ $^{\dagger}$Corresponding author}\\
% % \thanks{111111111111111111111111111111111111111}
% }

\maketitle

\begin{abstract}
This supplementary material consists of the following four parts: additional info regarding the loss function (section \ref{1}), a description of our evaluation interface for the user study (section \ref{2}), an experiment on DRGNet with different numbers of layers (section \ref{3}) and more comparison results (section \ref{4}).
% 不同视角的效果（gif)
\end{abstract}


\vspace{-4mm} % reduce vertical space between abstract and introduction



\section{Additional Info on 3D Reconstruction Loss}\label{1}
In shape transformation, single-view 3D reconstruction is performed. Due to space constraints, only mask loss and perceptual loss are presented in the main paper. However, as in UMR \cite{li2020self}, we also use some other losses to improve the visual effect of the reconstructed results. 
% Here, we provide a brief description of each of those losses.

Mask loss alone is insufficient for shape reconstruction because it only provides information about a single viewpoint. Therefore, we use deformation loss \cite{li2020self} to enable the model properly incorporate the 3D prior from the mesh template. In addition, graph laplacian constraint \cite{liu2019soft,cmrKanazawa2018,liu2019soft} and edge regularization \cite{wang2018pixel2mesh} are also employed to help smooth the reconstruct shape. 
In order to improve the visual effect of texture, we employ distance transform loss \cite{cmrKanazawa2018} to encourage texture flow select pixel inside the instance mask, and use texture cycle loss \cite{li2020self} to further optimize the position of the selected pixel.
Moreover, we ensure the semantic consistency \cite{li2020self} by constraining the chamfer distance and the l2 distance of each semantic part and its center between the input image and the rendered image. Lastly, multiple camera hypothesis \cite{tulsiani2018multi} is employed to avoid local minima (we used eight camera hypothesis here).

% % 平滑
% \textbf{Graph Laplacian Constriant}
% is to make the reconstructed object's surface smoother by constraining the laplacian of the mesh vertices \cite{liu2019soft,cmrKanazawa2018,liu2019soft}.


% \textbf{Edge Regularization}
% prevent the irregularly-sized faces through constraining the edge length \cite{wang2018pixel2mesh,gkioxari2019mesh}.

% %充分融合mesh template的先验信息
% \textbf{Deformation Loss}
% penalizes the total deformed distance to ensure that reconstructed results are not too different from the template mesh \cite{li2020self}.


% %保证texture的取值范围
% \textbf{Distance Transform Constraint}
% sampling the distance transform field of the instance mask based on each input image pixel, and prevent texture flow to select pixel from the background \cite{cmrKanazawa2018}.

% %进一步优化texture的取值位置
% \textbf{Texture Cycle Loss}
% penalizing the distance between the corresponding pixels in input image and the rendered image, and prevents texture flow to select pixels from wrong locations \cite{li2020self}.

% %局部最优
% \textbf{Multiple Camera Loss}
% helps to avoid the local minima of reconstruction through sampling multiple camera poses from probability distribution and minimizing their entropy \cite{tulsiani2018multi,insafutdinov2018unsupervised}.


% %语义一致
% \textbf{Part Center Loss}
%  is used to constrain the distance of semantic centers between rendered result and the input image, thus ensuring the semantic alignment \cite{li2020self}.

% \textbf{Semantic Consistency Constraint}
% assists in camera pose prediction by calculating the chamfer distance of semantic parts between rendered result and the input image \cite{li2020self}.


% Figure environment removed


\section{User Study}\label{2}
As stated in the "Experiments" section of our paper, we conducted a user study to compare our model to existing models. It consisted of three main components: a comparison of shape transformation (five questions about NC, DSN, KPD, NT and ours), a comparison of texture transformation (five questions about AdaIN and ours, five questions about LST and ours, five questions about EFDM and ours), and a judgement of realism (five T/F questions). Part of the evaluation interface is shown in Fig. \ref{fig:userstudy}.

\section{Additional Experiment on DRGNet}\label{3}
After proving the efficacy of DRGNet for feature coordination, it remains unclear how many layers of DRGNet are needed for the transformation task, so here we conducted further experiments on the number of layers of DRGNet. The results are shown in Fig. \ref{fig:ablation}. 

\section{More Results}\label{4}
Here, we provide more results to help the reader evaluate the performance of our model. We show our comparison results from two aspects: 1. shape transformation (Fig. \ref{fig:shape}), 2. texture transformation (Fig. \ref{fig:tex}).   

% \section{Application in other animals}\label{5}
% As our model is suitable for a wide range of animals, it is further evaluated on the horse category, see Fig. 7. In comparison to other methods, our results exhibit superior performance in terms of integrating the shape characteristics of the source and target. Furthermore, we also observe a greater diversity in the results.

% Figure environment removed


% Figure environment removed

% Figure environment removed
% \newpage
\clearpage\clearpage

{\small
\bibliographystyle{ieee_fullname}
\bibliography{supplement}
}

\end{document}