\section{Disparity Compensation Methods}
\label{sec:algo}

We now present our disparity compensation approach. We first highlight several challenges we aim to address in Section~\ref{sec:challenges}. In Section~\ref{sec:DCA_algo} we describe the Disparity Compensation Algorithm (DCA), our primary contribution; we discuss the accuracy of DCA in Section~\ref{sec:accuracy}, and provide its time complexity  in Section~\ref{sec:timecomplexity}. 


\subsection{Challenges}
\label{sec:challenges}
Our goal is to find the optimum number of points to allocate to protected groups in order to minimize disparity. This task can be thought of as an optimization task, in which the goal is to pick a bonus vector $\vec{B}$ such that the $L^2$ norm of the disparity vector $\vec{D}$ is minimized.  Formally, we want to minimize the disparity:
\begin{equation*}
\begin{aligned}
& \underset{B}{\text{minimize}}
& & ||\vec{D}(\vec{B})||_2 \\
& \text{subject to}
& & b_i \geq 0
\end{aligned}
\end{equation*}


Where $\Vec{D}(\vec{B})$ is the disparity on a given population as a function of the bonus vector as defined above, containing the number of bonus points given to each fairness attribute.
This minimization is complicated by the following challenges.


\begin{enumerate}
    \item There are a large number of possible solutions. This means that most traditional algorithmic solutions are very slow. For instance, recent fairness algorithms are super-polynomial~\cite{zehlike2022fair}, and not easily scalable.
    \item In a set selection task, such as identifying the $k$-objects, measures to assess the fairness of the rankings are step functions, as their value change with every new candidate selected, or each change in  the ranking order. A small change in the bonus point vector can therefore lead to an arbitrarily large change in the disparity, which  means the optimization functions are not smooth or continuous. As such, they are non-differentiable, and standard derivative-based optimization methods are inapplicable.  
    \item Our minimization function does not exhibit convexity, or even quasi-convexity, which precludes us from using convex optimization techniques.
    \item Evaluating each possible solution is expensive as it requires a re-ranking of the dataset. Non-differential (derivative-free) optimization solutions  are therefore inefficient because they typically re-rank the data hundreds of times ~\cite{belkhir2017per}.
    \item For practical purposes, it is desirable for our methods to be fast enough so that function designers (e.g. school administrators) can iterate over several options to assess the impacts of fairness adjustments. 
.
    
\end{enumerate}  


To address these challenges, we propose using a novel descent-based method to compute the correct number of bonus points to eliminate disparity

\subsection{Disparity Compensation Algorithm}
\label{sec:DCA_algo}
 


\begin{algorithm}
\caption{Core DCA}
\SetAlgoLined
\KwResult{$\vec{B}$}
 O $\leftarrow$ the entire set of available objects\;
 k $\leftarrow$ size of the selection\;
 lr $\leftarrow$ list of learning rates sorted in decreasing order\;
 t $\leftarrow$ number of iterations\;
 $\vec{B}$ $\leftarrow$ weight vector of dimensionality equal to number of fairness attributes initialized randomly\;
\For{L in lr}{
  \For{x in t}{
  S $\leftarrow$ A random sample of $sample\_size$   from O\;
  $\vec{D}_k$ $\leftarrow$ Disparity of the $k$ selection over S after applying $\vec{B}$ bonus points\;
  $\vec{B}$ $\leftarrow$ $\vec{B} - L \times \vec{D}_k$\;
  \For{D in $\vec{B}$}{$D \leftarrow max(D, 0)$}
  
 }
}
\label{algo_SGD}
\end{algorithm}


\begin{algorithm}
\KwResult{$\vec{B}$}
 A $\leftarrow$ An array for computing the average\;
 O $\leftarrow$ the entire set of available objects\;
 $\vec{B}$ $\leftarrow$ The output vector of DCA\;
 k $\leftarrow$ size of the selection\;
 t $\leftarrow$ number of iterations for refinement\;
 \For{x in t}{
  S $\leftarrow$ The next sample in O\;
  $\vec{D}_k$ $\leftarrow$ Disparity with $\vec{B}$ bonus points over S\;
  $\vec{B}$ $\leftarrow$ Adam.step($\vec{B},\vec{D}_k$)\;
  A $\leftarrow$ $A + \vec{B}$\;
 }
 \Return ROUND(AVERAGE(A))\;
 \caption{DCA Refinement}
\label{refine_function}
\end{algorithm}



Traditional descent-based methods cannot be applied to our setting as the presence of  large plateaus and steps renders the function non-differentiable; standard gradient descent methods are derivative-based and cannot be used on non-differentiable optimization functions. We circumvent this issue by using the disparity vector directly, instead of its gradient.

Our Disparity Compensation Algorithm (DCA) (Algorithm~\ref{algo_SGD})  is based on the observation that any descent movement in a dimension that aims at compensating the disparity in that dimension will results in a better outcome as long as it does not flip the sign of the Disparity $\vec{D}$ (i.e., create a reverse disparate impact). 

We consider cases where we want to prevent disparate outcomes in future decisions, not only on a known dataset. Therefore, it is not enough to perfectly address the disparity of the training data, our solution has to be applicable to any similar dataset. The training data can then be seen as a sample drawn from an underlying distribution, and our goal is to minimize disparity for that distribution. Therefore we can use the Central Limit Theorem and the Quantile Central Limit Theorem~\cite{ruppert2011statistics} to estimate the selectivity of the ranking function on the underlying distributions (Section~\ref{sec:accuracy}). Our methods can be applied similarly in the absence of training data if the expected distribution of the dataset is known. 


The algorithm is shown in Algorithm~\ref{algo_SGD}. DCA works by keeping a bonus vector, which is incrementally adjusted in the opposite direction of disparity. DCA loops though decreasing learning rates (step sizes) to reduce the disparity vector (noted $\vec{D}$) as close as possible to zero. For each learning rate, the algorithm adjusts the bonus vector over a fixed number of steps $t$ to get as close to zero as possible using that learning rate; it then goes down to the next learning rate.  At each step the Disparity is only computed on a small sample drawn uniformly at random from the overall distribution (or a representative training set). The entire set of objects $O$ is never looked at directly. For each learning rate, we take a fixed number of samples. In each step, DCA uses the Disparity on the sample to predict the Disparity on the distribution as a whole, using the current best guess for bonus points $\vec{B}$. DCA then adjusts the current best guess in the opposite direction of the disparity. For example, in the case above, if the population is 30\% low income and the selected set is 20\% low income that would lead to a 10\% disparity or 0.1. With a learning rate of 0.2, each Low-Income member of the population would receive $0.1 \times 0.2 = 0.02$ extra points in their scoring function for the next iteration. Then, a new sample would be taken and ranked and the disparity would be calculated again.

We propose a refinement step in Algorithm~\ref{refine_function}. The refinement consists of a for loop that uses an adaptive learning rate (using the Adam method~\cite{kingma2017adam}) to find the best estimate. Instead of using a fixed learning rate for all the parameters, the Adam method uses an individual learning rate for each parameter which is individually optimized based on the change in the gradient, or in our case the disparity. The Adam method is especially useful and popular to deal with the noise created by samples. 
Next, the average of the guesses is taken to further reduce the noise created by the random samples and get a more consistent result.  As we will see experimentally in Section~\ref{sec:refine}, this refinement step results in smoother Disparity compensation results.  

The values for $lr$ and $t$ provide a tradeoff between time and accuracy. We set them empirically for our experiments (Section~\ref{sec:settings}).



\subsection{Accuracy of DCA}
\label{sec:accuracy}
To show the accuracy of DCA, we first consider a variation of the DCA, called {\em Full DCA} that considers the entire dataset, not a sample. Consider two objects $p$ and $q$, $q$ in the top-$k$ and $p$ outside the top-$k$. If switching their positions will reduce the disparity, {\em Full DCA} will always reduce the difference in score between them. Formally:
\begin{theorem}
At every step of Full DCA, if removing object $q$ from the top-k and replacing it with object $p$ would reduce the overall disparity, Full DCA will allocate more bonus points at that step to $p$ than to $q$.
\end{theorem}
Mathematically this means that:
$$
(\vec{B} - L \times \vec{D}) \cdot \vec{F_q} - \vec{B} \cdot \vec{F_q} < (\vec{B} - L \times \vec{D}) \cdot \vec{F_p} - \vec{B} \cdot \vec{F_p})
$$
Or
$$0 > \vec{D}\cdot (\vec{F_p} - \vec{F_q})$$
Where $\vec{F_p}$ and $\vec{F_q}$ are the fairness attribute vectors, $\vec{D}$ is the disparity, $L$ is the step size, and $\vec{B}$ is the bonus vector. When using DCA without sampling this will always be true. This can be shown from the definition of disparity and the given assumption that switching these two objects will reduce disparity:
$$
|\vec{D}|_2 > |\frac{1}{s} \sum_{i \in \textbf{S}}F_i + \frac{1}{s}\times \vec{F_p} - \frac{1}{s}\times \vec{F_q} - \vec{Q}|_2
$$
Where Q is the centroid of the entire distribution (which is constant during the running of DCA) and s is the number of selected objects. Using the definition of the $L_2$ norm we see that the above inequality only holds if:
$$
\vec{D} \cdot \vec{D} > \vec{D} \cdot \vec{D} + 
\frac{2}{s}\times (\vec{F_p} - \vec{F_q}) \cdot \vec{D} + \frac{1}{s^2}\times (\vec{F_p} - \vec{F_q})\cdot(\vec{F_p} - \vec{F_q}))
$$
Which simplifies to:
$$
-\frac{1}{2s}\times (\vec{F_p} - \vec{F_q})\cdot(\vec{F_p} - \vec{F_q}) > \vec{D} \cdot (\vec{F_p} - \vec{F_q})  
$$
Since the left side is always negative, we have shown $$0 > \vec{D}\cdot (\vec{F_p} - \vec{F_q})$$ and that $p$ will always receive more additional bonus points than $q$. 


Unlike {\em Full DCA}, DCA relies on samples of the distribution to efficiently identify the best bonus point vector $\vec{B}$ to apply on the set of fairness attributes $F$ to minimize disparity. The accuracy of DCA then depends on the accuracy of the computation of the Disparity metric $\vec{D}$ over the samples as  estimators of the Disparity over the whole dataset.

The Disparity $\vec{D}$ is computed as the distance between the centroid over the set of all objects $O$, $\vec{D}_O$, and the centroid of the $K$ selected objects, $\vec{D}_k$ (Section~\ref{sec:definitions}). In this section, we will show that computing $\vec{D}_O$ and $\vec{D}_k$ over a sample of the dataset gives a good estimation of their value over the whole dataset.


\begin{lemma}
The centroid $\vec{D_s}$ of a sample $s$ over a set of objects $O$ is an unbiased, low-error, estimate of the centroid $\vec{D}_O$ over the entire set of objects $O$.
\end{lemma}

\begin{proof}
This results directly follows from the Central Limit Theorem which states that the mean of a sample will approximate that of the original distribution as long as the sample size is sufficiently large (at least 30). 
\end{proof}

Next we show that the score of the object at the  $k^{th}$ percentile in the sample $s$ is a good estimator of the score of the object at the $k^{th}$ percentile over the whole distribution. 

\begin{lemma}
The score at quantile value $k$ of a sample $s$ over a set of objects $O$ is an unbiased, low-error, estimate of score at quantile value $k$ over the entire set of objects $O$.
\label{lemma-quantile}
\end{lemma}

\begin{proof}
This lemma is a direct result of the Quantile Central Limit Theorem. \cite{ruppert2011statistics}, which says that the quantile $k$ of a sample approximate the corresponding quantile of the original distribution, as long as the density function of the sample at $1-k$ is positive. This qualification is met as long as the sample size is at least $\frac{1}{k}$, which gives us a lower bound on the sample size used in DCA. 
\end{proof}

The variance of the sample quantile as an estimator can be large for values of $k$ close to 0 or 1, and  may lead to low-quality estimations in those cases. This is reflected in real-world experiments, as shown in section \ref{sec:varying}; however for  reasonable values of $k$, the $k^{th}$ percentile of the sample is an accurate estimator of the $k^{th}$ percentile of the distribution.


\begin{lemma}
The centroid $\vec{D_s}_k$ of the  $k$ percent selected objects over a sample $s$ over a set of objects $O$ is an unbiased, low-error, estimate of the centroid $\vec{D}_k$ of the  $k$ percent of selected objects over the entire set of objects $O$.
\end{lemma}

\begin{proof}
From Lemma~\ref{lemma-quantile} we know that the $k^{th}$ percentile value of the sample $s$ is an estimator of the $k^{th}$ percentile value of the distribution of all objects $O$. The top $k$ percent objects from $s$ are therefore taken from the same distribution a the top $k$ percent objects from $O$: the distribution of $O$ truncated at the $k^{th}$ percentile value. 

From the Central Limit Theorem, we know that the mean of a sample will approximate that of the original distribution as long as the sample size is sufficiently large (at least 30). Since both top $k$ percent selections over the sample $s$ and $O$ are taken from the same distribution, the Central Limit Theorem applies, and $\vec{D_s}_k$  is an unbiased, low-error, estimate of $\vec{D}_k$.
\end{proof} 

From the above results, it follows that:

\begin{theorem}
The sample Disparity $\vec{D_s} \equiv \vec{D_s}_k - \vec{D_s}_O$ is an unbiased, low-error, estimate of $ \vec{D} \equiv \vec{D}_k - \vec{D}_O$ of the Disparity over the whole dataset.
\end{theorem}


 \subsection{Time Complexity of DCA}
 \label{sec:timecomplexity}
 The time complexity of DCA does not depend on the size of the dataset but on the sample size and characteristics of the distribution, which allows for fast performance in practice. This is because DCA focuses on correcting the disparity in the underlying distribution rather than on a specific dataset. A training dataset represents a larger sample over the hidden distribution. This allows DCA's execution time to depend on the (smaller) samples' size but not on the dataset size. The algorithm takes a constant multiple of the time taken to compute the disparity on one sample. This time is $$O({sample\_size \times log(sample\_size)})$$ if the sample is fully sorted. 
 The size of the sample needs to be large enough so that the  Central Limit Theorem can be applied, this is generally recognized to be around 30. This means that $30 = sample\_size * k$ and the sample size is $O(\frac{1}{k})$. 
 
 In addition, each subgroup of interest needs to appear in the sample in a reasonable number, for the same reason, so the Central limit theorem can apply. This leads to a final sample size of $O(max(\frac{1}{k},\frac{1}{r}))$ where $k$ is the proportion of elements selected by the ranking process and $r$ is the frequency of the least common group in the dataset. Given this, assuming that $k$ is large enough that the entire sample must be sorted, the time complexity of DCA does not depend on the size of the dataset and is: $$O(max(\frac{1}{k},\frac{1}{r})\times log(max(\frac{1}{k},\frac{1}{r}))$$ 

\subsection{Adjusting the Optimization Goal of DCA for multiple values of $k$}
\label{sec:log_discount}

We have presented DCA for the case where the size of the selection $k$ is known in advance. However, it is often useful to optimize an entire ranking, either when the $k$ is unknown in advance (such as ranked lists in school matching applications), or when the ranking over the entire population is used. 

We propose a modification of DCA that updates the definition of disparity to use the whole ranking along with the logarithmic discounting techniques described in~\cite{yang2017measuring}, to assign more importance to objects selected first than to those selected last. Logarithmic discounting replaces the disparity at k with in our minimization goal of Section~\ref{sec:disparity} with:  $$\frac{1}{Z}\sum_{i\in10,20,30..}^{i=k}\frac{\vec{D}_i}{log_2(i + 1)}$$ 
Where Z is defined as$$\sum_{i\in10,20,30..}^{i=k}\frac{1}{log_2(i + 1)}$$

The computation of $\vec{D}_k$ in Algorithms~\ref{algo_SGD} and~\ref{refine_function} is  replaced with this new logarithmically discounted disparity. This new metric retains the useful characteristics of the previous one: it is a vector with each dimension representing an individual fairness attribute and calculated independently, it ranges between -1 for completely unfair in one direction to 1 for completely unfair in the other direction, is equal to 0 for fair representation, and it can  be summarized by its norm. 

 When using logarithmically-discounted disparity, the minimization problem of Section~\ref{sec:challenges} is then changed to:
 \begin{equation*}
\begin{aligned}
& \underset{B}{\text{minimize}}
& & \sum_{j\in10,20,30..}^{j=k}\frac{||\vec{D}_j(\vec{B})||_2}{log_2(j + 1)} \\
& \text{subject to}
& & b_i \geq 0
\end{aligned}
\end{equation*}
Where $\vec{D}_j$ is the disparity with $k=j$.
 

In terms of time complexity, using the logarithmically-discounted disparity version of DCA takes longer by an additional factor of the size of the sample, as we need to evaluate disparity at every point in the sample, leading to an overall time of:
 $$O((max(\frac{1}{k},\frac{1}{r})\times log(max(\frac{1}{k},\frac{1}{r}))\times max(\frac{1}{k},\frac{1}{r}))$$


Often, only part of the ranking is interesting to the user. Logarithmic-discounted disparity can be adjusted to various ranking needs. For example, users might only be interested in the top half of the ranking. In this case, the disparity outside that section of the ranking can be ignored, and the discounted disparity can still be computed straightforwardly only for values of $k<\frac{N}{2}$.

