\section{Background}
\label{sec:background}
Ranking functions are used in a wide variety of decision systems with high societal impacts: job recruiting tools, school admissions, allocation of resources (e.g., vaccines, treatments, public housing), or risk assessment (e.g., fraud, recidivism). Ensuring that these mechanisms are fair is critical. To this end, we propose a model of disparity compensation measures based on the allocation of targeted bonus points.

We introduce two motivating examples for our work in Section~\ref{sec:motivations}, and set the definitions and parameters of our problem in Section~\ref{sec:definitions}.


\subsection{Motivating Examples}
\label{sec:motivations}

\paragraph{\textbf{NYC High School Admissions}} NYC high school admissions use a deferred acceptance (DA) matching
algorithm~\cite{NYCmatching} similar to the stable marriage algorithm designed by Gale-Shapley~\cite{galeshapley}. The  algorithm  matches students to schools based on students'
preferences and the schools' admission-ranked lists (rubrics). Schools set their own ranking rubrics using metrics such as grades, test scores,  
absences, auditions, or interviews.  Such screens have become a topic of controversy, being targeted as discriminatory because
the underlying metrics often exhibit a high level of
disproportionality in the ranked lists they produce, as students from disadvantaged groups often score lower in some of the metrics used in the rubrics.  

Currently, the NYC Department of Education mostly relies on set-asides (soft quotas) for low-income students to address disparity and produce a diverse group of students at each school. These measures have had mixed results depending on the demographics of the geographical area of the school and on the patterns of student choices. These set-asides are mostly limited to low-income students, although other dimensions of disadvantage have been considered (current school, English language learner, student in temporary housing). However, as mentioned in Section~\ref{sec:related}, combining several quotas can be cumbersome and computationally expensive.

In this paper, we explore the use of compensatory “bonus points" assigned to students who exhibit one or more
dimensions of disadvantage. Our goal is to ensure adequate representation of the underlying population in the students selected by the school
admission rubrics. Because NYC uses a matching algorithm, it is not known in advance how far down its list a school
will accept students; our techniques can adjust to unknown values of the number of selected objects $k$ by minimizing the disparity over all values of $k$, logarithmically discounted to favor smaller $k$ values (Section~\ref{sec:log_discount}). Our experimental evaluation, using NYC high school admission data, shows that our compensatory metrics adapt well to multiple selection percentages (Section~\ref{sec:resschools}).

Some school systems have considered the use of a point-based scheme to diversify schools. In particular, Paris, France has shown good results in improving
socioeconomic diversity~\cite{affelnet} through the use of ``bonus" points for disadvantaged students. However, the system was based on ad hoc bonus points decided somewhat arbitrarily by policymakers, which created some undesirable outcomes in some schools when the points were not calibrated correctly: in one case a high school was assigned a large majority (83\%) of low-income students (instead of the statistical parity goal of 40\%), defeating the diversity purpose. In contrast, we propose a data-driven assignment of bonus points that best reflects the data distribution and its impact on the rankings.  



\paragraph{\textbf{COMPAS Recidivism Data}}
Recidivism algorithms, such as COMPAS, are used to predict the likelihood that a person interacting with the criminal justice system will re-offend, and are used by U.S. Courts to  assist in bail and sentencing decisions. The impact of these decision algorithms  is unquestionable, yet the opacity of the decisions makes it hard to verify that the process is fair.
 
 In 2016 a ProPublica investigation~\cite{angwin_larson_2016} argued that the COMPAS algorithm was unfair to a number of disadvantaged groups, particularly Black Americans. The internal COMPAS ranking algorithm has not been made public; ProPublica based its investigation on  Broward County, Florida data acquired through a public records request, which they made public. Subsequently, this COMPAS data has become a popular dataset for evaluating fairness mechanisms; according to the fairness survey in~\cite{zehlike2021fairness}, it is the most popular large dataset for fairness analysis.

While COMPAS is used for classifying subjects into categories, the categories (deciles) are based on an underlying ranking of subjects. The deciles scores are then often (mistakenly) used as absolute, and not relative, scores of recidivism. Because the scores are based on comparative data, they exacerbate underlying discriminatory practices.
The internal COMPAS algorithm is proprietary, its inner workings, and potential disparate treatments, have been the subject of dispute and speculation~\cite{Rudin2020Age,Jackson2020Setting}. Yet the disparate impact of the COMPAS decile scores
, as they are used in practice, is undeniable. In addition, the process is opaque and not easily understandable. We explore using our disparity compensation techniques in conjunction with the COMPAS scores to address the disparate impacts of the COMPAS tool and report on our results in Section~\ref{sec:rescompas}.

The use of the COMPAS dataset has been the subject of multiple criticisms regarding the ethical use of such data~\cite{COMPASmessy21}.
Our inclusion of COMPAS as a case study is by no means an endorsement of its use for real-life decisions, but rather an illustration of how our compensatory-based approach can help significantly reduce disproportionality on various types of ranking –and classification– functions, even when those are hidden behind black-box proprietary systems. 




\subsection{Definitions}
\label{sec:definitions}

The driving force behind our choices is a focus on explainability so that the fairness compensation choices are simple, transparent, and clearly understandable for stakeholders. This notion of
explainability is especially important to gain support from stakeholders~\cite{goeljustice}. We now define the format of our ranking functions (Section~\ref{sec:ranking functions}) and compensatory bonus points (Section~\ref{sec:bonuspoints}) and our choice of fairness metric (Section~\ref{sec:disparity}).


\subsubsection{Ranking Functions}
\label{sec:ranking functions}



We focus our explanation on score-based ranking functions. Our bonus points can also be adapted to other ranking functions by simulating an underlying score based on rank (see Section~\ref{sec:rescompas}).

\begin{definition}\textit{Score-Based ranking function}
\label{def:WeightedFunction}
We define a score-based ranking function $f$ over a set of $A$ attributes   $a_1,
...., a_A$, over an object $o$ as $f(o)=f(a_1,
...., a_A)$. A ranking process $R$  selects the $k$\% best objects with the highest $f(o)$ values as its answer $R_k$.

\end{definition}



Each object has a set of attributes $A$ that defines its properties and assigns values to them. For the purpose of this work, we recognize a special subset of attributes {\em fairness attributes} (also called protected attributes in the literature), which represent the dimensions on which we want to control for bias and disparate impact. {\em Fairness attributes} may be used by the ranking function $f$ to score the objects, or may not be involved in the ranking but still of interest for assessing the fairness of the outcome.

For example, a school may rank applicants using a 100-point scoring function based on a weighted sum of students' GPA and test scores (attributes). Fairness attributes may include low-income or disability status of the student.


\subsection{Bonus Points}
\label{sec:bonuspoints}

Our approach centers around bonus points to compensate for various dimensions of disparity. Bonus points are multiplied  with the corresponding fairness attribute value and added to the final ranking function score $f(o)$. When the fairness attribute value is binary, this is equivalent to adding the bonus to the final score when that value is equal to 1. For instance, if the low-income status of a school applicant is encoded as a \{0,1\} binary, a bonus of 2 points would add 2 to each low-income applicant's final score; if the low-income status is encoded as a continuous value in [0,1], then the bonus of 2 will be  multiplied by the value of the attribute to give a more precise disparity compensation tool.

We define bonus points as:
\begin{definition}\textit{Bonus Points}
\label{def:Bonus_points}
Given a vector of fairness attributes $\Vec{A_f}$ and a identically shaped vector of bonus points $\Vec{B}$ let the score of an object $o$ be defined as $f_b(o) = f(o) + \Vec{A_f} \cdot \Vec{B} $
\end{definition}

We require bonus points to be positive (negative for scenarios where a lower score is desirable). Negative bonus points would be perceived as a penalty and may not be easily accepted by stakeholders.

In addition to the flexibility of the mechanism, the advantages of using bonus points include: \textbf{intersectionality}, bonus points can be combined and compounded to account for multiple dimensions of bias; \textbf{transparency}, the extent and impact of the fairness intervention are clear to stakeholders; \textbf{comparability}, the score of objects can be easily adjusted and objects compared, increasing transparency and trust; \textbf{predictability}, combined with information on how the selection is done (e.g., historical threshold values), applicants can easily assess their chances and be provided with  \textbf{clarity} as to which actions or interventions are required for selection.

For instance, in our school admission scenario, bonus points could be used to capture the \textit{intersectionality} of students with disability and low-income students: students with both characteristics would receive more bonus points than students with one, or none. This information can \textit{transparently} be published before applications are due, giving clear and \textit{predictable}, and \textit{comparable}, information to families. Admission decisions are \textit{clarified}, with clear thresholds published and the participation of each ranking attribute and fairness compensatory bonus points identified for each applicant.




\subsection{Disparity}
\label{sec:disparity}

We focus on the explainable disparity  from~\cite{Gale2020ExplainingMR} as our target fairness metric as it aims at satisfying statistical parity~\cite{lahoti2019ifair}. Furthermore, it is easily interpretable by humans, behaves well even when the number of dimensions increases and can deal with dimensions with either continuous or discrete data.


Disparity is defined as the vector difference between the average selected object and the average unselected object. Formally it is defined as follows:
\begin{definition}\textit{Disparity}
\label{def:Disparity}
Given a set of $O$ objects and a selection $K$ of $k$ percent of objects in $O$,
Let $\vec{D}^F_O$ be the centroid of $O$ over a set of fairness attributes $F$, and let $\vec{D}^F_k$ be the centroid of the $K$ selected objects  over the same set of attributes. We define the disparity $\vec{D}^F$ as the $|F|$ dimensional disparity vector where $ \vec{D}^F \equiv \vec{D}^F_k - \vec{D}^F_O$.
\end{definition}
When the set of fairness attributes is understood, we omit $^F$ for simplicity of notation: $ \vec{D} \equiv \vec{D}_k - \vec{D}_O$

Intuitively, disparity measures the difference between the average selected object and the average object overall. For example, if the population is 30\% low income and the selected set is 20\% low income that would lead to a 10\% disparity or 0.1. For continuous fairness attributes, disparity is normalized based on the range of values. For instance, in a population with income in [\$0;\$200,000], if the average income of the population is \$40,000 (normalized to 0.2) and the average income of the selected set is \$100,000  (normalized to 0.5) that would lead to a disparity of \$60,000  (normalized to 0.3). Each fairness attribute is one dimension of the disparity vector. Assuming all values are normalized between 0 and 1, a disparity magnitude of -1 or 1 means that the protected attribute is either present only in the population, or only in the protected set respectively. A disparity of zero indicates statistical parity. 
