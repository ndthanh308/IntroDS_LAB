\section{Experimental Setting}
\label{sec:settings}

We now describe the datasets used in our evaluation, the parameters we set for the implementation of our DCA algorithm, and our experimental environment.

\subsection{Datasets}


\paragraph{NYC school dataset}
We evaluate our algorithms using real student data from NYC, which we received through a NYC Data Request~\cite{nycdata}, and for which we have secured IRB approval.  

The data used in this paper consists of the grades, test scores, absences, and demographics of around 80,000 7$^{\text{th}}$ graders each for both the 2016-2017 and 2017-2018 academic years. NYC high schools use the admission matching system described in Section~\ref{sec:motivations} when students are in the 8$^{\text{th}}$ grade; the various attributes used for ranking students therefore are from their 7$^{\text{th}}$ grade report cards. We used data from the 2016-2017 academic year as our training data, and data from the 2017-2018 academic year as our test data. 

We selected our ranking function to model the admission function that several real NYC high schools used for admission in the years 2017 and 2018: a weighted-sum function $f=0.55* GPA + 0.45* TestScores$, where $GPA$ is the normalized average of the students' math, ELA, science, and social studies grades, and $TestScores$ is the normalized average of the math and ELA state test scores. When not otherwise stated, we consider that 5\% of students are selected.

The dataset includes demographics, as well as information about the student's current school. We consider the following dimensions of fairness:
\begin{itemize}
    \item {\em Low-income: } in the NYC public school system, $70\%$ of students qualify as low income.
    \item {\em ELL: }students who are English Learners. These students are obviously disadvantaged by an admission method that takes into account ELA (English Language Arts) grades and test scores.
    \item {\em ENI: } the Economic Need Index, a measure of the overall economic need of students attending the same school as the student. ENI is calculated as the percentage of students in the school who have an economic need. A school is defined as high-poverty if it has an Economic Need Index (ENI) of at least $60\%$. 
    \item {\em Special Ed: }students who are receiving special education services.\footnote{NYC DOE considers these students as part of a separate admission process to ensure that they are matched to a school that fits their need. In our experiments, we consider special education students in the same category as all other students, and consider special education status as a fairness target.}
\end{itemize}



\paragraph{COMPAS}
The COMPAS dataset consists of recidivism data from Broward County Florida as a result of the 2016 ProPublica investigation. The dataset   contains individual demographic information, criminal history, the COMPAS recidivism risk score, arrest records within a 2-year period, for 7214 defendants. COMPAS decile scores, which represent the decile rank of the defendant compared to a target comparison population of defendants, range from 1 to 10. 

We consider the decile score as the ranking function (the lower the better, see discussion in Section~\ref{sec:rescompas}), and compute compensatory bonus points using race as the fairness attribute. 


%The COMPAS algorithm has 5 inputs: age, age at first arrest, number of prior arrests, employment status, and prior parole revocations. \cite{equivant_2019}. After cleaning the dataset has 7214 points. We use decile score as the sole utility parameter and race as the fairness parameter. Race is divided into: African-American, Hispanic, Other, Native American, Caucasian, and Asian.
% Age in the dataset is one of the five main evaluation metrics to create the score. As such, equality across ages is not a reasonable fairness goal. Secondly, we are not given the scores, only the decile the scores fall into, as we will show, this can change the effectiveness of fairness algorithms. We use race as a fairness category. The exact method Northpointe used to compute the decile scores is unknown and as such we treat it as a black box algorithm.




\subsection{Evaluation Parameters}
\label{sec:evaluation_parameters}
\paragraph{Bonus Points.}  The bonus points can be understood as a multiplier over the attribute value. When the attribute is binary, the bonus point value is added to the score of objects with that attribute (e.g., the bonus points for ELL are added to the score of students who are marked as English Learners).  For continuous attributes,  the value of the bonus points is multiplied by the value of the attribute (e.g., the ENI value of the school a student is attending will be multiplied by the ENI bonus points, the resulting product will be added to the score of the student). 

In the final step of the algorithm, we round to the desired bonus point granularity, as decided by stakeholders. For simplicity and efficiency, we restricted bonus points to values with a granularity of 0.5 points in both evaluation scenarios. 

%For most experiments (when p is above 0.05) we used a batch of 500. This means that a selected set of 25 is large enough to give some idea of the distribution. If we select only 0.5\%, we need to increase the batch size linearly to 5000.

\paragraph{DCA vs. Core DCA} In Section~\ref{sec:refine} we evaluate the impact of the refinement step of Algorithm~\ref{refine_function} over the Core DCA algorithm of Algorithm~\ref{algo_SGD}. In the rest of the paper, we use the name DCA to refer to the algorithm {\em with the refinement steps applied}.

\paragraph{Algorithm DCA - Sample Size.} Our rarest fairness category has a frequency of ~10\%, so we picked a sample size of 500 elements to ensure a representation of 50 elements (for our defaults selection percentage of 5\%), enough to show most of the correlation between attributes. %This is why we cannot decrease the batch size as p becomes greater than 0.05 Foatr continuous parameters, we similarly want enough points to get an idea of the sample space. The correct batch size can be tuned like all hyper-parameters, by increasing it until we stop seeing an improvement in results. 

\paragraph{Algorithm DCA - Learning Rate.} We experimented with different learning rates and settled on 3 sets of DCA with 100 rounds for each learning rate. In the first pass we use a learning rate of 1. This gives us the right general area to search. We use a learning rate of 0.1 to further hone in on the correct location. Then, we take a second pass through the data using a  modern weight updating algorithm, Adam, to find the best bonus point values~\cite{kingma2017adam}. Finally, we take the rolling average of the last 100 points to increase stability and avoid too many random effects of unusual samples near the end. 

%\subsection{Evaluation Metrics} 
 



%We found good results taking taking 

%All bonus points for the school section where computed on the 2016-2017 data and tested on the 2017-2018 data.
% This entire process takes only a few passes through the data, and it orders of magnitude faster than even relatively quick other methods which run about 100 passes through the data. The quick results allow a function designer to iterate, trying different functions and seeing how many bonus points they would need in less than 1 minute for each calculation. The algorithm is shown in Algorithm \ref{algo_SGD}.

\subsection{Experimental Environment}
The experiments were all preformed on an Optiplex7060 with 30GB of RAM. The machine has a Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz. Our proposed algorithms were implemented using Python 3.8 and Pandas. The comparison algorithm (Multinomial FA**IR) was implemented in Java using the implementation by the authors of \cite{zehlike2022fair}. %We used the implementation of CMA-ES in \cite{nevergrad}.