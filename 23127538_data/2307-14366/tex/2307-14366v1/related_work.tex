\section{Related Work}
\label{sec:related}

The problem of providing ranking functions and systems with fair outcomes has recently received considerable scrutiny.
Several surveys provide a good overview of the literature both in the Data Management community~\cite{zehlike2021fairness,Fair_Classification}, and in the Recommender Systems community \cite{pitoura2021fairness}. These surveys classify fairness ranking approaches into  pre-processing, in-processing and post-processing techniques depending on when the fairness metrics are applied. Pre-processing techniques transform data before applying the ranking function~\cite{lahoti2019ifair}. Our approach could be classified as pre-processing as the bonus points are applied to the data before ranking.


Several types of pre-processing techniques have been discussed for use by classifiers. Their goal is to pre-process data so that an arbitrary classifier will be less likely to result in an unfair outcome. These transformations are usually done without prior knowledge of the classifier mechanics which can limit this approaches effectiveness. A seminal paper ~\cite{kamiran2012data} discusses several basic techniques for pre-processing data for classifiers, such as suppression, where the some attributes of the objects are removed entirely, and sampling, where some objects are duplicated and others are removed. They also discuss a fairness-accuracy trade-off: as the fairness of the data increases the classifiers become less accurate; this tradeoff is further explored in~\cite{feldman2015certifying}, which provides provably fair solutions, and ~\cite{calmon2017optimized}, which gives a solution based on convex optimization. Similary,  \cite{salimi2019interventional} and \cite{lahoti2019ifair} address a similar problem  using different notions of fairness:  causal fairness, where the user can specify which attributes are allowed to influence the result~\cite{salimi2019interventional}, and individual fairness where the goal is to make sure that objects are treated similarly to their nearest neighbors~\cite{lahoti2019ifair}. 


For ranking applications, in-processing approaches are  more commonly used. These approaches adjust the ranking function to optimize a given fairness goal, either through learning techniques~\cite{radlinski2008learning} or manipulation of weighted functions~\cite{asudeh2019designing}. Celis et al.~\cite{celis2017ranking} propose approximation algorithms to provide rankings as close to the original rank quality metric while satisfying fairness constraints. Yang and Stoyanovich~\cite{yang2017measuring} consider a model of probability-based fairness where each class of objects should be treated equally. This probabilistic interpretation is also considered in~\cite{zehlike2017fa}, which provides probabilistic fairness guarantees in settings with only one protected class; this setting is relaxed in~\cite{zehlike2022fair}, but the approach relies on a Cartesian product of all protected classes and is prohibitively expensive. A drawback of these approaches is that they result in ranking functions that are often opaque and hard to explain to stakeholders; in contrast, we aim to make the disparity compensation process transparent and easily explainable.

Several notions of fairness, as it relates to rankings,  have been proposed; \cite{dwork2012fairness} discusses the notion of group fairness vs. individual fairness in the context of fair classification and argues that  statistical parity is a desirable  measure of fairness. The Disparity metric that we use (Section~\ref{sec:definitions}) can be interpreted as a measure of statistical parity~\cite{Gale2020ExplainingMR}. Exposure~\cite{singh2018fairness} is often used as a measure of fairness, although the definition of exposure varies in the literature; we use the same definition as~\cite{gupta2021online} in our experimental evaluation (Section~\ref{sec:exposure}). Fair ranking inherently involves choices. In \cite{kleinberg2016inherent} the authors prove that different fairness metrics are incompatible, even in the approximate case. Before any dataset-specific choices are made, a choice of how to define fairness for the given problem is required. As mentioned above, we focus on a statistical parity interpretation of fairness, as it is easily explainable and does not require making assumptions about underlying sources of bias.



Outside of academia, the problem of fairness in rankings has also garnered interest recently, in part after a recruiting tool created by Amazon exhibited obvious gender bias~\cite{dena19}; for example, LinkedIn  implemented a fair ranking scheme  in their talent search \cite{geyik2019fairness}.




A common real-world application of fair rankings is school admissions. School districts have used lotteries to select students as a way to integrate schools and reduce bias in selection outcomes. However, lotteries are not a simple answer to solve for disparities and can in fact exacerbate selection bias~\cite{nycd1} or achievement bias~\cite{collegelotteries2021}. To ensure diversity, many school districts such as NYC and Chicago~\cite{ellison2021efficiency} have successfully opted for set-aside quotas for low-income students. However, these affirmative action-based approaches have been shown to be potentially detrimental to minorities~\cite{ehlers2014,kojima2012,hafalir2013} depending on the underlying school choice patterns. In addition, quota-based approaches are difficult to implement when multiple groups should be given protected treatment. Works have considered overlapping quotas~\cite{sonmez2019affirmative} to account for a candidate exhibiting multiple protected characteristics, minimum and maximum bounds for quotas~\cite{aziz2021multi}, or priority assignments rules for quotas~\cite{abdulkadirouglu2021priority}. These approaches are all computationally expensive. In contrast, our disparity compensation bonus points allow for (1) targeted intervention for each dimension of disparity, even overlapping ones, and (2) a computationally efficient approach.

 