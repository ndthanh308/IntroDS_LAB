\section{Experimental Setting}
\label{sec:settings}

We now describe the datasets used in our evaluation, the parameters we set for the implementation of our DCA algorithm, and our experimental environment.

\subsection{Datasets}


\paragraph{NYC school dataset}
We evaluate our algorithms using real student data from NYC, which we received through a NYC Data Request~\cite{nycdata}, and for which we have secured IRB approval.  

The data used in this paper consists of the grades, test scores, absences, and demographics of around 80,000 7$^{\text{th}}$ graders each for both the 2016-2017 and 2017-2018 academic years. NYC high schools use the admission matching system described in Section~\ref{sec:motivations} when students are in the 8$^{\text{th}}$ grade; the various attributes used for ranking students therefore are from their 7$^{\text{th}}$ grade report cards. We used data from the 2016-2017 academic year as our training data, and data from the 2017-2018 academic year as our test data. 

We selected our ranking function to model the admission function that several real NYC high schools used for admission in the years 2017 and 2018: a weighted-sum function $f=0.55* GPA + 0.45* TestScores$, where $GPA$ is the normalized average of the students' math, ELA, science, and social studies grades, and $TestScores$ is the normalized average of the math and ELA state test scores. When not otherwise stated, we consider that 5\% of students are selected.

The dataset includes demographics, as well as information about the student's current school. We consider the following dimensions of fairness:
\begin{itemize}
    \item {\em Low-income: } in the NYC public school system, $70\%$ of students qualify as low income.
    \item {\em ELL: }students who are English Learners. These students are obviously disadvantaged by an admission method that takes into account ELA (English Language Arts) grades and test scores.
    \item {\em ENI: } the Economic Need Index, a measure of the overall economic need of students attending the same school as the student. ENI is calculated as the percentage of students in the school who have an economic need. A school is defined as high-poverty if it has an Economic Need Index (ENI) of at least $60\%$. 
    \item {\em Special Ed: }students who are receiving special education services.
\end{itemize}



\paragraph{COMPAS}
The COMPAS dataset consists of recidivism data from Broward County Florida as a result of the 2016 ProPublica investigation. The dataset   contains individual demographic information, criminal history, the COMPAS recidivism risk score, arrest records within a 2-year period, for 7214 defendants. COMPAS decile scores, which represent the decile rank of the defendant compared to a target comparison population of defendants, range from 1 to 10. 

We consider the decile score as the ranking function (the lower the better, see discussion in Section~\ref{sec:rescompas}), and compute compensatory bonus points using race as the fairness attribute. 





\subsection{Evaluation Parameters}
\label{sec:evaluation_parameters}
\paragraph{Bonus Points.}  The bonus points can be understood as a multiplier over the attribute value. When the attribute is binary, the bonus point value is added to the score of objects with that attribute (e.g., the bonus points for ELL are added to the score of students who are marked as English Learners).  For continuous attributes,  the value of the bonus points is multiplied by the value of the attribute (e.g., the ENI value of the school a student is attending will be multiplied by the ENI bonus points, the resulting product will be added to the score of the student). 

In the final step of the algorithm, we round to the desired bonus point granularity, as decided by stakeholders. For simplicity and efficiency, we restricted bonus points to values with a granularity of 0.5 points in both evaluation scenarios. 


\paragraph{DCA vs. Core DCA} In Section~\ref{sec:refine} we evaluate the impact of the refinement step of Algorithm~\ref{refine_function} over the Core DCA algorithm of Algorithm~\ref{algo_SGD}. In the rest of the paper, we use the name DCA to refer to the algorithm {\em with the refinement steps applied}.

\paragraph{Algorithm DCA - Sample Size.} Our rarest fairness category has a frequency of ~10\%, so we picked a sample size of 500 elements to ensure a representation of 50 elements (for our defaults selection percentage of 5\%), enough to show most of the correlation between attributes. 

\paragraph{Algorithm DCA - Learning Rate.} We experimented with different learning rates and settled on 3 sets of DCA with 100 rounds for each learning rate. In the first pass we use a learning rate of 1. This gives us the right general area to search. We use a learning rate of 0.1 to further hone in on the correct location. Then, we take a second pass through the data using a  modern weight updating algorithm, Adam, to find the best bonus point values~\cite{kingma2017adam}. Finally, we take the rolling average of the last 100 points to increase stability and avoid too many random effects of unusual samples near the end. 


\subsection{Experimental Environment}
The experiments were all preformed on an Optiplex7060 with 30GB of RAM. The machine has a Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz. Our proposed algorithms were implemented using Python 3.8 and Pandas. The comparison algorithm (Multinomial FA**IR) was implemented in Java using the implementation by the authors of \cite{zehlike2022fair}. 