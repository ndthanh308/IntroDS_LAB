% \section{Conclusion of the Proof of
% \cref{thm:dontneedb}}\label{app:twobin}

We now use the powerful combinatorial  \cref{lem:epicbound}
to conclude the proof of \cref{thm:dontneedb}.

\begin{theorem}\label{cor:kindaepicevenifweak}
  $\sum_{x\in X} \lap(x) \le \tilo(p).$
\end{theorem}
\begin{proof}
  In \cref{lem:pigeons} we showed that each $x\in X$ can be
  represented by some $\modp(\sigma m^{-1} k)$ for $\sigma\in \pm
  1, m\in [n], k\in [\ceil{p/n}]$;
  Form a set $Y$ of triples by selecting for each $x\in X$ some
  such representative $(m,k,\sigma)$. Of course each triple can
  only represent one $x$.
  Then by \cref{lem:epicbound} we have:
   \[
     \sum_{x\in X} \lap(x) \le  \sum_{(m,k,\sigma)\in Y} \bigO\left(k+\left(m + \frac{p}{mk}\right)\cdot
     \gcd(k,m)\right) \le \bigO(p) + \bigO(p)\cdot\sum_{(m,k,\sigma)\in Y}
     \frac{\gcd(k,m)}{km} ,
   \]
   where the final inequality was obtained by replacing $k,m$ by
   their maximum possible values and using the trivial bound
   $\gcd(k,m)\le m$.
   We now bound the final term in the sum:
\[
  \sum_{(m,k,\sigma)\in Y} \frac{\gcd(k,m)}{km} \le \sum_{\sigma
  = \pm 1}\sum_{m\in [n]}
  \sum_{d\mid m}\sum_{dk\in [\ceil{p/n}]} \frac{d}{dk m} \le \sum_{m\in
  [n]}\frac{\tau(m)}{m}  \bigO(\log n)\le \bigO(\log^3 n).
\]
Where we have used the well-known bound that $\E_{x\in
[n]}[\tau(x)]\le\bigO(\log n)$ \cite{hardy1979introduction}.
Thus we have the desired bound: $\sum_{x\in X} \lap(x)\le
\tilo(p)$.


\end{proof}

Let random variables $C, M$ denote the number of collisions and
maxload respectively. Let  $\mu = \E[M]$. Let $C_{i,j}$ be $1$ if
$i,j$ collide and $0$ otherwise; our convention is $C_{i,i}=0$.
Using \cref{cor:kindaepicevenifweak} we obtain a bound on $\E[C]$.
\begin{cor}\label{lhs:complicated7}
  $\E[C] \le n^2/4 + \tilo(n^{1/2}).$
\end{cor}
\begin{proof}
  Interpreting \cref{cor:kindaepicevenifweak} probabilistically,
  for any $n$-element set $X\subset [p]$ we have
  \begin{equation}\label{eq:justabovefor1}
    \sum_{y \in X} \E[C_{1,y}] \le n/2 + \tilo(1).
  \end{equation}
  \eqref{eq:justabovefor1} is easily generalized to bound the number of collisions
  between $y\in X$ and any $x\neq 0$.
  In particular, the collisions between $x,X$ are the same as the collisions
  between $1,\modp(x^{-1}X)$ where $x^{-1}$ is the inverse of $x \bmod
    p$. Because \eqref{eq:justabovefor1} holds for arbitrary
  $n$-element sets $X$, we have for any $x\neq 0$
  \[
  \sum_{y \in X} \E[C_{x,y}] \le n/2 + \tilo(1).
  \]

  Now, we are equipped to bound $\E[C]$. Index $X$ as $X=\set{x_1,x_2,\ldots,
      x_n}$. Then
  \begin{equation*}
    \E[C]  = \sum_{i=1}^{n}\sum_{j=1}^{i-1} \E[C_{x_i,x_j}] 
          \le \sum_{i=1}^{n} \paren{i/2 + \tilo(1)}  
          \le n^2/4  + \tilo(n).
  \end{equation*}
\end{proof}
% note: we count self-collisions

Finally, we complete the proof of \cref{thm:dontneedb} by comparing the expected number
of collisions to the number of collisions induced by the maxload.
\begin{proof}[Proof of \cref{thm:dontneedb}]
  The number of collisions $C$ is determined by the maxload. In
  particular,
  % \begin{equation}\label{eq:rhs7ezv1}
    $C = \binom{M}{2}+\binom{n-M}{2}.$
  % \end{equation}
    This is a convex function of $M$.
  % Note that \cref{eq:rhs7ezv1} is a convex function of $M$.
  Thus, applying Jensen's inequality and comparing with 
  \cref{lhs:complicated7} gives:
  \[ \binom{\mu}{2}+\binom{n-\mu}{2} \le n^2/4 + \tilo(n) .\]
  Solving the quadratic in $\mu$ we find:
  $\mu \le n/2+\tilo(\sqrt{n}).$
\end{proof}

