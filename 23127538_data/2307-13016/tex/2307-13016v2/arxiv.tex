\documentclass[twocolumn]{article}[11pt]
\author{Alek Westover}
% \title{Linear Hashing isn't too Picky:\\ For $\R$eal, No Shift, it's Ama$\Z$ing}
\title{Linear Hashing: No Shift, Non-Prime Modulus, For $\R$eal!}
\input{head.tex}
\begin{document}
\maketitle

\abstract{
  In \defn{Linear Hashing} ($\LH$) with $\beta$ bins on a size
  $u$ universe ${\mathcal{U}=\set{0,1,\ldots, u-1}}$, items
  $\set{x_1,x_2,\ldots, x_n}\subset \mathcal{U}$ are placed in
  bins by the hash function
  $$x_i\mapsto (ax_i+b)\mod p \mod \beta$$ 
  for some prime $p\in [u,2u]$ and randomly chosen
integers $a,b \in [1,p]$. The \defn{maxload} of $\LH$ is the
number of items assigned to the fullest bin. Expected maxload for
a worst-case set of items is a natural measure of how well $\LH$
distributes items amongst the bins.

Fix $\beta=n$. Despite $\LH$'s simplicity, bounding $\LH$'s worst-case
maxload is extremely challenging. 
It is well-known that on random inputs $\LH$ achieves
maxload $\Omega\left(\frac{\log n}{\log\log n}\right)$; this is
currently the best lower bound for $\LH$'s expected maxload.
Recently Knudsen established an upper bound of
$\widetilde{\bigO}(n^{1 / 3})$. 
The question ``Is the worst-case expected maxload of $\LH$
$n^{o(1)}$?" is one of the most basic open problems in discrete
math.

In this paper we propose a set of intermediate open questions to
help researchers make progress on this problem. 
We establish the relationship between these intermediate open
questions and make some partial progress on them.
  % The \defn{hashing problem} is to assign $m$ items from a
  % universe $U$ to $n$ \defn{bins} so that all bins receive a
  % similar number of items. We measure the quality of a hashing
  % scheme's distribution by the number of items assigned to the
  % fullest bin, known as its \defn{maxload}. 

  % \defn{Linear Hashing} ($\LH$) is a
  % simple and classical solution to the hashing problem.
  % In $\LH$ items $x\in \set{1,2,\ldots, |U|}$ are
  % mapped to bins $\set{0,1,\ldots, n-1}$ by 
  % $$x\mapsto (ax+b)\mod p \mod n$$ for prime $p\in [|U|, 2|U|]$
  % and randomly chosen integers $a,b \in [1,p]$.
  % Despite $\LH$'s simplicity however, 
  % understanding the expected maxload of $\LH$ for worst-case
  % inputs is a notoriously challenging
  % and wide open question. For $m=n$ the best known lower bound is
  % $\Omega\left(\frac{\log n}{\log\log n}\right)$, whereas the
  % best known upper bound is $\widetilde{\bigO}(n^{1/3})$ due to
  % Knudsen\cite{knudsen_linear_2017}.

  % In this paper we consider three modifications of $\LH$: 
  % (1) $\LH$ without the ``$+b$" shift term, resulting in
  % loss of pairwise-independence. (2) $\LH$ with a composite,
  % rather than prime, modulus. (3) $\LH$ in a continuous setting
  % where the multiplier ``$a$" is chosen from $\R$ rather than $\Z$.
  % We show that $\LH$ is fairly robust to these changes, in
  % particular by demonstrating analogs of known maxload-bounds for
  % these new variants.

  % These results give several new perspectives on $\LH$, in
  % particular showing that properties of $\LH$ such as
  % pairwise-independence, a prime modulus, or even its setting in
  % the integers may not be fundamental. We believe that these new
  % perspectives, beyond being independently interesting, may
  % also be useful in future work towards understanding the maxload of
  % $\LH$.
}

\section{Introduction}
  The \defn{hashing problem} is to assign $n$ items from a
  universe $\mathcal{U}$ to $\beta$ \defn{bins} so that all bins receive a
  similar number of items. In particular, we measure the quality
  of a hashing scheme's load distribution by the number of items
  in the fullest bin; we refer to this quantity as the \defn{maxload}. 
  We desire three main properties of a hashing scheme: 
  (1) small expected maxload for all sets of items, (2) fast
  evaluation time, and (3) small description size.

  \paragraph{Related Work}
  The hashing problem has been extensively studied.
  We use the standard parameters $|\mathcal{U}|\in \poly(n)$,
  $\beta=n$ in the following discussion. We also assume the
  unit-cost RAM model, i.e., that arithmetic operations on
  numbers of size $\Theta(\log n)$ can be performed in constant
  time. We use the abbreviation $\ell(n) = \frac{\log n}{\log\log n}$.

  The hash function which assigns each item independently
  randomly to a bin achieves the smallest possible expected
  maxload in general, namely $\Theta(\ell(n))$
  \cite{mitzenmacher2017probability}.
  However, describing a fully random function requires
  $\Omega(|\mathcal{U}|\log n)$ bits which is extremely large.
  Full independence is not necessary to achieve optimal maxload.
  For instance, Carter and Wegman \cite{carter1977universal} show
  that degree $\Theta(\ell(n))$ polynomials over a finite
  field constitute a $\Theta(\ell(n))$-wise
  independent hash family while still achieving maxload
  $\Theta(\ell(n))$.
  Improving on this result, Celis et al. \cite{celis2013balls}
  demonstrate a hash family achieving maxload $\Theta(\ell(n))$
  with evaluation time $\bigO(\sqrt{\log n})$.

  In fact, it is even possible to achieve optimal maxload with
  constant evaluation time, as demonstrated by Seigel in
  \cite{siegel_universal_2004}. However, Seigel's hash function
  has description size $\poly(n)$.
  Furthermore, Seigel proved that it is impossible to
  simultaneously achieve optimal maxload, constant evaluation
  time, $n^{o(1)}$ description size, and $\Omega(\ell(n))$ independence. 
  However, this still leaves room for improvement if we do not
  require large degrees of independence.

  By itself small independence does not give any good bound on
  maxload; for example, there are pairwise-independent hash
  families with maxload $\Omega\left(\sqrt{n}\right)$ \cite{petershor}.
  However, Alon et al. \cite{alon_is_1997} show that the
  pairwise-independent hash family of multiplication by random
  matrices over $\F_2$ achieves $\bigO(\ell(n) \cdot (\log \log
  n)^2)$ maxload in $\bigO(\log(n))$ evaluation time. 

  The following question remains open:
  \begin{question}
    \label{question:fastandgood}
Is there a hash family with $\bigO(1)$ machine
word description whose evaluation requires $\bigO(1)$ arithmetic
operations that has expected maxload bounded by $n^{o(1)}$?
  \end{question}

\paragraph{Linear Hashing ($\LH$)}
$\LH$ \cite{motwani1995randomized, cormen2022introduction,
sedgewick2014algorithms} is an attractive potential solution to
\cref{question:fastandgood}, trivially satisfying the conditions
of small description size and fast evaluation time.
% $\LH$ heuristically does quite well at spreading out elements
% \cite{sedgewick2014algorithms}.
Despite $\LH$'s simplicity, understanding its
maxload is a notoriously challenging and wide open question. 
The best known lower bound on $\LH$'s maxload is $\Omega\left(\frac{\log
n}{\log\log n}\right)$, whereas the best known upper bound is
$\widetilde{\bigO}(n^{1/3})$ due to an elegant combinatorial
argument of Knudsen \cite{knudsen_linear_2017}.

Let $\mathcal{U} = \set{0,1,\ldots, u-1}$ denote the universe.
To keep the introduction simple we abuse notation and let $x
\bmod m$ denote the unique representative of the equivalence
class $x +m\Z$ lying in $[0,m)$.
In most textbooks (e.g., \cite{motwani1995randomized}) $\LH$ is
defined placing $x\in \mathcal{U}$ in bin
\begin{equation}\label{eq:LHdefn1}
(ax+b)\mod p \mod \beta
\end{equation}
for prime $p\in [u, 2u]$
and randomly chosen integers $a,b \in [1,p]$.
In \cite{dietzfelbinger1997reliable} Dietzfelbinger et al. give an
alternative definition placing $x\in \mathcal{U}$ in bin
\begin{equation}\label{eq:LHdefn2}
\floor{ \frac{(ax+b)\mod p}{p/\beta} }.
\end{equation}
We refer to \cref{eq:LHdefn1} as \defn{strided hashing} and
\cref{eq:LHdefn2} as \defn{blocked hashing}.

For $\beta=n$ Knudsen \cite{knudsen_linear_2017} implicitly observed that 
the maxload of \cref{eq:LHdefn1} and \cref{eq:LHdefn2} differ by
at most a factor-of-$2$; this follows from our
\cref{prop:blockZisok}. Roughly this equivalence follows by observing that 
if blocked hashing has large maxload for $a=a_0,$ then strided
hashing will have large maxload for $a=a_0 n \mod p$. Similarly,
if strided hashing has large maxload for $a=a_1$ then blocked
hashing will have large maxload for $a=a_1 n^{-1}\mod p$, where
$n^{-1}$ is the multiplicative inverse of $n$ in $\F_p$.
Thus, classically \cref{eq:LHdefn1}, \cref{eq:LHdefn2} are
essentially equivalent. On the other hand we show that blocked hashing
generalizes more readily.\footnote{In \cref{prop:blockZsucks} we
show that strided hashing does not generalize cleanly to composite
moduli. Furthermore, there is no natural way to generalize
strided hashing to real numbers.} 
Thus, the majority of our results will concern blocked hashing.

We further simplify the hash function \cref{eq:LHdefn2} by
removing the $+b$ \defn{``shift term"} obtaining 
\begin{equation} \label{eq:LHdefn3}
  x\mapsto \floor{\frac{ax \bmod p}{p/\beta}}.
\end{equation}
Removing the shift term also will not impact the maxload by more
than a factor-of-$2$: changing the shift term at most splits
fullest bins in half or merges parts of adjacent bins
into a single new fullest bin.

For the rest of this section \defn{Simple $\LH$} will refer to
the hashing scheme defined in \cref{eq:LHdefn3}.
We propose \cref{q:isLHsubpoly} as a potential solution to
\cref{question:fastandgood}.
\begin{question}\label{q:isLHsubpoly}
  Is the worst-case expected maxload of Simple $\LH$ bounded by $n^{o(1)}$?
\end{question}

% It is well known that pairwise independence is not sufficient to
% get any bound on maxload better than $\bigO(\sqrt{n})$
% \cite{petershor}.
% \begin{itemize}
%   \item Select $k\gets [n],\pi\gets S_{n-1}$ uniformly randomly.
%   \item Each ball $i\in [n-1]$ is placed in bin $k$ with
%     probability  $1/\sqrt{n}$ and in bin $k+\pi_i$ otherwise.
%   \item Ball $n$ is placed independently in a random bin.
% \end{itemize}
%  Here, the expected number of balls in any fixed bin is
%  $\bigO(1)$ (this is guaranteed for any pairwise independent hash
%  family by linearity of expectation), but the expected number of
%  balls in the fullest bin is $\Omega(\sqrt{n})$.

\subsection{Our Results}
In this paper we propose a set of intermediate open questions to
help researchers make progress on \cref{q:isLHsubpoly}. 
We establish the relationship between these intermediate open
questions and make some partial progress on them.

\paragraph{The Two Bin Case}
We start in \cref{sec:twobins} by considering an even simpler question than
\cref{q:isLHsubpoly}:
\begin{question}\label{question:twobincase}
  What can be said about Simple $\LH$ in the case where there are only
  $\beta=2$ bins?
\end{question}

What makes \cref{question:twobincase} interesting is that to a
first approximation it is the simplest question that one can ask
about $\LH$ that is already non-trivial.
Furthermore, understanding the two bin case may be helpful for
resolving \cref{q:isLHsubpoly}. One potential approach is: 
\begin{reptheorem}{question:reductiontwobins}
Is it possible to obtain bounds on Simple $\LH$'s maxload with $n$ bins
by recursively using concentration bounds on its maxload for
$2$ bins?
\end{reptheorem}

As partial progress towards \cref{question:reductiontwobins} we
analyze the maxload of Simple $\LH$ for two bins. For a
pairwise-independent hash function (e.g., \cref{eq:LHdefn2}) it
is straightforward to bound the expected maxload by $n/2 +
\bigO(\sqrt{n})$. However, for \cref{question:reductiontwobins}
pairwise-independence is not the reason we expect $\LH$ to
achieve small maxload; in particular as noted earlier the maxload
of $\LH$ with and without the shift term differ by at most a
factor-of-$2$. This motivates the study of Simple $\LH$'s maxload
in the two bin case as initial progress towards
\cref{question:reductiontwobins}.
Bounding Simple $\LH$'s maxload is complicated by
the fact some pairs of elements have probability as large as
$2/3$ of mapping to the same bin under Simple $\LH$. Despite
this, by combinatorially analyzing the amount of correlation
between different elements we show:
\begin{reptheorem}{thm:dontneedb}
  Simple $\LH$ with $\beta=2$ bins has expected maxload at most 
  $$n/2 + \bigO(n^{7/8}).$$
\end{reptheorem}

For the rest of the paper we consider the standard case of $\beta=n$
bins.

\paragraph{Connecting Prime and Integer Moduli}
In \cref{sec:Z} we consider the importance of
using a prime modulus for $\LH$.
Conventional wisdom (e.g., \cite{cormen2022introduction}) is that
using a non-prime modulus is catastrophic. Using a
non-prime modulus is complicated by the fact that in a general
ring, as opposed to a finite field, non-zero elements can
multiply to zero.
Fortunately for any $m$ there is a reasonably large subset of
$\Z_m$ which forms a group under multiplication. The subset is
$\Z_m^{\times }$: the set of integers coprime to $m$.
Using using $\Z_m^{\times }$ we define an alternative version of
$\LH$ called \defn{Smart} $\LH$  and show:
\begin{reptheorem}{thm:LHSLH}
  Fix integer $m\in \poly(n)$. The expected maxloads of
  Smart $\LH$ with modulus $m$ and Simple $\LH$ with modulus $m$
  differ by at most a factor-of-$n^{o(1)}$.
\end{reptheorem}
Intuitively, Smart $\LH$ with composite modulus behaves somewhat
similarly to Simple $\LH$ with prime modulus.
This similarity allows us to, with several new ideas, translate Knudsen's proof
\cite{knudsen_linear_2017} of a $\widetilde{\bigO}(n^{1/3})$
bound on Simple $\LH$'s maxload for prime modulus to the
composite modulus setting, giving:
\begin{reptheorem}{thm:Zm1_3}
  The expected maxload of Smart $\LH$ is at most
  $\widetilde{\bigO}(n^{1/3})$.
\end{reptheorem}

Part of why \cref{thm:Zm1_3} is interesting is that using 
\cref{thm:Zm1_3} in \cref{thm:LHSLH} gives:
\begin{reptheorem}{cor:translate}
  The expected maxload of Simple $\LH$ with composite modulus is
  at most $n^{1/3+o(1)}$.
\end{reptheorem}
In particular, in \cref{cor:translate} we have translated the
state-of-the-art bound for maxload from the prime modulus setting
to the composite modulus setting.
This gives tentative evidence that the behavior of $\LH$ with
composite modulus may actually be the same as that of $\LH$ with
prime modulus. We leave this as an open question:
\begin{reptheorem}{question:equivalenceFZ}
  Is the worst-case maxload of composite modulus $\LH$ the
  same, up to a factor-of-$n^{o(1)}$, as that of prime
  modulus $\LH$?
\end{reptheorem}

\paragraph{Connecting Integer and Real Moduli}
Finally, in \cref{sec:R} we consider \defn{Real $\LH$} where the
multiplier ``$a$" in \eqref{eq:LHdefn3} is chosen
from $\R$. Initially the change to a continuous setting seems
to produce a very different problem. 
In this continuous setting one equivalent way of formulating
\cref{q:isLHsubpoly} is:
\begin{question}[``Crowded Runner Problem"]
\label{question:dual}
  Say we have $n$ runners with distinct speeds $x_1,x_2, \ldots,
  x_n \in (0,1)$ starting at the same location on a length $1$
  circular race-track. $a\in (0,1)$ is chosen randomly and all
  runners run from time $0$ until time $a$. 
  Is it true that on average the largest ``clump" of runners,
  i.e., set of runners in single interval of size $1/n$, is of
  size at most $n^{o(1)}$?
\end{question}
As formulated in \cref{question:dual} the problem becomes a
dual to the famous unsolved ``Lonely Runner Conjecture"
of Wills \cite{wills1967zwei}  and Cusick \cite{cusick1982view}
as formulated in \cite{bienia1998flows}. In the Lonely Runner
Conjecture the question is for each runner whether there is any
time such that the runner is ``lonely", i.e., separated from all
other runners by distance at least $1/n$. Our question is whether for
most time steps there is any runner that is ``crowded",
i.e., with many other runners within an interval of size $1/n$
around the runner.
The difficulty of the Lonely Runner Conjecture may be indicative
that the ``Crowded Runner Conjecture" is also quite difficult.

In \cref{thm:itisreal} we show a surprising equivalence
between $\LH$ for integer and real moduli. 
Technically our equivalence is to a slightly stronger
version of integer modulus $\LH$ namely \defn{Random Integer $\LH$} where the
modulus is not simply the universe size $u$, but rather a
randomly chosen (and likely composite) integer in $[u/2, u]$.
Random Integer $\LH$ is clearly at most a factor-of-$2$ worse
that Simple $\LH$, but it is not obvious whether it is any better; we leave this as
an open question:
\begin{question}
  Does Random $\LH$ achieve smaller worst-case
  expected maxload than Simple $\LH$?
\end{question}

Formally the equivalence between Real $\LH$ and Random Integer
$\LH$ is as follows:
\begin{reptheorem}{thm:itisreal}
  Let $f(n), g(n)$ be lower and upper bounds respectively on the
  expected maxload of Random Integer $\LH$ that hold for all sufficiently large
  universes. Let $M_\R$ denote the expected maxload of
  Real $\LH$. Then 
  $$\Omega\paren{\frac{f(n)}{\log\log n}} \le M_\R \le
  \bigO(g(n)).$$
\end{reptheorem}
The proof of this theorem involves several beautiful
number-theoretical lemmas and is one of our main technical
contributions.

This equivalence between the real and integer versions of $\LH$ shows
that \cref{q:isLHsubpoly} may not fundamentally be about prime
numbers or even integers.

% In \cref{sec:nice} we analyze a particularly structured set
% which a priori is a reasonable candidate for a set which might
% incur large maxload; note that the maxload of an average set is
% of course small. However, we show that this set actually incurs
% small maxload. 

\section{Preliminaries}
\paragraph{Set Definitions}
We write $\PRM$ to denote the set of primes, $\F_p$ for $p\in \PRM$ to denote the
finite field with $p$ elements, and $\Z_m$ for $m\in \N$ to denote
the ring $\Z / m\Z.$

For $a,b\in \R$ we define 
$$[a,b]=\setof{x\in \R}{a\le x\le b}.$$
For $n\in \N$ we define 
$$[n] = \set{0,1,\ldots, n-1},$$
$$[n]_{\setminus 0} = [n]\setminus \set{0}.$$
For $p\in \N,x\in \R$ we define $\modp(x)$ as
the unique number in the set 
$$(x+p\Z) \cap [0,p),$$
and define
$$\circabs_p(x)= \min(\modp(x), \modp(-x)).$$
For $x\in \Z$, $\modp(x)$ is the positive remainder
obtained when $x$ is divided by $p$ and $\circabs_p(x)$ is the
smallest distance to an element of $p\Z$ from $x$.

For $a\in \R$ and set $X\subset \R$ we define
$$a+X = \setof{a+x}{x\in X},$$
$$a\cdot X = \setof{a\cdot x}{x\in X},$$
$$\modp(X) = \setof{\modp(x)}{x\in X}.$$


\paragraph{Number Theoretic Definitions}
For $x,y\in \N$ we write $x\perp y$ to denote that  $x,y$ are
coprime, and $x\mid y$ to denote that $x$ divides $y$.
We write $\gcd(x,y)$ to denote the largest $k$ satisfying both  $k\mid
x$  and $k\mid y$.
% We use $\nu_p(n)$ to denote the exponent of the largest
% power of $p$ dividing $n$. 

A \defn{unit}, with respect to an implicit ring, is an element
with an inverse. For $m\in \N$ we define $\Z_m^{\times}$ as the
set of units in $\Z_m$. That is,
$$\Z_m^{\times } =\setof{k \in [m]}{k\perp m}.$$ We write
$\phi$ to denote the Euler-Toitent function, which is defined to
be $\phi(m)=|\Z_m^{\times }|$. We write $\tau(m)$ to denote the number of
divisors of $m$. 

The following two facts (see, e.g.,
\cite{hardy1979introduction}), will be
useful in several bounds:
\begin{fact}\label{fact:numdivs}
  $$\tau(n) \le 2^{\bigO(\log n / \log\log n)} \leq n^{o(1)}.$$
\end{fact}
\begin{fact}\label{fact:toitent}
  $$\frac{n}{2\log\log n} \le \phi(n)< n.$$
\end{fact}

\paragraph{Hashing Definitions}
A \defn{hashing scheme} mapping universe $[u]$ to $\beta$ bins is a set of functions 
$$\setof{h_i: [u]\to [\beta]}{i\in I}$$ 
parameterized by $i\in I$ for some set $I$.
We say $h_i$ sends element  $x$ to bin $h_i(x)$.
The \defn{maxload} of $\setof{h_i}{i\in I}$ with respect to set $X$ for
parameter choice $i_0\in I$ is 
$$\max_{k\in [\beta]} \left|\setof{x\in X}{h_{i_0}(x)=k}\right|.$$
In other words, maxload is the number of elements mapped to the
fullest bin.
We are concerned with bounding the expected maxload of hashing
schemes with respect to uniformly randomly
chosen parameter $i\in I$ for arbitrary $X$.
We will abbreviate uniformly randomly to randomly when the
uniformity is clear from context.

\begin{rmk}\label{rmk:assumesize}
  Our analysis is asymptotic as a function of $n$, the number of
  bins. We assume $n$ is at least a sufficiently large constant.
  We also require the universe size $u$ to satisfy:
  $$n^{6}\le u \le \poly(n)$$
  We adopt these restrictions for the following reasons:
  \begin{itemize}
    \item If $u$ is too small then bounding maxload is not
      interesting. For instance, if $u\in \Theta(n)$ linear
      hashing trivially achieves maxload $\bigO(1)$.
      It is standard to think of the universe as being much larger
      than the number of bins. Our specific choice $u\ge \Omega(n^{6})$ is
      arbitrary, but simplifies some analysis.
\item If $u$ is too large then constant-time arithmetic
  operations becomes an unreasonable assumption. Thus, we require
  ${u\le \poly(n)}$.
  \end{itemize}
\end{rmk}

\input{two_bins.tex}
\input{composite_moduli.tex}
\input{sec_real.tex}

\paragraph{Acknowledgements}
The author thanks William Kuszmaul and Martin Farach-Colton for
proposing this problem and for helpful discussions.
% \bibliographystyle{plain}
% \bibliography{refs}
\medskip
\printbibliography
\medskip
\appendix

\input{slh13knudsen.tex}
\input{structured_set.tex}

\end{document}
