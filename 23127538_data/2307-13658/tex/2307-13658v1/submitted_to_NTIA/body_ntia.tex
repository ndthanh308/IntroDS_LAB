\textit{This white paper is a response to the ``\href{https://www.federalregister.gov/documents/2023/04/13/2023-07776/ai-accountability-policy-request-for-comment}{AI Accountability Policy Request for Comments}'' by the National Telecommunications and Information Administration of the United States. The question numbers for which comments were requested are provided in superscripts at the end of key sentences answering the respective questions. The white paper offers a set of interconnected recommendations for an AI accountability policy.}

\section{Introduction}
As new AI agents and models are released and claims of their novel capabilities are made, it is crucial for governments, corporations, and individual customers to verify their capabilities, safety, alignment, and limitations. AI systems often align with the interests of their developers and vary in terms of their political, corporate, and national views. 
How can countries and their governments incentivize AI system designs that best fulfill the needs, interests, and safety of various stakeholders?

The central claim of this document is that addressing the tradeoffs between the risks and benefits of complex AI technologies requires the creation of a \emph{public registry of AI systems}, and a new federal office responsible for protecting and promoting public good through AI accountability mechanisms. 
The purpose of such AI accountability mechanisms is to instill long-term and broad international trust in techno-social systems by incentivizing trustworthy AI designs, through minimal viable regulation.\textsuperscript{1} AI accountability practices can make a significant difference even if legal standards and enforceable risk thresholds are introduced solely for the purpose of maintaining transparency among the AI system applications, rather than for gatekeeping.\textsuperscript{1e} An AI registry that certifies AI systems would motivate AI developers to compete in the area of AI accountability; similar mechanisms are used to enforce vehicle safety standards, which in turn encourage car manufacturers to offer better safety features.\textsuperscript{2} Thus, comparing AI systems on \emph{objective} and \emph{transparent} grounds will positively impact their design.

Federal regulation would help develop standards and incentives for accountable AI systems; however, we argue for lightweight regulation. Overregulation (e.g. mandatory licensing to develop AI technologies) would frustrate the development of trustworthy AI, since it would primarily inhibit smaller independent AI system manufacturers from participating in AI development.\textsuperscript{7} Independent AI system developers include academic researchers and open-source developers, who are major innovators in the space of accountable AI systems for public good.

This white paper builds upon the prior work~\cite{LearnedMiller2020FACIALRT} that calls for a new federal office to regulate facial recognition AI systems, and extends its ideas beyond the facial recognition sector. Our proposed remedies include:
\begin{enumerate}[(a)]
    \item A public registry of AI systems used in high-risk applications; 
    \item More diverse and realistic benchmarks for testing AI accountability within different deployment types;
    \item Automated audits and resulting standard certificates for AI systems to improve assurances that the technology is ready for deployment;
    \item Techniques that explain AI systems and enable transparent reasoning about their fairness and alignment;
    \item Social media accountability mechanisms;
    \item A legislation and federal office to address AI accountability.
\end{enumerate}
Addressing the ``AI ecosystem'' as a whole --- from developers, to users, to regulators --- is necessary to elucidate the tradeoffs and underlying forces within these technologies. 
While these components could be created and managed via other means, such as a self-regulated industry consortium, business priorities do not always align with the public interest. 
As such, a federal office is an appropriate mechanism. 
This proposal offers preliminary directions towards a comprehensive evaluation process, and fleshes out the details of an AI system regulatory framework. 
We do not presume to offer a precise structuring of such an office within the federal government; 
we do, however, discuss the necessary elements to establish sorely needed controls for AI systems.

\section{Motivation}
Let us list the existing barriers and opportunities for effective AI accountability. These barriers can be addressed via federal regulation that will develop standards and incentives for accountable AI systems.\textsuperscript{25}

\subsection{Towards Open-World Benchmarks}
To mitigate issues of bias and representation in the performance of AI systems, calls to establish more comprehensive technical standards and benchmarks have been raised by researchers, policy makers, and industry leaders. 

The primary issue with relying on benchmarks to inform the use of AI systems is that they can only indicate how AI systems work in conditions that reflect the benchmark data. 
Contemporary benchmarks are often narrow in scope. 
A related issue concerns adaptation to benchmarks: once a benchmark is publicly available, developers quickly tailor their systems to maximize their performance on the benchmark. 
This practice may show apparent improvements without making sizable gains on previously unseen test data, which is facilitated by the fact that the same people develop benchmark datasets and AI systems. 
Realistic benchmarks should have open-world properties; in particular, they should include unexpected inputs. 
That is, systems must be tested not just for their accuracy, but for their ability to identify and adapt to novel data. 
A safe and effective system should report that it is unable to make a decision when insufficient information is provided, or is unable to effectively process the information, rather than simply providing its best guess. 
This aspect of safety assurance is severely underdeveloped. 
Open-world benchmarks addressing this issue have been recently explored in DARPA’s SAIL-ON program, which introduced a ``great firewall'' between the evaluated AI systems and the entities that evaluate them. 
The great firewall separates AI system manufacturers and auditors, which enables testing AI systems on input data that was previously unobserved by the AI systems and their developers. 
These open-world benchmarks control the amount of information that AI system developers have about the datasets and environments their AI systems are evaluated in.  
The SAIL-ON evaluations of multiple AI agents in multiple environments and datasets show that contemporary AI is far from achieving robust, flexible, and adaptive performance on a human level. 
Significantly more research focused on robustness is needed to achieve such capabilities in the AI space. 

Another issue is that gathering the volume and variety of data needed to robustly evaluate a system can be challenging when the process requires consent. 
Datasets may be proprietary, and require consent to manage. An AI accountability policy shall envision mechanisms for auditing AI systems on datasets having different affordances, e.g., internal audits conducted by AI system manufacturers and external audits conducted by a federal authority.\textsuperscript{22}

The other challenge is that AI researchers and manufacturers do not necessarily agree on measures of AI effectiveness and safety; this makes comparisons between them challenging.\textsuperscript{10} 
Such measurements can themselves evolve over time to reflect the state-of-the-art on AI safety, explainability, robustness, and fairness, as new measures are developed. 
We advocate for standardization of measures and benchmarks to enable AI comparisons and transparency, rather than fixating on particular measures and benchmarks.

Overall, we argue that the benchmarks are as close to real-world deployment scenarios as possible, including their unpredictable ``messiness'', and evolve over time, e.g., are updated every year. 
Contemporary focus on narrow benchmarks and the dearth of standard open-world learning benchmarks (such as the ones appearing in SAIL-ON) are a major barrier to accountable AI systems.\textsuperscript{29} 
There is a need for nudging AI research and manufacturers towards the development of more robust, explainable, and accountable AI systems that can safely operate in open-world and lifelong learning settings. 
Overall, significant effort and investment are needed in the development of open-world benchmarks to address these issues. 

\subsection{The Need for Transparency}
As AI systems grow in complexity, it is increasingly difficult to reason about \emph{why} they make certain decisions. Complex predictive algorithms make increasingly high-stakes decisions; however, in order to foster trust in these methods, their underlying decision-making processes need to be better understood. This need gives rise to a rich literature on \emph{explaining} AI systems.

\subsubsection{AI Transparency}
Apart from developing appropriate benchmarks, the results of the benchmarks need to be clearly and transparently communicated, explained, and compared across competing  AI systems. Recent events illustrate that there is insufficient transparency and explanation to affected people about the uses, capabilities, and limitations of AI systems:\textsuperscript{3e}
\begin{itemize}
    \item Some self-driving systems are designed to turn themselves off seconds before a crash under certain circumstances, allegedly embellishing the statistics of miles driven between crashes. Meanwhile, marketing videos claimed that drivers in these self-driving cars are at the wheel only for legal reasons, implying self-sufficiency. Overall, self-driving systems may be marketed as safer than they are in practice. This case is officially investigated by the U.S. Department of Transportation \cite{nhsta2022tesla}.
    \item OpenAI released a report suggesting that GPT4 has relatively high IQ, passes a lot of college-level tests, and solves coding problems. Later it was shown that GPT4 does not achieve such good results on hold-out tests that were outside of the AI training dataset. This suggests that high performance to some extent stems from training data contamination; however, this fact received far less attention than the initial report. LLMs are often presented as means to obtain a professional competitive advantage; this recently led an attorney to use an LLM in an unintended way, risking the attorney's career~\cite{brodkin2023lawyer-a}.
\end{itemize}
The importance of AI system comparisons is particularly evident in the employment sector. 
Explanations and comparisons of employment practices play a central role in the \emph{burden shifting framework} used in courts to determine employment discrimination. 
In the framework, the plaintiff and the employer respond to each other in turns. 
The process is started by the plaintiff pointing out disparate impact, i.e. the association between outcomes and group identity (for example, the employee alleges that they are being denied a promotion due to their race or gender). 
Then, the employer can provide an explanation (``business necessity'') for this association. 
As a response, the plaintiff can point to an ``alternative employment practice'' that alleviates the disparate impact while achieving the business necessity. 
In this context, explanations can be vague; identifying alternative employment practices is hardly ever possible, since typically plaintiffs and legal clerks lack information about any alternatives. 
To address this issue and integrate AI systems with the burden shifting framework, AI accountability mechanisms must enable comparisons among AI systems.

Establishing a public AI registry allowing comparisons among different AI systems alleviates some of these issues. In addition, requiring marketing approvals for AI systems similarly to the products regulated by the Food and Drugs Administration (FDA), such as medications and medical devices, supports a more transparent understanding of how these systems operate. 

\subsubsection{Social Media Transparency}
\label{sec:social-media-transparency}

Social Media Platforms (SMPs), such as Twitter and Facebook, have come under significant criticism for amplification of polarization and misinformation~\cite{bail2021breaking}. 
While polarization and misinformation are not solely the domain of SMPs, lack of social media transparency may contribute and reinforce these harmful effects. 

First, social media does not sufficiently inform its users about the provenance and authenticity of content spreading on its platforms, even if that content has political relevance. 
For instance, deep fakes generated by AI systems can spread and AI bots can respond to political polls on social media. 
While SMPs do not have ground truth information about the provenance of content and user responses, such platforms widely use AI systems to identify provenance of content. 
For instance, Twitter from 2016 to 2023 identified the inauthentic votes in Twitter polls that were purchased on external websites, but displayed them publicly as authentic votes~\cite{tvp2020info} (we were able to reproduce these results). Inauthentic content can take other forms than, e.g., deep fakes were recently used for political disinformation in India~\cite{shrutimenon2023wrestlers}. Unfortunately, the public is not typically informed about identified inauthenticity and is instead broadly exposed to inauthentic signals disguised as authentic ones. 
We argue that the information about provenance of political content that can be used for spreading disinformation is crucial to democratic processes. 

Second, biases are common in SMPs. 
Statistics computed on biased samples of the overall population can amplify polarization and misinformation. For instance, our study shows that thousands of social polls gauging support for the 2016 and 2020 U.S. presidential candidates were heavily biased and likely manipulated in favor of one candidate, Donald Trump. We show that these biases result in amplification of the voices of privileged groups, i.e., old males. Such partisan and demographic biases among social polls have potentially serious consequences, including voter fraud beliefs.

% 
Third, SMPs lack financial incentives to address polarization and misinformation because these concerning phenomena may have a positive effect on engagement and therefore on short-term earnings. For instance, before the 2020 U.S. presidential elections, SMPs purposefully reduced the virility amplification in the AI systems controling news feeds of their users, which resulted in less polarizing and misleading content spreading on social media, but returned to that amplification right after the presidential elections~\cite{reutersstaff2020twitter, roose2020facebook}. 

% Fourth, SMPs affect public health. They are effective means for the formation of harmful communities. These communities are often hard to detect. For example, recent studies show that Instagram is a primary venue for the dissemination of pedophilia~\cite{xyz}. 
% Teens suffering from eating disorders use SMPs to support one another (e.g. sharing strategies for minimizing calorie intake, or hiding their behavior from family and friends). 
% SMPs are either slow to respond to such issues, or ignore them outright.
 
% solutions
To avoid disinformation and misinformation, we call for more transparency about biases and inauthenticity of politically-relevant social media content.


\subsection{The Need for Comparing Explanations}
It is difficult to identify a single statistical notion that appropriately evaluates AI fairness, even for specific sectors and narrow well-defined domains.\textsuperscript{10} 
Consider the employment sector. There exists well-established employment legislation since 1964; the federal office that oversees its execution, the Equal Employment Opportunity Commision (EEOC), was established in 1965. The EEOC uses a statistical notion of fairness measuring the association between hiring outcomes and protected group identities via the so-called ``80\% rule''. That rule, however, is solely a rule of thumb. It admits exceptions (e.g., the supreme court case Ricci v. DeStefano~\cite{2009ricci-a}), and courts instead rely on the burden shifting framework, where the explanations, justifications, and comparisons of employment practices play a central role. 

The insufficiency of statistical measures of the association between outcomes and protected group identities is apparent also when we look across sectors. While in hiring the association of group identity and outcomes suggests unfairness, the same association is perceived differently in the context of health needs across ethnic groups; people of color tend to have worse health status even if we compare individuals having the same healthcare spending, as shown in a recent important work~\cite{obermeyer2019dissecting}. 
The study suggests that admissions to special care programs should be granted to the patients that need them the most, which are more likely to be people of color. 
In this case, the association between outcomes and protected group identities may be judged as fair and justified.

Finally, if we overly focus on the statistical association between outcomes and protected group identities, there will be tradeoffs among different goals of AI accountability, known as ``fairness gerrymandering''. 
This refers to instances where statistical fairness with respect to certain protected groups results in statistical unfairness with respect to others, e.g., fairness with respect to gender groups may result in unfairness with respect to ethnic groups.\textsuperscript{3}

We propose AI accountability mechanisms based on explanations of decision-making processes; since explanations are automatically generated and highlight the true underlying model decision process, they do not lead to the above tradeoffs. Instead, publicly available explanations facilitate their collective interpretation and understanding.

\subsection{From Fragmented Legislation to Global Alignment}
We argue that fragmented legislation is insufficient, following the white paper suggesting the FDA-like approach to facial recognition technology regulation~\cite{LearnedMiller2020FACIALRT}. 
The passage of city, statewide, and country bans and moratoriums on the use of specific AI systems, such as facial recognition technology or large language models, show urgent public concern about privacy, consent, discrimination, and surveillance. 
However, the temporary and local nature of many bans introduces complexity and inefficiency. 
What happens when the bans expire? 
Does the expertise needed to evaluate them need to be reassessed when these laws are reconsidered? 
Temporary bans at different levels of government buy time to consider further implications of the technology, but are not a long term solution. 
Most importantly, lack of unified legislation may hinder innovation, as companies must adapt to diverse and changing legislation. 
The challenges and opportunities introduced in this section require oversight mechanisms and guidance for industry practice, research norms, and procurement procedures. 
The global landscape of these technologies requires thinking through not just how to mitigate risk at city, state, and local levels but also federally and internationally. 
A federal office where these important issues can be considered together would be more effective than current ad-hoc responses. 
In total, the needs for standards, transparency, and incentives for accountable AI discussed here motivate the establishment of a federal office to manage these complexities.\textsuperscript{30c} 
The recent Brooking Institute report reaches a similar conclusion, and argues for cooperation and alignment of the US and the EU’s AI legislation~\cite{engler2023euus}, which we discuss next.

\section{Models for AI Accountability}
While fragmented legislation is insufficient, existing legislation from different sectors provides valuable models and lessons. 
We propose the establishment of a new federal office to oversee AI systems by examining how other complex technologies have been successfully regulated at the federal level. 
Specifically, we draw parallels to the following accountability structures and industries: the existing regulation for the medical device industry and the pharmaceutical industry (regulated by the FDA), the proposed legislation for AI in the European Union (the AI Act), and the existing US anti-discrimination legislation. 

\subsection{The Food and Drug Administration}
A white paper by researchers of facial recognition technology calls for a new federal authority to regulate facial recognition technology following the model of the Food and Drugs Administration (FDA)~\cite{LearnedMiller2020FACIALRT}. 
The authors argue that ``The US Food and Drug Administration, in collaboration with industry, lawmakers, and the professional medical community, has developed an extensive set of definitions, procedures, policies, conventions, laws, and regulations that have successfully managed tradeoffs among the many parties involved in this domain''. 
They carefully examine the risks and implications of facial recognition technologies, and assert that similar structures are required to regulate it. 
Via analogies with the FDA regulatory structures, they propose specific methodologies for managing the risk- benefit trade-offs of AI technology. 
This includes definitions to simplify and clarify key concepts, the classification of facial recognition technology applications into risk categories, the adoption of scoping concepts like ``intended use'', and recommendations about appropriate gatekeepers for different parts of the process. 
Finally, they address issues around deployment restrictions, research and development, consent and privacy, user training, and mandatory error reporting. 

% Figure environment removed

A key component of the FDA regulation is \emph{intended use}, which lays the groundwork for assessing the risk of a medication or a device. 
The FDA established three classes of medical devices based upon their inherent levels of risk. 
The primary purpose of this classification is to tailor regulation to risks. 
The FDA's risk management for each class of devices incrementally provides additional regulation for higher levels to account for the greater risk (Figure \ref{fig:fda-ai}). 
A similar model can be applied to AI, subjecting high-risk AI applications --- e.g. criminal recidivism prediction --- to greater scrutiny than low-risk applications --- e.g. recreational enhancement of personal photos. The degree of scrutiny should be commensurate with potential \emph{harms}. 
Hence, we recommend that risk levels are assigned to AI systems using a similar process to that of medical devices. 
The differences between the medical industry and the AI industry are reflected in the differences in the specific regulation assigned to each risk level. 
For instance, AI applications that pose minimal risk should not be regulated, to avoid stifling innovation and decrease regulatory burdens.

The white paper \cite{LearnedMiller2020FACIALRT} points out that when new medications are developed, they cannot be marketed or sold in the United States until they have been assessed by the FDA. 
This intricate process varies for over the counter vs. prescription medication. 
Generally, a study of the specific drug must be carried out, with the results being reported to the FDA for scrutiny. 
One such procedure is called a ``New Drug Application''. 
Companies are required to pay fees to facilitate this process, which not only aid in financing the necessary analysis but also act as a deterrent for the submission of risky or unnecessary applications that are likely to face rejection.

While the FDA serves as a model for AI regulation, there are lessons to be learned from its operations and opportunities for improvements.\textsuperscript{11} 
The white paper \cite{LearnedMiller2020FACIALRT} points out that medications, just like AI systems, can exhibit different levels of effectiveness on different population subgroups; however, these differences have not been captured and regulated by the FDA. 
The paper suggests evaluating AI technologies during the research and deployment phases on randomly drawn sets from the same overall population. 
% To avoid such issues, benchmarks need to be as close to the real-world deployment types as possible or correct for biases in data samples.
To avoid subgroup bias, benchmarks must include representative samples from all relevant samples, and tested for potential biases and harms. 

The European Union's AI Act takes a similar approach to AI regulation as the FDA model, and is based on intended use and associated risk classes. 
The act was proposed in 2021 and is actively being developed. 
The AI system risk categories envisioned by this proposed legislation are depicted in Figure \ref{fig:ai-act}. 
The main difference between the AI Act and existing FDA administration is that the Act aims to determine the mapping between deployment types and risk classes a priori, whereas the FDA administration first establishes intended use of an AI system based on manufacturer’s input and determines risk class based on this information. 
We recommend following the FDA model: new applications may arise over time and all deployment types require regular evaluation of risks. 
%fig 3
% Figure environment removed

\subsection{Anti-Discrimination Regulations in the United States}
Some accountability processes can be built upon federal anti-discrimination legislation and practices.\textsuperscript{13}

First, the EEOC gathers workforce data from employers with more than 100 employees that includes breakdowns by protected groups such as race and gender via the Bias in Hiring Reports (EEO-1)\cite{eeodata}. 
While this data is confidential, it could possibly be used to incentivize exemplary hiring practices and outcomes.\textsuperscript{11} 
We envision a public AI registry with model cards and voluntary AI audits granting sought-after certificates as a way to take advantage of such opportunity. 
The public AI registry would create positive incentives for the development of responsible AI without burdening AI system manufacturers with complex legal requirements, while preserving confidentiality about their current workforce.

Second, anti-discrimination laws in the employment sector, determined by Title VII of the Civil Rights Act, introduce fairness requirements and standards, culminating in the burden shifting framework. 
The burden shifting framework was recently introduced in the housing sector; 
we anticipate it to be introduced in other sectors as well~\cite{2019hud-a}. 
We argue that AI systems in the employment, housing, and possibly other sectors, shall obey these sector-specific requirements.\textsuperscript{6} 
The burden shifting framework, especially its business necessity component, relies on explanations and justifications.

In prior work \cite{grabowicz2022marrying}, we propose using AI explainability measures (such as feature highlighting methods) to reason about and train non-discriminatory AI systems. 
Our work introduces a formal technique for fair and explainable automated decision making that is closely tied to existing anti-discrimination legislation through AI explanations.\textsuperscript{10} 
The technique inhibits direct discrimination in historical training datasets and enables a restricted use of business necessity attributes that correlate with protected attributes, by preventing their use as proxies for the protected attributes. 
It achieves this by taking a real-world, possibly discriminatory, decision-making process or AI system (Figure \ref{fig:ii}A) and turning it into a non-discriminatory decision-making AI model (Figure \ref{fig:ii}C), without inducing indirect discrimination through proxies (Figure \ref{fig:ii}B). 
For instance, in the context of redlining, some financial institutions may use zip code as a way to estimate wealth and probability of paying back the loan if no other financial information is available. 
To identify that a bank is practicing unjustified redlining, a federal office for AI accountability could compare the influence of zip code on loan application outcomes across multiple such institutions. 
If some banks rely on zip code much more than others, this may correspond to unjustified redlining and these banks can be asked to update their AI systems. 
Overall, our approach ties together concepts of fairness, transparency, and explainability of AI systems.\textsuperscript{6} 
%fig 1 here, probably reorder numbers
% Figure environment removed

We recommend making model explanations a core component of the information stored in the AI registry to enable the comparisons of AI systems in terms of their explanations, as in Figure \ref{fig:ii}, for a particular type of consequential decision-making. 
These accountability mechanisms, combined with sufficient domain expertise would help in determining whether a given AI model discriminates against protected groups.\textsuperscript{3a} 
In situations where comparisons of input influences suggest that an AI system could be improved in terms of its fairness, an external audit could be mandated to verify the suspicion and gather further information. For instance, to verify that the AI system discriminates via proxies, the external auditors would need to observe how the model was trained and whether it learned to use correlated features as proxies for the missing protected attributes, as we point out in our prior research~\cite{grabowicz2022marrying}. 

Some researchers call for creating effective explainable AI models~\cite{rudin2019}. However, approaches based on model-agnostic explanations are less restrictive, as they do not constrain AI system architecture. 
This difference is crucial, because it enables unconstrained innovation and development of effective AI systems, as well as accurate models of human decisions. 
There exist multiple model-agnostic explanation frameworks, and new ones are under development. 
Choosing the appropriate mode of explanation requires careful consideration. In particular, explanation methods should satisfy provable concepts of fairness, privacy and robustness. 
While these measures are a useful tool for those concepts in AI systems, they can be adversarially manipulated~\cite{slack2020fooling} or provide lower quality explanations to protected subgroups~\cite{balagopalan2022road}. 
Paralleling prior authors’ recommendations, we call for these drawbacks to be considered at the time when standardized benchmarks and corresponding audits are designed and developed.

% \subsection{Social Science One}
%This part is relevant solely to social media accountability. To be described… 

\section{AI Accountability Policies}
AI accountability mechanisms should cover the following broad topics:\textsuperscript{1a}
\begin{enumerate}[(a)]
    \item High-risk AI applications, e.g., consequential automated decision-making;
    \item Social media platforms (SMPs).
\end{enumerate}
AI systems deployed in SMPs include recommendation and news feed algorithms. 
While such applications are not high risk at the individual level, we argue that they are high-risk at the collective level and, hence, require accountability mechanisms. 
The collective effect of SMPs is evident in political discourse. 
For instance, algorithms deployed on social media amplify the spread of political information~\cite{huszar2022algorithmic} and influence voting turnout in political elections~\cite{bond201261millionperson}. 
While SMPs have collective-level effects, AI systems create risks at the individual level. 
A consequential AI system can severely impact individuals, e.g. by misclassifying them or using their private information in an unintended manner. 
While SMPs may cause significant harm to individuals, the probability of such harm is very low at the level of any particular individual.
% and the harm is noticeable only at the collective level if a large number of users is subjected to the respective risk.
%they suggest content in a manner that does not often result in significant harm. 
However, the aggregate effect of SMPs on millions of users --- e.g., negative affects on their mood~\cite{kramer2014experimental}, swaying their views on climate change \cite{treen2020climate}, or spreading conspiracy theories \cite{ahmed2020conspiracy} --- may cause a significant collective harm. SMPs also facilitate significant real-world harms. A recent study shows that Instagram is being used to support pedophile networks \cite{horvitz2023meta}; worse still, the report alleges that Meta was able to identify problematic content but did nothing to remove it until the information was public. Indeed, children are susceptible to sexual exploitation online through the use of SMPs \cite{levine2022sexualabuse}. 
These incidents highlight the need for SMP regulation. 

Finally, we note that AI systems are often ``one-shot'' interactions: a user submits an input (say, a text prompt) and receives an output (a response from a large language model). SMPs usually offer far more complex interactions  driven by techno-social feedback loops. Thus, AI systems and SMPs require different accountability policies and mechanisms.

\subsection{AI Accountability Mechanisms}
\subsubsection{Overview}
We propose that at the center of AI accountability mechanisms is a federal AI registry. The registry would store information about each market-ready trained AI system that is used in high-risk decision-making, e.g., Equivant’s COMPAS or GPT4. The registration of AI systems should be obligatory only for high-risk and limited-risk systems that may impact consequential decision making. To preserve innovation and reduce AI accountability costs, we propose minimal AI accountability policies and AI registry obligations. We suggest not to regulate minimal-risk systems in any way, while requiring registration and intended use specification for limited-risk AI systems to ensure that they are not high risk. In other words, minimal-risk AI systems should not be registered at all, as is the case for nearly all AI systems at current. 

The purpose of the AI registry is to store information about all AI systems used in high-risk applications in the form of AI cards (model cards), inform the public about their characteristics and their alternatives, and to stimulate competition among AI system manufacturers in the space of accountable AI.\textsuperscript{1} The AI registry stores three kinds of standardized audits (we use the words audit and assessment synonymously), ordered in terms of their sophistication:\textsuperscript{8}
\begin{itemize}
    \item Internal AI audits – internal audits performed by AI manufacturers;
    \item Automatized AI audits – audits that result in AI certificates, performed automatically by the AI registry in cooperation with AI manufacturer;
    \item External AI audits – external audits that are involuntary, conducted by the AI accountability agency only if there is sufficient evidence of wrongdoing.
\end{itemize}
Internal AI audits are meant as a mass-scale solution to auditing and are most useful as obligatory audits providing basic structured input to the AI cards of each registered AI system.\textsuperscript{1b} The structure and measures of such audits should be standardized and determined by the AI accountability agency, so that audit results are comparable among all AI systems for a given deployment type. Both internal and automated AI audits test only the deployment stage of the AI lifecycle. This reduces the burden, costs, and complexity of auditing, making it feasible to scale the audits to a large number of AI systems.

The primary purpose of automated AI audits is to incentivize AI system manufacturers in the space of AI certificates, by providing a method to assure compliance with non-binding trustworthy AI goals.\textsuperscript{1b,1c} The secondary purpose is to verify the outcomes of an internal AI audit, a claim, or compliance with legal standards.\textsuperscript{1c} 

External AI audits, which refer to traditional comprehensive audits, would be conducted only in warranted exceptional cases.\textsuperscript{1b} The warrant may depend on evidence of wrongdoing or evidence-based suspicion of wrongdoing based on the outcomes of the internal and automated AI audits. 

The introduced AI accountability mechanisms will explain decisions supported by foundational generative models, e.g., ChatGPT or GPT4. Namely, large language models (LLMs) can be used to solve text classification problems and consequential decision-making typically can be represented as text classification problems. Then, the outcomes of such text classifiers can be explained with input influence measures. Hence, the AI registry could provide information about a wide range of diverse models, including well known general-purpose models such as OpenAI’s GPT4 and Google’s Bard, at different consequential decision-making tasks, such as hate speech detection or automatic essay grading. In this way, the AI registry could become the public registry of influential AI models.

The registry, audits, and certificates are valuable for promoting trust and improving internal processes, but their main purpose is to encourage developers to design AI systems that engender trust in the long-term, by providing incentives for developing accountable AI with minimal regulation.\textsuperscript{2}

\subsubsection{Sectors}
The introduced mechanisms can be applied across multiple sectors, such as:\textsuperscript{1b}
\begin{itemize}
    \item Biometric recognition technologies, e.g., facial recognition technologies~\cite{LearnedMiller2020FACIALRT};
    \item Hiring automation, e.g., activity of companies such as HireVue;
    \item Student support systems, e.g., activity of software such as EAD’s Navigate;
    \item Criminal system automation, e.g., activities of software such as Equivant’s COMPAS;
    \item Healthcare, e.g., health care needs predictions~\cite{obermeyer2019dissecting}; 
\end{itemize}

The structure of AI accountability mechanisms varies across sectors. For instance, AI card structure should depend both on the sector and the particular deployment (decision-making process) type. Furthermore, AI accountability mechanisms provide an opportunity to reinforce existing sector-specific accountability mechanisms.\textsuperscript{1d} For instance, the EEOC collects every year the bias in hiring EEO-1 reports~\cite{eeodata}, but these reports are not used for any public purpose, because of privacy and legal obstacles. We point that the AI accountability policy could address these challenges and use the AI registry to promote exemplary automated decision-making practices.

\subsubsection{Intended Use and Risk Levels}
AI systems represent too many applications for a single set of rules. Just as different FDA restrictions are applied to different medications, AI controls should be tailored to the application. This requires mechanisms for carefully defining the scope of applications. Intended use is central to the AI accountability framework. For instance, a facial recognition system might be approved and used for ``identifying customers in retail stores''. We consider this to be a deployment type that describes the general type of application. 

Risk level classification should depend on the intended use, that is the deployment type, i.e., the specific kind of a decision-making process. The given AI system should be approved for marketing only once it is registered or determined to have intended uses that fall within the minima-risk category. Then, the advertised intended use shall match registered intended use, to avoid misinformation.

Which decision-making processes shall be considered consequential or high risk?

\begin{itemize}
    \item The ones that are already protected by existing laws, e.g., hiring and housing decisions.
    \item For other decision-making processes that are consequential, but not protected by existing laws yet, the AI accountability policy could introduce a threshold for the number of people affected by the considered AI model.
\end{itemize}
Generative AI such as LLMs that are intended as having a broad usage and could possibly be used in consequential decision-making, would belong to the limited-risk or high-risk application category, depending on their intended use and would need registration and approval for marketing. We recommend that all limited-risk and high-risk AI systems are required to be identified to the people directly affected by them, e.g., by providing their AI registry card links or identifiers.\textsuperscript{5}

The same AI system can be deployed many times. After being approved for such a deployment type, it is important to register every individual deployment, such as the deployment to a specific retail store, for appropriateness. On every deployment there is the question of whether a piece of software is being used in accordance with its intended use (a valid deployment) or in violation of its intended use (an invalid deployment).

\subsubsection{AI Registry}
The AI registry would be the main interface between the public and the agency collecting information about AI systems used in consequential decision-making. The registry would gather and share information about AI systems, and possibly list their intended uses, to encourage good AI designs, rather than to penalize bad designs. It would be online and publicly available. The registry should be shared across sectors, to make it easier for end users to recognize, comprehend, and trust the platform.\textsuperscript{30a} The brand and reputation of the registry will likely determine the successfulness of the AI accountability policy.

The AI registry would communicate to the public the outcomes of internal audits (AI cards) and automated audits (certificates). While high-level structural components of the platform should be shared across sectors, there should be differences between sectors and deployment types at a lower level, e.g., the AI card should have different feature ontology for each deployment type. Hence, each deployment type would have a different AI card content and a different set of associated certificates. To decrease the cognitive burden to the end users of the AI registry, performance results could be shown in relation to the outcomes of other AI systems for the given deployment type, possibly as a percentile score, e.g., top 5\% accuracy in student success prediction.

\subsubsection{AI Cards}
Every AI system used in consequential decision-making shall have an AI card in the AI registry. AI cards would state results of an internal audit, which could include both model performance and input influence measures. An example of AI cards are Google’s "model cards"~\cite{googlemodelcard,googlemodelcardpub}. The content of AI cards should be readable and entirely structured in an iterative process that includes industry.\textsuperscript{33} That structure should evolve over time, to reflect the developments in the industry and research, e.g., of novel input influence measures. AI cards should include all input features of an AI model and be flexible enough to accommodate new features that are not a part of a feature ontology. The registry should provide a feature ontology that is specific to the deployment type to reflect the specificity of each decision-making problem. The structure of AI cards should be easy to use and understand by both humans and machines.

While the feature ontology is highly structured, the input to the AI systems can be provided in an unstructured way. For instance, imagine that an AI is evaluating a student CV. That CV could be mapped into a tabular, highly structured, format with semantically meaningful fields, such as race, gender, SAT score. A classifier could be trained on this highly-structured data, but if a LLM is our classifier, then it would not explicitly make use of the tabular representation of the data, but rather use the entire CV as input. However, to compute input influence measures, such as the impact of race on AI system’s decisions, the auditors will still need to extract structured information from CVs and manipulate that structured information to measure how such changes, e.g., changing race and associated information on the CV, impacts LLM's decisions. Such manipulations of inputs to observe AI system outputs correspond to randomized experiments performed by researchers to understand decision-making processes, e.g., the seminal callback studies change the first name on CVs to see whether callback rates from employers will be affected by racial soundingness of the name~\cite{bertrand2003are}.

\subsubsection{Automated Audits and Certificates}
Automatized audits can be used to generate AI certificates for any AI system in the AI registry. They could be performed online as ``QA sessions'' between a given AI model and the federal auditing API after prior authorization. 
Queries would be posed by the AI auditing agency, answered by the AI system. 
The audited AI model should specify what features it expects on input. 
The audit API will initiate the testing session by providing (synthetic or real-world) data on input and asking the AI model to output decisions or answers for each sample. 
These audits have a server-client architecture and characteristics of both internal and external audits, since the auditing agency provides a server, while the agent’s developer provides a client. 
They treat AI models as black-boxes and only require access to their inputs and outputs. 

We recommend that automated audits follow the SAIL-ON model of AI evaluations: the AI system manufacturers are informed about the goals and structure of an audit, but not its content. 
In particular, the AI system manufacturers should not know the (test) dataset that they are evaluated on. 
Such a dataset could contain novel samples to test an AI system’s resilience, adaptation, and confidence when faced with unexpected real-world inputs.

AI audits can be voluntary and be used to certify AI systems. 
Each deployment type should have a different set of associated certificates; for instance, a certificate that a foundational LLM offers 93\% accurate scientific references, or that a hate speech classifier is not discriminating against race and gender, while achieving top 12\% disparate impact score in comparison to other AI systems having the same intended use. 
Finally, certificates and the aspects they test will evolve over time in order to account for ever-changing industry, research, practice, and the overall AI ecosystem.

\subsubsection{External Audits}
External audits are generally more costly and comprehensive than internal and automated audits, since they test all stages of the AI lifecycle. Such audits should be infrequent, to reduce the overall costs of AI accountability policy. 
The rules determining occurrence of external audits shall be sector-specific. 
These external audits would be greatly facilitated by a policy requiring that manufacturers of AI models used in high-risk scenarios keep records from the model's lifetime so that all steps leading to the creation of the model are reproducible.\textsuperscript{20,21,30b} 
For instance, the manufacturers of such AI system may be required to maintain the source code, data, parameters, and random seeds used for training the respective AI model. 
Outcomes of external audits would be kept confidential, with only summaries published in the AI registry.\textsuperscript{1c}

\subsubsection{Explanations and Recourse for AI Decisions}
Finally, the legislation could require AI systems to provide explanations and recourse options to each user and each consequential decision. Similar requirements are suggested in Recital 71~\cite{gdprrecital71} of the European Union’s General Data Protection Regulation (GDPR)~\cite{EuropeanParliament2016a}. Recital 71 states that automated decision-making systems should include ``specific information to the data subject and the right to obtain human intervention, to express his or her point of view, to obtain an explanation of the decision reached after such assessment and to challenge the decision''. 
This Recital, however, is non-binding and provides no technical insight on what type of explanation method should be provided. 
In response, \citet{wachter2017} propose that explanations should provide users with 
\begin{inparaenum}[(1)]
\item understanding on why a particular decision was reached;
\item grounds to contest undesired decisions, and 
\item what could be changed to receive a desired result, i.e., recourse.
\end{inparaenum}
To amend the criticism of the GDPR, future legislation could follow these proposed explanation requirements.

The explanations we propose for the AI registry satisfy these requirements by providing users with specific details on how each of their features, or characteristics, influenced the outcome they received. 
Additionally, the \citeauthor{wachter2017} introduce counterfactual explanations. These methods explain  ``how the world would have to be different for a desirable outcome to occur'', as a means to satisfy these requirements without needing to expose the internal logic of automated decision-making systems. 
For instance, if a loan applicant was denied a loan, a counterfactual explanation will answer 
\begin{inparaenum}[(1)]
    \item what type of applicant was accepted and 
    \item what actions are required to resemble the accepted applicant.
\end{inparaenum} 
% Similar to our prior work on input influence, counterfactual explanations can be utilized to reveal discrimination in decision-making systems. If the users with undesirable outcomes from one protected subgroup requires more effort to achieve their desired outcome over another, then decisions are potentially influenced by features with a relationship to users’ subgroup status, thus requiring an audit.

\subsection{Social Media Accountability}

% inauthentic user behaviour
Social media can have a consequential societal effect. 
To limit the nefarious societal effects of social media, we propose that large SMPs proactively inform their users about the provenance of misinformation or problematic content, and user responses to such content. 
For instance, SMPs could report to users that a large fraction of votes in a political poll is coming from questionable accounts.\textsuperscript{3b} 
As we pointed out in Section~\ref{sec:social-media-transparency}, social media uses AI systems to identify provenance and authenticity of content, but in some cases (such as political polls, or child sexual exploitation) does not notify the public and does not correct the affected content. 
% deep fakes
Similar recommendations are developed in the European Union, which is leaning on signatories to its Code of Practice on Online Disinformation to label deepfakes and other AI-generated content~\cite{euCodePractice2022}.\textsuperscript{3b} 
We recommend that the outcomes of AI systems identifying provenance are communicated to the public and that these AI systems are themselves registered in the AI registry.\textsuperscript{4}

% biases
% Apart from that, misinformation and polarization can be exacerbated by social media design in an unintentional way. Biases in social media can cause misinformation and amplify the voices of privileged groups.

To identify responsible social media designs that curb misinformation and polarization, researchers and auditors need access to aggregated social media data that includes demographic and partisan breakdowns.\textsuperscript{4}
Such data elucidates how changes to AI systems controlling user news feeds affect the overall population and its democratic processes.
Importantly, this data can be released in a privacy-preserving manner using differentially private mechanisms \cite{dwork2014privacybook}. 
Data sharing agreements of this kind are currently offered and supported by Meta for Facebook data and mediated via Social Science One. 
We recommend that such aggregate and privacy-preserving datasets are shared by other SMPs as well and made more accessible, e.g., such privacy-preserving data could be used on systems that do not belong to the respective~SMP. 

% For instance, before the 2020 U.S. presidential elections, SMPs purposefully reduced the virility amplification in their news feed algorithm, which resulted in less polarizing and misleading content spreading on social media, but returned to that amplification right after the presidential elections~\cite{reutersstaff2020twitter, roose2020facebook}

 
% Crowdsourcing content moderation via panels that are demographically and politically representative of the overall population of the country.
% Social media. The following AI accountability mechanisms could be introduced:\textsuperscript{4}
% \begin{itemize}
%     \item Social media “lenses” – Provide access to temporal, spatial, political, and demographic breakdowns of user activity, in particular for political polls published on social media.
%     \item Academic A/B testing tools – Enabling researchers to run public A/B testing experiments
% \end{itemize}

\subsection{Organization}

It is important that there be uniformity of AI accountability standards and practices, especially for a given deployment type. This will enable oversight over deployed AI systems thanks to the possibility of making comparisons between them.\textsuperscript{34} 
This rationale extends to global jurisdictions as well. 
The creation of the Federal office and the AI registry would help the United States become a leader in the space of global AI accountability and control. 

To achieve an efficient and scalable AI registry and reduce its overall cost, it is important to maintain, to the extent possible, similar structures across sectors and deployment types. 
The government itself shall mandate stringent accountability practices for the AI systems it uses.\textsuperscript{30d}
There are various costs involved in the AI accountability, which should be covered by different stakeholders. We distinguish between the following kinds of costs:\textsuperscript{28} 
\begin{enumerate}[(1)]
    \item Operating the AI registry and audit API --- costs covered by the federal administration;
    \item Operating the evaluated AI systems during automated AI audits --- covered by AI system developers;
    \item Mechanisms for social media transparency --- cost covered by SMPs.
    % \item Social media experiments – researchers;
\end{enumerate}

To ensure a strong AI accountability ecosystem, the federal office will need to fund sufficient domain experts for each sector and for each deployment type to develop, maintain, and learn from AI cards.\textsuperscript{31} 
In addition, it will need to fund researchers advising the design and updates to automated audits and explainability measures used in the AI registry. 
To indirectly support these operations, the government could amplify the allocation of financial resources towards research endeavors in the areas of:
\begin{enumerate}[(1)]
    \item Explainable AI methods;
    \item Open-world benchmarks, where there is the ``great firewall'' between model developers and model auditors, such as evaluations in DARPA’s SAIL-ON program;
    \item Collaborative development of ontologies, since the structure of AI cards should be sector-specific and developed collaboratively.
    % \item Social media accountability.
\end{enumerate}



\section{Conclusion}
We propose an approach to regulate AI systems that is compatible with existing U.S. anti-discrimination regulation and inspired by the FDA and the AI Act. 
At a high level, we argue that the expanding influence, intricacy, and evolution of AI systems necessitates
\begin{inparaenum}[(1)]
    \item extensive supervision, which a federal authority office can offer, and 
    \item dedicated expertise, not only of the underlying technologies, but also of the risks they pose across various application fields.
\end{inparaenum}
We believe that this approach is necessary to keep up with the rapid evolution of AI research and industry and ensure long-term societal benefits of this developing technology.
While this white paper is focused broadly on AI and social media systems, more in-depth FDA-inspired recommendations about documentation, categorization, and evaluation of facial recognition systems are detailed in the prior work that this white paper builds upon~\cite{LearnedMiller2020FACIALRT}. 

% There would be many challenges in implementing these recommendations. To begin, like with the FDA’s regulation of the medical industries, there will always be a tension between considering those with financial incentives for producing the technologies and those who benefit from oversight and regulation. 

% Segments of the “tech” industry have opposed regulation within the US and abroad, citing concerns about hindering growth, innovation, and beneficial applications. Nevertheless, there is growing pressure for the regulation of technology companies, and even some evidence that the companies themselves would like some regulatory guidance, if for no other reason than to ensure a level playing field among the firms.

