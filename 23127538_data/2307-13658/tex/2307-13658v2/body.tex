
%\textit{This white paper is a response to the ``\href{https://www.federalregister.gov/documents/2023/04/13/2023-07776/ai-accountability-policy-request-for-comment}{AI Accountability Policy Request for Comments}'' by the National Telecommunications and Information Administration of the United States. The question numbers for which comments were requested are provided in superscripts at the end of key sentences answering the respective questions. The white paper offers a set of interconnected recommendations for an AI accountability policy.}

\section{Introduction}
As new AI agents and models are released and claims of their novel capabilities are made, it is crucial for governments, corporations, and individual customers to verify their capabilities, safety, alignment, and limitations. AI systems often align with the interests of their creators and maintainers and vary in terms of their political, corporate, and national perspectives and biases. 
How can countries and their governments incentivize AI system designs that best fulfill the needs, interests, and safety of various stakeholders?

This questions is not a purely theoretical question. The European Union (EU) enacted in 2024 the AI Act, which envisions development of an accountability policy for AI systems based on conformity assessment and audits of AI systems. However, details of the implementation of such conformity assessment are not determined yet. The European Commission (EC) over the next years plans to develop a series of delegated and implementing acts detailing the policy in communication with AI experts and the EU public.\footnote{\scriptsize{https://artificialintelligenceact.eu/implementation-timeline/}} Furthermore, other administrations are exploring similar accountability policies. For instance, in 2023, the U.S. administration requested public comments on the AI Accountability Policy to be submitted to National Telecommunications and Information Administration.

The fundamental claim of this proposal is that addressing the tradeoffs between the risks and benefits of complex AI technologies requires the creation of a \emph{public registry of AI systems}, and a new office responsible for protecting and promoting public good through AI accountability mechanisms.
This proposal mirrors the approach adopted by the EU and their AI Act\footnote{\scriptsize\url{https://eur-lex.europa.eu/eli/reg/2024/1689/oj/eng}}, which establishes an EU-wide AI database (see Article 71) and an AI Office\footnote{\scriptsize\url{https://digital-strategy.ec.europa.eu/en/policies/ai-office}}. As the functional specifications of the EU AI database have yet to be drawn up, this paper's proposals may not just be relevant but timely also.  


The purpose of AI accountability mechanisms is to instill long-term and broad international trust in techno-social systems by incentivizing trustworthy AI designs, through minimal viable regulation.\ntia{1}  
AI accountability practices can make a significant difference even if legal standards and enforceable risk thresholds are introduced solely for the purpose of maintaining transparency among the AI system applications, rather than for gatekeeping that prevents some AI providers from participating in the market.\ntia{1e} An AI registry that certifies AI systems would motivate AI developers to compete in the area of AI accountability; similar mechanisms are used to enforce vehicle safety standards, which in turn encourage car manufacturers to offer better safety features.\ntia{2} Thus, comparing AI systems on \emph{objective} and \emph{transparent} grounds will positively impact their design. 

Regulation would help develop standards and incentives for accountable AI systems. Moreover, the EU AI Act calls for such (harmonized) standards (see Article 40) to help regulated entities comply with the requirements set out within the Act. For example, ISO/IEC TR 24027:2021, outlines a set of appropriate methods for the assessment of bias in AI assisted decision making processes.\footnote{\scriptsize\url{https://www.iso.org/standard/77607.html}} 

Further to this, we caution against overregulation (e.g. mandatory licensing to develop AI technologies) as it could frustrate the development of trustworthy AI, since it would primarily inhibit smaller independent AI system manufacturers from participating in AI development. Independent AI system developers include academic researchers and open-source developers, who are major innovators in the space of accountable AI systems for public good.
The EU AI Act was written with such independent AI system manufacturers in mind. Recital 146 recommends that the EC develop guidelines to help such manufacturers by specifying the elements of the required (and potentially burdensome) quality management system that can be fulfilled in a simplified manner. This manuscript proposes automated and lightweight accountability mechanisms for high-risk intended uses of AI systems.


The two key contributions of our proposal to the literature and existing legislation are: 
\begin{itemize}
    \item Conformity assessment through computation of standardized measures enabling comparisons across AI systems within a given intended use, which would be reported publicly via the AI database.
    \item Automation of conformity assessment via internal and automated audits, executed by AI system providers, but defined and standardized by the AI Office in conjunction with other regulatory/standardization bodies.
\end{itemize}

Lightweight internal and automated audits can compute and report standardized measures through so-called \textit{AI cards} and \textit{AI certificates} to enable comparisons of AI systems with a given intended use. 
% Another key contribution of this paper is the proposal relating to automated audits and certificates. 
% Within the EU AI Act Article 27 "Fundamental rights impact assessment for high-risk AI systems" (5) it states: "The AI Office shall develop a template for a questionnaire, including through an automated tool, to facilitate deployers in complying with their obligations under this Article in a simplified manner." 
Article 27 (5) of the AI Act states: "The AI Office shall develop a template for a questionnaire, including through an automated tool, to facilitate deployers in complying with their obligations under this Article in a simplified manner." 
This proposals could support notified bodies tasked with issuing certificates under the AI Act (see Article 44) as well as audit reports where necessary and it could also help generate the "exit reports" within the AI regulatory sandboxes (see Article 57) that support the completion of the conformity assessment requirement for sandbox participants. 
% Moreover, according to Article 58 (2d) access to AI regulatory sandboxes is to be free of charge for SMEs and start-ups to reduce their AI Act regulatory burden.
Our proposal could be of relevance to both certificate-issuing EU notified bodies and the competent authorities tasked with running the AI regulatory sandboxes.
As such, this proposal is well aligned with the AI Act and provides suggestions for implementation of AI Act's conformity assessment and the EU's AI database. 


% This white paper builds upon the prior work that calls for a new federal office to regulate facial recognition AI systems~\cite{LearnedMiller2020FACIALRT}, and extends its ideas beyond the facial recognition sector. 

To summarize, our proposed remedies include:
\begin{enumerate}[(a)]
    \item A public database and publicly-accessible registry of AI systems used in %high-risk
    commercial applications, grouping AI systems by their intended use to enable their meaningful comparisons; 
    \item Introduction of standardized measures for evaluation of AI systems, enabling transparent reasoning about their fairness and alignment. In particular, we advocate for more diverse and realistic benchmarks for testing AI accountability within different deployment types and for the use of AI exaplainability techniques;
    \item Automated audits and standard certificates for AI systems as means for conformity assessment;
    %\item Social media accountability mechanisms;
    \item A %legislation and
    regulatory office to address AI accountability.
\end{enumerate}
Addressing the ``AI ecosystem'' as a whole --- from developers, to users, to regulators --- is necessary to elucidate the tradeoffs and underlying forces within these technologies. 
While these components could be created and managed via other means, such as a self-regulated industry consortium, business priorities do not always align with the public interest. 
As such, a regulatory office is an appropriate mechanism. 
This proposal offers preliminary directions towards a comprehensive evaluation process, and fleshes out the details of an AI system regulatory framework that is consistent with other regulatory frameworks such as the EU AI Act. 
We do not presume to offer a precise structuring of such an office within a government; 
we do, however, discuss the necessary elements to establish sorely needed controls for AI systems.
Our proposal is initially aimed to be deployed at a national or EU's level. However, we are hopeful that success at the national level will lead to adoption and cooperation at the international level.



\section{Motivation and Background}
% We begin by listing the existing barriers and opportunities for effective AI accountability. These barriers can be addressed via regulation that will develop standards and incentives for accountable AI systems.

% \subsection{The Need for Transparency}
% intro to explainability
% As AI systems grow in complexity, it is increasingly difficult to reason about \emph{why} they make certain decisions. Complex predictive algorithms make increasingly high-stakes decisions; however, in order to foster trust in these methods, their underlying decision-making processes need to be better understood. This need gives rise to a rich literature on \emph{explaining} AI systems.

%\subsubsection{AI Transparency}
% Apart from developing appropriate benchmarks, the results of the benchmarks need to be clearly and transparently communicated, explained, and compared across competing  AI systems. 
Recent events illustrate that there is insufficient transparency and explanation to affected people about the uses, capabilities, and limitations of AI systems.\ntia{3e} We illustrate them here using examples, and describe them in detail in the following subsections, respectively:
\begin{itemize}

    \item (\S\ref{sec:measures-vs-use}) 
    There are claims that some self-driving cars drive larger distances without crash than humans. These claims are based on measurements, but these measurements do not take into account that humans drive in all conditions, whereas AI systems in near-perfect conditions or turn itself off before a crash \cite{nhsta2022tesla}, possibly embellishing the statistics of miles driven between crashes. To account for such difference, there is a need for a greater transparency.
    % Some self-driving systems are designed to turn themselves off seconds before a crash under certain circumstances, allegedly embellishing the statistics of miles driven between crashes. Meanwhile, marketing videos claimed that drivers in these self-driving cars are at the wheel only for legal reasons, implying self-sufficiency. Overall, self-driving systems may be marketed as safer than they are in practice. This practice is officially investigated by the U.S. Department of Transportation \cite{nhsta2022tesla}.
    
    \item (\S\ref{sec:measure-xai})
    Due to suspicions of discrimination in algorithmic hiring, e.g., in AI systems of Amazon and HireVue, the systems were disabled by the AI providers. Reasoning about discrimination may require comparisons of explainable AI measures.
    % where U.S. courts make explanation based decisions via the burden shifting framework.
    %The importance of AI system comparisons is particularly evident in the employment sector.
    
    \item (\S\ref{sec:unseen-data}) OpenAI released a report suggesting that GPT4 has relatively high IQ, passes a lot of college-level tests, and solves coding problems. Later it was shown that GPT4 does not achieve such good results on hold-out tests that were outside of the AI training dataset~\cite{narayanan2024ai}. 
    % This suggests that high performance to some extent stems from training data contamination; however, this fact received far less attention than the initial report. %LLMs are often presented as means to obtain a professional competitive advantage; this recently led an attorney to use an LLM in an unintended way, risking the attorney's career~\cite{brodkin2023lawyer-a}.
\end{itemize}


Establishing a public AI registry allowing comparisons among different AI systems would alleviate some of these issues. 
%[TODO: risk also includes \# of people. SM is an example]
We propose that all commercially available AI systems posing risk to the society be \emph{registered} in AI registry, providing information about their intended use, deployment sector, as well as relevant standardized measures characterizing the AI system at hand, such as statistical fairness metrics (\S\ref{sec:statistical-fairness}), custom measures for the given use (\S\ref{sec:measures-vs-use}), explainability measures (\S\ref{sec:measure-xai}), or performance measures (\S\ref{sec:unseen-data}).
% obligations
% For example, high-risk applications like self-driving cars on public roads need to be heavily regulated regardless of scale, medium-risk applications like facial recognition verification systems would meet heavier obligations as they become more widespread, and low-risk applications like social media platforms would only become regulated when a significant portion of the population uses them.

% \subsection{New Liability Directives Incentivize AI System Transparency}

Furthermore, the EC has published two proposed directives which would alleviate the burden of proof on EU citizens about faultiness of an AI system: namely the adopted Product Liability Directive\footnote{\scriptsize\url{https://www.europarl.europa.eu/legislative-train/theme-a-europe-fit-for-the-digital-age/file-new-product-liability-directive}} and the tabled AI Liability Directive\footnote{\scriptsize\url{https://www.europarl.europa.eu/legislative-train/theme-a-europe-fit-for-the-digital-age/file-ai-liability-directive}}. 
% These directives are intended to work with the EU AI Act, which includes Article 86 "Right to explanation of individual decision-making". 
% This article states that "Any affected person subject to a decision which is taken by the deployer on the basis of the output from a high-risk AI system listed in Annex III, with the exception of systems listed under point 2 thereof, and which produces legal effects or similarly significantly affects that person in a way that they consider to have an adverse impact on their health, safety or fundamental rights shall have the right to obtain from the deployer clear and meaningful explanations of the role of the AI system in the decision-making procedure and the main elements of the decision taken." 
These directives shift the burden of proof towards AI providers. In this way, AI providers will be incentivized to compete in the space of civil liability and safety assurance, which our proposal reinforces. The directives will incentivize AI providers to seek transparency sufficient to prove that their AI systems are conformant and competitive with other products having the same intended use (including possibly non-AI software systems) in terms of upholding civil rights such as non-discrimination.

\subsection{Fairness and Accountability through Transparency}
\label{sec:statistical-fairness}

% intro
To mitigate issues of bias and representation in the performance of AI systems, calls to establish more comprehensive technical standards and benchmarks have been raised by researchers, policy makers, and industry leaders. 
Recital 74 in the AI Act calls upon the EC to encourage the development of benchmarks and measurement methodologies for AI systems and as such, the EC should work with international partners in pursuit of these benchmarks.  

% from statistical measures of fairness to explainability
% It is difficult to identify a single statistical notion that appropriately evaluates AI fairness, even for specific sectors and narrow well-defined domains.\ntia{10} 
In the employment sector, there exists well-established anti-discrimination legislation. In the U.S., it exists since 1964; the federal office that oversees its execution, the Equal Employment Opportunity Commission (EEOC), was established in 1965. The EEOC uses a statistical notion of fairness measuring the association between hiring outcomes and protected group identities via the so-called ``80\% rule.'' That rule, however, is solely a rule of thumb. It admits exceptions (e.g., the supreme court case Ricci v. DeStefano~\cite{2009ricci-a}), and U.S. courts instead rely on the burden shifting framework, where the explanations, justifications, comparisons of employment practices, as well as burden shifting, play a central role. 


In the so-called burden shifting framework, the plaintiff and the employer respond to each other in turns. 
The process is started by the plaintiff pointing out disparate impact, i.e. the association between outcomes and group identity (for example, the employee alleges that they are being denied a promotion due to their race or gender). 
Then, the employer can provide an explanation (``business necessity'') for this association. 
As a response, the plaintiff can point to an ``alternative employment practice'' that alleviates the disparate impact while achieving the business necessity. 
In this context, explanations can be vague; identifying alternative employment practices is hardly ever possible, since typically plaintiffs and legal clerks lack information about any alternatives. 
To address this issue and integrate AI systems with the burden shifting framework, AI accountability mechanisms must enable comparisons among AI systems. 
We propose that measures that are important for a given high-risk intended use of AI systems are reported in the AI registry.

\subsubsection{Transparency Measures for a Given Intended Use}
\label{sec:measures-vs-use}
Standardized measures need to be considered separately for each intended use of AI systems. We provide a couple of examples illustrating this point.

% The insufficiency of statistical measures of the association between outcomes and protected group identities is apparent also when we look across sectors. 
While in hiring the association of group identity and outcomes suggests unfairness, the same association is perceived differently in the context of health needs across ethnic groups; people of color tend to have worse health status even if we compare individuals having the same healthcare spending, as shown in a recent important work~\cite{obermeyer2019dissecting}. 
The study suggests that admissions to special care programs should be granted to the patients that need them the most, which are more likely to be people of color. 
In this case, the association between outcomes and protected group identities may be judged as fair and justified, opposite to the case discussed in the preceding section.

For other intended uses of AI systems, there may be a need for entirely different measures than statistical fairness measures. For instance, for self-driving AI systems installed in cars, customers may be interested in miles driven without a crash in conditions that are representative of typical human driving conditions. To achieve such meaningful comparisons, there is a need for specification and  standardization of measures used for each category of intended high-risk use of AI systems.
%

\subsubsection{Explainability Measures}
\label{sec:measure-xai}
If we overly focus on the statistical association between outcomes and protected group identities, there will be tradeoffs among different goals of AI accountability, known as ``fairness gerrymandering.'' 
This refers to instances where statistical fairness with respect to certain protected groups results in statistical unfairness with respect to others, e.g., fairness with respect to gender groups may result in unfairness with respect to ethnic groups.\ntia{3}
For such cases, prior works proposed to use AI explainability measures~\cite{nicholasperello2023learning}. Such explanations are automatically generated and adress such tradeoffs. Publicly available explanations for various AI systems would facilitate their collective interpretation and understanding.

\subsubsection{Model Evaluation on Data Never Seen Before}
\label{sec:unseen-data}

The primary issue with relying on benchmarks to inform the use of AI systems is that they can only indicate how AI systems work in conditions that reflect the benchmark data. 
Contemporary benchmarks are often narrow in scope. 
A related issue concerns adaptation to benchmarks: once a benchmark is publicly available, developers quickly tailor their systems to maximize their performance on the benchmark. 
This practice may show apparent improvements without making sizable gains on previously unseen test data, which is facilitated by the fact that the same people develop benchmark datasets and AI systems. 
Realistic benchmarks should have open-world properties; in particular, they should include unexpected inputs. 
That is, systems must be tested not just for their accuracy, but for their ability to identify and adapt to novel data. 
A safe and effective system should report that it is unable to make a decision when insufficient information is provided, or is unable to effectively process the information, rather than simply providing its best guess. 
This aspect of safety assurance is severely underdeveloped. 
Open-world benchmarks addressing this issue have been recently explored in DARPA’s SAIL-ON program, which introduced a gape between the evaluated AI systems and the entities that evaluate them. 
The gap separated AI system manufacturers and auditors, which enables testing AI systems on input data that was previously unobserved by the AI systems and their developers. 
These open-world benchmarks control the amount of information that AI system developers have about the datasets and environments their AI systems are evaluated in.  
The SAIL-ON evaluations of multiple AI agents in multiple environments and datasets show that contemporary AI is far from achieving robust, flexible, and adaptive performance on a human level. 
Significantly more research focused on robustness is needed to achieve such capabilities in the AI space. 

Another issue is that gathering the volume and variety of data needed to robustly evaluate a system can be challenging when the process requires consent. 
Datasets may be proprietary, and require consent to manage. An AI accountability policy shall envision mechanisms for auditing AI systems on datasets having different affordances, e.g., internal audits conducted by AI system manufacturers and external audits conducted by a regulatory authority.\ntia{22}

The other challenge is that AI researchers and manufacturers do not necessarily agree on measures of AI effectiveness and safety; this makes comparisons between them challenging.\ntia{10}
Such measurements can themselves evolve over time to reflect the state-of-the-art on AI safety, explainability, robustness, and fairness, as new measures are developed. 
We advocate for standardization of measures and benchmarks to enable AI comparisons and transparency, rather than fixating on particular measures and benchmarks.

Overall, we argue that the benchmarks used in the AI registry should be as close to real-world deployment scenarios as possible, including their unpredictable ``messiness,'' and evolve over time, e.g., are updated every year. 
Contemporary focus on narrow benchmarks and the dearth of standard open-world learning benchmarks (such as the ones appearing in SAIL-ON) are a major barrier to accountable AI systems.\ntia{29} 
There is a need for nudging AI research and manufacturers towards the development of more robust, explainable, and accountable AI systems that can safely operate in open-world and lifelong learning settings. 
Overall, significant effort and investment are needed in the development of open-world benchmarks to address these issues. 


% \subsection{Just-in-Time Marketing of AI Systems}
% \label{sec:marketing}
% Requiring marketing approvals for AI systems similarly to the products regulated by the Food and Drugs Administration (FDA), such as medications and medical devices, supports a more transparent understanding of how these systems operate. 



\subsection{From Fragmented Legislation to Global Alignment}
\label{sec:global-alignment}

We argue that fragmented legislation is insufficient, following ~\citet{LearnedMiller2020FACIALRT} who suggested a FDA-like approach to facial recognition technology regulation. 
The passage of local, state, and national bans and moratoria on the use of AI systems in applications such as facial recognition in crime prevention or large language models in legal filings, show urgent public concern about discrimination, as well as privacy, consent, and surveillance. 
However, the temporary and local nature of many bans introduces complexity and inefficiency, raising questions like ``What happens when the bans expire?'' and
``Does the expertise needed to evaluate them need to be reassessed when these laws are reconsidered?'' 
Temporary bans at different levels of government buy time to consider further implications of the technology, but are not a long term solution.
Most importantly, lack of unified legislation may hinder innovation, as companies must adapt to diverse and changing legislation. 
The challenges and opportunities introduced in this section require oversight mechanisms and guidance for industry practice, research norms, and procurement procedures. 
The global landscape of these technologies requires thinking through not just how to mitigate risk at local, state, and national levels but also federally and internationally. 
A national office where these important issues can be considered together would be more effective than current ad-hoc responses. 
In total, the needs for standards, transparency, and incentives for accountable AI discussed here motivate the establishment of a national office to manage these complexities.\ntia{30c} 
The recent Brooking Institute report reaches a similar conclusion, and argues for cooperation and alignment of the US and the EU’s AI legislation~\cite{engler2023euus}, which we discuss next.

\section{Models for AI Accountability}
While fragmented legislation is insufficient, existing legislation from different sectors provides valuable models and lessons. 
We propose the establishment of a new regulatory office to oversee AI systems by examining how other complex technologies have been successfully regulated at government level. 
Specifically, we draw parallels to the following accountability structures and industries: the existing regulation for the medical device industry and the pharmaceutical industry (regulated by the FDA), the proposed legislation for AI in the EU (the AI Act), and the existing US anti-discrimination legislation. 
% legal movements
There are significant legal developments towards creating accountable AI ecosystems across the world. In addition to the AI Act, Canada started its journey towards creating legislation in October 2016 with the drafting of the white paper that was involving numerous subject matter experts from academia, civil society, and government\cite{ResponsibleUseOfAI}. Deliberations are now in place for passing Bill C-27\cite{BillC27} that focus on the 'Principles for responsible, trustworthy and privacy-protective generative AI technologies.'

\subsection{The Food and Drug Administration of the United States}
A white paper by researchers of facial recognition technology calls for a new federal authority to regulate facial recognition technology following the model of the Food and Drugs Administration (FDA)~\cite{LearnedMiller2020FACIALRT}. 
% The authors argue that ``The US Food and Drug Administration, in collaboration with industry, lawmakers, and the professional medical community, has developed an extensive set of definitions, procedures, policies, conventions, laws, and regulations that have successfully managed tradeoffs among the many parties involved in this domain.'' 
% They carefully examine the risks and implications of facial recognition technologies, and assert that similar structures are required to regulate it. 
Via analogies with the FDA regulatory structures, the authors propose specific methodologies for managing the risk-benefit tradeoffs of AI technology. 
This includes definitions to simplify and clarify key concepts, the classification of facial recognition technology applications into risk categories, the adoption of scoping concepts like ``intended use,'' and recommendations about appropriate gatekeepers for different parts of the process. 
Finally, they address issues around deployment restrictions, research and development, consent and privacy, user training, and mandatory error reporting. 

% Figure environment removed

A key component of the FDA regulation is \emph{intended use}, which lays the groundwork for assessing the risk of a medication or a device. 
The FDA established three classes of medical devices based upon their inherent levels of risk. 
The primary purpose of this classification is to tailor regulation to risks. 
The FDA's risk management for each class of devices incrementally provides additional regulation for higher levels to account for the greater risk (Figure \ref{fig:fda-ai}). 
A similar model can be applied to AI, subjecting high-risk AI applications --- e.g. criminal recidivism prediction --- to greater scrutiny than low-risk applications --- e.g. recreational enhancement of personal photos. The degree of scrutiny should be commensurate with potential \emph{harms}. 
Hence, we recommend that risk levels are assigned to AI systems using a similar process to that of medical devices. 
The differences between the medical industry and the AI industry are reflected in the differences in the specific regulation assigned to each risk level. 
For instance, AI applications that pose minimal risk should not be regulated beyond registration, to avoid stifling innovation and decrease regulatory burdens.

% The white paper \cite{LearnedMiller2020FACIALRT} points out that when new medications are developed, they cannot be marketed or sold in the United States until they have been assessed by the FDA. 
% This intricate process varies for over the counter vs. prescription medication. 
% Generally, a study of the specific drug must be carried out, with the results being reported to the FDA for scrutiny. 
% One such procedure is called a ``New Drug Application.'' 
% Companies are required to pay fees to facilitate this process, which not only aid in financing the necessary analysis but also act as a deterrent for the submission of risky or unnecessary applications that are likely to face rejection.

While the FDA serves as a model for AI regulation, there are lessons to be learned from its operations and opportunities for improvements.\ntia{11} 
The white paper \cite{LearnedMiller2020FACIALRT} points out that medications, just like AI systems, can exhibit different levels of effectiveness on different population subgroups; however, these differences have not been captured and regulated by the FDA. 
The paper suggests evaluating AI technologies during the research and deployment phases on randomly drawn sets from the same overall population. 
% To avoid such issues, benchmarks need to be as close to the real-world deployment types as possible or correct for biases in data samples.
To avoid subgroup bias, benchmarks must include representative samples from all relevant samples, and tested for potential biases and harms. 

The EU's AI Act takes a similar approach to AI regulation as the FDA model, and is based on intended use and associated risk classes. 
% The act was proposed in 2021 and is actively being developed. 
The AI system risk categories envisioned by this proposed legislation are depicted in Figure \ref{fig:ai-act}. 
The main difference between the AI Act and existing FDA administration is that the Act aims to determine the mapping between deployment types and risk classes a priori, whereas the FDA administration first establishes intended use of an AI system based on manufacturer’s input and determines risk class based on this information. Our proposal focuses instead on standardization of measures and automation of conformity assessment as a way to enable meaningful comparisons between AI systems.
% We recommend following the FDA model: new applications may arise over time and all deployment types require regular evaluation of risks. 
%fig 3

% Figure environment removed

\subsection{Anti-Discrimination Regulations in the United States}
Some accountability processes can be built upon federal anti-discrimination legislation and practices.\ntia{13}

First, the EEOC gathers workforce data from employers with more than 100 employees that includes breakdowns by protected groups such as race and gender via the Bias in Hiring Reports (EEO-1) \cite{eeodata}. 
While this data is confidential, it could possibly be used to incentivize exemplary hiring practices and outcomes.\ntia{11} 

By way of another example, see the New York City Local Law 144 on Automated Employment Decision Tools\footnote{\scriptsize\url{https://rules.cityofnewyork.us/wp-content/uploads/2023/04/DCWP-NOA-for-Use-of-Automated-Employment-Decisionmaking-Tools-2.pdf}} for a law that established a mandatory third party annual bias audit that must be published on the deployer's website and updated every 12 months. This bias audit involves deriving and displaying statistical fairness measures such as impact ratios.
% This bias audit involves deriving and displaying impact ratios which can either be via a selection rate or a scoring rate as follows: (1) the selection rate for a category divided by the selection rate of the most selected category or (2) the scoring rate for a category divided by the scoring rate for the highest scoring category.

While these two pieces of legislation are aligned with our proposal for AI accountability, they are markedly limited in scope. The EEOC gathers measures that could be included in AI registry for human decision-making, but does not publish them. The New York City Local Law requires publication of similar measures, but does not facilitate their comparisons across AI systems and it has a local scope. 
We envision a public AI registry with model cards and voluntary AI audits granting sought-after certificates as a way to take advantage of the opportunity to incentivize the development of responsible AI systems without burdening AI providers with complex legal requirements.
% , while preserving confidentiality about their current workforce.

Second, anti-discrimination laws in the employment sector, determined by Title VII of the Civil Rights Act, introduce fairness requirements and standards, culminating in the burden shifting framework. 
The burden shifting framework was recently introduced in the housing sector; 
we anticipate it to be introduced in other sectors as well~\cite{2019hud-a}. 
We argue that AI systems in the employment, housing, and possibly other sectors, shall obey these sector-specific requirements.\ntia{6} 
The burden shifting framework, especially its business necessity component, relies on explanations and justifications.

Prior work %In prior work 
\cite{grabowicz2022marrying} %, we 
proposes using AI explainability measures (such as feature highlighting methods) to reason about and train non-discriminatory AI systems. 
% Our work introduces a formal technique for fair and explainable automated decision making that is closely tied to existing anti-discrimination legislation through AI explanations.\ntia{10} 
% The technique inhibits direct discrimination in historical training datasets and enables a restricted use of business necessity attributes that correlate with protected attributes, by preventing their use as proxies for the protected attributes. 
% It achieves this by taking a real-world, possibly discriminatory, decision-making process or AI system (Figure \ref{fig:ii}A) and turning it into a non-discriminatory decision-making AI model (Figure \ref{fig:ii}C), without inducing indirect discrimination through proxies (Figure \ref{fig:ii}B). 
% For instance, in the context of redlining, some financial institutions may use zip code as a way to estimate wealth and probability of paying back the loan if no other financial information is available. 
To identify that a bank is practicing unjustified redlining, a regulatory office for AI accountability could compare the influence of zip code on loan application outcomes across multiple such institutions. 
If some banks rely on zip code much more than others, this may correspond to unjustified redlining and these banks can be asked to update their AI systems. 
Overall, this approach ties together concepts of fairness, transparency, and explainability of AI systems.\ntia{6} 

%fig 1 here, probably reorder numbers
% Figure environment removed

%[TODO: ]

For relevant intended uses, such as hiring automation, we recommend to store such AI system explanations in the AI registry to enable more meaningful comparisons of AI systems, as in Figure \ref{fig:ii}. 
These accountability mechanisms, combined with sufficient domain expertise would help in determining whether a given AI model discriminates against protected groups~\cite{grabowicz2022marrying}.\ntia{3a} 
In situations where comparisons of input influences suggest that an AI system could be improved in terms of its fairness, an external audit could be mandated to verify the suspicion and gather further information. For instance, to verify that the AI system discriminates via proxies, the external auditors would need to observe how the model was trained and whether it learned to use correlated features as proxies for the missing protected attributes, as prior research points out~\cite{grabowicz2022marrying}. 

Some researchers call for creating effective explainable AI models~\cite{rudin2019}. However, approaches based on model-agnostic explanations are less restrictive, as they do not constrain AI system architecture (see more in Appendix A). 
This difference is crucial, because it enables unconstrained innovation and development of effective AI systems, as well as accurate models of human decisions. 
There exist multiple model-agnostic explanation frameworks, and new ones are under development. 
Choosing the appropriate mode of explanation requires careful consideration. In particular, explanation methods should satisfy provable concepts of fairness, privacy and robustness. 
While these measures are a useful tool for those concepts in AI systems, they can be adversarially manipulated~\cite{slack2020fooling} or provide lower quality explanations to protected subgroups~\cite{balagopalan2022road}. 
Paralleling prior authors’ recommendations, we call for these drawbacks to be considered at the time when standardized benchmarks and corresponding audits are designed and developed.




\section{An AI Accountability Policy}
\label{sec:prop}


% % Figure environment removed

We propose the creation of a regulatory AI office with the legislatively-ordained power to flexibly regulate commercial AI systems.
This office establishes guidelines to divide commercial AI systems into various risk levels based on their inherent properties and intended use case, as well as by sector.
Depending on the risk level, various regulatory obligations must also be met.
%This proposal is primarily focused on transparency through explainability.

All commercially available AI systems that may pose a significant risk to society must be registered, but for sufficiently limited risk and low scale applications, we propose no further obligations.
%[BC we check registrations]
For higher-risk applications, the registry maintains AI cards for commercial systems. These AI cards combine existing proposals for transparency, such as model cards \citep{googlemodelcardpub} and data sheets \citep{GebruDatasheets}, %and %also in this proposal have 
%we additionally require
with new requirements for %information relevant to 
explainability, transparency, and human impact.
%We first discuss explainability statistics and the benchmarking needed to compute them in \cref{sec:prop:exp,sec:prop:bench}, and then we describe the responsibilities of the AI office in \cref{sec:prop:details}.
% The overall organization we are proposing is summarized in \cref{fig:overview}.





It is important that there be uniformity of AI accountability standards and practices, especially for a given deployment type. This will enable oversight over deployed AI systems thanks to the possibility of making comparisons between them.\ntia{34} 
This rationale extends to global jurisdictions as well. 
The creation of our proposed regulatory office and accompanying AI registry would enhance any jurisdiction in becoming  a leader in the space of global AI accountability and control. 

To achieve an efficient and scalable AI registry and reduce its overall cost, it is important to maintain, to the extent possible, similar structures across sectors and deployment types. 
The government itself shall mandate stringent accountability practices for the AI systems it uses.\ntia{30d}
There are various costs involved in the AI accountability, which should be covered by different stakeholders. We distinguish between the following kinds of costs:\ntia{28} 
\begin{enumerate}[(1)]
    \item Operating the AI registry and audit API --- costs covered by the regulatory administration;
    \item Operating the evaluated AI systems during automated AI audits --- covered by AI system developers;
    % \item Social media experiments – researchers;
\end{enumerate}

To ensure a strong AI accountability ecosystem, the regulatory office will need to fund sufficient domain experts for each sector and for each deployment type to develop, maintain, and learn from AI cards.\ntia{31} 



\subsection{Outline of the Proposed AI Accountability Mechanisms}

We propose that at the center of AI accountability mechanisms is an AI registry. The registry would store information about each market-ready trained AI system that is used in high-risk decision-making, e.g., Equivant’s COMPAS or GPT4. The registration of AI systems should be obligatory only for high-risk and limited-risk systems that may impact consequential decision making. To preserve innovation and reduce AI accountability costs, we propose minimal AI accountability policies and AI registry obligations. We suggest not to regulate minimal-risk systems, %in any way,
while requiring registration and intended use specification for limited-risk AI systems to ensure that they are not high risk.
%In other words, minimal-risk AI systems should not be registered at all, as is the case for nearly all current AI systems. 
Registered models will be examined for potential risk, potentially reclassified if misreported, and as their application scale becomes %significantly
larger, they may be subject to additional transparency obligations.

The purpose of the AI registry is to store information about all AI systems used in high-risk applications in the form of AI cards (model cards), inform the public about their characteristics and their alternatives, and to stimulate competition among AI system manufacturers in the space of accountable AI.\ntia{1} The AI registry stores three kinds of standardized audits (we use the words audit and assessment synonymously), ordered in terms of their sophistication:\ntia{8}
\begin{itemize}
    \item Internal AI audits --- internal audits performed by AI manufacturers;
    \item Automated AI audits --- audits that result in AI certificates, performed automatically by the AI registry in cooperation with AI manufacturer;
    \item External AI audits – external audits that are involuntary, conducted by the AI accountability agency
    %only if
    randomly for large-scale high-risk models, or if there is sufficient evidence of wrongdoing.
\end{itemize}
Internal AI audits are meant as a mass-scale solution to auditing and are most useful as obligatory audits providing basic structured input to the AI cards of each registered AI system.\ntia{1b} The structure and measures of such audits should be standardized and determined by the AI accountability agency, so that audit results are comparable among all AI systems for a given deployment type. Both internal and automated AI audits test only the deployment stage of the AI lifecycle. This reduces the burden, costs, and complexity of auditing, making it feasible to scale the audits to a large number of AI systems.

The primary purpose of automated AI audits is to incentivize AI system manufacturers in the space of AI certificates, by providing a method to assure compliance with non-binding trustworthy AI goals.\ntia{1b,1c} The secondary purpose is to verify the outcomes of an internal AI audit, a claim, or compliance with legal standards.\ntia{1c} 

External AI audits, which refer to traditional comprehensive audits, would be conducted only in warranted exceptional cases.\ntia{1b} The warrant may depend on evidence of wrongdoing or evidence-based suspicion of wrongdoing based on the outcomes of the internal and automated AI audits. 

This approach differs from the prior work by~\citet{floridi2022capai}, a procedure for conducting conformity assessment of AI systems in line with the EU AI Act. While also being narrower in scope than the proposed model/ecosystem in this paper, capAI proposes a procedure consisting of three components; namely an internal review protocol, a summary datasheet to be deposited in the EU's database, and an external scorecard which can be made available to stakeholders of the AI system.

The introduced AI accountability mechanisms will explain decisions supported by foundational generative models, e.g., ChatGPT or GPT4. Namely, large language models (LLMs) can be used to solve text classification problems and consequential decision-making typically can be represented as text classification problems. Then, the outcomes of such text classifiers can be explained with input influence measures. Hence, the AI registry could provide information about a wide range of diverse models, including well known general-purpose models such as OpenAI’s GPT4 and Google’s Bard, at different consequential decision-making tasks, such as hate speech detection or automatic essay grading. In this way, the AI registry could become the public registry of influential AI models.

This unified approach, i.e., providing explanations for both high-risk AI systems and GPAI models goes beyond the current status quo within the EU. The latter's approach treats both forms of AI differently, i.e., their proposed Conformity Assessment applies to high-risk AI systems whereas their proposed Code of Practice applies to GPAI models with systemic risk, and this is to remain the case until new (harmonised) standards emerge to help providers comply with the AI Act. Therefore, we propose to handle both forms of AI in a uniform manner which is not yet adopted or proposed by the EU. 

The registry, audits, and certificates are valuable for promoting trust and improving internal processes, but their main purpose is to encourage developers to design AI systems that engender trust in the long-term, by providing incentives for developing accountable AI with minimal regulation.\ntia{2}

\subsubsection{Protections Against Gaming Standarized Measures}
As Goodhart's law states, any such metric becomes a target, and then ceases to be useful. To partially mitigate this issue, we propose to split benchmarking data sets into two similarly distributed subsets, analogous to \emph{test and validation sets} in machine learning. In addition, automated audits can be conducted in an air-gapped mode, where the tested AI system cannot leak information bout the data it is tested on to the external world. Such technique has been used recently to test reasoning capabilities of AI systems in the ARC-AGI semi-private evaluation~\cite{chollet2024openai}. We provide details about issues related to benchmarking in Appendix B.

\subsubsection{Sectors}
The introduced mechanisms can be applied across multiple sectors, such as:\ntia{1b}
\begin{itemize}
    \item Biometric recognition technologies, e.g., facial recognition technologies~\cite{LearnedMiller2020FACIALRT};
    \item Hiring automation, e.g., activity of companies such as HireVue;
    \item Student support systems, e.g., activity of software such as EAD’s Navigate;
    \item Criminal system automation, e.g., activities of software such as Equivant’s COMPAS;
    \item Healthcare, e.g., health care needs predictions~\cite{obermeyer2019dissecting}; 
\end{itemize}

The structure of AI accountability mechanisms varies across sectors. For instance, AI card structure should depend both on the sector and the particular deployment (decision-making process) type. Furthermore, AI accountability mechanisms provide an opportunity to reinforce existing sector-specific accountability mechanisms.\ntia{1d} For instance, the EEOC collects every year the bias in hiring EEO-1 reports~\cite{eeodata}, but these reports are not used for any public purpose, because of privacy and legal obstacles. We point that the AI accountability policy could address these challenges and use the AI registry to promote exemplary automated decision-making practices.

\subsection{Intended Use and Risk Levels}

AI systems represent too many applications for a single set of rules. Just as different FDA restrictions are applied to different medications, AI controls should be tailored to the application. This requires mechanisms for carefully defining the scope of applications. Intended use is central to the AI accountability framework. 
% For instance, a facial recognition system might be approved and used for ``identifying customers in retail stores.'' We consider this to be a deployment type that describes the general type of application. 
Risk level classification should depend on the intended use, that is the deployment type, i.e., the specific kind of a decision-making process. The given AI system should be approved for marketing only once it is registered or determined to have intended uses that fall within the minimal-risk category. Then, the advertised intended use shall match registered intended use, to avoid misinformation.

Which decision-making processes shall be considered consequential or high risk?

\begin{itemize}
    \item Processes that are already protected by existing laws, e.g., hiring and housing decisions.
    \item For other decision-making processes that are consequential, but not protected by existing laws yet, the AI accountability policy could introduce a threshold for the number of people affected by the considered AI model.
\end{itemize}

Generative AI, such as LLMs, that are intended as having a broad usage and could possibly be used in consequential decision-making, would belong to the limited-risk or high-risk application category, depending on their intended use and would need registration and approval for marketing. We recommend that all limited-risk and high-risk AI systems are required to be identified to the people directly affected by them, e.g., by providing their AI registry card links or identifiers.\ntia{5}

The same AI system can be deployed many times. After being approved for such a deployment type, it is important to register every individual deployment, such as the deployment to a specific retail store, for appropriateness. On every deployment there is the question of whether a piece of software is being used in accordance with its intended use (a valid deployment) or in violation of its intended use (an invalid deployment). 


\subsection{AI Registry}
The AI registry would be the main interface between the public and the agency collecting information about AI systems used in consequential decision-making. The registry would gather and share information about AI systems, and possibly list their intended uses, to encourage good AI designs, rather than to penalize bad designs. It would be online and publicly available. The registry should be shared across sectors, to make it easier for end users to recognize, comprehend, and trust the platform.\ntia{30a} The brand and reputation of the registry will likely determine the successfulness of the AI accountability policy.
Registration is necessary for all commercial AI systems, and upon registration, risk-level is assessed, which is used to determine further transparency obligations.

The AI registry would communicate to the public the outcomes of internal audits (AI cards) and automated audits (certificates). While high-level structural components of the platform should be shared across sectors, there should be differences between sectors and deployment types at a lower level, e.g., the AI card should have different feature ontology for each deployment type. Hence, each deployment type would have a different AI card content and a different set of associated certificates. To decrease the cognitive burden to the end users of the AI registry, performance results could be shown in relation to the outcomes of other AI systems for the given deployment type, possibly as a percentile score, e.g., top 5\% accuracy in student success prediction.

%[all systems registered because]

\subsection{AI Cards}
Every AI system used in consequential decision-making shall have an AI card in the AI registry, akin to the EU setting whereby every high-risk AI system must deposit its conformity assessment (see Article 43 as well as Article 27 for the fundamental rights impact assessment that must also be conducted prior to deployment) in the EU database before placing the CE mark on its system and deploying it within the EU marketplace. One key difference is the level of detail provided in our paper compared to the proposed EU conformity assessment. We believe our proposed approach could help shape and form part of a satisfactory conformity assessment. AI cards would state results of an internal audit, which could include both model performance and input influence measures. An example of AI cards are Google’s ``model cards''~\cite{googlemodelcard,googlemodelcardpub}. The content of AI cards should be readable and entirely structured in an iterative process that includes industry.\ntia{33} That structure should evolve over time, to reflect the developments in the industry and research, e.g., of novel input influence measures. AI cards should include all input features of an AI model and be flexible enough to accommodate new features that are not a part of a feature ontology. The registry should provide a feature ontology that is specific to the deployment type to reflect the specificity of each decision-making problem. The structure of AI cards should be easy to use and understand by both humans and machines.

While the feature ontology is highly structured, the input to the AI systems can be provided in an unstructured way. For instance, imagine that an AI is evaluating a student CV. That CV could be mapped into a tabular, highly structured, format with semantically meaningful fields, such as race, gender, SAT score. A classifier could be trained on this highly-structured data, but if a LLM is our classifier, then it would not explicitly make use of the tabular representation of the data, but rather use the entire CV as input. However, to compute input influence measures, such as the impact of race on AI system’s decisions, the auditors will still need to extract structured information from CVs and manipulate that structured information to measure how such changes, e.g., changing race and associated information on the CV, impacts LLM's decisions. Such manipulations of inputs to observe AI system outputs correspond to randomized experiments performed by researchers to understand decision-making processes, e.g., the seminal callback studies change the first name on CVs to see whether callback rates from employers will be affected by racial soundingness of the name~\cite{bertrand2003are}.

\subsection{Automated Audits and Certificates}
Automated audits can be used to generate AI certificates for any AI system in the AI registry. They could be performed online as ``QA sessions'' between a given AI model and the regulatory auditing API after prior authorization. 
Queries would be posed by the AI auditing agency, answered by the AI system. 
The audited AI model should specify what features it expects on input. 
The audit API will initiate the testing session by providing (synthetic or real-world) data on input and asking the AI model to output decisions or answers for each sample. 
These audits have a server-client architecture and characteristics of both internal and external audits, since the auditing agency provides a server, while the agent’s developer provides a client. 
They treat AI models as black-boxes and only require access to their inputs and outputs. 

We recommend that automated audits follow the SAIL-ON model of AI evaluations: the AI system manufacturers are informed about the goals and structure of an audit, but not its content. 
In particular, the AI system manufacturers should not know the (test) dataset that they are evaluated on. 
Such a dataset could contain novel samples to test an AI system’s resilience, adaptation, and confidence when faced with unexpected real-world inputs.

AI audits can be voluntary and be used to certify AI systems. 
Each deployment type should have a different set of associated certificates; for instance, a certificate that a foundational LLM offers 93\% accurate scientific references, or that a hate speech classifier is not discriminating against race and gender, while achieving top 12\% disparate impact score in comparison to other AI systems having the same intended use. 
Finally, certificates and the aspects they test will evolve over time in order to account for ever-changing industry, research, practice, and the overall AI ecosystem.



% Similar to our prior work on input influence, counterfactual explanations can be utilized to reveal discrimination in decision-making systems. If the users with undesirable outcomes from one protected subgroup requires more effort to achieve their desired outcome over another, then decisions are potentially influenced by features with a relationship to users’ subgroup status, thus requiring an audit.



\subsection{External Audits}
External audits are generally more costly and comprehensive than internal and automated audits, since they test all stages of the AI lifecycle. Such audits should be infrequent, to reduce the overall costs of AI accountability policy. 
The rules determining occurrence of external audits shall be sector-specific. 
These external audits would be greatly facilitated by a policy requiring that manufacturers of AI models used in high-risk scenarios keep records from the model's lifetime so that all steps leading to the creation of the model are reproducible.\ntia{20,21,30b} 
For instance, the manufacturers of such AI system may be required to maintain the source code, data, parameters, and random seeds used for training the respective AI model. 
Outcomes of external audits would be kept confidential, with only summaries published in the AI registry.\ntia{1c}







\section{Conclusion}
We propose a transparency and accountability policy for AI systems that is compatible with the leading existing regulation in this space, particularly the EU's AI Act. Our proposal is inspired by the U.S. FDA and U.S. anti-discrimination regulation. 
First, we propose that AI systems disclose their intended use and that an AI office coordinate the development of standardized evaluation measures for each high-risk intended use of AI systems.
Second, we propose that these standardized evaluation measures are computed internally by AI system providers and via automated AI audits, to diminish the need for more expensive external audits.
Third, we suggest that transparency and accountability of AI systems can be achieved by providing the results of such evaluations, that is the standardized measures, in a public-facing AI registry to facilitate comparisons among AI systems within a particular high-risk intended use.

To conclude, we argue that the expanding influence, intricacy, and evolution of AI systems necessitates
\begin{inparaenum}[(1)]
    \item extensive transparency, which a regulatory authority office can facilitate through standardization of obligations and measures, and 
    \item dedicated expertise, not only of the risks they pose across various application fields, but also of the underlying technologies.
\end{inparaenum}
Our proposal is synergetic with research in the related areas of responsible, robust, safe, explainable, and interpretable AI, as well as open-world learning (Appendix C).
We believe that this approach is necessary to keep up with the rapid evolution of AI research and industry and ensure long-term societal benefits of the developing, and potentially very powerful, AI technology.

% While this white paper is focused broadly on AI
% %and social media
% systems, more in-depth FDA-inspired recommendations about documentation, categorization, and evaluation of facial recognition systems are detailed in the prior work that this white paper builds upon~\cite{LearnedMiller2020FACIALRT}. 

% There would be many challenges in implementing these recommendations. To begin, like with the FDA’s regulation of the medical industries, there will always be a tension between considering those with financial incentives for producing the technologies and those who benefit from oversight and regulation. 

% Segments of the “tech” industry have opposed regulation within the US and abroad, citing concerns about hindering growth, innovation, and beneficial applications. Nevertheless, there is growing pressure for the regulation of technology companies, and even some evidence that the companies themselves would like some regulatory guidance, if for no other reason than to ensure a level playing field among the firms.


%\bibliographystyle{ACM-Reference-Format}
\bibliographystyle{plainnat}
\bibliography{zotero,manual_additions}


\appendix



\section{Explainability Metrics}
\label{sec:prop:exp}
%[Individual vs model]
%[company's private data -> model level expl and public data -> individual and model level expl]
%[saliency maps]
%[something about risk levels]

% intro to explainability
As AI systems grow in complexity, it is increasingly difficult to reason about \emph{why} they make certain decisions. Complex predictive algorithms make increasingly high-stakes decisions; however, in order to foster trust in these methods, their underlying decision-making processes need to be better understood. This need gives rise to a rich literature on \emph{explaining} AI systems.

We propose that the AI registry should contain information about AI systems via model explanations. Model explanations provides users with specific details on how each of their features, or characteristics, influenced the outcome they received from an AI system. These explanations can be generated using popular off-the-shelf methods such as SHAP and LIME \cite{lundberg2017unified,ribeiro2016why}. Under more complex data such as text and images, these characteristics could be the words from bodies' of text or pixels respectively. In the registry, AI systems should provide local explanations for a provided benchmarking dataset. Local explanations provide the influence of features for each individual sample in the dataset, providing an intuitive method to determine if an individual or others similar to them received decisions for the right reasons. Explainability methods may also require context data to generate explanations, e.g., SHAP requires data for integrating out features when measuring influence. Since we do not assume access to proprietary data from AI manufacturers and we call for even comparisons across AI systems, the provided benchmarking dataset should be used as the context data for these methods.  

AI systems could also be required to provide global explanations over entire datasets in the AI registry. These are typically aggregates of local explanations over a given dataset and remain consistent with them, e.g., global explanations for SHAP. While not as granular as an individual explanation for a given user/input, global explanations under the same dataset across differing AI systems provides an intuitive method for comparing explanations. Without needing to perform their own aggregations or sample selection, users and experts can observe the differences in the average influence of features across AI systems and either pick the best system for them or contest the system of the manufacture they are using if it's average feature influence is misaligned with other AI systems. %Additionally, if a manufacture desires to release influence measures in the AI registry but has private data, global explanations are appropriate as they do not require information about each sample since it is an aggregation. Conversely, each sample being explained must be provided for local explanations to be interpretable since they compute the influence of each feature with respect to the sample being explained.
%
Global explanations should not replace local explanations in the registry, as they fall short when the influence of features or characteristics on outcomes do not aggregate cleanly. For instance, computer vision explanation methods often use saliency maps to highlight the influence of each pixel of an image \cite{GuidottiSurveyExplaining}. Given that image classification methods are often complex deep learning methods and an image dataset's samples often vary in visual aspects such as perspective and shape of the subject, e.g. the CIFAR-10 dataset of different animals and vehicles \cite{Krizhevsky09learningmultiple}, the aggregation of these saliency maps can result in a map with seemingly random and uninterpretable influence highlights. Therefore, local explanations for individual samples will be needed for interpretable explanations.
%is this understandable? I'm thinking like CIFAR 10. It has a bunch of differently shaped animals and is shot from all sort of angles and distances.

Depending on risk levels and intended use, the office
could also require AI systems to provide explanations and recourse options, which is line with the EU AI Act Article 86 and accompanying proposed liability directives as outlined above,  to each user and each consequential decision, in addition to including them in the AI registry. Similar requirements are suggested in Recital 71~\cite{gdprrecital71} of the EU’s General Data Protection Regulation (GDPR)~\cite{EuropeanParliament2016a}. Recital 71 states that automated decision-making systems should include ``specific information to the data subject and the right to obtain human intervention, to express his or her point of view, to obtain an explanation of the decision reached after such assessment and to challenge the decision.'' 
This Recital, however, is non-binding and provides no technical insight on what type of explanation method should be provided. 
In response, \citet{wachter2017} propose that explanations should provide users with 
\begin{inparaenum}[(1)]
\item understanding on why a particular decision was reached;
\item grounds to contest undesired decisions, and 
\item what could be changed to receive a desired result, i.e., recourse.
\end{inparaenum}
To amend the criticism of the GDPR, \citeauthor{wachter2017} introduce counterfactual explanations. These methods explain  ``how the world would have to be different for a desirable outcome to occur,'' as a means to satisfy these requirements without needing to expose the internal logic of automated decision-making systems. 
For instance, if a loan applicant was denied a loan, a counterfactual explanation will answer 
\begin{inparaenum}[(1)]
    \item what type of applicant was accepted and 
    \item what actions are required to resemble the accepted applicant.
\end{inparaenum} 
Future legislation could follow these proposed explanation requirements and require both feature influence and counterfactual explanations. 


\section{Air-gapped Benchmarks}
\label{sec:prop:bench}

To achieve
%our goal of
transparency in AI systems, we propose a benchmarking-based approach to objective model metrics. Benchmarking results are used to compute individual level explanations, which are reported in full and also in aggregate, alongside other relevant statistics to model performance, such as loss, accuracy, or type 1 and type 2 errors, on data pertaining to various protected groups, as determined by the agency on a case-by-case basis.
%We do not establish
% Except in high-risk sectors with legal precedent (e.g., nondiscriminatory employment), 
We do not establish particular standards of what is and is not acceptable, but rather provide benchmarks and audit results, allowing models to be compared
%to one another
transparently.

Collecting and curating benchmarking datasets is a time-consuming and costly process, however it is necessary to compute explainability metrics, and %establishes a level playing field on which to
allow fair comparisons between models, which we feel is
%a sufficiently valuable objective
sufficiently important so as to justify this cost.
As Goodhart's law states, any such metric becomes a target, and then ceases to be useful. To partially mitigate this issue, we propose to split benchmarking data sets into two similarly distributed subsets, analogous to \emph{test and validation sets} in machine learning.

The first fold of data is publicly available, and is used in both voluntary self-reported internal audit and black box automated audits. We require that modelers not use this benchmarking data in training, % their models,
but this requirement is not directly enforceable, and any attempt to keep this data private would be futile, as it would eventually be fed into commercial systems in benchmarking.
%over the course of standard black-box benchmarking.
Even without explicit intent to game a benchmark, any released benchmarking data is liable to find its way into training data, intentionally or unintentionally, which again would bias results, albeit in subtler manner, and the agency must remain aware of this.


%[copy audit stuff from below]

%\paragraph{External Benchmarking Data Integrity}

Crucially, the second fold benchmarking data, used for external audits must be %an air-gapped independent fold of data from that made public for voluntary audits and used in internal audits.
remain air-gapped and confidential.
From a security perspective, we must treat any data made public as compromised, as bad actors could easily use this data to intentionally make better predictions and provide better explanations during voluntary and internal audits.
We assume an adversarial threat model, and no AI systems is allowed to %communicate
communicate with the outside world in any way after being evaluated on this air-gapped data.
Because %external audit
this benchmarking data must remain confidential, external audits can only release summary statistics. However, likely even this is unnecessary, as %we can release the
AI cards may report internal audit summary statistics and individual explanations, and we need only use the external audits to determine whether further investigation is necessary or rules have otherwise been violated (as
%ideally
internal and external audit statistics should match up to a small amount of statistical error).


%[TODO: auditees must check for benchmark data in their own training data, as they can be caught by external audits for this. This should be mentioned earlier?]

%\paragraph{Benchmarking Failure Modes}
Under this setup, we identify several failure modes, all by comparing the results of voluntary internal audits, automated Black Box audits, and air-gapped external audits. Many such errors are explainable by unintentional procedural mistakes or improper data stewardship, but intentional deceit is also a potential explanation, and all such discrepancies should be explored and diagnosed. 
%
%By keeping an air-gapped fold of confidential external audit data, we gain the ability to detect more forms of wrongdoing.
%We now
In particular, we
assess the internal audit performance on public benchmarking data, external audit performance on public benchmarking data, and external audit performance on air-gapped data.
A mismatch between internal and external performance on the same dataset indicates some irreproducibility issue or outright fabrication of audit results, which needs to be investigated.
If these match, but performance on the air-gapped and public datasets do not, this signifies that
%the training data were likely compromised in some way, or some other bad actions were taken. 
either benchmarking data were (ab)used for training, or possibly that modelers intentionally adapted the model to the benchmarking data, likely to gain some illicit advantage in benchmarking.


\section{Support for Researchers}

The AI Accountability regulation would support researchers advising the design and updates to automated audits and explainability measures used in the AI registry. 
% To indirectly support these operations, the government could amplify the allocation of financial resources towards research endeavors in the areas of:
Research areas that are synergetic with the proposed AI accountability policy include
\begin{enumerate}[(1)]
    \item Explainable and interpretable AI methods;
    \item Open-world benchmarks, where there is a gap between model developers and model auditors, such as evaluations in DARPA’s SAIL-ON program;
    \item Collaborative development of ontologies, since the structure of AI cards should be sector-specific and developed collaboratively.
    \item Safe and robust machine learning.
    \item Mechanistic interpretability and causal discovery.
    % \item Identify stakeholders beyond industry and government - coalitions, academic institutes, not for profits, advocacy groups for a holistic approach to accountability.
    \item Acknowledge inherent biases, both systemic and human that when left unchecked create ecosystems that unjustly profile historically disadvantaged communities. \cite{VEubanks2018}
    % \item Social media accountability.
\end{enumerate}

