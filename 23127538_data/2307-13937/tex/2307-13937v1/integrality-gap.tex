\section{Integrality Gap for Configuration LP}\label{sec:int-gap}

We begin by describing the configuration LP for GMP. We then
explain the high-level ideas behind the gap construction and the properties we need from the norm $\psi$. We then describe the norm $\psi$ and  the integrality gap instance formally and then prove \Cref{thm1}.
As mentioned earlier, this gap instance and the norm $\psi$ form the key gadget in our hardness construction in \Cref{subsec: reduction}, and understanding it is crucial to the results in \Cref{sec: reduction}.

{\bf Configuration LP.} The most natural LP relaxation of GMP is to consider assignment variables $x_{ij} \in [0,1]$ that determine the fraction of job $j$ assigned to machine $i$, and impose natural constraints. However, simple examples show that such an LP is too weak to handle general norms $\psi$. A stronger relaxation
is the configuration LP, where we have exponentially many variables $x_{i,C}$, one for each machine $i$, and a possible subset of jobs $C$ that can be feasibly assigned to machine $i$.  

Let $J$ denote the set of jobs, and $M=[m]$ be the set of machines. 
For a machine $i\in M$, and subset $C\subseteq J$, let $p_i[C] = (p_{ij}\cdot \mathbbm{1}[j\in C])_{j\in J}$ denote the vector of sizes of jobs in $C$.
Let $T$ be a guess on the optimum makespan (we can do a binary search on $T$).
Call a configuration $C$ {\em valid} for machine $i$ if the load $\psi_i(p_i[C])$ of $C$ on $i$ is at most $T$.
 We will slightly abuse notation and denote $\psi_i(p_i[C])$ by $\psi_i(C)$. 

Consider the following feasibility LP.
\begin{subequations}\label{ConfigLP}
\begin{align}
    \sum_{C\subseteq J} x_{i, C} &\leq 1 && \forall i\in M \label{LP:machine}\\
    \sum\limits_{i\in M, C: j\in C} x_{i,C} &= 1 && \forall j\in J \label{LP:job}\\
     x_{i,C} &= 0 &&\text{if } \psi_i(C) > T \label{LP:threshold}
\end{align}
\end{subequations}
The first constraint says that each machine has at most one configuration. The second constraint says that each job is assigned to a machine, and the last constraint ensures that only valid configurations are considered and hence the generalized makespan on any machine is at most $T$.
This LP can be solved efficiently for any desired accuracy $\epsilon>0$ (so the configurations satisfy $\psi_i(C) \leq (1+\epsilon) T$), see e.g.,~\cite{deng2023generalized}. 

\medskip

{\bf The main idea.}
We start with a simple instructive example,
that motivates our choice of the norm and the choice of parameters that lead to the $\Omega(\sqrt{\log n})$ integrality gap.
Consider a random set system on a universe $U$ of $n$ elements and $m$ sets $A_1,\ldots,A_m$ where each element independently lies in $m/2$ randomly chosen sets. 
We will set $m\ll \log n$.
Create an unrelated machine (in fact restricted assignment) scheduling instance $I$ with $m$ machines, where only the jobs in $A_i$ can be assigned to machine $i$ (all other jobs have infinite size on $i$). As each element lies in $m/2$ sets, the LP solution that picks the configuration $A_i$ on each machine $i$ with $x_{i,A_i}=2/m$ is feasible, and uses only $2$ machines in total.

However, in any integral solution, we claim that several machines must pick a non-trivial fraction of jobs from their sets $A_i$. Roughly, this is because the union of any $\ell\ll m$ sets $A_i$ still leaves about $2^{-\ell} |U|$  uncovered (as each element lies in a set with probability $1/2$). Hence in any feasible integral solution, for any $\ell>0$, at least $\ell$ machines must be assigned at least $2^{-\ell} |U|/m$ jobs.

To exploit this, suppose we define 
$\psi$ as the top-$k$ norm with $k\approx \Omega(2^{-\ell}n/m)$, so that any machine with $\geq k$ jobs incurs the same load, say $T$. Then in any integral solution, at least $\ell$ machines have load $T$, while fractionally at most $2$ machines (in total) have load $T$.
By creating $ m/2$ disjoint copies $I^{(1)},\ldots, I^{(m/2)}$ of these set-cover instances, one would then expect that fractionally each machine has load $T$, while integrally the average load becomes $\Omega(\ell)$.

Unfortunately, this does not quite work as stated above, because once there are several instances $I^{(1)},\ldots, I^{(m/2)}$, as these jobs share the same machines, an algorithm can find a low makespan even if it cannot figure out the good underlying set cover solution.
In fact, this is provably so, as \cite{deng2023generalized}
gave a $3$-approximation when $\psi$ is a $\Top{k}$ norm. 

However, our key observation is that this idea can still be made to work by choosing the instances $I^{(1)},\ldots,I^{(m/2)}$ at different scales (of the number of jobs and processing times) and defining the norm $\psi$ as a suitable mixture of the top-$k$ norms at these different scales.
Roughly, this norm $\psi$ still behaves as a top-$k$ norm at each individual scale, but when jobs from different scales are combined, it takes on a large value, which be used  to create a large gap in the reduction above.

Implementing this idea requires separating every two scales by $\Omega(2^{\ell})$. So the $\ell$ scales leads to instances with size about $2^{\ell^2}$ leading to the choice of $\ell = \Theta(\sqrt{\log n})$ to produce the $\Omega(\ell)$ gap. 
We now give the details.

\medskip

\textbf{The Instance.} The instance will have $n$ jobs and $m = \sqrt{\log n}$ machines. 
We first create $h = m/8$  disjoint set-cover instances $I^{(1)},\ldots, I^{(h)}$, where
 $I^{(s)} = (U(s); A_1(s), A_2(s), \ldots, A_m(s))$ forms an $(n_s, m, \ell, \beta)$ set-system with $\ell = m/10, \beta = \exp(-m)$ and $n_s = (\sqrt{n}/2) \cdot \exp(4ms) = (\sqrt{n}/2)\bt^{-4s}$.
 Note that $n_s$ increases as $\beta^{-4s}$ with $s$, and $n_1 \geq \sqrt{n}$ and $n_h =n/2$, and it is easily checked that parameters for each $I^{(s)}$ satisfy the condition in  \Cref{lem: mlb-set-system}.

For each $s\in[h]$, the elements in $U(s)$ correspond to jobs with size
$p^{(s)}=\bt^{s-1}$  (or infinite if the job cannot be assigned to a machine). Each job $j$ in $U(s)$ has size $p_{ij} = p^{(s)}$ on machine $i$ iff $j \in A_i(s)$ and $p_{ij}=\infty$ otherwise.
We abuse the notation slightly to refer to the resulting scheduling instance also as $I^{(s)}$.
As the instances $I^{(1)}, \ldots, I^{(h)}$ are at different scales in terms of the number of jobs and processing times, we refer to jobs in $I^{(s)}$ as being in the $s$-th \textit{size class} 
\footnote{The total number of jobs in the $h$ instances is $\sum_{i = 1}^h n_s \approx n/2$. To make the total number of jobs $n$,  we add dummy jobs that have processing time $0$ on all machines.}. 




We define the inner norm $\psi = \sum_{s\in [h]}\term{s}$, where each $\term{s}$ is a scaled top-$k$ norm given by
\begin{equation}\label{innernorm}
    \term{s}(\normvar) = \frac{\Top{\bt^2n_s}(\normvar)}{(\bt^{2}n_sp^{(s)})}.
\end{equation}

Each machine $i$ will have the same inner norm $\psi_i=\psi$.
 Informally, $\psi$ has the following key property: for any subset $C$ of $A_i(s)$ with at least $\beta^2 n_s$ jobs, $\term{s}_i(C) = 1$ and $\term{s'}_i(C) \approx 0$ for $s' \neq s$. This in particular implies that $\psi_i(A_i(s)) \approx 1$ for any class $s$. Moreover, if $C$ is an arbitrary subset of jobs with at least a $\beta^2$ fraction of jobs from $r$ distinct sets among $A_i(1), A_i(2), \cdots, A_i(h)$, then $\psi_i(C) \approx r$  (roughly, each such size-class $s$ affects a different term $\psi^{(s)}$ of the norm). This property motivates the following definition.
 


\begin{definition}[Heavy Size Class]\label{def:heavysets}
Given an assignment of jobs $\schedule: {J}\rightarrow {M}$, we say that size class $s\in[h]$ is heavy on machine $i\in M$, if at least $\bt^2n_s$ jobs from $A_i(s)$ are assigned to machine $i$. 
\end{definition}
We now formally state and prove
the property of the norm described above.

\begin{lemma}\label{lem:heavy_sets_bound}
   For any  $s\in [h]$ and $i\in M$, $\psi_i(A_i(s)) = 1+o(1)$. Furthermore, for an assignment $\schedule:J\rightarrow M$, if $C$ is the set of jobs assigned to machine $i$, then  $\psi_i(C)$ is at least the number of heavy size classes $s\in [h]$ on machine $i$.
\end{lemma}
\begin{proof}
We first show that $\psi_i(A_i(s)) = 1+o(1)$ by computing the value of $\term{s'}$ for different $s'$. By the definition of $\term{s'}$,
    \begin{align*}
    \term{s'}_i(A_i(s)) = \frac{\Top{\bt^2n_{s'}}(p_i[A_i(s)])}{(\bt^2n_{s'}p^{(s')})}  = \frac{\min\{\bt^2n_{s'},|A_i(s)|\}\cdot p^{(s)}}{(\bt^2n_{s'}p^{(s')})} = \left(\min\left\{1,\frac{|A_i(s)|}{\bt^2n_{s'}}\right\}\right)\frac{p^{(s)}}{p^{(s')}}
    \end{align*}
    For $s=s'$, this exactly equals $1$ (as $\beta\ll 1$ and $|A_i(s)|\approx \frac{n_s}{2}$).
    
\noindent For $s'<s$, we have   $\term{s'}_i(A_i(s)) \leq \frac{p^{(s)}}{p^{(s')}} = \bt^{(s-s')} \leq \bt.$


\noindent Finally, for $s'>s$,  we have
\[ \term{s'}_i(A_i(s))  \leq  \frac{|A_i(s)|}{\bt^2n_{s'}}\cdot\frac{p^{(s)}}{p^{(s')}}  \leq  \frac{n_s}{\bt^2n_{s'}}\cdot\frac{p^{(s)}}{p^{(s')}} = \bt^{3(s'-s)-2} \leq \bt.\]

Let us now consider an arbitrary set of jobs $C$ assigned to machine $i$. By the monotonicity of the norm,
    \[     \term{s}_i(C) \geq \term{s}_i(C\cap A_i(s)) = \min\left\{1,\frac{|C\cap A_i(s)|}{\bt^2n_{s}}\right\} \geq \mathbbm{1}[s \text{ is heavy on }i].    \]
    As $\psi_i(C) = \sum\limits_{s\in [h]}\term{s}_i(C)$, 
it follows that $\psi_i(C)$ is at least the number of heavy size classes on $i$.
\end{proof}

\medskip

\noindent{\bf Fractional Solution.} 
 
We claim that the following solution is feasible for the configuration LP \Cref{ConfigLP} with $T=2$:
 for each machine $i\in M$, set $x_{i, A_i(s)} = 2/m$  for each $s\in [h]$.
 
Clearly, \Cref{LP:machine} is satisfied for each $i\in {M}$ as there are $h<m/2$ sets of jobs $A_i(1), A_i(2), \ldots, A_i(h)$, each with value $2/m$. \Cref{LP:job} is satisfied as each job $j\in U(s)$, for each $s\in [h]$, lies in $m/2$ sets in $A_1(s), A_2(s), \ldots, A_m(s)$. \Cref{LP:threshold} is also satisfied as $\psi_i(A_i(s)) = 1+o(1) <2=T$ by \Cref{lem:heavy_sets_bound}.

\medskip

\noindent{\bf Integral Solution.}
We now show that any integral solution has generalized makespan  $\Omega(\sqrt{\log n})$.


\begin{lemma}
For any assignment $\schedule: J \rightarrow M$, there is some machine with load $\Omega(\sqrt{\log n})$. 
\end{lemma}
\begin{proof}
We first show that each size class is heavy on at least $\ell=\sqrt{\log n}/10$ machines. 
Suppose this is not true for some size class $s\in [h]$. Let $H\subseteq M$ be the set of machines on which $s$ is heavy, and so $|H|<\ell$. As $I^{(s)}$ forms an $(n_s, m, \ell, \bt)$ set-system, by \Cref{def: mlb-set-system}, $|\cup_{i\in H} A_i(s)| \leq (1-\beta)n_s$, and hence at least $\bt n_s$ jobs from $U(s)$ are assigned to machines $i\notin H$. However, as any machine $i\notin H$ can have at most $\bt^2 n_s$ jobs from $A_i(s)$, the machines in $i\notin H$ can have is at most $m\cdot \bt^2n_s < \bt n_s$, contradicting that each job in $U(s)$ was assigned to some machine.

By averaging over machines, there exists a machine $i$ on which at least $(h\ell/m) = \Omega(\sqrt{\log n})$ size classes are heavy. By \Cref{lem:heavy_sets_bound}, this implies that $\psi_i = \Omega(\sqrt{\log n})$.
\end{proof}
This concludes the proof of \Cref{thm1}.


