\documentclass[12pt, letterpaper]{article}
%verbose=true,
%\usepackage{arxiv}
\usepackage[margin=1in]{geometry}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage[round]{natbib}
\usepackage{doi}
\usepackage{amsmath,setspace, enumerate}
\usepackage{amssymb}
\usepackage{algorithm, comment}
\usepackage{algpseudocode, titlesec}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{color, colortbl}

\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{pro}{Proposition}
\newtheorem{rem}{Remark}
\newtheorem{assum}{Assumption}
\titlespacing*{\section}{0pt}{3pt plus 3pt minus 1pt}{3pt plus 3pt minus 1pt}
\titlespacing*{\subsection}{0pt}{3pt plus 3pt  minus 1pt}{3pt plus 3pt minus 1pt}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\E}{\mathbb{E}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\normx[1]{\left\Vert#1\right\Vert}
\newcommand{\mathbbm}[1]{\text{\usefont{U}{bbm}{m}{n}#1}}
\setlength{\parskip}{6pt}
\setlength{\parindent}{0pt}



\title{A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning\vspace{-12pt}}
\author{\small
Spencer Giddens$^1$\footnote{co-first authors}, Yiwang Zhou$^{2*}$, Kevin R. Krull$^{3,4}$, \vspace{-6pt}\\ 
\small Tara M. Brinkman$^{3,4}$, Peter X.K. Song$^5$, Fang Liu$^1$\footnote{corresponding author: fliu2@nd.edu}\\
\small$^1$ Department of Applied and Computational Mathematics and Statistics\vspace{-6pt}\\
\small University of Notre Dame, Notre Dame, IN 46556\\
\small$^2$ Department of Biostatistics, St. Jude Children's Research Hospital, Memphis, TN 38105\\
\small$^3$ Department of Epidemiology and Cancer Control, \vspace{-6pt}\\ \small St. Jude  Children's Research Hospital, Memphis, TN 38105\\
\small$^4$ Department of Psychology and Biobehavioral Sciences,  \vspace{-6pt}\\ \small St. Jude Children's Research Hospital, Memphis, TN 38105\\
\small$^5$ Department of Biostatistics, University of Michigan, Ann Arbor, MI 48109}

% Uncomment to remove the date
\date{}

%\renewcommand{\shorttitle}{DP-wERM and DP-OWL}

\begin{document}
\maketitle
\vspace{-12pt}
\begin{abstract}
It is commonplace to use data containing personal information to build predictive models in the framework of empirical risk minimization (ERM).
While these models can be highly accurate in prediction, results obtained from these models with the use of sensitive data may be susceptible to privacy attacks.
Differential privacy (DP) is an appealing framework for addressing such data privacy issues by providing mathematically provable bounds on the privacy loss incurred when releasing information from sensitive data.
Previous work has primarily concentrated on applying DP to unweighted ERM.
We consider an important generalization to weighted ERM (wERM).
In wERM, each individual's contribution to the objective function can be assigned varying weights.
In this context, we propose the first differentially private wERM algorithm, backed by a rigorous theoretical proof of its DP guarantees under mild regularity conditions.
Extending the existing DP-ERM procedures to wERM paves a path to deriving privacy-preserving learning methods for individualized treatment rules, including the popular outcome weighted learning (OWL).
We evaluate the performance of the DP-wERM application to OWL in a simulation study and in a real clinical trial of melatonin for sleep health.
All empirical results demonstrate the viability of training OWL models via wERM with DP guarantees while maintaining sufficiently useful model performance.
Therefore, we recommend practitioners consider implementing the proposed privacy-preserving OWL procedure in real-world scenarios involving sensitive data. 
%OWL is widely used in clinical studies but also has applications in personalized advertising and recommender systems.
%This work has broad implications in medical studies and other settings that use wERM models for making predicting while needing to protect the privacy of the individuals whose data is used in training such models.
\vspace{6pt}

\noindent \textbf{keywords}: %differential privacy; outcome weighted learning; 
Differential privacy; global sensitivity; individualized treatment rule; support vector machines; weighted empirical risk minimization.
\end{abstract}




\newpage
%-------------------------------------------
\section{Introduction}\label{sec:intro}\vspace{-6pt}
%{\color{red} I would like to switch the first two sections; 1.2 first and 1.1 second.  Perhaps 1.1 can be absorbed into 1.2.  I think there lacks a key insight on the need of data privacy in OWL. This insight/need should be stated in the opening sentences in this paper. Below is my try for the opening paragraph.  In addition, 1.3 is too long and needs to be shortened.  Overall the integration of three subsections needs an improvement.  }

% {\color{red} Outcome weighted learning (OWL) has gained its great popularity as one of the most effective data analytics to employ the arsenals of machine learning to deliver individualized medical or business decisions. In the current literature, establishing OWL is carried out under the assumption that data scientists have full access to training data with no protection of data privacy. This assumption is indeed impractical because in real-world applications many data sources such as electronic health records and national survey data contain sensitive information, so ensuring a certain level of data privacy is inevitable. To address this privacy requisite, in this paper we aim to make a practically important, non-trivial extension of OWL, and the resulting machinery is termed as \emph{differentially private outcome weighted learning (DP-OWL)}. This extension is technically viable due to the recent advances in differentially private support vector machines (SVMs) that play a central role in performing optimization required in the derivation of OWL.}


This work is motivated by outcome weighted learning (OWL) \citep{Zhao2012}.
OWL has gained great popularity as one of the most effective data analytics techniques to employ the arsenals of machine learning (ML) to deliver individualized medical or business decisions.
%Clinical trials are a common data source used to train OWL models In in the context of precision medicine.
For example, in a two-armed randomized trial, participants are randomly assigned to one of two treatments. After receiving the treatment, a ``benefit'' metric is measured to assess the effectiveness of the treatment for each participant.
The loss function used in OWL represents the benefit-weighted average shortfall between the randomly assigned treatment and predicted labels over the training data.
Based on the collected information (baseline characteristics and prognostic variables,  treatment assignment, and benefit measurement), a classifier can be trained through OWL to predict the optimal treatment with the highest expected benefit for individuals in the future. 

However, the current literature on OWL assumes that data scientists have unrestricted access to training data, disregarding privacy constraints and guarantees.
This assumption proves impractical in real-world applications where sensitive information exists in data sources, such as randomized clinical trials, electronic health records, and national surveys, necessitating the protection of data privacy for ethical and legal purposes.
Therefore, there is a need for privacy-preserving methods in OWL models that can enhance the efficiency of model development while ensuring the safeguarding of individuals' data.
To address this privacy requirement, we propose a general framework of \emph{differentially private weighted empirical risk minimization (DP-ERM)}, of which DP-OWL is a special case.
DP-OWL leverages recent advancements in differentially private support vector machines (SVMs), which play a vital role in the optimization process required for deriving OWL models.
DP-OWL aims to provide a practical solution for applying OWL with privacy guarantees to sensitive data, facilitating streamlined and accelerated model development while upholding data privacy standards.

\subsection{Background}\label{subsec:background}
OWL is a common framework for constructing individualized treatment rules (ITRs) that fall under the umbrella of causal inference machine learning (ML) methods.
ITRs are fundamental in the field of  precision medicine \citep{national2011toward, collins2015new}, an innovative framework for maximizing the clinical benefit that takes into account individual heterogeneity to tailor treatments for subgroups of patients \citep{Qian2011} instead of relying on a one-size-fits-all paradigm.
% various approaches have been proposed in the literature to construct ITRs \citep{watkins1989learning, murphy2003optimal}.  
%This approach contrasts with broadly applying the same treatment to an entire population presenting with the same set of symptoms and can lead to better patient outcomes \citep{Qian2011}. 
In addition to precision medicine, ITRs are also applicable to personalized advertising \citep{Wang2015, Sun2015} and recommender systems \citep{Schnabel2016, Lada2019}. 

Various methods have been proposed in the literature to establish ITRs, in addition to OWL.
OWL derives ITRs that directly optimize the population-level expected outcomes.
\citet{Zhao2012} showed that estimating the optimal ITR is equivalent to a classification problem of treatment groups, in which subjects in different groups are weighted proportionally to their observed clinical benefits. %In OWL, weighted support vector machine (SVM) is invoked to estimate the optimal ITR.  
\citet{Zhou2017} proposed residual weighted learning (RWL) to improve the finite sample performance of OWL by weighting individual subjects with the residuals of their benefits obtained by a regression model and solving the optimization problem using a difference of convex algorithm.
% To address these challenges, residual weighted learning (RWL) \citep{Zhou2017} was proposed to derive ITRs by weighting observations using residuals of the outcome from a regression fit and solve the optimization problem using a difference of convex (d.c.) algorithm. 
Matched learning (M-learning) \citep{Wu2020} further improves the OWL-based framework by utilizing matching instead of inverse probability weighting to balance subjects in different treatments, making the algorithm more applicable for the analysis of observational studies (e.g., large-scale electronic health records). %, which typically have much larger sample sizes than clinical trials. 
Extensions of OWL have also been made for the search of individualized continuous doses \citep{chen2016personalized}, the derivation of ITRs for multiple treatment options \citep{zhou2018outcome}, and the establishment of dynamic treatment regimes \citep{eguchi2022outcome}. 

\begin{comment}
Traditional privacy protection techniques such as anonymization %(i.e., removing identifiers like names, addresses, etc., or quasi-identifiers such as age, gender, etc.) 
have been shown to be insufficient in ensuring privacy.
For example, %\citet{Narayanan2008} demonstrated, using the anonymized 2006 Netflix Prize dataset, that uncovering sensitive, individual-level information from anonymized datasets is possible with access only to limited, publicly accessible side information.
\citet{Sweeney2015} demonstrated re-identification attacks using Washington State health records.
ML models learned from anonymized datasets have also been shown to be vulnerable to attacks inferring membership in and sensitive attributes of the dataset \citep{Shokri2017, Zhao2021}.
\end{comment}

Differential privacy (DP) \citep{Dwork2006} is a state-of-the-art privacy notion that provides a mathematically rigorous framework for ensuring the privacy of sensitive datasets.
In this framework, carefully calibrated random noise is injected through a randomized mechanism into statistics or function outputs derived from sensitive data, lowering the probability of learning personal information used to generate the outputs.
As a trade-off, mechanisms satisfying DP generally incur a cost in data utility.
Privacy loss parameters can be prespecified to yield a satisfactory trade-off between privacy protection and data utility.


\subsection{Related Work}\label{subsec:related_work}
To our knowledge, no privacy-preserving counterparts have been developed for the models constructing ITRs.
Though some recent works estimate the individual treatment effects themselves in a DP-satisfying manner \citep{Betlei2021, Niu2022}, our work is different in that it learns a treatment assignment rule with DP guarantees.

In terms of DP-ERM, \citet{Chaudhuri2009} analyzed the application of DP to releasing logistic regression coefficients.
%Two different perturbation methods for achieving DP, known as output perturbation and objective perturbation, were analyzed. 
\citet{chaudhuri2011} generalized DP logistic regression and developed DP counterparts to binary classification problems in a general unweighted ERM framework.
Subsequent works have incrementally improved DP-ERM analysis, such as by tightening bounds on excess risk or making the methods more computationally efficient \citep{Bassily2014, Kasiviswanathan2016}.
%Some other related works on DP ERM include works by \citet{Kifer2012} and \citet{Rubenstein2012}.Th
\citet{Kifer2012} extended the framework in \citet{chaudhuri2011} to the approximate DP framework with a broader set of regularizers.


%----------------------------------------
\subsection{Our Work and Contribution}
To achieve DP guarantees when training OWL models using sensitive patient data, we formulate OWL as a weighted empirical risk minimization (wERM) problem and present a general procedure using output perturbation.
wERM is an extension of vanilla, unweighted ERM where individual weights are applied to the loss function evaluated at each training observation. 
We present a set of regularity conditions and prove that, under these conditions, our DP-wERM algorithm is differentially private.
We also discuss how to maximize the utility of our algorithm for a given privacy budget through hyperparameter tuning and feature selection.
To our knowledge, our procedure represents the first extension to the wERM case of the  unweighted ERM models developed and analyzed in \citet{Chaudhuri2009, chaudhuri2011, Kifer2012}, among others. 

We apply the DP-wERM algorithm in a simulation study and a real melatonin clinical trial to train OWL with privacy guarantees as a special case of wERM.
The results demonstrate that our algorithm has the potential to build useful OWL models for optimal ITR derivation and fairly accurate estimation of the empirical treatment value relative to their nonprivate counterparts while preserving privacy for study participants.



%-----------------------------------------
\section{Preliminaries}
\label{sec:preliminaries}

In this section, we formalize the notions of wERM, OWL, and DP.
By default, an unspecified norm is the $\ell_2$ norm (i.e., $\norm{\cdot} = \norm{\cdot}_2$) and boldfaced variables are vector-valued.

\subsection{Weighted Empirical Risk Minimization (wERM)}
\label{subsec:wERM}
For the unweighted ERM problem, let $(\mathbf{x}_i, y_i) \in (\mathcal{X},\mathcal{Y})$ represent the $p$ features or predictors in observation $i$ and the corresponding outcome, respectively, for $i=1,\ldots,n$.
Let $\ell: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$ denote a non-negative loss function over the label space $\mathcal{Y}$.
Our goal is to determine a predictor function $f$ that minimizes the loss function over the data. We assume $f$ is parameterized by a vector $\boldsymbol{\theta}$; it is also common to employ a regularizer $R$ that penalizes the complexity of  $f_{\boldsymbol{\theta}}$.
%We denote the regularizer  by $R: \mathcal{F} \rightarrow \mathbb{R}$, where $\mathcal{F}$ represents the set of considered predictor functions $f: \mathcal{X} \rightarrow \mathcal{Y}$. 
The solution to a regularized ERM is 
\begin{equation}\label{eqn:ERM}
\argmin_{f_{\boldsymbol{\theta}}\in\mathcal{F}}\left\{ \frac{1}{n}\sum_{i=1}^n \ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i) + \frac{\gamma}{n}R(\boldsymbol{\theta})\right\}= \argmin_{f_{\boldsymbol{\theta}}\in\mathcal{F}}\left\{ \frac{1}{n}\sum_{i=1}^n \ell_i(\boldsymbol{\theta}) + \frac{\gamma}{n}R(\boldsymbol{\theta})\right\}, 
\end{equation}
where $\gamma>0$ is a tunable regularization constant and $\mathcal{F}$ is an appropriate predictor function space that governs specific instances of ERM.
Linear regression, logistic regression, and SVM are all examples of ERM.
%For example, ERM reduces to linear regression if $\mathcal{F}$ is defined to be the set of all functions of the form $f_{\boldsymbol{\theta}, b}(\mathbf{x}) = \mathbf{x}^T\boldsymbol{\theta}$. %where $\boldsymbol{\theta}$ feature weights, and $b$ is a bias term.

In some applications, each of the $n$ loss function terms $\ell_i(\boldsymbol{\theta})$ in the ERM problem in Eqn \eqref{eqn:ERM} may be weighted differently, leading to the definition of weighted ERM (wERM).
\begin{defn}[weighted empirical risk minimization (wERM)]
\label{def:wERM}
Define $(\mathbf{x}_i, y_i)$, $\ell$, $R$, $\gamma$, and $\mathcal{F}$ as above. 
Let $w_i$ denote the weight associated with individual loss $\ell_i(\boldsymbol{\theta}) = \ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i)$.
A wERM problem is defined as
    \begin{equation}\label{eqn:wERM}
        \argmin_{f_{\boldsymbol{\theta}}\in\mathcal{F}} \left\{\frac{1}{n}\sum_{i=1}^n w_i\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i) + \frac{\gamma}{n}R(\boldsymbol{\theta})\right\}= \argmin_{f_{\boldsymbol{\theta}}\in\mathcal{F}}\left\{ \frac{1}{n}\sum_{i=1}^n w_i \ell_i(\boldsymbol{\theta}) + \frac{\gamma}{n}R(\boldsymbol{\theta})\right\}.
    \end{equation}
\end{defn}


%---------------------------------
\subsection{Outcome Weighted Learning (OWL)}
\label{subsec:OWL}
%{\color{red} Notation in this section is not well aligned with that given in the previous section 2.1. I guess, here we don't have that lable outcome $y_i$ given in the standard setting as treatment is part of $x_i$ not outcome; in factor, the outcome is benefit $B_i$. This point is not clearly stated. As a matter of factor, we have a model with treatment-attribute interactions to predict outcome $B$. In the previous section, $f(x_i)$ directly predicts label $y_i$, while here we don't have this direct prediction scheme; rather we use sign($f(x)$) to make prediction. Thus, the $f$ function means different things, I guess.  } 

Consider a dataset $\mathcal{S}=\{(\mathbf{x}_i, A_i, B_i)\,;\,i=1,\dots,n\}$ collected from a two-armed randomized clinical trial with a total of $n$ participants. 
A $p$-dimensional vector $\mathbf{x}=(x_1,\dots,x_p)^\top\in\mathcal{X}\subseteq\mathbb{R}^p$ contains features used for ITR derivation. 
The randomized treatment $A\in\mathcal{A}=\{-1,1\}$ (e.g., $A=1$ represents the new treatment and $A=-1$ represents placebo) is assigned to each participant with probability $P(A|\mathbf{x})$ at the beginning of the trial. 
A complete randomization implies $P(A=1|\mathbf{x})=P(A=-1|\mathbf{x})=0.5$.
Note that $A$ does not represent the underlying optimal treatment assignment, but rather the randomly assigned treatment for each subject in the trial.
This is a key difference between OWL and traditional ML classification problems given there are no labels in the training data in the former.
The goal of OWL is not to produce a treatment assignment classification function that identifies the treatment assigned to an individual in the randomized trial, but rather to leverage treatment benefit to produce a function that assigns individuals to their underlying optimal treatment assignment.
The trials also collect data from the study participants on benefit $B$ to evaluate the therapeutic effects of the treatments. 
It is assumed that the larger the value of $B$, the greater the treatment benefit. 

The central goal of ITR is to derive a treatment assignment function $T: \mathcal{X}\rightarrow \mathcal{A}$, which is a mapping from the feature space to the space of possible treatments. 
OWL proposed by \citet{Zhao2012} is a seminal work enabling the estimation of an optimal ITR $T^*$ that maximizes the expected clinical benefit $E[\frac{B}{P(A|\mathbf{x})}\mathbbm{1}(A=T(\mathbf{x}))]$, where $\mathbbm{1}(\cdot)$ is an indicator function and $P(A|\mathbf{x})$ is the propensity score. 
Changing the indicator function from equality to inequality, the maximization problem becomes a minimization one, resulting in $T^*\in \argmin_{T\in\mathcal{T}} E[\frac{B}{P(A|\mathbf{x})}\mathbbm{1}(A\neq T(\mathbf{x}))]$.
Here, term $\frac{B}{P(A|\mathbf{x})}\mathbbm{1}(A\neq T(\mathbf{x}))$ is a weighted classification error, so OWL pertains to a weighted classification problem. 
With the set of $i.i.d$ observations $\mathcal{S}=\{(\mathbf{x}_i, A_i, B_i)\,;\,i=1,\dots,n\}$, we can approximate the optimization problem under its empirical value $T^*\in \argmin_{T\in\mathcal{T}} \frac{1}{n} \sum_{i=1}^n \frac{B_i}{P(A_i|\mathbf{x}_i)}\mathbbm{1}(A_i\neq T(\mathbf{x}_i))$. 

The OWL optimization problem is equivalent to $f^*\in \argmin_{f\in\mathcal{F}} \frac{1}{n} \sum_{i=1}^n \frac{B_i}{P(A_i|\mathbf{x}_i)}\mathbbm{1}(A_i\neq \text{sign}(f(\mathbf{x}_i)))$ by letting $T(\mathbf{x})=1$ if $f(\mathbf{x})>0$ and $T(\mathbf{x})=-1$ otherwise for some predictor function $f$.
The loss function is a weighted sum of 0-1 loss that is neither convex nor continuous.
To solve the optimization problem, OWL uses a convex surrogate hinge loss $\text{max}(0,x)$ to replace the 0-1 loss. 
To penalize the complexity of the predictor function $f$ to avoid overfitting, OWL adds an $\ell_2$ penalty in the optimization problem, leading to the following 
optimization problem:
\begin{equation}\label{eqn:OWL}
    \argmin_{f_{\boldsymbol{\theta}}\in\mathcal{F}} \left\{\frac{1}{n}\sum_{i=1}^n \frac{B_i}{P(A_i|\mathbf{x}_i)}\text{max}(0,1-A_i f_{\boldsymbol{\theta}}(\mathbf{x}_i))+\frac{\gamma}{n}\norm{\boldsymbol{\theta}}\right\},
\end{equation}
where $f_{\boldsymbol{\theta}}(\cdot)$ is a predictor function specified with parameters $\boldsymbol{\theta}$ that characterize contributions of feature predictors.

It is straightforward to see that OWL as formulated in Eqn \eqref{eqn:OWL}  is a special case of wERM from Definition \ref{def:wERM}, where  $A$ corresponds to $y$ in Eqn \eqref{eqn:wERM} and
\begin{align}\label{eqn:wl}
w_i=\frac{B_i}{P(A_i|\mathbf{x}_i)} \mbox{ and }
\ell_i(\boldsymbol{\theta})=\text{max}(0,1-A_i f_{\boldsymbol{\theta}}(\mathbf{x}_i)).
\end{align}

%-------------------------------------------------
\subsection{Differential Privacy (DP)}
\label{subsec:DP}
%The overarching goal of this work is to produce an OWL algorithm that preserves the privacy of the individuals whose data are used to fit the model.
We use the  DP framework for achieving privacy guarantees when releasing results from OWL models.
The DP framework ensures that, regardless of the inclusion or exclusion of any single individual's data, the results of a DP-satisfying mechanism acting on a sensitive dataset are sufficiently similar.
We formalize DP by first defining the notion of neighboring datasets.

\begin{defn}[neighboring datasets]
\label{def:neighbor}
    \textnormal{\citep{Dwork2006}} Two datasets $D, \tilde{D} \in \mathcal{D}$ are considered \textnormal{neighboring datasets}, denoted by  $d(D, \tilde{D})=1$, if $D$ can be obtained from $\tilde{D}$ by modifying a single individual's data.
    %If the distance $d$ between two datasets is defined to be the number of records differing between them, $D$ and $\tilde{D}$ are neighboring datasets if and only if $d(D, \tilde{D})=1$.
\end{defn}
The modification referred to in Definition \ref{def:neighbor} can be deletion, addition, or alteration/replacement.

\begin{defn}[$(\epsilon, \delta)$-differential privacy]
\label{def:DP}
    \textnormal{\citep{Dwork2006b}} A randomized mechanism $\mathcal{M}$ satisfies \textnormal{$(\epsilon, \delta)$-differential privacy} if for all $S \subset \textnormal{Range}(\mathcal{M})$ and $d(D, \tilde{D})=1$,
    \begin{equation}
        P(\mathcal{M}(D) \in S) \le e^\epsilon P(\mathcal{M}(\tilde{D}) \in S) + \delta,
    \end{equation}
    where $\epsilon>0$, and $\delta\in[0,1)$ are privacy loss or privacy budget parameters.
    When $\delta=0$, it becomes \textnormal{$\epsilon$-DP}.
\end{defn}

The differing individual between the neighboring datasets is arbitrary, and therefore the DP guarantees hold for all individuals simultaneously.
As a result, each individual feels protected for his/her privacy since the resulting output will be similar whether his/her data is included or not.
The tunable parameter $\epsilon$ controls the similarity of the output distributions on the neighboring datasets.  Smaller $\epsilon$ implies more privacy.
Generally, $\epsilon\le1$ is considered to provide strong privacy guarantees, though it is not unusual to see larger values of $\epsilon$ used in applications.
It is common to interpret $\delta$ as the probability that $\epsilon$-DP fails.
The value $\delta$ is often set on the order of $o(1/\mbox{poly}(n))$ 

Apart from being a formally rigorous mathematical concept for privacy, the popularity of DP can also be ascribed to its appealing theoretical properties, which make it easier to implement in practical settings.
The post-processing theorem, for example, ensures that the output of a DP mechanism cannot be further manipulated to weaken the DP guarantees.
Additionally, in many situations, it is common to generate multiple statistics or outputs from the sensitive dataset.
%For example, one may seek to compute both a mean and a standard deviation from a dataset or output gradients from the optimization of a deep neural network.
The quantification of the overall privacy loss over the applications of multiple DP mechanisms to the same dataset is known as privacy loss composition and has been well-studied in the literature \citep{Dwork2006b, mcsherry2007mechanism, Dwork2010, abadi2016deep, Bun2016, Mironov2017, Dong2019}.
%There exist  several composition theorems \citep{Dwork2006, Dwork2006b, Dwork2010} that allow the quantification of  DP guarantees of the composition of several DP mechanisms. The recent extensions of the original DP definitions in Definition \ref{def:DP} are driven by the motivation to have tighter upper bounds on privacy loss over composition \citep{Bun2016, Mironov2017, Dong2019}.
%Our algorithm DP-OWL has only one output and no composition is needed.

A common way of achieving DP guarantees for information release is to add noise, appropriately calibrated to a given privacy loss or budget, directly to the output before release.
The scale of the added noise is usually related to the global sensitivity of the output.
Global sensitivity is originally defined using the $\ell_1$ norm by \citet{Dwork2006}.
In this paper, we use a more general definition.

\begin{defn}[$\ell_p$-global sensitivity]\citep{Liu2019}
    Let $\mathbf{s}$ be an output calculated from a dataset.
    The \textnormal{$\ell_p$-global sensitivity} of $\mathbf{s}$ is 
    \begin{equation}
        \Delta_{p,\mathbf{s}} = \max_{d(D,\tilde{D}) = 1}\norm{\mathbf{s}(D) - \mathbf{s}(\tilde{D})}_p, \mbox{  where $\norm{\cdot}_p$ is the $\ell_p$ norm.}
    \end{equation}
\end{defn}

The global sensitivity bounds the amount by which the output $\mathbf{s}$ can vary between two neighboring datasets.
%As an example, from a dataset of $n$ observations on a single attribute with values bounded within $[5, 10]$ globally, the $\ell_p$-global sensitivity of the sample mean is $(10 - 5)/n$, representing the largest possible change in the sample mean from modifying a single individual value.
%In this example, the $\ell_p$-global sensitivity is the same for all $p$, as the statistic is a scalar.
In general, the larger the global sensitivity of an output, the larger the noise scale of a DP mechanism is to achieve privacy guarantees at a pre-specified $(\epsilon, \delta)$ level.
For example, the Laplace mechanism is a commonly used mechanism that satisfies $\epsilon$-DP \citep{Dwork2006} by adding noise drawn from a Laplace distribution with mean 0 and scale $\Delta_{1, \mathbf{s}}/\epsilon$ to the output of a target function/computed statistic.


%-----------------------------------------------------
\section{Differentially Private wERM for Binary Classification}
\label{sec:methods}

We now present a general procedure for privacy-preserving wERM via output perturbation and prove that it satisfies $\epsilon$-DP.
We restrict our attention to the space of linear predictor functions $\mathcal{F} = \{f_{\boldsymbol{\theta}}\,:\,f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{x}^\intercal\boldsymbol{\theta}, \boldsymbol{\theta} \in \mathbb{R}^p\}$ in Definition \ref{def:wERM}.
A predicted label $\hat{y}$ for a given $\mathbf{x}$ can be obtained from the estimated $\hat{f}=f_{\hat{\boldsymbol{\theta}}}(\mathbf{x})= \mathbf{x}^\intercal\hat{\boldsymbol{\theta}}$, such as sign($\hat{f}$) if $y\in\{1,-1\}$, where $\hat{\boldsymbol{\theta}}$ is an optimal solution to the wERM problem.
%We discuss the modifications necessary to the OWL procedure in order for it to be a special case of the DP-satisfying wERM algorithm.This collectively results in proving an approximation to OWL that satisfies DP.


%-----------------------------------------------------
\subsection{Regularity Conditions}
\label{subsec:regularity_conditions}
To develop a privacy-preserving version for the wERM problem in Definition \ref{def:wERM}, some regularity conditions are required, as listed in Assumption \ref{assump}.
The conditions not related to the weights $\mathbf{w}$ are the same as the conditions in \citet{chaudhuri2011}.

\begin{assum}[Regularity conditions for DP-wERM]\label{assump}\hspace{1in}\vspace{-9pt}
\begin{enumerate}
\setlength{\itemsep}{-2pt}
        \item[{A1.}] $\norm{\mathbf{x}_i}\le 1$ and $w_i\in(0,W]$ for all $i$; 
        \item[{A2.}] The set of predictor functions are linear predictors $\mathcal{F} = \{f_{\boldsymbol{\theta}}\,:\,f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{x}^\intercal\boldsymbol{\theta}, \boldsymbol{\theta} \in \mathbb{R}^p\}$; 
        \item[{A3.}] Loss function $\ell$ is convex and everywhere first-order differentiable;
        \item[{A4.}] For any observed label $y\in\{-1,1\}$ and any predicted value $\hat{f} = f_{\hat{\boldsymbol{\theta}}}(\mathbf{x})$, 
        the loss function $\ell(\hat{f}, y)$ must be expressible as a function of the product $z=y\hat{f}$; in other words, $\ell(\hat{f}, y) = \tilde{\ell}(z)$ for some function $\tilde{\ell}$.
        Also, $|\tilde{\ell}^\prime(z)| \le 1\;\forall\; z$;
        \item[{A5.}] The regularizer $R$ is $1$-strongly 
        convex \footnote{Technically, $R$ could be $\Lambda$-strongly convex, but this distinction does not impact our results. We instead assume 1-strong convexity WLOG as this simplifies the proof of Theorem \ref{thm:wERM_sens}.} and everywhere differentiable. %{\color{red}, $R$ is in general defined on a functional space, so its convexity and differeniabilty are not the same as those given in the Calculus. Of course, since the class of linear predictor functions is considered in this paper, this $R$ function is in fact defined on $\theta$ in the Euclidean space. This is a point worth a comment. }
    \end{enumerate}
\end{assum}

Some of the above conditions (e.g., A1 $\norm{\mathbf{x}_i}\le1$) are satisfied by minor modifications to datasets.
%However, knowledge of upper and lower bounds on the possible feature values can be used to preprocess the dataset so that the condition is met. 
%If the upper and lower bounds can be reasonably well inferred or approximated without reference to the dataset itself (e.g., the human age can be reasonably assumed to be bounded between $[0, 125]$ years old; some studies are designed to run in adults, which leads to natural and publicly known bounds of $[18, 45]$), this preprocessing can be done without incurring additional privacy cost.
%If for some features the bounds are difficult to obtain without referencing the data, a portion of the privacy budget could be allocated to utilize the data to select the bounds.
While focusing on linear predictor functions without an explicit bias (intercept) term, an implicit bias term can still be used by augmenting the feature vector to include a constant term and the proposed framework can indirectly handle nonlinear predictor functions.
For example, in the SVM problem, the nonlinear kernels can be approximated via random projections \citep{Rahimi2007, Rahimi2008} in a similar manner to \citet{chaudhuri2011}.
The above conditions on $\ell$ and $R$ can be achieved by weighted versions of common classification methods such as logistic regression and SVM with only minor modifications (if any).
The loss function, for example, for logistic regression automatically satisfies conditions A3 and A4.
The hinge loss function commonly used for SVM can be easily smoothed to an approximation that also satisfies conditions A3 and A4. 
The popular $\ell_2$-norm regularizer used for ridge regression satisfies condition A5.


\subsection{DP-wERM Algorithm} \label{subsec:DP-wERM}
Under Assumption~\ref{assump}, we develop a general procedure, named DP-wERM, as presented in Algorithm \ref{alg:DP-wERM} to achieve $\epsilon$-DP when solving wERM problems.
DP-wERM is a new extension of the existing DP-ERM algorithm \citep{chaudhuri2011}.
The derivation of the global sensitivity $2W/\gamma$ on line 3 of Algorithm~\ref{alg:DP-wERM} is provided in Section \ref{sec:DP_guarantees}.
Noise vector $\mathbf{z}^*$, the way it is constructed in lines 4 to 6 of Algorithm~\ref{alg:DP-wERM}, is equivalent to generating a random sample from a distribution with density function $f(\mathbf{z})\propto \exp(-\frac{\epsilon}{\Delta}\norm{\mathbf{z}})$ \citep{Chaudhuri2009}. 
%a uniform sample from a $p$-dimensional hypersphere \citep{Muller1959}.
The output predictor function $f^*$ on line 8 satisfies $\epsilon$-DP through sanitizing the parameters $\hat{\boldsymbol{\theta}}$ with DP guarantees.

\begin{algorithm}[H]
\caption{Differentially private weighted empirical risk minimization (DP-wERM)}\label{alg:DP-wERM}
\begin{algorithmic}[1]
    \State \textbf{Input}: Privacy budget $\epsilon>0$; dataset $D = (\mathbf{x}_i, y_i, w_i) \in \mathbb{R}^p \times \{-1, 1\} \times \mathbb{R}$, for $i=1,\ldots, n$, of feature-label-weight triples with $\norm{\mathbf{x}_i}\le 1$ and weights $0 < w_i \le W$ for all $i$; regularization constant $\gamma>0$; linear predictor function $f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{x}^\intercal\boldsymbol{\theta}$; loss function $\ell$ and regularization function $R$ each satisfying regularity conditions A3-A5.
    \State \textbf{Output}: Privacy preserving predictor function $f^*$
    \State Set $\hat{\boldsymbol{\theta}} \gets \argmin_{\boldsymbol{\theta} \in \mathbb{R}^p} \frac{1}{n}\sum_{i=1}^n w_i\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i) + \frac{\gamma}{n}R(\boldsymbol{\theta})$ \Comment{wERM without DP}
    \State Set $\Delta \gets \frac{2W}{\gamma}$ \Comment{$\ell_2$-global sensitivity of $\hat{\boldsymbol{\theta}}$}
    \State Sample $\zeta$ from a Gamma distribution with shape $p$ and rate $\frac{\epsilon}{\Delta}$
    \State Sample $\mathbf{z}$ from the $p$-dimensional multivariate standard Gaussian distribution
    \State Set $\hat{\boldsymbol{\theta}}^* \gets \hat{\boldsymbol{\theta}} + \zeta\frac{\mathbf{z}}{\norm{\mathbf{z}}}$ \Comment{Randomized mechanism to achieve DP}
    \State \Return $f^*=f_{\hat{\boldsymbol{\theta}}^*}= \mathbf{x}^\intercal\boldsymbol{\theta}$ for  any $\mathbf{x}\in \mathbb{R}^p$ with $\norm{\mathbf{x}}\le 1$
\end{algorithmic}
\end{algorithm}


%In the setting of OWL, the loss function can be formulated as a weighted SVM problem. SVM is still an ERM problem, so is weighted SVM relative to wERM. To achieve DP guarantees for SVM, using the well-known ``kernel trick''  that transforms the input data to a higher dimensional space and finds a linear separating hyperplane in that space to classify data would leak  

%Nonlinear separators (in the original dataset dimension) can also be constructed by 

\subsection{Prepossessing via Dimensionality Deduction or Variable Selection}
\label{subsec:dimensionality_deduction}
The dimension of $\boldsymbol{\theta}$ in the predictor function increases with the number of predictors in $\mathbf{x}_i$. %(in the case of OWL, they basically have the same dimensionality).
The larger the number of predictors, the larger the amount of noise being injected into $\hat{\boldsymbol{\theta}}$ for a pre-specified privacy budget (line 7 of Algorithm \ref{alg:DP-wERM}), leading to noisier and less useful privacy-preserving $\hat{\boldsymbol{\theta}}^*$ and $f^*$.
It is generally good practice to perform variable selection or dimensionality reduction to limit the number of parameters to only those that are most helpful in making predictions through some sure screening methods, such as principal component analysis or other dimension reduction techniques.  

When releasing the learned function $f^*$ from Algorithm \ref{alg:DP-wERM}, the results from the pre-processing step  are automatically released together.
If there is prior knowledge and subject matter that informs the relevant predictors or if there exists a public dataset that can be used to select predictors or perform dimensionality reduction, this pre-processing step would not incur a privacy cost.
If no such knowledge or datasets exist, we can allocate a small amount of overall privacy budget to determine a subset via a privacy-preserving variable selection procedure \citep{Kifer2012, Thakurta2013, Steinke2017} or learn a small low-dimensional representation via privacy-preserving dimensionality reduction techniques \citep{Chaudhuri2013, Dwork2014PCA, Gonem2018} on the same dataset where the DP-wERM will be run.
Alternatively, we could divide the whole dataset into two non-overlapping portions, one used for privacy-preserving variable selection/dimensionality reduction and the other for DP-wERM at the same privacy budget.
One benefit of performing variable selection on the original features instead of dimensionality reduction is that it preserves the interpretability of the predictive model, which is not necessarily the case in the alternative approach.

%However, for a real-world dataset, it is possible that there is sufficient correlation between features that selecting few enough features to achieve good performance of DP-OWL is difficult. In such cases, it may be valuable to first use correlation-reducing variable transformation techniques such as principal component analysis (PCA) before the feature selection step. These techniques were not utilized for our simulations because the features are already independent as an assumption of the simulation settings, but they may be valuable in a real-world implementation.

\subsection{Hyperparameter Tuning}\label{subsec:hyperparameter_tuning}
The regularization constant $\gamma$ in the non-DP setting is often tuned using cross-validation on the same dataset used to learn the model.
With DP, caution must be used so as not to violate DP guarantees when tuning hyperparameters, as utilizing information from the sensitive dataset to select the hyperparameters results in privacy loss.
There are a few approaches to handle this.
These include allocating either a portion of the total privacy budget or a portion of the dataset to tune $\gamma$.
The former approach allows for techniques such as in \citet{chaudhuri2011} that uses the whole dataset for both hyperparameter tuning and model training while the latter allows for tuning at the same privacy budget as used for training the DP-wERM on the rest of the data.
If the training dataset is not large enough, these approaches are likely to return noisy ``optimal'' hyperparameters that may not have good utility.
For the second approach, if there exists a publicly available dataset that is similar to the sensitive dataset, it can be used to tune $\gamma$ without incurring additional privacy costs.
Our experiments show that the optimal value for $\gamma$ may vary as a function of the privacy budget $\epsilon$ and the training set size $n$, we provide a $(\epsilon, n)$-adaptive procedure in Algorithm \ref{alg:reg_const_selection} when tuning  $\gamma$ using a public dataset.
%Assume we have a public dataset of size $N$ and that we will hold out a minimum of $m$ out of the $N$ data points as a testing dataset for evaluation purposes. We first draw a random sample of the target sample size $n$ from the public dataset and train a DP-wERM model via Algorithm \ref{alg:DP-wERM} using the target privacy budget $\epsilon$. Then we evaluate the model on the remaining data points, which form the testing dataset. If $n>N-m$, meaning there will not be sufficient testing data points for evaluation, then $m$ evaluation data points are first randomly selected from the public dataset and held out, then a random sample of target sample size $n$ is bootstrapped from the remaining data points. This sample, train, evaluate procedure is repeated 1000 times for each of a set of considered regularization constant values, and the value producing the largest mean empirical value function for the given combination of $\epsilon$ and $n$ is chosen.
\begin{algorithm}[H]
\caption{$(n,\epsilon)$-adaptive regularization hyperparameter tuning on a public dataset}\label{alg:reg_const_selection}
\begin{algorithmic}[1]
\State \textbf{Input}: training data size $n$; privacy budget $\epsilon$;  public dataset $D$ of size $N$; minimum validation set size $m$ ($m<N$);   a set of candidate regularization constants $\{\gamma_1, \gamma_2, \ldots, \gamma_k\}$; number of repeats $r$; evaluation metric function $v(f^*, D)$ for evaluating fitted model $f^*$
\State \textbf{Output}: Optimal $(n,\epsilon)$-adaptive regularization hyper-parameter $\gamma$
\For{$i = 1, 2, \ldots, k$}
\For{$j = 1, 2, \ldots, r$}
\If{$n>N-m$}
    \State Set $D_{j,\textnormal{valid}}$ to be a random subset of $D$ of size $m$, sampled without replacement
    \State Set $D_{j,\textnormal{train}}$ to be a set of size $n$, sampled with replacement from $D\setminus D_{j,\textnormal{valid}}$
\Else
    \State Set $D_{j,\textnormal{train}}$ to a set of size $n$,  sampled  without replacement from $D$
    \State Set $D_{j,\textnormal{valid}}=D\setminus D_{j,\textnormal{train}}$
\EndIf
\State Run Algorithm \ref{alg:DP-wERM} given $\epsilon$, $D_{j,\textnormal{train}}$, and $\gamma_i$ to obtain $f^*_{j,i}$
\State Compute and store $v(f^*_{j,i}, D_{j,\textnormal{valid}})$
\EndFor
\State Set $\bar{v}_i=r^{-1}\sum_{j=1}^r v(f^*_{j,i}, D_{j,\textnormal{valid}})$
\EndFor
\State Set $i^* = \argmax_i \{\bar{v}_1, \bar{v}_2, \ldots, \bar{v}_k\}$ \Comment{WLOG, suppose better model has larger metric}
\State \Return $\gamma=\gamma_{i^*}$
\end{algorithmic}
\end{algorithm}

The evaluation metric function $v$ could be chosen to output the value of a given prediction scoring metric achieved by a trained predictor function on the validation set.
For example, given a predictor function $f^*$ trained on the training set $D_{\textnormal{train}}$ and a disjoint validation set $D_{\textnormal{valid}}$ containing true labels, the accuracy of the label predictions of $f^*$ on $D_{\textnormal{valid}}$ may be a good choice for $v$.
Other commonly used ML model evaluation metrics, such as the F1 score, may also be good candidates for $v$.


%--------------------------------------
\section{DP Guarantees}
\label{sec:DP_guarantees}
To prove that Algorithm \ref{alg:DP-wERM} satisfies $\epsilon$-DP, we first introduce the concept of $\Lambda$-strong convexity, which is a necessary condition for proving DP.

\begin{defn}[$\Lambda$-strong convexity]
    Let $g: \mathbb{R}^p \rightarrow \mathbb{R}$.
    If for all $\alpha \in (0, 1)$ and for all $\mathbf{x}, \mathbf{y}\in\mathbb{R}^p$,
    \begin{equation}
       \textstyle g(\alpha\mathbf{x} + (1-\alpha)\mathbf{y}) \le \alpha g(\mathbf{x}) + (1-\alpha)g(\mathbf{y}) - \frac{1}{2}\Lambda\alpha(1-\alpha)\norm{\mathbf{x}-\mathbf{y}}^2,
    \end{equation}
    then $g$ is said to be \textnormal{$\Lambda$-strongly convex}.
\end{defn}
\vspace{-9pt}It is straightforward to show that (i) if $g$ is $\Lambda$-strongly convex, then $ag$ is $a\Lambda$-strongly convex for any $a\in\mathbb{R}$ and (ii) if $f$ is convex and $g$ is $\Lambda$-strongly convex, then $f+g$ is $\Lambda$-strongly convex.

Lemma~\ref{lem:Chaudhuri_lemma} below that was originally presented in \citet{chaudhuri2011} is re-stated for the sake of self-containment; readers may refer to the proof in the original paper.
\begin{lem}
    \label{lem:Chaudhuri_lemma}
    \textnormal{\citep{chaudhuri2011}} Let $h_1: \mathbb{R}^p \rightarrow \mathbb{R}, h_2 : \mathbb{R}^p \rightarrow \mathbb{R}$ be everywhere differentiable functions.
    Assume $h_1(\boldsymbol{\theta})$ and $h_1(\boldsymbol{\theta}) + h_2(\boldsymbol{\theta})$ are $\Lambda$-strongly convex.
    Then if $\hat{\boldsymbol{\theta}}_1 = \argmin_{\boldsymbol{\theta}}h_1(\boldsymbol{\theta})$ and $\hat{\boldsymbol{\theta}}_2 = \argmin_{\boldsymbol{\theta}}h_1(\boldsymbol{\theta}) + h_2(\boldsymbol{\theta})$, we have
    \begin{equation}
        \textstyle\norm{\hat{\boldsymbol{\theta}}_1 - \hat{\boldsymbol{\theta}}_2} \le \frac{1}{\Lambda}\max_{\boldsymbol{\theta}}\norm{\nabla h_2(\boldsymbol{\theta})}.
    \end{equation}
\end{lem}

Given Assumption~\ref{assump} and Lemma \ref{lem:Chaudhuri_lemma}, we are now equipped to derive the $\ell_2$-global sensitivity of $\hat{\boldsymbol{\theta}}$ in the wERM optimization and to establish the main result of this work (Theorem \ref{thm:wERM_sens}) that Algorithm~\ref{alg:DP-wERM} is $\epsilon$-DP.
Compared to the establishment of the unweighted ERM DP results in \citet{chaudhuri2011}, the establishment of DP guarantees for wERM with output perturbation has the additional layer of complexity of how weight $w_i=B_i/P(A_i|\mathbf{x}_i)$ in Eqn \eqref{eqn:wl} affects the sensitivity of the solution of wERM.
We note that the numerator $B_i$ is always observed (treatment outcome) and does not depend on other individuals than individual $i$ himself/herself.
As for the denominator $P(A_i|\mathbf{x}_i)$, the probability that an individual $i$ receives treatment $A_i$, aka the propensity score, there are several scenarios.

First, $P(A_i|\mathbf{x}_i)$ is a known constant.
For example, when the dataset comes from a randomized clinical trial, $P(A_i|\mathbf{x}_i)$ depends on the allocation ratio between two treatments and is a  known constant per the study design.
In both the simulation and the case studies in this work, we deal with data from randomized clinical trials with a 1:1 allocation ratio, and $P(A_i|\mathbf{x}_i)$ is 0.5. 

Second, $P(A_i|\mathbf{x}_i)$ is unknown but estimated using data other than the sensitive dataset where the wERM problem is based on.
For example, $P(A_i|\mathbf{x}_i) = \exp(\mathbf{x}_i\boldsymbol{\beta})/(1+\exp(\mathbf{x}_i\boldsymbol{\beta}))$, where the unknown parameter $\boldsymbol{\beta}$ is estimated via a logistic regression model fitted on a different dataset than the  sensitive dataset.
Once $\hat{P}(A_i|\mathbf{x}_i)=\exp(\mathbf{x}_i\hat{\boldsymbol{\beta}})/(1+\exp(\mathbf{x}_i\hat{\boldsymbol{\beta}}))$ is obtained, it is plugged in Eqn (\ref{eqn:wl}) to obtain $w_i$ for the wERM problem. %or depends on the individual's own attributes $\mathbf{x}_i$ (e.g., prescription given by the person's doctor in EHR data).

In both scenarios above, $P(A_i|\mathbf{x}_i)$ or its estimate does not depend on the information of individuals $i'\ne i$ in the sensitive dataset $(\mathbf{x, A, B})$, so neither does $w_i$.
The third scenario is different; $P(A_i|\mathbf{x}_i)$ is unknown, so is $w_i$, and it is estimated using the sensitive data on which the wERM problem is based.
In this case, altering one individual affects the dataset and also the estimate of $P(A_i|\mathbf{x}_i)$ and thus $w_i$.
We make an additional assumption that 
\begin{equation} \label{eq:w}
|\tilde{w}_i-w_i| =o_p \left(n^{-1}\right),
\end{equation} 
where $\tilde{w}_i$ and $w_i$ are the $i$-th weights in two neighboring datasets $D$ and $D'$, respectively.
In other words, when the sample size $n \rightarrow \infty$, $\tilde{w}_i$ and $w_i$ become asymptotically equivalent and are minimally affected by alternating one individual in the dataset.
Also noted is that Eqn (\ref{eq:w}) holds automatically if the weighting scheme is not data-dependent (e.g., in scenarios 1 and 2) because  $|\tilde{w}_i-w_i| =0$ in this case. 
\begin{thm}[\textbf{main result}]\label{thm:wERM_sens}
Let $\mathcal{L}$ represent a wERM problem in Eqn \eqref{eqn:wERM} satisfying the regularity conditions A1-A5 with a weighting scheme satisfying Eqn (\ref{eq:w}).
Then,
%Assume the weight $w_i$ in Eqn(4) does not depend on the information from individuals $i'\ne i$ for $i'=1,\ldots,n$ in dataset $(\mathbf{x, y, w})$.
the $\ell_2$-global sensitivity of the parameters in the predictor function $f_{\boldsymbol{\theta}}$ of the wERM problem on data $(\mathbf{x, y, w})$ is $\Delta_{2, \boldsymbol{\theta}} = \frac{2W}{\gamma}$, and Algorithm~\ref{alg:DP-wERM} satisfies $\epsilon$-DP.
\end{thm}


\begin{proof}
Let $D$ and $\tilde{D}$ be neighboring datasets consisting of feature-label-weight triples. 
Without loss of generality, assume they differ on individual $n$.
That is, $D_i = (\mathbf{x}_i, y_i, w_i) = (\tilde{\mathbf{x}}_i, \tilde{y}_i, \tilde{w}_i) = \tilde{D}_i$ for $i =1,2,\ldots, n-1$ and $D_n = (\mathbf{x}_n, y_n, w_n) \ne (\tilde{\mathbf{x}}_n, \tilde{y}_n, \tilde{w}_n) = \tilde{D}_n$.
Define
\begin{align}
    g_1(\boldsymbol{\theta}) & = \textstyle\frac{1}{n}\sum_{i=1}^n w_i\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i) + \frac{\gamma}{n}R(\boldsymbol{\theta}), \notag\\
    \tilde{g}_1(\boldsymbol{\theta}) &= \textstyle\frac{1}{n}\sum_{i=1}^n \tilde{w}_i\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_i), \tilde{y}_i) + \frac{\gamma}{n}R(\boldsymbol{\theta}), \notag\\
    g_2(\boldsymbol{\theta})
    &= \textstyle\tilde{g}_1(\boldsymbol{\theta}) - g_1(\boldsymbol{\theta}) \notag\\
    & = \textstyle\frac{1}{n}\sum_{i=1}^{n-1}(\tilde{w}_i-w_i)\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i),y_i)+\frac{1}{n}\{\tilde{w}_n\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_n), \tilde{y}_n) - w_n\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_n), y_n)\}.\label{eq:g2}
    %\\& \textstyle \leq \frac{C}{n}\sum_{i=1}^{n-1}(\tilde{w}_i-w_i)+\frac{1}{n}\left(\tilde{w}_n\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_n), \tilde{y}_n) - w_n\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_n), y_n)\right)
\end{align}
%The last equation holds given that the weight $w_i$ in Eqn \eqref{eqn:wl} does not depend on the information from individuals $i'\ne i$ in dataset $(\mathbf{x, y, w})$.
%The last equation holds since on a compact set $\norm{\mathbf{x}}\leq 1$ and $y\in\{-1,1\}$, we have $|\ell(\cdot)|\leq C$ for $\mathbf{x}$ and $y$.
By regularity condition A3, $\ell$ is convex and everywhere first-order differentiable, and by regularity condition A5, $R$ is 1-strongly convex and everywhere differentiable.
Since $w_i>0$ and $\tilde{w}_i>0$, $g_1$ and $\tilde{g}_1 = g_1 + g_2$ are both $\frac{\lambda}{n}$-strongly convex and everywhere differentiable.
By regularity conditions A2 and A4, we can re-write $\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i)$ as $\tilde{\ell}(y_i\mathbf{x}_i^\intercal\boldsymbol{\theta})$, which implies
\begin{align}
\nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta}) &= \textstyle\frac{1}{n}\sum_{i=1}^{n-1}(\tilde{w}_i-w_i)\nabla_{\boldsymbol{\theta}}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i)+\textstyle\frac{1}{n}\{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_n), \tilde{y}_n) - w_n\nabla_{\boldsymbol{\theta}}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_n), y_n)\}\notag \\
&=\textstyle\frac{1}{n}\sum_{i=1}^{n-1}(\tilde{w}_i-w_i)\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_i \mathbf{x}_i^\intercal\boldsymbol{\theta})+\textstyle \frac{1}{n}\{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(\tilde{y}_n\tilde{\mathbf{x}}_n^\intercal\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_n\mathbf{x}_n^\intercal\boldsymbol{\theta})\}\notag\\
&=\textstyle\frac{1}{n}\sum_{i=1}^{n-1}(\tilde{w}_i-w_i)y_i\mathbf{x}^\intercal_i\tilde{\ell}^\prime(y_i\mathbf{x}^\intercal_i\boldsymbol{\theta})+\textstyle \frac{1}{n}\{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(\tilde{y}_n\tilde{\mathbf{x}}_n^\intercal\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_n\mathbf{x}_n^\intercal\boldsymbol{\theta})\}\notag\\
&\leq \textstyle \frac{1}{n}\sum_{i=1}^{n-1}(\tilde{w}_i-w_i)+\textstyle \frac{1}{n}\{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(\tilde{y}_n\tilde{\mathbf{x}}_n^\intercal\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_n\mathbf{x}_n^\intercal\boldsymbol{\theta})\}, \label{eq:grad-g2}
\end{align} 

\vspace{-12pt}where the last inequality holds because $|\tilde{\ell}^\prime(z)| \le 1$, and $|y_i|\le 1$ and $\norm{\mathbf{x}_i}\le 1$ for all $i$.

By regularity conditions A1 and A4, and Eqn (\ref{eq:w}), we show below that the $\ell_2$-norm of the second term in Eqn (\ref{eq:grad-g2}) is of order $O_p(1/n)$. That is,
\begin{align}\label{eqn:g2g}
&\norm{\textstyle \frac{1}{n}\{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(\tilde{y}_n\tilde{\mathbf{x}}_n^\intercal\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_n\mathbf{x}_n^\intercal\boldsymbol{\theta})\}} 
%le\frac{1}{n}\norm{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_n\mathbf{x}_n\boldsymbol{\theta})}
=\frac{1}{n}\norm{\tilde{w}_n \tilde{y}_n\tilde{\mathbf{x}}^\intercal_n\tilde{\ell}^\prime(\tilde{y}_n\tilde{\mathbf{x}}^\intercal_n\boldsymbol{\theta}) - w_n y_n\mathbf{x}_n\tilde{\ell}^\prime(y_n\mathbf{x}^\intercal_n\boldsymbol{\theta})}\notag\ \\
\textstyle\le &\frac{1}{n}\left(\norm{\tilde{w}_n \tilde{y}_n\tilde{\mathbf{x}}^\intercal_n\tilde{\ell}^\prime(\tilde{y}_n\tilde{\mathbf{x}}^\intercal_n\boldsymbol{\theta})} + \norm{w_n y_n\mathbf{x}^\intercal_n\tilde{\ell}^\prime(y_n\mathbf{x}^\intercal_n\boldsymbol{\theta})}\right)  \notag\\\
= &\textstyle\frac{1}{n}\left(|\tilde{w}_n| |\tilde{y}_n| \norm{\tilde{\mathbf{x}}_n} |\tilde{\ell}^\prime(\tilde{y}_n\tilde{\mathbf{x}}^\intercal_n\boldsymbol{\theta})| + |w_n| |y_n| \norm{\mathbf{x}^\intercal_n} |\tilde{\ell}^\prime(y_n\mathbf{x}^\intercal_n\boldsymbol{\theta})|\right)
\le \frac{1}{n}\left(\tilde{w}_n + w_n\right)\le \frac{2W}{n}.
\end{align}
Since the first term $\frac{1}{n}\sum_{i=1}^{n-1}(\tilde{w}_i-w_i)$ is Eqn (\ref{eq:grad-g2}) is of order $o_p(1/n)$. Thus, the first term is asymptotically ignorable in comparison to the second term.  It follows that for a large $n$, $\norm{\nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta})} \leq \frac{2W}{n}$. \footnote{The above proof is based on the formulation of $g_2$ in Eqn \eqref{eq:g2}, which assumes  that, when $w_i$ is unknown and estimated from the sensitive data, the estimation of  $P(A_i|\mathbf{x}_i)$ and $w_i$ takes place before the calculation of $g_2$ (that is, $w_i=B_i/\hat{P}(A_i|\mathbf{x}_i)$) and thus the assumption in Eqn \ref{eqn:wl} is required to proceed with the proof. If one is willing to formulate $g_2$ using the true unknown $P(A_i|\mathbf{x}_i)$ without plugging its estimate, as long as the unknown parameters in $P(A_i|\mathbf{x}_i)$ are the same across all $i$'s, then the first term in Eqn \eqref{eq:g2} can be dropped and $g_2$  can be simplified to  $\frac{1}{n}\left(\tilde{w}_n\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_n), \tilde{y}_n) - w_n\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_n), y_n)\right)$ and Eqn (\ref{eqn:g2g}) follows immediately. } 
%When $n\to \infty$, the propensity score difference due to one data perturbation is arguably ignorable. Thus, $\norm{\nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta})} \leq \frac{2W}{n}$ when $n\to \infty$. 

Next we define $\boldsymbol{\hat{\theta}}_1 = \mathcal{L}(D) = \argmin_{\boldsymbol{\theta}} g_1(\boldsymbol{\theta})$ and $\boldsymbol{\hat{\theta}}_2 = \mathcal{L}(\tilde{D}) = \argmin_{\boldsymbol{\theta}} \tilde{g}_1(\boldsymbol{\theta}) = \argmin_{\boldsymbol{\theta}}g_1(\boldsymbol{\theta}) + g_2(\boldsymbol{\theta})$.
By Lemma \ref{lem:Chaudhuri_lemma}, 
\begin{align}
\pushQED{\qed}
\Delta_{2, \boldsymbol{\theta}} &= \textstyle\max\limits_{\substack{D, \tilde{D} \\ d(D,\tilde{D}) = 1}}\norm{\mathcal{L}(D) - \mathcal{L}(\tilde{D})}_2 = \max\limits_{\substack{D, \tilde{D} \\ d(D,\tilde{D}) = 1}}\norm{\boldsymbol{\hat{\theta}}_1 - \boldsymbol{\hat{\theta}}_2}_2\notag \\
&=\textstyle \frac{n}{\gamma} \max\limits_{\boldsymbol{\theta}}\norm{\nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta})} \leq  \left(\frac{n}{\gamma}\right) \left(\frac{2W}{n}\right) = \frac{2W}{\gamma}.\label{eqn:GS}
\end{align}

With the global sensitivity, the proof on the satisfaction of $\epsilon$-DP in Algorithm 1 DP is identical to the proof of Theorem 6 in \citet{chaudhuri2011}, by replacing the sensitivity from that work with the one in Eqn \eqref{eqn:GS}.
\end{proof}


In summary, Theorem \ref{thm:wERM_sens} holds regardless of whether $w_i$ is independent of the sensitive dataset or not.
If $w_i$ is data-dependent and the assumption in Eqn (\ref{eq:w}) is questionable, the values of $P(A_i|\mathbf{x}_i)$, whether known or estimated, of $k\ge0$ more individuals are different in their $w_i$ values in addition to the individual whose record is different between the two neighboring datasets, then $g_2(\boldsymbol{\theta})$ in Eqn \eqref{eq:g2} would be the summation of the $k\in[1,n]$ differences between $\tilde{g}(\boldsymbol{\theta})$ and $g(\boldsymbol{\theta})$ evaluated at those $k+1$ individuals, Eqn \eqref{eqn:g2g} would become $2W/n+2kW'/n$, where $W'\le W$, representing the upper global bound in the change of the weights of the $k$ individuals other than the individual whose record gets altered, and the sensitivity in Eqn \eqref{eqn:GS} would become $2W/\gamma+2kW'/\gamma$.



\begin{comment}
\begin{thm}[\textbf{main result}]\label{thm:wERM_sens}
Let $\mathcal{A}$ represent a wERM problem in Eqn \eqref{eqn:wERM} satisfying the regularity conditions A1-A5.
Assume the weight $w_i$ in Eqn \eqref{eqn:wl} does not depend on the information from individuals $i'\ne i$ for $i'=1,\ldots,n$ in dataset $(\mathbf{x, y, w})$.
The $\ell_2$-global sensitivity of the parameters in the predictor function $f_{\boldsymbol{\theta}}$ of the wERM problem on data $(\mathbf{x, y, w})$ is $\Delta_{2, \boldsymbol{\theta}} = \frac{2W}{\gamma}$, and Algorithm \ref{alg:DP-wERM} satisfies $\epsilon$-DP.
\end{thm}


\begin{proof}
Without loss of generality, assume they differ on individual $n$.
That is, $D_i = (\mathbf{x}_i, y_i, w_i) = (\tilde{\mathbf{x}}_i, \tilde{y}_i, \tilde{w}_i) = \tilde{D}_i$ for $i =1,2,\ldots, n-1$ and $D_n = (\mathbf{x}_n, y_n, w_n) \ne (\tilde{\mathbf{x}}_n, \tilde{y}_n, \tilde{w}_n) = \tilde{D}_n$.
Define
\begin{align}
    g_1(\boldsymbol{\theta}) &= \textstyle\frac{1}{n}\sum_{i=1}^n w_i\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i) + \frac{\gamma}{n}R(\boldsymbol{\theta}), \\
    \tilde{g}_1(\boldsymbol{\theta}) &= \textstyle\frac{1}{n}\sum_{i=1}^n \tilde{w}_i\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_i), \tilde{y}_i) + \frac{\gamma}{n}R(\boldsymbol{\theta}), \\
    g_2(\boldsymbol{\theta}) &= \textstyle\tilde{g}_1(\boldsymbol{\theta}) - g_1(\boldsymbol{\theta}) = \frac{1}{n}\left(\tilde{w}_n\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_n), \tilde{y}_n) - w_n\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_n), y_n)\right).\label{eq:g2}
\end{align}
The last equation holds given that the weight $w_i$ in Eqn \eqref{eqn:wl} does not depend on the information from individuals $i'\ne i$ in dataset $(\mathbf{x, y, w})$.
By regularity condition 3, $\ell$ is convex and everywhere first-order differentiable, and by regularity condition 5, $R$ is 1-strongly convex and everywhere differentiable.
Since $w_i>0$ and $\tilde{w}_i>0$, $g_1$ and $\tilde{g}_1 = g_1 + g_2$ are both $\frac{\lambda}{n}$-strongly convex and everywhere differentiable.
By regularity conditions 2 and 4, we can re-write $\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i)$ as $\tilde{\ell}(y_i\mathbf{x}_i\boldsymbol{\theta})$, which implies
\begin{equation}
    \begin{aligned}
        \nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta}) &= \textstyle\frac{1}{n}\left(\tilde{w}_n\nabla_{\boldsymbol{\theta}}\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_n), \tilde{y}_n) - w_n\nabla_{\boldsymbol{\theta}}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_n), y_n)\right) \\
        &=\textstyle \frac{1}{n}\left(\tilde{w}_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_n\mathbf{x}_n\boldsymbol{\theta})\right).
    \end{aligned}
\end{equation}

By regularity conditions 1 and 4, 
\begin{align}\label{eqn:g2g}
\norm{\nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta})} &\textstyle\le \frac{1}{n}\norm{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\tilde{\ell}(y_n\mathbf{x}_n\boldsymbol{\theta})} \notag\\
&\textstyle= \frac{1}{n}\norm{\tilde{w}_n \tilde{y}_n\tilde{\mathbf{x}}_n\tilde{\ell}^\prime(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta}) - w_n y_n\mathbf{x}_n\tilde{\ell}^\prime(y_n\mathbf{x}_n\boldsymbol{\theta})}\notag\ \\
&\textstyle\le \frac{1}{n}\left(\norm{\tilde{w}_n \tilde{y}_n\tilde{\mathbf{x}}_n\tilde{\ell}^\prime(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta})} + \norm{w_n y_n\mathbf{x}_n\tilde{\ell}^\prime(y_n\mathbf{x}_n\boldsymbol{\theta})}\right)  \notag\\\
&\textstyle= \frac{1}{n}\left(|\tilde{w}_n| |\tilde{y}_n| \norm{\tilde{\mathbf{x}}_n} |\tilde{\ell}^\prime(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta})| + |w_n| |y_n| \norm{\mathbf{x}_n} |\tilde{\ell}^\prime(y_n\mathbf{x}_n\boldsymbol{\theta})|\right)\notag\ \\
&\le \frac{1}{n}\left(\tilde{w}_n + w_n\right)\le \frac{2W}{n}.
\end{align}

Finally, define $\boldsymbol{\hat{\theta}}_1 = \mathcal{A}(D) = \argmin_{\boldsymbol{\theta}} g_1(\boldsymbol{\theta})$ and $\boldsymbol{\hat{\theta}}_2 = \mathcal{A}(\tilde{D}) = \argmin_{\boldsymbol{\theta}} \tilde{g}_1(\boldsymbol{\theta}) = \argmin_{\boldsymbol{\theta}}g_1(\boldsymbol{\theta}) + g_2(\boldsymbol{\theta})$.
By Lemma \ref{lem:Chaudhuri_lemma},
\begin{align}
\pushQED{\qed}
\Delta_{2, \boldsymbol{\theta}} &= \textstyle\max\limits_{\substack{D, \tilde{D} \\ d(D,\tilde{D}) = 1}}\norm{\mathcal{A}(D) - \mathcal{A}(\tilde{D})}_2 = \max\limits_{\substack{D, \tilde{D} \\ d(D,\tilde{D}) = 1}}\norm{\boldsymbol{\hat{\theta}}_1 - \boldsymbol{\hat{\theta}}_2}_2\notag \\
&=\textstyle \frac{n}{\gamma} \max\limits_{\boldsymbol{\theta}}\norm{\nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta})} = \left(\frac{n}{\gamma}\right) \left(\frac{2W}{n}\right) = \frac{2W}{\gamma}.\label{eqn:GS}
\end{align}

With the global sensitivity, the proof on the satisfaction of $\epsilon$-DP in Algorithm \ref{alg:DP-wERM} DP is identical to the proof of Theorem 6 in \citet{chaudhuri2011}, by replacing the sensitivity from that work with the one in Eqn \eqref{eqn:GS}.
\end{proof}

\textbf{Remark}: The assumption that  weight $w_i=B_i/P(A_i|\mathbf{x}_i)$ in Eqn \eqref{eqn:wl} does not depend on the information of individuals $i'\ne i$ in dataset $(\mathbf{x, A, B})$ holds in general.
First, $B_i$ is always  observed (treatment outcome) and does not depend on other individuals other than individual $i$ himself/herself.
As for $P(A_i|\mathbf{x}_i)$, it is the probability that an individual $i$ receives treatment $A_i$, aka his/her propensity score.
Supposedly, it is a value either determined by study design, such as in randomized clinical trials, where $P(A_i|\mathbf{x}_i)$ is the allocation ratio between two treatments (e.g. 0.5) or depends on the individual's own attributes $\mathbf{x}_i$ (e.g., prescription given by the person's doctor in EHR data).
In both the simulation studies and the case study in this work, we deal with data from randomized clinical trials, and $P(A_i|\mathbf{x}_i)$ is given per study design.
If $P(A_i|\mathbf{x}_i)$ is unknown (so is $w_i$) and needs to be estimated, the estimation takes place after the formulation of Eqn \eqref{eq:g2}.
Furthermore, the actual values of $P(A_i|\mathbf{x}_i)$ and $w_i$ for each individual $i$, whether known or unknown (but estimated), do not matter in the sensitivity calculation as long as the global bounds of $w_i$, which are independent of the dataset locally, are given.
If the true values of $P(A_i|\mathbf{x}_i)$, whether known or unknown, of $k\ge0$ additional individuals are affected other than the individual whose record gets altered in the dataset, then $g_2(\boldsymbol{\theta})$ in Eqn \eqref{eq:g2} would be the summation of the $k\in[1,n]$ differences between $\tilde{g}(\boldsymbol{\theta})$ and $g(\boldsymbol{\theta})$ evaluated at those $k+1$ individuals, Eqn \eqref{eqn:g2g} would become $2W/n+2kW'/n$, where $W'\le W$, representing the upper global bound in the change of the weights of the $k$ individuals other than the individual whose record gets altered, and the sensitivity in Eqn \eqref{eqn:GS} would become $2W/\gamma+2kW'/\gamma$.

The estimation of $P(A_i|\mathbf{x}_i)$ can be estimated from a different dataset other than the dataset $(\mathbf{x, A, B})$ to which OWL is applied, then $k=1$. Lastly, if $P(A_i|\mathbf{x}_i)$ is estimated using the dataset $(\mathbf{x, A, B})$, such as through fitting a logistic regression, then its value would be affected even if the information of one individual in the dataset is altered, then $k=n$. Or one may choose to use a subset of $(\mathbf{x, A, B})$  to estimate $P(A_i|\mathbf{x}_i)$ instead of the whole dataset then k would take a value between $\in(0,n)$. On the other hand, one may argument
\end{comment}

%----------------------------------------------
\section{Differentially Private Outcome Weighted Learning (DP-OWL)} \label{subsec:DP-OWL}

As OWL is a special case of wERM,  Algorithm \ref{alg:DP-wERM}, along with the remarks on variable selection, dimensionality reduction, and hyperparameter tuning, can be applied to generate a privacy-preserving OWL model that satisfies $\epsilon$-DP. 

Specifically, the algorithm runs in the setting of OWL by operating on the primal wSVM problem with a linear kernel.
This contrasts with the typical method of solving the weighted SVM problem by optimizing the dual problem using the popular ``kernel trick'' to efficiently compute the solution for a nonlinear basis (e.g., Gaussian or radial kernels and polynomial kernels).
This is because the dual problem poses special problems for DP as releasing the learned SVM would also release the information of the predictions of individuals in the training data.
\citet{chaudhuri2011} propose a solution to the problem by approximating the use of nonlinear kernels using random projections  \citep{Rahimi2007,Rahimi2008}, which is the approach adopted by Algorithm \ref{alg:DP-wERM} (other approaches exist; for example, \citet{Rubenstein2012} solve the SVM dual problem directly with DP, bypassing the privacy concerns of the kernel trick).

In addition, the hinge loss function in Eqn \eqref{eqn:OWL} is not smooth; we approximate the hinge loss in OWL with the Huber loss \citep{Chapelle2007} so that the regularity conditions A3 and A4 in Assumption \ref{assump} for DP-wERM are satisfied. 
\begin{defn}[Huber loss]
    \textnormal{\citep{Chapelle2007}} For a given Huber loss parameter $h>0$,  the \textnormal{Huber loss} $\ell_{\textnormal{Huber}}: \mathbb{R} \rightarrow \mathbb{R}$ can be defined as
    \begin{equation}\label{eqn:huber}
        \ell_{\textnormal{Huber}}(z) = \begin{cases} 
        0, & \textnormal{if } z > 1+h \\
        \frac{1}{4h}(1+h-z)^2, & \textnormal{if } |1 - z| \le h \\
        1 - z, & \textnormal{if } z < 1 - h \\
        \end{cases},
    \end{equation}
\end{defn}
In the context of wERM, $z$ in Eqn \eqref{eqn:huber} is replaced by $y\hat{f}$.
It is straightforward to show that the Huber loss is convex and everywhere first-order differentiable.
Additionally, the absolute value of the derivative $|\ell^\prime_{\textnormal{Huber}}(z)|$ is non-negative and maximized at 1; that is, $|\ell^\prime_{\textnormal{Huber}}(z)| \le 1$ for all $z$. 
In the experiments in Section \ref{sec:sim_studies} and \ref{sec:case_study}, we set the Huber loss parameter $h=0.5$, which is considered a ``typical value'' \citep{Chapelle2007}.

Corollary \ref{cor:DP-OWL} states Algorithm \ref{alg:DP-wERM} satisfies $\epsilon$-DP when it is run in the case of weighted OWL with the Huber loss.
The corollary is a direct extension from Theorem \ref{thm:wERM_sens} as all regularity conditions in Assumption \ref{assump} are satisfied. 
\begin{cor}[\textbf{DP guarantees of OWL via the DP-wERM algorithm}]\label{cor:DP-OWL}
Consider the approximate OWL problem
\begin{equation}
    \argmin_{f_{\boldsymbol{\theta}}\in\mathcal{F}} \frac{1}{n} \sum_{i=1}^n\frac{B_i}{P(A_i|\mathbf{x}_i)} \ell_{\textnormal{Huber}}(A_if_{\boldsymbol{\theta}}(\mathbf{x}_i)) + \frac{\gamma}{n}\norm{\boldsymbol{\theta}},
\end{equation}
where $\norm{\mathbf{x}_i}\!\le\!1$, $\mathcal{F}\!= \!\{f_{\boldsymbol{\theta}}:f_{\boldsymbol{\theta}}(\mathbf{x}) \!=\!\mathbf{x}^\intercal\boldsymbol{\theta}, \boldsymbol{\theta} \in \mathbb{R}^p\}$, and  $\frac{B_i}{P(A_i|\mathbf{x}_i)}=w_i\in(0,W]$ satisfies the weighting scheme in Eqn  \eqref{eq:w}.
Then Algorithm \ref{alg:DP-wERM} run to solve the OWL problem satisfies $\epsilon$-DP.
\end{cor}





%-------------------------------------------------------------------
\section{Simulation Study}\label{sec:sim_studies}
We conducted a simulation study to examine the prediction performance of the DP-OWL algorithm with privacy budgets $\epsilon=0.1, 0.5, 1, 2, 5$ and training dataset size $n = 200, 500, 800, 1000$.
As a baseline for comparison, we also run OWL without DP, which can be thought of as having a privacy budget of $\epsilon=\infty$.
The prediction accuracy is measured by both the correct underlying optimal treatment assignment and the empirical treatment value that estimates the expected clinical benefit from the treatment assignment.
We first introduce the simulation setting and then discuss the pre-processing step of variable selection and privacy-preserving hyperparameter tuning.


%-----------------------------------------------------
\subsection{Simulation Setting}
\label{subsec:sim_settings}

Our simulation setting replicates a typical phase II clinical trial used to estimate treatment effects with a few hundred to a thousand individuals.
The participants are randomly assigned to one of the possible treatments and an outcome for each participant is measured, along with
baseline characteristics or predictors $\mathbf{x}$.
%Medical studies that involve human subjects often collect sensitive individual information. As a result, legal and ethical concerns regarding individual privacy often restrict access to the data. These simulations seek to demonstrate how DP can be harnessed to provide an extra layer of privacy for this sensitive data.
%The following settings define the distributions from which the simulation values are drawn.
We draw each predictor $\mathbf{x}_i = (x_{i,1}, x_{i,2}, \ldots, x_{i,10})$ from a uniform distribution (i.e., $x_{i,j} \sim U[0, 1]$ independently for all $i=1,\ldots,n$ and $j=1,\ldots,10$). 
We examine two treatments $A\in\{-1,1\}$ and each individual is randomly assigned with a probability of 0.5 to either treatment.
Therefore, the propensity $P(A_i=1|\mathbf{x}_i) = P(A_i=-1|\mathbf{x}_i) = 0.5$.
We assume the underlying optimal treatment is the sign of the function $f(\mathbf{x}_i) = 1 + x_{i, 1} + x_{i, 2} - 1.8x_{i, 3} - 2.2x_{i, 4}$.
Note $\E[f(\mathbf{x}_i)] = 0$, so approximately half of the individuals in a simulated dataset receive optimal benefit from each of the two treatments.
Treatment benefits (the outcome) $B_i$ are drawn from  $\mathcal{N}(\mu_i, \sigma^2)$, with $\mu_i = 0.01 + 0.02x_{i, 4} + 3A_if(\mathbf{x}_i)$ and $\sigma=0.5$.
If any simulated $B_i<0$, then all benefits are shifted to be $B_i + |\min\{B_i\}| + 0.001\;\forall\; i=1,\ldots,n$.
This shift is done to ensure that $w_i = \frac{B_i}{P(A_i|\mathbf{x}_i)}> 0$ as required.
Based on this definition, an individual who receives the optimal treatment defined by $f$ is likely to receive a larger treatment benefit, while those assigned to the non-optimal treatment are likely to receive a smaller benefit.

Models trained via DP-OWL are evaluated in two ways.
First, since the true optimal treatment is known given this is a simulation study, we can evaluate the accuracy of the optimal treatment assignment via the learned DP-OWL models.
Second, we calculate the empirical treatment value, an estimate of the expected clinical benefit $E[\frac{B}{P(A|\mathbf{x})}\mathbbm{1}(A=T(\mathbf{x}))]$ and defined as
\begin{equation}\label{eqn:empirical_treatment_value}
    V(\hat{f}) = \frac{\sum_{i=1}^n\frac{\mathbbm{1}(A_i=\hat{f}(\mathbf{x}_i))}{P(A_i|\mathbf{x}_i)}B_i}{\sum_{i=1}^n \frac{\mathbbm{1}(A_i=\hat{f}(\mathbf{x}_i))}{P(A_i|\mathbf{x}_i)}},
\end{equation}
where $\hat{f}$ is the estimated predictor function in the  DP-OWL model and $\mathbbm{1}$ is the indicator function.
Since $P(A_i|\mathbf{x}_i)=0.5$ for all $i=1,\ldots,n$ in this simulation, $V(\hat{f})= \frac{\sum_{i=1}^n\mathbbm{1}(A_i=\hat{f}(\mathbf{x}_i))B_i}{\sum_{i=1}^n \mathbbm{1}(A_i=\hat{f}(\mathbf{x}_i))}$.


%-------------------------------------------
\subsection{Variable Selection and Hyperparameter Tuning}
\label{subsec:feature_selection}
To reduce the number of predictors that go into the DP-wERM optimization, we perform variable selection, assuming that there exists a publicly available dataset we can use for that purpose.
This public dataset contains $(\mathbf{x}_i, A_i, B_i)$ for $i=1,\ldots,1000$, simulated in the same way as outlined in Section \ref{subsec:sim_settings}.
We run 10-fold cross-validation to fit a LASSO-regularized weighted logistic regression model for variable selection, where $A_i$ is the outcome variable, and the weights are $w_i = \frac{B_i}{P(A_i|\mathbf{x}_i)}$.
The results suggest that the first four predictors $X_1$ to $X_4$ are relevant, matching our expectations based on the assumed underlying optimal treatment function from the simulation settings.
%The set $\{X_1, X_2, X_3, X_4\}$ is used as the input to the subsequent OWL model.
To meet the regularity conditions for DP-OWL  that $\mathbf{x}_i$ be subject to $\norm{\mathbf{x}_i}\le1$, we preprocess each $\mathbf{x}_i$ for $i=1,\ldots,n$ by dividing it by $\sqrt{5}$ (4 selected features and an additional intercept or bias term) to obtain $\tilde{\mathbf{x}}_i = \mathbf{x}_i/\sqrt{5}$, which is used in the DP-OWL algorithm.
%The implicit assumption here is that it is public knowledge each $x_{i,j}$ is bounded between $[0,1]$ and the number of features including the implicit bias term is $\le5$.
%Since this is done without reference to the sensitive dataset itself, we can scale the feature vectors as required without utilizing a portion of the privacy budget.

The (global) upper bound $W$ for the weights $w_i=\frac{B_i}{P(A_i|\mathbf{x}_i)}$ is also a hyperparameter that needs to be specified.
A larger $W$ corresponds to a larger scale for the additive noise in Algorithm \ref{alg:DP-wERM}, so it is ideal to select $W$ to be as close as possible to the largest $\max_i\{w_i\}$ calculated from the sensitive dataset, but it cannot be $\max_i\{w_i\}$ as using it directly would incur privacy loss unless a portion of the privacy budget is spent to sanitize it, such as by using the PrivateQuantile procedure \citep{smith2011privacy}.
In this simulated study,
the denominator of $w_i$ is the probability of being assigned to each treatment, which is a constant $0.5$.
Thus, we only need to bound $B_i$ which may have a natural bound.
In this simulation, we use the public dataset, which suggests that most $B_i$ values are captured within the range $(0, 15)$ (values $>15$ are clipped to 15), leading to $W=15/0.5=30$.

To tune the regularization constant $\gamma$, we use Algorithm \ref{alg:reg_const_selection} with the DP-OWL inputs on the same public dataset as used for variable selection so that there is no additional privacy loss and the entire privacy budget can be reserved for the DP-OWL model training.
We do this for each combination of privacy budget $\epsilon$ and sample size $n$ used in the simulation study, with a minimum validation set size of $m=500$, and the empirical treatment value from Eqn \eqref{eqn:empirical_treatment_value} as our evaluation metric, with $r=1000$ repeats.
$X_1$ to $X_4$ from the variable selection procedure are used in training the models in Algorithm \ref{alg:reg_const_selection}.


%-----------------------------------
\subsection{Results}\label{subec:results}
We ran the DP-OWL algorithm on 200 simulated datasets for each $n$ and $\epsilon$ combination and obtained the average optimal assignment accuracy rates and average empirical treatment values on a testing dataset of size $n=5,000$ independent from the training data in each repeat, together with the  95\% confidence intervals (CIs) for the means.
Simulations were performed using the DPpack R package \citep{DPpack}.

The results are presented in Figure \ref{fig:sim_results}. %(more detailed information is presented in a table in the supplementary materials).  
As expected, the accuracy rate and empirical value improve as $n$ or $\epsilon$ increases, approaching the performance for the non-private case, where the accuracy rate is 90\% to 95\%, depending on $n$, and the empirical value is around 10.5.
The DP-OWL algorithm can achieve an 80\% optimal treatment assignment accuracy rate or larger around $n=1000$ at $\epsilon=2$.
If $\epsilon$ increases to 5, 80\% or larger accuracy rates can be obtained for $n$ as small as 500.
As $n$ or $\epsilon$ gets smaller, the assignment accuracy rate goes down to 50\% to 60\%.
Similar trends are observed for the empirical value prediction.
At $\epsilon = 5$, the predicted values are between 10 and 10.5 for $n\ge500$, around $10$ for $n\ge800$ at $\epsilon=2$ and decrease to the range of 8 to 9 for smaller $n$ or $\epsilon$.

% Figure environment removed

To examine how much privacy loss and how big a training data size is needed to reach the level of performance without DP, we examine more and larger values of $\epsilon$ and $n$, and the results are presented in the supplementary materials.
For sample sizes $n>500$, we begin to see performance near non-DP levels in our simulation studies around $\epsilon=50$.
For sample sizes smaller than this, it may take until $\epsilon=100$ or more to see performance near non-DP levels.

We also made several modifications to the DP-OWL algorithm in an attempt to improve its performance at smaller $n$ or $\epsilon$.
These attempts aimed to reduce the noise scale by scaling or discretizing the benefit values and increasing the regularization constant from the tuned value, and applying privacy amplification via subsampling \citep{balle2018privacy}.
None of these attempts resulted in improved performance. 


%--------------------------------------
\section{Case Study}
\label{sec:case_study}
We applied the proposed DP-OWL algorithm to a real clinical trial that was conducted to study the effectiveness of melatonin on sleep and neurocognitive impairment in childhood cancer survivors \citep{lubas2022randomized}. 
The main objective of this analysis is to derive an ITR that guides childhood cancer survivors to take melatonin to maximize its effect on neurocognitive impairment while protecting the sensitive personal data used to derive the ITR, including benefit scores, demographic information, and medical histories. 

%Long-term survivors of childhood cancer are at risk of treatment-related neurocognitive and sleep problems \citep{spiegler2004change}. 
%Although these late effects may pose a significant impact on the psychological and social functioning of these childhood cancer survivors, there are few effective interventions for these problems. 
Melatonin is an endogenously secreted hormone that has a sleep-promoting effect and has been associated with cognitive performance \cite{zhdanova1997melatonin, cardinali2012therapeutic, furio2007possible}. A clinical trial was conducted at St. Jude Children's Research Hospital to study the effects of exogenous melatonin on alleviating problems related to sleep and neurocognitive impairment in adult survivors of childhood cancer \citep{lubas2022randomized}. 
Survivors were recruited from the St. Jude Lifetime Cohort Study (SJLIFE) \citep{howell2021cohort}. 
Based on the baseline evaluation of the neurocognitive and sleep impairment status, participants were classified into three strata: 1) neurocognitive impairment without sleep impairment; 2) both neurocognitive and sleep impairment; and 3) sleep impairment without neurocognitive impairment. 
In this work, we focused only on the effect of melatonin on improving the neurocognitive performance of the participants. 
Therefore, only participants in strata 1 and 2 were considered for ITR derivation. 
At the beginning of the trial, participants were randomized to receive 3 mg of time-released melatonin or placebo. 
Individual characteristics collected at baseline that are used in our ITR analysis include age at diagnosis (in years), age at enrollment (in years), sex (female or male), race (White or other), and diagnosis (leukemia or non-leukemia). 
%To be clear, these parameters were not set for intervention study enrollment.
Neurocognitive evaluations of the participants were completed at baseline and post-intervention (month 6). 
Previous analysis of the data from this trial found that among survivors with neurocognitive impairment, a larger proportion randomized to melatonin versus placebo showed a treatment response for nonverbal reasoning. 
Therefore, in this work, we took the assessment of nonverbal reasoning as our main endpoint of interest. 
Benefit values were calculated as the difference in scores on the endpoint measured at month 6 and baseline. 

A total of 246 participants were eventually included in our analysis, with 120 taking melatonin and 126 taking placebo. 
The participants were all adult survivors of childhood cancer, making the age at enrollment no younger than 18 years and no older than 60 years. Therefore, the lower and upper bounds of age at enrollment were set as 18 and 60, respectively.
Patients admitted to St. Jude are pediatric oncology patients, typically diagnosed at ages younger than 20 years, in general.
Thus, the lower and upper bounds of age at diagnosis were set as 0 and 20, respectively.
The other three predictors, sex, race, and diagnosis, were binary and coded as 0 or 1.
Scores on nonverbal reasoning were converted to age-adjusted Z-scores $B$, which followed a $N(0,1)$ distribution.
We obtained the shifted benefit $B'$ for each subject $i$ as in $B'_i=B_i + |\min\{B_i\}| + 0.001$ as the final benefit measure.

When applying the proposed DP-OWL method to derive a privacy-preserving ITR, we set the upper bound for the weight $w_i=\frac{B'_i}{P(A_i|\mathbf{x}_i)}$ at $W=4/0.5=8$, where $4$ is at two standard deviations from the mean of $B'$ and the propensity score $P(A_i|\mathbf{x}_i)=0.5$. 
We examined several privacy budgets $\epsilon=\{0.1, 0.5, 1, 2, 5\}$.
Since there was no public dataset available for hyperparameter tuning, we varied the tuning parameter $\gamma=\{30, 50, 90, 100, 150\}$ to examine the sensitivity of the derived ITR with DP guarantees.
We also run a total of 200 replicates combination of $\epsilon$ and $\gamma$ to examine the stability of the privacy-preserving output from DP-OWL for each 

Table~\ref{tab:etv} presents the means and standard deviations (SD) of the privacy-preserving empirical treatment values for the benefit of interest over 200 replicates in each  $(\epsilon, \gamma)$ scenario.
The ITR derived by the non-private OWL yields an empirical value of 2.101 (SD 0.104), while the ITRs estimated by DP-OWL under different values of $\epsilon$ and $\gamma$ generate similar empirical values, all around 2.09, suggesting that the utility of the privacy-preserving ITRs via DP-OWL is well preserved in terms of the estimated clinical benefit.
In addition to the empirical value, we also summarized the number of participants allocated to melatonin or placebo based on the privacy-preserving ITRs derived via DP-OWL. 
For each scenario, we calculated the probability of assigning a certain participant to melatonin over the 200 replicates and allocated that participant to melatonin if this probability exceeded 50\%. 
The ``Allocation'' column in Table~\ref{tab:etv} lists the allocation information. 
Chi-squared tests were conducted to test whether the allocations based on ITRs derived by DP-OWL were associated with the allocations based on the ITR derived by OWL.
The $p$-values from the test, listed in Table~\ref{tab:etv}, are all very close to 0, indicating that the privacy-preserving ITRs derived by DP-OWL gave similar patient allocations as to those obtained by OWL without DP. 
\begin{table}[!htb]
\centering
\caption{Mean and standard deviation (SD) of the privacy-preserving empirical treatment values  for the endpoint of interest and treatment allocation obtained by DP-OWL in the melatonin clinical trial. \label{tab:etv}}
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{cccccc}
\hline\hline
$\epsilon$ & $\gamma$ & Mean  & SD & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Allocation\\ (melatonin:placebo)\end{tabular}} & $p$-value$^\dagger$  \\ \hline
0.1 & 30    & 2.097 & 0.094  & 164:82 & $7.65\times 10^{-10}$ \\
0.1 & 50    & 2.093 & 0.091 & 156:90 & $1.37\times 10^{-6}$ \\
0.1 & 90    & 2.093 & 0.091 & 156:90 & $1.37\times 10^{-6}$ \\
0.1 & 100   & 2.093 & 0.091 & 156:90 & $1.37\times 10^{-6}$ \\
0.1 & 150   & 2.093 & 0.091 & 156:90 & $1.37\times 10^{-6}$ \\
\hline
0.5 & 30    & 2.088 & 0.094 & 174:72 & $7.58\times10^{-15}$ \\
0.5 & 50    & 2.094 & 0.101 & 172:74 & $4.96\times10^{-13}$ \\
0.5 & 90    & 2.094 & 0.101 & 172:74 & $4.96\times10^{-13}$ \\
0.5 & 100   & 2.094 & 0.101 & 172:74 & $4.96\times10^{-13}$ \\
0.5 & 150   & 2.094 & 0.101 & 172:74 & $4.96\times10^{-13}$ \\
\hline
1   & 30    & 2.086 & 0.115 & 182:64 & $<2.2\times10^{-16}$ \\
1   & 50    & 2.095 & 0.111 & 187:59 & $<2.2\times10^{-16}$ \\
1   & 90    & 2.095 & 0.110 & 187:59 & $<2.2\times10^{-16}$ \\
1   & 100   & 2.096 & 0.110 & 187:59 & $<2.2\times10^{-16}$ \\
1   & 150   & 2.095 & 0.110 & 187:59 & $<2.2\times10^{-16}$ \\
\hline
2   & 30    & 2.098 & 0.111 & 188:58 & $<2.2\times10^{-16}$ \\
2   & 50    & 2.098 & 0.110 & 188:58 & $<2.2\times10^{-16}$ \\
2   & 90    & 2.098 & 0.110 & 188:58 & $<2.2\times10^{-16}$ \\
2   & 100   & 2.098 & 0.110 & 188:58 & $<2.2\times10^{-16}$ \\
2   & 150   & 2.098 & 0.110 & 188:58 & $<2.2\times10^{-16}$ \\
\hline
5   & 30    & 2.099  & 0.103 & 194:52 & $<2.2\times10^{-16}$ \\
5   & 50    & 2.099 & 0.103 & 194:52 & $<2.2\times10^{-16}$ \\
5   & 90    & 2.099 & 0.103 & 194:52 & $<2.2\times10^{-16}$ \\
5   & 100   & 2.099 & 0.103 & 194:52 & $<2.2\times10^{-16}$ \\
5   & 150   & 2.099 & 0.103 & 194:52 & $<2.2\times10^{-16}$ \\
\hline
\multicolumn{6}{l}{The mean of non-private empirical treatment value is 2.101 (SD 0.104) with a }\\  
\multicolumn{6}{l}{197:49  melatonin:placebo allocation for all 5 examined $\gamma$ values.}\\  
\multicolumn{6}{l}{$^\dagger$ from the chi-squared independence test of the 2 allocation ratios }\\
\multicolumn{6}{l}{obtained by DP-OWL and OWL (without DP), respectively.}\\
\hline\hline  
\end{tabular}}
\end{table}

%--------------------------------------
\section{Discussion}
\label{sec:discussion}
We have presented and proved a general algorithm for achieving $\epsilon$-DP for wERM.
This work represents the first such extension of a previous work by \citet{chaudhuri2011} to allow for weighted loss functions, where the weights relate to the sensitive dataset but may or may not be related to the outcome.
We applied the general DP-wERM algorithm to OWL often used in medical studies.
Our simulation and the case study results demonstrate that with a reasonably small privacy budget and a reasonably large sample size, satisfactory optimal treatment prediction and empirical treatment value can be achieved while simultaneously providing privacy guarantees for individuals in the study dataset.  In both the simulation and case studies, we also examined  $\epsilon$ as large as 5, which leads to satisfactory results. %, though strong privacy protection with DP is generally thought to be achieved with $\epsilon\le 1$, 
It is not uncommon to use larger values of $\epsilon$ in practice.
For example, the US Census Bureau used $\epsilon=19.61$ to release statistical information from the 2020 census \citep{USCensus}.

The DP-OWL algorithm struggles with small sample sizes or privacy budgets.
An intuitive explanation for why this problem is as follows.
DP provides privacy protection by ensuring the outputs of a DP mechanism are similar between two neighboring datasets, meaning that a single individual cannot influence the output ``too much''.
OWL, in contrast, aims to fit a model for individualized treatment assignment by weighting the input by how much it should influence the fitted model output.
Thus, in some sense, DP and OWL have competing goals that must be balanced to adequately satisfy the demands of each.
%\citet{chaudhuri2011} presents two algorithms for achieving DP for vanilla, unweighted ERM problems.One is the perturbation of the output coefficients from the fitted model given the data and an assumed model, which is the method utilized in this paper.The other method is to perturb the objective function itself with DP guarantees, the output from which is also privacy-preserving per the immunity to post-processing property of DP.
Nevertheless, the problem warrants further exploration to improve performance.
\citet{chaudhuri2011} showed that the objective function perturbation method for unweighted ERM problems generally provided higher utility than the output perturbation method at the same privacy budget.
Thus, it may be worth exploring how to adapt the objective perturbation algorithm to the wERM case and analyzing its performance. 
Another potential  direction is to explore relaxing the DP guarantees from $\epsilon$-DP to $(\epsilon, \delta)$-DP or other types of DP relaxations such as Renyi DP \citep{Mironov2017}, zero-concentrated DP \citep{Bun2016}, or Gaussian DP \citep{Dong2019}, to examine if and how much the performance of DP-OWL improves.


It will be meaningful to explore algorithms that achieve $\epsilon$-DP for other ITR derivation frameworks, such as RWL and M-learning, and for other applications of ITRs such as dose-finding clinical trials, multi-treatment settings, and scenarios with multi-stage decision-making, as introduced in Section~\ref{subsec:background}.
As a preliminary study, we derive the $\ell_2$ global sensitivity of the output from M-learning (see the supplementary materials).  %various methods have been proposed as extensions of OWL to derive optimal ITRs. 
%Estimated predictor functions derived by OWL may be affected by a simple shift of the outcome and tends to keep treatment assignments that subjects actually received.  To address these challenges, residual weighted learning (RWL) \citep{Zhou2017} was proposed to derive ITRs by weighting observations using residuals of the outcome from a regression fit and solve the optimization problem using a difference of convex (d.c.) algorithm. 
%Matching-learning (M-learning) \citep{Wu2020} further improves the OWL-based framework by utilizing matching to balance subjects in different treatments instead of inverse probability weighting (IPW), making the algorithm more applicable for analysis of observational studies, which typically have much larger sample sizes than clinical trials. 
It would also be of interest to explore the performance of the DP-OWL algorithm in other application regimes where ITRs are commonly applied, such as electronic health records, personalized advertising, and recommender systems.
These applications use observational data collected from  various sources (e.g., hospitals, clinics, insurance companies, social media, and e-commerce websites) and tend to be much larger in size compared to controlled clinical trials, implying that the performance of DP-OWL can be potentially much better.

As clinical and medical study data is extremely sensitive, being able to provide privacy protection guarantees for participants when collecting and sharing data is extremely valuable.
By applying privacy-guarantee frameworks when analyzing data and training models, as we have demonstrated in this work, study participants would be more comfortable participating in a study and researchers and investigators would feel more confident about sharing study results without compromising participants' privacy.


%--------------------------------------
\subsection*{Acknowledgements}
The authors acknowledge the helpful insights gleaned from several discussions with Dr. Jie Ding and Mr. Ganghua Wang. This work is supported by the University of Notre Dame Schmitt Fellowship and Lucy Graduate Scholarship to Giddens, University of Notre Dame Technology Ethics Center Research Assistantship, and was partially supported by an NSF grant DMS2113564 to Song.

%--------------------------------------
\subsection*{Supplementary Materials}
The supplementary materials contain additional results from the simulation studies and the derivation of the $\ell_2$ global sensitivity of the output from M-learning. 

\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}