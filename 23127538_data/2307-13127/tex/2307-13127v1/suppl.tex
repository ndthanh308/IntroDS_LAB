\documentclass[12pt, letterpaper]{article}
%verbose=true,
%\usepackage{arxiv}
\usepackage[margin=1in]{geometry}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage[round]{natbib}
\usepackage{doi}
\usepackage{amsmath,setspace, enumerate}
\usepackage{amssymb}
\usepackage{algorithm, comment}
\usepackage{algpseudocode, titlesec}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{color, colortbl}

\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{pro}{Proposition}
\newtheorem{rem}{Remark}
\newtheorem{assum}{Assumption}
\titlespacing*{\section}{0pt}{3pt plus 3pt minus 1pt}{3pt plus 3pt minus 1pt}
\titlespacing*{\subsection}{0pt}{3pt plus 3pt  minus 1pt}{3pt plus 3pt minus 1pt}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\E}{\mathbb{E}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\normx[1]{\left\Vert#1\right\Vert}
\newcommand{\mathbbm}[1]{\text{\usefont{U}{bbm}{m}{n}#1}}
\setlength{\parskip}{6pt}
\setlength{\parindent}{0pt}



\title{Supplementary materials to ``A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning''}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

% \author{ \href{https://orcid.org/0000-0000-0000-0000}{% Figure removed\hspace{1mm}David S.~Hippocampus}\thanks{Use footnote for providing further
% 		information about author (webpage, alternative
% 		address)---\emph{not} for acknowledging funding agencies.} \\
% 	Department of Computer Science\\
% 	Cranberry-Lemon University\\
% 	Pittsburgh, PA 15213 \\
% 	\texttt{hippo@cs.cranberry-lemon.edu} \\
% 	%% examples of more authors
% 	\And
% 	\href{https://orcid.org/0000-0000-0000-0000}{% Figure removed\hspace{1mm}Elias D.~Striatum} \\
% 	Department of Electrical Engineering\\
% 	Mount-Sheikh University\\
% 	Santa Narimana, Levand \\
% 	\texttt{stariate@ee.mount-sheikh.edu} \\
% 	%% \AND
% 	%% Coauthor \\
% 	%% Affiliation \\
% 	%% Address \\
% 	%% \texttt{email} \\
% 	%% \And
% 	%% Coauthor \\
% 	%% Affiliation \\
% 	%% Address \\
% 	%% \texttt{email} \\
% 	%% \And
% 	%% Coauthor \\
% 	%% Affiliation \\
% 	%% Address \\
% 	%% \texttt{email} \\
% }
\author{\small
Spencer Giddens$^1$\footnote{co-first authors}, Yiwang Zhou$^{2*}$, Kevin R. Krull$^{3,4}$, \vspace{-6pt}\\ 
\small Tara M. Brinkman$^{3,4}$, Peter X.K. Song$^5$, Fang Liu$^1$\footnote{corresponding author: fliu2@nd.edu}\\
\small$^1$ Department of Applied and Computational Mathematics and Statistics\vspace{-6pt}\\
\small University of Notre Dame, Notre Dame, IN 46556\\
\small$^2$ Department of Biostatistics, St. Jude Children's Research Hospital, Memphis, TN 38105\\
\small$^3$ Department of Epidemiology and Cancer Control, \vspace{-6pt}\\ \small St. Jude  Children's Research Hospital, Memphis, TN 38105\\
\small$^4$ Department of Psychology and Biobehavioral Sciences,  \vspace{-6pt}\\ \small St. Jude Children's Research Hospital, Memphis, TN 38105\\
\small$^5$ Department of Biostatistics, University of Michigan, Ann Arbor, MI 48109}

% Uncomment to remove the date
\date{}

% Uncomment to override  the `A preprint' in the header
% \renewcommand{\headeright}{Technical Report}
% \renewcommand{\undertitle}{Technical Report}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
% \hypersetup{
% pdftitle={A template for the arxiv style},
% pdfsubject={q-bio.NC, q-bio.QM},
% pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
% pdfkeywords={First keyword, Second keyword, More},
% }

\begin{document}
\maketitle

\subsection*{Additional results from the simulation studies}

\begin{table}[!htp]
    \centering
    \begin{center}
    \begin{tabular}{c|c|c|c|c|cc}
    \hline
    \multicolumn{2}{c}{\textbf{}} & \multicolumn{2}{|c|}{\textbf{Treatment Assignment Accuracy (\%)}} & \multicolumn{2}{c}{\textbf{Treatment Value}} \\
    \hline
    $\epsilon$ & $n$ & Mean & 95\% CI & Mean & 95\% CI \\
    \hline
    \multirow{4}{*}{0.1} & 200 & 50.24 & (49.06, 51.42) & 8.36 & (8.26, 8.46)\\
    & 500 & 50.20 & (48.89, 51.52) & 8.42 & (8.32, 8.52)\\
    & 800 & 50.70 & (49.45, 51.94) & 8.40 & (8.30, 8.50)\\
    & 1000 & 51.08 & (49.72, 52.44) & 8.41 & (8.32, 8.51)\\
    \hline
    \multirow{4}{*}{0.5} & 200 & 52.87 & (51.49, 54.25) & 8.56 & (8.46, 8.66)\\

    & 500 & 58.02 & (56.58, 59.47) & 8.84 & (8.73, 8.95)\\

    & 800 & 60.20 & (58.61, 61.79) & 9.04 & (8.92, 9.16)\\

    & 1000 & 60.95 & (59.22, 62.68) & 9.05 & (8.93, 9.16)\\
    \hline
    \multirow{4}{*}{1} & 200 & 55.53 & (54.23, 56.83) & 8.77 & (8.67, 8.86)\\

    & 500 & 61.03 & (59.36, 62.70) & 9.07 & (8.96, 9.18)\\

    & 800 & 64.08 & (62.22, 65.93) & 9.25 & (9.13, 9.37)\\

    & 1000 & 64.70 & (62.93, 66.48) & 9.28 & (9.17, 9.40)\\
    \hline
    \multirow{4}{*}{2} & 200 & 58.99 & (57.30, 60.68) & 8.90 & (8.79, 9.02)\\

    & 500 & 68.17 & (66.27, 70.08) & 9.49 & (9.38, 9.61)\\

    & 800 & 74.32 & (72.50, 76.14) & 9.91 & (9.80, 10.02)\\

    & 1000 & 79.03 & (77.42, 80.63) & 10.12 & (10.03, 10.21)\\
    \hline
    \multirow{4}{*}{5} & 200 & 67.66 & (65.88, 69.43) & 9.51 & (9.40, 9.63)\\

    & 500 & 78.79 & (77.29, 80.29) & 10.13 & (10.04, 10.21)\\

    & 800 & 81.97 & (80.57, 83.37) & 10.23 & (10.15, 10.31)\\

    & 1000 & 84.80 & (83.62, 85.99) & 10.31 & (10.24, 10.38)\\
    \hline
    \multirow{4}{*}{$\infty$ (No DP)} & 200 & 88.65 & (88.02, 89.27) & 10.41 & (10.36, 10.47)\\

    & 500 & 92.18 & (91.75, 92.60) & 10.52 & (10.47, 10.57)\\

    & 800 & 93.39 & (93.02, 93.75) & 10.54 & (10.49, 10.60)\\

    & 1000 & 94.29 & (93.97, 94.61) & 10.62 & (10.57, 10.68)\\
    \hline
    \end{tabular}
    \end{center}
    \caption{Simulation results for various combinations of $\epsilon$ and $n$ are presented.
    The mean and 95\% CI, are shown for both the optimal treatment assignment accuracy (as a percentage) and the empirical treatment value computed over the validation set.
    The case without DP (corresponding to $\epsilon=\infty$) is also shown for reference.}
    \label{tab:sim_results}
\end{table}

% Figure environment removed


\subsection*{$\ell_2$ sensitivity in output perturbation of M-learning}

The appendix presents the $\ell_2$ sensitivity of the output from the M-learning framework \citep{Wu2020}, which is introduced in Section 1.1 of the main text.
The result is an extension of Theorem 1 on the sensitivity of the output from a DP-OWL procedure to  M-learning.
The regularity conditions in the derivation of the sensitivity listed in Assumption 1 apply here. 

M-learning methods perform matching instead of inverse probability weighting as used in OWL for estimating ITRs to alleviate confounding and assess individuals' treatment responses to alternative treatments more accurately.
Matching-based value functions are used to compare matched pairs.
Denote the matched set for subject $i$ as $\mathcal{M}_i$, which consists of subjects with opposite treatments but similar covariates as subject $i$, where similarity is defined under a suitable distance metric.
The loss function with Huber loss and an $\ell_2$ regularization of the model parameters in M-learning 
\begin{align*}
L=\sum_{i=1}^n \sum_{j\in\mathcal{M}_i}|\mathcal{M}_i|^{-1}g(|\tilde{B}_i-\tilde{B}_j|)\ell_{\text{Huber}}(z_{ij})+\gamma||\mathbf{w}||^2,
\end{align*}
where $g(.)$ is a user-specified monotonically increasing function (e.g. typical choices are $g(.)=1$ or the identity function), $z_{ij}= A_i\text{sign}(|\tilde{B}_i-\tilde{B}_j|)\mathbf{w}^T\mathbf{x}_i$.
$\tilde{B}_i$ is a surrogate residualized outcome for the observed outcome $B_i$ and  defined by $\tilde{B}_i= B_i -s(\mathbf{x}_i)$ with  $s()$ being any measurable function \citep{Liu2019}.
Using $\tilde{B}_i$ helps improve the performance of M-learning. 

Let $d_g(\mathbf{w})$ denote the difference of two loss functions evaluated at a pair of neighboring datasets $(\mathbf{x}, \mathbf{A},\mathbf{B})$ and $(\mathbf{x}', \mathbf{A}',\mathbf{B}')$, where $\mathbf{x}=\{x_i\}_{i=1,\ldots,n},  \mathbf{A}=\{A_i\}_{i=1,\ldots,n}, \mathbf{B}=\{B_i\}_{i=1,\ldots,n}$; similarly for $(\mathbf{x}', \mathbf{A}',\mathbf{B}')$.
WLOG, we assume the $n$-th individual in two datasets ($\mathbf{x}, \mathbf{A},\mathbf{B}$) and ($\mathbf{x}', \mathbf{A}',\mathbf{B}'$) are not the same and that the values of $\tilde{B}$ in a subset of individuals $\mathcal{S}$ of the dataset ($|\mathcal{S}|\ge1$ individuals) are affected from altering the $n$-th individual, then 
\begin{align*}
d_g(\mathbf{w})
=&\!\sum_{i\in\mathcal{S}}\!\bigg\{\!\!\sum_{j\in\mathcal{M}'_i}\!|\mathcal{M'}_i|^{-1}g(|\tilde{B}'_i-\tilde{B}'_j|)\ell_{\text{Huber}}(z'_{ij}))\!-\!\!\!\sum_{j\in\mathcal{M}_i}\!\!|\mathcal{M}_i|^{-1}g(|\tilde{B}_i-\tilde{B}_j|)\ell_{\text{Huber}}(z_{ij}))\bigg\}, \\
\nabla_\mathbf{w}d_g(\mathbf{w}) 
=&\sum_{i\in\mathcal{S}}\bigg\{ \sum_{j\in\mathcal{M}'_i}|\mathcal{M'}_i|^{-1}d_g(|\tilde{B}'_i-\tilde{B}'_j|) A'_i\text{sign}(|\tilde{B}'_i-\tilde{B}'_j|)\nabla_\mathbf{w}\ell_{\text{Huber}}(z'_{ij})\mathbf{x}'_i-\\
& \qquad\sum_{j\in\mathcal{M}_i}|\mathcal{M}_i|^{-1}g(|\tilde{B}_i-\tilde{B}_j|)A_i\text{sign}(\tilde{B}_i-\tilde{B}_j)\nabla_\mathbf{w}\ell_{\text{Huber}}(z_{ij})\mathbf{x}_i\bigg\},
\mbox{ and}\\
||\nabla_\mathbf{w}d_g(\mathbf{w})|| = &\bigg\|
\sum_{i\in\mathcal{S}}\bigg\{\sum_{j\in\mathcal{M}'_i}|\mathcal{M'}_i|^{-1}g(|\tilde{B}'_i-\tilde{B}'_j|) A'_i\text{sign}(|\tilde{B}'_i-\tilde{B}'_j|)\nabla_\mathbf{w}\ell_{\text{Huber}}(z'_{ij})\mathbf{x}'_i-\\
& \qquad\quad\sum_{j\in\mathcal{M}_i}|\mathcal{M}_i|^{-1}g(|\tilde{B}_i-\tilde{B}_j|)A_i\text{sign}(|\tilde{B}_i-\tilde{B}_j|)\nabla_\mathbf{w}\ell_{\text{Huber}}(z_{ij})\mathbf{x}_i\bigg\|\bigg\}\\
\le & |\mathcal{S}|\ \bigg\|
\sum_{j\in\mathcal{M}'}|\mathcal{M'}_i|^{-1}g(|\tilde{B}'_i-\tilde{B}'_j|) A'_i\text{sign}(|\tilde{B}'_i-\tilde{B}'_j|)\nabla_\mathbf{w}\ell_{\text{Huber}}(z'_{ij})\mathbf{x}'_i\bigg\|+\\
&|\mathcal{S}| \bigg\|\sum_{j\in\mathcal{M}_i}|\mathcal{M}_i|^{-1}g(|\tilde{B}_i-\tilde{B}_j|)A_i\text{sign}(|\tilde{B}_i-\tilde{B}_j|)\nabla_\mathbf{w}\ell_{\text{Huber}}(z_{ij})\mathbf{x}_i\bigg\|\\
\le & |\mathcal{S}| \sum_{j\in\mathcal{M}'_i}|\mathcal{M'}_i|^{-1} \bigg\|
g(|\tilde{B}'_i-\tilde{B}'_j|) A'_i\text{sign}(|\tilde{B}'_i-\tilde{B}'_j|)\nabla_\mathbf{w}\ell_{\text{Huber}}(z'_{ij})\mathbf{x}'_i\bigg\|+\\
& |\mathcal{S}|\sum_{j\in\mathcal{M}_i}|\mathcal{M}_i|^{-1} \bigg\|g(|\tilde{B}_i-\tilde{B}_j|)A_i\text{sign}(|\tilde{B}_i-\tilde{B}_j|)\nabla_\mathbf{w}\ell_{\text{Huber}}(z_{ij})\mathbf{x}_i\bigg\|.
\end{align*}
Given $A_i\in\{-1,1\}$, $\text{sign}(|\tilde{B}'_i-\tilde{B}'_j|)\in\{-1,1\}$, with the assumptions of $||\mathbf{x}_i||_2\le 1$ $\forall i$ and $\|\nabla_\mathbf{w}\ell_{\text{Huber}}\|\le1$ as listed in Assumption 1, then
\begin{align*}
||\nabla_\mathbf{w}g(\mathbf{w})|| 
& \le|\mathcal{S}|\sum_{j\in\mathcal{M}_i}\!|\mathcal{M}_i|^{-1} \bigg\|g(|\tilde{B}_i-\tilde{B}_j|)\bigg\|\! + |\mathcal{S}|\sum_{j\in\mathcal{M}'_i}|\mathcal{M}'_i|^{-1} \bigg\|g(|\tilde{B}'_i-\tilde{B}'_j|)\bigg\|\\
&\le\!2|\mathcal{S}|\max_i\max_j\big|g(|\tilde{B}_i-\tilde{B}_j|)\big|.
\end{align*}
Per Lemma 1 in \cite{chaudhuri2011arxiv}, the $\ell_2$ sensitivity of $\hat{\mathbf{\theta}}$ is
\begin{equation}\label{eqn:M.sens}
\Delta_{\hat{\mathbf{\theta}}}= \frac{2|\mathcal{S}|}{\gamma} \sup \big|g(|\tilde{B}_i-\tilde{B}_j|)\big|.
\end{equation}
As for the value of $\sup \big|g(|\tilde{B}_i-\tilde{B}_j|)\big|$, it depends on the function $g$ and the model used to obtain $\tilde{B}$.
If $g()=1$, then $|\mathcal{S}|=1,\sup \big|g(|\tilde{B}_i-\tilde{B}_j|)\big|=1$ and $\Delta_{\hat{\mathbf{\theta}}}=2/\gamma$.
If linear regression is used to estimate $\tilde{B}$ and $g$ is the identity function, then it is reasonable to set $\sup \big|g(|\tilde{B}_i-\tilde{B}_j|)\big|=6$. 

Finally, privacy-preserving estimated parameters of dimension $p$ from an M-learning procedure given the derived sensitivity in Eqn \eqref{eqn:M.sens} are released as in $\hat{\mathbf{\theta}}^*=\hat{\mathbf{\theta}}+\mathbf{e}$, where $f(\mathbf{e})\propto e^{-\beta\|e\|}=e^{-\beta\sqrt{e_i^2+ \cdots + e^2_p}}$. 

\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}