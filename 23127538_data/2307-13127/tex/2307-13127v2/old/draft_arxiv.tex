\documentclass{article}



\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{subcaption}

\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{assum}[defn]{Assumption}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\E}{\mathbb{E}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\normx[1]{\left\Vert#1\right\Vert}


\title{Differentially Private Outcome Weighted Learning}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

% \author{ \href{https://orcid.org/0000-0000-0000-0000}{% Figure removed\hspace{1mm}David S.~Hippocampus}\thanks{Use footnote for providing further
% 		information about author (webpage, alternative
% 		address)---\emph{not} for acknowledging funding agencies.} \\
% 	Department of Computer Science\\
% 	Cranberry-Lemon University\\
% 	Pittsburgh, PA 15213 \\
% 	\texttt{hippo@cs.cranberry-lemon.edu} \\
% 	%% examples of more authors
% 	\And
% 	\href{https://orcid.org/0000-0000-0000-0000}{% Figure removed\hspace{1mm}Elias D.~Striatum} \\
% 	Department of Electrical Engineering\\
% 	Mount-Sheikh University\\
% 	Santa Narimana, Levand \\
% 	\texttt{stariate@ee.mount-sheikh.edu} \\
% 	%% \AND
% 	%% Coauthor \\
% 	%% Affiliation \\
% 	%% Address \\
% 	%% \texttt{email} \\
% 	%% \And
% 	%% Coauthor \\
% 	%% Affiliation \\
% 	%% Address \\
% 	%% \texttt{email} \\
% 	%% \And
% 	%% Coauthor \\
% 	%% Affiliation \\
% 	%% Address \\
% 	%% \texttt{email} \\
% }
\author{
Spencer Giddens\\
Department of Applied and Computational Mathematics and Statistics\\
University of Notre Dame\\
Notre Dame, IN 46615\\
\texttt{sgiddens@nd.edu}
}

% Uncomment to remove the date
\date{}

% Uncomment to override  the `A preprint' in the header
% \renewcommand{\headeright}{Technical Report}
% \renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
% \hypersetup{
% pdftitle={A template for the arxiv style},
% pdfsubject={q-bio.NC, q-bio.QM},
% pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
% pdfkeywords={First keyword, Second keyword, More},
% }

\begin{document}
\maketitle

\begin{abstract}
It is commonplace to use sensitive datasets containing personal information to build predictive models.
While these models can be genuinely beneficial, they are also susceptible to attacks that may compromise the personal information used to fit the models.
Differential privacy (DP) is a popular framework for preserving privacy in these situations by providing mathematically provable bounds on the privacy loss incurred when running algorithms on sensitive data.
Previous work has focused on applying DP to typical empirical risk minimization (ERM) algorithms, a broad class of methods that includes techniques such as logistic regression and support vector machines (SVMs).
However, the algorithms developed for these cases have not yet been generalized to the case where individual observations are weighted by importance.
Weighted ERM forms the basis for several individualized treatment rule (ITR) algorithms, such as outcome weighted learning (OWL), which aim to assign ``treatments'' to individuals in a manner that maximizes their expected benefit.
OWL algorithms are widely used in the setting of randomized clinical trials, but also have applications in personalized advertizing and recommender systems.
All of these domains utilize sensitive information to fit their models, but until now no privacy-preserving counterpart to OWL has been developed.
This paper presents the first such DP-satisfying weighted ERM algorithm and proves the conditions under which the algorithm satisfies DP.
The paper then analyzes the performance of the algorithm for OWL in a clinical trial setting via a simulation study and a case study.
The results demonstrate that it is possible to provide DP guarantees when fitting weighted ERM models while still maintaining sufficient model performance to be useful.
This has broad implications for the future of data privacy in clinical trials and other settings utilizing weighted ERM models, allowing practitioners in such fields a method for ensuring greater privacy protections for individuals whose data is used.
\end{abstract}


% keywords can be removed
\keywords{Data privacy \and Differential privacy \and Outcome weighted learning \and Individualized treatment rules \and Empirical risk minimization \and Support vector machines}

\section{Introduction}
\label{sec:intro}

In today's data-driven world, it is common practice in many disciplines to utilize datasets containing sensitive information to aid critical decision making.
In these situations, it is crucial to ensure a level of privacy for the individuals whose data are used.
While cryptographical methods can be used to protect sensitive datasets from attacks seeking direct dataset access, these methods are powerless to protect against attacks aiming to infer sensitive data from publicly released dataset derivatives, such as aggregate statistics and statistical or machine learning (ML) models.
Thus, it is important to create and release such derivatives in a privacy-preserving manner.

Empirical risk minimization (ERM) models are one popular framework for classification models that are often trained using sensitive datasets.
In this framework, a classifier function is chosen that minimizes a chosen loss function representing the average shortfall between the correct and predicted labels over the training data.
A regularization term penalizing the complexity of the classifier function is also often included as part of the objective function to be minimized.
For some applications, it can be beneficial to use an extension of ERM where weights are applied to the computed loss function at each training observation. 

One example of a weighted ERM model, and the primary focus of this paper, is outcome weighted learning (OWL) \citep{Zhao2012}.
OWL is an example of an individualized treatment rule (ITR) algorithm, a class of causal inference ML methods that aim to assign ``treatments'' to individuals in a manner that maximizes their expected benefit, based on individual characteristics.
These algorithms are commonly applied in the healthcare setting, where they are used to assign medical treatments (e.g., a particular drug) in an individualized manner, taking into account their personal attributes and characteristics.
This approach contrasts with broadly applying the same treatment to an entire population presenting with the same set of symptoms, and can lead to better patient outcomes \citep{Qian2011}.
These models are also applicable to personalized advertizing \citep{Wang2015, Sun2015} and recommender systems \citep{Schnabel2016, Lada2019}.

Clinical trials are one area where OWL is commonly applied.
The setting considered in this paper is a two-arm randomized trial.
In this setting, patient prognostic variables are collected from a set of participants, and each participant is randomly assigned one of two treatments independent of the collected variables.
After receiving the treatment, a ``benefit'' metric is used to measure the effect of the treatment on each participant.
From the collected participant characteristics, random treatment assignment, and recorded benefit, the goal is to fit a model that can accurately assign an individual to the treatment with the largest expected benefit for that individual.
As the data collected for each clinical trial participant is highly sensitive, legal and ethical concerns over data privacy often significantly restrict or delay the development of these models. 
Therefore, the development of rigorous, privacy-preserving algorithms for fitting OWL models has the potential to streamline model development and better safeguard participant data.

As mentioned, these applications of OWL require the use of sensitive personal information to train the models, and therefore these models are susceptible to attacks that attempt to infer that sensitive information.
One may suppose that thorough ad hoc anonymization of the sensitive dataset (i.e. removing obvious identifiers likes names, addresses, etc.) before use would thwart attempts to infer sensitive individual-level information from released models.
Unfortunately, ad hoc anonymization techniques have been repeatedly shown to be insufficient in ensuring privacy.
By de-anonymizing the 2006 Netflix Prize dataset, \citet{Narayanan2008} demonstrated that uncovering sensitive, individual-level information from anonymized datasets is possible with access only to limited, publicly accessible side information.
\citet{Sweeney2015} showed a similar result for re-identification attacks on Washington State health records.
Machine learning models derived from anonymized datasets have also been shown to be vulnerable to attacks inferring membership in and sensitive attributes of the dataset \citep{Shokri2017, Zhao2021}.

In contrast to ad hoc anonymization techniques, $\epsilon$-differential privacy ($\epsilon$-DP) \citep{Dwork2006} provides a mathematically rigorous framework for ensuring privacy to sensitive datasets.
In this framework, carefully calibrated random noise is injected into statistics or model coefficients derived from sensitive data in order to obscure the personal information used to generate the output.
A mechanism acting on a sensitive dataset is considered to satisfy $\epsilon$-DP if the mechanism outputs (which are random variables) are sufficiently ``similar'' for two input \emph{neighboring datasets} (datasets that differ in only one individual's records).
Intuitively, this preserves privacy by ensuring that for any given individual, the mechanism outputs are similar enough that it is difficult to determine with certainty whether or not that individual's data was used to generate the mechanism output.
As a trade-off, mechanisms satisfying $\epsilon$-DP generally incur a cost in performance.
The tunable parameter $\epsilon$ controls the amount by which the mechanism outputs on neighboring datasets are allowed to differ, which in turn impacts the trade-off between privacy and utility, with a larger $\epsilon$ representing more utility, and a smaller $\epsilon$ representing more privacy.

We now summarize the major contributions of this paper.
\begin{enumerate}
    \item We present a general algorithm for achieving $\epsilon$-DP for weighted ERM models using the output perturbation method from \citet{Dwork2006}. 
    Similar algorithms for vanilla, unweighted ERM models have already been developed and analyzed \citep{Chaudhuri2009, chaudhuri2011, Kifer2012}, but this work represents the first extension of these DP algorithms to the weighted ERM case.
    \item The presented algorithm is proven to satisfy $\epsilon$-DP.
    \item We present a set of regularity conditions for OWL and prove that, under these conditions, the DP weighted ERM algorithm can be applied to guarantee $\epsilon$-DP for OWL models.
    \item We discuss how to maximize utility for this algorithm for a given privacy budget $\epsilon$ through hyperparameter tuning and feature selection.
    \item We present simulation study results, which demonstrate that our algorithm has the potential to simultaneously build valuable OWL models for treatment assignment and preserve privacy for clinical trial participants.
    \item Finally, we demonstrate our algorithm using a case study and discuss potential implications and directions for future work.
\end{enumerate}

\subsection{Related work}
\label{subsec:related_work}

Several works have studied the problem of determining optimal ITRs using machine learning techniques.
OWL was originally proposed by \citet{Zhao2012}, who showed that a weighted support vector machine (SVM) framework can be used to find a consistent treatment rule estimator for the optimal decision function, meaning the decision function which maximizes the expected clinical benefit.
Later, \citet{Zhou2017} improved on these results by introducing residual weighted learning (RWL).
This method uses a different technique to compute the weights used for the misclassification error approximation in SVM and improves on the performance of OWL.
Matched learning (M-learning) \citep{Wu2020} is another related ITR algorithm that uses matching in large-scale electronic health records as an alternative to the weighting method commonly employed in other ITR algorithms.

Perhaps most directly related to this paper is the previous work devoted to vanilla ERM algorithms under DP guarantees.
\citet{Chaudhuri2009} originally proposed and analyzed the application of $\epsilon$-DP to the computation of logistic regression coefficients for binary classification problems.
Two different perturbation methods for achieving DP, known as output perturbation and objective perturbation, were analyzed. 
\citet{chaudhuri2011} later generalized the logistic regression work by developing DP-satisfying counterparts to binary classification problems under the vanilla ERM framework, which we further extend to the weighted ERM case in this paper.
Subsequent works have incrementally improved DP ERM analysis, such as by tightening error bounds or making the algorithm more efficient \citep{Bassily2014, Kasiviswanathan2016}.

Some other related work on DP ERM include works by \citet{Kifer2012} and \citet{Rubenstein2012}.
\citet{Kifer2012} broadened DP applications to vanilla ERM by modifying the algorithm from \citet{chaudhuri2011} to permit approximate $(\epsilon, \delta)$-DP in addition to pure $\epsilon$-DP.
Additionally, this new algorithm can be applied to a broader set of possible regularization functions and more easily lends itself to regression problems within the ERM framework.
At its most basic, SVM is an ERM method that creates a linear separating hyperplane to classify data.
Nonlinear separators (in the original dataset dimension) can also be constructed by transforming the data to a higher dimensional space and finding a linear separating hyperplane in that space.
The well known ``kernel trick'' is often used to accomplish this, but this method has privacy issues, which are resolved in \citet{chaudhuri2011} via kernel approximation \citep{Rahimi2007, Rahimi2008}.
\citet{Rubenstein2012} takes a different approach, instead applying DP via an algorithm that solves the SVM dual problem directly, bypassing the privacy concerns of the kernel trick.

Finally, it is worth mentioning some recent work aiming to estimate the individual treatment effects themselves in a DP-satisfying manner \citep{Betlei2021, Niu2022}.
In contrast with these works, our paper presents a DP-satisfying algorithm for fitting a treatment assignment model using the treatment effect as an observed variable.

\section{Preliminaries}
\label{sec:preliminaries}

In this section, we formalize the notions of weighted empirical risk minimization, outcome weighted learning, and differential privacy.
We will assume by default that an unspecified norm is the $\ell_2$ norm (i.e., $\norm{\cdot} = \norm{\cdot}_2$) and that boldfaced variables are vector-valued.

\subsection{Weighted empirical risk minimization (wERM)}
\label{subsec:wERM}

For the vanilla ERM framework, we assume we have a dataset consisting of $n$ pairs of feature vectors with corresponding labels, as well as a non-negative loss function.
Our goal is then to determine a predictor function that minimizes the loss function over the data.
It is also common to employ a regularization function penalizing the complexity of the predictor function.

\begin{defn}
\label{def:ERM}
    \textnormal{(Empirical risk minimization)} Let $(\mathbf{x}_i, y_i) \in (\mathcal{X},\mathcal{Y})$ represent a set of $n$ pairs of feature vectors with corresponding labels and define $\ell: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$ to be a non-negative loss function over the label space $\mathcal{Y}$.
    Also define $R: \mathcal{F} \rightarrow \mathbb{R}$ to be a regularizer function, where $\mathcal{F}$ represents the set of considered predictor functions $f: \mathcal{X} \rightarrow \mathcal{Y}$.
    A (regularized) \textnormal{empirical risk minimization (ERM)} algorithm is one that solves the problem
    \begin{equation}
        \argmin_{f\in\mathcal{F}} \frac{1}{n}\sum_{i=1}^n \ell(f(\mathbf{x}_i), y_i) + \frac{\gamma}{n}R(f),
    \end{equation}
    where $\gamma>0$ is a tunable regularization constant.
\end{defn}

ERM algorithms are widely used in practice for constructing effective predictor functions.
Specific instances of ERM algorithms are governed by defining an appropriate predictor function set $\mathcal{F}$.
For example, linear regression is obtained by defining $\mathcal{F}$ to be the set of all functions of the form $f_{\boldsymbol{\theta}, b}(\mathbf{x}) = \mathbf{x}\boldsymbol{\theta} + b$, where $\mathbf{x}$ is a row vector of features, $\boldsymbol{\theta}$ is a column vector of feature weights, and $b$ is a bias term.
Other popular examples of ERM algorithms include logistic regression and SVM.

In some instances, it can be beneficial to assign a weight to each of the $n$ loss function values in the ERM algorithm.
This has the effect of assigning more importance to some observed data points than others and leads to the definition of weighted ERM (wERM).

\begin{defn}
\label{def:wERM}
    \textnormal{(Weighted empirical risk minimization)} Let $(\mathbf{x}_i, y_i)$, $\ell$, $R$, $\gamma$, and $\mathcal{F}$ be defined as in Definition \ref{def:ERM} and let $w_i$ be a set of $n$ weights.
    Then a \textnormal{weighted empirical risk minimization (wERM)} algorithm is one that solves the problem
    \begin{equation}
        \argmin_{f\in\mathcal{F}} \frac{1}{n}\sum_{i=1}^n w_i\ell(f(\mathbf{x}_i), y_i) + \frac{\gamma}{n}R(f).
    \end{equation}
\end{defn}

\subsection{Outcome weighted learning (OWL)}
\label{subsec:OWL}

OWL \citep{Zhao2012} is an algorithm developed for deriving ITRs commonly applied in clinical trials.
In this setting, clinical trial participants are randomly assigned to one of two possible treatments.
A set of characteristics or prognostic variables for each participant are also recorded.
The resulting observed treatment effect, which we refer to as the benefit, is then measured.
OWL then attempts to infer a treatment assignment function that maximizes the expected clinical benefit.
We formalize this idea below.

\begin{defn}
\label{def:OWL}
    \textnormal{\citep{Zhao2012} (Outcome weighted learning)} 
    Let $\mathbf{X} \in \mathbb{R}^d$, $A \in \{-1, 1\}$, and $B \in \mathbb{R}$ be random variables corresponding to the set of prognostic characteristics, the treatment assignment, and the treatment benefit, respectively.
    Let $(\mathbf{x}_i, A_i, B_i)$ be a set of data points from $n$ observed individuals, which are realizations of the corresponding random variables.
    In theory, the goal is to find a decision function $f \in \mathcal{F}$ solving the problem
    \begin{equation}
        \argmax_{f\in\mathcal{F}} \E \left( \frac{I(A=f(\mathbf{X}))}{P(A|X)}B\right) = \argmin_{f\in\mathcal{F}} \E \left(  \frac{I(A\ne f(\mathbf{X}))}{P(A|X)}B\right),
    \end{equation}
    where $\mathbb{E}$ is the expected value function, $I$ is the indicator function, and $P$ means probability.
    \citet{Zhao2012} showed that in essence this problem can be solved empirically using a weighted SVM, as follows.
    For the purposes of this paper, we define an \textnormal{outcome weighted learning (OWL)} algorithm to be one that solves the problem
    \begin{equation}
        \argmin_{f_{\boldsymbol{\theta}, b}\in\mathcal{F}} \frac{1}{n} \sum_{i=1}^n\frac{B_i}{P(A_i|\mathbf{x}_i)} \max\{0, 1 - A_if_{\boldsymbol{\theta}, b}(\mathbf{x}_i)\} + \frac{\gamma}{n}\norm{\boldsymbol{\theta}},
    \end{equation}
    where $\mathcal{F}$ represents all functions of the form $f_{\boldsymbol{\theta}, b} = \mathbf{x}_i\boldsymbol{\theta} + b$.
\end{defn}

It is straightforward to see that OWL in Definition \ref{def:OWL} is a special case of wERM from Definition \ref{def:wERM}.

\subsection{Differential privacy (DP)}
\label{subsec:DP}

The overarching goal of this work is to produce an OWL algorithm that preserves the privacy of the individuals whose data are used to fit the model.
To do this, we use the popular DP framework originally established in \citet{Dwork2006} and expanded in \citet{Dwork2006b}.
The DP framework ensures that, regardless of the inclusion or exclusion of any single individual's data, the results of a DP-satisfying mechanism acting on a sensitive dataset are sufficiently similar.
We formalize DP by first defining the notion of neighboring datasets.

\begin{defn}
    \textnormal{(Neighboring datasets)} Two datasets $D_1, D_2 \in \mathcal{D}$ are considered \textnormal{neighboring datasets} if $D_1$ can be obtained from $D_2$ by modifying a single individual's data.
    If the distance $d$ between two datasets is defined to be the number of records differing between them, $D_1$ and $D_2$ are neighboring datasets if and only if $d(D_1, D_2)=1$.
\end{defn}

With this, we can now formally define DP.

\begin{defn}
    \textnormal{($(\epsilon, \delta)$-Differential privacy)} A randomized mechanism $\mathcal{M}$ satisfies \textnormal{$(\epsilon, \delta)$-differential privacy} if for all $S \subset \textnormal{Range}(\mathcal{M})$,
    \begin{equation}
        P(\mathcal{M}(D_1) \in S) \le e^\epsilon P(\mathcal{M}(D_2) \in S) + \delta,
    \end{equation}
    where $D_1$ and $D_2$ are any neighboring datasets, $\epsilon>0$ and $\delta\ge0$.
    When $\delta=0$, it is standard to shorten this to \textnormal{$\epsilon$-differential privacy}.
\end{defn}

Intuitively, a mechanism satisfying DP provides mathematical guarantees that its outputs are similar on neighboring datasets.
The differing individual between the neighboring datasets is arbitrary, and therefore the guarantees hold for all individuals simultaneously.
As a result, each individual can feel comfortable allowing their data to be used since the resulting output will be similar whether their data is included or not.
The tunable parameter $\epsilon$ controls the similarity of the output distributions on the neighboring datasets. 
Smaller $\epsilon$ require more similar outputs and thus more privacy, while larger $\epsilon$ allows for less variance in the mechanism outputs and thus more accuracy.
Generally, $\epsilon\le1$ is considered to provide strong privacy guarantees, though it is not unusual to see larger values of $\epsilon$ used in many applications.
It is common to interpret $\delta$ as the probability that $\epsilon$-DP fails.

Some of the popularity of DP can be attributed to strong related theoretical results, many of which are collected and summarized by \citet{Dwork2014}.
The post-processing theorem, for example, ensures that the output of a DP mechanism cannot be further manipulated to weaken the DP guarantees.
The DP guarantees of the composition of several DP mechanisms are also straightforward to compute via several composition theorems \citep{Dwork2006, Dwork2006b, Dwork2010}.

A common way to obtain a DP mechanism approximating a given target function is to add appropriately scaled noise directly to the output of the function.
The scale of the added noise is usually related to the global sensitivity of the function.
Global sensitivity was originally defined using the $\ell_1$ norm by \citet{Dwork2006}.
Here we use a more general definition from \citet{Liu2019}.

\begin{defn}
    \textnormal{($\ell_p$-global sensitivity)} Let $f: \mathcal{D} \rightarrow \mathbb{R}^n$ be a target function operating on a dataset.
    The \textnormal{$\ell_p$-global sensitivity} of $f$ is defined to be
    \begin{equation}
        \Delta_{p,f} = \max_{\substack{D_1, D_2 \\ d(D_1,D_2) = 1}}\norm{f(D_1) - f(D_2)}_p,
    \end{equation}
    where $\norm{\cdot}_p$ is the $\ell_p$ norm.
\end{defn}

The global sensitivity bounds the amount by which the target function can vary between two neighboring dataset inputs.
As an example, if $D_1, D_2 \in \mathcal{D} = [5, 10]$ are datasets of $n$ individuals and $f$ is a function computing the mean, the global sensitivity will be $(10 - 5)/n$, which represents the largest amount that the mean can change under the modification of a single individual value.
Note that since the mean is a scalar-valued function, the $\ell_p$-global sensitivity is identical regardless of $p$.
As a general rule, the larger the global sensitivity of a target function, the larger the noise scale of a DP mechanism must be to achieve privacy guarantees at a pre-specified $(\epsilon, \delta)$ level.

\section{Methods}
\label{sec:methods}

In this section, we present a general algorithm for achieving $\epsilon$-DP for wERM. 
After presenting the algorithm, we prove that it does indeed satisfy DP by demonstrating the algorithm's global sensitivity. 
Finally, we discuss the modifications necessary to the OWL algorithm from Definition \ref{def:OWL} in order for it to be a special case of the DP-satisfying wERM algorithm.
This collectively results in proving an approximation to the OWL algorithm that satisfies DP.

\subsection{Differentially private weighted empirical risk minimization (DP-wERM)}
\label{subsec:DP-wERM}

Here we present Algorithm \ref{alg:DP-wERM}, a general procedure for achieving $\epsilon$-DP for wERM algorithms.
This algorithm represents a novel extension of the DP-satisfying ERM algorithm introduced previously \citep{chaudhuri2011}.
There are several required inputs to the algorithm.
A privacy budget $\epsilon>0$ and a regularization constant $\gamma>0$ must be selected.
A dataset consisting of $p$-dimensional feature vectors $\mathbf{x}_i$, corresponding labels $y_i$, and weights $w_i$ for fitting the wERM model is also provided.
The proof of DP requires that $\norm{\mathbf{x}_i}\le 1$ and that $0 \le w_i \le W$ for some selected weight upper bound $W$.
The selected loss function $\ell$ and regularizer $R$ must also satisfy certain regularity conditions to prove DP, which will be highlighted in Section \ref{sec:DP_guarantees}.
Finally, for this algorithm, we restrict the set of possible decision functions $\mathcal{F}$ to the set of linear predictor functions $f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{x}\boldsymbol{\theta}$ parameterized by a single real-valued vector $\boldsymbol{\theta}$.

\begin{algorithm}
\caption{Differentially private weighted empirical risk minimization (DP-wERM)}\label{alg:DP-wERM}
    \begin{algorithmic}[1]
        \Require Privacy budget $\epsilon>0$; dataset $D = (\mathbf{x}_i, y_i, w_i) \in \mathbb{R}^p \times \{-1, 1\} \times \mathbb{R}$ of $n$ feature-label-weight triples with $\norm{\mathbf{x}_i}\le 1$, and weights $0 \le w_i \le W$ for all $i$; regularization constant $\gamma>0$; set of considered output functions restricted to linear predictors $\mathcal{F} = \{f_{\boldsymbol{\theta}}\,:\,f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{x}\boldsymbol{\theta}, \boldsymbol{\theta} \in \mathbb{R}^p\}$; loss function $\ell$ and regularization function $R$ each satisfying necessary regularity conditions
        \State Set $\Delta \gets \frac{2W}{\gamma}$ \Comment{$\ell_2$-global sensitivity of wERM}
        \State Sample $\zeta$ from a Gamma distribution with shape $p$ and rate $\frac{\epsilon}{\Delta}$
        \State Sample $\mathbf{z}$ from a $p$-dimensional multivariate standard Gaussian distribution
        \State Set $\mathbf{\hat{z}} \gets \zeta\frac{\mathbf{z}}{\norm{\mathbf{z}}}$
        \State Set $\boldsymbol{\hat{\theta}} \gets \argmin_{\boldsymbol{\theta} \in \mathbb{R}^p} \frac{1}{n}\sum_{i=1}^n w_i\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i) + \frac{\gamma}{n}R(f_{\boldsymbol{\theta}})$ \Comment{wERM without DP}
        \State \Return $f_{\boldsymbol{\hat{\theta}} + \mathbf{\hat{z}}}$ \Comment{Add noise vector to achieve DP}
    \end{algorithmic}
\end{algorithm}

The algorithm first sets the global sensitivity to be $\frac{2W}{\gamma}$ (line 1).
A proof of correctness for this sensitivity is provided in Section \ref{sec:DP_guarantees}.
After this, the algorithm samples a scalar value $\zeta$ from a Gamma distribution with shape $p$ and rate $\frac{\epsilon}{\Delta}$ and a vector $\mathbf{z}$ from a $p$-dimensional multivariate standard Gaussian distribution (lines 2-3).
The sampled vector $\mathbf{z}$, when normalized, is equivalent to a uniform sample from a $p$-dimensional hypersphere \citep{Muller1959}.
Then, the vector $\mathbf{\hat{z}} = \zeta\frac{\mathbf{z}}{\norm{\mathbf{z}}}$ (line 4) represents a random sample from a distribution with density function $h(\mathbf{\hat{z}}) \propto \exp(-\frac{\epsilon}{\Delta}\norm{\mathbf{\hat{z}}})$ with uniformly random direction \citep{Chaudhuri2009}.
Finally, a set of linear predictor coefficients $\boldsymbol{\hat{\theta}}$ is selected that solves the standard wERM problem, the random noise vector $\mathbf{\hat{z}}$ is added to it, and the resulting prediction function $f_{\boldsymbol{\hat{\theta}} + \mathbf{\hat{z}}}$ is released (lines 5-6).

One important thing to note is that line 5 of Algorithm \ref{alg:DP-wERM} operates on the primal wSVM problem with a linear kernel.
This contrasts with the typical method of solving the wSVM problem by optimizing the dual problem using the popular ``kernel trick'' to efficiently compute the solution for a nonlinear basis.
Gaussian (also known as radial) and polynomial kernels, among others, are commonly used for this.
Unfortunately, the dual problem poses special problems for DP.
\citet{chaudhuri2011} outlines these problems and proposes a solution to them via a kernel approximation algorithm. 
That same approach can be applied directly to Algorithm \ref{alg:DP-wERM} to approximate the use of nonlinear kernels if desired.

\subsection{Proof of DP guarantees}
\label{sec:DP_guarantees}

Here, we prove that Algorithm \ref{alg:DP-wERM} satisfies $\epsilon$-DP.
We begin by introducing the concept of $\Lambda$-strong convexity, which is a necessary condition for the regularization function.

\begin{defn}
    \textnormal{($\Lambda$-strong convexity)} Let $g: \mathbb{R}^p \rightarrow \mathbb{R}$.
    If for all $\alpha \in (0, 1)$ and for all $\mathbf{x}, \mathbf{y}$,
    \begin{equation}
        g(\alpha\mathbf{x} + (1-\alpha)\mathbf{y}) \le \alpha g(\mathbf{x}) + (1-\alpha)g(\mathbf{y}) - \frac{1}{2}\Lambda\alpha(1-\alpha)\norm{\mathbf{x}-\mathbf{y}}^2,
    \end{equation}
    then we say that $g$ is \textnormal{$\Lambda$-strongly convex}.
\end{defn}

Regarding $\Lambda$-strong convexity, it is straightforward to show that if $g$ is $\Lambda$-strongly convex, then $ag$ is $a\Lambda$-strongly convex for any $a\in\mathbb{R}$.
It is also straightforward to show that if $f$ is convex and $g$ is $\Lambda$-strongly convex, then $f+g$ is $\Lambda$-strongly convex.

We now introduce a helpful lemma that was first stated and proven by \citet{chaudhuri2011}.
The lemma is re-stated here for convenience, but the reader is referred to the original work for the proof.

\begin{lem}
    \label{lem:Chaudhuri_lemma}
    Let $g_1: \mathbb{R}^p \rightarrow \mathbb{R}, g_2 : \mathbb{R}^p \rightarrow \mathbb{R}$ be everywhere differentiable functions.
    Assume $g_1(\boldsymbol{\theta})$ and $g_1(\boldsymbol{\theta}) + g_2(\boldsymbol{\theta})$ are $\Lambda$-strongly convex.
    Then if $\boldsymbol{\hat{\theta}}_1 = \argmin_{\boldsymbol{\theta}}g_1(\boldsymbol{\theta})$ and $\boldsymbol{\hat{\theta}}_2 = \argmin_{\boldsymbol{\theta}}g_1(\boldsymbol{\theta}) + g_2(\boldsymbol{\theta})$, we have
    \begin{equation}
        \norm{\boldsymbol{\hat{\theta}}_1 - \boldsymbol{\hat{\theta}}_2} \le \frac{1}{\Lambda}\max_{\boldsymbol{\theta}}\norm{\nabla g_2(\boldsymbol{\theta})}.
    \end{equation}
\end{lem}

As a final preliminary, we define the regularity conditions for wERM necessary to prove DP.
\begin{assum}
    \textnormal{(Regularity conditions for DP-wERM)} 
    Consider the wERM algorithm solving the problem
    \begin{equation}
        \argmin_{f\in\mathcal{F}} \frac{1}{n}\sum_{i=1}^n w_i\ell(f(\mathbf{x}_i), y_i) + \frac{\gamma}{n}R(f).
    \end{equation}
    We say that the it satisfies the \textnormal{regularity conditions for DP-wERM} if the below conditions hold.
    Conditions not related to the weight values are equivalent to conditions from \citet{chaudhuri2011}.
    \begin{enumerate}
        \item The considered dataset $D = (\mathbf{x}_i, y_i, w_i) \in \mathbb{R}^p \times \{-1, 1\} \times \mathbb{R}$ consists of $n$ feature-label-weight triples such that $\norm{\mathbf{x}_i}\le 1$, and $0 \le w_i \le W$ for all $i$,
        \item The set of considered output functions are linear predictors $\mathcal{F} = \{f_{\boldsymbol{\theta}}\,:\,f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{x}\boldsymbol{\theta}, \boldsymbol{\theta} \in \mathbb{R}^p\}$,
        \item $\ell(\hat{y}, y) = \ell(y\hat{y})$ for any inputs $\hat{y}, y\in\mathbb{R}$,
        \item $\ell$ is convex and everywhere differentiable,
        \item $|\ell^\prime(z)| \le 1$ for all $z=y\hat{y}$,
        \item $R$ is $1$-strongly convex and everywhere differentiable.
    \end{enumerate}
\end{assum}

At first glance, these conditions appear quite restrictive.
However, further inspection reveals that minor modifications to datasets and commonly used algorithms are often sufficient to meet the conditions.
It is unlikely, for example, that a real-world dataset will coincidentally satisfy the condition that $\norm{\mathbf{x}_i}\le1$.
However, knowledge of upper and lower bounds on the possible feature values can be used to preprocess the dataset so that the condition is met. 
If the upper and lower bounds can be reasonably well inferred or approximated without reference to the dataset itself (e.g., if a feature is known to be non-negative), this preprocessing can even be done with no impact on the privacy guarantees.
In a similar manner, while the set of considered output functions are restricted to linear predictors without an explicit bias term, an implicit bias term can still be used by augmenting the feature vector to include a constant term whose corresponding coefficient represents the bias.

The conditions on $\ell$ and $R$ can be achieved by weighted versions of common classification algorithms such as logistic regression and SVM with only minor modifications (if any).
The logistic loss function, for example, for logistic regression satisfies conditions 3, 4, and 5 with no modification.
The hinge loss function commonly used for SVM satisfies condition 3, and can be easily smoothed to an approximation that also satisfies conditions 4 and 5. 
The popular $\ell_2$ regularizer used for ridge regression satisfies condition 6, however, the $\ell_1$ regularized used for LASSO regression does not as it is not differentiable at 0.

We are now equipped to prove the $\ell_2$-global sensitivity of wERM under the required regularity conditions.

\begin{pro}
\label{prop:wERM_sens}
    Let $\mathcal{A}$ represent a wERM algorithm satisfying the regularity conditions for DP-wERM.
    Then the $\ell_2$-global sensitivity of $\mathcal{A}$ is $\Delta_{2, \mathcal{A}} = \frac{2W}{\gamma}$.
\end{pro}

\textbf{Proof} 
Let $D$ and $\tilde{D}$ be neighboring datasets consisting of feature-label-weight triples. 
Without loss of generality, assume they differ only on individual $n$.
That is, assume $D_i = (\mathbf{x}_i, y_i, w_i) = (\tilde{\mathbf{x}}_i, \tilde{y}_i, \tilde{w}_i) = \tilde{D}_i$ for $i \in \{1,2,\ldots, n-1\}$ and $D_n = (\mathbf{x}_n, y_n, w_n) \ne (\tilde{\mathbf{x}}_n, \tilde{y}_n, \tilde{w}_n) = \tilde{D}_n$.
Now, define the following functions:
\begin{align}
    g_1(\boldsymbol{\theta}) &= \frac{1}{n}\sum_{i=1}^n w_i\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i) + \frac{\gamma}{n}R(f_{\boldsymbol{\theta}}), \\
    \tilde{g}_1(\boldsymbol{\theta}) &= \frac{1}{n}\sum_{i=1}^n \tilde{w}_i\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_i), \tilde{y}_i) + \frac{\gamma}{n}R(f_{\boldsymbol{\theta}}), \\
    g_2(\boldsymbol{\theta}) &= \tilde{g}_1(\boldsymbol{\theta}) - g_1(\boldsymbol{\theta}) = \frac{1}{n}\left(\tilde{w}_n\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_n), \tilde{y}_n) - w_n\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_n), y_n)\right).
\end{align}
By regularity condition 4, $\ell$ is convex and everywhere differentiable, and by regularity condition 6, $R$ is 1-strongly convex and everywhere differentiable.
Since $w_i\ge0$ and $\tilde{w}_i\ge0$, $g_1$ and $\tilde{g}_1 = g_1 + g_2$ are both $\frac{\lambda}{n}$-strongly convex and everywhere differentiable.
By regularity conditions 2 and 3, we can re-write $\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), y_i)$ as  $\ell(y_i\mathbf{x}_i\boldsymbol{\theta})$, which implies
\begin{align}
    \nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta}) &= \frac{1}{n}\left(\tilde{w}_n\nabla_{\boldsymbol{\theta}}\ell(f_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_n), \tilde{y}_n) - w_n\nabla_{\boldsymbol{\theta}}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_n), y_n)\right) \\
    &= \frac{1}{n}\left(\tilde{w}_n\nabla_{\boldsymbol{\theta}}\ell(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\ell(y_n\mathbf{x}_n\boldsymbol{\theta})\right).
\end{align}
Now, by regularity conditions 1 and 5, we can bound $\norm{\nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta})}$ as
\begin{align}
    \norm{\nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta})} &\le \frac{1}{n}\norm{\tilde{w}_n\nabla_{\boldsymbol{\theta}}\ell(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta}) - w_n\nabla_{\boldsymbol{\theta}}\ell(y_n\mathbf{x}_n\boldsymbol{\theta})} \\
    &= \frac{1}{n}\norm{\tilde{w}_n \tilde{y}_n\tilde{\mathbf{x}}_n\ell^\prime(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta}) - w_n y_n\mathbf{x}_n\ell^\prime(y_n\mathbf{x}_n\boldsymbol{\theta})} \\
    &\le \frac{1}{n}\left(\norm{\tilde{w}_n \tilde{y}_n\tilde{\mathbf{x}}_n\ell^\prime(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta})} + \norm{w_n y_n\mathbf{x}_n\ell^\prime(y_n\mathbf{x}_n\boldsymbol{\theta})}\right) \\
    &= \frac{1}{n}\left(|\tilde{w}_n| |\tilde{y}_n| \norm{\tilde{\mathbf{x}}_n} |\ell^\prime(\tilde{y}_n\tilde{\mathbf{x}}_n\boldsymbol{\theta})| + |w_n| |y_n| \norm{\mathbf{x}_n} |\ell^\prime(y_n\mathbf{x}_n\boldsymbol{\theta})|\right) \\
    &\le \frac{1}{n}\left(\tilde{w}_n + w_n\right)\le \frac{2W}{n}.
\end{align}
Finally, define $\boldsymbol{\hat{\theta}}_1 = \mathcal{A}(D) = \argmin_{\boldsymbol{\theta}} g_1(\boldsymbol{\theta})$ and $\boldsymbol{\hat{\theta}}_2 = \mathcal{A}(\tilde{D}) = \argmin_{\boldsymbol{\theta}} \tilde{g}_1(\boldsymbol{\theta}) = \argmin_{\boldsymbol{\theta}}g_1(\boldsymbol{\theta}) + g_2(\boldsymbol{\theta})$.
By Lemma \ref{lem:Chaudhuri_lemma},
\begin{align}
    \pushQED{\qed}
    \Delta_{2, \mathcal{A}} &= \max_{\substack{D, \tilde{D} \\ d(D,\tilde{D}) = 1}}\norm{\mathcal{A}(D) - \mathcal{A}(\tilde{D})}_2 = \max_{\substack{D, \tilde{D} \\ d(D,\tilde{D}) = 1}}\norm{\boldsymbol{\hat{\theta}}_1 - \boldsymbol{\hat{\theta}}_2}_2 \\
    &= \frac{n}{\gamma} \max_{\boldsymbol{\theta}}\norm{\nabla_{\boldsymbol{\theta}}g_2(\boldsymbol{\theta})} = \left(\frac{n}{\gamma}\right) \left(\frac{2W}{n}\right) = \frac{2W}{\gamma}. \\
    & \qedhere
    \popQED
\end{align}

Proposition \ref{prop:wERM_sens} proves that the sensitivity chosen in Algorithm \ref{alg:DP-wERM} is indeed correct, which is the crucial piece for a proof of DP in this case.
With this sensitivity value, standard DP proof methods from \citet{Dwork2006} demonstrate that Algorithm \ref{alg:DP-wERM} satisfies DP.
To neatly summarize the work in this section, we state this in the following theorem, but omit the proof as it is identical to the proof of Theorem 6 from \citet{chaudhuri2011}, only replacing the sensitivity from that work with the one from Proposition \ref{prop:wERM_sens}.

\begin{thm}
    Assuming the regularity conditions for DP-wERM hold, Algorithm \ref{alg:DP-wERM} satisfies $\epsilon$-DP.
\end{thm}

\subsection{Differentially private outcome weighted learning (DP-OWL)}
\label{subsec:DP-OWL}

As OWL is a special case of wERM, it is natural to suspect that running Algorithm \ref{alg:DP-wERM} with appropriately defined inputs would fit an OWL model satisfying $\epsilon$-DP.
This is fundamentally true, but a few modifications need to be made to the OWL algorithm in order to satisfy the regularity conditions for DP-wERM.
Corollary \ref{cor:DP-OWL} and its accompanying proof specifically state which regularity conditions are already satisfied by OWL, and enumerate the modifications necessary to meet the remaining conditions.
We will refer to an OWL algorithm modified in such a way as a DP-OWL algorithm.

First, we define an approximation to the hinge loss used in OWL known as the Huber loss \citep{Chapelle2007}.

\begin{defn}
    \textnormal{(Huber loss)} For a given Huber loss parameter $h$, the \textnormal{Huber loss} $\ell_{\textnormal{Huber}}: \mathbb{R}\times \mathbb{R} \rightarrow \mathbb{R}$ is defined to be 
    \begin{equation}
        \ell_{\textnormal{Huber}}(\hat{y}, y) = \begin{cases} 
        0, & \textnormal{if } y\hat{y} > 1+h \\
        \frac{1}{4h}(1+h-y\hat{y})^2, & \textnormal{if } |1 - y\hat{y}| \le h \\
        1 - y\hat{y}, & \textnormal{if } y\hat{y} < 1 - h \\
        \end{cases}.
    \end{equation}
    Equivalently, by letting $z = y\hat{y}$, the \textnormal{Huber loss} $\ell_{\textnormal{Huber}}: \mathbb{R} \rightarrow \mathbb{R}$ could be defined as
    \begin{equation}
        \ell_{\textnormal{Huber}}(z) = \begin{cases} 
        0, & \textnormal{if } z > 1+h \\
        \frac{1}{4h}(1+h-z)^2, & \textnormal{if } |1 - z| \le h \\
        1 - z, & \textnormal{if } z < 1 - h \\
        \end{cases},
    \end{equation}
    distinguishing between the two definitions by the number of inputs.
\end{defn}

It is straightforward to show that the Huber loss is convex and everywhere differentiable.
Additionally, the absolute value of the derivative $|\ell^\prime_{\textnormal{Huber}}(z)|$ is maximized at 1 in the third case, meaning $|\ell^\prime_{\textnormal{Huber}}(z)| \le 1$ for all $z$.

\begin{cor}
\label{cor:DP-OWL}
    Consider the OWL algorithm solving the problem
    \begin{equation}
        \argmin_{f_{\boldsymbol{\theta}, b}\in\mathcal{F}} \frac{1}{n} \sum_{i=1}^n\frac{B_i}{P(A_i|\mathbf{x}_i)} \max\{0, 1 - A_if_{\boldsymbol{\theta}, b}(\mathbf{x}_i)\} + \frac{\gamma}{n}\norm{\boldsymbol{\theta}}.
    \end{equation}
    Replace the hinge loss $\ell_{\textnormal{hinge}}(f_{\boldsymbol{\theta}, b}, A_i) = \max\{0, 1 - A_if_{\boldsymbol{\theta}, b}(\mathbf{x}_i)\}$ with the Huber loss approximation $\ell_{\textnormal{Huber}}(f_{\boldsymbol{\theta}, b}, A_i)$.
    Then, if the bias term is fixed at $b=0$ and the dataset $D = (\mathbf{x}_i, y_i, \frac{B_i}{P(A_i|\mathbf{x}_i)})$ satisfies DP-wERM regularity condition 1 (i.e. $\norm{\mathbf{x}_i}\le1$ and $0\le \frac{B_i}{P(A_i|\mathbf{x}_i)}\le W$ for some $W\in\mathbb{R}$), then Algorithm \ref{alg:DP-wERM} run on these inputs satisfies $\epsilon$-DP.
\end{cor}

\textbf{Proof}
Regularity condition 1 for the dataset $D$ is satisfied by assumption.
Since $b=0$, the set of considered output functions $\mathcal{F}$ satisfies condition 2.
Replacing the hinge loss with the Huber loss approximation satisfies constraints 3, 4, and 5.
Finally, it is straightforward to show that the $\ell_2$ regularizer $R(f_{\boldsymbol{\theta}, b}) = \norm{\boldsymbol{\theta}}$ satisfies condition 6.
Since all regularization conditions are satisfied, Algorithm \ref{alg:DP-wERM} with these inputs is $\epsilon$-DP while approximating OWL. \qed

\section{Simulation Studies}
\label{sec:sim_studies}

This section demonstrates the performance of DP-OWL in a practical scenario via simulation studies.
We first introduce the setting for the simulated data before discussing feature selection and hyperparameter tuning, including best practices for accomplishing these with minimal privacy loss.
Finally, we present the results of the simulation studies as measured by the prediction accuracy of the underlying optimal treatment assignment and the estimated treatment value function, which measures the benefit provided by the DP-OWL algorithm.
These results are reported for various combinations of the privacy budget $\epsilon$ and the training dataset size $n$.

\subsection{Simulation Settings}
\label{subsec:sim_settings}

Our simulation settings aim to replicate a phase II clinical trial.
These trials are usually performed on a few hundred individuals and are used to determine whether a given treatment does or does not result in the hypothesized effect.
In the trials, prognostic characteristics are collected from each participant, the participants are randomly assigned to one of the possible treatments, and the treatment benefit for each participant is measured.
The information collected from each participant is important in evaluating the treatment efficacy, but is simultaneously extremely sensitive.
As a result, legal and ethical concerns regarding individual privacy often restrict access to the data.
These simulations seek to demonstrate how DP can be harnessed to provide an extra layer of privacy for this sensitive data.

The following settings define the distributions from which the simulation values are drawn.
\begin{itemize}
    \item Each feature vector $\mathbf{x}_i = (x_{i,1}, x_{i,2}, \ldots, x_{i,10}) \in\mathbb{R}^{10}$ is drawn iid from a standard uniform distribution (i.e. $x_{i,j} \stackrel{iid}{\sim} U[0, 1]$ for all $i, j$).
    These represent prognostic characteristics recorded for each individual in a clinical trial.
    \item The propensity $P(A_i=1|\mathbf{x}_i) = P(A_i=-1|\mathbf{x}_i) = 0.5$ for each treatment assignment $A_i\in\{-1, 1\}$.
    This means each individual is randomly assigned with probability 0.5 to either treatment $A=1$ or $A=-1$, independent of their characteristics.
    \item To simulate realistic benefit data and to evaluate the performance of the DP-OWL model, we assume the underlying optimal treatment is the sign of the function $f(\mathbf{x}_i) = 1 + x_{i, 1} + x_{i, 2} - 1.8x_{i, 3} - 2.2x_{i, 4}$.
    Note $\E[f(\mathbf{x}_i)] = 0$, so approximately half of the simulated individuals receive optimal benefit from each of the two treatments.
    \item Treatment benefits $B_i$ are drawn from a normal distribution $N(\mu_i, \sigma^2)$, with mean $\mu_i = 0.01 + 0.02x_{i, 4} + 3A_if(\mathbf{x}_i)$ and standard deviation $\sigma=0.5$.
    \item If any $B_i<0$, the simulated treatment benefits $B_i$ are then shifted to be $B_i + |\min\{B_i\}| + 0.001$.
    This shift is done to ensure that $w_i = \frac{B_i}{P(A_i|\mathbf{x}_i)}\ge 0$ as required.
    Based on this definition, an individual who is assigned to the optimal treatment for their characteristics as defined by $f$ is likely to receive a larger treatment benefit, while those assigned to the ``wrong'' treatment are likely to receive a smaller benefit.
\end{itemize}

Models trained via the DP-OWL algorithm are evaluated in two ways.
First, if the underlying optimal treatment function $f$ is available, the accuracy of the model at assigning samples to the optimal treatment can be calculated.
Alternatively for real-world situations where this function is unavailable, the empirical value function can be used.
This function $V: \mathcal{F} \rightarrow \mathbb{R}$ is defined as
\begin{equation}
    V(\hat{f}) = \frac{\frac{1}{n}\sum_{i=1}^n \frac{I(\hat{f}(\mathbf{x}_i) = A_i)}{P(A_i|\mathbf{x}_i)}B_i}{\frac{1}{n}\sum_{i=1}^n \frac{I(\hat{f}(\mathbf{x}_i) = A_i)}{P(A_i|\mathbf{x}_i)}},
\end{equation}
where $\hat{f}$ is the resulting decision function from running the DP-OWL algorithm and $I$ is the indicator function.

\subsection{Feature selection}
\label{subsec:feature_selection}

The number of features $\mathbf{x}_i$ in a DP-OWL model determines the number of coefficients $\boldsymbol{\theta}$ in the predictor function, and this in turn determines the size of the noise vector $\mathbf{\hat{z}}$ that is added to achieve DP.
To obtain optimal performance for ML models, it is generally good practice to perform feature selection to limit the number of coefficients to only those that are most helpful in making predictions.
This holds true for DP-OWL as well, but with the additional benefit that limiting the number of features also limits the size of the necessary noise vector.
 
Feature selection was performed prior to running the DP-OWL simulations presented in this paper.
To do this, we assume that there exists a publicly available dataset similar to the sensitive dataset, which we will use for feature selection as well as hyperparameter tuning as described in the next section.
We acknowledge that this is a strong assumption that may not be met in practice (though it is not totally impossible).
Nevertheless, our simulations seek to highlight the performance of the DP-OWL algorithm in the best-case scenario, where feature selection and hyperparameter tuning are performed well, so we proceed with this approach.
For this public data, we generated a dataset $(\mathbf{x}_i, A_i, B_i)$ of 1000 observations according to the simulation settings from Section \ref{subsec:sim_settings}.
 
Using our public dataset, we run 10-fold cross-validation to fit a LASSO-regularized weighted logistic regression model to the simulated data.
Following the DP-OWL structure, the $A_i$ values were used as the target variable and the weights were set to be $w_i = \frac{B_i}{P(A_i|\mathbf{x}_i)}$.
Based on the resulting trained coefficients, it was clear that the first four features were most impactful to the model, so that subset was used to run subsequent simulations.
This matches our expectations based on the assumed underlying optimal treatment function from the simulation settings.

One benefit of performing feature selection only on the original features is that it preserves the explanability of the predictive model.
However, for a real-world dataset, it is possible that there is sufficient correlation between features that selecting few enough features to achieve good performance of DP-OWL is difficult.
In such cases, it may be valuable to first use correlation-reducing variable transformation techniques such as principal component analysis (PCA) before the feature selection step.
These techniques were not utilized for our simulations because the features are already independent as an assumption of the simulation settings, but they may be valuable in a real-world implementation.

\subsection{Hyperparameter tuning}
\label{subsec:hyperparameter_tuning}

As a final preliminary step to running the DP-OWL algorithm, we discuss how hyperparameter values were selected.
In general, caution must be used to avoid violating DP guarantees when tuning hyperparameters, as utilizing information from the sensitive dataset to select the hyperparameters results in privacy loss.
There are two approaches that can be taken to handle this.
The first is to allocate a portion of the privacy budget $\epsilon$ to tuning the hyperparameters using the sensitive dataset itself.
If the dataset is sufficiently large, it may be possible to perform the hyperparameter turning without having to sacrifice too much of the budget to be detrimental to results.
However, for the dataset sizes typical for phase II clinical trials, this approach is likely to substantially hinder performance.

The second possible approach is to perform hyperparameter tuning independent from the sensitive dataset.
In this manner, the entire privacy budget can be reserved for the model training.
This is the approach we take for our simulations.
The hyperparameters necessary to tune are upper and lower bounds on the the features, an upper bound on the absolute value of the weights, and the regularization constant.

The regularity conditions require that the feature vectors $\mathbf{x}_i$ be subject to the bound $\norm{\mathbf{x}_i}\le1$.
Our simulation settings require some modifications to satisfy this condition, as by default it is only certain that $\norm{\mathbf{x}_i}\le \sqrt{10}$.
To maintain privacy without utilizing any privacy budget for our simulations, we preprocess the $\mathbf{x}_i$ feature vectors by dividing them by $\sqrt{10}$ to obtain $\tilde{\mathbf{x}}_i = \frac{\mathbf{x}_i}{\sqrt{10}}$, and then use $\tilde{\mathbf{x}}_i$ when running the DP-OWL algorithm.
To do this, we must assume that it is public knowledge (i.e. the information does not need to be kept private) that $x_{i,j} \stackrel{iid}{\sim} U[0, 1]$ for all $i, j$.
By so doing, we can use the global upper and lower bounds of 1 and 0, respectively, for each feature to obtain the $\sqrt{10}$ scaling value.
Since this is done without reference to the sensitive dataset itself, we can scale the feature vectors as required without utilizing a portion of the privacy budget.

Of course, practical real-world datasets are not likely to satisfy $\norm{\mathbf{x}_i}\le1$, so some preprocessing will be required.
Fortunately, it is not unreasonable to infer public bounds on many potential features independently of the sensitive data itself.
The age of a clinical trial participant is a good example.
Age information may be beneficial to use as a feature for training a DP-OWL model, as a participant's age may be influential in determining an ideal treatment.
This feature also has natural upper and lower bounds that can be inferred independent of any given sensitive dataset.
If there is no available public knowledge of the age range for the trial participants, for example, we may choose set age 0 as a lower bound and age 100 as an upper bound, clipping the age of any participants outside of the bounds before running the DP-OWL algorithm.
As an alternative example, if it is publicly known that this is a clinical trial for adults ages 20-40, we would set the bounds as such, being sure not to refer to the dataset itself when setting the bounds, but only to publicly available information about the trial.
Similar natural bounds based on public information are likely to exist for many features, and these should be used when possible to avoid unnecessary privacy budget expenditure.
That being said, if for some feature the bounds are difficult to obtain without referencing the data, a portion of the privacy budget could be allocated to utilize the data to select the bounds.

Another hyperparameter that must be tuned is the upper bound $W$ for the weights $w_i=\frac{B_i}{P(A_i|\mathbf{x}_i)}$.
A larger $W$ corresponds to a larger scale for the additive noise from Algorithm \ref{alg:DP-wERM}, so it is ideal to select $W$ to be as close as possible to the largest $w_i$ in the dataset.
The denominator of $w_i$ is the propensity for being assigned to each treatment, which we assume is publicly known to be $0.5$.
Thus, to select $W$ we need an idea of the range of values for the clinical benefit $B_i$.
As with the features $\mathbf{x}_i$, there may exist natural bounds to this value, which can be used to select $W$.
However, for our simulations, we rely on the similar, public dataset discussed in Section \ref{subsec:feature_selection} to bound the benefit.
The $B_i$ values in the public dataset indicate that most $B_i$ values are captured within the range $(0, 15)$. 
Dividing the bound for $B_i$ by the propensity, we obtain $W=30$ as a good weight upper bound for our simulations.
Weights that happen to be larger than $W$ are clipped to the bound.

The final hyperparameter that needs to be tuned is the regularization constant $\gamma$.
Experiments showed that the optimal value for $\gamma$ varied as a function of the privacy budget $\epsilon$ and the sample size $n$.
Our simulations present results for several combinations of $\epsilon$ and $n$, so the public dataset was used to tune an optimal regularization constant for each combination.
To do this, a set of possible values for the regularization constant is chosen. 
Then, a random sample of the same size as the target sample size $n$ is taken from the public dataset, and is used to train a DP-OWL model via Algorithm \ref{alg:DP-wERM} using the target $\epsilon$.
Feature selection, as well as other hyperparameter value selection is performed as previously described.
The resulting model is then evaluated on the remaining $1000-n$ samples from the public dataset.
A minimum of 500 samples are used in the evaluation set, meaning up to 500 of the 1000 samples are used for training.
If the target sample size $n$ is larger than 500, additional samples are bootstrapped from the training set until the target sample size is obtained.
After training the model in this manner, the empirical value function is computed on the evaluation set.
This sample, train, evaluate procedure is repeated 1000 times for each considered regularization constant value, and the regularization constant producing the largest mean empirical value function for the given combination of $\epsilon$ and $n$ is chosen.

\subsection{Outline of simulation procedure}
\label{subsec:sim_procedure}

The following is an outline of the simulation procedure used to demonstrate the performance of the DP-OWL algorithm.
The procedure is repeated for each combination of $\epsilon=\{0.1, 0.5, 1, 2, 5\}$ and $n = \{200, 500, 800, 1000\}$.
As a baseline for comparison, we also run the simulations for each sample size without using DP guarantees, which can be thought of as having a privacy budget of $\epsilon=\infty$.
\begin{enumerate}
    \item Training data of the given sample size $n$ is simulated based on the settings.
    \item Validation data of size 5000 is simulated based on the same settings.
    \item The feature selection procedure from Section \ref{subsec:feature_selection} is used to reduce the size of the feature vector for both datasets.
    \item A DP-OWL model is trained using Algorithm \ref{alg:DP-wERM} on the training data with the given privacy budget $\epsilon$ using the hyperparameters chosen as described in Section \ref{subsec:hyperparameter_tuning}.
    \item The trained model is evaluated on the validation dataset based on its optimal assignment accuracy and empirical treatment value function.
    \item Steps 1-5 are repeated 200 times and the mean accuracy and treatment value are collected, as well as 95\% confidence intervals for the means.
\end{enumerate}

\subsection{Results}
\label{subec:results}

\begin{table}[ht]
    \centering
    \begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \multicolumn{2}{|c|}{\textbf{Simulated Values}} & \multicolumn{2}{|c|}{\textbf{Assignment Accuracy (\%)}} & \multicolumn{2}{|c|}{\textbf{Treatment Value}} \\
    \hline
    $\epsilon$ & $n$ & Mean & 95\% CI & Mean & 95\% CI \\
    \hline
    \multirow{4}{*}{0.1} & 200 & 50.24 & (49.06, 51.42) & 8.36 & (8.26, 8.46)\\
    \cline{2-6}
    & 500 & 50.20 & (48.89, 51.52) & 8.42 & (8.32, 8.52)\\
    \cline{2-6}
    & 800 & 50.70 & (49.45, 51.94) & 8.40 & (8.30, 8.50)\\
    \cline{2-6}
    & 1000 & 51.08 & (49.72, 52.44) & 8.41 & (8.32, 8.51)\\
    \hline
    \multirow{4}{*}{0.5} & 200 & 52.87 & (51.49, 54.25) & 8.56 & (8.46, 8.66)\\
    \cline{2-6}
    & 500 & 58.02 & (56.58, 59.47) & 8.84 & (8.73, 8.95)\\
    \cline{2-6}
    & 800 & 60.20 & (58.61, 61.79) & 9.04 & (8.92, 9.16)\\
    \cline{2-6}
    & 1000 & 60.95 & (59.22, 62.68) & 9.05 & (8.93, 9.16)\\
    \hline
    \multirow{4}{*}{1} & 200 & 55.53 & (54.23, 56.83) & 8.77 & (8.67, 8.86)\\
    \cline{2-6}
    & 500 & 61.03 & (59.36, 62.70) & 9.07 & (8.96, 9.18)\\
    \cline{2-6}
    & 800 & 64.08 & (62.22, 65.93) & 9.25 & (9.13, 9.37)\\
    \cline{2-6}
    & 1000 & 64.70 & (62.93, 66.48) & 9.28 & (9.17, 9.40)\\
    \hline
    \multirow{4}{*}{2} & 200 & 58.99 & (57.30, 60.68) & 8.90 & (8.79, 9.02)\\
    \cline{2-6}
    & 500 & 68.17 & (66.27, 70.08) & 9.49 & (9.38, 9.61)\\
    \cline{2-6}
    & 800 & 74.32 & (72.50, 76.14) & 9.91 & (9.80, 10.02)\\
    \cline{2-6}
    & 1000 & 79.03 & (77.42, 80.63) & 10.12 & (10.03, 10.21)\\
    \hline
    \multirow{4}{*}{5} & 200 & 67.66 & (65.88, 69.43) & 9.51 & (9.40, 9.63)\\
    \cline{2-6}
    & 500 & 78.79 & (77.29, 80.29) & 10.13 & (10.04, 10.21)\\
    \cline{2-6}
    & 800 & 81.97 & (80.57, 83.37) & 10.23 & (10.15, 10.31)\\
    \cline{2-6}
    & 1000 & 84.80 & (83.62, 85.99) & 10.31 & (10.24, 10.38)\\
    \hline
    \multirow{4}{*}{$\infty$ (No DP)} & 200 & 88.65 & (88.02, 89.27) & 10.41 & (10.36, 10.47)\\
    \cline{2-6}
    & 500 & 92.18 & (91.75, 92.60) & 10.52 & (10.47, 10.57)\\
    \cline{2-6}
    & 800 & 93.39 & (93.02, 93.75) & 10.54 & (10.49, 10.60)\\
    \cline{2-6}
    & 1000 & 94.29 & (93.97, 94.61) & 10.62 & (10.57, 10.68)\\
    \hline
    \end{tabular}
    \end{center}
    \caption{Simulation results for various combinations of $\epsilon$ and $n$ are presented.
    The mean as well as 95\% confidence intervals (CI) are shown for both the optimal treatment assignment accuracy (as a percentage) and the empirical treatment value computed over the validation set.
    The case without DP (corresponding to $\epsilon=\infty$) is also shown for reference.}
    \label{tab:sim_results}
\end{table}

% Figure environment removed

This section analyzes the results of the simulation studies.
Table \ref{tab:sim_results} presents the mean and 95\% CIs for optimal treatment assignment accuracy and empirical treatment value from the simulation study, stratified by six different levels of privacy protection $\epsilon$ and four different sample sizes $n$.
Figure \ref{fig:sim_results} presents the same results visually.
As expected, the accuracy and treatment value improve as the level of privacy guarantees weaken, approaching the level of performance for the non-private case as $\epsilon$ increases.
Accuracy and treatment value also improve as the sample size increases for any fixed value of $\epsilon$.

We can draw a couple of conclusions from these simulation results.
First, we can conclude that, in some cases, it is possible to provide some DP protections to OWL algorithms while still achieving sufficiently useful model performance.
For example, our DP-OWL algorithm can reasonably achieve 80\% or larger optimal treatment prediction accuracy with a sample size of around 1000 while satisfying 2-DP.
If the DP guarantees are relaxed to 5-DP, 80\% or larger accuracy can be obtained with a sample size of only 500 or so.
Though strong privacy protection with DP is generally thought to be achieved with $\epsilon\le 1$, it is not uncommon to use larger values of $\epsilon$ in many applications.
For example, the US Census Bureau used $\epsilon=19.61$ to release statistical information from the 2020 census \citep{USCensus}.

On the other hand, the results also demonstrate that our DP-OWL algorithm in its current state is not capable of providing consistently high accuracy and treatment value for use cases limited to exceptionally small sample sizes or stringent privacy budgets.
Several modifications to the algorithm were explored to try to improve the performance.
These included attempts to reduce the noise scale by scaling or discretizing the benefit values and increasing the regularization constant from the tuned value, as well as attempts to amplify privacy guarantees via subsampling, but these attempts did not result in improved performance.
While unfortunate, there is an intuitive explanation for why this problem is difficult.
DP provides privacy protection by ensuring the outputs of a DP mechanism are similar on neighboring inputs, meaning that a single individual cannot influence the results ``too much''.
OWL, in contrast, aims to fit a model for individualized treatment assignment by weighting the input by how much it should influence the fitted model output.
Thus, in some sense, DP and OWL have competing goals that must be balanced to adequately satisfy the demands of each.

\section{Case Study}
\label{sec:case_study}

TODO: Discuss case study when data available

\section{Discussion}
\label{sec:discussion}

This paper presented and proved a general algorithm for achieving $\epsilon$-DP for wERM models.
This represents the first such extension of previous work by \citet{chaudhuri2011} to allow for weighted training examples.
While this result is of interest in its own right, this paper specifically applied the result to OWL algorithms with the goal of providing methods for enhancing privacy guarantees for individuals whose sensitive data is used in clinical trials.
Simulation study results demonstrate that for models fit with a sufficiently large privacy budget using datasets with a sufficiently large sample size, reasonable optimal treatment prediction accuracy and empirical treatment value can be achieved while simultaneously providing DP guarantees.

As clinical trail data is extremely sensitive, being able to provide privacy protection guarantees for participants is extremely valuable.
Individuals are likely to feel more comfortable participating in clinical trials the more secure they feel their personal data is.
Likewise, by utilizing DP guarantees to train treatment assignment algorithms, researchers can have more confidence that participant data is safe.

The DP-OWL algorithm from this paper struggles at small sample sizes and privacy budgets, which warrants further exploration of ways to improve performance.
\citet{chaudhuri2011} presents two algorithms for achieving DP for standard ERM methods.
One is to fit the ERM model without DP and then perturb the output, which is the method utilized in this paper.
The other method is to perturb the objective function used to fit the model.
It may be worth exploring how to adapt the objective perturbation algorithm to the wERM case and analyzing its performance.
Another potentially useful direction is to explore relaxing the DP guarantees from $\epsilon$-DP to $(\epsilon, \delta)$-DP and analyze how that impacts performance.

On a different note, it may be valuable to explore the performance of the DP-OWL algorithm presented in this paper in other application regimes where ITRs are commonly applied, such as personalized advertizing and recommender systems.
These applications also use sensitive datasets to build models, but are likely to be less restrictive in the potential sample size, which means that DP-OWL may be more easily applied.

Finally, other algorithms that build on the original OWL algorithm \citep{Zhao2012}, such as RWL \citep{Zhou2017} and M-learning \citep{Wu2020}, may be good candidates for developing DP-satisfying counterparts.
It may be interesting to compare the performance of these DP algorithms with the DP-OWL algorithm from this paper.

\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}